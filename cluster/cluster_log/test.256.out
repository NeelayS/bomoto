Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=256, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14336-14391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821625
Iteration 2/25 | Loss: 0.00146080
Iteration 3/25 | Loss: 0.00111690
Iteration 4/25 | Loss: 0.00100093
Iteration 5/25 | Loss: 0.00097651
Iteration 6/25 | Loss: 0.00097603
Iteration 7/25 | Loss: 0.00097882
Iteration 8/25 | Loss: 0.00095850
Iteration 9/25 | Loss: 0.00095124
Iteration 10/25 | Loss: 0.00094181
Iteration 11/25 | Loss: 0.00093336
Iteration 12/25 | Loss: 0.00093746
Iteration 13/25 | Loss: 0.00093087
Iteration 14/25 | Loss: 0.00093043
Iteration 15/25 | Loss: 0.00093235
Iteration 16/25 | Loss: 0.00092811
Iteration 17/25 | Loss: 0.00092687
Iteration 18/25 | Loss: 0.00092647
Iteration 19/25 | Loss: 0.00092628
Iteration 20/25 | Loss: 0.00092613
Iteration 21/25 | Loss: 0.00092609
Iteration 22/25 | Loss: 0.00092609
Iteration 23/25 | Loss: 0.00092609
Iteration 24/25 | Loss: 0.00092609
Iteration 25/25 | Loss: 0.00092609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.93908930
Iteration 2/25 | Loss: 0.00168031
Iteration 3/25 | Loss: 0.00168028
Iteration 4/25 | Loss: 0.00168028
Iteration 5/25 | Loss: 0.00168028
Iteration 6/25 | Loss: 0.00168028
Iteration 7/25 | Loss: 0.00168028
Iteration 8/25 | Loss: 0.00168028
Iteration 9/25 | Loss: 0.00168028
Iteration 10/25 | Loss: 0.00168028
Iteration 11/25 | Loss: 0.00168028
Iteration 12/25 | Loss: 0.00168028
Iteration 13/25 | Loss: 0.00168028
Iteration 14/25 | Loss: 0.00168028
Iteration 15/25 | Loss: 0.00168028
Iteration 16/25 | Loss: 0.00168028
Iteration 17/25 | Loss: 0.00168028
Iteration 18/25 | Loss: 0.00168028
Iteration 19/25 | Loss: 0.00168028
Iteration 20/25 | Loss: 0.00168028
Iteration 21/25 | Loss: 0.00168028
Iteration 22/25 | Loss: 0.00168028
Iteration 23/25 | Loss: 0.00168028
Iteration 24/25 | Loss: 0.00168028
Iteration 25/25 | Loss: 0.00168028
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016802794998511672, 0.0016802794998511672, 0.0016802794998511672, 0.0016802794998511672, 0.0016802794998511672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016802794998511672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168028
Iteration 2/1000 | Loss: 0.00003828
Iteration 3/1000 | Loss: 0.00002852
Iteration 4/1000 | Loss: 0.00002618
Iteration 5/1000 | Loss: 0.00045602
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002295
Iteration 8/1000 | Loss: 0.00002159
Iteration 9/1000 | Loss: 0.00002091
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002055
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002051
Iteration 14/1000 | Loss: 0.00002049
Iteration 15/1000 | Loss: 0.00002047
Iteration 16/1000 | Loss: 0.00002047
Iteration 17/1000 | Loss: 0.00002046
Iteration 18/1000 | Loss: 0.00002046
Iteration 19/1000 | Loss: 0.00002044
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002029
Iteration 22/1000 | Loss: 0.00002029
Iteration 23/1000 | Loss: 0.00002028
Iteration 24/1000 | Loss: 0.00002028
Iteration 25/1000 | Loss: 0.00002028
Iteration 26/1000 | Loss: 0.00002028
Iteration 27/1000 | Loss: 0.00002028
Iteration 28/1000 | Loss: 0.00002028
Iteration 29/1000 | Loss: 0.00002028
Iteration 30/1000 | Loss: 0.00002028
Iteration 31/1000 | Loss: 0.00002027
Iteration 32/1000 | Loss: 0.00002027
Iteration 33/1000 | Loss: 0.00002022
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002021
Iteration 36/1000 | Loss: 0.00002020
Iteration 37/1000 | Loss: 0.00002020
Iteration 38/1000 | Loss: 0.00002018
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002017
Iteration 43/1000 | Loss: 0.00002017
Iteration 44/1000 | Loss: 0.00002017
Iteration 45/1000 | Loss: 0.00002017
Iteration 46/1000 | Loss: 0.00002017
Iteration 47/1000 | Loss: 0.00002017
Iteration 48/1000 | Loss: 0.00002017
Iteration 49/1000 | Loss: 0.00002016
Iteration 50/1000 | Loss: 0.00002016
Iteration 51/1000 | Loss: 0.00002016
Iteration 52/1000 | Loss: 0.00002016
Iteration 53/1000 | Loss: 0.00002016
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002014
Iteration 56/1000 | Loss: 0.00002014
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002014
Iteration 60/1000 | Loss: 0.00002014
Iteration 61/1000 | Loss: 0.00002014
Iteration 62/1000 | Loss: 0.00002014
Iteration 63/1000 | Loss: 0.00002013
Iteration 64/1000 | Loss: 0.00002013
Iteration 65/1000 | Loss: 0.00002013
Iteration 66/1000 | Loss: 0.00002013
Iteration 67/1000 | Loss: 0.00002013
Iteration 68/1000 | Loss: 0.00002012
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002012
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002011
Iteration 73/1000 | Loss: 0.00002011
Iteration 74/1000 | Loss: 0.00002011
Iteration 75/1000 | Loss: 0.00002010
Iteration 76/1000 | Loss: 0.00002010
Iteration 77/1000 | Loss: 0.00002010
Iteration 78/1000 | Loss: 0.00002010
Iteration 79/1000 | Loss: 0.00002010
Iteration 80/1000 | Loss: 0.00002010
Iteration 81/1000 | Loss: 0.00002009
Iteration 82/1000 | Loss: 0.00002009
Iteration 83/1000 | Loss: 0.00002009
Iteration 84/1000 | Loss: 0.00002009
Iteration 85/1000 | Loss: 0.00002009
Iteration 86/1000 | Loss: 0.00002009
Iteration 87/1000 | Loss: 0.00002009
Iteration 88/1000 | Loss: 0.00002009
Iteration 89/1000 | Loss: 0.00002009
Iteration 90/1000 | Loss: 0.00002009
Iteration 91/1000 | Loss: 0.00002009
Iteration 92/1000 | Loss: 0.00002008
Iteration 93/1000 | Loss: 0.00002008
Iteration 94/1000 | Loss: 0.00002008
Iteration 95/1000 | Loss: 0.00002008
Iteration 96/1000 | Loss: 0.00002008
Iteration 97/1000 | Loss: 0.00002008
Iteration 98/1000 | Loss: 0.00002008
Iteration 99/1000 | Loss: 0.00002008
Iteration 100/1000 | Loss: 0.00002008
Iteration 101/1000 | Loss: 0.00002008
Iteration 102/1000 | Loss: 0.00002008
Iteration 103/1000 | Loss: 0.00002008
Iteration 104/1000 | Loss: 0.00002008
Iteration 105/1000 | Loss: 0.00002008
Iteration 106/1000 | Loss: 0.00002008
Iteration 107/1000 | Loss: 0.00002008
Iteration 108/1000 | Loss: 0.00002008
Iteration 109/1000 | Loss: 0.00002007
Iteration 110/1000 | Loss: 0.00002007
Iteration 111/1000 | Loss: 0.00002007
Iteration 112/1000 | Loss: 0.00002007
Iteration 113/1000 | Loss: 0.00002007
Iteration 114/1000 | Loss: 0.00002007
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002007
Iteration 119/1000 | Loss: 0.00002007
Iteration 120/1000 | Loss: 0.00002007
Iteration 121/1000 | Loss: 0.00002007
Iteration 122/1000 | Loss: 0.00002007
Iteration 123/1000 | Loss: 0.00002007
Iteration 124/1000 | Loss: 0.00002007
Iteration 125/1000 | Loss: 0.00002006
Iteration 126/1000 | Loss: 0.00002006
Iteration 127/1000 | Loss: 0.00002006
Iteration 128/1000 | Loss: 0.00002006
Iteration 129/1000 | Loss: 0.00002006
Iteration 130/1000 | Loss: 0.00002006
Iteration 131/1000 | Loss: 0.00002006
Iteration 132/1000 | Loss: 0.00002006
Iteration 133/1000 | Loss: 0.00002006
Iteration 134/1000 | Loss: 0.00002006
Iteration 135/1000 | Loss: 0.00002006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.0062318071722984e-05, 2.0062318071722984e-05, 2.0062318071722984e-05, 2.0062318071722984e-05, 2.0062318071722984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0062318071722984e-05

Optimization complete. Final v2v error: 3.742535352706909 mm

Highest mean error: 4.364529132843018 mm for frame 18

Lowest mean error: 3.2405078411102295 mm for frame 43

Saving results

Total time: 72.65230894088745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800593
Iteration 2/25 | Loss: 0.00117763
Iteration 3/25 | Loss: 0.00102224
Iteration 4/25 | Loss: 0.00099445
Iteration 5/25 | Loss: 0.00098812
Iteration 6/25 | Loss: 0.00098733
Iteration 7/25 | Loss: 0.00098733
Iteration 8/25 | Loss: 0.00098733
Iteration 9/25 | Loss: 0.00098733
Iteration 10/25 | Loss: 0.00098733
Iteration 11/25 | Loss: 0.00098733
Iteration 12/25 | Loss: 0.00098733
Iteration 13/25 | Loss: 0.00098733
Iteration 14/25 | Loss: 0.00098733
Iteration 15/25 | Loss: 0.00098733
Iteration 16/25 | Loss: 0.00098733
Iteration 17/25 | Loss: 0.00098733
Iteration 18/25 | Loss: 0.00098733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009873295202851295, 0.0009873295202851295, 0.0009873295202851295, 0.0009873295202851295, 0.0009873295202851295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009873295202851295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60365784
Iteration 2/25 | Loss: 0.00149287
Iteration 3/25 | Loss: 0.00149279
Iteration 4/25 | Loss: 0.00149279
Iteration 5/25 | Loss: 0.00149279
Iteration 6/25 | Loss: 0.00149279
Iteration 7/25 | Loss: 0.00149279
Iteration 8/25 | Loss: 0.00149279
Iteration 9/25 | Loss: 0.00149279
Iteration 10/25 | Loss: 0.00149279
Iteration 11/25 | Loss: 0.00149279
Iteration 12/25 | Loss: 0.00149279
Iteration 13/25 | Loss: 0.00149279
Iteration 14/25 | Loss: 0.00149279
Iteration 15/25 | Loss: 0.00149279
Iteration 16/25 | Loss: 0.00149279
Iteration 17/25 | Loss: 0.00149279
Iteration 18/25 | Loss: 0.00149279
Iteration 19/25 | Loss: 0.00149279
Iteration 20/25 | Loss: 0.00149279
Iteration 21/25 | Loss: 0.00149279
Iteration 22/25 | Loss: 0.00149279
Iteration 23/25 | Loss: 0.00149279
Iteration 24/25 | Loss: 0.00149279
Iteration 25/25 | Loss: 0.00149279
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014927926240488887, 0.0014927926240488887, 0.0014927926240488887, 0.0014927926240488887, 0.0014927926240488887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014927926240488887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149279
Iteration 2/1000 | Loss: 0.00004411
Iteration 3/1000 | Loss: 0.00002785
Iteration 4/1000 | Loss: 0.00002339
Iteration 5/1000 | Loss: 0.00002201
Iteration 6/1000 | Loss: 0.00002087
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001885
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001811
Iteration 12/1000 | Loss: 0.00001800
Iteration 13/1000 | Loss: 0.00001800
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001796
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001793
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001790
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001779
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001777
Iteration 88/1000 | Loss: 0.00001777
Iteration 89/1000 | Loss: 0.00001777
Iteration 90/1000 | Loss: 0.00001777
Iteration 91/1000 | Loss: 0.00001777
Iteration 92/1000 | Loss: 0.00001777
Iteration 93/1000 | Loss: 0.00001777
Iteration 94/1000 | Loss: 0.00001777
Iteration 95/1000 | Loss: 0.00001776
Iteration 96/1000 | Loss: 0.00001776
Iteration 97/1000 | Loss: 0.00001776
Iteration 98/1000 | Loss: 0.00001776
Iteration 99/1000 | Loss: 0.00001776
Iteration 100/1000 | Loss: 0.00001776
Iteration 101/1000 | Loss: 0.00001776
Iteration 102/1000 | Loss: 0.00001776
Iteration 103/1000 | Loss: 0.00001776
Iteration 104/1000 | Loss: 0.00001776
Iteration 105/1000 | Loss: 0.00001776
Iteration 106/1000 | Loss: 0.00001776
Iteration 107/1000 | Loss: 0.00001775
Iteration 108/1000 | Loss: 0.00001775
Iteration 109/1000 | Loss: 0.00001775
Iteration 110/1000 | Loss: 0.00001775
Iteration 111/1000 | Loss: 0.00001775
Iteration 112/1000 | Loss: 0.00001775
Iteration 113/1000 | Loss: 0.00001775
Iteration 114/1000 | Loss: 0.00001775
Iteration 115/1000 | Loss: 0.00001775
Iteration 116/1000 | Loss: 0.00001775
Iteration 117/1000 | Loss: 0.00001775
Iteration 118/1000 | Loss: 0.00001774
Iteration 119/1000 | Loss: 0.00001774
Iteration 120/1000 | Loss: 0.00001774
Iteration 121/1000 | Loss: 0.00001774
Iteration 122/1000 | Loss: 0.00001774
Iteration 123/1000 | Loss: 0.00001774
Iteration 124/1000 | Loss: 0.00001774
Iteration 125/1000 | Loss: 0.00001774
Iteration 126/1000 | Loss: 0.00001774
Iteration 127/1000 | Loss: 0.00001774
Iteration 128/1000 | Loss: 0.00001774
Iteration 129/1000 | Loss: 0.00001774
Iteration 130/1000 | Loss: 0.00001774
Iteration 131/1000 | Loss: 0.00001774
Iteration 132/1000 | Loss: 0.00001774
Iteration 133/1000 | Loss: 0.00001774
Iteration 134/1000 | Loss: 0.00001773
Iteration 135/1000 | Loss: 0.00001773
Iteration 136/1000 | Loss: 0.00001773
Iteration 137/1000 | Loss: 0.00001773
Iteration 138/1000 | Loss: 0.00001773
Iteration 139/1000 | Loss: 0.00001773
Iteration 140/1000 | Loss: 0.00001773
Iteration 141/1000 | Loss: 0.00001772
Iteration 142/1000 | Loss: 0.00001772
Iteration 143/1000 | Loss: 0.00001772
Iteration 144/1000 | Loss: 0.00001772
Iteration 145/1000 | Loss: 0.00001772
Iteration 146/1000 | Loss: 0.00001772
Iteration 147/1000 | Loss: 0.00001772
Iteration 148/1000 | Loss: 0.00001772
Iteration 149/1000 | Loss: 0.00001771
Iteration 150/1000 | Loss: 0.00001771
Iteration 151/1000 | Loss: 0.00001771
Iteration 152/1000 | Loss: 0.00001771
Iteration 153/1000 | Loss: 0.00001771
Iteration 154/1000 | Loss: 0.00001771
Iteration 155/1000 | Loss: 0.00001771
Iteration 156/1000 | Loss: 0.00001771
Iteration 157/1000 | Loss: 0.00001771
Iteration 158/1000 | Loss: 0.00001770
Iteration 159/1000 | Loss: 0.00001770
Iteration 160/1000 | Loss: 0.00001770
Iteration 161/1000 | Loss: 0.00001770
Iteration 162/1000 | Loss: 0.00001770
Iteration 163/1000 | Loss: 0.00001769
Iteration 164/1000 | Loss: 0.00001769
Iteration 165/1000 | Loss: 0.00001769
Iteration 166/1000 | Loss: 0.00001769
Iteration 167/1000 | Loss: 0.00001769
Iteration 168/1000 | Loss: 0.00001769
Iteration 169/1000 | Loss: 0.00001769
Iteration 170/1000 | Loss: 0.00001769
Iteration 171/1000 | Loss: 0.00001769
Iteration 172/1000 | Loss: 0.00001769
Iteration 173/1000 | Loss: 0.00001769
Iteration 174/1000 | Loss: 0.00001768
Iteration 175/1000 | Loss: 0.00001768
Iteration 176/1000 | Loss: 0.00001768
Iteration 177/1000 | Loss: 0.00001768
Iteration 178/1000 | Loss: 0.00001768
Iteration 179/1000 | Loss: 0.00001768
Iteration 180/1000 | Loss: 0.00001768
Iteration 181/1000 | Loss: 0.00001768
Iteration 182/1000 | Loss: 0.00001768
Iteration 183/1000 | Loss: 0.00001768
Iteration 184/1000 | Loss: 0.00001768
Iteration 185/1000 | Loss: 0.00001768
Iteration 186/1000 | Loss: 0.00001768
Iteration 187/1000 | Loss: 0.00001768
Iteration 188/1000 | Loss: 0.00001768
Iteration 189/1000 | Loss: 0.00001767
Iteration 190/1000 | Loss: 0.00001767
Iteration 191/1000 | Loss: 0.00001767
Iteration 192/1000 | Loss: 0.00001767
Iteration 193/1000 | Loss: 0.00001767
Iteration 194/1000 | Loss: 0.00001767
Iteration 195/1000 | Loss: 0.00001767
Iteration 196/1000 | Loss: 0.00001767
Iteration 197/1000 | Loss: 0.00001767
Iteration 198/1000 | Loss: 0.00001767
Iteration 199/1000 | Loss: 0.00001767
Iteration 200/1000 | Loss: 0.00001767
Iteration 201/1000 | Loss: 0.00001767
Iteration 202/1000 | Loss: 0.00001767
Iteration 203/1000 | Loss: 0.00001767
Iteration 204/1000 | Loss: 0.00001767
Iteration 205/1000 | Loss: 0.00001767
Iteration 206/1000 | Loss: 0.00001767
Iteration 207/1000 | Loss: 0.00001767
Iteration 208/1000 | Loss: 0.00001767
Iteration 209/1000 | Loss: 0.00001767
Iteration 210/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.767270077834837e-05, 1.767270077834837e-05, 1.767270077834837e-05, 1.767270077834837e-05, 1.767270077834837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.767270077834837e-05

Optimization complete. Final v2v error: 3.63669753074646 mm

Highest mean error: 3.899963617324829 mm for frame 170

Lowest mean error: 3.284412384033203 mm for frame 7

Saving results

Total time: 44.4876983165741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884959
Iteration 2/25 | Loss: 0.00161603
Iteration 3/25 | Loss: 0.00128322
Iteration 4/25 | Loss: 0.00118567
Iteration 5/25 | Loss: 0.00117640
Iteration 6/25 | Loss: 0.00114935
Iteration 7/25 | Loss: 0.00113003
Iteration 8/25 | Loss: 0.00112291
Iteration 9/25 | Loss: 0.00111596
Iteration 10/25 | Loss: 0.00112407
Iteration 11/25 | Loss: 0.00112395
Iteration 12/25 | Loss: 0.00112471
Iteration 13/25 | Loss: 0.00112814
Iteration 14/25 | Loss: 0.00112365
Iteration 15/25 | Loss: 0.00112069
Iteration 16/25 | Loss: 0.00112044
Iteration 17/25 | Loss: 0.00111896
Iteration 18/25 | Loss: 0.00112576
Iteration 19/25 | Loss: 0.00111369
Iteration 20/25 | Loss: 0.00111903
Iteration 21/25 | Loss: 0.00111788
Iteration 22/25 | Loss: 0.00111455
Iteration 23/25 | Loss: 0.00111990
Iteration 24/25 | Loss: 0.00111010
Iteration 25/25 | Loss: 0.00109640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.76827192
Iteration 2/25 | Loss: 0.00260247
Iteration 3/25 | Loss: 0.00260234
Iteration 4/25 | Loss: 0.00260233
Iteration 5/25 | Loss: 0.00260233
Iteration 6/25 | Loss: 0.00260233
Iteration 7/25 | Loss: 0.00260233
Iteration 8/25 | Loss: 0.00260233
Iteration 9/25 | Loss: 0.00260233
Iteration 10/25 | Loss: 0.00260233
Iteration 11/25 | Loss: 0.00260233
Iteration 12/25 | Loss: 0.00260233
Iteration 13/25 | Loss: 0.00260233
Iteration 14/25 | Loss: 0.00260233
Iteration 15/25 | Loss: 0.00260233
Iteration 16/25 | Loss: 0.00260233
Iteration 17/25 | Loss: 0.00260233
Iteration 18/25 | Loss: 0.00260233
Iteration 19/25 | Loss: 0.00260233
Iteration 20/25 | Loss: 0.00260233
Iteration 21/25 | Loss: 0.00260233
Iteration 22/25 | Loss: 0.00260233
Iteration 23/25 | Loss: 0.00260233
Iteration 24/25 | Loss: 0.00260233
Iteration 25/25 | Loss: 0.00260233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260233
Iteration 2/1000 | Loss: 0.00039354
Iteration 3/1000 | Loss: 0.00049637
Iteration 4/1000 | Loss: 0.00047355
Iteration 5/1000 | Loss: 0.00012707
Iteration 6/1000 | Loss: 0.00029077
Iteration 7/1000 | Loss: 0.00025487
Iteration 8/1000 | Loss: 0.00018722
Iteration 9/1000 | Loss: 0.00010773
Iteration 10/1000 | Loss: 0.00121915
Iteration 11/1000 | Loss: 0.00159140
Iteration 12/1000 | Loss: 0.00098531
Iteration 13/1000 | Loss: 0.00070840
Iteration 14/1000 | Loss: 0.00046855
Iteration 15/1000 | Loss: 0.00049305
Iteration 16/1000 | Loss: 0.00053478
Iteration 17/1000 | Loss: 0.00066428
Iteration 18/1000 | Loss: 0.00044354
Iteration 19/1000 | Loss: 0.00041241
Iteration 20/1000 | Loss: 0.00040612
Iteration 21/1000 | Loss: 0.00028547
Iteration 22/1000 | Loss: 0.00030314
Iteration 23/1000 | Loss: 0.00004809
Iteration 24/1000 | Loss: 0.00004547
Iteration 25/1000 | Loss: 0.00004163
Iteration 26/1000 | Loss: 0.00003968
Iteration 27/1000 | Loss: 0.00003820
Iteration 28/1000 | Loss: 0.00003726
Iteration 29/1000 | Loss: 0.00003665
Iteration 30/1000 | Loss: 0.00003596
Iteration 31/1000 | Loss: 0.00003556
Iteration 32/1000 | Loss: 0.00003534
Iteration 33/1000 | Loss: 0.00003507
Iteration 34/1000 | Loss: 0.00003484
Iteration 35/1000 | Loss: 0.00003472
Iteration 36/1000 | Loss: 0.00003455
Iteration 37/1000 | Loss: 0.00003455
Iteration 38/1000 | Loss: 0.00003452
Iteration 39/1000 | Loss: 0.00003448
Iteration 40/1000 | Loss: 0.00003441
Iteration 41/1000 | Loss: 0.00003441
Iteration 42/1000 | Loss: 0.00003439
Iteration 43/1000 | Loss: 0.00003438
Iteration 44/1000 | Loss: 0.00003438
Iteration 45/1000 | Loss: 0.00003437
Iteration 46/1000 | Loss: 0.00003437
Iteration 47/1000 | Loss: 0.00003437
Iteration 48/1000 | Loss: 0.00003437
Iteration 49/1000 | Loss: 0.00003436
Iteration 50/1000 | Loss: 0.00003436
Iteration 51/1000 | Loss: 0.00003436
Iteration 52/1000 | Loss: 0.00003435
Iteration 53/1000 | Loss: 0.00003435
Iteration 54/1000 | Loss: 0.00003434
Iteration 55/1000 | Loss: 0.00003431
Iteration 56/1000 | Loss: 0.00003427
Iteration 57/1000 | Loss: 0.00003427
Iteration 58/1000 | Loss: 0.00003427
Iteration 59/1000 | Loss: 0.00003426
Iteration 60/1000 | Loss: 0.00003426
Iteration 61/1000 | Loss: 0.00003426
Iteration 62/1000 | Loss: 0.00003426
Iteration 63/1000 | Loss: 0.00003426
Iteration 64/1000 | Loss: 0.00003424
Iteration 65/1000 | Loss: 0.00003424
Iteration 66/1000 | Loss: 0.00003423
Iteration 67/1000 | Loss: 0.00003423
Iteration 68/1000 | Loss: 0.00003421
Iteration 69/1000 | Loss: 0.00003421
Iteration 70/1000 | Loss: 0.00003421
Iteration 71/1000 | Loss: 0.00003421
Iteration 72/1000 | Loss: 0.00003421
Iteration 73/1000 | Loss: 0.00003421
Iteration 74/1000 | Loss: 0.00003421
Iteration 75/1000 | Loss: 0.00003420
Iteration 76/1000 | Loss: 0.00003420
Iteration 77/1000 | Loss: 0.00003420
Iteration 78/1000 | Loss: 0.00003420
Iteration 79/1000 | Loss: 0.00003420
Iteration 80/1000 | Loss: 0.00003418
Iteration 81/1000 | Loss: 0.00003418
Iteration 82/1000 | Loss: 0.00003418
Iteration 83/1000 | Loss: 0.00003417
Iteration 84/1000 | Loss: 0.00003417
Iteration 85/1000 | Loss: 0.00003417
Iteration 86/1000 | Loss: 0.00003416
Iteration 87/1000 | Loss: 0.00003416
Iteration 88/1000 | Loss: 0.00003416
Iteration 89/1000 | Loss: 0.00003415
Iteration 90/1000 | Loss: 0.00003415
Iteration 91/1000 | Loss: 0.00003415
Iteration 92/1000 | Loss: 0.00003414
Iteration 93/1000 | Loss: 0.00003414
Iteration 94/1000 | Loss: 0.00003414
Iteration 95/1000 | Loss: 0.00003414
Iteration 96/1000 | Loss: 0.00003414
Iteration 97/1000 | Loss: 0.00003414
Iteration 98/1000 | Loss: 0.00003414
Iteration 99/1000 | Loss: 0.00003414
Iteration 100/1000 | Loss: 0.00003414
Iteration 101/1000 | Loss: 0.00003413
Iteration 102/1000 | Loss: 0.00003413
Iteration 103/1000 | Loss: 0.00003413
Iteration 104/1000 | Loss: 0.00003413
Iteration 105/1000 | Loss: 0.00003413
Iteration 106/1000 | Loss: 0.00003413
Iteration 107/1000 | Loss: 0.00003413
Iteration 108/1000 | Loss: 0.00003413
Iteration 109/1000 | Loss: 0.00003412
Iteration 110/1000 | Loss: 0.00003412
Iteration 111/1000 | Loss: 0.00003412
Iteration 112/1000 | Loss: 0.00003412
Iteration 113/1000 | Loss: 0.00003412
Iteration 114/1000 | Loss: 0.00003412
Iteration 115/1000 | Loss: 0.00003412
Iteration 116/1000 | Loss: 0.00003411
Iteration 117/1000 | Loss: 0.00003411
Iteration 118/1000 | Loss: 0.00003411
Iteration 119/1000 | Loss: 0.00003411
Iteration 120/1000 | Loss: 0.00003411
Iteration 121/1000 | Loss: 0.00003411
Iteration 122/1000 | Loss: 0.00003410
Iteration 123/1000 | Loss: 0.00003410
Iteration 124/1000 | Loss: 0.00003410
Iteration 125/1000 | Loss: 0.00003410
Iteration 126/1000 | Loss: 0.00003410
Iteration 127/1000 | Loss: 0.00003410
Iteration 128/1000 | Loss: 0.00003410
Iteration 129/1000 | Loss: 0.00003410
Iteration 130/1000 | Loss: 0.00003410
Iteration 131/1000 | Loss: 0.00003410
Iteration 132/1000 | Loss: 0.00003410
Iteration 133/1000 | Loss: 0.00003409
Iteration 134/1000 | Loss: 0.00003409
Iteration 135/1000 | Loss: 0.00003409
Iteration 136/1000 | Loss: 0.00003409
Iteration 137/1000 | Loss: 0.00003409
Iteration 138/1000 | Loss: 0.00003409
Iteration 139/1000 | Loss: 0.00003408
Iteration 140/1000 | Loss: 0.00003408
Iteration 141/1000 | Loss: 0.00003408
Iteration 142/1000 | Loss: 0.00003408
Iteration 143/1000 | Loss: 0.00003408
Iteration 144/1000 | Loss: 0.00003408
Iteration 145/1000 | Loss: 0.00003408
Iteration 146/1000 | Loss: 0.00003408
Iteration 147/1000 | Loss: 0.00003408
Iteration 148/1000 | Loss: 0.00003408
Iteration 149/1000 | Loss: 0.00003408
Iteration 150/1000 | Loss: 0.00003407
Iteration 151/1000 | Loss: 0.00003407
Iteration 152/1000 | Loss: 0.00003407
Iteration 153/1000 | Loss: 0.00003407
Iteration 154/1000 | Loss: 0.00003407
Iteration 155/1000 | Loss: 0.00003407
Iteration 156/1000 | Loss: 0.00003407
Iteration 157/1000 | Loss: 0.00003406
Iteration 158/1000 | Loss: 0.00003406
Iteration 159/1000 | Loss: 0.00003406
Iteration 160/1000 | Loss: 0.00003406
Iteration 161/1000 | Loss: 0.00003406
Iteration 162/1000 | Loss: 0.00003406
Iteration 163/1000 | Loss: 0.00003406
Iteration 164/1000 | Loss: 0.00003406
Iteration 165/1000 | Loss: 0.00003406
Iteration 166/1000 | Loss: 0.00003405
Iteration 167/1000 | Loss: 0.00003405
Iteration 168/1000 | Loss: 0.00003405
Iteration 169/1000 | Loss: 0.00003405
Iteration 170/1000 | Loss: 0.00003405
Iteration 171/1000 | Loss: 0.00003405
Iteration 172/1000 | Loss: 0.00003405
Iteration 173/1000 | Loss: 0.00003405
Iteration 174/1000 | Loss: 0.00003405
Iteration 175/1000 | Loss: 0.00003405
Iteration 176/1000 | Loss: 0.00003405
Iteration 177/1000 | Loss: 0.00003405
Iteration 178/1000 | Loss: 0.00003405
Iteration 179/1000 | Loss: 0.00003405
Iteration 180/1000 | Loss: 0.00003405
Iteration 181/1000 | Loss: 0.00003405
Iteration 182/1000 | Loss: 0.00003405
Iteration 183/1000 | Loss: 0.00003404
Iteration 184/1000 | Loss: 0.00003404
Iteration 185/1000 | Loss: 0.00003404
Iteration 186/1000 | Loss: 0.00003404
Iteration 187/1000 | Loss: 0.00003404
Iteration 188/1000 | Loss: 0.00003404
Iteration 189/1000 | Loss: 0.00003404
Iteration 190/1000 | Loss: 0.00003404
Iteration 191/1000 | Loss: 0.00003403
Iteration 192/1000 | Loss: 0.00003403
Iteration 193/1000 | Loss: 0.00003403
Iteration 194/1000 | Loss: 0.00003403
Iteration 195/1000 | Loss: 0.00003403
Iteration 196/1000 | Loss: 0.00003402
Iteration 197/1000 | Loss: 0.00003402
Iteration 198/1000 | Loss: 0.00003402
Iteration 199/1000 | Loss: 0.00003402
Iteration 200/1000 | Loss: 0.00003402
Iteration 201/1000 | Loss: 0.00003402
Iteration 202/1000 | Loss: 0.00003402
Iteration 203/1000 | Loss: 0.00003402
Iteration 204/1000 | Loss: 0.00003402
Iteration 205/1000 | Loss: 0.00003401
Iteration 206/1000 | Loss: 0.00003401
Iteration 207/1000 | Loss: 0.00003401
Iteration 208/1000 | Loss: 0.00003401
Iteration 209/1000 | Loss: 0.00003401
Iteration 210/1000 | Loss: 0.00003401
Iteration 211/1000 | Loss: 0.00003401
Iteration 212/1000 | Loss: 0.00003401
Iteration 213/1000 | Loss: 0.00003401
Iteration 214/1000 | Loss: 0.00003401
Iteration 215/1000 | Loss: 0.00003400
Iteration 216/1000 | Loss: 0.00003400
Iteration 217/1000 | Loss: 0.00003400
Iteration 218/1000 | Loss: 0.00003400
Iteration 219/1000 | Loss: 0.00003400
Iteration 220/1000 | Loss: 0.00003400
Iteration 221/1000 | Loss: 0.00003400
Iteration 222/1000 | Loss: 0.00003400
Iteration 223/1000 | Loss: 0.00003400
Iteration 224/1000 | Loss: 0.00003399
Iteration 225/1000 | Loss: 0.00003399
Iteration 226/1000 | Loss: 0.00003399
Iteration 227/1000 | Loss: 0.00003399
Iteration 228/1000 | Loss: 0.00003399
Iteration 229/1000 | Loss: 0.00003399
Iteration 230/1000 | Loss: 0.00003399
Iteration 231/1000 | Loss: 0.00003399
Iteration 232/1000 | Loss: 0.00003399
Iteration 233/1000 | Loss: 0.00003399
Iteration 234/1000 | Loss: 0.00003398
Iteration 235/1000 | Loss: 0.00003398
Iteration 236/1000 | Loss: 0.00003398
Iteration 237/1000 | Loss: 0.00003398
Iteration 238/1000 | Loss: 0.00003398
Iteration 239/1000 | Loss: 0.00003398
Iteration 240/1000 | Loss: 0.00003398
Iteration 241/1000 | Loss: 0.00003398
Iteration 242/1000 | Loss: 0.00003398
Iteration 243/1000 | Loss: 0.00003397
Iteration 244/1000 | Loss: 0.00003397
Iteration 245/1000 | Loss: 0.00003397
Iteration 246/1000 | Loss: 0.00003397
Iteration 247/1000 | Loss: 0.00003397
Iteration 248/1000 | Loss: 0.00003397
Iteration 249/1000 | Loss: 0.00003397
Iteration 250/1000 | Loss: 0.00003397
Iteration 251/1000 | Loss: 0.00003397
Iteration 252/1000 | Loss: 0.00003397
Iteration 253/1000 | Loss: 0.00003397
Iteration 254/1000 | Loss: 0.00003397
Iteration 255/1000 | Loss: 0.00003397
Iteration 256/1000 | Loss: 0.00003397
Iteration 257/1000 | Loss: 0.00003397
Iteration 258/1000 | Loss: 0.00003397
Iteration 259/1000 | Loss: 0.00003397
Iteration 260/1000 | Loss: 0.00003397
Iteration 261/1000 | Loss: 0.00003397
Iteration 262/1000 | Loss: 0.00003397
Iteration 263/1000 | Loss: 0.00003397
Iteration 264/1000 | Loss: 0.00003397
Iteration 265/1000 | Loss: 0.00003397
Iteration 266/1000 | Loss: 0.00003397
Iteration 267/1000 | Loss: 0.00003397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [3.396821557544172e-05, 3.396821557544172e-05, 3.396821557544172e-05, 3.396821557544172e-05, 3.396821557544172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.396821557544172e-05

Optimization complete. Final v2v error: 4.654481410980225 mm

Highest mean error: 7.041695594787598 mm for frame 57

Lowest mean error: 3.3199410438537598 mm for frame 99

Saving results

Total time: 113.32263326644897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526942
Iteration 2/25 | Loss: 0.00112590
Iteration 3/25 | Loss: 0.00097745
Iteration 4/25 | Loss: 0.00096338
Iteration 5/25 | Loss: 0.00095937
Iteration 6/25 | Loss: 0.00095857
Iteration 7/25 | Loss: 0.00095857
Iteration 8/25 | Loss: 0.00095857
Iteration 9/25 | Loss: 0.00095857
Iteration 10/25 | Loss: 0.00095857
Iteration 11/25 | Loss: 0.00095857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009585749357938766, 0.0009585749357938766, 0.0009585749357938766, 0.0009585749357938766, 0.0009585749357938766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009585749357938766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93963557
Iteration 2/25 | Loss: 0.00142040
Iteration 3/25 | Loss: 0.00142039
Iteration 4/25 | Loss: 0.00142039
Iteration 5/25 | Loss: 0.00142039
Iteration 6/25 | Loss: 0.00142039
Iteration 7/25 | Loss: 0.00142039
Iteration 8/25 | Loss: 0.00142039
Iteration 9/25 | Loss: 0.00142039
Iteration 10/25 | Loss: 0.00142039
Iteration 11/25 | Loss: 0.00142039
Iteration 12/25 | Loss: 0.00142039
Iteration 13/25 | Loss: 0.00142039
Iteration 14/25 | Loss: 0.00142039
Iteration 15/25 | Loss: 0.00142039
Iteration 16/25 | Loss: 0.00142039
Iteration 17/25 | Loss: 0.00142039
Iteration 18/25 | Loss: 0.00142039
Iteration 19/25 | Loss: 0.00142039
Iteration 20/25 | Loss: 0.00142039
Iteration 21/25 | Loss: 0.00142039
Iteration 22/25 | Loss: 0.00142039
Iteration 23/25 | Loss: 0.00142039
Iteration 24/25 | Loss: 0.00142039
Iteration 25/25 | Loss: 0.00142039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001420391141436994, 0.001420391141436994, 0.001420391141436994, 0.001420391141436994, 0.001420391141436994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001420391141436994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142039
Iteration 2/1000 | Loss: 0.00003041
Iteration 3/1000 | Loss: 0.00002458
Iteration 4/1000 | Loss: 0.00002265
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002051
Iteration 8/1000 | Loss: 0.00002021
Iteration 9/1000 | Loss: 0.00002015
Iteration 10/1000 | Loss: 0.00001994
Iteration 11/1000 | Loss: 0.00001985
Iteration 12/1000 | Loss: 0.00001972
Iteration 13/1000 | Loss: 0.00001969
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00001962
Iteration 16/1000 | Loss: 0.00001958
Iteration 17/1000 | Loss: 0.00001958
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00001957
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001957
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001954
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001948
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001947
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001946
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001946
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001945
Iteration 39/1000 | Loss: 0.00001945
Iteration 40/1000 | Loss: 0.00001944
Iteration 41/1000 | Loss: 0.00001942
Iteration 42/1000 | Loss: 0.00001942
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00001934
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001933
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001932
Iteration 65/1000 | Loss: 0.00001932
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001932
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001932
Iteration 76/1000 | Loss: 0.00001932
Iteration 77/1000 | Loss: 0.00001932
Iteration 78/1000 | Loss: 0.00001932
Iteration 79/1000 | Loss: 0.00001932
Iteration 80/1000 | Loss: 0.00001932
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.9318114937050268e-05, 1.9318114937050268e-05, 1.9318114937050268e-05, 1.9318114937050268e-05, 1.9318114937050268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9318114937050268e-05

Optimization complete. Final v2v error: 3.782376766204834 mm

Highest mean error: 3.9757118225097656 mm for frame 218

Lowest mean error: 3.6242456436157227 mm for frame 136

Saving results

Total time: 35.696311235427856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427191
Iteration 2/25 | Loss: 0.00101937
Iteration 3/25 | Loss: 0.00087544
Iteration 4/25 | Loss: 0.00086235
Iteration 5/25 | Loss: 0.00085980
Iteration 6/25 | Loss: 0.00085892
Iteration 7/25 | Loss: 0.00085881
Iteration 8/25 | Loss: 0.00085881
Iteration 9/25 | Loss: 0.00085881
Iteration 10/25 | Loss: 0.00085881
Iteration 11/25 | Loss: 0.00085881
Iteration 12/25 | Loss: 0.00085881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008588126511313021, 0.0008588126511313021, 0.0008588126511313021, 0.0008588126511313021, 0.0008588126511313021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008588126511313021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58205116
Iteration 2/25 | Loss: 0.00162449
Iteration 3/25 | Loss: 0.00162448
Iteration 4/25 | Loss: 0.00162448
Iteration 5/25 | Loss: 0.00162448
Iteration 6/25 | Loss: 0.00162448
Iteration 7/25 | Loss: 0.00162448
Iteration 8/25 | Loss: 0.00162448
Iteration 9/25 | Loss: 0.00162448
Iteration 10/25 | Loss: 0.00162448
Iteration 11/25 | Loss: 0.00162448
Iteration 12/25 | Loss: 0.00162448
Iteration 13/25 | Loss: 0.00162448
Iteration 14/25 | Loss: 0.00162448
Iteration 15/25 | Loss: 0.00162448
Iteration 16/25 | Loss: 0.00162448
Iteration 17/25 | Loss: 0.00162448
Iteration 18/25 | Loss: 0.00162448
Iteration 19/25 | Loss: 0.00162448
Iteration 20/25 | Loss: 0.00162448
Iteration 21/25 | Loss: 0.00162448
Iteration 22/25 | Loss: 0.00162448
Iteration 23/25 | Loss: 0.00162448
Iteration 24/25 | Loss: 0.00162448
Iteration 25/25 | Loss: 0.00162448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016244819853454828, 0.0016244819853454828, 0.0016244819853454828, 0.0016244819853454828, 0.0016244819853454828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016244819853454828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162448
Iteration 2/1000 | Loss: 0.00002166
Iteration 3/1000 | Loss: 0.00001641
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001420
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00001331
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001319
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001315
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001315
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001314
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001314
Iteration 38/1000 | Loss: 0.00001314
Iteration 39/1000 | Loss: 0.00001313
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001313
Iteration 43/1000 | Loss: 0.00001313
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001312
Iteration 52/1000 | Loss: 0.00001312
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001311
Iteration 55/1000 | Loss: 0.00001311
Iteration 56/1000 | Loss: 0.00001311
Iteration 57/1000 | Loss: 0.00001310
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001310
Iteration 62/1000 | Loss: 0.00001310
Iteration 63/1000 | Loss: 0.00001310
Iteration 64/1000 | Loss: 0.00001310
Iteration 65/1000 | Loss: 0.00001310
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001309
Iteration 69/1000 | Loss: 0.00001309
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001309
Iteration 74/1000 | Loss: 0.00001309
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001308
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001308
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001307
Iteration 102/1000 | Loss: 0.00001307
Iteration 103/1000 | Loss: 0.00001307
Iteration 104/1000 | Loss: 0.00001307
Iteration 105/1000 | Loss: 0.00001307
Iteration 106/1000 | Loss: 0.00001307
Iteration 107/1000 | Loss: 0.00001307
Iteration 108/1000 | Loss: 0.00001307
Iteration 109/1000 | Loss: 0.00001307
Iteration 110/1000 | Loss: 0.00001307
Iteration 111/1000 | Loss: 0.00001307
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001307
Iteration 117/1000 | Loss: 0.00001307
Iteration 118/1000 | Loss: 0.00001307
Iteration 119/1000 | Loss: 0.00001307
Iteration 120/1000 | Loss: 0.00001307
Iteration 121/1000 | Loss: 0.00001307
Iteration 122/1000 | Loss: 0.00001307
Iteration 123/1000 | Loss: 0.00001307
Iteration 124/1000 | Loss: 0.00001307
Iteration 125/1000 | Loss: 0.00001307
Iteration 126/1000 | Loss: 0.00001307
Iteration 127/1000 | Loss: 0.00001307
Iteration 128/1000 | Loss: 0.00001307
Iteration 129/1000 | Loss: 0.00001307
Iteration 130/1000 | Loss: 0.00001307
Iteration 131/1000 | Loss: 0.00001307
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001307
Iteration 138/1000 | Loss: 0.00001307
Iteration 139/1000 | Loss: 0.00001307
Iteration 140/1000 | Loss: 0.00001307
Iteration 141/1000 | Loss: 0.00001307
Iteration 142/1000 | Loss: 0.00001307
Iteration 143/1000 | Loss: 0.00001307
Iteration 144/1000 | Loss: 0.00001307
Iteration 145/1000 | Loss: 0.00001307
Iteration 146/1000 | Loss: 0.00001307
Iteration 147/1000 | Loss: 0.00001307
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001307
Iteration 150/1000 | Loss: 0.00001307
Iteration 151/1000 | Loss: 0.00001307
Iteration 152/1000 | Loss: 0.00001307
Iteration 153/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.3065331586403772e-05, 1.3065331586403772e-05, 1.3065331586403772e-05, 1.3065331586403772e-05, 1.3065331586403772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3065331586403772e-05

Optimization complete. Final v2v error: 3.1146240234375 mm

Highest mean error: 3.577329397201538 mm for frame 54

Lowest mean error: 2.8556835651397705 mm for frame 42

Saving results

Total time: 30.791709899902344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121299
Iteration 2/25 | Loss: 0.01121299
Iteration 3/25 | Loss: 0.01121299
Iteration 4/25 | Loss: 0.01121299
Iteration 5/25 | Loss: 0.01121298
Iteration 6/25 | Loss: 0.01121298
Iteration 7/25 | Loss: 0.01121298
Iteration 8/25 | Loss: 0.01121298
Iteration 9/25 | Loss: 0.01121298
Iteration 10/25 | Loss: 0.01121297
Iteration 11/25 | Loss: 0.01121297
Iteration 12/25 | Loss: 0.01121296
Iteration 13/25 | Loss: 0.01121296
Iteration 14/25 | Loss: 0.01121296
Iteration 15/25 | Loss: 0.01121296
Iteration 16/25 | Loss: 0.01121296
Iteration 17/25 | Loss: 0.01121296
Iteration 18/25 | Loss: 0.01121295
Iteration 19/25 | Loss: 0.01121295
Iteration 20/25 | Loss: 0.01121295
Iteration 21/25 | Loss: 0.01121295
Iteration 22/25 | Loss: 0.01121294
Iteration 23/25 | Loss: 0.01121294
Iteration 24/25 | Loss: 0.01121294
Iteration 25/25 | Loss: 0.01121294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98871613
Iteration 2/25 | Loss: 0.10444356
Iteration 3/25 | Loss: 0.10442335
Iteration 4/25 | Loss: 0.10442334
Iteration 5/25 | Loss: 0.10442332
Iteration 6/25 | Loss: 0.10442332
Iteration 7/25 | Loss: 0.10442332
Iteration 8/25 | Loss: 0.10442330
Iteration 9/25 | Loss: 0.10442330
Iteration 10/25 | Loss: 0.10442330
Iteration 11/25 | Loss: 0.10442330
Iteration 12/25 | Loss: 0.10442330
Iteration 13/25 | Loss: 0.10442330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.10442329943180084, 0.10442329943180084, 0.10442329943180084, 0.10442329943180084, 0.10442329943180084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10442329943180084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10442330
Iteration 2/1000 | Loss: 0.00980868
Iteration 3/1000 | Loss: 0.00456119
Iteration 4/1000 | Loss: 0.00196253
Iteration 5/1000 | Loss: 0.00068399
Iteration 6/1000 | Loss: 0.00031612
Iteration 7/1000 | Loss: 0.00031968
Iteration 8/1000 | Loss: 0.00045307
Iteration 9/1000 | Loss: 0.00038246
Iteration 10/1000 | Loss: 0.00029240
Iteration 11/1000 | Loss: 0.00016017
Iteration 12/1000 | Loss: 0.00020699
Iteration 13/1000 | Loss: 0.00028125
Iteration 14/1000 | Loss: 0.00013909
Iteration 15/1000 | Loss: 0.00022872
Iteration 16/1000 | Loss: 0.00035436
Iteration 17/1000 | Loss: 0.00021610
Iteration 18/1000 | Loss: 0.00007874
Iteration 19/1000 | Loss: 0.00020493
Iteration 20/1000 | Loss: 0.00086052
Iteration 21/1000 | Loss: 0.00020425
Iteration 22/1000 | Loss: 0.00022430
Iteration 23/1000 | Loss: 0.00049021
Iteration 24/1000 | Loss: 0.00023315
Iteration 25/1000 | Loss: 0.00064109
Iteration 26/1000 | Loss: 0.00034966
Iteration 27/1000 | Loss: 0.00004782
Iteration 28/1000 | Loss: 0.00021272
Iteration 29/1000 | Loss: 0.00015884
Iteration 30/1000 | Loss: 0.00007776
Iteration 31/1000 | Loss: 0.00034281
Iteration 32/1000 | Loss: 0.00040061
Iteration 33/1000 | Loss: 0.00019116
Iteration 34/1000 | Loss: 0.00012913
Iteration 35/1000 | Loss: 0.00004476
Iteration 36/1000 | Loss: 0.00031277
Iteration 37/1000 | Loss: 0.00005325
Iteration 38/1000 | Loss: 0.00012497
Iteration 39/1000 | Loss: 0.00024711
Iteration 40/1000 | Loss: 0.00011687
Iteration 41/1000 | Loss: 0.00025862
Iteration 42/1000 | Loss: 0.00012412
Iteration 43/1000 | Loss: 0.00038905
Iteration 44/1000 | Loss: 0.00045646
Iteration 45/1000 | Loss: 0.00006136
Iteration 46/1000 | Loss: 0.00008272
Iteration 47/1000 | Loss: 0.00004036
Iteration 48/1000 | Loss: 0.00004251
Iteration 49/1000 | Loss: 0.00022080
Iteration 50/1000 | Loss: 0.00021516
Iteration 51/1000 | Loss: 0.00021783
Iteration 52/1000 | Loss: 0.00068205
Iteration 53/1000 | Loss: 0.00232653
Iteration 54/1000 | Loss: 0.00090977
Iteration 55/1000 | Loss: 0.00018715
Iteration 56/1000 | Loss: 0.00009145
Iteration 57/1000 | Loss: 0.00040481
Iteration 58/1000 | Loss: 0.00004286
Iteration 59/1000 | Loss: 0.00008928
Iteration 60/1000 | Loss: 0.00003796
Iteration 61/1000 | Loss: 0.00003626
Iteration 62/1000 | Loss: 0.00003528
Iteration 63/1000 | Loss: 0.00003440
Iteration 64/1000 | Loss: 0.00027118
Iteration 65/1000 | Loss: 0.00004947
Iteration 66/1000 | Loss: 0.00007823
Iteration 67/1000 | Loss: 0.00014759
Iteration 68/1000 | Loss: 0.00003334
Iteration 69/1000 | Loss: 0.00004088
Iteration 70/1000 | Loss: 0.00003282
Iteration 71/1000 | Loss: 0.00003370
Iteration 72/1000 | Loss: 0.00003196
Iteration 73/1000 | Loss: 0.00003240
Iteration 74/1000 | Loss: 0.00036908
Iteration 75/1000 | Loss: 0.00003811
Iteration 76/1000 | Loss: 0.00003204
Iteration 77/1000 | Loss: 0.00026915
Iteration 78/1000 | Loss: 0.00003097
Iteration 79/1000 | Loss: 0.00020410
Iteration 80/1000 | Loss: 0.00025960
Iteration 81/1000 | Loss: 0.00029378
Iteration 82/1000 | Loss: 0.00025551
Iteration 83/1000 | Loss: 0.00004186
Iteration 84/1000 | Loss: 0.00003594
Iteration 85/1000 | Loss: 0.00003321
Iteration 86/1000 | Loss: 0.00009258
Iteration 87/1000 | Loss: 0.00003046
Iteration 88/1000 | Loss: 0.00004438
Iteration 89/1000 | Loss: 0.00002984
Iteration 90/1000 | Loss: 0.00007261
Iteration 91/1000 | Loss: 0.00003170
Iteration 92/1000 | Loss: 0.00005176
Iteration 93/1000 | Loss: 0.00003638
Iteration 94/1000 | Loss: 0.00002913
Iteration 95/1000 | Loss: 0.00002911
Iteration 96/1000 | Loss: 0.00003487
Iteration 97/1000 | Loss: 0.00002891
Iteration 98/1000 | Loss: 0.00003403
Iteration 99/1000 | Loss: 0.00041029
Iteration 100/1000 | Loss: 0.00005407
Iteration 101/1000 | Loss: 0.00004425
Iteration 102/1000 | Loss: 0.00002875
Iteration 103/1000 | Loss: 0.00008371
Iteration 104/1000 | Loss: 0.00003391
Iteration 105/1000 | Loss: 0.00013172
Iteration 106/1000 | Loss: 0.00003000
Iteration 107/1000 | Loss: 0.00002871
Iteration 108/1000 | Loss: 0.00002850
Iteration 109/1000 | Loss: 0.00002848
Iteration 110/1000 | Loss: 0.00002847
Iteration 111/1000 | Loss: 0.00002847
Iteration 112/1000 | Loss: 0.00008909
Iteration 113/1000 | Loss: 0.00016265
Iteration 114/1000 | Loss: 0.00008570
Iteration 115/1000 | Loss: 0.00002873
Iteration 116/1000 | Loss: 0.00002848
Iteration 117/1000 | Loss: 0.00002847
Iteration 118/1000 | Loss: 0.00002847
Iteration 119/1000 | Loss: 0.00002847
Iteration 120/1000 | Loss: 0.00002843
Iteration 121/1000 | Loss: 0.00002843
Iteration 122/1000 | Loss: 0.00002842
Iteration 123/1000 | Loss: 0.00002841
Iteration 124/1000 | Loss: 0.00002841
Iteration 125/1000 | Loss: 0.00002841
Iteration 126/1000 | Loss: 0.00002841
Iteration 127/1000 | Loss: 0.00002841
Iteration 128/1000 | Loss: 0.00002840
Iteration 129/1000 | Loss: 0.00002840
Iteration 130/1000 | Loss: 0.00002840
Iteration 131/1000 | Loss: 0.00002840
Iteration 132/1000 | Loss: 0.00002839
Iteration 133/1000 | Loss: 0.00002839
Iteration 134/1000 | Loss: 0.00002839
Iteration 135/1000 | Loss: 0.00002839
Iteration 136/1000 | Loss: 0.00002838
Iteration 137/1000 | Loss: 0.00002838
Iteration 138/1000 | Loss: 0.00002838
Iteration 139/1000 | Loss: 0.00002837
Iteration 140/1000 | Loss: 0.00002837
Iteration 141/1000 | Loss: 0.00002837
Iteration 142/1000 | Loss: 0.00002837
Iteration 143/1000 | Loss: 0.00002836
Iteration 144/1000 | Loss: 0.00002836
Iteration 145/1000 | Loss: 0.00002836
Iteration 146/1000 | Loss: 0.00002835
Iteration 147/1000 | Loss: 0.00002835
Iteration 148/1000 | Loss: 0.00002835
Iteration 149/1000 | Loss: 0.00002834
Iteration 150/1000 | Loss: 0.00002834
Iteration 151/1000 | Loss: 0.00002834
Iteration 152/1000 | Loss: 0.00002834
Iteration 153/1000 | Loss: 0.00002833
Iteration 154/1000 | Loss: 0.00002833
Iteration 155/1000 | Loss: 0.00002833
Iteration 156/1000 | Loss: 0.00002833
Iteration 157/1000 | Loss: 0.00002833
Iteration 158/1000 | Loss: 0.00002833
Iteration 159/1000 | Loss: 0.00002833
Iteration 160/1000 | Loss: 0.00002833
Iteration 161/1000 | Loss: 0.00002832
Iteration 162/1000 | Loss: 0.00002832
Iteration 163/1000 | Loss: 0.00002832
Iteration 164/1000 | Loss: 0.00002832
Iteration 165/1000 | Loss: 0.00002832
Iteration 166/1000 | Loss: 0.00002832
Iteration 167/1000 | Loss: 0.00002832
Iteration 168/1000 | Loss: 0.00002832
Iteration 169/1000 | Loss: 0.00002832
Iteration 170/1000 | Loss: 0.00002832
Iteration 171/1000 | Loss: 0.00002832
Iteration 172/1000 | Loss: 0.00002832
Iteration 173/1000 | Loss: 0.00002832
Iteration 174/1000 | Loss: 0.00002832
Iteration 175/1000 | Loss: 0.00002832
Iteration 176/1000 | Loss: 0.00002832
Iteration 177/1000 | Loss: 0.00002832
Iteration 178/1000 | Loss: 0.00002832
Iteration 179/1000 | Loss: 0.00002832
Iteration 180/1000 | Loss: 0.00002832
Iteration 181/1000 | Loss: 0.00002832
Iteration 182/1000 | Loss: 0.00002832
Iteration 183/1000 | Loss: 0.00002832
Iteration 184/1000 | Loss: 0.00002832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.8318869226495735e-05, 2.8318869226495735e-05, 2.8318869226495735e-05, 2.8318869226495735e-05, 2.8318869226495735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8318869226495735e-05

Optimization complete. Final v2v error: 4.120800971984863 mm

Highest mean error: 17.344676971435547 mm for frame 42

Lowest mean error: 3.6093997955322266 mm for frame 91

Saving results

Total time: 190.61148738861084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857808
Iteration 2/25 | Loss: 0.00121927
Iteration 3/25 | Loss: 0.00100684
Iteration 4/25 | Loss: 0.00098472
Iteration 5/25 | Loss: 0.00097819
Iteration 6/25 | Loss: 0.00097672
Iteration 7/25 | Loss: 0.00097661
Iteration 8/25 | Loss: 0.00097661
Iteration 9/25 | Loss: 0.00097661
Iteration 10/25 | Loss: 0.00097661
Iteration 11/25 | Loss: 0.00097661
Iteration 12/25 | Loss: 0.00097661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009766106959432364, 0.0009766106959432364, 0.0009766106959432364, 0.0009766106959432364, 0.0009766106959432364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009766106959432364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11062074
Iteration 2/25 | Loss: 0.00161531
Iteration 3/25 | Loss: 0.00161528
Iteration 4/25 | Loss: 0.00161528
Iteration 5/25 | Loss: 0.00161528
Iteration 6/25 | Loss: 0.00161528
Iteration 7/25 | Loss: 0.00161528
Iteration 8/25 | Loss: 0.00161528
Iteration 9/25 | Loss: 0.00161528
Iteration 10/25 | Loss: 0.00161528
Iteration 11/25 | Loss: 0.00161528
Iteration 12/25 | Loss: 0.00161528
Iteration 13/25 | Loss: 0.00161528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0016152792377397418, 0.0016152792377397418, 0.0016152792377397418, 0.0016152792377397418, 0.0016152792377397418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016152792377397418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161528
Iteration 2/1000 | Loss: 0.00003520
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00002108
Iteration 7/1000 | Loss: 0.00002068
Iteration 8/1000 | Loss: 0.00002039
Iteration 9/1000 | Loss: 0.00002020
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00002004
Iteration 12/1000 | Loss: 0.00002004
Iteration 13/1000 | Loss: 0.00002004
Iteration 14/1000 | Loss: 0.00002004
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00002002
Iteration 17/1000 | Loss: 0.00002002
Iteration 18/1000 | Loss: 0.00002002
Iteration 19/1000 | Loss: 0.00002001
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00002000
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00001999
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001998
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00001998
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001997
Iteration 33/1000 | Loss: 0.00001997
Iteration 34/1000 | Loss: 0.00001997
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001996
Iteration 37/1000 | Loss: 0.00001996
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001996
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001996
Iteration 42/1000 | Loss: 0.00001996
Iteration 43/1000 | Loss: 0.00001996
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [1.9958270058850758e-05, 1.9958270058850758e-05, 1.9958270058850758e-05, 1.9958270058850758e-05, 1.9958270058850758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9958270058850758e-05

Optimization complete. Final v2v error: 3.790219306945801 mm

Highest mean error: 4.306198596954346 mm for frame 180

Lowest mean error: 3.394573211669922 mm for frame 64

Saving results

Total time: 28.94671654701233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857920
Iteration 2/25 | Loss: 0.00133749
Iteration 3/25 | Loss: 0.00098888
Iteration 4/25 | Loss: 0.00093423
Iteration 5/25 | Loss: 0.00091309
Iteration 6/25 | Loss: 0.00090952
Iteration 7/25 | Loss: 0.00090802
Iteration 8/25 | Loss: 0.00090930
Iteration 9/25 | Loss: 0.00090699
Iteration 10/25 | Loss: 0.00090555
Iteration 11/25 | Loss: 0.00090475
Iteration 12/25 | Loss: 0.00090450
Iteration 13/25 | Loss: 0.00090437
Iteration 14/25 | Loss: 0.00090427
Iteration 15/25 | Loss: 0.00090835
Iteration 16/25 | Loss: 0.00090994
Iteration 17/25 | Loss: 0.00091005
Iteration 18/25 | Loss: 0.00090843
Iteration 19/25 | Loss: 0.00090969
Iteration 20/25 | Loss: 0.00090821
Iteration 21/25 | Loss: 0.00090803
Iteration 22/25 | Loss: 0.00090906
Iteration 23/25 | Loss: 0.00090749
Iteration 24/25 | Loss: 0.00090833
Iteration 25/25 | Loss: 0.00090792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38411784
Iteration 2/25 | Loss: 0.00181887
Iteration 3/25 | Loss: 0.00175632
Iteration 4/25 | Loss: 0.00175632
Iteration 5/25 | Loss: 0.00175632
Iteration 6/25 | Loss: 0.00175632
Iteration 7/25 | Loss: 0.00175632
Iteration 8/25 | Loss: 0.00175632
Iteration 9/25 | Loss: 0.00175632
Iteration 10/25 | Loss: 0.00175632
Iteration 11/25 | Loss: 0.00175632
Iteration 12/25 | Loss: 0.00175632
Iteration 13/25 | Loss: 0.00175632
Iteration 14/25 | Loss: 0.00175632
Iteration 15/25 | Loss: 0.00175632
Iteration 16/25 | Loss: 0.00175632
Iteration 17/25 | Loss: 0.00175632
Iteration 18/25 | Loss: 0.00175632
Iteration 19/25 | Loss: 0.00175632
Iteration 20/25 | Loss: 0.00175632
Iteration 21/25 | Loss: 0.00175632
Iteration 22/25 | Loss: 0.00175632
Iteration 23/25 | Loss: 0.00175632
Iteration 24/25 | Loss: 0.00175632
Iteration 25/25 | Loss: 0.00175632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175632
Iteration 2/1000 | Loss: 0.00018264
Iteration 3/1000 | Loss: 0.00003871
Iteration 4/1000 | Loss: 0.00012908
Iteration 5/1000 | Loss: 0.00004082
Iteration 6/1000 | Loss: 0.00011730
Iteration 7/1000 | Loss: 0.00002782
Iteration 8/1000 | Loss: 0.00002563
Iteration 9/1000 | Loss: 0.00002429
Iteration 10/1000 | Loss: 0.00002343
Iteration 11/1000 | Loss: 0.00002306
Iteration 12/1000 | Loss: 0.00002256
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00002224
Iteration 15/1000 | Loss: 0.00002221
Iteration 16/1000 | Loss: 0.00002219
Iteration 17/1000 | Loss: 0.00002213
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00002211
Iteration 22/1000 | Loss: 0.00002210
Iteration 23/1000 | Loss: 0.00002210
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002206
Iteration 26/1000 | Loss: 0.00002206
Iteration 27/1000 | Loss: 0.00002205
Iteration 28/1000 | Loss: 0.00002205
Iteration 29/1000 | Loss: 0.00002204
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002203
Iteration 32/1000 | Loss: 0.00002203
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002201
Iteration 35/1000 | Loss: 0.00002200
Iteration 36/1000 | Loss: 0.00002200
Iteration 37/1000 | Loss: 0.00002200
Iteration 38/1000 | Loss: 0.00002199
Iteration 39/1000 | Loss: 0.00002199
Iteration 40/1000 | Loss: 0.00002199
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002198
Iteration 44/1000 | Loss: 0.00002198
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002196
Iteration 48/1000 | Loss: 0.00002196
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002195
Iteration 52/1000 | Loss: 0.00002195
Iteration 53/1000 | Loss: 0.00002195
Iteration 54/1000 | Loss: 0.00002194
Iteration 55/1000 | Loss: 0.00002194
Iteration 56/1000 | Loss: 0.00002194
Iteration 57/1000 | Loss: 0.00002193
Iteration 58/1000 | Loss: 0.00002193
Iteration 59/1000 | Loss: 0.00002193
Iteration 60/1000 | Loss: 0.00002193
Iteration 61/1000 | Loss: 0.00002193
Iteration 62/1000 | Loss: 0.00002193
Iteration 63/1000 | Loss: 0.00002193
Iteration 64/1000 | Loss: 0.00002193
Iteration 65/1000 | Loss: 0.00002193
Iteration 66/1000 | Loss: 0.00002193
Iteration 67/1000 | Loss: 0.00002193
Iteration 68/1000 | Loss: 0.00002193
Iteration 69/1000 | Loss: 0.00002192
Iteration 70/1000 | Loss: 0.00002192
Iteration 71/1000 | Loss: 0.00002192
Iteration 72/1000 | Loss: 0.00002192
Iteration 73/1000 | Loss: 0.00002191
Iteration 74/1000 | Loss: 0.00002191
Iteration 75/1000 | Loss: 0.00002191
Iteration 76/1000 | Loss: 0.00002191
Iteration 77/1000 | Loss: 0.00002191
Iteration 78/1000 | Loss: 0.00002191
Iteration 79/1000 | Loss: 0.00002191
Iteration 80/1000 | Loss: 0.00002190
Iteration 81/1000 | Loss: 0.00002190
Iteration 82/1000 | Loss: 0.00002190
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002189
Iteration 86/1000 | Loss: 0.00002189
Iteration 87/1000 | Loss: 0.00002189
Iteration 88/1000 | Loss: 0.00002189
Iteration 89/1000 | Loss: 0.00002189
Iteration 90/1000 | Loss: 0.00002189
Iteration 91/1000 | Loss: 0.00002189
Iteration 92/1000 | Loss: 0.00002188
Iteration 93/1000 | Loss: 0.00002188
Iteration 94/1000 | Loss: 0.00002188
Iteration 95/1000 | Loss: 0.00002188
Iteration 96/1000 | Loss: 0.00002188
Iteration 97/1000 | Loss: 0.00002188
Iteration 98/1000 | Loss: 0.00002188
Iteration 99/1000 | Loss: 0.00002188
Iteration 100/1000 | Loss: 0.00002188
Iteration 101/1000 | Loss: 0.00002188
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002187
Iteration 105/1000 | Loss: 0.00002187
Iteration 106/1000 | Loss: 0.00002187
Iteration 107/1000 | Loss: 0.00002187
Iteration 108/1000 | Loss: 0.00002187
Iteration 109/1000 | Loss: 0.00002187
Iteration 110/1000 | Loss: 0.00002187
Iteration 111/1000 | Loss: 0.00002187
Iteration 112/1000 | Loss: 0.00002187
Iteration 113/1000 | Loss: 0.00002186
Iteration 114/1000 | Loss: 0.00002186
Iteration 115/1000 | Loss: 0.00002186
Iteration 116/1000 | Loss: 0.00002186
Iteration 117/1000 | Loss: 0.00002186
Iteration 118/1000 | Loss: 0.00002186
Iteration 119/1000 | Loss: 0.00002186
Iteration 120/1000 | Loss: 0.00002186
Iteration 121/1000 | Loss: 0.00002186
Iteration 122/1000 | Loss: 0.00002186
Iteration 123/1000 | Loss: 0.00002186
Iteration 124/1000 | Loss: 0.00002186
Iteration 125/1000 | Loss: 0.00002186
Iteration 126/1000 | Loss: 0.00002186
Iteration 127/1000 | Loss: 0.00002186
Iteration 128/1000 | Loss: 0.00002186
Iteration 129/1000 | Loss: 0.00002186
Iteration 130/1000 | Loss: 0.00002186
Iteration 131/1000 | Loss: 0.00002185
Iteration 132/1000 | Loss: 0.00002185
Iteration 133/1000 | Loss: 0.00002185
Iteration 134/1000 | Loss: 0.00002185
Iteration 135/1000 | Loss: 0.00002185
Iteration 136/1000 | Loss: 0.00002185
Iteration 137/1000 | Loss: 0.00002185
Iteration 138/1000 | Loss: 0.00002185
Iteration 139/1000 | Loss: 0.00002185
Iteration 140/1000 | Loss: 0.00002185
Iteration 141/1000 | Loss: 0.00002185
Iteration 142/1000 | Loss: 0.00002185
Iteration 143/1000 | Loss: 0.00002185
Iteration 144/1000 | Loss: 0.00002184
Iteration 145/1000 | Loss: 0.00002184
Iteration 146/1000 | Loss: 0.00002184
Iteration 147/1000 | Loss: 0.00002184
Iteration 148/1000 | Loss: 0.00002184
Iteration 149/1000 | Loss: 0.00002184
Iteration 150/1000 | Loss: 0.00002184
Iteration 151/1000 | Loss: 0.00002184
Iteration 152/1000 | Loss: 0.00002184
Iteration 153/1000 | Loss: 0.00002183
Iteration 154/1000 | Loss: 0.00002183
Iteration 155/1000 | Loss: 0.00002183
Iteration 156/1000 | Loss: 0.00002183
Iteration 157/1000 | Loss: 0.00002183
Iteration 158/1000 | Loss: 0.00002183
Iteration 159/1000 | Loss: 0.00002183
Iteration 160/1000 | Loss: 0.00002183
Iteration 161/1000 | Loss: 0.00002183
Iteration 162/1000 | Loss: 0.00002183
Iteration 163/1000 | Loss: 0.00002183
Iteration 164/1000 | Loss: 0.00002183
Iteration 165/1000 | Loss: 0.00002183
Iteration 166/1000 | Loss: 0.00002183
Iteration 167/1000 | Loss: 0.00002183
Iteration 168/1000 | Loss: 0.00002183
Iteration 169/1000 | Loss: 0.00002183
Iteration 170/1000 | Loss: 0.00002183
Iteration 171/1000 | Loss: 0.00002183
Iteration 172/1000 | Loss: 0.00002183
Iteration 173/1000 | Loss: 0.00002183
Iteration 174/1000 | Loss: 0.00002183
Iteration 175/1000 | Loss: 0.00002183
Iteration 176/1000 | Loss: 0.00002183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.182650860049762e-05, 2.182650860049762e-05, 2.182650860049762e-05, 2.182650860049762e-05, 2.182650860049762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.182650860049762e-05

Optimization complete. Final v2v error: 3.9017746448516846 mm

Highest mean error: 9.685383796691895 mm for frame 43

Lowest mean error: 3.366576671600342 mm for frame 209

Saving results

Total time: 88.59900760650635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859033
Iteration 2/25 | Loss: 0.00122482
Iteration 3/25 | Loss: 0.00099814
Iteration 4/25 | Loss: 0.00097033
Iteration 5/25 | Loss: 0.00097347
Iteration 6/25 | Loss: 0.00096053
Iteration 7/25 | Loss: 0.00095960
Iteration 8/25 | Loss: 0.00096239
Iteration 9/25 | Loss: 0.00096212
Iteration 10/25 | Loss: 0.00095796
Iteration 11/25 | Loss: 0.00095783
Iteration 12/25 | Loss: 0.00095623
Iteration 13/25 | Loss: 0.00096278
Iteration 14/25 | Loss: 0.00096060
Iteration 15/25 | Loss: 0.00095572
Iteration 16/25 | Loss: 0.00095617
Iteration 17/25 | Loss: 0.00096096
Iteration 18/25 | Loss: 0.00095773
Iteration 19/25 | Loss: 0.00095927
Iteration 20/25 | Loss: 0.00095951
Iteration 21/25 | Loss: 0.00095588
Iteration 22/25 | Loss: 0.00095606
Iteration 23/25 | Loss: 0.00095925
Iteration 24/25 | Loss: 0.00095524
Iteration 25/25 | Loss: 0.00095796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.71272850
Iteration 2/25 | Loss: 0.00183317
Iteration 3/25 | Loss: 0.00183310
Iteration 4/25 | Loss: 0.00183310
Iteration 5/25 | Loss: 0.00183310
Iteration 6/25 | Loss: 0.00183310
Iteration 7/25 | Loss: 0.00183310
Iteration 8/25 | Loss: 0.00183310
Iteration 9/25 | Loss: 0.00183310
Iteration 10/25 | Loss: 0.00183310
Iteration 11/25 | Loss: 0.00183310
Iteration 12/25 | Loss: 0.00183310
Iteration 13/25 | Loss: 0.00183310
Iteration 14/25 | Loss: 0.00183310
Iteration 15/25 | Loss: 0.00183310
Iteration 16/25 | Loss: 0.00183310
Iteration 17/25 | Loss: 0.00183310
Iteration 18/25 | Loss: 0.00183310
Iteration 19/25 | Loss: 0.00183310
Iteration 20/25 | Loss: 0.00183310
Iteration 21/25 | Loss: 0.00183310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00183309696149081, 0.00183309696149081, 0.00183309696149081, 0.00183309696149081, 0.00183309696149081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00183309696149081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183310
Iteration 2/1000 | Loss: 0.00009138
Iteration 3/1000 | Loss: 0.00006269
Iteration 4/1000 | Loss: 0.00006824
Iteration 5/1000 | Loss: 0.00007790
Iteration 6/1000 | Loss: 0.00048254
Iteration 7/1000 | Loss: 0.00027500
Iteration 8/1000 | Loss: 0.00019039
Iteration 9/1000 | Loss: 0.00007762
Iteration 10/1000 | Loss: 0.00007424
Iteration 11/1000 | Loss: 0.00009316
Iteration 12/1000 | Loss: 0.00008403
Iteration 13/1000 | Loss: 0.00007582
Iteration 14/1000 | Loss: 0.00006873
Iteration 15/1000 | Loss: 0.00005910
Iteration 16/1000 | Loss: 0.00044615
Iteration 17/1000 | Loss: 0.00005272
Iteration 18/1000 | Loss: 0.00035099
Iteration 19/1000 | Loss: 0.00005927
Iteration 20/1000 | Loss: 0.00005665
Iteration 21/1000 | Loss: 0.00011464
Iteration 22/1000 | Loss: 0.00005607
Iteration 23/1000 | Loss: 0.00006007
Iteration 24/1000 | Loss: 0.00006413
Iteration 25/1000 | Loss: 0.00005733
Iteration 26/1000 | Loss: 0.00010089
Iteration 27/1000 | Loss: 0.00050148
Iteration 28/1000 | Loss: 0.00006775
Iteration 29/1000 | Loss: 0.00006168
Iteration 30/1000 | Loss: 0.00007293
Iteration 31/1000 | Loss: 0.00008521
Iteration 32/1000 | Loss: 0.00011859
Iteration 33/1000 | Loss: 0.00003223
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002212
Iteration 36/1000 | Loss: 0.00002097
Iteration 37/1000 | Loss: 0.00002022
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00001906
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001832
Iteration 42/1000 | Loss: 0.00001808
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001794
Iteration 48/1000 | Loss: 0.00001791
Iteration 49/1000 | Loss: 0.00001786
Iteration 50/1000 | Loss: 0.00001786
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001784
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001782
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001775
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001775
Iteration 98/1000 | Loss: 0.00001775
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001775
Iteration 101/1000 | Loss: 0.00001775
Iteration 102/1000 | Loss: 0.00001775
Iteration 103/1000 | Loss: 0.00001775
Iteration 104/1000 | Loss: 0.00001774
Iteration 105/1000 | Loss: 0.00001774
Iteration 106/1000 | Loss: 0.00001774
Iteration 107/1000 | Loss: 0.00001774
Iteration 108/1000 | Loss: 0.00001774
Iteration 109/1000 | Loss: 0.00001774
Iteration 110/1000 | Loss: 0.00001774
Iteration 111/1000 | Loss: 0.00001774
Iteration 112/1000 | Loss: 0.00001774
Iteration 113/1000 | Loss: 0.00001774
Iteration 114/1000 | Loss: 0.00001774
Iteration 115/1000 | Loss: 0.00001774
Iteration 116/1000 | Loss: 0.00001774
Iteration 117/1000 | Loss: 0.00001774
Iteration 118/1000 | Loss: 0.00001774
Iteration 119/1000 | Loss: 0.00001774
Iteration 120/1000 | Loss: 0.00001774
Iteration 121/1000 | Loss: 0.00001774
Iteration 122/1000 | Loss: 0.00001774
Iteration 123/1000 | Loss: 0.00001774
Iteration 124/1000 | Loss: 0.00001773
Iteration 125/1000 | Loss: 0.00001773
Iteration 126/1000 | Loss: 0.00001773
Iteration 127/1000 | Loss: 0.00001773
Iteration 128/1000 | Loss: 0.00001773
Iteration 129/1000 | Loss: 0.00001773
Iteration 130/1000 | Loss: 0.00001773
Iteration 131/1000 | Loss: 0.00001773
Iteration 132/1000 | Loss: 0.00001773
Iteration 133/1000 | Loss: 0.00001773
Iteration 134/1000 | Loss: 0.00001773
Iteration 135/1000 | Loss: 0.00001773
Iteration 136/1000 | Loss: 0.00001773
Iteration 137/1000 | Loss: 0.00001773
Iteration 138/1000 | Loss: 0.00001773
Iteration 139/1000 | Loss: 0.00001773
Iteration 140/1000 | Loss: 0.00001773
Iteration 141/1000 | Loss: 0.00001773
Iteration 142/1000 | Loss: 0.00001773
Iteration 143/1000 | Loss: 0.00001773
Iteration 144/1000 | Loss: 0.00001773
Iteration 145/1000 | Loss: 0.00001773
Iteration 146/1000 | Loss: 0.00001773
Iteration 147/1000 | Loss: 0.00001773
Iteration 148/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.773083204170689e-05, 1.773083204170689e-05, 1.773083204170689e-05, 1.773083204170689e-05, 1.773083204170689e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.773083204170689e-05

Optimization complete. Final v2v error: 3.61995267868042 mm

Highest mean error: 4.303994655609131 mm for frame 69

Lowest mean error: 3.176764488220215 mm for frame 212

Saving results

Total time: 128.6275086402893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457774
Iteration 2/25 | Loss: 0.00121798
Iteration 3/25 | Loss: 0.00094564
Iteration 4/25 | Loss: 0.00092449
Iteration 5/25 | Loss: 0.00091992
Iteration 6/25 | Loss: 0.00091871
Iteration 7/25 | Loss: 0.00091855
Iteration 8/25 | Loss: 0.00091855
Iteration 9/25 | Loss: 0.00091855
Iteration 10/25 | Loss: 0.00091855
Iteration 11/25 | Loss: 0.00091855
Iteration 12/25 | Loss: 0.00091855
Iteration 13/25 | Loss: 0.00091855
Iteration 14/25 | Loss: 0.00091855
Iteration 15/25 | Loss: 0.00091855
Iteration 16/25 | Loss: 0.00091855
Iteration 17/25 | Loss: 0.00091855
Iteration 18/25 | Loss: 0.00091855
Iteration 19/25 | Loss: 0.00091855
Iteration 20/25 | Loss: 0.00091855
Iteration 21/25 | Loss: 0.00091855
Iteration 22/25 | Loss: 0.00091855
Iteration 23/25 | Loss: 0.00091855
Iteration 24/25 | Loss: 0.00091855
Iteration 25/25 | Loss: 0.00091855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.49108934
Iteration 2/25 | Loss: 0.00153031
Iteration 3/25 | Loss: 0.00153031
Iteration 4/25 | Loss: 0.00153031
Iteration 5/25 | Loss: 0.00153031
Iteration 6/25 | Loss: 0.00153031
Iteration 7/25 | Loss: 0.00153031
Iteration 8/25 | Loss: 0.00153031
Iteration 9/25 | Loss: 0.00153031
Iteration 10/25 | Loss: 0.00153031
Iteration 11/25 | Loss: 0.00153031
Iteration 12/25 | Loss: 0.00153031
Iteration 13/25 | Loss: 0.00153031
Iteration 14/25 | Loss: 0.00153031
Iteration 15/25 | Loss: 0.00153031
Iteration 16/25 | Loss: 0.00153031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001530305715277791, 0.001530305715277791, 0.001530305715277791, 0.001530305715277791, 0.001530305715277791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001530305715277791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153031
Iteration 2/1000 | Loss: 0.00003692
Iteration 3/1000 | Loss: 0.00002737
Iteration 4/1000 | Loss: 0.00002474
Iteration 5/1000 | Loss: 0.00002348
Iteration 6/1000 | Loss: 0.00002272
Iteration 7/1000 | Loss: 0.00002209
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002144
Iteration 10/1000 | Loss: 0.00002125
Iteration 11/1000 | Loss: 0.00002119
Iteration 12/1000 | Loss: 0.00002116
Iteration 13/1000 | Loss: 0.00002111
Iteration 14/1000 | Loss: 0.00002110
Iteration 15/1000 | Loss: 0.00002108
Iteration 16/1000 | Loss: 0.00002108
Iteration 17/1000 | Loss: 0.00002107
Iteration 18/1000 | Loss: 0.00002107
Iteration 19/1000 | Loss: 0.00002106
Iteration 20/1000 | Loss: 0.00002106
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002106
Iteration 23/1000 | Loss: 0.00002105
Iteration 24/1000 | Loss: 0.00002105
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002104
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00002102
Iteration 29/1000 | Loss: 0.00002102
Iteration 30/1000 | Loss: 0.00002102
Iteration 31/1000 | Loss: 0.00002102
Iteration 32/1000 | Loss: 0.00002102
Iteration 33/1000 | Loss: 0.00002102
Iteration 34/1000 | Loss: 0.00002102
Iteration 35/1000 | Loss: 0.00002102
Iteration 36/1000 | Loss: 0.00002100
Iteration 37/1000 | Loss: 0.00002100
Iteration 38/1000 | Loss: 0.00002100
Iteration 39/1000 | Loss: 0.00002100
Iteration 40/1000 | Loss: 0.00002100
Iteration 41/1000 | Loss: 0.00002100
Iteration 42/1000 | Loss: 0.00002100
Iteration 43/1000 | Loss: 0.00002100
Iteration 44/1000 | Loss: 0.00002099
Iteration 45/1000 | Loss: 0.00002099
Iteration 46/1000 | Loss: 0.00002099
Iteration 47/1000 | Loss: 0.00002099
Iteration 48/1000 | Loss: 0.00002099
Iteration 49/1000 | Loss: 0.00002099
Iteration 50/1000 | Loss: 0.00002099
Iteration 51/1000 | Loss: 0.00002099
Iteration 52/1000 | Loss: 0.00002099
Iteration 53/1000 | Loss: 0.00002099
Iteration 54/1000 | Loss: 0.00002099
Iteration 55/1000 | Loss: 0.00002099
Iteration 56/1000 | Loss: 0.00002098
Iteration 57/1000 | Loss: 0.00002098
Iteration 58/1000 | Loss: 0.00002098
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002097
Iteration 61/1000 | Loss: 0.00002097
Iteration 62/1000 | Loss: 0.00002096
Iteration 63/1000 | Loss: 0.00002096
Iteration 64/1000 | Loss: 0.00002096
Iteration 65/1000 | Loss: 0.00002096
Iteration 66/1000 | Loss: 0.00002095
Iteration 67/1000 | Loss: 0.00002094
Iteration 68/1000 | Loss: 0.00002093
Iteration 69/1000 | Loss: 0.00002093
Iteration 70/1000 | Loss: 0.00002093
Iteration 71/1000 | Loss: 0.00002093
Iteration 72/1000 | Loss: 0.00002093
Iteration 73/1000 | Loss: 0.00002093
Iteration 74/1000 | Loss: 0.00002093
Iteration 75/1000 | Loss: 0.00002093
Iteration 76/1000 | Loss: 0.00002093
Iteration 77/1000 | Loss: 0.00002093
Iteration 78/1000 | Loss: 0.00002093
Iteration 79/1000 | Loss: 0.00002093
Iteration 80/1000 | Loss: 0.00002093
Iteration 81/1000 | Loss: 0.00002092
Iteration 82/1000 | Loss: 0.00002092
Iteration 83/1000 | Loss: 0.00002092
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002091
Iteration 86/1000 | Loss: 0.00002090
Iteration 87/1000 | Loss: 0.00002090
Iteration 88/1000 | Loss: 0.00002090
Iteration 89/1000 | Loss: 0.00002089
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002088
Iteration 93/1000 | Loss: 0.00002087
Iteration 94/1000 | Loss: 0.00002087
Iteration 95/1000 | Loss: 0.00002087
Iteration 96/1000 | Loss: 0.00002086
Iteration 97/1000 | Loss: 0.00002086
Iteration 98/1000 | Loss: 0.00002086
Iteration 99/1000 | Loss: 0.00002086
Iteration 100/1000 | Loss: 0.00002085
Iteration 101/1000 | Loss: 0.00002085
Iteration 102/1000 | Loss: 0.00002085
Iteration 103/1000 | Loss: 0.00002085
Iteration 104/1000 | Loss: 0.00002085
Iteration 105/1000 | Loss: 0.00002084
Iteration 106/1000 | Loss: 0.00002084
Iteration 107/1000 | Loss: 0.00002084
Iteration 108/1000 | Loss: 0.00002084
Iteration 109/1000 | Loss: 0.00002084
Iteration 110/1000 | Loss: 0.00002084
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002083
Iteration 115/1000 | Loss: 0.00002083
Iteration 116/1000 | Loss: 0.00002083
Iteration 117/1000 | Loss: 0.00002083
Iteration 118/1000 | Loss: 0.00002083
Iteration 119/1000 | Loss: 0.00002083
Iteration 120/1000 | Loss: 0.00002083
Iteration 121/1000 | Loss: 0.00002083
Iteration 122/1000 | Loss: 0.00002083
Iteration 123/1000 | Loss: 0.00002083
Iteration 124/1000 | Loss: 0.00002083
Iteration 125/1000 | Loss: 0.00002083
Iteration 126/1000 | Loss: 0.00002083
Iteration 127/1000 | Loss: 0.00002083
Iteration 128/1000 | Loss: 0.00002083
Iteration 129/1000 | Loss: 0.00002083
Iteration 130/1000 | Loss: 0.00002083
Iteration 131/1000 | Loss: 0.00002083
Iteration 132/1000 | Loss: 0.00002083
Iteration 133/1000 | Loss: 0.00002083
Iteration 134/1000 | Loss: 0.00002083
Iteration 135/1000 | Loss: 0.00002083
Iteration 136/1000 | Loss: 0.00002083
Iteration 137/1000 | Loss: 0.00002083
Iteration 138/1000 | Loss: 0.00002083
Iteration 139/1000 | Loss: 0.00002083
Iteration 140/1000 | Loss: 0.00002083
Iteration 141/1000 | Loss: 0.00002083
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002083
Iteration 144/1000 | Loss: 0.00002083
Iteration 145/1000 | Loss: 0.00002083
Iteration 146/1000 | Loss: 0.00002083
Iteration 147/1000 | Loss: 0.00002083
Iteration 148/1000 | Loss: 0.00002083
Iteration 149/1000 | Loss: 0.00002083
Iteration 150/1000 | Loss: 0.00002083
Iteration 151/1000 | Loss: 0.00002083
Iteration 152/1000 | Loss: 0.00002083
Iteration 153/1000 | Loss: 0.00002083
Iteration 154/1000 | Loss: 0.00002083
Iteration 155/1000 | Loss: 0.00002083
Iteration 156/1000 | Loss: 0.00002083
Iteration 157/1000 | Loss: 0.00002083
Iteration 158/1000 | Loss: 0.00002083
Iteration 159/1000 | Loss: 0.00002083
Iteration 160/1000 | Loss: 0.00002083
Iteration 161/1000 | Loss: 0.00002083
Iteration 162/1000 | Loss: 0.00002083
Iteration 163/1000 | Loss: 0.00002083
Iteration 164/1000 | Loss: 0.00002083
Iteration 165/1000 | Loss: 0.00002083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.082649189105723e-05, 2.082649189105723e-05, 2.082649189105723e-05, 2.082649189105723e-05, 2.082649189105723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.082649189105723e-05

Optimization complete. Final v2v error: 3.882164478302002 mm

Highest mean error: 4.470326900482178 mm for frame 70

Lowest mean error: 3.3049168586730957 mm for frame 49

Saving results

Total time: 33.08929443359375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01143309
Iteration 2/25 | Loss: 0.01143306
Iteration 3/25 | Loss: 0.01143305
Iteration 4/25 | Loss: 0.00342459
Iteration 5/25 | Loss: 0.00250617
Iteration 6/25 | Loss: 0.00246203
Iteration 7/25 | Loss: 0.00224273
Iteration 8/25 | Loss: 0.00183494
Iteration 9/25 | Loss: 0.00194161
Iteration 10/25 | Loss: 0.00200670
Iteration 11/25 | Loss: 0.00189909
Iteration 12/25 | Loss: 0.00182307
Iteration 13/25 | Loss: 0.00170438
Iteration 14/25 | Loss: 0.00163573
Iteration 15/25 | Loss: 0.00168839
Iteration 16/25 | Loss: 0.00168019
Iteration 17/25 | Loss: 0.00166909
Iteration 18/25 | Loss: 0.00167066
Iteration 19/25 | Loss: 0.00160903
Iteration 20/25 | Loss: 0.00158483
Iteration 21/25 | Loss: 0.00155125
Iteration 22/25 | Loss: 0.00154915
Iteration 23/25 | Loss: 0.00150057
Iteration 24/25 | Loss: 0.00155529
Iteration 25/25 | Loss: 0.00165871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78541267
Iteration 2/25 | Loss: 0.00925895
Iteration 3/25 | Loss: 0.00922335
Iteration 4/25 | Loss: 0.00922335
Iteration 5/25 | Loss: 0.00922334
Iteration 6/25 | Loss: 0.00922334
Iteration 7/25 | Loss: 0.00922334
Iteration 8/25 | Loss: 0.00922334
Iteration 9/25 | Loss: 0.00922334
Iteration 10/25 | Loss: 0.00922334
Iteration 11/25 | Loss: 0.00922334
Iteration 12/25 | Loss: 0.00922334
Iteration 13/25 | Loss: 0.00922334
Iteration 14/25 | Loss: 0.00922334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.009223340079188347, 0.009223340079188347, 0.009223340079188347, 0.009223340079188347, 0.009223340079188347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009223340079188347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00922334
Iteration 2/1000 | Loss: 0.00299064
Iteration 3/1000 | Loss: 0.00254080
Iteration 4/1000 | Loss: 0.00332493
Iteration 5/1000 | Loss: 0.00350475
Iteration 6/1000 | Loss: 0.00266450
Iteration 7/1000 | Loss: 0.00188474
Iteration 8/1000 | Loss: 0.00187993
Iteration 9/1000 | Loss: 0.00219903
Iteration 10/1000 | Loss: 0.00218811
Iteration 11/1000 | Loss: 0.00154842
Iteration 12/1000 | Loss: 0.00204426
Iteration 13/1000 | Loss: 0.00179822
Iteration 14/1000 | Loss: 0.00714007
Iteration 15/1000 | Loss: 0.00535944
Iteration 16/1000 | Loss: 0.00269418
Iteration 17/1000 | Loss: 0.00219408
Iteration 18/1000 | Loss: 0.00184650
Iteration 19/1000 | Loss: 0.00230273
Iteration 20/1000 | Loss: 0.00117435
Iteration 21/1000 | Loss: 0.00193723
Iteration 22/1000 | Loss: 0.00151841
Iteration 23/1000 | Loss: 0.00170767
Iteration 24/1000 | Loss: 0.00149640
Iteration 25/1000 | Loss: 0.00097289
Iteration 26/1000 | Loss: 0.00146440
Iteration 27/1000 | Loss: 0.00144546
Iteration 28/1000 | Loss: 0.00233787
Iteration 29/1000 | Loss: 0.00279637
Iteration 30/1000 | Loss: 0.00142785
Iteration 31/1000 | Loss: 0.00149516
Iteration 32/1000 | Loss: 0.00104311
Iteration 33/1000 | Loss: 0.00075347
Iteration 34/1000 | Loss: 0.00137530
Iteration 35/1000 | Loss: 0.00077185
Iteration 36/1000 | Loss: 0.00100674
Iteration 37/1000 | Loss: 0.00111083
Iteration 38/1000 | Loss: 0.00135389
Iteration 39/1000 | Loss: 0.00058603
Iteration 40/1000 | Loss: 0.00125423
Iteration 41/1000 | Loss: 0.00113664
Iteration 42/1000 | Loss: 0.00061458
Iteration 43/1000 | Loss: 0.00113277
Iteration 44/1000 | Loss: 0.00099142
Iteration 45/1000 | Loss: 0.00076931
Iteration 46/1000 | Loss: 0.00072412
Iteration 47/1000 | Loss: 0.00089748
Iteration 48/1000 | Loss: 0.00065077
Iteration 49/1000 | Loss: 0.00062987
Iteration 50/1000 | Loss: 0.00103209
Iteration 51/1000 | Loss: 0.00065494
Iteration 52/1000 | Loss: 0.00075952
Iteration 53/1000 | Loss: 0.00080925
Iteration 54/1000 | Loss: 0.00072295
Iteration 55/1000 | Loss: 0.00068504
Iteration 56/1000 | Loss: 0.00126797
Iteration 57/1000 | Loss: 0.00114340
Iteration 58/1000 | Loss: 0.00045036
Iteration 59/1000 | Loss: 0.00037594
Iteration 60/1000 | Loss: 0.00098431
Iteration 61/1000 | Loss: 0.00092549
Iteration 62/1000 | Loss: 0.00127244
Iteration 63/1000 | Loss: 0.00067138
Iteration 64/1000 | Loss: 0.00066120
Iteration 65/1000 | Loss: 0.00175075
Iteration 66/1000 | Loss: 0.00093255
Iteration 67/1000 | Loss: 0.00114928
Iteration 68/1000 | Loss: 0.00125304
Iteration 69/1000 | Loss: 0.00084794
Iteration 70/1000 | Loss: 0.00089829
Iteration 71/1000 | Loss: 0.00079570
Iteration 72/1000 | Loss: 0.00102728
Iteration 73/1000 | Loss: 0.00072394
Iteration 74/1000 | Loss: 0.00076495
Iteration 75/1000 | Loss: 0.00063794
Iteration 76/1000 | Loss: 0.00121403
Iteration 77/1000 | Loss: 0.00069548
Iteration 78/1000 | Loss: 0.00071840
Iteration 79/1000 | Loss: 0.00069583
Iteration 80/1000 | Loss: 0.00076952
Iteration 81/1000 | Loss: 0.00097288
Iteration 82/1000 | Loss: 0.00067042
Iteration 83/1000 | Loss: 0.00055838
Iteration 84/1000 | Loss: 0.00088096
Iteration 85/1000 | Loss: 0.00040855
Iteration 86/1000 | Loss: 0.00039801
Iteration 87/1000 | Loss: 0.00052189
Iteration 88/1000 | Loss: 0.00050000
Iteration 89/1000 | Loss: 0.00070064
Iteration 90/1000 | Loss: 0.00071145
Iteration 91/1000 | Loss: 0.00071509
Iteration 92/1000 | Loss: 0.00060259
Iteration 93/1000 | Loss: 0.00048916
Iteration 94/1000 | Loss: 0.00029965
Iteration 95/1000 | Loss: 0.00071343
Iteration 96/1000 | Loss: 0.00088610
Iteration 97/1000 | Loss: 0.00060337
Iteration 98/1000 | Loss: 0.00058035
Iteration 99/1000 | Loss: 0.00051616
Iteration 100/1000 | Loss: 0.00056122
Iteration 101/1000 | Loss: 0.00060119
Iteration 102/1000 | Loss: 0.00049913
Iteration 103/1000 | Loss: 0.00086332
Iteration 104/1000 | Loss: 0.00079904
Iteration 105/1000 | Loss: 0.00054171
Iteration 106/1000 | Loss: 0.00070192
Iteration 107/1000 | Loss: 0.00058536
Iteration 108/1000 | Loss: 0.00075310
Iteration 109/1000 | Loss: 0.00081364
Iteration 110/1000 | Loss: 0.00105345
Iteration 111/1000 | Loss: 0.00089196
Iteration 112/1000 | Loss: 0.00117114
Iteration 113/1000 | Loss: 0.00053726
Iteration 114/1000 | Loss: 0.00021704
Iteration 115/1000 | Loss: 0.00054032
Iteration 116/1000 | Loss: 0.00024132
Iteration 117/1000 | Loss: 0.00022808
Iteration 118/1000 | Loss: 0.00043677
Iteration 119/1000 | Loss: 0.00024645
Iteration 120/1000 | Loss: 0.00035919
Iteration 121/1000 | Loss: 0.00022560
Iteration 122/1000 | Loss: 0.00033439
Iteration 123/1000 | Loss: 0.00029347
Iteration 124/1000 | Loss: 0.00019793
Iteration 125/1000 | Loss: 0.00023068
Iteration 126/1000 | Loss: 0.00021866
Iteration 127/1000 | Loss: 0.00022845
Iteration 128/1000 | Loss: 0.00024071
Iteration 129/1000 | Loss: 0.00043819
Iteration 130/1000 | Loss: 0.00064684
Iteration 131/1000 | Loss: 0.00043977
Iteration 132/1000 | Loss: 0.00023048
Iteration 133/1000 | Loss: 0.00043713
Iteration 134/1000 | Loss: 0.00022185
Iteration 135/1000 | Loss: 0.00044658
Iteration 136/1000 | Loss: 0.00022468
Iteration 137/1000 | Loss: 0.00029987
Iteration 138/1000 | Loss: 0.00024788
Iteration 139/1000 | Loss: 0.00025217
Iteration 140/1000 | Loss: 0.00021403
Iteration 141/1000 | Loss: 0.00021925
Iteration 142/1000 | Loss: 0.00031756
Iteration 143/1000 | Loss: 0.00018133
Iteration 144/1000 | Loss: 0.00019870
Iteration 145/1000 | Loss: 0.00021520
Iteration 146/1000 | Loss: 0.00021094
Iteration 147/1000 | Loss: 0.00023464
Iteration 148/1000 | Loss: 0.00023970
Iteration 149/1000 | Loss: 0.00021449
Iteration 150/1000 | Loss: 0.00020839
Iteration 151/1000 | Loss: 0.00022028
Iteration 152/1000 | Loss: 0.00022415
Iteration 153/1000 | Loss: 0.00021920
Iteration 154/1000 | Loss: 0.00022543
Iteration 155/1000 | Loss: 0.00018188
Iteration 156/1000 | Loss: 0.00020530
Iteration 157/1000 | Loss: 0.00016796
Iteration 158/1000 | Loss: 0.00019793
Iteration 159/1000 | Loss: 0.00018561
Iteration 160/1000 | Loss: 0.00019236
Iteration 161/1000 | Loss: 0.00020896
Iteration 162/1000 | Loss: 0.00018770
Iteration 163/1000 | Loss: 0.00017419
Iteration 164/1000 | Loss: 0.00018065
Iteration 165/1000 | Loss: 0.00014622
Iteration 166/1000 | Loss: 0.00016665
Iteration 167/1000 | Loss: 0.00018538
Iteration 168/1000 | Loss: 0.00018023
Iteration 169/1000 | Loss: 0.00019474
Iteration 170/1000 | Loss: 0.00019718
Iteration 171/1000 | Loss: 0.00015643
Iteration 172/1000 | Loss: 0.00011917
Iteration 173/1000 | Loss: 0.00020436
Iteration 174/1000 | Loss: 0.00020254
Iteration 175/1000 | Loss: 0.00020144
Iteration 176/1000 | Loss: 0.00020082
Iteration 177/1000 | Loss: 0.00019759
Iteration 178/1000 | Loss: 0.00020054
Iteration 179/1000 | Loss: 0.00020775
Iteration 180/1000 | Loss: 0.00019695
Iteration 181/1000 | Loss: 0.00018966
Iteration 182/1000 | Loss: 0.00019867
Iteration 183/1000 | Loss: 0.00019684
Iteration 184/1000 | Loss: 0.00020421
Iteration 185/1000 | Loss: 0.00023079
Iteration 186/1000 | Loss: 0.00019965
Iteration 187/1000 | Loss: 0.00020517
Iteration 188/1000 | Loss: 0.00019614
Iteration 189/1000 | Loss: 0.00020456
Iteration 190/1000 | Loss: 0.00020643
Iteration 191/1000 | Loss: 0.00021028
Iteration 192/1000 | Loss: 0.00019427
Iteration 193/1000 | Loss: 0.00018169
Iteration 194/1000 | Loss: 0.00018530
Iteration 195/1000 | Loss: 0.00020512
Iteration 196/1000 | Loss: 0.00020035
Iteration 197/1000 | Loss: 0.00019497
Iteration 198/1000 | Loss: 0.00019188
Iteration 199/1000 | Loss: 0.00019790
Iteration 200/1000 | Loss: 0.00018990
Iteration 201/1000 | Loss: 0.00019789
Iteration 202/1000 | Loss: 0.00018738
Iteration 203/1000 | Loss: 0.00020204
Iteration 204/1000 | Loss: 0.00018905
Iteration 205/1000 | Loss: 0.00039650
Iteration 206/1000 | Loss: 0.00020572
Iteration 207/1000 | Loss: 0.00021073
Iteration 208/1000 | Loss: 0.00020672
Iteration 209/1000 | Loss: 0.00016221
Iteration 210/1000 | Loss: 0.00014233
Iteration 211/1000 | Loss: 0.00013272
Iteration 212/1000 | Loss: 0.00029632
Iteration 213/1000 | Loss: 0.00016134
Iteration 214/1000 | Loss: 0.00019286
Iteration 215/1000 | Loss: 0.00018873
Iteration 216/1000 | Loss: 0.00016463
Iteration 217/1000 | Loss: 0.00018113
Iteration 218/1000 | Loss: 0.00017995
Iteration 219/1000 | Loss: 0.00017039
Iteration 220/1000 | Loss: 0.00016593
Iteration 221/1000 | Loss: 0.00016877
Iteration 222/1000 | Loss: 0.00052642
Iteration 223/1000 | Loss: 0.00019604
Iteration 224/1000 | Loss: 0.00018105
Iteration 225/1000 | Loss: 0.00018432
Iteration 226/1000 | Loss: 0.00018302
Iteration 227/1000 | Loss: 0.00017236
Iteration 228/1000 | Loss: 0.00018564
Iteration 229/1000 | Loss: 0.00016442
Iteration 230/1000 | Loss: 0.00017685
Iteration 231/1000 | Loss: 0.00019101
Iteration 232/1000 | Loss: 0.00019190
Iteration 233/1000 | Loss: 0.00016804
Iteration 234/1000 | Loss: 0.00017440
Iteration 235/1000 | Loss: 0.00011792
Iteration 236/1000 | Loss: 0.00016462
Iteration 237/1000 | Loss: 0.00015952
Iteration 238/1000 | Loss: 0.00012234
Iteration 239/1000 | Loss: 0.00017965
Iteration 240/1000 | Loss: 0.00016102
Iteration 241/1000 | Loss: 0.00017514
Iteration 242/1000 | Loss: 0.00017269
Iteration 243/1000 | Loss: 0.00020024
Iteration 244/1000 | Loss: 0.00017090
Iteration 245/1000 | Loss: 0.00017884
Iteration 246/1000 | Loss: 0.00017364
Iteration 247/1000 | Loss: 0.00035128
Iteration 248/1000 | Loss: 0.00018681
Iteration 249/1000 | Loss: 0.00019674
Iteration 250/1000 | Loss: 0.00018134
Iteration 251/1000 | Loss: 0.00018161
Iteration 252/1000 | Loss: 0.00019879
Iteration 253/1000 | Loss: 0.00019021
Iteration 254/1000 | Loss: 0.00018118
Iteration 255/1000 | Loss: 0.00017398
Iteration 256/1000 | Loss: 0.00018660
Iteration 257/1000 | Loss: 0.00018446
Iteration 258/1000 | Loss: 0.00018925
Iteration 259/1000 | Loss: 0.00017923
Iteration 260/1000 | Loss: 0.00018835
Iteration 261/1000 | Loss: 0.00017791
Iteration 262/1000 | Loss: 0.00014607
Iteration 263/1000 | Loss: 0.00014014
Iteration 264/1000 | Loss: 0.00015884
Iteration 265/1000 | Loss: 0.00013957
Iteration 266/1000 | Loss: 0.00015157
Iteration 267/1000 | Loss: 0.00018506
Iteration 268/1000 | Loss: 0.00016575
Iteration 269/1000 | Loss: 0.00017608
Iteration 270/1000 | Loss: 0.00012230
Iteration 271/1000 | Loss: 0.00013476
Iteration 272/1000 | Loss: 0.00011607
Iteration 273/1000 | Loss: 0.00010570
Iteration 274/1000 | Loss: 0.00012718
Iteration 275/1000 | Loss: 0.00039343
Iteration 276/1000 | Loss: 0.00018871
Iteration 277/1000 | Loss: 0.00016537
Iteration 278/1000 | Loss: 0.00034297
Iteration 279/1000 | Loss: 0.00017483
Iteration 280/1000 | Loss: 0.00175627
Iteration 281/1000 | Loss: 0.00038554
Iteration 282/1000 | Loss: 0.00141868
Iteration 283/1000 | Loss: 0.00024868
Iteration 284/1000 | Loss: 0.00165388
Iteration 285/1000 | Loss: 0.00027510
Iteration 286/1000 | Loss: 0.00059124
Iteration 287/1000 | Loss: 0.00051362
Iteration 288/1000 | Loss: 0.00096840
Iteration 289/1000 | Loss: 0.00047675
Iteration 290/1000 | Loss: 0.00057686
Iteration 291/1000 | Loss: 0.00120557
Iteration 292/1000 | Loss: 0.00073227
Iteration 293/1000 | Loss: 0.00049665
Iteration 294/1000 | Loss: 0.00025755
Iteration 295/1000 | Loss: 0.00012493
Iteration 296/1000 | Loss: 0.00052809
Iteration 297/1000 | Loss: 0.00011769
Iteration 298/1000 | Loss: 0.00010290
Iteration 299/1000 | Loss: 0.00009623
Iteration 300/1000 | Loss: 0.00008440
Iteration 301/1000 | Loss: 0.00007714
Iteration 302/1000 | Loss: 0.00026134
Iteration 303/1000 | Loss: 0.00007577
Iteration 304/1000 | Loss: 0.00027263
Iteration 305/1000 | Loss: 0.00007333
Iteration 306/1000 | Loss: 0.00006989
Iteration 307/1000 | Loss: 0.00025171
Iteration 308/1000 | Loss: 0.00007186
Iteration 309/1000 | Loss: 0.00006823
Iteration 310/1000 | Loss: 0.00006325
Iteration 311/1000 | Loss: 0.00030025
Iteration 312/1000 | Loss: 0.00006648
Iteration 313/1000 | Loss: 0.00006851
Iteration 314/1000 | Loss: 0.00006025
Iteration 315/1000 | Loss: 0.00005731
Iteration 316/1000 | Loss: 0.00041598
Iteration 317/1000 | Loss: 0.00006445
Iteration 318/1000 | Loss: 0.00005891
Iteration 319/1000 | Loss: 0.00005571
Iteration 320/1000 | Loss: 0.00058414
Iteration 321/1000 | Loss: 0.00127519
Iteration 322/1000 | Loss: 0.00404964
Iteration 323/1000 | Loss: 0.00111833
Iteration 324/1000 | Loss: 0.00070207
Iteration 325/1000 | Loss: 0.00032500
Iteration 326/1000 | Loss: 0.00069181
Iteration 327/1000 | Loss: 0.00043235
Iteration 328/1000 | Loss: 0.00028708
Iteration 329/1000 | Loss: 0.00059871
Iteration 330/1000 | Loss: 0.00086573
Iteration 331/1000 | Loss: 0.00021013
Iteration 332/1000 | Loss: 0.00034845
Iteration 333/1000 | Loss: 0.00048519
Iteration 334/1000 | Loss: 0.00054668
Iteration 335/1000 | Loss: 0.00074823
Iteration 336/1000 | Loss: 0.00090996
Iteration 337/1000 | Loss: 0.00123490
Iteration 338/1000 | Loss: 0.00050378
Iteration 339/1000 | Loss: 0.00051058
Iteration 340/1000 | Loss: 0.00009007
Iteration 341/1000 | Loss: 0.00006911
Iteration 342/1000 | Loss: 0.00024976
Iteration 343/1000 | Loss: 0.00006076
Iteration 344/1000 | Loss: 0.00005384
Iteration 345/1000 | Loss: 0.00044051
Iteration 346/1000 | Loss: 0.00026947
Iteration 347/1000 | Loss: 0.00018946
Iteration 348/1000 | Loss: 0.00005038
Iteration 349/1000 | Loss: 0.00004471
Iteration 350/1000 | Loss: 0.00004153
Iteration 351/1000 | Loss: 0.00003930
Iteration 352/1000 | Loss: 0.00003823
Iteration 353/1000 | Loss: 0.00004314
Iteration 354/1000 | Loss: 0.00031367
Iteration 355/1000 | Loss: 0.00020582
Iteration 356/1000 | Loss: 0.00026809
Iteration 357/1000 | Loss: 0.00018861
Iteration 358/1000 | Loss: 0.00024720
Iteration 359/1000 | Loss: 0.00016296
Iteration 360/1000 | Loss: 0.00020641
Iteration 361/1000 | Loss: 0.00004865
Iteration 362/1000 | Loss: 0.00023569
Iteration 363/1000 | Loss: 0.00003744
Iteration 364/1000 | Loss: 0.00003575
Iteration 365/1000 | Loss: 0.00003452
Iteration 366/1000 | Loss: 0.00027821
Iteration 367/1000 | Loss: 0.00005029
Iteration 368/1000 | Loss: 0.00004174
Iteration 369/1000 | Loss: 0.00019735
Iteration 370/1000 | Loss: 0.00004528
Iteration 371/1000 | Loss: 0.00003926
Iteration 372/1000 | Loss: 0.00003689
Iteration 373/1000 | Loss: 0.00003590
Iteration 374/1000 | Loss: 0.00003562
Iteration 375/1000 | Loss: 0.00003527
Iteration 376/1000 | Loss: 0.00026712
Iteration 377/1000 | Loss: 0.00026092
Iteration 378/1000 | Loss: 0.00026152
Iteration 379/1000 | Loss: 0.00015664
Iteration 380/1000 | Loss: 0.00012862
Iteration 381/1000 | Loss: 0.00029298
Iteration 382/1000 | Loss: 0.00014558
Iteration 383/1000 | Loss: 0.00035648
Iteration 384/1000 | Loss: 0.00019120
Iteration 385/1000 | Loss: 0.00009761
Iteration 386/1000 | Loss: 0.00003552
Iteration 387/1000 | Loss: 0.00003474
Iteration 388/1000 | Loss: 0.00003416
Iteration 389/1000 | Loss: 0.00003335
Iteration 390/1000 | Loss: 0.00003263
Iteration 391/1000 | Loss: 0.00003209
Iteration 392/1000 | Loss: 0.00003191
Iteration 393/1000 | Loss: 0.00003189
Iteration 394/1000 | Loss: 0.00003175
Iteration 395/1000 | Loss: 0.00003167
Iteration 396/1000 | Loss: 0.00003167
Iteration 397/1000 | Loss: 0.00003167
Iteration 398/1000 | Loss: 0.00003166
Iteration 399/1000 | Loss: 0.00003166
Iteration 400/1000 | Loss: 0.00003165
Iteration 401/1000 | Loss: 0.00003165
Iteration 402/1000 | Loss: 0.00003165
Iteration 403/1000 | Loss: 0.00003165
Iteration 404/1000 | Loss: 0.00003164
Iteration 405/1000 | Loss: 0.00003164
Iteration 406/1000 | Loss: 0.00003164
Iteration 407/1000 | Loss: 0.00003164
Iteration 408/1000 | Loss: 0.00003164
Iteration 409/1000 | Loss: 0.00003164
Iteration 410/1000 | Loss: 0.00003164
Iteration 411/1000 | Loss: 0.00003164
Iteration 412/1000 | Loss: 0.00003164
Iteration 413/1000 | Loss: 0.00003164
Iteration 414/1000 | Loss: 0.00003163
Iteration 415/1000 | Loss: 0.00003163
Iteration 416/1000 | Loss: 0.00003163
Iteration 417/1000 | Loss: 0.00003163
Iteration 418/1000 | Loss: 0.00003163
Iteration 419/1000 | Loss: 0.00003161
Iteration 420/1000 | Loss: 0.00003161
Iteration 421/1000 | Loss: 0.00003160
Iteration 422/1000 | Loss: 0.00003159
Iteration 423/1000 | Loss: 0.00003158
Iteration 424/1000 | Loss: 0.00003158
Iteration 425/1000 | Loss: 0.00003158
Iteration 426/1000 | Loss: 0.00003158
Iteration 427/1000 | Loss: 0.00003157
Iteration 428/1000 | Loss: 0.00003157
Iteration 429/1000 | Loss: 0.00003157
Iteration 430/1000 | Loss: 0.00003157
Iteration 431/1000 | Loss: 0.00003157
Iteration 432/1000 | Loss: 0.00003157
Iteration 433/1000 | Loss: 0.00003157
Iteration 434/1000 | Loss: 0.00003157
Iteration 435/1000 | Loss: 0.00003157
Iteration 436/1000 | Loss: 0.00003157
Iteration 437/1000 | Loss: 0.00003157
Iteration 438/1000 | Loss: 0.00003157
Iteration 439/1000 | Loss: 0.00003157
Iteration 440/1000 | Loss: 0.00003157
Iteration 441/1000 | Loss: 0.00003157
Iteration 442/1000 | Loss: 0.00003157
Iteration 443/1000 | Loss: 0.00003157
Iteration 444/1000 | Loss: 0.00003157
Iteration 445/1000 | Loss: 0.00003157
Iteration 446/1000 | Loss: 0.00003157
Iteration 447/1000 | Loss: 0.00003157
Iteration 448/1000 | Loss: 0.00003157
Iteration 449/1000 | Loss: 0.00003157
Iteration 450/1000 | Loss: 0.00003157
Iteration 451/1000 | Loss: 0.00003157
Iteration 452/1000 | Loss: 0.00003157
Iteration 453/1000 | Loss: 0.00003157
Iteration 454/1000 | Loss: 0.00003157
Iteration 455/1000 | Loss: 0.00003157
Iteration 456/1000 | Loss: 0.00003157
Iteration 457/1000 | Loss: 0.00003157
Iteration 458/1000 | Loss: 0.00003157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 458. Stopping optimization.
Last 5 losses: [3.156700768158771e-05, 3.156700768158771e-05, 3.156700768158771e-05, 3.156700768158771e-05, 3.156700768158771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.156700768158771e-05

Optimization complete. Final v2v error: 4.718169689178467 mm

Highest mean error: 5.720666885375977 mm for frame 60

Lowest mean error: 4.088337421417236 mm for frame 87

Saving results

Total time: 691.1150465011597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098735
Iteration 2/25 | Loss: 0.00133088
Iteration 3/25 | Loss: 0.00098813
Iteration 4/25 | Loss: 0.00094224
Iteration 5/25 | Loss: 0.00093379
Iteration 6/25 | Loss: 0.00093236
Iteration 7/25 | Loss: 0.00093236
Iteration 8/25 | Loss: 0.00093236
Iteration 9/25 | Loss: 0.00093236
Iteration 10/25 | Loss: 0.00093236
Iteration 11/25 | Loss: 0.00093236
Iteration 12/25 | Loss: 0.00093236
Iteration 13/25 | Loss: 0.00093236
Iteration 14/25 | Loss: 0.00093236
Iteration 15/25 | Loss: 0.00093236
Iteration 16/25 | Loss: 0.00093236
Iteration 17/25 | Loss: 0.00093236
Iteration 18/25 | Loss: 0.00093236
Iteration 19/25 | Loss: 0.00093236
Iteration 20/25 | Loss: 0.00093236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000932362221647054, 0.000932362221647054, 0.000932362221647054, 0.000932362221647054, 0.000932362221647054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000932362221647054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55971515
Iteration 2/25 | Loss: 0.00154130
Iteration 3/25 | Loss: 0.00154130
Iteration 4/25 | Loss: 0.00154130
Iteration 5/25 | Loss: 0.00154130
Iteration 6/25 | Loss: 0.00154130
Iteration 7/25 | Loss: 0.00154130
Iteration 8/25 | Loss: 0.00154130
Iteration 9/25 | Loss: 0.00154130
Iteration 10/25 | Loss: 0.00154130
Iteration 11/25 | Loss: 0.00154130
Iteration 12/25 | Loss: 0.00154130
Iteration 13/25 | Loss: 0.00154130
Iteration 14/25 | Loss: 0.00154130
Iteration 15/25 | Loss: 0.00154130
Iteration 16/25 | Loss: 0.00154130
Iteration 17/25 | Loss: 0.00154130
Iteration 18/25 | Loss: 0.00154130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015413002111017704, 0.0015413002111017704, 0.0015413002111017704, 0.0015413002111017704, 0.0015413002111017704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015413002111017704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154130
Iteration 2/1000 | Loss: 0.00002746
Iteration 3/1000 | Loss: 0.00002066
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001822
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001702
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001691
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001689
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001687
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001687
Iteration 17/1000 | Loss: 0.00001687
Iteration 18/1000 | Loss: 0.00001686
Iteration 19/1000 | Loss: 0.00001686
Iteration 20/1000 | Loss: 0.00001686
Iteration 21/1000 | Loss: 0.00001686
Iteration 22/1000 | Loss: 0.00001686
Iteration 23/1000 | Loss: 0.00001686
Iteration 24/1000 | Loss: 0.00001686
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00001685
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001685
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001684
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001684
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001684
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00001684
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001683
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001683
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001682
Iteration 59/1000 | Loss: 0.00001682
Iteration 60/1000 | Loss: 0.00001682
Iteration 61/1000 | Loss: 0.00001682
Iteration 62/1000 | Loss: 0.00001682
Iteration 63/1000 | Loss: 0.00001682
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001682
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001681
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001679
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001679
Iteration 79/1000 | Loss: 0.00001679
Iteration 80/1000 | Loss: 0.00001679
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001678
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001678
Iteration 85/1000 | Loss: 0.00001678
Iteration 86/1000 | Loss: 0.00001678
Iteration 87/1000 | Loss: 0.00001678
Iteration 88/1000 | Loss: 0.00001678
Iteration 89/1000 | Loss: 0.00001678
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001677
Iteration 95/1000 | Loss: 0.00001677
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001677
Iteration 107/1000 | Loss: 0.00001677
Iteration 108/1000 | Loss: 0.00001677
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00001677
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001676
Iteration 113/1000 | Loss: 0.00001676
Iteration 114/1000 | Loss: 0.00001676
Iteration 115/1000 | Loss: 0.00001676
Iteration 116/1000 | Loss: 0.00001676
Iteration 117/1000 | Loss: 0.00001676
Iteration 118/1000 | Loss: 0.00001676
Iteration 119/1000 | Loss: 0.00001676
Iteration 120/1000 | Loss: 0.00001676
Iteration 121/1000 | Loss: 0.00001676
Iteration 122/1000 | Loss: 0.00001676
Iteration 123/1000 | Loss: 0.00001676
Iteration 124/1000 | Loss: 0.00001676
Iteration 125/1000 | Loss: 0.00001676
Iteration 126/1000 | Loss: 0.00001676
Iteration 127/1000 | Loss: 0.00001676
Iteration 128/1000 | Loss: 0.00001676
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00001676
Iteration 132/1000 | Loss: 0.00001676
Iteration 133/1000 | Loss: 0.00001676
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001676
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001676
Iteration 140/1000 | Loss: 0.00001676
Iteration 141/1000 | Loss: 0.00001676
Iteration 142/1000 | Loss: 0.00001676
Iteration 143/1000 | Loss: 0.00001676
Iteration 144/1000 | Loss: 0.00001676
Iteration 145/1000 | Loss: 0.00001676
Iteration 146/1000 | Loss: 0.00001676
Iteration 147/1000 | Loss: 0.00001676
Iteration 148/1000 | Loss: 0.00001676
Iteration 149/1000 | Loss: 0.00001676
Iteration 150/1000 | Loss: 0.00001676
Iteration 151/1000 | Loss: 0.00001676
Iteration 152/1000 | Loss: 0.00001676
Iteration 153/1000 | Loss: 0.00001676
Iteration 154/1000 | Loss: 0.00001676
Iteration 155/1000 | Loss: 0.00001676
Iteration 156/1000 | Loss: 0.00001676
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001676
Iteration 159/1000 | Loss: 0.00001676
Iteration 160/1000 | Loss: 0.00001676
Iteration 161/1000 | Loss: 0.00001676
Iteration 162/1000 | Loss: 0.00001676
Iteration 163/1000 | Loss: 0.00001676
Iteration 164/1000 | Loss: 0.00001676
Iteration 165/1000 | Loss: 0.00001676
Iteration 166/1000 | Loss: 0.00001676
Iteration 167/1000 | Loss: 0.00001676
Iteration 168/1000 | Loss: 0.00001676
Iteration 169/1000 | Loss: 0.00001676
Iteration 170/1000 | Loss: 0.00001676
Iteration 171/1000 | Loss: 0.00001676
Iteration 172/1000 | Loss: 0.00001676
Iteration 173/1000 | Loss: 0.00001676
Iteration 174/1000 | Loss: 0.00001676
Iteration 175/1000 | Loss: 0.00001676
Iteration 176/1000 | Loss: 0.00001676
Iteration 177/1000 | Loss: 0.00001676
Iteration 178/1000 | Loss: 0.00001676
Iteration 179/1000 | Loss: 0.00001676
Iteration 180/1000 | Loss: 0.00001676
Iteration 181/1000 | Loss: 0.00001676
Iteration 182/1000 | Loss: 0.00001676
Iteration 183/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.6756819604779594e-05, 1.6756819604779594e-05, 1.6756819604779594e-05, 1.6756819604779594e-05, 1.6756819604779594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6756819604779594e-05

Optimization complete. Final v2v error: 3.4549479484558105 mm

Highest mean error: 3.8032097816467285 mm for frame 20

Lowest mean error: 3.1036291122436523 mm for frame 126

Saving results

Total time: 29.747422456741333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093338
Iteration 2/25 | Loss: 0.00305969
Iteration 3/25 | Loss: 0.00191489
Iteration 4/25 | Loss: 0.00149384
Iteration 5/25 | Loss: 0.00161099
Iteration 6/25 | Loss: 0.00142251
Iteration 7/25 | Loss: 0.00119013
Iteration 8/25 | Loss: 0.00106468
Iteration 9/25 | Loss: 0.00100778
Iteration 10/25 | Loss: 0.00099336
Iteration 11/25 | Loss: 0.00099577
Iteration 12/25 | Loss: 0.00097934
Iteration 13/25 | Loss: 0.00096654
Iteration 14/25 | Loss: 0.00095993
Iteration 15/25 | Loss: 0.00095375
Iteration 16/25 | Loss: 0.00094612
Iteration 17/25 | Loss: 0.00094117
Iteration 18/25 | Loss: 0.00093964
Iteration 19/25 | Loss: 0.00094300
Iteration 20/25 | Loss: 0.00094295
Iteration 21/25 | Loss: 0.00094421
Iteration 22/25 | Loss: 0.00094533
Iteration 23/25 | Loss: 0.00094473
Iteration 24/25 | Loss: 0.00094342
Iteration 25/25 | Loss: 0.00094365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64314342
Iteration 2/25 | Loss: 0.00222295
Iteration 3/25 | Loss: 0.00216509
Iteration 4/25 | Loss: 0.00216509
Iteration 5/25 | Loss: 0.00216509
Iteration 6/25 | Loss: 0.00216509
Iteration 7/25 | Loss: 0.00216509
Iteration 8/25 | Loss: 0.00216509
Iteration 9/25 | Loss: 0.00216509
Iteration 10/25 | Loss: 0.00216509
Iteration 11/25 | Loss: 0.00216509
Iteration 12/25 | Loss: 0.00216509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002165090059861541, 0.002165090059861541, 0.002165090059861541, 0.002165090059861541, 0.002165090059861541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002165090059861541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216509
Iteration 2/1000 | Loss: 0.00091640
Iteration 3/1000 | Loss: 0.00039529
Iteration 4/1000 | Loss: 0.00058344
Iteration 5/1000 | Loss: 0.00017081
Iteration 6/1000 | Loss: 0.00050889
Iteration 7/1000 | Loss: 0.00023692
Iteration 8/1000 | Loss: 0.00017212
Iteration 9/1000 | Loss: 0.00016916
Iteration 10/1000 | Loss: 0.00009047
Iteration 11/1000 | Loss: 0.00021885
Iteration 12/1000 | Loss: 0.00006419
Iteration 13/1000 | Loss: 0.00037715
Iteration 14/1000 | Loss: 0.00026131
Iteration 15/1000 | Loss: 0.00019631
Iteration 16/1000 | Loss: 0.00021208
Iteration 17/1000 | Loss: 0.00040187
Iteration 18/1000 | Loss: 0.00013203
Iteration 19/1000 | Loss: 0.00007313
Iteration 20/1000 | Loss: 0.00016571
Iteration 21/1000 | Loss: 0.00040926
Iteration 22/1000 | Loss: 0.00017332
Iteration 23/1000 | Loss: 0.00005940
Iteration 24/1000 | Loss: 0.00013738
Iteration 25/1000 | Loss: 0.00007153
Iteration 26/1000 | Loss: 0.00006909
Iteration 27/1000 | Loss: 0.00006656
Iteration 28/1000 | Loss: 0.00012308
Iteration 29/1000 | Loss: 0.00006451
Iteration 30/1000 | Loss: 0.00005419
Iteration 31/1000 | Loss: 0.00025875
Iteration 32/1000 | Loss: 0.00009214
Iteration 33/1000 | Loss: 0.00020561
Iteration 34/1000 | Loss: 0.00022300
Iteration 35/1000 | Loss: 0.00008059
Iteration 36/1000 | Loss: 0.00014713
Iteration 37/1000 | Loss: 0.00021623
Iteration 38/1000 | Loss: 0.00014740
Iteration 39/1000 | Loss: 0.00018405
Iteration 40/1000 | Loss: 0.00010914
Iteration 41/1000 | Loss: 0.00014881
Iteration 42/1000 | Loss: 0.00012896
Iteration 43/1000 | Loss: 0.00010552
Iteration 44/1000 | Loss: 0.00021470
Iteration 45/1000 | Loss: 0.00005157
Iteration 46/1000 | Loss: 0.00004425
Iteration 47/1000 | Loss: 0.00004058
Iteration 48/1000 | Loss: 0.00013321
Iteration 49/1000 | Loss: 0.00045515
Iteration 50/1000 | Loss: 0.00020336
Iteration 51/1000 | Loss: 0.00005412
Iteration 52/1000 | Loss: 0.00004146
Iteration 53/1000 | Loss: 0.00004173
Iteration 54/1000 | Loss: 0.00020182
Iteration 55/1000 | Loss: 0.00015154
Iteration 56/1000 | Loss: 0.00033870
Iteration 57/1000 | Loss: 0.00037808
Iteration 58/1000 | Loss: 0.00046731
Iteration 59/1000 | Loss: 0.00033617
Iteration 60/1000 | Loss: 0.00028263
Iteration 61/1000 | Loss: 0.00041163
Iteration 62/1000 | Loss: 0.00031987
Iteration 63/1000 | Loss: 0.00030820
Iteration 64/1000 | Loss: 0.00040362
Iteration 65/1000 | Loss: 0.00041090
Iteration 66/1000 | Loss: 0.00042126
Iteration 67/1000 | Loss: 0.00049839
Iteration 68/1000 | Loss: 0.00028995
Iteration 69/1000 | Loss: 0.00010459
Iteration 70/1000 | Loss: 0.00030033
Iteration 71/1000 | Loss: 0.00005153
Iteration 72/1000 | Loss: 0.00005994
Iteration 73/1000 | Loss: 0.00004239
Iteration 74/1000 | Loss: 0.00004486
Iteration 75/1000 | Loss: 0.00004437
Iteration 76/1000 | Loss: 0.00004504
Iteration 77/1000 | Loss: 0.00004108
Iteration 78/1000 | Loss: 0.00003914
Iteration 79/1000 | Loss: 0.00004419
Iteration 80/1000 | Loss: 0.00004871
Iteration 81/1000 | Loss: 0.00005305
Iteration 82/1000 | Loss: 0.00005694
Iteration 83/1000 | Loss: 0.00004692
Iteration 84/1000 | Loss: 0.00004878
Iteration 85/1000 | Loss: 0.00011602
Iteration 86/1000 | Loss: 0.00004800
Iteration 87/1000 | Loss: 0.00005148
Iteration 88/1000 | Loss: 0.00006166
Iteration 89/1000 | Loss: 0.00004388
Iteration 90/1000 | Loss: 0.00005976
Iteration 91/1000 | Loss: 0.00004380
Iteration 92/1000 | Loss: 0.00004281
Iteration 93/1000 | Loss: 0.00004817
Iteration 94/1000 | Loss: 0.00006022
Iteration 95/1000 | Loss: 0.00005320
Iteration 96/1000 | Loss: 0.00005820
Iteration 97/1000 | Loss: 0.00005152
Iteration 98/1000 | Loss: 0.00005256
Iteration 99/1000 | Loss: 0.00004846
Iteration 100/1000 | Loss: 0.00004633
Iteration 101/1000 | Loss: 0.00003975
Iteration 102/1000 | Loss: 0.00003824
Iteration 103/1000 | Loss: 0.00004297
Iteration 104/1000 | Loss: 0.00004392
Iteration 105/1000 | Loss: 0.00003634
Iteration 106/1000 | Loss: 0.00026283
Iteration 107/1000 | Loss: 0.00013375
Iteration 108/1000 | Loss: 0.00005169
Iteration 109/1000 | Loss: 0.00005338
Iteration 110/1000 | Loss: 0.00004533
Iteration 111/1000 | Loss: 0.00004313
Iteration 112/1000 | Loss: 0.00025750
Iteration 113/1000 | Loss: 0.00015492
Iteration 114/1000 | Loss: 0.00035538
Iteration 115/1000 | Loss: 0.00012389
Iteration 116/1000 | Loss: 0.00007094
Iteration 117/1000 | Loss: 0.00006900
Iteration 118/1000 | Loss: 0.00005507
Iteration 119/1000 | Loss: 0.00005860
Iteration 120/1000 | Loss: 0.00004576
Iteration 121/1000 | Loss: 0.00004391
Iteration 122/1000 | Loss: 0.00003465
Iteration 123/1000 | Loss: 0.00004887
Iteration 124/1000 | Loss: 0.00029678
Iteration 125/1000 | Loss: 0.00012449
Iteration 126/1000 | Loss: 0.00004399
Iteration 127/1000 | Loss: 0.00023757
Iteration 128/1000 | Loss: 0.00011744
Iteration 129/1000 | Loss: 0.00019169
Iteration 130/1000 | Loss: 0.00006142
Iteration 131/1000 | Loss: 0.00005375
Iteration 132/1000 | Loss: 0.00014646
Iteration 133/1000 | Loss: 0.00011994
Iteration 134/1000 | Loss: 0.00014058
Iteration 135/1000 | Loss: 0.00011373
Iteration 136/1000 | Loss: 0.00015056
Iteration 137/1000 | Loss: 0.00018333
Iteration 138/1000 | Loss: 0.00005458
Iteration 139/1000 | Loss: 0.00003560
Iteration 140/1000 | Loss: 0.00005103
Iteration 141/1000 | Loss: 0.00017750
Iteration 142/1000 | Loss: 0.00007739
Iteration 143/1000 | Loss: 0.00015538
Iteration 144/1000 | Loss: 0.00016626
Iteration 145/1000 | Loss: 0.00013557
Iteration 146/1000 | Loss: 0.00007138
Iteration 147/1000 | Loss: 0.00013536
Iteration 148/1000 | Loss: 0.00016214
Iteration 149/1000 | Loss: 0.00004560
Iteration 150/1000 | Loss: 0.00006001
Iteration 151/1000 | Loss: 0.00006017
Iteration 152/1000 | Loss: 0.00003967
Iteration 153/1000 | Loss: 0.00005544
Iteration 154/1000 | Loss: 0.00014144
Iteration 155/1000 | Loss: 0.00015081
Iteration 156/1000 | Loss: 0.00013697
Iteration 157/1000 | Loss: 0.00013466
Iteration 158/1000 | Loss: 0.00010982
Iteration 159/1000 | Loss: 0.00007589
Iteration 160/1000 | Loss: 0.00006592
Iteration 161/1000 | Loss: 0.00010284
Iteration 162/1000 | Loss: 0.00012961
Iteration 163/1000 | Loss: 0.00008046
Iteration 164/1000 | Loss: 0.00016460
Iteration 165/1000 | Loss: 0.00008518
Iteration 166/1000 | Loss: 0.00006482
Iteration 167/1000 | Loss: 0.00009350
Iteration 168/1000 | Loss: 0.00005960
Iteration 169/1000 | Loss: 0.00004615
Iteration 170/1000 | Loss: 0.00003793
Iteration 171/1000 | Loss: 0.00003038
Iteration 172/1000 | Loss: 0.00005729
Iteration 173/1000 | Loss: 0.00004340
Iteration 174/1000 | Loss: 0.00006322
Iteration 175/1000 | Loss: 0.00010712
Iteration 176/1000 | Loss: 0.00010365
Iteration 177/1000 | Loss: 0.00041124
Iteration 178/1000 | Loss: 0.00004968
Iteration 179/1000 | Loss: 0.00036825
Iteration 180/1000 | Loss: 0.00004520
Iteration 181/1000 | Loss: 0.00010071
Iteration 182/1000 | Loss: 0.00012794
Iteration 183/1000 | Loss: 0.00004048
Iteration 184/1000 | Loss: 0.00004199
Iteration 185/1000 | Loss: 0.00003723
Iteration 186/1000 | Loss: 0.00003673
Iteration 187/1000 | Loss: 0.00004354
Iteration 188/1000 | Loss: 0.00004344
Iteration 189/1000 | Loss: 0.00004545
Iteration 190/1000 | Loss: 0.00004265
Iteration 191/1000 | Loss: 0.00004709
Iteration 192/1000 | Loss: 0.00004182
Iteration 193/1000 | Loss: 0.00004665
Iteration 194/1000 | Loss: 0.00006006
Iteration 195/1000 | Loss: 0.00010212
Iteration 196/1000 | Loss: 0.00010592
Iteration 197/1000 | Loss: 0.00022474
Iteration 198/1000 | Loss: 0.00038320
Iteration 199/1000 | Loss: 0.00014702
Iteration 200/1000 | Loss: 0.00007100
Iteration 201/1000 | Loss: 0.00005755
Iteration 202/1000 | Loss: 0.00003753
Iteration 203/1000 | Loss: 0.00025430
Iteration 204/1000 | Loss: 0.00003375
Iteration 205/1000 | Loss: 0.00003123
Iteration 206/1000 | Loss: 0.00019986
Iteration 207/1000 | Loss: 0.00013274
Iteration 208/1000 | Loss: 0.00004351
Iteration 209/1000 | Loss: 0.00004883
Iteration 210/1000 | Loss: 0.00017938
Iteration 211/1000 | Loss: 0.00026145
Iteration 212/1000 | Loss: 0.00047174
Iteration 213/1000 | Loss: 0.00003047
Iteration 214/1000 | Loss: 0.00002827
Iteration 215/1000 | Loss: 0.00002753
Iteration 216/1000 | Loss: 0.00002694
Iteration 217/1000 | Loss: 0.00002643
Iteration 218/1000 | Loss: 0.00002588
Iteration 219/1000 | Loss: 0.00002532
Iteration 220/1000 | Loss: 0.00002496
Iteration 221/1000 | Loss: 0.00002486
Iteration 222/1000 | Loss: 0.00014433
Iteration 223/1000 | Loss: 0.00003151
Iteration 224/1000 | Loss: 0.00002946
Iteration 225/1000 | Loss: 0.00002759
Iteration 226/1000 | Loss: 0.00002685
Iteration 227/1000 | Loss: 0.00002647
Iteration 228/1000 | Loss: 0.00025807
Iteration 229/1000 | Loss: 0.00003836
Iteration 230/1000 | Loss: 0.00003457
Iteration 231/1000 | Loss: 0.00016768
Iteration 232/1000 | Loss: 0.00003777
Iteration 233/1000 | Loss: 0.00003501
Iteration 234/1000 | Loss: 0.00023547
Iteration 235/1000 | Loss: 0.00014941
Iteration 236/1000 | Loss: 0.00003744
Iteration 237/1000 | Loss: 0.00003378
Iteration 238/1000 | Loss: 0.00003579
Iteration 239/1000 | Loss: 0.00015241
Iteration 240/1000 | Loss: 0.00003717
Iteration 241/1000 | Loss: 0.00003446
Iteration 242/1000 | Loss: 0.00003221
Iteration 243/1000 | Loss: 0.00003356
Iteration 244/1000 | Loss: 0.00003094
Iteration 245/1000 | Loss: 0.00003000
Iteration 246/1000 | Loss: 0.00014154
Iteration 247/1000 | Loss: 0.00013755
Iteration 248/1000 | Loss: 0.00018099
Iteration 249/1000 | Loss: 0.00003103
Iteration 250/1000 | Loss: 0.00013136
Iteration 251/1000 | Loss: 0.00011165
Iteration 252/1000 | Loss: 0.00002717
Iteration 253/1000 | Loss: 0.00002531
Iteration 254/1000 | Loss: 0.00002413
Iteration 255/1000 | Loss: 0.00002347
Iteration 256/1000 | Loss: 0.00003940
Iteration 257/1000 | Loss: 0.00002258
Iteration 258/1000 | Loss: 0.00002233
Iteration 259/1000 | Loss: 0.00002225
Iteration 260/1000 | Loss: 0.00004246
Iteration 261/1000 | Loss: 0.00002277
Iteration 262/1000 | Loss: 0.00002207
Iteration 263/1000 | Loss: 0.00002205
Iteration 264/1000 | Loss: 0.00002204
Iteration 265/1000 | Loss: 0.00002204
Iteration 266/1000 | Loss: 0.00002203
Iteration 267/1000 | Loss: 0.00002203
Iteration 268/1000 | Loss: 0.00002202
Iteration 269/1000 | Loss: 0.00002202
Iteration 270/1000 | Loss: 0.00002202
Iteration 271/1000 | Loss: 0.00002201
Iteration 272/1000 | Loss: 0.00002201
Iteration 273/1000 | Loss: 0.00002201
Iteration 274/1000 | Loss: 0.00002201
Iteration 275/1000 | Loss: 0.00002200
Iteration 276/1000 | Loss: 0.00002200
Iteration 277/1000 | Loss: 0.00002200
Iteration 278/1000 | Loss: 0.00002200
Iteration 279/1000 | Loss: 0.00002199
Iteration 280/1000 | Loss: 0.00002199
Iteration 281/1000 | Loss: 0.00002199
Iteration 282/1000 | Loss: 0.00002198
Iteration 283/1000 | Loss: 0.00002198
Iteration 284/1000 | Loss: 0.00002198
Iteration 285/1000 | Loss: 0.00002198
Iteration 286/1000 | Loss: 0.00002198
Iteration 287/1000 | Loss: 0.00002197
Iteration 288/1000 | Loss: 0.00002195
Iteration 289/1000 | Loss: 0.00002194
Iteration 290/1000 | Loss: 0.00002194
Iteration 291/1000 | Loss: 0.00002194
Iteration 292/1000 | Loss: 0.00002193
Iteration 293/1000 | Loss: 0.00002193
Iteration 294/1000 | Loss: 0.00002192
Iteration 295/1000 | Loss: 0.00002192
Iteration 296/1000 | Loss: 0.00002192
Iteration 297/1000 | Loss: 0.00002191
Iteration 298/1000 | Loss: 0.00002191
Iteration 299/1000 | Loss: 0.00002191
Iteration 300/1000 | Loss: 0.00002191
Iteration 301/1000 | Loss: 0.00002191
Iteration 302/1000 | Loss: 0.00002191
Iteration 303/1000 | Loss: 0.00002191
Iteration 304/1000 | Loss: 0.00002191
Iteration 305/1000 | Loss: 0.00002191
Iteration 306/1000 | Loss: 0.00002190
Iteration 307/1000 | Loss: 0.00002190
Iteration 308/1000 | Loss: 0.00002189
Iteration 309/1000 | Loss: 0.00002189
Iteration 310/1000 | Loss: 0.00002189
Iteration 311/1000 | Loss: 0.00002189
Iteration 312/1000 | Loss: 0.00002189
Iteration 313/1000 | Loss: 0.00002189
Iteration 314/1000 | Loss: 0.00002189
Iteration 315/1000 | Loss: 0.00002189
Iteration 316/1000 | Loss: 0.00002189
Iteration 317/1000 | Loss: 0.00002189
Iteration 318/1000 | Loss: 0.00002188
Iteration 319/1000 | Loss: 0.00002188
Iteration 320/1000 | Loss: 0.00002188
Iteration 321/1000 | Loss: 0.00002188
Iteration 322/1000 | Loss: 0.00002188
Iteration 323/1000 | Loss: 0.00002188
Iteration 324/1000 | Loss: 0.00002188
Iteration 325/1000 | Loss: 0.00002188
Iteration 326/1000 | Loss: 0.00002188
Iteration 327/1000 | Loss: 0.00002188
Iteration 328/1000 | Loss: 0.00002188
Iteration 329/1000 | Loss: 0.00002187
Iteration 330/1000 | Loss: 0.00002187
Iteration 331/1000 | Loss: 0.00002187
Iteration 332/1000 | Loss: 0.00002187
Iteration 333/1000 | Loss: 0.00002187
Iteration 334/1000 | Loss: 0.00002187
Iteration 335/1000 | Loss: 0.00002187
Iteration 336/1000 | Loss: 0.00002187
Iteration 337/1000 | Loss: 0.00002187
Iteration 338/1000 | Loss: 0.00002187
Iteration 339/1000 | Loss: 0.00002187
Iteration 340/1000 | Loss: 0.00002187
Iteration 341/1000 | Loss: 0.00002187
Iteration 342/1000 | Loss: 0.00002187
Iteration 343/1000 | Loss: 0.00002187
Iteration 344/1000 | Loss: 0.00002187
Iteration 345/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [2.1868152543902397e-05, 2.1868152543902397e-05, 2.1868152543902397e-05, 2.1868152543902397e-05, 2.1868152543902397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1868152543902397e-05

Optimization complete. Final v2v error: 3.624354839324951 mm

Highest mean error: 15.027721405029297 mm for frame 125

Lowest mean error: 3.047802448272705 mm for frame 67

Saving results

Total time: 477.8899915218353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_2342/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_2342/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000636
Iteration 2/25 | Loss: 0.00169897
Iteration 3/25 | Loss: 0.00122632
Iteration 4/25 | Loss: 0.00115055
Iteration 5/25 | Loss: 0.00108115
Iteration 6/25 | Loss: 0.00105804
Iteration 7/25 | Loss: 0.00107785
Iteration 8/25 | Loss: 0.00106451
Iteration 9/25 | Loss: 0.00106776
Iteration 10/25 | Loss: 0.00104681
Iteration 11/25 | Loss: 0.00104197
Iteration 12/25 | Loss: 0.00104162
Iteration 13/25 | Loss: 0.00104146
Iteration 14/25 | Loss: 0.00104141
Iteration 15/25 | Loss: 0.00104141
Iteration 16/25 | Loss: 0.00104141
Iteration 17/25 | Loss: 0.00104141
Iteration 18/25 | Loss: 0.00104140
Iteration 19/25 | Loss: 0.00104140
Iteration 20/25 | Loss: 0.00104140
Iteration 21/25 | Loss: 0.00104140
Iteration 22/25 | Loss: 0.00104140
Iteration 23/25 | Loss: 0.00104140
Iteration 24/25 | Loss: 0.00104139
Iteration 25/25 | Loss: 0.00104139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.33057213
Iteration 2/25 | Loss: 0.00189792
Iteration 3/25 | Loss: 0.00189788
Iteration 4/25 | Loss: 0.00189788
Iteration 5/25 | Loss: 0.00189788
Iteration 6/25 | Loss: 0.00189788
Iteration 7/25 | Loss: 0.00189788
Iteration 8/25 | Loss: 0.00189788
Iteration 9/25 | Loss: 0.00189788
Iteration 10/25 | Loss: 0.00189788
Iteration 11/25 | Loss: 0.00189788
Iteration 12/25 | Loss: 0.00189788
Iteration 13/25 | Loss: 0.00189788
Iteration 14/25 | Loss: 0.00189788
Iteration 15/25 | Loss: 0.00189788
Iteration 16/25 | Loss: 0.00189788
Iteration 17/25 | Loss: 0.00189788
Iteration 18/25 | Loss: 0.00189788
Iteration 19/25 | Loss: 0.00189788
Iteration 20/25 | Loss: 0.00189788
Iteration 21/25 | Loss: 0.00189788
Iteration 22/25 | Loss: 0.00189788
Iteration 23/25 | Loss: 0.00189788
Iteration 24/25 | Loss: 0.00189788
Iteration 25/25 | Loss: 0.00189788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189788
Iteration 2/1000 | Loss: 0.00003833
Iteration 3/1000 | Loss: 0.00002916
Iteration 4/1000 | Loss: 0.00002590
Iteration 5/1000 | Loss: 0.00002446
Iteration 6/1000 | Loss: 0.00002366
Iteration 7/1000 | Loss: 0.00002284
Iteration 8/1000 | Loss: 0.00002233
Iteration 9/1000 | Loss: 0.00002194
Iteration 10/1000 | Loss: 0.00002172
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00002151
Iteration 13/1000 | Loss: 0.00002150
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002140
Iteration 17/1000 | Loss: 0.00002137
Iteration 18/1000 | Loss: 0.00002136
Iteration 19/1000 | Loss: 0.00002136
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00002135
Iteration 22/1000 | Loss: 0.00002135
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002134
Iteration 25/1000 | Loss: 0.00002133
Iteration 26/1000 | Loss: 0.00002133
Iteration 27/1000 | Loss: 0.00002133
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002133
Iteration 30/1000 | Loss: 0.00002133
Iteration 31/1000 | Loss: 0.00002133
Iteration 32/1000 | Loss: 0.00002133
Iteration 33/1000 | Loss: 0.00002133
Iteration 34/1000 | Loss: 0.00002132
Iteration 35/1000 | Loss: 0.00002132
Iteration 36/1000 | Loss: 0.00002132
Iteration 37/1000 | Loss: 0.00002132
Iteration 38/1000 | Loss: 0.00002132
Iteration 39/1000 | Loss: 0.00002131
Iteration 40/1000 | Loss: 0.00002131
Iteration 41/1000 | Loss: 0.00002131
Iteration 42/1000 | Loss: 0.00002131
Iteration 43/1000 | Loss: 0.00002131
Iteration 44/1000 | Loss: 0.00002131
Iteration 45/1000 | Loss: 0.00002131
Iteration 46/1000 | Loss: 0.00002131
Iteration 47/1000 | Loss: 0.00002130
Iteration 48/1000 | Loss: 0.00002130
Iteration 49/1000 | Loss: 0.00002130
Iteration 50/1000 | Loss: 0.00002130
Iteration 51/1000 | Loss: 0.00002130
Iteration 52/1000 | Loss: 0.00002130
Iteration 53/1000 | Loss: 0.00002130
Iteration 54/1000 | Loss: 0.00002130
Iteration 55/1000 | Loss: 0.00002129
Iteration 56/1000 | Loss: 0.00002129
Iteration 57/1000 | Loss: 0.00002129
Iteration 58/1000 | Loss: 0.00002129
Iteration 59/1000 | Loss: 0.00002129
Iteration 60/1000 | Loss: 0.00002129
Iteration 61/1000 | Loss: 0.00002129
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002128
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002128
Iteration 67/1000 | Loss: 0.00002128
Iteration 68/1000 | Loss: 0.00002128
Iteration 69/1000 | Loss: 0.00002127
Iteration 70/1000 | Loss: 0.00002127
Iteration 71/1000 | Loss: 0.00002127
Iteration 72/1000 | Loss: 0.00002127
Iteration 73/1000 | Loss: 0.00002127
Iteration 74/1000 | Loss: 0.00002127
Iteration 75/1000 | Loss: 0.00002127
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Iteration 81/1000 | Loss: 0.00002126
Iteration 82/1000 | Loss: 0.00002126
Iteration 83/1000 | Loss: 0.00002126
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002124
Iteration 91/1000 | Loss: 0.00002124
Iteration 92/1000 | Loss: 0.00002124
Iteration 93/1000 | Loss: 0.00002124
Iteration 94/1000 | Loss: 0.00002124
Iteration 95/1000 | Loss: 0.00002124
Iteration 96/1000 | Loss: 0.00002124
Iteration 97/1000 | Loss: 0.00002124
Iteration 98/1000 | Loss: 0.00002123
Iteration 99/1000 | Loss: 0.00002123
Iteration 100/1000 | Loss: 0.00002123
Iteration 101/1000 | Loss: 0.00002123
Iteration 102/1000 | Loss: 0.00002123
Iteration 103/1000 | Loss: 0.00002123
Iteration 104/1000 | Loss: 0.00002123
Iteration 105/1000 | Loss: 0.00002123
Iteration 106/1000 | Loss: 0.00002123
Iteration 107/1000 | Loss: 0.00002123
Iteration 108/1000 | Loss: 0.00002123
Iteration 109/1000 | Loss: 0.00002123
Iteration 110/1000 | Loss: 0.00002123
Iteration 111/1000 | Loss: 0.00002123
Iteration 112/1000 | Loss: 0.00002123
Iteration 113/1000 | Loss: 0.00002123
Iteration 114/1000 | Loss: 0.00002123
Iteration 115/1000 | Loss: 0.00002123
Iteration 116/1000 | Loss: 0.00002123
Iteration 117/1000 | Loss: 0.00002123
Iteration 118/1000 | Loss: 0.00002123
Iteration 119/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.1230867787380703e-05, 2.1230867787380703e-05, 2.1230867787380703e-05, 2.1230867787380703e-05, 2.1230867787380703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1230867787380703e-05

Optimization complete. Final v2v error: 3.781153917312622 mm

Highest mean error: 4.353273391723633 mm for frame 99

Lowest mean error: 3.3875560760498047 mm for frame 119

Saving results

Total time: 48.85153031349182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623370
Iteration 2/25 | Loss: 0.00144400
Iteration 3/25 | Loss: 0.00099806
Iteration 4/25 | Loss: 0.00096831
Iteration 5/25 | Loss: 0.00096543
Iteration 6/25 | Loss: 0.00096457
Iteration 7/25 | Loss: 0.00096452
Iteration 8/25 | Loss: 0.00096452
Iteration 9/25 | Loss: 0.00096452
Iteration 10/25 | Loss: 0.00096452
Iteration 11/25 | Loss: 0.00096452
Iteration 12/25 | Loss: 0.00096452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009645164827816188, 0.0009645164827816188, 0.0009645164827816188, 0.0009645164827816188, 0.0009645164827816188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009645164827816188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27141452
Iteration 2/25 | Loss: 0.00037096
Iteration 3/25 | Loss: 0.00037096
Iteration 4/25 | Loss: 0.00037096
Iteration 5/25 | Loss: 0.00037096
Iteration 6/25 | Loss: 0.00037096
Iteration 7/25 | Loss: 0.00037096
Iteration 8/25 | Loss: 0.00037096
Iteration 9/25 | Loss: 0.00037096
Iteration 10/25 | Loss: 0.00037096
Iteration 11/25 | Loss: 0.00037096
Iteration 12/25 | Loss: 0.00037096
Iteration 13/25 | Loss: 0.00037096
Iteration 14/25 | Loss: 0.00037096
Iteration 15/25 | Loss: 0.00037096
Iteration 16/25 | Loss: 0.00037096
Iteration 17/25 | Loss: 0.00037096
Iteration 18/25 | Loss: 0.00037096
Iteration 19/25 | Loss: 0.00037096
Iteration 20/25 | Loss: 0.00037096
Iteration 21/25 | Loss: 0.00037096
Iteration 22/25 | Loss: 0.00037096
Iteration 23/25 | Loss: 0.00037096
Iteration 24/25 | Loss: 0.00037096
Iteration 25/25 | Loss: 0.00037096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037096
Iteration 2/1000 | Loss: 0.00004205
Iteration 3/1000 | Loss: 0.00002324
Iteration 4/1000 | Loss: 0.00002033
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001793
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001652
Iteration 12/1000 | Loss: 0.00001648
Iteration 13/1000 | Loss: 0.00001645
Iteration 14/1000 | Loss: 0.00001638
Iteration 15/1000 | Loss: 0.00001629
Iteration 16/1000 | Loss: 0.00001622
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001611
Iteration 19/1000 | Loss: 0.00001608
Iteration 20/1000 | Loss: 0.00001607
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001607
Iteration 23/1000 | Loss: 0.00001606
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001604
Iteration 26/1000 | Loss: 0.00001604
Iteration 27/1000 | Loss: 0.00001603
Iteration 28/1000 | Loss: 0.00001603
Iteration 29/1000 | Loss: 0.00001603
Iteration 30/1000 | Loss: 0.00001603
Iteration 31/1000 | Loss: 0.00001603
Iteration 32/1000 | Loss: 0.00001603
Iteration 33/1000 | Loss: 0.00001603
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001601
Iteration 37/1000 | Loss: 0.00001601
Iteration 38/1000 | Loss: 0.00001600
Iteration 39/1000 | Loss: 0.00001600
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001599
Iteration 45/1000 | Loss: 0.00001598
Iteration 46/1000 | Loss: 0.00001598
Iteration 47/1000 | Loss: 0.00001598
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001597
Iteration 50/1000 | Loss: 0.00001596
Iteration 51/1000 | Loss: 0.00001596
Iteration 52/1000 | Loss: 0.00001596
Iteration 53/1000 | Loss: 0.00001596
Iteration 54/1000 | Loss: 0.00001596
Iteration 55/1000 | Loss: 0.00001596
Iteration 56/1000 | Loss: 0.00001596
Iteration 57/1000 | Loss: 0.00001596
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001595
Iteration 61/1000 | Loss: 0.00001595
Iteration 62/1000 | Loss: 0.00001595
Iteration 63/1000 | Loss: 0.00001594
Iteration 64/1000 | Loss: 0.00001594
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001593
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001593
Iteration 69/1000 | Loss: 0.00001592
Iteration 70/1000 | Loss: 0.00001592
Iteration 71/1000 | Loss: 0.00001592
Iteration 72/1000 | Loss: 0.00001591
Iteration 73/1000 | Loss: 0.00001591
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001590
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001590
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001588
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001585
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001584
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001584
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001583
Iteration 125/1000 | Loss: 0.00001583
Iteration 126/1000 | Loss: 0.00001583
Iteration 127/1000 | Loss: 0.00001583
Iteration 128/1000 | Loss: 0.00001583
Iteration 129/1000 | Loss: 0.00001583
Iteration 130/1000 | Loss: 0.00001583
Iteration 131/1000 | Loss: 0.00001582
Iteration 132/1000 | Loss: 0.00001582
Iteration 133/1000 | Loss: 0.00001582
Iteration 134/1000 | Loss: 0.00001582
Iteration 135/1000 | Loss: 0.00001582
Iteration 136/1000 | Loss: 0.00001582
Iteration 137/1000 | Loss: 0.00001582
Iteration 138/1000 | Loss: 0.00001582
Iteration 139/1000 | Loss: 0.00001582
Iteration 140/1000 | Loss: 0.00001581
Iteration 141/1000 | Loss: 0.00001581
Iteration 142/1000 | Loss: 0.00001581
Iteration 143/1000 | Loss: 0.00001581
Iteration 144/1000 | Loss: 0.00001581
Iteration 145/1000 | Loss: 0.00001580
Iteration 146/1000 | Loss: 0.00001580
Iteration 147/1000 | Loss: 0.00001580
Iteration 148/1000 | Loss: 0.00001580
Iteration 149/1000 | Loss: 0.00001580
Iteration 150/1000 | Loss: 0.00001579
Iteration 151/1000 | Loss: 0.00001579
Iteration 152/1000 | Loss: 0.00001579
Iteration 153/1000 | Loss: 0.00001578
Iteration 154/1000 | Loss: 0.00001578
Iteration 155/1000 | Loss: 0.00001578
Iteration 156/1000 | Loss: 0.00001578
Iteration 157/1000 | Loss: 0.00001578
Iteration 158/1000 | Loss: 0.00001577
Iteration 159/1000 | Loss: 0.00001577
Iteration 160/1000 | Loss: 0.00001577
Iteration 161/1000 | Loss: 0.00001577
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001577
Iteration 164/1000 | Loss: 0.00001576
Iteration 165/1000 | Loss: 0.00001576
Iteration 166/1000 | Loss: 0.00001576
Iteration 167/1000 | Loss: 0.00001576
Iteration 168/1000 | Loss: 0.00001576
Iteration 169/1000 | Loss: 0.00001576
Iteration 170/1000 | Loss: 0.00001576
Iteration 171/1000 | Loss: 0.00001575
Iteration 172/1000 | Loss: 0.00001575
Iteration 173/1000 | Loss: 0.00001575
Iteration 174/1000 | Loss: 0.00001575
Iteration 175/1000 | Loss: 0.00001575
Iteration 176/1000 | Loss: 0.00001575
Iteration 177/1000 | Loss: 0.00001575
Iteration 178/1000 | Loss: 0.00001575
Iteration 179/1000 | Loss: 0.00001575
Iteration 180/1000 | Loss: 0.00001575
Iteration 181/1000 | Loss: 0.00001575
Iteration 182/1000 | Loss: 0.00001575
Iteration 183/1000 | Loss: 0.00001575
Iteration 184/1000 | Loss: 0.00001575
Iteration 185/1000 | Loss: 0.00001575
Iteration 186/1000 | Loss: 0.00001575
Iteration 187/1000 | Loss: 0.00001575
Iteration 188/1000 | Loss: 0.00001575
Iteration 189/1000 | Loss: 0.00001575
Iteration 190/1000 | Loss: 0.00001575
Iteration 191/1000 | Loss: 0.00001575
Iteration 192/1000 | Loss: 0.00001575
Iteration 193/1000 | Loss: 0.00001575
Iteration 194/1000 | Loss: 0.00001575
Iteration 195/1000 | Loss: 0.00001575
Iteration 196/1000 | Loss: 0.00001575
Iteration 197/1000 | Loss: 0.00001575
Iteration 198/1000 | Loss: 0.00001575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.575096212036442e-05, 1.575096212036442e-05, 1.575096212036442e-05, 1.575096212036442e-05, 1.575096212036442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.575096212036442e-05

Optimization complete. Final v2v error: 3.088862895965576 mm

Highest mean error: 5.380364894866943 mm for frame 58

Lowest mean error: 2.5168306827545166 mm for frame 157

Saving results

Total time: 41.3928062915802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00631819
Iteration 2/25 | Loss: 0.00118190
Iteration 3/25 | Loss: 0.00092296
Iteration 4/25 | Loss: 0.00090355
Iteration 5/25 | Loss: 0.00090139
Iteration 6/25 | Loss: 0.00090126
Iteration 7/25 | Loss: 0.00090126
Iteration 8/25 | Loss: 0.00090126
Iteration 9/25 | Loss: 0.00090126
Iteration 10/25 | Loss: 0.00090126
Iteration 11/25 | Loss: 0.00090126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009012633236125112, 0.0009012633236125112, 0.0009012633236125112, 0.0009012633236125112, 0.0009012633236125112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009012633236125112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19850159
Iteration 2/25 | Loss: 0.00041689
Iteration 3/25 | Loss: 0.00041682
Iteration 4/25 | Loss: 0.00041682
Iteration 5/25 | Loss: 0.00041682
Iteration 6/25 | Loss: 0.00041682
Iteration 7/25 | Loss: 0.00041682
Iteration 8/25 | Loss: 0.00041681
Iteration 9/25 | Loss: 0.00041681
Iteration 10/25 | Loss: 0.00041681
Iteration 11/25 | Loss: 0.00041681
Iteration 12/25 | Loss: 0.00041681
Iteration 13/25 | Loss: 0.00041681
Iteration 14/25 | Loss: 0.00041681
Iteration 15/25 | Loss: 0.00041681
Iteration 16/25 | Loss: 0.00041681
Iteration 17/25 | Loss: 0.00041681
Iteration 18/25 | Loss: 0.00041681
Iteration 19/25 | Loss: 0.00041681
Iteration 20/25 | Loss: 0.00041681
Iteration 21/25 | Loss: 0.00041681
Iteration 22/25 | Loss: 0.00041681
Iteration 23/25 | Loss: 0.00041681
Iteration 24/25 | Loss: 0.00041681
Iteration 25/25 | Loss: 0.00041681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041681
Iteration 2/1000 | Loss: 0.00002506
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001390
Iteration 5/1000 | Loss: 0.00001305
Iteration 6/1000 | Loss: 0.00001242
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001181
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001172
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001167
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001161
Iteration 25/1000 | Loss: 0.00001160
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001158
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001157
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001157
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001156
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00001153
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001150
Iteration 74/1000 | Loss: 0.00001150
Iteration 75/1000 | Loss: 0.00001150
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001149
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001148
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001146
Iteration 94/1000 | Loss: 0.00001146
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001145
Iteration 98/1000 | Loss: 0.00001145
Iteration 99/1000 | Loss: 0.00001145
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001143
Iteration 104/1000 | Loss: 0.00001143
Iteration 105/1000 | Loss: 0.00001143
Iteration 106/1000 | Loss: 0.00001143
Iteration 107/1000 | Loss: 0.00001143
Iteration 108/1000 | Loss: 0.00001143
Iteration 109/1000 | Loss: 0.00001143
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001142
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001142
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001142
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001140
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001138
Iteration 144/1000 | Loss: 0.00001138
Iteration 145/1000 | Loss: 0.00001138
Iteration 146/1000 | Loss: 0.00001138
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001138
Iteration 150/1000 | Loss: 0.00001138
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001136
Iteration 161/1000 | Loss: 0.00001136
Iteration 162/1000 | Loss: 0.00001136
Iteration 163/1000 | Loss: 0.00001136
Iteration 164/1000 | Loss: 0.00001136
Iteration 165/1000 | Loss: 0.00001136
Iteration 166/1000 | Loss: 0.00001136
Iteration 167/1000 | Loss: 0.00001136
Iteration 168/1000 | Loss: 0.00001136
Iteration 169/1000 | Loss: 0.00001135
Iteration 170/1000 | Loss: 0.00001135
Iteration 171/1000 | Loss: 0.00001135
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001135
Iteration 175/1000 | Loss: 0.00001134
Iteration 176/1000 | Loss: 0.00001134
Iteration 177/1000 | Loss: 0.00001134
Iteration 178/1000 | Loss: 0.00001134
Iteration 179/1000 | Loss: 0.00001134
Iteration 180/1000 | Loss: 0.00001134
Iteration 181/1000 | Loss: 0.00001134
Iteration 182/1000 | Loss: 0.00001134
Iteration 183/1000 | Loss: 0.00001133
Iteration 184/1000 | Loss: 0.00001133
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001133
Iteration 187/1000 | Loss: 0.00001133
Iteration 188/1000 | Loss: 0.00001133
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001133
Iteration 194/1000 | Loss: 0.00001133
Iteration 195/1000 | Loss: 0.00001133
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001132
Iteration 207/1000 | Loss: 0.00001132
Iteration 208/1000 | Loss: 0.00001132
Iteration 209/1000 | Loss: 0.00001132
Iteration 210/1000 | Loss: 0.00001132
Iteration 211/1000 | Loss: 0.00001132
Iteration 212/1000 | Loss: 0.00001132
Iteration 213/1000 | Loss: 0.00001132
Iteration 214/1000 | Loss: 0.00001132
Iteration 215/1000 | Loss: 0.00001132
Iteration 216/1000 | Loss: 0.00001132
Iteration 217/1000 | Loss: 0.00001132
Iteration 218/1000 | Loss: 0.00001132
Iteration 219/1000 | Loss: 0.00001132
Iteration 220/1000 | Loss: 0.00001132
Iteration 221/1000 | Loss: 0.00001132
Iteration 222/1000 | Loss: 0.00001132
Iteration 223/1000 | Loss: 0.00001132
Iteration 224/1000 | Loss: 0.00001132
Iteration 225/1000 | Loss: 0.00001132
Iteration 226/1000 | Loss: 0.00001132
Iteration 227/1000 | Loss: 0.00001132
Iteration 228/1000 | Loss: 0.00001132
Iteration 229/1000 | Loss: 0.00001132
Iteration 230/1000 | Loss: 0.00001132
Iteration 231/1000 | Loss: 0.00001132
Iteration 232/1000 | Loss: 0.00001132
Iteration 233/1000 | Loss: 0.00001132
Iteration 234/1000 | Loss: 0.00001132
Iteration 235/1000 | Loss: 0.00001132
Iteration 236/1000 | Loss: 0.00001132
Iteration 237/1000 | Loss: 0.00001132
Iteration 238/1000 | Loss: 0.00001132
Iteration 239/1000 | Loss: 0.00001132
Iteration 240/1000 | Loss: 0.00001132
Iteration 241/1000 | Loss: 0.00001132
Iteration 242/1000 | Loss: 0.00001132
Iteration 243/1000 | Loss: 0.00001132
Iteration 244/1000 | Loss: 0.00001132
Iteration 245/1000 | Loss: 0.00001132
Iteration 246/1000 | Loss: 0.00001132
Iteration 247/1000 | Loss: 0.00001132
Iteration 248/1000 | Loss: 0.00001132
Iteration 249/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.1316732525301632e-05, 1.1316732525301632e-05, 1.1316732525301632e-05, 1.1316732525301632e-05, 1.1316732525301632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1316732525301632e-05

Optimization complete. Final v2v error: 2.785963296890259 mm

Highest mean error: 3.9351913928985596 mm for frame 33

Lowest mean error: 2.35905122756958 mm for frame 171

Saving results

Total time: 36.45142889022827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776345
Iteration 2/25 | Loss: 0.00164229
Iteration 3/25 | Loss: 0.00113942
Iteration 4/25 | Loss: 0.00101202
Iteration 5/25 | Loss: 0.00099408
Iteration 6/25 | Loss: 0.00099268
Iteration 7/25 | Loss: 0.00099268
Iteration 8/25 | Loss: 0.00099268
Iteration 9/25 | Loss: 0.00099268
Iteration 10/25 | Loss: 0.00099268
Iteration 11/25 | Loss: 0.00099268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009926757775247097, 0.0009926757775247097, 0.0009926757775247097, 0.0009926757775247097, 0.0009926757775247097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009926757775247097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81313801
Iteration 2/25 | Loss: 0.00032917
Iteration 3/25 | Loss: 0.00032913
Iteration 4/25 | Loss: 0.00032913
Iteration 5/25 | Loss: 0.00032913
Iteration 6/25 | Loss: 0.00032913
Iteration 7/25 | Loss: 0.00032913
Iteration 8/25 | Loss: 0.00032913
Iteration 9/25 | Loss: 0.00032913
Iteration 10/25 | Loss: 0.00032913
Iteration 11/25 | Loss: 0.00032913
Iteration 12/25 | Loss: 0.00032913
Iteration 13/25 | Loss: 0.00032913
Iteration 14/25 | Loss: 0.00032913
Iteration 15/25 | Loss: 0.00032913
Iteration 16/25 | Loss: 0.00032913
Iteration 17/25 | Loss: 0.00032913
Iteration 18/25 | Loss: 0.00032913
Iteration 19/25 | Loss: 0.00032913
Iteration 20/25 | Loss: 0.00032913
Iteration 21/25 | Loss: 0.00032913
Iteration 22/25 | Loss: 0.00032913
Iteration 23/25 | Loss: 0.00032913
Iteration 24/25 | Loss: 0.00032913
Iteration 25/25 | Loss: 0.00032913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032913
Iteration 2/1000 | Loss: 0.00002803
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001865
Iteration 5/1000 | Loss: 0.00001759
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001673
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001611
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001606
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001605
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00001603
Iteration 24/1000 | Loss: 0.00001602
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001602
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001602
Iteration 31/1000 | Loss: 0.00001602
Iteration 32/1000 | Loss: 0.00001602
Iteration 33/1000 | Loss: 0.00001602
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001602
Iteration 37/1000 | Loss: 0.00001600
Iteration 38/1000 | Loss: 0.00001600
Iteration 39/1000 | Loss: 0.00001600
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001600
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001599
Iteration 45/1000 | Loss: 0.00001599
Iteration 46/1000 | Loss: 0.00001599
Iteration 47/1000 | Loss: 0.00001599
Iteration 48/1000 | Loss: 0.00001598
Iteration 49/1000 | Loss: 0.00001598
Iteration 50/1000 | Loss: 0.00001598
Iteration 51/1000 | Loss: 0.00001598
Iteration 52/1000 | Loss: 0.00001598
Iteration 53/1000 | Loss: 0.00001598
Iteration 54/1000 | Loss: 0.00001598
Iteration 55/1000 | Loss: 0.00001598
Iteration 56/1000 | Loss: 0.00001598
Iteration 57/1000 | Loss: 0.00001598
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001597
Iteration 60/1000 | Loss: 0.00001597
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001597
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001596
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001595
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001594
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001594
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001594
Iteration 101/1000 | Loss: 0.00001594
Iteration 102/1000 | Loss: 0.00001594
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.5937384887365624e-05, 1.5937384887365624e-05, 1.5937384887365624e-05, 1.5937384887365624e-05, 1.5937384887365624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5937384887365624e-05

Optimization complete. Final v2v error: 3.321108818054199 mm

Highest mean error: 3.6073293685913086 mm for frame 6

Lowest mean error: 3.0963568687438965 mm for frame 114

Saving results

Total time: 33.44209933280945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044731
Iteration 2/25 | Loss: 0.00552760
Iteration 3/25 | Loss: 0.00253420
Iteration 4/25 | Loss: 0.00199467
Iteration 5/25 | Loss: 0.00182564
Iteration 6/25 | Loss: 0.00161444
Iteration 7/25 | Loss: 0.00159081
Iteration 8/25 | Loss: 0.00148825
Iteration 9/25 | Loss: 0.00143339
Iteration 10/25 | Loss: 0.00141497
Iteration 11/25 | Loss: 0.00141606
Iteration 12/25 | Loss: 0.00141157
Iteration 13/25 | Loss: 0.00137158
Iteration 14/25 | Loss: 0.00136486
Iteration 15/25 | Loss: 0.00135941
Iteration 16/25 | Loss: 0.00135178
Iteration 17/25 | Loss: 0.00135393
Iteration 18/25 | Loss: 0.00134005
Iteration 19/25 | Loss: 0.00132462
Iteration 20/25 | Loss: 0.00130802
Iteration 21/25 | Loss: 0.00130513
Iteration 22/25 | Loss: 0.00130108
Iteration 23/25 | Loss: 0.00130341
Iteration 24/25 | Loss: 0.00130661
Iteration 25/25 | Loss: 0.00129871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46407473
Iteration 2/25 | Loss: 0.00628003
Iteration 3/25 | Loss: 0.00406230
Iteration 4/25 | Loss: 0.00406229
Iteration 5/25 | Loss: 0.00406229
Iteration 6/25 | Loss: 0.00406229
Iteration 7/25 | Loss: 0.00406229
Iteration 8/25 | Loss: 0.00406229
Iteration 9/25 | Loss: 0.00406229
Iteration 10/25 | Loss: 0.00406229
Iteration 11/25 | Loss: 0.00406229
Iteration 12/25 | Loss: 0.00406229
Iteration 13/25 | Loss: 0.00406229
Iteration 14/25 | Loss: 0.00406229
Iteration 15/25 | Loss: 0.00406229
Iteration 16/25 | Loss: 0.00406229
Iteration 17/25 | Loss: 0.00406229
Iteration 18/25 | Loss: 0.00406229
Iteration 19/25 | Loss: 0.00406229
Iteration 20/25 | Loss: 0.00406229
Iteration 21/25 | Loss: 0.00406229
Iteration 22/25 | Loss: 0.00406229
Iteration 23/25 | Loss: 0.00406229
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004062288906425238, 0.004062288906425238, 0.004062288906425238, 0.004062288906425238, 0.004062288906425238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004062288906425238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00406229
Iteration 2/1000 | Loss: 0.00607212
Iteration 3/1000 | Loss: 0.00582287
Iteration 4/1000 | Loss: 0.00326895
Iteration 5/1000 | Loss: 0.00203779
Iteration 6/1000 | Loss: 0.00150641
Iteration 7/1000 | Loss: 0.00155571
Iteration 8/1000 | Loss: 0.00186484
Iteration 9/1000 | Loss: 0.00110604
Iteration 10/1000 | Loss: 0.00121859
Iteration 11/1000 | Loss: 0.00094363
Iteration 12/1000 | Loss: 0.00774928
Iteration 13/1000 | Loss: 0.00402871
Iteration 14/1000 | Loss: 0.00168147
Iteration 15/1000 | Loss: 0.00076714
Iteration 16/1000 | Loss: 0.00454004
Iteration 17/1000 | Loss: 0.00343002
Iteration 18/1000 | Loss: 0.01086184
Iteration 19/1000 | Loss: 0.00304332
Iteration 20/1000 | Loss: 0.00504491
Iteration 21/1000 | Loss: 0.00479903
Iteration 22/1000 | Loss: 0.00596944
Iteration 23/1000 | Loss: 0.00488820
Iteration 24/1000 | Loss: 0.00433598
Iteration 25/1000 | Loss: 0.00713218
Iteration 26/1000 | Loss: 0.00487228
Iteration 27/1000 | Loss: 0.00393592
Iteration 28/1000 | Loss: 0.00402102
Iteration 29/1000 | Loss: 0.00374134
Iteration 30/1000 | Loss: 0.00595871
Iteration 31/1000 | Loss: 0.00162422
Iteration 32/1000 | Loss: 0.00278298
Iteration 33/1000 | Loss: 0.00180412
Iteration 34/1000 | Loss: 0.00322855
Iteration 35/1000 | Loss: 0.00167922
Iteration 36/1000 | Loss: 0.00213661
Iteration 37/1000 | Loss: 0.00402201
Iteration 38/1000 | Loss: 0.00315989
Iteration 39/1000 | Loss: 0.00349225
Iteration 40/1000 | Loss: 0.00168762
Iteration 41/1000 | Loss: 0.00176243
Iteration 42/1000 | Loss: 0.00166873
Iteration 43/1000 | Loss: 0.00262434
Iteration 44/1000 | Loss: 0.00524407
Iteration 45/1000 | Loss: 0.00223992
Iteration 46/1000 | Loss: 0.00174992
Iteration 47/1000 | Loss: 0.00111305
Iteration 48/1000 | Loss: 0.00169995
Iteration 49/1000 | Loss: 0.00154584
Iteration 50/1000 | Loss: 0.00045296
Iteration 51/1000 | Loss: 0.00095426
Iteration 52/1000 | Loss: 0.00061760
Iteration 53/1000 | Loss: 0.00062999
Iteration 54/1000 | Loss: 0.00201375
Iteration 55/1000 | Loss: 0.00121373
Iteration 56/1000 | Loss: 0.00092572
Iteration 57/1000 | Loss: 0.00083289
Iteration 58/1000 | Loss: 0.00162205
Iteration 59/1000 | Loss: 0.00165174
Iteration 60/1000 | Loss: 0.00311919
Iteration 61/1000 | Loss: 0.00150609
Iteration 62/1000 | Loss: 0.00275840
Iteration 63/1000 | Loss: 0.00128280
Iteration 64/1000 | Loss: 0.00224427
Iteration 65/1000 | Loss: 0.00105986
Iteration 66/1000 | Loss: 0.00039708
Iteration 67/1000 | Loss: 0.00067946
Iteration 68/1000 | Loss: 0.00175907
Iteration 69/1000 | Loss: 0.00190891
Iteration 70/1000 | Loss: 0.00147288
Iteration 71/1000 | Loss: 0.00120898
Iteration 72/1000 | Loss: 0.00185842
Iteration 73/1000 | Loss: 0.00044987
Iteration 74/1000 | Loss: 0.00081381
Iteration 75/1000 | Loss: 0.00197594
Iteration 76/1000 | Loss: 0.00171432
Iteration 77/1000 | Loss: 0.00098443
Iteration 78/1000 | Loss: 0.00155994
Iteration 79/1000 | Loss: 0.00063036
Iteration 80/1000 | Loss: 0.00219671
Iteration 81/1000 | Loss: 0.00044094
Iteration 82/1000 | Loss: 0.00037820
Iteration 83/1000 | Loss: 0.00094718
Iteration 84/1000 | Loss: 0.00079462
Iteration 85/1000 | Loss: 0.00091185
Iteration 86/1000 | Loss: 0.00107274
Iteration 87/1000 | Loss: 0.00126960
Iteration 88/1000 | Loss: 0.00137056
Iteration 89/1000 | Loss: 0.00062567
Iteration 90/1000 | Loss: 0.00033688
Iteration 91/1000 | Loss: 0.00100553
Iteration 92/1000 | Loss: 0.00187376
Iteration 93/1000 | Loss: 0.00240364
Iteration 94/1000 | Loss: 0.00289377
Iteration 95/1000 | Loss: 0.00364594
Iteration 96/1000 | Loss: 0.00309839
Iteration 97/1000 | Loss: 0.00283973
Iteration 98/1000 | Loss: 0.00130025
Iteration 99/1000 | Loss: 0.00046334
Iteration 100/1000 | Loss: 0.00043893
Iteration 101/1000 | Loss: 0.00049124
Iteration 102/1000 | Loss: 0.00065132
Iteration 103/1000 | Loss: 0.00050041
Iteration 104/1000 | Loss: 0.00070285
Iteration 105/1000 | Loss: 0.00050243
Iteration 106/1000 | Loss: 0.00060937
Iteration 107/1000 | Loss: 0.00101763
Iteration 108/1000 | Loss: 0.00104142
Iteration 109/1000 | Loss: 0.00096045
Iteration 110/1000 | Loss: 0.00135645
Iteration 111/1000 | Loss: 0.00172650
Iteration 112/1000 | Loss: 0.00138494
Iteration 113/1000 | Loss: 0.00113708
Iteration 114/1000 | Loss: 0.00153003
Iteration 115/1000 | Loss: 0.00103331
Iteration 116/1000 | Loss: 0.00092865
Iteration 117/1000 | Loss: 0.00080778
Iteration 118/1000 | Loss: 0.00076935
Iteration 119/1000 | Loss: 0.00028802
Iteration 120/1000 | Loss: 0.00044114
Iteration 121/1000 | Loss: 0.00047489
Iteration 122/1000 | Loss: 0.00037571
Iteration 123/1000 | Loss: 0.00052952
Iteration 124/1000 | Loss: 0.00068991
Iteration 125/1000 | Loss: 0.00063054
Iteration 126/1000 | Loss: 0.00058351
Iteration 127/1000 | Loss: 0.00052014
Iteration 128/1000 | Loss: 0.00054016
Iteration 129/1000 | Loss: 0.00074822
Iteration 130/1000 | Loss: 0.00040729
Iteration 131/1000 | Loss: 0.00037796
Iteration 132/1000 | Loss: 0.00049471
Iteration 133/1000 | Loss: 0.00056268
Iteration 134/1000 | Loss: 0.00065380
Iteration 135/1000 | Loss: 0.00043183
Iteration 136/1000 | Loss: 0.00056474
Iteration 137/1000 | Loss: 0.00111040
Iteration 138/1000 | Loss: 0.00080248
Iteration 139/1000 | Loss: 0.00064599
Iteration 140/1000 | Loss: 0.00056585
Iteration 141/1000 | Loss: 0.00072991
Iteration 142/1000 | Loss: 0.00054463
Iteration 143/1000 | Loss: 0.00055679
Iteration 144/1000 | Loss: 0.00114007
Iteration 145/1000 | Loss: 0.00050321
Iteration 146/1000 | Loss: 0.00047935
Iteration 147/1000 | Loss: 0.00019272
Iteration 148/1000 | Loss: 0.00065366
Iteration 149/1000 | Loss: 0.00030015
Iteration 150/1000 | Loss: 0.00038597
Iteration 151/1000 | Loss: 0.00091810
Iteration 152/1000 | Loss: 0.00069299
Iteration 153/1000 | Loss: 0.00038586
Iteration 154/1000 | Loss: 0.00014970
Iteration 155/1000 | Loss: 0.00068324
Iteration 156/1000 | Loss: 0.00032021
Iteration 157/1000 | Loss: 0.00019832
Iteration 158/1000 | Loss: 0.00019395
Iteration 159/1000 | Loss: 0.00039406
Iteration 160/1000 | Loss: 0.00060247
Iteration 161/1000 | Loss: 0.00045492
Iteration 162/1000 | Loss: 0.00041080
Iteration 163/1000 | Loss: 0.00042162
Iteration 164/1000 | Loss: 0.00047222
Iteration 165/1000 | Loss: 0.00037514
Iteration 166/1000 | Loss: 0.00024669
Iteration 167/1000 | Loss: 0.00066490
Iteration 168/1000 | Loss: 0.00065672
Iteration 169/1000 | Loss: 0.00061112
Iteration 170/1000 | Loss: 0.00144417
Iteration 171/1000 | Loss: 0.00220266
Iteration 172/1000 | Loss: 0.00147655
Iteration 173/1000 | Loss: 0.00062586
Iteration 174/1000 | Loss: 0.00050473
Iteration 175/1000 | Loss: 0.00036832
Iteration 176/1000 | Loss: 0.00037535
Iteration 177/1000 | Loss: 0.00031964
Iteration 178/1000 | Loss: 0.00041911
Iteration 179/1000 | Loss: 0.00044402
Iteration 180/1000 | Loss: 0.00073684
Iteration 181/1000 | Loss: 0.00054509
Iteration 182/1000 | Loss: 0.00026603
Iteration 183/1000 | Loss: 0.00027849
Iteration 184/1000 | Loss: 0.00039544
Iteration 185/1000 | Loss: 0.00019905
Iteration 186/1000 | Loss: 0.00023803
Iteration 187/1000 | Loss: 0.00021272
Iteration 188/1000 | Loss: 0.00032291
Iteration 189/1000 | Loss: 0.00031268
Iteration 190/1000 | Loss: 0.00064650
Iteration 191/1000 | Loss: 0.00052337
Iteration 192/1000 | Loss: 0.00066969
Iteration 193/1000 | Loss: 0.00057024
Iteration 194/1000 | Loss: 0.00062719
Iteration 195/1000 | Loss: 0.00076523
Iteration 196/1000 | Loss: 0.00176425
Iteration 197/1000 | Loss: 0.00068581
Iteration 198/1000 | Loss: 0.00057224
Iteration 199/1000 | Loss: 0.00046402
Iteration 200/1000 | Loss: 0.00039934
Iteration 201/1000 | Loss: 0.00036513
Iteration 202/1000 | Loss: 0.00046968
Iteration 203/1000 | Loss: 0.00130883
Iteration 204/1000 | Loss: 0.00048183
Iteration 205/1000 | Loss: 0.00050430
Iteration 206/1000 | Loss: 0.00087011
Iteration 207/1000 | Loss: 0.00044715
Iteration 208/1000 | Loss: 0.00089460
Iteration 209/1000 | Loss: 0.00033016
Iteration 210/1000 | Loss: 0.00050989
Iteration 211/1000 | Loss: 0.00044737
Iteration 212/1000 | Loss: 0.00055244
Iteration 213/1000 | Loss: 0.00053800
Iteration 214/1000 | Loss: 0.00055531
Iteration 215/1000 | Loss: 0.00057552
Iteration 216/1000 | Loss: 0.00097286
Iteration 217/1000 | Loss: 0.00068789
Iteration 218/1000 | Loss: 0.00036101
Iteration 219/1000 | Loss: 0.00104184
Iteration 220/1000 | Loss: 0.00073141
Iteration 221/1000 | Loss: 0.00037363
Iteration 222/1000 | Loss: 0.00100418
Iteration 223/1000 | Loss: 0.00063365
Iteration 224/1000 | Loss: 0.00021887
Iteration 225/1000 | Loss: 0.00026667
Iteration 226/1000 | Loss: 0.00026553
Iteration 227/1000 | Loss: 0.00053319
Iteration 228/1000 | Loss: 0.00032259
Iteration 229/1000 | Loss: 0.00026416
Iteration 230/1000 | Loss: 0.00019386
Iteration 231/1000 | Loss: 0.00026863
Iteration 232/1000 | Loss: 0.00028467
Iteration 233/1000 | Loss: 0.00027221
Iteration 234/1000 | Loss: 0.00057689
Iteration 235/1000 | Loss: 0.00018097
Iteration 236/1000 | Loss: 0.00046451
Iteration 237/1000 | Loss: 0.00057750
Iteration 238/1000 | Loss: 0.00042653
Iteration 239/1000 | Loss: 0.00126819
Iteration 240/1000 | Loss: 0.00096490
Iteration 241/1000 | Loss: 0.00057891
Iteration 242/1000 | Loss: 0.00024427
Iteration 243/1000 | Loss: 0.00031219
Iteration 244/1000 | Loss: 0.00018825
Iteration 245/1000 | Loss: 0.00061640
Iteration 246/1000 | Loss: 0.00047405
Iteration 247/1000 | Loss: 0.00073865
Iteration 248/1000 | Loss: 0.00028434
Iteration 249/1000 | Loss: 0.00034830
Iteration 250/1000 | Loss: 0.00025915
Iteration 251/1000 | Loss: 0.00017055
Iteration 252/1000 | Loss: 0.00009272
Iteration 253/1000 | Loss: 0.00019664
Iteration 254/1000 | Loss: 0.00011602
Iteration 255/1000 | Loss: 0.00062460
Iteration 256/1000 | Loss: 0.00062452
Iteration 257/1000 | Loss: 0.00050751
Iteration 258/1000 | Loss: 0.00026688
Iteration 259/1000 | Loss: 0.00145762
Iteration 260/1000 | Loss: 0.00039650
Iteration 261/1000 | Loss: 0.00017245
Iteration 262/1000 | Loss: 0.00027889
Iteration 263/1000 | Loss: 0.00011556
Iteration 264/1000 | Loss: 0.00027109
Iteration 265/1000 | Loss: 0.00030523
Iteration 266/1000 | Loss: 0.00008841
Iteration 267/1000 | Loss: 0.00018494
Iteration 268/1000 | Loss: 0.00022044
Iteration 269/1000 | Loss: 0.00014962
Iteration 270/1000 | Loss: 0.00053663
Iteration 271/1000 | Loss: 0.00041253
Iteration 272/1000 | Loss: 0.00043136
Iteration 273/1000 | Loss: 0.00020547
Iteration 274/1000 | Loss: 0.00016395
Iteration 275/1000 | Loss: 0.00018012
Iteration 276/1000 | Loss: 0.00022537
Iteration 277/1000 | Loss: 0.00045600
Iteration 278/1000 | Loss: 0.00040188
Iteration 279/1000 | Loss: 0.00014555
Iteration 280/1000 | Loss: 0.00034956
Iteration 281/1000 | Loss: 0.00018500
Iteration 282/1000 | Loss: 0.00026365
Iteration 283/1000 | Loss: 0.00108539
Iteration 284/1000 | Loss: 0.00119055
Iteration 285/1000 | Loss: 0.00084564
Iteration 286/1000 | Loss: 0.00057750
Iteration 287/1000 | Loss: 0.00060495
Iteration 288/1000 | Loss: 0.00054233
Iteration 289/1000 | Loss: 0.00054795
Iteration 290/1000 | Loss: 0.00037787
Iteration 291/1000 | Loss: 0.00079955
Iteration 292/1000 | Loss: 0.00027430
Iteration 293/1000 | Loss: 0.00010164
Iteration 294/1000 | Loss: 0.00053280
Iteration 295/1000 | Loss: 0.00066936
Iteration 296/1000 | Loss: 0.00017260
Iteration 297/1000 | Loss: 0.00050067
Iteration 298/1000 | Loss: 0.00041677
Iteration 299/1000 | Loss: 0.00115939
Iteration 300/1000 | Loss: 0.00039417
Iteration 301/1000 | Loss: 0.00036873
Iteration 302/1000 | Loss: 0.00041039
Iteration 303/1000 | Loss: 0.00066908
Iteration 304/1000 | Loss: 0.00038751
Iteration 305/1000 | Loss: 0.00012873
Iteration 306/1000 | Loss: 0.00025720
Iteration 307/1000 | Loss: 0.00010542
Iteration 308/1000 | Loss: 0.00018927
Iteration 309/1000 | Loss: 0.00020689
Iteration 310/1000 | Loss: 0.00014229
Iteration 311/1000 | Loss: 0.00009335
Iteration 312/1000 | Loss: 0.00024003
Iteration 313/1000 | Loss: 0.00030190
Iteration 314/1000 | Loss: 0.00029479
Iteration 315/1000 | Loss: 0.00023539
Iteration 316/1000 | Loss: 0.00010238
Iteration 317/1000 | Loss: 0.00008930
Iteration 318/1000 | Loss: 0.00037235
Iteration 319/1000 | Loss: 0.00021927
Iteration 320/1000 | Loss: 0.00043123
Iteration 321/1000 | Loss: 0.00037759
Iteration 322/1000 | Loss: 0.00013033
Iteration 323/1000 | Loss: 0.00011165
Iteration 324/1000 | Loss: 0.00010381
Iteration 325/1000 | Loss: 0.00054749
Iteration 326/1000 | Loss: 0.00150693
Iteration 327/1000 | Loss: 0.00045846
Iteration 328/1000 | Loss: 0.00009398
Iteration 329/1000 | Loss: 0.00048455
Iteration 330/1000 | Loss: 0.00047191
Iteration 331/1000 | Loss: 0.00039100
Iteration 332/1000 | Loss: 0.00046407
Iteration 333/1000 | Loss: 0.00008537
Iteration 334/1000 | Loss: 0.00011169
Iteration 335/1000 | Loss: 0.00007499
Iteration 336/1000 | Loss: 0.00008367
Iteration 337/1000 | Loss: 0.00012452
Iteration 338/1000 | Loss: 0.00008878
Iteration 339/1000 | Loss: 0.00008503
Iteration 340/1000 | Loss: 0.00086277
Iteration 341/1000 | Loss: 0.00012702
Iteration 342/1000 | Loss: 0.00024010
Iteration 343/1000 | Loss: 0.00014246
Iteration 344/1000 | Loss: 0.00011507
Iteration 345/1000 | Loss: 0.00006960
Iteration 346/1000 | Loss: 0.00015619
Iteration 347/1000 | Loss: 0.00007476
Iteration 348/1000 | Loss: 0.00008339
Iteration 349/1000 | Loss: 0.00007547
Iteration 350/1000 | Loss: 0.00007321
Iteration 351/1000 | Loss: 0.00014279
Iteration 352/1000 | Loss: 0.00019172
Iteration 353/1000 | Loss: 0.00005613
Iteration 354/1000 | Loss: 0.00008217
Iteration 355/1000 | Loss: 0.00007293
Iteration 356/1000 | Loss: 0.00005332
Iteration 357/1000 | Loss: 0.00007136
Iteration 358/1000 | Loss: 0.00004835
Iteration 359/1000 | Loss: 0.00070711
Iteration 360/1000 | Loss: 0.00044575
Iteration 361/1000 | Loss: 0.00005177
Iteration 362/1000 | Loss: 0.00055523
Iteration 363/1000 | Loss: 0.00025089
Iteration 364/1000 | Loss: 0.00069037
Iteration 365/1000 | Loss: 0.00060821
Iteration 366/1000 | Loss: 0.00055986
Iteration 367/1000 | Loss: 0.00076002
Iteration 368/1000 | Loss: 0.00057704
Iteration 369/1000 | Loss: 0.00036866
Iteration 370/1000 | Loss: 0.00089320
Iteration 371/1000 | Loss: 0.00024865
Iteration 372/1000 | Loss: 0.00063158
Iteration 373/1000 | Loss: 0.00031387
Iteration 374/1000 | Loss: 0.00048362
Iteration 375/1000 | Loss: 0.00037349
Iteration 376/1000 | Loss: 0.00061762
Iteration 377/1000 | Loss: 0.00050120
Iteration 378/1000 | Loss: 0.00046689
Iteration 379/1000 | Loss: 0.00045168
Iteration 380/1000 | Loss: 0.00041802
Iteration 381/1000 | Loss: 0.00068193
Iteration 382/1000 | Loss: 0.00007415
Iteration 383/1000 | Loss: 0.00032555
Iteration 384/1000 | Loss: 0.00006715
Iteration 385/1000 | Loss: 0.00086487
Iteration 386/1000 | Loss: 0.00036465
Iteration 387/1000 | Loss: 0.00008347
Iteration 388/1000 | Loss: 0.00134037
Iteration 389/1000 | Loss: 0.00038398
Iteration 390/1000 | Loss: 0.00021824
Iteration 391/1000 | Loss: 0.00014437
Iteration 392/1000 | Loss: 0.00018831
Iteration 393/1000 | Loss: 0.00004974
Iteration 394/1000 | Loss: 0.00046779
Iteration 395/1000 | Loss: 0.00033272
Iteration 396/1000 | Loss: 0.00040350
Iteration 397/1000 | Loss: 0.00027341
Iteration 398/1000 | Loss: 0.00014537
Iteration 399/1000 | Loss: 0.00006041
Iteration 400/1000 | Loss: 0.00004378
Iteration 401/1000 | Loss: 0.00010167
Iteration 402/1000 | Loss: 0.00004484
Iteration 403/1000 | Loss: 0.00007660
Iteration 404/1000 | Loss: 0.00005235
Iteration 405/1000 | Loss: 0.00004207
Iteration 406/1000 | Loss: 0.00004748
Iteration 407/1000 | Loss: 0.00004151
Iteration 408/1000 | Loss: 0.00005841
Iteration 409/1000 | Loss: 0.00004162
Iteration 410/1000 | Loss: 0.00004117
Iteration 411/1000 | Loss: 0.00004105
Iteration 412/1000 | Loss: 0.00004104
Iteration 413/1000 | Loss: 0.00004103
Iteration 414/1000 | Loss: 0.00004103
Iteration 415/1000 | Loss: 0.00004096
Iteration 416/1000 | Loss: 0.00004096
Iteration 417/1000 | Loss: 0.00004095
Iteration 418/1000 | Loss: 0.00004092
Iteration 419/1000 | Loss: 0.00004092
Iteration 420/1000 | Loss: 0.00004091
Iteration 421/1000 | Loss: 0.00004091
Iteration 422/1000 | Loss: 0.00004088
Iteration 423/1000 | Loss: 0.00004080
Iteration 424/1000 | Loss: 0.00004079
Iteration 425/1000 | Loss: 0.00004079
Iteration 426/1000 | Loss: 0.00004076
Iteration 427/1000 | Loss: 0.00004073
Iteration 428/1000 | Loss: 0.00004073
Iteration 429/1000 | Loss: 0.00004073
Iteration 430/1000 | Loss: 0.00004072
Iteration 431/1000 | Loss: 0.00004072
Iteration 432/1000 | Loss: 0.00004072
Iteration 433/1000 | Loss: 0.00004072
Iteration 434/1000 | Loss: 0.00004071
Iteration 435/1000 | Loss: 0.00004070
Iteration 436/1000 | Loss: 0.00004070
Iteration 437/1000 | Loss: 0.00004070
Iteration 438/1000 | Loss: 0.00004069
Iteration 439/1000 | Loss: 0.00004069
Iteration 440/1000 | Loss: 0.00004069
Iteration 441/1000 | Loss: 0.00004068
Iteration 442/1000 | Loss: 0.00004068
Iteration 443/1000 | Loss: 0.00004068
Iteration 444/1000 | Loss: 0.00004068
Iteration 445/1000 | Loss: 0.00004068
Iteration 446/1000 | Loss: 0.00004067
Iteration 447/1000 | Loss: 0.00004067
Iteration 448/1000 | Loss: 0.00004067
Iteration 449/1000 | Loss: 0.00004067
Iteration 450/1000 | Loss: 0.00004065
Iteration 451/1000 | Loss: 0.00004065
Iteration 452/1000 | Loss: 0.00004065
Iteration 453/1000 | Loss: 0.00004065
Iteration 454/1000 | Loss: 0.00004065
Iteration 455/1000 | Loss: 0.00004065
Iteration 456/1000 | Loss: 0.00004064
Iteration 457/1000 | Loss: 0.00004064
Iteration 458/1000 | Loss: 0.00004064
Iteration 459/1000 | Loss: 0.00004063
Iteration 460/1000 | Loss: 0.00006966
Iteration 461/1000 | Loss: 0.00019508
Iteration 462/1000 | Loss: 0.00023738
Iteration 463/1000 | Loss: 0.00005042
Iteration 464/1000 | Loss: 0.00004310
Iteration 465/1000 | Loss: 0.00004137
Iteration 466/1000 | Loss: 0.00012802
Iteration 467/1000 | Loss: 0.00009457
Iteration 468/1000 | Loss: 0.00004071
Iteration 469/1000 | Loss: 0.00004059
Iteration 470/1000 | Loss: 0.00004058
Iteration 471/1000 | Loss: 0.00004058
Iteration 472/1000 | Loss: 0.00004057
Iteration 473/1000 | Loss: 0.00004057
Iteration 474/1000 | Loss: 0.00004056
Iteration 475/1000 | Loss: 0.00004056
Iteration 476/1000 | Loss: 0.00004056
Iteration 477/1000 | Loss: 0.00004056
Iteration 478/1000 | Loss: 0.00004055
Iteration 479/1000 | Loss: 0.00004055
Iteration 480/1000 | Loss: 0.00004055
Iteration 481/1000 | Loss: 0.00004054
Iteration 482/1000 | Loss: 0.00004054
Iteration 483/1000 | Loss: 0.00004054
Iteration 484/1000 | Loss: 0.00004054
Iteration 485/1000 | Loss: 0.00004054
Iteration 486/1000 | Loss: 0.00004053
Iteration 487/1000 | Loss: 0.00004053
Iteration 488/1000 | Loss: 0.00004053
Iteration 489/1000 | Loss: 0.00004053
Iteration 490/1000 | Loss: 0.00004053
Iteration 491/1000 | Loss: 0.00004053
Iteration 492/1000 | Loss: 0.00004053
Iteration 493/1000 | Loss: 0.00004052
Iteration 494/1000 | Loss: 0.00004052
Iteration 495/1000 | Loss: 0.00004052
Iteration 496/1000 | Loss: 0.00004051
Iteration 497/1000 | Loss: 0.00004051
Iteration 498/1000 | Loss: 0.00004051
Iteration 499/1000 | Loss: 0.00004051
Iteration 500/1000 | Loss: 0.00004051
Iteration 501/1000 | Loss: 0.00004051
Iteration 502/1000 | Loss: 0.00004051
Iteration 503/1000 | Loss: 0.00004051
Iteration 504/1000 | Loss: 0.00004051
Iteration 505/1000 | Loss: 0.00004051
Iteration 506/1000 | Loss: 0.00004051
Iteration 507/1000 | Loss: 0.00004051
Iteration 508/1000 | Loss: 0.00004051
Iteration 509/1000 | Loss: 0.00004051
Iteration 510/1000 | Loss: 0.00004050
Iteration 511/1000 | Loss: 0.00004050
Iteration 512/1000 | Loss: 0.00004050
Iteration 513/1000 | Loss: 0.00004050
Iteration 514/1000 | Loss: 0.00004050
Iteration 515/1000 | Loss: 0.00004050
Iteration 516/1000 | Loss: 0.00004050
Iteration 517/1000 | Loss: 0.00004050
Iteration 518/1000 | Loss: 0.00004050
Iteration 519/1000 | Loss: 0.00004050
Iteration 520/1000 | Loss: 0.00004050
Iteration 521/1000 | Loss: 0.00004049
Iteration 522/1000 | Loss: 0.00004049
Iteration 523/1000 | Loss: 0.00004049
Iteration 524/1000 | Loss: 0.00004049
Iteration 525/1000 | Loss: 0.00004049
Iteration 526/1000 | Loss: 0.00004049
Iteration 527/1000 | Loss: 0.00006812
Iteration 528/1000 | Loss: 0.00004057
Iteration 529/1000 | Loss: 0.00004052
Iteration 530/1000 | Loss: 0.00004049
Iteration 531/1000 | Loss: 0.00004049
Iteration 532/1000 | Loss: 0.00004048
Iteration 533/1000 | Loss: 0.00004048
Iteration 534/1000 | Loss: 0.00004048
Iteration 535/1000 | Loss: 0.00004048
Iteration 536/1000 | Loss: 0.00004048
Iteration 537/1000 | Loss: 0.00004048
Iteration 538/1000 | Loss: 0.00004048
Iteration 539/1000 | Loss: 0.00004048
Iteration 540/1000 | Loss: 0.00004047
Iteration 541/1000 | Loss: 0.00004047
Iteration 542/1000 | Loss: 0.00004046
Iteration 543/1000 | Loss: 0.00004046
Iteration 544/1000 | Loss: 0.00004046
Iteration 545/1000 | Loss: 0.00004046
Iteration 546/1000 | Loss: 0.00004046
Iteration 547/1000 | Loss: 0.00004046
Iteration 548/1000 | Loss: 0.00004046
Iteration 549/1000 | Loss: 0.00004046
Iteration 550/1000 | Loss: 0.00004046
Iteration 551/1000 | Loss: 0.00004046
Iteration 552/1000 | Loss: 0.00004046
Iteration 553/1000 | Loss: 0.00004045
Iteration 554/1000 | Loss: 0.00004045
Iteration 555/1000 | Loss: 0.00004045
Iteration 556/1000 | Loss: 0.00004045
Iteration 557/1000 | Loss: 0.00004045
Iteration 558/1000 | Loss: 0.00004045
Iteration 559/1000 | Loss: 0.00004045
Iteration 560/1000 | Loss: 0.00004045
Iteration 561/1000 | Loss: 0.00004045
Iteration 562/1000 | Loss: 0.00004045
Iteration 563/1000 | Loss: 0.00004044
Iteration 564/1000 | Loss: 0.00004044
Iteration 565/1000 | Loss: 0.00004044
Iteration 566/1000 | Loss: 0.00004044
Iteration 567/1000 | Loss: 0.00004044
Iteration 568/1000 | Loss: 0.00004044
Iteration 569/1000 | Loss: 0.00004044
Iteration 570/1000 | Loss: 0.00004044
Iteration 571/1000 | Loss: 0.00004044
Iteration 572/1000 | Loss: 0.00004044
Iteration 573/1000 | Loss: 0.00004044
Iteration 574/1000 | Loss: 0.00004044
Iteration 575/1000 | Loss: 0.00004044
Iteration 576/1000 | Loss: 0.00004044
Iteration 577/1000 | Loss: 0.00004044
Iteration 578/1000 | Loss: 0.00004044
Iteration 579/1000 | Loss: 0.00004044
Iteration 580/1000 | Loss: 0.00004044
Iteration 581/1000 | Loss: 0.00004044
Iteration 582/1000 | Loss: 0.00004044
Iteration 583/1000 | Loss: 0.00004044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 583. Stopping optimization.
Last 5 losses: [4.044122397317551e-05, 4.044122397317551e-05, 4.044122397317551e-05, 4.044122397317551e-05, 4.044122397317551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.044122397317551e-05

Optimization complete. Final v2v error: 3.7772209644317627 mm

Highest mean error: 12.749492645263672 mm for frame 87

Lowest mean error: 2.6870858669281006 mm for frame 216

Saving results

Total time: 743.99023270607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726113
Iteration 2/25 | Loss: 0.00136343
Iteration 3/25 | Loss: 0.00100157
Iteration 4/25 | Loss: 0.00096009
Iteration 5/25 | Loss: 0.00095873
Iteration 6/25 | Loss: 0.00096564
Iteration 7/25 | Loss: 0.00097062
Iteration 8/25 | Loss: 0.00094855
Iteration 9/25 | Loss: 0.00093477
Iteration 10/25 | Loss: 0.00093320
Iteration 11/25 | Loss: 0.00093297
Iteration 12/25 | Loss: 0.00093295
Iteration 13/25 | Loss: 0.00093295
Iteration 14/25 | Loss: 0.00093294
Iteration 15/25 | Loss: 0.00093294
Iteration 16/25 | Loss: 0.00093294
Iteration 17/25 | Loss: 0.00093294
Iteration 18/25 | Loss: 0.00093294
Iteration 19/25 | Loss: 0.00093294
Iteration 20/25 | Loss: 0.00093294
Iteration 21/25 | Loss: 0.00093294
Iteration 22/25 | Loss: 0.00093294
Iteration 23/25 | Loss: 0.00093294
Iteration 24/25 | Loss: 0.00093294
Iteration 25/25 | Loss: 0.00093294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.77646637
Iteration 2/25 | Loss: 0.00036471
Iteration 3/25 | Loss: 0.00036471
Iteration 4/25 | Loss: 0.00036471
Iteration 5/25 | Loss: 0.00036471
Iteration 6/25 | Loss: 0.00036471
Iteration 7/25 | Loss: 0.00036471
Iteration 8/25 | Loss: 0.00036471
Iteration 9/25 | Loss: 0.00036471
Iteration 10/25 | Loss: 0.00036471
Iteration 11/25 | Loss: 0.00036471
Iteration 12/25 | Loss: 0.00036471
Iteration 13/25 | Loss: 0.00036471
Iteration 14/25 | Loss: 0.00036471
Iteration 15/25 | Loss: 0.00036471
Iteration 16/25 | Loss: 0.00036471
Iteration 17/25 | Loss: 0.00036471
Iteration 18/25 | Loss: 0.00036471
Iteration 19/25 | Loss: 0.00036471
Iteration 20/25 | Loss: 0.00036471
Iteration 21/25 | Loss: 0.00036471
Iteration 22/25 | Loss: 0.00036471
Iteration 23/25 | Loss: 0.00036471
Iteration 24/25 | Loss: 0.00036471
Iteration 25/25 | Loss: 0.00036471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036471
Iteration 2/1000 | Loss: 0.00002307
Iteration 3/1000 | Loss: 0.00001371
Iteration 4/1000 | Loss: 0.00001154
Iteration 5/1000 | Loss: 0.00001071
Iteration 6/1000 | Loss: 0.00001045
Iteration 7/1000 | Loss: 0.00001022
Iteration 8/1000 | Loss: 0.00001022
Iteration 9/1000 | Loss: 0.00001015
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00001002
Iteration 12/1000 | Loss: 0.00001001
Iteration 13/1000 | Loss: 0.00001000
Iteration 14/1000 | Loss: 0.00001000
Iteration 15/1000 | Loss: 0.00001000
Iteration 16/1000 | Loss: 0.00000995
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000993
Iteration 20/1000 | Loss: 0.00000991
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000990
Iteration 25/1000 | Loss: 0.00000990
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000989
Iteration 28/1000 | Loss: 0.00000989
Iteration 29/1000 | Loss: 0.00000988
Iteration 30/1000 | Loss: 0.00000988
Iteration 31/1000 | Loss: 0.00000988
Iteration 32/1000 | Loss: 0.00000988
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000987
Iteration 35/1000 | Loss: 0.00000987
Iteration 36/1000 | Loss: 0.00000987
Iteration 37/1000 | Loss: 0.00000986
Iteration 38/1000 | Loss: 0.00000986
Iteration 39/1000 | Loss: 0.00000986
Iteration 40/1000 | Loss: 0.00000985
Iteration 41/1000 | Loss: 0.00000985
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000984
Iteration 44/1000 | Loss: 0.00000984
Iteration 45/1000 | Loss: 0.00000984
Iteration 46/1000 | Loss: 0.00000984
Iteration 47/1000 | Loss: 0.00000984
Iteration 48/1000 | Loss: 0.00000983
Iteration 49/1000 | Loss: 0.00000983
Iteration 50/1000 | Loss: 0.00000983
Iteration 51/1000 | Loss: 0.00000982
Iteration 52/1000 | Loss: 0.00000982
Iteration 53/1000 | Loss: 0.00000982
Iteration 54/1000 | Loss: 0.00000982
Iteration 55/1000 | Loss: 0.00000982
Iteration 56/1000 | Loss: 0.00000982
Iteration 57/1000 | Loss: 0.00000981
Iteration 58/1000 | Loss: 0.00000981
Iteration 59/1000 | Loss: 0.00000981
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000981
Iteration 63/1000 | Loss: 0.00000981
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000979
Iteration 71/1000 | Loss: 0.00000979
Iteration 72/1000 | Loss: 0.00000979
Iteration 73/1000 | Loss: 0.00000979
Iteration 74/1000 | Loss: 0.00000979
Iteration 75/1000 | Loss: 0.00000979
Iteration 76/1000 | Loss: 0.00000978
Iteration 77/1000 | Loss: 0.00000978
Iteration 78/1000 | Loss: 0.00000978
Iteration 79/1000 | Loss: 0.00000978
Iteration 80/1000 | Loss: 0.00000978
Iteration 81/1000 | Loss: 0.00000978
Iteration 82/1000 | Loss: 0.00000978
Iteration 83/1000 | Loss: 0.00000978
Iteration 84/1000 | Loss: 0.00000978
Iteration 85/1000 | Loss: 0.00000978
Iteration 86/1000 | Loss: 0.00000978
Iteration 87/1000 | Loss: 0.00000978
Iteration 88/1000 | Loss: 0.00000978
Iteration 89/1000 | Loss: 0.00000978
Iteration 90/1000 | Loss: 0.00000978
Iteration 91/1000 | Loss: 0.00000978
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000978
Iteration 111/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [9.78432763076853e-06, 9.78432763076853e-06, 9.78432763076853e-06, 9.78432763076853e-06, 9.78432763076853e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.78432763076853e-06

Optimization complete. Final v2v error: 2.6430935859680176 mm

Highest mean error: 3.476346969604492 mm for frame 0

Lowest mean error: 2.380354881286621 mm for frame 68

Saving results

Total time: 36.741761207580566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885831
Iteration 2/25 | Loss: 0.00132337
Iteration 3/25 | Loss: 0.00097200
Iteration 4/25 | Loss: 0.00090964
Iteration 5/25 | Loss: 0.00090449
Iteration 6/25 | Loss: 0.00090371
Iteration 7/25 | Loss: 0.00090371
Iteration 8/25 | Loss: 0.00090371
Iteration 9/25 | Loss: 0.00090371
Iteration 10/25 | Loss: 0.00090371
Iteration 11/25 | Loss: 0.00090371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009037075797095895, 0.0009037075797095895, 0.0009037075797095895, 0.0009037075797095895, 0.0009037075797095895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009037075797095895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46018386
Iteration 2/25 | Loss: 0.00035637
Iteration 3/25 | Loss: 0.00035637
Iteration 4/25 | Loss: 0.00035637
Iteration 5/25 | Loss: 0.00035637
Iteration 6/25 | Loss: 0.00035637
Iteration 7/25 | Loss: 0.00035637
Iteration 8/25 | Loss: 0.00035637
Iteration 9/25 | Loss: 0.00035637
Iteration 10/25 | Loss: 0.00035636
Iteration 11/25 | Loss: 0.00035636
Iteration 12/25 | Loss: 0.00035636
Iteration 13/25 | Loss: 0.00035636
Iteration 14/25 | Loss: 0.00035636
Iteration 15/25 | Loss: 0.00035636
Iteration 16/25 | Loss: 0.00035636
Iteration 17/25 | Loss: 0.00035636
Iteration 18/25 | Loss: 0.00035636
Iteration 19/25 | Loss: 0.00035636
Iteration 20/25 | Loss: 0.00035636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003563645004760474, 0.0003563645004760474, 0.0003563645004760474, 0.0003563645004760474, 0.0003563645004760474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003563645004760474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035636
Iteration 2/1000 | Loss: 0.00001896
Iteration 3/1000 | Loss: 0.00001377
Iteration 4/1000 | Loss: 0.00001226
Iteration 5/1000 | Loss: 0.00001145
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001069
Iteration 8/1000 | Loss: 0.00001042
Iteration 9/1000 | Loss: 0.00001037
Iteration 10/1000 | Loss: 0.00001036
Iteration 11/1000 | Loss: 0.00001027
Iteration 12/1000 | Loss: 0.00001011
Iteration 13/1000 | Loss: 0.00001005
Iteration 14/1000 | Loss: 0.00001003
Iteration 15/1000 | Loss: 0.00000999
Iteration 16/1000 | Loss: 0.00000998
Iteration 17/1000 | Loss: 0.00000998
Iteration 18/1000 | Loss: 0.00000992
Iteration 19/1000 | Loss: 0.00000991
Iteration 20/1000 | Loss: 0.00000990
Iteration 21/1000 | Loss: 0.00000990
Iteration 22/1000 | Loss: 0.00000989
Iteration 23/1000 | Loss: 0.00000989
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000988
Iteration 26/1000 | Loss: 0.00000987
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000986
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000986
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000984
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000982
Iteration 41/1000 | Loss: 0.00000982
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000981
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000979
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000978
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000978
Iteration 52/1000 | Loss: 0.00000978
Iteration 53/1000 | Loss: 0.00000978
Iteration 54/1000 | Loss: 0.00000978
Iteration 55/1000 | Loss: 0.00000978
Iteration 56/1000 | Loss: 0.00000978
Iteration 57/1000 | Loss: 0.00000978
Iteration 58/1000 | Loss: 0.00000978
Iteration 59/1000 | Loss: 0.00000977
Iteration 60/1000 | Loss: 0.00000977
Iteration 61/1000 | Loss: 0.00000976
Iteration 62/1000 | Loss: 0.00000976
Iteration 63/1000 | Loss: 0.00000976
Iteration 64/1000 | Loss: 0.00000975
Iteration 65/1000 | Loss: 0.00000975
Iteration 66/1000 | Loss: 0.00000975
Iteration 67/1000 | Loss: 0.00000975
Iteration 68/1000 | Loss: 0.00000975
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000973
Iteration 72/1000 | Loss: 0.00000973
Iteration 73/1000 | Loss: 0.00000973
Iteration 74/1000 | Loss: 0.00000973
Iteration 75/1000 | Loss: 0.00000973
Iteration 76/1000 | Loss: 0.00000973
Iteration 77/1000 | Loss: 0.00000973
Iteration 78/1000 | Loss: 0.00000973
Iteration 79/1000 | Loss: 0.00000973
Iteration 80/1000 | Loss: 0.00000973
Iteration 81/1000 | Loss: 0.00000973
Iteration 82/1000 | Loss: 0.00000973
Iteration 83/1000 | Loss: 0.00000973
Iteration 84/1000 | Loss: 0.00000973
Iteration 85/1000 | Loss: 0.00000973
Iteration 86/1000 | Loss: 0.00000973
Iteration 87/1000 | Loss: 0.00000972
Iteration 88/1000 | Loss: 0.00000972
Iteration 89/1000 | Loss: 0.00000972
Iteration 90/1000 | Loss: 0.00000972
Iteration 91/1000 | Loss: 0.00000971
Iteration 92/1000 | Loss: 0.00000971
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000971
Iteration 98/1000 | Loss: 0.00000971
Iteration 99/1000 | Loss: 0.00000971
Iteration 100/1000 | Loss: 0.00000971
Iteration 101/1000 | Loss: 0.00000971
Iteration 102/1000 | Loss: 0.00000971
Iteration 103/1000 | Loss: 0.00000970
Iteration 104/1000 | Loss: 0.00000970
Iteration 105/1000 | Loss: 0.00000970
Iteration 106/1000 | Loss: 0.00000970
Iteration 107/1000 | Loss: 0.00000970
Iteration 108/1000 | Loss: 0.00000970
Iteration 109/1000 | Loss: 0.00000970
Iteration 110/1000 | Loss: 0.00000970
Iteration 111/1000 | Loss: 0.00000970
Iteration 112/1000 | Loss: 0.00000970
Iteration 113/1000 | Loss: 0.00000970
Iteration 114/1000 | Loss: 0.00000970
Iteration 115/1000 | Loss: 0.00000970
Iteration 116/1000 | Loss: 0.00000970
Iteration 117/1000 | Loss: 0.00000970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [9.701434464659542e-06, 9.701434464659542e-06, 9.701434464659542e-06, 9.701434464659542e-06, 9.701434464659542e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.701434464659542e-06

Optimization complete. Final v2v error: 2.632481336593628 mm

Highest mean error: 3.1857516765594482 mm for frame 96

Lowest mean error: 2.2687532901763916 mm for frame 32

Saving results

Total time: 36.111305952072144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841888
Iteration 2/25 | Loss: 0.00181230
Iteration 3/25 | Loss: 0.00129576
Iteration 4/25 | Loss: 0.00137324
Iteration 5/25 | Loss: 0.00120337
Iteration 6/25 | Loss: 0.00108915
Iteration 7/25 | Loss: 0.00108408
Iteration 8/25 | Loss: 0.00106089
Iteration 9/25 | Loss: 0.00107069
Iteration 10/25 | Loss: 0.00104614
Iteration 11/25 | Loss: 0.00104546
Iteration 12/25 | Loss: 0.00104047
Iteration 13/25 | Loss: 0.00103564
Iteration 14/25 | Loss: 0.00103422
Iteration 15/25 | Loss: 0.00103402
Iteration 16/25 | Loss: 0.00103398
Iteration 17/25 | Loss: 0.00103397
Iteration 18/25 | Loss: 0.00103397
Iteration 19/25 | Loss: 0.00103397
Iteration 20/25 | Loss: 0.00103397
Iteration 21/25 | Loss: 0.00103397
Iteration 22/25 | Loss: 0.00103397
Iteration 23/25 | Loss: 0.00103397
Iteration 24/25 | Loss: 0.00103397
Iteration 25/25 | Loss: 0.00103397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47967124
Iteration 2/25 | Loss: 0.00066521
Iteration 3/25 | Loss: 0.00066519
Iteration 4/25 | Loss: 0.00066519
Iteration 5/25 | Loss: 0.00066519
Iteration 6/25 | Loss: 0.00066519
Iteration 7/25 | Loss: 0.00066519
Iteration 8/25 | Loss: 0.00066519
Iteration 9/25 | Loss: 0.00066519
Iteration 10/25 | Loss: 0.00066519
Iteration 11/25 | Loss: 0.00066519
Iteration 12/25 | Loss: 0.00066519
Iteration 13/25 | Loss: 0.00066519
Iteration 14/25 | Loss: 0.00066519
Iteration 15/25 | Loss: 0.00066519
Iteration 16/25 | Loss: 0.00066519
Iteration 17/25 | Loss: 0.00066519
Iteration 18/25 | Loss: 0.00066519
Iteration 19/25 | Loss: 0.00066519
Iteration 20/25 | Loss: 0.00066519
Iteration 21/25 | Loss: 0.00066519
Iteration 22/25 | Loss: 0.00066519
Iteration 23/25 | Loss: 0.00066519
Iteration 24/25 | Loss: 0.00066519
Iteration 25/25 | Loss: 0.00066519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066519
Iteration 2/1000 | Loss: 0.00008087
Iteration 3/1000 | Loss: 0.00061119
Iteration 4/1000 | Loss: 0.00006761
Iteration 5/1000 | Loss: 0.00004981
Iteration 6/1000 | Loss: 0.00060836
Iteration 7/1000 | Loss: 0.00004494
Iteration 8/1000 | Loss: 0.00047624
Iteration 9/1000 | Loss: 0.00004216
Iteration 10/1000 | Loss: 0.00003768
Iteration 11/1000 | Loss: 0.00003610
Iteration 12/1000 | Loss: 0.00003385
Iteration 13/1000 | Loss: 0.00015508
Iteration 14/1000 | Loss: 0.00018885
Iteration 15/1000 | Loss: 0.00015122
Iteration 16/1000 | Loss: 0.00018231
Iteration 17/1000 | Loss: 0.00004237
Iteration 18/1000 | Loss: 0.00003376
Iteration 19/1000 | Loss: 0.00003162
Iteration 20/1000 | Loss: 0.00002900
Iteration 21/1000 | Loss: 0.00002770
Iteration 22/1000 | Loss: 0.00002689
Iteration 23/1000 | Loss: 0.00002626
Iteration 24/1000 | Loss: 0.00002596
Iteration 25/1000 | Loss: 0.00002562
Iteration 26/1000 | Loss: 0.00037616
Iteration 27/1000 | Loss: 0.00018822
Iteration 28/1000 | Loss: 0.00002611
Iteration 29/1000 | Loss: 0.00029436
Iteration 30/1000 | Loss: 0.00006405
Iteration 31/1000 | Loss: 0.00002590
Iteration 32/1000 | Loss: 0.00024432
Iteration 33/1000 | Loss: 0.00008195
Iteration 34/1000 | Loss: 0.00019289
Iteration 35/1000 | Loss: 0.00003251
Iteration 36/1000 | Loss: 0.00002806
Iteration 37/1000 | Loss: 0.00002676
Iteration 38/1000 | Loss: 0.00002614
Iteration 39/1000 | Loss: 0.00002578
Iteration 40/1000 | Loss: 0.00002566
Iteration 41/1000 | Loss: 0.00002553
Iteration 42/1000 | Loss: 0.00032975
Iteration 43/1000 | Loss: 0.00017937
Iteration 44/1000 | Loss: 0.00016776
Iteration 45/1000 | Loss: 0.00022864
Iteration 46/1000 | Loss: 0.00002699
Iteration 47/1000 | Loss: 0.00021699
Iteration 48/1000 | Loss: 0.00013199
Iteration 49/1000 | Loss: 0.00002608
Iteration 50/1000 | Loss: 0.00021911
Iteration 51/1000 | Loss: 0.00017235
Iteration 52/1000 | Loss: 0.00018473
Iteration 53/1000 | Loss: 0.00015378
Iteration 54/1000 | Loss: 0.00009288
Iteration 55/1000 | Loss: 0.00021001
Iteration 56/1000 | Loss: 0.00018039
Iteration 57/1000 | Loss: 0.00012931
Iteration 58/1000 | Loss: 0.00003309
Iteration 59/1000 | Loss: 0.00003062
Iteration 60/1000 | Loss: 0.00002905
Iteration 61/1000 | Loss: 0.00002700
Iteration 62/1000 | Loss: 0.00002577
Iteration 63/1000 | Loss: 0.00002521
Iteration 64/1000 | Loss: 0.00002468
Iteration 65/1000 | Loss: 0.00002439
Iteration 66/1000 | Loss: 0.00002407
Iteration 67/1000 | Loss: 0.00002365
Iteration 68/1000 | Loss: 0.00002339
Iteration 69/1000 | Loss: 0.00002331
Iteration 70/1000 | Loss: 0.00002330
Iteration 71/1000 | Loss: 0.00002329
Iteration 72/1000 | Loss: 0.00002328
Iteration 73/1000 | Loss: 0.00002327
Iteration 74/1000 | Loss: 0.00002324
Iteration 75/1000 | Loss: 0.00002324
Iteration 76/1000 | Loss: 0.00002323
Iteration 77/1000 | Loss: 0.00002323
Iteration 78/1000 | Loss: 0.00002323
Iteration 79/1000 | Loss: 0.00002322
Iteration 80/1000 | Loss: 0.00002321
Iteration 81/1000 | Loss: 0.00002321
Iteration 82/1000 | Loss: 0.00002321
Iteration 83/1000 | Loss: 0.00002321
Iteration 84/1000 | Loss: 0.00002320
Iteration 85/1000 | Loss: 0.00002320
Iteration 86/1000 | Loss: 0.00002319
Iteration 87/1000 | Loss: 0.00002319
Iteration 88/1000 | Loss: 0.00002319
Iteration 89/1000 | Loss: 0.00002318
Iteration 90/1000 | Loss: 0.00002318
Iteration 91/1000 | Loss: 0.00002318
Iteration 92/1000 | Loss: 0.00002317
Iteration 93/1000 | Loss: 0.00002317
Iteration 94/1000 | Loss: 0.00002316
Iteration 95/1000 | Loss: 0.00002316
Iteration 96/1000 | Loss: 0.00002316
Iteration 97/1000 | Loss: 0.00002316
Iteration 98/1000 | Loss: 0.00002315
Iteration 99/1000 | Loss: 0.00002315
Iteration 100/1000 | Loss: 0.00002315
Iteration 101/1000 | Loss: 0.00002315
Iteration 102/1000 | Loss: 0.00002314
Iteration 103/1000 | Loss: 0.00002314
Iteration 104/1000 | Loss: 0.00002314
Iteration 105/1000 | Loss: 0.00002314
Iteration 106/1000 | Loss: 0.00002313
Iteration 107/1000 | Loss: 0.00002313
Iteration 108/1000 | Loss: 0.00002313
Iteration 109/1000 | Loss: 0.00002313
Iteration 110/1000 | Loss: 0.00002310
Iteration 111/1000 | Loss: 0.00002310
Iteration 112/1000 | Loss: 0.00002310
Iteration 113/1000 | Loss: 0.00002310
Iteration 114/1000 | Loss: 0.00002309
Iteration 115/1000 | Loss: 0.00002309
Iteration 116/1000 | Loss: 0.00002309
Iteration 117/1000 | Loss: 0.00002309
Iteration 118/1000 | Loss: 0.00002309
Iteration 119/1000 | Loss: 0.00002309
Iteration 120/1000 | Loss: 0.00002309
Iteration 121/1000 | Loss: 0.00002309
Iteration 122/1000 | Loss: 0.00002307
Iteration 123/1000 | Loss: 0.00002307
Iteration 124/1000 | Loss: 0.00002306
Iteration 125/1000 | Loss: 0.00002306
Iteration 126/1000 | Loss: 0.00002305
Iteration 127/1000 | Loss: 0.00002305
Iteration 128/1000 | Loss: 0.00002305
Iteration 129/1000 | Loss: 0.00002305
Iteration 130/1000 | Loss: 0.00002305
Iteration 131/1000 | Loss: 0.00002305
Iteration 132/1000 | Loss: 0.00002305
Iteration 133/1000 | Loss: 0.00002305
Iteration 134/1000 | Loss: 0.00002304
Iteration 135/1000 | Loss: 0.00002304
Iteration 136/1000 | Loss: 0.00002304
Iteration 137/1000 | Loss: 0.00002303
Iteration 138/1000 | Loss: 0.00002303
Iteration 139/1000 | Loss: 0.00002303
Iteration 140/1000 | Loss: 0.00002303
Iteration 141/1000 | Loss: 0.00002303
Iteration 142/1000 | Loss: 0.00002302
Iteration 143/1000 | Loss: 0.00002302
Iteration 144/1000 | Loss: 0.00002302
Iteration 145/1000 | Loss: 0.00002301
Iteration 146/1000 | Loss: 0.00002301
Iteration 147/1000 | Loss: 0.00002301
Iteration 148/1000 | Loss: 0.00002301
Iteration 149/1000 | Loss: 0.00002301
Iteration 150/1000 | Loss: 0.00002301
Iteration 151/1000 | Loss: 0.00002301
Iteration 152/1000 | Loss: 0.00002300
Iteration 153/1000 | Loss: 0.00002300
Iteration 154/1000 | Loss: 0.00002300
Iteration 155/1000 | Loss: 0.00002299
Iteration 156/1000 | Loss: 0.00002299
Iteration 157/1000 | Loss: 0.00002299
Iteration 158/1000 | Loss: 0.00002298
Iteration 159/1000 | Loss: 0.00002298
Iteration 160/1000 | Loss: 0.00002298
Iteration 161/1000 | Loss: 0.00002298
Iteration 162/1000 | Loss: 0.00002297
Iteration 163/1000 | Loss: 0.00002297
Iteration 164/1000 | Loss: 0.00002296
Iteration 165/1000 | Loss: 0.00002296
Iteration 166/1000 | Loss: 0.00002296
Iteration 167/1000 | Loss: 0.00002296
Iteration 168/1000 | Loss: 0.00002295
Iteration 169/1000 | Loss: 0.00002295
Iteration 170/1000 | Loss: 0.00002295
Iteration 171/1000 | Loss: 0.00002294
Iteration 172/1000 | Loss: 0.00002294
Iteration 173/1000 | Loss: 0.00002293
Iteration 174/1000 | Loss: 0.00002293
Iteration 175/1000 | Loss: 0.00002293
Iteration 176/1000 | Loss: 0.00002293
Iteration 177/1000 | Loss: 0.00002293
Iteration 178/1000 | Loss: 0.00002293
Iteration 179/1000 | Loss: 0.00002293
Iteration 180/1000 | Loss: 0.00002293
Iteration 181/1000 | Loss: 0.00002293
Iteration 182/1000 | Loss: 0.00002292
Iteration 183/1000 | Loss: 0.00002292
Iteration 184/1000 | Loss: 0.00002292
Iteration 185/1000 | Loss: 0.00002292
Iteration 186/1000 | Loss: 0.00002292
Iteration 187/1000 | Loss: 0.00002291
Iteration 188/1000 | Loss: 0.00002291
Iteration 189/1000 | Loss: 0.00002291
Iteration 190/1000 | Loss: 0.00002290
Iteration 191/1000 | Loss: 0.00002290
Iteration 192/1000 | Loss: 0.00002290
Iteration 193/1000 | Loss: 0.00002290
Iteration 194/1000 | Loss: 0.00002289
Iteration 195/1000 | Loss: 0.00002289
Iteration 196/1000 | Loss: 0.00002289
Iteration 197/1000 | Loss: 0.00002289
Iteration 198/1000 | Loss: 0.00002289
Iteration 199/1000 | Loss: 0.00002288
Iteration 200/1000 | Loss: 0.00002288
Iteration 201/1000 | Loss: 0.00002288
Iteration 202/1000 | Loss: 0.00002288
Iteration 203/1000 | Loss: 0.00002288
Iteration 204/1000 | Loss: 0.00002288
Iteration 205/1000 | Loss: 0.00002288
Iteration 206/1000 | Loss: 0.00002288
Iteration 207/1000 | Loss: 0.00002288
Iteration 208/1000 | Loss: 0.00002288
Iteration 209/1000 | Loss: 0.00002288
Iteration 210/1000 | Loss: 0.00002287
Iteration 211/1000 | Loss: 0.00002287
Iteration 212/1000 | Loss: 0.00002287
Iteration 213/1000 | Loss: 0.00002287
Iteration 214/1000 | Loss: 0.00002287
Iteration 215/1000 | Loss: 0.00002286
Iteration 216/1000 | Loss: 0.00002286
Iteration 217/1000 | Loss: 0.00002286
Iteration 218/1000 | Loss: 0.00002286
Iteration 219/1000 | Loss: 0.00002286
Iteration 220/1000 | Loss: 0.00002286
Iteration 221/1000 | Loss: 0.00002286
Iteration 222/1000 | Loss: 0.00002285
Iteration 223/1000 | Loss: 0.00002285
Iteration 224/1000 | Loss: 0.00002285
Iteration 225/1000 | Loss: 0.00002285
Iteration 226/1000 | Loss: 0.00002285
Iteration 227/1000 | Loss: 0.00002285
Iteration 228/1000 | Loss: 0.00002285
Iteration 229/1000 | Loss: 0.00002285
Iteration 230/1000 | Loss: 0.00002285
Iteration 231/1000 | Loss: 0.00002285
Iteration 232/1000 | Loss: 0.00002285
Iteration 233/1000 | Loss: 0.00002285
Iteration 234/1000 | Loss: 0.00002284
Iteration 235/1000 | Loss: 0.00002284
Iteration 236/1000 | Loss: 0.00002284
Iteration 237/1000 | Loss: 0.00002284
Iteration 238/1000 | Loss: 0.00002284
Iteration 239/1000 | Loss: 0.00002284
Iteration 240/1000 | Loss: 0.00002284
Iteration 241/1000 | Loss: 0.00002284
Iteration 242/1000 | Loss: 0.00002284
Iteration 243/1000 | Loss: 0.00002283
Iteration 244/1000 | Loss: 0.00002283
Iteration 245/1000 | Loss: 0.00002283
Iteration 246/1000 | Loss: 0.00002283
Iteration 247/1000 | Loss: 0.00002283
Iteration 248/1000 | Loss: 0.00002283
Iteration 249/1000 | Loss: 0.00002283
Iteration 250/1000 | Loss: 0.00002282
Iteration 251/1000 | Loss: 0.00002282
Iteration 252/1000 | Loss: 0.00002282
Iteration 253/1000 | Loss: 0.00002282
Iteration 254/1000 | Loss: 0.00002282
Iteration 255/1000 | Loss: 0.00002282
Iteration 256/1000 | Loss: 0.00002281
Iteration 257/1000 | Loss: 0.00002281
Iteration 258/1000 | Loss: 0.00002281
Iteration 259/1000 | Loss: 0.00002281
Iteration 260/1000 | Loss: 0.00002281
Iteration 261/1000 | Loss: 0.00002281
Iteration 262/1000 | Loss: 0.00002281
Iteration 263/1000 | Loss: 0.00002281
Iteration 264/1000 | Loss: 0.00002281
Iteration 265/1000 | Loss: 0.00002281
Iteration 266/1000 | Loss: 0.00002280
Iteration 267/1000 | Loss: 0.00002280
Iteration 268/1000 | Loss: 0.00002280
Iteration 269/1000 | Loss: 0.00002280
Iteration 270/1000 | Loss: 0.00002280
Iteration 271/1000 | Loss: 0.00002280
Iteration 272/1000 | Loss: 0.00002280
Iteration 273/1000 | Loss: 0.00002280
Iteration 274/1000 | Loss: 0.00002280
Iteration 275/1000 | Loss: 0.00002280
Iteration 276/1000 | Loss: 0.00002279
Iteration 277/1000 | Loss: 0.00002279
Iteration 278/1000 | Loss: 0.00002279
Iteration 279/1000 | Loss: 0.00002279
Iteration 280/1000 | Loss: 0.00002279
Iteration 281/1000 | Loss: 0.00002279
Iteration 282/1000 | Loss: 0.00002279
Iteration 283/1000 | Loss: 0.00002278
Iteration 284/1000 | Loss: 0.00002278
Iteration 285/1000 | Loss: 0.00002278
Iteration 286/1000 | Loss: 0.00002278
Iteration 287/1000 | Loss: 0.00002278
Iteration 288/1000 | Loss: 0.00002278
Iteration 289/1000 | Loss: 0.00002277
Iteration 290/1000 | Loss: 0.00002277
Iteration 291/1000 | Loss: 0.00002277
Iteration 292/1000 | Loss: 0.00002277
Iteration 293/1000 | Loss: 0.00002276
Iteration 294/1000 | Loss: 0.00002276
Iteration 295/1000 | Loss: 0.00002276
Iteration 296/1000 | Loss: 0.00002276
Iteration 297/1000 | Loss: 0.00002275
Iteration 298/1000 | Loss: 0.00002275
Iteration 299/1000 | Loss: 0.00002275
Iteration 300/1000 | Loss: 0.00002275
Iteration 301/1000 | Loss: 0.00002275
Iteration 302/1000 | Loss: 0.00002274
Iteration 303/1000 | Loss: 0.00002274
Iteration 304/1000 | Loss: 0.00002274
Iteration 305/1000 | Loss: 0.00002274
Iteration 306/1000 | Loss: 0.00002273
Iteration 307/1000 | Loss: 0.00002273
Iteration 308/1000 | Loss: 0.00002273
Iteration 309/1000 | Loss: 0.00002273
Iteration 310/1000 | Loss: 0.00002272
Iteration 311/1000 | Loss: 0.00002272
Iteration 312/1000 | Loss: 0.00002272
Iteration 313/1000 | Loss: 0.00002271
Iteration 314/1000 | Loss: 0.00002271
Iteration 315/1000 | Loss: 0.00002271
Iteration 316/1000 | Loss: 0.00002270
Iteration 317/1000 | Loss: 0.00002270
Iteration 318/1000 | Loss: 0.00002270
Iteration 319/1000 | Loss: 0.00002270
Iteration 320/1000 | Loss: 0.00002270
Iteration 321/1000 | Loss: 0.00002269
Iteration 322/1000 | Loss: 0.00002268
Iteration 323/1000 | Loss: 0.00002267
Iteration 324/1000 | Loss: 0.00002267
Iteration 325/1000 | Loss: 0.00002266
Iteration 326/1000 | Loss: 0.00002266
Iteration 327/1000 | Loss: 0.00002264
Iteration 328/1000 | Loss: 0.00002264
Iteration 329/1000 | Loss: 0.00002264
Iteration 330/1000 | Loss: 0.00002263
Iteration 331/1000 | Loss: 0.00002263
Iteration 332/1000 | Loss: 0.00002262
Iteration 333/1000 | Loss: 0.00002262
Iteration 334/1000 | Loss: 0.00002262
Iteration 335/1000 | Loss: 0.00002262
Iteration 336/1000 | Loss: 0.00002261
Iteration 337/1000 | Loss: 0.00002261
Iteration 338/1000 | Loss: 0.00002261
Iteration 339/1000 | Loss: 0.00002261
Iteration 340/1000 | Loss: 0.00002261
Iteration 341/1000 | Loss: 0.00002261
Iteration 342/1000 | Loss: 0.00002261
Iteration 343/1000 | Loss: 0.00002261
Iteration 344/1000 | Loss: 0.00002261
Iteration 345/1000 | Loss: 0.00002261
Iteration 346/1000 | Loss: 0.00002260
Iteration 347/1000 | Loss: 0.00002260
Iteration 348/1000 | Loss: 0.00002260
Iteration 349/1000 | Loss: 0.00002260
Iteration 350/1000 | Loss: 0.00002260
Iteration 351/1000 | Loss: 0.00002260
Iteration 352/1000 | Loss: 0.00002260
Iteration 353/1000 | Loss: 0.00002260
Iteration 354/1000 | Loss: 0.00002260
Iteration 355/1000 | Loss: 0.00002259
Iteration 356/1000 | Loss: 0.00002259
Iteration 357/1000 | Loss: 0.00002259
Iteration 358/1000 | Loss: 0.00002259
Iteration 359/1000 | Loss: 0.00002258
Iteration 360/1000 | Loss: 0.00002258
Iteration 361/1000 | Loss: 0.00002258
Iteration 362/1000 | Loss: 0.00002258
Iteration 363/1000 | Loss: 0.00002257
Iteration 364/1000 | Loss: 0.00002257
Iteration 365/1000 | Loss: 0.00002257
Iteration 366/1000 | Loss: 0.00002256
Iteration 367/1000 | Loss: 0.00002256
Iteration 368/1000 | Loss: 0.00002256
Iteration 369/1000 | Loss: 0.00002256
Iteration 370/1000 | Loss: 0.00002256
Iteration 371/1000 | Loss: 0.00002256
Iteration 372/1000 | Loss: 0.00002255
Iteration 373/1000 | Loss: 0.00002255
Iteration 374/1000 | Loss: 0.00002255
Iteration 375/1000 | Loss: 0.00002254
Iteration 376/1000 | Loss: 0.00002254
Iteration 377/1000 | Loss: 0.00002254
Iteration 378/1000 | Loss: 0.00002254
Iteration 379/1000 | Loss: 0.00002254
Iteration 380/1000 | Loss: 0.00002254
Iteration 381/1000 | Loss: 0.00002254
Iteration 382/1000 | Loss: 0.00002254
Iteration 383/1000 | Loss: 0.00002254
Iteration 384/1000 | Loss: 0.00002254
Iteration 385/1000 | Loss: 0.00002253
Iteration 386/1000 | Loss: 0.00002253
Iteration 387/1000 | Loss: 0.00002253
Iteration 388/1000 | Loss: 0.00002253
Iteration 389/1000 | Loss: 0.00002253
Iteration 390/1000 | Loss: 0.00002253
Iteration 391/1000 | Loss: 0.00002253
Iteration 392/1000 | Loss: 0.00002253
Iteration 393/1000 | Loss: 0.00002253
Iteration 394/1000 | Loss: 0.00002252
Iteration 395/1000 | Loss: 0.00002252
Iteration 396/1000 | Loss: 0.00002252
Iteration 397/1000 | Loss: 0.00002252
Iteration 398/1000 | Loss: 0.00002252
Iteration 399/1000 | Loss: 0.00002252
Iteration 400/1000 | Loss: 0.00002252
Iteration 401/1000 | Loss: 0.00002251
Iteration 402/1000 | Loss: 0.00002251
Iteration 403/1000 | Loss: 0.00002251
Iteration 404/1000 | Loss: 0.00002251
Iteration 405/1000 | Loss: 0.00002251
Iteration 406/1000 | Loss: 0.00002251
Iteration 407/1000 | Loss: 0.00002251
Iteration 408/1000 | Loss: 0.00002251
Iteration 409/1000 | Loss: 0.00002251
Iteration 410/1000 | Loss: 0.00002251
Iteration 411/1000 | Loss: 0.00002251
Iteration 412/1000 | Loss: 0.00002251
Iteration 413/1000 | Loss: 0.00002251
Iteration 414/1000 | Loss: 0.00002251
Iteration 415/1000 | Loss: 0.00002250
Iteration 416/1000 | Loss: 0.00002250
Iteration 417/1000 | Loss: 0.00002250
Iteration 418/1000 | Loss: 0.00002250
Iteration 419/1000 | Loss: 0.00002250
Iteration 420/1000 | Loss: 0.00002250
Iteration 421/1000 | Loss: 0.00002250
Iteration 422/1000 | Loss: 0.00002250
Iteration 423/1000 | Loss: 0.00002250
Iteration 424/1000 | Loss: 0.00002250
Iteration 425/1000 | Loss: 0.00002250
Iteration 426/1000 | Loss: 0.00002250
Iteration 427/1000 | Loss: 0.00002250
Iteration 428/1000 | Loss: 0.00002250
Iteration 429/1000 | Loss: 0.00002250
Iteration 430/1000 | Loss: 0.00002250
Iteration 431/1000 | Loss: 0.00002250
Iteration 432/1000 | Loss: 0.00002250
Iteration 433/1000 | Loss: 0.00002250
Iteration 434/1000 | Loss: 0.00002250
Iteration 435/1000 | Loss: 0.00002250
Iteration 436/1000 | Loss: 0.00002250
Iteration 437/1000 | Loss: 0.00002249
Iteration 438/1000 | Loss: 0.00002249
Iteration 439/1000 | Loss: 0.00002249
Iteration 440/1000 | Loss: 0.00002249
Iteration 441/1000 | Loss: 0.00002249
Iteration 442/1000 | Loss: 0.00002249
Iteration 443/1000 | Loss: 0.00002249
Iteration 444/1000 | Loss: 0.00002249
Iteration 445/1000 | Loss: 0.00002249
Iteration 446/1000 | Loss: 0.00002249
Iteration 447/1000 | Loss: 0.00002249
Iteration 448/1000 | Loss: 0.00002249
Iteration 449/1000 | Loss: 0.00002249
Iteration 450/1000 | Loss: 0.00002249
Iteration 451/1000 | Loss: 0.00002249
Iteration 452/1000 | Loss: 0.00002249
Iteration 453/1000 | Loss: 0.00002249
Iteration 454/1000 | Loss: 0.00002248
Iteration 455/1000 | Loss: 0.00002248
Iteration 456/1000 | Loss: 0.00002248
Iteration 457/1000 | Loss: 0.00002248
Iteration 458/1000 | Loss: 0.00002248
Iteration 459/1000 | Loss: 0.00002248
Iteration 460/1000 | Loss: 0.00002248
Iteration 461/1000 | Loss: 0.00002248
Iteration 462/1000 | Loss: 0.00002248
Iteration 463/1000 | Loss: 0.00002248
Iteration 464/1000 | Loss: 0.00002248
Iteration 465/1000 | Loss: 0.00002248
Iteration 466/1000 | Loss: 0.00002248
Iteration 467/1000 | Loss: 0.00002248
Iteration 468/1000 | Loss: 0.00002248
Iteration 469/1000 | Loss: 0.00002248
Iteration 470/1000 | Loss: 0.00002248
Iteration 471/1000 | Loss: 0.00002248
Iteration 472/1000 | Loss: 0.00002248
Iteration 473/1000 | Loss: 0.00002247
Iteration 474/1000 | Loss: 0.00002247
Iteration 475/1000 | Loss: 0.00002247
Iteration 476/1000 | Loss: 0.00002247
Iteration 477/1000 | Loss: 0.00002247
Iteration 478/1000 | Loss: 0.00002247
Iteration 479/1000 | Loss: 0.00002247
Iteration 480/1000 | Loss: 0.00002247
Iteration 481/1000 | Loss: 0.00002247
Iteration 482/1000 | Loss: 0.00002247
Iteration 483/1000 | Loss: 0.00002247
Iteration 484/1000 | Loss: 0.00002247
Iteration 485/1000 | Loss: 0.00002247
Iteration 486/1000 | Loss: 0.00002247
Iteration 487/1000 | Loss: 0.00002247
Iteration 488/1000 | Loss: 0.00002247
Iteration 489/1000 | Loss: 0.00002247
Iteration 490/1000 | Loss: 0.00002247
Iteration 491/1000 | Loss: 0.00002247
Iteration 492/1000 | Loss: 0.00002247
Iteration 493/1000 | Loss: 0.00002247
Iteration 494/1000 | Loss: 0.00002247
Iteration 495/1000 | Loss: 0.00002247
Iteration 496/1000 | Loss: 0.00002247
Iteration 497/1000 | Loss: 0.00002247
Iteration 498/1000 | Loss: 0.00002247
Iteration 499/1000 | Loss: 0.00002247
Iteration 500/1000 | Loss: 0.00002247
Iteration 501/1000 | Loss: 0.00002247
Iteration 502/1000 | Loss: 0.00002247
Iteration 503/1000 | Loss: 0.00002247
Iteration 504/1000 | Loss: 0.00002247
Iteration 505/1000 | Loss: 0.00002247
Iteration 506/1000 | Loss: 0.00002247
Iteration 507/1000 | Loss: 0.00002247
Iteration 508/1000 | Loss: 0.00002247
Iteration 509/1000 | Loss: 0.00002247
Iteration 510/1000 | Loss: 0.00002247
Iteration 511/1000 | Loss: 0.00002247
Iteration 512/1000 | Loss: 0.00002247
Iteration 513/1000 | Loss: 0.00002247
Iteration 514/1000 | Loss: 0.00002247
Iteration 515/1000 | Loss: 0.00002247
Iteration 516/1000 | Loss: 0.00002247
Iteration 517/1000 | Loss: 0.00002247
Iteration 518/1000 | Loss: 0.00002247
Iteration 519/1000 | Loss: 0.00002247
Iteration 520/1000 | Loss: 0.00002247
Iteration 521/1000 | Loss: 0.00002247
Iteration 522/1000 | Loss: 0.00002247
Iteration 523/1000 | Loss: 0.00002247
Iteration 524/1000 | Loss: 0.00002247
Iteration 525/1000 | Loss: 0.00002247
Iteration 526/1000 | Loss: 0.00002247
Iteration 527/1000 | Loss: 0.00002247
Iteration 528/1000 | Loss: 0.00002247
Iteration 529/1000 | Loss: 0.00002247
Iteration 530/1000 | Loss: 0.00002247
Iteration 531/1000 | Loss: 0.00002247
Iteration 532/1000 | Loss: 0.00002247
Iteration 533/1000 | Loss: 0.00002247
Iteration 534/1000 | Loss: 0.00002247
Iteration 535/1000 | Loss: 0.00002247
Iteration 536/1000 | Loss: 0.00002247
Iteration 537/1000 | Loss: 0.00002247
Iteration 538/1000 | Loss: 0.00002247
Iteration 539/1000 | Loss: 0.00002247
Iteration 540/1000 | Loss: 0.00002247
Iteration 541/1000 | Loss: 0.00002247
Iteration 542/1000 | Loss: 0.00002247
Iteration 543/1000 | Loss: 0.00002247
Iteration 544/1000 | Loss: 0.00002247
Iteration 545/1000 | Loss: 0.00002247
Iteration 546/1000 | Loss: 0.00002247
Iteration 547/1000 | Loss: 0.00002247
Iteration 548/1000 | Loss: 0.00002247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 548. Stopping optimization.
Last 5 losses: [2.246615622425452e-05, 2.246615622425452e-05, 2.246615622425452e-05, 2.246615622425452e-05, 2.246615622425452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.246615622425452e-05

Optimization complete. Final v2v error: 3.8346569538116455 mm

Highest mean error: 12.667093276977539 mm for frame 30

Lowest mean error: 3.0779213905334473 mm for frame 29

Saving results

Total time: 178.00813603401184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924916
Iteration 2/25 | Loss: 0.00221388
Iteration 3/25 | Loss: 0.00154736
Iteration 4/25 | Loss: 0.00136785
Iteration 5/25 | Loss: 0.00138625
Iteration 6/25 | Loss: 0.00139250
Iteration 7/25 | Loss: 0.00121875
Iteration 8/25 | Loss: 0.00112854
Iteration 9/25 | Loss: 0.00111461
Iteration 10/25 | Loss: 0.00109150
Iteration 11/25 | Loss: 0.00109054
Iteration 12/25 | Loss: 0.00107231
Iteration 13/25 | Loss: 0.00107008
Iteration 14/25 | Loss: 0.00107249
Iteration 15/25 | Loss: 0.00107298
Iteration 16/25 | Loss: 0.00106564
Iteration 17/25 | Loss: 0.00105986
Iteration 18/25 | Loss: 0.00105694
Iteration 19/25 | Loss: 0.00105555
Iteration 20/25 | Loss: 0.00105467
Iteration 21/25 | Loss: 0.00105421
Iteration 22/25 | Loss: 0.00105397
Iteration 23/25 | Loss: 0.00105753
Iteration 24/25 | Loss: 0.00105373
Iteration 25/25 | Loss: 0.00105252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56763005
Iteration 2/25 | Loss: 0.00073638
Iteration 3/25 | Loss: 0.00061988
Iteration 4/25 | Loss: 0.00061988
Iteration 5/25 | Loss: 0.00061988
Iteration 6/25 | Loss: 0.00061988
Iteration 7/25 | Loss: 0.00061988
Iteration 8/25 | Loss: 0.00061988
Iteration 9/25 | Loss: 0.00061988
Iteration 10/25 | Loss: 0.00061988
Iteration 11/25 | Loss: 0.00061988
Iteration 12/25 | Loss: 0.00061988
Iteration 13/25 | Loss: 0.00061988
Iteration 14/25 | Loss: 0.00061988
Iteration 15/25 | Loss: 0.00061988
Iteration 16/25 | Loss: 0.00061988
Iteration 17/25 | Loss: 0.00061988
Iteration 18/25 | Loss: 0.00061988
Iteration 19/25 | Loss: 0.00061988
Iteration 20/25 | Loss: 0.00061988
Iteration 21/25 | Loss: 0.00061988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006198795745149255, 0.0006198795745149255, 0.0006198795745149255, 0.0006198795745149255, 0.0006198795745149255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006198795745149255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061988
Iteration 2/1000 | Loss: 0.00009822
Iteration 3/1000 | Loss: 0.00005226
Iteration 4/1000 | Loss: 0.00013321
Iteration 5/1000 | Loss: 0.00004292
Iteration 6/1000 | Loss: 0.00004046
Iteration 7/1000 | Loss: 0.00003894
Iteration 8/1000 | Loss: 0.00003821
Iteration 9/1000 | Loss: 0.00012590
Iteration 10/1000 | Loss: 0.00013456
Iteration 11/1000 | Loss: 0.00015313
Iteration 12/1000 | Loss: 0.00018616
Iteration 13/1000 | Loss: 0.00003750
Iteration 14/1000 | Loss: 0.00003505
Iteration 15/1000 | Loss: 0.00003379
Iteration 16/1000 | Loss: 0.00003293
Iteration 17/1000 | Loss: 0.00003232
Iteration 18/1000 | Loss: 0.00003198
Iteration 19/1000 | Loss: 0.00003172
Iteration 20/1000 | Loss: 0.00003152
Iteration 21/1000 | Loss: 0.00003151
Iteration 22/1000 | Loss: 0.00003146
Iteration 23/1000 | Loss: 0.00003140
Iteration 24/1000 | Loss: 0.00003139
Iteration 25/1000 | Loss: 0.00003135
Iteration 26/1000 | Loss: 0.00003133
Iteration 27/1000 | Loss: 0.00003133
Iteration 28/1000 | Loss: 0.00003133
Iteration 29/1000 | Loss: 0.00003133
Iteration 30/1000 | Loss: 0.00003133
Iteration 31/1000 | Loss: 0.00003132
Iteration 32/1000 | Loss: 0.00003132
Iteration 33/1000 | Loss: 0.00003132
Iteration 34/1000 | Loss: 0.00003132
Iteration 35/1000 | Loss: 0.00003131
Iteration 36/1000 | Loss: 0.00003130
Iteration 37/1000 | Loss: 0.00003130
Iteration 38/1000 | Loss: 0.00003130
Iteration 39/1000 | Loss: 0.00003130
Iteration 40/1000 | Loss: 0.00003130
Iteration 41/1000 | Loss: 0.00003130
Iteration 42/1000 | Loss: 0.00003130
Iteration 43/1000 | Loss: 0.00003130
Iteration 44/1000 | Loss: 0.00003130
Iteration 45/1000 | Loss: 0.00003129
Iteration 46/1000 | Loss: 0.00003129
Iteration 47/1000 | Loss: 0.00003129
Iteration 48/1000 | Loss: 0.00003129
Iteration 49/1000 | Loss: 0.00003128
Iteration 50/1000 | Loss: 0.00003128
Iteration 51/1000 | Loss: 0.00003125
Iteration 52/1000 | Loss: 0.00003125
Iteration 53/1000 | Loss: 0.00003124
Iteration 54/1000 | Loss: 0.00003123
Iteration 55/1000 | Loss: 0.00003123
Iteration 56/1000 | Loss: 0.00003123
Iteration 57/1000 | Loss: 0.00003122
Iteration 58/1000 | Loss: 0.00003122
Iteration 59/1000 | Loss: 0.00003121
Iteration 60/1000 | Loss: 0.00003121
Iteration 61/1000 | Loss: 0.00003120
Iteration 62/1000 | Loss: 0.00003120
Iteration 63/1000 | Loss: 0.00003119
Iteration 64/1000 | Loss: 0.00003119
Iteration 65/1000 | Loss: 0.00003119
Iteration 66/1000 | Loss: 0.00003119
Iteration 67/1000 | Loss: 0.00003119
Iteration 68/1000 | Loss: 0.00003119
Iteration 69/1000 | Loss: 0.00003119
Iteration 70/1000 | Loss: 0.00003119
Iteration 71/1000 | Loss: 0.00003118
Iteration 72/1000 | Loss: 0.00003117
Iteration 73/1000 | Loss: 0.00003117
Iteration 74/1000 | Loss: 0.00003117
Iteration 75/1000 | Loss: 0.00003117
Iteration 76/1000 | Loss: 0.00003117
Iteration 77/1000 | Loss: 0.00003117
Iteration 78/1000 | Loss: 0.00003116
Iteration 79/1000 | Loss: 0.00003116
Iteration 80/1000 | Loss: 0.00003116
Iteration 81/1000 | Loss: 0.00003116
Iteration 82/1000 | Loss: 0.00003116
Iteration 83/1000 | Loss: 0.00003116
Iteration 84/1000 | Loss: 0.00003115
Iteration 85/1000 | Loss: 0.00003115
Iteration 86/1000 | Loss: 0.00003115
Iteration 87/1000 | Loss: 0.00003114
Iteration 88/1000 | Loss: 0.00003114
Iteration 89/1000 | Loss: 0.00003114
Iteration 90/1000 | Loss: 0.00003113
Iteration 91/1000 | Loss: 0.00003113
Iteration 92/1000 | Loss: 0.00003112
Iteration 93/1000 | Loss: 0.00003112
Iteration 94/1000 | Loss: 0.00003112
Iteration 95/1000 | Loss: 0.00003111
Iteration 96/1000 | Loss: 0.00003111
Iteration 97/1000 | Loss: 0.00003111
Iteration 98/1000 | Loss: 0.00003111
Iteration 99/1000 | Loss: 0.00003110
Iteration 100/1000 | Loss: 0.00003110
Iteration 101/1000 | Loss: 0.00003109
Iteration 102/1000 | Loss: 0.00003108
Iteration 103/1000 | Loss: 0.00003108
Iteration 104/1000 | Loss: 0.00003107
Iteration 105/1000 | Loss: 0.00003107
Iteration 106/1000 | Loss: 0.00003107
Iteration 107/1000 | Loss: 0.00003106
Iteration 108/1000 | Loss: 0.00003106
Iteration 109/1000 | Loss: 0.00003106
Iteration 110/1000 | Loss: 0.00003105
Iteration 111/1000 | Loss: 0.00003105
Iteration 112/1000 | Loss: 0.00003105
Iteration 113/1000 | Loss: 0.00003105
Iteration 114/1000 | Loss: 0.00003105
Iteration 115/1000 | Loss: 0.00003105
Iteration 116/1000 | Loss: 0.00003105
Iteration 117/1000 | Loss: 0.00003104
Iteration 118/1000 | Loss: 0.00003104
Iteration 119/1000 | Loss: 0.00003104
Iteration 120/1000 | Loss: 0.00003104
Iteration 121/1000 | Loss: 0.00003104
Iteration 122/1000 | Loss: 0.00003104
Iteration 123/1000 | Loss: 0.00003104
Iteration 124/1000 | Loss: 0.00003104
Iteration 125/1000 | Loss: 0.00003104
Iteration 126/1000 | Loss: 0.00003104
Iteration 127/1000 | Loss: 0.00003104
Iteration 128/1000 | Loss: 0.00003104
Iteration 129/1000 | Loss: 0.00003103
Iteration 130/1000 | Loss: 0.00003103
Iteration 131/1000 | Loss: 0.00003103
Iteration 132/1000 | Loss: 0.00003103
Iteration 133/1000 | Loss: 0.00003102
Iteration 134/1000 | Loss: 0.00003102
Iteration 135/1000 | Loss: 0.00003102
Iteration 136/1000 | Loss: 0.00003102
Iteration 137/1000 | Loss: 0.00003102
Iteration 138/1000 | Loss: 0.00003102
Iteration 139/1000 | Loss: 0.00003102
Iteration 140/1000 | Loss: 0.00003101
Iteration 141/1000 | Loss: 0.00003101
Iteration 142/1000 | Loss: 0.00003101
Iteration 143/1000 | Loss: 0.00003101
Iteration 144/1000 | Loss: 0.00003101
Iteration 145/1000 | Loss: 0.00003101
Iteration 146/1000 | Loss: 0.00003101
Iteration 147/1000 | Loss: 0.00003101
Iteration 148/1000 | Loss: 0.00003101
Iteration 149/1000 | Loss: 0.00003101
Iteration 150/1000 | Loss: 0.00003100
Iteration 151/1000 | Loss: 0.00003100
Iteration 152/1000 | Loss: 0.00003100
Iteration 153/1000 | Loss: 0.00003100
Iteration 154/1000 | Loss: 0.00003100
Iteration 155/1000 | Loss: 0.00003100
Iteration 156/1000 | Loss: 0.00003099
Iteration 157/1000 | Loss: 0.00003099
Iteration 158/1000 | Loss: 0.00003099
Iteration 159/1000 | Loss: 0.00003099
Iteration 160/1000 | Loss: 0.00003099
Iteration 161/1000 | Loss: 0.00003099
Iteration 162/1000 | Loss: 0.00003099
Iteration 163/1000 | Loss: 0.00003099
Iteration 164/1000 | Loss: 0.00003099
Iteration 165/1000 | Loss: 0.00003099
Iteration 166/1000 | Loss: 0.00003099
Iteration 167/1000 | Loss: 0.00003099
Iteration 168/1000 | Loss: 0.00003099
Iteration 169/1000 | Loss: 0.00003099
Iteration 170/1000 | Loss: 0.00003099
Iteration 171/1000 | Loss: 0.00003099
Iteration 172/1000 | Loss: 0.00003099
Iteration 173/1000 | Loss: 0.00003098
Iteration 174/1000 | Loss: 0.00003098
Iteration 175/1000 | Loss: 0.00003098
Iteration 176/1000 | Loss: 0.00003098
Iteration 177/1000 | Loss: 0.00003098
Iteration 178/1000 | Loss: 0.00003098
Iteration 179/1000 | Loss: 0.00003098
Iteration 180/1000 | Loss: 0.00003098
Iteration 181/1000 | Loss: 0.00003097
Iteration 182/1000 | Loss: 0.00003097
Iteration 183/1000 | Loss: 0.00003097
Iteration 184/1000 | Loss: 0.00003097
Iteration 185/1000 | Loss: 0.00003097
Iteration 186/1000 | Loss: 0.00003097
Iteration 187/1000 | Loss: 0.00003097
Iteration 188/1000 | Loss: 0.00003097
Iteration 189/1000 | Loss: 0.00003097
Iteration 190/1000 | Loss: 0.00003097
Iteration 191/1000 | Loss: 0.00003097
Iteration 192/1000 | Loss: 0.00003097
Iteration 193/1000 | Loss: 0.00003097
Iteration 194/1000 | Loss: 0.00003097
Iteration 195/1000 | Loss: 0.00003097
Iteration 196/1000 | Loss: 0.00003097
Iteration 197/1000 | Loss: 0.00003097
Iteration 198/1000 | Loss: 0.00003097
Iteration 199/1000 | Loss: 0.00003097
Iteration 200/1000 | Loss: 0.00003097
Iteration 201/1000 | Loss: 0.00003097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [3.0967872589826584e-05, 3.0967872589826584e-05, 3.0967872589826584e-05, 3.0967872589826584e-05, 3.0967872589826584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0967872589826584e-05

Optimization complete. Final v2v error: 3.8363535404205322 mm

Highest mean error: 11.901444435119629 mm for frame 33

Lowest mean error: 3.229727029800415 mm for frame 220

Saving results

Total time: 102.78427028656006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108503
Iteration 2/25 | Loss: 0.00383282
Iteration 3/25 | Loss: 0.00254796
Iteration 4/25 | Loss: 0.00211361
Iteration 5/25 | Loss: 0.00174533
Iteration 6/25 | Loss: 0.00166927
Iteration 7/25 | Loss: 0.00168226
Iteration 8/25 | Loss: 0.00156424
Iteration 9/25 | Loss: 0.00146685
Iteration 10/25 | Loss: 0.00144690
Iteration 11/25 | Loss: 0.00141861
Iteration 12/25 | Loss: 0.00139892
Iteration 13/25 | Loss: 0.00138196
Iteration 14/25 | Loss: 0.00137632
Iteration 15/25 | Loss: 0.00132617
Iteration 16/25 | Loss: 0.00132701
Iteration 17/25 | Loss: 0.00130525
Iteration 18/25 | Loss: 0.00129464
Iteration 19/25 | Loss: 0.00126770
Iteration 20/25 | Loss: 0.00127601
Iteration 21/25 | Loss: 0.00125698
Iteration 22/25 | Loss: 0.00127169
Iteration 23/25 | Loss: 0.00125632
Iteration 24/25 | Loss: 0.00125533
Iteration 25/25 | Loss: 0.00125507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42249084
Iteration 2/25 | Loss: 0.00314540
Iteration 3/25 | Loss: 0.00314539
Iteration 4/25 | Loss: 0.00314539
Iteration 5/25 | Loss: 0.00314539
Iteration 6/25 | Loss: 0.00314539
Iteration 7/25 | Loss: 0.00314539
Iteration 8/25 | Loss: 0.00314539
Iteration 9/25 | Loss: 0.00314539
Iteration 10/25 | Loss: 0.00314539
Iteration 11/25 | Loss: 0.00314539
Iteration 12/25 | Loss: 0.00314539
Iteration 13/25 | Loss: 0.00314539
Iteration 14/25 | Loss: 0.00314539
Iteration 15/25 | Loss: 0.00314539
Iteration 16/25 | Loss: 0.00314539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003145386464893818, 0.003145386464893818, 0.003145386464893818, 0.003145386464893818, 0.003145386464893818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003145386464893818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00314539
Iteration 2/1000 | Loss: 0.00065863
Iteration 3/1000 | Loss: 0.00085776
Iteration 4/1000 | Loss: 0.00098265
Iteration 5/1000 | Loss: 0.00085200
Iteration 6/1000 | Loss: 0.00135386
Iteration 7/1000 | Loss: 0.00030944
Iteration 8/1000 | Loss: 0.00193925
Iteration 9/1000 | Loss: 0.00282456
Iteration 10/1000 | Loss: 0.00035631
Iteration 11/1000 | Loss: 0.00334705
Iteration 12/1000 | Loss: 0.00034515
Iteration 13/1000 | Loss: 0.00066231
Iteration 14/1000 | Loss: 0.00097780
Iteration 15/1000 | Loss: 0.00022387
Iteration 16/1000 | Loss: 0.00019747
Iteration 17/1000 | Loss: 0.00045857
Iteration 18/1000 | Loss: 0.00103888
Iteration 19/1000 | Loss: 0.00211332
Iteration 20/1000 | Loss: 0.00786825
Iteration 21/1000 | Loss: 0.00420640
Iteration 22/1000 | Loss: 0.00371459
Iteration 23/1000 | Loss: 0.00601820
Iteration 24/1000 | Loss: 0.00258564
Iteration 25/1000 | Loss: 0.00227813
Iteration 26/1000 | Loss: 0.00156398
Iteration 27/1000 | Loss: 0.00153646
Iteration 28/1000 | Loss: 0.00376604
Iteration 29/1000 | Loss: 0.00140637
Iteration 30/1000 | Loss: 0.00130254
Iteration 31/1000 | Loss: 0.00098821
Iteration 32/1000 | Loss: 0.00055810
Iteration 33/1000 | Loss: 0.00157107
Iteration 34/1000 | Loss: 0.00120241
Iteration 35/1000 | Loss: 0.00027433
Iteration 36/1000 | Loss: 0.00189887
Iteration 37/1000 | Loss: 0.00076093
Iteration 38/1000 | Loss: 0.00045336
Iteration 39/1000 | Loss: 0.00031941
Iteration 40/1000 | Loss: 0.00039693
Iteration 41/1000 | Loss: 0.00022313
Iteration 42/1000 | Loss: 0.00070070
Iteration 43/1000 | Loss: 0.00024709
Iteration 44/1000 | Loss: 0.00037050
Iteration 45/1000 | Loss: 0.00032850
Iteration 46/1000 | Loss: 0.00037846
Iteration 47/1000 | Loss: 0.00037599
Iteration 48/1000 | Loss: 0.00049750
Iteration 49/1000 | Loss: 0.00078411
Iteration 50/1000 | Loss: 0.00074615
Iteration 51/1000 | Loss: 0.00054464
Iteration 52/1000 | Loss: 0.00010604
Iteration 53/1000 | Loss: 0.00053665
Iteration 54/1000 | Loss: 0.00031182
Iteration 55/1000 | Loss: 0.00010021
Iteration 56/1000 | Loss: 0.00028579
Iteration 57/1000 | Loss: 0.00008883
Iteration 58/1000 | Loss: 0.00005654
Iteration 59/1000 | Loss: 0.00074870
Iteration 60/1000 | Loss: 0.00076911
Iteration 61/1000 | Loss: 0.00071181
Iteration 62/1000 | Loss: 0.00020466
Iteration 63/1000 | Loss: 0.00031131
Iteration 64/1000 | Loss: 0.00020759
Iteration 65/1000 | Loss: 0.00014758
Iteration 66/1000 | Loss: 0.00030074
Iteration 67/1000 | Loss: 0.00018840
Iteration 68/1000 | Loss: 0.00027445
Iteration 69/1000 | Loss: 0.00025217
Iteration 70/1000 | Loss: 0.00038773
Iteration 71/1000 | Loss: 0.00060753
Iteration 72/1000 | Loss: 0.00027293
Iteration 73/1000 | Loss: 0.00018487
Iteration 74/1000 | Loss: 0.00040230
Iteration 75/1000 | Loss: 0.00028472
Iteration 76/1000 | Loss: 0.00012639
Iteration 77/1000 | Loss: 0.00022914
Iteration 78/1000 | Loss: 0.00016660
Iteration 79/1000 | Loss: 0.00109615
Iteration 80/1000 | Loss: 0.00055780
Iteration 81/1000 | Loss: 0.00057710
Iteration 82/1000 | Loss: 0.00042488
Iteration 83/1000 | Loss: 0.00013040
Iteration 84/1000 | Loss: 0.00004935
Iteration 85/1000 | Loss: 0.00058245
Iteration 86/1000 | Loss: 0.00020981
Iteration 87/1000 | Loss: 0.00044596
Iteration 88/1000 | Loss: 0.00088049
Iteration 89/1000 | Loss: 0.00054008
Iteration 90/1000 | Loss: 0.00029835
Iteration 91/1000 | Loss: 0.00003953
Iteration 92/1000 | Loss: 0.00047800
Iteration 93/1000 | Loss: 0.00087479
Iteration 94/1000 | Loss: 0.00130439
Iteration 95/1000 | Loss: 0.00078312
Iteration 96/1000 | Loss: 0.00051976
Iteration 97/1000 | Loss: 0.00044654
Iteration 98/1000 | Loss: 0.00029761
Iteration 99/1000 | Loss: 0.00005291
Iteration 100/1000 | Loss: 0.00004519
Iteration 101/1000 | Loss: 0.00003614
Iteration 102/1000 | Loss: 0.00040508
Iteration 103/1000 | Loss: 0.00037656
Iteration 104/1000 | Loss: 0.00034992
Iteration 105/1000 | Loss: 0.00004513
Iteration 106/1000 | Loss: 0.00012685
Iteration 107/1000 | Loss: 0.00015297
Iteration 108/1000 | Loss: 0.00003715
Iteration 109/1000 | Loss: 0.00048446
Iteration 110/1000 | Loss: 0.00026992
Iteration 111/1000 | Loss: 0.00045410
Iteration 112/1000 | Loss: 0.00050288
Iteration 113/1000 | Loss: 0.00129051
Iteration 114/1000 | Loss: 0.00027077
Iteration 115/1000 | Loss: 0.00174089
Iteration 116/1000 | Loss: 0.00010202
Iteration 117/1000 | Loss: 0.00012150
Iteration 118/1000 | Loss: 0.00045212
Iteration 119/1000 | Loss: 0.00016489
Iteration 120/1000 | Loss: 0.00038470
Iteration 121/1000 | Loss: 0.00062742
Iteration 122/1000 | Loss: 0.00047738
Iteration 123/1000 | Loss: 0.00012603
Iteration 124/1000 | Loss: 0.00011852
Iteration 125/1000 | Loss: 0.00024001
Iteration 126/1000 | Loss: 0.00010294
Iteration 127/1000 | Loss: 0.00035827
Iteration 128/1000 | Loss: 0.00004068
Iteration 129/1000 | Loss: 0.00004564
Iteration 130/1000 | Loss: 0.00008045
Iteration 131/1000 | Loss: 0.00023461
Iteration 132/1000 | Loss: 0.00007442
Iteration 133/1000 | Loss: 0.00005711
Iteration 134/1000 | Loss: 0.00006343
Iteration 135/1000 | Loss: 0.00008179
Iteration 136/1000 | Loss: 0.00006277
Iteration 137/1000 | Loss: 0.00003362
Iteration 138/1000 | Loss: 0.00003802
Iteration 139/1000 | Loss: 0.00005433
Iteration 140/1000 | Loss: 0.00003762
Iteration 141/1000 | Loss: 0.00004397
Iteration 142/1000 | Loss: 0.00031443
Iteration 143/1000 | Loss: 0.00004493
Iteration 144/1000 | Loss: 0.00004011
Iteration 145/1000 | Loss: 0.00101029
Iteration 146/1000 | Loss: 0.00021381
Iteration 147/1000 | Loss: 0.00055409
Iteration 148/1000 | Loss: 0.00006560
Iteration 149/1000 | Loss: 0.00003772
Iteration 150/1000 | Loss: 0.00003445
Iteration 151/1000 | Loss: 0.00003270
Iteration 152/1000 | Loss: 0.00003147
Iteration 153/1000 | Loss: 0.00003064
Iteration 154/1000 | Loss: 0.00003242
Iteration 155/1000 | Loss: 0.00002968
Iteration 156/1000 | Loss: 0.00064109
Iteration 157/1000 | Loss: 0.00033882
Iteration 158/1000 | Loss: 0.00011897
Iteration 159/1000 | Loss: 0.00004409
Iteration 160/1000 | Loss: 0.00025363
Iteration 161/1000 | Loss: 0.00003555
Iteration 162/1000 | Loss: 0.00003176
Iteration 163/1000 | Loss: 0.00002887
Iteration 164/1000 | Loss: 0.00002785
Iteration 165/1000 | Loss: 0.00002729
Iteration 166/1000 | Loss: 0.00002694
Iteration 167/1000 | Loss: 0.00002683
Iteration 168/1000 | Loss: 0.00002668
Iteration 169/1000 | Loss: 0.00002648
Iteration 170/1000 | Loss: 0.00002645
Iteration 171/1000 | Loss: 0.00087461
Iteration 172/1000 | Loss: 0.00034402
Iteration 173/1000 | Loss: 0.00005336
Iteration 174/1000 | Loss: 0.00016828
Iteration 175/1000 | Loss: 0.00034439
Iteration 176/1000 | Loss: 0.00028351
Iteration 177/1000 | Loss: 0.00029204
Iteration 178/1000 | Loss: 0.00003859
Iteration 179/1000 | Loss: 0.00003511
Iteration 180/1000 | Loss: 0.00005698
Iteration 181/1000 | Loss: 0.00002761
Iteration 182/1000 | Loss: 0.00002653
Iteration 183/1000 | Loss: 0.00002619
Iteration 184/1000 | Loss: 0.00002614
Iteration 185/1000 | Loss: 0.00002613
Iteration 186/1000 | Loss: 0.00002609
Iteration 187/1000 | Loss: 0.00002611
Iteration 188/1000 | Loss: 0.00002585
Iteration 189/1000 | Loss: 0.00010167
Iteration 190/1000 | Loss: 0.00003020
Iteration 191/1000 | Loss: 0.00002766
Iteration 192/1000 | Loss: 0.00002657
Iteration 193/1000 | Loss: 0.00002626
Iteration 194/1000 | Loss: 0.00002627
Iteration 195/1000 | Loss: 0.00002624
Iteration 196/1000 | Loss: 0.00002613
Iteration 197/1000 | Loss: 0.00002612
Iteration 198/1000 | Loss: 0.00002618
Iteration 199/1000 | Loss: 0.00002616
Iteration 200/1000 | Loss: 0.00002615
Iteration 201/1000 | Loss: 0.00002615
Iteration 202/1000 | Loss: 0.00002615
Iteration 203/1000 | Loss: 0.00002615
Iteration 204/1000 | Loss: 0.00002615
Iteration 205/1000 | Loss: 0.00002614
Iteration 206/1000 | Loss: 0.00002614
Iteration 207/1000 | Loss: 0.00002614
Iteration 208/1000 | Loss: 0.00002609
Iteration 209/1000 | Loss: 0.00002607
Iteration 210/1000 | Loss: 0.00002607
Iteration 211/1000 | Loss: 0.00002606
Iteration 212/1000 | Loss: 0.00002606
Iteration 213/1000 | Loss: 0.00002606
Iteration 214/1000 | Loss: 0.00002606
Iteration 215/1000 | Loss: 0.00002605
Iteration 216/1000 | Loss: 0.00002605
Iteration 217/1000 | Loss: 0.00002605
Iteration 218/1000 | Loss: 0.00002605
Iteration 219/1000 | Loss: 0.00028256
Iteration 220/1000 | Loss: 0.00010299
Iteration 221/1000 | Loss: 0.00014939
Iteration 222/1000 | Loss: 0.00004330
Iteration 223/1000 | Loss: 0.00002816
Iteration 224/1000 | Loss: 0.00002623
Iteration 225/1000 | Loss: 0.00002631
Iteration 226/1000 | Loss: 0.00002619
Iteration 227/1000 | Loss: 0.00002619
Iteration 228/1000 | Loss: 0.00024048
Iteration 229/1000 | Loss: 0.00003643
Iteration 230/1000 | Loss: 0.00018007
Iteration 231/1000 | Loss: 0.00004873
Iteration 232/1000 | Loss: 0.00003857
Iteration 233/1000 | Loss: 0.00002625
Iteration 234/1000 | Loss: 0.00002603
Iteration 235/1000 | Loss: 0.00002610
Iteration 236/1000 | Loss: 0.00002609
Iteration 237/1000 | Loss: 0.00002608
Iteration 238/1000 | Loss: 0.00002600
Iteration 239/1000 | Loss: 0.00002596
Iteration 240/1000 | Loss: 0.00031086
Iteration 241/1000 | Loss: 0.00062765
Iteration 242/1000 | Loss: 0.00007927
Iteration 243/1000 | Loss: 0.00030041
Iteration 244/1000 | Loss: 0.00006297
Iteration 245/1000 | Loss: 0.00002732
Iteration 246/1000 | Loss: 0.00038658
Iteration 247/1000 | Loss: 0.00040094
Iteration 248/1000 | Loss: 0.00039403
Iteration 249/1000 | Loss: 0.00004287
Iteration 250/1000 | Loss: 0.00042452
Iteration 251/1000 | Loss: 0.00037035
Iteration 252/1000 | Loss: 0.00028877
Iteration 253/1000 | Loss: 0.00024370
Iteration 254/1000 | Loss: 0.00044882
Iteration 255/1000 | Loss: 0.00024428
Iteration 256/1000 | Loss: 0.00013159
Iteration 257/1000 | Loss: 0.00032323
Iteration 258/1000 | Loss: 0.00021778
Iteration 259/1000 | Loss: 0.00005194
Iteration 260/1000 | Loss: 0.00021566
Iteration 261/1000 | Loss: 0.00031542
Iteration 262/1000 | Loss: 0.00003662
Iteration 263/1000 | Loss: 0.00002997
Iteration 264/1000 | Loss: 0.00034723
Iteration 265/1000 | Loss: 0.00039532
Iteration 266/1000 | Loss: 0.00045364
Iteration 267/1000 | Loss: 0.00003232
Iteration 268/1000 | Loss: 0.00002865
Iteration 269/1000 | Loss: 0.00002737
Iteration 270/1000 | Loss: 0.00002668
Iteration 271/1000 | Loss: 0.00002636
Iteration 272/1000 | Loss: 0.00002627
Iteration 273/1000 | Loss: 0.00002630
Iteration 274/1000 | Loss: 0.00010207
Iteration 275/1000 | Loss: 0.00002728
Iteration 276/1000 | Loss: 0.00002628
Iteration 277/1000 | Loss: 0.00002589
Iteration 278/1000 | Loss: 0.00002570
Iteration 279/1000 | Loss: 0.00002569
Iteration 280/1000 | Loss: 0.00002553
Iteration 281/1000 | Loss: 0.00002557
Iteration 282/1000 | Loss: 0.00002550
Iteration 283/1000 | Loss: 0.00002533
Iteration 284/1000 | Loss: 0.00002528
Iteration 285/1000 | Loss: 0.00002527
Iteration 286/1000 | Loss: 0.00002527
Iteration 287/1000 | Loss: 0.00002526
Iteration 288/1000 | Loss: 0.00002526
Iteration 289/1000 | Loss: 0.00002526
Iteration 290/1000 | Loss: 0.00002526
Iteration 291/1000 | Loss: 0.00002525
Iteration 292/1000 | Loss: 0.00002525
Iteration 293/1000 | Loss: 0.00002525
Iteration 294/1000 | Loss: 0.00002525
Iteration 295/1000 | Loss: 0.00002525
Iteration 296/1000 | Loss: 0.00002525
Iteration 297/1000 | Loss: 0.00002525
Iteration 298/1000 | Loss: 0.00002525
Iteration 299/1000 | Loss: 0.00002525
Iteration 300/1000 | Loss: 0.00002534
Iteration 301/1000 | Loss: 0.00002528
Iteration 302/1000 | Loss: 0.00002530
Iteration 303/1000 | Loss: 0.00002530
Iteration 304/1000 | Loss: 0.00002527
Iteration 305/1000 | Loss: 0.00002527
Iteration 306/1000 | Loss: 0.00002526
Iteration 307/1000 | Loss: 0.00002525
Iteration 308/1000 | Loss: 0.00002523
Iteration 309/1000 | Loss: 0.00002526
Iteration 310/1000 | Loss: 0.00002526
Iteration 311/1000 | Loss: 0.00002526
Iteration 312/1000 | Loss: 0.00002526
Iteration 313/1000 | Loss: 0.00002526
Iteration 314/1000 | Loss: 0.00002526
Iteration 315/1000 | Loss: 0.00002526
Iteration 316/1000 | Loss: 0.00002526
Iteration 317/1000 | Loss: 0.00002525
Iteration 318/1000 | Loss: 0.00002525
Iteration 319/1000 | Loss: 0.00002525
Iteration 320/1000 | Loss: 0.00002525
Iteration 321/1000 | Loss: 0.00002525
Iteration 322/1000 | Loss: 0.00002523
Iteration 323/1000 | Loss: 0.00002523
Iteration 324/1000 | Loss: 0.00002522
Iteration 325/1000 | Loss: 0.00002525
Iteration 326/1000 | Loss: 0.00002525
Iteration 327/1000 | Loss: 0.00002525
Iteration 328/1000 | Loss: 0.00002531
Iteration 329/1000 | Loss: 0.00002530
Iteration 330/1000 | Loss: 0.00002526
Iteration 331/1000 | Loss: 0.00002525
Iteration 332/1000 | Loss: 0.00002525
Iteration 333/1000 | Loss: 0.00002525
Iteration 334/1000 | Loss: 0.00002524
Iteration 335/1000 | Loss: 0.00002524
Iteration 336/1000 | Loss: 0.00002523
Iteration 337/1000 | Loss: 0.00002521
Iteration 338/1000 | Loss: 0.00002520
Iteration 339/1000 | Loss: 0.00002518
Iteration 340/1000 | Loss: 0.00002518
Iteration 341/1000 | Loss: 0.00076011
Iteration 342/1000 | Loss: 0.00069752
Iteration 343/1000 | Loss: 0.00083772
Iteration 344/1000 | Loss: 0.00025394
Iteration 345/1000 | Loss: 0.00003004
Iteration 346/1000 | Loss: 0.00006382
Iteration 347/1000 | Loss: 0.00002508
Iteration 348/1000 | Loss: 0.00002433
Iteration 349/1000 | Loss: 0.00005105
Iteration 350/1000 | Loss: 0.00002862
Iteration 351/1000 | Loss: 0.00002336
Iteration 352/1000 | Loss: 0.00002314
Iteration 353/1000 | Loss: 0.00002309
Iteration 354/1000 | Loss: 0.00002309
Iteration 355/1000 | Loss: 0.00002302
Iteration 356/1000 | Loss: 0.00002301
Iteration 357/1000 | Loss: 0.00002301
Iteration 358/1000 | Loss: 0.00002301
Iteration 359/1000 | Loss: 0.00002289
Iteration 360/1000 | Loss: 0.00002279
Iteration 361/1000 | Loss: 0.00002277
Iteration 362/1000 | Loss: 0.00002277
Iteration 363/1000 | Loss: 0.00002277
Iteration 364/1000 | Loss: 0.00002277
Iteration 365/1000 | Loss: 0.00002277
Iteration 366/1000 | Loss: 0.00002277
Iteration 367/1000 | Loss: 0.00002277
Iteration 368/1000 | Loss: 0.00002276
Iteration 369/1000 | Loss: 0.00002276
Iteration 370/1000 | Loss: 0.00002275
Iteration 371/1000 | Loss: 0.00002275
Iteration 372/1000 | Loss: 0.00002274
Iteration 373/1000 | Loss: 0.00002274
Iteration 374/1000 | Loss: 0.00002273
Iteration 375/1000 | Loss: 0.00002273
Iteration 376/1000 | Loss: 0.00052730
Iteration 377/1000 | Loss: 0.00052730
Iteration 378/1000 | Loss: 0.00036385
Iteration 379/1000 | Loss: 0.00003962
Iteration 380/1000 | Loss: 0.00054446
Iteration 381/1000 | Loss: 0.00022910
Iteration 382/1000 | Loss: 0.00052312
Iteration 383/1000 | Loss: 0.00020717
Iteration 384/1000 | Loss: 0.00052328
Iteration 385/1000 | Loss: 0.00022244
Iteration 386/1000 | Loss: 0.00051589
Iteration 387/1000 | Loss: 0.00017507
Iteration 388/1000 | Loss: 0.00048374
Iteration 389/1000 | Loss: 0.00011675
Iteration 390/1000 | Loss: 0.00006124
Iteration 391/1000 | Loss: 0.00002561
Iteration 392/1000 | Loss: 0.00012903
Iteration 393/1000 | Loss: 0.00003536
Iteration 394/1000 | Loss: 0.00004314
Iteration 395/1000 | Loss: 0.00002344
Iteration 396/1000 | Loss: 0.00002244
Iteration 397/1000 | Loss: 0.00002189
Iteration 398/1000 | Loss: 0.00002133
Iteration 399/1000 | Loss: 0.00002093
Iteration 400/1000 | Loss: 0.00002077
Iteration 401/1000 | Loss: 0.00002067
Iteration 402/1000 | Loss: 0.00002066
Iteration 403/1000 | Loss: 0.00002059
Iteration 404/1000 | Loss: 0.00002056
Iteration 405/1000 | Loss: 0.00002056
Iteration 406/1000 | Loss: 0.00002055
Iteration 407/1000 | Loss: 0.00002055
Iteration 408/1000 | Loss: 0.00002054
Iteration 409/1000 | Loss: 0.00002053
Iteration 410/1000 | Loss: 0.00002053
Iteration 411/1000 | Loss: 0.00002052
Iteration 412/1000 | Loss: 0.00002051
Iteration 413/1000 | Loss: 0.00002051
Iteration 414/1000 | Loss: 0.00002049
Iteration 415/1000 | Loss: 0.00002049
Iteration 416/1000 | Loss: 0.00002049
Iteration 417/1000 | Loss: 0.00002046
Iteration 418/1000 | Loss: 0.00002046
Iteration 419/1000 | Loss: 0.00002046
Iteration 420/1000 | Loss: 0.00002047
Iteration 421/1000 | Loss: 0.00002046
Iteration 422/1000 | Loss: 0.00002042
Iteration 423/1000 | Loss: 0.00002041
Iteration 424/1000 | Loss: 0.00002041
Iteration 425/1000 | Loss: 0.00002040
Iteration 426/1000 | Loss: 0.00002040
Iteration 427/1000 | Loss: 0.00002039
Iteration 428/1000 | Loss: 0.00002039
Iteration 429/1000 | Loss: 0.00002038
Iteration 430/1000 | Loss: 0.00002037
Iteration 431/1000 | Loss: 0.00002036
Iteration 432/1000 | Loss: 0.00002036
Iteration 433/1000 | Loss: 0.00002035
Iteration 434/1000 | Loss: 0.00002035
Iteration 435/1000 | Loss: 0.00002035
Iteration 436/1000 | Loss: 0.00002035
Iteration 437/1000 | Loss: 0.00002035
Iteration 438/1000 | Loss: 0.00002034
Iteration 439/1000 | Loss: 0.00002034
Iteration 440/1000 | Loss: 0.00002034
Iteration 441/1000 | Loss: 0.00002034
Iteration 442/1000 | Loss: 0.00002033
Iteration 443/1000 | Loss: 0.00002033
Iteration 444/1000 | Loss: 0.00002033
Iteration 445/1000 | Loss: 0.00002032
Iteration 446/1000 | Loss: 0.00002032
Iteration 447/1000 | Loss: 0.00002032
Iteration 448/1000 | Loss: 0.00002032
Iteration 449/1000 | Loss: 0.00002032
Iteration 450/1000 | Loss: 0.00002032
Iteration 451/1000 | Loss: 0.00002032
Iteration 452/1000 | Loss: 0.00002031
Iteration 453/1000 | Loss: 0.00002031
Iteration 454/1000 | Loss: 0.00002031
Iteration 455/1000 | Loss: 0.00002031
Iteration 456/1000 | Loss: 0.00002030
Iteration 457/1000 | Loss: 0.00002030
Iteration 458/1000 | Loss: 0.00002030
Iteration 459/1000 | Loss: 0.00002030
Iteration 460/1000 | Loss: 0.00002029
Iteration 461/1000 | Loss: 0.00002029
Iteration 462/1000 | Loss: 0.00002029
Iteration 463/1000 | Loss: 0.00002029
Iteration 464/1000 | Loss: 0.00002029
Iteration 465/1000 | Loss: 0.00002029
Iteration 466/1000 | Loss: 0.00002029
Iteration 467/1000 | Loss: 0.00002029
Iteration 468/1000 | Loss: 0.00002028
Iteration 469/1000 | Loss: 0.00002028
Iteration 470/1000 | Loss: 0.00002028
Iteration 471/1000 | Loss: 0.00002028
Iteration 472/1000 | Loss: 0.00002028
Iteration 473/1000 | Loss: 0.00002027
Iteration 474/1000 | Loss: 0.00002027
Iteration 475/1000 | Loss: 0.00002027
Iteration 476/1000 | Loss: 0.00002027
Iteration 477/1000 | Loss: 0.00002027
Iteration 478/1000 | Loss: 0.00002027
Iteration 479/1000 | Loss: 0.00002032
Iteration 480/1000 | Loss: 0.00002032
Iteration 481/1000 | Loss: 0.00002031
Iteration 482/1000 | Loss: 0.00002030
Iteration 483/1000 | Loss: 0.00002027
Iteration 484/1000 | Loss: 0.00002027
Iteration 485/1000 | Loss: 0.00002027
Iteration 486/1000 | Loss: 0.00002027
Iteration 487/1000 | Loss: 0.00002027
Iteration 488/1000 | Loss: 0.00002027
Iteration 489/1000 | Loss: 0.00002027
Iteration 490/1000 | Loss: 0.00002027
Iteration 491/1000 | Loss: 0.00002026
Iteration 492/1000 | Loss: 0.00002026
Iteration 493/1000 | Loss: 0.00002026
Iteration 494/1000 | Loss: 0.00002026
Iteration 495/1000 | Loss: 0.00002025
Iteration 496/1000 | Loss: 0.00002025
Iteration 497/1000 | Loss: 0.00002025
Iteration 498/1000 | Loss: 0.00002025
Iteration 499/1000 | Loss: 0.00002024
Iteration 500/1000 | Loss: 0.00002024
Iteration 501/1000 | Loss: 0.00002024
Iteration 502/1000 | Loss: 0.00002024
Iteration 503/1000 | Loss: 0.00002024
Iteration 504/1000 | Loss: 0.00002024
Iteration 505/1000 | Loss: 0.00002023
Iteration 506/1000 | Loss: 0.00002023
Iteration 507/1000 | Loss: 0.00002023
Iteration 508/1000 | Loss: 0.00002023
Iteration 509/1000 | Loss: 0.00002023
Iteration 510/1000 | Loss: 0.00002023
Iteration 511/1000 | Loss: 0.00002023
Iteration 512/1000 | Loss: 0.00002023
Iteration 513/1000 | Loss: 0.00002023
Iteration 514/1000 | Loss: 0.00002023
Iteration 515/1000 | Loss: 0.00002023
Iteration 516/1000 | Loss: 0.00002023
Iteration 517/1000 | Loss: 0.00002023
Iteration 518/1000 | Loss: 0.00002023
Iteration 519/1000 | Loss: 0.00002023
Iteration 520/1000 | Loss: 0.00002023
Iteration 521/1000 | Loss: 0.00002023
Iteration 522/1000 | Loss: 0.00002023
Iteration 523/1000 | Loss: 0.00002023
Iteration 524/1000 | Loss: 0.00002023
Iteration 525/1000 | Loss: 0.00002023
Iteration 526/1000 | Loss: 0.00002023
Iteration 527/1000 | Loss: 0.00002023
Iteration 528/1000 | Loss: 0.00002023
Iteration 529/1000 | Loss: 0.00002023
Iteration 530/1000 | Loss: 0.00002023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 530. Stopping optimization.
Last 5 losses: [2.022989428951405e-05, 2.022989428951405e-05, 2.022989428951405e-05, 2.022989428951405e-05, 2.022989428951405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.022989428951405e-05

Optimization complete. Final v2v error: 2.982848882675171 mm

Highest mean error: 12.676939010620117 mm for frame 89

Lowest mean error: 2.3117263317108154 mm for frame 225

Saving results

Total time: 532.3058557510376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920654
Iteration 2/25 | Loss: 0.00281043
Iteration 3/25 | Loss: 0.00177246
Iteration 4/25 | Loss: 0.00136386
Iteration 5/25 | Loss: 0.00127864
Iteration 6/25 | Loss: 0.00129358
Iteration 7/25 | Loss: 0.00126927
Iteration 8/25 | Loss: 0.00126349
Iteration 9/25 | Loss: 0.00125007
Iteration 10/25 | Loss: 0.00123993
Iteration 11/25 | Loss: 0.00123044
Iteration 12/25 | Loss: 0.00122491
Iteration 13/25 | Loss: 0.00122852
Iteration 14/25 | Loss: 0.00122068
Iteration 15/25 | Loss: 0.00121759
Iteration 16/25 | Loss: 0.00121459
Iteration 17/25 | Loss: 0.00121037
Iteration 18/25 | Loss: 0.00120986
Iteration 19/25 | Loss: 0.00120511
Iteration 20/25 | Loss: 0.00119931
Iteration 21/25 | Loss: 0.00119528
Iteration 22/25 | Loss: 0.00119632
Iteration 23/25 | Loss: 0.00120405
Iteration 24/25 | Loss: 0.00120461
Iteration 25/25 | Loss: 0.00120090

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09279680
Iteration 2/25 | Loss: 0.00368864
Iteration 3/25 | Loss: 0.00368863
Iteration 4/25 | Loss: 0.00368864
Iteration 5/25 | Loss: 0.00368864
Iteration 6/25 | Loss: 0.00368864
Iteration 7/25 | Loss: 0.00368863
Iteration 8/25 | Loss: 0.00368863
Iteration 9/25 | Loss: 0.00368863
Iteration 10/25 | Loss: 0.00368863
Iteration 11/25 | Loss: 0.00368863
Iteration 12/25 | Loss: 0.00368863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0036886348389089108, 0.0036886348389089108, 0.0036886348389089108, 0.0036886348389089108, 0.0036886348389089108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036886348389089108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00368863
Iteration 2/1000 | Loss: 0.00087488
Iteration 3/1000 | Loss: 0.00092827
Iteration 4/1000 | Loss: 0.00123317
Iteration 5/1000 | Loss: 0.00096560
Iteration 6/1000 | Loss: 0.00242280
Iteration 7/1000 | Loss: 0.00343875
Iteration 8/1000 | Loss: 0.00094540
Iteration 9/1000 | Loss: 0.00039223
Iteration 10/1000 | Loss: 0.00159941
Iteration 11/1000 | Loss: 0.00057657
Iteration 12/1000 | Loss: 0.00198968
Iteration 13/1000 | Loss: 0.00022274
Iteration 14/1000 | Loss: 0.00112963
Iteration 15/1000 | Loss: 0.00022717
Iteration 16/1000 | Loss: 0.00008289
Iteration 17/1000 | Loss: 0.00087739
Iteration 18/1000 | Loss: 0.00028851
Iteration 19/1000 | Loss: 0.00028363
Iteration 20/1000 | Loss: 0.00128692
Iteration 21/1000 | Loss: 0.00064060
Iteration 22/1000 | Loss: 0.00087769
Iteration 23/1000 | Loss: 0.00027668
Iteration 24/1000 | Loss: 0.00033941
Iteration 25/1000 | Loss: 0.00006544
Iteration 26/1000 | Loss: 0.00011214
Iteration 27/1000 | Loss: 0.00117464
Iteration 28/1000 | Loss: 0.00033833
Iteration 29/1000 | Loss: 0.00026260
Iteration 30/1000 | Loss: 0.00008822
Iteration 31/1000 | Loss: 0.00016449
Iteration 32/1000 | Loss: 0.00025364
Iteration 33/1000 | Loss: 0.00017328
Iteration 34/1000 | Loss: 0.00020477
Iteration 35/1000 | Loss: 0.00025840
Iteration 36/1000 | Loss: 0.00014573
Iteration 37/1000 | Loss: 0.00010274
Iteration 38/1000 | Loss: 0.00011820
Iteration 39/1000 | Loss: 0.00044496
Iteration 40/1000 | Loss: 0.00036067
Iteration 41/1000 | Loss: 0.00005770
Iteration 42/1000 | Loss: 0.00012337
Iteration 43/1000 | Loss: 0.00061134
Iteration 44/1000 | Loss: 0.00007745
Iteration 45/1000 | Loss: 0.00005794
Iteration 46/1000 | Loss: 0.00004385
Iteration 47/1000 | Loss: 0.00029665
Iteration 48/1000 | Loss: 0.00018018
Iteration 49/1000 | Loss: 0.00004255
Iteration 50/1000 | Loss: 0.00022731
Iteration 51/1000 | Loss: 0.00018228
Iteration 52/1000 | Loss: 0.00022140
Iteration 53/1000 | Loss: 0.00018133
Iteration 54/1000 | Loss: 0.00020087
Iteration 55/1000 | Loss: 0.00018185
Iteration 56/1000 | Loss: 0.00016377
Iteration 57/1000 | Loss: 0.00003593
Iteration 58/1000 | Loss: 0.00003422
Iteration 59/1000 | Loss: 0.00027695
Iteration 60/1000 | Loss: 0.00017861
Iteration 61/1000 | Loss: 0.00016305
Iteration 62/1000 | Loss: 0.00017074
Iteration 63/1000 | Loss: 0.00016994
Iteration 64/1000 | Loss: 0.00004380
Iteration 65/1000 | Loss: 0.00007529
Iteration 66/1000 | Loss: 0.00009735
Iteration 67/1000 | Loss: 0.00003338
Iteration 68/1000 | Loss: 0.00013926
Iteration 69/1000 | Loss: 0.00010033
Iteration 70/1000 | Loss: 0.00019849
Iteration 71/1000 | Loss: 0.00005019
Iteration 72/1000 | Loss: 0.00003666
Iteration 73/1000 | Loss: 0.00006416
Iteration 74/1000 | Loss: 0.00003132
Iteration 75/1000 | Loss: 0.00002918
Iteration 76/1000 | Loss: 0.00002778
Iteration 77/1000 | Loss: 0.00002684
Iteration 78/1000 | Loss: 0.00002607
Iteration 79/1000 | Loss: 0.00002553
Iteration 80/1000 | Loss: 0.00002518
Iteration 81/1000 | Loss: 0.00002488
Iteration 82/1000 | Loss: 0.00002602
Iteration 83/1000 | Loss: 0.00002451
Iteration 84/1000 | Loss: 0.00002427
Iteration 85/1000 | Loss: 0.00002427
Iteration 86/1000 | Loss: 0.00002420
Iteration 87/1000 | Loss: 0.00002402
Iteration 88/1000 | Loss: 0.00002398
Iteration 89/1000 | Loss: 0.00002392
Iteration 90/1000 | Loss: 0.00002382
Iteration 91/1000 | Loss: 0.00002379
Iteration 92/1000 | Loss: 0.00002374
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002372
Iteration 95/1000 | Loss: 0.00002372
Iteration 96/1000 | Loss: 0.00002371
Iteration 97/1000 | Loss: 0.00002371
Iteration 98/1000 | Loss: 0.00002371
Iteration 99/1000 | Loss: 0.00002370
Iteration 100/1000 | Loss: 0.00002370
Iteration 101/1000 | Loss: 0.00002369
Iteration 102/1000 | Loss: 0.00002369
Iteration 103/1000 | Loss: 0.00002368
Iteration 104/1000 | Loss: 0.00002368
Iteration 105/1000 | Loss: 0.00002368
Iteration 106/1000 | Loss: 0.00002367
Iteration 107/1000 | Loss: 0.00002366
Iteration 108/1000 | Loss: 0.00002366
Iteration 109/1000 | Loss: 0.00002366
Iteration 110/1000 | Loss: 0.00002362
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00002360
Iteration 113/1000 | Loss: 0.00002355
Iteration 114/1000 | Loss: 0.00002351
Iteration 115/1000 | Loss: 0.00002345
Iteration 116/1000 | Loss: 0.00002345
Iteration 117/1000 | Loss: 0.00002339
Iteration 118/1000 | Loss: 0.00002339
Iteration 119/1000 | Loss: 0.00002339
Iteration 120/1000 | Loss: 0.00002339
Iteration 121/1000 | Loss: 0.00002339
Iteration 122/1000 | Loss: 0.00002339
Iteration 123/1000 | Loss: 0.00002339
Iteration 124/1000 | Loss: 0.00002339
Iteration 125/1000 | Loss: 0.00002338
Iteration 126/1000 | Loss: 0.00002338
Iteration 127/1000 | Loss: 0.00002338
Iteration 128/1000 | Loss: 0.00002338
Iteration 129/1000 | Loss: 0.00002338
Iteration 130/1000 | Loss: 0.00002337
Iteration 131/1000 | Loss: 0.00002337
Iteration 132/1000 | Loss: 0.00002336
Iteration 133/1000 | Loss: 0.00002336
Iteration 134/1000 | Loss: 0.00002336
Iteration 135/1000 | Loss: 0.00002336
Iteration 136/1000 | Loss: 0.00002336
Iteration 137/1000 | Loss: 0.00002335
Iteration 138/1000 | Loss: 0.00002335
Iteration 139/1000 | Loss: 0.00002335
Iteration 140/1000 | Loss: 0.00002335
Iteration 141/1000 | Loss: 0.00002335
Iteration 142/1000 | Loss: 0.00002335
Iteration 143/1000 | Loss: 0.00002334
Iteration 144/1000 | Loss: 0.00002334
Iteration 145/1000 | Loss: 0.00002334
Iteration 146/1000 | Loss: 0.00002334
Iteration 147/1000 | Loss: 0.00002334
Iteration 148/1000 | Loss: 0.00002333
Iteration 149/1000 | Loss: 0.00002333
Iteration 150/1000 | Loss: 0.00002332
Iteration 151/1000 | Loss: 0.00002332
Iteration 152/1000 | Loss: 0.00002332
Iteration 153/1000 | Loss: 0.00002332
Iteration 154/1000 | Loss: 0.00002331
Iteration 155/1000 | Loss: 0.00002331
Iteration 156/1000 | Loss: 0.00002331
Iteration 157/1000 | Loss: 0.00002331
Iteration 158/1000 | Loss: 0.00002331
Iteration 159/1000 | Loss: 0.00002331
Iteration 160/1000 | Loss: 0.00002331
Iteration 161/1000 | Loss: 0.00002331
Iteration 162/1000 | Loss: 0.00002330
Iteration 163/1000 | Loss: 0.00002330
Iteration 164/1000 | Loss: 0.00002330
Iteration 165/1000 | Loss: 0.00002330
Iteration 166/1000 | Loss: 0.00002330
Iteration 167/1000 | Loss: 0.00002330
Iteration 168/1000 | Loss: 0.00002330
Iteration 169/1000 | Loss: 0.00002330
Iteration 170/1000 | Loss: 0.00002330
Iteration 171/1000 | Loss: 0.00002329
Iteration 172/1000 | Loss: 0.00002329
Iteration 173/1000 | Loss: 0.00002329
Iteration 174/1000 | Loss: 0.00002329
Iteration 175/1000 | Loss: 0.00002328
Iteration 176/1000 | Loss: 0.00002328
Iteration 177/1000 | Loss: 0.00002328
Iteration 178/1000 | Loss: 0.00002328
Iteration 179/1000 | Loss: 0.00002328
Iteration 180/1000 | Loss: 0.00002328
Iteration 181/1000 | Loss: 0.00002328
Iteration 182/1000 | Loss: 0.00002327
Iteration 183/1000 | Loss: 0.00002327
Iteration 184/1000 | Loss: 0.00002327
Iteration 185/1000 | Loss: 0.00002327
Iteration 186/1000 | Loss: 0.00002327
Iteration 187/1000 | Loss: 0.00002327
Iteration 188/1000 | Loss: 0.00002327
Iteration 189/1000 | Loss: 0.00002327
Iteration 190/1000 | Loss: 0.00002327
Iteration 191/1000 | Loss: 0.00002326
Iteration 192/1000 | Loss: 0.00002326
Iteration 193/1000 | Loss: 0.00002326
Iteration 194/1000 | Loss: 0.00002326
Iteration 195/1000 | Loss: 0.00002326
Iteration 196/1000 | Loss: 0.00002326
Iteration 197/1000 | Loss: 0.00002326
Iteration 198/1000 | Loss: 0.00002326
Iteration 199/1000 | Loss: 0.00002326
Iteration 200/1000 | Loss: 0.00002326
Iteration 201/1000 | Loss: 0.00002326
Iteration 202/1000 | Loss: 0.00002326
Iteration 203/1000 | Loss: 0.00002325
Iteration 204/1000 | Loss: 0.00002325
Iteration 205/1000 | Loss: 0.00002325
Iteration 206/1000 | Loss: 0.00002325
Iteration 207/1000 | Loss: 0.00002325
Iteration 208/1000 | Loss: 0.00002325
Iteration 209/1000 | Loss: 0.00002325
Iteration 210/1000 | Loss: 0.00002325
Iteration 211/1000 | Loss: 0.00002325
Iteration 212/1000 | Loss: 0.00002325
Iteration 213/1000 | Loss: 0.00002325
Iteration 214/1000 | Loss: 0.00002325
Iteration 215/1000 | Loss: 0.00002325
Iteration 216/1000 | Loss: 0.00002325
Iteration 217/1000 | Loss: 0.00002325
Iteration 218/1000 | Loss: 0.00002325
Iteration 219/1000 | Loss: 0.00002325
Iteration 220/1000 | Loss: 0.00002325
Iteration 221/1000 | Loss: 0.00002325
Iteration 222/1000 | Loss: 0.00002325
Iteration 223/1000 | Loss: 0.00002325
Iteration 224/1000 | Loss: 0.00002325
Iteration 225/1000 | Loss: 0.00002325
Iteration 226/1000 | Loss: 0.00002325
Iteration 227/1000 | Loss: 0.00002325
Iteration 228/1000 | Loss: 0.00002325
Iteration 229/1000 | Loss: 0.00002325
Iteration 230/1000 | Loss: 0.00002325
Iteration 231/1000 | Loss: 0.00002325
Iteration 232/1000 | Loss: 0.00002325
Iteration 233/1000 | Loss: 0.00002325
Iteration 234/1000 | Loss: 0.00002325
Iteration 235/1000 | Loss: 0.00002325
Iteration 236/1000 | Loss: 0.00002325
Iteration 237/1000 | Loss: 0.00002325
Iteration 238/1000 | Loss: 0.00002325
Iteration 239/1000 | Loss: 0.00002325
Iteration 240/1000 | Loss: 0.00002325
Iteration 241/1000 | Loss: 0.00002325
Iteration 242/1000 | Loss: 0.00002325
Iteration 243/1000 | Loss: 0.00002325
Iteration 244/1000 | Loss: 0.00002325
Iteration 245/1000 | Loss: 0.00002325
Iteration 246/1000 | Loss: 0.00002325
Iteration 247/1000 | Loss: 0.00002325
Iteration 248/1000 | Loss: 0.00002325
Iteration 249/1000 | Loss: 0.00002325
Iteration 250/1000 | Loss: 0.00002325
Iteration 251/1000 | Loss: 0.00002325
Iteration 252/1000 | Loss: 0.00002325
Iteration 253/1000 | Loss: 0.00002325
Iteration 254/1000 | Loss: 0.00002325
Iteration 255/1000 | Loss: 0.00002325
Iteration 256/1000 | Loss: 0.00002325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [2.324687193322461e-05, 2.324687193322461e-05, 2.324687193322461e-05, 2.324687193322461e-05, 2.324687193322461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.324687193322461e-05

Optimization complete. Final v2v error: 3.131175994873047 mm

Highest mean error: 12.914642333984375 mm for frame 76

Lowest mean error: 2.374600648880005 mm for frame 28

Saving results

Total time: 189.06904006004333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891789
Iteration 2/25 | Loss: 0.00230682
Iteration 3/25 | Loss: 0.00179690
Iteration 4/25 | Loss: 0.00173210
Iteration 5/25 | Loss: 0.00156278
Iteration 6/25 | Loss: 0.00145002
Iteration 7/25 | Loss: 0.00141009
Iteration 8/25 | Loss: 0.00139263
Iteration 9/25 | Loss: 0.00138453
Iteration 10/25 | Loss: 0.00139058
Iteration 11/25 | Loss: 0.00142941
Iteration 12/25 | Loss: 0.00142566
Iteration 13/25 | Loss: 0.00138088
Iteration 14/25 | Loss: 0.00135612
Iteration 15/25 | Loss: 0.00132973
Iteration 16/25 | Loss: 0.00132625
Iteration 17/25 | Loss: 0.00132662
Iteration 18/25 | Loss: 0.00134119
Iteration 19/25 | Loss: 0.00136375
Iteration 20/25 | Loss: 0.00131943
Iteration 21/25 | Loss: 0.00133557
Iteration 22/25 | Loss: 0.00131515
Iteration 23/25 | Loss: 0.00131149
Iteration 24/25 | Loss: 0.00130792
Iteration 25/25 | Loss: 0.00130748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61078537
Iteration 2/25 | Loss: 0.00165494
Iteration 3/25 | Loss: 0.00165459
Iteration 4/25 | Loss: 0.00165459
Iteration 5/25 | Loss: 0.00165459
Iteration 6/25 | Loss: 0.00165458
Iteration 7/25 | Loss: 0.00165458
Iteration 8/25 | Loss: 0.00165458
Iteration 9/25 | Loss: 0.00165458
Iteration 10/25 | Loss: 0.00165458
Iteration 11/25 | Loss: 0.00165458
Iteration 12/25 | Loss: 0.00165458
Iteration 13/25 | Loss: 0.00165458
Iteration 14/25 | Loss: 0.00165458
Iteration 15/25 | Loss: 0.00165458
Iteration 16/25 | Loss: 0.00165458
Iteration 17/25 | Loss: 0.00165458
Iteration 18/25 | Loss: 0.00165458
Iteration 19/25 | Loss: 0.00165458
Iteration 20/25 | Loss: 0.00165458
Iteration 21/25 | Loss: 0.00165458
Iteration 22/25 | Loss: 0.00165458
Iteration 23/25 | Loss: 0.00165458
Iteration 24/25 | Loss: 0.00165458
Iteration 25/25 | Loss: 0.00165458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165458
Iteration 2/1000 | Loss: 0.00875031
Iteration 3/1000 | Loss: 0.00014443
Iteration 4/1000 | Loss: 0.00009381
Iteration 5/1000 | Loss: 0.00007533
Iteration 6/1000 | Loss: 0.00006750
Iteration 7/1000 | Loss: 0.00006268
Iteration 8/1000 | Loss: 0.00005886
Iteration 9/1000 | Loss: 0.00005662
Iteration 10/1000 | Loss: 0.00005460
Iteration 11/1000 | Loss: 0.00005311
Iteration 12/1000 | Loss: 0.00005194
Iteration 13/1000 | Loss: 0.00005087
Iteration 14/1000 | Loss: 0.00005004
Iteration 15/1000 | Loss: 0.00004942
Iteration 16/1000 | Loss: 0.00004889
Iteration 17/1000 | Loss: 0.00004853
Iteration 18/1000 | Loss: 0.00004829
Iteration 19/1000 | Loss: 0.00004813
Iteration 20/1000 | Loss: 0.00004805
Iteration 21/1000 | Loss: 0.00004802
Iteration 22/1000 | Loss: 0.00004797
Iteration 23/1000 | Loss: 0.00004788
Iteration 24/1000 | Loss: 0.00004787
Iteration 25/1000 | Loss: 0.00004786
Iteration 26/1000 | Loss: 0.00004780
Iteration 27/1000 | Loss: 0.00004778
Iteration 28/1000 | Loss: 0.00004778
Iteration 29/1000 | Loss: 0.00004777
Iteration 30/1000 | Loss: 0.00004777
Iteration 31/1000 | Loss: 0.00004776
Iteration 32/1000 | Loss: 0.00004776
Iteration 33/1000 | Loss: 0.00004776
Iteration 34/1000 | Loss: 0.00004775
Iteration 35/1000 | Loss: 0.00004775
Iteration 36/1000 | Loss: 0.00004774
Iteration 37/1000 | Loss: 0.00004774
Iteration 38/1000 | Loss: 0.00004774
Iteration 39/1000 | Loss: 0.00004773
Iteration 40/1000 | Loss: 0.00004773
Iteration 41/1000 | Loss: 0.00004772
Iteration 42/1000 | Loss: 0.00004772
Iteration 43/1000 | Loss: 0.00004771
Iteration 44/1000 | Loss: 0.00004771
Iteration 45/1000 | Loss: 0.00004768
Iteration 46/1000 | Loss: 0.00004768
Iteration 47/1000 | Loss: 0.00004767
Iteration 48/1000 | Loss: 0.00004767
Iteration 49/1000 | Loss: 0.00004766
Iteration 50/1000 | Loss: 0.00004766
Iteration 51/1000 | Loss: 0.00004766
Iteration 52/1000 | Loss: 0.00004765
Iteration 53/1000 | Loss: 0.00004765
Iteration 54/1000 | Loss: 0.00004765
Iteration 55/1000 | Loss: 0.00004764
Iteration 56/1000 | Loss: 0.00004764
Iteration 57/1000 | Loss: 0.00004764
Iteration 58/1000 | Loss: 0.00004764
Iteration 59/1000 | Loss: 0.00004764
Iteration 60/1000 | Loss: 0.00004763
Iteration 61/1000 | Loss: 0.00004763
Iteration 62/1000 | Loss: 0.00004763
Iteration 63/1000 | Loss: 0.00004763
Iteration 64/1000 | Loss: 0.00004762
Iteration 65/1000 | Loss: 0.00004762
Iteration 66/1000 | Loss: 0.00004762
Iteration 67/1000 | Loss: 0.00004762
Iteration 68/1000 | Loss: 0.00004761
Iteration 69/1000 | Loss: 0.00004761
Iteration 70/1000 | Loss: 0.00004761
Iteration 71/1000 | Loss: 0.00004761
Iteration 72/1000 | Loss: 0.00004761
Iteration 73/1000 | Loss: 0.00004761
Iteration 74/1000 | Loss: 0.00004760
Iteration 75/1000 | Loss: 0.00004760
Iteration 76/1000 | Loss: 0.00004760
Iteration 77/1000 | Loss: 0.00004760
Iteration 78/1000 | Loss: 0.00004760
Iteration 79/1000 | Loss: 0.00004760
Iteration 80/1000 | Loss: 0.00004760
Iteration 81/1000 | Loss: 0.00004760
Iteration 82/1000 | Loss: 0.00004760
Iteration 83/1000 | Loss: 0.00004760
Iteration 84/1000 | Loss: 0.00004760
Iteration 85/1000 | Loss: 0.00004759
Iteration 86/1000 | Loss: 0.00004759
Iteration 87/1000 | Loss: 0.00004759
Iteration 88/1000 | Loss: 0.00004759
Iteration 89/1000 | Loss: 0.00004759
Iteration 90/1000 | Loss: 0.00004759
Iteration 91/1000 | Loss: 0.00004758
Iteration 92/1000 | Loss: 0.00004758
Iteration 93/1000 | Loss: 0.00004758
Iteration 94/1000 | Loss: 0.00004758
Iteration 95/1000 | Loss: 0.00004758
Iteration 96/1000 | Loss: 0.00004757
Iteration 97/1000 | Loss: 0.00004757
Iteration 98/1000 | Loss: 0.00004757
Iteration 99/1000 | Loss: 0.00004757
Iteration 100/1000 | Loss: 0.00004757
Iteration 101/1000 | Loss: 0.00004757
Iteration 102/1000 | Loss: 0.00004757
Iteration 103/1000 | Loss: 0.00004757
Iteration 104/1000 | Loss: 0.00004757
Iteration 105/1000 | Loss: 0.00004757
Iteration 106/1000 | Loss: 0.00004756
Iteration 107/1000 | Loss: 0.00004756
Iteration 108/1000 | Loss: 0.00004756
Iteration 109/1000 | Loss: 0.00004756
Iteration 110/1000 | Loss: 0.00004755
Iteration 111/1000 | Loss: 0.00004755
Iteration 112/1000 | Loss: 0.00004755
Iteration 113/1000 | Loss: 0.00004755
Iteration 114/1000 | Loss: 0.00004755
Iteration 115/1000 | Loss: 0.00004755
Iteration 116/1000 | Loss: 0.00004755
Iteration 117/1000 | Loss: 0.00004755
Iteration 118/1000 | Loss: 0.00004755
Iteration 119/1000 | Loss: 0.00004755
Iteration 120/1000 | Loss: 0.00004754
Iteration 121/1000 | Loss: 0.00004754
Iteration 122/1000 | Loss: 0.00004754
Iteration 123/1000 | Loss: 0.00004754
Iteration 124/1000 | Loss: 0.00004754
Iteration 125/1000 | Loss: 0.00004754
Iteration 126/1000 | Loss: 0.00004754
Iteration 127/1000 | Loss: 0.00004754
Iteration 128/1000 | Loss: 0.00004754
Iteration 129/1000 | Loss: 0.00004754
Iteration 130/1000 | Loss: 0.00004754
Iteration 131/1000 | Loss: 0.00004753
Iteration 132/1000 | Loss: 0.00004753
Iteration 133/1000 | Loss: 0.00004753
Iteration 134/1000 | Loss: 0.00004753
Iteration 135/1000 | Loss: 0.00004753
Iteration 136/1000 | Loss: 0.00004753
Iteration 137/1000 | Loss: 0.00004753
Iteration 138/1000 | Loss: 0.00004753
Iteration 139/1000 | Loss: 0.00004753
Iteration 140/1000 | Loss: 0.00004753
Iteration 141/1000 | Loss: 0.00004753
Iteration 142/1000 | Loss: 0.00004753
Iteration 143/1000 | Loss: 0.00004753
Iteration 144/1000 | Loss: 0.00004753
Iteration 145/1000 | Loss: 0.00004753
Iteration 146/1000 | Loss: 0.00004753
Iteration 147/1000 | Loss: 0.00004753
Iteration 148/1000 | Loss: 0.00004753
Iteration 149/1000 | Loss: 0.00004753
Iteration 150/1000 | Loss: 0.00004753
Iteration 151/1000 | Loss: 0.00004753
Iteration 152/1000 | Loss: 0.00004753
Iteration 153/1000 | Loss: 0.00004753
Iteration 154/1000 | Loss: 0.00004753
Iteration 155/1000 | Loss: 0.00004753
Iteration 156/1000 | Loss: 0.00004753
Iteration 157/1000 | Loss: 0.00004753
Iteration 158/1000 | Loss: 0.00004753
Iteration 159/1000 | Loss: 0.00004753
Iteration 160/1000 | Loss: 0.00004753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [4.7528395953122526e-05, 4.7528395953122526e-05, 4.7528395953122526e-05, 4.7528395953122526e-05, 4.7528395953122526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7528395953122526e-05

Optimization complete. Final v2v error: 5.1547064781188965 mm

Highest mean error: 11.369749069213867 mm for frame 46

Lowest mean error: 3.135993480682373 mm for frame 85

Saving results

Total time: 100.22808575630188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471965
Iteration 2/25 | Loss: 0.00123470
Iteration 3/25 | Loss: 0.00099930
Iteration 4/25 | Loss: 0.00098132
Iteration 5/25 | Loss: 0.00097757
Iteration 6/25 | Loss: 0.00097718
Iteration 7/25 | Loss: 0.00097718
Iteration 8/25 | Loss: 0.00097718
Iteration 9/25 | Loss: 0.00097718
Iteration 10/25 | Loss: 0.00097718
Iteration 11/25 | Loss: 0.00097718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009771756595000625, 0.0009771756595000625, 0.0009771756595000625, 0.0009771756595000625, 0.0009771756595000625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009771756595000625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44088924
Iteration 2/25 | Loss: 0.00034213
Iteration 3/25 | Loss: 0.00034213
Iteration 4/25 | Loss: 0.00034213
Iteration 5/25 | Loss: 0.00034212
Iteration 6/25 | Loss: 0.00034212
Iteration 7/25 | Loss: 0.00034212
Iteration 8/25 | Loss: 0.00034212
Iteration 9/25 | Loss: 0.00034212
Iteration 10/25 | Loss: 0.00034212
Iteration 11/25 | Loss: 0.00034212
Iteration 12/25 | Loss: 0.00034212
Iteration 13/25 | Loss: 0.00034212
Iteration 14/25 | Loss: 0.00034212
Iteration 15/25 | Loss: 0.00034212
Iteration 16/25 | Loss: 0.00034212
Iteration 17/25 | Loss: 0.00034212
Iteration 18/25 | Loss: 0.00034212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000342123385053128, 0.000342123385053128, 0.000342123385053128, 0.000342123385053128, 0.000342123385053128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000342123385053128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034212
Iteration 2/1000 | Loss: 0.00004013
Iteration 3/1000 | Loss: 0.00003367
Iteration 4/1000 | Loss: 0.00003180
Iteration 5/1000 | Loss: 0.00003087
Iteration 6/1000 | Loss: 0.00003008
Iteration 7/1000 | Loss: 0.00002941
Iteration 8/1000 | Loss: 0.00002910
Iteration 9/1000 | Loss: 0.00002884
Iteration 10/1000 | Loss: 0.00002869
Iteration 11/1000 | Loss: 0.00002865
Iteration 12/1000 | Loss: 0.00002865
Iteration 13/1000 | Loss: 0.00002864
Iteration 14/1000 | Loss: 0.00002859
Iteration 15/1000 | Loss: 0.00002859
Iteration 16/1000 | Loss: 0.00002853
Iteration 17/1000 | Loss: 0.00002853
Iteration 18/1000 | Loss: 0.00002844
Iteration 19/1000 | Loss: 0.00002839
Iteration 20/1000 | Loss: 0.00002830
Iteration 21/1000 | Loss: 0.00002824
Iteration 22/1000 | Loss: 0.00002822
Iteration 23/1000 | Loss: 0.00002820
Iteration 24/1000 | Loss: 0.00002820
Iteration 25/1000 | Loss: 0.00002820
Iteration 26/1000 | Loss: 0.00002820
Iteration 27/1000 | Loss: 0.00002820
Iteration 28/1000 | Loss: 0.00002820
Iteration 29/1000 | Loss: 0.00002820
Iteration 30/1000 | Loss: 0.00002820
Iteration 31/1000 | Loss: 0.00002819
Iteration 32/1000 | Loss: 0.00002819
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002818
Iteration 35/1000 | Loss: 0.00002818
Iteration 36/1000 | Loss: 0.00002818
Iteration 37/1000 | Loss: 0.00002818
Iteration 38/1000 | Loss: 0.00002817
Iteration 39/1000 | Loss: 0.00002817
Iteration 40/1000 | Loss: 0.00002817
Iteration 41/1000 | Loss: 0.00002817
Iteration 42/1000 | Loss: 0.00002817
Iteration 43/1000 | Loss: 0.00002816
Iteration 44/1000 | Loss: 0.00002816
Iteration 45/1000 | Loss: 0.00002816
Iteration 46/1000 | Loss: 0.00002815
Iteration 47/1000 | Loss: 0.00002815
Iteration 48/1000 | Loss: 0.00002814
Iteration 49/1000 | Loss: 0.00002814
Iteration 50/1000 | Loss: 0.00002814
Iteration 51/1000 | Loss: 0.00002814
Iteration 52/1000 | Loss: 0.00002814
Iteration 53/1000 | Loss: 0.00002814
Iteration 54/1000 | Loss: 0.00002813
Iteration 55/1000 | Loss: 0.00002813
Iteration 56/1000 | Loss: 0.00002812
Iteration 57/1000 | Loss: 0.00002812
Iteration 58/1000 | Loss: 0.00002812
Iteration 59/1000 | Loss: 0.00002812
Iteration 60/1000 | Loss: 0.00002812
Iteration 61/1000 | Loss: 0.00002812
Iteration 62/1000 | Loss: 0.00002811
Iteration 63/1000 | Loss: 0.00002811
Iteration 64/1000 | Loss: 0.00002810
Iteration 65/1000 | Loss: 0.00002810
Iteration 66/1000 | Loss: 0.00002810
Iteration 67/1000 | Loss: 0.00002809
Iteration 68/1000 | Loss: 0.00002809
Iteration 69/1000 | Loss: 0.00002809
Iteration 70/1000 | Loss: 0.00002809
Iteration 71/1000 | Loss: 0.00002809
Iteration 72/1000 | Loss: 0.00002809
Iteration 73/1000 | Loss: 0.00002809
Iteration 74/1000 | Loss: 0.00002808
Iteration 75/1000 | Loss: 0.00002808
Iteration 76/1000 | Loss: 0.00002808
Iteration 77/1000 | Loss: 0.00002808
Iteration 78/1000 | Loss: 0.00002808
Iteration 79/1000 | Loss: 0.00002808
Iteration 80/1000 | Loss: 0.00002807
Iteration 81/1000 | Loss: 0.00002807
Iteration 82/1000 | Loss: 0.00002807
Iteration 83/1000 | Loss: 0.00002807
Iteration 84/1000 | Loss: 0.00002807
Iteration 85/1000 | Loss: 0.00002807
Iteration 86/1000 | Loss: 0.00002807
Iteration 87/1000 | Loss: 0.00002807
Iteration 88/1000 | Loss: 0.00002807
Iteration 89/1000 | Loss: 0.00002806
Iteration 90/1000 | Loss: 0.00002806
Iteration 91/1000 | Loss: 0.00002806
Iteration 92/1000 | Loss: 0.00002806
Iteration 93/1000 | Loss: 0.00002806
Iteration 94/1000 | Loss: 0.00002806
Iteration 95/1000 | Loss: 0.00002806
Iteration 96/1000 | Loss: 0.00002806
Iteration 97/1000 | Loss: 0.00002806
Iteration 98/1000 | Loss: 0.00002806
Iteration 99/1000 | Loss: 0.00002804
Iteration 100/1000 | Loss: 0.00002804
Iteration 101/1000 | Loss: 0.00002804
Iteration 102/1000 | Loss: 0.00002804
Iteration 103/1000 | Loss: 0.00002804
Iteration 104/1000 | Loss: 0.00002804
Iteration 105/1000 | Loss: 0.00002804
Iteration 106/1000 | Loss: 0.00002804
Iteration 107/1000 | Loss: 0.00002804
Iteration 108/1000 | Loss: 0.00002804
Iteration 109/1000 | Loss: 0.00002803
Iteration 110/1000 | Loss: 0.00002803
Iteration 111/1000 | Loss: 0.00002803
Iteration 112/1000 | Loss: 0.00002803
Iteration 113/1000 | Loss: 0.00002803
Iteration 114/1000 | Loss: 0.00002802
Iteration 115/1000 | Loss: 0.00002802
Iteration 116/1000 | Loss: 0.00002802
Iteration 117/1000 | Loss: 0.00002802
Iteration 118/1000 | Loss: 0.00002802
Iteration 119/1000 | Loss: 0.00002802
Iteration 120/1000 | Loss: 0.00002801
Iteration 121/1000 | Loss: 0.00002801
Iteration 122/1000 | Loss: 0.00002801
Iteration 123/1000 | Loss: 0.00002801
Iteration 124/1000 | Loss: 0.00002801
Iteration 125/1000 | Loss: 0.00002801
Iteration 126/1000 | Loss: 0.00002801
Iteration 127/1000 | Loss: 0.00002800
Iteration 128/1000 | Loss: 0.00002800
Iteration 129/1000 | Loss: 0.00002800
Iteration 130/1000 | Loss: 0.00002800
Iteration 131/1000 | Loss: 0.00002800
Iteration 132/1000 | Loss: 0.00002799
Iteration 133/1000 | Loss: 0.00002799
Iteration 134/1000 | Loss: 0.00002799
Iteration 135/1000 | Loss: 0.00002798
Iteration 136/1000 | Loss: 0.00002798
Iteration 137/1000 | Loss: 0.00002798
Iteration 138/1000 | Loss: 0.00002798
Iteration 139/1000 | Loss: 0.00002798
Iteration 140/1000 | Loss: 0.00002797
Iteration 141/1000 | Loss: 0.00002797
Iteration 142/1000 | Loss: 0.00002797
Iteration 143/1000 | Loss: 0.00002797
Iteration 144/1000 | Loss: 0.00002796
Iteration 145/1000 | Loss: 0.00002796
Iteration 146/1000 | Loss: 0.00002796
Iteration 147/1000 | Loss: 0.00002796
Iteration 148/1000 | Loss: 0.00002796
Iteration 149/1000 | Loss: 0.00002796
Iteration 150/1000 | Loss: 0.00002796
Iteration 151/1000 | Loss: 0.00002796
Iteration 152/1000 | Loss: 0.00002795
Iteration 153/1000 | Loss: 0.00002795
Iteration 154/1000 | Loss: 0.00002795
Iteration 155/1000 | Loss: 0.00002795
Iteration 156/1000 | Loss: 0.00002795
Iteration 157/1000 | Loss: 0.00002795
Iteration 158/1000 | Loss: 0.00002795
Iteration 159/1000 | Loss: 0.00002795
Iteration 160/1000 | Loss: 0.00002795
Iteration 161/1000 | Loss: 0.00002794
Iteration 162/1000 | Loss: 0.00002794
Iteration 163/1000 | Loss: 0.00002794
Iteration 164/1000 | Loss: 0.00002794
Iteration 165/1000 | Loss: 0.00002794
Iteration 166/1000 | Loss: 0.00002793
Iteration 167/1000 | Loss: 0.00002793
Iteration 168/1000 | Loss: 0.00002793
Iteration 169/1000 | Loss: 0.00002793
Iteration 170/1000 | Loss: 0.00002792
Iteration 171/1000 | Loss: 0.00002792
Iteration 172/1000 | Loss: 0.00002791
Iteration 173/1000 | Loss: 0.00002791
Iteration 174/1000 | Loss: 0.00002791
Iteration 175/1000 | Loss: 0.00002791
Iteration 176/1000 | Loss: 0.00002791
Iteration 177/1000 | Loss: 0.00002791
Iteration 178/1000 | Loss: 0.00002791
Iteration 179/1000 | Loss: 0.00002791
Iteration 180/1000 | Loss: 0.00002791
Iteration 181/1000 | Loss: 0.00002791
Iteration 182/1000 | Loss: 0.00002791
Iteration 183/1000 | Loss: 0.00002791
Iteration 184/1000 | Loss: 0.00002791
Iteration 185/1000 | Loss: 0.00002791
Iteration 186/1000 | Loss: 0.00002791
Iteration 187/1000 | Loss: 0.00002791
Iteration 188/1000 | Loss: 0.00002791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.7906518880627118e-05, 2.7906518880627118e-05, 2.7906518880627118e-05, 2.7906518880627118e-05, 2.7906518880627118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7906518880627118e-05

Optimization complete. Final v2v error: 4.0249505043029785 mm

Highest mean error: 4.605393886566162 mm for frame 160

Lowest mean error: 3.2197306156158447 mm for frame 121

Saving results

Total time: 45.719786167144775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870784
Iteration 2/25 | Loss: 0.00140355
Iteration 3/25 | Loss: 0.00106753
Iteration 4/25 | Loss: 0.00103548
Iteration 5/25 | Loss: 0.00103021
Iteration 6/25 | Loss: 0.00102875
Iteration 7/25 | Loss: 0.00102875
Iteration 8/25 | Loss: 0.00102875
Iteration 9/25 | Loss: 0.00102875
Iteration 10/25 | Loss: 0.00102875
Iteration 11/25 | Loss: 0.00102875
Iteration 12/25 | Loss: 0.00102875
Iteration 13/25 | Loss: 0.00102875
Iteration 14/25 | Loss: 0.00102875
Iteration 15/25 | Loss: 0.00102875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001028750790283084, 0.001028750790283084, 0.001028750790283084, 0.001028750790283084, 0.001028750790283084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001028750790283084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38141561
Iteration 2/25 | Loss: 0.00039940
Iteration 3/25 | Loss: 0.00039934
Iteration 4/25 | Loss: 0.00039934
Iteration 5/25 | Loss: 0.00039934
Iteration 6/25 | Loss: 0.00039934
Iteration 7/25 | Loss: 0.00039934
Iteration 8/25 | Loss: 0.00039934
Iteration 9/25 | Loss: 0.00039934
Iteration 10/25 | Loss: 0.00039934
Iteration 11/25 | Loss: 0.00039934
Iteration 12/25 | Loss: 0.00039934
Iteration 13/25 | Loss: 0.00039934
Iteration 14/25 | Loss: 0.00039934
Iteration 15/25 | Loss: 0.00039934
Iteration 16/25 | Loss: 0.00039934
Iteration 17/25 | Loss: 0.00039934
Iteration 18/25 | Loss: 0.00039934
Iteration 19/25 | Loss: 0.00039934
Iteration 20/25 | Loss: 0.00039934
Iteration 21/25 | Loss: 0.00039934
Iteration 22/25 | Loss: 0.00039934
Iteration 23/25 | Loss: 0.00039934
Iteration 24/25 | Loss: 0.00039934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00039933796506375074, 0.00039933796506375074, 0.00039933796506375074, 0.00039933796506375074, 0.00039933796506375074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039933796506375074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039934
Iteration 2/1000 | Loss: 0.00002897
Iteration 3/1000 | Loss: 0.00002188
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00001754
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001674
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001626
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001592
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001588
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001587
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001585
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001576
Iteration 31/1000 | Loss: 0.00001576
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001575
Iteration 36/1000 | Loss: 0.00001575
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001571
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001560
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001559
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001557
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001557
Iteration 85/1000 | Loss: 0.00001557
Iteration 86/1000 | Loss: 0.00001557
Iteration 87/1000 | Loss: 0.00001557
Iteration 88/1000 | Loss: 0.00001557
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001556
Iteration 91/1000 | Loss: 0.00001556
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001556
Iteration 94/1000 | Loss: 0.00001556
Iteration 95/1000 | Loss: 0.00001556
Iteration 96/1000 | Loss: 0.00001556
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001555
Iteration 101/1000 | Loss: 0.00001555
Iteration 102/1000 | Loss: 0.00001555
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001555
Iteration 107/1000 | Loss: 0.00001555
Iteration 108/1000 | Loss: 0.00001555
Iteration 109/1000 | Loss: 0.00001555
Iteration 110/1000 | Loss: 0.00001555
Iteration 111/1000 | Loss: 0.00001555
Iteration 112/1000 | Loss: 0.00001555
Iteration 113/1000 | Loss: 0.00001555
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001554
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001554
Iteration 120/1000 | Loss: 0.00001554
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001554
Iteration 132/1000 | Loss: 0.00001554
Iteration 133/1000 | Loss: 0.00001554
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001554
Iteration 137/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.554245136503596e-05, 1.554245136503596e-05, 1.554245136503596e-05, 1.554245136503596e-05, 1.554245136503596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.554245136503596e-05

Optimization complete. Final v2v error: 3.303877353668213 mm

Highest mean error: 3.656942367553711 mm for frame 138

Lowest mean error: 2.7527496814727783 mm for frame 0

Saving results

Total time: 33.13253664970398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832309
Iteration 2/25 | Loss: 0.00122525
Iteration 3/25 | Loss: 0.00100422
Iteration 4/25 | Loss: 0.00097213
Iteration 5/25 | Loss: 0.00095989
Iteration 6/25 | Loss: 0.00095685
Iteration 7/25 | Loss: 0.00095606
Iteration 8/25 | Loss: 0.00095606
Iteration 9/25 | Loss: 0.00095606
Iteration 10/25 | Loss: 0.00095606
Iteration 11/25 | Loss: 0.00095606
Iteration 12/25 | Loss: 0.00095606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009560642647556961, 0.0009560642647556961, 0.0009560642647556961, 0.0009560642647556961, 0.0009560642647556961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009560642647556961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43189836
Iteration 2/25 | Loss: 0.00027943
Iteration 3/25 | Loss: 0.00027942
Iteration 4/25 | Loss: 0.00027942
Iteration 5/25 | Loss: 0.00027942
Iteration 6/25 | Loss: 0.00027942
Iteration 7/25 | Loss: 0.00027942
Iteration 8/25 | Loss: 0.00027942
Iteration 9/25 | Loss: 0.00027942
Iteration 10/25 | Loss: 0.00027942
Iteration 11/25 | Loss: 0.00027942
Iteration 12/25 | Loss: 0.00027942
Iteration 13/25 | Loss: 0.00027942
Iteration 14/25 | Loss: 0.00027942
Iteration 15/25 | Loss: 0.00027942
Iteration 16/25 | Loss: 0.00027942
Iteration 17/25 | Loss: 0.00027942
Iteration 18/25 | Loss: 0.00027942
Iteration 19/25 | Loss: 0.00027942
Iteration 20/25 | Loss: 0.00027942
Iteration 21/25 | Loss: 0.00027942
Iteration 22/25 | Loss: 0.00027942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00027942194719798863, 0.00027942194719798863, 0.00027942194719798863, 0.00027942194719798863, 0.00027942194719798863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027942194719798863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027942
Iteration 2/1000 | Loss: 0.00004102
Iteration 3/1000 | Loss: 0.00002268
Iteration 4/1000 | Loss: 0.00001950
Iteration 5/1000 | Loss: 0.00001816
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001611
Iteration 11/1000 | Loss: 0.00001611
Iteration 12/1000 | Loss: 0.00001610
Iteration 13/1000 | Loss: 0.00001595
Iteration 14/1000 | Loss: 0.00001590
Iteration 15/1000 | Loss: 0.00001588
Iteration 16/1000 | Loss: 0.00001587
Iteration 17/1000 | Loss: 0.00001587
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001585
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001578
Iteration 28/1000 | Loss: 0.00001578
Iteration 29/1000 | Loss: 0.00001577
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001568
Iteration 32/1000 | Loss: 0.00001568
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001567
Iteration 35/1000 | Loss: 0.00001566
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001564
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001564
Iteration 41/1000 | Loss: 0.00001564
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.553683978272602e-05, 1.553683978272602e-05, 1.553683978272602e-05, 1.553683978272602e-05, 1.553683978272602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.553683978272602e-05

Optimization complete. Final v2v error: 3.175405502319336 mm

Highest mean error: 3.6444060802459717 mm for frame 123

Lowest mean error: 2.8104004859924316 mm for frame 2

Saving results

Total time: 32.89566922187805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446468
Iteration 2/25 | Loss: 0.00104136
Iteration 3/25 | Loss: 0.00089247
Iteration 4/25 | Loss: 0.00086854
Iteration 5/25 | Loss: 0.00086222
Iteration 6/25 | Loss: 0.00086034
Iteration 7/25 | Loss: 0.00086000
Iteration 8/25 | Loss: 0.00086000
Iteration 9/25 | Loss: 0.00086000
Iteration 10/25 | Loss: 0.00086000
Iteration 11/25 | Loss: 0.00086000
Iteration 12/25 | Loss: 0.00086000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008599978755228221, 0.0008599978755228221, 0.0008599978755228221, 0.0008599978755228221, 0.0008599978755228221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008599978755228221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43194163
Iteration 2/25 | Loss: 0.00039875
Iteration 3/25 | Loss: 0.00039873
Iteration 4/25 | Loss: 0.00039873
Iteration 5/25 | Loss: 0.00039872
Iteration 6/25 | Loss: 0.00039872
Iteration 7/25 | Loss: 0.00039872
Iteration 8/25 | Loss: 0.00039872
Iteration 9/25 | Loss: 0.00039872
Iteration 10/25 | Loss: 0.00039872
Iteration 11/25 | Loss: 0.00039872
Iteration 12/25 | Loss: 0.00039872
Iteration 13/25 | Loss: 0.00039872
Iteration 14/25 | Loss: 0.00039872
Iteration 15/25 | Loss: 0.00039872
Iteration 16/25 | Loss: 0.00039872
Iteration 17/25 | Loss: 0.00039872
Iteration 18/25 | Loss: 0.00039872
Iteration 19/25 | Loss: 0.00039872
Iteration 20/25 | Loss: 0.00039872
Iteration 21/25 | Loss: 0.00039872
Iteration 22/25 | Loss: 0.00039872
Iteration 23/25 | Loss: 0.00039872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003987229720223695, 0.0003987229720223695, 0.0003987229720223695, 0.0003987229720223695, 0.0003987229720223695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003987229720223695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039872
Iteration 2/1000 | Loss: 0.00001811
Iteration 3/1000 | Loss: 0.00001286
Iteration 4/1000 | Loss: 0.00001129
Iteration 5/1000 | Loss: 0.00001050
Iteration 6/1000 | Loss: 0.00000990
Iteration 7/1000 | Loss: 0.00000965
Iteration 8/1000 | Loss: 0.00000961
Iteration 9/1000 | Loss: 0.00000960
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000958
Iteration 12/1000 | Loss: 0.00000956
Iteration 13/1000 | Loss: 0.00000955
Iteration 14/1000 | Loss: 0.00000954
Iteration 15/1000 | Loss: 0.00000954
Iteration 16/1000 | Loss: 0.00000953
Iteration 17/1000 | Loss: 0.00000951
Iteration 18/1000 | Loss: 0.00000945
Iteration 19/1000 | Loss: 0.00000945
Iteration 20/1000 | Loss: 0.00000945
Iteration 21/1000 | Loss: 0.00000944
Iteration 22/1000 | Loss: 0.00000943
Iteration 23/1000 | Loss: 0.00000939
Iteration 24/1000 | Loss: 0.00000939
Iteration 25/1000 | Loss: 0.00000939
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000938
Iteration 29/1000 | Loss: 0.00000938
Iteration 30/1000 | Loss: 0.00000938
Iteration 31/1000 | Loss: 0.00000938
Iteration 32/1000 | Loss: 0.00000938
Iteration 33/1000 | Loss: 0.00000938
Iteration 34/1000 | Loss: 0.00000937
Iteration 35/1000 | Loss: 0.00000935
Iteration 36/1000 | Loss: 0.00000935
Iteration 37/1000 | Loss: 0.00000934
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000934
Iteration 40/1000 | Loss: 0.00000934
Iteration 41/1000 | Loss: 0.00000934
Iteration 42/1000 | Loss: 0.00000933
Iteration 43/1000 | Loss: 0.00000933
Iteration 44/1000 | Loss: 0.00000933
Iteration 45/1000 | Loss: 0.00000931
Iteration 46/1000 | Loss: 0.00000931
Iteration 47/1000 | Loss: 0.00000931
Iteration 48/1000 | Loss: 0.00000930
Iteration 49/1000 | Loss: 0.00000930
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000930
Iteration 52/1000 | Loss: 0.00000930
Iteration 53/1000 | Loss: 0.00000930
Iteration 54/1000 | Loss: 0.00000930
Iteration 55/1000 | Loss: 0.00000929
Iteration 56/1000 | Loss: 0.00000929
Iteration 57/1000 | Loss: 0.00000929
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000929
Iteration 61/1000 | Loss: 0.00000929
Iteration 62/1000 | Loss: 0.00000928
Iteration 63/1000 | Loss: 0.00000927
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000926
Iteration 66/1000 | Loss: 0.00000925
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000923
Iteration 76/1000 | Loss: 0.00000923
Iteration 77/1000 | Loss: 0.00000922
Iteration 78/1000 | Loss: 0.00000922
Iteration 79/1000 | Loss: 0.00000922
Iteration 80/1000 | Loss: 0.00000922
Iteration 81/1000 | Loss: 0.00000922
Iteration 82/1000 | Loss: 0.00000921
Iteration 83/1000 | Loss: 0.00000921
Iteration 84/1000 | Loss: 0.00000920
Iteration 85/1000 | Loss: 0.00000920
Iteration 86/1000 | Loss: 0.00000919
Iteration 87/1000 | Loss: 0.00000919
Iteration 88/1000 | Loss: 0.00000919
Iteration 89/1000 | Loss: 0.00000919
Iteration 90/1000 | Loss: 0.00000919
Iteration 91/1000 | Loss: 0.00000919
Iteration 92/1000 | Loss: 0.00000919
Iteration 93/1000 | Loss: 0.00000919
Iteration 94/1000 | Loss: 0.00000919
Iteration 95/1000 | Loss: 0.00000919
Iteration 96/1000 | Loss: 0.00000919
Iteration 97/1000 | Loss: 0.00000918
Iteration 98/1000 | Loss: 0.00000918
Iteration 99/1000 | Loss: 0.00000918
Iteration 100/1000 | Loss: 0.00000918
Iteration 101/1000 | Loss: 0.00000918
Iteration 102/1000 | Loss: 0.00000918
Iteration 103/1000 | Loss: 0.00000918
Iteration 104/1000 | Loss: 0.00000918
Iteration 105/1000 | Loss: 0.00000918
Iteration 106/1000 | Loss: 0.00000918
Iteration 107/1000 | Loss: 0.00000918
Iteration 108/1000 | Loss: 0.00000918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [9.18403657124145e-06, 9.18403657124145e-06, 9.18403657124145e-06, 9.18403657124145e-06, 9.18403657124145e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.18403657124145e-06

Optimization complete. Final v2v error: 2.6017701625823975 mm

Highest mean error: 2.934999704360962 mm for frame 75

Lowest mean error: 2.304574728012085 mm for frame 125

Saving results

Total time: 29.657264471054077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992727
Iteration 2/25 | Loss: 0.00202257
Iteration 3/25 | Loss: 0.00147966
Iteration 4/25 | Loss: 0.00139674
Iteration 5/25 | Loss: 0.00139920
Iteration 6/25 | Loss: 0.00139782
Iteration 7/25 | Loss: 0.00122348
Iteration 8/25 | Loss: 0.00115618
Iteration 9/25 | Loss: 0.00112803
Iteration 10/25 | Loss: 0.00111625
Iteration 11/25 | Loss: 0.00111930
Iteration 12/25 | Loss: 0.00111742
Iteration 13/25 | Loss: 0.00111371
Iteration 14/25 | Loss: 0.00111174
Iteration 15/25 | Loss: 0.00110997
Iteration 16/25 | Loss: 0.00110919
Iteration 17/25 | Loss: 0.00110791
Iteration 18/25 | Loss: 0.00110463
Iteration 19/25 | Loss: 0.00110399
Iteration 20/25 | Loss: 0.00110383
Iteration 21/25 | Loss: 0.00110377
Iteration 22/25 | Loss: 0.00110377
Iteration 23/25 | Loss: 0.00110377
Iteration 24/25 | Loss: 0.00110377
Iteration 25/25 | Loss: 0.00110376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44020963
Iteration 2/25 | Loss: 0.00123813
Iteration 3/25 | Loss: 0.00062057
Iteration 4/25 | Loss: 0.00062057
Iteration 5/25 | Loss: 0.00062057
Iteration 6/25 | Loss: 0.00062057
Iteration 7/25 | Loss: 0.00062057
Iteration 8/25 | Loss: 0.00062057
Iteration 9/25 | Loss: 0.00062057
Iteration 10/25 | Loss: 0.00062056
Iteration 11/25 | Loss: 0.00062056
Iteration 12/25 | Loss: 0.00062056
Iteration 13/25 | Loss: 0.00062056
Iteration 14/25 | Loss: 0.00062056
Iteration 15/25 | Loss: 0.00062056
Iteration 16/25 | Loss: 0.00062056
Iteration 17/25 | Loss: 0.00062056
Iteration 18/25 | Loss: 0.00062056
Iteration 19/25 | Loss: 0.00062056
Iteration 20/25 | Loss: 0.00062056
Iteration 21/25 | Loss: 0.00062056
Iteration 22/25 | Loss: 0.00062056
Iteration 23/25 | Loss: 0.00062056
Iteration 24/25 | Loss: 0.00062056
Iteration 25/25 | Loss: 0.00062056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062056
Iteration 2/1000 | Loss: 0.00043872
Iteration 3/1000 | Loss: 0.00006259
Iteration 4/1000 | Loss: 0.00036817
Iteration 5/1000 | Loss: 0.00006668
Iteration 6/1000 | Loss: 0.00032577
Iteration 7/1000 | Loss: 0.00011598
Iteration 8/1000 | Loss: 0.00021226
Iteration 9/1000 | Loss: 0.00044028
Iteration 10/1000 | Loss: 0.00018079
Iteration 11/1000 | Loss: 0.00016314
Iteration 12/1000 | Loss: 0.00019873
Iteration 13/1000 | Loss: 0.00006301
Iteration 14/1000 | Loss: 0.00028306
Iteration 15/1000 | Loss: 0.00005314
Iteration 16/1000 | Loss: 0.00022233
Iteration 17/1000 | Loss: 0.00014069
Iteration 18/1000 | Loss: 0.00020556
Iteration 19/1000 | Loss: 0.00030572
Iteration 20/1000 | Loss: 0.00006755
Iteration 21/1000 | Loss: 0.00004232
Iteration 22/1000 | Loss: 0.00004111
Iteration 23/1000 | Loss: 0.00004018
Iteration 24/1000 | Loss: 0.00003960
Iteration 25/1000 | Loss: 0.00072167
Iteration 26/1000 | Loss: 0.00042089
Iteration 27/1000 | Loss: 0.00052629
Iteration 28/1000 | Loss: 0.00010700
Iteration 29/1000 | Loss: 0.00008197
Iteration 30/1000 | Loss: 0.00005433
Iteration 31/1000 | Loss: 0.00016898
Iteration 32/1000 | Loss: 0.00009448
Iteration 33/1000 | Loss: 0.00013285
Iteration 34/1000 | Loss: 0.00014844
Iteration 35/1000 | Loss: 0.00016641
Iteration 36/1000 | Loss: 0.00039026
Iteration 37/1000 | Loss: 0.00050961
Iteration 38/1000 | Loss: 0.00012022
Iteration 39/1000 | Loss: 0.00004266
Iteration 40/1000 | Loss: 0.00033854
Iteration 41/1000 | Loss: 0.00005072
Iteration 42/1000 | Loss: 0.00004263
Iteration 43/1000 | Loss: 0.00003834
Iteration 44/1000 | Loss: 0.00034885
Iteration 45/1000 | Loss: 0.00032397
Iteration 46/1000 | Loss: 0.00008174
Iteration 47/1000 | Loss: 0.00004495
Iteration 48/1000 | Loss: 0.00003593
Iteration 49/1000 | Loss: 0.00003400
Iteration 50/1000 | Loss: 0.00003285
Iteration 51/1000 | Loss: 0.00003244
Iteration 52/1000 | Loss: 0.00003209
Iteration 53/1000 | Loss: 0.00003180
Iteration 54/1000 | Loss: 0.00003159
Iteration 55/1000 | Loss: 0.00003144
Iteration 56/1000 | Loss: 0.00003140
Iteration 57/1000 | Loss: 0.00003140
Iteration 58/1000 | Loss: 0.00003132
Iteration 59/1000 | Loss: 0.00003132
Iteration 60/1000 | Loss: 0.00003132
Iteration 61/1000 | Loss: 0.00003132
Iteration 62/1000 | Loss: 0.00003132
Iteration 63/1000 | Loss: 0.00003132
Iteration 64/1000 | Loss: 0.00003132
Iteration 65/1000 | Loss: 0.00003132
Iteration 66/1000 | Loss: 0.00003132
Iteration 67/1000 | Loss: 0.00003131
Iteration 68/1000 | Loss: 0.00003131
Iteration 69/1000 | Loss: 0.00003131
Iteration 70/1000 | Loss: 0.00003131
Iteration 71/1000 | Loss: 0.00003131
Iteration 72/1000 | Loss: 0.00003130
Iteration 73/1000 | Loss: 0.00003129
Iteration 74/1000 | Loss: 0.00003129
Iteration 75/1000 | Loss: 0.00003129
Iteration 76/1000 | Loss: 0.00003128
Iteration 77/1000 | Loss: 0.00003128
Iteration 78/1000 | Loss: 0.00003128
Iteration 79/1000 | Loss: 0.00003128
Iteration 80/1000 | Loss: 0.00003128
Iteration 81/1000 | Loss: 0.00003128
Iteration 82/1000 | Loss: 0.00003128
Iteration 83/1000 | Loss: 0.00003128
Iteration 84/1000 | Loss: 0.00003128
Iteration 85/1000 | Loss: 0.00003128
Iteration 86/1000 | Loss: 0.00003127
Iteration 87/1000 | Loss: 0.00003127
Iteration 88/1000 | Loss: 0.00003127
Iteration 89/1000 | Loss: 0.00003127
Iteration 90/1000 | Loss: 0.00003127
Iteration 91/1000 | Loss: 0.00003126
Iteration 92/1000 | Loss: 0.00003126
Iteration 93/1000 | Loss: 0.00003126
Iteration 94/1000 | Loss: 0.00003126
Iteration 95/1000 | Loss: 0.00003126
Iteration 96/1000 | Loss: 0.00003126
Iteration 97/1000 | Loss: 0.00003126
Iteration 98/1000 | Loss: 0.00003126
Iteration 99/1000 | Loss: 0.00003126
Iteration 100/1000 | Loss: 0.00003126
Iteration 101/1000 | Loss: 0.00003126
Iteration 102/1000 | Loss: 0.00003126
Iteration 103/1000 | Loss: 0.00003126
Iteration 104/1000 | Loss: 0.00003126
Iteration 105/1000 | Loss: 0.00003126
Iteration 106/1000 | Loss: 0.00003125
Iteration 107/1000 | Loss: 0.00003125
Iteration 108/1000 | Loss: 0.00003125
Iteration 109/1000 | Loss: 0.00003125
Iteration 110/1000 | Loss: 0.00003125
Iteration 111/1000 | Loss: 0.00003124
Iteration 112/1000 | Loss: 0.00003124
Iteration 113/1000 | Loss: 0.00003124
Iteration 114/1000 | Loss: 0.00003124
Iteration 115/1000 | Loss: 0.00003124
Iteration 116/1000 | Loss: 0.00003124
Iteration 117/1000 | Loss: 0.00003124
Iteration 118/1000 | Loss: 0.00003124
Iteration 119/1000 | Loss: 0.00003124
Iteration 120/1000 | Loss: 0.00003124
Iteration 121/1000 | Loss: 0.00003123
Iteration 122/1000 | Loss: 0.00003123
Iteration 123/1000 | Loss: 0.00003123
Iteration 124/1000 | Loss: 0.00003123
Iteration 125/1000 | Loss: 0.00003123
Iteration 126/1000 | Loss: 0.00003123
Iteration 127/1000 | Loss: 0.00003123
Iteration 128/1000 | Loss: 0.00003123
Iteration 129/1000 | Loss: 0.00003122
Iteration 130/1000 | Loss: 0.00003122
Iteration 131/1000 | Loss: 0.00003122
Iteration 132/1000 | Loss: 0.00003122
Iteration 133/1000 | Loss: 0.00003122
Iteration 134/1000 | Loss: 0.00003122
Iteration 135/1000 | Loss: 0.00003122
Iteration 136/1000 | Loss: 0.00003122
Iteration 137/1000 | Loss: 0.00003122
Iteration 138/1000 | Loss: 0.00003122
Iteration 139/1000 | Loss: 0.00003122
Iteration 140/1000 | Loss: 0.00003122
Iteration 141/1000 | Loss: 0.00003122
Iteration 142/1000 | Loss: 0.00003122
Iteration 143/1000 | Loss: 0.00003122
Iteration 144/1000 | Loss: 0.00003121
Iteration 145/1000 | Loss: 0.00003121
Iteration 146/1000 | Loss: 0.00003121
Iteration 147/1000 | Loss: 0.00003121
Iteration 148/1000 | Loss: 0.00003121
Iteration 149/1000 | Loss: 0.00003121
Iteration 150/1000 | Loss: 0.00003121
Iteration 151/1000 | Loss: 0.00003121
Iteration 152/1000 | Loss: 0.00003121
Iteration 153/1000 | Loss: 0.00003121
Iteration 154/1000 | Loss: 0.00003121
Iteration 155/1000 | Loss: 0.00003121
Iteration 156/1000 | Loss: 0.00003121
Iteration 157/1000 | Loss: 0.00003120
Iteration 158/1000 | Loss: 0.00003120
Iteration 159/1000 | Loss: 0.00003120
Iteration 160/1000 | Loss: 0.00003120
Iteration 161/1000 | Loss: 0.00003120
Iteration 162/1000 | Loss: 0.00003120
Iteration 163/1000 | Loss: 0.00003120
Iteration 164/1000 | Loss: 0.00003120
Iteration 165/1000 | Loss: 0.00003120
Iteration 166/1000 | Loss: 0.00003120
Iteration 167/1000 | Loss: 0.00003120
Iteration 168/1000 | Loss: 0.00003120
Iteration 169/1000 | Loss: 0.00003120
Iteration 170/1000 | Loss: 0.00003120
Iteration 171/1000 | Loss: 0.00003120
Iteration 172/1000 | Loss: 0.00003119
Iteration 173/1000 | Loss: 0.00003119
Iteration 174/1000 | Loss: 0.00003119
Iteration 175/1000 | Loss: 0.00003119
Iteration 176/1000 | Loss: 0.00003119
Iteration 177/1000 | Loss: 0.00003119
Iteration 178/1000 | Loss: 0.00003119
Iteration 179/1000 | Loss: 0.00003119
Iteration 180/1000 | Loss: 0.00003119
Iteration 181/1000 | Loss: 0.00003119
Iteration 182/1000 | Loss: 0.00003119
Iteration 183/1000 | Loss: 0.00003118
Iteration 184/1000 | Loss: 0.00003118
Iteration 185/1000 | Loss: 0.00003118
Iteration 186/1000 | Loss: 0.00003118
Iteration 187/1000 | Loss: 0.00003118
Iteration 188/1000 | Loss: 0.00003118
Iteration 189/1000 | Loss: 0.00003118
Iteration 190/1000 | Loss: 0.00003118
Iteration 191/1000 | Loss: 0.00003118
Iteration 192/1000 | Loss: 0.00003118
Iteration 193/1000 | Loss: 0.00003118
Iteration 194/1000 | Loss: 0.00003118
Iteration 195/1000 | Loss: 0.00003118
Iteration 196/1000 | Loss: 0.00003118
Iteration 197/1000 | Loss: 0.00003118
Iteration 198/1000 | Loss: 0.00003117
Iteration 199/1000 | Loss: 0.00003117
Iteration 200/1000 | Loss: 0.00003117
Iteration 201/1000 | Loss: 0.00003117
Iteration 202/1000 | Loss: 0.00003117
Iteration 203/1000 | Loss: 0.00003117
Iteration 204/1000 | Loss: 0.00003117
Iteration 205/1000 | Loss: 0.00003117
Iteration 206/1000 | Loss: 0.00003117
Iteration 207/1000 | Loss: 0.00003117
Iteration 208/1000 | Loss: 0.00003117
Iteration 209/1000 | Loss: 0.00003117
Iteration 210/1000 | Loss: 0.00003117
Iteration 211/1000 | Loss: 0.00003117
Iteration 212/1000 | Loss: 0.00003117
Iteration 213/1000 | Loss: 0.00003117
Iteration 214/1000 | Loss: 0.00003117
Iteration 215/1000 | Loss: 0.00003117
Iteration 216/1000 | Loss: 0.00003117
Iteration 217/1000 | Loss: 0.00003117
Iteration 218/1000 | Loss: 0.00003117
Iteration 219/1000 | Loss: 0.00003117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [3.117034793831408e-05, 3.117034793831408e-05, 3.117034793831408e-05, 3.117034793831408e-05, 3.117034793831408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.117034793831408e-05

Optimization complete. Final v2v error: 4.126452922821045 mm

Highest mean error: 21.45882225036621 mm for frame 130

Lowest mean error: 3.50380277633667 mm for frame 131

Saving results

Total time: 123.1082615852356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974610
Iteration 2/25 | Loss: 0.00202505
Iteration 3/25 | Loss: 0.00129484
Iteration 4/25 | Loss: 0.00122025
Iteration 5/25 | Loss: 0.00116686
Iteration 6/25 | Loss: 0.00115037
Iteration 7/25 | Loss: 0.00113602
Iteration 8/25 | Loss: 0.00112465
Iteration 9/25 | Loss: 0.00110835
Iteration 10/25 | Loss: 0.00109366
Iteration 11/25 | Loss: 0.00108872
Iteration 12/25 | Loss: 0.00108138
Iteration 13/25 | Loss: 0.00108099
Iteration 14/25 | Loss: 0.00107618
Iteration 15/25 | Loss: 0.00107278
Iteration 16/25 | Loss: 0.00107121
Iteration 17/25 | Loss: 0.00107498
Iteration 18/25 | Loss: 0.00107502
Iteration 19/25 | Loss: 0.00107011
Iteration 20/25 | Loss: 0.00106777
Iteration 21/25 | Loss: 0.00106661
Iteration 22/25 | Loss: 0.00106718
Iteration 23/25 | Loss: 0.00106724
Iteration 24/25 | Loss: 0.00106778
Iteration 25/25 | Loss: 0.00106741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37950754
Iteration 2/25 | Loss: 0.00226845
Iteration 3/25 | Loss: 0.00226845
Iteration 4/25 | Loss: 0.00226844
Iteration 5/25 | Loss: 0.00226844
Iteration 6/25 | Loss: 0.00226844
Iteration 7/25 | Loss: 0.00226844
Iteration 8/25 | Loss: 0.00226844
Iteration 9/25 | Loss: 0.00226844
Iteration 10/25 | Loss: 0.00226844
Iteration 11/25 | Loss: 0.00226844
Iteration 12/25 | Loss: 0.00226844
Iteration 13/25 | Loss: 0.00226844
Iteration 14/25 | Loss: 0.00226844
Iteration 15/25 | Loss: 0.00226844
Iteration 16/25 | Loss: 0.00226844
Iteration 17/25 | Loss: 0.00226844
Iteration 18/25 | Loss: 0.00226844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0022684424184262753, 0.0022684424184262753, 0.0022684424184262753, 0.0022684424184262753, 0.0022684424184262753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022684424184262753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226844
Iteration 2/1000 | Loss: 0.00026229
Iteration 3/1000 | Loss: 0.00222638
Iteration 4/1000 | Loss: 0.00348272
Iteration 5/1000 | Loss: 0.00171808
Iteration 6/1000 | Loss: 0.00258196
Iteration 7/1000 | Loss: 0.00130233
Iteration 8/1000 | Loss: 0.00099185
Iteration 9/1000 | Loss: 0.00015243
Iteration 10/1000 | Loss: 0.00083432
Iteration 11/1000 | Loss: 0.00011677
Iteration 12/1000 | Loss: 0.00113141
Iteration 13/1000 | Loss: 0.00042638
Iteration 14/1000 | Loss: 0.00021272
Iteration 15/1000 | Loss: 0.00071918
Iteration 16/1000 | Loss: 0.00062521
Iteration 17/1000 | Loss: 0.00010466
Iteration 18/1000 | Loss: 0.00151220
Iteration 19/1000 | Loss: 0.00096567
Iteration 20/1000 | Loss: 0.00069267
Iteration 21/1000 | Loss: 0.00165428
Iteration 22/1000 | Loss: 0.00046693
Iteration 23/1000 | Loss: 0.00036687
Iteration 24/1000 | Loss: 0.00018805
Iteration 25/1000 | Loss: 0.00018162
Iteration 26/1000 | Loss: 0.00085038
Iteration 27/1000 | Loss: 0.00068415
Iteration 28/1000 | Loss: 0.00067455
Iteration 29/1000 | Loss: 0.00075097
Iteration 30/1000 | Loss: 0.00121795
Iteration 31/1000 | Loss: 0.00195717
Iteration 32/1000 | Loss: 0.00068549
Iteration 33/1000 | Loss: 0.00022630
Iteration 34/1000 | Loss: 0.00007006
Iteration 35/1000 | Loss: 0.00005820
Iteration 36/1000 | Loss: 0.00005450
Iteration 37/1000 | Loss: 0.00010899
Iteration 38/1000 | Loss: 0.00130768
Iteration 39/1000 | Loss: 0.00035213
Iteration 40/1000 | Loss: 0.00005673
Iteration 41/1000 | Loss: 0.00130668
Iteration 42/1000 | Loss: 0.00011813
Iteration 43/1000 | Loss: 0.00024736
Iteration 44/1000 | Loss: 0.00015262
Iteration 45/1000 | Loss: 0.00004677
Iteration 46/1000 | Loss: 0.00004051
Iteration 47/1000 | Loss: 0.00005638
Iteration 48/1000 | Loss: 0.00117370
Iteration 49/1000 | Loss: 0.00063763
Iteration 50/1000 | Loss: 0.00050321
Iteration 51/1000 | Loss: 0.00011632
Iteration 52/1000 | Loss: 0.00010174
Iteration 53/1000 | Loss: 0.00006045
Iteration 54/1000 | Loss: 0.00013372
Iteration 55/1000 | Loss: 0.00111438
Iteration 56/1000 | Loss: 0.00006876
Iteration 57/1000 | Loss: 0.00003751
Iteration 58/1000 | Loss: 0.00003583
Iteration 59/1000 | Loss: 0.00105281
Iteration 60/1000 | Loss: 0.00036808
Iteration 61/1000 | Loss: 0.00007481
Iteration 62/1000 | Loss: 0.00081897
Iteration 63/1000 | Loss: 0.00011149
Iteration 64/1000 | Loss: 0.00003666
Iteration 65/1000 | Loss: 0.00003428
Iteration 66/1000 | Loss: 0.00091617
Iteration 67/1000 | Loss: 0.00005560
Iteration 68/1000 | Loss: 0.00004443
Iteration 69/1000 | Loss: 0.00003811
Iteration 70/1000 | Loss: 0.00003453
Iteration 71/1000 | Loss: 0.00026128
Iteration 72/1000 | Loss: 0.00021089
Iteration 73/1000 | Loss: 0.00008643
Iteration 74/1000 | Loss: 0.00009797
Iteration 75/1000 | Loss: 0.00015059
Iteration 76/1000 | Loss: 0.00016554
Iteration 77/1000 | Loss: 0.00023149
Iteration 78/1000 | Loss: 0.00016554
Iteration 79/1000 | Loss: 0.00025854
Iteration 80/1000 | Loss: 0.00029328
Iteration 81/1000 | Loss: 0.00026145
Iteration 82/1000 | Loss: 0.00068906
Iteration 83/1000 | Loss: 0.00028536
Iteration 84/1000 | Loss: 0.00024312
Iteration 85/1000 | Loss: 0.00013524
Iteration 86/1000 | Loss: 0.00060365
Iteration 87/1000 | Loss: 0.00017416
Iteration 88/1000 | Loss: 0.00011882
Iteration 89/1000 | Loss: 0.00051758
Iteration 90/1000 | Loss: 0.00024391
Iteration 91/1000 | Loss: 0.00018761
Iteration 92/1000 | Loss: 0.00053899
Iteration 93/1000 | Loss: 0.00026414
Iteration 94/1000 | Loss: 0.00026360
Iteration 95/1000 | Loss: 0.00021386
Iteration 96/1000 | Loss: 0.00025310
Iteration 97/1000 | Loss: 0.00022063
Iteration 98/1000 | Loss: 0.00028318
Iteration 99/1000 | Loss: 0.00023014
Iteration 100/1000 | Loss: 0.00011139
Iteration 101/1000 | Loss: 0.00015155
Iteration 102/1000 | Loss: 0.00013509
Iteration 103/1000 | Loss: 0.00013907
Iteration 104/1000 | Loss: 0.00017864
Iteration 105/1000 | Loss: 0.00025157
Iteration 106/1000 | Loss: 0.00017823
Iteration 107/1000 | Loss: 0.00024452
Iteration 108/1000 | Loss: 0.00015167
Iteration 109/1000 | Loss: 0.00011119
Iteration 110/1000 | Loss: 0.00012761
Iteration 111/1000 | Loss: 0.00008458
Iteration 112/1000 | Loss: 0.00018610
Iteration 113/1000 | Loss: 0.00008433
Iteration 114/1000 | Loss: 0.00012435
Iteration 115/1000 | Loss: 0.00006219
Iteration 116/1000 | Loss: 0.00005712
Iteration 117/1000 | Loss: 0.00006095
Iteration 118/1000 | Loss: 0.00005164
Iteration 119/1000 | Loss: 0.00007722
Iteration 120/1000 | Loss: 0.00011711
Iteration 121/1000 | Loss: 0.00012999
Iteration 122/1000 | Loss: 0.00012692
Iteration 123/1000 | Loss: 0.00017396
Iteration 124/1000 | Loss: 0.00017573
Iteration 125/1000 | Loss: 0.00011412
Iteration 126/1000 | Loss: 0.00013173
Iteration 127/1000 | Loss: 0.00003634
Iteration 128/1000 | Loss: 0.00027038
Iteration 129/1000 | Loss: 0.00014801
Iteration 130/1000 | Loss: 0.00002850
Iteration 131/1000 | Loss: 0.00025283
Iteration 132/1000 | Loss: 0.00005884
Iteration 133/1000 | Loss: 0.00002607
Iteration 134/1000 | Loss: 0.00009913
Iteration 135/1000 | Loss: 0.00002463
Iteration 136/1000 | Loss: 0.00002365
Iteration 137/1000 | Loss: 0.00002336
Iteration 138/1000 | Loss: 0.00002298
Iteration 139/1000 | Loss: 0.00002257
Iteration 140/1000 | Loss: 0.00002220
Iteration 141/1000 | Loss: 0.00002182
Iteration 142/1000 | Loss: 0.00002147
Iteration 143/1000 | Loss: 0.00002129
Iteration 144/1000 | Loss: 0.00002124
Iteration 145/1000 | Loss: 0.00002118
Iteration 146/1000 | Loss: 0.00002113
Iteration 147/1000 | Loss: 0.00002106
Iteration 148/1000 | Loss: 0.00002101
Iteration 149/1000 | Loss: 0.00002101
Iteration 150/1000 | Loss: 0.00002100
Iteration 151/1000 | Loss: 0.00002100
Iteration 152/1000 | Loss: 0.00002098
Iteration 153/1000 | Loss: 0.00002097
Iteration 154/1000 | Loss: 0.00002095
Iteration 155/1000 | Loss: 0.00002094
Iteration 156/1000 | Loss: 0.00002093
Iteration 157/1000 | Loss: 0.00002093
Iteration 158/1000 | Loss: 0.00002093
Iteration 159/1000 | Loss: 0.00002091
Iteration 160/1000 | Loss: 0.00002090
Iteration 161/1000 | Loss: 0.00002090
Iteration 162/1000 | Loss: 0.00002076
Iteration 163/1000 | Loss: 0.00002075
Iteration 164/1000 | Loss: 0.00083622
Iteration 165/1000 | Loss: 0.00003747
Iteration 166/1000 | Loss: 0.00002882
Iteration 167/1000 | Loss: 0.00002047
Iteration 168/1000 | Loss: 0.00001958
Iteration 169/1000 | Loss: 0.00001860
Iteration 170/1000 | Loss: 0.00001816
Iteration 171/1000 | Loss: 0.00001786
Iteration 172/1000 | Loss: 0.00001770
Iteration 173/1000 | Loss: 0.00001755
Iteration 174/1000 | Loss: 0.00001750
Iteration 175/1000 | Loss: 0.00001745
Iteration 176/1000 | Loss: 0.00001743
Iteration 177/1000 | Loss: 0.00001743
Iteration 178/1000 | Loss: 0.00001742
Iteration 179/1000 | Loss: 0.00001742
Iteration 180/1000 | Loss: 0.00001742
Iteration 181/1000 | Loss: 0.00001741
Iteration 182/1000 | Loss: 0.00001741
Iteration 183/1000 | Loss: 0.00001741
Iteration 184/1000 | Loss: 0.00001741
Iteration 185/1000 | Loss: 0.00001740
Iteration 186/1000 | Loss: 0.00001740
Iteration 187/1000 | Loss: 0.00001740
Iteration 188/1000 | Loss: 0.00001740
Iteration 189/1000 | Loss: 0.00001740
Iteration 190/1000 | Loss: 0.00001740
Iteration 191/1000 | Loss: 0.00001740
Iteration 192/1000 | Loss: 0.00001740
Iteration 193/1000 | Loss: 0.00001740
Iteration 194/1000 | Loss: 0.00001739
Iteration 195/1000 | Loss: 0.00001739
Iteration 196/1000 | Loss: 0.00001739
Iteration 197/1000 | Loss: 0.00001739
Iteration 198/1000 | Loss: 0.00001739
Iteration 199/1000 | Loss: 0.00001739
Iteration 200/1000 | Loss: 0.00001738
Iteration 201/1000 | Loss: 0.00001738
Iteration 202/1000 | Loss: 0.00001738
Iteration 203/1000 | Loss: 0.00001738
Iteration 204/1000 | Loss: 0.00001738
Iteration 205/1000 | Loss: 0.00001738
Iteration 206/1000 | Loss: 0.00001738
Iteration 207/1000 | Loss: 0.00001738
Iteration 208/1000 | Loss: 0.00001737
Iteration 209/1000 | Loss: 0.00001737
Iteration 210/1000 | Loss: 0.00001737
Iteration 211/1000 | Loss: 0.00001737
Iteration 212/1000 | Loss: 0.00001737
Iteration 213/1000 | Loss: 0.00001737
Iteration 214/1000 | Loss: 0.00001737
Iteration 215/1000 | Loss: 0.00001737
Iteration 216/1000 | Loss: 0.00001737
Iteration 217/1000 | Loss: 0.00001736
Iteration 218/1000 | Loss: 0.00001736
Iteration 219/1000 | Loss: 0.00001736
Iteration 220/1000 | Loss: 0.00001736
Iteration 221/1000 | Loss: 0.00001735
Iteration 222/1000 | Loss: 0.00001735
Iteration 223/1000 | Loss: 0.00001735
Iteration 224/1000 | Loss: 0.00001735
Iteration 225/1000 | Loss: 0.00001734
Iteration 226/1000 | Loss: 0.00001734
Iteration 227/1000 | Loss: 0.00001734
Iteration 228/1000 | Loss: 0.00001733
Iteration 229/1000 | Loss: 0.00001733
Iteration 230/1000 | Loss: 0.00001733
Iteration 231/1000 | Loss: 0.00001733
Iteration 232/1000 | Loss: 0.00001733
Iteration 233/1000 | Loss: 0.00001733
Iteration 234/1000 | Loss: 0.00001733
Iteration 235/1000 | Loss: 0.00001733
Iteration 236/1000 | Loss: 0.00001733
Iteration 237/1000 | Loss: 0.00001733
Iteration 238/1000 | Loss: 0.00001733
Iteration 239/1000 | Loss: 0.00001732
Iteration 240/1000 | Loss: 0.00001732
Iteration 241/1000 | Loss: 0.00001732
Iteration 242/1000 | Loss: 0.00001732
Iteration 243/1000 | Loss: 0.00001732
Iteration 244/1000 | Loss: 0.00001732
Iteration 245/1000 | Loss: 0.00001732
Iteration 246/1000 | Loss: 0.00001732
Iteration 247/1000 | Loss: 0.00001732
Iteration 248/1000 | Loss: 0.00001732
Iteration 249/1000 | Loss: 0.00001731
Iteration 250/1000 | Loss: 0.00001731
Iteration 251/1000 | Loss: 0.00001731
Iteration 252/1000 | Loss: 0.00001731
Iteration 253/1000 | Loss: 0.00001731
Iteration 254/1000 | Loss: 0.00001731
Iteration 255/1000 | Loss: 0.00001731
Iteration 256/1000 | Loss: 0.00001731
Iteration 257/1000 | Loss: 0.00001731
Iteration 258/1000 | Loss: 0.00001731
Iteration 259/1000 | Loss: 0.00001731
Iteration 260/1000 | Loss: 0.00001730
Iteration 261/1000 | Loss: 0.00001730
Iteration 262/1000 | Loss: 0.00001730
Iteration 263/1000 | Loss: 0.00001730
Iteration 264/1000 | Loss: 0.00001730
Iteration 265/1000 | Loss: 0.00001730
Iteration 266/1000 | Loss: 0.00001730
Iteration 267/1000 | Loss: 0.00001730
Iteration 268/1000 | Loss: 0.00001730
Iteration 269/1000 | Loss: 0.00001730
Iteration 270/1000 | Loss: 0.00001730
Iteration 271/1000 | Loss: 0.00001729
Iteration 272/1000 | Loss: 0.00001729
Iteration 273/1000 | Loss: 0.00001729
Iteration 274/1000 | Loss: 0.00001729
Iteration 275/1000 | Loss: 0.00001729
Iteration 276/1000 | Loss: 0.00001729
Iteration 277/1000 | Loss: 0.00001729
Iteration 278/1000 | Loss: 0.00001729
Iteration 279/1000 | Loss: 0.00001729
Iteration 280/1000 | Loss: 0.00001729
Iteration 281/1000 | Loss: 0.00001729
Iteration 282/1000 | Loss: 0.00001729
Iteration 283/1000 | Loss: 0.00001729
Iteration 284/1000 | Loss: 0.00001728
Iteration 285/1000 | Loss: 0.00001728
Iteration 286/1000 | Loss: 0.00001728
Iteration 287/1000 | Loss: 0.00001728
Iteration 288/1000 | Loss: 0.00001728
Iteration 289/1000 | Loss: 0.00001728
Iteration 290/1000 | Loss: 0.00001728
Iteration 291/1000 | Loss: 0.00001728
Iteration 292/1000 | Loss: 0.00001728
Iteration 293/1000 | Loss: 0.00001728
Iteration 294/1000 | Loss: 0.00001728
Iteration 295/1000 | Loss: 0.00001728
Iteration 296/1000 | Loss: 0.00001728
Iteration 297/1000 | Loss: 0.00001728
Iteration 298/1000 | Loss: 0.00001728
Iteration 299/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [1.728338611428626e-05, 1.728338611428626e-05, 1.728338611428626e-05, 1.728338611428626e-05, 1.728338611428626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.728338611428626e-05

Optimization complete. Final v2v error: 3.033559560775757 mm

Highest mean error: 12.887468338012695 mm for frame 54

Lowest mean error: 2.1990177631378174 mm for frame 3

Saving results

Total time: 279.5804874897003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496207
Iteration 2/25 | Loss: 0.00124646
Iteration 3/25 | Loss: 0.00098354
Iteration 4/25 | Loss: 0.00094896
Iteration 5/25 | Loss: 0.00094434
Iteration 6/25 | Loss: 0.00094302
Iteration 7/25 | Loss: 0.00094302
Iteration 8/25 | Loss: 0.00094302
Iteration 9/25 | Loss: 0.00094302
Iteration 10/25 | Loss: 0.00094302
Iteration 11/25 | Loss: 0.00094302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009430176578462124, 0.0009430176578462124, 0.0009430176578462124, 0.0009430176578462124, 0.0009430176578462124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009430176578462124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46336412
Iteration 2/25 | Loss: 0.00043287
Iteration 3/25 | Loss: 0.00043285
Iteration 4/25 | Loss: 0.00043285
Iteration 5/25 | Loss: 0.00043285
Iteration 6/25 | Loss: 0.00043285
Iteration 7/25 | Loss: 0.00043285
Iteration 8/25 | Loss: 0.00043285
Iteration 9/25 | Loss: 0.00043285
Iteration 10/25 | Loss: 0.00043285
Iteration 11/25 | Loss: 0.00043285
Iteration 12/25 | Loss: 0.00043285
Iteration 13/25 | Loss: 0.00043285
Iteration 14/25 | Loss: 0.00043285
Iteration 15/25 | Loss: 0.00043285
Iteration 16/25 | Loss: 0.00043285
Iteration 17/25 | Loss: 0.00043285
Iteration 18/25 | Loss: 0.00043285
Iteration 19/25 | Loss: 0.00043285
Iteration 20/25 | Loss: 0.00043285
Iteration 21/25 | Loss: 0.00043285
Iteration 22/25 | Loss: 0.00043285
Iteration 23/25 | Loss: 0.00043285
Iteration 24/25 | Loss: 0.00043285
Iteration 25/25 | Loss: 0.00043285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043285
Iteration 2/1000 | Loss: 0.00003871
Iteration 3/1000 | Loss: 0.00002360
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001735
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001591
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001484
Iteration 13/1000 | Loss: 0.00001476
Iteration 14/1000 | Loss: 0.00001473
Iteration 15/1000 | Loss: 0.00001472
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001459
Iteration 22/1000 | Loss: 0.00001459
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001458
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001458
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001458
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001457
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001452
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001448
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001447
Iteration 69/1000 | Loss: 0.00001447
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001446
Iteration 96/1000 | Loss: 0.00001446
Iteration 97/1000 | Loss: 0.00001446
Iteration 98/1000 | Loss: 0.00001446
Iteration 99/1000 | Loss: 0.00001446
Iteration 100/1000 | Loss: 0.00001446
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.4455743439611979e-05, 1.4455743439611979e-05, 1.4455743439611979e-05, 1.4455743439611979e-05, 1.4455743439611979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4455743439611979e-05

Optimization complete. Final v2v error: 3.2051162719726562 mm

Highest mean error: 4.066244602203369 mm for frame 133

Lowest mean error: 2.650433301925659 mm for frame 1

Saving results

Total time: 33.74063754081726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074312
Iteration 2/25 | Loss: 0.00253019
Iteration 3/25 | Loss: 0.00179465
Iteration 4/25 | Loss: 0.00159472
Iteration 5/25 | Loss: 0.00142062
Iteration 6/25 | Loss: 0.00124961
Iteration 7/25 | Loss: 0.00111937
Iteration 8/25 | Loss: 0.00107585
Iteration 9/25 | Loss: 0.00106776
Iteration 10/25 | Loss: 0.00106566
Iteration 11/25 | Loss: 0.00106493
Iteration 12/25 | Loss: 0.00106443
Iteration 13/25 | Loss: 0.00106349
Iteration 14/25 | Loss: 0.00106627
Iteration 15/25 | Loss: 0.00107715
Iteration 16/25 | Loss: 0.00106795
Iteration 17/25 | Loss: 0.00106320
Iteration 18/25 | Loss: 0.00106361
Iteration 19/25 | Loss: 0.00106243
Iteration 20/25 | Loss: 0.00106304
Iteration 21/25 | Loss: 0.00106195
Iteration 22/25 | Loss: 0.00106161
Iteration 23/25 | Loss: 0.00106184
Iteration 24/25 | Loss: 0.00106210
Iteration 25/25 | Loss: 0.00106160

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45671654
Iteration 2/25 | Loss: 0.00045562
Iteration 3/25 | Loss: 0.00045562
Iteration 4/25 | Loss: 0.00045562
Iteration 5/25 | Loss: 0.00045562
Iteration 6/25 | Loss: 0.00045562
Iteration 7/25 | Loss: 0.00045562
Iteration 8/25 | Loss: 0.00045562
Iteration 9/25 | Loss: 0.00045561
Iteration 10/25 | Loss: 0.00045561
Iteration 11/25 | Loss: 0.00045561
Iteration 12/25 | Loss: 0.00045561
Iteration 13/25 | Loss: 0.00045561
Iteration 14/25 | Loss: 0.00045561
Iteration 15/25 | Loss: 0.00045561
Iteration 16/25 | Loss: 0.00045561
Iteration 17/25 | Loss: 0.00045561
Iteration 18/25 | Loss: 0.00045561
Iteration 19/25 | Loss: 0.00045561
Iteration 20/25 | Loss: 0.00045561
Iteration 21/25 | Loss: 0.00045561
Iteration 22/25 | Loss: 0.00045561
Iteration 23/25 | Loss: 0.00045561
Iteration 24/25 | Loss: 0.00045561
Iteration 25/25 | Loss: 0.00045561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045561
Iteration 2/1000 | Loss: 0.00131282
Iteration 3/1000 | Loss: 0.00030289
Iteration 4/1000 | Loss: 0.00012958
Iteration 5/1000 | Loss: 0.00008479
Iteration 6/1000 | Loss: 0.00006629
Iteration 7/1000 | Loss: 0.00005094
Iteration 8/1000 | Loss: 0.00004314
Iteration 9/1000 | Loss: 0.00021045
Iteration 10/1000 | Loss: 0.00010692
Iteration 11/1000 | Loss: 0.00003653
Iteration 12/1000 | Loss: 0.00003487
Iteration 13/1000 | Loss: 0.00019014
Iteration 14/1000 | Loss: 0.00004291
Iteration 15/1000 | Loss: 0.00003803
Iteration 16/1000 | Loss: 0.00003409
Iteration 17/1000 | Loss: 0.00003248
Iteration 18/1000 | Loss: 0.00003113
Iteration 19/1000 | Loss: 0.00003043
Iteration 20/1000 | Loss: 0.00003066
Iteration 21/1000 | Loss: 0.00002960
Iteration 22/1000 | Loss: 0.00002912
Iteration 23/1000 | Loss: 0.00002899
Iteration 24/1000 | Loss: 0.00002875
Iteration 25/1000 | Loss: 0.00002869
Iteration 26/1000 | Loss: 0.00002854
Iteration 27/1000 | Loss: 0.00002835
Iteration 28/1000 | Loss: 0.00004187
Iteration 29/1000 | Loss: 0.00004086
Iteration 30/1000 | Loss: 0.00002869
Iteration 31/1000 | Loss: 0.00004028
Iteration 32/1000 | Loss: 0.00003785
Iteration 33/1000 | Loss: 0.00003194
Iteration 34/1000 | Loss: 0.00002977
Iteration 35/1000 | Loss: 0.00002877
Iteration 36/1000 | Loss: 0.00003616
Iteration 37/1000 | Loss: 0.00003187
Iteration 38/1000 | Loss: 0.00003633
Iteration 39/1000 | Loss: 0.00003042
Iteration 40/1000 | Loss: 0.00003764
Iteration 41/1000 | Loss: 0.00002929
Iteration 42/1000 | Loss: 0.00002829
Iteration 43/1000 | Loss: 0.00002880
Iteration 44/1000 | Loss: 0.00003066
Iteration 45/1000 | Loss: 0.00002804
Iteration 46/1000 | Loss: 0.00002803
Iteration 47/1000 | Loss: 0.00002803
Iteration 48/1000 | Loss: 0.00003205
Iteration 49/1000 | Loss: 0.00002849
Iteration 50/1000 | Loss: 0.00002825
Iteration 51/1000 | Loss: 0.00002813
Iteration 52/1000 | Loss: 0.00002796
Iteration 53/1000 | Loss: 0.00002793
Iteration 54/1000 | Loss: 0.00002791
Iteration 55/1000 | Loss: 0.00002791
Iteration 56/1000 | Loss: 0.00002791
Iteration 57/1000 | Loss: 0.00002790
Iteration 58/1000 | Loss: 0.00002790
Iteration 59/1000 | Loss: 0.00002789
Iteration 60/1000 | Loss: 0.00002789
Iteration 61/1000 | Loss: 0.00002788
Iteration 62/1000 | Loss: 0.00002788
Iteration 63/1000 | Loss: 0.00002788
Iteration 64/1000 | Loss: 0.00002788
Iteration 65/1000 | Loss: 0.00002788
Iteration 66/1000 | Loss: 0.00002788
Iteration 67/1000 | Loss: 0.00002788
Iteration 68/1000 | Loss: 0.00002788
Iteration 69/1000 | Loss: 0.00002788
Iteration 70/1000 | Loss: 0.00002788
Iteration 71/1000 | Loss: 0.00002787
Iteration 72/1000 | Loss: 0.00002787
Iteration 73/1000 | Loss: 0.00002787
Iteration 74/1000 | Loss: 0.00002787
Iteration 75/1000 | Loss: 0.00002787
Iteration 76/1000 | Loss: 0.00002787
Iteration 77/1000 | Loss: 0.00002787
Iteration 78/1000 | Loss: 0.00002786
Iteration 79/1000 | Loss: 0.00002786
Iteration 80/1000 | Loss: 0.00002783
Iteration 81/1000 | Loss: 0.00002783
Iteration 82/1000 | Loss: 0.00002783
Iteration 83/1000 | Loss: 0.00002783
Iteration 84/1000 | Loss: 0.00002783
Iteration 85/1000 | Loss: 0.00002782
Iteration 86/1000 | Loss: 0.00002782
Iteration 87/1000 | Loss: 0.00002782
Iteration 88/1000 | Loss: 0.00002782
Iteration 89/1000 | Loss: 0.00002782
Iteration 90/1000 | Loss: 0.00002782
Iteration 91/1000 | Loss: 0.00002782
Iteration 92/1000 | Loss: 0.00003476
Iteration 93/1000 | Loss: 0.00002803
Iteration 94/1000 | Loss: 0.00003438
Iteration 95/1000 | Loss: 0.00002804
Iteration 96/1000 | Loss: 0.00002802
Iteration 97/1000 | Loss: 0.00002802
Iteration 98/1000 | Loss: 0.00002801
Iteration 99/1000 | Loss: 0.00002801
Iteration 100/1000 | Loss: 0.00002800
Iteration 101/1000 | Loss: 0.00002799
Iteration 102/1000 | Loss: 0.00002793
Iteration 103/1000 | Loss: 0.00002792
Iteration 104/1000 | Loss: 0.00002791
Iteration 105/1000 | Loss: 0.00002791
Iteration 106/1000 | Loss: 0.00002786
Iteration 107/1000 | Loss: 0.00002786
Iteration 108/1000 | Loss: 0.00002785
Iteration 109/1000 | Loss: 0.00002785
Iteration 110/1000 | Loss: 0.00002784
Iteration 111/1000 | Loss: 0.00002784
Iteration 112/1000 | Loss: 0.00002783
Iteration 113/1000 | Loss: 0.00002783
Iteration 114/1000 | Loss: 0.00002783
Iteration 115/1000 | Loss: 0.00002783
Iteration 116/1000 | Loss: 0.00002783
Iteration 117/1000 | Loss: 0.00002783
Iteration 118/1000 | Loss: 0.00002783
Iteration 119/1000 | Loss: 0.00002783
Iteration 120/1000 | Loss: 0.00002782
Iteration 121/1000 | Loss: 0.00002782
Iteration 122/1000 | Loss: 0.00002782
Iteration 123/1000 | Loss: 0.00002782
Iteration 124/1000 | Loss: 0.00002782
Iteration 125/1000 | Loss: 0.00002781
Iteration 126/1000 | Loss: 0.00002781
Iteration 127/1000 | Loss: 0.00002781
Iteration 128/1000 | Loss: 0.00002780
Iteration 129/1000 | Loss: 0.00002780
Iteration 130/1000 | Loss: 0.00002780
Iteration 131/1000 | Loss: 0.00002779
Iteration 132/1000 | Loss: 0.00002779
Iteration 133/1000 | Loss: 0.00003153
Iteration 134/1000 | Loss: 0.00002795
Iteration 135/1000 | Loss: 0.00002795
Iteration 136/1000 | Loss: 0.00003083
Iteration 137/1000 | Loss: 0.00002795
Iteration 138/1000 | Loss: 0.00002795
Iteration 139/1000 | Loss: 0.00002795
Iteration 140/1000 | Loss: 0.00002779
Iteration 141/1000 | Loss: 0.00002778
Iteration 142/1000 | Loss: 0.00003120
Iteration 143/1000 | Loss: 0.00002796
Iteration 144/1000 | Loss: 0.00003202
Iteration 145/1000 | Loss: 0.00002803
Iteration 146/1000 | Loss: 0.00002777
Iteration 147/1000 | Loss: 0.00002777
Iteration 148/1000 | Loss: 0.00002772
Iteration 149/1000 | Loss: 0.00002771
Iteration 150/1000 | Loss: 0.00002770
Iteration 151/1000 | Loss: 0.00002770
Iteration 152/1000 | Loss: 0.00002769
Iteration 153/1000 | Loss: 0.00002769
Iteration 154/1000 | Loss: 0.00002769
Iteration 155/1000 | Loss: 0.00002769
Iteration 156/1000 | Loss: 0.00002768
Iteration 157/1000 | Loss: 0.00002768
Iteration 158/1000 | Loss: 0.00002768
Iteration 159/1000 | Loss: 0.00002768
Iteration 160/1000 | Loss: 0.00002768
Iteration 161/1000 | Loss: 0.00002768
Iteration 162/1000 | Loss: 0.00002768
Iteration 163/1000 | Loss: 0.00002767
Iteration 164/1000 | Loss: 0.00002767
Iteration 165/1000 | Loss: 0.00002767
Iteration 166/1000 | Loss: 0.00002767
Iteration 167/1000 | Loss: 0.00002767
Iteration 168/1000 | Loss: 0.00002767
Iteration 169/1000 | Loss: 0.00002766
Iteration 170/1000 | Loss: 0.00002766
Iteration 171/1000 | Loss: 0.00002766
Iteration 172/1000 | Loss: 0.00002766
Iteration 173/1000 | Loss: 0.00002766
Iteration 174/1000 | Loss: 0.00002766
Iteration 175/1000 | Loss: 0.00002766
Iteration 176/1000 | Loss: 0.00002766
Iteration 177/1000 | Loss: 0.00002766
Iteration 178/1000 | Loss: 0.00002766
Iteration 179/1000 | Loss: 0.00002766
Iteration 180/1000 | Loss: 0.00002766
Iteration 181/1000 | Loss: 0.00002766
Iteration 182/1000 | Loss: 0.00002766
Iteration 183/1000 | Loss: 0.00002766
Iteration 184/1000 | Loss: 0.00002766
Iteration 185/1000 | Loss: 0.00002766
Iteration 186/1000 | Loss: 0.00002766
Iteration 187/1000 | Loss: 0.00002766
Iteration 188/1000 | Loss: 0.00002766
Iteration 189/1000 | Loss: 0.00002766
Iteration 190/1000 | Loss: 0.00002766
Iteration 191/1000 | Loss: 0.00002766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.766297257039696e-05, 2.766297257039696e-05, 2.766297257039696e-05, 2.766297257039696e-05, 2.766297257039696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.766297257039696e-05

Optimization complete. Final v2v error: 3.8356199264526367 mm

Highest mean error: 20.881885528564453 mm for frame 5

Lowest mean error: 3.0784740447998047 mm for frame 214

Saving results

Total time: 154.8805491924286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514333
Iteration 2/25 | Loss: 0.00103832
Iteration 3/25 | Loss: 0.00092596
Iteration 4/25 | Loss: 0.00090514
Iteration 5/25 | Loss: 0.00089796
Iteration 6/25 | Loss: 0.00089582
Iteration 7/25 | Loss: 0.00089511
Iteration 8/25 | Loss: 0.00089511
Iteration 9/25 | Loss: 0.00089511
Iteration 10/25 | Loss: 0.00089511
Iteration 11/25 | Loss: 0.00089511
Iteration 12/25 | Loss: 0.00089511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008951079798862338, 0.0008951079798862338, 0.0008951079798862338, 0.0008951079798862338, 0.0008951079798862338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008951079798862338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59056389
Iteration 2/25 | Loss: 0.00043787
Iteration 3/25 | Loss: 0.00043787
Iteration 4/25 | Loss: 0.00043787
Iteration 5/25 | Loss: 0.00043787
Iteration 6/25 | Loss: 0.00043787
Iteration 7/25 | Loss: 0.00043787
Iteration 8/25 | Loss: 0.00043787
Iteration 9/25 | Loss: 0.00043786
Iteration 10/25 | Loss: 0.00043786
Iteration 11/25 | Loss: 0.00043786
Iteration 12/25 | Loss: 0.00043786
Iteration 13/25 | Loss: 0.00043786
Iteration 14/25 | Loss: 0.00043786
Iteration 15/25 | Loss: 0.00043786
Iteration 16/25 | Loss: 0.00043786
Iteration 17/25 | Loss: 0.00043786
Iteration 18/25 | Loss: 0.00043786
Iteration 19/25 | Loss: 0.00043786
Iteration 20/25 | Loss: 0.00043786
Iteration 21/25 | Loss: 0.00043786
Iteration 22/25 | Loss: 0.00043786
Iteration 23/25 | Loss: 0.00043786
Iteration 24/25 | Loss: 0.00043786
Iteration 25/25 | Loss: 0.00043786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043786
Iteration 2/1000 | Loss: 0.00002007
Iteration 3/1000 | Loss: 0.00001385
Iteration 4/1000 | Loss: 0.00001246
Iteration 5/1000 | Loss: 0.00001146
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001069
Iteration 8/1000 | Loss: 0.00001053
Iteration 9/1000 | Loss: 0.00001052
Iteration 10/1000 | Loss: 0.00001051
Iteration 11/1000 | Loss: 0.00001051
Iteration 12/1000 | Loss: 0.00001048
Iteration 13/1000 | Loss: 0.00001037
Iteration 14/1000 | Loss: 0.00001037
Iteration 15/1000 | Loss: 0.00001036
Iteration 16/1000 | Loss: 0.00001036
Iteration 17/1000 | Loss: 0.00001035
Iteration 18/1000 | Loss: 0.00001035
Iteration 19/1000 | Loss: 0.00001035
Iteration 20/1000 | Loss: 0.00001033
Iteration 21/1000 | Loss: 0.00001032
Iteration 22/1000 | Loss: 0.00001031
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001031
Iteration 25/1000 | Loss: 0.00001031
Iteration 26/1000 | Loss: 0.00001031
Iteration 27/1000 | Loss: 0.00001030
Iteration 28/1000 | Loss: 0.00001030
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001030
Iteration 32/1000 | Loss: 0.00001030
Iteration 33/1000 | Loss: 0.00001030
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001028
Iteration 36/1000 | Loss: 0.00001028
Iteration 37/1000 | Loss: 0.00001028
Iteration 38/1000 | Loss: 0.00001027
Iteration 39/1000 | Loss: 0.00001027
Iteration 40/1000 | Loss: 0.00001027
Iteration 41/1000 | Loss: 0.00001027
Iteration 42/1000 | Loss: 0.00001027
Iteration 43/1000 | Loss: 0.00001027
Iteration 44/1000 | Loss: 0.00001027
Iteration 45/1000 | Loss: 0.00001026
Iteration 46/1000 | Loss: 0.00001026
Iteration 47/1000 | Loss: 0.00001026
Iteration 48/1000 | Loss: 0.00001026
Iteration 49/1000 | Loss: 0.00001026
Iteration 50/1000 | Loss: 0.00001026
Iteration 51/1000 | Loss: 0.00001025
Iteration 52/1000 | Loss: 0.00001025
Iteration 53/1000 | Loss: 0.00001025
Iteration 54/1000 | Loss: 0.00001025
Iteration 55/1000 | Loss: 0.00001025
Iteration 56/1000 | Loss: 0.00001025
Iteration 57/1000 | Loss: 0.00001024
Iteration 58/1000 | Loss: 0.00001024
Iteration 59/1000 | Loss: 0.00001024
Iteration 60/1000 | Loss: 0.00001024
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001022
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001021
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001021
Iteration 81/1000 | Loss: 0.00001021
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.0199796633969527e-05, 1.0199796633969527e-05, 1.0199796633969527e-05, 1.0199796633969527e-05, 1.0199796633969527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0199796633969527e-05

Optimization complete. Final v2v error: 2.7130749225616455 mm

Highest mean error: 3.180903911590576 mm for frame 34

Lowest mean error: 2.2873284816741943 mm for frame 145

Saving results

Total time: 29.118242740631104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040454
Iteration 2/25 | Loss: 0.00198487
Iteration 3/25 | Loss: 0.00163775
Iteration 4/25 | Loss: 0.00134931
Iteration 5/25 | Loss: 0.00138357
Iteration 6/25 | Loss: 0.00132377
Iteration 7/25 | Loss: 0.00124420
Iteration 8/25 | Loss: 0.00105205
Iteration 9/25 | Loss: 0.00102007
Iteration 10/25 | Loss: 0.00102107
Iteration 11/25 | Loss: 0.00101809
Iteration 12/25 | Loss: 0.00101113
Iteration 13/25 | Loss: 0.00101254
Iteration 14/25 | Loss: 0.00101649
Iteration 15/25 | Loss: 0.00101088
Iteration 16/25 | Loss: 0.00101195
Iteration 17/25 | Loss: 0.00100962
Iteration 18/25 | Loss: 0.00100900
Iteration 19/25 | Loss: 0.00100584
Iteration 20/25 | Loss: 0.00100749
Iteration 21/25 | Loss: 0.00101231
Iteration 22/25 | Loss: 0.00100843
Iteration 23/25 | Loss: 0.00101175
Iteration 24/25 | Loss: 0.00100739
Iteration 25/25 | Loss: 0.00100902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86302471
Iteration 2/25 | Loss: 0.00122656
Iteration 3/25 | Loss: 0.00122656
Iteration 4/25 | Loss: 0.00122656
Iteration 5/25 | Loss: 0.00122656
Iteration 6/25 | Loss: 0.00122656
Iteration 7/25 | Loss: 0.00122656
Iteration 8/25 | Loss: 0.00122656
Iteration 9/25 | Loss: 0.00122655
Iteration 10/25 | Loss: 0.00122655
Iteration 11/25 | Loss: 0.00122655
Iteration 12/25 | Loss: 0.00122655
Iteration 13/25 | Loss: 0.00122655
Iteration 14/25 | Loss: 0.00122655
Iteration 15/25 | Loss: 0.00122655
Iteration 16/25 | Loss: 0.00122655
Iteration 17/25 | Loss: 0.00122655
Iteration 18/25 | Loss: 0.00122655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012265547411516309, 0.0012265547411516309, 0.0012265547411516309, 0.0012265547411516309, 0.0012265547411516309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012265547411516309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122655
Iteration 2/1000 | Loss: 0.00051994
Iteration 3/1000 | Loss: 0.00037747
Iteration 4/1000 | Loss: 0.00005914
Iteration 5/1000 | Loss: 0.00004988
Iteration 6/1000 | Loss: 0.00018197
Iteration 7/1000 | Loss: 0.00018039
Iteration 8/1000 | Loss: 0.00003664
Iteration 9/1000 | Loss: 0.00003206
Iteration 10/1000 | Loss: 0.00002717
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00001884
Iteration 13/1000 | Loss: 0.00001723
Iteration 14/1000 | Loss: 0.00001601
Iteration 15/1000 | Loss: 0.00001540
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001426
Iteration 21/1000 | Loss: 0.00001422
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001419
Iteration 24/1000 | Loss: 0.00001419
Iteration 25/1000 | Loss: 0.00001418
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001417
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001416
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001414
Iteration 35/1000 | Loss: 0.00001413
Iteration 36/1000 | Loss: 0.00001413
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001412
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001410
Iteration 43/1000 | Loss: 0.00001410
Iteration 44/1000 | Loss: 0.00001410
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001409
Iteration 47/1000 | Loss: 0.00001409
Iteration 48/1000 | Loss: 0.00001408
Iteration 49/1000 | Loss: 0.00001408
Iteration 50/1000 | Loss: 0.00001408
Iteration 51/1000 | Loss: 0.00001408
Iteration 52/1000 | Loss: 0.00001408
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001407
Iteration 55/1000 | Loss: 0.00001407
Iteration 56/1000 | Loss: 0.00001407
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001405
Iteration 67/1000 | Loss: 0.00001405
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001404
Iteration 70/1000 | Loss: 0.00001404
Iteration 71/1000 | Loss: 0.00001404
Iteration 72/1000 | Loss: 0.00001404
Iteration 73/1000 | Loss: 0.00001403
Iteration 74/1000 | Loss: 0.00001403
Iteration 75/1000 | Loss: 0.00001403
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001403
Iteration 78/1000 | Loss: 0.00001403
Iteration 79/1000 | Loss: 0.00001403
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001402
Iteration 83/1000 | Loss: 0.00001402
Iteration 84/1000 | Loss: 0.00001402
Iteration 85/1000 | Loss: 0.00001402
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001401
Iteration 90/1000 | Loss: 0.00001401
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001401
Iteration 96/1000 | Loss: 0.00001401
Iteration 97/1000 | Loss: 0.00001401
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001400
Iteration 101/1000 | Loss: 0.00001400
Iteration 102/1000 | Loss: 0.00001400
Iteration 103/1000 | Loss: 0.00001400
Iteration 104/1000 | Loss: 0.00001400
Iteration 105/1000 | Loss: 0.00001400
Iteration 106/1000 | Loss: 0.00001400
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001399
Iteration 113/1000 | Loss: 0.00001399
Iteration 114/1000 | Loss: 0.00001399
Iteration 115/1000 | Loss: 0.00001399
Iteration 116/1000 | Loss: 0.00001399
Iteration 117/1000 | Loss: 0.00001399
Iteration 118/1000 | Loss: 0.00001399
Iteration 119/1000 | Loss: 0.00001399
Iteration 120/1000 | Loss: 0.00001399
Iteration 121/1000 | Loss: 0.00001399
Iteration 122/1000 | Loss: 0.00001399
Iteration 123/1000 | Loss: 0.00001399
Iteration 124/1000 | Loss: 0.00001399
Iteration 125/1000 | Loss: 0.00001398
Iteration 126/1000 | Loss: 0.00001398
Iteration 127/1000 | Loss: 0.00001398
Iteration 128/1000 | Loss: 0.00001398
Iteration 129/1000 | Loss: 0.00001398
Iteration 130/1000 | Loss: 0.00001397
Iteration 131/1000 | Loss: 0.00001397
Iteration 132/1000 | Loss: 0.00001397
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001395
Iteration 141/1000 | Loss: 0.00001395
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001394
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001393
Iteration 163/1000 | Loss: 0.00001393
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001393
Iteration 169/1000 | Loss: 0.00001393
Iteration 170/1000 | Loss: 0.00001393
Iteration 171/1000 | Loss: 0.00001393
Iteration 172/1000 | Loss: 0.00001393
Iteration 173/1000 | Loss: 0.00001393
Iteration 174/1000 | Loss: 0.00001393
Iteration 175/1000 | Loss: 0.00001393
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001393
Iteration 179/1000 | Loss: 0.00001393
Iteration 180/1000 | Loss: 0.00001393
Iteration 181/1000 | Loss: 0.00001393
Iteration 182/1000 | Loss: 0.00001393
Iteration 183/1000 | Loss: 0.00001393
Iteration 184/1000 | Loss: 0.00001393
Iteration 185/1000 | Loss: 0.00001393
Iteration 186/1000 | Loss: 0.00001393
Iteration 187/1000 | Loss: 0.00001393
Iteration 188/1000 | Loss: 0.00001393
Iteration 189/1000 | Loss: 0.00001393
Iteration 190/1000 | Loss: 0.00001393
Iteration 191/1000 | Loss: 0.00001393
Iteration 192/1000 | Loss: 0.00001393
Iteration 193/1000 | Loss: 0.00001393
Iteration 194/1000 | Loss: 0.00001393
Iteration 195/1000 | Loss: 0.00001393
Iteration 196/1000 | Loss: 0.00001393
Iteration 197/1000 | Loss: 0.00001393
Iteration 198/1000 | Loss: 0.00001393
Iteration 199/1000 | Loss: 0.00001393
Iteration 200/1000 | Loss: 0.00001393
Iteration 201/1000 | Loss: 0.00001393
Iteration 202/1000 | Loss: 0.00001393
Iteration 203/1000 | Loss: 0.00001393
Iteration 204/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.3926650353823788e-05, 1.3926650353823788e-05, 1.3926650353823788e-05, 1.3926650353823788e-05, 1.3926650353823788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3926650353823788e-05

Optimization complete. Final v2v error: 3.1218478679656982 mm

Highest mean error: 4.442375659942627 mm for frame 66

Lowest mean error: 2.6702117919921875 mm for frame 0

Saving results

Total time: 84.8519217967987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468559
Iteration 2/25 | Loss: 0.00119151
Iteration 3/25 | Loss: 0.00094727
Iteration 4/25 | Loss: 0.00093878
Iteration 5/25 | Loss: 0.00093630
Iteration 6/25 | Loss: 0.00093575
Iteration 7/25 | Loss: 0.00093575
Iteration 8/25 | Loss: 0.00093575
Iteration 9/25 | Loss: 0.00093575
Iteration 10/25 | Loss: 0.00093575
Iteration 11/25 | Loss: 0.00093575
Iteration 12/25 | Loss: 0.00093575
Iteration 13/25 | Loss: 0.00093575
Iteration 14/25 | Loss: 0.00093575
Iteration 15/25 | Loss: 0.00093575
Iteration 16/25 | Loss: 0.00093575
Iteration 17/25 | Loss: 0.00093575
Iteration 18/25 | Loss: 0.00093575
Iteration 19/25 | Loss: 0.00093575
Iteration 20/25 | Loss: 0.00093575
Iteration 21/25 | Loss: 0.00093575
Iteration 22/25 | Loss: 0.00093575
Iteration 23/25 | Loss: 0.00093575
Iteration 24/25 | Loss: 0.00093575
Iteration 25/25 | Loss: 0.00093575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53482270
Iteration 2/25 | Loss: 0.00050441
Iteration 3/25 | Loss: 0.00050441
Iteration 4/25 | Loss: 0.00050441
Iteration 5/25 | Loss: 0.00050441
Iteration 6/25 | Loss: 0.00050441
Iteration 7/25 | Loss: 0.00050441
Iteration 8/25 | Loss: 0.00050441
Iteration 9/25 | Loss: 0.00050441
Iteration 10/25 | Loss: 0.00050441
Iteration 11/25 | Loss: 0.00050441
Iteration 12/25 | Loss: 0.00050441
Iteration 13/25 | Loss: 0.00050441
Iteration 14/25 | Loss: 0.00050441
Iteration 15/25 | Loss: 0.00050441
Iteration 16/25 | Loss: 0.00050441
Iteration 17/25 | Loss: 0.00050441
Iteration 18/25 | Loss: 0.00050441
Iteration 19/25 | Loss: 0.00050441
Iteration 20/25 | Loss: 0.00050441
Iteration 21/25 | Loss: 0.00050441
Iteration 22/25 | Loss: 0.00050441
Iteration 23/25 | Loss: 0.00050441
Iteration 24/25 | Loss: 0.00050441
Iteration 25/25 | Loss: 0.00050441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050441
Iteration 2/1000 | Loss: 0.00002936
Iteration 3/1000 | Loss: 0.00002384
Iteration 4/1000 | Loss: 0.00002159
Iteration 5/1000 | Loss: 0.00002024
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001860
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001815
Iteration 12/1000 | Loss: 0.00001805
Iteration 13/1000 | Loss: 0.00001804
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00001788
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001774
Iteration 28/1000 | Loss: 0.00001770
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001753
Iteration 35/1000 | Loss: 0.00001753
Iteration 36/1000 | Loss: 0.00001751
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001745
Iteration 52/1000 | Loss: 0.00001745
Iteration 53/1000 | Loss: 0.00001745
Iteration 54/1000 | Loss: 0.00001745
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001744
Iteration 60/1000 | Loss: 0.00001744
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001742
Iteration 69/1000 | Loss: 0.00001742
Iteration 70/1000 | Loss: 0.00001742
Iteration 71/1000 | Loss: 0.00001742
Iteration 72/1000 | Loss: 0.00001741
Iteration 73/1000 | Loss: 0.00001741
Iteration 74/1000 | Loss: 0.00001741
Iteration 75/1000 | Loss: 0.00001740
Iteration 76/1000 | Loss: 0.00001740
Iteration 77/1000 | Loss: 0.00001739
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001738
Iteration 81/1000 | Loss: 0.00001738
Iteration 82/1000 | Loss: 0.00001738
Iteration 83/1000 | Loss: 0.00001737
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001737
Iteration 86/1000 | Loss: 0.00001737
Iteration 87/1000 | Loss: 0.00001736
Iteration 88/1000 | Loss: 0.00001736
Iteration 89/1000 | Loss: 0.00001736
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001735
Iteration 92/1000 | Loss: 0.00001735
Iteration 93/1000 | Loss: 0.00001735
Iteration 94/1000 | Loss: 0.00001735
Iteration 95/1000 | Loss: 0.00001735
Iteration 96/1000 | Loss: 0.00001735
Iteration 97/1000 | Loss: 0.00001735
Iteration 98/1000 | Loss: 0.00001735
Iteration 99/1000 | Loss: 0.00001735
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001734
Iteration 103/1000 | Loss: 0.00001734
Iteration 104/1000 | Loss: 0.00001734
Iteration 105/1000 | Loss: 0.00001733
Iteration 106/1000 | Loss: 0.00001733
Iteration 107/1000 | Loss: 0.00001733
Iteration 108/1000 | Loss: 0.00001733
Iteration 109/1000 | Loss: 0.00001733
Iteration 110/1000 | Loss: 0.00001732
Iteration 111/1000 | Loss: 0.00001732
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00001732
Iteration 114/1000 | Loss: 0.00001732
Iteration 115/1000 | Loss: 0.00001732
Iteration 116/1000 | Loss: 0.00001732
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001731
Iteration 121/1000 | Loss: 0.00001731
Iteration 122/1000 | Loss: 0.00001731
Iteration 123/1000 | Loss: 0.00001730
Iteration 124/1000 | Loss: 0.00001730
Iteration 125/1000 | Loss: 0.00001730
Iteration 126/1000 | Loss: 0.00001729
Iteration 127/1000 | Loss: 0.00001729
Iteration 128/1000 | Loss: 0.00001729
Iteration 129/1000 | Loss: 0.00001729
Iteration 130/1000 | Loss: 0.00001729
Iteration 131/1000 | Loss: 0.00001728
Iteration 132/1000 | Loss: 0.00001728
Iteration 133/1000 | Loss: 0.00001728
Iteration 134/1000 | Loss: 0.00001728
Iteration 135/1000 | Loss: 0.00001727
Iteration 136/1000 | Loss: 0.00001727
Iteration 137/1000 | Loss: 0.00001727
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001726
Iteration 140/1000 | Loss: 0.00001726
Iteration 141/1000 | Loss: 0.00001725
Iteration 142/1000 | Loss: 0.00001725
Iteration 143/1000 | Loss: 0.00001725
Iteration 144/1000 | Loss: 0.00001725
Iteration 145/1000 | Loss: 0.00001724
Iteration 146/1000 | Loss: 0.00001724
Iteration 147/1000 | Loss: 0.00001724
Iteration 148/1000 | Loss: 0.00001723
Iteration 149/1000 | Loss: 0.00001723
Iteration 150/1000 | Loss: 0.00001723
Iteration 151/1000 | Loss: 0.00001723
Iteration 152/1000 | Loss: 0.00001723
Iteration 153/1000 | Loss: 0.00001723
Iteration 154/1000 | Loss: 0.00001723
Iteration 155/1000 | Loss: 0.00001723
Iteration 156/1000 | Loss: 0.00001723
Iteration 157/1000 | Loss: 0.00001723
Iteration 158/1000 | Loss: 0.00001723
Iteration 159/1000 | Loss: 0.00001723
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001722
Iteration 164/1000 | Loss: 0.00001722
Iteration 165/1000 | Loss: 0.00001721
Iteration 166/1000 | Loss: 0.00001721
Iteration 167/1000 | Loss: 0.00001721
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001720
Iteration 173/1000 | Loss: 0.00001720
Iteration 174/1000 | Loss: 0.00001720
Iteration 175/1000 | Loss: 0.00001720
Iteration 176/1000 | Loss: 0.00001720
Iteration 177/1000 | Loss: 0.00001720
Iteration 178/1000 | Loss: 0.00001720
Iteration 179/1000 | Loss: 0.00001720
Iteration 180/1000 | Loss: 0.00001720
Iteration 181/1000 | Loss: 0.00001720
Iteration 182/1000 | Loss: 0.00001720
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001720
Iteration 186/1000 | Loss: 0.00001720
Iteration 187/1000 | Loss: 0.00001720
Iteration 188/1000 | Loss: 0.00001720
Iteration 189/1000 | Loss: 0.00001720
Iteration 190/1000 | Loss: 0.00001720
Iteration 191/1000 | Loss: 0.00001720
Iteration 192/1000 | Loss: 0.00001720
Iteration 193/1000 | Loss: 0.00001720
Iteration 194/1000 | Loss: 0.00001720
Iteration 195/1000 | Loss: 0.00001720
Iteration 196/1000 | Loss: 0.00001720
Iteration 197/1000 | Loss: 0.00001720
Iteration 198/1000 | Loss: 0.00001720
Iteration 199/1000 | Loss: 0.00001720
Iteration 200/1000 | Loss: 0.00001720
Iteration 201/1000 | Loss: 0.00001720
Iteration 202/1000 | Loss: 0.00001720
Iteration 203/1000 | Loss: 0.00001720
Iteration 204/1000 | Loss: 0.00001720
Iteration 205/1000 | Loss: 0.00001720
Iteration 206/1000 | Loss: 0.00001720
Iteration 207/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.7198351997649297e-05, 1.7198351997649297e-05, 1.7198351997649297e-05, 1.7198351997649297e-05, 1.7198351997649297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7198351997649297e-05

Optimization complete. Final v2v error: 3.3067684173583984 mm

Highest mean error: 4.154909610748291 mm for frame 17

Lowest mean error: 2.2113492488861084 mm for frame 1

Saving results

Total time: 43.78055238723755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066659
Iteration 2/25 | Loss: 0.01066658
Iteration 3/25 | Loss: 0.00217918
Iteration 4/25 | Loss: 0.00159821
Iteration 5/25 | Loss: 0.00135375
Iteration 6/25 | Loss: 0.00114646
Iteration 7/25 | Loss: 0.00111504
Iteration 8/25 | Loss: 0.00110420
Iteration 9/25 | Loss: 0.00108701
Iteration 10/25 | Loss: 0.00107307
Iteration 11/25 | Loss: 0.00106567
Iteration 12/25 | Loss: 0.00105604
Iteration 13/25 | Loss: 0.00104812
Iteration 14/25 | Loss: 0.00103689
Iteration 15/25 | Loss: 0.00102996
Iteration 16/25 | Loss: 0.00102012
Iteration 17/25 | Loss: 0.00101467
Iteration 18/25 | Loss: 0.00103028
Iteration 19/25 | Loss: 0.00101940
Iteration 20/25 | Loss: 0.00101218
Iteration 21/25 | Loss: 0.00100692
Iteration 22/25 | Loss: 0.00100504
Iteration 23/25 | Loss: 0.00100342
Iteration 24/25 | Loss: 0.00100858
Iteration 25/25 | Loss: 0.00101120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44923544
Iteration 2/25 | Loss: 0.00108750
Iteration 3/25 | Loss: 0.00107784
Iteration 4/25 | Loss: 0.00107784
Iteration 5/25 | Loss: 0.00107784
Iteration 6/25 | Loss: 0.00107784
Iteration 7/25 | Loss: 0.00107784
Iteration 8/25 | Loss: 0.00107784
Iteration 9/25 | Loss: 0.00107784
Iteration 10/25 | Loss: 0.00107784
Iteration 11/25 | Loss: 0.00107784
Iteration 12/25 | Loss: 0.00107784
Iteration 13/25 | Loss: 0.00107784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001077837310731411, 0.001077837310731411, 0.001077837310731411, 0.001077837310731411, 0.001077837310731411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001077837310731411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107784
Iteration 2/1000 | Loss: 0.00021139
Iteration 3/1000 | Loss: 0.00010413
Iteration 4/1000 | Loss: 0.00009974
Iteration 5/1000 | Loss: 0.00015340
Iteration 6/1000 | Loss: 0.00015150
Iteration 7/1000 | Loss: 0.00012754
Iteration 8/1000 | Loss: 0.00038332
Iteration 9/1000 | Loss: 0.00050409
Iteration 10/1000 | Loss: 0.00020218
Iteration 11/1000 | Loss: 0.00029024
Iteration 12/1000 | Loss: 0.00014166
Iteration 13/1000 | Loss: 0.00009364
Iteration 14/1000 | Loss: 0.00007005
Iteration 15/1000 | Loss: 0.00006556
Iteration 16/1000 | Loss: 0.00009405
Iteration 17/1000 | Loss: 0.00008835
Iteration 18/1000 | Loss: 0.00008061
Iteration 19/1000 | Loss: 0.00008934
Iteration 20/1000 | Loss: 0.00007751
Iteration 21/1000 | Loss: 0.00008529
Iteration 22/1000 | Loss: 0.00008561
Iteration 23/1000 | Loss: 0.00007059
Iteration 24/1000 | Loss: 0.00007139
Iteration 25/1000 | Loss: 0.00007704
Iteration 26/1000 | Loss: 0.00009120
Iteration 27/1000 | Loss: 0.00009376
Iteration 28/1000 | Loss: 0.00008814
Iteration 29/1000 | Loss: 0.00008171
Iteration 30/1000 | Loss: 0.00007928
Iteration 31/1000 | Loss: 0.00008230
Iteration 32/1000 | Loss: 0.00011727
Iteration 33/1000 | Loss: 0.00012556
Iteration 34/1000 | Loss: 0.00007042
Iteration 35/1000 | Loss: 0.00009117
Iteration 36/1000 | Loss: 0.00009765
Iteration 37/1000 | Loss: 0.00009749
Iteration 38/1000 | Loss: 0.00008892
Iteration 39/1000 | Loss: 0.00023042
Iteration 40/1000 | Loss: 0.00010741
Iteration 41/1000 | Loss: 0.00008014
Iteration 42/1000 | Loss: 0.00008707
Iteration 43/1000 | Loss: 0.00008973
Iteration 44/1000 | Loss: 0.00008169
Iteration 45/1000 | Loss: 0.00014397
Iteration 46/1000 | Loss: 0.00007956
Iteration 47/1000 | Loss: 0.00004970
Iteration 48/1000 | Loss: 0.00004513
Iteration 49/1000 | Loss: 0.00012778
Iteration 50/1000 | Loss: 0.00004279
Iteration 51/1000 | Loss: 0.00005931
Iteration 52/1000 | Loss: 0.00004064
Iteration 53/1000 | Loss: 0.00005576
Iteration 54/1000 | Loss: 0.00006040
Iteration 55/1000 | Loss: 0.00008537
Iteration 56/1000 | Loss: 0.00003974
Iteration 57/1000 | Loss: 0.00003869
Iteration 58/1000 | Loss: 0.00003964
Iteration 59/1000 | Loss: 0.00003821
Iteration 60/1000 | Loss: 0.00043334
Iteration 61/1000 | Loss: 0.00004371
Iteration 62/1000 | Loss: 0.00004057
Iteration 63/1000 | Loss: 0.00052884
Iteration 64/1000 | Loss: 0.00033271
Iteration 65/1000 | Loss: 0.00008573
Iteration 66/1000 | Loss: 0.00003771
Iteration 67/1000 | Loss: 0.00047840
Iteration 68/1000 | Loss: 0.00021304
Iteration 69/1000 | Loss: 0.00015628
Iteration 70/1000 | Loss: 0.00005043
Iteration 71/1000 | Loss: 0.00004206
Iteration 72/1000 | Loss: 0.00003665
Iteration 73/1000 | Loss: 0.00056612
Iteration 74/1000 | Loss: 0.00040285
Iteration 75/1000 | Loss: 0.00014142
Iteration 76/1000 | Loss: 0.00063704
Iteration 77/1000 | Loss: 0.00005378
Iteration 78/1000 | Loss: 0.00023759
Iteration 79/1000 | Loss: 0.00034244
Iteration 80/1000 | Loss: 0.00003671
Iteration 81/1000 | Loss: 0.00003501
Iteration 82/1000 | Loss: 0.00003698
Iteration 83/1000 | Loss: 0.00003561
Iteration 84/1000 | Loss: 0.00003919
Iteration 85/1000 | Loss: 0.00003605
Iteration 86/1000 | Loss: 0.00003366
Iteration 87/1000 | Loss: 0.00003725
Iteration 88/1000 | Loss: 0.00003172
Iteration 89/1000 | Loss: 0.00003893
Iteration 90/1000 | Loss: 0.00003116
Iteration 91/1000 | Loss: 0.00005117
Iteration 92/1000 | Loss: 0.00004633
Iteration 93/1000 | Loss: 0.00003085
Iteration 94/1000 | Loss: 0.00003649
Iteration 95/1000 | Loss: 0.00003070
Iteration 96/1000 | Loss: 0.00003649
Iteration 97/1000 | Loss: 0.00003784
Iteration 98/1000 | Loss: 0.00003298
Iteration 99/1000 | Loss: 0.00003043
Iteration 100/1000 | Loss: 0.00003041
Iteration 101/1000 | Loss: 0.00003087
Iteration 102/1000 | Loss: 0.00003034
Iteration 103/1000 | Loss: 0.00003034
Iteration 104/1000 | Loss: 0.00003033
Iteration 105/1000 | Loss: 0.00003033
Iteration 106/1000 | Loss: 0.00003033
Iteration 107/1000 | Loss: 0.00003033
Iteration 108/1000 | Loss: 0.00003033
Iteration 109/1000 | Loss: 0.00003033
Iteration 110/1000 | Loss: 0.00003033
Iteration 111/1000 | Loss: 0.00003033
Iteration 112/1000 | Loss: 0.00003033
Iteration 113/1000 | Loss: 0.00003033
Iteration 114/1000 | Loss: 0.00003033
Iteration 115/1000 | Loss: 0.00003115
Iteration 116/1000 | Loss: 0.00003030
Iteration 117/1000 | Loss: 0.00003023
Iteration 118/1000 | Loss: 0.00003043
Iteration 119/1000 | Loss: 0.00003010
Iteration 120/1000 | Loss: 0.00003010
Iteration 121/1000 | Loss: 0.00003010
Iteration 122/1000 | Loss: 0.00003010
Iteration 123/1000 | Loss: 0.00003010
Iteration 124/1000 | Loss: 0.00003009
Iteration 125/1000 | Loss: 0.00003009
Iteration 126/1000 | Loss: 0.00003009
Iteration 127/1000 | Loss: 0.00003009
Iteration 128/1000 | Loss: 0.00003009
Iteration 129/1000 | Loss: 0.00003009
Iteration 130/1000 | Loss: 0.00003009
Iteration 131/1000 | Loss: 0.00003008
Iteration 132/1000 | Loss: 0.00003030
Iteration 133/1000 | Loss: 0.00003060
Iteration 134/1000 | Loss: 0.00003060
Iteration 135/1000 | Loss: 0.00003002
Iteration 136/1000 | Loss: 0.00003002
Iteration 137/1000 | Loss: 0.00003002
Iteration 138/1000 | Loss: 0.00003002
Iteration 139/1000 | Loss: 0.00003002
Iteration 140/1000 | Loss: 0.00003002
Iteration 141/1000 | Loss: 0.00003002
Iteration 142/1000 | Loss: 0.00003002
Iteration 143/1000 | Loss: 0.00003001
Iteration 144/1000 | Loss: 0.00003001
Iteration 145/1000 | Loss: 0.00003001
Iteration 146/1000 | Loss: 0.00003001
Iteration 147/1000 | Loss: 0.00003001
Iteration 148/1000 | Loss: 0.00003001
Iteration 149/1000 | Loss: 0.00003001
Iteration 150/1000 | Loss: 0.00003001
Iteration 151/1000 | Loss: 0.00003001
Iteration 152/1000 | Loss: 0.00003001
Iteration 153/1000 | Loss: 0.00003001
Iteration 154/1000 | Loss: 0.00003001
Iteration 155/1000 | Loss: 0.00003001
Iteration 156/1000 | Loss: 0.00003001
Iteration 157/1000 | Loss: 0.00003001
Iteration 158/1000 | Loss: 0.00003001
Iteration 159/1000 | Loss: 0.00003001
Iteration 160/1000 | Loss: 0.00003001
Iteration 161/1000 | Loss: 0.00003001
Iteration 162/1000 | Loss: 0.00003001
Iteration 163/1000 | Loss: 0.00003001
Iteration 164/1000 | Loss: 0.00003001
Iteration 165/1000 | Loss: 0.00003001
Iteration 166/1000 | Loss: 0.00003001
Iteration 167/1000 | Loss: 0.00003001
Iteration 168/1000 | Loss: 0.00003001
Iteration 169/1000 | Loss: 0.00003001
Iteration 170/1000 | Loss: 0.00003001
Iteration 171/1000 | Loss: 0.00003001
Iteration 172/1000 | Loss: 0.00003001
Iteration 173/1000 | Loss: 0.00003001
Iteration 174/1000 | Loss: 0.00003001
Iteration 175/1000 | Loss: 0.00003001
Iteration 176/1000 | Loss: 0.00003001
Iteration 177/1000 | Loss: 0.00003001
Iteration 178/1000 | Loss: 0.00003001
Iteration 179/1000 | Loss: 0.00003001
Iteration 180/1000 | Loss: 0.00003001
Iteration 181/1000 | Loss: 0.00003001
Iteration 182/1000 | Loss: 0.00003001
Iteration 183/1000 | Loss: 0.00003001
Iteration 184/1000 | Loss: 0.00003001
Iteration 185/1000 | Loss: 0.00003001
Iteration 186/1000 | Loss: 0.00003001
Iteration 187/1000 | Loss: 0.00003001
Iteration 188/1000 | Loss: 0.00003001
Iteration 189/1000 | Loss: 0.00003001
Iteration 190/1000 | Loss: 0.00003001
Iteration 191/1000 | Loss: 0.00003001
Iteration 192/1000 | Loss: 0.00003001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [3.0006451197550632e-05, 3.0006451197550632e-05, 3.0006451197550632e-05, 3.0006451197550632e-05, 3.0006451197550632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0006451197550632e-05

Optimization complete. Final v2v error: 3.695971727371216 mm

Highest mean error: 12.448211669921875 mm for frame 39

Lowest mean error: 2.983588695526123 mm for frame 172

Saving results

Total time: 220.0163426399231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498181
Iteration 2/25 | Loss: 0.00108006
Iteration 3/25 | Loss: 0.00095084
Iteration 4/25 | Loss: 0.00093782
Iteration 5/25 | Loss: 0.00093287
Iteration 6/25 | Loss: 0.00093088
Iteration 7/25 | Loss: 0.00093076
Iteration 8/25 | Loss: 0.00093076
Iteration 9/25 | Loss: 0.00093076
Iteration 10/25 | Loss: 0.00093076
Iteration 11/25 | Loss: 0.00093076
Iteration 12/25 | Loss: 0.00093076
Iteration 13/25 | Loss: 0.00093076
Iteration 14/25 | Loss: 0.00093076
Iteration 15/25 | Loss: 0.00093076
Iteration 16/25 | Loss: 0.00093076
Iteration 17/25 | Loss: 0.00093076
Iteration 18/25 | Loss: 0.00093076
Iteration 19/25 | Loss: 0.00093076
Iteration 20/25 | Loss: 0.00093076
Iteration 21/25 | Loss: 0.00093076
Iteration 22/25 | Loss: 0.00093076
Iteration 23/25 | Loss: 0.00093076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009307587752118707, 0.0009307587752118707, 0.0009307587752118707, 0.0009307587752118707, 0.0009307587752118707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009307587752118707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51991928
Iteration 2/25 | Loss: 0.00060897
Iteration 3/25 | Loss: 0.00060895
Iteration 4/25 | Loss: 0.00060895
Iteration 5/25 | Loss: 0.00060895
Iteration 6/25 | Loss: 0.00060895
Iteration 7/25 | Loss: 0.00060895
Iteration 8/25 | Loss: 0.00060895
Iteration 9/25 | Loss: 0.00060895
Iteration 10/25 | Loss: 0.00060895
Iteration 11/25 | Loss: 0.00060895
Iteration 12/25 | Loss: 0.00060895
Iteration 13/25 | Loss: 0.00060895
Iteration 14/25 | Loss: 0.00060895
Iteration 15/25 | Loss: 0.00060895
Iteration 16/25 | Loss: 0.00060895
Iteration 17/25 | Loss: 0.00060895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006089500384405255, 0.0006089500384405255, 0.0006089500384405255, 0.0006089500384405255, 0.0006089500384405255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006089500384405255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060895
Iteration 2/1000 | Loss: 0.00002173
Iteration 3/1000 | Loss: 0.00001609
Iteration 4/1000 | Loss: 0.00001440
Iteration 5/1000 | Loss: 0.00001369
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001280
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001275
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001263
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001259
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001256
Iteration 43/1000 | Loss: 0.00001256
Iteration 44/1000 | Loss: 0.00001256
Iteration 45/1000 | Loss: 0.00001256
Iteration 46/1000 | Loss: 0.00001256
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001252
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001249
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001248
Iteration 76/1000 | Loss: 0.00001248
Iteration 77/1000 | Loss: 0.00001248
Iteration 78/1000 | Loss: 0.00001248
Iteration 79/1000 | Loss: 0.00001248
Iteration 80/1000 | Loss: 0.00001248
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001247
Iteration 83/1000 | Loss: 0.00001247
Iteration 84/1000 | Loss: 0.00001247
Iteration 85/1000 | Loss: 0.00001247
Iteration 86/1000 | Loss: 0.00001247
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001246
Iteration 93/1000 | Loss: 0.00001246
Iteration 94/1000 | Loss: 0.00001246
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001246
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001245
Iteration 114/1000 | Loss: 0.00001245
Iteration 115/1000 | Loss: 0.00001245
Iteration 116/1000 | Loss: 0.00001245
Iteration 117/1000 | Loss: 0.00001245
Iteration 118/1000 | Loss: 0.00001245
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001244
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.2444143976608757e-05, 1.2444143976608757e-05, 1.2444143976608757e-05, 1.2444143976608757e-05, 1.2444143976608757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2444143976608757e-05

Optimization complete. Final v2v error: 3.0193655490875244 mm

Highest mean error: 3.58601450920105 mm for frame 101

Lowest mean error: 2.6232056617736816 mm for frame 149

Saving results

Total time: 36.09726428985596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_2918/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_2918/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469541
Iteration 2/25 | Loss: 0.00103243
Iteration 3/25 | Loss: 0.00092431
Iteration 4/25 | Loss: 0.00091664
Iteration 5/25 | Loss: 0.00091476
Iteration 6/25 | Loss: 0.00091427
Iteration 7/25 | Loss: 0.00091427
Iteration 8/25 | Loss: 0.00091427
Iteration 9/25 | Loss: 0.00091427
Iteration 10/25 | Loss: 0.00091427
Iteration 11/25 | Loss: 0.00091427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009142704657278955, 0.0009142704657278955, 0.0009142704657278955, 0.0009142704657278955, 0.0009142704657278955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009142704657278955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.23280144
Iteration 2/25 | Loss: 0.00037378
Iteration 3/25 | Loss: 0.00037378
Iteration 4/25 | Loss: 0.00037377
Iteration 5/25 | Loss: 0.00037377
Iteration 6/25 | Loss: 0.00037377
Iteration 7/25 | Loss: 0.00037377
Iteration 8/25 | Loss: 0.00037377
Iteration 9/25 | Loss: 0.00037377
Iteration 10/25 | Loss: 0.00037377
Iteration 11/25 | Loss: 0.00037377
Iteration 12/25 | Loss: 0.00037377
Iteration 13/25 | Loss: 0.00037377
Iteration 14/25 | Loss: 0.00037377
Iteration 15/25 | Loss: 0.00037377
Iteration 16/25 | Loss: 0.00037377
Iteration 17/25 | Loss: 0.00037377
Iteration 18/25 | Loss: 0.00037377
Iteration 19/25 | Loss: 0.00037377
Iteration 20/25 | Loss: 0.00037377
Iteration 21/25 | Loss: 0.00037377
Iteration 22/25 | Loss: 0.00037377
Iteration 23/25 | Loss: 0.00037377
Iteration 24/25 | Loss: 0.00037377
Iteration 25/25 | Loss: 0.00037377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037377
Iteration 2/1000 | Loss: 0.00002836
Iteration 3/1000 | Loss: 0.00001599
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001159
Iteration 7/1000 | Loss: 0.00001132
Iteration 8/1000 | Loss: 0.00001106
Iteration 9/1000 | Loss: 0.00001097
Iteration 10/1000 | Loss: 0.00001077
Iteration 11/1000 | Loss: 0.00001072
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001061
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001059
Iteration 18/1000 | Loss: 0.00001059
Iteration 19/1000 | Loss: 0.00001058
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001054
Iteration 24/1000 | Loss: 0.00001054
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001050
Iteration 30/1000 | Loss: 0.00001050
Iteration 31/1000 | Loss: 0.00001050
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001049
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001049
Iteration 38/1000 | Loss: 0.00001048
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001046
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001045
Iteration 44/1000 | Loss: 0.00001045
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001045
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001044
Iteration 49/1000 | Loss: 0.00001044
Iteration 50/1000 | Loss: 0.00001044
Iteration 51/1000 | Loss: 0.00001043
Iteration 52/1000 | Loss: 0.00001043
Iteration 53/1000 | Loss: 0.00001043
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001043
Iteration 56/1000 | Loss: 0.00001042
Iteration 57/1000 | Loss: 0.00001042
Iteration 58/1000 | Loss: 0.00001042
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001041
Iteration 62/1000 | Loss: 0.00001041
Iteration 63/1000 | Loss: 0.00001041
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00001041
Iteration 66/1000 | Loss: 0.00001041
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001040
Iteration 73/1000 | Loss: 0.00001040
Iteration 74/1000 | Loss: 0.00001040
Iteration 75/1000 | Loss: 0.00001040
Iteration 76/1000 | Loss: 0.00001040
Iteration 77/1000 | Loss: 0.00001040
Iteration 78/1000 | Loss: 0.00001040
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001039
Iteration 82/1000 | Loss: 0.00001039
Iteration 83/1000 | Loss: 0.00001039
Iteration 84/1000 | Loss: 0.00001039
Iteration 85/1000 | Loss: 0.00001039
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001039
Iteration 88/1000 | Loss: 0.00001039
Iteration 89/1000 | Loss: 0.00001039
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001039
Iteration 94/1000 | Loss: 0.00001039
Iteration 95/1000 | Loss: 0.00001039
Iteration 96/1000 | Loss: 0.00001039
Iteration 97/1000 | Loss: 0.00001039
Iteration 98/1000 | Loss: 0.00001039
Iteration 99/1000 | Loss: 0.00001039
Iteration 100/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.039056951412931e-05, 1.039056951412931e-05, 1.039056951412931e-05, 1.039056951412931e-05, 1.039056951412931e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.039056951412931e-05

Optimization complete. Final v2v error: 2.7744851112365723 mm

Highest mean error: 3.1446874141693115 mm for frame 62

Lowest mean error: 2.366098403930664 mm for frame 134

Saving results

Total time: 30.31531310081482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01197705
Iteration 2/25 | Loss: 0.01197705
Iteration 3/25 | Loss: 0.01197705
Iteration 4/25 | Loss: 0.01197704
Iteration 5/25 | Loss: 0.01197704
Iteration 6/25 | Loss: 0.01197704
Iteration 7/25 | Loss: 0.01197704
Iteration 8/25 | Loss: 0.01197704
Iteration 9/25 | Loss: 0.01197704
Iteration 10/25 | Loss: 0.01197704
Iteration 11/25 | Loss: 0.01197704
Iteration 12/25 | Loss: 0.01197704
Iteration 13/25 | Loss: 0.01197704
Iteration 14/25 | Loss: 0.01197703
Iteration 15/25 | Loss: 0.01197703
Iteration 16/25 | Loss: 0.01197703
Iteration 17/25 | Loss: 0.01197703
Iteration 18/25 | Loss: 0.01197703
Iteration 19/25 | Loss: 0.01197703
Iteration 20/25 | Loss: 0.01197703
Iteration 21/25 | Loss: 0.01197703
Iteration 22/25 | Loss: 0.01197702
Iteration 23/25 | Loss: 0.01197702
Iteration 24/25 | Loss: 0.01197702
Iteration 25/25 | Loss: 0.01197702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79048634
Iteration 2/25 | Loss: 0.06793547
Iteration 3/25 | Loss: 0.06790436
Iteration 4/25 | Loss: 0.06786315
Iteration 5/25 | Loss: 0.06786313
Iteration 6/25 | Loss: 0.06786312
Iteration 7/25 | Loss: 0.06786312
Iteration 8/25 | Loss: 0.06786312
Iteration 9/25 | Loss: 0.06786312
Iteration 10/25 | Loss: 0.06786312
Iteration 11/25 | Loss: 0.06786311
Iteration 12/25 | Loss: 0.06786311
Iteration 13/25 | Loss: 0.06786311
Iteration 14/25 | Loss: 0.06786311
Iteration 15/25 | Loss: 0.06786311
Iteration 16/25 | Loss: 0.06786311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0678631141781807, 0.0678631141781807, 0.0678631141781807, 0.0678631141781807, 0.0678631141781807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0678631141781807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06786311
Iteration 2/1000 | Loss: 0.00290710
Iteration 3/1000 | Loss: 0.00134533
Iteration 4/1000 | Loss: 0.00164233
Iteration 5/1000 | Loss: 0.00073384
Iteration 6/1000 | Loss: 0.00014086
Iteration 7/1000 | Loss: 0.00046383
Iteration 8/1000 | Loss: 0.00014587
Iteration 9/1000 | Loss: 0.00040517
Iteration 10/1000 | Loss: 0.00036581
Iteration 11/1000 | Loss: 0.00050023
Iteration 12/1000 | Loss: 0.00005978
Iteration 13/1000 | Loss: 0.00005522
Iteration 14/1000 | Loss: 0.00006500
Iteration 15/1000 | Loss: 0.00032941
Iteration 16/1000 | Loss: 0.00007704
Iteration 17/1000 | Loss: 0.00028512
Iteration 18/1000 | Loss: 0.00004229
Iteration 19/1000 | Loss: 0.00004875
Iteration 20/1000 | Loss: 0.00003848
Iteration 21/1000 | Loss: 0.00003749
Iteration 22/1000 | Loss: 0.00004989
Iteration 23/1000 | Loss: 0.00007681
Iteration 24/1000 | Loss: 0.00003458
Iteration 25/1000 | Loss: 0.00003384
Iteration 26/1000 | Loss: 0.00003327
Iteration 27/1000 | Loss: 0.00003557
Iteration 28/1000 | Loss: 0.00003252
Iteration 29/1000 | Loss: 0.00003735
Iteration 30/1000 | Loss: 0.00003163
Iteration 31/1000 | Loss: 0.00003136
Iteration 32/1000 | Loss: 0.00003134
Iteration 33/1000 | Loss: 0.00003132
Iteration 34/1000 | Loss: 0.00003128
Iteration 35/1000 | Loss: 0.00003128
Iteration 36/1000 | Loss: 0.00003127
Iteration 37/1000 | Loss: 0.00003117
Iteration 38/1000 | Loss: 0.00003116
Iteration 39/1000 | Loss: 0.00003107
Iteration 40/1000 | Loss: 0.00003102
Iteration 41/1000 | Loss: 0.00003101
Iteration 42/1000 | Loss: 0.00003101
Iteration 43/1000 | Loss: 0.00003100
Iteration 44/1000 | Loss: 0.00003099
Iteration 45/1000 | Loss: 0.00003096
Iteration 46/1000 | Loss: 0.00003095
Iteration 47/1000 | Loss: 0.00003095
Iteration 48/1000 | Loss: 0.00003095
Iteration 49/1000 | Loss: 0.00003095
Iteration 50/1000 | Loss: 0.00003095
Iteration 51/1000 | Loss: 0.00003094
Iteration 52/1000 | Loss: 0.00003094
Iteration 53/1000 | Loss: 0.00003094
Iteration 54/1000 | Loss: 0.00003093
Iteration 55/1000 | Loss: 0.00003093
Iteration 56/1000 | Loss: 0.00003093
Iteration 57/1000 | Loss: 0.00003092
Iteration 58/1000 | Loss: 0.00003091
Iteration 59/1000 | Loss: 0.00003091
Iteration 60/1000 | Loss: 0.00003091
Iteration 61/1000 | Loss: 0.00003090
Iteration 62/1000 | Loss: 0.00003090
Iteration 63/1000 | Loss: 0.00003090
Iteration 64/1000 | Loss: 0.00003089
Iteration 65/1000 | Loss: 0.00003089
Iteration 66/1000 | Loss: 0.00003089
Iteration 67/1000 | Loss: 0.00003088
Iteration 68/1000 | Loss: 0.00003088
Iteration 69/1000 | Loss: 0.00003088
Iteration 70/1000 | Loss: 0.00003087
Iteration 71/1000 | Loss: 0.00003087
Iteration 72/1000 | Loss: 0.00003087
Iteration 73/1000 | Loss: 0.00003087
Iteration 74/1000 | Loss: 0.00003086
Iteration 75/1000 | Loss: 0.00003086
Iteration 76/1000 | Loss: 0.00003086
Iteration 77/1000 | Loss: 0.00003086
Iteration 78/1000 | Loss: 0.00003085
Iteration 79/1000 | Loss: 0.00003085
Iteration 80/1000 | Loss: 0.00003085
Iteration 81/1000 | Loss: 0.00003085
Iteration 82/1000 | Loss: 0.00003084
Iteration 83/1000 | Loss: 0.00003084
Iteration 84/1000 | Loss: 0.00003084
Iteration 85/1000 | Loss: 0.00003084
Iteration 86/1000 | Loss: 0.00003084
Iteration 87/1000 | Loss: 0.00003084
Iteration 88/1000 | Loss: 0.00003083
Iteration 89/1000 | Loss: 0.00003083
Iteration 90/1000 | Loss: 0.00003083
Iteration 91/1000 | Loss: 0.00003083
Iteration 92/1000 | Loss: 0.00003083
Iteration 93/1000 | Loss: 0.00003082
Iteration 94/1000 | Loss: 0.00003082
Iteration 95/1000 | Loss: 0.00003082
Iteration 96/1000 | Loss: 0.00003082
Iteration 97/1000 | Loss: 0.00003082
Iteration 98/1000 | Loss: 0.00003082
Iteration 99/1000 | Loss: 0.00003082
Iteration 100/1000 | Loss: 0.00003082
Iteration 101/1000 | Loss: 0.00003082
Iteration 102/1000 | Loss: 0.00003082
Iteration 103/1000 | Loss: 0.00003082
Iteration 104/1000 | Loss: 0.00003082
Iteration 105/1000 | Loss: 0.00003082
Iteration 106/1000 | Loss: 0.00003082
Iteration 107/1000 | Loss: 0.00003081
Iteration 108/1000 | Loss: 0.00003081
Iteration 109/1000 | Loss: 0.00003081
Iteration 110/1000 | Loss: 0.00003081
Iteration 111/1000 | Loss: 0.00003081
Iteration 112/1000 | Loss: 0.00003081
Iteration 113/1000 | Loss: 0.00003081
Iteration 114/1000 | Loss: 0.00003081
Iteration 115/1000 | Loss: 0.00003081
Iteration 116/1000 | Loss: 0.00003081
Iteration 117/1000 | Loss: 0.00003081
Iteration 118/1000 | Loss: 0.00003081
Iteration 119/1000 | Loss: 0.00003081
Iteration 120/1000 | Loss: 0.00003080
Iteration 121/1000 | Loss: 0.00003080
Iteration 122/1000 | Loss: 0.00003080
Iteration 123/1000 | Loss: 0.00003080
Iteration 124/1000 | Loss: 0.00003080
Iteration 125/1000 | Loss: 0.00003080
Iteration 126/1000 | Loss: 0.00003080
Iteration 127/1000 | Loss: 0.00003080
Iteration 128/1000 | Loss: 0.00003080
Iteration 129/1000 | Loss: 0.00003080
Iteration 130/1000 | Loss: 0.00003080
Iteration 131/1000 | Loss: 0.00003080
Iteration 132/1000 | Loss: 0.00003080
Iteration 133/1000 | Loss: 0.00003080
Iteration 134/1000 | Loss: 0.00003080
Iteration 135/1000 | Loss: 0.00003080
Iteration 136/1000 | Loss: 0.00003080
Iteration 137/1000 | Loss: 0.00003080
Iteration 138/1000 | Loss: 0.00003080
Iteration 139/1000 | Loss: 0.00003080
Iteration 140/1000 | Loss: 0.00003080
Iteration 141/1000 | Loss: 0.00003080
Iteration 142/1000 | Loss: 0.00003080
Iteration 143/1000 | Loss: 0.00003080
Iteration 144/1000 | Loss: 0.00003080
Iteration 145/1000 | Loss: 0.00003080
Iteration 146/1000 | Loss: 0.00003080
Iteration 147/1000 | Loss: 0.00003080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [3.079585076193325e-05, 3.079585076193325e-05, 3.079585076193325e-05, 3.079585076193325e-05, 3.079585076193325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.079585076193325e-05

Optimization complete. Final v2v error: 4.908250331878662 mm

Highest mean error: 5.047832012176514 mm for frame 173

Lowest mean error: 4.715824127197266 mm for frame 183

Saving results

Total time: 61.50028705596924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510307
Iteration 2/25 | Loss: 0.00155819
Iteration 3/25 | Loss: 0.00140817
Iteration 4/25 | Loss: 0.00138043
Iteration 5/25 | Loss: 0.00137544
Iteration 6/25 | Loss: 0.00137481
Iteration 7/25 | Loss: 0.00137481
Iteration 8/25 | Loss: 0.00137481
Iteration 9/25 | Loss: 0.00137481
Iteration 10/25 | Loss: 0.00137481
Iteration 11/25 | Loss: 0.00137481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013748130295425653, 0.0013748130295425653, 0.0013748130295425653, 0.0013748130295425653, 0.0013748130295425653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013748130295425653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39403856
Iteration 2/25 | Loss: 0.00110122
Iteration 3/25 | Loss: 0.00110122
Iteration 4/25 | Loss: 0.00110122
Iteration 5/25 | Loss: 0.00110122
Iteration 6/25 | Loss: 0.00110122
Iteration 7/25 | Loss: 0.00110122
Iteration 8/25 | Loss: 0.00110122
Iteration 9/25 | Loss: 0.00110122
Iteration 10/25 | Loss: 0.00110122
Iteration 11/25 | Loss: 0.00110122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011012174654752016, 0.0011012174654752016, 0.0011012174654752016, 0.0011012174654752016, 0.0011012174654752016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011012174654752016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110122
Iteration 2/1000 | Loss: 0.00004883
Iteration 3/1000 | Loss: 0.00003677
Iteration 4/1000 | Loss: 0.00003330
Iteration 5/1000 | Loss: 0.00003178
Iteration 6/1000 | Loss: 0.00003094
Iteration 7/1000 | Loss: 0.00003046
Iteration 8/1000 | Loss: 0.00003001
Iteration 9/1000 | Loss: 0.00002964
Iteration 10/1000 | Loss: 0.00002954
Iteration 11/1000 | Loss: 0.00002935
Iteration 12/1000 | Loss: 0.00002930
Iteration 13/1000 | Loss: 0.00002920
Iteration 14/1000 | Loss: 0.00002916
Iteration 15/1000 | Loss: 0.00002915
Iteration 16/1000 | Loss: 0.00002914
Iteration 17/1000 | Loss: 0.00002914
Iteration 18/1000 | Loss: 0.00002913
Iteration 19/1000 | Loss: 0.00002913
Iteration 20/1000 | Loss: 0.00002913
Iteration 21/1000 | Loss: 0.00002912
Iteration 22/1000 | Loss: 0.00002912
Iteration 23/1000 | Loss: 0.00002912
Iteration 24/1000 | Loss: 0.00002912
Iteration 25/1000 | Loss: 0.00002911
Iteration 26/1000 | Loss: 0.00002911
Iteration 27/1000 | Loss: 0.00002911
Iteration 28/1000 | Loss: 0.00002911
Iteration 29/1000 | Loss: 0.00002911
Iteration 30/1000 | Loss: 0.00002911
Iteration 31/1000 | Loss: 0.00002911
Iteration 32/1000 | Loss: 0.00002911
Iteration 33/1000 | Loss: 0.00002910
Iteration 34/1000 | Loss: 0.00002910
Iteration 35/1000 | Loss: 0.00002910
Iteration 36/1000 | Loss: 0.00002910
Iteration 37/1000 | Loss: 0.00002910
Iteration 38/1000 | Loss: 0.00002910
Iteration 39/1000 | Loss: 0.00002910
Iteration 40/1000 | Loss: 0.00002910
Iteration 41/1000 | Loss: 0.00002909
Iteration 42/1000 | Loss: 0.00002909
Iteration 43/1000 | Loss: 0.00002907
Iteration 44/1000 | Loss: 0.00002906
Iteration 45/1000 | Loss: 0.00002905
Iteration 46/1000 | Loss: 0.00002905
Iteration 47/1000 | Loss: 0.00002904
Iteration 48/1000 | Loss: 0.00002904
Iteration 49/1000 | Loss: 0.00002904
Iteration 50/1000 | Loss: 0.00002904
Iteration 51/1000 | Loss: 0.00002903
Iteration 52/1000 | Loss: 0.00002903
Iteration 53/1000 | Loss: 0.00002903
Iteration 54/1000 | Loss: 0.00002903
Iteration 55/1000 | Loss: 0.00002903
Iteration 56/1000 | Loss: 0.00002903
Iteration 57/1000 | Loss: 0.00002903
Iteration 58/1000 | Loss: 0.00002903
Iteration 59/1000 | Loss: 0.00002903
Iteration 60/1000 | Loss: 0.00002903
Iteration 61/1000 | Loss: 0.00002903
Iteration 62/1000 | Loss: 0.00002903
Iteration 63/1000 | Loss: 0.00002903
Iteration 64/1000 | Loss: 0.00002903
Iteration 65/1000 | Loss: 0.00002903
Iteration 66/1000 | Loss: 0.00002903
Iteration 67/1000 | Loss: 0.00002903
Iteration 68/1000 | Loss: 0.00002902
Iteration 69/1000 | Loss: 0.00002902
Iteration 70/1000 | Loss: 0.00002902
Iteration 71/1000 | Loss: 0.00002902
Iteration 72/1000 | Loss: 0.00002902
Iteration 73/1000 | Loss: 0.00002900
Iteration 74/1000 | Loss: 0.00002900
Iteration 75/1000 | Loss: 0.00002900
Iteration 76/1000 | Loss: 0.00002900
Iteration 77/1000 | Loss: 0.00002900
Iteration 78/1000 | Loss: 0.00002900
Iteration 79/1000 | Loss: 0.00002900
Iteration 80/1000 | Loss: 0.00002900
Iteration 81/1000 | Loss: 0.00002900
Iteration 82/1000 | Loss: 0.00002899
Iteration 83/1000 | Loss: 0.00002899
Iteration 84/1000 | Loss: 0.00002899
Iteration 85/1000 | Loss: 0.00002899
Iteration 86/1000 | Loss: 0.00002899
Iteration 87/1000 | Loss: 0.00002899
Iteration 88/1000 | Loss: 0.00002898
Iteration 89/1000 | Loss: 0.00002898
Iteration 90/1000 | Loss: 0.00002898
Iteration 91/1000 | Loss: 0.00002898
Iteration 92/1000 | Loss: 0.00002898
Iteration 93/1000 | Loss: 0.00002898
Iteration 94/1000 | Loss: 0.00002898
Iteration 95/1000 | Loss: 0.00002898
Iteration 96/1000 | Loss: 0.00002898
Iteration 97/1000 | Loss: 0.00002898
Iteration 98/1000 | Loss: 0.00002898
Iteration 99/1000 | Loss: 0.00002898
Iteration 100/1000 | Loss: 0.00002898
Iteration 101/1000 | Loss: 0.00002898
Iteration 102/1000 | Loss: 0.00002898
Iteration 103/1000 | Loss: 0.00002898
Iteration 104/1000 | Loss: 0.00002898
Iteration 105/1000 | Loss: 0.00002898
Iteration 106/1000 | Loss: 0.00002898
Iteration 107/1000 | Loss: 0.00002898
Iteration 108/1000 | Loss: 0.00002898
Iteration 109/1000 | Loss: 0.00002898
Iteration 110/1000 | Loss: 0.00002898
Iteration 111/1000 | Loss: 0.00002898
Iteration 112/1000 | Loss: 0.00002898
Iteration 113/1000 | Loss: 0.00002898
Iteration 114/1000 | Loss: 0.00002898
Iteration 115/1000 | Loss: 0.00002898
Iteration 116/1000 | Loss: 0.00002898
Iteration 117/1000 | Loss: 0.00002898
Iteration 118/1000 | Loss: 0.00002898
Iteration 119/1000 | Loss: 0.00002898
Iteration 120/1000 | Loss: 0.00002898
Iteration 121/1000 | Loss: 0.00002898
Iteration 122/1000 | Loss: 0.00002898
Iteration 123/1000 | Loss: 0.00002898
Iteration 124/1000 | Loss: 0.00002898
Iteration 125/1000 | Loss: 0.00002898
Iteration 126/1000 | Loss: 0.00002898
Iteration 127/1000 | Loss: 0.00002898
Iteration 128/1000 | Loss: 0.00002898
Iteration 129/1000 | Loss: 0.00002898
Iteration 130/1000 | Loss: 0.00002898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.897738158935681e-05, 2.897738158935681e-05, 2.897738158935681e-05, 2.897738158935681e-05, 2.897738158935681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.897738158935681e-05

Optimization complete. Final v2v error: 4.753401756286621 mm

Highest mean error: 5.020652770996094 mm for frame 138

Lowest mean error: 4.558115005493164 mm for frame 80

Saving results

Total time: 31.189847707748413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01178199
Iteration 2/25 | Loss: 0.00262618
Iteration 3/25 | Loss: 0.00216818
Iteration 4/25 | Loss: 0.00183333
Iteration 5/25 | Loss: 0.00185190
Iteration 6/25 | Loss: 0.00175805
Iteration 7/25 | Loss: 0.00171462
Iteration 8/25 | Loss: 0.00169493
Iteration 9/25 | Loss: 0.00174285
Iteration 10/25 | Loss: 0.00164866
Iteration 11/25 | Loss: 0.00167835
Iteration 12/25 | Loss: 0.00161498
Iteration 13/25 | Loss: 0.00161176
Iteration 14/25 | Loss: 0.00160960
Iteration 15/25 | Loss: 0.00169179
Iteration 16/25 | Loss: 0.00179367
Iteration 17/25 | Loss: 0.00176203
Iteration 18/25 | Loss: 0.00161317
Iteration 19/25 | Loss: 0.00157688
Iteration 20/25 | Loss: 0.00157246
Iteration 21/25 | Loss: 0.00157176
Iteration 22/25 | Loss: 0.00157144
Iteration 23/25 | Loss: 0.00157116
Iteration 24/25 | Loss: 0.00162579
Iteration 25/25 | Loss: 0.00162599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43132365
Iteration 2/25 | Loss: 0.00611843
Iteration 3/25 | Loss: 0.00329539
Iteration 4/25 | Loss: 0.00329539
Iteration 5/25 | Loss: 0.00329539
Iteration 6/25 | Loss: 0.00329539
Iteration 7/25 | Loss: 0.00329539
Iteration 8/25 | Loss: 0.00329539
Iteration 9/25 | Loss: 0.00329539
Iteration 10/25 | Loss: 0.00329539
Iteration 11/25 | Loss: 0.00329539
Iteration 12/25 | Loss: 0.00329539
Iteration 13/25 | Loss: 0.00329539
Iteration 14/25 | Loss: 0.00329539
Iteration 15/25 | Loss: 0.00329539
Iteration 16/25 | Loss: 0.00329539
Iteration 17/25 | Loss: 0.00329539
Iteration 18/25 | Loss: 0.00329539
Iteration 19/25 | Loss: 0.00329539
Iteration 20/25 | Loss: 0.00329539
Iteration 21/25 | Loss: 0.00329539
Iteration 22/25 | Loss: 0.00329539
Iteration 23/25 | Loss: 0.00329539
Iteration 24/25 | Loss: 0.00329539
Iteration 25/25 | Loss: 0.00329539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329539
Iteration 2/1000 | Loss: 0.00126742
Iteration 3/1000 | Loss: 0.00405920
Iteration 4/1000 | Loss: 0.00205938
Iteration 5/1000 | Loss: 0.00384571
Iteration 6/1000 | Loss: 0.00375237
Iteration 7/1000 | Loss: 0.00411708
Iteration 8/1000 | Loss: 0.00641868
Iteration 9/1000 | Loss: 0.00329210
Iteration 10/1000 | Loss: 0.00353271
Iteration 11/1000 | Loss: 0.00302058
Iteration 12/1000 | Loss: 0.00186035
Iteration 13/1000 | Loss: 0.00239767
Iteration 14/1000 | Loss: 0.00478888
Iteration 15/1000 | Loss: 0.00203111
Iteration 16/1000 | Loss: 0.00334102
Iteration 17/1000 | Loss: 0.00249651
Iteration 18/1000 | Loss: 0.00468720
Iteration 19/1000 | Loss: 0.00341811
Iteration 20/1000 | Loss: 0.00290189
Iteration 21/1000 | Loss: 0.00335971
Iteration 22/1000 | Loss: 0.00105576
Iteration 23/1000 | Loss: 0.00029589
Iteration 24/1000 | Loss: 0.00042550
Iteration 25/1000 | Loss: 0.00198628
Iteration 26/1000 | Loss: 0.00193130
Iteration 27/1000 | Loss: 0.00179993
Iteration 28/1000 | Loss: 0.00367333
Iteration 29/1000 | Loss: 0.00120505
Iteration 30/1000 | Loss: 0.00143983
Iteration 31/1000 | Loss: 0.00267632
Iteration 32/1000 | Loss: 0.00408823
Iteration 33/1000 | Loss: 0.00200188
Iteration 34/1000 | Loss: 0.00041048
Iteration 35/1000 | Loss: 0.00259054
Iteration 36/1000 | Loss: 0.00144983
Iteration 37/1000 | Loss: 0.00051844
Iteration 38/1000 | Loss: 0.00034895
Iteration 39/1000 | Loss: 0.00070590
Iteration 40/1000 | Loss: 0.00024755
Iteration 41/1000 | Loss: 0.00015023
Iteration 42/1000 | Loss: 0.00065935
Iteration 43/1000 | Loss: 0.00070031
Iteration 44/1000 | Loss: 0.00017877
Iteration 45/1000 | Loss: 0.00038040
Iteration 46/1000 | Loss: 0.00084643
Iteration 47/1000 | Loss: 0.00029387
Iteration 48/1000 | Loss: 0.00040707
Iteration 49/1000 | Loss: 0.00041878
Iteration 50/1000 | Loss: 0.00019941
Iteration 51/1000 | Loss: 0.00022187
Iteration 52/1000 | Loss: 0.00020936
Iteration 53/1000 | Loss: 0.00045477
Iteration 54/1000 | Loss: 0.00039342
Iteration 55/1000 | Loss: 0.00032017
Iteration 56/1000 | Loss: 0.00024984
Iteration 57/1000 | Loss: 0.00013386
Iteration 58/1000 | Loss: 0.00020786
Iteration 59/1000 | Loss: 0.00080372
Iteration 60/1000 | Loss: 0.00070574
Iteration 61/1000 | Loss: 0.00067634
Iteration 62/1000 | Loss: 0.00094157
Iteration 63/1000 | Loss: 0.00025096
Iteration 64/1000 | Loss: 0.00055187
Iteration 65/1000 | Loss: 0.00087203
Iteration 66/1000 | Loss: 0.00072196
Iteration 67/1000 | Loss: 0.00094526
Iteration 68/1000 | Loss: 0.00029954
Iteration 69/1000 | Loss: 0.00100753
Iteration 70/1000 | Loss: 0.00029251
Iteration 71/1000 | Loss: 0.00065480
Iteration 72/1000 | Loss: 0.00031091
Iteration 73/1000 | Loss: 0.00022998
Iteration 74/1000 | Loss: 0.00008044
Iteration 75/1000 | Loss: 0.00010682
Iteration 76/1000 | Loss: 0.00016666
Iteration 77/1000 | Loss: 0.00007620
Iteration 78/1000 | Loss: 0.00007268
Iteration 79/1000 | Loss: 0.00010469
Iteration 80/1000 | Loss: 0.00009280
Iteration 81/1000 | Loss: 0.00007484
Iteration 82/1000 | Loss: 0.00008396
Iteration 83/1000 | Loss: 0.00008743
Iteration 84/1000 | Loss: 0.00009448
Iteration 85/1000 | Loss: 0.00007146
Iteration 86/1000 | Loss: 0.00006807
Iteration 87/1000 | Loss: 0.00038138
Iteration 88/1000 | Loss: 0.00033815
Iteration 89/1000 | Loss: 0.00022893
Iteration 90/1000 | Loss: 0.00023974
Iteration 91/1000 | Loss: 0.00007266
Iteration 92/1000 | Loss: 0.00006703
Iteration 93/1000 | Loss: 0.00006204
Iteration 94/1000 | Loss: 0.00005953
Iteration 95/1000 | Loss: 0.00005834
Iteration 96/1000 | Loss: 0.00009895
Iteration 97/1000 | Loss: 0.00060358
Iteration 98/1000 | Loss: 0.00025226
Iteration 99/1000 | Loss: 0.00009819
Iteration 100/1000 | Loss: 0.00053736
Iteration 101/1000 | Loss: 0.00036644
Iteration 102/1000 | Loss: 0.00006014
Iteration 103/1000 | Loss: 0.00005750
Iteration 104/1000 | Loss: 0.00010130
Iteration 105/1000 | Loss: 0.00061980
Iteration 106/1000 | Loss: 0.00013713
Iteration 107/1000 | Loss: 0.00010292
Iteration 108/1000 | Loss: 0.00009421
Iteration 109/1000 | Loss: 0.00006640
Iteration 110/1000 | Loss: 0.00006017
Iteration 111/1000 | Loss: 0.00005563
Iteration 112/1000 | Loss: 0.00005342
Iteration 113/1000 | Loss: 0.00005199
Iteration 114/1000 | Loss: 0.00005128
Iteration 115/1000 | Loss: 0.00005094
Iteration 116/1000 | Loss: 0.00005074
Iteration 117/1000 | Loss: 0.00005059
Iteration 118/1000 | Loss: 0.00005055
Iteration 119/1000 | Loss: 0.00005052
Iteration 120/1000 | Loss: 0.00005052
Iteration 121/1000 | Loss: 0.00005051
Iteration 122/1000 | Loss: 0.00005050
Iteration 123/1000 | Loss: 0.00005049
Iteration 124/1000 | Loss: 0.00005049
Iteration 125/1000 | Loss: 0.00005049
Iteration 126/1000 | Loss: 0.00005048
Iteration 127/1000 | Loss: 0.00005048
Iteration 128/1000 | Loss: 0.00005048
Iteration 129/1000 | Loss: 0.00005047
Iteration 130/1000 | Loss: 0.00005047
Iteration 131/1000 | Loss: 0.00005046
Iteration 132/1000 | Loss: 0.00005046
Iteration 133/1000 | Loss: 0.00005046
Iteration 134/1000 | Loss: 0.00005046
Iteration 135/1000 | Loss: 0.00005045
Iteration 136/1000 | Loss: 0.00005045
Iteration 137/1000 | Loss: 0.00005045
Iteration 138/1000 | Loss: 0.00005045
Iteration 139/1000 | Loss: 0.00005044
Iteration 140/1000 | Loss: 0.00005044
Iteration 141/1000 | Loss: 0.00005044
Iteration 142/1000 | Loss: 0.00005043
Iteration 143/1000 | Loss: 0.00005043
Iteration 144/1000 | Loss: 0.00005043
Iteration 145/1000 | Loss: 0.00005042
Iteration 146/1000 | Loss: 0.00005042
Iteration 147/1000 | Loss: 0.00005042
Iteration 148/1000 | Loss: 0.00005042
Iteration 149/1000 | Loss: 0.00005042
Iteration 150/1000 | Loss: 0.00005042
Iteration 151/1000 | Loss: 0.00005042
Iteration 152/1000 | Loss: 0.00005042
Iteration 153/1000 | Loss: 0.00005041
Iteration 154/1000 | Loss: 0.00005041
Iteration 155/1000 | Loss: 0.00005041
Iteration 156/1000 | Loss: 0.00005041
Iteration 157/1000 | Loss: 0.00005041
Iteration 158/1000 | Loss: 0.00005041
Iteration 159/1000 | Loss: 0.00005041
Iteration 160/1000 | Loss: 0.00005040
Iteration 161/1000 | Loss: 0.00005040
Iteration 162/1000 | Loss: 0.00005040
Iteration 163/1000 | Loss: 0.00005040
Iteration 164/1000 | Loss: 0.00005040
Iteration 165/1000 | Loss: 0.00005040
Iteration 166/1000 | Loss: 0.00005040
Iteration 167/1000 | Loss: 0.00005040
Iteration 168/1000 | Loss: 0.00005040
Iteration 169/1000 | Loss: 0.00005040
Iteration 170/1000 | Loss: 0.00005039
Iteration 171/1000 | Loss: 0.00005039
Iteration 172/1000 | Loss: 0.00005039
Iteration 173/1000 | Loss: 0.00005039
Iteration 174/1000 | Loss: 0.00005039
Iteration 175/1000 | Loss: 0.00005039
Iteration 176/1000 | Loss: 0.00005039
Iteration 177/1000 | Loss: 0.00005039
Iteration 178/1000 | Loss: 0.00005039
Iteration 179/1000 | Loss: 0.00005038
Iteration 180/1000 | Loss: 0.00005038
Iteration 181/1000 | Loss: 0.00005038
Iteration 182/1000 | Loss: 0.00005038
Iteration 183/1000 | Loss: 0.00005038
Iteration 184/1000 | Loss: 0.00005038
Iteration 185/1000 | Loss: 0.00005038
Iteration 186/1000 | Loss: 0.00005038
Iteration 187/1000 | Loss: 0.00005037
Iteration 188/1000 | Loss: 0.00005037
Iteration 189/1000 | Loss: 0.00005037
Iteration 190/1000 | Loss: 0.00005037
Iteration 191/1000 | Loss: 0.00005037
Iteration 192/1000 | Loss: 0.00005036
Iteration 193/1000 | Loss: 0.00005036
Iteration 194/1000 | Loss: 0.00005036
Iteration 195/1000 | Loss: 0.00005036
Iteration 196/1000 | Loss: 0.00005035
Iteration 197/1000 | Loss: 0.00005035
Iteration 198/1000 | Loss: 0.00005035
Iteration 199/1000 | Loss: 0.00005035
Iteration 200/1000 | Loss: 0.00005035
Iteration 201/1000 | Loss: 0.00005035
Iteration 202/1000 | Loss: 0.00005035
Iteration 203/1000 | Loss: 0.00005035
Iteration 204/1000 | Loss: 0.00005035
Iteration 205/1000 | Loss: 0.00005035
Iteration 206/1000 | Loss: 0.00005035
Iteration 207/1000 | Loss: 0.00005035
Iteration 208/1000 | Loss: 0.00005035
Iteration 209/1000 | Loss: 0.00005035
Iteration 210/1000 | Loss: 0.00005035
Iteration 211/1000 | Loss: 0.00005034
Iteration 212/1000 | Loss: 0.00005034
Iteration 213/1000 | Loss: 0.00005034
Iteration 214/1000 | Loss: 0.00005034
Iteration 215/1000 | Loss: 0.00005034
Iteration 216/1000 | Loss: 0.00005034
Iteration 217/1000 | Loss: 0.00005034
Iteration 218/1000 | Loss: 0.00005034
Iteration 219/1000 | Loss: 0.00005034
Iteration 220/1000 | Loss: 0.00005034
Iteration 221/1000 | Loss: 0.00005034
Iteration 222/1000 | Loss: 0.00005034
Iteration 223/1000 | Loss: 0.00005034
Iteration 224/1000 | Loss: 0.00005034
Iteration 225/1000 | Loss: 0.00005034
Iteration 226/1000 | Loss: 0.00005033
Iteration 227/1000 | Loss: 0.00005033
Iteration 228/1000 | Loss: 0.00005033
Iteration 229/1000 | Loss: 0.00005033
Iteration 230/1000 | Loss: 0.00005033
Iteration 231/1000 | Loss: 0.00005033
Iteration 232/1000 | Loss: 0.00005033
Iteration 233/1000 | Loss: 0.00005033
Iteration 234/1000 | Loss: 0.00005033
Iteration 235/1000 | Loss: 0.00005033
Iteration 236/1000 | Loss: 0.00005033
Iteration 237/1000 | Loss: 0.00005033
Iteration 238/1000 | Loss: 0.00005033
Iteration 239/1000 | Loss: 0.00005033
Iteration 240/1000 | Loss: 0.00005033
Iteration 241/1000 | Loss: 0.00005033
Iteration 242/1000 | Loss: 0.00005033
Iteration 243/1000 | Loss: 0.00005033
Iteration 244/1000 | Loss: 0.00005033
Iteration 245/1000 | Loss: 0.00005033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [5.032767876400612e-05, 5.032767876400612e-05, 5.032767876400612e-05, 5.032767876400612e-05, 5.032767876400612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.032767876400612e-05

Optimization complete. Final v2v error: 5.339087963104248 mm

Highest mean error: 14.00388240814209 mm for frame 100

Lowest mean error: 4.699069976806641 mm for frame 22

Saving results

Total time: 218.16413927078247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857897
Iteration 2/25 | Loss: 0.00188062
Iteration 3/25 | Loss: 0.00151246
Iteration 4/25 | Loss: 0.00146595
Iteration 5/25 | Loss: 0.00145564
Iteration 6/25 | Loss: 0.00145293
Iteration 7/25 | Loss: 0.00145237
Iteration 8/25 | Loss: 0.00145237
Iteration 9/25 | Loss: 0.00145237
Iteration 10/25 | Loss: 0.00145237
Iteration 11/25 | Loss: 0.00145237
Iteration 12/25 | Loss: 0.00145237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014523703139275312, 0.0014523703139275312, 0.0014523703139275312, 0.0014523703139275312, 0.0014523703139275312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014523703139275312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.24087475
Iteration 2/25 | Loss: 0.00069747
Iteration 3/25 | Loss: 0.00069747
Iteration 4/25 | Loss: 0.00069747
Iteration 5/25 | Loss: 0.00069747
Iteration 6/25 | Loss: 0.00069747
Iteration 7/25 | Loss: 0.00069747
Iteration 8/25 | Loss: 0.00069747
Iteration 9/25 | Loss: 0.00069747
Iteration 10/25 | Loss: 0.00069747
Iteration 11/25 | Loss: 0.00069747
Iteration 12/25 | Loss: 0.00069747
Iteration 13/25 | Loss: 0.00069747
Iteration 14/25 | Loss: 0.00069747
Iteration 15/25 | Loss: 0.00069747
Iteration 16/25 | Loss: 0.00069747
Iteration 17/25 | Loss: 0.00069747
Iteration 18/25 | Loss: 0.00069747
Iteration 19/25 | Loss: 0.00069747
Iteration 20/25 | Loss: 0.00069747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006974713178351521, 0.0006974713178351521, 0.0006974713178351521, 0.0006974713178351521, 0.0006974713178351521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006974713178351521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069747
Iteration 2/1000 | Loss: 0.00009946
Iteration 3/1000 | Loss: 0.00007337
Iteration 4/1000 | Loss: 0.00006484
Iteration 5/1000 | Loss: 0.00005908
Iteration 6/1000 | Loss: 0.00005643
Iteration 7/1000 | Loss: 0.00005460
Iteration 8/1000 | Loss: 0.00005382
Iteration 9/1000 | Loss: 0.00005336
Iteration 10/1000 | Loss: 0.00005303
Iteration 11/1000 | Loss: 0.00005271
Iteration 12/1000 | Loss: 0.00005248
Iteration 13/1000 | Loss: 0.00005240
Iteration 14/1000 | Loss: 0.00005240
Iteration 15/1000 | Loss: 0.00005238
Iteration 16/1000 | Loss: 0.00005235
Iteration 17/1000 | Loss: 0.00005235
Iteration 18/1000 | Loss: 0.00005231
Iteration 19/1000 | Loss: 0.00005230
Iteration 20/1000 | Loss: 0.00005230
Iteration 21/1000 | Loss: 0.00005229
Iteration 22/1000 | Loss: 0.00005229
Iteration 23/1000 | Loss: 0.00005227
Iteration 24/1000 | Loss: 0.00005226
Iteration 25/1000 | Loss: 0.00005226
Iteration 26/1000 | Loss: 0.00005226
Iteration 27/1000 | Loss: 0.00005226
Iteration 28/1000 | Loss: 0.00005226
Iteration 29/1000 | Loss: 0.00005226
Iteration 30/1000 | Loss: 0.00005226
Iteration 31/1000 | Loss: 0.00005225
Iteration 32/1000 | Loss: 0.00005225
Iteration 33/1000 | Loss: 0.00005224
Iteration 34/1000 | Loss: 0.00005223
Iteration 35/1000 | Loss: 0.00005223
Iteration 36/1000 | Loss: 0.00005223
Iteration 37/1000 | Loss: 0.00005222
Iteration 38/1000 | Loss: 0.00005222
Iteration 39/1000 | Loss: 0.00005222
Iteration 40/1000 | Loss: 0.00005221
Iteration 41/1000 | Loss: 0.00005220
Iteration 42/1000 | Loss: 0.00005220
Iteration 43/1000 | Loss: 0.00005220
Iteration 44/1000 | Loss: 0.00005220
Iteration 45/1000 | Loss: 0.00005219
Iteration 46/1000 | Loss: 0.00005218
Iteration 47/1000 | Loss: 0.00005218
Iteration 48/1000 | Loss: 0.00005218
Iteration 49/1000 | Loss: 0.00005218
Iteration 50/1000 | Loss: 0.00005218
Iteration 51/1000 | Loss: 0.00005217
Iteration 52/1000 | Loss: 0.00005217
Iteration 53/1000 | Loss: 0.00005217
Iteration 54/1000 | Loss: 0.00005217
Iteration 55/1000 | Loss: 0.00005217
Iteration 56/1000 | Loss: 0.00005217
Iteration 57/1000 | Loss: 0.00005217
Iteration 58/1000 | Loss: 0.00005217
Iteration 59/1000 | Loss: 0.00005217
Iteration 60/1000 | Loss: 0.00005217
Iteration 61/1000 | Loss: 0.00005217
Iteration 62/1000 | Loss: 0.00005217
Iteration 63/1000 | Loss: 0.00005216
Iteration 64/1000 | Loss: 0.00005216
Iteration 65/1000 | Loss: 0.00005216
Iteration 66/1000 | Loss: 0.00005216
Iteration 67/1000 | Loss: 0.00005216
Iteration 68/1000 | Loss: 0.00005216
Iteration 69/1000 | Loss: 0.00005216
Iteration 70/1000 | Loss: 0.00005216
Iteration 71/1000 | Loss: 0.00005216
Iteration 72/1000 | Loss: 0.00005215
Iteration 73/1000 | Loss: 0.00005215
Iteration 74/1000 | Loss: 0.00005215
Iteration 75/1000 | Loss: 0.00005214
Iteration 76/1000 | Loss: 0.00005214
Iteration 77/1000 | Loss: 0.00005214
Iteration 78/1000 | Loss: 0.00005214
Iteration 79/1000 | Loss: 0.00005213
Iteration 80/1000 | Loss: 0.00005213
Iteration 81/1000 | Loss: 0.00005213
Iteration 82/1000 | Loss: 0.00005213
Iteration 83/1000 | Loss: 0.00005212
Iteration 84/1000 | Loss: 0.00005212
Iteration 85/1000 | Loss: 0.00005212
Iteration 86/1000 | Loss: 0.00005212
Iteration 87/1000 | Loss: 0.00005212
Iteration 88/1000 | Loss: 0.00005212
Iteration 89/1000 | Loss: 0.00005212
Iteration 90/1000 | Loss: 0.00005212
Iteration 91/1000 | Loss: 0.00005212
Iteration 92/1000 | Loss: 0.00005212
Iteration 93/1000 | Loss: 0.00005212
Iteration 94/1000 | Loss: 0.00005211
Iteration 95/1000 | Loss: 0.00005211
Iteration 96/1000 | Loss: 0.00005210
Iteration 97/1000 | Loss: 0.00005210
Iteration 98/1000 | Loss: 0.00005210
Iteration 99/1000 | Loss: 0.00005210
Iteration 100/1000 | Loss: 0.00005209
Iteration 101/1000 | Loss: 0.00005209
Iteration 102/1000 | Loss: 0.00005209
Iteration 103/1000 | Loss: 0.00005209
Iteration 104/1000 | Loss: 0.00005209
Iteration 105/1000 | Loss: 0.00005209
Iteration 106/1000 | Loss: 0.00005209
Iteration 107/1000 | Loss: 0.00005209
Iteration 108/1000 | Loss: 0.00005209
Iteration 109/1000 | Loss: 0.00005208
Iteration 110/1000 | Loss: 0.00005208
Iteration 111/1000 | Loss: 0.00005208
Iteration 112/1000 | Loss: 0.00005208
Iteration 113/1000 | Loss: 0.00005208
Iteration 114/1000 | Loss: 0.00005208
Iteration 115/1000 | Loss: 0.00005207
Iteration 116/1000 | Loss: 0.00005207
Iteration 117/1000 | Loss: 0.00005207
Iteration 118/1000 | Loss: 0.00005207
Iteration 119/1000 | Loss: 0.00005207
Iteration 120/1000 | Loss: 0.00005207
Iteration 121/1000 | Loss: 0.00005206
Iteration 122/1000 | Loss: 0.00005206
Iteration 123/1000 | Loss: 0.00005206
Iteration 124/1000 | Loss: 0.00005206
Iteration 125/1000 | Loss: 0.00005206
Iteration 126/1000 | Loss: 0.00005206
Iteration 127/1000 | Loss: 0.00005206
Iteration 128/1000 | Loss: 0.00005206
Iteration 129/1000 | Loss: 0.00005206
Iteration 130/1000 | Loss: 0.00005206
Iteration 131/1000 | Loss: 0.00005205
Iteration 132/1000 | Loss: 0.00005205
Iteration 133/1000 | Loss: 0.00005205
Iteration 134/1000 | Loss: 0.00005205
Iteration 135/1000 | Loss: 0.00005205
Iteration 136/1000 | Loss: 0.00005205
Iteration 137/1000 | Loss: 0.00005204
Iteration 138/1000 | Loss: 0.00005204
Iteration 139/1000 | Loss: 0.00005204
Iteration 140/1000 | Loss: 0.00005204
Iteration 141/1000 | Loss: 0.00005204
Iteration 142/1000 | Loss: 0.00005204
Iteration 143/1000 | Loss: 0.00005204
Iteration 144/1000 | Loss: 0.00005204
Iteration 145/1000 | Loss: 0.00005204
Iteration 146/1000 | Loss: 0.00005204
Iteration 147/1000 | Loss: 0.00005204
Iteration 148/1000 | Loss: 0.00005204
Iteration 149/1000 | Loss: 0.00005204
Iteration 150/1000 | Loss: 0.00005203
Iteration 151/1000 | Loss: 0.00005203
Iteration 152/1000 | Loss: 0.00005203
Iteration 153/1000 | Loss: 0.00005203
Iteration 154/1000 | Loss: 0.00005203
Iteration 155/1000 | Loss: 0.00005203
Iteration 156/1000 | Loss: 0.00005203
Iteration 157/1000 | Loss: 0.00005203
Iteration 158/1000 | Loss: 0.00005203
Iteration 159/1000 | Loss: 0.00005203
Iteration 160/1000 | Loss: 0.00005203
Iteration 161/1000 | Loss: 0.00005203
Iteration 162/1000 | Loss: 0.00005202
Iteration 163/1000 | Loss: 0.00005202
Iteration 164/1000 | Loss: 0.00005202
Iteration 165/1000 | Loss: 0.00005202
Iteration 166/1000 | Loss: 0.00005202
Iteration 167/1000 | Loss: 0.00005202
Iteration 168/1000 | Loss: 0.00005202
Iteration 169/1000 | Loss: 0.00005202
Iteration 170/1000 | Loss: 0.00005202
Iteration 171/1000 | Loss: 0.00005202
Iteration 172/1000 | Loss: 0.00005202
Iteration 173/1000 | Loss: 0.00005202
Iteration 174/1000 | Loss: 0.00005202
Iteration 175/1000 | Loss: 0.00005202
Iteration 176/1000 | Loss: 0.00005201
Iteration 177/1000 | Loss: 0.00005201
Iteration 178/1000 | Loss: 0.00005201
Iteration 179/1000 | Loss: 0.00005201
Iteration 180/1000 | Loss: 0.00005201
Iteration 181/1000 | Loss: 0.00005201
Iteration 182/1000 | Loss: 0.00005201
Iteration 183/1000 | Loss: 0.00005201
Iteration 184/1000 | Loss: 0.00005201
Iteration 185/1000 | Loss: 0.00005201
Iteration 186/1000 | Loss: 0.00005201
Iteration 187/1000 | Loss: 0.00005201
Iteration 188/1000 | Loss: 0.00005201
Iteration 189/1000 | Loss: 0.00005201
Iteration 190/1000 | Loss: 0.00005200
Iteration 191/1000 | Loss: 0.00005200
Iteration 192/1000 | Loss: 0.00005200
Iteration 193/1000 | Loss: 0.00005200
Iteration 194/1000 | Loss: 0.00005200
Iteration 195/1000 | Loss: 0.00005200
Iteration 196/1000 | Loss: 0.00005200
Iteration 197/1000 | Loss: 0.00005200
Iteration 198/1000 | Loss: 0.00005200
Iteration 199/1000 | Loss: 0.00005200
Iteration 200/1000 | Loss: 0.00005200
Iteration 201/1000 | Loss: 0.00005200
Iteration 202/1000 | Loss: 0.00005200
Iteration 203/1000 | Loss: 0.00005200
Iteration 204/1000 | Loss: 0.00005200
Iteration 205/1000 | Loss: 0.00005199
Iteration 206/1000 | Loss: 0.00005199
Iteration 207/1000 | Loss: 0.00005199
Iteration 208/1000 | Loss: 0.00005199
Iteration 209/1000 | Loss: 0.00005199
Iteration 210/1000 | Loss: 0.00005199
Iteration 211/1000 | Loss: 0.00005199
Iteration 212/1000 | Loss: 0.00005199
Iteration 213/1000 | Loss: 0.00005199
Iteration 214/1000 | Loss: 0.00005199
Iteration 215/1000 | Loss: 0.00005198
Iteration 216/1000 | Loss: 0.00005198
Iteration 217/1000 | Loss: 0.00005198
Iteration 218/1000 | Loss: 0.00005198
Iteration 219/1000 | Loss: 0.00005198
Iteration 220/1000 | Loss: 0.00005198
Iteration 221/1000 | Loss: 0.00005198
Iteration 222/1000 | Loss: 0.00005198
Iteration 223/1000 | Loss: 0.00005198
Iteration 224/1000 | Loss: 0.00005198
Iteration 225/1000 | Loss: 0.00005198
Iteration 226/1000 | Loss: 0.00005198
Iteration 227/1000 | Loss: 0.00005198
Iteration 228/1000 | Loss: 0.00005198
Iteration 229/1000 | Loss: 0.00005198
Iteration 230/1000 | Loss: 0.00005198
Iteration 231/1000 | Loss: 0.00005198
Iteration 232/1000 | Loss: 0.00005198
Iteration 233/1000 | Loss: 0.00005198
Iteration 234/1000 | Loss: 0.00005198
Iteration 235/1000 | Loss: 0.00005198
Iteration 236/1000 | Loss: 0.00005198
Iteration 237/1000 | Loss: 0.00005198
Iteration 238/1000 | Loss: 0.00005198
Iteration 239/1000 | Loss: 0.00005198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [5.1975450332975015e-05, 5.1975450332975015e-05, 5.1975450332975015e-05, 5.1975450332975015e-05, 5.1975450332975015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.1975450332975015e-05

Optimization complete. Final v2v error: 5.907904624938965 mm

Highest mean error: 7.346457004547119 mm for frame 9

Lowest mean error: 4.618790149688721 mm for frame 155

Saving results

Total time: 43.85915946960449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622047
Iteration 2/25 | Loss: 0.00152790
Iteration 3/25 | Loss: 0.00144415
Iteration 4/25 | Loss: 0.00142506
Iteration 5/25 | Loss: 0.00142018
Iteration 6/25 | Loss: 0.00141942
Iteration 7/25 | Loss: 0.00141942
Iteration 8/25 | Loss: 0.00141942
Iteration 9/25 | Loss: 0.00141942
Iteration 10/25 | Loss: 0.00141942
Iteration 11/25 | Loss: 0.00141942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014194194227457047, 0.0014194194227457047, 0.0014194194227457047, 0.0014194194227457047, 0.0014194194227457047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014194194227457047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35722744
Iteration 2/25 | Loss: 0.00123081
Iteration 3/25 | Loss: 0.00123078
Iteration 4/25 | Loss: 0.00123077
Iteration 5/25 | Loss: 0.00123077
Iteration 6/25 | Loss: 0.00123077
Iteration 7/25 | Loss: 0.00123077
Iteration 8/25 | Loss: 0.00123077
Iteration 9/25 | Loss: 0.00123077
Iteration 10/25 | Loss: 0.00123077
Iteration 11/25 | Loss: 0.00123077
Iteration 12/25 | Loss: 0.00123077
Iteration 13/25 | Loss: 0.00123077
Iteration 14/25 | Loss: 0.00123077
Iteration 15/25 | Loss: 0.00123077
Iteration 16/25 | Loss: 0.00123077
Iteration 17/25 | Loss: 0.00123077
Iteration 18/25 | Loss: 0.00123077
Iteration 19/25 | Loss: 0.00123077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012307728175073862, 0.0012307728175073862, 0.0012307728175073862, 0.0012307728175073862, 0.0012307728175073862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012307728175073862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123077
Iteration 2/1000 | Loss: 0.00006030
Iteration 3/1000 | Loss: 0.00004999
Iteration 4/1000 | Loss: 0.00004568
Iteration 5/1000 | Loss: 0.00004309
Iteration 6/1000 | Loss: 0.00004176
Iteration 7/1000 | Loss: 0.00004111
Iteration 8/1000 | Loss: 0.00004063
Iteration 9/1000 | Loss: 0.00004032
Iteration 10/1000 | Loss: 0.00004001
Iteration 11/1000 | Loss: 0.00003981
Iteration 12/1000 | Loss: 0.00003968
Iteration 13/1000 | Loss: 0.00003965
Iteration 14/1000 | Loss: 0.00003959
Iteration 15/1000 | Loss: 0.00003959
Iteration 16/1000 | Loss: 0.00003958
Iteration 17/1000 | Loss: 0.00003957
Iteration 18/1000 | Loss: 0.00003956
Iteration 19/1000 | Loss: 0.00003956
Iteration 20/1000 | Loss: 0.00003955
Iteration 21/1000 | Loss: 0.00003955
Iteration 22/1000 | Loss: 0.00003954
Iteration 23/1000 | Loss: 0.00003953
Iteration 24/1000 | Loss: 0.00003952
Iteration 25/1000 | Loss: 0.00003952
Iteration 26/1000 | Loss: 0.00003952
Iteration 27/1000 | Loss: 0.00003952
Iteration 28/1000 | Loss: 0.00003952
Iteration 29/1000 | Loss: 0.00003952
Iteration 30/1000 | Loss: 0.00003952
Iteration 31/1000 | Loss: 0.00003952
Iteration 32/1000 | Loss: 0.00003952
Iteration 33/1000 | Loss: 0.00003952
Iteration 34/1000 | Loss: 0.00003952
Iteration 35/1000 | Loss: 0.00003951
Iteration 36/1000 | Loss: 0.00003951
Iteration 37/1000 | Loss: 0.00003951
Iteration 38/1000 | Loss: 0.00003950
Iteration 39/1000 | Loss: 0.00003950
Iteration 40/1000 | Loss: 0.00003949
Iteration 41/1000 | Loss: 0.00003949
Iteration 42/1000 | Loss: 0.00003949
Iteration 43/1000 | Loss: 0.00003949
Iteration 44/1000 | Loss: 0.00003948
Iteration 45/1000 | Loss: 0.00003948
Iteration 46/1000 | Loss: 0.00003948
Iteration 47/1000 | Loss: 0.00003948
Iteration 48/1000 | Loss: 0.00003948
Iteration 49/1000 | Loss: 0.00003947
Iteration 50/1000 | Loss: 0.00003947
Iteration 51/1000 | Loss: 0.00003947
Iteration 52/1000 | Loss: 0.00003946
Iteration 53/1000 | Loss: 0.00003946
Iteration 54/1000 | Loss: 0.00003946
Iteration 55/1000 | Loss: 0.00003946
Iteration 56/1000 | Loss: 0.00003946
Iteration 57/1000 | Loss: 0.00003945
Iteration 58/1000 | Loss: 0.00003945
Iteration 59/1000 | Loss: 0.00003945
Iteration 60/1000 | Loss: 0.00003944
Iteration 61/1000 | Loss: 0.00003944
Iteration 62/1000 | Loss: 0.00003943
Iteration 63/1000 | Loss: 0.00003943
Iteration 64/1000 | Loss: 0.00003943
Iteration 65/1000 | Loss: 0.00003943
Iteration 66/1000 | Loss: 0.00003943
Iteration 67/1000 | Loss: 0.00003943
Iteration 68/1000 | Loss: 0.00003943
Iteration 69/1000 | Loss: 0.00003943
Iteration 70/1000 | Loss: 0.00003942
Iteration 71/1000 | Loss: 0.00003942
Iteration 72/1000 | Loss: 0.00003942
Iteration 73/1000 | Loss: 0.00003941
Iteration 74/1000 | Loss: 0.00003941
Iteration 75/1000 | Loss: 0.00003941
Iteration 76/1000 | Loss: 0.00003941
Iteration 77/1000 | Loss: 0.00003941
Iteration 78/1000 | Loss: 0.00003941
Iteration 79/1000 | Loss: 0.00003940
Iteration 80/1000 | Loss: 0.00003940
Iteration 81/1000 | Loss: 0.00003940
Iteration 82/1000 | Loss: 0.00003940
Iteration 83/1000 | Loss: 0.00003940
Iteration 84/1000 | Loss: 0.00003940
Iteration 85/1000 | Loss: 0.00003940
Iteration 86/1000 | Loss: 0.00003940
Iteration 87/1000 | Loss: 0.00003939
Iteration 88/1000 | Loss: 0.00003939
Iteration 89/1000 | Loss: 0.00003939
Iteration 90/1000 | Loss: 0.00003939
Iteration 91/1000 | Loss: 0.00003939
Iteration 92/1000 | Loss: 0.00003939
Iteration 93/1000 | Loss: 0.00003939
Iteration 94/1000 | Loss: 0.00003939
Iteration 95/1000 | Loss: 0.00003939
Iteration 96/1000 | Loss: 0.00003939
Iteration 97/1000 | Loss: 0.00003939
Iteration 98/1000 | Loss: 0.00003939
Iteration 99/1000 | Loss: 0.00003939
Iteration 100/1000 | Loss: 0.00003939
Iteration 101/1000 | Loss: 0.00003939
Iteration 102/1000 | Loss: 0.00003939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [3.9389960875269026e-05, 3.9389960875269026e-05, 3.9389960875269026e-05, 3.9389960875269026e-05, 3.9389960875269026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9389960875269026e-05

Optimization complete. Final v2v error: 5.54709529876709 mm

Highest mean error: 5.9621357917785645 mm for frame 108

Lowest mean error: 5.059944152832031 mm for frame 33

Saving results

Total time: 34.482813596725464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934350
Iteration 2/25 | Loss: 0.00165726
Iteration 3/25 | Loss: 0.00149659
Iteration 4/25 | Loss: 0.00145395
Iteration 5/25 | Loss: 0.00145878
Iteration 6/25 | Loss: 0.00145980
Iteration 7/25 | Loss: 0.00143993
Iteration 8/25 | Loss: 0.00142519
Iteration 9/25 | Loss: 0.00142059
Iteration 10/25 | Loss: 0.00141934
Iteration 11/25 | Loss: 0.00141905
Iteration 12/25 | Loss: 0.00141897
Iteration 13/25 | Loss: 0.00141889
Iteration 14/25 | Loss: 0.00141889
Iteration 15/25 | Loss: 0.00141889
Iteration 16/25 | Loss: 0.00141889
Iteration 17/25 | Loss: 0.00141889
Iteration 18/25 | Loss: 0.00141889
Iteration 19/25 | Loss: 0.00141889
Iteration 20/25 | Loss: 0.00141888
Iteration 21/25 | Loss: 0.00141888
Iteration 22/25 | Loss: 0.00141888
Iteration 23/25 | Loss: 0.00141888
Iteration 24/25 | Loss: 0.00141888
Iteration 25/25 | Loss: 0.00141888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42235422
Iteration 2/25 | Loss: 0.00112565
Iteration 3/25 | Loss: 0.00112564
Iteration 4/25 | Loss: 0.00112564
Iteration 5/25 | Loss: 0.00112564
Iteration 6/25 | Loss: 0.00112564
Iteration 7/25 | Loss: 0.00112564
Iteration 8/25 | Loss: 0.00112564
Iteration 9/25 | Loss: 0.00112564
Iteration 10/25 | Loss: 0.00112564
Iteration 11/25 | Loss: 0.00112564
Iteration 12/25 | Loss: 0.00112564
Iteration 13/25 | Loss: 0.00112564
Iteration 14/25 | Loss: 0.00112564
Iteration 15/25 | Loss: 0.00112564
Iteration 16/25 | Loss: 0.00112564
Iteration 17/25 | Loss: 0.00112564
Iteration 18/25 | Loss: 0.00112564
Iteration 19/25 | Loss: 0.00112564
Iteration 20/25 | Loss: 0.00112564
Iteration 21/25 | Loss: 0.00112564
Iteration 22/25 | Loss: 0.00112564
Iteration 23/25 | Loss: 0.00112564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011256389552727342, 0.0011256389552727342, 0.0011256389552727342, 0.0011256389552727342, 0.0011256389552727342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011256389552727342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112564
Iteration 2/1000 | Loss: 0.00006960
Iteration 3/1000 | Loss: 0.00005056
Iteration 4/1000 | Loss: 0.00004340
Iteration 5/1000 | Loss: 0.00003871
Iteration 6/1000 | Loss: 0.00003673
Iteration 7/1000 | Loss: 0.00003534
Iteration 8/1000 | Loss: 0.00003450
Iteration 9/1000 | Loss: 0.00003385
Iteration 10/1000 | Loss: 0.00003348
Iteration 11/1000 | Loss: 0.00003316
Iteration 12/1000 | Loss: 0.00003290
Iteration 13/1000 | Loss: 0.00003266
Iteration 14/1000 | Loss: 0.00003248
Iteration 15/1000 | Loss: 0.00003241
Iteration 16/1000 | Loss: 0.00003238
Iteration 17/1000 | Loss: 0.00003237
Iteration 18/1000 | Loss: 0.00003237
Iteration 19/1000 | Loss: 0.00003236
Iteration 20/1000 | Loss: 0.00003234
Iteration 21/1000 | Loss: 0.00003234
Iteration 22/1000 | Loss: 0.00003234
Iteration 23/1000 | Loss: 0.00003232
Iteration 24/1000 | Loss: 0.00003231
Iteration 25/1000 | Loss: 0.00003230
Iteration 26/1000 | Loss: 0.00003228
Iteration 27/1000 | Loss: 0.00003227
Iteration 28/1000 | Loss: 0.00003227
Iteration 29/1000 | Loss: 0.00003227
Iteration 30/1000 | Loss: 0.00003226
Iteration 31/1000 | Loss: 0.00003226
Iteration 32/1000 | Loss: 0.00003224
Iteration 33/1000 | Loss: 0.00003224
Iteration 34/1000 | Loss: 0.00003224
Iteration 35/1000 | Loss: 0.00003224
Iteration 36/1000 | Loss: 0.00003224
Iteration 37/1000 | Loss: 0.00003222
Iteration 38/1000 | Loss: 0.00003221
Iteration 39/1000 | Loss: 0.00003221
Iteration 40/1000 | Loss: 0.00003221
Iteration 41/1000 | Loss: 0.00003220
Iteration 42/1000 | Loss: 0.00003220
Iteration 43/1000 | Loss: 0.00003220
Iteration 44/1000 | Loss: 0.00003219
Iteration 45/1000 | Loss: 0.00003219
Iteration 46/1000 | Loss: 0.00003219
Iteration 47/1000 | Loss: 0.00003218
Iteration 48/1000 | Loss: 0.00003218
Iteration 49/1000 | Loss: 0.00003218
Iteration 50/1000 | Loss: 0.00003218
Iteration 51/1000 | Loss: 0.00003218
Iteration 52/1000 | Loss: 0.00003218
Iteration 53/1000 | Loss: 0.00003217
Iteration 54/1000 | Loss: 0.00003217
Iteration 55/1000 | Loss: 0.00003217
Iteration 56/1000 | Loss: 0.00003217
Iteration 57/1000 | Loss: 0.00003217
Iteration 58/1000 | Loss: 0.00003217
Iteration 59/1000 | Loss: 0.00003217
Iteration 60/1000 | Loss: 0.00003216
Iteration 61/1000 | Loss: 0.00003216
Iteration 62/1000 | Loss: 0.00003216
Iteration 63/1000 | Loss: 0.00003216
Iteration 64/1000 | Loss: 0.00003216
Iteration 65/1000 | Loss: 0.00003216
Iteration 66/1000 | Loss: 0.00003216
Iteration 67/1000 | Loss: 0.00003216
Iteration 68/1000 | Loss: 0.00003215
Iteration 69/1000 | Loss: 0.00003215
Iteration 70/1000 | Loss: 0.00003215
Iteration 71/1000 | Loss: 0.00003215
Iteration 72/1000 | Loss: 0.00003214
Iteration 73/1000 | Loss: 0.00003214
Iteration 74/1000 | Loss: 0.00003214
Iteration 75/1000 | Loss: 0.00003213
Iteration 76/1000 | Loss: 0.00003213
Iteration 77/1000 | Loss: 0.00003213
Iteration 78/1000 | Loss: 0.00003213
Iteration 79/1000 | Loss: 0.00003213
Iteration 80/1000 | Loss: 0.00003213
Iteration 81/1000 | Loss: 0.00003213
Iteration 82/1000 | Loss: 0.00003213
Iteration 83/1000 | Loss: 0.00003213
Iteration 84/1000 | Loss: 0.00003213
Iteration 85/1000 | Loss: 0.00003212
Iteration 86/1000 | Loss: 0.00003212
Iteration 87/1000 | Loss: 0.00003212
Iteration 88/1000 | Loss: 0.00003212
Iteration 89/1000 | Loss: 0.00003212
Iteration 90/1000 | Loss: 0.00003212
Iteration 91/1000 | Loss: 0.00003211
Iteration 92/1000 | Loss: 0.00003211
Iteration 93/1000 | Loss: 0.00003211
Iteration 94/1000 | Loss: 0.00003211
Iteration 95/1000 | Loss: 0.00003211
Iteration 96/1000 | Loss: 0.00003210
Iteration 97/1000 | Loss: 0.00003210
Iteration 98/1000 | Loss: 0.00003210
Iteration 99/1000 | Loss: 0.00003210
Iteration 100/1000 | Loss: 0.00003210
Iteration 101/1000 | Loss: 0.00003209
Iteration 102/1000 | Loss: 0.00003209
Iteration 103/1000 | Loss: 0.00003209
Iteration 104/1000 | Loss: 0.00003209
Iteration 105/1000 | Loss: 0.00003209
Iteration 106/1000 | Loss: 0.00003208
Iteration 107/1000 | Loss: 0.00003208
Iteration 108/1000 | Loss: 0.00003208
Iteration 109/1000 | Loss: 0.00003208
Iteration 110/1000 | Loss: 0.00003208
Iteration 111/1000 | Loss: 0.00003208
Iteration 112/1000 | Loss: 0.00003208
Iteration 113/1000 | Loss: 0.00003208
Iteration 114/1000 | Loss: 0.00003208
Iteration 115/1000 | Loss: 0.00003207
Iteration 116/1000 | Loss: 0.00003207
Iteration 117/1000 | Loss: 0.00003207
Iteration 118/1000 | Loss: 0.00003207
Iteration 119/1000 | Loss: 0.00003207
Iteration 120/1000 | Loss: 0.00003207
Iteration 121/1000 | Loss: 0.00003207
Iteration 122/1000 | Loss: 0.00003207
Iteration 123/1000 | Loss: 0.00003207
Iteration 124/1000 | Loss: 0.00003207
Iteration 125/1000 | Loss: 0.00003207
Iteration 126/1000 | Loss: 0.00003207
Iteration 127/1000 | Loss: 0.00003207
Iteration 128/1000 | Loss: 0.00003207
Iteration 129/1000 | Loss: 0.00003207
Iteration 130/1000 | Loss: 0.00003207
Iteration 131/1000 | Loss: 0.00003207
Iteration 132/1000 | Loss: 0.00003207
Iteration 133/1000 | Loss: 0.00003206
Iteration 134/1000 | Loss: 0.00003206
Iteration 135/1000 | Loss: 0.00003206
Iteration 136/1000 | Loss: 0.00003206
Iteration 137/1000 | Loss: 0.00003206
Iteration 138/1000 | Loss: 0.00003206
Iteration 139/1000 | Loss: 0.00003206
Iteration 140/1000 | Loss: 0.00003206
Iteration 141/1000 | Loss: 0.00003206
Iteration 142/1000 | Loss: 0.00003206
Iteration 143/1000 | Loss: 0.00003206
Iteration 144/1000 | Loss: 0.00003206
Iteration 145/1000 | Loss: 0.00003206
Iteration 146/1000 | Loss: 0.00003206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.206012479495257e-05, 3.206012479495257e-05, 3.206012479495257e-05, 3.206012479495257e-05, 3.206012479495257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.206012479495257e-05

Optimization complete. Final v2v error: 5.00194787979126 mm

Highest mean error: 6.590862274169922 mm for frame 64

Lowest mean error: 4.445799827575684 mm for frame 204

Saving results

Total time: 62.56101393699646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441823
Iteration 2/25 | Loss: 0.00162989
Iteration 3/25 | Loss: 0.00135807
Iteration 4/25 | Loss: 0.00132947
Iteration 5/25 | Loss: 0.00132407
Iteration 6/25 | Loss: 0.00132219
Iteration 7/25 | Loss: 0.00132177
Iteration 8/25 | Loss: 0.00132177
Iteration 9/25 | Loss: 0.00132177
Iteration 10/25 | Loss: 0.00132177
Iteration 11/25 | Loss: 0.00132177
Iteration 12/25 | Loss: 0.00132177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013217652449384332, 0.0013217652449384332, 0.0013217652449384332, 0.0013217652449384332, 0.0013217652449384332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013217652449384332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39912665
Iteration 2/25 | Loss: 0.00095291
Iteration 3/25 | Loss: 0.00095291
Iteration 4/25 | Loss: 0.00095291
Iteration 5/25 | Loss: 0.00095291
Iteration 6/25 | Loss: 0.00095291
Iteration 7/25 | Loss: 0.00095291
Iteration 8/25 | Loss: 0.00095291
Iteration 9/25 | Loss: 0.00095290
Iteration 10/25 | Loss: 0.00095290
Iteration 11/25 | Loss: 0.00095290
Iteration 12/25 | Loss: 0.00095290
Iteration 13/25 | Loss: 0.00095290
Iteration 14/25 | Loss: 0.00095290
Iteration 15/25 | Loss: 0.00095290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000952904811128974, 0.000952904811128974, 0.000952904811128974, 0.000952904811128974, 0.000952904811128974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000952904811128974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095290
Iteration 2/1000 | Loss: 0.00005010
Iteration 3/1000 | Loss: 0.00003619
Iteration 4/1000 | Loss: 0.00003188
Iteration 5/1000 | Loss: 0.00002913
Iteration 6/1000 | Loss: 0.00002797
Iteration 7/1000 | Loss: 0.00002711
Iteration 8/1000 | Loss: 0.00002658
Iteration 9/1000 | Loss: 0.00002635
Iteration 10/1000 | Loss: 0.00002627
Iteration 11/1000 | Loss: 0.00002603
Iteration 12/1000 | Loss: 0.00002578
Iteration 13/1000 | Loss: 0.00002562
Iteration 14/1000 | Loss: 0.00002560
Iteration 15/1000 | Loss: 0.00002559
Iteration 16/1000 | Loss: 0.00002556
Iteration 17/1000 | Loss: 0.00002548
Iteration 18/1000 | Loss: 0.00002547
Iteration 19/1000 | Loss: 0.00002544
Iteration 20/1000 | Loss: 0.00002544
Iteration 21/1000 | Loss: 0.00002544
Iteration 22/1000 | Loss: 0.00002544
Iteration 23/1000 | Loss: 0.00002543
Iteration 24/1000 | Loss: 0.00002543
Iteration 25/1000 | Loss: 0.00002542
Iteration 26/1000 | Loss: 0.00002542
Iteration 27/1000 | Loss: 0.00002542
Iteration 28/1000 | Loss: 0.00002540
Iteration 29/1000 | Loss: 0.00002540
Iteration 30/1000 | Loss: 0.00002539
Iteration 31/1000 | Loss: 0.00002539
Iteration 32/1000 | Loss: 0.00002539
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002538
Iteration 35/1000 | Loss: 0.00002536
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002535
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002534
Iteration 40/1000 | Loss: 0.00002534
Iteration 41/1000 | Loss: 0.00002533
Iteration 42/1000 | Loss: 0.00002533
Iteration 43/1000 | Loss: 0.00002533
Iteration 44/1000 | Loss: 0.00002532
Iteration 45/1000 | Loss: 0.00002531
Iteration 46/1000 | Loss: 0.00002530
Iteration 47/1000 | Loss: 0.00002530
Iteration 48/1000 | Loss: 0.00002530
Iteration 49/1000 | Loss: 0.00002530
Iteration 50/1000 | Loss: 0.00002529
Iteration 51/1000 | Loss: 0.00002529
Iteration 52/1000 | Loss: 0.00002529
Iteration 53/1000 | Loss: 0.00002529
Iteration 54/1000 | Loss: 0.00002528
Iteration 55/1000 | Loss: 0.00002528
Iteration 56/1000 | Loss: 0.00002528
Iteration 57/1000 | Loss: 0.00002528
Iteration 58/1000 | Loss: 0.00002528
Iteration 59/1000 | Loss: 0.00002528
Iteration 60/1000 | Loss: 0.00002528
Iteration 61/1000 | Loss: 0.00002528
Iteration 62/1000 | Loss: 0.00002528
Iteration 63/1000 | Loss: 0.00002528
Iteration 64/1000 | Loss: 0.00002527
Iteration 65/1000 | Loss: 0.00002527
Iteration 66/1000 | Loss: 0.00002527
Iteration 67/1000 | Loss: 0.00002527
Iteration 68/1000 | Loss: 0.00002527
Iteration 69/1000 | Loss: 0.00002527
Iteration 70/1000 | Loss: 0.00002526
Iteration 71/1000 | Loss: 0.00002526
Iteration 72/1000 | Loss: 0.00002526
Iteration 73/1000 | Loss: 0.00002525
Iteration 74/1000 | Loss: 0.00002525
Iteration 75/1000 | Loss: 0.00002525
Iteration 76/1000 | Loss: 0.00002525
Iteration 77/1000 | Loss: 0.00002525
Iteration 78/1000 | Loss: 0.00002525
Iteration 79/1000 | Loss: 0.00002525
Iteration 80/1000 | Loss: 0.00002525
Iteration 81/1000 | Loss: 0.00002525
Iteration 82/1000 | Loss: 0.00002524
Iteration 83/1000 | Loss: 0.00002524
Iteration 84/1000 | Loss: 0.00002524
Iteration 85/1000 | Loss: 0.00002524
Iteration 86/1000 | Loss: 0.00002524
Iteration 87/1000 | Loss: 0.00002524
Iteration 88/1000 | Loss: 0.00002524
Iteration 89/1000 | Loss: 0.00002524
Iteration 90/1000 | Loss: 0.00002524
Iteration 91/1000 | Loss: 0.00002524
Iteration 92/1000 | Loss: 0.00002523
Iteration 93/1000 | Loss: 0.00002523
Iteration 94/1000 | Loss: 0.00002523
Iteration 95/1000 | Loss: 0.00002523
Iteration 96/1000 | Loss: 0.00002523
Iteration 97/1000 | Loss: 0.00002523
Iteration 98/1000 | Loss: 0.00002523
Iteration 99/1000 | Loss: 0.00002523
Iteration 100/1000 | Loss: 0.00002522
Iteration 101/1000 | Loss: 0.00002522
Iteration 102/1000 | Loss: 0.00002522
Iteration 103/1000 | Loss: 0.00002522
Iteration 104/1000 | Loss: 0.00002522
Iteration 105/1000 | Loss: 0.00002522
Iteration 106/1000 | Loss: 0.00002522
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002521
Iteration 109/1000 | Loss: 0.00002521
Iteration 110/1000 | Loss: 0.00002521
Iteration 111/1000 | Loss: 0.00002521
Iteration 112/1000 | Loss: 0.00002521
Iteration 113/1000 | Loss: 0.00002521
Iteration 114/1000 | Loss: 0.00002521
Iteration 115/1000 | Loss: 0.00002520
Iteration 116/1000 | Loss: 0.00002520
Iteration 117/1000 | Loss: 0.00002520
Iteration 118/1000 | Loss: 0.00002520
Iteration 119/1000 | Loss: 0.00002520
Iteration 120/1000 | Loss: 0.00002520
Iteration 121/1000 | Loss: 0.00002520
Iteration 122/1000 | Loss: 0.00002520
Iteration 123/1000 | Loss: 0.00002520
Iteration 124/1000 | Loss: 0.00002519
Iteration 125/1000 | Loss: 0.00002519
Iteration 126/1000 | Loss: 0.00002519
Iteration 127/1000 | Loss: 0.00002519
Iteration 128/1000 | Loss: 0.00002519
Iteration 129/1000 | Loss: 0.00002519
Iteration 130/1000 | Loss: 0.00002519
Iteration 131/1000 | Loss: 0.00002519
Iteration 132/1000 | Loss: 0.00002519
Iteration 133/1000 | Loss: 0.00002519
Iteration 134/1000 | Loss: 0.00002519
Iteration 135/1000 | Loss: 0.00002519
Iteration 136/1000 | Loss: 0.00002519
Iteration 137/1000 | Loss: 0.00002519
Iteration 138/1000 | Loss: 0.00002519
Iteration 139/1000 | Loss: 0.00002519
Iteration 140/1000 | Loss: 0.00002519
Iteration 141/1000 | Loss: 0.00002519
Iteration 142/1000 | Loss: 0.00002519
Iteration 143/1000 | Loss: 0.00002519
Iteration 144/1000 | Loss: 0.00002519
Iteration 145/1000 | Loss: 0.00002519
Iteration 146/1000 | Loss: 0.00002519
Iteration 147/1000 | Loss: 0.00002519
Iteration 148/1000 | Loss: 0.00002519
Iteration 149/1000 | Loss: 0.00002519
Iteration 150/1000 | Loss: 0.00002519
Iteration 151/1000 | Loss: 0.00002519
Iteration 152/1000 | Loss: 0.00002519
Iteration 153/1000 | Loss: 0.00002519
Iteration 154/1000 | Loss: 0.00002519
Iteration 155/1000 | Loss: 0.00002519
Iteration 156/1000 | Loss: 0.00002519
Iteration 157/1000 | Loss: 0.00002519
Iteration 158/1000 | Loss: 0.00002519
Iteration 159/1000 | Loss: 0.00002519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.5185136109939776e-05, 2.5185136109939776e-05, 2.5185136109939776e-05, 2.5185136109939776e-05, 2.5185136109939776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5185136109939776e-05

Optimization complete. Final v2v error: 4.451511859893799 mm

Highest mean error: 4.987156867980957 mm for frame 84

Lowest mean error: 3.914339065551758 mm for frame 1

Saving results

Total time: 44.0754930973053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538455
Iteration 2/25 | Loss: 0.00164119
Iteration 3/25 | Loss: 0.00153849
Iteration 4/25 | Loss: 0.00151034
Iteration 5/25 | Loss: 0.00150132
Iteration 6/25 | Loss: 0.00150024
Iteration 7/25 | Loss: 0.00150024
Iteration 8/25 | Loss: 0.00150024
Iteration 9/25 | Loss: 0.00150024
Iteration 10/25 | Loss: 0.00150024
Iteration 11/25 | Loss: 0.00150024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015002402942627668, 0.0015002402942627668, 0.0015002402942627668, 0.0015002402942627668, 0.0015002402942627668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015002402942627668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42301655
Iteration 2/25 | Loss: 0.00158415
Iteration 3/25 | Loss: 0.00158415
Iteration 4/25 | Loss: 0.00158414
Iteration 5/25 | Loss: 0.00158414
Iteration 6/25 | Loss: 0.00158414
Iteration 7/25 | Loss: 0.00158414
Iteration 8/25 | Loss: 0.00158414
Iteration 9/25 | Loss: 0.00158414
Iteration 10/25 | Loss: 0.00158414
Iteration 11/25 | Loss: 0.00158414
Iteration 12/25 | Loss: 0.00158414
Iteration 13/25 | Loss: 0.00158414
Iteration 14/25 | Loss: 0.00158414
Iteration 15/25 | Loss: 0.00158414
Iteration 16/25 | Loss: 0.00158414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015841437270864844, 0.0015841437270864844, 0.0015841437270864844, 0.0015841437270864844, 0.0015841437270864844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015841437270864844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158414
Iteration 2/1000 | Loss: 0.00007828
Iteration 3/1000 | Loss: 0.00005601
Iteration 4/1000 | Loss: 0.00004416
Iteration 5/1000 | Loss: 0.00003795
Iteration 6/1000 | Loss: 0.00003532
Iteration 7/1000 | Loss: 0.00003330
Iteration 8/1000 | Loss: 0.00003177
Iteration 9/1000 | Loss: 0.00003099
Iteration 10/1000 | Loss: 0.00003037
Iteration 11/1000 | Loss: 0.00002991
Iteration 12/1000 | Loss: 0.00002946
Iteration 13/1000 | Loss: 0.00002909
Iteration 14/1000 | Loss: 0.00002884
Iteration 15/1000 | Loss: 0.00002863
Iteration 16/1000 | Loss: 0.00002861
Iteration 17/1000 | Loss: 0.00002849
Iteration 18/1000 | Loss: 0.00002848
Iteration 19/1000 | Loss: 0.00002845
Iteration 20/1000 | Loss: 0.00002845
Iteration 21/1000 | Loss: 0.00002844
Iteration 22/1000 | Loss: 0.00002844
Iteration 23/1000 | Loss: 0.00002844
Iteration 24/1000 | Loss: 0.00002843
Iteration 25/1000 | Loss: 0.00002841
Iteration 26/1000 | Loss: 0.00002841
Iteration 27/1000 | Loss: 0.00002840
Iteration 28/1000 | Loss: 0.00002840
Iteration 29/1000 | Loss: 0.00002840
Iteration 30/1000 | Loss: 0.00002839
Iteration 31/1000 | Loss: 0.00002839
Iteration 32/1000 | Loss: 0.00002839
Iteration 33/1000 | Loss: 0.00002839
Iteration 34/1000 | Loss: 0.00002838
Iteration 35/1000 | Loss: 0.00002838
Iteration 36/1000 | Loss: 0.00002837
Iteration 37/1000 | Loss: 0.00002836
Iteration 38/1000 | Loss: 0.00002835
Iteration 39/1000 | Loss: 0.00002835
Iteration 40/1000 | Loss: 0.00002834
Iteration 41/1000 | Loss: 0.00002834
Iteration 42/1000 | Loss: 0.00002833
Iteration 43/1000 | Loss: 0.00002833
Iteration 44/1000 | Loss: 0.00002833
Iteration 45/1000 | Loss: 0.00002833
Iteration 46/1000 | Loss: 0.00002833
Iteration 47/1000 | Loss: 0.00002832
Iteration 48/1000 | Loss: 0.00002832
Iteration 49/1000 | Loss: 0.00002832
Iteration 50/1000 | Loss: 0.00002832
Iteration 51/1000 | Loss: 0.00002832
Iteration 52/1000 | Loss: 0.00002832
Iteration 53/1000 | Loss: 0.00002832
Iteration 54/1000 | Loss: 0.00002831
Iteration 55/1000 | Loss: 0.00002831
Iteration 56/1000 | Loss: 0.00002831
Iteration 57/1000 | Loss: 0.00002831
Iteration 58/1000 | Loss: 0.00002831
Iteration 59/1000 | Loss: 0.00002831
Iteration 60/1000 | Loss: 0.00002831
Iteration 61/1000 | Loss: 0.00002830
Iteration 62/1000 | Loss: 0.00002830
Iteration 63/1000 | Loss: 0.00002830
Iteration 64/1000 | Loss: 0.00002829
Iteration 65/1000 | Loss: 0.00002829
Iteration 66/1000 | Loss: 0.00002828
Iteration 67/1000 | Loss: 0.00002828
Iteration 68/1000 | Loss: 0.00002828
Iteration 69/1000 | Loss: 0.00002827
Iteration 70/1000 | Loss: 0.00002827
Iteration 71/1000 | Loss: 0.00002827
Iteration 72/1000 | Loss: 0.00002827
Iteration 73/1000 | Loss: 0.00002827
Iteration 74/1000 | Loss: 0.00002827
Iteration 75/1000 | Loss: 0.00002827
Iteration 76/1000 | Loss: 0.00002827
Iteration 77/1000 | Loss: 0.00002826
Iteration 78/1000 | Loss: 0.00002826
Iteration 79/1000 | Loss: 0.00002826
Iteration 80/1000 | Loss: 0.00002826
Iteration 81/1000 | Loss: 0.00002825
Iteration 82/1000 | Loss: 0.00002825
Iteration 83/1000 | Loss: 0.00002825
Iteration 84/1000 | Loss: 0.00002825
Iteration 85/1000 | Loss: 0.00002824
Iteration 86/1000 | Loss: 0.00002824
Iteration 87/1000 | Loss: 0.00002824
Iteration 88/1000 | Loss: 0.00002824
Iteration 89/1000 | Loss: 0.00002824
Iteration 90/1000 | Loss: 0.00002823
Iteration 91/1000 | Loss: 0.00002823
Iteration 92/1000 | Loss: 0.00002823
Iteration 93/1000 | Loss: 0.00002823
Iteration 94/1000 | Loss: 0.00002823
Iteration 95/1000 | Loss: 0.00002823
Iteration 96/1000 | Loss: 0.00002823
Iteration 97/1000 | Loss: 0.00002823
Iteration 98/1000 | Loss: 0.00002823
Iteration 99/1000 | Loss: 0.00002823
Iteration 100/1000 | Loss: 0.00002822
Iteration 101/1000 | Loss: 0.00002822
Iteration 102/1000 | Loss: 0.00002822
Iteration 103/1000 | Loss: 0.00002821
Iteration 104/1000 | Loss: 0.00002821
Iteration 105/1000 | Loss: 0.00002821
Iteration 106/1000 | Loss: 0.00002820
Iteration 107/1000 | Loss: 0.00002820
Iteration 108/1000 | Loss: 0.00002820
Iteration 109/1000 | Loss: 0.00002820
Iteration 110/1000 | Loss: 0.00002820
Iteration 111/1000 | Loss: 0.00002820
Iteration 112/1000 | Loss: 0.00002819
Iteration 113/1000 | Loss: 0.00002819
Iteration 114/1000 | Loss: 0.00002819
Iteration 115/1000 | Loss: 0.00002819
Iteration 116/1000 | Loss: 0.00002818
Iteration 117/1000 | Loss: 0.00002818
Iteration 118/1000 | Loss: 0.00002818
Iteration 119/1000 | Loss: 0.00002817
Iteration 120/1000 | Loss: 0.00002817
Iteration 121/1000 | Loss: 0.00002817
Iteration 122/1000 | Loss: 0.00002817
Iteration 123/1000 | Loss: 0.00002817
Iteration 124/1000 | Loss: 0.00002817
Iteration 125/1000 | Loss: 0.00002817
Iteration 126/1000 | Loss: 0.00002817
Iteration 127/1000 | Loss: 0.00002817
Iteration 128/1000 | Loss: 0.00002817
Iteration 129/1000 | Loss: 0.00002817
Iteration 130/1000 | Loss: 0.00002816
Iteration 131/1000 | Loss: 0.00002816
Iteration 132/1000 | Loss: 0.00002816
Iteration 133/1000 | Loss: 0.00002816
Iteration 134/1000 | Loss: 0.00002816
Iteration 135/1000 | Loss: 0.00002816
Iteration 136/1000 | Loss: 0.00002816
Iteration 137/1000 | Loss: 0.00002816
Iteration 138/1000 | Loss: 0.00002816
Iteration 139/1000 | Loss: 0.00002816
Iteration 140/1000 | Loss: 0.00002816
Iteration 141/1000 | Loss: 0.00002816
Iteration 142/1000 | Loss: 0.00002816
Iteration 143/1000 | Loss: 0.00002816
Iteration 144/1000 | Loss: 0.00002815
Iteration 145/1000 | Loss: 0.00002815
Iteration 146/1000 | Loss: 0.00002815
Iteration 147/1000 | Loss: 0.00002815
Iteration 148/1000 | Loss: 0.00002814
Iteration 149/1000 | Loss: 0.00002814
Iteration 150/1000 | Loss: 0.00002814
Iteration 151/1000 | Loss: 0.00002814
Iteration 152/1000 | Loss: 0.00002814
Iteration 153/1000 | Loss: 0.00002814
Iteration 154/1000 | Loss: 0.00002814
Iteration 155/1000 | Loss: 0.00002814
Iteration 156/1000 | Loss: 0.00002814
Iteration 157/1000 | Loss: 0.00002813
Iteration 158/1000 | Loss: 0.00002813
Iteration 159/1000 | Loss: 0.00002813
Iteration 160/1000 | Loss: 0.00002813
Iteration 161/1000 | Loss: 0.00002813
Iteration 162/1000 | Loss: 0.00002813
Iteration 163/1000 | Loss: 0.00002813
Iteration 164/1000 | Loss: 0.00002813
Iteration 165/1000 | Loss: 0.00002813
Iteration 166/1000 | Loss: 0.00002813
Iteration 167/1000 | Loss: 0.00002813
Iteration 168/1000 | Loss: 0.00002813
Iteration 169/1000 | Loss: 0.00002813
Iteration 170/1000 | Loss: 0.00002812
Iteration 171/1000 | Loss: 0.00002812
Iteration 172/1000 | Loss: 0.00002812
Iteration 173/1000 | Loss: 0.00002812
Iteration 174/1000 | Loss: 0.00002812
Iteration 175/1000 | Loss: 0.00002812
Iteration 176/1000 | Loss: 0.00002812
Iteration 177/1000 | Loss: 0.00002812
Iteration 178/1000 | Loss: 0.00002811
Iteration 179/1000 | Loss: 0.00002811
Iteration 180/1000 | Loss: 0.00002811
Iteration 181/1000 | Loss: 0.00002811
Iteration 182/1000 | Loss: 0.00002811
Iteration 183/1000 | Loss: 0.00002811
Iteration 184/1000 | Loss: 0.00002811
Iteration 185/1000 | Loss: 0.00002810
Iteration 186/1000 | Loss: 0.00002810
Iteration 187/1000 | Loss: 0.00002810
Iteration 188/1000 | Loss: 0.00002810
Iteration 189/1000 | Loss: 0.00002810
Iteration 190/1000 | Loss: 0.00002810
Iteration 191/1000 | Loss: 0.00002810
Iteration 192/1000 | Loss: 0.00002810
Iteration 193/1000 | Loss: 0.00002810
Iteration 194/1000 | Loss: 0.00002810
Iteration 195/1000 | Loss: 0.00002810
Iteration 196/1000 | Loss: 0.00002810
Iteration 197/1000 | Loss: 0.00002810
Iteration 198/1000 | Loss: 0.00002810
Iteration 199/1000 | Loss: 0.00002809
Iteration 200/1000 | Loss: 0.00002809
Iteration 201/1000 | Loss: 0.00002809
Iteration 202/1000 | Loss: 0.00002809
Iteration 203/1000 | Loss: 0.00002809
Iteration 204/1000 | Loss: 0.00002809
Iteration 205/1000 | Loss: 0.00002809
Iteration 206/1000 | Loss: 0.00002809
Iteration 207/1000 | Loss: 0.00002809
Iteration 208/1000 | Loss: 0.00002809
Iteration 209/1000 | Loss: 0.00002809
Iteration 210/1000 | Loss: 0.00002808
Iteration 211/1000 | Loss: 0.00002808
Iteration 212/1000 | Loss: 0.00002808
Iteration 213/1000 | Loss: 0.00002808
Iteration 214/1000 | Loss: 0.00002808
Iteration 215/1000 | Loss: 0.00002808
Iteration 216/1000 | Loss: 0.00002808
Iteration 217/1000 | Loss: 0.00002808
Iteration 218/1000 | Loss: 0.00002808
Iteration 219/1000 | Loss: 0.00002808
Iteration 220/1000 | Loss: 0.00002808
Iteration 221/1000 | Loss: 0.00002808
Iteration 222/1000 | Loss: 0.00002807
Iteration 223/1000 | Loss: 0.00002807
Iteration 224/1000 | Loss: 0.00002807
Iteration 225/1000 | Loss: 0.00002807
Iteration 226/1000 | Loss: 0.00002807
Iteration 227/1000 | Loss: 0.00002807
Iteration 228/1000 | Loss: 0.00002807
Iteration 229/1000 | Loss: 0.00002807
Iteration 230/1000 | Loss: 0.00002807
Iteration 231/1000 | Loss: 0.00002807
Iteration 232/1000 | Loss: 0.00002807
Iteration 233/1000 | Loss: 0.00002807
Iteration 234/1000 | Loss: 0.00002807
Iteration 235/1000 | Loss: 0.00002807
Iteration 236/1000 | Loss: 0.00002807
Iteration 237/1000 | Loss: 0.00002807
Iteration 238/1000 | Loss: 0.00002807
Iteration 239/1000 | Loss: 0.00002807
Iteration 240/1000 | Loss: 0.00002807
Iteration 241/1000 | Loss: 0.00002807
Iteration 242/1000 | Loss: 0.00002807
Iteration 243/1000 | Loss: 0.00002807
Iteration 244/1000 | Loss: 0.00002807
Iteration 245/1000 | Loss: 0.00002807
Iteration 246/1000 | Loss: 0.00002807
Iteration 247/1000 | Loss: 0.00002807
Iteration 248/1000 | Loss: 0.00002807
Iteration 249/1000 | Loss: 0.00002807
Iteration 250/1000 | Loss: 0.00002807
Iteration 251/1000 | Loss: 0.00002807
Iteration 252/1000 | Loss: 0.00002807
Iteration 253/1000 | Loss: 0.00002807
Iteration 254/1000 | Loss: 0.00002807
Iteration 255/1000 | Loss: 0.00002807
Iteration 256/1000 | Loss: 0.00002807
Iteration 257/1000 | Loss: 0.00002807
Iteration 258/1000 | Loss: 0.00002807
Iteration 259/1000 | Loss: 0.00002807
Iteration 260/1000 | Loss: 0.00002807
Iteration 261/1000 | Loss: 0.00002807
Iteration 262/1000 | Loss: 0.00002807
Iteration 263/1000 | Loss: 0.00002807
Iteration 264/1000 | Loss: 0.00002807
Iteration 265/1000 | Loss: 0.00002807
Iteration 266/1000 | Loss: 0.00002807
Iteration 267/1000 | Loss: 0.00002807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.807263081194833e-05, 2.807263081194833e-05, 2.807263081194833e-05, 2.807263081194833e-05, 2.807263081194833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.807263081194833e-05

Optimization complete. Final v2v error: 4.581298351287842 mm

Highest mean error: 5.176883697509766 mm for frame 180

Lowest mean error: 4.3084259033203125 mm for frame 15

Saving results

Total time: 48.475173234939575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00665164
Iteration 2/25 | Loss: 0.00147077
Iteration 3/25 | Loss: 0.00137343
Iteration 4/25 | Loss: 0.00135599
Iteration 5/25 | Loss: 0.00134709
Iteration 6/25 | Loss: 0.00134489
Iteration 7/25 | Loss: 0.00134479
Iteration 8/25 | Loss: 0.00134479
Iteration 9/25 | Loss: 0.00134479
Iteration 10/25 | Loss: 0.00134479
Iteration 11/25 | Loss: 0.00134473
Iteration 12/25 | Loss: 0.00134473
Iteration 13/25 | Loss: 0.00134473
Iteration 14/25 | Loss: 0.00134473
Iteration 15/25 | Loss: 0.00134473
Iteration 16/25 | Loss: 0.00134473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001344725489616394, 0.001344725489616394, 0.001344725489616394, 0.001344725489616394, 0.001344725489616394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001344725489616394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92293453
Iteration 2/25 | Loss: 0.00100573
Iteration 3/25 | Loss: 0.00100573
Iteration 4/25 | Loss: 0.00100573
Iteration 5/25 | Loss: 0.00100573
Iteration 6/25 | Loss: 0.00100573
Iteration 7/25 | Loss: 0.00100573
Iteration 8/25 | Loss: 0.00100573
Iteration 9/25 | Loss: 0.00100573
Iteration 10/25 | Loss: 0.00100573
Iteration 11/25 | Loss: 0.00100573
Iteration 12/25 | Loss: 0.00100573
Iteration 13/25 | Loss: 0.00100573
Iteration 14/25 | Loss: 0.00100573
Iteration 15/25 | Loss: 0.00100573
Iteration 16/25 | Loss: 0.00100573
Iteration 17/25 | Loss: 0.00100573
Iteration 18/25 | Loss: 0.00100573
Iteration 19/25 | Loss: 0.00100573
Iteration 20/25 | Loss: 0.00100573
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010057282634079456, 0.0010057282634079456, 0.0010057282634079456, 0.0010057282634079456, 0.0010057282634079456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010057282634079456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100573
Iteration 2/1000 | Loss: 0.00004425
Iteration 3/1000 | Loss: 0.00003549
Iteration 4/1000 | Loss: 0.00003164
Iteration 5/1000 | Loss: 0.00002995
Iteration 6/1000 | Loss: 0.00002916
Iteration 7/1000 | Loss: 0.00002875
Iteration 8/1000 | Loss: 0.00002851
Iteration 9/1000 | Loss: 0.00002819
Iteration 10/1000 | Loss: 0.00002802
Iteration 11/1000 | Loss: 0.00002801
Iteration 12/1000 | Loss: 0.00002801
Iteration 13/1000 | Loss: 0.00002801
Iteration 14/1000 | Loss: 0.00002796
Iteration 15/1000 | Loss: 0.00002795
Iteration 16/1000 | Loss: 0.00002795
Iteration 17/1000 | Loss: 0.00002795
Iteration 18/1000 | Loss: 0.00002793
Iteration 19/1000 | Loss: 0.00002792
Iteration 20/1000 | Loss: 0.00002791
Iteration 21/1000 | Loss: 0.00002791
Iteration 22/1000 | Loss: 0.00002789
Iteration 23/1000 | Loss: 0.00002789
Iteration 24/1000 | Loss: 0.00002788
Iteration 25/1000 | Loss: 0.00002788
Iteration 26/1000 | Loss: 0.00002787
Iteration 27/1000 | Loss: 0.00002787
Iteration 28/1000 | Loss: 0.00002787
Iteration 29/1000 | Loss: 0.00002787
Iteration 30/1000 | Loss: 0.00002785
Iteration 31/1000 | Loss: 0.00002785
Iteration 32/1000 | Loss: 0.00002785
Iteration 33/1000 | Loss: 0.00002785
Iteration 34/1000 | Loss: 0.00002785
Iteration 35/1000 | Loss: 0.00002785
Iteration 36/1000 | Loss: 0.00002785
Iteration 37/1000 | Loss: 0.00002785
Iteration 38/1000 | Loss: 0.00002785
Iteration 39/1000 | Loss: 0.00002784
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002784
Iteration 42/1000 | Loss: 0.00002781
Iteration 43/1000 | Loss: 0.00002781
Iteration 44/1000 | Loss: 0.00002781
Iteration 45/1000 | Loss: 0.00002781
Iteration 46/1000 | Loss: 0.00002781
Iteration 47/1000 | Loss: 0.00002780
Iteration 48/1000 | Loss: 0.00002780
Iteration 49/1000 | Loss: 0.00002779
Iteration 50/1000 | Loss: 0.00002779
Iteration 51/1000 | Loss: 0.00002778
Iteration 52/1000 | Loss: 0.00002777
Iteration 53/1000 | Loss: 0.00002777
Iteration 54/1000 | Loss: 0.00002777
Iteration 55/1000 | Loss: 0.00002776
Iteration 56/1000 | Loss: 0.00002776
Iteration 57/1000 | Loss: 0.00002776
Iteration 58/1000 | Loss: 0.00002776
Iteration 59/1000 | Loss: 0.00002776
Iteration 60/1000 | Loss: 0.00002775
Iteration 61/1000 | Loss: 0.00002775
Iteration 62/1000 | Loss: 0.00002775
Iteration 63/1000 | Loss: 0.00002775
Iteration 64/1000 | Loss: 0.00002775
Iteration 65/1000 | Loss: 0.00002775
Iteration 66/1000 | Loss: 0.00002775
Iteration 67/1000 | Loss: 0.00002775
Iteration 68/1000 | Loss: 0.00002775
Iteration 69/1000 | Loss: 0.00002775
Iteration 70/1000 | Loss: 0.00002775
Iteration 71/1000 | Loss: 0.00002775
Iteration 72/1000 | Loss: 0.00002775
Iteration 73/1000 | Loss: 0.00002774
Iteration 74/1000 | Loss: 0.00002773
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00002773
Iteration 77/1000 | Loss: 0.00002773
Iteration 78/1000 | Loss: 0.00002772
Iteration 79/1000 | Loss: 0.00002772
Iteration 80/1000 | Loss: 0.00002772
Iteration 81/1000 | Loss: 0.00002772
Iteration 82/1000 | Loss: 0.00002772
Iteration 83/1000 | Loss: 0.00002772
Iteration 84/1000 | Loss: 0.00002772
Iteration 85/1000 | Loss: 0.00002772
Iteration 86/1000 | Loss: 0.00002772
Iteration 87/1000 | Loss: 0.00002772
Iteration 88/1000 | Loss: 0.00002771
Iteration 89/1000 | Loss: 0.00002771
Iteration 90/1000 | Loss: 0.00002771
Iteration 91/1000 | Loss: 0.00002771
Iteration 92/1000 | Loss: 0.00002771
Iteration 93/1000 | Loss: 0.00002770
Iteration 94/1000 | Loss: 0.00002770
Iteration 95/1000 | Loss: 0.00002770
Iteration 96/1000 | Loss: 0.00002770
Iteration 97/1000 | Loss: 0.00002770
Iteration 98/1000 | Loss: 0.00002770
Iteration 99/1000 | Loss: 0.00002770
Iteration 100/1000 | Loss: 0.00002770
Iteration 101/1000 | Loss: 0.00002769
Iteration 102/1000 | Loss: 0.00002769
Iteration 103/1000 | Loss: 0.00002769
Iteration 104/1000 | Loss: 0.00002769
Iteration 105/1000 | Loss: 0.00002769
Iteration 106/1000 | Loss: 0.00002769
Iteration 107/1000 | Loss: 0.00002769
Iteration 108/1000 | Loss: 0.00002769
Iteration 109/1000 | Loss: 0.00002769
Iteration 110/1000 | Loss: 0.00002769
Iteration 111/1000 | Loss: 0.00002769
Iteration 112/1000 | Loss: 0.00002768
Iteration 113/1000 | Loss: 0.00002768
Iteration 114/1000 | Loss: 0.00002768
Iteration 115/1000 | Loss: 0.00002768
Iteration 116/1000 | Loss: 0.00002768
Iteration 117/1000 | Loss: 0.00002768
Iteration 118/1000 | Loss: 0.00002768
Iteration 119/1000 | Loss: 0.00002768
Iteration 120/1000 | Loss: 0.00002768
Iteration 121/1000 | Loss: 0.00002768
Iteration 122/1000 | Loss: 0.00002768
Iteration 123/1000 | Loss: 0.00002768
Iteration 124/1000 | Loss: 0.00002768
Iteration 125/1000 | Loss: 0.00002768
Iteration 126/1000 | Loss: 0.00002768
Iteration 127/1000 | Loss: 0.00002768
Iteration 128/1000 | Loss: 0.00002768
Iteration 129/1000 | Loss: 0.00002768
Iteration 130/1000 | Loss: 0.00002768
Iteration 131/1000 | Loss: 0.00002768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.7680094717652537e-05, 2.7680094717652537e-05, 2.7680094717652537e-05, 2.7680094717652537e-05, 2.7680094717652537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7680094717652537e-05

Optimization complete. Final v2v error: 4.672120094299316 mm

Highest mean error: 4.916564464569092 mm for frame 138

Lowest mean error: 4.294905662536621 mm for frame 67

Saving results

Total time: 34.392685890197754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502708
Iteration 2/25 | Loss: 0.00170241
Iteration 3/25 | Loss: 0.00148347
Iteration 4/25 | Loss: 0.00144952
Iteration 5/25 | Loss: 0.00144153
Iteration 6/25 | Loss: 0.00143975
Iteration 7/25 | Loss: 0.00143943
Iteration 8/25 | Loss: 0.00143943
Iteration 9/25 | Loss: 0.00143943
Iteration 10/25 | Loss: 0.00143943
Iteration 11/25 | Loss: 0.00143943
Iteration 12/25 | Loss: 0.00143943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014394253958016634, 0.0014394253958016634, 0.0014394253958016634, 0.0014394253958016634, 0.0014394253958016634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014394253958016634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19051754
Iteration 2/25 | Loss: 0.00096552
Iteration 3/25 | Loss: 0.00096551
Iteration 4/25 | Loss: 0.00096551
Iteration 5/25 | Loss: 0.00096551
Iteration 6/25 | Loss: 0.00096551
Iteration 7/25 | Loss: 0.00096550
Iteration 8/25 | Loss: 0.00096550
Iteration 9/25 | Loss: 0.00096550
Iteration 10/25 | Loss: 0.00096550
Iteration 11/25 | Loss: 0.00096550
Iteration 12/25 | Loss: 0.00096550
Iteration 13/25 | Loss: 0.00096550
Iteration 14/25 | Loss: 0.00096550
Iteration 15/25 | Loss: 0.00096550
Iteration 16/25 | Loss: 0.00096550
Iteration 17/25 | Loss: 0.00096550
Iteration 18/25 | Loss: 0.00096550
Iteration 19/25 | Loss: 0.00096550
Iteration 20/25 | Loss: 0.00096550
Iteration 21/25 | Loss: 0.00096550
Iteration 22/25 | Loss: 0.00096550
Iteration 23/25 | Loss: 0.00096550
Iteration 24/25 | Loss: 0.00096550
Iteration 25/25 | Loss: 0.00096550

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096550
Iteration 2/1000 | Loss: 0.00007907
Iteration 3/1000 | Loss: 0.00006021
Iteration 4/1000 | Loss: 0.00005409
Iteration 5/1000 | Loss: 0.00004948
Iteration 6/1000 | Loss: 0.00004748
Iteration 7/1000 | Loss: 0.00004596
Iteration 8/1000 | Loss: 0.00004463
Iteration 9/1000 | Loss: 0.00004364
Iteration 10/1000 | Loss: 0.00004307
Iteration 11/1000 | Loss: 0.00004268
Iteration 12/1000 | Loss: 0.00004232
Iteration 13/1000 | Loss: 0.00004198
Iteration 14/1000 | Loss: 0.00004174
Iteration 15/1000 | Loss: 0.00004155
Iteration 16/1000 | Loss: 0.00004153
Iteration 17/1000 | Loss: 0.00004152
Iteration 18/1000 | Loss: 0.00004151
Iteration 19/1000 | Loss: 0.00004151
Iteration 20/1000 | Loss: 0.00004150
Iteration 21/1000 | Loss: 0.00004150
Iteration 22/1000 | Loss: 0.00004147
Iteration 23/1000 | Loss: 0.00004146
Iteration 24/1000 | Loss: 0.00004145
Iteration 25/1000 | Loss: 0.00004144
Iteration 26/1000 | Loss: 0.00004144
Iteration 27/1000 | Loss: 0.00004144
Iteration 28/1000 | Loss: 0.00004143
Iteration 29/1000 | Loss: 0.00004143
Iteration 30/1000 | Loss: 0.00004143
Iteration 31/1000 | Loss: 0.00004136
Iteration 32/1000 | Loss: 0.00004136
Iteration 33/1000 | Loss: 0.00004136
Iteration 34/1000 | Loss: 0.00004136
Iteration 35/1000 | Loss: 0.00004136
Iteration 36/1000 | Loss: 0.00004135
Iteration 37/1000 | Loss: 0.00004135
Iteration 38/1000 | Loss: 0.00004135
Iteration 39/1000 | Loss: 0.00004134
Iteration 40/1000 | Loss: 0.00004133
Iteration 41/1000 | Loss: 0.00004133
Iteration 42/1000 | Loss: 0.00004133
Iteration 43/1000 | Loss: 0.00004133
Iteration 44/1000 | Loss: 0.00004132
Iteration 45/1000 | Loss: 0.00004132
Iteration 46/1000 | Loss: 0.00004132
Iteration 47/1000 | Loss: 0.00004132
Iteration 48/1000 | Loss: 0.00004132
Iteration 49/1000 | Loss: 0.00004132
Iteration 50/1000 | Loss: 0.00004132
Iteration 51/1000 | Loss: 0.00004132
Iteration 52/1000 | Loss: 0.00004132
Iteration 53/1000 | Loss: 0.00004132
Iteration 54/1000 | Loss: 0.00004132
Iteration 55/1000 | Loss: 0.00004132
Iteration 56/1000 | Loss: 0.00004131
Iteration 57/1000 | Loss: 0.00004131
Iteration 58/1000 | Loss: 0.00004131
Iteration 59/1000 | Loss: 0.00004131
Iteration 60/1000 | Loss: 0.00004131
Iteration 61/1000 | Loss: 0.00004131
Iteration 62/1000 | Loss: 0.00004130
Iteration 63/1000 | Loss: 0.00004130
Iteration 64/1000 | Loss: 0.00004128
Iteration 65/1000 | Loss: 0.00004128
Iteration 66/1000 | Loss: 0.00004128
Iteration 67/1000 | Loss: 0.00004128
Iteration 68/1000 | Loss: 0.00004128
Iteration 69/1000 | Loss: 0.00004128
Iteration 70/1000 | Loss: 0.00004128
Iteration 71/1000 | Loss: 0.00004128
Iteration 72/1000 | Loss: 0.00004128
Iteration 73/1000 | Loss: 0.00004128
Iteration 74/1000 | Loss: 0.00004128
Iteration 75/1000 | Loss: 0.00004127
Iteration 76/1000 | Loss: 0.00004127
Iteration 77/1000 | Loss: 0.00004127
Iteration 78/1000 | Loss: 0.00004127
Iteration 79/1000 | Loss: 0.00004126
Iteration 80/1000 | Loss: 0.00004125
Iteration 81/1000 | Loss: 0.00004125
Iteration 82/1000 | Loss: 0.00004125
Iteration 83/1000 | Loss: 0.00004125
Iteration 84/1000 | Loss: 0.00004125
Iteration 85/1000 | Loss: 0.00004125
Iteration 86/1000 | Loss: 0.00004125
Iteration 87/1000 | Loss: 0.00004125
Iteration 88/1000 | Loss: 0.00004125
Iteration 89/1000 | Loss: 0.00004124
Iteration 90/1000 | Loss: 0.00004124
Iteration 91/1000 | Loss: 0.00004123
Iteration 92/1000 | Loss: 0.00004123
Iteration 93/1000 | Loss: 0.00004123
Iteration 94/1000 | Loss: 0.00004122
Iteration 95/1000 | Loss: 0.00004122
Iteration 96/1000 | Loss: 0.00004122
Iteration 97/1000 | Loss: 0.00004122
Iteration 98/1000 | Loss: 0.00004122
Iteration 99/1000 | Loss: 0.00004122
Iteration 100/1000 | Loss: 0.00004122
Iteration 101/1000 | Loss: 0.00004122
Iteration 102/1000 | Loss: 0.00004121
Iteration 103/1000 | Loss: 0.00004121
Iteration 104/1000 | Loss: 0.00004121
Iteration 105/1000 | Loss: 0.00004121
Iteration 106/1000 | Loss: 0.00004121
Iteration 107/1000 | Loss: 0.00004121
Iteration 108/1000 | Loss: 0.00004120
Iteration 109/1000 | Loss: 0.00004120
Iteration 110/1000 | Loss: 0.00004120
Iteration 111/1000 | Loss: 0.00004120
Iteration 112/1000 | Loss: 0.00004119
Iteration 113/1000 | Loss: 0.00004119
Iteration 114/1000 | Loss: 0.00004119
Iteration 115/1000 | Loss: 0.00004119
Iteration 116/1000 | Loss: 0.00004119
Iteration 117/1000 | Loss: 0.00004119
Iteration 118/1000 | Loss: 0.00004119
Iteration 119/1000 | Loss: 0.00004119
Iteration 120/1000 | Loss: 0.00004119
Iteration 121/1000 | Loss: 0.00004119
Iteration 122/1000 | Loss: 0.00004119
Iteration 123/1000 | Loss: 0.00004119
Iteration 124/1000 | Loss: 0.00004119
Iteration 125/1000 | Loss: 0.00004119
Iteration 126/1000 | Loss: 0.00004119
Iteration 127/1000 | Loss: 0.00004119
Iteration 128/1000 | Loss: 0.00004119
Iteration 129/1000 | Loss: 0.00004119
Iteration 130/1000 | Loss: 0.00004119
Iteration 131/1000 | Loss: 0.00004119
Iteration 132/1000 | Loss: 0.00004119
Iteration 133/1000 | Loss: 0.00004119
Iteration 134/1000 | Loss: 0.00004119
Iteration 135/1000 | Loss: 0.00004119
Iteration 136/1000 | Loss: 0.00004119
Iteration 137/1000 | Loss: 0.00004119
Iteration 138/1000 | Loss: 0.00004119
Iteration 139/1000 | Loss: 0.00004119
Iteration 140/1000 | Loss: 0.00004119
Iteration 141/1000 | Loss: 0.00004119
Iteration 142/1000 | Loss: 0.00004119
Iteration 143/1000 | Loss: 0.00004119
Iteration 144/1000 | Loss: 0.00004119
Iteration 145/1000 | Loss: 0.00004119
Iteration 146/1000 | Loss: 0.00004119
Iteration 147/1000 | Loss: 0.00004119
Iteration 148/1000 | Loss: 0.00004119
Iteration 149/1000 | Loss: 0.00004119
Iteration 150/1000 | Loss: 0.00004119
Iteration 151/1000 | Loss: 0.00004119
Iteration 152/1000 | Loss: 0.00004119
Iteration 153/1000 | Loss: 0.00004119
Iteration 154/1000 | Loss: 0.00004119
Iteration 155/1000 | Loss: 0.00004119
Iteration 156/1000 | Loss: 0.00004119
Iteration 157/1000 | Loss: 0.00004119
Iteration 158/1000 | Loss: 0.00004119
Iteration 159/1000 | Loss: 0.00004119
Iteration 160/1000 | Loss: 0.00004119
Iteration 161/1000 | Loss: 0.00004119
Iteration 162/1000 | Loss: 0.00004119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [4.118527067475952e-05, 4.118527067475952e-05, 4.118527067475952e-05, 4.118527067475952e-05, 4.118527067475952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.118527067475952e-05

Optimization complete. Final v2v error: 5.327286720275879 mm

Highest mean error: 7.148948669433594 mm for frame 81

Lowest mean error: 4.368954181671143 mm for frame 49

Saving results

Total time: 41.1424286365509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395524
Iteration 2/25 | Loss: 0.00171341
Iteration 3/25 | Loss: 0.00141908
Iteration 4/25 | Loss: 0.00135782
Iteration 5/25 | Loss: 0.00134593
Iteration 6/25 | Loss: 0.00134412
Iteration 7/25 | Loss: 0.00134323
Iteration 8/25 | Loss: 0.00134323
Iteration 9/25 | Loss: 0.00134323
Iteration 10/25 | Loss: 0.00134323
Iteration 11/25 | Loss: 0.00134323
Iteration 12/25 | Loss: 0.00134323
Iteration 13/25 | Loss: 0.00134323
Iteration 14/25 | Loss: 0.00134323
Iteration 15/25 | Loss: 0.00134323
Iteration 16/25 | Loss: 0.00134323
Iteration 17/25 | Loss: 0.00134323
Iteration 18/25 | Loss: 0.00134323
Iteration 19/25 | Loss: 0.00134323
Iteration 20/25 | Loss: 0.00134323
Iteration 21/25 | Loss: 0.00134323
Iteration 22/25 | Loss: 0.00134323
Iteration 23/25 | Loss: 0.00134323
Iteration 24/25 | Loss: 0.00134323
Iteration 25/25 | Loss: 0.00134323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44066072
Iteration 2/25 | Loss: 0.00097688
Iteration 3/25 | Loss: 0.00097688
Iteration 4/25 | Loss: 0.00097688
Iteration 5/25 | Loss: 0.00097688
Iteration 6/25 | Loss: 0.00097688
Iteration 7/25 | Loss: 0.00097688
Iteration 8/25 | Loss: 0.00097688
Iteration 9/25 | Loss: 0.00097688
Iteration 10/25 | Loss: 0.00097688
Iteration 11/25 | Loss: 0.00097688
Iteration 12/25 | Loss: 0.00097688
Iteration 13/25 | Loss: 0.00097688
Iteration 14/25 | Loss: 0.00097688
Iteration 15/25 | Loss: 0.00097688
Iteration 16/25 | Loss: 0.00097688
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000976879266090691, 0.000976879266090691, 0.000976879266090691, 0.000976879266090691, 0.000976879266090691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000976879266090691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097688
Iteration 2/1000 | Loss: 0.00005409
Iteration 3/1000 | Loss: 0.00004125
Iteration 4/1000 | Loss: 0.00003505
Iteration 5/1000 | Loss: 0.00003110
Iteration 6/1000 | Loss: 0.00002874
Iteration 7/1000 | Loss: 0.00002759
Iteration 8/1000 | Loss: 0.00002686
Iteration 9/1000 | Loss: 0.00002642
Iteration 10/1000 | Loss: 0.00002613
Iteration 11/1000 | Loss: 0.00002593
Iteration 12/1000 | Loss: 0.00002571
Iteration 13/1000 | Loss: 0.00002563
Iteration 14/1000 | Loss: 0.00002558
Iteration 15/1000 | Loss: 0.00002554
Iteration 16/1000 | Loss: 0.00002550
Iteration 17/1000 | Loss: 0.00002549
Iteration 18/1000 | Loss: 0.00002548
Iteration 19/1000 | Loss: 0.00002547
Iteration 20/1000 | Loss: 0.00002543
Iteration 21/1000 | Loss: 0.00002539
Iteration 22/1000 | Loss: 0.00002537
Iteration 23/1000 | Loss: 0.00002533
Iteration 24/1000 | Loss: 0.00002532
Iteration 25/1000 | Loss: 0.00002532
Iteration 26/1000 | Loss: 0.00002530
Iteration 27/1000 | Loss: 0.00002530
Iteration 28/1000 | Loss: 0.00002529
Iteration 29/1000 | Loss: 0.00002529
Iteration 30/1000 | Loss: 0.00002529
Iteration 31/1000 | Loss: 0.00002529
Iteration 32/1000 | Loss: 0.00002529
Iteration 33/1000 | Loss: 0.00002528
Iteration 34/1000 | Loss: 0.00002528
Iteration 35/1000 | Loss: 0.00002528
Iteration 36/1000 | Loss: 0.00002528
Iteration 37/1000 | Loss: 0.00002528
Iteration 38/1000 | Loss: 0.00002527
Iteration 39/1000 | Loss: 0.00002527
Iteration 40/1000 | Loss: 0.00002527
Iteration 41/1000 | Loss: 0.00002526
Iteration 42/1000 | Loss: 0.00002526
Iteration 43/1000 | Loss: 0.00002526
Iteration 44/1000 | Loss: 0.00002525
Iteration 45/1000 | Loss: 0.00002525
Iteration 46/1000 | Loss: 0.00002525
Iteration 47/1000 | Loss: 0.00002525
Iteration 48/1000 | Loss: 0.00002524
Iteration 49/1000 | Loss: 0.00002524
Iteration 50/1000 | Loss: 0.00002524
Iteration 51/1000 | Loss: 0.00002524
Iteration 52/1000 | Loss: 0.00002524
Iteration 53/1000 | Loss: 0.00002524
Iteration 54/1000 | Loss: 0.00002524
Iteration 55/1000 | Loss: 0.00002524
Iteration 56/1000 | Loss: 0.00002524
Iteration 57/1000 | Loss: 0.00002523
Iteration 58/1000 | Loss: 0.00002523
Iteration 59/1000 | Loss: 0.00002523
Iteration 60/1000 | Loss: 0.00002523
Iteration 61/1000 | Loss: 0.00002523
Iteration 62/1000 | Loss: 0.00002522
Iteration 63/1000 | Loss: 0.00002522
Iteration 64/1000 | Loss: 0.00002522
Iteration 65/1000 | Loss: 0.00002522
Iteration 66/1000 | Loss: 0.00002522
Iteration 67/1000 | Loss: 0.00002522
Iteration 68/1000 | Loss: 0.00002522
Iteration 69/1000 | Loss: 0.00002522
Iteration 70/1000 | Loss: 0.00002522
Iteration 71/1000 | Loss: 0.00002522
Iteration 72/1000 | Loss: 0.00002522
Iteration 73/1000 | Loss: 0.00002522
Iteration 74/1000 | Loss: 0.00002521
Iteration 75/1000 | Loss: 0.00002521
Iteration 76/1000 | Loss: 0.00002521
Iteration 77/1000 | Loss: 0.00002521
Iteration 78/1000 | Loss: 0.00002521
Iteration 79/1000 | Loss: 0.00002520
Iteration 80/1000 | Loss: 0.00002520
Iteration 81/1000 | Loss: 0.00002520
Iteration 82/1000 | Loss: 0.00002519
Iteration 83/1000 | Loss: 0.00002519
Iteration 84/1000 | Loss: 0.00002519
Iteration 85/1000 | Loss: 0.00002519
Iteration 86/1000 | Loss: 0.00002519
Iteration 87/1000 | Loss: 0.00002519
Iteration 88/1000 | Loss: 0.00002519
Iteration 89/1000 | Loss: 0.00002519
Iteration 90/1000 | Loss: 0.00002518
Iteration 91/1000 | Loss: 0.00002518
Iteration 92/1000 | Loss: 0.00002518
Iteration 93/1000 | Loss: 0.00002518
Iteration 94/1000 | Loss: 0.00002517
Iteration 95/1000 | Loss: 0.00002517
Iteration 96/1000 | Loss: 0.00002517
Iteration 97/1000 | Loss: 0.00002517
Iteration 98/1000 | Loss: 0.00002517
Iteration 99/1000 | Loss: 0.00002517
Iteration 100/1000 | Loss: 0.00002517
Iteration 101/1000 | Loss: 0.00002516
Iteration 102/1000 | Loss: 0.00002516
Iteration 103/1000 | Loss: 0.00002516
Iteration 104/1000 | Loss: 0.00002516
Iteration 105/1000 | Loss: 0.00002516
Iteration 106/1000 | Loss: 0.00002516
Iteration 107/1000 | Loss: 0.00002516
Iteration 108/1000 | Loss: 0.00002516
Iteration 109/1000 | Loss: 0.00002516
Iteration 110/1000 | Loss: 0.00002516
Iteration 111/1000 | Loss: 0.00002516
Iteration 112/1000 | Loss: 0.00002516
Iteration 113/1000 | Loss: 0.00002516
Iteration 114/1000 | Loss: 0.00002516
Iteration 115/1000 | Loss: 0.00002516
Iteration 116/1000 | Loss: 0.00002516
Iteration 117/1000 | Loss: 0.00002516
Iteration 118/1000 | Loss: 0.00002516
Iteration 119/1000 | Loss: 0.00002516
Iteration 120/1000 | Loss: 0.00002516
Iteration 121/1000 | Loss: 0.00002516
Iteration 122/1000 | Loss: 0.00002516
Iteration 123/1000 | Loss: 0.00002516
Iteration 124/1000 | Loss: 0.00002516
Iteration 125/1000 | Loss: 0.00002516
Iteration 126/1000 | Loss: 0.00002516
Iteration 127/1000 | Loss: 0.00002516
Iteration 128/1000 | Loss: 0.00002516
Iteration 129/1000 | Loss: 0.00002516
Iteration 130/1000 | Loss: 0.00002516
Iteration 131/1000 | Loss: 0.00002516
Iteration 132/1000 | Loss: 0.00002516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.51577039307449e-05, 2.51577039307449e-05, 2.51577039307449e-05, 2.51577039307449e-05, 2.51577039307449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.51577039307449e-05

Optimization complete. Final v2v error: 4.5082478523254395 mm

Highest mean error: 4.782747745513916 mm for frame 108

Lowest mean error: 4.19268798828125 mm for frame 37

Saving results

Total time: 37.42975115776062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952620
Iteration 2/25 | Loss: 0.00167116
Iteration 3/25 | Loss: 0.00143776
Iteration 4/25 | Loss: 0.00140552
Iteration 5/25 | Loss: 0.00138987
Iteration 6/25 | Loss: 0.00138522
Iteration 7/25 | Loss: 0.00138343
Iteration 8/25 | Loss: 0.00138320
Iteration 9/25 | Loss: 0.00138320
Iteration 10/25 | Loss: 0.00138320
Iteration 11/25 | Loss: 0.00138320
Iteration 12/25 | Loss: 0.00138320
Iteration 13/25 | Loss: 0.00138320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013832045951858163, 0.0013832045951858163, 0.0013832045951858163, 0.0013832045951858163, 0.0013832045951858163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013832045951858163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42920375
Iteration 2/25 | Loss: 0.00100482
Iteration 3/25 | Loss: 0.00100482
Iteration 4/25 | Loss: 0.00100482
Iteration 5/25 | Loss: 0.00100482
Iteration 6/25 | Loss: 0.00100482
Iteration 7/25 | Loss: 0.00100482
Iteration 8/25 | Loss: 0.00100482
Iteration 9/25 | Loss: 0.00100482
Iteration 10/25 | Loss: 0.00100482
Iteration 11/25 | Loss: 0.00100482
Iteration 12/25 | Loss: 0.00100482
Iteration 13/25 | Loss: 0.00100482
Iteration 14/25 | Loss: 0.00100482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010048222029581666, 0.0010048222029581666, 0.0010048222029581666, 0.0010048222029581666, 0.0010048222029581666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010048222029581666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100482
Iteration 2/1000 | Loss: 0.00005995
Iteration 3/1000 | Loss: 0.00004523
Iteration 4/1000 | Loss: 0.00004050
Iteration 5/1000 | Loss: 0.00003780
Iteration 6/1000 | Loss: 0.00003601
Iteration 7/1000 | Loss: 0.00003466
Iteration 8/1000 | Loss: 0.00003380
Iteration 9/1000 | Loss: 0.00003327
Iteration 10/1000 | Loss: 0.00003291
Iteration 11/1000 | Loss: 0.00003260
Iteration 12/1000 | Loss: 0.00003231
Iteration 13/1000 | Loss: 0.00003205
Iteration 14/1000 | Loss: 0.00003200
Iteration 15/1000 | Loss: 0.00003181
Iteration 16/1000 | Loss: 0.00003175
Iteration 17/1000 | Loss: 0.00003174
Iteration 18/1000 | Loss: 0.00003173
Iteration 19/1000 | Loss: 0.00003173
Iteration 20/1000 | Loss: 0.00003165
Iteration 21/1000 | Loss: 0.00003161
Iteration 22/1000 | Loss: 0.00003160
Iteration 23/1000 | Loss: 0.00003159
Iteration 24/1000 | Loss: 0.00003158
Iteration 25/1000 | Loss: 0.00003158
Iteration 26/1000 | Loss: 0.00003158
Iteration 27/1000 | Loss: 0.00003157
Iteration 28/1000 | Loss: 0.00003156
Iteration 29/1000 | Loss: 0.00003156
Iteration 30/1000 | Loss: 0.00003156
Iteration 31/1000 | Loss: 0.00003156
Iteration 32/1000 | Loss: 0.00003155
Iteration 33/1000 | Loss: 0.00003155
Iteration 34/1000 | Loss: 0.00003155
Iteration 35/1000 | Loss: 0.00003155
Iteration 36/1000 | Loss: 0.00003155
Iteration 37/1000 | Loss: 0.00003155
Iteration 38/1000 | Loss: 0.00003154
Iteration 39/1000 | Loss: 0.00003154
Iteration 40/1000 | Loss: 0.00003153
Iteration 41/1000 | Loss: 0.00003153
Iteration 42/1000 | Loss: 0.00003153
Iteration 43/1000 | Loss: 0.00003152
Iteration 44/1000 | Loss: 0.00003152
Iteration 45/1000 | Loss: 0.00003152
Iteration 46/1000 | Loss: 0.00003151
Iteration 47/1000 | Loss: 0.00003151
Iteration 48/1000 | Loss: 0.00003151
Iteration 49/1000 | Loss: 0.00003151
Iteration 50/1000 | Loss: 0.00003151
Iteration 51/1000 | Loss: 0.00003150
Iteration 52/1000 | Loss: 0.00003150
Iteration 53/1000 | Loss: 0.00003150
Iteration 54/1000 | Loss: 0.00003150
Iteration 55/1000 | Loss: 0.00003150
Iteration 56/1000 | Loss: 0.00003149
Iteration 57/1000 | Loss: 0.00003149
Iteration 58/1000 | Loss: 0.00003149
Iteration 59/1000 | Loss: 0.00003149
Iteration 60/1000 | Loss: 0.00003149
Iteration 61/1000 | Loss: 0.00003149
Iteration 62/1000 | Loss: 0.00003149
Iteration 63/1000 | Loss: 0.00003149
Iteration 64/1000 | Loss: 0.00003149
Iteration 65/1000 | Loss: 0.00003149
Iteration 66/1000 | Loss: 0.00003149
Iteration 67/1000 | Loss: 0.00003149
Iteration 68/1000 | Loss: 0.00003148
Iteration 69/1000 | Loss: 0.00003148
Iteration 70/1000 | Loss: 0.00003148
Iteration 71/1000 | Loss: 0.00003147
Iteration 72/1000 | Loss: 0.00003147
Iteration 73/1000 | Loss: 0.00003147
Iteration 74/1000 | Loss: 0.00003147
Iteration 75/1000 | Loss: 0.00003147
Iteration 76/1000 | Loss: 0.00003147
Iteration 77/1000 | Loss: 0.00003147
Iteration 78/1000 | Loss: 0.00003147
Iteration 79/1000 | Loss: 0.00003147
Iteration 80/1000 | Loss: 0.00003147
Iteration 81/1000 | Loss: 0.00003147
Iteration 82/1000 | Loss: 0.00003146
Iteration 83/1000 | Loss: 0.00003146
Iteration 84/1000 | Loss: 0.00003146
Iteration 85/1000 | Loss: 0.00003146
Iteration 86/1000 | Loss: 0.00003146
Iteration 87/1000 | Loss: 0.00003145
Iteration 88/1000 | Loss: 0.00003145
Iteration 89/1000 | Loss: 0.00003145
Iteration 90/1000 | Loss: 0.00003145
Iteration 91/1000 | Loss: 0.00003145
Iteration 92/1000 | Loss: 0.00003145
Iteration 93/1000 | Loss: 0.00003145
Iteration 94/1000 | Loss: 0.00003145
Iteration 95/1000 | Loss: 0.00003145
Iteration 96/1000 | Loss: 0.00003145
Iteration 97/1000 | Loss: 0.00003145
Iteration 98/1000 | Loss: 0.00003145
Iteration 99/1000 | Loss: 0.00003144
Iteration 100/1000 | Loss: 0.00003144
Iteration 101/1000 | Loss: 0.00003144
Iteration 102/1000 | Loss: 0.00003144
Iteration 103/1000 | Loss: 0.00003144
Iteration 104/1000 | Loss: 0.00003144
Iteration 105/1000 | Loss: 0.00003144
Iteration 106/1000 | Loss: 0.00003144
Iteration 107/1000 | Loss: 0.00003144
Iteration 108/1000 | Loss: 0.00003144
Iteration 109/1000 | Loss: 0.00003144
Iteration 110/1000 | Loss: 0.00003144
Iteration 111/1000 | Loss: 0.00003144
Iteration 112/1000 | Loss: 0.00003144
Iteration 113/1000 | Loss: 0.00003144
Iteration 114/1000 | Loss: 0.00003144
Iteration 115/1000 | Loss: 0.00003144
Iteration 116/1000 | Loss: 0.00003144
Iteration 117/1000 | Loss: 0.00003144
Iteration 118/1000 | Loss: 0.00003144
Iteration 119/1000 | Loss: 0.00003144
Iteration 120/1000 | Loss: 0.00003144
Iteration 121/1000 | Loss: 0.00003144
Iteration 122/1000 | Loss: 0.00003144
Iteration 123/1000 | Loss: 0.00003144
Iteration 124/1000 | Loss: 0.00003144
Iteration 125/1000 | Loss: 0.00003144
Iteration 126/1000 | Loss: 0.00003144
Iteration 127/1000 | Loss: 0.00003144
Iteration 128/1000 | Loss: 0.00003144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [3.1441788451047614e-05, 3.1441788451047614e-05, 3.1441788451047614e-05, 3.1441788451047614e-05, 3.1441788451047614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1441788451047614e-05

Optimization complete. Final v2v error: 4.877306938171387 mm

Highest mean error: 6.771859645843506 mm for frame 71

Lowest mean error: 4.12185525894165 mm for frame 5

Saving results

Total time: 39.196497201919556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529748
Iteration 2/25 | Loss: 0.00151236
Iteration 3/25 | Loss: 0.00140779
Iteration 4/25 | Loss: 0.00139008
Iteration 5/25 | Loss: 0.00138616
Iteration 6/25 | Loss: 0.00138503
Iteration 7/25 | Loss: 0.00138503
Iteration 8/25 | Loss: 0.00138503
Iteration 9/25 | Loss: 0.00138503
Iteration 10/25 | Loss: 0.00138503
Iteration 11/25 | Loss: 0.00138503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013850294053554535, 0.0013850294053554535, 0.0013850294053554535, 0.0013850294053554535, 0.0013850294053554535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013850294053554535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.82914352
Iteration 2/25 | Loss: 0.00125096
Iteration 3/25 | Loss: 0.00125095
Iteration 4/25 | Loss: 0.00125095
Iteration 5/25 | Loss: 0.00125095
Iteration 6/25 | Loss: 0.00125095
Iteration 7/25 | Loss: 0.00125095
Iteration 8/25 | Loss: 0.00125095
Iteration 9/25 | Loss: 0.00125095
Iteration 10/25 | Loss: 0.00125095
Iteration 11/25 | Loss: 0.00125095
Iteration 12/25 | Loss: 0.00125095
Iteration 13/25 | Loss: 0.00125095
Iteration 14/25 | Loss: 0.00125095
Iteration 15/25 | Loss: 0.00125095
Iteration 16/25 | Loss: 0.00125095
Iteration 17/25 | Loss: 0.00125095
Iteration 18/25 | Loss: 0.00125095
Iteration 19/25 | Loss: 0.00125095
Iteration 20/25 | Loss: 0.00125095
Iteration 21/25 | Loss: 0.00125095
Iteration 22/25 | Loss: 0.00125095
Iteration 23/25 | Loss: 0.00125095
Iteration 24/25 | Loss: 0.00125095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012509512016549706, 0.0012509512016549706, 0.0012509512016549706, 0.0012509512016549706, 0.0012509512016549706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012509512016549706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125095
Iteration 2/1000 | Loss: 0.00005553
Iteration 3/1000 | Loss: 0.00004156
Iteration 4/1000 | Loss: 0.00003743
Iteration 5/1000 | Loss: 0.00003484
Iteration 6/1000 | Loss: 0.00003384
Iteration 7/1000 | Loss: 0.00003314
Iteration 8/1000 | Loss: 0.00003277
Iteration 9/1000 | Loss: 0.00003253
Iteration 10/1000 | Loss: 0.00003230
Iteration 11/1000 | Loss: 0.00003226
Iteration 12/1000 | Loss: 0.00003222
Iteration 13/1000 | Loss: 0.00003221
Iteration 14/1000 | Loss: 0.00003220
Iteration 15/1000 | Loss: 0.00003220
Iteration 16/1000 | Loss: 0.00003220
Iteration 17/1000 | Loss: 0.00003220
Iteration 18/1000 | Loss: 0.00003219
Iteration 19/1000 | Loss: 0.00003219
Iteration 20/1000 | Loss: 0.00003219
Iteration 21/1000 | Loss: 0.00003215
Iteration 22/1000 | Loss: 0.00003209
Iteration 23/1000 | Loss: 0.00003208
Iteration 24/1000 | Loss: 0.00003208
Iteration 25/1000 | Loss: 0.00003207
Iteration 26/1000 | Loss: 0.00003207
Iteration 27/1000 | Loss: 0.00003206
Iteration 28/1000 | Loss: 0.00003206
Iteration 29/1000 | Loss: 0.00003205
Iteration 30/1000 | Loss: 0.00003205
Iteration 31/1000 | Loss: 0.00003205
Iteration 32/1000 | Loss: 0.00003204
Iteration 33/1000 | Loss: 0.00003204
Iteration 34/1000 | Loss: 0.00003203
Iteration 35/1000 | Loss: 0.00003202
Iteration 36/1000 | Loss: 0.00003201
Iteration 37/1000 | Loss: 0.00003201
Iteration 38/1000 | Loss: 0.00003201
Iteration 39/1000 | Loss: 0.00003201
Iteration 40/1000 | Loss: 0.00003201
Iteration 41/1000 | Loss: 0.00003201
Iteration 42/1000 | Loss: 0.00003201
Iteration 43/1000 | Loss: 0.00003201
Iteration 44/1000 | Loss: 0.00003201
Iteration 45/1000 | Loss: 0.00003201
Iteration 46/1000 | Loss: 0.00003201
Iteration 47/1000 | Loss: 0.00003200
Iteration 48/1000 | Loss: 0.00003200
Iteration 49/1000 | Loss: 0.00003200
Iteration 50/1000 | Loss: 0.00003198
Iteration 51/1000 | Loss: 0.00003198
Iteration 52/1000 | Loss: 0.00003198
Iteration 53/1000 | Loss: 0.00003197
Iteration 54/1000 | Loss: 0.00003197
Iteration 55/1000 | Loss: 0.00003196
Iteration 56/1000 | Loss: 0.00003195
Iteration 57/1000 | Loss: 0.00003195
Iteration 58/1000 | Loss: 0.00003194
Iteration 59/1000 | Loss: 0.00003194
Iteration 60/1000 | Loss: 0.00003194
Iteration 61/1000 | Loss: 0.00003193
Iteration 62/1000 | Loss: 0.00003193
Iteration 63/1000 | Loss: 0.00003193
Iteration 64/1000 | Loss: 0.00003192
Iteration 65/1000 | Loss: 0.00003191
Iteration 66/1000 | Loss: 0.00003191
Iteration 67/1000 | Loss: 0.00003190
Iteration 68/1000 | Loss: 0.00003190
Iteration 69/1000 | Loss: 0.00003190
Iteration 70/1000 | Loss: 0.00003190
Iteration 71/1000 | Loss: 0.00003190
Iteration 72/1000 | Loss: 0.00003190
Iteration 73/1000 | Loss: 0.00003190
Iteration 74/1000 | Loss: 0.00003190
Iteration 75/1000 | Loss: 0.00003190
Iteration 76/1000 | Loss: 0.00003189
Iteration 77/1000 | Loss: 0.00003189
Iteration 78/1000 | Loss: 0.00003189
Iteration 79/1000 | Loss: 0.00003189
Iteration 80/1000 | Loss: 0.00003189
Iteration 81/1000 | Loss: 0.00003189
Iteration 82/1000 | Loss: 0.00003189
Iteration 83/1000 | Loss: 0.00003189
Iteration 84/1000 | Loss: 0.00003189
Iteration 85/1000 | Loss: 0.00003189
Iteration 86/1000 | Loss: 0.00003189
Iteration 87/1000 | Loss: 0.00003189
Iteration 88/1000 | Loss: 0.00003189
Iteration 89/1000 | Loss: 0.00003188
Iteration 90/1000 | Loss: 0.00003188
Iteration 91/1000 | Loss: 0.00003187
Iteration 92/1000 | Loss: 0.00003187
Iteration 93/1000 | Loss: 0.00003186
Iteration 94/1000 | Loss: 0.00003185
Iteration 95/1000 | Loss: 0.00003185
Iteration 96/1000 | Loss: 0.00003185
Iteration 97/1000 | Loss: 0.00003185
Iteration 98/1000 | Loss: 0.00003185
Iteration 99/1000 | Loss: 0.00003184
Iteration 100/1000 | Loss: 0.00003184
Iteration 101/1000 | Loss: 0.00003184
Iteration 102/1000 | Loss: 0.00003184
Iteration 103/1000 | Loss: 0.00003184
Iteration 104/1000 | Loss: 0.00003184
Iteration 105/1000 | Loss: 0.00003184
Iteration 106/1000 | Loss: 0.00003183
Iteration 107/1000 | Loss: 0.00003183
Iteration 108/1000 | Loss: 0.00003183
Iteration 109/1000 | Loss: 0.00003183
Iteration 110/1000 | Loss: 0.00003183
Iteration 111/1000 | Loss: 0.00003183
Iteration 112/1000 | Loss: 0.00003183
Iteration 113/1000 | Loss: 0.00003183
Iteration 114/1000 | Loss: 0.00003183
Iteration 115/1000 | Loss: 0.00003183
Iteration 116/1000 | Loss: 0.00003183
Iteration 117/1000 | Loss: 0.00003183
Iteration 118/1000 | Loss: 0.00003183
Iteration 119/1000 | Loss: 0.00003183
Iteration 120/1000 | Loss: 0.00003183
Iteration 121/1000 | Loss: 0.00003183
Iteration 122/1000 | Loss: 0.00003183
Iteration 123/1000 | Loss: 0.00003183
Iteration 124/1000 | Loss: 0.00003182
Iteration 125/1000 | Loss: 0.00003182
Iteration 126/1000 | Loss: 0.00003182
Iteration 127/1000 | Loss: 0.00003182
Iteration 128/1000 | Loss: 0.00003182
Iteration 129/1000 | Loss: 0.00003182
Iteration 130/1000 | Loss: 0.00003182
Iteration 131/1000 | Loss: 0.00003182
Iteration 132/1000 | Loss: 0.00003182
Iteration 133/1000 | Loss: 0.00003182
Iteration 134/1000 | Loss: 0.00003182
Iteration 135/1000 | Loss: 0.00003182
Iteration 136/1000 | Loss: 0.00003182
Iteration 137/1000 | Loss: 0.00003182
Iteration 138/1000 | Loss: 0.00003181
Iteration 139/1000 | Loss: 0.00003181
Iteration 140/1000 | Loss: 0.00003181
Iteration 141/1000 | Loss: 0.00003181
Iteration 142/1000 | Loss: 0.00003181
Iteration 143/1000 | Loss: 0.00003181
Iteration 144/1000 | Loss: 0.00003181
Iteration 145/1000 | Loss: 0.00003181
Iteration 146/1000 | Loss: 0.00003181
Iteration 147/1000 | Loss: 0.00003181
Iteration 148/1000 | Loss: 0.00003181
Iteration 149/1000 | Loss: 0.00003181
Iteration 150/1000 | Loss: 0.00003181
Iteration 151/1000 | Loss: 0.00003181
Iteration 152/1000 | Loss: 0.00003181
Iteration 153/1000 | Loss: 0.00003181
Iteration 154/1000 | Loss: 0.00003181
Iteration 155/1000 | Loss: 0.00003181
Iteration 156/1000 | Loss: 0.00003181
Iteration 157/1000 | Loss: 0.00003181
Iteration 158/1000 | Loss: 0.00003181
Iteration 159/1000 | Loss: 0.00003181
Iteration 160/1000 | Loss: 0.00003181
Iteration 161/1000 | Loss: 0.00003181
Iteration 162/1000 | Loss: 0.00003181
Iteration 163/1000 | Loss: 0.00003181
Iteration 164/1000 | Loss: 0.00003181
Iteration 165/1000 | Loss: 0.00003181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [3.181404099450447e-05, 3.181404099450447e-05, 3.181404099450447e-05, 3.181404099450447e-05, 3.181404099450447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.181404099450447e-05

Optimization complete. Final v2v error: 5.062761306762695 mm

Highest mean error: 5.366444110870361 mm for frame 115

Lowest mean error: 4.7566118240356445 mm for frame 64

Saving results

Total time: 34.1429398059845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037385
Iteration 2/25 | Loss: 0.00186097
Iteration 3/25 | Loss: 0.00158915
Iteration 4/25 | Loss: 0.00155723
Iteration 5/25 | Loss: 0.00154439
Iteration 6/25 | Loss: 0.00153586
Iteration 7/25 | Loss: 0.00152966
Iteration 8/25 | Loss: 0.00152334
Iteration 9/25 | Loss: 0.00152172
Iteration 10/25 | Loss: 0.00152375
Iteration 11/25 | Loss: 0.00152824
Iteration 12/25 | Loss: 0.00152607
Iteration 13/25 | Loss: 0.00151762
Iteration 14/25 | Loss: 0.00151412
Iteration 15/25 | Loss: 0.00151069
Iteration 16/25 | Loss: 0.00150928
Iteration 17/25 | Loss: 0.00150982
Iteration 18/25 | Loss: 0.00150870
Iteration 19/25 | Loss: 0.00150739
Iteration 20/25 | Loss: 0.00150616
Iteration 21/25 | Loss: 0.00150593
Iteration 22/25 | Loss: 0.00150592
Iteration 23/25 | Loss: 0.00150592
Iteration 24/25 | Loss: 0.00150592
Iteration 25/25 | Loss: 0.00150592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79589796
Iteration 2/25 | Loss: 0.00168620
Iteration 3/25 | Loss: 0.00168619
Iteration 4/25 | Loss: 0.00168619
Iteration 5/25 | Loss: 0.00168619
Iteration 6/25 | Loss: 0.00168619
Iteration 7/25 | Loss: 0.00168619
Iteration 8/25 | Loss: 0.00168619
Iteration 9/25 | Loss: 0.00168619
Iteration 10/25 | Loss: 0.00168619
Iteration 11/25 | Loss: 0.00168619
Iteration 12/25 | Loss: 0.00168619
Iteration 13/25 | Loss: 0.00168619
Iteration 14/25 | Loss: 0.00168619
Iteration 15/25 | Loss: 0.00168619
Iteration 16/25 | Loss: 0.00168619
Iteration 17/25 | Loss: 0.00168619
Iteration 18/25 | Loss: 0.00168619
Iteration 19/25 | Loss: 0.00168619
Iteration 20/25 | Loss: 0.00168619
Iteration 21/25 | Loss: 0.00168619
Iteration 22/25 | Loss: 0.00168619
Iteration 23/25 | Loss: 0.00168619
Iteration 24/25 | Loss: 0.00168619
Iteration 25/25 | Loss: 0.00168619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168619
Iteration 2/1000 | Loss: 0.00015845
Iteration 3/1000 | Loss: 0.00010529
Iteration 4/1000 | Loss: 0.00342259
Iteration 5/1000 | Loss: 0.00063596
Iteration 6/1000 | Loss: 0.00733156
Iteration 7/1000 | Loss: 0.00093565
Iteration 8/1000 | Loss: 0.00160906
Iteration 9/1000 | Loss: 0.00697553
Iteration 10/1000 | Loss: 0.00047734
Iteration 11/1000 | Loss: 0.00098047
Iteration 12/1000 | Loss: 0.00133297
Iteration 13/1000 | Loss: 0.00110971
Iteration 14/1000 | Loss: 0.00082006
Iteration 15/1000 | Loss: 0.00048068
Iteration 16/1000 | Loss: 0.00115923
Iteration 17/1000 | Loss: 0.00085863
Iteration 18/1000 | Loss: 0.00085225
Iteration 19/1000 | Loss: 0.00058475
Iteration 20/1000 | Loss: 0.00064293
Iteration 21/1000 | Loss: 0.00120456
Iteration 22/1000 | Loss: 0.00150825
Iteration 23/1000 | Loss: 0.00050548
Iteration 24/1000 | Loss: 0.00046271
Iteration 25/1000 | Loss: 0.00010095
Iteration 26/1000 | Loss: 0.00008897
Iteration 27/1000 | Loss: 0.00062966
Iteration 28/1000 | Loss: 0.00099809
Iteration 29/1000 | Loss: 0.00014078
Iteration 30/1000 | Loss: 0.00009493
Iteration 31/1000 | Loss: 0.00036164
Iteration 32/1000 | Loss: 0.00024937
Iteration 33/1000 | Loss: 0.00026231
Iteration 34/1000 | Loss: 0.00026820
Iteration 35/1000 | Loss: 0.00061694
Iteration 36/1000 | Loss: 0.00082751
Iteration 37/1000 | Loss: 0.00032437
Iteration 38/1000 | Loss: 0.00032642
Iteration 39/1000 | Loss: 0.00031883
Iteration 40/1000 | Loss: 0.00039181
Iteration 41/1000 | Loss: 0.00063850
Iteration 42/1000 | Loss: 0.00011677
Iteration 43/1000 | Loss: 0.00007199
Iteration 44/1000 | Loss: 0.00043857
Iteration 45/1000 | Loss: 0.00044496
Iteration 46/1000 | Loss: 0.00060840
Iteration 47/1000 | Loss: 0.00015740
Iteration 48/1000 | Loss: 0.00022124
Iteration 49/1000 | Loss: 0.00011208
Iteration 50/1000 | Loss: 0.00016509
Iteration 51/1000 | Loss: 0.00015867
Iteration 52/1000 | Loss: 0.00004535
Iteration 53/1000 | Loss: 0.00004255
Iteration 54/1000 | Loss: 0.00004083
Iteration 55/1000 | Loss: 0.00003989
Iteration 56/1000 | Loss: 0.00004509
Iteration 57/1000 | Loss: 0.00003927
Iteration 58/1000 | Loss: 0.00003849
Iteration 59/1000 | Loss: 0.00003786
Iteration 60/1000 | Loss: 0.00003759
Iteration 61/1000 | Loss: 0.00003736
Iteration 62/1000 | Loss: 0.00003718
Iteration 63/1000 | Loss: 0.00003712
Iteration 64/1000 | Loss: 0.00003701
Iteration 65/1000 | Loss: 0.00003698
Iteration 66/1000 | Loss: 0.00003697
Iteration 67/1000 | Loss: 0.00003697
Iteration 68/1000 | Loss: 0.00003696
Iteration 69/1000 | Loss: 0.00003696
Iteration 70/1000 | Loss: 0.00003695
Iteration 71/1000 | Loss: 0.00003693
Iteration 72/1000 | Loss: 0.00003693
Iteration 73/1000 | Loss: 0.00003693
Iteration 74/1000 | Loss: 0.00003693
Iteration 75/1000 | Loss: 0.00003693
Iteration 76/1000 | Loss: 0.00003693
Iteration 77/1000 | Loss: 0.00003693
Iteration 78/1000 | Loss: 0.00003693
Iteration 79/1000 | Loss: 0.00003692
Iteration 80/1000 | Loss: 0.00003692
Iteration 81/1000 | Loss: 0.00003692
Iteration 82/1000 | Loss: 0.00003691
Iteration 83/1000 | Loss: 0.00003690
Iteration 84/1000 | Loss: 0.00003690
Iteration 85/1000 | Loss: 0.00003689
Iteration 86/1000 | Loss: 0.00003689
Iteration 87/1000 | Loss: 0.00003689
Iteration 88/1000 | Loss: 0.00003689
Iteration 89/1000 | Loss: 0.00003689
Iteration 90/1000 | Loss: 0.00003689
Iteration 91/1000 | Loss: 0.00003689
Iteration 92/1000 | Loss: 0.00003689
Iteration 93/1000 | Loss: 0.00003689
Iteration 94/1000 | Loss: 0.00003689
Iteration 95/1000 | Loss: 0.00003689
Iteration 96/1000 | Loss: 0.00003688
Iteration 97/1000 | Loss: 0.00003688
Iteration 98/1000 | Loss: 0.00003688
Iteration 99/1000 | Loss: 0.00003688
Iteration 100/1000 | Loss: 0.00003688
Iteration 101/1000 | Loss: 0.00003688
Iteration 102/1000 | Loss: 0.00003687
Iteration 103/1000 | Loss: 0.00003687
Iteration 104/1000 | Loss: 0.00003687
Iteration 105/1000 | Loss: 0.00003687
Iteration 106/1000 | Loss: 0.00003687
Iteration 107/1000 | Loss: 0.00003687
Iteration 108/1000 | Loss: 0.00003687
Iteration 109/1000 | Loss: 0.00003687
Iteration 110/1000 | Loss: 0.00003686
Iteration 111/1000 | Loss: 0.00003686
Iteration 112/1000 | Loss: 0.00003686
Iteration 113/1000 | Loss: 0.00003686
Iteration 114/1000 | Loss: 0.00003686
Iteration 115/1000 | Loss: 0.00003686
Iteration 116/1000 | Loss: 0.00003686
Iteration 117/1000 | Loss: 0.00003685
Iteration 118/1000 | Loss: 0.00003685
Iteration 119/1000 | Loss: 0.00003685
Iteration 120/1000 | Loss: 0.00003685
Iteration 121/1000 | Loss: 0.00003685
Iteration 122/1000 | Loss: 0.00003685
Iteration 123/1000 | Loss: 0.00003685
Iteration 124/1000 | Loss: 0.00003685
Iteration 125/1000 | Loss: 0.00003685
Iteration 126/1000 | Loss: 0.00003685
Iteration 127/1000 | Loss: 0.00003684
Iteration 128/1000 | Loss: 0.00003684
Iteration 129/1000 | Loss: 0.00003684
Iteration 130/1000 | Loss: 0.00003684
Iteration 131/1000 | Loss: 0.00003684
Iteration 132/1000 | Loss: 0.00003684
Iteration 133/1000 | Loss: 0.00003684
Iteration 134/1000 | Loss: 0.00003684
Iteration 135/1000 | Loss: 0.00003684
Iteration 136/1000 | Loss: 0.00003684
Iteration 137/1000 | Loss: 0.00003684
Iteration 138/1000 | Loss: 0.00003684
Iteration 139/1000 | Loss: 0.00003684
Iteration 140/1000 | Loss: 0.00003684
Iteration 141/1000 | Loss: 0.00003684
Iteration 142/1000 | Loss: 0.00003684
Iteration 143/1000 | Loss: 0.00003683
Iteration 144/1000 | Loss: 0.00003683
Iteration 145/1000 | Loss: 0.00003683
Iteration 146/1000 | Loss: 0.00003683
Iteration 147/1000 | Loss: 0.00003683
Iteration 148/1000 | Loss: 0.00003683
Iteration 149/1000 | Loss: 0.00003683
Iteration 150/1000 | Loss: 0.00003683
Iteration 151/1000 | Loss: 0.00003683
Iteration 152/1000 | Loss: 0.00003683
Iteration 153/1000 | Loss: 0.00003683
Iteration 154/1000 | Loss: 0.00003683
Iteration 155/1000 | Loss: 0.00003683
Iteration 156/1000 | Loss: 0.00003682
Iteration 157/1000 | Loss: 0.00003682
Iteration 158/1000 | Loss: 0.00003682
Iteration 159/1000 | Loss: 0.00003682
Iteration 160/1000 | Loss: 0.00003682
Iteration 161/1000 | Loss: 0.00003682
Iteration 162/1000 | Loss: 0.00003682
Iteration 163/1000 | Loss: 0.00003682
Iteration 164/1000 | Loss: 0.00003682
Iteration 165/1000 | Loss: 0.00003682
Iteration 166/1000 | Loss: 0.00003682
Iteration 167/1000 | Loss: 0.00003682
Iteration 168/1000 | Loss: 0.00003682
Iteration 169/1000 | Loss: 0.00003682
Iteration 170/1000 | Loss: 0.00003682
Iteration 171/1000 | Loss: 0.00003682
Iteration 172/1000 | Loss: 0.00003682
Iteration 173/1000 | Loss: 0.00003682
Iteration 174/1000 | Loss: 0.00003682
Iteration 175/1000 | Loss: 0.00003682
Iteration 176/1000 | Loss: 0.00003682
Iteration 177/1000 | Loss: 0.00003681
Iteration 178/1000 | Loss: 0.00003681
Iteration 179/1000 | Loss: 0.00003681
Iteration 180/1000 | Loss: 0.00003681
Iteration 181/1000 | Loss: 0.00003681
Iteration 182/1000 | Loss: 0.00003681
Iteration 183/1000 | Loss: 0.00003681
Iteration 184/1000 | Loss: 0.00003681
Iteration 185/1000 | Loss: 0.00003681
Iteration 186/1000 | Loss: 0.00003681
Iteration 187/1000 | Loss: 0.00003681
Iteration 188/1000 | Loss: 0.00003681
Iteration 189/1000 | Loss: 0.00003681
Iteration 190/1000 | Loss: 0.00003681
Iteration 191/1000 | Loss: 0.00003681
Iteration 192/1000 | Loss: 0.00003681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [3.681254020193592e-05, 3.681254020193592e-05, 3.681254020193592e-05, 3.681254020193592e-05, 3.681254020193592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.681254020193592e-05

Optimization complete. Final v2v error: 5.304288387298584 mm

Highest mean error: 6.257662296295166 mm for frame 142

Lowest mean error: 4.786899566650391 mm for frame 186

Saving results

Total time: 154.87949228286743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00664557
Iteration 2/25 | Loss: 0.00188996
Iteration 3/25 | Loss: 0.00146789
Iteration 4/25 | Loss: 0.00142185
Iteration 5/25 | Loss: 0.00141577
Iteration 6/25 | Loss: 0.00141447
Iteration 7/25 | Loss: 0.00141443
Iteration 8/25 | Loss: 0.00141443
Iteration 9/25 | Loss: 0.00141443
Iteration 10/25 | Loss: 0.00141443
Iteration 11/25 | Loss: 0.00141443
Iteration 12/25 | Loss: 0.00141443
Iteration 13/25 | Loss: 0.00141443
Iteration 14/25 | Loss: 0.00141443
Iteration 15/25 | Loss: 0.00141443
Iteration 16/25 | Loss: 0.00141443
Iteration 17/25 | Loss: 0.00141443
Iteration 18/25 | Loss: 0.00141443
Iteration 19/25 | Loss: 0.00141443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001414429978467524, 0.001414429978467524, 0.001414429978467524, 0.001414429978467524, 0.001414429978467524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001414429978467524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39781249
Iteration 2/25 | Loss: 0.00086645
Iteration 3/25 | Loss: 0.00086644
Iteration 4/25 | Loss: 0.00086644
Iteration 5/25 | Loss: 0.00086644
Iteration 6/25 | Loss: 0.00086644
Iteration 7/25 | Loss: 0.00086644
Iteration 8/25 | Loss: 0.00086644
Iteration 9/25 | Loss: 0.00086644
Iteration 10/25 | Loss: 0.00086644
Iteration 11/25 | Loss: 0.00086644
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008664447232149541, 0.0008664447232149541, 0.0008664447232149541, 0.0008664447232149541, 0.0008664447232149541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008664447232149541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086644
Iteration 2/1000 | Loss: 0.00006746
Iteration 3/1000 | Loss: 0.00004870
Iteration 4/1000 | Loss: 0.00004213
Iteration 5/1000 | Loss: 0.00003929
Iteration 6/1000 | Loss: 0.00003803
Iteration 7/1000 | Loss: 0.00003735
Iteration 8/1000 | Loss: 0.00003678
Iteration 9/1000 | Loss: 0.00003637
Iteration 10/1000 | Loss: 0.00003602
Iteration 11/1000 | Loss: 0.00003575
Iteration 12/1000 | Loss: 0.00003550
Iteration 13/1000 | Loss: 0.00003536
Iteration 14/1000 | Loss: 0.00003534
Iteration 15/1000 | Loss: 0.00003533
Iteration 16/1000 | Loss: 0.00003533
Iteration 17/1000 | Loss: 0.00003532
Iteration 18/1000 | Loss: 0.00003532
Iteration 19/1000 | Loss: 0.00003531
Iteration 20/1000 | Loss: 0.00003531
Iteration 21/1000 | Loss: 0.00003530
Iteration 22/1000 | Loss: 0.00003529
Iteration 23/1000 | Loss: 0.00003529
Iteration 24/1000 | Loss: 0.00003529
Iteration 25/1000 | Loss: 0.00003528
Iteration 26/1000 | Loss: 0.00003528
Iteration 27/1000 | Loss: 0.00003528
Iteration 28/1000 | Loss: 0.00003528
Iteration 29/1000 | Loss: 0.00003528
Iteration 30/1000 | Loss: 0.00003528
Iteration 31/1000 | Loss: 0.00003528
Iteration 32/1000 | Loss: 0.00003527
Iteration 33/1000 | Loss: 0.00003526
Iteration 34/1000 | Loss: 0.00003526
Iteration 35/1000 | Loss: 0.00003526
Iteration 36/1000 | Loss: 0.00003526
Iteration 37/1000 | Loss: 0.00003526
Iteration 38/1000 | Loss: 0.00003526
Iteration 39/1000 | Loss: 0.00003526
Iteration 40/1000 | Loss: 0.00003526
Iteration 41/1000 | Loss: 0.00003525
Iteration 42/1000 | Loss: 0.00003525
Iteration 43/1000 | Loss: 0.00003525
Iteration 44/1000 | Loss: 0.00003525
Iteration 45/1000 | Loss: 0.00003525
Iteration 46/1000 | Loss: 0.00003525
Iteration 47/1000 | Loss: 0.00003525
Iteration 48/1000 | Loss: 0.00003524
Iteration 49/1000 | Loss: 0.00003524
Iteration 50/1000 | Loss: 0.00003524
Iteration 51/1000 | Loss: 0.00003524
Iteration 52/1000 | Loss: 0.00003524
Iteration 53/1000 | Loss: 0.00003523
Iteration 54/1000 | Loss: 0.00003523
Iteration 55/1000 | Loss: 0.00003523
Iteration 56/1000 | Loss: 0.00003523
Iteration 57/1000 | Loss: 0.00003523
Iteration 58/1000 | Loss: 0.00003523
Iteration 59/1000 | Loss: 0.00003523
Iteration 60/1000 | Loss: 0.00003523
Iteration 61/1000 | Loss: 0.00003523
Iteration 62/1000 | Loss: 0.00003523
Iteration 63/1000 | Loss: 0.00003523
Iteration 64/1000 | Loss: 0.00003522
Iteration 65/1000 | Loss: 0.00003522
Iteration 66/1000 | Loss: 0.00003522
Iteration 67/1000 | Loss: 0.00003521
Iteration 68/1000 | Loss: 0.00003521
Iteration 69/1000 | Loss: 0.00003521
Iteration 70/1000 | Loss: 0.00003521
Iteration 71/1000 | Loss: 0.00003521
Iteration 72/1000 | Loss: 0.00003521
Iteration 73/1000 | Loss: 0.00003521
Iteration 74/1000 | Loss: 0.00003521
Iteration 75/1000 | Loss: 0.00003521
Iteration 76/1000 | Loss: 0.00003521
Iteration 77/1000 | Loss: 0.00003521
Iteration 78/1000 | Loss: 0.00003521
Iteration 79/1000 | Loss: 0.00003520
Iteration 80/1000 | Loss: 0.00003520
Iteration 81/1000 | Loss: 0.00003520
Iteration 82/1000 | Loss: 0.00003520
Iteration 83/1000 | Loss: 0.00003520
Iteration 84/1000 | Loss: 0.00003519
Iteration 85/1000 | Loss: 0.00003519
Iteration 86/1000 | Loss: 0.00003519
Iteration 87/1000 | Loss: 0.00003519
Iteration 88/1000 | Loss: 0.00003519
Iteration 89/1000 | Loss: 0.00003519
Iteration 90/1000 | Loss: 0.00003519
Iteration 91/1000 | Loss: 0.00003519
Iteration 92/1000 | Loss: 0.00003519
Iteration 93/1000 | Loss: 0.00003519
Iteration 94/1000 | Loss: 0.00003519
Iteration 95/1000 | Loss: 0.00003519
Iteration 96/1000 | Loss: 0.00003519
Iteration 97/1000 | Loss: 0.00003519
Iteration 98/1000 | Loss: 0.00003518
Iteration 99/1000 | Loss: 0.00003518
Iteration 100/1000 | Loss: 0.00003518
Iteration 101/1000 | Loss: 0.00003517
Iteration 102/1000 | Loss: 0.00003517
Iteration 103/1000 | Loss: 0.00003517
Iteration 104/1000 | Loss: 0.00003517
Iteration 105/1000 | Loss: 0.00003517
Iteration 106/1000 | Loss: 0.00003517
Iteration 107/1000 | Loss: 0.00003517
Iteration 108/1000 | Loss: 0.00003517
Iteration 109/1000 | Loss: 0.00003517
Iteration 110/1000 | Loss: 0.00003517
Iteration 111/1000 | Loss: 0.00003517
Iteration 112/1000 | Loss: 0.00003517
Iteration 113/1000 | Loss: 0.00003517
Iteration 114/1000 | Loss: 0.00003517
Iteration 115/1000 | Loss: 0.00003517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [3.516848664730787e-05, 3.516848664730787e-05, 3.516848664730787e-05, 3.516848664730787e-05, 3.516848664730787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.516848664730787e-05

Optimization complete. Final v2v error: 5.28038215637207 mm

Highest mean error: 5.426955223083496 mm for frame 11

Lowest mean error: 4.99275016784668 mm for frame 3

Saving results

Total time: 34.09664988517761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471129
Iteration 2/25 | Loss: 0.00148906
Iteration 3/25 | Loss: 0.00140587
Iteration 4/25 | Loss: 0.00139196
Iteration 5/25 | Loss: 0.00138604
Iteration 6/25 | Loss: 0.00138500
Iteration 7/25 | Loss: 0.00138500
Iteration 8/25 | Loss: 0.00138500
Iteration 9/25 | Loss: 0.00138500
Iteration 10/25 | Loss: 0.00138500
Iteration 11/25 | Loss: 0.00138500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013850011164322495, 0.0013850011164322495, 0.0013850011164322495, 0.0013850011164322495, 0.0013850011164322495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013850011164322495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.16471767
Iteration 2/25 | Loss: 0.00116615
Iteration 3/25 | Loss: 0.00116615
Iteration 4/25 | Loss: 0.00116615
Iteration 5/25 | Loss: 0.00116615
Iteration 6/25 | Loss: 0.00116615
Iteration 7/25 | Loss: 0.00116615
Iteration 8/25 | Loss: 0.00116615
Iteration 9/25 | Loss: 0.00116615
Iteration 10/25 | Loss: 0.00116615
Iteration 11/25 | Loss: 0.00116615
Iteration 12/25 | Loss: 0.00116615
Iteration 13/25 | Loss: 0.00116615
Iteration 14/25 | Loss: 0.00116615
Iteration 15/25 | Loss: 0.00116615
Iteration 16/25 | Loss: 0.00116615
Iteration 17/25 | Loss: 0.00116615
Iteration 18/25 | Loss: 0.00116615
Iteration 19/25 | Loss: 0.00116615
Iteration 20/25 | Loss: 0.00116615
Iteration 21/25 | Loss: 0.00116615
Iteration 22/25 | Loss: 0.00116615
Iteration 23/25 | Loss: 0.00116615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011661527678370476, 0.0011661527678370476, 0.0011661527678370476, 0.0011661527678370476, 0.0011661527678370476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011661527678370476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116615
Iteration 2/1000 | Loss: 0.00004935
Iteration 3/1000 | Loss: 0.00004090
Iteration 4/1000 | Loss: 0.00003578
Iteration 5/1000 | Loss: 0.00003444
Iteration 6/1000 | Loss: 0.00003335
Iteration 7/1000 | Loss: 0.00003289
Iteration 8/1000 | Loss: 0.00003281
Iteration 9/1000 | Loss: 0.00003244
Iteration 10/1000 | Loss: 0.00003219
Iteration 11/1000 | Loss: 0.00003219
Iteration 12/1000 | Loss: 0.00003218
Iteration 13/1000 | Loss: 0.00003213
Iteration 14/1000 | Loss: 0.00003213
Iteration 15/1000 | Loss: 0.00003212
Iteration 16/1000 | Loss: 0.00003211
Iteration 17/1000 | Loss: 0.00003210
Iteration 18/1000 | Loss: 0.00003210
Iteration 19/1000 | Loss: 0.00003209
Iteration 20/1000 | Loss: 0.00003209
Iteration 21/1000 | Loss: 0.00003208
Iteration 22/1000 | Loss: 0.00003208
Iteration 23/1000 | Loss: 0.00003207
Iteration 24/1000 | Loss: 0.00003207
Iteration 25/1000 | Loss: 0.00003207
Iteration 26/1000 | Loss: 0.00003206
Iteration 27/1000 | Loss: 0.00003206
Iteration 28/1000 | Loss: 0.00003206
Iteration 29/1000 | Loss: 0.00003206
Iteration 30/1000 | Loss: 0.00003205
Iteration 31/1000 | Loss: 0.00003205
Iteration 32/1000 | Loss: 0.00003205
Iteration 33/1000 | Loss: 0.00003205
Iteration 34/1000 | Loss: 0.00003205
Iteration 35/1000 | Loss: 0.00003205
Iteration 36/1000 | Loss: 0.00003205
Iteration 37/1000 | Loss: 0.00003205
Iteration 38/1000 | Loss: 0.00003205
Iteration 39/1000 | Loss: 0.00003205
Iteration 40/1000 | Loss: 0.00003204
Iteration 41/1000 | Loss: 0.00003204
Iteration 42/1000 | Loss: 0.00003203
Iteration 43/1000 | Loss: 0.00003203
Iteration 44/1000 | Loss: 0.00003202
Iteration 45/1000 | Loss: 0.00003202
Iteration 46/1000 | Loss: 0.00003202
Iteration 47/1000 | Loss: 0.00003202
Iteration 48/1000 | Loss: 0.00003202
Iteration 49/1000 | Loss: 0.00003201
Iteration 50/1000 | Loss: 0.00003200
Iteration 51/1000 | Loss: 0.00003199
Iteration 52/1000 | Loss: 0.00003199
Iteration 53/1000 | Loss: 0.00003199
Iteration 54/1000 | Loss: 0.00003198
Iteration 55/1000 | Loss: 0.00003198
Iteration 56/1000 | Loss: 0.00003197
Iteration 57/1000 | Loss: 0.00003197
Iteration 58/1000 | Loss: 0.00003197
Iteration 59/1000 | Loss: 0.00003196
Iteration 60/1000 | Loss: 0.00003196
Iteration 61/1000 | Loss: 0.00003194
Iteration 62/1000 | Loss: 0.00003194
Iteration 63/1000 | Loss: 0.00003194
Iteration 64/1000 | Loss: 0.00003194
Iteration 65/1000 | Loss: 0.00003194
Iteration 66/1000 | Loss: 0.00003194
Iteration 67/1000 | Loss: 0.00003194
Iteration 68/1000 | Loss: 0.00003194
Iteration 69/1000 | Loss: 0.00003194
Iteration 70/1000 | Loss: 0.00003193
Iteration 71/1000 | Loss: 0.00003193
Iteration 72/1000 | Loss: 0.00003193
Iteration 73/1000 | Loss: 0.00003193
Iteration 74/1000 | Loss: 0.00003193
Iteration 75/1000 | Loss: 0.00003193
Iteration 76/1000 | Loss: 0.00003193
Iteration 77/1000 | Loss: 0.00003192
Iteration 78/1000 | Loss: 0.00003192
Iteration 79/1000 | Loss: 0.00003192
Iteration 80/1000 | Loss: 0.00003192
Iteration 81/1000 | Loss: 0.00003192
Iteration 82/1000 | Loss: 0.00003191
Iteration 83/1000 | Loss: 0.00003191
Iteration 84/1000 | Loss: 0.00003191
Iteration 85/1000 | Loss: 0.00003191
Iteration 86/1000 | Loss: 0.00003191
Iteration 87/1000 | Loss: 0.00003191
Iteration 88/1000 | Loss: 0.00003191
Iteration 89/1000 | Loss: 0.00003191
Iteration 90/1000 | Loss: 0.00003191
Iteration 91/1000 | Loss: 0.00003191
Iteration 92/1000 | Loss: 0.00003191
Iteration 93/1000 | Loss: 0.00003191
Iteration 94/1000 | Loss: 0.00003191
Iteration 95/1000 | Loss: 0.00003191
Iteration 96/1000 | Loss: 0.00003190
Iteration 97/1000 | Loss: 0.00003190
Iteration 98/1000 | Loss: 0.00003190
Iteration 99/1000 | Loss: 0.00003190
Iteration 100/1000 | Loss: 0.00003189
Iteration 101/1000 | Loss: 0.00003189
Iteration 102/1000 | Loss: 0.00003189
Iteration 103/1000 | Loss: 0.00003188
Iteration 104/1000 | Loss: 0.00003188
Iteration 105/1000 | Loss: 0.00003188
Iteration 106/1000 | Loss: 0.00003188
Iteration 107/1000 | Loss: 0.00003187
Iteration 108/1000 | Loss: 0.00003187
Iteration 109/1000 | Loss: 0.00003187
Iteration 110/1000 | Loss: 0.00003186
Iteration 111/1000 | Loss: 0.00003186
Iteration 112/1000 | Loss: 0.00003186
Iteration 113/1000 | Loss: 0.00003186
Iteration 114/1000 | Loss: 0.00003186
Iteration 115/1000 | Loss: 0.00003186
Iteration 116/1000 | Loss: 0.00003186
Iteration 117/1000 | Loss: 0.00003186
Iteration 118/1000 | Loss: 0.00003186
Iteration 119/1000 | Loss: 0.00003186
Iteration 120/1000 | Loss: 0.00003186
Iteration 121/1000 | Loss: 0.00003185
Iteration 122/1000 | Loss: 0.00003185
Iteration 123/1000 | Loss: 0.00003185
Iteration 124/1000 | Loss: 0.00003185
Iteration 125/1000 | Loss: 0.00003185
Iteration 126/1000 | Loss: 0.00003185
Iteration 127/1000 | Loss: 0.00003185
Iteration 128/1000 | Loss: 0.00003185
Iteration 129/1000 | Loss: 0.00003185
Iteration 130/1000 | Loss: 0.00003185
Iteration 131/1000 | Loss: 0.00003185
Iteration 132/1000 | Loss: 0.00003185
Iteration 133/1000 | Loss: 0.00003185
Iteration 134/1000 | Loss: 0.00003185
Iteration 135/1000 | Loss: 0.00003185
Iteration 136/1000 | Loss: 0.00003185
Iteration 137/1000 | Loss: 0.00003185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [3.1854633562033996e-05, 3.1854633562033996e-05, 3.1854633562033996e-05, 3.1854633562033996e-05, 3.1854633562033996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1854633562033996e-05

Optimization complete. Final v2v error: 4.9051713943481445 mm

Highest mean error: 5.11340856552124 mm for frame 191

Lowest mean error: 4.650195121765137 mm for frame 103

Saving results

Total time: 34.41763257980347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0306/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0306/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441788
Iteration 2/25 | Loss: 0.00146983
Iteration 3/25 | Loss: 0.00134158
Iteration 4/25 | Loss: 0.00132676
Iteration 5/25 | Loss: 0.00132296
Iteration 6/25 | Loss: 0.00132130
Iteration 7/25 | Loss: 0.00132093
Iteration 8/25 | Loss: 0.00132093
Iteration 9/25 | Loss: 0.00132093
Iteration 10/25 | Loss: 0.00132093
Iteration 11/25 | Loss: 0.00132093
Iteration 12/25 | Loss: 0.00132093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013209270546212792, 0.0013209270546212792, 0.0013209270546212792, 0.0013209270546212792, 0.0013209270546212792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013209270546212792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40196991
Iteration 2/25 | Loss: 0.00100275
Iteration 3/25 | Loss: 0.00100274
Iteration 4/25 | Loss: 0.00100274
Iteration 5/25 | Loss: 0.00100274
Iteration 6/25 | Loss: 0.00100274
Iteration 7/25 | Loss: 0.00100274
Iteration 8/25 | Loss: 0.00100274
Iteration 9/25 | Loss: 0.00100274
Iteration 10/25 | Loss: 0.00100274
Iteration 11/25 | Loss: 0.00100274
Iteration 12/25 | Loss: 0.00100274
Iteration 13/25 | Loss: 0.00100274
Iteration 14/25 | Loss: 0.00100274
Iteration 15/25 | Loss: 0.00100274
Iteration 16/25 | Loss: 0.00100274
Iteration 17/25 | Loss: 0.00100274
Iteration 18/25 | Loss: 0.00100274
Iteration 19/25 | Loss: 0.00100274
Iteration 20/25 | Loss: 0.00100274
Iteration 21/25 | Loss: 0.00100274
Iteration 22/25 | Loss: 0.00100274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010027405805885792, 0.0010027405805885792, 0.0010027405805885792, 0.0010027405805885792, 0.0010027405805885792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010027405805885792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100274
Iteration 2/1000 | Loss: 0.00004469
Iteration 3/1000 | Loss: 0.00003072
Iteration 4/1000 | Loss: 0.00002732
Iteration 5/1000 | Loss: 0.00002518
Iteration 6/1000 | Loss: 0.00002412
Iteration 7/1000 | Loss: 0.00002365
Iteration 8/1000 | Loss: 0.00002343
Iteration 9/1000 | Loss: 0.00002343
Iteration 10/1000 | Loss: 0.00002340
Iteration 11/1000 | Loss: 0.00002339
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002336
Iteration 14/1000 | Loss: 0.00002336
Iteration 15/1000 | Loss: 0.00002334
Iteration 16/1000 | Loss: 0.00002315
Iteration 17/1000 | Loss: 0.00002312
Iteration 18/1000 | Loss: 0.00002308
Iteration 19/1000 | Loss: 0.00002301
Iteration 20/1000 | Loss: 0.00002298
Iteration 21/1000 | Loss: 0.00002296
Iteration 22/1000 | Loss: 0.00002295
Iteration 23/1000 | Loss: 0.00002295
Iteration 24/1000 | Loss: 0.00002295
Iteration 25/1000 | Loss: 0.00002295
Iteration 26/1000 | Loss: 0.00002295
Iteration 27/1000 | Loss: 0.00002295
Iteration 28/1000 | Loss: 0.00002295
Iteration 29/1000 | Loss: 0.00002294
Iteration 30/1000 | Loss: 0.00002294
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002293
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002287
Iteration 38/1000 | Loss: 0.00002285
Iteration 39/1000 | Loss: 0.00002284
Iteration 40/1000 | Loss: 0.00002284
Iteration 41/1000 | Loss: 0.00002284
Iteration 42/1000 | Loss: 0.00002282
Iteration 43/1000 | Loss: 0.00002282
Iteration 44/1000 | Loss: 0.00002282
Iteration 45/1000 | Loss: 0.00002281
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002281
Iteration 48/1000 | Loss: 0.00002281
Iteration 49/1000 | Loss: 0.00002280
Iteration 50/1000 | Loss: 0.00002280
Iteration 51/1000 | Loss: 0.00002279
Iteration 52/1000 | Loss: 0.00002279
Iteration 53/1000 | Loss: 0.00002279
Iteration 54/1000 | Loss: 0.00002278
Iteration 55/1000 | Loss: 0.00002278
Iteration 56/1000 | Loss: 0.00002278
Iteration 57/1000 | Loss: 0.00002277
Iteration 58/1000 | Loss: 0.00002277
Iteration 59/1000 | Loss: 0.00002277
Iteration 60/1000 | Loss: 0.00002277
Iteration 61/1000 | Loss: 0.00002277
Iteration 62/1000 | Loss: 0.00002276
Iteration 63/1000 | Loss: 0.00002276
Iteration 64/1000 | Loss: 0.00002276
Iteration 65/1000 | Loss: 0.00002275
Iteration 66/1000 | Loss: 0.00002275
Iteration 67/1000 | Loss: 0.00002274
Iteration 68/1000 | Loss: 0.00002274
Iteration 69/1000 | Loss: 0.00002274
Iteration 70/1000 | Loss: 0.00002274
Iteration 71/1000 | Loss: 0.00002274
Iteration 72/1000 | Loss: 0.00002274
Iteration 73/1000 | Loss: 0.00002273
Iteration 74/1000 | Loss: 0.00002273
Iteration 75/1000 | Loss: 0.00002273
Iteration 76/1000 | Loss: 0.00002273
Iteration 77/1000 | Loss: 0.00002273
Iteration 78/1000 | Loss: 0.00002273
Iteration 79/1000 | Loss: 0.00002273
Iteration 80/1000 | Loss: 0.00002273
Iteration 81/1000 | Loss: 0.00002273
Iteration 82/1000 | Loss: 0.00002272
Iteration 83/1000 | Loss: 0.00002272
Iteration 84/1000 | Loss: 0.00002272
Iteration 85/1000 | Loss: 0.00002272
Iteration 86/1000 | Loss: 0.00002271
Iteration 87/1000 | Loss: 0.00002271
Iteration 88/1000 | Loss: 0.00002271
Iteration 89/1000 | Loss: 0.00002270
Iteration 90/1000 | Loss: 0.00002270
Iteration 91/1000 | Loss: 0.00002270
Iteration 92/1000 | Loss: 0.00002270
Iteration 93/1000 | Loss: 0.00002270
Iteration 94/1000 | Loss: 0.00002270
Iteration 95/1000 | Loss: 0.00002270
Iteration 96/1000 | Loss: 0.00002270
Iteration 97/1000 | Loss: 0.00002270
Iteration 98/1000 | Loss: 0.00002270
Iteration 99/1000 | Loss: 0.00002270
Iteration 100/1000 | Loss: 0.00002270
Iteration 101/1000 | Loss: 0.00002270
Iteration 102/1000 | Loss: 0.00002269
Iteration 103/1000 | Loss: 0.00002269
Iteration 104/1000 | Loss: 0.00002269
Iteration 105/1000 | Loss: 0.00002269
Iteration 106/1000 | Loss: 0.00002268
Iteration 107/1000 | Loss: 0.00002268
Iteration 108/1000 | Loss: 0.00002268
Iteration 109/1000 | Loss: 0.00002268
Iteration 110/1000 | Loss: 0.00002268
Iteration 111/1000 | Loss: 0.00002268
Iteration 112/1000 | Loss: 0.00002268
Iteration 113/1000 | Loss: 0.00002268
Iteration 114/1000 | Loss: 0.00002267
Iteration 115/1000 | Loss: 0.00002267
Iteration 116/1000 | Loss: 0.00002267
Iteration 117/1000 | Loss: 0.00002267
Iteration 118/1000 | Loss: 0.00002267
Iteration 119/1000 | Loss: 0.00002267
Iteration 120/1000 | Loss: 0.00002267
Iteration 121/1000 | Loss: 0.00002267
Iteration 122/1000 | Loss: 0.00002267
Iteration 123/1000 | Loss: 0.00002267
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002266
Iteration 126/1000 | Loss: 0.00002266
Iteration 127/1000 | Loss: 0.00002266
Iteration 128/1000 | Loss: 0.00002266
Iteration 129/1000 | Loss: 0.00002266
Iteration 130/1000 | Loss: 0.00002266
Iteration 131/1000 | Loss: 0.00002266
Iteration 132/1000 | Loss: 0.00002266
Iteration 133/1000 | Loss: 0.00002266
Iteration 134/1000 | Loss: 0.00002266
Iteration 135/1000 | Loss: 0.00002266
Iteration 136/1000 | Loss: 0.00002266
Iteration 137/1000 | Loss: 0.00002266
Iteration 138/1000 | Loss: 0.00002266
Iteration 139/1000 | Loss: 0.00002266
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00002265
Iteration 142/1000 | Loss: 0.00002265
Iteration 143/1000 | Loss: 0.00002265
Iteration 144/1000 | Loss: 0.00002265
Iteration 145/1000 | Loss: 0.00002265
Iteration 146/1000 | Loss: 0.00002265
Iteration 147/1000 | Loss: 0.00002265
Iteration 148/1000 | Loss: 0.00002265
Iteration 149/1000 | Loss: 0.00002265
Iteration 150/1000 | Loss: 0.00002265
Iteration 151/1000 | Loss: 0.00002265
Iteration 152/1000 | Loss: 0.00002265
Iteration 153/1000 | Loss: 0.00002265
Iteration 154/1000 | Loss: 0.00002265
Iteration 155/1000 | Loss: 0.00002265
Iteration 156/1000 | Loss: 0.00002265
Iteration 157/1000 | Loss: 0.00002265
Iteration 158/1000 | Loss: 0.00002265
Iteration 159/1000 | Loss: 0.00002265
Iteration 160/1000 | Loss: 0.00002265
Iteration 161/1000 | Loss: 0.00002265
Iteration 162/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.2651358449365944e-05, 2.2651358449365944e-05, 2.2651358449365944e-05, 2.2651358449365944e-05, 2.2651358449365944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2651358449365944e-05

Optimization complete. Final v2v error: 4.198763370513916 mm

Highest mean error: 4.633072853088379 mm for frame 72

Lowest mean error: 3.9748306274414062 mm for frame 105

Saving results

Total time: 37.10209083557129
