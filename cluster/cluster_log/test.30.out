Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=30, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1680-1735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821884
Iteration 2/25 | Loss: 0.00123243
Iteration 3/25 | Loss: 0.00099492
Iteration 4/25 | Loss: 0.00096840
Iteration 5/25 | Loss: 0.00096560
Iteration 6/25 | Loss: 0.00096513
Iteration 7/25 | Loss: 0.00096513
Iteration 8/25 | Loss: 0.00096513
Iteration 9/25 | Loss: 0.00096513
Iteration 10/25 | Loss: 0.00096513
Iteration 11/25 | Loss: 0.00096513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009651337750256062, 0.0009651337750256062, 0.0009651337750256062, 0.0009651337750256062, 0.0009651337750256062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009651337750256062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31471336
Iteration 2/25 | Loss: 0.00049007
Iteration 3/25 | Loss: 0.00049007
Iteration 4/25 | Loss: 0.00049007
Iteration 5/25 | Loss: 0.00049007
Iteration 6/25 | Loss: 0.00049007
Iteration 7/25 | Loss: 0.00049007
Iteration 8/25 | Loss: 0.00049007
Iteration 9/25 | Loss: 0.00049007
Iteration 10/25 | Loss: 0.00049007
Iteration 11/25 | Loss: 0.00049007
Iteration 12/25 | Loss: 0.00049007
Iteration 13/25 | Loss: 0.00049007
Iteration 14/25 | Loss: 0.00049007
Iteration 15/25 | Loss: 0.00049007
Iteration 16/25 | Loss: 0.00049007
Iteration 17/25 | Loss: 0.00049007
Iteration 18/25 | Loss: 0.00049007
Iteration 19/25 | Loss: 0.00049007
Iteration 20/25 | Loss: 0.00049007
Iteration 21/25 | Loss: 0.00049007
Iteration 22/25 | Loss: 0.00049007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00049007119378075, 0.00049007119378075, 0.00049007119378075, 0.00049007119378075, 0.00049007119378075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00049007119378075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049007
Iteration 2/1000 | Loss: 0.00002273
Iteration 3/1000 | Loss: 0.00001503
Iteration 4/1000 | Loss: 0.00001319
Iteration 5/1000 | Loss: 0.00001248
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001161
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001127
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001125
Iteration 15/1000 | Loss: 0.00001125
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001120
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001117
Iteration 29/1000 | Loss: 0.00001117
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00001116
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001116
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001111
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001109
Iteration 42/1000 | Loss: 0.00001109
Iteration 43/1000 | Loss: 0.00001108
Iteration 44/1000 | Loss: 0.00001108
Iteration 45/1000 | Loss: 0.00001108
Iteration 46/1000 | Loss: 0.00001108
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001107
Iteration 50/1000 | Loss: 0.00001106
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001105
Iteration 54/1000 | Loss: 0.00001105
Iteration 55/1000 | Loss: 0.00001105
Iteration 56/1000 | Loss: 0.00001104
Iteration 57/1000 | Loss: 0.00001104
Iteration 58/1000 | Loss: 0.00001104
Iteration 59/1000 | Loss: 0.00001104
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001103
Iteration 63/1000 | Loss: 0.00001103
Iteration 64/1000 | Loss: 0.00001103
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001100
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001100
Iteration 89/1000 | Loss: 0.00001100
Iteration 90/1000 | Loss: 0.00001100
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001100
Iteration 94/1000 | Loss: 0.00001100
Iteration 95/1000 | Loss: 0.00001100
Iteration 96/1000 | Loss: 0.00001100
Iteration 97/1000 | Loss: 0.00001100
Iteration 98/1000 | Loss: 0.00001100
Iteration 99/1000 | Loss: 0.00001100
Iteration 100/1000 | Loss: 0.00001100
Iteration 101/1000 | Loss: 0.00001100
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001100
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001100
Iteration 107/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.0996357559633907e-05, 1.0996357559633907e-05, 1.0996357559633907e-05, 1.0996357559633907e-05, 1.0996357559633907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0996357559633907e-05

Optimization complete. Final v2v error: 2.7844834327697754 mm

Highest mean error: 3.2111783027648926 mm for frame 234

Lowest mean error: 2.2596428394317627 mm for frame 80

Saving results

Total time: 34.50463914871216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794222
Iteration 2/25 | Loss: 0.00118869
Iteration 3/25 | Loss: 0.00099503
Iteration 4/25 | Loss: 0.00097072
Iteration 5/25 | Loss: 0.00096627
Iteration 6/25 | Loss: 0.00096581
Iteration 7/25 | Loss: 0.00096581
Iteration 8/25 | Loss: 0.00096581
Iteration 9/25 | Loss: 0.00096581
Iteration 10/25 | Loss: 0.00096581
Iteration 11/25 | Loss: 0.00096581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009658101480454206, 0.0009658101480454206, 0.0009658101480454206, 0.0009658101480454206, 0.0009658101480454206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009658101480454206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30947924
Iteration 2/25 | Loss: 0.00061704
Iteration 3/25 | Loss: 0.00061702
Iteration 4/25 | Loss: 0.00061701
Iteration 5/25 | Loss: 0.00061701
Iteration 6/25 | Loss: 0.00061701
Iteration 7/25 | Loss: 0.00061701
Iteration 8/25 | Loss: 0.00061701
Iteration 9/25 | Loss: 0.00061701
Iteration 10/25 | Loss: 0.00061701
Iteration 11/25 | Loss: 0.00061701
Iteration 12/25 | Loss: 0.00061701
Iteration 13/25 | Loss: 0.00061701
Iteration 14/25 | Loss: 0.00061701
Iteration 15/25 | Loss: 0.00061701
Iteration 16/25 | Loss: 0.00061701
Iteration 17/25 | Loss: 0.00061701
Iteration 18/25 | Loss: 0.00061701
Iteration 19/25 | Loss: 0.00061701
Iteration 20/25 | Loss: 0.00061701
Iteration 21/25 | Loss: 0.00061701
Iteration 22/25 | Loss: 0.00061701
Iteration 23/25 | Loss: 0.00061701
Iteration 24/25 | Loss: 0.00061701
Iteration 25/25 | Loss: 0.00061701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061701
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001676
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001385
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001281
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001213
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001200
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001198
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001196
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001194
Iteration 45/1000 | Loss: 0.00001194
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001192
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001191
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001190
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001189
Iteration 87/1000 | Loss: 0.00001189
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001189
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.1893693226738833e-05, 1.1893693226738833e-05, 1.1893693226738833e-05, 1.1893693226738833e-05, 1.1893693226738833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1893693226738833e-05

Optimization complete. Final v2v error: 2.931061267852783 mm

Highest mean error: 3.3027255535125732 mm for frame 195

Lowest mean error: 2.5505545139312744 mm for frame 57

Saving results

Total time: 34.32449126243591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477433
Iteration 2/25 | Loss: 0.00106068
Iteration 3/25 | Loss: 0.00093604
Iteration 4/25 | Loss: 0.00091892
Iteration 5/25 | Loss: 0.00091332
Iteration 6/25 | Loss: 0.00091139
Iteration 7/25 | Loss: 0.00091139
Iteration 8/25 | Loss: 0.00091139
Iteration 9/25 | Loss: 0.00091139
Iteration 10/25 | Loss: 0.00091139
Iteration 11/25 | Loss: 0.00091139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009113869164139032, 0.0009113869164139032, 0.0009113869164139032, 0.0009113869164139032, 0.0009113869164139032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009113869164139032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45311749
Iteration 2/25 | Loss: 0.00057722
Iteration 3/25 | Loss: 0.00057722
Iteration 4/25 | Loss: 0.00057722
Iteration 5/25 | Loss: 0.00057722
Iteration 6/25 | Loss: 0.00057722
Iteration 7/25 | Loss: 0.00057722
Iteration 8/25 | Loss: 0.00057722
Iteration 9/25 | Loss: 0.00057722
Iteration 10/25 | Loss: 0.00057722
Iteration 11/25 | Loss: 0.00057722
Iteration 12/25 | Loss: 0.00057722
Iteration 13/25 | Loss: 0.00057722
Iteration 14/25 | Loss: 0.00057722
Iteration 15/25 | Loss: 0.00057722
Iteration 16/25 | Loss: 0.00057722
Iteration 17/25 | Loss: 0.00057722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005772150470875204, 0.0005772150470875204, 0.0005772150470875204, 0.0005772150470875204, 0.0005772150470875204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005772150470875204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057721
Iteration 2/1000 | Loss: 0.00001542
Iteration 3/1000 | Loss: 0.00001170
Iteration 4/1000 | Loss: 0.00001091
Iteration 5/1000 | Loss: 0.00001019
Iteration 6/1000 | Loss: 0.00000987
Iteration 7/1000 | Loss: 0.00000967
Iteration 8/1000 | Loss: 0.00000957
Iteration 9/1000 | Loss: 0.00000957
Iteration 10/1000 | Loss: 0.00000955
Iteration 11/1000 | Loss: 0.00000954
Iteration 12/1000 | Loss: 0.00000952
Iteration 13/1000 | Loss: 0.00000951
Iteration 14/1000 | Loss: 0.00000948
Iteration 15/1000 | Loss: 0.00000947
Iteration 16/1000 | Loss: 0.00000946
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000945
Iteration 19/1000 | Loss: 0.00000945
Iteration 20/1000 | Loss: 0.00000944
Iteration 21/1000 | Loss: 0.00000944
Iteration 22/1000 | Loss: 0.00000943
Iteration 23/1000 | Loss: 0.00000943
Iteration 24/1000 | Loss: 0.00000942
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000940
Iteration 27/1000 | Loss: 0.00000939
Iteration 28/1000 | Loss: 0.00000939
Iteration 29/1000 | Loss: 0.00000939
Iteration 30/1000 | Loss: 0.00000939
Iteration 31/1000 | Loss: 0.00000938
Iteration 32/1000 | Loss: 0.00000938
Iteration 33/1000 | Loss: 0.00000938
Iteration 34/1000 | Loss: 0.00000938
Iteration 35/1000 | Loss: 0.00000938
Iteration 36/1000 | Loss: 0.00000938
Iteration 37/1000 | Loss: 0.00000938
Iteration 38/1000 | Loss: 0.00000938
Iteration 39/1000 | Loss: 0.00000938
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [9.381867130286992e-06, 9.381867130286992e-06, 9.381867130286992e-06, 9.381867130286992e-06, 9.381867130286992e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.381867130286992e-06

Optimization complete. Final v2v error: 2.605971097946167 mm

Highest mean error: 3.1245272159576416 mm for frame 83

Lowest mean error: 2.34318208694458 mm for frame 12

Saving results

Total time: 21.919697046279907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883489
Iteration 2/25 | Loss: 0.00189720
Iteration 3/25 | Loss: 0.00151596
Iteration 4/25 | Loss: 0.00206450
Iteration 5/25 | Loss: 0.00131234
Iteration 6/25 | Loss: 0.00124923
Iteration 7/25 | Loss: 0.00126936
Iteration 8/25 | Loss: 0.00126045
Iteration 9/25 | Loss: 0.00124058
Iteration 10/25 | Loss: 0.00122387
Iteration 11/25 | Loss: 0.00122108
Iteration 12/25 | Loss: 0.00122065
Iteration 13/25 | Loss: 0.00122057
Iteration 14/25 | Loss: 0.00122057
Iteration 15/25 | Loss: 0.00122057
Iteration 16/25 | Loss: 0.00122057
Iteration 17/25 | Loss: 0.00122057
Iteration 18/25 | Loss: 0.00122057
Iteration 19/25 | Loss: 0.00122057
Iteration 20/25 | Loss: 0.00122057
Iteration 21/25 | Loss: 0.00122057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012205673847347498, 0.0012205673847347498, 0.0012205673847347498, 0.0012205673847347498, 0.0012205673847347498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012205673847347498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05265498
Iteration 2/25 | Loss: 0.00070517
Iteration 3/25 | Loss: 0.00070517
Iteration 4/25 | Loss: 0.00070517
Iteration 5/25 | Loss: 0.00070517
Iteration 6/25 | Loss: 0.00070516
Iteration 7/25 | Loss: 0.00070516
Iteration 8/25 | Loss: 0.00070516
Iteration 9/25 | Loss: 0.00070516
Iteration 10/25 | Loss: 0.00070516
Iteration 11/25 | Loss: 0.00070516
Iteration 12/25 | Loss: 0.00070516
Iteration 13/25 | Loss: 0.00070516
Iteration 14/25 | Loss: 0.00070516
Iteration 15/25 | Loss: 0.00070516
Iteration 16/25 | Loss: 0.00070516
Iteration 17/25 | Loss: 0.00070516
Iteration 18/25 | Loss: 0.00070516
Iteration 19/25 | Loss: 0.00070516
Iteration 20/25 | Loss: 0.00070516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007051636930555105, 0.0007051636930555105, 0.0007051636930555105, 0.0007051636930555105, 0.0007051636930555105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007051636930555105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070516
Iteration 2/1000 | Loss: 0.00005810
Iteration 3/1000 | Loss: 0.00004523
Iteration 4/1000 | Loss: 0.00004170
Iteration 5/1000 | Loss: 0.00004003
Iteration 6/1000 | Loss: 0.00003922
Iteration 7/1000 | Loss: 0.00003861
Iteration 8/1000 | Loss: 0.00003820
Iteration 9/1000 | Loss: 0.00003792
Iteration 10/1000 | Loss: 0.00003775
Iteration 11/1000 | Loss: 0.00003748
Iteration 12/1000 | Loss: 0.00003731
Iteration 13/1000 | Loss: 0.00003709
Iteration 14/1000 | Loss: 0.00003709
Iteration 15/1000 | Loss: 0.00003709
Iteration 16/1000 | Loss: 0.00003709
Iteration 17/1000 | Loss: 0.00003709
Iteration 18/1000 | Loss: 0.00003708
Iteration 19/1000 | Loss: 0.00003706
Iteration 20/1000 | Loss: 0.00003703
Iteration 21/1000 | Loss: 0.00003700
Iteration 22/1000 | Loss: 0.00003700
Iteration 23/1000 | Loss: 0.00003700
Iteration 24/1000 | Loss: 0.00003699
Iteration 25/1000 | Loss: 0.00003698
Iteration 26/1000 | Loss: 0.00003696
Iteration 27/1000 | Loss: 0.00003692
Iteration 28/1000 | Loss: 0.00003691
Iteration 29/1000 | Loss: 0.00003691
Iteration 30/1000 | Loss: 0.00003690
Iteration 31/1000 | Loss: 0.00003690
Iteration 32/1000 | Loss: 0.00003689
Iteration 33/1000 | Loss: 0.00003683
Iteration 34/1000 | Loss: 0.00003683
Iteration 35/1000 | Loss: 0.00003682
Iteration 36/1000 | Loss: 0.00003682
Iteration 37/1000 | Loss: 0.00003682
Iteration 38/1000 | Loss: 0.00003682
Iteration 39/1000 | Loss: 0.00003682
Iteration 40/1000 | Loss: 0.00003682
Iteration 41/1000 | Loss: 0.00003681
Iteration 42/1000 | Loss: 0.00003681
Iteration 43/1000 | Loss: 0.00003680
Iteration 44/1000 | Loss: 0.00003680
Iteration 45/1000 | Loss: 0.00003678
Iteration 46/1000 | Loss: 0.00003678
Iteration 47/1000 | Loss: 0.00003678
Iteration 48/1000 | Loss: 0.00003677
Iteration 49/1000 | Loss: 0.00003677
Iteration 50/1000 | Loss: 0.00003677
Iteration 51/1000 | Loss: 0.00003677
Iteration 52/1000 | Loss: 0.00003676
Iteration 53/1000 | Loss: 0.00003676
Iteration 54/1000 | Loss: 0.00003676
Iteration 55/1000 | Loss: 0.00003676
Iteration 56/1000 | Loss: 0.00003676
Iteration 57/1000 | Loss: 0.00003676
Iteration 58/1000 | Loss: 0.00003676
Iteration 59/1000 | Loss: 0.00003676
Iteration 60/1000 | Loss: 0.00003676
Iteration 61/1000 | Loss: 0.00003676
Iteration 62/1000 | Loss: 0.00003676
Iteration 63/1000 | Loss: 0.00003676
Iteration 64/1000 | Loss: 0.00003676
Iteration 65/1000 | Loss: 0.00003676
Iteration 66/1000 | Loss: 0.00003676
Iteration 67/1000 | Loss: 0.00003676
Iteration 68/1000 | Loss: 0.00003676
Iteration 69/1000 | Loss: 0.00003676
Iteration 70/1000 | Loss: 0.00003676
Iteration 71/1000 | Loss: 0.00003676
Iteration 72/1000 | Loss: 0.00003676
Iteration 73/1000 | Loss: 0.00003676
Iteration 74/1000 | Loss: 0.00003676
Iteration 75/1000 | Loss: 0.00003676
Iteration 76/1000 | Loss: 0.00003676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [3.675516200019047e-05, 3.675516200019047e-05, 3.675516200019047e-05, 3.675516200019047e-05, 3.675516200019047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.675516200019047e-05

Optimization complete. Final v2v error: 4.965803623199463 mm

Highest mean error: 5.091558456420898 mm for frame 26

Lowest mean error: 4.903213977813721 mm for frame 54

Saving results

Total time: 44.39621186256409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084182
Iteration 2/25 | Loss: 0.00143090
Iteration 3/25 | Loss: 0.00131788
Iteration 4/25 | Loss: 0.00099661
Iteration 5/25 | Loss: 0.00097919
Iteration 6/25 | Loss: 0.00098023
Iteration 7/25 | Loss: 0.00100312
Iteration 8/25 | Loss: 0.00099045
Iteration 9/25 | Loss: 0.00098384
Iteration 10/25 | Loss: 0.00097937
Iteration 11/25 | Loss: 0.00097953
Iteration 12/25 | Loss: 0.00097084
Iteration 13/25 | Loss: 0.00096282
Iteration 14/25 | Loss: 0.00096019
Iteration 15/25 | Loss: 0.00095742
Iteration 16/25 | Loss: 0.00095851
Iteration 17/25 | Loss: 0.00095699
Iteration 18/25 | Loss: 0.00095565
Iteration 19/25 | Loss: 0.00096296
Iteration 20/25 | Loss: 0.00095543
Iteration 21/25 | Loss: 0.00095359
Iteration 22/25 | Loss: 0.00095394
Iteration 23/25 | Loss: 0.00096000
Iteration 24/25 | Loss: 0.00095809
Iteration 25/25 | Loss: 0.00095954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38325214
Iteration 2/25 | Loss: 0.00062423
Iteration 3/25 | Loss: 0.00062423
Iteration 4/25 | Loss: 0.00062423
Iteration 5/25 | Loss: 0.00062422
Iteration 6/25 | Loss: 0.00062422
Iteration 7/25 | Loss: 0.00062422
Iteration 8/25 | Loss: 0.00062422
Iteration 9/25 | Loss: 0.00062422
Iteration 10/25 | Loss: 0.00062422
Iteration 11/25 | Loss: 0.00062422
Iteration 12/25 | Loss: 0.00062422
Iteration 13/25 | Loss: 0.00062422
Iteration 14/25 | Loss: 0.00062422
Iteration 15/25 | Loss: 0.00062422
Iteration 16/25 | Loss: 0.00062422
Iteration 17/25 | Loss: 0.00062422
Iteration 18/25 | Loss: 0.00062422
Iteration 19/25 | Loss: 0.00062422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006242225645110011, 0.0006242225645110011, 0.0006242225645110011, 0.0006242225645110011, 0.0006242225645110011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006242225645110011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062422
Iteration 2/1000 | Loss: 0.00005564
Iteration 3/1000 | Loss: 0.00004356
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00003485
Iteration 6/1000 | Loss: 0.00002540
Iteration 7/1000 | Loss: 0.00004060
Iteration 8/1000 | Loss: 0.00004254
Iteration 9/1000 | Loss: 0.00002746
Iteration 10/1000 | Loss: 0.00004276
Iteration 11/1000 | Loss: 0.00004184
Iteration 12/1000 | Loss: 0.00004280
Iteration 13/1000 | Loss: 0.00038243
Iteration 14/1000 | Loss: 0.00020339
Iteration 15/1000 | Loss: 0.00009693
Iteration 16/1000 | Loss: 0.00036417
Iteration 17/1000 | Loss: 0.00020712
Iteration 18/1000 | Loss: 0.00004571
Iteration 19/1000 | Loss: 0.00004257
Iteration 20/1000 | Loss: 0.00004159
Iteration 21/1000 | Loss: 0.00036775
Iteration 22/1000 | Loss: 0.00006914
Iteration 23/1000 | Loss: 0.00003683
Iteration 24/1000 | Loss: 0.00006753
Iteration 25/1000 | Loss: 0.00003982
Iteration 26/1000 | Loss: 0.00005607
Iteration 27/1000 | Loss: 0.00003898
Iteration 28/1000 | Loss: 0.00005606
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001072
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001036
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001030
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001029
Iteration 41/1000 | Loss: 0.00001029
Iteration 42/1000 | Loss: 0.00001021
Iteration 43/1000 | Loss: 0.00001016
Iteration 44/1000 | Loss: 0.00001015
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001014
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001013
Iteration 49/1000 | Loss: 0.00001013
Iteration 50/1000 | Loss: 0.00001013
Iteration 51/1000 | Loss: 0.00001013
Iteration 52/1000 | Loss: 0.00001012
Iteration 53/1000 | Loss: 0.00001011
Iteration 54/1000 | Loss: 0.00001010
Iteration 55/1000 | Loss: 0.00001010
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001009
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001008
Iteration 60/1000 | Loss: 0.00001008
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001007
Iteration 63/1000 | Loss: 0.00001006
Iteration 64/1000 | Loss: 0.00001006
Iteration 65/1000 | Loss: 0.00001006
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001004
Iteration 70/1000 | Loss: 0.00001004
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001003
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001002
Iteration 77/1000 | Loss: 0.00001002
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001000
Iteration 83/1000 | Loss: 0.00001000
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00000999
Iteration 86/1000 | Loss: 0.00000999
Iteration 87/1000 | Loss: 0.00000999
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000998
Iteration 90/1000 | Loss: 0.00000998
Iteration 91/1000 | Loss: 0.00000998
Iteration 92/1000 | Loss: 0.00000997
Iteration 93/1000 | Loss: 0.00000997
Iteration 94/1000 | Loss: 0.00000997
Iteration 95/1000 | Loss: 0.00000997
Iteration 96/1000 | Loss: 0.00000997
Iteration 97/1000 | Loss: 0.00000997
Iteration 98/1000 | Loss: 0.00000996
Iteration 99/1000 | Loss: 0.00000996
Iteration 100/1000 | Loss: 0.00000996
Iteration 101/1000 | Loss: 0.00000996
Iteration 102/1000 | Loss: 0.00000996
Iteration 103/1000 | Loss: 0.00000995
Iteration 104/1000 | Loss: 0.00000995
Iteration 105/1000 | Loss: 0.00000995
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000995
Iteration 108/1000 | Loss: 0.00000995
Iteration 109/1000 | Loss: 0.00000995
Iteration 110/1000 | Loss: 0.00000995
Iteration 111/1000 | Loss: 0.00000994
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000994
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000993
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000992
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000992
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000992
Iteration 131/1000 | Loss: 0.00000992
Iteration 132/1000 | Loss: 0.00000992
Iteration 133/1000 | Loss: 0.00000992
Iteration 134/1000 | Loss: 0.00000992
Iteration 135/1000 | Loss: 0.00000992
Iteration 136/1000 | Loss: 0.00000992
Iteration 137/1000 | Loss: 0.00000992
Iteration 138/1000 | Loss: 0.00000992
Iteration 139/1000 | Loss: 0.00000991
Iteration 140/1000 | Loss: 0.00000991
Iteration 141/1000 | Loss: 0.00000991
Iteration 142/1000 | Loss: 0.00000991
Iteration 143/1000 | Loss: 0.00000991
Iteration 144/1000 | Loss: 0.00000991
Iteration 145/1000 | Loss: 0.00000991
Iteration 146/1000 | Loss: 0.00000991
Iteration 147/1000 | Loss: 0.00000991
Iteration 148/1000 | Loss: 0.00000991
Iteration 149/1000 | Loss: 0.00000991
Iteration 150/1000 | Loss: 0.00000991
Iteration 151/1000 | Loss: 0.00000991
Iteration 152/1000 | Loss: 0.00000991
Iteration 153/1000 | Loss: 0.00000991
Iteration 154/1000 | Loss: 0.00000991
Iteration 155/1000 | Loss: 0.00000990
Iteration 156/1000 | Loss: 0.00000990
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000990
Iteration 163/1000 | Loss: 0.00000990
Iteration 164/1000 | Loss: 0.00000990
Iteration 165/1000 | Loss: 0.00000990
Iteration 166/1000 | Loss: 0.00000990
Iteration 167/1000 | Loss: 0.00000990
Iteration 168/1000 | Loss: 0.00000990
Iteration 169/1000 | Loss: 0.00000990
Iteration 170/1000 | Loss: 0.00000990
Iteration 171/1000 | Loss: 0.00000990
Iteration 172/1000 | Loss: 0.00000990
Iteration 173/1000 | Loss: 0.00000990
Iteration 174/1000 | Loss: 0.00000990
Iteration 175/1000 | Loss: 0.00000990
Iteration 176/1000 | Loss: 0.00000990
Iteration 177/1000 | Loss: 0.00000990
Iteration 178/1000 | Loss: 0.00000990
Iteration 179/1000 | Loss: 0.00000990
Iteration 180/1000 | Loss: 0.00000990
Iteration 181/1000 | Loss: 0.00000990
Iteration 182/1000 | Loss: 0.00000990
Iteration 183/1000 | Loss: 0.00000990
Iteration 184/1000 | Loss: 0.00000990
Iteration 185/1000 | Loss: 0.00000990
Iteration 186/1000 | Loss: 0.00000990
Iteration 187/1000 | Loss: 0.00000990
Iteration 188/1000 | Loss: 0.00000990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [9.897414201986976e-06, 9.897414201986976e-06, 9.897414201986976e-06, 9.897414201986976e-06, 9.897414201986976e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.897414201986976e-06

Optimization complete. Final v2v error: 2.606752872467041 mm

Highest mean error: 3.420039176940918 mm for frame 80

Lowest mean error: 2.2425355911254883 mm for frame 109

Saving results

Total time: 102.01566863059998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109840
Iteration 2/25 | Loss: 0.00202024
Iteration 3/25 | Loss: 0.00134882
Iteration 4/25 | Loss: 0.00123627
Iteration 5/25 | Loss: 0.00120252
Iteration 6/25 | Loss: 0.00117319
Iteration 7/25 | Loss: 0.00113582
Iteration 8/25 | Loss: 0.00111764
Iteration 9/25 | Loss: 0.00111818
Iteration 10/25 | Loss: 0.00111936
Iteration 11/25 | Loss: 0.00110612
Iteration 12/25 | Loss: 0.00110023
Iteration 13/25 | Loss: 0.00109835
Iteration 14/25 | Loss: 0.00109753
Iteration 15/25 | Loss: 0.00109745
Iteration 16/25 | Loss: 0.00109745
Iteration 17/25 | Loss: 0.00109765
Iteration 18/25 | Loss: 0.00109656
Iteration 19/25 | Loss: 0.00109656
Iteration 20/25 | Loss: 0.00109656
Iteration 21/25 | Loss: 0.00109656
Iteration 22/25 | Loss: 0.00109656
Iteration 23/25 | Loss: 0.00109655
Iteration 24/25 | Loss: 0.00109655
Iteration 25/25 | Loss: 0.00109655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34582376
Iteration 2/25 | Loss: 0.00093918
Iteration 3/25 | Loss: 0.00078130
Iteration 4/25 | Loss: 0.00078130
Iteration 5/25 | Loss: 0.00078130
Iteration 6/25 | Loss: 0.00078130
Iteration 7/25 | Loss: 0.00078130
Iteration 8/25 | Loss: 0.00078129
Iteration 9/25 | Loss: 0.00078129
Iteration 10/25 | Loss: 0.00078129
Iteration 11/25 | Loss: 0.00078129
Iteration 12/25 | Loss: 0.00078129
Iteration 13/25 | Loss: 0.00078129
Iteration 14/25 | Loss: 0.00078129
Iteration 15/25 | Loss: 0.00078129
Iteration 16/25 | Loss: 0.00078129
Iteration 17/25 | Loss: 0.00078129
Iteration 18/25 | Loss: 0.00078129
Iteration 19/25 | Loss: 0.00078129
Iteration 20/25 | Loss: 0.00078129
Iteration 21/25 | Loss: 0.00078129
Iteration 22/25 | Loss: 0.00078129
Iteration 23/25 | Loss: 0.00078129
Iteration 24/25 | Loss: 0.00078129
Iteration 25/25 | Loss: 0.00078129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078129
Iteration 2/1000 | Loss: 0.00017167
Iteration 3/1000 | Loss: 0.00016549
Iteration 4/1000 | Loss: 0.00008814
Iteration 5/1000 | Loss: 0.00011543
Iteration 6/1000 | Loss: 0.00006334
Iteration 7/1000 | Loss: 0.00009059
Iteration 8/1000 | Loss: 0.00003448
Iteration 9/1000 | Loss: 0.00005225
Iteration 10/1000 | Loss: 0.00003322
Iteration 11/1000 | Loss: 0.00006841
Iteration 12/1000 | Loss: 0.00003463
Iteration 13/1000 | Loss: 0.00003286
Iteration 14/1000 | Loss: 0.00003226
Iteration 15/1000 | Loss: 0.00006448
Iteration 16/1000 | Loss: 0.00010449
Iteration 17/1000 | Loss: 0.00005746
Iteration 18/1000 | Loss: 0.00003167
Iteration 19/1000 | Loss: 0.00003150
Iteration 20/1000 | Loss: 0.00003354
Iteration 21/1000 | Loss: 0.00003184
Iteration 22/1000 | Loss: 0.00005267
Iteration 23/1000 | Loss: 0.00066696
Iteration 24/1000 | Loss: 0.00015704
Iteration 25/1000 | Loss: 0.00030618
Iteration 26/1000 | Loss: 0.00003549
Iteration 27/1000 | Loss: 0.00006712
Iteration 28/1000 | Loss: 0.00031586
Iteration 29/1000 | Loss: 0.00004512
Iteration 30/1000 | Loss: 0.00005042
Iteration 31/1000 | Loss: 0.00003418
Iteration 32/1000 | Loss: 0.00004510
Iteration 33/1000 | Loss: 0.00003158
Iteration 34/1000 | Loss: 0.00007581
Iteration 35/1000 | Loss: 0.00005475
Iteration 36/1000 | Loss: 0.00020247
Iteration 37/1000 | Loss: 0.00005637
Iteration 38/1000 | Loss: 0.00003191
Iteration 39/1000 | Loss: 0.00005482
Iteration 40/1000 | Loss: 0.00003189
Iteration 41/1000 | Loss: 0.00003245
Iteration 42/1000 | Loss: 0.00003186
Iteration 43/1000 | Loss: 0.00003216
Iteration 44/1000 | Loss: 0.00003190
Iteration 45/1000 | Loss: 0.00003188
Iteration 46/1000 | Loss: 0.00003172
Iteration 47/1000 | Loss: 0.00003215
Iteration 48/1000 | Loss: 0.00003184
Iteration 49/1000 | Loss: 0.00003219
Iteration 50/1000 | Loss: 0.00003193
Iteration 51/1000 | Loss: 0.00003192
Iteration 52/1000 | Loss: 0.00006369
Iteration 53/1000 | Loss: 0.00003177
Iteration 54/1000 | Loss: 0.00004222
Iteration 55/1000 | Loss: 0.00005654
Iteration 56/1000 | Loss: 0.00003919
Iteration 57/1000 | Loss: 0.00003194
Iteration 58/1000 | Loss: 0.00003211
Iteration 59/1000 | Loss: 0.00003210
Iteration 60/1000 | Loss: 0.00003210
Iteration 61/1000 | Loss: 0.00007736
Iteration 62/1000 | Loss: 0.00010047
Iteration 63/1000 | Loss: 0.00004444
Iteration 64/1000 | Loss: 0.00003111
Iteration 65/1000 | Loss: 0.00003105
Iteration 66/1000 | Loss: 0.00003103
Iteration 67/1000 | Loss: 0.00003103
Iteration 68/1000 | Loss: 0.00003103
Iteration 69/1000 | Loss: 0.00003103
Iteration 70/1000 | Loss: 0.00003102
Iteration 71/1000 | Loss: 0.00003102
Iteration 72/1000 | Loss: 0.00003102
Iteration 73/1000 | Loss: 0.00003102
Iteration 74/1000 | Loss: 0.00003102
Iteration 75/1000 | Loss: 0.00003102
Iteration 76/1000 | Loss: 0.00003102
Iteration 77/1000 | Loss: 0.00003102
Iteration 78/1000 | Loss: 0.00003102
Iteration 79/1000 | Loss: 0.00003101
Iteration 80/1000 | Loss: 0.00003101
Iteration 81/1000 | Loss: 0.00003101
Iteration 82/1000 | Loss: 0.00003101
Iteration 83/1000 | Loss: 0.00003101
Iteration 84/1000 | Loss: 0.00003101
Iteration 85/1000 | Loss: 0.00003101
Iteration 86/1000 | Loss: 0.00003101
Iteration 87/1000 | Loss: 0.00003101
Iteration 88/1000 | Loss: 0.00003101
Iteration 89/1000 | Loss: 0.00003101
Iteration 90/1000 | Loss: 0.00003101
Iteration 91/1000 | Loss: 0.00003101
Iteration 92/1000 | Loss: 0.00003101
Iteration 93/1000 | Loss: 0.00003101
Iteration 94/1000 | Loss: 0.00003101
Iteration 95/1000 | Loss: 0.00003101
Iteration 96/1000 | Loss: 0.00003101
Iteration 97/1000 | Loss: 0.00003101
Iteration 98/1000 | Loss: 0.00003101
Iteration 99/1000 | Loss: 0.00003101
Iteration 100/1000 | Loss: 0.00003101
Iteration 101/1000 | Loss: 0.00003101
Iteration 102/1000 | Loss: 0.00003101
Iteration 103/1000 | Loss: 0.00003101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [3.1006762583274394e-05, 3.1006762583274394e-05, 3.1006762583274394e-05, 3.1006762583274394e-05, 3.1006762583274394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1006762583274394e-05

Optimization complete. Final v2v error: 4.1389546394348145 mm

Highest mean error: 21.402822494506836 mm for frame 7

Lowest mean error: 3.6680753231048584 mm for frame 4

Saving results

Total time: 108.93014883995056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119123
Iteration 2/25 | Loss: 0.01119123
Iteration 3/25 | Loss: 0.00587191
Iteration 4/25 | Loss: 0.00411740
Iteration 5/25 | Loss: 0.00281134
Iteration 6/25 | Loss: 0.00251839
Iteration 7/25 | Loss: 0.00232244
Iteration 8/25 | Loss: 0.00226114
Iteration 9/25 | Loss: 0.00215190
Iteration 10/25 | Loss: 0.00206912
Iteration 11/25 | Loss: 0.00197145
Iteration 12/25 | Loss: 0.00190849
Iteration 13/25 | Loss: 0.00183842
Iteration 14/25 | Loss: 0.00184264
Iteration 15/25 | Loss: 0.00176644
Iteration 16/25 | Loss: 0.00168676
Iteration 17/25 | Loss: 0.00163533
Iteration 18/25 | Loss: 0.00160251
Iteration 19/25 | Loss: 0.00157192
Iteration 20/25 | Loss: 0.00155424
Iteration 21/25 | Loss: 0.00152009
Iteration 22/25 | Loss: 0.00149940
Iteration 23/25 | Loss: 0.00149593
Iteration 24/25 | Loss: 0.00148095
Iteration 25/25 | Loss: 0.00149548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65138751
Iteration 2/25 | Loss: 0.00330157
Iteration 3/25 | Loss: 0.00330157
Iteration 4/25 | Loss: 0.00330157
Iteration 5/25 | Loss: 0.00330157
Iteration 6/25 | Loss: 0.00330157
Iteration 7/25 | Loss: 0.00330157
Iteration 8/25 | Loss: 0.00330157
Iteration 9/25 | Loss: 0.00330157
Iteration 10/25 | Loss: 0.00330157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0033015671651810408, 0.0033015671651810408, 0.0033015671651810408, 0.0033015671651810408, 0.0033015671651810408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033015671651810408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00330157
Iteration 2/1000 | Loss: 0.00343612
Iteration 3/1000 | Loss: 0.00155595
Iteration 4/1000 | Loss: 0.00120714
Iteration 5/1000 | Loss: 0.00272179
Iteration 6/1000 | Loss: 0.00276026
Iteration 7/1000 | Loss: 0.00080156
Iteration 8/1000 | Loss: 0.00055284
Iteration 9/1000 | Loss: 0.00034570
Iteration 10/1000 | Loss: 0.00033107
Iteration 11/1000 | Loss: 0.00073543
Iteration 12/1000 | Loss: 0.00059733
Iteration 13/1000 | Loss: 0.00036131
Iteration 14/1000 | Loss: 0.00217668
Iteration 15/1000 | Loss: 0.00021459
Iteration 16/1000 | Loss: 0.00038568
Iteration 17/1000 | Loss: 0.00027620
Iteration 18/1000 | Loss: 0.00015733
Iteration 19/1000 | Loss: 0.00011770
Iteration 20/1000 | Loss: 0.00051222
Iteration 21/1000 | Loss: 0.00039313
Iteration 22/1000 | Loss: 0.00019129
Iteration 23/1000 | Loss: 0.00032820
Iteration 24/1000 | Loss: 0.00056311
Iteration 25/1000 | Loss: 0.00030026
Iteration 26/1000 | Loss: 0.00014004
Iteration 27/1000 | Loss: 0.00015382
Iteration 28/1000 | Loss: 0.00012599
Iteration 29/1000 | Loss: 0.00012349
Iteration 30/1000 | Loss: 0.00029401
Iteration 31/1000 | Loss: 0.00021222
Iteration 32/1000 | Loss: 0.00009723
Iteration 33/1000 | Loss: 0.00018211
Iteration 34/1000 | Loss: 0.00039930
Iteration 35/1000 | Loss: 0.00047905
Iteration 36/1000 | Loss: 0.00012256
Iteration 37/1000 | Loss: 0.00009676
Iteration 38/1000 | Loss: 0.00013188
Iteration 39/1000 | Loss: 0.00012322
Iteration 40/1000 | Loss: 0.00042023
Iteration 41/1000 | Loss: 0.00029132
Iteration 42/1000 | Loss: 0.00010717
Iteration 43/1000 | Loss: 0.00008334
Iteration 44/1000 | Loss: 0.00033081
Iteration 45/1000 | Loss: 0.00053463
Iteration 46/1000 | Loss: 0.00013731
Iteration 47/1000 | Loss: 0.00009239
Iteration 48/1000 | Loss: 0.00008817
Iteration 49/1000 | Loss: 0.00027515
Iteration 50/1000 | Loss: 0.00039003
Iteration 51/1000 | Loss: 0.00008231
Iteration 52/1000 | Loss: 0.00028406
Iteration 53/1000 | Loss: 0.00016207
Iteration 54/1000 | Loss: 0.00005836
Iteration 55/1000 | Loss: 0.00025593
Iteration 56/1000 | Loss: 0.00015806
Iteration 57/1000 | Loss: 0.00005216
Iteration 58/1000 | Loss: 0.00024755
Iteration 59/1000 | Loss: 0.00006076
Iteration 60/1000 | Loss: 0.00005377
Iteration 61/1000 | Loss: 0.00005006
Iteration 62/1000 | Loss: 0.00004821
Iteration 63/1000 | Loss: 0.00004717
Iteration 64/1000 | Loss: 0.00004631
Iteration 65/1000 | Loss: 0.00004570
Iteration 66/1000 | Loss: 0.00004527
Iteration 67/1000 | Loss: 0.00004501
Iteration 68/1000 | Loss: 0.00004485
Iteration 69/1000 | Loss: 0.00004468
Iteration 70/1000 | Loss: 0.00004454
Iteration 71/1000 | Loss: 0.00004451
Iteration 72/1000 | Loss: 0.00004438
Iteration 73/1000 | Loss: 0.00004437
Iteration 74/1000 | Loss: 0.00004437
Iteration 75/1000 | Loss: 0.00004437
Iteration 76/1000 | Loss: 0.00004437
Iteration 77/1000 | Loss: 0.00004436
Iteration 78/1000 | Loss: 0.00004436
Iteration 79/1000 | Loss: 0.00004436
Iteration 80/1000 | Loss: 0.00004436
Iteration 81/1000 | Loss: 0.00004436
Iteration 82/1000 | Loss: 0.00004435
Iteration 83/1000 | Loss: 0.00004433
Iteration 84/1000 | Loss: 0.00004432
Iteration 85/1000 | Loss: 0.00004431
Iteration 86/1000 | Loss: 0.00004429
Iteration 87/1000 | Loss: 0.00004429
Iteration 88/1000 | Loss: 0.00004428
Iteration 89/1000 | Loss: 0.00004428
Iteration 90/1000 | Loss: 0.00004427
Iteration 91/1000 | Loss: 0.00004427
Iteration 92/1000 | Loss: 0.00004427
Iteration 93/1000 | Loss: 0.00004427
Iteration 94/1000 | Loss: 0.00004426
Iteration 95/1000 | Loss: 0.00004426
Iteration 96/1000 | Loss: 0.00004426
Iteration 97/1000 | Loss: 0.00004426
Iteration 98/1000 | Loss: 0.00004426
Iteration 99/1000 | Loss: 0.00004426
Iteration 100/1000 | Loss: 0.00004425
Iteration 101/1000 | Loss: 0.00004425
Iteration 102/1000 | Loss: 0.00004425
Iteration 103/1000 | Loss: 0.00004425
Iteration 104/1000 | Loss: 0.00004424
Iteration 105/1000 | Loss: 0.00004424
Iteration 106/1000 | Loss: 0.00004423
Iteration 107/1000 | Loss: 0.00004423
Iteration 108/1000 | Loss: 0.00004422
Iteration 109/1000 | Loss: 0.00004421
Iteration 110/1000 | Loss: 0.00004420
Iteration 111/1000 | Loss: 0.00004419
Iteration 112/1000 | Loss: 0.00004418
Iteration 113/1000 | Loss: 0.00004418
Iteration 114/1000 | Loss: 0.00004418
Iteration 115/1000 | Loss: 0.00004418
Iteration 116/1000 | Loss: 0.00004418
Iteration 117/1000 | Loss: 0.00004417
Iteration 118/1000 | Loss: 0.00004417
Iteration 119/1000 | Loss: 0.00004417
Iteration 120/1000 | Loss: 0.00004417
Iteration 121/1000 | Loss: 0.00004417
Iteration 122/1000 | Loss: 0.00004417
Iteration 123/1000 | Loss: 0.00004417
Iteration 124/1000 | Loss: 0.00004417
Iteration 125/1000 | Loss: 0.00004417
Iteration 126/1000 | Loss: 0.00004417
Iteration 127/1000 | Loss: 0.00004417
Iteration 128/1000 | Loss: 0.00004417
Iteration 129/1000 | Loss: 0.00004417
Iteration 130/1000 | Loss: 0.00004417
Iteration 131/1000 | Loss: 0.00004417
Iteration 132/1000 | Loss: 0.00004417
Iteration 133/1000 | Loss: 0.00004417
Iteration 134/1000 | Loss: 0.00004415
Iteration 135/1000 | Loss: 0.00004415
Iteration 136/1000 | Loss: 0.00004415
Iteration 137/1000 | Loss: 0.00004415
Iteration 138/1000 | Loss: 0.00004415
Iteration 139/1000 | Loss: 0.00004414
Iteration 140/1000 | Loss: 0.00004414
Iteration 141/1000 | Loss: 0.00004414
Iteration 142/1000 | Loss: 0.00004414
Iteration 143/1000 | Loss: 0.00004414
Iteration 144/1000 | Loss: 0.00004414
Iteration 145/1000 | Loss: 0.00004414
Iteration 146/1000 | Loss: 0.00004414
Iteration 147/1000 | Loss: 0.00004414
Iteration 148/1000 | Loss: 0.00004414
Iteration 149/1000 | Loss: 0.00004414
Iteration 150/1000 | Loss: 0.00004414
Iteration 151/1000 | Loss: 0.00004413
Iteration 152/1000 | Loss: 0.00004413
Iteration 153/1000 | Loss: 0.00004412
Iteration 154/1000 | Loss: 0.00004412
Iteration 155/1000 | Loss: 0.00004411
Iteration 156/1000 | Loss: 0.00004411
Iteration 157/1000 | Loss: 0.00004411
Iteration 158/1000 | Loss: 0.00004411
Iteration 159/1000 | Loss: 0.00004411
Iteration 160/1000 | Loss: 0.00004411
Iteration 161/1000 | Loss: 0.00004411
Iteration 162/1000 | Loss: 0.00004411
Iteration 163/1000 | Loss: 0.00004411
Iteration 164/1000 | Loss: 0.00004411
Iteration 165/1000 | Loss: 0.00004411
Iteration 166/1000 | Loss: 0.00004411
Iteration 167/1000 | Loss: 0.00004411
Iteration 168/1000 | Loss: 0.00004410
Iteration 169/1000 | Loss: 0.00004410
Iteration 170/1000 | Loss: 0.00004410
Iteration 171/1000 | Loss: 0.00004410
Iteration 172/1000 | Loss: 0.00004409
Iteration 173/1000 | Loss: 0.00004409
Iteration 174/1000 | Loss: 0.00004409
Iteration 175/1000 | Loss: 0.00004409
Iteration 176/1000 | Loss: 0.00004409
Iteration 177/1000 | Loss: 0.00004409
Iteration 178/1000 | Loss: 0.00004408
Iteration 179/1000 | Loss: 0.00004408
Iteration 180/1000 | Loss: 0.00004408
Iteration 181/1000 | Loss: 0.00004408
Iteration 182/1000 | Loss: 0.00004408
Iteration 183/1000 | Loss: 0.00004408
Iteration 184/1000 | Loss: 0.00004408
Iteration 185/1000 | Loss: 0.00004408
Iteration 186/1000 | Loss: 0.00004408
Iteration 187/1000 | Loss: 0.00004408
Iteration 188/1000 | Loss: 0.00004408
Iteration 189/1000 | Loss: 0.00004408
Iteration 190/1000 | Loss: 0.00004408
Iteration 191/1000 | Loss: 0.00004408
Iteration 192/1000 | Loss: 0.00004408
Iteration 193/1000 | Loss: 0.00004408
Iteration 194/1000 | Loss: 0.00004408
Iteration 195/1000 | Loss: 0.00004408
Iteration 196/1000 | Loss: 0.00004408
Iteration 197/1000 | Loss: 0.00004408
Iteration 198/1000 | Loss: 0.00004408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [4.4081763917347416e-05, 4.4081763917347416e-05, 4.4081763917347416e-05, 4.4081763917347416e-05, 4.4081763917347416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4081763917347416e-05

Optimization complete. Final v2v error: 4.431325912475586 mm

Highest mean error: 21.280527114868164 mm for frame 78

Lowest mean error: 3.8102970123291016 mm for frame 68

Saving results

Total time: 172.51526618003845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431505
Iteration 2/25 | Loss: 0.00112183
Iteration 3/25 | Loss: 0.00096376
Iteration 4/25 | Loss: 0.00095220
Iteration 5/25 | Loss: 0.00094924
Iteration 6/25 | Loss: 0.00094902
Iteration 7/25 | Loss: 0.00094902
Iteration 8/25 | Loss: 0.00094902
Iteration 9/25 | Loss: 0.00094902
Iteration 10/25 | Loss: 0.00094902
Iteration 11/25 | Loss: 0.00094902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000949015433434397, 0.000949015433434397, 0.000949015433434397, 0.000949015433434397, 0.000949015433434397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000949015433434397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30478954
Iteration 2/25 | Loss: 0.00067722
Iteration 3/25 | Loss: 0.00067721
Iteration 4/25 | Loss: 0.00067721
Iteration 5/25 | Loss: 0.00067721
Iteration 6/25 | Loss: 0.00067721
Iteration 7/25 | Loss: 0.00067721
Iteration 8/25 | Loss: 0.00067721
Iteration 9/25 | Loss: 0.00067721
Iteration 10/25 | Loss: 0.00067721
Iteration 11/25 | Loss: 0.00067721
Iteration 12/25 | Loss: 0.00067721
Iteration 13/25 | Loss: 0.00067721
Iteration 14/25 | Loss: 0.00067721
Iteration 15/25 | Loss: 0.00067721
Iteration 16/25 | Loss: 0.00067721
Iteration 17/25 | Loss: 0.00067721
Iteration 18/25 | Loss: 0.00067721
Iteration 19/25 | Loss: 0.00067721
Iteration 20/25 | Loss: 0.00067721
Iteration 21/25 | Loss: 0.00067721
Iteration 22/25 | Loss: 0.00067721
Iteration 23/25 | Loss: 0.00067721
Iteration 24/25 | Loss: 0.00067721
Iteration 25/25 | Loss: 0.00067721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067721
Iteration 2/1000 | Loss: 0.00003356
Iteration 3/1000 | Loss: 0.00001998
Iteration 4/1000 | Loss: 0.00001582
Iteration 5/1000 | Loss: 0.00001477
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001379
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001324
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001313
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001303
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001300
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001297
Iteration 46/1000 | Loss: 0.00001297
Iteration 47/1000 | Loss: 0.00001297
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001296
Iteration 50/1000 | Loss: 0.00001296
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001295
Iteration 53/1000 | Loss: 0.00001295
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001293
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001293
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001292
Iteration 65/1000 | Loss: 0.00001292
Iteration 66/1000 | Loss: 0.00001292
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001291
Iteration 71/1000 | Loss: 0.00001291
Iteration 72/1000 | Loss: 0.00001291
Iteration 73/1000 | Loss: 0.00001291
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001291
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001290
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001290
Iteration 82/1000 | Loss: 0.00001290
Iteration 83/1000 | Loss: 0.00001290
Iteration 84/1000 | Loss: 0.00001290
Iteration 85/1000 | Loss: 0.00001290
Iteration 86/1000 | Loss: 0.00001290
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001289
Iteration 89/1000 | Loss: 0.00001289
Iteration 90/1000 | Loss: 0.00001289
Iteration 91/1000 | Loss: 0.00001289
Iteration 92/1000 | Loss: 0.00001289
Iteration 93/1000 | Loss: 0.00001289
Iteration 94/1000 | Loss: 0.00001289
Iteration 95/1000 | Loss: 0.00001289
Iteration 96/1000 | Loss: 0.00001289
Iteration 97/1000 | Loss: 0.00001288
Iteration 98/1000 | Loss: 0.00001288
Iteration 99/1000 | Loss: 0.00001288
Iteration 100/1000 | Loss: 0.00001288
Iteration 101/1000 | Loss: 0.00001288
Iteration 102/1000 | Loss: 0.00001288
Iteration 103/1000 | Loss: 0.00001288
Iteration 104/1000 | Loss: 0.00001288
Iteration 105/1000 | Loss: 0.00001288
Iteration 106/1000 | Loss: 0.00001288
Iteration 107/1000 | Loss: 0.00001288
Iteration 108/1000 | Loss: 0.00001287
Iteration 109/1000 | Loss: 0.00001287
Iteration 110/1000 | Loss: 0.00001287
Iteration 111/1000 | Loss: 0.00001287
Iteration 112/1000 | Loss: 0.00001287
Iteration 113/1000 | Loss: 0.00001287
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001287
Iteration 116/1000 | Loss: 0.00001287
Iteration 117/1000 | Loss: 0.00001287
Iteration 118/1000 | Loss: 0.00001286
Iteration 119/1000 | Loss: 0.00001286
Iteration 120/1000 | Loss: 0.00001286
Iteration 121/1000 | Loss: 0.00001286
Iteration 122/1000 | Loss: 0.00001286
Iteration 123/1000 | Loss: 0.00001286
Iteration 124/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.2864916243415792e-05, 1.2864916243415792e-05, 1.2864916243415792e-05, 1.2864916243415792e-05, 1.2864916243415792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2864916243415792e-05

Optimization complete. Final v2v error: 2.9580435752868652 mm

Highest mean error: 3.4786086082458496 mm for frame 2

Lowest mean error: 2.3946902751922607 mm for frame 82

Saving results

Total time: 34.475369691848755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905076
Iteration 2/25 | Loss: 0.00168119
Iteration 3/25 | Loss: 0.00119796
Iteration 4/25 | Loss: 0.00112323
Iteration 5/25 | Loss: 0.00112053
Iteration 6/25 | Loss: 0.00112051
Iteration 7/25 | Loss: 0.00112051
Iteration 8/25 | Loss: 0.00112051
Iteration 9/25 | Loss: 0.00112051
Iteration 10/25 | Loss: 0.00112051
Iteration 11/25 | Loss: 0.00112051
Iteration 12/25 | Loss: 0.00112051
Iteration 13/25 | Loss: 0.00112051
Iteration 14/25 | Loss: 0.00112051
Iteration 15/25 | Loss: 0.00112051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011205143528059125, 0.0011205143528059125, 0.0011205143528059125, 0.0011205143528059125, 0.0011205143528059125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011205143528059125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17060316
Iteration 2/25 | Loss: 0.00037311
Iteration 3/25 | Loss: 0.00037311
Iteration 4/25 | Loss: 0.00037311
Iteration 5/25 | Loss: 0.00037311
Iteration 6/25 | Loss: 0.00037311
Iteration 7/25 | Loss: 0.00037311
Iteration 8/25 | Loss: 0.00037311
Iteration 9/25 | Loss: 0.00037311
Iteration 10/25 | Loss: 0.00037311
Iteration 11/25 | Loss: 0.00037311
Iteration 12/25 | Loss: 0.00037311
Iteration 13/25 | Loss: 0.00037311
Iteration 14/25 | Loss: 0.00037311
Iteration 15/25 | Loss: 0.00037311
Iteration 16/25 | Loss: 0.00037311
Iteration 17/25 | Loss: 0.00037311
Iteration 18/25 | Loss: 0.00037311
Iteration 19/25 | Loss: 0.00037311
Iteration 20/25 | Loss: 0.00037311
Iteration 21/25 | Loss: 0.00037311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00037311212508939207, 0.00037311212508939207, 0.00037311212508939207, 0.00037311212508939207, 0.00037311212508939207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037311212508939207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037311
Iteration 2/1000 | Loss: 0.00004295
Iteration 3/1000 | Loss: 0.00002729
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001917
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00001812
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001715
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001654
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001653
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00001649
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001649
Iteration 33/1000 | Loss: 0.00001649
Iteration 34/1000 | Loss: 0.00001649
Iteration 35/1000 | Loss: 0.00001648
Iteration 36/1000 | Loss: 0.00001648
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001648
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001647
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001646
Iteration 48/1000 | Loss: 0.00001646
Iteration 49/1000 | Loss: 0.00001646
Iteration 50/1000 | Loss: 0.00001646
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001645
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001644
Iteration 58/1000 | Loss: 0.00001644
Iteration 59/1000 | Loss: 0.00001643
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00001642
Iteration 66/1000 | Loss: 0.00001642
Iteration 67/1000 | Loss: 0.00001642
Iteration 68/1000 | Loss: 0.00001642
Iteration 69/1000 | Loss: 0.00001642
Iteration 70/1000 | Loss: 0.00001642
Iteration 71/1000 | Loss: 0.00001642
Iteration 72/1000 | Loss: 0.00001642
Iteration 73/1000 | Loss: 0.00001642
Iteration 74/1000 | Loss: 0.00001642
Iteration 75/1000 | Loss: 0.00001642
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001642
Iteration 78/1000 | Loss: 0.00001641
Iteration 79/1000 | Loss: 0.00001641
Iteration 80/1000 | Loss: 0.00001641
Iteration 81/1000 | Loss: 0.00001641
Iteration 82/1000 | Loss: 0.00001641
Iteration 83/1000 | Loss: 0.00001640
Iteration 84/1000 | Loss: 0.00001640
Iteration 85/1000 | Loss: 0.00001640
Iteration 86/1000 | Loss: 0.00001640
Iteration 87/1000 | Loss: 0.00001640
Iteration 88/1000 | Loss: 0.00001640
Iteration 89/1000 | Loss: 0.00001640
Iteration 90/1000 | Loss: 0.00001639
Iteration 91/1000 | Loss: 0.00001639
Iteration 92/1000 | Loss: 0.00001639
Iteration 93/1000 | Loss: 0.00001639
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001638
Iteration 97/1000 | Loss: 0.00001638
Iteration 98/1000 | Loss: 0.00001638
Iteration 99/1000 | Loss: 0.00001637
Iteration 100/1000 | Loss: 0.00001637
Iteration 101/1000 | Loss: 0.00001637
Iteration 102/1000 | Loss: 0.00001637
Iteration 103/1000 | Loss: 0.00001637
Iteration 104/1000 | Loss: 0.00001637
Iteration 105/1000 | Loss: 0.00001637
Iteration 106/1000 | Loss: 0.00001637
Iteration 107/1000 | Loss: 0.00001637
Iteration 108/1000 | Loss: 0.00001637
Iteration 109/1000 | Loss: 0.00001637
Iteration 110/1000 | Loss: 0.00001637
Iteration 111/1000 | Loss: 0.00001637
Iteration 112/1000 | Loss: 0.00001637
Iteration 113/1000 | Loss: 0.00001637
Iteration 114/1000 | Loss: 0.00001637
Iteration 115/1000 | Loss: 0.00001636
Iteration 116/1000 | Loss: 0.00001636
Iteration 117/1000 | Loss: 0.00001636
Iteration 118/1000 | Loss: 0.00001636
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001636
Iteration 121/1000 | Loss: 0.00001636
Iteration 122/1000 | Loss: 0.00001636
Iteration 123/1000 | Loss: 0.00001636
Iteration 124/1000 | Loss: 0.00001636
Iteration 125/1000 | Loss: 0.00001636
Iteration 126/1000 | Loss: 0.00001636
Iteration 127/1000 | Loss: 0.00001636
Iteration 128/1000 | Loss: 0.00001636
Iteration 129/1000 | Loss: 0.00001635
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001635
Iteration 134/1000 | Loss: 0.00001635
Iteration 135/1000 | Loss: 0.00001635
Iteration 136/1000 | Loss: 0.00001635
Iteration 137/1000 | Loss: 0.00001635
Iteration 138/1000 | Loss: 0.00001635
Iteration 139/1000 | Loss: 0.00001635
Iteration 140/1000 | Loss: 0.00001635
Iteration 141/1000 | Loss: 0.00001635
Iteration 142/1000 | Loss: 0.00001635
Iteration 143/1000 | Loss: 0.00001635
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001635
Iteration 148/1000 | Loss: 0.00001635
Iteration 149/1000 | Loss: 0.00001635
Iteration 150/1000 | Loss: 0.00001635
Iteration 151/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.6352154489140958e-05, 1.6352154489140958e-05, 1.6352154489140958e-05, 1.6352154489140958e-05, 1.6352154489140958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6352154489140958e-05

Optimization complete. Final v2v error: 3.4521119594573975 mm

Highest mean error: 3.6800594329833984 mm for frame 123

Lowest mean error: 3.181779146194458 mm for frame 35

Saving results

Total time: 33.55494213104248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850973
Iteration 2/25 | Loss: 0.00157580
Iteration 3/25 | Loss: 0.00122944
Iteration 4/25 | Loss: 0.00119628
Iteration 5/25 | Loss: 0.00133587
Iteration 6/25 | Loss: 0.00118254
Iteration 7/25 | Loss: 0.00110532
Iteration 8/25 | Loss: 0.00104878
Iteration 9/25 | Loss: 0.00103473
Iteration 10/25 | Loss: 0.00103085
Iteration 11/25 | Loss: 0.00106939
Iteration 12/25 | Loss: 0.00106429
Iteration 13/25 | Loss: 0.00102957
Iteration 14/25 | Loss: 0.00102849
Iteration 15/25 | Loss: 0.00102790
Iteration 16/25 | Loss: 0.00102734
Iteration 17/25 | Loss: 0.00111276
Iteration 18/25 | Loss: 0.00105339
Iteration 19/25 | Loss: 0.00106656
Iteration 20/25 | Loss: 0.00104717
Iteration 21/25 | Loss: 0.00105518
Iteration 22/25 | Loss: 0.00102931
Iteration 23/25 | Loss: 0.00102884
Iteration 24/25 | Loss: 0.00106956
Iteration 25/25 | Loss: 0.00106452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26541114
Iteration 2/25 | Loss: 0.00094892
Iteration 3/25 | Loss: 0.00094890
Iteration 4/25 | Loss: 0.00094890
Iteration 5/25 | Loss: 0.00094890
Iteration 6/25 | Loss: 0.00094890
Iteration 7/25 | Loss: 0.00094890
Iteration 8/25 | Loss: 0.00094890
Iteration 9/25 | Loss: 0.00094890
Iteration 10/25 | Loss: 0.00094890
Iteration 11/25 | Loss: 0.00094890
Iteration 12/25 | Loss: 0.00094890
Iteration 13/25 | Loss: 0.00094890
Iteration 14/25 | Loss: 0.00094890
Iteration 15/25 | Loss: 0.00094890
Iteration 16/25 | Loss: 0.00094890
Iteration 17/25 | Loss: 0.00094890
Iteration 18/25 | Loss: 0.00094890
Iteration 19/25 | Loss: 0.00094890
Iteration 20/25 | Loss: 0.00094890
Iteration 21/25 | Loss: 0.00094890
Iteration 22/25 | Loss: 0.00094890
Iteration 23/25 | Loss: 0.00094890
Iteration 24/25 | Loss: 0.00094890
Iteration 25/25 | Loss: 0.00094890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094890
Iteration 2/1000 | Loss: 0.00010560
Iteration 3/1000 | Loss: 0.00603681
Iteration 4/1000 | Loss: 0.00292087
Iteration 5/1000 | Loss: 0.00572227
Iteration 6/1000 | Loss: 0.00098956
Iteration 7/1000 | Loss: 0.00006480
Iteration 8/1000 | Loss: 0.00011238
Iteration 9/1000 | Loss: 0.00003836
Iteration 10/1000 | Loss: 0.00187150
Iteration 11/1000 | Loss: 0.00159917
Iteration 12/1000 | Loss: 0.00296747
Iteration 13/1000 | Loss: 0.00195004
Iteration 14/1000 | Loss: 0.00430809
Iteration 15/1000 | Loss: 0.00330386
Iteration 16/1000 | Loss: 0.00290514
Iteration 17/1000 | Loss: 0.00120028
Iteration 18/1000 | Loss: 0.00290122
Iteration 19/1000 | Loss: 0.00295679
Iteration 20/1000 | Loss: 0.00183838
Iteration 21/1000 | Loss: 0.00175801
Iteration 22/1000 | Loss: 0.00198216
Iteration 23/1000 | Loss: 0.00248388
Iteration 24/1000 | Loss: 0.00255229
Iteration 25/1000 | Loss: 0.00192657
Iteration 26/1000 | Loss: 0.00035830
Iteration 27/1000 | Loss: 0.00044219
Iteration 28/1000 | Loss: 0.00038667
Iteration 29/1000 | Loss: 0.00252906
Iteration 30/1000 | Loss: 0.00149578
Iteration 31/1000 | Loss: 0.00191256
Iteration 32/1000 | Loss: 0.00328853
Iteration 33/1000 | Loss: 0.00012123
Iteration 34/1000 | Loss: 0.00020128
Iteration 35/1000 | Loss: 0.00022499
Iteration 36/1000 | Loss: 0.00057282
Iteration 37/1000 | Loss: 0.00007716
Iteration 38/1000 | Loss: 0.00043446
Iteration 39/1000 | Loss: 0.00005242
Iteration 40/1000 | Loss: 0.00003525
Iteration 41/1000 | Loss: 0.00002857
Iteration 42/1000 | Loss: 0.00002473
Iteration 43/1000 | Loss: 0.00031328
Iteration 44/1000 | Loss: 0.00002282
Iteration 45/1000 | Loss: 0.00001957
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001648
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001529
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001433
Iteration 58/1000 | Loss: 0.00001433
Iteration 59/1000 | Loss: 0.00001433
Iteration 60/1000 | Loss: 0.00001432
Iteration 61/1000 | Loss: 0.00001432
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001424
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001422
Iteration 67/1000 | Loss: 0.00001421
Iteration 68/1000 | Loss: 0.00001421
Iteration 69/1000 | Loss: 0.00001420
Iteration 70/1000 | Loss: 0.00001420
Iteration 71/1000 | Loss: 0.00001420
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001419
Iteration 74/1000 | Loss: 0.00001419
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001416
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001416
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001415
Iteration 85/1000 | Loss: 0.00001415
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001414
Iteration 88/1000 | Loss: 0.00001414
Iteration 89/1000 | Loss: 0.00001414
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001413
Iteration 92/1000 | Loss: 0.00001413
Iteration 93/1000 | Loss: 0.00001413
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001412
Iteration 99/1000 | Loss: 0.00001412
Iteration 100/1000 | Loss: 0.00001412
Iteration 101/1000 | Loss: 0.00001412
Iteration 102/1000 | Loss: 0.00001412
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001411
Iteration 110/1000 | Loss: 0.00001411
Iteration 111/1000 | Loss: 0.00001411
Iteration 112/1000 | Loss: 0.00001410
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001409
Iteration 117/1000 | Loss: 0.00001409
Iteration 118/1000 | Loss: 0.00001409
Iteration 119/1000 | Loss: 0.00001409
Iteration 120/1000 | Loss: 0.00001409
Iteration 121/1000 | Loss: 0.00001408
Iteration 122/1000 | Loss: 0.00001408
Iteration 123/1000 | Loss: 0.00001408
Iteration 124/1000 | Loss: 0.00001408
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001407
Iteration 127/1000 | Loss: 0.00001407
Iteration 128/1000 | Loss: 0.00001407
Iteration 129/1000 | Loss: 0.00001407
Iteration 130/1000 | Loss: 0.00001407
Iteration 131/1000 | Loss: 0.00001407
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001406
Iteration 136/1000 | Loss: 0.00001406
Iteration 137/1000 | Loss: 0.00001406
Iteration 138/1000 | Loss: 0.00001405
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001405
Iteration 141/1000 | Loss: 0.00001405
Iteration 142/1000 | Loss: 0.00001404
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001404
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001403
Iteration 150/1000 | Loss: 0.00001403
Iteration 151/1000 | Loss: 0.00001403
Iteration 152/1000 | Loss: 0.00001403
Iteration 153/1000 | Loss: 0.00001403
Iteration 154/1000 | Loss: 0.00001402
Iteration 155/1000 | Loss: 0.00001402
Iteration 156/1000 | Loss: 0.00001402
Iteration 157/1000 | Loss: 0.00001402
Iteration 158/1000 | Loss: 0.00001402
Iteration 159/1000 | Loss: 0.00001402
Iteration 160/1000 | Loss: 0.00001402
Iteration 161/1000 | Loss: 0.00001402
Iteration 162/1000 | Loss: 0.00001402
Iteration 163/1000 | Loss: 0.00001401
Iteration 164/1000 | Loss: 0.00001401
Iteration 165/1000 | Loss: 0.00001401
Iteration 166/1000 | Loss: 0.00001401
Iteration 167/1000 | Loss: 0.00001401
Iteration 168/1000 | Loss: 0.00001401
Iteration 169/1000 | Loss: 0.00001401
Iteration 170/1000 | Loss: 0.00001401
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Iteration 173/1000 | Loss: 0.00001400
Iteration 174/1000 | Loss: 0.00001400
Iteration 175/1000 | Loss: 0.00001400
Iteration 176/1000 | Loss: 0.00001400
Iteration 177/1000 | Loss: 0.00001400
Iteration 178/1000 | Loss: 0.00001400
Iteration 179/1000 | Loss: 0.00001400
Iteration 180/1000 | Loss: 0.00001400
Iteration 181/1000 | Loss: 0.00001400
Iteration 182/1000 | Loss: 0.00001399
Iteration 183/1000 | Loss: 0.00001399
Iteration 184/1000 | Loss: 0.00001399
Iteration 185/1000 | Loss: 0.00001399
Iteration 186/1000 | Loss: 0.00001399
Iteration 187/1000 | Loss: 0.00001399
Iteration 188/1000 | Loss: 0.00001399
Iteration 189/1000 | Loss: 0.00001399
Iteration 190/1000 | Loss: 0.00001399
Iteration 191/1000 | Loss: 0.00001399
Iteration 192/1000 | Loss: 0.00001399
Iteration 193/1000 | Loss: 0.00001399
Iteration 194/1000 | Loss: 0.00001399
Iteration 195/1000 | Loss: 0.00001399
Iteration 196/1000 | Loss: 0.00001399
Iteration 197/1000 | Loss: 0.00001399
Iteration 198/1000 | Loss: 0.00001399
Iteration 199/1000 | Loss: 0.00001399
Iteration 200/1000 | Loss: 0.00001399
Iteration 201/1000 | Loss: 0.00001399
Iteration 202/1000 | Loss: 0.00001399
Iteration 203/1000 | Loss: 0.00001399
Iteration 204/1000 | Loss: 0.00001398
Iteration 205/1000 | Loss: 0.00001398
Iteration 206/1000 | Loss: 0.00001398
Iteration 207/1000 | Loss: 0.00001398
Iteration 208/1000 | Loss: 0.00001398
Iteration 209/1000 | Loss: 0.00001398
Iteration 210/1000 | Loss: 0.00001398
Iteration 211/1000 | Loss: 0.00001398
Iteration 212/1000 | Loss: 0.00001398
Iteration 213/1000 | Loss: 0.00001398
Iteration 214/1000 | Loss: 0.00001398
Iteration 215/1000 | Loss: 0.00001398
Iteration 216/1000 | Loss: 0.00001398
Iteration 217/1000 | Loss: 0.00001398
Iteration 218/1000 | Loss: 0.00001398
Iteration 219/1000 | Loss: 0.00001398
Iteration 220/1000 | Loss: 0.00001398
Iteration 221/1000 | Loss: 0.00001398
Iteration 222/1000 | Loss: 0.00001397
Iteration 223/1000 | Loss: 0.00001397
Iteration 224/1000 | Loss: 0.00001397
Iteration 225/1000 | Loss: 0.00001397
Iteration 226/1000 | Loss: 0.00001397
Iteration 227/1000 | Loss: 0.00001397
Iteration 228/1000 | Loss: 0.00001397
Iteration 229/1000 | Loss: 0.00001397
Iteration 230/1000 | Loss: 0.00001397
Iteration 231/1000 | Loss: 0.00001397
Iteration 232/1000 | Loss: 0.00001397
Iteration 233/1000 | Loss: 0.00001397
Iteration 234/1000 | Loss: 0.00001397
Iteration 235/1000 | Loss: 0.00001396
Iteration 236/1000 | Loss: 0.00001396
Iteration 237/1000 | Loss: 0.00001396
Iteration 238/1000 | Loss: 0.00001396
Iteration 239/1000 | Loss: 0.00001396
Iteration 240/1000 | Loss: 0.00001396
Iteration 241/1000 | Loss: 0.00001396
Iteration 242/1000 | Loss: 0.00001396
Iteration 243/1000 | Loss: 0.00001396
Iteration 244/1000 | Loss: 0.00001396
Iteration 245/1000 | Loss: 0.00001396
Iteration 246/1000 | Loss: 0.00001396
Iteration 247/1000 | Loss: 0.00001396
Iteration 248/1000 | Loss: 0.00001396
Iteration 249/1000 | Loss: 0.00001396
Iteration 250/1000 | Loss: 0.00001396
Iteration 251/1000 | Loss: 0.00001395
Iteration 252/1000 | Loss: 0.00001395
Iteration 253/1000 | Loss: 0.00001395
Iteration 254/1000 | Loss: 0.00001395
Iteration 255/1000 | Loss: 0.00001395
Iteration 256/1000 | Loss: 0.00001395
Iteration 257/1000 | Loss: 0.00001395
Iteration 258/1000 | Loss: 0.00001395
Iteration 259/1000 | Loss: 0.00001395
Iteration 260/1000 | Loss: 0.00001395
Iteration 261/1000 | Loss: 0.00001395
Iteration 262/1000 | Loss: 0.00001395
Iteration 263/1000 | Loss: 0.00001395
Iteration 264/1000 | Loss: 0.00001395
Iteration 265/1000 | Loss: 0.00001395
Iteration 266/1000 | Loss: 0.00001395
Iteration 267/1000 | Loss: 0.00001395
Iteration 268/1000 | Loss: 0.00001395
Iteration 269/1000 | Loss: 0.00001395
Iteration 270/1000 | Loss: 0.00001395
Iteration 271/1000 | Loss: 0.00001395
Iteration 272/1000 | Loss: 0.00001395
Iteration 273/1000 | Loss: 0.00001395
Iteration 274/1000 | Loss: 0.00001395
Iteration 275/1000 | Loss: 0.00001395
Iteration 276/1000 | Loss: 0.00001395
Iteration 277/1000 | Loss: 0.00001395
Iteration 278/1000 | Loss: 0.00001395
Iteration 279/1000 | Loss: 0.00001394
Iteration 280/1000 | Loss: 0.00001394
Iteration 281/1000 | Loss: 0.00001394
Iteration 282/1000 | Loss: 0.00001394
Iteration 283/1000 | Loss: 0.00001394
Iteration 284/1000 | Loss: 0.00001394
Iteration 285/1000 | Loss: 0.00001394
Iteration 286/1000 | Loss: 0.00001394
Iteration 287/1000 | Loss: 0.00001394
Iteration 288/1000 | Loss: 0.00001394
Iteration 289/1000 | Loss: 0.00001394
Iteration 290/1000 | Loss: 0.00001394
Iteration 291/1000 | Loss: 0.00001394
Iteration 292/1000 | Loss: 0.00001394
Iteration 293/1000 | Loss: 0.00001394
Iteration 294/1000 | Loss: 0.00001394
Iteration 295/1000 | Loss: 0.00001394
Iteration 296/1000 | Loss: 0.00001394
Iteration 297/1000 | Loss: 0.00001394
Iteration 298/1000 | Loss: 0.00001394
Iteration 299/1000 | Loss: 0.00001394
Iteration 300/1000 | Loss: 0.00001394
Iteration 301/1000 | Loss: 0.00001394
Iteration 302/1000 | Loss: 0.00001394
Iteration 303/1000 | Loss: 0.00001394
Iteration 304/1000 | Loss: 0.00001394
Iteration 305/1000 | Loss: 0.00001394
Iteration 306/1000 | Loss: 0.00001394
Iteration 307/1000 | Loss: 0.00001394
Iteration 308/1000 | Loss: 0.00001394
Iteration 309/1000 | Loss: 0.00001394
Iteration 310/1000 | Loss: 0.00001394
Iteration 311/1000 | Loss: 0.00001394
Iteration 312/1000 | Loss: 0.00001394
Iteration 313/1000 | Loss: 0.00001394
Iteration 314/1000 | Loss: 0.00001394
Iteration 315/1000 | Loss: 0.00001394
Iteration 316/1000 | Loss: 0.00001394
Iteration 317/1000 | Loss: 0.00001394
Iteration 318/1000 | Loss: 0.00001394
Iteration 319/1000 | Loss: 0.00001394
Iteration 320/1000 | Loss: 0.00001394
Iteration 321/1000 | Loss: 0.00001394
Iteration 322/1000 | Loss: 0.00001394
Iteration 323/1000 | Loss: 0.00001394
Iteration 324/1000 | Loss: 0.00001394
Iteration 325/1000 | Loss: 0.00001394
Iteration 326/1000 | Loss: 0.00001394
Iteration 327/1000 | Loss: 0.00001394
Iteration 328/1000 | Loss: 0.00001394
Iteration 329/1000 | Loss: 0.00001394
Iteration 330/1000 | Loss: 0.00001394
Iteration 331/1000 | Loss: 0.00001394
Iteration 332/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [1.3936311916040722e-05, 1.3936311916040722e-05, 1.3936311916040722e-05, 1.3936311916040722e-05, 1.3936311916040722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3936311916040722e-05

Optimization complete. Final v2v error: 3.162895917892456 mm

Highest mean error: 4.465918064117432 mm for frame 36

Lowest mean error: 2.6589467525482178 mm for frame 154

Saving results

Total time: 135.33965730667114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01138668
Iteration 2/25 | Loss: 0.01138667
Iteration 3/25 | Loss: 0.00241218
Iteration 4/25 | Loss: 0.00146610
Iteration 5/25 | Loss: 0.00112651
Iteration 6/25 | Loss: 0.00105156
Iteration 7/25 | Loss: 0.00103797
Iteration 8/25 | Loss: 0.00102783
Iteration 9/25 | Loss: 0.00100869
Iteration 10/25 | Loss: 0.00098626
Iteration 11/25 | Loss: 0.00098216
Iteration 12/25 | Loss: 0.00097672
Iteration 13/25 | Loss: 0.00097540
Iteration 14/25 | Loss: 0.00097134
Iteration 15/25 | Loss: 0.00096935
Iteration 16/25 | Loss: 0.00098079
Iteration 17/25 | Loss: 0.00097346
Iteration 18/25 | Loss: 0.00096698
Iteration 19/25 | Loss: 0.00096727
Iteration 20/25 | Loss: 0.00096190
Iteration 21/25 | Loss: 0.00096135
Iteration 22/25 | Loss: 0.00096114
Iteration 23/25 | Loss: 0.00096307
Iteration 24/25 | Loss: 0.00095929
Iteration 25/25 | Loss: 0.00095824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38207603
Iteration 2/25 | Loss: 0.00080219
Iteration 3/25 | Loss: 0.00080218
Iteration 4/25 | Loss: 0.00080218
Iteration 5/25 | Loss: 0.00080218
Iteration 6/25 | Loss: 0.00080218
Iteration 7/25 | Loss: 0.00080218
Iteration 8/25 | Loss: 0.00080218
Iteration 9/25 | Loss: 0.00080218
Iteration 10/25 | Loss: 0.00080218
Iteration 11/25 | Loss: 0.00080218
Iteration 12/25 | Loss: 0.00080218
Iteration 13/25 | Loss: 0.00080218
Iteration 14/25 | Loss: 0.00080218
Iteration 15/25 | Loss: 0.00080218
Iteration 16/25 | Loss: 0.00080218
Iteration 17/25 | Loss: 0.00080218
Iteration 18/25 | Loss: 0.00080218
Iteration 19/25 | Loss: 0.00080218
Iteration 20/25 | Loss: 0.00080218
Iteration 21/25 | Loss: 0.00080218
Iteration 22/25 | Loss: 0.00080218
Iteration 23/25 | Loss: 0.00080218
Iteration 24/25 | Loss: 0.00080218
Iteration 25/25 | Loss: 0.00080218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080218
Iteration 2/1000 | Loss: 0.00087059
Iteration 3/1000 | Loss: 0.00005345
Iteration 4/1000 | Loss: 0.00003761
Iteration 5/1000 | Loss: 0.00003957
Iteration 6/1000 | Loss: 0.00002219
Iteration 7/1000 | Loss: 0.00006635
Iteration 8/1000 | Loss: 0.00013233
Iteration 9/1000 | Loss: 0.00008004
Iteration 10/1000 | Loss: 0.00002509
Iteration 11/1000 | Loss: 0.00003939
Iteration 12/1000 | Loss: 0.00013334
Iteration 13/1000 | Loss: 0.00005691
Iteration 14/1000 | Loss: 0.00009474
Iteration 15/1000 | Loss: 0.00002065
Iteration 16/1000 | Loss: 0.00002281
Iteration 17/1000 | Loss: 0.00004194
Iteration 18/1000 | Loss: 0.00006986
Iteration 19/1000 | Loss: 0.00024396
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00007489
Iteration 22/1000 | Loss: 0.00003527
Iteration 23/1000 | Loss: 0.00003143
Iteration 24/1000 | Loss: 0.00003769
Iteration 25/1000 | Loss: 0.00005505
Iteration 26/1000 | Loss: 0.00005421
Iteration 27/1000 | Loss: 0.00002842
Iteration 28/1000 | Loss: 0.00002423
Iteration 29/1000 | Loss: 0.00005777
Iteration 30/1000 | Loss: 0.00002317
Iteration 31/1000 | Loss: 0.00005814
Iteration 32/1000 | Loss: 0.00003084
Iteration 33/1000 | Loss: 0.00004920
Iteration 34/1000 | Loss: 0.00002575
Iteration 35/1000 | Loss: 0.00004716
Iteration 36/1000 | Loss: 0.00002127
Iteration 37/1000 | Loss: 0.00002087
Iteration 38/1000 | Loss: 0.00001953
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002752
Iteration 41/1000 | Loss: 0.00002508
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00002555
Iteration 44/1000 | Loss: 0.00002025
Iteration 45/1000 | Loss: 0.00001774
Iteration 46/1000 | Loss: 0.00001745
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001738
Iteration 49/1000 | Loss: 0.00001736
Iteration 50/1000 | Loss: 0.00002255
Iteration 51/1000 | Loss: 0.00001716
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001711
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001709
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001695
Iteration 68/1000 | Loss: 0.00001695
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00001694
Iteration 72/1000 | Loss: 0.00001694
Iteration 73/1000 | Loss: 0.00001694
Iteration 74/1000 | Loss: 0.00001694
Iteration 75/1000 | Loss: 0.00001694
Iteration 76/1000 | Loss: 0.00001694
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001693
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001693
Iteration 81/1000 | Loss: 0.00001693
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001692
Iteration 90/1000 | Loss: 0.00001692
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001689
Iteration 105/1000 | Loss: 0.00001689
Iteration 106/1000 | Loss: 0.00001689
Iteration 107/1000 | Loss: 0.00001689
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001688
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001683
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001683
Iteration 147/1000 | Loss: 0.00001683
Iteration 148/1000 | Loss: 0.00001683
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001682
Iteration 151/1000 | Loss: 0.00001682
Iteration 152/1000 | Loss: 0.00001682
Iteration 153/1000 | Loss: 0.00001682
Iteration 154/1000 | Loss: 0.00001682
Iteration 155/1000 | Loss: 0.00001682
Iteration 156/1000 | Loss: 0.00001682
Iteration 157/1000 | Loss: 0.00001682
Iteration 158/1000 | Loss: 0.00001681
Iteration 159/1000 | Loss: 0.00001680
Iteration 160/1000 | Loss: 0.00001680
Iteration 161/1000 | Loss: 0.00001680
Iteration 162/1000 | Loss: 0.00001679
Iteration 163/1000 | Loss: 0.00001679
Iteration 164/1000 | Loss: 0.00001679
Iteration 165/1000 | Loss: 0.00001679
Iteration 166/1000 | Loss: 0.00001679
Iteration 167/1000 | Loss: 0.00001678
Iteration 168/1000 | Loss: 0.00001678
Iteration 169/1000 | Loss: 0.00001678
Iteration 170/1000 | Loss: 0.00001678
Iteration 171/1000 | Loss: 0.00001678
Iteration 172/1000 | Loss: 0.00001678
Iteration 173/1000 | Loss: 0.00001678
Iteration 174/1000 | Loss: 0.00001678
Iteration 175/1000 | Loss: 0.00001678
Iteration 176/1000 | Loss: 0.00001678
Iteration 177/1000 | Loss: 0.00001678
Iteration 178/1000 | Loss: 0.00001678
Iteration 179/1000 | Loss: 0.00001677
Iteration 180/1000 | Loss: 0.00001677
Iteration 181/1000 | Loss: 0.00001677
Iteration 182/1000 | Loss: 0.00001676
Iteration 183/1000 | Loss: 0.00001676
Iteration 184/1000 | Loss: 0.00001676
Iteration 185/1000 | Loss: 0.00001676
Iteration 186/1000 | Loss: 0.00001676
Iteration 187/1000 | Loss: 0.00001676
Iteration 188/1000 | Loss: 0.00001675
Iteration 189/1000 | Loss: 0.00001675
Iteration 190/1000 | Loss: 0.00001675
Iteration 191/1000 | Loss: 0.00001674
Iteration 192/1000 | Loss: 0.00001674
Iteration 193/1000 | Loss: 0.00001674
Iteration 194/1000 | Loss: 0.00001674
Iteration 195/1000 | Loss: 0.00001674
Iteration 196/1000 | Loss: 0.00001674
Iteration 197/1000 | Loss: 0.00001673
Iteration 198/1000 | Loss: 0.00001673
Iteration 199/1000 | Loss: 0.00001673
Iteration 200/1000 | Loss: 0.00001673
Iteration 201/1000 | Loss: 0.00001673
Iteration 202/1000 | Loss: 0.00001673
Iteration 203/1000 | Loss: 0.00001673
Iteration 204/1000 | Loss: 0.00001673
Iteration 205/1000 | Loss: 0.00001672
Iteration 206/1000 | Loss: 0.00001672
Iteration 207/1000 | Loss: 0.00001672
Iteration 208/1000 | Loss: 0.00001672
Iteration 209/1000 | Loss: 0.00001672
Iteration 210/1000 | Loss: 0.00001672
Iteration 211/1000 | Loss: 0.00001672
Iteration 212/1000 | Loss: 0.00001672
Iteration 213/1000 | Loss: 0.00001672
Iteration 214/1000 | Loss: 0.00001672
Iteration 215/1000 | Loss: 0.00001672
Iteration 216/1000 | Loss: 0.00001672
Iteration 217/1000 | Loss: 0.00001672
Iteration 218/1000 | Loss: 0.00001672
Iteration 219/1000 | Loss: 0.00001672
Iteration 220/1000 | Loss: 0.00001672
Iteration 221/1000 | Loss: 0.00001672
Iteration 222/1000 | Loss: 0.00001672
Iteration 223/1000 | Loss: 0.00001672
Iteration 224/1000 | Loss: 0.00001672
Iteration 225/1000 | Loss: 0.00001672
Iteration 226/1000 | Loss: 0.00001672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.671553764026612e-05, 1.671553764026612e-05, 1.671553764026612e-05, 1.671553764026612e-05, 1.671553764026612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.671553764026612e-05

Optimization complete. Final v2v error: 3.2625625133514404 mm

Highest mean error: 8.899154663085938 mm for frame 98

Lowest mean error: 2.731471538543701 mm for frame 65

Saving results

Total time: 118.91706418991089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024509
Iteration 2/25 | Loss: 0.00176718
Iteration 3/25 | Loss: 0.00121778
Iteration 4/25 | Loss: 0.00104787
Iteration 5/25 | Loss: 0.00101526
Iteration 6/25 | Loss: 0.00101007
Iteration 7/25 | Loss: 0.00100912
Iteration 8/25 | Loss: 0.00100894
Iteration 9/25 | Loss: 0.00100888
Iteration 10/25 | Loss: 0.00100888
Iteration 11/25 | Loss: 0.00100888
Iteration 12/25 | Loss: 0.00100888
Iteration 13/25 | Loss: 0.00100887
Iteration 14/25 | Loss: 0.00100887
Iteration 15/25 | Loss: 0.00100887
Iteration 16/25 | Loss: 0.00100887
Iteration 17/25 | Loss: 0.00100887
Iteration 18/25 | Loss: 0.00100887
Iteration 19/25 | Loss: 0.00100887
Iteration 20/25 | Loss: 0.00100887
Iteration 21/25 | Loss: 0.00100886
Iteration 22/25 | Loss: 0.00100886
Iteration 23/25 | Loss: 0.00100886
Iteration 24/25 | Loss: 0.00100886
Iteration 25/25 | Loss: 0.00100886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52093947
Iteration 2/25 | Loss: 0.00057586
Iteration 3/25 | Loss: 0.00057584
Iteration 4/25 | Loss: 0.00057584
Iteration 5/25 | Loss: 0.00057584
Iteration 6/25 | Loss: 0.00057584
Iteration 7/25 | Loss: 0.00057584
Iteration 8/25 | Loss: 0.00057584
Iteration 9/25 | Loss: 0.00057584
Iteration 10/25 | Loss: 0.00057584
Iteration 11/25 | Loss: 0.00057584
Iteration 12/25 | Loss: 0.00057584
Iteration 13/25 | Loss: 0.00057584
Iteration 14/25 | Loss: 0.00057584
Iteration 15/25 | Loss: 0.00057584
Iteration 16/25 | Loss: 0.00057584
Iteration 17/25 | Loss: 0.00057584
Iteration 18/25 | Loss: 0.00057584
Iteration 19/25 | Loss: 0.00057584
Iteration 20/25 | Loss: 0.00057584
Iteration 21/25 | Loss: 0.00057584
Iteration 22/25 | Loss: 0.00057584
Iteration 23/25 | Loss: 0.00057584
Iteration 24/25 | Loss: 0.00057584
Iteration 25/25 | Loss: 0.00057584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057584
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00002035
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001531
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001461
Iteration 9/1000 | Loss: 0.00001440
Iteration 10/1000 | Loss: 0.00001430
Iteration 11/1000 | Loss: 0.00001430
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001414
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001412
Iteration 19/1000 | Loss: 0.00001411
Iteration 20/1000 | Loss: 0.00001411
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001409
Iteration 23/1000 | Loss: 0.00001409
Iteration 24/1000 | Loss: 0.00001409
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001404
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001404
Iteration 33/1000 | Loss: 0.00001403
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001401
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001394
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001392
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001384
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001383
Iteration 91/1000 | Loss: 0.00001383
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001380
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001379
Iteration 113/1000 | Loss: 0.00001379
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001379
Iteration 116/1000 | Loss: 0.00001379
Iteration 117/1000 | Loss: 0.00001378
Iteration 118/1000 | Loss: 0.00001378
Iteration 119/1000 | Loss: 0.00001378
Iteration 120/1000 | Loss: 0.00001378
Iteration 121/1000 | Loss: 0.00001378
Iteration 122/1000 | Loss: 0.00001378
Iteration 123/1000 | Loss: 0.00001378
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001378
Iteration 129/1000 | Loss: 0.00001377
Iteration 130/1000 | Loss: 0.00001377
Iteration 131/1000 | Loss: 0.00001377
Iteration 132/1000 | Loss: 0.00001377
Iteration 133/1000 | Loss: 0.00001377
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001377
Iteration 138/1000 | Loss: 0.00001377
Iteration 139/1000 | Loss: 0.00001376
Iteration 140/1000 | Loss: 0.00001376
Iteration 141/1000 | Loss: 0.00001376
Iteration 142/1000 | Loss: 0.00001376
Iteration 143/1000 | Loss: 0.00001376
Iteration 144/1000 | Loss: 0.00001376
Iteration 145/1000 | Loss: 0.00001376
Iteration 146/1000 | Loss: 0.00001376
Iteration 147/1000 | Loss: 0.00001376
Iteration 148/1000 | Loss: 0.00001375
Iteration 149/1000 | Loss: 0.00001375
Iteration 150/1000 | Loss: 0.00001375
Iteration 151/1000 | Loss: 0.00001375
Iteration 152/1000 | Loss: 0.00001375
Iteration 153/1000 | Loss: 0.00001375
Iteration 154/1000 | Loss: 0.00001375
Iteration 155/1000 | Loss: 0.00001374
Iteration 156/1000 | Loss: 0.00001374
Iteration 157/1000 | Loss: 0.00001374
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001374
Iteration 161/1000 | Loss: 0.00001373
Iteration 162/1000 | Loss: 0.00001373
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Iteration 165/1000 | Loss: 0.00001373
Iteration 166/1000 | Loss: 0.00001373
Iteration 167/1000 | Loss: 0.00001373
Iteration 168/1000 | Loss: 0.00001373
Iteration 169/1000 | Loss: 0.00001373
Iteration 170/1000 | Loss: 0.00001373
Iteration 171/1000 | Loss: 0.00001372
Iteration 172/1000 | Loss: 0.00001372
Iteration 173/1000 | Loss: 0.00001372
Iteration 174/1000 | Loss: 0.00001372
Iteration 175/1000 | Loss: 0.00001372
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001372
Iteration 178/1000 | Loss: 0.00001372
Iteration 179/1000 | Loss: 0.00001372
Iteration 180/1000 | Loss: 0.00001372
Iteration 181/1000 | Loss: 0.00001372
Iteration 182/1000 | Loss: 0.00001372
Iteration 183/1000 | Loss: 0.00001372
Iteration 184/1000 | Loss: 0.00001372
Iteration 185/1000 | Loss: 0.00001372
Iteration 186/1000 | Loss: 0.00001372
Iteration 187/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.3724678865401074e-05, 1.3724678865401074e-05, 1.3724678865401074e-05, 1.3724678865401074e-05, 1.3724678865401074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3724678865401074e-05

Optimization complete. Final v2v error: 3.131972074508667 mm

Highest mean error: 4.147827625274658 mm for frame 166

Lowest mean error: 2.632051467895508 mm for frame 138

Saving results

Total time: 49.245325565338135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819978
Iteration 2/25 | Loss: 0.00171466
Iteration 3/25 | Loss: 0.00119824
Iteration 4/25 | Loss: 0.00112526
Iteration 5/25 | Loss: 0.00113516
Iteration 6/25 | Loss: 0.00110579
Iteration 7/25 | Loss: 0.00109302
Iteration 8/25 | Loss: 0.00109029
Iteration 9/25 | Loss: 0.00108964
Iteration 10/25 | Loss: 0.00109121
Iteration 11/25 | Loss: 0.00108913
Iteration 12/25 | Loss: 0.00108853
Iteration 13/25 | Loss: 0.00108807
Iteration 14/25 | Loss: 0.00108722
Iteration 15/25 | Loss: 0.00108682
Iteration 16/25 | Loss: 0.00108669
Iteration 17/25 | Loss: 0.00108604
Iteration 18/25 | Loss: 0.00108502
Iteration 19/25 | Loss: 0.00108465
Iteration 20/25 | Loss: 0.00108456
Iteration 21/25 | Loss: 0.00108456
Iteration 22/25 | Loss: 0.00108456
Iteration 23/25 | Loss: 0.00108456
Iteration 24/25 | Loss: 0.00108456
Iteration 25/25 | Loss: 0.00108456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54160249
Iteration 2/25 | Loss: 0.00046488
Iteration 3/25 | Loss: 0.00046487
Iteration 4/25 | Loss: 0.00046487
Iteration 5/25 | Loss: 0.00046487
Iteration 6/25 | Loss: 0.00046487
Iteration 7/25 | Loss: 0.00046487
Iteration 8/25 | Loss: 0.00046487
Iteration 9/25 | Loss: 0.00046487
Iteration 10/25 | Loss: 0.00046487
Iteration 11/25 | Loss: 0.00046487
Iteration 12/25 | Loss: 0.00046487
Iteration 13/25 | Loss: 0.00046487
Iteration 14/25 | Loss: 0.00046487
Iteration 15/25 | Loss: 0.00046487
Iteration 16/25 | Loss: 0.00046487
Iteration 17/25 | Loss: 0.00046487
Iteration 18/25 | Loss: 0.00046487
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004648650938179344, 0.0004648650938179344, 0.0004648650938179344, 0.0004648650938179344, 0.0004648650938179344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004648650938179344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046487
Iteration 2/1000 | Loss: 0.00004144
Iteration 3/1000 | Loss: 0.00002735
Iteration 4/1000 | Loss: 0.00002456
Iteration 5/1000 | Loss: 0.00002308
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002194
Iteration 8/1000 | Loss: 0.00002163
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002106
Iteration 12/1000 | Loss: 0.00002100
Iteration 13/1000 | Loss: 0.00002099
Iteration 14/1000 | Loss: 0.00002094
Iteration 15/1000 | Loss: 0.00002093
Iteration 16/1000 | Loss: 0.00002088
Iteration 17/1000 | Loss: 0.00002083
Iteration 18/1000 | Loss: 0.00002077
Iteration 19/1000 | Loss: 0.00002069
Iteration 20/1000 | Loss: 0.00002066
Iteration 21/1000 | Loss: 0.00002066
Iteration 22/1000 | Loss: 0.00002066
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002063
Iteration 25/1000 | Loss: 0.00002062
Iteration 26/1000 | Loss: 0.00002062
Iteration 27/1000 | Loss: 0.00002061
Iteration 28/1000 | Loss: 0.00002060
Iteration 29/1000 | Loss: 0.00002059
Iteration 30/1000 | Loss: 0.00002059
Iteration 31/1000 | Loss: 0.00002059
Iteration 32/1000 | Loss: 0.00002059
Iteration 33/1000 | Loss: 0.00002059
Iteration 34/1000 | Loss: 0.00002059
Iteration 35/1000 | Loss: 0.00002059
Iteration 36/1000 | Loss: 0.00002059
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002058
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002056
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002055
Iteration 47/1000 | Loss: 0.00002054
Iteration 48/1000 | Loss: 0.00002054
Iteration 49/1000 | Loss: 0.00002054
Iteration 50/1000 | Loss: 0.00002054
Iteration 51/1000 | Loss: 0.00002053
Iteration 52/1000 | Loss: 0.00002053
Iteration 53/1000 | Loss: 0.00002053
Iteration 54/1000 | Loss: 0.00002053
Iteration 55/1000 | Loss: 0.00002053
Iteration 56/1000 | Loss: 0.00002052
Iteration 57/1000 | Loss: 0.00002052
Iteration 58/1000 | Loss: 0.00002052
Iteration 59/1000 | Loss: 0.00002052
Iteration 60/1000 | Loss: 0.00002052
Iteration 61/1000 | Loss: 0.00002052
Iteration 62/1000 | Loss: 0.00002052
Iteration 63/1000 | Loss: 0.00002052
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002051
Iteration 67/1000 | Loss: 0.00002051
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002051
Iteration 70/1000 | Loss: 0.00002050
Iteration 71/1000 | Loss: 0.00002050
Iteration 72/1000 | Loss: 0.00002050
Iteration 73/1000 | Loss: 0.00002050
Iteration 74/1000 | Loss: 0.00002049
Iteration 75/1000 | Loss: 0.00002049
Iteration 76/1000 | Loss: 0.00002049
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002047
Iteration 81/1000 | Loss: 0.00002047
Iteration 82/1000 | Loss: 0.00002047
Iteration 83/1000 | Loss: 0.00002046
Iteration 84/1000 | Loss: 0.00002046
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002046
Iteration 90/1000 | Loss: 0.00002046
Iteration 91/1000 | Loss: 0.00002045
Iteration 92/1000 | Loss: 0.00002045
Iteration 93/1000 | Loss: 0.00002045
Iteration 94/1000 | Loss: 0.00002045
Iteration 95/1000 | Loss: 0.00002044
Iteration 96/1000 | Loss: 0.00002044
Iteration 97/1000 | Loss: 0.00002044
Iteration 98/1000 | Loss: 0.00002044
Iteration 99/1000 | Loss: 0.00002043
Iteration 100/1000 | Loss: 0.00002043
Iteration 101/1000 | Loss: 0.00002043
Iteration 102/1000 | Loss: 0.00002043
Iteration 103/1000 | Loss: 0.00002043
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002042
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.0413008314790204e-05, 2.0413008314790204e-05, 2.0413008314790204e-05, 2.0413008314790204e-05, 2.0413008314790204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0413008314790204e-05

Optimization complete. Final v2v error: 3.6150617599487305 mm

Highest mean error: 6.038109302520752 mm for frame 41

Lowest mean error: 2.718520402908325 mm for frame 159

Saving results

Total time: 68.05949473381042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894386
Iteration 2/25 | Loss: 0.00108610
Iteration 3/25 | Loss: 0.00095286
Iteration 4/25 | Loss: 0.00093978
Iteration 5/25 | Loss: 0.00093584
Iteration 6/25 | Loss: 0.00093466
Iteration 7/25 | Loss: 0.00093466
Iteration 8/25 | Loss: 0.00093466
Iteration 9/25 | Loss: 0.00093466
Iteration 10/25 | Loss: 0.00093466
Iteration 11/25 | Loss: 0.00093466
Iteration 12/25 | Loss: 0.00093466
Iteration 13/25 | Loss: 0.00093466
Iteration 14/25 | Loss: 0.00093466
Iteration 15/25 | Loss: 0.00093466
Iteration 16/25 | Loss: 0.00093466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009346581646241248, 0.0009346581646241248, 0.0009346581646241248, 0.0009346581646241248, 0.0009346581646241248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009346581646241248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74315250
Iteration 2/25 | Loss: 0.00065422
Iteration 3/25 | Loss: 0.00065422
Iteration 4/25 | Loss: 0.00065422
Iteration 5/25 | Loss: 0.00065422
Iteration 6/25 | Loss: 0.00065421
Iteration 7/25 | Loss: 0.00065421
Iteration 8/25 | Loss: 0.00065421
Iteration 9/25 | Loss: 0.00065421
Iteration 10/25 | Loss: 0.00065421
Iteration 11/25 | Loss: 0.00065421
Iteration 12/25 | Loss: 0.00065421
Iteration 13/25 | Loss: 0.00065421
Iteration 14/25 | Loss: 0.00065421
Iteration 15/25 | Loss: 0.00065421
Iteration 16/25 | Loss: 0.00065421
Iteration 17/25 | Loss: 0.00065421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006542130722664297, 0.0006542130722664297, 0.0006542130722664297, 0.0006542130722664297, 0.0006542130722664297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006542130722664297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065421
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00001800
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001207
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001158
Iteration 10/1000 | Loss: 0.00001157
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001156
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001151
Iteration 18/1000 | Loss: 0.00001151
Iteration 19/1000 | Loss: 0.00001151
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001147
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001147
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001143
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001139
Iteration 36/1000 | Loss: 0.00001138
Iteration 37/1000 | Loss: 0.00001138
Iteration 38/1000 | Loss: 0.00001138
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001136
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001134
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001134
Iteration 58/1000 | Loss: 0.00001134
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001130
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001127
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001126
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001125
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001125
Iteration 87/1000 | Loss: 0.00001125
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001124
Iteration 95/1000 | Loss: 0.00001124
Iteration 96/1000 | Loss: 0.00001124
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001123
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001123
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001122
Iteration 104/1000 | Loss: 0.00001122
Iteration 105/1000 | Loss: 0.00001122
Iteration 106/1000 | Loss: 0.00001122
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001121
Iteration 109/1000 | Loss: 0.00001121
Iteration 110/1000 | Loss: 0.00001121
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001120
Iteration 115/1000 | Loss: 0.00001120
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001120
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001120
Iteration 127/1000 | Loss: 0.00001120
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001120
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001120
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.1196330888196826e-05, 1.1196330888196826e-05, 1.1196330888196826e-05, 1.1196330888196826e-05, 1.1196330888196826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1196330888196826e-05

Optimization complete. Final v2v error: 2.787836790084839 mm

Highest mean error: 3.6130406856536865 mm for frame 55

Lowest mean error: 2.4871902465820312 mm for frame 23

Saving results

Total time: 29.703155040740967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031377
Iteration 2/25 | Loss: 0.00287852
Iteration 3/25 | Loss: 0.00196125
Iteration 4/25 | Loss: 0.00175619
Iteration 5/25 | Loss: 0.00160580
Iteration 6/25 | Loss: 0.00176554
Iteration 7/25 | Loss: 0.00165176
Iteration 8/25 | Loss: 0.00147386
Iteration 9/25 | Loss: 0.00140072
Iteration 10/25 | Loss: 0.00135383
Iteration 11/25 | Loss: 0.00133733
Iteration 12/25 | Loss: 0.00132475
Iteration 13/25 | Loss: 0.00130178
Iteration 14/25 | Loss: 0.00129198
Iteration 15/25 | Loss: 0.00128811
Iteration 16/25 | Loss: 0.00127732
Iteration 17/25 | Loss: 0.00126856
Iteration 18/25 | Loss: 0.00126511
Iteration 19/25 | Loss: 0.00126775
Iteration 20/25 | Loss: 0.00126729
Iteration 21/25 | Loss: 0.00126395
Iteration 22/25 | Loss: 0.00126257
Iteration 23/25 | Loss: 0.00126223
Iteration 24/25 | Loss: 0.00126209
Iteration 25/25 | Loss: 0.00126245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31787598
Iteration 2/25 | Loss: 0.00298165
Iteration 3/25 | Loss: 0.00294425
Iteration 4/25 | Loss: 0.00294425
Iteration 5/25 | Loss: 0.00294425
Iteration 6/25 | Loss: 0.00294425
Iteration 7/25 | Loss: 0.00294424
Iteration 8/25 | Loss: 0.00294424
Iteration 9/25 | Loss: 0.00294424
Iteration 10/25 | Loss: 0.00294424
Iteration 11/25 | Loss: 0.00294424
Iteration 12/25 | Loss: 0.00294424
Iteration 13/25 | Loss: 0.00294424
Iteration 14/25 | Loss: 0.00294424
Iteration 15/25 | Loss: 0.00294424
Iteration 16/25 | Loss: 0.00294424
Iteration 17/25 | Loss: 0.00294424
Iteration 18/25 | Loss: 0.00294424
Iteration 19/25 | Loss: 0.00294424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0029442438390105963, 0.0029442438390105963, 0.0029442438390105963, 0.0029442438390105963, 0.0029442438390105963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029442438390105963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294424
Iteration 2/1000 | Loss: 0.00080121
Iteration 3/1000 | Loss: 0.00074721
Iteration 4/1000 | Loss: 0.00053416
Iteration 5/1000 | Loss: 0.00102454
Iteration 6/1000 | Loss: 0.00115686
Iteration 7/1000 | Loss: 0.00074484
Iteration 8/1000 | Loss: 0.00055570
Iteration 9/1000 | Loss: 0.00088281
Iteration 10/1000 | Loss: 0.00184097
Iteration 11/1000 | Loss: 0.00027465
Iteration 12/1000 | Loss: 0.00029245
Iteration 13/1000 | Loss: 0.00069569
Iteration 14/1000 | Loss: 0.00069446
Iteration 15/1000 | Loss: 0.00109344
Iteration 16/1000 | Loss: 0.00047057
Iteration 17/1000 | Loss: 0.00100487
Iteration 18/1000 | Loss: 0.00032735
Iteration 19/1000 | Loss: 0.00024156
Iteration 20/1000 | Loss: 0.00053513
Iteration 21/1000 | Loss: 0.00030476
Iteration 22/1000 | Loss: 0.00029210
Iteration 23/1000 | Loss: 0.00014730
Iteration 24/1000 | Loss: 0.00033025
Iteration 25/1000 | Loss: 0.00075350
Iteration 26/1000 | Loss: 0.00096816
Iteration 27/1000 | Loss: 0.00099497
Iteration 28/1000 | Loss: 0.00552940
Iteration 29/1000 | Loss: 0.00410060
Iteration 30/1000 | Loss: 0.00556884
Iteration 31/1000 | Loss: 0.00434333
Iteration 32/1000 | Loss: 0.00463438
Iteration 33/1000 | Loss: 0.00220499
Iteration 34/1000 | Loss: 0.00063642
Iteration 35/1000 | Loss: 0.00095836
Iteration 36/1000 | Loss: 0.00018093
Iteration 37/1000 | Loss: 0.00017120
Iteration 38/1000 | Loss: 0.00012239
Iteration 39/1000 | Loss: 0.00009494
Iteration 40/1000 | Loss: 0.00081380
Iteration 41/1000 | Loss: 0.00062743
Iteration 42/1000 | Loss: 0.00032243
Iteration 43/1000 | Loss: 0.00005782
Iteration 44/1000 | Loss: 0.00022456
Iteration 45/1000 | Loss: 0.00015215
Iteration 46/1000 | Loss: 0.00013297
Iteration 47/1000 | Loss: 0.00033662
Iteration 48/1000 | Loss: 0.00024590
Iteration 49/1000 | Loss: 0.00009561
Iteration 50/1000 | Loss: 0.00018146
Iteration 51/1000 | Loss: 0.00024410
Iteration 52/1000 | Loss: 0.00055650
Iteration 53/1000 | Loss: 0.00016176
Iteration 54/1000 | Loss: 0.00004821
Iteration 55/1000 | Loss: 0.00003176
Iteration 56/1000 | Loss: 0.00004421
Iteration 57/1000 | Loss: 0.00003388
Iteration 58/1000 | Loss: 0.00004748
Iteration 59/1000 | Loss: 0.00002585
Iteration 60/1000 | Loss: 0.00009772
Iteration 61/1000 | Loss: 0.00003502
Iteration 62/1000 | Loss: 0.00005540
Iteration 63/1000 | Loss: 0.00004508
Iteration 64/1000 | Loss: 0.00019085
Iteration 65/1000 | Loss: 0.00010466
Iteration 66/1000 | Loss: 0.00012146
Iteration 67/1000 | Loss: 0.00003480
Iteration 68/1000 | Loss: 0.00004455
Iteration 69/1000 | Loss: 0.00003292
Iteration 70/1000 | Loss: 0.00004071
Iteration 71/1000 | Loss: 0.00002496
Iteration 72/1000 | Loss: 0.00023026
Iteration 73/1000 | Loss: 0.00004697
Iteration 74/1000 | Loss: 0.00004888
Iteration 75/1000 | Loss: 0.00007457
Iteration 76/1000 | Loss: 0.00004634
Iteration 77/1000 | Loss: 0.00002842
Iteration 78/1000 | Loss: 0.00003626
Iteration 79/1000 | Loss: 0.00008728
Iteration 80/1000 | Loss: 0.00004294
Iteration 81/1000 | Loss: 0.00007904
Iteration 82/1000 | Loss: 0.00004561
Iteration 83/1000 | Loss: 0.00025219
Iteration 84/1000 | Loss: 0.00009771
Iteration 85/1000 | Loss: 0.00002352
Iteration 86/1000 | Loss: 0.00002819
Iteration 87/1000 | Loss: 0.00002619
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00005132
Iteration 91/1000 | Loss: 0.00040129
Iteration 92/1000 | Loss: 0.00005366
Iteration 93/1000 | Loss: 0.00002093
Iteration 94/1000 | Loss: 0.00001847
Iteration 95/1000 | Loss: 0.00001648
Iteration 96/1000 | Loss: 0.00003070
Iteration 97/1000 | Loss: 0.00002284
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00002563
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001412
Iteration 102/1000 | Loss: 0.00001410
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001399
Iteration 111/1000 | Loss: 0.00001399
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001396
Iteration 114/1000 | Loss: 0.00001396
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001394
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001392
Iteration 122/1000 | Loss: 0.00001392
Iteration 123/1000 | Loss: 0.00001392
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001389
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001379
Iteration 139/1000 | Loss: 0.00001379
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001377
Iteration 145/1000 | Loss: 0.00001377
Iteration 146/1000 | Loss: 0.00001377
Iteration 147/1000 | Loss: 0.00001376
Iteration 148/1000 | Loss: 0.00001376
Iteration 149/1000 | Loss: 0.00001375
Iteration 150/1000 | Loss: 0.00002900
Iteration 151/1000 | Loss: 0.00048212
Iteration 152/1000 | Loss: 0.00037980
Iteration 153/1000 | Loss: 0.00005634
Iteration 154/1000 | Loss: 0.00001567
Iteration 155/1000 | Loss: 0.00002298
Iteration 156/1000 | Loss: 0.00002679
Iteration 157/1000 | Loss: 0.00009682
Iteration 158/1000 | Loss: 0.00001766
Iteration 159/1000 | Loss: 0.00004538
Iteration 160/1000 | Loss: 0.00001036
Iteration 161/1000 | Loss: 0.00004380
Iteration 162/1000 | Loss: 0.00003642
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00000953
Iteration 165/1000 | Loss: 0.00000945
Iteration 166/1000 | Loss: 0.00000938
Iteration 167/1000 | Loss: 0.00000936
Iteration 168/1000 | Loss: 0.00000933
Iteration 169/1000 | Loss: 0.00000930
Iteration 170/1000 | Loss: 0.00000929
Iteration 171/1000 | Loss: 0.00000929
Iteration 172/1000 | Loss: 0.00000928
Iteration 173/1000 | Loss: 0.00000928
Iteration 174/1000 | Loss: 0.00000927
Iteration 175/1000 | Loss: 0.00000926
Iteration 176/1000 | Loss: 0.00000926
Iteration 177/1000 | Loss: 0.00000926
Iteration 178/1000 | Loss: 0.00000926
Iteration 179/1000 | Loss: 0.00000925
Iteration 180/1000 | Loss: 0.00000925
Iteration 181/1000 | Loss: 0.00000924
Iteration 182/1000 | Loss: 0.00000924
Iteration 183/1000 | Loss: 0.00000923
Iteration 184/1000 | Loss: 0.00000923
Iteration 185/1000 | Loss: 0.00000923
Iteration 186/1000 | Loss: 0.00000922
Iteration 187/1000 | Loss: 0.00000922
Iteration 188/1000 | Loss: 0.00000922
Iteration 189/1000 | Loss: 0.00000921
Iteration 190/1000 | Loss: 0.00000921
Iteration 191/1000 | Loss: 0.00000921
Iteration 192/1000 | Loss: 0.00000921
Iteration 193/1000 | Loss: 0.00000920
Iteration 194/1000 | Loss: 0.00000920
Iteration 195/1000 | Loss: 0.00000920
Iteration 196/1000 | Loss: 0.00000920
Iteration 197/1000 | Loss: 0.00000920
Iteration 198/1000 | Loss: 0.00000920
Iteration 199/1000 | Loss: 0.00000920
Iteration 200/1000 | Loss: 0.00000919
Iteration 201/1000 | Loss: 0.00000919
Iteration 202/1000 | Loss: 0.00002317
Iteration 203/1000 | Loss: 0.00000919
Iteration 204/1000 | Loss: 0.00000916
Iteration 205/1000 | Loss: 0.00000916
Iteration 206/1000 | Loss: 0.00000916
Iteration 207/1000 | Loss: 0.00000916
Iteration 208/1000 | Loss: 0.00000916
Iteration 209/1000 | Loss: 0.00000916
Iteration 210/1000 | Loss: 0.00000916
Iteration 211/1000 | Loss: 0.00000916
Iteration 212/1000 | Loss: 0.00000916
Iteration 213/1000 | Loss: 0.00000916
Iteration 214/1000 | Loss: 0.00000916
Iteration 215/1000 | Loss: 0.00000915
Iteration 216/1000 | Loss: 0.00000915
Iteration 217/1000 | Loss: 0.00000915
Iteration 218/1000 | Loss: 0.00000915
Iteration 219/1000 | Loss: 0.00000915
Iteration 220/1000 | Loss: 0.00000915
Iteration 221/1000 | Loss: 0.00000915
Iteration 222/1000 | Loss: 0.00000915
Iteration 223/1000 | Loss: 0.00000915
Iteration 224/1000 | Loss: 0.00000915
Iteration 225/1000 | Loss: 0.00000915
Iteration 226/1000 | Loss: 0.00000915
Iteration 227/1000 | Loss: 0.00000915
Iteration 228/1000 | Loss: 0.00000915
Iteration 229/1000 | Loss: 0.00000915
Iteration 230/1000 | Loss: 0.00000915
Iteration 231/1000 | Loss: 0.00000915
Iteration 232/1000 | Loss: 0.00000915
Iteration 233/1000 | Loss: 0.00000915
Iteration 234/1000 | Loss: 0.00000915
Iteration 235/1000 | Loss: 0.00000915
Iteration 236/1000 | Loss: 0.00000915
Iteration 237/1000 | Loss: 0.00000915
Iteration 238/1000 | Loss: 0.00000915
Iteration 239/1000 | Loss: 0.00000915
Iteration 240/1000 | Loss: 0.00000915
Iteration 241/1000 | Loss: 0.00000915
Iteration 242/1000 | Loss: 0.00000915
Iteration 243/1000 | Loss: 0.00000915
Iteration 244/1000 | Loss: 0.00000915
Iteration 245/1000 | Loss: 0.00000915
Iteration 246/1000 | Loss: 0.00000915
Iteration 247/1000 | Loss: 0.00000915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [9.151537597062998e-06, 9.151537597062998e-06, 9.151537597062998e-06, 9.151537597062998e-06, 9.151537597062998e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.151537597062998e-06

Optimization complete. Final v2v error: 2.571974277496338 mm

Highest mean error: 3.941417694091797 mm for frame 219

Lowest mean error: 2.1910555362701416 mm for frame 217

Saving results

Total time: 246.3671839237213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445455
Iteration 2/25 | Loss: 0.00106405
Iteration 3/25 | Loss: 0.00095851
Iteration 4/25 | Loss: 0.00093871
Iteration 5/25 | Loss: 0.00093161
Iteration 6/25 | Loss: 0.00093004
Iteration 7/25 | Loss: 0.00092975
Iteration 8/25 | Loss: 0.00092975
Iteration 9/25 | Loss: 0.00092975
Iteration 10/25 | Loss: 0.00092975
Iteration 11/25 | Loss: 0.00092975
Iteration 12/25 | Loss: 0.00092975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000929746893234551, 0.000929746893234551, 0.000929746893234551, 0.000929746893234551, 0.000929746893234551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000929746893234551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48700523
Iteration 2/25 | Loss: 0.00062956
Iteration 3/25 | Loss: 0.00062956
Iteration 4/25 | Loss: 0.00062956
Iteration 5/25 | Loss: 0.00062956
Iteration 6/25 | Loss: 0.00062956
Iteration 7/25 | Loss: 0.00062956
Iteration 8/25 | Loss: 0.00062956
Iteration 9/25 | Loss: 0.00062956
Iteration 10/25 | Loss: 0.00062956
Iteration 11/25 | Loss: 0.00062956
Iteration 12/25 | Loss: 0.00062956
Iteration 13/25 | Loss: 0.00062956
Iteration 14/25 | Loss: 0.00062956
Iteration 15/25 | Loss: 0.00062956
Iteration 16/25 | Loss: 0.00062956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006295572384260595, 0.0006295572384260595, 0.0006295572384260595, 0.0006295572384260595, 0.0006295572384260595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006295572384260595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062956
Iteration 2/1000 | Loss: 0.00001863
Iteration 3/1000 | Loss: 0.00001364
Iteration 4/1000 | Loss: 0.00001269
Iteration 5/1000 | Loss: 0.00001193
Iteration 6/1000 | Loss: 0.00001160
Iteration 7/1000 | Loss: 0.00001114
Iteration 8/1000 | Loss: 0.00001098
Iteration 9/1000 | Loss: 0.00001093
Iteration 10/1000 | Loss: 0.00001092
Iteration 11/1000 | Loss: 0.00001091
Iteration 12/1000 | Loss: 0.00001085
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001075
Iteration 15/1000 | Loss: 0.00001075
Iteration 16/1000 | Loss: 0.00001075
Iteration 17/1000 | Loss: 0.00001075
Iteration 18/1000 | Loss: 0.00001074
Iteration 19/1000 | Loss: 0.00001071
Iteration 20/1000 | Loss: 0.00001071
Iteration 21/1000 | Loss: 0.00001070
Iteration 22/1000 | Loss: 0.00001069
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001069
Iteration 26/1000 | Loss: 0.00001069
Iteration 27/1000 | Loss: 0.00001069
Iteration 28/1000 | Loss: 0.00001069
Iteration 29/1000 | Loss: 0.00001069
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001067
Iteration 33/1000 | Loss: 0.00001067
Iteration 34/1000 | Loss: 0.00001067
Iteration 35/1000 | Loss: 0.00001067
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001065
Iteration 47/1000 | Loss: 0.00001063
Iteration 48/1000 | Loss: 0.00001063
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001062
Iteration 51/1000 | Loss: 0.00001062
Iteration 52/1000 | Loss: 0.00001062
Iteration 53/1000 | Loss: 0.00001062
Iteration 54/1000 | Loss: 0.00001061
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001061
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001060
Iteration 60/1000 | Loss: 0.00001059
Iteration 61/1000 | Loss: 0.00001059
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001058
Iteration 65/1000 | Loss: 0.00001058
Iteration 66/1000 | Loss: 0.00001058
Iteration 67/1000 | Loss: 0.00001058
Iteration 68/1000 | Loss: 0.00001058
Iteration 69/1000 | Loss: 0.00001058
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001057
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001057
Iteration 77/1000 | Loss: 0.00001057
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001057
Iteration 80/1000 | Loss: 0.00001057
Iteration 81/1000 | Loss: 0.00001057
Iteration 82/1000 | Loss: 0.00001057
Iteration 83/1000 | Loss: 0.00001057
Iteration 84/1000 | Loss: 0.00001057
Iteration 85/1000 | Loss: 0.00001057
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001057
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.0573021427262574e-05, 1.0573021427262574e-05, 1.0573021427262574e-05, 1.0573021427262574e-05, 1.0573021427262574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0573021427262574e-05

Optimization complete. Final v2v error: 2.7820119857788086 mm

Highest mean error: 3.1383941173553467 mm for frame 113

Lowest mean error: 2.5627341270446777 mm for frame 95

Saving results

Total time: 28.152015209197998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393432
Iteration 2/25 | Loss: 0.00108326
Iteration 3/25 | Loss: 0.00094289
Iteration 4/25 | Loss: 0.00093307
Iteration 5/25 | Loss: 0.00093003
Iteration 6/25 | Loss: 0.00092976
Iteration 7/25 | Loss: 0.00092976
Iteration 8/25 | Loss: 0.00092976
Iteration 9/25 | Loss: 0.00092976
Iteration 10/25 | Loss: 0.00092976
Iteration 11/25 | Loss: 0.00092976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009297578944824636, 0.0009297578944824636, 0.0009297578944824636, 0.0009297578944824636, 0.0009297578944824636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009297578944824636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61679173
Iteration 2/25 | Loss: 0.00051689
Iteration 3/25 | Loss: 0.00051689
Iteration 4/25 | Loss: 0.00051689
Iteration 5/25 | Loss: 0.00051689
Iteration 6/25 | Loss: 0.00051689
Iteration 7/25 | Loss: 0.00051689
Iteration 8/25 | Loss: 0.00051689
Iteration 9/25 | Loss: 0.00051689
Iteration 10/25 | Loss: 0.00051689
Iteration 11/25 | Loss: 0.00051689
Iteration 12/25 | Loss: 0.00051689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005168915376998484, 0.0005168915376998484, 0.0005168915376998484, 0.0005168915376998484, 0.0005168915376998484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005168915376998484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051689
Iteration 2/1000 | Loss: 0.00002027
Iteration 3/1000 | Loss: 0.00001148
Iteration 4/1000 | Loss: 0.00001003
Iteration 5/1000 | Loss: 0.00000951
Iteration 6/1000 | Loss: 0.00000926
Iteration 7/1000 | Loss: 0.00000922
Iteration 8/1000 | Loss: 0.00000895
Iteration 9/1000 | Loss: 0.00000880
Iteration 10/1000 | Loss: 0.00000871
Iteration 11/1000 | Loss: 0.00000871
Iteration 12/1000 | Loss: 0.00000861
Iteration 13/1000 | Loss: 0.00000850
Iteration 14/1000 | Loss: 0.00000849
Iteration 15/1000 | Loss: 0.00000849
Iteration 16/1000 | Loss: 0.00000849
Iteration 17/1000 | Loss: 0.00000849
Iteration 18/1000 | Loss: 0.00000848
Iteration 19/1000 | Loss: 0.00000848
Iteration 20/1000 | Loss: 0.00000848
Iteration 21/1000 | Loss: 0.00000847
Iteration 22/1000 | Loss: 0.00000847
Iteration 23/1000 | Loss: 0.00000846
Iteration 24/1000 | Loss: 0.00000845
Iteration 25/1000 | Loss: 0.00000845
Iteration 26/1000 | Loss: 0.00000845
Iteration 27/1000 | Loss: 0.00000845
Iteration 28/1000 | Loss: 0.00000844
Iteration 29/1000 | Loss: 0.00000844
Iteration 30/1000 | Loss: 0.00000844
Iteration 31/1000 | Loss: 0.00000844
Iteration 32/1000 | Loss: 0.00000844
Iteration 33/1000 | Loss: 0.00000844
Iteration 34/1000 | Loss: 0.00000844
Iteration 35/1000 | Loss: 0.00000844
Iteration 36/1000 | Loss: 0.00000844
Iteration 37/1000 | Loss: 0.00000844
Iteration 38/1000 | Loss: 0.00000844
Iteration 39/1000 | Loss: 0.00000843
Iteration 40/1000 | Loss: 0.00000843
Iteration 41/1000 | Loss: 0.00000843
Iteration 42/1000 | Loss: 0.00000843
Iteration 43/1000 | Loss: 0.00000842
Iteration 44/1000 | Loss: 0.00000842
Iteration 45/1000 | Loss: 0.00000841
Iteration 46/1000 | Loss: 0.00000841
Iteration 47/1000 | Loss: 0.00000841
Iteration 48/1000 | Loss: 0.00000841
Iteration 49/1000 | Loss: 0.00000840
Iteration 50/1000 | Loss: 0.00000840
Iteration 51/1000 | Loss: 0.00000839
Iteration 52/1000 | Loss: 0.00000839
Iteration 53/1000 | Loss: 0.00000839
Iteration 54/1000 | Loss: 0.00000839
Iteration 55/1000 | Loss: 0.00000839
Iteration 56/1000 | Loss: 0.00000839
Iteration 57/1000 | Loss: 0.00000839
Iteration 58/1000 | Loss: 0.00000839
Iteration 59/1000 | Loss: 0.00000838
Iteration 60/1000 | Loss: 0.00000838
Iteration 61/1000 | Loss: 0.00000837
Iteration 62/1000 | Loss: 0.00000837
Iteration 63/1000 | Loss: 0.00000837
Iteration 64/1000 | Loss: 0.00000837
Iteration 65/1000 | Loss: 0.00000837
Iteration 66/1000 | Loss: 0.00000837
Iteration 67/1000 | Loss: 0.00000837
Iteration 68/1000 | Loss: 0.00000836
Iteration 69/1000 | Loss: 0.00000836
Iteration 70/1000 | Loss: 0.00000836
Iteration 71/1000 | Loss: 0.00000835
Iteration 72/1000 | Loss: 0.00000835
Iteration 73/1000 | Loss: 0.00000835
Iteration 74/1000 | Loss: 0.00000834
Iteration 75/1000 | Loss: 0.00000834
Iteration 76/1000 | Loss: 0.00000834
Iteration 77/1000 | Loss: 0.00000834
Iteration 78/1000 | Loss: 0.00000834
Iteration 79/1000 | Loss: 0.00000834
Iteration 80/1000 | Loss: 0.00000834
Iteration 81/1000 | Loss: 0.00000834
Iteration 82/1000 | Loss: 0.00000834
Iteration 83/1000 | Loss: 0.00000834
Iteration 84/1000 | Loss: 0.00000834
Iteration 85/1000 | Loss: 0.00000834
Iteration 86/1000 | Loss: 0.00000834
Iteration 87/1000 | Loss: 0.00000834
Iteration 88/1000 | Loss: 0.00000834
Iteration 89/1000 | Loss: 0.00000834
Iteration 90/1000 | Loss: 0.00000834
Iteration 91/1000 | Loss: 0.00000834
Iteration 92/1000 | Loss: 0.00000834
Iteration 93/1000 | Loss: 0.00000834
Iteration 94/1000 | Loss: 0.00000834
Iteration 95/1000 | Loss: 0.00000834
Iteration 96/1000 | Loss: 0.00000834
Iteration 97/1000 | Loss: 0.00000834
Iteration 98/1000 | Loss: 0.00000834
Iteration 99/1000 | Loss: 0.00000834
Iteration 100/1000 | Loss: 0.00000834
Iteration 101/1000 | Loss: 0.00000834
Iteration 102/1000 | Loss: 0.00000834
Iteration 103/1000 | Loss: 0.00000834
Iteration 104/1000 | Loss: 0.00000834
Iteration 105/1000 | Loss: 0.00000834
Iteration 106/1000 | Loss: 0.00000834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [8.339400665136054e-06, 8.339400665136054e-06, 8.339400665136054e-06, 8.339400665136054e-06, 8.339400665136054e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.339400665136054e-06

Optimization complete. Final v2v error: 2.475224018096924 mm

Highest mean error: 2.7535295486450195 mm for frame 52

Lowest mean error: 2.256065607070923 mm for frame 191

Saving results

Total time: 29.836421251296997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098929
Iteration 2/25 | Loss: 0.00230101
Iteration 3/25 | Loss: 0.00177171
Iteration 4/25 | Loss: 0.00155977
Iteration 5/25 | Loss: 0.00128480
Iteration 6/25 | Loss: 0.00131183
Iteration 7/25 | Loss: 0.00138149
Iteration 8/25 | Loss: 0.00126500
Iteration 9/25 | Loss: 0.00113099
Iteration 10/25 | Loss: 0.00113520
Iteration 11/25 | Loss: 0.00107086
Iteration 12/25 | Loss: 0.00105666
Iteration 13/25 | Loss: 0.00104683
Iteration 14/25 | Loss: 0.00103543
Iteration 15/25 | Loss: 0.00102245
Iteration 16/25 | Loss: 0.00102213
Iteration 17/25 | Loss: 0.00102213
Iteration 18/25 | Loss: 0.00102057
Iteration 19/25 | Loss: 0.00101776
Iteration 20/25 | Loss: 0.00100885
Iteration 21/25 | Loss: 0.00101729
Iteration 22/25 | Loss: 0.00100760
Iteration 23/25 | Loss: 0.00100175
Iteration 24/25 | Loss: 0.00099535
Iteration 25/25 | Loss: 0.00099294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42114949
Iteration 2/25 | Loss: 0.00083180
Iteration 3/25 | Loss: 0.00083179
Iteration 4/25 | Loss: 0.00083179
Iteration 5/25 | Loss: 0.00083179
Iteration 6/25 | Loss: 0.00083179
Iteration 7/25 | Loss: 0.00083179
Iteration 8/25 | Loss: 0.00083179
Iteration 9/25 | Loss: 0.00083179
Iteration 10/25 | Loss: 0.00083179
Iteration 11/25 | Loss: 0.00083179
Iteration 12/25 | Loss: 0.00083179
Iteration 13/25 | Loss: 0.00083179
Iteration 14/25 | Loss: 0.00083179
Iteration 15/25 | Loss: 0.00083179
Iteration 16/25 | Loss: 0.00083179
Iteration 17/25 | Loss: 0.00083179
Iteration 18/25 | Loss: 0.00083179
Iteration 19/25 | Loss: 0.00083179
Iteration 20/25 | Loss: 0.00083179
Iteration 21/25 | Loss: 0.00083179
Iteration 22/25 | Loss: 0.00083179
Iteration 23/25 | Loss: 0.00083179
Iteration 24/25 | Loss: 0.00083179
Iteration 25/25 | Loss: 0.00083179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083179
Iteration 2/1000 | Loss: 0.00018510
Iteration 3/1000 | Loss: 0.00177818
Iteration 4/1000 | Loss: 0.00074350
Iteration 5/1000 | Loss: 0.00074876
Iteration 6/1000 | Loss: 0.00072195
Iteration 7/1000 | Loss: 0.00054089
Iteration 8/1000 | Loss: 0.00048932
Iteration 9/1000 | Loss: 0.00027014
Iteration 10/1000 | Loss: 0.00024424
Iteration 11/1000 | Loss: 0.00035385
Iteration 12/1000 | Loss: 0.00023562
Iteration 13/1000 | Loss: 0.00034937
Iteration 14/1000 | Loss: 0.00036250
Iteration 15/1000 | Loss: 0.00029264
Iteration 16/1000 | Loss: 0.00062558
Iteration 17/1000 | Loss: 0.00025322
Iteration 18/1000 | Loss: 0.00019481
Iteration 19/1000 | Loss: 0.00032700
Iteration 20/1000 | Loss: 0.00030429
Iteration 21/1000 | Loss: 0.00012817
Iteration 22/1000 | Loss: 0.00016115
Iteration 23/1000 | Loss: 0.00035987
Iteration 24/1000 | Loss: 0.00011487
Iteration 25/1000 | Loss: 0.00017240
Iteration 26/1000 | Loss: 0.00019152
Iteration 27/1000 | Loss: 0.00025929
Iteration 28/1000 | Loss: 0.00022373
Iteration 29/1000 | Loss: 0.00024040
Iteration 30/1000 | Loss: 0.00018945
Iteration 31/1000 | Loss: 0.00026860
Iteration 32/1000 | Loss: 0.00018689
Iteration 33/1000 | Loss: 0.00025893
Iteration 34/1000 | Loss: 0.00036298
Iteration 35/1000 | Loss: 0.00031201
Iteration 36/1000 | Loss: 0.00029788
Iteration 37/1000 | Loss: 0.00028082
Iteration 38/1000 | Loss: 0.00031923
Iteration 39/1000 | Loss: 0.00041656
Iteration 40/1000 | Loss: 0.00026305
Iteration 41/1000 | Loss: 0.00021411
Iteration 42/1000 | Loss: 0.00036523
Iteration 43/1000 | Loss: 0.00030609
Iteration 44/1000 | Loss: 0.00019936
Iteration 45/1000 | Loss: 0.00034596
Iteration 46/1000 | Loss: 0.00038895
Iteration 47/1000 | Loss: 0.00025599
Iteration 48/1000 | Loss: 0.00035041
Iteration 49/1000 | Loss: 0.00036064
Iteration 50/1000 | Loss: 0.00033012
Iteration 51/1000 | Loss: 0.00057390
Iteration 52/1000 | Loss: 0.00029597
Iteration 53/1000 | Loss: 0.00034419
Iteration 54/1000 | Loss: 0.00019836
Iteration 55/1000 | Loss: 0.00031416
Iteration 56/1000 | Loss: 0.00021441
Iteration 57/1000 | Loss: 0.00028602
Iteration 58/1000 | Loss: 0.00025924
Iteration 59/1000 | Loss: 0.00017318
Iteration 60/1000 | Loss: 0.00024871
Iteration 61/1000 | Loss: 0.00019766
Iteration 62/1000 | Loss: 0.00023456
Iteration 63/1000 | Loss: 0.00039937
Iteration 64/1000 | Loss: 0.00049918
Iteration 65/1000 | Loss: 0.00019120
Iteration 66/1000 | Loss: 0.00026824
Iteration 67/1000 | Loss: 0.00037738
Iteration 68/1000 | Loss: 0.00063871
Iteration 69/1000 | Loss: 0.00048784
Iteration 70/1000 | Loss: 0.00066521
Iteration 71/1000 | Loss: 0.00055527
Iteration 72/1000 | Loss: 0.00034865
Iteration 73/1000 | Loss: 0.00028310
Iteration 74/1000 | Loss: 0.00080819
Iteration 75/1000 | Loss: 0.00084679
Iteration 76/1000 | Loss: 0.00120557
Iteration 77/1000 | Loss: 0.00029398
Iteration 78/1000 | Loss: 0.00007786
Iteration 79/1000 | Loss: 0.00019821
Iteration 80/1000 | Loss: 0.00004374
Iteration 81/1000 | Loss: 0.00003766
Iteration 82/1000 | Loss: 0.00128846
Iteration 83/1000 | Loss: 0.00080028
Iteration 84/1000 | Loss: 0.00006294
Iteration 85/1000 | Loss: 0.00008256
Iteration 86/1000 | Loss: 0.00048426
Iteration 87/1000 | Loss: 0.00030356
Iteration 88/1000 | Loss: 0.00019532
Iteration 89/1000 | Loss: 0.00034354
Iteration 90/1000 | Loss: 0.00002566
Iteration 91/1000 | Loss: 0.00025980
Iteration 92/1000 | Loss: 0.00002818
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00003696
Iteration 95/1000 | Loss: 0.00022037
Iteration 96/1000 | Loss: 0.00002638
Iteration 97/1000 | Loss: 0.00025906
Iteration 98/1000 | Loss: 0.00003718
Iteration 99/1000 | Loss: 0.00016266
Iteration 100/1000 | Loss: 0.00011508
Iteration 101/1000 | Loss: 0.00013270
Iteration 102/1000 | Loss: 0.00010000
Iteration 103/1000 | Loss: 0.00007542
Iteration 104/1000 | Loss: 0.00029035
Iteration 105/1000 | Loss: 0.00010616
Iteration 106/1000 | Loss: 0.00019101
Iteration 107/1000 | Loss: 0.00009918
Iteration 108/1000 | Loss: 0.00006349
Iteration 109/1000 | Loss: 0.00016803
Iteration 110/1000 | Loss: 0.00020222
Iteration 111/1000 | Loss: 0.00016461
Iteration 112/1000 | Loss: 0.00018736
Iteration 113/1000 | Loss: 0.00002904
Iteration 114/1000 | Loss: 0.00003074
Iteration 115/1000 | Loss: 0.00002930
Iteration 116/1000 | Loss: 0.00014013
Iteration 117/1000 | Loss: 0.00052036
Iteration 118/1000 | Loss: 0.00004755
Iteration 119/1000 | Loss: 0.00003908
Iteration 120/1000 | Loss: 0.00003633
Iteration 121/1000 | Loss: 0.00003790
Iteration 122/1000 | Loss: 0.00003825
Iteration 123/1000 | Loss: 0.00007797
Iteration 124/1000 | Loss: 0.00008554
Iteration 125/1000 | Loss: 0.00007037
Iteration 126/1000 | Loss: 0.00005461
Iteration 127/1000 | Loss: 0.00002673
Iteration 128/1000 | Loss: 0.00002336
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001445
Iteration 131/1000 | Loss: 0.00001288
Iteration 132/1000 | Loss: 0.00001222
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001138
Iteration 137/1000 | Loss: 0.00001136
Iteration 138/1000 | Loss: 0.00001136
Iteration 139/1000 | Loss: 0.00001136
Iteration 140/1000 | Loss: 0.00001136
Iteration 141/1000 | Loss: 0.00001136
Iteration 142/1000 | Loss: 0.00001135
Iteration 143/1000 | Loss: 0.00001135
Iteration 144/1000 | Loss: 0.00001135
Iteration 145/1000 | Loss: 0.00001135
Iteration 146/1000 | Loss: 0.00001135
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001130
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001127
Iteration 156/1000 | Loss: 0.00001127
Iteration 157/1000 | Loss: 0.00001127
Iteration 158/1000 | Loss: 0.00001127
Iteration 159/1000 | Loss: 0.00001127
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001125
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001125
Iteration 171/1000 | Loss: 0.00001125
Iteration 172/1000 | Loss: 0.00001125
Iteration 173/1000 | Loss: 0.00001125
Iteration 174/1000 | Loss: 0.00001125
Iteration 175/1000 | Loss: 0.00001125
Iteration 176/1000 | Loss: 0.00001125
Iteration 177/1000 | Loss: 0.00001125
Iteration 178/1000 | Loss: 0.00001125
Iteration 179/1000 | Loss: 0.00001124
Iteration 180/1000 | Loss: 0.00001124
Iteration 181/1000 | Loss: 0.00001124
Iteration 182/1000 | Loss: 0.00001124
Iteration 183/1000 | Loss: 0.00001124
Iteration 184/1000 | Loss: 0.00001124
Iteration 185/1000 | Loss: 0.00001124
Iteration 186/1000 | Loss: 0.00001124
Iteration 187/1000 | Loss: 0.00001124
Iteration 188/1000 | Loss: 0.00001124
Iteration 189/1000 | Loss: 0.00001124
Iteration 190/1000 | Loss: 0.00001124
Iteration 191/1000 | Loss: 0.00001124
Iteration 192/1000 | Loss: 0.00001124
Iteration 193/1000 | Loss: 0.00001124
Iteration 194/1000 | Loss: 0.00001124
Iteration 195/1000 | Loss: 0.00001124
Iteration 196/1000 | Loss: 0.00001124
Iteration 197/1000 | Loss: 0.00001124
Iteration 198/1000 | Loss: 0.00001123
Iteration 199/1000 | Loss: 0.00001123
Iteration 200/1000 | Loss: 0.00001123
Iteration 201/1000 | Loss: 0.00001123
Iteration 202/1000 | Loss: 0.00001123
Iteration 203/1000 | Loss: 0.00001123
Iteration 204/1000 | Loss: 0.00001123
Iteration 205/1000 | Loss: 0.00001123
Iteration 206/1000 | Loss: 0.00001123
Iteration 207/1000 | Loss: 0.00001123
Iteration 208/1000 | Loss: 0.00001123
Iteration 209/1000 | Loss: 0.00001123
Iteration 210/1000 | Loss: 0.00001123
Iteration 211/1000 | Loss: 0.00001123
Iteration 212/1000 | Loss: 0.00001123
Iteration 213/1000 | Loss: 0.00001123
Iteration 214/1000 | Loss: 0.00001123
Iteration 215/1000 | Loss: 0.00001123
Iteration 216/1000 | Loss: 0.00001122
Iteration 217/1000 | Loss: 0.00001122
Iteration 218/1000 | Loss: 0.00001122
Iteration 219/1000 | Loss: 0.00001122
Iteration 220/1000 | Loss: 0.00001122
Iteration 221/1000 | Loss: 0.00001122
Iteration 222/1000 | Loss: 0.00001122
Iteration 223/1000 | Loss: 0.00001122
Iteration 224/1000 | Loss: 0.00001122
Iteration 225/1000 | Loss: 0.00001121
Iteration 226/1000 | Loss: 0.00001121
Iteration 227/1000 | Loss: 0.00001121
Iteration 228/1000 | Loss: 0.00001121
Iteration 229/1000 | Loss: 0.00001121
Iteration 230/1000 | Loss: 0.00001121
Iteration 231/1000 | Loss: 0.00001121
Iteration 232/1000 | Loss: 0.00001121
Iteration 233/1000 | Loss: 0.00001121
Iteration 234/1000 | Loss: 0.00001121
Iteration 235/1000 | Loss: 0.00001121
Iteration 236/1000 | Loss: 0.00001121
Iteration 237/1000 | Loss: 0.00001120
Iteration 238/1000 | Loss: 0.00001120
Iteration 239/1000 | Loss: 0.00001120
Iteration 240/1000 | Loss: 0.00001120
Iteration 241/1000 | Loss: 0.00001120
Iteration 242/1000 | Loss: 0.00001120
Iteration 243/1000 | Loss: 0.00001120
Iteration 244/1000 | Loss: 0.00001120
Iteration 245/1000 | Loss: 0.00001120
Iteration 246/1000 | Loss: 0.00001120
Iteration 247/1000 | Loss: 0.00001120
Iteration 248/1000 | Loss: 0.00001120
Iteration 249/1000 | Loss: 0.00001120
Iteration 250/1000 | Loss: 0.00001120
Iteration 251/1000 | Loss: 0.00001120
Iteration 252/1000 | Loss: 0.00001120
Iteration 253/1000 | Loss: 0.00001120
Iteration 254/1000 | Loss: 0.00001120
Iteration 255/1000 | Loss: 0.00001120
Iteration 256/1000 | Loss: 0.00001119
Iteration 257/1000 | Loss: 0.00001119
Iteration 258/1000 | Loss: 0.00001119
Iteration 259/1000 | Loss: 0.00001119
Iteration 260/1000 | Loss: 0.00001119
Iteration 261/1000 | Loss: 0.00001119
Iteration 262/1000 | Loss: 0.00001119
Iteration 263/1000 | Loss: 0.00001119
Iteration 264/1000 | Loss: 0.00001119
Iteration 265/1000 | Loss: 0.00001119
Iteration 266/1000 | Loss: 0.00001119
Iteration 267/1000 | Loss: 0.00001119
Iteration 268/1000 | Loss: 0.00001119
Iteration 269/1000 | Loss: 0.00001119
Iteration 270/1000 | Loss: 0.00001119
Iteration 271/1000 | Loss: 0.00001119
Iteration 272/1000 | Loss: 0.00001119
Iteration 273/1000 | Loss: 0.00001119
Iteration 274/1000 | Loss: 0.00001119
Iteration 275/1000 | Loss: 0.00001119
Iteration 276/1000 | Loss: 0.00001118
Iteration 277/1000 | Loss: 0.00001118
Iteration 278/1000 | Loss: 0.00001118
Iteration 279/1000 | Loss: 0.00001118
Iteration 280/1000 | Loss: 0.00001118
Iteration 281/1000 | Loss: 0.00001118
Iteration 282/1000 | Loss: 0.00001118
Iteration 283/1000 | Loss: 0.00001118
Iteration 284/1000 | Loss: 0.00001118
Iteration 285/1000 | Loss: 0.00001118
Iteration 286/1000 | Loss: 0.00001118
Iteration 287/1000 | Loss: 0.00001118
Iteration 288/1000 | Loss: 0.00001118
Iteration 289/1000 | Loss: 0.00001118
Iteration 290/1000 | Loss: 0.00001118
Iteration 291/1000 | Loss: 0.00001118
Iteration 292/1000 | Loss: 0.00001118
Iteration 293/1000 | Loss: 0.00001118
Iteration 294/1000 | Loss: 0.00001118
Iteration 295/1000 | Loss: 0.00001118
Iteration 296/1000 | Loss: 0.00001118
Iteration 297/1000 | Loss: 0.00001118
Iteration 298/1000 | Loss: 0.00001118
Iteration 299/1000 | Loss: 0.00001118
Iteration 300/1000 | Loss: 0.00001118
Iteration 301/1000 | Loss: 0.00001118
Iteration 302/1000 | Loss: 0.00001118
Iteration 303/1000 | Loss: 0.00001118
Iteration 304/1000 | Loss: 0.00001118
Iteration 305/1000 | Loss: 0.00001118
Iteration 306/1000 | Loss: 0.00001118
Iteration 307/1000 | Loss: 0.00001118
Iteration 308/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [1.1182694834133144e-05, 1.1182694834133144e-05, 1.1182694834133144e-05, 1.1182694834133144e-05, 1.1182694834133144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1182694834133144e-05

Optimization complete. Final v2v error: 2.8030545711517334 mm

Highest mean error: 3.9062533378601074 mm for frame 58

Lowest mean error: 2.5324463844299316 mm for frame 12

Saving results

Total time: 238.69756603240967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065996
Iteration 2/25 | Loss: 0.00198271
Iteration 3/25 | Loss: 0.00162983
Iteration 4/25 | Loss: 0.00151253
Iteration 5/25 | Loss: 0.00141938
Iteration 6/25 | Loss: 0.00125783
Iteration 7/25 | Loss: 0.00116997
Iteration 8/25 | Loss: 0.00114558
Iteration 9/25 | Loss: 0.00118979
Iteration 10/25 | Loss: 0.00113368
Iteration 11/25 | Loss: 0.00117604
Iteration 12/25 | Loss: 0.00113852
Iteration 13/25 | Loss: 0.00112304
Iteration 14/25 | Loss: 0.00111302
Iteration 15/25 | Loss: 0.00110550
Iteration 16/25 | Loss: 0.00110239
Iteration 17/25 | Loss: 0.00111264
Iteration 18/25 | Loss: 0.00110659
Iteration 19/25 | Loss: 0.00110035
Iteration 20/25 | Loss: 0.00109672
Iteration 21/25 | Loss: 0.00108809
Iteration 22/25 | Loss: 0.00109142
Iteration 23/25 | Loss: 0.00108524
Iteration 24/25 | Loss: 0.00107821
Iteration 25/25 | Loss: 0.00107973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32407916
Iteration 2/25 | Loss: 0.00146378
Iteration 3/25 | Loss: 0.00146378
Iteration 4/25 | Loss: 0.00146378
Iteration 5/25 | Loss: 0.00146378
Iteration 6/25 | Loss: 0.00146378
Iteration 7/25 | Loss: 0.00146378
Iteration 8/25 | Loss: 0.00146378
Iteration 9/25 | Loss: 0.00146378
Iteration 10/25 | Loss: 0.00146378
Iteration 11/25 | Loss: 0.00146378
Iteration 12/25 | Loss: 0.00146378
Iteration 13/25 | Loss: 0.00146378
Iteration 14/25 | Loss: 0.00146378
Iteration 15/25 | Loss: 0.00146378
Iteration 16/25 | Loss: 0.00146378
Iteration 17/25 | Loss: 0.00146378
Iteration 18/25 | Loss: 0.00146378
Iteration 19/25 | Loss: 0.00146378
Iteration 20/25 | Loss: 0.00146378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014637793647125363, 0.0014637793647125363, 0.0014637793647125363, 0.0014637793647125363, 0.0014637793647125363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014637793647125363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146378
Iteration 2/1000 | Loss: 0.00017358
Iteration 3/1000 | Loss: 0.00013966
Iteration 4/1000 | Loss: 0.00014044
Iteration 5/1000 | Loss: 0.00010898
Iteration 6/1000 | Loss: 0.00011359
Iteration 7/1000 | Loss: 0.00011745
Iteration 8/1000 | Loss: 0.00010637
Iteration 9/1000 | Loss: 0.00010610
Iteration 10/1000 | Loss: 0.00010143
Iteration 11/1000 | Loss: 0.00010237
Iteration 12/1000 | Loss: 0.00011033
Iteration 13/1000 | Loss: 0.00009021
Iteration 14/1000 | Loss: 0.00009180
Iteration 15/1000 | Loss: 0.00035837
Iteration 16/1000 | Loss: 0.00062367
Iteration 17/1000 | Loss: 0.00071102
Iteration 18/1000 | Loss: 0.00032314
Iteration 19/1000 | Loss: 0.00037852
Iteration 20/1000 | Loss: 0.00081272
Iteration 21/1000 | Loss: 0.00497735
Iteration 22/1000 | Loss: 0.00248157
Iteration 23/1000 | Loss: 0.00177848
Iteration 24/1000 | Loss: 0.00052661
Iteration 25/1000 | Loss: 0.00051582
Iteration 26/1000 | Loss: 0.00094053
Iteration 27/1000 | Loss: 0.00087292
Iteration 28/1000 | Loss: 0.00031269
Iteration 29/1000 | Loss: 0.00046160
Iteration 30/1000 | Loss: 0.00019892
Iteration 31/1000 | Loss: 0.00008685
Iteration 32/1000 | Loss: 0.00012792
Iteration 33/1000 | Loss: 0.00039737
Iteration 34/1000 | Loss: 0.00025399
Iteration 35/1000 | Loss: 0.00024468
Iteration 36/1000 | Loss: 0.00015032
Iteration 37/1000 | Loss: 0.00028358
Iteration 38/1000 | Loss: 0.00004609
Iteration 39/1000 | Loss: 0.00038978
Iteration 40/1000 | Loss: 0.00072516
Iteration 41/1000 | Loss: 0.00120515
Iteration 42/1000 | Loss: 0.00182930
Iteration 43/1000 | Loss: 0.00007823
Iteration 44/1000 | Loss: 0.00019847
Iteration 45/1000 | Loss: 0.00003249
Iteration 46/1000 | Loss: 0.00002873
Iteration 47/1000 | Loss: 0.00079565
Iteration 48/1000 | Loss: 0.00056450
Iteration 49/1000 | Loss: 0.00143833
Iteration 50/1000 | Loss: 0.00008842
Iteration 51/1000 | Loss: 0.00002873
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001378
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001174
Iteration 57/1000 | Loss: 0.00001094
Iteration 58/1000 | Loss: 0.00002095
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001009
Iteration 63/1000 | Loss: 0.00001057
Iteration 64/1000 | Loss: 0.00000993
Iteration 65/1000 | Loss: 0.00000991
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000990
Iteration 68/1000 | Loss: 0.00000983
Iteration 69/1000 | Loss: 0.00000960
Iteration 70/1000 | Loss: 0.00000957
Iteration 71/1000 | Loss: 0.00000956
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000954
Iteration 80/1000 | Loss: 0.00000954
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000954
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000953
Iteration 88/1000 | Loss: 0.00000953
Iteration 89/1000 | Loss: 0.00000953
Iteration 90/1000 | Loss: 0.00000953
Iteration 91/1000 | Loss: 0.00000953
Iteration 92/1000 | Loss: 0.00000953
Iteration 93/1000 | Loss: 0.00000953
Iteration 94/1000 | Loss: 0.00000953
Iteration 95/1000 | Loss: 0.00000953
Iteration 96/1000 | Loss: 0.00000953
Iteration 97/1000 | Loss: 0.00000952
Iteration 98/1000 | Loss: 0.00000952
Iteration 99/1000 | Loss: 0.00000952
Iteration 100/1000 | Loss: 0.00000952
Iteration 101/1000 | Loss: 0.00000952
Iteration 102/1000 | Loss: 0.00000952
Iteration 103/1000 | Loss: 0.00000952
Iteration 104/1000 | Loss: 0.00000952
Iteration 105/1000 | Loss: 0.00000952
Iteration 106/1000 | Loss: 0.00000952
Iteration 107/1000 | Loss: 0.00000952
Iteration 108/1000 | Loss: 0.00000952
Iteration 109/1000 | Loss: 0.00000952
Iteration 110/1000 | Loss: 0.00000952
Iteration 111/1000 | Loss: 0.00000952
Iteration 112/1000 | Loss: 0.00000951
Iteration 113/1000 | Loss: 0.00000951
Iteration 114/1000 | Loss: 0.00000951
Iteration 115/1000 | Loss: 0.00000951
Iteration 116/1000 | Loss: 0.00000951
Iteration 117/1000 | Loss: 0.00000950
Iteration 118/1000 | Loss: 0.00000949
Iteration 119/1000 | Loss: 0.00000949
Iteration 120/1000 | Loss: 0.00000949
Iteration 121/1000 | Loss: 0.00000949
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000948
Iteration 127/1000 | Loss: 0.00000948
Iteration 128/1000 | Loss: 0.00000948
Iteration 129/1000 | Loss: 0.00000948
Iteration 130/1000 | Loss: 0.00000948
Iteration 131/1000 | Loss: 0.00000947
Iteration 132/1000 | Loss: 0.00000947
Iteration 133/1000 | Loss: 0.00000947
Iteration 134/1000 | Loss: 0.00000947
Iteration 135/1000 | Loss: 0.00000947
Iteration 136/1000 | Loss: 0.00000947
Iteration 137/1000 | Loss: 0.00000947
Iteration 138/1000 | Loss: 0.00000947
Iteration 139/1000 | Loss: 0.00000947
Iteration 140/1000 | Loss: 0.00000946
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000946
Iteration 144/1000 | Loss: 0.00000946
Iteration 145/1000 | Loss: 0.00000946
Iteration 146/1000 | Loss: 0.00000946
Iteration 147/1000 | Loss: 0.00000946
Iteration 148/1000 | Loss: 0.00000946
Iteration 149/1000 | Loss: 0.00000946
Iteration 150/1000 | Loss: 0.00000946
Iteration 151/1000 | Loss: 0.00000945
Iteration 152/1000 | Loss: 0.00000945
Iteration 153/1000 | Loss: 0.00000945
Iteration 154/1000 | Loss: 0.00000945
Iteration 155/1000 | Loss: 0.00000945
Iteration 156/1000 | Loss: 0.00000945
Iteration 157/1000 | Loss: 0.00000945
Iteration 158/1000 | Loss: 0.00000945
Iteration 159/1000 | Loss: 0.00000945
Iteration 160/1000 | Loss: 0.00000945
Iteration 161/1000 | Loss: 0.00000945
Iteration 162/1000 | Loss: 0.00000945
Iteration 163/1000 | Loss: 0.00000944
Iteration 164/1000 | Loss: 0.00000944
Iteration 165/1000 | Loss: 0.00000944
Iteration 166/1000 | Loss: 0.00000944
Iteration 167/1000 | Loss: 0.00000944
Iteration 168/1000 | Loss: 0.00000944
Iteration 169/1000 | Loss: 0.00000944
Iteration 170/1000 | Loss: 0.00000944
Iteration 171/1000 | Loss: 0.00000944
Iteration 172/1000 | Loss: 0.00000944
Iteration 173/1000 | Loss: 0.00000944
Iteration 174/1000 | Loss: 0.00000944
Iteration 175/1000 | Loss: 0.00000944
Iteration 176/1000 | Loss: 0.00000944
Iteration 177/1000 | Loss: 0.00000944
Iteration 178/1000 | Loss: 0.00000943
Iteration 179/1000 | Loss: 0.00000943
Iteration 180/1000 | Loss: 0.00000943
Iteration 181/1000 | Loss: 0.00000943
Iteration 182/1000 | Loss: 0.00000943
Iteration 183/1000 | Loss: 0.00000943
Iteration 184/1000 | Loss: 0.00000943
Iteration 185/1000 | Loss: 0.00000943
Iteration 186/1000 | Loss: 0.00000943
Iteration 187/1000 | Loss: 0.00000943
Iteration 188/1000 | Loss: 0.00000942
Iteration 189/1000 | Loss: 0.00000942
Iteration 190/1000 | Loss: 0.00000942
Iteration 191/1000 | Loss: 0.00000942
Iteration 192/1000 | Loss: 0.00000942
Iteration 193/1000 | Loss: 0.00000942
Iteration 194/1000 | Loss: 0.00000942
Iteration 195/1000 | Loss: 0.00000942
Iteration 196/1000 | Loss: 0.00000942
Iteration 197/1000 | Loss: 0.00000942
Iteration 198/1000 | Loss: 0.00000942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [9.424788913747761e-06, 9.424788913747761e-06, 9.424788913747761e-06, 9.424788913747761e-06, 9.424788913747761e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.424788913747761e-06

Optimization complete. Final v2v error: 2.446218252182007 mm

Highest mean error: 9.655570983886719 mm for frame 112

Lowest mean error: 2.181072235107422 mm for frame 106

Saving results

Total time: 140.0129954814911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903059
Iteration 2/25 | Loss: 0.00210749
Iteration 3/25 | Loss: 0.00126603
Iteration 4/25 | Loss: 0.00114433
Iteration 5/25 | Loss: 0.00112238
Iteration 6/25 | Loss: 0.00110533
Iteration 7/25 | Loss: 0.00110391
Iteration 8/25 | Loss: 0.00110304
Iteration 9/25 | Loss: 0.00109917
Iteration 10/25 | Loss: 0.00109057
Iteration 11/25 | Loss: 0.00111467
Iteration 12/25 | Loss: 0.00108678
Iteration 13/25 | Loss: 0.00107738
Iteration 14/25 | Loss: 0.00107017
Iteration 15/25 | Loss: 0.00106714
Iteration 16/25 | Loss: 0.00106815
Iteration 17/25 | Loss: 0.00106463
Iteration 18/25 | Loss: 0.00106526
Iteration 19/25 | Loss: 0.00106365
Iteration 20/25 | Loss: 0.00106191
Iteration 21/25 | Loss: 0.00106151
Iteration 22/25 | Loss: 0.00106036
Iteration 23/25 | Loss: 0.00106070
Iteration 24/25 | Loss: 0.00106021
Iteration 25/25 | Loss: 0.00106021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42628336
Iteration 2/25 | Loss: 0.00029923
Iteration 3/25 | Loss: 0.00029923
Iteration 4/25 | Loss: 0.00029923
Iteration 5/25 | Loss: 0.00029923
Iteration 6/25 | Loss: 0.00029923
Iteration 7/25 | Loss: 0.00029923
Iteration 8/25 | Loss: 0.00029923
Iteration 9/25 | Loss: 0.00029923
Iteration 10/25 | Loss: 0.00029923
Iteration 11/25 | Loss: 0.00029923
Iteration 12/25 | Loss: 0.00029923
Iteration 13/25 | Loss: 0.00029923
Iteration 14/25 | Loss: 0.00029923
Iteration 15/25 | Loss: 0.00029923
Iteration 16/25 | Loss: 0.00029923
Iteration 17/25 | Loss: 0.00029923
Iteration 18/25 | Loss: 0.00029923
Iteration 19/25 | Loss: 0.00029923
Iteration 20/25 | Loss: 0.00029923
Iteration 21/25 | Loss: 0.00029923
Iteration 22/25 | Loss: 0.00029923
Iteration 23/25 | Loss: 0.00029923
Iteration 24/25 | Loss: 0.00029923
Iteration 25/25 | Loss: 0.00029923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029923
Iteration 2/1000 | Loss: 0.00003268
Iteration 3/1000 | Loss: 0.00002468
Iteration 4/1000 | Loss: 0.00002333
Iteration 5/1000 | Loss: 0.00002242
Iteration 6/1000 | Loss: 0.00002175
Iteration 7/1000 | Loss: 0.00002130
Iteration 8/1000 | Loss: 0.00015229
Iteration 9/1000 | Loss: 0.00012859
Iteration 10/1000 | Loss: 0.00002691
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002094
Iteration 13/1000 | Loss: 0.00002062
Iteration 14/1000 | Loss: 0.00002062
Iteration 15/1000 | Loss: 0.00015632
Iteration 16/1000 | Loss: 0.00005524
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00014863
Iteration 19/1000 | Loss: 0.00016508
Iteration 20/1000 | Loss: 0.00024245
Iteration 21/1000 | Loss: 0.00003348
Iteration 22/1000 | Loss: 0.00011000
Iteration 23/1000 | Loss: 0.00015552
Iteration 24/1000 | Loss: 0.00002867
Iteration 25/1000 | Loss: 0.00002873
Iteration 26/1000 | Loss: 0.00002215
Iteration 27/1000 | Loss: 0.00002034
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001948
Iteration 31/1000 | Loss: 0.00001933
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001919
Iteration 35/1000 | Loss: 0.00001919
Iteration 36/1000 | Loss: 0.00001916
Iteration 37/1000 | Loss: 0.00001915
Iteration 38/1000 | Loss: 0.00001915
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001914
Iteration 41/1000 | Loss: 0.00001914
Iteration 42/1000 | Loss: 0.00001914
Iteration 43/1000 | Loss: 0.00001914
Iteration 44/1000 | Loss: 0.00001914
Iteration 45/1000 | Loss: 0.00001912
Iteration 46/1000 | Loss: 0.00001912
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00003379
Iteration 58/1000 | Loss: 0.00002193
Iteration 59/1000 | Loss: 0.00002785
Iteration 60/1000 | Loss: 0.00003264
Iteration 61/1000 | Loss: 0.00002759
Iteration 62/1000 | Loss: 0.00003155
Iteration 63/1000 | Loss: 0.00002594
Iteration 64/1000 | Loss: 0.00002091
Iteration 65/1000 | Loss: 0.00002132
Iteration 66/1000 | Loss: 0.00001898
Iteration 67/1000 | Loss: 0.00001878
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Iteration 100/1000 | Loss: 0.00001856
Iteration 101/1000 | Loss: 0.00001856
Iteration 102/1000 | Loss: 0.00001856
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001855
Iteration 105/1000 | Loss: 0.00001855
Iteration 106/1000 | Loss: 0.00001855
Iteration 107/1000 | Loss: 0.00001855
Iteration 108/1000 | Loss: 0.00001855
Iteration 109/1000 | Loss: 0.00001855
Iteration 110/1000 | Loss: 0.00001855
Iteration 111/1000 | Loss: 0.00001855
Iteration 112/1000 | Loss: 0.00001855
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001854
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001854
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001853
Iteration 122/1000 | Loss: 0.00001853
Iteration 123/1000 | Loss: 0.00001853
Iteration 124/1000 | Loss: 0.00001853
Iteration 125/1000 | Loss: 0.00001853
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001852
Iteration 129/1000 | Loss: 0.00001852
Iteration 130/1000 | Loss: 0.00001852
Iteration 131/1000 | Loss: 0.00001852
Iteration 132/1000 | Loss: 0.00001852
Iteration 133/1000 | Loss: 0.00001852
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.8518401702749543e-05, 1.8518401702749543e-05, 1.8518401702749543e-05, 1.8518401702749543e-05, 1.8518401702749543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8518401702749543e-05

Optimization complete. Final v2v error: 3.4517669677734375 mm

Highest mean error: 5.246161937713623 mm for frame 9

Lowest mean error: 2.959031105041504 mm for frame 205

Saving results

Total time: 121.58123898506165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896175
Iteration 2/25 | Loss: 0.00153842
Iteration 3/25 | Loss: 0.00122964
Iteration 4/25 | Loss: 0.00118235
Iteration 5/25 | Loss: 0.00115054
Iteration 6/25 | Loss: 0.00113488
Iteration 7/25 | Loss: 0.00111755
Iteration 8/25 | Loss: 0.00110992
Iteration 9/25 | Loss: 0.00110191
Iteration 10/25 | Loss: 0.00110083
Iteration 11/25 | Loss: 0.00109648
Iteration 12/25 | Loss: 0.00109414
Iteration 13/25 | Loss: 0.00109368
Iteration 14/25 | Loss: 0.00109619
Iteration 15/25 | Loss: 0.00109248
Iteration 16/25 | Loss: 0.00109047
Iteration 17/25 | Loss: 0.00108793
Iteration 18/25 | Loss: 0.00108741
Iteration 19/25 | Loss: 0.00108839
Iteration 20/25 | Loss: 0.00108680
Iteration 21/25 | Loss: 0.00109179
Iteration 22/25 | Loss: 0.00108746
Iteration 23/25 | Loss: 0.00107921
Iteration 24/25 | Loss: 0.00107554
Iteration 25/25 | Loss: 0.00107384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71896827
Iteration 2/25 | Loss: 0.00080294
Iteration 3/25 | Loss: 0.00080294
Iteration 4/25 | Loss: 0.00080294
Iteration 5/25 | Loss: 0.00080294
Iteration 6/25 | Loss: 0.00080294
Iteration 7/25 | Loss: 0.00080294
Iteration 8/25 | Loss: 0.00080294
Iteration 9/25 | Loss: 0.00080294
Iteration 10/25 | Loss: 0.00080294
Iteration 11/25 | Loss: 0.00080294
Iteration 12/25 | Loss: 0.00080294
Iteration 13/25 | Loss: 0.00080294
Iteration 14/25 | Loss: 0.00080294
Iteration 15/25 | Loss: 0.00080294
Iteration 16/25 | Loss: 0.00080294
Iteration 17/25 | Loss: 0.00080294
Iteration 18/25 | Loss: 0.00080294
Iteration 19/25 | Loss: 0.00080294
Iteration 20/25 | Loss: 0.00080294
Iteration 21/25 | Loss: 0.00080294
Iteration 22/25 | Loss: 0.00080294
Iteration 23/25 | Loss: 0.00080294
Iteration 24/25 | Loss: 0.00080294
Iteration 25/25 | Loss: 0.00080294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080294
Iteration 2/1000 | Loss: 0.00010490
Iteration 3/1000 | Loss: 0.00015582
Iteration 4/1000 | Loss: 0.00004599
Iteration 5/1000 | Loss: 0.00010991
Iteration 6/1000 | Loss: 0.00163197
Iteration 7/1000 | Loss: 0.00058333
Iteration 8/1000 | Loss: 0.00135985
Iteration 9/1000 | Loss: 0.00048170
Iteration 10/1000 | Loss: 0.00037503
Iteration 11/1000 | Loss: 0.00026640
Iteration 12/1000 | Loss: 0.00005762
Iteration 13/1000 | Loss: 0.00004430
Iteration 14/1000 | Loss: 0.00012587
Iteration 15/1000 | Loss: 0.00006591
Iteration 16/1000 | Loss: 0.00008755
Iteration 17/1000 | Loss: 0.00073642
Iteration 18/1000 | Loss: 0.00021643
Iteration 19/1000 | Loss: 0.00011241
Iteration 20/1000 | Loss: 0.00096249
Iteration 21/1000 | Loss: 0.00045844
Iteration 22/1000 | Loss: 0.00006883
Iteration 23/1000 | Loss: 0.00004842
Iteration 24/1000 | Loss: 0.00005735
Iteration 25/1000 | Loss: 0.00010355
Iteration 26/1000 | Loss: 0.00009282
Iteration 27/1000 | Loss: 0.00004770
Iteration 28/1000 | Loss: 0.00013776
Iteration 29/1000 | Loss: 0.00005935
Iteration 30/1000 | Loss: 0.00010398
Iteration 31/1000 | Loss: 0.00004059
Iteration 32/1000 | Loss: 0.00028367
Iteration 33/1000 | Loss: 0.00006584
Iteration 34/1000 | Loss: 0.00010055
Iteration 35/1000 | Loss: 0.00025026
Iteration 36/1000 | Loss: 0.00012290
Iteration 37/1000 | Loss: 0.00023768
Iteration 38/1000 | Loss: 0.00016201
Iteration 39/1000 | Loss: 0.00024972
Iteration 40/1000 | Loss: 0.00022414
Iteration 41/1000 | Loss: 0.00031939
Iteration 42/1000 | Loss: 0.00074527
Iteration 43/1000 | Loss: 0.00036235
Iteration 44/1000 | Loss: 0.00010242
Iteration 45/1000 | Loss: 0.00013808
Iteration 46/1000 | Loss: 0.00003685
Iteration 47/1000 | Loss: 0.00003319
Iteration 48/1000 | Loss: 0.00003118
Iteration 49/1000 | Loss: 0.00004654
Iteration 50/1000 | Loss: 0.00003017
Iteration 51/1000 | Loss: 0.00002936
Iteration 52/1000 | Loss: 0.00003475
Iteration 53/1000 | Loss: 0.00002900
Iteration 54/1000 | Loss: 0.00002830
Iteration 55/1000 | Loss: 0.00002790
Iteration 56/1000 | Loss: 0.00002755
Iteration 57/1000 | Loss: 0.00002664
Iteration 58/1000 | Loss: 0.00002520
Iteration 59/1000 | Loss: 0.00002408
Iteration 60/1000 | Loss: 0.00002336
Iteration 61/1000 | Loss: 0.00002303
Iteration 62/1000 | Loss: 0.00002281
Iteration 63/1000 | Loss: 0.00002280
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002248
Iteration 66/1000 | Loss: 0.00002247
Iteration 67/1000 | Loss: 0.00002245
Iteration 68/1000 | Loss: 0.00002245
Iteration 69/1000 | Loss: 0.00002245
Iteration 70/1000 | Loss: 0.00002245
Iteration 71/1000 | Loss: 0.00002245
Iteration 72/1000 | Loss: 0.00002244
Iteration 73/1000 | Loss: 0.00002244
Iteration 74/1000 | Loss: 0.00002244
Iteration 75/1000 | Loss: 0.00002244
Iteration 76/1000 | Loss: 0.00002244
Iteration 77/1000 | Loss: 0.00002244
Iteration 78/1000 | Loss: 0.00002243
Iteration 79/1000 | Loss: 0.00002243
Iteration 80/1000 | Loss: 0.00002243
Iteration 81/1000 | Loss: 0.00002243
Iteration 82/1000 | Loss: 0.00002243
Iteration 83/1000 | Loss: 0.00002243
Iteration 84/1000 | Loss: 0.00002243
Iteration 85/1000 | Loss: 0.00002243
Iteration 86/1000 | Loss: 0.00002243
Iteration 87/1000 | Loss: 0.00002242
Iteration 88/1000 | Loss: 0.00002242
Iteration 89/1000 | Loss: 0.00002242
Iteration 90/1000 | Loss: 0.00002241
Iteration 91/1000 | Loss: 0.00002241
Iteration 92/1000 | Loss: 0.00002241
Iteration 93/1000 | Loss: 0.00002241
Iteration 94/1000 | Loss: 0.00002241
Iteration 95/1000 | Loss: 0.00002240
Iteration 96/1000 | Loss: 0.00002240
Iteration 97/1000 | Loss: 0.00002240
Iteration 98/1000 | Loss: 0.00002240
Iteration 99/1000 | Loss: 0.00002240
Iteration 100/1000 | Loss: 0.00002240
Iteration 101/1000 | Loss: 0.00002239
Iteration 102/1000 | Loss: 0.00002239
Iteration 103/1000 | Loss: 0.00002239
Iteration 104/1000 | Loss: 0.00002239
Iteration 105/1000 | Loss: 0.00002239
Iteration 106/1000 | Loss: 0.00002239
Iteration 107/1000 | Loss: 0.00002239
Iteration 108/1000 | Loss: 0.00002239
Iteration 109/1000 | Loss: 0.00002238
Iteration 110/1000 | Loss: 0.00002238
Iteration 111/1000 | Loss: 0.00002238
Iteration 112/1000 | Loss: 0.00002238
Iteration 113/1000 | Loss: 0.00002238
Iteration 114/1000 | Loss: 0.00002238
Iteration 115/1000 | Loss: 0.00002238
Iteration 116/1000 | Loss: 0.00002238
Iteration 117/1000 | Loss: 0.00002237
Iteration 118/1000 | Loss: 0.00002237
Iteration 119/1000 | Loss: 0.00002237
Iteration 120/1000 | Loss: 0.00002237
Iteration 121/1000 | Loss: 0.00002236
Iteration 122/1000 | Loss: 0.00002236
Iteration 123/1000 | Loss: 0.00002236
Iteration 124/1000 | Loss: 0.00002235
Iteration 125/1000 | Loss: 0.00002235
Iteration 126/1000 | Loss: 0.00002235
Iteration 127/1000 | Loss: 0.00002235
Iteration 128/1000 | Loss: 0.00002235
Iteration 129/1000 | Loss: 0.00002235
Iteration 130/1000 | Loss: 0.00002235
Iteration 131/1000 | Loss: 0.00002235
Iteration 132/1000 | Loss: 0.00002235
Iteration 133/1000 | Loss: 0.00002235
Iteration 134/1000 | Loss: 0.00002235
Iteration 135/1000 | Loss: 0.00002235
Iteration 136/1000 | Loss: 0.00002235
Iteration 137/1000 | Loss: 0.00002235
Iteration 138/1000 | Loss: 0.00002235
Iteration 139/1000 | Loss: 0.00002235
Iteration 140/1000 | Loss: 0.00002235
Iteration 141/1000 | Loss: 0.00002235
Iteration 142/1000 | Loss: 0.00002235
Iteration 143/1000 | Loss: 0.00002235
Iteration 144/1000 | Loss: 0.00002235
Iteration 145/1000 | Loss: 0.00002235
Iteration 146/1000 | Loss: 0.00002235
Iteration 147/1000 | Loss: 0.00002235
Iteration 148/1000 | Loss: 0.00002235
Iteration 149/1000 | Loss: 0.00002235
Iteration 150/1000 | Loss: 0.00002235
Iteration 151/1000 | Loss: 0.00002235
Iteration 152/1000 | Loss: 0.00002235
Iteration 153/1000 | Loss: 0.00002235
Iteration 154/1000 | Loss: 0.00002235
Iteration 155/1000 | Loss: 0.00002235
Iteration 156/1000 | Loss: 0.00002235
Iteration 157/1000 | Loss: 0.00002235
Iteration 158/1000 | Loss: 0.00002235
Iteration 159/1000 | Loss: 0.00002235
Iteration 160/1000 | Loss: 0.00002235
Iteration 161/1000 | Loss: 0.00002235
Iteration 162/1000 | Loss: 0.00002235
Iteration 163/1000 | Loss: 0.00002235
Iteration 164/1000 | Loss: 0.00002235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.234844578197226e-05, 2.234844578197226e-05, 2.234844578197226e-05, 2.234844578197226e-05, 2.234844578197226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.234844578197226e-05

Optimization complete. Final v2v error: 3.9912779331207275 mm

Highest mean error: 6.3573760986328125 mm for frame 48

Lowest mean error: 3.205857753753662 mm for frame 1

Saving results

Total time: 158.2762427330017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090278
Iteration 2/25 | Loss: 0.00302712
Iteration 3/25 | Loss: 0.00256727
Iteration 4/25 | Loss: 0.00229362
Iteration 5/25 | Loss: 0.00214759
Iteration 6/25 | Loss: 0.00196853
Iteration 7/25 | Loss: 0.00188375
Iteration 8/25 | Loss: 0.00175432
Iteration 9/25 | Loss: 0.00163793
Iteration 10/25 | Loss: 0.00155782
Iteration 11/25 | Loss: 0.00148688
Iteration 12/25 | Loss: 0.00145162
Iteration 13/25 | Loss: 0.00141937
Iteration 14/25 | Loss: 0.00141152
Iteration 15/25 | Loss: 0.00138765
Iteration 16/25 | Loss: 0.00137919
Iteration 17/25 | Loss: 0.00137688
Iteration 18/25 | Loss: 0.00137988
Iteration 19/25 | Loss: 0.00137969
Iteration 20/25 | Loss: 0.00137100
Iteration 21/25 | Loss: 0.00136490
Iteration 22/25 | Loss: 0.00136348
Iteration 23/25 | Loss: 0.00136314
Iteration 24/25 | Loss: 0.00136300
Iteration 25/25 | Loss: 0.00136295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30980504
Iteration 2/25 | Loss: 0.00345970
Iteration 3/25 | Loss: 0.00345970
Iteration 4/25 | Loss: 0.00345970
Iteration 5/25 | Loss: 0.00345970
Iteration 6/25 | Loss: 0.00345970
Iteration 7/25 | Loss: 0.00345970
Iteration 8/25 | Loss: 0.00345970
Iteration 9/25 | Loss: 0.00345970
Iteration 10/25 | Loss: 0.00345969
Iteration 11/25 | Loss: 0.00345969
Iteration 12/25 | Loss: 0.00345969
Iteration 13/25 | Loss: 0.00345969
Iteration 14/25 | Loss: 0.00345969
Iteration 15/25 | Loss: 0.00345969
Iteration 16/25 | Loss: 0.00345969
Iteration 17/25 | Loss: 0.00345969
Iteration 18/25 | Loss: 0.00345969
Iteration 19/25 | Loss: 0.00345969
Iteration 20/25 | Loss: 0.00345969
Iteration 21/25 | Loss: 0.00345969
Iteration 22/25 | Loss: 0.00345969
Iteration 23/25 | Loss: 0.00345969
Iteration 24/25 | Loss: 0.00345969
Iteration 25/25 | Loss: 0.00345969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345969
Iteration 2/1000 | Loss: 0.00564944
Iteration 3/1000 | Loss: 0.00442438
Iteration 4/1000 | Loss: 0.00062681
Iteration 5/1000 | Loss: 0.00044101
Iteration 6/1000 | Loss: 0.00030934
Iteration 7/1000 | Loss: 0.00021958
Iteration 8/1000 | Loss: 0.00014979
Iteration 9/1000 | Loss: 0.00011816
Iteration 10/1000 | Loss: 0.00010306
Iteration 11/1000 | Loss: 0.00009389
Iteration 12/1000 | Loss: 0.00008729
Iteration 13/1000 | Loss: 0.00008235
Iteration 14/1000 | Loss: 0.00007983
Iteration 15/1000 | Loss: 0.00007809
Iteration 16/1000 | Loss: 0.00007677
Iteration 17/1000 | Loss: 0.00007538
Iteration 18/1000 | Loss: 0.00007402
Iteration 19/1000 | Loss: 0.00007226
Iteration 20/1000 | Loss: 0.00007083
Iteration 21/1000 | Loss: 0.00006967
Iteration 22/1000 | Loss: 0.00006857
Iteration 23/1000 | Loss: 0.00006741
Iteration 24/1000 | Loss: 0.00006648
Iteration 25/1000 | Loss: 0.00006549
Iteration 26/1000 | Loss: 0.00006490
Iteration 27/1000 | Loss: 0.00006430
Iteration 28/1000 | Loss: 0.00006392
Iteration 29/1000 | Loss: 0.00006353
Iteration 30/1000 | Loss: 0.00006324
Iteration 31/1000 | Loss: 0.00006297
Iteration 32/1000 | Loss: 0.00006277
Iteration 33/1000 | Loss: 0.00006261
Iteration 34/1000 | Loss: 0.00006256
Iteration 35/1000 | Loss: 0.00006253
Iteration 36/1000 | Loss: 0.00006247
Iteration 37/1000 | Loss: 0.00006247
Iteration 38/1000 | Loss: 0.00006243
Iteration 39/1000 | Loss: 0.00006243
Iteration 40/1000 | Loss: 0.00006242
Iteration 41/1000 | Loss: 0.00006241
Iteration 42/1000 | Loss: 0.00006241
Iteration 43/1000 | Loss: 0.00006240
Iteration 44/1000 | Loss: 0.00006239
Iteration 45/1000 | Loss: 0.00006239
Iteration 46/1000 | Loss: 0.00006232
Iteration 47/1000 | Loss: 0.00006226
Iteration 48/1000 | Loss: 0.00006226
Iteration 49/1000 | Loss: 0.00006225
Iteration 50/1000 | Loss: 0.00006225
Iteration 51/1000 | Loss: 0.00006224
Iteration 52/1000 | Loss: 0.00006224
Iteration 53/1000 | Loss: 0.00006224
Iteration 54/1000 | Loss: 0.00006224
Iteration 55/1000 | Loss: 0.00006224
Iteration 56/1000 | Loss: 0.00006224
Iteration 57/1000 | Loss: 0.00006224
Iteration 58/1000 | Loss: 0.00006224
Iteration 59/1000 | Loss: 0.00006224
Iteration 60/1000 | Loss: 0.00006223
Iteration 61/1000 | Loss: 0.00006223
Iteration 62/1000 | Loss: 0.00006222
Iteration 63/1000 | Loss: 0.00006222
Iteration 64/1000 | Loss: 0.00006221
Iteration 65/1000 | Loss: 0.00006221
Iteration 66/1000 | Loss: 0.00006221
Iteration 67/1000 | Loss: 0.00006220
Iteration 68/1000 | Loss: 0.00006219
Iteration 69/1000 | Loss: 0.00006219
Iteration 70/1000 | Loss: 0.00006219
Iteration 71/1000 | Loss: 0.00006219
Iteration 72/1000 | Loss: 0.00006219
Iteration 73/1000 | Loss: 0.00006219
Iteration 74/1000 | Loss: 0.00006218
Iteration 75/1000 | Loss: 0.00006218
Iteration 76/1000 | Loss: 0.00006218
Iteration 77/1000 | Loss: 0.00006217
Iteration 78/1000 | Loss: 0.00006217
Iteration 79/1000 | Loss: 0.00006217
Iteration 80/1000 | Loss: 0.00006217
Iteration 81/1000 | Loss: 0.00006217
Iteration 82/1000 | Loss: 0.00006217
Iteration 83/1000 | Loss: 0.00006217
Iteration 84/1000 | Loss: 0.00006217
Iteration 85/1000 | Loss: 0.00006217
Iteration 86/1000 | Loss: 0.00006217
Iteration 87/1000 | Loss: 0.00006217
Iteration 88/1000 | Loss: 0.00006217
Iteration 89/1000 | Loss: 0.00006217
Iteration 90/1000 | Loss: 0.00006217
Iteration 91/1000 | Loss: 0.00006217
Iteration 92/1000 | Loss: 0.00006217
Iteration 93/1000 | Loss: 0.00006217
Iteration 94/1000 | Loss: 0.00006217
Iteration 95/1000 | Loss: 0.00006217
Iteration 96/1000 | Loss: 0.00006217
Iteration 97/1000 | Loss: 0.00006217
Iteration 98/1000 | Loss: 0.00006217
Iteration 99/1000 | Loss: 0.00006217
Iteration 100/1000 | Loss: 0.00006217
Iteration 101/1000 | Loss: 0.00006217
Iteration 102/1000 | Loss: 0.00006217
Iteration 103/1000 | Loss: 0.00006217
Iteration 104/1000 | Loss: 0.00006217
Iteration 105/1000 | Loss: 0.00006217
Iteration 106/1000 | Loss: 0.00006217
Iteration 107/1000 | Loss: 0.00006217
Iteration 108/1000 | Loss: 0.00006217
Iteration 109/1000 | Loss: 0.00006217
Iteration 110/1000 | Loss: 0.00006217
Iteration 111/1000 | Loss: 0.00006217
Iteration 112/1000 | Loss: 0.00006217
Iteration 113/1000 | Loss: 0.00006217
Iteration 114/1000 | Loss: 0.00006217
Iteration 115/1000 | Loss: 0.00006217
Iteration 116/1000 | Loss: 0.00006217
Iteration 117/1000 | Loss: 0.00006217
Iteration 118/1000 | Loss: 0.00006217
Iteration 119/1000 | Loss: 0.00006217
Iteration 120/1000 | Loss: 0.00006217
Iteration 121/1000 | Loss: 0.00006217
Iteration 122/1000 | Loss: 0.00006217
Iteration 123/1000 | Loss: 0.00006217
Iteration 124/1000 | Loss: 0.00006217
Iteration 125/1000 | Loss: 0.00006217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [6.216978363227099e-05, 6.216978363227099e-05, 6.216978363227099e-05, 6.216978363227099e-05, 6.216978363227099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.216978363227099e-05

Optimization complete. Final v2v error: 4.0027241706848145 mm

Highest mean error: 10.571489334106445 mm for frame 116

Lowest mean error: 2.7303733825683594 mm for frame 45

Saving results

Total time: 95.53250741958618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540281
Iteration 2/25 | Loss: 0.00136674
Iteration 3/25 | Loss: 0.00109416
Iteration 4/25 | Loss: 0.00107996
Iteration 5/25 | Loss: 0.00107547
Iteration 6/25 | Loss: 0.00107443
Iteration 7/25 | Loss: 0.00107443
Iteration 8/25 | Loss: 0.00107443
Iteration 9/25 | Loss: 0.00107443
Iteration 10/25 | Loss: 0.00107443
Iteration 11/25 | Loss: 0.00107443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010744320461526513, 0.0010744320461526513, 0.0010744320461526513, 0.0010744320461526513, 0.0010744320461526513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010744320461526513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89949721
Iteration 2/25 | Loss: 0.00075548
Iteration 3/25 | Loss: 0.00075548
Iteration 4/25 | Loss: 0.00075548
Iteration 5/25 | Loss: 0.00075548
Iteration 6/25 | Loss: 0.00075548
Iteration 7/25 | Loss: 0.00075548
Iteration 8/25 | Loss: 0.00075548
Iteration 9/25 | Loss: 0.00075548
Iteration 10/25 | Loss: 0.00075548
Iteration 11/25 | Loss: 0.00075548
Iteration 12/25 | Loss: 0.00075548
Iteration 13/25 | Loss: 0.00075548
Iteration 14/25 | Loss: 0.00075548
Iteration 15/25 | Loss: 0.00075548
Iteration 16/25 | Loss: 0.00075548
Iteration 17/25 | Loss: 0.00075548
Iteration 18/25 | Loss: 0.00075548
Iteration 19/25 | Loss: 0.00075548
Iteration 20/25 | Loss: 0.00075548
Iteration 21/25 | Loss: 0.00075548
Iteration 22/25 | Loss: 0.00075548
Iteration 23/25 | Loss: 0.00075548
Iteration 24/25 | Loss: 0.00075548
Iteration 25/25 | Loss: 0.00075548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075548
Iteration 2/1000 | Loss: 0.00006395
Iteration 3/1000 | Loss: 0.00004608
Iteration 4/1000 | Loss: 0.00003758
Iteration 5/1000 | Loss: 0.00003589
Iteration 6/1000 | Loss: 0.00003435
Iteration 7/1000 | Loss: 0.00003345
Iteration 8/1000 | Loss: 0.00003244
Iteration 9/1000 | Loss: 0.00003160
Iteration 10/1000 | Loss: 0.00003111
Iteration 11/1000 | Loss: 0.00003078
Iteration 12/1000 | Loss: 0.00003043
Iteration 13/1000 | Loss: 0.00003011
Iteration 14/1000 | Loss: 0.00002986
Iteration 15/1000 | Loss: 0.00002954
Iteration 16/1000 | Loss: 0.00002933
Iteration 17/1000 | Loss: 0.00002916
Iteration 18/1000 | Loss: 0.00002895
Iteration 19/1000 | Loss: 0.00002877
Iteration 20/1000 | Loss: 0.00002861
Iteration 21/1000 | Loss: 0.00002851
Iteration 22/1000 | Loss: 0.00002844
Iteration 23/1000 | Loss: 0.00002841
Iteration 24/1000 | Loss: 0.00002840
Iteration 25/1000 | Loss: 0.00002839
Iteration 26/1000 | Loss: 0.00002838
Iteration 27/1000 | Loss: 0.00002836
Iteration 28/1000 | Loss: 0.00002835
Iteration 29/1000 | Loss: 0.00002835
Iteration 30/1000 | Loss: 0.00002832
Iteration 31/1000 | Loss: 0.00002827
Iteration 32/1000 | Loss: 0.00002827
Iteration 33/1000 | Loss: 0.00002826
Iteration 34/1000 | Loss: 0.00002826
Iteration 35/1000 | Loss: 0.00002826
Iteration 36/1000 | Loss: 0.00002826
Iteration 37/1000 | Loss: 0.00002826
Iteration 38/1000 | Loss: 0.00002826
Iteration 39/1000 | Loss: 0.00002825
Iteration 40/1000 | Loss: 0.00002825
Iteration 41/1000 | Loss: 0.00002824
Iteration 42/1000 | Loss: 0.00002824
Iteration 43/1000 | Loss: 0.00002823
Iteration 44/1000 | Loss: 0.00002823
Iteration 45/1000 | Loss: 0.00002823
Iteration 46/1000 | Loss: 0.00002823
Iteration 47/1000 | Loss: 0.00002823
Iteration 48/1000 | Loss: 0.00002823
Iteration 49/1000 | Loss: 0.00002823
Iteration 50/1000 | Loss: 0.00002823
Iteration 51/1000 | Loss: 0.00002823
Iteration 52/1000 | Loss: 0.00002823
Iteration 53/1000 | Loss: 0.00002823
Iteration 54/1000 | Loss: 0.00002823
Iteration 55/1000 | Loss: 0.00002822
Iteration 56/1000 | Loss: 0.00002822
Iteration 57/1000 | Loss: 0.00002822
Iteration 58/1000 | Loss: 0.00002822
Iteration 59/1000 | Loss: 0.00002822
Iteration 60/1000 | Loss: 0.00002822
Iteration 61/1000 | Loss: 0.00002822
Iteration 62/1000 | Loss: 0.00002822
Iteration 63/1000 | Loss: 0.00002822
Iteration 64/1000 | Loss: 0.00002821
Iteration 65/1000 | Loss: 0.00002821
Iteration 66/1000 | Loss: 0.00002821
Iteration 67/1000 | Loss: 0.00002820
Iteration 68/1000 | Loss: 0.00002820
Iteration 69/1000 | Loss: 0.00002820
Iteration 70/1000 | Loss: 0.00002820
Iteration 71/1000 | Loss: 0.00002819
Iteration 72/1000 | Loss: 0.00002819
Iteration 73/1000 | Loss: 0.00002819
Iteration 74/1000 | Loss: 0.00002819
Iteration 75/1000 | Loss: 0.00002818
Iteration 76/1000 | Loss: 0.00002818
Iteration 77/1000 | Loss: 0.00002817
Iteration 78/1000 | Loss: 0.00002817
Iteration 79/1000 | Loss: 0.00002817
Iteration 80/1000 | Loss: 0.00002817
Iteration 81/1000 | Loss: 0.00002817
Iteration 82/1000 | Loss: 0.00002817
Iteration 83/1000 | Loss: 0.00002816
Iteration 84/1000 | Loss: 0.00002816
Iteration 85/1000 | Loss: 0.00002816
Iteration 86/1000 | Loss: 0.00002816
Iteration 87/1000 | Loss: 0.00002815
Iteration 88/1000 | Loss: 0.00002815
Iteration 89/1000 | Loss: 0.00002815
Iteration 90/1000 | Loss: 0.00002815
Iteration 91/1000 | Loss: 0.00002815
Iteration 92/1000 | Loss: 0.00002814
Iteration 93/1000 | Loss: 0.00002814
Iteration 94/1000 | Loss: 0.00002814
Iteration 95/1000 | Loss: 0.00002813
Iteration 96/1000 | Loss: 0.00002813
Iteration 97/1000 | Loss: 0.00002813
Iteration 98/1000 | Loss: 0.00002813
Iteration 99/1000 | Loss: 0.00002812
Iteration 100/1000 | Loss: 0.00002812
Iteration 101/1000 | Loss: 0.00002812
Iteration 102/1000 | Loss: 0.00002812
Iteration 103/1000 | Loss: 0.00002812
Iteration 104/1000 | Loss: 0.00002812
Iteration 105/1000 | Loss: 0.00002812
Iteration 106/1000 | Loss: 0.00002812
Iteration 107/1000 | Loss: 0.00002811
Iteration 108/1000 | Loss: 0.00002811
Iteration 109/1000 | Loss: 0.00002811
Iteration 110/1000 | Loss: 0.00002811
Iteration 111/1000 | Loss: 0.00002811
Iteration 112/1000 | Loss: 0.00002811
Iteration 113/1000 | Loss: 0.00002811
Iteration 114/1000 | Loss: 0.00002811
Iteration 115/1000 | Loss: 0.00002811
Iteration 116/1000 | Loss: 0.00002810
Iteration 117/1000 | Loss: 0.00002810
Iteration 118/1000 | Loss: 0.00002810
Iteration 119/1000 | Loss: 0.00002810
Iteration 120/1000 | Loss: 0.00002810
Iteration 121/1000 | Loss: 0.00002809
Iteration 122/1000 | Loss: 0.00002809
Iteration 123/1000 | Loss: 0.00002809
Iteration 124/1000 | Loss: 0.00002808
Iteration 125/1000 | Loss: 0.00002808
Iteration 126/1000 | Loss: 0.00002808
Iteration 127/1000 | Loss: 0.00002808
Iteration 128/1000 | Loss: 0.00002808
Iteration 129/1000 | Loss: 0.00002807
Iteration 130/1000 | Loss: 0.00002807
Iteration 131/1000 | Loss: 0.00002807
Iteration 132/1000 | Loss: 0.00002807
Iteration 133/1000 | Loss: 0.00002807
Iteration 134/1000 | Loss: 0.00002807
Iteration 135/1000 | Loss: 0.00002807
Iteration 136/1000 | Loss: 0.00002807
Iteration 137/1000 | Loss: 0.00002807
Iteration 138/1000 | Loss: 0.00002807
Iteration 139/1000 | Loss: 0.00002807
Iteration 140/1000 | Loss: 0.00002806
Iteration 141/1000 | Loss: 0.00002806
Iteration 142/1000 | Loss: 0.00002806
Iteration 143/1000 | Loss: 0.00002806
Iteration 144/1000 | Loss: 0.00002806
Iteration 145/1000 | Loss: 0.00002805
Iteration 146/1000 | Loss: 0.00002805
Iteration 147/1000 | Loss: 0.00002805
Iteration 148/1000 | Loss: 0.00002805
Iteration 149/1000 | Loss: 0.00002805
Iteration 150/1000 | Loss: 0.00002805
Iteration 151/1000 | Loss: 0.00002805
Iteration 152/1000 | Loss: 0.00002805
Iteration 153/1000 | Loss: 0.00002805
Iteration 154/1000 | Loss: 0.00002805
Iteration 155/1000 | Loss: 0.00002805
Iteration 156/1000 | Loss: 0.00002805
Iteration 157/1000 | Loss: 0.00002805
Iteration 158/1000 | Loss: 0.00002805
Iteration 159/1000 | Loss: 0.00002804
Iteration 160/1000 | Loss: 0.00002804
Iteration 161/1000 | Loss: 0.00002804
Iteration 162/1000 | Loss: 0.00002804
Iteration 163/1000 | Loss: 0.00002804
Iteration 164/1000 | Loss: 0.00002803
Iteration 165/1000 | Loss: 0.00002803
Iteration 166/1000 | Loss: 0.00002803
Iteration 167/1000 | Loss: 0.00002803
Iteration 168/1000 | Loss: 0.00002803
Iteration 169/1000 | Loss: 0.00002803
Iteration 170/1000 | Loss: 0.00002803
Iteration 171/1000 | Loss: 0.00002803
Iteration 172/1000 | Loss: 0.00002803
Iteration 173/1000 | Loss: 0.00002803
Iteration 174/1000 | Loss: 0.00002803
Iteration 175/1000 | Loss: 0.00002803
Iteration 176/1000 | Loss: 0.00002803
Iteration 177/1000 | Loss: 0.00002803
Iteration 178/1000 | Loss: 0.00002803
Iteration 179/1000 | Loss: 0.00002802
Iteration 180/1000 | Loss: 0.00002802
Iteration 181/1000 | Loss: 0.00002802
Iteration 182/1000 | Loss: 0.00002802
Iteration 183/1000 | Loss: 0.00002802
Iteration 184/1000 | Loss: 0.00002802
Iteration 185/1000 | Loss: 0.00002802
Iteration 186/1000 | Loss: 0.00002802
Iteration 187/1000 | Loss: 0.00002802
Iteration 188/1000 | Loss: 0.00002802
Iteration 189/1000 | Loss: 0.00002802
Iteration 190/1000 | Loss: 0.00002802
Iteration 191/1000 | Loss: 0.00002802
Iteration 192/1000 | Loss: 0.00002802
Iteration 193/1000 | Loss: 0.00002802
Iteration 194/1000 | Loss: 0.00002802
Iteration 195/1000 | Loss: 0.00002802
Iteration 196/1000 | Loss: 0.00002802
Iteration 197/1000 | Loss: 0.00002802
Iteration 198/1000 | Loss: 0.00002802
Iteration 199/1000 | Loss: 0.00002802
Iteration 200/1000 | Loss: 0.00002802
Iteration 201/1000 | Loss: 0.00002802
Iteration 202/1000 | Loss: 0.00002802
Iteration 203/1000 | Loss: 0.00002802
Iteration 204/1000 | Loss: 0.00002802
Iteration 205/1000 | Loss: 0.00002802
Iteration 206/1000 | Loss: 0.00002802
Iteration 207/1000 | Loss: 0.00002802
Iteration 208/1000 | Loss: 0.00002802
Iteration 209/1000 | Loss: 0.00002802
Iteration 210/1000 | Loss: 0.00002802
Iteration 211/1000 | Loss: 0.00002802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.8019379897159524e-05, 2.8019379897159524e-05, 2.8019379897159524e-05, 2.8019379897159524e-05, 2.8019379897159524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8019379897159524e-05

Optimization complete. Final v2v error: 4.101430416107178 mm

Highest mean error: 5.768271446228027 mm for frame 18

Lowest mean error: 3.100905418395996 mm for frame 47

Saving results

Total time: 53.558958292007446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446484
Iteration 2/25 | Loss: 0.00120597
Iteration 3/25 | Loss: 0.00101993
Iteration 4/25 | Loss: 0.00100023
Iteration 5/25 | Loss: 0.00099543
Iteration 6/25 | Loss: 0.00099447
Iteration 7/25 | Loss: 0.00099447
Iteration 8/25 | Loss: 0.00099447
Iteration 9/25 | Loss: 0.00099447
Iteration 10/25 | Loss: 0.00099447
Iteration 11/25 | Loss: 0.00099447
Iteration 12/25 | Loss: 0.00099447
Iteration 13/25 | Loss: 0.00099447
Iteration 14/25 | Loss: 0.00099447
Iteration 15/25 | Loss: 0.00099447
Iteration 16/25 | Loss: 0.00099447
Iteration 17/25 | Loss: 0.00099447
Iteration 18/25 | Loss: 0.00099447
Iteration 19/25 | Loss: 0.00099447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009944697376340628, 0.0009944697376340628, 0.0009944697376340628, 0.0009944697376340628, 0.0009944697376340628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009944697376340628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34958243
Iteration 2/25 | Loss: 0.00062921
Iteration 3/25 | Loss: 0.00062921
Iteration 4/25 | Loss: 0.00062921
Iteration 5/25 | Loss: 0.00062921
Iteration 6/25 | Loss: 0.00062921
Iteration 7/25 | Loss: 0.00062921
Iteration 8/25 | Loss: 0.00062921
Iteration 9/25 | Loss: 0.00062921
Iteration 10/25 | Loss: 0.00062921
Iteration 11/25 | Loss: 0.00062921
Iteration 12/25 | Loss: 0.00062921
Iteration 13/25 | Loss: 0.00062921
Iteration 14/25 | Loss: 0.00062921
Iteration 15/25 | Loss: 0.00062921
Iteration 16/25 | Loss: 0.00062921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006292063044384122, 0.0006292063044384122, 0.0006292063044384122, 0.0006292063044384122, 0.0006292063044384122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006292063044384122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062921
Iteration 2/1000 | Loss: 0.00002987
Iteration 3/1000 | Loss: 0.00002317
Iteration 4/1000 | Loss: 0.00002195
Iteration 5/1000 | Loss: 0.00002139
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002039
Iteration 8/1000 | Loss: 0.00002009
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001979
Iteration 15/1000 | Loss: 0.00001979
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001978
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001978
Iteration 21/1000 | Loss: 0.00001978
Iteration 22/1000 | Loss: 0.00001976
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001970
Iteration 35/1000 | Loss: 0.00001969
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001969
Iteration 39/1000 | Loss: 0.00001969
Iteration 40/1000 | Loss: 0.00001969
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001969
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001969
Iteration 49/1000 | Loss: 0.00001969
Iteration 50/1000 | Loss: 0.00001969
Iteration 51/1000 | Loss: 0.00001969
Iteration 52/1000 | Loss: 0.00001969
Iteration 53/1000 | Loss: 0.00001969
Iteration 54/1000 | Loss: 0.00001969
Iteration 55/1000 | Loss: 0.00001969
Iteration 56/1000 | Loss: 0.00001969
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001969
Iteration 59/1000 | Loss: 0.00001969
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001969
Iteration 62/1000 | Loss: 0.00001969
Iteration 63/1000 | Loss: 0.00001969
Iteration 64/1000 | Loss: 0.00001969
Iteration 65/1000 | Loss: 0.00001969
Iteration 66/1000 | Loss: 0.00001969
Iteration 67/1000 | Loss: 0.00001969
Iteration 68/1000 | Loss: 0.00001969
Iteration 69/1000 | Loss: 0.00001969
Iteration 70/1000 | Loss: 0.00001969
Iteration 71/1000 | Loss: 0.00001969
Iteration 72/1000 | Loss: 0.00001969
Iteration 73/1000 | Loss: 0.00001969
Iteration 74/1000 | Loss: 0.00001969
Iteration 75/1000 | Loss: 0.00001969
Iteration 76/1000 | Loss: 0.00001969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.9690553017426282e-05, 1.9690553017426282e-05, 1.9690553017426282e-05, 1.9690553017426282e-05, 1.9690553017426282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9690553017426282e-05

Optimization complete. Final v2v error: 3.6331825256347656 mm

Highest mean error: 4.297089099884033 mm for frame 142

Lowest mean error: 3.201651096343994 mm for frame 4

Saving results

Total time: 29.03659200668335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075850
Iteration 2/25 | Loss: 0.00339511
Iteration 3/25 | Loss: 0.00192909
Iteration 4/25 | Loss: 0.00164082
Iteration 5/25 | Loss: 0.00180964
Iteration 6/25 | Loss: 0.00153118
Iteration 7/25 | Loss: 0.00117587
Iteration 8/25 | Loss: 0.00101787
Iteration 9/25 | Loss: 0.00094766
Iteration 10/25 | Loss: 0.00093990
Iteration 11/25 | Loss: 0.00093609
Iteration 12/25 | Loss: 0.00093544
Iteration 13/25 | Loss: 0.00093752
Iteration 14/25 | Loss: 0.00093579
Iteration 15/25 | Loss: 0.00093532
Iteration 16/25 | Loss: 0.00093202
Iteration 17/25 | Loss: 0.00093165
Iteration 18/25 | Loss: 0.00093143
Iteration 19/25 | Loss: 0.00093137
Iteration 20/25 | Loss: 0.00093137
Iteration 21/25 | Loss: 0.00093134
Iteration 22/25 | Loss: 0.00093134
Iteration 23/25 | Loss: 0.00093134
Iteration 24/25 | Loss: 0.00093133
Iteration 25/25 | Loss: 0.00093133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35315502
Iteration 2/25 | Loss: 0.00076389
Iteration 3/25 | Loss: 0.00076389
Iteration 4/25 | Loss: 0.00057105
Iteration 5/25 | Loss: 0.00057105
Iteration 6/25 | Loss: 0.00057105
Iteration 7/25 | Loss: 0.00057105
Iteration 8/25 | Loss: 0.00057105
Iteration 9/25 | Loss: 0.00057105
Iteration 10/25 | Loss: 0.00057104
Iteration 11/25 | Loss: 0.00057104
Iteration 12/25 | Loss: 0.00057104
Iteration 13/25 | Loss: 0.00057104
Iteration 14/25 | Loss: 0.00057104
Iteration 15/25 | Loss: 0.00057104
Iteration 16/25 | Loss: 0.00057104
Iteration 17/25 | Loss: 0.00057104
Iteration 18/25 | Loss: 0.00057104
Iteration 19/25 | Loss: 0.00057104
Iteration 20/25 | Loss: 0.00057104
Iteration 21/25 | Loss: 0.00057104
Iteration 22/25 | Loss: 0.00057104
Iteration 23/25 | Loss: 0.00057104
Iteration 24/25 | Loss: 0.00057104
Iteration 25/25 | Loss: 0.00057104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057104
Iteration 2/1000 | Loss: 0.00025891
Iteration 3/1000 | Loss: 0.00002249
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00024037
Iteration 7/1000 | Loss: 0.00011828
Iteration 8/1000 | Loss: 0.00012392
Iteration 9/1000 | Loss: 0.00012679
Iteration 10/1000 | Loss: 0.00001668
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00018139
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00017702
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00026302
Iteration 20/1000 | Loss: 0.00016632
Iteration 21/1000 | Loss: 0.00028284
Iteration 22/1000 | Loss: 0.00048433
Iteration 23/1000 | Loss: 0.00003044
Iteration 24/1000 | Loss: 0.00024778
Iteration 25/1000 | Loss: 0.00001651
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00019707
Iteration 29/1000 | Loss: 0.00024283
Iteration 30/1000 | Loss: 0.00017530
Iteration 31/1000 | Loss: 0.00012115
Iteration 32/1000 | Loss: 0.00019308
Iteration 33/1000 | Loss: 0.00002219
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00020654
Iteration 36/1000 | Loss: 0.00025241
Iteration 37/1000 | Loss: 0.00015397
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00013476
Iteration 40/1000 | Loss: 0.00011163
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001358
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001068
Iteration 47/1000 | Loss: 0.00001037
Iteration 48/1000 | Loss: 0.00001030
Iteration 49/1000 | Loss: 0.00001029
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001015
Iteration 53/1000 | Loss: 0.00001013
Iteration 54/1000 | Loss: 0.00001010
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001007
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001005
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001002
Iteration 69/1000 | Loss: 0.00001002
Iteration 70/1000 | Loss: 0.00001002
Iteration 71/1000 | Loss: 0.00001001
Iteration 72/1000 | Loss: 0.00001001
Iteration 73/1000 | Loss: 0.00000998
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000998
Iteration 83/1000 | Loss: 0.00000997
Iteration 84/1000 | Loss: 0.00000997
Iteration 85/1000 | Loss: 0.00000997
Iteration 86/1000 | Loss: 0.00000997
Iteration 87/1000 | Loss: 0.00000997
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000994
Iteration 90/1000 | Loss: 0.00000994
Iteration 91/1000 | Loss: 0.00000994
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000993
Iteration 98/1000 | Loss: 0.00000993
Iteration 99/1000 | Loss: 0.00000993
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000992
Iteration 102/1000 | Loss: 0.00000992
Iteration 103/1000 | Loss: 0.00000992
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000992
Iteration 106/1000 | Loss: 0.00000992
Iteration 107/1000 | Loss: 0.00000991
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000990
Iteration 112/1000 | Loss: 0.00000990
Iteration 113/1000 | Loss: 0.00000990
Iteration 114/1000 | Loss: 0.00000990
Iteration 115/1000 | Loss: 0.00000990
Iteration 116/1000 | Loss: 0.00000989
Iteration 117/1000 | Loss: 0.00000989
Iteration 118/1000 | Loss: 0.00000989
Iteration 119/1000 | Loss: 0.00000989
Iteration 120/1000 | Loss: 0.00000989
Iteration 121/1000 | Loss: 0.00000989
Iteration 122/1000 | Loss: 0.00000989
Iteration 123/1000 | Loss: 0.00000989
Iteration 124/1000 | Loss: 0.00000989
Iteration 125/1000 | Loss: 0.00000989
Iteration 126/1000 | Loss: 0.00000989
Iteration 127/1000 | Loss: 0.00000989
Iteration 128/1000 | Loss: 0.00000989
Iteration 129/1000 | Loss: 0.00000989
Iteration 130/1000 | Loss: 0.00000989
Iteration 131/1000 | Loss: 0.00000988
Iteration 132/1000 | Loss: 0.00000988
Iteration 133/1000 | Loss: 0.00000988
Iteration 134/1000 | Loss: 0.00000988
Iteration 135/1000 | Loss: 0.00000987
Iteration 136/1000 | Loss: 0.00000987
Iteration 137/1000 | Loss: 0.00000987
Iteration 138/1000 | Loss: 0.00000987
Iteration 139/1000 | Loss: 0.00000987
Iteration 140/1000 | Loss: 0.00000987
Iteration 141/1000 | Loss: 0.00000987
Iteration 142/1000 | Loss: 0.00000987
Iteration 143/1000 | Loss: 0.00000987
Iteration 144/1000 | Loss: 0.00000987
Iteration 145/1000 | Loss: 0.00000987
Iteration 146/1000 | Loss: 0.00000987
Iteration 147/1000 | Loss: 0.00000986
Iteration 148/1000 | Loss: 0.00000986
Iteration 149/1000 | Loss: 0.00000986
Iteration 150/1000 | Loss: 0.00000986
Iteration 151/1000 | Loss: 0.00000986
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000986
Iteration 156/1000 | Loss: 0.00000986
Iteration 157/1000 | Loss: 0.00000986
Iteration 158/1000 | Loss: 0.00000986
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000985
Iteration 163/1000 | Loss: 0.00000985
Iteration 164/1000 | Loss: 0.00000985
Iteration 165/1000 | Loss: 0.00000985
Iteration 166/1000 | Loss: 0.00000985
Iteration 167/1000 | Loss: 0.00000985
Iteration 168/1000 | Loss: 0.00000985
Iteration 169/1000 | Loss: 0.00000985
Iteration 170/1000 | Loss: 0.00000985
Iteration 171/1000 | Loss: 0.00000985
Iteration 172/1000 | Loss: 0.00000985
Iteration 173/1000 | Loss: 0.00000985
Iteration 174/1000 | Loss: 0.00000985
Iteration 175/1000 | Loss: 0.00000984
Iteration 176/1000 | Loss: 0.00000984
Iteration 177/1000 | Loss: 0.00000984
Iteration 178/1000 | Loss: 0.00000984
Iteration 179/1000 | Loss: 0.00000984
Iteration 180/1000 | Loss: 0.00000984
Iteration 181/1000 | Loss: 0.00000984
Iteration 182/1000 | Loss: 0.00000984
Iteration 183/1000 | Loss: 0.00000984
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000984
Iteration 186/1000 | Loss: 0.00000984
Iteration 187/1000 | Loss: 0.00000984
Iteration 188/1000 | Loss: 0.00000984
Iteration 189/1000 | Loss: 0.00000984
Iteration 190/1000 | Loss: 0.00000984
Iteration 191/1000 | Loss: 0.00000984
Iteration 192/1000 | Loss: 0.00000984
Iteration 193/1000 | Loss: 0.00000984
Iteration 194/1000 | Loss: 0.00000984
Iteration 195/1000 | Loss: 0.00000984
Iteration 196/1000 | Loss: 0.00000984
Iteration 197/1000 | Loss: 0.00000983
Iteration 198/1000 | Loss: 0.00000983
Iteration 199/1000 | Loss: 0.00000983
Iteration 200/1000 | Loss: 0.00000983
Iteration 201/1000 | Loss: 0.00000983
Iteration 202/1000 | Loss: 0.00000983
Iteration 203/1000 | Loss: 0.00000983
Iteration 204/1000 | Loss: 0.00000983
Iteration 205/1000 | Loss: 0.00000983
Iteration 206/1000 | Loss: 0.00000983
Iteration 207/1000 | Loss: 0.00000983
Iteration 208/1000 | Loss: 0.00000983
Iteration 209/1000 | Loss: 0.00000983
Iteration 210/1000 | Loss: 0.00000983
Iteration 211/1000 | Loss: 0.00000983
Iteration 212/1000 | Loss: 0.00000983
Iteration 213/1000 | Loss: 0.00000983
Iteration 214/1000 | Loss: 0.00000983
Iteration 215/1000 | Loss: 0.00000983
Iteration 216/1000 | Loss: 0.00000983
Iteration 217/1000 | Loss: 0.00000983
Iteration 218/1000 | Loss: 0.00000983
Iteration 219/1000 | Loss: 0.00000983
Iteration 220/1000 | Loss: 0.00000983
Iteration 221/1000 | Loss: 0.00000983
Iteration 222/1000 | Loss: 0.00000983
Iteration 223/1000 | Loss: 0.00000983
Iteration 224/1000 | Loss: 0.00000983
Iteration 225/1000 | Loss: 0.00000983
Iteration 226/1000 | Loss: 0.00000983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [9.833353033172898e-06, 9.833353033172898e-06, 9.833353033172898e-06, 9.833353033172898e-06, 9.833353033172898e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.833353033172898e-06

Optimization complete. Final v2v error: 2.6548855304718018 mm

Highest mean error: 3.6337621212005615 mm for frame 73

Lowest mean error: 2.301931619644165 mm for frame 47

Saving results

Total time: 110.48054885864258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378676
Iteration 2/25 | Loss: 0.00105522
Iteration 3/25 | Loss: 0.00096882
Iteration 4/25 | Loss: 0.00095906
Iteration 5/25 | Loss: 0.00095460
Iteration 6/25 | Loss: 0.00095416
Iteration 7/25 | Loss: 0.00095416
Iteration 8/25 | Loss: 0.00095416
Iteration 9/25 | Loss: 0.00095416
Iteration 10/25 | Loss: 0.00095416
Iteration 11/25 | Loss: 0.00095416
Iteration 12/25 | Loss: 0.00095416
Iteration 13/25 | Loss: 0.00095416
Iteration 14/25 | Loss: 0.00095416
Iteration 15/25 | Loss: 0.00095416
Iteration 16/25 | Loss: 0.00095416
Iteration 17/25 | Loss: 0.00095416
Iteration 18/25 | Loss: 0.00095416
Iteration 19/25 | Loss: 0.00095416
Iteration 20/25 | Loss: 0.00095416
Iteration 21/25 | Loss: 0.00095416
Iteration 22/25 | Loss: 0.00095416
Iteration 23/25 | Loss: 0.00095416
Iteration 24/25 | Loss: 0.00095416
Iteration 25/25 | Loss: 0.00095416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50113583
Iteration 2/25 | Loss: 0.00054514
Iteration 3/25 | Loss: 0.00054510
Iteration 4/25 | Loss: 0.00054510
Iteration 5/25 | Loss: 0.00054510
Iteration 6/25 | Loss: 0.00054510
Iteration 7/25 | Loss: 0.00054510
Iteration 8/25 | Loss: 0.00054510
Iteration 9/25 | Loss: 0.00054510
Iteration 10/25 | Loss: 0.00054510
Iteration 11/25 | Loss: 0.00054510
Iteration 12/25 | Loss: 0.00054510
Iteration 13/25 | Loss: 0.00054510
Iteration 14/25 | Loss: 0.00054510
Iteration 15/25 | Loss: 0.00054510
Iteration 16/25 | Loss: 0.00054510
Iteration 17/25 | Loss: 0.00054510
Iteration 18/25 | Loss: 0.00054510
Iteration 19/25 | Loss: 0.00054510
Iteration 20/25 | Loss: 0.00054510
Iteration 21/25 | Loss: 0.00054510
Iteration 22/25 | Loss: 0.00054510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005450978642329574, 0.0005450978642329574, 0.0005450978642329574, 0.0005450978642329574, 0.0005450978642329574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005450978642329574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054510
Iteration 2/1000 | Loss: 0.00001993
Iteration 3/1000 | Loss: 0.00001328
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001122
Iteration 6/1000 | Loss: 0.00001083
Iteration 7/1000 | Loss: 0.00001060
Iteration 8/1000 | Loss: 0.00001043
Iteration 9/1000 | Loss: 0.00001042
Iteration 10/1000 | Loss: 0.00001034
Iteration 11/1000 | Loss: 0.00001031
Iteration 12/1000 | Loss: 0.00001031
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001030
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001030
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001029
Iteration 20/1000 | Loss: 0.00001029
Iteration 21/1000 | Loss: 0.00001028
Iteration 22/1000 | Loss: 0.00001028
Iteration 23/1000 | Loss: 0.00001027
Iteration 24/1000 | Loss: 0.00001027
Iteration 25/1000 | Loss: 0.00001027
Iteration 26/1000 | Loss: 0.00001027
Iteration 27/1000 | Loss: 0.00001027
Iteration 28/1000 | Loss: 0.00001027
Iteration 29/1000 | Loss: 0.00001026
Iteration 30/1000 | Loss: 0.00001026
Iteration 31/1000 | Loss: 0.00001026
Iteration 32/1000 | Loss: 0.00001026
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001026
Iteration 35/1000 | Loss: 0.00001026
Iteration 36/1000 | Loss: 0.00001026
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001026
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001024
Iteration 42/1000 | Loss: 0.00001024
Iteration 43/1000 | Loss: 0.00001023
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001023
Iteration 48/1000 | Loss: 0.00001023
Iteration 49/1000 | Loss: 0.00001023
Iteration 50/1000 | Loss: 0.00001023
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001022
Iteration 53/1000 | Loss: 0.00001021
Iteration 54/1000 | Loss: 0.00001021
Iteration 55/1000 | Loss: 0.00001020
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001019
Iteration 59/1000 | Loss: 0.00001019
Iteration 60/1000 | Loss: 0.00001019
Iteration 61/1000 | Loss: 0.00001019
Iteration 62/1000 | Loss: 0.00001019
Iteration 63/1000 | Loss: 0.00001019
Iteration 64/1000 | Loss: 0.00001019
Iteration 65/1000 | Loss: 0.00001019
Iteration 66/1000 | Loss: 0.00001018
Iteration 67/1000 | Loss: 0.00001018
Iteration 68/1000 | Loss: 0.00001018
Iteration 69/1000 | Loss: 0.00001018
Iteration 70/1000 | Loss: 0.00001018
Iteration 71/1000 | Loss: 0.00001018
Iteration 72/1000 | Loss: 0.00001018
Iteration 73/1000 | Loss: 0.00001018
Iteration 74/1000 | Loss: 0.00001018
Iteration 75/1000 | Loss: 0.00001018
Iteration 76/1000 | Loss: 0.00001018
Iteration 77/1000 | Loss: 0.00001018
Iteration 78/1000 | Loss: 0.00001018
Iteration 79/1000 | Loss: 0.00001018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.0183184713241644e-05, 1.0183184713241644e-05, 1.0183184713241644e-05, 1.0183184713241644e-05, 1.0183184713241644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0183184713241644e-05

Optimization complete. Final v2v error: 2.681381940841675 mm

Highest mean error: 3.241880416870117 mm for frame 129

Lowest mean error: 2.3190200328826904 mm for frame 169

Saving results

Total time: 28.420446634292603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602371
Iteration 2/25 | Loss: 0.00105009
Iteration 3/25 | Loss: 0.00093751
Iteration 4/25 | Loss: 0.00092620
Iteration 5/25 | Loss: 0.00092224
Iteration 6/25 | Loss: 0.00092119
Iteration 7/25 | Loss: 0.00092119
Iteration 8/25 | Loss: 0.00092119
Iteration 9/25 | Loss: 0.00092119
Iteration 10/25 | Loss: 0.00092119
Iteration 11/25 | Loss: 0.00092119
Iteration 12/25 | Loss: 0.00092119
Iteration 13/25 | Loss: 0.00092119
Iteration 14/25 | Loss: 0.00092119
Iteration 15/25 | Loss: 0.00092119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009211929864250124, 0.0009211929864250124, 0.0009211929864250124, 0.0009211929864250124, 0.0009211929864250124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009211929864250124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.01808810
Iteration 2/25 | Loss: 0.00064119
Iteration 3/25 | Loss: 0.00064118
Iteration 4/25 | Loss: 0.00064118
Iteration 5/25 | Loss: 0.00064118
Iteration 6/25 | Loss: 0.00064118
Iteration 7/25 | Loss: 0.00064118
Iteration 8/25 | Loss: 0.00064118
Iteration 9/25 | Loss: 0.00064118
Iteration 10/25 | Loss: 0.00064118
Iteration 11/25 | Loss: 0.00064118
Iteration 12/25 | Loss: 0.00064118
Iteration 13/25 | Loss: 0.00064118
Iteration 14/25 | Loss: 0.00064118
Iteration 15/25 | Loss: 0.00064118
Iteration 16/25 | Loss: 0.00064118
Iteration 17/25 | Loss: 0.00064118
Iteration 18/25 | Loss: 0.00064118
Iteration 19/25 | Loss: 0.00064118
Iteration 20/25 | Loss: 0.00064118
Iteration 21/25 | Loss: 0.00064118
Iteration 22/25 | Loss: 0.00064118
Iteration 23/25 | Loss: 0.00064118
Iteration 24/25 | Loss: 0.00064118
Iteration 25/25 | Loss: 0.00064118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064118
Iteration 2/1000 | Loss: 0.00001870
Iteration 3/1000 | Loss: 0.00001228
Iteration 4/1000 | Loss: 0.00001114
Iteration 5/1000 | Loss: 0.00001045
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000990
Iteration 8/1000 | Loss: 0.00000967
Iteration 9/1000 | Loss: 0.00000958
Iteration 10/1000 | Loss: 0.00000958
Iteration 11/1000 | Loss: 0.00000957
Iteration 12/1000 | Loss: 0.00000956
Iteration 13/1000 | Loss: 0.00000954
Iteration 14/1000 | Loss: 0.00000954
Iteration 15/1000 | Loss: 0.00000954
Iteration 16/1000 | Loss: 0.00000954
Iteration 17/1000 | Loss: 0.00000954
Iteration 18/1000 | Loss: 0.00000953
Iteration 19/1000 | Loss: 0.00000953
Iteration 20/1000 | Loss: 0.00000952
Iteration 21/1000 | Loss: 0.00000952
Iteration 22/1000 | Loss: 0.00000951
Iteration 23/1000 | Loss: 0.00000950
Iteration 24/1000 | Loss: 0.00000950
Iteration 25/1000 | Loss: 0.00000950
Iteration 26/1000 | Loss: 0.00000949
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000948
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000947
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000946
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000945
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000944
Iteration 37/1000 | Loss: 0.00000944
Iteration 38/1000 | Loss: 0.00000943
Iteration 39/1000 | Loss: 0.00000943
Iteration 40/1000 | Loss: 0.00000942
Iteration 41/1000 | Loss: 0.00000942
Iteration 42/1000 | Loss: 0.00000942
Iteration 43/1000 | Loss: 0.00000942
Iteration 44/1000 | Loss: 0.00000941
Iteration 45/1000 | Loss: 0.00000941
Iteration 46/1000 | Loss: 0.00000940
Iteration 47/1000 | Loss: 0.00000940
Iteration 48/1000 | Loss: 0.00000940
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000940
Iteration 51/1000 | Loss: 0.00000940
Iteration 52/1000 | Loss: 0.00000939
Iteration 53/1000 | Loss: 0.00000939
Iteration 54/1000 | Loss: 0.00000939
Iteration 55/1000 | Loss: 0.00000938
Iteration 56/1000 | Loss: 0.00000938
Iteration 57/1000 | Loss: 0.00000937
Iteration 58/1000 | Loss: 0.00000937
Iteration 59/1000 | Loss: 0.00000937
Iteration 60/1000 | Loss: 0.00000936
Iteration 61/1000 | Loss: 0.00000936
Iteration 62/1000 | Loss: 0.00000936
Iteration 63/1000 | Loss: 0.00000936
Iteration 64/1000 | Loss: 0.00000936
Iteration 65/1000 | Loss: 0.00000936
Iteration 66/1000 | Loss: 0.00000936
Iteration 67/1000 | Loss: 0.00000936
Iteration 68/1000 | Loss: 0.00000936
Iteration 69/1000 | Loss: 0.00000936
Iteration 70/1000 | Loss: 0.00000936
Iteration 71/1000 | Loss: 0.00000935
Iteration 72/1000 | Loss: 0.00000935
Iteration 73/1000 | Loss: 0.00000935
Iteration 74/1000 | Loss: 0.00000935
Iteration 75/1000 | Loss: 0.00000935
Iteration 76/1000 | Loss: 0.00000935
Iteration 77/1000 | Loss: 0.00000935
Iteration 78/1000 | Loss: 0.00000934
Iteration 79/1000 | Loss: 0.00000934
Iteration 80/1000 | Loss: 0.00000934
Iteration 81/1000 | Loss: 0.00000934
Iteration 82/1000 | Loss: 0.00000933
Iteration 83/1000 | Loss: 0.00000933
Iteration 84/1000 | Loss: 0.00000933
Iteration 85/1000 | Loss: 0.00000933
Iteration 86/1000 | Loss: 0.00000933
Iteration 87/1000 | Loss: 0.00000933
Iteration 88/1000 | Loss: 0.00000933
Iteration 89/1000 | Loss: 0.00000932
Iteration 90/1000 | Loss: 0.00000932
Iteration 91/1000 | Loss: 0.00000932
Iteration 92/1000 | Loss: 0.00000931
Iteration 93/1000 | Loss: 0.00000931
Iteration 94/1000 | Loss: 0.00000930
Iteration 95/1000 | Loss: 0.00000930
Iteration 96/1000 | Loss: 0.00000930
Iteration 97/1000 | Loss: 0.00000930
Iteration 98/1000 | Loss: 0.00000930
Iteration 99/1000 | Loss: 0.00000930
Iteration 100/1000 | Loss: 0.00000929
Iteration 101/1000 | Loss: 0.00000929
Iteration 102/1000 | Loss: 0.00000929
Iteration 103/1000 | Loss: 0.00000929
Iteration 104/1000 | Loss: 0.00000929
Iteration 105/1000 | Loss: 0.00000929
Iteration 106/1000 | Loss: 0.00000928
Iteration 107/1000 | Loss: 0.00000928
Iteration 108/1000 | Loss: 0.00000928
Iteration 109/1000 | Loss: 0.00000928
Iteration 110/1000 | Loss: 0.00000928
Iteration 111/1000 | Loss: 0.00000928
Iteration 112/1000 | Loss: 0.00000928
Iteration 113/1000 | Loss: 0.00000928
Iteration 114/1000 | Loss: 0.00000928
Iteration 115/1000 | Loss: 0.00000928
Iteration 116/1000 | Loss: 0.00000928
Iteration 117/1000 | Loss: 0.00000928
Iteration 118/1000 | Loss: 0.00000928
Iteration 119/1000 | Loss: 0.00000928
Iteration 120/1000 | Loss: 0.00000928
Iteration 121/1000 | Loss: 0.00000928
Iteration 122/1000 | Loss: 0.00000928
Iteration 123/1000 | Loss: 0.00000928
Iteration 124/1000 | Loss: 0.00000928
Iteration 125/1000 | Loss: 0.00000928
Iteration 126/1000 | Loss: 0.00000928
Iteration 127/1000 | Loss: 0.00000928
Iteration 128/1000 | Loss: 0.00000928
Iteration 129/1000 | Loss: 0.00000928
Iteration 130/1000 | Loss: 0.00000928
Iteration 131/1000 | Loss: 0.00000928
Iteration 132/1000 | Loss: 0.00000928
Iteration 133/1000 | Loss: 0.00000928
Iteration 134/1000 | Loss: 0.00000928
Iteration 135/1000 | Loss: 0.00000928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [9.276895070797764e-06, 9.276895070797764e-06, 9.276895070797764e-06, 9.276895070797764e-06, 9.276895070797764e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.276895070797764e-06

Optimization complete. Final v2v error: 2.5909762382507324 mm

Highest mean error: 2.8495986461639404 mm for frame 66

Lowest mean error: 2.401157855987549 mm for frame 30

Saving results

Total time: 27.68505859375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386057
Iteration 2/25 | Loss: 0.00109586
Iteration 3/25 | Loss: 0.00097243
Iteration 4/25 | Loss: 0.00095190
Iteration 5/25 | Loss: 0.00094315
Iteration 6/25 | Loss: 0.00094035
Iteration 7/25 | Loss: 0.00094030
Iteration 8/25 | Loss: 0.00094030
Iteration 9/25 | Loss: 0.00094030
Iteration 10/25 | Loss: 0.00094030
Iteration 11/25 | Loss: 0.00094030
Iteration 12/25 | Loss: 0.00094030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009403035510331392, 0.0009403035510331392, 0.0009403035510331392, 0.0009403035510331392, 0.0009403035510331392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009403035510331392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.38581848
Iteration 2/25 | Loss: 0.00066591
Iteration 3/25 | Loss: 0.00066589
Iteration 4/25 | Loss: 0.00066589
Iteration 5/25 | Loss: 0.00066588
Iteration 6/25 | Loss: 0.00066588
Iteration 7/25 | Loss: 0.00066588
Iteration 8/25 | Loss: 0.00066588
Iteration 9/25 | Loss: 0.00066588
Iteration 10/25 | Loss: 0.00066588
Iteration 11/25 | Loss: 0.00066588
Iteration 12/25 | Loss: 0.00066588
Iteration 13/25 | Loss: 0.00066588
Iteration 14/25 | Loss: 0.00066588
Iteration 15/25 | Loss: 0.00066588
Iteration 16/25 | Loss: 0.00066588
Iteration 17/25 | Loss: 0.00066588
Iteration 18/25 | Loss: 0.00066588
Iteration 19/25 | Loss: 0.00066588
Iteration 20/25 | Loss: 0.00066588
Iteration 21/25 | Loss: 0.00066588
Iteration 22/25 | Loss: 0.00066588
Iteration 23/25 | Loss: 0.00066588
Iteration 24/25 | Loss: 0.00066588
Iteration 25/25 | Loss: 0.00066588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066588
Iteration 2/1000 | Loss: 0.00003168
Iteration 3/1000 | Loss: 0.00001586
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001387
Iteration 6/1000 | Loss: 0.00001344
Iteration 7/1000 | Loss: 0.00001310
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001261
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001236
Iteration 13/1000 | Loss: 0.00001234
Iteration 14/1000 | Loss: 0.00001233
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001221
Iteration 25/1000 | Loss: 0.00001219
Iteration 26/1000 | Loss: 0.00001219
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001205
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001204
Iteration 93/1000 | Loss: 0.00001204
Iteration 94/1000 | Loss: 0.00001204
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001203
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001202
Iteration 114/1000 | Loss: 0.00001202
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001201
Iteration 121/1000 | Loss: 0.00001201
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001200
Iteration 128/1000 | Loss: 0.00001200
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001198
Iteration 138/1000 | Loss: 0.00001198
Iteration 139/1000 | Loss: 0.00001198
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001198
Iteration 145/1000 | Loss: 0.00001198
Iteration 146/1000 | Loss: 0.00001198
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001196
Iteration 157/1000 | Loss: 0.00001196
Iteration 158/1000 | Loss: 0.00001196
Iteration 159/1000 | Loss: 0.00001196
Iteration 160/1000 | Loss: 0.00001196
Iteration 161/1000 | Loss: 0.00001196
Iteration 162/1000 | Loss: 0.00001196
Iteration 163/1000 | Loss: 0.00001196
Iteration 164/1000 | Loss: 0.00001196
Iteration 165/1000 | Loss: 0.00001196
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001196
Iteration 168/1000 | Loss: 0.00001196
Iteration 169/1000 | Loss: 0.00001196
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001194
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001194
Iteration 192/1000 | Loss: 0.00001194
Iteration 193/1000 | Loss: 0.00001194
Iteration 194/1000 | Loss: 0.00001194
Iteration 195/1000 | Loss: 0.00001194
Iteration 196/1000 | Loss: 0.00001194
Iteration 197/1000 | Loss: 0.00001194
Iteration 198/1000 | Loss: 0.00001194
Iteration 199/1000 | Loss: 0.00001194
Iteration 200/1000 | Loss: 0.00001194
Iteration 201/1000 | Loss: 0.00001194
Iteration 202/1000 | Loss: 0.00001194
Iteration 203/1000 | Loss: 0.00001194
Iteration 204/1000 | Loss: 0.00001194
Iteration 205/1000 | Loss: 0.00001194
Iteration 206/1000 | Loss: 0.00001194
Iteration 207/1000 | Loss: 0.00001194
Iteration 208/1000 | Loss: 0.00001194
Iteration 209/1000 | Loss: 0.00001194
Iteration 210/1000 | Loss: 0.00001194
Iteration 211/1000 | Loss: 0.00001194
Iteration 212/1000 | Loss: 0.00001194
Iteration 213/1000 | Loss: 0.00001194
Iteration 214/1000 | Loss: 0.00001194
Iteration 215/1000 | Loss: 0.00001194
Iteration 216/1000 | Loss: 0.00001194
Iteration 217/1000 | Loss: 0.00001194
Iteration 218/1000 | Loss: 0.00001194
Iteration 219/1000 | Loss: 0.00001194
Iteration 220/1000 | Loss: 0.00001194
Iteration 221/1000 | Loss: 0.00001194
Iteration 222/1000 | Loss: 0.00001194
Iteration 223/1000 | Loss: 0.00001194
Iteration 224/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.1942101082240697e-05, 1.1942101082240697e-05, 1.1942101082240697e-05, 1.1942101082240697e-05, 1.1942101082240697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1942101082240697e-05

Optimization complete. Final v2v error: 2.9274778366088867 mm

Highest mean error: 3.384477376937866 mm for frame 87

Lowest mean error: 2.5734894275665283 mm for frame 60

Saving results

Total time: 36.57746362686157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881255
Iteration 2/25 | Loss: 0.00101856
Iteration 3/25 | Loss: 0.00093564
Iteration 4/25 | Loss: 0.00092217
Iteration 5/25 | Loss: 0.00091752
Iteration 6/25 | Loss: 0.00091633
Iteration 7/25 | Loss: 0.00091626
Iteration 8/25 | Loss: 0.00091626
Iteration 9/25 | Loss: 0.00091626
Iteration 10/25 | Loss: 0.00091626
Iteration 11/25 | Loss: 0.00091626
Iteration 12/25 | Loss: 0.00091626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009162570349872112, 0.0009162570349872112, 0.0009162570349872112, 0.0009162570349872112, 0.0009162570349872112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009162570349872112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.43102217
Iteration 2/25 | Loss: 0.00059007
Iteration 3/25 | Loss: 0.00059007
Iteration 4/25 | Loss: 0.00059007
Iteration 5/25 | Loss: 0.00059007
Iteration 6/25 | Loss: 0.00059007
Iteration 7/25 | Loss: 0.00059007
Iteration 8/25 | Loss: 0.00059007
Iteration 9/25 | Loss: 0.00059007
Iteration 10/25 | Loss: 0.00059007
Iteration 11/25 | Loss: 0.00059007
Iteration 12/25 | Loss: 0.00059007
Iteration 13/25 | Loss: 0.00059007
Iteration 14/25 | Loss: 0.00059007
Iteration 15/25 | Loss: 0.00059007
Iteration 16/25 | Loss: 0.00059007
Iteration 17/25 | Loss: 0.00059007
Iteration 18/25 | Loss: 0.00059007
Iteration 19/25 | Loss: 0.00059007
Iteration 20/25 | Loss: 0.00059007
Iteration 21/25 | Loss: 0.00059007
Iteration 22/25 | Loss: 0.00059007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005900650285184383, 0.0005900650285184383, 0.0005900650285184383, 0.0005900650285184383, 0.0005900650285184383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005900650285184383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059007
Iteration 2/1000 | Loss: 0.00002677
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001343
Iteration 5/1000 | Loss: 0.00001248
Iteration 6/1000 | Loss: 0.00001186
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001115
Iteration 9/1000 | Loss: 0.00001101
Iteration 10/1000 | Loss: 0.00001088
Iteration 11/1000 | Loss: 0.00001085
Iteration 12/1000 | Loss: 0.00001081
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001081
Iteration 15/1000 | Loss: 0.00001081
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001081
Iteration 19/1000 | Loss: 0.00001081
Iteration 20/1000 | Loss: 0.00001081
Iteration 21/1000 | Loss: 0.00001081
Iteration 22/1000 | Loss: 0.00001081
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001080
Iteration 25/1000 | Loss: 0.00001079
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001077
Iteration 28/1000 | Loss: 0.00001077
Iteration 29/1000 | Loss: 0.00001077
Iteration 30/1000 | Loss: 0.00001077
Iteration 31/1000 | Loss: 0.00001076
Iteration 32/1000 | Loss: 0.00001076
Iteration 33/1000 | Loss: 0.00001075
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001071
Iteration 37/1000 | Loss: 0.00001071
Iteration 38/1000 | Loss: 0.00001071
Iteration 39/1000 | Loss: 0.00001070
Iteration 40/1000 | Loss: 0.00001070
Iteration 41/1000 | Loss: 0.00001070
Iteration 42/1000 | Loss: 0.00001070
Iteration 43/1000 | Loss: 0.00001070
Iteration 44/1000 | Loss: 0.00001069
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001066
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001063
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001062
Iteration 64/1000 | Loss: 0.00001062
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001061
Iteration 67/1000 | Loss: 0.00001061
Iteration 68/1000 | Loss: 0.00001061
Iteration 69/1000 | Loss: 0.00001061
Iteration 70/1000 | Loss: 0.00001061
Iteration 71/1000 | Loss: 0.00001061
Iteration 72/1000 | Loss: 0.00001060
Iteration 73/1000 | Loss: 0.00001060
Iteration 74/1000 | Loss: 0.00001060
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001059
Iteration 77/1000 | Loss: 0.00001059
Iteration 78/1000 | Loss: 0.00001058
Iteration 79/1000 | Loss: 0.00001058
Iteration 80/1000 | Loss: 0.00001058
Iteration 81/1000 | Loss: 0.00001057
Iteration 82/1000 | Loss: 0.00001057
Iteration 83/1000 | Loss: 0.00001057
Iteration 84/1000 | Loss: 0.00001057
Iteration 85/1000 | Loss: 0.00001057
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001057
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001057
Iteration 94/1000 | Loss: 0.00001057
Iteration 95/1000 | Loss: 0.00001057
Iteration 96/1000 | Loss: 0.00001056
Iteration 97/1000 | Loss: 0.00001056
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001056
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001056
Iteration 105/1000 | Loss: 0.00001056
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001056
Iteration 108/1000 | Loss: 0.00001055
Iteration 109/1000 | Loss: 0.00001055
Iteration 110/1000 | Loss: 0.00001055
Iteration 111/1000 | Loss: 0.00001055
Iteration 112/1000 | Loss: 0.00001055
Iteration 113/1000 | Loss: 0.00001055
Iteration 114/1000 | Loss: 0.00001055
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001055
Iteration 119/1000 | Loss: 0.00001055
Iteration 120/1000 | Loss: 0.00001055
Iteration 121/1000 | Loss: 0.00001055
Iteration 122/1000 | Loss: 0.00001055
Iteration 123/1000 | Loss: 0.00001055
Iteration 124/1000 | Loss: 0.00001055
Iteration 125/1000 | Loss: 0.00001055
Iteration 126/1000 | Loss: 0.00001055
Iteration 127/1000 | Loss: 0.00001055
Iteration 128/1000 | Loss: 0.00001055
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001054
Iteration 131/1000 | Loss: 0.00001054
Iteration 132/1000 | Loss: 0.00001054
Iteration 133/1000 | Loss: 0.00001054
Iteration 134/1000 | Loss: 0.00001054
Iteration 135/1000 | Loss: 0.00001054
Iteration 136/1000 | Loss: 0.00001054
Iteration 137/1000 | Loss: 0.00001054
Iteration 138/1000 | Loss: 0.00001054
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001054
Iteration 141/1000 | Loss: 0.00001054
Iteration 142/1000 | Loss: 0.00001054
Iteration 143/1000 | Loss: 0.00001054
Iteration 144/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.0543157259235159e-05, 1.0543157259235159e-05, 1.0543157259235159e-05, 1.0543157259235159e-05, 1.0543157259235159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0543157259235159e-05

Optimization complete. Final v2v error: 2.7220427989959717 mm

Highest mean error: 3.3374977111816406 mm for frame 102

Lowest mean error: 2.3184690475463867 mm for frame 46

Saving results

Total time: 30.187641859054565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407408
Iteration 2/25 | Loss: 0.00109639
Iteration 3/25 | Loss: 0.00096950
Iteration 4/25 | Loss: 0.00095763
Iteration 5/25 | Loss: 0.00095369
Iteration 6/25 | Loss: 0.00095225
Iteration 7/25 | Loss: 0.00095209
Iteration 8/25 | Loss: 0.00095209
Iteration 9/25 | Loss: 0.00095209
Iteration 10/25 | Loss: 0.00095209
Iteration 11/25 | Loss: 0.00095209
Iteration 12/25 | Loss: 0.00095209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009520947351120412, 0.0009520947351120412, 0.0009520947351120412, 0.0009520947351120412, 0.0009520947351120412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009520947351120412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34885120
Iteration 2/25 | Loss: 0.00067260
Iteration 3/25 | Loss: 0.00067258
Iteration 4/25 | Loss: 0.00067257
Iteration 5/25 | Loss: 0.00067257
Iteration 6/25 | Loss: 0.00067257
Iteration 7/25 | Loss: 0.00067257
Iteration 8/25 | Loss: 0.00067257
Iteration 9/25 | Loss: 0.00067257
Iteration 10/25 | Loss: 0.00067257
Iteration 11/25 | Loss: 0.00067257
Iteration 12/25 | Loss: 0.00067257
Iteration 13/25 | Loss: 0.00067257
Iteration 14/25 | Loss: 0.00067257
Iteration 15/25 | Loss: 0.00067257
Iteration 16/25 | Loss: 0.00067257
Iteration 17/25 | Loss: 0.00067257
Iteration 18/25 | Loss: 0.00067257
Iteration 19/25 | Loss: 0.00067257
Iteration 20/25 | Loss: 0.00067257
Iteration 21/25 | Loss: 0.00067257
Iteration 22/25 | Loss: 0.00067257
Iteration 23/25 | Loss: 0.00067257
Iteration 24/25 | Loss: 0.00067257
Iteration 25/25 | Loss: 0.00067257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067257
Iteration 2/1000 | Loss: 0.00004320
Iteration 3/1000 | Loss: 0.00002193
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001472
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001277
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001276
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001275
Iteration 26/1000 | Loss: 0.00001274
Iteration 27/1000 | Loss: 0.00001274
Iteration 28/1000 | Loss: 0.00001274
Iteration 29/1000 | Loss: 0.00001273
Iteration 30/1000 | Loss: 0.00001273
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001272
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001272
Iteration 36/1000 | Loss: 0.00001271
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001270
Iteration 39/1000 | Loss: 0.00001270
Iteration 40/1000 | Loss: 0.00001269
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001265
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001260
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001254
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001250
Iteration 94/1000 | Loss: 0.00001250
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001249
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001249
Iteration 106/1000 | Loss: 0.00001249
Iteration 107/1000 | Loss: 0.00001249
Iteration 108/1000 | Loss: 0.00001249
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001248
Iteration 111/1000 | Loss: 0.00001248
Iteration 112/1000 | Loss: 0.00001248
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001245
Iteration 121/1000 | Loss: 0.00001245
Iteration 122/1000 | Loss: 0.00001245
Iteration 123/1000 | Loss: 0.00001245
Iteration 124/1000 | Loss: 0.00001245
Iteration 125/1000 | Loss: 0.00001245
Iteration 126/1000 | Loss: 0.00001245
Iteration 127/1000 | Loss: 0.00001245
Iteration 128/1000 | Loss: 0.00001245
Iteration 129/1000 | Loss: 0.00001245
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001244
Iteration 132/1000 | Loss: 0.00001244
Iteration 133/1000 | Loss: 0.00001244
Iteration 134/1000 | Loss: 0.00001244
Iteration 135/1000 | Loss: 0.00001244
Iteration 136/1000 | Loss: 0.00001244
Iteration 137/1000 | Loss: 0.00001244
Iteration 138/1000 | Loss: 0.00001243
Iteration 139/1000 | Loss: 0.00001243
Iteration 140/1000 | Loss: 0.00001243
Iteration 141/1000 | Loss: 0.00001243
Iteration 142/1000 | Loss: 0.00001243
Iteration 143/1000 | Loss: 0.00001243
Iteration 144/1000 | Loss: 0.00001243
Iteration 145/1000 | Loss: 0.00001243
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001243
Iteration 148/1000 | Loss: 0.00001243
Iteration 149/1000 | Loss: 0.00001243
Iteration 150/1000 | Loss: 0.00001243
Iteration 151/1000 | Loss: 0.00001243
Iteration 152/1000 | Loss: 0.00001243
Iteration 153/1000 | Loss: 0.00001243
Iteration 154/1000 | Loss: 0.00001243
Iteration 155/1000 | Loss: 0.00001243
Iteration 156/1000 | Loss: 0.00001243
Iteration 157/1000 | Loss: 0.00001243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2430146853148472e-05, 1.2430146853148472e-05, 1.2430146853148472e-05, 1.2430146853148472e-05, 1.2430146853148472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2430146853148472e-05

Optimization complete. Final v2v error: 2.7329492568969727 mm

Highest mean error: 5.0011067390441895 mm for frame 74

Lowest mean error: 2.117017984390259 mm for frame 109

Saving results

Total time: 37.54416298866272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477222
Iteration 2/25 | Loss: 0.00123765
Iteration 3/25 | Loss: 0.00097457
Iteration 4/25 | Loss: 0.00094255
Iteration 5/25 | Loss: 0.00093922
Iteration 6/25 | Loss: 0.00093828
Iteration 7/25 | Loss: 0.00093828
Iteration 8/25 | Loss: 0.00093828
Iteration 9/25 | Loss: 0.00093828
Iteration 10/25 | Loss: 0.00093828
Iteration 11/25 | Loss: 0.00093828
Iteration 12/25 | Loss: 0.00093828
Iteration 13/25 | Loss: 0.00093828
Iteration 14/25 | Loss: 0.00093828
Iteration 15/25 | Loss: 0.00093828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009382774587720633, 0.0009382774587720633, 0.0009382774587720633, 0.0009382774587720633, 0.0009382774587720633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009382774587720633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35219860
Iteration 2/25 | Loss: 0.00054602
Iteration 3/25 | Loss: 0.00054601
Iteration 4/25 | Loss: 0.00054601
Iteration 5/25 | Loss: 0.00054601
Iteration 6/25 | Loss: 0.00054601
Iteration 7/25 | Loss: 0.00054601
Iteration 8/25 | Loss: 0.00054601
Iteration 9/25 | Loss: 0.00054601
Iteration 10/25 | Loss: 0.00054601
Iteration 11/25 | Loss: 0.00054601
Iteration 12/25 | Loss: 0.00054601
Iteration 13/25 | Loss: 0.00054601
Iteration 14/25 | Loss: 0.00054601
Iteration 15/25 | Loss: 0.00054601
Iteration 16/25 | Loss: 0.00054601
Iteration 17/25 | Loss: 0.00054601
Iteration 18/25 | Loss: 0.00054601
Iteration 19/25 | Loss: 0.00054601
Iteration 20/25 | Loss: 0.00054601
Iteration 21/25 | Loss: 0.00054601
Iteration 22/25 | Loss: 0.00054601
Iteration 23/25 | Loss: 0.00054601
Iteration 24/25 | Loss: 0.00054601
Iteration 25/25 | Loss: 0.00054601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054601
Iteration 2/1000 | Loss: 0.00002474
Iteration 3/1000 | Loss: 0.00001481
Iteration 4/1000 | Loss: 0.00001315
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001164
Iteration 7/1000 | Loss: 0.00001144
Iteration 8/1000 | Loss: 0.00001116
Iteration 9/1000 | Loss: 0.00001107
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001087
Iteration 12/1000 | Loss: 0.00001087
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001087
Iteration 15/1000 | Loss: 0.00001087
Iteration 16/1000 | Loss: 0.00001086
Iteration 17/1000 | Loss: 0.00001086
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001085
Iteration 20/1000 | Loss: 0.00001085
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001084
Iteration 23/1000 | Loss: 0.00001083
Iteration 24/1000 | Loss: 0.00001083
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001082
Iteration 28/1000 | Loss: 0.00001081
Iteration 29/1000 | Loss: 0.00001081
Iteration 30/1000 | Loss: 0.00001080
Iteration 31/1000 | Loss: 0.00001080
Iteration 32/1000 | Loss: 0.00001079
Iteration 33/1000 | Loss: 0.00001079
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001075
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001075
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001074
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001073
Iteration 54/1000 | Loss: 0.00001073
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001072
Iteration 59/1000 | Loss: 0.00001072
Iteration 60/1000 | Loss: 0.00001072
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001072
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001071
Iteration 65/1000 | Loss: 0.00001071
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001070
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001070
Iteration 73/1000 | Loss: 0.00001070
Iteration 74/1000 | Loss: 0.00001070
Iteration 75/1000 | Loss: 0.00001069
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001069
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001067
Iteration 82/1000 | Loss: 0.00001067
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001067
Iteration 87/1000 | Loss: 0.00001066
Iteration 88/1000 | Loss: 0.00001066
Iteration 89/1000 | Loss: 0.00001065
Iteration 90/1000 | Loss: 0.00001065
Iteration 91/1000 | Loss: 0.00001065
Iteration 92/1000 | Loss: 0.00001065
Iteration 93/1000 | Loss: 0.00001065
Iteration 94/1000 | Loss: 0.00001065
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001065
Iteration 97/1000 | Loss: 0.00001065
Iteration 98/1000 | Loss: 0.00001065
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001065
Iteration 102/1000 | Loss: 0.00001065
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001063
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001063
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001062
Iteration 119/1000 | Loss: 0.00001062
Iteration 120/1000 | Loss: 0.00001062
Iteration 121/1000 | Loss: 0.00001062
Iteration 122/1000 | Loss: 0.00001062
Iteration 123/1000 | Loss: 0.00001061
Iteration 124/1000 | Loss: 0.00001061
Iteration 125/1000 | Loss: 0.00001061
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001061
Iteration 131/1000 | Loss: 0.00001061
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001058
Iteration 142/1000 | Loss: 0.00001058
Iteration 143/1000 | Loss: 0.00001058
Iteration 144/1000 | Loss: 0.00001058
Iteration 145/1000 | Loss: 0.00001058
Iteration 146/1000 | Loss: 0.00001058
Iteration 147/1000 | Loss: 0.00001058
Iteration 148/1000 | Loss: 0.00001058
Iteration 149/1000 | Loss: 0.00001058
Iteration 150/1000 | Loss: 0.00001057
Iteration 151/1000 | Loss: 0.00001057
Iteration 152/1000 | Loss: 0.00001057
Iteration 153/1000 | Loss: 0.00001057
Iteration 154/1000 | Loss: 0.00001057
Iteration 155/1000 | Loss: 0.00001057
Iteration 156/1000 | Loss: 0.00001057
Iteration 157/1000 | Loss: 0.00001057
Iteration 158/1000 | Loss: 0.00001057
Iteration 159/1000 | Loss: 0.00001057
Iteration 160/1000 | Loss: 0.00001057
Iteration 161/1000 | Loss: 0.00001056
Iteration 162/1000 | Loss: 0.00001056
Iteration 163/1000 | Loss: 0.00001056
Iteration 164/1000 | Loss: 0.00001056
Iteration 165/1000 | Loss: 0.00001056
Iteration 166/1000 | Loss: 0.00001056
Iteration 167/1000 | Loss: 0.00001056
Iteration 168/1000 | Loss: 0.00001056
Iteration 169/1000 | Loss: 0.00001056
Iteration 170/1000 | Loss: 0.00001056
Iteration 171/1000 | Loss: 0.00001056
Iteration 172/1000 | Loss: 0.00001056
Iteration 173/1000 | Loss: 0.00001056
Iteration 174/1000 | Loss: 0.00001056
Iteration 175/1000 | Loss: 0.00001056
Iteration 176/1000 | Loss: 0.00001056
Iteration 177/1000 | Loss: 0.00001056
Iteration 178/1000 | Loss: 0.00001056
Iteration 179/1000 | Loss: 0.00001056
Iteration 180/1000 | Loss: 0.00001055
Iteration 181/1000 | Loss: 0.00001055
Iteration 182/1000 | Loss: 0.00001055
Iteration 183/1000 | Loss: 0.00001055
Iteration 184/1000 | Loss: 0.00001055
Iteration 185/1000 | Loss: 0.00001055
Iteration 186/1000 | Loss: 0.00001055
Iteration 187/1000 | Loss: 0.00001055
Iteration 188/1000 | Loss: 0.00001055
Iteration 189/1000 | Loss: 0.00001055
Iteration 190/1000 | Loss: 0.00001054
Iteration 191/1000 | Loss: 0.00001054
Iteration 192/1000 | Loss: 0.00001054
Iteration 193/1000 | Loss: 0.00001054
Iteration 194/1000 | Loss: 0.00001054
Iteration 195/1000 | Loss: 0.00001054
Iteration 196/1000 | Loss: 0.00001054
Iteration 197/1000 | Loss: 0.00001054
Iteration 198/1000 | Loss: 0.00001053
Iteration 199/1000 | Loss: 0.00001053
Iteration 200/1000 | Loss: 0.00001053
Iteration 201/1000 | Loss: 0.00001053
Iteration 202/1000 | Loss: 0.00001053
Iteration 203/1000 | Loss: 0.00001053
Iteration 204/1000 | Loss: 0.00001053
Iteration 205/1000 | Loss: 0.00001053
Iteration 206/1000 | Loss: 0.00001053
Iteration 207/1000 | Loss: 0.00001053
Iteration 208/1000 | Loss: 0.00001053
Iteration 209/1000 | Loss: 0.00001053
Iteration 210/1000 | Loss: 0.00001053
Iteration 211/1000 | Loss: 0.00001053
Iteration 212/1000 | Loss: 0.00001053
Iteration 213/1000 | Loss: 0.00001053
Iteration 214/1000 | Loss: 0.00001053
Iteration 215/1000 | Loss: 0.00001053
Iteration 216/1000 | Loss: 0.00001053
Iteration 217/1000 | Loss: 0.00001053
Iteration 218/1000 | Loss: 0.00001053
Iteration 219/1000 | Loss: 0.00001053
Iteration 220/1000 | Loss: 0.00001053
Iteration 221/1000 | Loss: 0.00001053
Iteration 222/1000 | Loss: 0.00001053
Iteration 223/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.053161577146966e-05, 1.053161577146966e-05, 1.053161577146966e-05, 1.053161577146966e-05, 1.053161577146966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.053161577146966e-05

Optimization complete. Final v2v error: 2.6118292808532715 mm

Highest mean error: 3.7914700508117676 mm for frame 77

Lowest mean error: 2.2710816860198975 mm for frame 128

Saving results

Total time: 36.09983563423157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872576
Iteration 2/25 | Loss: 0.00155859
Iteration 3/25 | Loss: 0.00118444
Iteration 4/25 | Loss: 0.00112611
Iteration 5/25 | Loss: 0.00113445
Iteration 6/25 | Loss: 0.00111616
Iteration 7/25 | Loss: 0.00108767
Iteration 8/25 | Loss: 0.00107340
Iteration 9/25 | Loss: 0.00107222
Iteration 10/25 | Loss: 0.00106978
Iteration 11/25 | Loss: 0.00106651
Iteration 12/25 | Loss: 0.00106553
Iteration 13/25 | Loss: 0.00106511
Iteration 14/25 | Loss: 0.00106500
Iteration 15/25 | Loss: 0.00106498
Iteration 16/25 | Loss: 0.00106497
Iteration 17/25 | Loss: 0.00106496
Iteration 18/25 | Loss: 0.00106496
Iteration 19/25 | Loss: 0.00106496
Iteration 20/25 | Loss: 0.00106496
Iteration 21/25 | Loss: 0.00106496
Iteration 22/25 | Loss: 0.00106496
Iteration 23/25 | Loss: 0.00106496
Iteration 24/25 | Loss: 0.00106496
Iteration 25/25 | Loss: 0.00106495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25023484
Iteration 2/25 | Loss: 0.00050553
Iteration 3/25 | Loss: 0.00050553
Iteration 4/25 | Loss: 0.00050553
Iteration 5/25 | Loss: 0.00050553
Iteration 6/25 | Loss: 0.00050553
Iteration 7/25 | Loss: 0.00050553
Iteration 8/25 | Loss: 0.00050553
Iteration 9/25 | Loss: 0.00050553
Iteration 10/25 | Loss: 0.00050553
Iteration 11/25 | Loss: 0.00050553
Iteration 12/25 | Loss: 0.00050553
Iteration 13/25 | Loss: 0.00050553
Iteration 14/25 | Loss: 0.00050553
Iteration 15/25 | Loss: 0.00050553
Iteration 16/25 | Loss: 0.00050553
Iteration 17/25 | Loss: 0.00050553
Iteration 18/25 | Loss: 0.00050553
Iteration 19/25 | Loss: 0.00050553
Iteration 20/25 | Loss: 0.00050553
Iteration 21/25 | Loss: 0.00050553
Iteration 22/25 | Loss: 0.00050553
Iteration 23/25 | Loss: 0.00050553
Iteration 24/25 | Loss: 0.00050553
Iteration 25/25 | Loss: 0.00050553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050553
Iteration 2/1000 | Loss: 0.00004552
Iteration 3/1000 | Loss: 0.00002719
Iteration 4/1000 | Loss: 0.00002247
Iteration 5/1000 | Loss: 0.00002065
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001882
Iteration 9/1000 | Loss: 0.00001848
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00001802
Iteration 13/1000 | Loss: 0.00001800
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001786
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001769
Iteration 19/1000 | Loss: 0.00001768
Iteration 20/1000 | Loss: 0.00001767
Iteration 21/1000 | Loss: 0.00001766
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001763
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001762
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001757
Iteration 28/1000 | Loss: 0.00001748
Iteration 29/1000 | Loss: 0.00001744
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001739
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001739
Iteration 41/1000 | Loss: 0.00001739
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001736
Iteration 49/1000 | Loss: 0.00001736
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001736
Iteration 54/1000 | Loss: 0.00001736
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001735
Iteration 57/1000 | Loss: 0.00001735
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001731
Iteration 83/1000 | Loss: 0.00001731
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001730
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.7301252228207886e-05, 1.7301252228207886e-05, 1.7301252228207886e-05, 1.7301252228207886e-05, 1.7301252228207886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7301252228207886e-05

Optimization complete. Final v2v error: 3.513474702835083 mm

Highest mean error: 3.850343942642212 mm for frame 63

Lowest mean error: 3.0949342250823975 mm for frame 35

Saving results

Total time: 56.53759455680847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857558
Iteration 2/25 | Loss: 0.00159719
Iteration 3/25 | Loss: 0.00116473
Iteration 4/25 | Loss: 0.00110692
Iteration 5/25 | Loss: 0.00110053
Iteration 6/25 | Loss: 0.00109950
Iteration 7/25 | Loss: 0.00109944
Iteration 8/25 | Loss: 0.00109944
Iteration 9/25 | Loss: 0.00109944
Iteration 10/25 | Loss: 0.00109944
Iteration 11/25 | Loss: 0.00109944
Iteration 12/25 | Loss: 0.00109944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010994437616318464, 0.0010994437616318464, 0.0010994437616318464, 0.0010994437616318464, 0.0010994437616318464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010994437616318464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37978864
Iteration 2/25 | Loss: 0.00042131
Iteration 3/25 | Loss: 0.00042131
Iteration 4/25 | Loss: 0.00042131
Iteration 5/25 | Loss: 0.00042131
Iteration 6/25 | Loss: 0.00042131
Iteration 7/25 | Loss: 0.00042131
Iteration 8/25 | Loss: 0.00042131
Iteration 9/25 | Loss: 0.00042131
Iteration 10/25 | Loss: 0.00042131
Iteration 11/25 | Loss: 0.00042131
Iteration 12/25 | Loss: 0.00042131
Iteration 13/25 | Loss: 0.00042131
Iteration 14/25 | Loss: 0.00042131
Iteration 15/25 | Loss: 0.00042131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004213059146422893, 0.0004213059146422893, 0.0004213059146422893, 0.0004213059146422893, 0.0004213059146422893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004213059146422893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042131
Iteration 2/1000 | Loss: 0.00004975
Iteration 3/1000 | Loss: 0.00003695
Iteration 4/1000 | Loss: 0.00003095
Iteration 5/1000 | Loss: 0.00002905
Iteration 6/1000 | Loss: 0.00002788
Iteration 7/1000 | Loss: 0.00002722
Iteration 8/1000 | Loss: 0.00002678
Iteration 9/1000 | Loss: 0.00002634
Iteration 10/1000 | Loss: 0.00002596
Iteration 11/1000 | Loss: 0.00002577
Iteration 12/1000 | Loss: 0.00002564
Iteration 13/1000 | Loss: 0.00002549
Iteration 14/1000 | Loss: 0.00002538
Iteration 15/1000 | Loss: 0.00002535
Iteration 16/1000 | Loss: 0.00002534
Iteration 17/1000 | Loss: 0.00002534
Iteration 18/1000 | Loss: 0.00002530
Iteration 19/1000 | Loss: 0.00002529
Iteration 20/1000 | Loss: 0.00002525
Iteration 21/1000 | Loss: 0.00002524
Iteration 22/1000 | Loss: 0.00002524
Iteration 23/1000 | Loss: 0.00002524
Iteration 24/1000 | Loss: 0.00002524
Iteration 25/1000 | Loss: 0.00002522
Iteration 26/1000 | Loss: 0.00002520
Iteration 27/1000 | Loss: 0.00002519
Iteration 28/1000 | Loss: 0.00002519
Iteration 29/1000 | Loss: 0.00002519
Iteration 30/1000 | Loss: 0.00002518
Iteration 31/1000 | Loss: 0.00002518
Iteration 32/1000 | Loss: 0.00002518
Iteration 33/1000 | Loss: 0.00002518
Iteration 34/1000 | Loss: 0.00002518
Iteration 35/1000 | Loss: 0.00002518
Iteration 36/1000 | Loss: 0.00002518
Iteration 37/1000 | Loss: 0.00002518
Iteration 38/1000 | Loss: 0.00002518
Iteration 39/1000 | Loss: 0.00002518
Iteration 40/1000 | Loss: 0.00002518
Iteration 41/1000 | Loss: 0.00002518
Iteration 42/1000 | Loss: 0.00002517
Iteration 43/1000 | Loss: 0.00002516
Iteration 44/1000 | Loss: 0.00002515
Iteration 45/1000 | Loss: 0.00002515
Iteration 46/1000 | Loss: 0.00002515
Iteration 47/1000 | Loss: 0.00002514
Iteration 48/1000 | Loss: 0.00002514
Iteration 49/1000 | Loss: 0.00002514
Iteration 50/1000 | Loss: 0.00002514
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002514
Iteration 54/1000 | Loss: 0.00002514
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002514
Iteration 57/1000 | Loss: 0.00002513
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002512
Iteration 60/1000 | Loss: 0.00002512
Iteration 61/1000 | Loss: 0.00002512
Iteration 62/1000 | Loss: 0.00002512
Iteration 63/1000 | Loss: 0.00002512
Iteration 64/1000 | Loss: 0.00002512
Iteration 65/1000 | Loss: 0.00002512
Iteration 66/1000 | Loss: 0.00002512
Iteration 67/1000 | Loss: 0.00002512
Iteration 68/1000 | Loss: 0.00002511
Iteration 69/1000 | Loss: 0.00002511
Iteration 70/1000 | Loss: 0.00002511
Iteration 71/1000 | Loss: 0.00002511
Iteration 72/1000 | Loss: 0.00002511
Iteration 73/1000 | Loss: 0.00002511
Iteration 74/1000 | Loss: 0.00002510
Iteration 75/1000 | Loss: 0.00002510
Iteration 76/1000 | Loss: 0.00002510
Iteration 77/1000 | Loss: 0.00002510
Iteration 78/1000 | Loss: 0.00002510
Iteration 79/1000 | Loss: 0.00002509
Iteration 80/1000 | Loss: 0.00002509
Iteration 81/1000 | Loss: 0.00002509
Iteration 82/1000 | Loss: 0.00002509
Iteration 83/1000 | Loss: 0.00002509
Iteration 84/1000 | Loss: 0.00002509
Iteration 85/1000 | Loss: 0.00002509
Iteration 86/1000 | Loss: 0.00002509
Iteration 87/1000 | Loss: 0.00002508
Iteration 88/1000 | Loss: 0.00002508
Iteration 89/1000 | Loss: 0.00002508
Iteration 90/1000 | Loss: 0.00002508
Iteration 91/1000 | Loss: 0.00002508
Iteration 92/1000 | Loss: 0.00002508
Iteration 93/1000 | Loss: 0.00002507
Iteration 94/1000 | Loss: 0.00002507
Iteration 95/1000 | Loss: 0.00002507
Iteration 96/1000 | Loss: 0.00002507
Iteration 97/1000 | Loss: 0.00002507
Iteration 98/1000 | Loss: 0.00002507
Iteration 99/1000 | Loss: 0.00002507
Iteration 100/1000 | Loss: 0.00002507
Iteration 101/1000 | Loss: 0.00002507
Iteration 102/1000 | Loss: 0.00002507
Iteration 103/1000 | Loss: 0.00002507
Iteration 104/1000 | Loss: 0.00002506
Iteration 105/1000 | Loss: 0.00002506
Iteration 106/1000 | Loss: 0.00002506
Iteration 107/1000 | Loss: 0.00002506
Iteration 108/1000 | Loss: 0.00002506
Iteration 109/1000 | Loss: 0.00002506
Iteration 110/1000 | Loss: 0.00002505
Iteration 111/1000 | Loss: 0.00002505
Iteration 112/1000 | Loss: 0.00002505
Iteration 113/1000 | Loss: 0.00002505
Iteration 114/1000 | Loss: 0.00002505
Iteration 115/1000 | Loss: 0.00002504
Iteration 116/1000 | Loss: 0.00002504
Iteration 117/1000 | Loss: 0.00002504
Iteration 118/1000 | Loss: 0.00002504
Iteration 119/1000 | Loss: 0.00002504
Iteration 120/1000 | Loss: 0.00002504
Iteration 121/1000 | Loss: 0.00002504
Iteration 122/1000 | Loss: 0.00002504
Iteration 123/1000 | Loss: 0.00002504
Iteration 124/1000 | Loss: 0.00002504
Iteration 125/1000 | Loss: 0.00002504
Iteration 126/1000 | Loss: 0.00002504
Iteration 127/1000 | Loss: 0.00002504
Iteration 128/1000 | Loss: 0.00002504
Iteration 129/1000 | Loss: 0.00002503
Iteration 130/1000 | Loss: 0.00002503
Iteration 131/1000 | Loss: 0.00002503
Iteration 132/1000 | Loss: 0.00002503
Iteration 133/1000 | Loss: 0.00002503
Iteration 134/1000 | Loss: 0.00002503
Iteration 135/1000 | Loss: 0.00002503
Iteration 136/1000 | Loss: 0.00002503
Iteration 137/1000 | Loss: 0.00002503
Iteration 138/1000 | Loss: 0.00002503
Iteration 139/1000 | Loss: 0.00002502
Iteration 140/1000 | Loss: 0.00002502
Iteration 141/1000 | Loss: 0.00002502
Iteration 142/1000 | Loss: 0.00002502
Iteration 143/1000 | Loss: 0.00002502
Iteration 144/1000 | Loss: 0.00002502
Iteration 145/1000 | Loss: 0.00002502
Iteration 146/1000 | Loss: 0.00002502
Iteration 147/1000 | Loss: 0.00002502
Iteration 148/1000 | Loss: 0.00002502
Iteration 149/1000 | Loss: 0.00002502
Iteration 150/1000 | Loss: 0.00002502
Iteration 151/1000 | Loss: 0.00002502
Iteration 152/1000 | Loss: 0.00002502
Iteration 153/1000 | Loss: 0.00002502
Iteration 154/1000 | Loss: 0.00002501
Iteration 155/1000 | Loss: 0.00002501
Iteration 156/1000 | Loss: 0.00002501
Iteration 157/1000 | Loss: 0.00002501
Iteration 158/1000 | Loss: 0.00002501
Iteration 159/1000 | Loss: 0.00002501
Iteration 160/1000 | Loss: 0.00002501
Iteration 161/1000 | Loss: 0.00002501
Iteration 162/1000 | Loss: 0.00002501
Iteration 163/1000 | Loss: 0.00002501
Iteration 164/1000 | Loss: 0.00002500
Iteration 165/1000 | Loss: 0.00002500
Iteration 166/1000 | Loss: 0.00002500
Iteration 167/1000 | Loss: 0.00002500
Iteration 168/1000 | Loss: 0.00002500
Iteration 169/1000 | Loss: 0.00002500
Iteration 170/1000 | Loss: 0.00002500
Iteration 171/1000 | Loss: 0.00002500
Iteration 172/1000 | Loss: 0.00002499
Iteration 173/1000 | Loss: 0.00002499
Iteration 174/1000 | Loss: 0.00002499
Iteration 175/1000 | Loss: 0.00002499
Iteration 176/1000 | Loss: 0.00002499
Iteration 177/1000 | Loss: 0.00002499
Iteration 178/1000 | Loss: 0.00002499
Iteration 179/1000 | Loss: 0.00002499
Iteration 180/1000 | Loss: 0.00002499
Iteration 181/1000 | Loss: 0.00002499
Iteration 182/1000 | Loss: 0.00002499
Iteration 183/1000 | Loss: 0.00002499
Iteration 184/1000 | Loss: 0.00002499
Iteration 185/1000 | Loss: 0.00002499
Iteration 186/1000 | Loss: 0.00002499
Iteration 187/1000 | Loss: 0.00002499
Iteration 188/1000 | Loss: 0.00002499
Iteration 189/1000 | Loss: 0.00002499
Iteration 190/1000 | Loss: 0.00002499
Iteration 191/1000 | Loss: 0.00002499
Iteration 192/1000 | Loss: 0.00002499
Iteration 193/1000 | Loss: 0.00002499
Iteration 194/1000 | Loss: 0.00002499
Iteration 195/1000 | Loss: 0.00002499
Iteration 196/1000 | Loss: 0.00002499
Iteration 197/1000 | Loss: 0.00002499
Iteration 198/1000 | Loss: 0.00002499
Iteration 199/1000 | Loss: 0.00002499
Iteration 200/1000 | Loss: 0.00002499
Iteration 201/1000 | Loss: 0.00002499
Iteration 202/1000 | Loss: 0.00002499
Iteration 203/1000 | Loss: 0.00002499
Iteration 204/1000 | Loss: 0.00002499
Iteration 205/1000 | Loss: 0.00002499
Iteration 206/1000 | Loss: 0.00002499
Iteration 207/1000 | Loss: 0.00002499
Iteration 208/1000 | Loss: 0.00002499
Iteration 209/1000 | Loss: 0.00002499
Iteration 210/1000 | Loss: 0.00002499
Iteration 211/1000 | Loss: 0.00002499
Iteration 212/1000 | Loss: 0.00002499
Iteration 213/1000 | Loss: 0.00002499
Iteration 214/1000 | Loss: 0.00002499
Iteration 215/1000 | Loss: 0.00002499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.4985283744172193e-05, 2.4985283744172193e-05, 2.4985283744172193e-05, 2.4985283744172193e-05, 2.4985283744172193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4985283744172193e-05

Optimization complete. Final v2v error: 4.08963680267334 mm

Highest mean error: 5.387420654296875 mm for frame 36

Lowest mean error: 3.4104163646698 mm for frame 14

Saving results

Total time: 39.577521562576294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425357
Iteration 2/25 | Loss: 0.00106208
Iteration 3/25 | Loss: 0.00097104
Iteration 4/25 | Loss: 0.00096117
Iteration 5/25 | Loss: 0.00095737
Iteration 6/25 | Loss: 0.00095718
Iteration 7/25 | Loss: 0.00095718
Iteration 8/25 | Loss: 0.00095718
Iteration 9/25 | Loss: 0.00095718
Iteration 10/25 | Loss: 0.00095718
Iteration 11/25 | Loss: 0.00095718
Iteration 12/25 | Loss: 0.00095718
Iteration 13/25 | Loss: 0.00095718
Iteration 14/25 | Loss: 0.00095718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009571791742928326, 0.0009571791742928326, 0.0009571791742928326, 0.0009571791742928326, 0.0009571791742928326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009571791742928326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42951691
Iteration 2/25 | Loss: 0.00049710
Iteration 3/25 | Loss: 0.00049709
Iteration 4/25 | Loss: 0.00049709
Iteration 5/25 | Loss: 0.00049709
Iteration 6/25 | Loss: 0.00049709
Iteration 7/25 | Loss: 0.00049709
Iteration 8/25 | Loss: 0.00049709
Iteration 9/25 | Loss: 0.00049709
Iteration 10/25 | Loss: 0.00049709
Iteration 11/25 | Loss: 0.00049709
Iteration 12/25 | Loss: 0.00049709
Iteration 13/25 | Loss: 0.00049709
Iteration 14/25 | Loss: 0.00049709
Iteration 15/25 | Loss: 0.00049709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004970923764631152, 0.0004970923764631152, 0.0004970923764631152, 0.0004970923764631152, 0.0004970923764631152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004970923764631152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049709
Iteration 2/1000 | Loss: 0.00001714
Iteration 3/1000 | Loss: 0.00001128
Iteration 4/1000 | Loss: 0.00000979
Iteration 5/1000 | Loss: 0.00000929
Iteration 6/1000 | Loss: 0.00000899
Iteration 7/1000 | Loss: 0.00000881
Iteration 8/1000 | Loss: 0.00000877
Iteration 9/1000 | Loss: 0.00000863
Iteration 10/1000 | Loss: 0.00000862
Iteration 11/1000 | Loss: 0.00000860
Iteration 12/1000 | Loss: 0.00000859
Iteration 13/1000 | Loss: 0.00000858
Iteration 14/1000 | Loss: 0.00000858
Iteration 15/1000 | Loss: 0.00000858
Iteration 16/1000 | Loss: 0.00000858
Iteration 17/1000 | Loss: 0.00000857
Iteration 18/1000 | Loss: 0.00000857
Iteration 19/1000 | Loss: 0.00000857
Iteration 20/1000 | Loss: 0.00000857
Iteration 21/1000 | Loss: 0.00000857
Iteration 22/1000 | Loss: 0.00000856
Iteration 23/1000 | Loss: 0.00000856
Iteration 24/1000 | Loss: 0.00000856
Iteration 25/1000 | Loss: 0.00000854
Iteration 26/1000 | Loss: 0.00000854
Iteration 27/1000 | Loss: 0.00000854
Iteration 28/1000 | Loss: 0.00000854
Iteration 29/1000 | Loss: 0.00000854
Iteration 30/1000 | Loss: 0.00000854
Iteration 31/1000 | Loss: 0.00000853
Iteration 32/1000 | Loss: 0.00000853
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000853
Iteration 35/1000 | Loss: 0.00000853
Iteration 36/1000 | Loss: 0.00000853
Iteration 37/1000 | Loss: 0.00000851
Iteration 38/1000 | Loss: 0.00000850
Iteration 39/1000 | Loss: 0.00000850
Iteration 40/1000 | Loss: 0.00000850
Iteration 41/1000 | Loss: 0.00000849
Iteration 42/1000 | Loss: 0.00000849
Iteration 43/1000 | Loss: 0.00000849
Iteration 44/1000 | Loss: 0.00000848
Iteration 45/1000 | Loss: 0.00000848
Iteration 46/1000 | Loss: 0.00000848
Iteration 47/1000 | Loss: 0.00000847
Iteration 48/1000 | Loss: 0.00000847
Iteration 49/1000 | Loss: 0.00000847
Iteration 50/1000 | Loss: 0.00000847
Iteration 51/1000 | Loss: 0.00000847
Iteration 52/1000 | Loss: 0.00000847
Iteration 53/1000 | Loss: 0.00000847
Iteration 54/1000 | Loss: 0.00000847
Iteration 55/1000 | Loss: 0.00000846
Iteration 56/1000 | Loss: 0.00000846
Iteration 57/1000 | Loss: 0.00000846
Iteration 58/1000 | Loss: 0.00000846
Iteration 59/1000 | Loss: 0.00000846
Iteration 60/1000 | Loss: 0.00000846
Iteration 61/1000 | Loss: 0.00000846
Iteration 62/1000 | Loss: 0.00000846
Iteration 63/1000 | Loss: 0.00000845
Iteration 64/1000 | Loss: 0.00000845
Iteration 65/1000 | Loss: 0.00000845
Iteration 66/1000 | Loss: 0.00000844
Iteration 67/1000 | Loss: 0.00000844
Iteration 68/1000 | Loss: 0.00000844
Iteration 69/1000 | Loss: 0.00000844
Iteration 70/1000 | Loss: 0.00000844
Iteration 71/1000 | Loss: 0.00000844
Iteration 72/1000 | Loss: 0.00000844
Iteration 73/1000 | Loss: 0.00000844
Iteration 74/1000 | Loss: 0.00000843
Iteration 75/1000 | Loss: 0.00000843
Iteration 76/1000 | Loss: 0.00000843
Iteration 77/1000 | Loss: 0.00000843
Iteration 78/1000 | Loss: 0.00000842
Iteration 79/1000 | Loss: 0.00000842
Iteration 80/1000 | Loss: 0.00000842
Iteration 81/1000 | Loss: 0.00000842
Iteration 82/1000 | Loss: 0.00000842
Iteration 83/1000 | Loss: 0.00000841
Iteration 84/1000 | Loss: 0.00000841
Iteration 85/1000 | Loss: 0.00000841
Iteration 86/1000 | Loss: 0.00000841
Iteration 87/1000 | Loss: 0.00000841
Iteration 88/1000 | Loss: 0.00000840
Iteration 89/1000 | Loss: 0.00000840
Iteration 90/1000 | Loss: 0.00000840
Iteration 91/1000 | Loss: 0.00000840
Iteration 92/1000 | Loss: 0.00000840
Iteration 93/1000 | Loss: 0.00000839
Iteration 94/1000 | Loss: 0.00000839
Iteration 95/1000 | Loss: 0.00000839
Iteration 96/1000 | Loss: 0.00000839
Iteration 97/1000 | Loss: 0.00000839
Iteration 98/1000 | Loss: 0.00000839
Iteration 99/1000 | Loss: 0.00000839
Iteration 100/1000 | Loss: 0.00000839
Iteration 101/1000 | Loss: 0.00000838
Iteration 102/1000 | Loss: 0.00000838
Iteration 103/1000 | Loss: 0.00000838
Iteration 104/1000 | Loss: 0.00000838
Iteration 105/1000 | Loss: 0.00000838
Iteration 106/1000 | Loss: 0.00000837
Iteration 107/1000 | Loss: 0.00000837
Iteration 108/1000 | Loss: 0.00000837
Iteration 109/1000 | Loss: 0.00000837
Iteration 110/1000 | Loss: 0.00000837
Iteration 111/1000 | Loss: 0.00000837
Iteration 112/1000 | Loss: 0.00000837
Iteration 113/1000 | Loss: 0.00000836
Iteration 114/1000 | Loss: 0.00000836
Iteration 115/1000 | Loss: 0.00000836
Iteration 116/1000 | Loss: 0.00000836
Iteration 117/1000 | Loss: 0.00000836
Iteration 118/1000 | Loss: 0.00000836
Iteration 119/1000 | Loss: 0.00000835
Iteration 120/1000 | Loss: 0.00000835
Iteration 121/1000 | Loss: 0.00000835
Iteration 122/1000 | Loss: 0.00000834
Iteration 123/1000 | Loss: 0.00000834
Iteration 124/1000 | Loss: 0.00000834
Iteration 125/1000 | Loss: 0.00000834
Iteration 126/1000 | Loss: 0.00000834
Iteration 127/1000 | Loss: 0.00000834
Iteration 128/1000 | Loss: 0.00000833
Iteration 129/1000 | Loss: 0.00000833
Iteration 130/1000 | Loss: 0.00000833
Iteration 131/1000 | Loss: 0.00000833
Iteration 132/1000 | Loss: 0.00000833
Iteration 133/1000 | Loss: 0.00000832
Iteration 134/1000 | Loss: 0.00000832
Iteration 135/1000 | Loss: 0.00000832
Iteration 136/1000 | Loss: 0.00000832
Iteration 137/1000 | Loss: 0.00000831
Iteration 138/1000 | Loss: 0.00000831
Iteration 139/1000 | Loss: 0.00000831
Iteration 140/1000 | Loss: 0.00000831
Iteration 141/1000 | Loss: 0.00000831
Iteration 142/1000 | Loss: 0.00000831
Iteration 143/1000 | Loss: 0.00000831
Iteration 144/1000 | Loss: 0.00000831
Iteration 145/1000 | Loss: 0.00000831
Iteration 146/1000 | Loss: 0.00000831
Iteration 147/1000 | Loss: 0.00000831
Iteration 148/1000 | Loss: 0.00000831
Iteration 149/1000 | Loss: 0.00000831
Iteration 150/1000 | Loss: 0.00000831
Iteration 151/1000 | Loss: 0.00000831
Iteration 152/1000 | Loss: 0.00000830
Iteration 153/1000 | Loss: 0.00000830
Iteration 154/1000 | Loss: 0.00000830
Iteration 155/1000 | Loss: 0.00000830
Iteration 156/1000 | Loss: 0.00000830
Iteration 157/1000 | Loss: 0.00000830
Iteration 158/1000 | Loss: 0.00000830
Iteration 159/1000 | Loss: 0.00000830
Iteration 160/1000 | Loss: 0.00000830
Iteration 161/1000 | Loss: 0.00000830
Iteration 162/1000 | Loss: 0.00000829
Iteration 163/1000 | Loss: 0.00000829
Iteration 164/1000 | Loss: 0.00000829
Iteration 165/1000 | Loss: 0.00000829
Iteration 166/1000 | Loss: 0.00000829
Iteration 167/1000 | Loss: 0.00000829
Iteration 168/1000 | Loss: 0.00000829
Iteration 169/1000 | Loss: 0.00000829
Iteration 170/1000 | Loss: 0.00000829
Iteration 171/1000 | Loss: 0.00000829
Iteration 172/1000 | Loss: 0.00000829
Iteration 173/1000 | Loss: 0.00000829
Iteration 174/1000 | Loss: 0.00000828
Iteration 175/1000 | Loss: 0.00000828
Iteration 176/1000 | Loss: 0.00000828
Iteration 177/1000 | Loss: 0.00000828
Iteration 178/1000 | Loss: 0.00000828
Iteration 179/1000 | Loss: 0.00000828
Iteration 180/1000 | Loss: 0.00000828
Iteration 181/1000 | Loss: 0.00000828
Iteration 182/1000 | Loss: 0.00000828
Iteration 183/1000 | Loss: 0.00000828
Iteration 184/1000 | Loss: 0.00000828
Iteration 185/1000 | Loss: 0.00000828
Iteration 186/1000 | Loss: 0.00000828
Iteration 187/1000 | Loss: 0.00000828
Iteration 188/1000 | Loss: 0.00000828
Iteration 189/1000 | Loss: 0.00000828
Iteration 190/1000 | Loss: 0.00000828
Iteration 191/1000 | Loss: 0.00000828
Iteration 192/1000 | Loss: 0.00000828
Iteration 193/1000 | Loss: 0.00000828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [8.279595931526273e-06, 8.279595931526273e-06, 8.279595931526273e-06, 8.279595931526273e-06, 8.279595931526273e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.279595931526273e-06

Optimization complete. Final v2v error: 2.4835946559906006 mm

Highest mean error: 2.6984190940856934 mm for frame 104

Lowest mean error: 2.3224031925201416 mm for frame 155

Saving results

Total time: 36.922096252441406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521573
Iteration 2/25 | Loss: 0.00117191
Iteration 3/25 | Loss: 0.00099444
Iteration 4/25 | Loss: 0.00096722
Iteration 5/25 | Loss: 0.00096075
Iteration 6/25 | Loss: 0.00095986
Iteration 7/25 | Loss: 0.00095986
Iteration 8/25 | Loss: 0.00095986
Iteration 9/25 | Loss: 0.00095986
Iteration 10/25 | Loss: 0.00095986
Iteration 11/25 | Loss: 0.00095986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009598620235919952, 0.0009598620235919952, 0.0009598620235919952, 0.0009598620235919952, 0.0009598620235919952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009598620235919952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32909894
Iteration 2/25 | Loss: 0.00054463
Iteration 3/25 | Loss: 0.00054460
Iteration 4/25 | Loss: 0.00054460
Iteration 5/25 | Loss: 0.00054460
Iteration 6/25 | Loss: 0.00054460
Iteration 7/25 | Loss: 0.00054460
Iteration 8/25 | Loss: 0.00054460
Iteration 9/25 | Loss: 0.00054460
Iteration 10/25 | Loss: 0.00054460
Iteration 11/25 | Loss: 0.00054460
Iteration 12/25 | Loss: 0.00054460
Iteration 13/25 | Loss: 0.00054460
Iteration 14/25 | Loss: 0.00054460
Iteration 15/25 | Loss: 0.00054460
Iteration 16/25 | Loss: 0.00054460
Iteration 17/25 | Loss: 0.00054460
Iteration 18/25 | Loss: 0.00054460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005445965798571706, 0.0005445965798571706, 0.0005445965798571706, 0.0005445965798571706, 0.0005445965798571706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005445965798571706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054460
Iteration 2/1000 | Loss: 0.00001754
Iteration 3/1000 | Loss: 0.00001278
Iteration 4/1000 | Loss: 0.00001207
Iteration 5/1000 | Loss: 0.00001121
Iteration 6/1000 | Loss: 0.00001097
Iteration 7/1000 | Loss: 0.00001088
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001062
Iteration 10/1000 | Loss: 0.00001061
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001040
Iteration 13/1000 | Loss: 0.00001038
Iteration 14/1000 | Loss: 0.00001037
Iteration 15/1000 | Loss: 0.00001036
Iteration 16/1000 | Loss: 0.00001035
Iteration 17/1000 | Loss: 0.00001033
Iteration 18/1000 | Loss: 0.00001033
Iteration 19/1000 | Loss: 0.00001033
Iteration 20/1000 | Loss: 0.00001033
Iteration 21/1000 | Loss: 0.00001032
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001032
Iteration 24/1000 | Loss: 0.00001028
Iteration 25/1000 | Loss: 0.00001027
Iteration 26/1000 | Loss: 0.00001027
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001026
Iteration 29/1000 | Loss: 0.00001025
Iteration 30/1000 | Loss: 0.00001025
Iteration 31/1000 | Loss: 0.00001025
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001023
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001022
Iteration 37/1000 | Loss: 0.00001022
Iteration 38/1000 | Loss: 0.00001022
Iteration 39/1000 | Loss: 0.00001022
Iteration 40/1000 | Loss: 0.00001021
Iteration 41/1000 | Loss: 0.00001021
Iteration 42/1000 | Loss: 0.00001021
Iteration 43/1000 | Loss: 0.00001021
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001021
Iteration 46/1000 | Loss: 0.00001021
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001020
Iteration 50/1000 | Loss: 0.00001020
Iteration 51/1000 | Loss: 0.00001020
Iteration 52/1000 | Loss: 0.00001020
Iteration 53/1000 | Loss: 0.00001020
Iteration 54/1000 | Loss: 0.00001020
Iteration 55/1000 | Loss: 0.00001020
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001019
Iteration 59/1000 | Loss: 0.00001019
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001018
Iteration 62/1000 | Loss: 0.00001018
Iteration 63/1000 | Loss: 0.00001018
Iteration 64/1000 | Loss: 0.00001018
Iteration 65/1000 | Loss: 0.00001018
Iteration 66/1000 | Loss: 0.00001018
Iteration 67/1000 | Loss: 0.00001018
Iteration 68/1000 | Loss: 0.00001018
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001017
Iteration 71/1000 | Loss: 0.00001017
Iteration 72/1000 | Loss: 0.00001017
Iteration 73/1000 | Loss: 0.00001017
Iteration 74/1000 | Loss: 0.00001017
Iteration 75/1000 | Loss: 0.00001017
Iteration 76/1000 | Loss: 0.00001017
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001016
Iteration 79/1000 | Loss: 0.00001016
Iteration 80/1000 | Loss: 0.00001016
Iteration 81/1000 | Loss: 0.00001016
Iteration 82/1000 | Loss: 0.00001016
Iteration 83/1000 | Loss: 0.00001016
Iteration 84/1000 | Loss: 0.00001016
Iteration 85/1000 | Loss: 0.00001016
Iteration 86/1000 | Loss: 0.00001016
Iteration 87/1000 | Loss: 0.00001016
Iteration 88/1000 | Loss: 0.00001016
Iteration 89/1000 | Loss: 0.00001016
Iteration 90/1000 | Loss: 0.00001016
Iteration 91/1000 | Loss: 0.00001016
Iteration 92/1000 | Loss: 0.00001016
Iteration 93/1000 | Loss: 0.00001016
Iteration 94/1000 | Loss: 0.00001016
Iteration 95/1000 | Loss: 0.00001016
Iteration 96/1000 | Loss: 0.00001016
Iteration 97/1000 | Loss: 0.00001016
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00001016
Iteration 100/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.0159566045331303e-05, 1.0159566045331303e-05, 1.0159566045331303e-05, 1.0159566045331303e-05, 1.0159566045331303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0159566045331303e-05

Optimization complete. Final v2v error: 2.706037998199463 mm

Highest mean error: 2.875959873199463 mm for frame 148

Lowest mean error: 2.5905563831329346 mm for frame 210

Saving results

Total time: 31.127132892608643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00613974
Iteration 2/25 | Loss: 0.00132807
Iteration 3/25 | Loss: 0.00104316
Iteration 4/25 | Loss: 0.00096061
Iteration 5/25 | Loss: 0.00094457
Iteration 6/25 | Loss: 0.00093818
Iteration 7/25 | Loss: 0.00094044
Iteration 8/25 | Loss: 0.00093976
Iteration 9/25 | Loss: 0.00093571
Iteration 10/25 | Loss: 0.00093343
Iteration 11/25 | Loss: 0.00093257
Iteration 12/25 | Loss: 0.00093200
Iteration 13/25 | Loss: 0.00093185
Iteration 14/25 | Loss: 0.00093175
Iteration 15/25 | Loss: 0.00093169
Iteration 16/25 | Loss: 0.00093168
Iteration 17/25 | Loss: 0.00093168
Iteration 18/25 | Loss: 0.00093168
Iteration 19/25 | Loss: 0.00093168
Iteration 20/25 | Loss: 0.00093168
Iteration 21/25 | Loss: 0.00093168
Iteration 22/25 | Loss: 0.00093168
Iteration 23/25 | Loss: 0.00093168
Iteration 24/25 | Loss: 0.00093168
Iteration 25/25 | Loss: 0.00093168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.63270473
Iteration 2/25 | Loss: 0.00061458
Iteration 3/25 | Loss: 0.00061458
Iteration 4/25 | Loss: 0.00061458
Iteration 5/25 | Loss: 0.00061458
Iteration 6/25 | Loss: 0.00061457
Iteration 7/25 | Loss: 0.00061457
Iteration 8/25 | Loss: 0.00061457
Iteration 9/25 | Loss: 0.00061457
Iteration 10/25 | Loss: 0.00061457
Iteration 11/25 | Loss: 0.00061457
Iteration 12/25 | Loss: 0.00061457
Iteration 13/25 | Loss: 0.00061457
Iteration 14/25 | Loss: 0.00061457
Iteration 15/25 | Loss: 0.00061457
Iteration 16/25 | Loss: 0.00061457
Iteration 17/25 | Loss: 0.00061457
Iteration 18/25 | Loss: 0.00061457
Iteration 19/25 | Loss: 0.00061457
Iteration 20/25 | Loss: 0.00061457
Iteration 21/25 | Loss: 0.00061457
Iteration 22/25 | Loss: 0.00061457
Iteration 23/25 | Loss: 0.00061457
Iteration 24/25 | Loss: 0.00061457
Iteration 25/25 | Loss: 0.00061457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006145726074464619, 0.0006145726074464619, 0.0006145726074464619, 0.0006145726074464619, 0.0006145726074464619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006145726074464619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061457
Iteration 2/1000 | Loss: 0.00002226
Iteration 3/1000 | Loss: 0.00001388
Iteration 4/1000 | Loss: 0.00001245
Iteration 5/1000 | Loss: 0.00001155
Iteration 6/1000 | Loss: 0.00001114
Iteration 7/1000 | Loss: 0.00001082
Iteration 8/1000 | Loss: 0.00001061
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001054
Iteration 11/1000 | Loss: 0.00001043
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001023
Iteration 15/1000 | Loss: 0.00001022
Iteration 16/1000 | Loss: 0.00001021
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001018
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001017
Iteration 21/1000 | Loss: 0.00001016
Iteration 22/1000 | Loss: 0.00001016
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001013
Iteration 26/1000 | Loss: 0.00001012
Iteration 27/1000 | Loss: 0.00001012
Iteration 28/1000 | Loss: 0.00001012
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001011
Iteration 31/1000 | Loss: 0.00001011
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001011
Iteration 34/1000 | Loss: 0.00001010
Iteration 35/1000 | Loss: 0.00001010
Iteration 36/1000 | Loss: 0.00001010
Iteration 37/1000 | Loss: 0.00001009
Iteration 38/1000 | Loss: 0.00001009
Iteration 39/1000 | Loss: 0.00001009
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001008
Iteration 42/1000 | Loss: 0.00001008
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001004
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001003
Iteration 59/1000 | Loss: 0.00001003
Iteration 60/1000 | Loss: 0.00001003
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001003
Iteration 63/1000 | Loss: 0.00001003
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001002
Iteration 69/1000 | Loss: 0.00001002
Iteration 70/1000 | Loss: 0.00001001
Iteration 71/1000 | Loss: 0.00001001
Iteration 72/1000 | Loss: 0.00001001
Iteration 73/1000 | Loss: 0.00001001
Iteration 74/1000 | Loss: 0.00001001
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00001000
Iteration 77/1000 | Loss: 0.00001000
Iteration 78/1000 | Loss: 0.00001000
Iteration 79/1000 | Loss: 0.00001000
Iteration 80/1000 | Loss: 0.00001000
Iteration 81/1000 | Loss: 0.00001000
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000999
Iteration 84/1000 | Loss: 0.00000999
Iteration 85/1000 | Loss: 0.00000999
Iteration 86/1000 | Loss: 0.00000999
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000998
Iteration 90/1000 | Loss: 0.00000998
Iteration 91/1000 | Loss: 0.00000998
Iteration 92/1000 | Loss: 0.00000998
Iteration 93/1000 | Loss: 0.00000998
Iteration 94/1000 | Loss: 0.00000997
Iteration 95/1000 | Loss: 0.00000997
Iteration 96/1000 | Loss: 0.00000997
Iteration 97/1000 | Loss: 0.00000997
Iteration 98/1000 | Loss: 0.00000997
Iteration 99/1000 | Loss: 0.00000996
Iteration 100/1000 | Loss: 0.00000996
Iteration 101/1000 | Loss: 0.00000996
Iteration 102/1000 | Loss: 0.00000996
Iteration 103/1000 | Loss: 0.00000996
Iteration 104/1000 | Loss: 0.00000996
Iteration 105/1000 | Loss: 0.00000996
Iteration 106/1000 | Loss: 0.00000996
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000996
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000996
Iteration 111/1000 | Loss: 0.00000996
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000996
Iteration 115/1000 | Loss: 0.00000995
Iteration 116/1000 | Loss: 0.00000995
Iteration 117/1000 | Loss: 0.00000995
Iteration 118/1000 | Loss: 0.00000995
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000995
Iteration 124/1000 | Loss: 0.00000995
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000995
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000995
Iteration 137/1000 | Loss: 0.00000995
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000995
Iteration 143/1000 | Loss: 0.00000995
Iteration 144/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [9.945102647179738e-06, 9.945102647179738e-06, 9.945102647179738e-06, 9.945102647179738e-06, 9.945102647179738e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.945102647179738e-06

Optimization complete. Final v2v error: 2.6527328491210938 mm

Highest mean error: 7.934759140014648 mm for frame 211

Lowest mean error: 2.2777769565582275 mm for frame 151

Saving results

Total time: 53.98009204864502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737456
Iteration 2/25 | Loss: 0.00151408
Iteration 3/25 | Loss: 0.00112061
Iteration 4/25 | Loss: 0.00108027
Iteration 5/25 | Loss: 0.00105873
Iteration 6/25 | Loss: 0.00105486
Iteration 7/25 | Loss: 0.00105289
Iteration 8/25 | Loss: 0.00105216
Iteration 9/25 | Loss: 0.00105196
Iteration 10/25 | Loss: 0.00105190
Iteration 11/25 | Loss: 0.00105190
Iteration 12/25 | Loss: 0.00105190
Iteration 13/25 | Loss: 0.00105190
Iteration 14/25 | Loss: 0.00105190
Iteration 15/25 | Loss: 0.00105190
Iteration 16/25 | Loss: 0.00105190
Iteration 17/25 | Loss: 0.00105190
Iteration 18/25 | Loss: 0.00105190
Iteration 19/25 | Loss: 0.00105190
Iteration 20/25 | Loss: 0.00105190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010518956696614623, 0.0010518956696614623, 0.0010518956696614623, 0.0010518956696614623, 0.0010518956696614623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010518956696614623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50090313
Iteration 2/25 | Loss: 0.00101695
Iteration 3/25 | Loss: 0.00101691
Iteration 4/25 | Loss: 0.00101691
Iteration 5/25 | Loss: 0.00101690
Iteration 6/25 | Loss: 0.00101690
Iteration 7/25 | Loss: 0.00101690
Iteration 8/25 | Loss: 0.00101690
Iteration 9/25 | Loss: 0.00101690
Iteration 10/25 | Loss: 0.00101690
Iteration 11/25 | Loss: 0.00101690
Iteration 12/25 | Loss: 0.00101690
Iteration 13/25 | Loss: 0.00101690
Iteration 14/25 | Loss: 0.00101690
Iteration 15/25 | Loss: 0.00101690
Iteration 16/25 | Loss: 0.00101690
Iteration 17/25 | Loss: 0.00101690
Iteration 18/25 | Loss: 0.00101690
Iteration 19/25 | Loss: 0.00101690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010169033193960786, 0.0010169033193960786, 0.0010169033193960786, 0.0010169033193960786, 0.0010169033193960786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010169033193960786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101690
Iteration 2/1000 | Loss: 0.00009347
Iteration 3/1000 | Loss: 0.00005371
Iteration 4/1000 | Loss: 0.00003700
Iteration 5/1000 | Loss: 0.00003297
Iteration 6/1000 | Loss: 0.00003016
Iteration 7/1000 | Loss: 0.00004671
Iteration 8/1000 | Loss: 0.00003086
Iteration 9/1000 | Loss: 0.00002892
Iteration 10/1000 | Loss: 0.00002765
Iteration 11/1000 | Loss: 0.00003099
Iteration 12/1000 | Loss: 0.00002746
Iteration 13/1000 | Loss: 0.00002714
Iteration 14/1000 | Loss: 0.00002661
Iteration 15/1000 | Loss: 0.00002633
Iteration 16/1000 | Loss: 0.00002591
Iteration 17/1000 | Loss: 0.00002533
Iteration 18/1000 | Loss: 0.00002479
Iteration 19/1000 | Loss: 0.00002459
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002400
Iteration 22/1000 | Loss: 0.00002365
Iteration 23/1000 | Loss: 0.00002343
Iteration 24/1000 | Loss: 0.00002321
Iteration 25/1000 | Loss: 0.00002304
Iteration 26/1000 | Loss: 0.00002300
Iteration 27/1000 | Loss: 0.00002297
Iteration 28/1000 | Loss: 0.00002293
Iteration 29/1000 | Loss: 0.00002290
Iteration 30/1000 | Loss: 0.00002290
Iteration 31/1000 | Loss: 0.00002290
Iteration 32/1000 | Loss: 0.00002289
Iteration 33/1000 | Loss: 0.00002289
Iteration 34/1000 | Loss: 0.00002288
Iteration 35/1000 | Loss: 0.00002288
Iteration 36/1000 | Loss: 0.00002288
Iteration 37/1000 | Loss: 0.00002287
Iteration 38/1000 | Loss: 0.00002287
Iteration 39/1000 | Loss: 0.00002286
Iteration 40/1000 | Loss: 0.00002286
Iteration 41/1000 | Loss: 0.00002286
Iteration 42/1000 | Loss: 0.00002285
Iteration 43/1000 | Loss: 0.00002284
Iteration 44/1000 | Loss: 0.00002284
Iteration 45/1000 | Loss: 0.00002284
Iteration 46/1000 | Loss: 0.00002283
Iteration 47/1000 | Loss: 0.00002283
Iteration 48/1000 | Loss: 0.00002283
Iteration 49/1000 | Loss: 0.00002282
Iteration 50/1000 | Loss: 0.00002278
Iteration 51/1000 | Loss: 0.00002278
Iteration 52/1000 | Loss: 0.00002278
Iteration 53/1000 | Loss: 0.00002277
Iteration 54/1000 | Loss: 0.00002276
Iteration 55/1000 | Loss: 0.00002276
Iteration 56/1000 | Loss: 0.00002276
Iteration 57/1000 | Loss: 0.00002276
Iteration 58/1000 | Loss: 0.00002276
Iteration 59/1000 | Loss: 0.00002275
Iteration 60/1000 | Loss: 0.00002274
Iteration 61/1000 | Loss: 0.00002273
Iteration 62/1000 | Loss: 0.00002273
Iteration 63/1000 | Loss: 0.00002273
Iteration 64/1000 | Loss: 0.00002273
Iteration 65/1000 | Loss: 0.00002273
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00002272
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002270
Iteration 70/1000 | Loss: 0.00002269
Iteration 71/1000 | Loss: 0.00002269
Iteration 72/1000 | Loss: 0.00002269
Iteration 73/1000 | Loss: 0.00002269
Iteration 74/1000 | Loss: 0.00002268
Iteration 75/1000 | Loss: 0.00002268
Iteration 76/1000 | Loss: 0.00002268
Iteration 77/1000 | Loss: 0.00002267
Iteration 78/1000 | Loss: 0.00002267
Iteration 79/1000 | Loss: 0.00002267
Iteration 80/1000 | Loss: 0.00002266
Iteration 81/1000 | Loss: 0.00002266
Iteration 82/1000 | Loss: 0.00002266
Iteration 83/1000 | Loss: 0.00002265
Iteration 84/1000 | Loss: 0.00002265
Iteration 85/1000 | Loss: 0.00002265
Iteration 86/1000 | Loss: 0.00002264
Iteration 87/1000 | Loss: 0.00002264
Iteration 88/1000 | Loss: 0.00002264
Iteration 89/1000 | Loss: 0.00002264
Iteration 90/1000 | Loss: 0.00002264
Iteration 91/1000 | Loss: 0.00002263
Iteration 92/1000 | Loss: 0.00002263
Iteration 93/1000 | Loss: 0.00002263
Iteration 94/1000 | Loss: 0.00002263
Iteration 95/1000 | Loss: 0.00002263
Iteration 96/1000 | Loss: 0.00002263
Iteration 97/1000 | Loss: 0.00002262
Iteration 98/1000 | Loss: 0.00002262
Iteration 99/1000 | Loss: 0.00002262
Iteration 100/1000 | Loss: 0.00002262
Iteration 101/1000 | Loss: 0.00002262
Iteration 102/1000 | Loss: 0.00002262
Iteration 103/1000 | Loss: 0.00002261
Iteration 104/1000 | Loss: 0.00002261
Iteration 105/1000 | Loss: 0.00002261
Iteration 106/1000 | Loss: 0.00002261
Iteration 107/1000 | Loss: 0.00002261
Iteration 108/1000 | Loss: 0.00002261
Iteration 109/1000 | Loss: 0.00002261
Iteration 110/1000 | Loss: 0.00002261
Iteration 111/1000 | Loss: 0.00002261
Iteration 112/1000 | Loss: 0.00002261
Iteration 113/1000 | Loss: 0.00002261
Iteration 114/1000 | Loss: 0.00002260
Iteration 115/1000 | Loss: 0.00002260
Iteration 116/1000 | Loss: 0.00002260
Iteration 117/1000 | Loss: 0.00002260
Iteration 118/1000 | Loss: 0.00002260
Iteration 119/1000 | Loss: 0.00002260
Iteration 120/1000 | Loss: 0.00002260
Iteration 121/1000 | Loss: 0.00002260
Iteration 122/1000 | Loss: 0.00002260
Iteration 123/1000 | Loss: 0.00002260
Iteration 124/1000 | Loss: 0.00002260
Iteration 125/1000 | Loss: 0.00002260
Iteration 126/1000 | Loss: 0.00002260
Iteration 127/1000 | Loss: 0.00002260
Iteration 128/1000 | Loss: 0.00002260
Iteration 129/1000 | Loss: 0.00002260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.2600208467338234e-05, 2.2600208467338234e-05, 2.2600208467338234e-05, 2.2600208467338234e-05, 2.2600208467338234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2600208467338234e-05

Optimization complete. Final v2v error: 3.7356858253479004 mm

Highest mean error: 5.2145538330078125 mm for frame 110

Lowest mean error: 2.686250925064087 mm for frame 211

Saving results

Total time: 68.1032612323761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103347
Iteration 2/25 | Loss: 0.00163047
Iteration 3/25 | Loss: 0.00116847
Iteration 4/25 | Loss: 0.00113815
Iteration 5/25 | Loss: 0.00107059
Iteration 6/25 | Loss: 0.00105449
Iteration 7/25 | Loss: 0.00103338
Iteration 8/25 | Loss: 0.00102207
Iteration 9/25 | Loss: 0.00101389
Iteration 10/25 | Loss: 0.00101420
Iteration 11/25 | Loss: 0.00100446
Iteration 12/25 | Loss: 0.00100582
Iteration 13/25 | Loss: 0.00100126
Iteration 14/25 | Loss: 0.00099535
Iteration 15/25 | Loss: 0.00099439
Iteration 16/25 | Loss: 0.00099481
Iteration 17/25 | Loss: 0.00100337
Iteration 18/25 | Loss: 0.00100301
Iteration 19/25 | Loss: 0.00100204
Iteration 20/25 | Loss: 0.00100161
Iteration 21/25 | Loss: 0.00100238
Iteration 22/25 | Loss: 0.00100235
Iteration 23/25 | Loss: 0.00100356
Iteration 24/25 | Loss: 0.00100024
Iteration 25/25 | Loss: 0.00100147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.04767227
Iteration 2/25 | Loss: 0.00097220
Iteration 3/25 | Loss: 0.00097204
Iteration 4/25 | Loss: 0.00097204
Iteration 5/25 | Loss: 0.00097204
Iteration 6/25 | Loss: 0.00097204
Iteration 7/25 | Loss: 0.00097204
Iteration 8/25 | Loss: 0.00097204
Iteration 9/25 | Loss: 0.00097204
Iteration 10/25 | Loss: 0.00097204
Iteration 11/25 | Loss: 0.00097204
Iteration 12/25 | Loss: 0.00097204
Iteration 13/25 | Loss: 0.00097204
Iteration 14/25 | Loss: 0.00097204
Iteration 15/25 | Loss: 0.00097204
Iteration 16/25 | Loss: 0.00097204
Iteration 17/25 | Loss: 0.00097204
Iteration 18/25 | Loss: 0.00097204
Iteration 19/25 | Loss: 0.00097204
Iteration 20/25 | Loss: 0.00097204
Iteration 21/25 | Loss: 0.00097204
Iteration 22/25 | Loss: 0.00097204
Iteration 23/25 | Loss: 0.00097204
Iteration 24/25 | Loss: 0.00097204
Iteration 25/25 | Loss: 0.00097204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097204
Iteration 2/1000 | Loss: 0.00013211
Iteration 3/1000 | Loss: 0.00010503
Iteration 4/1000 | Loss: 0.00004973
Iteration 5/1000 | Loss: 0.00012524
Iteration 6/1000 | Loss: 0.00028583
Iteration 7/1000 | Loss: 0.00026531
Iteration 8/1000 | Loss: 0.00026513
Iteration 9/1000 | Loss: 0.00016406
Iteration 10/1000 | Loss: 0.00016439
Iteration 11/1000 | Loss: 0.00018995
Iteration 12/1000 | Loss: 0.00015417
Iteration 13/1000 | Loss: 0.00026385
Iteration 14/1000 | Loss: 0.00011945
Iteration 15/1000 | Loss: 0.00010684
Iteration 16/1000 | Loss: 0.00004471
Iteration 17/1000 | Loss: 0.00014842
Iteration 18/1000 | Loss: 0.00006734
Iteration 19/1000 | Loss: 0.00011603
Iteration 20/1000 | Loss: 0.00011180
Iteration 21/1000 | Loss: 0.00006010
Iteration 22/1000 | Loss: 0.00012081
Iteration 23/1000 | Loss: 0.00009529
Iteration 24/1000 | Loss: 0.00055187
Iteration 25/1000 | Loss: 0.00007738
Iteration 26/1000 | Loss: 0.00012634
Iteration 27/1000 | Loss: 0.00011871
Iteration 28/1000 | Loss: 0.00015692
Iteration 29/1000 | Loss: 0.00013894
Iteration 30/1000 | Loss: 0.00004740
Iteration 31/1000 | Loss: 0.00018054
Iteration 32/1000 | Loss: 0.00017606
Iteration 33/1000 | Loss: 0.00005702
Iteration 34/1000 | Loss: 0.00009931
Iteration 35/1000 | Loss: 0.00003196
Iteration 36/1000 | Loss: 0.00006969
Iteration 37/1000 | Loss: 0.00007971
Iteration 38/1000 | Loss: 0.00007165
Iteration 39/1000 | Loss: 0.00006841
Iteration 40/1000 | Loss: 0.00007140
Iteration 41/1000 | Loss: 0.00006853
Iteration 42/1000 | Loss: 0.00003055
Iteration 43/1000 | Loss: 0.00006394
Iteration 44/1000 | Loss: 0.00018855
Iteration 45/1000 | Loss: 0.00008038
Iteration 46/1000 | Loss: 0.00004595
Iteration 47/1000 | Loss: 0.00012878
Iteration 48/1000 | Loss: 0.00006627
Iteration 49/1000 | Loss: 0.00004655
Iteration 50/1000 | Loss: 0.00017246
Iteration 51/1000 | Loss: 0.00013645
Iteration 52/1000 | Loss: 0.00011535
Iteration 53/1000 | Loss: 0.00011884
Iteration 54/1000 | Loss: 0.00015408
Iteration 55/1000 | Loss: 0.00009590
Iteration 56/1000 | Loss: 0.00005686
Iteration 57/1000 | Loss: 0.00003558
Iteration 58/1000 | Loss: 0.00003111
Iteration 59/1000 | Loss: 0.00024817
Iteration 60/1000 | Loss: 0.00003296
Iteration 61/1000 | Loss: 0.00002982
Iteration 62/1000 | Loss: 0.00002900
Iteration 63/1000 | Loss: 0.00002841
Iteration 64/1000 | Loss: 0.00018353
Iteration 65/1000 | Loss: 0.00010944
Iteration 66/1000 | Loss: 0.00002940
Iteration 67/1000 | Loss: 0.00002773
Iteration 68/1000 | Loss: 0.00002772
Iteration 69/1000 | Loss: 0.00002765
Iteration 70/1000 | Loss: 0.00032829
Iteration 71/1000 | Loss: 0.00021931
Iteration 72/1000 | Loss: 0.00006210
Iteration 73/1000 | Loss: 0.00017376
Iteration 74/1000 | Loss: 0.00016504
Iteration 75/1000 | Loss: 0.00028281
Iteration 76/1000 | Loss: 0.00002840
Iteration 77/1000 | Loss: 0.00002755
Iteration 78/1000 | Loss: 0.00033667
Iteration 79/1000 | Loss: 0.00020588
Iteration 80/1000 | Loss: 0.00034324
Iteration 81/1000 | Loss: 0.00020311
Iteration 82/1000 | Loss: 0.00039026
Iteration 83/1000 | Loss: 0.00025695
Iteration 84/1000 | Loss: 0.00043785
Iteration 85/1000 | Loss: 0.00028056
Iteration 86/1000 | Loss: 0.00031511
Iteration 87/1000 | Loss: 0.00035166
Iteration 88/1000 | Loss: 0.00013144
Iteration 89/1000 | Loss: 0.00005634
Iteration 90/1000 | Loss: 0.00012890
Iteration 91/1000 | Loss: 0.00018261
Iteration 92/1000 | Loss: 0.00016983
Iteration 93/1000 | Loss: 0.00018844
Iteration 94/1000 | Loss: 0.00011001
Iteration 95/1000 | Loss: 0.00020401
Iteration 96/1000 | Loss: 0.00013535
Iteration 97/1000 | Loss: 0.00006819
Iteration 98/1000 | Loss: 0.00006502
Iteration 99/1000 | Loss: 0.00012863
Iteration 100/1000 | Loss: 0.00011615
Iteration 101/1000 | Loss: 0.00012198
Iteration 102/1000 | Loss: 0.00018891
Iteration 103/1000 | Loss: 0.00015155
Iteration 104/1000 | Loss: 0.00013136
Iteration 105/1000 | Loss: 0.00014009
Iteration 106/1000 | Loss: 0.00035192
Iteration 107/1000 | Loss: 0.00012997
Iteration 108/1000 | Loss: 0.00028696
Iteration 109/1000 | Loss: 0.00014906
Iteration 110/1000 | Loss: 0.00006921
Iteration 111/1000 | Loss: 0.00006940
Iteration 112/1000 | Loss: 0.00010928
Iteration 113/1000 | Loss: 0.00015953
Iteration 114/1000 | Loss: 0.00003633
Iteration 115/1000 | Loss: 0.00003433
Iteration 116/1000 | Loss: 0.00016966
Iteration 117/1000 | Loss: 0.00009632
Iteration 118/1000 | Loss: 0.00009402
Iteration 119/1000 | Loss: 0.00004178
Iteration 120/1000 | Loss: 0.00061939
Iteration 121/1000 | Loss: 0.00027977
Iteration 122/1000 | Loss: 0.00004674
Iteration 123/1000 | Loss: 0.00023605
Iteration 124/1000 | Loss: 0.00004959
Iteration 125/1000 | Loss: 0.00003667
Iteration 126/1000 | Loss: 0.00014107
Iteration 127/1000 | Loss: 0.00003490
Iteration 128/1000 | Loss: 0.00003180
Iteration 129/1000 | Loss: 0.00005056
Iteration 130/1000 | Loss: 0.00003171
Iteration 131/1000 | Loss: 0.00002856
Iteration 132/1000 | Loss: 0.00002785
Iteration 133/1000 | Loss: 0.00027175
Iteration 134/1000 | Loss: 0.00031452
Iteration 135/1000 | Loss: 0.00013084
Iteration 136/1000 | Loss: 0.00003154
Iteration 137/1000 | Loss: 0.00010357
Iteration 138/1000 | Loss: 0.00018952
Iteration 139/1000 | Loss: 0.00003538
Iteration 140/1000 | Loss: 0.00002953
Iteration 141/1000 | Loss: 0.00002769
Iteration 142/1000 | Loss: 0.00002672
Iteration 143/1000 | Loss: 0.00002621
Iteration 144/1000 | Loss: 0.00002584
Iteration 145/1000 | Loss: 0.00002557
Iteration 146/1000 | Loss: 0.00002532
Iteration 147/1000 | Loss: 0.00002514
Iteration 148/1000 | Loss: 0.00002514
Iteration 149/1000 | Loss: 0.00002514
Iteration 150/1000 | Loss: 0.00002514
Iteration 151/1000 | Loss: 0.00002513
Iteration 152/1000 | Loss: 0.00002513
Iteration 153/1000 | Loss: 0.00002513
Iteration 154/1000 | Loss: 0.00002513
Iteration 155/1000 | Loss: 0.00002513
Iteration 156/1000 | Loss: 0.00002513
Iteration 157/1000 | Loss: 0.00002513
Iteration 158/1000 | Loss: 0.00002513
Iteration 159/1000 | Loss: 0.00002513
Iteration 160/1000 | Loss: 0.00002513
Iteration 161/1000 | Loss: 0.00002512
Iteration 162/1000 | Loss: 0.00002512
Iteration 163/1000 | Loss: 0.00002511
Iteration 164/1000 | Loss: 0.00002511
Iteration 165/1000 | Loss: 0.00002510
Iteration 166/1000 | Loss: 0.00002510
Iteration 167/1000 | Loss: 0.00002509
Iteration 168/1000 | Loss: 0.00002509
Iteration 169/1000 | Loss: 0.00002509
Iteration 170/1000 | Loss: 0.00002508
Iteration 171/1000 | Loss: 0.00002508
Iteration 172/1000 | Loss: 0.00002508
Iteration 173/1000 | Loss: 0.00002507
Iteration 174/1000 | Loss: 0.00002507
Iteration 175/1000 | Loss: 0.00002507
Iteration 176/1000 | Loss: 0.00002507
Iteration 177/1000 | Loss: 0.00002507
Iteration 178/1000 | Loss: 0.00002506
Iteration 179/1000 | Loss: 0.00002506
Iteration 180/1000 | Loss: 0.00002506
Iteration 181/1000 | Loss: 0.00002506
Iteration 182/1000 | Loss: 0.00002505
Iteration 183/1000 | Loss: 0.00002505
Iteration 184/1000 | Loss: 0.00002505
Iteration 185/1000 | Loss: 0.00002505
Iteration 186/1000 | Loss: 0.00002505
Iteration 187/1000 | Loss: 0.00002504
Iteration 188/1000 | Loss: 0.00002504
Iteration 189/1000 | Loss: 0.00002504
Iteration 190/1000 | Loss: 0.00002504
Iteration 191/1000 | Loss: 0.00002503
Iteration 192/1000 | Loss: 0.00002503
Iteration 193/1000 | Loss: 0.00002502
Iteration 194/1000 | Loss: 0.00002502
Iteration 195/1000 | Loss: 0.00002502
Iteration 196/1000 | Loss: 0.00002500
Iteration 197/1000 | Loss: 0.00002500
Iteration 198/1000 | Loss: 0.00002500
Iteration 199/1000 | Loss: 0.00002499
Iteration 200/1000 | Loss: 0.00002498
Iteration 201/1000 | Loss: 0.00002497
Iteration 202/1000 | Loss: 0.00002497
Iteration 203/1000 | Loss: 0.00002497
Iteration 204/1000 | Loss: 0.00002496
Iteration 205/1000 | Loss: 0.00002496
Iteration 206/1000 | Loss: 0.00002495
Iteration 207/1000 | Loss: 0.00002495
Iteration 208/1000 | Loss: 0.00002495
Iteration 209/1000 | Loss: 0.00002494
Iteration 210/1000 | Loss: 0.00002494
Iteration 211/1000 | Loss: 0.00002494
Iteration 212/1000 | Loss: 0.00002494
Iteration 213/1000 | Loss: 0.00002494
Iteration 214/1000 | Loss: 0.00002494
Iteration 215/1000 | Loss: 0.00002493
Iteration 216/1000 | Loss: 0.00002493
Iteration 217/1000 | Loss: 0.00002493
Iteration 218/1000 | Loss: 0.00002493
Iteration 219/1000 | Loss: 0.00002493
Iteration 220/1000 | Loss: 0.00002493
Iteration 221/1000 | Loss: 0.00002492
Iteration 222/1000 | Loss: 0.00002492
Iteration 223/1000 | Loss: 0.00002492
Iteration 224/1000 | Loss: 0.00002492
Iteration 225/1000 | Loss: 0.00002492
Iteration 226/1000 | Loss: 0.00002492
Iteration 227/1000 | Loss: 0.00002492
Iteration 228/1000 | Loss: 0.00002492
Iteration 229/1000 | Loss: 0.00002492
Iteration 230/1000 | Loss: 0.00002492
Iteration 231/1000 | Loss: 0.00002492
Iteration 232/1000 | Loss: 0.00002492
Iteration 233/1000 | Loss: 0.00002492
Iteration 234/1000 | Loss: 0.00002492
Iteration 235/1000 | Loss: 0.00002491
Iteration 236/1000 | Loss: 0.00002491
Iteration 237/1000 | Loss: 0.00002491
Iteration 238/1000 | Loss: 0.00002491
Iteration 239/1000 | Loss: 0.00002491
Iteration 240/1000 | Loss: 0.00002491
Iteration 241/1000 | Loss: 0.00002491
Iteration 242/1000 | Loss: 0.00002491
Iteration 243/1000 | Loss: 0.00002491
Iteration 244/1000 | Loss: 0.00002491
Iteration 245/1000 | Loss: 0.00002491
Iteration 246/1000 | Loss: 0.00002491
Iteration 247/1000 | Loss: 0.00002491
Iteration 248/1000 | Loss: 0.00002491
Iteration 249/1000 | Loss: 0.00002491
Iteration 250/1000 | Loss: 0.00002491
Iteration 251/1000 | Loss: 0.00002491
Iteration 252/1000 | Loss: 0.00002491
Iteration 253/1000 | Loss: 0.00002491
Iteration 254/1000 | Loss: 0.00002491
Iteration 255/1000 | Loss: 0.00002491
Iteration 256/1000 | Loss: 0.00002491
Iteration 257/1000 | Loss: 0.00002491
Iteration 258/1000 | Loss: 0.00002491
Iteration 259/1000 | Loss: 0.00002491
Iteration 260/1000 | Loss: 0.00002491
Iteration 261/1000 | Loss: 0.00002491
Iteration 262/1000 | Loss: 0.00002491
Iteration 263/1000 | Loss: 0.00002491
Iteration 264/1000 | Loss: 0.00002491
Iteration 265/1000 | Loss: 0.00002491
Iteration 266/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [2.490677979949396e-05, 2.490677979949396e-05, 2.490677979949396e-05, 2.490677979949396e-05, 2.490677979949396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.490677979949396e-05

Optimization complete. Final v2v error: 3.9292232990264893 mm

Highest mean error: 8.925390243530273 mm for frame 84

Lowest mean error: 2.553809642791748 mm for frame 163

Saving results

Total time: 256.51409816741943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00620550
Iteration 2/25 | Loss: 0.00115788
Iteration 3/25 | Loss: 0.00098903
Iteration 4/25 | Loss: 0.00097485
Iteration 5/25 | Loss: 0.00097168
Iteration 6/25 | Loss: 0.00097099
Iteration 7/25 | Loss: 0.00097099
Iteration 8/25 | Loss: 0.00097099
Iteration 9/25 | Loss: 0.00097099
Iteration 10/25 | Loss: 0.00097099
Iteration 11/25 | Loss: 0.00097099
Iteration 12/25 | Loss: 0.00097099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009709924925118685, 0.0009709924925118685, 0.0009709924925118685, 0.0009709924925118685, 0.0009709924925118685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009709924925118685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39403653
Iteration 2/25 | Loss: 0.00043095
Iteration 3/25 | Loss: 0.00043093
Iteration 4/25 | Loss: 0.00043093
Iteration 5/25 | Loss: 0.00043093
Iteration 6/25 | Loss: 0.00043093
Iteration 7/25 | Loss: 0.00043093
Iteration 8/25 | Loss: 0.00043093
Iteration 9/25 | Loss: 0.00043093
Iteration 10/25 | Loss: 0.00043093
Iteration 11/25 | Loss: 0.00043093
Iteration 12/25 | Loss: 0.00043093
Iteration 13/25 | Loss: 0.00043093
Iteration 14/25 | Loss: 0.00043093
Iteration 15/25 | Loss: 0.00043093
Iteration 16/25 | Loss: 0.00043093
Iteration 17/25 | Loss: 0.00043093
Iteration 18/25 | Loss: 0.00043093
Iteration 19/25 | Loss: 0.00043093
Iteration 20/25 | Loss: 0.00043093
Iteration 21/25 | Loss: 0.00043093
Iteration 22/25 | Loss: 0.00043093
Iteration 23/25 | Loss: 0.00043093
Iteration 24/25 | Loss: 0.00043093
Iteration 25/25 | Loss: 0.00043093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043093
Iteration 2/1000 | Loss: 0.00002224
Iteration 3/1000 | Loss: 0.00001496
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001285
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001198
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001167
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001166
Iteration 16/1000 | Loss: 0.00001164
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001164
Iteration 19/1000 | Loss: 0.00001164
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001164
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001163
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001159
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001156
Iteration 42/1000 | Loss: 0.00001156
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001155
Iteration 45/1000 | Loss: 0.00001155
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001155
Iteration 48/1000 | Loss: 0.00001155
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001154
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001153
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001153
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001152
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001151
Iteration 84/1000 | Loss: 0.00001151
Iteration 85/1000 | Loss: 0.00001151
Iteration 86/1000 | Loss: 0.00001151
Iteration 87/1000 | Loss: 0.00001151
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001150
Iteration 91/1000 | Loss: 0.00001150
Iteration 92/1000 | Loss: 0.00001150
Iteration 93/1000 | Loss: 0.00001150
Iteration 94/1000 | Loss: 0.00001150
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001150
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001150
Iteration 104/1000 | Loss: 0.00001150
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001149
Iteration 115/1000 | Loss: 0.00001149
Iteration 116/1000 | Loss: 0.00001149
Iteration 117/1000 | Loss: 0.00001149
Iteration 118/1000 | Loss: 0.00001149
Iteration 119/1000 | Loss: 0.00001149
Iteration 120/1000 | Loss: 0.00001149
Iteration 121/1000 | Loss: 0.00001149
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001148
Iteration 126/1000 | Loss: 0.00001148
Iteration 127/1000 | Loss: 0.00001148
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001148
Iteration 137/1000 | Loss: 0.00001148
Iteration 138/1000 | Loss: 0.00001148
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001147
Iteration 148/1000 | Loss: 0.00001147
Iteration 149/1000 | Loss: 0.00001147
Iteration 150/1000 | Loss: 0.00001147
Iteration 151/1000 | Loss: 0.00001146
Iteration 152/1000 | Loss: 0.00001146
Iteration 153/1000 | Loss: 0.00001146
Iteration 154/1000 | Loss: 0.00001146
Iteration 155/1000 | Loss: 0.00001146
Iteration 156/1000 | Loss: 0.00001146
Iteration 157/1000 | Loss: 0.00001146
Iteration 158/1000 | Loss: 0.00001146
Iteration 159/1000 | Loss: 0.00001145
Iteration 160/1000 | Loss: 0.00001145
Iteration 161/1000 | Loss: 0.00001145
Iteration 162/1000 | Loss: 0.00001145
Iteration 163/1000 | Loss: 0.00001145
Iteration 164/1000 | Loss: 0.00001145
Iteration 165/1000 | Loss: 0.00001145
Iteration 166/1000 | Loss: 0.00001145
Iteration 167/1000 | Loss: 0.00001145
Iteration 168/1000 | Loss: 0.00001145
Iteration 169/1000 | Loss: 0.00001145
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001145
Iteration 172/1000 | Loss: 0.00001145
Iteration 173/1000 | Loss: 0.00001145
Iteration 174/1000 | Loss: 0.00001145
Iteration 175/1000 | Loss: 0.00001145
Iteration 176/1000 | Loss: 0.00001145
Iteration 177/1000 | Loss: 0.00001145
Iteration 178/1000 | Loss: 0.00001145
Iteration 179/1000 | Loss: 0.00001145
Iteration 180/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.1447452379798051e-05, 1.1447452379798051e-05, 1.1447452379798051e-05, 1.1447452379798051e-05, 1.1447452379798051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1447452379798051e-05

Optimization complete. Final v2v error: 2.797997236251831 mm

Highest mean error: 3.6723392009735107 mm for frame 69

Lowest mean error: 2.341700315475464 mm for frame 30

Saving results

Total time: 38.473328828811646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486206
Iteration 2/25 | Loss: 0.00123740
Iteration 3/25 | Loss: 0.00102933
Iteration 4/25 | Loss: 0.00101190
Iteration 5/25 | Loss: 0.00100844
Iteration 6/25 | Loss: 0.00100754
Iteration 7/25 | Loss: 0.00100754
Iteration 8/25 | Loss: 0.00100754
Iteration 9/25 | Loss: 0.00100754
Iteration 10/25 | Loss: 0.00100754
Iteration 11/25 | Loss: 0.00100754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001007544924505055, 0.001007544924505055, 0.001007544924505055, 0.001007544924505055, 0.001007544924505055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001007544924505055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32242620
Iteration 2/25 | Loss: 0.00063329
Iteration 3/25 | Loss: 0.00063327
Iteration 4/25 | Loss: 0.00063327
Iteration 5/25 | Loss: 0.00063327
Iteration 6/25 | Loss: 0.00063327
Iteration 7/25 | Loss: 0.00063327
Iteration 8/25 | Loss: 0.00063327
Iteration 9/25 | Loss: 0.00063327
Iteration 10/25 | Loss: 0.00063327
Iteration 11/25 | Loss: 0.00063327
Iteration 12/25 | Loss: 0.00063327
Iteration 13/25 | Loss: 0.00063327
Iteration 14/25 | Loss: 0.00063327
Iteration 15/25 | Loss: 0.00063327
Iteration 16/25 | Loss: 0.00063327
Iteration 17/25 | Loss: 0.00063327
Iteration 18/25 | Loss: 0.00063327
Iteration 19/25 | Loss: 0.00063327
Iteration 20/25 | Loss: 0.00063327
Iteration 21/25 | Loss: 0.00063327
Iteration 22/25 | Loss: 0.00063327
Iteration 23/25 | Loss: 0.00063327
Iteration 24/25 | Loss: 0.00063327
Iteration 25/25 | Loss: 0.00063327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063327
Iteration 2/1000 | Loss: 0.00002945
Iteration 3/1000 | Loss: 0.00001823
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001405
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001340
Iteration 11/1000 | Loss: 0.00001334
Iteration 12/1000 | Loss: 0.00001320
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001313
Iteration 15/1000 | Loss: 0.00001311
Iteration 16/1000 | Loss: 0.00001306
Iteration 17/1000 | Loss: 0.00001305
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001300
Iteration 25/1000 | Loss: 0.00001299
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001299
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001297
Iteration 34/1000 | Loss: 0.00001297
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001295
Iteration 39/1000 | Loss: 0.00001295
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001283
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001283
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001282
Iteration 97/1000 | Loss: 0.00001282
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001281
Iteration 106/1000 | Loss: 0.00001281
Iteration 107/1000 | Loss: 0.00001280
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001277
Iteration 113/1000 | Loss: 0.00001277
Iteration 114/1000 | Loss: 0.00001277
Iteration 115/1000 | Loss: 0.00001276
Iteration 116/1000 | Loss: 0.00001276
Iteration 117/1000 | Loss: 0.00001276
Iteration 118/1000 | Loss: 0.00001276
Iteration 119/1000 | Loss: 0.00001276
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001275
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001275
Iteration 125/1000 | Loss: 0.00001275
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001273
Iteration 130/1000 | Loss: 0.00001273
Iteration 131/1000 | Loss: 0.00001273
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001273
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001271
Iteration 139/1000 | Loss: 0.00001271
Iteration 140/1000 | Loss: 0.00001271
Iteration 141/1000 | Loss: 0.00001271
Iteration 142/1000 | Loss: 0.00001271
Iteration 143/1000 | Loss: 0.00001271
Iteration 144/1000 | Loss: 0.00001270
Iteration 145/1000 | Loss: 0.00001270
Iteration 146/1000 | Loss: 0.00001270
Iteration 147/1000 | Loss: 0.00001270
Iteration 148/1000 | Loss: 0.00001270
Iteration 149/1000 | Loss: 0.00001270
Iteration 150/1000 | Loss: 0.00001269
Iteration 151/1000 | Loss: 0.00001269
Iteration 152/1000 | Loss: 0.00001269
Iteration 153/1000 | Loss: 0.00001269
Iteration 154/1000 | Loss: 0.00001269
Iteration 155/1000 | Loss: 0.00001269
Iteration 156/1000 | Loss: 0.00001269
Iteration 157/1000 | Loss: 0.00001269
Iteration 158/1000 | Loss: 0.00001268
Iteration 159/1000 | Loss: 0.00001268
Iteration 160/1000 | Loss: 0.00001268
Iteration 161/1000 | Loss: 0.00001268
Iteration 162/1000 | Loss: 0.00001268
Iteration 163/1000 | Loss: 0.00001268
Iteration 164/1000 | Loss: 0.00001268
Iteration 165/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2683995009865612e-05, 1.2683995009865612e-05, 1.2683995009865612e-05, 1.2683995009865612e-05, 1.2683995009865612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2683995009865612e-05

Optimization complete. Final v2v error: 2.992847442626953 mm

Highest mean error: 3.3940908908843994 mm for frame 91

Lowest mean error: 2.609379529953003 mm for frame 229

Saving results

Total time: 42.25490355491638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847118
Iteration 2/25 | Loss: 0.00122420
Iteration 3/25 | Loss: 0.00102120
Iteration 4/25 | Loss: 0.00099795
Iteration 5/25 | Loss: 0.00099356
Iteration 6/25 | Loss: 0.00099275
Iteration 7/25 | Loss: 0.00099275
Iteration 8/25 | Loss: 0.00099275
Iteration 9/25 | Loss: 0.00099275
Iteration 10/25 | Loss: 0.00099275
Iteration 11/25 | Loss: 0.00099275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009927480714395642, 0.0009927480714395642, 0.0009927480714395642, 0.0009927480714395642, 0.0009927480714395642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009927480714395642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27288723
Iteration 2/25 | Loss: 0.00060964
Iteration 3/25 | Loss: 0.00060960
Iteration 4/25 | Loss: 0.00060960
Iteration 5/25 | Loss: 0.00060960
Iteration 6/25 | Loss: 0.00060960
Iteration 7/25 | Loss: 0.00060960
Iteration 8/25 | Loss: 0.00060960
Iteration 9/25 | Loss: 0.00060960
Iteration 10/25 | Loss: 0.00060960
Iteration 11/25 | Loss: 0.00060960
Iteration 12/25 | Loss: 0.00060960
Iteration 13/25 | Loss: 0.00060960
Iteration 14/25 | Loss: 0.00060960
Iteration 15/25 | Loss: 0.00060960
Iteration 16/25 | Loss: 0.00060960
Iteration 17/25 | Loss: 0.00060960
Iteration 18/25 | Loss: 0.00060960
Iteration 19/25 | Loss: 0.00060960
Iteration 20/25 | Loss: 0.00060960
Iteration 21/25 | Loss: 0.00060960
Iteration 22/25 | Loss: 0.00060960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006095987046137452, 0.0006095987046137452, 0.0006095987046137452, 0.0006095987046137452, 0.0006095987046137452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006095987046137452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060960
Iteration 2/1000 | Loss: 0.00003037
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001452
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001411
Iteration 15/1000 | Loss: 0.00001410
Iteration 16/1000 | Loss: 0.00001410
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001384
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001375
Iteration 34/1000 | Loss: 0.00001374
Iteration 35/1000 | Loss: 0.00001374
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001368
Iteration 50/1000 | Loss: 0.00001368
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001367
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001366
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001364
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001363
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001363
Iteration 78/1000 | Loss: 0.00001363
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001363
Iteration 83/1000 | Loss: 0.00001363
Iteration 84/1000 | Loss: 0.00001363
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001362
Iteration 91/1000 | Loss: 0.00001362
Iteration 92/1000 | Loss: 0.00001362
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001361
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001361
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001361
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001360
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001360
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001360
Iteration 112/1000 | Loss: 0.00001360
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001358
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001357
Iteration 130/1000 | Loss: 0.00001357
Iteration 131/1000 | Loss: 0.00001357
Iteration 132/1000 | Loss: 0.00001357
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001357
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001356
Iteration 141/1000 | Loss: 0.00001356
Iteration 142/1000 | Loss: 0.00001356
Iteration 143/1000 | Loss: 0.00001356
Iteration 144/1000 | Loss: 0.00001356
Iteration 145/1000 | Loss: 0.00001356
Iteration 146/1000 | Loss: 0.00001356
Iteration 147/1000 | Loss: 0.00001356
Iteration 148/1000 | Loss: 0.00001356
Iteration 149/1000 | Loss: 0.00001356
Iteration 150/1000 | Loss: 0.00001355
Iteration 151/1000 | Loss: 0.00001355
Iteration 152/1000 | Loss: 0.00001355
Iteration 153/1000 | Loss: 0.00001355
Iteration 154/1000 | Loss: 0.00001355
Iteration 155/1000 | Loss: 0.00001355
Iteration 156/1000 | Loss: 0.00001355
Iteration 157/1000 | Loss: 0.00001355
Iteration 158/1000 | Loss: 0.00001355
Iteration 159/1000 | Loss: 0.00001355
Iteration 160/1000 | Loss: 0.00001355
Iteration 161/1000 | Loss: 0.00001355
Iteration 162/1000 | Loss: 0.00001355
Iteration 163/1000 | Loss: 0.00001355
Iteration 164/1000 | Loss: 0.00001354
Iteration 165/1000 | Loss: 0.00001354
Iteration 166/1000 | Loss: 0.00001354
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.3539508472604211e-05, 1.3539508472604211e-05, 1.3539508472604211e-05, 1.3539508472604211e-05, 1.3539508472604211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3539508472604211e-05

Optimization complete. Final v2v error: 2.992245674133301 mm

Highest mean error: 4.142642021179199 mm for frame 66

Lowest mean error: 2.3553519248962402 mm for frame 182

Saving results

Total time: 44.28866386413574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801276
Iteration 2/25 | Loss: 0.00132080
Iteration 3/25 | Loss: 0.00113533
Iteration 4/25 | Loss: 0.00108715
Iteration 5/25 | Loss: 0.00107663
Iteration 6/25 | Loss: 0.00107443
Iteration 7/25 | Loss: 0.00107352
Iteration 8/25 | Loss: 0.00107328
Iteration 9/25 | Loss: 0.00107315
Iteration 10/25 | Loss: 0.00107308
Iteration 11/25 | Loss: 0.00107300
Iteration 12/25 | Loss: 0.00107291
Iteration 13/25 | Loss: 0.00107282
Iteration 14/25 | Loss: 0.00107282
Iteration 15/25 | Loss: 0.00107282
Iteration 16/25 | Loss: 0.00107282
Iteration 17/25 | Loss: 0.00107282
Iteration 18/25 | Loss: 0.00107282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010728156194090843, 0.0010728156194090843, 0.0010728156194090843, 0.0010728156194090843, 0.0010728156194090843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010728156194090843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84084630
Iteration 2/25 | Loss: 0.00079209
Iteration 3/25 | Loss: 0.00079193
Iteration 4/25 | Loss: 0.00079193
Iteration 5/25 | Loss: 0.00079193
Iteration 6/25 | Loss: 0.00079193
Iteration 7/25 | Loss: 0.00079193
Iteration 8/25 | Loss: 0.00079193
Iteration 9/25 | Loss: 0.00079193
Iteration 10/25 | Loss: 0.00079193
Iteration 11/25 | Loss: 0.00079193
Iteration 12/25 | Loss: 0.00079193
Iteration 13/25 | Loss: 0.00079193
Iteration 14/25 | Loss: 0.00079193
Iteration 15/25 | Loss: 0.00079193
Iteration 16/25 | Loss: 0.00079193
Iteration 17/25 | Loss: 0.00079193
Iteration 18/25 | Loss: 0.00079192
Iteration 19/25 | Loss: 0.00079192
Iteration 20/25 | Loss: 0.00079193
Iteration 21/25 | Loss: 0.00079193
Iteration 22/25 | Loss: 0.00079193
Iteration 23/25 | Loss: 0.00079193
Iteration 24/25 | Loss: 0.00079193
Iteration 25/25 | Loss: 0.00079193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007919250638224185, 0.0007919250638224185, 0.0007919250638224185, 0.0007919250638224185, 0.0007919250638224185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007919250638224185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079193
Iteration 2/1000 | Loss: 0.00007422
Iteration 3/1000 | Loss: 0.00004514
Iteration 4/1000 | Loss: 0.00003628
Iteration 5/1000 | Loss: 0.00003288
Iteration 6/1000 | Loss: 0.00003103
Iteration 7/1000 | Loss: 0.00002974
Iteration 8/1000 | Loss: 0.00028790
Iteration 9/1000 | Loss: 0.00003529
Iteration 10/1000 | Loss: 0.00002853
Iteration 11/1000 | Loss: 0.00002734
Iteration 12/1000 | Loss: 0.00002646
Iteration 13/1000 | Loss: 0.00002569
Iteration 14/1000 | Loss: 0.00002519
Iteration 15/1000 | Loss: 0.00002488
Iteration 16/1000 | Loss: 0.00002467
Iteration 17/1000 | Loss: 0.00002442
Iteration 18/1000 | Loss: 0.00002422
Iteration 19/1000 | Loss: 0.00002420
Iteration 20/1000 | Loss: 0.00002415
Iteration 21/1000 | Loss: 0.00002415
Iteration 22/1000 | Loss: 0.00002414
Iteration 23/1000 | Loss: 0.00002414
Iteration 24/1000 | Loss: 0.00002413
Iteration 25/1000 | Loss: 0.00002413
Iteration 26/1000 | Loss: 0.00002412
Iteration 27/1000 | Loss: 0.00002412
Iteration 28/1000 | Loss: 0.00002411
Iteration 29/1000 | Loss: 0.00002410
Iteration 30/1000 | Loss: 0.00002409
Iteration 31/1000 | Loss: 0.00002409
Iteration 32/1000 | Loss: 0.00002408
Iteration 33/1000 | Loss: 0.00002406
Iteration 34/1000 | Loss: 0.00002405
Iteration 35/1000 | Loss: 0.00002396
Iteration 36/1000 | Loss: 0.00002393
Iteration 37/1000 | Loss: 0.00002390
Iteration 38/1000 | Loss: 0.00002389
Iteration 39/1000 | Loss: 0.00002386
Iteration 40/1000 | Loss: 0.00002386
Iteration 41/1000 | Loss: 0.00002386
Iteration 42/1000 | Loss: 0.00002386
Iteration 43/1000 | Loss: 0.00002386
Iteration 44/1000 | Loss: 0.00002386
Iteration 45/1000 | Loss: 0.00002386
Iteration 46/1000 | Loss: 0.00002386
Iteration 47/1000 | Loss: 0.00002386
Iteration 48/1000 | Loss: 0.00002386
Iteration 49/1000 | Loss: 0.00002386
Iteration 50/1000 | Loss: 0.00002385
Iteration 51/1000 | Loss: 0.00002385
Iteration 52/1000 | Loss: 0.00002385
Iteration 53/1000 | Loss: 0.00002385
Iteration 54/1000 | Loss: 0.00002385
Iteration 55/1000 | Loss: 0.00002385
Iteration 56/1000 | Loss: 0.00002384
Iteration 57/1000 | Loss: 0.00002384
Iteration 58/1000 | Loss: 0.00002384
Iteration 59/1000 | Loss: 0.00002384
Iteration 60/1000 | Loss: 0.00002384
Iteration 61/1000 | Loss: 0.00002384
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002384
Iteration 64/1000 | Loss: 0.00002384
Iteration 65/1000 | Loss: 0.00002383
Iteration 66/1000 | Loss: 0.00002383
Iteration 67/1000 | Loss: 0.00002382
Iteration 68/1000 | Loss: 0.00002382
Iteration 69/1000 | Loss: 0.00002382
Iteration 70/1000 | Loss: 0.00002381
Iteration 71/1000 | Loss: 0.00002381
Iteration 72/1000 | Loss: 0.00002380
Iteration 73/1000 | Loss: 0.00002380
Iteration 74/1000 | Loss: 0.00002380
Iteration 75/1000 | Loss: 0.00002379
Iteration 76/1000 | Loss: 0.00002379
Iteration 77/1000 | Loss: 0.00002379
Iteration 78/1000 | Loss: 0.00002378
Iteration 79/1000 | Loss: 0.00002378
Iteration 80/1000 | Loss: 0.00002378
Iteration 81/1000 | Loss: 0.00002377
Iteration 82/1000 | Loss: 0.00002377
Iteration 83/1000 | Loss: 0.00002377
Iteration 84/1000 | Loss: 0.00002377
Iteration 85/1000 | Loss: 0.00002377
Iteration 86/1000 | Loss: 0.00002377
Iteration 87/1000 | Loss: 0.00002377
Iteration 88/1000 | Loss: 0.00002377
Iteration 89/1000 | Loss: 0.00002377
Iteration 90/1000 | Loss: 0.00002376
Iteration 91/1000 | Loss: 0.00002376
Iteration 92/1000 | Loss: 0.00002376
Iteration 93/1000 | Loss: 0.00002376
Iteration 94/1000 | Loss: 0.00002376
Iteration 95/1000 | Loss: 0.00002376
Iteration 96/1000 | Loss: 0.00002376
Iteration 97/1000 | Loss: 0.00002376
Iteration 98/1000 | Loss: 0.00002376
Iteration 99/1000 | Loss: 0.00002376
Iteration 100/1000 | Loss: 0.00002375
Iteration 101/1000 | Loss: 0.00002375
Iteration 102/1000 | Loss: 0.00002375
Iteration 103/1000 | Loss: 0.00002375
Iteration 104/1000 | Loss: 0.00002375
Iteration 105/1000 | Loss: 0.00002375
Iteration 106/1000 | Loss: 0.00002375
Iteration 107/1000 | Loss: 0.00002375
Iteration 108/1000 | Loss: 0.00002375
Iteration 109/1000 | Loss: 0.00002375
Iteration 110/1000 | Loss: 0.00002375
Iteration 111/1000 | Loss: 0.00002375
Iteration 112/1000 | Loss: 0.00002374
Iteration 113/1000 | Loss: 0.00002374
Iteration 114/1000 | Loss: 0.00002374
Iteration 115/1000 | Loss: 0.00002374
Iteration 116/1000 | Loss: 0.00002374
Iteration 117/1000 | Loss: 0.00002373
Iteration 118/1000 | Loss: 0.00002373
Iteration 119/1000 | Loss: 0.00002373
Iteration 120/1000 | Loss: 0.00002373
Iteration 121/1000 | Loss: 0.00002372
Iteration 122/1000 | Loss: 0.00002372
Iteration 123/1000 | Loss: 0.00002372
Iteration 124/1000 | Loss: 0.00002372
Iteration 125/1000 | Loss: 0.00002372
Iteration 126/1000 | Loss: 0.00002371
Iteration 127/1000 | Loss: 0.00002371
Iteration 128/1000 | Loss: 0.00002371
Iteration 129/1000 | Loss: 0.00002371
Iteration 130/1000 | Loss: 0.00002371
Iteration 131/1000 | Loss: 0.00002371
Iteration 132/1000 | Loss: 0.00002371
Iteration 133/1000 | Loss: 0.00002371
Iteration 134/1000 | Loss: 0.00002371
Iteration 135/1000 | Loss: 0.00002371
Iteration 136/1000 | Loss: 0.00002371
Iteration 137/1000 | Loss: 0.00002371
Iteration 138/1000 | Loss: 0.00002371
Iteration 139/1000 | Loss: 0.00002370
Iteration 140/1000 | Loss: 0.00002370
Iteration 141/1000 | Loss: 0.00002370
Iteration 142/1000 | Loss: 0.00002370
Iteration 143/1000 | Loss: 0.00002370
Iteration 144/1000 | Loss: 0.00002370
Iteration 145/1000 | Loss: 0.00002370
Iteration 146/1000 | Loss: 0.00002370
Iteration 147/1000 | Loss: 0.00002370
Iteration 148/1000 | Loss: 0.00002369
Iteration 149/1000 | Loss: 0.00002369
Iteration 150/1000 | Loss: 0.00002369
Iteration 151/1000 | Loss: 0.00002369
Iteration 152/1000 | Loss: 0.00002369
Iteration 153/1000 | Loss: 0.00002369
Iteration 154/1000 | Loss: 0.00002369
Iteration 155/1000 | Loss: 0.00002369
Iteration 156/1000 | Loss: 0.00002369
Iteration 157/1000 | Loss: 0.00002369
Iteration 158/1000 | Loss: 0.00002368
Iteration 159/1000 | Loss: 0.00002368
Iteration 160/1000 | Loss: 0.00002368
Iteration 161/1000 | Loss: 0.00002368
Iteration 162/1000 | Loss: 0.00002368
Iteration 163/1000 | Loss: 0.00002368
Iteration 164/1000 | Loss: 0.00002367
Iteration 165/1000 | Loss: 0.00002367
Iteration 166/1000 | Loss: 0.00002367
Iteration 167/1000 | Loss: 0.00002367
Iteration 168/1000 | Loss: 0.00002367
Iteration 169/1000 | Loss: 0.00002367
Iteration 170/1000 | Loss: 0.00002367
Iteration 171/1000 | Loss: 0.00002367
Iteration 172/1000 | Loss: 0.00002366
Iteration 173/1000 | Loss: 0.00002366
Iteration 174/1000 | Loss: 0.00002366
Iteration 175/1000 | Loss: 0.00002366
Iteration 176/1000 | Loss: 0.00002366
Iteration 177/1000 | Loss: 0.00002366
Iteration 178/1000 | Loss: 0.00002366
Iteration 179/1000 | Loss: 0.00002366
Iteration 180/1000 | Loss: 0.00002366
Iteration 181/1000 | Loss: 0.00002366
Iteration 182/1000 | Loss: 0.00002366
Iteration 183/1000 | Loss: 0.00002366
Iteration 184/1000 | Loss: 0.00002366
Iteration 185/1000 | Loss: 0.00002366
Iteration 186/1000 | Loss: 0.00002366
Iteration 187/1000 | Loss: 0.00002366
Iteration 188/1000 | Loss: 0.00002366
Iteration 189/1000 | Loss: 0.00002366
Iteration 190/1000 | Loss: 0.00002366
Iteration 191/1000 | Loss: 0.00002366
Iteration 192/1000 | Loss: 0.00002366
Iteration 193/1000 | Loss: 0.00002366
Iteration 194/1000 | Loss: 0.00002366
Iteration 195/1000 | Loss: 0.00002366
Iteration 196/1000 | Loss: 0.00002366
Iteration 197/1000 | Loss: 0.00002366
Iteration 198/1000 | Loss: 0.00002366
Iteration 199/1000 | Loss: 0.00002366
Iteration 200/1000 | Loss: 0.00002366
Iteration 201/1000 | Loss: 0.00002366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.3658010832150467e-05, 2.3658010832150467e-05, 2.3658010832150467e-05, 2.3658010832150467e-05, 2.3658010832150467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3658010832150467e-05

Optimization complete. Final v2v error: 4.122442245483398 mm

Highest mean error: 6.144663333892822 mm for frame 106

Lowest mean error: 3.0487656593322754 mm for frame 232

Saving results

Total time: 68.57412981987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893332
Iteration 2/25 | Loss: 0.00112846
Iteration 3/25 | Loss: 0.00103563
Iteration 4/25 | Loss: 0.00100966
Iteration 5/25 | Loss: 0.00099869
Iteration 6/25 | Loss: 0.00099659
Iteration 7/25 | Loss: 0.00099659
Iteration 8/25 | Loss: 0.00099659
Iteration 9/25 | Loss: 0.00099659
Iteration 10/25 | Loss: 0.00099659
Iteration 11/25 | Loss: 0.00099659
Iteration 12/25 | Loss: 0.00099659
Iteration 13/25 | Loss: 0.00099659
Iteration 14/25 | Loss: 0.00099659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009965912904590368, 0.0009965912904590368, 0.0009965912904590368, 0.0009965912904590368, 0.0009965912904590368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009965912904590368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30682456
Iteration 2/25 | Loss: 0.00074979
Iteration 3/25 | Loss: 0.00074979
Iteration 4/25 | Loss: 0.00074979
Iteration 5/25 | Loss: 0.00074978
Iteration 6/25 | Loss: 0.00074978
Iteration 7/25 | Loss: 0.00074978
Iteration 8/25 | Loss: 0.00074978
Iteration 9/25 | Loss: 0.00074978
Iteration 10/25 | Loss: 0.00074978
Iteration 11/25 | Loss: 0.00074978
Iteration 12/25 | Loss: 0.00074978
Iteration 13/25 | Loss: 0.00074978
Iteration 14/25 | Loss: 0.00074978
Iteration 15/25 | Loss: 0.00074978
Iteration 16/25 | Loss: 0.00074978
Iteration 17/25 | Loss: 0.00074978
Iteration 18/25 | Loss: 0.00074978
Iteration 19/25 | Loss: 0.00074978
Iteration 20/25 | Loss: 0.00074978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007497836486436427, 0.0007497836486436427, 0.0007497836486436427, 0.0007497836486436427, 0.0007497836486436427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007497836486436427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074978
Iteration 2/1000 | Loss: 0.00002251
Iteration 3/1000 | Loss: 0.00001860
Iteration 4/1000 | Loss: 0.00001751
Iteration 5/1000 | Loss: 0.00001719
Iteration 6/1000 | Loss: 0.00001653
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001607
Iteration 9/1000 | Loss: 0.00001594
Iteration 10/1000 | Loss: 0.00001586
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001579
Iteration 16/1000 | Loss: 0.00001579
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001570
Iteration 23/1000 | Loss: 0.00001570
Iteration 24/1000 | Loss: 0.00001570
Iteration 25/1000 | Loss: 0.00001570
Iteration 26/1000 | Loss: 0.00001569
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001568
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001566
Iteration 33/1000 | Loss: 0.00001566
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001565
Iteration 37/1000 | Loss: 0.00001565
Iteration 38/1000 | Loss: 0.00001564
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001564
Iteration 41/1000 | Loss: 0.00001564
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001564
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001563
Iteration 56/1000 | Loss: 0.00001563
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001562
Iteration 63/1000 | Loss: 0.00001562
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001562
Iteration 67/1000 | Loss: 0.00001562
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001562
Iteration 73/1000 | Loss: 0.00001562
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001561
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001561
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001557
Iteration 128/1000 | Loss: 0.00001557
Iteration 129/1000 | Loss: 0.00001557
Iteration 130/1000 | Loss: 0.00001557
Iteration 131/1000 | Loss: 0.00001557
Iteration 132/1000 | Loss: 0.00001557
Iteration 133/1000 | Loss: 0.00001557
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.557136965857353e-05, 1.557136965857353e-05, 1.557136965857353e-05, 1.557136965857353e-05, 1.557136965857353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.557136965857353e-05

Optimization complete. Final v2v error: 3.3086137771606445 mm

Highest mean error: 3.360567569732666 mm for frame 18

Lowest mean error: 3.183203935623169 mm for frame 0

Saving results

Total time: 34.402864933013916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778291
Iteration 2/25 | Loss: 0.00162222
Iteration 3/25 | Loss: 0.00121101
Iteration 4/25 | Loss: 0.00114552
Iteration 5/25 | Loss: 0.00112234
Iteration 6/25 | Loss: 0.00111574
Iteration 7/25 | Loss: 0.00110939
Iteration 8/25 | Loss: 0.00110164
Iteration 9/25 | Loss: 0.00109771
Iteration 10/25 | Loss: 0.00110004
Iteration 11/25 | Loss: 0.00108993
Iteration 12/25 | Loss: 0.00108761
Iteration 13/25 | Loss: 0.00108701
Iteration 14/25 | Loss: 0.00108679
Iteration 15/25 | Loss: 0.00108660
Iteration 16/25 | Loss: 0.00108639
Iteration 17/25 | Loss: 0.00108565
Iteration 18/25 | Loss: 0.00108497
Iteration 19/25 | Loss: 0.00108478
Iteration 20/25 | Loss: 0.00108829
Iteration 21/25 | Loss: 0.00108829
Iteration 22/25 | Loss: 0.00108781
Iteration 23/25 | Loss: 0.00108810
Iteration 24/25 | Loss: 0.00108794
Iteration 25/25 | Loss: 0.00108720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44690859
Iteration 2/25 | Loss: 0.00127768
Iteration 3/25 | Loss: 0.00127768
Iteration 4/25 | Loss: 0.00127768
Iteration 5/25 | Loss: 0.00127768
Iteration 6/25 | Loss: 0.00127768
Iteration 7/25 | Loss: 0.00127768
Iteration 8/25 | Loss: 0.00127768
Iteration 9/25 | Loss: 0.00127768
Iteration 10/25 | Loss: 0.00127768
Iteration 11/25 | Loss: 0.00127768
Iteration 12/25 | Loss: 0.00127768
Iteration 13/25 | Loss: 0.00127768
Iteration 14/25 | Loss: 0.00127768
Iteration 15/25 | Loss: 0.00127768
Iteration 16/25 | Loss: 0.00127768
Iteration 17/25 | Loss: 0.00127768
Iteration 18/25 | Loss: 0.00127768
Iteration 19/25 | Loss: 0.00127768
Iteration 20/25 | Loss: 0.00127768
Iteration 21/25 | Loss: 0.00127768
Iteration 22/25 | Loss: 0.00127768
Iteration 23/25 | Loss: 0.00127768
Iteration 24/25 | Loss: 0.00127768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001277679344639182, 0.001277679344639182, 0.001277679344639182, 0.001277679344639182, 0.001277679344639182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001277679344639182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127768
Iteration 2/1000 | Loss: 0.00031886
Iteration 3/1000 | Loss: 0.00040401
Iteration 4/1000 | Loss: 0.00007976
Iteration 5/1000 | Loss: 0.00014432
Iteration 6/1000 | Loss: 0.00015506
Iteration 7/1000 | Loss: 0.00003800
Iteration 8/1000 | Loss: 0.00010029
Iteration 9/1000 | Loss: 0.00015781
Iteration 10/1000 | Loss: 0.00010258
Iteration 11/1000 | Loss: 0.00010640
Iteration 12/1000 | Loss: 0.00009449
Iteration 13/1000 | Loss: 0.00010965
Iteration 14/1000 | Loss: 0.00009508
Iteration 15/1000 | Loss: 0.00013005
Iteration 16/1000 | Loss: 0.00021679
Iteration 17/1000 | Loss: 0.00012546
Iteration 18/1000 | Loss: 0.00020612
Iteration 19/1000 | Loss: 0.00003487
Iteration 20/1000 | Loss: 0.00002829
Iteration 21/1000 | Loss: 0.00002572
Iteration 22/1000 | Loss: 0.00002485
Iteration 23/1000 | Loss: 0.00002407
Iteration 24/1000 | Loss: 0.00002361
Iteration 25/1000 | Loss: 0.00002333
Iteration 26/1000 | Loss: 0.00002298
Iteration 27/1000 | Loss: 0.00002270
Iteration 28/1000 | Loss: 0.00002245
Iteration 29/1000 | Loss: 0.00002221
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002181
Iteration 32/1000 | Loss: 0.00002178
Iteration 33/1000 | Loss: 0.00002177
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002173
Iteration 36/1000 | Loss: 0.00002173
Iteration 37/1000 | Loss: 0.00002173
Iteration 38/1000 | Loss: 0.00002172
Iteration 39/1000 | Loss: 0.00002170
Iteration 40/1000 | Loss: 0.00002167
Iteration 41/1000 | Loss: 0.00002167
Iteration 42/1000 | Loss: 0.00002167
Iteration 43/1000 | Loss: 0.00002166
Iteration 44/1000 | Loss: 0.00002166
Iteration 45/1000 | Loss: 0.00002165
Iteration 46/1000 | Loss: 0.00002165
Iteration 47/1000 | Loss: 0.00002164
Iteration 48/1000 | Loss: 0.00002164
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002163
Iteration 51/1000 | Loss: 0.00002162
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002161
Iteration 55/1000 | Loss: 0.00002158
Iteration 56/1000 | Loss: 0.00002158
Iteration 57/1000 | Loss: 0.00002156
Iteration 58/1000 | Loss: 0.00002156
Iteration 59/1000 | Loss: 0.00002156
Iteration 60/1000 | Loss: 0.00002155
Iteration 61/1000 | Loss: 0.00002155
Iteration 62/1000 | Loss: 0.00002155
Iteration 63/1000 | Loss: 0.00002154
Iteration 64/1000 | Loss: 0.00002154
Iteration 65/1000 | Loss: 0.00002154
Iteration 66/1000 | Loss: 0.00002153
Iteration 67/1000 | Loss: 0.00002153
Iteration 68/1000 | Loss: 0.00002152
Iteration 69/1000 | Loss: 0.00002152
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002151
Iteration 73/1000 | Loss: 0.00002150
Iteration 74/1000 | Loss: 0.00002150
Iteration 75/1000 | Loss: 0.00002149
Iteration 76/1000 | Loss: 0.00002149
Iteration 77/1000 | Loss: 0.00002149
Iteration 78/1000 | Loss: 0.00002148
Iteration 79/1000 | Loss: 0.00002148
Iteration 80/1000 | Loss: 0.00002148
Iteration 81/1000 | Loss: 0.00002148
Iteration 82/1000 | Loss: 0.00002148
Iteration 83/1000 | Loss: 0.00002148
Iteration 84/1000 | Loss: 0.00002148
Iteration 85/1000 | Loss: 0.00002148
Iteration 86/1000 | Loss: 0.00002147
Iteration 87/1000 | Loss: 0.00002147
Iteration 88/1000 | Loss: 0.00002147
Iteration 89/1000 | Loss: 0.00002147
Iteration 90/1000 | Loss: 0.00002147
Iteration 91/1000 | Loss: 0.00002147
Iteration 92/1000 | Loss: 0.00002147
Iteration 93/1000 | Loss: 0.00002146
Iteration 94/1000 | Loss: 0.00002146
Iteration 95/1000 | Loss: 0.00002146
Iteration 96/1000 | Loss: 0.00002146
Iteration 97/1000 | Loss: 0.00002146
Iteration 98/1000 | Loss: 0.00002146
Iteration 99/1000 | Loss: 0.00002145
Iteration 100/1000 | Loss: 0.00002145
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002145
Iteration 108/1000 | Loss: 0.00002144
Iteration 109/1000 | Loss: 0.00002144
Iteration 110/1000 | Loss: 0.00002144
Iteration 111/1000 | Loss: 0.00002144
Iteration 112/1000 | Loss: 0.00002144
Iteration 113/1000 | Loss: 0.00002144
Iteration 114/1000 | Loss: 0.00002144
Iteration 115/1000 | Loss: 0.00002144
Iteration 116/1000 | Loss: 0.00002144
Iteration 117/1000 | Loss: 0.00002144
Iteration 118/1000 | Loss: 0.00002144
Iteration 119/1000 | Loss: 0.00002144
Iteration 120/1000 | Loss: 0.00002144
Iteration 121/1000 | Loss: 0.00002144
Iteration 122/1000 | Loss: 0.00002143
Iteration 123/1000 | Loss: 0.00002143
Iteration 124/1000 | Loss: 0.00002143
Iteration 125/1000 | Loss: 0.00002143
Iteration 126/1000 | Loss: 0.00002143
Iteration 127/1000 | Loss: 0.00002143
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002143
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002143
Iteration 134/1000 | Loss: 0.00002143
Iteration 135/1000 | Loss: 0.00002143
Iteration 136/1000 | Loss: 0.00002142
Iteration 137/1000 | Loss: 0.00002142
Iteration 138/1000 | Loss: 0.00002142
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002142
Iteration 142/1000 | Loss: 0.00002142
Iteration 143/1000 | Loss: 0.00002142
Iteration 144/1000 | Loss: 0.00002142
Iteration 145/1000 | Loss: 0.00002142
Iteration 146/1000 | Loss: 0.00002142
Iteration 147/1000 | Loss: 0.00002142
Iteration 148/1000 | Loss: 0.00002142
Iteration 149/1000 | Loss: 0.00002142
Iteration 150/1000 | Loss: 0.00002142
Iteration 151/1000 | Loss: 0.00002142
Iteration 152/1000 | Loss: 0.00002142
Iteration 153/1000 | Loss: 0.00002142
Iteration 154/1000 | Loss: 0.00002142
Iteration 155/1000 | Loss: 0.00002142
Iteration 156/1000 | Loss: 0.00002142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.1418187316157855e-05, 2.1418187316157855e-05, 2.1418187316157855e-05, 2.1418187316157855e-05, 2.1418187316157855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1418187316157855e-05

Optimization complete. Final v2v error: 3.720080852508545 mm

Highest mean error: 6.97451639175415 mm for frame 91

Lowest mean error: 2.6573312282562256 mm for frame 226

Saving results

Total time: 107.96307492256165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416596
Iteration 2/25 | Loss: 0.00105781
Iteration 3/25 | Loss: 0.00094776
Iteration 4/25 | Loss: 0.00093421
Iteration 5/25 | Loss: 0.00093055
Iteration 6/25 | Loss: 0.00092932
Iteration 7/25 | Loss: 0.00092927
Iteration 8/25 | Loss: 0.00092927
Iteration 9/25 | Loss: 0.00092927
Iteration 10/25 | Loss: 0.00092927
Iteration 11/25 | Loss: 0.00092927
Iteration 12/25 | Loss: 0.00092927
Iteration 13/25 | Loss: 0.00092927
Iteration 14/25 | Loss: 0.00092927
Iteration 15/25 | Loss: 0.00092927
Iteration 16/25 | Loss: 0.00092927
Iteration 17/25 | Loss: 0.00092927
Iteration 18/25 | Loss: 0.00092927
Iteration 19/25 | Loss: 0.00092927
Iteration 20/25 | Loss: 0.00092927
Iteration 21/25 | Loss: 0.00092927
Iteration 22/25 | Loss: 0.00092927
Iteration 23/25 | Loss: 0.00092927
Iteration 24/25 | Loss: 0.00092927
Iteration 25/25 | Loss: 0.00092927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.71732807
Iteration 2/25 | Loss: 0.00058426
Iteration 3/25 | Loss: 0.00058424
Iteration 4/25 | Loss: 0.00058424
Iteration 5/25 | Loss: 0.00058424
Iteration 6/25 | Loss: 0.00058424
Iteration 7/25 | Loss: 0.00058424
Iteration 8/25 | Loss: 0.00058424
Iteration 9/25 | Loss: 0.00058424
Iteration 10/25 | Loss: 0.00058424
Iteration 11/25 | Loss: 0.00058424
Iteration 12/25 | Loss: 0.00058424
Iteration 13/25 | Loss: 0.00058424
Iteration 14/25 | Loss: 0.00058424
Iteration 15/25 | Loss: 0.00058424
Iteration 16/25 | Loss: 0.00058424
Iteration 17/25 | Loss: 0.00058424
Iteration 18/25 | Loss: 0.00058424
Iteration 19/25 | Loss: 0.00058424
Iteration 20/25 | Loss: 0.00058424
Iteration 21/25 | Loss: 0.00058424
Iteration 22/25 | Loss: 0.00058424
Iteration 23/25 | Loss: 0.00058424
Iteration 24/25 | Loss: 0.00058424
Iteration 25/25 | Loss: 0.00058424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005842401296831667, 0.0005842401296831667, 0.0005842401296831667, 0.0005842401296831667, 0.0005842401296831667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005842401296831667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058424
Iteration 2/1000 | Loss: 0.00002733
Iteration 3/1000 | Loss: 0.00001621
Iteration 4/1000 | Loss: 0.00001266
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001124
Iteration 7/1000 | Loss: 0.00001095
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001055
Iteration 11/1000 | Loss: 0.00001055
Iteration 12/1000 | Loss: 0.00001054
Iteration 13/1000 | Loss: 0.00001054
Iteration 14/1000 | Loss: 0.00001053
Iteration 15/1000 | Loss: 0.00001053
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001051
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001048
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001044
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001040
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001040
Iteration 49/1000 | Loss: 0.00001040
Iteration 50/1000 | Loss: 0.00001039
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001038
Iteration 54/1000 | Loss: 0.00001038
Iteration 55/1000 | Loss: 0.00001037
Iteration 56/1000 | Loss: 0.00001037
Iteration 57/1000 | Loss: 0.00001037
Iteration 58/1000 | Loss: 0.00001037
Iteration 59/1000 | Loss: 0.00001037
Iteration 60/1000 | Loss: 0.00001037
Iteration 61/1000 | Loss: 0.00001037
Iteration 62/1000 | Loss: 0.00001037
Iteration 63/1000 | Loss: 0.00001037
Iteration 64/1000 | Loss: 0.00001037
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001037
Iteration 71/1000 | Loss: 0.00001037
Iteration 72/1000 | Loss: 0.00001037
Iteration 73/1000 | Loss: 0.00001037
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001037
Iteration 78/1000 | Loss: 0.00001037
Iteration 79/1000 | Loss: 0.00001037
Iteration 80/1000 | Loss: 0.00001037
Iteration 81/1000 | Loss: 0.00001037
Iteration 82/1000 | Loss: 0.00001037
Iteration 83/1000 | Loss: 0.00001037
Iteration 84/1000 | Loss: 0.00001037
Iteration 85/1000 | Loss: 0.00001037
Iteration 86/1000 | Loss: 0.00001037
Iteration 87/1000 | Loss: 0.00001037
Iteration 88/1000 | Loss: 0.00001037
Iteration 89/1000 | Loss: 0.00001037
Iteration 90/1000 | Loss: 0.00001037
Iteration 91/1000 | Loss: 0.00001037
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.0366799870098475e-05, 1.0366799870098475e-05, 1.0366799870098475e-05, 1.0366799870098475e-05, 1.0366799870098475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0366799870098475e-05

Optimization complete. Final v2v error: 2.713245153427124 mm

Highest mean error: 3.242015838623047 mm for frame 85

Lowest mean error: 2.43034029006958 mm for frame 43

Saving results

Total time: 27.077658891677856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406422
Iteration 2/25 | Loss: 0.00117010
Iteration 3/25 | Loss: 0.00106511
Iteration 4/25 | Loss: 0.00103606
Iteration 5/25 | Loss: 0.00102316
Iteration 6/25 | Loss: 0.00101977
Iteration 7/25 | Loss: 0.00101867
Iteration 8/25 | Loss: 0.00101867
Iteration 9/25 | Loss: 0.00101867
Iteration 10/25 | Loss: 0.00101867
Iteration 11/25 | Loss: 0.00101867
Iteration 12/25 | Loss: 0.00101867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010186689905822277, 0.0010186689905822277, 0.0010186689905822277, 0.0010186689905822277, 0.0010186689905822277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010186689905822277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63170505
Iteration 2/25 | Loss: 0.00066761
Iteration 3/25 | Loss: 0.00066760
Iteration 4/25 | Loss: 0.00066760
Iteration 5/25 | Loss: 0.00066760
Iteration 6/25 | Loss: 0.00066760
Iteration 7/25 | Loss: 0.00066760
Iteration 8/25 | Loss: 0.00066760
Iteration 9/25 | Loss: 0.00066760
Iteration 10/25 | Loss: 0.00066760
Iteration 11/25 | Loss: 0.00066760
Iteration 12/25 | Loss: 0.00066760
Iteration 13/25 | Loss: 0.00066760
Iteration 14/25 | Loss: 0.00066760
Iteration 15/25 | Loss: 0.00066760
Iteration 16/25 | Loss: 0.00066760
Iteration 17/25 | Loss: 0.00066760
Iteration 18/25 | Loss: 0.00066760
Iteration 19/25 | Loss: 0.00066760
Iteration 20/25 | Loss: 0.00066760
Iteration 21/25 | Loss: 0.00066760
Iteration 22/25 | Loss: 0.00066760
Iteration 23/25 | Loss: 0.00066760
Iteration 24/25 | Loss: 0.00066760
Iteration 25/25 | Loss: 0.00066760

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066760
Iteration 2/1000 | Loss: 0.00003088
Iteration 3/1000 | Loss: 0.00002520
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002251
Iteration 6/1000 | Loss: 0.00002184
Iteration 7/1000 | Loss: 0.00002134
Iteration 8/1000 | Loss: 0.00002113
Iteration 9/1000 | Loss: 0.00002089
Iteration 10/1000 | Loss: 0.00002076
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002070
Iteration 13/1000 | Loss: 0.00002069
Iteration 14/1000 | Loss: 0.00002069
Iteration 15/1000 | Loss: 0.00002069
Iteration 16/1000 | Loss: 0.00002068
Iteration 17/1000 | Loss: 0.00002068
Iteration 18/1000 | Loss: 0.00002067
Iteration 19/1000 | Loss: 0.00002067
Iteration 20/1000 | Loss: 0.00002065
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002065
Iteration 24/1000 | Loss: 0.00002064
Iteration 25/1000 | Loss: 0.00002063
Iteration 26/1000 | Loss: 0.00002063
Iteration 27/1000 | Loss: 0.00002062
Iteration 28/1000 | Loss: 0.00002062
Iteration 29/1000 | Loss: 0.00002061
Iteration 30/1000 | Loss: 0.00002060
Iteration 31/1000 | Loss: 0.00002060
Iteration 32/1000 | Loss: 0.00002059
Iteration 33/1000 | Loss: 0.00002059
Iteration 34/1000 | Loss: 0.00002059
Iteration 35/1000 | Loss: 0.00002059
Iteration 36/1000 | Loss: 0.00002059
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002058
Iteration 39/1000 | Loss: 0.00002058
Iteration 40/1000 | Loss: 0.00002058
Iteration 41/1000 | Loss: 0.00002058
Iteration 42/1000 | Loss: 0.00002058
Iteration 43/1000 | Loss: 0.00002058
Iteration 44/1000 | Loss: 0.00002058
Iteration 45/1000 | Loss: 0.00002058
Iteration 46/1000 | Loss: 0.00002057
Iteration 47/1000 | Loss: 0.00002057
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002057
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002057
Iteration 56/1000 | Loss: 0.00002057
Iteration 57/1000 | Loss: 0.00002057
Iteration 58/1000 | Loss: 0.00002057
Iteration 59/1000 | Loss: 0.00002057
Iteration 60/1000 | Loss: 0.00002057
Iteration 61/1000 | Loss: 0.00002057
Iteration 62/1000 | Loss: 0.00002057
Iteration 63/1000 | Loss: 0.00002057
Iteration 64/1000 | Loss: 0.00002057
Iteration 65/1000 | Loss: 0.00002057
Iteration 66/1000 | Loss: 0.00002057
Iteration 67/1000 | Loss: 0.00002057
Iteration 68/1000 | Loss: 0.00002057
Iteration 69/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.0566305465763435e-05, 2.0566305465763435e-05, 2.0566305465763435e-05, 2.0566305465763435e-05, 2.0566305465763435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0566305465763435e-05

Optimization complete. Final v2v error: 3.8029847145080566 mm

Highest mean error: 4.186776638031006 mm for frame 141

Lowest mean error: 3.4593992233276367 mm for frame 11

Saving results

Total time: 29.74321985244751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037144
Iteration 2/25 | Loss: 0.00317500
Iteration 3/25 | Loss: 0.00199049
Iteration 4/25 | Loss: 0.00174602
Iteration 5/25 | Loss: 0.00156833
Iteration 6/25 | Loss: 0.00147123
Iteration 7/25 | Loss: 0.00143496
Iteration 8/25 | Loss: 0.00147600
Iteration 9/25 | Loss: 0.00135652
Iteration 10/25 | Loss: 0.00129881
Iteration 11/25 | Loss: 0.00127753
Iteration 12/25 | Loss: 0.00129790
Iteration 13/25 | Loss: 0.00122216
Iteration 14/25 | Loss: 0.00119043
Iteration 15/25 | Loss: 0.00117791
Iteration 16/25 | Loss: 0.00118022
Iteration 17/25 | Loss: 0.00117768
Iteration 18/25 | Loss: 0.00117703
Iteration 19/25 | Loss: 0.00117469
Iteration 20/25 | Loss: 0.00117067
Iteration 21/25 | Loss: 0.00117040
Iteration 22/25 | Loss: 0.00116531
Iteration 23/25 | Loss: 0.00116839
Iteration 24/25 | Loss: 0.00116802
Iteration 25/25 | Loss: 0.00116922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30552471
Iteration 2/25 | Loss: 0.00159884
Iteration 3/25 | Loss: 0.00155577
Iteration 4/25 | Loss: 0.00155577
Iteration 5/25 | Loss: 0.00155577
Iteration 6/25 | Loss: 0.00155577
Iteration 7/25 | Loss: 0.00155577
Iteration 8/25 | Loss: 0.00155577
Iteration 9/25 | Loss: 0.00155577
Iteration 10/25 | Loss: 0.00155577
Iteration 11/25 | Loss: 0.00155577
Iteration 12/25 | Loss: 0.00155577
Iteration 13/25 | Loss: 0.00155577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015557670267298818, 0.0015557670267298818, 0.0015557670267298818, 0.0015557670267298818, 0.0015557670267298818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015557670267298818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155577
Iteration 2/1000 | Loss: 0.00040096
Iteration 3/1000 | Loss: 0.00022335
Iteration 4/1000 | Loss: 0.00022146
Iteration 5/1000 | Loss: 0.00024565
Iteration 6/1000 | Loss: 0.00023630
Iteration 7/1000 | Loss: 0.00017206
Iteration 8/1000 | Loss: 0.00023904
Iteration 9/1000 | Loss: 0.00030769
Iteration 10/1000 | Loss: 0.00018995
Iteration 11/1000 | Loss: 0.00021792
Iteration 12/1000 | Loss: 0.00023444
Iteration 13/1000 | Loss: 0.00024757
Iteration 14/1000 | Loss: 0.00019527
Iteration 15/1000 | Loss: 0.00014552
Iteration 16/1000 | Loss: 0.00009396
Iteration 17/1000 | Loss: 0.00008994
Iteration 18/1000 | Loss: 0.00008717
Iteration 19/1000 | Loss: 0.00008531
Iteration 20/1000 | Loss: 0.00008380
Iteration 21/1000 | Loss: 0.00008247
Iteration 22/1000 | Loss: 0.00008113
Iteration 23/1000 | Loss: 0.00007987
Iteration 24/1000 | Loss: 0.00007819
Iteration 25/1000 | Loss: 0.00007713
Iteration 26/1000 | Loss: 0.00155604
Iteration 27/1000 | Loss: 0.00599983
Iteration 28/1000 | Loss: 0.00040429
Iteration 29/1000 | Loss: 0.00098056
Iteration 30/1000 | Loss: 0.00081850
Iteration 31/1000 | Loss: 0.00606841
Iteration 32/1000 | Loss: 0.00241885
Iteration 33/1000 | Loss: 0.00230090
Iteration 34/1000 | Loss: 0.00116334
Iteration 35/1000 | Loss: 0.00195989
Iteration 36/1000 | Loss: 0.00067721
Iteration 37/1000 | Loss: 0.00021631
Iteration 38/1000 | Loss: 0.00024725
Iteration 39/1000 | Loss: 0.00030393
Iteration 40/1000 | Loss: 0.00021737
Iteration 41/1000 | Loss: 0.00022062
Iteration 42/1000 | Loss: 0.00024528
Iteration 43/1000 | Loss: 0.00072104
Iteration 44/1000 | Loss: 0.00031071
Iteration 45/1000 | Loss: 0.00019894
Iteration 46/1000 | Loss: 0.00014814
Iteration 47/1000 | Loss: 0.00008538
Iteration 48/1000 | Loss: 0.00011447
Iteration 49/1000 | Loss: 0.00019389
Iteration 50/1000 | Loss: 0.00011037
Iteration 51/1000 | Loss: 0.00014528
Iteration 52/1000 | Loss: 0.00030073
Iteration 53/1000 | Loss: 0.00011414
Iteration 54/1000 | Loss: 0.00008278
Iteration 55/1000 | Loss: 0.00009741
Iteration 56/1000 | Loss: 0.00007806
Iteration 57/1000 | Loss: 0.00007650
Iteration 58/1000 | Loss: 0.00007564
Iteration 59/1000 | Loss: 0.00007494
Iteration 60/1000 | Loss: 0.00007548
Iteration 61/1000 | Loss: 0.00007203
Iteration 62/1000 | Loss: 0.00061906
Iteration 63/1000 | Loss: 0.00033365
Iteration 64/1000 | Loss: 0.00070610
Iteration 65/1000 | Loss: 0.00041682
Iteration 66/1000 | Loss: 0.00075055
Iteration 67/1000 | Loss: 0.00040172
Iteration 68/1000 | Loss: 0.00071606
Iteration 69/1000 | Loss: 0.00007493
Iteration 70/1000 | Loss: 0.00007196
Iteration 71/1000 | Loss: 0.00006964
Iteration 72/1000 | Loss: 0.00006835
Iteration 73/1000 | Loss: 0.00093382
Iteration 74/1000 | Loss: 0.00030315
Iteration 75/1000 | Loss: 0.00010263
Iteration 76/1000 | Loss: 0.00042997
Iteration 77/1000 | Loss: 0.00013176
Iteration 78/1000 | Loss: 0.00052508
Iteration 79/1000 | Loss: 0.00014632
Iteration 80/1000 | Loss: 0.00011092
Iteration 81/1000 | Loss: 0.00029659
Iteration 82/1000 | Loss: 0.00018275
Iteration 83/1000 | Loss: 0.00010099
Iteration 84/1000 | Loss: 0.00006849
Iteration 85/1000 | Loss: 0.00006742
Iteration 86/1000 | Loss: 0.00006705
Iteration 87/1000 | Loss: 0.00035390
Iteration 88/1000 | Loss: 0.00010696
Iteration 89/1000 | Loss: 0.00006753
Iteration 90/1000 | Loss: 0.00032185
Iteration 91/1000 | Loss: 0.00009633
Iteration 92/1000 | Loss: 0.00023815
Iteration 93/1000 | Loss: 0.00042551
Iteration 94/1000 | Loss: 0.00044440
Iteration 95/1000 | Loss: 0.00053820
Iteration 96/1000 | Loss: 0.00010656
Iteration 97/1000 | Loss: 0.00056927
Iteration 98/1000 | Loss: 0.00048850
Iteration 99/1000 | Loss: 0.00010941
Iteration 100/1000 | Loss: 0.00021769
Iteration 101/1000 | Loss: 0.00009280
Iteration 102/1000 | Loss: 0.00049101
Iteration 103/1000 | Loss: 0.00073751
Iteration 104/1000 | Loss: 0.00043619
Iteration 105/1000 | Loss: 0.00035448
Iteration 106/1000 | Loss: 0.00045877
Iteration 107/1000 | Loss: 0.00047509
Iteration 108/1000 | Loss: 0.00016935
Iteration 109/1000 | Loss: 0.00054044
Iteration 110/1000 | Loss: 0.00009155
Iteration 111/1000 | Loss: 0.00007398
Iteration 112/1000 | Loss: 0.00094904
Iteration 113/1000 | Loss: 0.00065598
Iteration 114/1000 | Loss: 0.00051915
Iteration 115/1000 | Loss: 0.00007227
Iteration 116/1000 | Loss: 0.00006834
Iteration 117/1000 | Loss: 0.00006682
Iteration 118/1000 | Loss: 0.00006653
Iteration 119/1000 | Loss: 0.00006629
Iteration 120/1000 | Loss: 0.00041412
Iteration 121/1000 | Loss: 0.00029809
Iteration 122/1000 | Loss: 0.00023138
Iteration 123/1000 | Loss: 0.00047331
Iteration 124/1000 | Loss: 0.00033908
Iteration 125/1000 | Loss: 0.00034925
Iteration 126/1000 | Loss: 0.00010863
Iteration 127/1000 | Loss: 0.00011703
Iteration 128/1000 | Loss: 0.00013919
Iteration 129/1000 | Loss: 0.00011477
Iteration 130/1000 | Loss: 0.00011279
Iteration 131/1000 | Loss: 0.00011740
Iteration 132/1000 | Loss: 0.00012818
Iteration 133/1000 | Loss: 0.00008954
Iteration 134/1000 | Loss: 0.00014295
Iteration 135/1000 | Loss: 0.00011724
Iteration 136/1000 | Loss: 0.00012780
Iteration 137/1000 | Loss: 0.00017584
Iteration 138/1000 | Loss: 0.00012613
Iteration 139/1000 | Loss: 0.00018959
Iteration 140/1000 | Loss: 0.00012475
Iteration 141/1000 | Loss: 0.00017769
Iteration 142/1000 | Loss: 0.00048125
Iteration 143/1000 | Loss: 0.00018371
Iteration 144/1000 | Loss: 0.00006719
Iteration 145/1000 | Loss: 0.00010143
Iteration 146/1000 | Loss: 0.00011632
Iteration 147/1000 | Loss: 0.00016370
Iteration 148/1000 | Loss: 0.00039702
Iteration 149/1000 | Loss: 0.00048040
Iteration 150/1000 | Loss: 0.00035258
Iteration 151/1000 | Loss: 0.00017062
Iteration 152/1000 | Loss: 0.00013394
Iteration 153/1000 | Loss: 0.00036681
Iteration 154/1000 | Loss: 0.00019550
Iteration 155/1000 | Loss: 0.00014612
Iteration 156/1000 | Loss: 0.00014873
Iteration 157/1000 | Loss: 0.00043198
Iteration 158/1000 | Loss: 0.00032216
Iteration 159/1000 | Loss: 0.00019958
Iteration 160/1000 | Loss: 0.00011171
Iteration 161/1000 | Loss: 0.00024697
Iteration 162/1000 | Loss: 0.00073740
Iteration 163/1000 | Loss: 0.00041196
Iteration 164/1000 | Loss: 0.00052475
Iteration 165/1000 | Loss: 0.00038527
Iteration 166/1000 | Loss: 0.00050315
Iteration 167/1000 | Loss: 0.00036120
Iteration 168/1000 | Loss: 0.00032131
Iteration 169/1000 | Loss: 0.00039726
Iteration 170/1000 | Loss: 0.00040098
Iteration 171/1000 | Loss: 0.00014191
Iteration 172/1000 | Loss: 0.00041901
Iteration 173/1000 | Loss: 0.00025865
Iteration 174/1000 | Loss: 0.00010618
Iteration 175/1000 | Loss: 0.00035560
Iteration 176/1000 | Loss: 0.00010782
Iteration 177/1000 | Loss: 0.00037982
Iteration 178/1000 | Loss: 0.00011213
Iteration 179/1000 | Loss: 0.00047187
Iteration 180/1000 | Loss: 0.00068073
Iteration 181/1000 | Loss: 0.00036025
Iteration 182/1000 | Loss: 0.00073866
Iteration 183/1000 | Loss: 0.00245147
Iteration 184/1000 | Loss: 0.00112478
Iteration 185/1000 | Loss: 0.00042323
Iteration 186/1000 | Loss: 0.00018332
Iteration 187/1000 | Loss: 0.00016078
Iteration 188/1000 | Loss: 0.00042024
Iteration 189/1000 | Loss: 0.00040133
Iteration 190/1000 | Loss: 0.00049523
Iteration 191/1000 | Loss: 0.00021475
Iteration 192/1000 | Loss: 0.00054859
Iteration 193/1000 | Loss: 0.00042925
Iteration 194/1000 | Loss: 0.00010227
Iteration 195/1000 | Loss: 0.00007938
Iteration 196/1000 | Loss: 0.00007289
Iteration 197/1000 | Loss: 0.00006983
Iteration 198/1000 | Loss: 0.00052403
Iteration 199/1000 | Loss: 0.00053983
Iteration 200/1000 | Loss: 0.00053388
Iteration 201/1000 | Loss: 0.00032024
Iteration 202/1000 | Loss: 0.00007752
Iteration 203/1000 | Loss: 0.00007076
Iteration 204/1000 | Loss: 0.00006741
Iteration 205/1000 | Loss: 0.00006453
Iteration 206/1000 | Loss: 0.00006240
Iteration 207/1000 | Loss: 0.00007830
Iteration 208/1000 | Loss: 0.00006103
Iteration 209/1000 | Loss: 0.00006039
Iteration 210/1000 | Loss: 0.00006450
Iteration 211/1000 | Loss: 0.00005962
Iteration 212/1000 | Loss: 0.00005921
Iteration 213/1000 | Loss: 0.00005898
Iteration 214/1000 | Loss: 0.00005897
Iteration 215/1000 | Loss: 0.00005889
Iteration 216/1000 | Loss: 0.00005887
Iteration 217/1000 | Loss: 0.00147417
Iteration 218/1000 | Loss: 0.00041603
Iteration 219/1000 | Loss: 0.00010136
Iteration 220/1000 | Loss: 0.00006708
Iteration 221/1000 | Loss: 0.00171026
Iteration 222/1000 | Loss: 0.00072412
Iteration 223/1000 | Loss: 0.00027509
Iteration 224/1000 | Loss: 0.00007911
Iteration 225/1000 | Loss: 0.00006731
Iteration 226/1000 | Loss: 0.00007289
Iteration 227/1000 | Loss: 0.00006278
Iteration 228/1000 | Loss: 0.00114400
Iteration 229/1000 | Loss: 0.00131921
Iteration 230/1000 | Loss: 0.00081593
Iteration 231/1000 | Loss: 0.00092197
Iteration 232/1000 | Loss: 0.00113761
Iteration 233/1000 | Loss: 0.00221728
Iteration 234/1000 | Loss: 0.00176191
Iteration 235/1000 | Loss: 0.00112450
Iteration 236/1000 | Loss: 0.00036754
Iteration 237/1000 | Loss: 0.00142983
Iteration 238/1000 | Loss: 0.00086877
Iteration 239/1000 | Loss: 0.00173014
Iteration 240/1000 | Loss: 0.00101627
Iteration 241/1000 | Loss: 0.00116399
Iteration 242/1000 | Loss: 0.00092832
Iteration 243/1000 | Loss: 0.00133062
Iteration 244/1000 | Loss: 0.00093465
Iteration 245/1000 | Loss: 0.00123618
Iteration 246/1000 | Loss: 0.00090679
Iteration 247/1000 | Loss: 0.00014514
Iteration 248/1000 | Loss: 0.00052847
Iteration 249/1000 | Loss: 0.00034715
Iteration 250/1000 | Loss: 0.00038106
Iteration 251/1000 | Loss: 0.00010570
Iteration 252/1000 | Loss: 0.00014643
Iteration 253/1000 | Loss: 0.00007993
Iteration 254/1000 | Loss: 0.00061493
Iteration 255/1000 | Loss: 0.00032462
Iteration 256/1000 | Loss: 0.00055716
Iteration 257/1000 | Loss: 0.00026863
Iteration 258/1000 | Loss: 0.00007063
Iteration 259/1000 | Loss: 0.00028307
Iteration 260/1000 | Loss: 0.00064427
Iteration 261/1000 | Loss: 0.00011233
Iteration 262/1000 | Loss: 0.00008298
Iteration 263/1000 | Loss: 0.00007224
Iteration 264/1000 | Loss: 0.00006699
Iteration 265/1000 | Loss: 0.00013737
Iteration 266/1000 | Loss: 0.00082742
Iteration 267/1000 | Loss: 0.00064624
Iteration 268/1000 | Loss: 0.00006109
Iteration 269/1000 | Loss: 0.00082543
Iteration 270/1000 | Loss: 0.00057561
Iteration 271/1000 | Loss: 0.00018082
Iteration 272/1000 | Loss: 0.00018440
Iteration 273/1000 | Loss: 0.00013422
Iteration 274/1000 | Loss: 0.00092415
Iteration 275/1000 | Loss: 0.00088675
Iteration 276/1000 | Loss: 0.00093876
Iteration 277/1000 | Loss: 0.00007324
Iteration 278/1000 | Loss: 0.00008357
Iteration 279/1000 | Loss: 0.00061733
Iteration 280/1000 | Loss: 0.00022324
Iteration 281/1000 | Loss: 0.00007489
Iteration 282/1000 | Loss: 0.00005870
Iteration 283/1000 | Loss: 0.00009739
Iteration 284/1000 | Loss: 0.00007649
Iteration 285/1000 | Loss: 0.00005236
Iteration 286/1000 | Loss: 0.00005380
Iteration 287/1000 | Loss: 0.00005033
Iteration 288/1000 | Loss: 0.00005203
Iteration 289/1000 | Loss: 0.00004900
Iteration 290/1000 | Loss: 0.00004859
Iteration 291/1000 | Loss: 0.00153778
Iteration 292/1000 | Loss: 0.00082066
Iteration 293/1000 | Loss: 0.00136830
Iteration 294/1000 | Loss: 0.00011417
Iteration 295/1000 | Loss: 0.00030237
Iteration 296/1000 | Loss: 0.00006447
Iteration 297/1000 | Loss: 0.00008210
Iteration 298/1000 | Loss: 0.00005415
Iteration 299/1000 | Loss: 0.00025743
Iteration 300/1000 | Loss: 0.00012259
Iteration 301/1000 | Loss: 0.00014723
Iteration 302/1000 | Loss: 0.00025137
Iteration 303/1000 | Loss: 0.00005077
Iteration 304/1000 | Loss: 0.00027604
Iteration 305/1000 | Loss: 0.00028480
Iteration 306/1000 | Loss: 0.00021353
Iteration 307/1000 | Loss: 0.00147871
Iteration 308/1000 | Loss: 0.00092314
Iteration 309/1000 | Loss: 0.00077990
Iteration 310/1000 | Loss: 0.00074237
Iteration 311/1000 | Loss: 0.00126928
Iteration 312/1000 | Loss: 0.00105634
Iteration 313/1000 | Loss: 0.00073145
Iteration 314/1000 | Loss: 0.00092909
Iteration 315/1000 | Loss: 0.00098793
Iteration 316/1000 | Loss: 0.00087962
Iteration 317/1000 | Loss: 0.00107554
Iteration 318/1000 | Loss: 0.00123171
Iteration 319/1000 | Loss: 0.00155189
Iteration 320/1000 | Loss: 0.00008929
Iteration 321/1000 | Loss: 0.00010966
Iteration 322/1000 | Loss: 0.00013496
Iteration 323/1000 | Loss: 0.00043728
Iteration 324/1000 | Loss: 0.00032343
Iteration 325/1000 | Loss: 0.00031480
Iteration 326/1000 | Loss: 0.00004790
Iteration 327/1000 | Loss: 0.00028829
Iteration 328/1000 | Loss: 0.00059876
Iteration 329/1000 | Loss: 0.00006389
Iteration 330/1000 | Loss: 0.00006794
Iteration 331/1000 | Loss: 0.00005345
Iteration 332/1000 | Loss: 0.00004595
Iteration 333/1000 | Loss: 0.00004278
Iteration 334/1000 | Loss: 0.00004035
Iteration 335/1000 | Loss: 0.00005765
Iteration 336/1000 | Loss: 0.00003845
Iteration 337/1000 | Loss: 0.00003763
Iteration 338/1000 | Loss: 0.00003734
Iteration 339/1000 | Loss: 0.00005330
Iteration 340/1000 | Loss: 0.00003992
Iteration 341/1000 | Loss: 0.00003750
Iteration 342/1000 | Loss: 0.00003660
Iteration 343/1000 | Loss: 0.00003648
Iteration 344/1000 | Loss: 0.00003644
Iteration 345/1000 | Loss: 0.00003641
Iteration 346/1000 | Loss: 0.00003638
Iteration 347/1000 | Loss: 0.00003636
Iteration 348/1000 | Loss: 0.00003635
Iteration 349/1000 | Loss: 0.00003633
Iteration 350/1000 | Loss: 0.00003633
Iteration 351/1000 | Loss: 0.00003632
Iteration 352/1000 | Loss: 0.00003632
Iteration 353/1000 | Loss: 0.00003629
Iteration 354/1000 | Loss: 0.00003629
Iteration 355/1000 | Loss: 0.00003629
Iteration 356/1000 | Loss: 0.00003628
Iteration 357/1000 | Loss: 0.00003628
Iteration 358/1000 | Loss: 0.00003628
Iteration 359/1000 | Loss: 0.00003627
Iteration 360/1000 | Loss: 0.00003627
Iteration 361/1000 | Loss: 0.00003627
Iteration 362/1000 | Loss: 0.00003627
Iteration 363/1000 | Loss: 0.00003627
Iteration 364/1000 | Loss: 0.00003627
Iteration 365/1000 | Loss: 0.00003626
Iteration 366/1000 | Loss: 0.00003626
Iteration 367/1000 | Loss: 0.00003626
Iteration 368/1000 | Loss: 0.00003626
Iteration 369/1000 | Loss: 0.00003626
Iteration 370/1000 | Loss: 0.00003625
Iteration 371/1000 | Loss: 0.00003625
Iteration 372/1000 | Loss: 0.00003625
Iteration 373/1000 | Loss: 0.00003625
Iteration 374/1000 | Loss: 0.00003625
Iteration 375/1000 | Loss: 0.00003625
Iteration 376/1000 | Loss: 0.00003625
Iteration 377/1000 | Loss: 0.00003624
Iteration 378/1000 | Loss: 0.00003624
Iteration 379/1000 | Loss: 0.00003624
Iteration 380/1000 | Loss: 0.00003624
Iteration 381/1000 | Loss: 0.00003624
Iteration 382/1000 | Loss: 0.00003624
Iteration 383/1000 | Loss: 0.00003624
Iteration 384/1000 | Loss: 0.00003624
Iteration 385/1000 | Loss: 0.00003624
Iteration 386/1000 | Loss: 0.00003624
Iteration 387/1000 | Loss: 0.00003623
Iteration 388/1000 | Loss: 0.00003623
Iteration 389/1000 | Loss: 0.00003623
Iteration 390/1000 | Loss: 0.00003623
Iteration 391/1000 | Loss: 0.00003622
Iteration 392/1000 | Loss: 0.00003622
Iteration 393/1000 | Loss: 0.00003621
Iteration 394/1000 | Loss: 0.00003621
Iteration 395/1000 | Loss: 0.00003621
Iteration 396/1000 | Loss: 0.00003621
Iteration 397/1000 | Loss: 0.00003620
Iteration 398/1000 | Loss: 0.00003620
Iteration 399/1000 | Loss: 0.00003620
Iteration 400/1000 | Loss: 0.00003620
Iteration 401/1000 | Loss: 0.00003620
Iteration 402/1000 | Loss: 0.00003620
Iteration 403/1000 | Loss: 0.00003619
Iteration 404/1000 | Loss: 0.00003619
Iteration 405/1000 | Loss: 0.00003619
Iteration 406/1000 | Loss: 0.00003619
Iteration 407/1000 | Loss: 0.00003619
Iteration 408/1000 | Loss: 0.00003619
Iteration 409/1000 | Loss: 0.00003619
Iteration 410/1000 | Loss: 0.00003619
Iteration 411/1000 | Loss: 0.00003618
Iteration 412/1000 | Loss: 0.00003618
Iteration 413/1000 | Loss: 0.00003618
Iteration 414/1000 | Loss: 0.00003618
Iteration 415/1000 | Loss: 0.00003618
Iteration 416/1000 | Loss: 0.00003618
Iteration 417/1000 | Loss: 0.00003618
Iteration 418/1000 | Loss: 0.00003618
Iteration 419/1000 | Loss: 0.00003618
Iteration 420/1000 | Loss: 0.00003618
Iteration 421/1000 | Loss: 0.00003618
Iteration 422/1000 | Loss: 0.00003618
Iteration 423/1000 | Loss: 0.00003618
Iteration 424/1000 | Loss: 0.00003618
Iteration 425/1000 | Loss: 0.00003618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 425. Stopping optimization.
Last 5 losses: [3.6181339964969084e-05, 3.6181339964969084e-05, 3.6181339964969084e-05, 3.6181339964969084e-05, 3.6181339964969084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6181339964969084e-05

Optimization complete. Final v2v error: 3.937330484390259 mm

Highest mean error: 11.246085166931152 mm for frame 10

Lowest mean error: 2.651252031326294 mm for frame 38

Saving results

Total time: 509.59426856040955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876921
Iteration 2/25 | Loss: 0.00148238
Iteration 3/25 | Loss: 0.00113147
Iteration 4/25 | Loss: 0.00110078
Iteration 5/25 | Loss: 0.00109641
Iteration 6/25 | Loss: 0.00109562
Iteration 7/25 | Loss: 0.00109562
Iteration 8/25 | Loss: 0.00109562
Iteration 9/25 | Loss: 0.00109562
Iteration 10/25 | Loss: 0.00109562
Iteration 11/25 | Loss: 0.00109562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010956197511404753, 0.0010956197511404753, 0.0010956197511404753, 0.0010956197511404753, 0.0010956197511404753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010956197511404753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34080684
Iteration 2/25 | Loss: 0.00030841
Iteration 3/25 | Loss: 0.00030841
Iteration 4/25 | Loss: 0.00030841
Iteration 5/25 | Loss: 0.00030841
Iteration 6/25 | Loss: 0.00030840
Iteration 7/25 | Loss: 0.00030840
Iteration 8/25 | Loss: 0.00030840
Iteration 9/25 | Loss: 0.00030840
Iteration 10/25 | Loss: 0.00030840
Iteration 11/25 | Loss: 0.00030840
Iteration 12/25 | Loss: 0.00030840
Iteration 13/25 | Loss: 0.00030840
Iteration 14/25 | Loss: 0.00030840
Iteration 15/25 | Loss: 0.00030840
Iteration 16/25 | Loss: 0.00030840
Iteration 17/25 | Loss: 0.00030840
Iteration 18/25 | Loss: 0.00030840
Iteration 19/25 | Loss: 0.00030840
Iteration 20/25 | Loss: 0.00030840
Iteration 21/25 | Loss: 0.00030840
Iteration 22/25 | Loss: 0.00030840
Iteration 23/25 | Loss: 0.00030840
Iteration 24/25 | Loss: 0.00030840
Iteration 25/25 | Loss: 0.00030840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00030840374529361725, 0.00030840374529361725, 0.00030840374529361725, 0.00030840374529361725, 0.00030840374529361725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030840374529361725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030840
Iteration 2/1000 | Loss: 0.00005613
Iteration 3/1000 | Loss: 0.00004455
Iteration 4/1000 | Loss: 0.00004183
Iteration 5/1000 | Loss: 0.00004011
Iteration 6/1000 | Loss: 0.00003903
Iteration 7/1000 | Loss: 0.00003830
Iteration 8/1000 | Loss: 0.00003752
Iteration 9/1000 | Loss: 0.00003690
Iteration 10/1000 | Loss: 0.00003662
Iteration 11/1000 | Loss: 0.00003651
Iteration 12/1000 | Loss: 0.00003648
Iteration 13/1000 | Loss: 0.00003648
Iteration 14/1000 | Loss: 0.00003648
Iteration 15/1000 | Loss: 0.00003648
Iteration 16/1000 | Loss: 0.00003648
Iteration 17/1000 | Loss: 0.00003648
Iteration 18/1000 | Loss: 0.00003648
Iteration 19/1000 | Loss: 0.00003648
Iteration 20/1000 | Loss: 0.00003648
Iteration 21/1000 | Loss: 0.00003647
Iteration 22/1000 | Loss: 0.00003646
Iteration 23/1000 | Loss: 0.00003646
Iteration 24/1000 | Loss: 0.00003643
Iteration 25/1000 | Loss: 0.00003642
Iteration 26/1000 | Loss: 0.00003642
Iteration 27/1000 | Loss: 0.00003641
Iteration 28/1000 | Loss: 0.00003640
Iteration 29/1000 | Loss: 0.00003638
Iteration 30/1000 | Loss: 0.00003638
Iteration 31/1000 | Loss: 0.00003638
Iteration 32/1000 | Loss: 0.00003638
Iteration 33/1000 | Loss: 0.00003637
Iteration 34/1000 | Loss: 0.00003637
Iteration 35/1000 | Loss: 0.00003637
Iteration 36/1000 | Loss: 0.00003637
Iteration 37/1000 | Loss: 0.00003637
Iteration 38/1000 | Loss: 0.00003637
Iteration 39/1000 | Loss: 0.00003637
Iteration 40/1000 | Loss: 0.00003636
Iteration 41/1000 | Loss: 0.00003636
Iteration 42/1000 | Loss: 0.00003634
Iteration 43/1000 | Loss: 0.00003634
Iteration 44/1000 | Loss: 0.00003634
Iteration 45/1000 | Loss: 0.00003634
Iteration 46/1000 | Loss: 0.00003634
Iteration 47/1000 | Loss: 0.00003634
Iteration 48/1000 | Loss: 0.00003634
Iteration 49/1000 | Loss: 0.00003634
Iteration 50/1000 | Loss: 0.00003634
Iteration 51/1000 | Loss: 0.00003633
Iteration 52/1000 | Loss: 0.00003633
Iteration 53/1000 | Loss: 0.00003633
Iteration 54/1000 | Loss: 0.00003633
Iteration 55/1000 | Loss: 0.00003632
Iteration 56/1000 | Loss: 0.00003632
Iteration 57/1000 | Loss: 0.00003631
Iteration 58/1000 | Loss: 0.00003631
Iteration 59/1000 | Loss: 0.00003631
Iteration 60/1000 | Loss: 0.00003631
Iteration 61/1000 | Loss: 0.00003631
Iteration 62/1000 | Loss: 0.00003630
Iteration 63/1000 | Loss: 0.00003630
Iteration 64/1000 | Loss: 0.00003630
Iteration 65/1000 | Loss: 0.00003630
Iteration 66/1000 | Loss: 0.00003629
Iteration 67/1000 | Loss: 0.00003629
Iteration 68/1000 | Loss: 0.00003629
Iteration 69/1000 | Loss: 0.00003629
Iteration 70/1000 | Loss: 0.00003629
Iteration 71/1000 | Loss: 0.00003629
Iteration 72/1000 | Loss: 0.00003629
Iteration 73/1000 | Loss: 0.00003628
Iteration 74/1000 | Loss: 0.00003628
Iteration 75/1000 | Loss: 0.00003628
Iteration 76/1000 | Loss: 0.00003628
Iteration 77/1000 | Loss: 0.00003628
Iteration 78/1000 | Loss: 0.00003627
Iteration 79/1000 | Loss: 0.00003627
Iteration 80/1000 | Loss: 0.00003627
Iteration 81/1000 | Loss: 0.00003627
Iteration 82/1000 | Loss: 0.00003627
Iteration 83/1000 | Loss: 0.00003627
Iteration 84/1000 | Loss: 0.00003627
Iteration 85/1000 | Loss: 0.00003627
Iteration 86/1000 | Loss: 0.00003627
Iteration 87/1000 | Loss: 0.00003626
Iteration 88/1000 | Loss: 0.00003626
Iteration 89/1000 | Loss: 0.00003626
Iteration 90/1000 | Loss: 0.00003626
Iteration 91/1000 | Loss: 0.00003626
Iteration 92/1000 | Loss: 0.00003626
Iteration 93/1000 | Loss: 0.00003626
Iteration 94/1000 | Loss: 0.00003626
Iteration 95/1000 | Loss: 0.00003626
Iteration 96/1000 | Loss: 0.00003626
Iteration 97/1000 | Loss: 0.00003625
Iteration 98/1000 | Loss: 0.00003625
Iteration 99/1000 | Loss: 0.00003624
Iteration 100/1000 | Loss: 0.00003624
Iteration 101/1000 | Loss: 0.00003624
Iteration 102/1000 | Loss: 0.00003624
Iteration 103/1000 | Loss: 0.00003624
Iteration 104/1000 | Loss: 0.00003624
Iteration 105/1000 | Loss: 0.00003624
Iteration 106/1000 | Loss: 0.00003624
Iteration 107/1000 | Loss: 0.00003624
Iteration 108/1000 | Loss: 0.00003623
Iteration 109/1000 | Loss: 0.00003623
Iteration 110/1000 | Loss: 0.00003623
Iteration 111/1000 | Loss: 0.00003622
Iteration 112/1000 | Loss: 0.00003622
Iteration 113/1000 | Loss: 0.00003622
Iteration 114/1000 | Loss: 0.00003622
Iteration 115/1000 | Loss: 0.00003622
Iteration 116/1000 | Loss: 0.00003621
Iteration 117/1000 | Loss: 0.00003621
Iteration 118/1000 | Loss: 0.00003621
Iteration 119/1000 | Loss: 0.00003621
Iteration 120/1000 | Loss: 0.00003621
Iteration 121/1000 | Loss: 0.00003621
Iteration 122/1000 | Loss: 0.00003621
Iteration 123/1000 | Loss: 0.00003620
Iteration 124/1000 | Loss: 0.00003620
Iteration 125/1000 | Loss: 0.00003620
Iteration 126/1000 | Loss: 0.00003620
Iteration 127/1000 | Loss: 0.00003620
Iteration 128/1000 | Loss: 0.00003620
Iteration 129/1000 | Loss: 0.00003619
Iteration 130/1000 | Loss: 0.00003619
Iteration 131/1000 | Loss: 0.00003619
Iteration 132/1000 | Loss: 0.00003619
Iteration 133/1000 | Loss: 0.00003618
Iteration 134/1000 | Loss: 0.00003618
Iteration 135/1000 | Loss: 0.00003618
Iteration 136/1000 | Loss: 0.00003617
Iteration 137/1000 | Loss: 0.00003617
Iteration 138/1000 | Loss: 0.00003617
Iteration 139/1000 | Loss: 0.00003617
Iteration 140/1000 | Loss: 0.00003617
Iteration 141/1000 | Loss: 0.00003617
Iteration 142/1000 | Loss: 0.00003617
Iteration 143/1000 | Loss: 0.00003617
Iteration 144/1000 | Loss: 0.00003617
Iteration 145/1000 | Loss: 0.00003616
Iteration 146/1000 | Loss: 0.00003616
Iteration 147/1000 | Loss: 0.00003616
Iteration 148/1000 | Loss: 0.00003616
Iteration 149/1000 | Loss: 0.00003616
Iteration 150/1000 | Loss: 0.00003616
Iteration 151/1000 | Loss: 0.00003616
Iteration 152/1000 | Loss: 0.00003615
Iteration 153/1000 | Loss: 0.00003615
Iteration 154/1000 | Loss: 0.00003615
Iteration 155/1000 | Loss: 0.00003615
Iteration 156/1000 | Loss: 0.00003615
Iteration 157/1000 | Loss: 0.00003615
Iteration 158/1000 | Loss: 0.00003615
Iteration 159/1000 | Loss: 0.00003614
Iteration 160/1000 | Loss: 0.00003614
Iteration 161/1000 | Loss: 0.00003614
Iteration 162/1000 | Loss: 0.00003614
Iteration 163/1000 | Loss: 0.00003614
Iteration 164/1000 | Loss: 0.00003614
Iteration 165/1000 | Loss: 0.00003613
Iteration 166/1000 | Loss: 0.00003613
Iteration 167/1000 | Loss: 0.00003613
Iteration 168/1000 | Loss: 0.00003613
Iteration 169/1000 | Loss: 0.00003613
Iteration 170/1000 | Loss: 0.00003613
Iteration 171/1000 | Loss: 0.00003613
Iteration 172/1000 | Loss: 0.00003613
Iteration 173/1000 | Loss: 0.00003613
Iteration 174/1000 | Loss: 0.00003613
Iteration 175/1000 | Loss: 0.00003613
Iteration 176/1000 | Loss: 0.00003613
Iteration 177/1000 | Loss: 0.00003613
Iteration 178/1000 | Loss: 0.00003613
Iteration 179/1000 | Loss: 0.00003613
Iteration 180/1000 | Loss: 0.00003613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [3.613161243265495e-05, 3.613161243265495e-05, 3.613161243265495e-05, 3.613161243265495e-05, 3.613161243265495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.613161243265495e-05

Optimization complete. Final v2v error: 4.878174781799316 mm

Highest mean error: 5.392139434814453 mm for frame 78

Lowest mean error: 4.293582916259766 mm for frame 0

Saving results

Total time: 34.19355034828186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855619
Iteration 2/25 | Loss: 0.00144321
Iteration 3/25 | Loss: 0.00113424
Iteration 4/25 | Loss: 0.00110536
Iteration 5/25 | Loss: 0.00109664
Iteration 6/25 | Loss: 0.00109462
Iteration 7/25 | Loss: 0.00109454
Iteration 8/25 | Loss: 0.00109454
Iteration 9/25 | Loss: 0.00109454
Iteration 10/25 | Loss: 0.00109454
Iteration 11/25 | Loss: 0.00109454
Iteration 12/25 | Loss: 0.00109454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010945401154458523, 0.0010945401154458523, 0.0010945401154458523, 0.0010945401154458523, 0.0010945401154458523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010945401154458523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04718268
Iteration 2/25 | Loss: 0.00070272
Iteration 3/25 | Loss: 0.00070272
Iteration 4/25 | Loss: 0.00070271
Iteration 5/25 | Loss: 0.00070271
Iteration 6/25 | Loss: 0.00070271
Iteration 7/25 | Loss: 0.00070271
Iteration 8/25 | Loss: 0.00070271
Iteration 9/25 | Loss: 0.00070271
Iteration 10/25 | Loss: 0.00070271
Iteration 11/25 | Loss: 0.00070271
Iteration 12/25 | Loss: 0.00070271
Iteration 13/25 | Loss: 0.00070271
Iteration 14/25 | Loss: 0.00070271
Iteration 15/25 | Loss: 0.00070271
Iteration 16/25 | Loss: 0.00070271
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007027126266621053, 0.0007027126266621053, 0.0007027126266621053, 0.0007027126266621053, 0.0007027126266621053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007027126266621053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070271
Iteration 2/1000 | Loss: 0.00007286
Iteration 3/1000 | Loss: 0.00005104
Iteration 4/1000 | Loss: 0.00004239
Iteration 5/1000 | Loss: 0.00004035
Iteration 6/1000 | Loss: 0.00003902
Iteration 7/1000 | Loss: 0.00003824
Iteration 8/1000 | Loss: 0.00003742
Iteration 9/1000 | Loss: 0.00003665
Iteration 10/1000 | Loss: 0.00003596
Iteration 11/1000 | Loss: 0.00003537
Iteration 12/1000 | Loss: 0.00003507
Iteration 13/1000 | Loss: 0.00003479
Iteration 14/1000 | Loss: 0.00003454
Iteration 15/1000 | Loss: 0.00003434
Iteration 16/1000 | Loss: 0.00003416
Iteration 17/1000 | Loss: 0.00003398
Iteration 18/1000 | Loss: 0.00003398
Iteration 19/1000 | Loss: 0.00003390
Iteration 20/1000 | Loss: 0.00003382
Iteration 21/1000 | Loss: 0.00003372
Iteration 22/1000 | Loss: 0.00003362
Iteration 23/1000 | Loss: 0.00003352
Iteration 24/1000 | Loss: 0.00003349
Iteration 25/1000 | Loss: 0.00003340
Iteration 26/1000 | Loss: 0.00003340
Iteration 27/1000 | Loss: 0.00003338
Iteration 28/1000 | Loss: 0.00003338
Iteration 29/1000 | Loss: 0.00003336
Iteration 30/1000 | Loss: 0.00003335
Iteration 31/1000 | Loss: 0.00003334
Iteration 32/1000 | Loss: 0.00003334
Iteration 33/1000 | Loss: 0.00003333
Iteration 34/1000 | Loss: 0.00003333
Iteration 35/1000 | Loss: 0.00003332
Iteration 36/1000 | Loss: 0.00003329
Iteration 37/1000 | Loss: 0.00003329
Iteration 38/1000 | Loss: 0.00003329
Iteration 39/1000 | Loss: 0.00003329
Iteration 40/1000 | Loss: 0.00003329
Iteration 41/1000 | Loss: 0.00003329
Iteration 42/1000 | Loss: 0.00003329
Iteration 43/1000 | Loss: 0.00003329
Iteration 44/1000 | Loss: 0.00003329
Iteration 45/1000 | Loss: 0.00003329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [3.3289845305262133e-05, 3.3289845305262133e-05, 3.3289845305262133e-05, 3.3289845305262133e-05, 3.3289845305262133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3289845305262133e-05

Optimization complete. Final v2v error: 4.471589088439941 mm

Highest mean error: 6.470061779022217 mm for frame 155

Lowest mean error: 3.403738021850586 mm for frame 110

Saving results

Total time: 46.48361849784851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814552
Iteration 2/25 | Loss: 0.00169669
Iteration 3/25 | Loss: 0.00122021
Iteration 4/25 | Loss: 0.00120012
Iteration 5/25 | Loss: 0.00119498
Iteration 6/25 | Loss: 0.00119358
Iteration 7/25 | Loss: 0.00119344
Iteration 8/25 | Loss: 0.00119344
Iteration 9/25 | Loss: 0.00119342
Iteration 10/25 | Loss: 0.00119342
Iteration 11/25 | Loss: 0.00119342
Iteration 12/25 | Loss: 0.00119342
Iteration 13/25 | Loss: 0.00119342
Iteration 14/25 | Loss: 0.00119342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011934160720556974, 0.0011934160720556974, 0.0011934160720556974, 0.0011934160720556974, 0.0011934160720556974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011934160720556974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62115943
Iteration 2/25 | Loss: 0.00071449
Iteration 3/25 | Loss: 0.00071449
Iteration 4/25 | Loss: 0.00071449
Iteration 5/25 | Loss: 0.00071449
Iteration 6/25 | Loss: 0.00071449
Iteration 7/25 | Loss: 0.00071449
Iteration 8/25 | Loss: 0.00071449
Iteration 9/25 | Loss: 0.00071449
Iteration 10/25 | Loss: 0.00071449
Iteration 11/25 | Loss: 0.00071449
Iteration 12/25 | Loss: 0.00071449
Iteration 13/25 | Loss: 0.00071449
Iteration 14/25 | Loss: 0.00071449
Iteration 15/25 | Loss: 0.00071449
Iteration 16/25 | Loss: 0.00071449
Iteration 17/25 | Loss: 0.00071449
Iteration 18/25 | Loss: 0.00071449
Iteration 19/25 | Loss: 0.00071449
Iteration 20/25 | Loss: 0.00071449
Iteration 21/25 | Loss: 0.00071449
Iteration 22/25 | Loss: 0.00071449
Iteration 23/25 | Loss: 0.00071449
Iteration 24/25 | Loss: 0.00071449
Iteration 25/25 | Loss: 0.00071449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071449
Iteration 2/1000 | Loss: 0.00011768
Iteration 3/1000 | Loss: 0.00008518
Iteration 4/1000 | Loss: 0.00007260
Iteration 5/1000 | Loss: 0.00006646
Iteration 6/1000 | Loss: 0.00006393
Iteration 7/1000 | Loss: 0.00006242
Iteration 8/1000 | Loss: 0.00006092
Iteration 9/1000 | Loss: 0.00005983
Iteration 10/1000 | Loss: 0.00005877
Iteration 11/1000 | Loss: 0.00005777
Iteration 12/1000 | Loss: 0.00005690
Iteration 13/1000 | Loss: 0.00005596
Iteration 14/1000 | Loss: 0.00005502
Iteration 15/1000 | Loss: 0.00005444
Iteration 16/1000 | Loss: 0.00005409
Iteration 17/1000 | Loss: 0.00005368
Iteration 18/1000 | Loss: 0.00005325
Iteration 19/1000 | Loss: 0.00005283
Iteration 20/1000 | Loss: 0.00005252
Iteration 21/1000 | Loss: 0.00005231
Iteration 22/1000 | Loss: 0.00005213
Iteration 23/1000 | Loss: 0.00005194
Iteration 24/1000 | Loss: 0.00005179
Iteration 25/1000 | Loss: 0.00005167
Iteration 26/1000 | Loss: 0.00005163
Iteration 27/1000 | Loss: 0.00005163
Iteration 28/1000 | Loss: 0.00005160
Iteration 29/1000 | Loss: 0.00005159
Iteration 30/1000 | Loss: 0.00005159
Iteration 31/1000 | Loss: 0.00005150
Iteration 32/1000 | Loss: 0.00005148
Iteration 33/1000 | Loss: 0.00005148
Iteration 34/1000 | Loss: 0.00005144
Iteration 35/1000 | Loss: 0.00005144
Iteration 36/1000 | Loss: 0.00005144
Iteration 37/1000 | Loss: 0.00005143
Iteration 38/1000 | Loss: 0.00005142
Iteration 39/1000 | Loss: 0.00005139
Iteration 40/1000 | Loss: 0.00005136
Iteration 41/1000 | Loss: 0.00005136
Iteration 42/1000 | Loss: 0.00005136
Iteration 43/1000 | Loss: 0.00005136
Iteration 44/1000 | Loss: 0.00005135
Iteration 45/1000 | Loss: 0.00005135
Iteration 46/1000 | Loss: 0.00005134
Iteration 47/1000 | Loss: 0.00005134
Iteration 48/1000 | Loss: 0.00005131
Iteration 49/1000 | Loss: 0.00005131
Iteration 50/1000 | Loss: 0.00005131
Iteration 51/1000 | Loss: 0.00005131
Iteration 52/1000 | Loss: 0.00005131
Iteration 53/1000 | Loss: 0.00005131
Iteration 54/1000 | Loss: 0.00005131
Iteration 55/1000 | Loss: 0.00005131
Iteration 56/1000 | Loss: 0.00005131
Iteration 57/1000 | Loss: 0.00005131
Iteration 58/1000 | Loss: 0.00005130
Iteration 59/1000 | Loss: 0.00005130
Iteration 60/1000 | Loss: 0.00005130
Iteration 61/1000 | Loss: 0.00005130
Iteration 62/1000 | Loss: 0.00005130
Iteration 63/1000 | Loss: 0.00005130
Iteration 64/1000 | Loss: 0.00005130
Iteration 65/1000 | Loss: 0.00005130
Iteration 66/1000 | Loss: 0.00005130
Iteration 67/1000 | Loss: 0.00005130
Iteration 68/1000 | Loss: 0.00005130
Iteration 69/1000 | Loss: 0.00005130
Iteration 70/1000 | Loss: 0.00005130
Iteration 71/1000 | Loss: 0.00005129
Iteration 72/1000 | Loss: 0.00005129
Iteration 73/1000 | Loss: 0.00005129
Iteration 74/1000 | Loss: 0.00005129
Iteration 75/1000 | Loss: 0.00005129
Iteration 76/1000 | Loss: 0.00005129
Iteration 77/1000 | Loss: 0.00005129
Iteration 78/1000 | Loss: 0.00005129
Iteration 79/1000 | Loss: 0.00005128
Iteration 80/1000 | Loss: 0.00005128
Iteration 81/1000 | Loss: 0.00005128
Iteration 82/1000 | Loss: 0.00005128
Iteration 83/1000 | Loss: 0.00005128
Iteration 84/1000 | Loss: 0.00005128
Iteration 85/1000 | Loss: 0.00005128
Iteration 86/1000 | Loss: 0.00005128
Iteration 87/1000 | Loss: 0.00005128
Iteration 88/1000 | Loss: 0.00005128
Iteration 89/1000 | Loss: 0.00005128
Iteration 90/1000 | Loss: 0.00005127
Iteration 91/1000 | Loss: 0.00005127
Iteration 92/1000 | Loss: 0.00005127
Iteration 93/1000 | Loss: 0.00005127
Iteration 94/1000 | Loss: 0.00005126
Iteration 95/1000 | Loss: 0.00005126
Iteration 96/1000 | Loss: 0.00005126
Iteration 97/1000 | Loss: 0.00005126
Iteration 98/1000 | Loss: 0.00005126
Iteration 99/1000 | Loss: 0.00005126
Iteration 100/1000 | Loss: 0.00005126
Iteration 101/1000 | Loss: 0.00005126
Iteration 102/1000 | Loss: 0.00005126
Iteration 103/1000 | Loss: 0.00005126
Iteration 104/1000 | Loss: 0.00005126
Iteration 105/1000 | Loss: 0.00005126
Iteration 106/1000 | Loss: 0.00005126
Iteration 107/1000 | Loss: 0.00005125
Iteration 108/1000 | Loss: 0.00005125
Iteration 109/1000 | Loss: 0.00005125
Iteration 110/1000 | Loss: 0.00005125
Iteration 111/1000 | Loss: 0.00005124
Iteration 112/1000 | Loss: 0.00005124
Iteration 113/1000 | Loss: 0.00005124
Iteration 114/1000 | Loss: 0.00005124
Iteration 115/1000 | Loss: 0.00005124
Iteration 116/1000 | Loss: 0.00005124
Iteration 117/1000 | Loss: 0.00005124
Iteration 118/1000 | Loss: 0.00005123
Iteration 119/1000 | Loss: 0.00005123
Iteration 120/1000 | Loss: 0.00005123
Iteration 121/1000 | Loss: 0.00005123
Iteration 122/1000 | Loss: 0.00005123
Iteration 123/1000 | Loss: 0.00005123
Iteration 124/1000 | Loss: 0.00005123
Iteration 125/1000 | Loss: 0.00005123
Iteration 126/1000 | Loss: 0.00005122
Iteration 127/1000 | Loss: 0.00005122
Iteration 128/1000 | Loss: 0.00005122
Iteration 129/1000 | Loss: 0.00005122
Iteration 130/1000 | Loss: 0.00005122
Iteration 131/1000 | Loss: 0.00005122
Iteration 132/1000 | Loss: 0.00005121
Iteration 133/1000 | Loss: 0.00005121
Iteration 134/1000 | Loss: 0.00005121
Iteration 135/1000 | Loss: 0.00005121
Iteration 136/1000 | Loss: 0.00005121
Iteration 137/1000 | Loss: 0.00005121
Iteration 138/1000 | Loss: 0.00005121
Iteration 139/1000 | Loss: 0.00005121
Iteration 140/1000 | Loss: 0.00005121
Iteration 141/1000 | Loss: 0.00005120
Iteration 142/1000 | Loss: 0.00005120
Iteration 143/1000 | Loss: 0.00005120
Iteration 144/1000 | Loss: 0.00005120
Iteration 145/1000 | Loss: 0.00005120
Iteration 146/1000 | Loss: 0.00005120
Iteration 147/1000 | Loss: 0.00005119
Iteration 148/1000 | Loss: 0.00005119
Iteration 149/1000 | Loss: 0.00005119
Iteration 150/1000 | Loss: 0.00005119
Iteration 151/1000 | Loss: 0.00005119
Iteration 152/1000 | Loss: 0.00005119
Iteration 153/1000 | Loss: 0.00005119
Iteration 154/1000 | Loss: 0.00005119
Iteration 155/1000 | Loss: 0.00005119
Iteration 156/1000 | Loss: 0.00005119
Iteration 157/1000 | Loss: 0.00005119
Iteration 158/1000 | Loss: 0.00005118
Iteration 159/1000 | Loss: 0.00005118
Iteration 160/1000 | Loss: 0.00005118
Iteration 161/1000 | Loss: 0.00005118
Iteration 162/1000 | Loss: 0.00005118
Iteration 163/1000 | Loss: 0.00005118
Iteration 164/1000 | Loss: 0.00005118
Iteration 165/1000 | Loss: 0.00005118
Iteration 166/1000 | Loss: 0.00005118
Iteration 167/1000 | Loss: 0.00005118
Iteration 168/1000 | Loss: 0.00005118
Iteration 169/1000 | Loss: 0.00005118
Iteration 170/1000 | Loss: 0.00005118
Iteration 171/1000 | Loss: 0.00005118
Iteration 172/1000 | Loss: 0.00005118
Iteration 173/1000 | Loss: 0.00005118
Iteration 174/1000 | Loss: 0.00005118
Iteration 175/1000 | Loss: 0.00005118
Iteration 176/1000 | Loss: 0.00005118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [5.117863474879414e-05, 5.117863474879414e-05, 5.117863474879414e-05, 5.117863474879414e-05, 5.117863474879414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.117863474879414e-05

Optimization complete. Final v2v error: 5.614590644836426 mm

Highest mean error: 6.280830383300781 mm for frame 61

Lowest mean error: 4.119699954986572 mm for frame 7

Saving results

Total time: 58.40823769569397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434726
Iteration 2/25 | Loss: 0.00116982
Iteration 3/25 | Loss: 0.00099557
Iteration 4/25 | Loss: 0.00096762
Iteration 5/25 | Loss: 0.00096086
Iteration 6/25 | Loss: 0.00095948
Iteration 7/25 | Loss: 0.00095939
Iteration 8/25 | Loss: 0.00095939
Iteration 9/25 | Loss: 0.00095939
Iteration 10/25 | Loss: 0.00095939
Iteration 11/25 | Loss: 0.00095939
Iteration 12/25 | Loss: 0.00095939
Iteration 13/25 | Loss: 0.00095939
Iteration 14/25 | Loss: 0.00095939
Iteration 15/25 | Loss: 0.00095939
Iteration 16/25 | Loss: 0.00095939
Iteration 17/25 | Loss: 0.00095939
Iteration 18/25 | Loss: 0.00095939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009593928116373718, 0.0009593928116373718, 0.0009593928116373718, 0.0009593928116373718, 0.0009593928116373718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009593928116373718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35332990
Iteration 2/25 | Loss: 0.00067771
Iteration 3/25 | Loss: 0.00067770
Iteration 4/25 | Loss: 0.00067770
Iteration 5/25 | Loss: 0.00067770
Iteration 6/25 | Loss: 0.00067770
Iteration 7/25 | Loss: 0.00067770
Iteration 8/25 | Loss: 0.00067770
Iteration 9/25 | Loss: 0.00067770
Iteration 10/25 | Loss: 0.00067770
Iteration 11/25 | Loss: 0.00067770
Iteration 12/25 | Loss: 0.00067770
Iteration 13/25 | Loss: 0.00067770
Iteration 14/25 | Loss: 0.00067770
Iteration 15/25 | Loss: 0.00067770
Iteration 16/25 | Loss: 0.00067770
Iteration 17/25 | Loss: 0.00067770
Iteration 18/25 | Loss: 0.00067770
Iteration 19/25 | Loss: 0.00067770
Iteration 20/25 | Loss: 0.00067770
Iteration 21/25 | Loss: 0.00067770
Iteration 22/25 | Loss: 0.00067770
Iteration 23/25 | Loss: 0.00067770
Iteration 24/25 | Loss: 0.00067770
Iteration 25/25 | Loss: 0.00067770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006777007365599275, 0.0006777007365599275, 0.0006777007365599275, 0.0006777007365599275, 0.0006777007365599275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006777007365599275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067770
Iteration 2/1000 | Loss: 0.00003280
Iteration 3/1000 | Loss: 0.00001487
Iteration 4/1000 | Loss: 0.00001297
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001180
Iteration 7/1000 | Loss: 0.00001156
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001147
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001138
Iteration 14/1000 | Loss: 0.00001137
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001118
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001118
Iteration 23/1000 | Loss: 0.00001118
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001108
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001106
Iteration 31/1000 | Loss: 0.00001105
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001100
Iteration 37/1000 | Loss: 0.00001099
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001096
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001092
Iteration 54/1000 | Loss: 0.00001092
Iteration 55/1000 | Loss: 0.00001092
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001092
Iteration 58/1000 | Loss: 0.00001091
Iteration 59/1000 | Loss: 0.00001091
Iteration 60/1000 | Loss: 0.00001091
Iteration 61/1000 | Loss: 0.00001091
Iteration 62/1000 | Loss: 0.00001091
Iteration 63/1000 | Loss: 0.00001091
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001090
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001088
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001088
Iteration 74/1000 | Loss: 0.00001088
Iteration 75/1000 | Loss: 0.00001087
Iteration 76/1000 | Loss: 0.00001087
Iteration 77/1000 | Loss: 0.00001087
Iteration 78/1000 | Loss: 0.00001087
Iteration 79/1000 | Loss: 0.00001087
Iteration 80/1000 | Loss: 0.00001087
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001086
Iteration 83/1000 | Loss: 0.00001086
Iteration 84/1000 | Loss: 0.00001085
Iteration 85/1000 | Loss: 0.00001085
Iteration 86/1000 | Loss: 0.00001085
Iteration 87/1000 | Loss: 0.00001085
Iteration 88/1000 | Loss: 0.00001084
Iteration 89/1000 | Loss: 0.00001084
Iteration 90/1000 | Loss: 0.00001084
Iteration 91/1000 | Loss: 0.00001084
Iteration 92/1000 | Loss: 0.00001084
Iteration 93/1000 | Loss: 0.00001084
Iteration 94/1000 | Loss: 0.00001084
Iteration 95/1000 | Loss: 0.00001084
Iteration 96/1000 | Loss: 0.00001083
Iteration 97/1000 | Loss: 0.00001083
Iteration 98/1000 | Loss: 0.00001083
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001082
Iteration 102/1000 | Loss: 0.00001082
Iteration 103/1000 | Loss: 0.00001082
Iteration 104/1000 | Loss: 0.00001082
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001082
Iteration 108/1000 | Loss: 0.00001082
Iteration 109/1000 | Loss: 0.00001081
Iteration 110/1000 | Loss: 0.00001081
Iteration 111/1000 | Loss: 0.00001081
Iteration 112/1000 | Loss: 0.00001081
Iteration 113/1000 | Loss: 0.00001081
Iteration 114/1000 | Loss: 0.00001081
Iteration 115/1000 | Loss: 0.00001081
Iteration 116/1000 | Loss: 0.00001081
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001080
Iteration 126/1000 | Loss: 0.00001080
Iteration 127/1000 | Loss: 0.00001080
Iteration 128/1000 | Loss: 0.00001080
Iteration 129/1000 | Loss: 0.00001080
Iteration 130/1000 | Loss: 0.00001080
Iteration 131/1000 | Loss: 0.00001080
Iteration 132/1000 | Loss: 0.00001080
Iteration 133/1000 | Loss: 0.00001080
Iteration 134/1000 | Loss: 0.00001080
Iteration 135/1000 | Loss: 0.00001080
Iteration 136/1000 | Loss: 0.00001080
Iteration 137/1000 | Loss: 0.00001080
Iteration 138/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.07958776425221e-05, 1.07958776425221e-05, 1.07958776425221e-05, 1.07958776425221e-05, 1.07958776425221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.07958776425221e-05

Optimization complete. Final v2v error: 2.7700045108795166 mm

Highest mean error: 3.1348767280578613 mm for frame 19

Lowest mean error: 2.307401418685913 mm for frame 81

Saving results

Total time: 31.073837757110596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403091
Iteration 2/25 | Loss: 0.00102842
Iteration 3/25 | Loss: 0.00094333
Iteration 4/25 | Loss: 0.00092819
Iteration 5/25 | Loss: 0.00092217
Iteration 6/25 | Loss: 0.00092079
Iteration 7/25 | Loss: 0.00092063
Iteration 8/25 | Loss: 0.00092063
Iteration 9/25 | Loss: 0.00092063
Iteration 10/25 | Loss: 0.00092063
Iteration 11/25 | Loss: 0.00092063
Iteration 12/25 | Loss: 0.00092063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009206343675032258, 0.0009206343675032258, 0.0009206343675032258, 0.0009206343675032258, 0.0009206343675032258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009206343675032258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76439774
Iteration 2/25 | Loss: 0.00059640
Iteration 3/25 | Loss: 0.00059640
Iteration 4/25 | Loss: 0.00059640
Iteration 5/25 | Loss: 0.00059640
Iteration 6/25 | Loss: 0.00059640
Iteration 7/25 | Loss: 0.00059640
Iteration 8/25 | Loss: 0.00059640
Iteration 9/25 | Loss: 0.00059640
Iteration 10/25 | Loss: 0.00059640
Iteration 11/25 | Loss: 0.00059640
Iteration 12/25 | Loss: 0.00059640
Iteration 13/25 | Loss: 0.00059640
Iteration 14/25 | Loss: 0.00059640
Iteration 15/25 | Loss: 0.00059640
Iteration 16/25 | Loss: 0.00059640
Iteration 17/25 | Loss: 0.00059640
Iteration 18/25 | Loss: 0.00059640
Iteration 19/25 | Loss: 0.00059640
Iteration 20/25 | Loss: 0.00059640
Iteration 21/25 | Loss: 0.00059640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005964016309008002, 0.0005964016309008002, 0.0005964016309008002, 0.0005964016309008002, 0.0005964016309008002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005964016309008002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059640
Iteration 2/1000 | Loss: 0.00002903
Iteration 3/1000 | Loss: 0.00001697
Iteration 4/1000 | Loss: 0.00001298
Iteration 5/1000 | Loss: 0.00001220
Iteration 6/1000 | Loss: 0.00001165
Iteration 7/1000 | Loss: 0.00001117
Iteration 8/1000 | Loss: 0.00001089
Iteration 9/1000 | Loss: 0.00001070
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001058
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001057
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001056
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001053
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001049
Iteration 24/1000 | Loss: 0.00001048
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001046
Iteration 29/1000 | Loss: 0.00001046
Iteration 30/1000 | Loss: 0.00001045
Iteration 31/1000 | Loss: 0.00001045
Iteration 32/1000 | Loss: 0.00001044
Iteration 33/1000 | Loss: 0.00001044
Iteration 34/1000 | Loss: 0.00001044
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001041
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001041
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001037
Iteration 55/1000 | Loss: 0.00001037
Iteration 56/1000 | Loss: 0.00001037
Iteration 57/1000 | Loss: 0.00001037
Iteration 58/1000 | Loss: 0.00001037
Iteration 59/1000 | Loss: 0.00001036
Iteration 60/1000 | Loss: 0.00001036
Iteration 61/1000 | Loss: 0.00001036
Iteration 62/1000 | Loss: 0.00001036
Iteration 63/1000 | Loss: 0.00001036
Iteration 64/1000 | Loss: 0.00001036
Iteration 65/1000 | Loss: 0.00001036
Iteration 66/1000 | Loss: 0.00001036
Iteration 67/1000 | Loss: 0.00001036
Iteration 68/1000 | Loss: 0.00001036
Iteration 69/1000 | Loss: 0.00001036
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001035
Iteration 72/1000 | Loss: 0.00001035
Iteration 73/1000 | Loss: 0.00001035
Iteration 74/1000 | Loss: 0.00001035
Iteration 75/1000 | Loss: 0.00001035
Iteration 76/1000 | Loss: 0.00001035
Iteration 77/1000 | Loss: 0.00001035
Iteration 78/1000 | Loss: 0.00001035
Iteration 79/1000 | Loss: 0.00001034
Iteration 80/1000 | Loss: 0.00001034
Iteration 81/1000 | Loss: 0.00001034
Iteration 82/1000 | Loss: 0.00001034
Iteration 83/1000 | Loss: 0.00001034
Iteration 84/1000 | Loss: 0.00001034
Iteration 85/1000 | Loss: 0.00001034
Iteration 86/1000 | Loss: 0.00001034
Iteration 87/1000 | Loss: 0.00001034
Iteration 88/1000 | Loss: 0.00001034
Iteration 89/1000 | Loss: 0.00001034
Iteration 90/1000 | Loss: 0.00001034
Iteration 91/1000 | Loss: 0.00001034
Iteration 92/1000 | Loss: 0.00001034
Iteration 93/1000 | Loss: 0.00001034
Iteration 94/1000 | Loss: 0.00001034
Iteration 95/1000 | Loss: 0.00001034
Iteration 96/1000 | Loss: 0.00001034
Iteration 97/1000 | Loss: 0.00001034
Iteration 98/1000 | Loss: 0.00001034
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001034
Iteration 101/1000 | Loss: 0.00001034
Iteration 102/1000 | Loss: 0.00001034
Iteration 103/1000 | Loss: 0.00001034
Iteration 104/1000 | Loss: 0.00001034
Iteration 105/1000 | Loss: 0.00001034
Iteration 106/1000 | Loss: 0.00001034
Iteration 107/1000 | Loss: 0.00001034
Iteration 108/1000 | Loss: 0.00001034
Iteration 109/1000 | Loss: 0.00001034
Iteration 110/1000 | Loss: 0.00001034
Iteration 111/1000 | Loss: 0.00001034
Iteration 112/1000 | Loss: 0.00001034
Iteration 113/1000 | Loss: 0.00001034
Iteration 114/1000 | Loss: 0.00001034
Iteration 115/1000 | Loss: 0.00001034
Iteration 116/1000 | Loss: 0.00001034
Iteration 117/1000 | Loss: 0.00001034
Iteration 118/1000 | Loss: 0.00001034
Iteration 119/1000 | Loss: 0.00001034
Iteration 120/1000 | Loss: 0.00001034
Iteration 121/1000 | Loss: 0.00001034
Iteration 122/1000 | Loss: 0.00001034
Iteration 123/1000 | Loss: 0.00001034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.0342489986214787e-05, 1.0342489986214787e-05, 1.0342489986214787e-05, 1.0342489986214787e-05, 1.0342489986214787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0342489986214787e-05

Optimization complete. Final v2v error: 2.7118759155273438 mm

Highest mean error: 3.5117461681365967 mm for frame 63

Lowest mean error: 2.432051658630371 mm for frame 79

Saving results

Total time: 29.286513805389404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073460
Iteration 2/25 | Loss: 0.00194626
Iteration 3/25 | Loss: 0.00134102
Iteration 4/25 | Loss: 0.00118952
Iteration 5/25 | Loss: 0.00112128
Iteration 6/25 | Loss: 0.00110534
Iteration 7/25 | Loss: 0.00110968
Iteration 8/25 | Loss: 0.00106114
Iteration 9/25 | Loss: 0.00102892
Iteration 10/25 | Loss: 0.00100712
Iteration 11/25 | Loss: 0.00100482
Iteration 12/25 | Loss: 0.00099460
Iteration 13/25 | Loss: 0.00099240
Iteration 14/25 | Loss: 0.00099757
Iteration 15/25 | Loss: 0.00098638
Iteration 16/25 | Loss: 0.00099306
Iteration 17/25 | Loss: 0.00099262
Iteration 18/25 | Loss: 0.00097830
Iteration 19/25 | Loss: 0.00098098
Iteration 20/25 | Loss: 0.00098553
Iteration 21/25 | Loss: 0.00099173
Iteration 22/25 | Loss: 0.00098924
Iteration 23/25 | Loss: 0.00098366
Iteration 24/25 | Loss: 0.00098429
Iteration 25/25 | Loss: 0.00099114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36975658
Iteration 2/25 | Loss: 0.00172240
Iteration 3/25 | Loss: 0.00099652
Iteration 4/25 | Loss: 0.00099652
Iteration 5/25 | Loss: 0.00099651
Iteration 6/25 | Loss: 0.00099651
Iteration 7/25 | Loss: 0.00099651
Iteration 8/25 | Loss: 0.00099651
Iteration 9/25 | Loss: 0.00099651
Iteration 10/25 | Loss: 0.00099651
Iteration 11/25 | Loss: 0.00099651
Iteration 12/25 | Loss: 0.00099651
Iteration 13/25 | Loss: 0.00099651
Iteration 14/25 | Loss: 0.00099651
Iteration 15/25 | Loss: 0.00099651
Iteration 16/25 | Loss: 0.00099651
Iteration 17/25 | Loss: 0.00099651
Iteration 18/25 | Loss: 0.00099651
Iteration 19/25 | Loss: 0.00099651
Iteration 20/25 | Loss: 0.00099651
Iteration 21/25 | Loss: 0.00099651
Iteration 22/25 | Loss: 0.00099651
Iteration 23/25 | Loss: 0.00099651
Iteration 24/25 | Loss: 0.00099651
Iteration 25/25 | Loss: 0.00099651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099651
Iteration 2/1000 | Loss: 0.00118768
Iteration 3/1000 | Loss: 0.00084428
Iteration 4/1000 | Loss: 0.00086056
Iteration 5/1000 | Loss: 0.00023124
Iteration 6/1000 | Loss: 0.00023988
Iteration 7/1000 | Loss: 0.00010855
Iteration 8/1000 | Loss: 0.00017459
Iteration 9/1000 | Loss: 0.00054411
Iteration 10/1000 | Loss: 0.00053655
Iteration 11/1000 | Loss: 0.00039064
Iteration 12/1000 | Loss: 0.00049819
Iteration 13/1000 | Loss: 0.00059157
Iteration 14/1000 | Loss: 0.00054346
Iteration 15/1000 | Loss: 0.00046673
Iteration 16/1000 | Loss: 0.00059288
Iteration 17/1000 | Loss: 0.00042756
Iteration 18/1000 | Loss: 0.00032690
Iteration 19/1000 | Loss: 0.00046687
Iteration 20/1000 | Loss: 0.00026463
Iteration 21/1000 | Loss: 0.00017073
Iteration 22/1000 | Loss: 0.00054470
Iteration 23/1000 | Loss: 0.00008322
Iteration 24/1000 | Loss: 0.00009923
Iteration 25/1000 | Loss: 0.00016767
Iteration 26/1000 | Loss: 0.00026481
Iteration 27/1000 | Loss: 0.00031991
Iteration 28/1000 | Loss: 0.00026165
Iteration 29/1000 | Loss: 0.00030787
Iteration 30/1000 | Loss: 0.00031647
Iteration 31/1000 | Loss: 0.00030146
Iteration 32/1000 | Loss: 0.00039824
Iteration 33/1000 | Loss: 0.00049868
Iteration 34/1000 | Loss: 0.00032786
Iteration 35/1000 | Loss: 0.00058984
Iteration 36/1000 | Loss: 0.00036881
Iteration 37/1000 | Loss: 0.00036826
Iteration 38/1000 | Loss: 0.00037036
Iteration 39/1000 | Loss: 0.00035342
Iteration 40/1000 | Loss: 0.00039261
Iteration 41/1000 | Loss: 0.00091763
Iteration 42/1000 | Loss: 0.00117400
Iteration 43/1000 | Loss: 0.00037618
Iteration 44/1000 | Loss: 0.00048887
Iteration 45/1000 | Loss: 0.00034818
Iteration 46/1000 | Loss: 0.00018929
Iteration 47/1000 | Loss: 0.00035137
Iteration 48/1000 | Loss: 0.00031767
Iteration 49/1000 | Loss: 0.00028218
Iteration 50/1000 | Loss: 0.00027425
Iteration 51/1000 | Loss: 0.00026237
Iteration 52/1000 | Loss: 0.00011697
Iteration 53/1000 | Loss: 0.00034287
Iteration 54/1000 | Loss: 0.00031037
Iteration 55/1000 | Loss: 0.00030178
Iteration 56/1000 | Loss: 0.00025369
Iteration 57/1000 | Loss: 0.00022993
Iteration 58/1000 | Loss: 0.00015184
Iteration 59/1000 | Loss: 0.00003409
Iteration 60/1000 | Loss: 0.00011620
Iteration 61/1000 | Loss: 0.00010648
Iteration 62/1000 | Loss: 0.00009252
Iteration 63/1000 | Loss: 0.00046852
Iteration 64/1000 | Loss: 0.00015326
Iteration 65/1000 | Loss: 0.00017825
Iteration 66/1000 | Loss: 0.00023367
Iteration 67/1000 | Loss: 0.00007078
Iteration 68/1000 | Loss: 0.00003570
Iteration 69/1000 | Loss: 0.00026862
Iteration 70/1000 | Loss: 0.00011329
Iteration 71/1000 | Loss: 0.00020214
Iteration 72/1000 | Loss: 0.00006767
Iteration 73/1000 | Loss: 0.00025348
Iteration 74/1000 | Loss: 0.00008295
Iteration 75/1000 | Loss: 0.00035254
Iteration 76/1000 | Loss: 0.00025814
Iteration 77/1000 | Loss: 0.00020086
Iteration 78/1000 | Loss: 0.00006316
Iteration 79/1000 | Loss: 0.00023806
Iteration 80/1000 | Loss: 0.00015970
Iteration 81/1000 | Loss: 0.00041158
Iteration 82/1000 | Loss: 0.00016094
Iteration 83/1000 | Loss: 0.00016565
Iteration 84/1000 | Loss: 0.00031029
Iteration 85/1000 | Loss: 0.00016013
Iteration 86/1000 | Loss: 0.00026849
Iteration 87/1000 | Loss: 0.00019118
Iteration 88/1000 | Loss: 0.00019162
Iteration 89/1000 | Loss: 0.00021874
Iteration 90/1000 | Loss: 0.00044454
Iteration 91/1000 | Loss: 0.00013495
Iteration 92/1000 | Loss: 0.00008939
Iteration 93/1000 | Loss: 0.00020775
Iteration 94/1000 | Loss: 0.00018676
Iteration 95/1000 | Loss: 0.00012078
Iteration 96/1000 | Loss: 0.00011811
Iteration 97/1000 | Loss: 0.00029947
Iteration 98/1000 | Loss: 0.00020116
Iteration 99/1000 | Loss: 0.00033914
Iteration 100/1000 | Loss: 0.00054492
Iteration 101/1000 | Loss: 0.00004285
Iteration 102/1000 | Loss: 0.00007424
Iteration 103/1000 | Loss: 0.00011553
Iteration 104/1000 | Loss: 0.00008594
Iteration 105/1000 | Loss: 0.00007373
Iteration 106/1000 | Loss: 0.00018915
Iteration 107/1000 | Loss: 0.00018685
Iteration 108/1000 | Loss: 0.00004872
Iteration 109/1000 | Loss: 0.00005047
Iteration 110/1000 | Loss: 0.00008869
Iteration 111/1000 | Loss: 0.00025132
Iteration 112/1000 | Loss: 0.00020890
Iteration 113/1000 | Loss: 0.00020270
Iteration 114/1000 | Loss: 0.00022753
Iteration 115/1000 | Loss: 0.00122192
Iteration 116/1000 | Loss: 0.00092281
Iteration 117/1000 | Loss: 0.00097805
Iteration 118/1000 | Loss: 0.00041259
Iteration 119/1000 | Loss: 0.00022657
Iteration 120/1000 | Loss: 0.00028029
Iteration 121/1000 | Loss: 0.00005380
Iteration 122/1000 | Loss: 0.00010865
Iteration 123/1000 | Loss: 0.00023090
Iteration 124/1000 | Loss: 0.00022940
Iteration 125/1000 | Loss: 0.00022575
Iteration 126/1000 | Loss: 0.00008984
Iteration 127/1000 | Loss: 0.00020324
Iteration 128/1000 | Loss: 0.00022209
Iteration 129/1000 | Loss: 0.00019572
Iteration 130/1000 | Loss: 0.00011884
Iteration 131/1000 | Loss: 0.00022025
Iteration 132/1000 | Loss: 0.00034764
Iteration 133/1000 | Loss: 0.00018406
Iteration 134/1000 | Loss: 0.00021080
Iteration 135/1000 | Loss: 0.00014806
Iteration 136/1000 | Loss: 0.00017851
Iteration 137/1000 | Loss: 0.00005180
Iteration 138/1000 | Loss: 0.00001942
Iteration 139/1000 | Loss: 0.00001671
Iteration 140/1000 | Loss: 0.00004388
Iteration 141/1000 | Loss: 0.00003371
Iteration 142/1000 | Loss: 0.00005222
Iteration 143/1000 | Loss: 0.00003072
Iteration 144/1000 | Loss: 0.00002969
Iteration 145/1000 | Loss: 0.00002864
Iteration 146/1000 | Loss: 0.00003446
Iteration 147/1000 | Loss: 0.00003186
Iteration 148/1000 | Loss: 0.00003273
Iteration 149/1000 | Loss: 0.00004542
Iteration 150/1000 | Loss: 0.00005507
Iteration 151/1000 | Loss: 0.00005177
Iteration 152/1000 | Loss: 0.00002840
Iteration 153/1000 | Loss: 0.00003339
Iteration 154/1000 | Loss: 0.00003343
Iteration 155/1000 | Loss: 0.00004065
Iteration 156/1000 | Loss: 0.00002925
Iteration 157/1000 | Loss: 0.00003051
Iteration 158/1000 | Loss: 0.00003026
Iteration 159/1000 | Loss: 0.00002237
Iteration 160/1000 | Loss: 0.00003390
Iteration 161/1000 | Loss: 0.00003708
Iteration 162/1000 | Loss: 0.00003862
Iteration 163/1000 | Loss: 0.00002725
Iteration 164/1000 | Loss: 0.00003887
Iteration 165/1000 | Loss: 0.00003722
Iteration 166/1000 | Loss: 0.00004919
Iteration 167/1000 | Loss: 0.00002735
Iteration 168/1000 | Loss: 0.00002816
Iteration 169/1000 | Loss: 0.00002705
Iteration 170/1000 | Loss: 0.00003744
Iteration 171/1000 | Loss: 0.00002891
Iteration 172/1000 | Loss: 0.00004179
Iteration 173/1000 | Loss: 0.00003361
Iteration 174/1000 | Loss: 0.00004102
Iteration 175/1000 | Loss: 0.00003810
Iteration 176/1000 | Loss: 0.00004708
Iteration 177/1000 | Loss: 0.00003792
Iteration 178/1000 | Loss: 0.00004823
Iteration 179/1000 | Loss: 0.00003795
Iteration 180/1000 | Loss: 0.00004124
Iteration 181/1000 | Loss: 0.00002305
Iteration 182/1000 | Loss: 0.00004238
Iteration 183/1000 | Loss: 0.00003383
Iteration 184/1000 | Loss: 0.00004026
Iteration 185/1000 | Loss: 0.00003540
Iteration 186/1000 | Loss: 0.00004377
Iteration 187/1000 | Loss: 0.00004153
Iteration 188/1000 | Loss: 0.00004935
Iteration 189/1000 | Loss: 0.00003903
Iteration 190/1000 | Loss: 0.00004264
Iteration 191/1000 | Loss: 0.00003578
Iteration 192/1000 | Loss: 0.00004575
Iteration 193/1000 | Loss: 0.00003636
Iteration 194/1000 | Loss: 0.00004452
Iteration 195/1000 | Loss: 0.00003846
Iteration 196/1000 | Loss: 0.00004418
Iteration 197/1000 | Loss: 0.00004227
Iteration 198/1000 | Loss: 0.00005049
Iteration 199/1000 | Loss: 0.00004204
Iteration 200/1000 | Loss: 0.00004156
Iteration 201/1000 | Loss: 0.00004005
Iteration 202/1000 | Loss: 0.00004221
Iteration 203/1000 | Loss: 0.00004670
Iteration 204/1000 | Loss: 0.00003473
Iteration 205/1000 | Loss: 0.00003616
Iteration 206/1000 | Loss: 0.00002878
Iteration 207/1000 | Loss: 0.00004070
Iteration 208/1000 | Loss: 0.00003832
Iteration 209/1000 | Loss: 0.00003092
Iteration 210/1000 | Loss: 0.00004174
Iteration 211/1000 | Loss: 0.00004052
Iteration 212/1000 | Loss: 0.00003783
Iteration 213/1000 | Loss: 0.00003154
Iteration 214/1000 | Loss: 0.00005286
Iteration 215/1000 | Loss: 0.00004417
Iteration 216/1000 | Loss: 0.00003186
Iteration 217/1000 | Loss: 0.00005076
Iteration 218/1000 | Loss: 0.00004041
Iteration 219/1000 | Loss: 0.00004150
Iteration 220/1000 | Loss: 0.00004381
Iteration 221/1000 | Loss: 0.00004023
Iteration 222/1000 | Loss: 0.00004356
Iteration 223/1000 | Loss: 0.00003929
Iteration 224/1000 | Loss: 0.00004057
Iteration 225/1000 | Loss: 0.00004012
Iteration 226/1000 | Loss: 0.00004145
Iteration 227/1000 | Loss: 0.00003324
Iteration 228/1000 | Loss: 0.00003802
Iteration 229/1000 | Loss: 0.00006266
Iteration 230/1000 | Loss: 0.00028161
Iteration 231/1000 | Loss: 0.00020287
Iteration 232/1000 | Loss: 0.00001404
Iteration 233/1000 | Loss: 0.00021597
Iteration 234/1000 | Loss: 0.00002284
Iteration 235/1000 | Loss: 0.00001484
Iteration 236/1000 | Loss: 0.00001311
Iteration 237/1000 | Loss: 0.00001196
Iteration 238/1000 | Loss: 0.00001103
Iteration 239/1000 | Loss: 0.00001047
Iteration 240/1000 | Loss: 0.00001025
Iteration 241/1000 | Loss: 0.00001007
Iteration 242/1000 | Loss: 0.00001006
Iteration 243/1000 | Loss: 0.00001005
Iteration 244/1000 | Loss: 0.00001001
Iteration 245/1000 | Loss: 0.00001000
Iteration 246/1000 | Loss: 0.00000998
Iteration 247/1000 | Loss: 0.00000997
Iteration 248/1000 | Loss: 0.00000996
Iteration 249/1000 | Loss: 0.00000995
Iteration 250/1000 | Loss: 0.00000995
Iteration 251/1000 | Loss: 0.00000990
Iteration 252/1000 | Loss: 0.00000986
Iteration 253/1000 | Loss: 0.00000985
Iteration 254/1000 | Loss: 0.00000979
Iteration 255/1000 | Loss: 0.00000975
Iteration 256/1000 | Loss: 0.00000967
Iteration 257/1000 | Loss: 0.00000960
Iteration 258/1000 | Loss: 0.00000960
Iteration 259/1000 | Loss: 0.00000959
Iteration 260/1000 | Loss: 0.00000959
Iteration 261/1000 | Loss: 0.00000959
Iteration 262/1000 | Loss: 0.00000959
Iteration 263/1000 | Loss: 0.00000958
Iteration 264/1000 | Loss: 0.00000958
Iteration 265/1000 | Loss: 0.00000958
Iteration 266/1000 | Loss: 0.00000958
Iteration 267/1000 | Loss: 0.00000958
Iteration 268/1000 | Loss: 0.00000958
Iteration 269/1000 | Loss: 0.00000958
Iteration 270/1000 | Loss: 0.00000958
Iteration 271/1000 | Loss: 0.00000958
Iteration 272/1000 | Loss: 0.00000958
Iteration 273/1000 | Loss: 0.00000958
Iteration 274/1000 | Loss: 0.00000957
Iteration 275/1000 | Loss: 0.00000957
Iteration 276/1000 | Loss: 0.00000956
Iteration 277/1000 | Loss: 0.00000956
Iteration 278/1000 | Loss: 0.00000956
Iteration 279/1000 | Loss: 0.00000955
Iteration 280/1000 | Loss: 0.00000955
Iteration 281/1000 | Loss: 0.00000954
Iteration 282/1000 | Loss: 0.00000954
Iteration 283/1000 | Loss: 0.00000954
Iteration 284/1000 | Loss: 0.00000954
Iteration 285/1000 | Loss: 0.00000953
Iteration 286/1000 | Loss: 0.00000953
Iteration 287/1000 | Loss: 0.00000953
Iteration 288/1000 | Loss: 0.00000953
Iteration 289/1000 | Loss: 0.00000953
Iteration 290/1000 | Loss: 0.00000952
Iteration 291/1000 | Loss: 0.00000952
Iteration 292/1000 | Loss: 0.00000952
Iteration 293/1000 | Loss: 0.00000951
Iteration 294/1000 | Loss: 0.00000951
Iteration 295/1000 | Loss: 0.00000951
Iteration 296/1000 | Loss: 0.00000951
Iteration 297/1000 | Loss: 0.00000951
Iteration 298/1000 | Loss: 0.00000951
Iteration 299/1000 | Loss: 0.00000950
Iteration 300/1000 | Loss: 0.00000950
Iteration 301/1000 | Loss: 0.00000950
Iteration 302/1000 | Loss: 0.00000950
Iteration 303/1000 | Loss: 0.00000950
Iteration 304/1000 | Loss: 0.00000950
Iteration 305/1000 | Loss: 0.00000950
Iteration 306/1000 | Loss: 0.00000950
Iteration 307/1000 | Loss: 0.00000950
Iteration 308/1000 | Loss: 0.00000949
Iteration 309/1000 | Loss: 0.00000949
Iteration 310/1000 | Loss: 0.00000949
Iteration 311/1000 | Loss: 0.00000949
Iteration 312/1000 | Loss: 0.00000949
Iteration 313/1000 | Loss: 0.00000949
Iteration 314/1000 | Loss: 0.00000949
Iteration 315/1000 | Loss: 0.00000949
Iteration 316/1000 | Loss: 0.00000949
Iteration 317/1000 | Loss: 0.00000949
Iteration 318/1000 | Loss: 0.00000949
Iteration 319/1000 | Loss: 0.00000948
Iteration 320/1000 | Loss: 0.00000948
Iteration 321/1000 | Loss: 0.00000948
Iteration 322/1000 | Loss: 0.00000948
Iteration 323/1000 | Loss: 0.00000948
Iteration 324/1000 | Loss: 0.00000948
Iteration 325/1000 | Loss: 0.00000948
Iteration 326/1000 | Loss: 0.00000948
Iteration 327/1000 | Loss: 0.00000948
Iteration 328/1000 | Loss: 0.00000948
Iteration 329/1000 | Loss: 0.00000948
Iteration 330/1000 | Loss: 0.00000948
Iteration 331/1000 | Loss: 0.00000948
Iteration 332/1000 | Loss: 0.00000948
Iteration 333/1000 | Loss: 0.00000948
Iteration 334/1000 | Loss: 0.00000948
Iteration 335/1000 | Loss: 0.00000948
Iteration 336/1000 | Loss: 0.00000948
Iteration 337/1000 | Loss: 0.00000948
Iteration 338/1000 | Loss: 0.00000948
Iteration 339/1000 | Loss: 0.00000948
Iteration 340/1000 | Loss: 0.00000948
Iteration 341/1000 | Loss: 0.00000948
Iteration 342/1000 | Loss: 0.00000948
Iteration 343/1000 | Loss: 0.00000948
Iteration 344/1000 | Loss: 0.00000948
Iteration 345/1000 | Loss: 0.00000948
Iteration 346/1000 | Loss: 0.00000948
Iteration 347/1000 | Loss: 0.00000948
Iteration 348/1000 | Loss: 0.00000948
Iteration 349/1000 | Loss: 0.00000948
Iteration 350/1000 | Loss: 0.00000948
Iteration 351/1000 | Loss: 0.00000948
Iteration 352/1000 | Loss: 0.00000948
Iteration 353/1000 | Loss: 0.00000948
Iteration 354/1000 | Loss: 0.00000948
Iteration 355/1000 | Loss: 0.00000948
Iteration 356/1000 | Loss: 0.00000948
Iteration 357/1000 | Loss: 0.00000948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [9.476407285546884e-06, 9.476407285546884e-06, 9.476407285546884e-06, 9.476407285546884e-06, 9.476407285546884e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.476407285546884e-06

Optimization complete. Final v2v error: 2.595207452774048 mm

Highest mean error: 3.5780928134918213 mm for frame 23

Lowest mean error: 2.258957624435425 mm for frame 121

Saving results

Total time: 376.007150888443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649835
Iteration 2/25 | Loss: 0.00155618
Iteration 3/25 | Loss: 0.00110834
Iteration 4/25 | Loss: 0.00104496
Iteration 5/25 | Loss: 0.00103103
Iteration 6/25 | Loss: 0.00101436
Iteration 7/25 | Loss: 0.00100437
Iteration 8/25 | Loss: 0.00100126
Iteration 9/25 | Loss: 0.00099473
Iteration 10/25 | Loss: 0.00098718
Iteration 11/25 | Loss: 0.00098791
Iteration 12/25 | Loss: 0.00098330
Iteration 13/25 | Loss: 0.00098091
Iteration 14/25 | Loss: 0.00098280
Iteration 15/25 | Loss: 0.00098185
Iteration 16/25 | Loss: 0.00098465
Iteration 17/25 | Loss: 0.00098151
Iteration 18/25 | Loss: 0.00097908
Iteration 19/25 | Loss: 0.00097910
Iteration 20/25 | Loss: 0.00097890
Iteration 21/25 | Loss: 0.00097905
Iteration 22/25 | Loss: 0.00097914
Iteration 23/25 | Loss: 0.00097876
Iteration 24/25 | Loss: 0.00097891
Iteration 25/25 | Loss: 0.00097880

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71139550
Iteration 2/25 | Loss: 0.00095109
Iteration 3/25 | Loss: 0.00086539
Iteration 4/25 | Loss: 0.00086538
Iteration 5/25 | Loss: 0.00086538
Iteration 6/25 | Loss: 0.00086538
Iteration 7/25 | Loss: 0.00086538
Iteration 8/25 | Loss: 0.00086538
Iteration 9/25 | Loss: 0.00086538
Iteration 10/25 | Loss: 0.00086538
Iteration 11/25 | Loss: 0.00086538
Iteration 12/25 | Loss: 0.00086538
Iteration 13/25 | Loss: 0.00086538
Iteration 14/25 | Loss: 0.00086538
Iteration 15/25 | Loss: 0.00086538
Iteration 16/25 | Loss: 0.00086538
Iteration 17/25 | Loss: 0.00086538
Iteration 18/25 | Loss: 0.00086538
Iteration 19/25 | Loss: 0.00086538
Iteration 20/25 | Loss: 0.00086538
Iteration 21/25 | Loss: 0.00086538
Iteration 22/25 | Loss: 0.00086538
Iteration 23/25 | Loss: 0.00086538
Iteration 24/25 | Loss: 0.00086538
Iteration 25/25 | Loss: 0.00086538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086538
Iteration 2/1000 | Loss: 0.00013808
Iteration 3/1000 | Loss: 0.00004123
Iteration 4/1000 | Loss: 0.00002611
Iteration 5/1000 | Loss: 0.00002862
Iteration 6/1000 | Loss: 0.00002690
Iteration 7/1000 | Loss: 0.00004456
Iteration 8/1000 | Loss: 0.00002946
Iteration 9/1000 | Loss: 0.00001800
Iteration 10/1000 | Loss: 0.00002866
Iteration 11/1000 | Loss: 0.00003786
Iteration 12/1000 | Loss: 0.00002932
Iteration 13/1000 | Loss: 0.00003018
Iteration 14/1000 | Loss: 0.00002880
Iteration 15/1000 | Loss: 0.00004134
Iteration 16/1000 | Loss: 0.00002226
Iteration 17/1000 | Loss: 0.00002033
Iteration 18/1000 | Loss: 0.00003901
Iteration 19/1000 | Loss: 0.00056052
Iteration 20/1000 | Loss: 0.00005349
Iteration 21/1000 | Loss: 0.00003392
Iteration 22/1000 | Loss: 0.00005969
Iteration 23/1000 | Loss: 0.00003368
Iteration 24/1000 | Loss: 0.00003605
Iteration 25/1000 | Loss: 0.00003008
Iteration 26/1000 | Loss: 0.00002885
Iteration 27/1000 | Loss: 0.00003618
Iteration 28/1000 | Loss: 0.00002602
Iteration 29/1000 | Loss: 0.00002990
Iteration 30/1000 | Loss: 0.00002688
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00002132
Iteration 33/1000 | Loss: 0.00002482
Iteration 34/1000 | Loss: 0.00002152
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00002085
Iteration 37/1000 | Loss: 0.00001973
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001604
Iteration 40/1000 | Loss: 0.00002004
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00002037
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001583
Iteration 45/1000 | Loss: 0.00002335
Iteration 46/1000 | Loss: 0.00001521
Iteration 47/1000 | Loss: 0.00004280
Iteration 48/1000 | Loss: 0.00006768
Iteration 49/1000 | Loss: 0.00014040
Iteration 50/1000 | Loss: 0.00006061
Iteration 51/1000 | Loss: 0.00010017
Iteration 52/1000 | Loss: 0.00009044
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001820
Iteration 55/1000 | Loss: 0.00005703
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00003645
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00011548
Iteration 67/1000 | Loss: 0.00011548
Iteration 68/1000 | Loss: 0.00009332
Iteration 69/1000 | Loss: 0.00003527
Iteration 70/1000 | Loss: 0.00002537
Iteration 71/1000 | Loss: 0.00001840
Iteration 72/1000 | Loss: 0.00001602
Iteration 73/1000 | Loss: 0.00003163
Iteration 74/1000 | Loss: 0.00002712
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001822
Iteration 77/1000 | Loss: 0.00001822
Iteration 78/1000 | Loss: 0.00007501
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001522
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001509
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001491
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001484
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001483
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001476
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001473
Iteration 107/1000 | Loss: 0.00001473
Iteration 108/1000 | Loss: 0.00001473
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001467
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001466
Iteration 125/1000 | Loss: 0.00001466
Iteration 126/1000 | Loss: 0.00001466
Iteration 127/1000 | Loss: 0.00001466
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001466
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001466
Iteration 135/1000 | Loss: 0.00001466
Iteration 136/1000 | Loss: 0.00001466
Iteration 137/1000 | Loss: 0.00001466
Iteration 138/1000 | Loss: 0.00001466
Iteration 139/1000 | Loss: 0.00001466
Iteration 140/1000 | Loss: 0.00001466
Iteration 141/1000 | Loss: 0.00001466
Iteration 142/1000 | Loss: 0.00001466
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001466
Iteration 147/1000 | Loss: 0.00001466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.4659493899671361e-05, 1.4659493899671361e-05, 1.4659493899671361e-05, 1.4659493899671361e-05, 1.4659493899671361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4659493899671361e-05

Optimization complete. Final v2v error: 3.0649025440216064 mm

Highest mean error: 9.670700073242188 mm for frame 65

Lowest mean error: 2.281677484512329 mm for frame 210

Saving results

Total time: 176.91229557991028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452931
Iteration 2/25 | Loss: 0.00102511
Iteration 3/25 | Loss: 0.00093695
Iteration 4/25 | Loss: 0.00092874
Iteration 5/25 | Loss: 0.00092682
Iteration 6/25 | Loss: 0.00092648
Iteration 7/25 | Loss: 0.00092648
Iteration 8/25 | Loss: 0.00092648
Iteration 9/25 | Loss: 0.00092648
Iteration 10/25 | Loss: 0.00092648
Iteration 11/25 | Loss: 0.00092648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009264758555218577, 0.0009264758555218577, 0.0009264758555218577, 0.0009264758555218577, 0.0009264758555218577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009264758555218577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28117967
Iteration 2/25 | Loss: 0.00053829
Iteration 3/25 | Loss: 0.00053828
Iteration 4/25 | Loss: 0.00053828
Iteration 5/25 | Loss: 0.00053828
Iteration 6/25 | Loss: 0.00053828
Iteration 7/25 | Loss: 0.00053828
Iteration 8/25 | Loss: 0.00053828
Iteration 9/25 | Loss: 0.00053828
Iteration 10/25 | Loss: 0.00053828
Iteration 11/25 | Loss: 0.00053828
Iteration 12/25 | Loss: 0.00053828
Iteration 13/25 | Loss: 0.00053828
Iteration 14/25 | Loss: 0.00053828
Iteration 15/25 | Loss: 0.00053828
Iteration 16/25 | Loss: 0.00053828
Iteration 17/25 | Loss: 0.00053828
Iteration 18/25 | Loss: 0.00053828
Iteration 19/25 | Loss: 0.00053828
Iteration 20/25 | Loss: 0.00053828
Iteration 21/25 | Loss: 0.00053828
Iteration 22/25 | Loss: 0.00053828
Iteration 23/25 | Loss: 0.00053828
Iteration 24/25 | Loss: 0.00053828
Iteration 25/25 | Loss: 0.00053828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053828
Iteration 2/1000 | Loss: 0.00001950
Iteration 3/1000 | Loss: 0.00001333
Iteration 4/1000 | Loss: 0.00001156
Iteration 5/1000 | Loss: 0.00001056
Iteration 6/1000 | Loss: 0.00000998
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000958
Iteration 9/1000 | Loss: 0.00000947
Iteration 10/1000 | Loss: 0.00000935
Iteration 11/1000 | Loss: 0.00000933
Iteration 12/1000 | Loss: 0.00000928
Iteration 13/1000 | Loss: 0.00000927
Iteration 14/1000 | Loss: 0.00000927
Iteration 15/1000 | Loss: 0.00000926
Iteration 16/1000 | Loss: 0.00000926
Iteration 17/1000 | Loss: 0.00000922
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000918
Iteration 21/1000 | Loss: 0.00000918
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000917
Iteration 24/1000 | Loss: 0.00000914
Iteration 25/1000 | Loss: 0.00000912
Iteration 26/1000 | Loss: 0.00000911
Iteration 27/1000 | Loss: 0.00000911
Iteration 28/1000 | Loss: 0.00000911
Iteration 29/1000 | Loss: 0.00000907
Iteration 30/1000 | Loss: 0.00000905
Iteration 31/1000 | Loss: 0.00000905
Iteration 32/1000 | Loss: 0.00000904
Iteration 33/1000 | Loss: 0.00000903
Iteration 34/1000 | Loss: 0.00000901
Iteration 35/1000 | Loss: 0.00000901
Iteration 36/1000 | Loss: 0.00000900
Iteration 37/1000 | Loss: 0.00000900
Iteration 38/1000 | Loss: 0.00000900
Iteration 39/1000 | Loss: 0.00000900
Iteration 40/1000 | Loss: 0.00000899
Iteration 41/1000 | Loss: 0.00000899
Iteration 42/1000 | Loss: 0.00000899
Iteration 43/1000 | Loss: 0.00000898
Iteration 44/1000 | Loss: 0.00000898
Iteration 45/1000 | Loss: 0.00000898
Iteration 46/1000 | Loss: 0.00000898
Iteration 47/1000 | Loss: 0.00000897
Iteration 48/1000 | Loss: 0.00000896
Iteration 49/1000 | Loss: 0.00000896
Iteration 50/1000 | Loss: 0.00000896
Iteration 51/1000 | Loss: 0.00000896
Iteration 52/1000 | Loss: 0.00000895
Iteration 53/1000 | Loss: 0.00000895
Iteration 54/1000 | Loss: 0.00000895
Iteration 55/1000 | Loss: 0.00000895
Iteration 56/1000 | Loss: 0.00000894
Iteration 57/1000 | Loss: 0.00000894
Iteration 58/1000 | Loss: 0.00000893
Iteration 59/1000 | Loss: 0.00000892
Iteration 60/1000 | Loss: 0.00000892
Iteration 61/1000 | Loss: 0.00000892
Iteration 62/1000 | Loss: 0.00000892
Iteration 63/1000 | Loss: 0.00000892
Iteration 64/1000 | Loss: 0.00000891
Iteration 65/1000 | Loss: 0.00000891
Iteration 66/1000 | Loss: 0.00000891
Iteration 67/1000 | Loss: 0.00000891
Iteration 68/1000 | Loss: 0.00000891
Iteration 69/1000 | Loss: 0.00000891
Iteration 70/1000 | Loss: 0.00000891
Iteration 71/1000 | Loss: 0.00000891
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000890
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000888
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000887
Iteration 82/1000 | Loss: 0.00000886
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000885
Iteration 88/1000 | Loss: 0.00000885
Iteration 89/1000 | Loss: 0.00000885
Iteration 90/1000 | Loss: 0.00000884
Iteration 91/1000 | Loss: 0.00000884
Iteration 92/1000 | Loss: 0.00000884
Iteration 93/1000 | Loss: 0.00000884
Iteration 94/1000 | Loss: 0.00000883
Iteration 95/1000 | Loss: 0.00000883
Iteration 96/1000 | Loss: 0.00000883
Iteration 97/1000 | Loss: 0.00000883
Iteration 98/1000 | Loss: 0.00000883
Iteration 99/1000 | Loss: 0.00000883
Iteration 100/1000 | Loss: 0.00000882
Iteration 101/1000 | Loss: 0.00000882
Iteration 102/1000 | Loss: 0.00000882
Iteration 103/1000 | Loss: 0.00000882
Iteration 104/1000 | Loss: 0.00000882
Iteration 105/1000 | Loss: 0.00000882
Iteration 106/1000 | Loss: 0.00000882
Iteration 107/1000 | Loss: 0.00000882
Iteration 108/1000 | Loss: 0.00000882
Iteration 109/1000 | Loss: 0.00000882
Iteration 110/1000 | Loss: 0.00000882
Iteration 111/1000 | Loss: 0.00000882
Iteration 112/1000 | Loss: 0.00000882
Iteration 113/1000 | Loss: 0.00000882
Iteration 114/1000 | Loss: 0.00000882
Iteration 115/1000 | Loss: 0.00000882
Iteration 116/1000 | Loss: 0.00000881
Iteration 117/1000 | Loss: 0.00000881
Iteration 118/1000 | Loss: 0.00000881
Iteration 119/1000 | Loss: 0.00000881
Iteration 120/1000 | Loss: 0.00000881
Iteration 121/1000 | Loss: 0.00000881
Iteration 122/1000 | Loss: 0.00000881
Iteration 123/1000 | Loss: 0.00000881
Iteration 124/1000 | Loss: 0.00000881
Iteration 125/1000 | Loss: 0.00000881
Iteration 126/1000 | Loss: 0.00000881
Iteration 127/1000 | Loss: 0.00000881
Iteration 128/1000 | Loss: 0.00000881
Iteration 129/1000 | Loss: 0.00000881
Iteration 130/1000 | Loss: 0.00000881
Iteration 131/1000 | Loss: 0.00000881
Iteration 132/1000 | Loss: 0.00000881
Iteration 133/1000 | Loss: 0.00000881
Iteration 134/1000 | Loss: 0.00000880
Iteration 135/1000 | Loss: 0.00000880
Iteration 136/1000 | Loss: 0.00000880
Iteration 137/1000 | Loss: 0.00000880
Iteration 138/1000 | Loss: 0.00000880
Iteration 139/1000 | Loss: 0.00000880
Iteration 140/1000 | Loss: 0.00000880
Iteration 141/1000 | Loss: 0.00000880
Iteration 142/1000 | Loss: 0.00000880
Iteration 143/1000 | Loss: 0.00000880
Iteration 144/1000 | Loss: 0.00000880
Iteration 145/1000 | Loss: 0.00000880
Iteration 146/1000 | Loss: 0.00000880
Iteration 147/1000 | Loss: 0.00000880
Iteration 148/1000 | Loss: 0.00000880
Iteration 149/1000 | Loss: 0.00000880
Iteration 150/1000 | Loss: 0.00000880
Iteration 151/1000 | Loss: 0.00000880
Iteration 152/1000 | Loss: 0.00000880
Iteration 153/1000 | Loss: 0.00000880
Iteration 154/1000 | Loss: 0.00000880
Iteration 155/1000 | Loss: 0.00000880
Iteration 156/1000 | Loss: 0.00000880
Iteration 157/1000 | Loss: 0.00000880
Iteration 158/1000 | Loss: 0.00000880
Iteration 159/1000 | Loss: 0.00000880
Iteration 160/1000 | Loss: 0.00000880
Iteration 161/1000 | Loss: 0.00000880
Iteration 162/1000 | Loss: 0.00000880
Iteration 163/1000 | Loss: 0.00000880
Iteration 164/1000 | Loss: 0.00000880
Iteration 165/1000 | Loss: 0.00000880
Iteration 166/1000 | Loss: 0.00000880
Iteration 167/1000 | Loss: 0.00000880
Iteration 168/1000 | Loss: 0.00000880
Iteration 169/1000 | Loss: 0.00000880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [8.795063877187204e-06, 8.795063877187204e-06, 8.795063877187204e-06, 8.795063877187204e-06, 8.795063877187204e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.795063877187204e-06

Optimization complete. Final v2v error: 2.5142018795013428 mm

Highest mean error: 4.0205278396606445 mm for frame 219

Lowest mean error: 2.3239688873291016 mm for frame 56

Saving results

Total time: 38.47300410270691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00694697
Iteration 2/25 | Loss: 0.00110718
Iteration 3/25 | Loss: 0.00094974
Iteration 4/25 | Loss: 0.00093430
Iteration 5/25 | Loss: 0.00092960
Iteration 6/25 | Loss: 0.00092814
Iteration 7/25 | Loss: 0.00092797
Iteration 8/25 | Loss: 0.00092797
Iteration 9/25 | Loss: 0.00092797
Iteration 10/25 | Loss: 0.00092797
Iteration 11/25 | Loss: 0.00092797
Iteration 12/25 | Loss: 0.00092797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009279653895646334, 0.0009279653895646334, 0.0009279653895646334, 0.0009279653895646334, 0.0009279653895646334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009279653895646334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42131996
Iteration 2/25 | Loss: 0.00061771
Iteration 3/25 | Loss: 0.00061770
Iteration 4/25 | Loss: 0.00061770
Iteration 5/25 | Loss: 0.00061770
Iteration 6/25 | Loss: 0.00061770
Iteration 7/25 | Loss: 0.00061770
Iteration 8/25 | Loss: 0.00061770
Iteration 9/25 | Loss: 0.00061770
Iteration 10/25 | Loss: 0.00061770
Iteration 11/25 | Loss: 0.00061770
Iteration 12/25 | Loss: 0.00061770
Iteration 13/25 | Loss: 0.00061770
Iteration 14/25 | Loss: 0.00061770
Iteration 15/25 | Loss: 0.00061770
Iteration 16/25 | Loss: 0.00061770
Iteration 17/25 | Loss: 0.00061770
Iteration 18/25 | Loss: 0.00061770
Iteration 19/25 | Loss: 0.00061770
Iteration 20/25 | Loss: 0.00061770
Iteration 21/25 | Loss: 0.00061770
Iteration 22/25 | Loss: 0.00061770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000617700454313308, 0.000617700454313308, 0.000617700454313308, 0.000617700454313308, 0.000617700454313308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000617700454313308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061770
Iteration 2/1000 | Loss: 0.00003093
Iteration 3/1000 | Loss: 0.00001713
Iteration 4/1000 | Loss: 0.00001281
Iteration 5/1000 | Loss: 0.00001185
Iteration 6/1000 | Loss: 0.00001128
Iteration 7/1000 | Loss: 0.00001103
Iteration 8/1000 | Loss: 0.00001081
Iteration 9/1000 | Loss: 0.00001063
Iteration 10/1000 | Loss: 0.00001057
Iteration 11/1000 | Loss: 0.00001057
Iteration 12/1000 | Loss: 0.00001056
Iteration 13/1000 | Loss: 0.00001056
Iteration 14/1000 | Loss: 0.00001055
Iteration 15/1000 | Loss: 0.00001052
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001046
Iteration 30/1000 | Loss: 0.00001045
Iteration 31/1000 | Loss: 0.00001045
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001044
Iteration 34/1000 | Loss: 0.00001043
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001040
Iteration 53/1000 | Loss: 0.00001040
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001039
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001038
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001037
Iteration 62/1000 | Loss: 0.00001037
Iteration 63/1000 | Loss: 0.00001037
Iteration 64/1000 | Loss: 0.00001036
Iteration 65/1000 | Loss: 0.00001036
Iteration 66/1000 | Loss: 0.00001036
Iteration 67/1000 | Loss: 0.00001036
Iteration 68/1000 | Loss: 0.00001035
Iteration 69/1000 | Loss: 0.00001035
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001034
Iteration 72/1000 | Loss: 0.00001034
Iteration 73/1000 | Loss: 0.00001034
Iteration 74/1000 | Loss: 0.00001033
Iteration 75/1000 | Loss: 0.00001033
Iteration 76/1000 | Loss: 0.00001033
Iteration 77/1000 | Loss: 0.00001033
Iteration 78/1000 | Loss: 0.00001033
Iteration 79/1000 | Loss: 0.00001033
Iteration 80/1000 | Loss: 0.00001033
Iteration 81/1000 | Loss: 0.00001032
Iteration 82/1000 | Loss: 0.00001032
Iteration 83/1000 | Loss: 0.00001032
Iteration 84/1000 | Loss: 0.00001031
Iteration 85/1000 | Loss: 0.00001031
Iteration 86/1000 | Loss: 0.00001031
Iteration 87/1000 | Loss: 0.00001031
Iteration 88/1000 | Loss: 0.00001030
Iteration 89/1000 | Loss: 0.00001030
Iteration 90/1000 | Loss: 0.00001030
Iteration 91/1000 | Loss: 0.00001030
Iteration 92/1000 | Loss: 0.00001030
Iteration 93/1000 | Loss: 0.00001030
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001029
Iteration 99/1000 | Loss: 0.00001029
Iteration 100/1000 | Loss: 0.00001029
Iteration 101/1000 | Loss: 0.00001029
Iteration 102/1000 | Loss: 0.00001028
Iteration 103/1000 | Loss: 0.00001028
Iteration 104/1000 | Loss: 0.00001028
Iteration 105/1000 | Loss: 0.00001028
Iteration 106/1000 | Loss: 0.00001028
Iteration 107/1000 | Loss: 0.00001028
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001027
Iteration 116/1000 | Loss: 0.00001027
Iteration 117/1000 | Loss: 0.00001027
Iteration 118/1000 | Loss: 0.00001026
Iteration 119/1000 | Loss: 0.00001026
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001026
Iteration 122/1000 | Loss: 0.00001026
Iteration 123/1000 | Loss: 0.00001026
Iteration 124/1000 | Loss: 0.00001025
Iteration 125/1000 | Loss: 0.00001025
Iteration 126/1000 | Loss: 0.00001025
Iteration 127/1000 | Loss: 0.00001025
Iteration 128/1000 | Loss: 0.00001025
Iteration 129/1000 | Loss: 0.00001025
Iteration 130/1000 | Loss: 0.00001025
Iteration 131/1000 | Loss: 0.00001025
Iteration 132/1000 | Loss: 0.00001025
Iteration 133/1000 | Loss: 0.00001025
Iteration 134/1000 | Loss: 0.00001025
Iteration 135/1000 | Loss: 0.00001025
Iteration 136/1000 | Loss: 0.00001025
Iteration 137/1000 | Loss: 0.00001024
Iteration 138/1000 | Loss: 0.00001024
Iteration 139/1000 | Loss: 0.00001024
Iteration 140/1000 | Loss: 0.00001024
Iteration 141/1000 | Loss: 0.00001024
Iteration 142/1000 | Loss: 0.00001024
Iteration 143/1000 | Loss: 0.00001024
Iteration 144/1000 | Loss: 0.00001024
Iteration 145/1000 | Loss: 0.00001023
Iteration 146/1000 | Loss: 0.00001023
Iteration 147/1000 | Loss: 0.00001023
Iteration 148/1000 | Loss: 0.00001023
Iteration 149/1000 | Loss: 0.00001023
Iteration 150/1000 | Loss: 0.00001023
Iteration 151/1000 | Loss: 0.00001023
Iteration 152/1000 | Loss: 0.00001023
Iteration 153/1000 | Loss: 0.00001023
Iteration 154/1000 | Loss: 0.00001023
Iteration 155/1000 | Loss: 0.00001023
Iteration 156/1000 | Loss: 0.00001023
Iteration 157/1000 | Loss: 0.00001023
Iteration 158/1000 | Loss: 0.00001023
Iteration 159/1000 | Loss: 0.00001023
Iteration 160/1000 | Loss: 0.00001023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.023125969368266e-05, 1.023125969368266e-05, 1.023125969368266e-05, 1.023125969368266e-05, 1.023125969368266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.023125969368266e-05

Optimization complete. Final v2v error: 2.663707971572876 mm

Highest mean error: 3.98547625541687 mm for frame 72

Lowest mean error: 2.19451642036438 mm for frame 115

Saving results

Total time: 32.078001976013184
