Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=220, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12320-12375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412994
Iteration 2/25 | Loss: 0.00134715
Iteration 3/25 | Loss: 0.00127189
Iteration 4/25 | Loss: 0.00126069
Iteration 5/25 | Loss: 0.00125688
Iteration 6/25 | Loss: 0.00125642
Iteration 7/25 | Loss: 0.00125642
Iteration 8/25 | Loss: 0.00125642
Iteration 9/25 | Loss: 0.00125642
Iteration 10/25 | Loss: 0.00125642
Iteration 11/25 | Loss: 0.00125642
Iteration 12/25 | Loss: 0.00125642
Iteration 13/25 | Loss: 0.00125642
Iteration 14/25 | Loss: 0.00125642
Iteration 15/25 | Loss: 0.00125642
Iteration 16/25 | Loss: 0.00125642
Iteration 17/25 | Loss: 0.00125642
Iteration 18/25 | Loss: 0.00125642
Iteration 19/25 | Loss: 0.00125642
Iteration 20/25 | Loss: 0.00125642
Iteration 21/25 | Loss: 0.00125642
Iteration 22/25 | Loss: 0.00125642
Iteration 23/25 | Loss: 0.00125642
Iteration 24/25 | Loss: 0.00125642
Iteration 25/25 | Loss: 0.00125642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99094462
Iteration 2/25 | Loss: 0.00099236
Iteration 3/25 | Loss: 0.00099236
Iteration 4/25 | Loss: 0.00099236
Iteration 5/25 | Loss: 0.00099236
Iteration 6/25 | Loss: 0.00099236
Iteration 7/25 | Loss: 0.00099236
Iteration 8/25 | Loss: 0.00099236
Iteration 9/25 | Loss: 0.00099236
Iteration 10/25 | Loss: 0.00099236
Iteration 11/25 | Loss: 0.00099236
Iteration 12/25 | Loss: 0.00099236
Iteration 13/25 | Loss: 0.00099236
Iteration 14/25 | Loss: 0.00099236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000992357381619513, 0.000992357381619513, 0.000992357381619513, 0.000992357381619513, 0.000992357381619513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000992357381619513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099236
Iteration 2/1000 | Loss: 0.00003287
Iteration 3/1000 | Loss: 0.00002559
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002191
Iteration 6/1000 | Loss: 0.00002081
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001917
Iteration 10/1000 | Loss: 0.00001877
Iteration 11/1000 | Loss: 0.00001852
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001832
Iteration 14/1000 | Loss: 0.00001823
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001808
Iteration 17/1000 | Loss: 0.00001808
Iteration 18/1000 | Loss: 0.00001807
Iteration 19/1000 | Loss: 0.00001806
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001804
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001802
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001801
Iteration 28/1000 | Loss: 0.00001801
Iteration 29/1000 | Loss: 0.00001800
Iteration 30/1000 | Loss: 0.00001800
Iteration 31/1000 | Loss: 0.00001799
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001797
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001796
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001772
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001769
Iteration 62/1000 | Loss: 0.00001769
Iteration 63/1000 | Loss: 0.00001769
Iteration 64/1000 | Loss: 0.00001769
Iteration 65/1000 | Loss: 0.00001769
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001769
Iteration 68/1000 | Loss: 0.00001769
Iteration 69/1000 | Loss: 0.00001769
Iteration 70/1000 | Loss: 0.00001769
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001768
Iteration 73/1000 | Loss: 0.00001768
Iteration 74/1000 | Loss: 0.00001768
Iteration 75/1000 | Loss: 0.00001768
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001767
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00001766
Iteration 82/1000 | Loss: 0.00001766
Iteration 83/1000 | Loss: 0.00001766
Iteration 84/1000 | Loss: 0.00001765
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001765
Iteration 87/1000 | Loss: 0.00001764
Iteration 88/1000 | Loss: 0.00001764
Iteration 89/1000 | Loss: 0.00001764
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001763
Iteration 93/1000 | Loss: 0.00001763
Iteration 94/1000 | Loss: 0.00001763
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001762
Iteration 97/1000 | Loss: 0.00001762
Iteration 98/1000 | Loss: 0.00001762
Iteration 99/1000 | Loss: 0.00001762
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001761
Iteration 102/1000 | Loss: 0.00001760
Iteration 103/1000 | Loss: 0.00001760
Iteration 104/1000 | Loss: 0.00001760
Iteration 105/1000 | Loss: 0.00001760
Iteration 106/1000 | Loss: 0.00001760
Iteration 107/1000 | Loss: 0.00001760
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001760
Iteration 113/1000 | Loss: 0.00001760
Iteration 114/1000 | Loss: 0.00001760
Iteration 115/1000 | Loss: 0.00001760
Iteration 116/1000 | Loss: 0.00001760
Iteration 117/1000 | Loss: 0.00001760
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001760
Iteration 120/1000 | Loss: 0.00001759
Iteration 121/1000 | Loss: 0.00001759
Iteration 122/1000 | Loss: 0.00001759
Iteration 123/1000 | Loss: 0.00001759
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001759
Iteration 126/1000 | Loss: 0.00001759
Iteration 127/1000 | Loss: 0.00001759
Iteration 128/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.7594311430002563e-05, 1.7594311430002563e-05, 1.7594311430002563e-05, 1.7594311430002563e-05, 1.7594311430002563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7594311430002563e-05

Optimization complete. Final v2v error: 3.554696559906006 mm

Highest mean error: 4.125579357147217 mm for frame 30

Lowest mean error: 3.282534599304199 mm for frame 60

Saving results

Total time: 39.25943326950073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827584
Iteration 2/25 | Loss: 0.00130829
Iteration 3/25 | Loss: 0.00122823
Iteration 4/25 | Loss: 0.00122021
Iteration 5/25 | Loss: 0.00121854
Iteration 6/25 | Loss: 0.00121846
Iteration 7/25 | Loss: 0.00121846
Iteration 8/25 | Loss: 0.00121846
Iteration 9/25 | Loss: 0.00121846
Iteration 10/25 | Loss: 0.00121846
Iteration 11/25 | Loss: 0.00121846
Iteration 12/25 | Loss: 0.00121846
Iteration 13/25 | Loss: 0.00121846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001218457706272602, 0.001218457706272602, 0.001218457706272602, 0.001218457706272602, 0.001218457706272602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001218457706272602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33889878
Iteration 2/25 | Loss: 0.00090545
Iteration 3/25 | Loss: 0.00090542
Iteration 4/25 | Loss: 0.00090542
Iteration 5/25 | Loss: 0.00090542
Iteration 6/25 | Loss: 0.00090542
Iteration 7/25 | Loss: 0.00090542
Iteration 8/25 | Loss: 0.00090542
Iteration 9/25 | Loss: 0.00090542
Iteration 10/25 | Loss: 0.00090542
Iteration 11/25 | Loss: 0.00090542
Iteration 12/25 | Loss: 0.00090542
Iteration 13/25 | Loss: 0.00090542
Iteration 14/25 | Loss: 0.00090542
Iteration 15/25 | Loss: 0.00090542
Iteration 16/25 | Loss: 0.00090542
Iteration 17/25 | Loss: 0.00090542
Iteration 18/25 | Loss: 0.00090542
Iteration 19/25 | Loss: 0.00090542
Iteration 20/25 | Loss: 0.00090542
Iteration 21/25 | Loss: 0.00090542
Iteration 22/25 | Loss: 0.00090542
Iteration 23/25 | Loss: 0.00090542
Iteration 24/25 | Loss: 0.00090542
Iteration 25/25 | Loss: 0.00090542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090542
Iteration 2/1000 | Loss: 0.00002076
Iteration 3/1000 | Loss: 0.00001429
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001127
Iteration 7/1000 | Loss: 0.00001079
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001060
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001021
Iteration 14/1000 | Loss: 0.00001020
Iteration 15/1000 | Loss: 0.00001018
Iteration 16/1000 | Loss: 0.00001017
Iteration 17/1000 | Loss: 0.00001010
Iteration 18/1000 | Loss: 0.00001005
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001002
Iteration 22/1000 | Loss: 0.00001002
Iteration 23/1000 | Loss: 0.00001000
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000986
Iteration 26/1000 | Loss: 0.00000986
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000980
Iteration 29/1000 | Loss: 0.00000979
Iteration 30/1000 | Loss: 0.00000979
Iteration 31/1000 | Loss: 0.00000979
Iteration 32/1000 | Loss: 0.00000978
Iteration 33/1000 | Loss: 0.00000977
Iteration 34/1000 | Loss: 0.00000976
Iteration 35/1000 | Loss: 0.00000976
Iteration 36/1000 | Loss: 0.00000975
Iteration 37/1000 | Loss: 0.00000975
Iteration 38/1000 | Loss: 0.00000975
Iteration 39/1000 | Loss: 0.00000975
Iteration 40/1000 | Loss: 0.00000975
Iteration 41/1000 | Loss: 0.00000974
Iteration 42/1000 | Loss: 0.00000974
Iteration 43/1000 | Loss: 0.00000974
Iteration 44/1000 | Loss: 0.00000973
Iteration 45/1000 | Loss: 0.00000973
Iteration 46/1000 | Loss: 0.00000973
Iteration 47/1000 | Loss: 0.00000972
Iteration 48/1000 | Loss: 0.00000972
Iteration 49/1000 | Loss: 0.00000972
Iteration 50/1000 | Loss: 0.00000972
Iteration 51/1000 | Loss: 0.00000972
Iteration 52/1000 | Loss: 0.00000972
Iteration 53/1000 | Loss: 0.00000971
Iteration 54/1000 | Loss: 0.00000971
Iteration 55/1000 | Loss: 0.00000971
Iteration 56/1000 | Loss: 0.00000971
Iteration 57/1000 | Loss: 0.00000971
Iteration 58/1000 | Loss: 0.00000971
Iteration 59/1000 | Loss: 0.00000971
Iteration 60/1000 | Loss: 0.00000971
Iteration 61/1000 | Loss: 0.00000971
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000970
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000968
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000968
Iteration 74/1000 | Loss: 0.00000967
Iteration 75/1000 | Loss: 0.00000967
Iteration 76/1000 | Loss: 0.00000967
Iteration 77/1000 | Loss: 0.00000967
Iteration 78/1000 | Loss: 0.00000967
Iteration 79/1000 | Loss: 0.00000967
Iteration 80/1000 | Loss: 0.00000967
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000967
Iteration 85/1000 | Loss: 0.00000966
Iteration 86/1000 | Loss: 0.00000966
Iteration 87/1000 | Loss: 0.00000966
Iteration 88/1000 | Loss: 0.00000966
Iteration 89/1000 | Loss: 0.00000965
Iteration 90/1000 | Loss: 0.00000965
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000963
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000963
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000963
Iteration 101/1000 | Loss: 0.00000963
Iteration 102/1000 | Loss: 0.00000963
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000961
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000960
Iteration 110/1000 | Loss: 0.00000960
Iteration 111/1000 | Loss: 0.00000960
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000958
Iteration 116/1000 | Loss: 0.00000958
Iteration 117/1000 | Loss: 0.00000957
Iteration 118/1000 | Loss: 0.00000956
Iteration 119/1000 | Loss: 0.00000956
Iteration 120/1000 | Loss: 0.00000956
Iteration 121/1000 | Loss: 0.00000955
Iteration 122/1000 | Loss: 0.00000955
Iteration 123/1000 | Loss: 0.00000955
Iteration 124/1000 | Loss: 0.00000955
Iteration 125/1000 | Loss: 0.00000955
Iteration 126/1000 | Loss: 0.00000955
Iteration 127/1000 | Loss: 0.00000954
Iteration 128/1000 | Loss: 0.00000954
Iteration 129/1000 | Loss: 0.00000953
Iteration 130/1000 | Loss: 0.00000953
Iteration 131/1000 | Loss: 0.00000952
Iteration 132/1000 | Loss: 0.00000952
Iteration 133/1000 | Loss: 0.00000952
Iteration 134/1000 | Loss: 0.00000951
Iteration 135/1000 | Loss: 0.00000951
Iteration 136/1000 | Loss: 0.00000951
Iteration 137/1000 | Loss: 0.00000951
Iteration 138/1000 | Loss: 0.00000951
Iteration 139/1000 | Loss: 0.00000951
Iteration 140/1000 | Loss: 0.00000951
Iteration 141/1000 | Loss: 0.00000951
Iteration 142/1000 | Loss: 0.00000950
Iteration 143/1000 | Loss: 0.00000950
Iteration 144/1000 | Loss: 0.00000950
Iteration 145/1000 | Loss: 0.00000950
Iteration 146/1000 | Loss: 0.00000949
Iteration 147/1000 | Loss: 0.00000949
Iteration 148/1000 | Loss: 0.00000949
Iteration 149/1000 | Loss: 0.00000949
Iteration 150/1000 | Loss: 0.00000949
Iteration 151/1000 | Loss: 0.00000949
Iteration 152/1000 | Loss: 0.00000949
Iteration 153/1000 | Loss: 0.00000949
Iteration 154/1000 | Loss: 0.00000949
Iteration 155/1000 | Loss: 0.00000949
Iteration 156/1000 | Loss: 0.00000949
Iteration 157/1000 | Loss: 0.00000949
Iteration 158/1000 | Loss: 0.00000949
Iteration 159/1000 | Loss: 0.00000949
Iteration 160/1000 | Loss: 0.00000949
Iteration 161/1000 | Loss: 0.00000949
Iteration 162/1000 | Loss: 0.00000949
Iteration 163/1000 | Loss: 0.00000949
Iteration 164/1000 | Loss: 0.00000949
Iteration 165/1000 | Loss: 0.00000949
Iteration 166/1000 | Loss: 0.00000949
Iteration 167/1000 | Loss: 0.00000949
Iteration 168/1000 | Loss: 0.00000949
Iteration 169/1000 | Loss: 0.00000949
Iteration 170/1000 | Loss: 0.00000949
Iteration 171/1000 | Loss: 0.00000949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [9.488984687777702e-06, 9.488984687777702e-06, 9.488984687777702e-06, 9.488984687777702e-06, 9.488984687777702e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.488984687777702e-06

Optimization complete. Final v2v error: 2.656674861907959 mm

Highest mean error: 2.870107412338257 mm for frame 30

Lowest mean error: 2.5348165035247803 mm for frame 104

Saving results

Total time: 37.40450978279114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403089
Iteration 2/25 | Loss: 0.00133888
Iteration 3/25 | Loss: 0.00124457
Iteration 4/25 | Loss: 0.00123371
Iteration 5/25 | Loss: 0.00123125
Iteration 6/25 | Loss: 0.00123056
Iteration 7/25 | Loss: 0.00123036
Iteration 8/25 | Loss: 0.00123036
Iteration 9/25 | Loss: 0.00123036
Iteration 10/25 | Loss: 0.00123036
Iteration 11/25 | Loss: 0.00123036
Iteration 12/25 | Loss: 0.00123036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012303554685786366, 0.0012303554685786366, 0.0012303554685786366, 0.0012303554685786366, 0.0012303554685786366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012303554685786366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44275796
Iteration 2/25 | Loss: 0.00103098
Iteration 3/25 | Loss: 0.00103098
Iteration 4/25 | Loss: 0.00103098
Iteration 5/25 | Loss: 0.00103098
Iteration 6/25 | Loss: 0.00103098
Iteration 7/25 | Loss: 0.00103098
Iteration 8/25 | Loss: 0.00103098
Iteration 9/25 | Loss: 0.00103098
Iteration 10/25 | Loss: 0.00103098
Iteration 11/25 | Loss: 0.00103098
Iteration 12/25 | Loss: 0.00103098
Iteration 13/25 | Loss: 0.00103098
Iteration 14/25 | Loss: 0.00103098
Iteration 15/25 | Loss: 0.00103098
Iteration 16/25 | Loss: 0.00103098
Iteration 17/25 | Loss: 0.00103098
Iteration 18/25 | Loss: 0.00103098
Iteration 19/25 | Loss: 0.00103098
Iteration 20/25 | Loss: 0.00103098
Iteration 21/25 | Loss: 0.00103098
Iteration 22/25 | Loss: 0.00103098
Iteration 23/25 | Loss: 0.00103098
Iteration 24/25 | Loss: 0.00103098
Iteration 25/25 | Loss: 0.00103098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103098
Iteration 2/1000 | Loss: 0.00003265
Iteration 3/1000 | Loss: 0.00002189
Iteration 4/1000 | Loss: 0.00001713
Iteration 5/1000 | Loss: 0.00001557
Iteration 6/1000 | Loss: 0.00001457
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001329
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001322
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001312
Iteration 14/1000 | Loss: 0.00001295
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001263
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001253
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001249
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001237
Iteration 85/1000 | Loss: 0.00001237
Iteration 86/1000 | Loss: 0.00001237
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001235
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001225
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001220
Iteration 174/1000 | Loss: 0.00001220
Iteration 175/1000 | Loss: 0.00001220
Iteration 176/1000 | Loss: 0.00001220
Iteration 177/1000 | Loss: 0.00001220
Iteration 178/1000 | Loss: 0.00001220
Iteration 179/1000 | Loss: 0.00001220
Iteration 180/1000 | Loss: 0.00001220
Iteration 181/1000 | Loss: 0.00001220
Iteration 182/1000 | Loss: 0.00001220
Iteration 183/1000 | Loss: 0.00001220
Iteration 184/1000 | Loss: 0.00001220
Iteration 185/1000 | Loss: 0.00001219
Iteration 186/1000 | Loss: 0.00001219
Iteration 187/1000 | Loss: 0.00001219
Iteration 188/1000 | Loss: 0.00001219
Iteration 189/1000 | Loss: 0.00001219
Iteration 190/1000 | Loss: 0.00001219
Iteration 191/1000 | Loss: 0.00001219
Iteration 192/1000 | Loss: 0.00001219
Iteration 193/1000 | Loss: 0.00001219
Iteration 194/1000 | Loss: 0.00001219
Iteration 195/1000 | Loss: 0.00001219
Iteration 196/1000 | Loss: 0.00001219
Iteration 197/1000 | Loss: 0.00001219
Iteration 198/1000 | Loss: 0.00001219
Iteration 199/1000 | Loss: 0.00001219
Iteration 200/1000 | Loss: 0.00001219
Iteration 201/1000 | Loss: 0.00001219
Iteration 202/1000 | Loss: 0.00001219
Iteration 203/1000 | Loss: 0.00001219
Iteration 204/1000 | Loss: 0.00001219
Iteration 205/1000 | Loss: 0.00001219
Iteration 206/1000 | Loss: 0.00001219
Iteration 207/1000 | Loss: 0.00001219
Iteration 208/1000 | Loss: 0.00001219
Iteration 209/1000 | Loss: 0.00001219
Iteration 210/1000 | Loss: 0.00001219
Iteration 211/1000 | Loss: 0.00001219
Iteration 212/1000 | Loss: 0.00001219
Iteration 213/1000 | Loss: 0.00001219
Iteration 214/1000 | Loss: 0.00001219
Iteration 215/1000 | Loss: 0.00001219
Iteration 216/1000 | Loss: 0.00001219
Iteration 217/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.2191858331789263e-05, 1.2191858331789263e-05, 1.2191858331789263e-05, 1.2191858331789263e-05, 1.2191858331789263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2191858331789263e-05

Optimization complete. Final v2v error: 2.9462409019470215 mm

Highest mean error: 4.015020370483398 mm for frame 64

Lowest mean error: 2.673163890838623 mm for frame 101

Saving results

Total time: 44.15320086479187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392675
Iteration 2/25 | Loss: 0.00135646
Iteration 3/25 | Loss: 0.00126869
Iteration 4/25 | Loss: 0.00125905
Iteration 5/25 | Loss: 0.00125678
Iteration 6/25 | Loss: 0.00125664
Iteration 7/25 | Loss: 0.00125664
Iteration 8/25 | Loss: 0.00125664
Iteration 9/25 | Loss: 0.00125664
Iteration 10/25 | Loss: 0.00125664
Iteration 11/25 | Loss: 0.00125664
Iteration 12/25 | Loss: 0.00125664
Iteration 13/25 | Loss: 0.00125664
Iteration 14/25 | Loss: 0.00125664
Iteration 15/25 | Loss: 0.00125664
Iteration 16/25 | Loss: 0.00125664
Iteration 17/25 | Loss: 0.00125664
Iteration 18/25 | Loss: 0.00125664
Iteration 19/25 | Loss: 0.00125664
Iteration 20/25 | Loss: 0.00125664
Iteration 21/25 | Loss: 0.00125664
Iteration 22/25 | Loss: 0.00125664
Iteration 23/25 | Loss: 0.00125664
Iteration 24/25 | Loss: 0.00125664
Iteration 25/25 | Loss: 0.00125664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35153461
Iteration 2/25 | Loss: 0.00115027
Iteration 3/25 | Loss: 0.00115027
Iteration 4/25 | Loss: 0.00115027
Iteration 5/25 | Loss: 0.00115026
Iteration 6/25 | Loss: 0.00115026
Iteration 7/25 | Loss: 0.00115026
Iteration 8/25 | Loss: 0.00115026
Iteration 9/25 | Loss: 0.00115026
Iteration 10/25 | Loss: 0.00115026
Iteration 11/25 | Loss: 0.00115026
Iteration 12/25 | Loss: 0.00115026
Iteration 13/25 | Loss: 0.00115026
Iteration 14/25 | Loss: 0.00115026
Iteration 15/25 | Loss: 0.00115026
Iteration 16/25 | Loss: 0.00115026
Iteration 17/25 | Loss: 0.00115026
Iteration 18/25 | Loss: 0.00115026
Iteration 19/25 | Loss: 0.00115026
Iteration 20/25 | Loss: 0.00115026
Iteration 21/25 | Loss: 0.00115026
Iteration 22/25 | Loss: 0.00115026
Iteration 23/25 | Loss: 0.00115026
Iteration 24/25 | Loss: 0.00115026
Iteration 25/25 | Loss: 0.00115026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011502631241455674, 0.0011502631241455674, 0.0011502631241455674, 0.0011502631241455674, 0.0011502631241455674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011502631241455674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115026
Iteration 2/1000 | Loss: 0.00003604
Iteration 3/1000 | Loss: 0.00002448
Iteration 4/1000 | Loss: 0.00002022
Iteration 5/1000 | Loss: 0.00001827
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001572
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001502
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001482
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001462
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001455
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001454
Iteration 37/1000 | Loss: 0.00001454
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001453
Iteration 41/1000 | Loss: 0.00001452
Iteration 42/1000 | Loss: 0.00001452
Iteration 43/1000 | Loss: 0.00001452
Iteration 44/1000 | Loss: 0.00001451
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001448
Iteration 55/1000 | Loss: 0.00001448
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001446
Iteration 62/1000 | Loss: 0.00001445
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001444
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001442
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001441
Iteration 82/1000 | Loss: 0.00001441
Iteration 83/1000 | Loss: 0.00001441
Iteration 84/1000 | Loss: 0.00001441
Iteration 85/1000 | Loss: 0.00001440
Iteration 86/1000 | Loss: 0.00001440
Iteration 87/1000 | Loss: 0.00001440
Iteration 88/1000 | Loss: 0.00001440
Iteration 89/1000 | Loss: 0.00001440
Iteration 90/1000 | Loss: 0.00001440
Iteration 91/1000 | Loss: 0.00001440
Iteration 92/1000 | Loss: 0.00001439
Iteration 93/1000 | Loss: 0.00001439
Iteration 94/1000 | Loss: 0.00001439
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001439
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001438
Iteration 101/1000 | Loss: 0.00001437
Iteration 102/1000 | Loss: 0.00001437
Iteration 103/1000 | Loss: 0.00001437
Iteration 104/1000 | Loss: 0.00001437
Iteration 105/1000 | Loss: 0.00001436
Iteration 106/1000 | Loss: 0.00001436
Iteration 107/1000 | Loss: 0.00001436
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001435
Iteration 112/1000 | Loss: 0.00001435
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001434
Iteration 120/1000 | Loss: 0.00001434
Iteration 121/1000 | Loss: 0.00001434
Iteration 122/1000 | Loss: 0.00001434
Iteration 123/1000 | Loss: 0.00001434
Iteration 124/1000 | Loss: 0.00001433
Iteration 125/1000 | Loss: 0.00001433
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001432
Iteration 131/1000 | Loss: 0.00001432
Iteration 132/1000 | Loss: 0.00001432
Iteration 133/1000 | Loss: 0.00001432
Iteration 134/1000 | Loss: 0.00001431
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001431
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001430
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001429
Iteration 148/1000 | Loss: 0.00001429
Iteration 149/1000 | Loss: 0.00001429
Iteration 150/1000 | Loss: 0.00001429
Iteration 151/1000 | Loss: 0.00001429
Iteration 152/1000 | Loss: 0.00001429
Iteration 153/1000 | Loss: 0.00001429
Iteration 154/1000 | Loss: 0.00001428
Iteration 155/1000 | Loss: 0.00001428
Iteration 156/1000 | Loss: 0.00001428
Iteration 157/1000 | Loss: 0.00001428
Iteration 158/1000 | Loss: 0.00001428
Iteration 159/1000 | Loss: 0.00001428
Iteration 160/1000 | Loss: 0.00001428
Iteration 161/1000 | Loss: 0.00001428
Iteration 162/1000 | Loss: 0.00001428
Iteration 163/1000 | Loss: 0.00001427
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001427
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Iteration 176/1000 | Loss: 0.00001425
Iteration 177/1000 | Loss: 0.00001425
Iteration 178/1000 | Loss: 0.00001425
Iteration 179/1000 | Loss: 0.00001425
Iteration 180/1000 | Loss: 0.00001425
Iteration 181/1000 | Loss: 0.00001425
Iteration 182/1000 | Loss: 0.00001425
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001424
Iteration 188/1000 | Loss: 0.00001424
Iteration 189/1000 | Loss: 0.00001424
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001424
Iteration 192/1000 | Loss: 0.00001424
Iteration 193/1000 | Loss: 0.00001424
Iteration 194/1000 | Loss: 0.00001423
Iteration 195/1000 | Loss: 0.00001423
Iteration 196/1000 | Loss: 0.00001423
Iteration 197/1000 | Loss: 0.00001423
Iteration 198/1000 | Loss: 0.00001423
Iteration 199/1000 | Loss: 0.00001423
Iteration 200/1000 | Loss: 0.00001423
Iteration 201/1000 | Loss: 0.00001422
Iteration 202/1000 | Loss: 0.00001422
Iteration 203/1000 | Loss: 0.00001422
Iteration 204/1000 | Loss: 0.00001422
Iteration 205/1000 | Loss: 0.00001422
Iteration 206/1000 | Loss: 0.00001422
Iteration 207/1000 | Loss: 0.00001422
Iteration 208/1000 | Loss: 0.00001422
Iteration 209/1000 | Loss: 0.00001422
Iteration 210/1000 | Loss: 0.00001422
Iteration 211/1000 | Loss: 0.00001422
Iteration 212/1000 | Loss: 0.00001422
Iteration 213/1000 | Loss: 0.00001422
Iteration 214/1000 | Loss: 0.00001422
Iteration 215/1000 | Loss: 0.00001422
Iteration 216/1000 | Loss: 0.00001422
Iteration 217/1000 | Loss: 0.00001422
Iteration 218/1000 | Loss: 0.00001422
Iteration 219/1000 | Loss: 0.00001422
Iteration 220/1000 | Loss: 0.00001422
Iteration 221/1000 | Loss: 0.00001422
Iteration 222/1000 | Loss: 0.00001422
Iteration 223/1000 | Loss: 0.00001422
Iteration 224/1000 | Loss: 0.00001422
Iteration 225/1000 | Loss: 0.00001422
Iteration 226/1000 | Loss: 0.00001422
Iteration 227/1000 | Loss: 0.00001422
Iteration 228/1000 | Loss: 0.00001422
Iteration 229/1000 | Loss: 0.00001422
Iteration 230/1000 | Loss: 0.00001422
Iteration 231/1000 | Loss: 0.00001422
Iteration 232/1000 | Loss: 0.00001422
Iteration 233/1000 | Loss: 0.00001422
Iteration 234/1000 | Loss: 0.00001422
Iteration 235/1000 | Loss: 0.00001422
Iteration 236/1000 | Loss: 0.00001422
Iteration 237/1000 | Loss: 0.00001422
Iteration 238/1000 | Loss: 0.00001422
Iteration 239/1000 | Loss: 0.00001422
Iteration 240/1000 | Loss: 0.00001422
Iteration 241/1000 | Loss: 0.00001422
Iteration 242/1000 | Loss: 0.00001422
Iteration 243/1000 | Loss: 0.00001422
Iteration 244/1000 | Loss: 0.00001422
Iteration 245/1000 | Loss: 0.00001422
Iteration 246/1000 | Loss: 0.00001422
Iteration 247/1000 | Loss: 0.00001422
Iteration 248/1000 | Loss: 0.00001422
Iteration 249/1000 | Loss: 0.00001422
Iteration 250/1000 | Loss: 0.00001422
Iteration 251/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.4222654499462806e-05, 1.4222654499462806e-05, 1.4222654499462806e-05, 1.4222654499462806e-05, 1.4222654499462806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4222654499462806e-05

Optimization complete. Final v2v error: 3.12262225151062 mm

Highest mean error: 3.5862317085266113 mm for frame 88

Lowest mean error: 2.6068286895751953 mm for frame 16

Saving results

Total time: 46.69247889518738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927071
Iteration 2/25 | Loss: 0.00320589
Iteration 3/25 | Loss: 0.00243177
Iteration 4/25 | Loss: 0.00232095
Iteration 5/25 | Loss: 0.00210349
Iteration 6/25 | Loss: 0.00190771
Iteration 7/25 | Loss: 0.00187796
Iteration 8/25 | Loss: 0.00167560
Iteration 9/25 | Loss: 0.00159343
Iteration 10/25 | Loss: 0.00155845
Iteration 11/25 | Loss: 0.00154172
Iteration 12/25 | Loss: 0.00154792
Iteration 13/25 | Loss: 0.00154770
Iteration 14/25 | Loss: 0.00154253
Iteration 15/25 | Loss: 0.00153000
Iteration 16/25 | Loss: 0.00152580
Iteration 17/25 | Loss: 0.00152365
Iteration 18/25 | Loss: 0.00152206
Iteration 19/25 | Loss: 0.00152134
Iteration 20/25 | Loss: 0.00152110
Iteration 21/25 | Loss: 0.00152100
Iteration 22/25 | Loss: 0.00152099
Iteration 23/25 | Loss: 0.00152099
Iteration 24/25 | Loss: 0.00152098
Iteration 25/25 | Loss: 0.00152098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33384979
Iteration 2/25 | Loss: 0.00262479
Iteration 3/25 | Loss: 0.00262479
Iteration 4/25 | Loss: 0.00262479
Iteration 5/25 | Loss: 0.00262478
Iteration 6/25 | Loss: 0.00262478
Iteration 7/25 | Loss: 0.00262478
Iteration 8/25 | Loss: 0.00262478
Iteration 9/25 | Loss: 0.00262478
Iteration 10/25 | Loss: 0.00262478
Iteration 11/25 | Loss: 0.00262478
Iteration 12/25 | Loss: 0.00262478
Iteration 13/25 | Loss: 0.00262478
Iteration 14/25 | Loss: 0.00262478
Iteration 15/25 | Loss: 0.00262478
Iteration 16/25 | Loss: 0.00262478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026247836649417877, 0.0026247836649417877, 0.0026247836649417877, 0.0026247836649417877, 0.0026247836649417877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026247836649417877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262478
Iteration 2/1000 | Loss: 0.00145707
Iteration 3/1000 | Loss: 0.00021742
Iteration 4/1000 | Loss: 0.00014502
Iteration 5/1000 | Loss: 0.00009593
Iteration 6/1000 | Loss: 0.00007279
Iteration 7/1000 | Loss: 0.00005657
Iteration 8/1000 | Loss: 0.00004673
Iteration 9/1000 | Loss: 0.00003910
Iteration 10/1000 | Loss: 0.00003541
Iteration 11/1000 | Loss: 0.00061650
Iteration 12/1000 | Loss: 0.00020787
Iteration 13/1000 | Loss: 0.00004892
Iteration 14/1000 | Loss: 0.00003521
Iteration 15/1000 | Loss: 0.00003028
Iteration 16/1000 | Loss: 0.00002782
Iteration 17/1000 | Loss: 0.00002596
Iteration 18/1000 | Loss: 0.00002500
Iteration 19/1000 | Loss: 0.00002421
Iteration 20/1000 | Loss: 0.00002342
Iteration 21/1000 | Loss: 0.00002284
Iteration 22/1000 | Loss: 0.00002261
Iteration 23/1000 | Loss: 0.00002243
Iteration 24/1000 | Loss: 0.00002221
Iteration 25/1000 | Loss: 0.00002203
Iteration 26/1000 | Loss: 0.00002203
Iteration 27/1000 | Loss: 0.00002202
Iteration 28/1000 | Loss: 0.00002202
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002200
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002200
Iteration 34/1000 | Loss: 0.00002200
Iteration 35/1000 | Loss: 0.00002200
Iteration 36/1000 | Loss: 0.00002200
Iteration 37/1000 | Loss: 0.00002199
Iteration 38/1000 | Loss: 0.00002198
Iteration 39/1000 | Loss: 0.00002194
Iteration 40/1000 | Loss: 0.00002190
Iteration 41/1000 | Loss: 0.00002189
Iteration 42/1000 | Loss: 0.00002188
Iteration 43/1000 | Loss: 0.00002188
Iteration 44/1000 | Loss: 0.00002188
Iteration 45/1000 | Loss: 0.00002187
Iteration 46/1000 | Loss: 0.00002187
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002186
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002185
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002185
Iteration 58/1000 | Loss: 0.00002184
Iteration 59/1000 | Loss: 0.00002184
Iteration 60/1000 | Loss: 0.00002184
Iteration 61/1000 | Loss: 0.00002184
Iteration 62/1000 | Loss: 0.00002184
Iteration 63/1000 | Loss: 0.00002183
Iteration 64/1000 | Loss: 0.00002183
Iteration 65/1000 | Loss: 0.00002183
Iteration 66/1000 | Loss: 0.00002183
Iteration 67/1000 | Loss: 0.00002182
Iteration 68/1000 | Loss: 0.00002182
Iteration 69/1000 | Loss: 0.00002182
Iteration 70/1000 | Loss: 0.00002182
Iteration 71/1000 | Loss: 0.00002182
Iteration 72/1000 | Loss: 0.00002182
Iteration 73/1000 | Loss: 0.00002182
Iteration 74/1000 | Loss: 0.00002182
Iteration 75/1000 | Loss: 0.00002182
Iteration 76/1000 | Loss: 0.00002182
Iteration 77/1000 | Loss: 0.00002181
Iteration 78/1000 | Loss: 0.00002181
Iteration 79/1000 | Loss: 0.00002181
Iteration 80/1000 | Loss: 0.00002181
Iteration 81/1000 | Loss: 0.00002180
Iteration 82/1000 | Loss: 0.00002180
Iteration 83/1000 | Loss: 0.00002180
Iteration 84/1000 | Loss: 0.00002180
Iteration 85/1000 | Loss: 0.00002180
Iteration 86/1000 | Loss: 0.00002180
Iteration 87/1000 | Loss: 0.00002180
Iteration 88/1000 | Loss: 0.00002180
Iteration 89/1000 | Loss: 0.00002179
Iteration 90/1000 | Loss: 0.00002179
Iteration 91/1000 | Loss: 0.00002178
Iteration 92/1000 | Loss: 0.00002178
Iteration 93/1000 | Loss: 0.00002178
Iteration 94/1000 | Loss: 0.00002178
Iteration 95/1000 | Loss: 0.00002178
Iteration 96/1000 | Loss: 0.00002178
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002178
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00002178
Iteration 102/1000 | Loss: 0.00002178
Iteration 103/1000 | Loss: 0.00002178
Iteration 104/1000 | Loss: 0.00002177
Iteration 105/1000 | Loss: 0.00002177
Iteration 106/1000 | Loss: 0.00002177
Iteration 107/1000 | Loss: 0.00002177
Iteration 108/1000 | Loss: 0.00002177
Iteration 109/1000 | Loss: 0.00002177
Iteration 110/1000 | Loss: 0.00002177
Iteration 111/1000 | Loss: 0.00002177
Iteration 112/1000 | Loss: 0.00002177
Iteration 113/1000 | Loss: 0.00002177
Iteration 114/1000 | Loss: 0.00002177
Iteration 115/1000 | Loss: 0.00002177
Iteration 116/1000 | Loss: 0.00002177
Iteration 117/1000 | Loss: 0.00002176
Iteration 118/1000 | Loss: 0.00002176
Iteration 119/1000 | Loss: 0.00002176
Iteration 120/1000 | Loss: 0.00002176
Iteration 121/1000 | Loss: 0.00002176
Iteration 122/1000 | Loss: 0.00002176
Iteration 123/1000 | Loss: 0.00002176
Iteration 124/1000 | Loss: 0.00002176
Iteration 125/1000 | Loss: 0.00002176
Iteration 126/1000 | Loss: 0.00002176
Iteration 127/1000 | Loss: 0.00002176
Iteration 128/1000 | Loss: 0.00002176
Iteration 129/1000 | Loss: 0.00002176
Iteration 130/1000 | Loss: 0.00002176
Iteration 131/1000 | Loss: 0.00002176
Iteration 132/1000 | Loss: 0.00002176
Iteration 133/1000 | Loss: 0.00002176
Iteration 134/1000 | Loss: 0.00002176
Iteration 135/1000 | Loss: 0.00002176
Iteration 136/1000 | Loss: 0.00002175
Iteration 137/1000 | Loss: 0.00002175
Iteration 138/1000 | Loss: 0.00002175
Iteration 139/1000 | Loss: 0.00002175
Iteration 140/1000 | Loss: 0.00002175
Iteration 141/1000 | Loss: 0.00002175
Iteration 142/1000 | Loss: 0.00002175
Iteration 143/1000 | Loss: 0.00002175
Iteration 144/1000 | Loss: 0.00002175
Iteration 145/1000 | Loss: 0.00002175
Iteration 146/1000 | Loss: 0.00002175
Iteration 147/1000 | Loss: 0.00002175
Iteration 148/1000 | Loss: 0.00002175
Iteration 149/1000 | Loss: 0.00002175
Iteration 150/1000 | Loss: 0.00002175
Iteration 151/1000 | Loss: 0.00002175
Iteration 152/1000 | Loss: 0.00002175
Iteration 153/1000 | Loss: 0.00002175
Iteration 154/1000 | Loss: 0.00002175
Iteration 155/1000 | Loss: 0.00002175
Iteration 156/1000 | Loss: 0.00002175
Iteration 157/1000 | Loss: 0.00002175
Iteration 158/1000 | Loss: 0.00002175
Iteration 159/1000 | Loss: 0.00002175
Iteration 160/1000 | Loss: 0.00002175
Iteration 161/1000 | Loss: 0.00002175
Iteration 162/1000 | Loss: 0.00002175
Iteration 163/1000 | Loss: 0.00002175
Iteration 164/1000 | Loss: 0.00002175
Iteration 165/1000 | Loss: 0.00002175
Iteration 166/1000 | Loss: 0.00002175
Iteration 167/1000 | Loss: 0.00002175
Iteration 168/1000 | Loss: 0.00002175
Iteration 169/1000 | Loss: 0.00002175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.1749830921180546e-05, 2.1749830921180546e-05, 2.1749830921180546e-05, 2.1749830921180546e-05, 2.1749830921180546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1749830921180546e-05

Optimization complete. Final v2v error: 3.939291000366211 mm

Highest mean error: 5.855805397033691 mm for frame 84

Lowest mean error: 3.332249879837036 mm for frame 155

Saving results

Total time: 86.79261875152588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439027
Iteration 2/25 | Loss: 0.00150438
Iteration 3/25 | Loss: 0.00136767
Iteration 4/25 | Loss: 0.00134979
Iteration 5/25 | Loss: 0.00134505
Iteration 6/25 | Loss: 0.00134382
Iteration 7/25 | Loss: 0.00134382
Iteration 8/25 | Loss: 0.00134382
Iteration 9/25 | Loss: 0.00134382
Iteration 10/25 | Loss: 0.00134382
Iteration 11/25 | Loss: 0.00134382
Iteration 12/25 | Loss: 0.00134382
Iteration 13/25 | Loss: 0.00134382
Iteration 14/25 | Loss: 0.00134382
Iteration 15/25 | Loss: 0.00134382
Iteration 16/25 | Loss: 0.00134382
Iteration 17/25 | Loss: 0.00134382
Iteration 18/25 | Loss: 0.00134382
Iteration 19/25 | Loss: 0.00134382
Iteration 20/25 | Loss: 0.00134382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013438225723803043, 0.0013438225723803043, 0.0013438225723803043, 0.0013438225723803043, 0.0013438225723803043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013438225723803043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39693630
Iteration 2/25 | Loss: 0.00085811
Iteration 3/25 | Loss: 0.00085811
Iteration 4/25 | Loss: 0.00085811
Iteration 5/25 | Loss: 0.00085811
Iteration 6/25 | Loss: 0.00085811
Iteration 7/25 | Loss: 0.00085811
Iteration 8/25 | Loss: 0.00085810
Iteration 9/25 | Loss: 0.00085810
Iteration 10/25 | Loss: 0.00085810
Iteration 11/25 | Loss: 0.00085810
Iteration 12/25 | Loss: 0.00085810
Iteration 13/25 | Loss: 0.00085810
Iteration 14/25 | Loss: 0.00085810
Iteration 15/25 | Loss: 0.00085810
Iteration 16/25 | Loss: 0.00085810
Iteration 17/25 | Loss: 0.00085810
Iteration 18/25 | Loss: 0.00085810
Iteration 19/25 | Loss: 0.00085810
Iteration 20/25 | Loss: 0.00085810
Iteration 21/25 | Loss: 0.00085810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008581042056903243, 0.0008581042056903243, 0.0008581042056903243, 0.0008581042056903243, 0.0008581042056903243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008581042056903243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085810
Iteration 2/1000 | Loss: 0.00004340
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00002039
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001765
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001734
Iteration 13/1000 | Loss: 0.00001733
Iteration 14/1000 | Loss: 0.00001721
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001714
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001713
Iteration 21/1000 | Loss: 0.00001713
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001701
Iteration 55/1000 | Loss: 0.00001701
Iteration 56/1000 | Loss: 0.00001701
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001700
Iteration 61/1000 | Loss: 0.00001700
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001699
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001698
Iteration 76/1000 | Loss: 0.00001698
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001695
Iteration 98/1000 | Loss: 0.00001695
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001694
Iteration 101/1000 | Loss: 0.00001694
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001693
Iteration 104/1000 | Loss: 0.00001693
Iteration 105/1000 | Loss: 0.00001693
Iteration 106/1000 | Loss: 0.00001693
Iteration 107/1000 | Loss: 0.00001693
Iteration 108/1000 | Loss: 0.00001692
Iteration 109/1000 | Loss: 0.00001692
Iteration 110/1000 | Loss: 0.00001692
Iteration 111/1000 | Loss: 0.00001692
Iteration 112/1000 | Loss: 0.00001692
Iteration 113/1000 | Loss: 0.00001692
Iteration 114/1000 | Loss: 0.00001692
Iteration 115/1000 | Loss: 0.00001692
Iteration 116/1000 | Loss: 0.00001692
Iteration 117/1000 | Loss: 0.00001691
Iteration 118/1000 | Loss: 0.00001691
Iteration 119/1000 | Loss: 0.00001691
Iteration 120/1000 | Loss: 0.00001691
Iteration 121/1000 | Loss: 0.00001691
Iteration 122/1000 | Loss: 0.00001691
Iteration 123/1000 | Loss: 0.00001691
Iteration 124/1000 | Loss: 0.00001691
Iteration 125/1000 | Loss: 0.00001691
Iteration 126/1000 | Loss: 0.00001691
Iteration 127/1000 | Loss: 0.00001691
Iteration 128/1000 | Loss: 0.00001691
Iteration 129/1000 | Loss: 0.00001691
Iteration 130/1000 | Loss: 0.00001691
Iteration 131/1000 | Loss: 0.00001691
Iteration 132/1000 | Loss: 0.00001691
Iteration 133/1000 | Loss: 0.00001690
Iteration 134/1000 | Loss: 0.00001690
Iteration 135/1000 | Loss: 0.00001690
Iteration 136/1000 | Loss: 0.00001690
Iteration 137/1000 | Loss: 0.00001690
Iteration 138/1000 | Loss: 0.00001690
Iteration 139/1000 | Loss: 0.00001690
Iteration 140/1000 | Loss: 0.00001690
Iteration 141/1000 | Loss: 0.00001690
Iteration 142/1000 | Loss: 0.00001690
Iteration 143/1000 | Loss: 0.00001690
Iteration 144/1000 | Loss: 0.00001690
Iteration 145/1000 | Loss: 0.00001690
Iteration 146/1000 | Loss: 0.00001690
Iteration 147/1000 | Loss: 0.00001690
Iteration 148/1000 | Loss: 0.00001690
Iteration 149/1000 | Loss: 0.00001690
Iteration 150/1000 | Loss: 0.00001690
Iteration 151/1000 | Loss: 0.00001690
Iteration 152/1000 | Loss: 0.00001690
Iteration 153/1000 | Loss: 0.00001690
Iteration 154/1000 | Loss: 0.00001690
Iteration 155/1000 | Loss: 0.00001690
Iteration 156/1000 | Loss: 0.00001690
Iteration 157/1000 | Loss: 0.00001690
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001690
Iteration 163/1000 | Loss: 0.00001690
Iteration 164/1000 | Loss: 0.00001690
Iteration 165/1000 | Loss: 0.00001690
Iteration 166/1000 | Loss: 0.00001690
Iteration 167/1000 | Loss: 0.00001690
Iteration 168/1000 | Loss: 0.00001690
Iteration 169/1000 | Loss: 0.00001690
Iteration 170/1000 | Loss: 0.00001690
Iteration 171/1000 | Loss: 0.00001690
Iteration 172/1000 | Loss: 0.00001690
Iteration 173/1000 | Loss: 0.00001690
Iteration 174/1000 | Loss: 0.00001690
Iteration 175/1000 | Loss: 0.00001690
Iteration 176/1000 | Loss: 0.00001690
Iteration 177/1000 | Loss: 0.00001690
Iteration 178/1000 | Loss: 0.00001690
Iteration 179/1000 | Loss: 0.00001690
Iteration 180/1000 | Loss: 0.00001690
Iteration 181/1000 | Loss: 0.00001690
Iteration 182/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.690371755103115e-05, 1.690371755103115e-05, 1.690371755103115e-05, 1.690371755103115e-05, 1.690371755103115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.690371755103115e-05

Optimization complete. Final v2v error: 3.581054925918579 mm

Highest mean error: 3.875321388244629 mm for frame 66

Lowest mean error: 3.3264822959899902 mm for frame 169

Saving results

Total time: 33.80369329452515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926374
Iteration 2/25 | Loss: 0.00183701
Iteration 3/25 | Loss: 0.00149787
Iteration 4/25 | Loss: 0.00147303
Iteration 5/25 | Loss: 0.00146895
Iteration 6/25 | Loss: 0.00146864
Iteration 7/25 | Loss: 0.00146864
Iteration 8/25 | Loss: 0.00146864
Iteration 9/25 | Loss: 0.00146864
Iteration 10/25 | Loss: 0.00146864
Iteration 11/25 | Loss: 0.00146864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014686428476125002, 0.0014686428476125002, 0.0014686428476125002, 0.0014686428476125002, 0.0014686428476125002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014686428476125002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20679927
Iteration 2/25 | Loss: 0.00111897
Iteration 3/25 | Loss: 0.00111897
Iteration 4/25 | Loss: 0.00111897
Iteration 5/25 | Loss: 0.00111897
Iteration 6/25 | Loss: 0.00111897
Iteration 7/25 | Loss: 0.00111897
Iteration 8/25 | Loss: 0.00111897
Iteration 9/25 | Loss: 0.00111897
Iteration 10/25 | Loss: 0.00111897
Iteration 11/25 | Loss: 0.00111897
Iteration 12/25 | Loss: 0.00111897
Iteration 13/25 | Loss: 0.00111897
Iteration 14/25 | Loss: 0.00111897
Iteration 15/25 | Loss: 0.00111897
Iteration 16/25 | Loss: 0.00111897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011189660290256143, 0.0011189660290256143, 0.0011189660290256143, 0.0011189660290256143, 0.0011189660290256143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011189660290256143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111897
Iteration 2/1000 | Loss: 0.00006423
Iteration 3/1000 | Loss: 0.00003611
Iteration 4/1000 | Loss: 0.00003116
Iteration 5/1000 | Loss: 0.00002948
Iteration 6/1000 | Loss: 0.00002828
Iteration 7/1000 | Loss: 0.00002760
Iteration 8/1000 | Loss: 0.00002683
Iteration 9/1000 | Loss: 0.00002627
Iteration 10/1000 | Loss: 0.00002584
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002531
Iteration 13/1000 | Loss: 0.00002511
Iteration 14/1000 | Loss: 0.00002496
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002490
Iteration 17/1000 | Loss: 0.00002489
Iteration 18/1000 | Loss: 0.00002489
Iteration 19/1000 | Loss: 0.00002486
Iteration 20/1000 | Loss: 0.00002485
Iteration 21/1000 | Loss: 0.00002484
Iteration 22/1000 | Loss: 0.00002484
Iteration 23/1000 | Loss: 0.00002483
Iteration 24/1000 | Loss: 0.00002483
Iteration 25/1000 | Loss: 0.00002482
Iteration 26/1000 | Loss: 0.00002482
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002480
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002478
Iteration 31/1000 | Loss: 0.00002474
Iteration 32/1000 | Loss: 0.00002474
Iteration 33/1000 | Loss: 0.00002470
Iteration 34/1000 | Loss: 0.00002470
Iteration 35/1000 | Loss: 0.00002470
Iteration 36/1000 | Loss: 0.00002470
Iteration 37/1000 | Loss: 0.00002468
Iteration 38/1000 | Loss: 0.00002468
Iteration 39/1000 | Loss: 0.00002467
Iteration 40/1000 | Loss: 0.00002467
Iteration 41/1000 | Loss: 0.00002467
Iteration 42/1000 | Loss: 0.00002466
Iteration 43/1000 | Loss: 0.00002466
Iteration 44/1000 | Loss: 0.00002465
Iteration 45/1000 | Loss: 0.00002465
Iteration 46/1000 | Loss: 0.00002465
Iteration 47/1000 | Loss: 0.00002464
Iteration 48/1000 | Loss: 0.00002464
Iteration 49/1000 | Loss: 0.00002463
Iteration 50/1000 | Loss: 0.00002462
Iteration 51/1000 | Loss: 0.00002462
Iteration 52/1000 | Loss: 0.00002457
Iteration 53/1000 | Loss: 0.00002457
Iteration 54/1000 | Loss: 0.00002455
Iteration 55/1000 | Loss: 0.00002454
Iteration 56/1000 | Loss: 0.00002454
Iteration 57/1000 | Loss: 0.00002452
Iteration 58/1000 | Loss: 0.00002449
Iteration 59/1000 | Loss: 0.00002448
Iteration 60/1000 | Loss: 0.00002448
Iteration 61/1000 | Loss: 0.00002446
Iteration 62/1000 | Loss: 0.00002444
Iteration 63/1000 | Loss: 0.00002444
Iteration 64/1000 | Loss: 0.00002444
Iteration 65/1000 | Loss: 0.00002443
Iteration 66/1000 | Loss: 0.00002443
Iteration 67/1000 | Loss: 0.00002443
Iteration 68/1000 | Loss: 0.00002442
Iteration 69/1000 | Loss: 0.00002442
Iteration 70/1000 | Loss: 0.00002441
Iteration 71/1000 | Loss: 0.00002441
Iteration 72/1000 | Loss: 0.00002441
Iteration 73/1000 | Loss: 0.00002440
Iteration 74/1000 | Loss: 0.00002440
Iteration 75/1000 | Loss: 0.00002440
Iteration 76/1000 | Loss: 0.00002439
Iteration 77/1000 | Loss: 0.00002439
Iteration 78/1000 | Loss: 0.00002439
Iteration 79/1000 | Loss: 0.00002439
Iteration 80/1000 | Loss: 0.00002438
Iteration 81/1000 | Loss: 0.00002438
Iteration 82/1000 | Loss: 0.00002438
Iteration 83/1000 | Loss: 0.00002437
Iteration 84/1000 | Loss: 0.00002437
Iteration 85/1000 | Loss: 0.00002437
Iteration 86/1000 | Loss: 0.00002437
Iteration 87/1000 | Loss: 0.00002436
Iteration 88/1000 | Loss: 0.00002436
Iteration 89/1000 | Loss: 0.00002436
Iteration 90/1000 | Loss: 0.00002435
Iteration 91/1000 | Loss: 0.00002435
Iteration 92/1000 | Loss: 0.00002435
Iteration 93/1000 | Loss: 0.00002434
Iteration 94/1000 | Loss: 0.00002434
Iteration 95/1000 | Loss: 0.00002433
Iteration 96/1000 | Loss: 0.00002433
Iteration 97/1000 | Loss: 0.00002433
Iteration 98/1000 | Loss: 0.00002432
Iteration 99/1000 | Loss: 0.00002432
Iteration 100/1000 | Loss: 0.00002432
Iteration 101/1000 | Loss: 0.00002432
Iteration 102/1000 | Loss: 0.00002431
Iteration 103/1000 | Loss: 0.00002431
Iteration 104/1000 | Loss: 0.00002431
Iteration 105/1000 | Loss: 0.00002430
Iteration 106/1000 | Loss: 0.00002430
Iteration 107/1000 | Loss: 0.00002430
Iteration 108/1000 | Loss: 0.00002430
Iteration 109/1000 | Loss: 0.00002430
Iteration 110/1000 | Loss: 0.00002430
Iteration 111/1000 | Loss: 0.00002430
Iteration 112/1000 | Loss: 0.00002430
Iteration 113/1000 | Loss: 0.00002430
Iteration 114/1000 | Loss: 0.00002430
Iteration 115/1000 | Loss: 0.00002430
Iteration 116/1000 | Loss: 0.00002429
Iteration 117/1000 | Loss: 0.00002429
Iteration 118/1000 | Loss: 0.00002429
Iteration 119/1000 | Loss: 0.00002429
Iteration 120/1000 | Loss: 0.00002428
Iteration 121/1000 | Loss: 0.00002428
Iteration 122/1000 | Loss: 0.00002428
Iteration 123/1000 | Loss: 0.00002428
Iteration 124/1000 | Loss: 0.00002428
Iteration 125/1000 | Loss: 0.00002427
Iteration 126/1000 | Loss: 0.00002427
Iteration 127/1000 | Loss: 0.00002427
Iteration 128/1000 | Loss: 0.00002427
Iteration 129/1000 | Loss: 0.00002427
Iteration 130/1000 | Loss: 0.00002427
Iteration 131/1000 | Loss: 0.00002426
Iteration 132/1000 | Loss: 0.00002426
Iteration 133/1000 | Loss: 0.00002426
Iteration 134/1000 | Loss: 0.00002426
Iteration 135/1000 | Loss: 0.00002426
Iteration 136/1000 | Loss: 0.00002426
Iteration 137/1000 | Loss: 0.00002426
Iteration 138/1000 | Loss: 0.00002426
Iteration 139/1000 | Loss: 0.00002426
Iteration 140/1000 | Loss: 0.00002426
Iteration 141/1000 | Loss: 0.00002426
Iteration 142/1000 | Loss: 0.00002426
Iteration 143/1000 | Loss: 0.00002425
Iteration 144/1000 | Loss: 0.00002425
Iteration 145/1000 | Loss: 0.00002425
Iteration 146/1000 | Loss: 0.00002425
Iteration 147/1000 | Loss: 0.00002425
Iteration 148/1000 | Loss: 0.00002425
Iteration 149/1000 | Loss: 0.00002425
Iteration 150/1000 | Loss: 0.00002424
Iteration 151/1000 | Loss: 0.00002424
Iteration 152/1000 | Loss: 0.00002424
Iteration 153/1000 | Loss: 0.00002424
Iteration 154/1000 | Loss: 0.00002424
Iteration 155/1000 | Loss: 0.00002424
Iteration 156/1000 | Loss: 0.00002424
Iteration 157/1000 | Loss: 0.00002424
Iteration 158/1000 | Loss: 0.00002424
Iteration 159/1000 | Loss: 0.00002424
Iteration 160/1000 | Loss: 0.00002424
Iteration 161/1000 | Loss: 0.00002424
Iteration 162/1000 | Loss: 0.00002424
Iteration 163/1000 | Loss: 0.00002424
Iteration 164/1000 | Loss: 0.00002424
Iteration 165/1000 | Loss: 0.00002424
Iteration 166/1000 | Loss: 0.00002424
Iteration 167/1000 | Loss: 0.00002424
Iteration 168/1000 | Loss: 0.00002424
Iteration 169/1000 | Loss: 0.00002424
Iteration 170/1000 | Loss: 0.00002424
Iteration 171/1000 | Loss: 0.00002424
Iteration 172/1000 | Loss: 0.00002424
Iteration 173/1000 | Loss: 0.00002424
Iteration 174/1000 | Loss: 0.00002424
Iteration 175/1000 | Loss: 0.00002424
Iteration 176/1000 | Loss: 0.00002424
Iteration 177/1000 | Loss: 0.00002424
Iteration 178/1000 | Loss: 0.00002424
Iteration 179/1000 | Loss: 0.00002424
Iteration 180/1000 | Loss: 0.00002424
Iteration 181/1000 | Loss: 0.00002424
Iteration 182/1000 | Loss: 0.00002424
Iteration 183/1000 | Loss: 0.00002424
Iteration 184/1000 | Loss: 0.00002424
Iteration 185/1000 | Loss: 0.00002424
Iteration 186/1000 | Loss: 0.00002424
Iteration 187/1000 | Loss: 0.00002424
Iteration 188/1000 | Loss: 0.00002424
Iteration 189/1000 | Loss: 0.00002424
Iteration 190/1000 | Loss: 0.00002424
Iteration 191/1000 | Loss: 0.00002424
Iteration 192/1000 | Loss: 0.00002424
Iteration 193/1000 | Loss: 0.00002424
Iteration 194/1000 | Loss: 0.00002424
Iteration 195/1000 | Loss: 0.00002424
Iteration 196/1000 | Loss: 0.00002424
Iteration 197/1000 | Loss: 0.00002424
Iteration 198/1000 | Loss: 0.00002424
Iteration 199/1000 | Loss: 0.00002424
Iteration 200/1000 | Loss: 0.00002424
Iteration 201/1000 | Loss: 0.00002424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.4240791390184313e-05, 2.4240791390184313e-05, 2.4240791390184313e-05, 2.4240791390184313e-05, 2.4240791390184313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4240791390184313e-05

Optimization complete. Final v2v error: 4.24716854095459 mm

Highest mean error: 4.592415809631348 mm for frame 239

Lowest mean error: 3.9280521869659424 mm for frame 200

Saving results

Total time: 50.1170859336853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484824
Iteration 2/25 | Loss: 0.00142632
Iteration 3/25 | Loss: 0.00135049
Iteration 4/25 | Loss: 0.00134111
Iteration 5/25 | Loss: 0.00133803
Iteration 6/25 | Loss: 0.00133741
Iteration 7/25 | Loss: 0.00133741
Iteration 8/25 | Loss: 0.00133741
Iteration 9/25 | Loss: 0.00133741
Iteration 10/25 | Loss: 0.00133741
Iteration 11/25 | Loss: 0.00133741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001337410882115364, 0.001337410882115364, 0.001337410882115364, 0.001337410882115364, 0.001337410882115364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001337410882115364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42995572
Iteration 2/25 | Loss: 0.00082453
Iteration 3/25 | Loss: 0.00082452
Iteration 4/25 | Loss: 0.00082452
Iteration 5/25 | Loss: 0.00082452
Iteration 6/25 | Loss: 0.00082452
Iteration 7/25 | Loss: 0.00082452
Iteration 8/25 | Loss: 0.00082452
Iteration 9/25 | Loss: 0.00082452
Iteration 10/25 | Loss: 0.00082452
Iteration 11/25 | Loss: 0.00082452
Iteration 12/25 | Loss: 0.00082452
Iteration 13/25 | Loss: 0.00082452
Iteration 14/25 | Loss: 0.00082452
Iteration 15/25 | Loss: 0.00082452
Iteration 16/25 | Loss: 0.00082452
Iteration 17/25 | Loss: 0.00082452
Iteration 18/25 | Loss: 0.00082452
Iteration 19/25 | Loss: 0.00082452
Iteration 20/25 | Loss: 0.00082452
Iteration 21/25 | Loss: 0.00082452
Iteration 22/25 | Loss: 0.00082452
Iteration 23/25 | Loss: 0.00082452
Iteration 24/25 | Loss: 0.00082452
Iteration 25/25 | Loss: 0.00082452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082452
Iteration 2/1000 | Loss: 0.00003782
Iteration 3/1000 | Loss: 0.00002497
Iteration 4/1000 | Loss: 0.00002348
Iteration 5/1000 | Loss: 0.00002275
Iteration 6/1000 | Loss: 0.00002213
Iteration 7/1000 | Loss: 0.00002172
Iteration 8/1000 | Loss: 0.00002146
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002112
Iteration 12/1000 | Loss: 0.00002111
Iteration 13/1000 | Loss: 0.00002111
Iteration 14/1000 | Loss: 0.00002107
Iteration 15/1000 | Loss: 0.00002106
Iteration 16/1000 | Loss: 0.00002106
Iteration 17/1000 | Loss: 0.00002105
Iteration 18/1000 | Loss: 0.00002100
Iteration 19/1000 | Loss: 0.00002100
Iteration 20/1000 | Loss: 0.00002097
Iteration 21/1000 | Loss: 0.00002096
Iteration 22/1000 | Loss: 0.00002095
Iteration 23/1000 | Loss: 0.00002095
Iteration 24/1000 | Loss: 0.00002094
Iteration 25/1000 | Loss: 0.00002094
Iteration 26/1000 | Loss: 0.00002093
Iteration 27/1000 | Loss: 0.00002090
Iteration 28/1000 | Loss: 0.00002087
Iteration 29/1000 | Loss: 0.00002087
Iteration 30/1000 | Loss: 0.00002085
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002084
Iteration 34/1000 | Loss: 0.00002084
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002083
Iteration 40/1000 | Loss: 0.00002083
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002082
Iteration 44/1000 | Loss: 0.00002081
Iteration 45/1000 | Loss: 0.00002081
Iteration 46/1000 | Loss: 0.00002081
Iteration 47/1000 | Loss: 0.00002081
Iteration 48/1000 | Loss: 0.00002081
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002078
Iteration 57/1000 | Loss: 0.00002078
Iteration 58/1000 | Loss: 0.00002078
Iteration 59/1000 | Loss: 0.00002077
Iteration 60/1000 | Loss: 0.00002077
Iteration 61/1000 | Loss: 0.00002077
Iteration 62/1000 | Loss: 0.00002077
Iteration 63/1000 | Loss: 0.00002076
Iteration 64/1000 | Loss: 0.00002075
Iteration 65/1000 | Loss: 0.00002075
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002073
Iteration 70/1000 | Loss: 0.00002073
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002071
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002069
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002066
Iteration 111/1000 | Loss: 0.00002066
Iteration 112/1000 | Loss: 0.00002066
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002065
Iteration 119/1000 | Loss: 0.00002065
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002065
Iteration 122/1000 | Loss: 0.00002065
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002064
Iteration 128/1000 | Loss: 0.00002064
Iteration 129/1000 | Loss: 0.00002064
Iteration 130/1000 | Loss: 0.00002064
Iteration 131/1000 | Loss: 0.00002064
Iteration 132/1000 | Loss: 0.00002064
Iteration 133/1000 | Loss: 0.00002064
Iteration 134/1000 | Loss: 0.00002064
Iteration 135/1000 | Loss: 0.00002064
Iteration 136/1000 | Loss: 0.00002063
Iteration 137/1000 | Loss: 0.00002063
Iteration 138/1000 | Loss: 0.00002063
Iteration 139/1000 | Loss: 0.00002063
Iteration 140/1000 | Loss: 0.00002063
Iteration 141/1000 | Loss: 0.00002063
Iteration 142/1000 | Loss: 0.00002062
Iteration 143/1000 | Loss: 0.00002062
Iteration 144/1000 | Loss: 0.00002062
Iteration 145/1000 | Loss: 0.00002062
Iteration 146/1000 | Loss: 0.00002062
Iteration 147/1000 | Loss: 0.00002062
Iteration 148/1000 | Loss: 0.00002062
Iteration 149/1000 | Loss: 0.00002062
Iteration 150/1000 | Loss: 0.00002062
Iteration 151/1000 | Loss: 0.00002062
Iteration 152/1000 | Loss: 0.00002062
Iteration 153/1000 | Loss: 0.00002062
Iteration 154/1000 | Loss: 0.00002062
Iteration 155/1000 | Loss: 0.00002062
Iteration 156/1000 | Loss: 0.00002061
Iteration 157/1000 | Loss: 0.00002061
Iteration 158/1000 | Loss: 0.00002061
Iteration 159/1000 | Loss: 0.00002061
Iteration 160/1000 | Loss: 0.00002061
Iteration 161/1000 | Loss: 0.00002061
Iteration 162/1000 | Loss: 0.00002061
Iteration 163/1000 | Loss: 0.00002061
Iteration 164/1000 | Loss: 0.00002061
Iteration 165/1000 | Loss: 0.00002061
Iteration 166/1000 | Loss: 0.00002061
Iteration 167/1000 | Loss: 0.00002061
Iteration 168/1000 | Loss: 0.00002061
Iteration 169/1000 | Loss: 0.00002061
Iteration 170/1000 | Loss: 0.00002061
Iteration 171/1000 | Loss: 0.00002060
Iteration 172/1000 | Loss: 0.00002060
Iteration 173/1000 | Loss: 0.00002060
Iteration 174/1000 | Loss: 0.00002060
Iteration 175/1000 | Loss: 0.00002060
Iteration 176/1000 | Loss: 0.00002060
Iteration 177/1000 | Loss: 0.00002060
Iteration 178/1000 | Loss: 0.00002060
Iteration 179/1000 | Loss: 0.00002060
Iteration 180/1000 | Loss: 0.00002060
Iteration 181/1000 | Loss: 0.00002060
Iteration 182/1000 | Loss: 0.00002060
Iteration 183/1000 | Loss: 0.00002060
Iteration 184/1000 | Loss: 0.00002060
Iteration 185/1000 | Loss: 0.00002060
Iteration 186/1000 | Loss: 0.00002060
Iteration 187/1000 | Loss: 0.00002060
Iteration 188/1000 | Loss: 0.00002060
Iteration 189/1000 | Loss: 0.00002060
Iteration 190/1000 | Loss: 0.00002060
Iteration 191/1000 | Loss: 0.00002060
Iteration 192/1000 | Loss: 0.00002060
Iteration 193/1000 | Loss: 0.00002060
Iteration 194/1000 | Loss: 0.00002060
Iteration 195/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.059899634332396e-05, 2.059899634332396e-05, 2.059899634332396e-05, 2.059899634332396e-05, 2.059899634332396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.059899634332396e-05

Optimization complete. Final v2v error: 3.920656681060791 mm

Highest mean error: 4.217592716217041 mm for frame 116

Lowest mean error: 3.216712713241577 mm for frame 6

Saving results

Total time: 42.208386182785034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440492
Iteration 2/25 | Loss: 0.00152085
Iteration 3/25 | Loss: 0.00140654
Iteration 4/25 | Loss: 0.00139066
Iteration 5/25 | Loss: 0.00138545
Iteration 6/25 | Loss: 0.00138433
Iteration 7/25 | Loss: 0.00138433
Iteration 8/25 | Loss: 0.00138433
Iteration 9/25 | Loss: 0.00138433
Iteration 10/25 | Loss: 0.00138433
Iteration 11/25 | Loss: 0.00138433
Iteration 12/25 | Loss: 0.00138433
Iteration 13/25 | Loss: 0.00138433
Iteration 14/25 | Loss: 0.00138433
Iteration 15/25 | Loss: 0.00138433
Iteration 16/25 | Loss: 0.00138433
Iteration 17/25 | Loss: 0.00138433
Iteration 18/25 | Loss: 0.00138433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013843292836099863, 0.0013843292836099863, 0.0013843292836099863, 0.0013843292836099863, 0.0013843292836099863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013843292836099863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42458522
Iteration 2/25 | Loss: 0.00082504
Iteration 3/25 | Loss: 0.00082503
Iteration 4/25 | Loss: 0.00082503
Iteration 5/25 | Loss: 0.00082503
Iteration 6/25 | Loss: 0.00082503
Iteration 7/25 | Loss: 0.00082503
Iteration 8/25 | Loss: 0.00082503
Iteration 9/25 | Loss: 0.00082503
Iteration 10/25 | Loss: 0.00082503
Iteration 11/25 | Loss: 0.00082503
Iteration 12/25 | Loss: 0.00082503
Iteration 13/25 | Loss: 0.00082503
Iteration 14/25 | Loss: 0.00082503
Iteration 15/25 | Loss: 0.00082503
Iteration 16/25 | Loss: 0.00082503
Iteration 17/25 | Loss: 0.00082503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008250289247371256, 0.0008250289247371256, 0.0008250289247371256, 0.0008250289247371256, 0.0008250289247371256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008250289247371256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082503
Iteration 2/1000 | Loss: 0.00006583
Iteration 3/1000 | Loss: 0.00003582
Iteration 4/1000 | Loss: 0.00002804
Iteration 5/1000 | Loss: 0.00002572
Iteration 6/1000 | Loss: 0.00002424
Iteration 7/1000 | Loss: 0.00002315
Iteration 8/1000 | Loss: 0.00002255
Iteration 9/1000 | Loss: 0.00002181
Iteration 10/1000 | Loss: 0.00002136
Iteration 11/1000 | Loss: 0.00002101
Iteration 12/1000 | Loss: 0.00002074
Iteration 13/1000 | Loss: 0.00002062
Iteration 14/1000 | Loss: 0.00002060
Iteration 15/1000 | Loss: 0.00002045
Iteration 16/1000 | Loss: 0.00002045
Iteration 17/1000 | Loss: 0.00002044
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002042
Iteration 20/1000 | Loss: 0.00002040
Iteration 21/1000 | Loss: 0.00002040
Iteration 22/1000 | Loss: 0.00002039
Iteration 23/1000 | Loss: 0.00002039
Iteration 24/1000 | Loss: 0.00002038
Iteration 25/1000 | Loss: 0.00002038
Iteration 26/1000 | Loss: 0.00002037
Iteration 27/1000 | Loss: 0.00002036
Iteration 28/1000 | Loss: 0.00002034
Iteration 29/1000 | Loss: 0.00002033
Iteration 30/1000 | Loss: 0.00002029
Iteration 31/1000 | Loss: 0.00002024
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002023
Iteration 34/1000 | Loss: 0.00002021
Iteration 35/1000 | Loss: 0.00002020
Iteration 36/1000 | Loss: 0.00002020
Iteration 37/1000 | Loss: 0.00002020
Iteration 38/1000 | Loss: 0.00002019
Iteration 39/1000 | Loss: 0.00002019
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002017
Iteration 43/1000 | Loss: 0.00002016
Iteration 44/1000 | Loss: 0.00002015
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002015
Iteration 47/1000 | Loss: 0.00002015
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002014
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002013
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00002012
Iteration 55/1000 | Loss: 0.00002012
Iteration 56/1000 | Loss: 0.00002012
Iteration 57/1000 | Loss: 0.00002012
Iteration 58/1000 | Loss: 0.00002012
Iteration 59/1000 | Loss: 0.00002011
Iteration 60/1000 | Loss: 0.00002011
Iteration 61/1000 | Loss: 0.00002011
Iteration 62/1000 | Loss: 0.00002011
Iteration 63/1000 | Loss: 0.00002011
Iteration 64/1000 | Loss: 0.00002011
Iteration 65/1000 | Loss: 0.00002011
Iteration 66/1000 | Loss: 0.00002010
Iteration 67/1000 | Loss: 0.00002010
Iteration 68/1000 | Loss: 0.00002009
Iteration 69/1000 | Loss: 0.00002009
Iteration 70/1000 | Loss: 0.00002009
Iteration 71/1000 | Loss: 0.00002008
Iteration 72/1000 | Loss: 0.00002008
Iteration 73/1000 | Loss: 0.00002008
Iteration 74/1000 | Loss: 0.00002008
Iteration 75/1000 | Loss: 0.00002008
Iteration 76/1000 | Loss: 0.00002008
Iteration 77/1000 | Loss: 0.00002008
Iteration 78/1000 | Loss: 0.00002007
Iteration 79/1000 | Loss: 0.00002007
Iteration 80/1000 | Loss: 0.00002007
Iteration 81/1000 | Loss: 0.00002007
Iteration 82/1000 | Loss: 0.00002006
Iteration 83/1000 | Loss: 0.00002006
Iteration 84/1000 | Loss: 0.00002006
Iteration 85/1000 | Loss: 0.00002006
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002005
Iteration 88/1000 | Loss: 0.00002005
Iteration 89/1000 | Loss: 0.00002005
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002004
Iteration 96/1000 | Loss: 0.00002004
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002003
Iteration 104/1000 | Loss: 0.00002003
Iteration 105/1000 | Loss: 0.00002003
Iteration 106/1000 | Loss: 0.00002003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.00344202312408e-05, 2.00344202312408e-05, 2.00344202312408e-05, 2.00344202312408e-05, 2.00344202312408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.00344202312408e-05

Optimization complete. Final v2v error: 3.8240103721618652 mm

Highest mean error: 4.390594482421875 mm for frame 56

Lowest mean error: 3.3729145526885986 mm for frame 197

Saving results

Total time: 42.23071098327637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374898
Iteration 2/25 | Loss: 0.00156018
Iteration 3/25 | Loss: 0.00143727
Iteration 4/25 | Loss: 0.00140563
Iteration 5/25 | Loss: 0.00139528
Iteration 6/25 | Loss: 0.00139314
Iteration 7/25 | Loss: 0.00139309
Iteration 8/25 | Loss: 0.00139309
Iteration 9/25 | Loss: 0.00139309
Iteration 10/25 | Loss: 0.00139309
Iteration 11/25 | Loss: 0.00139309
Iteration 12/25 | Loss: 0.00139309
Iteration 13/25 | Loss: 0.00139309
Iteration 14/25 | Loss: 0.00139309
Iteration 15/25 | Loss: 0.00139309
Iteration 16/25 | Loss: 0.00139309
Iteration 17/25 | Loss: 0.00139309
Iteration 18/25 | Loss: 0.00139309
Iteration 19/25 | Loss: 0.00139309
Iteration 20/25 | Loss: 0.00139309
Iteration 21/25 | Loss: 0.00139309
Iteration 22/25 | Loss: 0.00139309
Iteration 23/25 | Loss: 0.00139309
Iteration 24/25 | Loss: 0.00139309
Iteration 25/25 | Loss: 0.00139309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35940254
Iteration 2/25 | Loss: 0.00085544
Iteration 3/25 | Loss: 0.00085544
Iteration 4/25 | Loss: 0.00085544
Iteration 5/25 | Loss: 0.00085544
Iteration 6/25 | Loss: 0.00085544
Iteration 7/25 | Loss: 0.00085544
Iteration 8/25 | Loss: 0.00085544
Iteration 9/25 | Loss: 0.00085544
Iteration 10/25 | Loss: 0.00085544
Iteration 11/25 | Loss: 0.00085544
Iteration 12/25 | Loss: 0.00085544
Iteration 13/25 | Loss: 0.00085544
Iteration 14/25 | Loss: 0.00085544
Iteration 15/25 | Loss: 0.00085544
Iteration 16/25 | Loss: 0.00085544
Iteration 17/25 | Loss: 0.00085544
Iteration 18/25 | Loss: 0.00085544
Iteration 19/25 | Loss: 0.00085544
Iteration 20/25 | Loss: 0.00085544
Iteration 21/25 | Loss: 0.00085544
Iteration 22/25 | Loss: 0.00085544
Iteration 23/25 | Loss: 0.00085544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008554409141652286, 0.0008554409141652286, 0.0008554409141652286, 0.0008554409141652286, 0.0008554409141652286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008554409141652286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085544
Iteration 2/1000 | Loss: 0.00005830
Iteration 3/1000 | Loss: 0.00003257
Iteration 4/1000 | Loss: 0.00002827
Iteration 5/1000 | Loss: 0.00002635
Iteration 6/1000 | Loss: 0.00002516
Iteration 7/1000 | Loss: 0.00002424
Iteration 8/1000 | Loss: 0.00002358
Iteration 9/1000 | Loss: 0.00002296
Iteration 10/1000 | Loss: 0.00002259
Iteration 11/1000 | Loss: 0.00002238
Iteration 12/1000 | Loss: 0.00002218
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002203
Iteration 15/1000 | Loss: 0.00002202
Iteration 16/1000 | Loss: 0.00002201
Iteration 17/1000 | Loss: 0.00002199
Iteration 18/1000 | Loss: 0.00002195
Iteration 19/1000 | Loss: 0.00002195
Iteration 20/1000 | Loss: 0.00002193
Iteration 21/1000 | Loss: 0.00002192
Iteration 22/1000 | Loss: 0.00002192
Iteration 23/1000 | Loss: 0.00002191
Iteration 24/1000 | Loss: 0.00002191
Iteration 25/1000 | Loss: 0.00002190
Iteration 26/1000 | Loss: 0.00002190
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002189
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002188
Iteration 32/1000 | Loss: 0.00002188
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002187
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002186
Iteration 37/1000 | Loss: 0.00002186
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002185
Iteration 41/1000 | Loss: 0.00002184
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002184
Iteration 44/1000 | Loss: 0.00002184
Iteration 45/1000 | Loss: 0.00002183
Iteration 46/1000 | Loss: 0.00002183
Iteration 47/1000 | Loss: 0.00002182
Iteration 48/1000 | Loss: 0.00002182
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002181
Iteration 52/1000 | Loss: 0.00002181
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002180
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002180
Iteration 59/1000 | Loss: 0.00002180
Iteration 60/1000 | Loss: 0.00002180
Iteration 61/1000 | Loss: 0.00002180
Iteration 62/1000 | Loss: 0.00002180
Iteration 63/1000 | Loss: 0.00002180
Iteration 64/1000 | Loss: 0.00002180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [2.1795174689032137e-05, 2.1795174689032137e-05, 2.1795174689032137e-05, 2.1795174689032137e-05, 2.1795174689032137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1795174689032137e-05

Optimization complete. Final v2v error: 3.9566917419433594 mm

Highest mean error: 4.534787178039551 mm for frame 239

Lowest mean error: 3.4278554916381836 mm for frame 112

Saving results

Total time: 37.257102251052856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905309
Iteration 2/25 | Loss: 0.00197501
Iteration 3/25 | Loss: 0.00174261
Iteration 4/25 | Loss: 0.00168361
Iteration 5/25 | Loss: 0.00166077
Iteration 6/25 | Loss: 0.00166559
Iteration 7/25 | Loss: 0.00164844
Iteration 8/25 | Loss: 0.00163054
Iteration 9/25 | Loss: 0.00161495
Iteration 10/25 | Loss: 0.00159796
Iteration 11/25 | Loss: 0.00159233
Iteration 12/25 | Loss: 0.00159657
Iteration 13/25 | Loss: 0.00159294
Iteration 14/25 | Loss: 0.00158881
Iteration 15/25 | Loss: 0.00158685
Iteration 16/25 | Loss: 0.00158615
Iteration 17/25 | Loss: 0.00158587
Iteration 18/25 | Loss: 0.00158579
Iteration 19/25 | Loss: 0.00158568
Iteration 20/25 | Loss: 0.00158538
Iteration 21/25 | Loss: 0.00158506
Iteration 22/25 | Loss: 0.00159394
Iteration 23/25 | Loss: 0.00159295
Iteration 24/25 | Loss: 0.00157898
Iteration 25/25 | Loss: 0.00156895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33681381
Iteration 2/25 | Loss: 0.00285623
Iteration 3/25 | Loss: 0.00285621
Iteration 4/25 | Loss: 0.00285621
Iteration 5/25 | Loss: 0.00285621
Iteration 6/25 | Loss: 0.00285621
Iteration 7/25 | Loss: 0.00285621
Iteration 8/25 | Loss: 0.00285621
Iteration 9/25 | Loss: 0.00285621
Iteration 10/25 | Loss: 0.00285621
Iteration 11/25 | Loss: 0.00285621
Iteration 12/25 | Loss: 0.00285621
Iteration 13/25 | Loss: 0.00285621
Iteration 14/25 | Loss: 0.00285621
Iteration 15/25 | Loss: 0.00285621
Iteration 16/25 | Loss: 0.00285621
Iteration 17/25 | Loss: 0.00285621
Iteration 18/25 | Loss: 0.00285621
Iteration 19/25 | Loss: 0.00285621
Iteration 20/25 | Loss: 0.00285621
Iteration 21/25 | Loss: 0.00285621
Iteration 22/25 | Loss: 0.00285621
Iteration 23/25 | Loss: 0.00285621
Iteration 24/25 | Loss: 0.00285621
Iteration 25/25 | Loss: 0.00285621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285621
Iteration 2/1000 | Loss: 0.00042165
Iteration 3/1000 | Loss: 0.00059734
Iteration 4/1000 | Loss: 0.00040354
Iteration 5/1000 | Loss: 0.00022635
Iteration 6/1000 | Loss: 0.00027875
Iteration 7/1000 | Loss: 0.00101525
Iteration 8/1000 | Loss: 0.00039635
Iteration 9/1000 | Loss: 0.00082378
Iteration 10/1000 | Loss: 0.00089181
Iteration 11/1000 | Loss: 0.00033636
Iteration 12/1000 | Loss: 0.00089450
Iteration 13/1000 | Loss: 0.00107350
Iteration 14/1000 | Loss: 0.00021588
Iteration 15/1000 | Loss: 0.00028272
Iteration 16/1000 | Loss: 0.00104983
Iteration 17/1000 | Loss: 0.00023679
Iteration 18/1000 | Loss: 0.00080863
Iteration 19/1000 | Loss: 0.00018532
Iteration 20/1000 | Loss: 0.00132585
Iteration 21/1000 | Loss: 0.00016908
Iteration 22/1000 | Loss: 0.00064896
Iteration 23/1000 | Loss: 0.00101652
Iteration 24/1000 | Loss: 0.00047830
Iteration 25/1000 | Loss: 0.00008962
Iteration 26/1000 | Loss: 0.00007170
Iteration 27/1000 | Loss: 0.00008265
Iteration 28/1000 | Loss: 0.00005672
Iteration 29/1000 | Loss: 0.00044802
Iteration 30/1000 | Loss: 0.00015661
Iteration 31/1000 | Loss: 0.00004747
Iteration 32/1000 | Loss: 0.00040901
Iteration 33/1000 | Loss: 0.00008497
Iteration 34/1000 | Loss: 0.00056005
Iteration 35/1000 | Loss: 0.00017386
Iteration 36/1000 | Loss: 0.00027785
Iteration 37/1000 | Loss: 0.00033217
Iteration 38/1000 | Loss: 0.00004576
Iteration 39/1000 | Loss: 0.00020161
Iteration 40/1000 | Loss: 0.00005386
Iteration 41/1000 | Loss: 0.00004567
Iteration 42/1000 | Loss: 0.00084651
Iteration 43/1000 | Loss: 0.00050340
Iteration 44/1000 | Loss: 0.00075118
Iteration 45/1000 | Loss: 0.00006288
Iteration 46/1000 | Loss: 0.00004890
Iteration 47/1000 | Loss: 0.00045639
Iteration 48/1000 | Loss: 0.00011722
Iteration 49/1000 | Loss: 0.00004003
Iteration 50/1000 | Loss: 0.00067246
Iteration 51/1000 | Loss: 0.00051188
Iteration 52/1000 | Loss: 0.00038705
Iteration 53/1000 | Loss: 0.00004446
Iteration 54/1000 | Loss: 0.00003555
Iteration 55/1000 | Loss: 0.00003003
Iteration 56/1000 | Loss: 0.00002778
Iteration 57/1000 | Loss: 0.00002659
Iteration 58/1000 | Loss: 0.00002616
Iteration 59/1000 | Loss: 0.00002563
Iteration 60/1000 | Loss: 0.00002527
Iteration 61/1000 | Loss: 0.00002501
Iteration 62/1000 | Loss: 0.00002484
Iteration 63/1000 | Loss: 0.00002475
Iteration 64/1000 | Loss: 0.00002475
Iteration 65/1000 | Loss: 0.00002474
Iteration 66/1000 | Loss: 0.00002471
Iteration 67/1000 | Loss: 0.00002469
Iteration 68/1000 | Loss: 0.00002468
Iteration 69/1000 | Loss: 0.00002468
Iteration 70/1000 | Loss: 0.00002468
Iteration 71/1000 | Loss: 0.00002467
Iteration 72/1000 | Loss: 0.00002467
Iteration 73/1000 | Loss: 0.00002466
Iteration 74/1000 | Loss: 0.00002466
Iteration 75/1000 | Loss: 0.00002466
Iteration 76/1000 | Loss: 0.00002466
Iteration 77/1000 | Loss: 0.00002466
Iteration 78/1000 | Loss: 0.00002466
Iteration 79/1000 | Loss: 0.00002465
Iteration 80/1000 | Loss: 0.00002465
Iteration 81/1000 | Loss: 0.00002465
Iteration 82/1000 | Loss: 0.00002465
Iteration 83/1000 | Loss: 0.00002465
Iteration 84/1000 | Loss: 0.00002465
Iteration 85/1000 | Loss: 0.00002465
Iteration 86/1000 | Loss: 0.00002464
Iteration 87/1000 | Loss: 0.00002464
Iteration 88/1000 | Loss: 0.00002464
Iteration 89/1000 | Loss: 0.00002464
Iteration 90/1000 | Loss: 0.00002463
Iteration 91/1000 | Loss: 0.00002463
Iteration 92/1000 | Loss: 0.00002461
Iteration 93/1000 | Loss: 0.00002461
Iteration 94/1000 | Loss: 0.00002461
Iteration 95/1000 | Loss: 0.00002460
Iteration 96/1000 | Loss: 0.00002460
Iteration 97/1000 | Loss: 0.00002460
Iteration 98/1000 | Loss: 0.00002460
Iteration 99/1000 | Loss: 0.00002460
Iteration 100/1000 | Loss: 0.00002460
Iteration 101/1000 | Loss: 0.00002460
Iteration 102/1000 | Loss: 0.00002459
Iteration 103/1000 | Loss: 0.00002459
Iteration 104/1000 | Loss: 0.00002459
Iteration 105/1000 | Loss: 0.00002459
Iteration 106/1000 | Loss: 0.00002459
Iteration 107/1000 | Loss: 0.00002458
Iteration 108/1000 | Loss: 0.00002458
Iteration 109/1000 | Loss: 0.00002458
Iteration 110/1000 | Loss: 0.00002458
Iteration 111/1000 | Loss: 0.00002458
Iteration 112/1000 | Loss: 0.00002458
Iteration 113/1000 | Loss: 0.00002458
Iteration 114/1000 | Loss: 0.00002458
Iteration 115/1000 | Loss: 0.00002458
Iteration 116/1000 | Loss: 0.00002458
Iteration 117/1000 | Loss: 0.00002457
Iteration 118/1000 | Loss: 0.00002457
Iteration 119/1000 | Loss: 0.00002457
Iteration 120/1000 | Loss: 0.00002457
Iteration 121/1000 | Loss: 0.00002456
Iteration 122/1000 | Loss: 0.00002456
Iteration 123/1000 | Loss: 0.00002456
Iteration 124/1000 | Loss: 0.00002456
Iteration 125/1000 | Loss: 0.00002456
Iteration 126/1000 | Loss: 0.00002455
Iteration 127/1000 | Loss: 0.00002455
Iteration 128/1000 | Loss: 0.00002455
Iteration 129/1000 | Loss: 0.00002455
Iteration 130/1000 | Loss: 0.00002455
Iteration 131/1000 | Loss: 0.00002455
Iteration 132/1000 | Loss: 0.00002455
Iteration 133/1000 | Loss: 0.00002455
Iteration 134/1000 | Loss: 0.00002455
Iteration 135/1000 | Loss: 0.00002455
Iteration 136/1000 | Loss: 0.00002455
Iteration 137/1000 | Loss: 0.00002455
Iteration 138/1000 | Loss: 0.00002455
Iteration 139/1000 | Loss: 0.00002455
Iteration 140/1000 | Loss: 0.00002454
Iteration 141/1000 | Loss: 0.00002454
Iteration 142/1000 | Loss: 0.00002454
Iteration 143/1000 | Loss: 0.00002454
Iteration 144/1000 | Loss: 0.00002454
Iteration 145/1000 | Loss: 0.00002454
Iteration 146/1000 | Loss: 0.00002454
Iteration 147/1000 | Loss: 0.00002454
Iteration 148/1000 | Loss: 0.00002453
Iteration 149/1000 | Loss: 0.00002453
Iteration 150/1000 | Loss: 0.00002453
Iteration 151/1000 | Loss: 0.00002453
Iteration 152/1000 | Loss: 0.00002453
Iteration 153/1000 | Loss: 0.00002453
Iteration 154/1000 | Loss: 0.00002453
Iteration 155/1000 | Loss: 0.00002453
Iteration 156/1000 | Loss: 0.00002453
Iteration 157/1000 | Loss: 0.00002453
Iteration 158/1000 | Loss: 0.00002453
Iteration 159/1000 | Loss: 0.00002453
Iteration 160/1000 | Loss: 0.00002453
Iteration 161/1000 | Loss: 0.00002452
Iteration 162/1000 | Loss: 0.00002452
Iteration 163/1000 | Loss: 0.00002452
Iteration 164/1000 | Loss: 0.00002452
Iteration 165/1000 | Loss: 0.00002451
Iteration 166/1000 | Loss: 0.00002451
Iteration 167/1000 | Loss: 0.00002451
Iteration 168/1000 | Loss: 0.00002451
Iteration 169/1000 | Loss: 0.00002451
Iteration 170/1000 | Loss: 0.00002451
Iteration 171/1000 | Loss: 0.00002451
Iteration 172/1000 | Loss: 0.00002451
Iteration 173/1000 | Loss: 0.00002451
Iteration 174/1000 | Loss: 0.00002451
Iteration 175/1000 | Loss: 0.00002451
Iteration 176/1000 | Loss: 0.00002451
Iteration 177/1000 | Loss: 0.00002451
Iteration 178/1000 | Loss: 0.00002451
Iteration 179/1000 | Loss: 0.00002451
Iteration 180/1000 | Loss: 0.00002451
Iteration 181/1000 | Loss: 0.00002451
Iteration 182/1000 | Loss: 0.00002451
Iteration 183/1000 | Loss: 0.00002451
Iteration 184/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.4505670808139257e-05, 2.4505670808139257e-05, 2.4505670808139257e-05, 2.4505670808139257e-05, 2.4505670808139257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4505670808139257e-05

Optimization complete. Final v2v error: 4.1646833419799805 mm

Highest mean error: 4.663025856018066 mm for frame 81

Lowest mean error: 3.5254642963409424 mm for frame 2

Saving results

Total time: 141.5146849155426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876386
Iteration 2/25 | Loss: 0.00172827
Iteration 3/25 | Loss: 0.00141371
Iteration 4/25 | Loss: 0.00136806
Iteration 5/25 | Loss: 0.00135812
Iteration 6/25 | Loss: 0.00135642
Iteration 7/25 | Loss: 0.00135664
Iteration 8/25 | Loss: 0.00135445
Iteration 9/25 | Loss: 0.00135418
Iteration 10/25 | Loss: 0.00135405
Iteration 11/25 | Loss: 0.00135405
Iteration 12/25 | Loss: 0.00135404
Iteration 13/25 | Loss: 0.00135404
Iteration 14/25 | Loss: 0.00135404
Iteration 15/25 | Loss: 0.00135404
Iteration 16/25 | Loss: 0.00135404
Iteration 17/25 | Loss: 0.00135404
Iteration 18/25 | Loss: 0.00135404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001354041276499629, 0.001354041276499629, 0.001354041276499629, 0.001354041276499629, 0.001354041276499629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001354041276499629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16622543
Iteration 2/25 | Loss: 0.00073774
Iteration 3/25 | Loss: 0.00073100
Iteration 4/25 | Loss: 0.00073100
Iteration 5/25 | Loss: 0.00073100
Iteration 6/25 | Loss: 0.00073100
Iteration 7/25 | Loss: 0.00073100
Iteration 8/25 | Loss: 0.00073100
Iteration 9/25 | Loss: 0.00073100
Iteration 10/25 | Loss: 0.00073100
Iteration 11/25 | Loss: 0.00073100
Iteration 12/25 | Loss: 0.00073100
Iteration 13/25 | Loss: 0.00073100
Iteration 14/25 | Loss: 0.00073100
Iteration 15/25 | Loss: 0.00073100
Iteration 16/25 | Loss: 0.00073100
Iteration 17/25 | Loss: 0.00073100
Iteration 18/25 | Loss: 0.00073100
Iteration 19/25 | Loss: 0.00073100
Iteration 20/25 | Loss: 0.00073100
Iteration 21/25 | Loss: 0.00073100
Iteration 22/25 | Loss: 0.00073100
Iteration 23/25 | Loss: 0.00073100
Iteration 24/25 | Loss: 0.00073100
Iteration 25/25 | Loss: 0.00073100

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073100
Iteration 2/1000 | Loss: 0.00004243
Iteration 3/1000 | Loss: 0.00002745
Iteration 4/1000 | Loss: 0.00002300
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002077
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001941
Iteration 11/1000 | Loss: 0.00001922
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001912
Iteration 14/1000 | Loss: 0.00001911
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001903
Iteration 17/1000 | Loss: 0.00001896
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001893
Iteration 20/1000 | Loss: 0.00001893
Iteration 21/1000 | Loss: 0.00001892
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001889
Iteration 24/1000 | Loss: 0.00001889
Iteration 25/1000 | Loss: 0.00001888
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001886
Iteration 31/1000 | Loss: 0.00001886
Iteration 32/1000 | Loss: 0.00001886
Iteration 33/1000 | Loss: 0.00001885
Iteration 34/1000 | Loss: 0.00001885
Iteration 35/1000 | Loss: 0.00001884
Iteration 36/1000 | Loss: 0.00001884
Iteration 37/1000 | Loss: 0.00001883
Iteration 38/1000 | Loss: 0.00001883
Iteration 39/1000 | Loss: 0.00001883
Iteration 40/1000 | Loss: 0.00001883
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001882
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001880
Iteration 61/1000 | Loss: 0.00001880
Iteration 62/1000 | Loss: 0.00001880
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001879
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001878
Iteration 74/1000 | Loss: 0.00001878
Iteration 75/1000 | Loss: 0.00001878
Iteration 76/1000 | Loss: 0.00001878
Iteration 77/1000 | Loss: 0.00001878
Iteration 78/1000 | Loss: 0.00001878
Iteration 79/1000 | Loss: 0.00001878
Iteration 80/1000 | Loss: 0.00001877
Iteration 81/1000 | Loss: 0.00001877
Iteration 82/1000 | Loss: 0.00001877
Iteration 83/1000 | Loss: 0.00001877
Iteration 84/1000 | Loss: 0.00001877
Iteration 85/1000 | Loss: 0.00001877
Iteration 86/1000 | Loss: 0.00001877
Iteration 87/1000 | Loss: 0.00001877
Iteration 88/1000 | Loss: 0.00001877
Iteration 89/1000 | Loss: 0.00001877
Iteration 90/1000 | Loss: 0.00001877
Iteration 91/1000 | Loss: 0.00001877
Iteration 92/1000 | Loss: 0.00001877
Iteration 93/1000 | Loss: 0.00001877
Iteration 94/1000 | Loss: 0.00001877
Iteration 95/1000 | Loss: 0.00001877
Iteration 96/1000 | Loss: 0.00001877
Iteration 97/1000 | Loss: 0.00001877
Iteration 98/1000 | Loss: 0.00001877
Iteration 99/1000 | Loss: 0.00001877
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001876
Iteration 102/1000 | Loss: 0.00001876
Iteration 103/1000 | Loss: 0.00001876
Iteration 104/1000 | Loss: 0.00001876
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.8764418200589716e-05, 1.8764418200589716e-05, 1.8764418200589716e-05, 1.8764418200589716e-05, 1.8764418200589716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8764418200589716e-05

Optimization complete. Final v2v error: 3.7328662872314453 mm

Highest mean error: 4.2855048179626465 mm for frame 0

Lowest mean error: 3.300333261489868 mm for frame 155

Saving results

Total time: 46.608118534088135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904560
Iteration 2/25 | Loss: 0.00168872
Iteration 3/25 | Loss: 0.00138743
Iteration 4/25 | Loss: 0.00135710
Iteration 5/25 | Loss: 0.00135152
Iteration 6/25 | Loss: 0.00134954
Iteration 7/25 | Loss: 0.00134944
Iteration 8/25 | Loss: 0.00134944
Iteration 9/25 | Loss: 0.00134944
Iteration 10/25 | Loss: 0.00134944
Iteration 11/25 | Loss: 0.00134944
Iteration 12/25 | Loss: 0.00134944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013494424056261778, 0.0013494424056261778, 0.0013494424056261778, 0.0013494424056261778, 0.0013494424056261778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013494424056261778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40890574
Iteration 2/25 | Loss: 0.00084759
Iteration 3/25 | Loss: 0.00084759
Iteration 4/25 | Loss: 0.00084759
Iteration 5/25 | Loss: 0.00084758
Iteration 6/25 | Loss: 0.00084758
Iteration 7/25 | Loss: 0.00084758
Iteration 8/25 | Loss: 0.00084758
Iteration 9/25 | Loss: 0.00084758
Iteration 10/25 | Loss: 0.00084758
Iteration 11/25 | Loss: 0.00084758
Iteration 12/25 | Loss: 0.00084758
Iteration 13/25 | Loss: 0.00084758
Iteration 14/25 | Loss: 0.00084758
Iteration 15/25 | Loss: 0.00084758
Iteration 16/25 | Loss: 0.00084758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008475834038108587, 0.0008475834038108587, 0.0008475834038108587, 0.0008475834038108587, 0.0008475834038108587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008475834038108587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084758
Iteration 2/1000 | Loss: 0.00006100
Iteration 3/1000 | Loss: 0.00002657
Iteration 4/1000 | Loss: 0.00002117
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001786
Iteration 8/1000 | Loss: 0.00001742
Iteration 9/1000 | Loss: 0.00001723
Iteration 10/1000 | Loss: 0.00001699
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001681
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001657
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001656
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001645
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001640
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001639
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001638
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001637
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001635
Iteration 76/1000 | Loss: 0.00001635
Iteration 77/1000 | Loss: 0.00001635
Iteration 78/1000 | Loss: 0.00001635
Iteration 79/1000 | Loss: 0.00001635
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001635
Iteration 83/1000 | Loss: 0.00001635
Iteration 84/1000 | Loss: 0.00001635
Iteration 85/1000 | Loss: 0.00001635
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00001635
Iteration 92/1000 | Loss: 0.00001635
Iteration 93/1000 | Loss: 0.00001635
Iteration 94/1000 | Loss: 0.00001635
Iteration 95/1000 | Loss: 0.00001635
Iteration 96/1000 | Loss: 0.00001635
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.6346681150025688e-05, 1.6346681150025688e-05, 1.6346681150025688e-05, 1.6346681150025688e-05, 1.6346681150025688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6346681150025688e-05

Optimization complete. Final v2v error: 3.506554126739502 mm

Highest mean error: 3.757662057876587 mm for frame 24

Lowest mean error: 3.246943712234497 mm for frame 99

Saving results

Total time: 33.79404139518738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01168715
Iteration 2/25 | Loss: 0.00203034
Iteration 3/25 | Loss: 0.00160510
Iteration 4/25 | Loss: 0.00155785
Iteration 5/25 | Loss: 0.00154820
Iteration 6/25 | Loss: 0.00154610
Iteration 7/25 | Loss: 0.00154610
Iteration 8/25 | Loss: 0.00154610
Iteration 9/25 | Loss: 0.00154610
Iteration 10/25 | Loss: 0.00154610
Iteration 11/25 | Loss: 0.00154610
Iteration 12/25 | Loss: 0.00154610
Iteration 13/25 | Loss: 0.00154610
Iteration 14/25 | Loss: 0.00154610
Iteration 15/25 | Loss: 0.00154610
Iteration 16/25 | Loss: 0.00154610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001546102692373097, 0.001546102692373097, 0.001546102692373097, 0.001546102692373097, 0.001546102692373097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001546102692373097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88409483
Iteration 2/25 | Loss: 0.00124497
Iteration 3/25 | Loss: 0.00124488
Iteration 4/25 | Loss: 0.00124488
Iteration 5/25 | Loss: 0.00124488
Iteration 6/25 | Loss: 0.00124488
Iteration 7/25 | Loss: 0.00124488
Iteration 8/25 | Loss: 0.00124488
Iteration 9/25 | Loss: 0.00124488
Iteration 10/25 | Loss: 0.00124488
Iteration 11/25 | Loss: 0.00124488
Iteration 12/25 | Loss: 0.00124488
Iteration 13/25 | Loss: 0.00124488
Iteration 14/25 | Loss: 0.00124488
Iteration 15/25 | Loss: 0.00124488
Iteration 16/25 | Loss: 0.00124488
Iteration 17/25 | Loss: 0.00124488
Iteration 18/25 | Loss: 0.00124488
Iteration 19/25 | Loss: 0.00124488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012448782799765468, 0.0012448782799765468, 0.0012448782799765468, 0.0012448782799765468, 0.0012448782799765468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012448782799765468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124488
Iteration 2/1000 | Loss: 0.00010298
Iteration 3/1000 | Loss: 0.00005662
Iteration 4/1000 | Loss: 0.00004874
Iteration 5/1000 | Loss: 0.00004575
Iteration 6/1000 | Loss: 0.00004412
Iteration 7/1000 | Loss: 0.00004299
Iteration 8/1000 | Loss: 0.00004220
Iteration 9/1000 | Loss: 0.00004150
Iteration 10/1000 | Loss: 0.00004103
Iteration 11/1000 | Loss: 0.00004061
Iteration 12/1000 | Loss: 0.00004017
Iteration 13/1000 | Loss: 0.00003989
Iteration 14/1000 | Loss: 0.00003982
Iteration 15/1000 | Loss: 0.00003967
Iteration 16/1000 | Loss: 0.00003967
Iteration 17/1000 | Loss: 0.00003967
Iteration 18/1000 | Loss: 0.00003964
Iteration 19/1000 | Loss: 0.00003963
Iteration 20/1000 | Loss: 0.00003949
Iteration 21/1000 | Loss: 0.00003946
Iteration 22/1000 | Loss: 0.00003944
Iteration 23/1000 | Loss: 0.00003944
Iteration 24/1000 | Loss: 0.00003941
Iteration 25/1000 | Loss: 0.00003939
Iteration 26/1000 | Loss: 0.00003938
Iteration 27/1000 | Loss: 0.00003938
Iteration 28/1000 | Loss: 0.00003937
Iteration 29/1000 | Loss: 0.00003937
Iteration 30/1000 | Loss: 0.00003937
Iteration 31/1000 | Loss: 0.00003936
Iteration 32/1000 | Loss: 0.00003936
Iteration 33/1000 | Loss: 0.00003935
Iteration 34/1000 | Loss: 0.00003935
Iteration 35/1000 | Loss: 0.00003935
Iteration 36/1000 | Loss: 0.00003934
Iteration 37/1000 | Loss: 0.00003934
Iteration 38/1000 | Loss: 0.00003933
Iteration 39/1000 | Loss: 0.00003933
Iteration 40/1000 | Loss: 0.00003931
Iteration 41/1000 | Loss: 0.00003931
Iteration 42/1000 | Loss: 0.00003931
Iteration 43/1000 | Loss: 0.00003930
Iteration 44/1000 | Loss: 0.00003930
Iteration 45/1000 | Loss: 0.00003929
Iteration 46/1000 | Loss: 0.00003929
Iteration 47/1000 | Loss: 0.00003928
Iteration 48/1000 | Loss: 0.00003928
Iteration 49/1000 | Loss: 0.00003928
Iteration 50/1000 | Loss: 0.00003927
Iteration 51/1000 | Loss: 0.00003927
Iteration 52/1000 | Loss: 0.00003926
Iteration 53/1000 | Loss: 0.00003926
Iteration 54/1000 | Loss: 0.00003926
Iteration 55/1000 | Loss: 0.00003925
Iteration 56/1000 | Loss: 0.00003925
Iteration 57/1000 | Loss: 0.00003925
Iteration 58/1000 | Loss: 0.00003925
Iteration 59/1000 | Loss: 0.00003924
Iteration 60/1000 | Loss: 0.00003924
Iteration 61/1000 | Loss: 0.00003922
Iteration 62/1000 | Loss: 0.00003922
Iteration 63/1000 | Loss: 0.00003922
Iteration 64/1000 | Loss: 0.00003921
Iteration 65/1000 | Loss: 0.00003921
Iteration 66/1000 | Loss: 0.00003921
Iteration 67/1000 | Loss: 0.00003921
Iteration 68/1000 | Loss: 0.00003921
Iteration 69/1000 | Loss: 0.00003921
Iteration 70/1000 | Loss: 0.00003921
Iteration 71/1000 | Loss: 0.00003921
Iteration 72/1000 | Loss: 0.00003921
Iteration 73/1000 | Loss: 0.00003921
Iteration 74/1000 | Loss: 0.00003921
Iteration 75/1000 | Loss: 0.00003921
Iteration 76/1000 | Loss: 0.00003921
Iteration 77/1000 | Loss: 0.00003921
Iteration 78/1000 | Loss: 0.00003921
Iteration 79/1000 | Loss: 0.00003921
Iteration 80/1000 | Loss: 0.00003921
Iteration 81/1000 | Loss: 0.00003921
Iteration 82/1000 | Loss: 0.00003921
Iteration 83/1000 | Loss: 0.00003921
Iteration 84/1000 | Loss: 0.00003921
Iteration 85/1000 | Loss: 0.00003921
Iteration 86/1000 | Loss: 0.00003921
Iteration 87/1000 | Loss: 0.00003921
Iteration 88/1000 | Loss: 0.00003921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [3.92064503103029e-05, 3.92064503103029e-05, 3.92064503103029e-05, 3.92064503103029e-05, 3.92064503103029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.92064503103029e-05

Optimization complete. Final v2v error: 5.290290832519531 mm

Highest mean error: 6.070224285125732 mm for frame 143

Lowest mean error: 4.159043312072754 mm for frame 217

Saving results

Total time: 42.38711857795715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066020
Iteration 2/25 | Loss: 0.00380905
Iteration 3/25 | Loss: 0.00226567
Iteration 4/25 | Loss: 0.00159282
Iteration 5/25 | Loss: 0.00152729
Iteration 6/25 | Loss: 0.00145105
Iteration 7/25 | Loss: 0.00138074
Iteration 8/25 | Loss: 0.00120378
Iteration 9/25 | Loss: 0.00117381
Iteration 10/25 | Loss: 0.00112433
Iteration 11/25 | Loss: 0.00108963
Iteration 12/25 | Loss: 0.00108885
Iteration 13/25 | Loss: 0.00107355
Iteration 14/25 | Loss: 0.00107215
Iteration 15/25 | Loss: 0.00106220
Iteration 16/25 | Loss: 0.00106041
Iteration 17/25 | Loss: 0.00106253
Iteration 18/25 | Loss: 0.00106298
Iteration 19/25 | Loss: 0.00105583
Iteration 20/25 | Loss: 0.00105969
Iteration 21/25 | Loss: 0.00105270
Iteration 22/25 | Loss: 0.00105586
Iteration 23/25 | Loss: 0.00105272
Iteration 24/25 | Loss: 0.00105114
Iteration 25/25 | Loss: 0.00104519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36626458
Iteration 2/25 | Loss: 0.00327970
Iteration 3/25 | Loss: 0.00153708
Iteration 4/25 | Loss: 0.00153708
Iteration 5/25 | Loss: 0.00153708
Iteration 6/25 | Loss: 0.00153708
Iteration 7/25 | Loss: 0.00153708
Iteration 8/25 | Loss: 0.00153708
Iteration 9/25 | Loss: 0.00153708
Iteration 10/25 | Loss: 0.00153708
Iteration 11/25 | Loss: 0.00153708
Iteration 12/25 | Loss: 0.00153708
Iteration 13/25 | Loss: 0.00153708
Iteration 14/25 | Loss: 0.00153708
Iteration 15/25 | Loss: 0.00153708
Iteration 16/25 | Loss: 0.00153708
Iteration 17/25 | Loss: 0.00153708
Iteration 18/25 | Loss: 0.00153708
Iteration 19/25 | Loss: 0.00153708
Iteration 20/25 | Loss: 0.00153708
Iteration 21/25 | Loss: 0.00153708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015370775945484638, 0.0015370775945484638, 0.0015370775945484638, 0.0015370775945484638, 0.0015370775945484638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015370775945484638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153708
Iteration 2/1000 | Loss: 0.00379498
Iteration 3/1000 | Loss: 0.00076732
Iteration 4/1000 | Loss: 0.00168721
Iteration 5/1000 | Loss: 0.00142180
Iteration 6/1000 | Loss: 0.00059211
Iteration 7/1000 | Loss: 0.00047162
Iteration 8/1000 | Loss: 0.00032366
Iteration 9/1000 | Loss: 0.00037816
Iteration 10/1000 | Loss: 0.00057241
Iteration 11/1000 | Loss: 0.00034064
Iteration 12/1000 | Loss: 0.00022547
Iteration 13/1000 | Loss: 0.00028659
Iteration 14/1000 | Loss: 0.00047534
Iteration 15/1000 | Loss: 0.00028982
Iteration 16/1000 | Loss: 0.00029373
Iteration 17/1000 | Loss: 0.00043615
Iteration 18/1000 | Loss: 0.00022016
Iteration 19/1000 | Loss: 0.00035770
Iteration 20/1000 | Loss: 0.00164515
Iteration 21/1000 | Loss: 0.00038194
Iteration 22/1000 | Loss: 0.00037143
Iteration 23/1000 | Loss: 0.00037460
Iteration 24/1000 | Loss: 0.00029765
Iteration 25/1000 | Loss: 0.00028080
Iteration 26/1000 | Loss: 0.00622193
Iteration 27/1000 | Loss: 0.00088387
Iteration 28/1000 | Loss: 0.00057837
Iteration 29/1000 | Loss: 0.00061204
Iteration 30/1000 | Loss: 0.00119358
Iteration 31/1000 | Loss: 0.00271702
Iteration 32/1000 | Loss: 0.00078310
Iteration 33/1000 | Loss: 0.00042985
Iteration 34/1000 | Loss: 0.00038559
Iteration 35/1000 | Loss: 0.00030530
Iteration 36/1000 | Loss: 0.00121044
Iteration 37/1000 | Loss: 0.00058540
Iteration 38/1000 | Loss: 0.00068218
Iteration 39/1000 | Loss: 0.00074376
Iteration 40/1000 | Loss: 0.00049483
Iteration 41/1000 | Loss: 0.00044972
Iteration 42/1000 | Loss: 0.00053935
Iteration 43/1000 | Loss: 0.00055215
Iteration 44/1000 | Loss: 0.00048751
Iteration 45/1000 | Loss: 0.00063301
Iteration 46/1000 | Loss: 0.00054154
Iteration 47/1000 | Loss: 0.00072202
Iteration 48/1000 | Loss: 0.00052085
Iteration 49/1000 | Loss: 0.00058345
Iteration 50/1000 | Loss: 0.00110718
Iteration 51/1000 | Loss: 0.00058119
Iteration 52/1000 | Loss: 0.00048702
Iteration 53/1000 | Loss: 0.00047065
Iteration 54/1000 | Loss: 0.00050009
Iteration 55/1000 | Loss: 0.00042352
Iteration 56/1000 | Loss: 0.00047812
Iteration 57/1000 | Loss: 0.00034472
Iteration 58/1000 | Loss: 0.00030453
Iteration 59/1000 | Loss: 0.00119353
Iteration 60/1000 | Loss: 0.00044029
Iteration 61/1000 | Loss: 0.00062633
Iteration 62/1000 | Loss: 0.00049236
Iteration 63/1000 | Loss: 0.00037189
Iteration 64/1000 | Loss: 0.00060470
Iteration 65/1000 | Loss: 0.00026903
Iteration 66/1000 | Loss: 0.00030020
Iteration 67/1000 | Loss: 0.00080556
Iteration 68/1000 | Loss: 0.00020016
Iteration 69/1000 | Loss: 0.00029335
Iteration 70/1000 | Loss: 0.00027687
Iteration 71/1000 | Loss: 0.00039811
Iteration 72/1000 | Loss: 0.00060317
Iteration 73/1000 | Loss: 0.00041031
Iteration 74/1000 | Loss: 0.00029252
Iteration 75/1000 | Loss: 0.00030827
Iteration 76/1000 | Loss: 0.00036256
Iteration 77/1000 | Loss: 0.00026102
Iteration 78/1000 | Loss: 0.00026582
Iteration 79/1000 | Loss: 0.00036074
Iteration 80/1000 | Loss: 0.00026556
Iteration 81/1000 | Loss: 0.00036200
Iteration 82/1000 | Loss: 0.00033393
Iteration 83/1000 | Loss: 0.00028667
Iteration 84/1000 | Loss: 0.00030381
Iteration 85/1000 | Loss: 0.00042074
Iteration 86/1000 | Loss: 0.00027301
Iteration 87/1000 | Loss: 0.00036534
Iteration 88/1000 | Loss: 0.00076459
Iteration 89/1000 | Loss: 0.00048796
Iteration 90/1000 | Loss: 0.00027467
Iteration 91/1000 | Loss: 0.00052406
Iteration 92/1000 | Loss: 0.00057301
Iteration 93/1000 | Loss: 0.00034446
Iteration 94/1000 | Loss: 0.00036864
Iteration 95/1000 | Loss: 0.00030019
Iteration 96/1000 | Loss: 0.00020888
Iteration 97/1000 | Loss: 0.00031112
Iteration 98/1000 | Loss: 0.00025044
Iteration 99/1000 | Loss: 0.00028128
Iteration 100/1000 | Loss: 0.00034601
Iteration 101/1000 | Loss: 0.00033323
Iteration 102/1000 | Loss: 0.00026863
Iteration 103/1000 | Loss: 0.00043444
Iteration 104/1000 | Loss: 0.00036029
Iteration 105/1000 | Loss: 0.00036900
Iteration 106/1000 | Loss: 0.00041155
Iteration 107/1000 | Loss: 0.00035664
Iteration 108/1000 | Loss: 0.00038628
Iteration 109/1000 | Loss: 0.00059637
Iteration 110/1000 | Loss: 0.00088713
Iteration 111/1000 | Loss: 0.00080901
Iteration 112/1000 | Loss: 0.00026266
Iteration 113/1000 | Loss: 0.00007752
Iteration 114/1000 | Loss: 0.00026804
Iteration 115/1000 | Loss: 0.00014357
Iteration 116/1000 | Loss: 0.00013228
Iteration 117/1000 | Loss: 0.00016679
Iteration 118/1000 | Loss: 0.00017168
Iteration 119/1000 | Loss: 0.00044703
Iteration 120/1000 | Loss: 0.00013938
Iteration 121/1000 | Loss: 0.00012863
Iteration 122/1000 | Loss: 0.00014227
Iteration 123/1000 | Loss: 0.00014921
Iteration 124/1000 | Loss: 0.00059552
Iteration 125/1000 | Loss: 0.00020055
Iteration 126/1000 | Loss: 0.00011167
Iteration 127/1000 | Loss: 0.00015274
Iteration 128/1000 | Loss: 0.00021046
Iteration 129/1000 | Loss: 0.00026802
Iteration 130/1000 | Loss: 0.00015594
Iteration 131/1000 | Loss: 0.00016951
Iteration 132/1000 | Loss: 0.00015985
Iteration 133/1000 | Loss: 0.00017408
Iteration 134/1000 | Loss: 0.00017956
Iteration 135/1000 | Loss: 0.00015818
Iteration 136/1000 | Loss: 0.00012584
Iteration 137/1000 | Loss: 0.00006105
Iteration 138/1000 | Loss: 0.00012259
Iteration 139/1000 | Loss: 0.00010999
Iteration 140/1000 | Loss: 0.00004921
Iteration 141/1000 | Loss: 0.00012087
Iteration 142/1000 | Loss: 0.00017529
Iteration 143/1000 | Loss: 0.00019693
Iteration 144/1000 | Loss: 0.00013305
Iteration 145/1000 | Loss: 0.00015959
Iteration 146/1000 | Loss: 0.00026008
Iteration 147/1000 | Loss: 0.00005725
Iteration 148/1000 | Loss: 0.00013908
Iteration 149/1000 | Loss: 0.00010319
Iteration 150/1000 | Loss: 0.00011698
Iteration 151/1000 | Loss: 0.00012902
Iteration 152/1000 | Loss: 0.00013216
Iteration 153/1000 | Loss: 0.00013256
Iteration 154/1000 | Loss: 0.00021300
Iteration 155/1000 | Loss: 0.00011505
Iteration 156/1000 | Loss: 0.00013798
Iteration 157/1000 | Loss: 0.00009894
Iteration 158/1000 | Loss: 0.00013367
Iteration 159/1000 | Loss: 0.00014280
Iteration 160/1000 | Loss: 0.00017163
Iteration 161/1000 | Loss: 0.00015090
Iteration 162/1000 | Loss: 0.00025767
Iteration 163/1000 | Loss: 0.00031604
Iteration 164/1000 | Loss: 0.00028695
Iteration 165/1000 | Loss: 0.00005950
Iteration 166/1000 | Loss: 0.00012670
Iteration 167/1000 | Loss: 0.00005259
Iteration 168/1000 | Loss: 0.00004771
Iteration 169/1000 | Loss: 0.00004681
Iteration 170/1000 | Loss: 0.00004641
Iteration 171/1000 | Loss: 0.00004607
Iteration 172/1000 | Loss: 0.00004571
Iteration 173/1000 | Loss: 0.00004566
Iteration 174/1000 | Loss: 0.00004536
Iteration 175/1000 | Loss: 0.00004516
Iteration 176/1000 | Loss: 0.00004516
Iteration 177/1000 | Loss: 0.00004512
Iteration 178/1000 | Loss: 0.00004504
Iteration 179/1000 | Loss: 0.00004500
Iteration 180/1000 | Loss: 0.00004500
Iteration 181/1000 | Loss: 0.00004499
Iteration 182/1000 | Loss: 0.00004499
Iteration 183/1000 | Loss: 0.00004498
Iteration 184/1000 | Loss: 0.00004498
Iteration 185/1000 | Loss: 0.00004497
Iteration 186/1000 | Loss: 0.00004496
Iteration 187/1000 | Loss: 0.00004493
Iteration 188/1000 | Loss: 0.00004493
Iteration 189/1000 | Loss: 0.00004489
Iteration 190/1000 | Loss: 0.00004484
Iteration 191/1000 | Loss: 0.00004484
Iteration 192/1000 | Loss: 0.00004481
Iteration 193/1000 | Loss: 0.00004479
Iteration 194/1000 | Loss: 0.00004479
Iteration 195/1000 | Loss: 0.00004478
Iteration 196/1000 | Loss: 0.00004478
Iteration 197/1000 | Loss: 0.00004478
Iteration 198/1000 | Loss: 0.00004478
Iteration 199/1000 | Loss: 0.00004478
Iteration 200/1000 | Loss: 0.00004478
Iteration 201/1000 | Loss: 0.00004478
Iteration 202/1000 | Loss: 0.00004478
Iteration 203/1000 | Loss: 0.00004478
Iteration 204/1000 | Loss: 0.00004478
Iteration 205/1000 | Loss: 0.00004478
Iteration 206/1000 | Loss: 0.00004477
Iteration 207/1000 | Loss: 0.00004477
Iteration 208/1000 | Loss: 0.00004477
Iteration 209/1000 | Loss: 0.00004477
Iteration 210/1000 | Loss: 0.00004477
Iteration 211/1000 | Loss: 0.00004477
Iteration 212/1000 | Loss: 0.00004474
Iteration 213/1000 | Loss: 0.00004474
Iteration 214/1000 | Loss: 0.00004473
Iteration 215/1000 | Loss: 0.00004473
Iteration 216/1000 | Loss: 0.00004473
Iteration 217/1000 | Loss: 0.00004473
Iteration 218/1000 | Loss: 0.00004472
Iteration 219/1000 | Loss: 0.00004472
Iteration 220/1000 | Loss: 0.00004470
Iteration 221/1000 | Loss: 0.00004470
Iteration 222/1000 | Loss: 0.00004470
Iteration 223/1000 | Loss: 0.00004470
Iteration 224/1000 | Loss: 0.00004470
Iteration 225/1000 | Loss: 0.00004470
Iteration 226/1000 | Loss: 0.00004469
Iteration 227/1000 | Loss: 0.00004469
Iteration 228/1000 | Loss: 0.00004469
Iteration 229/1000 | Loss: 0.00004469
Iteration 230/1000 | Loss: 0.00004469
Iteration 231/1000 | Loss: 0.00004469
Iteration 232/1000 | Loss: 0.00004468
Iteration 233/1000 | Loss: 0.00004468
Iteration 234/1000 | Loss: 0.00004467
Iteration 235/1000 | Loss: 0.00004467
Iteration 236/1000 | Loss: 0.00004467
Iteration 237/1000 | Loss: 0.00004467
Iteration 238/1000 | Loss: 0.00004466
Iteration 239/1000 | Loss: 0.00004466
Iteration 240/1000 | Loss: 0.00004465
Iteration 241/1000 | Loss: 0.00004465
Iteration 242/1000 | Loss: 0.00004465
Iteration 243/1000 | Loss: 0.00004465
Iteration 244/1000 | Loss: 0.00004465
Iteration 245/1000 | Loss: 0.00004465
Iteration 246/1000 | Loss: 0.00004465
Iteration 247/1000 | Loss: 0.00004464
Iteration 248/1000 | Loss: 0.00004464
Iteration 249/1000 | Loss: 0.00004464
Iteration 250/1000 | Loss: 0.00004463
Iteration 251/1000 | Loss: 0.00004463
Iteration 252/1000 | Loss: 0.00004463
Iteration 253/1000 | Loss: 0.00004463
Iteration 254/1000 | Loss: 0.00004463
Iteration 255/1000 | Loss: 0.00004463
Iteration 256/1000 | Loss: 0.00004463
Iteration 257/1000 | Loss: 0.00004463
Iteration 258/1000 | Loss: 0.00004463
Iteration 259/1000 | Loss: 0.00004463
Iteration 260/1000 | Loss: 0.00004462
Iteration 261/1000 | Loss: 0.00004462
Iteration 262/1000 | Loss: 0.00004462
Iteration 263/1000 | Loss: 0.00004462
Iteration 264/1000 | Loss: 0.00004462
Iteration 265/1000 | Loss: 0.00004462
Iteration 266/1000 | Loss: 0.00004462
Iteration 267/1000 | Loss: 0.00004462
Iteration 268/1000 | Loss: 0.00004462
Iteration 269/1000 | Loss: 0.00004462
Iteration 270/1000 | Loss: 0.00004462
Iteration 271/1000 | Loss: 0.00004462
Iteration 272/1000 | Loss: 0.00004462
Iteration 273/1000 | Loss: 0.00004462
Iteration 274/1000 | Loss: 0.00004462
Iteration 275/1000 | Loss: 0.00004462
Iteration 276/1000 | Loss: 0.00004462
Iteration 277/1000 | Loss: 0.00004462
Iteration 278/1000 | Loss: 0.00004462
Iteration 279/1000 | Loss: 0.00004462
Iteration 280/1000 | Loss: 0.00004462
Iteration 281/1000 | Loss: 0.00004462
Iteration 282/1000 | Loss: 0.00004462
Iteration 283/1000 | Loss: 0.00004462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [4.461753633222543e-05, 4.461753633222543e-05, 4.461753633222543e-05, 4.461753633222543e-05, 4.461753633222543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.461753633222543e-05

Optimization complete. Final v2v error: 4.208334922790527 mm

Highest mean error: 23.097827911376953 mm for frame 158

Lowest mean error: 2.8381316661834717 mm for frame 6

Saving results

Total time: 317.71602177619934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002104
Iteration 2/25 | Loss: 0.00216084
Iteration 3/25 | Loss: 0.00172232
Iteration 4/25 | Loss: 0.00164154
Iteration 5/25 | Loss: 0.00160508
Iteration 6/25 | Loss: 0.00153850
Iteration 7/25 | Loss: 0.00150577
Iteration 8/25 | Loss: 0.00149879
Iteration 9/25 | Loss: 0.00149061
Iteration 10/25 | Loss: 0.00147664
Iteration 11/25 | Loss: 0.00147283
Iteration 12/25 | Loss: 0.00147176
Iteration 13/25 | Loss: 0.00147159
Iteration 14/25 | Loss: 0.00147437
Iteration 15/25 | Loss: 0.00146877
Iteration 16/25 | Loss: 0.00146996
Iteration 17/25 | Loss: 0.00145735
Iteration 18/25 | Loss: 0.00145541
Iteration 19/25 | Loss: 0.00145515
Iteration 20/25 | Loss: 0.00145509
Iteration 21/25 | Loss: 0.00145507
Iteration 22/25 | Loss: 0.00145507
Iteration 23/25 | Loss: 0.00145507
Iteration 24/25 | Loss: 0.00145506
Iteration 25/25 | Loss: 0.00145506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.31411266
Iteration 2/25 | Loss: 0.00095025
Iteration 3/25 | Loss: 0.00095016
Iteration 4/25 | Loss: 0.00095016
Iteration 5/25 | Loss: 0.00095016
Iteration 6/25 | Loss: 0.00095016
Iteration 7/25 | Loss: 0.00095016
Iteration 8/25 | Loss: 0.00095016
Iteration 9/25 | Loss: 0.00095016
Iteration 10/25 | Loss: 0.00095016
Iteration 11/25 | Loss: 0.00095016
Iteration 12/25 | Loss: 0.00095016
Iteration 13/25 | Loss: 0.00095016
Iteration 14/25 | Loss: 0.00095016
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009501627064310014, 0.0009501627064310014, 0.0009501627064310014, 0.0009501627064310014, 0.0009501627064310014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009501627064310014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095016
Iteration 2/1000 | Loss: 0.00016739
Iteration 3/1000 | Loss: 0.00020662
Iteration 4/1000 | Loss: 0.00011112
Iteration 5/1000 | Loss: 0.00009283
Iteration 6/1000 | Loss: 0.00007447
Iteration 7/1000 | Loss: 0.00006709
Iteration 8/1000 | Loss: 0.00006319
Iteration 9/1000 | Loss: 0.00008002
Iteration 10/1000 | Loss: 0.00006522
Iteration 11/1000 | Loss: 0.00011571
Iteration 12/1000 | Loss: 0.00006436
Iteration 13/1000 | Loss: 0.00010139
Iteration 14/1000 | Loss: 0.00005671
Iteration 15/1000 | Loss: 0.00005157
Iteration 16/1000 | Loss: 0.00004769
Iteration 17/1000 | Loss: 0.00004558
Iteration 18/1000 | Loss: 0.00004446
Iteration 19/1000 | Loss: 0.00004358
Iteration 20/1000 | Loss: 0.00004268
Iteration 21/1000 | Loss: 0.00004220
Iteration 22/1000 | Loss: 0.00004174
Iteration 23/1000 | Loss: 0.00004090
Iteration 24/1000 | Loss: 0.00004013
Iteration 25/1000 | Loss: 0.00003968
Iteration 26/1000 | Loss: 0.00003939
Iteration 27/1000 | Loss: 0.00003918
Iteration 28/1000 | Loss: 0.00003902
Iteration 29/1000 | Loss: 0.00003900
Iteration 30/1000 | Loss: 0.00003883
Iteration 31/1000 | Loss: 0.00003862
Iteration 32/1000 | Loss: 0.00003837
Iteration 33/1000 | Loss: 0.00003810
Iteration 34/1000 | Loss: 0.00003786
Iteration 35/1000 | Loss: 0.00003759
Iteration 36/1000 | Loss: 0.00003743
Iteration 37/1000 | Loss: 0.00003734
Iteration 38/1000 | Loss: 0.00003734
Iteration 39/1000 | Loss: 0.00003733
Iteration 40/1000 | Loss: 0.00006161
Iteration 41/1000 | Loss: 0.00004211
Iteration 42/1000 | Loss: 0.00006156
Iteration 43/1000 | Loss: 0.00006910
Iteration 44/1000 | Loss: 0.00012975
Iteration 45/1000 | Loss: 0.00006185
Iteration 46/1000 | Loss: 0.00005471
Iteration 47/1000 | Loss: 0.00006702
Iteration 48/1000 | Loss: 0.00006401
Iteration 49/1000 | Loss: 0.00004568
Iteration 50/1000 | Loss: 0.00004234
Iteration 51/1000 | Loss: 0.00004056
Iteration 52/1000 | Loss: 0.00003875
Iteration 53/1000 | Loss: 0.00003809
Iteration 54/1000 | Loss: 0.00006095
Iteration 55/1000 | Loss: 0.00006322
Iteration 56/1000 | Loss: 0.00006539
Iteration 57/1000 | Loss: 0.00005606
Iteration 58/1000 | Loss: 0.00003974
Iteration 59/1000 | Loss: 0.00005669
Iteration 60/1000 | Loss: 0.00005039
Iteration 61/1000 | Loss: 0.00003914
Iteration 62/1000 | Loss: 0.00005599
Iteration 63/1000 | Loss: 0.00004792
Iteration 64/1000 | Loss: 0.00003983
Iteration 65/1000 | Loss: 0.00003883
Iteration 66/1000 | Loss: 0.00005537
Iteration 67/1000 | Loss: 0.00004349
Iteration 68/1000 | Loss: 0.00007950
Iteration 69/1000 | Loss: 0.00004293
Iteration 70/1000 | Loss: 0.00004131
Iteration 71/1000 | Loss: 0.00004960
Iteration 72/1000 | Loss: 0.00003982
Iteration 73/1000 | Loss: 0.00003880
Iteration 74/1000 | Loss: 0.00014657
Iteration 75/1000 | Loss: 0.00006343
Iteration 76/1000 | Loss: 0.00004629
Iteration 77/1000 | Loss: 0.00004253
Iteration 78/1000 | Loss: 0.00004051
Iteration 79/1000 | Loss: 0.00004592
Iteration 80/1000 | Loss: 0.00003744
Iteration 81/1000 | Loss: 0.00003720
Iteration 82/1000 | Loss: 0.00003714
Iteration 83/1000 | Loss: 0.00003705
Iteration 84/1000 | Loss: 0.00003705
Iteration 85/1000 | Loss: 0.00003703
Iteration 86/1000 | Loss: 0.00003702
Iteration 87/1000 | Loss: 0.00003702
Iteration 88/1000 | Loss: 0.00003702
Iteration 89/1000 | Loss: 0.00003699
Iteration 90/1000 | Loss: 0.00003697
Iteration 91/1000 | Loss: 0.00003697
Iteration 92/1000 | Loss: 0.00003696
Iteration 93/1000 | Loss: 0.00003696
Iteration 94/1000 | Loss: 0.00003695
Iteration 95/1000 | Loss: 0.00003694
Iteration 96/1000 | Loss: 0.00003693
Iteration 97/1000 | Loss: 0.00003691
Iteration 98/1000 | Loss: 0.00003690
Iteration 99/1000 | Loss: 0.00003690
Iteration 100/1000 | Loss: 0.00003689
Iteration 101/1000 | Loss: 0.00003689
Iteration 102/1000 | Loss: 0.00003688
Iteration 103/1000 | Loss: 0.00003686
Iteration 104/1000 | Loss: 0.00003686
Iteration 105/1000 | Loss: 0.00003686
Iteration 106/1000 | Loss: 0.00003686
Iteration 107/1000 | Loss: 0.00003685
Iteration 108/1000 | Loss: 0.00003685
Iteration 109/1000 | Loss: 0.00003685
Iteration 110/1000 | Loss: 0.00003685
Iteration 111/1000 | Loss: 0.00003685
Iteration 112/1000 | Loss: 0.00003685
Iteration 113/1000 | Loss: 0.00003685
Iteration 114/1000 | Loss: 0.00003685
Iteration 115/1000 | Loss: 0.00003685
Iteration 116/1000 | Loss: 0.00003685
Iteration 117/1000 | Loss: 0.00003685
Iteration 118/1000 | Loss: 0.00003685
Iteration 119/1000 | Loss: 0.00003685
Iteration 120/1000 | Loss: 0.00003685
Iteration 121/1000 | Loss: 0.00003685
Iteration 122/1000 | Loss: 0.00003685
Iteration 123/1000 | Loss: 0.00003685
Iteration 124/1000 | Loss: 0.00003685
Iteration 125/1000 | Loss: 0.00003685
Iteration 126/1000 | Loss: 0.00003685
Iteration 127/1000 | Loss: 0.00003685
Iteration 128/1000 | Loss: 0.00003685
Iteration 129/1000 | Loss: 0.00003685
Iteration 130/1000 | Loss: 0.00003685
Iteration 131/1000 | Loss: 0.00003685
Iteration 132/1000 | Loss: 0.00003685
Iteration 133/1000 | Loss: 0.00003685
Iteration 134/1000 | Loss: 0.00003685
Iteration 135/1000 | Loss: 0.00003685
Iteration 136/1000 | Loss: 0.00003685
Iteration 137/1000 | Loss: 0.00003685
Iteration 138/1000 | Loss: 0.00003685
Iteration 139/1000 | Loss: 0.00003685
Iteration 140/1000 | Loss: 0.00003685
Iteration 141/1000 | Loss: 0.00003685
Iteration 142/1000 | Loss: 0.00003685
Iteration 143/1000 | Loss: 0.00003685
Iteration 144/1000 | Loss: 0.00003685
Iteration 145/1000 | Loss: 0.00003685
Iteration 146/1000 | Loss: 0.00003685
Iteration 147/1000 | Loss: 0.00003685
Iteration 148/1000 | Loss: 0.00003685
Iteration 149/1000 | Loss: 0.00003685
Iteration 150/1000 | Loss: 0.00003685
Iteration 151/1000 | Loss: 0.00003685
Iteration 152/1000 | Loss: 0.00003685
Iteration 153/1000 | Loss: 0.00003685
Iteration 154/1000 | Loss: 0.00003685
Iteration 155/1000 | Loss: 0.00003685
Iteration 156/1000 | Loss: 0.00003685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [3.684649709612131e-05, 3.684649709612131e-05, 3.684649709612131e-05, 3.684649709612131e-05, 3.684649709612131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.684649709612131e-05

Optimization complete. Final v2v error: 4.726152420043945 mm

Highest mean error: 8.251864433288574 mm for frame 105

Lowest mean error: 3.609313726425171 mm for frame 74

Saving results

Total time: 160.71218252182007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790864
Iteration 2/25 | Loss: 0.00200441
Iteration 3/25 | Loss: 0.00156645
Iteration 4/25 | Loss: 0.00154561
Iteration 5/25 | Loss: 0.00154020
Iteration 6/25 | Loss: 0.00153904
Iteration 7/25 | Loss: 0.00153904
Iteration 8/25 | Loss: 0.00153904
Iteration 9/25 | Loss: 0.00153904
Iteration 10/25 | Loss: 0.00153904
Iteration 11/25 | Loss: 0.00153904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015390445478260517, 0.0015390445478260517, 0.0015390445478260517, 0.0015390445478260517, 0.0015390445478260517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015390445478260517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.25321013
Iteration 2/25 | Loss: 0.00098469
Iteration 3/25 | Loss: 0.00098469
Iteration 4/25 | Loss: 0.00098469
Iteration 5/25 | Loss: 0.00098469
Iteration 6/25 | Loss: 0.00098469
Iteration 7/25 | Loss: 0.00098468
Iteration 8/25 | Loss: 0.00098468
Iteration 9/25 | Loss: 0.00098468
Iteration 10/25 | Loss: 0.00098468
Iteration 11/25 | Loss: 0.00098468
Iteration 12/25 | Loss: 0.00098468
Iteration 13/25 | Loss: 0.00098468
Iteration 14/25 | Loss: 0.00098468
Iteration 15/25 | Loss: 0.00098468
Iteration 16/25 | Loss: 0.00098468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009846840985119343, 0.0009846840985119343, 0.0009846840985119343, 0.0009846840985119343, 0.0009846840985119343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009846840985119343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098468
Iteration 2/1000 | Loss: 0.00012316
Iteration 3/1000 | Loss: 0.00007093
Iteration 4/1000 | Loss: 0.00006204
Iteration 5/1000 | Loss: 0.00005934
Iteration 6/1000 | Loss: 0.00005763
Iteration 7/1000 | Loss: 0.00005650
Iteration 8/1000 | Loss: 0.00005544
Iteration 9/1000 | Loss: 0.00005454
Iteration 10/1000 | Loss: 0.00005385
Iteration 11/1000 | Loss: 0.00005324
Iteration 12/1000 | Loss: 0.00005277
Iteration 13/1000 | Loss: 0.00005229
Iteration 14/1000 | Loss: 0.00005195
Iteration 15/1000 | Loss: 0.00005161
Iteration 16/1000 | Loss: 0.00005137
Iteration 17/1000 | Loss: 0.00005112
Iteration 18/1000 | Loss: 0.00005092
Iteration 19/1000 | Loss: 0.00005069
Iteration 20/1000 | Loss: 0.00005049
Iteration 21/1000 | Loss: 0.00005033
Iteration 22/1000 | Loss: 0.00005024
Iteration 23/1000 | Loss: 0.00005017
Iteration 24/1000 | Loss: 0.00005014
Iteration 25/1000 | Loss: 0.00005004
Iteration 26/1000 | Loss: 0.00004999
Iteration 27/1000 | Loss: 0.00004998
Iteration 28/1000 | Loss: 0.00004993
Iteration 29/1000 | Loss: 0.00004992
Iteration 30/1000 | Loss: 0.00004989
Iteration 31/1000 | Loss: 0.00004988
Iteration 32/1000 | Loss: 0.00004988
Iteration 33/1000 | Loss: 0.00004988
Iteration 34/1000 | Loss: 0.00004988
Iteration 35/1000 | Loss: 0.00004988
Iteration 36/1000 | Loss: 0.00004988
Iteration 37/1000 | Loss: 0.00004987
Iteration 38/1000 | Loss: 0.00004985
Iteration 39/1000 | Loss: 0.00004985
Iteration 40/1000 | Loss: 0.00004985
Iteration 41/1000 | Loss: 0.00004985
Iteration 42/1000 | Loss: 0.00004984
Iteration 43/1000 | Loss: 0.00004983
Iteration 44/1000 | Loss: 0.00004983
Iteration 45/1000 | Loss: 0.00004982
Iteration 46/1000 | Loss: 0.00004982
Iteration 47/1000 | Loss: 0.00004982
Iteration 48/1000 | Loss: 0.00004982
Iteration 49/1000 | Loss: 0.00004982
Iteration 50/1000 | Loss: 0.00004981
Iteration 51/1000 | Loss: 0.00004980
Iteration 52/1000 | Loss: 0.00004980
Iteration 53/1000 | Loss: 0.00004980
Iteration 54/1000 | Loss: 0.00004980
Iteration 55/1000 | Loss: 0.00004980
Iteration 56/1000 | Loss: 0.00004980
Iteration 57/1000 | Loss: 0.00004980
Iteration 58/1000 | Loss: 0.00004980
Iteration 59/1000 | Loss: 0.00004980
Iteration 60/1000 | Loss: 0.00004980
Iteration 61/1000 | Loss: 0.00004979
Iteration 62/1000 | Loss: 0.00004979
Iteration 63/1000 | Loss: 0.00004979
Iteration 64/1000 | Loss: 0.00004979
Iteration 65/1000 | Loss: 0.00004979
Iteration 66/1000 | Loss: 0.00004979
Iteration 67/1000 | Loss: 0.00004979
Iteration 68/1000 | Loss: 0.00004979
Iteration 69/1000 | Loss: 0.00004979
Iteration 70/1000 | Loss: 0.00004978
Iteration 71/1000 | Loss: 0.00004978
Iteration 72/1000 | Loss: 0.00004977
Iteration 73/1000 | Loss: 0.00004977
Iteration 74/1000 | Loss: 0.00004977
Iteration 75/1000 | Loss: 0.00004977
Iteration 76/1000 | Loss: 0.00004977
Iteration 77/1000 | Loss: 0.00004976
Iteration 78/1000 | Loss: 0.00004976
Iteration 79/1000 | Loss: 0.00004976
Iteration 80/1000 | Loss: 0.00004976
Iteration 81/1000 | Loss: 0.00004976
Iteration 82/1000 | Loss: 0.00004974
Iteration 83/1000 | Loss: 0.00004974
Iteration 84/1000 | Loss: 0.00004974
Iteration 85/1000 | Loss: 0.00004973
Iteration 86/1000 | Loss: 0.00004973
Iteration 87/1000 | Loss: 0.00004973
Iteration 88/1000 | Loss: 0.00004972
Iteration 89/1000 | Loss: 0.00004972
Iteration 90/1000 | Loss: 0.00004972
Iteration 91/1000 | Loss: 0.00004972
Iteration 92/1000 | Loss: 0.00004971
Iteration 93/1000 | Loss: 0.00004971
Iteration 94/1000 | Loss: 0.00004971
Iteration 95/1000 | Loss: 0.00004970
Iteration 96/1000 | Loss: 0.00004970
Iteration 97/1000 | Loss: 0.00004970
Iteration 98/1000 | Loss: 0.00004970
Iteration 99/1000 | Loss: 0.00004970
Iteration 100/1000 | Loss: 0.00004970
Iteration 101/1000 | Loss: 0.00004969
Iteration 102/1000 | Loss: 0.00004969
Iteration 103/1000 | Loss: 0.00004969
Iteration 104/1000 | Loss: 0.00004969
Iteration 105/1000 | Loss: 0.00004969
Iteration 106/1000 | Loss: 0.00004968
Iteration 107/1000 | Loss: 0.00004968
Iteration 108/1000 | Loss: 0.00004968
Iteration 109/1000 | Loss: 0.00004968
Iteration 110/1000 | Loss: 0.00004967
Iteration 111/1000 | Loss: 0.00004967
Iteration 112/1000 | Loss: 0.00004967
Iteration 113/1000 | Loss: 0.00004967
Iteration 114/1000 | Loss: 0.00004967
Iteration 115/1000 | Loss: 0.00004967
Iteration 116/1000 | Loss: 0.00004967
Iteration 117/1000 | Loss: 0.00004967
Iteration 118/1000 | Loss: 0.00004967
Iteration 119/1000 | Loss: 0.00004967
Iteration 120/1000 | Loss: 0.00004966
Iteration 121/1000 | Loss: 0.00004966
Iteration 122/1000 | Loss: 0.00004966
Iteration 123/1000 | Loss: 0.00004966
Iteration 124/1000 | Loss: 0.00004966
Iteration 125/1000 | Loss: 0.00004966
Iteration 126/1000 | Loss: 0.00004966
Iteration 127/1000 | Loss: 0.00004965
Iteration 128/1000 | Loss: 0.00004965
Iteration 129/1000 | Loss: 0.00004965
Iteration 130/1000 | Loss: 0.00004965
Iteration 131/1000 | Loss: 0.00004965
Iteration 132/1000 | Loss: 0.00004965
Iteration 133/1000 | Loss: 0.00004965
Iteration 134/1000 | Loss: 0.00004965
Iteration 135/1000 | Loss: 0.00004964
Iteration 136/1000 | Loss: 0.00004964
Iteration 137/1000 | Loss: 0.00004964
Iteration 138/1000 | Loss: 0.00004964
Iteration 139/1000 | Loss: 0.00004964
Iteration 140/1000 | Loss: 0.00004964
Iteration 141/1000 | Loss: 0.00004964
Iteration 142/1000 | Loss: 0.00004964
Iteration 143/1000 | Loss: 0.00004964
Iteration 144/1000 | Loss: 0.00004964
Iteration 145/1000 | Loss: 0.00004964
Iteration 146/1000 | Loss: 0.00004964
Iteration 147/1000 | Loss: 0.00004964
Iteration 148/1000 | Loss: 0.00004964
Iteration 149/1000 | Loss: 0.00004964
Iteration 150/1000 | Loss: 0.00004964
Iteration 151/1000 | Loss: 0.00004964
Iteration 152/1000 | Loss: 0.00004964
Iteration 153/1000 | Loss: 0.00004964
Iteration 154/1000 | Loss: 0.00004964
Iteration 155/1000 | Loss: 0.00004964
Iteration 156/1000 | Loss: 0.00004964
Iteration 157/1000 | Loss: 0.00004964
Iteration 158/1000 | Loss: 0.00004964
Iteration 159/1000 | Loss: 0.00004964
Iteration 160/1000 | Loss: 0.00004964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [4.963502215105109e-05, 4.963502215105109e-05, 4.963502215105109e-05, 4.963502215105109e-05, 4.963502215105109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.963502215105109e-05

Optimization complete. Final v2v error: 5.866927146911621 mm

Highest mean error: 6.771102428436279 mm for frame 0

Lowest mean error: 5.230400562286377 mm for frame 152

Saving results

Total time: 60.4415385723114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078764
Iteration 2/25 | Loss: 0.00285575
Iteration 3/25 | Loss: 0.00211192
Iteration 4/25 | Loss: 0.00197798
Iteration 5/25 | Loss: 0.00188213
Iteration 6/25 | Loss: 0.00180482
Iteration 7/25 | Loss: 0.00176327
Iteration 8/25 | Loss: 0.00177511
Iteration 9/25 | Loss: 0.00172494
Iteration 10/25 | Loss: 0.00169881
Iteration 11/25 | Loss: 0.00168679
Iteration 12/25 | Loss: 0.00167374
Iteration 13/25 | Loss: 0.00165699
Iteration 14/25 | Loss: 0.00165163
Iteration 15/25 | Loss: 0.00164170
Iteration 16/25 | Loss: 0.00164252
Iteration 17/25 | Loss: 0.00163892
Iteration 18/25 | Loss: 0.00165612
Iteration 19/25 | Loss: 0.00163794
Iteration 20/25 | Loss: 0.00158134
Iteration 21/25 | Loss: 0.00157352
Iteration 22/25 | Loss: 0.00157179
Iteration 23/25 | Loss: 0.00157277
Iteration 24/25 | Loss: 0.00159050
Iteration 25/25 | Loss: 0.00158785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45252764
Iteration 2/25 | Loss: 0.00172111
Iteration 3/25 | Loss: 0.00172111
Iteration 4/25 | Loss: 0.00172111
Iteration 5/25 | Loss: 0.00172111
Iteration 6/25 | Loss: 0.00172111
Iteration 7/25 | Loss: 0.00172111
Iteration 8/25 | Loss: 0.00172111
Iteration 9/25 | Loss: 0.00172111
Iteration 10/25 | Loss: 0.00172111
Iteration 11/25 | Loss: 0.00172111
Iteration 12/25 | Loss: 0.00172111
Iteration 13/25 | Loss: 0.00172111
Iteration 14/25 | Loss: 0.00172111
Iteration 15/25 | Loss: 0.00172111
Iteration 16/25 | Loss: 0.00172111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017211108934134245, 0.0017211108934134245, 0.0017211108934134245, 0.0017211108934134245, 0.0017211108934134245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017211108934134245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172111
Iteration 2/1000 | Loss: 0.00034072
Iteration 3/1000 | Loss: 0.00014594
Iteration 4/1000 | Loss: 0.00070460
Iteration 5/1000 | Loss: 0.00008044
Iteration 6/1000 | Loss: 0.00006334
Iteration 7/1000 | Loss: 0.00005438
Iteration 8/1000 | Loss: 0.00005505
Iteration 9/1000 | Loss: 0.00004699
Iteration 10/1000 | Loss: 0.00004354
Iteration 11/1000 | Loss: 0.00004182
Iteration 12/1000 | Loss: 0.00044906
Iteration 13/1000 | Loss: 0.00036998
Iteration 14/1000 | Loss: 0.00008410
Iteration 15/1000 | Loss: 0.00005028
Iteration 16/1000 | Loss: 0.00004148
Iteration 17/1000 | Loss: 0.00022009
Iteration 18/1000 | Loss: 0.00013063
Iteration 19/1000 | Loss: 0.00005813
Iteration 20/1000 | Loss: 0.00003537
Iteration 21/1000 | Loss: 0.00008584
Iteration 22/1000 | Loss: 0.00003378
Iteration 23/1000 | Loss: 0.00014437
Iteration 24/1000 | Loss: 0.00003311
Iteration 25/1000 | Loss: 0.00003272
Iteration 26/1000 | Loss: 0.00017908
Iteration 27/1000 | Loss: 0.00071049
Iteration 28/1000 | Loss: 0.00025655
Iteration 29/1000 | Loss: 0.00021339
Iteration 30/1000 | Loss: 0.00003313
Iteration 31/1000 | Loss: 0.00003239
Iteration 32/1000 | Loss: 0.00003212
Iteration 33/1000 | Loss: 0.00003201
Iteration 34/1000 | Loss: 0.00016622
Iteration 35/1000 | Loss: 0.00003199
Iteration 36/1000 | Loss: 0.00003177
Iteration 37/1000 | Loss: 0.00003173
Iteration 38/1000 | Loss: 0.00003162
Iteration 39/1000 | Loss: 0.00003146
Iteration 40/1000 | Loss: 0.00003144
Iteration 41/1000 | Loss: 0.00003144
Iteration 42/1000 | Loss: 0.00003143
Iteration 43/1000 | Loss: 0.00003142
Iteration 44/1000 | Loss: 0.00026950
Iteration 45/1000 | Loss: 0.00003453
Iteration 46/1000 | Loss: 0.00010998
Iteration 47/1000 | Loss: 0.00003175
Iteration 48/1000 | Loss: 0.00003129
Iteration 49/1000 | Loss: 0.00003123
Iteration 50/1000 | Loss: 0.00003115
Iteration 51/1000 | Loss: 0.00003111
Iteration 52/1000 | Loss: 0.00003110
Iteration 53/1000 | Loss: 0.00003110
Iteration 54/1000 | Loss: 0.00003109
Iteration 55/1000 | Loss: 0.00003109
Iteration 56/1000 | Loss: 0.00003109
Iteration 57/1000 | Loss: 0.00003108
Iteration 58/1000 | Loss: 0.00003107
Iteration 59/1000 | Loss: 0.00003105
Iteration 60/1000 | Loss: 0.00003101
Iteration 61/1000 | Loss: 0.00003101
Iteration 62/1000 | Loss: 0.00003097
Iteration 63/1000 | Loss: 0.00003093
Iteration 64/1000 | Loss: 0.00003092
Iteration 65/1000 | Loss: 0.00003092
Iteration 66/1000 | Loss: 0.00082333
Iteration 67/1000 | Loss: 0.00017001
Iteration 68/1000 | Loss: 0.00006605
Iteration 69/1000 | Loss: 0.00009453
Iteration 70/1000 | Loss: 0.00007400
Iteration 71/1000 | Loss: 0.00038041
Iteration 72/1000 | Loss: 0.00004764
Iteration 73/1000 | Loss: 0.00009626
Iteration 74/1000 | Loss: 0.00003346
Iteration 75/1000 | Loss: 0.00004778
Iteration 76/1000 | Loss: 0.00006478
Iteration 77/1000 | Loss: 0.00014563
Iteration 78/1000 | Loss: 0.00010572
Iteration 79/1000 | Loss: 0.00006862
Iteration 80/1000 | Loss: 0.00003524
Iteration 81/1000 | Loss: 0.00003182
Iteration 82/1000 | Loss: 0.00003163
Iteration 83/1000 | Loss: 0.00003160
Iteration 84/1000 | Loss: 0.00003156
Iteration 85/1000 | Loss: 0.00003149
Iteration 86/1000 | Loss: 0.00003143
Iteration 87/1000 | Loss: 0.00003143
Iteration 88/1000 | Loss: 0.00003143
Iteration 89/1000 | Loss: 0.00003142
Iteration 90/1000 | Loss: 0.00003142
Iteration 91/1000 | Loss: 0.00003142
Iteration 92/1000 | Loss: 0.00003141
Iteration 93/1000 | Loss: 0.00003141
Iteration 94/1000 | Loss: 0.00003140
Iteration 95/1000 | Loss: 0.00003140
Iteration 96/1000 | Loss: 0.00003139
Iteration 97/1000 | Loss: 0.00003139
Iteration 98/1000 | Loss: 0.00055409
Iteration 99/1000 | Loss: 0.00003741
Iteration 100/1000 | Loss: 0.00003205
Iteration 101/1000 | Loss: 0.00003043
Iteration 102/1000 | Loss: 0.00002926
Iteration 103/1000 | Loss: 0.00002855
Iteration 104/1000 | Loss: 0.00002825
Iteration 105/1000 | Loss: 0.00002816
Iteration 106/1000 | Loss: 0.00002815
Iteration 107/1000 | Loss: 0.00002808
Iteration 108/1000 | Loss: 0.00002789
Iteration 109/1000 | Loss: 0.00002773
Iteration 110/1000 | Loss: 0.00002772
Iteration 111/1000 | Loss: 0.00002770
Iteration 112/1000 | Loss: 0.00002769
Iteration 113/1000 | Loss: 0.00002769
Iteration 114/1000 | Loss: 0.00002763
Iteration 115/1000 | Loss: 0.00002763
Iteration 116/1000 | Loss: 0.00002762
Iteration 117/1000 | Loss: 0.00002761
Iteration 118/1000 | Loss: 0.00002761
Iteration 119/1000 | Loss: 0.00002759
Iteration 120/1000 | Loss: 0.00002759
Iteration 121/1000 | Loss: 0.00002759
Iteration 122/1000 | Loss: 0.00002758
Iteration 123/1000 | Loss: 0.00002758
Iteration 124/1000 | Loss: 0.00002758
Iteration 125/1000 | Loss: 0.00002757
Iteration 126/1000 | Loss: 0.00002757
Iteration 127/1000 | Loss: 0.00002757
Iteration 128/1000 | Loss: 0.00002756
Iteration 129/1000 | Loss: 0.00002756
Iteration 130/1000 | Loss: 0.00002756
Iteration 131/1000 | Loss: 0.00002756
Iteration 132/1000 | Loss: 0.00002756
Iteration 133/1000 | Loss: 0.00002755
Iteration 134/1000 | Loss: 0.00002755
Iteration 135/1000 | Loss: 0.00002755
Iteration 136/1000 | Loss: 0.00002755
Iteration 137/1000 | Loss: 0.00002755
Iteration 138/1000 | Loss: 0.00002755
Iteration 139/1000 | Loss: 0.00002755
Iteration 140/1000 | Loss: 0.00002754
Iteration 141/1000 | Loss: 0.00002754
Iteration 142/1000 | Loss: 0.00002754
Iteration 143/1000 | Loss: 0.00002754
Iteration 144/1000 | Loss: 0.00002753
Iteration 145/1000 | Loss: 0.00002753
Iteration 146/1000 | Loss: 0.00002753
Iteration 147/1000 | Loss: 0.00002753
Iteration 148/1000 | Loss: 0.00002753
Iteration 149/1000 | Loss: 0.00002753
Iteration 150/1000 | Loss: 0.00002753
Iteration 151/1000 | Loss: 0.00002753
Iteration 152/1000 | Loss: 0.00002753
Iteration 153/1000 | Loss: 0.00002753
Iteration 154/1000 | Loss: 0.00002753
Iteration 155/1000 | Loss: 0.00002753
Iteration 156/1000 | Loss: 0.00002752
Iteration 157/1000 | Loss: 0.00002752
Iteration 158/1000 | Loss: 0.00002752
Iteration 159/1000 | Loss: 0.00002752
Iteration 160/1000 | Loss: 0.00002752
Iteration 161/1000 | Loss: 0.00002752
Iteration 162/1000 | Loss: 0.00002752
Iteration 163/1000 | Loss: 0.00002752
Iteration 164/1000 | Loss: 0.00002752
Iteration 165/1000 | Loss: 0.00002752
Iteration 166/1000 | Loss: 0.00002752
Iteration 167/1000 | Loss: 0.00002752
Iteration 168/1000 | Loss: 0.00002752
Iteration 169/1000 | Loss: 0.00002752
Iteration 170/1000 | Loss: 0.00002752
Iteration 171/1000 | Loss: 0.00002752
Iteration 172/1000 | Loss: 0.00002752
Iteration 173/1000 | Loss: 0.00002752
Iteration 174/1000 | Loss: 0.00002752
Iteration 175/1000 | Loss: 0.00002752
Iteration 176/1000 | Loss: 0.00002752
Iteration 177/1000 | Loss: 0.00002752
Iteration 178/1000 | Loss: 0.00002751
Iteration 179/1000 | Loss: 0.00002751
Iteration 180/1000 | Loss: 0.00002751
Iteration 181/1000 | Loss: 0.00002751
Iteration 182/1000 | Loss: 0.00002751
Iteration 183/1000 | Loss: 0.00002751
Iteration 184/1000 | Loss: 0.00002751
Iteration 185/1000 | Loss: 0.00002751
Iteration 186/1000 | Loss: 0.00002751
Iteration 187/1000 | Loss: 0.00002751
Iteration 188/1000 | Loss: 0.00002751
Iteration 189/1000 | Loss: 0.00002751
Iteration 190/1000 | Loss: 0.00002751
Iteration 191/1000 | Loss: 0.00002751
Iteration 192/1000 | Loss: 0.00002751
Iteration 193/1000 | Loss: 0.00002751
Iteration 194/1000 | Loss: 0.00002751
Iteration 195/1000 | Loss: 0.00002751
Iteration 196/1000 | Loss: 0.00002751
Iteration 197/1000 | Loss: 0.00002751
Iteration 198/1000 | Loss: 0.00002751
Iteration 199/1000 | Loss: 0.00002751
Iteration 200/1000 | Loss: 0.00002751
Iteration 201/1000 | Loss: 0.00002751
Iteration 202/1000 | Loss: 0.00002751
Iteration 203/1000 | Loss: 0.00002750
Iteration 204/1000 | Loss: 0.00002750
Iteration 205/1000 | Loss: 0.00002750
Iteration 206/1000 | Loss: 0.00002750
Iteration 207/1000 | Loss: 0.00002750
Iteration 208/1000 | Loss: 0.00002750
Iteration 209/1000 | Loss: 0.00002750
Iteration 210/1000 | Loss: 0.00002750
Iteration 211/1000 | Loss: 0.00002750
Iteration 212/1000 | Loss: 0.00002750
Iteration 213/1000 | Loss: 0.00002750
Iteration 214/1000 | Loss: 0.00002750
Iteration 215/1000 | Loss: 0.00002750
Iteration 216/1000 | Loss: 0.00002750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.7503747332957573e-05, 2.7503747332957573e-05, 2.7503747332957573e-05, 2.7503747332957573e-05, 2.7503747332957573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7503747332957573e-05

Optimization complete. Final v2v error: 4.53069543838501 mm

Highest mean error: 5.443197727203369 mm for frame 35

Lowest mean error: 3.5481884479522705 mm for frame 0

Saving results

Total time: 162.1759114265442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876585
Iteration 2/25 | Loss: 0.00159909
Iteration 3/25 | Loss: 0.00147903
Iteration 4/25 | Loss: 0.00145401
Iteration 5/25 | Loss: 0.00144615
Iteration 6/25 | Loss: 0.00144449
Iteration 7/25 | Loss: 0.00144445
Iteration 8/25 | Loss: 0.00144445
Iteration 9/25 | Loss: 0.00144445
Iteration 10/25 | Loss: 0.00144445
Iteration 11/25 | Loss: 0.00144445
Iteration 12/25 | Loss: 0.00144445
Iteration 13/25 | Loss: 0.00144445
Iteration 14/25 | Loss: 0.00144445
Iteration 15/25 | Loss: 0.00144445
Iteration 16/25 | Loss: 0.00144445
Iteration 17/25 | Loss: 0.00144445
Iteration 18/25 | Loss: 0.00144445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014444472035393119, 0.0014444472035393119, 0.0014444472035393119, 0.0014444472035393119, 0.0014444472035393119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014444472035393119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.19741344
Iteration 2/25 | Loss: 0.00087531
Iteration 3/25 | Loss: 0.00087504
Iteration 4/25 | Loss: 0.00087504
Iteration 5/25 | Loss: 0.00087504
Iteration 6/25 | Loss: 0.00087504
Iteration 7/25 | Loss: 0.00087504
Iteration 8/25 | Loss: 0.00087504
Iteration 9/25 | Loss: 0.00087504
Iteration 10/25 | Loss: 0.00087504
Iteration 11/25 | Loss: 0.00087504
Iteration 12/25 | Loss: 0.00087504
Iteration 13/25 | Loss: 0.00087504
Iteration 14/25 | Loss: 0.00087504
Iteration 15/25 | Loss: 0.00087504
Iteration 16/25 | Loss: 0.00087504
Iteration 17/25 | Loss: 0.00087504
Iteration 18/25 | Loss: 0.00087504
Iteration 19/25 | Loss: 0.00087504
Iteration 20/25 | Loss: 0.00087504
Iteration 21/25 | Loss: 0.00087504
Iteration 22/25 | Loss: 0.00087504
Iteration 23/25 | Loss: 0.00087504
Iteration 24/25 | Loss: 0.00087504
Iteration 25/25 | Loss: 0.00087504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087504
Iteration 2/1000 | Loss: 0.00007611
Iteration 3/1000 | Loss: 0.00004435
Iteration 4/1000 | Loss: 0.00003206
Iteration 5/1000 | Loss: 0.00002940
Iteration 6/1000 | Loss: 0.00002761
Iteration 7/1000 | Loss: 0.00002660
Iteration 8/1000 | Loss: 0.00002580
Iteration 9/1000 | Loss: 0.00002520
Iteration 10/1000 | Loss: 0.00002459
Iteration 11/1000 | Loss: 0.00002421
Iteration 12/1000 | Loss: 0.00002395
Iteration 13/1000 | Loss: 0.00002373
Iteration 14/1000 | Loss: 0.00002355
Iteration 15/1000 | Loss: 0.00002355
Iteration 16/1000 | Loss: 0.00002342
Iteration 17/1000 | Loss: 0.00002331
Iteration 18/1000 | Loss: 0.00002322
Iteration 19/1000 | Loss: 0.00002313
Iteration 20/1000 | Loss: 0.00002311
Iteration 21/1000 | Loss: 0.00002310
Iteration 22/1000 | Loss: 0.00002309
Iteration 23/1000 | Loss: 0.00002308
Iteration 24/1000 | Loss: 0.00002308
Iteration 25/1000 | Loss: 0.00002307
Iteration 26/1000 | Loss: 0.00002307
Iteration 27/1000 | Loss: 0.00002307
Iteration 28/1000 | Loss: 0.00002306
Iteration 29/1000 | Loss: 0.00002306
Iteration 30/1000 | Loss: 0.00002306
Iteration 31/1000 | Loss: 0.00002305
Iteration 32/1000 | Loss: 0.00002305
Iteration 33/1000 | Loss: 0.00002305
Iteration 34/1000 | Loss: 0.00002304
Iteration 35/1000 | Loss: 0.00002304
Iteration 36/1000 | Loss: 0.00002304
Iteration 37/1000 | Loss: 0.00002304
Iteration 38/1000 | Loss: 0.00002304
Iteration 39/1000 | Loss: 0.00002303
Iteration 40/1000 | Loss: 0.00002303
Iteration 41/1000 | Loss: 0.00002303
Iteration 42/1000 | Loss: 0.00002303
Iteration 43/1000 | Loss: 0.00002303
Iteration 44/1000 | Loss: 0.00002303
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002302
Iteration 47/1000 | Loss: 0.00002302
Iteration 48/1000 | Loss: 0.00002302
Iteration 49/1000 | Loss: 0.00002302
Iteration 50/1000 | Loss: 0.00002302
Iteration 51/1000 | Loss: 0.00002302
Iteration 52/1000 | Loss: 0.00002301
Iteration 53/1000 | Loss: 0.00002301
Iteration 54/1000 | Loss: 0.00002301
Iteration 55/1000 | Loss: 0.00002301
Iteration 56/1000 | Loss: 0.00002300
Iteration 57/1000 | Loss: 0.00002300
Iteration 58/1000 | Loss: 0.00002300
Iteration 59/1000 | Loss: 0.00002300
Iteration 60/1000 | Loss: 0.00002300
Iteration 61/1000 | Loss: 0.00002299
Iteration 62/1000 | Loss: 0.00002299
Iteration 63/1000 | Loss: 0.00002299
Iteration 64/1000 | Loss: 0.00002299
Iteration 65/1000 | Loss: 0.00002298
Iteration 66/1000 | Loss: 0.00002298
Iteration 67/1000 | Loss: 0.00002298
Iteration 68/1000 | Loss: 0.00002298
Iteration 69/1000 | Loss: 0.00002298
Iteration 70/1000 | Loss: 0.00002298
Iteration 71/1000 | Loss: 0.00002297
Iteration 72/1000 | Loss: 0.00002297
Iteration 73/1000 | Loss: 0.00002297
Iteration 74/1000 | Loss: 0.00002297
Iteration 75/1000 | Loss: 0.00002297
Iteration 76/1000 | Loss: 0.00002297
Iteration 77/1000 | Loss: 0.00002297
Iteration 78/1000 | Loss: 0.00002297
Iteration 79/1000 | Loss: 0.00002297
Iteration 80/1000 | Loss: 0.00002296
Iteration 81/1000 | Loss: 0.00002296
Iteration 82/1000 | Loss: 0.00002296
Iteration 83/1000 | Loss: 0.00002296
Iteration 84/1000 | Loss: 0.00002296
Iteration 85/1000 | Loss: 0.00002296
Iteration 86/1000 | Loss: 0.00002296
Iteration 87/1000 | Loss: 0.00002295
Iteration 88/1000 | Loss: 0.00002295
Iteration 89/1000 | Loss: 0.00002295
Iteration 90/1000 | Loss: 0.00002295
Iteration 91/1000 | Loss: 0.00002295
Iteration 92/1000 | Loss: 0.00002295
Iteration 93/1000 | Loss: 0.00002295
Iteration 94/1000 | Loss: 0.00002295
Iteration 95/1000 | Loss: 0.00002294
Iteration 96/1000 | Loss: 0.00002294
Iteration 97/1000 | Loss: 0.00002294
Iteration 98/1000 | Loss: 0.00002294
Iteration 99/1000 | Loss: 0.00002294
Iteration 100/1000 | Loss: 0.00002294
Iteration 101/1000 | Loss: 0.00002294
Iteration 102/1000 | Loss: 0.00002294
Iteration 103/1000 | Loss: 0.00002294
Iteration 104/1000 | Loss: 0.00002293
Iteration 105/1000 | Loss: 0.00002293
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Iteration 108/1000 | Loss: 0.00002292
Iteration 109/1000 | Loss: 0.00002292
Iteration 110/1000 | Loss: 0.00002292
Iteration 111/1000 | Loss: 0.00002292
Iteration 112/1000 | Loss: 0.00002292
Iteration 113/1000 | Loss: 0.00002291
Iteration 114/1000 | Loss: 0.00002291
Iteration 115/1000 | Loss: 0.00002291
Iteration 116/1000 | Loss: 0.00002290
Iteration 117/1000 | Loss: 0.00002290
Iteration 118/1000 | Loss: 0.00002290
Iteration 119/1000 | Loss: 0.00002290
Iteration 120/1000 | Loss: 0.00002290
Iteration 121/1000 | Loss: 0.00002290
Iteration 122/1000 | Loss: 0.00002289
Iteration 123/1000 | Loss: 0.00002289
Iteration 124/1000 | Loss: 0.00002289
Iteration 125/1000 | Loss: 0.00002289
Iteration 126/1000 | Loss: 0.00002289
Iteration 127/1000 | Loss: 0.00002289
Iteration 128/1000 | Loss: 0.00002288
Iteration 129/1000 | Loss: 0.00002288
Iteration 130/1000 | Loss: 0.00002288
Iteration 131/1000 | Loss: 0.00002288
Iteration 132/1000 | Loss: 0.00002288
Iteration 133/1000 | Loss: 0.00002288
Iteration 134/1000 | Loss: 0.00002287
Iteration 135/1000 | Loss: 0.00002287
Iteration 136/1000 | Loss: 0.00002287
Iteration 137/1000 | Loss: 0.00002287
Iteration 138/1000 | Loss: 0.00002287
Iteration 139/1000 | Loss: 0.00002287
Iteration 140/1000 | Loss: 0.00002287
Iteration 141/1000 | Loss: 0.00002287
Iteration 142/1000 | Loss: 0.00002287
Iteration 143/1000 | Loss: 0.00002287
Iteration 144/1000 | Loss: 0.00002287
Iteration 145/1000 | Loss: 0.00002287
Iteration 146/1000 | Loss: 0.00002286
Iteration 147/1000 | Loss: 0.00002286
Iteration 148/1000 | Loss: 0.00002286
Iteration 149/1000 | Loss: 0.00002286
Iteration 150/1000 | Loss: 0.00002286
Iteration 151/1000 | Loss: 0.00002286
Iteration 152/1000 | Loss: 0.00002286
Iteration 153/1000 | Loss: 0.00002286
Iteration 154/1000 | Loss: 0.00002286
Iteration 155/1000 | Loss: 0.00002286
Iteration 156/1000 | Loss: 0.00002286
Iteration 157/1000 | Loss: 0.00002286
Iteration 158/1000 | Loss: 0.00002286
Iteration 159/1000 | Loss: 0.00002286
Iteration 160/1000 | Loss: 0.00002286
Iteration 161/1000 | Loss: 0.00002285
Iteration 162/1000 | Loss: 0.00002285
Iteration 163/1000 | Loss: 0.00002285
Iteration 164/1000 | Loss: 0.00002285
Iteration 165/1000 | Loss: 0.00002285
Iteration 166/1000 | Loss: 0.00002285
Iteration 167/1000 | Loss: 0.00002285
Iteration 168/1000 | Loss: 0.00002285
Iteration 169/1000 | Loss: 0.00002284
Iteration 170/1000 | Loss: 0.00002284
Iteration 171/1000 | Loss: 0.00002284
Iteration 172/1000 | Loss: 0.00002284
Iteration 173/1000 | Loss: 0.00002284
Iteration 174/1000 | Loss: 0.00002284
Iteration 175/1000 | Loss: 0.00002284
Iteration 176/1000 | Loss: 0.00002284
Iteration 177/1000 | Loss: 0.00002284
Iteration 178/1000 | Loss: 0.00002283
Iteration 179/1000 | Loss: 0.00002283
Iteration 180/1000 | Loss: 0.00002283
Iteration 181/1000 | Loss: 0.00002283
Iteration 182/1000 | Loss: 0.00002283
Iteration 183/1000 | Loss: 0.00002283
Iteration 184/1000 | Loss: 0.00002283
Iteration 185/1000 | Loss: 0.00002283
Iteration 186/1000 | Loss: 0.00002283
Iteration 187/1000 | Loss: 0.00002283
Iteration 188/1000 | Loss: 0.00002283
Iteration 189/1000 | Loss: 0.00002283
Iteration 190/1000 | Loss: 0.00002283
Iteration 191/1000 | Loss: 0.00002283
Iteration 192/1000 | Loss: 0.00002283
Iteration 193/1000 | Loss: 0.00002283
Iteration 194/1000 | Loss: 0.00002283
Iteration 195/1000 | Loss: 0.00002283
Iteration 196/1000 | Loss: 0.00002282
Iteration 197/1000 | Loss: 0.00002282
Iteration 198/1000 | Loss: 0.00002282
Iteration 199/1000 | Loss: 0.00002282
Iteration 200/1000 | Loss: 0.00002282
Iteration 201/1000 | Loss: 0.00002282
Iteration 202/1000 | Loss: 0.00002282
Iteration 203/1000 | Loss: 0.00002282
Iteration 204/1000 | Loss: 0.00002281
Iteration 205/1000 | Loss: 0.00002281
Iteration 206/1000 | Loss: 0.00002281
Iteration 207/1000 | Loss: 0.00002281
Iteration 208/1000 | Loss: 0.00002281
Iteration 209/1000 | Loss: 0.00002281
Iteration 210/1000 | Loss: 0.00002281
Iteration 211/1000 | Loss: 0.00002281
Iteration 212/1000 | Loss: 0.00002281
Iteration 213/1000 | Loss: 0.00002281
Iteration 214/1000 | Loss: 0.00002281
Iteration 215/1000 | Loss: 0.00002281
Iteration 216/1000 | Loss: 0.00002281
Iteration 217/1000 | Loss: 0.00002281
Iteration 218/1000 | Loss: 0.00002281
Iteration 219/1000 | Loss: 0.00002281
Iteration 220/1000 | Loss: 0.00002281
Iteration 221/1000 | Loss: 0.00002281
Iteration 222/1000 | Loss: 0.00002281
Iteration 223/1000 | Loss: 0.00002281
Iteration 224/1000 | Loss: 0.00002281
Iteration 225/1000 | Loss: 0.00002281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.2807014829595573e-05, 2.2807014829595573e-05, 2.2807014829595573e-05, 2.2807014829595573e-05, 2.2807014829595573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2807014829595573e-05

Optimization complete. Final v2v error: 4.132187366485596 mm

Highest mean error: 5.283555030822754 mm for frame 55

Lowest mean error: 3.734718084335327 mm for frame 144

Saving results

Total time: 49.20252704620361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01143432
Iteration 2/25 | Loss: 0.01143432
Iteration 3/25 | Loss: 0.01143432
Iteration 4/25 | Loss: 0.01143431
Iteration 5/25 | Loss: 0.01143431
Iteration 6/25 | Loss: 0.01143431
Iteration 7/25 | Loss: 0.01143431
Iteration 8/25 | Loss: 0.01143431
Iteration 9/25 | Loss: 0.01143431
Iteration 10/25 | Loss: 0.01143431
Iteration 11/25 | Loss: 0.00326902
Iteration 12/25 | Loss: 0.00224919
Iteration 13/25 | Loss: 0.00223227
Iteration 14/25 | Loss: 0.00203912
Iteration 15/25 | Loss: 0.00196946
Iteration 16/25 | Loss: 0.00190603
Iteration 17/25 | Loss: 0.00179788
Iteration 18/25 | Loss: 0.00168625
Iteration 19/25 | Loss: 0.00166311
Iteration 20/25 | Loss: 0.00165539
Iteration 21/25 | Loss: 0.00162490
Iteration 22/25 | Loss: 0.00161409
Iteration 23/25 | Loss: 0.00161482
Iteration 24/25 | Loss: 0.00160663
Iteration 25/25 | Loss: 0.00161286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38289237
Iteration 2/25 | Loss: 0.00384381
Iteration 3/25 | Loss: 0.00364395
Iteration 4/25 | Loss: 0.00364394
Iteration 5/25 | Loss: 0.00364394
Iteration 6/25 | Loss: 0.00364394
Iteration 7/25 | Loss: 0.00364394
Iteration 8/25 | Loss: 0.00364394
Iteration 9/25 | Loss: 0.00364394
Iteration 10/25 | Loss: 0.00364394
Iteration 11/25 | Loss: 0.00364394
Iteration 12/25 | Loss: 0.00364394
Iteration 13/25 | Loss: 0.00364394
Iteration 14/25 | Loss: 0.00364394
Iteration 15/25 | Loss: 0.00364394
Iteration 16/25 | Loss: 0.00364394
Iteration 17/25 | Loss: 0.00364394
Iteration 18/25 | Loss: 0.00364394
Iteration 19/25 | Loss: 0.00364394
Iteration 20/25 | Loss: 0.00364394
Iteration 21/25 | Loss: 0.00364394
Iteration 22/25 | Loss: 0.00364394
Iteration 23/25 | Loss: 0.00364394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003643943229690194, 0.003643943229690194, 0.003643943229690194, 0.003643943229690194, 0.003643943229690194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003643943229690194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00364394
Iteration 2/1000 | Loss: 0.00158406
Iteration 3/1000 | Loss: 0.00194184
Iteration 4/1000 | Loss: 0.00432108
Iteration 5/1000 | Loss: 0.01036198
Iteration 6/1000 | Loss: 0.00073033
Iteration 7/1000 | Loss: 0.00227438
Iteration 8/1000 | Loss: 0.00202802
Iteration 9/1000 | Loss: 0.00151298
Iteration 10/1000 | Loss: 0.00181435
Iteration 11/1000 | Loss: 0.00160467
Iteration 12/1000 | Loss: 0.00103907
Iteration 13/1000 | Loss: 0.00040633
Iteration 14/1000 | Loss: 0.00113127
Iteration 15/1000 | Loss: 0.00187313
Iteration 16/1000 | Loss: 0.00071493
Iteration 17/1000 | Loss: 0.00109370
Iteration 18/1000 | Loss: 0.00201622
Iteration 19/1000 | Loss: 0.00141938
Iteration 20/1000 | Loss: 0.00103984
Iteration 21/1000 | Loss: 0.00033963
Iteration 22/1000 | Loss: 0.00148113
Iteration 23/1000 | Loss: 0.00144908
Iteration 24/1000 | Loss: 0.00199227
Iteration 25/1000 | Loss: 0.00078603
Iteration 26/1000 | Loss: 0.00120339
Iteration 27/1000 | Loss: 0.00128876
Iteration 28/1000 | Loss: 0.00370744
Iteration 29/1000 | Loss: 0.00196223
Iteration 30/1000 | Loss: 0.00371136
Iteration 31/1000 | Loss: 0.00277162
Iteration 32/1000 | Loss: 0.00489364
Iteration 33/1000 | Loss: 0.00220577
Iteration 34/1000 | Loss: 0.00322289
Iteration 35/1000 | Loss: 0.00147340
Iteration 36/1000 | Loss: 0.00137934
Iteration 37/1000 | Loss: 0.00229364
Iteration 38/1000 | Loss: 0.00045957
Iteration 39/1000 | Loss: 0.00239200
Iteration 40/1000 | Loss: 0.00159780
Iteration 41/1000 | Loss: 0.00111445
Iteration 42/1000 | Loss: 0.00114218
Iteration 43/1000 | Loss: 0.00136249
Iteration 44/1000 | Loss: 0.00118345
Iteration 45/1000 | Loss: 0.00056185
Iteration 46/1000 | Loss: 0.00074510
Iteration 47/1000 | Loss: 0.00054115
Iteration 48/1000 | Loss: 0.00089308
Iteration 49/1000 | Loss: 0.00091819
Iteration 50/1000 | Loss: 0.00058612
Iteration 51/1000 | Loss: 0.00036157
Iteration 52/1000 | Loss: 0.00051367
Iteration 53/1000 | Loss: 0.00047920
Iteration 54/1000 | Loss: 0.00074854
Iteration 55/1000 | Loss: 0.00032397
Iteration 56/1000 | Loss: 0.00028420
Iteration 57/1000 | Loss: 0.00108000
Iteration 58/1000 | Loss: 0.00089230
Iteration 59/1000 | Loss: 0.00047678
Iteration 60/1000 | Loss: 0.00032532
Iteration 61/1000 | Loss: 0.00077168
Iteration 62/1000 | Loss: 0.00018648
Iteration 63/1000 | Loss: 0.00106133
Iteration 64/1000 | Loss: 0.00071353
Iteration 65/1000 | Loss: 0.00040922
Iteration 66/1000 | Loss: 0.00013825
Iteration 67/1000 | Loss: 0.00059238
Iteration 68/1000 | Loss: 0.00053267
Iteration 69/1000 | Loss: 0.00105331
Iteration 70/1000 | Loss: 0.00073527
Iteration 71/1000 | Loss: 0.00139886
Iteration 72/1000 | Loss: 0.00041498
Iteration 73/1000 | Loss: 0.00036468
Iteration 74/1000 | Loss: 0.00033970
Iteration 75/1000 | Loss: 0.00051684
Iteration 76/1000 | Loss: 0.00044165
Iteration 77/1000 | Loss: 0.00021165
Iteration 78/1000 | Loss: 0.00020655
Iteration 79/1000 | Loss: 0.00045713
Iteration 80/1000 | Loss: 0.00033133
Iteration 81/1000 | Loss: 0.00045486
Iteration 82/1000 | Loss: 0.00098521
Iteration 83/1000 | Loss: 0.00098805
Iteration 84/1000 | Loss: 0.00040657
Iteration 85/1000 | Loss: 0.00029880
Iteration 86/1000 | Loss: 0.00020559
Iteration 87/1000 | Loss: 0.00026372
Iteration 88/1000 | Loss: 0.00032366
Iteration 89/1000 | Loss: 0.00018305
Iteration 90/1000 | Loss: 0.00092249
Iteration 91/1000 | Loss: 0.00026467
Iteration 92/1000 | Loss: 0.00078805
Iteration 93/1000 | Loss: 0.00083226
Iteration 94/1000 | Loss: 0.00031337
Iteration 95/1000 | Loss: 0.00069447
Iteration 96/1000 | Loss: 0.00106380
Iteration 97/1000 | Loss: 0.00050772
Iteration 98/1000 | Loss: 0.00044243
Iteration 99/1000 | Loss: 0.00076369
Iteration 100/1000 | Loss: 0.00091513
Iteration 101/1000 | Loss: 0.00077464
Iteration 102/1000 | Loss: 0.00066553
Iteration 103/1000 | Loss: 0.00043719
Iteration 104/1000 | Loss: 0.00076159
Iteration 105/1000 | Loss: 0.00039418
Iteration 106/1000 | Loss: 0.00017472
Iteration 107/1000 | Loss: 0.00015926
Iteration 108/1000 | Loss: 0.00019226
Iteration 109/1000 | Loss: 0.00024491
Iteration 110/1000 | Loss: 0.00009450
Iteration 111/1000 | Loss: 0.00011020
Iteration 112/1000 | Loss: 0.00068994
Iteration 113/1000 | Loss: 0.00030485
Iteration 114/1000 | Loss: 0.00042244
Iteration 115/1000 | Loss: 0.00059414
Iteration 116/1000 | Loss: 0.00032962
Iteration 117/1000 | Loss: 0.00031141
Iteration 118/1000 | Loss: 0.00048116
Iteration 119/1000 | Loss: 0.00054107
Iteration 120/1000 | Loss: 0.00047996
Iteration 121/1000 | Loss: 0.00172775
Iteration 122/1000 | Loss: 0.00044730
Iteration 123/1000 | Loss: 0.00078878
Iteration 124/1000 | Loss: 0.00038207
Iteration 125/1000 | Loss: 0.00042968
Iteration 126/1000 | Loss: 0.00024003
Iteration 127/1000 | Loss: 0.00019556
Iteration 128/1000 | Loss: 0.00018773
Iteration 129/1000 | Loss: 0.00008584
Iteration 130/1000 | Loss: 0.00015810
Iteration 131/1000 | Loss: 0.00049216
Iteration 132/1000 | Loss: 0.00026560
Iteration 133/1000 | Loss: 0.00067258
Iteration 134/1000 | Loss: 0.00110193
Iteration 135/1000 | Loss: 0.00048487
Iteration 136/1000 | Loss: 0.00008236
Iteration 137/1000 | Loss: 0.00008111
Iteration 138/1000 | Loss: 0.00024523
Iteration 139/1000 | Loss: 0.00037488
Iteration 140/1000 | Loss: 0.00107738
Iteration 141/1000 | Loss: 0.00036789
Iteration 142/1000 | Loss: 0.00033601
Iteration 143/1000 | Loss: 0.00044809
Iteration 144/1000 | Loss: 0.00037643
Iteration 145/1000 | Loss: 0.00053685
Iteration 146/1000 | Loss: 0.00039414
Iteration 147/1000 | Loss: 0.00014931
Iteration 148/1000 | Loss: 0.00040748
Iteration 149/1000 | Loss: 0.00025748
Iteration 150/1000 | Loss: 0.00086894
Iteration 151/1000 | Loss: 0.00040681
Iteration 152/1000 | Loss: 0.00022570
Iteration 153/1000 | Loss: 0.00025052
Iteration 154/1000 | Loss: 0.00029572
Iteration 155/1000 | Loss: 0.00025300
Iteration 156/1000 | Loss: 0.00040802
Iteration 157/1000 | Loss: 0.00032948
Iteration 158/1000 | Loss: 0.00066531
Iteration 159/1000 | Loss: 0.00015903
Iteration 160/1000 | Loss: 0.00011393
Iteration 161/1000 | Loss: 0.00064454
Iteration 162/1000 | Loss: 0.00047691
Iteration 163/1000 | Loss: 0.00019100
Iteration 164/1000 | Loss: 0.00009740
Iteration 165/1000 | Loss: 0.00007382
Iteration 166/1000 | Loss: 0.00010072
Iteration 167/1000 | Loss: 0.00013981
Iteration 168/1000 | Loss: 0.00007792
Iteration 169/1000 | Loss: 0.00007099
Iteration 170/1000 | Loss: 0.00033197
Iteration 171/1000 | Loss: 0.00010547
Iteration 172/1000 | Loss: 0.00005795
Iteration 173/1000 | Loss: 0.00015190
Iteration 174/1000 | Loss: 0.00008156
Iteration 175/1000 | Loss: 0.00063370
Iteration 176/1000 | Loss: 0.00019658
Iteration 177/1000 | Loss: 0.00020210
Iteration 178/1000 | Loss: 0.00041781
Iteration 179/1000 | Loss: 0.00010052
Iteration 180/1000 | Loss: 0.00050399
Iteration 181/1000 | Loss: 0.00049651
Iteration 182/1000 | Loss: 0.00007669
Iteration 183/1000 | Loss: 0.00008737
Iteration 184/1000 | Loss: 0.00007127
Iteration 185/1000 | Loss: 0.00041712
Iteration 186/1000 | Loss: 0.00020605
Iteration 187/1000 | Loss: 0.00080489
Iteration 188/1000 | Loss: 0.00129592
Iteration 189/1000 | Loss: 0.00109640
Iteration 190/1000 | Loss: 0.00021794
Iteration 191/1000 | Loss: 0.00036845
Iteration 192/1000 | Loss: 0.00054926
Iteration 193/1000 | Loss: 0.00036451
Iteration 194/1000 | Loss: 0.00023230
Iteration 195/1000 | Loss: 0.00083003
Iteration 196/1000 | Loss: 0.00148354
Iteration 197/1000 | Loss: 0.00022218
Iteration 198/1000 | Loss: 0.00095836
Iteration 199/1000 | Loss: 0.00047685
Iteration 200/1000 | Loss: 0.00007804
Iteration 201/1000 | Loss: 0.00035472
Iteration 202/1000 | Loss: 0.00010017
Iteration 203/1000 | Loss: 0.00008459
Iteration 204/1000 | Loss: 0.00008152
Iteration 205/1000 | Loss: 0.00026903
Iteration 206/1000 | Loss: 0.00043671
Iteration 207/1000 | Loss: 0.00067710
Iteration 208/1000 | Loss: 0.00092692
Iteration 209/1000 | Loss: 0.00031712
Iteration 210/1000 | Loss: 0.00015668
Iteration 211/1000 | Loss: 0.00020490
Iteration 212/1000 | Loss: 0.00005514
Iteration 213/1000 | Loss: 0.00007944
Iteration 214/1000 | Loss: 0.00005574
Iteration 215/1000 | Loss: 0.00005206
Iteration 216/1000 | Loss: 0.00035857
Iteration 217/1000 | Loss: 0.00035019
Iteration 218/1000 | Loss: 0.00045048
Iteration 219/1000 | Loss: 0.00054720
Iteration 220/1000 | Loss: 0.00036557
Iteration 221/1000 | Loss: 0.00035873
Iteration 222/1000 | Loss: 0.00011949
Iteration 223/1000 | Loss: 0.00061689
Iteration 224/1000 | Loss: 0.00022517
Iteration 225/1000 | Loss: 0.00032721
Iteration 226/1000 | Loss: 0.00020677
Iteration 227/1000 | Loss: 0.00008628
Iteration 228/1000 | Loss: 0.00005430
Iteration 229/1000 | Loss: 0.00005608
Iteration 230/1000 | Loss: 0.00006010
Iteration 231/1000 | Loss: 0.00008089
Iteration 232/1000 | Loss: 0.00004976
Iteration 233/1000 | Loss: 0.00007620
Iteration 234/1000 | Loss: 0.00006404
Iteration 235/1000 | Loss: 0.00004566
Iteration 236/1000 | Loss: 0.00004504
Iteration 237/1000 | Loss: 0.00004776
Iteration 238/1000 | Loss: 0.00004411
Iteration 239/1000 | Loss: 0.00004376
Iteration 240/1000 | Loss: 0.00059442
Iteration 241/1000 | Loss: 0.00017069
Iteration 242/1000 | Loss: 0.00005602
Iteration 243/1000 | Loss: 0.00006975
Iteration 244/1000 | Loss: 0.00004843
Iteration 245/1000 | Loss: 0.00005316
Iteration 246/1000 | Loss: 0.00004835
Iteration 247/1000 | Loss: 0.00004481
Iteration 248/1000 | Loss: 0.00004419
Iteration 249/1000 | Loss: 0.00004378
Iteration 250/1000 | Loss: 0.00004369
Iteration 251/1000 | Loss: 0.00004510
Iteration 252/1000 | Loss: 0.00004357
Iteration 253/1000 | Loss: 0.00004342
Iteration 254/1000 | Loss: 0.00004322
Iteration 255/1000 | Loss: 0.00032437
Iteration 256/1000 | Loss: 0.00005193
Iteration 257/1000 | Loss: 0.00004620
Iteration 258/1000 | Loss: 0.00004375
Iteration 259/1000 | Loss: 0.00004233
Iteration 260/1000 | Loss: 0.00004145
Iteration 261/1000 | Loss: 0.00004106
Iteration 262/1000 | Loss: 0.00004092
Iteration 263/1000 | Loss: 0.00004076
Iteration 264/1000 | Loss: 0.00004075
Iteration 265/1000 | Loss: 0.00004063
Iteration 266/1000 | Loss: 0.00004062
Iteration 267/1000 | Loss: 0.00004062
Iteration 268/1000 | Loss: 0.00004062
Iteration 269/1000 | Loss: 0.00004062
Iteration 270/1000 | Loss: 0.00004062
Iteration 271/1000 | Loss: 0.00004062
Iteration 272/1000 | Loss: 0.00004062
Iteration 273/1000 | Loss: 0.00004062
Iteration 274/1000 | Loss: 0.00004061
Iteration 275/1000 | Loss: 0.00004060
Iteration 276/1000 | Loss: 0.00004060
Iteration 277/1000 | Loss: 0.00004060
Iteration 278/1000 | Loss: 0.00004059
Iteration 279/1000 | Loss: 0.00004059
Iteration 280/1000 | Loss: 0.00004058
Iteration 281/1000 | Loss: 0.00004058
Iteration 282/1000 | Loss: 0.00004057
Iteration 283/1000 | Loss: 0.00004057
Iteration 284/1000 | Loss: 0.00004057
Iteration 285/1000 | Loss: 0.00004056
Iteration 286/1000 | Loss: 0.00004056
Iteration 287/1000 | Loss: 0.00004056
Iteration 288/1000 | Loss: 0.00004056
Iteration 289/1000 | Loss: 0.00004055
Iteration 290/1000 | Loss: 0.00004055
Iteration 291/1000 | Loss: 0.00004055
Iteration 292/1000 | Loss: 0.00004055
Iteration 293/1000 | Loss: 0.00004055
Iteration 294/1000 | Loss: 0.00004055
Iteration 295/1000 | Loss: 0.00004054
Iteration 296/1000 | Loss: 0.00004054
Iteration 297/1000 | Loss: 0.00004054
Iteration 298/1000 | Loss: 0.00004054
Iteration 299/1000 | Loss: 0.00004054
Iteration 300/1000 | Loss: 0.00004054
Iteration 301/1000 | Loss: 0.00004054
Iteration 302/1000 | Loss: 0.00004054
Iteration 303/1000 | Loss: 0.00004054
Iteration 304/1000 | Loss: 0.00004054
Iteration 305/1000 | Loss: 0.00004054
Iteration 306/1000 | Loss: 0.00004054
Iteration 307/1000 | Loss: 0.00004053
Iteration 308/1000 | Loss: 0.00004053
Iteration 309/1000 | Loss: 0.00004053
Iteration 310/1000 | Loss: 0.00004053
Iteration 311/1000 | Loss: 0.00004053
Iteration 312/1000 | Loss: 0.00004053
Iteration 313/1000 | Loss: 0.00004052
Iteration 314/1000 | Loss: 0.00004052
Iteration 315/1000 | Loss: 0.00004052
Iteration 316/1000 | Loss: 0.00004052
Iteration 317/1000 | Loss: 0.00004052
Iteration 318/1000 | Loss: 0.00004052
Iteration 319/1000 | Loss: 0.00004052
Iteration 320/1000 | Loss: 0.00004051
Iteration 321/1000 | Loss: 0.00004051
Iteration 322/1000 | Loss: 0.00004051
Iteration 323/1000 | Loss: 0.00004051
Iteration 324/1000 | Loss: 0.00004051
Iteration 325/1000 | Loss: 0.00004051
Iteration 326/1000 | Loss: 0.00004051
Iteration 327/1000 | Loss: 0.00004051
Iteration 328/1000 | Loss: 0.00004051
Iteration 329/1000 | Loss: 0.00004051
Iteration 330/1000 | Loss: 0.00004051
Iteration 331/1000 | Loss: 0.00004051
Iteration 332/1000 | Loss: 0.00004051
Iteration 333/1000 | Loss: 0.00004051
Iteration 334/1000 | Loss: 0.00004051
Iteration 335/1000 | Loss: 0.00004051
Iteration 336/1000 | Loss: 0.00004051
Iteration 337/1000 | Loss: 0.00004051
Iteration 338/1000 | Loss: 0.00004051
Iteration 339/1000 | Loss: 0.00004051
Iteration 340/1000 | Loss: 0.00004051
Iteration 341/1000 | Loss: 0.00004050
Iteration 342/1000 | Loss: 0.00004050
Iteration 343/1000 | Loss: 0.00004050
Iteration 344/1000 | Loss: 0.00004050
Iteration 345/1000 | Loss: 0.00004050
Iteration 346/1000 | Loss: 0.00004050
Iteration 347/1000 | Loss: 0.00004050
Iteration 348/1000 | Loss: 0.00004050
Iteration 349/1000 | Loss: 0.00004050
Iteration 350/1000 | Loss: 0.00004050
Iteration 351/1000 | Loss: 0.00004050
Iteration 352/1000 | Loss: 0.00004050
Iteration 353/1000 | Loss: 0.00004050
Iteration 354/1000 | Loss: 0.00004049
Iteration 355/1000 | Loss: 0.00004049
Iteration 356/1000 | Loss: 0.00004049
Iteration 357/1000 | Loss: 0.00004049
Iteration 358/1000 | Loss: 0.00004049
Iteration 359/1000 | Loss: 0.00004049
Iteration 360/1000 | Loss: 0.00004049
Iteration 361/1000 | Loss: 0.00004049
Iteration 362/1000 | Loss: 0.00004049
Iteration 363/1000 | Loss: 0.00004049
Iteration 364/1000 | Loss: 0.00004049
Iteration 365/1000 | Loss: 0.00004049
Iteration 366/1000 | Loss: 0.00004049
Iteration 367/1000 | Loss: 0.00004049
Iteration 368/1000 | Loss: 0.00004049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 368. Stopping optimization.
Last 5 losses: [4.048907430842519e-05, 4.048907430842519e-05, 4.048907430842519e-05, 4.048907430842519e-05, 4.048907430842519e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.048907430842519e-05

Optimization complete. Final v2v error: 4.365833759307861 mm

Highest mean error: 21.492586135864258 mm for frame 201

Lowest mean error: 3.3067450523376465 mm for frame 113

Saving results

Total time: 462.23366141319275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844528
Iteration 2/25 | Loss: 0.00165640
Iteration 3/25 | Loss: 0.00149175
Iteration 4/25 | Loss: 0.00145803
Iteration 5/25 | Loss: 0.00144458
Iteration 6/25 | Loss: 0.00144188
Iteration 7/25 | Loss: 0.00144147
Iteration 8/25 | Loss: 0.00144147
Iteration 9/25 | Loss: 0.00144147
Iteration 10/25 | Loss: 0.00144147
Iteration 11/25 | Loss: 0.00144147
Iteration 12/25 | Loss: 0.00144147
Iteration 13/25 | Loss: 0.00144147
Iteration 14/25 | Loss: 0.00144147
Iteration 15/25 | Loss: 0.00144147
Iteration 16/25 | Loss: 0.00144147
Iteration 17/25 | Loss: 0.00144147
Iteration 18/25 | Loss: 0.00144147
Iteration 19/25 | Loss: 0.00144147
Iteration 20/25 | Loss: 0.00144147
Iteration 21/25 | Loss: 0.00144147
Iteration 22/25 | Loss: 0.00144147
Iteration 23/25 | Loss: 0.00144147
Iteration 24/25 | Loss: 0.00144147
Iteration 25/25 | Loss: 0.00144147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40589333
Iteration 2/25 | Loss: 0.00087061
Iteration 3/25 | Loss: 0.00087061
Iteration 4/25 | Loss: 0.00087061
Iteration 5/25 | Loss: 0.00087060
Iteration 6/25 | Loss: 0.00087060
Iteration 7/25 | Loss: 0.00087060
Iteration 8/25 | Loss: 0.00087060
Iteration 9/25 | Loss: 0.00087060
Iteration 10/25 | Loss: 0.00087060
Iteration 11/25 | Loss: 0.00087060
Iteration 12/25 | Loss: 0.00087060
Iteration 13/25 | Loss: 0.00087060
Iteration 14/25 | Loss: 0.00087060
Iteration 15/25 | Loss: 0.00087060
Iteration 16/25 | Loss: 0.00087060
Iteration 17/25 | Loss: 0.00087060
Iteration 18/25 | Loss: 0.00087060
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008706028456799686, 0.0008706028456799686, 0.0008706028456799686, 0.0008706028456799686, 0.0008706028456799686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008706028456799686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087060
Iteration 2/1000 | Loss: 0.00006799
Iteration 3/1000 | Loss: 0.00003911
Iteration 4/1000 | Loss: 0.00003361
Iteration 5/1000 | Loss: 0.00003127
Iteration 6/1000 | Loss: 0.00002999
Iteration 7/1000 | Loss: 0.00002910
Iteration 8/1000 | Loss: 0.00002847
Iteration 9/1000 | Loss: 0.00002773
Iteration 10/1000 | Loss: 0.00002733
Iteration 11/1000 | Loss: 0.00002699
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00002657
Iteration 14/1000 | Loss: 0.00002645
Iteration 15/1000 | Loss: 0.00002638
Iteration 16/1000 | Loss: 0.00002631
Iteration 17/1000 | Loss: 0.00002628
Iteration 18/1000 | Loss: 0.00002627
Iteration 19/1000 | Loss: 0.00002627
Iteration 20/1000 | Loss: 0.00002623
Iteration 21/1000 | Loss: 0.00002623
Iteration 22/1000 | Loss: 0.00002621
Iteration 23/1000 | Loss: 0.00002621
Iteration 24/1000 | Loss: 0.00002621
Iteration 25/1000 | Loss: 0.00002620
Iteration 26/1000 | Loss: 0.00002620
Iteration 27/1000 | Loss: 0.00002619
Iteration 28/1000 | Loss: 0.00002617
Iteration 29/1000 | Loss: 0.00002617
Iteration 30/1000 | Loss: 0.00002616
Iteration 31/1000 | Loss: 0.00002616
Iteration 32/1000 | Loss: 0.00002616
Iteration 33/1000 | Loss: 0.00002616
Iteration 34/1000 | Loss: 0.00002615
Iteration 35/1000 | Loss: 0.00002615
Iteration 36/1000 | Loss: 0.00002615
Iteration 37/1000 | Loss: 0.00002614
Iteration 38/1000 | Loss: 0.00002614
Iteration 39/1000 | Loss: 0.00002613
Iteration 40/1000 | Loss: 0.00002613
Iteration 41/1000 | Loss: 0.00002613
Iteration 42/1000 | Loss: 0.00002613
Iteration 43/1000 | Loss: 0.00002612
Iteration 44/1000 | Loss: 0.00002612
Iteration 45/1000 | Loss: 0.00002612
Iteration 46/1000 | Loss: 0.00002612
Iteration 47/1000 | Loss: 0.00002612
Iteration 48/1000 | Loss: 0.00002612
Iteration 49/1000 | Loss: 0.00002611
Iteration 50/1000 | Loss: 0.00002611
Iteration 51/1000 | Loss: 0.00002611
Iteration 52/1000 | Loss: 0.00002610
Iteration 53/1000 | Loss: 0.00002610
Iteration 54/1000 | Loss: 0.00002610
Iteration 55/1000 | Loss: 0.00002610
Iteration 56/1000 | Loss: 0.00002609
Iteration 57/1000 | Loss: 0.00002609
Iteration 58/1000 | Loss: 0.00002609
Iteration 59/1000 | Loss: 0.00002609
Iteration 60/1000 | Loss: 0.00002609
Iteration 61/1000 | Loss: 0.00002609
Iteration 62/1000 | Loss: 0.00002609
Iteration 63/1000 | Loss: 0.00002609
Iteration 64/1000 | Loss: 0.00002609
Iteration 65/1000 | Loss: 0.00002608
Iteration 66/1000 | Loss: 0.00002608
Iteration 67/1000 | Loss: 0.00002608
Iteration 68/1000 | Loss: 0.00002607
Iteration 69/1000 | Loss: 0.00002607
Iteration 70/1000 | Loss: 0.00002607
Iteration 71/1000 | Loss: 0.00002606
Iteration 72/1000 | Loss: 0.00002606
Iteration 73/1000 | Loss: 0.00002606
Iteration 74/1000 | Loss: 0.00002606
Iteration 75/1000 | Loss: 0.00002606
Iteration 76/1000 | Loss: 0.00002606
Iteration 77/1000 | Loss: 0.00002606
Iteration 78/1000 | Loss: 0.00002606
Iteration 79/1000 | Loss: 0.00002606
Iteration 80/1000 | Loss: 0.00002605
Iteration 81/1000 | Loss: 0.00002605
Iteration 82/1000 | Loss: 0.00002605
Iteration 83/1000 | Loss: 0.00002605
Iteration 84/1000 | Loss: 0.00002605
Iteration 85/1000 | Loss: 0.00002605
Iteration 86/1000 | Loss: 0.00002605
Iteration 87/1000 | Loss: 0.00002605
Iteration 88/1000 | Loss: 0.00002604
Iteration 89/1000 | Loss: 0.00002604
Iteration 90/1000 | Loss: 0.00002604
Iteration 91/1000 | Loss: 0.00002604
Iteration 92/1000 | Loss: 0.00002604
Iteration 93/1000 | Loss: 0.00002604
Iteration 94/1000 | Loss: 0.00002604
Iteration 95/1000 | Loss: 0.00002604
Iteration 96/1000 | Loss: 0.00002604
Iteration 97/1000 | Loss: 0.00002604
Iteration 98/1000 | Loss: 0.00002604
Iteration 99/1000 | Loss: 0.00002604
Iteration 100/1000 | Loss: 0.00002604
Iteration 101/1000 | Loss: 0.00002603
Iteration 102/1000 | Loss: 0.00002603
Iteration 103/1000 | Loss: 0.00002603
Iteration 104/1000 | Loss: 0.00002603
Iteration 105/1000 | Loss: 0.00002603
Iteration 106/1000 | Loss: 0.00002603
Iteration 107/1000 | Loss: 0.00002603
Iteration 108/1000 | Loss: 0.00002603
Iteration 109/1000 | Loss: 0.00002603
Iteration 110/1000 | Loss: 0.00002603
Iteration 111/1000 | Loss: 0.00002603
Iteration 112/1000 | Loss: 0.00002603
Iteration 113/1000 | Loss: 0.00002603
Iteration 114/1000 | Loss: 0.00002603
Iteration 115/1000 | Loss: 0.00002603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.60343003901653e-05, 2.60343003901653e-05, 2.60343003901653e-05, 2.60343003901653e-05, 2.60343003901653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.60343003901653e-05

Optimization complete. Final v2v error: 4.331376552581787 mm

Highest mean error: 5.053187370300293 mm for frame 21

Lowest mean error: 3.690659999847412 mm for frame 171

Saving results

Total time: 40.326345682144165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420832
Iteration 2/25 | Loss: 0.00137625
Iteration 3/25 | Loss: 0.00130438
Iteration 4/25 | Loss: 0.00129728
Iteration 5/25 | Loss: 0.00129421
Iteration 6/25 | Loss: 0.00129364
Iteration 7/25 | Loss: 0.00129364
Iteration 8/25 | Loss: 0.00129364
Iteration 9/25 | Loss: 0.00129364
Iteration 10/25 | Loss: 0.00129364
Iteration 11/25 | Loss: 0.00129364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012936440762132406, 0.0012936440762132406, 0.0012936440762132406, 0.0012936440762132406, 0.0012936440762132406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012936440762132406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41935813
Iteration 2/25 | Loss: 0.00072698
Iteration 3/25 | Loss: 0.00072698
Iteration 4/25 | Loss: 0.00072698
Iteration 5/25 | Loss: 0.00072698
Iteration 6/25 | Loss: 0.00072698
Iteration 7/25 | Loss: 0.00072698
Iteration 8/25 | Loss: 0.00072698
Iteration 9/25 | Loss: 0.00072698
Iteration 10/25 | Loss: 0.00072698
Iteration 11/25 | Loss: 0.00072698
Iteration 12/25 | Loss: 0.00072698
Iteration 13/25 | Loss: 0.00072698
Iteration 14/25 | Loss: 0.00072698
Iteration 15/25 | Loss: 0.00072698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007269788184203207, 0.0007269788184203207, 0.0007269788184203207, 0.0007269788184203207, 0.0007269788184203207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007269788184203207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072698
Iteration 2/1000 | Loss: 0.00003593
Iteration 3/1000 | Loss: 0.00001760
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001386
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001350
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001344
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001344
Iteration 22/1000 | Loss: 0.00001344
Iteration 23/1000 | Loss: 0.00001343
Iteration 24/1000 | Loss: 0.00001343
Iteration 25/1000 | Loss: 0.00001340
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001333
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001329
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001328
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001328
Iteration 67/1000 | Loss: 0.00001328
Iteration 68/1000 | Loss: 0.00001328
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001328
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001328
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.3283102816785686e-05, 1.3283102816785686e-05, 1.3283102816785686e-05, 1.3283102816785686e-05, 1.3283102816785686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3283102816785686e-05

Optimization complete. Final v2v error: 3.1280665397644043 mm

Highest mean error: 3.369652271270752 mm for frame 95

Lowest mean error: 2.928671360015869 mm for frame 121

Saving results

Total time: 27.416870594024658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962812
Iteration 2/25 | Loss: 0.00372749
Iteration 3/25 | Loss: 0.00321237
Iteration 4/25 | Loss: 0.00277837
Iteration 5/25 | Loss: 0.00214095
Iteration 6/25 | Loss: 0.00194306
Iteration 7/25 | Loss: 0.00187346
Iteration 8/25 | Loss: 0.00185024
Iteration 9/25 | Loss: 0.00182736
Iteration 10/25 | Loss: 0.00181750
Iteration 11/25 | Loss: 0.00177013
Iteration 12/25 | Loss: 0.00177239
Iteration 13/25 | Loss: 0.00154656
Iteration 14/25 | Loss: 0.00131657
Iteration 15/25 | Loss: 0.00127577
Iteration 16/25 | Loss: 0.00127378
Iteration 17/25 | Loss: 0.00117883
Iteration 18/25 | Loss: 0.00113762
Iteration 19/25 | Loss: 0.00113405
Iteration 20/25 | Loss: 0.00112841
Iteration 21/25 | Loss: 0.00112084
Iteration 22/25 | Loss: 0.00111781
Iteration 23/25 | Loss: 0.00111722
Iteration 24/25 | Loss: 0.00112031
Iteration 25/25 | Loss: 0.00111528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37513089
Iteration 2/25 | Loss: 0.00123169
Iteration 3/25 | Loss: 0.00123169
Iteration 4/25 | Loss: 0.00123169
Iteration 5/25 | Loss: 0.00123169
Iteration 6/25 | Loss: 0.00123169
Iteration 7/25 | Loss: 0.00123169
Iteration 8/25 | Loss: 0.00123169
Iteration 9/25 | Loss: 0.00123169
Iteration 10/25 | Loss: 0.00123169
Iteration 11/25 | Loss: 0.00123169
Iteration 12/25 | Loss: 0.00123169
Iteration 13/25 | Loss: 0.00123169
Iteration 14/25 | Loss: 0.00123169
Iteration 15/25 | Loss: 0.00123169
Iteration 16/25 | Loss: 0.00123169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012316869106143713, 0.0012316869106143713, 0.0012316869106143713, 0.0012316869106143713, 0.0012316869106143713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012316869106143713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123169
Iteration 2/1000 | Loss: 0.00019844
Iteration 3/1000 | Loss: 0.00014487
Iteration 4/1000 | Loss: 0.00012437
Iteration 5/1000 | Loss: 0.00011024
Iteration 6/1000 | Loss: 0.00010219
Iteration 7/1000 | Loss: 0.00009563
Iteration 8/1000 | Loss: 0.00009201
Iteration 9/1000 | Loss: 0.00008906
Iteration 10/1000 | Loss: 0.00008639
Iteration 11/1000 | Loss: 0.00008348
Iteration 12/1000 | Loss: 0.00008096
Iteration 13/1000 | Loss: 0.00007942
Iteration 14/1000 | Loss: 0.00007795
Iteration 15/1000 | Loss: 0.00007640
Iteration 16/1000 | Loss: 0.00327821
Iteration 17/1000 | Loss: 0.00773579
Iteration 18/1000 | Loss: 0.00119240
Iteration 19/1000 | Loss: 0.00015292
Iteration 20/1000 | Loss: 0.00161186
Iteration 21/1000 | Loss: 0.00023607
Iteration 22/1000 | Loss: 0.00058368
Iteration 23/1000 | Loss: 0.00065348
Iteration 24/1000 | Loss: 0.00030494
Iteration 25/1000 | Loss: 0.00012818
Iteration 26/1000 | Loss: 0.00009149
Iteration 27/1000 | Loss: 0.00326297
Iteration 28/1000 | Loss: 0.00983039
Iteration 29/1000 | Loss: 0.00731128
Iteration 30/1000 | Loss: 0.00421319
Iteration 31/1000 | Loss: 0.00031484
Iteration 32/1000 | Loss: 0.00450001
Iteration 33/1000 | Loss: 0.00134781
Iteration 34/1000 | Loss: 0.00088347
Iteration 35/1000 | Loss: 0.00101627
Iteration 36/1000 | Loss: 0.00109573
Iteration 37/1000 | Loss: 0.00028852
Iteration 38/1000 | Loss: 0.00010763
Iteration 39/1000 | Loss: 0.00053920
Iteration 40/1000 | Loss: 0.00056854
Iteration 41/1000 | Loss: 0.00007425
Iteration 42/1000 | Loss: 0.00006388
Iteration 43/1000 | Loss: 0.00006313
Iteration 44/1000 | Loss: 0.00040237
Iteration 45/1000 | Loss: 0.00323369
Iteration 46/1000 | Loss: 0.00548257
Iteration 47/1000 | Loss: 0.00351126
Iteration 48/1000 | Loss: 0.00377621
Iteration 49/1000 | Loss: 0.00390107
Iteration 50/1000 | Loss: 0.00429595
Iteration 51/1000 | Loss: 0.00248644
Iteration 52/1000 | Loss: 0.00226219
Iteration 53/1000 | Loss: 0.00142707
Iteration 54/1000 | Loss: 0.00041838
Iteration 55/1000 | Loss: 0.00024706
Iteration 56/1000 | Loss: 0.00080398
Iteration 57/1000 | Loss: 0.00279368
Iteration 58/1000 | Loss: 0.00008573
Iteration 59/1000 | Loss: 0.00074308
Iteration 60/1000 | Loss: 0.00013287
Iteration 61/1000 | Loss: 0.00016555
Iteration 62/1000 | Loss: 0.00005305
Iteration 63/1000 | Loss: 0.00027479
Iteration 64/1000 | Loss: 0.00105363
Iteration 65/1000 | Loss: 0.00052900
Iteration 66/1000 | Loss: 0.00064781
Iteration 67/1000 | Loss: 0.00073533
Iteration 68/1000 | Loss: 0.00080540
Iteration 69/1000 | Loss: 0.00152328
Iteration 70/1000 | Loss: 0.00110049
Iteration 71/1000 | Loss: 0.00023132
Iteration 72/1000 | Loss: 0.00007154
Iteration 73/1000 | Loss: 0.00004881
Iteration 74/1000 | Loss: 0.00003785
Iteration 75/1000 | Loss: 0.00011587
Iteration 76/1000 | Loss: 0.00003205
Iteration 77/1000 | Loss: 0.00002863
Iteration 78/1000 | Loss: 0.00002661
Iteration 79/1000 | Loss: 0.00002461
Iteration 80/1000 | Loss: 0.00002365
Iteration 81/1000 | Loss: 0.00027235
Iteration 82/1000 | Loss: 0.00014492
Iteration 83/1000 | Loss: 0.00002293
Iteration 84/1000 | Loss: 0.00002642
Iteration 85/1000 | Loss: 0.00002539
Iteration 86/1000 | Loss: 0.00002208
Iteration 87/1000 | Loss: 0.00002186
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002922
Iteration 90/1000 | Loss: 0.00002786
Iteration 91/1000 | Loss: 0.00002652
Iteration 92/1000 | Loss: 0.00002164
Iteration 93/1000 | Loss: 0.00002365
Iteration 94/1000 | Loss: 0.00002130
Iteration 95/1000 | Loss: 0.00002130
Iteration 96/1000 | Loss: 0.00002130
Iteration 97/1000 | Loss: 0.00002130
Iteration 98/1000 | Loss: 0.00002130
Iteration 99/1000 | Loss: 0.00002130
Iteration 100/1000 | Loss: 0.00002130
Iteration 101/1000 | Loss: 0.00002130
Iteration 102/1000 | Loss: 0.00002130
Iteration 103/1000 | Loss: 0.00002129
Iteration 104/1000 | Loss: 0.00002129
Iteration 105/1000 | Loss: 0.00002129
Iteration 106/1000 | Loss: 0.00002129
Iteration 107/1000 | Loss: 0.00002128
Iteration 108/1000 | Loss: 0.00002127
Iteration 109/1000 | Loss: 0.00002127
Iteration 110/1000 | Loss: 0.00002126
Iteration 111/1000 | Loss: 0.00002126
Iteration 112/1000 | Loss: 0.00002125
Iteration 113/1000 | Loss: 0.00002353
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002123
Iteration 124/1000 | Loss: 0.00002123
Iteration 125/1000 | Loss: 0.00002123
Iteration 126/1000 | Loss: 0.00002123
Iteration 127/1000 | Loss: 0.00002123
Iteration 128/1000 | Loss: 0.00002123
Iteration 129/1000 | Loss: 0.00002123
Iteration 130/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.1233521692920476e-05, 2.1233521692920476e-05, 2.1233521692920476e-05, 2.1233521692920476e-05, 2.1233521692920476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1233521692920476e-05

Optimization complete. Final v2v error: 3.490818977355957 mm

Highest mean error: 10.221362113952637 mm for frame 145

Lowest mean error: 2.8362033367156982 mm for frame 48

Saving results

Total time: 186.2973554134369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581174
Iteration 2/25 | Loss: 0.00161999
Iteration 3/25 | Loss: 0.00142750
Iteration 4/25 | Loss: 0.00141315
Iteration 5/25 | Loss: 0.00140909
Iteration 6/25 | Loss: 0.00140818
Iteration 7/25 | Loss: 0.00140805
Iteration 8/25 | Loss: 0.00140805
Iteration 9/25 | Loss: 0.00140805
Iteration 10/25 | Loss: 0.00140805
Iteration 11/25 | Loss: 0.00140805
Iteration 12/25 | Loss: 0.00140805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001408053794875741, 0.001408053794875741, 0.001408053794875741, 0.001408053794875741, 0.001408053794875741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001408053794875741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79678077
Iteration 2/25 | Loss: 0.00118853
Iteration 3/25 | Loss: 0.00118852
Iteration 4/25 | Loss: 0.00118852
Iteration 5/25 | Loss: 0.00118852
Iteration 6/25 | Loss: 0.00118852
Iteration 7/25 | Loss: 0.00118852
Iteration 8/25 | Loss: 0.00118852
Iteration 9/25 | Loss: 0.00118852
Iteration 10/25 | Loss: 0.00118852
Iteration 11/25 | Loss: 0.00118852
Iteration 12/25 | Loss: 0.00118852
Iteration 13/25 | Loss: 0.00118852
Iteration 14/25 | Loss: 0.00118852
Iteration 15/25 | Loss: 0.00118852
Iteration 16/25 | Loss: 0.00118852
Iteration 17/25 | Loss: 0.00118852
Iteration 18/25 | Loss: 0.00118852
Iteration 19/25 | Loss: 0.00118852
Iteration 20/25 | Loss: 0.00118852
Iteration 21/25 | Loss: 0.00118852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011885181302204728, 0.0011885181302204728, 0.0011885181302204728, 0.0011885181302204728, 0.0011885181302204728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011885181302204728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118852
Iteration 2/1000 | Loss: 0.00008597
Iteration 3/1000 | Loss: 0.00005608
Iteration 4/1000 | Loss: 0.00004926
Iteration 5/1000 | Loss: 0.00004576
Iteration 6/1000 | Loss: 0.00004440
Iteration 7/1000 | Loss: 0.00004357
Iteration 8/1000 | Loss: 0.00004259
Iteration 9/1000 | Loss: 0.00004193
Iteration 10/1000 | Loss: 0.00004153
Iteration 11/1000 | Loss: 0.00004107
Iteration 12/1000 | Loss: 0.00004060
Iteration 13/1000 | Loss: 0.00004026
Iteration 14/1000 | Loss: 0.00003989
Iteration 15/1000 | Loss: 0.00003952
Iteration 16/1000 | Loss: 0.00003916
Iteration 17/1000 | Loss: 0.00003876
Iteration 18/1000 | Loss: 0.00003842
Iteration 19/1000 | Loss: 0.00003823
Iteration 20/1000 | Loss: 0.00003799
Iteration 21/1000 | Loss: 0.00003784
Iteration 22/1000 | Loss: 0.00003768
Iteration 23/1000 | Loss: 0.00003761
Iteration 24/1000 | Loss: 0.00003751
Iteration 25/1000 | Loss: 0.00003750
Iteration 26/1000 | Loss: 0.00003750
Iteration 27/1000 | Loss: 0.00003750
Iteration 28/1000 | Loss: 0.00003749
Iteration 29/1000 | Loss: 0.00003749
Iteration 30/1000 | Loss: 0.00003748
Iteration 31/1000 | Loss: 0.00003747
Iteration 32/1000 | Loss: 0.00003747
Iteration 33/1000 | Loss: 0.00003746
Iteration 34/1000 | Loss: 0.00003744
Iteration 35/1000 | Loss: 0.00003744
Iteration 36/1000 | Loss: 0.00003744
Iteration 37/1000 | Loss: 0.00003743
Iteration 38/1000 | Loss: 0.00003743
Iteration 39/1000 | Loss: 0.00003743
Iteration 40/1000 | Loss: 0.00003741
Iteration 41/1000 | Loss: 0.00003741
Iteration 42/1000 | Loss: 0.00003740
Iteration 43/1000 | Loss: 0.00003740
Iteration 44/1000 | Loss: 0.00003740
Iteration 45/1000 | Loss: 0.00003740
Iteration 46/1000 | Loss: 0.00003740
Iteration 47/1000 | Loss: 0.00003740
Iteration 48/1000 | Loss: 0.00003740
Iteration 49/1000 | Loss: 0.00003740
Iteration 50/1000 | Loss: 0.00003740
Iteration 51/1000 | Loss: 0.00003740
Iteration 52/1000 | Loss: 0.00003740
Iteration 53/1000 | Loss: 0.00003739
Iteration 54/1000 | Loss: 0.00003739
Iteration 55/1000 | Loss: 0.00003739
Iteration 56/1000 | Loss: 0.00003739
Iteration 57/1000 | Loss: 0.00003739
Iteration 58/1000 | Loss: 0.00003739
Iteration 59/1000 | Loss: 0.00003738
Iteration 60/1000 | Loss: 0.00003738
Iteration 61/1000 | Loss: 0.00003738
Iteration 62/1000 | Loss: 0.00003738
Iteration 63/1000 | Loss: 0.00003737
Iteration 64/1000 | Loss: 0.00003737
Iteration 65/1000 | Loss: 0.00003737
Iteration 66/1000 | Loss: 0.00003737
Iteration 67/1000 | Loss: 0.00003736
Iteration 68/1000 | Loss: 0.00003736
Iteration 69/1000 | Loss: 0.00003736
Iteration 70/1000 | Loss: 0.00003736
Iteration 71/1000 | Loss: 0.00003736
Iteration 72/1000 | Loss: 0.00003736
Iteration 73/1000 | Loss: 0.00003736
Iteration 74/1000 | Loss: 0.00003735
Iteration 75/1000 | Loss: 0.00003735
Iteration 76/1000 | Loss: 0.00003735
Iteration 77/1000 | Loss: 0.00003735
Iteration 78/1000 | Loss: 0.00003735
Iteration 79/1000 | Loss: 0.00003735
Iteration 80/1000 | Loss: 0.00003735
Iteration 81/1000 | Loss: 0.00003735
Iteration 82/1000 | Loss: 0.00003735
Iteration 83/1000 | Loss: 0.00003735
Iteration 84/1000 | Loss: 0.00003735
Iteration 85/1000 | Loss: 0.00003734
Iteration 86/1000 | Loss: 0.00003734
Iteration 87/1000 | Loss: 0.00003734
Iteration 88/1000 | Loss: 0.00003734
Iteration 89/1000 | Loss: 0.00003734
Iteration 90/1000 | Loss: 0.00003734
Iteration 91/1000 | Loss: 0.00003733
Iteration 92/1000 | Loss: 0.00003733
Iteration 93/1000 | Loss: 0.00003733
Iteration 94/1000 | Loss: 0.00003732
Iteration 95/1000 | Loss: 0.00003732
Iteration 96/1000 | Loss: 0.00003732
Iteration 97/1000 | Loss: 0.00003732
Iteration 98/1000 | Loss: 0.00003732
Iteration 99/1000 | Loss: 0.00003732
Iteration 100/1000 | Loss: 0.00003732
Iteration 101/1000 | Loss: 0.00003732
Iteration 102/1000 | Loss: 0.00003732
Iteration 103/1000 | Loss: 0.00003732
Iteration 104/1000 | Loss: 0.00003731
Iteration 105/1000 | Loss: 0.00003731
Iteration 106/1000 | Loss: 0.00003731
Iteration 107/1000 | Loss: 0.00003731
Iteration 108/1000 | Loss: 0.00003731
Iteration 109/1000 | Loss: 0.00003731
Iteration 110/1000 | Loss: 0.00003731
Iteration 111/1000 | Loss: 0.00003731
Iteration 112/1000 | Loss: 0.00003730
Iteration 113/1000 | Loss: 0.00003730
Iteration 114/1000 | Loss: 0.00003730
Iteration 115/1000 | Loss: 0.00003730
Iteration 116/1000 | Loss: 0.00003730
Iteration 117/1000 | Loss: 0.00003730
Iteration 118/1000 | Loss: 0.00003730
Iteration 119/1000 | Loss: 0.00003730
Iteration 120/1000 | Loss: 0.00003730
Iteration 121/1000 | Loss: 0.00003730
Iteration 122/1000 | Loss: 0.00003730
Iteration 123/1000 | Loss: 0.00003729
Iteration 124/1000 | Loss: 0.00003729
Iteration 125/1000 | Loss: 0.00003729
Iteration 126/1000 | Loss: 0.00003729
Iteration 127/1000 | Loss: 0.00003729
Iteration 128/1000 | Loss: 0.00003729
Iteration 129/1000 | Loss: 0.00003729
Iteration 130/1000 | Loss: 0.00003729
Iteration 131/1000 | Loss: 0.00003729
Iteration 132/1000 | Loss: 0.00003729
Iteration 133/1000 | Loss: 0.00003729
Iteration 134/1000 | Loss: 0.00003729
Iteration 135/1000 | Loss: 0.00003729
Iteration 136/1000 | Loss: 0.00003729
Iteration 137/1000 | Loss: 0.00003729
Iteration 138/1000 | Loss: 0.00003729
Iteration 139/1000 | Loss: 0.00003729
Iteration 140/1000 | Loss: 0.00003729
Iteration 141/1000 | Loss: 0.00003729
Iteration 142/1000 | Loss: 0.00003729
Iteration 143/1000 | Loss: 0.00003729
Iteration 144/1000 | Loss: 0.00003729
Iteration 145/1000 | Loss: 0.00003729
Iteration 146/1000 | Loss: 0.00003729
Iteration 147/1000 | Loss: 0.00003729
Iteration 148/1000 | Loss: 0.00003729
Iteration 149/1000 | Loss: 0.00003729
Iteration 150/1000 | Loss: 0.00003729
Iteration 151/1000 | Loss: 0.00003729
Iteration 152/1000 | Loss: 0.00003729
Iteration 153/1000 | Loss: 0.00003729
Iteration 154/1000 | Loss: 0.00003729
Iteration 155/1000 | Loss: 0.00003729
Iteration 156/1000 | Loss: 0.00003729
Iteration 157/1000 | Loss: 0.00003729
Iteration 158/1000 | Loss: 0.00003729
Iteration 159/1000 | Loss: 0.00003729
Iteration 160/1000 | Loss: 0.00003729
Iteration 161/1000 | Loss: 0.00003729
Iteration 162/1000 | Loss: 0.00003729
Iteration 163/1000 | Loss: 0.00003729
Iteration 164/1000 | Loss: 0.00003729
Iteration 165/1000 | Loss: 0.00003729
Iteration 166/1000 | Loss: 0.00003729
Iteration 167/1000 | Loss: 0.00003729
Iteration 168/1000 | Loss: 0.00003729
Iteration 169/1000 | Loss: 0.00003729
Iteration 170/1000 | Loss: 0.00003729
Iteration 171/1000 | Loss: 0.00003729
Iteration 172/1000 | Loss: 0.00003729
Iteration 173/1000 | Loss: 0.00003729
Iteration 174/1000 | Loss: 0.00003729
Iteration 175/1000 | Loss: 0.00003729
Iteration 176/1000 | Loss: 0.00003729
Iteration 177/1000 | Loss: 0.00003729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [3.72861759387888e-05, 3.72861759387888e-05, 3.72861759387888e-05, 3.72861759387888e-05, 3.72861759387888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.72861759387888e-05

Optimization complete. Final v2v error: 5.031066417694092 mm

Highest mean error: 5.239962577819824 mm for frame 10

Lowest mean error: 4.592845439910889 mm for frame 0

Saving results

Total time: 52.22969841957092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005420
Iteration 2/25 | Loss: 0.00192793
Iteration 3/25 | Loss: 0.00178115
Iteration 4/25 | Loss: 0.00174557
Iteration 5/25 | Loss: 0.00173864
Iteration 6/25 | Loss: 0.00173617
Iteration 7/25 | Loss: 0.00173280
Iteration 8/25 | Loss: 0.00172934
Iteration 9/25 | Loss: 0.00172898
Iteration 10/25 | Loss: 0.00172884
Iteration 11/25 | Loss: 0.00172877
Iteration 12/25 | Loss: 0.00172871
Iteration 13/25 | Loss: 0.00172869
Iteration 14/25 | Loss: 0.00172869
Iteration 15/25 | Loss: 0.00172869
Iteration 16/25 | Loss: 0.00172869
Iteration 17/25 | Loss: 0.00172869
Iteration 18/25 | Loss: 0.00172869
Iteration 19/25 | Loss: 0.00172869
Iteration 20/25 | Loss: 0.00172869
Iteration 21/25 | Loss: 0.00172869
Iteration 22/25 | Loss: 0.00172869
Iteration 23/25 | Loss: 0.00172869
Iteration 24/25 | Loss: 0.00172869
Iteration 25/25 | Loss: 0.00172869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001728686736896634, 0.001728686736896634, 0.001728686736896634, 0.001728686736896634, 0.001728686736896634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001728686736896634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33943212
Iteration 2/25 | Loss: 0.00371949
Iteration 3/25 | Loss: 0.00371949
Iteration 4/25 | Loss: 0.00371949
Iteration 5/25 | Loss: 0.00371948
Iteration 6/25 | Loss: 0.00371948
Iteration 7/25 | Loss: 0.00371948
Iteration 8/25 | Loss: 0.00371948
Iteration 9/25 | Loss: 0.00371948
Iteration 10/25 | Loss: 0.00371948
Iteration 11/25 | Loss: 0.00371948
Iteration 12/25 | Loss: 0.00371948
Iteration 13/25 | Loss: 0.00371948
Iteration 14/25 | Loss: 0.00371948
Iteration 15/25 | Loss: 0.00371948
Iteration 16/25 | Loss: 0.00371948
Iteration 17/25 | Loss: 0.00371948
Iteration 18/25 | Loss: 0.00371948
Iteration 19/25 | Loss: 0.00371948
Iteration 20/25 | Loss: 0.00371948
Iteration 21/25 | Loss: 0.00371948
Iteration 22/25 | Loss: 0.00371948
Iteration 23/25 | Loss: 0.00371948
Iteration 24/25 | Loss: 0.00371948
Iteration 25/25 | Loss: 0.00371948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00371948
Iteration 2/1000 | Loss: 0.00041652
Iteration 3/1000 | Loss: 0.00070663
Iteration 4/1000 | Loss: 0.00261636
Iteration 5/1000 | Loss: 0.00026601
Iteration 6/1000 | Loss: 0.00679283
Iteration 7/1000 | Loss: 0.00986733
Iteration 8/1000 | Loss: 0.00683823
Iteration 9/1000 | Loss: 0.00241098
Iteration 10/1000 | Loss: 0.00811199
Iteration 11/1000 | Loss: 0.00318268
Iteration 12/1000 | Loss: 0.00457393
Iteration 13/1000 | Loss: 0.00079304
Iteration 14/1000 | Loss: 0.00031218
Iteration 15/1000 | Loss: 0.00029330
Iteration 16/1000 | Loss: 0.00125031
Iteration 17/1000 | Loss: 0.00078785
Iteration 18/1000 | Loss: 0.00111960
Iteration 19/1000 | Loss: 0.00310350
Iteration 20/1000 | Loss: 0.00088042
Iteration 21/1000 | Loss: 0.00019145
Iteration 22/1000 | Loss: 0.00084488
Iteration 23/1000 | Loss: 0.00016048
Iteration 24/1000 | Loss: 0.00010158
Iteration 25/1000 | Loss: 0.00008669
Iteration 26/1000 | Loss: 0.00058111
Iteration 27/1000 | Loss: 0.00080416
Iteration 28/1000 | Loss: 0.00010078
Iteration 29/1000 | Loss: 0.00032553
Iteration 30/1000 | Loss: 0.00058949
Iteration 31/1000 | Loss: 0.00045860
Iteration 32/1000 | Loss: 0.00041940
Iteration 33/1000 | Loss: 0.00040477
Iteration 34/1000 | Loss: 0.00012251
Iteration 35/1000 | Loss: 0.00022236
Iteration 36/1000 | Loss: 0.00008941
Iteration 37/1000 | Loss: 0.00032093
Iteration 38/1000 | Loss: 0.00018767
Iteration 39/1000 | Loss: 0.00005816
Iteration 40/1000 | Loss: 0.00036239
Iteration 41/1000 | Loss: 0.00021584
Iteration 42/1000 | Loss: 0.00031678
Iteration 43/1000 | Loss: 0.00065810
Iteration 44/1000 | Loss: 0.00085176
Iteration 45/1000 | Loss: 0.00044713
Iteration 46/1000 | Loss: 0.00087025
Iteration 47/1000 | Loss: 0.00088432
Iteration 48/1000 | Loss: 0.00047972
Iteration 49/1000 | Loss: 0.00054296
Iteration 50/1000 | Loss: 0.00045503
Iteration 51/1000 | Loss: 0.00037997
Iteration 52/1000 | Loss: 0.00043933
Iteration 53/1000 | Loss: 0.00052519
Iteration 54/1000 | Loss: 0.00044229
Iteration 55/1000 | Loss: 0.00046046
Iteration 56/1000 | Loss: 0.00057227
Iteration 57/1000 | Loss: 0.00075234
Iteration 58/1000 | Loss: 0.00060714
Iteration 59/1000 | Loss: 0.00046653
Iteration 60/1000 | Loss: 0.00033363
Iteration 61/1000 | Loss: 0.00028266
Iteration 62/1000 | Loss: 0.00033075
Iteration 63/1000 | Loss: 0.00037503
Iteration 64/1000 | Loss: 0.00008414
Iteration 65/1000 | Loss: 0.00005922
Iteration 66/1000 | Loss: 0.00005491
Iteration 67/1000 | Loss: 0.00034778
Iteration 68/1000 | Loss: 0.00047234
Iteration 69/1000 | Loss: 0.00049752
Iteration 70/1000 | Loss: 0.00033064
Iteration 71/1000 | Loss: 0.00036251
Iteration 72/1000 | Loss: 0.00043014
Iteration 73/1000 | Loss: 0.00055856
Iteration 74/1000 | Loss: 0.00045144
Iteration 75/1000 | Loss: 0.00033660
Iteration 76/1000 | Loss: 0.00028547
Iteration 77/1000 | Loss: 0.00052252
Iteration 78/1000 | Loss: 0.00065083
Iteration 79/1000 | Loss: 0.00031646
Iteration 80/1000 | Loss: 0.00030077
Iteration 81/1000 | Loss: 0.00029065
Iteration 82/1000 | Loss: 0.00007275
Iteration 83/1000 | Loss: 0.00005244
Iteration 84/1000 | Loss: 0.00004957
Iteration 85/1000 | Loss: 0.00004701
Iteration 86/1000 | Loss: 0.00004549
Iteration 87/1000 | Loss: 0.00023046
Iteration 88/1000 | Loss: 0.00016281
Iteration 89/1000 | Loss: 0.00015364
Iteration 90/1000 | Loss: 0.00015275
Iteration 91/1000 | Loss: 0.00005686
Iteration 92/1000 | Loss: 0.00004879
Iteration 93/1000 | Loss: 0.00025110
Iteration 94/1000 | Loss: 0.00020505
Iteration 95/1000 | Loss: 0.00020430
Iteration 96/1000 | Loss: 0.00006058
Iteration 97/1000 | Loss: 0.00004999
Iteration 98/1000 | Loss: 0.00008868
Iteration 99/1000 | Loss: 0.00013702
Iteration 100/1000 | Loss: 0.00027514
Iteration 101/1000 | Loss: 0.00029174
Iteration 102/1000 | Loss: 0.00026848
Iteration 103/1000 | Loss: 0.00008047
Iteration 104/1000 | Loss: 0.00030469
Iteration 105/1000 | Loss: 0.00005508
Iteration 106/1000 | Loss: 0.00022284
Iteration 107/1000 | Loss: 0.00061356
Iteration 108/1000 | Loss: 0.00038347
Iteration 109/1000 | Loss: 0.00024990
Iteration 110/1000 | Loss: 0.00004878
Iteration 111/1000 | Loss: 0.00006939
Iteration 112/1000 | Loss: 0.00061199
Iteration 113/1000 | Loss: 0.00026584
Iteration 114/1000 | Loss: 0.00036173
Iteration 115/1000 | Loss: 0.00005442
Iteration 116/1000 | Loss: 0.00004586
Iteration 117/1000 | Loss: 0.00012958
Iteration 118/1000 | Loss: 0.00005715
Iteration 119/1000 | Loss: 0.00004654
Iteration 120/1000 | Loss: 0.00004360
Iteration 121/1000 | Loss: 0.00004195
Iteration 122/1000 | Loss: 0.00004155
Iteration 123/1000 | Loss: 0.00040671
Iteration 124/1000 | Loss: 0.00059043
Iteration 125/1000 | Loss: 0.00054634
Iteration 126/1000 | Loss: 0.00033405
Iteration 127/1000 | Loss: 0.00024269
Iteration 128/1000 | Loss: 0.00007088
Iteration 129/1000 | Loss: 0.00005063
Iteration 130/1000 | Loss: 0.00004544
Iteration 131/1000 | Loss: 0.00018768
Iteration 132/1000 | Loss: 0.00012797
Iteration 133/1000 | Loss: 0.00015079
Iteration 134/1000 | Loss: 0.00008326
Iteration 135/1000 | Loss: 0.00020736
Iteration 136/1000 | Loss: 0.00005438
Iteration 137/1000 | Loss: 0.00004599
Iteration 138/1000 | Loss: 0.00004232
Iteration 139/1000 | Loss: 0.00004088
Iteration 140/1000 | Loss: 0.00003971
Iteration 141/1000 | Loss: 0.00003868
Iteration 142/1000 | Loss: 0.00003781
Iteration 143/1000 | Loss: 0.00003718
Iteration 144/1000 | Loss: 0.00003663
Iteration 145/1000 | Loss: 0.00019853
Iteration 146/1000 | Loss: 0.00004195
Iteration 147/1000 | Loss: 0.00018585
Iteration 148/1000 | Loss: 0.00010305
Iteration 149/1000 | Loss: 0.00020543
Iteration 150/1000 | Loss: 0.00021910
Iteration 151/1000 | Loss: 0.00021392
Iteration 152/1000 | Loss: 0.00004265
Iteration 153/1000 | Loss: 0.00004050
Iteration 154/1000 | Loss: 0.00018604
Iteration 155/1000 | Loss: 0.00004332
Iteration 156/1000 | Loss: 0.00020735
Iteration 157/1000 | Loss: 0.00004120
Iteration 158/1000 | Loss: 0.00018701
Iteration 159/1000 | Loss: 0.00021611
Iteration 160/1000 | Loss: 0.00019311
Iteration 161/1000 | Loss: 0.00021497
Iteration 162/1000 | Loss: 0.00016657
Iteration 163/1000 | Loss: 0.00019882
Iteration 164/1000 | Loss: 0.00013759
Iteration 165/1000 | Loss: 0.00017709
Iteration 166/1000 | Loss: 0.00013813
Iteration 167/1000 | Loss: 0.00016189
Iteration 168/1000 | Loss: 0.00009558
Iteration 169/1000 | Loss: 0.00014308
Iteration 170/1000 | Loss: 0.00005106
Iteration 171/1000 | Loss: 0.00015366
Iteration 172/1000 | Loss: 0.00010006
Iteration 173/1000 | Loss: 0.00016529
Iteration 174/1000 | Loss: 0.00009497
Iteration 175/1000 | Loss: 0.00003879
Iteration 176/1000 | Loss: 0.00021357
Iteration 177/1000 | Loss: 0.00012135
Iteration 178/1000 | Loss: 0.00021685
Iteration 179/1000 | Loss: 0.00008033
Iteration 180/1000 | Loss: 0.00011094
Iteration 181/1000 | Loss: 0.00007916
Iteration 182/1000 | Loss: 0.00003857
Iteration 183/1000 | Loss: 0.00003654
Iteration 184/1000 | Loss: 0.00003609
Iteration 185/1000 | Loss: 0.00003576
Iteration 186/1000 | Loss: 0.00003557
Iteration 187/1000 | Loss: 0.00003556
Iteration 188/1000 | Loss: 0.00003554
Iteration 189/1000 | Loss: 0.00003554
Iteration 190/1000 | Loss: 0.00003553
Iteration 191/1000 | Loss: 0.00003553
Iteration 192/1000 | Loss: 0.00003552
Iteration 193/1000 | Loss: 0.00003552
Iteration 194/1000 | Loss: 0.00003551
Iteration 195/1000 | Loss: 0.00003546
Iteration 196/1000 | Loss: 0.00003546
Iteration 197/1000 | Loss: 0.00003546
Iteration 198/1000 | Loss: 0.00003546
Iteration 199/1000 | Loss: 0.00003546
Iteration 200/1000 | Loss: 0.00003546
Iteration 201/1000 | Loss: 0.00003546
Iteration 202/1000 | Loss: 0.00003545
Iteration 203/1000 | Loss: 0.00003545
Iteration 204/1000 | Loss: 0.00003545
Iteration 205/1000 | Loss: 0.00003545
Iteration 206/1000 | Loss: 0.00003545
Iteration 207/1000 | Loss: 0.00003544
Iteration 208/1000 | Loss: 0.00003544
Iteration 209/1000 | Loss: 0.00003544
Iteration 210/1000 | Loss: 0.00003544
Iteration 211/1000 | Loss: 0.00003543
Iteration 212/1000 | Loss: 0.00003543
Iteration 213/1000 | Loss: 0.00003543
Iteration 214/1000 | Loss: 0.00003542
Iteration 215/1000 | Loss: 0.00003542
Iteration 216/1000 | Loss: 0.00003542
Iteration 217/1000 | Loss: 0.00003541
Iteration 218/1000 | Loss: 0.00003541
Iteration 219/1000 | Loss: 0.00003541
Iteration 220/1000 | Loss: 0.00003540
Iteration 221/1000 | Loss: 0.00003540
Iteration 222/1000 | Loss: 0.00003540
Iteration 223/1000 | Loss: 0.00003540
Iteration 224/1000 | Loss: 0.00003540
Iteration 225/1000 | Loss: 0.00003539
Iteration 226/1000 | Loss: 0.00003539
Iteration 227/1000 | Loss: 0.00003539
Iteration 228/1000 | Loss: 0.00003539
Iteration 229/1000 | Loss: 0.00003539
Iteration 230/1000 | Loss: 0.00003539
Iteration 231/1000 | Loss: 0.00003539
Iteration 232/1000 | Loss: 0.00003539
Iteration 233/1000 | Loss: 0.00003539
Iteration 234/1000 | Loss: 0.00003539
Iteration 235/1000 | Loss: 0.00003539
Iteration 236/1000 | Loss: 0.00003539
Iteration 237/1000 | Loss: 0.00003539
Iteration 238/1000 | Loss: 0.00003539
Iteration 239/1000 | Loss: 0.00003539
Iteration 240/1000 | Loss: 0.00003539
Iteration 241/1000 | Loss: 0.00003538
Iteration 242/1000 | Loss: 0.00003538
Iteration 243/1000 | Loss: 0.00003538
Iteration 244/1000 | Loss: 0.00003538
Iteration 245/1000 | Loss: 0.00003538
Iteration 246/1000 | Loss: 0.00003537
Iteration 247/1000 | Loss: 0.00003537
Iteration 248/1000 | Loss: 0.00003537
Iteration 249/1000 | Loss: 0.00003537
Iteration 250/1000 | Loss: 0.00003537
Iteration 251/1000 | Loss: 0.00003537
Iteration 252/1000 | Loss: 0.00003537
Iteration 253/1000 | Loss: 0.00003537
Iteration 254/1000 | Loss: 0.00003537
Iteration 255/1000 | Loss: 0.00003537
Iteration 256/1000 | Loss: 0.00003536
Iteration 257/1000 | Loss: 0.00003536
Iteration 258/1000 | Loss: 0.00003536
Iteration 259/1000 | Loss: 0.00003536
Iteration 260/1000 | Loss: 0.00003536
Iteration 261/1000 | Loss: 0.00003536
Iteration 262/1000 | Loss: 0.00003536
Iteration 263/1000 | Loss: 0.00003536
Iteration 264/1000 | Loss: 0.00003536
Iteration 265/1000 | Loss: 0.00003536
Iteration 266/1000 | Loss: 0.00003536
Iteration 267/1000 | Loss: 0.00003535
Iteration 268/1000 | Loss: 0.00003535
Iteration 269/1000 | Loss: 0.00003535
Iteration 270/1000 | Loss: 0.00003535
Iteration 271/1000 | Loss: 0.00003534
Iteration 272/1000 | Loss: 0.00003534
Iteration 273/1000 | Loss: 0.00003534
Iteration 274/1000 | Loss: 0.00003534
Iteration 275/1000 | Loss: 0.00003534
Iteration 276/1000 | Loss: 0.00003534
Iteration 277/1000 | Loss: 0.00003534
Iteration 278/1000 | Loss: 0.00003533
Iteration 279/1000 | Loss: 0.00003533
Iteration 280/1000 | Loss: 0.00003533
Iteration 281/1000 | Loss: 0.00003533
Iteration 282/1000 | Loss: 0.00003532
Iteration 283/1000 | Loss: 0.00003532
Iteration 284/1000 | Loss: 0.00003532
Iteration 285/1000 | Loss: 0.00003532
Iteration 286/1000 | Loss: 0.00003531
Iteration 287/1000 | Loss: 0.00003531
Iteration 288/1000 | Loss: 0.00003531
Iteration 289/1000 | Loss: 0.00003531
Iteration 290/1000 | Loss: 0.00003531
Iteration 291/1000 | Loss: 0.00003531
Iteration 292/1000 | Loss: 0.00003530
Iteration 293/1000 | Loss: 0.00003530
Iteration 294/1000 | Loss: 0.00003530
Iteration 295/1000 | Loss: 0.00003530
Iteration 296/1000 | Loss: 0.00003529
Iteration 297/1000 | Loss: 0.00003529
Iteration 298/1000 | Loss: 0.00003529
Iteration 299/1000 | Loss: 0.00003529
Iteration 300/1000 | Loss: 0.00003529
Iteration 301/1000 | Loss: 0.00003529
Iteration 302/1000 | Loss: 0.00003529
Iteration 303/1000 | Loss: 0.00003529
Iteration 304/1000 | Loss: 0.00003529
Iteration 305/1000 | Loss: 0.00003528
Iteration 306/1000 | Loss: 0.00003528
Iteration 307/1000 | Loss: 0.00003528
Iteration 308/1000 | Loss: 0.00003528
Iteration 309/1000 | Loss: 0.00003528
Iteration 310/1000 | Loss: 0.00003528
Iteration 311/1000 | Loss: 0.00003528
Iteration 312/1000 | Loss: 0.00003528
Iteration 313/1000 | Loss: 0.00003528
Iteration 314/1000 | Loss: 0.00003528
Iteration 315/1000 | Loss: 0.00003528
Iteration 316/1000 | Loss: 0.00003528
Iteration 317/1000 | Loss: 0.00003528
Iteration 318/1000 | Loss: 0.00003528
Iteration 319/1000 | Loss: 0.00003528
Iteration 320/1000 | Loss: 0.00003528
Iteration 321/1000 | Loss: 0.00003528
Iteration 322/1000 | Loss: 0.00014697
Iteration 323/1000 | Loss: 0.00004280
Iteration 324/1000 | Loss: 0.00003848
Iteration 325/1000 | Loss: 0.00003747
Iteration 326/1000 | Loss: 0.00003707
Iteration 327/1000 | Loss: 0.00003661
Iteration 328/1000 | Loss: 0.00003638
Iteration 329/1000 | Loss: 0.00020646
Iteration 330/1000 | Loss: 0.00009645
Iteration 331/1000 | Loss: 0.00004006
Iteration 332/1000 | Loss: 0.00003800
Iteration 333/1000 | Loss: 0.00003688
Iteration 334/1000 | Loss: 0.00003638
Iteration 335/1000 | Loss: 0.00003624
Iteration 336/1000 | Loss: 0.00003621
Iteration 337/1000 | Loss: 0.00003618
Iteration 338/1000 | Loss: 0.00003596
Iteration 339/1000 | Loss: 0.00003558
Iteration 340/1000 | Loss: 0.00003526
Iteration 341/1000 | Loss: 0.00003501
Iteration 342/1000 | Loss: 0.00003489
Iteration 343/1000 | Loss: 0.00003486
Iteration 344/1000 | Loss: 0.00003486
Iteration 345/1000 | Loss: 0.00003486
Iteration 346/1000 | Loss: 0.00003485
Iteration 347/1000 | Loss: 0.00003485
Iteration 348/1000 | Loss: 0.00003485
Iteration 349/1000 | Loss: 0.00003484
Iteration 350/1000 | Loss: 0.00003484
Iteration 351/1000 | Loss: 0.00003484
Iteration 352/1000 | Loss: 0.00003483
Iteration 353/1000 | Loss: 0.00003483
Iteration 354/1000 | Loss: 0.00003483
Iteration 355/1000 | Loss: 0.00003483
Iteration 356/1000 | Loss: 0.00003483
Iteration 357/1000 | Loss: 0.00003482
Iteration 358/1000 | Loss: 0.00003482
Iteration 359/1000 | Loss: 0.00003482
Iteration 360/1000 | Loss: 0.00003482
Iteration 361/1000 | Loss: 0.00003482
Iteration 362/1000 | Loss: 0.00003482
Iteration 363/1000 | Loss: 0.00003482
Iteration 364/1000 | Loss: 0.00003482
Iteration 365/1000 | Loss: 0.00003482
Iteration 366/1000 | Loss: 0.00003481
Iteration 367/1000 | Loss: 0.00003481
Iteration 368/1000 | Loss: 0.00003481
Iteration 369/1000 | Loss: 0.00003481
Iteration 370/1000 | Loss: 0.00003481
Iteration 371/1000 | Loss: 0.00003481
Iteration 372/1000 | Loss: 0.00003481
Iteration 373/1000 | Loss: 0.00003481
Iteration 374/1000 | Loss: 0.00003481
Iteration 375/1000 | Loss: 0.00003481
Iteration 376/1000 | Loss: 0.00003481
Iteration 377/1000 | Loss: 0.00003481
Iteration 378/1000 | Loss: 0.00003480
Iteration 379/1000 | Loss: 0.00003480
Iteration 380/1000 | Loss: 0.00003480
Iteration 381/1000 | Loss: 0.00003480
Iteration 382/1000 | Loss: 0.00003479
Iteration 383/1000 | Loss: 0.00003479
Iteration 384/1000 | Loss: 0.00003479
Iteration 385/1000 | Loss: 0.00003479
Iteration 386/1000 | Loss: 0.00003479
Iteration 387/1000 | Loss: 0.00003478
Iteration 388/1000 | Loss: 0.00003478
Iteration 389/1000 | Loss: 0.00003478
Iteration 390/1000 | Loss: 0.00003478
Iteration 391/1000 | Loss: 0.00003477
Iteration 392/1000 | Loss: 0.00003477
Iteration 393/1000 | Loss: 0.00003477
Iteration 394/1000 | Loss: 0.00003477
Iteration 395/1000 | Loss: 0.00003477
Iteration 396/1000 | Loss: 0.00003477
Iteration 397/1000 | Loss: 0.00003477
Iteration 398/1000 | Loss: 0.00003476
Iteration 399/1000 | Loss: 0.00003476
Iteration 400/1000 | Loss: 0.00003476
Iteration 401/1000 | Loss: 0.00003476
Iteration 402/1000 | Loss: 0.00003476
Iteration 403/1000 | Loss: 0.00003476
Iteration 404/1000 | Loss: 0.00003476
Iteration 405/1000 | Loss: 0.00003476
Iteration 406/1000 | Loss: 0.00003476
Iteration 407/1000 | Loss: 0.00003476
Iteration 408/1000 | Loss: 0.00003476
Iteration 409/1000 | Loss: 0.00003476
Iteration 410/1000 | Loss: 0.00003476
Iteration 411/1000 | Loss: 0.00003476
Iteration 412/1000 | Loss: 0.00003476
Iteration 413/1000 | Loss: 0.00003476
Iteration 414/1000 | Loss: 0.00003476
Iteration 415/1000 | Loss: 0.00003476
Iteration 416/1000 | Loss: 0.00003476
Iteration 417/1000 | Loss: 0.00003476
Iteration 418/1000 | Loss: 0.00003476
Iteration 419/1000 | Loss: 0.00003476
Iteration 420/1000 | Loss: 0.00003476
Iteration 421/1000 | Loss: 0.00003476
Iteration 422/1000 | Loss: 0.00003476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 422. Stopping optimization.
Last 5 losses: [3.475780977169052e-05, 3.475780977169052e-05, 3.475780977169052e-05, 3.475780977169052e-05, 3.475780977169052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.475780977169052e-05

Optimization complete. Final v2v error: 4.812209129333496 mm

Highest mean error: 6.146348476409912 mm for frame 44

Lowest mean error: 3.642951726913452 mm for frame 10

Saving results

Total time: 371.17097449302673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432646
Iteration 2/25 | Loss: 0.00150877
Iteration 3/25 | Loss: 0.00142989
Iteration 4/25 | Loss: 0.00140609
Iteration 5/25 | Loss: 0.00139606
Iteration 6/25 | Loss: 0.00139423
Iteration 7/25 | Loss: 0.00139423
Iteration 8/25 | Loss: 0.00139423
Iteration 9/25 | Loss: 0.00139423
Iteration 10/25 | Loss: 0.00139423
Iteration 11/25 | Loss: 0.00139423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013942293589934707, 0.0013942293589934707, 0.0013942293589934707, 0.0013942293589934707, 0.0013942293589934707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013942293589934707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.30718732
Iteration 2/25 | Loss: 0.00082840
Iteration 3/25 | Loss: 0.00082840
Iteration 4/25 | Loss: 0.00082840
Iteration 5/25 | Loss: 0.00082840
Iteration 6/25 | Loss: 0.00082840
Iteration 7/25 | Loss: 0.00082840
Iteration 8/25 | Loss: 0.00082840
Iteration 9/25 | Loss: 0.00082840
Iteration 10/25 | Loss: 0.00082840
Iteration 11/25 | Loss: 0.00082840
Iteration 12/25 | Loss: 0.00082840
Iteration 13/25 | Loss: 0.00082840
Iteration 14/25 | Loss: 0.00082840
Iteration 15/25 | Loss: 0.00082840
Iteration 16/25 | Loss: 0.00082840
Iteration 17/25 | Loss: 0.00082840
Iteration 18/25 | Loss: 0.00082840
Iteration 19/25 | Loss: 0.00082840
Iteration 20/25 | Loss: 0.00082840
Iteration 21/25 | Loss: 0.00082840
Iteration 22/25 | Loss: 0.00082840
Iteration 23/25 | Loss: 0.00082840
Iteration 24/25 | Loss: 0.00082840
Iteration 25/25 | Loss: 0.00082840

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082840
Iteration 2/1000 | Loss: 0.00005074
Iteration 3/1000 | Loss: 0.00003478
Iteration 4/1000 | Loss: 0.00003261
Iteration 5/1000 | Loss: 0.00003109
Iteration 6/1000 | Loss: 0.00002973
Iteration 7/1000 | Loss: 0.00002887
Iteration 8/1000 | Loss: 0.00002839
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002803
Iteration 11/1000 | Loss: 0.00002801
Iteration 12/1000 | Loss: 0.00002800
Iteration 13/1000 | Loss: 0.00002792
Iteration 14/1000 | Loss: 0.00002791
Iteration 15/1000 | Loss: 0.00002790
Iteration 16/1000 | Loss: 0.00002786
Iteration 17/1000 | Loss: 0.00002783
Iteration 18/1000 | Loss: 0.00002782
Iteration 19/1000 | Loss: 0.00002781
Iteration 20/1000 | Loss: 0.00002780
Iteration 21/1000 | Loss: 0.00002779
Iteration 22/1000 | Loss: 0.00002779
Iteration 23/1000 | Loss: 0.00002779
Iteration 24/1000 | Loss: 0.00002779
Iteration 25/1000 | Loss: 0.00002779
Iteration 26/1000 | Loss: 0.00002779
Iteration 27/1000 | Loss: 0.00002779
Iteration 28/1000 | Loss: 0.00002778
Iteration 29/1000 | Loss: 0.00002778
Iteration 30/1000 | Loss: 0.00002777
Iteration 31/1000 | Loss: 0.00002776
Iteration 32/1000 | Loss: 0.00002776
Iteration 33/1000 | Loss: 0.00002776
Iteration 34/1000 | Loss: 0.00002775
Iteration 35/1000 | Loss: 0.00002775
Iteration 36/1000 | Loss: 0.00002774
Iteration 37/1000 | Loss: 0.00002774
Iteration 38/1000 | Loss: 0.00002773
Iteration 39/1000 | Loss: 0.00002771
Iteration 40/1000 | Loss: 0.00002771
Iteration 41/1000 | Loss: 0.00002770
Iteration 42/1000 | Loss: 0.00002770
Iteration 43/1000 | Loss: 0.00002770
Iteration 44/1000 | Loss: 0.00002770
Iteration 45/1000 | Loss: 0.00002770
Iteration 46/1000 | Loss: 0.00002769
Iteration 47/1000 | Loss: 0.00002769
Iteration 48/1000 | Loss: 0.00002769
Iteration 49/1000 | Loss: 0.00002769
Iteration 50/1000 | Loss: 0.00002769
Iteration 51/1000 | Loss: 0.00002769
Iteration 52/1000 | Loss: 0.00002769
Iteration 53/1000 | Loss: 0.00002769
Iteration 54/1000 | Loss: 0.00002769
Iteration 55/1000 | Loss: 0.00002768
Iteration 56/1000 | Loss: 0.00002768
Iteration 57/1000 | Loss: 0.00002768
Iteration 58/1000 | Loss: 0.00002768
Iteration 59/1000 | Loss: 0.00002767
Iteration 60/1000 | Loss: 0.00002767
Iteration 61/1000 | Loss: 0.00002767
Iteration 62/1000 | Loss: 0.00002767
Iteration 63/1000 | Loss: 0.00002767
Iteration 64/1000 | Loss: 0.00002767
Iteration 65/1000 | Loss: 0.00002766
Iteration 66/1000 | Loss: 0.00002766
Iteration 67/1000 | Loss: 0.00002766
Iteration 68/1000 | Loss: 0.00002766
Iteration 69/1000 | Loss: 0.00002766
Iteration 70/1000 | Loss: 0.00002766
Iteration 71/1000 | Loss: 0.00002765
Iteration 72/1000 | Loss: 0.00002765
Iteration 73/1000 | Loss: 0.00002765
Iteration 74/1000 | Loss: 0.00002764
Iteration 75/1000 | Loss: 0.00002764
Iteration 76/1000 | Loss: 0.00002764
Iteration 77/1000 | Loss: 0.00002764
Iteration 78/1000 | Loss: 0.00002764
Iteration 79/1000 | Loss: 0.00002764
Iteration 80/1000 | Loss: 0.00002764
Iteration 81/1000 | Loss: 0.00002763
Iteration 82/1000 | Loss: 0.00002763
Iteration 83/1000 | Loss: 0.00002763
Iteration 84/1000 | Loss: 0.00002762
Iteration 85/1000 | Loss: 0.00002762
Iteration 86/1000 | Loss: 0.00002762
Iteration 87/1000 | Loss: 0.00002762
Iteration 88/1000 | Loss: 0.00002762
Iteration 89/1000 | Loss: 0.00002762
Iteration 90/1000 | Loss: 0.00002762
Iteration 91/1000 | Loss: 0.00002762
Iteration 92/1000 | Loss: 0.00002762
Iteration 93/1000 | Loss: 0.00002762
Iteration 94/1000 | Loss: 0.00002762
Iteration 95/1000 | Loss: 0.00002762
Iteration 96/1000 | Loss: 0.00002762
Iteration 97/1000 | Loss: 0.00002762
Iteration 98/1000 | Loss: 0.00002762
Iteration 99/1000 | Loss: 0.00002762
Iteration 100/1000 | Loss: 0.00002762
Iteration 101/1000 | Loss: 0.00002762
Iteration 102/1000 | Loss: 0.00002762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.7621124900178984e-05, 2.7621124900178984e-05, 2.7621124900178984e-05, 2.7621124900178984e-05, 2.7621124900178984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7621124900178984e-05

Optimization complete. Final v2v error: 4.4787821769714355 mm

Highest mean error: 4.760735988616943 mm for frame 69

Lowest mean error: 4.07834529876709 mm for frame 1

Saving results

Total time: 34.81829214096069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124534
Iteration 2/25 | Loss: 0.00337093
Iteration 3/25 | Loss: 0.00269923
Iteration 4/25 | Loss: 0.00249698
Iteration 5/25 | Loss: 0.00225489
Iteration 6/25 | Loss: 0.00214080
Iteration 7/25 | Loss: 0.00188050
Iteration 8/25 | Loss: 0.00173234
Iteration 9/25 | Loss: 0.00164259
Iteration 10/25 | Loss: 0.00159340
Iteration 11/25 | Loss: 0.00156696
Iteration 12/25 | Loss: 0.00154288
Iteration 13/25 | Loss: 0.00153240
Iteration 14/25 | Loss: 0.00153332
Iteration 15/25 | Loss: 0.00151812
Iteration 16/25 | Loss: 0.00150820
Iteration 17/25 | Loss: 0.00150073
Iteration 18/25 | Loss: 0.00150065
Iteration 19/25 | Loss: 0.00149463
Iteration 20/25 | Loss: 0.00149151
Iteration 21/25 | Loss: 0.00149034
Iteration 22/25 | Loss: 0.00148834
Iteration 23/25 | Loss: 0.00148704
Iteration 24/25 | Loss: 0.00148649
Iteration 25/25 | Loss: 0.00148641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39336097
Iteration 2/25 | Loss: 0.00239192
Iteration 3/25 | Loss: 0.00239189
Iteration 4/25 | Loss: 0.00239188
Iteration 5/25 | Loss: 0.00239188
Iteration 6/25 | Loss: 0.00239188
Iteration 7/25 | Loss: 0.00239188
Iteration 8/25 | Loss: 0.00239188
Iteration 9/25 | Loss: 0.00239188
Iteration 10/25 | Loss: 0.00239188
Iteration 11/25 | Loss: 0.00239188
Iteration 12/25 | Loss: 0.00239188
Iteration 13/25 | Loss: 0.00239188
Iteration 14/25 | Loss: 0.00239188
Iteration 15/25 | Loss: 0.00239188
Iteration 16/25 | Loss: 0.00239188
Iteration 17/25 | Loss: 0.00239188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023918808437883854, 0.0023918808437883854, 0.0023918808437883854, 0.0023918808437883854, 0.0023918808437883854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023918808437883854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239188
Iteration 2/1000 | Loss: 0.00386133
Iteration 3/1000 | Loss: 0.00306534
Iteration 4/1000 | Loss: 0.00040064
Iteration 5/1000 | Loss: 0.00020970
Iteration 6/1000 | Loss: 0.00013720
Iteration 7/1000 | Loss: 0.00008494
Iteration 8/1000 | Loss: 0.00006369
Iteration 9/1000 | Loss: 0.00004941
Iteration 10/1000 | Loss: 0.00004173
Iteration 11/1000 | Loss: 0.00003758
Iteration 12/1000 | Loss: 0.00003516
Iteration 13/1000 | Loss: 0.00003380
Iteration 14/1000 | Loss: 0.00003294
Iteration 15/1000 | Loss: 0.00003211
Iteration 16/1000 | Loss: 0.00003142
Iteration 17/1000 | Loss: 0.00003085
Iteration 18/1000 | Loss: 0.00003043
Iteration 19/1000 | Loss: 0.00003016
Iteration 20/1000 | Loss: 0.00003002
Iteration 21/1000 | Loss: 0.00002986
Iteration 22/1000 | Loss: 0.00002971
Iteration 23/1000 | Loss: 0.00002968
Iteration 24/1000 | Loss: 0.00002962
Iteration 25/1000 | Loss: 0.00002959
Iteration 26/1000 | Loss: 0.00002956
Iteration 27/1000 | Loss: 0.00002948
Iteration 28/1000 | Loss: 0.00002948
Iteration 29/1000 | Loss: 0.00002948
Iteration 30/1000 | Loss: 0.00002948
Iteration 31/1000 | Loss: 0.00002948
Iteration 32/1000 | Loss: 0.00002948
Iteration 33/1000 | Loss: 0.00002948
Iteration 34/1000 | Loss: 0.00002948
Iteration 35/1000 | Loss: 0.00002948
Iteration 36/1000 | Loss: 0.00002948
Iteration 37/1000 | Loss: 0.00002948
Iteration 38/1000 | Loss: 0.00002947
Iteration 39/1000 | Loss: 0.00002947
Iteration 40/1000 | Loss: 0.00002946
Iteration 41/1000 | Loss: 0.00002946
Iteration 42/1000 | Loss: 0.00002946
Iteration 43/1000 | Loss: 0.00002945
Iteration 44/1000 | Loss: 0.00002945
Iteration 45/1000 | Loss: 0.00002944
Iteration 46/1000 | Loss: 0.00002944
Iteration 47/1000 | Loss: 0.00002944
Iteration 48/1000 | Loss: 0.00002944
Iteration 49/1000 | Loss: 0.00002944
Iteration 50/1000 | Loss: 0.00002943
Iteration 51/1000 | Loss: 0.00002943
Iteration 52/1000 | Loss: 0.00002943
Iteration 53/1000 | Loss: 0.00002943
Iteration 54/1000 | Loss: 0.00002943
Iteration 55/1000 | Loss: 0.00002943
Iteration 56/1000 | Loss: 0.00002943
Iteration 57/1000 | Loss: 0.00002943
Iteration 58/1000 | Loss: 0.00002943
Iteration 59/1000 | Loss: 0.00002943
Iteration 60/1000 | Loss: 0.00002942
Iteration 61/1000 | Loss: 0.00002942
Iteration 62/1000 | Loss: 0.00002942
Iteration 63/1000 | Loss: 0.00002941
Iteration 64/1000 | Loss: 0.00002941
Iteration 65/1000 | Loss: 0.00002941
Iteration 66/1000 | Loss: 0.00002941
Iteration 67/1000 | Loss: 0.00002941
Iteration 68/1000 | Loss: 0.00002941
Iteration 69/1000 | Loss: 0.00002941
Iteration 70/1000 | Loss: 0.00002940
Iteration 71/1000 | Loss: 0.00002940
Iteration 72/1000 | Loss: 0.00002940
Iteration 73/1000 | Loss: 0.00002940
Iteration 74/1000 | Loss: 0.00002939
Iteration 75/1000 | Loss: 0.00002939
Iteration 76/1000 | Loss: 0.00002939
Iteration 77/1000 | Loss: 0.00002939
Iteration 78/1000 | Loss: 0.00002939
Iteration 79/1000 | Loss: 0.00002939
Iteration 80/1000 | Loss: 0.00002939
Iteration 81/1000 | Loss: 0.00002939
Iteration 82/1000 | Loss: 0.00002939
Iteration 83/1000 | Loss: 0.00002939
Iteration 84/1000 | Loss: 0.00002939
Iteration 85/1000 | Loss: 0.00002939
Iteration 86/1000 | Loss: 0.00002939
Iteration 87/1000 | Loss: 0.00002939
Iteration 88/1000 | Loss: 0.00002939
Iteration 89/1000 | Loss: 0.00002939
Iteration 90/1000 | Loss: 0.00002939
Iteration 91/1000 | Loss: 0.00002939
Iteration 92/1000 | Loss: 0.00002939
Iteration 93/1000 | Loss: 0.00002938
Iteration 94/1000 | Loss: 0.00002938
Iteration 95/1000 | Loss: 0.00002938
Iteration 96/1000 | Loss: 0.00002938
Iteration 97/1000 | Loss: 0.00002938
Iteration 98/1000 | Loss: 0.00002938
Iteration 99/1000 | Loss: 0.00002938
Iteration 100/1000 | Loss: 0.00002938
Iteration 101/1000 | Loss: 0.00002938
Iteration 102/1000 | Loss: 0.00002938
Iteration 103/1000 | Loss: 0.00002938
Iteration 104/1000 | Loss: 0.00002938
Iteration 105/1000 | Loss: 0.00002938
Iteration 106/1000 | Loss: 0.00002938
Iteration 107/1000 | Loss: 0.00002938
Iteration 108/1000 | Loss: 0.00002938
Iteration 109/1000 | Loss: 0.00002937
Iteration 110/1000 | Loss: 0.00002937
Iteration 111/1000 | Loss: 0.00002937
Iteration 112/1000 | Loss: 0.00002937
Iteration 113/1000 | Loss: 0.00002937
Iteration 114/1000 | Loss: 0.00002937
Iteration 115/1000 | Loss: 0.00002937
Iteration 116/1000 | Loss: 0.00002937
Iteration 117/1000 | Loss: 0.00002937
Iteration 118/1000 | Loss: 0.00002937
Iteration 119/1000 | Loss: 0.00002937
Iteration 120/1000 | Loss: 0.00002937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.9372826247708872e-05, 2.9372826247708872e-05, 2.9372826247708872e-05, 2.9372826247708872e-05, 2.9372826247708872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9372826247708872e-05

Optimization complete. Final v2v error: 4.001898288726807 mm

Highest mean error: 22.165891647338867 mm for frame 73

Lowest mean error: 3.420098066329956 mm for frame 49

Saving results

Total time: 84.76124572753906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584446
Iteration 2/25 | Loss: 0.00144726
Iteration 3/25 | Loss: 0.00136421
Iteration 4/25 | Loss: 0.00135459
Iteration 5/25 | Loss: 0.00135087
Iteration 6/25 | Loss: 0.00134987
Iteration 7/25 | Loss: 0.00134987
Iteration 8/25 | Loss: 0.00134987
Iteration 9/25 | Loss: 0.00134987
Iteration 10/25 | Loss: 0.00134987
Iteration 11/25 | Loss: 0.00134987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013498656917363405, 0.0013498656917363405, 0.0013498656917363405, 0.0013498656917363405, 0.0013498656917363405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013498656917363405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.62262297
Iteration 2/25 | Loss: 0.00077686
Iteration 3/25 | Loss: 0.00077684
Iteration 4/25 | Loss: 0.00077684
Iteration 5/25 | Loss: 0.00077684
Iteration 6/25 | Loss: 0.00077684
Iteration 7/25 | Loss: 0.00077684
Iteration 8/25 | Loss: 0.00077684
Iteration 9/25 | Loss: 0.00077684
Iteration 10/25 | Loss: 0.00077684
Iteration 11/25 | Loss: 0.00077684
Iteration 12/25 | Loss: 0.00077684
Iteration 13/25 | Loss: 0.00077684
Iteration 14/25 | Loss: 0.00077684
Iteration 15/25 | Loss: 0.00077684
Iteration 16/25 | Loss: 0.00077684
Iteration 17/25 | Loss: 0.00077684
Iteration 18/25 | Loss: 0.00077684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000776836764998734, 0.000776836764998734, 0.000776836764998734, 0.000776836764998734, 0.000776836764998734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000776836764998734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077684
Iteration 2/1000 | Loss: 0.00004154
Iteration 3/1000 | Loss: 0.00002593
Iteration 4/1000 | Loss: 0.00002353
Iteration 5/1000 | Loss: 0.00002210
Iteration 6/1000 | Loss: 0.00002138
Iteration 7/1000 | Loss: 0.00002080
Iteration 8/1000 | Loss: 0.00002034
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001992
Iteration 11/1000 | Loss: 0.00001991
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001980
Iteration 15/1000 | Loss: 0.00001978
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001976
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001976
Iteration 20/1000 | Loss: 0.00001975
Iteration 21/1000 | Loss: 0.00001975
Iteration 22/1000 | Loss: 0.00001974
Iteration 23/1000 | Loss: 0.00001974
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001970
Iteration 35/1000 | Loss: 0.00001970
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001969
Iteration 39/1000 | Loss: 0.00001969
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001968
Iteration 42/1000 | Loss: 0.00001967
Iteration 43/1000 | Loss: 0.00001967
Iteration 44/1000 | Loss: 0.00001967
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001967
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001967
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001966
Iteration 53/1000 | Loss: 0.00001966
Iteration 54/1000 | Loss: 0.00001966
Iteration 55/1000 | Loss: 0.00001966
Iteration 56/1000 | Loss: 0.00001966
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001965
Iteration 60/1000 | Loss: 0.00001965
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00001964
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001963
Iteration 71/1000 | Loss: 0.00001963
Iteration 72/1000 | Loss: 0.00001963
Iteration 73/1000 | Loss: 0.00001962
Iteration 74/1000 | Loss: 0.00001962
Iteration 75/1000 | Loss: 0.00001962
Iteration 76/1000 | Loss: 0.00001962
Iteration 77/1000 | Loss: 0.00001962
Iteration 78/1000 | Loss: 0.00001962
Iteration 79/1000 | Loss: 0.00001962
Iteration 80/1000 | Loss: 0.00001961
Iteration 81/1000 | Loss: 0.00001961
Iteration 82/1000 | Loss: 0.00001961
Iteration 83/1000 | Loss: 0.00001961
Iteration 84/1000 | Loss: 0.00001960
Iteration 85/1000 | Loss: 0.00001960
Iteration 86/1000 | Loss: 0.00001960
Iteration 87/1000 | Loss: 0.00001960
Iteration 88/1000 | Loss: 0.00001960
Iteration 89/1000 | Loss: 0.00001960
Iteration 90/1000 | Loss: 0.00001960
Iteration 91/1000 | Loss: 0.00001960
Iteration 92/1000 | Loss: 0.00001960
Iteration 93/1000 | Loss: 0.00001960
Iteration 94/1000 | Loss: 0.00001960
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.9602401152951643e-05, 1.9602401152951643e-05, 1.9602401152951643e-05, 1.9602401152951643e-05, 1.9602401152951643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9602401152951643e-05

Optimization complete. Final v2v error: 3.8170692920684814 mm

Highest mean error: 4.437109470367432 mm for frame 67

Lowest mean error: 3.4784178733825684 mm for frame 0

Saving results

Total time: 30.58094811439514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778786
Iteration 2/25 | Loss: 0.00188041
Iteration 3/25 | Loss: 0.00157777
Iteration 4/25 | Loss: 0.00154433
Iteration 5/25 | Loss: 0.00153861
Iteration 6/25 | Loss: 0.00153793
Iteration 7/25 | Loss: 0.00153793
Iteration 8/25 | Loss: 0.00153793
Iteration 9/25 | Loss: 0.00153793
Iteration 10/25 | Loss: 0.00153793
Iteration 11/25 | Loss: 0.00153793
Iteration 12/25 | Loss: 0.00153793
Iteration 13/25 | Loss: 0.00153793
Iteration 14/25 | Loss: 0.00153793
Iteration 15/25 | Loss: 0.00153793
Iteration 16/25 | Loss: 0.00153793
Iteration 17/25 | Loss: 0.00153793
Iteration 18/25 | Loss: 0.00153793
Iteration 19/25 | Loss: 0.00153793
Iteration 20/25 | Loss: 0.00153793
Iteration 21/25 | Loss: 0.00153793
Iteration 22/25 | Loss: 0.00153793
Iteration 23/25 | Loss: 0.00153793
Iteration 24/25 | Loss: 0.00153793
Iteration 25/25 | Loss: 0.00153793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49740291
Iteration 2/25 | Loss: 0.00116453
Iteration 3/25 | Loss: 0.00116453
Iteration 4/25 | Loss: 0.00116453
Iteration 5/25 | Loss: 0.00116453
Iteration 6/25 | Loss: 0.00116453
Iteration 7/25 | Loss: 0.00116453
Iteration 8/25 | Loss: 0.00116453
Iteration 9/25 | Loss: 0.00116453
Iteration 10/25 | Loss: 0.00116453
Iteration 11/25 | Loss: 0.00116453
Iteration 12/25 | Loss: 0.00116453
Iteration 13/25 | Loss: 0.00116453
Iteration 14/25 | Loss: 0.00116453
Iteration 15/25 | Loss: 0.00116453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011645302874967456, 0.0011645302874967456, 0.0011645302874967456, 0.0011645302874967456, 0.0011645302874967456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011645302874967456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116453
Iteration 2/1000 | Loss: 0.00010853
Iteration 3/1000 | Loss: 0.00005399
Iteration 4/1000 | Loss: 0.00004123
Iteration 5/1000 | Loss: 0.00003774
Iteration 6/1000 | Loss: 0.00003645
Iteration 7/1000 | Loss: 0.00003543
Iteration 8/1000 | Loss: 0.00003480
Iteration 9/1000 | Loss: 0.00003437
Iteration 10/1000 | Loss: 0.00003406
Iteration 11/1000 | Loss: 0.00003381
Iteration 12/1000 | Loss: 0.00003363
Iteration 13/1000 | Loss: 0.00003347
Iteration 14/1000 | Loss: 0.00003338
Iteration 15/1000 | Loss: 0.00003327
Iteration 16/1000 | Loss: 0.00003317
Iteration 17/1000 | Loss: 0.00003317
Iteration 18/1000 | Loss: 0.00003314
Iteration 19/1000 | Loss: 0.00003311
Iteration 20/1000 | Loss: 0.00003310
Iteration 21/1000 | Loss: 0.00003310
Iteration 22/1000 | Loss: 0.00003310
Iteration 23/1000 | Loss: 0.00003309
Iteration 24/1000 | Loss: 0.00003308
Iteration 25/1000 | Loss: 0.00003307
Iteration 26/1000 | Loss: 0.00003307
Iteration 27/1000 | Loss: 0.00003305
Iteration 28/1000 | Loss: 0.00003304
Iteration 29/1000 | Loss: 0.00003304
Iteration 30/1000 | Loss: 0.00003303
Iteration 31/1000 | Loss: 0.00003302
Iteration 32/1000 | Loss: 0.00003302
Iteration 33/1000 | Loss: 0.00003301
Iteration 34/1000 | Loss: 0.00003301
Iteration 35/1000 | Loss: 0.00003301
Iteration 36/1000 | Loss: 0.00003301
Iteration 37/1000 | Loss: 0.00003301
Iteration 38/1000 | Loss: 0.00003301
Iteration 39/1000 | Loss: 0.00003301
Iteration 40/1000 | Loss: 0.00003301
Iteration 41/1000 | Loss: 0.00003300
Iteration 42/1000 | Loss: 0.00003300
Iteration 43/1000 | Loss: 0.00003300
Iteration 44/1000 | Loss: 0.00003300
Iteration 45/1000 | Loss: 0.00003299
Iteration 46/1000 | Loss: 0.00003299
Iteration 47/1000 | Loss: 0.00003299
Iteration 48/1000 | Loss: 0.00003299
Iteration 49/1000 | Loss: 0.00003299
Iteration 50/1000 | Loss: 0.00003298
Iteration 51/1000 | Loss: 0.00003298
Iteration 52/1000 | Loss: 0.00003298
Iteration 53/1000 | Loss: 0.00003298
Iteration 54/1000 | Loss: 0.00003298
Iteration 55/1000 | Loss: 0.00003298
Iteration 56/1000 | Loss: 0.00003298
Iteration 57/1000 | Loss: 0.00003297
Iteration 58/1000 | Loss: 0.00003297
Iteration 59/1000 | Loss: 0.00003297
Iteration 60/1000 | Loss: 0.00003297
Iteration 61/1000 | Loss: 0.00003297
Iteration 62/1000 | Loss: 0.00003297
Iteration 63/1000 | Loss: 0.00003296
Iteration 64/1000 | Loss: 0.00003296
Iteration 65/1000 | Loss: 0.00003296
Iteration 66/1000 | Loss: 0.00003296
Iteration 67/1000 | Loss: 0.00003296
Iteration 68/1000 | Loss: 0.00003295
Iteration 69/1000 | Loss: 0.00003295
Iteration 70/1000 | Loss: 0.00003294
Iteration 71/1000 | Loss: 0.00003294
Iteration 72/1000 | Loss: 0.00003293
Iteration 73/1000 | Loss: 0.00003293
Iteration 74/1000 | Loss: 0.00003293
Iteration 75/1000 | Loss: 0.00003293
Iteration 76/1000 | Loss: 0.00003293
Iteration 77/1000 | Loss: 0.00003293
Iteration 78/1000 | Loss: 0.00003293
Iteration 79/1000 | Loss: 0.00003293
Iteration 80/1000 | Loss: 0.00003293
Iteration 81/1000 | Loss: 0.00003292
Iteration 82/1000 | Loss: 0.00003292
Iteration 83/1000 | Loss: 0.00003292
Iteration 84/1000 | Loss: 0.00003291
Iteration 85/1000 | Loss: 0.00003291
Iteration 86/1000 | Loss: 0.00003290
Iteration 87/1000 | Loss: 0.00003290
Iteration 88/1000 | Loss: 0.00003290
Iteration 89/1000 | Loss: 0.00003290
Iteration 90/1000 | Loss: 0.00003289
Iteration 91/1000 | Loss: 0.00003289
Iteration 92/1000 | Loss: 0.00003289
Iteration 93/1000 | Loss: 0.00003289
Iteration 94/1000 | Loss: 0.00003289
Iteration 95/1000 | Loss: 0.00003288
Iteration 96/1000 | Loss: 0.00003288
Iteration 97/1000 | Loss: 0.00003288
Iteration 98/1000 | Loss: 0.00003287
Iteration 99/1000 | Loss: 0.00003287
Iteration 100/1000 | Loss: 0.00003286
Iteration 101/1000 | Loss: 0.00003286
Iteration 102/1000 | Loss: 0.00003285
Iteration 103/1000 | Loss: 0.00003285
Iteration 104/1000 | Loss: 0.00003284
Iteration 105/1000 | Loss: 0.00003283
Iteration 106/1000 | Loss: 0.00003283
Iteration 107/1000 | Loss: 0.00003282
Iteration 108/1000 | Loss: 0.00003282
Iteration 109/1000 | Loss: 0.00003281
Iteration 110/1000 | Loss: 0.00003281
Iteration 111/1000 | Loss: 0.00003280
Iteration 112/1000 | Loss: 0.00003280
Iteration 113/1000 | Loss: 0.00003280
Iteration 114/1000 | Loss: 0.00003280
Iteration 115/1000 | Loss: 0.00003280
Iteration 116/1000 | Loss: 0.00003280
Iteration 117/1000 | Loss: 0.00003279
Iteration 118/1000 | Loss: 0.00003279
Iteration 119/1000 | Loss: 0.00003279
Iteration 120/1000 | Loss: 0.00003279
Iteration 121/1000 | Loss: 0.00003279
Iteration 122/1000 | Loss: 0.00003279
Iteration 123/1000 | Loss: 0.00003279
Iteration 124/1000 | Loss: 0.00003279
Iteration 125/1000 | Loss: 0.00003278
Iteration 126/1000 | Loss: 0.00003278
Iteration 127/1000 | Loss: 0.00003278
Iteration 128/1000 | Loss: 0.00003277
Iteration 129/1000 | Loss: 0.00003276
Iteration 130/1000 | Loss: 0.00003276
Iteration 131/1000 | Loss: 0.00003276
Iteration 132/1000 | Loss: 0.00003276
Iteration 133/1000 | Loss: 0.00003276
Iteration 134/1000 | Loss: 0.00003276
Iteration 135/1000 | Loss: 0.00003276
Iteration 136/1000 | Loss: 0.00003276
Iteration 137/1000 | Loss: 0.00003276
Iteration 138/1000 | Loss: 0.00003275
Iteration 139/1000 | Loss: 0.00003275
Iteration 140/1000 | Loss: 0.00003275
Iteration 141/1000 | Loss: 0.00003274
Iteration 142/1000 | Loss: 0.00003274
Iteration 143/1000 | Loss: 0.00003274
Iteration 144/1000 | Loss: 0.00003273
Iteration 145/1000 | Loss: 0.00003273
Iteration 146/1000 | Loss: 0.00003272
Iteration 147/1000 | Loss: 0.00003272
Iteration 148/1000 | Loss: 0.00003272
Iteration 149/1000 | Loss: 0.00003272
Iteration 150/1000 | Loss: 0.00003271
Iteration 151/1000 | Loss: 0.00003271
Iteration 152/1000 | Loss: 0.00003271
Iteration 153/1000 | Loss: 0.00003271
Iteration 154/1000 | Loss: 0.00003271
Iteration 155/1000 | Loss: 0.00003270
Iteration 156/1000 | Loss: 0.00003270
Iteration 157/1000 | Loss: 0.00003270
Iteration 158/1000 | Loss: 0.00003270
Iteration 159/1000 | Loss: 0.00003270
Iteration 160/1000 | Loss: 0.00003270
Iteration 161/1000 | Loss: 0.00003270
Iteration 162/1000 | Loss: 0.00003270
Iteration 163/1000 | Loss: 0.00003270
Iteration 164/1000 | Loss: 0.00003270
Iteration 165/1000 | Loss: 0.00003270
Iteration 166/1000 | Loss: 0.00003270
Iteration 167/1000 | Loss: 0.00003270
Iteration 168/1000 | Loss: 0.00003270
Iteration 169/1000 | Loss: 0.00003270
Iteration 170/1000 | Loss: 0.00003270
Iteration 171/1000 | Loss: 0.00003270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.269752050982788e-05, 3.269752050982788e-05, 3.269752050982788e-05, 3.269752050982788e-05, 3.269752050982788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.269752050982788e-05

Optimization complete. Final v2v error: 4.928499221801758 mm

Highest mean error: 5.622156143188477 mm for frame 21

Lowest mean error: 4.3032450675964355 mm for frame 221

Saving results

Total time: 47.997952699661255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_us_2923/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_us_2923/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930600
Iteration 2/25 | Loss: 0.00182145
Iteration 3/25 | Loss: 0.00143255
Iteration 4/25 | Loss: 0.00138407
Iteration 5/25 | Loss: 0.00137583
Iteration 6/25 | Loss: 0.00137103
Iteration 7/25 | Loss: 0.00136950
Iteration 8/25 | Loss: 0.00136906
Iteration 9/25 | Loss: 0.00136894
Iteration 10/25 | Loss: 0.00136894
Iteration 11/25 | Loss: 0.00136894
Iteration 12/25 | Loss: 0.00136894
Iteration 13/25 | Loss: 0.00136894
Iteration 14/25 | Loss: 0.00136894
Iteration 15/25 | Loss: 0.00136894
Iteration 16/25 | Loss: 0.00136894
Iteration 17/25 | Loss: 0.00136894
Iteration 18/25 | Loss: 0.00136894
Iteration 19/25 | Loss: 0.00136894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013689376646652818, 0.0013689376646652818, 0.0013689376646652818, 0.0013689376646652818, 0.0013689376646652818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013689376646652818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33763695
Iteration 2/25 | Loss: 0.00087807
Iteration 3/25 | Loss: 0.00087806
Iteration 4/25 | Loss: 0.00087806
Iteration 5/25 | Loss: 0.00087806
Iteration 6/25 | Loss: 0.00087806
Iteration 7/25 | Loss: 0.00087806
Iteration 8/25 | Loss: 0.00087806
Iteration 9/25 | Loss: 0.00087806
Iteration 10/25 | Loss: 0.00087806
Iteration 11/25 | Loss: 0.00087806
Iteration 12/25 | Loss: 0.00087806
Iteration 13/25 | Loss: 0.00087806
Iteration 14/25 | Loss: 0.00087806
Iteration 15/25 | Loss: 0.00087806
Iteration 16/25 | Loss: 0.00087806
Iteration 17/25 | Loss: 0.00087806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008780599455349147, 0.0008780599455349147, 0.0008780599455349147, 0.0008780599455349147, 0.0008780599455349147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008780599455349147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087806
Iteration 2/1000 | Loss: 0.00005460
Iteration 3/1000 | Loss: 0.00002937
Iteration 4/1000 | Loss: 0.00002405
Iteration 5/1000 | Loss: 0.00002237
Iteration 6/1000 | Loss: 0.00002154
Iteration 7/1000 | Loss: 0.00002099
Iteration 8/1000 | Loss: 0.00002040
Iteration 9/1000 | Loss: 0.00001994
Iteration 10/1000 | Loss: 0.00001962
Iteration 11/1000 | Loss: 0.00001942
Iteration 12/1000 | Loss: 0.00001924
Iteration 13/1000 | Loss: 0.00001907
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001890
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001886
Iteration 19/1000 | Loss: 0.00001885
Iteration 20/1000 | Loss: 0.00001885
Iteration 21/1000 | Loss: 0.00001884
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001880
Iteration 24/1000 | Loss: 0.00001880
Iteration 25/1000 | Loss: 0.00001879
Iteration 26/1000 | Loss: 0.00001879
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001870
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001869
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001866
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001859
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001857
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001856
Iteration 68/1000 | Loss: 0.00001856
Iteration 69/1000 | Loss: 0.00001856
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001855
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001853
Iteration 76/1000 | Loss: 0.00001853
Iteration 77/1000 | Loss: 0.00001853
Iteration 78/1000 | Loss: 0.00001853
Iteration 79/1000 | Loss: 0.00001853
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001852
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001852
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001850
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001849
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.849230466177687e-05, 1.849230466177687e-05, 1.849230466177687e-05, 1.849230466177687e-05, 1.849230466177687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.849230466177687e-05

Optimization complete. Final v2v error: 3.689620018005371 mm

Highest mean error: 4.298593997955322 mm for frame 6

Lowest mean error: 3.3447494506835938 mm for frame 224

Saving results

Total time: 48.15953707695007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796681
Iteration 2/25 | Loss: 0.00126367
Iteration 3/25 | Loss: 0.00117396
Iteration 4/25 | Loss: 0.00116066
Iteration 5/25 | Loss: 0.00115729
Iteration 6/25 | Loss: 0.00115679
Iteration 7/25 | Loss: 0.00115679
Iteration 8/25 | Loss: 0.00115679
Iteration 9/25 | Loss: 0.00115679
Iteration 10/25 | Loss: 0.00115679
Iteration 11/25 | Loss: 0.00115679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001156787620857358, 0.001156787620857358, 0.001156787620857358, 0.001156787620857358, 0.001156787620857358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001156787620857358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40654635
Iteration 2/25 | Loss: 0.00076468
Iteration 3/25 | Loss: 0.00076468
Iteration 4/25 | Loss: 0.00076468
Iteration 5/25 | Loss: 0.00076467
Iteration 6/25 | Loss: 0.00076467
Iteration 7/25 | Loss: 0.00076467
Iteration 8/25 | Loss: 0.00076467
Iteration 9/25 | Loss: 0.00076467
Iteration 10/25 | Loss: 0.00076467
Iteration 11/25 | Loss: 0.00076467
Iteration 12/25 | Loss: 0.00076467
Iteration 13/25 | Loss: 0.00076467
Iteration 14/25 | Loss: 0.00076467
Iteration 15/25 | Loss: 0.00076467
Iteration 16/25 | Loss: 0.00076467
Iteration 17/25 | Loss: 0.00076467
Iteration 18/25 | Loss: 0.00076467
Iteration 19/25 | Loss: 0.00076467
Iteration 20/25 | Loss: 0.00076467
Iteration 21/25 | Loss: 0.00076467
Iteration 22/25 | Loss: 0.00076467
Iteration 23/25 | Loss: 0.00076467
Iteration 24/25 | Loss: 0.00076467
Iteration 25/25 | Loss: 0.00076467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076467
Iteration 2/1000 | Loss: 0.00002741
Iteration 3/1000 | Loss: 0.00001923
Iteration 4/1000 | Loss: 0.00001709
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00001575
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001384
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001374
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001369
Iteration 29/1000 | Loss: 0.00001368
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001367
Iteration 32/1000 | Loss: 0.00001367
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001363
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001358
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001357
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001357
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001347
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001347
Iteration 111/1000 | Loss: 0.00001347
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001346
Iteration 117/1000 | Loss: 0.00001346
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001342
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001342
Iteration 134/1000 | Loss: 0.00001342
Iteration 135/1000 | Loss: 0.00001342
Iteration 136/1000 | Loss: 0.00001342
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Iteration 141/1000 | Loss: 0.00001341
Iteration 142/1000 | Loss: 0.00001341
Iteration 143/1000 | Loss: 0.00001341
Iteration 144/1000 | Loss: 0.00001341
Iteration 145/1000 | Loss: 0.00001341
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001339
Iteration 155/1000 | Loss: 0.00001339
Iteration 156/1000 | Loss: 0.00001339
Iteration 157/1000 | Loss: 0.00001339
Iteration 158/1000 | Loss: 0.00001339
Iteration 159/1000 | Loss: 0.00001339
Iteration 160/1000 | Loss: 0.00001339
Iteration 161/1000 | Loss: 0.00001339
Iteration 162/1000 | Loss: 0.00001339
Iteration 163/1000 | Loss: 0.00001339
Iteration 164/1000 | Loss: 0.00001339
Iteration 165/1000 | Loss: 0.00001339
Iteration 166/1000 | Loss: 0.00001339
Iteration 167/1000 | Loss: 0.00001339
Iteration 168/1000 | Loss: 0.00001338
Iteration 169/1000 | Loss: 0.00001338
Iteration 170/1000 | Loss: 0.00001338
Iteration 171/1000 | Loss: 0.00001338
Iteration 172/1000 | Loss: 0.00001338
Iteration 173/1000 | Loss: 0.00001338
Iteration 174/1000 | Loss: 0.00001338
Iteration 175/1000 | Loss: 0.00001338
Iteration 176/1000 | Loss: 0.00001338
Iteration 177/1000 | Loss: 0.00001338
Iteration 178/1000 | Loss: 0.00001338
Iteration 179/1000 | Loss: 0.00001338
Iteration 180/1000 | Loss: 0.00001338
Iteration 181/1000 | Loss: 0.00001338
Iteration 182/1000 | Loss: 0.00001338
Iteration 183/1000 | Loss: 0.00001338
Iteration 184/1000 | Loss: 0.00001338
Iteration 185/1000 | Loss: 0.00001338
Iteration 186/1000 | Loss: 0.00001338
Iteration 187/1000 | Loss: 0.00001338
Iteration 188/1000 | Loss: 0.00001337
Iteration 189/1000 | Loss: 0.00001337
Iteration 190/1000 | Loss: 0.00001337
Iteration 191/1000 | Loss: 0.00001337
Iteration 192/1000 | Loss: 0.00001337
Iteration 193/1000 | Loss: 0.00001337
Iteration 194/1000 | Loss: 0.00001337
Iteration 195/1000 | Loss: 0.00001337
Iteration 196/1000 | Loss: 0.00001337
Iteration 197/1000 | Loss: 0.00001337
Iteration 198/1000 | Loss: 0.00001337
Iteration 199/1000 | Loss: 0.00001337
Iteration 200/1000 | Loss: 0.00001337
Iteration 201/1000 | Loss: 0.00001337
Iteration 202/1000 | Loss: 0.00001337
Iteration 203/1000 | Loss: 0.00001337
Iteration 204/1000 | Loss: 0.00001337
Iteration 205/1000 | Loss: 0.00001337
Iteration 206/1000 | Loss: 0.00001337
Iteration 207/1000 | Loss: 0.00001337
Iteration 208/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.3371155546337832e-05, 1.3371155546337832e-05, 1.3371155546337832e-05, 1.3371155546337832e-05, 1.3371155546337832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3371155546337832e-05

Optimization complete. Final v2v error: 3.137167453765869 mm

Highest mean error: 3.798293352127075 mm for frame 86

Lowest mean error: 2.949431896209717 mm for frame 134

Saving results

Total time: 40.741111755371094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481854
Iteration 2/25 | Loss: 0.00130310
Iteration 3/25 | Loss: 0.00121058
Iteration 4/25 | Loss: 0.00120387
Iteration 5/25 | Loss: 0.00120187
Iteration 6/25 | Loss: 0.00120187
Iteration 7/25 | Loss: 0.00120187
Iteration 8/25 | Loss: 0.00120187
Iteration 9/25 | Loss: 0.00120187
Iteration 10/25 | Loss: 0.00120187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001201867824420333, 0.001201867824420333, 0.001201867824420333, 0.001201867824420333, 0.001201867824420333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001201867824420333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36699665
Iteration 2/25 | Loss: 0.00092558
Iteration 3/25 | Loss: 0.00092557
Iteration 4/25 | Loss: 0.00092557
Iteration 5/25 | Loss: 0.00092557
Iteration 6/25 | Loss: 0.00092557
Iteration 7/25 | Loss: 0.00092557
Iteration 8/25 | Loss: 0.00092557
Iteration 9/25 | Loss: 0.00092557
Iteration 10/25 | Loss: 0.00092557
Iteration 11/25 | Loss: 0.00092557
Iteration 12/25 | Loss: 0.00092557
Iteration 13/25 | Loss: 0.00092557
Iteration 14/25 | Loss: 0.00092557
Iteration 15/25 | Loss: 0.00092557
Iteration 16/25 | Loss: 0.00092557
Iteration 17/25 | Loss: 0.00092557
Iteration 18/25 | Loss: 0.00092557
Iteration 19/25 | Loss: 0.00092557
Iteration 20/25 | Loss: 0.00092557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009255710756406188, 0.0009255710756406188, 0.0009255710756406188, 0.0009255710756406188, 0.0009255710756406188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009255710756406188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092557
Iteration 2/1000 | Loss: 0.00002959
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001880
Iteration 5/1000 | Loss: 0.00001767
Iteration 6/1000 | Loss: 0.00001710
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001634
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001586
Iteration 15/1000 | Loss: 0.00001585
Iteration 16/1000 | Loss: 0.00001584
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001584
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001573
Iteration 22/1000 | Loss: 0.00001570
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001567
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001555
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001541
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001533
Iteration 45/1000 | Loss: 0.00001533
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001530
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001529
Iteration 50/1000 | Loss: 0.00001529
Iteration 51/1000 | Loss: 0.00001529
Iteration 52/1000 | Loss: 0.00001529
Iteration 53/1000 | Loss: 0.00001528
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001527
Iteration 56/1000 | Loss: 0.00001527
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001525
Iteration 59/1000 | Loss: 0.00001525
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001517
Iteration 71/1000 | Loss: 0.00001517
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001516
Iteration 75/1000 | Loss: 0.00001516
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001515
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001511
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001511
Iteration 101/1000 | Loss: 0.00001511
Iteration 102/1000 | Loss: 0.00001511
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001510
Iteration 108/1000 | Loss: 0.00001510
Iteration 109/1000 | Loss: 0.00001510
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001504
Iteration 122/1000 | Loss: 0.00001504
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001497
Iteration 152/1000 | Loss: 0.00001497
Iteration 153/1000 | Loss: 0.00001497
Iteration 154/1000 | Loss: 0.00001497
Iteration 155/1000 | Loss: 0.00001496
Iteration 156/1000 | Loss: 0.00001496
Iteration 157/1000 | Loss: 0.00001496
Iteration 158/1000 | Loss: 0.00001496
Iteration 159/1000 | Loss: 0.00001496
Iteration 160/1000 | Loss: 0.00001496
Iteration 161/1000 | Loss: 0.00001496
Iteration 162/1000 | Loss: 0.00001496
Iteration 163/1000 | Loss: 0.00001496
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001496
Iteration 166/1000 | Loss: 0.00001496
Iteration 167/1000 | Loss: 0.00001496
Iteration 168/1000 | Loss: 0.00001496
Iteration 169/1000 | Loss: 0.00001495
Iteration 170/1000 | Loss: 0.00001495
Iteration 171/1000 | Loss: 0.00001495
Iteration 172/1000 | Loss: 0.00001495
Iteration 173/1000 | Loss: 0.00001495
Iteration 174/1000 | Loss: 0.00001495
Iteration 175/1000 | Loss: 0.00001494
Iteration 176/1000 | Loss: 0.00001494
Iteration 177/1000 | Loss: 0.00001493
Iteration 178/1000 | Loss: 0.00001493
Iteration 179/1000 | Loss: 0.00001493
Iteration 180/1000 | Loss: 0.00001492
Iteration 181/1000 | Loss: 0.00001492
Iteration 182/1000 | Loss: 0.00001492
Iteration 183/1000 | Loss: 0.00001492
Iteration 184/1000 | Loss: 0.00001492
Iteration 185/1000 | Loss: 0.00001492
Iteration 186/1000 | Loss: 0.00001492
Iteration 187/1000 | Loss: 0.00001492
Iteration 188/1000 | Loss: 0.00001492
Iteration 189/1000 | Loss: 0.00001492
Iteration 190/1000 | Loss: 0.00001491
Iteration 191/1000 | Loss: 0.00001491
Iteration 192/1000 | Loss: 0.00001491
Iteration 193/1000 | Loss: 0.00001490
Iteration 194/1000 | Loss: 0.00001489
Iteration 195/1000 | Loss: 0.00001489
Iteration 196/1000 | Loss: 0.00001489
Iteration 197/1000 | Loss: 0.00001489
Iteration 198/1000 | Loss: 0.00001489
Iteration 199/1000 | Loss: 0.00001489
Iteration 200/1000 | Loss: 0.00001489
Iteration 201/1000 | Loss: 0.00001489
Iteration 202/1000 | Loss: 0.00001489
Iteration 203/1000 | Loss: 0.00001489
Iteration 204/1000 | Loss: 0.00001489
Iteration 205/1000 | Loss: 0.00001489
Iteration 206/1000 | Loss: 0.00001488
Iteration 207/1000 | Loss: 0.00001488
Iteration 208/1000 | Loss: 0.00001488
Iteration 209/1000 | Loss: 0.00001488
Iteration 210/1000 | Loss: 0.00001488
Iteration 211/1000 | Loss: 0.00001488
Iteration 212/1000 | Loss: 0.00001488
Iteration 213/1000 | Loss: 0.00001488
Iteration 214/1000 | Loss: 0.00001488
Iteration 215/1000 | Loss: 0.00001488
Iteration 216/1000 | Loss: 0.00001488
Iteration 217/1000 | Loss: 0.00001488
Iteration 218/1000 | Loss: 0.00001487
Iteration 219/1000 | Loss: 0.00001487
Iteration 220/1000 | Loss: 0.00001487
Iteration 221/1000 | Loss: 0.00001487
Iteration 222/1000 | Loss: 0.00001487
Iteration 223/1000 | Loss: 0.00001487
Iteration 224/1000 | Loss: 0.00001487
Iteration 225/1000 | Loss: 0.00001487
Iteration 226/1000 | Loss: 0.00001487
Iteration 227/1000 | Loss: 0.00001487
Iteration 228/1000 | Loss: 0.00001487
Iteration 229/1000 | Loss: 0.00001487
Iteration 230/1000 | Loss: 0.00001487
Iteration 231/1000 | Loss: 0.00001487
Iteration 232/1000 | Loss: 0.00001486
Iteration 233/1000 | Loss: 0.00001486
Iteration 234/1000 | Loss: 0.00001486
Iteration 235/1000 | Loss: 0.00001486
Iteration 236/1000 | Loss: 0.00001486
Iteration 237/1000 | Loss: 0.00001486
Iteration 238/1000 | Loss: 0.00001486
Iteration 239/1000 | Loss: 0.00001486
Iteration 240/1000 | Loss: 0.00001486
Iteration 241/1000 | Loss: 0.00001486
Iteration 242/1000 | Loss: 0.00001486
Iteration 243/1000 | Loss: 0.00001486
Iteration 244/1000 | Loss: 0.00001486
Iteration 245/1000 | Loss: 0.00001486
Iteration 246/1000 | Loss: 0.00001486
Iteration 247/1000 | Loss: 0.00001486
Iteration 248/1000 | Loss: 0.00001486
Iteration 249/1000 | Loss: 0.00001486
Iteration 250/1000 | Loss: 0.00001486
Iteration 251/1000 | Loss: 0.00001485
Iteration 252/1000 | Loss: 0.00001485
Iteration 253/1000 | Loss: 0.00001485
Iteration 254/1000 | Loss: 0.00001485
Iteration 255/1000 | Loss: 0.00001485
Iteration 256/1000 | Loss: 0.00001485
Iteration 257/1000 | Loss: 0.00001485
Iteration 258/1000 | Loss: 0.00001485
Iteration 259/1000 | Loss: 0.00001485
Iteration 260/1000 | Loss: 0.00001485
Iteration 261/1000 | Loss: 0.00001485
Iteration 262/1000 | Loss: 0.00001485
Iteration 263/1000 | Loss: 0.00001484
Iteration 264/1000 | Loss: 0.00001484
Iteration 265/1000 | Loss: 0.00001484
Iteration 266/1000 | Loss: 0.00001484
Iteration 267/1000 | Loss: 0.00001484
Iteration 268/1000 | Loss: 0.00001484
Iteration 269/1000 | Loss: 0.00001484
Iteration 270/1000 | Loss: 0.00001484
Iteration 271/1000 | Loss: 0.00001484
Iteration 272/1000 | Loss: 0.00001484
Iteration 273/1000 | Loss: 0.00001484
Iteration 274/1000 | Loss: 0.00001484
Iteration 275/1000 | Loss: 0.00001484
Iteration 276/1000 | Loss: 0.00001484
Iteration 277/1000 | Loss: 0.00001484
Iteration 278/1000 | Loss: 0.00001484
Iteration 279/1000 | Loss: 0.00001484
Iteration 280/1000 | Loss: 0.00001484
Iteration 281/1000 | Loss: 0.00001484
Iteration 282/1000 | Loss: 0.00001484
Iteration 283/1000 | Loss: 0.00001484
Iteration 284/1000 | Loss: 0.00001484
Iteration 285/1000 | Loss: 0.00001484
Iteration 286/1000 | Loss: 0.00001484
Iteration 287/1000 | Loss: 0.00001484
Iteration 288/1000 | Loss: 0.00001484
Iteration 289/1000 | Loss: 0.00001484
Iteration 290/1000 | Loss: 0.00001484
Iteration 291/1000 | Loss: 0.00001484
Iteration 292/1000 | Loss: 0.00001484
Iteration 293/1000 | Loss: 0.00001484
Iteration 294/1000 | Loss: 0.00001484
Iteration 295/1000 | Loss: 0.00001484
Iteration 296/1000 | Loss: 0.00001484
Iteration 297/1000 | Loss: 0.00001484
Iteration 298/1000 | Loss: 0.00001484
Iteration 299/1000 | Loss: 0.00001484
Iteration 300/1000 | Loss: 0.00001484
Iteration 301/1000 | Loss: 0.00001484
Iteration 302/1000 | Loss: 0.00001484
Iteration 303/1000 | Loss: 0.00001484
Iteration 304/1000 | Loss: 0.00001484
Iteration 305/1000 | Loss: 0.00001484
Iteration 306/1000 | Loss: 0.00001484
Iteration 307/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [1.483663436374627e-05, 1.483663436374627e-05, 1.483663436374627e-05, 1.483663436374627e-05, 1.483663436374627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.483663436374627e-05

Optimization complete. Final v2v error: 3.2066023349761963 mm

Highest mean error: 3.5879523754119873 mm for frame 62

Lowest mean error: 3.0630276203155518 mm for frame 12

Saving results

Total time: 48.650490522384644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711605
Iteration 2/25 | Loss: 0.00228888
Iteration 3/25 | Loss: 0.00145329
Iteration 4/25 | Loss: 0.00133907
Iteration 5/25 | Loss: 0.00128829
Iteration 6/25 | Loss: 0.00125268
Iteration 7/25 | Loss: 0.00119219
Iteration 8/25 | Loss: 0.00115819
Iteration 9/25 | Loss: 0.00112193
Iteration 10/25 | Loss: 0.00111170
Iteration 11/25 | Loss: 0.00110799
Iteration 12/25 | Loss: 0.00110641
Iteration 13/25 | Loss: 0.00110324
Iteration 14/25 | Loss: 0.00110219
Iteration 15/25 | Loss: 0.00110175
Iteration 16/25 | Loss: 0.00110160
Iteration 17/25 | Loss: 0.00110371
Iteration 18/25 | Loss: 0.00110151
Iteration 19/25 | Loss: 0.00110150
Iteration 20/25 | Loss: 0.00110150
Iteration 21/25 | Loss: 0.00110150
Iteration 22/25 | Loss: 0.00110150
Iteration 23/25 | Loss: 0.00110150
Iteration 24/25 | Loss: 0.00110150
Iteration 25/25 | Loss: 0.00110150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68371928
Iteration 2/25 | Loss: 0.00083879
Iteration 3/25 | Loss: 0.00083879
Iteration 4/25 | Loss: 0.00083879
Iteration 5/25 | Loss: 0.00083879
Iteration 6/25 | Loss: 0.00083879
Iteration 7/25 | Loss: 0.00083879
Iteration 8/25 | Loss: 0.00083879
Iteration 9/25 | Loss: 0.00083879
Iteration 10/25 | Loss: 0.00083879
Iteration 11/25 | Loss: 0.00083879
Iteration 12/25 | Loss: 0.00083879
Iteration 13/25 | Loss: 0.00083879
Iteration 14/25 | Loss: 0.00083879
Iteration 15/25 | Loss: 0.00083879
Iteration 16/25 | Loss: 0.00083879
Iteration 17/25 | Loss: 0.00083879
Iteration 18/25 | Loss: 0.00083879
Iteration 19/25 | Loss: 0.00083879
Iteration 20/25 | Loss: 0.00083879
Iteration 21/25 | Loss: 0.00083879
Iteration 22/25 | Loss: 0.00083879
Iteration 23/25 | Loss: 0.00083879
Iteration 24/25 | Loss: 0.00083879
Iteration 25/25 | Loss: 0.00083879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083879
Iteration 2/1000 | Loss: 0.00001827
Iteration 3/1000 | Loss: 0.00002959
Iteration 4/1000 | Loss: 0.00002991
Iteration 5/1000 | Loss: 0.00001163
Iteration 6/1000 | Loss: 0.00003512
Iteration 7/1000 | Loss: 0.00001101
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001925
Iteration 10/1000 | Loss: 0.00001038
Iteration 11/1000 | Loss: 0.00001034
Iteration 12/1000 | Loss: 0.00001030
Iteration 13/1000 | Loss: 0.00001029
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001009
Iteration 17/1000 | Loss: 0.00002686
Iteration 18/1000 | Loss: 0.00001013
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00000987
Iteration 22/1000 | Loss: 0.00000987
Iteration 23/1000 | Loss: 0.00000987
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00000983
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000983
Iteration 28/1000 | Loss: 0.00000983
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000983
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000982
Iteration 41/1000 | Loss: 0.00000982
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000981
Iteration 44/1000 | Loss: 0.00000981
Iteration 45/1000 | Loss: 0.00000981
Iteration 46/1000 | Loss: 0.00000981
Iteration 47/1000 | Loss: 0.00000981
Iteration 48/1000 | Loss: 0.00000981
Iteration 49/1000 | Loss: 0.00000981
Iteration 50/1000 | Loss: 0.00000981
Iteration 51/1000 | Loss: 0.00000981
Iteration 52/1000 | Loss: 0.00000981
Iteration 53/1000 | Loss: 0.00000981
Iteration 54/1000 | Loss: 0.00000981
Iteration 55/1000 | Loss: 0.00000980
Iteration 56/1000 | Loss: 0.00002540
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000969
Iteration 59/1000 | Loss: 0.00000969
Iteration 60/1000 | Loss: 0.00000968
Iteration 61/1000 | Loss: 0.00000968
Iteration 62/1000 | Loss: 0.00000968
Iteration 63/1000 | Loss: 0.00000968
Iteration 64/1000 | Loss: 0.00000968
Iteration 65/1000 | Loss: 0.00000968
Iteration 66/1000 | Loss: 0.00000968
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000967
Iteration 69/1000 | Loss: 0.00000967
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000967
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000965
Iteration 83/1000 | Loss: 0.00000965
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000965
Iteration 87/1000 | Loss: 0.00000965
Iteration 88/1000 | Loss: 0.00000965
Iteration 89/1000 | Loss: 0.00000965
Iteration 90/1000 | Loss: 0.00000965
Iteration 91/1000 | Loss: 0.00000965
Iteration 92/1000 | Loss: 0.00000965
Iteration 93/1000 | Loss: 0.00000965
Iteration 94/1000 | Loss: 0.00000965
Iteration 95/1000 | Loss: 0.00000965
Iteration 96/1000 | Loss: 0.00000965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [9.645827958593145e-06, 9.645827958593145e-06, 9.645827958593145e-06, 9.645827958593145e-06, 9.645827958593145e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.645827958593145e-06

Optimization complete. Final v2v error: 2.688431978225708 mm

Highest mean error: 3.1153831481933594 mm for frame 105

Lowest mean error: 2.4953689575195312 mm for frame 182

Saving results

Total time: 65.21772694587708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356881
Iteration 2/25 | Loss: 0.00129600
Iteration 3/25 | Loss: 0.00117156
Iteration 4/25 | Loss: 0.00113616
Iteration 5/25 | Loss: 0.00112939
Iteration 6/25 | Loss: 0.00112814
Iteration 7/25 | Loss: 0.00112814
Iteration 8/25 | Loss: 0.00112814
Iteration 9/25 | Loss: 0.00112814
Iteration 10/25 | Loss: 0.00112814
Iteration 11/25 | Loss: 0.00112814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011281404877081513, 0.0011281404877081513, 0.0011281404877081513, 0.0011281404877081513, 0.0011281404877081513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011281404877081513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35218382
Iteration 2/25 | Loss: 0.00105614
Iteration 3/25 | Loss: 0.00105614
Iteration 4/25 | Loss: 0.00105614
Iteration 5/25 | Loss: 0.00105614
Iteration 6/25 | Loss: 0.00105614
Iteration 7/25 | Loss: 0.00105614
Iteration 8/25 | Loss: 0.00105614
Iteration 9/25 | Loss: 0.00105614
Iteration 10/25 | Loss: 0.00105614
Iteration 11/25 | Loss: 0.00105614
Iteration 12/25 | Loss: 0.00105614
Iteration 13/25 | Loss: 0.00105614
Iteration 14/25 | Loss: 0.00105614
Iteration 15/25 | Loss: 0.00105614
Iteration 16/25 | Loss: 0.00105614
Iteration 17/25 | Loss: 0.00105614
Iteration 18/25 | Loss: 0.00105614
Iteration 19/25 | Loss: 0.00105614
Iteration 20/25 | Loss: 0.00105614
Iteration 21/25 | Loss: 0.00105614
Iteration 22/25 | Loss: 0.00105614
Iteration 23/25 | Loss: 0.00105614
Iteration 24/25 | Loss: 0.00105614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001056139008142054, 0.001056139008142054, 0.001056139008142054, 0.001056139008142054, 0.001056139008142054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001056139008142054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105614
Iteration 2/1000 | Loss: 0.00004907
Iteration 3/1000 | Loss: 0.00003196
Iteration 4/1000 | Loss: 0.00002521
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00002239
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002097
Iteration 9/1000 | Loss: 0.00002065
Iteration 10/1000 | Loss: 0.00002037
Iteration 11/1000 | Loss: 0.00002010
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001978
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00001959
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001954
Iteration 21/1000 | Loss: 0.00001953
Iteration 22/1000 | Loss: 0.00001952
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001942
Iteration 29/1000 | Loss: 0.00001936
Iteration 30/1000 | Loss: 0.00001933
Iteration 31/1000 | Loss: 0.00001933
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001928
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00001926
Iteration 41/1000 | Loss: 0.00001926
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001921
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001920
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001917
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001916
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001915
Iteration 57/1000 | Loss: 0.00001914
Iteration 58/1000 | Loss: 0.00001914
Iteration 59/1000 | Loss: 0.00001913
Iteration 60/1000 | Loss: 0.00001913
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001912
Iteration 63/1000 | Loss: 0.00001911
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001910
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001907
Iteration 73/1000 | Loss: 0.00001907
Iteration 74/1000 | Loss: 0.00001907
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001906
Iteration 77/1000 | Loss: 0.00001906
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001904
Iteration 80/1000 | Loss: 0.00001904
Iteration 81/1000 | Loss: 0.00001904
Iteration 82/1000 | Loss: 0.00001903
Iteration 83/1000 | Loss: 0.00001903
Iteration 84/1000 | Loss: 0.00001903
Iteration 85/1000 | Loss: 0.00001902
Iteration 86/1000 | Loss: 0.00001902
Iteration 87/1000 | Loss: 0.00001902
Iteration 88/1000 | Loss: 0.00001902
Iteration 89/1000 | Loss: 0.00001901
Iteration 90/1000 | Loss: 0.00001901
Iteration 91/1000 | Loss: 0.00001901
Iteration 92/1000 | Loss: 0.00001901
Iteration 93/1000 | Loss: 0.00001901
Iteration 94/1000 | Loss: 0.00001900
Iteration 95/1000 | Loss: 0.00001900
Iteration 96/1000 | Loss: 0.00001900
Iteration 97/1000 | Loss: 0.00001900
Iteration 98/1000 | Loss: 0.00001900
Iteration 99/1000 | Loss: 0.00001900
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001899
Iteration 106/1000 | Loss: 0.00001899
Iteration 107/1000 | Loss: 0.00001899
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001899
Iteration 111/1000 | Loss: 0.00001899
Iteration 112/1000 | Loss: 0.00001899
Iteration 113/1000 | Loss: 0.00001899
Iteration 114/1000 | Loss: 0.00001899
Iteration 115/1000 | Loss: 0.00001899
Iteration 116/1000 | Loss: 0.00001899
Iteration 117/1000 | Loss: 0.00001899
Iteration 118/1000 | Loss: 0.00001899
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001899
Iteration 121/1000 | Loss: 0.00001898
Iteration 122/1000 | Loss: 0.00001898
Iteration 123/1000 | Loss: 0.00001898
Iteration 124/1000 | Loss: 0.00001898
Iteration 125/1000 | Loss: 0.00001898
Iteration 126/1000 | Loss: 0.00001898
Iteration 127/1000 | Loss: 0.00001898
Iteration 128/1000 | Loss: 0.00001898
Iteration 129/1000 | Loss: 0.00001897
Iteration 130/1000 | Loss: 0.00001897
Iteration 131/1000 | Loss: 0.00001897
Iteration 132/1000 | Loss: 0.00001897
Iteration 133/1000 | Loss: 0.00001897
Iteration 134/1000 | Loss: 0.00001897
Iteration 135/1000 | Loss: 0.00001896
Iteration 136/1000 | Loss: 0.00001896
Iteration 137/1000 | Loss: 0.00001896
Iteration 138/1000 | Loss: 0.00001896
Iteration 139/1000 | Loss: 0.00001896
Iteration 140/1000 | Loss: 0.00001896
Iteration 141/1000 | Loss: 0.00001896
Iteration 142/1000 | Loss: 0.00001896
Iteration 143/1000 | Loss: 0.00001896
Iteration 144/1000 | Loss: 0.00001896
Iteration 145/1000 | Loss: 0.00001896
Iteration 146/1000 | Loss: 0.00001896
Iteration 147/1000 | Loss: 0.00001896
Iteration 148/1000 | Loss: 0.00001896
Iteration 149/1000 | Loss: 0.00001895
Iteration 150/1000 | Loss: 0.00001895
Iteration 151/1000 | Loss: 0.00001895
Iteration 152/1000 | Loss: 0.00001895
Iteration 153/1000 | Loss: 0.00001895
Iteration 154/1000 | Loss: 0.00001895
Iteration 155/1000 | Loss: 0.00001895
Iteration 156/1000 | Loss: 0.00001895
Iteration 157/1000 | Loss: 0.00001895
Iteration 158/1000 | Loss: 0.00001895
Iteration 159/1000 | Loss: 0.00001895
Iteration 160/1000 | Loss: 0.00001895
Iteration 161/1000 | Loss: 0.00001895
Iteration 162/1000 | Loss: 0.00001895
Iteration 163/1000 | Loss: 0.00001895
Iteration 164/1000 | Loss: 0.00001895
Iteration 165/1000 | Loss: 0.00001895
Iteration 166/1000 | Loss: 0.00001895
Iteration 167/1000 | Loss: 0.00001895
Iteration 168/1000 | Loss: 0.00001895
Iteration 169/1000 | Loss: 0.00001895
Iteration 170/1000 | Loss: 0.00001894
Iteration 171/1000 | Loss: 0.00001894
Iteration 172/1000 | Loss: 0.00001894
Iteration 173/1000 | Loss: 0.00001894
Iteration 174/1000 | Loss: 0.00001894
Iteration 175/1000 | Loss: 0.00001894
Iteration 176/1000 | Loss: 0.00001894
Iteration 177/1000 | Loss: 0.00001894
Iteration 178/1000 | Loss: 0.00001894
Iteration 179/1000 | Loss: 0.00001894
Iteration 180/1000 | Loss: 0.00001894
Iteration 181/1000 | Loss: 0.00001894
Iteration 182/1000 | Loss: 0.00001894
Iteration 183/1000 | Loss: 0.00001894
Iteration 184/1000 | Loss: 0.00001893
Iteration 185/1000 | Loss: 0.00001893
Iteration 186/1000 | Loss: 0.00001893
Iteration 187/1000 | Loss: 0.00001893
Iteration 188/1000 | Loss: 0.00001893
Iteration 189/1000 | Loss: 0.00001892
Iteration 190/1000 | Loss: 0.00001892
Iteration 191/1000 | Loss: 0.00001892
Iteration 192/1000 | Loss: 0.00001892
Iteration 193/1000 | Loss: 0.00001892
Iteration 194/1000 | Loss: 0.00001892
Iteration 195/1000 | Loss: 0.00001892
Iteration 196/1000 | Loss: 0.00001892
Iteration 197/1000 | Loss: 0.00001891
Iteration 198/1000 | Loss: 0.00001891
Iteration 199/1000 | Loss: 0.00001891
Iteration 200/1000 | Loss: 0.00001891
Iteration 201/1000 | Loss: 0.00001891
Iteration 202/1000 | Loss: 0.00001891
Iteration 203/1000 | Loss: 0.00001891
Iteration 204/1000 | Loss: 0.00001891
Iteration 205/1000 | Loss: 0.00001891
Iteration 206/1000 | Loss: 0.00001891
Iteration 207/1000 | Loss: 0.00001890
Iteration 208/1000 | Loss: 0.00001890
Iteration 209/1000 | Loss: 0.00001890
Iteration 210/1000 | Loss: 0.00001890
Iteration 211/1000 | Loss: 0.00001890
Iteration 212/1000 | Loss: 0.00001890
Iteration 213/1000 | Loss: 0.00001890
Iteration 214/1000 | Loss: 0.00001890
Iteration 215/1000 | Loss: 0.00001890
Iteration 216/1000 | Loss: 0.00001890
Iteration 217/1000 | Loss: 0.00001890
Iteration 218/1000 | Loss: 0.00001890
Iteration 219/1000 | Loss: 0.00001890
Iteration 220/1000 | Loss: 0.00001890
Iteration 221/1000 | Loss: 0.00001890
Iteration 222/1000 | Loss: 0.00001890
Iteration 223/1000 | Loss: 0.00001890
Iteration 224/1000 | Loss: 0.00001890
Iteration 225/1000 | Loss: 0.00001890
Iteration 226/1000 | Loss: 0.00001890
Iteration 227/1000 | Loss: 0.00001889
Iteration 228/1000 | Loss: 0.00001889
Iteration 229/1000 | Loss: 0.00001889
Iteration 230/1000 | Loss: 0.00001889
Iteration 231/1000 | Loss: 0.00001889
Iteration 232/1000 | Loss: 0.00001889
Iteration 233/1000 | Loss: 0.00001889
Iteration 234/1000 | Loss: 0.00001889
Iteration 235/1000 | Loss: 0.00001889
Iteration 236/1000 | Loss: 0.00001889
Iteration 237/1000 | Loss: 0.00001889
Iteration 238/1000 | Loss: 0.00001889
Iteration 239/1000 | Loss: 0.00001889
Iteration 240/1000 | Loss: 0.00001889
Iteration 241/1000 | Loss: 0.00001889
Iteration 242/1000 | Loss: 0.00001889
Iteration 243/1000 | Loss: 0.00001889
Iteration 244/1000 | Loss: 0.00001889
Iteration 245/1000 | Loss: 0.00001888
Iteration 246/1000 | Loss: 0.00001888
Iteration 247/1000 | Loss: 0.00001888
Iteration 248/1000 | Loss: 0.00001888
Iteration 249/1000 | Loss: 0.00001888
Iteration 250/1000 | Loss: 0.00001888
Iteration 251/1000 | Loss: 0.00001888
Iteration 252/1000 | Loss: 0.00001888
Iteration 253/1000 | Loss: 0.00001888
Iteration 254/1000 | Loss: 0.00001888
Iteration 255/1000 | Loss: 0.00001888
Iteration 256/1000 | Loss: 0.00001888
Iteration 257/1000 | Loss: 0.00001888
Iteration 258/1000 | Loss: 0.00001888
Iteration 259/1000 | Loss: 0.00001888
Iteration 260/1000 | Loss: 0.00001888
Iteration 261/1000 | Loss: 0.00001888
Iteration 262/1000 | Loss: 0.00001888
Iteration 263/1000 | Loss: 0.00001888
Iteration 264/1000 | Loss: 0.00001888
Iteration 265/1000 | Loss: 0.00001888
Iteration 266/1000 | Loss: 0.00001888
Iteration 267/1000 | Loss: 0.00001888
Iteration 268/1000 | Loss: 0.00001888
Iteration 269/1000 | Loss: 0.00001887
Iteration 270/1000 | Loss: 0.00001887
Iteration 271/1000 | Loss: 0.00001887
Iteration 272/1000 | Loss: 0.00001887
Iteration 273/1000 | Loss: 0.00001887
Iteration 274/1000 | Loss: 0.00001887
Iteration 275/1000 | Loss: 0.00001887
Iteration 276/1000 | Loss: 0.00001887
Iteration 277/1000 | Loss: 0.00001887
Iteration 278/1000 | Loss: 0.00001887
Iteration 279/1000 | Loss: 0.00001887
Iteration 280/1000 | Loss: 0.00001887
Iteration 281/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.8869830455514602e-05, 1.8869830455514602e-05, 1.8869830455514602e-05, 1.8869830455514602e-05, 1.8869830455514602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8869830455514602e-05

Optimization complete. Final v2v error: 3.5799336433410645 mm

Highest mean error: 4.734846115112305 mm for frame 189

Lowest mean error: 2.5554447174072266 mm for frame 223

Saving results

Total time: 56.820340156555176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813382
Iteration 2/25 | Loss: 0.00143185
Iteration 3/25 | Loss: 0.00121005
Iteration 4/25 | Loss: 0.00119194
Iteration 5/25 | Loss: 0.00118859
Iteration 6/25 | Loss: 0.00118816
Iteration 7/25 | Loss: 0.00118816
Iteration 8/25 | Loss: 0.00118816
Iteration 9/25 | Loss: 0.00118816
Iteration 10/25 | Loss: 0.00118816
Iteration 11/25 | Loss: 0.00118816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011881565442308784, 0.0011881565442308784, 0.0011881565442308784, 0.0011881565442308784, 0.0011881565442308784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011881565442308784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91462153
Iteration 2/25 | Loss: 0.00051562
Iteration 3/25 | Loss: 0.00051562
Iteration 4/25 | Loss: 0.00051562
Iteration 5/25 | Loss: 0.00051562
Iteration 6/25 | Loss: 0.00051562
Iteration 7/25 | Loss: 0.00051562
Iteration 8/25 | Loss: 0.00051562
Iteration 9/25 | Loss: 0.00051562
Iteration 10/25 | Loss: 0.00051562
Iteration 11/25 | Loss: 0.00051562
Iteration 12/25 | Loss: 0.00051562
Iteration 13/25 | Loss: 0.00051562
Iteration 14/25 | Loss: 0.00051562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005156213301233947, 0.0005156213301233947, 0.0005156213301233947, 0.0005156213301233947, 0.0005156213301233947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005156213301233947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051562
Iteration 2/1000 | Loss: 0.00003152
Iteration 3/1000 | Loss: 0.00002450
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002184
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002091
Iteration 8/1000 | Loss: 0.00002069
Iteration 9/1000 | Loss: 0.00002044
Iteration 10/1000 | Loss: 0.00002028
Iteration 11/1000 | Loss: 0.00002022
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002021
Iteration 14/1000 | Loss: 0.00002020
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002019
Iteration 18/1000 | Loss: 0.00002018
Iteration 19/1000 | Loss: 0.00002013
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002007
Iteration 23/1000 | Loss: 0.00002007
Iteration 24/1000 | Loss: 0.00002004
Iteration 25/1000 | Loss: 0.00002004
Iteration 26/1000 | Loss: 0.00002004
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00002004
Iteration 29/1000 | Loss: 0.00002004
Iteration 30/1000 | Loss: 0.00002004
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002004
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002003
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00001998
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001996
Iteration 42/1000 | Loss: 0.00001996
Iteration 43/1000 | Loss: 0.00001996
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001995
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001994
Iteration 54/1000 | Loss: 0.00001994
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001994
Iteration 59/1000 | Loss: 0.00001994
Iteration 60/1000 | Loss: 0.00001994
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001986
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001985
Iteration 69/1000 | Loss: 0.00001985
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001985
Iteration 73/1000 | Loss: 0.00001985
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001984
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001983
Iteration 81/1000 | Loss: 0.00001983
Iteration 82/1000 | Loss: 0.00001983
Iteration 83/1000 | Loss: 0.00001983
Iteration 84/1000 | Loss: 0.00001982
Iteration 85/1000 | Loss: 0.00001982
Iteration 86/1000 | Loss: 0.00001981
Iteration 87/1000 | Loss: 0.00001981
Iteration 88/1000 | Loss: 0.00001980
Iteration 89/1000 | Loss: 0.00001980
Iteration 90/1000 | Loss: 0.00001980
Iteration 91/1000 | Loss: 0.00001980
Iteration 92/1000 | Loss: 0.00001979
Iteration 93/1000 | Loss: 0.00001979
Iteration 94/1000 | Loss: 0.00001978
Iteration 95/1000 | Loss: 0.00001978
Iteration 96/1000 | Loss: 0.00001978
Iteration 97/1000 | Loss: 0.00001978
Iteration 98/1000 | Loss: 0.00001978
Iteration 99/1000 | Loss: 0.00001977
Iteration 100/1000 | Loss: 0.00001977
Iteration 101/1000 | Loss: 0.00001977
Iteration 102/1000 | Loss: 0.00001977
Iteration 103/1000 | Loss: 0.00001977
Iteration 104/1000 | Loss: 0.00001977
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001977
Iteration 108/1000 | Loss: 0.00001977
Iteration 109/1000 | Loss: 0.00001977
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001976
Iteration 118/1000 | Loss: 0.00001976
Iteration 119/1000 | Loss: 0.00001976
Iteration 120/1000 | Loss: 0.00001976
Iteration 121/1000 | Loss: 0.00001976
Iteration 122/1000 | Loss: 0.00001975
Iteration 123/1000 | Loss: 0.00001975
Iteration 124/1000 | Loss: 0.00001975
Iteration 125/1000 | Loss: 0.00001975
Iteration 126/1000 | Loss: 0.00001975
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001973
Iteration 129/1000 | Loss: 0.00001973
Iteration 130/1000 | Loss: 0.00001973
Iteration 131/1000 | Loss: 0.00001973
Iteration 132/1000 | Loss: 0.00001973
Iteration 133/1000 | Loss: 0.00001973
Iteration 134/1000 | Loss: 0.00001973
Iteration 135/1000 | Loss: 0.00001973
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00001973
Iteration 138/1000 | Loss: 0.00001973
Iteration 139/1000 | Loss: 0.00001973
Iteration 140/1000 | Loss: 0.00001972
Iteration 141/1000 | Loss: 0.00001972
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001972
Iteration 148/1000 | Loss: 0.00001972
Iteration 149/1000 | Loss: 0.00001972
Iteration 150/1000 | Loss: 0.00001972
Iteration 151/1000 | Loss: 0.00001972
Iteration 152/1000 | Loss: 0.00001972
Iteration 153/1000 | Loss: 0.00001972
Iteration 154/1000 | Loss: 0.00001972
Iteration 155/1000 | Loss: 0.00001972
Iteration 156/1000 | Loss: 0.00001972
Iteration 157/1000 | Loss: 0.00001972
Iteration 158/1000 | Loss: 0.00001972
Iteration 159/1000 | Loss: 0.00001972
Iteration 160/1000 | Loss: 0.00001972
Iteration 161/1000 | Loss: 0.00001972
Iteration 162/1000 | Loss: 0.00001972
Iteration 163/1000 | Loss: 0.00001972
Iteration 164/1000 | Loss: 0.00001972
Iteration 165/1000 | Loss: 0.00001972
Iteration 166/1000 | Loss: 0.00001972
Iteration 167/1000 | Loss: 0.00001972
Iteration 168/1000 | Loss: 0.00001972
Iteration 169/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.97175450011855e-05, 1.97175450011855e-05, 1.97175450011855e-05, 1.97175450011855e-05, 1.97175450011855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.97175450011855e-05

Optimization complete. Final v2v error: 3.69596529006958 mm

Highest mean error: 3.9027183055877686 mm for frame 121

Lowest mean error: 3.5701420307159424 mm for frame 136

Saving results

Total time: 35.741416215896606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079062
Iteration 2/25 | Loss: 0.00260035
Iteration 3/25 | Loss: 0.00179228
Iteration 4/25 | Loss: 0.00167080
Iteration 5/25 | Loss: 0.00155177
Iteration 6/25 | Loss: 0.00159776
Iteration 7/25 | Loss: 0.00150274
Iteration 8/25 | Loss: 0.00144533
Iteration 9/25 | Loss: 0.00144290
Iteration 10/25 | Loss: 0.00138096
Iteration 11/25 | Loss: 0.00138121
Iteration 12/25 | Loss: 0.00137581
Iteration 13/25 | Loss: 0.00134864
Iteration 14/25 | Loss: 0.00131715
Iteration 15/25 | Loss: 0.00131123
Iteration 16/25 | Loss: 0.00130409
Iteration 17/25 | Loss: 0.00130281
Iteration 18/25 | Loss: 0.00128788
Iteration 19/25 | Loss: 0.00127744
Iteration 20/25 | Loss: 0.00126942
Iteration 21/25 | Loss: 0.00125985
Iteration 22/25 | Loss: 0.00125935
Iteration 23/25 | Loss: 0.00125426
Iteration 24/25 | Loss: 0.00125261
Iteration 25/25 | Loss: 0.00125139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09650111
Iteration 2/25 | Loss: 0.00099034
Iteration 3/25 | Loss: 0.00099034
Iteration 4/25 | Loss: 0.00099034
Iteration 5/25 | Loss: 0.00099034
Iteration 6/25 | Loss: 0.00099034
Iteration 7/25 | Loss: 0.00099034
Iteration 8/25 | Loss: 0.00099034
Iteration 9/25 | Loss: 0.00099034
Iteration 10/25 | Loss: 0.00099034
Iteration 11/25 | Loss: 0.00099034
Iteration 12/25 | Loss: 0.00099034
Iteration 13/25 | Loss: 0.00099034
Iteration 14/25 | Loss: 0.00099034
Iteration 15/25 | Loss: 0.00099034
Iteration 16/25 | Loss: 0.00099034
Iteration 17/25 | Loss: 0.00099034
Iteration 18/25 | Loss: 0.00099034
Iteration 19/25 | Loss: 0.00099034
Iteration 20/25 | Loss: 0.00099034
Iteration 21/25 | Loss: 0.00099034
Iteration 22/25 | Loss: 0.00099034
Iteration 23/25 | Loss: 0.00099034
Iteration 24/25 | Loss: 0.00099034
Iteration 25/25 | Loss: 0.00099034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099034
Iteration 2/1000 | Loss: 0.00013336
Iteration 3/1000 | Loss: 0.00007389
Iteration 4/1000 | Loss: 0.00006159
Iteration 5/1000 | Loss: 0.00006911
Iteration 6/1000 | Loss: 0.00007184
Iteration 7/1000 | Loss: 0.00004778
Iteration 8/1000 | Loss: 0.00005420
Iteration 9/1000 | Loss: 0.00006415
Iteration 10/1000 | Loss: 0.00005562
Iteration 11/1000 | Loss: 0.00005558
Iteration 12/1000 | Loss: 0.00005992
Iteration 13/1000 | Loss: 0.00006425
Iteration 14/1000 | Loss: 0.00004820
Iteration 15/1000 | Loss: 0.00007074
Iteration 16/1000 | Loss: 0.00023999
Iteration 17/1000 | Loss: 0.00065961
Iteration 18/1000 | Loss: 0.00006295
Iteration 19/1000 | Loss: 0.00006788
Iteration 20/1000 | Loss: 0.00003318
Iteration 21/1000 | Loss: 0.00002810
Iteration 22/1000 | Loss: 0.00002456
Iteration 23/1000 | Loss: 0.00002233
Iteration 24/1000 | Loss: 0.00002125
Iteration 25/1000 | Loss: 0.00002048
Iteration 26/1000 | Loss: 0.00002003
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001908
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001831
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001827
Iteration 34/1000 | Loss: 0.00001823
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001823
Iteration 37/1000 | Loss: 0.00001823
Iteration 38/1000 | Loss: 0.00001822
Iteration 39/1000 | Loss: 0.00001822
Iteration 40/1000 | Loss: 0.00001821
Iteration 41/1000 | Loss: 0.00001821
Iteration 42/1000 | Loss: 0.00001820
Iteration 43/1000 | Loss: 0.00001820
Iteration 44/1000 | Loss: 0.00001820
Iteration 45/1000 | Loss: 0.00001820
Iteration 46/1000 | Loss: 0.00001820
Iteration 47/1000 | Loss: 0.00001820
Iteration 48/1000 | Loss: 0.00001820
Iteration 49/1000 | Loss: 0.00001819
Iteration 50/1000 | Loss: 0.00001819
Iteration 51/1000 | Loss: 0.00001818
Iteration 52/1000 | Loss: 0.00001818
Iteration 53/1000 | Loss: 0.00001818
Iteration 54/1000 | Loss: 0.00001817
Iteration 55/1000 | Loss: 0.00001817
Iteration 56/1000 | Loss: 0.00001817
Iteration 57/1000 | Loss: 0.00001817
Iteration 58/1000 | Loss: 0.00001817
Iteration 59/1000 | Loss: 0.00001817
Iteration 60/1000 | Loss: 0.00001817
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001816
Iteration 64/1000 | Loss: 0.00001816
Iteration 65/1000 | Loss: 0.00001816
Iteration 66/1000 | Loss: 0.00001816
Iteration 67/1000 | Loss: 0.00001816
Iteration 68/1000 | Loss: 0.00001815
Iteration 69/1000 | Loss: 0.00001815
Iteration 70/1000 | Loss: 0.00001815
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00001814
Iteration 73/1000 | Loss: 0.00001814
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001814
Iteration 76/1000 | Loss: 0.00001813
Iteration 77/1000 | Loss: 0.00001813
Iteration 78/1000 | Loss: 0.00001813
Iteration 79/1000 | Loss: 0.00001813
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001812
Iteration 88/1000 | Loss: 0.00001812
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001811
Iteration 91/1000 | Loss: 0.00001811
Iteration 92/1000 | Loss: 0.00001811
Iteration 93/1000 | Loss: 0.00001810
Iteration 94/1000 | Loss: 0.00001810
Iteration 95/1000 | Loss: 0.00001810
Iteration 96/1000 | Loss: 0.00001810
Iteration 97/1000 | Loss: 0.00001810
Iteration 98/1000 | Loss: 0.00001810
Iteration 99/1000 | Loss: 0.00001809
Iteration 100/1000 | Loss: 0.00001809
Iteration 101/1000 | Loss: 0.00001809
Iteration 102/1000 | Loss: 0.00001809
Iteration 103/1000 | Loss: 0.00001809
Iteration 104/1000 | Loss: 0.00001809
Iteration 105/1000 | Loss: 0.00001809
Iteration 106/1000 | Loss: 0.00001808
Iteration 107/1000 | Loss: 0.00001808
Iteration 108/1000 | Loss: 0.00001808
Iteration 109/1000 | Loss: 0.00001808
Iteration 110/1000 | Loss: 0.00001808
Iteration 111/1000 | Loss: 0.00001808
Iteration 112/1000 | Loss: 0.00001808
Iteration 113/1000 | Loss: 0.00001808
Iteration 114/1000 | Loss: 0.00001808
Iteration 115/1000 | Loss: 0.00001808
Iteration 116/1000 | Loss: 0.00001808
Iteration 117/1000 | Loss: 0.00001808
Iteration 118/1000 | Loss: 0.00001808
Iteration 119/1000 | Loss: 0.00001808
Iteration 120/1000 | Loss: 0.00001808
Iteration 121/1000 | Loss: 0.00001808
Iteration 122/1000 | Loss: 0.00001808
Iteration 123/1000 | Loss: 0.00001808
Iteration 124/1000 | Loss: 0.00001808
Iteration 125/1000 | Loss: 0.00001808
Iteration 126/1000 | Loss: 0.00001808
Iteration 127/1000 | Loss: 0.00001808
Iteration 128/1000 | Loss: 0.00001808
Iteration 129/1000 | Loss: 0.00001808
Iteration 130/1000 | Loss: 0.00001808
Iteration 131/1000 | Loss: 0.00001808
Iteration 132/1000 | Loss: 0.00001808
Iteration 133/1000 | Loss: 0.00001808
Iteration 134/1000 | Loss: 0.00001808
Iteration 135/1000 | Loss: 0.00001808
Iteration 136/1000 | Loss: 0.00001808
Iteration 137/1000 | Loss: 0.00001808
Iteration 138/1000 | Loss: 0.00001808
Iteration 139/1000 | Loss: 0.00001808
Iteration 140/1000 | Loss: 0.00001808
Iteration 141/1000 | Loss: 0.00001808
Iteration 142/1000 | Loss: 0.00001808
Iteration 143/1000 | Loss: 0.00001808
Iteration 144/1000 | Loss: 0.00001808
Iteration 145/1000 | Loss: 0.00001808
Iteration 146/1000 | Loss: 0.00001808
Iteration 147/1000 | Loss: 0.00001808
Iteration 148/1000 | Loss: 0.00001808
Iteration 149/1000 | Loss: 0.00001808
Iteration 150/1000 | Loss: 0.00001808
Iteration 151/1000 | Loss: 0.00001808
Iteration 152/1000 | Loss: 0.00001808
Iteration 153/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.8076183550874703e-05, 1.8076183550874703e-05, 1.8076183550874703e-05, 1.8076183550874703e-05, 1.8076183550874703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8076183550874703e-05

Optimization complete. Final v2v error: 3.6193668842315674 mm

Highest mean error: 4.007819175720215 mm for frame 106

Lowest mean error: 3.446215867996216 mm for frame 130

Saving results

Total time: 96.01731133460999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033554
Iteration 2/25 | Loss: 0.00141168
Iteration 3/25 | Loss: 0.00119285
Iteration 4/25 | Loss: 0.00118103
Iteration 5/25 | Loss: 0.00119210
Iteration 6/25 | Loss: 0.00118372
Iteration 7/25 | Loss: 0.00117216
Iteration 8/25 | Loss: 0.00117124
Iteration 9/25 | Loss: 0.00116773
Iteration 10/25 | Loss: 0.00116302
Iteration 11/25 | Loss: 0.00115819
Iteration 12/25 | Loss: 0.00115361
Iteration 13/25 | Loss: 0.00114949
Iteration 14/25 | Loss: 0.00114872
Iteration 15/25 | Loss: 0.00114969
Iteration 16/25 | Loss: 0.00114406
Iteration 17/25 | Loss: 0.00114301
Iteration 18/25 | Loss: 0.00114326
Iteration 19/25 | Loss: 0.00113980
Iteration 20/25 | Loss: 0.00113881
Iteration 21/25 | Loss: 0.00113853
Iteration 22/25 | Loss: 0.00113846
Iteration 23/25 | Loss: 0.00113846
Iteration 24/25 | Loss: 0.00113846
Iteration 25/25 | Loss: 0.00113846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39995289
Iteration 2/25 | Loss: 0.00083264
Iteration 3/25 | Loss: 0.00083264
Iteration 4/25 | Loss: 0.00083264
Iteration 5/25 | Loss: 0.00083264
Iteration 6/25 | Loss: 0.00083264
Iteration 7/25 | Loss: 0.00083264
Iteration 8/25 | Loss: 0.00083264
Iteration 9/25 | Loss: 0.00083263
Iteration 10/25 | Loss: 0.00083263
Iteration 11/25 | Loss: 0.00083263
Iteration 12/25 | Loss: 0.00083263
Iteration 13/25 | Loss: 0.00083263
Iteration 14/25 | Loss: 0.00083263
Iteration 15/25 | Loss: 0.00083263
Iteration 16/25 | Loss: 0.00083263
Iteration 17/25 | Loss: 0.00083263
Iteration 18/25 | Loss: 0.00083263
Iteration 19/25 | Loss: 0.00083263
Iteration 20/25 | Loss: 0.00083263
Iteration 21/25 | Loss: 0.00083263
Iteration 22/25 | Loss: 0.00083263
Iteration 23/25 | Loss: 0.00083263
Iteration 24/25 | Loss: 0.00083263
Iteration 25/25 | Loss: 0.00083263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083263
Iteration 2/1000 | Loss: 0.00002192
Iteration 3/1000 | Loss: 0.00001640
Iteration 4/1000 | Loss: 0.00001463
Iteration 5/1000 | Loss: 0.00001358
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001188
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001166
Iteration 12/1000 | Loss: 0.00001155
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001141
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001134
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001122
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001121
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001120
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001120
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001118
Iteration 104/1000 | Loss: 0.00001118
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.1179114153492264e-05, 1.1179114153492264e-05, 1.1179114153492264e-05, 1.1179114153492264e-05, 1.1179114153492264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1179114153492264e-05

Optimization complete. Final v2v error: 2.8578696250915527 mm

Highest mean error: 3.144771099090576 mm for frame 64

Lowest mean error: 2.6493754386901855 mm for frame 208

Saving results

Total time: 73.22754240036011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485502
Iteration 2/25 | Loss: 0.00119717
Iteration 3/25 | Loss: 0.00113383
Iteration 4/25 | Loss: 0.00112169
Iteration 5/25 | Loss: 0.00111767
Iteration 6/25 | Loss: 0.00111763
Iteration 7/25 | Loss: 0.00111763
Iteration 8/25 | Loss: 0.00111763
Iteration 9/25 | Loss: 0.00111763
Iteration 10/25 | Loss: 0.00111763
Iteration 11/25 | Loss: 0.00111763
Iteration 12/25 | Loss: 0.00111763
Iteration 13/25 | Loss: 0.00111763
Iteration 14/25 | Loss: 0.00111763
Iteration 15/25 | Loss: 0.00111763
Iteration 16/25 | Loss: 0.00111763
Iteration 17/25 | Loss: 0.00111763
Iteration 18/25 | Loss: 0.00111763
Iteration 19/25 | Loss: 0.00111763
Iteration 20/25 | Loss: 0.00111763
Iteration 21/25 | Loss: 0.00111763
Iteration 22/25 | Loss: 0.00111763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011176279513165355, 0.0011176279513165355, 0.0011176279513165355, 0.0011176279513165355, 0.0011176279513165355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011176279513165355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47460365
Iteration 2/25 | Loss: 0.00079711
Iteration 3/25 | Loss: 0.00079711
Iteration 4/25 | Loss: 0.00079711
Iteration 5/25 | Loss: 0.00079711
Iteration 6/25 | Loss: 0.00079711
Iteration 7/25 | Loss: 0.00079710
Iteration 8/25 | Loss: 0.00079710
Iteration 9/25 | Loss: 0.00079710
Iteration 10/25 | Loss: 0.00079710
Iteration 11/25 | Loss: 0.00079710
Iteration 12/25 | Loss: 0.00079710
Iteration 13/25 | Loss: 0.00079710
Iteration 14/25 | Loss: 0.00079710
Iteration 15/25 | Loss: 0.00079710
Iteration 16/25 | Loss: 0.00079710
Iteration 17/25 | Loss: 0.00079710
Iteration 18/25 | Loss: 0.00079710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007971031591296196, 0.0007971031591296196, 0.0007971031591296196, 0.0007971031591296196, 0.0007971031591296196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007971031591296196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079710
Iteration 2/1000 | Loss: 0.00002411
Iteration 3/1000 | Loss: 0.00001558
Iteration 4/1000 | Loss: 0.00001414
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001274
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001167
Iteration 14/1000 | Loss: 0.00001167
Iteration 15/1000 | Loss: 0.00001156
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001134
Iteration 26/1000 | Loss: 0.00001133
Iteration 27/1000 | Loss: 0.00001132
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001125
Iteration 36/1000 | Loss: 0.00001125
Iteration 37/1000 | Loss: 0.00001124
Iteration 38/1000 | Loss: 0.00001124
Iteration 39/1000 | Loss: 0.00001122
Iteration 40/1000 | Loss: 0.00001122
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001120
Iteration 46/1000 | Loss: 0.00001120
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001120
Iteration 49/1000 | Loss: 0.00001120
Iteration 50/1000 | Loss: 0.00001119
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001118
Iteration 53/1000 | Loss: 0.00001118
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001107
Iteration 76/1000 | Loss: 0.00001107
Iteration 77/1000 | Loss: 0.00001107
Iteration 78/1000 | Loss: 0.00001107
Iteration 79/1000 | Loss: 0.00001107
Iteration 80/1000 | Loss: 0.00001107
Iteration 81/1000 | Loss: 0.00001107
Iteration 82/1000 | Loss: 0.00001107
Iteration 83/1000 | Loss: 0.00001107
Iteration 84/1000 | Loss: 0.00001106
Iteration 85/1000 | Loss: 0.00001106
Iteration 86/1000 | Loss: 0.00001106
Iteration 87/1000 | Loss: 0.00001106
Iteration 88/1000 | Loss: 0.00001106
Iteration 89/1000 | Loss: 0.00001106
Iteration 90/1000 | Loss: 0.00001106
Iteration 91/1000 | Loss: 0.00001106
Iteration 92/1000 | Loss: 0.00001106
Iteration 93/1000 | Loss: 0.00001105
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001105
Iteration 96/1000 | Loss: 0.00001105
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001104
Iteration 105/1000 | Loss: 0.00001104
Iteration 106/1000 | Loss: 0.00001104
Iteration 107/1000 | Loss: 0.00001103
Iteration 108/1000 | Loss: 0.00001103
Iteration 109/1000 | Loss: 0.00001103
Iteration 110/1000 | Loss: 0.00001103
Iteration 111/1000 | Loss: 0.00001102
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001102
Iteration 114/1000 | Loss: 0.00001102
Iteration 115/1000 | Loss: 0.00001102
Iteration 116/1000 | Loss: 0.00001102
Iteration 117/1000 | Loss: 0.00001102
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001102
Iteration 122/1000 | Loss: 0.00001102
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001101
Iteration 126/1000 | Loss: 0.00001101
Iteration 127/1000 | Loss: 0.00001101
Iteration 128/1000 | Loss: 0.00001101
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001101
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001099
Iteration 136/1000 | Loss: 0.00001099
Iteration 137/1000 | Loss: 0.00001099
Iteration 138/1000 | Loss: 0.00001099
Iteration 139/1000 | Loss: 0.00001099
Iteration 140/1000 | Loss: 0.00001099
Iteration 141/1000 | Loss: 0.00001099
Iteration 142/1000 | Loss: 0.00001099
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Iteration 146/1000 | Loss: 0.00001099
Iteration 147/1000 | Loss: 0.00001098
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001098
Iteration 154/1000 | Loss: 0.00001098
Iteration 155/1000 | Loss: 0.00001098
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001097
Iteration 159/1000 | Loss: 0.00001097
Iteration 160/1000 | Loss: 0.00001097
Iteration 161/1000 | Loss: 0.00001097
Iteration 162/1000 | Loss: 0.00001097
Iteration 163/1000 | Loss: 0.00001097
Iteration 164/1000 | Loss: 0.00001097
Iteration 165/1000 | Loss: 0.00001096
Iteration 166/1000 | Loss: 0.00001096
Iteration 167/1000 | Loss: 0.00001096
Iteration 168/1000 | Loss: 0.00001096
Iteration 169/1000 | Loss: 0.00001096
Iteration 170/1000 | Loss: 0.00001096
Iteration 171/1000 | Loss: 0.00001096
Iteration 172/1000 | Loss: 0.00001096
Iteration 173/1000 | Loss: 0.00001096
Iteration 174/1000 | Loss: 0.00001096
Iteration 175/1000 | Loss: 0.00001096
Iteration 176/1000 | Loss: 0.00001096
Iteration 177/1000 | Loss: 0.00001096
Iteration 178/1000 | Loss: 0.00001096
Iteration 179/1000 | Loss: 0.00001095
Iteration 180/1000 | Loss: 0.00001095
Iteration 181/1000 | Loss: 0.00001095
Iteration 182/1000 | Loss: 0.00001095
Iteration 183/1000 | Loss: 0.00001095
Iteration 184/1000 | Loss: 0.00001095
Iteration 185/1000 | Loss: 0.00001095
Iteration 186/1000 | Loss: 0.00001095
Iteration 187/1000 | Loss: 0.00001095
Iteration 188/1000 | Loss: 0.00001095
Iteration 189/1000 | Loss: 0.00001095
Iteration 190/1000 | Loss: 0.00001095
Iteration 191/1000 | Loss: 0.00001095
Iteration 192/1000 | Loss: 0.00001095
Iteration 193/1000 | Loss: 0.00001095
Iteration 194/1000 | Loss: 0.00001095
Iteration 195/1000 | Loss: 0.00001095
Iteration 196/1000 | Loss: 0.00001095
Iteration 197/1000 | Loss: 0.00001095
Iteration 198/1000 | Loss: 0.00001095
Iteration 199/1000 | Loss: 0.00001095
Iteration 200/1000 | Loss: 0.00001095
Iteration 201/1000 | Loss: 0.00001095
Iteration 202/1000 | Loss: 0.00001095
Iteration 203/1000 | Loss: 0.00001095
Iteration 204/1000 | Loss: 0.00001095
Iteration 205/1000 | Loss: 0.00001095
Iteration 206/1000 | Loss: 0.00001095
Iteration 207/1000 | Loss: 0.00001095
Iteration 208/1000 | Loss: 0.00001095
Iteration 209/1000 | Loss: 0.00001095
Iteration 210/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.0950722753477748e-05, 1.0950722753477748e-05, 1.0950722753477748e-05, 1.0950722753477748e-05, 1.0950722753477748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0950722753477748e-05

Optimization complete. Final v2v error: 2.874605178833008 mm

Highest mean error: 3.1039040088653564 mm for frame 170

Lowest mean error: 2.797203302383423 mm for frame 152

Saving results

Total time: 43.45347213745117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402603
Iteration 2/25 | Loss: 0.00119336
Iteration 3/25 | Loss: 0.00111387
Iteration 4/25 | Loss: 0.00110001
Iteration 5/25 | Loss: 0.00109557
Iteration 6/25 | Loss: 0.00109467
Iteration 7/25 | Loss: 0.00109467
Iteration 8/25 | Loss: 0.00109467
Iteration 9/25 | Loss: 0.00109467
Iteration 10/25 | Loss: 0.00109467
Iteration 11/25 | Loss: 0.00109467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010946717811748385, 0.0010946717811748385, 0.0010946717811748385, 0.0010946717811748385, 0.0010946717811748385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010946717811748385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74505186
Iteration 2/25 | Loss: 0.00076909
Iteration 3/25 | Loss: 0.00076909
Iteration 4/25 | Loss: 0.00076909
Iteration 5/25 | Loss: 0.00076909
Iteration 6/25 | Loss: 0.00076909
Iteration 7/25 | Loss: 0.00076908
Iteration 8/25 | Loss: 0.00076908
Iteration 9/25 | Loss: 0.00076908
Iteration 10/25 | Loss: 0.00076908
Iteration 11/25 | Loss: 0.00076908
Iteration 12/25 | Loss: 0.00076908
Iteration 13/25 | Loss: 0.00076908
Iteration 14/25 | Loss: 0.00076908
Iteration 15/25 | Loss: 0.00076908
Iteration 16/25 | Loss: 0.00076908
Iteration 17/25 | Loss: 0.00076908
Iteration 18/25 | Loss: 0.00076908
Iteration 19/25 | Loss: 0.00076908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000769083562772721, 0.000769083562772721, 0.000769083562772721, 0.000769083562772721, 0.000769083562772721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000769083562772721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076908
Iteration 2/1000 | Loss: 0.00002143
Iteration 3/1000 | Loss: 0.00001399
Iteration 4/1000 | Loss: 0.00001265
Iteration 5/1000 | Loss: 0.00001194
Iteration 6/1000 | Loss: 0.00001147
Iteration 7/1000 | Loss: 0.00001115
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001066
Iteration 11/1000 | Loss: 0.00001050
Iteration 12/1000 | Loss: 0.00001047
Iteration 13/1000 | Loss: 0.00001044
Iteration 14/1000 | Loss: 0.00001038
Iteration 15/1000 | Loss: 0.00001037
Iteration 16/1000 | Loss: 0.00001035
Iteration 17/1000 | Loss: 0.00001035
Iteration 18/1000 | Loss: 0.00001034
Iteration 19/1000 | Loss: 0.00001032
Iteration 20/1000 | Loss: 0.00001031
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001022
Iteration 27/1000 | Loss: 0.00001021
Iteration 28/1000 | Loss: 0.00001021
Iteration 29/1000 | Loss: 0.00001021
Iteration 30/1000 | Loss: 0.00001020
Iteration 31/1000 | Loss: 0.00001019
Iteration 32/1000 | Loss: 0.00001017
Iteration 33/1000 | Loss: 0.00001016
Iteration 34/1000 | Loss: 0.00001016
Iteration 35/1000 | Loss: 0.00001014
Iteration 36/1000 | Loss: 0.00001014
Iteration 37/1000 | Loss: 0.00001014
Iteration 38/1000 | Loss: 0.00001013
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001007
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001000
Iteration 59/1000 | Loss: 0.00000999
Iteration 60/1000 | Loss: 0.00000996
Iteration 61/1000 | Loss: 0.00000996
Iteration 62/1000 | Loss: 0.00000994
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000993
Iteration 65/1000 | Loss: 0.00000993
Iteration 66/1000 | Loss: 0.00000993
Iteration 67/1000 | Loss: 0.00000992
Iteration 68/1000 | Loss: 0.00000992
Iteration 69/1000 | Loss: 0.00000991
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000990
Iteration 73/1000 | Loss: 0.00000990
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000990
Iteration 77/1000 | Loss: 0.00000990
Iteration 78/1000 | Loss: 0.00000990
Iteration 79/1000 | Loss: 0.00000990
Iteration 80/1000 | Loss: 0.00000990
Iteration 81/1000 | Loss: 0.00000990
Iteration 82/1000 | Loss: 0.00000990
Iteration 83/1000 | Loss: 0.00000990
Iteration 84/1000 | Loss: 0.00000990
Iteration 85/1000 | Loss: 0.00000989
Iteration 86/1000 | Loss: 0.00000989
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000988
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000988
Iteration 91/1000 | Loss: 0.00000988
Iteration 92/1000 | Loss: 0.00000987
Iteration 93/1000 | Loss: 0.00000987
Iteration 94/1000 | Loss: 0.00000987
Iteration 95/1000 | Loss: 0.00000987
Iteration 96/1000 | Loss: 0.00000987
Iteration 97/1000 | Loss: 0.00000987
Iteration 98/1000 | Loss: 0.00000987
Iteration 99/1000 | Loss: 0.00000987
Iteration 100/1000 | Loss: 0.00000986
Iteration 101/1000 | Loss: 0.00000986
Iteration 102/1000 | Loss: 0.00000986
Iteration 103/1000 | Loss: 0.00000985
Iteration 104/1000 | Loss: 0.00000985
Iteration 105/1000 | Loss: 0.00000985
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000985
Iteration 108/1000 | Loss: 0.00000985
Iteration 109/1000 | Loss: 0.00000985
Iteration 110/1000 | Loss: 0.00000985
Iteration 111/1000 | Loss: 0.00000985
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000984
Iteration 116/1000 | Loss: 0.00000984
Iteration 117/1000 | Loss: 0.00000984
Iteration 118/1000 | Loss: 0.00000984
Iteration 119/1000 | Loss: 0.00000984
Iteration 120/1000 | Loss: 0.00000984
Iteration 121/1000 | Loss: 0.00000984
Iteration 122/1000 | Loss: 0.00000983
Iteration 123/1000 | Loss: 0.00000983
Iteration 124/1000 | Loss: 0.00000983
Iteration 125/1000 | Loss: 0.00000983
Iteration 126/1000 | Loss: 0.00000983
Iteration 127/1000 | Loss: 0.00000983
Iteration 128/1000 | Loss: 0.00000983
Iteration 129/1000 | Loss: 0.00000983
Iteration 130/1000 | Loss: 0.00000983
Iteration 131/1000 | Loss: 0.00000982
Iteration 132/1000 | Loss: 0.00000982
Iteration 133/1000 | Loss: 0.00000982
Iteration 134/1000 | Loss: 0.00000981
Iteration 135/1000 | Loss: 0.00000981
Iteration 136/1000 | Loss: 0.00000981
Iteration 137/1000 | Loss: 0.00000981
Iteration 138/1000 | Loss: 0.00000980
Iteration 139/1000 | Loss: 0.00000980
Iteration 140/1000 | Loss: 0.00000980
Iteration 141/1000 | Loss: 0.00000980
Iteration 142/1000 | Loss: 0.00000980
Iteration 143/1000 | Loss: 0.00000980
Iteration 144/1000 | Loss: 0.00000980
Iteration 145/1000 | Loss: 0.00000980
Iteration 146/1000 | Loss: 0.00000980
Iteration 147/1000 | Loss: 0.00000980
Iteration 148/1000 | Loss: 0.00000980
Iteration 149/1000 | Loss: 0.00000980
Iteration 150/1000 | Loss: 0.00000980
Iteration 151/1000 | Loss: 0.00000979
Iteration 152/1000 | Loss: 0.00000979
Iteration 153/1000 | Loss: 0.00000978
Iteration 154/1000 | Loss: 0.00000978
Iteration 155/1000 | Loss: 0.00000978
Iteration 156/1000 | Loss: 0.00000978
Iteration 157/1000 | Loss: 0.00000978
Iteration 158/1000 | Loss: 0.00000978
Iteration 159/1000 | Loss: 0.00000978
Iteration 160/1000 | Loss: 0.00000978
Iteration 161/1000 | Loss: 0.00000978
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000978
Iteration 165/1000 | Loss: 0.00000978
Iteration 166/1000 | Loss: 0.00000978
Iteration 167/1000 | Loss: 0.00000977
Iteration 168/1000 | Loss: 0.00000977
Iteration 169/1000 | Loss: 0.00000977
Iteration 170/1000 | Loss: 0.00000977
Iteration 171/1000 | Loss: 0.00000977
Iteration 172/1000 | Loss: 0.00000977
Iteration 173/1000 | Loss: 0.00000977
Iteration 174/1000 | Loss: 0.00000977
Iteration 175/1000 | Loss: 0.00000977
Iteration 176/1000 | Loss: 0.00000977
Iteration 177/1000 | Loss: 0.00000977
Iteration 178/1000 | Loss: 0.00000976
Iteration 179/1000 | Loss: 0.00000976
Iteration 180/1000 | Loss: 0.00000976
Iteration 181/1000 | Loss: 0.00000976
Iteration 182/1000 | Loss: 0.00000976
Iteration 183/1000 | Loss: 0.00000976
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Iteration 191/1000 | Loss: 0.00000976
Iteration 192/1000 | Loss: 0.00000976
Iteration 193/1000 | Loss: 0.00000976
Iteration 194/1000 | Loss: 0.00000976
Iteration 195/1000 | Loss: 0.00000976
Iteration 196/1000 | Loss: 0.00000976
Iteration 197/1000 | Loss: 0.00000976
Iteration 198/1000 | Loss: 0.00000976
Iteration 199/1000 | Loss: 0.00000976
Iteration 200/1000 | Loss: 0.00000976
Iteration 201/1000 | Loss: 0.00000976
Iteration 202/1000 | Loss: 0.00000976
Iteration 203/1000 | Loss: 0.00000976
Iteration 204/1000 | Loss: 0.00000976
Iteration 205/1000 | Loss: 0.00000976
Iteration 206/1000 | Loss: 0.00000976
Iteration 207/1000 | Loss: 0.00000976
Iteration 208/1000 | Loss: 0.00000976
Iteration 209/1000 | Loss: 0.00000976
Iteration 210/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [9.757541192811914e-06, 9.757541192811914e-06, 9.757541192811914e-06, 9.757541192811914e-06, 9.757541192811914e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.757541192811914e-06

Optimization complete. Final v2v error: 2.711226224899292 mm

Highest mean error: 3.00398325920105 mm for frame 125

Lowest mean error: 2.5641467571258545 mm for frame 180

Saving results

Total time: 41.03333353996277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882168
Iteration 2/25 | Loss: 0.00147876
Iteration 3/25 | Loss: 0.00125654
Iteration 4/25 | Loss: 0.00123812
Iteration 5/25 | Loss: 0.00123719
Iteration 6/25 | Loss: 0.00124173
Iteration 7/25 | Loss: 0.00123269
Iteration 8/25 | Loss: 0.00123107
Iteration 9/25 | Loss: 0.00123081
Iteration 10/25 | Loss: 0.00123054
Iteration 11/25 | Loss: 0.00123466
Iteration 12/25 | Loss: 0.00123366
Iteration 13/25 | Loss: 0.00123368
Iteration 14/25 | Loss: 0.00123198
Iteration 15/25 | Loss: 0.00123131
Iteration 16/25 | Loss: 0.00123109
Iteration 17/25 | Loss: 0.00123442
Iteration 18/25 | Loss: 0.00123023
Iteration 19/25 | Loss: 0.00122948
Iteration 20/25 | Loss: 0.00122897
Iteration 21/25 | Loss: 0.00122881
Iteration 22/25 | Loss: 0.00122880
Iteration 23/25 | Loss: 0.00122879
Iteration 24/25 | Loss: 0.00122879
Iteration 25/25 | Loss: 0.00122879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.35222721
Iteration 2/25 | Loss: 0.00071401
Iteration 3/25 | Loss: 0.00071390
Iteration 4/25 | Loss: 0.00071390
Iteration 5/25 | Loss: 0.00071390
Iteration 6/25 | Loss: 0.00071390
Iteration 7/25 | Loss: 0.00071390
Iteration 8/25 | Loss: 0.00071390
Iteration 9/25 | Loss: 0.00071390
Iteration 10/25 | Loss: 0.00071390
Iteration 11/25 | Loss: 0.00071390
Iteration 12/25 | Loss: 0.00071390
Iteration 13/25 | Loss: 0.00071390
Iteration 14/25 | Loss: 0.00071390
Iteration 15/25 | Loss: 0.00071390
Iteration 16/25 | Loss: 0.00071390
Iteration 17/25 | Loss: 0.00071390
Iteration 18/25 | Loss: 0.00071390
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007138953078538179, 0.0007138953078538179, 0.0007138953078538179, 0.0007138953078538179, 0.0007138953078538179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007138953078538179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071390
Iteration 2/1000 | Loss: 0.00004501
Iteration 3/1000 | Loss: 0.00003111
Iteration 4/1000 | Loss: 0.00002690
Iteration 5/1000 | Loss: 0.00002516
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002328
Iteration 8/1000 | Loss: 0.00002280
Iteration 9/1000 | Loss: 0.00002234
Iteration 10/1000 | Loss: 0.00002198
Iteration 11/1000 | Loss: 0.00002179
Iteration 12/1000 | Loss: 0.00002173
Iteration 13/1000 | Loss: 0.00002157
Iteration 14/1000 | Loss: 0.00002156
Iteration 15/1000 | Loss: 0.00002155
Iteration 16/1000 | Loss: 0.00002151
Iteration 17/1000 | Loss: 0.00002147
Iteration 18/1000 | Loss: 0.00002147
Iteration 19/1000 | Loss: 0.00002146
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002143
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002141
Iteration 24/1000 | Loss: 0.00002140
Iteration 25/1000 | Loss: 0.00002139
Iteration 26/1000 | Loss: 0.00002139
Iteration 27/1000 | Loss: 0.00002136
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002133
Iteration 30/1000 | Loss: 0.00002132
Iteration 31/1000 | Loss: 0.00002132
Iteration 32/1000 | Loss: 0.00002132
Iteration 33/1000 | Loss: 0.00002131
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002130
Iteration 37/1000 | Loss: 0.00002130
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002129
Iteration 44/1000 | Loss: 0.00002129
Iteration 45/1000 | Loss: 0.00002129
Iteration 46/1000 | Loss: 0.00002129
Iteration 47/1000 | Loss: 0.00002129
Iteration 48/1000 | Loss: 0.00002128
Iteration 49/1000 | Loss: 0.00002128
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002127
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002127
Iteration 54/1000 | Loss: 0.00002127
Iteration 55/1000 | Loss: 0.00002126
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002125
Iteration 58/1000 | Loss: 0.00002125
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002124
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002122
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002122
Iteration 70/1000 | Loss: 0.00002121
Iteration 71/1000 | Loss: 0.00002121
Iteration 72/1000 | Loss: 0.00002120
Iteration 73/1000 | Loss: 0.00002120
Iteration 74/1000 | Loss: 0.00002120
Iteration 75/1000 | Loss: 0.00002120
Iteration 76/1000 | Loss: 0.00002119
Iteration 77/1000 | Loss: 0.00002119
Iteration 78/1000 | Loss: 0.00002119
Iteration 79/1000 | Loss: 0.00002118
Iteration 80/1000 | Loss: 0.00002118
Iteration 81/1000 | Loss: 0.00002118
Iteration 82/1000 | Loss: 0.00002118
Iteration 83/1000 | Loss: 0.00002117
Iteration 84/1000 | Loss: 0.00002117
Iteration 85/1000 | Loss: 0.00002117
Iteration 86/1000 | Loss: 0.00002117
Iteration 87/1000 | Loss: 0.00002116
Iteration 88/1000 | Loss: 0.00002116
Iteration 89/1000 | Loss: 0.00002116
Iteration 90/1000 | Loss: 0.00002115
Iteration 91/1000 | Loss: 0.00002115
Iteration 92/1000 | Loss: 0.00002115
Iteration 93/1000 | Loss: 0.00002114
Iteration 94/1000 | Loss: 0.00002114
Iteration 95/1000 | Loss: 0.00002114
Iteration 96/1000 | Loss: 0.00002114
Iteration 97/1000 | Loss: 0.00002113
Iteration 98/1000 | Loss: 0.00002113
Iteration 99/1000 | Loss: 0.00002113
Iteration 100/1000 | Loss: 0.00002113
Iteration 101/1000 | Loss: 0.00002113
Iteration 102/1000 | Loss: 0.00002113
Iteration 103/1000 | Loss: 0.00002113
Iteration 104/1000 | Loss: 0.00002112
Iteration 105/1000 | Loss: 0.00002112
Iteration 106/1000 | Loss: 0.00002112
Iteration 107/1000 | Loss: 0.00002112
Iteration 108/1000 | Loss: 0.00002111
Iteration 109/1000 | Loss: 0.00002111
Iteration 110/1000 | Loss: 0.00002111
Iteration 111/1000 | Loss: 0.00002111
Iteration 112/1000 | Loss: 0.00002111
Iteration 113/1000 | Loss: 0.00002111
Iteration 114/1000 | Loss: 0.00002110
Iteration 115/1000 | Loss: 0.00002110
Iteration 116/1000 | Loss: 0.00002110
Iteration 117/1000 | Loss: 0.00002110
Iteration 118/1000 | Loss: 0.00002110
Iteration 119/1000 | Loss: 0.00002110
Iteration 120/1000 | Loss: 0.00002110
Iteration 121/1000 | Loss: 0.00002110
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002110
Iteration 125/1000 | Loss: 0.00002110
Iteration 126/1000 | Loss: 0.00002110
Iteration 127/1000 | Loss: 0.00002110
Iteration 128/1000 | Loss: 0.00002110
Iteration 129/1000 | Loss: 0.00002110
Iteration 130/1000 | Loss: 0.00002110
Iteration 131/1000 | Loss: 0.00002110
Iteration 132/1000 | Loss: 0.00002110
Iteration 133/1000 | Loss: 0.00002110
Iteration 134/1000 | Loss: 0.00002110
Iteration 135/1000 | Loss: 0.00002110
Iteration 136/1000 | Loss: 0.00002110
Iteration 137/1000 | Loss: 0.00002110
Iteration 138/1000 | Loss: 0.00002110
Iteration 139/1000 | Loss: 0.00002110
Iteration 140/1000 | Loss: 0.00002110
Iteration 141/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.110254172293935e-05, 2.110254172293935e-05, 2.110254172293935e-05, 2.110254172293935e-05, 2.110254172293935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.110254172293935e-05

Optimization complete. Final v2v error: 3.831284999847412 mm

Highest mean error: 4.7514119148254395 mm for frame 30

Lowest mean error: 3.2358646392822266 mm for frame 206

Saving results

Total time: 73.26050972938538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895632
Iteration 2/25 | Loss: 0.00139225
Iteration 3/25 | Loss: 0.00123776
Iteration 4/25 | Loss: 0.00121340
Iteration 5/25 | Loss: 0.00120531
Iteration 6/25 | Loss: 0.00120356
Iteration 7/25 | Loss: 0.00120356
Iteration 8/25 | Loss: 0.00120356
Iteration 9/25 | Loss: 0.00120356
Iteration 10/25 | Loss: 0.00120356
Iteration 11/25 | Loss: 0.00120356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012035646941512823, 0.0012035646941512823, 0.0012035646941512823, 0.0012035646941512823, 0.0012035646941512823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012035646941512823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34094715
Iteration 2/25 | Loss: 0.00105615
Iteration 3/25 | Loss: 0.00105615
Iteration 4/25 | Loss: 0.00105614
Iteration 5/25 | Loss: 0.00105614
Iteration 6/25 | Loss: 0.00105614
Iteration 7/25 | Loss: 0.00105614
Iteration 8/25 | Loss: 0.00105614
Iteration 9/25 | Loss: 0.00105614
Iteration 10/25 | Loss: 0.00105614
Iteration 11/25 | Loss: 0.00105614
Iteration 12/25 | Loss: 0.00105614
Iteration 13/25 | Loss: 0.00105614
Iteration 14/25 | Loss: 0.00105614
Iteration 15/25 | Loss: 0.00105614
Iteration 16/25 | Loss: 0.00105614
Iteration 17/25 | Loss: 0.00105614
Iteration 18/25 | Loss: 0.00105614
Iteration 19/25 | Loss: 0.00105614
Iteration 20/25 | Loss: 0.00105614
Iteration 21/25 | Loss: 0.00105614
Iteration 22/25 | Loss: 0.00105614
Iteration 23/25 | Loss: 0.00105614
Iteration 24/25 | Loss: 0.00105614
Iteration 25/25 | Loss: 0.00105614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105614
Iteration 2/1000 | Loss: 0.00005697
Iteration 3/1000 | Loss: 0.00003717
Iteration 4/1000 | Loss: 0.00002907
Iteration 5/1000 | Loss: 0.00002764
Iteration 6/1000 | Loss: 0.00002655
Iteration 7/1000 | Loss: 0.00002565
Iteration 8/1000 | Loss: 0.00002516
Iteration 9/1000 | Loss: 0.00002471
Iteration 10/1000 | Loss: 0.00002440
Iteration 11/1000 | Loss: 0.00002415
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002398
Iteration 14/1000 | Loss: 0.00002395
Iteration 15/1000 | Loss: 0.00002384
Iteration 16/1000 | Loss: 0.00002379
Iteration 17/1000 | Loss: 0.00002376
Iteration 18/1000 | Loss: 0.00002375
Iteration 19/1000 | Loss: 0.00002375
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002374
Iteration 22/1000 | Loss: 0.00002373
Iteration 23/1000 | Loss: 0.00002373
Iteration 24/1000 | Loss: 0.00002372
Iteration 25/1000 | Loss: 0.00002372
Iteration 26/1000 | Loss: 0.00002371
Iteration 27/1000 | Loss: 0.00002371
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002370
Iteration 30/1000 | Loss: 0.00002369
Iteration 31/1000 | Loss: 0.00002369
Iteration 32/1000 | Loss: 0.00002368
Iteration 33/1000 | Loss: 0.00002367
Iteration 34/1000 | Loss: 0.00002367
Iteration 35/1000 | Loss: 0.00002366
Iteration 36/1000 | Loss: 0.00002365
Iteration 37/1000 | Loss: 0.00002365
Iteration 38/1000 | Loss: 0.00002364
Iteration 39/1000 | Loss: 0.00002362
Iteration 40/1000 | Loss: 0.00002361
Iteration 41/1000 | Loss: 0.00002361
Iteration 42/1000 | Loss: 0.00002361
Iteration 43/1000 | Loss: 0.00002359
Iteration 44/1000 | Loss: 0.00002358
Iteration 45/1000 | Loss: 0.00002358
Iteration 46/1000 | Loss: 0.00002357
Iteration 47/1000 | Loss: 0.00002355
Iteration 48/1000 | Loss: 0.00002354
Iteration 49/1000 | Loss: 0.00002354
Iteration 50/1000 | Loss: 0.00002354
Iteration 51/1000 | Loss: 0.00002353
Iteration 52/1000 | Loss: 0.00002353
Iteration 53/1000 | Loss: 0.00002353
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002353
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00002353
Iteration 58/1000 | Loss: 0.00002352
Iteration 59/1000 | Loss: 0.00002352
Iteration 60/1000 | Loss: 0.00002352
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002351
Iteration 63/1000 | Loss: 0.00002350
Iteration 64/1000 | Loss: 0.00002349
Iteration 65/1000 | Loss: 0.00002349
Iteration 66/1000 | Loss: 0.00002349
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00002348
Iteration 69/1000 | Loss: 0.00002347
Iteration 70/1000 | Loss: 0.00002347
Iteration 71/1000 | Loss: 0.00002347
Iteration 72/1000 | Loss: 0.00002347
Iteration 73/1000 | Loss: 0.00002346
Iteration 74/1000 | Loss: 0.00002346
Iteration 75/1000 | Loss: 0.00002346
Iteration 76/1000 | Loss: 0.00002345
Iteration 77/1000 | Loss: 0.00002345
Iteration 78/1000 | Loss: 0.00002345
Iteration 79/1000 | Loss: 0.00002345
Iteration 80/1000 | Loss: 0.00002344
Iteration 81/1000 | Loss: 0.00002344
Iteration 82/1000 | Loss: 0.00002344
Iteration 83/1000 | Loss: 0.00002344
Iteration 84/1000 | Loss: 0.00002344
Iteration 85/1000 | Loss: 0.00002343
Iteration 86/1000 | Loss: 0.00002343
Iteration 87/1000 | Loss: 0.00002343
Iteration 88/1000 | Loss: 0.00002343
Iteration 89/1000 | Loss: 0.00002343
Iteration 90/1000 | Loss: 0.00002343
Iteration 91/1000 | Loss: 0.00002343
Iteration 92/1000 | Loss: 0.00002343
Iteration 93/1000 | Loss: 0.00002343
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002343
Iteration 99/1000 | Loss: 0.00002343
Iteration 100/1000 | Loss: 0.00002342
Iteration 101/1000 | Loss: 0.00002342
Iteration 102/1000 | Loss: 0.00002342
Iteration 103/1000 | Loss: 0.00002342
Iteration 104/1000 | Loss: 0.00002342
Iteration 105/1000 | Loss: 0.00002341
Iteration 106/1000 | Loss: 0.00002341
Iteration 107/1000 | Loss: 0.00002341
Iteration 108/1000 | Loss: 0.00002340
Iteration 109/1000 | Loss: 0.00002340
Iteration 110/1000 | Loss: 0.00002340
Iteration 111/1000 | Loss: 0.00002339
Iteration 112/1000 | Loss: 0.00002339
Iteration 113/1000 | Loss: 0.00002339
Iteration 114/1000 | Loss: 0.00002339
Iteration 115/1000 | Loss: 0.00002339
Iteration 116/1000 | Loss: 0.00002339
Iteration 117/1000 | Loss: 0.00002338
Iteration 118/1000 | Loss: 0.00002338
Iteration 119/1000 | Loss: 0.00002338
Iteration 120/1000 | Loss: 0.00002338
Iteration 121/1000 | Loss: 0.00002337
Iteration 122/1000 | Loss: 0.00002337
Iteration 123/1000 | Loss: 0.00002337
Iteration 124/1000 | Loss: 0.00002337
Iteration 125/1000 | Loss: 0.00002336
Iteration 126/1000 | Loss: 0.00002336
Iteration 127/1000 | Loss: 0.00002336
Iteration 128/1000 | Loss: 0.00002336
Iteration 129/1000 | Loss: 0.00002336
Iteration 130/1000 | Loss: 0.00002336
Iteration 131/1000 | Loss: 0.00002336
Iteration 132/1000 | Loss: 0.00002335
Iteration 133/1000 | Loss: 0.00002335
Iteration 134/1000 | Loss: 0.00002335
Iteration 135/1000 | Loss: 0.00002335
Iteration 136/1000 | Loss: 0.00002335
Iteration 137/1000 | Loss: 0.00002335
Iteration 138/1000 | Loss: 0.00002335
Iteration 139/1000 | Loss: 0.00002335
Iteration 140/1000 | Loss: 0.00002334
Iteration 141/1000 | Loss: 0.00002334
Iteration 142/1000 | Loss: 0.00002334
Iteration 143/1000 | Loss: 0.00002334
Iteration 144/1000 | Loss: 0.00002334
Iteration 145/1000 | Loss: 0.00002334
Iteration 146/1000 | Loss: 0.00002334
Iteration 147/1000 | Loss: 0.00002334
Iteration 148/1000 | Loss: 0.00002334
Iteration 149/1000 | Loss: 0.00002334
Iteration 150/1000 | Loss: 0.00002334
Iteration 151/1000 | Loss: 0.00002334
Iteration 152/1000 | Loss: 0.00002334
Iteration 153/1000 | Loss: 0.00002333
Iteration 154/1000 | Loss: 0.00002333
Iteration 155/1000 | Loss: 0.00002333
Iteration 156/1000 | Loss: 0.00002333
Iteration 157/1000 | Loss: 0.00002333
Iteration 158/1000 | Loss: 0.00002333
Iteration 159/1000 | Loss: 0.00002333
Iteration 160/1000 | Loss: 0.00002333
Iteration 161/1000 | Loss: 0.00002333
Iteration 162/1000 | Loss: 0.00002333
Iteration 163/1000 | Loss: 0.00002333
Iteration 164/1000 | Loss: 0.00002333
Iteration 165/1000 | Loss: 0.00002333
Iteration 166/1000 | Loss: 0.00002333
Iteration 167/1000 | Loss: 0.00002333
Iteration 168/1000 | Loss: 0.00002333
Iteration 169/1000 | Loss: 0.00002333
Iteration 170/1000 | Loss: 0.00002333
Iteration 171/1000 | Loss: 0.00002332
Iteration 172/1000 | Loss: 0.00002332
Iteration 173/1000 | Loss: 0.00002332
Iteration 174/1000 | Loss: 0.00002332
Iteration 175/1000 | Loss: 0.00002332
Iteration 176/1000 | Loss: 0.00002332
Iteration 177/1000 | Loss: 0.00002332
Iteration 178/1000 | Loss: 0.00002332
Iteration 179/1000 | Loss: 0.00002332
Iteration 180/1000 | Loss: 0.00002332
Iteration 181/1000 | Loss: 0.00002332
Iteration 182/1000 | Loss: 0.00002332
Iteration 183/1000 | Loss: 0.00002332
Iteration 184/1000 | Loss: 0.00002331
Iteration 185/1000 | Loss: 0.00002331
Iteration 186/1000 | Loss: 0.00002331
Iteration 187/1000 | Loss: 0.00002331
Iteration 188/1000 | Loss: 0.00002331
Iteration 189/1000 | Loss: 0.00002331
Iteration 190/1000 | Loss: 0.00002331
Iteration 191/1000 | Loss: 0.00002331
Iteration 192/1000 | Loss: 0.00002331
Iteration 193/1000 | Loss: 0.00002331
Iteration 194/1000 | Loss: 0.00002330
Iteration 195/1000 | Loss: 0.00002330
Iteration 196/1000 | Loss: 0.00002330
Iteration 197/1000 | Loss: 0.00002330
Iteration 198/1000 | Loss: 0.00002330
Iteration 199/1000 | Loss: 0.00002330
Iteration 200/1000 | Loss: 0.00002330
Iteration 201/1000 | Loss: 0.00002330
Iteration 202/1000 | Loss: 0.00002330
Iteration 203/1000 | Loss: 0.00002330
Iteration 204/1000 | Loss: 0.00002330
Iteration 205/1000 | Loss: 0.00002330
Iteration 206/1000 | Loss: 0.00002329
Iteration 207/1000 | Loss: 0.00002329
Iteration 208/1000 | Loss: 0.00002329
Iteration 209/1000 | Loss: 0.00002329
Iteration 210/1000 | Loss: 0.00002329
Iteration 211/1000 | Loss: 0.00002329
Iteration 212/1000 | Loss: 0.00002329
Iteration 213/1000 | Loss: 0.00002329
Iteration 214/1000 | Loss: 0.00002329
Iteration 215/1000 | Loss: 0.00002329
Iteration 216/1000 | Loss: 0.00002329
Iteration 217/1000 | Loss: 0.00002329
Iteration 218/1000 | Loss: 0.00002329
Iteration 219/1000 | Loss: 0.00002329
Iteration 220/1000 | Loss: 0.00002329
Iteration 221/1000 | Loss: 0.00002329
Iteration 222/1000 | Loss: 0.00002329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.3288850570679642e-05, 2.3288850570679642e-05, 2.3288850570679642e-05, 2.3288850570679642e-05, 2.3288850570679642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3288850570679642e-05

Optimization complete. Final v2v error: 3.575587034225464 mm

Highest mean error: 5.6765241622924805 mm for frame 88

Lowest mean error: 2.4721453189849854 mm for frame 49

Saving results

Total time: 42.801432609558105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834061
Iteration 2/25 | Loss: 0.00164128
Iteration 3/25 | Loss: 0.00129396
Iteration 4/25 | Loss: 0.00127781
Iteration 5/25 | Loss: 0.00127384
Iteration 6/25 | Loss: 0.00127303
Iteration 7/25 | Loss: 0.00127303
Iteration 8/25 | Loss: 0.00127303
Iteration 9/25 | Loss: 0.00127303
Iteration 10/25 | Loss: 0.00127303
Iteration 11/25 | Loss: 0.00127303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012730273883789778, 0.0012730273883789778, 0.0012730273883789778, 0.0012730273883789778, 0.0012730273883789778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012730273883789778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50171256
Iteration 2/25 | Loss: 0.00089651
Iteration 3/25 | Loss: 0.00089651
Iteration 4/25 | Loss: 0.00089651
Iteration 5/25 | Loss: 0.00089651
Iteration 6/25 | Loss: 0.00089651
Iteration 7/25 | Loss: 0.00089651
Iteration 8/25 | Loss: 0.00089651
Iteration 9/25 | Loss: 0.00089651
Iteration 10/25 | Loss: 0.00089651
Iteration 11/25 | Loss: 0.00089651
Iteration 12/25 | Loss: 0.00089651
Iteration 13/25 | Loss: 0.00089650
Iteration 14/25 | Loss: 0.00089650
Iteration 15/25 | Loss: 0.00089650
Iteration 16/25 | Loss: 0.00089650
Iteration 17/25 | Loss: 0.00089650
Iteration 18/25 | Loss: 0.00089650
Iteration 19/25 | Loss: 0.00089650
Iteration 20/25 | Loss: 0.00089650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008965049055404961, 0.0008965049055404961, 0.0008965049055404961, 0.0008965049055404961, 0.0008965049055404961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008965049055404961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089650
Iteration 2/1000 | Loss: 0.00004992
Iteration 3/1000 | Loss: 0.00004108
Iteration 4/1000 | Loss: 0.00003663
Iteration 5/1000 | Loss: 0.00003501
Iteration 6/1000 | Loss: 0.00003392
Iteration 7/1000 | Loss: 0.00003317
Iteration 8/1000 | Loss: 0.00003244
Iteration 9/1000 | Loss: 0.00003193
Iteration 10/1000 | Loss: 0.00003151
Iteration 11/1000 | Loss: 0.00003106
Iteration 12/1000 | Loss: 0.00003081
Iteration 13/1000 | Loss: 0.00003059
Iteration 14/1000 | Loss: 0.00003041
Iteration 15/1000 | Loss: 0.00003024
Iteration 16/1000 | Loss: 0.00003022
Iteration 17/1000 | Loss: 0.00003008
Iteration 18/1000 | Loss: 0.00003004
Iteration 19/1000 | Loss: 0.00002997
Iteration 20/1000 | Loss: 0.00002988
Iteration 21/1000 | Loss: 0.00002974
Iteration 22/1000 | Loss: 0.00002962
Iteration 23/1000 | Loss: 0.00002954
Iteration 24/1000 | Loss: 0.00002951
Iteration 25/1000 | Loss: 0.00002951
Iteration 26/1000 | Loss: 0.00002950
Iteration 27/1000 | Loss: 0.00002950
Iteration 28/1000 | Loss: 0.00002948
Iteration 29/1000 | Loss: 0.00002947
Iteration 30/1000 | Loss: 0.00002947
Iteration 31/1000 | Loss: 0.00002947
Iteration 32/1000 | Loss: 0.00002947
Iteration 33/1000 | Loss: 0.00002947
Iteration 34/1000 | Loss: 0.00002947
Iteration 35/1000 | Loss: 0.00002946
Iteration 36/1000 | Loss: 0.00002946
Iteration 37/1000 | Loss: 0.00002946
Iteration 38/1000 | Loss: 0.00002945
Iteration 39/1000 | Loss: 0.00002945
Iteration 40/1000 | Loss: 0.00002945
Iteration 41/1000 | Loss: 0.00002945
Iteration 42/1000 | Loss: 0.00002944
Iteration 43/1000 | Loss: 0.00002944
Iteration 44/1000 | Loss: 0.00002944
Iteration 45/1000 | Loss: 0.00002944
Iteration 46/1000 | Loss: 0.00002944
Iteration 47/1000 | Loss: 0.00002943
Iteration 48/1000 | Loss: 0.00002943
Iteration 49/1000 | Loss: 0.00002943
Iteration 50/1000 | Loss: 0.00002943
Iteration 51/1000 | Loss: 0.00002943
Iteration 52/1000 | Loss: 0.00002942
Iteration 53/1000 | Loss: 0.00002942
Iteration 54/1000 | Loss: 0.00002942
Iteration 55/1000 | Loss: 0.00002942
Iteration 56/1000 | Loss: 0.00002942
Iteration 57/1000 | Loss: 0.00002942
Iteration 58/1000 | Loss: 0.00002940
Iteration 59/1000 | Loss: 0.00002940
Iteration 60/1000 | Loss: 0.00002939
Iteration 61/1000 | Loss: 0.00002939
Iteration 62/1000 | Loss: 0.00002939
Iteration 63/1000 | Loss: 0.00002938
Iteration 64/1000 | Loss: 0.00002938
Iteration 65/1000 | Loss: 0.00002938
Iteration 66/1000 | Loss: 0.00002938
Iteration 67/1000 | Loss: 0.00002938
Iteration 68/1000 | Loss: 0.00002937
Iteration 69/1000 | Loss: 0.00002937
Iteration 70/1000 | Loss: 0.00002937
Iteration 71/1000 | Loss: 0.00002937
Iteration 72/1000 | Loss: 0.00002937
Iteration 73/1000 | Loss: 0.00002937
Iteration 74/1000 | Loss: 0.00002937
Iteration 75/1000 | Loss: 0.00002937
Iteration 76/1000 | Loss: 0.00002937
Iteration 77/1000 | Loss: 0.00002937
Iteration 78/1000 | Loss: 0.00002936
Iteration 79/1000 | Loss: 0.00002936
Iteration 80/1000 | Loss: 0.00002935
Iteration 81/1000 | Loss: 0.00002935
Iteration 82/1000 | Loss: 0.00002935
Iteration 83/1000 | Loss: 0.00002935
Iteration 84/1000 | Loss: 0.00002934
Iteration 85/1000 | Loss: 0.00002934
Iteration 86/1000 | Loss: 0.00002934
Iteration 87/1000 | Loss: 0.00002933
Iteration 88/1000 | Loss: 0.00002933
Iteration 89/1000 | Loss: 0.00002933
Iteration 90/1000 | Loss: 0.00002933
Iteration 91/1000 | Loss: 0.00002933
Iteration 92/1000 | Loss: 0.00002933
Iteration 93/1000 | Loss: 0.00002932
Iteration 94/1000 | Loss: 0.00002932
Iteration 95/1000 | Loss: 0.00002932
Iteration 96/1000 | Loss: 0.00002932
Iteration 97/1000 | Loss: 0.00002932
Iteration 98/1000 | Loss: 0.00002932
Iteration 99/1000 | Loss: 0.00002932
Iteration 100/1000 | Loss: 0.00002932
Iteration 101/1000 | Loss: 0.00002931
Iteration 102/1000 | Loss: 0.00002931
Iteration 103/1000 | Loss: 0.00002931
Iteration 104/1000 | Loss: 0.00002931
Iteration 105/1000 | Loss: 0.00002931
Iteration 106/1000 | Loss: 0.00002931
Iteration 107/1000 | Loss: 0.00002931
Iteration 108/1000 | Loss: 0.00002931
Iteration 109/1000 | Loss: 0.00002931
Iteration 110/1000 | Loss: 0.00002930
Iteration 111/1000 | Loss: 0.00002930
Iteration 112/1000 | Loss: 0.00002930
Iteration 113/1000 | Loss: 0.00002930
Iteration 114/1000 | Loss: 0.00002930
Iteration 115/1000 | Loss: 0.00002929
Iteration 116/1000 | Loss: 0.00002929
Iteration 117/1000 | Loss: 0.00002929
Iteration 118/1000 | Loss: 0.00002929
Iteration 119/1000 | Loss: 0.00002929
Iteration 120/1000 | Loss: 0.00002928
Iteration 121/1000 | Loss: 0.00002928
Iteration 122/1000 | Loss: 0.00002928
Iteration 123/1000 | Loss: 0.00002928
Iteration 124/1000 | Loss: 0.00002927
Iteration 125/1000 | Loss: 0.00002927
Iteration 126/1000 | Loss: 0.00002927
Iteration 127/1000 | Loss: 0.00002927
Iteration 128/1000 | Loss: 0.00002927
Iteration 129/1000 | Loss: 0.00002927
Iteration 130/1000 | Loss: 0.00002927
Iteration 131/1000 | Loss: 0.00002927
Iteration 132/1000 | Loss: 0.00002926
Iteration 133/1000 | Loss: 0.00002926
Iteration 134/1000 | Loss: 0.00002926
Iteration 135/1000 | Loss: 0.00002926
Iteration 136/1000 | Loss: 0.00002926
Iteration 137/1000 | Loss: 0.00002926
Iteration 138/1000 | Loss: 0.00002926
Iteration 139/1000 | Loss: 0.00002926
Iteration 140/1000 | Loss: 0.00002925
Iteration 141/1000 | Loss: 0.00002925
Iteration 142/1000 | Loss: 0.00002925
Iteration 143/1000 | Loss: 0.00002925
Iteration 144/1000 | Loss: 0.00002925
Iteration 145/1000 | Loss: 0.00002925
Iteration 146/1000 | Loss: 0.00002925
Iteration 147/1000 | Loss: 0.00002925
Iteration 148/1000 | Loss: 0.00002925
Iteration 149/1000 | Loss: 0.00002925
Iteration 150/1000 | Loss: 0.00002925
Iteration 151/1000 | Loss: 0.00002925
Iteration 152/1000 | Loss: 0.00002925
Iteration 153/1000 | Loss: 0.00002925
Iteration 154/1000 | Loss: 0.00002925
Iteration 155/1000 | Loss: 0.00002925
Iteration 156/1000 | Loss: 0.00002925
Iteration 157/1000 | Loss: 0.00002925
Iteration 158/1000 | Loss: 0.00002925
Iteration 159/1000 | Loss: 0.00002925
Iteration 160/1000 | Loss: 0.00002925
Iteration 161/1000 | Loss: 0.00002925
Iteration 162/1000 | Loss: 0.00002925
Iteration 163/1000 | Loss: 0.00002925
Iteration 164/1000 | Loss: 0.00002925
Iteration 165/1000 | Loss: 0.00002925
Iteration 166/1000 | Loss: 0.00002925
Iteration 167/1000 | Loss: 0.00002925
Iteration 168/1000 | Loss: 0.00002925
Iteration 169/1000 | Loss: 0.00002925
Iteration 170/1000 | Loss: 0.00002925
Iteration 171/1000 | Loss: 0.00002925
Iteration 172/1000 | Loss: 0.00002925
Iteration 173/1000 | Loss: 0.00002925
Iteration 174/1000 | Loss: 0.00002925
Iteration 175/1000 | Loss: 0.00002925
Iteration 176/1000 | Loss: 0.00002925
Iteration 177/1000 | Loss: 0.00002925
Iteration 178/1000 | Loss: 0.00002925
Iteration 179/1000 | Loss: 0.00002925
Iteration 180/1000 | Loss: 0.00002925
Iteration 181/1000 | Loss: 0.00002925
Iteration 182/1000 | Loss: 0.00002925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.9250337320263498e-05, 2.9250337320263498e-05, 2.9250337320263498e-05, 2.9250337320263498e-05, 2.9250337320263498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9250337320263498e-05

Optimization complete. Final v2v error: 4.440431594848633 mm

Highest mean error: 5.459315776824951 mm for frame 0

Lowest mean error: 3.698110342025757 mm for frame 11

Saving results

Total time: 48.372753858566284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091216
Iteration 2/25 | Loss: 0.00151107
Iteration 3/25 | Loss: 0.00127583
Iteration 4/25 | Loss: 0.00126134
Iteration 5/25 | Loss: 0.00126535
Iteration 6/25 | Loss: 0.00126083
Iteration 7/25 | Loss: 0.00125841
Iteration 8/25 | Loss: 0.00125821
Iteration 9/25 | Loss: 0.00125821
Iteration 10/25 | Loss: 0.00125821
Iteration 11/25 | Loss: 0.00125820
Iteration 12/25 | Loss: 0.00125820
Iteration 13/25 | Loss: 0.00125820
Iteration 14/25 | Loss: 0.00125820
Iteration 15/25 | Loss: 0.00125820
Iteration 16/25 | Loss: 0.00125820
Iteration 17/25 | Loss: 0.00125820
Iteration 18/25 | Loss: 0.00125820
Iteration 19/25 | Loss: 0.00125820
Iteration 20/25 | Loss: 0.00125820
Iteration 21/25 | Loss: 0.00125820
Iteration 22/25 | Loss: 0.00125820
Iteration 23/25 | Loss: 0.00125820
Iteration 24/25 | Loss: 0.00125820
Iteration 25/25 | Loss: 0.00125820

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86253810
Iteration 2/25 | Loss: 0.00059340
Iteration 3/25 | Loss: 0.00059339
Iteration 4/25 | Loss: 0.00059339
Iteration 5/25 | Loss: 0.00059339
Iteration 6/25 | Loss: 0.00059339
Iteration 7/25 | Loss: 0.00059339
Iteration 8/25 | Loss: 0.00059339
Iteration 9/25 | Loss: 0.00059339
Iteration 10/25 | Loss: 0.00059338
Iteration 11/25 | Loss: 0.00059338
Iteration 12/25 | Loss: 0.00059338
Iteration 13/25 | Loss: 0.00059338
Iteration 14/25 | Loss: 0.00059338
Iteration 15/25 | Loss: 0.00059338
Iteration 16/25 | Loss: 0.00059338
Iteration 17/25 | Loss: 0.00059338
Iteration 18/25 | Loss: 0.00059338
Iteration 19/25 | Loss: 0.00059338
Iteration 20/25 | Loss: 0.00059338
Iteration 21/25 | Loss: 0.00059338
Iteration 22/25 | Loss: 0.00059338
Iteration 23/25 | Loss: 0.00059338
Iteration 24/25 | Loss: 0.00059338
Iteration 25/25 | Loss: 0.00059338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059338
Iteration 2/1000 | Loss: 0.00003788
Iteration 3/1000 | Loss: 0.00002568
Iteration 4/1000 | Loss: 0.00002255
Iteration 5/1000 | Loss: 0.00002161
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00010312
Iteration 8/1000 | Loss: 0.00002266
Iteration 9/1000 | Loss: 0.00002034
Iteration 10/1000 | Loss: 0.00002015
Iteration 11/1000 | Loss: 0.00002012
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00001996
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001982
Iteration 17/1000 | Loss: 0.00013508
Iteration 18/1000 | Loss: 0.00003113
Iteration 19/1000 | Loss: 0.00001973
Iteration 20/1000 | Loss: 0.00007588
Iteration 21/1000 | Loss: 0.00005458
Iteration 22/1000 | Loss: 0.00002518
Iteration 23/1000 | Loss: 0.00002148
Iteration 24/1000 | Loss: 0.00002009
Iteration 25/1000 | Loss: 0.00001966
Iteration 26/1000 | Loss: 0.00001963
Iteration 27/1000 | Loss: 0.00007665
Iteration 28/1000 | Loss: 0.00002964
Iteration 29/1000 | Loss: 0.00001967
Iteration 30/1000 | Loss: 0.00003546
Iteration 31/1000 | Loss: 0.00002025
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00002802
Iteration 34/1000 | Loss: 0.00002167
Iteration 35/1000 | Loss: 0.00002046
Iteration 36/1000 | Loss: 0.00001962
Iteration 37/1000 | Loss: 0.00001957
Iteration 38/1000 | Loss: 0.00001956
Iteration 39/1000 | Loss: 0.00001955
Iteration 40/1000 | Loss: 0.00001954
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001953
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001951
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001951
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001950
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001950
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001950
Iteration 63/1000 | Loss: 0.00001950
Iteration 64/1000 | Loss: 0.00001950
Iteration 65/1000 | Loss: 0.00001950
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001948
Iteration 71/1000 | Loss: 0.00001948
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001948
Iteration 74/1000 | Loss: 0.00001947
Iteration 75/1000 | Loss: 0.00001947
Iteration 76/1000 | Loss: 0.00001947
Iteration 77/1000 | Loss: 0.00001947
Iteration 78/1000 | Loss: 0.00001947
Iteration 79/1000 | Loss: 0.00001947
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001946
Iteration 85/1000 | Loss: 0.00001946
Iteration 86/1000 | Loss: 0.00001946
Iteration 87/1000 | Loss: 0.00001946
Iteration 88/1000 | Loss: 0.00001946
Iteration 89/1000 | Loss: 0.00001946
Iteration 90/1000 | Loss: 0.00001946
Iteration 91/1000 | Loss: 0.00001946
Iteration 92/1000 | Loss: 0.00001945
Iteration 93/1000 | Loss: 0.00001945
Iteration 94/1000 | Loss: 0.00001945
Iteration 95/1000 | Loss: 0.00001945
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001945
Iteration 98/1000 | Loss: 0.00001945
Iteration 99/1000 | Loss: 0.00001945
Iteration 100/1000 | Loss: 0.00001945
Iteration 101/1000 | Loss: 0.00001945
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001945
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001945
Iteration 107/1000 | Loss: 0.00001945
Iteration 108/1000 | Loss: 0.00001945
Iteration 109/1000 | Loss: 0.00001945
Iteration 110/1000 | Loss: 0.00001945
Iteration 111/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.9451952539384365e-05, 1.9451952539384365e-05, 1.9451952539384365e-05, 1.9451952539384365e-05, 1.9451952539384365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9451952539384365e-05

Optimization complete. Final v2v error: 3.5796191692352295 mm

Highest mean error: 4.065469741821289 mm for frame 104

Lowest mean error: 3.24351167678833 mm for frame 52

Saving results

Total time: 57.522929430007935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062985
Iteration 2/25 | Loss: 0.00285922
Iteration 3/25 | Loss: 0.00203721
Iteration 4/25 | Loss: 0.00159161
Iteration 5/25 | Loss: 0.00148776
Iteration 6/25 | Loss: 0.00147904
Iteration 7/25 | Loss: 0.00141407
Iteration 8/25 | Loss: 0.00127988
Iteration 9/25 | Loss: 0.00123819
Iteration 10/25 | Loss: 0.00122197
Iteration 11/25 | Loss: 0.00121057
Iteration 12/25 | Loss: 0.00121199
Iteration 13/25 | Loss: 0.00120233
Iteration 14/25 | Loss: 0.00120005
Iteration 15/25 | Loss: 0.00119958
Iteration 16/25 | Loss: 0.00119944
Iteration 17/25 | Loss: 0.00119943
Iteration 18/25 | Loss: 0.00119943
Iteration 19/25 | Loss: 0.00119943
Iteration 20/25 | Loss: 0.00119943
Iteration 21/25 | Loss: 0.00119943
Iteration 22/25 | Loss: 0.00119942
Iteration 23/25 | Loss: 0.00119942
Iteration 24/25 | Loss: 0.00119942
Iteration 25/25 | Loss: 0.00119942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88902116
Iteration 2/25 | Loss: 0.00108664
Iteration 3/25 | Loss: 0.00108664
Iteration 4/25 | Loss: 0.00108664
Iteration 5/25 | Loss: 0.00107510
Iteration 6/25 | Loss: 0.00107510
Iteration 7/25 | Loss: 0.00107510
Iteration 8/25 | Loss: 0.00107510
Iteration 9/25 | Loss: 0.00107510
Iteration 10/25 | Loss: 0.00107510
Iteration 11/25 | Loss: 0.00107510
Iteration 12/25 | Loss: 0.00107510
Iteration 13/25 | Loss: 0.00107510
Iteration 14/25 | Loss: 0.00107510
Iteration 15/25 | Loss: 0.00107510
Iteration 16/25 | Loss: 0.00107510
Iteration 17/25 | Loss: 0.00107510
Iteration 18/25 | Loss: 0.00107510
Iteration 19/25 | Loss: 0.00107510
Iteration 20/25 | Loss: 0.00107510
Iteration 21/25 | Loss: 0.00107510
Iteration 22/25 | Loss: 0.00107510
Iteration 23/25 | Loss: 0.00107510
Iteration 24/25 | Loss: 0.00107510
Iteration 25/25 | Loss: 0.00107510

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107510
Iteration 2/1000 | Loss: 0.00003912
Iteration 3/1000 | Loss: 0.00001831
Iteration 4/1000 | Loss: 0.00003673
Iteration 5/1000 | Loss: 0.00001717
Iteration 6/1000 | Loss: 0.00004643
Iteration 7/1000 | Loss: 0.00001580
Iteration 8/1000 | Loss: 0.00002978
Iteration 9/1000 | Loss: 0.00004270
Iteration 10/1000 | Loss: 0.00001530
Iteration 11/1000 | Loss: 0.00003085
Iteration 12/1000 | Loss: 0.00001677
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001508
Iteration 16/1000 | Loss: 0.00001508
Iteration 17/1000 | Loss: 0.00001508
Iteration 18/1000 | Loss: 0.00001508
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001507
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001506
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001506
Iteration 25/1000 | Loss: 0.00001505
Iteration 26/1000 | Loss: 0.00001504
Iteration 27/1000 | Loss: 0.00001856
Iteration 28/1000 | Loss: 0.00003017
Iteration 29/1000 | Loss: 0.00001605
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001487
Iteration 36/1000 | Loss: 0.00001487
Iteration 37/1000 | Loss: 0.00001487
Iteration 38/1000 | Loss: 0.00001487
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001486
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001480
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00005183
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001470
Iteration 69/1000 | Loss: 0.00001470
Iteration 70/1000 | Loss: 0.00001470
Iteration 71/1000 | Loss: 0.00001470
Iteration 72/1000 | Loss: 0.00001470
Iteration 73/1000 | Loss: 0.00001470
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001469
Iteration 79/1000 | Loss: 0.00001469
Iteration 80/1000 | Loss: 0.00001469
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001468
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001468
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001467
Iteration 96/1000 | Loss: 0.00003249
Iteration 97/1000 | Loss: 0.00001472
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001465
Iteration 104/1000 | Loss: 0.00001465
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00001465
Iteration 107/1000 | Loss: 0.00001465
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Iteration 112/1000 | Loss: 0.00001464
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001464
Iteration 118/1000 | Loss: 0.00001464
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001463
Iteration 134/1000 | Loss: 0.00001463
Iteration 135/1000 | Loss: 0.00001463
Iteration 136/1000 | Loss: 0.00001463
Iteration 137/1000 | Loss: 0.00001463
Iteration 138/1000 | Loss: 0.00001463
Iteration 139/1000 | Loss: 0.00001463
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.4629665201937314e-05, 1.4629665201937314e-05, 1.4629665201937314e-05, 1.4629665201937314e-05, 1.4629665201937314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4629665201937314e-05

Optimization complete. Final v2v error: 3.1664469242095947 mm

Highest mean error: 3.8488881587982178 mm for frame 34

Lowest mean error: 2.931285858154297 mm for frame 48

Saving results

Total time: 64.11883759498596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346621
Iteration 2/25 | Loss: 0.00126053
Iteration 3/25 | Loss: 0.00114765
Iteration 4/25 | Loss: 0.00112542
Iteration 5/25 | Loss: 0.00111703
Iteration 6/25 | Loss: 0.00111542
Iteration 7/25 | Loss: 0.00111542
Iteration 8/25 | Loss: 0.00111542
Iteration 9/25 | Loss: 0.00111542
Iteration 10/25 | Loss: 0.00111542
Iteration 11/25 | Loss: 0.00111542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011154162930324674, 0.0011154162930324674, 0.0011154162930324674, 0.0011154162930324674, 0.0011154162930324674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011154162930324674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31843472
Iteration 2/25 | Loss: 0.00094528
Iteration 3/25 | Loss: 0.00094528
Iteration 4/25 | Loss: 0.00094527
Iteration 5/25 | Loss: 0.00094527
Iteration 6/25 | Loss: 0.00094527
Iteration 7/25 | Loss: 0.00094527
Iteration 8/25 | Loss: 0.00094527
Iteration 9/25 | Loss: 0.00094527
Iteration 10/25 | Loss: 0.00094527
Iteration 11/25 | Loss: 0.00094527
Iteration 12/25 | Loss: 0.00094527
Iteration 13/25 | Loss: 0.00094527
Iteration 14/25 | Loss: 0.00094527
Iteration 15/25 | Loss: 0.00094527
Iteration 16/25 | Loss: 0.00094527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009452719823457301, 0.0009452719823457301, 0.0009452719823457301, 0.0009452719823457301, 0.0009452719823457301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009452719823457301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094527
Iteration 2/1000 | Loss: 0.00004312
Iteration 3/1000 | Loss: 0.00003057
Iteration 4/1000 | Loss: 0.00002401
Iteration 5/1000 | Loss: 0.00002199
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001912
Iteration 8/1000 | Loss: 0.00001844
Iteration 9/1000 | Loss: 0.00001793
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001729
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001690
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001673
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001662
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001642
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001635
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001634
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001629
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001627
Iteration 70/1000 | Loss: 0.00001627
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001625
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001620
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.6153217075043358e-05, 1.6153217075043358e-05, 1.6153217075043358e-05, 1.6153217075043358e-05, 1.6153217075043358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6153217075043358e-05

Optimization complete. Final v2v error: 3.361074209213257 mm

Highest mean error: 4.211494445800781 mm for frame 143

Lowest mean error: 2.8271515369415283 mm for frame 178

Saving results

Total time: 46.44276142120361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00701357
Iteration 2/25 | Loss: 0.00133538
Iteration 3/25 | Loss: 0.00117985
Iteration 4/25 | Loss: 0.00113799
Iteration 5/25 | Loss: 0.00113695
Iteration 6/25 | Loss: 0.00112849
Iteration 7/25 | Loss: 0.00112756
Iteration 8/25 | Loss: 0.00112727
Iteration 9/25 | Loss: 0.00112694
Iteration 10/25 | Loss: 0.00112710
Iteration 11/25 | Loss: 0.00112661
Iteration 12/25 | Loss: 0.00112608
Iteration 13/25 | Loss: 0.00112599
Iteration 14/25 | Loss: 0.00112599
Iteration 15/25 | Loss: 0.00112599
Iteration 16/25 | Loss: 0.00112598
Iteration 17/25 | Loss: 0.00112598
Iteration 18/25 | Loss: 0.00112598
Iteration 19/25 | Loss: 0.00112598
Iteration 20/25 | Loss: 0.00112598
Iteration 21/25 | Loss: 0.00112598
Iteration 22/25 | Loss: 0.00112598
Iteration 23/25 | Loss: 0.00112598
Iteration 24/25 | Loss: 0.00112598
Iteration 25/25 | Loss: 0.00112597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88337255
Iteration 2/25 | Loss: 0.00088040
Iteration 3/25 | Loss: 0.00088040
Iteration 4/25 | Loss: 0.00088040
Iteration 5/25 | Loss: 0.00088040
Iteration 6/25 | Loss: 0.00088039
Iteration 7/25 | Loss: 0.00088039
Iteration 8/25 | Loss: 0.00088039
Iteration 9/25 | Loss: 0.00088039
Iteration 10/25 | Loss: 0.00088039
Iteration 11/25 | Loss: 0.00088039
Iteration 12/25 | Loss: 0.00088039
Iteration 13/25 | Loss: 0.00088039
Iteration 14/25 | Loss: 0.00088039
Iteration 15/25 | Loss: 0.00088039
Iteration 16/25 | Loss: 0.00088039
Iteration 17/25 | Loss: 0.00088039
Iteration 18/25 | Loss: 0.00088039
Iteration 19/25 | Loss: 0.00088039
Iteration 20/25 | Loss: 0.00088039
Iteration 21/25 | Loss: 0.00088039
Iteration 22/25 | Loss: 0.00088039
Iteration 23/25 | Loss: 0.00088039
Iteration 24/25 | Loss: 0.00088039
Iteration 25/25 | Loss: 0.00088039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088039
Iteration 2/1000 | Loss: 0.00001812
Iteration 3/1000 | Loss: 0.00001446
Iteration 4/1000 | Loss: 0.00006842
Iteration 5/1000 | Loss: 0.00001313
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001233
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001151
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00007490
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001098
Iteration 19/1000 | Loss: 0.00001098
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001090
Iteration 22/1000 | Loss: 0.00001089
Iteration 23/1000 | Loss: 0.00001088
Iteration 24/1000 | Loss: 0.00001085
Iteration 25/1000 | Loss: 0.00001082
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001074
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001073
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001069
Iteration 36/1000 | Loss: 0.00001068
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001067
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001064
Iteration 45/1000 | Loss: 0.00001064
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001063
Iteration 48/1000 | Loss: 0.00001063
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001063
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001063
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001062
Iteration 56/1000 | Loss: 0.00001062
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001061
Iteration 59/1000 | Loss: 0.00001061
Iteration 60/1000 | Loss: 0.00001060
Iteration 61/1000 | Loss: 0.00001060
Iteration 62/1000 | Loss: 0.00001060
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001060
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001059
Iteration 69/1000 | Loss: 0.00001059
Iteration 70/1000 | Loss: 0.00001059
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001058
Iteration 73/1000 | Loss: 0.00001058
Iteration 74/1000 | Loss: 0.00001058
Iteration 75/1000 | Loss: 0.00001058
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001057
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001057
Iteration 80/1000 | Loss: 0.00001057
Iteration 81/1000 | Loss: 0.00001056
Iteration 82/1000 | Loss: 0.00001056
Iteration 83/1000 | Loss: 0.00001056
Iteration 84/1000 | Loss: 0.00001055
Iteration 85/1000 | Loss: 0.00001055
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001054
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001052
Iteration 91/1000 | Loss: 0.00001052
Iteration 92/1000 | Loss: 0.00001052
Iteration 93/1000 | Loss: 0.00001051
Iteration 94/1000 | Loss: 0.00001050
Iteration 95/1000 | Loss: 0.00001050
Iteration 96/1000 | Loss: 0.00001049
Iteration 97/1000 | Loss: 0.00001049
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001048
Iteration 102/1000 | Loss: 0.00001048
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001048
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001047
Iteration 107/1000 | Loss: 0.00001047
Iteration 108/1000 | Loss: 0.00001047
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001045
Iteration 112/1000 | Loss: 0.00001045
Iteration 113/1000 | Loss: 0.00001045
Iteration 114/1000 | Loss: 0.00001044
Iteration 115/1000 | Loss: 0.00001044
Iteration 116/1000 | Loss: 0.00001044
Iteration 117/1000 | Loss: 0.00001044
Iteration 118/1000 | Loss: 0.00001044
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.0442219718242995e-05, 1.0442219718242995e-05, 1.0442219718242995e-05, 1.0442219718242995e-05, 1.0442219718242995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0442219718242995e-05

Optimization complete. Final v2v error: 2.7885735034942627 mm

Highest mean error: 3.0949158668518066 mm for frame 127

Lowest mean error: 2.5809803009033203 mm for frame 262

Saving results

Total time: 60.41315579414368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403792
Iteration 2/25 | Loss: 0.00126114
Iteration 3/25 | Loss: 0.00114010
Iteration 4/25 | Loss: 0.00112477
Iteration 5/25 | Loss: 0.00112141
Iteration 6/25 | Loss: 0.00112053
Iteration 7/25 | Loss: 0.00112053
Iteration 8/25 | Loss: 0.00112053
Iteration 9/25 | Loss: 0.00112053
Iteration 10/25 | Loss: 0.00112053
Iteration 11/25 | Loss: 0.00112053
Iteration 12/25 | Loss: 0.00112053
Iteration 13/25 | Loss: 0.00112053
Iteration 14/25 | Loss: 0.00112053
Iteration 15/25 | Loss: 0.00112053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011205271584913135, 0.0011205271584913135, 0.0011205271584913135, 0.0011205271584913135, 0.0011205271584913135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011205271584913135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34988928
Iteration 2/25 | Loss: 0.00073174
Iteration 3/25 | Loss: 0.00073174
Iteration 4/25 | Loss: 0.00073174
Iteration 5/25 | Loss: 0.00073174
Iteration 6/25 | Loss: 0.00073174
Iteration 7/25 | Loss: 0.00073174
Iteration 8/25 | Loss: 0.00073174
Iteration 9/25 | Loss: 0.00073174
Iteration 10/25 | Loss: 0.00073174
Iteration 11/25 | Loss: 0.00073174
Iteration 12/25 | Loss: 0.00073174
Iteration 13/25 | Loss: 0.00073174
Iteration 14/25 | Loss: 0.00073174
Iteration 15/25 | Loss: 0.00073174
Iteration 16/25 | Loss: 0.00073174
Iteration 17/25 | Loss: 0.00073174
Iteration 18/25 | Loss: 0.00073174
Iteration 19/25 | Loss: 0.00073174
Iteration 20/25 | Loss: 0.00073174
Iteration 21/25 | Loss: 0.00073174
Iteration 22/25 | Loss: 0.00073174
Iteration 23/25 | Loss: 0.00073174
Iteration 24/25 | Loss: 0.00073174
Iteration 25/25 | Loss: 0.00073174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073174
Iteration 2/1000 | Loss: 0.00002747
Iteration 3/1000 | Loss: 0.00001944
Iteration 4/1000 | Loss: 0.00001714
Iteration 5/1000 | Loss: 0.00001584
Iteration 6/1000 | Loss: 0.00001501
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001354
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001324
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001317
Iteration 16/1000 | Loss: 0.00001314
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001313
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001309
Iteration 30/1000 | Loss: 0.00001309
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001297
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001295
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001294
Iteration 86/1000 | Loss: 0.00001294
Iteration 87/1000 | Loss: 0.00001294
Iteration 88/1000 | Loss: 0.00001293
Iteration 89/1000 | Loss: 0.00001293
Iteration 90/1000 | Loss: 0.00001293
Iteration 91/1000 | Loss: 0.00001293
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001292
Iteration 95/1000 | Loss: 0.00001292
Iteration 96/1000 | Loss: 0.00001292
Iteration 97/1000 | Loss: 0.00001292
Iteration 98/1000 | Loss: 0.00001292
Iteration 99/1000 | Loss: 0.00001292
Iteration 100/1000 | Loss: 0.00001292
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001291
Iteration 103/1000 | Loss: 0.00001291
Iteration 104/1000 | Loss: 0.00001291
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001290
Iteration 108/1000 | Loss: 0.00001290
Iteration 109/1000 | Loss: 0.00001290
Iteration 110/1000 | Loss: 0.00001290
Iteration 111/1000 | Loss: 0.00001290
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001290
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001287
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00001287
Iteration 125/1000 | Loss: 0.00001287
Iteration 126/1000 | Loss: 0.00001287
Iteration 127/1000 | Loss: 0.00001287
Iteration 128/1000 | Loss: 0.00001287
Iteration 129/1000 | Loss: 0.00001286
Iteration 130/1000 | Loss: 0.00001286
Iteration 131/1000 | Loss: 0.00001286
Iteration 132/1000 | Loss: 0.00001286
Iteration 133/1000 | Loss: 0.00001286
Iteration 134/1000 | Loss: 0.00001286
Iteration 135/1000 | Loss: 0.00001286
Iteration 136/1000 | Loss: 0.00001285
Iteration 137/1000 | Loss: 0.00001285
Iteration 138/1000 | Loss: 0.00001285
Iteration 139/1000 | Loss: 0.00001285
Iteration 140/1000 | Loss: 0.00001285
Iteration 141/1000 | Loss: 0.00001285
Iteration 142/1000 | Loss: 0.00001284
Iteration 143/1000 | Loss: 0.00001284
Iteration 144/1000 | Loss: 0.00001284
Iteration 145/1000 | Loss: 0.00001284
Iteration 146/1000 | Loss: 0.00001284
Iteration 147/1000 | Loss: 0.00001284
Iteration 148/1000 | Loss: 0.00001284
Iteration 149/1000 | Loss: 0.00001284
Iteration 150/1000 | Loss: 0.00001284
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001283
Iteration 153/1000 | Loss: 0.00001283
Iteration 154/1000 | Loss: 0.00001283
Iteration 155/1000 | Loss: 0.00001283
Iteration 156/1000 | Loss: 0.00001283
Iteration 157/1000 | Loss: 0.00001282
Iteration 158/1000 | Loss: 0.00001282
Iteration 159/1000 | Loss: 0.00001282
Iteration 160/1000 | Loss: 0.00001282
Iteration 161/1000 | Loss: 0.00001281
Iteration 162/1000 | Loss: 0.00001281
Iteration 163/1000 | Loss: 0.00001281
Iteration 164/1000 | Loss: 0.00001281
Iteration 165/1000 | Loss: 0.00001281
Iteration 166/1000 | Loss: 0.00001281
Iteration 167/1000 | Loss: 0.00001281
Iteration 168/1000 | Loss: 0.00001280
Iteration 169/1000 | Loss: 0.00001280
Iteration 170/1000 | Loss: 0.00001280
Iteration 171/1000 | Loss: 0.00001280
Iteration 172/1000 | Loss: 0.00001280
Iteration 173/1000 | Loss: 0.00001280
Iteration 174/1000 | Loss: 0.00001280
Iteration 175/1000 | Loss: 0.00001280
Iteration 176/1000 | Loss: 0.00001280
Iteration 177/1000 | Loss: 0.00001280
Iteration 178/1000 | Loss: 0.00001280
Iteration 179/1000 | Loss: 0.00001280
Iteration 180/1000 | Loss: 0.00001280
Iteration 181/1000 | Loss: 0.00001280
Iteration 182/1000 | Loss: 0.00001280
Iteration 183/1000 | Loss: 0.00001280
Iteration 184/1000 | Loss: 0.00001280
Iteration 185/1000 | Loss: 0.00001280
Iteration 186/1000 | Loss: 0.00001280
Iteration 187/1000 | Loss: 0.00001280
Iteration 188/1000 | Loss: 0.00001280
Iteration 189/1000 | Loss: 0.00001280
Iteration 190/1000 | Loss: 0.00001280
Iteration 191/1000 | Loss: 0.00001280
Iteration 192/1000 | Loss: 0.00001280
Iteration 193/1000 | Loss: 0.00001280
Iteration 194/1000 | Loss: 0.00001280
Iteration 195/1000 | Loss: 0.00001280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.2801881894120015e-05, 1.2801881894120015e-05, 1.2801881894120015e-05, 1.2801881894120015e-05, 1.2801881894120015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2801881894120015e-05

Optimization complete. Final v2v error: 3.0102529525756836 mm

Highest mean error: 3.461465835571289 mm for frame 84

Lowest mean error: 2.681687355041504 mm for frame 14

Saving results

Total time: 38.69634962081909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711943
Iteration 2/25 | Loss: 0.00128338
Iteration 3/25 | Loss: 0.00115583
Iteration 4/25 | Loss: 0.00111926
Iteration 5/25 | Loss: 0.00111941
Iteration 6/25 | Loss: 0.00111238
Iteration 7/25 | Loss: 0.00111132
Iteration 8/25 | Loss: 0.00111071
Iteration 9/25 | Loss: 0.00111023
Iteration 10/25 | Loss: 0.00111007
Iteration 11/25 | Loss: 0.00110998
Iteration 12/25 | Loss: 0.00110998
Iteration 13/25 | Loss: 0.00110998
Iteration 14/25 | Loss: 0.00110997
Iteration 15/25 | Loss: 0.00110997
Iteration 16/25 | Loss: 0.00110997
Iteration 17/25 | Loss: 0.00110997
Iteration 18/25 | Loss: 0.00110997
Iteration 19/25 | Loss: 0.00110997
Iteration 20/25 | Loss: 0.00110997
Iteration 21/25 | Loss: 0.00110997
Iteration 22/25 | Loss: 0.00110997
Iteration 23/25 | Loss: 0.00110997
Iteration 24/25 | Loss: 0.00110997
Iteration 25/25 | Loss: 0.00110997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82760501
Iteration 2/25 | Loss: 0.00085557
Iteration 3/25 | Loss: 0.00085557
Iteration 4/25 | Loss: 0.00085557
Iteration 5/25 | Loss: 0.00085557
Iteration 6/25 | Loss: 0.00085557
Iteration 7/25 | Loss: 0.00085557
Iteration 8/25 | Loss: 0.00085556
Iteration 9/25 | Loss: 0.00085556
Iteration 10/25 | Loss: 0.00085556
Iteration 11/25 | Loss: 0.00085556
Iteration 12/25 | Loss: 0.00085556
Iteration 13/25 | Loss: 0.00085556
Iteration 14/25 | Loss: 0.00085556
Iteration 15/25 | Loss: 0.00085556
Iteration 16/25 | Loss: 0.00085556
Iteration 17/25 | Loss: 0.00085556
Iteration 18/25 | Loss: 0.00085556
Iteration 19/25 | Loss: 0.00085556
Iteration 20/25 | Loss: 0.00085556
Iteration 21/25 | Loss: 0.00085556
Iteration 22/25 | Loss: 0.00085556
Iteration 23/25 | Loss: 0.00085556
Iteration 24/25 | Loss: 0.00085556
Iteration 25/25 | Loss: 0.00085556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085556
Iteration 2/1000 | Loss: 0.00001623
Iteration 3/1000 | Loss: 0.00001271
Iteration 4/1000 | Loss: 0.00001175
Iteration 5/1000 | Loss: 0.00001115
Iteration 6/1000 | Loss: 0.00005658
Iteration 7/1000 | Loss: 0.00001075
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001029
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001012
Iteration 12/1000 | Loss: 0.00001004
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00001003
Iteration 15/1000 | Loss: 0.00000999
Iteration 16/1000 | Loss: 0.00000995
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000987
Iteration 19/1000 | Loss: 0.00000980
Iteration 20/1000 | Loss: 0.00000980
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000979
Iteration 23/1000 | Loss: 0.00000979
Iteration 24/1000 | Loss: 0.00000975
Iteration 25/1000 | Loss: 0.00000975
Iteration 26/1000 | Loss: 0.00000973
Iteration 27/1000 | Loss: 0.00000972
Iteration 28/1000 | Loss: 0.00000972
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000971
Iteration 31/1000 | Loss: 0.00000971
Iteration 32/1000 | Loss: 0.00000971
Iteration 33/1000 | Loss: 0.00000971
Iteration 34/1000 | Loss: 0.00000970
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000967
Iteration 37/1000 | Loss: 0.00005972
Iteration 38/1000 | Loss: 0.00000966
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000962
Iteration 41/1000 | Loss: 0.00000959
Iteration 42/1000 | Loss: 0.00000958
Iteration 43/1000 | Loss: 0.00000958
Iteration 44/1000 | Loss: 0.00002575
Iteration 45/1000 | Loss: 0.00000994
Iteration 46/1000 | Loss: 0.00000958
Iteration 47/1000 | Loss: 0.00000953
Iteration 48/1000 | Loss: 0.00000951
Iteration 49/1000 | Loss: 0.00000951
Iteration 50/1000 | Loss: 0.00000950
Iteration 51/1000 | Loss: 0.00000950
Iteration 52/1000 | Loss: 0.00000950
Iteration 53/1000 | Loss: 0.00000950
Iteration 54/1000 | Loss: 0.00000950
Iteration 55/1000 | Loss: 0.00000949
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000948
Iteration 59/1000 | Loss: 0.00000948
Iteration 60/1000 | Loss: 0.00000948
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000948
Iteration 64/1000 | Loss: 0.00000948
Iteration 65/1000 | Loss: 0.00000948
Iteration 66/1000 | Loss: 0.00000948
Iteration 67/1000 | Loss: 0.00000948
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000946
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [9.459080501983408e-06, 9.459080501983408e-06, 9.459080501983408e-06, 9.459080501983408e-06, 9.459080501983408e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.459080501983408e-06

Optimization complete. Final v2v error: 2.6596906185150146 mm

Highest mean error: 3.0867974758148193 mm for frame 145

Lowest mean error: 2.47466778755188 mm for frame 27

Saving results

Total time: 53.101398944854736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628362
Iteration 2/25 | Loss: 0.00132518
Iteration 3/25 | Loss: 0.00117604
Iteration 4/25 | Loss: 0.00114723
Iteration 5/25 | Loss: 0.00113888
Iteration 6/25 | Loss: 0.00113639
Iteration 7/25 | Loss: 0.00113639
Iteration 8/25 | Loss: 0.00113639
Iteration 9/25 | Loss: 0.00113639
Iteration 10/25 | Loss: 0.00113639
Iteration 11/25 | Loss: 0.00113639
Iteration 12/25 | Loss: 0.00113639
Iteration 13/25 | Loss: 0.00113639
Iteration 14/25 | Loss: 0.00113639
Iteration 15/25 | Loss: 0.00113639
Iteration 16/25 | Loss: 0.00113639
Iteration 17/25 | Loss: 0.00113639
Iteration 18/25 | Loss: 0.00113639
Iteration 19/25 | Loss: 0.00113639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011363859521225095, 0.0011363859521225095, 0.0011363859521225095, 0.0011363859521225095, 0.0011363859521225095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011363859521225095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38734329
Iteration 2/25 | Loss: 0.00109830
Iteration 3/25 | Loss: 0.00109830
Iteration 4/25 | Loss: 0.00109830
Iteration 5/25 | Loss: 0.00109829
Iteration 6/25 | Loss: 0.00109829
Iteration 7/25 | Loss: 0.00109829
Iteration 8/25 | Loss: 0.00109829
Iteration 9/25 | Loss: 0.00109829
Iteration 10/25 | Loss: 0.00109829
Iteration 11/25 | Loss: 0.00109829
Iteration 12/25 | Loss: 0.00109829
Iteration 13/25 | Loss: 0.00109829
Iteration 14/25 | Loss: 0.00109829
Iteration 15/25 | Loss: 0.00109829
Iteration 16/25 | Loss: 0.00109829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010982932290062308, 0.0010982932290062308, 0.0010982932290062308, 0.0010982932290062308, 0.0010982932290062308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010982932290062308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109829
Iteration 2/1000 | Loss: 0.00003760
Iteration 3/1000 | Loss: 0.00002239
Iteration 4/1000 | Loss: 0.00001722
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001241
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001191
Iteration 13/1000 | Loss: 0.00001184
Iteration 14/1000 | Loss: 0.00001183
Iteration 15/1000 | Loss: 0.00001182
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001181
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001177
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001176
Iteration 22/1000 | Loss: 0.00001175
Iteration 23/1000 | Loss: 0.00001175
Iteration 24/1000 | Loss: 0.00001174
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001171
Iteration 28/1000 | Loss: 0.00001170
Iteration 29/1000 | Loss: 0.00001170
Iteration 30/1000 | Loss: 0.00001170
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001168
Iteration 33/1000 | Loss: 0.00001167
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001166
Iteration 36/1000 | Loss: 0.00001165
Iteration 37/1000 | Loss: 0.00001165
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001156
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001153
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001151
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001148
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001147
Iteration 85/1000 | Loss: 0.00001147
Iteration 86/1000 | Loss: 0.00001147
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001146
Iteration 95/1000 | Loss: 0.00001146
Iteration 96/1000 | Loss: 0.00001146
Iteration 97/1000 | Loss: 0.00001146
Iteration 98/1000 | Loss: 0.00001146
Iteration 99/1000 | Loss: 0.00001146
Iteration 100/1000 | Loss: 0.00001145
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001145
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001144
Iteration 109/1000 | Loss: 0.00001144
Iteration 110/1000 | Loss: 0.00001144
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001143
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001141
Iteration 123/1000 | Loss: 0.00001141
Iteration 124/1000 | Loss: 0.00001141
Iteration 125/1000 | Loss: 0.00001141
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001140
Iteration 130/1000 | Loss: 0.00001140
Iteration 131/1000 | Loss: 0.00001140
Iteration 132/1000 | Loss: 0.00001140
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001138
Iteration 144/1000 | Loss: 0.00001138
Iteration 145/1000 | Loss: 0.00001138
Iteration 146/1000 | Loss: 0.00001138
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001138
Iteration 150/1000 | Loss: 0.00001138
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001136
Iteration 163/1000 | Loss: 0.00001136
Iteration 164/1000 | Loss: 0.00001136
Iteration 165/1000 | Loss: 0.00001136
Iteration 166/1000 | Loss: 0.00001136
Iteration 167/1000 | Loss: 0.00001136
Iteration 168/1000 | Loss: 0.00001136
Iteration 169/1000 | Loss: 0.00001136
Iteration 170/1000 | Loss: 0.00001136
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001136
Iteration 174/1000 | Loss: 0.00001136
Iteration 175/1000 | Loss: 0.00001136
Iteration 176/1000 | Loss: 0.00001136
Iteration 177/1000 | Loss: 0.00001136
Iteration 178/1000 | Loss: 0.00001136
Iteration 179/1000 | Loss: 0.00001136
Iteration 180/1000 | Loss: 0.00001136
Iteration 181/1000 | Loss: 0.00001136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.135887941927649e-05, 1.135887941927649e-05, 1.135887941927649e-05, 1.135887941927649e-05, 1.135887941927649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.135887941927649e-05

Optimization complete. Final v2v error: 2.860548734664917 mm

Highest mean error: 3.4800233840942383 mm for frame 74

Lowest mean error: 2.4799067974090576 mm for frame 29

Saving results

Total time: 40.303751707077026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484560
Iteration 2/25 | Loss: 0.00123874
Iteration 3/25 | Loss: 0.00117580
Iteration 4/25 | Loss: 0.00116704
Iteration 5/25 | Loss: 0.00116461
Iteration 6/25 | Loss: 0.00116411
Iteration 7/25 | Loss: 0.00116411
Iteration 8/25 | Loss: 0.00116411
Iteration 9/25 | Loss: 0.00116411
Iteration 10/25 | Loss: 0.00116411
Iteration 11/25 | Loss: 0.00116411
Iteration 12/25 | Loss: 0.00116411
Iteration 13/25 | Loss: 0.00116411
Iteration 14/25 | Loss: 0.00116411
Iteration 15/25 | Loss: 0.00116411
Iteration 16/25 | Loss: 0.00116411
Iteration 17/25 | Loss: 0.00116411
Iteration 18/25 | Loss: 0.00116411
Iteration 19/25 | Loss: 0.00116411
Iteration 20/25 | Loss: 0.00116411
Iteration 21/25 | Loss: 0.00116411
Iteration 22/25 | Loss: 0.00116411
Iteration 23/25 | Loss: 0.00116411
Iteration 24/25 | Loss: 0.00116411
Iteration 25/25 | Loss: 0.00116411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39058328
Iteration 2/25 | Loss: 0.00079642
Iteration 3/25 | Loss: 0.00079639
Iteration 4/25 | Loss: 0.00079638
Iteration 5/25 | Loss: 0.00079638
Iteration 6/25 | Loss: 0.00079638
Iteration 7/25 | Loss: 0.00079638
Iteration 8/25 | Loss: 0.00079638
Iteration 9/25 | Loss: 0.00079638
Iteration 10/25 | Loss: 0.00079638
Iteration 11/25 | Loss: 0.00079638
Iteration 12/25 | Loss: 0.00079638
Iteration 13/25 | Loss: 0.00079638
Iteration 14/25 | Loss: 0.00079638
Iteration 15/25 | Loss: 0.00079638
Iteration 16/25 | Loss: 0.00079638
Iteration 17/25 | Loss: 0.00079638
Iteration 18/25 | Loss: 0.00079638
Iteration 19/25 | Loss: 0.00079638
Iteration 20/25 | Loss: 0.00079638
Iteration 21/25 | Loss: 0.00079638
Iteration 22/25 | Loss: 0.00079638
Iteration 23/25 | Loss: 0.00079638
Iteration 24/25 | Loss: 0.00079638
Iteration 25/25 | Loss: 0.00079638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079638
Iteration 2/1000 | Loss: 0.00002697
Iteration 3/1000 | Loss: 0.00001835
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001399
Iteration 9/1000 | Loss: 0.00001397
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001344
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001337
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001321
Iteration 19/1000 | Loss: 0.00001319
Iteration 20/1000 | Loss: 0.00001319
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001313
Iteration 32/1000 | Loss: 0.00001312
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001310
Iteration 37/1000 | Loss: 0.00001310
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001309
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001306
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001304
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001300
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001297
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001295
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001293
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001293
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001290
Iteration 100/1000 | Loss: 0.00001289
Iteration 101/1000 | Loss: 0.00001289
Iteration 102/1000 | Loss: 0.00001289
Iteration 103/1000 | Loss: 0.00001289
Iteration 104/1000 | Loss: 0.00001289
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001289
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001288
Iteration 112/1000 | Loss: 0.00001288
Iteration 113/1000 | Loss: 0.00001287
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001286
Iteration 116/1000 | Loss: 0.00001286
Iteration 117/1000 | Loss: 0.00001286
Iteration 118/1000 | Loss: 0.00001286
Iteration 119/1000 | Loss: 0.00001285
Iteration 120/1000 | Loss: 0.00001285
Iteration 121/1000 | Loss: 0.00001284
Iteration 122/1000 | Loss: 0.00001284
Iteration 123/1000 | Loss: 0.00001284
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001282
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001281
Iteration 132/1000 | Loss: 0.00001281
Iteration 133/1000 | Loss: 0.00001281
Iteration 134/1000 | Loss: 0.00001281
Iteration 135/1000 | Loss: 0.00001281
Iteration 136/1000 | Loss: 0.00001281
Iteration 137/1000 | Loss: 0.00001281
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001280
Iteration 140/1000 | Loss: 0.00001280
Iteration 141/1000 | Loss: 0.00001280
Iteration 142/1000 | Loss: 0.00001280
Iteration 143/1000 | Loss: 0.00001280
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001280
Iteration 146/1000 | Loss: 0.00001280
Iteration 147/1000 | Loss: 0.00001280
Iteration 148/1000 | Loss: 0.00001280
Iteration 149/1000 | Loss: 0.00001280
Iteration 150/1000 | Loss: 0.00001280
Iteration 151/1000 | Loss: 0.00001279
Iteration 152/1000 | Loss: 0.00001279
Iteration 153/1000 | Loss: 0.00001279
Iteration 154/1000 | Loss: 0.00001279
Iteration 155/1000 | Loss: 0.00001279
Iteration 156/1000 | Loss: 0.00001279
Iteration 157/1000 | Loss: 0.00001279
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001279
Iteration 163/1000 | Loss: 0.00001279
Iteration 164/1000 | Loss: 0.00001279
Iteration 165/1000 | Loss: 0.00001279
Iteration 166/1000 | Loss: 0.00001279
Iteration 167/1000 | Loss: 0.00001279
Iteration 168/1000 | Loss: 0.00001279
Iteration 169/1000 | Loss: 0.00001279
Iteration 170/1000 | Loss: 0.00001278
Iteration 171/1000 | Loss: 0.00001278
Iteration 172/1000 | Loss: 0.00001278
Iteration 173/1000 | Loss: 0.00001278
Iteration 174/1000 | Loss: 0.00001278
Iteration 175/1000 | Loss: 0.00001278
Iteration 176/1000 | Loss: 0.00001278
Iteration 177/1000 | Loss: 0.00001278
Iteration 178/1000 | Loss: 0.00001278
Iteration 179/1000 | Loss: 0.00001278
Iteration 180/1000 | Loss: 0.00001278
Iteration 181/1000 | Loss: 0.00001278
Iteration 182/1000 | Loss: 0.00001278
Iteration 183/1000 | Loss: 0.00001278
Iteration 184/1000 | Loss: 0.00001278
Iteration 185/1000 | Loss: 0.00001278
Iteration 186/1000 | Loss: 0.00001278
Iteration 187/1000 | Loss: 0.00001278
Iteration 188/1000 | Loss: 0.00001278
Iteration 189/1000 | Loss: 0.00001278
Iteration 190/1000 | Loss: 0.00001277
Iteration 191/1000 | Loss: 0.00001277
Iteration 192/1000 | Loss: 0.00001277
Iteration 193/1000 | Loss: 0.00001277
Iteration 194/1000 | Loss: 0.00001277
Iteration 195/1000 | Loss: 0.00001277
Iteration 196/1000 | Loss: 0.00001277
Iteration 197/1000 | Loss: 0.00001277
Iteration 198/1000 | Loss: 0.00001277
Iteration 199/1000 | Loss: 0.00001277
Iteration 200/1000 | Loss: 0.00001277
Iteration 201/1000 | Loss: 0.00001277
Iteration 202/1000 | Loss: 0.00001277
Iteration 203/1000 | Loss: 0.00001277
Iteration 204/1000 | Loss: 0.00001277
Iteration 205/1000 | Loss: 0.00001277
Iteration 206/1000 | Loss: 0.00001277
Iteration 207/1000 | Loss: 0.00001277
Iteration 208/1000 | Loss: 0.00001277
Iteration 209/1000 | Loss: 0.00001277
Iteration 210/1000 | Loss: 0.00001277
Iteration 211/1000 | Loss: 0.00001277
Iteration 212/1000 | Loss: 0.00001277
Iteration 213/1000 | Loss: 0.00001277
Iteration 214/1000 | Loss: 0.00001277
Iteration 215/1000 | Loss: 0.00001277
Iteration 216/1000 | Loss: 0.00001277
Iteration 217/1000 | Loss: 0.00001277
Iteration 218/1000 | Loss: 0.00001277
Iteration 219/1000 | Loss: 0.00001277
Iteration 220/1000 | Loss: 0.00001277
Iteration 221/1000 | Loss: 0.00001277
Iteration 222/1000 | Loss: 0.00001277
Iteration 223/1000 | Loss: 0.00001277
Iteration 224/1000 | Loss: 0.00001277
Iteration 225/1000 | Loss: 0.00001277
Iteration 226/1000 | Loss: 0.00001277
Iteration 227/1000 | Loss: 0.00001277
Iteration 228/1000 | Loss: 0.00001277
Iteration 229/1000 | Loss: 0.00001277
Iteration 230/1000 | Loss: 0.00001277
Iteration 231/1000 | Loss: 0.00001277
Iteration 232/1000 | Loss: 0.00001277
Iteration 233/1000 | Loss: 0.00001277
Iteration 234/1000 | Loss: 0.00001277
Iteration 235/1000 | Loss: 0.00001277
Iteration 236/1000 | Loss: 0.00001277
Iteration 237/1000 | Loss: 0.00001277
Iteration 238/1000 | Loss: 0.00001277
Iteration 239/1000 | Loss: 0.00001277
Iteration 240/1000 | Loss: 0.00001277
Iteration 241/1000 | Loss: 0.00001277
Iteration 242/1000 | Loss: 0.00001277
Iteration 243/1000 | Loss: 0.00001277
Iteration 244/1000 | Loss: 0.00001277
Iteration 245/1000 | Loss: 0.00001277
Iteration 246/1000 | Loss: 0.00001277
Iteration 247/1000 | Loss: 0.00001277
Iteration 248/1000 | Loss: 0.00001277
Iteration 249/1000 | Loss: 0.00001277
Iteration 250/1000 | Loss: 0.00001277
Iteration 251/1000 | Loss: 0.00001277
Iteration 252/1000 | Loss: 0.00001277
Iteration 253/1000 | Loss: 0.00001277
Iteration 254/1000 | Loss: 0.00001277
Iteration 255/1000 | Loss: 0.00001277
Iteration 256/1000 | Loss: 0.00001277
Iteration 257/1000 | Loss: 0.00001277
Iteration 258/1000 | Loss: 0.00001277
Iteration 259/1000 | Loss: 0.00001277
Iteration 260/1000 | Loss: 0.00001277
Iteration 261/1000 | Loss: 0.00001277
Iteration 262/1000 | Loss: 0.00001277
Iteration 263/1000 | Loss: 0.00001277
Iteration 264/1000 | Loss: 0.00001277
Iteration 265/1000 | Loss: 0.00001277
Iteration 266/1000 | Loss: 0.00001277
Iteration 267/1000 | Loss: 0.00001277
Iteration 268/1000 | Loss: 0.00001277
Iteration 269/1000 | Loss: 0.00001277
Iteration 270/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.2772449736075941e-05, 1.2772449736075941e-05, 1.2772449736075941e-05, 1.2772449736075941e-05, 1.2772449736075941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2772449736075941e-05

Optimization complete. Final v2v error: 3.0121655464172363 mm

Highest mean error: 3.4652066230773926 mm for frame 47

Lowest mean error: 2.740745782852173 mm for frame 147

Saving results

Total time: 43.58983492851257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046152
Iteration 2/25 | Loss: 0.01046152
Iteration 3/25 | Loss: 0.01046152
Iteration 4/25 | Loss: 0.01046152
Iteration 5/25 | Loss: 0.01046152
Iteration 6/25 | Loss: 0.01046152
Iteration 7/25 | Loss: 0.01046152
Iteration 8/25 | Loss: 0.01046152
Iteration 9/25 | Loss: 0.01046151
Iteration 10/25 | Loss: 0.01046151
Iteration 11/25 | Loss: 0.01046151
Iteration 12/25 | Loss: 0.01046151
Iteration 13/25 | Loss: 0.01046151
Iteration 14/25 | Loss: 0.01046151
Iteration 15/25 | Loss: 0.01046151
Iteration 16/25 | Loss: 0.01046151
Iteration 17/25 | Loss: 0.01046151
Iteration 18/25 | Loss: 0.01046150
Iteration 19/25 | Loss: 0.01046150
Iteration 20/25 | Loss: 0.01046150
Iteration 21/25 | Loss: 0.01046150
Iteration 22/25 | Loss: 0.01046150
Iteration 23/25 | Loss: 0.01046150
Iteration 24/25 | Loss: 0.01046150
Iteration 25/25 | Loss: 0.01046150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64960039
Iteration 2/25 | Loss: 0.09680950
Iteration 3/25 | Loss: 0.09623332
Iteration 4/25 | Loss: 0.09608734
Iteration 5/25 | Loss: 0.09607421
Iteration 6/25 | Loss: 0.09607421
Iteration 7/25 | Loss: 0.09607419
Iteration 8/25 | Loss: 0.09607419
Iteration 9/25 | Loss: 0.09607419
Iteration 10/25 | Loss: 0.09607419
Iteration 11/25 | Loss: 0.09607419
Iteration 12/25 | Loss: 0.09607419
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0960741862654686, 0.0960741862654686, 0.0960741862654686, 0.0960741862654686, 0.0960741862654686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0960741862654686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09607419
Iteration 2/1000 | Loss: 0.00363238
Iteration 3/1000 | Loss: 0.00142369
Iteration 4/1000 | Loss: 0.00096995
Iteration 5/1000 | Loss: 0.00032688
Iteration 6/1000 | Loss: 0.00031607
Iteration 7/1000 | Loss: 0.00132992
Iteration 8/1000 | Loss: 0.00007055
Iteration 9/1000 | Loss: 0.00022004
Iteration 10/1000 | Loss: 0.00023012
Iteration 11/1000 | Loss: 0.00016154
Iteration 12/1000 | Loss: 0.00071303
Iteration 13/1000 | Loss: 0.00008590
Iteration 14/1000 | Loss: 0.00004223
Iteration 15/1000 | Loss: 0.00005867
Iteration 16/1000 | Loss: 0.00006146
Iteration 17/1000 | Loss: 0.00007325
Iteration 18/1000 | Loss: 0.00002867
Iteration 19/1000 | Loss: 0.00024994
Iteration 20/1000 | Loss: 0.00013052
Iteration 21/1000 | Loss: 0.00009741
Iteration 22/1000 | Loss: 0.00002230
Iteration 23/1000 | Loss: 0.00020852
Iteration 24/1000 | Loss: 0.00006181
Iteration 25/1000 | Loss: 0.00051424
Iteration 26/1000 | Loss: 0.00011389
Iteration 27/1000 | Loss: 0.00006358
Iteration 28/1000 | Loss: 0.00004565
Iteration 29/1000 | Loss: 0.00002204
Iteration 30/1000 | Loss: 0.00008251
Iteration 31/1000 | Loss: 0.00007309
Iteration 32/1000 | Loss: 0.00007654
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00024404
Iteration 35/1000 | Loss: 0.00019846
Iteration 36/1000 | Loss: 0.00040156
Iteration 37/1000 | Loss: 0.00004375
Iteration 38/1000 | Loss: 0.00004907
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00010767
Iteration 41/1000 | Loss: 0.00018822
Iteration 42/1000 | Loss: 0.00005740
Iteration 43/1000 | Loss: 0.00029459
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00012602
Iteration 46/1000 | Loss: 0.00006203
Iteration 47/1000 | Loss: 0.00012226
Iteration 48/1000 | Loss: 0.00002017
Iteration 49/1000 | Loss: 0.00003470
Iteration 50/1000 | Loss: 0.00019301
Iteration 51/1000 | Loss: 0.00003200
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00008145
Iteration 55/1000 | Loss: 0.00027023
Iteration 56/1000 | Loss: 0.00003040
Iteration 57/1000 | Loss: 0.00005102
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00002580
Iteration 60/1000 | Loss: 0.00001958
Iteration 61/1000 | Loss: 0.00002045
Iteration 62/1000 | Loss: 0.00010796
Iteration 63/1000 | Loss: 0.00002233
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00001891
Iteration 66/1000 | Loss: 0.00001411
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00010255
Iteration 70/1000 | Loss: 0.00002257
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00007024
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00010096
Iteration 77/1000 | Loss: 0.00026090
Iteration 78/1000 | Loss: 0.00009678
Iteration 79/1000 | Loss: 0.00024594
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00009585
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00010102
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001320
Iteration 92/1000 | Loss: 0.00001320
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00004453
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00003478
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001399
Iteration 106/1000 | Loss: 0.00001285
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00002613
Iteration 122/1000 | Loss: 0.00001591
Iteration 123/1000 | Loss: 0.00009346
Iteration 124/1000 | Loss: 0.00002022
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00005575
Iteration 127/1000 | Loss: 0.00001750
Iteration 128/1000 | Loss: 0.00001792
Iteration 129/1000 | Loss: 0.00002325
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00013588
Iteration 132/1000 | Loss: 0.00002151
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001251
Iteration 135/1000 | Loss: 0.00002174
Iteration 136/1000 | Loss: 0.00016975
Iteration 137/1000 | Loss: 0.00001606
Iteration 138/1000 | Loss: 0.00001539
Iteration 139/1000 | Loss: 0.00005373
Iteration 140/1000 | Loss: 0.00002317
Iteration 141/1000 | Loss: 0.00006616
Iteration 142/1000 | Loss: 0.00002146
Iteration 143/1000 | Loss: 0.00004442
Iteration 144/1000 | Loss: 0.00001663
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001468
Iteration 148/1000 | Loss: 0.00001243
Iteration 149/1000 | Loss: 0.00001243
Iteration 150/1000 | Loss: 0.00001243
Iteration 151/1000 | Loss: 0.00001243
Iteration 152/1000 | Loss: 0.00001388
Iteration 153/1000 | Loss: 0.00001387
Iteration 154/1000 | Loss: 0.00002082
Iteration 155/1000 | Loss: 0.00001479
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001569
Iteration 158/1000 | Loss: 0.00001232
Iteration 159/1000 | Loss: 0.00001224
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001219
Iteration 163/1000 | Loss: 0.00001219
Iteration 164/1000 | Loss: 0.00001218
Iteration 165/1000 | Loss: 0.00001218
Iteration 166/1000 | Loss: 0.00001218
Iteration 167/1000 | Loss: 0.00001218
Iteration 168/1000 | Loss: 0.00001217
Iteration 169/1000 | Loss: 0.00001217
Iteration 170/1000 | Loss: 0.00001216
Iteration 171/1000 | Loss: 0.00001216
Iteration 172/1000 | Loss: 0.00001216
Iteration 173/1000 | Loss: 0.00001216
Iteration 174/1000 | Loss: 0.00001216
Iteration 175/1000 | Loss: 0.00001216
Iteration 176/1000 | Loss: 0.00001216
Iteration 177/1000 | Loss: 0.00001215
Iteration 178/1000 | Loss: 0.00001215
Iteration 179/1000 | Loss: 0.00002401
Iteration 180/1000 | Loss: 0.00001245
Iteration 181/1000 | Loss: 0.00001216
Iteration 182/1000 | Loss: 0.00001216
Iteration 183/1000 | Loss: 0.00001215
Iteration 184/1000 | Loss: 0.00001861
Iteration 185/1000 | Loss: 0.00001216
Iteration 186/1000 | Loss: 0.00001380
Iteration 187/1000 | Loss: 0.00001228
Iteration 188/1000 | Loss: 0.00001228
Iteration 189/1000 | Loss: 0.00001281
Iteration 190/1000 | Loss: 0.00001210
Iteration 191/1000 | Loss: 0.00001209
Iteration 192/1000 | Loss: 0.00001209
Iteration 193/1000 | Loss: 0.00001209
Iteration 194/1000 | Loss: 0.00001209
Iteration 195/1000 | Loss: 0.00001209
Iteration 196/1000 | Loss: 0.00001209
Iteration 197/1000 | Loss: 0.00001209
Iteration 198/1000 | Loss: 0.00001209
Iteration 199/1000 | Loss: 0.00001208
Iteration 200/1000 | Loss: 0.00001208
Iteration 201/1000 | Loss: 0.00001208
Iteration 202/1000 | Loss: 0.00001208
Iteration 203/1000 | Loss: 0.00001208
Iteration 204/1000 | Loss: 0.00001208
Iteration 205/1000 | Loss: 0.00001208
Iteration 206/1000 | Loss: 0.00001208
Iteration 207/1000 | Loss: 0.00001208
Iteration 208/1000 | Loss: 0.00001208
Iteration 209/1000 | Loss: 0.00001208
Iteration 210/1000 | Loss: 0.00001208
Iteration 211/1000 | Loss: 0.00001208
Iteration 212/1000 | Loss: 0.00001208
Iteration 213/1000 | Loss: 0.00001208
Iteration 214/1000 | Loss: 0.00001208
Iteration 215/1000 | Loss: 0.00001208
Iteration 216/1000 | Loss: 0.00001208
Iteration 217/1000 | Loss: 0.00001208
Iteration 218/1000 | Loss: 0.00001208
Iteration 219/1000 | Loss: 0.00001208
Iteration 220/1000 | Loss: 0.00001208
Iteration 221/1000 | Loss: 0.00001208
Iteration 222/1000 | Loss: 0.00001207
Iteration 223/1000 | Loss: 0.00001207
Iteration 224/1000 | Loss: 0.00001207
Iteration 225/1000 | Loss: 0.00001207
Iteration 226/1000 | Loss: 0.00001207
Iteration 227/1000 | Loss: 0.00001207
Iteration 228/1000 | Loss: 0.00001207
Iteration 229/1000 | Loss: 0.00001207
Iteration 230/1000 | Loss: 0.00001207
Iteration 231/1000 | Loss: 0.00001207
Iteration 232/1000 | Loss: 0.00001207
Iteration 233/1000 | Loss: 0.00001207
Iteration 234/1000 | Loss: 0.00001207
Iteration 235/1000 | Loss: 0.00001207
Iteration 236/1000 | Loss: 0.00001207
Iteration 237/1000 | Loss: 0.00001207
Iteration 238/1000 | Loss: 0.00001207
Iteration 239/1000 | Loss: 0.00001207
Iteration 240/1000 | Loss: 0.00001207
Iteration 241/1000 | Loss: 0.00001207
Iteration 242/1000 | Loss: 0.00001207
Iteration 243/1000 | Loss: 0.00001207
Iteration 244/1000 | Loss: 0.00001207
Iteration 245/1000 | Loss: 0.00001207
Iteration 246/1000 | Loss: 0.00001207
Iteration 247/1000 | Loss: 0.00001207
Iteration 248/1000 | Loss: 0.00001207
Iteration 249/1000 | Loss: 0.00001207
Iteration 250/1000 | Loss: 0.00001207
Iteration 251/1000 | Loss: 0.00001207
Iteration 252/1000 | Loss: 0.00001207
Iteration 253/1000 | Loss: 0.00001207
Iteration 254/1000 | Loss: 0.00001207
Iteration 255/1000 | Loss: 0.00001207
Iteration 256/1000 | Loss: 0.00001207
Iteration 257/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.2074294318153989e-05, 1.2074294318153989e-05, 1.2074294318153989e-05, 1.2074294318153989e-05, 1.2074294318153989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2074294318153989e-05

Optimization complete. Final v2v error: 2.9256770610809326 mm

Highest mean error: 9.380620956420898 mm for frame 218

Lowest mean error: 2.4875855445861816 mm for frame 97

Saving results

Total time: 226.90369582176208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767748
Iteration 2/25 | Loss: 0.00160386
Iteration 3/25 | Loss: 0.00130099
Iteration 4/25 | Loss: 0.00127938
Iteration 5/25 | Loss: 0.00127791
Iteration 6/25 | Loss: 0.00127791
Iteration 7/25 | Loss: 0.00127791
Iteration 8/25 | Loss: 0.00127791
Iteration 9/25 | Loss: 0.00127791
Iteration 10/25 | Loss: 0.00127791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012779103126376867, 0.0012779103126376867, 0.0012779103126376867, 0.0012779103126376867, 0.0012779103126376867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012779103126376867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43660069
Iteration 2/25 | Loss: 0.00080608
Iteration 3/25 | Loss: 0.00080606
Iteration 4/25 | Loss: 0.00080606
Iteration 5/25 | Loss: 0.00080605
Iteration 6/25 | Loss: 0.00080605
Iteration 7/25 | Loss: 0.00080605
Iteration 8/25 | Loss: 0.00080605
Iteration 9/25 | Loss: 0.00080605
Iteration 10/25 | Loss: 0.00080605
Iteration 11/25 | Loss: 0.00080605
Iteration 12/25 | Loss: 0.00080605
Iteration 13/25 | Loss: 0.00080605
Iteration 14/25 | Loss: 0.00080605
Iteration 15/25 | Loss: 0.00080605
Iteration 16/25 | Loss: 0.00080605
Iteration 17/25 | Loss: 0.00080605
Iteration 18/25 | Loss: 0.00080605
Iteration 19/25 | Loss: 0.00080605
Iteration 20/25 | Loss: 0.00080605
Iteration 21/25 | Loss: 0.00080605
Iteration 22/25 | Loss: 0.00080605
Iteration 23/25 | Loss: 0.00080605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008060532272793353, 0.0008060532272793353, 0.0008060532272793353, 0.0008060532272793353, 0.0008060532272793353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008060532272793353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080605
Iteration 2/1000 | Loss: 0.00003267
Iteration 3/1000 | Loss: 0.00002485
Iteration 4/1000 | Loss: 0.00002318
Iteration 5/1000 | Loss: 0.00002249
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00002159
Iteration 8/1000 | Loss: 0.00002122
Iteration 9/1000 | Loss: 0.00002083
Iteration 10/1000 | Loss: 0.00002081
Iteration 11/1000 | Loss: 0.00002056
Iteration 12/1000 | Loss: 0.00002041
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002024
Iteration 15/1000 | Loss: 0.00002023
Iteration 16/1000 | Loss: 0.00002022
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00002010
Iteration 19/1000 | Loss: 0.00002010
Iteration 20/1000 | Loss: 0.00002003
Iteration 21/1000 | Loss: 0.00002003
Iteration 22/1000 | Loss: 0.00002003
Iteration 23/1000 | Loss: 0.00002003
Iteration 24/1000 | Loss: 0.00002003
Iteration 25/1000 | Loss: 0.00002003
Iteration 26/1000 | Loss: 0.00002003
Iteration 27/1000 | Loss: 0.00002003
Iteration 28/1000 | Loss: 0.00002002
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002002
Iteration 32/1000 | Loss: 0.00002002
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00002002
Iteration 35/1000 | Loss: 0.00001999
Iteration 36/1000 | Loss: 0.00001999
Iteration 37/1000 | Loss: 0.00001998
Iteration 38/1000 | Loss: 0.00001998
Iteration 39/1000 | Loss: 0.00001998
Iteration 40/1000 | Loss: 0.00001988
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001981
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00001981
Iteration 50/1000 | Loss: 0.00001981
Iteration 51/1000 | Loss: 0.00001981
Iteration 52/1000 | Loss: 0.00001981
Iteration 53/1000 | Loss: 0.00001981
Iteration 54/1000 | Loss: 0.00001981
Iteration 55/1000 | Loss: 0.00001981
Iteration 56/1000 | Loss: 0.00001981
Iteration 57/1000 | Loss: 0.00001981
Iteration 58/1000 | Loss: 0.00001981
Iteration 59/1000 | Loss: 0.00001981
Iteration 60/1000 | Loss: 0.00001981
Iteration 61/1000 | Loss: 0.00001981
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001981
Iteration 65/1000 | Loss: 0.00001981
Iteration 66/1000 | Loss: 0.00001981
Iteration 67/1000 | Loss: 0.00001981
Iteration 68/1000 | Loss: 0.00001981
Iteration 69/1000 | Loss: 0.00001981
Iteration 70/1000 | Loss: 0.00001981
Iteration 71/1000 | Loss: 0.00001981
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001981
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001981
Iteration 78/1000 | Loss: 0.00001981
Iteration 79/1000 | Loss: 0.00001981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.9805404008366168e-05, 1.9805404008366168e-05, 1.9805404008366168e-05, 1.9805404008366168e-05, 1.9805404008366168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9805404008366168e-05

Optimization complete. Final v2v error: 3.738921880722046 mm

Highest mean error: 3.8706681728363037 mm for frame 2

Lowest mean error: 3.5411994457244873 mm for frame 212

Saving results

Total time: 35.661662101745605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466666
Iteration 2/25 | Loss: 0.00124651
Iteration 3/25 | Loss: 0.00118362
Iteration 4/25 | Loss: 0.00117196
Iteration 5/25 | Loss: 0.00116838
Iteration 6/25 | Loss: 0.00116838
Iteration 7/25 | Loss: 0.00116838
Iteration 8/25 | Loss: 0.00116838
Iteration 9/25 | Loss: 0.00116838
Iteration 10/25 | Loss: 0.00116838
Iteration 11/25 | Loss: 0.00116838
Iteration 12/25 | Loss: 0.00116838
Iteration 13/25 | Loss: 0.00116838
Iteration 14/25 | Loss: 0.00116838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011683831689879298, 0.0011683831689879298, 0.0011683831689879298, 0.0011683831689879298, 0.0011683831689879298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011683831689879298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35585117
Iteration 2/25 | Loss: 0.00075124
Iteration 3/25 | Loss: 0.00075122
Iteration 4/25 | Loss: 0.00075122
Iteration 5/25 | Loss: 0.00075122
Iteration 6/25 | Loss: 0.00075122
Iteration 7/25 | Loss: 0.00075122
Iteration 8/25 | Loss: 0.00075122
Iteration 9/25 | Loss: 0.00075122
Iteration 10/25 | Loss: 0.00075122
Iteration 11/25 | Loss: 0.00075122
Iteration 12/25 | Loss: 0.00075122
Iteration 13/25 | Loss: 0.00075122
Iteration 14/25 | Loss: 0.00075122
Iteration 15/25 | Loss: 0.00075122
Iteration 16/25 | Loss: 0.00075122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000751215498894453, 0.000751215498894453, 0.000751215498894453, 0.000751215498894453, 0.000751215498894453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000751215498894453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075122
Iteration 2/1000 | Loss: 0.00002741
Iteration 3/1000 | Loss: 0.00001910
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001615
Iteration 6/1000 | Loss: 0.00001564
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001498
Iteration 9/1000 | Loss: 0.00001475
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001430
Iteration 13/1000 | Loss: 0.00001425
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001419
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001400
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001396
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001391
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001389
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001381
Iteration 50/1000 | Loss: 0.00001381
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001380
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001380
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001380
Iteration 63/1000 | Loss: 0.00001379
Iteration 64/1000 | Loss: 0.00001379
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001374
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001373
Iteration 75/1000 | Loss: 0.00001373
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001368
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001366
Iteration 105/1000 | Loss: 0.00001366
Iteration 106/1000 | Loss: 0.00001366
Iteration 107/1000 | Loss: 0.00001366
Iteration 108/1000 | Loss: 0.00001366
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001365
Iteration 113/1000 | Loss: 0.00001365
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001364
Iteration 119/1000 | Loss: 0.00001364
Iteration 120/1000 | Loss: 0.00001364
Iteration 121/1000 | Loss: 0.00001364
Iteration 122/1000 | Loss: 0.00001364
Iteration 123/1000 | Loss: 0.00001364
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001363
Iteration 126/1000 | Loss: 0.00001363
Iteration 127/1000 | Loss: 0.00001363
Iteration 128/1000 | Loss: 0.00001363
Iteration 129/1000 | Loss: 0.00001363
Iteration 130/1000 | Loss: 0.00001363
Iteration 131/1000 | Loss: 0.00001363
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001363
Iteration 135/1000 | Loss: 0.00001363
Iteration 136/1000 | Loss: 0.00001363
Iteration 137/1000 | Loss: 0.00001363
Iteration 138/1000 | Loss: 0.00001363
Iteration 139/1000 | Loss: 0.00001363
Iteration 140/1000 | Loss: 0.00001362
Iteration 141/1000 | Loss: 0.00001362
Iteration 142/1000 | Loss: 0.00001362
Iteration 143/1000 | Loss: 0.00001362
Iteration 144/1000 | Loss: 0.00001362
Iteration 145/1000 | Loss: 0.00001362
Iteration 146/1000 | Loss: 0.00001362
Iteration 147/1000 | Loss: 0.00001362
Iteration 148/1000 | Loss: 0.00001362
Iteration 149/1000 | Loss: 0.00001361
Iteration 150/1000 | Loss: 0.00001361
Iteration 151/1000 | Loss: 0.00001361
Iteration 152/1000 | Loss: 0.00001361
Iteration 153/1000 | Loss: 0.00001361
Iteration 154/1000 | Loss: 0.00001361
Iteration 155/1000 | Loss: 0.00001361
Iteration 156/1000 | Loss: 0.00001361
Iteration 157/1000 | Loss: 0.00001361
Iteration 158/1000 | Loss: 0.00001360
Iteration 159/1000 | Loss: 0.00001360
Iteration 160/1000 | Loss: 0.00001360
Iteration 161/1000 | Loss: 0.00001360
Iteration 162/1000 | Loss: 0.00001360
Iteration 163/1000 | Loss: 0.00001360
Iteration 164/1000 | Loss: 0.00001360
Iteration 165/1000 | Loss: 0.00001359
Iteration 166/1000 | Loss: 0.00001359
Iteration 167/1000 | Loss: 0.00001359
Iteration 168/1000 | Loss: 0.00001359
Iteration 169/1000 | Loss: 0.00001358
Iteration 170/1000 | Loss: 0.00001358
Iteration 171/1000 | Loss: 0.00001358
Iteration 172/1000 | Loss: 0.00001358
Iteration 173/1000 | Loss: 0.00001358
Iteration 174/1000 | Loss: 0.00001358
Iteration 175/1000 | Loss: 0.00001358
Iteration 176/1000 | Loss: 0.00001358
Iteration 177/1000 | Loss: 0.00001358
Iteration 178/1000 | Loss: 0.00001358
Iteration 179/1000 | Loss: 0.00001358
Iteration 180/1000 | Loss: 0.00001358
Iteration 181/1000 | Loss: 0.00001358
Iteration 182/1000 | Loss: 0.00001358
Iteration 183/1000 | Loss: 0.00001358
Iteration 184/1000 | Loss: 0.00001358
Iteration 185/1000 | Loss: 0.00001358
Iteration 186/1000 | Loss: 0.00001358
Iteration 187/1000 | Loss: 0.00001357
Iteration 188/1000 | Loss: 0.00001357
Iteration 189/1000 | Loss: 0.00001357
Iteration 190/1000 | Loss: 0.00001357
Iteration 191/1000 | Loss: 0.00001357
Iteration 192/1000 | Loss: 0.00001357
Iteration 193/1000 | Loss: 0.00001357
Iteration 194/1000 | Loss: 0.00001357
Iteration 195/1000 | Loss: 0.00001357
Iteration 196/1000 | Loss: 0.00001357
Iteration 197/1000 | Loss: 0.00001357
Iteration 198/1000 | Loss: 0.00001357
Iteration 199/1000 | Loss: 0.00001357
Iteration 200/1000 | Loss: 0.00001356
Iteration 201/1000 | Loss: 0.00001356
Iteration 202/1000 | Loss: 0.00001356
Iteration 203/1000 | Loss: 0.00001356
Iteration 204/1000 | Loss: 0.00001356
Iteration 205/1000 | Loss: 0.00001356
Iteration 206/1000 | Loss: 0.00001356
Iteration 207/1000 | Loss: 0.00001356
Iteration 208/1000 | Loss: 0.00001356
Iteration 209/1000 | Loss: 0.00001356
Iteration 210/1000 | Loss: 0.00001356
Iteration 211/1000 | Loss: 0.00001356
Iteration 212/1000 | Loss: 0.00001356
Iteration 213/1000 | Loss: 0.00001356
Iteration 214/1000 | Loss: 0.00001356
Iteration 215/1000 | Loss: 0.00001356
Iteration 216/1000 | Loss: 0.00001356
Iteration 217/1000 | Loss: 0.00001355
Iteration 218/1000 | Loss: 0.00001355
Iteration 219/1000 | Loss: 0.00001355
Iteration 220/1000 | Loss: 0.00001355
Iteration 221/1000 | Loss: 0.00001355
Iteration 222/1000 | Loss: 0.00001355
Iteration 223/1000 | Loss: 0.00001355
Iteration 224/1000 | Loss: 0.00001355
Iteration 225/1000 | Loss: 0.00001355
Iteration 226/1000 | Loss: 0.00001355
Iteration 227/1000 | Loss: 0.00001355
Iteration 228/1000 | Loss: 0.00001355
Iteration 229/1000 | Loss: 0.00001355
Iteration 230/1000 | Loss: 0.00001355
Iteration 231/1000 | Loss: 0.00001355
Iteration 232/1000 | Loss: 0.00001355
Iteration 233/1000 | Loss: 0.00001355
Iteration 234/1000 | Loss: 0.00001355
Iteration 235/1000 | Loss: 0.00001355
Iteration 236/1000 | Loss: 0.00001355
Iteration 237/1000 | Loss: 0.00001355
Iteration 238/1000 | Loss: 0.00001355
Iteration 239/1000 | Loss: 0.00001355
Iteration 240/1000 | Loss: 0.00001355
Iteration 241/1000 | Loss: 0.00001355
Iteration 242/1000 | Loss: 0.00001355
Iteration 243/1000 | Loss: 0.00001355
Iteration 244/1000 | Loss: 0.00001355
Iteration 245/1000 | Loss: 0.00001355
Iteration 246/1000 | Loss: 0.00001355
Iteration 247/1000 | Loss: 0.00001355
Iteration 248/1000 | Loss: 0.00001355
Iteration 249/1000 | Loss: 0.00001355
Iteration 250/1000 | Loss: 0.00001355
Iteration 251/1000 | Loss: 0.00001355
Iteration 252/1000 | Loss: 0.00001355
Iteration 253/1000 | Loss: 0.00001355
Iteration 254/1000 | Loss: 0.00001355
Iteration 255/1000 | Loss: 0.00001355
Iteration 256/1000 | Loss: 0.00001355
Iteration 257/1000 | Loss: 0.00001355
Iteration 258/1000 | Loss: 0.00001355
Iteration 259/1000 | Loss: 0.00001355
Iteration 260/1000 | Loss: 0.00001355
Iteration 261/1000 | Loss: 0.00001355
Iteration 262/1000 | Loss: 0.00001355
Iteration 263/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.3549048162531108e-05, 1.3549048162531108e-05, 1.3549048162531108e-05, 1.3549048162531108e-05, 1.3549048162531108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3549048162531108e-05

Optimization complete. Final v2v error: 3.0621964931488037 mm

Highest mean error: 3.4489142894744873 mm for frame 178

Lowest mean error: 2.8181841373443604 mm for frame 223

Saving results

Total time: 48.688300132751465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486133
Iteration 2/25 | Loss: 0.00132942
Iteration 3/25 | Loss: 0.00121793
Iteration 4/25 | Loss: 0.00120946
Iteration 5/25 | Loss: 0.00120755
Iteration 6/25 | Loss: 0.00120746
Iteration 7/25 | Loss: 0.00120746
Iteration 8/25 | Loss: 0.00120746
Iteration 9/25 | Loss: 0.00120746
Iteration 10/25 | Loss: 0.00120746
Iteration 11/25 | Loss: 0.00120746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012074591359123588, 0.0012074591359123588, 0.0012074591359123588, 0.0012074591359123588, 0.0012074591359123588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012074591359123588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.74269295
Iteration 2/25 | Loss: 0.00066641
Iteration 3/25 | Loss: 0.00066631
Iteration 4/25 | Loss: 0.00066631
Iteration 5/25 | Loss: 0.00066631
Iteration 6/25 | Loss: 0.00066631
Iteration 7/25 | Loss: 0.00066631
Iteration 8/25 | Loss: 0.00066631
Iteration 9/25 | Loss: 0.00066631
Iteration 10/25 | Loss: 0.00066631
Iteration 11/25 | Loss: 0.00066631
Iteration 12/25 | Loss: 0.00066631
Iteration 13/25 | Loss: 0.00066631
Iteration 14/25 | Loss: 0.00066631
Iteration 15/25 | Loss: 0.00066631
Iteration 16/25 | Loss: 0.00066631
Iteration 17/25 | Loss: 0.00066631
Iteration 18/25 | Loss: 0.00066631
Iteration 19/25 | Loss: 0.00066631
Iteration 20/25 | Loss: 0.00066631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006663113599643111, 0.0006663113599643111, 0.0006663113599643111, 0.0006663113599643111, 0.0006663113599643111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006663113599643111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066631
Iteration 2/1000 | Loss: 0.00003664
Iteration 3/1000 | Loss: 0.00002185
Iteration 4/1000 | Loss: 0.00001948
Iteration 5/1000 | Loss: 0.00001876
Iteration 6/1000 | Loss: 0.00001820
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001667
Iteration 15/1000 | Loss: 0.00001666
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001652
Iteration 24/1000 | Loss: 0.00001652
Iteration 25/1000 | Loss: 0.00001651
Iteration 26/1000 | Loss: 0.00001651
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001647
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001625
Iteration 53/1000 | Loss: 0.00001624
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001621
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001617
Iteration 83/1000 | Loss: 0.00001617
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001616
Iteration 90/1000 | Loss: 0.00001616
Iteration 91/1000 | Loss: 0.00001616
Iteration 92/1000 | Loss: 0.00001616
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001616
Iteration 95/1000 | Loss: 0.00001616
Iteration 96/1000 | Loss: 0.00001616
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001616
Iteration 111/1000 | Loss: 0.00001616
Iteration 112/1000 | Loss: 0.00001616
Iteration 113/1000 | Loss: 0.00001616
Iteration 114/1000 | Loss: 0.00001616
Iteration 115/1000 | Loss: 0.00001616
Iteration 116/1000 | Loss: 0.00001616
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.6157231584656984e-05, 1.6157231584656984e-05, 1.6157231584656984e-05, 1.6157231584656984e-05, 1.6157231584656984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6157231584656984e-05

Optimization complete. Final v2v error: 3.363316297531128 mm

Highest mean error: 3.8847455978393555 mm for frame 39

Lowest mean error: 2.9710631370544434 mm for frame 0

Saving results

Total time: 35.598623514175415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408137
Iteration 2/25 | Loss: 0.00130176
Iteration 3/25 | Loss: 0.00118149
Iteration 4/25 | Loss: 0.00116074
Iteration 5/25 | Loss: 0.00115433
Iteration 6/25 | Loss: 0.00115243
Iteration 7/25 | Loss: 0.00115193
Iteration 8/25 | Loss: 0.00115187
Iteration 9/25 | Loss: 0.00115187
Iteration 10/25 | Loss: 0.00115187
Iteration 11/25 | Loss: 0.00115187
Iteration 12/25 | Loss: 0.00115187
Iteration 13/25 | Loss: 0.00115187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011518736137077212, 0.0011518736137077212, 0.0011518736137077212, 0.0011518736137077212, 0.0011518736137077212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011518736137077212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34874642
Iteration 2/25 | Loss: 0.00097807
Iteration 3/25 | Loss: 0.00097807
Iteration 4/25 | Loss: 0.00097807
Iteration 5/25 | Loss: 0.00097807
Iteration 6/25 | Loss: 0.00097807
Iteration 7/25 | Loss: 0.00097807
Iteration 8/25 | Loss: 0.00097807
Iteration 9/25 | Loss: 0.00097807
Iteration 10/25 | Loss: 0.00097807
Iteration 11/25 | Loss: 0.00097807
Iteration 12/25 | Loss: 0.00097807
Iteration 13/25 | Loss: 0.00097807
Iteration 14/25 | Loss: 0.00097807
Iteration 15/25 | Loss: 0.00097807
Iteration 16/25 | Loss: 0.00097807
Iteration 17/25 | Loss: 0.00097807
Iteration 18/25 | Loss: 0.00097807
Iteration 19/25 | Loss: 0.00097807
Iteration 20/25 | Loss: 0.00097807
Iteration 21/25 | Loss: 0.00097807
Iteration 22/25 | Loss: 0.00097807
Iteration 23/25 | Loss: 0.00097807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009780683321878314, 0.0009780683321878314, 0.0009780683321878314, 0.0009780683321878314, 0.0009780683321878314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009780683321878314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097807
Iteration 2/1000 | Loss: 0.00004456
Iteration 3/1000 | Loss: 0.00003171
Iteration 4/1000 | Loss: 0.00002511
Iteration 5/1000 | Loss: 0.00002325
Iteration 6/1000 | Loss: 0.00002227
Iteration 7/1000 | Loss: 0.00002134
Iteration 8/1000 | Loss: 0.00002081
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001941
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001934
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001916
Iteration 17/1000 | Loss: 0.00001907
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00001894
Iteration 20/1000 | Loss: 0.00001889
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001886
Iteration 26/1000 | Loss: 0.00001886
Iteration 27/1000 | Loss: 0.00001885
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001884
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00001883
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00001883
Iteration 35/1000 | Loss: 0.00001882
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001875
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001874
Iteration 45/1000 | Loss: 0.00001873
Iteration 46/1000 | Loss: 0.00001873
Iteration 47/1000 | Loss: 0.00001872
Iteration 48/1000 | Loss: 0.00001872
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001872
Iteration 52/1000 | Loss: 0.00001872
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001870
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001868
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00002306
Iteration 72/1000 | Loss: 0.00002306
Iteration 73/1000 | Loss: 0.00001954
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001847
Iteration 79/1000 | Loss: 0.00001847
Iteration 80/1000 | Loss: 0.00001846
Iteration 81/1000 | Loss: 0.00001845
Iteration 82/1000 | Loss: 0.00001845
Iteration 83/1000 | Loss: 0.00001845
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001844
Iteration 86/1000 | Loss: 0.00001844
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001842
Iteration 90/1000 | Loss: 0.00001842
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001818
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001803
Iteration 95/1000 | Loss: 0.00001799
Iteration 96/1000 | Loss: 0.00001798
Iteration 97/1000 | Loss: 0.00001795
Iteration 98/1000 | Loss: 0.00001794
Iteration 99/1000 | Loss: 0.00001793
Iteration 100/1000 | Loss: 0.00001793
Iteration 101/1000 | Loss: 0.00001792
Iteration 102/1000 | Loss: 0.00001792
Iteration 103/1000 | Loss: 0.00001792
Iteration 104/1000 | Loss: 0.00001792
Iteration 105/1000 | Loss: 0.00001792
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001792
Iteration 108/1000 | Loss: 0.00001792
Iteration 109/1000 | Loss: 0.00001791
Iteration 110/1000 | Loss: 0.00001791
Iteration 111/1000 | Loss: 0.00001791
Iteration 112/1000 | Loss: 0.00001791
Iteration 113/1000 | Loss: 0.00001791
Iteration 114/1000 | Loss: 0.00001791
Iteration 115/1000 | Loss: 0.00001791
Iteration 116/1000 | Loss: 0.00001791
Iteration 117/1000 | Loss: 0.00001791
Iteration 118/1000 | Loss: 0.00001791
Iteration 119/1000 | Loss: 0.00001791
Iteration 120/1000 | Loss: 0.00001790
Iteration 121/1000 | Loss: 0.00001790
Iteration 122/1000 | Loss: 0.00001790
Iteration 123/1000 | Loss: 0.00001790
Iteration 124/1000 | Loss: 0.00001790
Iteration 125/1000 | Loss: 0.00001790
Iteration 126/1000 | Loss: 0.00001790
Iteration 127/1000 | Loss: 0.00001790
Iteration 128/1000 | Loss: 0.00001789
Iteration 129/1000 | Loss: 0.00001789
Iteration 130/1000 | Loss: 0.00001789
Iteration 131/1000 | Loss: 0.00001789
Iteration 132/1000 | Loss: 0.00001788
Iteration 133/1000 | Loss: 0.00001788
Iteration 134/1000 | Loss: 0.00001788
Iteration 135/1000 | Loss: 0.00001788
Iteration 136/1000 | Loss: 0.00001787
Iteration 137/1000 | Loss: 0.00001787
Iteration 138/1000 | Loss: 0.00001787
Iteration 139/1000 | Loss: 0.00001786
Iteration 140/1000 | Loss: 0.00001786
Iteration 141/1000 | Loss: 0.00001785
Iteration 142/1000 | Loss: 0.00001785
Iteration 143/1000 | Loss: 0.00001785
Iteration 144/1000 | Loss: 0.00001785
Iteration 145/1000 | Loss: 0.00001784
Iteration 146/1000 | Loss: 0.00001784
Iteration 147/1000 | Loss: 0.00001783
Iteration 148/1000 | Loss: 0.00001783
Iteration 149/1000 | Loss: 0.00001783
Iteration 150/1000 | Loss: 0.00001783
Iteration 151/1000 | Loss: 0.00001783
Iteration 152/1000 | Loss: 0.00001783
Iteration 153/1000 | Loss: 0.00001783
Iteration 154/1000 | Loss: 0.00001783
Iteration 155/1000 | Loss: 0.00001782
Iteration 156/1000 | Loss: 0.00001782
Iteration 157/1000 | Loss: 0.00001782
Iteration 158/1000 | Loss: 0.00001782
Iteration 159/1000 | Loss: 0.00001782
Iteration 160/1000 | Loss: 0.00001782
Iteration 161/1000 | Loss: 0.00001782
Iteration 162/1000 | Loss: 0.00001781
Iteration 163/1000 | Loss: 0.00001781
Iteration 164/1000 | Loss: 0.00001781
Iteration 165/1000 | Loss: 0.00001781
Iteration 166/1000 | Loss: 0.00001780
Iteration 167/1000 | Loss: 0.00001780
Iteration 168/1000 | Loss: 0.00001780
Iteration 169/1000 | Loss: 0.00001780
Iteration 170/1000 | Loss: 0.00001780
Iteration 171/1000 | Loss: 0.00001779
Iteration 172/1000 | Loss: 0.00001779
Iteration 173/1000 | Loss: 0.00001779
Iteration 174/1000 | Loss: 0.00001779
Iteration 175/1000 | Loss: 0.00001778
Iteration 176/1000 | Loss: 0.00001778
Iteration 177/1000 | Loss: 0.00001778
Iteration 178/1000 | Loss: 0.00001777
Iteration 179/1000 | Loss: 0.00001777
Iteration 180/1000 | Loss: 0.00001777
Iteration 181/1000 | Loss: 0.00001777
Iteration 182/1000 | Loss: 0.00001777
Iteration 183/1000 | Loss: 0.00001776
Iteration 184/1000 | Loss: 0.00001776
Iteration 185/1000 | Loss: 0.00001776
Iteration 186/1000 | Loss: 0.00001775
Iteration 187/1000 | Loss: 0.00001775
Iteration 188/1000 | Loss: 0.00001775
Iteration 189/1000 | Loss: 0.00001775
Iteration 190/1000 | Loss: 0.00001774
Iteration 191/1000 | Loss: 0.00001774
Iteration 192/1000 | Loss: 0.00001774
Iteration 193/1000 | Loss: 0.00001773
Iteration 194/1000 | Loss: 0.00001773
Iteration 195/1000 | Loss: 0.00001773
Iteration 196/1000 | Loss: 0.00001773
Iteration 197/1000 | Loss: 0.00001772
Iteration 198/1000 | Loss: 0.00001772
Iteration 199/1000 | Loss: 0.00001772
Iteration 200/1000 | Loss: 0.00001772
Iteration 201/1000 | Loss: 0.00001772
Iteration 202/1000 | Loss: 0.00001772
Iteration 203/1000 | Loss: 0.00001772
Iteration 204/1000 | Loss: 0.00001771
Iteration 205/1000 | Loss: 0.00001771
Iteration 206/1000 | Loss: 0.00001771
Iteration 207/1000 | Loss: 0.00001771
Iteration 208/1000 | Loss: 0.00001771
Iteration 209/1000 | Loss: 0.00001771
Iteration 210/1000 | Loss: 0.00001771
Iteration 211/1000 | Loss: 0.00001771
Iteration 212/1000 | Loss: 0.00001771
Iteration 213/1000 | Loss: 0.00001771
Iteration 214/1000 | Loss: 0.00001771
Iteration 215/1000 | Loss: 0.00001771
Iteration 216/1000 | Loss: 0.00001771
Iteration 217/1000 | Loss: 0.00001770
Iteration 218/1000 | Loss: 0.00001770
Iteration 219/1000 | Loss: 0.00001770
Iteration 220/1000 | Loss: 0.00001770
Iteration 221/1000 | Loss: 0.00001770
Iteration 222/1000 | Loss: 0.00001770
Iteration 223/1000 | Loss: 0.00001770
Iteration 224/1000 | Loss: 0.00001770
Iteration 225/1000 | Loss: 0.00001770
Iteration 226/1000 | Loss: 0.00001770
Iteration 227/1000 | Loss: 0.00001770
Iteration 228/1000 | Loss: 0.00001770
Iteration 229/1000 | Loss: 0.00001770
Iteration 230/1000 | Loss: 0.00001770
Iteration 231/1000 | Loss: 0.00001770
Iteration 232/1000 | Loss: 0.00001769
Iteration 233/1000 | Loss: 0.00001769
Iteration 234/1000 | Loss: 0.00001769
Iteration 235/1000 | Loss: 0.00001769
Iteration 236/1000 | Loss: 0.00001769
Iteration 237/1000 | Loss: 0.00001769
Iteration 238/1000 | Loss: 0.00001769
Iteration 239/1000 | Loss: 0.00001769
Iteration 240/1000 | Loss: 0.00001769
Iteration 241/1000 | Loss: 0.00001769
Iteration 242/1000 | Loss: 0.00001769
Iteration 243/1000 | Loss: 0.00001769
Iteration 244/1000 | Loss: 0.00001769
Iteration 245/1000 | Loss: 0.00001769
Iteration 246/1000 | Loss: 0.00001769
Iteration 247/1000 | Loss: 0.00001769
Iteration 248/1000 | Loss: 0.00001769
Iteration 249/1000 | Loss: 0.00001769
Iteration 250/1000 | Loss: 0.00001769
Iteration 251/1000 | Loss: 0.00001769
Iteration 252/1000 | Loss: 0.00001769
Iteration 253/1000 | Loss: 0.00001768
Iteration 254/1000 | Loss: 0.00001768
Iteration 255/1000 | Loss: 0.00001768
Iteration 256/1000 | Loss: 0.00001768
Iteration 257/1000 | Loss: 0.00001768
Iteration 258/1000 | Loss: 0.00001768
Iteration 259/1000 | Loss: 0.00001768
Iteration 260/1000 | Loss: 0.00001768
Iteration 261/1000 | Loss: 0.00001768
Iteration 262/1000 | Loss: 0.00001768
Iteration 263/1000 | Loss: 0.00001768
Iteration 264/1000 | Loss: 0.00001768
Iteration 265/1000 | Loss: 0.00001768
Iteration 266/1000 | Loss: 0.00001768
Iteration 267/1000 | Loss: 0.00001768
Iteration 268/1000 | Loss: 0.00001768
Iteration 269/1000 | Loss: 0.00001768
Iteration 270/1000 | Loss: 0.00001768
Iteration 271/1000 | Loss: 0.00001768
Iteration 272/1000 | Loss: 0.00001768
Iteration 273/1000 | Loss: 0.00001768
Iteration 274/1000 | Loss: 0.00001768
Iteration 275/1000 | Loss: 0.00001768
Iteration 276/1000 | Loss: 0.00001768
Iteration 277/1000 | Loss: 0.00001768
Iteration 278/1000 | Loss: 0.00001768
Iteration 279/1000 | Loss: 0.00001768
Iteration 280/1000 | Loss: 0.00001768
Iteration 281/1000 | Loss: 0.00001768
Iteration 282/1000 | Loss: 0.00001768
Iteration 283/1000 | Loss: 0.00001768
Iteration 284/1000 | Loss: 0.00001768
Iteration 285/1000 | Loss: 0.00001768
Iteration 286/1000 | Loss: 0.00001768
Iteration 287/1000 | Loss: 0.00001768
Iteration 288/1000 | Loss: 0.00001768
Iteration 289/1000 | Loss: 0.00001768
Iteration 290/1000 | Loss: 0.00001768
Iteration 291/1000 | Loss: 0.00001768
Iteration 292/1000 | Loss: 0.00001768
Iteration 293/1000 | Loss: 0.00001768
Iteration 294/1000 | Loss: 0.00001768
Iteration 295/1000 | Loss: 0.00001768
Iteration 296/1000 | Loss: 0.00001768
Iteration 297/1000 | Loss: 0.00001768
Iteration 298/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [1.7679514712654054e-05, 1.7679514712654054e-05, 1.7679514712654054e-05, 1.7679514712654054e-05, 1.7679514712654054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7679514712654054e-05

Optimization complete. Final v2v error: 3.457620143890381 mm

Highest mean error: 4.190965175628662 mm for frame 53

Lowest mean error: 2.841999053955078 mm for frame 92

Saving results

Total time: 63.86780071258545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842949
Iteration 2/25 | Loss: 0.00123166
Iteration 3/25 | Loss: 0.00113254
Iteration 4/25 | Loss: 0.00112054
Iteration 5/25 | Loss: 0.00111655
Iteration 6/25 | Loss: 0.00111551
Iteration 7/25 | Loss: 0.00111551
Iteration 8/25 | Loss: 0.00111551
Iteration 9/25 | Loss: 0.00111551
Iteration 10/25 | Loss: 0.00111551
Iteration 11/25 | Loss: 0.00111551
Iteration 12/25 | Loss: 0.00111551
Iteration 13/25 | Loss: 0.00111551
Iteration 14/25 | Loss: 0.00111551
Iteration 15/25 | Loss: 0.00111551
Iteration 16/25 | Loss: 0.00111551
Iteration 17/25 | Loss: 0.00111551
Iteration 18/25 | Loss: 0.00111551
Iteration 19/25 | Loss: 0.00111551
Iteration 20/25 | Loss: 0.00111551
Iteration 21/25 | Loss: 0.00111551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011155145475640893, 0.0011155145475640893, 0.0011155145475640893, 0.0011155145475640893, 0.0011155145475640893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011155145475640893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67565668
Iteration 2/25 | Loss: 0.00089037
Iteration 3/25 | Loss: 0.00089037
Iteration 4/25 | Loss: 0.00089037
Iteration 5/25 | Loss: 0.00089037
Iteration 6/25 | Loss: 0.00089037
Iteration 7/25 | Loss: 0.00089037
Iteration 8/25 | Loss: 0.00089037
Iteration 9/25 | Loss: 0.00089037
Iteration 10/25 | Loss: 0.00089037
Iteration 11/25 | Loss: 0.00089037
Iteration 12/25 | Loss: 0.00089037
Iteration 13/25 | Loss: 0.00089037
Iteration 14/25 | Loss: 0.00089037
Iteration 15/25 | Loss: 0.00089037
Iteration 16/25 | Loss: 0.00089037
Iteration 17/25 | Loss: 0.00089036
Iteration 18/25 | Loss: 0.00089036
Iteration 19/25 | Loss: 0.00089036
Iteration 20/25 | Loss: 0.00089036
Iteration 21/25 | Loss: 0.00089036
Iteration 22/25 | Loss: 0.00089036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008903649868443608, 0.0008903649868443608, 0.0008903649868443608, 0.0008903649868443608, 0.0008903649868443608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008903649868443608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089036
Iteration 2/1000 | Loss: 0.00002008
Iteration 3/1000 | Loss: 0.00001360
Iteration 4/1000 | Loss: 0.00001182
Iteration 5/1000 | Loss: 0.00001108
Iteration 6/1000 | Loss: 0.00001063
Iteration 7/1000 | Loss: 0.00001028
Iteration 8/1000 | Loss: 0.00001015
Iteration 9/1000 | Loss: 0.00001004
Iteration 10/1000 | Loss: 0.00001001
Iteration 11/1000 | Loss: 0.00000988
Iteration 12/1000 | Loss: 0.00000967
Iteration 13/1000 | Loss: 0.00000965
Iteration 14/1000 | Loss: 0.00000964
Iteration 15/1000 | Loss: 0.00000954
Iteration 16/1000 | Loss: 0.00000954
Iteration 17/1000 | Loss: 0.00000951
Iteration 18/1000 | Loss: 0.00000946
Iteration 19/1000 | Loss: 0.00000946
Iteration 20/1000 | Loss: 0.00000944
Iteration 21/1000 | Loss: 0.00000942
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000940
Iteration 24/1000 | Loss: 0.00000939
Iteration 25/1000 | Loss: 0.00000938
Iteration 26/1000 | Loss: 0.00000937
Iteration 27/1000 | Loss: 0.00000937
Iteration 28/1000 | Loss: 0.00000936
Iteration 29/1000 | Loss: 0.00000934
Iteration 30/1000 | Loss: 0.00000934
Iteration 31/1000 | Loss: 0.00000934
Iteration 32/1000 | Loss: 0.00000934
Iteration 33/1000 | Loss: 0.00000934
Iteration 34/1000 | Loss: 0.00000934
Iteration 35/1000 | Loss: 0.00000934
Iteration 36/1000 | Loss: 0.00000934
Iteration 37/1000 | Loss: 0.00000934
Iteration 38/1000 | Loss: 0.00000933
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000929
Iteration 42/1000 | Loss: 0.00000929
Iteration 43/1000 | Loss: 0.00000929
Iteration 44/1000 | Loss: 0.00000929
Iteration 45/1000 | Loss: 0.00000929
Iteration 46/1000 | Loss: 0.00000929
Iteration 47/1000 | Loss: 0.00000928
Iteration 48/1000 | Loss: 0.00000928
Iteration 49/1000 | Loss: 0.00000928
Iteration 50/1000 | Loss: 0.00000927
Iteration 51/1000 | Loss: 0.00000927
Iteration 52/1000 | Loss: 0.00000926
Iteration 53/1000 | Loss: 0.00000926
Iteration 54/1000 | Loss: 0.00000926
Iteration 55/1000 | Loss: 0.00000925
Iteration 56/1000 | Loss: 0.00000925
Iteration 57/1000 | Loss: 0.00000925
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000925
Iteration 60/1000 | Loss: 0.00000924
Iteration 61/1000 | Loss: 0.00000924
Iteration 62/1000 | Loss: 0.00000924
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00000923
Iteration 66/1000 | Loss: 0.00000922
Iteration 67/1000 | Loss: 0.00000922
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000922
Iteration 70/1000 | Loss: 0.00000921
Iteration 71/1000 | Loss: 0.00000921
Iteration 72/1000 | Loss: 0.00000921
Iteration 73/1000 | Loss: 0.00000921
Iteration 74/1000 | Loss: 0.00000921
Iteration 75/1000 | Loss: 0.00000921
Iteration 76/1000 | Loss: 0.00000921
Iteration 77/1000 | Loss: 0.00000921
Iteration 78/1000 | Loss: 0.00000920
Iteration 79/1000 | Loss: 0.00000920
Iteration 80/1000 | Loss: 0.00000920
Iteration 81/1000 | Loss: 0.00000920
Iteration 82/1000 | Loss: 0.00000919
Iteration 83/1000 | Loss: 0.00000919
Iteration 84/1000 | Loss: 0.00000919
Iteration 85/1000 | Loss: 0.00000919
Iteration 86/1000 | Loss: 0.00000919
Iteration 87/1000 | Loss: 0.00000919
Iteration 88/1000 | Loss: 0.00000919
Iteration 89/1000 | Loss: 0.00000919
Iteration 90/1000 | Loss: 0.00000919
Iteration 91/1000 | Loss: 0.00000919
Iteration 92/1000 | Loss: 0.00000918
Iteration 93/1000 | Loss: 0.00000918
Iteration 94/1000 | Loss: 0.00000918
Iteration 95/1000 | Loss: 0.00000918
Iteration 96/1000 | Loss: 0.00000917
Iteration 97/1000 | Loss: 0.00000917
Iteration 98/1000 | Loss: 0.00000917
Iteration 99/1000 | Loss: 0.00000917
Iteration 100/1000 | Loss: 0.00000917
Iteration 101/1000 | Loss: 0.00000916
Iteration 102/1000 | Loss: 0.00000916
Iteration 103/1000 | Loss: 0.00000916
Iteration 104/1000 | Loss: 0.00000916
Iteration 105/1000 | Loss: 0.00000916
Iteration 106/1000 | Loss: 0.00000916
Iteration 107/1000 | Loss: 0.00000916
Iteration 108/1000 | Loss: 0.00000915
Iteration 109/1000 | Loss: 0.00000915
Iteration 110/1000 | Loss: 0.00000915
Iteration 111/1000 | Loss: 0.00000915
Iteration 112/1000 | Loss: 0.00000915
Iteration 113/1000 | Loss: 0.00000915
Iteration 114/1000 | Loss: 0.00000915
Iteration 115/1000 | Loss: 0.00000915
Iteration 116/1000 | Loss: 0.00000914
Iteration 117/1000 | Loss: 0.00000914
Iteration 118/1000 | Loss: 0.00000914
Iteration 119/1000 | Loss: 0.00000914
Iteration 120/1000 | Loss: 0.00000914
Iteration 121/1000 | Loss: 0.00000914
Iteration 122/1000 | Loss: 0.00000914
Iteration 123/1000 | Loss: 0.00000914
Iteration 124/1000 | Loss: 0.00000914
Iteration 125/1000 | Loss: 0.00000914
Iteration 126/1000 | Loss: 0.00000914
Iteration 127/1000 | Loss: 0.00000914
Iteration 128/1000 | Loss: 0.00000913
Iteration 129/1000 | Loss: 0.00000913
Iteration 130/1000 | Loss: 0.00000913
Iteration 131/1000 | Loss: 0.00000913
Iteration 132/1000 | Loss: 0.00000913
Iteration 133/1000 | Loss: 0.00000913
Iteration 134/1000 | Loss: 0.00000913
Iteration 135/1000 | Loss: 0.00000913
Iteration 136/1000 | Loss: 0.00000913
Iteration 137/1000 | Loss: 0.00000912
Iteration 138/1000 | Loss: 0.00000912
Iteration 139/1000 | Loss: 0.00000912
Iteration 140/1000 | Loss: 0.00000912
Iteration 141/1000 | Loss: 0.00000911
Iteration 142/1000 | Loss: 0.00000911
Iteration 143/1000 | Loss: 0.00000911
Iteration 144/1000 | Loss: 0.00000911
Iteration 145/1000 | Loss: 0.00000911
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000910
Iteration 151/1000 | Loss: 0.00000910
Iteration 152/1000 | Loss: 0.00000910
Iteration 153/1000 | Loss: 0.00000909
Iteration 154/1000 | Loss: 0.00000909
Iteration 155/1000 | Loss: 0.00000909
Iteration 156/1000 | Loss: 0.00000909
Iteration 157/1000 | Loss: 0.00000908
Iteration 158/1000 | Loss: 0.00000908
Iteration 159/1000 | Loss: 0.00000908
Iteration 160/1000 | Loss: 0.00000908
Iteration 161/1000 | Loss: 0.00000908
Iteration 162/1000 | Loss: 0.00000908
Iteration 163/1000 | Loss: 0.00000908
Iteration 164/1000 | Loss: 0.00000908
Iteration 165/1000 | Loss: 0.00000907
Iteration 166/1000 | Loss: 0.00000907
Iteration 167/1000 | Loss: 0.00000907
Iteration 168/1000 | Loss: 0.00000907
Iteration 169/1000 | Loss: 0.00000907
Iteration 170/1000 | Loss: 0.00000907
Iteration 171/1000 | Loss: 0.00000907
Iteration 172/1000 | Loss: 0.00000907
Iteration 173/1000 | Loss: 0.00000907
Iteration 174/1000 | Loss: 0.00000907
Iteration 175/1000 | Loss: 0.00000907
Iteration 176/1000 | Loss: 0.00000907
Iteration 177/1000 | Loss: 0.00000907
Iteration 178/1000 | Loss: 0.00000907
Iteration 179/1000 | Loss: 0.00000907
Iteration 180/1000 | Loss: 0.00000907
Iteration 181/1000 | Loss: 0.00000907
Iteration 182/1000 | Loss: 0.00000907
Iteration 183/1000 | Loss: 0.00000907
Iteration 184/1000 | Loss: 0.00000907
Iteration 185/1000 | Loss: 0.00000907
Iteration 186/1000 | Loss: 0.00000907
Iteration 187/1000 | Loss: 0.00000907
Iteration 188/1000 | Loss: 0.00000907
Iteration 189/1000 | Loss: 0.00000907
Iteration 190/1000 | Loss: 0.00000907
Iteration 191/1000 | Loss: 0.00000907
Iteration 192/1000 | Loss: 0.00000907
Iteration 193/1000 | Loss: 0.00000907
Iteration 194/1000 | Loss: 0.00000907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [9.071119166037533e-06, 9.071119166037533e-06, 9.071119166037533e-06, 9.071119166037533e-06, 9.071119166037533e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.071119166037533e-06

Optimization complete. Final v2v error: 2.588371515274048 mm

Highest mean error: 2.996346950531006 mm for frame 92

Lowest mean error: 2.3854329586029053 mm for frame 131

Saving results

Total time: 37.875524282455444
