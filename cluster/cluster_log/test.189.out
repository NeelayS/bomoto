Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=189, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10584-10639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01142361
Iteration 2/25 | Loss: 0.00218081
Iteration 3/25 | Loss: 0.00169956
Iteration 4/25 | Loss: 0.00155383
Iteration 5/25 | Loss: 0.00142174
Iteration 6/25 | Loss: 0.00135724
Iteration 7/25 | Loss: 0.00121875
Iteration 8/25 | Loss: 0.00123453
Iteration 9/25 | Loss: 0.00115886
Iteration 10/25 | Loss: 0.00113115
Iteration 11/25 | Loss: 0.00111074
Iteration 12/25 | Loss: 0.00106388
Iteration 13/25 | Loss: 0.00104365
Iteration 14/25 | Loss: 0.00104233
Iteration 15/25 | Loss: 0.00103325
Iteration 16/25 | Loss: 0.00103335
Iteration 17/25 | Loss: 0.00102181
Iteration 18/25 | Loss: 0.00101595
Iteration 19/25 | Loss: 0.00101451
Iteration 20/25 | Loss: 0.00102365
Iteration 21/25 | Loss: 0.00101876
Iteration 22/25 | Loss: 0.00101851
Iteration 23/25 | Loss: 0.00101694
Iteration 24/25 | Loss: 0.00101488
Iteration 25/25 | Loss: 0.00101465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60920203
Iteration 2/25 | Loss: 0.00117295
Iteration 3/25 | Loss: 0.00117295
Iteration 4/25 | Loss: 0.00117295
Iteration 5/25 | Loss: 0.00117295
Iteration 6/25 | Loss: 0.00117294
Iteration 7/25 | Loss: 0.00117295
Iteration 8/25 | Loss: 0.00117295
Iteration 9/25 | Loss: 0.00117295
Iteration 10/25 | Loss: 0.00117295
Iteration 11/25 | Loss: 0.00117295
Iteration 12/25 | Loss: 0.00117295
Iteration 13/25 | Loss: 0.00117295
Iteration 14/25 | Loss: 0.00117295
Iteration 15/25 | Loss: 0.00117295
Iteration 16/25 | Loss: 0.00117295
Iteration 17/25 | Loss: 0.00117295
Iteration 18/25 | Loss: 0.00117295
Iteration 19/25 | Loss: 0.00117295
Iteration 20/25 | Loss: 0.00117295
Iteration 21/25 | Loss: 0.00117295
Iteration 22/25 | Loss: 0.00117295
Iteration 23/25 | Loss: 0.00117295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011729452526196837, 0.0011729452526196837, 0.0011729452526196837, 0.0011729452526196837, 0.0011729452526196837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011729452526196837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117295
Iteration 2/1000 | Loss: 0.00020332
Iteration 3/1000 | Loss: 0.00027624
Iteration 4/1000 | Loss: 0.00027123
Iteration 5/1000 | Loss: 0.00012275
Iteration 6/1000 | Loss: 0.00017837
Iteration 7/1000 | Loss: 0.00010341
Iteration 8/1000 | Loss: 0.00016520
Iteration 9/1000 | Loss: 0.00008556
Iteration 10/1000 | Loss: 0.00008730
Iteration 11/1000 | Loss: 0.00007634
Iteration 12/1000 | Loss: 0.00053964
Iteration 13/1000 | Loss: 0.00126134
Iteration 14/1000 | Loss: 0.00062500
Iteration 15/1000 | Loss: 0.00032569
Iteration 16/1000 | Loss: 0.00008214
Iteration 17/1000 | Loss: 0.00031496
Iteration 18/1000 | Loss: 0.00005299
Iteration 19/1000 | Loss: 0.00004066
Iteration 20/1000 | Loss: 0.00038249
Iteration 21/1000 | Loss: 0.00005270
Iteration 22/1000 | Loss: 0.00012325
Iteration 23/1000 | Loss: 0.00003552
Iteration 24/1000 | Loss: 0.00003208
Iteration 25/1000 | Loss: 0.00002883
Iteration 26/1000 | Loss: 0.00002672
Iteration 27/1000 | Loss: 0.00002512
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002266
Iteration 30/1000 | Loss: 0.00002216
Iteration 31/1000 | Loss: 0.00002172
Iteration 32/1000 | Loss: 0.00002149
Iteration 33/1000 | Loss: 0.00002137
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002122
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002120
Iteration 38/1000 | Loss: 0.00002120
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002120
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002119
Iteration 43/1000 | Loss: 0.00002118
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002117
Iteration 46/1000 | Loss: 0.00002117
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002117
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00002117
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002116
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002116
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002115
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002115
Iteration 66/1000 | Loss: 0.00002115
Iteration 67/1000 | Loss: 0.00002115
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002114
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002114
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002114
Iteration 76/1000 | Loss: 0.00002114
Iteration 77/1000 | Loss: 0.00002114
Iteration 78/1000 | Loss: 0.00002114
Iteration 79/1000 | Loss: 0.00002114
Iteration 80/1000 | Loss: 0.00002114
Iteration 81/1000 | Loss: 0.00002114
Iteration 82/1000 | Loss: 0.00002114
Iteration 83/1000 | Loss: 0.00002114
Iteration 84/1000 | Loss: 0.00002114
Iteration 85/1000 | Loss: 0.00002114
Iteration 86/1000 | Loss: 0.00002114
Iteration 87/1000 | Loss: 0.00002114
Iteration 88/1000 | Loss: 0.00002114
Iteration 89/1000 | Loss: 0.00002114
Iteration 90/1000 | Loss: 0.00002114
Iteration 91/1000 | Loss: 0.00002114
Iteration 92/1000 | Loss: 0.00002114
Iteration 93/1000 | Loss: 0.00002114
Iteration 94/1000 | Loss: 0.00002114
Iteration 95/1000 | Loss: 0.00002114
Iteration 96/1000 | Loss: 0.00002114
Iteration 97/1000 | Loss: 0.00002114
Iteration 98/1000 | Loss: 0.00002114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.114310882461723e-05, 2.114310882461723e-05, 2.114310882461723e-05, 2.114310882461723e-05, 2.114310882461723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.114310882461723e-05

Optimization complete. Final v2v error: 3.915591239929199 mm

Highest mean error: 4.529352188110352 mm for frame 64

Lowest mean error: 3.590998649597168 mm for frame 2

Saving results

Total time: 97.67610359191895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888811
Iteration 2/25 | Loss: 0.00114509
Iteration 3/25 | Loss: 0.00103361
Iteration 4/25 | Loss: 0.00097752
Iteration 5/25 | Loss: 0.00096887
Iteration 6/25 | Loss: 0.00096823
Iteration 7/25 | Loss: 0.00096823
Iteration 8/25 | Loss: 0.00096823
Iteration 9/25 | Loss: 0.00096823
Iteration 10/25 | Loss: 0.00096823
Iteration 11/25 | Loss: 0.00096823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009682329837232828, 0.0009682329837232828, 0.0009682329837232828, 0.0009682329837232828, 0.0009682329837232828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009682329837232828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.61428404
Iteration 2/25 | Loss: 0.00040745
Iteration 3/25 | Loss: 0.00040739
Iteration 4/25 | Loss: 0.00040739
Iteration 5/25 | Loss: 0.00040739
Iteration 6/25 | Loss: 0.00040739
Iteration 7/25 | Loss: 0.00040739
Iteration 8/25 | Loss: 0.00040739
Iteration 9/25 | Loss: 0.00040738
Iteration 10/25 | Loss: 0.00040738
Iteration 11/25 | Loss: 0.00040738
Iteration 12/25 | Loss: 0.00040738
Iteration 13/25 | Loss: 0.00040738
Iteration 14/25 | Loss: 0.00040738
Iteration 15/25 | Loss: 0.00040738
Iteration 16/25 | Loss: 0.00040738
Iteration 17/25 | Loss: 0.00040738
Iteration 18/25 | Loss: 0.00040738
Iteration 19/25 | Loss: 0.00040738
Iteration 20/25 | Loss: 0.00040738
Iteration 21/25 | Loss: 0.00040738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00040738447569310665, 0.00040738447569310665, 0.00040738447569310665, 0.00040738447569310665, 0.00040738447569310665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040738447569310665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040738
Iteration 2/1000 | Loss: 0.00004756
Iteration 3/1000 | Loss: 0.00003266
Iteration 4/1000 | Loss: 0.00002779
Iteration 5/1000 | Loss: 0.00002661
Iteration 6/1000 | Loss: 0.00002595
Iteration 7/1000 | Loss: 0.00002526
Iteration 8/1000 | Loss: 0.00002462
Iteration 9/1000 | Loss: 0.00002416
Iteration 10/1000 | Loss: 0.00002403
Iteration 11/1000 | Loss: 0.00002399
Iteration 12/1000 | Loss: 0.00002398
Iteration 13/1000 | Loss: 0.00002397
Iteration 14/1000 | Loss: 0.00002396
Iteration 15/1000 | Loss: 0.00002394
Iteration 16/1000 | Loss: 0.00002394
Iteration 17/1000 | Loss: 0.00002393
Iteration 18/1000 | Loss: 0.00002392
Iteration 19/1000 | Loss: 0.00002387
Iteration 20/1000 | Loss: 0.00002378
Iteration 21/1000 | Loss: 0.00002377
Iteration 22/1000 | Loss: 0.00002373
Iteration 23/1000 | Loss: 0.00002372
Iteration 24/1000 | Loss: 0.00002372
Iteration 25/1000 | Loss: 0.00002372
Iteration 26/1000 | Loss: 0.00002370
Iteration 27/1000 | Loss: 0.00002369
Iteration 28/1000 | Loss: 0.00002369
Iteration 29/1000 | Loss: 0.00002368
Iteration 30/1000 | Loss: 0.00002368
Iteration 31/1000 | Loss: 0.00002366
Iteration 32/1000 | Loss: 0.00002366
Iteration 33/1000 | Loss: 0.00002365
Iteration 34/1000 | Loss: 0.00002365
Iteration 35/1000 | Loss: 0.00002365
Iteration 36/1000 | Loss: 0.00002365
Iteration 37/1000 | Loss: 0.00002364
Iteration 38/1000 | Loss: 0.00002364
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002364
Iteration 42/1000 | Loss: 0.00002364
Iteration 43/1000 | Loss: 0.00002364
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002363
Iteration 46/1000 | Loss: 0.00002363
Iteration 47/1000 | Loss: 0.00002363
Iteration 48/1000 | Loss: 0.00002363
Iteration 49/1000 | Loss: 0.00002363
Iteration 50/1000 | Loss: 0.00002363
Iteration 51/1000 | Loss: 0.00002363
Iteration 52/1000 | Loss: 0.00002363
Iteration 53/1000 | Loss: 0.00002362
Iteration 54/1000 | Loss: 0.00002362
Iteration 55/1000 | Loss: 0.00002362
Iteration 56/1000 | Loss: 0.00002362
Iteration 57/1000 | Loss: 0.00002362
Iteration 58/1000 | Loss: 0.00002362
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00002361
Iteration 61/1000 | Loss: 0.00002361
Iteration 62/1000 | Loss: 0.00002361
Iteration 63/1000 | Loss: 0.00002361
Iteration 64/1000 | Loss: 0.00002361
Iteration 65/1000 | Loss: 0.00002361
Iteration 66/1000 | Loss: 0.00002361
Iteration 67/1000 | Loss: 0.00002361
Iteration 68/1000 | Loss: 0.00002361
Iteration 69/1000 | Loss: 0.00002361
Iteration 70/1000 | Loss: 0.00002361
Iteration 71/1000 | Loss: 0.00002361
Iteration 72/1000 | Loss: 0.00002361
Iteration 73/1000 | Loss: 0.00002361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.3609727577422746e-05, 2.3609727577422746e-05, 2.3609727577422746e-05, 2.3609727577422746e-05, 2.3609727577422746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3609727577422746e-05

Optimization complete. Final v2v error: 4.138323783874512 mm

Highest mean error: 4.551387310028076 mm for frame 26

Lowest mean error: 3.8267757892608643 mm for frame 120

Saving results

Total time: 32.43742775917053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073225
Iteration 2/25 | Loss: 0.00244502
Iteration 3/25 | Loss: 0.00173716
Iteration 4/25 | Loss: 0.00153751
Iteration 5/25 | Loss: 0.00138387
Iteration 6/25 | Loss: 0.00135008
Iteration 7/25 | Loss: 0.00151669
Iteration 8/25 | Loss: 0.00144637
Iteration 9/25 | Loss: 0.00124906
Iteration 10/25 | Loss: 0.00111901
Iteration 11/25 | Loss: 0.00106428
Iteration 12/25 | Loss: 0.00101088
Iteration 13/25 | Loss: 0.00099793
Iteration 14/25 | Loss: 0.00098693
Iteration 15/25 | Loss: 0.00097872
Iteration 16/25 | Loss: 0.00096839
Iteration 17/25 | Loss: 0.00096051
Iteration 18/25 | Loss: 0.00095696
Iteration 19/25 | Loss: 0.00095692
Iteration 20/25 | Loss: 0.00095719
Iteration 21/25 | Loss: 0.00095517
Iteration 22/25 | Loss: 0.00095196
Iteration 23/25 | Loss: 0.00095424
Iteration 24/25 | Loss: 0.00095207
Iteration 25/25 | Loss: 0.00094906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45583630
Iteration 2/25 | Loss: 0.00154343
Iteration 3/25 | Loss: 0.00070786
Iteration 4/25 | Loss: 0.00070786
Iteration 5/25 | Loss: 0.00070786
Iteration 6/25 | Loss: 0.00070786
Iteration 7/25 | Loss: 0.00070786
Iteration 8/25 | Loss: 0.00070786
Iteration 9/25 | Loss: 0.00070786
Iteration 10/25 | Loss: 0.00070786
Iteration 11/25 | Loss: 0.00070786
Iteration 12/25 | Loss: 0.00070786
Iteration 13/25 | Loss: 0.00070786
Iteration 14/25 | Loss: 0.00070786
Iteration 15/25 | Loss: 0.00070786
Iteration 16/25 | Loss: 0.00070786
Iteration 17/25 | Loss: 0.00070786
Iteration 18/25 | Loss: 0.00070786
Iteration 19/25 | Loss: 0.00070786
Iteration 20/25 | Loss: 0.00070786
Iteration 21/25 | Loss: 0.00070786
Iteration 22/25 | Loss: 0.00070786
Iteration 23/25 | Loss: 0.00070786
Iteration 24/25 | Loss: 0.00070786
Iteration 25/25 | Loss: 0.00070786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070786
Iteration 2/1000 | Loss: 0.00087840
Iteration 3/1000 | Loss: 0.00009062
Iteration 4/1000 | Loss: 0.00026702
Iteration 5/1000 | Loss: 0.00025198
Iteration 6/1000 | Loss: 0.00014843
Iteration 7/1000 | Loss: 0.00012483
Iteration 8/1000 | Loss: 0.00008450
Iteration 9/1000 | Loss: 0.00012380
Iteration 10/1000 | Loss: 0.00016695
Iteration 11/1000 | Loss: 0.00143046
Iteration 12/1000 | Loss: 0.00012879
Iteration 13/1000 | Loss: 0.00015210
Iteration 14/1000 | Loss: 0.00005430
Iteration 15/1000 | Loss: 0.00004706
Iteration 16/1000 | Loss: 0.00035471
Iteration 17/1000 | Loss: 0.00030795
Iteration 18/1000 | Loss: 0.00008222
Iteration 19/1000 | Loss: 0.00012423
Iteration 20/1000 | Loss: 0.00008713
Iteration 21/1000 | Loss: 0.00004616
Iteration 22/1000 | Loss: 0.00004411
Iteration 23/1000 | Loss: 0.00008988
Iteration 24/1000 | Loss: 0.00004154
Iteration 25/1000 | Loss: 0.00005352
Iteration 26/1000 | Loss: 0.00003985
Iteration 27/1000 | Loss: 0.00003905
Iteration 28/1000 | Loss: 0.00003811
Iteration 29/1000 | Loss: 0.00007363
Iteration 30/1000 | Loss: 0.00020190
Iteration 31/1000 | Loss: 0.00006790
Iteration 32/1000 | Loss: 0.00004213
Iteration 33/1000 | Loss: 0.00026272
Iteration 34/1000 | Loss: 0.00011206
Iteration 35/1000 | Loss: 0.00003896
Iteration 36/1000 | Loss: 0.00005796
Iteration 37/1000 | Loss: 0.00004623
Iteration 38/1000 | Loss: 0.00003757
Iteration 39/1000 | Loss: 0.00003604
Iteration 40/1000 | Loss: 0.00003502
Iteration 41/1000 | Loss: 0.00009166
Iteration 42/1000 | Loss: 0.00004047
Iteration 43/1000 | Loss: 0.00003459
Iteration 44/1000 | Loss: 0.00005755
Iteration 45/1000 | Loss: 0.00003412
Iteration 46/1000 | Loss: 0.00003398
Iteration 47/1000 | Loss: 0.00003396
Iteration 48/1000 | Loss: 0.00003386
Iteration 49/1000 | Loss: 0.00003385
Iteration 50/1000 | Loss: 0.00003385
Iteration 51/1000 | Loss: 0.00006886
Iteration 52/1000 | Loss: 0.00003384
Iteration 53/1000 | Loss: 0.00003382
Iteration 54/1000 | Loss: 0.00003378
Iteration 55/1000 | Loss: 0.00003373
Iteration 56/1000 | Loss: 0.00003373
Iteration 57/1000 | Loss: 0.00003371
Iteration 58/1000 | Loss: 0.00003370
Iteration 59/1000 | Loss: 0.00003369
Iteration 60/1000 | Loss: 0.00003369
Iteration 61/1000 | Loss: 0.00003369
Iteration 62/1000 | Loss: 0.00003368
Iteration 63/1000 | Loss: 0.00003368
Iteration 64/1000 | Loss: 0.00003368
Iteration 65/1000 | Loss: 0.00003367
Iteration 66/1000 | Loss: 0.00003367
Iteration 67/1000 | Loss: 0.00003367
Iteration 68/1000 | Loss: 0.00003361
Iteration 69/1000 | Loss: 0.00003361
Iteration 70/1000 | Loss: 0.00003361
Iteration 71/1000 | Loss: 0.00003361
Iteration 72/1000 | Loss: 0.00003361
Iteration 73/1000 | Loss: 0.00003361
Iteration 74/1000 | Loss: 0.00003361
Iteration 75/1000 | Loss: 0.00003361
Iteration 76/1000 | Loss: 0.00003360
Iteration 77/1000 | Loss: 0.00003360
Iteration 78/1000 | Loss: 0.00003360
Iteration 79/1000 | Loss: 0.00003360
Iteration 80/1000 | Loss: 0.00003360
Iteration 81/1000 | Loss: 0.00003360
Iteration 82/1000 | Loss: 0.00003360
Iteration 83/1000 | Loss: 0.00003360
Iteration 84/1000 | Loss: 0.00003360
Iteration 85/1000 | Loss: 0.00003360
Iteration 86/1000 | Loss: 0.00003359
Iteration 87/1000 | Loss: 0.00003359
Iteration 88/1000 | Loss: 0.00003359
Iteration 89/1000 | Loss: 0.00003358
Iteration 90/1000 | Loss: 0.00003358
Iteration 91/1000 | Loss: 0.00003358
Iteration 92/1000 | Loss: 0.00003357
Iteration 93/1000 | Loss: 0.00006414
Iteration 94/1000 | Loss: 0.00003363
Iteration 95/1000 | Loss: 0.00003347
Iteration 96/1000 | Loss: 0.00003347
Iteration 97/1000 | Loss: 0.00003347
Iteration 98/1000 | Loss: 0.00003347
Iteration 99/1000 | Loss: 0.00003346
Iteration 100/1000 | Loss: 0.00003346
Iteration 101/1000 | Loss: 0.00003346
Iteration 102/1000 | Loss: 0.00003346
Iteration 103/1000 | Loss: 0.00003346
Iteration 104/1000 | Loss: 0.00003346
Iteration 105/1000 | Loss: 0.00003345
Iteration 106/1000 | Loss: 0.00003345
Iteration 107/1000 | Loss: 0.00003345
Iteration 108/1000 | Loss: 0.00003344
Iteration 109/1000 | Loss: 0.00003344
Iteration 110/1000 | Loss: 0.00003344
Iteration 111/1000 | Loss: 0.00003343
Iteration 112/1000 | Loss: 0.00003343
Iteration 113/1000 | Loss: 0.00003343
Iteration 114/1000 | Loss: 0.00003343
Iteration 115/1000 | Loss: 0.00003343
Iteration 116/1000 | Loss: 0.00003343
Iteration 117/1000 | Loss: 0.00003343
Iteration 118/1000 | Loss: 0.00003343
Iteration 119/1000 | Loss: 0.00003343
Iteration 120/1000 | Loss: 0.00003343
Iteration 121/1000 | Loss: 0.00003342
Iteration 122/1000 | Loss: 0.00003342
Iteration 123/1000 | Loss: 0.00003342
Iteration 124/1000 | Loss: 0.00003342
Iteration 125/1000 | Loss: 0.00003342
Iteration 126/1000 | Loss: 0.00003342
Iteration 127/1000 | Loss: 0.00003342
Iteration 128/1000 | Loss: 0.00003341
Iteration 129/1000 | Loss: 0.00003341
Iteration 130/1000 | Loss: 0.00003341
Iteration 131/1000 | Loss: 0.00003341
Iteration 132/1000 | Loss: 0.00003340
Iteration 133/1000 | Loss: 0.00003340
Iteration 134/1000 | Loss: 0.00003340
Iteration 135/1000 | Loss: 0.00003340
Iteration 136/1000 | Loss: 0.00003340
Iteration 137/1000 | Loss: 0.00003339
Iteration 138/1000 | Loss: 0.00003339
Iteration 139/1000 | Loss: 0.00003339
Iteration 140/1000 | Loss: 0.00003339
Iteration 141/1000 | Loss: 0.00018465
Iteration 142/1000 | Loss: 0.00018801
Iteration 143/1000 | Loss: 0.00004385
Iteration 144/1000 | Loss: 0.00004022
Iteration 145/1000 | Loss: 0.00004843
Iteration 146/1000 | Loss: 0.00003963
Iteration 147/1000 | Loss: 0.00003657
Iteration 148/1000 | Loss: 0.00003601
Iteration 149/1000 | Loss: 0.00003644
Iteration 150/1000 | Loss: 0.00003518
Iteration 151/1000 | Loss: 0.00003487
Iteration 152/1000 | Loss: 0.00003455
Iteration 153/1000 | Loss: 0.00003453
Iteration 154/1000 | Loss: 0.00003452
Iteration 155/1000 | Loss: 0.00003450
Iteration 156/1000 | Loss: 0.00003449
Iteration 157/1000 | Loss: 0.00003439
Iteration 158/1000 | Loss: 0.00003437
Iteration 159/1000 | Loss: 0.00006335
Iteration 160/1000 | Loss: 0.00003427
Iteration 161/1000 | Loss: 0.00003410
Iteration 162/1000 | Loss: 0.00003389
Iteration 163/1000 | Loss: 0.00003375
Iteration 164/1000 | Loss: 0.00003372
Iteration 165/1000 | Loss: 0.00003371
Iteration 166/1000 | Loss: 0.00003358
Iteration 167/1000 | Loss: 0.00003351
Iteration 168/1000 | Loss: 0.00003351
Iteration 169/1000 | Loss: 0.00003346
Iteration 170/1000 | Loss: 0.00003345
Iteration 171/1000 | Loss: 0.00003345
Iteration 172/1000 | Loss: 0.00003344
Iteration 173/1000 | Loss: 0.00003344
Iteration 174/1000 | Loss: 0.00003343
Iteration 175/1000 | Loss: 0.00003343
Iteration 176/1000 | Loss: 0.00003343
Iteration 177/1000 | Loss: 0.00003342
Iteration 178/1000 | Loss: 0.00003341
Iteration 179/1000 | Loss: 0.00003341
Iteration 180/1000 | Loss: 0.00003340
Iteration 181/1000 | Loss: 0.00003340
Iteration 182/1000 | Loss: 0.00003339
Iteration 183/1000 | Loss: 0.00003339
Iteration 184/1000 | Loss: 0.00003339
Iteration 185/1000 | Loss: 0.00003339
Iteration 186/1000 | Loss: 0.00003339
Iteration 187/1000 | Loss: 0.00003338
Iteration 188/1000 | Loss: 0.00003338
Iteration 189/1000 | Loss: 0.00003338
Iteration 190/1000 | Loss: 0.00003338
Iteration 191/1000 | Loss: 0.00003338
Iteration 192/1000 | Loss: 0.00003338
Iteration 193/1000 | Loss: 0.00003338
Iteration 194/1000 | Loss: 0.00003338
Iteration 195/1000 | Loss: 0.00003338
Iteration 196/1000 | Loss: 0.00003337
Iteration 197/1000 | Loss: 0.00003337
Iteration 198/1000 | Loss: 0.00003337
Iteration 199/1000 | Loss: 0.00003337
Iteration 200/1000 | Loss: 0.00003337
Iteration 201/1000 | Loss: 0.00003337
Iteration 202/1000 | Loss: 0.00003336
Iteration 203/1000 | Loss: 0.00003336
Iteration 204/1000 | Loss: 0.00019116
Iteration 205/1000 | Loss: 0.00014918
Iteration 206/1000 | Loss: 0.00007617
Iteration 207/1000 | Loss: 0.00003868
Iteration 208/1000 | Loss: 0.00003616
Iteration 209/1000 | Loss: 0.00005989
Iteration 210/1000 | Loss: 0.00003517
Iteration 211/1000 | Loss: 0.00003464
Iteration 212/1000 | Loss: 0.00003406
Iteration 213/1000 | Loss: 0.00009386
Iteration 214/1000 | Loss: 0.00003709
Iteration 215/1000 | Loss: 0.00003583
Iteration 216/1000 | Loss: 0.00003311
Iteration 217/1000 | Loss: 0.00005192
Iteration 218/1000 | Loss: 0.00003283
Iteration 219/1000 | Loss: 0.00003278
Iteration 220/1000 | Loss: 0.00003277
Iteration 221/1000 | Loss: 0.00003277
Iteration 222/1000 | Loss: 0.00003276
Iteration 223/1000 | Loss: 0.00003264
Iteration 224/1000 | Loss: 0.00003264
Iteration 225/1000 | Loss: 0.00003264
Iteration 226/1000 | Loss: 0.00003264
Iteration 227/1000 | Loss: 0.00003264
Iteration 228/1000 | Loss: 0.00003264
Iteration 229/1000 | Loss: 0.00003264
Iteration 230/1000 | Loss: 0.00003264
Iteration 231/1000 | Loss: 0.00003264
Iteration 232/1000 | Loss: 0.00003264
Iteration 233/1000 | Loss: 0.00003264
Iteration 234/1000 | Loss: 0.00003263
Iteration 235/1000 | Loss: 0.00003263
Iteration 236/1000 | Loss: 0.00003262
Iteration 237/1000 | Loss: 0.00003261
Iteration 238/1000 | Loss: 0.00003261
Iteration 239/1000 | Loss: 0.00003261
Iteration 240/1000 | Loss: 0.00003260
Iteration 241/1000 | Loss: 0.00003260
Iteration 242/1000 | Loss: 0.00003260
Iteration 243/1000 | Loss: 0.00003260
Iteration 244/1000 | Loss: 0.00003260
Iteration 245/1000 | Loss: 0.00003260
Iteration 246/1000 | Loss: 0.00003260
Iteration 247/1000 | Loss: 0.00003260
Iteration 248/1000 | Loss: 0.00003259
Iteration 249/1000 | Loss: 0.00003259
Iteration 250/1000 | Loss: 0.00003259
Iteration 251/1000 | Loss: 0.00003259
Iteration 252/1000 | Loss: 0.00003258
Iteration 253/1000 | Loss: 0.00003258
Iteration 254/1000 | Loss: 0.00003258
Iteration 255/1000 | Loss: 0.00003258
Iteration 256/1000 | Loss: 0.00003258
Iteration 257/1000 | Loss: 0.00003258
Iteration 258/1000 | Loss: 0.00003258
Iteration 259/1000 | Loss: 0.00003258
Iteration 260/1000 | Loss: 0.00003258
Iteration 261/1000 | Loss: 0.00003258
Iteration 262/1000 | Loss: 0.00003258
Iteration 263/1000 | Loss: 0.00003258
Iteration 264/1000 | Loss: 0.00003258
Iteration 265/1000 | Loss: 0.00003258
Iteration 266/1000 | Loss: 0.00003258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [3.258019933127798e-05, 3.258019933127798e-05, 3.258019933127798e-05, 3.258019933127798e-05, 3.258019933127798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.258019933127798e-05

Optimization complete. Final v2v error: 4.179856777191162 mm

Highest mean error: 20.34503173828125 mm for frame 118

Lowest mean error: 3.605949640274048 mm for frame 238

Saving results

Total time: 205.43316054344177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842244
Iteration 2/25 | Loss: 0.00176945
Iteration 3/25 | Loss: 0.00114211
Iteration 4/25 | Loss: 0.00102584
Iteration 5/25 | Loss: 0.00096242
Iteration 6/25 | Loss: 0.00094351
Iteration 7/25 | Loss: 0.00094960
Iteration 8/25 | Loss: 0.00096982
Iteration 9/25 | Loss: 0.00093049
Iteration 10/25 | Loss: 0.00093586
Iteration 11/25 | Loss: 0.00096402
Iteration 12/25 | Loss: 0.00093156
Iteration 13/25 | Loss: 0.00094117
Iteration 14/25 | Loss: 0.00093374
Iteration 15/25 | Loss: 0.00093382
Iteration 16/25 | Loss: 0.00092879
Iteration 17/25 | Loss: 0.00093358
Iteration 18/25 | Loss: 0.00092976
Iteration 19/25 | Loss: 0.00093480
Iteration 20/25 | Loss: 0.00092523
Iteration 21/25 | Loss: 0.00092586
Iteration 22/25 | Loss: 0.00092232
Iteration 23/25 | Loss: 0.00092197
Iteration 24/25 | Loss: 0.00092312
Iteration 25/25 | Loss: 0.00092307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10564756
Iteration 2/25 | Loss: 0.00047857
Iteration 3/25 | Loss: 0.00032101
Iteration 4/25 | Loss: 0.00032101
Iteration 5/25 | Loss: 0.00032101
Iteration 6/25 | Loss: 0.00032101
Iteration 7/25 | Loss: 0.00032101
Iteration 8/25 | Loss: 0.00032101
Iteration 9/25 | Loss: 0.00032101
Iteration 10/25 | Loss: 0.00032101
Iteration 11/25 | Loss: 0.00032101
Iteration 12/25 | Loss: 0.00032101
Iteration 13/25 | Loss: 0.00032101
Iteration 14/25 | Loss: 0.00032101
Iteration 15/25 | Loss: 0.00032101
Iteration 16/25 | Loss: 0.00032101
Iteration 17/25 | Loss: 0.00032101
Iteration 18/25 | Loss: 0.00032101
Iteration 19/25 | Loss: 0.00032101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00032101207762025297, 0.00032101207762025297, 0.00032101207762025297, 0.00032101207762025297, 0.00032101207762025297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032101207762025297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032101
Iteration 2/1000 | Loss: 0.00010254
Iteration 3/1000 | Loss: 0.00007474
Iteration 4/1000 | Loss: 0.00002778
Iteration 5/1000 | Loss: 0.00002416
Iteration 6/1000 | Loss: 0.00016925
Iteration 7/1000 | Loss: 0.00002343
Iteration 8/1000 | Loss: 0.00002194
Iteration 9/1000 | Loss: 0.00002612
Iteration 10/1000 | Loss: 0.00002262
Iteration 11/1000 | Loss: 0.00003274
Iteration 12/1000 | Loss: 0.00003270
Iteration 13/1000 | Loss: 0.00004180
Iteration 14/1000 | Loss: 0.00002315
Iteration 15/1000 | Loss: 0.00002517
Iteration 16/1000 | Loss: 0.00002147
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002056
Iteration 19/1000 | Loss: 0.00002056
Iteration 20/1000 | Loss: 0.00002056
Iteration 21/1000 | Loss: 0.00002056
Iteration 22/1000 | Loss: 0.00002056
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002056
Iteration 26/1000 | Loss: 0.00002056
Iteration 27/1000 | Loss: 0.00002054
Iteration 28/1000 | Loss: 0.00002054
Iteration 29/1000 | Loss: 0.00002053
Iteration 30/1000 | Loss: 0.00002052
Iteration 31/1000 | Loss: 0.00002051
Iteration 32/1000 | Loss: 0.00002051
Iteration 33/1000 | Loss: 0.00002428
Iteration 34/1000 | Loss: 0.00002419
Iteration 35/1000 | Loss: 0.00002339
Iteration 36/1000 | Loss: 0.00002047
Iteration 37/1000 | Loss: 0.00002047
Iteration 38/1000 | Loss: 0.00002047
Iteration 39/1000 | Loss: 0.00002047
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002047
Iteration 42/1000 | Loss: 0.00002047
Iteration 43/1000 | Loss: 0.00002047
Iteration 44/1000 | Loss: 0.00002047
Iteration 45/1000 | Loss: 0.00002047
Iteration 46/1000 | Loss: 0.00002047
Iteration 47/1000 | Loss: 0.00002047
Iteration 48/1000 | Loss: 0.00002047
Iteration 49/1000 | Loss: 0.00002047
Iteration 50/1000 | Loss: 0.00002046
Iteration 51/1000 | Loss: 0.00002046
Iteration 52/1000 | Loss: 0.00002046
Iteration 53/1000 | Loss: 0.00002046
Iteration 54/1000 | Loss: 0.00002046
Iteration 55/1000 | Loss: 0.00002045
Iteration 56/1000 | Loss: 0.00002045
Iteration 57/1000 | Loss: 0.00002045
Iteration 58/1000 | Loss: 0.00002045
Iteration 59/1000 | Loss: 0.00002045
Iteration 60/1000 | Loss: 0.00002044
Iteration 61/1000 | Loss: 0.00002044
Iteration 62/1000 | Loss: 0.00002044
Iteration 63/1000 | Loss: 0.00002044
Iteration 64/1000 | Loss: 0.00002044
Iteration 65/1000 | Loss: 0.00002044
Iteration 66/1000 | Loss: 0.00002044
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002044
Iteration 69/1000 | Loss: 0.00002043
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002043
Iteration 72/1000 | Loss: 0.00002043
Iteration 73/1000 | Loss: 0.00002042
Iteration 74/1000 | Loss: 0.00002042
Iteration 75/1000 | Loss: 0.00002042
Iteration 76/1000 | Loss: 0.00002042
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002042
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002042
Iteration 83/1000 | Loss: 0.00002042
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002041
Iteration 86/1000 | Loss: 0.00002041
Iteration 87/1000 | Loss: 0.00002041
Iteration 88/1000 | Loss: 0.00002041
Iteration 89/1000 | Loss: 0.00002041
Iteration 90/1000 | Loss: 0.00002041
Iteration 91/1000 | Loss: 0.00002041
Iteration 92/1000 | Loss: 0.00002041
Iteration 93/1000 | Loss: 0.00002041
Iteration 94/1000 | Loss: 0.00002041
Iteration 95/1000 | Loss: 0.00002040
Iteration 96/1000 | Loss: 0.00002040
Iteration 97/1000 | Loss: 0.00002039
Iteration 98/1000 | Loss: 0.00002039
Iteration 99/1000 | Loss: 0.00006479
Iteration 100/1000 | Loss: 0.00003924
Iteration 101/1000 | Loss: 0.00002072
Iteration 102/1000 | Loss: 0.00004105
Iteration 103/1000 | Loss: 0.00005305
Iteration 104/1000 | Loss: 0.00004116
Iteration 105/1000 | Loss: 0.00008216
Iteration 106/1000 | Loss: 0.00004629
Iteration 107/1000 | Loss: 0.00002060
Iteration 108/1000 | Loss: 0.00002301
Iteration 109/1000 | Loss: 0.00002196
Iteration 110/1000 | Loss: 0.00002037
Iteration 111/1000 | Loss: 0.00002036
Iteration 112/1000 | Loss: 0.00002036
Iteration 113/1000 | Loss: 0.00002036
Iteration 114/1000 | Loss: 0.00002036
Iteration 115/1000 | Loss: 0.00002036
Iteration 116/1000 | Loss: 0.00002036
Iteration 117/1000 | Loss: 0.00002035
Iteration 118/1000 | Loss: 0.00002035
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002035
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002035
Iteration 132/1000 | Loss: 0.00002035
Iteration 133/1000 | Loss: 0.00002035
Iteration 134/1000 | Loss: 0.00002035
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002034
Iteration 140/1000 | Loss: 0.00002034
Iteration 141/1000 | Loss: 0.00002034
Iteration 142/1000 | Loss: 0.00002034
Iteration 143/1000 | Loss: 0.00002034
Iteration 144/1000 | Loss: 0.00002034
Iteration 145/1000 | Loss: 0.00002034
Iteration 146/1000 | Loss: 0.00002034
Iteration 147/1000 | Loss: 0.00002034
Iteration 148/1000 | Loss: 0.00002034
Iteration 149/1000 | Loss: 0.00002034
Iteration 150/1000 | Loss: 0.00002034
Iteration 151/1000 | Loss: 0.00002034
Iteration 152/1000 | Loss: 0.00002034
Iteration 153/1000 | Loss: 0.00002034
Iteration 154/1000 | Loss: 0.00002034
Iteration 155/1000 | Loss: 0.00002034
Iteration 156/1000 | Loss: 0.00002034
Iteration 157/1000 | Loss: 0.00002034
Iteration 158/1000 | Loss: 0.00002034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.0341589333838783e-05, 2.0341589333838783e-05, 2.0341589333838783e-05, 2.0341589333838783e-05, 2.0341589333838783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0341589333838783e-05

Optimization complete. Final v2v error: 3.8690648078918457 mm

Highest mean error: 4.318227291107178 mm for frame 182

Lowest mean error: 3.6060988903045654 mm for frame 32

Saving results

Total time: 95.67042350769043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419059
Iteration 2/25 | Loss: 0.00103668
Iteration 3/25 | Loss: 0.00091162
Iteration 4/25 | Loss: 0.00089547
Iteration 5/25 | Loss: 0.00089081
Iteration 6/25 | Loss: 0.00088925
Iteration 7/25 | Loss: 0.00088895
Iteration 8/25 | Loss: 0.00088895
Iteration 9/25 | Loss: 0.00088894
Iteration 10/25 | Loss: 0.00088894
Iteration 11/25 | Loss: 0.00088894
Iteration 12/25 | Loss: 0.00088894
Iteration 13/25 | Loss: 0.00088894
Iteration 14/25 | Loss: 0.00088894
Iteration 15/25 | Loss: 0.00088894
Iteration 16/25 | Loss: 0.00088894
Iteration 17/25 | Loss: 0.00088894
Iteration 18/25 | Loss: 0.00088894
Iteration 19/25 | Loss: 0.00088894
Iteration 20/25 | Loss: 0.00088894
Iteration 21/25 | Loss: 0.00088894
Iteration 22/25 | Loss: 0.00088894
Iteration 23/25 | Loss: 0.00088894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000888938782736659, 0.000888938782736659, 0.000888938782736659, 0.000888938782736659, 0.000888938782736659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000888938782736659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44287777
Iteration 2/25 | Loss: 0.00031762
Iteration 3/25 | Loss: 0.00031762
Iteration 4/25 | Loss: 0.00031762
Iteration 5/25 | Loss: 0.00031762
Iteration 6/25 | Loss: 0.00031762
Iteration 7/25 | Loss: 0.00031762
Iteration 8/25 | Loss: 0.00031762
Iteration 9/25 | Loss: 0.00031762
Iteration 10/25 | Loss: 0.00031762
Iteration 11/25 | Loss: 0.00031762
Iteration 12/25 | Loss: 0.00031762
Iteration 13/25 | Loss: 0.00031762
Iteration 14/25 | Loss: 0.00031762
Iteration 15/25 | Loss: 0.00031762
Iteration 16/25 | Loss: 0.00031762
Iteration 17/25 | Loss: 0.00031762
Iteration 18/25 | Loss: 0.00031762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00031761586433276534, 0.00031761586433276534, 0.00031761586433276534, 0.00031761586433276534, 0.00031761586433276534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031761586433276534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031762
Iteration 2/1000 | Loss: 0.00003435
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00002058
Iteration 5/1000 | Loss: 0.00001903
Iteration 6/1000 | Loss: 0.00001813
Iteration 7/1000 | Loss: 0.00001768
Iteration 8/1000 | Loss: 0.00001727
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001696
Iteration 12/1000 | Loss: 0.00001694
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001691
Iteration 15/1000 | Loss: 0.00001691
Iteration 16/1000 | Loss: 0.00001690
Iteration 17/1000 | Loss: 0.00001690
Iteration 18/1000 | Loss: 0.00001690
Iteration 19/1000 | Loss: 0.00001690
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001690
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001689
Iteration 25/1000 | Loss: 0.00001688
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001686
Iteration 30/1000 | Loss: 0.00001686
Iteration 31/1000 | Loss: 0.00001686
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001686
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001685
Iteration 41/1000 | Loss: 0.00001685
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001685
Iteration 44/1000 | Loss: 0.00001685
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001671
Iteration 59/1000 | Loss: 0.00001671
Iteration 60/1000 | Loss: 0.00001669
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001667
Iteration 66/1000 | Loss: 0.00001666
Iteration 67/1000 | Loss: 0.00001666
Iteration 68/1000 | Loss: 0.00001666
Iteration 69/1000 | Loss: 0.00001665
Iteration 70/1000 | Loss: 0.00001665
Iteration 71/1000 | Loss: 0.00001664
Iteration 72/1000 | Loss: 0.00001664
Iteration 73/1000 | Loss: 0.00001663
Iteration 74/1000 | Loss: 0.00001663
Iteration 75/1000 | Loss: 0.00001663
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001663
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001663
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001663
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001662
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001661
Iteration 90/1000 | Loss: 0.00001661
Iteration 91/1000 | Loss: 0.00001661
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001660
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001660
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001659
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001658
Iteration 106/1000 | Loss: 0.00001658
Iteration 107/1000 | Loss: 0.00001658
Iteration 108/1000 | Loss: 0.00001658
Iteration 109/1000 | Loss: 0.00001658
Iteration 110/1000 | Loss: 0.00001658
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001657
Iteration 118/1000 | Loss: 0.00001657
Iteration 119/1000 | Loss: 0.00001657
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Iteration 124/1000 | Loss: 0.00001657
Iteration 125/1000 | Loss: 0.00001657
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001656
Iteration 129/1000 | Loss: 0.00001656
Iteration 130/1000 | Loss: 0.00001656
Iteration 131/1000 | Loss: 0.00001656
Iteration 132/1000 | Loss: 0.00001656
Iteration 133/1000 | Loss: 0.00001656
Iteration 134/1000 | Loss: 0.00001656
Iteration 135/1000 | Loss: 0.00001656
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001655
Iteration 143/1000 | Loss: 0.00001655
Iteration 144/1000 | Loss: 0.00001655
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001655
Iteration 151/1000 | Loss: 0.00001655
Iteration 152/1000 | Loss: 0.00001654
Iteration 153/1000 | Loss: 0.00001654
Iteration 154/1000 | Loss: 0.00001654
Iteration 155/1000 | Loss: 0.00001654
Iteration 156/1000 | Loss: 0.00001654
Iteration 157/1000 | Loss: 0.00001654
Iteration 158/1000 | Loss: 0.00001654
Iteration 159/1000 | Loss: 0.00001654
Iteration 160/1000 | Loss: 0.00001654
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001654
Iteration 165/1000 | Loss: 0.00001654
Iteration 166/1000 | Loss: 0.00001654
Iteration 167/1000 | Loss: 0.00001654
Iteration 168/1000 | Loss: 0.00001654
Iteration 169/1000 | Loss: 0.00001654
Iteration 170/1000 | Loss: 0.00001654
Iteration 171/1000 | Loss: 0.00001654
Iteration 172/1000 | Loss: 0.00001653
Iteration 173/1000 | Loss: 0.00001653
Iteration 174/1000 | Loss: 0.00001653
Iteration 175/1000 | Loss: 0.00001653
Iteration 176/1000 | Loss: 0.00001653
Iteration 177/1000 | Loss: 0.00001653
Iteration 178/1000 | Loss: 0.00001653
Iteration 179/1000 | Loss: 0.00001653
Iteration 180/1000 | Loss: 0.00001653
Iteration 181/1000 | Loss: 0.00001653
Iteration 182/1000 | Loss: 0.00001653
Iteration 183/1000 | Loss: 0.00001653
Iteration 184/1000 | Loss: 0.00001653
Iteration 185/1000 | Loss: 0.00001653
Iteration 186/1000 | Loss: 0.00001653
Iteration 187/1000 | Loss: 0.00001653
Iteration 188/1000 | Loss: 0.00001653
Iteration 189/1000 | Loss: 0.00001653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.6533906091353856e-05, 1.6533906091353856e-05, 1.6533906091353856e-05, 1.6533906091353856e-05, 1.6533906091353856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6533906091353856e-05

Optimization complete. Final v2v error: 3.5189719200134277 mm

Highest mean error: 4.177680969238281 mm for frame 62

Lowest mean error: 3.2258079051971436 mm for frame 50

Saving results

Total time: 35.83685851097107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107270
Iteration 2/25 | Loss: 0.00197892
Iteration 3/25 | Loss: 0.00140207
Iteration 4/25 | Loss: 0.00132107
Iteration 5/25 | Loss: 0.00128338
Iteration 6/25 | Loss: 0.00128872
Iteration 7/25 | Loss: 0.00126868
Iteration 8/25 | Loss: 0.00124164
Iteration 9/25 | Loss: 0.00123386
Iteration 10/25 | Loss: 0.00122183
Iteration 11/25 | Loss: 0.00121878
Iteration 12/25 | Loss: 0.00120747
Iteration 13/25 | Loss: 0.00119657
Iteration 14/25 | Loss: 0.00121225
Iteration 15/25 | Loss: 0.00120438
Iteration 16/25 | Loss: 0.00120323
Iteration 17/25 | Loss: 0.00120301
Iteration 18/25 | Loss: 0.00119236
Iteration 19/25 | Loss: 0.00119416
Iteration 20/25 | Loss: 0.00119358
Iteration 21/25 | Loss: 0.00118877
Iteration 22/25 | Loss: 0.00118603
Iteration 23/25 | Loss: 0.00119918
Iteration 24/25 | Loss: 0.00119909
Iteration 25/25 | Loss: 0.00121027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08169603
Iteration 2/25 | Loss: 0.00970465
Iteration 3/25 | Loss: 0.02125848
Iteration 4/25 | Loss: 0.01209849
Iteration 5/25 | Loss: 0.00662231
Iteration 6/25 | Loss: 0.00462013
Iteration 7/25 | Loss: 0.00327592
Iteration 8/25 | Loss: 0.00327592
Iteration 9/25 | Loss: 0.00327591
Iteration 10/25 | Loss: 0.00327591
Iteration 11/25 | Loss: 0.00327591
Iteration 12/25 | Loss: 0.00327591
Iteration 13/25 | Loss: 0.00327591
Iteration 14/25 | Loss: 0.00327591
Iteration 15/25 | Loss: 0.00327591
Iteration 16/25 | Loss: 0.00327591
Iteration 17/25 | Loss: 0.00327591
Iteration 18/25 | Loss: 0.00327591
Iteration 19/25 | Loss: 0.00327591
Iteration 20/25 | Loss: 0.00327591
Iteration 21/25 | Loss: 0.00327591
Iteration 22/25 | Loss: 0.00327591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0032759124878793955, 0.0032759124878793955, 0.0032759124878793955, 0.0032759124878793955, 0.0032759124878793955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032759124878793955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00327591
Iteration 2/1000 | Loss: 0.00905672
Iteration 3/1000 | Loss: 0.00213349
Iteration 4/1000 | Loss: 0.00262747
Iteration 5/1000 | Loss: 0.00147674
Iteration 6/1000 | Loss: 0.00633809
Iteration 7/1000 | Loss: 0.00151581
Iteration 8/1000 | Loss: 0.00201974
Iteration 9/1000 | Loss: 0.00256062
Iteration 10/1000 | Loss: 0.00460425
Iteration 11/1000 | Loss: 0.00186390
Iteration 12/1000 | Loss: 0.00233787
Iteration 13/1000 | Loss: 0.00096467
Iteration 14/1000 | Loss: 0.00145858
Iteration 15/1000 | Loss: 0.00159260
Iteration 16/1000 | Loss: 0.00148106
Iteration 17/1000 | Loss: 0.00082600
Iteration 18/1000 | Loss: 0.00134388
Iteration 19/1000 | Loss: 0.00278147
Iteration 20/1000 | Loss: 0.00158628
Iteration 21/1000 | Loss: 0.00218988
Iteration 22/1000 | Loss: 0.00180467
Iteration 23/1000 | Loss: 0.00283627
Iteration 24/1000 | Loss: 0.00178053
Iteration 25/1000 | Loss: 0.00141779
Iteration 26/1000 | Loss: 0.00096664
Iteration 27/1000 | Loss: 0.00148753
Iteration 28/1000 | Loss: 0.00132368
Iteration 29/1000 | Loss: 0.00134952
Iteration 30/1000 | Loss: 0.00283824
Iteration 31/1000 | Loss: 0.00153613
Iteration 32/1000 | Loss: 0.00320380
Iteration 33/1000 | Loss: 0.00279625
Iteration 34/1000 | Loss: 0.00106587
Iteration 35/1000 | Loss: 0.00103544
Iteration 36/1000 | Loss: 0.00132939
Iteration 37/1000 | Loss: 0.00094364
Iteration 38/1000 | Loss: 0.00058257
Iteration 39/1000 | Loss: 0.00053330
Iteration 40/1000 | Loss: 0.00051166
Iteration 41/1000 | Loss: 0.00055371
Iteration 42/1000 | Loss: 0.00080272
Iteration 43/1000 | Loss: 0.00099998
Iteration 44/1000 | Loss: 0.00088604
Iteration 45/1000 | Loss: 0.00077243
Iteration 46/1000 | Loss: 0.00105729
Iteration 47/1000 | Loss: 0.00048399
Iteration 48/1000 | Loss: 0.00064507
Iteration 49/1000 | Loss: 0.00114630
Iteration 50/1000 | Loss: 0.00083634
Iteration 51/1000 | Loss: 0.00065311
Iteration 52/1000 | Loss: 0.00174426
Iteration 53/1000 | Loss: 0.00304560
Iteration 54/1000 | Loss: 0.00081491
Iteration 55/1000 | Loss: 0.00089562
Iteration 56/1000 | Loss: 0.00097084
Iteration 57/1000 | Loss: 0.00097885
Iteration 58/1000 | Loss: 0.00106279
Iteration 59/1000 | Loss: 0.00122038
Iteration 60/1000 | Loss: 0.00169355
Iteration 61/1000 | Loss: 0.00121253
Iteration 62/1000 | Loss: 0.00113787
Iteration 63/1000 | Loss: 0.00113502
Iteration 64/1000 | Loss: 0.00094492
Iteration 65/1000 | Loss: 0.00119402
Iteration 66/1000 | Loss: 0.00131163
Iteration 67/1000 | Loss: 0.00130892
Iteration 68/1000 | Loss: 0.00113901
Iteration 69/1000 | Loss: 0.00201940
Iteration 70/1000 | Loss: 0.00115594
Iteration 71/1000 | Loss: 0.00127224
Iteration 72/1000 | Loss: 0.00122058
Iteration 73/1000 | Loss: 0.00098875
Iteration 74/1000 | Loss: 0.00098122
Iteration 75/1000 | Loss: 0.00115980
Iteration 76/1000 | Loss: 0.00113212
Iteration 77/1000 | Loss: 0.00110833
Iteration 78/1000 | Loss: 0.00098093
Iteration 79/1000 | Loss: 0.00083338
Iteration 80/1000 | Loss: 0.00105604
Iteration 81/1000 | Loss: 0.00101336
Iteration 82/1000 | Loss: 0.00085566
Iteration 83/1000 | Loss: 0.00100334
Iteration 84/1000 | Loss: 0.00103851
Iteration 85/1000 | Loss: 0.00102830
Iteration 86/1000 | Loss: 0.00066453
Iteration 87/1000 | Loss: 0.00052146
Iteration 88/1000 | Loss: 0.00062601
Iteration 89/1000 | Loss: 0.00075007
Iteration 90/1000 | Loss: 0.00073828
Iteration 91/1000 | Loss: 0.00101038
Iteration 92/1000 | Loss: 0.00083796
Iteration 93/1000 | Loss: 0.00063261
Iteration 94/1000 | Loss: 0.00084477
Iteration 95/1000 | Loss: 0.00070562
Iteration 96/1000 | Loss: 0.00082872
Iteration 97/1000 | Loss: 0.00064959
Iteration 98/1000 | Loss: 0.00033883
Iteration 99/1000 | Loss: 0.00033023
Iteration 100/1000 | Loss: 0.00051065
Iteration 101/1000 | Loss: 0.00040806
Iteration 102/1000 | Loss: 0.00059737
Iteration 103/1000 | Loss: 0.00054267
Iteration 104/1000 | Loss: 0.00053722
Iteration 105/1000 | Loss: 0.00040763
Iteration 106/1000 | Loss: 0.00053287
Iteration 107/1000 | Loss: 0.00130726
Iteration 108/1000 | Loss: 0.00044969
Iteration 109/1000 | Loss: 0.00035784
Iteration 110/1000 | Loss: 0.00040701
Iteration 111/1000 | Loss: 0.00059926
Iteration 112/1000 | Loss: 0.00070481
Iteration 113/1000 | Loss: 0.00052626
Iteration 114/1000 | Loss: 0.00075409
Iteration 115/1000 | Loss: 0.00076997
Iteration 116/1000 | Loss: 0.00084743
Iteration 117/1000 | Loss: 0.00079340
Iteration 118/1000 | Loss: 0.00092288
Iteration 119/1000 | Loss: 0.00103441
Iteration 120/1000 | Loss: 0.00089201
Iteration 121/1000 | Loss: 0.00092731
Iteration 122/1000 | Loss: 0.00062555
Iteration 123/1000 | Loss: 0.00036278
Iteration 124/1000 | Loss: 0.00096565
Iteration 125/1000 | Loss: 0.00100183
Iteration 126/1000 | Loss: 0.00105943
Iteration 127/1000 | Loss: 0.00105749
Iteration 128/1000 | Loss: 0.00122765
Iteration 129/1000 | Loss: 0.00087490
Iteration 130/1000 | Loss: 0.00048550
Iteration 131/1000 | Loss: 0.00065604
Iteration 132/1000 | Loss: 0.00059584
Iteration 133/1000 | Loss: 0.00035870
Iteration 134/1000 | Loss: 0.00153715
Iteration 135/1000 | Loss: 0.00090868
Iteration 136/1000 | Loss: 0.00070649
Iteration 137/1000 | Loss: 0.00058328
Iteration 138/1000 | Loss: 0.00032559
Iteration 139/1000 | Loss: 0.00040093
Iteration 140/1000 | Loss: 0.00038705
Iteration 141/1000 | Loss: 0.00023052
Iteration 142/1000 | Loss: 0.00040878
Iteration 143/1000 | Loss: 0.00047339
Iteration 144/1000 | Loss: 0.00062822
Iteration 145/1000 | Loss: 0.00050670
Iteration 146/1000 | Loss: 0.00040989
Iteration 147/1000 | Loss: 0.00053136
Iteration 148/1000 | Loss: 0.00032691
Iteration 149/1000 | Loss: 0.00025616
Iteration 150/1000 | Loss: 0.00021440
Iteration 151/1000 | Loss: 0.00024593
Iteration 152/1000 | Loss: 0.00022686
Iteration 153/1000 | Loss: 0.00063483
Iteration 154/1000 | Loss: 0.00025490
Iteration 155/1000 | Loss: 0.00024207
Iteration 156/1000 | Loss: 0.00050281
Iteration 157/1000 | Loss: 0.00043997
Iteration 158/1000 | Loss: 0.00028967
Iteration 159/1000 | Loss: 0.00157391
Iteration 160/1000 | Loss: 0.00102413
Iteration 161/1000 | Loss: 0.00101519
Iteration 162/1000 | Loss: 0.00052605
Iteration 163/1000 | Loss: 0.00088498
Iteration 164/1000 | Loss: 0.00063841
Iteration 165/1000 | Loss: 0.00047620
Iteration 166/1000 | Loss: 0.00067567
Iteration 167/1000 | Loss: 0.00036329
Iteration 168/1000 | Loss: 0.00071472
Iteration 169/1000 | Loss: 0.00055348
Iteration 170/1000 | Loss: 0.00055310
Iteration 171/1000 | Loss: 0.00038538
Iteration 172/1000 | Loss: 0.00056522
Iteration 173/1000 | Loss: 0.00060571
Iteration 174/1000 | Loss: 0.00062862
Iteration 175/1000 | Loss: 0.00021190
Iteration 176/1000 | Loss: 0.00046191
Iteration 177/1000 | Loss: 0.00042219
Iteration 178/1000 | Loss: 0.00053829
Iteration 179/1000 | Loss: 0.00056256
Iteration 180/1000 | Loss: 0.00047395
Iteration 181/1000 | Loss: 0.00048346
Iteration 182/1000 | Loss: 0.00044599
Iteration 183/1000 | Loss: 0.00058568
Iteration 184/1000 | Loss: 0.00034705
Iteration 185/1000 | Loss: 0.00018684
Iteration 186/1000 | Loss: 0.00009742
Iteration 187/1000 | Loss: 0.00016277
Iteration 188/1000 | Loss: 0.00034259
Iteration 189/1000 | Loss: 0.00036647
Iteration 190/1000 | Loss: 0.00089925
Iteration 191/1000 | Loss: 0.00161093
Iteration 192/1000 | Loss: 0.00021273
Iteration 193/1000 | Loss: 0.00096893
Iteration 194/1000 | Loss: 0.00194340
Iteration 195/1000 | Loss: 0.00024434
Iteration 196/1000 | Loss: 0.00017993
Iteration 197/1000 | Loss: 0.00013424
Iteration 198/1000 | Loss: 0.00037536
Iteration 199/1000 | Loss: 0.00023450
Iteration 200/1000 | Loss: 0.00019017
Iteration 201/1000 | Loss: 0.00008127
Iteration 202/1000 | Loss: 0.00017461
Iteration 203/1000 | Loss: 0.00016292
Iteration 204/1000 | Loss: 0.00022430
Iteration 205/1000 | Loss: 0.00025954
Iteration 206/1000 | Loss: 0.00023015
Iteration 207/1000 | Loss: 0.00037019
Iteration 208/1000 | Loss: 0.00026726
Iteration 209/1000 | Loss: 0.00036892
Iteration 210/1000 | Loss: 0.00025321
Iteration 211/1000 | Loss: 0.00021976
Iteration 212/1000 | Loss: 0.00019777
Iteration 213/1000 | Loss: 0.00065094
Iteration 214/1000 | Loss: 0.00089045
Iteration 215/1000 | Loss: 0.00019365
Iteration 216/1000 | Loss: 0.00020798
Iteration 217/1000 | Loss: 0.00013721
Iteration 218/1000 | Loss: 0.00015295
Iteration 219/1000 | Loss: 0.00023656
Iteration 220/1000 | Loss: 0.00016180
Iteration 221/1000 | Loss: 0.00013668
Iteration 222/1000 | Loss: 0.00017495
Iteration 223/1000 | Loss: 0.00029372
Iteration 224/1000 | Loss: 0.00031027
Iteration 225/1000 | Loss: 0.00033647
Iteration 226/1000 | Loss: 0.00030237
Iteration 227/1000 | Loss: 0.00022682
Iteration 228/1000 | Loss: 0.00014298
Iteration 229/1000 | Loss: 0.00014364
Iteration 230/1000 | Loss: 0.00010019
Iteration 231/1000 | Loss: 0.00016179
Iteration 232/1000 | Loss: 0.00022460
Iteration 233/1000 | Loss: 0.00041811
Iteration 234/1000 | Loss: 0.00037336
Iteration 235/1000 | Loss: 0.00042845
Iteration 236/1000 | Loss: 0.00040755
Iteration 237/1000 | Loss: 0.00024089
Iteration 238/1000 | Loss: 0.00071821
Iteration 239/1000 | Loss: 0.00017499
Iteration 240/1000 | Loss: 0.00026133
Iteration 241/1000 | Loss: 0.00027429
Iteration 242/1000 | Loss: 0.00024857
Iteration 243/1000 | Loss: 0.00019639
Iteration 244/1000 | Loss: 0.00023580
Iteration 245/1000 | Loss: 0.00020020
Iteration 246/1000 | Loss: 0.00019635
Iteration 247/1000 | Loss: 0.00017985
Iteration 248/1000 | Loss: 0.00037019
Iteration 249/1000 | Loss: 0.00015690
Iteration 250/1000 | Loss: 0.00022847
Iteration 251/1000 | Loss: 0.00011011
Iteration 252/1000 | Loss: 0.00025750
Iteration 253/1000 | Loss: 0.00021061
Iteration 254/1000 | Loss: 0.00018373
Iteration 255/1000 | Loss: 0.00029990
Iteration 256/1000 | Loss: 0.00055450
Iteration 257/1000 | Loss: 0.00054186
Iteration 258/1000 | Loss: 0.00019489
Iteration 259/1000 | Loss: 0.00016203
Iteration 260/1000 | Loss: 0.00037874
Iteration 261/1000 | Loss: 0.00015352
Iteration 262/1000 | Loss: 0.00005779
Iteration 263/1000 | Loss: 0.00003894
Iteration 264/1000 | Loss: 0.00005002
Iteration 265/1000 | Loss: 0.00011940
Iteration 266/1000 | Loss: 0.00021165
Iteration 267/1000 | Loss: 0.00006817
Iteration 268/1000 | Loss: 0.00017248
Iteration 269/1000 | Loss: 0.00013037
Iteration 270/1000 | Loss: 0.00012996
Iteration 271/1000 | Loss: 0.00007795
Iteration 272/1000 | Loss: 0.00032846
Iteration 273/1000 | Loss: 0.00024133
Iteration 274/1000 | Loss: 0.00018801
Iteration 275/1000 | Loss: 0.00017541
Iteration 276/1000 | Loss: 0.00012038
Iteration 277/1000 | Loss: 0.00011955
Iteration 278/1000 | Loss: 0.00014397
Iteration 279/1000 | Loss: 0.00016655
Iteration 280/1000 | Loss: 0.00013006
Iteration 281/1000 | Loss: 0.00013022
Iteration 282/1000 | Loss: 0.00024790
Iteration 283/1000 | Loss: 0.00070434
Iteration 284/1000 | Loss: 0.00019417
Iteration 285/1000 | Loss: 0.00016175
Iteration 286/1000 | Loss: 0.00021445
Iteration 287/1000 | Loss: 0.00022217
Iteration 288/1000 | Loss: 0.00016917
Iteration 289/1000 | Loss: 0.00022056
Iteration 290/1000 | Loss: 0.00017769
Iteration 291/1000 | Loss: 0.00021057
Iteration 292/1000 | Loss: 0.00018498
Iteration 293/1000 | Loss: 0.00020545
Iteration 294/1000 | Loss: 0.00029186
Iteration 295/1000 | Loss: 0.00107553
Iteration 296/1000 | Loss: 0.00018486
Iteration 297/1000 | Loss: 0.00019778
Iteration 298/1000 | Loss: 0.00062232
Iteration 299/1000 | Loss: 0.00007899
Iteration 300/1000 | Loss: 0.00013241
Iteration 301/1000 | Loss: 0.00005666
Iteration 302/1000 | Loss: 0.00004141
Iteration 303/1000 | Loss: 0.00003472
Iteration 304/1000 | Loss: 0.00003150
Iteration 305/1000 | Loss: 0.00002902
Iteration 306/1000 | Loss: 0.00002823
Iteration 307/1000 | Loss: 0.00002742
Iteration 308/1000 | Loss: 0.00013918
Iteration 309/1000 | Loss: 0.00003630
Iteration 310/1000 | Loss: 0.00003042
Iteration 311/1000 | Loss: 0.00002852
Iteration 312/1000 | Loss: 0.00002742
Iteration 313/1000 | Loss: 0.00002689
Iteration 314/1000 | Loss: 0.00002641
Iteration 315/1000 | Loss: 0.00002580
Iteration 316/1000 | Loss: 0.00002540
Iteration 317/1000 | Loss: 0.00014237
Iteration 318/1000 | Loss: 0.00005313
Iteration 319/1000 | Loss: 0.00014867
Iteration 320/1000 | Loss: 0.00010756
Iteration 321/1000 | Loss: 0.00011343
Iteration 322/1000 | Loss: 0.00007422
Iteration 323/1000 | Loss: 0.00019559
Iteration 324/1000 | Loss: 0.00004179
Iteration 325/1000 | Loss: 0.00002908
Iteration 326/1000 | Loss: 0.00002642
Iteration 327/1000 | Loss: 0.00002546
Iteration 328/1000 | Loss: 0.00002471
Iteration 329/1000 | Loss: 0.00002628
Iteration 330/1000 | Loss: 0.00002463
Iteration 331/1000 | Loss: 0.00002445
Iteration 332/1000 | Loss: 0.00002442
Iteration 333/1000 | Loss: 0.00002441
Iteration 334/1000 | Loss: 0.00002437
Iteration 335/1000 | Loss: 0.00002436
Iteration 336/1000 | Loss: 0.00002435
Iteration 337/1000 | Loss: 0.00002435
Iteration 338/1000 | Loss: 0.00002434
Iteration 339/1000 | Loss: 0.00002434
Iteration 340/1000 | Loss: 0.00002433
Iteration 341/1000 | Loss: 0.00002430
Iteration 342/1000 | Loss: 0.00002427
Iteration 343/1000 | Loss: 0.00002426
Iteration 344/1000 | Loss: 0.00002426
Iteration 345/1000 | Loss: 0.00002426
Iteration 346/1000 | Loss: 0.00002426
Iteration 347/1000 | Loss: 0.00002425
Iteration 348/1000 | Loss: 0.00002425
Iteration 349/1000 | Loss: 0.00002425
Iteration 350/1000 | Loss: 0.00002425
Iteration 351/1000 | Loss: 0.00002424
Iteration 352/1000 | Loss: 0.00002424
Iteration 353/1000 | Loss: 0.00002424
Iteration 354/1000 | Loss: 0.00002418
Iteration 355/1000 | Loss: 0.00002413
Iteration 356/1000 | Loss: 0.00002413
Iteration 357/1000 | Loss: 0.00002412
Iteration 358/1000 | Loss: 0.00002410
Iteration 359/1000 | Loss: 0.00002410
Iteration 360/1000 | Loss: 0.00002410
Iteration 361/1000 | Loss: 0.00002409
Iteration 362/1000 | Loss: 0.00002409
Iteration 363/1000 | Loss: 0.00002406
Iteration 364/1000 | Loss: 0.00002404
Iteration 365/1000 | Loss: 0.00002403
Iteration 366/1000 | Loss: 0.00002402
Iteration 367/1000 | Loss: 0.00002402
Iteration 368/1000 | Loss: 0.00002381
Iteration 369/1000 | Loss: 0.00002355
Iteration 370/1000 | Loss: 0.00002341
Iteration 371/1000 | Loss: 0.00002339
Iteration 372/1000 | Loss: 0.00002337
Iteration 373/1000 | Loss: 0.00002335
Iteration 374/1000 | Loss: 0.00002325
Iteration 375/1000 | Loss: 0.00002319
Iteration 376/1000 | Loss: 0.00002314
Iteration 377/1000 | Loss: 0.00002313
Iteration 378/1000 | Loss: 0.00002313
Iteration 379/1000 | Loss: 0.00002312
Iteration 380/1000 | Loss: 0.00002312
Iteration 381/1000 | Loss: 0.00002306
Iteration 382/1000 | Loss: 0.00002303
Iteration 383/1000 | Loss: 0.00002301
Iteration 384/1000 | Loss: 0.00002293
Iteration 385/1000 | Loss: 0.00002288
Iteration 386/1000 | Loss: 0.00002287
Iteration 387/1000 | Loss: 0.00002286
Iteration 388/1000 | Loss: 0.00002286
Iteration 389/1000 | Loss: 0.00002285
Iteration 390/1000 | Loss: 0.00002285
Iteration 391/1000 | Loss: 0.00002284
Iteration 392/1000 | Loss: 0.00002282
Iteration 393/1000 | Loss: 0.00002282
Iteration 394/1000 | Loss: 0.00002282
Iteration 395/1000 | Loss: 0.00002282
Iteration 396/1000 | Loss: 0.00002282
Iteration 397/1000 | Loss: 0.00002282
Iteration 398/1000 | Loss: 0.00002280
Iteration 399/1000 | Loss: 0.00002280
Iteration 400/1000 | Loss: 0.00002279
Iteration 401/1000 | Loss: 0.00002279
Iteration 402/1000 | Loss: 0.00002277
Iteration 403/1000 | Loss: 0.00002277
Iteration 404/1000 | Loss: 0.00002276
Iteration 405/1000 | Loss: 0.00002276
Iteration 406/1000 | Loss: 0.00002276
Iteration 407/1000 | Loss: 0.00002275
Iteration 408/1000 | Loss: 0.00002274
Iteration 409/1000 | Loss: 0.00002273
Iteration 410/1000 | Loss: 0.00002273
Iteration 411/1000 | Loss: 0.00002273
Iteration 412/1000 | Loss: 0.00002272
Iteration 413/1000 | Loss: 0.00002272
Iteration 414/1000 | Loss: 0.00002272
Iteration 415/1000 | Loss: 0.00002272
Iteration 416/1000 | Loss: 0.00002272
Iteration 417/1000 | Loss: 0.00002271
Iteration 418/1000 | Loss: 0.00002271
Iteration 419/1000 | Loss: 0.00002271
Iteration 420/1000 | Loss: 0.00002271
Iteration 421/1000 | Loss: 0.00002270
Iteration 422/1000 | Loss: 0.00002270
Iteration 423/1000 | Loss: 0.00002270
Iteration 424/1000 | Loss: 0.00002270
Iteration 425/1000 | Loss: 0.00002270
Iteration 426/1000 | Loss: 0.00002270
Iteration 427/1000 | Loss: 0.00002270
Iteration 428/1000 | Loss: 0.00002270
Iteration 429/1000 | Loss: 0.00002270
Iteration 430/1000 | Loss: 0.00002269
Iteration 431/1000 | Loss: 0.00002269
Iteration 432/1000 | Loss: 0.00002269
Iteration 433/1000 | Loss: 0.00002269
Iteration 434/1000 | Loss: 0.00002269
Iteration 435/1000 | Loss: 0.00002269
Iteration 436/1000 | Loss: 0.00002269
Iteration 437/1000 | Loss: 0.00002269
Iteration 438/1000 | Loss: 0.00002269
Iteration 439/1000 | Loss: 0.00002269
Iteration 440/1000 | Loss: 0.00002269
Iteration 441/1000 | Loss: 0.00002269
Iteration 442/1000 | Loss: 0.00002269
Iteration 443/1000 | Loss: 0.00002269
Iteration 444/1000 | Loss: 0.00002269
Iteration 445/1000 | Loss: 0.00002268
Iteration 446/1000 | Loss: 0.00002268
Iteration 447/1000 | Loss: 0.00002268
Iteration 448/1000 | Loss: 0.00002268
Iteration 449/1000 | Loss: 0.00002268
Iteration 450/1000 | Loss: 0.00002268
Iteration 451/1000 | Loss: 0.00002268
Iteration 452/1000 | Loss: 0.00002268
Iteration 453/1000 | Loss: 0.00002268
Iteration 454/1000 | Loss: 0.00002268
Iteration 455/1000 | Loss: 0.00002267
Iteration 456/1000 | Loss: 0.00002267
Iteration 457/1000 | Loss: 0.00002267
Iteration 458/1000 | Loss: 0.00002267
Iteration 459/1000 | Loss: 0.00002267
Iteration 460/1000 | Loss: 0.00002267
Iteration 461/1000 | Loss: 0.00002267
Iteration 462/1000 | Loss: 0.00002267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 462. Stopping optimization.
Last 5 losses: [2.2674665160593577e-05, 2.2674665160593577e-05, 2.2674665160593577e-05, 2.2674665160593577e-05, 2.2674665160593577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2674665160593577e-05

Optimization complete. Final v2v error: 4.078757286071777 mm

Highest mean error: 5.559268951416016 mm for frame 193

Lowest mean error: 3.4498815536499023 mm for frame 180

Saving results

Total time: 612.0635418891907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01132900
Iteration 2/25 | Loss: 0.01132900
Iteration 3/25 | Loss: 0.01132900
Iteration 4/25 | Loss: 0.01132899
Iteration 5/25 | Loss: 0.01132899
Iteration 6/25 | Loss: 0.01132899
Iteration 7/25 | Loss: 0.01132899
Iteration 8/25 | Loss: 0.01132899
Iteration 9/25 | Loss: 0.01132899
Iteration 10/25 | Loss: 0.01132899
Iteration 11/25 | Loss: 0.01132898
Iteration 12/25 | Loss: 0.01132898
Iteration 13/25 | Loss: 0.01132898
Iteration 14/25 | Loss: 0.01132898
Iteration 15/25 | Loss: 0.01132898
Iteration 16/25 | Loss: 0.01132898
Iteration 17/25 | Loss: 0.01132898
Iteration 18/25 | Loss: 0.01132897
Iteration 19/25 | Loss: 0.01132897
Iteration 20/25 | Loss: 0.01132897
Iteration 21/25 | Loss: 0.01132897
Iteration 22/25 | Loss: 0.01132897
Iteration 23/25 | Loss: 0.01132896
Iteration 24/25 | Loss: 0.01132896
Iteration 25/25 | Loss: 0.01132896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79812622
Iteration 2/25 | Loss: 0.13144135
Iteration 3/25 | Loss: 0.08398443
Iteration 4/25 | Loss: 0.07523684
Iteration 5/25 | Loss: 0.07484494
Iteration 6/25 | Loss: 0.07484493
Iteration 7/25 | Loss: 0.07484494
Iteration 8/25 | Loss: 0.07484494
Iteration 9/25 | Loss: 0.07484493
Iteration 10/25 | Loss: 0.07484493
Iteration 11/25 | Loss: 0.07484493
Iteration 12/25 | Loss: 0.07484493
Iteration 13/25 | Loss: 0.07484493
Iteration 14/25 | Loss: 0.07484492
Iteration 15/25 | Loss: 0.07484492
Iteration 16/25 | Loss: 0.07484492
Iteration 17/25 | Loss: 0.07484492
Iteration 18/25 | Loss: 0.07484492
Iteration 19/25 | Loss: 0.07484493
Iteration 20/25 | Loss: 0.07484493
Iteration 21/25 | Loss: 0.07484492
Iteration 22/25 | Loss: 0.07484492
Iteration 23/25 | Loss: 0.07484492
Iteration 24/25 | Loss: 0.07484492
Iteration 25/25 | Loss: 0.07484492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07484492
Iteration 2/1000 | Loss: 0.00330600
Iteration 3/1000 | Loss: 0.00158178
Iteration 4/1000 | Loss: 0.00145669
Iteration 5/1000 | Loss: 0.00028414
Iteration 6/1000 | Loss: 0.00050702
Iteration 7/1000 | Loss: 0.00024842
Iteration 8/1000 | Loss: 0.00012040
Iteration 9/1000 | Loss: 0.00014240
Iteration 10/1000 | Loss: 0.00014273
Iteration 11/1000 | Loss: 0.00030881
Iteration 12/1000 | Loss: 0.00060354
Iteration 13/1000 | Loss: 0.00029701
Iteration 14/1000 | Loss: 0.00009584
Iteration 15/1000 | Loss: 0.00008441
Iteration 16/1000 | Loss: 0.00005744
Iteration 17/1000 | Loss: 0.00015742
Iteration 18/1000 | Loss: 0.00031527
Iteration 19/1000 | Loss: 0.00065348
Iteration 20/1000 | Loss: 0.00037610
Iteration 21/1000 | Loss: 0.00011648
Iteration 22/1000 | Loss: 0.00011364
Iteration 23/1000 | Loss: 0.00031318
Iteration 24/1000 | Loss: 0.00011475
Iteration 25/1000 | Loss: 0.00022841
Iteration 26/1000 | Loss: 0.00011659
Iteration 27/1000 | Loss: 0.00005972
Iteration 28/1000 | Loss: 0.00006454
Iteration 29/1000 | Loss: 0.00004726
Iteration 30/1000 | Loss: 0.00004346
Iteration 31/1000 | Loss: 0.00016230
Iteration 32/1000 | Loss: 0.00034177
Iteration 33/1000 | Loss: 0.00016628
Iteration 34/1000 | Loss: 0.00005784
Iteration 35/1000 | Loss: 0.00027715
Iteration 36/1000 | Loss: 0.00023839
Iteration 37/1000 | Loss: 0.00028399
Iteration 38/1000 | Loss: 0.00016726
Iteration 39/1000 | Loss: 0.00006270
Iteration 40/1000 | Loss: 0.00021774
Iteration 41/1000 | Loss: 0.00004098
Iteration 42/1000 | Loss: 0.00003877
Iteration 43/1000 | Loss: 0.00003634
Iteration 44/1000 | Loss: 0.00024423
Iteration 45/1000 | Loss: 0.00004138
Iteration 46/1000 | Loss: 0.00003978
Iteration 47/1000 | Loss: 0.00009763
Iteration 48/1000 | Loss: 0.00004796
Iteration 49/1000 | Loss: 0.00030000
Iteration 50/1000 | Loss: 0.00035600
Iteration 51/1000 | Loss: 0.00016474
Iteration 52/1000 | Loss: 0.00023087
Iteration 53/1000 | Loss: 0.00016717
Iteration 54/1000 | Loss: 0.00015758
Iteration 55/1000 | Loss: 0.00010016
Iteration 56/1000 | Loss: 0.00012197
Iteration 57/1000 | Loss: 0.00003346
Iteration 58/1000 | Loss: 0.00022954
Iteration 59/1000 | Loss: 0.00007490
Iteration 60/1000 | Loss: 0.00024073
Iteration 61/1000 | Loss: 0.00016747
Iteration 62/1000 | Loss: 0.00007527
Iteration 63/1000 | Loss: 0.00014332
Iteration 64/1000 | Loss: 0.00003124
Iteration 65/1000 | Loss: 0.00003028
Iteration 66/1000 | Loss: 0.00013869
Iteration 67/1000 | Loss: 0.00005208
Iteration 68/1000 | Loss: 0.00004350
Iteration 69/1000 | Loss: 0.00003103
Iteration 70/1000 | Loss: 0.00003288
Iteration 71/1000 | Loss: 0.00016142
Iteration 72/1000 | Loss: 0.00003417
Iteration 73/1000 | Loss: 0.00003126
Iteration 74/1000 | Loss: 0.00002892
Iteration 75/1000 | Loss: 0.00002799
Iteration 76/1000 | Loss: 0.00002815
Iteration 77/1000 | Loss: 0.00002686
Iteration 78/1000 | Loss: 0.00009104
Iteration 79/1000 | Loss: 0.00002633
Iteration 80/1000 | Loss: 0.00002580
Iteration 81/1000 | Loss: 0.00002532
Iteration 82/1000 | Loss: 0.00002504
Iteration 83/1000 | Loss: 0.00002486
Iteration 84/1000 | Loss: 0.00002468
Iteration 85/1000 | Loss: 0.00002476
Iteration 86/1000 | Loss: 0.00002471
Iteration 87/1000 | Loss: 0.00002463
Iteration 88/1000 | Loss: 0.00002455
Iteration 89/1000 | Loss: 0.00002449
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002446
Iteration 92/1000 | Loss: 0.00002446
Iteration 93/1000 | Loss: 0.00002445
Iteration 94/1000 | Loss: 0.00002445
Iteration 95/1000 | Loss: 0.00002439
Iteration 96/1000 | Loss: 0.00002438
Iteration 97/1000 | Loss: 0.00002438
Iteration 98/1000 | Loss: 0.00002437
Iteration 99/1000 | Loss: 0.00002436
Iteration 100/1000 | Loss: 0.00002436
Iteration 101/1000 | Loss: 0.00002436
Iteration 102/1000 | Loss: 0.00002435
Iteration 103/1000 | Loss: 0.00002435
Iteration 104/1000 | Loss: 0.00002434
Iteration 105/1000 | Loss: 0.00002434
Iteration 106/1000 | Loss: 0.00002433
Iteration 107/1000 | Loss: 0.00002433
Iteration 108/1000 | Loss: 0.00002433
Iteration 109/1000 | Loss: 0.00002433
Iteration 110/1000 | Loss: 0.00002433
Iteration 111/1000 | Loss: 0.00002433
Iteration 112/1000 | Loss: 0.00002433
Iteration 113/1000 | Loss: 0.00002433
Iteration 114/1000 | Loss: 0.00002433
Iteration 115/1000 | Loss: 0.00002433
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002433
Iteration 118/1000 | Loss: 0.00002425
Iteration 119/1000 | Loss: 0.00002425
Iteration 120/1000 | Loss: 0.00002420
Iteration 121/1000 | Loss: 0.00002419
Iteration 122/1000 | Loss: 0.00002417
Iteration 123/1000 | Loss: 0.00002417
Iteration 124/1000 | Loss: 0.00002417
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002417
Iteration 128/1000 | Loss: 0.00002417
Iteration 129/1000 | Loss: 0.00002417
Iteration 130/1000 | Loss: 0.00002416
Iteration 131/1000 | Loss: 0.00002416
Iteration 132/1000 | Loss: 0.00002416
Iteration 133/1000 | Loss: 0.00002416
Iteration 134/1000 | Loss: 0.00002416
Iteration 135/1000 | Loss: 0.00002415
Iteration 136/1000 | Loss: 0.00002415
Iteration 137/1000 | Loss: 0.00002415
Iteration 138/1000 | Loss: 0.00002415
Iteration 139/1000 | Loss: 0.00002414
Iteration 140/1000 | Loss: 0.00002414
Iteration 141/1000 | Loss: 0.00002414
Iteration 142/1000 | Loss: 0.00002414
Iteration 143/1000 | Loss: 0.00002413
Iteration 144/1000 | Loss: 0.00002413
Iteration 145/1000 | Loss: 0.00002413
Iteration 146/1000 | Loss: 0.00002412
Iteration 147/1000 | Loss: 0.00002412
Iteration 148/1000 | Loss: 0.00002412
Iteration 149/1000 | Loss: 0.00002411
Iteration 150/1000 | Loss: 0.00002411
Iteration 151/1000 | Loss: 0.00002411
Iteration 152/1000 | Loss: 0.00002410
Iteration 153/1000 | Loss: 0.00002410
Iteration 154/1000 | Loss: 0.00002410
Iteration 155/1000 | Loss: 0.00002410
Iteration 156/1000 | Loss: 0.00002409
Iteration 157/1000 | Loss: 0.00002409
Iteration 158/1000 | Loss: 0.00002409
Iteration 159/1000 | Loss: 0.00002407
Iteration 160/1000 | Loss: 0.00002407
Iteration 161/1000 | Loss: 0.00002407
Iteration 162/1000 | Loss: 0.00002407
Iteration 163/1000 | Loss: 0.00002412
Iteration 164/1000 | Loss: 0.00002411
Iteration 165/1000 | Loss: 0.00002411
Iteration 166/1000 | Loss: 0.00002411
Iteration 167/1000 | Loss: 0.00002407
Iteration 168/1000 | Loss: 0.00002407
Iteration 169/1000 | Loss: 0.00002407
Iteration 170/1000 | Loss: 0.00002407
Iteration 171/1000 | Loss: 0.00002407
Iteration 172/1000 | Loss: 0.00002407
Iteration 173/1000 | Loss: 0.00002407
Iteration 174/1000 | Loss: 0.00002407
Iteration 175/1000 | Loss: 0.00002407
Iteration 176/1000 | Loss: 0.00002407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.406500789220445e-05, 2.406500789220445e-05, 2.406500789220445e-05, 2.406500789220445e-05, 2.406500789220445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.406500789220445e-05

Optimization complete. Final v2v error: 4.005502700805664 mm

Highest mean error: 11.297713279724121 mm for frame 1

Lowest mean error: 3.3596696853637695 mm for frame 183

Saving results

Total time: 140.68700909614563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924031
Iteration 2/25 | Loss: 0.00131142
Iteration 3/25 | Loss: 0.00096078
Iteration 4/25 | Loss: 0.00091838
Iteration 5/25 | Loss: 0.00090815
Iteration 6/25 | Loss: 0.00090549
Iteration 7/25 | Loss: 0.00090488
Iteration 8/25 | Loss: 0.00090488
Iteration 9/25 | Loss: 0.00090488
Iteration 10/25 | Loss: 0.00090488
Iteration 11/25 | Loss: 0.00090488
Iteration 12/25 | Loss: 0.00090488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000904881686437875, 0.000904881686437875, 0.000904881686437875, 0.000904881686437875, 0.000904881686437875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000904881686437875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76835847
Iteration 2/25 | Loss: 0.00031812
Iteration 3/25 | Loss: 0.00031812
Iteration 4/25 | Loss: 0.00031812
Iteration 5/25 | Loss: 0.00031812
Iteration 6/25 | Loss: 0.00031812
Iteration 7/25 | Loss: 0.00031812
Iteration 8/25 | Loss: 0.00031812
Iteration 9/25 | Loss: 0.00031812
Iteration 10/25 | Loss: 0.00031812
Iteration 11/25 | Loss: 0.00031812
Iteration 12/25 | Loss: 0.00031812
Iteration 13/25 | Loss: 0.00031812
Iteration 14/25 | Loss: 0.00031812
Iteration 15/25 | Loss: 0.00031812
Iteration 16/25 | Loss: 0.00031812
Iteration 17/25 | Loss: 0.00031812
Iteration 18/25 | Loss: 0.00031812
Iteration 19/25 | Loss: 0.00031812
Iteration 20/25 | Loss: 0.00031812
Iteration 21/25 | Loss: 0.00031812
Iteration 22/25 | Loss: 0.00031812
Iteration 23/25 | Loss: 0.00031812
Iteration 24/25 | Loss: 0.00031812
Iteration 25/25 | Loss: 0.00031812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00031811578082852066, 0.00031811578082852066, 0.00031811578082852066, 0.00031811578082852066, 0.00031811578082852066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031811578082852066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031812
Iteration 2/1000 | Loss: 0.00003273
Iteration 3/1000 | Loss: 0.00002084
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001815
Iteration 6/1000 | Loss: 0.00001761
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001698
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001689
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001687
Iteration 15/1000 | Loss: 0.00001686
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001683
Iteration 21/1000 | Loss: 0.00001683
Iteration 22/1000 | Loss: 0.00001682
Iteration 23/1000 | Loss: 0.00001682
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001681
Iteration 26/1000 | Loss: 0.00001680
Iteration 27/1000 | Loss: 0.00001680
Iteration 28/1000 | Loss: 0.00001680
Iteration 29/1000 | Loss: 0.00001680
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001679
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001678
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001677
Iteration 40/1000 | Loss: 0.00001677
Iteration 41/1000 | Loss: 0.00001677
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001676
Iteration 44/1000 | Loss: 0.00001676
Iteration 45/1000 | Loss: 0.00001675
Iteration 46/1000 | Loss: 0.00001675
Iteration 47/1000 | Loss: 0.00001675
Iteration 48/1000 | Loss: 0.00001675
Iteration 49/1000 | Loss: 0.00001672
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001672
Iteration 54/1000 | Loss: 0.00001672
Iteration 55/1000 | Loss: 0.00001672
Iteration 56/1000 | Loss: 0.00001672
Iteration 57/1000 | Loss: 0.00001671
Iteration 58/1000 | Loss: 0.00001671
Iteration 59/1000 | Loss: 0.00001671
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001670
Iteration 62/1000 | Loss: 0.00001670
Iteration 63/1000 | Loss: 0.00001670
Iteration 64/1000 | Loss: 0.00001670
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001669
Iteration 67/1000 | Loss: 0.00001669
Iteration 68/1000 | Loss: 0.00001669
Iteration 69/1000 | Loss: 0.00001669
Iteration 70/1000 | Loss: 0.00001669
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001669
Iteration 73/1000 | Loss: 0.00001668
Iteration 74/1000 | Loss: 0.00001668
Iteration 75/1000 | Loss: 0.00001668
Iteration 76/1000 | Loss: 0.00001668
Iteration 77/1000 | Loss: 0.00001668
Iteration 78/1000 | Loss: 0.00001667
Iteration 79/1000 | Loss: 0.00001667
Iteration 80/1000 | Loss: 0.00001667
Iteration 81/1000 | Loss: 0.00001667
Iteration 82/1000 | Loss: 0.00001667
Iteration 83/1000 | Loss: 0.00001667
Iteration 84/1000 | Loss: 0.00001667
Iteration 85/1000 | Loss: 0.00001667
Iteration 86/1000 | Loss: 0.00001667
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001667
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001667
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001666
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.6663983842590824e-05, 1.6663983842590824e-05, 1.6663983842590824e-05, 1.6663983842590824e-05, 1.6663983842590824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6663983842590824e-05

Optimization complete. Final v2v error: 3.503943920135498 mm

Highest mean error: 4.096065521240234 mm for frame 141

Lowest mean error: 2.9732351303100586 mm for frame 170

Saving results

Total time: 32.81701922416687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877030
Iteration 2/25 | Loss: 0.00106949
Iteration 3/25 | Loss: 0.00090142
Iteration 4/25 | Loss: 0.00088041
Iteration 5/25 | Loss: 0.00087464
Iteration 6/25 | Loss: 0.00087283
Iteration 7/25 | Loss: 0.00087243
Iteration 8/25 | Loss: 0.00087243
Iteration 9/25 | Loss: 0.00087243
Iteration 10/25 | Loss: 0.00087243
Iteration 11/25 | Loss: 0.00087243
Iteration 12/25 | Loss: 0.00087243
Iteration 13/25 | Loss: 0.00087243
Iteration 14/25 | Loss: 0.00087243
Iteration 15/25 | Loss: 0.00087243
Iteration 16/25 | Loss: 0.00087243
Iteration 17/25 | Loss: 0.00087243
Iteration 18/25 | Loss: 0.00087243
Iteration 19/25 | Loss: 0.00087243
Iteration 20/25 | Loss: 0.00087243
Iteration 21/25 | Loss: 0.00087243
Iteration 22/25 | Loss: 0.00087243
Iteration 23/25 | Loss: 0.00087243
Iteration 24/25 | Loss: 0.00087243
Iteration 25/25 | Loss: 0.00087243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43610430
Iteration 2/25 | Loss: 0.00029885
Iteration 3/25 | Loss: 0.00029884
Iteration 4/25 | Loss: 0.00029884
Iteration 5/25 | Loss: 0.00029884
Iteration 6/25 | Loss: 0.00029884
Iteration 7/25 | Loss: 0.00029884
Iteration 8/25 | Loss: 0.00029884
Iteration 9/25 | Loss: 0.00029884
Iteration 10/25 | Loss: 0.00029884
Iteration 11/25 | Loss: 0.00029884
Iteration 12/25 | Loss: 0.00029884
Iteration 13/25 | Loss: 0.00029884
Iteration 14/25 | Loss: 0.00029884
Iteration 15/25 | Loss: 0.00029884
Iteration 16/25 | Loss: 0.00029884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002988424676004797, 0.0002988424676004797, 0.0002988424676004797, 0.0002988424676004797, 0.0002988424676004797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002988424676004797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029884
Iteration 2/1000 | Loss: 0.00002803
Iteration 3/1000 | Loss: 0.00001919
Iteration 4/1000 | Loss: 0.00001727
Iteration 5/1000 | Loss: 0.00001659
Iteration 6/1000 | Loss: 0.00001628
Iteration 7/1000 | Loss: 0.00001588
Iteration 8/1000 | Loss: 0.00001565
Iteration 9/1000 | Loss: 0.00001555
Iteration 10/1000 | Loss: 0.00001554
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001550
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001549
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001549
Iteration 23/1000 | Loss: 0.00001549
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001548
Iteration 26/1000 | Loss: 0.00001548
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001547
Iteration 31/1000 | Loss: 0.00001547
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001546
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001545
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001545
Iteration 38/1000 | Loss: 0.00001545
Iteration 39/1000 | Loss: 0.00001544
Iteration 40/1000 | Loss: 0.00001544
Iteration 41/1000 | Loss: 0.00001544
Iteration 42/1000 | Loss: 0.00001544
Iteration 43/1000 | Loss: 0.00001543
Iteration 44/1000 | Loss: 0.00001543
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001542
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001541
Iteration 51/1000 | Loss: 0.00001541
Iteration 52/1000 | Loss: 0.00001541
Iteration 53/1000 | Loss: 0.00001541
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001540
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001540
Iteration 59/1000 | Loss: 0.00001540
Iteration 60/1000 | Loss: 0.00001540
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001539
Iteration 63/1000 | Loss: 0.00001539
Iteration 64/1000 | Loss: 0.00001539
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001537
Iteration 71/1000 | Loss: 0.00001537
Iteration 72/1000 | Loss: 0.00001537
Iteration 73/1000 | Loss: 0.00001536
Iteration 74/1000 | Loss: 0.00001536
Iteration 75/1000 | Loss: 0.00001536
Iteration 76/1000 | Loss: 0.00001535
Iteration 77/1000 | Loss: 0.00001535
Iteration 78/1000 | Loss: 0.00001535
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001531
Iteration 93/1000 | Loss: 0.00001531
Iteration 94/1000 | Loss: 0.00001531
Iteration 95/1000 | Loss: 0.00001531
Iteration 96/1000 | Loss: 0.00001531
Iteration 97/1000 | Loss: 0.00001531
Iteration 98/1000 | Loss: 0.00001531
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001530
Iteration 103/1000 | Loss: 0.00001530
Iteration 104/1000 | Loss: 0.00001529
Iteration 105/1000 | Loss: 0.00001529
Iteration 106/1000 | Loss: 0.00001529
Iteration 107/1000 | Loss: 0.00001528
Iteration 108/1000 | Loss: 0.00001528
Iteration 109/1000 | Loss: 0.00001528
Iteration 110/1000 | Loss: 0.00001528
Iteration 111/1000 | Loss: 0.00001528
Iteration 112/1000 | Loss: 0.00001528
Iteration 113/1000 | Loss: 0.00001528
Iteration 114/1000 | Loss: 0.00001527
Iteration 115/1000 | Loss: 0.00001527
Iteration 116/1000 | Loss: 0.00001527
Iteration 117/1000 | Loss: 0.00001527
Iteration 118/1000 | Loss: 0.00001527
Iteration 119/1000 | Loss: 0.00001527
Iteration 120/1000 | Loss: 0.00001527
Iteration 121/1000 | Loss: 0.00001527
Iteration 122/1000 | Loss: 0.00001526
Iteration 123/1000 | Loss: 0.00001526
Iteration 124/1000 | Loss: 0.00001526
Iteration 125/1000 | Loss: 0.00001526
Iteration 126/1000 | Loss: 0.00001526
Iteration 127/1000 | Loss: 0.00001526
Iteration 128/1000 | Loss: 0.00001526
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001526
Iteration 131/1000 | Loss: 0.00001526
Iteration 132/1000 | Loss: 0.00001526
Iteration 133/1000 | Loss: 0.00001526
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001525
Iteration 136/1000 | Loss: 0.00001525
Iteration 137/1000 | Loss: 0.00001525
Iteration 138/1000 | Loss: 0.00001525
Iteration 139/1000 | Loss: 0.00001525
Iteration 140/1000 | Loss: 0.00001525
Iteration 141/1000 | Loss: 0.00001525
Iteration 142/1000 | Loss: 0.00001525
Iteration 143/1000 | Loss: 0.00001525
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001525
Iteration 146/1000 | Loss: 0.00001525
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001525
Iteration 149/1000 | Loss: 0.00001525
Iteration 150/1000 | Loss: 0.00001525
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Iteration 154/1000 | Loss: 0.00001524
Iteration 155/1000 | Loss: 0.00001524
Iteration 156/1000 | Loss: 0.00001524
Iteration 157/1000 | Loss: 0.00001524
Iteration 158/1000 | Loss: 0.00001524
Iteration 159/1000 | Loss: 0.00001524
Iteration 160/1000 | Loss: 0.00001524
Iteration 161/1000 | Loss: 0.00001524
Iteration 162/1000 | Loss: 0.00001524
Iteration 163/1000 | Loss: 0.00001524
Iteration 164/1000 | Loss: 0.00001524
Iteration 165/1000 | Loss: 0.00001524
Iteration 166/1000 | Loss: 0.00001524
Iteration 167/1000 | Loss: 0.00001524
Iteration 168/1000 | Loss: 0.00001524
Iteration 169/1000 | Loss: 0.00001524
Iteration 170/1000 | Loss: 0.00001524
Iteration 171/1000 | Loss: 0.00001524
Iteration 172/1000 | Loss: 0.00001524
Iteration 173/1000 | Loss: 0.00001524
Iteration 174/1000 | Loss: 0.00001524
Iteration 175/1000 | Loss: 0.00001524
Iteration 176/1000 | Loss: 0.00001524
Iteration 177/1000 | Loss: 0.00001524
Iteration 178/1000 | Loss: 0.00001524
Iteration 179/1000 | Loss: 0.00001524
Iteration 180/1000 | Loss: 0.00001524
Iteration 181/1000 | Loss: 0.00001524
Iteration 182/1000 | Loss: 0.00001524
Iteration 183/1000 | Loss: 0.00001524
Iteration 184/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.5243843336065765e-05, 1.5243843336065765e-05, 1.5243843336065765e-05, 1.5243843336065765e-05, 1.5243843336065765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5243843336065765e-05

Optimization complete. Final v2v error: 3.374933958053589 mm

Highest mean error: 3.6442883014678955 mm for frame 74

Lowest mean error: 3.0250797271728516 mm for frame 181

Saving results

Total time: 34.502482414245605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039154
Iteration 2/25 | Loss: 0.00200153
Iteration 3/25 | Loss: 0.00119364
Iteration 4/25 | Loss: 0.00115430
Iteration 5/25 | Loss: 0.00114540
Iteration 6/25 | Loss: 0.00114303
Iteration 7/25 | Loss: 0.00114225
Iteration 8/25 | Loss: 0.00114212
Iteration 9/25 | Loss: 0.00114212
Iteration 10/25 | Loss: 0.00114212
Iteration 11/25 | Loss: 0.00114212
Iteration 12/25 | Loss: 0.00114212
Iteration 13/25 | Loss: 0.00114212
Iteration 14/25 | Loss: 0.00114212
Iteration 15/25 | Loss: 0.00114212
Iteration 16/25 | Loss: 0.00114212
Iteration 17/25 | Loss: 0.00114212
Iteration 18/25 | Loss: 0.00114212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011421175440773368, 0.0011421175440773368, 0.0011421175440773368, 0.0011421175440773368, 0.0011421175440773368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011421175440773368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60273182
Iteration 2/25 | Loss: 0.00057223
Iteration 3/25 | Loss: 0.00057223
Iteration 4/25 | Loss: 0.00057223
Iteration 5/25 | Loss: 0.00057223
Iteration 6/25 | Loss: 0.00057223
Iteration 7/25 | Loss: 0.00057223
Iteration 8/25 | Loss: 0.00057223
Iteration 9/25 | Loss: 0.00057223
Iteration 10/25 | Loss: 0.00057223
Iteration 11/25 | Loss: 0.00057223
Iteration 12/25 | Loss: 0.00057223
Iteration 13/25 | Loss: 0.00057223
Iteration 14/25 | Loss: 0.00057223
Iteration 15/25 | Loss: 0.00057223
Iteration 16/25 | Loss: 0.00057223
Iteration 17/25 | Loss: 0.00057223
Iteration 18/25 | Loss: 0.00057223
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005722299683839083, 0.0005722299683839083, 0.0005722299683839083, 0.0005722299683839083, 0.0005722299683839083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005722299683839083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057223
Iteration 2/1000 | Loss: 0.00007158
Iteration 3/1000 | Loss: 0.00005843
Iteration 4/1000 | Loss: 0.00005430
Iteration 5/1000 | Loss: 0.00005282
Iteration 6/1000 | Loss: 0.00005158
Iteration 7/1000 | Loss: 0.00005077
Iteration 8/1000 | Loss: 0.00004995
Iteration 9/1000 | Loss: 0.00004941
Iteration 10/1000 | Loss: 0.00004906
Iteration 11/1000 | Loss: 0.00004871
Iteration 12/1000 | Loss: 0.00004842
Iteration 13/1000 | Loss: 0.00004810
Iteration 14/1000 | Loss: 0.00004790
Iteration 15/1000 | Loss: 0.00004770
Iteration 16/1000 | Loss: 0.00004748
Iteration 17/1000 | Loss: 0.00004735
Iteration 18/1000 | Loss: 0.00004731
Iteration 19/1000 | Loss: 0.00004724
Iteration 20/1000 | Loss: 0.00004701
Iteration 21/1000 | Loss: 0.00004685
Iteration 22/1000 | Loss: 0.00004669
Iteration 23/1000 | Loss: 0.00004657
Iteration 24/1000 | Loss: 0.00004646
Iteration 25/1000 | Loss: 0.00004640
Iteration 26/1000 | Loss: 0.00004640
Iteration 27/1000 | Loss: 0.00004640
Iteration 28/1000 | Loss: 0.00004640
Iteration 29/1000 | Loss: 0.00004639
Iteration 30/1000 | Loss: 0.00004639
Iteration 31/1000 | Loss: 0.00004639
Iteration 32/1000 | Loss: 0.00004639
Iteration 33/1000 | Loss: 0.00004639
Iteration 34/1000 | Loss: 0.00004639
Iteration 35/1000 | Loss: 0.00004639
Iteration 36/1000 | Loss: 0.00004639
Iteration 37/1000 | Loss: 0.00004638
Iteration 38/1000 | Loss: 0.00004633
Iteration 39/1000 | Loss: 0.00004629
Iteration 40/1000 | Loss: 0.00004628
Iteration 41/1000 | Loss: 0.00004628
Iteration 42/1000 | Loss: 0.00004628
Iteration 43/1000 | Loss: 0.00004628
Iteration 44/1000 | Loss: 0.00004628
Iteration 45/1000 | Loss: 0.00004628
Iteration 46/1000 | Loss: 0.00004628
Iteration 47/1000 | Loss: 0.00004628
Iteration 48/1000 | Loss: 0.00004627
Iteration 49/1000 | Loss: 0.00004627
Iteration 50/1000 | Loss: 0.00004627
Iteration 51/1000 | Loss: 0.00004627
Iteration 52/1000 | Loss: 0.00004627
Iteration 53/1000 | Loss: 0.00004621
Iteration 54/1000 | Loss: 0.00004619
Iteration 55/1000 | Loss: 0.00004618
Iteration 56/1000 | Loss: 0.00004618
Iteration 57/1000 | Loss: 0.00004618
Iteration 58/1000 | Loss: 0.00004617
Iteration 59/1000 | Loss: 0.00004616
Iteration 60/1000 | Loss: 0.00004616
Iteration 61/1000 | Loss: 0.00004616
Iteration 62/1000 | Loss: 0.00004616
Iteration 63/1000 | Loss: 0.00004616
Iteration 64/1000 | Loss: 0.00004616
Iteration 65/1000 | Loss: 0.00004616
Iteration 66/1000 | Loss: 0.00004615
Iteration 67/1000 | Loss: 0.00004615
Iteration 68/1000 | Loss: 0.00004614
Iteration 69/1000 | Loss: 0.00004613
Iteration 70/1000 | Loss: 0.00004610
Iteration 71/1000 | Loss: 0.00004610
Iteration 72/1000 | Loss: 0.00004609
Iteration 73/1000 | Loss: 0.00004609
Iteration 74/1000 | Loss: 0.00004609
Iteration 75/1000 | Loss: 0.00004609
Iteration 76/1000 | Loss: 0.00004609
Iteration 77/1000 | Loss: 0.00004609
Iteration 78/1000 | Loss: 0.00004609
Iteration 79/1000 | Loss: 0.00004608
Iteration 80/1000 | Loss: 0.00004608
Iteration 81/1000 | Loss: 0.00004608
Iteration 82/1000 | Loss: 0.00004607
Iteration 83/1000 | Loss: 0.00004607
Iteration 84/1000 | Loss: 0.00004607
Iteration 85/1000 | Loss: 0.00004607
Iteration 86/1000 | Loss: 0.00004607
Iteration 87/1000 | Loss: 0.00004607
Iteration 88/1000 | Loss: 0.00004607
Iteration 89/1000 | Loss: 0.00004607
Iteration 90/1000 | Loss: 0.00004607
Iteration 91/1000 | Loss: 0.00004607
Iteration 92/1000 | Loss: 0.00004606
Iteration 93/1000 | Loss: 0.00004606
Iteration 94/1000 | Loss: 0.00004606
Iteration 95/1000 | Loss: 0.00004606
Iteration 96/1000 | Loss: 0.00004606
Iteration 97/1000 | Loss: 0.00004606
Iteration 98/1000 | Loss: 0.00004606
Iteration 99/1000 | Loss: 0.00004606
Iteration 100/1000 | Loss: 0.00004606
Iteration 101/1000 | Loss: 0.00004606
Iteration 102/1000 | Loss: 0.00004606
Iteration 103/1000 | Loss: 0.00004605
Iteration 104/1000 | Loss: 0.00004605
Iteration 105/1000 | Loss: 0.00004605
Iteration 106/1000 | Loss: 0.00004605
Iteration 107/1000 | Loss: 0.00004605
Iteration 108/1000 | Loss: 0.00004605
Iteration 109/1000 | Loss: 0.00004605
Iteration 110/1000 | Loss: 0.00004605
Iteration 111/1000 | Loss: 0.00004605
Iteration 112/1000 | Loss: 0.00004605
Iteration 113/1000 | Loss: 0.00004605
Iteration 114/1000 | Loss: 0.00004604
Iteration 115/1000 | Loss: 0.00004604
Iteration 116/1000 | Loss: 0.00004604
Iteration 117/1000 | Loss: 0.00004604
Iteration 118/1000 | Loss: 0.00004604
Iteration 119/1000 | Loss: 0.00004604
Iteration 120/1000 | Loss: 0.00004604
Iteration 121/1000 | Loss: 0.00004604
Iteration 122/1000 | Loss: 0.00004604
Iteration 123/1000 | Loss: 0.00004603
Iteration 124/1000 | Loss: 0.00004603
Iteration 125/1000 | Loss: 0.00004603
Iteration 126/1000 | Loss: 0.00004603
Iteration 127/1000 | Loss: 0.00004603
Iteration 128/1000 | Loss: 0.00004603
Iteration 129/1000 | Loss: 0.00004603
Iteration 130/1000 | Loss: 0.00004603
Iteration 131/1000 | Loss: 0.00004603
Iteration 132/1000 | Loss: 0.00004602
Iteration 133/1000 | Loss: 0.00004602
Iteration 134/1000 | Loss: 0.00004602
Iteration 135/1000 | Loss: 0.00004602
Iteration 136/1000 | Loss: 0.00004602
Iteration 137/1000 | Loss: 0.00004602
Iteration 138/1000 | Loss: 0.00004602
Iteration 139/1000 | Loss: 0.00004602
Iteration 140/1000 | Loss: 0.00004602
Iteration 141/1000 | Loss: 0.00004602
Iteration 142/1000 | Loss: 0.00004602
Iteration 143/1000 | Loss: 0.00004602
Iteration 144/1000 | Loss: 0.00004601
Iteration 145/1000 | Loss: 0.00004601
Iteration 146/1000 | Loss: 0.00004601
Iteration 147/1000 | Loss: 0.00004601
Iteration 148/1000 | Loss: 0.00004601
Iteration 149/1000 | Loss: 0.00004601
Iteration 150/1000 | Loss: 0.00004601
Iteration 151/1000 | Loss: 0.00004601
Iteration 152/1000 | Loss: 0.00004601
Iteration 153/1000 | Loss: 0.00004601
Iteration 154/1000 | Loss: 0.00004601
Iteration 155/1000 | Loss: 0.00004601
Iteration 156/1000 | Loss: 0.00004601
Iteration 157/1000 | Loss: 0.00004600
Iteration 158/1000 | Loss: 0.00004600
Iteration 159/1000 | Loss: 0.00004600
Iteration 160/1000 | Loss: 0.00004600
Iteration 161/1000 | Loss: 0.00004600
Iteration 162/1000 | Loss: 0.00004600
Iteration 163/1000 | Loss: 0.00004600
Iteration 164/1000 | Loss: 0.00004600
Iteration 165/1000 | Loss: 0.00004600
Iteration 166/1000 | Loss: 0.00004600
Iteration 167/1000 | Loss: 0.00004600
Iteration 168/1000 | Loss: 0.00004600
Iteration 169/1000 | Loss: 0.00004600
Iteration 170/1000 | Loss: 0.00004600
Iteration 171/1000 | Loss: 0.00004600
Iteration 172/1000 | Loss: 0.00004600
Iteration 173/1000 | Loss: 0.00004600
Iteration 174/1000 | Loss: 0.00004600
Iteration 175/1000 | Loss: 0.00004600
Iteration 176/1000 | Loss: 0.00004600
Iteration 177/1000 | Loss: 0.00004600
Iteration 178/1000 | Loss: 0.00004600
Iteration 179/1000 | Loss: 0.00004600
Iteration 180/1000 | Loss: 0.00004600
Iteration 181/1000 | Loss: 0.00004600
Iteration 182/1000 | Loss: 0.00004600
Iteration 183/1000 | Loss: 0.00004600
Iteration 184/1000 | Loss: 0.00004600
Iteration 185/1000 | Loss: 0.00004600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [4.599689054884948e-05, 4.599689054884948e-05, 4.599689054884948e-05, 4.599689054884948e-05, 4.599689054884948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.599689054884948e-05

Optimization complete. Final v2v error: 5.571662902832031 mm

Highest mean error: 6.351142883300781 mm for frame 101

Lowest mean error: 5.10795259475708 mm for frame 87

Saving results

Total time: 55.985435009002686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565676
Iteration 2/25 | Loss: 0.00140672
Iteration 3/25 | Loss: 0.00114049
Iteration 4/25 | Loss: 0.00111039
Iteration 5/25 | Loss: 0.00109563
Iteration 6/25 | Loss: 0.00109302
Iteration 7/25 | Loss: 0.00109286
Iteration 8/25 | Loss: 0.00109286
Iteration 9/25 | Loss: 0.00109286
Iteration 10/25 | Loss: 0.00109286
Iteration 11/25 | Loss: 0.00109286
Iteration 12/25 | Loss: 0.00109286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010928558185696602, 0.0010928558185696602, 0.0010928558185696602, 0.0010928558185696602, 0.0010928558185696602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010928558185696602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73546231
Iteration 2/25 | Loss: 0.00077178
Iteration 3/25 | Loss: 0.00077178
Iteration 4/25 | Loss: 0.00077178
Iteration 5/25 | Loss: 0.00077178
Iteration 6/25 | Loss: 0.00077178
Iteration 7/25 | Loss: 0.00077178
Iteration 8/25 | Loss: 0.00077178
Iteration 9/25 | Loss: 0.00077178
Iteration 10/25 | Loss: 0.00077178
Iteration 11/25 | Loss: 0.00077178
Iteration 12/25 | Loss: 0.00077178
Iteration 13/25 | Loss: 0.00077178
Iteration 14/25 | Loss: 0.00077178
Iteration 15/25 | Loss: 0.00077178
Iteration 16/25 | Loss: 0.00077178
Iteration 17/25 | Loss: 0.00077178
Iteration 18/25 | Loss: 0.00077178
Iteration 19/25 | Loss: 0.00077178
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007717761327512562, 0.0007717761327512562, 0.0007717761327512562, 0.0007717761327512562, 0.0007717761327512562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007717761327512562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077178
Iteration 2/1000 | Loss: 0.00007325
Iteration 3/1000 | Loss: 0.00004943
Iteration 4/1000 | Loss: 0.00004533
Iteration 5/1000 | Loss: 0.00004350
Iteration 6/1000 | Loss: 0.00004218
Iteration 7/1000 | Loss: 0.00004117
Iteration 8/1000 | Loss: 0.00004032
Iteration 9/1000 | Loss: 0.00003979
Iteration 10/1000 | Loss: 0.00003948
Iteration 11/1000 | Loss: 0.00003922
Iteration 12/1000 | Loss: 0.00003894
Iteration 13/1000 | Loss: 0.00003868
Iteration 14/1000 | Loss: 0.00003857
Iteration 15/1000 | Loss: 0.00003857
Iteration 16/1000 | Loss: 0.00003856
Iteration 17/1000 | Loss: 0.00003848
Iteration 18/1000 | Loss: 0.00003835
Iteration 19/1000 | Loss: 0.00003833
Iteration 20/1000 | Loss: 0.00003831
Iteration 21/1000 | Loss: 0.00003831
Iteration 22/1000 | Loss: 0.00003830
Iteration 23/1000 | Loss: 0.00003830
Iteration 24/1000 | Loss: 0.00003829
Iteration 25/1000 | Loss: 0.00003825
Iteration 26/1000 | Loss: 0.00003823
Iteration 27/1000 | Loss: 0.00003822
Iteration 28/1000 | Loss: 0.00003822
Iteration 29/1000 | Loss: 0.00003821
Iteration 30/1000 | Loss: 0.00003821
Iteration 31/1000 | Loss: 0.00003820
Iteration 32/1000 | Loss: 0.00003820
Iteration 33/1000 | Loss: 0.00003817
Iteration 34/1000 | Loss: 0.00003816
Iteration 35/1000 | Loss: 0.00003816
Iteration 36/1000 | Loss: 0.00003816
Iteration 37/1000 | Loss: 0.00003809
Iteration 38/1000 | Loss: 0.00003805
Iteration 39/1000 | Loss: 0.00003802
Iteration 40/1000 | Loss: 0.00003802
Iteration 41/1000 | Loss: 0.00003802
Iteration 42/1000 | Loss: 0.00003802
Iteration 43/1000 | Loss: 0.00003802
Iteration 44/1000 | Loss: 0.00003802
Iteration 45/1000 | Loss: 0.00003802
Iteration 46/1000 | Loss: 0.00003801
Iteration 47/1000 | Loss: 0.00003801
Iteration 48/1000 | Loss: 0.00003801
Iteration 49/1000 | Loss: 0.00003801
Iteration 50/1000 | Loss: 0.00003801
Iteration 51/1000 | Loss: 0.00003801
Iteration 52/1000 | Loss: 0.00003801
Iteration 53/1000 | Loss: 0.00003801
Iteration 54/1000 | Loss: 0.00003801
Iteration 55/1000 | Loss: 0.00003801
Iteration 56/1000 | Loss: 0.00003801
Iteration 57/1000 | Loss: 0.00003800
Iteration 58/1000 | Loss: 0.00003799
Iteration 59/1000 | Loss: 0.00003799
Iteration 60/1000 | Loss: 0.00003799
Iteration 61/1000 | Loss: 0.00003798
Iteration 62/1000 | Loss: 0.00003797
Iteration 63/1000 | Loss: 0.00003797
Iteration 64/1000 | Loss: 0.00003797
Iteration 65/1000 | Loss: 0.00003796
Iteration 66/1000 | Loss: 0.00003796
Iteration 67/1000 | Loss: 0.00003796
Iteration 68/1000 | Loss: 0.00003796
Iteration 69/1000 | Loss: 0.00003796
Iteration 70/1000 | Loss: 0.00003796
Iteration 71/1000 | Loss: 0.00003795
Iteration 72/1000 | Loss: 0.00003795
Iteration 73/1000 | Loss: 0.00003795
Iteration 74/1000 | Loss: 0.00003794
Iteration 75/1000 | Loss: 0.00003794
Iteration 76/1000 | Loss: 0.00003792
Iteration 77/1000 | Loss: 0.00003792
Iteration 78/1000 | Loss: 0.00003792
Iteration 79/1000 | Loss: 0.00003792
Iteration 80/1000 | Loss: 0.00003792
Iteration 81/1000 | Loss: 0.00003792
Iteration 82/1000 | Loss: 0.00003792
Iteration 83/1000 | Loss: 0.00003792
Iteration 84/1000 | Loss: 0.00003791
Iteration 85/1000 | Loss: 0.00003791
Iteration 86/1000 | Loss: 0.00003791
Iteration 87/1000 | Loss: 0.00003791
Iteration 88/1000 | Loss: 0.00003791
Iteration 89/1000 | Loss: 0.00003791
Iteration 90/1000 | Loss: 0.00003790
Iteration 91/1000 | Loss: 0.00003786
Iteration 92/1000 | Loss: 0.00003786
Iteration 93/1000 | Loss: 0.00003786
Iteration 94/1000 | Loss: 0.00003785
Iteration 95/1000 | Loss: 0.00003785
Iteration 96/1000 | Loss: 0.00003785
Iteration 97/1000 | Loss: 0.00003781
Iteration 98/1000 | Loss: 0.00003781
Iteration 99/1000 | Loss: 0.00003781
Iteration 100/1000 | Loss: 0.00003781
Iteration 101/1000 | Loss: 0.00003781
Iteration 102/1000 | Loss: 0.00003781
Iteration 103/1000 | Loss: 0.00003781
Iteration 104/1000 | Loss: 0.00003781
Iteration 105/1000 | Loss: 0.00003780
Iteration 106/1000 | Loss: 0.00003780
Iteration 107/1000 | Loss: 0.00003780
Iteration 108/1000 | Loss: 0.00003780
Iteration 109/1000 | Loss: 0.00003779
Iteration 110/1000 | Loss: 0.00003779
Iteration 111/1000 | Loss: 0.00003779
Iteration 112/1000 | Loss: 0.00003778
Iteration 113/1000 | Loss: 0.00003778
Iteration 114/1000 | Loss: 0.00003778
Iteration 115/1000 | Loss: 0.00003775
Iteration 116/1000 | Loss: 0.00003774
Iteration 117/1000 | Loss: 0.00003774
Iteration 118/1000 | Loss: 0.00003771
Iteration 119/1000 | Loss: 0.00003770
Iteration 120/1000 | Loss: 0.00003769
Iteration 121/1000 | Loss: 0.00003769
Iteration 122/1000 | Loss: 0.00003768
Iteration 123/1000 | Loss: 0.00003768
Iteration 124/1000 | Loss: 0.00003768
Iteration 125/1000 | Loss: 0.00003767
Iteration 126/1000 | Loss: 0.00003767
Iteration 127/1000 | Loss: 0.00003767
Iteration 128/1000 | Loss: 0.00003767
Iteration 129/1000 | Loss: 0.00003766
Iteration 130/1000 | Loss: 0.00003766
Iteration 131/1000 | Loss: 0.00003766
Iteration 132/1000 | Loss: 0.00003766
Iteration 133/1000 | Loss: 0.00003766
Iteration 134/1000 | Loss: 0.00003766
Iteration 135/1000 | Loss: 0.00003766
Iteration 136/1000 | Loss: 0.00003766
Iteration 137/1000 | Loss: 0.00003766
Iteration 138/1000 | Loss: 0.00003766
Iteration 139/1000 | Loss: 0.00003766
Iteration 140/1000 | Loss: 0.00003765
Iteration 141/1000 | Loss: 0.00003765
Iteration 142/1000 | Loss: 0.00003765
Iteration 143/1000 | Loss: 0.00003765
Iteration 144/1000 | Loss: 0.00003765
Iteration 145/1000 | Loss: 0.00003765
Iteration 146/1000 | Loss: 0.00003764
Iteration 147/1000 | Loss: 0.00003764
Iteration 148/1000 | Loss: 0.00003764
Iteration 149/1000 | Loss: 0.00003763
Iteration 150/1000 | Loss: 0.00003763
Iteration 151/1000 | Loss: 0.00003763
Iteration 152/1000 | Loss: 0.00003763
Iteration 153/1000 | Loss: 0.00003763
Iteration 154/1000 | Loss: 0.00003762
Iteration 155/1000 | Loss: 0.00003762
Iteration 156/1000 | Loss: 0.00003762
Iteration 157/1000 | Loss: 0.00003762
Iteration 158/1000 | Loss: 0.00003762
Iteration 159/1000 | Loss: 0.00003762
Iteration 160/1000 | Loss: 0.00003762
Iteration 161/1000 | Loss: 0.00003762
Iteration 162/1000 | Loss: 0.00003761
Iteration 163/1000 | Loss: 0.00003761
Iteration 164/1000 | Loss: 0.00003761
Iteration 165/1000 | Loss: 0.00003760
Iteration 166/1000 | Loss: 0.00003760
Iteration 167/1000 | Loss: 0.00003760
Iteration 168/1000 | Loss: 0.00003760
Iteration 169/1000 | Loss: 0.00003760
Iteration 170/1000 | Loss: 0.00003760
Iteration 171/1000 | Loss: 0.00003760
Iteration 172/1000 | Loss: 0.00003760
Iteration 173/1000 | Loss: 0.00003760
Iteration 174/1000 | Loss: 0.00003760
Iteration 175/1000 | Loss: 0.00003760
Iteration 176/1000 | Loss: 0.00003760
Iteration 177/1000 | Loss: 0.00003760
Iteration 178/1000 | Loss: 0.00003760
Iteration 179/1000 | Loss: 0.00003760
Iteration 180/1000 | Loss: 0.00003760
Iteration 181/1000 | Loss: 0.00003760
Iteration 182/1000 | Loss: 0.00003760
Iteration 183/1000 | Loss: 0.00003760
Iteration 184/1000 | Loss: 0.00003760
Iteration 185/1000 | Loss: 0.00003760
Iteration 186/1000 | Loss: 0.00003760
Iteration 187/1000 | Loss: 0.00003760
Iteration 188/1000 | Loss: 0.00003759
Iteration 189/1000 | Loss: 0.00003759
Iteration 190/1000 | Loss: 0.00003759
Iteration 191/1000 | Loss: 0.00003759
Iteration 192/1000 | Loss: 0.00003759
Iteration 193/1000 | Loss: 0.00003759
Iteration 194/1000 | Loss: 0.00003759
Iteration 195/1000 | Loss: 0.00003759
Iteration 196/1000 | Loss: 0.00003759
Iteration 197/1000 | Loss: 0.00003758
Iteration 198/1000 | Loss: 0.00003758
Iteration 199/1000 | Loss: 0.00003758
Iteration 200/1000 | Loss: 0.00003758
Iteration 201/1000 | Loss: 0.00003758
Iteration 202/1000 | Loss: 0.00003758
Iteration 203/1000 | Loss: 0.00003758
Iteration 204/1000 | Loss: 0.00003758
Iteration 205/1000 | Loss: 0.00003758
Iteration 206/1000 | Loss: 0.00003758
Iteration 207/1000 | Loss: 0.00003758
Iteration 208/1000 | Loss: 0.00003758
Iteration 209/1000 | Loss: 0.00003758
Iteration 210/1000 | Loss: 0.00003758
Iteration 211/1000 | Loss: 0.00003758
Iteration 212/1000 | Loss: 0.00003758
Iteration 213/1000 | Loss: 0.00003758
Iteration 214/1000 | Loss: 0.00003758
Iteration 215/1000 | Loss: 0.00003758
Iteration 216/1000 | Loss: 0.00003758
Iteration 217/1000 | Loss: 0.00003758
Iteration 218/1000 | Loss: 0.00003758
Iteration 219/1000 | Loss: 0.00003758
Iteration 220/1000 | Loss: 0.00003758
Iteration 221/1000 | Loss: 0.00003758
Iteration 222/1000 | Loss: 0.00003758
Iteration 223/1000 | Loss: 0.00003758
Iteration 224/1000 | Loss: 0.00003758
Iteration 225/1000 | Loss: 0.00003758
Iteration 226/1000 | Loss: 0.00003758
Iteration 227/1000 | Loss: 0.00003758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [3.7576053728116676e-05, 3.7576053728116676e-05, 3.7576053728116676e-05, 3.7576053728116676e-05, 3.7576053728116676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7576053728116676e-05

Optimization complete. Final v2v error: 5.278548240661621 mm

Highest mean error: 5.583422660827637 mm for frame 124

Lowest mean error: 4.993614673614502 mm for frame 224

Saving results

Total time: 59.369199991226196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01155018
Iteration 2/25 | Loss: 0.00364141
Iteration 3/25 | Loss: 0.00210087
Iteration 4/25 | Loss: 0.00189922
Iteration 5/25 | Loss: 0.00243051
Iteration 6/25 | Loss: 0.00160684
Iteration 7/25 | Loss: 0.00134838
Iteration 8/25 | Loss: 0.00114638
Iteration 9/25 | Loss: 0.00125427
Iteration 10/25 | Loss: 0.00103490
Iteration 11/25 | Loss: 0.00096872
Iteration 12/25 | Loss: 0.00093044
Iteration 13/25 | Loss: 0.00092176
Iteration 14/25 | Loss: 0.00093879
Iteration 15/25 | Loss: 0.00093356
Iteration 16/25 | Loss: 0.00093750
Iteration 17/25 | Loss: 0.00093853
Iteration 18/25 | Loss: 0.00093614
Iteration 19/25 | Loss: 0.00091456
Iteration 20/25 | Loss: 0.00090159
Iteration 21/25 | Loss: 0.00089872
Iteration 22/25 | Loss: 0.00090186
Iteration 23/25 | Loss: 0.00089885
Iteration 24/25 | Loss: 0.00089844
Iteration 25/25 | Loss: 0.00090004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53287113
Iteration 2/25 | Loss: 0.00084614
Iteration 3/25 | Loss: 0.00034533
Iteration 4/25 | Loss: 0.00034533
Iteration 5/25 | Loss: 0.00034532
Iteration 6/25 | Loss: 0.00034532
Iteration 7/25 | Loss: 0.00034532
Iteration 8/25 | Loss: 0.00036376
Iteration 9/25 | Loss: 0.00037127
Iteration 10/25 | Loss: 0.00037127
Iteration 11/25 | Loss: 0.00037127
Iteration 12/25 | Loss: 0.00037127
Iteration 13/25 | Loss: 0.00037127
Iteration 14/25 | Loss: 0.00037127
Iteration 15/25 | Loss: 0.00037127
Iteration 16/25 | Loss: 0.00037127
Iteration 17/25 | Loss: 0.00037127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003712728212121874, 0.0003712728212121874, 0.0003712728212121874, 0.0003712728212121874, 0.0003712728212121874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003712728212121874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037127
Iteration 2/1000 | Loss: 0.00056744
Iteration 3/1000 | Loss: 0.00013619
Iteration 4/1000 | Loss: 0.00008974
Iteration 5/1000 | Loss: 0.00019736
Iteration 6/1000 | Loss: 0.00014817
Iteration 7/1000 | Loss: 0.00006029
Iteration 8/1000 | Loss: 0.00005606
Iteration 9/1000 | Loss: 0.00013090
Iteration 10/1000 | Loss: 0.00011404
Iteration 11/1000 | Loss: 0.00013652
Iteration 12/1000 | Loss: 0.00005399
Iteration 13/1000 | Loss: 0.00007158
Iteration 14/1000 | Loss: 0.00009911
Iteration 15/1000 | Loss: 0.00006498
Iteration 16/1000 | Loss: 0.00005956
Iteration 17/1000 | Loss: 0.00007617
Iteration 18/1000 | Loss: 0.00169056
Iteration 19/1000 | Loss: 0.00144277
Iteration 20/1000 | Loss: 0.00009739
Iteration 21/1000 | Loss: 0.00006397
Iteration 22/1000 | Loss: 0.00270494
Iteration 23/1000 | Loss: 0.00166870
Iteration 24/1000 | Loss: 0.00006220
Iteration 25/1000 | Loss: 0.00082729
Iteration 26/1000 | Loss: 0.00109019
Iteration 27/1000 | Loss: 0.00189207
Iteration 28/1000 | Loss: 0.00166586
Iteration 29/1000 | Loss: 0.00034494
Iteration 30/1000 | Loss: 0.00015688
Iteration 31/1000 | Loss: 0.00009388
Iteration 32/1000 | Loss: 0.00012679
Iteration 33/1000 | Loss: 0.00009285
Iteration 34/1000 | Loss: 0.00009767
Iteration 35/1000 | Loss: 0.00006507
Iteration 36/1000 | Loss: 0.00008123
Iteration 37/1000 | Loss: 0.00005567
Iteration 38/1000 | Loss: 0.00006906
Iteration 39/1000 | Loss: 0.00008702
Iteration 40/1000 | Loss: 0.00262882
Iteration 41/1000 | Loss: 0.00150195
Iteration 42/1000 | Loss: 0.00203213
Iteration 43/1000 | Loss: 0.00044148
Iteration 44/1000 | Loss: 0.00045681
Iteration 45/1000 | Loss: 0.00166119
Iteration 46/1000 | Loss: 0.00138338
Iteration 47/1000 | Loss: 0.00152670
Iteration 48/1000 | Loss: 0.00111679
Iteration 49/1000 | Loss: 0.00119718
Iteration 50/1000 | Loss: 0.00102964
Iteration 51/1000 | Loss: 0.00112868
Iteration 52/1000 | Loss: 0.00078241
Iteration 53/1000 | Loss: 0.00112909
Iteration 54/1000 | Loss: 0.00046627
Iteration 55/1000 | Loss: 0.00033360
Iteration 56/1000 | Loss: 0.00039701
Iteration 57/1000 | Loss: 0.00032475
Iteration 58/1000 | Loss: 0.00048731
Iteration 59/1000 | Loss: 0.00028516
Iteration 60/1000 | Loss: 0.00010892
Iteration 61/1000 | Loss: 0.00008362
Iteration 62/1000 | Loss: 0.00006577
Iteration 63/1000 | Loss: 0.00005995
Iteration 64/1000 | Loss: 0.00006799
Iteration 65/1000 | Loss: 0.00006078
Iteration 66/1000 | Loss: 0.00006896
Iteration 67/1000 | Loss: 0.00006239
Iteration 68/1000 | Loss: 0.00007967
Iteration 69/1000 | Loss: 0.00005769
Iteration 70/1000 | Loss: 0.00005755
Iteration 71/1000 | Loss: 0.00004964
Iteration 72/1000 | Loss: 0.00007239
Iteration 73/1000 | Loss: 0.00010557
Iteration 74/1000 | Loss: 0.00031436
Iteration 75/1000 | Loss: 0.00080682
Iteration 76/1000 | Loss: 0.00020418
Iteration 77/1000 | Loss: 0.00045978
Iteration 78/1000 | Loss: 0.00006319
Iteration 79/1000 | Loss: 0.00006585
Iteration 80/1000 | Loss: 0.00005428
Iteration 81/1000 | Loss: 0.00005136
Iteration 82/1000 | Loss: 0.00004943
Iteration 83/1000 | Loss: 0.00006336
Iteration 84/1000 | Loss: 0.00008198
Iteration 85/1000 | Loss: 0.00005866
Iteration 86/1000 | Loss: 0.00018978
Iteration 87/1000 | Loss: 0.00006344
Iteration 88/1000 | Loss: 0.00006121
Iteration 89/1000 | Loss: 0.00005197
Iteration 90/1000 | Loss: 0.00005904
Iteration 91/1000 | Loss: 0.00005710
Iteration 92/1000 | Loss: 0.00020195
Iteration 93/1000 | Loss: 0.00008568
Iteration 94/1000 | Loss: 0.00006361
Iteration 95/1000 | Loss: 0.00004831
Iteration 96/1000 | Loss: 0.00005196
Iteration 97/1000 | Loss: 0.00007279
Iteration 98/1000 | Loss: 0.00005033
Iteration 99/1000 | Loss: 0.00005424
Iteration 100/1000 | Loss: 0.00005039
Iteration 101/1000 | Loss: 0.00005134
Iteration 102/1000 | Loss: 0.00004868
Iteration 103/1000 | Loss: 0.00004868
Iteration 104/1000 | Loss: 0.00005062
Iteration 105/1000 | Loss: 0.00005393
Iteration 106/1000 | Loss: 0.00005050
Iteration 107/1000 | Loss: 0.00004851
Iteration 108/1000 | Loss: 0.00004852
Iteration 109/1000 | Loss: 0.00005558
Iteration 110/1000 | Loss: 0.00004990
Iteration 111/1000 | Loss: 0.00004972
Iteration 112/1000 | Loss: 0.00004834
Iteration 113/1000 | Loss: 0.00004848
Iteration 114/1000 | Loss: 0.00004895
Iteration 115/1000 | Loss: 0.00005235
Iteration 116/1000 | Loss: 0.00005800
Iteration 117/1000 | Loss: 0.00004889
Iteration 118/1000 | Loss: 0.00005094
Iteration 119/1000 | Loss: 0.00005828
Iteration 120/1000 | Loss: 0.00005063
Iteration 121/1000 | Loss: 0.00005118
Iteration 122/1000 | Loss: 0.00005004
Iteration 123/1000 | Loss: 0.00004947
Iteration 124/1000 | Loss: 0.00005033
Iteration 125/1000 | Loss: 0.00004970
Iteration 126/1000 | Loss: 0.00005077
Iteration 127/1000 | Loss: 0.00005105
Iteration 128/1000 | Loss: 0.00004964
Iteration 129/1000 | Loss: 0.00004963
Iteration 130/1000 | Loss: 0.00004963
Iteration 131/1000 | Loss: 0.00008052
Iteration 132/1000 | Loss: 0.00005260
Iteration 133/1000 | Loss: 0.00005343
Iteration 134/1000 | Loss: 0.00004980
Iteration 135/1000 | Loss: 0.00004967
Iteration 136/1000 | Loss: 0.00004916
Iteration 137/1000 | Loss: 0.00004896
Iteration 138/1000 | Loss: 0.00004955
Iteration 139/1000 | Loss: 0.00005272
Iteration 140/1000 | Loss: 0.00005350
Iteration 141/1000 | Loss: 0.00004897
Iteration 142/1000 | Loss: 0.00004895
Iteration 143/1000 | Loss: 0.00006224
Iteration 144/1000 | Loss: 0.00004911
Iteration 145/1000 | Loss: 0.00004886
Iteration 146/1000 | Loss: 0.00004894
Iteration 147/1000 | Loss: 0.00005025
Iteration 148/1000 | Loss: 0.00005010
Iteration 149/1000 | Loss: 0.00005063
Iteration 150/1000 | Loss: 0.00005086
Iteration 151/1000 | Loss: 0.00004971
Iteration 152/1000 | Loss: 0.00005219
Iteration 153/1000 | Loss: 0.00005180
Iteration 154/1000 | Loss: 0.00004970
Iteration 155/1000 | Loss: 0.00006103
Iteration 156/1000 | Loss: 0.00004905
Iteration 157/1000 | Loss: 0.00004955
Iteration 158/1000 | Loss: 0.00004880
Iteration 159/1000 | Loss: 0.00004867
Iteration 160/1000 | Loss: 0.00004911
Iteration 161/1000 | Loss: 0.00004869
Iteration 162/1000 | Loss: 0.00004893
Iteration 163/1000 | Loss: 0.00004893
Iteration 164/1000 | Loss: 0.00005124
Iteration 165/1000 | Loss: 0.00005002
Iteration 166/1000 | Loss: 0.00005152
Iteration 167/1000 | Loss: 0.00004943
Iteration 168/1000 | Loss: 0.00004854
Iteration 169/1000 | Loss: 0.00004910
Iteration 170/1000 | Loss: 0.00007767
Iteration 171/1000 | Loss: 0.00012679
Iteration 172/1000 | Loss: 0.00005235
Iteration 173/1000 | Loss: 0.00005089
Iteration 174/1000 | Loss: 0.00004905
Iteration 175/1000 | Loss: 0.00004875
Iteration 176/1000 | Loss: 0.00004874
Iteration 177/1000 | Loss: 0.00004874
Iteration 178/1000 | Loss: 0.00005263
Iteration 179/1000 | Loss: 0.00004932
Iteration 180/1000 | Loss: 0.00004948
Iteration 181/1000 | Loss: 0.00005158
Iteration 182/1000 | Loss: 0.00004849
Iteration 183/1000 | Loss: 0.00005572
Iteration 184/1000 | Loss: 0.00004889
Iteration 185/1000 | Loss: 0.00004893
Iteration 186/1000 | Loss: 0.00004827
Iteration 187/1000 | Loss: 0.00004908
Iteration 188/1000 | Loss: 0.00004857
Iteration 189/1000 | Loss: 0.00004860
Iteration 190/1000 | Loss: 0.00004989
Iteration 191/1000 | Loss: 0.00004817
Iteration 192/1000 | Loss: 0.00004817
Iteration 193/1000 | Loss: 0.00005412
Iteration 194/1000 | Loss: 0.00004874
Iteration 195/1000 | Loss: 0.00004896
Iteration 196/1000 | Loss: 0.00004920
Iteration 197/1000 | Loss: 0.00005215
Iteration 198/1000 | Loss: 0.00006480
Iteration 199/1000 | Loss: 0.00005296
Iteration 200/1000 | Loss: 0.00005296
Iteration 201/1000 | Loss: 0.00004837
Iteration 202/1000 | Loss: 0.00005136
Iteration 203/1000 | Loss: 0.00005680
Iteration 204/1000 | Loss: 0.00005568
Iteration 205/1000 | Loss: 0.00005058
Iteration 206/1000 | Loss: 0.00004959
Iteration 207/1000 | Loss: 0.00004892
Iteration 208/1000 | Loss: 0.00004847
Iteration 209/1000 | Loss: 0.00005018
Iteration 210/1000 | Loss: 0.00004858
Iteration 211/1000 | Loss: 0.00004820
Iteration 212/1000 | Loss: 0.00004808
Iteration 213/1000 | Loss: 0.00004808
Iteration 214/1000 | Loss: 0.00004808
Iteration 215/1000 | Loss: 0.00004808
Iteration 216/1000 | Loss: 0.00004808
Iteration 217/1000 | Loss: 0.00004919
Iteration 218/1000 | Loss: 0.00005023
Iteration 219/1000 | Loss: 0.00005232
Iteration 220/1000 | Loss: 0.00008139
Iteration 221/1000 | Loss: 0.00005123
Iteration 222/1000 | Loss: 0.00005473
Iteration 223/1000 | Loss: 0.00004957
Iteration 224/1000 | Loss: 0.00005038
Iteration 225/1000 | Loss: 0.00004882
Iteration 226/1000 | Loss: 0.00005220
Iteration 227/1000 | Loss: 0.00004867
Iteration 228/1000 | Loss: 0.00004812
Iteration 229/1000 | Loss: 0.00004811
Iteration 230/1000 | Loss: 0.00004811
Iteration 231/1000 | Loss: 0.00004811
Iteration 232/1000 | Loss: 0.00004811
Iteration 233/1000 | Loss: 0.00004811
Iteration 234/1000 | Loss: 0.00004811
Iteration 235/1000 | Loss: 0.00004811
Iteration 236/1000 | Loss: 0.00004811
Iteration 237/1000 | Loss: 0.00004811
Iteration 238/1000 | Loss: 0.00004811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [4.810967220691964e-05, 4.810967220691964e-05, 4.810967220691964e-05, 4.810967220691964e-05, 4.810967220691964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.810967220691964e-05

Optimization complete. Final v2v error: 4.581066131591797 mm

Highest mean error: 19.708335876464844 mm for frame 99

Lowest mean error: 3.9518518447875977 mm for frame 12

Saving results

Total time: 354.73481130599976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426108
Iteration 2/25 | Loss: 0.00111066
Iteration 3/25 | Loss: 0.00098850
Iteration 4/25 | Loss: 0.00097439
Iteration 5/25 | Loss: 0.00096482
Iteration 6/25 | Loss: 0.00096291
Iteration 7/25 | Loss: 0.00096289
Iteration 8/25 | Loss: 0.00096289
Iteration 9/25 | Loss: 0.00096289
Iteration 10/25 | Loss: 0.00096289
Iteration 11/25 | Loss: 0.00096289
Iteration 12/25 | Loss: 0.00096289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009628870757296681, 0.0009628870757296681, 0.0009628870757296681, 0.0009628870757296681, 0.0009628870757296681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009628870757296681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72337389
Iteration 2/25 | Loss: 0.00045631
Iteration 3/25 | Loss: 0.00045627
Iteration 4/25 | Loss: 0.00045627
Iteration 5/25 | Loss: 0.00045627
Iteration 6/25 | Loss: 0.00045627
Iteration 7/25 | Loss: 0.00045627
Iteration 8/25 | Loss: 0.00045627
Iteration 9/25 | Loss: 0.00045627
Iteration 10/25 | Loss: 0.00045627
Iteration 11/25 | Loss: 0.00045627
Iteration 12/25 | Loss: 0.00045627
Iteration 13/25 | Loss: 0.00045627
Iteration 14/25 | Loss: 0.00045627
Iteration 15/25 | Loss: 0.00045627
Iteration 16/25 | Loss: 0.00045627
Iteration 17/25 | Loss: 0.00045627
Iteration 18/25 | Loss: 0.00045627
Iteration 19/25 | Loss: 0.00045627
Iteration 20/25 | Loss: 0.00045627
Iteration 21/25 | Loss: 0.00045627
Iteration 22/25 | Loss: 0.00045627
Iteration 23/25 | Loss: 0.00045627
Iteration 24/25 | Loss: 0.00045627
Iteration 25/25 | Loss: 0.00045627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045627
Iteration 2/1000 | Loss: 0.00003191
Iteration 3/1000 | Loss: 0.00002336
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001941
Iteration 7/1000 | Loss: 0.00001912
Iteration 8/1000 | Loss: 0.00001891
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001861
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001850
Iteration 14/1000 | Loss: 0.00001843
Iteration 15/1000 | Loss: 0.00001842
Iteration 16/1000 | Loss: 0.00001840
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001839
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00001838
Iteration 21/1000 | Loss: 0.00001838
Iteration 22/1000 | Loss: 0.00001838
Iteration 23/1000 | Loss: 0.00001838
Iteration 24/1000 | Loss: 0.00001837
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001836
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001835
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001834
Iteration 32/1000 | Loss: 0.00001834
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001833
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001833
Iteration 37/1000 | Loss: 0.00001833
Iteration 38/1000 | Loss: 0.00001833
Iteration 39/1000 | Loss: 0.00001833
Iteration 40/1000 | Loss: 0.00001833
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001832
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001832
Iteration 45/1000 | Loss: 0.00001832
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001831
Iteration 49/1000 | Loss: 0.00001831
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001831
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001830
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001829
Iteration 59/1000 | Loss: 0.00001829
Iteration 60/1000 | Loss: 0.00001829
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001829
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001828
Iteration 75/1000 | Loss: 0.00001828
Iteration 76/1000 | Loss: 0.00001828
Iteration 77/1000 | Loss: 0.00001828
Iteration 78/1000 | Loss: 0.00001828
Iteration 79/1000 | Loss: 0.00001828
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001828
Iteration 92/1000 | Loss: 0.00001828
Iteration 93/1000 | Loss: 0.00001828
Iteration 94/1000 | Loss: 0.00001828
Iteration 95/1000 | Loss: 0.00001828
Iteration 96/1000 | Loss: 0.00001828
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001828
Iteration 114/1000 | Loss: 0.00001828
Iteration 115/1000 | Loss: 0.00001828
Iteration 116/1000 | Loss: 0.00001828
Iteration 117/1000 | Loss: 0.00001828
Iteration 118/1000 | Loss: 0.00001828
Iteration 119/1000 | Loss: 0.00001828
Iteration 120/1000 | Loss: 0.00001828
Iteration 121/1000 | Loss: 0.00001828
Iteration 122/1000 | Loss: 0.00001828
Iteration 123/1000 | Loss: 0.00001828
Iteration 124/1000 | Loss: 0.00001828
Iteration 125/1000 | Loss: 0.00001828
Iteration 126/1000 | Loss: 0.00001828
Iteration 127/1000 | Loss: 0.00001828
Iteration 128/1000 | Loss: 0.00001828
Iteration 129/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8281334632774815e-05, 1.8281334632774815e-05, 1.8281334632774815e-05, 1.8281334632774815e-05, 1.8281334632774815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8281334632774815e-05

Optimization complete. Final v2v error: 3.7880311012268066 mm

Highest mean error: 4.081851005554199 mm for frame 128

Lowest mean error: 3.535033702850342 mm for frame 57

Saving results

Total time: 35.65082836151123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475857
Iteration 2/25 | Loss: 0.00122421
Iteration 3/25 | Loss: 0.00098527
Iteration 4/25 | Loss: 0.00095852
Iteration 5/25 | Loss: 0.00095263
Iteration 6/25 | Loss: 0.00095153
Iteration 7/25 | Loss: 0.00095141
Iteration 8/25 | Loss: 0.00095141
Iteration 9/25 | Loss: 0.00095141
Iteration 10/25 | Loss: 0.00095141
Iteration 11/25 | Loss: 0.00095141
Iteration 12/25 | Loss: 0.00095141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009514075936749578, 0.0009514075936749578, 0.0009514075936749578, 0.0009514075936749578, 0.0009514075936749578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009514075936749578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17446184
Iteration 2/25 | Loss: 0.00034576
Iteration 3/25 | Loss: 0.00034573
Iteration 4/25 | Loss: 0.00034573
Iteration 5/25 | Loss: 0.00034573
Iteration 6/25 | Loss: 0.00034573
Iteration 7/25 | Loss: 0.00034573
Iteration 8/25 | Loss: 0.00034573
Iteration 9/25 | Loss: 0.00034573
Iteration 10/25 | Loss: 0.00034573
Iteration 11/25 | Loss: 0.00034573
Iteration 12/25 | Loss: 0.00034573
Iteration 13/25 | Loss: 0.00034573
Iteration 14/25 | Loss: 0.00034573
Iteration 15/25 | Loss: 0.00034573
Iteration 16/25 | Loss: 0.00034573
Iteration 17/25 | Loss: 0.00034573
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003457290877122432, 0.0003457290877122432, 0.0003457290877122432, 0.0003457290877122432, 0.0003457290877122432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003457290877122432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034573
Iteration 2/1000 | Loss: 0.00004281
Iteration 3/1000 | Loss: 0.00003237
Iteration 4/1000 | Loss: 0.00002898
Iteration 5/1000 | Loss: 0.00002741
Iteration 6/1000 | Loss: 0.00002678
Iteration 7/1000 | Loss: 0.00002619
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002563
Iteration 10/1000 | Loss: 0.00002550
Iteration 11/1000 | Loss: 0.00002549
Iteration 12/1000 | Loss: 0.00002548
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002545
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002540
Iteration 18/1000 | Loss: 0.00002540
Iteration 19/1000 | Loss: 0.00002540
Iteration 20/1000 | Loss: 0.00002540
Iteration 21/1000 | Loss: 0.00002539
Iteration 22/1000 | Loss: 0.00002539
Iteration 23/1000 | Loss: 0.00002539
Iteration 24/1000 | Loss: 0.00002538
Iteration 25/1000 | Loss: 0.00002538
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002537
Iteration 28/1000 | Loss: 0.00002537
Iteration 29/1000 | Loss: 0.00002536
Iteration 30/1000 | Loss: 0.00002534
Iteration 31/1000 | Loss: 0.00002534
Iteration 32/1000 | Loss: 0.00002534
Iteration 33/1000 | Loss: 0.00002533
Iteration 34/1000 | Loss: 0.00002532
Iteration 35/1000 | Loss: 0.00002532
Iteration 36/1000 | Loss: 0.00002532
Iteration 37/1000 | Loss: 0.00002532
Iteration 38/1000 | Loss: 0.00002531
Iteration 39/1000 | Loss: 0.00002530
Iteration 40/1000 | Loss: 0.00002530
Iteration 41/1000 | Loss: 0.00002530
Iteration 42/1000 | Loss: 0.00002529
Iteration 43/1000 | Loss: 0.00002528
Iteration 44/1000 | Loss: 0.00002528
Iteration 45/1000 | Loss: 0.00002528
Iteration 46/1000 | Loss: 0.00002528
Iteration 47/1000 | Loss: 0.00002527
Iteration 48/1000 | Loss: 0.00002527
Iteration 49/1000 | Loss: 0.00002527
Iteration 50/1000 | Loss: 0.00002526
Iteration 51/1000 | Loss: 0.00002526
Iteration 52/1000 | Loss: 0.00002526
Iteration 53/1000 | Loss: 0.00002526
Iteration 54/1000 | Loss: 0.00002526
Iteration 55/1000 | Loss: 0.00002525
Iteration 56/1000 | Loss: 0.00002525
Iteration 57/1000 | Loss: 0.00002525
Iteration 58/1000 | Loss: 0.00002525
Iteration 59/1000 | Loss: 0.00002525
Iteration 60/1000 | Loss: 0.00002525
Iteration 61/1000 | Loss: 0.00002525
Iteration 62/1000 | Loss: 0.00002525
Iteration 63/1000 | Loss: 0.00002525
Iteration 64/1000 | Loss: 0.00002525
Iteration 65/1000 | Loss: 0.00002525
Iteration 66/1000 | Loss: 0.00002525
Iteration 67/1000 | Loss: 0.00002524
Iteration 68/1000 | Loss: 0.00002524
Iteration 69/1000 | Loss: 0.00002524
Iteration 70/1000 | Loss: 0.00002524
Iteration 71/1000 | Loss: 0.00002524
Iteration 72/1000 | Loss: 0.00002524
Iteration 73/1000 | Loss: 0.00002524
Iteration 74/1000 | Loss: 0.00002524
Iteration 75/1000 | Loss: 0.00002524
Iteration 76/1000 | Loss: 0.00002523
Iteration 77/1000 | Loss: 0.00002523
Iteration 78/1000 | Loss: 0.00002523
Iteration 79/1000 | Loss: 0.00002523
Iteration 80/1000 | Loss: 0.00002523
Iteration 81/1000 | Loss: 0.00002523
Iteration 82/1000 | Loss: 0.00002523
Iteration 83/1000 | Loss: 0.00002523
Iteration 84/1000 | Loss: 0.00002522
Iteration 85/1000 | Loss: 0.00002522
Iteration 86/1000 | Loss: 0.00002522
Iteration 87/1000 | Loss: 0.00002522
Iteration 88/1000 | Loss: 0.00002522
Iteration 89/1000 | Loss: 0.00002522
Iteration 90/1000 | Loss: 0.00002522
Iteration 91/1000 | Loss: 0.00002522
Iteration 92/1000 | Loss: 0.00002521
Iteration 93/1000 | Loss: 0.00002521
Iteration 94/1000 | Loss: 0.00002521
Iteration 95/1000 | Loss: 0.00002521
Iteration 96/1000 | Loss: 0.00002521
Iteration 97/1000 | Loss: 0.00002521
Iteration 98/1000 | Loss: 0.00002521
Iteration 99/1000 | Loss: 0.00002521
Iteration 100/1000 | Loss: 0.00002520
Iteration 101/1000 | Loss: 0.00002520
Iteration 102/1000 | Loss: 0.00002519
Iteration 103/1000 | Loss: 0.00002519
Iteration 104/1000 | Loss: 0.00002519
Iteration 105/1000 | Loss: 0.00002519
Iteration 106/1000 | Loss: 0.00002519
Iteration 107/1000 | Loss: 0.00002518
Iteration 108/1000 | Loss: 0.00002518
Iteration 109/1000 | Loss: 0.00002518
Iteration 110/1000 | Loss: 0.00002517
Iteration 111/1000 | Loss: 0.00002517
Iteration 112/1000 | Loss: 0.00002517
Iteration 113/1000 | Loss: 0.00002517
Iteration 114/1000 | Loss: 0.00002517
Iteration 115/1000 | Loss: 0.00002516
Iteration 116/1000 | Loss: 0.00002516
Iteration 117/1000 | Loss: 0.00002516
Iteration 118/1000 | Loss: 0.00002516
Iteration 119/1000 | Loss: 0.00002516
Iteration 120/1000 | Loss: 0.00002515
Iteration 121/1000 | Loss: 0.00002515
Iteration 122/1000 | Loss: 0.00002515
Iteration 123/1000 | Loss: 0.00002514
Iteration 124/1000 | Loss: 0.00002514
Iteration 125/1000 | Loss: 0.00002514
Iteration 126/1000 | Loss: 0.00002514
Iteration 127/1000 | Loss: 0.00002514
Iteration 128/1000 | Loss: 0.00002514
Iteration 129/1000 | Loss: 0.00002514
Iteration 130/1000 | Loss: 0.00002514
Iteration 131/1000 | Loss: 0.00002514
Iteration 132/1000 | Loss: 0.00002513
Iteration 133/1000 | Loss: 0.00002513
Iteration 134/1000 | Loss: 0.00002513
Iteration 135/1000 | Loss: 0.00002513
Iteration 136/1000 | Loss: 0.00002513
Iteration 137/1000 | Loss: 0.00002512
Iteration 138/1000 | Loss: 0.00002512
Iteration 139/1000 | Loss: 0.00002512
Iteration 140/1000 | Loss: 0.00002512
Iteration 141/1000 | Loss: 0.00002512
Iteration 142/1000 | Loss: 0.00002512
Iteration 143/1000 | Loss: 0.00002512
Iteration 144/1000 | Loss: 0.00002512
Iteration 145/1000 | Loss: 0.00002512
Iteration 146/1000 | Loss: 0.00002512
Iteration 147/1000 | Loss: 0.00002511
Iteration 148/1000 | Loss: 0.00002511
Iteration 149/1000 | Loss: 0.00002511
Iteration 150/1000 | Loss: 0.00002511
Iteration 151/1000 | Loss: 0.00002511
Iteration 152/1000 | Loss: 0.00002511
Iteration 153/1000 | Loss: 0.00002511
Iteration 154/1000 | Loss: 0.00002511
Iteration 155/1000 | Loss: 0.00002511
Iteration 156/1000 | Loss: 0.00002511
Iteration 157/1000 | Loss: 0.00002511
Iteration 158/1000 | Loss: 0.00002511
Iteration 159/1000 | Loss: 0.00002511
Iteration 160/1000 | Loss: 0.00002511
Iteration 161/1000 | Loss: 0.00002510
Iteration 162/1000 | Loss: 0.00002510
Iteration 163/1000 | Loss: 0.00002510
Iteration 164/1000 | Loss: 0.00002510
Iteration 165/1000 | Loss: 0.00002510
Iteration 166/1000 | Loss: 0.00002510
Iteration 167/1000 | Loss: 0.00002510
Iteration 168/1000 | Loss: 0.00002510
Iteration 169/1000 | Loss: 0.00002510
Iteration 170/1000 | Loss: 0.00002510
Iteration 171/1000 | Loss: 0.00002510
Iteration 172/1000 | Loss: 0.00002509
Iteration 173/1000 | Loss: 0.00002509
Iteration 174/1000 | Loss: 0.00002509
Iteration 175/1000 | Loss: 0.00002509
Iteration 176/1000 | Loss: 0.00002509
Iteration 177/1000 | Loss: 0.00002509
Iteration 178/1000 | Loss: 0.00002509
Iteration 179/1000 | Loss: 0.00002509
Iteration 180/1000 | Loss: 0.00002509
Iteration 181/1000 | Loss: 0.00002509
Iteration 182/1000 | Loss: 0.00002509
Iteration 183/1000 | Loss: 0.00002509
Iteration 184/1000 | Loss: 0.00002508
Iteration 185/1000 | Loss: 0.00002508
Iteration 186/1000 | Loss: 0.00002508
Iteration 187/1000 | Loss: 0.00002508
Iteration 188/1000 | Loss: 0.00002508
Iteration 189/1000 | Loss: 0.00002508
Iteration 190/1000 | Loss: 0.00002508
Iteration 191/1000 | Loss: 0.00002508
Iteration 192/1000 | Loss: 0.00002508
Iteration 193/1000 | Loss: 0.00002508
Iteration 194/1000 | Loss: 0.00002508
Iteration 195/1000 | Loss: 0.00002508
Iteration 196/1000 | Loss: 0.00002508
Iteration 197/1000 | Loss: 0.00002508
Iteration 198/1000 | Loss: 0.00002508
Iteration 199/1000 | Loss: 0.00002508
Iteration 200/1000 | Loss: 0.00002508
Iteration 201/1000 | Loss: 0.00002508
Iteration 202/1000 | Loss: 0.00002508
Iteration 203/1000 | Loss: 0.00002508
Iteration 204/1000 | Loss: 0.00002508
Iteration 205/1000 | Loss: 0.00002508
Iteration 206/1000 | Loss: 0.00002508
Iteration 207/1000 | Loss: 0.00002508
Iteration 208/1000 | Loss: 0.00002508
Iteration 209/1000 | Loss: 0.00002508
Iteration 210/1000 | Loss: 0.00002508
Iteration 211/1000 | Loss: 0.00002508
Iteration 212/1000 | Loss: 0.00002508
Iteration 213/1000 | Loss: 0.00002508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.5080296836677007e-05, 2.5080296836677007e-05, 2.5080296836677007e-05, 2.5080296836677007e-05, 2.5080296836677007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5080296836677007e-05

Optimization complete. Final v2v error: 4.257349967956543 mm

Highest mean error: 4.778604507446289 mm for frame 69

Lowest mean error: 3.886775493621826 mm for frame 102

Saving results

Total time: 37.130449056625366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807512
Iteration 2/25 | Loss: 0.00138074
Iteration 3/25 | Loss: 0.00111808
Iteration 4/25 | Loss: 0.00108778
Iteration 5/25 | Loss: 0.00107636
Iteration 6/25 | Loss: 0.00107440
Iteration 7/25 | Loss: 0.00107424
Iteration 8/25 | Loss: 0.00107424
Iteration 9/25 | Loss: 0.00107424
Iteration 10/25 | Loss: 0.00107424
Iteration 11/25 | Loss: 0.00107424
Iteration 12/25 | Loss: 0.00107424
Iteration 13/25 | Loss: 0.00107424
Iteration 14/25 | Loss: 0.00107424
Iteration 15/25 | Loss: 0.00107424
Iteration 16/25 | Loss: 0.00107424
Iteration 17/25 | Loss: 0.00107424
Iteration 18/25 | Loss: 0.00107424
Iteration 19/25 | Loss: 0.00107424
Iteration 20/25 | Loss: 0.00107424
Iteration 21/25 | Loss: 0.00107424
Iteration 22/25 | Loss: 0.00107424
Iteration 23/25 | Loss: 0.00107424
Iteration 24/25 | Loss: 0.00107424
Iteration 25/25 | Loss: 0.00107424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010742413578554988, 0.0010742413578554988, 0.0010742413578554988, 0.0010742413578554988, 0.0010742413578554988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010742413578554988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.35238075
Iteration 2/25 | Loss: 0.00047374
Iteration 3/25 | Loss: 0.00047369
Iteration 4/25 | Loss: 0.00047369
Iteration 5/25 | Loss: 0.00047369
Iteration 6/25 | Loss: 0.00047369
Iteration 7/25 | Loss: 0.00047369
Iteration 8/25 | Loss: 0.00047369
Iteration 9/25 | Loss: 0.00047369
Iteration 10/25 | Loss: 0.00047369
Iteration 11/25 | Loss: 0.00047369
Iteration 12/25 | Loss: 0.00047369
Iteration 13/25 | Loss: 0.00047369
Iteration 14/25 | Loss: 0.00047369
Iteration 15/25 | Loss: 0.00047369
Iteration 16/25 | Loss: 0.00047369
Iteration 17/25 | Loss: 0.00047369
Iteration 18/25 | Loss: 0.00047369
Iteration 19/25 | Loss: 0.00047369
Iteration 20/25 | Loss: 0.00047369
Iteration 21/25 | Loss: 0.00047369
Iteration 22/25 | Loss: 0.00047369
Iteration 23/25 | Loss: 0.00047369
Iteration 24/25 | Loss: 0.00047369
Iteration 25/25 | Loss: 0.00047369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047369
Iteration 2/1000 | Loss: 0.00006272
Iteration 3/1000 | Loss: 0.00004466
Iteration 4/1000 | Loss: 0.00003942
Iteration 5/1000 | Loss: 0.00003753
Iteration 6/1000 | Loss: 0.00003655
Iteration 7/1000 | Loss: 0.00003603
Iteration 8/1000 | Loss: 0.00003557
Iteration 9/1000 | Loss: 0.00003524
Iteration 10/1000 | Loss: 0.00003504
Iteration 11/1000 | Loss: 0.00003485
Iteration 12/1000 | Loss: 0.00003476
Iteration 13/1000 | Loss: 0.00003475
Iteration 14/1000 | Loss: 0.00003474
Iteration 15/1000 | Loss: 0.00003472
Iteration 16/1000 | Loss: 0.00003471
Iteration 17/1000 | Loss: 0.00003467
Iteration 18/1000 | Loss: 0.00003460
Iteration 19/1000 | Loss: 0.00003460
Iteration 20/1000 | Loss: 0.00003456
Iteration 21/1000 | Loss: 0.00003452
Iteration 22/1000 | Loss: 0.00003450
Iteration 23/1000 | Loss: 0.00003449
Iteration 24/1000 | Loss: 0.00003448
Iteration 25/1000 | Loss: 0.00003448
Iteration 26/1000 | Loss: 0.00003448
Iteration 27/1000 | Loss: 0.00003448
Iteration 28/1000 | Loss: 0.00003448
Iteration 29/1000 | Loss: 0.00003447
Iteration 30/1000 | Loss: 0.00003446
Iteration 31/1000 | Loss: 0.00003445
Iteration 32/1000 | Loss: 0.00003445
Iteration 33/1000 | Loss: 0.00003445
Iteration 34/1000 | Loss: 0.00003445
Iteration 35/1000 | Loss: 0.00003445
Iteration 36/1000 | Loss: 0.00003445
Iteration 37/1000 | Loss: 0.00003445
Iteration 38/1000 | Loss: 0.00003445
Iteration 39/1000 | Loss: 0.00003444
Iteration 40/1000 | Loss: 0.00003444
Iteration 41/1000 | Loss: 0.00003444
Iteration 42/1000 | Loss: 0.00003444
Iteration 43/1000 | Loss: 0.00003444
Iteration 44/1000 | Loss: 0.00003444
Iteration 45/1000 | Loss: 0.00003444
Iteration 46/1000 | Loss: 0.00003443
Iteration 47/1000 | Loss: 0.00003443
Iteration 48/1000 | Loss: 0.00003443
Iteration 49/1000 | Loss: 0.00003443
Iteration 50/1000 | Loss: 0.00003443
Iteration 51/1000 | Loss: 0.00003443
Iteration 52/1000 | Loss: 0.00003443
Iteration 53/1000 | Loss: 0.00003442
Iteration 54/1000 | Loss: 0.00003442
Iteration 55/1000 | Loss: 0.00003442
Iteration 56/1000 | Loss: 0.00003442
Iteration 57/1000 | Loss: 0.00003442
Iteration 58/1000 | Loss: 0.00003442
Iteration 59/1000 | Loss: 0.00003442
Iteration 60/1000 | Loss: 0.00003441
Iteration 61/1000 | Loss: 0.00003441
Iteration 62/1000 | Loss: 0.00003440
Iteration 63/1000 | Loss: 0.00003440
Iteration 64/1000 | Loss: 0.00003440
Iteration 65/1000 | Loss: 0.00003440
Iteration 66/1000 | Loss: 0.00003440
Iteration 67/1000 | Loss: 0.00003440
Iteration 68/1000 | Loss: 0.00003440
Iteration 69/1000 | Loss: 0.00003440
Iteration 70/1000 | Loss: 0.00003440
Iteration 71/1000 | Loss: 0.00003440
Iteration 72/1000 | Loss: 0.00003440
Iteration 73/1000 | Loss: 0.00003440
Iteration 74/1000 | Loss: 0.00003439
Iteration 75/1000 | Loss: 0.00003439
Iteration 76/1000 | Loss: 0.00003439
Iteration 77/1000 | Loss: 0.00003439
Iteration 78/1000 | Loss: 0.00003439
Iteration 79/1000 | Loss: 0.00003439
Iteration 80/1000 | Loss: 0.00003439
Iteration 81/1000 | Loss: 0.00003439
Iteration 82/1000 | Loss: 0.00003438
Iteration 83/1000 | Loss: 0.00003438
Iteration 84/1000 | Loss: 0.00003438
Iteration 85/1000 | Loss: 0.00003438
Iteration 86/1000 | Loss: 0.00003438
Iteration 87/1000 | Loss: 0.00003438
Iteration 88/1000 | Loss: 0.00003438
Iteration 89/1000 | Loss: 0.00003437
Iteration 90/1000 | Loss: 0.00003437
Iteration 91/1000 | Loss: 0.00003437
Iteration 92/1000 | Loss: 0.00003437
Iteration 93/1000 | Loss: 0.00003437
Iteration 94/1000 | Loss: 0.00003437
Iteration 95/1000 | Loss: 0.00003437
Iteration 96/1000 | Loss: 0.00003437
Iteration 97/1000 | Loss: 0.00003437
Iteration 98/1000 | Loss: 0.00003437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [3.437377017689869e-05, 3.437377017689869e-05, 3.437377017689869e-05, 3.437377017689869e-05, 3.437377017689869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.437377017689869e-05

Optimization complete. Final v2v error: 4.85508394241333 mm

Highest mean error: 5.804080963134766 mm for frame 199

Lowest mean error: 4.005247592926025 mm for frame 82

Saving results

Total time: 35.871169328689575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772318
Iteration 2/25 | Loss: 0.00203682
Iteration 3/25 | Loss: 0.00137394
Iteration 4/25 | Loss: 0.00123113
Iteration 5/25 | Loss: 0.00116790
Iteration 6/25 | Loss: 0.00114832
Iteration 7/25 | Loss: 0.00114102
Iteration 8/25 | Loss: 0.00114395
Iteration 9/25 | Loss: 0.00114200
Iteration 10/25 | Loss: 0.00114062
Iteration 11/25 | Loss: 0.00114164
Iteration 12/25 | Loss: 0.00113607
Iteration 13/25 | Loss: 0.00113540
Iteration 14/25 | Loss: 0.00113637
Iteration 15/25 | Loss: 0.00113321
Iteration 16/25 | Loss: 0.00112383
Iteration 17/25 | Loss: 0.00112511
Iteration 18/25 | Loss: 0.00112736
Iteration 19/25 | Loss: 0.00112670
Iteration 20/25 | Loss: 0.00112734
Iteration 21/25 | Loss: 0.00113217
Iteration 22/25 | Loss: 0.00112933
Iteration 23/25 | Loss: 0.00112763
Iteration 24/25 | Loss: 0.00112961
Iteration 25/25 | Loss: 0.00112860

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.21710300
Iteration 2/25 | Loss: 0.00163542
Iteration 3/25 | Loss: 0.00163541
Iteration 4/25 | Loss: 0.00163540
Iteration 5/25 | Loss: 0.00163540
Iteration 6/25 | Loss: 0.00163540
Iteration 7/25 | Loss: 0.00163540
Iteration 8/25 | Loss: 0.00163540
Iteration 9/25 | Loss: 0.00163540
Iteration 10/25 | Loss: 0.00163540
Iteration 11/25 | Loss: 0.00163540
Iteration 12/25 | Loss: 0.00163540
Iteration 13/25 | Loss: 0.00163540
Iteration 14/25 | Loss: 0.00163540
Iteration 15/25 | Loss: 0.00163540
Iteration 16/25 | Loss: 0.00163540
Iteration 17/25 | Loss: 0.00163540
Iteration 18/25 | Loss: 0.00163540
Iteration 19/25 | Loss: 0.00163540
Iteration 20/25 | Loss: 0.00163540
Iteration 21/25 | Loss: 0.00163540
Iteration 22/25 | Loss: 0.00163540
Iteration 23/25 | Loss: 0.00163540
Iteration 24/25 | Loss: 0.00163540
Iteration 25/25 | Loss: 0.00163540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163540
Iteration 2/1000 | Loss: 0.00059451
Iteration 3/1000 | Loss: 0.00365109
Iteration 4/1000 | Loss: 0.00396577
Iteration 5/1000 | Loss: 0.00077845
Iteration 6/1000 | Loss: 0.00040457
Iteration 7/1000 | Loss: 0.00300970
Iteration 8/1000 | Loss: 0.00126900
Iteration 9/1000 | Loss: 0.00185570
Iteration 10/1000 | Loss: 0.00162322
Iteration 11/1000 | Loss: 0.00191152
Iteration 12/1000 | Loss: 0.00112431
Iteration 13/1000 | Loss: 0.00097928
Iteration 14/1000 | Loss: 0.00111487
Iteration 15/1000 | Loss: 0.00104718
Iteration 16/1000 | Loss: 0.00049776
Iteration 17/1000 | Loss: 0.00047998
Iteration 18/1000 | Loss: 0.00041819
Iteration 19/1000 | Loss: 0.00018440
Iteration 20/1000 | Loss: 0.00011153
Iteration 21/1000 | Loss: 0.00020081
Iteration 22/1000 | Loss: 0.00034821
Iteration 23/1000 | Loss: 0.00032771
Iteration 24/1000 | Loss: 0.00039081
Iteration 25/1000 | Loss: 0.00054563
Iteration 26/1000 | Loss: 0.00046851
Iteration 27/1000 | Loss: 0.00046340
Iteration 28/1000 | Loss: 0.00041092
Iteration 29/1000 | Loss: 0.00061481
Iteration 30/1000 | Loss: 0.00041417
Iteration 31/1000 | Loss: 0.00012748
Iteration 32/1000 | Loss: 0.00028646
Iteration 33/1000 | Loss: 0.00019439
Iteration 34/1000 | Loss: 0.00033758
Iteration 35/1000 | Loss: 0.00023511
Iteration 36/1000 | Loss: 0.00027264
Iteration 37/1000 | Loss: 0.00021828
Iteration 38/1000 | Loss: 0.00041158
Iteration 39/1000 | Loss: 0.00023230
Iteration 40/1000 | Loss: 0.00115150
Iteration 41/1000 | Loss: 0.00052168
Iteration 42/1000 | Loss: 0.00026519
Iteration 43/1000 | Loss: 0.00020497
Iteration 44/1000 | Loss: 0.00021168
Iteration 45/1000 | Loss: 0.00211312
Iteration 46/1000 | Loss: 0.00090617
Iteration 47/1000 | Loss: 0.00016610
Iteration 48/1000 | Loss: 0.00071554
Iteration 49/1000 | Loss: 0.00180517
Iteration 50/1000 | Loss: 0.00084842
Iteration 51/1000 | Loss: 0.00110733
Iteration 52/1000 | Loss: 0.00186398
Iteration 53/1000 | Loss: 0.00104651
Iteration 54/1000 | Loss: 0.00152477
Iteration 55/1000 | Loss: 0.00103284
Iteration 56/1000 | Loss: 0.00076734
Iteration 57/1000 | Loss: 0.00039453
Iteration 58/1000 | Loss: 0.00157958
Iteration 59/1000 | Loss: 0.00349755
Iteration 60/1000 | Loss: 0.00178885
Iteration 61/1000 | Loss: 0.00070197
Iteration 62/1000 | Loss: 0.00180806
Iteration 63/1000 | Loss: 0.00072722
Iteration 64/1000 | Loss: 0.00095337
Iteration 65/1000 | Loss: 0.00096294
Iteration 66/1000 | Loss: 0.00068040
Iteration 67/1000 | Loss: 0.00118866
Iteration 68/1000 | Loss: 0.00065201
Iteration 69/1000 | Loss: 0.00070139
Iteration 70/1000 | Loss: 0.00048689
Iteration 71/1000 | Loss: 0.00060639
Iteration 72/1000 | Loss: 0.00077099
Iteration 73/1000 | Loss: 0.00067928
Iteration 74/1000 | Loss: 0.00064856
Iteration 75/1000 | Loss: 0.00052374
Iteration 76/1000 | Loss: 0.00063711
Iteration 77/1000 | Loss: 0.00060205
Iteration 78/1000 | Loss: 0.00059559
Iteration 79/1000 | Loss: 0.00025273
Iteration 80/1000 | Loss: 0.00057365
Iteration 81/1000 | Loss: 0.00059621
Iteration 82/1000 | Loss: 0.00066215
Iteration 83/1000 | Loss: 0.00086234
Iteration 84/1000 | Loss: 0.00043388
Iteration 85/1000 | Loss: 0.00046111
Iteration 86/1000 | Loss: 0.00073996
Iteration 87/1000 | Loss: 0.00023928
Iteration 88/1000 | Loss: 0.00062455
Iteration 89/1000 | Loss: 0.00023482
Iteration 90/1000 | Loss: 0.00031365
Iteration 91/1000 | Loss: 0.00030243
Iteration 92/1000 | Loss: 0.00024981
Iteration 93/1000 | Loss: 0.00044376
Iteration 94/1000 | Loss: 0.00012946
Iteration 95/1000 | Loss: 0.00018162
Iteration 96/1000 | Loss: 0.00010913
Iteration 97/1000 | Loss: 0.00072611
Iteration 98/1000 | Loss: 0.00033132
Iteration 99/1000 | Loss: 0.00016583
Iteration 100/1000 | Loss: 0.00008675
Iteration 101/1000 | Loss: 0.00019518
Iteration 102/1000 | Loss: 0.00025537
Iteration 103/1000 | Loss: 0.00019336
Iteration 104/1000 | Loss: 0.00057873
Iteration 105/1000 | Loss: 0.00022573
Iteration 106/1000 | Loss: 0.00013849
Iteration 107/1000 | Loss: 0.00025906
Iteration 108/1000 | Loss: 0.00022382
Iteration 109/1000 | Loss: 0.00022058
Iteration 110/1000 | Loss: 0.00023513
Iteration 111/1000 | Loss: 0.00019603
Iteration 112/1000 | Loss: 0.00020306
Iteration 113/1000 | Loss: 0.00070632
Iteration 114/1000 | Loss: 0.00030903
Iteration 115/1000 | Loss: 0.00005465
Iteration 116/1000 | Loss: 0.00004753
Iteration 117/1000 | Loss: 0.00055542
Iteration 118/1000 | Loss: 0.00015378
Iteration 119/1000 | Loss: 0.00006329
Iteration 120/1000 | Loss: 0.00011027
Iteration 121/1000 | Loss: 0.00021362
Iteration 122/1000 | Loss: 0.00011112
Iteration 123/1000 | Loss: 0.00027789
Iteration 124/1000 | Loss: 0.00009039
Iteration 125/1000 | Loss: 0.00005187
Iteration 126/1000 | Loss: 0.00004238
Iteration 127/1000 | Loss: 0.00027017
Iteration 128/1000 | Loss: 0.00084138
Iteration 129/1000 | Loss: 0.00025893
Iteration 130/1000 | Loss: 0.00009126
Iteration 131/1000 | Loss: 0.00009576
Iteration 132/1000 | Loss: 0.00005650
Iteration 133/1000 | Loss: 0.00049495
Iteration 134/1000 | Loss: 0.00017205
Iteration 135/1000 | Loss: 0.00042072
Iteration 136/1000 | Loss: 0.00007892
Iteration 137/1000 | Loss: 0.00005692
Iteration 138/1000 | Loss: 0.00019909
Iteration 139/1000 | Loss: 0.00033195
Iteration 140/1000 | Loss: 0.00022426
Iteration 141/1000 | Loss: 0.00048988
Iteration 142/1000 | Loss: 0.00023741
Iteration 143/1000 | Loss: 0.00059184
Iteration 144/1000 | Loss: 0.00008962
Iteration 145/1000 | Loss: 0.00012618
Iteration 146/1000 | Loss: 0.00062472
Iteration 147/1000 | Loss: 0.00011778
Iteration 148/1000 | Loss: 0.00015087
Iteration 149/1000 | Loss: 0.00008289
Iteration 150/1000 | Loss: 0.00004592
Iteration 151/1000 | Loss: 0.00004065
Iteration 152/1000 | Loss: 0.00003761
Iteration 153/1000 | Loss: 0.00003578
Iteration 154/1000 | Loss: 0.00015142
Iteration 155/1000 | Loss: 0.00003631
Iteration 156/1000 | Loss: 0.00003411
Iteration 157/1000 | Loss: 0.00003279
Iteration 158/1000 | Loss: 0.00003234
Iteration 159/1000 | Loss: 0.00003203
Iteration 160/1000 | Loss: 0.00003170
Iteration 161/1000 | Loss: 0.00003134
Iteration 162/1000 | Loss: 0.00003087
Iteration 163/1000 | Loss: 0.00003052
Iteration 164/1000 | Loss: 0.00003030
Iteration 165/1000 | Loss: 0.00003014
Iteration 166/1000 | Loss: 0.00003012
Iteration 167/1000 | Loss: 0.00003006
Iteration 168/1000 | Loss: 0.00003002
Iteration 169/1000 | Loss: 0.00003002
Iteration 170/1000 | Loss: 0.00002999
Iteration 171/1000 | Loss: 0.00002999
Iteration 172/1000 | Loss: 0.00002998
Iteration 173/1000 | Loss: 0.00002997
Iteration 174/1000 | Loss: 0.00002996
Iteration 175/1000 | Loss: 0.00002996
Iteration 176/1000 | Loss: 0.00002996
Iteration 177/1000 | Loss: 0.00002994
Iteration 178/1000 | Loss: 0.00002993
Iteration 179/1000 | Loss: 0.00002993
Iteration 180/1000 | Loss: 0.00002992
Iteration 181/1000 | Loss: 0.00002992
Iteration 182/1000 | Loss: 0.00002992
Iteration 183/1000 | Loss: 0.00002992
Iteration 184/1000 | Loss: 0.00002992
Iteration 185/1000 | Loss: 0.00002992
Iteration 186/1000 | Loss: 0.00002992
Iteration 187/1000 | Loss: 0.00002992
Iteration 188/1000 | Loss: 0.00002992
Iteration 189/1000 | Loss: 0.00002991
Iteration 190/1000 | Loss: 0.00002991
Iteration 191/1000 | Loss: 0.00002991
Iteration 192/1000 | Loss: 0.00002990
Iteration 193/1000 | Loss: 0.00002990
Iteration 194/1000 | Loss: 0.00002989
Iteration 195/1000 | Loss: 0.00002989
Iteration 196/1000 | Loss: 0.00002989
Iteration 197/1000 | Loss: 0.00002989
Iteration 198/1000 | Loss: 0.00002989
Iteration 199/1000 | Loss: 0.00002989
Iteration 200/1000 | Loss: 0.00002989
Iteration 201/1000 | Loss: 0.00002989
Iteration 202/1000 | Loss: 0.00002988
Iteration 203/1000 | Loss: 0.00002988
Iteration 204/1000 | Loss: 0.00002988
Iteration 205/1000 | Loss: 0.00002988
Iteration 206/1000 | Loss: 0.00002988
Iteration 207/1000 | Loss: 0.00002988
Iteration 208/1000 | Loss: 0.00002988
Iteration 209/1000 | Loss: 0.00002988
Iteration 210/1000 | Loss: 0.00002988
Iteration 211/1000 | Loss: 0.00002988
Iteration 212/1000 | Loss: 0.00002988
Iteration 213/1000 | Loss: 0.00002988
Iteration 214/1000 | Loss: 0.00002988
Iteration 215/1000 | Loss: 0.00002987
Iteration 216/1000 | Loss: 0.00002987
Iteration 217/1000 | Loss: 0.00002987
Iteration 218/1000 | Loss: 0.00002986
Iteration 219/1000 | Loss: 0.00002986
Iteration 220/1000 | Loss: 0.00002985
Iteration 221/1000 | Loss: 0.00002985
Iteration 222/1000 | Loss: 0.00002985
Iteration 223/1000 | Loss: 0.00002985
Iteration 224/1000 | Loss: 0.00002985
Iteration 225/1000 | Loss: 0.00002985
Iteration 226/1000 | Loss: 0.00002985
Iteration 227/1000 | Loss: 0.00002984
Iteration 228/1000 | Loss: 0.00002984
Iteration 229/1000 | Loss: 0.00002984
Iteration 230/1000 | Loss: 0.00002984
Iteration 231/1000 | Loss: 0.00002984
Iteration 232/1000 | Loss: 0.00002984
Iteration 233/1000 | Loss: 0.00002984
Iteration 234/1000 | Loss: 0.00002984
Iteration 235/1000 | Loss: 0.00002984
Iteration 236/1000 | Loss: 0.00002984
Iteration 237/1000 | Loss: 0.00002983
Iteration 238/1000 | Loss: 0.00002983
Iteration 239/1000 | Loss: 0.00002983
Iteration 240/1000 | Loss: 0.00002983
Iteration 241/1000 | Loss: 0.00002982
Iteration 242/1000 | Loss: 0.00002982
Iteration 243/1000 | Loss: 0.00002982
Iteration 244/1000 | Loss: 0.00002981
Iteration 245/1000 | Loss: 0.00002981
Iteration 246/1000 | Loss: 0.00002980
Iteration 247/1000 | Loss: 0.00002980
Iteration 248/1000 | Loss: 0.00002980
Iteration 249/1000 | Loss: 0.00002979
Iteration 250/1000 | Loss: 0.00002979
Iteration 251/1000 | Loss: 0.00002979
Iteration 252/1000 | Loss: 0.00002978
Iteration 253/1000 | Loss: 0.00002978
Iteration 254/1000 | Loss: 0.00002978
Iteration 255/1000 | Loss: 0.00002977
Iteration 256/1000 | Loss: 0.00002977
Iteration 257/1000 | Loss: 0.00002977
Iteration 258/1000 | Loss: 0.00002977
Iteration 259/1000 | Loss: 0.00002977
Iteration 260/1000 | Loss: 0.00002977
Iteration 261/1000 | Loss: 0.00002977
Iteration 262/1000 | Loss: 0.00002977
Iteration 263/1000 | Loss: 0.00002977
Iteration 264/1000 | Loss: 0.00002976
Iteration 265/1000 | Loss: 0.00002976
Iteration 266/1000 | Loss: 0.00002976
Iteration 267/1000 | Loss: 0.00002976
Iteration 268/1000 | Loss: 0.00002976
Iteration 269/1000 | Loss: 0.00002976
Iteration 270/1000 | Loss: 0.00002976
Iteration 271/1000 | Loss: 0.00002976
Iteration 272/1000 | Loss: 0.00002976
Iteration 273/1000 | Loss: 0.00002976
Iteration 274/1000 | Loss: 0.00002976
Iteration 275/1000 | Loss: 0.00002976
Iteration 276/1000 | Loss: 0.00002976
Iteration 277/1000 | Loss: 0.00002976
Iteration 278/1000 | Loss: 0.00002976
Iteration 279/1000 | Loss: 0.00002976
Iteration 280/1000 | Loss: 0.00002976
Iteration 281/1000 | Loss: 0.00002976
Iteration 282/1000 | Loss: 0.00002976
Iteration 283/1000 | Loss: 0.00002975
Iteration 284/1000 | Loss: 0.00002975
Iteration 285/1000 | Loss: 0.00002975
Iteration 286/1000 | Loss: 0.00002975
Iteration 287/1000 | Loss: 0.00002975
Iteration 288/1000 | Loss: 0.00002975
Iteration 289/1000 | Loss: 0.00002975
Iteration 290/1000 | Loss: 0.00002974
Iteration 291/1000 | Loss: 0.00002974
Iteration 292/1000 | Loss: 0.00002974
Iteration 293/1000 | Loss: 0.00002973
Iteration 294/1000 | Loss: 0.00002973
Iteration 295/1000 | Loss: 0.00002973
Iteration 296/1000 | Loss: 0.00002973
Iteration 297/1000 | Loss: 0.00002973
Iteration 298/1000 | Loss: 0.00002972
Iteration 299/1000 | Loss: 0.00002972
Iteration 300/1000 | Loss: 0.00002972
Iteration 301/1000 | Loss: 0.00002972
Iteration 302/1000 | Loss: 0.00002972
Iteration 303/1000 | Loss: 0.00002972
Iteration 304/1000 | Loss: 0.00002972
Iteration 305/1000 | Loss: 0.00002972
Iteration 306/1000 | Loss: 0.00002972
Iteration 307/1000 | Loss: 0.00002972
Iteration 308/1000 | Loss: 0.00002972
Iteration 309/1000 | Loss: 0.00002972
Iteration 310/1000 | Loss: 0.00002971
Iteration 311/1000 | Loss: 0.00002971
Iteration 312/1000 | Loss: 0.00002971
Iteration 313/1000 | Loss: 0.00002971
Iteration 314/1000 | Loss: 0.00002971
Iteration 315/1000 | Loss: 0.00002971
Iteration 316/1000 | Loss: 0.00002971
Iteration 317/1000 | Loss: 0.00002971
Iteration 318/1000 | Loss: 0.00002971
Iteration 319/1000 | Loss: 0.00002971
Iteration 320/1000 | Loss: 0.00002971
Iteration 321/1000 | Loss: 0.00002971
Iteration 322/1000 | Loss: 0.00002971
Iteration 323/1000 | Loss: 0.00002970
Iteration 324/1000 | Loss: 0.00002970
Iteration 325/1000 | Loss: 0.00002970
Iteration 326/1000 | Loss: 0.00002970
Iteration 327/1000 | Loss: 0.00002970
Iteration 328/1000 | Loss: 0.00002970
Iteration 329/1000 | Loss: 0.00002970
Iteration 330/1000 | Loss: 0.00002970
Iteration 331/1000 | Loss: 0.00002970
Iteration 332/1000 | Loss: 0.00002970
Iteration 333/1000 | Loss: 0.00002970
Iteration 334/1000 | Loss: 0.00002970
Iteration 335/1000 | Loss: 0.00002970
Iteration 336/1000 | Loss: 0.00002970
Iteration 337/1000 | Loss: 0.00002970
Iteration 338/1000 | Loss: 0.00002970
Iteration 339/1000 | Loss: 0.00002970
Iteration 340/1000 | Loss: 0.00002970
Iteration 341/1000 | Loss: 0.00002970
Iteration 342/1000 | Loss: 0.00002970
Iteration 343/1000 | Loss: 0.00002970
Iteration 344/1000 | Loss: 0.00002970
Iteration 345/1000 | Loss: 0.00002970
Iteration 346/1000 | Loss: 0.00002970
Iteration 347/1000 | Loss: 0.00002970
Iteration 348/1000 | Loss: 0.00002970
Iteration 349/1000 | Loss: 0.00002970
Iteration 350/1000 | Loss: 0.00002970
Iteration 351/1000 | Loss: 0.00002970
Iteration 352/1000 | Loss: 0.00002970
Iteration 353/1000 | Loss: 0.00002970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [2.970020977954846e-05, 2.970020977954846e-05, 2.970020977954846e-05, 2.970020977954846e-05, 2.970020977954846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.970020977954846e-05

Optimization complete. Final v2v error: 4.49296236038208 mm

Highest mean error: 5.670379161834717 mm for frame 124

Lowest mean error: 3.219531774520874 mm for frame 0

Saving results

Total time: 293.7144284248352
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921021
Iteration 2/25 | Loss: 0.00108287
Iteration 3/25 | Loss: 0.00096177
Iteration 4/25 | Loss: 0.00093744
Iteration 5/25 | Loss: 0.00093001
Iteration 6/25 | Loss: 0.00092842
Iteration 7/25 | Loss: 0.00092829
Iteration 8/25 | Loss: 0.00092829
Iteration 9/25 | Loss: 0.00092829
Iteration 10/25 | Loss: 0.00092829
Iteration 11/25 | Loss: 0.00092829
Iteration 12/25 | Loss: 0.00092829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009282893151976168, 0.0009282893151976168, 0.0009282893151976168, 0.0009282893151976168, 0.0009282893151976168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009282893151976168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03406334
Iteration 2/25 | Loss: 0.00229585
Iteration 3/25 | Loss: 0.00229585
Iteration 4/25 | Loss: 0.00229585
Iteration 5/25 | Loss: 0.00229585
Iteration 6/25 | Loss: 0.00229585
Iteration 7/25 | Loss: 0.00229585
Iteration 8/25 | Loss: 0.00229585
Iteration 9/25 | Loss: 0.00229585
Iteration 10/25 | Loss: 0.00229585
Iteration 11/25 | Loss: 0.00229585
Iteration 12/25 | Loss: 0.00229585
Iteration 13/25 | Loss: 0.00229585
Iteration 14/25 | Loss: 0.00229585
Iteration 15/25 | Loss: 0.00229585
Iteration 16/25 | Loss: 0.00229585
Iteration 17/25 | Loss: 0.00229585
Iteration 18/25 | Loss: 0.00229585
Iteration 19/25 | Loss: 0.00229585
Iteration 20/25 | Loss: 0.00229585
Iteration 21/25 | Loss: 0.00229585
Iteration 22/25 | Loss: 0.00229585
Iteration 23/25 | Loss: 0.00229585
Iteration 24/25 | Loss: 0.00229585
Iteration 25/25 | Loss: 0.00229585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002295849611982703, 0.002295849611982703, 0.002295849611982703, 0.002295849611982703, 0.002295849611982703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002295849611982703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229585
Iteration 2/1000 | Loss: 0.00002794
Iteration 3/1000 | Loss: 0.00002320
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001861
Iteration 8/1000 | Loss: 0.00001831
Iteration 9/1000 | Loss: 0.00001806
Iteration 10/1000 | Loss: 0.00001805
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001789
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001783
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001781
Iteration 21/1000 | Loss: 0.00001780
Iteration 22/1000 | Loss: 0.00001780
Iteration 23/1000 | Loss: 0.00001779
Iteration 24/1000 | Loss: 0.00001779
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001774
Iteration 29/1000 | Loss: 0.00001774
Iteration 30/1000 | Loss: 0.00001774
Iteration 31/1000 | Loss: 0.00001773
Iteration 32/1000 | Loss: 0.00001772
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001770
Iteration 37/1000 | Loss: 0.00001770
Iteration 38/1000 | Loss: 0.00001770
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001770
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001769
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001767
Iteration 48/1000 | Loss: 0.00001767
Iteration 49/1000 | Loss: 0.00001767
Iteration 50/1000 | Loss: 0.00001767
Iteration 51/1000 | Loss: 0.00001766
Iteration 52/1000 | Loss: 0.00001766
Iteration 53/1000 | Loss: 0.00001766
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00001765
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001761
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001756
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001756
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001756
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001755
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001755
Iteration 116/1000 | Loss: 0.00001755
Iteration 117/1000 | Loss: 0.00001755
Iteration 118/1000 | Loss: 0.00001755
Iteration 119/1000 | Loss: 0.00001755
Iteration 120/1000 | Loss: 0.00001755
Iteration 121/1000 | Loss: 0.00001755
Iteration 122/1000 | Loss: 0.00001755
Iteration 123/1000 | Loss: 0.00001755
Iteration 124/1000 | Loss: 0.00001755
Iteration 125/1000 | Loss: 0.00001755
Iteration 126/1000 | Loss: 0.00001755
Iteration 127/1000 | Loss: 0.00001755
Iteration 128/1000 | Loss: 0.00001755
Iteration 129/1000 | Loss: 0.00001755
Iteration 130/1000 | Loss: 0.00001755
Iteration 131/1000 | Loss: 0.00001755
Iteration 132/1000 | Loss: 0.00001755
Iteration 133/1000 | Loss: 0.00001755
Iteration 134/1000 | Loss: 0.00001755
Iteration 135/1000 | Loss: 0.00001755
Iteration 136/1000 | Loss: 0.00001755
Iteration 137/1000 | Loss: 0.00001755
Iteration 138/1000 | Loss: 0.00001755
Iteration 139/1000 | Loss: 0.00001755
Iteration 140/1000 | Loss: 0.00001755
Iteration 141/1000 | Loss: 0.00001755
Iteration 142/1000 | Loss: 0.00001755
Iteration 143/1000 | Loss: 0.00001755
Iteration 144/1000 | Loss: 0.00001755
Iteration 145/1000 | Loss: 0.00001755
Iteration 146/1000 | Loss: 0.00001755
Iteration 147/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.754511504259426e-05, 1.754511504259426e-05, 1.754511504259426e-05, 1.754511504259426e-05, 1.754511504259426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.754511504259426e-05

Optimization complete. Final v2v error: 3.5833237171173096 mm

Highest mean error: 3.9969534873962402 mm for frame 76

Lowest mean error: 3.41739821434021 mm for frame 123

Saving results

Total time: 32.253252267837524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917790
Iteration 2/25 | Loss: 0.00119829
Iteration 3/25 | Loss: 0.00101257
Iteration 4/25 | Loss: 0.00098949
Iteration 5/25 | Loss: 0.00098077
Iteration 6/25 | Loss: 0.00097773
Iteration 7/25 | Loss: 0.00097936
Iteration 8/25 | Loss: 0.00098314
Iteration 9/25 | Loss: 0.00097985
Iteration 10/25 | Loss: 0.00097786
Iteration 11/25 | Loss: 0.00097554
Iteration 12/25 | Loss: 0.00097497
Iteration 13/25 | Loss: 0.00097578
Iteration 14/25 | Loss: 0.00097472
Iteration 15/25 | Loss: 0.00097473
Iteration 16/25 | Loss: 0.00097509
Iteration 17/25 | Loss: 0.00097478
Iteration 18/25 | Loss: 0.00097493
Iteration 19/25 | Loss: 0.00097530
Iteration 20/25 | Loss: 0.00097503
Iteration 21/25 | Loss: 0.00097493
Iteration 22/25 | Loss: 0.00097589
Iteration 23/25 | Loss: 0.00097462
Iteration 24/25 | Loss: 0.00097243
Iteration 25/25 | Loss: 0.00097452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66813874
Iteration 2/25 | Loss: 0.00313210
Iteration 3/25 | Loss: 0.00313209
Iteration 4/25 | Loss: 0.00313209
Iteration 5/25 | Loss: 0.00313209
Iteration 6/25 | Loss: 0.00313209
Iteration 7/25 | Loss: 0.00313209
Iteration 8/25 | Loss: 0.00313209
Iteration 9/25 | Loss: 0.00313209
Iteration 10/25 | Loss: 0.00313209
Iteration 11/25 | Loss: 0.00313209
Iteration 12/25 | Loss: 0.00313209
Iteration 13/25 | Loss: 0.00313209
Iteration 14/25 | Loss: 0.00313209
Iteration 15/25 | Loss: 0.00313209
Iteration 16/25 | Loss: 0.00313209
Iteration 17/25 | Loss: 0.00313209
Iteration 18/25 | Loss: 0.00313209
Iteration 19/25 | Loss: 0.00313209
Iteration 20/25 | Loss: 0.00313209
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003132091136649251, 0.003132091136649251, 0.003132091136649251, 0.003132091136649251, 0.003132091136649251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003132091136649251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00313209
Iteration 2/1000 | Loss: 0.00016202
Iteration 3/1000 | Loss: 0.00014540
Iteration 4/1000 | Loss: 0.00021395
Iteration 5/1000 | Loss: 0.00009715
Iteration 6/1000 | Loss: 0.00008659
Iteration 7/1000 | Loss: 0.00021641
Iteration 8/1000 | Loss: 0.00023398
Iteration 9/1000 | Loss: 0.00007875
Iteration 10/1000 | Loss: 0.00007308
Iteration 11/1000 | Loss: 0.00006993
Iteration 12/1000 | Loss: 0.00046424
Iteration 13/1000 | Loss: 0.00021355
Iteration 14/1000 | Loss: 0.00023459
Iteration 15/1000 | Loss: 0.00027628
Iteration 16/1000 | Loss: 0.00007426
Iteration 17/1000 | Loss: 0.00006763
Iteration 18/1000 | Loss: 0.00007584
Iteration 19/1000 | Loss: 0.00007030
Iteration 20/1000 | Loss: 0.00006705
Iteration 21/1000 | Loss: 0.00006290
Iteration 22/1000 | Loss: 0.00049122
Iteration 23/1000 | Loss: 0.00008097
Iteration 24/1000 | Loss: 0.00058506
Iteration 25/1000 | Loss: 0.00144487
Iteration 26/1000 | Loss: 0.00011775
Iteration 27/1000 | Loss: 0.00053428
Iteration 28/1000 | Loss: 0.00007582
Iteration 29/1000 | Loss: 0.00006856
Iteration 30/1000 | Loss: 0.00006125
Iteration 31/1000 | Loss: 0.00005809
Iteration 32/1000 | Loss: 0.00005628
Iteration 33/1000 | Loss: 0.00061273
Iteration 34/1000 | Loss: 0.00006512
Iteration 35/1000 | Loss: 0.00005382
Iteration 36/1000 | Loss: 0.00005200
Iteration 37/1000 | Loss: 0.00005107
Iteration 38/1000 | Loss: 0.00005006
Iteration 39/1000 | Loss: 0.00057764
Iteration 40/1000 | Loss: 0.00006221
Iteration 41/1000 | Loss: 0.00004875
Iteration 42/1000 | Loss: 0.00004779
Iteration 43/1000 | Loss: 0.00053973
Iteration 44/1000 | Loss: 0.00021593
Iteration 45/1000 | Loss: 0.00014899
Iteration 46/1000 | Loss: 0.00005704
Iteration 47/1000 | Loss: 0.00004964
Iteration 48/1000 | Loss: 0.00004619
Iteration 49/1000 | Loss: 0.00049828
Iteration 50/1000 | Loss: 0.00073106
Iteration 51/1000 | Loss: 0.00029992
Iteration 52/1000 | Loss: 0.00005374
Iteration 53/1000 | Loss: 0.00004565
Iteration 54/1000 | Loss: 0.00004340
Iteration 55/1000 | Loss: 0.00021780
Iteration 56/1000 | Loss: 0.00022313
Iteration 57/1000 | Loss: 0.00020353
Iteration 58/1000 | Loss: 0.00109234
Iteration 59/1000 | Loss: 0.00101253
Iteration 60/1000 | Loss: 0.00007585
Iteration 61/1000 | Loss: 0.00005014
Iteration 62/1000 | Loss: 0.00004365
Iteration 63/1000 | Loss: 0.00003977
Iteration 64/1000 | Loss: 0.00003809
Iteration 65/1000 | Loss: 0.00075959
Iteration 66/1000 | Loss: 0.00021439
Iteration 67/1000 | Loss: 0.00023115
Iteration 68/1000 | Loss: 0.00005337
Iteration 69/1000 | Loss: 0.00004202
Iteration 70/1000 | Loss: 0.00003758
Iteration 71/1000 | Loss: 0.00003539
Iteration 72/1000 | Loss: 0.00003425
Iteration 73/1000 | Loss: 0.00003367
Iteration 74/1000 | Loss: 0.00003323
Iteration 75/1000 | Loss: 0.00003287
Iteration 76/1000 | Loss: 0.00003246
Iteration 77/1000 | Loss: 0.00050187
Iteration 78/1000 | Loss: 0.00005682
Iteration 79/1000 | Loss: 0.00003300
Iteration 80/1000 | Loss: 0.00003123
Iteration 81/1000 | Loss: 0.00003084
Iteration 82/1000 | Loss: 0.00003054
Iteration 83/1000 | Loss: 0.00003052
Iteration 84/1000 | Loss: 0.00003038
Iteration 85/1000 | Loss: 0.00003037
Iteration 86/1000 | Loss: 0.00003037
Iteration 87/1000 | Loss: 0.00003024
Iteration 88/1000 | Loss: 0.00003024
Iteration 89/1000 | Loss: 0.00003023
Iteration 90/1000 | Loss: 0.00003022
Iteration 91/1000 | Loss: 0.00003022
Iteration 92/1000 | Loss: 0.00003019
Iteration 93/1000 | Loss: 0.00003019
Iteration 94/1000 | Loss: 0.00003018
Iteration 95/1000 | Loss: 0.00003017
Iteration 96/1000 | Loss: 0.00003015
Iteration 97/1000 | Loss: 0.00003014
Iteration 98/1000 | Loss: 0.00003014
Iteration 99/1000 | Loss: 0.00003013
Iteration 100/1000 | Loss: 0.00003013
Iteration 101/1000 | Loss: 0.00003013
Iteration 102/1000 | Loss: 0.00003012
Iteration 103/1000 | Loss: 0.00003012
Iteration 104/1000 | Loss: 0.00003012
Iteration 105/1000 | Loss: 0.00003011
Iteration 106/1000 | Loss: 0.00003011
Iteration 107/1000 | Loss: 0.00003011
Iteration 108/1000 | Loss: 0.00003010
Iteration 109/1000 | Loss: 0.00003010
Iteration 110/1000 | Loss: 0.00003010
Iteration 111/1000 | Loss: 0.00003009
Iteration 112/1000 | Loss: 0.00003009
Iteration 113/1000 | Loss: 0.00003008
Iteration 114/1000 | Loss: 0.00003008
Iteration 115/1000 | Loss: 0.00003007
Iteration 116/1000 | Loss: 0.00003007
Iteration 117/1000 | Loss: 0.00003007
Iteration 118/1000 | Loss: 0.00003007
Iteration 119/1000 | Loss: 0.00003007
Iteration 120/1000 | Loss: 0.00003006
Iteration 121/1000 | Loss: 0.00003006
Iteration 122/1000 | Loss: 0.00003006
Iteration 123/1000 | Loss: 0.00003006
Iteration 124/1000 | Loss: 0.00003006
Iteration 125/1000 | Loss: 0.00003006
Iteration 126/1000 | Loss: 0.00003006
Iteration 127/1000 | Loss: 0.00003006
Iteration 128/1000 | Loss: 0.00003006
Iteration 129/1000 | Loss: 0.00003006
Iteration 130/1000 | Loss: 0.00003006
Iteration 131/1000 | Loss: 0.00003006
Iteration 132/1000 | Loss: 0.00003005
Iteration 133/1000 | Loss: 0.00003005
Iteration 134/1000 | Loss: 0.00003005
Iteration 135/1000 | Loss: 0.00003005
Iteration 136/1000 | Loss: 0.00003005
Iteration 137/1000 | Loss: 0.00003005
Iteration 138/1000 | Loss: 0.00003005
Iteration 139/1000 | Loss: 0.00003005
Iteration 140/1000 | Loss: 0.00003005
Iteration 141/1000 | Loss: 0.00003005
Iteration 142/1000 | Loss: 0.00003005
Iteration 143/1000 | Loss: 0.00003005
Iteration 144/1000 | Loss: 0.00003005
Iteration 145/1000 | Loss: 0.00003005
Iteration 146/1000 | Loss: 0.00003004
Iteration 147/1000 | Loss: 0.00003004
Iteration 148/1000 | Loss: 0.00003004
Iteration 149/1000 | Loss: 0.00003004
Iteration 150/1000 | Loss: 0.00003004
Iteration 151/1000 | Loss: 0.00003004
Iteration 152/1000 | Loss: 0.00003004
Iteration 153/1000 | Loss: 0.00003004
Iteration 154/1000 | Loss: 0.00003004
Iteration 155/1000 | Loss: 0.00003003
Iteration 156/1000 | Loss: 0.00003003
Iteration 157/1000 | Loss: 0.00003003
Iteration 158/1000 | Loss: 0.00003003
Iteration 159/1000 | Loss: 0.00003003
Iteration 160/1000 | Loss: 0.00003002
Iteration 161/1000 | Loss: 0.00003002
Iteration 162/1000 | Loss: 0.00003002
Iteration 163/1000 | Loss: 0.00003002
Iteration 164/1000 | Loss: 0.00003002
Iteration 165/1000 | Loss: 0.00003002
Iteration 166/1000 | Loss: 0.00003002
Iteration 167/1000 | Loss: 0.00003001
Iteration 168/1000 | Loss: 0.00003001
Iteration 169/1000 | Loss: 0.00003001
Iteration 170/1000 | Loss: 0.00003001
Iteration 171/1000 | Loss: 0.00003001
Iteration 172/1000 | Loss: 0.00003001
Iteration 173/1000 | Loss: 0.00003000
Iteration 174/1000 | Loss: 0.00003000
Iteration 175/1000 | Loss: 0.00003000
Iteration 176/1000 | Loss: 0.00003000
Iteration 177/1000 | Loss: 0.00003000
Iteration 178/1000 | Loss: 0.00003000
Iteration 179/1000 | Loss: 0.00003000
Iteration 180/1000 | Loss: 0.00003000
Iteration 181/1000 | Loss: 0.00002999
Iteration 182/1000 | Loss: 0.00002999
Iteration 183/1000 | Loss: 0.00002999
Iteration 184/1000 | Loss: 0.00002999
Iteration 185/1000 | Loss: 0.00002999
Iteration 186/1000 | Loss: 0.00002999
Iteration 187/1000 | Loss: 0.00002999
Iteration 188/1000 | Loss: 0.00002999
Iteration 189/1000 | Loss: 0.00002999
Iteration 190/1000 | Loss: 0.00002999
Iteration 191/1000 | Loss: 0.00002999
Iteration 192/1000 | Loss: 0.00002998
Iteration 193/1000 | Loss: 0.00002998
Iteration 194/1000 | Loss: 0.00002998
Iteration 195/1000 | Loss: 0.00002998
Iteration 196/1000 | Loss: 0.00002998
Iteration 197/1000 | Loss: 0.00002998
Iteration 198/1000 | Loss: 0.00002998
Iteration 199/1000 | Loss: 0.00002998
Iteration 200/1000 | Loss: 0.00002998
Iteration 201/1000 | Loss: 0.00002997
Iteration 202/1000 | Loss: 0.00002997
Iteration 203/1000 | Loss: 0.00002997
Iteration 204/1000 | Loss: 0.00002997
Iteration 205/1000 | Loss: 0.00002997
Iteration 206/1000 | Loss: 0.00002997
Iteration 207/1000 | Loss: 0.00002997
Iteration 208/1000 | Loss: 0.00002997
Iteration 209/1000 | Loss: 0.00002997
Iteration 210/1000 | Loss: 0.00002996
Iteration 211/1000 | Loss: 0.00002996
Iteration 212/1000 | Loss: 0.00002996
Iteration 213/1000 | Loss: 0.00002996
Iteration 214/1000 | Loss: 0.00002996
Iteration 215/1000 | Loss: 0.00002996
Iteration 216/1000 | Loss: 0.00002996
Iteration 217/1000 | Loss: 0.00002996
Iteration 218/1000 | Loss: 0.00002996
Iteration 219/1000 | Loss: 0.00002996
Iteration 220/1000 | Loss: 0.00002996
Iteration 221/1000 | Loss: 0.00002996
Iteration 222/1000 | Loss: 0.00002996
Iteration 223/1000 | Loss: 0.00002995
Iteration 224/1000 | Loss: 0.00002995
Iteration 225/1000 | Loss: 0.00002995
Iteration 226/1000 | Loss: 0.00002995
Iteration 227/1000 | Loss: 0.00002995
Iteration 228/1000 | Loss: 0.00002995
Iteration 229/1000 | Loss: 0.00002995
Iteration 230/1000 | Loss: 0.00002995
Iteration 231/1000 | Loss: 0.00002995
Iteration 232/1000 | Loss: 0.00002995
Iteration 233/1000 | Loss: 0.00002995
Iteration 234/1000 | Loss: 0.00002995
Iteration 235/1000 | Loss: 0.00002995
Iteration 236/1000 | Loss: 0.00002995
Iteration 237/1000 | Loss: 0.00002994
Iteration 238/1000 | Loss: 0.00002994
Iteration 239/1000 | Loss: 0.00002994
Iteration 240/1000 | Loss: 0.00002994
Iteration 241/1000 | Loss: 0.00002994
Iteration 242/1000 | Loss: 0.00002994
Iteration 243/1000 | Loss: 0.00002994
Iteration 244/1000 | Loss: 0.00002994
Iteration 245/1000 | Loss: 0.00002994
Iteration 246/1000 | Loss: 0.00002994
Iteration 247/1000 | Loss: 0.00002994
Iteration 248/1000 | Loss: 0.00002994
Iteration 249/1000 | Loss: 0.00002994
Iteration 250/1000 | Loss: 0.00002994
Iteration 251/1000 | Loss: 0.00002994
Iteration 252/1000 | Loss: 0.00002994
Iteration 253/1000 | Loss: 0.00002994
Iteration 254/1000 | Loss: 0.00002994
Iteration 255/1000 | Loss: 0.00002994
Iteration 256/1000 | Loss: 0.00002994
Iteration 257/1000 | Loss: 0.00002994
Iteration 258/1000 | Loss: 0.00002994
Iteration 259/1000 | Loss: 0.00002994
Iteration 260/1000 | Loss: 0.00002994
Iteration 261/1000 | Loss: 0.00002994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.9944940251880325e-05, 2.9944940251880325e-05, 2.9944940251880325e-05, 2.9944940251880325e-05, 2.9944940251880325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9944940251880325e-05

Optimization complete. Final v2v error: 3.466801404953003 mm

Highest mean error: 13.935877799987793 mm for frame 116

Lowest mean error: 2.9068784713745117 mm for frame 196

Saving results

Total time: 203.00195837020874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481880
Iteration 2/25 | Loss: 0.00100144
Iteration 3/25 | Loss: 0.00090044
Iteration 4/25 | Loss: 0.00089242
Iteration 5/25 | Loss: 0.00089032
Iteration 6/25 | Loss: 0.00088975
Iteration 7/25 | Loss: 0.00088975
Iteration 8/25 | Loss: 0.00088975
Iteration 9/25 | Loss: 0.00088975
Iteration 10/25 | Loss: 0.00088975
Iteration 11/25 | Loss: 0.00088975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008897469961084425, 0.0008897469961084425, 0.0008897469961084425, 0.0008897469961084425, 0.0008897469961084425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008897469961084425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66216636
Iteration 2/25 | Loss: 0.00204250
Iteration 3/25 | Loss: 0.00204248
Iteration 4/25 | Loss: 0.00204248
Iteration 5/25 | Loss: 0.00204248
Iteration 6/25 | Loss: 0.00204248
Iteration 7/25 | Loss: 0.00204248
Iteration 8/25 | Loss: 0.00204247
Iteration 9/25 | Loss: 0.00204247
Iteration 10/25 | Loss: 0.00204247
Iteration 11/25 | Loss: 0.00204247
Iteration 12/25 | Loss: 0.00204247
Iteration 13/25 | Loss: 0.00204247
Iteration 14/25 | Loss: 0.00204247
Iteration 15/25 | Loss: 0.00204247
Iteration 16/25 | Loss: 0.00204247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020424742251634598, 0.0020424742251634598, 0.0020424742251634598, 0.0020424742251634598, 0.0020424742251634598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020424742251634598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204247
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002116
Iteration 4/1000 | Loss: 0.00001811
Iteration 5/1000 | Loss: 0.00001737
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001639
Iteration 8/1000 | Loss: 0.00001607
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001566
Iteration 13/1000 | Loss: 0.00001566
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001562
Iteration 16/1000 | Loss: 0.00001561
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001561
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001560
Iteration 24/1000 | Loss: 0.00001559
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001559
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001556
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001552
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001551
Iteration 34/1000 | Loss: 0.00001550
Iteration 35/1000 | Loss: 0.00001550
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001549
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001544
Iteration 61/1000 | Loss: 0.00001544
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001544
Iteration 64/1000 | Loss: 0.00001544
Iteration 65/1000 | Loss: 0.00001544
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001543
Iteration 68/1000 | Loss: 0.00001543
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001543
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001542
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001541
Iteration 79/1000 | Loss: 0.00001541
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001540
Iteration 85/1000 | Loss: 0.00001540
Iteration 86/1000 | Loss: 0.00001540
Iteration 87/1000 | Loss: 0.00001540
Iteration 88/1000 | Loss: 0.00001540
Iteration 89/1000 | Loss: 0.00001540
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001540
Iteration 94/1000 | Loss: 0.00001540
Iteration 95/1000 | Loss: 0.00001540
Iteration 96/1000 | Loss: 0.00001540
Iteration 97/1000 | Loss: 0.00001540
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5403198631247506e-05, 1.5403198631247506e-05, 1.5403198631247506e-05, 1.5403198631247506e-05, 1.5403198631247506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5403198631247506e-05

Optimization complete. Final v2v error: 3.2433459758758545 mm

Highest mean error: 4.006464004516602 mm for frame 62

Lowest mean error: 2.8155131340026855 mm for frame 104

Saving results

Total time: 29.561965465545654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479323
Iteration 2/25 | Loss: 0.00102351
Iteration 3/25 | Loss: 0.00092190
Iteration 4/25 | Loss: 0.00091298
Iteration 5/25 | Loss: 0.00091021
Iteration 6/25 | Loss: 0.00090983
Iteration 7/25 | Loss: 0.00090983
Iteration 8/25 | Loss: 0.00090983
Iteration 9/25 | Loss: 0.00090983
Iteration 10/25 | Loss: 0.00090983
Iteration 11/25 | Loss: 0.00090983
Iteration 12/25 | Loss: 0.00090983
Iteration 13/25 | Loss: 0.00090983
Iteration 14/25 | Loss: 0.00090983
Iteration 15/25 | Loss: 0.00090983
Iteration 16/25 | Loss: 0.00090983
Iteration 17/25 | Loss: 0.00090983
Iteration 18/25 | Loss: 0.00090983
Iteration 19/25 | Loss: 0.00090983
Iteration 20/25 | Loss: 0.00090983
Iteration 21/25 | Loss: 0.00090983
Iteration 22/25 | Loss: 0.00090983
Iteration 23/25 | Loss: 0.00090983
Iteration 24/25 | Loss: 0.00090983
Iteration 25/25 | Loss: 0.00090983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66691005
Iteration 2/25 | Loss: 0.00226958
Iteration 3/25 | Loss: 0.00226958
Iteration 4/25 | Loss: 0.00226958
Iteration 5/25 | Loss: 0.00226957
Iteration 6/25 | Loss: 0.00226957
Iteration 7/25 | Loss: 0.00226957
Iteration 8/25 | Loss: 0.00226957
Iteration 9/25 | Loss: 0.00226957
Iteration 10/25 | Loss: 0.00226957
Iteration 11/25 | Loss: 0.00226957
Iteration 12/25 | Loss: 0.00226957
Iteration 13/25 | Loss: 0.00226957
Iteration 14/25 | Loss: 0.00226957
Iteration 15/25 | Loss: 0.00226957
Iteration 16/25 | Loss: 0.00226957
Iteration 17/25 | Loss: 0.00226957
Iteration 18/25 | Loss: 0.00226957
Iteration 19/25 | Loss: 0.00226957
Iteration 20/25 | Loss: 0.00226957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022695735096931458, 0.0022695735096931458, 0.0022695735096931458, 0.0022695735096931458, 0.0022695735096931458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022695735096931458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226957
Iteration 2/1000 | Loss: 0.00003209
Iteration 3/1000 | Loss: 0.00002216
Iteration 4/1000 | Loss: 0.00001985
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00001741
Iteration 8/1000 | Loss: 0.00001688
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001645
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00001616
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001596
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001593
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001593
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001591
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001588
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001587
Iteration 61/1000 | Loss: 0.00001587
Iteration 62/1000 | Loss: 0.00001587
Iteration 63/1000 | Loss: 0.00001587
Iteration 64/1000 | Loss: 0.00001587
Iteration 65/1000 | Loss: 0.00001586
Iteration 66/1000 | Loss: 0.00001586
Iteration 67/1000 | Loss: 0.00001586
Iteration 68/1000 | Loss: 0.00001586
Iteration 69/1000 | Loss: 0.00001586
Iteration 70/1000 | Loss: 0.00001586
Iteration 71/1000 | Loss: 0.00001586
Iteration 72/1000 | Loss: 0.00001586
Iteration 73/1000 | Loss: 0.00001586
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001584
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001584
Iteration 82/1000 | Loss: 0.00001583
Iteration 83/1000 | Loss: 0.00001583
Iteration 84/1000 | Loss: 0.00001583
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001582
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001579
Iteration 110/1000 | Loss: 0.00001579
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001578
Iteration 118/1000 | Loss: 0.00001578
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001577
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001577
Iteration 135/1000 | Loss: 0.00001577
Iteration 136/1000 | Loss: 0.00001577
Iteration 137/1000 | Loss: 0.00001577
Iteration 138/1000 | Loss: 0.00001577
Iteration 139/1000 | Loss: 0.00001577
Iteration 140/1000 | Loss: 0.00001577
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001577
Iteration 144/1000 | Loss: 0.00001577
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001577
Iteration 147/1000 | Loss: 0.00001577
Iteration 148/1000 | Loss: 0.00001577
Iteration 149/1000 | Loss: 0.00001577
Iteration 150/1000 | Loss: 0.00001577
Iteration 151/1000 | Loss: 0.00001577
Iteration 152/1000 | Loss: 0.00001577
Iteration 153/1000 | Loss: 0.00001577
Iteration 154/1000 | Loss: 0.00001577
Iteration 155/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.5765297575853765e-05, 1.5765297575853765e-05, 1.5765297575853765e-05, 1.5765297575853765e-05, 1.5765297575853765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5765297575853765e-05

Optimization complete. Final v2v error: 3.4117813110351562 mm

Highest mean error: 3.7395904064178467 mm for frame 45

Lowest mean error: 3.182018756866455 mm for frame 228

Saving results

Total time: 40.964879274368286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409090
Iteration 2/25 | Loss: 0.00114774
Iteration 3/25 | Loss: 0.00100168
Iteration 4/25 | Loss: 0.00097622
Iteration 5/25 | Loss: 0.00097174
Iteration 6/25 | Loss: 0.00097108
Iteration 7/25 | Loss: 0.00097108
Iteration 8/25 | Loss: 0.00097108
Iteration 9/25 | Loss: 0.00097108
Iteration 10/25 | Loss: 0.00097108
Iteration 11/25 | Loss: 0.00097108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009710832964628935, 0.0009710832964628935, 0.0009710832964628935, 0.0009710832964628935, 0.0009710832964628935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009710832964628935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70049131
Iteration 2/25 | Loss: 0.00319585
Iteration 3/25 | Loss: 0.00319584
Iteration 4/25 | Loss: 0.00319584
Iteration 5/25 | Loss: 0.00319584
Iteration 6/25 | Loss: 0.00319584
Iteration 7/25 | Loss: 0.00319584
Iteration 8/25 | Loss: 0.00319584
Iteration 9/25 | Loss: 0.00319584
Iteration 10/25 | Loss: 0.00319584
Iteration 11/25 | Loss: 0.00319584
Iteration 12/25 | Loss: 0.00319584
Iteration 13/25 | Loss: 0.00319584
Iteration 14/25 | Loss: 0.00319584
Iteration 15/25 | Loss: 0.00319584
Iteration 16/25 | Loss: 0.00319584
Iteration 17/25 | Loss: 0.00319584
Iteration 18/25 | Loss: 0.00319584
Iteration 19/25 | Loss: 0.00319584
Iteration 20/25 | Loss: 0.00319584
Iteration 21/25 | Loss: 0.00319584
Iteration 22/25 | Loss: 0.00319584
Iteration 23/25 | Loss: 0.00319584
Iteration 24/25 | Loss: 0.00319584
Iteration 25/25 | Loss: 0.00319584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319584
Iteration 2/1000 | Loss: 0.00003747
Iteration 3/1000 | Loss: 0.00002542
Iteration 4/1000 | Loss: 0.00002155
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001841
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001683
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001630
Iteration 14/1000 | Loss: 0.00001628
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001625
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001621
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001614
Iteration 25/1000 | Loss: 0.00001614
Iteration 26/1000 | Loss: 0.00001613
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001611
Iteration 30/1000 | Loss: 0.00001611
Iteration 31/1000 | Loss: 0.00001611
Iteration 32/1000 | Loss: 0.00001610
Iteration 33/1000 | Loss: 0.00001610
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001607
Iteration 37/1000 | Loss: 0.00001607
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001607
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001602
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001601
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001600
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001600
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001599
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001599
Iteration 72/1000 | Loss: 0.00001598
Iteration 73/1000 | Loss: 0.00001598
Iteration 74/1000 | Loss: 0.00001598
Iteration 75/1000 | Loss: 0.00001597
Iteration 76/1000 | Loss: 0.00001597
Iteration 77/1000 | Loss: 0.00001597
Iteration 78/1000 | Loss: 0.00001597
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001597
Iteration 81/1000 | Loss: 0.00001597
Iteration 82/1000 | Loss: 0.00001597
Iteration 83/1000 | Loss: 0.00001596
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001596
Iteration 86/1000 | Loss: 0.00001596
Iteration 87/1000 | Loss: 0.00001596
Iteration 88/1000 | Loss: 0.00001596
Iteration 89/1000 | Loss: 0.00001596
Iteration 90/1000 | Loss: 0.00001596
Iteration 91/1000 | Loss: 0.00001596
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001595
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001595
Iteration 103/1000 | Loss: 0.00001595
Iteration 104/1000 | Loss: 0.00001595
Iteration 105/1000 | Loss: 0.00001595
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00001595
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001594
Iteration 119/1000 | Loss: 0.00001593
Iteration 120/1000 | Loss: 0.00001593
Iteration 121/1000 | Loss: 0.00001593
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001593
Iteration 124/1000 | Loss: 0.00001593
Iteration 125/1000 | Loss: 0.00001593
Iteration 126/1000 | Loss: 0.00001593
Iteration 127/1000 | Loss: 0.00001593
Iteration 128/1000 | Loss: 0.00001593
Iteration 129/1000 | Loss: 0.00001593
Iteration 130/1000 | Loss: 0.00001593
Iteration 131/1000 | Loss: 0.00001593
Iteration 132/1000 | Loss: 0.00001592
Iteration 133/1000 | Loss: 0.00001592
Iteration 134/1000 | Loss: 0.00001592
Iteration 135/1000 | Loss: 0.00001592
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001591
Iteration 138/1000 | Loss: 0.00001591
Iteration 139/1000 | Loss: 0.00001591
Iteration 140/1000 | Loss: 0.00001591
Iteration 141/1000 | Loss: 0.00001591
Iteration 142/1000 | Loss: 0.00001591
Iteration 143/1000 | Loss: 0.00001591
Iteration 144/1000 | Loss: 0.00001591
Iteration 145/1000 | Loss: 0.00001591
Iteration 146/1000 | Loss: 0.00001591
Iteration 147/1000 | Loss: 0.00001591
Iteration 148/1000 | Loss: 0.00001591
Iteration 149/1000 | Loss: 0.00001591
Iteration 150/1000 | Loss: 0.00001591
Iteration 151/1000 | Loss: 0.00001591
Iteration 152/1000 | Loss: 0.00001591
Iteration 153/1000 | Loss: 0.00001591
Iteration 154/1000 | Loss: 0.00001591
Iteration 155/1000 | Loss: 0.00001591
Iteration 156/1000 | Loss: 0.00001591
Iteration 157/1000 | Loss: 0.00001591
Iteration 158/1000 | Loss: 0.00001591
Iteration 159/1000 | Loss: 0.00001591
Iteration 160/1000 | Loss: 0.00001591
Iteration 161/1000 | Loss: 0.00001591
Iteration 162/1000 | Loss: 0.00001591
Iteration 163/1000 | Loss: 0.00001591
Iteration 164/1000 | Loss: 0.00001591
Iteration 165/1000 | Loss: 0.00001591
Iteration 166/1000 | Loss: 0.00001591
Iteration 167/1000 | Loss: 0.00001591
Iteration 168/1000 | Loss: 0.00001591
Iteration 169/1000 | Loss: 0.00001591
Iteration 170/1000 | Loss: 0.00001591
Iteration 171/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.591319050930906e-05, 1.591319050930906e-05, 1.591319050930906e-05, 1.591319050930906e-05, 1.591319050930906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.591319050930906e-05

Optimization complete. Final v2v error: 3.3590121269226074 mm

Highest mean error: 3.880136489868164 mm for frame 58

Lowest mean error: 2.928952693939209 mm for frame 43

Saving results

Total time: 37.1291663646698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071940
Iteration 2/25 | Loss: 0.00172007
Iteration 3/25 | Loss: 0.00117940
Iteration 4/25 | Loss: 0.00113637
Iteration 5/25 | Loss: 0.00112504
Iteration 6/25 | Loss: 0.00112118
Iteration 7/25 | Loss: 0.00111995
Iteration 8/25 | Loss: 0.00111995
Iteration 9/25 | Loss: 0.00111995
Iteration 10/25 | Loss: 0.00111995
Iteration 11/25 | Loss: 0.00111995
Iteration 12/25 | Loss: 0.00111995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001119950320571661, 0.001119950320571661, 0.001119950320571661, 0.001119950320571661, 0.001119950320571661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001119950320571661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02323055
Iteration 2/25 | Loss: 0.00161687
Iteration 3/25 | Loss: 0.00161686
Iteration 4/25 | Loss: 0.00161686
Iteration 5/25 | Loss: 0.00161686
Iteration 6/25 | Loss: 0.00161686
Iteration 7/25 | Loss: 0.00161686
Iteration 8/25 | Loss: 0.00161686
Iteration 9/25 | Loss: 0.00161686
Iteration 10/25 | Loss: 0.00161686
Iteration 11/25 | Loss: 0.00161686
Iteration 12/25 | Loss: 0.00161686
Iteration 13/25 | Loss: 0.00161686
Iteration 14/25 | Loss: 0.00161686
Iteration 15/25 | Loss: 0.00161686
Iteration 16/25 | Loss: 0.00161686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016168582951650023, 0.0016168582951650023, 0.0016168582951650023, 0.0016168582951650023, 0.0016168582951650023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016168582951650023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161686
Iteration 2/1000 | Loss: 0.00006597
Iteration 3/1000 | Loss: 0.00004892
Iteration 4/1000 | Loss: 0.00004435
Iteration 5/1000 | Loss: 0.00004248
Iteration 6/1000 | Loss: 0.00004136
Iteration 7/1000 | Loss: 0.00004047
Iteration 8/1000 | Loss: 0.00003984
Iteration 9/1000 | Loss: 0.00003939
Iteration 10/1000 | Loss: 0.00003896
Iteration 11/1000 | Loss: 0.00003870
Iteration 12/1000 | Loss: 0.00003847
Iteration 13/1000 | Loss: 0.00003839
Iteration 14/1000 | Loss: 0.00003835
Iteration 15/1000 | Loss: 0.00003832
Iteration 16/1000 | Loss: 0.00003831
Iteration 17/1000 | Loss: 0.00003828
Iteration 18/1000 | Loss: 0.00003827
Iteration 19/1000 | Loss: 0.00003825
Iteration 20/1000 | Loss: 0.00003822
Iteration 21/1000 | Loss: 0.00003822
Iteration 22/1000 | Loss: 0.00003818
Iteration 23/1000 | Loss: 0.00003818
Iteration 24/1000 | Loss: 0.00003816
Iteration 25/1000 | Loss: 0.00003816
Iteration 26/1000 | Loss: 0.00003813
Iteration 27/1000 | Loss: 0.00003809
Iteration 28/1000 | Loss: 0.00003807
Iteration 29/1000 | Loss: 0.00003807
Iteration 30/1000 | Loss: 0.00003804
Iteration 31/1000 | Loss: 0.00003802
Iteration 32/1000 | Loss: 0.00003802
Iteration 33/1000 | Loss: 0.00003799
Iteration 34/1000 | Loss: 0.00003799
Iteration 35/1000 | Loss: 0.00003798
Iteration 36/1000 | Loss: 0.00003798
Iteration 37/1000 | Loss: 0.00003798
Iteration 38/1000 | Loss: 0.00003798
Iteration 39/1000 | Loss: 0.00003798
Iteration 40/1000 | Loss: 0.00003798
Iteration 41/1000 | Loss: 0.00003798
Iteration 42/1000 | Loss: 0.00003797
Iteration 43/1000 | Loss: 0.00003797
Iteration 44/1000 | Loss: 0.00003797
Iteration 45/1000 | Loss: 0.00003796
Iteration 46/1000 | Loss: 0.00003796
Iteration 47/1000 | Loss: 0.00003796
Iteration 48/1000 | Loss: 0.00003795
Iteration 49/1000 | Loss: 0.00003795
Iteration 50/1000 | Loss: 0.00003795
Iteration 51/1000 | Loss: 0.00003795
Iteration 52/1000 | Loss: 0.00003794
Iteration 53/1000 | Loss: 0.00003794
Iteration 54/1000 | Loss: 0.00003794
Iteration 55/1000 | Loss: 0.00003794
Iteration 56/1000 | Loss: 0.00003794
Iteration 57/1000 | Loss: 0.00003794
Iteration 58/1000 | Loss: 0.00003794
Iteration 59/1000 | Loss: 0.00003793
Iteration 60/1000 | Loss: 0.00003793
Iteration 61/1000 | Loss: 0.00003792
Iteration 62/1000 | Loss: 0.00003792
Iteration 63/1000 | Loss: 0.00003792
Iteration 64/1000 | Loss: 0.00003792
Iteration 65/1000 | Loss: 0.00003792
Iteration 66/1000 | Loss: 0.00003792
Iteration 67/1000 | Loss: 0.00003792
Iteration 68/1000 | Loss: 0.00003792
Iteration 69/1000 | Loss: 0.00003792
Iteration 70/1000 | Loss: 0.00003791
Iteration 71/1000 | Loss: 0.00003791
Iteration 72/1000 | Loss: 0.00003791
Iteration 73/1000 | Loss: 0.00003791
Iteration 74/1000 | Loss: 0.00003790
Iteration 75/1000 | Loss: 0.00003790
Iteration 76/1000 | Loss: 0.00003789
Iteration 77/1000 | Loss: 0.00003788
Iteration 78/1000 | Loss: 0.00003788
Iteration 79/1000 | Loss: 0.00003788
Iteration 80/1000 | Loss: 0.00003788
Iteration 81/1000 | Loss: 0.00003788
Iteration 82/1000 | Loss: 0.00003787
Iteration 83/1000 | Loss: 0.00003786
Iteration 84/1000 | Loss: 0.00003786
Iteration 85/1000 | Loss: 0.00003786
Iteration 86/1000 | Loss: 0.00003786
Iteration 87/1000 | Loss: 0.00003786
Iteration 88/1000 | Loss: 0.00003786
Iteration 89/1000 | Loss: 0.00003786
Iteration 90/1000 | Loss: 0.00003786
Iteration 91/1000 | Loss: 0.00003785
Iteration 92/1000 | Loss: 0.00003785
Iteration 93/1000 | Loss: 0.00003785
Iteration 94/1000 | Loss: 0.00003785
Iteration 95/1000 | Loss: 0.00003785
Iteration 96/1000 | Loss: 0.00003785
Iteration 97/1000 | Loss: 0.00003785
Iteration 98/1000 | Loss: 0.00003785
Iteration 99/1000 | Loss: 0.00003785
Iteration 100/1000 | Loss: 0.00003785
Iteration 101/1000 | Loss: 0.00003784
Iteration 102/1000 | Loss: 0.00003784
Iteration 103/1000 | Loss: 0.00003784
Iteration 104/1000 | Loss: 0.00003783
Iteration 105/1000 | Loss: 0.00003783
Iteration 106/1000 | Loss: 0.00003783
Iteration 107/1000 | Loss: 0.00003783
Iteration 108/1000 | Loss: 0.00003783
Iteration 109/1000 | Loss: 0.00003783
Iteration 110/1000 | Loss: 0.00003783
Iteration 111/1000 | Loss: 0.00003782
Iteration 112/1000 | Loss: 0.00003782
Iteration 113/1000 | Loss: 0.00003782
Iteration 114/1000 | Loss: 0.00003781
Iteration 115/1000 | Loss: 0.00003781
Iteration 116/1000 | Loss: 0.00003781
Iteration 117/1000 | Loss: 0.00003781
Iteration 118/1000 | Loss: 0.00003781
Iteration 119/1000 | Loss: 0.00003781
Iteration 120/1000 | Loss: 0.00003780
Iteration 121/1000 | Loss: 0.00003780
Iteration 122/1000 | Loss: 0.00003780
Iteration 123/1000 | Loss: 0.00003780
Iteration 124/1000 | Loss: 0.00003779
Iteration 125/1000 | Loss: 0.00003779
Iteration 126/1000 | Loss: 0.00003779
Iteration 127/1000 | Loss: 0.00003779
Iteration 128/1000 | Loss: 0.00003779
Iteration 129/1000 | Loss: 0.00003779
Iteration 130/1000 | Loss: 0.00003779
Iteration 131/1000 | Loss: 0.00003779
Iteration 132/1000 | Loss: 0.00003779
Iteration 133/1000 | Loss: 0.00003778
Iteration 134/1000 | Loss: 0.00003778
Iteration 135/1000 | Loss: 0.00003778
Iteration 136/1000 | Loss: 0.00003778
Iteration 137/1000 | Loss: 0.00003778
Iteration 138/1000 | Loss: 0.00003778
Iteration 139/1000 | Loss: 0.00003778
Iteration 140/1000 | Loss: 0.00003777
Iteration 141/1000 | Loss: 0.00003777
Iteration 142/1000 | Loss: 0.00003777
Iteration 143/1000 | Loss: 0.00003777
Iteration 144/1000 | Loss: 0.00003777
Iteration 145/1000 | Loss: 0.00003776
Iteration 146/1000 | Loss: 0.00003776
Iteration 147/1000 | Loss: 0.00003776
Iteration 148/1000 | Loss: 0.00003776
Iteration 149/1000 | Loss: 0.00003776
Iteration 150/1000 | Loss: 0.00003776
Iteration 151/1000 | Loss: 0.00003775
Iteration 152/1000 | Loss: 0.00003775
Iteration 153/1000 | Loss: 0.00003775
Iteration 154/1000 | Loss: 0.00003775
Iteration 155/1000 | Loss: 0.00003775
Iteration 156/1000 | Loss: 0.00003775
Iteration 157/1000 | Loss: 0.00003774
Iteration 158/1000 | Loss: 0.00003774
Iteration 159/1000 | Loss: 0.00003774
Iteration 160/1000 | Loss: 0.00003774
Iteration 161/1000 | Loss: 0.00003774
Iteration 162/1000 | Loss: 0.00003774
Iteration 163/1000 | Loss: 0.00003773
Iteration 164/1000 | Loss: 0.00003773
Iteration 165/1000 | Loss: 0.00003773
Iteration 166/1000 | Loss: 0.00003773
Iteration 167/1000 | Loss: 0.00003773
Iteration 168/1000 | Loss: 0.00003773
Iteration 169/1000 | Loss: 0.00003773
Iteration 170/1000 | Loss: 0.00003773
Iteration 171/1000 | Loss: 0.00003773
Iteration 172/1000 | Loss: 0.00003773
Iteration 173/1000 | Loss: 0.00003772
Iteration 174/1000 | Loss: 0.00003772
Iteration 175/1000 | Loss: 0.00003772
Iteration 176/1000 | Loss: 0.00003772
Iteration 177/1000 | Loss: 0.00003772
Iteration 178/1000 | Loss: 0.00003772
Iteration 179/1000 | Loss: 0.00003772
Iteration 180/1000 | Loss: 0.00003772
Iteration 181/1000 | Loss: 0.00003772
Iteration 182/1000 | Loss: 0.00003772
Iteration 183/1000 | Loss: 0.00003772
Iteration 184/1000 | Loss: 0.00003771
Iteration 185/1000 | Loss: 0.00003771
Iteration 186/1000 | Loss: 0.00003771
Iteration 187/1000 | Loss: 0.00003771
Iteration 188/1000 | Loss: 0.00003771
Iteration 189/1000 | Loss: 0.00003771
Iteration 190/1000 | Loss: 0.00003771
Iteration 191/1000 | Loss: 0.00003771
Iteration 192/1000 | Loss: 0.00003771
Iteration 193/1000 | Loss: 0.00003770
Iteration 194/1000 | Loss: 0.00003770
Iteration 195/1000 | Loss: 0.00003770
Iteration 196/1000 | Loss: 0.00003770
Iteration 197/1000 | Loss: 0.00003770
Iteration 198/1000 | Loss: 0.00003770
Iteration 199/1000 | Loss: 0.00003770
Iteration 200/1000 | Loss: 0.00003770
Iteration 201/1000 | Loss: 0.00003770
Iteration 202/1000 | Loss: 0.00003770
Iteration 203/1000 | Loss: 0.00003770
Iteration 204/1000 | Loss: 0.00003770
Iteration 205/1000 | Loss: 0.00003770
Iteration 206/1000 | Loss: 0.00003770
Iteration 207/1000 | Loss: 0.00003770
Iteration 208/1000 | Loss: 0.00003770
Iteration 209/1000 | Loss: 0.00003770
Iteration 210/1000 | Loss: 0.00003770
Iteration 211/1000 | Loss: 0.00003770
Iteration 212/1000 | Loss: 0.00003770
Iteration 213/1000 | Loss: 0.00003770
Iteration 214/1000 | Loss: 0.00003769
Iteration 215/1000 | Loss: 0.00003769
Iteration 216/1000 | Loss: 0.00003769
Iteration 217/1000 | Loss: 0.00003769
Iteration 218/1000 | Loss: 0.00003769
Iteration 219/1000 | Loss: 0.00003769
Iteration 220/1000 | Loss: 0.00003769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [3.7694728234782815e-05, 3.7694728234782815e-05, 3.7694728234782815e-05, 3.7694728234782815e-05, 3.7694728234782815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7694728234782815e-05

Optimization complete. Final v2v error: 4.9785614013671875 mm

Highest mean error: 5.774971008300781 mm for frame 196

Lowest mean error: 4.07243537902832 mm for frame 54

Saving results

Total time: 53.59650444984436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00307561
Iteration 2/25 | Loss: 0.00123472
Iteration 3/25 | Loss: 0.00100638
Iteration 4/25 | Loss: 0.00097361
Iteration 5/25 | Loss: 0.00096702
Iteration 6/25 | Loss: 0.00096544
Iteration 7/25 | Loss: 0.00096480
Iteration 8/25 | Loss: 0.00096466
Iteration 9/25 | Loss: 0.00096466
Iteration 10/25 | Loss: 0.00096466
Iteration 11/25 | Loss: 0.00096466
Iteration 12/25 | Loss: 0.00096466
Iteration 13/25 | Loss: 0.00096466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009646602557040751, 0.0009646602557040751, 0.0009646602557040751, 0.0009646602557040751, 0.0009646602557040751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009646602557040751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70367849
Iteration 2/25 | Loss: 0.00326728
Iteration 3/25 | Loss: 0.00326728
Iteration 4/25 | Loss: 0.00326728
Iteration 5/25 | Loss: 0.00326728
Iteration 6/25 | Loss: 0.00326728
Iteration 7/25 | Loss: 0.00326728
Iteration 8/25 | Loss: 0.00326728
Iteration 9/25 | Loss: 0.00326728
Iteration 10/25 | Loss: 0.00326728
Iteration 11/25 | Loss: 0.00326728
Iteration 12/25 | Loss: 0.00326728
Iteration 13/25 | Loss: 0.00326728
Iteration 14/25 | Loss: 0.00326728
Iteration 15/25 | Loss: 0.00326728
Iteration 16/25 | Loss: 0.00326728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00326727656647563, 0.00326727656647563, 0.00326727656647563, 0.00326727656647563, 0.00326727656647563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00326727656647563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326728
Iteration 2/1000 | Loss: 0.00004262
Iteration 3/1000 | Loss: 0.00002481
Iteration 4/1000 | Loss: 0.00002090
Iteration 5/1000 | Loss: 0.00001916
Iteration 6/1000 | Loss: 0.00001833
Iteration 7/1000 | Loss: 0.00001756
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001641
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001602
Iteration 13/1000 | Loss: 0.00001598
Iteration 14/1000 | Loss: 0.00001583
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001581
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001580
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001573
Iteration 23/1000 | Loss: 0.00001570
Iteration 24/1000 | Loss: 0.00001570
Iteration 25/1000 | Loss: 0.00001570
Iteration 26/1000 | Loss: 0.00001569
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001568
Iteration 29/1000 | Loss: 0.00001568
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001564
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001557
Iteration 37/1000 | Loss: 0.00001557
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001556
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001556
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001555
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001554
Iteration 47/1000 | Loss: 0.00001554
Iteration 48/1000 | Loss: 0.00001554
Iteration 49/1000 | Loss: 0.00001554
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001553
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001551
Iteration 63/1000 | Loss: 0.00001551
Iteration 64/1000 | Loss: 0.00001551
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001547
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001547
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001546
Iteration 91/1000 | Loss: 0.00001546
Iteration 92/1000 | Loss: 0.00001546
Iteration 93/1000 | Loss: 0.00001546
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001544
Iteration 99/1000 | Loss: 0.00001544
Iteration 100/1000 | Loss: 0.00001544
Iteration 101/1000 | Loss: 0.00001544
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00001542
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001541
Iteration 124/1000 | Loss: 0.00001541
Iteration 125/1000 | Loss: 0.00001541
Iteration 126/1000 | Loss: 0.00001541
Iteration 127/1000 | Loss: 0.00001541
Iteration 128/1000 | Loss: 0.00001541
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001541
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001541
Iteration 138/1000 | Loss: 0.00001541
Iteration 139/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5409796105814166e-05, 1.5409796105814166e-05, 1.5409796105814166e-05, 1.5409796105814166e-05, 1.5409796105814166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5409796105814166e-05

Optimization complete. Final v2v error: 3.3355345726013184 mm

Highest mean error: 3.650533437728882 mm for frame 0

Lowest mean error: 3.0285534858703613 mm for frame 216

Saving results

Total time: 45.215718507766724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495711
Iteration 2/25 | Loss: 0.00155964
Iteration 3/25 | Loss: 0.00122719
Iteration 4/25 | Loss: 0.00113635
Iteration 5/25 | Loss: 0.00109698
Iteration 6/25 | Loss: 0.00107682
Iteration 7/25 | Loss: 0.00106898
Iteration 8/25 | Loss: 0.00107035
Iteration 9/25 | Loss: 0.00105436
Iteration 10/25 | Loss: 0.00103272
Iteration 11/25 | Loss: 0.00103666
Iteration 12/25 | Loss: 0.00102048
Iteration 13/25 | Loss: 0.00103102
Iteration 14/25 | Loss: 0.00103135
Iteration 15/25 | Loss: 0.00103004
Iteration 16/25 | Loss: 0.00103589
Iteration 17/25 | Loss: 0.00102509
Iteration 18/25 | Loss: 0.00101830
Iteration 19/25 | Loss: 0.00101554
Iteration 20/25 | Loss: 0.00102547
Iteration 21/25 | Loss: 0.00102256
Iteration 22/25 | Loss: 0.00103998
Iteration 23/25 | Loss: 0.00103197
Iteration 24/25 | Loss: 0.00102559
Iteration 25/25 | Loss: 0.00101986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68012440
Iteration 2/25 | Loss: 0.00259557
Iteration 3/25 | Loss: 0.00259556
Iteration 4/25 | Loss: 0.00259556
Iteration 5/25 | Loss: 0.00259556
Iteration 6/25 | Loss: 0.00259556
Iteration 7/25 | Loss: 0.00259556
Iteration 8/25 | Loss: 0.00259556
Iteration 9/25 | Loss: 0.00259556
Iteration 10/25 | Loss: 0.00259556
Iteration 11/25 | Loss: 0.00259556
Iteration 12/25 | Loss: 0.00259556
Iteration 13/25 | Loss: 0.00259556
Iteration 14/25 | Loss: 0.00259556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0025955599267035723, 0.0025955599267035723, 0.0025955599267035723, 0.0025955599267035723, 0.0025955599267035723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025955599267035723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259556
Iteration 2/1000 | Loss: 0.00011962
Iteration 3/1000 | Loss: 0.00009141
Iteration 4/1000 | Loss: 0.00007173
Iteration 5/1000 | Loss: 0.00006129
Iteration 6/1000 | Loss: 0.00004549
Iteration 7/1000 | Loss: 0.00003543
Iteration 8/1000 | Loss: 0.00003569
Iteration 9/1000 | Loss: 0.00003562
Iteration 10/1000 | Loss: 0.00005311
Iteration 11/1000 | Loss: 0.00004333
Iteration 12/1000 | Loss: 0.00003773
Iteration 13/1000 | Loss: 0.00008201
Iteration 14/1000 | Loss: 0.00004967
Iteration 15/1000 | Loss: 0.00004693
Iteration 16/1000 | Loss: 0.00004524
Iteration 17/1000 | Loss: 0.00003841
Iteration 18/1000 | Loss: 0.00002992
Iteration 19/1000 | Loss: 0.00003111
Iteration 20/1000 | Loss: 0.00003302
Iteration 21/1000 | Loss: 0.00003303
Iteration 22/1000 | Loss: 0.00003952
Iteration 23/1000 | Loss: 0.00005110
Iteration 24/1000 | Loss: 0.00004452
Iteration 25/1000 | Loss: 0.00003836
Iteration 26/1000 | Loss: 0.00003457
Iteration 27/1000 | Loss: 0.00003058
Iteration 28/1000 | Loss: 0.00005333
Iteration 29/1000 | Loss: 0.00006590
Iteration 30/1000 | Loss: 0.00004984
Iteration 31/1000 | Loss: 0.00006022
Iteration 32/1000 | Loss: 0.00006447
Iteration 33/1000 | Loss: 0.00007116
Iteration 34/1000 | Loss: 0.00006492
Iteration 35/1000 | Loss: 0.00006558
Iteration 36/1000 | Loss: 0.00006152
Iteration 37/1000 | Loss: 0.00006631
Iteration 38/1000 | Loss: 0.00007046
Iteration 39/1000 | Loss: 0.00007775
Iteration 40/1000 | Loss: 0.00008458
Iteration 41/1000 | Loss: 0.00008179
Iteration 42/1000 | Loss: 0.00010382
Iteration 43/1000 | Loss: 0.00009513
Iteration 44/1000 | Loss: 0.00008391
Iteration 45/1000 | Loss: 0.00010578
Iteration 46/1000 | Loss: 0.00011139
Iteration 47/1000 | Loss: 0.00013115
Iteration 48/1000 | Loss: 0.00012870
Iteration 49/1000 | Loss: 0.00014324
Iteration 50/1000 | Loss: 0.00013258
Iteration 51/1000 | Loss: 0.00011710
Iteration 52/1000 | Loss: 0.00011014
Iteration 53/1000 | Loss: 0.00010352
Iteration 54/1000 | Loss: 0.00009102
Iteration 55/1000 | Loss: 0.00011355
Iteration 56/1000 | Loss: 0.00010480
Iteration 57/1000 | Loss: 0.00009835
Iteration 58/1000 | Loss: 0.00010827
Iteration 59/1000 | Loss: 0.00011360
Iteration 60/1000 | Loss: 0.00014376
Iteration 61/1000 | Loss: 0.00012228
Iteration 62/1000 | Loss: 0.00013542
Iteration 63/1000 | Loss: 0.00010955
Iteration 64/1000 | Loss: 0.00012589
Iteration 65/1000 | Loss: 0.00012017
Iteration 66/1000 | Loss: 0.00015805
Iteration 67/1000 | Loss: 0.00010880
Iteration 68/1000 | Loss: 0.00004117
Iteration 69/1000 | Loss: 0.00005073
Iteration 70/1000 | Loss: 0.00003282
Iteration 71/1000 | Loss: 0.00003170
Iteration 72/1000 | Loss: 0.00003397
Iteration 73/1000 | Loss: 0.00003642
Iteration 74/1000 | Loss: 0.00003846
Iteration 75/1000 | Loss: 0.00003836
Iteration 76/1000 | Loss: 0.00003280
Iteration 77/1000 | Loss: 0.00003274
Iteration 78/1000 | Loss: 0.00006572
Iteration 79/1000 | Loss: 0.00004340
Iteration 80/1000 | Loss: 0.00003134
Iteration 81/1000 | Loss: 0.00003743
Iteration 82/1000 | Loss: 0.00003543
Iteration 83/1000 | Loss: 0.00004387
Iteration 84/1000 | Loss: 0.00003954
Iteration 85/1000 | Loss: 0.00005764
Iteration 86/1000 | Loss: 0.00004137
Iteration 87/1000 | Loss: 0.00009276
Iteration 88/1000 | Loss: 0.00004908
Iteration 89/1000 | Loss: 0.00012188
Iteration 90/1000 | Loss: 0.00004976
Iteration 91/1000 | Loss: 0.00008419
Iteration 92/1000 | Loss: 0.00005955
Iteration 93/1000 | Loss: 0.00008173
Iteration 94/1000 | Loss: 0.00007320
Iteration 95/1000 | Loss: 0.00011681
Iteration 96/1000 | Loss: 0.00008458
Iteration 97/1000 | Loss: 0.00013276
Iteration 98/1000 | Loss: 0.00010391
Iteration 99/1000 | Loss: 0.00012390
Iteration 100/1000 | Loss: 0.00012702
Iteration 101/1000 | Loss: 0.00014899
Iteration 102/1000 | Loss: 0.00010768
Iteration 103/1000 | Loss: 0.00012462
Iteration 104/1000 | Loss: 0.00004800
Iteration 105/1000 | Loss: 0.00008817
Iteration 106/1000 | Loss: 0.00005123
Iteration 107/1000 | Loss: 0.00012131
Iteration 108/1000 | Loss: 0.00006079
Iteration 109/1000 | Loss: 0.00006453
Iteration 110/1000 | Loss: 0.00004197
Iteration 111/1000 | Loss: 0.00008443
Iteration 112/1000 | Loss: 0.00004716
Iteration 113/1000 | Loss: 0.00003900
Iteration 114/1000 | Loss: 0.00003257
Iteration 115/1000 | Loss: 0.00005225
Iteration 116/1000 | Loss: 0.00004580
Iteration 117/1000 | Loss: 0.00004713
Iteration 118/1000 | Loss: 0.00003352
Iteration 119/1000 | Loss: 0.00003075
Iteration 120/1000 | Loss: 0.00003206
Iteration 121/1000 | Loss: 0.00003195
Iteration 122/1000 | Loss: 0.00003149
Iteration 123/1000 | Loss: 0.00003300
Iteration 124/1000 | Loss: 0.00005738
Iteration 125/1000 | Loss: 0.00003932
Iteration 126/1000 | Loss: 0.00003814
Iteration 127/1000 | Loss: 0.00004352
Iteration 128/1000 | Loss: 0.00006056
Iteration 129/1000 | Loss: 0.00005658
Iteration 130/1000 | Loss: 0.00007761
Iteration 131/1000 | Loss: 0.00006864
Iteration 132/1000 | Loss: 0.00011163
Iteration 133/1000 | Loss: 0.00009104
Iteration 134/1000 | Loss: 0.00012901
Iteration 135/1000 | Loss: 0.00012107
Iteration 136/1000 | Loss: 0.00006417
Iteration 137/1000 | Loss: 0.00003964
Iteration 138/1000 | Loss: 0.00004457
Iteration 139/1000 | Loss: 0.00004506
Iteration 140/1000 | Loss: 0.00005051
Iteration 141/1000 | Loss: 0.00004539
Iteration 142/1000 | Loss: 0.00008478
Iteration 143/1000 | Loss: 0.00009553
Iteration 144/1000 | Loss: 0.00012472
Iteration 145/1000 | Loss: 0.00012148
Iteration 146/1000 | Loss: 0.00013889
Iteration 147/1000 | Loss: 0.00011800
Iteration 148/1000 | Loss: 0.00007386
Iteration 149/1000 | Loss: 0.00008045
Iteration 150/1000 | Loss: 0.00012635
Iteration 151/1000 | Loss: 0.00012265
Iteration 152/1000 | Loss: 0.00013420
Iteration 153/1000 | Loss: 0.00009356
Iteration 154/1000 | Loss: 0.00015563
Iteration 155/1000 | Loss: 0.00008338
Iteration 156/1000 | Loss: 0.00014618
Iteration 157/1000 | Loss: 0.00010892
Iteration 158/1000 | Loss: 0.00009316
Iteration 159/1000 | Loss: 0.00009773
Iteration 160/1000 | Loss: 0.00015329
Iteration 161/1000 | Loss: 0.00010746
Iteration 162/1000 | Loss: 0.00011214
Iteration 163/1000 | Loss: 0.00011768
Iteration 164/1000 | Loss: 0.00012596
Iteration 165/1000 | Loss: 0.00006228
Iteration 166/1000 | Loss: 0.00009800
Iteration 167/1000 | Loss: 0.00006717
Iteration 168/1000 | Loss: 0.00013336
Iteration 169/1000 | Loss: 0.00008714
Iteration 170/1000 | Loss: 0.00009789
Iteration 171/1000 | Loss: 0.00006746
Iteration 172/1000 | Loss: 0.00008881
Iteration 173/1000 | Loss: 0.00003922
Iteration 174/1000 | Loss: 0.00006619
Iteration 175/1000 | Loss: 0.00003799
Iteration 176/1000 | Loss: 0.00003980
Iteration 177/1000 | Loss: 0.00004501
Iteration 178/1000 | Loss: 0.00007559
Iteration 179/1000 | Loss: 0.00004645
Iteration 180/1000 | Loss: 0.00003800
Iteration 181/1000 | Loss: 0.00004215
Iteration 182/1000 | Loss: 0.00003869
Iteration 183/1000 | Loss: 0.00005756
Iteration 184/1000 | Loss: 0.00004537
Iteration 185/1000 | Loss: 0.00011930
Iteration 186/1000 | Loss: 0.00006774
Iteration 187/1000 | Loss: 0.00012728
Iteration 188/1000 | Loss: 0.00007527
Iteration 189/1000 | Loss: 0.00008882
Iteration 190/1000 | Loss: 0.00005863
Iteration 191/1000 | Loss: 0.00007368
Iteration 192/1000 | Loss: 0.00005181
Iteration 193/1000 | Loss: 0.00006632
Iteration 194/1000 | Loss: 0.00007259
Iteration 195/1000 | Loss: 0.00010658
Iteration 196/1000 | Loss: 0.00008877
Iteration 197/1000 | Loss: 0.00013405
Iteration 198/1000 | Loss: 0.00010440
Iteration 199/1000 | Loss: 0.00014485
Iteration 200/1000 | Loss: 0.00013084
Iteration 201/1000 | Loss: 0.00016447
Iteration 202/1000 | Loss: 0.00010640
Iteration 203/1000 | Loss: 0.00011842
Iteration 204/1000 | Loss: 0.00013629
Iteration 205/1000 | Loss: 0.00010336
Iteration 206/1000 | Loss: 0.00007230
Iteration 207/1000 | Loss: 0.00005812
Iteration 208/1000 | Loss: 0.00003047
Iteration 209/1000 | Loss: 0.00002956
Iteration 210/1000 | Loss: 0.00003497
Iteration 211/1000 | Loss: 0.00006274
Iteration 212/1000 | Loss: 0.00006255
Iteration 213/1000 | Loss: 0.00005932
Iteration 214/1000 | Loss: 0.00004738
Iteration 215/1000 | Loss: 0.00004577
Iteration 216/1000 | Loss: 0.00004563
Iteration 217/1000 | Loss: 0.00005242
Iteration 218/1000 | Loss: 0.00005144
Iteration 219/1000 | Loss: 0.00005254
Iteration 220/1000 | Loss: 0.00006551
Iteration 221/1000 | Loss: 0.00004319
Iteration 222/1000 | Loss: 0.00004338
Iteration 223/1000 | Loss: 0.00003416
Iteration 224/1000 | Loss: 0.00006417
Iteration 225/1000 | Loss: 0.00005517
Iteration 226/1000 | Loss: 0.00008675
Iteration 227/1000 | Loss: 0.00006566
Iteration 228/1000 | Loss: 0.00009981
Iteration 229/1000 | Loss: 0.00007565
Iteration 230/1000 | Loss: 0.00012468
Iteration 231/1000 | Loss: 0.00009269
Iteration 232/1000 | Loss: 0.00014208
Iteration 233/1000 | Loss: 0.00009926
Iteration 234/1000 | Loss: 0.00013358
Iteration 235/1000 | Loss: 0.00010491
Iteration 236/1000 | Loss: 0.00016466
Iteration 237/1000 | Loss: 0.00012700
Iteration 238/1000 | Loss: 0.00014186
Iteration 239/1000 | Loss: 0.00008459
Iteration 240/1000 | Loss: 0.00011918
Iteration 241/1000 | Loss: 0.00007514
Iteration 242/1000 | Loss: 0.00004937
Iteration 243/1000 | Loss: 0.00003653
Iteration 244/1000 | Loss: 0.00003011
Iteration 245/1000 | Loss: 0.00003157
Iteration 246/1000 | Loss: 0.00004045
Iteration 247/1000 | Loss: 0.00003376
Iteration 248/1000 | Loss: 0.00005625
Iteration 249/1000 | Loss: 0.00004035
Iteration 250/1000 | Loss: 0.00009117
Iteration 251/1000 | Loss: 0.00005692
Iteration 252/1000 | Loss: 0.00011703
Iteration 253/1000 | Loss: 0.00006424
Iteration 254/1000 | Loss: 0.00011756
Iteration 255/1000 | Loss: 0.00006044
Iteration 256/1000 | Loss: 0.00012616
Iteration 257/1000 | Loss: 0.00009454
Iteration 258/1000 | Loss: 0.00012573
Iteration 259/1000 | Loss: 0.00011533
Iteration 260/1000 | Loss: 0.00017837
Iteration 261/1000 | Loss: 0.00009844
Iteration 262/1000 | Loss: 0.00015724
Iteration 263/1000 | Loss: 0.00009048
Iteration 264/1000 | Loss: 0.00005977
Iteration 265/1000 | Loss: 0.00004643
Iteration 266/1000 | Loss: 0.00004811
Iteration 267/1000 | Loss: 0.00004725
Iteration 268/1000 | Loss: 0.00005287
Iteration 269/1000 | Loss: 0.00004996
Iteration 270/1000 | Loss: 0.00007834
Iteration 271/1000 | Loss: 0.00006389
Iteration 272/1000 | Loss: 0.00007515
Iteration 273/1000 | Loss: 0.00006058
Iteration 274/1000 | Loss: 0.00008755
Iteration 275/1000 | Loss: 0.00005145
Iteration 276/1000 | Loss: 0.00009020
Iteration 277/1000 | Loss: 0.00006568
Iteration 278/1000 | Loss: 0.00008316
Iteration 279/1000 | Loss: 0.00005905
Iteration 280/1000 | Loss: 0.00005139
Iteration 281/1000 | Loss: 0.00005208
Iteration 282/1000 | Loss: 0.00003456
Iteration 283/1000 | Loss: 0.00003795
Iteration 284/1000 | Loss: 0.00003222
Iteration 285/1000 | Loss: 0.00003810
Iteration 286/1000 | Loss: 0.00003826
Iteration 287/1000 | Loss: 0.00004604
Iteration 288/1000 | Loss: 0.00006179
Iteration 289/1000 | Loss: 0.00008319
Iteration 290/1000 | Loss: 0.00007310
Iteration 291/1000 | Loss: 0.00009126
Iteration 292/1000 | Loss: 0.00007309
Iteration 293/1000 | Loss: 0.00005818
Iteration 294/1000 | Loss: 0.00005740
Iteration 295/1000 | Loss: 0.00008682
Iteration 296/1000 | Loss: 0.00008211
Iteration 297/1000 | Loss: 0.00008395
Iteration 298/1000 | Loss: 0.00007427
Iteration 299/1000 | Loss: 0.00008796
Iteration 300/1000 | Loss: 0.00006233
Iteration 301/1000 | Loss: 0.00008750
Iteration 302/1000 | Loss: 0.00007125
Iteration 303/1000 | Loss: 0.00009659
Iteration 304/1000 | Loss: 0.00006919
Iteration 305/1000 | Loss: 0.00008237
Iteration 306/1000 | Loss: 0.00005327
Iteration 307/1000 | Loss: 0.00007409
Iteration 308/1000 | Loss: 0.00006378
Iteration 309/1000 | Loss: 0.00008794
Iteration 310/1000 | Loss: 0.00006811
Iteration 311/1000 | Loss: 0.00012546
Iteration 312/1000 | Loss: 0.00008064
Iteration 313/1000 | Loss: 0.00013735
Iteration 314/1000 | Loss: 0.00007780
Iteration 315/1000 | Loss: 0.00013351
Iteration 316/1000 | Loss: 0.00008697
Iteration 317/1000 | Loss: 0.00014794
Iteration 318/1000 | Loss: 0.00009597
Iteration 319/1000 | Loss: 0.00016613
Iteration 320/1000 | Loss: 0.00009869
Iteration 321/1000 | Loss: 0.00008602
Iteration 322/1000 | Loss: 0.00009670
Iteration 323/1000 | Loss: 0.00017081
Iteration 324/1000 | Loss: 0.00008752
Iteration 325/1000 | Loss: 0.00007709
Iteration 326/1000 | Loss: 0.00007510
Iteration 327/1000 | Loss: 0.00005046
Iteration 328/1000 | Loss: 0.00004522
Iteration 329/1000 | Loss: 0.00004747
Iteration 330/1000 | Loss: 0.00008086
Iteration 331/1000 | Loss: 0.00008397
Iteration 332/1000 | Loss: 0.00007912
Iteration 333/1000 | Loss: 0.00007231
Iteration 334/1000 | Loss: 0.00004708
Iteration 335/1000 | Loss: 0.00004123
Iteration 336/1000 | Loss: 0.00003120
Iteration 337/1000 | Loss: 0.00002806
Iteration 338/1000 | Loss: 0.00003154
Iteration 339/1000 | Loss: 0.00002850
Iteration 340/1000 | Loss: 0.00004671
Iteration 341/1000 | Loss: 0.00006038
Iteration 342/1000 | Loss: 0.00006750
Iteration 343/1000 | Loss: 0.00006785
Iteration 344/1000 | Loss: 0.00007804
Iteration 345/1000 | Loss: 0.00009241
Iteration 346/1000 | Loss: 0.00010548
Iteration 347/1000 | Loss: 0.00007726
Iteration 348/1000 | Loss: 0.00005485
Iteration 349/1000 | Loss: 0.00006071
Iteration 350/1000 | Loss: 0.00008564
Iteration 351/1000 | Loss: 0.00008205
Iteration 352/1000 | Loss: 0.00010731
Iteration 353/1000 | Loss: 0.00008219
Iteration 354/1000 | Loss: 0.00013976
Iteration 355/1000 | Loss: 0.00008528
Iteration 356/1000 | Loss: 0.00009023
Iteration 357/1000 | Loss: 0.00008428
Iteration 358/1000 | Loss: 0.00004193
Iteration 359/1000 | Loss: 0.00002748
Iteration 360/1000 | Loss: 0.00004316
Iteration 361/1000 | Loss: 0.00007054
Iteration 362/1000 | Loss: 0.00004678
Iteration 363/1000 | Loss: 0.00004979
Iteration 364/1000 | Loss: 0.00003067
Iteration 365/1000 | Loss: 0.00004225
Iteration 366/1000 | Loss: 0.00003518
Iteration 367/1000 | Loss: 0.00003604
Iteration 368/1000 | Loss: 0.00004116
Iteration 369/1000 | Loss: 0.00005000
Iteration 370/1000 | Loss: 0.00003001
Iteration 371/1000 | Loss: 0.00005746
Iteration 372/1000 | Loss: 0.00005878
Iteration 373/1000 | Loss: 0.00008006
Iteration 374/1000 | Loss: 0.00004201
Iteration 375/1000 | Loss: 0.00004106
Iteration 376/1000 | Loss: 0.00003309
Iteration 377/1000 | Loss: 0.00003319
Iteration 378/1000 | Loss: 0.00005005
Iteration 379/1000 | Loss: 0.00005077
Iteration 380/1000 | Loss: 0.00003550
Iteration 381/1000 | Loss: 0.00003241
Iteration 382/1000 | Loss: 0.00003041
Iteration 383/1000 | Loss: 0.00005597
Iteration 384/1000 | Loss: 0.00004981
Iteration 385/1000 | Loss: 0.00005735
Iteration 386/1000 | Loss: 0.00004739
Iteration 387/1000 | Loss: 0.00004964
Iteration 388/1000 | Loss: 0.00004538
Iteration 389/1000 | Loss: 0.00006649
Iteration 390/1000 | Loss: 0.00005513
Iteration 391/1000 | Loss: 0.00007481
Iteration 392/1000 | Loss: 0.00007683
Iteration 393/1000 | Loss: 0.00010609
Iteration 394/1000 | Loss: 0.00008215
Iteration 395/1000 | Loss: 0.00009350
Iteration 396/1000 | Loss: 0.00008290
Iteration 397/1000 | Loss: 0.00011432
Iteration 398/1000 | Loss: 0.00006323
Iteration 399/1000 | Loss: 0.00003645
Iteration 400/1000 | Loss: 0.00004034
Iteration 401/1000 | Loss: 0.00005259
Iteration 402/1000 | Loss: 0.00006927
Iteration 403/1000 | Loss: 0.00007485
Iteration 404/1000 | Loss: 0.00004735
Iteration 405/1000 | Loss: 0.00004511
Iteration 406/1000 | Loss: 0.00003520
Iteration 407/1000 | Loss: 0.00002969
Iteration 408/1000 | Loss: 0.00003527
Iteration 409/1000 | Loss: 0.00003366
Iteration 410/1000 | Loss: 0.00003266
Iteration 411/1000 | Loss: 0.00003384
Iteration 412/1000 | Loss: 0.00003623
Iteration 413/1000 | Loss: 0.00003380
Iteration 414/1000 | Loss: 0.00006530
Iteration 415/1000 | Loss: 0.00005185
Iteration 416/1000 | Loss: 0.00009370
Iteration 417/1000 | Loss: 0.00006530
Iteration 418/1000 | Loss: 0.00011399
Iteration 419/1000 | Loss: 0.00008838
Iteration 420/1000 | Loss: 0.00007135
Iteration 421/1000 | Loss: 0.00009067
Iteration 422/1000 | Loss: 0.00008571
Iteration 423/1000 | Loss: 0.00009063
Iteration 424/1000 | Loss: 0.00008115
Iteration 425/1000 | Loss: 0.00007663
Iteration 426/1000 | Loss: 0.00009439
Iteration 427/1000 | Loss: 0.00010328
Iteration 428/1000 | Loss: 0.00010288
Iteration 429/1000 | Loss: 0.00009808
Iteration 430/1000 | Loss: 0.00008399
Iteration 431/1000 | Loss: 0.00003547
Iteration 432/1000 | Loss: 0.00003464
Iteration 433/1000 | Loss: 0.00003156
Iteration 434/1000 | Loss: 0.00003010
Iteration 435/1000 | Loss: 0.00003936
Iteration 436/1000 | Loss: 0.00004466
Iteration 437/1000 | Loss: 0.00006624
Iteration 438/1000 | Loss: 0.00006506
Iteration 439/1000 | Loss: 0.00010340
Iteration 440/1000 | Loss: 0.00006817
Iteration 441/1000 | Loss: 0.00011213
Iteration 442/1000 | Loss: 0.00005672
Iteration 443/1000 | Loss: 0.00006357
Iteration 444/1000 | Loss: 0.00004886
Iteration 445/1000 | Loss: 0.00006911
Iteration 446/1000 | Loss: 0.00005240
Iteration 447/1000 | Loss: 0.00007064
Iteration 448/1000 | Loss: 0.00004271
Iteration 449/1000 | Loss: 0.00004888
Iteration 450/1000 | Loss: 0.00004303
Iteration 451/1000 | Loss: 0.00005166
Iteration 452/1000 | Loss: 0.00003274
Iteration 453/1000 | Loss: 0.00002858
Iteration 454/1000 | Loss: 0.00002818
Iteration 455/1000 | Loss: 0.00003304
Iteration 456/1000 | Loss: 0.00004488
Iteration 457/1000 | Loss: 0.00005868
Iteration 458/1000 | Loss: 0.00005839
Iteration 459/1000 | Loss: 0.00007609
Iteration 460/1000 | Loss: 0.00008053
Iteration 461/1000 | Loss: 0.00009380
Iteration 462/1000 | Loss: 0.00010459
Iteration 463/1000 | Loss: 0.00009537
Iteration 464/1000 | Loss: 0.00009148
Iteration 465/1000 | Loss: 0.00008809
Iteration 466/1000 | Loss: 0.00008454
Iteration 467/1000 | Loss: 0.00012660
Iteration 468/1000 | Loss: 0.00012667
Iteration 469/1000 | Loss: 0.00007187
Iteration 470/1000 | Loss: 0.00004496
Iteration 471/1000 | Loss: 0.00004675
Iteration 472/1000 | Loss: 0.00003389
Iteration 473/1000 | Loss: 0.00006345
Iteration 474/1000 | Loss: 0.00009388
Iteration 475/1000 | Loss: 0.00006605
Iteration 476/1000 | Loss: 0.00007049
Iteration 477/1000 | Loss: 0.00008214
Iteration 478/1000 | Loss: 0.00006098
Iteration 479/1000 | Loss: 0.00003725
Iteration 480/1000 | Loss: 0.00003846
Iteration 481/1000 | Loss: 0.00005725
Iteration 482/1000 | Loss: 0.00004156
Iteration 483/1000 | Loss: 0.00003944
Iteration 484/1000 | Loss: 0.00003338
Iteration 485/1000 | Loss: 0.00004971
Iteration 486/1000 | Loss: 0.00005506
Iteration 487/1000 | Loss: 0.00008936
Iteration 488/1000 | Loss: 0.00007563
Iteration 489/1000 | Loss: 0.00010321
Iteration 490/1000 | Loss: 0.00007803
Iteration 491/1000 | Loss: 0.00009605
Iteration 492/1000 | Loss: 0.00009694
Iteration 493/1000 | Loss: 0.00009508
Iteration 494/1000 | Loss: 0.00009619
Iteration 495/1000 | Loss: 0.00011470
Iteration 496/1000 | Loss: 0.00008131
Iteration 497/1000 | Loss: 0.00010946
Iteration 498/1000 | Loss: 0.00010019
Iteration 499/1000 | Loss: 0.00009785
Iteration 500/1000 | Loss: 0.00007315
Iteration 501/1000 | Loss: 0.00012597
Iteration 502/1000 | Loss: 0.00007494
Iteration 503/1000 | Loss: 0.00003239
Iteration 504/1000 | Loss: 0.00005266
Iteration 505/1000 | Loss: 0.00003816
Iteration 506/1000 | Loss: 0.00003091
Iteration 507/1000 | Loss: 0.00002969
Iteration 508/1000 | Loss: 0.00003344
Iteration 509/1000 | Loss: 0.00003203
Iteration 510/1000 | Loss: 0.00004436
Iteration 511/1000 | Loss: 0.00004794
Iteration 512/1000 | Loss: 0.00006602
Iteration 513/1000 | Loss: 0.00006230
Iteration 514/1000 | Loss: 0.00006488
Iteration 515/1000 | Loss: 0.00006905
Iteration 516/1000 | Loss: 0.00005550
Iteration 517/1000 | Loss: 0.00006712
Iteration 518/1000 | Loss: 0.00008129
Iteration 519/1000 | Loss: 0.00007650
Iteration 520/1000 | Loss: 0.00009407
Iteration 521/1000 | Loss: 0.00007784
Iteration 522/1000 | Loss: 0.00007607
Iteration 523/1000 | Loss: 0.00006681
Iteration 524/1000 | Loss: 0.00010069
Iteration 525/1000 | Loss: 0.00008877
Iteration 526/1000 | Loss: 0.00008687
Iteration 527/1000 | Loss: 0.00008320
Iteration 528/1000 | Loss: 0.00008729
Iteration 529/1000 | Loss: 0.00008778
Iteration 530/1000 | Loss: 0.00013823
Iteration 531/1000 | Loss: 0.00009283
Iteration 532/1000 | Loss: 0.00010902
Iteration 533/1000 | Loss: 0.00008145
Iteration 534/1000 | Loss: 0.00007660
Iteration 535/1000 | Loss: 0.00006408
Iteration 536/1000 | Loss: 0.00012681
Iteration 537/1000 | Loss: 0.00009611
Iteration 538/1000 | Loss: 0.00008771
Iteration 539/1000 | Loss: 0.00006920
Iteration 540/1000 | Loss: 0.00010393
Iteration 541/1000 | Loss: 0.00007617
Iteration 542/1000 | Loss: 0.00012536
Iteration 543/1000 | Loss: 0.00009100
Iteration 544/1000 | Loss: 0.00011639
Iteration 545/1000 | Loss: 0.00008854
Iteration 546/1000 | Loss: 0.00014331
Iteration 547/1000 | Loss: 0.00007343
Iteration 548/1000 | Loss: 0.00011668
Iteration 549/1000 | Loss: 0.00008489
Iteration 550/1000 | Loss: 0.00016964
Iteration 551/1000 | Loss: 0.00010088
Iteration 552/1000 | Loss: 0.00015035
Iteration 553/1000 | Loss: 0.00011808
Iteration 554/1000 | Loss: 0.00012480
Iteration 555/1000 | Loss: 0.00010867
Iteration 556/1000 | Loss: 0.00016282
Iteration 557/1000 | Loss: 0.00010999
Iteration 558/1000 | Loss: 0.00009717
Iteration 559/1000 | Loss: 0.00009875
Iteration 560/1000 | Loss: 0.00008546
Iteration 561/1000 | Loss: 0.00007763
Iteration 562/1000 | Loss: 0.00008696
Iteration 563/1000 | Loss: 0.00008540
Iteration 564/1000 | Loss: 0.00009482
Iteration 565/1000 | Loss: 0.00006684
Iteration 566/1000 | Loss: 0.00003657
Iteration 567/1000 | Loss: 0.00003197
Iteration 568/1000 | Loss: 0.00002938
Iteration 569/1000 | Loss: 0.00003269
Iteration 570/1000 | Loss: 0.00003254
Iteration 571/1000 | Loss: 0.00004193
Iteration 572/1000 | Loss: 0.00004648
Iteration 573/1000 | Loss: 0.00005674
Iteration 574/1000 | Loss: 0.00003513
Iteration 575/1000 | Loss: 0.00003216
Iteration 576/1000 | Loss: 0.00003352
Iteration 577/1000 | Loss: 0.00004013
Iteration 578/1000 | Loss: 0.00004929
Iteration 579/1000 | Loss: 0.00005500
Iteration 580/1000 | Loss: 0.00005343
Iteration 581/1000 | Loss: 0.00005072
Iteration 582/1000 | Loss: 0.00005372
Iteration 583/1000 | Loss: 0.00004388
Iteration 584/1000 | Loss: 0.00006222
Iteration 585/1000 | Loss: 0.00006102
Iteration 586/1000 | Loss: 0.00005745
Iteration 587/1000 | Loss: 0.00006036
Iteration 588/1000 | Loss: 0.00006147
Iteration 589/1000 | Loss: 0.00006878
Iteration 590/1000 | Loss: 0.00006378
Iteration 591/1000 | Loss: 0.00004850
Iteration 592/1000 | Loss: 0.00006644
Iteration 593/1000 | Loss: 0.00003757
Iteration 594/1000 | Loss: 0.00003242
Iteration 595/1000 | Loss: 0.00004061
Iteration 596/1000 | Loss: 0.00005147
Iteration 597/1000 | Loss: 0.00005684
Iteration 598/1000 | Loss: 0.00004447
Iteration 599/1000 | Loss: 0.00004595
Iteration 600/1000 | Loss: 0.00003944
Iteration 601/1000 | Loss: 0.00003964
Iteration 602/1000 | Loss: 0.00003572
Iteration 603/1000 | Loss: 0.00003927
Iteration 604/1000 | Loss: 0.00004807
Iteration 605/1000 | Loss: 0.00006378
Iteration 606/1000 | Loss: 0.00005664
Iteration 607/1000 | Loss: 0.00006773
Iteration 608/1000 | Loss: 0.00006252
Iteration 609/1000 | Loss: 0.00008308
Iteration 610/1000 | Loss: 0.00007744
Iteration 611/1000 | Loss: 0.00009531
Iteration 612/1000 | Loss: 0.00007678
Iteration 613/1000 | Loss: 0.00006216
Iteration 614/1000 | Loss: 0.00007336
Iteration 615/1000 | Loss: 0.00009708
Iteration 616/1000 | Loss: 0.00009383
Iteration 617/1000 | Loss: 0.00010566
Iteration 618/1000 | Loss: 0.00008988
Iteration 619/1000 | Loss: 0.00008877
Iteration 620/1000 | Loss: 0.00009755
Iteration 621/1000 | Loss: 0.00011437
Iteration 622/1000 | Loss: 0.00010623
Iteration 623/1000 | Loss: 0.00010377
Iteration 624/1000 | Loss: 0.00004437
Iteration 625/1000 | Loss: 0.00003468
Iteration 626/1000 | Loss: 0.00004989
Iteration 627/1000 | Loss: 0.00005712
Iteration 628/1000 | Loss: 0.00004786
Iteration 629/1000 | Loss: 0.00003378
Iteration 630/1000 | Loss: 0.00004744
Iteration 631/1000 | Loss: 0.00004083
Iteration 632/1000 | Loss: 0.00004810
Iteration 633/1000 | Loss: 0.00003922
Iteration 634/1000 | Loss: 0.00003379
Iteration 635/1000 | Loss: 0.00003677
Iteration 636/1000 | Loss: 0.00003276
Iteration 637/1000 | Loss: 0.00003630
Iteration 638/1000 | Loss: 0.00004654
Iteration 639/1000 | Loss: 0.00003873
Iteration 640/1000 | Loss: 0.00004663
Iteration 641/1000 | Loss: 0.00003697
Iteration 642/1000 | Loss: 0.00003554
Iteration 643/1000 | Loss: 0.00004136
Iteration 644/1000 | Loss: 0.00004015
Iteration 645/1000 | Loss: 0.00003080
Iteration 646/1000 | Loss: 0.00002925
Iteration 647/1000 | Loss: 0.00003125
Iteration 648/1000 | Loss: 0.00003651
Iteration 649/1000 | Loss: 0.00004182
Iteration 650/1000 | Loss: 0.00003515
Iteration 651/1000 | Loss: 0.00002808
Iteration 652/1000 | Loss: 0.00002830
Iteration 653/1000 | Loss: 0.00003079
Iteration 654/1000 | Loss: 0.00003949
Iteration 655/1000 | Loss: 0.00003426
Iteration 656/1000 | Loss: 0.00008607
Iteration 657/1000 | Loss: 0.00004945
Iteration 658/1000 | Loss: 0.00008170
Iteration 659/1000 | Loss: 0.00006523
Iteration 660/1000 | Loss: 0.00008504
Iteration 661/1000 | Loss: 0.00005693
Iteration 662/1000 | Loss: 0.00010225
Iteration 663/1000 | Loss: 0.00008340
Iteration 664/1000 | Loss: 0.00010505
Iteration 665/1000 | Loss: 0.00007826
Iteration 666/1000 | Loss: 0.00009368
Iteration 667/1000 | Loss: 0.00007902
Iteration 668/1000 | Loss: 0.00009885
Iteration 669/1000 | Loss: 0.00005447
Iteration 670/1000 | Loss: 0.00008138
Iteration 671/1000 | Loss: 0.00007261
Iteration 672/1000 | Loss: 0.00008612
Iteration 673/1000 | Loss: 0.00007007
Iteration 674/1000 | Loss: 0.00011098
Iteration 675/1000 | Loss: 0.00009838
Iteration 676/1000 | Loss: 0.00009423
Iteration 677/1000 | Loss: 0.00006296
Iteration 678/1000 | Loss: 0.00012854
Iteration 679/1000 | Loss: 0.00009217
Iteration 680/1000 | Loss: 0.00011313
Iteration 681/1000 | Loss: 0.00010815
Iteration 682/1000 | Loss: 0.00013105
Iteration 683/1000 | Loss: 0.00006896
Iteration 684/1000 | Loss: 0.00009849
Iteration 685/1000 | Loss: 0.00004040
Iteration 686/1000 | Loss: 0.00003495
Iteration 687/1000 | Loss: 0.00003451
Iteration 688/1000 | Loss: 0.00003095
Iteration 689/1000 | Loss: 0.00003055
Iteration 690/1000 | Loss: 0.00002959
Iteration 691/1000 | Loss: 0.00003469
Iteration 692/1000 | Loss: 0.00003518
Iteration 693/1000 | Loss: 0.00004742
Iteration 694/1000 | Loss: 0.00004855
Iteration 695/1000 | Loss: 0.00005745
Iteration 696/1000 | Loss: 0.00007445
Iteration 697/1000 | Loss: 0.00007112
Iteration 698/1000 | Loss: 0.00009762
Iteration 699/1000 | Loss: 0.00009394
Iteration 700/1000 | Loss: 0.00010982
Iteration 701/1000 | Loss: 0.00009862
Iteration 702/1000 | Loss: 0.00016741
Iteration 703/1000 | Loss: 0.00010314
Iteration 704/1000 | Loss: 0.00014184
Iteration 705/1000 | Loss: 0.00011576
Iteration 706/1000 | Loss: 0.00015341
Iteration 707/1000 | Loss: 0.00012715
Iteration 708/1000 | Loss: 0.00011271
Iteration 709/1000 | Loss: 0.00008317
Iteration 710/1000 | Loss: 0.00011195
Iteration 711/1000 | Loss: 0.00009679
Iteration 712/1000 | Loss: 0.00011529
Iteration 713/1000 | Loss: 0.00011942
Iteration 714/1000 | Loss: 0.00013869
Iteration 715/1000 | Loss: 0.00007430
Iteration 716/1000 | Loss: 0.00004500
Iteration 717/1000 | Loss: 0.00004564
Iteration 718/1000 | Loss: 0.00003858
Iteration 719/1000 | Loss: 0.00008078
Iteration 720/1000 | Loss: 0.00006211
Iteration 721/1000 | Loss: 0.00013605
Iteration 722/1000 | Loss: 0.00009403
Iteration 723/1000 | Loss: 0.00015112
Iteration 724/1000 | Loss: 0.00011188
Iteration 725/1000 | Loss: 0.00008256
Iteration 726/1000 | Loss: 0.00005949
Iteration 727/1000 | Loss: 0.00004977
Iteration 728/1000 | Loss: 0.00012328
Iteration 729/1000 | Loss: 0.00012254
Iteration 730/1000 | Loss: 0.00007327
Iteration 731/1000 | Loss: 0.00007992
Iteration 732/1000 | Loss: 0.00004148
Iteration 733/1000 | Loss: 0.00005907
Iteration 734/1000 | Loss: 0.00003752
Iteration 735/1000 | Loss: 0.00002862
Iteration 736/1000 | Loss: 0.00003555
Iteration 737/1000 | Loss: 0.00003916
Iteration 738/1000 | Loss: 0.00007517
Iteration 739/1000 | Loss: 0.00005632
Iteration 740/1000 | Loss: 0.00008332
Iteration 741/1000 | Loss: 0.00005955
Iteration 742/1000 | Loss: 0.00007918
Iteration 743/1000 | Loss: 0.00006135
Iteration 744/1000 | Loss: 0.00010290
Iteration 745/1000 | Loss: 0.00006370
Iteration 746/1000 | Loss: 0.00009792
Iteration 747/1000 | Loss: 0.00007575
Iteration 748/1000 | Loss: 0.00014351
Iteration 749/1000 | Loss: 0.00008779
Iteration 750/1000 | Loss: 0.00015228
Iteration 751/1000 | Loss: 0.00009586
Iteration 752/1000 | Loss: 0.00012078
Iteration 753/1000 | Loss: 0.00007013
Iteration 754/1000 | Loss: 0.00012468
Iteration 755/1000 | Loss: 0.00009531
Iteration 756/1000 | Loss: 0.00015071
Iteration 757/1000 | Loss: 0.00009945
Iteration 758/1000 | Loss: 0.00009744
Iteration 759/1000 | Loss: 0.00008060
Iteration 760/1000 | Loss: 0.00006767
Iteration 761/1000 | Loss: 0.00008367
Iteration 762/1000 | Loss: 0.00008009
Iteration 763/1000 | Loss: 0.00009364
Iteration 764/1000 | Loss: 0.00008201
Iteration 765/1000 | Loss: 0.00008637
Iteration 766/1000 | Loss: 0.00009680
Iteration 767/1000 | Loss: 0.00007508
Iteration 768/1000 | Loss: 0.00008007
Iteration 769/1000 | Loss: 0.00009513
Iteration 770/1000 | Loss: 0.00009360
Iteration 771/1000 | Loss: 0.00011309
Iteration 772/1000 | Loss: 0.00011992
Iteration 773/1000 | Loss: 0.00012534
Iteration 774/1000 | Loss: 0.00011733
Iteration 775/1000 | Loss: 0.00009879
Iteration 776/1000 | Loss: 0.00009776
Iteration 777/1000 | Loss: 0.00007776
Iteration 778/1000 | Loss: 0.00005278
Iteration 779/1000 | Loss: 0.00004029
Iteration 780/1000 | Loss: 0.00003593
Iteration 781/1000 | Loss: 0.00003418
Iteration 782/1000 | Loss: 0.00006256
Iteration 783/1000 | Loss: 0.00009434
Iteration 784/1000 | Loss: 0.00009396
Iteration 785/1000 | Loss: 0.00010254
Iteration 786/1000 | Loss: 0.00011169
Iteration 787/1000 | Loss: 0.00011741
Iteration 788/1000 | Loss: 0.00010538
Iteration 789/1000 | Loss: 0.00009835
Iteration 790/1000 | Loss: 0.00008587
Iteration 791/1000 | Loss: 0.00003555
Iteration 792/1000 | Loss: 0.00009310
Iteration 793/1000 | Loss: 0.00010070
Iteration 794/1000 | Loss: 0.00014694
Iteration 795/1000 | Loss: 0.00010047
Iteration 796/1000 | Loss: 0.00013837
Iteration 797/1000 | Loss: 0.00009117
Iteration 798/1000 | Loss: 0.00014442
Iteration 799/1000 | Loss: 0.00010673
Iteration 800/1000 | Loss: 0.00014036
Iteration 801/1000 | Loss: 0.00009063
Iteration 802/1000 | Loss: 0.00010386
Iteration 803/1000 | Loss: 0.00008921
Iteration 804/1000 | Loss: 0.00014849
Iteration 805/1000 | Loss: 0.00011144
Iteration 806/1000 | Loss: 0.00016742
Iteration 807/1000 | Loss: 0.00010395
Iteration 808/1000 | Loss: 0.00003956
Iteration 809/1000 | Loss: 0.00006758
Iteration 810/1000 | Loss: 0.00005846
Iteration 811/1000 | Loss: 0.00010574
Iteration 812/1000 | Loss: 0.00010376
Iteration 813/1000 | Loss: 0.00013849
Iteration 814/1000 | Loss: 0.00011128
Iteration 815/1000 | Loss: 0.00007932
Iteration 816/1000 | Loss: 0.00004539
Iteration 817/1000 | Loss: 0.00005646
Iteration 818/1000 | Loss: 0.00003694
Iteration 819/1000 | Loss: 0.00003260
Iteration 820/1000 | Loss: 0.00002877
Iteration 821/1000 | Loss: 0.00003267
Iteration 822/1000 | Loss: 0.00006368
Iteration 823/1000 | Loss: 0.00005861
Iteration 824/1000 | Loss: 0.00008634
Iteration 825/1000 | Loss: 0.00009493
Iteration 826/1000 | Loss: 0.00005315
Iteration 827/1000 | Loss: 0.00004905
Iteration 828/1000 | Loss: 0.00003072
Iteration 829/1000 | Loss: 0.00002752
Iteration 830/1000 | Loss: 0.00002826
Iteration 831/1000 | Loss: 0.00002672
Iteration 832/1000 | Loss: 0.00004214
Iteration 833/1000 | Loss: 0.00005898
Iteration 834/1000 | Loss: 0.00005848
Iteration 835/1000 | Loss: 0.00006716
Iteration 836/1000 | Loss: 0.00009637
Iteration 837/1000 | Loss: 0.00010270
Iteration 838/1000 | Loss: 0.00012720
Iteration 839/1000 | Loss: 0.00011995
Iteration 840/1000 | Loss: 0.00014218
Iteration 841/1000 | Loss: 0.00009939
Iteration 842/1000 | Loss: 0.00014725
Iteration 843/1000 | Loss: 0.00009512
Iteration 844/1000 | Loss: 0.00015887
Iteration 845/1000 | Loss: 0.00011139
Iteration 846/1000 | Loss: 0.00010932
Iteration 847/1000 | Loss: 0.00009152
Iteration 848/1000 | Loss: 0.00012770
Iteration 849/1000 | Loss: 0.00011266
Iteration 850/1000 | Loss: 0.00013352
Iteration 851/1000 | Loss: 0.00017945
Iteration 852/1000 | Loss: 0.00010601
Iteration 853/1000 | Loss: 0.00012667
Iteration 854/1000 | Loss: 0.00014252
Iteration 855/1000 | Loss: 0.00010310
Iteration 856/1000 | Loss: 0.00012151
Iteration 857/1000 | Loss: 0.00015482
Iteration 858/1000 | Loss: 0.00011665
Iteration 859/1000 | Loss: 0.00010145
Iteration 860/1000 | Loss: 0.00011318
Iteration 861/1000 | Loss: 0.00012571
Iteration 862/1000 | Loss: 0.00011435
Iteration 863/1000 | Loss: 0.00007417
Iteration 864/1000 | Loss: 0.00007801
Iteration 865/1000 | Loss: 0.00005268
Iteration 866/1000 | Loss: 0.00005417
Iteration 867/1000 | Loss: 0.00005245
Iteration 868/1000 | Loss: 0.00004828
Iteration 869/1000 | Loss: 0.00006356
Iteration 870/1000 | Loss: 0.00005129
Iteration 871/1000 | Loss: 0.00003157
Iteration 872/1000 | Loss: 0.00004726
Iteration 873/1000 | Loss: 0.00004488
Iteration 874/1000 | Loss: 0.00006835
Iteration 875/1000 | Loss: 0.00007066
Iteration 876/1000 | Loss: 0.00005303
Iteration 877/1000 | Loss: 0.00006137
Iteration 878/1000 | Loss: 0.00006904
Iteration 879/1000 | Loss: 0.00005774
Iteration 880/1000 | Loss: 0.00005359
Iteration 881/1000 | Loss: 0.00004839
Iteration 882/1000 | Loss: 0.00003785
Iteration 883/1000 | Loss: 0.00005473
Iteration 884/1000 | Loss: 0.00005579
Iteration 885/1000 | Loss: 0.00007436
Iteration 886/1000 | Loss: 0.00007263
Iteration 887/1000 | Loss: 0.00007582
Iteration 888/1000 | Loss: 0.00007840
Iteration 889/1000 | Loss: 0.00008324
Iteration 890/1000 | Loss: 0.00008333
Iteration 891/1000 | Loss: 0.00008891
Iteration 892/1000 | Loss: 0.00009433
Iteration 893/1000 | Loss: 0.00009509
Iteration 894/1000 | Loss: 0.00009615
Iteration 895/1000 | Loss: 0.00010504
Iteration 896/1000 | Loss: 0.00013789
Iteration 897/1000 | Loss: 0.00007015
Iteration 898/1000 | Loss: 0.00005416
Iteration 899/1000 | Loss: 0.00002989
Iteration 900/1000 | Loss: 0.00002780
Iteration 901/1000 | Loss: 0.00003308
Iteration 902/1000 | Loss: 0.00004899
Iteration 903/1000 | Loss: 0.00004919
Iteration 904/1000 | Loss: 0.00003769
Iteration 905/1000 | Loss: 0.00006436
Iteration 906/1000 | Loss: 0.00006774
Iteration 907/1000 | Loss: 0.00007580
Iteration 908/1000 | Loss: 0.00007678
Iteration 909/1000 | Loss: 0.00009249
Iteration 910/1000 | Loss: 0.00007796
Iteration 911/1000 | Loss: 0.00008217
Iteration 912/1000 | Loss: 0.00006769
Iteration 913/1000 | Loss: 0.00008244
Iteration 914/1000 | Loss: 0.00006388
Iteration 915/1000 | Loss: 0.00008065
Iteration 916/1000 | Loss: 0.00007199
Iteration 917/1000 | Loss: 0.00006975
Iteration 918/1000 | Loss: 0.00009886
Iteration 919/1000 | Loss: 0.00009815
Iteration 920/1000 | Loss: 0.00010779
Iteration 921/1000 | Loss: 0.00008110
Iteration 922/1000 | Loss: 0.00009830
Iteration 923/1000 | Loss: 0.00010780
Iteration 924/1000 | Loss: 0.00010158
Iteration 925/1000 | Loss: 0.00008311
Iteration 926/1000 | Loss: 0.00010585
Iteration 927/1000 | Loss: 0.00009340
Iteration 928/1000 | Loss: 0.00010735
Iteration 929/1000 | Loss: 0.00010500
Iteration 930/1000 | Loss: 0.00011585
Iteration 931/1000 | Loss: 0.00012714
Iteration 932/1000 | Loss: 0.00009138
Iteration 933/1000 | Loss: 0.00011469
Iteration 934/1000 | Loss: 0.00012135
Iteration 935/1000 | Loss: 0.00015879
Iteration 936/1000 | Loss: 0.00009273
Iteration 937/1000 | Loss: 0.00010663
Iteration 938/1000 | Loss: 0.00010443
Iteration 939/1000 | Loss: 0.00014202
Iteration 940/1000 | Loss: 0.00006177
Iteration 941/1000 | Loss: 0.00004233
Iteration 942/1000 | Loss: 0.00006265
Iteration 943/1000 | Loss: 0.00008887
Iteration 944/1000 | Loss: 0.00008876
Iteration 945/1000 | Loss: 0.00013737
Iteration 946/1000 | Loss: 0.00009099
Iteration 947/1000 | Loss: 0.00009647
Iteration 948/1000 | Loss: 0.00009278
Iteration 949/1000 | Loss: 0.00004261
Iteration 950/1000 | Loss: 0.00008936
Iteration 951/1000 | Loss: 0.00007609
Iteration 952/1000 | Loss: 0.00008794
Iteration 953/1000 | Loss: 0.00007735
Iteration 954/1000 | Loss: 0.00003458
Iteration 955/1000 | Loss: 0.00003138
Iteration 956/1000 | Loss: 0.00003739
Iteration 957/1000 | Loss: 0.00006129
Iteration 958/1000 | Loss: 0.00005059
Iteration 959/1000 | Loss: 0.00007535
Iteration 960/1000 | Loss: 0.00004533
Iteration 961/1000 | Loss: 0.00007631
Iteration 962/1000 | Loss: 0.00005769
Iteration 963/1000 | Loss: 0.00008524
Iteration 964/1000 | Loss: 0.00006213
Iteration 965/1000 | Loss: 0.00007260
Iteration 966/1000 | Loss: 0.00006060
Iteration 967/1000 | Loss: 0.00012993
Iteration 968/1000 | Loss: 0.00007852
Iteration 969/1000 | Loss: 0.00003556
Iteration 970/1000 | Loss: 0.00002895
Iteration 971/1000 | Loss: 0.00002880
Iteration 972/1000 | Loss: 0.00002863
Iteration 973/1000 | Loss: 0.00004437
Iteration 974/1000 | Loss: 0.00007054
Iteration 975/1000 | Loss: 0.00005038
Iteration 976/1000 | Loss: 0.00012770
Iteration 977/1000 | Loss: 0.00006146
Iteration 978/1000 | Loss: 0.00005608
Iteration 979/1000 | Loss: 0.00010562
Iteration 980/1000 | Loss: 0.00016184
Iteration 981/1000 | Loss: 0.00009028
Iteration 982/1000 | Loss: 0.00005332
Iteration 983/1000 | Loss: 0.00014120
Iteration 984/1000 | Loss: 0.00013526
Iteration 985/1000 | Loss: 0.00015299
Iteration 986/1000 | Loss: 0.00006360
Iteration 987/1000 | Loss: 0.00013503
Iteration 988/1000 | Loss: 0.00004477
Iteration 989/1000 | Loss: 0.00006978
Iteration 990/1000 | Loss: 0.00004596
Iteration 991/1000 | Loss: 0.00003406
Iteration 992/1000 | Loss: 0.00004979
Iteration 993/1000 | Loss: 0.00006919
Iteration 994/1000 | Loss: 0.00007739
Iteration 995/1000 | Loss: 0.00009373
Iteration 996/1000 | Loss: 0.00014568
Iteration 997/1000 | Loss: 0.00005413
Iteration 998/1000 | Loss: 0.00006761
Iteration 999/1000 | Loss: 0.00008524
Iteration 1000/1000 | Loss: 0.00006009

Optimization complete. Final v2v error: 4.823336601257324 mm

Highest mean error: 15.08538818359375 mm for frame 141

Lowest mean error: 2.8866071701049805 mm for frame 45

Saving results

Total time: 1422.1352968215942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964775
Iteration 2/25 | Loss: 0.00156628
Iteration 3/25 | Loss: 0.00121132
Iteration 4/25 | Loss: 0.00117250
Iteration 5/25 | Loss: 0.00116246
Iteration 6/25 | Loss: 0.00116033
Iteration 7/25 | Loss: 0.00115977
Iteration 8/25 | Loss: 0.00115977
Iteration 9/25 | Loss: 0.00115977
Iteration 10/25 | Loss: 0.00115977
Iteration 11/25 | Loss: 0.00115977
Iteration 12/25 | Loss: 0.00115977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011597732082009315, 0.0011597732082009315, 0.0011597732082009315, 0.0011597732082009315, 0.0011597732082009315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011597732082009315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16552341
Iteration 2/25 | Loss: 0.00239471
Iteration 3/25 | Loss: 0.00239471
Iteration 4/25 | Loss: 0.00239471
Iteration 5/25 | Loss: 0.00239471
Iteration 6/25 | Loss: 0.00239471
Iteration 7/25 | Loss: 0.00239471
Iteration 8/25 | Loss: 0.00239471
Iteration 9/25 | Loss: 0.00239471
Iteration 10/25 | Loss: 0.00239471
Iteration 11/25 | Loss: 0.00239471
Iteration 12/25 | Loss: 0.00239471
Iteration 13/25 | Loss: 0.00239471
Iteration 14/25 | Loss: 0.00239471
Iteration 15/25 | Loss: 0.00239471
Iteration 16/25 | Loss: 0.00239471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002394708339124918, 0.002394708339124918, 0.002394708339124918, 0.002394708339124918, 0.002394708339124918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002394708339124918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239471
Iteration 2/1000 | Loss: 0.00005931
Iteration 3/1000 | Loss: 0.00004405
Iteration 4/1000 | Loss: 0.00003864
Iteration 5/1000 | Loss: 0.00003618
Iteration 6/1000 | Loss: 0.00003413
Iteration 7/1000 | Loss: 0.00003304
Iteration 8/1000 | Loss: 0.00003243
Iteration 9/1000 | Loss: 0.00003205
Iteration 10/1000 | Loss: 0.00003157
Iteration 11/1000 | Loss: 0.00003134
Iteration 12/1000 | Loss: 0.00003114
Iteration 13/1000 | Loss: 0.00003107
Iteration 14/1000 | Loss: 0.00003097
Iteration 15/1000 | Loss: 0.00003097
Iteration 16/1000 | Loss: 0.00003095
Iteration 17/1000 | Loss: 0.00003095
Iteration 18/1000 | Loss: 0.00003095
Iteration 19/1000 | Loss: 0.00003095
Iteration 20/1000 | Loss: 0.00003095
Iteration 21/1000 | Loss: 0.00003095
Iteration 22/1000 | Loss: 0.00003095
Iteration 23/1000 | Loss: 0.00003095
Iteration 24/1000 | Loss: 0.00003095
Iteration 25/1000 | Loss: 0.00003095
Iteration 26/1000 | Loss: 0.00003094
Iteration 27/1000 | Loss: 0.00003094
Iteration 28/1000 | Loss: 0.00003094
Iteration 29/1000 | Loss: 0.00003092
Iteration 30/1000 | Loss: 0.00003092
Iteration 31/1000 | Loss: 0.00003091
Iteration 32/1000 | Loss: 0.00003091
Iteration 33/1000 | Loss: 0.00003090
Iteration 34/1000 | Loss: 0.00003090
Iteration 35/1000 | Loss: 0.00003090
Iteration 36/1000 | Loss: 0.00003090
Iteration 37/1000 | Loss: 0.00003090
Iteration 38/1000 | Loss: 0.00003090
Iteration 39/1000 | Loss: 0.00003090
Iteration 40/1000 | Loss: 0.00003090
Iteration 41/1000 | Loss: 0.00003090
Iteration 42/1000 | Loss: 0.00003089
Iteration 43/1000 | Loss: 0.00003089
Iteration 44/1000 | Loss: 0.00003089
Iteration 45/1000 | Loss: 0.00003089
Iteration 46/1000 | Loss: 0.00003089
Iteration 47/1000 | Loss: 0.00003089
Iteration 48/1000 | Loss: 0.00003088
Iteration 49/1000 | Loss: 0.00003088
Iteration 50/1000 | Loss: 0.00003088
Iteration 51/1000 | Loss: 0.00003087
Iteration 52/1000 | Loss: 0.00003087
Iteration 53/1000 | Loss: 0.00003087
Iteration 54/1000 | Loss: 0.00003087
Iteration 55/1000 | Loss: 0.00003086
Iteration 56/1000 | Loss: 0.00003086
Iteration 57/1000 | Loss: 0.00003086
Iteration 58/1000 | Loss: 0.00003086
Iteration 59/1000 | Loss: 0.00003086
Iteration 60/1000 | Loss: 0.00003086
Iteration 61/1000 | Loss: 0.00003086
Iteration 62/1000 | Loss: 0.00003086
Iteration 63/1000 | Loss: 0.00003086
Iteration 64/1000 | Loss: 0.00003086
Iteration 65/1000 | Loss: 0.00003086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [3.0861592676956207e-05, 3.0861592676956207e-05, 3.0861592676956207e-05, 3.0861592676956207e-05, 3.0861592676956207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0861592676956207e-05

Optimization complete. Final v2v error: 4.62557315826416 mm

Highest mean error: 5.033510684967041 mm for frame 28

Lowest mean error: 4.421905994415283 mm for frame 136

Saving results

Total time: 31.879766702651978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919278
Iteration 2/25 | Loss: 0.00194131
Iteration 3/25 | Loss: 0.00118900
Iteration 4/25 | Loss: 0.00106148
Iteration 5/25 | Loss: 0.00104961
Iteration 6/25 | Loss: 0.00104789
Iteration 7/25 | Loss: 0.00104789
Iteration 8/25 | Loss: 0.00104789
Iteration 9/25 | Loss: 0.00104789
Iteration 10/25 | Loss: 0.00104789
Iteration 11/25 | Loss: 0.00104789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010478857439011335, 0.0010478857439011335, 0.0010478857439011335, 0.0010478857439011335, 0.0010478857439011335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010478857439011335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60789108
Iteration 2/25 | Loss: 0.00267075
Iteration 3/25 | Loss: 0.00267075
Iteration 4/25 | Loss: 0.00267075
Iteration 5/25 | Loss: 0.00267075
Iteration 6/25 | Loss: 0.00267075
Iteration 7/25 | Loss: 0.00267075
Iteration 8/25 | Loss: 0.00267075
Iteration 9/25 | Loss: 0.00267075
Iteration 10/25 | Loss: 0.00267075
Iteration 11/25 | Loss: 0.00267075
Iteration 12/25 | Loss: 0.00267075
Iteration 13/25 | Loss: 0.00267075
Iteration 14/25 | Loss: 0.00267075
Iteration 15/25 | Loss: 0.00267075
Iteration 16/25 | Loss: 0.00267075
Iteration 17/25 | Loss: 0.00267075
Iteration 18/25 | Loss: 0.00267075
Iteration 19/25 | Loss: 0.00267075
Iteration 20/25 | Loss: 0.00267075
Iteration 21/25 | Loss: 0.00267075
Iteration 22/25 | Loss: 0.00267075
Iteration 23/25 | Loss: 0.00267075
Iteration 24/25 | Loss: 0.00267075
Iteration 25/25 | Loss: 0.00267075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267075
Iteration 2/1000 | Loss: 0.00005181
Iteration 3/1000 | Loss: 0.00003339
Iteration 4/1000 | Loss: 0.00002898
Iteration 5/1000 | Loss: 0.00002693
Iteration 6/1000 | Loss: 0.00002579
Iteration 7/1000 | Loss: 0.00002519
Iteration 8/1000 | Loss: 0.00002487
Iteration 9/1000 | Loss: 0.00002459
Iteration 10/1000 | Loss: 0.00002435
Iteration 11/1000 | Loss: 0.00002417
Iteration 12/1000 | Loss: 0.00002405
Iteration 13/1000 | Loss: 0.00002403
Iteration 14/1000 | Loss: 0.00002403
Iteration 15/1000 | Loss: 0.00002403
Iteration 16/1000 | Loss: 0.00002402
Iteration 17/1000 | Loss: 0.00002402
Iteration 18/1000 | Loss: 0.00002402
Iteration 19/1000 | Loss: 0.00002402
Iteration 20/1000 | Loss: 0.00002402
Iteration 21/1000 | Loss: 0.00002402
Iteration 22/1000 | Loss: 0.00002401
Iteration 23/1000 | Loss: 0.00002401
Iteration 24/1000 | Loss: 0.00002400
Iteration 25/1000 | Loss: 0.00002400
Iteration 26/1000 | Loss: 0.00002399
Iteration 27/1000 | Loss: 0.00002399
Iteration 28/1000 | Loss: 0.00002399
Iteration 29/1000 | Loss: 0.00002397
Iteration 30/1000 | Loss: 0.00002397
Iteration 31/1000 | Loss: 0.00002396
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00002395
Iteration 34/1000 | Loss: 0.00002392
Iteration 35/1000 | Loss: 0.00002392
Iteration 36/1000 | Loss: 0.00002392
Iteration 37/1000 | Loss: 0.00002391
Iteration 38/1000 | Loss: 0.00002391
Iteration 39/1000 | Loss: 0.00002391
Iteration 40/1000 | Loss: 0.00002391
Iteration 41/1000 | Loss: 0.00002390
Iteration 42/1000 | Loss: 0.00002389
Iteration 43/1000 | Loss: 0.00002388
Iteration 44/1000 | Loss: 0.00002388
Iteration 45/1000 | Loss: 0.00002388
Iteration 46/1000 | Loss: 0.00002387
Iteration 47/1000 | Loss: 0.00002387
Iteration 48/1000 | Loss: 0.00002387
Iteration 49/1000 | Loss: 0.00002387
Iteration 50/1000 | Loss: 0.00002387
Iteration 51/1000 | Loss: 0.00002387
Iteration 52/1000 | Loss: 0.00002387
Iteration 53/1000 | Loss: 0.00002387
Iteration 54/1000 | Loss: 0.00002387
Iteration 55/1000 | Loss: 0.00002387
Iteration 56/1000 | Loss: 0.00002386
Iteration 57/1000 | Loss: 0.00002386
Iteration 58/1000 | Loss: 0.00002386
Iteration 59/1000 | Loss: 0.00002385
Iteration 60/1000 | Loss: 0.00002385
Iteration 61/1000 | Loss: 0.00002385
Iteration 62/1000 | Loss: 0.00002385
Iteration 63/1000 | Loss: 0.00002385
Iteration 64/1000 | Loss: 0.00002385
Iteration 65/1000 | Loss: 0.00002385
Iteration 66/1000 | Loss: 0.00002385
Iteration 67/1000 | Loss: 0.00002385
Iteration 68/1000 | Loss: 0.00002385
Iteration 69/1000 | Loss: 0.00002385
Iteration 70/1000 | Loss: 0.00002385
Iteration 71/1000 | Loss: 0.00002384
Iteration 72/1000 | Loss: 0.00002384
Iteration 73/1000 | Loss: 0.00002384
Iteration 74/1000 | Loss: 0.00002384
Iteration 75/1000 | Loss: 0.00002383
Iteration 76/1000 | Loss: 0.00002383
Iteration 77/1000 | Loss: 0.00002383
Iteration 78/1000 | Loss: 0.00002383
Iteration 79/1000 | Loss: 0.00002382
Iteration 80/1000 | Loss: 0.00002382
Iteration 81/1000 | Loss: 0.00002382
Iteration 82/1000 | Loss: 0.00002382
Iteration 83/1000 | Loss: 0.00002382
Iteration 84/1000 | Loss: 0.00002382
Iteration 85/1000 | Loss: 0.00002381
Iteration 86/1000 | Loss: 0.00002381
Iteration 87/1000 | Loss: 0.00002381
Iteration 88/1000 | Loss: 0.00002381
Iteration 89/1000 | Loss: 0.00002381
Iteration 90/1000 | Loss: 0.00002381
Iteration 91/1000 | Loss: 0.00002380
Iteration 92/1000 | Loss: 0.00002380
Iteration 93/1000 | Loss: 0.00002380
Iteration 94/1000 | Loss: 0.00002380
Iteration 95/1000 | Loss: 0.00002380
Iteration 96/1000 | Loss: 0.00002379
Iteration 97/1000 | Loss: 0.00002379
Iteration 98/1000 | Loss: 0.00002379
Iteration 99/1000 | Loss: 0.00002379
Iteration 100/1000 | Loss: 0.00002379
Iteration 101/1000 | Loss: 0.00002379
Iteration 102/1000 | Loss: 0.00002379
Iteration 103/1000 | Loss: 0.00002379
Iteration 104/1000 | Loss: 0.00002379
Iteration 105/1000 | Loss: 0.00002379
Iteration 106/1000 | Loss: 0.00002379
Iteration 107/1000 | Loss: 0.00002379
Iteration 108/1000 | Loss: 0.00002379
Iteration 109/1000 | Loss: 0.00002379
Iteration 110/1000 | Loss: 0.00002378
Iteration 111/1000 | Loss: 0.00002378
Iteration 112/1000 | Loss: 0.00002378
Iteration 113/1000 | Loss: 0.00002378
Iteration 114/1000 | Loss: 0.00002378
Iteration 115/1000 | Loss: 0.00002378
Iteration 116/1000 | Loss: 0.00002378
Iteration 117/1000 | Loss: 0.00002378
Iteration 118/1000 | Loss: 0.00002378
Iteration 119/1000 | Loss: 0.00002378
Iteration 120/1000 | Loss: 0.00002377
Iteration 121/1000 | Loss: 0.00002377
Iteration 122/1000 | Loss: 0.00002377
Iteration 123/1000 | Loss: 0.00002377
Iteration 124/1000 | Loss: 0.00002377
Iteration 125/1000 | Loss: 0.00002377
Iteration 126/1000 | Loss: 0.00002377
Iteration 127/1000 | Loss: 0.00002377
Iteration 128/1000 | Loss: 0.00002377
Iteration 129/1000 | Loss: 0.00002377
Iteration 130/1000 | Loss: 0.00002377
Iteration 131/1000 | Loss: 0.00002377
Iteration 132/1000 | Loss: 0.00002377
Iteration 133/1000 | Loss: 0.00002377
Iteration 134/1000 | Loss: 0.00002377
Iteration 135/1000 | Loss: 0.00002377
Iteration 136/1000 | Loss: 0.00002377
Iteration 137/1000 | Loss: 0.00002377
Iteration 138/1000 | Loss: 0.00002377
Iteration 139/1000 | Loss: 0.00002377
Iteration 140/1000 | Loss: 0.00002377
Iteration 141/1000 | Loss: 0.00002377
Iteration 142/1000 | Loss: 0.00002377
Iteration 143/1000 | Loss: 0.00002377
Iteration 144/1000 | Loss: 0.00002377
Iteration 145/1000 | Loss: 0.00002377
Iteration 146/1000 | Loss: 0.00002377
Iteration 147/1000 | Loss: 0.00002377
Iteration 148/1000 | Loss: 0.00002377
Iteration 149/1000 | Loss: 0.00002377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.3770453481120057e-05, 2.3770453481120057e-05, 2.3770453481120057e-05, 2.3770453481120057e-05, 2.3770453481120057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3770453481120057e-05

Optimization complete. Final v2v error: 4.13543176651001 mm

Highest mean error: 4.581350326538086 mm for frame 19

Lowest mean error: 3.5860090255737305 mm for frame 186

Saving results

Total time: 38.72372078895569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992594
Iteration 2/25 | Loss: 0.00185860
Iteration 3/25 | Loss: 0.00138662
Iteration 4/25 | Loss: 0.00126881
Iteration 5/25 | Loss: 0.00122086
Iteration 6/25 | Loss: 0.00119594
Iteration 7/25 | Loss: 0.00115458
Iteration 8/25 | Loss: 0.00113283
Iteration 9/25 | Loss: 0.00111598
Iteration 10/25 | Loss: 0.00110547
Iteration 11/25 | Loss: 0.00109697
Iteration 12/25 | Loss: 0.00109124
Iteration 13/25 | Loss: 0.00108967
Iteration 14/25 | Loss: 0.00109381
Iteration 15/25 | Loss: 0.00107693
Iteration 16/25 | Loss: 0.00106907
Iteration 17/25 | Loss: 0.00106586
Iteration 18/25 | Loss: 0.00106542
Iteration 19/25 | Loss: 0.00106535
Iteration 20/25 | Loss: 0.00106535
Iteration 21/25 | Loss: 0.00106534
Iteration 22/25 | Loss: 0.00106534
Iteration 23/25 | Loss: 0.00106534
Iteration 24/25 | Loss: 0.00106534
Iteration 25/25 | Loss: 0.00106533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.41049242
Iteration 2/25 | Loss: 0.00207531
Iteration 3/25 | Loss: 0.00207466
Iteration 4/25 | Loss: 0.00207466
Iteration 5/25 | Loss: 0.00207466
Iteration 6/25 | Loss: 0.00207466
Iteration 7/25 | Loss: 0.00207466
Iteration 8/25 | Loss: 0.00207466
Iteration 9/25 | Loss: 0.00207466
Iteration 10/25 | Loss: 0.00207466
Iteration 11/25 | Loss: 0.00207466
Iteration 12/25 | Loss: 0.00207466
Iteration 13/25 | Loss: 0.00207466
Iteration 14/25 | Loss: 0.00207466
Iteration 15/25 | Loss: 0.00207466
Iteration 16/25 | Loss: 0.00207466
Iteration 17/25 | Loss: 0.00207466
Iteration 18/25 | Loss: 0.00207466
Iteration 19/25 | Loss: 0.00207466
Iteration 20/25 | Loss: 0.00207466
Iteration 21/25 | Loss: 0.00207466
Iteration 22/25 | Loss: 0.00207466
Iteration 23/25 | Loss: 0.00207466
Iteration 24/25 | Loss: 0.00207466
Iteration 25/25 | Loss: 0.00207466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002074656542390585, 0.002074656542390585, 0.002074656542390585, 0.002074656542390585, 0.002074656542390585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002074656542390585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207466
Iteration 2/1000 | Loss: 0.00048726
Iteration 3/1000 | Loss: 0.00029307
Iteration 4/1000 | Loss: 0.00022496
Iteration 5/1000 | Loss: 0.00026267
Iteration 6/1000 | Loss: 0.00013518
Iteration 7/1000 | Loss: 0.00010864
Iteration 8/1000 | Loss: 0.00025021
Iteration 9/1000 | Loss: 0.00017758
Iteration 10/1000 | Loss: 0.00013392
Iteration 11/1000 | Loss: 0.00018072
Iteration 12/1000 | Loss: 0.00014762
Iteration 13/1000 | Loss: 0.00011202
Iteration 14/1000 | Loss: 0.00018051
Iteration 15/1000 | Loss: 0.00010197
Iteration 16/1000 | Loss: 0.00016723
Iteration 17/1000 | Loss: 0.00008755
Iteration 18/1000 | Loss: 0.00007613
Iteration 19/1000 | Loss: 0.00006873
Iteration 20/1000 | Loss: 0.00008333
Iteration 21/1000 | Loss: 0.00006303
Iteration 22/1000 | Loss: 0.00007292
Iteration 23/1000 | Loss: 0.00005538
Iteration 24/1000 | Loss: 0.00007190
Iteration 25/1000 | Loss: 0.00006550
Iteration 26/1000 | Loss: 0.00004826
Iteration 27/1000 | Loss: 0.00005900
Iteration 28/1000 | Loss: 0.00008167
Iteration 29/1000 | Loss: 0.00009788
Iteration 30/1000 | Loss: 0.00005749
Iteration 31/1000 | Loss: 0.00005851
Iteration 32/1000 | Loss: 0.00005099
Iteration 33/1000 | Loss: 0.00004778
Iteration 34/1000 | Loss: 0.00004562
Iteration 35/1000 | Loss: 0.00004420
Iteration 36/1000 | Loss: 0.00004317
Iteration 37/1000 | Loss: 0.00004234
Iteration 38/1000 | Loss: 0.00005624
Iteration 39/1000 | Loss: 0.00004531
Iteration 40/1000 | Loss: 0.00004329
Iteration 41/1000 | Loss: 0.00004227
Iteration 42/1000 | Loss: 0.00012342
Iteration 43/1000 | Loss: 0.00008724
Iteration 44/1000 | Loss: 0.00005493
Iteration 45/1000 | Loss: 0.00004494
Iteration 46/1000 | Loss: 0.00004504
Iteration 47/1000 | Loss: 0.00005178
Iteration 48/1000 | Loss: 0.00012343
Iteration 49/1000 | Loss: 0.00011526
Iteration 50/1000 | Loss: 0.00008695
Iteration 51/1000 | Loss: 0.00008051
Iteration 52/1000 | Loss: 0.00004671
Iteration 53/1000 | Loss: 0.00004931
Iteration 54/1000 | Loss: 0.00013404
Iteration 55/1000 | Loss: 0.00010854
Iteration 56/1000 | Loss: 0.00005884
Iteration 57/1000 | Loss: 0.00004672
Iteration 58/1000 | Loss: 0.00005077
Iteration 59/1000 | Loss: 0.00004370
Iteration 60/1000 | Loss: 0.00004152
Iteration 61/1000 | Loss: 0.00004646
Iteration 62/1000 | Loss: 0.00004554
Iteration 63/1000 | Loss: 0.00004084
Iteration 64/1000 | Loss: 0.00004664
Iteration 65/1000 | Loss: 0.00004896
Iteration 66/1000 | Loss: 0.00004684
Iteration 67/1000 | Loss: 0.00004896
Iteration 68/1000 | Loss: 0.00004773
Iteration 69/1000 | Loss: 0.00005268
Iteration 70/1000 | Loss: 0.00004976
Iteration 71/1000 | Loss: 0.00005291
Iteration 72/1000 | Loss: 0.00005816
Iteration 73/1000 | Loss: 0.00005130
Iteration 74/1000 | Loss: 0.00005542
Iteration 75/1000 | Loss: 0.00005108
Iteration 76/1000 | Loss: 0.00005451
Iteration 77/1000 | Loss: 0.00005100
Iteration 78/1000 | Loss: 0.00005499
Iteration 79/1000 | Loss: 0.00004078
Iteration 80/1000 | Loss: 0.00004013
Iteration 81/1000 | Loss: 0.00003951
Iteration 82/1000 | Loss: 0.00003913
Iteration 83/1000 | Loss: 0.00003894
Iteration 84/1000 | Loss: 0.00003888
Iteration 85/1000 | Loss: 0.00003886
Iteration 86/1000 | Loss: 0.00003863
Iteration 87/1000 | Loss: 0.00003844
Iteration 88/1000 | Loss: 0.00003842
Iteration 89/1000 | Loss: 0.00003840
Iteration 90/1000 | Loss: 0.00003840
Iteration 91/1000 | Loss: 0.00003840
Iteration 92/1000 | Loss: 0.00003839
Iteration 93/1000 | Loss: 0.00003839
Iteration 94/1000 | Loss: 0.00003839
Iteration 95/1000 | Loss: 0.00003838
Iteration 96/1000 | Loss: 0.00003838
Iteration 97/1000 | Loss: 0.00003838
Iteration 98/1000 | Loss: 0.00003837
Iteration 99/1000 | Loss: 0.00003837
Iteration 100/1000 | Loss: 0.00003836
Iteration 101/1000 | Loss: 0.00003836
Iteration 102/1000 | Loss: 0.00003836
Iteration 103/1000 | Loss: 0.00003836
Iteration 104/1000 | Loss: 0.00003835
Iteration 105/1000 | Loss: 0.00003835
Iteration 106/1000 | Loss: 0.00007597
Iteration 107/1000 | Loss: 0.00009002
Iteration 108/1000 | Loss: 0.00011232
Iteration 109/1000 | Loss: 0.00011595
Iteration 110/1000 | Loss: 0.00005501
Iteration 111/1000 | Loss: 0.00004885
Iteration 112/1000 | Loss: 0.00004543
Iteration 113/1000 | Loss: 0.00004308
Iteration 114/1000 | Loss: 0.00004112
Iteration 115/1000 | Loss: 0.00003980
Iteration 116/1000 | Loss: 0.00003912
Iteration 117/1000 | Loss: 0.00003872
Iteration 118/1000 | Loss: 0.00003849
Iteration 119/1000 | Loss: 0.00003843
Iteration 120/1000 | Loss: 0.00003842
Iteration 121/1000 | Loss: 0.00003841
Iteration 122/1000 | Loss: 0.00003840
Iteration 123/1000 | Loss: 0.00003839
Iteration 124/1000 | Loss: 0.00003836
Iteration 125/1000 | Loss: 0.00003835
Iteration 126/1000 | Loss: 0.00003828
Iteration 127/1000 | Loss: 0.00003824
Iteration 128/1000 | Loss: 0.00003819
Iteration 129/1000 | Loss: 0.00003814
Iteration 130/1000 | Loss: 0.00003814
Iteration 131/1000 | Loss: 0.00003811
Iteration 132/1000 | Loss: 0.00003808
Iteration 133/1000 | Loss: 0.00003808
Iteration 134/1000 | Loss: 0.00003805
Iteration 135/1000 | Loss: 0.00003805
Iteration 136/1000 | Loss: 0.00003804
Iteration 137/1000 | Loss: 0.00003804
Iteration 138/1000 | Loss: 0.00003804
Iteration 139/1000 | Loss: 0.00003804
Iteration 140/1000 | Loss: 0.00003804
Iteration 141/1000 | Loss: 0.00003803
Iteration 142/1000 | Loss: 0.00003803
Iteration 143/1000 | Loss: 0.00003803
Iteration 144/1000 | Loss: 0.00003801
Iteration 145/1000 | Loss: 0.00003801
Iteration 146/1000 | Loss: 0.00003801
Iteration 147/1000 | Loss: 0.00003800
Iteration 148/1000 | Loss: 0.00003800
Iteration 149/1000 | Loss: 0.00003799
Iteration 150/1000 | Loss: 0.00003799
Iteration 151/1000 | Loss: 0.00003799
Iteration 152/1000 | Loss: 0.00003799
Iteration 153/1000 | Loss: 0.00003799
Iteration 154/1000 | Loss: 0.00003799
Iteration 155/1000 | Loss: 0.00003799
Iteration 156/1000 | Loss: 0.00003799
Iteration 157/1000 | Loss: 0.00003799
Iteration 158/1000 | Loss: 0.00003798
Iteration 159/1000 | Loss: 0.00003798
Iteration 160/1000 | Loss: 0.00003798
Iteration 161/1000 | Loss: 0.00003798
Iteration 162/1000 | Loss: 0.00003798
Iteration 163/1000 | Loss: 0.00003798
Iteration 164/1000 | Loss: 0.00003797
Iteration 165/1000 | Loss: 0.00003796
Iteration 166/1000 | Loss: 0.00003796
Iteration 167/1000 | Loss: 0.00003795
Iteration 168/1000 | Loss: 0.00003795
Iteration 169/1000 | Loss: 0.00003795
Iteration 170/1000 | Loss: 0.00003792
Iteration 171/1000 | Loss: 0.00003792
Iteration 172/1000 | Loss: 0.00003791
Iteration 173/1000 | Loss: 0.00003790
Iteration 174/1000 | Loss: 0.00003790
Iteration 175/1000 | Loss: 0.00003790
Iteration 176/1000 | Loss: 0.00003790
Iteration 177/1000 | Loss: 0.00003789
Iteration 178/1000 | Loss: 0.00003789
Iteration 179/1000 | Loss: 0.00003789
Iteration 180/1000 | Loss: 0.00003788
Iteration 181/1000 | Loss: 0.00003788
Iteration 182/1000 | Loss: 0.00003788
Iteration 183/1000 | Loss: 0.00003788
Iteration 184/1000 | Loss: 0.00003788
Iteration 185/1000 | Loss: 0.00003788
Iteration 186/1000 | Loss: 0.00003788
Iteration 187/1000 | Loss: 0.00003787
Iteration 188/1000 | Loss: 0.00003787
Iteration 189/1000 | Loss: 0.00003787
Iteration 190/1000 | Loss: 0.00003787
Iteration 191/1000 | Loss: 0.00003787
Iteration 192/1000 | Loss: 0.00003787
Iteration 193/1000 | Loss: 0.00003787
Iteration 194/1000 | Loss: 0.00003787
Iteration 195/1000 | Loss: 0.00003787
Iteration 196/1000 | Loss: 0.00003786
Iteration 197/1000 | Loss: 0.00003786
Iteration 198/1000 | Loss: 0.00003785
Iteration 199/1000 | Loss: 0.00003785
Iteration 200/1000 | Loss: 0.00003785
Iteration 201/1000 | Loss: 0.00003784
Iteration 202/1000 | Loss: 0.00003784
Iteration 203/1000 | Loss: 0.00003784
Iteration 204/1000 | Loss: 0.00003784
Iteration 205/1000 | Loss: 0.00003784
Iteration 206/1000 | Loss: 0.00003784
Iteration 207/1000 | Loss: 0.00003784
Iteration 208/1000 | Loss: 0.00003784
Iteration 209/1000 | Loss: 0.00003784
Iteration 210/1000 | Loss: 0.00003784
Iteration 211/1000 | Loss: 0.00003783
Iteration 212/1000 | Loss: 0.00003783
Iteration 213/1000 | Loss: 0.00003783
Iteration 214/1000 | Loss: 0.00003783
Iteration 215/1000 | Loss: 0.00003783
Iteration 216/1000 | Loss: 0.00003783
Iteration 217/1000 | Loss: 0.00003783
Iteration 218/1000 | Loss: 0.00003782
Iteration 219/1000 | Loss: 0.00003782
Iteration 220/1000 | Loss: 0.00003782
Iteration 221/1000 | Loss: 0.00003782
Iteration 222/1000 | Loss: 0.00003782
Iteration 223/1000 | Loss: 0.00003782
Iteration 224/1000 | Loss: 0.00003781
Iteration 225/1000 | Loss: 0.00003781
Iteration 226/1000 | Loss: 0.00003781
Iteration 227/1000 | Loss: 0.00003781
Iteration 228/1000 | Loss: 0.00003781
Iteration 229/1000 | Loss: 0.00003781
Iteration 230/1000 | Loss: 0.00003781
Iteration 231/1000 | Loss: 0.00003781
Iteration 232/1000 | Loss: 0.00003781
Iteration 233/1000 | Loss: 0.00003781
Iteration 234/1000 | Loss: 0.00003780
Iteration 235/1000 | Loss: 0.00003780
Iteration 236/1000 | Loss: 0.00003780
Iteration 237/1000 | Loss: 0.00003780
Iteration 238/1000 | Loss: 0.00003780
Iteration 239/1000 | Loss: 0.00003780
Iteration 240/1000 | Loss: 0.00003780
Iteration 241/1000 | Loss: 0.00003780
Iteration 242/1000 | Loss: 0.00003780
Iteration 243/1000 | Loss: 0.00003780
Iteration 244/1000 | Loss: 0.00003780
Iteration 245/1000 | Loss: 0.00003780
Iteration 246/1000 | Loss: 0.00003779
Iteration 247/1000 | Loss: 0.00003779
Iteration 248/1000 | Loss: 0.00003779
Iteration 249/1000 | Loss: 0.00003779
Iteration 250/1000 | Loss: 0.00003779
Iteration 251/1000 | Loss: 0.00003779
Iteration 252/1000 | Loss: 0.00003779
Iteration 253/1000 | Loss: 0.00003779
Iteration 254/1000 | Loss: 0.00003779
Iteration 255/1000 | Loss: 0.00003779
Iteration 256/1000 | Loss: 0.00003779
Iteration 257/1000 | Loss: 0.00003778
Iteration 258/1000 | Loss: 0.00003778
Iteration 259/1000 | Loss: 0.00003778
Iteration 260/1000 | Loss: 0.00003778
Iteration 261/1000 | Loss: 0.00003778
Iteration 262/1000 | Loss: 0.00003778
Iteration 263/1000 | Loss: 0.00003778
Iteration 264/1000 | Loss: 0.00003778
Iteration 265/1000 | Loss: 0.00003778
Iteration 266/1000 | Loss: 0.00003778
Iteration 267/1000 | Loss: 0.00003778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [3.778250902541913e-05, 3.778250902541913e-05, 3.778250902541913e-05, 3.778250902541913e-05, 3.778250902541913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.778250902541913e-05

Optimization complete. Final v2v error: 4.835318088531494 mm

Highest mean error: 9.00313663482666 mm for frame 119

Lowest mean error: 3.8713629245758057 mm for frame 74

Saving results

Total time: 197.3155233860016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01161192
Iteration 2/25 | Loss: 0.00286510
Iteration 3/25 | Loss: 0.00251291
Iteration 4/25 | Loss: 0.00220681
Iteration 5/25 | Loss: 0.00183004
Iteration 6/25 | Loss: 0.00151184
Iteration 7/25 | Loss: 0.00155793
Iteration 8/25 | Loss: 0.00132420
Iteration 9/25 | Loss: 0.00119423
Iteration 10/25 | Loss: 0.00114741
Iteration 11/25 | Loss: 0.00113583
Iteration 12/25 | Loss: 0.00108328
Iteration 13/25 | Loss: 0.00106615
Iteration 14/25 | Loss: 0.00106599
Iteration 15/25 | Loss: 0.00106075
Iteration 16/25 | Loss: 0.00105224
Iteration 17/25 | Loss: 0.00103270
Iteration 18/25 | Loss: 0.00102447
Iteration 19/25 | Loss: 0.00103608
Iteration 20/25 | Loss: 0.00102845
Iteration 21/25 | Loss: 0.00101823
Iteration 22/25 | Loss: 0.00101210
Iteration 23/25 | Loss: 0.00101124
Iteration 24/25 | Loss: 0.00101103
Iteration 25/25 | Loss: 0.00101099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66520226
Iteration 2/25 | Loss: 0.00224834
Iteration 3/25 | Loss: 0.00224834
Iteration 4/25 | Loss: 0.00224834
Iteration 5/25 | Loss: 0.00224834
Iteration 6/25 | Loss: 0.00224834
Iteration 7/25 | Loss: 0.00224834
Iteration 8/25 | Loss: 0.00224834
Iteration 9/25 | Loss: 0.00224834
Iteration 10/25 | Loss: 0.00224834
Iteration 11/25 | Loss: 0.00224834
Iteration 12/25 | Loss: 0.00224834
Iteration 13/25 | Loss: 0.00224834
Iteration 14/25 | Loss: 0.00224834
Iteration 15/25 | Loss: 0.00224834
Iteration 16/25 | Loss: 0.00224834
Iteration 17/25 | Loss: 0.00224834
Iteration 18/25 | Loss: 0.00224834
Iteration 19/25 | Loss: 0.00224834
Iteration 20/25 | Loss: 0.00224834
Iteration 21/25 | Loss: 0.00224834
Iteration 22/25 | Loss: 0.00224834
Iteration 23/25 | Loss: 0.00224834
Iteration 24/25 | Loss: 0.00224834
Iteration 25/25 | Loss: 0.00224834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224834
Iteration 2/1000 | Loss: 0.00003573
Iteration 3/1000 | Loss: 0.00002985
Iteration 4/1000 | Loss: 0.00057903
Iteration 5/1000 | Loss: 0.00004017
Iteration 6/1000 | Loss: 0.00002495
Iteration 7/1000 | Loss: 0.00007317
Iteration 8/1000 | Loss: 0.00002393
Iteration 9/1000 | Loss: 0.00002311
Iteration 10/1000 | Loss: 0.00002257
Iteration 11/1000 | Loss: 0.00063909
Iteration 12/1000 | Loss: 0.00054988
Iteration 13/1000 | Loss: 0.00019137
Iteration 14/1000 | Loss: 0.00004363
Iteration 15/1000 | Loss: 0.00035212
Iteration 16/1000 | Loss: 0.00002235
Iteration 17/1000 | Loss: 0.00047350
Iteration 18/1000 | Loss: 0.00002902
Iteration 19/1000 | Loss: 0.00005496
Iteration 20/1000 | Loss: 0.00001999
Iteration 21/1000 | Loss: 0.00062632
Iteration 22/1000 | Loss: 0.00004413
Iteration 23/1000 | Loss: 0.00003683
Iteration 24/1000 | Loss: 0.00075711
Iteration 25/1000 | Loss: 0.00002155
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00052980
Iteration 28/1000 | Loss: 0.00013405
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00021270
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001897
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001851
Iteration 41/1000 | Loss: 0.00001851
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001834
Iteration 44/1000 | Loss: 0.00001828
Iteration 45/1000 | Loss: 0.00001828
Iteration 46/1000 | Loss: 0.00001827
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001825
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001824
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001819
Iteration 54/1000 | Loss: 0.00001819
Iteration 55/1000 | Loss: 0.00001818
Iteration 56/1000 | Loss: 0.00001818
Iteration 57/1000 | Loss: 0.00001818
Iteration 58/1000 | Loss: 0.00001818
Iteration 59/1000 | Loss: 0.00001818
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001818
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001817
Iteration 65/1000 | Loss: 0.00001817
Iteration 66/1000 | Loss: 0.00001817
Iteration 67/1000 | Loss: 0.00001817
Iteration 68/1000 | Loss: 0.00001816
Iteration 69/1000 | Loss: 0.00001816
Iteration 70/1000 | Loss: 0.00001816
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00001815
Iteration 73/1000 | Loss: 0.00001815
Iteration 74/1000 | Loss: 0.00001815
Iteration 75/1000 | Loss: 0.00001815
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001814
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001814
Iteration 81/1000 | Loss: 0.00001814
Iteration 82/1000 | Loss: 0.00001814
Iteration 83/1000 | Loss: 0.00001814
Iteration 84/1000 | Loss: 0.00001814
Iteration 85/1000 | Loss: 0.00001814
Iteration 86/1000 | Loss: 0.00001814
Iteration 87/1000 | Loss: 0.00001814
Iteration 88/1000 | Loss: 0.00001813
Iteration 89/1000 | Loss: 0.00001813
Iteration 90/1000 | Loss: 0.00001813
Iteration 91/1000 | Loss: 0.00001813
Iteration 92/1000 | Loss: 0.00001813
Iteration 93/1000 | Loss: 0.00001813
Iteration 94/1000 | Loss: 0.00001812
Iteration 95/1000 | Loss: 0.00001812
Iteration 96/1000 | Loss: 0.00001812
Iteration 97/1000 | Loss: 0.00001812
Iteration 98/1000 | Loss: 0.00001812
Iteration 99/1000 | Loss: 0.00001812
Iteration 100/1000 | Loss: 0.00001812
Iteration 101/1000 | Loss: 0.00001812
Iteration 102/1000 | Loss: 0.00001811
Iteration 103/1000 | Loss: 0.00001811
Iteration 104/1000 | Loss: 0.00001811
Iteration 105/1000 | Loss: 0.00001811
Iteration 106/1000 | Loss: 0.00001811
Iteration 107/1000 | Loss: 0.00001811
Iteration 108/1000 | Loss: 0.00001810
Iteration 109/1000 | Loss: 0.00001810
Iteration 110/1000 | Loss: 0.00001810
Iteration 111/1000 | Loss: 0.00001810
Iteration 112/1000 | Loss: 0.00001810
Iteration 113/1000 | Loss: 0.00001810
Iteration 114/1000 | Loss: 0.00001809
Iteration 115/1000 | Loss: 0.00001809
Iteration 116/1000 | Loss: 0.00001809
Iteration 117/1000 | Loss: 0.00001809
Iteration 118/1000 | Loss: 0.00001809
Iteration 119/1000 | Loss: 0.00001809
Iteration 120/1000 | Loss: 0.00001809
Iteration 121/1000 | Loss: 0.00001809
Iteration 122/1000 | Loss: 0.00001809
Iteration 123/1000 | Loss: 0.00001809
Iteration 124/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.8093120161211118e-05, 1.8093120161211118e-05, 1.8093120161211118e-05, 1.8093120161211118e-05, 1.8093120161211118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8093120161211118e-05

Optimization complete. Final v2v error: 3.546989679336548 mm

Highest mean error: 4.345534801483154 mm for frame 22

Lowest mean error: 2.8304107189178467 mm for frame 125

Saving results

Total time: 96.91747498512268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092009
Iteration 2/25 | Loss: 0.00187626
Iteration 3/25 | Loss: 0.00138762
Iteration 4/25 | Loss: 0.00127687
Iteration 5/25 | Loss: 0.00123250
Iteration 6/25 | Loss: 0.00131786
Iteration 7/25 | Loss: 0.00125475
Iteration 8/25 | Loss: 0.00119882
Iteration 9/25 | Loss: 0.00114065
Iteration 10/25 | Loss: 0.00112594
Iteration 11/25 | Loss: 0.00111393
Iteration 12/25 | Loss: 0.00110471
Iteration 13/25 | Loss: 0.00110155
Iteration 14/25 | Loss: 0.00109972
Iteration 15/25 | Loss: 0.00109820
Iteration 16/25 | Loss: 0.00109721
Iteration 17/25 | Loss: 0.00109561
Iteration 18/25 | Loss: 0.00109743
Iteration 19/25 | Loss: 0.00109813
Iteration 20/25 | Loss: 0.00109371
Iteration 21/25 | Loss: 0.00109158
Iteration 22/25 | Loss: 0.00109063
Iteration 23/25 | Loss: 0.00109074
Iteration 24/25 | Loss: 0.00109011
Iteration 25/25 | Loss: 0.00109052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10886645
Iteration 2/25 | Loss: 0.00216515
Iteration 3/25 | Loss: 0.00216515
Iteration 4/25 | Loss: 0.00216515
Iteration 5/25 | Loss: 0.00216515
Iteration 6/25 | Loss: 0.00216515
Iteration 7/25 | Loss: 0.00216515
Iteration 8/25 | Loss: 0.00216515
Iteration 9/25 | Loss: 0.00216515
Iteration 10/25 | Loss: 0.00216515
Iteration 11/25 | Loss: 0.00216515
Iteration 12/25 | Loss: 0.00216515
Iteration 13/25 | Loss: 0.00216515
Iteration 14/25 | Loss: 0.00216515
Iteration 15/25 | Loss: 0.00216515
Iteration 16/25 | Loss: 0.00216515
Iteration 17/25 | Loss: 0.00216515
Iteration 18/25 | Loss: 0.00216515
Iteration 19/25 | Loss: 0.00216515
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002165148500353098, 0.002165148500353098, 0.002165148500353098, 0.002165148500353098, 0.002165148500353098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002165148500353098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216515
Iteration 2/1000 | Loss: 0.00020247
Iteration 3/1000 | Loss: 0.00006406
Iteration 4/1000 | Loss: 0.00007230
Iteration 5/1000 | Loss: 0.00005138
Iteration 6/1000 | Loss: 0.00005460
Iteration 7/1000 | Loss: 0.00004622
Iteration 8/1000 | Loss: 0.00004306
Iteration 9/1000 | Loss: 0.00006461
Iteration 10/1000 | Loss: 0.00006170
Iteration 11/1000 | Loss: 0.00006434
Iteration 12/1000 | Loss: 0.00004405
Iteration 13/1000 | Loss: 0.00003662
Iteration 14/1000 | Loss: 0.00006087
Iteration 15/1000 | Loss: 0.00006277
Iteration 16/1000 | Loss: 0.00005887
Iteration 17/1000 | Loss: 0.00022615
Iteration 18/1000 | Loss: 0.00020152
Iteration 19/1000 | Loss: 0.00019869
Iteration 20/1000 | Loss: 0.00003617
Iteration 21/1000 | Loss: 0.00025405
Iteration 22/1000 | Loss: 0.00021397
Iteration 23/1000 | Loss: 0.00026577
Iteration 24/1000 | Loss: 0.00004071
Iteration 25/1000 | Loss: 0.00070234
Iteration 26/1000 | Loss: 0.00051447
Iteration 27/1000 | Loss: 0.00007703
Iteration 28/1000 | Loss: 0.00003926
Iteration 29/1000 | Loss: 0.00005657
Iteration 30/1000 | Loss: 0.00022495
Iteration 31/1000 | Loss: 0.00003615
Iteration 32/1000 | Loss: 0.00003308
Iteration 33/1000 | Loss: 0.00003077
Iteration 34/1000 | Loss: 0.00002961
Iteration 35/1000 | Loss: 0.00002901
Iteration 36/1000 | Loss: 0.00002860
Iteration 37/1000 | Loss: 0.00002845
Iteration 38/1000 | Loss: 0.00002841
Iteration 39/1000 | Loss: 0.00002840
Iteration 40/1000 | Loss: 0.00002827
Iteration 41/1000 | Loss: 0.00002827
Iteration 42/1000 | Loss: 0.00002827
Iteration 43/1000 | Loss: 0.00002826
Iteration 44/1000 | Loss: 0.00002826
Iteration 45/1000 | Loss: 0.00002825
Iteration 46/1000 | Loss: 0.00002820
Iteration 47/1000 | Loss: 0.00002815
Iteration 48/1000 | Loss: 0.00002814
Iteration 49/1000 | Loss: 0.00002812
Iteration 50/1000 | Loss: 0.00002803
Iteration 51/1000 | Loss: 0.00002799
Iteration 52/1000 | Loss: 0.00002820
Iteration 53/1000 | Loss: 0.00002767
Iteration 54/1000 | Loss: 0.00002746
Iteration 55/1000 | Loss: 0.00002743
Iteration 56/1000 | Loss: 0.00002741
Iteration 57/1000 | Loss: 0.00002741
Iteration 58/1000 | Loss: 0.00002741
Iteration 59/1000 | Loss: 0.00002741
Iteration 60/1000 | Loss: 0.00002741
Iteration 61/1000 | Loss: 0.00002741
Iteration 62/1000 | Loss: 0.00002740
Iteration 63/1000 | Loss: 0.00002740
Iteration 64/1000 | Loss: 0.00002740
Iteration 65/1000 | Loss: 0.00002740
Iteration 66/1000 | Loss: 0.00002740
Iteration 67/1000 | Loss: 0.00002740
Iteration 68/1000 | Loss: 0.00002739
Iteration 69/1000 | Loss: 0.00002739
Iteration 70/1000 | Loss: 0.00002739
Iteration 71/1000 | Loss: 0.00002739
Iteration 72/1000 | Loss: 0.00002739
Iteration 73/1000 | Loss: 0.00002739
Iteration 74/1000 | Loss: 0.00002738
Iteration 75/1000 | Loss: 0.00002738
Iteration 76/1000 | Loss: 0.00002738
Iteration 77/1000 | Loss: 0.00002738
Iteration 78/1000 | Loss: 0.00002737
Iteration 79/1000 | Loss: 0.00002737
Iteration 80/1000 | Loss: 0.00002737
Iteration 81/1000 | Loss: 0.00002737
Iteration 82/1000 | Loss: 0.00002737
Iteration 83/1000 | Loss: 0.00002737
Iteration 84/1000 | Loss: 0.00002737
Iteration 85/1000 | Loss: 0.00002736
Iteration 86/1000 | Loss: 0.00002736
Iteration 87/1000 | Loss: 0.00002736
Iteration 88/1000 | Loss: 0.00002735
Iteration 89/1000 | Loss: 0.00002735
Iteration 90/1000 | Loss: 0.00002735
Iteration 91/1000 | Loss: 0.00002735
Iteration 92/1000 | Loss: 0.00002735
Iteration 93/1000 | Loss: 0.00002735
Iteration 94/1000 | Loss: 0.00002735
Iteration 95/1000 | Loss: 0.00002734
Iteration 96/1000 | Loss: 0.00002734
Iteration 97/1000 | Loss: 0.00002734
Iteration 98/1000 | Loss: 0.00002734
Iteration 99/1000 | Loss: 0.00002733
Iteration 100/1000 | Loss: 0.00002733
Iteration 101/1000 | Loss: 0.00002733
Iteration 102/1000 | Loss: 0.00002733
Iteration 103/1000 | Loss: 0.00002733
Iteration 104/1000 | Loss: 0.00002733
Iteration 105/1000 | Loss: 0.00002732
Iteration 106/1000 | Loss: 0.00002732
Iteration 107/1000 | Loss: 0.00002732
Iteration 108/1000 | Loss: 0.00002732
Iteration 109/1000 | Loss: 0.00002732
Iteration 110/1000 | Loss: 0.00002732
Iteration 111/1000 | Loss: 0.00002732
Iteration 112/1000 | Loss: 0.00002732
Iteration 113/1000 | Loss: 0.00002732
Iteration 114/1000 | Loss: 0.00002732
Iteration 115/1000 | Loss: 0.00002732
Iteration 116/1000 | Loss: 0.00002732
Iteration 117/1000 | Loss: 0.00002732
Iteration 118/1000 | Loss: 0.00002731
Iteration 119/1000 | Loss: 0.00002731
Iteration 120/1000 | Loss: 0.00002731
Iteration 121/1000 | Loss: 0.00002731
Iteration 122/1000 | Loss: 0.00002731
Iteration 123/1000 | Loss: 0.00002731
Iteration 124/1000 | Loss: 0.00002730
Iteration 125/1000 | Loss: 0.00002730
Iteration 126/1000 | Loss: 0.00002730
Iteration 127/1000 | Loss: 0.00002730
Iteration 128/1000 | Loss: 0.00002730
Iteration 129/1000 | Loss: 0.00002730
Iteration 130/1000 | Loss: 0.00002729
Iteration 131/1000 | Loss: 0.00002729
Iteration 132/1000 | Loss: 0.00002729
Iteration 133/1000 | Loss: 0.00002729
Iteration 134/1000 | Loss: 0.00002729
Iteration 135/1000 | Loss: 0.00002729
Iteration 136/1000 | Loss: 0.00002729
Iteration 137/1000 | Loss: 0.00002729
Iteration 138/1000 | Loss: 0.00002728
Iteration 139/1000 | Loss: 0.00002728
Iteration 140/1000 | Loss: 0.00002728
Iteration 141/1000 | Loss: 0.00002728
Iteration 142/1000 | Loss: 0.00002728
Iteration 143/1000 | Loss: 0.00002727
Iteration 144/1000 | Loss: 0.00002727
Iteration 145/1000 | Loss: 0.00002727
Iteration 146/1000 | Loss: 0.00002727
Iteration 147/1000 | Loss: 0.00002727
Iteration 148/1000 | Loss: 0.00002727
Iteration 149/1000 | Loss: 0.00002727
Iteration 150/1000 | Loss: 0.00002727
Iteration 151/1000 | Loss: 0.00002727
Iteration 152/1000 | Loss: 0.00002727
Iteration 153/1000 | Loss: 0.00002727
Iteration 154/1000 | Loss: 0.00002727
Iteration 155/1000 | Loss: 0.00002727
Iteration 156/1000 | Loss: 0.00002727
Iteration 157/1000 | Loss: 0.00002727
Iteration 158/1000 | Loss: 0.00002727
Iteration 159/1000 | Loss: 0.00002727
Iteration 160/1000 | Loss: 0.00002727
Iteration 161/1000 | Loss: 0.00002727
Iteration 162/1000 | Loss: 0.00002727
Iteration 163/1000 | Loss: 0.00002727
Iteration 164/1000 | Loss: 0.00002727
Iteration 165/1000 | Loss: 0.00002727
Iteration 166/1000 | Loss: 0.00002727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.726851926126983e-05, 2.726851926126983e-05, 2.726851926126983e-05, 2.726851926126983e-05, 2.726851926126983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.726851926126983e-05

Optimization complete. Final v2v error: 4.405011177062988 mm

Highest mean error: 5.33462381362915 mm for frame 30

Lowest mean error: 4.035836696624756 mm for frame 5

Saving results

Total time: 129.89434599876404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01148666
Iteration 2/25 | Loss: 0.00190983
Iteration 3/25 | Loss: 0.00128446
Iteration 4/25 | Loss: 0.00116850
Iteration 5/25 | Loss: 0.00114726
Iteration 6/25 | Loss: 0.00114231
Iteration 7/25 | Loss: 0.00114126
Iteration 8/25 | Loss: 0.00114080
Iteration 9/25 | Loss: 0.00114037
Iteration 10/25 | Loss: 0.00113909
Iteration 11/25 | Loss: 0.00113778
Iteration 12/25 | Loss: 0.00113699
Iteration 13/25 | Loss: 0.00113536
Iteration 14/25 | Loss: 0.00113510
Iteration 15/25 | Loss: 0.00113341
Iteration 16/25 | Loss: 0.00113323
Iteration 17/25 | Loss: 0.00113321
Iteration 18/25 | Loss: 0.00113321
Iteration 19/25 | Loss: 0.00113320
Iteration 20/25 | Loss: 0.00113320
Iteration 21/25 | Loss: 0.00113319
Iteration 22/25 | Loss: 0.00113319
Iteration 23/25 | Loss: 0.00113319
Iteration 24/25 | Loss: 0.00113319
Iteration 25/25 | Loss: 0.00113319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96899223
Iteration 2/25 | Loss: 0.00325637
Iteration 3/25 | Loss: 0.00325636
Iteration 4/25 | Loss: 0.00325636
Iteration 5/25 | Loss: 0.00325636
Iteration 6/25 | Loss: 0.00325636
Iteration 7/25 | Loss: 0.00325636
Iteration 8/25 | Loss: 0.00325636
Iteration 9/25 | Loss: 0.00325636
Iteration 10/25 | Loss: 0.00325636
Iteration 11/25 | Loss: 0.00325636
Iteration 12/25 | Loss: 0.00325636
Iteration 13/25 | Loss: 0.00325636
Iteration 14/25 | Loss: 0.00325636
Iteration 15/25 | Loss: 0.00325636
Iteration 16/25 | Loss: 0.00325636
Iteration 17/25 | Loss: 0.00325636
Iteration 18/25 | Loss: 0.00325636
Iteration 19/25 | Loss: 0.00325636
Iteration 20/25 | Loss: 0.00325636
Iteration 21/25 | Loss: 0.00325636
Iteration 22/25 | Loss: 0.00325636
Iteration 23/25 | Loss: 0.00325636
Iteration 24/25 | Loss: 0.00325636
Iteration 25/25 | Loss: 0.00325636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325636
Iteration 2/1000 | Loss: 0.00005097
Iteration 3/1000 | Loss: 0.00004004
Iteration 4/1000 | Loss: 0.00003737
Iteration 5/1000 | Loss: 0.00003624
Iteration 6/1000 | Loss: 0.00003528
Iteration 7/1000 | Loss: 0.00003450
Iteration 8/1000 | Loss: 0.00003390
Iteration 9/1000 | Loss: 0.00003341
Iteration 10/1000 | Loss: 0.00003320
Iteration 11/1000 | Loss: 0.00003296
Iteration 12/1000 | Loss: 0.00003273
Iteration 13/1000 | Loss: 0.00003254
Iteration 14/1000 | Loss: 0.00003246
Iteration 15/1000 | Loss: 0.00003241
Iteration 16/1000 | Loss: 0.00003241
Iteration 17/1000 | Loss: 0.00003241
Iteration 18/1000 | Loss: 0.00003241
Iteration 19/1000 | Loss: 0.00003241
Iteration 20/1000 | Loss: 0.00003239
Iteration 21/1000 | Loss: 0.00003232
Iteration 22/1000 | Loss: 0.00003230
Iteration 23/1000 | Loss: 0.00003230
Iteration 24/1000 | Loss: 0.00003230
Iteration 25/1000 | Loss: 0.00003230
Iteration 26/1000 | Loss: 0.00003230
Iteration 27/1000 | Loss: 0.00003228
Iteration 28/1000 | Loss: 0.00003228
Iteration 29/1000 | Loss: 0.00003226
Iteration 30/1000 | Loss: 0.00003226
Iteration 31/1000 | Loss: 0.00003226
Iteration 32/1000 | Loss: 0.00003226
Iteration 33/1000 | Loss: 0.00003226
Iteration 34/1000 | Loss: 0.00003226
Iteration 35/1000 | Loss: 0.00003225
Iteration 36/1000 | Loss: 0.00003225
Iteration 37/1000 | Loss: 0.00003225
Iteration 38/1000 | Loss: 0.00003225
Iteration 39/1000 | Loss: 0.00003224
Iteration 40/1000 | Loss: 0.00003223
Iteration 41/1000 | Loss: 0.00003223
Iteration 42/1000 | Loss: 0.00003223
Iteration 43/1000 | Loss: 0.00003223
Iteration 44/1000 | Loss: 0.00003223
Iteration 45/1000 | Loss: 0.00003222
Iteration 46/1000 | Loss: 0.00003222
Iteration 47/1000 | Loss: 0.00003222
Iteration 48/1000 | Loss: 0.00003221
Iteration 49/1000 | Loss: 0.00003218
Iteration 50/1000 | Loss: 0.00003218
Iteration 51/1000 | Loss: 0.00003218
Iteration 52/1000 | Loss: 0.00003218
Iteration 53/1000 | Loss: 0.00003218
Iteration 54/1000 | Loss: 0.00003218
Iteration 55/1000 | Loss: 0.00003218
Iteration 56/1000 | Loss: 0.00003218
Iteration 57/1000 | Loss: 0.00003216
Iteration 58/1000 | Loss: 0.00003216
Iteration 59/1000 | Loss: 0.00003213
Iteration 60/1000 | Loss: 0.00003206
Iteration 61/1000 | Loss: 0.00003206
Iteration 62/1000 | Loss: 0.00003202
Iteration 63/1000 | Loss: 0.00003201
Iteration 64/1000 | Loss: 0.00003201
Iteration 65/1000 | Loss: 0.00003201
Iteration 66/1000 | Loss: 0.00003201
Iteration 67/1000 | Loss: 0.00003201
Iteration 68/1000 | Loss: 0.00003201
Iteration 69/1000 | Loss: 0.00003200
Iteration 70/1000 | Loss: 0.00003196
Iteration 71/1000 | Loss: 0.00003196
Iteration 72/1000 | Loss: 0.00003196
Iteration 73/1000 | Loss: 0.00003195
Iteration 74/1000 | Loss: 0.00003193
Iteration 75/1000 | Loss: 0.00003192
Iteration 76/1000 | Loss: 0.00003192
Iteration 77/1000 | Loss: 0.00003192
Iteration 78/1000 | Loss: 0.00003191
Iteration 79/1000 | Loss: 0.00003191
Iteration 80/1000 | Loss: 0.00003191
Iteration 81/1000 | Loss: 0.00003191
Iteration 82/1000 | Loss: 0.00003191
Iteration 83/1000 | Loss: 0.00003191
Iteration 84/1000 | Loss: 0.00003191
Iteration 85/1000 | Loss: 0.00003191
Iteration 86/1000 | Loss: 0.00003190
Iteration 87/1000 | Loss: 0.00003190
Iteration 88/1000 | Loss: 0.00003190
Iteration 89/1000 | Loss: 0.00003190
Iteration 90/1000 | Loss: 0.00003190
Iteration 91/1000 | Loss: 0.00003190
Iteration 92/1000 | Loss: 0.00003189
Iteration 93/1000 | Loss: 0.00003189
Iteration 94/1000 | Loss: 0.00003189
Iteration 95/1000 | Loss: 0.00003188
Iteration 96/1000 | Loss: 0.00003188
Iteration 97/1000 | Loss: 0.00003187
Iteration 98/1000 | Loss: 0.00003187
Iteration 99/1000 | Loss: 0.00003187
Iteration 100/1000 | Loss: 0.00003187
Iteration 101/1000 | Loss: 0.00003187
Iteration 102/1000 | Loss: 0.00003186
Iteration 103/1000 | Loss: 0.00003186
Iteration 104/1000 | Loss: 0.00003186
Iteration 105/1000 | Loss: 0.00003185
Iteration 106/1000 | Loss: 0.00003185
Iteration 107/1000 | Loss: 0.00003185
Iteration 108/1000 | Loss: 0.00003185
Iteration 109/1000 | Loss: 0.00003185
Iteration 110/1000 | Loss: 0.00003185
Iteration 111/1000 | Loss: 0.00003185
Iteration 112/1000 | Loss: 0.00003184
Iteration 113/1000 | Loss: 0.00003184
Iteration 114/1000 | Loss: 0.00003184
Iteration 115/1000 | Loss: 0.00003184
Iteration 116/1000 | Loss: 0.00003184
Iteration 117/1000 | Loss: 0.00003183
Iteration 118/1000 | Loss: 0.00003183
Iteration 119/1000 | Loss: 0.00003183
Iteration 120/1000 | Loss: 0.00003183
Iteration 121/1000 | Loss: 0.00003182
Iteration 122/1000 | Loss: 0.00003182
Iteration 123/1000 | Loss: 0.00003182
Iteration 124/1000 | Loss: 0.00003182
Iteration 125/1000 | Loss: 0.00003182
Iteration 126/1000 | Loss: 0.00003182
Iteration 127/1000 | Loss: 0.00003182
Iteration 128/1000 | Loss: 0.00003182
Iteration 129/1000 | Loss: 0.00003181
Iteration 130/1000 | Loss: 0.00003180
Iteration 131/1000 | Loss: 0.00003180
Iteration 132/1000 | Loss: 0.00003180
Iteration 133/1000 | Loss: 0.00003180
Iteration 134/1000 | Loss: 0.00003180
Iteration 135/1000 | Loss: 0.00003180
Iteration 136/1000 | Loss: 0.00003180
Iteration 137/1000 | Loss: 0.00003179
Iteration 138/1000 | Loss: 0.00003179
Iteration 139/1000 | Loss: 0.00003179
Iteration 140/1000 | Loss: 0.00003179
Iteration 141/1000 | Loss: 0.00003179
Iteration 142/1000 | Loss: 0.00003179
Iteration 143/1000 | Loss: 0.00003179
Iteration 144/1000 | Loss: 0.00003179
Iteration 145/1000 | Loss: 0.00003179
Iteration 146/1000 | Loss: 0.00003179
Iteration 147/1000 | Loss: 0.00003179
Iteration 148/1000 | Loss: 0.00003177
Iteration 149/1000 | Loss: 0.00003177
Iteration 150/1000 | Loss: 0.00003177
Iteration 151/1000 | Loss: 0.00003177
Iteration 152/1000 | Loss: 0.00003177
Iteration 153/1000 | Loss: 0.00003177
Iteration 154/1000 | Loss: 0.00003176
Iteration 155/1000 | Loss: 0.00003176
Iteration 156/1000 | Loss: 0.00003176
Iteration 157/1000 | Loss: 0.00003176
Iteration 158/1000 | Loss: 0.00003176
Iteration 159/1000 | Loss: 0.00003176
Iteration 160/1000 | Loss: 0.00003176
Iteration 161/1000 | Loss: 0.00003176
Iteration 162/1000 | Loss: 0.00003176
Iteration 163/1000 | Loss: 0.00003176
Iteration 164/1000 | Loss: 0.00003174
Iteration 165/1000 | Loss: 0.00003174
Iteration 166/1000 | Loss: 0.00003174
Iteration 167/1000 | Loss: 0.00003174
Iteration 168/1000 | Loss: 0.00003174
Iteration 169/1000 | Loss: 0.00003174
Iteration 170/1000 | Loss: 0.00003174
Iteration 171/1000 | Loss: 0.00003174
Iteration 172/1000 | Loss: 0.00003174
Iteration 173/1000 | Loss: 0.00003174
Iteration 174/1000 | Loss: 0.00003173
Iteration 175/1000 | Loss: 0.00003173
Iteration 176/1000 | Loss: 0.00003173
Iteration 177/1000 | Loss: 0.00003173
Iteration 178/1000 | Loss: 0.00003173
Iteration 179/1000 | Loss: 0.00003173
Iteration 180/1000 | Loss: 0.00003173
Iteration 181/1000 | Loss: 0.00003173
Iteration 182/1000 | Loss: 0.00003173
Iteration 183/1000 | Loss: 0.00003173
Iteration 184/1000 | Loss: 0.00003172
Iteration 185/1000 | Loss: 0.00003172
Iteration 186/1000 | Loss: 0.00003172
Iteration 187/1000 | Loss: 0.00003172
Iteration 188/1000 | Loss: 0.00003172
Iteration 189/1000 | Loss: 0.00003171
Iteration 190/1000 | Loss: 0.00003171
Iteration 191/1000 | Loss: 0.00003171
Iteration 192/1000 | Loss: 0.00003171
Iteration 193/1000 | Loss: 0.00003171
Iteration 194/1000 | Loss: 0.00003170
Iteration 195/1000 | Loss: 0.00003170
Iteration 196/1000 | Loss: 0.00003170
Iteration 197/1000 | Loss: 0.00003170
Iteration 198/1000 | Loss: 0.00003170
Iteration 199/1000 | Loss: 0.00003170
Iteration 200/1000 | Loss: 0.00003170
Iteration 201/1000 | Loss: 0.00003170
Iteration 202/1000 | Loss: 0.00003170
Iteration 203/1000 | Loss: 0.00003169
Iteration 204/1000 | Loss: 0.00003169
Iteration 205/1000 | Loss: 0.00003169
Iteration 206/1000 | Loss: 0.00003169
Iteration 207/1000 | Loss: 0.00003169
Iteration 208/1000 | Loss: 0.00003169
Iteration 209/1000 | Loss: 0.00003169
Iteration 210/1000 | Loss: 0.00003168
Iteration 211/1000 | Loss: 0.00003168
Iteration 212/1000 | Loss: 0.00003168
Iteration 213/1000 | Loss: 0.00003168
Iteration 214/1000 | Loss: 0.00003168
Iteration 215/1000 | Loss: 0.00003168
Iteration 216/1000 | Loss: 0.00003168
Iteration 217/1000 | Loss: 0.00003167
Iteration 218/1000 | Loss: 0.00003167
Iteration 219/1000 | Loss: 0.00003167
Iteration 220/1000 | Loss: 0.00003167
Iteration 221/1000 | Loss: 0.00003167
Iteration 222/1000 | Loss: 0.00003167
Iteration 223/1000 | Loss: 0.00003167
Iteration 224/1000 | Loss: 0.00003167
Iteration 225/1000 | Loss: 0.00003167
Iteration 226/1000 | Loss: 0.00003167
Iteration 227/1000 | Loss: 0.00003167
Iteration 228/1000 | Loss: 0.00003167
Iteration 229/1000 | Loss: 0.00003167
Iteration 230/1000 | Loss: 0.00003167
Iteration 231/1000 | Loss: 0.00003167
Iteration 232/1000 | Loss: 0.00003167
Iteration 233/1000 | Loss: 0.00003167
Iteration 234/1000 | Loss: 0.00003166
Iteration 235/1000 | Loss: 0.00003166
Iteration 236/1000 | Loss: 0.00003166
Iteration 237/1000 | Loss: 0.00003166
Iteration 238/1000 | Loss: 0.00003166
Iteration 239/1000 | Loss: 0.00003166
Iteration 240/1000 | Loss: 0.00003166
Iteration 241/1000 | Loss: 0.00003166
Iteration 242/1000 | Loss: 0.00003166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [3.16639052471146e-05, 3.16639052471146e-05, 3.16639052471146e-05, 3.16639052471146e-05, 3.16639052471146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.16639052471146e-05

Optimization complete. Final v2v error: 4.689262866973877 mm

Highest mean error: 5.362595558166504 mm for frame 100

Lowest mean error: 3.896148681640625 mm for frame 156

Saving results

Total time: 76.29415583610535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097163
Iteration 2/25 | Loss: 0.01097163
Iteration 3/25 | Loss: 0.01097163
Iteration 4/25 | Loss: 0.00338115
Iteration 5/25 | Loss: 0.00226415
Iteration 6/25 | Loss: 0.00188449
Iteration 7/25 | Loss: 0.00177891
Iteration 8/25 | Loss: 0.00175886
Iteration 9/25 | Loss: 0.00162095
Iteration 10/25 | Loss: 0.00145942
Iteration 11/25 | Loss: 0.00144483
Iteration 12/25 | Loss: 0.00144463
Iteration 13/25 | Loss: 0.00138916
Iteration 14/25 | Loss: 0.00134204
Iteration 15/25 | Loss: 0.00132778
Iteration 16/25 | Loss: 0.00130948
Iteration 17/25 | Loss: 0.00130276
Iteration 18/25 | Loss: 0.00130903
Iteration 19/25 | Loss: 0.00128710
Iteration 20/25 | Loss: 0.00127066
Iteration 21/25 | Loss: 0.00127246
Iteration 22/25 | Loss: 0.00126371
Iteration 23/25 | Loss: 0.00125868
Iteration 24/25 | Loss: 0.00125741
Iteration 25/25 | Loss: 0.00125470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69618642
Iteration 2/25 | Loss: 0.00617870
Iteration 3/25 | Loss: 0.00585886
Iteration 4/25 | Loss: 0.00585882
Iteration 5/25 | Loss: 0.00585881
Iteration 6/25 | Loss: 0.00585881
Iteration 7/25 | Loss: 0.00585881
Iteration 8/25 | Loss: 0.00585881
Iteration 9/25 | Loss: 0.00585881
Iteration 10/25 | Loss: 0.00585881
Iteration 11/25 | Loss: 0.00585881
Iteration 12/25 | Loss: 0.00585881
Iteration 13/25 | Loss: 0.00585881
Iteration 14/25 | Loss: 0.00585881
Iteration 15/25 | Loss: 0.00585881
Iteration 16/25 | Loss: 0.00585881
Iteration 17/25 | Loss: 0.00585881
Iteration 18/25 | Loss: 0.00585881
Iteration 19/25 | Loss: 0.00585881
Iteration 20/25 | Loss: 0.00585881
Iteration 21/25 | Loss: 0.00585881
Iteration 22/25 | Loss: 0.00585881
Iteration 23/25 | Loss: 0.00585881
Iteration 24/25 | Loss: 0.00585881
Iteration 25/25 | Loss: 0.00585881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00585881
Iteration 2/1000 | Loss: 0.00121372
Iteration 3/1000 | Loss: 0.00099095
Iteration 4/1000 | Loss: 0.00087544
Iteration 5/1000 | Loss: 0.00126013
Iteration 6/1000 | Loss: 0.00084638
Iteration 7/1000 | Loss: 0.00310379
Iteration 8/1000 | Loss: 0.00083879
Iteration 9/1000 | Loss: 0.00076619
Iteration 10/1000 | Loss: 0.00074618
Iteration 11/1000 | Loss: 0.00083337
Iteration 12/1000 | Loss: 0.00102506
Iteration 13/1000 | Loss: 0.00039987
Iteration 14/1000 | Loss: 0.00039975
Iteration 15/1000 | Loss: 0.00043186
Iteration 16/1000 | Loss: 0.00036236
Iteration 17/1000 | Loss: 0.00034739
Iteration 18/1000 | Loss: 0.00058035
Iteration 19/1000 | Loss: 0.00024847
Iteration 20/1000 | Loss: 0.00045935
Iteration 21/1000 | Loss: 0.00056037
Iteration 22/1000 | Loss: 0.00024396
Iteration 23/1000 | Loss: 0.00018691
Iteration 24/1000 | Loss: 0.00030991
Iteration 25/1000 | Loss: 0.00027070
Iteration 26/1000 | Loss: 0.00023003
Iteration 27/1000 | Loss: 0.00044156
Iteration 28/1000 | Loss: 0.00022438
Iteration 29/1000 | Loss: 0.00020087
Iteration 30/1000 | Loss: 0.00032321
Iteration 31/1000 | Loss: 0.00030080
Iteration 32/1000 | Loss: 0.00055096
Iteration 33/1000 | Loss: 0.00031106
Iteration 34/1000 | Loss: 0.00056424
Iteration 35/1000 | Loss: 0.00043304
Iteration 36/1000 | Loss: 0.00039398
Iteration 37/1000 | Loss: 0.00054293
Iteration 38/1000 | Loss: 0.00070932
Iteration 39/1000 | Loss: 0.00022744
Iteration 40/1000 | Loss: 0.00020263
Iteration 41/1000 | Loss: 0.00044364
Iteration 42/1000 | Loss: 0.00020057
Iteration 43/1000 | Loss: 0.00086483
Iteration 44/1000 | Loss: 0.00137998
Iteration 45/1000 | Loss: 0.00245341
Iteration 46/1000 | Loss: 0.00308364
Iteration 47/1000 | Loss: 0.01126871
Iteration 48/1000 | Loss: 0.00612756
Iteration 49/1000 | Loss: 0.00327686
Iteration 50/1000 | Loss: 0.00273768
Iteration 51/1000 | Loss: 0.00380311
Iteration 52/1000 | Loss: 0.00238743
Iteration 53/1000 | Loss: 0.00613271
Iteration 54/1000 | Loss: 0.00270414
Iteration 55/1000 | Loss: 0.00339888
Iteration 56/1000 | Loss: 0.00623512
Iteration 57/1000 | Loss: 0.00125421
Iteration 58/1000 | Loss: 0.00109260
Iteration 59/1000 | Loss: 0.00146639
Iteration 60/1000 | Loss: 0.00027672
Iteration 61/1000 | Loss: 0.00080254
Iteration 62/1000 | Loss: 0.00021295
Iteration 63/1000 | Loss: 0.00200327
Iteration 64/1000 | Loss: 0.00132074
Iteration 65/1000 | Loss: 0.00077791
Iteration 66/1000 | Loss: 0.00076815
Iteration 67/1000 | Loss: 0.00055864
Iteration 68/1000 | Loss: 0.00084121
Iteration 69/1000 | Loss: 0.00049041
Iteration 70/1000 | Loss: 0.00032035
Iteration 71/1000 | Loss: 0.00028143
Iteration 72/1000 | Loss: 0.00029041
Iteration 73/1000 | Loss: 0.00237021
Iteration 74/1000 | Loss: 0.00194292
Iteration 75/1000 | Loss: 0.00034060
Iteration 76/1000 | Loss: 0.00161631
Iteration 77/1000 | Loss: 0.00162497
Iteration 78/1000 | Loss: 0.00162185
Iteration 79/1000 | Loss: 0.00139752
Iteration 80/1000 | Loss: 0.00061787
Iteration 81/1000 | Loss: 0.00034709
Iteration 82/1000 | Loss: 0.00033659
Iteration 83/1000 | Loss: 0.00021323
Iteration 84/1000 | Loss: 0.00012081
Iteration 85/1000 | Loss: 0.00012824
Iteration 86/1000 | Loss: 0.00013565
Iteration 87/1000 | Loss: 0.00098209
Iteration 88/1000 | Loss: 0.00045038
Iteration 89/1000 | Loss: 0.00009186
Iteration 90/1000 | Loss: 0.00140017
Iteration 91/1000 | Loss: 0.00062910
Iteration 92/1000 | Loss: 0.00040854
Iteration 93/1000 | Loss: 0.00050262
Iteration 94/1000 | Loss: 0.00028615
Iteration 95/1000 | Loss: 0.00044807
Iteration 96/1000 | Loss: 0.00040026
Iteration 97/1000 | Loss: 0.00024929
Iteration 98/1000 | Loss: 0.00023639
Iteration 99/1000 | Loss: 0.00042037
Iteration 100/1000 | Loss: 0.00024279
Iteration 101/1000 | Loss: 0.00012591
Iteration 102/1000 | Loss: 0.00011910
Iteration 103/1000 | Loss: 0.00012047
Iteration 104/1000 | Loss: 0.00011512
Iteration 105/1000 | Loss: 0.00084963
Iteration 106/1000 | Loss: 0.00045180
Iteration 107/1000 | Loss: 0.00022418
Iteration 108/1000 | Loss: 0.00048701
Iteration 109/1000 | Loss: 0.00231195
Iteration 110/1000 | Loss: 0.00140258
Iteration 111/1000 | Loss: 0.00026017
Iteration 112/1000 | Loss: 0.00027768
Iteration 113/1000 | Loss: 0.00010480
Iteration 114/1000 | Loss: 0.00023246
Iteration 115/1000 | Loss: 0.00009150
Iteration 116/1000 | Loss: 0.00039685
Iteration 117/1000 | Loss: 0.00037651
Iteration 118/1000 | Loss: 0.00049169
Iteration 119/1000 | Loss: 0.00031476
Iteration 120/1000 | Loss: 0.00107738
Iteration 121/1000 | Loss: 0.00080001
Iteration 122/1000 | Loss: 0.00256276
Iteration 123/1000 | Loss: 0.00208854
Iteration 124/1000 | Loss: 0.00561487
Iteration 125/1000 | Loss: 0.00099366
Iteration 126/1000 | Loss: 0.00143479
Iteration 127/1000 | Loss: 0.00068299
Iteration 128/1000 | Loss: 0.00147224
Iteration 129/1000 | Loss: 0.00078692
Iteration 130/1000 | Loss: 0.00077363
Iteration 131/1000 | Loss: 0.00063137
Iteration 132/1000 | Loss: 0.00171019
Iteration 133/1000 | Loss: 0.00203037
Iteration 134/1000 | Loss: 0.00188058
Iteration 135/1000 | Loss: 0.00087559
Iteration 136/1000 | Loss: 0.00097882
Iteration 137/1000 | Loss: 0.00110434
Iteration 138/1000 | Loss: 0.00110142
Iteration 139/1000 | Loss: 0.00141914
Iteration 140/1000 | Loss: 0.00188456
Iteration 141/1000 | Loss: 0.00074183
Iteration 142/1000 | Loss: 0.00098879
Iteration 143/1000 | Loss: 0.00065635
Iteration 144/1000 | Loss: 0.00234522
Iteration 145/1000 | Loss: 0.00090856
Iteration 146/1000 | Loss: 0.00163938
Iteration 147/1000 | Loss: 0.00081408
Iteration 148/1000 | Loss: 0.00124467
Iteration 149/1000 | Loss: 0.00082617
Iteration 150/1000 | Loss: 0.00101099
Iteration 151/1000 | Loss: 0.00078165
Iteration 152/1000 | Loss: 0.00048485
Iteration 153/1000 | Loss: 0.00050204
Iteration 154/1000 | Loss: 0.00112370
Iteration 155/1000 | Loss: 0.00048777
Iteration 156/1000 | Loss: 0.00025132
Iteration 157/1000 | Loss: 0.00009075
Iteration 158/1000 | Loss: 0.00013667
Iteration 159/1000 | Loss: 0.00007692
Iteration 160/1000 | Loss: 0.00097702
Iteration 161/1000 | Loss: 0.00049813
Iteration 162/1000 | Loss: 0.00011748
Iteration 163/1000 | Loss: 0.00029861
Iteration 164/1000 | Loss: 0.00031347
Iteration 165/1000 | Loss: 0.00081489
Iteration 166/1000 | Loss: 0.00068970
Iteration 167/1000 | Loss: 0.00036744
Iteration 168/1000 | Loss: 0.00067748
Iteration 169/1000 | Loss: 0.00029323
Iteration 170/1000 | Loss: 0.00044155
Iteration 171/1000 | Loss: 0.00084948
Iteration 172/1000 | Loss: 0.00107985
Iteration 173/1000 | Loss: 0.00044642
Iteration 174/1000 | Loss: 0.00050578
Iteration 175/1000 | Loss: 0.00025539
Iteration 176/1000 | Loss: 0.00012043
Iteration 177/1000 | Loss: 0.00007647
Iteration 178/1000 | Loss: 0.00023126
Iteration 179/1000 | Loss: 0.00043716
Iteration 180/1000 | Loss: 0.00009212
Iteration 181/1000 | Loss: 0.00022744
Iteration 182/1000 | Loss: 0.00040410
Iteration 183/1000 | Loss: 0.00031675
Iteration 184/1000 | Loss: 0.00024547
Iteration 185/1000 | Loss: 0.00010414
Iteration 186/1000 | Loss: 0.00022827
Iteration 187/1000 | Loss: 0.00019694
Iteration 188/1000 | Loss: 0.00062974
Iteration 189/1000 | Loss: 0.00055418
Iteration 190/1000 | Loss: 0.00027309
Iteration 191/1000 | Loss: 0.00015862
Iteration 192/1000 | Loss: 0.00024561
Iteration 193/1000 | Loss: 0.00010564
Iteration 194/1000 | Loss: 0.00026022
Iteration 195/1000 | Loss: 0.00021144
Iteration 196/1000 | Loss: 0.00005732
Iteration 197/1000 | Loss: 0.00031139
Iteration 198/1000 | Loss: 0.00043693
Iteration 199/1000 | Loss: 0.00005945
Iteration 200/1000 | Loss: 0.00007393
Iteration 201/1000 | Loss: 0.00005283
Iteration 202/1000 | Loss: 0.00008420
Iteration 203/1000 | Loss: 0.00051304
Iteration 204/1000 | Loss: 0.00040670
Iteration 205/1000 | Loss: 0.00027407
Iteration 206/1000 | Loss: 0.00013237
Iteration 207/1000 | Loss: 0.00010739
Iteration 208/1000 | Loss: 0.00023360
Iteration 209/1000 | Loss: 0.00024844
Iteration 210/1000 | Loss: 0.00042575
Iteration 211/1000 | Loss: 0.00007461
Iteration 212/1000 | Loss: 0.00033729
Iteration 213/1000 | Loss: 0.00006878
Iteration 214/1000 | Loss: 0.00006573
Iteration 215/1000 | Loss: 0.00019758
Iteration 216/1000 | Loss: 0.00005465
Iteration 217/1000 | Loss: 0.00006211
Iteration 218/1000 | Loss: 0.00006674
Iteration 219/1000 | Loss: 0.00026502
Iteration 220/1000 | Loss: 0.00006954
Iteration 221/1000 | Loss: 0.00037494
Iteration 222/1000 | Loss: 0.00022020
Iteration 223/1000 | Loss: 0.00096070
Iteration 224/1000 | Loss: 0.00012182
Iteration 225/1000 | Loss: 0.00007166
Iteration 226/1000 | Loss: 0.00011937
Iteration 227/1000 | Loss: 0.00015758
Iteration 228/1000 | Loss: 0.00009345
Iteration 229/1000 | Loss: 0.00007929
Iteration 230/1000 | Loss: 0.00007023
Iteration 231/1000 | Loss: 0.00005136
Iteration 232/1000 | Loss: 0.00005191
Iteration 233/1000 | Loss: 0.00006279
Iteration 234/1000 | Loss: 0.00007607
Iteration 235/1000 | Loss: 0.00048828
Iteration 236/1000 | Loss: 0.00035515
Iteration 237/1000 | Loss: 0.00007522
Iteration 238/1000 | Loss: 0.00006239
Iteration 239/1000 | Loss: 0.00048407
Iteration 240/1000 | Loss: 0.00197282
Iteration 241/1000 | Loss: 0.00048784
Iteration 242/1000 | Loss: 0.00018148
Iteration 243/1000 | Loss: 0.00110814
Iteration 244/1000 | Loss: 0.00033840
Iteration 245/1000 | Loss: 0.00035443
Iteration 246/1000 | Loss: 0.00005600
Iteration 247/1000 | Loss: 0.00005010
Iteration 248/1000 | Loss: 0.00004592
Iteration 249/1000 | Loss: 0.00005202
Iteration 250/1000 | Loss: 0.00004565
Iteration 251/1000 | Loss: 0.00004715
Iteration 252/1000 | Loss: 0.00003795
Iteration 253/1000 | Loss: 0.00003637
Iteration 254/1000 | Loss: 0.00023057
Iteration 255/1000 | Loss: 0.00005184
Iteration 256/1000 | Loss: 0.00003776
Iteration 257/1000 | Loss: 0.00007171
Iteration 258/1000 | Loss: 0.00060075
Iteration 259/1000 | Loss: 0.00004231
Iteration 260/1000 | Loss: 0.00008889
Iteration 261/1000 | Loss: 0.00003407
Iteration 262/1000 | Loss: 0.00003470
Iteration 263/1000 | Loss: 0.00029379
Iteration 264/1000 | Loss: 0.00024750
Iteration 265/1000 | Loss: 0.00028302
Iteration 266/1000 | Loss: 0.00019879
Iteration 267/1000 | Loss: 0.00070759
Iteration 268/1000 | Loss: 0.00012686
Iteration 269/1000 | Loss: 0.00003571
Iteration 270/1000 | Loss: 0.00013550
Iteration 271/1000 | Loss: 0.00004384
Iteration 272/1000 | Loss: 0.00005417
Iteration 273/1000 | Loss: 0.00004765
Iteration 274/1000 | Loss: 0.00005875
Iteration 275/1000 | Loss: 0.00004653
Iteration 276/1000 | Loss: 0.00020888
Iteration 277/1000 | Loss: 0.00035542
Iteration 278/1000 | Loss: 0.00038190
Iteration 279/1000 | Loss: 0.00010904
Iteration 280/1000 | Loss: 0.00004408
Iteration 281/1000 | Loss: 0.00003646
Iteration 282/1000 | Loss: 0.00003819
Iteration 283/1000 | Loss: 0.00003434
Iteration 284/1000 | Loss: 0.00026876
Iteration 285/1000 | Loss: 0.00041813
Iteration 286/1000 | Loss: 0.00027767
Iteration 287/1000 | Loss: 0.00040078
Iteration 288/1000 | Loss: 0.00016756
Iteration 289/1000 | Loss: 0.00019447
Iteration 290/1000 | Loss: 0.00047136
Iteration 291/1000 | Loss: 0.00024647
Iteration 292/1000 | Loss: 0.00009130
Iteration 293/1000 | Loss: 0.00003899
Iteration 294/1000 | Loss: 0.00026133
Iteration 295/1000 | Loss: 0.00013525
Iteration 296/1000 | Loss: 0.00007356
Iteration 297/1000 | Loss: 0.00029485
Iteration 298/1000 | Loss: 0.00004522
Iteration 299/1000 | Loss: 0.00003676
Iteration 300/1000 | Loss: 0.00003427
Iteration 301/1000 | Loss: 0.00003370
Iteration 302/1000 | Loss: 0.00003656
Iteration 303/1000 | Loss: 0.00037047
Iteration 304/1000 | Loss: 0.00024425
Iteration 305/1000 | Loss: 0.00024684
Iteration 306/1000 | Loss: 0.00003847
Iteration 307/1000 | Loss: 0.00003387
Iteration 308/1000 | Loss: 0.00003284
Iteration 309/1000 | Loss: 0.00003766
Iteration 310/1000 | Loss: 0.00003128
Iteration 311/1000 | Loss: 0.00003074
Iteration 312/1000 | Loss: 0.00003035
Iteration 313/1000 | Loss: 0.00003032
Iteration 314/1000 | Loss: 0.00003030
Iteration 315/1000 | Loss: 0.00003456
Iteration 316/1000 | Loss: 0.00003025
Iteration 317/1000 | Loss: 0.00003025
Iteration 318/1000 | Loss: 0.00003025
Iteration 319/1000 | Loss: 0.00003024
Iteration 320/1000 | Loss: 0.00003024
Iteration 321/1000 | Loss: 0.00003024
Iteration 322/1000 | Loss: 0.00003023
Iteration 323/1000 | Loss: 0.00003022
Iteration 324/1000 | Loss: 0.00003022
Iteration 325/1000 | Loss: 0.00003021
Iteration 326/1000 | Loss: 0.00003021
Iteration 327/1000 | Loss: 0.00003021
Iteration 328/1000 | Loss: 0.00003021
Iteration 329/1000 | Loss: 0.00003316
Iteration 330/1000 | Loss: 0.00003018
Iteration 331/1000 | Loss: 0.00003017
Iteration 332/1000 | Loss: 0.00003017
Iteration 333/1000 | Loss: 0.00003017
Iteration 334/1000 | Loss: 0.00003016
Iteration 335/1000 | Loss: 0.00003016
Iteration 336/1000 | Loss: 0.00003016
Iteration 337/1000 | Loss: 0.00003015
Iteration 338/1000 | Loss: 0.00003015
Iteration 339/1000 | Loss: 0.00003015
Iteration 340/1000 | Loss: 0.00003207
Iteration 341/1000 | Loss: 0.00003014
Iteration 342/1000 | Loss: 0.00003012
Iteration 343/1000 | Loss: 0.00003012
Iteration 344/1000 | Loss: 0.00003012
Iteration 345/1000 | Loss: 0.00003012
Iteration 346/1000 | Loss: 0.00003012
Iteration 347/1000 | Loss: 0.00003012
Iteration 348/1000 | Loss: 0.00003011
Iteration 349/1000 | Loss: 0.00003011
Iteration 350/1000 | Loss: 0.00003011
Iteration 351/1000 | Loss: 0.00003011
Iteration 352/1000 | Loss: 0.00003011
Iteration 353/1000 | Loss: 0.00003010
Iteration 354/1000 | Loss: 0.00003009
Iteration 355/1000 | Loss: 0.00003008
Iteration 356/1000 | Loss: 0.00003008
Iteration 357/1000 | Loss: 0.00003007
Iteration 358/1000 | Loss: 0.00003007
Iteration 359/1000 | Loss: 0.00003007
Iteration 360/1000 | Loss: 0.00003006
Iteration 361/1000 | Loss: 0.00003006
Iteration 362/1000 | Loss: 0.00003005
Iteration 363/1000 | Loss: 0.00003005
Iteration 364/1000 | Loss: 0.00003005
Iteration 365/1000 | Loss: 0.00003004
Iteration 366/1000 | Loss: 0.00003004
Iteration 367/1000 | Loss: 0.00003004
Iteration 368/1000 | Loss: 0.00003004
Iteration 369/1000 | Loss: 0.00003003
Iteration 370/1000 | Loss: 0.00003003
Iteration 371/1000 | Loss: 0.00003002
Iteration 372/1000 | Loss: 0.00003002
Iteration 373/1000 | Loss: 0.00003002
Iteration 374/1000 | Loss: 0.00003002
Iteration 375/1000 | Loss: 0.00003001
Iteration 376/1000 | Loss: 0.00003001
Iteration 377/1000 | Loss: 0.00003001
Iteration 378/1000 | Loss: 0.00003001
Iteration 379/1000 | Loss: 0.00003001
Iteration 380/1000 | Loss: 0.00003000
Iteration 381/1000 | Loss: 0.00003000
Iteration 382/1000 | Loss: 0.00002999
Iteration 383/1000 | Loss: 0.00002999
Iteration 384/1000 | Loss: 0.00002999
Iteration 385/1000 | Loss: 0.00002999
Iteration 386/1000 | Loss: 0.00002999
Iteration 387/1000 | Loss: 0.00002999
Iteration 388/1000 | Loss: 0.00002999
Iteration 389/1000 | Loss: 0.00002999
Iteration 390/1000 | Loss: 0.00002999
Iteration 391/1000 | Loss: 0.00002999
Iteration 392/1000 | Loss: 0.00002999
Iteration 393/1000 | Loss: 0.00002999
Iteration 394/1000 | Loss: 0.00002999
Iteration 395/1000 | Loss: 0.00002999
Iteration 396/1000 | Loss: 0.00002999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 396. Stopping optimization.
Last 5 losses: [2.9991464543854818e-05, 2.9991464543854818e-05, 2.9991464543854818e-05, 2.9991464543854818e-05, 2.9991464543854818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9991464543854818e-05

Optimization complete. Final v2v error: 3.7386555671691895 mm

Highest mean error: 12.594947814941406 mm for frame 203

Lowest mean error: 2.8031504154205322 mm for frame 151

Saving results

Total time: 558.9398913383484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009449
Iteration 2/25 | Loss: 0.00151742
Iteration 3/25 | Loss: 0.00129207
Iteration 4/25 | Loss: 0.00116922
Iteration 5/25 | Loss: 0.00114576
Iteration 6/25 | Loss: 0.00113976
Iteration 7/25 | Loss: 0.00113663
Iteration 8/25 | Loss: 0.00113588
Iteration 9/25 | Loss: 0.00113572
Iteration 10/25 | Loss: 0.00114123
Iteration 11/25 | Loss: 0.00114056
Iteration 12/25 | Loss: 0.00113300
Iteration 13/25 | Loss: 0.00113148
Iteration 14/25 | Loss: 0.00113053
Iteration 15/25 | Loss: 0.00113114
Iteration 16/25 | Loss: 0.00113036
Iteration 17/25 | Loss: 0.00113657
Iteration 18/25 | Loss: 0.00113338
Iteration 19/25 | Loss: 0.00113300
Iteration 20/25 | Loss: 0.00112765
Iteration 21/25 | Loss: 0.00112584
Iteration 22/25 | Loss: 0.00112537
Iteration 23/25 | Loss: 0.00112599
Iteration 24/25 | Loss: 0.00112575
Iteration 25/25 | Loss: 0.00112522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64386988
Iteration 2/25 | Loss: 0.00305213
Iteration 3/25 | Loss: 0.00305212
Iteration 4/25 | Loss: 0.00305212
Iteration 5/25 | Loss: 0.00305212
Iteration 6/25 | Loss: 0.00305212
Iteration 7/25 | Loss: 0.00305212
Iteration 8/25 | Loss: 0.00305212
Iteration 9/25 | Loss: 0.00305212
Iteration 10/25 | Loss: 0.00305212
Iteration 11/25 | Loss: 0.00305212
Iteration 12/25 | Loss: 0.00305212
Iteration 13/25 | Loss: 0.00305212
Iteration 14/25 | Loss: 0.00305212
Iteration 15/25 | Loss: 0.00305212
Iteration 16/25 | Loss: 0.00305212
Iteration 17/25 | Loss: 0.00305212
Iteration 18/25 | Loss: 0.00305212
Iteration 19/25 | Loss: 0.00305212
Iteration 20/25 | Loss: 0.00305212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0030521161388605833, 0.0030521161388605833, 0.0030521161388605833, 0.0030521161388605833, 0.0030521161388605833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030521161388605833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305212
Iteration 2/1000 | Loss: 0.00011356
Iteration 3/1000 | Loss: 0.00007634
Iteration 4/1000 | Loss: 0.00006076
Iteration 5/1000 | Loss: 0.00006193
Iteration 6/1000 | Loss: 0.00005506
Iteration 7/1000 | Loss: 0.00184835
Iteration 8/1000 | Loss: 0.00139464
Iteration 9/1000 | Loss: 0.00015387
Iteration 10/1000 | Loss: 0.00007381
Iteration 11/1000 | Loss: 0.00005813
Iteration 12/1000 | Loss: 0.00005400
Iteration 13/1000 | Loss: 0.00004949
Iteration 14/1000 | Loss: 0.00005690
Iteration 15/1000 | Loss: 0.00089143
Iteration 16/1000 | Loss: 0.00007504
Iteration 17/1000 | Loss: 0.00005028
Iteration 18/1000 | Loss: 0.00004468
Iteration 19/1000 | Loss: 0.00004457
Iteration 20/1000 | Loss: 0.00004126
Iteration 21/1000 | Loss: 0.00004701
Iteration 22/1000 | Loss: 0.00004745
Iteration 23/1000 | Loss: 0.00004150
Iteration 24/1000 | Loss: 0.00003964
Iteration 25/1000 | Loss: 0.00003833
Iteration 26/1000 | Loss: 0.00004068
Iteration 27/1000 | Loss: 0.00004042
Iteration 28/1000 | Loss: 0.00003849
Iteration 29/1000 | Loss: 0.00004132
Iteration 30/1000 | Loss: 0.00003802
Iteration 31/1000 | Loss: 0.00004497
Iteration 32/1000 | Loss: 0.00004563
Iteration 33/1000 | Loss: 0.00004147
Iteration 34/1000 | Loss: 0.00004064
Iteration 35/1000 | Loss: 0.00004158
Iteration 36/1000 | Loss: 0.00004101
Iteration 37/1000 | Loss: 0.00004165
Iteration 38/1000 | Loss: 0.00004048
Iteration 39/1000 | Loss: 0.00004167
Iteration 40/1000 | Loss: 0.00004113
Iteration 41/1000 | Loss: 0.00004317
Iteration 42/1000 | Loss: 0.00004127
Iteration 43/1000 | Loss: 0.00003950
Iteration 44/1000 | Loss: 0.00003868
Iteration 45/1000 | Loss: 0.00005165
Iteration 46/1000 | Loss: 0.00004600
Iteration 47/1000 | Loss: 0.00005007
Iteration 48/1000 | Loss: 0.00005021
Iteration 49/1000 | Loss: 0.00004716
Iteration 50/1000 | Loss: 0.00004388
Iteration 51/1000 | Loss: 0.00004752
Iteration 52/1000 | Loss: 0.00004700
Iteration 53/1000 | Loss: 0.00004496
Iteration 54/1000 | Loss: 0.00004550
Iteration 55/1000 | Loss: 0.00005575
Iteration 56/1000 | Loss: 0.00004537
Iteration 57/1000 | Loss: 0.00004537
Iteration 58/1000 | Loss: 0.00004148
Iteration 59/1000 | Loss: 0.00003940
Iteration 60/1000 | Loss: 0.00003787
Iteration 61/1000 | Loss: 0.00003686
Iteration 62/1000 | Loss: 0.00003659
Iteration 63/1000 | Loss: 0.00003650
Iteration 64/1000 | Loss: 0.00003645
Iteration 65/1000 | Loss: 0.00003642
Iteration 66/1000 | Loss: 0.00003625
Iteration 67/1000 | Loss: 0.00003623
Iteration 68/1000 | Loss: 0.00003618
Iteration 69/1000 | Loss: 0.00003617
Iteration 70/1000 | Loss: 0.00003617
Iteration 71/1000 | Loss: 0.00003616
Iteration 72/1000 | Loss: 0.00003616
Iteration 73/1000 | Loss: 0.00003615
Iteration 74/1000 | Loss: 0.00003615
Iteration 75/1000 | Loss: 0.00003613
Iteration 76/1000 | Loss: 0.00003612
Iteration 77/1000 | Loss: 0.00003612
Iteration 78/1000 | Loss: 0.00003611
Iteration 79/1000 | Loss: 0.00003607
Iteration 80/1000 | Loss: 0.00003607
Iteration 81/1000 | Loss: 0.00003607
Iteration 82/1000 | Loss: 0.00003607
Iteration 83/1000 | Loss: 0.00003606
Iteration 84/1000 | Loss: 0.00003606
Iteration 85/1000 | Loss: 0.00003606
Iteration 86/1000 | Loss: 0.00003605
Iteration 87/1000 | Loss: 0.00003605
Iteration 88/1000 | Loss: 0.00003605
Iteration 89/1000 | Loss: 0.00003605
Iteration 90/1000 | Loss: 0.00003605
Iteration 91/1000 | Loss: 0.00003605
Iteration 92/1000 | Loss: 0.00003605
Iteration 93/1000 | Loss: 0.00003605
Iteration 94/1000 | Loss: 0.00003605
Iteration 95/1000 | Loss: 0.00003604
Iteration 96/1000 | Loss: 0.00003604
Iteration 97/1000 | Loss: 0.00003604
Iteration 98/1000 | Loss: 0.00003604
Iteration 99/1000 | Loss: 0.00003604
Iteration 100/1000 | Loss: 0.00003604
Iteration 101/1000 | Loss: 0.00003603
Iteration 102/1000 | Loss: 0.00003603
Iteration 103/1000 | Loss: 0.00003603
Iteration 104/1000 | Loss: 0.00003603
Iteration 105/1000 | Loss: 0.00003603
Iteration 106/1000 | Loss: 0.00003603
Iteration 107/1000 | Loss: 0.00003603
Iteration 108/1000 | Loss: 0.00003603
Iteration 109/1000 | Loss: 0.00003603
Iteration 110/1000 | Loss: 0.00003603
Iteration 111/1000 | Loss: 0.00003602
Iteration 112/1000 | Loss: 0.00003602
Iteration 113/1000 | Loss: 0.00003602
Iteration 114/1000 | Loss: 0.00003602
Iteration 115/1000 | Loss: 0.00003602
Iteration 116/1000 | Loss: 0.00003602
Iteration 117/1000 | Loss: 0.00003602
Iteration 118/1000 | Loss: 0.00003602
Iteration 119/1000 | Loss: 0.00003601
Iteration 120/1000 | Loss: 0.00003601
Iteration 121/1000 | Loss: 0.00003601
Iteration 122/1000 | Loss: 0.00003601
Iteration 123/1000 | Loss: 0.00003601
Iteration 124/1000 | Loss: 0.00003601
Iteration 125/1000 | Loss: 0.00003601
Iteration 126/1000 | Loss: 0.00003601
Iteration 127/1000 | Loss: 0.00003601
Iteration 128/1000 | Loss: 0.00003601
Iteration 129/1000 | Loss: 0.00003601
Iteration 130/1000 | Loss: 0.00003601
Iteration 131/1000 | Loss: 0.00003601
Iteration 132/1000 | Loss: 0.00003601
Iteration 133/1000 | Loss: 0.00003601
Iteration 134/1000 | Loss: 0.00003601
Iteration 135/1000 | Loss: 0.00003601
Iteration 136/1000 | Loss: 0.00003600
Iteration 137/1000 | Loss: 0.00003600
Iteration 138/1000 | Loss: 0.00003600
Iteration 139/1000 | Loss: 0.00003600
Iteration 140/1000 | Loss: 0.00003600
Iteration 141/1000 | Loss: 0.00003600
Iteration 142/1000 | Loss: 0.00003600
Iteration 143/1000 | Loss: 0.00003600
Iteration 144/1000 | Loss: 0.00003600
Iteration 145/1000 | Loss: 0.00003600
Iteration 146/1000 | Loss: 0.00003600
Iteration 147/1000 | Loss: 0.00003600
Iteration 148/1000 | Loss: 0.00003600
Iteration 149/1000 | Loss: 0.00003600
Iteration 150/1000 | Loss: 0.00003600
Iteration 151/1000 | Loss: 0.00003600
Iteration 152/1000 | Loss: 0.00003600
Iteration 153/1000 | Loss: 0.00003600
Iteration 154/1000 | Loss: 0.00003600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [3.5997669328935444e-05, 3.5997669328935444e-05, 3.5997669328935444e-05, 3.5997669328935444e-05, 3.5997669328935444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5997669328935444e-05

Optimization complete. Final v2v error: 5.118813991546631 mm

Highest mean error: 6.629335880279541 mm for frame 136

Lowest mean error: 4.493481636047363 mm for frame 156

Saving results

Total time: 141.70008516311646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424409
Iteration 2/25 | Loss: 0.00107445
Iteration 3/25 | Loss: 0.00099351
Iteration 4/25 | Loss: 0.00097605
Iteration 5/25 | Loss: 0.00096990
Iteration 6/25 | Loss: 0.00096789
Iteration 7/25 | Loss: 0.00096754
Iteration 8/25 | Loss: 0.00096754
Iteration 9/25 | Loss: 0.00096754
Iteration 10/25 | Loss: 0.00096754
Iteration 11/25 | Loss: 0.00096754
Iteration 12/25 | Loss: 0.00096754
Iteration 13/25 | Loss: 0.00096754
Iteration 14/25 | Loss: 0.00096754
Iteration 15/25 | Loss: 0.00096754
Iteration 16/25 | Loss: 0.00096754
Iteration 17/25 | Loss: 0.00096754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009675368200987577, 0.0009675368200987577, 0.0009675368200987577, 0.0009675368200987577, 0.0009675368200987577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009675368200987577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73582625
Iteration 2/25 | Loss: 0.00344361
Iteration 3/25 | Loss: 0.00344361
Iteration 4/25 | Loss: 0.00344361
Iteration 5/25 | Loss: 0.00344361
Iteration 6/25 | Loss: 0.00344361
Iteration 7/25 | Loss: 0.00344361
Iteration 8/25 | Loss: 0.00344361
Iteration 9/25 | Loss: 0.00344361
Iteration 10/25 | Loss: 0.00344361
Iteration 11/25 | Loss: 0.00344361
Iteration 12/25 | Loss: 0.00344361
Iteration 13/25 | Loss: 0.00344361
Iteration 14/25 | Loss: 0.00344361
Iteration 15/25 | Loss: 0.00344361
Iteration 16/25 | Loss: 0.00344361
Iteration 17/25 | Loss: 0.00344361
Iteration 18/25 | Loss: 0.00344361
Iteration 19/25 | Loss: 0.00344361
Iteration 20/25 | Loss: 0.00344361
Iteration 21/25 | Loss: 0.00344361
Iteration 22/25 | Loss: 0.00344361
Iteration 23/25 | Loss: 0.00344361
Iteration 24/25 | Loss: 0.00344361
Iteration 25/25 | Loss: 0.00344361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00344361
Iteration 2/1000 | Loss: 0.00003732
Iteration 3/1000 | Loss: 0.00002382
Iteration 4/1000 | Loss: 0.00001912
Iteration 5/1000 | Loss: 0.00001704
Iteration 6/1000 | Loss: 0.00001604
Iteration 7/1000 | Loss: 0.00001539
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001460
Iteration 10/1000 | Loss: 0.00001440
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001413
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001408
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001407
Iteration 26/1000 | Loss: 0.00001407
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001407
Iteration 31/1000 | Loss: 0.00001407
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001405
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001402
Iteration 46/1000 | Loss: 0.00001402
Iteration 47/1000 | Loss: 0.00001402
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001401
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001400
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001397
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001393
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001391
Iteration 74/1000 | Loss: 0.00001391
Iteration 75/1000 | Loss: 0.00001391
Iteration 76/1000 | Loss: 0.00001391
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001390
Iteration 80/1000 | Loss: 0.00001390
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001388
Iteration 86/1000 | Loss: 0.00001388
Iteration 87/1000 | Loss: 0.00001388
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001387
Iteration 93/1000 | Loss: 0.00001387
Iteration 94/1000 | Loss: 0.00001387
Iteration 95/1000 | Loss: 0.00001387
Iteration 96/1000 | Loss: 0.00001387
Iteration 97/1000 | Loss: 0.00001387
Iteration 98/1000 | Loss: 0.00001386
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001386
Iteration 102/1000 | Loss: 0.00001386
Iteration 103/1000 | Loss: 0.00001386
Iteration 104/1000 | Loss: 0.00001385
Iteration 105/1000 | Loss: 0.00001385
Iteration 106/1000 | Loss: 0.00001385
Iteration 107/1000 | Loss: 0.00001385
Iteration 108/1000 | Loss: 0.00001385
Iteration 109/1000 | Loss: 0.00001385
Iteration 110/1000 | Loss: 0.00001385
Iteration 111/1000 | Loss: 0.00001385
Iteration 112/1000 | Loss: 0.00001385
Iteration 113/1000 | Loss: 0.00001385
Iteration 114/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.3854364624421578e-05, 1.3854364624421578e-05, 1.3854364624421578e-05, 1.3854364624421578e-05, 1.3854364624421578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3854364624421578e-05

Optimization complete. Final v2v error: 3.0829732418060303 mm

Highest mean error: 3.550616502761841 mm for frame 79

Lowest mean error: 2.6417734622955322 mm for frame 69

Saving results

Total time: 38.38805651664734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896279
Iteration 2/25 | Loss: 0.00105441
Iteration 3/25 | Loss: 0.00091842
Iteration 4/25 | Loss: 0.00090202
Iteration 5/25 | Loss: 0.00089736
Iteration 6/25 | Loss: 0.00089614
Iteration 7/25 | Loss: 0.00089614
Iteration 8/25 | Loss: 0.00089614
Iteration 9/25 | Loss: 0.00089614
Iteration 10/25 | Loss: 0.00089614
Iteration 11/25 | Loss: 0.00089614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008961362182162702, 0.0008961362182162702, 0.0008961362182162702, 0.0008961362182162702, 0.0008961362182162702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008961362182162702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66031599
Iteration 2/25 | Loss: 0.00226975
Iteration 3/25 | Loss: 0.00226975
Iteration 4/25 | Loss: 0.00226975
Iteration 5/25 | Loss: 0.00226975
Iteration 6/25 | Loss: 0.00226975
Iteration 7/25 | Loss: 0.00226975
Iteration 8/25 | Loss: 0.00226975
Iteration 9/25 | Loss: 0.00226975
Iteration 10/25 | Loss: 0.00226975
Iteration 11/25 | Loss: 0.00226975
Iteration 12/25 | Loss: 0.00226975
Iteration 13/25 | Loss: 0.00226975
Iteration 14/25 | Loss: 0.00226975
Iteration 15/25 | Loss: 0.00226975
Iteration 16/25 | Loss: 0.00226975
Iteration 17/25 | Loss: 0.00226975
Iteration 18/25 | Loss: 0.00226975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0022697478998452425, 0.0022697478998452425, 0.0022697478998452425, 0.0022697478998452425, 0.0022697478998452425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022697478998452425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226975
Iteration 2/1000 | Loss: 0.00002618
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00001644
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001366
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001336
Iteration 18/1000 | Loss: 0.00001336
Iteration 19/1000 | Loss: 0.00001335
Iteration 20/1000 | Loss: 0.00001335
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001335
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001334
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001332
Iteration 33/1000 | Loss: 0.00001332
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001316
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001313
Iteration 52/1000 | Loss: 0.00001313
Iteration 53/1000 | Loss: 0.00001312
Iteration 54/1000 | Loss: 0.00001312
Iteration 55/1000 | Loss: 0.00001312
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001310
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001310
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001307
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001307
Iteration 75/1000 | Loss: 0.00001307
Iteration 76/1000 | Loss: 0.00001307
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001306
Iteration 81/1000 | Loss: 0.00001306
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001306
Iteration 89/1000 | Loss: 0.00001306
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001305
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001305
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001302
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001302
Iteration 133/1000 | Loss: 0.00001302
Iteration 134/1000 | Loss: 0.00001302
Iteration 135/1000 | Loss: 0.00001302
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001302
Iteration 140/1000 | Loss: 0.00001302
Iteration 141/1000 | Loss: 0.00001302
Iteration 142/1000 | Loss: 0.00001302
Iteration 143/1000 | Loss: 0.00001302
Iteration 144/1000 | Loss: 0.00001302
Iteration 145/1000 | Loss: 0.00001302
Iteration 146/1000 | Loss: 0.00001302
Iteration 147/1000 | Loss: 0.00001302
Iteration 148/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3019908692513127e-05, 1.3019908692513127e-05, 1.3019908692513127e-05, 1.3019908692513127e-05, 1.3019908692513127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3019908692513127e-05

Optimization complete. Final v2v error: 3.0817909240722656 mm

Highest mean error: 3.4766812324523926 mm for frame 125

Lowest mean error: 2.7537219524383545 mm for frame 42

Saving results

Total time: 35.788543701171875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00701695
Iteration 2/25 | Loss: 0.00113386
Iteration 3/25 | Loss: 0.00100727
Iteration 4/25 | Loss: 0.00098959
Iteration 5/25 | Loss: 0.00098297
Iteration 6/25 | Loss: 0.00098191
Iteration 7/25 | Loss: 0.00098191
Iteration 8/25 | Loss: 0.00098191
Iteration 9/25 | Loss: 0.00098191
Iteration 10/25 | Loss: 0.00098191
Iteration 11/25 | Loss: 0.00098191
Iteration 12/25 | Loss: 0.00098191
Iteration 13/25 | Loss: 0.00098191
Iteration 14/25 | Loss: 0.00098191
Iteration 15/25 | Loss: 0.00098191
Iteration 16/25 | Loss: 0.00098191
Iteration 17/25 | Loss: 0.00098191
Iteration 18/25 | Loss: 0.00098191
Iteration 19/25 | Loss: 0.00098191
Iteration 20/25 | Loss: 0.00098191
Iteration 21/25 | Loss: 0.00098191
Iteration 22/25 | Loss: 0.00098191
Iteration 23/25 | Loss: 0.00098191
Iteration 24/25 | Loss: 0.00098191
Iteration 25/25 | Loss: 0.00098191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.33578014
Iteration 2/25 | Loss: 0.00250173
Iteration 3/25 | Loss: 0.00250165
Iteration 4/25 | Loss: 0.00250165
Iteration 5/25 | Loss: 0.00250165
Iteration 6/25 | Loss: 0.00250165
Iteration 7/25 | Loss: 0.00250165
Iteration 8/25 | Loss: 0.00250165
Iteration 9/25 | Loss: 0.00250165
Iteration 10/25 | Loss: 0.00250165
Iteration 11/25 | Loss: 0.00250165
Iteration 12/25 | Loss: 0.00250165
Iteration 13/25 | Loss: 0.00250165
Iteration 14/25 | Loss: 0.00250165
Iteration 15/25 | Loss: 0.00250165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002501649083569646, 0.002501649083569646, 0.002501649083569646, 0.002501649083569646, 0.002501649083569646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002501649083569646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250165
Iteration 2/1000 | Loss: 0.00004716
Iteration 3/1000 | Loss: 0.00003687
Iteration 4/1000 | Loss: 0.00003311
Iteration 5/1000 | Loss: 0.00003166
Iteration 6/1000 | Loss: 0.00003009
Iteration 7/1000 | Loss: 0.00002899
Iteration 8/1000 | Loss: 0.00002841
Iteration 9/1000 | Loss: 0.00002799
Iteration 10/1000 | Loss: 0.00002768
Iteration 11/1000 | Loss: 0.00002749
Iteration 12/1000 | Loss: 0.00002735
Iteration 13/1000 | Loss: 0.00002734
Iteration 14/1000 | Loss: 0.00002734
Iteration 15/1000 | Loss: 0.00002734
Iteration 16/1000 | Loss: 0.00002733
Iteration 17/1000 | Loss: 0.00002733
Iteration 18/1000 | Loss: 0.00002732
Iteration 19/1000 | Loss: 0.00002732
Iteration 20/1000 | Loss: 0.00002731
Iteration 21/1000 | Loss: 0.00002731
Iteration 22/1000 | Loss: 0.00002730
Iteration 23/1000 | Loss: 0.00002730
Iteration 24/1000 | Loss: 0.00002730
Iteration 25/1000 | Loss: 0.00002729
Iteration 26/1000 | Loss: 0.00002728
Iteration 27/1000 | Loss: 0.00002728
Iteration 28/1000 | Loss: 0.00002728
Iteration 29/1000 | Loss: 0.00002728
Iteration 30/1000 | Loss: 0.00002727
Iteration 31/1000 | Loss: 0.00002727
Iteration 32/1000 | Loss: 0.00002727
Iteration 33/1000 | Loss: 0.00002727
Iteration 34/1000 | Loss: 0.00002727
Iteration 35/1000 | Loss: 0.00002727
Iteration 36/1000 | Loss: 0.00002726
Iteration 37/1000 | Loss: 0.00002725
Iteration 38/1000 | Loss: 0.00002724
Iteration 39/1000 | Loss: 0.00002724
Iteration 40/1000 | Loss: 0.00002724
Iteration 41/1000 | Loss: 0.00002724
Iteration 42/1000 | Loss: 0.00002724
Iteration 43/1000 | Loss: 0.00002723
Iteration 44/1000 | Loss: 0.00002723
Iteration 45/1000 | Loss: 0.00002723
Iteration 46/1000 | Loss: 0.00002723
Iteration 47/1000 | Loss: 0.00002723
Iteration 48/1000 | Loss: 0.00002722
Iteration 49/1000 | Loss: 0.00002722
Iteration 50/1000 | Loss: 0.00002722
Iteration 51/1000 | Loss: 0.00002721
Iteration 52/1000 | Loss: 0.00002721
Iteration 53/1000 | Loss: 0.00002721
Iteration 54/1000 | Loss: 0.00002721
Iteration 55/1000 | Loss: 0.00002720
Iteration 56/1000 | Loss: 0.00002720
Iteration 57/1000 | Loss: 0.00002720
Iteration 58/1000 | Loss: 0.00002720
Iteration 59/1000 | Loss: 0.00002719
Iteration 60/1000 | Loss: 0.00002719
Iteration 61/1000 | Loss: 0.00002719
Iteration 62/1000 | Loss: 0.00002719
Iteration 63/1000 | Loss: 0.00002719
Iteration 64/1000 | Loss: 0.00002719
Iteration 65/1000 | Loss: 0.00002719
Iteration 66/1000 | Loss: 0.00002719
Iteration 67/1000 | Loss: 0.00002718
Iteration 68/1000 | Loss: 0.00002718
Iteration 69/1000 | Loss: 0.00002718
Iteration 70/1000 | Loss: 0.00002718
Iteration 71/1000 | Loss: 0.00002718
Iteration 72/1000 | Loss: 0.00002718
Iteration 73/1000 | Loss: 0.00002717
Iteration 74/1000 | Loss: 0.00002717
Iteration 75/1000 | Loss: 0.00002717
Iteration 76/1000 | Loss: 0.00002717
Iteration 77/1000 | Loss: 0.00002717
Iteration 78/1000 | Loss: 0.00002717
Iteration 79/1000 | Loss: 0.00002717
Iteration 80/1000 | Loss: 0.00002717
Iteration 81/1000 | Loss: 0.00002717
Iteration 82/1000 | Loss: 0.00002717
Iteration 83/1000 | Loss: 0.00002717
Iteration 84/1000 | Loss: 0.00002717
Iteration 85/1000 | Loss: 0.00002717
Iteration 86/1000 | Loss: 0.00002717
Iteration 87/1000 | Loss: 0.00002717
Iteration 88/1000 | Loss: 0.00002716
Iteration 89/1000 | Loss: 0.00002716
Iteration 90/1000 | Loss: 0.00002716
Iteration 91/1000 | Loss: 0.00002716
Iteration 92/1000 | Loss: 0.00002716
Iteration 93/1000 | Loss: 0.00002715
Iteration 94/1000 | Loss: 0.00002715
Iteration 95/1000 | Loss: 0.00002715
Iteration 96/1000 | Loss: 0.00002715
Iteration 97/1000 | Loss: 0.00002715
Iteration 98/1000 | Loss: 0.00002715
Iteration 99/1000 | Loss: 0.00002715
Iteration 100/1000 | Loss: 0.00002715
Iteration 101/1000 | Loss: 0.00002715
Iteration 102/1000 | Loss: 0.00002715
Iteration 103/1000 | Loss: 0.00002715
Iteration 104/1000 | Loss: 0.00002715
Iteration 105/1000 | Loss: 0.00002715
Iteration 106/1000 | Loss: 0.00002715
Iteration 107/1000 | Loss: 0.00002715
Iteration 108/1000 | Loss: 0.00002715
Iteration 109/1000 | Loss: 0.00002715
Iteration 110/1000 | Loss: 0.00002715
Iteration 111/1000 | Loss: 0.00002715
Iteration 112/1000 | Loss: 0.00002715
Iteration 113/1000 | Loss: 0.00002715
Iteration 114/1000 | Loss: 0.00002715
Iteration 115/1000 | Loss: 0.00002715
Iteration 116/1000 | Loss: 0.00002715
Iteration 117/1000 | Loss: 0.00002715
Iteration 118/1000 | Loss: 0.00002715
Iteration 119/1000 | Loss: 0.00002715
Iteration 120/1000 | Loss: 0.00002715
Iteration 121/1000 | Loss: 0.00002715
Iteration 122/1000 | Loss: 0.00002715
Iteration 123/1000 | Loss: 0.00002715
Iteration 124/1000 | Loss: 0.00002715
Iteration 125/1000 | Loss: 0.00002715
Iteration 126/1000 | Loss: 0.00002715
Iteration 127/1000 | Loss: 0.00002715
Iteration 128/1000 | Loss: 0.00002715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.71464632533025e-05, 2.71464632533025e-05, 2.71464632533025e-05, 2.71464632533025e-05, 2.71464632533025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.71464632533025e-05

Optimization complete. Final v2v error: 4.418856620788574 mm

Highest mean error: 4.930402755737305 mm for frame 55

Lowest mean error: 4.025699138641357 mm for frame 118

Saving results

Total time: 33.44820284843445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108446
Iteration 2/25 | Loss: 0.00577170
Iteration 3/25 | Loss: 0.00337507
Iteration 4/25 | Loss: 0.00280246
Iteration 5/25 | Loss: 0.00259070
Iteration 6/25 | Loss: 0.00234021
Iteration 7/25 | Loss: 0.00215184
Iteration 8/25 | Loss: 0.00203067
Iteration 9/25 | Loss: 0.00196279
Iteration 10/25 | Loss: 0.00194833
Iteration 11/25 | Loss: 0.00191772
Iteration 12/25 | Loss: 0.00188704
Iteration 13/25 | Loss: 0.00186876
Iteration 14/25 | Loss: 0.00187038
Iteration 15/25 | Loss: 0.00182889
Iteration 16/25 | Loss: 0.00185709
Iteration 17/25 | Loss: 0.00183425
Iteration 18/25 | Loss: 0.00182798
Iteration 19/25 | Loss: 0.00181175
Iteration 20/25 | Loss: 0.00180196
Iteration 21/25 | Loss: 0.00179425
Iteration 22/25 | Loss: 0.00179113
Iteration 23/25 | Loss: 0.00177900
Iteration 24/25 | Loss: 0.00177165
Iteration 25/25 | Loss: 0.00176624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68719113
Iteration 2/25 | Loss: 0.01200983
Iteration 3/25 | Loss: 0.01027151
Iteration 4/25 | Loss: 0.00867604
Iteration 5/25 | Loss: 0.00784960
Iteration 6/25 | Loss: 0.00784959
Iteration 7/25 | Loss: 0.00784959
Iteration 8/25 | Loss: 0.00784959
Iteration 9/25 | Loss: 0.00784959
Iteration 10/25 | Loss: 0.00784959
Iteration 11/25 | Loss: 0.00784959
Iteration 12/25 | Loss: 0.00784959
Iteration 13/25 | Loss: 0.00784959
Iteration 14/25 | Loss: 0.00784959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007849592715501785, 0.007849592715501785, 0.007849592715501785, 0.007849592715501785, 0.007849592715501785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007849592715501785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00784959
Iteration 2/1000 | Loss: 0.00448167
Iteration 3/1000 | Loss: 0.00560326
Iteration 4/1000 | Loss: 0.01346097
Iteration 5/1000 | Loss: 0.00293069
Iteration 6/1000 | Loss: 0.00163629
Iteration 7/1000 | Loss: 0.00092872
Iteration 8/1000 | Loss: 0.00386833
Iteration 9/1000 | Loss: 0.00278413
Iteration 10/1000 | Loss: 0.00154338
Iteration 11/1000 | Loss: 0.00280655
Iteration 12/1000 | Loss: 0.00270744
Iteration 13/1000 | Loss: 0.00204219
Iteration 14/1000 | Loss: 0.00298176
Iteration 15/1000 | Loss: 0.00281801
Iteration 16/1000 | Loss: 0.00374496
Iteration 17/1000 | Loss: 0.00168753
Iteration 18/1000 | Loss: 0.00094335
Iteration 19/1000 | Loss: 0.00096088
Iteration 20/1000 | Loss: 0.00076788
Iteration 21/1000 | Loss: 0.00066171
Iteration 22/1000 | Loss: 0.00072723
Iteration 23/1000 | Loss: 0.00152661
Iteration 24/1000 | Loss: 0.00153021
Iteration 25/1000 | Loss: 0.00102605
Iteration 26/1000 | Loss: 0.00102896
Iteration 27/1000 | Loss: 0.00116287
Iteration 28/1000 | Loss: 0.00203338
Iteration 29/1000 | Loss: 0.00059046
Iteration 30/1000 | Loss: 0.00123302
Iteration 31/1000 | Loss: 0.00052668
Iteration 32/1000 | Loss: 0.00081494
Iteration 33/1000 | Loss: 0.00046139
Iteration 34/1000 | Loss: 0.00042296
Iteration 35/1000 | Loss: 0.00041237
Iteration 36/1000 | Loss: 0.00068339
Iteration 37/1000 | Loss: 0.00044381
Iteration 38/1000 | Loss: 0.00047039
Iteration 39/1000 | Loss: 0.00133943
Iteration 40/1000 | Loss: 0.00045701
Iteration 41/1000 | Loss: 0.00040286
Iteration 42/1000 | Loss: 0.00117258
Iteration 43/1000 | Loss: 0.00058046
Iteration 44/1000 | Loss: 0.00032481
Iteration 45/1000 | Loss: 0.00030805
Iteration 46/1000 | Loss: 0.00046584
Iteration 47/1000 | Loss: 0.00083942
Iteration 48/1000 | Loss: 0.00044303
Iteration 49/1000 | Loss: 0.00066142
Iteration 50/1000 | Loss: 0.00032317
Iteration 51/1000 | Loss: 0.00027966
Iteration 52/1000 | Loss: 0.00027169
Iteration 53/1000 | Loss: 0.00026640
Iteration 54/1000 | Loss: 0.00111704
Iteration 55/1000 | Loss: 0.00052173
Iteration 56/1000 | Loss: 0.00169088
Iteration 57/1000 | Loss: 0.01316870
Iteration 58/1000 | Loss: 0.00396235
Iteration 59/1000 | Loss: 0.00542085
Iteration 60/1000 | Loss: 0.00978489
Iteration 61/1000 | Loss: 0.00134094
Iteration 62/1000 | Loss: 0.00076081
Iteration 63/1000 | Loss: 0.00054032
Iteration 64/1000 | Loss: 0.00254948
Iteration 65/1000 | Loss: 0.00058345
Iteration 66/1000 | Loss: 0.00251094
Iteration 67/1000 | Loss: 0.00179797
Iteration 68/1000 | Loss: 0.00042340
Iteration 69/1000 | Loss: 0.00014636
Iteration 70/1000 | Loss: 0.00021882
Iteration 71/1000 | Loss: 0.00041691
Iteration 72/1000 | Loss: 0.00086659
Iteration 73/1000 | Loss: 0.00009025
Iteration 74/1000 | Loss: 0.00083675
Iteration 75/1000 | Loss: 0.00007380
Iteration 76/1000 | Loss: 0.00032921
Iteration 77/1000 | Loss: 0.00007984
Iteration 78/1000 | Loss: 0.00026516
Iteration 79/1000 | Loss: 0.00007714
Iteration 80/1000 | Loss: 0.00040883
Iteration 81/1000 | Loss: 0.00037735
Iteration 82/1000 | Loss: 0.00005709
Iteration 83/1000 | Loss: 0.00021823
Iteration 84/1000 | Loss: 0.00005818
Iteration 85/1000 | Loss: 0.00005776
Iteration 86/1000 | Loss: 0.00033324
Iteration 87/1000 | Loss: 0.00004701
Iteration 88/1000 | Loss: 0.00004683
Iteration 89/1000 | Loss: 0.00085493
Iteration 90/1000 | Loss: 0.00062553
Iteration 91/1000 | Loss: 0.00091344
Iteration 92/1000 | Loss: 0.00055398
Iteration 93/1000 | Loss: 0.00179624
Iteration 94/1000 | Loss: 0.00100404
Iteration 95/1000 | Loss: 0.00011721
Iteration 96/1000 | Loss: 0.00014647
Iteration 97/1000 | Loss: 0.00017135
Iteration 98/1000 | Loss: 0.00019227
Iteration 99/1000 | Loss: 0.00005636
Iteration 100/1000 | Loss: 0.00015846
Iteration 101/1000 | Loss: 0.00103864
Iteration 102/1000 | Loss: 0.00055075
Iteration 103/1000 | Loss: 0.00005996
Iteration 104/1000 | Loss: 0.00026448
Iteration 105/1000 | Loss: 0.00024959
Iteration 106/1000 | Loss: 0.00015903
Iteration 107/1000 | Loss: 0.00039417
Iteration 108/1000 | Loss: 0.00005706
Iteration 109/1000 | Loss: 0.00027571
Iteration 110/1000 | Loss: 0.00053238
Iteration 111/1000 | Loss: 0.00031452
Iteration 112/1000 | Loss: 0.00013222
Iteration 113/1000 | Loss: 0.00005935
Iteration 114/1000 | Loss: 0.00004259
Iteration 115/1000 | Loss: 0.00003883
Iteration 116/1000 | Loss: 0.00024143
Iteration 117/1000 | Loss: 0.00025427
Iteration 118/1000 | Loss: 0.00009021
Iteration 119/1000 | Loss: 0.00009060
Iteration 120/1000 | Loss: 0.00006932
Iteration 121/1000 | Loss: 0.00008408
Iteration 122/1000 | Loss: 0.00006143
Iteration 123/1000 | Loss: 0.00017027
Iteration 124/1000 | Loss: 0.00005422
Iteration 125/1000 | Loss: 0.00016475
Iteration 126/1000 | Loss: 0.00006407
Iteration 127/1000 | Loss: 0.00009441
Iteration 128/1000 | Loss: 0.00006770
Iteration 129/1000 | Loss: 0.00008076
Iteration 130/1000 | Loss: 0.00006394
Iteration 131/1000 | Loss: 0.00008504
Iteration 132/1000 | Loss: 0.00015550
Iteration 133/1000 | Loss: 0.00039363
Iteration 134/1000 | Loss: 0.00004705
Iteration 135/1000 | Loss: 0.00008373
Iteration 136/1000 | Loss: 0.00006589
Iteration 137/1000 | Loss: 0.00003582
Iteration 138/1000 | Loss: 0.00003486
Iteration 139/1000 | Loss: 0.00020423
Iteration 140/1000 | Loss: 0.00004389
Iteration 141/1000 | Loss: 0.00005632
Iteration 142/1000 | Loss: 0.00003466
Iteration 143/1000 | Loss: 0.00016534
Iteration 144/1000 | Loss: 0.00013018
Iteration 145/1000 | Loss: 0.00005675
Iteration 146/1000 | Loss: 0.00013590
Iteration 147/1000 | Loss: 0.00005171
Iteration 148/1000 | Loss: 0.00009592
Iteration 149/1000 | Loss: 0.00003387
Iteration 150/1000 | Loss: 0.00003351
Iteration 151/1000 | Loss: 0.00003350
Iteration 152/1000 | Loss: 0.00003294
Iteration 153/1000 | Loss: 0.00003271
Iteration 154/1000 | Loss: 0.00003299
Iteration 155/1000 | Loss: 0.00034128
Iteration 156/1000 | Loss: 0.00008476
Iteration 157/1000 | Loss: 0.00004281
Iteration 158/1000 | Loss: 0.00008256
Iteration 159/1000 | Loss: 0.00006236
Iteration 160/1000 | Loss: 0.00004632
Iteration 161/1000 | Loss: 0.00003266
Iteration 162/1000 | Loss: 0.00003283
Iteration 163/1000 | Loss: 0.00003236
Iteration 164/1000 | Loss: 0.00003232
Iteration 165/1000 | Loss: 0.00003232
Iteration 166/1000 | Loss: 0.00003232
Iteration 167/1000 | Loss: 0.00003231
Iteration 168/1000 | Loss: 0.00003231
Iteration 169/1000 | Loss: 0.00003231
Iteration 170/1000 | Loss: 0.00003231
Iteration 171/1000 | Loss: 0.00003231
Iteration 172/1000 | Loss: 0.00003228
Iteration 173/1000 | Loss: 0.00003226
Iteration 174/1000 | Loss: 0.00003226
Iteration 175/1000 | Loss: 0.00003225
Iteration 176/1000 | Loss: 0.00003224
Iteration 177/1000 | Loss: 0.00003224
Iteration 178/1000 | Loss: 0.00003224
Iteration 179/1000 | Loss: 0.00003224
Iteration 180/1000 | Loss: 0.00003224
Iteration 181/1000 | Loss: 0.00003224
Iteration 182/1000 | Loss: 0.00003221
Iteration 183/1000 | Loss: 0.00003221
Iteration 184/1000 | Loss: 0.00003221
Iteration 185/1000 | Loss: 0.00003221
Iteration 186/1000 | Loss: 0.00003221
Iteration 187/1000 | Loss: 0.00003221
Iteration 188/1000 | Loss: 0.00003221
Iteration 189/1000 | Loss: 0.00003221
Iteration 190/1000 | Loss: 0.00003221
Iteration 191/1000 | Loss: 0.00003221
Iteration 192/1000 | Loss: 0.00003221
Iteration 193/1000 | Loss: 0.00003221
Iteration 194/1000 | Loss: 0.00003220
Iteration 195/1000 | Loss: 0.00003220
Iteration 196/1000 | Loss: 0.00003220
Iteration 197/1000 | Loss: 0.00003220
Iteration 198/1000 | Loss: 0.00003220
Iteration 199/1000 | Loss: 0.00003220
Iteration 200/1000 | Loss: 0.00003244
Iteration 201/1000 | Loss: 0.00003244
Iteration 202/1000 | Loss: 0.00003244
Iteration 203/1000 | Loss: 0.00003222
Iteration 204/1000 | Loss: 0.00003221
Iteration 205/1000 | Loss: 0.00003219
Iteration 206/1000 | Loss: 0.00003218
Iteration 207/1000 | Loss: 0.00003218
Iteration 208/1000 | Loss: 0.00003218
Iteration 209/1000 | Loss: 0.00003218
Iteration 210/1000 | Loss: 0.00003218
Iteration 211/1000 | Loss: 0.00003217
Iteration 212/1000 | Loss: 0.00003217
Iteration 213/1000 | Loss: 0.00003217
Iteration 214/1000 | Loss: 0.00003217
Iteration 215/1000 | Loss: 0.00003217
Iteration 216/1000 | Loss: 0.00003217
Iteration 217/1000 | Loss: 0.00003217
Iteration 218/1000 | Loss: 0.00003217
Iteration 219/1000 | Loss: 0.00003217
Iteration 220/1000 | Loss: 0.00003217
Iteration 221/1000 | Loss: 0.00003216
Iteration 222/1000 | Loss: 0.00003216
Iteration 223/1000 | Loss: 0.00003215
Iteration 224/1000 | Loss: 0.00003215
Iteration 225/1000 | Loss: 0.00003238
Iteration 226/1000 | Loss: 0.00003237
Iteration 227/1000 | Loss: 0.00003221
Iteration 228/1000 | Loss: 0.00003221
Iteration 229/1000 | Loss: 0.00003215
Iteration 230/1000 | Loss: 0.00003214
Iteration 231/1000 | Loss: 0.00003214
Iteration 232/1000 | Loss: 0.00003213
Iteration 233/1000 | Loss: 0.00003213
Iteration 234/1000 | Loss: 0.00003213
Iteration 235/1000 | Loss: 0.00003213
Iteration 236/1000 | Loss: 0.00003213
Iteration 237/1000 | Loss: 0.00003213
Iteration 238/1000 | Loss: 0.00003213
Iteration 239/1000 | Loss: 0.00003213
Iteration 240/1000 | Loss: 0.00003213
Iteration 241/1000 | Loss: 0.00003237
Iteration 242/1000 | Loss: 0.00003237
Iteration 243/1000 | Loss: 0.00003222
Iteration 244/1000 | Loss: 0.00003222
Iteration 245/1000 | Loss: 0.00003210
Iteration 246/1000 | Loss: 0.00003210
Iteration 247/1000 | Loss: 0.00003231
Iteration 248/1000 | Loss: 0.00028989
Iteration 249/1000 | Loss: 0.00028035
Iteration 250/1000 | Loss: 0.00003312
Iteration 251/1000 | Loss: 0.00004704
Iteration 252/1000 | Loss: 0.00003222
Iteration 253/1000 | Loss: 0.00003208
Iteration 254/1000 | Loss: 0.00003208
Iteration 255/1000 | Loss: 0.00003208
Iteration 256/1000 | Loss: 0.00003208
Iteration 257/1000 | Loss: 0.00003208
Iteration 258/1000 | Loss: 0.00003206
Iteration 259/1000 | Loss: 0.00003206
Iteration 260/1000 | Loss: 0.00003205
Iteration 261/1000 | Loss: 0.00003205
Iteration 262/1000 | Loss: 0.00003205
Iteration 263/1000 | Loss: 0.00003205
Iteration 264/1000 | Loss: 0.00003205
Iteration 265/1000 | Loss: 0.00003204
Iteration 266/1000 | Loss: 0.00003204
Iteration 267/1000 | Loss: 0.00003204
Iteration 268/1000 | Loss: 0.00003204
Iteration 269/1000 | Loss: 0.00003204
Iteration 270/1000 | Loss: 0.00003204
Iteration 271/1000 | Loss: 0.00003204
Iteration 272/1000 | Loss: 0.00003204
Iteration 273/1000 | Loss: 0.00003204
Iteration 274/1000 | Loss: 0.00003204
Iteration 275/1000 | Loss: 0.00003203
Iteration 276/1000 | Loss: 0.00003203
Iteration 277/1000 | Loss: 0.00003203
Iteration 278/1000 | Loss: 0.00003203
Iteration 279/1000 | Loss: 0.00003203
Iteration 280/1000 | Loss: 0.00003203
Iteration 281/1000 | Loss: 0.00003203
Iteration 282/1000 | Loss: 0.00003203
Iteration 283/1000 | Loss: 0.00003203
Iteration 284/1000 | Loss: 0.00003202
Iteration 285/1000 | Loss: 0.00003202
Iteration 286/1000 | Loss: 0.00003202
Iteration 287/1000 | Loss: 0.00003202
Iteration 288/1000 | Loss: 0.00003202
Iteration 289/1000 | Loss: 0.00003202
Iteration 290/1000 | Loss: 0.00003202
Iteration 291/1000 | Loss: 0.00003202
Iteration 292/1000 | Loss: 0.00003201
Iteration 293/1000 | Loss: 0.00003201
Iteration 294/1000 | Loss: 0.00003201
Iteration 295/1000 | Loss: 0.00003201
Iteration 296/1000 | Loss: 0.00003201
Iteration 297/1000 | Loss: 0.00003201
Iteration 298/1000 | Loss: 0.00003201
Iteration 299/1000 | Loss: 0.00003200
Iteration 300/1000 | Loss: 0.00003200
Iteration 301/1000 | Loss: 0.00003200
Iteration 302/1000 | Loss: 0.00003200
Iteration 303/1000 | Loss: 0.00003200
Iteration 304/1000 | Loss: 0.00003200
Iteration 305/1000 | Loss: 0.00003200
Iteration 306/1000 | Loss: 0.00003200
Iteration 307/1000 | Loss: 0.00003200
Iteration 308/1000 | Loss: 0.00003200
Iteration 309/1000 | Loss: 0.00003200
Iteration 310/1000 | Loss: 0.00003200
Iteration 311/1000 | Loss: 0.00003200
Iteration 312/1000 | Loss: 0.00003200
Iteration 313/1000 | Loss: 0.00003200
Iteration 314/1000 | Loss: 0.00003200
Iteration 315/1000 | Loss: 0.00003200
Iteration 316/1000 | Loss: 0.00003200
Iteration 317/1000 | Loss: 0.00003200
Iteration 318/1000 | Loss: 0.00003200
Iteration 319/1000 | Loss: 0.00003200
Iteration 320/1000 | Loss: 0.00003200
Iteration 321/1000 | Loss: 0.00003200
Iteration 322/1000 | Loss: 0.00003200
Iteration 323/1000 | Loss: 0.00003200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [3.199511411366984e-05, 3.199511411366984e-05, 3.199511411366984e-05, 3.199511411366984e-05, 3.199511411366984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.199511411366984e-05

Optimization complete. Final v2v error: 4.488008499145508 mm

Highest mean error: 12.58585262298584 mm for frame 110

Lowest mean error: 3.896012783050537 mm for frame 145

Saving results

Total time: 297.5938775539398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522204
Iteration 2/25 | Loss: 0.00107400
Iteration 3/25 | Loss: 0.00095100
Iteration 4/25 | Loss: 0.00092834
Iteration 5/25 | Loss: 0.00092118
Iteration 6/25 | Loss: 0.00091911
Iteration 7/25 | Loss: 0.00091864
Iteration 8/25 | Loss: 0.00091864
Iteration 9/25 | Loss: 0.00091864
Iteration 10/25 | Loss: 0.00091864
Iteration 11/25 | Loss: 0.00091864
Iteration 12/25 | Loss: 0.00091864
Iteration 13/25 | Loss: 0.00091864
Iteration 14/25 | Loss: 0.00091864
Iteration 15/25 | Loss: 0.00091864
Iteration 16/25 | Loss: 0.00091864
Iteration 17/25 | Loss: 0.00091864
Iteration 18/25 | Loss: 0.00091864
Iteration 19/25 | Loss: 0.00091864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009186351089738309, 0.0009186351089738309, 0.0009186351089738309, 0.0009186351089738309, 0.0009186351089738309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009186351089738309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66297781
Iteration 2/25 | Loss: 0.00225975
Iteration 3/25 | Loss: 0.00225970
Iteration 4/25 | Loss: 0.00225970
Iteration 5/25 | Loss: 0.00225970
Iteration 6/25 | Loss: 0.00225970
Iteration 7/25 | Loss: 0.00225970
Iteration 8/25 | Loss: 0.00225970
Iteration 9/25 | Loss: 0.00225970
Iteration 10/25 | Loss: 0.00225970
Iteration 11/25 | Loss: 0.00225970
Iteration 12/25 | Loss: 0.00225970
Iteration 13/25 | Loss: 0.00225970
Iteration 14/25 | Loss: 0.00225970
Iteration 15/25 | Loss: 0.00225970
Iteration 16/25 | Loss: 0.00225970
Iteration 17/25 | Loss: 0.00225970
Iteration 18/25 | Loss: 0.00225970
Iteration 19/25 | Loss: 0.00225970
Iteration 20/25 | Loss: 0.00225970
Iteration 21/25 | Loss: 0.00225970
Iteration 22/25 | Loss: 0.00225970
Iteration 23/25 | Loss: 0.00225970
Iteration 24/25 | Loss: 0.00225970
Iteration 25/25 | Loss: 0.00225970

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225970
Iteration 2/1000 | Loss: 0.00002668
Iteration 3/1000 | Loss: 0.00001858
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001339
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001313
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001296
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001290
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001287
Iteration 23/1000 | Loss: 0.00001286
Iteration 24/1000 | Loss: 0.00001286
Iteration 25/1000 | Loss: 0.00001286
Iteration 26/1000 | Loss: 0.00001286
Iteration 27/1000 | Loss: 0.00001285
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001284
Iteration 32/1000 | Loss: 0.00001284
Iteration 33/1000 | Loss: 0.00001284
Iteration 34/1000 | Loss: 0.00001284
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001283
Iteration 37/1000 | Loss: 0.00001283
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001283
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001283
Iteration 47/1000 | Loss: 0.00001283
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001279
Iteration 59/1000 | Loss: 0.00001278
Iteration 60/1000 | Loss: 0.00001278
Iteration 61/1000 | Loss: 0.00001277
Iteration 62/1000 | Loss: 0.00001277
Iteration 63/1000 | Loss: 0.00001277
Iteration 64/1000 | Loss: 0.00001277
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001276
Iteration 69/1000 | Loss: 0.00001275
Iteration 70/1000 | Loss: 0.00001275
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001274
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001273
Iteration 85/1000 | Loss: 0.00001273
Iteration 86/1000 | Loss: 0.00001273
Iteration 87/1000 | Loss: 0.00001273
Iteration 88/1000 | Loss: 0.00001273
Iteration 89/1000 | Loss: 0.00001273
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001273
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.2732946743199136e-05, 1.2732946743199136e-05, 1.2732946743199136e-05, 1.2732946743199136e-05, 1.2732946743199136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2732946743199136e-05

Optimization complete. Final v2v error: 3.104858875274658 mm

Highest mean error: 3.5421009063720703 mm for frame 133

Lowest mean error: 2.620771646499634 mm for frame 8

Saving results

Total time: 30.777878284454346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01194057
Iteration 2/25 | Loss: 0.01194057
Iteration 3/25 | Loss: 0.01194057
Iteration 4/25 | Loss: 0.01194057
Iteration 5/25 | Loss: 0.01194057
Iteration 6/25 | Loss: 0.01194057
Iteration 7/25 | Loss: 0.01194057
Iteration 8/25 | Loss: 0.01194057
Iteration 9/25 | Loss: 0.01194057
Iteration 10/25 | Loss: 0.01194057
Iteration 11/25 | Loss: 0.01194057
Iteration 12/25 | Loss: 0.01194057
Iteration 13/25 | Loss: 0.01194056
Iteration 14/25 | Loss: 0.01194056
Iteration 15/25 | Loss: 0.01194056
Iteration 16/25 | Loss: 0.01194056
Iteration 17/25 | Loss: 0.01194056
Iteration 18/25 | Loss: 0.01194056
Iteration 19/25 | Loss: 0.01194056
Iteration 20/25 | Loss: 0.01194056
Iteration 21/25 | Loss: 0.01194056
Iteration 22/25 | Loss: 0.01194056
Iteration 23/25 | Loss: 0.01194056
Iteration 24/25 | Loss: 0.01194056
Iteration 25/25 | Loss: 0.01194055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.82904816
Iteration 2/25 | Loss: 0.18156774
Iteration 3/25 | Loss: 0.18153676
Iteration 4/25 | Loss: 0.18147685
Iteration 5/25 | Loss: 0.18147561
Iteration 6/25 | Loss: 0.18147555
Iteration 7/25 | Loss: 0.18147549
Iteration 8/25 | Loss: 0.18147549
Iteration 9/25 | Loss: 0.18147549
Iteration 10/25 | Loss: 0.18147549
Iteration 11/25 | Loss: 0.18147545
Iteration 12/25 | Loss: 0.18147545
Iteration 13/25 | Loss: 0.18147543
Iteration 14/25 | Loss: 0.18147543
Iteration 15/25 | Loss: 0.18147542
Iteration 16/25 | Loss: 0.18147542
Iteration 17/25 | Loss: 0.18147542
Iteration 18/25 | Loss: 0.18147542
Iteration 19/25 | Loss: 0.18147542
Iteration 20/25 | Loss: 0.18147542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.1814754158258438, 0.1814754158258438, 0.1814754158258438, 0.1814754158258438, 0.1814754158258438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1814754158258438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18147542
Iteration 2/1000 | Loss: 0.00592940
Iteration 3/1000 | Loss: 0.00150576
Iteration 4/1000 | Loss: 0.00195929
Iteration 5/1000 | Loss: 0.00154673
Iteration 6/1000 | Loss: 0.00093214
Iteration 7/1000 | Loss: 0.00020831
Iteration 8/1000 | Loss: 0.00145958
Iteration 9/1000 | Loss: 0.00011875
Iteration 10/1000 | Loss: 0.00021353
Iteration 11/1000 | Loss: 0.00081941
Iteration 12/1000 | Loss: 0.00022695
Iteration 13/1000 | Loss: 0.00023345
Iteration 14/1000 | Loss: 0.00011472
Iteration 15/1000 | Loss: 0.00007043
Iteration 16/1000 | Loss: 0.00008450
Iteration 17/1000 | Loss: 0.00007598
Iteration 18/1000 | Loss: 0.00027162
Iteration 19/1000 | Loss: 0.00005190
Iteration 20/1000 | Loss: 0.00020775
Iteration 21/1000 | Loss: 0.00004826
Iteration 22/1000 | Loss: 0.00005016
Iteration 23/1000 | Loss: 0.00014160
Iteration 24/1000 | Loss: 0.00006771
Iteration 25/1000 | Loss: 0.00042300
Iteration 26/1000 | Loss: 0.00072568
Iteration 27/1000 | Loss: 0.00017461
Iteration 28/1000 | Loss: 0.00006762
Iteration 29/1000 | Loss: 0.00005200
Iteration 30/1000 | Loss: 0.00017301
Iteration 31/1000 | Loss: 0.00011860
Iteration 32/1000 | Loss: 0.00006819
Iteration 33/1000 | Loss: 0.00006685
Iteration 34/1000 | Loss: 0.00006909
Iteration 35/1000 | Loss: 0.00008436
Iteration 36/1000 | Loss: 0.00003937
Iteration 37/1000 | Loss: 0.00003722
Iteration 38/1000 | Loss: 0.00008231
Iteration 39/1000 | Loss: 0.00010105
Iteration 40/1000 | Loss: 0.00007714
Iteration 41/1000 | Loss: 0.00003496
Iteration 42/1000 | Loss: 0.00003510
Iteration 43/1000 | Loss: 0.00006794
Iteration 44/1000 | Loss: 0.00003447
Iteration 45/1000 | Loss: 0.00003389
Iteration 46/1000 | Loss: 0.00003341
Iteration 47/1000 | Loss: 0.00003377
Iteration 48/1000 | Loss: 0.00003323
Iteration 49/1000 | Loss: 0.00005204
Iteration 50/1000 | Loss: 0.00003284
Iteration 51/1000 | Loss: 0.00003327
Iteration 52/1000 | Loss: 0.00003267
Iteration 53/1000 | Loss: 0.00003270
Iteration 54/1000 | Loss: 0.00003517
Iteration 55/1000 | Loss: 0.00008901
Iteration 56/1000 | Loss: 0.00003384
Iteration 57/1000 | Loss: 0.00007112
Iteration 58/1000 | Loss: 0.00005617
Iteration 59/1000 | Loss: 0.00006601
Iteration 60/1000 | Loss: 0.00003226
Iteration 61/1000 | Loss: 0.00003214
Iteration 62/1000 | Loss: 0.00003188
Iteration 63/1000 | Loss: 0.00003182
Iteration 64/1000 | Loss: 0.00003181
Iteration 65/1000 | Loss: 0.00003180
Iteration 66/1000 | Loss: 0.00005446
Iteration 67/1000 | Loss: 0.00003177
Iteration 68/1000 | Loss: 0.00003176
Iteration 69/1000 | Loss: 0.00003176
Iteration 70/1000 | Loss: 0.00003176
Iteration 71/1000 | Loss: 0.00003176
Iteration 72/1000 | Loss: 0.00003176
Iteration 73/1000 | Loss: 0.00003176
Iteration 74/1000 | Loss: 0.00003176
Iteration 75/1000 | Loss: 0.00003175
Iteration 76/1000 | Loss: 0.00003175
Iteration 77/1000 | Loss: 0.00003175
Iteration 78/1000 | Loss: 0.00003174
Iteration 79/1000 | Loss: 0.00003174
Iteration 80/1000 | Loss: 0.00003174
Iteration 81/1000 | Loss: 0.00003173
Iteration 82/1000 | Loss: 0.00003173
Iteration 83/1000 | Loss: 0.00003173
Iteration 84/1000 | Loss: 0.00003173
Iteration 85/1000 | Loss: 0.00003221
Iteration 86/1000 | Loss: 0.00004806
Iteration 87/1000 | Loss: 0.00003177
Iteration 88/1000 | Loss: 0.00003202
Iteration 89/1000 | Loss: 0.00006132
Iteration 90/1000 | Loss: 0.00003217
Iteration 91/1000 | Loss: 0.00004998
Iteration 92/1000 | Loss: 0.00003172
Iteration 93/1000 | Loss: 0.00003167
Iteration 94/1000 | Loss: 0.00003167
Iteration 95/1000 | Loss: 0.00003167
Iteration 96/1000 | Loss: 0.00003166
Iteration 97/1000 | Loss: 0.00003166
Iteration 98/1000 | Loss: 0.00003166
Iteration 99/1000 | Loss: 0.00003166
Iteration 100/1000 | Loss: 0.00003165
Iteration 101/1000 | Loss: 0.00003165
Iteration 102/1000 | Loss: 0.00003165
Iteration 103/1000 | Loss: 0.00003165
Iteration 104/1000 | Loss: 0.00003165
Iteration 105/1000 | Loss: 0.00003165
Iteration 106/1000 | Loss: 0.00003165
Iteration 107/1000 | Loss: 0.00003165
Iteration 108/1000 | Loss: 0.00003165
Iteration 109/1000 | Loss: 0.00003165
Iteration 110/1000 | Loss: 0.00003165
Iteration 111/1000 | Loss: 0.00003165
Iteration 112/1000 | Loss: 0.00003165
Iteration 113/1000 | Loss: 0.00003165
Iteration 114/1000 | Loss: 0.00003165
Iteration 115/1000 | Loss: 0.00003165
Iteration 116/1000 | Loss: 0.00003165
Iteration 117/1000 | Loss: 0.00003165
Iteration 118/1000 | Loss: 0.00003165
Iteration 119/1000 | Loss: 0.00003165
Iteration 120/1000 | Loss: 0.00003165
Iteration 121/1000 | Loss: 0.00003165
Iteration 122/1000 | Loss: 0.00003165
Iteration 123/1000 | Loss: 0.00003165
Iteration 124/1000 | Loss: 0.00003165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [3.164563895552419e-05, 3.164563895552419e-05, 3.164563895552419e-05, 3.164563895552419e-05, 3.164563895552419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.164563895552419e-05

Optimization complete. Final v2v error: 4.406706809997559 mm

Highest mean error: 10.30980396270752 mm for frame 158

Lowest mean error: 3.4272687435150146 mm for frame 207

Saving results

Total time: 122.43574714660645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00679883
Iteration 2/25 | Loss: 0.00105633
Iteration 3/25 | Loss: 0.00092904
Iteration 4/25 | Loss: 0.00090593
Iteration 5/25 | Loss: 0.00089699
Iteration 6/25 | Loss: 0.00089538
Iteration 7/25 | Loss: 0.00089490
Iteration 8/25 | Loss: 0.00089490
Iteration 9/25 | Loss: 0.00089490
Iteration 10/25 | Loss: 0.00089490
Iteration 11/25 | Loss: 0.00089490
Iteration 12/25 | Loss: 0.00089490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008948976756073534, 0.0008948976756073534, 0.0008948976756073534, 0.0008948976756073534, 0.0008948976756073534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008948976756073534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72684181
Iteration 2/25 | Loss: 0.00232393
Iteration 3/25 | Loss: 0.00232393
Iteration 4/25 | Loss: 0.00232393
Iteration 5/25 | Loss: 0.00232393
Iteration 6/25 | Loss: 0.00232393
Iteration 7/25 | Loss: 0.00232393
Iteration 8/25 | Loss: 0.00232393
Iteration 9/25 | Loss: 0.00232393
Iteration 10/25 | Loss: 0.00232393
Iteration 11/25 | Loss: 0.00232393
Iteration 12/25 | Loss: 0.00232393
Iteration 13/25 | Loss: 0.00232393
Iteration 14/25 | Loss: 0.00232393
Iteration 15/25 | Loss: 0.00232393
Iteration 16/25 | Loss: 0.00232393
Iteration 17/25 | Loss: 0.00232393
Iteration 18/25 | Loss: 0.00232393
Iteration 19/25 | Loss: 0.00232393
Iteration 20/25 | Loss: 0.00232393
Iteration 21/25 | Loss: 0.00232393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002323932247236371, 0.002323932247236371, 0.002323932247236371, 0.002323932247236371, 0.002323932247236371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002323932247236371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232393
Iteration 2/1000 | Loss: 0.00002624
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001563
Iteration 8/1000 | Loss: 0.00001535
Iteration 9/1000 | Loss: 0.00001515
Iteration 10/1000 | Loss: 0.00001506
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001505
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001497
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001493
Iteration 23/1000 | Loss: 0.00001492
Iteration 24/1000 | Loss: 0.00001492
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001490
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001489
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001489
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001487
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001483
Iteration 48/1000 | Loss: 0.00001483
Iteration 49/1000 | Loss: 0.00001483
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001481
Iteration 60/1000 | Loss: 0.00001481
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001477
Iteration 100/1000 | Loss: 0.00001477
Iteration 101/1000 | Loss: 0.00001477
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001477
Iteration 105/1000 | Loss: 0.00001476
Iteration 106/1000 | Loss: 0.00001476
Iteration 107/1000 | Loss: 0.00001476
Iteration 108/1000 | Loss: 0.00001476
Iteration 109/1000 | Loss: 0.00001476
Iteration 110/1000 | Loss: 0.00001476
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001476
Iteration 117/1000 | Loss: 0.00001476
Iteration 118/1000 | Loss: 0.00001476
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001474
Iteration 126/1000 | Loss: 0.00001474
Iteration 127/1000 | Loss: 0.00001474
Iteration 128/1000 | Loss: 0.00001474
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001474
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001474
Iteration 149/1000 | Loss: 0.00001474
Iteration 150/1000 | Loss: 0.00001474
Iteration 151/1000 | Loss: 0.00001474
Iteration 152/1000 | Loss: 0.00001474
Iteration 153/1000 | Loss: 0.00001474
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.4737685887666885e-05, 1.4737685887666885e-05, 1.4737685887666885e-05, 1.4737685887666885e-05, 1.4737685887666885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4737685887666885e-05

Optimization complete. Final v2v error: 3.29020619392395 mm

Highest mean error: 3.723428964614868 mm for frame 83

Lowest mean error: 2.882242441177368 mm for frame 136

Saving results

Total time: 33.04764676094055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568703
Iteration 2/25 | Loss: 0.00108520
Iteration 3/25 | Loss: 0.00098290
Iteration 4/25 | Loss: 0.00097076
Iteration 5/25 | Loss: 0.00096724
Iteration 6/25 | Loss: 0.00096676
Iteration 7/25 | Loss: 0.00096676
Iteration 8/25 | Loss: 0.00096676
Iteration 9/25 | Loss: 0.00096676
Iteration 10/25 | Loss: 0.00096676
Iteration 11/25 | Loss: 0.00096676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009667599224485457, 0.0009667599224485457, 0.0009667599224485457, 0.0009667599224485457, 0.0009667599224485457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009667599224485457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63299799
Iteration 2/25 | Loss: 0.00222079
Iteration 3/25 | Loss: 0.00222075
Iteration 4/25 | Loss: 0.00222075
Iteration 5/25 | Loss: 0.00222075
Iteration 6/25 | Loss: 0.00222075
Iteration 7/25 | Loss: 0.00222075
Iteration 8/25 | Loss: 0.00222075
Iteration 9/25 | Loss: 0.00222075
Iteration 10/25 | Loss: 0.00222075
Iteration 11/25 | Loss: 0.00222075
Iteration 12/25 | Loss: 0.00222075
Iteration 13/25 | Loss: 0.00222075
Iteration 14/25 | Loss: 0.00222075
Iteration 15/25 | Loss: 0.00222075
Iteration 16/25 | Loss: 0.00222075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022207479923963547, 0.0022207479923963547, 0.0022207479923963547, 0.0022207479923963547, 0.0022207479923963547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022207479923963547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222075
Iteration 2/1000 | Loss: 0.00004630
Iteration 3/1000 | Loss: 0.00003466
Iteration 4/1000 | Loss: 0.00003036
Iteration 5/1000 | Loss: 0.00002774
Iteration 6/1000 | Loss: 0.00002637
Iteration 7/1000 | Loss: 0.00002542
Iteration 8/1000 | Loss: 0.00002477
Iteration 9/1000 | Loss: 0.00002420
Iteration 10/1000 | Loss: 0.00002381
Iteration 11/1000 | Loss: 0.00002361
Iteration 12/1000 | Loss: 0.00002341
Iteration 13/1000 | Loss: 0.00002332
Iteration 14/1000 | Loss: 0.00002331
Iteration 15/1000 | Loss: 0.00002327
Iteration 16/1000 | Loss: 0.00002323
Iteration 17/1000 | Loss: 0.00002323
Iteration 18/1000 | Loss: 0.00002322
Iteration 19/1000 | Loss: 0.00002321
Iteration 20/1000 | Loss: 0.00002321
Iteration 21/1000 | Loss: 0.00002321
Iteration 22/1000 | Loss: 0.00002318
Iteration 23/1000 | Loss: 0.00002315
Iteration 24/1000 | Loss: 0.00002312
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002311
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002309
Iteration 29/1000 | Loss: 0.00002309
Iteration 30/1000 | Loss: 0.00002307
Iteration 31/1000 | Loss: 0.00002303
Iteration 32/1000 | Loss: 0.00002300
Iteration 33/1000 | Loss: 0.00002300
Iteration 34/1000 | Loss: 0.00002299
Iteration 35/1000 | Loss: 0.00002299
Iteration 36/1000 | Loss: 0.00002298
Iteration 37/1000 | Loss: 0.00002298
Iteration 38/1000 | Loss: 0.00002297
Iteration 39/1000 | Loss: 0.00002297
Iteration 40/1000 | Loss: 0.00002296
Iteration 41/1000 | Loss: 0.00002296
Iteration 42/1000 | Loss: 0.00002296
Iteration 43/1000 | Loss: 0.00002296
Iteration 44/1000 | Loss: 0.00002296
Iteration 45/1000 | Loss: 0.00002296
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00002295
Iteration 48/1000 | Loss: 0.00002295
Iteration 49/1000 | Loss: 0.00002295
Iteration 50/1000 | Loss: 0.00002295
Iteration 51/1000 | Loss: 0.00002295
Iteration 52/1000 | Loss: 0.00002295
Iteration 53/1000 | Loss: 0.00002295
Iteration 54/1000 | Loss: 0.00002295
Iteration 55/1000 | Loss: 0.00002295
Iteration 56/1000 | Loss: 0.00002295
Iteration 57/1000 | Loss: 0.00002295
Iteration 58/1000 | Loss: 0.00002295
Iteration 59/1000 | Loss: 0.00002295
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002295
Iteration 62/1000 | Loss: 0.00002295
Iteration 63/1000 | Loss: 0.00002295
Iteration 64/1000 | Loss: 0.00002295
Iteration 65/1000 | Loss: 0.00002295
Iteration 66/1000 | Loss: 0.00002295
Iteration 67/1000 | Loss: 0.00002295
Iteration 68/1000 | Loss: 0.00002295
Iteration 69/1000 | Loss: 0.00002295
Iteration 70/1000 | Loss: 0.00002295
Iteration 71/1000 | Loss: 0.00002295
Iteration 72/1000 | Loss: 0.00002295
Iteration 73/1000 | Loss: 0.00002295
Iteration 74/1000 | Loss: 0.00002295
Iteration 75/1000 | Loss: 0.00002295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.2952535800868645e-05, 2.2952535800868645e-05, 2.2952535800868645e-05, 2.2952535800868645e-05, 2.2952535800868645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2952535800868645e-05

Optimization complete. Final v2v error: 4.082481384277344 mm

Highest mean error: 4.5671281814575195 mm for frame 104

Lowest mean error: 3.4228851795196533 mm for frame 238

Saving results

Total time: 36.785295486450195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1377/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1377/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948930
Iteration 2/25 | Loss: 0.00238525
Iteration 3/25 | Loss: 0.00176615
Iteration 4/25 | Loss: 0.00144842
Iteration 5/25 | Loss: 0.00142930
Iteration 6/25 | Loss: 0.00140268
Iteration 7/25 | Loss: 0.00137237
Iteration 8/25 | Loss: 0.00134381
Iteration 9/25 | Loss: 0.00131421
Iteration 10/25 | Loss: 0.00130923
Iteration 11/25 | Loss: 0.00129153
Iteration 12/25 | Loss: 0.00128860
Iteration 13/25 | Loss: 0.00127941
Iteration 14/25 | Loss: 0.00127711
Iteration 15/25 | Loss: 0.00127655
Iteration 16/25 | Loss: 0.00127620
Iteration 17/25 | Loss: 0.00127597
Iteration 18/25 | Loss: 0.00127586
Iteration 19/25 | Loss: 0.00127586
Iteration 20/25 | Loss: 0.00127583
Iteration 21/25 | Loss: 0.00127583
Iteration 22/25 | Loss: 0.00127582
Iteration 23/25 | Loss: 0.00127582
Iteration 24/25 | Loss: 0.00127582
Iteration 25/25 | Loss: 0.00127582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.57098389
Iteration 2/25 | Loss: 0.00523724
Iteration 3/25 | Loss: 0.00523724
Iteration 4/25 | Loss: 0.00523724
Iteration 5/25 | Loss: 0.00523724
Iteration 6/25 | Loss: 0.00523724
Iteration 7/25 | Loss: 0.00523723
Iteration 8/25 | Loss: 0.00523723
Iteration 9/25 | Loss: 0.00523723
Iteration 10/25 | Loss: 0.00523723
Iteration 11/25 | Loss: 0.00523723
Iteration 12/25 | Loss: 0.00523723
Iteration 13/25 | Loss: 0.00523723
Iteration 14/25 | Loss: 0.00523723
Iteration 15/25 | Loss: 0.00523723
Iteration 16/25 | Loss: 0.00523723
Iteration 17/25 | Loss: 0.00523723
Iteration 18/25 | Loss: 0.00523723
Iteration 19/25 | Loss: 0.00523723
Iteration 20/25 | Loss: 0.00523723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.005237234756350517, 0.005237234756350517, 0.005237234756350517, 0.005237234756350517, 0.005237234756350517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005237234756350517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00523723
Iteration 2/1000 | Loss: 0.00143832
Iteration 3/1000 | Loss: 0.00176057
Iteration 4/1000 | Loss: 0.00106150
Iteration 5/1000 | Loss: 0.00111257
Iteration 6/1000 | Loss: 0.00042100
Iteration 7/1000 | Loss: 0.00013459
Iteration 8/1000 | Loss: 0.00007772
Iteration 9/1000 | Loss: 0.00029836
Iteration 10/1000 | Loss: 0.00007141
Iteration 11/1000 | Loss: 0.00005113
Iteration 12/1000 | Loss: 0.00004549
Iteration 13/1000 | Loss: 0.00004227
Iteration 14/1000 | Loss: 0.00004065
Iteration 15/1000 | Loss: 0.00003936
Iteration 16/1000 | Loss: 0.00078598
Iteration 17/1000 | Loss: 0.00004906
Iteration 18/1000 | Loss: 0.00004046
Iteration 19/1000 | Loss: 0.00003636
Iteration 20/1000 | Loss: 0.00003417
Iteration 21/1000 | Loss: 0.00003285
Iteration 22/1000 | Loss: 0.00003197
Iteration 23/1000 | Loss: 0.00003157
Iteration 24/1000 | Loss: 0.00003113
Iteration 25/1000 | Loss: 0.00003080
Iteration 26/1000 | Loss: 0.00003046
Iteration 27/1000 | Loss: 0.00003043
Iteration 28/1000 | Loss: 0.00003009
Iteration 29/1000 | Loss: 0.00002982
Iteration 30/1000 | Loss: 0.00002974
Iteration 31/1000 | Loss: 0.00002952
Iteration 32/1000 | Loss: 0.00044087
Iteration 33/1000 | Loss: 0.00005249
Iteration 34/1000 | Loss: 0.00003313
Iteration 35/1000 | Loss: 0.00002934
Iteration 36/1000 | Loss: 0.00002793
Iteration 37/1000 | Loss: 0.00002727
Iteration 38/1000 | Loss: 0.00002687
Iteration 39/1000 | Loss: 0.00002664
Iteration 40/1000 | Loss: 0.00002653
Iteration 41/1000 | Loss: 0.00002650
Iteration 42/1000 | Loss: 0.00002649
Iteration 43/1000 | Loss: 0.00002649
Iteration 44/1000 | Loss: 0.00002648
Iteration 45/1000 | Loss: 0.00002647
Iteration 46/1000 | Loss: 0.00002641
Iteration 47/1000 | Loss: 0.00002637
Iteration 48/1000 | Loss: 0.00002637
Iteration 49/1000 | Loss: 0.00002636
Iteration 50/1000 | Loss: 0.00002635
Iteration 51/1000 | Loss: 0.00002635
Iteration 52/1000 | Loss: 0.00002635
Iteration 53/1000 | Loss: 0.00002634
Iteration 54/1000 | Loss: 0.00002634
Iteration 55/1000 | Loss: 0.00002633
Iteration 56/1000 | Loss: 0.00002633
Iteration 57/1000 | Loss: 0.00002631
Iteration 58/1000 | Loss: 0.00002631
Iteration 59/1000 | Loss: 0.00002628
Iteration 60/1000 | Loss: 0.00002627
Iteration 61/1000 | Loss: 0.00002627
Iteration 62/1000 | Loss: 0.00002627
Iteration 63/1000 | Loss: 0.00002626
Iteration 64/1000 | Loss: 0.00002623
Iteration 65/1000 | Loss: 0.00002622
Iteration 66/1000 | Loss: 0.00002621
Iteration 67/1000 | Loss: 0.00002621
Iteration 68/1000 | Loss: 0.00002621
Iteration 69/1000 | Loss: 0.00002620
Iteration 70/1000 | Loss: 0.00002620
Iteration 71/1000 | Loss: 0.00002620
Iteration 72/1000 | Loss: 0.00002619
Iteration 73/1000 | Loss: 0.00002619
Iteration 74/1000 | Loss: 0.00002619
Iteration 75/1000 | Loss: 0.00002619
Iteration 76/1000 | Loss: 0.00002618
Iteration 77/1000 | Loss: 0.00002618
Iteration 78/1000 | Loss: 0.00002618
Iteration 79/1000 | Loss: 0.00002618
Iteration 80/1000 | Loss: 0.00002618
Iteration 81/1000 | Loss: 0.00002618
Iteration 82/1000 | Loss: 0.00002618
Iteration 83/1000 | Loss: 0.00002617
Iteration 84/1000 | Loss: 0.00002617
Iteration 85/1000 | Loss: 0.00002617
Iteration 86/1000 | Loss: 0.00002616
Iteration 87/1000 | Loss: 0.00002616
Iteration 88/1000 | Loss: 0.00002616
Iteration 89/1000 | Loss: 0.00002616
Iteration 90/1000 | Loss: 0.00002616
Iteration 91/1000 | Loss: 0.00002616
Iteration 92/1000 | Loss: 0.00002615
Iteration 93/1000 | Loss: 0.00002615
Iteration 94/1000 | Loss: 0.00002615
Iteration 95/1000 | Loss: 0.00002615
Iteration 96/1000 | Loss: 0.00002615
Iteration 97/1000 | Loss: 0.00002615
Iteration 98/1000 | Loss: 0.00002615
Iteration 99/1000 | Loss: 0.00002614
Iteration 100/1000 | Loss: 0.00002614
Iteration 101/1000 | Loss: 0.00002614
Iteration 102/1000 | Loss: 0.00002614
Iteration 103/1000 | Loss: 0.00002614
Iteration 104/1000 | Loss: 0.00002613
Iteration 105/1000 | Loss: 0.00002613
Iteration 106/1000 | Loss: 0.00002613
Iteration 107/1000 | Loss: 0.00002613
Iteration 108/1000 | Loss: 0.00002612
Iteration 109/1000 | Loss: 0.00002612
Iteration 110/1000 | Loss: 0.00002612
Iteration 111/1000 | Loss: 0.00002612
Iteration 112/1000 | Loss: 0.00002612
Iteration 113/1000 | Loss: 0.00002612
Iteration 114/1000 | Loss: 0.00002611
Iteration 115/1000 | Loss: 0.00002611
Iteration 116/1000 | Loss: 0.00002611
Iteration 117/1000 | Loss: 0.00002611
Iteration 118/1000 | Loss: 0.00002611
Iteration 119/1000 | Loss: 0.00002611
Iteration 120/1000 | Loss: 0.00002611
Iteration 121/1000 | Loss: 0.00002611
Iteration 122/1000 | Loss: 0.00002611
Iteration 123/1000 | Loss: 0.00002611
Iteration 124/1000 | Loss: 0.00002611
Iteration 125/1000 | Loss: 0.00002611
Iteration 126/1000 | Loss: 0.00002610
Iteration 127/1000 | Loss: 0.00002610
Iteration 128/1000 | Loss: 0.00002610
Iteration 129/1000 | Loss: 0.00002610
Iteration 130/1000 | Loss: 0.00002610
Iteration 131/1000 | Loss: 0.00002610
Iteration 132/1000 | Loss: 0.00002610
Iteration 133/1000 | Loss: 0.00002610
Iteration 134/1000 | Loss: 0.00002610
Iteration 135/1000 | Loss: 0.00002610
Iteration 136/1000 | Loss: 0.00002610
Iteration 137/1000 | Loss: 0.00002610
Iteration 138/1000 | Loss: 0.00002610
Iteration 139/1000 | Loss: 0.00002610
Iteration 140/1000 | Loss: 0.00002610
Iteration 141/1000 | Loss: 0.00002610
Iteration 142/1000 | Loss: 0.00002610
Iteration 143/1000 | Loss: 0.00002610
Iteration 144/1000 | Loss: 0.00002610
Iteration 145/1000 | Loss: 0.00002610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.6101017283508554e-05, 2.6101017283508554e-05, 2.6101017283508554e-05, 2.6101017283508554e-05, 2.6101017283508554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6101017283508554e-05

Optimization complete. Final v2v error: 4.1294097900390625 mm

Highest mean error: 6.715112686157227 mm for frame 71

Lowest mean error: 3.1278648376464844 mm for frame 15

Saving results

Total time: 101.27030992507935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021433
Iteration 2/25 | Loss: 0.00256622
Iteration 3/25 | Loss: 0.00191447
Iteration 4/25 | Loss: 0.00182055
Iteration 5/25 | Loss: 0.00167354
Iteration 6/25 | Loss: 0.00159601
Iteration 7/25 | Loss: 0.00155338
Iteration 8/25 | Loss: 0.00152042
Iteration 9/25 | Loss: 0.00150536
Iteration 10/25 | Loss: 0.00148293
Iteration 11/25 | Loss: 0.00145871
Iteration 12/25 | Loss: 0.00145933
Iteration 13/25 | Loss: 0.00145944
Iteration 14/25 | Loss: 0.00146514
Iteration 15/25 | Loss: 0.00145221
Iteration 16/25 | Loss: 0.00144723
Iteration 17/25 | Loss: 0.00144059
Iteration 18/25 | Loss: 0.00143366
Iteration 19/25 | Loss: 0.00143187
Iteration 20/25 | Loss: 0.00143128
Iteration 21/25 | Loss: 0.00143111
Iteration 22/25 | Loss: 0.00143098
Iteration 23/25 | Loss: 0.00143087
Iteration 24/25 | Loss: 0.00143077
Iteration 25/25 | Loss: 0.00143060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77185822
Iteration 2/25 | Loss: 0.00282117
Iteration 3/25 | Loss: 0.00282117
Iteration 4/25 | Loss: 0.00282117
Iteration 5/25 | Loss: 0.00282117
Iteration 6/25 | Loss: 0.00282117
Iteration 7/25 | Loss: 0.00282117
Iteration 8/25 | Loss: 0.00282117
Iteration 9/25 | Loss: 0.00282117
Iteration 10/25 | Loss: 0.00282117
Iteration 11/25 | Loss: 0.00282117
Iteration 12/25 | Loss: 0.00282117
Iteration 13/25 | Loss: 0.00282117
Iteration 14/25 | Loss: 0.00282117
Iteration 15/25 | Loss: 0.00282117
Iteration 16/25 | Loss: 0.00282117
Iteration 17/25 | Loss: 0.00282117
Iteration 18/25 | Loss: 0.00282117
Iteration 19/25 | Loss: 0.00282117
Iteration 20/25 | Loss: 0.00282117
Iteration 21/25 | Loss: 0.00282117
Iteration 22/25 | Loss: 0.00282117
Iteration 23/25 | Loss: 0.00282117
Iteration 24/25 | Loss: 0.00282117
Iteration 25/25 | Loss: 0.00282117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282117
Iteration 2/1000 | Loss: 0.00031014
Iteration 3/1000 | Loss: 0.00021109
Iteration 4/1000 | Loss: 0.00107492
Iteration 5/1000 | Loss: 0.00027349
Iteration 6/1000 | Loss: 0.00020448
Iteration 7/1000 | Loss: 0.00016461
Iteration 8/1000 | Loss: 0.00014968
Iteration 9/1000 | Loss: 0.00013556
Iteration 10/1000 | Loss: 0.00012633
Iteration 11/1000 | Loss: 0.00012016
Iteration 12/1000 | Loss: 0.00011630
Iteration 13/1000 | Loss: 0.00047910
Iteration 14/1000 | Loss: 0.00013558
Iteration 15/1000 | Loss: 0.00011803
Iteration 16/1000 | Loss: 0.00042388
Iteration 17/1000 | Loss: 0.00035394
Iteration 18/1000 | Loss: 0.00012062
Iteration 19/1000 | Loss: 0.00010823
Iteration 20/1000 | Loss: 0.00010425
Iteration 21/1000 | Loss: 0.00010089
Iteration 22/1000 | Loss: 0.00009941
Iteration 23/1000 | Loss: 0.00009735
Iteration 24/1000 | Loss: 0.00009597
Iteration 25/1000 | Loss: 0.00009456
Iteration 26/1000 | Loss: 0.00009359
Iteration 27/1000 | Loss: 0.00038277
Iteration 28/1000 | Loss: 0.00010781
Iteration 29/1000 | Loss: 0.00009704
Iteration 30/1000 | Loss: 0.00009408
Iteration 31/1000 | Loss: 0.00009181
Iteration 32/1000 | Loss: 0.00009001
Iteration 33/1000 | Loss: 0.00008858
Iteration 34/1000 | Loss: 0.00008783
Iteration 35/1000 | Loss: 0.00008729
Iteration 36/1000 | Loss: 0.00017018
Iteration 37/1000 | Loss: 0.00042699
Iteration 38/1000 | Loss: 0.00101933
Iteration 39/1000 | Loss: 0.00014559
Iteration 40/1000 | Loss: 0.00010236
Iteration 41/1000 | Loss: 0.00008949
Iteration 42/1000 | Loss: 0.00007978
Iteration 43/1000 | Loss: 0.00007314
Iteration 44/1000 | Loss: 0.00006874
Iteration 45/1000 | Loss: 0.00006612
Iteration 46/1000 | Loss: 0.00006742
Iteration 47/1000 | Loss: 0.00007240
Iteration 48/1000 | Loss: 0.00007294
Iteration 49/1000 | Loss: 0.00006944
Iteration 50/1000 | Loss: 0.00006208
Iteration 51/1000 | Loss: 0.00006159
Iteration 52/1000 | Loss: 0.00006107
Iteration 53/1000 | Loss: 0.00006067
Iteration 54/1000 | Loss: 0.00006044
Iteration 55/1000 | Loss: 0.00006018
Iteration 56/1000 | Loss: 0.00006005
Iteration 57/1000 | Loss: 0.00005998
Iteration 58/1000 | Loss: 0.00005980
Iteration 59/1000 | Loss: 0.00005967
Iteration 60/1000 | Loss: 0.00005966
Iteration 61/1000 | Loss: 0.00005965
Iteration 62/1000 | Loss: 0.00005965
Iteration 63/1000 | Loss: 0.00005964
Iteration 64/1000 | Loss: 0.00005964
Iteration 65/1000 | Loss: 0.00005964
Iteration 66/1000 | Loss: 0.00005963
Iteration 67/1000 | Loss: 0.00005963
Iteration 68/1000 | Loss: 0.00005963
Iteration 69/1000 | Loss: 0.00005960
Iteration 70/1000 | Loss: 0.00005959
Iteration 71/1000 | Loss: 0.00005959
Iteration 72/1000 | Loss: 0.00005959
Iteration 73/1000 | Loss: 0.00005957
Iteration 74/1000 | Loss: 0.00005957
Iteration 75/1000 | Loss: 0.00005957
Iteration 76/1000 | Loss: 0.00005957
Iteration 77/1000 | Loss: 0.00005957
Iteration 78/1000 | Loss: 0.00005956
Iteration 79/1000 | Loss: 0.00005956
Iteration 80/1000 | Loss: 0.00005956
Iteration 81/1000 | Loss: 0.00005956
Iteration 82/1000 | Loss: 0.00005956
Iteration 83/1000 | Loss: 0.00005956
Iteration 84/1000 | Loss: 0.00005955
Iteration 85/1000 | Loss: 0.00005955
Iteration 86/1000 | Loss: 0.00005955
Iteration 87/1000 | Loss: 0.00005955
Iteration 88/1000 | Loss: 0.00005955
Iteration 89/1000 | Loss: 0.00005954
Iteration 90/1000 | Loss: 0.00005954
Iteration 91/1000 | Loss: 0.00005954
Iteration 92/1000 | Loss: 0.00005954
Iteration 93/1000 | Loss: 0.00005954
Iteration 94/1000 | Loss: 0.00005953
Iteration 95/1000 | Loss: 0.00005953
Iteration 96/1000 | Loss: 0.00005953
Iteration 97/1000 | Loss: 0.00005953
Iteration 98/1000 | Loss: 0.00005953
Iteration 99/1000 | Loss: 0.00005953
Iteration 100/1000 | Loss: 0.00005953
Iteration 101/1000 | Loss: 0.00005953
Iteration 102/1000 | Loss: 0.00005952
Iteration 103/1000 | Loss: 0.00005952
Iteration 104/1000 | Loss: 0.00005952
Iteration 105/1000 | Loss: 0.00005952
Iteration 106/1000 | Loss: 0.00005951
Iteration 107/1000 | Loss: 0.00005951
Iteration 108/1000 | Loss: 0.00005951
Iteration 109/1000 | Loss: 0.00005951
Iteration 110/1000 | Loss: 0.00005950
Iteration 111/1000 | Loss: 0.00005950
Iteration 112/1000 | Loss: 0.00005949
Iteration 113/1000 | Loss: 0.00005949
Iteration 114/1000 | Loss: 0.00005948
Iteration 115/1000 | Loss: 0.00005947
Iteration 116/1000 | Loss: 0.00005947
Iteration 117/1000 | Loss: 0.00005946
Iteration 118/1000 | Loss: 0.00005946
Iteration 119/1000 | Loss: 0.00005946
Iteration 120/1000 | Loss: 0.00005945
Iteration 121/1000 | Loss: 0.00005945
Iteration 122/1000 | Loss: 0.00005945
Iteration 123/1000 | Loss: 0.00005944
Iteration 124/1000 | Loss: 0.00005944
Iteration 125/1000 | Loss: 0.00005944
Iteration 126/1000 | Loss: 0.00005944
Iteration 127/1000 | Loss: 0.00005944
Iteration 128/1000 | Loss: 0.00005944
Iteration 129/1000 | Loss: 0.00005944
Iteration 130/1000 | Loss: 0.00005944
Iteration 131/1000 | Loss: 0.00005944
Iteration 132/1000 | Loss: 0.00005944
Iteration 133/1000 | Loss: 0.00005944
Iteration 134/1000 | Loss: 0.00005943
Iteration 135/1000 | Loss: 0.00005943
Iteration 136/1000 | Loss: 0.00005943
Iteration 137/1000 | Loss: 0.00005943
Iteration 138/1000 | Loss: 0.00005943
Iteration 139/1000 | Loss: 0.00005943
Iteration 140/1000 | Loss: 0.00005943
Iteration 141/1000 | Loss: 0.00005943
Iteration 142/1000 | Loss: 0.00005943
Iteration 143/1000 | Loss: 0.00005943
Iteration 144/1000 | Loss: 0.00005943
Iteration 145/1000 | Loss: 0.00005943
Iteration 146/1000 | Loss: 0.00005942
Iteration 147/1000 | Loss: 0.00005942
Iteration 148/1000 | Loss: 0.00005942
Iteration 149/1000 | Loss: 0.00005942
Iteration 150/1000 | Loss: 0.00005942
Iteration 151/1000 | Loss: 0.00005942
Iteration 152/1000 | Loss: 0.00005942
Iteration 153/1000 | Loss: 0.00005942
Iteration 154/1000 | Loss: 0.00005942
Iteration 155/1000 | Loss: 0.00005942
Iteration 156/1000 | Loss: 0.00005942
Iteration 157/1000 | Loss: 0.00005942
Iteration 158/1000 | Loss: 0.00005942
Iteration 159/1000 | Loss: 0.00005942
Iteration 160/1000 | Loss: 0.00005942
Iteration 161/1000 | Loss: 0.00005942
Iteration 162/1000 | Loss: 0.00005942
Iteration 163/1000 | Loss: 0.00005942
Iteration 164/1000 | Loss: 0.00005942
Iteration 165/1000 | Loss: 0.00005942
Iteration 166/1000 | Loss: 0.00005941
Iteration 167/1000 | Loss: 0.00005941
Iteration 168/1000 | Loss: 0.00005941
Iteration 169/1000 | Loss: 0.00005941
Iteration 170/1000 | Loss: 0.00005941
Iteration 171/1000 | Loss: 0.00005941
Iteration 172/1000 | Loss: 0.00005941
Iteration 173/1000 | Loss: 0.00005941
Iteration 174/1000 | Loss: 0.00005941
Iteration 175/1000 | Loss: 0.00005941
Iteration 176/1000 | Loss: 0.00005941
Iteration 177/1000 | Loss: 0.00005941
Iteration 178/1000 | Loss: 0.00005941
Iteration 179/1000 | Loss: 0.00005941
Iteration 180/1000 | Loss: 0.00005941
Iteration 181/1000 | Loss: 0.00005941
Iteration 182/1000 | Loss: 0.00005941
Iteration 183/1000 | Loss: 0.00005941
Iteration 184/1000 | Loss: 0.00005941
Iteration 185/1000 | Loss: 0.00005941
Iteration 186/1000 | Loss: 0.00005941
Iteration 187/1000 | Loss: 0.00005941
Iteration 188/1000 | Loss: 0.00005941
Iteration 189/1000 | Loss: 0.00005941
Iteration 190/1000 | Loss: 0.00005941
Iteration 191/1000 | Loss: 0.00005941
Iteration 192/1000 | Loss: 0.00005941
Iteration 193/1000 | Loss: 0.00005941
Iteration 194/1000 | Loss: 0.00005941
Iteration 195/1000 | Loss: 0.00005941
Iteration 196/1000 | Loss: 0.00005941
Iteration 197/1000 | Loss: 0.00005941
Iteration 198/1000 | Loss: 0.00005941
Iteration 199/1000 | Loss: 0.00005941
Iteration 200/1000 | Loss: 0.00005941
Iteration 201/1000 | Loss: 0.00005941
Iteration 202/1000 | Loss: 0.00005941
Iteration 203/1000 | Loss: 0.00005941
Iteration 204/1000 | Loss: 0.00005941
Iteration 205/1000 | Loss: 0.00005941
Iteration 206/1000 | Loss: 0.00005941
Iteration 207/1000 | Loss: 0.00005941
Iteration 208/1000 | Loss: 0.00005941
Iteration 209/1000 | Loss: 0.00005941
Iteration 210/1000 | Loss: 0.00005941
Iteration 211/1000 | Loss: 0.00005941
Iteration 212/1000 | Loss: 0.00005941
Iteration 213/1000 | Loss: 0.00005941
Iteration 214/1000 | Loss: 0.00005941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [5.941075869486667e-05, 5.941075869486667e-05, 5.941075869486667e-05, 5.941075869486667e-05, 5.941075869486667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.941075869486667e-05

Optimization complete. Final v2v error: 4.412848949432373 mm

Highest mean error: 11.654985427856445 mm for frame 51

Lowest mean error: 2.9198966026306152 mm for frame 5

Saving results

Total time: 132.89985418319702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020689
Iteration 2/25 | Loss: 0.00225935
Iteration 3/25 | Loss: 0.00157662
Iteration 4/25 | Loss: 0.00148894
Iteration 5/25 | Loss: 0.00157644
Iteration 6/25 | Loss: 0.00155135
Iteration 7/25 | Loss: 0.00144701
Iteration 8/25 | Loss: 0.00143876
Iteration 9/25 | Loss: 0.00138203
Iteration 10/25 | Loss: 0.00134261
Iteration 11/25 | Loss: 0.00132880
Iteration 12/25 | Loss: 0.00130551
Iteration 13/25 | Loss: 0.00129077
Iteration 14/25 | Loss: 0.00128677
Iteration 15/25 | Loss: 0.00127581
Iteration 16/25 | Loss: 0.00123511
Iteration 17/25 | Loss: 0.00124459
Iteration 18/25 | Loss: 0.00122989
Iteration 19/25 | Loss: 0.00121569
Iteration 20/25 | Loss: 0.00121026
Iteration 21/25 | Loss: 0.00121759
Iteration 22/25 | Loss: 0.00120362
Iteration 23/25 | Loss: 0.00120132
Iteration 24/25 | Loss: 0.00119682
Iteration 25/25 | Loss: 0.00119399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.11632252
Iteration 2/25 | Loss: 0.00159689
Iteration 3/25 | Loss: 0.00156102
Iteration 4/25 | Loss: 0.00156101
Iteration 5/25 | Loss: 0.00156101
Iteration 6/25 | Loss: 0.00156101
Iteration 7/25 | Loss: 0.00156101
Iteration 8/25 | Loss: 0.00156101
Iteration 9/25 | Loss: 0.00156101
Iteration 10/25 | Loss: 0.00156101
Iteration 11/25 | Loss: 0.00156101
Iteration 12/25 | Loss: 0.00156101
Iteration 13/25 | Loss: 0.00156101
Iteration 14/25 | Loss: 0.00156101
Iteration 15/25 | Loss: 0.00156101
Iteration 16/25 | Loss: 0.00156101
Iteration 17/25 | Loss: 0.00156101
Iteration 18/25 | Loss: 0.00156101
Iteration 19/25 | Loss: 0.00156101
Iteration 20/25 | Loss: 0.00156101
Iteration 21/25 | Loss: 0.00156101
Iteration 22/25 | Loss: 0.00156101
Iteration 23/25 | Loss: 0.00156101
Iteration 24/25 | Loss: 0.00156101
Iteration 25/25 | Loss: 0.00156101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156101
Iteration 2/1000 | Loss: 0.00022597
Iteration 3/1000 | Loss: 0.00045608
Iteration 4/1000 | Loss: 0.00025937
Iteration 5/1000 | Loss: 0.00018688
Iteration 6/1000 | Loss: 0.00016244
Iteration 7/1000 | Loss: 0.00019280
Iteration 8/1000 | Loss: 0.00010869
Iteration 9/1000 | Loss: 0.00020414
Iteration 10/1000 | Loss: 0.00009441
Iteration 11/1000 | Loss: 0.00020815
Iteration 12/1000 | Loss: 0.00003325
Iteration 13/1000 | Loss: 0.00013270
Iteration 14/1000 | Loss: 0.00004795
Iteration 15/1000 | Loss: 0.00016501
Iteration 16/1000 | Loss: 0.00015290
Iteration 17/1000 | Loss: 0.00007787
Iteration 18/1000 | Loss: 0.00008124
Iteration 19/1000 | Loss: 0.00007694
Iteration 20/1000 | Loss: 0.00010458
Iteration 21/1000 | Loss: 0.00014220
Iteration 22/1000 | Loss: 0.00008718
Iteration 23/1000 | Loss: 0.00008557
Iteration 24/1000 | Loss: 0.00005683
Iteration 25/1000 | Loss: 0.00014401
Iteration 26/1000 | Loss: 0.00039470
Iteration 27/1000 | Loss: 0.00027859
Iteration 28/1000 | Loss: 0.00020024
Iteration 29/1000 | Loss: 0.00043332
Iteration 30/1000 | Loss: 0.00050448
Iteration 31/1000 | Loss: 0.00022350
Iteration 32/1000 | Loss: 0.00004800
Iteration 33/1000 | Loss: 0.00019060
Iteration 34/1000 | Loss: 0.00023520
Iteration 35/1000 | Loss: 0.00008389
Iteration 36/1000 | Loss: 0.00017202
Iteration 37/1000 | Loss: 0.00028102
Iteration 38/1000 | Loss: 0.00023066
Iteration 39/1000 | Loss: 0.00008281
Iteration 40/1000 | Loss: 0.00009568
Iteration 41/1000 | Loss: 0.00005961
Iteration 42/1000 | Loss: 0.00003035
Iteration 43/1000 | Loss: 0.00003731
Iteration 44/1000 | Loss: 0.00003865
Iteration 45/1000 | Loss: 0.00003456
Iteration 46/1000 | Loss: 0.00004971
Iteration 47/1000 | Loss: 0.00005133
Iteration 48/1000 | Loss: 0.00005277
Iteration 49/1000 | Loss: 0.00005118
Iteration 50/1000 | Loss: 0.00012799
Iteration 51/1000 | Loss: 0.00016560
Iteration 52/1000 | Loss: 0.00020427
Iteration 53/1000 | Loss: 0.00002601
Iteration 54/1000 | Loss: 0.00004049
Iteration 55/1000 | Loss: 0.00011954
Iteration 56/1000 | Loss: 0.00006435
Iteration 57/1000 | Loss: 0.00004334
Iteration 58/1000 | Loss: 0.00006197
Iteration 59/1000 | Loss: 0.00009784
Iteration 60/1000 | Loss: 0.00011281
Iteration 61/1000 | Loss: 0.00007080
Iteration 62/1000 | Loss: 0.00020491
Iteration 63/1000 | Loss: 0.00018708
Iteration 64/1000 | Loss: 0.00001971
Iteration 65/1000 | Loss: 0.00015020
Iteration 66/1000 | Loss: 0.00015307
Iteration 67/1000 | Loss: 0.00004530
Iteration 68/1000 | Loss: 0.00015293
Iteration 69/1000 | Loss: 0.00009042
Iteration 70/1000 | Loss: 0.00014928
Iteration 71/1000 | Loss: 0.00002029
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00029811
Iteration 80/1000 | Loss: 0.00020681
Iteration 81/1000 | Loss: 0.00022149
Iteration 82/1000 | Loss: 0.00016772
Iteration 83/1000 | Loss: 0.00011065
Iteration 84/1000 | Loss: 0.00027025
Iteration 85/1000 | Loss: 0.00018038
Iteration 86/1000 | Loss: 0.00016317
Iteration 87/1000 | Loss: 0.00023654
Iteration 88/1000 | Loss: 0.00018555
Iteration 89/1000 | Loss: 0.00021182
Iteration 90/1000 | Loss: 0.00015870
Iteration 91/1000 | Loss: 0.00019305
Iteration 92/1000 | Loss: 0.00012216
Iteration 93/1000 | Loss: 0.00021048
Iteration 94/1000 | Loss: 0.00010969
Iteration 95/1000 | Loss: 0.00021253
Iteration 96/1000 | Loss: 0.00002895
Iteration 97/1000 | Loss: 0.00002116
Iteration 98/1000 | Loss: 0.00001734
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001321
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001198
Iteration 103/1000 | Loss: 0.00001138
Iteration 104/1000 | Loss: 0.00001083
Iteration 105/1000 | Loss: 0.00001038
Iteration 106/1000 | Loss: 0.00001016
Iteration 107/1000 | Loss: 0.00001013
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000992
Iteration 114/1000 | Loss: 0.00000991
Iteration 115/1000 | Loss: 0.00000991
Iteration 116/1000 | Loss: 0.00000991
Iteration 117/1000 | Loss: 0.00000991
Iteration 118/1000 | Loss: 0.00000991
Iteration 119/1000 | Loss: 0.00000990
Iteration 120/1000 | Loss: 0.00000990
Iteration 121/1000 | Loss: 0.00000990
Iteration 122/1000 | Loss: 0.00000989
Iteration 123/1000 | Loss: 0.00000988
Iteration 124/1000 | Loss: 0.00000987
Iteration 125/1000 | Loss: 0.00000987
Iteration 126/1000 | Loss: 0.00000987
Iteration 127/1000 | Loss: 0.00000987
Iteration 128/1000 | Loss: 0.00000986
Iteration 129/1000 | Loss: 0.00000986
Iteration 130/1000 | Loss: 0.00000986
Iteration 131/1000 | Loss: 0.00000985
Iteration 132/1000 | Loss: 0.00000985
Iteration 133/1000 | Loss: 0.00000984
Iteration 134/1000 | Loss: 0.00000983
Iteration 135/1000 | Loss: 0.00000983
Iteration 136/1000 | Loss: 0.00000982
Iteration 137/1000 | Loss: 0.00000982
Iteration 138/1000 | Loss: 0.00000982
Iteration 139/1000 | Loss: 0.00000979
Iteration 140/1000 | Loss: 0.00000979
Iteration 141/1000 | Loss: 0.00000978
Iteration 142/1000 | Loss: 0.00000978
Iteration 143/1000 | Loss: 0.00000977
Iteration 144/1000 | Loss: 0.00000977
Iteration 145/1000 | Loss: 0.00000977
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000976
Iteration 148/1000 | Loss: 0.00000976
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000974
Iteration 152/1000 | Loss: 0.00000974
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000973
Iteration 155/1000 | Loss: 0.00000973
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000972
Iteration 161/1000 | Loss: 0.00000972
Iteration 162/1000 | Loss: 0.00000972
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000972
Iteration 165/1000 | Loss: 0.00000972
Iteration 166/1000 | Loss: 0.00000972
Iteration 167/1000 | Loss: 0.00000971
Iteration 168/1000 | Loss: 0.00000971
Iteration 169/1000 | Loss: 0.00000970
Iteration 170/1000 | Loss: 0.00000970
Iteration 171/1000 | Loss: 0.00000969
Iteration 172/1000 | Loss: 0.00000969
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000968
Iteration 175/1000 | Loss: 0.00000967
Iteration 176/1000 | Loss: 0.00000967
Iteration 177/1000 | Loss: 0.00000967
Iteration 178/1000 | Loss: 0.00000967
Iteration 179/1000 | Loss: 0.00000966
Iteration 180/1000 | Loss: 0.00000966
Iteration 181/1000 | Loss: 0.00000966
Iteration 182/1000 | Loss: 0.00000966
Iteration 183/1000 | Loss: 0.00000966
Iteration 184/1000 | Loss: 0.00000965
Iteration 185/1000 | Loss: 0.00000965
Iteration 186/1000 | Loss: 0.00000965
Iteration 187/1000 | Loss: 0.00000965
Iteration 188/1000 | Loss: 0.00000965
Iteration 189/1000 | Loss: 0.00000964
Iteration 190/1000 | Loss: 0.00000964
Iteration 191/1000 | Loss: 0.00000964
Iteration 192/1000 | Loss: 0.00000964
Iteration 193/1000 | Loss: 0.00000964
Iteration 194/1000 | Loss: 0.00000964
Iteration 195/1000 | Loss: 0.00000964
Iteration 196/1000 | Loss: 0.00000964
Iteration 197/1000 | Loss: 0.00000964
Iteration 198/1000 | Loss: 0.00000964
Iteration 199/1000 | Loss: 0.00000964
Iteration 200/1000 | Loss: 0.00000964
Iteration 201/1000 | Loss: 0.00000964
Iteration 202/1000 | Loss: 0.00000964
Iteration 203/1000 | Loss: 0.00000964
Iteration 204/1000 | Loss: 0.00000964
Iteration 205/1000 | Loss: 0.00000964
Iteration 206/1000 | Loss: 0.00000964
Iteration 207/1000 | Loss: 0.00000964
Iteration 208/1000 | Loss: 0.00000964
Iteration 209/1000 | Loss: 0.00000964
Iteration 210/1000 | Loss: 0.00000964
Iteration 211/1000 | Loss: 0.00000964
Iteration 212/1000 | Loss: 0.00000964
Iteration 213/1000 | Loss: 0.00000964
Iteration 214/1000 | Loss: 0.00000964
Iteration 215/1000 | Loss: 0.00000964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [9.640862117521465e-06, 9.640862117521465e-06, 9.640862117521465e-06, 9.640862117521465e-06, 9.640862117521465e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.640862117521465e-06

Optimization complete. Final v2v error: 2.6823818683624268 mm

Highest mean error: 3.862318754196167 mm for frame 158

Lowest mean error: 2.4721524715423584 mm for frame 89

Saving results

Total time: 207.89719104766846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973737
Iteration 2/25 | Loss: 0.00973737
Iteration 3/25 | Loss: 0.00290858
Iteration 4/25 | Loss: 0.00196356
Iteration 5/25 | Loss: 0.00168563
Iteration 6/25 | Loss: 0.00164234
Iteration 7/25 | Loss: 0.00166957
Iteration 8/25 | Loss: 0.00163359
Iteration 9/25 | Loss: 0.00152327
Iteration 10/25 | Loss: 0.00134980
Iteration 11/25 | Loss: 0.00133868
Iteration 12/25 | Loss: 0.00128497
Iteration 13/25 | Loss: 0.00125567
Iteration 14/25 | Loss: 0.00123553
Iteration 15/25 | Loss: 0.00122553
Iteration 16/25 | Loss: 0.00122172
Iteration 17/25 | Loss: 0.00121232
Iteration 18/25 | Loss: 0.00120517
Iteration 19/25 | Loss: 0.00120474
Iteration 20/25 | Loss: 0.00120443
Iteration 21/25 | Loss: 0.00120400
Iteration 22/25 | Loss: 0.00120334
Iteration 23/25 | Loss: 0.00120620
Iteration 24/25 | Loss: 0.00120234
Iteration 25/25 | Loss: 0.00119965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31408250
Iteration 2/25 | Loss: 0.00164348
Iteration 3/25 | Loss: 0.00133165
Iteration 4/25 | Loss: 0.00133165
Iteration 5/25 | Loss: 0.00133164
Iteration 6/25 | Loss: 0.00133164
Iteration 7/25 | Loss: 0.00133164
Iteration 8/25 | Loss: 0.00133164
Iteration 9/25 | Loss: 0.00133164
Iteration 10/25 | Loss: 0.00133164
Iteration 11/25 | Loss: 0.00133164
Iteration 12/25 | Loss: 0.00133164
Iteration 13/25 | Loss: 0.00133164
Iteration 14/25 | Loss: 0.00133164
Iteration 15/25 | Loss: 0.00133164
Iteration 16/25 | Loss: 0.00133164
Iteration 17/25 | Loss: 0.00133164
Iteration 18/25 | Loss: 0.00133164
Iteration 19/25 | Loss: 0.00133164
Iteration 20/25 | Loss: 0.00133164
Iteration 21/25 | Loss: 0.00133164
Iteration 22/25 | Loss: 0.00133164
Iteration 23/25 | Loss: 0.00133164
Iteration 24/25 | Loss: 0.00133164
Iteration 25/25 | Loss: 0.00133164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133164
Iteration 2/1000 | Loss: 0.00015137
Iteration 3/1000 | Loss: 0.00020268
Iteration 4/1000 | Loss: 0.00010626
Iteration 5/1000 | Loss: 0.00019576
Iteration 6/1000 | Loss: 0.00012067
Iteration 7/1000 | Loss: 0.00003574
Iteration 8/1000 | Loss: 0.00017547
Iteration 9/1000 | Loss: 0.00003395
Iteration 10/1000 | Loss: 0.00009981
Iteration 11/1000 | Loss: 0.00003271
Iteration 12/1000 | Loss: 0.00003211
Iteration 13/1000 | Loss: 0.00018276
Iteration 14/1000 | Loss: 0.00082652
Iteration 15/1000 | Loss: 0.00018366
Iteration 16/1000 | Loss: 0.00034711
Iteration 17/1000 | Loss: 0.00003186
Iteration 18/1000 | Loss: 0.00012418
Iteration 19/1000 | Loss: 0.00026168
Iteration 20/1000 | Loss: 0.00012154
Iteration 21/1000 | Loss: 0.00007712
Iteration 22/1000 | Loss: 0.00003124
Iteration 23/1000 | Loss: 0.00003102
Iteration 24/1000 | Loss: 0.00003075
Iteration 25/1000 | Loss: 0.00003075
Iteration 26/1000 | Loss: 0.00010797
Iteration 27/1000 | Loss: 0.00143060
Iteration 28/1000 | Loss: 0.00043244
Iteration 29/1000 | Loss: 0.00003453
Iteration 30/1000 | Loss: 0.00011017
Iteration 31/1000 | Loss: 0.00045982
Iteration 32/1000 | Loss: 0.00003058
Iteration 33/1000 | Loss: 0.00003023
Iteration 34/1000 | Loss: 0.00002999
Iteration 35/1000 | Loss: 0.00013289
Iteration 36/1000 | Loss: 0.00035627
Iteration 37/1000 | Loss: 0.00013681
Iteration 38/1000 | Loss: 0.00017478
Iteration 39/1000 | Loss: 0.00003342
Iteration 40/1000 | Loss: 0.00010226
Iteration 41/1000 | Loss: 0.00004747
Iteration 42/1000 | Loss: 0.00002990
Iteration 43/1000 | Loss: 0.00011792
Iteration 44/1000 | Loss: 0.00002982
Iteration 45/1000 | Loss: 0.00002949
Iteration 46/1000 | Loss: 0.00002930
Iteration 47/1000 | Loss: 0.00113629
Iteration 48/1000 | Loss: 0.00100623
Iteration 49/1000 | Loss: 0.00009044
Iteration 50/1000 | Loss: 0.00002754
Iteration 51/1000 | Loss: 0.00002564
Iteration 52/1000 | Loss: 0.00002471
Iteration 53/1000 | Loss: 0.00002415
Iteration 54/1000 | Loss: 0.00021605
Iteration 55/1000 | Loss: 0.00002372
Iteration 56/1000 | Loss: 0.00002327
Iteration 57/1000 | Loss: 0.00002304
Iteration 58/1000 | Loss: 0.00002287
Iteration 59/1000 | Loss: 0.00002284
Iteration 60/1000 | Loss: 0.00002278
Iteration 61/1000 | Loss: 0.00002267
Iteration 62/1000 | Loss: 0.00002267
Iteration 63/1000 | Loss: 0.00002253
Iteration 64/1000 | Loss: 0.00002252
Iteration 65/1000 | Loss: 0.00002251
Iteration 66/1000 | Loss: 0.00002251
Iteration 67/1000 | Loss: 0.00002251
Iteration 68/1000 | Loss: 0.00002244
Iteration 69/1000 | Loss: 0.00002242
Iteration 70/1000 | Loss: 0.00002241
Iteration 71/1000 | Loss: 0.00002241
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002240
Iteration 74/1000 | Loss: 0.00002240
Iteration 75/1000 | Loss: 0.00002240
Iteration 76/1000 | Loss: 0.00002240
Iteration 77/1000 | Loss: 0.00002240
Iteration 78/1000 | Loss: 0.00002240
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002239
Iteration 81/1000 | Loss: 0.00002239
Iteration 82/1000 | Loss: 0.00002239
Iteration 83/1000 | Loss: 0.00002239
Iteration 84/1000 | Loss: 0.00002239
Iteration 85/1000 | Loss: 0.00002239
Iteration 86/1000 | Loss: 0.00002239
Iteration 87/1000 | Loss: 0.00002239
Iteration 88/1000 | Loss: 0.00002239
Iteration 89/1000 | Loss: 0.00002238
Iteration 90/1000 | Loss: 0.00002238
Iteration 91/1000 | Loss: 0.00002238
Iteration 92/1000 | Loss: 0.00002238
Iteration 93/1000 | Loss: 0.00002238
Iteration 94/1000 | Loss: 0.00002238
Iteration 95/1000 | Loss: 0.00002238
Iteration 96/1000 | Loss: 0.00002238
Iteration 97/1000 | Loss: 0.00002238
Iteration 98/1000 | Loss: 0.00002238
Iteration 99/1000 | Loss: 0.00002238
Iteration 100/1000 | Loss: 0.00002238
Iteration 101/1000 | Loss: 0.00002237
Iteration 102/1000 | Loss: 0.00002237
Iteration 103/1000 | Loss: 0.00002237
Iteration 104/1000 | Loss: 0.00002237
Iteration 105/1000 | Loss: 0.00002236
Iteration 106/1000 | Loss: 0.00002236
Iteration 107/1000 | Loss: 0.00002236
Iteration 108/1000 | Loss: 0.00002236
Iteration 109/1000 | Loss: 0.00002236
Iteration 110/1000 | Loss: 0.00002235
Iteration 111/1000 | Loss: 0.00002235
Iteration 112/1000 | Loss: 0.00002235
Iteration 113/1000 | Loss: 0.00002235
Iteration 114/1000 | Loss: 0.00002234
Iteration 115/1000 | Loss: 0.00002234
Iteration 116/1000 | Loss: 0.00002234
Iteration 117/1000 | Loss: 0.00002234
Iteration 118/1000 | Loss: 0.00002234
Iteration 119/1000 | Loss: 0.00002234
Iteration 120/1000 | Loss: 0.00002233
Iteration 121/1000 | Loss: 0.00002233
Iteration 122/1000 | Loss: 0.00002233
Iteration 123/1000 | Loss: 0.00002233
Iteration 124/1000 | Loss: 0.00002233
Iteration 125/1000 | Loss: 0.00002233
Iteration 126/1000 | Loss: 0.00002233
Iteration 127/1000 | Loss: 0.00002233
Iteration 128/1000 | Loss: 0.00002233
Iteration 129/1000 | Loss: 0.00002233
Iteration 130/1000 | Loss: 0.00002233
Iteration 131/1000 | Loss: 0.00002233
Iteration 132/1000 | Loss: 0.00002233
Iteration 133/1000 | Loss: 0.00002233
Iteration 134/1000 | Loss: 0.00002233
Iteration 135/1000 | Loss: 0.00002233
Iteration 136/1000 | Loss: 0.00002233
Iteration 137/1000 | Loss: 0.00002233
Iteration 138/1000 | Loss: 0.00002233
Iteration 139/1000 | Loss: 0.00002233
Iteration 140/1000 | Loss: 0.00002233
Iteration 141/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.2334104869514704e-05, 2.2334104869514704e-05, 2.2334104869514704e-05, 2.2334104869514704e-05, 2.2334104869514704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2334104869514704e-05

Optimization complete. Final v2v error: 4.060491561889648 mm

Highest mean error: 4.692943572998047 mm for frame 45

Lowest mean error: 3.3239333629608154 mm for frame 96

Saving results

Total time: 131.19721364974976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803242
Iteration 2/25 | Loss: 0.00126646
Iteration 3/25 | Loss: 0.00118819
Iteration 4/25 | Loss: 0.00117496
Iteration 5/25 | Loss: 0.00117169
Iteration 6/25 | Loss: 0.00117169
Iteration 7/25 | Loss: 0.00117169
Iteration 8/25 | Loss: 0.00117169
Iteration 9/25 | Loss: 0.00117169
Iteration 10/25 | Loss: 0.00117169
Iteration 11/25 | Loss: 0.00117169
Iteration 12/25 | Loss: 0.00117169
Iteration 13/25 | Loss: 0.00117169
Iteration 14/25 | Loss: 0.00117169
Iteration 15/25 | Loss: 0.00117169
Iteration 16/25 | Loss: 0.00117169
Iteration 17/25 | Loss: 0.00117169
Iteration 18/25 | Loss: 0.00117169
Iteration 19/25 | Loss: 0.00117169
Iteration 20/25 | Loss: 0.00117169
Iteration 21/25 | Loss: 0.00117169
Iteration 22/25 | Loss: 0.00117169
Iteration 23/25 | Loss: 0.00117169
Iteration 24/25 | Loss: 0.00117169
Iteration 25/25 | Loss: 0.00117169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.00939512
Iteration 2/25 | Loss: 0.00124343
Iteration 3/25 | Loss: 0.00124343
Iteration 4/25 | Loss: 0.00124342
Iteration 5/25 | Loss: 0.00124342
Iteration 6/25 | Loss: 0.00124342
Iteration 7/25 | Loss: 0.00124342
Iteration 8/25 | Loss: 0.00124342
Iteration 9/25 | Loss: 0.00124342
Iteration 10/25 | Loss: 0.00124342
Iteration 11/25 | Loss: 0.00124342
Iteration 12/25 | Loss: 0.00124342
Iteration 13/25 | Loss: 0.00124342
Iteration 14/25 | Loss: 0.00124342
Iteration 15/25 | Loss: 0.00124342
Iteration 16/25 | Loss: 0.00124342
Iteration 17/25 | Loss: 0.00124342
Iteration 18/25 | Loss: 0.00124342
Iteration 19/25 | Loss: 0.00124342
Iteration 20/25 | Loss: 0.00124342
Iteration 21/25 | Loss: 0.00124342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00124342308845371, 0.00124342308845371, 0.00124342308845371, 0.00124342308845371, 0.00124342308845371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00124342308845371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124342
Iteration 2/1000 | Loss: 0.00002594
Iteration 3/1000 | Loss: 0.00001997
Iteration 4/1000 | Loss: 0.00001885
Iteration 5/1000 | Loss: 0.00001792
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001626
Iteration 9/1000 | Loss: 0.00001564
Iteration 10/1000 | Loss: 0.00001532
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001488
Iteration 13/1000 | Loss: 0.00001472
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001460
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001457
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001455
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001453
Iteration 25/1000 | Loss: 0.00001453
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001453
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001450
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001438
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001437
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001434
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001433
Iteration 57/1000 | Loss: 0.00001432
Iteration 58/1000 | Loss: 0.00001432
Iteration 59/1000 | Loss: 0.00001431
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001429
Iteration 63/1000 | Loss: 0.00001429
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001428
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001423
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001421
Iteration 80/1000 | Loss: 0.00001421
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001420
Iteration 86/1000 | Loss: 0.00001418
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001417
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001416
Iteration 91/1000 | Loss: 0.00001416
Iteration 92/1000 | Loss: 0.00001416
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00001415
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001414
Iteration 105/1000 | Loss: 0.00001414
Iteration 106/1000 | Loss: 0.00001414
Iteration 107/1000 | Loss: 0.00001414
Iteration 108/1000 | Loss: 0.00001414
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001413
Iteration 111/1000 | Loss: 0.00001413
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001413
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001413
Iteration 118/1000 | Loss: 0.00001413
Iteration 119/1000 | Loss: 0.00001413
Iteration 120/1000 | Loss: 0.00001413
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001412
Iteration 123/1000 | Loss: 0.00001412
Iteration 124/1000 | Loss: 0.00001412
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001412
Iteration 127/1000 | Loss: 0.00001412
Iteration 128/1000 | Loss: 0.00001412
Iteration 129/1000 | Loss: 0.00001412
Iteration 130/1000 | Loss: 0.00001412
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Iteration 140/1000 | Loss: 0.00001411
Iteration 141/1000 | Loss: 0.00001411
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001411
Iteration 144/1000 | Loss: 0.00001411
Iteration 145/1000 | Loss: 0.00001411
Iteration 146/1000 | Loss: 0.00001411
Iteration 147/1000 | Loss: 0.00001411
Iteration 148/1000 | Loss: 0.00001411
Iteration 149/1000 | Loss: 0.00001411
Iteration 150/1000 | Loss: 0.00001411
Iteration 151/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.4112767530605197e-05, 1.4112767530605197e-05, 1.4112767530605197e-05, 1.4112767530605197e-05, 1.4112767530605197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4112767530605197e-05

Optimization complete. Final v2v error: 3.1949567794799805 mm

Highest mean error: 3.456974983215332 mm for frame 129

Lowest mean error: 2.9888525009155273 mm for frame 203

Saving results

Total time: 44.300029277801514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460323
Iteration 2/25 | Loss: 0.00156081
Iteration 3/25 | Loss: 0.00134751
Iteration 4/25 | Loss: 0.00130892
Iteration 5/25 | Loss: 0.00129758
Iteration 6/25 | Loss: 0.00128600
Iteration 7/25 | Loss: 0.00126453
Iteration 8/25 | Loss: 0.00125041
Iteration 9/25 | Loss: 0.00123335
Iteration 10/25 | Loss: 0.00120908
Iteration 11/25 | Loss: 0.00119790
Iteration 12/25 | Loss: 0.00119424
Iteration 13/25 | Loss: 0.00119238
Iteration 14/25 | Loss: 0.00119121
Iteration 15/25 | Loss: 0.00118867
Iteration 16/25 | Loss: 0.00118810
Iteration 17/25 | Loss: 0.00118793
Iteration 18/25 | Loss: 0.00118792
Iteration 19/25 | Loss: 0.00118792
Iteration 20/25 | Loss: 0.00118792
Iteration 21/25 | Loss: 0.00118792
Iteration 22/25 | Loss: 0.00118792
Iteration 23/25 | Loss: 0.00118791
Iteration 24/25 | Loss: 0.00118791
Iteration 25/25 | Loss: 0.00118791

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33060634
Iteration 2/25 | Loss: 0.00111801
Iteration 3/25 | Loss: 0.00111800
Iteration 4/25 | Loss: 0.00111800
Iteration 5/25 | Loss: 0.00111800
Iteration 6/25 | Loss: 0.00111800
Iteration 7/25 | Loss: 0.00111800
Iteration 8/25 | Loss: 0.00111800
Iteration 9/25 | Loss: 0.00111800
Iteration 10/25 | Loss: 0.00111800
Iteration 11/25 | Loss: 0.00111800
Iteration 12/25 | Loss: 0.00111800
Iteration 13/25 | Loss: 0.00111800
Iteration 14/25 | Loss: 0.00111800
Iteration 15/25 | Loss: 0.00111800
Iteration 16/25 | Loss: 0.00111800
Iteration 17/25 | Loss: 0.00111800
Iteration 18/25 | Loss: 0.00111800
Iteration 19/25 | Loss: 0.00111800
Iteration 20/25 | Loss: 0.00111800
Iteration 21/25 | Loss: 0.00111800
Iteration 22/25 | Loss: 0.00111800
Iteration 23/25 | Loss: 0.00111800
Iteration 24/25 | Loss: 0.00111800
Iteration 25/25 | Loss: 0.00111800

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111800
Iteration 2/1000 | Loss: 0.00002389
Iteration 3/1000 | Loss: 0.00001826
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001361
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001342
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001339
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001331
Iteration 28/1000 | Loss: 0.00001331
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001330
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001327
Iteration 35/1000 | Loss: 0.00001326
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001323
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001322
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001315
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001314
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001314
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001304
Iteration 93/1000 | Loss: 0.00001304
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001303
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001302
Iteration 101/1000 | Loss: 0.00001301
Iteration 102/1000 | Loss: 0.00001301
Iteration 103/1000 | Loss: 0.00001301
Iteration 104/1000 | Loss: 0.00001300
Iteration 105/1000 | Loss: 0.00001300
Iteration 106/1000 | Loss: 0.00001300
Iteration 107/1000 | Loss: 0.00001300
Iteration 108/1000 | Loss: 0.00001300
Iteration 109/1000 | Loss: 0.00001300
Iteration 110/1000 | Loss: 0.00001300
Iteration 111/1000 | Loss: 0.00001300
Iteration 112/1000 | Loss: 0.00001299
Iteration 113/1000 | Loss: 0.00001299
Iteration 114/1000 | Loss: 0.00001299
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001299
Iteration 117/1000 | Loss: 0.00001299
Iteration 118/1000 | Loss: 0.00001299
Iteration 119/1000 | Loss: 0.00001299
Iteration 120/1000 | Loss: 0.00001299
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001299
Iteration 123/1000 | Loss: 0.00001299
Iteration 124/1000 | Loss: 0.00001299
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001298
Iteration 129/1000 | Loss: 0.00001298
Iteration 130/1000 | Loss: 0.00001297
Iteration 131/1000 | Loss: 0.00001297
Iteration 132/1000 | Loss: 0.00001297
Iteration 133/1000 | Loss: 0.00001297
Iteration 134/1000 | Loss: 0.00001297
Iteration 135/1000 | Loss: 0.00001297
Iteration 136/1000 | Loss: 0.00001297
Iteration 137/1000 | Loss: 0.00001297
Iteration 138/1000 | Loss: 0.00001296
Iteration 139/1000 | Loss: 0.00001296
Iteration 140/1000 | Loss: 0.00001296
Iteration 141/1000 | Loss: 0.00001296
Iteration 142/1000 | Loss: 0.00001296
Iteration 143/1000 | Loss: 0.00001296
Iteration 144/1000 | Loss: 0.00001296
Iteration 145/1000 | Loss: 0.00001296
Iteration 146/1000 | Loss: 0.00001296
Iteration 147/1000 | Loss: 0.00001296
Iteration 148/1000 | Loss: 0.00001295
Iteration 149/1000 | Loss: 0.00001295
Iteration 150/1000 | Loss: 0.00001295
Iteration 151/1000 | Loss: 0.00001295
Iteration 152/1000 | Loss: 0.00001295
Iteration 153/1000 | Loss: 0.00001295
Iteration 154/1000 | Loss: 0.00001295
Iteration 155/1000 | Loss: 0.00001295
Iteration 156/1000 | Loss: 0.00001295
Iteration 157/1000 | Loss: 0.00001294
Iteration 158/1000 | Loss: 0.00001294
Iteration 159/1000 | Loss: 0.00001294
Iteration 160/1000 | Loss: 0.00001294
Iteration 161/1000 | Loss: 0.00001294
Iteration 162/1000 | Loss: 0.00001294
Iteration 163/1000 | Loss: 0.00001294
Iteration 164/1000 | Loss: 0.00001294
Iteration 165/1000 | Loss: 0.00001294
Iteration 166/1000 | Loss: 0.00001294
Iteration 167/1000 | Loss: 0.00001294
Iteration 168/1000 | Loss: 0.00001294
Iteration 169/1000 | Loss: 0.00001294
Iteration 170/1000 | Loss: 0.00001294
Iteration 171/1000 | Loss: 0.00001294
Iteration 172/1000 | Loss: 0.00001294
Iteration 173/1000 | Loss: 0.00001294
Iteration 174/1000 | Loss: 0.00001294
Iteration 175/1000 | Loss: 0.00001294
Iteration 176/1000 | Loss: 0.00001294
Iteration 177/1000 | Loss: 0.00001294
Iteration 178/1000 | Loss: 0.00001294
Iteration 179/1000 | Loss: 0.00001294
Iteration 180/1000 | Loss: 0.00001294
Iteration 181/1000 | Loss: 0.00001294
Iteration 182/1000 | Loss: 0.00001294
Iteration 183/1000 | Loss: 0.00001294
Iteration 184/1000 | Loss: 0.00001294
Iteration 185/1000 | Loss: 0.00001294
Iteration 186/1000 | Loss: 0.00001294
Iteration 187/1000 | Loss: 0.00001294
Iteration 188/1000 | Loss: 0.00001294
Iteration 189/1000 | Loss: 0.00001294
Iteration 190/1000 | Loss: 0.00001294
Iteration 191/1000 | Loss: 0.00001294
Iteration 192/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.2939240150444675e-05, 1.2939240150444675e-05, 1.2939240150444675e-05, 1.2939240150444675e-05, 1.2939240150444675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2939240150444675e-05

Optimization complete. Final v2v error: 3.085172176361084 mm

Highest mean error: 3.517077684402466 mm for frame 122

Lowest mean error: 2.9342501163482666 mm for frame 26

Saving results

Total time: 58.17012095451355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730455
Iteration 2/25 | Loss: 0.00189788
Iteration 3/25 | Loss: 0.00154922
Iteration 4/25 | Loss: 0.00146412
Iteration 5/25 | Loss: 0.00134046
Iteration 6/25 | Loss: 0.00124105
Iteration 7/25 | Loss: 0.00118479
Iteration 8/25 | Loss: 0.00117364
Iteration 9/25 | Loss: 0.00116694
Iteration 10/25 | Loss: 0.00116184
Iteration 11/25 | Loss: 0.00115782
Iteration 12/25 | Loss: 0.00115480
Iteration 13/25 | Loss: 0.00115398
Iteration 14/25 | Loss: 0.00115384
Iteration 15/25 | Loss: 0.00115383
Iteration 16/25 | Loss: 0.00115383
Iteration 17/25 | Loss: 0.00115383
Iteration 18/25 | Loss: 0.00115383
Iteration 19/25 | Loss: 0.00115383
Iteration 20/25 | Loss: 0.00115383
Iteration 21/25 | Loss: 0.00115383
Iteration 22/25 | Loss: 0.00115383
Iteration 23/25 | Loss: 0.00115383
Iteration 24/25 | Loss: 0.00115383
Iteration 25/25 | Loss: 0.00115383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25699961
Iteration 2/25 | Loss: 0.00098325
Iteration 3/25 | Loss: 0.00098324
Iteration 4/25 | Loss: 0.00098324
Iteration 5/25 | Loss: 0.00098324
Iteration 6/25 | Loss: 0.00098324
Iteration 7/25 | Loss: 0.00098324
Iteration 8/25 | Loss: 0.00098324
Iteration 9/25 | Loss: 0.00098324
Iteration 10/25 | Loss: 0.00098324
Iteration 11/25 | Loss: 0.00098324
Iteration 12/25 | Loss: 0.00098324
Iteration 13/25 | Loss: 0.00098324
Iteration 14/25 | Loss: 0.00098324
Iteration 15/25 | Loss: 0.00098324
Iteration 16/25 | Loss: 0.00098324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009832398500293493, 0.0009832398500293493, 0.0009832398500293493, 0.0009832398500293493, 0.0009832398500293493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009832398500293493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098324
Iteration 2/1000 | Loss: 0.00002766
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001377
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001242
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001202
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001192
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001190
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001187
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001184
Iteration 37/1000 | Loss: 0.00001184
Iteration 38/1000 | Loss: 0.00001184
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001183
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001182
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001181
Iteration 50/1000 | Loss: 0.00001181
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001177
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001177
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.176775822386844e-05, 1.176775822386844e-05, 1.176775822386844e-05, 1.176775822386844e-05, 1.176775822386844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.176775822386844e-05

Optimization complete. Final v2v error: 2.96867036819458 mm

Highest mean error: 3.1573801040649414 mm for frame 18

Lowest mean error: 2.858196258544922 mm for frame 76

Saving results

Total time: 53.69419050216675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066197
Iteration 2/25 | Loss: 0.01066197
Iteration 3/25 | Loss: 0.01066197
Iteration 4/25 | Loss: 0.01066197
Iteration 5/25 | Loss: 0.01066197
Iteration 6/25 | Loss: 0.01066197
Iteration 7/25 | Loss: 0.01066197
Iteration 8/25 | Loss: 0.01066197
Iteration 9/25 | Loss: 0.01066196
Iteration 10/25 | Loss: 0.01066196
Iteration 11/25 | Loss: 0.01066196
Iteration 12/25 | Loss: 0.01066196
Iteration 13/25 | Loss: 0.01066196
Iteration 14/25 | Loss: 0.01066196
Iteration 15/25 | Loss: 0.01066196
Iteration 16/25 | Loss: 0.01066196
Iteration 17/25 | Loss: 0.01066195
Iteration 18/25 | Loss: 0.01066195
Iteration 19/25 | Loss: 0.01066195
Iteration 20/25 | Loss: 0.01066195
Iteration 21/25 | Loss: 0.01066195
Iteration 22/25 | Loss: 0.01066195
Iteration 23/25 | Loss: 0.01066195
Iteration 24/25 | Loss: 0.01066195
Iteration 25/25 | Loss: 0.01066195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61978614
Iteration 2/25 | Loss: 0.06606998
Iteration 3/25 | Loss: 0.06603832
Iteration 4/25 | Loss: 0.06603832
Iteration 5/25 | Loss: 0.06603831
Iteration 6/25 | Loss: 0.06603830
Iteration 7/25 | Loss: 0.06603830
Iteration 8/25 | Loss: 0.06603830
Iteration 9/25 | Loss: 0.06603830
Iteration 10/25 | Loss: 0.06603830
Iteration 11/25 | Loss: 0.06603830
Iteration 12/25 | Loss: 0.06603830
Iteration 13/25 | Loss: 0.06603830
Iteration 14/25 | Loss: 0.06603830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.06603829562664032, 0.06603829562664032, 0.06603829562664032, 0.06603829562664032, 0.06603829562664032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06603829562664032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06603830
Iteration 2/1000 | Loss: 0.01795306
Iteration 3/1000 | Loss: 0.00204113
Iteration 4/1000 | Loss: 0.00145707
Iteration 5/1000 | Loss: 0.00596301
Iteration 6/1000 | Loss: 0.00043147
Iteration 7/1000 | Loss: 0.00288255
Iteration 8/1000 | Loss: 0.00023887
Iteration 9/1000 | Loss: 0.00041524
Iteration 10/1000 | Loss: 0.00018253
Iteration 11/1000 | Loss: 0.00026838
Iteration 12/1000 | Loss: 0.00016678
Iteration 13/1000 | Loss: 0.00063268
Iteration 14/1000 | Loss: 0.00047943
Iteration 15/1000 | Loss: 0.00014898
Iteration 16/1000 | Loss: 0.00009963
Iteration 17/1000 | Loss: 0.00006094
Iteration 18/1000 | Loss: 0.00018328
Iteration 19/1000 | Loss: 0.00041004
Iteration 20/1000 | Loss: 0.00122007
Iteration 21/1000 | Loss: 0.00007646
Iteration 22/1000 | Loss: 0.00007882
Iteration 23/1000 | Loss: 0.00039507
Iteration 24/1000 | Loss: 0.00023564
Iteration 25/1000 | Loss: 0.00004541
Iteration 26/1000 | Loss: 0.00011315
Iteration 27/1000 | Loss: 0.00007991
Iteration 28/1000 | Loss: 0.00019514
Iteration 29/1000 | Loss: 0.00013995
Iteration 30/1000 | Loss: 0.00003598
Iteration 31/1000 | Loss: 0.00011538
Iteration 32/1000 | Loss: 0.00009105
Iteration 33/1000 | Loss: 0.00020314
Iteration 34/1000 | Loss: 0.00004930
Iteration 35/1000 | Loss: 0.00022525
Iteration 36/1000 | Loss: 0.00006921
Iteration 37/1000 | Loss: 0.00002794
Iteration 38/1000 | Loss: 0.00003953
Iteration 39/1000 | Loss: 0.00009853
Iteration 40/1000 | Loss: 0.00003595
Iteration 41/1000 | Loss: 0.00007228
Iteration 42/1000 | Loss: 0.00191816
Iteration 43/1000 | Loss: 0.00017210
Iteration 44/1000 | Loss: 0.00006874
Iteration 45/1000 | Loss: 0.00003621
Iteration 46/1000 | Loss: 0.00038637
Iteration 47/1000 | Loss: 0.00004436
Iteration 48/1000 | Loss: 0.00004361
Iteration 49/1000 | Loss: 0.00002797
Iteration 50/1000 | Loss: 0.00002370
Iteration 51/1000 | Loss: 0.00002111
Iteration 52/1000 | Loss: 0.00006909
Iteration 53/1000 | Loss: 0.00002070
Iteration 54/1000 | Loss: 0.00003349
Iteration 55/1000 | Loss: 0.00004766
Iteration 56/1000 | Loss: 0.00001995
Iteration 57/1000 | Loss: 0.00005424
Iteration 58/1000 | Loss: 0.00001959
Iteration 59/1000 | Loss: 0.00009286
Iteration 60/1000 | Loss: 0.00005891
Iteration 61/1000 | Loss: 0.00001954
Iteration 62/1000 | Loss: 0.00002767
Iteration 63/1000 | Loss: 0.00003016
Iteration 64/1000 | Loss: 0.00008715
Iteration 65/1000 | Loss: 0.00010147
Iteration 66/1000 | Loss: 0.00005309
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00005335
Iteration 70/1000 | Loss: 0.00002173
Iteration 71/1000 | Loss: 0.00001862
Iteration 72/1000 | Loss: 0.00001860
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001836
Iteration 76/1000 | Loss: 0.00004841
Iteration 77/1000 | Loss: 0.00001833
Iteration 78/1000 | Loss: 0.00004276
Iteration 79/1000 | Loss: 0.00001822
Iteration 80/1000 | Loss: 0.00001822
Iteration 81/1000 | Loss: 0.00001820
Iteration 82/1000 | Loss: 0.00001819
Iteration 83/1000 | Loss: 0.00001819
Iteration 84/1000 | Loss: 0.00001818
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001816
Iteration 88/1000 | Loss: 0.00003147
Iteration 89/1000 | Loss: 0.00001814
Iteration 90/1000 | Loss: 0.00003486
Iteration 91/1000 | Loss: 0.00001975
Iteration 92/1000 | Loss: 0.00004394
Iteration 93/1000 | Loss: 0.00005909
Iteration 94/1000 | Loss: 0.00003230
Iteration 95/1000 | Loss: 0.00002392
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001805
Iteration 98/1000 | Loss: 0.00001805
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001804
Iteration 103/1000 | Loss: 0.00001804
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001804
Iteration 108/1000 | Loss: 0.00001804
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001802
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001800
Iteration 117/1000 | Loss: 0.00001800
Iteration 118/1000 | Loss: 0.00001800
Iteration 119/1000 | Loss: 0.00001799
Iteration 120/1000 | Loss: 0.00001799
Iteration 121/1000 | Loss: 0.00001797
Iteration 122/1000 | Loss: 0.00001797
Iteration 123/1000 | Loss: 0.00001797
Iteration 124/1000 | Loss: 0.00001797
Iteration 125/1000 | Loss: 0.00001797
Iteration 126/1000 | Loss: 0.00001797
Iteration 127/1000 | Loss: 0.00001797
Iteration 128/1000 | Loss: 0.00001796
Iteration 129/1000 | Loss: 0.00002541
Iteration 130/1000 | Loss: 0.00005211
Iteration 131/1000 | Loss: 0.00002062
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001796
Iteration 134/1000 | Loss: 0.00001796
Iteration 135/1000 | Loss: 0.00001796
Iteration 136/1000 | Loss: 0.00001796
Iteration 137/1000 | Loss: 0.00001796
Iteration 138/1000 | Loss: 0.00001796
Iteration 139/1000 | Loss: 0.00001796
Iteration 140/1000 | Loss: 0.00001796
Iteration 141/1000 | Loss: 0.00001795
Iteration 142/1000 | Loss: 0.00001795
Iteration 143/1000 | Loss: 0.00002394
Iteration 144/1000 | Loss: 0.00004970
Iteration 145/1000 | Loss: 0.00002904
Iteration 146/1000 | Loss: 0.00002138
Iteration 147/1000 | Loss: 0.00002832
Iteration 148/1000 | Loss: 0.00001976
Iteration 149/1000 | Loss: 0.00001789
Iteration 150/1000 | Loss: 0.00001789
Iteration 151/1000 | Loss: 0.00001789
Iteration 152/1000 | Loss: 0.00001789
Iteration 153/1000 | Loss: 0.00001788
Iteration 154/1000 | Loss: 0.00001788
Iteration 155/1000 | Loss: 0.00001788
Iteration 156/1000 | Loss: 0.00001788
Iteration 157/1000 | Loss: 0.00001788
Iteration 158/1000 | Loss: 0.00001788
Iteration 159/1000 | Loss: 0.00001788
Iteration 160/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.7882755855680443e-05, 1.7882755855680443e-05, 1.7882755855680443e-05, 1.7882755855680443e-05, 1.7882755855680443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7882755855680443e-05

Optimization complete. Final v2v error: 3.335352659225464 mm

Highest mean error: 14.767457008361816 mm for frame 206

Lowest mean error: 2.6744942665100098 mm for frame 130

Saving results

Total time: 158.80308961868286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963546
Iteration 2/25 | Loss: 0.00273325
Iteration 3/25 | Loss: 0.00205647
Iteration 4/25 | Loss: 0.00191669
Iteration 5/25 | Loss: 0.00178023
Iteration 6/25 | Loss: 0.00159828
Iteration 7/25 | Loss: 0.00153232
Iteration 8/25 | Loss: 0.00146935
Iteration 9/25 | Loss: 0.00139435
Iteration 10/25 | Loss: 0.00136778
Iteration 11/25 | Loss: 0.00135906
Iteration 12/25 | Loss: 0.00135562
Iteration 13/25 | Loss: 0.00134718
Iteration 14/25 | Loss: 0.00134473
Iteration 15/25 | Loss: 0.00132627
Iteration 16/25 | Loss: 0.00130704
Iteration 17/25 | Loss: 0.00130388
Iteration 18/25 | Loss: 0.00130058
Iteration 19/25 | Loss: 0.00130265
Iteration 20/25 | Loss: 0.00128882
Iteration 21/25 | Loss: 0.00128495
Iteration 22/25 | Loss: 0.00128295
Iteration 23/25 | Loss: 0.00128536
Iteration 24/25 | Loss: 0.00128237
Iteration 25/25 | Loss: 0.00128182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25670135
Iteration 2/25 | Loss: 0.00127005
Iteration 3/25 | Loss: 0.00127005
Iteration 4/25 | Loss: 0.00126997
Iteration 5/25 | Loss: 0.00124951
Iteration 6/25 | Loss: 0.00124872
Iteration 7/25 | Loss: 0.00124530
Iteration 8/25 | Loss: 0.00124529
Iteration 9/25 | Loss: 0.00124529
Iteration 10/25 | Loss: 0.00124529
Iteration 11/25 | Loss: 0.00124529
Iteration 12/25 | Loss: 0.00124529
Iteration 13/25 | Loss: 0.00124529
Iteration 14/25 | Loss: 0.00124529
Iteration 15/25 | Loss: 0.00124529
Iteration 16/25 | Loss: 0.00124529
Iteration 17/25 | Loss: 0.00124529
Iteration 18/25 | Loss: 0.00124529
Iteration 19/25 | Loss: 0.00124529
Iteration 20/25 | Loss: 0.00124529
Iteration 21/25 | Loss: 0.00124529
Iteration 22/25 | Loss: 0.00124529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012452888768166304, 0.0012452888768166304, 0.0012452888768166304, 0.0012452888768166304, 0.0012452888768166304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012452888768166304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124529
Iteration 2/1000 | Loss: 0.00020408
Iteration 3/1000 | Loss: 0.00010580
Iteration 4/1000 | Loss: 0.00012415
Iteration 5/1000 | Loss: 0.00006206
Iteration 6/1000 | Loss: 0.00005276
Iteration 7/1000 | Loss: 0.00004745
Iteration 8/1000 | Loss: 0.00008740
Iteration 9/1000 | Loss: 0.00004245
Iteration 10/1000 | Loss: 0.00005693
Iteration 11/1000 | Loss: 0.00005212
Iteration 12/1000 | Loss: 0.00005172
Iteration 13/1000 | Loss: 0.00004346
Iteration 14/1000 | Loss: 0.00009363
Iteration 15/1000 | Loss: 0.00003838
Iteration 16/1000 | Loss: 0.00006666
Iteration 17/1000 | Loss: 0.00007900
Iteration 18/1000 | Loss: 0.00035936
Iteration 19/1000 | Loss: 0.00006897
Iteration 20/1000 | Loss: 0.00005570
Iteration 21/1000 | Loss: 0.00004410
Iteration 22/1000 | Loss: 0.00003988
Iteration 23/1000 | Loss: 0.00005660
Iteration 24/1000 | Loss: 0.00007786
Iteration 25/1000 | Loss: 0.00003983
Iteration 26/1000 | Loss: 0.00004019
Iteration 27/1000 | Loss: 0.00004624
Iteration 28/1000 | Loss: 0.00005015
Iteration 29/1000 | Loss: 0.00003533
Iteration 30/1000 | Loss: 0.00003472
Iteration 31/1000 | Loss: 0.00003467
Iteration 32/1000 | Loss: 0.00004681
Iteration 33/1000 | Loss: 0.00008309
Iteration 34/1000 | Loss: 0.00005440
Iteration 35/1000 | Loss: 0.00004001
Iteration 36/1000 | Loss: 0.00003408
Iteration 37/1000 | Loss: 0.00003402
Iteration 38/1000 | Loss: 0.00003402
Iteration 39/1000 | Loss: 0.00003688
Iteration 40/1000 | Loss: 0.00003549
Iteration 41/1000 | Loss: 0.00003398
Iteration 42/1000 | Loss: 0.00003392
Iteration 43/1000 | Loss: 0.00003391
Iteration 44/1000 | Loss: 0.00003391
Iteration 45/1000 | Loss: 0.00003391
Iteration 46/1000 | Loss: 0.00003391
Iteration 47/1000 | Loss: 0.00003391
Iteration 48/1000 | Loss: 0.00003391
Iteration 49/1000 | Loss: 0.00003391
Iteration 50/1000 | Loss: 0.00003391
Iteration 51/1000 | Loss: 0.00003391
Iteration 52/1000 | Loss: 0.00003391
Iteration 53/1000 | Loss: 0.00003391
Iteration 54/1000 | Loss: 0.00003391
Iteration 55/1000 | Loss: 0.00003391
Iteration 56/1000 | Loss: 0.00003391
Iteration 57/1000 | Loss: 0.00003391
Iteration 58/1000 | Loss: 0.00003391
Iteration 59/1000 | Loss: 0.00003391
Iteration 60/1000 | Loss: 0.00003391
Iteration 61/1000 | Loss: 0.00003391
Iteration 62/1000 | Loss: 0.00003391
Iteration 63/1000 | Loss: 0.00003391
Iteration 64/1000 | Loss: 0.00003391
Iteration 65/1000 | Loss: 0.00003391
Iteration 66/1000 | Loss: 0.00003391
Iteration 67/1000 | Loss: 0.00003391
Iteration 68/1000 | Loss: 0.00003391
Iteration 69/1000 | Loss: 0.00003391
Iteration 70/1000 | Loss: 0.00003391
Iteration 71/1000 | Loss: 0.00003391
Iteration 72/1000 | Loss: 0.00003391
Iteration 73/1000 | Loss: 0.00003391
Iteration 74/1000 | Loss: 0.00003391
Iteration 75/1000 | Loss: 0.00003391
Iteration 76/1000 | Loss: 0.00003391
Iteration 77/1000 | Loss: 0.00003391
Iteration 78/1000 | Loss: 0.00003391
Iteration 79/1000 | Loss: 0.00003391
Iteration 80/1000 | Loss: 0.00003391
Iteration 81/1000 | Loss: 0.00003391
Iteration 82/1000 | Loss: 0.00003391
Iteration 83/1000 | Loss: 0.00003391
Iteration 84/1000 | Loss: 0.00003391
Iteration 85/1000 | Loss: 0.00003391
Iteration 86/1000 | Loss: 0.00003391
Iteration 87/1000 | Loss: 0.00003391
Iteration 88/1000 | Loss: 0.00003391
Iteration 89/1000 | Loss: 0.00003391
Iteration 90/1000 | Loss: 0.00003391
Iteration 91/1000 | Loss: 0.00003391
Iteration 92/1000 | Loss: 0.00003391
Iteration 93/1000 | Loss: 0.00003391
Iteration 94/1000 | Loss: 0.00003391
Iteration 95/1000 | Loss: 0.00003391
Iteration 96/1000 | Loss: 0.00003391
Iteration 97/1000 | Loss: 0.00003391
Iteration 98/1000 | Loss: 0.00003391
Iteration 99/1000 | Loss: 0.00003391
Iteration 100/1000 | Loss: 0.00003391
Iteration 101/1000 | Loss: 0.00003391
Iteration 102/1000 | Loss: 0.00003391
Iteration 103/1000 | Loss: 0.00003391
Iteration 104/1000 | Loss: 0.00003391
Iteration 105/1000 | Loss: 0.00003391
Iteration 106/1000 | Loss: 0.00003391
Iteration 107/1000 | Loss: 0.00003391
Iteration 108/1000 | Loss: 0.00003391
Iteration 109/1000 | Loss: 0.00003391
Iteration 110/1000 | Loss: 0.00003391
Iteration 111/1000 | Loss: 0.00003391
Iteration 112/1000 | Loss: 0.00003391
Iteration 113/1000 | Loss: 0.00003391
Iteration 114/1000 | Loss: 0.00003391
Iteration 115/1000 | Loss: 0.00003391
Iteration 116/1000 | Loss: 0.00003391
Iteration 117/1000 | Loss: 0.00003391
Iteration 118/1000 | Loss: 0.00003391
Iteration 119/1000 | Loss: 0.00003391
Iteration 120/1000 | Loss: 0.00003391
Iteration 121/1000 | Loss: 0.00003391
Iteration 122/1000 | Loss: 0.00003391
Iteration 123/1000 | Loss: 0.00003391
Iteration 124/1000 | Loss: 0.00003391
Iteration 125/1000 | Loss: 0.00003391
Iteration 126/1000 | Loss: 0.00003391
Iteration 127/1000 | Loss: 0.00003391
Iteration 128/1000 | Loss: 0.00003391
Iteration 129/1000 | Loss: 0.00003391
Iteration 130/1000 | Loss: 0.00003391
Iteration 131/1000 | Loss: 0.00003391
Iteration 132/1000 | Loss: 0.00003391
Iteration 133/1000 | Loss: 0.00003391
Iteration 134/1000 | Loss: 0.00003391
Iteration 135/1000 | Loss: 0.00003391
Iteration 136/1000 | Loss: 0.00003391
Iteration 137/1000 | Loss: 0.00003391
Iteration 138/1000 | Loss: 0.00003391
Iteration 139/1000 | Loss: 0.00003391
Iteration 140/1000 | Loss: 0.00003391
Iteration 141/1000 | Loss: 0.00003391
Iteration 142/1000 | Loss: 0.00003391
Iteration 143/1000 | Loss: 0.00003391
Iteration 144/1000 | Loss: 0.00003391
Iteration 145/1000 | Loss: 0.00003391
Iteration 146/1000 | Loss: 0.00003391
Iteration 147/1000 | Loss: 0.00003391
Iteration 148/1000 | Loss: 0.00003391
Iteration 149/1000 | Loss: 0.00003391
Iteration 150/1000 | Loss: 0.00003391
Iteration 151/1000 | Loss: 0.00003391
Iteration 152/1000 | Loss: 0.00003391
Iteration 153/1000 | Loss: 0.00003391
Iteration 154/1000 | Loss: 0.00003391
Iteration 155/1000 | Loss: 0.00003391
Iteration 156/1000 | Loss: 0.00003391
Iteration 157/1000 | Loss: 0.00003391
Iteration 158/1000 | Loss: 0.00003391
Iteration 159/1000 | Loss: 0.00003391
Iteration 160/1000 | Loss: 0.00003391
Iteration 161/1000 | Loss: 0.00003391
Iteration 162/1000 | Loss: 0.00003391
Iteration 163/1000 | Loss: 0.00003391
Iteration 164/1000 | Loss: 0.00003391
Iteration 165/1000 | Loss: 0.00003391
Iteration 166/1000 | Loss: 0.00003391
Iteration 167/1000 | Loss: 0.00003391
Iteration 168/1000 | Loss: 0.00003391
Iteration 169/1000 | Loss: 0.00003391
Iteration 170/1000 | Loss: 0.00003391
Iteration 171/1000 | Loss: 0.00003391
Iteration 172/1000 | Loss: 0.00003391
Iteration 173/1000 | Loss: 0.00003391
Iteration 174/1000 | Loss: 0.00003391
Iteration 175/1000 | Loss: 0.00003391
Iteration 176/1000 | Loss: 0.00003391
Iteration 177/1000 | Loss: 0.00003391
Iteration 178/1000 | Loss: 0.00003391
Iteration 179/1000 | Loss: 0.00003391
Iteration 180/1000 | Loss: 0.00003391
Iteration 181/1000 | Loss: 0.00003391
Iteration 182/1000 | Loss: 0.00003391
Iteration 183/1000 | Loss: 0.00003391
Iteration 184/1000 | Loss: 0.00003391
Iteration 185/1000 | Loss: 0.00003391
Iteration 186/1000 | Loss: 0.00003391
Iteration 187/1000 | Loss: 0.00003391
Iteration 188/1000 | Loss: 0.00003391
Iteration 189/1000 | Loss: 0.00003391
Iteration 190/1000 | Loss: 0.00003391
Iteration 191/1000 | Loss: 0.00003391
Iteration 192/1000 | Loss: 0.00003391
Iteration 193/1000 | Loss: 0.00003391
Iteration 194/1000 | Loss: 0.00003391
Iteration 195/1000 | Loss: 0.00003391
Iteration 196/1000 | Loss: 0.00003391
Iteration 197/1000 | Loss: 0.00003391
Iteration 198/1000 | Loss: 0.00003391
Iteration 199/1000 | Loss: 0.00003391
Iteration 200/1000 | Loss: 0.00003391
Iteration 201/1000 | Loss: 0.00003391
Iteration 202/1000 | Loss: 0.00003391
Iteration 203/1000 | Loss: 0.00003391
Iteration 204/1000 | Loss: 0.00003391
Iteration 205/1000 | Loss: 0.00003391
Iteration 206/1000 | Loss: 0.00003391
Iteration 207/1000 | Loss: 0.00003391
Iteration 208/1000 | Loss: 0.00003391
Iteration 209/1000 | Loss: 0.00003391
Iteration 210/1000 | Loss: 0.00003391
Iteration 211/1000 | Loss: 0.00003391
Iteration 212/1000 | Loss: 0.00003391
Iteration 213/1000 | Loss: 0.00003391
Iteration 214/1000 | Loss: 0.00003391
Iteration 215/1000 | Loss: 0.00003391
Iteration 216/1000 | Loss: 0.00003391
Iteration 217/1000 | Loss: 0.00003391
Iteration 218/1000 | Loss: 0.00003391
Iteration 219/1000 | Loss: 0.00003391
Iteration 220/1000 | Loss: 0.00003391
Iteration 221/1000 | Loss: 0.00003391
Iteration 222/1000 | Loss: 0.00003391
Iteration 223/1000 | Loss: 0.00003391
Iteration 224/1000 | Loss: 0.00003391
Iteration 225/1000 | Loss: 0.00003391
Iteration 226/1000 | Loss: 0.00003391
Iteration 227/1000 | Loss: 0.00003391
Iteration 228/1000 | Loss: 0.00003391
Iteration 229/1000 | Loss: 0.00003391
Iteration 230/1000 | Loss: 0.00003391
Iteration 231/1000 | Loss: 0.00003391
Iteration 232/1000 | Loss: 0.00003391
Iteration 233/1000 | Loss: 0.00003391
Iteration 234/1000 | Loss: 0.00003391
Iteration 235/1000 | Loss: 0.00003391
Iteration 236/1000 | Loss: 0.00003391
Iteration 237/1000 | Loss: 0.00003391
Iteration 238/1000 | Loss: 0.00003391
Iteration 239/1000 | Loss: 0.00003391
Iteration 240/1000 | Loss: 0.00003391
Iteration 241/1000 | Loss: 0.00003391
Iteration 242/1000 | Loss: 0.00003391
Iteration 243/1000 | Loss: 0.00003391
Iteration 244/1000 | Loss: 0.00003391
Iteration 245/1000 | Loss: 0.00003391
Iteration 246/1000 | Loss: 0.00003391
Iteration 247/1000 | Loss: 0.00003391
Iteration 248/1000 | Loss: 0.00003391
Iteration 249/1000 | Loss: 0.00003391
Iteration 250/1000 | Loss: 0.00003391
Iteration 251/1000 | Loss: 0.00003391
Iteration 252/1000 | Loss: 0.00003391
Iteration 253/1000 | Loss: 0.00003391
Iteration 254/1000 | Loss: 0.00003391
Iteration 255/1000 | Loss: 0.00003391
Iteration 256/1000 | Loss: 0.00003391
Iteration 257/1000 | Loss: 0.00003391
Iteration 258/1000 | Loss: 0.00003391
Iteration 259/1000 | Loss: 0.00003391
Iteration 260/1000 | Loss: 0.00003391
Iteration 261/1000 | Loss: 0.00003391
Iteration 262/1000 | Loss: 0.00003391
Iteration 263/1000 | Loss: 0.00003391
Iteration 264/1000 | Loss: 0.00003391
Iteration 265/1000 | Loss: 0.00003391
Iteration 266/1000 | Loss: 0.00003391
Iteration 267/1000 | Loss: 0.00003391
Iteration 268/1000 | Loss: 0.00003391
Iteration 269/1000 | Loss: 0.00003391
Iteration 270/1000 | Loss: 0.00003391
Iteration 271/1000 | Loss: 0.00003391
Iteration 272/1000 | Loss: 0.00003391
Iteration 273/1000 | Loss: 0.00003391
Iteration 274/1000 | Loss: 0.00003391
Iteration 275/1000 | Loss: 0.00003391
Iteration 276/1000 | Loss: 0.00003391
Iteration 277/1000 | Loss: 0.00003391
Iteration 278/1000 | Loss: 0.00003391
Iteration 279/1000 | Loss: 0.00003391
Iteration 280/1000 | Loss: 0.00003391
Iteration 281/1000 | Loss: 0.00003391
Iteration 282/1000 | Loss: 0.00003391
Iteration 283/1000 | Loss: 0.00003391
Iteration 284/1000 | Loss: 0.00003391
Iteration 285/1000 | Loss: 0.00003391
Iteration 286/1000 | Loss: 0.00003391
Iteration 287/1000 | Loss: 0.00003391
Iteration 288/1000 | Loss: 0.00003391
Iteration 289/1000 | Loss: 0.00003391
Iteration 290/1000 | Loss: 0.00003391
Iteration 291/1000 | Loss: 0.00003391
Iteration 292/1000 | Loss: 0.00003391
Iteration 293/1000 | Loss: 0.00003391
Iteration 294/1000 | Loss: 0.00003391
Iteration 295/1000 | Loss: 0.00003391
Iteration 296/1000 | Loss: 0.00003391
Iteration 297/1000 | Loss: 0.00003391
Iteration 298/1000 | Loss: 0.00003391
Iteration 299/1000 | Loss: 0.00003391
Iteration 300/1000 | Loss: 0.00003391
Iteration 301/1000 | Loss: 0.00003391
Iteration 302/1000 | Loss: 0.00003391
Iteration 303/1000 | Loss: 0.00003391
Iteration 304/1000 | Loss: 0.00003391
Iteration 305/1000 | Loss: 0.00003391
Iteration 306/1000 | Loss: 0.00003391
Iteration 307/1000 | Loss: 0.00003391
Iteration 308/1000 | Loss: 0.00003391
Iteration 309/1000 | Loss: 0.00003391
Iteration 310/1000 | Loss: 0.00003391
Iteration 311/1000 | Loss: 0.00003391
Iteration 312/1000 | Loss: 0.00003391
Iteration 313/1000 | Loss: 0.00003391
Iteration 314/1000 | Loss: 0.00003391
Iteration 315/1000 | Loss: 0.00003391
Iteration 316/1000 | Loss: 0.00003391
Iteration 317/1000 | Loss: 0.00003391
Iteration 318/1000 | Loss: 0.00003391
Iteration 319/1000 | Loss: 0.00003391
Iteration 320/1000 | Loss: 0.00003391
Iteration 321/1000 | Loss: 0.00003391
Iteration 322/1000 | Loss: 0.00003391
Iteration 323/1000 | Loss: 0.00003391
Iteration 324/1000 | Loss: 0.00003391
Iteration 325/1000 | Loss: 0.00003391
Iteration 326/1000 | Loss: 0.00003391
Iteration 327/1000 | Loss: 0.00003391
Iteration 328/1000 | Loss: 0.00003391
Iteration 329/1000 | Loss: 0.00003391
Iteration 330/1000 | Loss: 0.00003391
Iteration 331/1000 | Loss: 0.00003391
Iteration 332/1000 | Loss: 0.00003391
Iteration 333/1000 | Loss: 0.00003391
Iteration 334/1000 | Loss: 0.00003391
Iteration 335/1000 | Loss: 0.00003391
Iteration 336/1000 | Loss: 0.00003391
Iteration 337/1000 | Loss: 0.00003391
Iteration 338/1000 | Loss: 0.00003391
Iteration 339/1000 | Loss: 0.00003391
Iteration 340/1000 | Loss: 0.00003391
Iteration 341/1000 | Loss: 0.00003391
Iteration 342/1000 | Loss: 0.00003391
Iteration 343/1000 | Loss: 0.00003391
Iteration 344/1000 | Loss: 0.00003391
Iteration 345/1000 | Loss: 0.00003391
Iteration 346/1000 | Loss: 0.00003391
Iteration 347/1000 | Loss: 0.00003391
Iteration 348/1000 | Loss: 0.00003391
Iteration 349/1000 | Loss: 0.00003391
Iteration 350/1000 | Loss: 0.00003391
Iteration 351/1000 | Loss: 0.00003391
Iteration 352/1000 | Loss: 0.00003391
Iteration 353/1000 | Loss: 0.00003391
Iteration 354/1000 | Loss: 0.00003391
Iteration 355/1000 | Loss: 0.00003391
Iteration 356/1000 | Loss: 0.00003391
Iteration 357/1000 | Loss: 0.00003391
Iteration 358/1000 | Loss: 0.00003391
Iteration 359/1000 | Loss: 0.00003391
Iteration 360/1000 | Loss: 0.00003391
Iteration 361/1000 | Loss: 0.00003391
Iteration 362/1000 | Loss: 0.00003391
Iteration 363/1000 | Loss: 0.00003391
Iteration 364/1000 | Loss: 0.00003391
Iteration 365/1000 | Loss: 0.00003391
Iteration 366/1000 | Loss: 0.00003391
Iteration 367/1000 | Loss: 0.00003391
Iteration 368/1000 | Loss: 0.00003391
Iteration 369/1000 | Loss: 0.00003391
Iteration 370/1000 | Loss: 0.00003391
Iteration 371/1000 | Loss: 0.00003391
Iteration 372/1000 | Loss: 0.00003391
Iteration 373/1000 | Loss: 0.00003391
Iteration 374/1000 | Loss: 0.00003391
Iteration 375/1000 | Loss: 0.00003391
Iteration 376/1000 | Loss: 0.00003391
Iteration 377/1000 | Loss: 0.00003391
Iteration 378/1000 | Loss: 0.00003391
Iteration 379/1000 | Loss: 0.00003391
Iteration 380/1000 | Loss: 0.00003391
Iteration 381/1000 | Loss: 0.00003391
Iteration 382/1000 | Loss: 0.00003391
Iteration 383/1000 | Loss: 0.00003391
Iteration 384/1000 | Loss: 0.00003391
Iteration 385/1000 | Loss: 0.00003391
Iteration 386/1000 | Loss: 0.00003391
Iteration 387/1000 | Loss: 0.00003391
Iteration 388/1000 | Loss: 0.00003391
Iteration 389/1000 | Loss: 0.00003391
Iteration 390/1000 | Loss: 0.00003391
Iteration 391/1000 | Loss: 0.00003391
Iteration 392/1000 | Loss: 0.00003391
Iteration 393/1000 | Loss: 0.00003391
Iteration 394/1000 | Loss: 0.00003391
Iteration 395/1000 | Loss: 0.00003391
Iteration 396/1000 | Loss: 0.00003391
Iteration 397/1000 | Loss: 0.00003391
Iteration 398/1000 | Loss: 0.00003391
Iteration 399/1000 | Loss: 0.00003391
Iteration 400/1000 | Loss: 0.00003391
Iteration 401/1000 | Loss: 0.00003391
Iteration 402/1000 | Loss: 0.00003391
Iteration 403/1000 | Loss: 0.00003391
Iteration 404/1000 | Loss: 0.00003391
Iteration 405/1000 | Loss: 0.00003391
Iteration 406/1000 | Loss: 0.00003391
Iteration 407/1000 | Loss: 0.00003391
Iteration 408/1000 | Loss: 0.00003391
Iteration 409/1000 | Loss: 0.00003391
Iteration 410/1000 | Loss: 0.00003391
Iteration 411/1000 | Loss: 0.00003391
Iteration 412/1000 | Loss: 0.00003391
Iteration 413/1000 | Loss: 0.00003391
Iteration 414/1000 | Loss: 0.00003391
Iteration 415/1000 | Loss: 0.00003391
Iteration 416/1000 | Loss: 0.00003391
Iteration 417/1000 | Loss: 0.00003391
Iteration 418/1000 | Loss: 0.00003391
Iteration 419/1000 | Loss: 0.00003391
Iteration 420/1000 | Loss: 0.00003391
Iteration 421/1000 | Loss: 0.00003391
Iteration 422/1000 | Loss: 0.00003391
Iteration 423/1000 | Loss: 0.00003391
Iteration 424/1000 | Loss: 0.00003391
Iteration 425/1000 | Loss: 0.00003391
Iteration 426/1000 | Loss: 0.00003391
Iteration 427/1000 | Loss: 0.00003391
Iteration 428/1000 | Loss: 0.00003391
Iteration 429/1000 | Loss: 0.00003391
Iteration 430/1000 | Loss: 0.00003391
Iteration 431/1000 | Loss: 0.00003391
Iteration 432/1000 | Loss: 0.00003391
Iteration 433/1000 | Loss: 0.00003391
Iteration 434/1000 | Loss: 0.00003391
Iteration 435/1000 | Loss: 0.00003391
Iteration 436/1000 | Loss: 0.00003391
Iteration 437/1000 | Loss: 0.00003391
Iteration 438/1000 | Loss: 0.00003391
Iteration 439/1000 | Loss: 0.00003391
Iteration 440/1000 | Loss: 0.00003391
Iteration 441/1000 | Loss: 0.00003391
Iteration 442/1000 | Loss: 0.00003391
Iteration 443/1000 | Loss: 0.00003391
Iteration 444/1000 | Loss: 0.00003391
Iteration 445/1000 | Loss: 0.00003391
Iteration 446/1000 | Loss: 0.00003391
Iteration 447/1000 | Loss: 0.00003391
Iteration 448/1000 | Loss: 0.00003391
Iteration 449/1000 | Loss: 0.00003391
Iteration 450/1000 | Loss: 0.00003391
Iteration 451/1000 | Loss: 0.00003391
Iteration 452/1000 | Loss: 0.00003391
Iteration 453/1000 | Loss: 0.00003391
Iteration 454/1000 | Loss: 0.00003391
Iteration 455/1000 | Loss: 0.00003391
Iteration 456/1000 | Loss: 0.00003391
Iteration 457/1000 | Loss: 0.00003391
Iteration 458/1000 | Loss: 0.00003391
Iteration 459/1000 | Loss: 0.00003391
Iteration 460/1000 | Loss: 0.00003391
Iteration 461/1000 | Loss: 0.00003391
Iteration 462/1000 | Loss: 0.00003391
Iteration 463/1000 | Loss: 0.00003391
Iteration 464/1000 | Loss: 0.00003391
Iteration 465/1000 | Loss: 0.00003391
Iteration 466/1000 | Loss: 0.00003391
Iteration 467/1000 | Loss: 0.00003391
Iteration 468/1000 | Loss: 0.00003391
Iteration 469/1000 | Loss: 0.00003391
Iteration 470/1000 | Loss: 0.00003391
Iteration 471/1000 | Loss: 0.00003391
Iteration 472/1000 | Loss: 0.00003391
Iteration 473/1000 | Loss: 0.00003391
Iteration 474/1000 | Loss: 0.00003391
Iteration 475/1000 | Loss: 0.00003391
Iteration 476/1000 | Loss: 0.00003391
Iteration 477/1000 | Loss: 0.00003391
Iteration 478/1000 | Loss: 0.00003391
Iteration 479/1000 | Loss: 0.00003391
Iteration 480/1000 | Loss: 0.00003391
Iteration 481/1000 | Loss: 0.00003391
Iteration 482/1000 | Loss: 0.00003391
Iteration 483/1000 | Loss: 0.00003391
Iteration 484/1000 | Loss: 0.00003391
Iteration 485/1000 | Loss: 0.00003391
Iteration 486/1000 | Loss: 0.00003391
Iteration 487/1000 | Loss: 0.00003391
Iteration 488/1000 | Loss: 0.00003391
Iteration 489/1000 | Loss: 0.00003391
Iteration 490/1000 | Loss: 0.00003391
Iteration 491/1000 | Loss: 0.00003391
Iteration 492/1000 | Loss: 0.00003391
Iteration 493/1000 | Loss: 0.00003391
Iteration 494/1000 | Loss: 0.00003391
Iteration 495/1000 | Loss: 0.00003391
Iteration 496/1000 | Loss: 0.00003391
Iteration 497/1000 | Loss: 0.00003391
Iteration 498/1000 | Loss: 0.00003391
Iteration 499/1000 | Loss: 0.00003391
Iteration 500/1000 | Loss: 0.00003391
Iteration 501/1000 | Loss: 0.00003391
Iteration 502/1000 | Loss: 0.00003391
Iteration 503/1000 | Loss: 0.00003391
Iteration 504/1000 | Loss: 0.00003391
Iteration 505/1000 | Loss: 0.00003391
Iteration 506/1000 | Loss: 0.00003391
Iteration 507/1000 | Loss: 0.00003391
Iteration 508/1000 | Loss: 0.00003391
Iteration 509/1000 | Loss: 0.00003391
Iteration 510/1000 | Loss: 0.00003391
Iteration 511/1000 | Loss: 0.00003391
Iteration 512/1000 | Loss: 0.00003391
Iteration 513/1000 | Loss: 0.00003391
Iteration 514/1000 | Loss: 0.00003391
Iteration 515/1000 | Loss: 0.00003391
Iteration 516/1000 | Loss: 0.00003391
Iteration 517/1000 | Loss: 0.00003391
Iteration 518/1000 | Loss: 0.00003391
Iteration 519/1000 | Loss: 0.00003391
Iteration 520/1000 | Loss: 0.00003391
Iteration 521/1000 | Loss: 0.00003391
Iteration 522/1000 | Loss: 0.00003391
Iteration 523/1000 | Loss: 0.00003391
Iteration 524/1000 | Loss: 0.00003391
Iteration 525/1000 | Loss: 0.00003391
Iteration 526/1000 | Loss: 0.00003391
Iteration 527/1000 | Loss: 0.00003391
Iteration 528/1000 | Loss: 0.00003391
Iteration 529/1000 | Loss: 0.00003391
Iteration 530/1000 | Loss: 0.00003391
Iteration 531/1000 | Loss: 0.00003391
Iteration 532/1000 | Loss: 0.00003391
Iteration 533/1000 | Loss: 0.00003391
Iteration 534/1000 | Loss: 0.00003391
Iteration 535/1000 | Loss: 0.00003391
Iteration 536/1000 | Loss: 0.00003391
Iteration 537/1000 | Loss: 0.00003391
Iteration 538/1000 | Loss: 0.00003391
Iteration 539/1000 | Loss: 0.00003391
Iteration 540/1000 | Loss: 0.00003391
Iteration 541/1000 | Loss: 0.00003391
Iteration 542/1000 | Loss: 0.00003391
Iteration 543/1000 | Loss: 0.00003391
Iteration 544/1000 | Loss: 0.00003391
Iteration 545/1000 | Loss: 0.00003391
Iteration 546/1000 | Loss: 0.00003391
Iteration 547/1000 | Loss: 0.00003391
Iteration 548/1000 | Loss: 0.00003391
Iteration 549/1000 | Loss: 0.00003391
Iteration 550/1000 | Loss: 0.00003391
Iteration 551/1000 | Loss: 0.00003391
Iteration 552/1000 | Loss: 0.00003391
Iteration 553/1000 | Loss: 0.00003391
Iteration 554/1000 | Loss: 0.00003391
Iteration 555/1000 | Loss: 0.00003391
Iteration 556/1000 | Loss: 0.00003391
Iteration 557/1000 | Loss: 0.00003391
Iteration 558/1000 | Loss: 0.00003391
Iteration 559/1000 | Loss: 0.00003391
Iteration 560/1000 | Loss: 0.00003391
Iteration 561/1000 | Loss: 0.00003391
Iteration 562/1000 | Loss: 0.00003391
Iteration 563/1000 | Loss: 0.00003391
Iteration 564/1000 | Loss: 0.00003391
Iteration 565/1000 | Loss: 0.00003391
Iteration 566/1000 | Loss: 0.00003391
Iteration 567/1000 | Loss: 0.00003391
Iteration 568/1000 | Loss: 0.00003391
Iteration 569/1000 | Loss: 0.00003391
Iteration 570/1000 | Loss: 0.00003391
Iteration 571/1000 | Loss: 0.00003391
Iteration 572/1000 | Loss: 0.00003391
Iteration 573/1000 | Loss: 0.00003391
Iteration 574/1000 | Loss: 0.00003391
Iteration 575/1000 | Loss: 0.00003391
Iteration 576/1000 | Loss: 0.00003391
Iteration 577/1000 | Loss: 0.00003391
Iteration 578/1000 | Loss: 0.00003391
Iteration 579/1000 | Loss: 0.00003391
Iteration 580/1000 | Loss: 0.00003391
Iteration 581/1000 | Loss: 0.00003391
Iteration 582/1000 | Loss: 0.00003391
Iteration 583/1000 | Loss: 0.00003391
Iteration 584/1000 | Loss: 0.00003391
Iteration 585/1000 | Loss: 0.00003391
Iteration 586/1000 | Loss: 0.00003391
Iteration 587/1000 | Loss: 0.00003391
Iteration 588/1000 | Loss: 0.00003391
Iteration 589/1000 | Loss: 0.00003391
Iteration 590/1000 | Loss: 0.00003391
Iteration 591/1000 | Loss: 0.00003391
Iteration 592/1000 | Loss: 0.00003391
Iteration 593/1000 | Loss: 0.00003391
Iteration 594/1000 | Loss: 0.00003391
Iteration 595/1000 | Loss: 0.00003391
Iteration 596/1000 | Loss: 0.00003391
Iteration 597/1000 | Loss: 0.00003391
Iteration 598/1000 | Loss: 0.00003391
Iteration 599/1000 | Loss: 0.00003391
Iteration 600/1000 | Loss: 0.00003391
Iteration 601/1000 | Loss: 0.00003391
Iteration 602/1000 | Loss: 0.00003391
Iteration 603/1000 | Loss: 0.00003391
Iteration 604/1000 | Loss: 0.00003391
Iteration 605/1000 | Loss: 0.00003391
Iteration 606/1000 | Loss: 0.00003391
Iteration 607/1000 | Loss: 0.00003391
Iteration 608/1000 | Loss: 0.00003391
Iteration 609/1000 | Loss: 0.00003391
Iteration 610/1000 | Loss: 0.00003391
Iteration 611/1000 | Loss: 0.00003391
Iteration 612/1000 | Loss: 0.00003391
Iteration 613/1000 | Loss: 0.00003391
Iteration 614/1000 | Loss: 0.00003391
Iteration 615/1000 | Loss: 0.00003391
Iteration 616/1000 | Loss: 0.00003391
Iteration 617/1000 | Loss: 0.00003391
Iteration 618/1000 | Loss: 0.00003391
Iteration 619/1000 | Loss: 0.00003391
Iteration 620/1000 | Loss: 0.00003391
Iteration 621/1000 | Loss: 0.00003391
Iteration 622/1000 | Loss: 0.00003391
Iteration 623/1000 | Loss: 0.00003391
Iteration 624/1000 | Loss: 0.00003391
Iteration 625/1000 | Loss: 0.00003391
Iteration 626/1000 | Loss: 0.00003391
Iteration 627/1000 | Loss: 0.00003391
Iteration 628/1000 | Loss: 0.00003391
Iteration 629/1000 | Loss: 0.00003391
Iteration 630/1000 | Loss: 0.00003391
Iteration 631/1000 | Loss: 0.00003391
Iteration 632/1000 | Loss: 0.00003391
Iteration 633/1000 | Loss: 0.00003391
Iteration 634/1000 | Loss: 0.00003391
Iteration 635/1000 | Loss: 0.00003391
Iteration 636/1000 | Loss: 0.00003391
Iteration 637/1000 | Loss: 0.00003391
Iteration 638/1000 | Loss: 0.00003391
Iteration 639/1000 | Loss: 0.00003391
Iteration 640/1000 | Loss: 0.00003391
Iteration 641/1000 | Loss: 0.00003391
Iteration 642/1000 | Loss: 0.00003391
Iteration 643/1000 | Loss: 0.00003391
Iteration 644/1000 | Loss: 0.00003391
Iteration 645/1000 | Loss: 0.00003391
Iteration 646/1000 | Loss: 0.00003391
Iteration 647/1000 | Loss: 0.00003391
Iteration 648/1000 | Loss: 0.00003391
Iteration 649/1000 | Loss: 0.00003391
Iteration 650/1000 | Loss: 0.00003391
Iteration 651/1000 | Loss: 0.00003391
Iteration 652/1000 | Loss: 0.00003391
Iteration 653/1000 | Loss: 0.00003391
Iteration 654/1000 | Loss: 0.00003391
Iteration 655/1000 | Loss: 0.00003391
Iteration 656/1000 | Loss: 0.00003391
Iteration 657/1000 | Loss: 0.00003391
Iteration 658/1000 | Loss: 0.00003391
Iteration 659/1000 | Loss: 0.00003391
Iteration 660/1000 | Loss: 0.00003391
Iteration 661/1000 | Loss: 0.00003391
Iteration 662/1000 | Loss: 0.00003391
Iteration 663/1000 | Loss: 0.00003391
Iteration 664/1000 | Loss: 0.00003391
Iteration 665/1000 | Loss: 0.00003391
Iteration 666/1000 | Loss: 0.00003391
Iteration 667/1000 | Loss: 0.00003391
Iteration 668/1000 | Loss: 0.00003391
Iteration 669/1000 | Loss: 0.00003391
Iteration 670/1000 | Loss: 0.00003391
Iteration 671/1000 | Loss: 0.00003391
Iteration 672/1000 | Loss: 0.00003391
Iteration 673/1000 | Loss: 0.00003391
Iteration 674/1000 | Loss: 0.00003391
Iteration 675/1000 | Loss: 0.00003391
Iteration 676/1000 | Loss: 0.00003391
Iteration 677/1000 | Loss: 0.00003391
Iteration 678/1000 | Loss: 0.00003391
Iteration 679/1000 | Loss: 0.00003391
Iteration 680/1000 | Loss: 0.00003391
Iteration 681/1000 | Loss: 0.00003391
Iteration 682/1000 | Loss: 0.00003391
Iteration 683/1000 | Loss: 0.00003391
Iteration 684/1000 | Loss: 0.00003391
Iteration 685/1000 | Loss: 0.00003391
Iteration 686/1000 | Loss: 0.00003391
Iteration 687/1000 | Loss: 0.00003391
Iteration 688/1000 | Loss: 0.00003391
Iteration 689/1000 | Loss: 0.00003391
Iteration 690/1000 | Loss: 0.00003391
Iteration 691/1000 | Loss: 0.00003391
Iteration 692/1000 | Loss: 0.00003391
Iteration 693/1000 | Loss: 0.00003391
Iteration 694/1000 | Loss: 0.00003391
Iteration 695/1000 | Loss: 0.00003391
Iteration 696/1000 | Loss: 0.00003391
Iteration 697/1000 | Loss: 0.00003391
Iteration 698/1000 | Loss: 0.00003391
Iteration 699/1000 | Loss: 0.00003391
Iteration 700/1000 | Loss: 0.00003391
Iteration 701/1000 | Loss: 0.00003391
Iteration 702/1000 | Loss: 0.00003391
Iteration 703/1000 | Loss: 0.00003391
Iteration 704/1000 | Loss: 0.00003391
Iteration 705/1000 | Loss: 0.00003391
Iteration 706/1000 | Loss: 0.00003391
Iteration 707/1000 | Loss: 0.00003391
Iteration 708/1000 | Loss: 0.00003391
Iteration 709/1000 | Loss: 0.00003391
Iteration 710/1000 | Loss: 0.00003391
Iteration 711/1000 | Loss: 0.00003391
Iteration 712/1000 | Loss: 0.00003391
Iteration 713/1000 | Loss: 0.00003391
Iteration 714/1000 | Loss: 0.00003391
Iteration 715/1000 | Loss: 0.00003391
Iteration 716/1000 | Loss: 0.00003391
Iteration 717/1000 | Loss: 0.00003391
Iteration 718/1000 | Loss: 0.00003391
Iteration 719/1000 | Loss: 0.00003391
Iteration 720/1000 | Loss: 0.00003391
Iteration 721/1000 | Loss: 0.00003391
Iteration 722/1000 | Loss: 0.00003391
Iteration 723/1000 | Loss: 0.00003391
Iteration 724/1000 | Loss: 0.00003391
Iteration 725/1000 | Loss: 0.00003391
Iteration 726/1000 | Loss: 0.00003391
Iteration 727/1000 | Loss: 0.00003391
Iteration 728/1000 | Loss: 0.00003391
Iteration 729/1000 | Loss: 0.00003391
Iteration 730/1000 | Loss: 0.00003391
Iteration 731/1000 | Loss: 0.00003391
Iteration 732/1000 | Loss: 0.00003391
Iteration 733/1000 | Loss: 0.00003391
Iteration 734/1000 | Loss: 0.00003391
Iteration 735/1000 | Loss: 0.00003391
Iteration 736/1000 | Loss: 0.00003391
Iteration 737/1000 | Loss: 0.00003391
Iteration 738/1000 | Loss: 0.00003391
Iteration 739/1000 | Loss: 0.00003391
Iteration 740/1000 | Loss: 0.00003391
Iteration 741/1000 | Loss: 0.00003391
Iteration 742/1000 | Loss: 0.00003391
Iteration 743/1000 | Loss: 0.00003391
Iteration 744/1000 | Loss: 0.00003391
Iteration 745/1000 | Loss: 0.00003391
Iteration 746/1000 | Loss: 0.00003391
Iteration 747/1000 | Loss: 0.00003391
Iteration 748/1000 | Loss: 0.00003391
Iteration 749/1000 | Loss: 0.00003391
Iteration 750/1000 | Loss: 0.00003391
Iteration 751/1000 | Loss: 0.00003391
Iteration 752/1000 | Loss: 0.00003391
Iteration 753/1000 | Loss: 0.00003391
Iteration 754/1000 | Loss: 0.00003391
Iteration 755/1000 | Loss: 0.00003391
Iteration 756/1000 | Loss: 0.00003391
Iteration 757/1000 | Loss: 0.00003391
Iteration 758/1000 | Loss: 0.00003391
Iteration 759/1000 | Loss: 0.00003391
Iteration 760/1000 | Loss: 0.00003391
Iteration 761/1000 | Loss: 0.00003391
Iteration 762/1000 | Loss: 0.00003391
Iteration 763/1000 | Loss: 0.00003391
Iteration 764/1000 | Loss: 0.00003391
Iteration 765/1000 | Loss: 0.00003391
Iteration 766/1000 | Loss: 0.00003391
Iteration 767/1000 | Loss: 0.00003391
Iteration 768/1000 | Loss: 0.00003391
Iteration 769/1000 | Loss: 0.00003391
Iteration 770/1000 | Loss: 0.00003391
Iteration 771/1000 | Loss: 0.00003391
Iteration 772/1000 | Loss: 0.00003391
Iteration 773/1000 | Loss: 0.00003391
Iteration 774/1000 | Loss: 0.00003391
Iteration 775/1000 | Loss: 0.00003391
Iteration 776/1000 | Loss: 0.00003391
Iteration 777/1000 | Loss: 0.00003391
Iteration 778/1000 | Loss: 0.00003391
Iteration 779/1000 | Loss: 0.00003391
Iteration 780/1000 | Loss: 0.00003391
Iteration 781/1000 | Loss: 0.00003391
Iteration 782/1000 | Loss: 0.00003391
Iteration 783/1000 | Loss: 0.00003391
Iteration 784/1000 | Loss: 0.00003391
Iteration 785/1000 | Loss: 0.00003391
Iteration 786/1000 | Loss: 0.00003391
Iteration 787/1000 | Loss: 0.00003391
Iteration 788/1000 | Loss: 0.00003391
Iteration 789/1000 | Loss: 0.00003391
Iteration 790/1000 | Loss: 0.00003391
Iteration 791/1000 | Loss: 0.00003391
Iteration 792/1000 | Loss: 0.00003391
Iteration 793/1000 | Loss: 0.00003391
Iteration 794/1000 | Loss: 0.00003391
Iteration 795/1000 | Loss: 0.00003391
Iteration 796/1000 | Loss: 0.00003391
Iteration 797/1000 | Loss: 0.00003391
Iteration 798/1000 | Loss: 0.00003391
Iteration 799/1000 | Loss: 0.00003391
Iteration 800/1000 | Loss: 0.00003391
Iteration 801/1000 | Loss: 0.00003391
Iteration 802/1000 | Loss: 0.00003391
Iteration 803/1000 | Loss: 0.00003391
Iteration 804/1000 | Loss: 0.00003391
Iteration 805/1000 | Loss: 0.00003391
Iteration 806/1000 | Loss: 0.00003391
Iteration 807/1000 | Loss: 0.00003391
Iteration 808/1000 | Loss: 0.00003391
Iteration 809/1000 | Loss: 0.00003391
Iteration 810/1000 | Loss: 0.00003391
Iteration 811/1000 | Loss: 0.00003391
Iteration 812/1000 | Loss: 0.00003391
Iteration 813/1000 | Loss: 0.00003391
Iteration 814/1000 | Loss: 0.00003391
Iteration 815/1000 | Loss: 0.00003391
Iteration 816/1000 | Loss: 0.00003391
Iteration 817/1000 | Loss: 0.00003391
Iteration 818/1000 | Loss: 0.00003391
Iteration 819/1000 | Loss: 0.00003391
Iteration 820/1000 | Loss: 0.00003391
Iteration 821/1000 | Loss: 0.00003391
Iteration 822/1000 | Loss: 0.00003391
Iteration 823/1000 | Loss: 0.00003391
Iteration 824/1000 | Loss: 0.00003391
Iteration 825/1000 | Loss: 0.00003391
Iteration 826/1000 | Loss: 0.00003391
Iteration 827/1000 | Loss: 0.00003391
Iteration 828/1000 | Loss: 0.00003391
Iteration 829/1000 | Loss: 0.00003391
Iteration 830/1000 | Loss: 0.00003391
Iteration 831/1000 | Loss: 0.00003391
Iteration 832/1000 | Loss: 0.00003391
Iteration 833/1000 | Loss: 0.00003391
Iteration 834/1000 | Loss: 0.00003391
Iteration 835/1000 | Loss: 0.00003391
Iteration 836/1000 | Loss: 0.00003391
Iteration 837/1000 | Loss: 0.00003391
Iteration 838/1000 | Loss: 0.00003391
Iteration 839/1000 | Loss: 0.00003391
Iteration 840/1000 | Loss: 0.00003391
Iteration 841/1000 | Loss: 0.00003391
Iteration 842/1000 | Loss: 0.00003391
Iteration 843/1000 | Loss: 0.00003391
Iteration 844/1000 | Loss: 0.00003391
Iteration 845/1000 | Loss: 0.00003391
Iteration 846/1000 | Loss: 0.00003391
Iteration 847/1000 | Loss: 0.00003391
Iteration 848/1000 | Loss: 0.00003391
Iteration 849/1000 | Loss: 0.00003391
Iteration 850/1000 | Loss: 0.00003391
Iteration 851/1000 | Loss: 0.00003391
Iteration 852/1000 | Loss: 0.00003391
Iteration 853/1000 | Loss: 0.00003391
Iteration 854/1000 | Loss: 0.00003391
Iteration 855/1000 | Loss: 0.00003391
Iteration 856/1000 | Loss: 0.00003391
Iteration 857/1000 | Loss: 0.00003391
Iteration 858/1000 | Loss: 0.00003391
Iteration 859/1000 | Loss: 0.00003391
Iteration 860/1000 | Loss: 0.00003391
Iteration 861/1000 | Loss: 0.00003391
Iteration 862/1000 | Loss: 0.00003391
Iteration 863/1000 | Loss: 0.00003391
Iteration 864/1000 | Loss: 0.00003391
Iteration 865/1000 | Loss: 0.00003391
Iteration 866/1000 | Loss: 0.00003391
Iteration 867/1000 | Loss: 0.00003391
Iteration 868/1000 | Loss: 0.00003391
Iteration 869/1000 | Loss: 0.00003391
Iteration 870/1000 | Loss: 0.00003391
Iteration 871/1000 | Loss: 0.00003391
Iteration 872/1000 | Loss: 0.00003391
Iteration 873/1000 | Loss: 0.00003391
Iteration 874/1000 | Loss: 0.00003391
Iteration 875/1000 | Loss: 0.00003391
Iteration 876/1000 | Loss: 0.00003391
Iteration 877/1000 | Loss: 0.00003391
Iteration 878/1000 | Loss: 0.00003391
Iteration 879/1000 | Loss: 0.00003391
Iteration 880/1000 | Loss: 0.00003391
Iteration 881/1000 | Loss: 0.00003391
Iteration 882/1000 | Loss: 0.00003391
Iteration 883/1000 | Loss: 0.00003391
Iteration 884/1000 | Loss: 0.00003391
Iteration 885/1000 | Loss: 0.00003391
Iteration 886/1000 | Loss: 0.00003391
Iteration 887/1000 | Loss: 0.00003391
Iteration 888/1000 | Loss: 0.00003391
Iteration 889/1000 | Loss: 0.00003391
Iteration 890/1000 | Loss: 0.00003391
Iteration 891/1000 | Loss: 0.00003391
Iteration 892/1000 | Loss: 0.00003391
Iteration 893/1000 | Loss: 0.00003391
Iteration 894/1000 | Loss: 0.00003391
Iteration 895/1000 | Loss: 0.00003391
Iteration 896/1000 | Loss: 0.00003391
Iteration 897/1000 | Loss: 0.00003391
Iteration 898/1000 | Loss: 0.00003391
Iteration 899/1000 | Loss: 0.00003391
Iteration 900/1000 | Loss: 0.00003391
Iteration 901/1000 | Loss: 0.00003391
Iteration 902/1000 | Loss: 0.00003391
Iteration 903/1000 | Loss: 0.00003391
Iteration 904/1000 | Loss: 0.00003391
Iteration 905/1000 | Loss: 0.00003391
Iteration 906/1000 | Loss: 0.00003391
Iteration 907/1000 | Loss: 0.00003391
Iteration 908/1000 | Loss: 0.00003391
Iteration 909/1000 | Loss: 0.00003391
Iteration 910/1000 | Loss: 0.00003391
Iteration 911/1000 | Loss: 0.00003391
Iteration 912/1000 | Loss: 0.00003391
Iteration 913/1000 | Loss: 0.00003391
Iteration 914/1000 | Loss: 0.00003391
Iteration 915/1000 | Loss: 0.00003391
Iteration 916/1000 | Loss: 0.00003391
Iteration 917/1000 | Loss: 0.00003391
Iteration 918/1000 | Loss: 0.00003391
Iteration 919/1000 | Loss: 0.00003391
Iteration 920/1000 | Loss: 0.00003391
Iteration 921/1000 | Loss: 0.00003391
Iteration 922/1000 | Loss: 0.00003391
Iteration 923/1000 | Loss: 0.00003391
Iteration 924/1000 | Loss: 0.00003391
Iteration 925/1000 | Loss: 0.00003391
Iteration 926/1000 | Loss: 0.00003391
Iteration 927/1000 | Loss: 0.00003391
Iteration 928/1000 | Loss: 0.00003391
Iteration 929/1000 | Loss: 0.00003391
Iteration 930/1000 | Loss: 0.00003391
Iteration 931/1000 | Loss: 0.00003391
Iteration 932/1000 | Loss: 0.00003391
Iteration 933/1000 | Loss: 0.00003391
Iteration 934/1000 | Loss: 0.00003391
Iteration 935/1000 | Loss: 0.00003391
Iteration 936/1000 | Loss: 0.00003391
Iteration 937/1000 | Loss: 0.00003391
Iteration 938/1000 | Loss: 0.00003391
Iteration 939/1000 | Loss: 0.00003391
Iteration 940/1000 | Loss: 0.00003391
Iteration 941/1000 | Loss: 0.00003391
Iteration 942/1000 | Loss: 0.00003391
Iteration 943/1000 | Loss: 0.00003391
Iteration 944/1000 | Loss: 0.00003391
Iteration 945/1000 | Loss: 0.00003391
Iteration 946/1000 | Loss: 0.00003391
Iteration 947/1000 | Loss: 0.00003391
Iteration 948/1000 | Loss: 0.00003391
Iteration 949/1000 | Loss: 0.00003391
Iteration 950/1000 | Loss: 0.00003391
Iteration 951/1000 | Loss: 0.00003391
Iteration 952/1000 | Loss: 0.00003391
Iteration 953/1000 | Loss: 0.00003391
Iteration 954/1000 | Loss: 0.00003391
Iteration 955/1000 | Loss: 0.00003391
Iteration 956/1000 | Loss: 0.00003391
Iteration 957/1000 | Loss: 0.00003391
Iteration 958/1000 | Loss: 0.00003391
Iteration 959/1000 | Loss: 0.00003391
Iteration 960/1000 | Loss: 0.00003391
Iteration 961/1000 | Loss: 0.00003391
Iteration 962/1000 | Loss: 0.00003391
Iteration 963/1000 | Loss: 0.00003391
Iteration 964/1000 | Loss: 0.00003391
Iteration 965/1000 | Loss: 0.00003391
Iteration 966/1000 | Loss: 0.00003391
Iteration 967/1000 | Loss: 0.00003391
Iteration 968/1000 | Loss: 0.00003391
Iteration 969/1000 | Loss: 0.00003391
Iteration 970/1000 | Loss: 0.00003391
Iteration 971/1000 | Loss: 0.00003391
Iteration 972/1000 | Loss: 0.00003391
Iteration 973/1000 | Loss: 0.00003391
Iteration 974/1000 | Loss: 0.00003391
Iteration 975/1000 | Loss: 0.00003391
Iteration 976/1000 | Loss: 0.00003391
Iteration 977/1000 | Loss: 0.00003391
Iteration 978/1000 | Loss: 0.00003391
Iteration 979/1000 | Loss: 0.00003391
Iteration 980/1000 | Loss: 0.00003391
Iteration 981/1000 | Loss: 0.00003391
Iteration 982/1000 | Loss: 0.00003391
Iteration 983/1000 | Loss: 0.00003391
Iteration 984/1000 | Loss: 0.00003391
Iteration 985/1000 | Loss: 0.00003391
Iteration 986/1000 | Loss: 0.00003391
Iteration 987/1000 | Loss: 0.00003391
Iteration 988/1000 | Loss: 0.00003391
Iteration 989/1000 | Loss: 0.00003391
Iteration 990/1000 | Loss: 0.00003391
Iteration 991/1000 | Loss: 0.00003391
Iteration 992/1000 | Loss: 0.00003391
Iteration 993/1000 | Loss: 0.00003391
Iteration 994/1000 | Loss: 0.00003391
Iteration 995/1000 | Loss: 0.00003391
Iteration 996/1000 | Loss: 0.00003391
Iteration 997/1000 | Loss: 0.00003391
Iteration 998/1000 | Loss: 0.00003391
Iteration 999/1000 | Loss: 0.00003391
Iteration 1000/1000 | Loss: 0.00003391

Optimization complete. Final v2v error: 4.072916030883789 mm

Highest mean error: 11.508049964904785 mm for frame 110

Lowest mean error: 3.428880214691162 mm for frame 91

Saving results

Total time: 143.11045670509338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721828
Iteration 2/25 | Loss: 0.00143188
Iteration 3/25 | Loss: 0.00127011
Iteration 4/25 | Loss: 0.00124024
Iteration 5/25 | Loss: 0.00123108
Iteration 6/25 | Loss: 0.00122847
Iteration 7/25 | Loss: 0.00122847
Iteration 8/25 | Loss: 0.00122847
Iteration 9/25 | Loss: 0.00122847
Iteration 10/25 | Loss: 0.00122847
Iteration 11/25 | Loss: 0.00122847
Iteration 12/25 | Loss: 0.00122847
Iteration 13/25 | Loss: 0.00122847
Iteration 14/25 | Loss: 0.00122847
Iteration 15/25 | Loss: 0.00122847
Iteration 16/25 | Loss: 0.00122847
Iteration 17/25 | Loss: 0.00122847
Iteration 18/25 | Loss: 0.00122847
Iteration 19/25 | Loss: 0.00122847
Iteration 20/25 | Loss: 0.00122847
Iteration 21/25 | Loss: 0.00122847
Iteration 22/25 | Loss: 0.00122847
Iteration 23/25 | Loss: 0.00122847
Iteration 24/25 | Loss: 0.00122847
Iteration 25/25 | Loss: 0.00122847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50810695
Iteration 2/25 | Loss: 0.00179262
Iteration 3/25 | Loss: 0.00179261
Iteration 4/25 | Loss: 0.00179261
Iteration 5/25 | Loss: 0.00179261
Iteration 6/25 | Loss: 0.00179261
Iteration 7/25 | Loss: 0.00179261
Iteration 8/25 | Loss: 0.00179261
Iteration 9/25 | Loss: 0.00179261
Iteration 10/25 | Loss: 0.00179261
Iteration 11/25 | Loss: 0.00179261
Iteration 12/25 | Loss: 0.00179261
Iteration 13/25 | Loss: 0.00179261
Iteration 14/25 | Loss: 0.00179261
Iteration 15/25 | Loss: 0.00179261
Iteration 16/25 | Loss: 0.00179261
Iteration 17/25 | Loss: 0.00179261
Iteration 18/25 | Loss: 0.00179261
Iteration 19/25 | Loss: 0.00179261
Iteration 20/25 | Loss: 0.00179261
Iteration 21/25 | Loss: 0.00179261
Iteration 22/25 | Loss: 0.00179261
Iteration 23/25 | Loss: 0.00179261
Iteration 24/25 | Loss: 0.00179261
Iteration 25/25 | Loss: 0.00179261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179261
Iteration 2/1000 | Loss: 0.00006362
Iteration 3/1000 | Loss: 0.00003628
Iteration 4/1000 | Loss: 0.00002846
Iteration 5/1000 | Loss: 0.00002557
Iteration 6/1000 | Loss: 0.00002377
Iteration 7/1000 | Loss: 0.00002238
Iteration 8/1000 | Loss: 0.00002157
Iteration 9/1000 | Loss: 0.00002093
Iteration 10/1000 | Loss: 0.00002052
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00001989
Iteration 13/1000 | Loss: 0.00001971
Iteration 14/1000 | Loss: 0.00001955
Iteration 15/1000 | Loss: 0.00001945
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001940
Iteration 18/1000 | Loss: 0.00001940
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00001931
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001930
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001929
Iteration 32/1000 | Loss: 0.00001929
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00001928
Iteration 35/1000 | Loss: 0.00001928
Iteration 36/1000 | Loss: 0.00001928
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001926
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001925
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001921
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001920
Iteration 51/1000 | Loss: 0.00001920
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001919
Iteration 54/1000 | Loss: 0.00001919
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001914
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001913
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001912
Iteration 73/1000 | Loss: 0.00001912
Iteration 74/1000 | Loss: 0.00001911
Iteration 75/1000 | Loss: 0.00001911
Iteration 76/1000 | Loss: 0.00001911
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001909
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001909
Iteration 82/1000 | Loss: 0.00001909
Iteration 83/1000 | Loss: 0.00001908
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001908
Iteration 86/1000 | Loss: 0.00001908
Iteration 87/1000 | Loss: 0.00001908
Iteration 88/1000 | Loss: 0.00001908
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001907
Iteration 91/1000 | Loss: 0.00001907
Iteration 92/1000 | Loss: 0.00001907
Iteration 93/1000 | Loss: 0.00001907
Iteration 94/1000 | Loss: 0.00001907
Iteration 95/1000 | Loss: 0.00001906
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001904
Iteration 103/1000 | Loss: 0.00001904
Iteration 104/1000 | Loss: 0.00001904
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001902
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001901
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001901
Iteration 114/1000 | Loss: 0.00001901
Iteration 115/1000 | Loss: 0.00001900
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001900
Iteration 118/1000 | Loss: 0.00001900
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001899
Iteration 121/1000 | Loss: 0.00001899
Iteration 122/1000 | Loss: 0.00001899
Iteration 123/1000 | Loss: 0.00001899
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00001899
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00001899
Iteration 128/1000 | Loss: 0.00001898
Iteration 129/1000 | Loss: 0.00001898
Iteration 130/1000 | Loss: 0.00001898
Iteration 131/1000 | Loss: 0.00001898
Iteration 132/1000 | Loss: 0.00001898
Iteration 133/1000 | Loss: 0.00001897
Iteration 134/1000 | Loss: 0.00001897
Iteration 135/1000 | Loss: 0.00001897
Iteration 136/1000 | Loss: 0.00001897
Iteration 137/1000 | Loss: 0.00001897
Iteration 138/1000 | Loss: 0.00001897
Iteration 139/1000 | Loss: 0.00001897
Iteration 140/1000 | Loss: 0.00001897
Iteration 141/1000 | Loss: 0.00001897
Iteration 142/1000 | Loss: 0.00001897
Iteration 143/1000 | Loss: 0.00001896
Iteration 144/1000 | Loss: 0.00001896
Iteration 145/1000 | Loss: 0.00001896
Iteration 146/1000 | Loss: 0.00001896
Iteration 147/1000 | Loss: 0.00001896
Iteration 148/1000 | Loss: 0.00001896
Iteration 149/1000 | Loss: 0.00001896
Iteration 150/1000 | Loss: 0.00001896
Iteration 151/1000 | Loss: 0.00001896
Iteration 152/1000 | Loss: 0.00001896
Iteration 153/1000 | Loss: 0.00001896
Iteration 154/1000 | Loss: 0.00001896
Iteration 155/1000 | Loss: 0.00001896
Iteration 156/1000 | Loss: 0.00001895
Iteration 157/1000 | Loss: 0.00001895
Iteration 158/1000 | Loss: 0.00001895
Iteration 159/1000 | Loss: 0.00001895
Iteration 160/1000 | Loss: 0.00001895
Iteration 161/1000 | Loss: 0.00001895
Iteration 162/1000 | Loss: 0.00001895
Iteration 163/1000 | Loss: 0.00001895
Iteration 164/1000 | Loss: 0.00001894
Iteration 165/1000 | Loss: 0.00001894
Iteration 166/1000 | Loss: 0.00001894
Iteration 167/1000 | Loss: 0.00001894
Iteration 168/1000 | Loss: 0.00001894
Iteration 169/1000 | Loss: 0.00001894
Iteration 170/1000 | Loss: 0.00001894
Iteration 171/1000 | Loss: 0.00001894
Iteration 172/1000 | Loss: 0.00001894
Iteration 173/1000 | Loss: 0.00001894
Iteration 174/1000 | Loss: 0.00001894
Iteration 175/1000 | Loss: 0.00001894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.893791340989992e-05, 1.893791340989992e-05, 1.893791340989992e-05, 1.893791340989992e-05, 1.893791340989992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.893791340989992e-05

Optimization complete. Final v2v error: 3.7020318508148193 mm

Highest mean error: 5.13765287399292 mm for frame 65

Lowest mean error: 2.913774251937866 mm for frame 49

Saving results

Total time: 49.88646149635315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00680996
Iteration 2/25 | Loss: 0.00170581
Iteration 3/25 | Loss: 0.00141843
Iteration 4/25 | Loss: 0.00130760
Iteration 5/25 | Loss: 0.00128814
Iteration 6/25 | Loss: 0.00127966
Iteration 7/25 | Loss: 0.00129082
Iteration 8/25 | Loss: 0.00129907
Iteration 9/25 | Loss: 0.00127934
Iteration 10/25 | Loss: 0.00127116
Iteration 11/25 | Loss: 0.00126844
Iteration 12/25 | Loss: 0.00126647
Iteration 13/25 | Loss: 0.00126580
Iteration 14/25 | Loss: 0.00126562
Iteration 15/25 | Loss: 0.00126547
Iteration 16/25 | Loss: 0.00126537
Iteration 17/25 | Loss: 0.00126536
Iteration 18/25 | Loss: 0.00126536
Iteration 19/25 | Loss: 0.00126536
Iteration 20/25 | Loss: 0.00126536
Iteration 21/25 | Loss: 0.00126536
Iteration 22/25 | Loss: 0.00126536
Iteration 23/25 | Loss: 0.00126536
Iteration 24/25 | Loss: 0.00126536
Iteration 25/25 | Loss: 0.00126536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19170547
Iteration 2/25 | Loss: 0.00119880
Iteration 3/25 | Loss: 0.00119876
Iteration 4/25 | Loss: 0.00119875
Iteration 5/25 | Loss: 0.00119875
Iteration 6/25 | Loss: 0.00119875
Iteration 7/25 | Loss: 0.00119875
Iteration 8/25 | Loss: 0.00119875
Iteration 9/25 | Loss: 0.00119875
Iteration 10/25 | Loss: 0.00119875
Iteration 11/25 | Loss: 0.00119875
Iteration 12/25 | Loss: 0.00119875
Iteration 13/25 | Loss: 0.00119875
Iteration 14/25 | Loss: 0.00119875
Iteration 15/25 | Loss: 0.00119875
Iteration 16/25 | Loss: 0.00119875
Iteration 17/25 | Loss: 0.00119875
Iteration 18/25 | Loss: 0.00119875
Iteration 19/25 | Loss: 0.00119875
Iteration 20/25 | Loss: 0.00119875
Iteration 21/25 | Loss: 0.00119875
Iteration 22/25 | Loss: 0.00119875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001198751968331635, 0.001198751968331635, 0.001198751968331635, 0.001198751968331635, 0.001198751968331635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001198751968331635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119875
Iteration 2/1000 | Loss: 0.00003675
Iteration 3/1000 | Loss: 0.00002564
Iteration 4/1000 | Loss: 0.00002209
Iteration 5/1000 | Loss: 0.00002065
Iteration 6/1000 | Loss: 0.00002001
Iteration 7/1000 | Loss: 0.00001930
Iteration 8/1000 | Loss: 0.00001878
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00008693
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00001768
Iteration 15/1000 | Loss: 0.00001680
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001551
Iteration 22/1000 | Loss: 0.00001539
Iteration 23/1000 | Loss: 0.00001530
Iteration 24/1000 | Loss: 0.00001529
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001529
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001528
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001526
Iteration 39/1000 | Loss: 0.00001526
Iteration 40/1000 | Loss: 0.00001526
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001525
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001525
Iteration 56/1000 | Loss: 0.00001525
Iteration 57/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.5254432582878508e-05, 1.5254432582878508e-05, 1.5254432582878508e-05, 1.5254432582878508e-05, 1.5254432582878508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5254432582878508e-05

Optimization complete. Final v2v error: 3.271789073944092 mm

Highest mean error: 5.75805139541626 mm for frame 18

Lowest mean error: 2.8138206005096436 mm for frame 134

Saving results

Total time: 58.017006158828735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047611
Iteration 2/25 | Loss: 0.00200654
Iteration 3/25 | Loss: 0.00141856
Iteration 4/25 | Loss: 0.00135034
Iteration 5/25 | Loss: 0.00131612
Iteration 6/25 | Loss: 0.00124765
Iteration 7/25 | Loss: 0.00123900
Iteration 8/25 | Loss: 0.00121256
Iteration 9/25 | Loss: 0.00119460
Iteration 10/25 | Loss: 0.00119396
Iteration 11/25 | Loss: 0.00120274
Iteration 12/25 | Loss: 0.00117201
Iteration 13/25 | Loss: 0.00116921
Iteration 14/25 | Loss: 0.00116840
Iteration 15/25 | Loss: 0.00116793
Iteration 16/25 | Loss: 0.00116771
Iteration 17/25 | Loss: 0.00116758
Iteration 18/25 | Loss: 0.00116750
Iteration 19/25 | Loss: 0.00116743
Iteration 20/25 | Loss: 0.00116740
Iteration 21/25 | Loss: 0.00116740
Iteration 22/25 | Loss: 0.00116740
Iteration 23/25 | Loss: 0.00116739
Iteration 24/25 | Loss: 0.00116738
Iteration 25/25 | Loss: 0.00117033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25026202
Iteration 2/25 | Loss: 0.00139654
Iteration 3/25 | Loss: 0.00125867
Iteration 4/25 | Loss: 0.00125867
Iteration 5/25 | Loss: 0.00125867
Iteration 6/25 | Loss: 0.00125866
Iteration 7/25 | Loss: 0.00125866
Iteration 8/25 | Loss: 0.00125866
Iteration 9/25 | Loss: 0.00125866
Iteration 10/25 | Loss: 0.00125866
Iteration 11/25 | Loss: 0.00125866
Iteration 12/25 | Loss: 0.00125866
Iteration 13/25 | Loss: 0.00125866
Iteration 14/25 | Loss: 0.00125866
Iteration 15/25 | Loss: 0.00125866
Iteration 16/25 | Loss: 0.00125866
Iteration 17/25 | Loss: 0.00125866
Iteration 18/25 | Loss: 0.00125866
Iteration 19/25 | Loss: 0.00125866
Iteration 20/25 | Loss: 0.00125866
Iteration 21/25 | Loss: 0.00125866
Iteration 22/25 | Loss: 0.00125866
Iteration 23/25 | Loss: 0.00125866
Iteration 24/25 | Loss: 0.00125866
Iteration 25/25 | Loss: 0.00125866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125866
Iteration 2/1000 | Loss: 0.00005998
Iteration 3/1000 | Loss: 0.00004610
Iteration 4/1000 | Loss: 0.00003976
Iteration 5/1000 | Loss: 0.00003698
Iteration 6/1000 | Loss: 0.00003505
Iteration 7/1000 | Loss: 0.00003381
Iteration 8/1000 | Loss: 0.00003294
Iteration 9/1000 | Loss: 0.00003196
Iteration 10/1000 | Loss: 0.00003121
Iteration 11/1000 | Loss: 0.00003054
Iteration 12/1000 | Loss: 0.00002988
Iteration 13/1000 | Loss: 0.00002938
Iteration 14/1000 | Loss: 0.00110542
Iteration 15/1000 | Loss: 0.00304366
Iteration 16/1000 | Loss: 0.00023672
Iteration 17/1000 | Loss: 0.00496778
Iteration 18/1000 | Loss: 0.00264312
Iteration 19/1000 | Loss: 0.00265874
Iteration 20/1000 | Loss: 0.00225044
Iteration 21/1000 | Loss: 0.00201331
Iteration 22/1000 | Loss: 0.00207280
Iteration 23/1000 | Loss: 0.00375803
Iteration 24/1000 | Loss: 0.00261203
Iteration 25/1000 | Loss: 0.00085132
Iteration 26/1000 | Loss: 0.00035963
Iteration 27/1000 | Loss: 0.00005481
Iteration 28/1000 | Loss: 0.00094668
Iteration 29/1000 | Loss: 0.00042525
Iteration 30/1000 | Loss: 0.00022772
Iteration 31/1000 | Loss: 0.00065583
Iteration 32/1000 | Loss: 0.00081881
Iteration 33/1000 | Loss: 0.00101526
Iteration 34/1000 | Loss: 0.00069328
Iteration 35/1000 | Loss: 0.00124481
Iteration 36/1000 | Loss: 0.00042937
Iteration 37/1000 | Loss: 0.00071438
Iteration 38/1000 | Loss: 0.00127674
Iteration 39/1000 | Loss: 0.00032281
Iteration 40/1000 | Loss: 0.00098263
Iteration 41/1000 | Loss: 0.00078503
Iteration 42/1000 | Loss: 0.00062825
Iteration 43/1000 | Loss: 0.00097116
Iteration 44/1000 | Loss: 0.00028332
Iteration 45/1000 | Loss: 0.00065537
Iteration 46/1000 | Loss: 0.00050997
Iteration 47/1000 | Loss: 0.00067673
Iteration 48/1000 | Loss: 0.00110833
Iteration 49/1000 | Loss: 0.00146860
Iteration 50/1000 | Loss: 0.00204381
Iteration 51/1000 | Loss: 0.00067649
Iteration 52/1000 | Loss: 0.00048212
Iteration 53/1000 | Loss: 0.00042567
Iteration 54/1000 | Loss: 0.00007800
Iteration 55/1000 | Loss: 0.00008978
Iteration 56/1000 | Loss: 0.00022872
Iteration 57/1000 | Loss: 0.00004038
Iteration 58/1000 | Loss: 0.00007297
Iteration 59/1000 | Loss: 0.00008541
Iteration 60/1000 | Loss: 0.00014336
Iteration 61/1000 | Loss: 0.00028174
Iteration 62/1000 | Loss: 0.00017674
Iteration 63/1000 | Loss: 0.00043624
Iteration 64/1000 | Loss: 0.00003931
Iteration 65/1000 | Loss: 0.00054663
Iteration 66/1000 | Loss: 0.00064617
Iteration 67/1000 | Loss: 0.00029080
Iteration 68/1000 | Loss: 0.00041257
Iteration 69/1000 | Loss: 0.00037864
Iteration 70/1000 | Loss: 0.00056989
Iteration 71/1000 | Loss: 0.00019496
Iteration 72/1000 | Loss: 0.00015454
Iteration 73/1000 | Loss: 0.00049756
Iteration 74/1000 | Loss: 0.00051795
Iteration 75/1000 | Loss: 0.00056811
Iteration 76/1000 | Loss: 0.00027974
Iteration 77/1000 | Loss: 0.00049053
Iteration 78/1000 | Loss: 0.00022798
Iteration 79/1000 | Loss: 0.00003304
Iteration 80/1000 | Loss: 0.00003078
Iteration 81/1000 | Loss: 0.00004703
Iteration 82/1000 | Loss: 0.00003336
Iteration 83/1000 | Loss: 0.00005242
Iteration 84/1000 | Loss: 0.00003475
Iteration 85/1000 | Loss: 0.00051379
Iteration 86/1000 | Loss: 0.00013180
Iteration 87/1000 | Loss: 0.00027748
Iteration 88/1000 | Loss: 0.00011587
Iteration 89/1000 | Loss: 0.00046567
Iteration 90/1000 | Loss: 0.00030800
Iteration 91/1000 | Loss: 0.00003409
Iteration 92/1000 | Loss: 0.00003421
Iteration 93/1000 | Loss: 0.00003239
Iteration 94/1000 | Loss: 0.00003771
Iteration 95/1000 | Loss: 0.00011476
Iteration 96/1000 | Loss: 0.00007489
Iteration 97/1000 | Loss: 0.00004116
Iteration 98/1000 | Loss: 0.00014933
Iteration 99/1000 | Loss: 0.00002758
Iteration 100/1000 | Loss: 0.00002653
Iteration 101/1000 | Loss: 0.00002616
Iteration 102/1000 | Loss: 0.00002586
Iteration 103/1000 | Loss: 0.00002572
Iteration 104/1000 | Loss: 0.00002534
Iteration 105/1000 | Loss: 0.00031793
Iteration 106/1000 | Loss: 0.00044310
Iteration 107/1000 | Loss: 0.00112841
Iteration 108/1000 | Loss: 0.00063031
Iteration 109/1000 | Loss: 0.00117021
Iteration 110/1000 | Loss: 0.00060197
Iteration 111/1000 | Loss: 0.00108637
Iteration 112/1000 | Loss: 0.00048054
Iteration 113/1000 | Loss: 0.00183077
Iteration 114/1000 | Loss: 0.00087769
Iteration 115/1000 | Loss: 0.00067595
Iteration 116/1000 | Loss: 0.00127399
Iteration 117/1000 | Loss: 0.00073269
Iteration 118/1000 | Loss: 0.00121761
Iteration 119/1000 | Loss: 0.00081420
Iteration 120/1000 | Loss: 0.00046403
Iteration 121/1000 | Loss: 0.00046821
Iteration 122/1000 | Loss: 0.00056158
Iteration 123/1000 | Loss: 0.00037853
Iteration 124/1000 | Loss: 0.00035382
Iteration 125/1000 | Loss: 0.00023342
Iteration 126/1000 | Loss: 0.00039825
Iteration 127/1000 | Loss: 0.00046299
Iteration 128/1000 | Loss: 0.00084713
Iteration 129/1000 | Loss: 0.00058020
Iteration 130/1000 | Loss: 0.00056407
Iteration 131/1000 | Loss: 0.00063032
Iteration 132/1000 | Loss: 0.00049772
Iteration 133/1000 | Loss: 0.00022533
Iteration 134/1000 | Loss: 0.00021816
Iteration 135/1000 | Loss: 0.00024188
Iteration 136/1000 | Loss: 0.00022206
Iteration 137/1000 | Loss: 0.00022217
Iteration 138/1000 | Loss: 0.00017884
Iteration 139/1000 | Loss: 0.00005424
Iteration 140/1000 | Loss: 0.00003584
Iteration 141/1000 | Loss: 0.00002942
Iteration 142/1000 | Loss: 0.00006392
Iteration 143/1000 | Loss: 0.00002428
Iteration 144/1000 | Loss: 0.00002257
Iteration 145/1000 | Loss: 0.00002145
Iteration 146/1000 | Loss: 0.00002082
Iteration 147/1000 | Loss: 0.00002019
Iteration 148/1000 | Loss: 0.00001976
Iteration 149/1000 | Loss: 0.00001940
Iteration 150/1000 | Loss: 0.00001915
Iteration 151/1000 | Loss: 0.00001903
Iteration 152/1000 | Loss: 0.00001896
Iteration 153/1000 | Loss: 0.00001896
Iteration 154/1000 | Loss: 0.00001895
Iteration 155/1000 | Loss: 0.00001893
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001891
Iteration 158/1000 | Loss: 0.00001886
Iteration 159/1000 | Loss: 0.00001886
Iteration 160/1000 | Loss: 0.00001884
Iteration 161/1000 | Loss: 0.00001883
Iteration 162/1000 | Loss: 0.00001883
Iteration 163/1000 | Loss: 0.00001882
Iteration 164/1000 | Loss: 0.00001882
Iteration 165/1000 | Loss: 0.00001881
Iteration 166/1000 | Loss: 0.00001881
Iteration 167/1000 | Loss: 0.00001881
Iteration 168/1000 | Loss: 0.00001881
Iteration 169/1000 | Loss: 0.00001880
Iteration 170/1000 | Loss: 0.00001880
Iteration 171/1000 | Loss: 0.00001880
Iteration 172/1000 | Loss: 0.00001880
Iteration 173/1000 | Loss: 0.00001879
Iteration 174/1000 | Loss: 0.00001879
Iteration 175/1000 | Loss: 0.00001879
Iteration 176/1000 | Loss: 0.00001879
Iteration 177/1000 | Loss: 0.00001879
Iteration 178/1000 | Loss: 0.00001878
Iteration 179/1000 | Loss: 0.00001878
Iteration 180/1000 | Loss: 0.00001878
Iteration 181/1000 | Loss: 0.00001878
Iteration 182/1000 | Loss: 0.00001877
Iteration 183/1000 | Loss: 0.00001877
Iteration 184/1000 | Loss: 0.00001876
Iteration 185/1000 | Loss: 0.00001876
Iteration 186/1000 | Loss: 0.00001876
Iteration 187/1000 | Loss: 0.00001875
Iteration 188/1000 | Loss: 0.00001875
Iteration 189/1000 | Loss: 0.00001874
Iteration 190/1000 | Loss: 0.00001874
Iteration 191/1000 | Loss: 0.00001874
Iteration 192/1000 | Loss: 0.00001873
Iteration 193/1000 | Loss: 0.00001873
Iteration 194/1000 | Loss: 0.00001873
Iteration 195/1000 | Loss: 0.00001873
Iteration 196/1000 | Loss: 0.00001872
Iteration 197/1000 | Loss: 0.00001872
Iteration 198/1000 | Loss: 0.00001872
Iteration 199/1000 | Loss: 0.00001872
Iteration 200/1000 | Loss: 0.00001872
Iteration 201/1000 | Loss: 0.00001872
Iteration 202/1000 | Loss: 0.00001872
Iteration 203/1000 | Loss: 0.00001872
Iteration 204/1000 | Loss: 0.00001872
Iteration 205/1000 | Loss: 0.00001871
Iteration 206/1000 | Loss: 0.00001871
Iteration 207/1000 | Loss: 0.00001871
Iteration 208/1000 | Loss: 0.00001871
Iteration 209/1000 | Loss: 0.00001870
Iteration 210/1000 | Loss: 0.00001870
Iteration 211/1000 | Loss: 0.00001869
Iteration 212/1000 | Loss: 0.00001869
Iteration 213/1000 | Loss: 0.00001869
Iteration 214/1000 | Loss: 0.00001869
Iteration 215/1000 | Loss: 0.00001869
Iteration 216/1000 | Loss: 0.00001869
Iteration 217/1000 | Loss: 0.00001869
Iteration 218/1000 | Loss: 0.00001869
Iteration 219/1000 | Loss: 0.00001868
Iteration 220/1000 | Loss: 0.00001868
Iteration 221/1000 | Loss: 0.00001868
Iteration 222/1000 | Loss: 0.00001868
Iteration 223/1000 | Loss: 0.00001868
Iteration 224/1000 | Loss: 0.00001868
Iteration 225/1000 | Loss: 0.00001868
Iteration 226/1000 | Loss: 0.00001868
Iteration 227/1000 | Loss: 0.00001867
Iteration 228/1000 | Loss: 0.00001867
Iteration 229/1000 | Loss: 0.00001866
Iteration 230/1000 | Loss: 0.00001866
Iteration 231/1000 | Loss: 0.00001866
Iteration 232/1000 | Loss: 0.00001866
Iteration 233/1000 | Loss: 0.00001866
Iteration 234/1000 | Loss: 0.00001865
Iteration 235/1000 | Loss: 0.00001865
Iteration 236/1000 | Loss: 0.00001865
Iteration 237/1000 | Loss: 0.00001865
Iteration 238/1000 | Loss: 0.00001865
Iteration 239/1000 | Loss: 0.00001865
Iteration 240/1000 | Loss: 0.00001865
Iteration 241/1000 | Loss: 0.00001865
Iteration 242/1000 | Loss: 0.00001865
Iteration 243/1000 | Loss: 0.00001865
Iteration 244/1000 | Loss: 0.00001865
Iteration 245/1000 | Loss: 0.00001865
Iteration 246/1000 | Loss: 0.00001865
Iteration 247/1000 | Loss: 0.00001865
Iteration 248/1000 | Loss: 0.00001865
Iteration 249/1000 | Loss: 0.00001865
Iteration 250/1000 | Loss: 0.00001865
Iteration 251/1000 | Loss: 0.00001865
Iteration 252/1000 | Loss: 0.00001865
Iteration 253/1000 | Loss: 0.00001865
Iteration 254/1000 | Loss: 0.00001865
Iteration 255/1000 | Loss: 0.00001865
Iteration 256/1000 | Loss: 0.00001865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.8646136595634744e-05, 1.8646136595634744e-05, 1.8646136595634744e-05, 1.8646136595634744e-05, 1.8646136595634744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8646136595634744e-05

Optimization complete. Final v2v error: 2.7155017852783203 mm

Highest mean error: 12.105321884155273 mm for frame 3

Lowest mean error: 2.451597213745117 mm for frame 0

Saving results

Total time: 250.84793782234192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475876
Iteration 2/25 | Loss: 0.00129667
Iteration 3/25 | Loss: 0.00118806
Iteration 4/25 | Loss: 0.00117920
Iteration 5/25 | Loss: 0.00117574
Iteration 6/25 | Loss: 0.00117574
Iteration 7/25 | Loss: 0.00117574
Iteration 8/25 | Loss: 0.00117574
Iteration 9/25 | Loss: 0.00117574
Iteration 10/25 | Loss: 0.00117574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011757432948797941, 0.0011757432948797941, 0.0011757432948797941, 0.0011757432948797941, 0.0011757432948797941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011757432948797941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81137133
Iteration 2/25 | Loss: 0.00106540
Iteration 3/25 | Loss: 0.00106540
Iteration 4/25 | Loss: 0.00106540
Iteration 5/25 | Loss: 0.00106540
Iteration 6/25 | Loss: 0.00106540
Iteration 7/25 | Loss: 0.00106540
Iteration 8/25 | Loss: 0.00106540
Iteration 9/25 | Loss: 0.00106540
Iteration 10/25 | Loss: 0.00106539
Iteration 11/25 | Loss: 0.00106539
Iteration 12/25 | Loss: 0.00106539
Iteration 13/25 | Loss: 0.00106539
Iteration 14/25 | Loss: 0.00106539
Iteration 15/25 | Loss: 0.00106539
Iteration 16/25 | Loss: 0.00106539
Iteration 17/25 | Loss: 0.00106539
Iteration 18/25 | Loss: 0.00106539
Iteration 19/25 | Loss: 0.00106539
Iteration 20/25 | Loss: 0.00106539
Iteration 21/25 | Loss: 0.00106539
Iteration 22/25 | Loss: 0.00106539
Iteration 23/25 | Loss: 0.00106539
Iteration 24/25 | Loss: 0.00106539
Iteration 25/25 | Loss: 0.00106539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010653940262272954, 0.0010653940262272954, 0.0010653940262272954, 0.0010653940262272954, 0.0010653940262272954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010653940262272954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106539
Iteration 2/1000 | Loss: 0.00002799
Iteration 3/1000 | Loss: 0.00001976
Iteration 4/1000 | Loss: 0.00001801
Iteration 5/1000 | Loss: 0.00001667
Iteration 6/1000 | Loss: 0.00001576
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001485
Iteration 9/1000 | Loss: 0.00001446
Iteration 10/1000 | Loss: 0.00001420
Iteration 11/1000 | Loss: 0.00001397
Iteration 12/1000 | Loss: 0.00001393
Iteration 13/1000 | Loss: 0.00001379
Iteration 14/1000 | Loss: 0.00001373
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001344
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001329
Iteration 21/1000 | Loss: 0.00001327
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001323
Iteration 29/1000 | Loss: 0.00001322
Iteration 30/1000 | Loss: 0.00001319
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001299
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001296
Iteration 49/1000 | Loss: 0.00001296
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001295
Iteration 53/1000 | Loss: 0.00001295
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001292
Iteration 65/1000 | Loss: 0.00001292
Iteration 66/1000 | Loss: 0.00001292
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001288
Iteration 80/1000 | Loss: 0.00001288
Iteration 81/1000 | Loss: 0.00001287
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001286
Iteration 93/1000 | Loss: 0.00001286
Iteration 94/1000 | Loss: 0.00001286
Iteration 95/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.2859150956501253e-05, 1.2859150956501253e-05, 1.2859150956501253e-05, 1.2859150956501253e-05, 1.2859150956501253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2859150956501253e-05

Optimization complete. Final v2v error: 3.050830364227295 mm

Highest mean error: 3.4218077659606934 mm for frame 0

Lowest mean error: 2.874528646469116 mm for frame 31

Saving results

Total time: 44.7238392829895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432668
Iteration 2/25 | Loss: 0.00124604
Iteration 3/25 | Loss: 0.00116107
Iteration 4/25 | Loss: 0.00114757
Iteration 5/25 | Loss: 0.00114312
Iteration 6/25 | Loss: 0.00114214
Iteration 7/25 | Loss: 0.00114214
Iteration 8/25 | Loss: 0.00114214
Iteration 9/25 | Loss: 0.00114214
Iteration 10/25 | Loss: 0.00114214
Iteration 11/25 | Loss: 0.00114214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011421405943110585, 0.0011421405943110585, 0.0011421405943110585, 0.0011421405943110585, 0.0011421405943110585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011421405943110585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.53204012
Iteration 2/25 | Loss: 0.00127969
Iteration 3/25 | Loss: 0.00127967
Iteration 4/25 | Loss: 0.00127967
Iteration 5/25 | Loss: 0.00127967
Iteration 6/25 | Loss: 0.00127967
Iteration 7/25 | Loss: 0.00127967
Iteration 8/25 | Loss: 0.00127967
Iteration 9/25 | Loss: 0.00127967
Iteration 10/25 | Loss: 0.00127967
Iteration 11/25 | Loss: 0.00127967
Iteration 12/25 | Loss: 0.00127967
Iteration 13/25 | Loss: 0.00127967
Iteration 14/25 | Loss: 0.00127967
Iteration 15/25 | Loss: 0.00127967
Iteration 16/25 | Loss: 0.00127967
Iteration 17/25 | Loss: 0.00127967
Iteration 18/25 | Loss: 0.00127967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012796680675819516, 0.0012796680675819516, 0.0012796680675819516, 0.0012796680675819516, 0.0012796680675819516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012796680675819516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127967
Iteration 2/1000 | Loss: 0.00002172
Iteration 3/1000 | Loss: 0.00001571
Iteration 4/1000 | Loss: 0.00001444
Iteration 5/1000 | Loss: 0.00001352
Iteration 6/1000 | Loss: 0.00001298
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001182
Iteration 10/1000 | Loss: 0.00001160
Iteration 11/1000 | Loss: 0.00001149
Iteration 12/1000 | Loss: 0.00001149
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001122
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001115
Iteration 20/1000 | Loss: 0.00001114
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001108
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001101
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001081
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001075
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001074
Iteration 42/1000 | Loss: 0.00001072
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001071
Iteration 47/1000 | Loss: 0.00001069
Iteration 48/1000 | Loss: 0.00001068
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001067
Iteration 51/1000 | Loss: 0.00001067
Iteration 52/1000 | Loss: 0.00001067
Iteration 53/1000 | Loss: 0.00001067
Iteration 54/1000 | Loss: 0.00001067
Iteration 55/1000 | Loss: 0.00001067
Iteration 56/1000 | Loss: 0.00001067
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001061
Iteration 79/1000 | Loss: 0.00001061
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001060
Iteration 84/1000 | Loss: 0.00001060
Iteration 85/1000 | Loss: 0.00001059
Iteration 86/1000 | Loss: 0.00001058
Iteration 87/1000 | Loss: 0.00001058
Iteration 88/1000 | Loss: 0.00001058
Iteration 89/1000 | Loss: 0.00001058
Iteration 90/1000 | Loss: 0.00001058
Iteration 91/1000 | Loss: 0.00001058
Iteration 92/1000 | Loss: 0.00001058
Iteration 93/1000 | Loss: 0.00001057
Iteration 94/1000 | Loss: 0.00001057
Iteration 95/1000 | Loss: 0.00001057
Iteration 96/1000 | Loss: 0.00001056
Iteration 97/1000 | Loss: 0.00001056
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001055
Iteration 103/1000 | Loss: 0.00001055
Iteration 104/1000 | Loss: 0.00001055
Iteration 105/1000 | Loss: 0.00001055
Iteration 106/1000 | Loss: 0.00001055
Iteration 107/1000 | Loss: 0.00001055
Iteration 108/1000 | Loss: 0.00001055
Iteration 109/1000 | Loss: 0.00001055
Iteration 110/1000 | Loss: 0.00001055
Iteration 111/1000 | Loss: 0.00001055
Iteration 112/1000 | Loss: 0.00001055
Iteration 113/1000 | Loss: 0.00001054
Iteration 114/1000 | Loss: 0.00001054
Iteration 115/1000 | Loss: 0.00001054
Iteration 116/1000 | Loss: 0.00001054
Iteration 117/1000 | Loss: 0.00001054
Iteration 118/1000 | Loss: 0.00001054
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001054
Iteration 131/1000 | Loss: 0.00001054
Iteration 132/1000 | Loss: 0.00001054
Iteration 133/1000 | Loss: 0.00001054
Iteration 134/1000 | Loss: 0.00001054
Iteration 135/1000 | Loss: 0.00001054
Iteration 136/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.0539124559727497e-05, 1.0539124559727497e-05, 1.0539124559727497e-05, 1.0539124559727497e-05, 1.0539124559727497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0539124559727497e-05

Optimization complete. Final v2v error: 2.8115322589874268 mm

Highest mean error: 3.444716453552246 mm for frame 92

Lowest mean error: 2.506157159805298 mm for frame 204

Saving results

Total time: 43.35226583480835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00588691
Iteration 2/25 | Loss: 0.00120678
Iteration 3/25 | Loss: 0.00115528
Iteration 4/25 | Loss: 0.00114750
Iteration 5/25 | Loss: 0.00114540
Iteration 6/25 | Loss: 0.00114540
Iteration 7/25 | Loss: 0.00114540
Iteration 8/25 | Loss: 0.00114540
Iteration 9/25 | Loss: 0.00114540
Iteration 10/25 | Loss: 0.00114540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011453988263383508, 0.0011453988263383508, 0.0011453988263383508, 0.0011453988263383508, 0.0011453988263383508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011453988263383508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06237078
Iteration 2/25 | Loss: 0.00114250
Iteration 3/25 | Loss: 0.00114250
Iteration 4/25 | Loss: 0.00114250
Iteration 5/25 | Loss: 0.00114250
Iteration 6/25 | Loss: 0.00114250
Iteration 7/25 | Loss: 0.00114250
Iteration 8/25 | Loss: 0.00114250
Iteration 9/25 | Loss: 0.00114250
Iteration 10/25 | Loss: 0.00114250
Iteration 11/25 | Loss: 0.00114250
Iteration 12/25 | Loss: 0.00114250
Iteration 13/25 | Loss: 0.00114250
Iteration 14/25 | Loss: 0.00114250
Iteration 15/25 | Loss: 0.00114250
Iteration 16/25 | Loss: 0.00114250
Iteration 17/25 | Loss: 0.00114250
Iteration 18/25 | Loss: 0.00114250
Iteration 19/25 | Loss: 0.00114250
Iteration 20/25 | Loss: 0.00114250
Iteration 21/25 | Loss: 0.00114250
Iteration 22/25 | Loss: 0.00114250
Iteration 23/25 | Loss: 0.00114250
Iteration 24/25 | Loss: 0.00114250
Iteration 25/25 | Loss: 0.00114250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114250
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001533
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001218
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001208
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001207
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001206
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001201
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001196
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001178
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001176
Iteration 92/1000 | Loss: 0.00001176
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001173
Iteration 109/1000 | Loss: 0.00001173
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.1718374480551574e-05, 1.1718374480551574e-05, 1.1718374480551574e-05, 1.1718374480551574e-05, 1.1718374480551574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1718374480551574e-05

Optimization complete. Final v2v error: 2.968820095062256 mm

Highest mean error: 3.2278079986572266 mm for frame 141

Lowest mean error: 2.797470808029175 mm for frame 43

Saving results

Total time: 40.12642431259155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_antonia_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_antonia_posed_003/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026012
Iteration 2/25 | Loss: 0.00280873
Iteration 3/25 | Loss: 0.00206779
Iteration 4/25 | Loss: 0.00197132
Iteration 5/25 | Loss: 0.00195598
Iteration 6/25 | Loss: 0.00188415
Iteration 7/25 | Loss: 0.00162801
Iteration 8/25 | Loss: 0.00150710
Iteration 9/25 | Loss: 0.00151213
Iteration 10/25 | Loss: 0.00143489
Iteration 11/25 | Loss: 0.00142589
Iteration 12/25 | Loss: 0.00141473
Iteration 13/25 | Loss: 0.00138821
Iteration 14/25 | Loss: 0.00134621
Iteration 15/25 | Loss: 0.00133440
Iteration 16/25 | Loss: 0.00132701
Iteration 17/25 | Loss: 0.00132281
Iteration 18/25 | Loss: 0.00132558
Iteration 19/25 | Loss: 0.00132736
Iteration 20/25 | Loss: 0.00132445
Iteration 21/25 | Loss: 0.00132711
Iteration 22/25 | Loss: 0.00132590
Iteration 23/25 | Loss: 0.00132661
Iteration 24/25 | Loss: 0.00132539
Iteration 25/25 | Loss: 0.00132834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28364813
Iteration 2/25 | Loss: 0.00303347
Iteration 3/25 | Loss: 0.00202314
Iteration 4/25 | Loss: 0.00202313
Iteration 5/25 | Loss: 0.00202313
Iteration 6/25 | Loss: 0.00202313
Iteration 7/25 | Loss: 0.00202313
Iteration 8/25 | Loss: 0.00202313
Iteration 9/25 | Loss: 0.00202313
Iteration 10/25 | Loss: 0.00202313
Iteration 11/25 | Loss: 0.00202313
Iteration 12/25 | Loss: 0.00202313
Iteration 13/25 | Loss: 0.00202313
Iteration 14/25 | Loss: 0.00202313
Iteration 15/25 | Loss: 0.00202313
Iteration 16/25 | Loss: 0.00202313
Iteration 17/25 | Loss: 0.00202313
Iteration 18/25 | Loss: 0.00202313
Iteration 19/25 | Loss: 0.00202313
Iteration 20/25 | Loss: 0.00202313
Iteration 21/25 | Loss: 0.00202313
Iteration 22/25 | Loss: 0.00202313
Iteration 23/25 | Loss: 0.00202313
Iteration 24/25 | Loss: 0.00202313
Iteration 25/25 | Loss: 0.00202313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202313
Iteration 2/1000 | Loss: 0.00044802
Iteration 3/1000 | Loss: 0.00112573
Iteration 4/1000 | Loss: 0.00061077
Iteration 5/1000 | Loss: 0.00038252
Iteration 6/1000 | Loss: 0.00012560
Iteration 7/1000 | Loss: 0.00048684
Iteration 8/1000 | Loss: 0.00057879
Iteration 9/1000 | Loss: 0.00020412
Iteration 10/1000 | Loss: 0.00010576
Iteration 11/1000 | Loss: 0.00025584
Iteration 12/1000 | Loss: 0.00010196
Iteration 13/1000 | Loss: 0.00009722
Iteration 14/1000 | Loss: 0.00009752
Iteration 15/1000 | Loss: 0.00023844
Iteration 16/1000 | Loss: 0.00060813
Iteration 17/1000 | Loss: 0.00031473
Iteration 18/1000 | Loss: 0.00018998
Iteration 19/1000 | Loss: 0.00022341
Iteration 20/1000 | Loss: 0.00009528
Iteration 21/1000 | Loss: 0.00008878
Iteration 22/1000 | Loss: 0.00100281
Iteration 23/1000 | Loss: 0.01029810
Iteration 24/1000 | Loss: 0.00570985
Iteration 25/1000 | Loss: 0.00106796
Iteration 26/1000 | Loss: 0.00202234
Iteration 27/1000 | Loss: 0.00054519
Iteration 28/1000 | Loss: 0.00021651
Iteration 29/1000 | Loss: 0.00023142
Iteration 30/1000 | Loss: 0.00098851
Iteration 31/1000 | Loss: 0.00006640
Iteration 32/1000 | Loss: 0.00019095
Iteration 33/1000 | Loss: 0.00004765
Iteration 34/1000 | Loss: 0.00012969
Iteration 35/1000 | Loss: 0.00004108
Iteration 36/1000 | Loss: 0.00003959
Iteration 37/1000 | Loss: 0.00010074
Iteration 38/1000 | Loss: 0.00060825
Iteration 39/1000 | Loss: 0.00010793
Iteration 40/1000 | Loss: 0.00016734
Iteration 41/1000 | Loss: 0.00008069
Iteration 42/1000 | Loss: 0.00031878
Iteration 43/1000 | Loss: 0.00009190
Iteration 44/1000 | Loss: 0.00019823
Iteration 45/1000 | Loss: 0.00050316
Iteration 46/1000 | Loss: 0.00006746
Iteration 47/1000 | Loss: 0.00015751
Iteration 48/1000 | Loss: 0.00143626
Iteration 49/1000 | Loss: 0.00005848
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00009673
Iteration 52/1000 | Loss: 0.00007754
Iteration 53/1000 | Loss: 0.00011018
Iteration 54/1000 | Loss: 0.00003613
Iteration 55/1000 | Loss: 0.00004744
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00002015
Iteration 58/1000 | Loss: 0.00007075
Iteration 59/1000 | Loss: 0.00001043
Iteration 60/1000 | Loss: 0.00001799
Iteration 61/1000 | Loss: 0.00000947
Iteration 62/1000 | Loss: 0.00002058
Iteration 63/1000 | Loss: 0.00009607
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00003336
Iteration 66/1000 | Loss: 0.00000895
Iteration 67/1000 | Loss: 0.00000883
Iteration 68/1000 | Loss: 0.00000867
Iteration 69/1000 | Loss: 0.00000865
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000862
Iteration 72/1000 | Loss: 0.00000860
Iteration 73/1000 | Loss: 0.00000860
Iteration 74/1000 | Loss: 0.00000859
Iteration 75/1000 | Loss: 0.00000859
Iteration 76/1000 | Loss: 0.00000858
Iteration 77/1000 | Loss: 0.00000858
Iteration 78/1000 | Loss: 0.00001967
Iteration 79/1000 | Loss: 0.00001106
Iteration 80/1000 | Loss: 0.00000850
Iteration 81/1000 | Loss: 0.00000850
Iteration 82/1000 | Loss: 0.00000849
Iteration 83/1000 | Loss: 0.00000849
Iteration 84/1000 | Loss: 0.00000848
Iteration 85/1000 | Loss: 0.00001283
Iteration 86/1000 | Loss: 0.00000844
Iteration 87/1000 | Loss: 0.00000844
Iteration 88/1000 | Loss: 0.00000844
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000843
Iteration 91/1000 | Loss: 0.00000843
Iteration 92/1000 | Loss: 0.00000843
Iteration 93/1000 | Loss: 0.00000843
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000843
Iteration 97/1000 | Loss: 0.00000843
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000841
Iteration 101/1000 | Loss: 0.00000841
Iteration 102/1000 | Loss: 0.00001965
Iteration 103/1000 | Loss: 0.00004519
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001508
Iteration 106/1000 | Loss: 0.00000857
Iteration 107/1000 | Loss: 0.00000834
Iteration 108/1000 | Loss: 0.00000834
Iteration 109/1000 | Loss: 0.00000834
Iteration 110/1000 | Loss: 0.00000834
Iteration 111/1000 | Loss: 0.00000834
Iteration 112/1000 | Loss: 0.00000834
Iteration 113/1000 | Loss: 0.00000833
Iteration 114/1000 | Loss: 0.00000833
Iteration 115/1000 | Loss: 0.00000833
Iteration 116/1000 | Loss: 0.00000833
Iteration 117/1000 | Loss: 0.00000833
Iteration 118/1000 | Loss: 0.00000833
Iteration 119/1000 | Loss: 0.00000833
Iteration 120/1000 | Loss: 0.00000833
Iteration 121/1000 | Loss: 0.00000833
Iteration 122/1000 | Loss: 0.00000832
Iteration 123/1000 | Loss: 0.00000832
Iteration 124/1000 | Loss: 0.00000832
Iteration 125/1000 | Loss: 0.00000832
Iteration 126/1000 | Loss: 0.00000832
Iteration 127/1000 | Loss: 0.00000832
Iteration 128/1000 | Loss: 0.00000832
Iteration 129/1000 | Loss: 0.00000832
Iteration 130/1000 | Loss: 0.00000832
Iteration 131/1000 | Loss: 0.00000832
Iteration 132/1000 | Loss: 0.00000832
Iteration 133/1000 | Loss: 0.00000831
Iteration 134/1000 | Loss: 0.00000831
Iteration 135/1000 | Loss: 0.00000831
Iteration 136/1000 | Loss: 0.00000831
Iteration 137/1000 | Loss: 0.00000831
Iteration 138/1000 | Loss: 0.00000831
Iteration 139/1000 | Loss: 0.00000831
Iteration 140/1000 | Loss: 0.00000831
Iteration 141/1000 | Loss: 0.00000831
Iteration 142/1000 | Loss: 0.00000831
Iteration 143/1000 | Loss: 0.00000831
Iteration 144/1000 | Loss: 0.00000831
Iteration 145/1000 | Loss: 0.00000831
Iteration 146/1000 | Loss: 0.00000831
Iteration 147/1000 | Loss: 0.00000831
Iteration 148/1000 | Loss: 0.00000831
Iteration 149/1000 | Loss: 0.00000831
Iteration 150/1000 | Loss: 0.00000831
Iteration 151/1000 | Loss: 0.00000831
Iteration 152/1000 | Loss: 0.00000831
Iteration 153/1000 | Loss: 0.00000831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [8.313701073348057e-06, 8.313701073348057e-06, 8.313701073348057e-06, 8.313701073348057e-06, 8.313701073348057e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.313701073348057e-06

Optimization complete. Final v2v error: 2.494971990585327 mm

Highest mean error: 2.9349284172058105 mm for frame 126

Lowest mean error: 2.399756908416748 mm for frame 115

Saving results

Total time: 154.4819552898407
