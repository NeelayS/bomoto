Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=109, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6104-6159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372561
Iteration 2/25 | Loss: 0.00125990
Iteration 3/25 | Loss: 0.00110846
Iteration 4/25 | Loss: 0.00108022
Iteration 5/25 | Loss: 0.00107228
Iteration 6/25 | Loss: 0.00107017
Iteration 7/25 | Loss: 0.00106973
Iteration 8/25 | Loss: 0.00106973
Iteration 9/25 | Loss: 0.00106973
Iteration 10/25 | Loss: 0.00106973
Iteration 11/25 | Loss: 0.00106973
Iteration 12/25 | Loss: 0.00106973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010697287507355213, 0.0010697287507355213, 0.0010697287507355213, 0.0010697287507355213, 0.0010697287507355213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010697287507355213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30167079
Iteration 2/25 | Loss: 0.00087547
Iteration 3/25 | Loss: 0.00087547
Iteration 4/25 | Loss: 0.00087547
Iteration 5/25 | Loss: 0.00087546
Iteration 6/25 | Loss: 0.00087546
Iteration 7/25 | Loss: 0.00087546
Iteration 8/25 | Loss: 0.00087546
Iteration 9/25 | Loss: 0.00087546
Iteration 10/25 | Loss: 0.00087546
Iteration 11/25 | Loss: 0.00087546
Iteration 12/25 | Loss: 0.00087546
Iteration 13/25 | Loss: 0.00087546
Iteration 14/25 | Loss: 0.00087546
Iteration 15/25 | Loss: 0.00087546
Iteration 16/25 | Loss: 0.00087546
Iteration 17/25 | Loss: 0.00087546
Iteration 18/25 | Loss: 0.00087546
Iteration 19/25 | Loss: 0.00087546
Iteration 20/25 | Loss: 0.00087546
Iteration 21/25 | Loss: 0.00087546
Iteration 22/25 | Loss: 0.00087546
Iteration 23/25 | Loss: 0.00087546
Iteration 24/25 | Loss: 0.00087546
Iteration 25/25 | Loss: 0.00087546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087546
Iteration 2/1000 | Loss: 0.00004901
Iteration 3/1000 | Loss: 0.00003278
Iteration 4/1000 | Loss: 0.00002498
Iteration 5/1000 | Loss: 0.00002201
Iteration 6/1000 | Loss: 0.00002034
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001800
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001637
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001612
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001607
Iteration 20/1000 | Loss: 0.00001604
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001599
Iteration 24/1000 | Loss: 0.00001595
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001591
Iteration 27/1000 | Loss: 0.00001590
Iteration 28/1000 | Loss: 0.00001590
Iteration 29/1000 | Loss: 0.00001589
Iteration 30/1000 | Loss: 0.00001589
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001587
Iteration 36/1000 | Loss: 0.00001586
Iteration 37/1000 | Loss: 0.00001585
Iteration 38/1000 | Loss: 0.00001584
Iteration 39/1000 | Loss: 0.00001584
Iteration 40/1000 | Loss: 0.00001583
Iteration 41/1000 | Loss: 0.00001583
Iteration 42/1000 | Loss: 0.00001582
Iteration 43/1000 | Loss: 0.00001582
Iteration 44/1000 | Loss: 0.00001582
Iteration 45/1000 | Loss: 0.00001581
Iteration 46/1000 | Loss: 0.00001581
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001579
Iteration 49/1000 | Loss: 0.00001579
Iteration 50/1000 | Loss: 0.00001579
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001578
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001577
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001575
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.5707884813309647e-05, 1.5707884813309647e-05, 1.5707884813309647e-05, 1.5707884813309647e-05, 1.5707884813309647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5707884813309647e-05

Optimization complete. Final v2v error: 3.310119152069092 mm

Highest mean error: 3.8838822841644287 mm for frame 93

Lowest mean error: 2.632075071334839 mm for frame 11

Saving results

Total time: 40.45360851287842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375935
Iteration 2/25 | Loss: 0.00124835
Iteration 3/25 | Loss: 0.00109749
Iteration 4/25 | Loss: 0.00108440
Iteration 5/25 | Loss: 0.00108106
Iteration 6/25 | Loss: 0.00108087
Iteration 7/25 | Loss: 0.00108087
Iteration 8/25 | Loss: 0.00108087
Iteration 9/25 | Loss: 0.00108087
Iteration 10/25 | Loss: 0.00108087
Iteration 11/25 | Loss: 0.00108087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010808684164658189, 0.0010808684164658189, 0.0010808684164658189, 0.0010808684164658189, 0.0010808684164658189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010808684164658189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32751787
Iteration 2/25 | Loss: 0.00086702
Iteration 3/25 | Loss: 0.00086702
Iteration 4/25 | Loss: 0.00086701
Iteration 5/25 | Loss: 0.00086701
Iteration 6/25 | Loss: 0.00086701
Iteration 7/25 | Loss: 0.00086701
Iteration 8/25 | Loss: 0.00086701
Iteration 9/25 | Loss: 0.00086701
Iteration 10/25 | Loss: 0.00086701
Iteration 11/25 | Loss: 0.00086701
Iteration 12/25 | Loss: 0.00086701
Iteration 13/25 | Loss: 0.00086701
Iteration 14/25 | Loss: 0.00086701
Iteration 15/25 | Loss: 0.00086701
Iteration 16/25 | Loss: 0.00086701
Iteration 17/25 | Loss: 0.00086701
Iteration 18/25 | Loss: 0.00086701
Iteration 19/25 | Loss: 0.00086701
Iteration 20/25 | Loss: 0.00086701
Iteration 21/25 | Loss: 0.00086701
Iteration 22/25 | Loss: 0.00086701
Iteration 23/25 | Loss: 0.00086701
Iteration 24/25 | Loss: 0.00086701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008670126553624868, 0.0008670126553624868, 0.0008670126553624868, 0.0008670126553624868, 0.0008670126553624868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008670126553624868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086701
Iteration 2/1000 | Loss: 0.00003137
Iteration 3/1000 | Loss: 0.00001955
Iteration 4/1000 | Loss: 0.00001776
Iteration 5/1000 | Loss: 0.00001677
Iteration 6/1000 | Loss: 0.00001587
Iteration 7/1000 | Loss: 0.00001532
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00001402
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001369
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001365
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001360
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001359
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001336
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001332
Iteration 48/1000 | Loss: 0.00001331
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001329
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001327
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001322
Iteration 83/1000 | Loss: 0.00001322
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001321
Iteration 91/1000 | Loss: 0.00001321
Iteration 92/1000 | Loss: 0.00001321
Iteration 93/1000 | Loss: 0.00001320
Iteration 94/1000 | Loss: 0.00001320
Iteration 95/1000 | Loss: 0.00001320
Iteration 96/1000 | Loss: 0.00001320
Iteration 97/1000 | Loss: 0.00001320
Iteration 98/1000 | Loss: 0.00001320
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001319
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001317
Iteration 113/1000 | Loss: 0.00001317
Iteration 114/1000 | Loss: 0.00001317
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001315
Iteration 119/1000 | Loss: 0.00001315
Iteration 120/1000 | Loss: 0.00001315
Iteration 121/1000 | Loss: 0.00001315
Iteration 122/1000 | Loss: 0.00001314
Iteration 123/1000 | Loss: 0.00001314
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001313
Iteration 127/1000 | Loss: 0.00001313
Iteration 128/1000 | Loss: 0.00001313
Iteration 129/1000 | Loss: 0.00001313
Iteration 130/1000 | Loss: 0.00001313
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001312
Iteration 136/1000 | Loss: 0.00001312
Iteration 137/1000 | Loss: 0.00001312
Iteration 138/1000 | Loss: 0.00001312
Iteration 139/1000 | Loss: 0.00001312
Iteration 140/1000 | Loss: 0.00001312
Iteration 141/1000 | Loss: 0.00001312
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001311
Iteration 146/1000 | Loss: 0.00001311
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001311
Iteration 152/1000 | Loss: 0.00001311
Iteration 153/1000 | Loss: 0.00001311
Iteration 154/1000 | Loss: 0.00001311
Iteration 155/1000 | Loss: 0.00001311
Iteration 156/1000 | Loss: 0.00001311
Iteration 157/1000 | Loss: 0.00001310
Iteration 158/1000 | Loss: 0.00001310
Iteration 159/1000 | Loss: 0.00001310
Iteration 160/1000 | Loss: 0.00001310
Iteration 161/1000 | Loss: 0.00001310
Iteration 162/1000 | Loss: 0.00001310
Iteration 163/1000 | Loss: 0.00001310
Iteration 164/1000 | Loss: 0.00001310
Iteration 165/1000 | Loss: 0.00001309
Iteration 166/1000 | Loss: 0.00001309
Iteration 167/1000 | Loss: 0.00001309
Iteration 168/1000 | Loss: 0.00001309
Iteration 169/1000 | Loss: 0.00001309
Iteration 170/1000 | Loss: 0.00001309
Iteration 171/1000 | Loss: 0.00001309
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001308
Iteration 175/1000 | Loss: 0.00001308
Iteration 176/1000 | Loss: 0.00001308
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Iteration 180/1000 | Loss: 0.00001308
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001307
Iteration 184/1000 | Loss: 0.00001307
Iteration 185/1000 | Loss: 0.00001307
Iteration 186/1000 | Loss: 0.00001307
Iteration 187/1000 | Loss: 0.00001307
Iteration 188/1000 | Loss: 0.00001307
Iteration 189/1000 | Loss: 0.00001307
Iteration 190/1000 | Loss: 0.00001306
Iteration 191/1000 | Loss: 0.00001306
Iteration 192/1000 | Loss: 0.00001306
Iteration 193/1000 | Loss: 0.00001306
Iteration 194/1000 | Loss: 0.00001306
Iteration 195/1000 | Loss: 0.00001306
Iteration 196/1000 | Loss: 0.00001306
Iteration 197/1000 | Loss: 0.00001306
Iteration 198/1000 | Loss: 0.00001306
Iteration 199/1000 | Loss: 0.00001306
Iteration 200/1000 | Loss: 0.00001306
Iteration 201/1000 | Loss: 0.00001306
Iteration 202/1000 | Loss: 0.00001306
Iteration 203/1000 | Loss: 0.00001306
Iteration 204/1000 | Loss: 0.00001306
Iteration 205/1000 | Loss: 0.00001305
Iteration 206/1000 | Loss: 0.00001305
Iteration 207/1000 | Loss: 0.00001305
Iteration 208/1000 | Loss: 0.00001305
Iteration 209/1000 | Loss: 0.00001305
Iteration 210/1000 | Loss: 0.00001305
Iteration 211/1000 | Loss: 0.00001305
Iteration 212/1000 | Loss: 0.00001305
Iteration 213/1000 | Loss: 0.00001305
Iteration 214/1000 | Loss: 0.00001305
Iteration 215/1000 | Loss: 0.00001305
Iteration 216/1000 | Loss: 0.00001305
Iteration 217/1000 | Loss: 0.00001305
Iteration 218/1000 | Loss: 0.00001305
Iteration 219/1000 | Loss: 0.00001305
Iteration 220/1000 | Loss: 0.00001305
Iteration 221/1000 | Loss: 0.00001305
Iteration 222/1000 | Loss: 0.00001305
Iteration 223/1000 | Loss: 0.00001305
Iteration 224/1000 | Loss: 0.00001304
Iteration 225/1000 | Loss: 0.00001304
Iteration 226/1000 | Loss: 0.00001304
Iteration 227/1000 | Loss: 0.00001304
Iteration 228/1000 | Loss: 0.00001304
Iteration 229/1000 | Loss: 0.00001304
Iteration 230/1000 | Loss: 0.00001304
Iteration 231/1000 | Loss: 0.00001304
Iteration 232/1000 | Loss: 0.00001304
Iteration 233/1000 | Loss: 0.00001304
Iteration 234/1000 | Loss: 0.00001304
Iteration 235/1000 | Loss: 0.00001303
Iteration 236/1000 | Loss: 0.00001303
Iteration 237/1000 | Loss: 0.00001303
Iteration 238/1000 | Loss: 0.00001303
Iteration 239/1000 | Loss: 0.00001303
Iteration 240/1000 | Loss: 0.00001303
Iteration 241/1000 | Loss: 0.00001303
Iteration 242/1000 | Loss: 0.00001303
Iteration 243/1000 | Loss: 0.00001303
Iteration 244/1000 | Loss: 0.00001303
Iteration 245/1000 | Loss: 0.00001303
Iteration 246/1000 | Loss: 0.00001303
Iteration 247/1000 | Loss: 0.00001303
Iteration 248/1000 | Loss: 0.00001303
Iteration 249/1000 | Loss: 0.00001303
Iteration 250/1000 | Loss: 0.00001303
Iteration 251/1000 | Loss: 0.00001303
Iteration 252/1000 | Loss: 0.00001303
Iteration 253/1000 | Loss: 0.00001303
Iteration 254/1000 | Loss: 0.00001303
Iteration 255/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.3031367416260764e-05, 1.3031367416260764e-05, 1.3031367416260764e-05, 1.3031367416260764e-05, 1.3031367416260764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3031367416260764e-05

Optimization complete. Final v2v error: 3.0331010818481445 mm

Highest mean error: 3.523453712463379 mm for frame 197

Lowest mean error: 2.5870261192321777 mm for frame 131

Saving results

Total time: 49.50987768173218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921380
Iteration 2/25 | Loss: 0.00124225
Iteration 3/25 | Loss: 0.00112904
Iteration 4/25 | Loss: 0.00111721
Iteration 5/25 | Loss: 0.00111472
Iteration 6/25 | Loss: 0.00111472
Iteration 7/25 | Loss: 0.00111472
Iteration 8/25 | Loss: 0.00111472
Iteration 9/25 | Loss: 0.00111472
Iteration 10/25 | Loss: 0.00111472
Iteration 11/25 | Loss: 0.00111472
Iteration 12/25 | Loss: 0.00111472
Iteration 13/25 | Loss: 0.00111472
Iteration 14/25 | Loss: 0.00111472
Iteration 15/25 | Loss: 0.00111472
Iteration 16/25 | Loss: 0.00111472
Iteration 17/25 | Loss: 0.00111472
Iteration 18/25 | Loss: 0.00111472
Iteration 19/25 | Loss: 0.00111472
Iteration 20/25 | Loss: 0.00111472
Iteration 21/25 | Loss: 0.00111472
Iteration 22/25 | Loss: 0.00111472
Iteration 23/25 | Loss: 0.00111472
Iteration 24/25 | Loss: 0.00111472
Iteration 25/25 | Loss: 0.00111472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36952233
Iteration 2/25 | Loss: 0.00080386
Iteration 3/25 | Loss: 0.00080385
Iteration 4/25 | Loss: 0.00080385
Iteration 5/25 | Loss: 0.00080385
Iteration 6/25 | Loss: 0.00080385
Iteration 7/25 | Loss: 0.00080385
Iteration 8/25 | Loss: 0.00080385
Iteration 9/25 | Loss: 0.00080385
Iteration 10/25 | Loss: 0.00080385
Iteration 11/25 | Loss: 0.00080385
Iteration 12/25 | Loss: 0.00080385
Iteration 13/25 | Loss: 0.00080385
Iteration 14/25 | Loss: 0.00080385
Iteration 15/25 | Loss: 0.00080385
Iteration 16/25 | Loss: 0.00080385
Iteration 17/25 | Loss: 0.00080385
Iteration 18/25 | Loss: 0.00080385
Iteration 19/25 | Loss: 0.00080385
Iteration 20/25 | Loss: 0.00080385
Iteration 21/25 | Loss: 0.00080385
Iteration 22/25 | Loss: 0.00080385
Iteration 23/25 | Loss: 0.00080385
Iteration 24/25 | Loss: 0.00080385
Iteration 25/25 | Loss: 0.00080385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080385
Iteration 2/1000 | Loss: 0.00002267
Iteration 3/1000 | Loss: 0.00001435
Iteration 4/1000 | Loss: 0.00001324
Iteration 5/1000 | Loss: 0.00001268
Iteration 6/1000 | Loss: 0.00001237
Iteration 7/1000 | Loss: 0.00001190
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001136
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001104
Iteration 13/1000 | Loss: 0.00001098
Iteration 14/1000 | Loss: 0.00001098
Iteration 15/1000 | Loss: 0.00001097
Iteration 16/1000 | Loss: 0.00001095
Iteration 17/1000 | Loss: 0.00001092
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001079
Iteration 22/1000 | Loss: 0.00001073
Iteration 23/1000 | Loss: 0.00001073
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001071
Iteration 26/1000 | Loss: 0.00001070
Iteration 27/1000 | Loss: 0.00001070
Iteration 28/1000 | Loss: 0.00001069
Iteration 29/1000 | Loss: 0.00001068
Iteration 30/1000 | Loss: 0.00001067
Iteration 31/1000 | Loss: 0.00001067
Iteration 32/1000 | Loss: 0.00001066
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001065
Iteration 35/1000 | Loss: 0.00001065
Iteration 36/1000 | Loss: 0.00001062
Iteration 37/1000 | Loss: 0.00001062
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001061
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001060
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001056
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001055
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001054
Iteration 51/1000 | Loss: 0.00001054
Iteration 52/1000 | Loss: 0.00001054
Iteration 53/1000 | Loss: 0.00001053
Iteration 54/1000 | Loss: 0.00001052
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001051
Iteration 58/1000 | Loss: 0.00001051
Iteration 59/1000 | Loss: 0.00001051
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001050
Iteration 62/1000 | Loss: 0.00001050
Iteration 63/1000 | Loss: 0.00001050
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001049
Iteration 66/1000 | Loss: 0.00001048
Iteration 67/1000 | Loss: 0.00001047
Iteration 68/1000 | Loss: 0.00001047
Iteration 69/1000 | Loss: 0.00001047
Iteration 70/1000 | Loss: 0.00001046
Iteration 71/1000 | Loss: 0.00001046
Iteration 72/1000 | Loss: 0.00001045
Iteration 73/1000 | Loss: 0.00001045
Iteration 74/1000 | Loss: 0.00001045
Iteration 75/1000 | Loss: 0.00001045
Iteration 76/1000 | Loss: 0.00001045
Iteration 77/1000 | Loss: 0.00001044
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001044
Iteration 80/1000 | Loss: 0.00001043
Iteration 81/1000 | Loss: 0.00001043
Iteration 82/1000 | Loss: 0.00001042
Iteration 83/1000 | Loss: 0.00001042
Iteration 84/1000 | Loss: 0.00001041
Iteration 85/1000 | Loss: 0.00001041
Iteration 86/1000 | Loss: 0.00001041
Iteration 87/1000 | Loss: 0.00001041
Iteration 88/1000 | Loss: 0.00001041
Iteration 89/1000 | Loss: 0.00001040
Iteration 90/1000 | Loss: 0.00001040
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001039
Iteration 94/1000 | Loss: 0.00001039
Iteration 95/1000 | Loss: 0.00001039
Iteration 96/1000 | Loss: 0.00001039
Iteration 97/1000 | Loss: 0.00001039
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001038
Iteration 100/1000 | Loss: 0.00001038
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001037
Iteration 103/1000 | Loss: 0.00001037
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001035
Iteration 110/1000 | Loss: 0.00001035
Iteration 111/1000 | Loss: 0.00001035
Iteration 112/1000 | Loss: 0.00001035
Iteration 113/1000 | Loss: 0.00001035
Iteration 114/1000 | Loss: 0.00001035
Iteration 115/1000 | Loss: 0.00001035
Iteration 116/1000 | Loss: 0.00001035
Iteration 117/1000 | Loss: 0.00001035
Iteration 118/1000 | Loss: 0.00001035
Iteration 119/1000 | Loss: 0.00001034
Iteration 120/1000 | Loss: 0.00001034
Iteration 121/1000 | Loss: 0.00001034
Iteration 122/1000 | Loss: 0.00001034
Iteration 123/1000 | Loss: 0.00001034
Iteration 124/1000 | Loss: 0.00001034
Iteration 125/1000 | Loss: 0.00001034
Iteration 126/1000 | Loss: 0.00001034
Iteration 127/1000 | Loss: 0.00001033
Iteration 128/1000 | Loss: 0.00001033
Iteration 129/1000 | Loss: 0.00001033
Iteration 130/1000 | Loss: 0.00001033
Iteration 131/1000 | Loss: 0.00001033
Iteration 132/1000 | Loss: 0.00001033
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001032
Iteration 137/1000 | Loss: 0.00001032
Iteration 138/1000 | Loss: 0.00001032
Iteration 139/1000 | Loss: 0.00001032
Iteration 140/1000 | Loss: 0.00001032
Iteration 141/1000 | Loss: 0.00001032
Iteration 142/1000 | Loss: 0.00001032
Iteration 143/1000 | Loss: 0.00001031
Iteration 144/1000 | Loss: 0.00001031
Iteration 145/1000 | Loss: 0.00001031
Iteration 146/1000 | Loss: 0.00001031
Iteration 147/1000 | Loss: 0.00001031
Iteration 148/1000 | Loss: 0.00001030
Iteration 149/1000 | Loss: 0.00001030
Iteration 150/1000 | Loss: 0.00001030
Iteration 151/1000 | Loss: 0.00001030
Iteration 152/1000 | Loss: 0.00001030
Iteration 153/1000 | Loss: 0.00001030
Iteration 154/1000 | Loss: 0.00001030
Iteration 155/1000 | Loss: 0.00001030
Iteration 156/1000 | Loss: 0.00001030
Iteration 157/1000 | Loss: 0.00001030
Iteration 158/1000 | Loss: 0.00001029
Iteration 159/1000 | Loss: 0.00001029
Iteration 160/1000 | Loss: 0.00001029
Iteration 161/1000 | Loss: 0.00001029
Iteration 162/1000 | Loss: 0.00001029
Iteration 163/1000 | Loss: 0.00001029
Iteration 164/1000 | Loss: 0.00001029
Iteration 165/1000 | Loss: 0.00001029
Iteration 166/1000 | Loss: 0.00001029
Iteration 167/1000 | Loss: 0.00001029
Iteration 168/1000 | Loss: 0.00001029
Iteration 169/1000 | Loss: 0.00001029
Iteration 170/1000 | Loss: 0.00001029
Iteration 171/1000 | Loss: 0.00001029
Iteration 172/1000 | Loss: 0.00001029
Iteration 173/1000 | Loss: 0.00001029
Iteration 174/1000 | Loss: 0.00001029
Iteration 175/1000 | Loss: 0.00001029
Iteration 176/1000 | Loss: 0.00001029
Iteration 177/1000 | Loss: 0.00001029
Iteration 178/1000 | Loss: 0.00001029
Iteration 179/1000 | Loss: 0.00001029
Iteration 180/1000 | Loss: 0.00001029
Iteration 181/1000 | Loss: 0.00001029
Iteration 182/1000 | Loss: 0.00001029
Iteration 183/1000 | Loss: 0.00001029
Iteration 184/1000 | Loss: 0.00001029
Iteration 185/1000 | Loss: 0.00001029
Iteration 186/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.0288365047017578e-05, 1.0288365047017578e-05, 1.0288365047017578e-05, 1.0288365047017578e-05, 1.0288365047017578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0288365047017578e-05

Optimization complete. Final v2v error: 2.733267307281494 mm

Highest mean error: 3.3495194911956787 mm for frame 58

Lowest mean error: 2.4983270168304443 mm for frame 137

Saving results

Total time: 37.69027400016785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822913
Iteration 2/25 | Loss: 0.00120544
Iteration 3/25 | Loss: 0.00112299
Iteration 4/25 | Loss: 0.00111238
Iteration 5/25 | Loss: 0.00110854
Iteration 6/25 | Loss: 0.00110824
Iteration 7/25 | Loss: 0.00110824
Iteration 8/25 | Loss: 0.00110824
Iteration 9/25 | Loss: 0.00110824
Iteration 10/25 | Loss: 0.00110824
Iteration 11/25 | Loss: 0.00110824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011082417331635952, 0.0011082417331635952, 0.0011082417331635952, 0.0011082417331635952, 0.0011082417331635952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011082417331635952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43445599
Iteration 2/25 | Loss: 0.00072807
Iteration 3/25 | Loss: 0.00072806
Iteration 4/25 | Loss: 0.00072806
Iteration 5/25 | Loss: 0.00072806
Iteration 6/25 | Loss: 0.00072806
Iteration 7/25 | Loss: 0.00072806
Iteration 8/25 | Loss: 0.00072806
Iteration 9/25 | Loss: 0.00072806
Iteration 10/25 | Loss: 0.00072806
Iteration 11/25 | Loss: 0.00072806
Iteration 12/25 | Loss: 0.00072806
Iteration 13/25 | Loss: 0.00072806
Iteration 14/25 | Loss: 0.00072806
Iteration 15/25 | Loss: 0.00072806
Iteration 16/25 | Loss: 0.00072806
Iteration 17/25 | Loss: 0.00072806
Iteration 18/25 | Loss: 0.00072806
Iteration 19/25 | Loss: 0.00072806
Iteration 20/25 | Loss: 0.00072806
Iteration 21/25 | Loss: 0.00072806
Iteration 22/25 | Loss: 0.00072806
Iteration 23/25 | Loss: 0.00072806
Iteration 24/25 | Loss: 0.00072806
Iteration 25/25 | Loss: 0.00072806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072806
Iteration 2/1000 | Loss: 0.00002290
Iteration 3/1000 | Loss: 0.00001639
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001436
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001392
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001348
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001318
Iteration 19/1000 | Loss: 0.00001317
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001306
Iteration 32/1000 | Loss: 0.00001306
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001305
Iteration 35/1000 | Loss: 0.00001303
Iteration 36/1000 | Loss: 0.00001303
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001301
Iteration 39/1000 | Loss: 0.00001300
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001290
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001283
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001282
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001282
Iteration 66/1000 | Loss: 0.00001282
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001278
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001275
Iteration 83/1000 | Loss: 0.00001275
Iteration 84/1000 | Loss: 0.00001274
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001274
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001272
Iteration 95/1000 | Loss: 0.00001272
Iteration 96/1000 | Loss: 0.00001272
Iteration 97/1000 | Loss: 0.00001272
Iteration 98/1000 | Loss: 0.00001272
Iteration 99/1000 | Loss: 0.00001272
Iteration 100/1000 | Loss: 0.00001272
Iteration 101/1000 | Loss: 0.00001272
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001271
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001268
Iteration 121/1000 | Loss: 0.00001266
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001266
Iteration 125/1000 | Loss: 0.00001266
Iteration 126/1000 | Loss: 0.00001266
Iteration 127/1000 | Loss: 0.00001266
Iteration 128/1000 | Loss: 0.00001266
Iteration 129/1000 | Loss: 0.00001266
Iteration 130/1000 | Loss: 0.00001266
Iteration 131/1000 | Loss: 0.00001266
Iteration 132/1000 | Loss: 0.00001266
Iteration 133/1000 | Loss: 0.00001266
Iteration 134/1000 | Loss: 0.00001266
Iteration 135/1000 | Loss: 0.00001265
Iteration 136/1000 | Loss: 0.00001265
Iteration 137/1000 | Loss: 0.00001265
Iteration 138/1000 | Loss: 0.00001265
Iteration 139/1000 | Loss: 0.00001265
Iteration 140/1000 | Loss: 0.00001264
Iteration 141/1000 | Loss: 0.00001264
Iteration 142/1000 | Loss: 0.00001263
Iteration 143/1000 | Loss: 0.00001263
Iteration 144/1000 | Loss: 0.00001263
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001263
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001262
Iteration 149/1000 | Loss: 0.00001262
Iteration 150/1000 | Loss: 0.00001262
Iteration 151/1000 | Loss: 0.00001262
Iteration 152/1000 | Loss: 0.00001262
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001262
Iteration 155/1000 | Loss: 0.00001262
Iteration 156/1000 | Loss: 0.00001262
Iteration 157/1000 | Loss: 0.00001262
Iteration 158/1000 | Loss: 0.00001261
Iteration 159/1000 | Loss: 0.00001261
Iteration 160/1000 | Loss: 0.00001261
Iteration 161/1000 | Loss: 0.00001261
Iteration 162/1000 | Loss: 0.00001261
Iteration 163/1000 | Loss: 0.00001261
Iteration 164/1000 | Loss: 0.00001261
Iteration 165/1000 | Loss: 0.00001261
Iteration 166/1000 | Loss: 0.00001261
Iteration 167/1000 | Loss: 0.00001261
Iteration 168/1000 | Loss: 0.00001260
Iteration 169/1000 | Loss: 0.00001260
Iteration 170/1000 | Loss: 0.00001260
Iteration 171/1000 | Loss: 0.00001260
Iteration 172/1000 | Loss: 0.00001260
Iteration 173/1000 | Loss: 0.00001260
Iteration 174/1000 | Loss: 0.00001260
Iteration 175/1000 | Loss: 0.00001260
Iteration 176/1000 | Loss: 0.00001260
Iteration 177/1000 | Loss: 0.00001260
Iteration 178/1000 | Loss: 0.00001260
Iteration 179/1000 | Loss: 0.00001260
Iteration 180/1000 | Loss: 0.00001260
Iteration 181/1000 | Loss: 0.00001260
Iteration 182/1000 | Loss: 0.00001260
Iteration 183/1000 | Loss: 0.00001260
Iteration 184/1000 | Loss: 0.00001260
Iteration 185/1000 | Loss: 0.00001260
Iteration 186/1000 | Loss: 0.00001260
Iteration 187/1000 | Loss: 0.00001260
Iteration 188/1000 | Loss: 0.00001260
Iteration 189/1000 | Loss: 0.00001260
Iteration 190/1000 | Loss: 0.00001259
Iteration 191/1000 | Loss: 0.00001259
Iteration 192/1000 | Loss: 0.00001259
Iteration 193/1000 | Loss: 0.00001259
Iteration 194/1000 | Loss: 0.00001259
Iteration 195/1000 | Loss: 0.00001259
Iteration 196/1000 | Loss: 0.00001259
Iteration 197/1000 | Loss: 0.00001258
Iteration 198/1000 | Loss: 0.00001258
Iteration 199/1000 | Loss: 0.00001258
Iteration 200/1000 | Loss: 0.00001258
Iteration 201/1000 | Loss: 0.00001258
Iteration 202/1000 | Loss: 0.00001258
Iteration 203/1000 | Loss: 0.00001258
Iteration 204/1000 | Loss: 0.00001258
Iteration 205/1000 | Loss: 0.00001258
Iteration 206/1000 | Loss: 0.00001258
Iteration 207/1000 | Loss: 0.00001258
Iteration 208/1000 | Loss: 0.00001258
Iteration 209/1000 | Loss: 0.00001258
Iteration 210/1000 | Loss: 0.00001258
Iteration 211/1000 | Loss: 0.00001258
Iteration 212/1000 | Loss: 0.00001258
Iteration 213/1000 | Loss: 0.00001258
Iteration 214/1000 | Loss: 0.00001258
Iteration 215/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.2577816050907131e-05, 1.2577816050907131e-05, 1.2577816050907131e-05, 1.2577816050907131e-05, 1.2577816050907131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2577816050907131e-05

Optimization complete. Final v2v error: 3.0116677284240723 mm

Highest mean error: 3.3805525302886963 mm for frame 177

Lowest mean error: 2.6952733993530273 mm for frame 131

Saving results

Total time: 40.234182596206665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980896
Iteration 2/25 | Loss: 0.00213269
Iteration 3/25 | Loss: 0.00181069
Iteration 4/25 | Loss: 0.00153300
Iteration 5/25 | Loss: 0.00145989
Iteration 6/25 | Loss: 0.00147894
Iteration 7/25 | Loss: 0.00136044
Iteration 8/25 | Loss: 0.00127209
Iteration 9/25 | Loss: 0.00123202
Iteration 10/25 | Loss: 0.00120849
Iteration 11/25 | Loss: 0.00119707
Iteration 12/25 | Loss: 0.00121890
Iteration 13/25 | Loss: 0.00118385
Iteration 14/25 | Loss: 0.00118160
Iteration 15/25 | Loss: 0.00118007
Iteration 16/25 | Loss: 0.00117959
Iteration 17/25 | Loss: 0.00120788
Iteration 18/25 | Loss: 0.00117279
Iteration 19/25 | Loss: 0.00116977
Iteration 20/25 | Loss: 0.00117024
Iteration 21/25 | Loss: 0.00117162
Iteration 22/25 | Loss: 0.00119377
Iteration 23/25 | Loss: 0.00115672
Iteration 24/25 | Loss: 0.00115103
Iteration 25/25 | Loss: 0.00115580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40396237
Iteration 2/25 | Loss: 0.00117646
Iteration 3/25 | Loss: 0.00117646
Iteration 4/25 | Loss: 0.00117646
Iteration 5/25 | Loss: 0.00117646
Iteration 6/25 | Loss: 0.00117646
Iteration 7/25 | Loss: 0.00117646
Iteration 8/25 | Loss: 0.00117646
Iteration 9/25 | Loss: 0.00117646
Iteration 10/25 | Loss: 0.00117646
Iteration 11/25 | Loss: 0.00117646
Iteration 12/25 | Loss: 0.00117646
Iteration 13/25 | Loss: 0.00117646
Iteration 14/25 | Loss: 0.00117646
Iteration 15/25 | Loss: 0.00117646
Iteration 16/25 | Loss: 0.00117646
Iteration 17/25 | Loss: 0.00117646
Iteration 18/25 | Loss: 0.00117646
Iteration 19/25 | Loss: 0.00117646
Iteration 20/25 | Loss: 0.00117646
Iteration 21/25 | Loss: 0.00117646
Iteration 22/25 | Loss: 0.00117646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011764575028792024, 0.0011764575028792024, 0.0011764575028792024, 0.0011764575028792024, 0.0011764575028792024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011764575028792024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117646
Iteration 2/1000 | Loss: 0.00016927
Iteration 3/1000 | Loss: 0.00111456
Iteration 4/1000 | Loss: 0.00024974
Iteration 5/1000 | Loss: 0.00093721
Iteration 6/1000 | Loss: 0.00035476
Iteration 7/1000 | Loss: 0.00096168
Iteration 8/1000 | Loss: 0.00086567
Iteration 9/1000 | Loss: 0.00076577
Iteration 10/1000 | Loss: 0.00092380
Iteration 11/1000 | Loss: 0.00070829
Iteration 12/1000 | Loss: 0.00089184
Iteration 13/1000 | Loss: 0.00078313
Iteration 14/1000 | Loss: 0.00081854
Iteration 15/1000 | Loss: 0.00063381
Iteration 16/1000 | Loss: 0.00024727
Iteration 17/1000 | Loss: 0.00022482
Iteration 18/1000 | Loss: 0.00023011
Iteration 19/1000 | Loss: 0.00021682
Iteration 20/1000 | Loss: 0.00085060
Iteration 21/1000 | Loss: 0.00020452
Iteration 22/1000 | Loss: 0.00083327
Iteration 23/1000 | Loss: 0.00049021
Iteration 24/1000 | Loss: 0.00011744
Iteration 25/1000 | Loss: 0.00029168
Iteration 26/1000 | Loss: 0.00019522
Iteration 27/1000 | Loss: 0.00120528
Iteration 28/1000 | Loss: 0.00064973
Iteration 29/1000 | Loss: 0.00034534
Iteration 30/1000 | Loss: 0.00033674
Iteration 31/1000 | Loss: 0.00024190
Iteration 32/1000 | Loss: 0.00015182
Iteration 33/1000 | Loss: 0.00006667
Iteration 34/1000 | Loss: 0.00044370
Iteration 35/1000 | Loss: 0.00063168
Iteration 36/1000 | Loss: 0.00054044
Iteration 37/1000 | Loss: 0.00056928
Iteration 38/1000 | Loss: 0.00011331
Iteration 39/1000 | Loss: 0.00028717
Iteration 40/1000 | Loss: 0.00045958
Iteration 41/1000 | Loss: 0.00013193
Iteration 42/1000 | Loss: 0.00030560
Iteration 43/1000 | Loss: 0.00071169
Iteration 44/1000 | Loss: 0.00049442
Iteration 45/1000 | Loss: 0.00076962
Iteration 46/1000 | Loss: 0.00082356
Iteration 47/1000 | Loss: 0.00080807
Iteration 48/1000 | Loss: 0.00075679
Iteration 49/1000 | Loss: 0.00051287
Iteration 50/1000 | Loss: 0.00054820
Iteration 51/1000 | Loss: 0.00044647
Iteration 52/1000 | Loss: 0.00023202
Iteration 53/1000 | Loss: 0.00044204
Iteration 54/1000 | Loss: 0.00025163
Iteration 55/1000 | Loss: 0.00071686
Iteration 56/1000 | Loss: 0.00078564
Iteration 57/1000 | Loss: 0.00089754
Iteration 58/1000 | Loss: 0.00013090
Iteration 59/1000 | Loss: 0.00018959
Iteration 60/1000 | Loss: 0.00017652
Iteration 61/1000 | Loss: 0.00017382
Iteration 62/1000 | Loss: 0.00017954
Iteration 63/1000 | Loss: 0.00022956
Iteration 64/1000 | Loss: 0.00052379
Iteration 65/1000 | Loss: 0.00020084
Iteration 66/1000 | Loss: 0.00019443
Iteration 67/1000 | Loss: 0.00018682
Iteration 68/1000 | Loss: 0.00017519
Iteration 69/1000 | Loss: 0.00077682
Iteration 70/1000 | Loss: 0.00056372
Iteration 71/1000 | Loss: 0.00033304
Iteration 72/1000 | Loss: 0.00034994
Iteration 73/1000 | Loss: 0.00017707
Iteration 74/1000 | Loss: 0.00020052
Iteration 75/1000 | Loss: 0.00034132
Iteration 76/1000 | Loss: 0.00026068
Iteration 77/1000 | Loss: 0.00046855
Iteration 78/1000 | Loss: 0.00041242
Iteration 79/1000 | Loss: 0.00042462
Iteration 80/1000 | Loss: 0.00058611
Iteration 81/1000 | Loss: 0.00022072
Iteration 82/1000 | Loss: 0.00023564
Iteration 83/1000 | Loss: 0.00045605
Iteration 84/1000 | Loss: 0.00037441
Iteration 85/1000 | Loss: 0.00035966
Iteration 86/1000 | Loss: 0.00018813
Iteration 87/1000 | Loss: 0.00003876
Iteration 88/1000 | Loss: 0.00003098
Iteration 89/1000 | Loss: 0.00002745
Iteration 90/1000 | Loss: 0.00002486
Iteration 91/1000 | Loss: 0.00002359
Iteration 92/1000 | Loss: 0.00002262
Iteration 93/1000 | Loss: 0.00002215
Iteration 94/1000 | Loss: 0.00002170
Iteration 95/1000 | Loss: 0.00056905
Iteration 96/1000 | Loss: 0.00004046
Iteration 97/1000 | Loss: 0.00002385
Iteration 98/1000 | Loss: 0.00002116
Iteration 99/1000 | Loss: 0.00001952
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001800
Iteration 102/1000 | Loss: 0.00001771
Iteration 103/1000 | Loss: 0.00001746
Iteration 104/1000 | Loss: 0.00001738
Iteration 105/1000 | Loss: 0.00001723
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001711
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001709
Iteration 111/1000 | Loss: 0.00001709
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001706
Iteration 117/1000 | Loss: 0.00001706
Iteration 118/1000 | Loss: 0.00001706
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001702
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001695
Iteration 132/1000 | Loss: 0.00001693
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001692
Iteration 135/1000 | Loss: 0.00001689
Iteration 136/1000 | Loss: 0.00001689
Iteration 137/1000 | Loss: 0.00001689
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001681
Iteration 169/1000 | Loss: 0.00001681
Iteration 170/1000 | Loss: 0.00001680
Iteration 171/1000 | Loss: 0.00001680
Iteration 172/1000 | Loss: 0.00001680
Iteration 173/1000 | Loss: 0.00001680
Iteration 174/1000 | Loss: 0.00001680
Iteration 175/1000 | Loss: 0.00001679
Iteration 176/1000 | Loss: 0.00001679
Iteration 177/1000 | Loss: 0.00001679
Iteration 178/1000 | Loss: 0.00001679
Iteration 179/1000 | Loss: 0.00001679
Iteration 180/1000 | Loss: 0.00001679
Iteration 181/1000 | Loss: 0.00001679
Iteration 182/1000 | Loss: 0.00001679
Iteration 183/1000 | Loss: 0.00001679
Iteration 184/1000 | Loss: 0.00001679
Iteration 185/1000 | Loss: 0.00001679
Iteration 186/1000 | Loss: 0.00001679
Iteration 187/1000 | Loss: 0.00001678
Iteration 188/1000 | Loss: 0.00001678
Iteration 189/1000 | Loss: 0.00001678
Iteration 190/1000 | Loss: 0.00001678
Iteration 191/1000 | Loss: 0.00001677
Iteration 192/1000 | Loss: 0.00001677
Iteration 193/1000 | Loss: 0.00001677
Iteration 194/1000 | Loss: 0.00001677
Iteration 195/1000 | Loss: 0.00001677
Iteration 196/1000 | Loss: 0.00001677
Iteration 197/1000 | Loss: 0.00001677
Iteration 198/1000 | Loss: 0.00001677
Iteration 199/1000 | Loss: 0.00001676
Iteration 200/1000 | Loss: 0.00001676
Iteration 201/1000 | Loss: 0.00001676
Iteration 202/1000 | Loss: 0.00001676
Iteration 203/1000 | Loss: 0.00001676
Iteration 204/1000 | Loss: 0.00001676
Iteration 205/1000 | Loss: 0.00001676
Iteration 206/1000 | Loss: 0.00001676
Iteration 207/1000 | Loss: 0.00001676
Iteration 208/1000 | Loss: 0.00001676
Iteration 209/1000 | Loss: 0.00001675
Iteration 210/1000 | Loss: 0.00001675
Iteration 211/1000 | Loss: 0.00077235
Iteration 212/1000 | Loss: 0.00001961
Iteration 213/1000 | Loss: 0.00001653
Iteration 214/1000 | Loss: 0.00001562
Iteration 215/1000 | Loss: 0.00001480
Iteration 216/1000 | Loss: 0.00001414
Iteration 217/1000 | Loss: 0.00001377
Iteration 218/1000 | Loss: 0.00001357
Iteration 219/1000 | Loss: 0.00001352
Iteration 220/1000 | Loss: 0.00001350
Iteration 221/1000 | Loss: 0.00001348
Iteration 222/1000 | Loss: 0.00001348
Iteration 223/1000 | Loss: 0.00001342
Iteration 224/1000 | Loss: 0.00001335
Iteration 225/1000 | Loss: 0.00001333
Iteration 226/1000 | Loss: 0.00001333
Iteration 227/1000 | Loss: 0.00001332
Iteration 228/1000 | Loss: 0.00001332
Iteration 229/1000 | Loss: 0.00001327
Iteration 230/1000 | Loss: 0.00001326
Iteration 231/1000 | Loss: 0.00001326
Iteration 232/1000 | Loss: 0.00001325
Iteration 233/1000 | Loss: 0.00001325
Iteration 234/1000 | Loss: 0.00001324
Iteration 235/1000 | Loss: 0.00001323
Iteration 236/1000 | Loss: 0.00001323
Iteration 237/1000 | Loss: 0.00001322
Iteration 238/1000 | Loss: 0.00001322
Iteration 239/1000 | Loss: 0.00001322
Iteration 240/1000 | Loss: 0.00001322
Iteration 241/1000 | Loss: 0.00001322
Iteration 242/1000 | Loss: 0.00001322
Iteration 243/1000 | Loss: 0.00001321
Iteration 244/1000 | Loss: 0.00001321
Iteration 245/1000 | Loss: 0.00001321
Iteration 246/1000 | Loss: 0.00001321
Iteration 247/1000 | Loss: 0.00001320
Iteration 248/1000 | Loss: 0.00001320
Iteration 249/1000 | Loss: 0.00001320
Iteration 250/1000 | Loss: 0.00001320
Iteration 251/1000 | Loss: 0.00001320
Iteration 252/1000 | Loss: 0.00001320
Iteration 253/1000 | Loss: 0.00001320
Iteration 254/1000 | Loss: 0.00001320
Iteration 255/1000 | Loss: 0.00001320
Iteration 256/1000 | Loss: 0.00001319
Iteration 257/1000 | Loss: 0.00001319
Iteration 258/1000 | Loss: 0.00001319
Iteration 259/1000 | Loss: 0.00001319
Iteration 260/1000 | Loss: 0.00001318
Iteration 261/1000 | Loss: 0.00001318
Iteration 262/1000 | Loss: 0.00001318
Iteration 263/1000 | Loss: 0.00001317
Iteration 264/1000 | Loss: 0.00001317
Iteration 265/1000 | Loss: 0.00001317
Iteration 266/1000 | Loss: 0.00001317
Iteration 267/1000 | Loss: 0.00001316
Iteration 268/1000 | Loss: 0.00001316
Iteration 269/1000 | Loss: 0.00001316
Iteration 270/1000 | Loss: 0.00001316
Iteration 271/1000 | Loss: 0.00001316
Iteration 272/1000 | Loss: 0.00001316
Iteration 273/1000 | Loss: 0.00001316
Iteration 274/1000 | Loss: 0.00001316
Iteration 275/1000 | Loss: 0.00001316
Iteration 276/1000 | Loss: 0.00001316
Iteration 277/1000 | Loss: 0.00001315
Iteration 278/1000 | Loss: 0.00001315
Iteration 279/1000 | Loss: 0.00001315
Iteration 280/1000 | Loss: 0.00001315
Iteration 281/1000 | Loss: 0.00001314
Iteration 282/1000 | Loss: 0.00001314
Iteration 283/1000 | Loss: 0.00001314
Iteration 284/1000 | Loss: 0.00001314
Iteration 285/1000 | Loss: 0.00001314
Iteration 286/1000 | Loss: 0.00001314
Iteration 287/1000 | Loss: 0.00001314
Iteration 288/1000 | Loss: 0.00001314
Iteration 289/1000 | Loss: 0.00001314
Iteration 290/1000 | Loss: 0.00001314
Iteration 291/1000 | Loss: 0.00001314
Iteration 292/1000 | Loss: 0.00001314
Iteration 293/1000 | Loss: 0.00001314
Iteration 294/1000 | Loss: 0.00001314
Iteration 295/1000 | Loss: 0.00001314
Iteration 296/1000 | Loss: 0.00001314
Iteration 297/1000 | Loss: 0.00001314
Iteration 298/1000 | Loss: 0.00001314
Iteration 299/1000 | Loss: 0.00001314
Iteration 300/1000 | Loss: 0.00001314
Iteration 301/1000 | Loss: 0.00001314
Iteration 302/1000 | Loss: 0.00001313
Iteration 303/1000 | Loss: 0.00001313
Iteration 304/1000 | Loss: 0.00001313
Iteration 305/1000 | Loss: 0.00001313
Iteration 306/1000 | Loss: 0.00001313
Iteration 307/1000 | Loss: 0.00001313
Iteration 308/1000 | Loss: 0.00001313
Iteration 309/1000 | Loss: 0.00001313
Iteration 310/1000 | Loss: 0.00001313
Iteration 311/1000 | Loss: 0.00001313
Iteration 312/1000 | Loss: 0.00001313
Iteration 313/1000 | Loss: 0.00001313
Iteration 314/1000 | Loss: 0.00001313
Iteration 315/1000 | Loss: 0.00001313
Iteration 316/1000 | Loss: 0.00001313
Iteration 317/1000 | Loss: 0.00001313
Iteration 318/1000 | Loss: 0.00001313
Iteration 319/1000 | Loss: 0.00001313
Iteration 320/1000 | Loss: 0.00001313
Iteration 321/1000 | Loss: 0.00001313
Iteration 322/1000 | Loss: 0.00001313
Iteration 323/1000 | Loss: 0.00001313
Iteration 324/1000 | Loss: 0.00001313
Iteration 325/1000 | Loss: 0.00001312
Iteration 326/1000 | Loss: 0.00001312
Iteration 327/1000 | Loss: 0.00001312
Iteration 328/1000 | Loss: 0.00001312
Iteration 329/1000 | Loss: 0.00001312
Iteration 330/1000 | Loss: 0.00001312
Iteration 331/1000 | Loss: 0.00001312
Iteration 332/1000 | Loss: 0.00001312
Iteration 333/1000 | Loss: 0.00001312
Iteration 334/1000 | Loss: 0.00001312
Iteration 335/1000 | Loss: 0.00001312
Iteration 336/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 336. Stopping optimization.
Last 5 losses: [1.312379117734963e-05, 1.312379117734963e-05, 1.312379117734963e-05, 1.312379117734963e-05, 1.312379117734963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.312379117734963e-05

Optimization complete. Final v2v error: 3.07600474357605 mm

Highest mean error: 4.322587490081787 mm for frame 57

Lowest mean error: 2.8482532501220703 mm for frame 22

Saving results

Total time: 217.049658536911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117678
Iteration 2/25 | Loss: 0.00194627
Iteration 3/25 | Loss: 0.00141500
Iteration 4/25 | Loss: 0.00136521
Iteration 5/25 | Loss: 0.00136138
Iteration 6/25 | Loss: 0.00136333
Iteration 7/25 | Loss: 0.00136073
Iteration 8/25 | Loss: 0.00134518
Iteration 9/25 | Loss: 0.00133390
Iteration 10/25 | Loss: 0.00133212
Iteration 11/25 | Loss: 0.00133171
Iteration 12/25 | Loss: 0.00132921
Iteration 13/25 | Loss: 0.00132865
Iteration 14/25 | Loss: 0.00132646
Iteration 15/25 | Loss: 0.00132345
Iteration 16/25 | Loss: 0.00132517
Iteration 17/25 | Loss: 0.00132413
Iteration 18/25 | Loss: 0.00132397
Iteration 19/25 | Loss: 0.00132328
Iteration 20/25 | Loss: 0.00132271
Iteration 21/25 | Loss: 0.00132327
Iteration 22/25 | Loss: 0.00132237
Iteration 23/25 | Loss: 0.00132027
Iteration 24/25 | Loss: 0.00132224
Iteration 25/25 | Loss: 0.00132314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28258574
Iteration 2/25 | Loss: 0.00147923
Iteration 3/25 | Loss: 0.00147920
Iteration 4/25 | Loss: 0.00147920
Iteration 5/25 | Loss: 0.00147920
Iteration 6/25 | Loss: 0.00147920
Iteration 7/25 | Loss: 0.00147920
Iteration 8/25 | Loss: 0.00147920
Iteration 9/25 | Loss: 0.00147920
Iteration 10/25 | Loss: 0.00147920
Iteration 11/25 | Loss: 0.00147920
Iteration 12/25 | Loss: 0.00147920
Iteration 13/25 | Loss: 0.00147920
Iteration 14/25 | Loss: 0.00147920
Iteration 15/25 | Loss: 0.00147920
Iteration 16/25 | Loss: 0.00147920
Iteration 17/25 | Loss: 0.00147920
Iteration 18/25 | Loss: 0.00147920
Iteration 19/25 | Loss: 0.00147920
Iteration 20/25 | Loss: 0.00147920
Iteration 21/25 | Loss: 0.00147920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014791961293667555, 0.0014791961293667555, 0.0014791961293667555, 0.0014791961293667555, 0.0014791961293667555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014791961293667555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147920
Iteration 2/1000 | Loss: 0.00044061
Iteration 3/1000 | Loss: 0.00180461
Iteration 4/1000 | Loss: 0.00055171
Iteration 5/1000 | Loss: 0.00016039
Iteration 6/1000 | Loss: 0.00016954
Iteration 7/1000 | Loss: 0.00016123
Iteration 8/1000 | Loss: 0.00024446
Iteration 9/1000 | Loss: 0.00020280
Iteration 10/1000 | Loss: 0.00020491
Iteration 11/1000 | Loss: 0.00016580
Iteration 12/1000 | Loss: 0.00005141
Iteration 13/1000 | Loss: 0.00011395
Iteration 14/1000 | Loss: 0.00012110
Iteration 15/1000 | Loss: 0.00013070
Iteration 16/1000 | Loss: 0.00009094
Iteration 17/1000 | Loss: 0.00013069
Iteration 18/1000 | Loss: 0.00021024
Iteration 19/1000 | Loss: 0.00010737
Iteration 20/1000 | Loss: 0.00012470
Iteration 21/1000 | Loss: 0.00005836
Iteration 22/1000 | Loss: 0.00009235
Iteration 23/1000 | Loss: 0.00010668
Iteration 24/1000 | Loss: 0.00025186
Iteration 25/1000 | Loss: 0.00015430
Iteration 26/1000 | Loss: 0.00010815
Iteration 27/1000 | Loss: 0.00018838
Iteration 28/1000 | Loss: 0.00017270
Iteration 29/1000 | Loss: 0.00016044
Iteration 30/1000 | Loss: 0.00041031
Iteration 31/1000 | Loss: 0.00026125
Iteration 32/1000 | Loss: 0.00014851
Iteration 33/1000 | Loss: 0.00007203
Iteration 34/1000 | Loss: 0.00022028
Iteration 35/1000 | Loss: 0.00021164
Iteration 36/1000 | Loss: 0.00008231
Iteration 37/1000 | Loss: 0.00023722
Iteration 38/1000 | Loss: 0.00010471
Iteration 39/1000 | Loss: 0.00014181
Iteration 40/1000 | Loss: 0.00015087
Iteration 41/1000 | Loss: 0.00013185
Iteration 42/1000 | Loss: 0.00007865
Iteration 43/1000 | Loss: 0.00005457
Iteration 44/1000 | Loss: 0.00013575
Iteration 45/1000 | Loss: 0.00012883
Iteration 46/1000 | Loss: 0.00021703
Iteration 47/1000 | Loss: 0.00012823
Iteration 48/1000 | Loss: 0.00010742
Iteration 49/1000 | Loss: 0.00004776
Iteration 50/1000 | Loss: 0.00011415
Iteration 51/1000 | Loss: 0.00007068
Iteration 52/1000 | Loss: 0.00009484
Iteration 53/1000 | Loss: 0.00017076
Iteration 54/1000 | Loss: 0.00011550
Iteration 55/1000 | Loss: 0.00005178
Iteration 56/1000 | Loss: 0.00003658
Iteration 57/1000 | Loss: 0.00010309
Iteration 58/1000 | Loss: 0.00005569
Iteration 59/1000 | Loss: 0.00011711
Iteration 60/1000 | Loss: 0.00016760
Iteration 61/1000 | Loss: 0.00012434
Iteration 62/1000 | Loss: 0.00008101
Iteration 63/1000 | Loss: 0.00010657
Iteration 64/1000 | Loss: 0.00008774
Iteration 65/1000 | Loss: 0.00006600
Iteration 66/1000 | Loss: 0.00012288
Iteration 67/1000 | Loss: 0.00008660
Iteration 68/1000 | Loss: 0.00008788
Iteration 69/1000 | Loss: 0.00008311
Iteration 70/1000 | Loss: 0.00007927
Iteration 71/1000 | Loss: 0.00007920
Iteration 72/1000 | Loss: 0.00009420
Iteration 73/1000 | Loss: 0.00009360
Iteration 74/1000 | Loss: 0.00003801
Iteration 75/1000 | Loss: 0.00015288
Iteration 76/1000 | Loss: 0.00003337
Iteration 77/1000 | Loss: 0.00003185
Iteration 78/1000 | Loss: 0.00003096
Iteration 79/1000 | Loss: 0.00003053
Iteration 80/1000 | Loss: 0.00003017
Iteration 81/1000 | Loss: 0.00002972
Iteration 82/1000 | Loss: 0.00002924
Iteration 83/1000 | Loss: 0.00002891
Iteration 84/1000 | Loss: 0.00002870
Iteration 85/1000 | Loss: 0.00002850
Iteration 86/1000 | Loss: 0.00002823
Iteration 87/1000 | Loss: 0.00045857
Iteration 88/1000 | Loss: 0.00004097
Iteration 89/1000 | Loss: 0.00003719
Iteration 90/1000 | Loss: 0.00003338
Iteration 91/1000 | Loss: 0.00004432
Iteration 92/1000 | Loss: 0.00003010
Iteration 93/1000 | Loss: 0.00002865
Iteration 94/1000 | Loss: 0.00002761
Iteration 95/1000 | Loss: 0.00002719
Iteration 96/1000 | Loss: 0.00002676
Iteration 97/1000 | Loss: 0.00002649
Iteration 98/1000 | Loss: 0.00002621
Iteration 99/1000 | Loss: 0.00002601
Iteration 100/1000 | Loss: 0.00002600
Iteration 101/1000 | Loss: 0.00002592
Iteration 102/1000 | Loss: 0.00002585
Iteration 103/1000 | Loss: 0.00002582
Iteration 104/1000 | Loss: 0.00002582
Iteration 105/1000 | Loss: 0.00002580
Iteration 106/1000 | Loss: 0.00002580
Iteration 107/1000 | Loss: 0.00002580
Iteration 108/1000 | Loss: 0.00002580
Iteration 109/1000 | Loss: 0.00002580
Iteration 110/1000 | Loss: 0.00002580
Iteration 111/1000 | Loss: 0.00002580
Iteration 112/1000 | Loss: 0.00002579
Iteration 113/1000 | Loss: 0.00002579
Iteration 114/1000 | Loss: 0.00002579
Iteration 115/1000 | Loss: 0.00002579
Iteration 116/1000 | Loss: 0.00002579
Iteration 117/1000 | Loss: 0.00002579
Iteration 118/1000 | Loss: 0.00002579
Iteration 119/1000 | Loss: 0.00002579
Iteration 120/1000 | Loss: 0.00002579
Iteration 121/1000 | Loss: 0.00002579
Iteration 122/1000 | Loss: 0.00002579
Iteration 123/1000 | Loss: 0.00002579
Iteration 124/1000 | Loss: 0.00002579
Iteration 125/1000 | Loss: 0.00002579
Iteration 126/1000 | Loss: 0.00002579
Iteration 127/1000 | Loss: 0.00002578
Iteration 128/1000 | Loss: 0.00002578
Iteration 129/1000 | Loss: 0.00002578
Iteration 130/1000 | Loss: 0.00002577
Iteration 131/1000 | Loss: 0.00002577
Iteration 132/1000 | Loss: 0.00002577
Iteration 133/1000 | Loss: 0.00002577
Iteration 134/1000 | Loss: 0.00002577
Iteration 135/1000 | Loss: 0.00002577
Iteration 136/1000 | Loss: 0.00002577
Iteration 137/1000 | Loss: 0.00002577
Iteration 138/1000 | Loss: 0.00002577
Iteration 139/1000 | Loss: 0.00002576
Iteration 140/1000 | Loss: 0.00002576
Iteration 141/1000 | Loss: 0.00002576
Iteration 142/1000 | Loss: 0.00002576
Iteration 143/1000 | Loss: 0.00002576
Iteration 144/1000 | Loss: 0.00002576
Iteration 145/1000 | Loss: 0.00002576
Iteration 146/1000 | Loss: 0.00002576
Iteration 147/1000 | Loss: 0.00002576
Iteration 148/1000 | Loss: 0.00002575
Iteration 149/1000 | Loss: 0.00002575
Iteration 150/1000 | Loss: 0.00002575
Iteration 151/1000 | Loss: 0.00002575
Iteration 152/1000 | Loss: 0.00002575
Iteration 153/1000 | Loss: 0.00002575
Iteration 154/1000 | Loss: 0.00002575
Iteration 155/1000 | Loss: 0.00002575
Iteration 156/1000 | Loss: 0.00002575
Iteration 157/1000 | Loss: 0.00002574
Iteration 158/1000 | Loss: 0.00002574
Iteration 159/1000 | Loss: 0.00002574
Iteration 160/1000 | Loss: 0.00002574
Iteration 161/1000 | Loss: 0.00002573
Iteration 162/1000 | Loss: 0.00002573
Iteration 163/1000 | Loss: 0.00002573
Iteration 164/1000 | Loss: 0.00002573
Iteration 165/1000 | Loss: 0.00002573
Iteration 166/1000 | Loss: 0.00002573
Iteration 167/1000 | Loss: 0.00002573
Iteration 168/1000 | Loss: 0.00002572
Iteration 169/1000 | Loss: 0.00002572
Iteration 170/1000 | Loss: 0.00002572
Iteration 171/1000 | Loss: 0.00002572
Iteration 172/1000 | Loss: 0.00002572
Iteration 173/1000 | Loss: 0.00002572
Iteration 174/1000 | Loss: 0.00002572
Iteration 175/1000 | Loss: 0.00002572
Iteration 176/1000 | Loss: 0.00002572
Iteration 177/1000 | Loss: 0.00002572
Iteration 178/1000 | Loss: 0.00002572
Iteration 179/1000 | Loss: 0.00002571
Iteration 180/1000 | Loss: 0.00002571
Iteration 181/1000 | Loss: 0.00002571
Iteration 182/1000 | Loss: 0.00002571
Iteration 183/1000 | Loss: 0.00002570
Iteration 184/1000 | Loss: 0.00002570
Iteration 185/1000 | Loss: 0.00002570
Iteration 186/1000 | Loss: 0.00002570
Iteration 187/1000 | Loss: 0.00002570
Iteration 188/1000 | Loss: 0.00002570
Iteration 189/1000 | Loss: 0.00002570
Iteration 190/1000 | Loss: 0.00002570
Iteration 191/1000 | Loss: 0.00002569
Iteration 192/1000 | Loss: 0.00002569
Iteration 193/1000 | Loss: 0.00002569
Iteration 194/1000 | Loss: 0.00002569
Iteration 195/1000 | Loss: 0.00002569
Iteration 196/1000 | Loss: 0.00002569
Iteration 197/1000 | Loss: 0.00002569
Iteration 198/1000 | Loss: 0.00002569
Iteration 199/1000 | Loss: 0.00002568
Iteration 200/1000 | Loss: 0.00002568
Iteration 201/1000 | Loss: 0.00002568
Iteration 202/1000 | Loss: 0.00002568
Iteration 203/1000 | Loss: 0.00002568
Iteration 204/1000 | Loss: 0.00002568
Iteration 205/1000 | Loss: 0.00002568
Iteration 206/1000 | Loss: 0.00002568
Iteration 207/1000 | Loss: 0.00002568
Iteration 208/1000 | Loss: 0.00002568
Iteration 209/1000 | Loss: 0.00002568
Iteration 210/1000 | Loss: 0.00002568
Iteration 211/1000 | Loss: 0.00002568
Iteration 212/1000 | Loss: 0.00002568
Iteration 213/1000 | Loss: 0.00002567
Iteration 214/1000 | Loss: 0.00002567
Iteration 215/1000 | Loss: 0.00002566
Iteration 216/1000 | Loss: 0.00002566
Iteration 217/1000 | Loss: 0.00002566
Iteration 218/1000 | Loss: 0.00002566
Iteration 219/1000 | Loss: 0.00002565
Iteration 220/1000 | Loss: 0.00002565
Iteration 221/1000 | Loss: 0.00002565
Iteration 222/1000 | Loss: 0.00002564
Iteration 223/1000 | Loss: 0.00002564
Iteration 224/1000 | Loss: 0.00002564
Iteration 225/1000 | Loss: 0.00002564
Iteration 226/1000 | Loss: 0.00002563
Iteration 227/1000 | Loss: 0.00002563
Iteration 228/1000 | Loss: 0.00002563
Iteration 229/1000 | Loss: 0.00002563
Iteration 230/1000 | Loss: 0.00002563
Iteration 231/1000 | Loss: 0.00002562
Iteration 232/1000 | Loss: 0.00002562
Iteration 233/1000 | Loss: 0.00002562
Iteration 234/1000 | Loss: 0.00002562
Iteration 235/1000 | Loss: 0.00002562
Iteration 236/1000 | Loss: 0.00002562
Iteration 237/1000 | Loss: 0.00002562
Iteration 238/1000 | Loss: 0.00002562
Iteration 239/1000 | Loss: 0.00002562
Iteration 240/1000 | Loss: 0.00002562
Iteration 241/1000 | Loss: 0.00002561
Iteration 242/1000 | Loss: 0.00002561
Iteration 243/1000 | Loss: 0.00002561
Iteration 244/1000 | Loss: 0.00002561
Iteration 245/1000 | Loss: 0.00002561
Iteration 246/1000 | Loss: 0.00002561
Iteration 247/1000 | Loss: 0.00002561
Iteration 248/1000 | Loss: 0.00002561
Iteration 249/1000 | Loss: 0.00002560
Iteration 250/1000 | Loss: 0.00002560
Iteration 251/1000 | Loss: 0.00002560
Iteration 252/1000 | Loss: 0.00002560
Iteration 253/1000 | Loss: 0.00002560
Iteration 254/1000 | Loss: 0.00002560
Iteration 255/1000 | Loss: 0.00002559
Iteration 256/1000 | Loss: 0.00002559
Iteration 257/1000 | Loss: 0.00002559
Iteration 258/1000 | Loss: 0.00002559
Iteration 259/1000 | Loss: 0.00002559
Iteration 260/1000 | Loss: 0.00002558
Iteration 261/1000 | Loss: 0.00002558
Iteration 262/1000 | Loss: 0.00002558
Iteration 263/1000 | Loss: 0.00002558
Iteration 264/1000 | Loss: 0.00002557
Iteration 265/1000 | Loss: 0.00002557
Iteration 266/1000 | Loss: 0.00002557
Iteration 267/1000 | Loss: 0.00002557
Iteration 268/1000 | Loss: 0.00002557
Iteration 269/1000 | Loss: 0.00002557
Iteration 270/1000 | Loss: 0.00002557
Iteration 271/1000 | Loss: 0.00002557
Iteration 272/1000 | Loss: 0.00002557
Iteration 273/1000 | Loss: 0.00002557
Iteration 274/1000 | Loss: 0.00002557
Iteration 275/1000 | Loss: 0.00002557
Iteration 276/1000 | Loss: 0.00002557
Iteration 277/1000 | Loss: 0.00002557
Iteration 278/1000 | Loss: 0.00002556
Iteration 279/1000 | Loss: 0.00002556
Iteration 280/1000 | Loss: 0.00002556
Iteration 281/1000 | Loss: 0.00002556
Iteration 282/1000 | Loss: 0.00002556
Iteration 283/1000 | Loss: 0.00002556
Iteration 284/1000 | Loss: 0.00002555
Iteration 285/1000 | Loss: 0.00002555
Iteration 286/1000 | Loss: 0.00002555
Iteration 287/1000 | Loss: 0.00002555
Iteration 288/1000 | Loss: 0.00002555
Iteration 289/1000 | Loss: 0.00002555
Iteration 290/1000 | Loss: 0.00002555
Iteration 291/1000 | Loss: 0.00002555
Iteration 292/1000 | Loss: 0.00002555
Iteration 293/1000 | Loss: 0.00002555
Iteration 294/1000 | Loss: 0.00002555
Iteration 295/1000 | Loss: 0.00002554
Iteration 296/1000 | Loss: 0.00002554
Iteration 297/1000 | Loss: 0.00002554
Iteration 298/1000 | Loss: 0.00002554
Iteration 299/1000 | Loss: 0.00002554
Iteration 300/1000 | Loss: 0.00002554
Iteration 301/1000 | Loss: 0.00002554
Iteration 302/1000 | Loss: 0.00002554
Iteration 303/1000 | Loss: 0.00002554
Iteration 304/1000 | Loss: 0.00002554
Iteration 305/1000 | Loss: 0.00002554
Iteration 306/1000 | Loss: 0.00002554
Iteration 307/1000 | Loss: 0.00002554
Iteration 308/1000 | Loss: 0.00002554
Iteration 309/1000 | Loss: 0.00002554
Iteration 310/1000 | Loss: 0.00002553
Iteration 311/1000 | Loss: 0.00002553
Iteration 312/1000 | Loss: 0.00002553
Iteration 313/1000 | Loss: 0.00002553
Iteration 314/1000 | Loss: 0.00002553
Iteration 315/1000 | Loss: 0.00002553
Iteration 316/1000 | Loss: 0.00002553
Iteration 317/1000 | Loss: 0.00002553
Iteration 318/1000 | Loss: 0.00002553
Iteration 319/1000 | Loss: 0.00002553
Iteration 320/1000 | Loss: 0.00002553
Iteration 321/1000 | Loss: 0.00002553
Iteration 322/1000 | Loss: 0.00002553
Iteration 323/1000 | Loss: 0.00002553
Iteration 324/1000 | Loss: 0.00002553
Iteration 325/1000 | Loss: 0.00002553
Iteration 326/1000 | Loss: 0.00002553
Iteration 327/1000 | Loss: 0.00002553
Iteration 328/1000 | Loss: 0.00002553
Iteration 329/1000 | Loss: 0.00002553
Iteration 330/1000 | Loss: 0.00002553
Iteration 331/1000 | Loss: 0.00002553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [2.5533609004924074e-05, 2.5533609004924074e-05, 2.5533609004924074e-05, 2.5533609004924074e-05, 2.5533609004924074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5533609004924074e-05

Optimization complete. Final v2v error: 4.108942031860352 mm

Highest mean error: 5.185097694396973 mm for frame 88

Lowest mean error: 3.2517824172973633 mm for frame 34

Saving results

Total time: 233.62848567962646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395583
Iteration 2/25 | Loss: 0.00116245
Iteration 3/25 | Loss: 0.00108313
Iteration 4/25 | Loss: 0.00107831
Iteration 5/25 | Loss: 0.00107596
Iteration 6/25 | Loss: 0.00107596
Iteration 7/25 | Loss: 0.00107596
Iteration 8/25 | Loss: 0.00107596
Iteration 9/25 | Loss: 0.00107596
Iteration 10/25 | Loss: 0.00107596
Iteration 11/25 | Loss: 0.00107596
Iteration 12/25 | Loss: 0.00107596
Iteration 13/25 | Loss: 0.00107596
Iteration 14/25 | Loss: 0.00107596
Iteration 15/25 | Loss: 0.00107596
Iteration 16/25 | Loss: 0.00107596
Iteration 17/25 | Loss: 0.00107596
Iteration 18/25 | Loss: 0.00107596
Iteration 19/25 | Loss: 0.00107596
Iteration 20/25 | Loss: 0.00107596
Iteration 21/25 | Loss: 0.00107596
Iteration 22/25 | Loss: 0.00107596
Iteration 23/25 | Loss: 0.00107596
Iteration 24/25 | Loss: 0.00107594
Iteration 25/25 | Loss: 0.00107594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58942485
Iteration 2/25 | Loss: 0.00073504
Iteration 3/25 | Loss: 0.00073504
Iteration 4/25 | Loss: 0.00073504
Iteration 5/25 | Loss: 0.00073504
Iteration 6/25 | Loss: 0.00073504
Iteration 7/25 | Loss: 0.00073504
Iteration 8/25 | Loss: 0.00073504
Iteration 9/25 | Loss: 0.00073504
Iteration 10/25 | Loss: 0.00073504
Iteration 11/25 | Loss: 0.00073504
Iteration 12/25 | Loss: 0.00073504
Iteration 13/25 | Loss: 0.00073504
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007350391242653131, 0.0007350391242653131, 0.0007350391242653131, 0.0007350391242653131, 0.0007350391242653131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007350391242653131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073504
Iteration 2/1000 | Loss: 0.00002182
Iteration 3/1000 | Loss: 0.00001358
Iteration 4/1000 | Loss: 0.00001208
Iteration 5/1000 | Loss: 0.00001117
Iteration 6/1000 | Loss: 0.00001060
Iteration 7/1000 | Loss: 0.00001023
Iteration 8/1000 | Loss: 0.00001019
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000961
Iteration 11/1000 | Loss: 0.00000947
Iteration 12/1000 | Loss: 0.00000944
Iteration 13/1000 | Loss: 0.00000942
Iteration 14/1000 | Loss: 0.00000942
Iteration 15/1000 | Loss: 0.00000941
Iteration 16/1000 | Loss: 0.00000940
Iteration 17/1000 | Loss: 0.00000940
Iteration 18/1000 | Loss: 0.00000936
Iteration 19/1000 | Loss: 0.00000934
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000927
Iteration 22/1000 | Loss: 0.00000926
Iteration 23/1000 | Loss: 0.00000925
Iteration 24/1000 | Loss: 0.00000920
Iteration 25/1000 | Loss: 0.00000920
Iteration 26/1000 | Loss: 0.00000918
Iteration 27/1000 | Loss: 0.00000918
Iteration 28/1000 | Loss: 0.00000917
Iteration 29/1000 | Loss: 0.00000917
Iteration 30/1000 | Loss: 0.00000916
Iteration 31/1000 | Loss: 0.00000915
Iteration 32/1000 | Loss: 0.00000915
Iteration 33/1000 | Loss: 0.00000914
Iteration 34/1000 | Loss: 0.00000914
Iteration 35/1000 | Loss: 0.00000914
Iteration 36/1000 | Loss: 0.00000914
Iteration 37/1000 | Loss: 0.00000913
Iteration 38/1000 | Loss: 0.00000913
Iteration 39/1000 | Loss: 0.00000913
Iteration 40/1000 | Loss: 0.00000912
Iteration 41/1000 | Loss: 0.00000912
Iteration 42/1000 | Loss: 0.00000911
Iteration 43/1000 | Loss: 0.00000911
Iteration 44/1000 | Loss: 0.00000910
Iteration 45/1000 | Loss: 0.00000910
Iteration 46/1000 | Loss: 0.00000910
Iteration 47/1000 | Loss: 0.00000909
Iteration 48/1000 | Loss: 0.00000909
Iteration 49/1000 | Loss: 0.00000908
Iteration 50/1000 | Loss: 0.00000908
Iteration 51/1000 | Loss: 0.00000908
Iteration 52/1000 | Loss: 0.00000907
Iteration 53/1000 | Loss: 0.00000907
Iteration 54/1000 | Loss: 0.00000907
Iteration 55/1000 | Loss: 0.00000906
Iteration 56/1000 | Loss: 0.00000906
Iteration 57/1000 | Loss: 0.00000906
Iteration 58/1000 | Loss: 0.00000906
Iteration 59/1000 | Loss: 0.00000906
Iteration 60/1000 | Loss: 0.00000906
Iteration 61/1000 | Loss: 0.00000906
Iteration 62/1000 | Loss: 0.00000906
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000905
Iteration 68/1000 | Loss: 0.00000905
Iteration 69/1000 | Loss: 0.00000904
Iteration 70/1000 | Loss: 0.00000903
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000902
Iteration 74/1000 | Loss: 0.00000902
Iteration 75/1000 | Loss: 0.00000901
Iteration 76/1000 | Loss: 0.00000901
Iteration 77/1000 | Loss: 0.00000901
Iteration 78/1000 | Loss: 0.00000900
Iteration 79/1000 | Loss: 0.00000900
Iteration 80/1000 | Loss: 0.00000899
Iteration 81/1000 | Loss: 0.00000899
Iteration 82/1000 | Loss: 0.00000899
Iteration 83/1000 | Loss: 0.00000899
Iteration 84/1000 | Loss: 0.00000899
Iteration 85/1000 | Loss: 0.00000899
Iteration 86/1000 | Loss: 0.00000898
Iteration 87/1000 | Loss: 0.00000898
Iteration 88/1000 | Loss: 0.00000898
Iteration 89/1000 | Loss: 0.00000898
Iteration 90/1000 | Loss: 0.00000898
Iteration 91/1000 | Loss: 0.00000898
Iteration 92/1000 | Loss: 0.00000898
Iteration 93/1000 | Loss: 0.00000898
Iteration 94/1000 | Loss: 0.00000898
Iteration 95/1000 | Loss: 0.00000898
Iteration 96/1000 | Loss: 0.00000898
Iteration 97/1000 | Loss: 0.00000898
Iteration 98/1000 | Loss: 0.00000897
Iteration 99/1000 | Loss: 0.00000897
Iteration 100/1000 | Loss: 0.00000897
Iteration 101/1000 | Loss: 0.00000897
Iteration 102/1000 | Loss: 0.00000897
Iteration 103/1000 | Loss: 0.00000897
Iteration 104/1000 | Loss: 0.00000896
Iteration 105/1000 | Loss: 0.00000896
Iteration 106/1000 | Loss: 0.00000896
Iteration 107/1000 | Loss: 0.00000895
Iteration 108/1000 | Loss: 0.00000895
Iteration 109/1000 | Loss: 0.00000895
Iteration 110/1000 | Loss: 0.00000895
Iteration 111/1000 | Loss: 0.00000895
Iteration 112/1000 | Loss: 0.00000895
Iteration 113/1000 | Loss: 0.00000895
Iteration 114/1000 | Loss: 0.00000895
Iteration 115/1000 | Loss: 0.00000895
Iteration 116/1000 | Loss: 0.00000895
Iteration 117/1000 | Loss: 0.00000895
Iteration 118/1000 | Loss: 0.00000895
Iteration 119/1000 | Loss: 0.00000895
Iteration 120/1000 | Loss: 0.00000894
Iteration 121/1000 | Loss: 0.00000894
Iteration 122/1000 | Loss: 0.00000894
Iteration 123/1000 | Loss: 0.00000894
Iteration 124/1000 | Loss: 0.00000894
Iteration 125/1000 | Loss: 0.00000893
Iteration 126/1000 | Loss: 0.00000893
Iteration 127/1000 | Loss: 0.00000893
Iteration 128/1000 | Loss: 0.00000893
Iteration 129/1000 | Loss: 0.00000893
Iteration 130/1000 | Loss: 0.00000893
Iteration 131/1000 | Loss: 0.00000893
Iteration 132/1000 | Loss: 0.00000893
Iteration 133/1000 | Loss: 0.00000893
Iteration 134/1000 | Loss: 0.00000893
Iteration 135/1000 | Loss: 0.00000893
Iteration 136/1000 | Loss: 0.00000893
Iteration 137/1000 | Loss: 0.00000893
Iteration 138/1000 | Loss: 0.00000893
Iteration 139/1000 | Loss: 0.00000893
Iteration 140/1000 | Loss: 0.00000893
Iteration 141/1000 | Loss: 0.00000893
Iteration 142/1000 | Loss: 0.00000893
Iteration 143/1000 | Loss: 0.00000893
Iteration 144/1000 | Loss: 0.00000893
Iteration 145/1000 | Loss: 0.00000893
Iteration 146/1000 | Loss: 0.00000893
Iteration 147/1000 | Loss: 0.00000893
Iteration 148/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [8.926675945986062e-06, 8.926675945986062e-06, 8.926675945986062e-06, 8.926675945986062e-06, 8.926675945986062e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.926675945986062e-06

Optimization complete. Final v2v error: 2.5783634185791016 mm

Highest mean error: 2.790717840194702 mm for frame 105

Lowest mean error: 2.4302918910980225 mm for frame 198

Saving results

Total time: 39.07999610900879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834184
Iteration 2/25 | Loss: 0.00120419
Iteration 3/25 | Loss: 0.00111322
Iteration 4/25 | Loss: 0.00110250
Iteration 5/25 | Loss: 0.00109930
Iteration 6/25 | Loss: 0.00109859
Iteration 7/25 | Loss: 0.00109859
Iteration 8/25 | Loss: 0.00109859
Iteration 9/25 | Loss: 0.00109859
Iteration 10/25 | Loss: 0.00109859
Iteration 11/25 | Loss: 0.00109859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010985899716615677, 0.0010985899716615677, 0.0010985899716615677, 0.0010985899716615677, 0.0010985899716615677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010985899716615677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.21570969
Iteration 2/25 | Loss: 0.00088375
Iteration 3/25 | Loss: 0.00088375
Iteration 4/25 | Loss: 0.00088375
Iteration 5/25 | Loss: 0.00088375
Iteration 6/25 | Loss: 0.00088375
Iteration 7/25 | Loss: 0.00088375
Iteration 8/25 | Loss: 0.00088375
Iteration 9/25 | Loss: 0.00088375
Iteration 10/25 | Loss: 0.00088375
Iteration 11/25 | Loss: 0.00088375
Iteration 12/25 | Loss: 0.00088375
Iteration 13/25 | Loss: 0.00088375
Iteration 14/25 | Loss: 0.00088375
Iteration 15/25 | Loss: 0.00088375
Iteration 16/25 | Loss: 0.00088375
Iteration 17/25 | Loss: 0.00088375
Iteration 18/25 | Loss: 0.00088375
Iteration 19/25 | Loss: 0.00088375
Iteration 20/25 | Loss: 0.00088375
Iteration 21/25 | Loss: 0.00088375
Iteration 22/25 | Loss: 0.00088375
Iteration 23/25 | Loss: 0.00088375
Iteration 24/25 | Loss: 0.00088375
Iteration 25/25 | Loss: 0.00088375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088375
Iteration 2/1000 | Loss: 0.00002475
Iteration 3/1000 | Loss: 0.00001772
Iteration 4/1000 | Loss: 0.00001479
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001355
Iteration 7/1000 | Loss: 0.00001323
Iteration 8/1000 | Loss: 0.00001286
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001253
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001193
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001185
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001179
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001169
Iteration 101/1000 | Loss: 0.00001169
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001166
Iteration 108/1000 | Loss: 0.00001166
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001166
Iteration 113/1000 | Loss: 0.00001165
Iteration 114/1000 | Loss: 0.00001165
Iteration 115/1000 | Loss: 0.00001165
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001165
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001163
Iteration 132/1000 | Loss: 0.00001163
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001161
Iteration 137/1000 | Loss: 0.00001161
Iteration 138/1000 | Loss: 0.00001161
Iteration 139/1000 | Loss: 0.00001161
Iteration 140/1000 | Loss: 0.00001161
Iteration 141/1000 | Loss: 0.00001160
Iteration 142/1000 | Loss: 0.00001160
Iteration 143/1000 | Loss: 0.00001160
Iteration 144/1000 | Loss: 0.00001160
Iteration 145/1000 | Loss: 0.00001160
Iteration 146/1000 | Loss: 0.00001160
Iteration 147/1000 | Loss: 0.00001159
Iteration 148/1000 | Loss: 0.00001159
Iteration 149/1000 | Loss: 0.00001159
Iteration 150/1000 | Loss: 0.00001159
Iteration 151/1000 | Loss: 0.00001159
Iteration 152/1000 | Loss: 0.00001159
Iteration 153/1000 | Loss: 0.00001159
Iteration 154/1000 | Loss: 0.00001159
Iteration 155/1000 | Loss: 0.00001159
Iteration 156/1000 | Loss: 0.00001159
Iteration 157/1000 | Loss: 0.00001159
Iteration 158/1000 | Loss: 0.00001159
Iteration 159/1000 | Loss: 0.00001158
Iteration 160/1000 | Loss: 0.00001158
Iteration 161/1000 | Loss: 0.00001158
Iteration 162/1000 | Loss: 0.00001158
Iteration 163/1000 | Loss: 0.00001158
Iteration 164/1000 | Loss: 0.00001158
Iteration 165/1000 | Loss: 0.00001158
Iteration 166/1000 | Loss: 0.00001158
Iteration 167/1000 | Loss: 0.00001158
Iteration 168/1000 | Loss: 0.00001157
Iteration 169/1000 | Loss: 0.00001157
Iteration 170/1000 | Loss: 0.00001157
Iteration 171/1000 | Loss: 0.00001157
Iteration 172/1000 | Loss: 0.00001157
Iteration 173/1000 | Loss: 0.00001157
Iteration 174/1000 | Loss: 0.00001157
Iteration 175/1000 | Loss: 0.00001157
Iteration 176/1000 | Loss: 0.00001157
Iteration 177/1000 | Loss: 0.00001156
Iteration 178/1000 | Loss: 0.00001156
Iteration 179/1000 | Loss: 0.00001156
Iteration 180/1000 | Loss: 0.00001156
Iteration 181/1000 | Loss: 0.00001156
Iteration 182/1000 | Loss: 0.00001156
Iteration 183/1000 | Loss: 0.00001156
Iteration 184/1000 | Loss: 0.00001156
Iteration 185/1000 | Loss: 0.00001156
Iteration 186/1000 | Loss: 0.00001156
Iteration 187/1000 | Loss: 0.00001156
Iteration 188/1000 | Loss: 0.00001156
Iteration 189/1000 | Loss: 0.00001156
Iteration 190/1000 | Loss: 0.00001156
Iteration 191/1000 | Loss: 0.00001156
Iteration 192/1000 | Loss: 0.00001156
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001156
Iteration 196/1000 | Loss: 0.00001156
Iteration 197/1000 | Loss: 0.00001156
Iteration 198/1000 | Loss: 0.00001156
Iteration 199/1000 | Loss: 0.00001156
Iteration 200/1000 | Loss: 0.00001156
Iteration 201/1000 | Loss: 0.00001156
Iteration 202/1000 | Loss: 0.00001156
Iteration 203/1000 | Loss: 0.00001156
Iteration 204/1000 | Loss: 0.00001156
Iteration 205/1000 | Loss: 0.00001156
Iteration 206/1000 | Loss: 0.00001156
Iteration 207/1000 | Loss: 0.00001156
Iteration 208/1000 | Loss: 0.00001156
Iteration 209/1000 | Loss: 0.00001156
Iteration 210/1000 | Loss: 0.00001156
Iteration 211/1000 | Loss: 0.00001156
Iteration 212/1000 | Loss: 0.00001156
Iteration 213/1000 | Loss: 0.00001156
Iteration 214/1000 | Loss: 0.00001156
Iteration 215/1000 | Loss: 0.00001156
Iteration 216/1000 | Loss: 0.00001156
Iteration 217/1000 | Loss: 0.00001156
Iteration 218/1000 | Loss: 0.00001156
Iteration 219/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.1557733159861527e-05, 1.1557733159861527e-05, 1.1557733159861527e-05, 1.1557733159861527e-05, 1.1557733159861527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1557733159861527e-05

Optimization complete. Final v2v error: 2.864824056625366 mm

Highest mean error: 3.6707308292388916 mm for frame 90

Lowest mean error: 2.554110288619995 mm for frame 14

Saving results

Total time: 39.83229637145996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837074
Iteration 2/25 | Loss: 0.00137231
Iteration 3/25 | Loss: 0.00113622
Iteration 4/25 | Loss: 0.00110032
Iteration 5/25 | Loss: 0.00109548
Iteration 6/25 | Loss: 0.00109447
Iteration 7/25 | Loss: 0.00109447
Iteration 8/25 | Loss: 0.00109447
Iteration 9/25 | Loss: 0.00109447
Iteration 10/25 | Loss: 0.00109447
Iteration 11/25 | Loss: 0.00109447
Iteration 12/25 | Loss: 0.00109447
Iteration 13/25 | Loss: 0.00109447
Iteration 14/25 | Loss: 0.00109447
Iteration 15/25 | Loss: 0.00109447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010944721288979053, 0.0010944721288979053, 0.0010944721288979053, 0.0010944721288979053, 0.0010944721288979053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010944721288979053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35452485
Iteration 2/25 | Loss: 0.00082413
Iteration 3/25 | Loss: 0.00082412
Iteration 4/25 | Loss: 0.00082412
Iteration 5/25 | Loss: 0.00082411
Iteration 6/25 | Loss: 0.00082411
Iteration 7/25 | Loss: 0.00082411
Iteration 8/25 | Loss: 0.00082411
Iteration 9/25 | Loss: 0.00082411
Iteration 10/25 | Loss: 0.00082411
Iteration 11/25 | Loss: 0.00082411
Iteration 12/25 | Loss: 0.00082411
Iteration 13/25 | Loss: 0.00082411
Iteration 14/25 | Loss: 0.00082411
Iteration 15/25 | Loss: 0.00082411
Iteration 16/25 | Loss: 0.00082411
Iteration 17/25 | Loss: 0.00082411
Iteration 18/25 | Loss: 0.00082411
Iteration 19/25 | Loss: 0.00082411
Iteration 20/25 | Loss: 0.00082411
Iteration 21/25 | Loss: 0.00082411
Iteration 22/25 | Loss: 0.00082411
Iteration 23/25 | Loss: 0.00082411
Iteration 24/25 | Loss: 0.00082411
Iteration 25/25 | Loss: 0.00082411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082411
Iteration 2/1000 | Loss: 0.00003097
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001437
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001211
Iteration 11/1000 | Loss: 0.00001176
Iteration 12/1000 | Loss: 0.00001176
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001157
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001155
Iteration 20/1000 | Loss: 0.00001154
Iteration 21/1000 | Loss: 0.00001154
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001142
Iteration 24/1000 | Loss: 0.00001141
Iteration 25/1000 | Loss: 0.00001141
Iteration 26/1000 | Loss: 0.00001141
Iteration 27/1000 | Loss: 0.00001141
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001140
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001140
Iteration 34/1000 | Loss: 0.00001140
Iteration 35/1000 | Loss: 0.00001140
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001140
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001127
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001127
Iteration 54/1000 | Loss: 0.00001127
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001126
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001123
Iteration 78/1000 | Loss: 0.00001122
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00001122
Iteration 81/1000 | Loss: 0.00001122
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001116
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001116
Iteration 122/1000 | Loss: 0.00001116
Iteration 123/1000 | Loss: 0.00001116
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001116
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001114
Iteration 133/1000 | Loss: 0.00001114
Iteration 134/1000 | Loss: 0.00001114
Iteration 135/1000 | Loss: 0.00001114
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001114
Iteration 138/1000 | Loss: 0.00001114
Iteration 139/1000 | Loss: 0.00001114
Iteration 140/1000 | Loss: 0.00001114
Iteration 141/1000 | Loss: 0.00001114
Iteration 142/1000 | Loss: 0.00001114
Iteration 143/1000 | Loss: 0.00001114
Iteration 144/1000 | Loss: 0.00001114
Iteration 145/1000 | Loss: 0.00001114
Iteration 146/1000 | Loss: 0.00001114
Iteration 147/1000 | Loss: 0.00001114
Iteration 148/1000 | Loss: 0.00001114
Iteration 149/1000 | Loss: 0.00001114
Iteration 150/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.1136555258417502e-05, 1.1136555258417502e-05, 1.1136555258417502e-05, 1.1136555258417502e-05, 1.1136555258417502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1136555258417502e-05

Optimization complete. Final v2v error: 2.8324031829833984 mm

Highest mean error: 3.7370986938476562 mm for frame 163

Lowest mean error: 2.411266803741455 mm for frame 48

Saving results

Total time: 41.34717583656311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471754
Iteration 2/25 | Loss: 0.00123891
Iteration 3/25 | Loss: 0.00113631
Iteration 4/25 | Loss: 0.00112025
Iteration 5/25 | Loss: 0.00111290
Iteration 6/25 | Loss: 0.00111144
Iteration 7/25 | Loss: 0.00111144
Iteration 8/25 | Loss: 0.00111144
Iteration 9/25 | Loss: 0.00111144
Iteration 10/25 | Loss: 0.00111144
Iteration 11/25 | Loss: 0.00111144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001111439778469503, 0.001111439778469503, 0.001111439778469503, 0.001111439778469503, 0.001111439778469503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001111439778469503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75969619
Iteration 2/25 | Loss: 0.00080282
Iteration 3/25 | Loss: 0.00080282
Iteration 4/25 | Loss: 0.00080282
Iteration 5/25 | Loss: 0.00080282
Iteration 6/25 | Loss: 0.00080282
Iteration 7/25 | Loss: 0.00080282
Iteration 8/25 | Loss: 0.00080282
Iteration 9/25 | Loss: 0.00080282
Iteration 10/25 | Loss: 0.00080282
Iteration 11/25 | Loss: 0.00080282
Iteration 12/25 | Loss: 0.00080282
Iteration 13/25 | Loss: 0.00080282
Iteration 14/25 | Loss: 0.00080282
Iteration 15/25 | Loss: 0.00080282
Iteration 16/25 | Loss: 0.00080282
Iteration 17/25 | Loss: 0.00080282
Iteration 18/25 | Loss: 0.00080282
Iteration 19/25 | Loss: 0.00080282
Iteration 20/25 | Loss: 0.00080282
Iteration 21/25 | Loss: 0.00080282
Iteration 22/25 | Loss: 0.00080282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008028191514313221, 0.0008028191514313221, 0.0008028191514313221, 0.0008028191514313221, 0.0008028191514313221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008028191514313221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080282
Iteration 2/1000 | Loss: 0.00003291
Iteration 3/1000 | Loss: 0.00002425
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002180
Iteration 6/1000 | Loss: 0.00002110
Iteration 7/1000 | Loss: 0.00002057
Iteration 8/1000 | Loss: 0.00002017
Iteration 9/1000 | Loss: 0.00001982
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00001931
Iteration 12/1000 | Loss: 0.00001928
Iteration 13/1000 | Loss: 0.00001911
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001890
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001882
Iteration 22/1000 | Loss: 0.00001880
Iteration 23/1000 | Loss: 0.00001878
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00001859
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001856
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001855
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001848
Iteration 35/1000 | Loss: 0.00001848
Iteration 36/1000 | Loss: 0.00001847
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001845
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001844
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001841
Iteration 50/1000 | Loss: 0.00001840
Iteration 51/1000 | Loss: 0.00001840
Iteration 52/1000 | Loss: 0.00001840
Iteration 53/1000 | Loss: 0.00001838
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001836
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001833
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001829
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001828
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001821
Iteration 87/1000 | Loss: 0.00001821
Iteration 88/1000 | Loss: 0.00001821
Iteration 89/1000 | Loss: 0.00001821
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001819
Iteration 102/1000 | Loss: 0.00001819
Iteration 103/1000 | Loss: 0.00001819
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001818
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001817
Iteration 112/1000 | Loss: 0.00001817
Iteration 113/1000 | Loss: 0.00001817
Iteration 114/1000 | Loss: 0.00001817
Iteration 115/1000 | Loss: 0.00001817
Iteration 116/1000 | Loss: 0.00001817
Iteration 117/1000 | Loss: 0.00001817
Iteration 118/1000 | Loss: 0.00001817
Iteration 119/1000 | Loss: 0.00001817
Iteration 120/1000 | Loss: 0.00001816
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001816
Iteration 123/1000 | Loss: 0.00001816
Iteration 124/1000 | Loss: 0.00001816
Iteration 125/1000 | Loss: 0.00001816
Iteration 126/1000 | Loss: 0.00001816
Iteration 127/1000 | Loss: 0.00001816
Iteration 128/1000 | Loss: 0.00001816
Iteration 129/1000 | Loss: 0.00001816
Iteration 130/1000 | Loss: 0.00001816
Iteration 131/1000 | Loss: 0.00001815
Iteration 132/1000 | Loss: 0.00001815
Iteration 133/1000 | Loss: 0.00001815
Iteration 134/1000 | Loss: 0.00001815
Iteration 135/1000 | Loss: 0.00001815
Iteration 136/1000 | Loss: 0.00001814
Iteration 137/1000 | Loss: 0.00001814
Iteration 138/1000 | Loss: 0.00001812
Iteration 139/1000 | Loss: 0.00001811
Iteration 140/1000 | Loss: 0.00001811
Iteration 141/1000 | Loss: 0.00001811
Iteration 142/1000 | Loss: 0.00001811
Iteration 143/1000 | Loss: 0.00001810
Iteration 144/1000 | Loss: 0.00001810
Iteration 145/1000 | Loss: 0.00001809
Iteration 146/1000 | Loss: 0.00001809
Iteration 147/1000 | Loss: 0.00001809
Iteration 148/1000 | Loss: 0.00001808
Iteration 149/1000 | Loss: 0.00001808
Iteration 150/1000 | Loss: 0.00001808
Iteration 151/1000 | Loss: 0.00001808
Iteration 152/1000 | Loss: 0.00001807
Iteration 153/1000 | Loss: 0.00001807
Iteration 154/1000 | Loss: 0.00001807
Iteration 155/1000 | Loss: 0.00001806
Iteration 156/1000 | Loss: 0.00001806
Iteration 157/1000 | Loss: 0.00001805
Iteration 158/1000 | Loss: 0.00001804
Iteration 159/1000 | Loss: 0.00001804
Iteration 160/1000 | Loss: 0.00001803
Iteration 161/1000 | Loss: 0.00001802
Iteration 162/1000 | Loss: 0.00001802
Iteration 163/1000 | Loss: 0.00001802
Iteration 164/1000 | Loss: 0.00001801
Iteration 165/1000 | Loss: 0.00001801
Iteration 166/1000 | Loss: 0.00001801
Iteration 167/1000 | Loss: 0.00001800
Iteration 168/1000 | Loss: 0.00001800
Iteration 169/1000 | Loss: 0.00001800
Iteration 170/1000 | Loss: 0.00001800
Iteration 171/1000 | Loss: 0.00001800
Iteration 172/1000 | Loss: 0.00001800
Iteration 173/1000 | Loss: 0.00001800
Iteration 174/1000 | Loss: 0.00001800
Iteration 175/1000 | Loss: 0.00001800
Iteration 176/1000 | Loss: 0.00001800
Iteration 177/1000 | Loss: 0.00001799
Iteration 178/1000 | Loss: 0.00001799
Iteration 179/1000 | Loss: 0.00001798
Iteration 180/1000 | Loss: 0.00001798
Iteration 181/1000 | Loss: 0.00001797
Iteration 182/1000 | Loss: 0.00001797
Iteration 183/1000 | Loss: 0.00001797
Iteration 184/1000 | Loss: 0.00001797
Iteration 185/1000 | Loss: 0.00001797
Iteration 186/1000 | Loss: 0.00001797
Iteration 187/1000 | Loss: 0.00001797
Iteration 188/1000 | Loss: 0.00001797
Iteration 189/1000 | Loss: 0.00001797
Iteration 190/1000 | Loss: 0.00001797
Iteration 191/1000 | Loss: 0.00001797
Iteration 192/1000 | Loss: 0.00001797
Iteration 193/1000 | Loss: 0.00001797
Iteration 194/1000 | Loss: 0.00001797
Iteration 195/1000 | Loss: 0.00001797
Iteration 196/1000 | Loss: 0.00001797
Iteration 197/1000 | Loss: 0.00001797
Iteration 198/1000 | Loss: 0.00001797
Iteration 199/1000 | Loss: 0.00001797
Iteration 200/1000 | Loss: 0.00001797
Iteration 201/1000 | Loss: 0.00001797
Iteration 202/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.7966405721381307e-05, 1.7966405721381307e-05, 1.7966405721381307e-05, 1.7966405721381307e-05, 1.7966405721381307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7966405721381307e-05

Optimization complete. Final v2v error: 3.6202051639556885 mm

Highest mean error: 4.27614688873291 mm for frame 258

Lowest mean error: 3.457538604736328 mm for frame 105

Saving results

Total time: 56.44764256477356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977894
Iteration 2/25 | Loss: 0.00201608
Iteration 3/25 | Loss: 0.00147472
Iteration 4/25 | Loss: 0.00154312
Iteration 5/25 | Loss: 0.00128522
Iteration 6/25 | Loss: 0.00123820
Iteration 7/25 | Loss: 0.00123334
Iteration 8/25 | Loss: 0.00121829
Iteration 9/25 | Loss: 0.00118638
Iteration 10/25 | Loss: 0.00117268
Iteration 11/25 | Loss: 0.00116992
Iteration 12/25 | Loss: 0.00116808
Iteration 13/25 | Loss: 0.00116532
Iteration 14/25 | Loss: 0.00116011
Iteration 15/25 | Loss: 0.00115922
Iteration 16/25 | Loss: 0.00115719
Iteration 17/25 | Loss: 0.00115716
Iteration 18/25 | Loss: 0.00115670
Iteration 19/25 | Loss: 0.00115675
Iteration 20/25 | Loss: 0.00115701
Iteration 21/25 | Loss: 0.00115558
Iteration 22/25 | Loss: 0.00115624
Iteration 23/25 | Loss: 0.00115706
Iteration 24/25 | Loss: 0.00115775
Iteration 25/25 | Loss: 0.00115656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32874155
Iteration 2/25 | Loss: 0.00063743
Iteration 3/25 | Loss: 0.00063743
Iteration 4/25 | Loss: 0.00063743
Iteration 5/25 | Loss: 0.00063743
Iteration 6/25 | Loss: 0.00063743
Iteration 7/25 | Loss: 0.00063743
Iteration 8/25 | Loss: 0.00063743
Iteration 9/25 | Loss: 0.00063743
Iteration 10/25 | Loss: 0.00063743
Iteration 11/25 | Loss: 0.00063743
Iteration 12/25 | Loss: 0.00063743
Iteration 13/25 | Loss: 0.00063743
Iteration 14/25 | Loss: 0.00063743
Iteration 15/25 | Loss: 0.00063743
Iteration 16/25 | Loss: 0.00063743
Iteration 17/25 | Loss: 0.00063743
Iteration 18/25 | Loss: 0.00063743
Iteration 19/25 | Loss: 0.00063743
Iteration 20/25 | Loss: 0.00063743
Iteration 21/25 | Loss: 0.00063743
Iteration 22/25 | Loss: 0.00063743
Iteration 23/25 | Loss: 0.00063743
Iteration 24/25 | Loss: 0.00063743
Iteration 25/25 | Loss: 0.00063743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063743
Iteration 2/1000 | Loss: 0.00003657
Iteration 3/1000 | Loss: 0.00004524
Iteration 4/1000 | Loss: 0.00003866
Iteration 5/1000 | Loss: 0.00004666
Iteration 6/1000 | Loss: 0.00002728
Iteration 7/1000 | Loss: 0.00004026
Iteration 8/1000 | Loss: 0.00003661
Iteration 9/1000 | Loss: 0.00004245
Iteration 10/1000 | Loss: 0.00003478
Iteration 11/1000 | Loss: 0.00003788
Iteration 12/1000 | Loss: 0.00004158
Iteration 13/1000 | Loss: 0.00003855
Iteration 14/1000 | Loss: 0.00003905
Iteration 15/1000 | Loss: 0.00003907
Iteration 16/1000 | Loss: 0.00003834
Iteration 17/1000 | Loss: 0.00002934
Iteration 18/1000 | Loss: 0.00003742
Iteration 19/1000 | Loss: 0.00003833
Iteration 20/1000 | Loss: 0.00004733
Iteration 21/1000 | Loss: 0.00002757
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002773
Iteration 24/1000 | Loss: 0.00002697
Iteration 25/1000 | Loss: 0.00002794
Iteration 26/1000 | Loss: 0.00002610
Iteration 27/1000 | Loss: 0.00002813
Iteration 28/1000 | Loss: 0.00003651
Iteration 29/1000 | Loss: 0.00002746
Iteration 30/1000 | Loss: 0.00002932
Iteration 31/1000 | Loss: 0.00002656
Iteration 32/1000 | Loss: 0.00002857
Iteration 33/1000 | Loss: 0.00002136
Iteration 34/1000 | Loss: 0.00002260
Iteration 35/1000 | Loss: 0.00002109
Iteration 36/1000 | Loss: 0.00002429
Iteration 37/1000 | Loss: 0.00002481
Iteration 38/1000 | Loss: 0.00002183
Iteration 39/1000 | Loss: 0.00002888
Iteration 40/1000 | Loss: 0.00002569
Iteration 41/1000 | Loss: 0.00003077
Iteration 42/1000 | Loss: 0.00002977
Iteration 43/1000 | Loss: 0.00003294
Iteration 44/1000 | Loss: 0.00002720
Iteration 45/1000 | Loss: 0.00002519
Iteration 46/1000 | Loss: 0.00002967
Iteration 47/1000 | Loss: 0.00002750
Iteration 48/1000 | Loss: 0.00002928
Iteration 49/1000 | Loss: 0.00002574
Iteration 50/1000 | Loss: 0.00002944
Iteration 51/1000 | Loss: 0.00004313
Iteration 52/1000 | Loss: 0.00002878
Iteration 53/1000 | Loss: 0.00002759
Iteration 54/1000 | Loss: 0.00003083
Iteration 55/1000 | Loss: 0.00003797
Iteration 56/1000 | Loss: 0.00002832
Iteration 57/1000 | Loss: 0.00002856
Iteration 58/1000 | Loss: 0.00003162
Iteration 59/1000 | Loss: 0.00003311
Iteration 60/1000 | Loss: 0.00003093
Iteration 61/1000 | Loss: 0.00003209
Iteration 62/1000 | Loss: 0.00003079
Iteration 63/1000 | Loss: 0.00003169
Iteration 64/1000 | Loss: 0.00003056
Iteration 65/1000 | Loss: 0.00002698
Iteration 66/1000 | Loss: 0.00002856
Iteration 67/1000 | Loss: 0.00003624
Iteration 68/1000 | Loss: 0.00003021
Iteration 69/1000 | Loss: 0.00003024
Iteration 70/1000 | Loss: 0.00002719
Iteration 71/1000 | Loss: 0.00002150
Iteration 72/1000 | Loss: 0.00002017
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001920
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001876
Iteration 80/1000 | Loss: 0.00001875
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001874
Iteration 84/1000 | Loss: 0.00001874
Iteration 85/1000 | Loss: 0.00001873
Iteration 86/1000 | Loss: 0.00001873
Iteration 87/1000 | Loss: 0.00001873
Iteration 88/1000 | Loss: 0.00001872
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001872
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001871
Iteration 93/1000 | Loss: 0.00001871
Iteration 94/1000 | Loss: 0.00001871
Iteration 95/1000 | Loss: 0.00001871
Iteration 96/1000 | Loss: 0.00001870
Iteration 97/1000 | Loss: 0.00001870
Iteration 98/1000 | Loss: 0.00001870
Iteration 99/1000 | Loss: 0.00001870
Iteration 100/1000 | Loss: 0.00001870
Iteration 101/1000 | Loss: 0.00001870
Iteration 102/1000 | Loss: 0.00001870
Iteration 103/1000 | Loss: 0.00001870
Iteration 104/1000 | Loss: 0.00001869
Iteration 105/1000 | Loss: 0.00001869
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001869
Iteration 108/1000 | Loss: 0.00001869
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001867
Iteration 112/1000 | Loss: 0.00001866
Iteration 113/1000 | Loss: 0.00001866
Iteration 114/1000 | Loss: 0.00001866
Iteration 115/1000 | Loss: 0.00001866
Iteration 116/1000 | Loss: 0.00001866
Iteration 117/1000 | Loss: 0.00001866
Iteration 118/1000 | Loss: 0.00001866
Iteration 119/1000 | Loss: 0.00001866
Iteration 120/1000 | Loss: 0.00001865
Iteration 121/1000 | Loss: 0.00001865
Iteration 122/1000 | Loss: 0.00001865
Iteration 123/1000 | Loss: 0.00001864
Iteration 124/1000 | Loss: 0.00001864
Iteration 125/1000 | Loss: 0.00001864
Iteration 126/1000 | Loss: 0.00001864
Iteration 127/1000 | Loss: 0.00001864
Iteration 128/1000 | Loss: 0.00001863
Iteration 129/1000 | Loss: 0.00001863
Iteration 130/1000 | Loss: 0.00001863
Iteration 131/1000 | Loss: 0.00001863
Iteration 132/1000 | Loss: 0.00001863
Iteration 133/1000 | Loss: 0.00001863
Iteration 134/1000 | Loss: 0.00001863
Iteration 135/1000 | Loss: 0.00001863
Iteration 136/1000 | Loss: 0.00001863
Iteration 137/1000 | Loss: 0.00001863
Iteration 138/1000 | Loss: 0.00001863
Iteration 139/1000 | Loss: 0.00001863
Iteration 140/1000 | Loss: 0.00001863
Iteration 141/1000 | Loss: 0.00001863
Iteration 142/1000 | Loss: 0.00001863
Iteration 143/1000 | Loss: 0.00001863
Iteration 144/1000 | Loss: 0.00001862
Iteration 145/1000 | Loss: 0.00001862
Iteration 146/1000 | Loss: 0.00001862
Iteration 147/1000 | Loss: 0.00001862
Iteration 148/1000 | Loss: 0.00001862
Iteration 149/1000 | Loss: 0.00001862
Iteration 150/1000 | Loss: 0.00001862
Iteration 151/1000 | Loss: 0.00001862
Iteration 152/1000 | Loss: 0.00001862
Iteration 153/1000 | Loss: 0.00001862
Iteration 154/1000 | Loss: 0.00001862
Iteration 155/1000 | Loss: 0.00001862
Iteration 156/1000 | Loss: 0.00001862
Iteration 157/1000 | Loss: 0.00001862
Iteration 158/1000 | Loss: 0.00001862
Iteration 159/1000 | Loss: 0.00001862
Iteration 160/1000 | Loss: 0.00001862
Iteration 161/1000 | Loss: 0.00001862
Iteration 162/1000 | Loss: 0.00001861
Iteration 163/1000 | Loss: 0.00001861
Iteration 164/1000 | Loss: 0.00001861
Iteration 165/1000 | Loss: 0.00001861
Iteration 166/1000 | Loss: 0.00001861
Iteration 167/1000 | Loss: 0.00001861
Iteration 168/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.8614051441545598e-05, 1.8614051441545598e-05, 1.8614051441545598e-05, 1.8614051441545598e-05, 1.8614051441545598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8614051441545598e-05

Optimization complete. Final v2v error: 3.6246018409729004 mm

Highest mean error: 5.008597373962402 mm for frame 205

Lowest mean error: 3.4660627841949463 mm for frame 227

Saving results

Total time: 182.78618001937866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737550
Iteration 2/25 | Loss: 0.00216454
Iteration 3/25 | Loss: 0.00165077
Iteration 4/25 | Loss: 0.00157810
Iteration 5/25 | Loss: 0.00148994
Iteration 6/25 | Loss: 0.00146532
Iteration 7/25 | Loss: 0.00142700
Iteration 8/25 | Loss: 0.00142156
Iteration 9/25 | Loss: 0.00167257
Iteration 10/25 | Loss: 0.00177487
Iteration 11/25 | Loss: 0.00133383
Iteration 12/25 | Loss: 0.00127559
Iteration 13/25 | Loss: 0.00119774
Iteration 14/25 | Loss: 0.00115368
Iteration 15/25 | Loss: 0.00114146
Iteration 16/25 | Loss: 0.00113811
Iteration 17/25 | Loss: 0.00113630
Iteration 18/25 | Loss: 0.00113580
Iteration 19/25 | Loss: 0.00113571
Iteration 20/25 | Loss: 0.00113570
Iteration 21/25 | Loss: 0.00113563
Iteration 22/25 | Loss: 0.00113562
Iteration 23/25 | Loss: 0.00113562
Iteration 24/25 | Loss: 0.00113562
Iteration 25/25 | Loss: 0.00113562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42267156
Iteration 2/25 | Loss: 0.00123072
Iteration 3/25 | Loss: 0.00104261
Iteration 4/25 | Loss: 0.00104261
Iteration 5/25 | Loss: 0.00104261
Iteration 6/25 | Loss: 0.00104260
Iteration 7/25 | Loss: 0.00104260
Iteration 8/25 | Loss: 0.00104260
Iteration 9/25 | Loss: 0.00104260
Iteration 10/25 | Loss: 0.00104260
Iteration 11/25 | Loss: 0.00104260
Iteration 12/25 | Loss: 0.00104260
Iteration 13/25 | Loss: 0.00104260
Iteration 14/25 | Loss: 0.00104260
Iteration 15/25 | Loss: 0.00104260
Iteration 16/25 | Loss: 0.00104260
Iteration 17/25 | Loss: 0.00104260
Iteration 18/25 | Loss: 0.00104260
Iteration 19/25 | Loss: 0.00104260
Iteration 20/25 | Loss: 0.00104260
Iteration 21/25 | Loss: 0.00104260
Iteration 22/25 | Loss: 0.00104260
Iteration 23/25 | Loss: 0.00104260
Iteration 24/25 | Loss: 0.00104260
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010426010703667998, 0.0010426010703667998, 0.0010426010703667998, 0.0010426010703667998, 0.0010426010703667998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010426010703667998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104260
Iteration 2/1000 | Loss: 0.00006658
Iteration 3/1000 | Loss: 0.00004045
Iteration 4/1000 | Loss: 0.00003329
Iteration 5/1000 | Loss: 0.00003023
Iteration 6/1000 | Loss: 0.00002738
Iteration 7/1000 | Loss: 0.00002599
Iteration 8/1000 | Loss: 0.00002519
Iteration 9/1000 | Loss: 0.00002459
Iteration 10/1000 | Loss: 0.00002404
Iteration 11/1000 | Loss: 0.00002364
Iteration 12/1000 | Loss: 0.00002325
Iteration 13/1000 | Loss: 0.00002299
Iteration 14/1000 | Loss: 0.00002285
Iteration 15/1000 | Loss: 0.00002265
Iteration 16/1000 | Loss: 0.00002245
Iteration 17/1000 | Loss: 0.00002230
Iteration 18/1000 | Loss: 0.00002219
Iteration 19/1000 | Loss: 0.00002215
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002205
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002204
Iteration 25/1000 | Loss: 0.00002203
Iteration 26/1000 | Loss: 0.00002202
Iteration 27/1000 | Loss: 0.00002199
Iteration 28/1000 | Loss: 0.00002196
Iteration 29/1000 | Loss: 0.00002196
Iteration 30/1000 | Loss: 0.00002196
Iteration 31/1000 | Loss: 0.00002195
Iteration 32/1000 | Loss: 0.00002195
Iteration 33/1000 | Loss: 0.00002194
Iteration 34/1000 | Loss: 0.00002194
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002193
Iteration 37/1000 | Loss: 0.00002193
Iteration 38/1000 | Loss: 0.00002193
Iteration 39/1000 | Loss: 0.00002193
Iteration 40/1000 | Loss: 0.00002192
Iteration 41/1000 | Loss: 0.00002192
Iteration 42/1000 | Loss: 0.00002192
Iteration 43/1000 | Loss: 0.00002191
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002191
Iteration 46/1000 | Loss: 0.00002191
Iteration 47/1000 | Loss: 0.00002190
Iteration 48/1000 | Loss: 0.00002190
Iteration 49/1000 | Loss: 0.00002190
Iteration 50/1000 | Loss: 0.00002189
Iteration 51/1000 | Loss: 0.00002189
Iteration 52/1000 | Loss: 0.00002189
Iteration 53/1000 | Loss: 0.00002188
Iteration 54/1000 | Loss: 0.00002188
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002184
Iteration 57/1000 | Loss: 0.00002182
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002180
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002171
Iteration 64/1000 | Loss: 0.00002171
Iteration 65/1000 | Loss: 0.00002169
Iteration 66/1000 | Loss: 0.00002169
Iteration 67/1000 | Loss: 0.00002168
Iteration 68/1000 | Loss: 0.00002168
Iteration 69/1000 | Loss: 0.00002168
Iteration 70/1000 | Loss: 0.00002168
Iteration 71/1000 | Loss: 0.00002168
Iteration 72/1000 | Loss: 0.00002167
Iteration 73/1000 | Loss: 0.00002167
Iteration 74/1000 | Loss: 0.00002167
Iteration 75/1000 | Loss: 0.00002167
Iteration 76/1000 | Loss: 0.00002167
Iteration 77/1000 | Loss: 0.00002166
Iteration 78/1000 | Loss: 0.00002166
Iteration 79/1000 | Loss: 0.00002166
Iteration 80/1000 | Loss: 0.00002165
Iteration 81/1000 | Loss: 0.00002165
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002163
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002163
Iteration 87/1000 | Loss: 0.00002162
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002161
Iteration 90/1000 | Loss: 0.00002161
Iteration 91/1000 | Loss: 0.00002161
Iteration 92/1000 | Loss: 0.00002160
Iteration 93/1000 | Loss: 0.00002160
Iteration 94/1000 | Loss: 0.00002160
Iteration 95/1000 | Loss: 0.00002159
Iteration 96/1000 | Loss: 0.00002159
Iteration 97/1000 | Loss: 0.00002159
Iteration 98/1000 | Loss: 0.00002158
Iteration 99/1000 | Loss: 0.00002158
Iteration 100/1000 | Loss: 0.00002157
Iteration 101/1000 | Loss: 0.00002157
Iteration 102/1000 | Loss: 0.00002157
Iteration 103/1000 | Loss: 0.00002157
Iteration 104/1000 | Loss: 0.00002156
Iteration 105/1000 | Loss: 0.00002156
Iteration 106/1000 | Loss: 0.00002156
Iteration 107/1000 | Loss: 0.00002156
Iteration 108/1000 | Loss: 0.00002156
Iteration 109/1000 | Loss: 0.00002156
Iteration 110/1000 | Loss: 0.00002156
Iteration 111/1000 | Loss: 0.00002156
Iteration 112/1000 | Loss: 0.00002156
Iteration 113/1000 | Loss: 0.00002155
Iteration 114/1000 | Loss: 0.00002155
Iteration 115/1000 | Loss: 0.00002155
Iteration 116/1000 | Loss: 0.00002155
Iteration 117/1000 | Loss: 0.00002155
Iteration 118/1000 | Loss: 0.00002154
Iteration 119/1000 | Loss: 0.00002154
Iteration 120/1000 | Loss: 0.00002154
Iteration 121/1000 | Loss: 0.00002154
Iteration 122/1000 | Loss: 0.00002153
Iteration 123/1000 | Loss: 0.00002153
Iteration 124/1000 | Loss: 0.00002153
Iteration 125/1000 | Loss: 0.00002153
Iteration 126/1000 | Loss: 0.00002152
Iteration 127/1000 | Loss: 0.00002152
Iteration 128/1000 | Loss: 0.00002152
Iteration 129/1000 | Loss: 0.00002152
Iteration 130/1000 | Loss: 0.00002152
Iteration 131/1000 | Loss: 0.00002152
Iteration 132/1000 | Loss: 0.00002152
Iteration 133/1000 | Loss: 0.00002152
Iteration 134/1000 | Loss: 0.00002152
Iteration 135/1000 | Loss: 0.00002152
Iteration 136/1000 | Loss: 0.00002152
Iteration 137/1000 | Loss: 0.00002152
Iteration 138/1000 | Loss: 0.00002152
Iteration 139/1000 | Loss: 0.00002151
Iteration 140/1000 | Loss: 0.00002151
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00002151
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002151
Iteration 145/1000 | Loss: 0.00002151
Iteration 146/1000 | Loss: 0.00002151
Iteration 147/1000 | Loss: 0.00002150
Iteration 148/1000 | Loss: 0.00002150
Iteration 149/1000 | Loss: 0.00002150
Iteration 150/1000 | Loss: 0.00002150
Iteration 151/1000 | Loss: 0.00002150
Iteration 152/1000 | Loss: 0.00002150
Iteration 153/1000 | Loss: 0.00002150
Iteration 154/1000 | Loss: 0.00002150
Iteration 155/1000 | Loss: 0.00002149
Iteration 156/1000 | Loss: 0.00002149
Iteration 157/1000 | Loss: 0.00002149
Iteration 158/1000 | Loss: 0.00002149
Iteration 159/1000 | Loss: 0.00002149
Iteration 160/1000 | Loss: 0.00002149
Iteration 161/1000 | Loss: 0.00002149
Iteration 162/1000 | Loss: 0.00002149
Iteration 163/1000 | Loss: 0.00002149
Iteration 164/1000 | Loss: 0.00002149
Iteration 165/1000 | Loss: 0.00002149
Iteration 166/1000 | Loss: 0.00002149
Iteration 167/1000 | Loss: 0.00002148
Iteration 168/1000 | Loss: 0.00002148
Iteration 169/1000 | Loss: 0.00002148
Iteration 170/1000 | Loss: 0.00002148
Iteration 171/1000 | Loss: 0.00002148
Iteration 172/1000 | Loss: 0.00002148
Iteration 173/1000 | Loss: 0.00002148
Iteration 174/1000 | Loss: 0.00002148
Iteration 175/1000 | Loss: 0.00002148
Iteration 176/1000 | Loss: 0.00002148
Iteration 177/1000 | Loss: 0.00002148
Iteration 178/1000 | Loss: 0.00002148
Iteration 179/1000 | Loss: 0.00002148
Iteration 180/1000 | Loss: 0.00002148
Iteration 181/1000 | Loss: 0.00002148
Iteration 182/1000 | Loss: 0.00002148
Iteration 183/1000 | Loss: 0.00002148
Iteration 184/1000 | Loss: 0.00002148
Iteration 185/1000 | Loss: 0.00002148
Iteration 186/1000 | Loss: 0.00002148
Iteration 187/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.147795385099016e-05, 2.147795385099016e-05, 2.147795385099016e-05, 2.147795385099016e-05, 2.147795385099016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.147795385099016e-05

Optimization complete. Final v2v error: 3.7003865242004395 mm

Highest mean error: 6.044934272766113 mm for frame 158

Lowest mean error: 2.7262825965881348 mm for frame 12

Saving results

Total time: 90.13365435600281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596034
Iteration 2/25 | Loss: 0.00114768
Iteration 3/25 | Loss: 0.00107369
Iteration 4/25 | Loss: 0.00106165
Iteration 5/25 | Loss: 0.00105750
Iteration 6/25 | Loss: 0.00105657
Iteration 7/25 | Loss: 0.00105657
Iteration 8/25 | Loss: 0.00105657
Iteration 9/25 | Loss: 0.00105657
Iteration 10/25 | Loss: 0.00105657
Iteration 11/25 | Loss: 0.00105657
Iteration 12/25 | Loss: 0.00105657
Iteration 13/25 | Loss: 0.00105657
Iteration 14/25 | Loss: 0.00105657
Iteration 15/25 | Loss: 0.00105657
Iteration 16/25 | Loss: 0.00105657
Iteration 17/25 | Loss: 0.00105657
Iteration 18/25 | Loss: 0.00105657
Iteration 19/25 | Loss: 0.00105657
Iteration 20/25 | Loss: 0.00105657
Iteration 21/25 | Loss: 0.00105657
Iteration 22/25 | Loss: 0.00105657
Iteration 23/25 | Loss: 0.00105657
Iteration 24/25 | Loss: 0.00105657
Iteration 25/25 | Loss: 0.00105657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09578681
Iteration 2/25 | Loss: 0.00084534
Iteration 3/25 | Loss: 0.00084534
Iteration 4/25 | Loss: 0.00084534
Iteration 5/25 | Loss: 0.00084534
Iteration 6/25 | Loss: 0.00084534
Iteration 7/25 | Loss: 0.00084534
Iteration 8/25 | Loss: 0.00084534
Iteration 9/25 | Loss: 0.00084534
Iteration 10/25 | Loss: 0.00084534
Iteration 11/25 | Loss: 0.00084534
Iteration 12/25 | Loss: 0.00084534
Iteration 13/25 | Loss: 0.00084534
Iteration 14/25 | Loss: 0.00084534
Iteration 15/25 | Loss: 0.00084534
Iteration 16/25 | Loss: 0.00084534
Iteration 17/25 | Loss: 0.00084534
Iteration 18/25 | Loss: 0.00084534
Iteration 19/25 | Loss: 0.00084534
Iteration 20/25 | Loss: 0.00084534
Iteration 21/25 | Loss: 0.00084534
Iteration 22/25 | Loss: 0.00084534
Iteration 23/25 | Loss: 0.00084534
Iteration 24/25 | Loss: 0.00084534
Iteration 25/25 | Loss: 0.00084534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084534
Iteration 2/1000 | Loss: 0.00002143
Iteration 3/1000 | Loss: 0.00001473
Iteration 4/1000 | Loss: 0.00001238
Iteration 5/1000 | Loss: 0.00001147
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001061
Iteration 8/1000 | Loss: 0.00001030
Iteration 9/1000 | Loss: 0.00001018
Iteration 10/1000 | Loss: 0.00001008
Iteration 11/1000 | Loss: 0.00000994
Iteration 12/1000 | Loss: 0.00000987
Iteration 13/1000 | Loss: 0.00000986
Iteration 14/1000 | Loss: 0.00000985
Iteration 15/1000 | Loss: 0.00000984
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000980
Iteration 18/1000 | Loss: 0.00000980
Iteration 19/1000 | Loss: 0.00000975
Iteration 20/1000 | Loss: 0.00000973
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000970
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000966
Iteration 25/1000 | Loss: 0.00000966
Iteration 26/1000 | Loss: 0.00000965
Iteration 27/1000 | Loss: 0.00000963
Iteration 28/1000 | Loss: 0.00000960
Iteration 29/1000 | Loss: 0.00000960
Iteration 30/1000 | Loss: 0.00000959
Iteration 31/1000 | Loss: 0.00000959
Iteration 32/1000 | Loss: 0.00000956
Iteration 33/1000 | Loss: 0.00000955
Iteration 34/1000 | Loss: 0.00000955
Iteration 35/1000 | Loss: 0.00000954
Iteration 36/1000 | Loss: 0.00000953
Iteration 37/1000 | Loss: 0.00000953
Iteration 38/1000 | Loss: 0.00000953
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000950
Iteration 42/1000 | Loss: 0.00000950
Iteration 43/1000 | Loss: 0.00000950
Iteration 44/1000 | Loss: 0.00000950
Iteration 45/1000 | Loss: 0.00000949
Iteration 46/1000 | Loss: 0.00000949
Iteration 47/1000 | Loss: 0.00000948
Iteration 48/1000 | Loss: 0.00000948
Iteration 49/1000 | Loss: 0.00000947
Iteration 50/1000 | Loss: 0.00000947
Iteration 51/1000 | Loss: 0.00000947
Iteration 52/1000 | Loss: 0.00000947
Iteration 53/1000 | Loss: 0.00000946
Iteration 54/1000 | Loss: 0.00000946
Iteration 55/1000 | Loss: 0.00000945
Iteration 56/1000 | Loss: 0.00000945
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000944
Iteration 59/1000 | Loss: 0.00000943
Iteration 60/1000 | Loss: 0.00000943
Iteration 61/1000 | Loss: 0.00000943
Iteration 62/1000 | Loss: 0.00000943
Iteration 63/1000 | Loss: 0.00000943
Iteration 64/1000 | Loss: 0.00000942
Iteration 65/1000 | Loss: 0.00000942
Iteration 66/1000 | Loss: 0.00000941
Iteration 67/1000 | Loss: 0.00000941
Iteration 68/1000 | Loss: 0.00000941
Iteration 69/1000 | Loss: 0.00000941
Iteration 70/1000 | Loss: 0.00000941
Iteration 71/1000 | Loss: 0.00000940
Iteration 72/1000 | Loss: 0.00000940
Iteration 73/1000 | Loss: 0.00000940
Iteration 74/1000 | Loss: 0.00000940
Iteration 75/1000 | Loss: 0.00000939
Iteration 76/1000 | Loss: 0.00000939
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000939
Iteration 79/1000 | Loss: 0.00000939
Iteration 80/1000 | Loss: 0.00000938
Iteration 81/1000 | Loss: 0.00000938
Iteration 82/1000 | Loss: 0.00000938
Iteration 83/1000 | Loss: 0.00000938
Iteration 84/1000 | Loss: 0.00000938
Iteration 85/1000 | Loss: 0.00000937
Iteration 86/1000 | Loss: 0.00000937
Iteration 87/1000 | Loss: 0.00000937
Iteration 88/1000 | Loss: 0.00000936
Iteration 89/1000 | Loss: 0.00000936
Iteration 90/1000 | Loss: 0.00000936
Iteration 91/1000 | Loss: 0.00000936
Iteration 92/1000 | Loss: 0.00000936
Iteration 93/1000 | Loss: 0.00000936
Iteration 94/1000 | Loss: 0.00000936
Iteration 95/1000 | Loss: 0.00000936
Iteration 96/1000 | Loss: 0.00000936
Iteration 97/1000 | Loss: 0.00000936
Iteration 98/1000 | Loss: 0.00000935
Iteration 99/1000 | Loss: 0.00000935
Iteration 100/1000 | Loss: 0.00000935
Iteration 101/1000 | Loss: 0.00000935
Iteration 102/1000 | Loss: 0.00000935
Iteration 103/1000 | Loss: 0.00000935
Iteration 104/1000 | Loss: 0.00000935
Iteration 105/1000 | Loss: 0.00000934
Iteration 106/1000 | Loss: 0.00000934
Iteration 107/1000 | Loss: 0.00000933
Iteration 108/1000 | Loss: 0.00000933
Iteration 109/1000 | Loss: 0.00000933
Iteration 110/1000 | Loss: 0.00000932
Iteration 111/1000 | Loss: 0.00000932
Iteration 112/1000 | Loss: 0.00000932
Iteration 113/1000 | Loss: 0.00000931
Iteration 114/1000 | Loss: 0.00000931
Iteration 115/1000 | Loss: 0.00000930
Iteration 116/1000 | Loss: 0.00000930
Iteration 117/1000 | Loss: 0.00000930
Iteration 118/1000 | Loss: 0.00000929
Iteration 119/1000 | Loss: 0.00000929
Iteration 120/1000 | Loss: 0.00000929
Iteration 121/1000 | Loss: 0.00000929
Iteration 122/1000 | Loss: 0.00000929
Iteration 123/1000 | Loss: 0.00000929
Iteration 124/1000 | Loss: 0.00000929
Iteration 125/1000 | Loss: 0.00000929
Iteration 126/1000 | Loss: 0.00000929
Iteration 127/1000 | Loss: 0.00000929
Iteration 128/1000 | Loss: 0.00000929
Iteration 129/1000 | Loss: 0.00000929
Iteration 130/1000 | Loss: 0.00000929
Iteration 131/1000 | Loss: 0.00000928
Iteration 132/1000 | Loss: 0.00000928
Iteration 133/1000 | Loss: 0.00000927
Iteration 134/1000 | Loss: 0.00000927
Iteration 135/1000 | Loss: 0.00000927
Iteration 136/1000 | Loss: 0.00000926
Iteration 137/1000 | Loss: 0.00000926
Iteration 138/1000 | Loss: 0.00000926
Iteration 139/1000 | Loss: 0.00000926
Iteration 140/1000 | Loss: 0.00000925
Iteration 141/1000 | Loss: 0.00000925
Iteration 142/1000 | Loss: 0.00000925
Iteration 143/1000 | Loss: 0.00000925
Iteration 144/1000 | Loss: 0.00000925
Iteration 145/1000 | Loss: 0.00000925
Iteration 146/1000 | Loss: 0.00000925
Iteration 147/1000 | Loss: 0.00000925
Iteration 148/1000 | Loss: 0.00000925
Iteration 149/1000 | Loss: 0.00000925
Iteration 150/1000 | Loss: 0.00000925
Iteration 151/1000 | Loss: 0.00000925
Iteration 152/1000 | Loss: 0.00000925
Iteration 153/1000 | Loss: 0.00000925
Iteration 154/1000 | Loss: 0.00000925
Iteration 155/1000 | Loss: 0.00000925
Iteration 156/1000 | Loss: 0.00000925
Iteration 157/1000 | Loss: 0.00000925
Iteration 158/1000 | Loss: 0.00000925
Iteration 159/1000 | Loss: 0.00000925
Iteration 160/1000 | Loss: 0.00000925
Iteration 161/1000 | Loss: 0.00000925
Iteration 162/1000 | Loss: 0.00000925
Iteration 163/1000 | Loss: 0.00000925
Iteration 164/1000 | Loss: 0.00000925
Iteration 165/1000 | Loss: 0.00000925
Iteration 166/1000 | Loss: 0.00000925
Iteration 167/1000 | Loss: 0.00000925
Iteration 168/1000 | Loss: 0.00000925
Iteration 169/1000 | Loss: 0.00000925
Iteration 170/1000 | Loss: 0.00000925
Iteration 171/1000 | Loss: 0.00000925
Iteration 172/1000 | Loss: 0.00000925
Iteration 173/1000 | Loss: 0.00000925
Iteration 174/1000 | Loss: 0.00000925
Iteration 175/1000 | Loss: 0.00000925
Iteration 176/1000 | Loss: 0.00000925
Iteration 177/1000 | Loss: 0.00000925
Iteration 178/1000 | Loss: 0.00000925
Iteration 179/1000 | Loss: 0.00000925
Iteration 180/1000 | Loss: 0.00000925
Iteration 181/1000 | Loss: 0.00000925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [9.247326488548424e-06, 9.247326488548424e-06, 9.247326488548424e-06, 9.247326488548424e-06, 9.247326488548424e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.247326488548424e-06

Optimization complete. Final v2v error: 2.6204307079315186 mm

Highest mean error: 2.8861517906188965 mm for frame 59

Lowest mean error: 2.4628849029541016 mm for frame 12

Saving results

Total time: 38.64957618713379
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759458
Iteration 2/25 | Loss: 0.00117997
Iteration 3/25 | Loss: 0.00107128
Iteration 4/25 | Loss: 0.00105845
Iteration 5/25 | Loss: 0.00105475
Iteration 6/25 | Loss: 0.00105402
Iteration 7/25 | Loss: 0.00105402
Iteration 8/25 | Loss: 0.00105402
Iteration 9/25 | Loss: 0.00105402
Iteration 10/25 | Loss: 0.00105402
Iteration 11/25 | Loss: 0.00105402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010540209477767348, 0.0010540209477767348, 0.0010540209477767348, 0.0010540209477767348, 0.0010540209477767348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010540209477767348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90981448
Iteration 2/25 | Loss: 0.00084124
Iteration 3/25 | Loss: 0.00084124
Iteration 4/25 | Loss: 0.00084123
Iteration 5/25 | Loss: 0.00084123
Iteration 6/25 | Loss: 0.00084123
Iteration 7/25 | Loss: 0.00084123
Iteration 8/25 | Loss: 0.00084123
Iteration 9/25 | Loss: 0.00084123
Iteration 10/25 | Loss: 0.00084123
Iteration 11/25 | Loss: 0.00084123
Iteration 12/25 | Loss: 0.00084123
Iteration 13/25 | Loss: 0.00084123
Iteration 14/25 | Loss: 0.00084123
Iteration 15/25 | Loss: 0.00084123
Iteration 16/25 | Loss: 0.00084123
Iteration 17/25 | Loss: 0.00084123
Iteration 18/25 | Loss: 0.00084123
Iteration 19/25 | Loss: 0.00084123
Iteration 20/25 | Loss: 0.00084123
Iteration 21/25 | Loss: 0.00084123
Iteration 22/25 | Loss: 0.00084123
Iteration 23/25 | Loss: 0.00084123
Iteration 24/25 | Loss: 0.00084123
Iteration 25/25 | Loss: 0.00084123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084123
Iteration 2/1000 | Loss: 0.00001983
Iteration 3/1000 | Loss: 0.00001447
Iteration 4/1000 | Loss: 0.00001225
Iteration 5/1000 | Loss: 0.00001158
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001054
Iteration 8/1000 | Loss: 0.00001024
Iteration 9/1000 | Loss: 0.00001024
Iteration 10/1000 | Loss: 0.00001023
Iteration 11/1000 | Loss: 0.00001003
Iteration 12/1000 | Loss: 0.00000981
Iteration 13/1000 | Loss: 0.00000980
Iteration 14/1000 | Loss: 0.00000979
Iteration 15/1000 | Loss: 0.00000977
Iteration 16/1000 | Loss: 0.00000974
Iteration 17/1000 | Loss: 0.00000973
Iteration 18/1000 | Loss: 0.00000970
Iteration 19/1000 | Loss: 0.00000968
Iteration 20/1000 | Loss: 0.00000967
Iteration 21/1000 | Loss: 0.00000967
Iteration 22/1000 | Loss: 0.00000966
Iteration 23/1000 | Loss: 0.00000965
Iteration 24/1000 | Loss: 0.00000958
Iteration 25/1000 | Loss: 0.00000957
Iteration 26/1000 | Loss: 0.00000956
Iteration 27/1000 | Loss: 0.00000956
Iteration 28/1000 | Loss: 0.00000952
Iteration 29/1000 | Loss: 0.00000950
Iteration 30/1000 | Loss: 0.00000946
Iteration 31/1000 | Loss: 0.00000945
Iteration 32/1000 | Loss: 0.00000944
Iteration 33/1000 | Loss: 0.00000943
Iteration 34/1000 | Loss: 0.00000941
Iteration 35/1000 | Loss: 0.00000940
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000938
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000936
Iteration 48/1000 | Loss: 0.00000935
Iteration 49/1000 | Loss: 0.00000935
Iteration 50/1000 | Loss: 0.00000935
Iteration 51/1000 | Loss: 0.00000934
Iteration 52/1000 | Loss: 0.00000934
Iteration 53/1000 | Loss: 0.00000933
Iteration 54/1000 | Loss: 0.00000931
Iteration 55/1000 | Loss: 0.00000930
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000930
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000927
Iteration 63/1000 | Loss: 0.00000927
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000926
Iteration 66/1000 | Loss: 0.00000926
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000923
Iteration 71/1000 | Loss: 0.00000923
Iteration 72/1000 | Loss: 0.00000923
Iteration 73/1000 | Loss: 0.00000923
Iteration 74/1000 | Loss: 0.00000922
Iteration 75/1000 | Loss: 0.00000922
Iteration 76/1000 | Loss: 0.00000922
Iteration 77/1000 | Loss: 0.00000921
Iteration 78/1000 | Loss: 0.00000921
Iteration 79/1000 | Loss: 0.00000921
Iteration 80/1000 | Loss: 0.00000921
Iteration 81/1000 | Loss: 0.00000920
Iteration 82/1000 | Loss: 0.00000920
Iteration 83/1000 | Loss: 0.00000919
Iteration 84/1000 | Loss: 0.00000919
Iteration 85/1000 | Loss: 0.00000919
Iteration 86/1000 | Loss: 0.00000918
Iteration 87/1000 | Loss: 0.00000918
Iteration 88/1000 | Loss: 0.00000918
Iteration 89/1000 | Loss: 0.00000917
Iteration 90/1000 | Loss: 0.00000917
Iteration 91/1000 | Loss: 0.00000917
Iteration 92/1000 | Loss: 0.00000916
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000916
Iteration 95/1000 | Loss: 0.00000916
Iteration 96/1000 | Loss: 0.00000916
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000915
Iteration 100/1000 | Loss: 0.00000915
Iteration 101/1000 | Loss: 0.00000914
Iteration 102/1000 | Loss: 0.00000914
Iteration 103/1000 | Loss: 0.00000914
Iteration 104/1000 | Loss: 0.00000914
Iteration 105/1000 | Loss: 0.00000914
Iteration 106/1000 | Loss: 0.00000914
Iteration 107/1000 | Loss: 0.00000914
Iteration 108/1000 | Loss: 0.00000914
Iteration 109/1000 | Loss: 0.00000914
Iteration 110/1000 | Loss: 0.00000914
Iteration 111/1000 | Loss: 0.00000914
Iteration 112/1000 | Loss: 0.00000914
Iteration 113/1000 | Loss: 0.00000914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [9.137178494711407e-06, 9.137178494711407e-06, 9.137178494711407e-06, 9.137178494711407e-06, 9.137178494711407e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.137178494711407e-06

Optimization complete. Final v2v error: 2.627042055130005 mm

Highest mean error: 2.766070604324341 mm for frame 39

Lowest mean error: 2.498364210128784 mm for frame 0

Saving results

Total time: 35.21949577331543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787067
Iteration 2/25 | Loss: 0.00121768
Iteration 3/25 | Loss: 0.00111184
Iteration 4/25 | Loss: 0.00109166
Iteration 5/25 | Loss: 0.00108546
Iteration 6/25 | Loss: 0.00108444
Iteration 7/25 | Loss: 0.00108444
Iteration 8/25 | Loss: 0.00108444
Iteration 9/25 | Loss: 0.00108444
Iteration 10/25 | Loss: 0.00108444
Iteration 11/25 | Loss: 0.00108444
Iteration 12/25 | Loss: 0.00108444
Iteration 13/25 | Loss: 0.00108444
Iteration 14/25 | Loss: 0.00108444
Iteration 15/25 | Loss: 0.00108444
Iteration 16/25 | Loss: 0.00108444
Iteration 17/25 | Loss: 0.00108444
Iteration 18/25 | Loss: 0.00108444
Iteration 19/25 | Loss: 0.00108444
Iteration 20/25 | Loss: 0.00108444
Iteration 21/25 | Loss: 0.00108444
Iteration 22/25 | Loss: 0.00108444
Iteration 23/25 | Loss: 0.00108444
Iteration 24/25 | Loss: 0.00108444
Iteration 25/25 | Loss: 0.00108444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31856883
Iteration 2/25 | Loss: 0.00098331
Iteration 3/25 | Loss: 0.00098331
Iteration 4/25 | Loss: 0.00098331
Iteration 5/25 | Loss: 0.00098331
Iteration 6/25 | Loss: 0.00098331
Iteration 7/25 | Loss: 0.00098331
Iteration 8/25 | Loss: 0.00098331
Iteration 9/25 | Loss: 0.00098331
Iteration 10/25 | Loss: 0.00098331
Iteration 11/25 | Loss: 0.00098331
Iteration 12/25 | Loss: 0.00098331
Iteration 13/25 | Loss: 0.00098331
Iteration 14/25 | Loss: 0.00098331
Iteration 15/25 | Loss: 0.00098331
Iteration 16/25 | Loss: 0.00098331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000983305275440216, 0.000983305275440216, 0.000983305275440216, 0.000983305275440216, 0.000983305275440216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000983305275440216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098331
Iteration 2/1000 | Loss: 0.00003525
Iteration 3/1000 | Loss: 0.00002495
Iteration 4/1000 | Loss: 0.00002090
Iteration 5/1000 | Loss: 0.00001942
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001775
Iteration 8/1000 | Loss: 0.00001724
Iteration 9/1000 | Loss: 0.00001685
Iteration 10/1000 | Loss: 0.00001658
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001572
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001555
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001532
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001525
Iteration 22/1000 | Loss: 0.00001521
Iteration 23/1000 | Loss: 0.00001521
Iteration 24/1000 | Loss: 0.00001521
Iteration 25/1000 | Loss: 0.00001520
Iteration 26/1000 | Loss: 0.00001519
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001518
Iteration 30/1000 | Loss: 0.00001518
Iteration 31/1000 | Loss: 0.00001518
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001518
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001517
Iteration 36/1000 | Loss: 0.00001517
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001516
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001514
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001513
Iteration 46/1000 | Loss: 0.00001513
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001511
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001511
Iteration 53/1000 | Loss: 0.00001510
Iteration 54/1000 | Loss: 0.00001510
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001508
Iteration 59/1000 | Loss: 0.00001508
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001507
Iteration 62/1000 | Loss: 0.00001507
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001506
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001505
Iteration 68/1000 | Loss: 0.00001505
Iteration 69/1000 | Loss: 0.00001505
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001504
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001503
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001501
Iteration 82/1000 | Loss: 0.00001501
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001500
Iteration 85/1000 | Loss: 0.00001500
Iteration 86/1000 | Loss: 0.00001500
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001500
Iteration 90/1000 | Loss: 0.00001500
Iteration 91/1000 | Loss: 0.00001500
Iteration 92/1000 | Loss: 0.00001500
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001499
Iteration 95/1000 | Loss: 0.00001499
Iteration 96/1000 | Loss: 0.00001499
Iteration 97/1000 | Loss: 0.00001499
Iteration 98/1000 | Loss: 0.00001499
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001497
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001497
Iteration 115/1000 | Loss: 0.00001497
Iteration 116/1000 | Loss: 0.00001497
Iteration 117/1000 | Loss: 0.00001497
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001496
Iteration 122/1000 | Loss: 0.00001496
Iteration 123/1000 | Loss: 0.00001496
Iteration 124/1000 | Loss: 0.00001496
Iteration 125/1000 | Loss: 0.00001496
Iteration 126/1000 | Loss: 0.00001496
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001496
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001496
Iteration 131/1000 | Loss: 0.00001496
Iteration 132/1000 | Loss: 0.00001496
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001495
Iteration 136/1000 | Loss: 0.00001495
Iteration 137/1000 | Loss: 0.00001495
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001495
Iteration 144/1000 | Loss: 0.00001495
Iteration 145/1000 | Loss: 0.00001495
Iteration 146/1000 | Loss: 0.00001495
Iteration 147/1000 | Loss: 0.00001495
Iteration 148/1000 | Loss: 0.00001495
Iteration 149/1000 | Loss: 0.00001495
Iteration 150/1000 | Loss: 0.00001494
Iteration 151/1000 | Loss: 0.00001494
Iteration 152/1000 | Loss: 0.00001494
Iteration 153/1000 | Loss: 0.00001494
Iteration 154/1000 | Loss: 0.00001494
Iteration 155/1000 | Loss: 0.00001494
Iteration 156/1000 | Loss: 0.00001494
Iteration 157/1000 | Loss: 0.00001494
Iteration 158/1000 | Loss: 0.00001494
Iteration 159/1000 | Loss: 0.00001494
Iteration 160/1000 | Loss: 0.00001494
Iteration 161/1000 | Loss: 0.00001494
Iteration 162/1000 | Loss: 0.00001494
Iteration 163/1000 | Loss: 0.00001494
Iteration 164/1000 | Loss: 0.00001494
Iteration 165/1000 | Loss: 0.00001494
Iteration 166/1000 | Loss: 0.00001494
Iteration 167/1000 | Loss: 0.00001494
Iteration 168/1000 | Loss: 0.00001494
Iteration 169/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4938622371118981e-05, 1.4938622371118981e-05, 1.4938622371118981e-05, 1.4938622371118981e-05, 1.4938622371118981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4938622371118981e-05

Optimization complete. Final v2v error: 3.2900619506835938 mm

Highest mean error: 4.107223033905029 mm for frame 239

Lowest mean error: 3.0171403884887695 mm for frame 36

Saving results

Total time: 47.47641038894653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771641
Iteration 2/25 | Loss: 0.00144940
Iteration 3/25 | Loss: 0.00128114
Iteration 4/25 | Loss: 0.00128551
Iteration 5/25 | Loss: 0.00127533
Iteration 6/25 | Loss: 0.00119062
Iteration 7/25 | Loss: 0.00117306
Iteration 8/25 | Loss: 0.00115613
Iteration 9/25 | Loss: 0.00114862
Iteration 10/25 | Loss: 0.00114159
Iteration 11/25 | Loss: 0.00113874
Iteration 12/25 | Loss: 0.00113988
Iteration 13/25 | Loss: 0.00113687
Iteration 14/25 | Loss: 0.00113878
Iteration 15/25 | Loss: 0.00113469
Iteration 16/25 | Loss: 0.00112943
Iteration 17/25 | Loss: 0.00112650
Iteration 18/25 | Loss: 0.00112846
Iteration 19/25 | Loss: 0.00112325
Iteration 20/25 | Loss: 0.00112162
Iteration 21/25 | Loss: 0.00112390
Iteration 22/25 | Loss: 0.00112196
Iteration 23/25 | Loss: 0.00112410
Iteration 24/25 | Loss: 0.00112247
Iteration 25/25 | Loss: 0.00111968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60369349
Iteration 2/25 | Loss: 0.00105464
Iteration 3/25 | Loss: 0.00105464
Iteration 4/25 | Loss: 0.00105464
Iteration 5/25 | Loss: 0.00105464
Iteration 6/25 | Loss: 0.00105464
Iteration 7/25 | Loss: 0.00105464
Iteration 8/25 | Loss: 0.00105464
Iteration 9/25 | Loss: 0.00105464
Iteration 10/25 | Loss: 0.00105464
Iteration 11/25 | Loss: 0.00105464
Iteration 12/25 | Loss: 0.00105464
Iteration 13/25 | Loss: 0.00105464
Iteration 14/25 | Loss: 0.00105464
Iteration 15/25 | Loss: 0.00105464
Iteration 16/25 | Loss: 0.00105464
Iteration 17/25 | Loss: 0.00105464
Iteration 18/25 | Loss: 0.00105464
Iteration 19/25 | Loss: 0.00105464
Iteration 20/25 | Loss: 0.00105464
Iteration 21/25 | Loss: 0.00105464
Iteration 22/25 | Loss: 0.00105464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010546352714300156, 0.0010546352714300156, 0.0010546352714300156, 0.0010546352714300156, 0.0010546352714300156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010546352714300156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105464
Iteration 2/1000 | Loss: 0.00004483
Iteration 3/1000 | Loss: 0.00002594
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001881
Iteration 7/1000 | Loss: 0.00001803
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001678
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001629
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001614
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001608
Iteration 22/1000 | Loss: 0.00001604
Iteration 23/1000 | Loss: 0.00001604
Iteration 24/1000 | Loss: 0.00001603
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001600
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001599
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001596
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001591
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001587
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001586
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001582
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001580
Iteration 72/1000 | Loss: 0.00001580
Iteration 73/1000 | Loss: 0.00001579
Iteration 74/1000 | Loss: 0.00001579
Iteration 75/1000 | Loss: 0.00001578
Iteration 76/1000 | Loss: 0.00001578
Iteration 77/1000 | Loss: 0.00001578
Iteration 78/1000 | Loss: 0.00001578
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001576
Iteration 82/1000 | Loss: 0.00001576
Iteration 83/1000 | Loss: 0.00001576
Iteration 84/1000 | Loss: 0.00001576
Iteration 85/1000 | Loss: 0.00001576
Iteration 86/1000 | Loss: 0.00001576
Iteration 87/1000 | Loss: 0.00001576
Iteration 88/1000 | Loss: 0.00001575
Iteration 89/1000 | Loss: 0.00001575
Iteration 90/1000 | Loss: 0.00001575
Iteration 91/1000 | Loss: 0.00001575
Iteration 92/1000 | Loss: 0.00001575
Iteration 93/1000 | Loss: 0.00001574
Iteration 94/1000 | Loss: 0.00001574
Iteration 95/1000 | Loss: 0.00001574
Iteration 96/1000 | Loss: 0.00001574
Iteration 97/1000 | Loss: 0.00001574
Iteration 98/1000 | Loss: 0.00001574
Iteration 99/1000 | Loss: 0.00001573
Iteration 100/1000 | Loss: 0.00001573
Iteration 101/1000 | Loss: 0.00001573
Iteration 102/1000 | Loss: 0.00001572
Iteration 103/1000 | Loss: 0.00001572
Iteration 104/1000 | Loss: 0.00001572
Iteration 105/1000 | Loss: 0.00001571
Iteration 106/1000 | Loss: 0.00001571
Iteration 107/1000 | Loss: 0.00001571
Iteration 108/1000 | Loss: 0.00001570
Iteration 109/1000 | Loss: 0.00001570
Iteration 110/1000 | Loss: 0.00001570
Iteration 111/1000 | Loss: 0.00001570
Iteration 112/1000 | Loss: 0.00001569
Iteration 113/1000 | Loss: 0.00001569
Iteration 114/1000 | Loss: 0.00001569
Iteration 115/1000 | Loss: 0.00001569
Iteration 116/1000 | Loss: 0.00001569
Iteration 117/1000 | Loss: 0.00001569
Iteration 118/1000 | Loss: 0.00001569
Iteration 119/1000 | Loss: 0.00001569
Iteration 120/1000 | Loss: 0.00001569
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001566
Iteration 131/1000 | Loss: 0.00001566
Iteration 132/1000 | Loss: 0.00001566
Iteration 133/1000 | Loss: 0.00001566
Iteration 134/1000 | Loss: 0.00001565
Iteration 135/1000 | Loss: 0.00001565
Iteration 136/1000 | Loss: 0.00001565
Iteration 137/1000 | Loss: 0.00001565
Iteration 138/1000 | Loss: 0.00001564
Iteration 139/1000 | Loss: 0.00001564
Iteration 140/1000 | Loss: 0.00001564
Iteration 141/1000 | Loss: 0.00001564
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001563
Iteration 144/1000 | Loss: 0.00001563
Iteration 145/1000 | Loss: 0.00001563
Iteration 146/1000 | Loss: 0.00001562
Iteration 147/1000 | Loss: 0.00001562
Iteration 148/1000 | Loss: 0.00001562
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001562
Iteration 151/1000 | Loss: 0.00001562
Iteration 152/1000 | Loss: 0.00001562
Iteration 153/1000 | Loss: 0.00001561
Iteration 154/1000 | Loss: 0.00001561
Iteration 155/1000 | Loss: 0.00001561
Iteration 156/1000 | Loss: 0.00001561
Iteration 157/1000 | Loss: 0.00001561
Iteration 158/1000 | Loss: 0.00001561
Iteration 159/1000 | Loss: 0.00001561
Iteration 160/1000 | Loss: 0.00001561
Iteration 161/1000 | Loss: 0.00001561
Iteration 162/1000 | Loss: 0.00001561
Iteration 163/1000 | Loss: 0.00001561
Iteration 164/1000 | Loss: 0.00001560
Iteration 165/1000 | Loss: 0.00001560
Iteration 166/1000 | Loss: 0.00001560
Iteration 167/1000 | Loss: 0.00001560
Iteration 168/1000 | Loss: 0.00001560
Iteration 169/1000 | Loss: 0.00001560
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001559
Iteration 173/1000 | Loss: 0.00001559
Iteration 174/1000 | Loss: 0.00001559
Iteration 175/1000 | Loss: 0.00001559
Iteration 176/1000 | Loss: 0.00001559
Iteration 177/1000 | Loss: 0.00001558
Iteration 178/1000 | Loss: 0.00001558
Iteration 179/1000 | Loss: 0.00001558
Iteration 180/1000 | Loss: 0.00001558
Iteration 181/1000 | Loss: 0.00001558
Iteration 182/1000 | Loss: 0.00001557
Iteration 183/1000 | Loss: 0.00001557
Iteration 184/1000 | Loss: 0.00001557
Iteration 185/1000 | Loss: 0.00001557
Iteration 186/1000 | Loss: 0.00001557
Iteration 187/1000 | Loss: 0.00001557
Iteration 188/1000 | Loss: 0.00001557
Iteration 189/1000 | Loss: 0.00001557
Iteration 190/1000 | Loss: 0.00001556
Iteration 191/1000 | Loss: 0.00001556
Iteration 192/1000 | Loss: 0.00001556
Iteration 193/1000 | Loss: 0.00001556
Iteration 194/1000 | Loss: 0.00001556
Iteration 195/1000 | Loss: 0.00001556
Iteration 196/1000 | Loss: 0.00001556
Iteration 197/1000 | Loss: 0.00001555
Iteration 198/1000 | Loss: 0.00001555
Iteration 199/1000 | Loss: 0.00001555
Iteration 200/1000 | Loss: 0.00001555
Iteration 201/1000 | Loss: 0.00001555
Iteration 202/1000 | Loss: 0.00001555
Iteration 203/1000 | Loss: 0.00001555
Iteration 204/1000 | Loss: 0.00001554
Iteration 205/1000 | Loss: 0.00001554
Iteration 206/1000 | Loss: 0.00001554
Iteration 207/1000 | Loss: 0.00001554
Iteration 208/1000 | Loss: 0.00001554
Iteration 209/1000 | Loss: 0.00001554
Iteration 210/1000 | Loss: 0.00001554
Iteration 211/1000 | Loss: 0.00001554
Iteration 212/1000 | Loss: 0.00001554
Iteration 213/1000 | Loss: 0.00001554
Iteration 214/1000 | Loss: 0.00001554
Iteration 215/1000 | Loss: 0.00001554
Iteration 216/1000 | Loss: 0.00001554
Iteration 217/1000 | Loss: 0.00001554
Iteration 218/1000 | Loss: 0.00001554
Iteration 219/1000 | Loss: 0.00001554
Iteration 220/1000 | Loss: 0.00001553
Iteration 221/1000 | Loss: 0.00001553
Iteration 222/1000 | Loss: 0.00001553
Iteration 223/1000 | Loss: 0.00001553
Iteration 224/1000 | Loss: 0.00001553
Iteration 225/1000 | Loss: 0.00001553
Iteration 226/1000 | Loss: 0.00001553
Iteration 227/1000 | Loss: 0.00001553
Iteration 228/1000 | Loss: 0.00001553
Iteration 229/1000 | Loss: 0.00001553
Iteration 230/1000 | Loss: 0.00001553
Iteration 231/1000 | Loss: 0.00001553
Iteration 232/1000 | Loss: 0.00001553
Iteration 233/1000 | Loss: 0.00001553
Iteration 234/1000 | Loss: 0.00001553
Iteration 235/1000 | Loss: 0.00001553
Iteration 236/1000 | Loss: 0.00001553
Iteration 237/1000 | Loss: 0.00001553
Iteration 238/1000 | Loss: 0.00001553
Iteration 239/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.5530271411989816e-05, 1.5530271411989816e-05, 1.5530271411989816e-05, 1.5530271411989816e-05, 1.5530271411989816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5530271411989816e-05

Optimization complete. Final v2v error: 3.0916247367858887 mm

Highest mean error: 10.450255393981934 mm for frame 206

Lowest mean error: 2.3725504875183105 mm for frame 215

Saving results

Total time: 95.44290947914124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018522
Iteration 2/25 | Loss: 0.01018522
Iteration 3/25 | Loss: 0.00364883
Iteration 4/25 | Loss: 0.00245611
Iteration 5/25 | Loss: 0.00211425
Iteration 6/25 | Loss: 0.00193988
Iteration 7/25 | Loss: 0.00163486
Iteration 8/25 | Loss: 0.00146853
Iteration 9/25 | Loss: 0.00134410
Iteration 10/25 | Loss: 0.00128809
Iteration 11/25 | Loss: 0.00124398
Iteration 12/25 | Loss: 0.00122222
Iteration 13/25 | Loss: 0.00121562
Iteration 14/25 | Loss: 0.00121139
Iteration 15/25 | Loss: 0.00121172
Iteration 16/25 | Loss: 0.00120572
Iteration 17/25 | Loss: 0.00120376
Iteration 18/25 | Loss: 0.00120584
Iteration 19/25 | Loss: 0.00120171
Iteration 20/25 | Loss: 0.00120296
Iteration 21/25 | Loss: 0.00119895
Iteration 22/25 | Loss: 0.00119760
Iteration 23/25 | Loss: 0.00119729
Iteration 24/25 | Loss: 0.00119719
Iteration 25/25 | Loss: 0.00119719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37151849
Iteration 2/25 | Loss: 0.00098874
Iteration 3/25 | Loss: 0.00098873
Iteration 4/25 | Loss: 0.00098873
Iteration 5/25 | Loss: 0.00098873
Iteration 6/25 | Loss: 0.00098873
Iteration 7/25 | Loss: 0.00098873
Iteration 8/25 | Loss: 0.00098873
Iteration 9/25 | Loss: 0.00098873
Iteration 10/25 | Loss: 0.00098873
Iteration 11/25 | Loss: 0.00098873
Iteration 12/25 | Loss: 0.00098873
Iteration 13/25 | Loss: 0.00098873
Iteration 14/25 | Loss: 0.00098873
Iteration 15/25 | Loss: 0.00098873
Iteration 16/25 | Loss: 0.00098873
Iteration 17/25 | Loss: 0.00098873
Iteration 18/25 | Loss: 0.00098873
Iteration 19/25 | Loss: 0.00098873
Iteration 20/25 | Loss: 0.00098873
Iteration 21/25 | Loss: 0.00098873
Iteration 22/25 | Loss: 0.00098873
Iteration 23/25 | Loss: 0.00098873
Iteration 24/25 | Loss: 0.00098873
Iteration 25/25 | Loss: 0.00098873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009887265041470528, 0.0009887265041470528, 0.0009887265041470528, 0.0009887265041470528, 0.0009887265041470528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009887265041470528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098873
Iteration 2/1000 | Loss: 0.00036976
Iteration 3/1000 | Loss: 0.00006969
Iteration 4/1000 | Loss: 0.00006089
Iteration 5/1000 | Loss: 0.00005604
Iteration 6/1000 | Loss: 0.00005362
Iteration 7/1000 | Loss: 0.00005194
Iteration 8/1000 | Loss: 0.00005052
Iteration 9/1000 | Loss: 0.00004915
Iteration 10/1000 | Loss: 0.00024209
Iteration 11/1000 | Loss: 0.00441896
Iteration 12/1000 | Loss: 0.00041272
Iteration 13/1000 | Loss: 0.00079073
Iteration 14/1000 | Loss: 0.00012193
Iteration 15/1000 | Loss: 0.00008602
Iteration 16/1000 | Loss: 0.00006835
Iteration 17/1000 | Loss: 0.00009276
Iteration 18/1000 | Loss: 0.00003284
Iteration 19/1000 | Loss: 0.00006868
Iteration 20/1000 | Loss: 0.00002348
Iteration 21/1000 | Loss: 0.00002183
Iteration 22/1000 | Loss: 0.00002006
Iteration 23/1000 | Loss: 0.00001867
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001621
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001500
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001480
Iteration 32/1000 | Loss: 0.00001479
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001476
Iteration 35/1000 | Loss: 0.00001476
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001475
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001470
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001462
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001461
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001460
Iteration 53/1000 | Loss: 0.00001460
Iteration 54/1000 | Loss: 0.00001460
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001459
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001459
Iteration 61/1000 | Loss: 0.00001459
Iteration 62/1000 | Loss: 0.00001458
Iteration 63/1000 | Loss: 0.00001458
Iteration 64/1000 | Loss: 0.00001458
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001455
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001454
Iteration 85/1000 | Loss: 0.00001454
Iteration 86/1000 | Loss: 0.00001454
Iteration 87/1000 | Loss: 0.00001454
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.4522127457894385e-05, 1.4522127457894385e-05, 1.4522127457894385e-05, 1.4522127457894385e-05, 1.4522127457894385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4522127457894385e-05

Optimization complete. Final v2v error: 3.252129554748535 mm

Highest mean error: 3.9735641479492188 mm for frame 44

Lowest mean error: 3.01576566696167 mm for frame 238

Saving results

Total time: 102.53828811645508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891122
Iteration 2/25 | Loss: 0.00202053
Iteration 3/25 | Loss: 0.00155361
Iteration 4/25 | Loss: 0.00138974
Iteration 5/25 | Loss: 0.00139622
Iteration 6/25 | Loss: 0.00131542
Iteration 7/25 | Loss: 0.00127996
Iteration 8/25 | Loss: 0.00130752
Iteration 9/25 | Loss: 0.00121621
Iteration 10/25 | Loss: 0.00120096
Iteration 11/25 | Loss: 0.00118816
Iteration 12/25 | Loss: 0.00118265
Iteration 13/25 | Loss: 0.00119023
Iteration 14/25 | Loss: 0.00117508
Iteration 15/25 | Loss: 0.00117017
Iteration 16/25 | Loss: 0.00117453
Iteration 17/25 | Loss: 0.00116720
Iteration 18/25 | Loss: 0.00116486
Iteration 19/25 | Loss: 0.00116394
Iteration 20/25 | Loss: 0.00116714
Iteration 21/25 | Loss: 0.00116568
Iteration 22/25 | Loss: 0.00116427
Iteration 23/25 | Loss: 0.00116194
Iteration 24/25 | Loss: 0.00116126
Iteration 25/25 | Loss: 0.00115752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48153889
Iteration 2/25 | Loss: 0.00079276
Iteration 3/25 | Loss: 0.00079274
Iteration 4/25 | Loss: 0.00079274
Iteration 5/25 | Loss: 0.00079274
Iteration 6/25 | Loss: 0.00079274
Iteration 7/25 | Loss: 0.00079274
Iteration 8/25 | Loss: 0.00079274
Iteration 9/25 | Loss: 0.00079274
Iteration 10/25 | Loss: 0.00079274
Iteration 11/25 | Loss: 0.00079274
Iteration 12/25 | Loss: 0.00079274
Iteration 13/25 | Loss: 0.00079274
Iteration 14/25 | Loss: 0.00079274
Iteration 15/25 | Loss: 0.00079274
Iteration 16/25 | Loss: 0.00079274
Iteration 17/25 | Loss: 0.00079274
Iteration 18/25 | Loss: 0.00079274
Iteration 19/25 | Loss: 0.00079274
Iteration 20/25 | Loss: 0.00079274
Iteration 21/25 | Loss: 0.00079274
Iteration 22/25 | Loss: 0.00079274
Iteration 23/25 | Loss: 0.00079274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000792739971075207, 0.000792739971075207, 0.000792739971075207, 0.000792739971075207, 0.000792739971075207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000792739971075207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079274
Iteration 2/1000 | Loss: 0.00005801
Iteration 3/1000 | Loss: 0.00003869
Iteration 4/1000 | Loss: 0.00003401
Iteration 5/1000 | Loss: 0.00003150
Iteration 6/1000 | Loss: 0.00002969
Iteration 7/1000 | Loss: 0.00002843
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002713
Iteration 10/1000 | Loss: 0.00028120
Iteration 11/1000 | Loss: 0.00026209
Iteration 12/1000 | Loss: 0.00002947
Iteration 13/1000 | Loss: 0.00002557
Iteration 14/1000 | Loss: 0.00002208
Iteration 15/1000 | Loss: 0.00001998
Iteration 16/1000 | Loss: 0.00001850
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001717
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001684
Iteration 21/1000 | Loss: 0.00001659
Iteration 22/1000 | Loss: 0.00001636
Iteration 23/1000 | Loss: 0.00001616
Iteration 24/1000 | Loss: 0.00001607
Iteration 25/1000 | Loss: 0.00001606
Iteration 26/1000 | Loss: 0.00001605
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001593
Iteration 32/1000 | Loss: 0.00001592
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001589
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001586
Iteration 38/1000 | Loss: 0.00001586
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001585
Iteration 41/1000 | Loss: 0.00001584
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001583
Iteration 45/1000 | Loss: 0.00001583
Iteration 46/1000 | Loss: 0.00001583
Iteration 47/1000 | Loss: 0.00001583
Iteration 48/1000 | Loss: 0.00001582
Iteration 49/1000 | Loss: 0.00001582
Iteration 50/1000 | Loss: 0.00001581
Iteration 51/1000 | Loss: 0.00001581
Iteration 52/1000 | Loss: 0.00001581
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001580
Iteration 55/1000 | Loss: 0.00001580
Iteration 56/1000 | Loss: 0.00001580
Iteration 57/1000 | Loss: 0.00001579
Iteration 58/1000 | Loss: 0.00001579
Iteration 59/1000 | Loss: 0.00001579
Iteration 60/1000 | Loss: 0.00001578
Iteration 61/1000 | Loss: 0.00001578
Iteration 62/1000 | Loss: 0.00001578
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001578
Iteration 65/1000 | Loss: 0.00001578
Iteration 66/1000 | Loss: 0.00001578
Iteration 67/1000 | Loss: 0.00001578
Iteration 68/1000 | Loss: 0.00001578
Iteration 69/1000 | Loss: 0.00001578
Iteration 70/1000 | Loss: 0.00001577
Iteration 71/1000 | Loss: 0.00001577
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001577
Iteration 75/1000 | Loss: 0.00001577
Iteration 76/1000 | Loss: 0.00001577
Iteration 77/1000 | Loss: 0.00001577
Iteration 78/1000 | Loss: 0.00001577
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001577
Iteration 83/1000 | Loss: 0.00001577
Iteration 84/1000 | Loss: 0.00001577
Iteration 85/1000 | Loss: 0.00001577
Iteration 86/1000 | Loss: 0.00001577
Iteration 87/1000 | Loss: 0.00001577
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001577
Iteration 91/1000 | Loss: 0.00001577
Iteration 92/1000 | Loss: 0.00001577
Iteration 93/1000 | Loss: 0.00001577
Iteration 94/1000 | Loss: 0.00001577
Iteration 95/1000 | Loss: 0.00001577
Iteration 96/1000 | Loss: 0.00001577
Iteration 97/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.5766045180498622e-05, 1.5766045180498622e-05, 1.5766045180498622e-05, 1.5766045180498622e-05, 1.5766045180498622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5766045180498622e-05

Optimization complete. Final v2v error: 3.3321540355682373 mm

Highest mean error: 4.021675109863281 mm for frame 95

Lowest mean error: 2.719311237335205 mm for frame 142

Saving results

Total time: 83.94073414802551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00670532
Iteration 2/25 | Loss: 0.00119317
Iteration 3/25 | Loss: 0.00111098
Iteration 4/25 | Loss: 0.00109493
Iteration 5/25 | Loss: 0.00108893
Iteration 6/25 | Loss: 0.00108726
Iteration 7/25 | Loss: 0.00108726
Iteration 8/25 | Loss: 0.00108726
Iteration 9/25 | Loss: 0.00108726
Iteration 10/25 | Loss: 0.00108726
Iteration 11/25 | Loss: 0.00108726
Iteration 12/25 | Loss: 0.00108726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001087262644432485, 0.001087262644432485, 0.001087262644432485, 0.001087262644432485, 0.001087262644432485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001087262644432485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93584704
Iteration 2/25 | Loss: 0.00079476
Iteration 3/25 | Loss: 0.00079476
Iteration 4/25 | Loss: 0.00079476
Iteration 5/25 | Loss: 0.00079476
Iteration 6/25 | Loss: 0.00079476
Iteration 7/25 | Loss: 0.00079476
Iteration 8/25 | Loss: 0.00079476
Iteration 9/25 | Loss: 0.00079476
Iteration 10/25 | Loss: 0.00079476
Iteration 11/25 | Loss: 0.00079476
Iteration 12/25 | Loss: 0.00079476
Iteration 13/25 | Loss: 0.00079476
Iteration 14/25 | Loss: 0.00079476
Iteration 15/25 | Loss: 0.00079476
Iteration 16/25 | Loss: 0.00079476
Iteration 17/25 | Loss: 0.00079475
Iteration 18/25 | Loss: 0.00079476
Iteration 19/25 | Loss: 0.00079475
Iteration 20/25 | Loss: 0.00079475
Iteration 21/25 | Loss: 0.00079475
Iteration 22/25 | Loss: 0.00079475
Iteration 23/25 | Loss: 0.00079475
Iteration 24/25 | Loss: 0.00079475
Iteration 25/25 | Loss: 0.00079475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079475
Iteration 2/1000 | Loss: 0.00003522
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001583
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001278
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001235
Iteration 30/1000 | Loss: 0.00001234
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001233
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001227
Iteration 37/1000 | Loss: 0.00001227
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001219
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001213
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001212
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001212
Iteration 71/1000 | Loss: 0.00001212
Iteration 72/1000 | Loss: 0.00001212
Iteration 73/1000 | Loss: 0.00001212
Iteration 74/1000 | Loss: 0.00001212
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001201
Iteration 114/1000 | Loss: 0.00001201
Iteration 115/1000 | Loss: 0.00001201
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001199
Iteration 120/1000 | Loss: 0.00001199
Iteration 121/1000 | Loss: 0.00001199
Iteration 122/1000 | Loss: 0.00001199
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001196
Iteration 131/1000 | Loss: 0.00001196
Iteration 132/1000 | Loss: 0.00001196
Iteration 133/1000 | Loss: 0.00001196
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001196
Iteration 140/1000 | Loss: 0.00001196
Iteration 141/1000 | Loss: 0.00001196
Iteration 142/1000 | Loss: 0.00001196
Iteration 143/1000 | Loss: 0.00001196
Iteration 144/1000 | Loss: 0.00001196
Iteration 145/1000 | Loss: 0.00001196
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001195
Iteration 148/1000 | Loss: 0.00001195
Iteration 149/1000 | Loss: 0.00001195
Iteration 150/1000 | Loss: 0.00001195
Iteration 151/1000 | Loss: 0.00001195
Iteration 152/1000 | Loss: 0.00001195
Iteration 153/1000 | Loss: 0.00001195
Iteration 154/1000 | Loss: 0.00001195
Iteration 155/1000 | Loss: 0.00001195
Iteration 156/1000 | Loss: 0.00001195
Iteration 157/1000 | Loss: 0.00001195
Iteration 158/1000 | Loss: 0.00001195
Iteration 159/1000 | Loss: 0.00001195
Iteration 160/1000 | Loss: 0.00001195
Iteration 161/1000 | Loss: 0.00001195
Iteration 162/1000 | Loss: 0.00001194
Iteration 163/1000 | Loss: 0.00001194
Iteration 164/1000 | Loss: 0.00001194
Iteration 165/1000 | Loss: 0.00001194
Iteration 166/1000 | Loss: 0.00001194
Iteration 167/1000 | Loss: 0.00001194
Iteration 168/1000 | Loss: 0.00001194
Iteration 169/1000 | Loss: 0.00001194
Iteration 170/1000 | Loss: 0.00001194
Iteration 171/1000 | Loss: 0.00001194
Iteration 172/1000 | Loss: 0.00001194
Iteration 173/1000 | Loss: 0.00001194
Iteration 174/1000 | Loss: 0.00001194
Iteration 175/1000 | Loss: 0.00001194
Iteration 176/1000 | Loss: 0.00001194
Iteration 177/1000 | Loss: 0.00001194
Iteration 178/1000 | Loss: 0.00001194
Iteration 179/1000 | Loss: 0.00001194
Iteration 180/1000 | Loss: 0.00001194
Iteration 181/1000 | Loss: 0.00001194
Iteration 182/1000 | Loss: 0.00001194
Iteration 183/1000 | Loss: 0.00001194
Iteration 184/1000 | Loss: 0.00001194
Iteration 185/1000 | Loss: 0.00001194
Iteration 186/1000 | Loss: 0.00001194
Iteration 187/1000 | Loss: 0.00001194
Iteration 188/1000 | Loss: 0.00001194
Iteration 189/1000 | Loss: 0.00001194
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001194
Iteration 192/1000 | Loss: 0.00001194
Iteration 193/1000 | Loss: 0.00001194
Iteration 194/1000 | Loss: 0.00001194
Iteration 195/1000 | Loss: 0.00001194
Iteration 196/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.1939980140596163e-05, 1.1939980140596163e-05, 1.1939980140596163e-05, 1.1939980140596163e-05, 1.1939980140596163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1939980140596163e-05

Optimization complete. Final v2v error: 2.9844417572021484 mm

Highest mean error: 3.2989840507507324 mm for frame 101

Lowest mean error: 2.7387659549713135 mm for frame 21

Saving results

Total time: 39.85042595863342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459329
Iteration 2/25 | Loss: 0.00117398
Iteration 3/25 | Loss: 0.00109581
Iteration 4/25 | Loss: 0.00108861
Iteration 5/25 | Loss: 0.00108700
Iteration 6/25 | Loss: 0.00108700
Iteration 7/25 | Loss: 0.00108700
Iteration 8/25 | Loss: 0.00108700
Iteration 9/25 | Loss: 0.00108700
Iteration 10/25 | Loss: 0.00108700
Iteration 11/25 | Loss: 0.00108700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001087001757696271, 0.001087001757696271, 0.001087001757696271, 0.001087001757696271, 0.001087001757696271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001087001757696271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35322714
Iteration 2/25 | Loss: 0.00088519
Iteration 3/25 | Loss: 0.00088518
Iteration 4/25 | Loss: 0.00088518
Iteration 5/25 | Loss: 0.00088518
Iteration 6/25 | Loss: 0.00088518
Iteration 7/25 | Loss: 0.00088518
Iteration 8/25 | Loss: 0.00088518
Iteration 9/25 | Loss: 0.00088518
Iteration 10/25 | Loss: 0.00088518
Iteration 11/25 | Loss: 0.00088518
Iteration 12/25 | Loss: 0.00088518
Iteration 13/25 | Loss: 0.00088518
Iteration 14/25 | Loss: 0.00088518
Iteration 15/25 | Loss: 0.00088518
Iteration 16/25 | Loss: 0.00088518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008851771126501262, 0.0008851771126501262, 0.0008851771126501262, 0.0008851771126501262, 0.0008851771126501262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008851771126501262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088518
Iteration 2/1000 | Loss: 0.00002330
Iteration 3/1000 | Loss: 0.00001363
Iteration 4/1000 | Loss: 0.00001199
Iteration 5/1000 | Loss: 0.00001124
Iteration 6/1000 | Loss: 0.00001079
Iteration 7/1000 | Loss: 0.00001046
Iteration 8/1000 | Loss: 0.00001031
Iteration 9/1000 | Loss: 0.00001025
Iteration 10/1000 | Loss: 0.00001010
Iteration 11/1000 | Loss: 0.00001003
Iteration 12/1000 | Loss: 0.00001002
Iteration 13/1000 | Loss: 0.00000999
Iteration 14/1000 | Loss: 0.00000998
Iteration 15/1000 | Loss: 0.00000998
Iteration 16/1000 | Loss: 0.00000997
Iteration 17/1000 | Loss: 0.00000997
Iteration 18/1000 | Loss: 0.00000996
Iteration 19/1000 | Loss: 0.00000995
Iteration 20/1000 | Loss: 0.00000995
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000993
Iteration 25/1000 | Loss: 0.00000992
Iteration 26/1000 | Loss: 0.00000991
Iteration 27/1000 | Loss: 0.00000991
Iteration 28/1000 | Loss: 0.00000991
Iteration 29/1000 | Loss: 0.00000991
Iteration 30/1000 | Loss: 0.00000990
Iteration 31/1000 | Loss: 0.00000989
Iteration 32/1000 | Loss: 0.00000988
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000987
Iteration 36/1000 | Loss: 0.00000987
Iteration 37/1000 | Loss: 0.00000986
Iteration 38/1000 | Loss: 0.00000986
Iteration 39/1000 | Loss: 0.00000986
Iteration 40/1000 | Loss: 0.00000983
Iteration 41/1000 | Loss: 0.00000983
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000981
Iteration 44/1000 | Loss: 0.00000981
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000978
Iteration 49/1000 | Loss: 0.00000978
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000977
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000975
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000975
Iteration 60/1000 | Loss: 0.00000975
Iteration 61/1000 | Loss: 0.00000974
Iteration 62/1000 | Loss: 0.00000974
Iteration 63/1000 | Loss: 0.00000974
Iteration 64/1000 | Loss: 0.00000974
Iteration 65/1000 | Loss: 0.00000973
Iteration 66/1000 | Loss: 0.00000972
Iteration 67/1000 | Loss: 0.00000972
Iteration 68/1000 | Loss: 0.00000972
Iteration 69/1000 | Loss: 0.00000971
Iteration 70/1000 | Loss: 0.00000971
Iteration 71/1000 | Loss: 0.00000971
Iteration 72/1000 | Loss: 0.00000971
Iteration 73/1000 | Loss: 0.00000971
Iteration 74/1000 | Loss: 0.00000971
Iteration 75/1000 | Loss: 0.00000971
Iteration 76/1000 | Loss: 0.00000971
Iteration 77/1000 | Loss: 0.00000970
Iteration 78/1000 | Loss: 0.00000970
Iteration 79/1000 | Loss: 0.00000969
Iteration 80/1000 | Loss: 0.00000969
Iteration 81/1000 | Loss: 0.00000969
Iteration 82/1000 | Loss: 0.00000968
Iteration 83/1000 | Loss: 0.00000968
Iteration 84/1000 | Loss: 0.00000967
Iteration 85/1000 | Loss: 0.00000967
Iteration 86/1000 | Loss: 0.00000967
Iteration 87/1000 | Loss: 0.00000967
Iteration 88/1000 | Loss: 0.00000967
Iteration 89/1000 | Loss: 0.00000967
Iteration 90/1000 | Loss: 0.00000967
Iteration 91/1000 | Loss: 0.00000967
Iteration 92/1000 | Loss: 0.00000966
Iteration 93/1000 | Loss: 0.00000966
Iteration 94/1000 | Loss: 0.00000966
Iteration 95/1000 | Loss: 0.00000965
Iteration 96/1000 | Loss: 0.00000965
Iteration 97/1000 | Loss: 0.00000964
Iteration 98/1000 | Loss: 0.00000964
Iteration 99/1000 | Loss: 0.00000964
Iteration 100/1000 | Loss: 0.00000964
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000964
Iteration 103/1000 | Loss: 0.00000964
Iteration 104/1000 | Loss: 0.00000964
Iteration 105/1000 | Loss: 0.00000964
Iteration 106/1000 | Loss: 0.00000964
Iteration 107/1000 | Loss: 0.00000964
Iteration 108/1000 | Loss: 0.00000964
Iteration 109/1000 | Loss: 0.00000963
Iteration 110/1000 | Loss: 0.00000963
Iteration 111/1000 | Loss: 0.00000963
Iteration 112/1000 | Loss: 0.00000962
Iteration 113/1000 | Loss: 0.00000962
Iteration 114/1000 | Loss: 0.00000962
Iteration 115/1000 | Loss: 0.00000962
Iteration 116/1000 | Loss: 0.00000962
Iteration 117/1000 | Loss: 0.00000962
Iteration 118/1000 | Loss: 0.00000962
Iteration 119/1000 | Loss: 0.00000962
Iteration 120/1000 | Loss: 0.00000961
Iteration 121/1000 | Loss: 0.00000961
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000961
Iteration 124/1000 | Loss: 0.00000961
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000960
Iteration 128/1000 | Loss: 0.00000960
Iteration 129/1000 | Loss: 0.00000960
Iteration 130/1000 | Loss: 0.00000960
Iteration 131/1000 | Loss: 0.00000960
Iteration 132/1000 | Loss: 0.00000960
Iteration 133/1000 | Loss: 0.00000960
Iteration 134/1000 | Loss: 0.00000959
Iteration 135/1000 | Loss: 0.00000959
Iteration 136/1000 | Loss: 0.00000959
Iteration 137/1000 | Loss: 0.00000959
Iteration 138/1000 | Loss: 0.00000959
Iteration 139/1000 | Loss: 0.00000959
Iteration 140/1000 | Loss: 0.00000959
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000958
Iteration 147/1000 | Loss: 0.00000958
Iteration 148/1000 | Loss: 0.00000958
Iteration 149/1000 | Loss: 0.00000958
Iteration 150/1000 | Loss: 0.00000958
Iteration 151/1000 | Loss: 0.00000958
Iteration 152/1000 | Loss: 0.00000958
Iteration 153/1000 | Loss: 0.00000958
Iteration 154/1000 | Loss: 0.00000957
Iteration 155/1000 | Loss: 0.00000957
Iteration 156/1000 | Loss: 0.00000957
Iteration 157/1000 | Loss: 0.00000957
Iteration 158/1000 | Loss: 0.00000957
Iteration 159/1000 | Loss: 0.00000957
Iteration 160/1000 | Loss: 0.00000956
Iteration 161/1000 | Loss: 0.00000956
Iteration 162/1000 | Loss: 0.00000956
Iteration 163/1000 | Loss: 0.00000956
Iteration 164/1000 | Loss: 0.00000956
Iteration 165/1000 | Loss: 0.00000956
Iteration 166/1000 | Loss: 0.00000955
Iteration 167/1000 | Loss: 0.00000955
Iteration 168/1000 | Loss: 0.00000955
Iteration 169/1000 | Loss: 0.00000954
Iteration 170/1000 | Loss: 0.00000954
Iteration 171/1000 | Loss: 0.00000954
Iteration 172/1000 | Loss: 0.00000954
Iteration 173/1000 | Loss: 0.00000954
Iteration 174/1000 | Loss: 0.00000954
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000954
Iteration 178/1000 | Loss: 0.00000954
Iteration 179/1000 | Loss: 0.00000954
Iteration 180/1000 | Loss: 0.00000954
Iteration 181/1000 | Loss: 0.00000954
Iteration 182/1000 | Loss: 0.00000953
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000953
Iteration 185/1000 | Loss: 0.00000952
Iteration 186/1000 | Loss: 0.00000952
Iteration 187/1000 | Loss: 0.00000951
Iteration 188/1000 | Loss: 0.00000951
Iteration 189/1000 | Loss: 0.00000951
Iteration 190/1000 | Loss: 0.00000951
Iteration 191/1000 | Loss: 0.00000951
Iteration 192/1000 | Loss: 0.00000951
Iteration 193/1000 | Loss: 0.00000951
Iteration 194/1000 | Loss: 0.00000951
Iteration 195/1000 | Loss: 0.00000951
Iteration 196/1000 | Loss: 0.00000951
Iteration 197/1000 | Loss: 0.00000951
Iteration 198/1000 | Loss: 0.00000950
Iteration 199/1000 | Loss: 0.00000950
Iteration 200/1000 | Loss: 0.00000950
Iteration 201/1000 | Loss: 0.00000950
Iteration 202/1000 | Loss: 0.00000950
Iteration 203/1000 | Loss: 0.00000950
Iteration 204/1000 | Loss: 0.00000950
Iteration 205/1000 | Loss: 0.00000949
Iteration 206/1000 | Loss: 0.00000949
Iteration 207/1000 | Loss: 0.00000949
Iteration 208/1000 | Loss: 0.00000948
Iteration 209/1000 | Loss: 0.00000948
Iteration 210/1000 | Loss: 0.00000948
Iteration 211/1000 | Loss: 0.00000948
Iteration 212/1000 | Loss: 0.00000948
Iteration 213/1000 | Loss: 0.00000948
Iteration 214/1000 | Loss: 0.00000947
Iteration 215/1000 | Loss: 0.00000947
Iteration 216/1000 | Loss: 0.00000947
Iteration 217/1000 | Loss: 0.00000947
Iteration 218/1000 | Loss: 0.00000946
Iteration 219/1000 | Loss: 0.00000946
Iteration 220/1000 | Loss: 0.00000946
Iteration 221/1000 | Loss: 0.00000946
Iteration 222/1000 | Loss: 0.00000946
Iteration 223/1000 | Loss: 0.00000946
Iteration 224/1000 | Loss: 0.00000946
Iteration 225/1000 | Loss: 0.00000946
Iteration 226/1000 | Loss: 0.00000945
Iteration 227/1000 | Loss: 0.00000945
Iteration 228/1000 | Loss: 0.00000945
Iteration 229/1000 | Loss: 0.00000945
Iteration 230/1000 | Loss: 0.00000945
Iteration 231/1000 | Loss: 0.00000945
Iteration 232/1000 | Loss: 0.00000945
Iteration 233/1000 | Loss: 0.00000945
Iteration 234/1000 | Loss: 0.00000945
Iteration 235/1000 | Loss: 0.00000945
Iteration 236/1000 | Loss: 0.00000944
Iteration 237/1000 | Loss: 0.00000944
Iteration 238/1000 | Loss: 0.00000944
Iteration 239/1000 | Loss: 0.00000944
Iteration 240/1000 | Loss: 0.00000944
Iteration 241/1000 | Loss: 0.00000944
Iteration 242/1000 | Loss: 0.00000944
Iteration 243/1000 | Loss: 0.00000944
Iteration 244/1000 | Loss: 0.00000944
Iteration 245/1000 | Loss: 0.00000944
Iteration 246/1000 | Loss: 0.00000944
Iteration 247/1000 | Loss: 0.00000944
Iteration 248/1000 | Loss: 0.00000944
Iteration 249/1000 | Loss: 0.00000944
Iteration 250/1000 | Loss: 0.00000943
Iteration 251/1000 | Loss: 0.00000943
Iteration 252/1000 | Loss: 0.00000943
Iteration 253/1000 | Loss: 0.00000943
Iteration 254/1000 | Loss: 0.00000943
Iteration 255/1000 | Loss: 0.00000943
Iteration 256/1000 | Loss: 0.00000943
Iteration 257/1000 | Loss: 0.00000943
Iteration 258/1000 | Loss: 0.00000943
Iteration 259/1000 | Loss: 0.00000942
Iteration 260/1000 | Loss: 0.00000942
Iteration 261/1000 | Loss: 0.00000942
Iteration 262/1000 | Loss: 0.00000942
Iteration 263/1000 | Loss: 0.00000942
Iteration 264/1000 | Loss: 0.00000942
Iteration 265/1000 | Loss: 0.00000942
Iteration 266/1000 | Loss: 0.00000942
Iteration 267/1000 | Loss: 0.00000942
Iteration 268/1000 | Loss: 0.00000942
Iteration 269/1000 | Loss: 0.00000942
Iteration 270/1000 | Loss: 0.00000942
Iteration 271/1000 | Loss: 0.00000941
Iteration 272/1000 | Loss: 0.00000941
Iteration 273/1000 | Loss: 0.00000941
Iteration 274/1000 | Loss: 0.00000941
Iteration 275/1000 | Loss: 0.00000941
Iteration 276/1000 | Loss: 0.00000941
Iteration 277/1000 | Loss: 0.00000941
Iteration 278/1000 | Loss: 0.00000941
Iteration 279/1000 | Loss: 0.00000941
Iteration 280/1000 | Loss: 0.00000941
Iteration 281/1000 | Loss: 0.00000941
Iteration 282/1000 | Loss: 0.00000941
Iteration 283/1000 | Loss: 0.00000941
Iteration 284/1000 | Loss: 0.00000941
Iteration 285/1000 | Loss: 0.00000940
Iteration 286/1000 | Loss: 0.00000940
Iteration 287/1000 | Loss: 0.00000940
Iteration 288/1000 | Loss: 0.00000940
Iteration 289/1000 | Loss: 0.00000940
Iteration 290/1000 | Loss: 0.00000940
Iteration 291/1000 | Loss: 0.00000940
Iteration 292/1000 | Loss: 0.00000939
Iteration 293/1000 | Loss: 0.00000939
Iteration 294/1000 | Loss: 0.00000939
Iteration 295/1000 | Loss: 0.00000939
Iteration 296/1000 | Loss: 0.00000939
Iteration 297/1000 | Loss: 0.00000938
Iteration 298/1000 | Loss: 0.00000938
Iteration 299/1000 | Loss: 0.00000938
Iteration 300/1000 | Loss: 0.00000938
Iteration 301/1000 | Loss: 0.00000938
Iteration 302/1000 | Loss: 0.00000938
Iteration 303/1000 | Loss: 0.00000937
Iteration 304/1000 | Loss: 0.00000937
Iteration 305/1000 | Loss: 0.00000937
Iteration 306/1000 | Loss: 0.00000937
Iteration 307/1000 | Loss: 0.00000937
Iteration 308/1000 | Loss: 0.00000937
Iteration 309/1000 | Loss: 0.00000937
Iteration 310/1000 | Loss: 0.00000937
Iteration 311/1000 | Loss: 0.00000937
Iteration 312/1000 | Loss: 0.00000937
Iteration 313/1000 | Loss: 0.00000937
Iteration 314/1000 | Loss: 0.00000937
Iteration 315/1000 | Loss: 0.00000937
Iteration 316/1000 | Loss: 0.00000937
Iteration 317/1000 | Loss: 0.00000937
Iteration 318/1000 | Loss: 0.00000937
Iteration 319/1000 | Loss: 0.00000936
Iteration 320/1000 | Loss: 0.00000936
Iteration 321/1000 | Loss: 0.00000936
Iteration 322/1000 | Loss: 0.00000936
Iteration 323/1000 | Loss: 0.00000936
Iteration 324/1000 | Loss: 0.00000936
Iteration 325/1000 | Loss: 0.00000936
Iteration 326/1000 | Loss: 0.00000936
Iteration 327/1000 | Loss: 0.00000936
Iteration 328/1000 | Loss: 0.00000936
Iteration 329/1000 | Loss: 0.00000936
Iteration 330/1000 | Loss: 0.00000936
Iteration 331/1000 | Loss: 0.00000936
Iteration 332/1000 | Loss: 0.00000936
Iteration 333/1000 | Loss: 0.00000935
Iteration 334/1000 | Loss: 0.00000935
Iteration 335/1000 | Loss: 0.00000935
Iteration 336/1000 | Loss: 0.00000935
Iteration 337/1000 | Loss: 0.00000935
Iteration 338/1000 | Loss: 0.00000935
Iteration 339/1000 | Loss: 0.00000935
Iteration 340/1000 | Loss: 0.00000935
Iteration 341/1000 | Loss: 0.00000935
Iteration 342/1000 | Loss: 0.00000935
Iteration 343/1000 | Loss: 0.00000935
Iteration 344/1000 | Loss: 0.00000935
Iteration 345/1000 | Loss: 0.00000935
Iteration 346/1000 | Loss: 0.00000935
Iteration 347/1000 | Loss: 0.00000935
Iteration 348/1000 | Loss: 0.00000935
Iteration 349/1000 | Loss: 0.00000935
Iteration 350/1000 | Loss: 0.00000934
Iteration 351/1000 | Loss: 0.00000934
Iteration 352/1000 | Loss: 0.00000934
Iteration 353/1000 | Loss: 0.00000934
Iteration 354/1000 | Loss: 0.00000934
Iteration 355/1000 | Loss: 0.00000934
Iteration 356/1000 | Loss: 0.00000934
Iteration 357/1000 | Loss: 0.00000934
Iteration 358/1000 | Loss: 0.00000934
Iteration 359/1000 | Loss: 0.00000934
Iteration 360/1000 | Loss: 0.00000934
Iteration 361/1000 | Loss: 0.00000934
Iteration 362/1000 | Loss: 0.00000934
Iteration 363/1000 | Loss: 0.00000934
Iteration 364/1000 | Loss: 0.00000934
Iteration 365/1000 | Loss: 0.00000934
Iteration 366/1000 | Loss: 0.00000934
Iteration 367/1000 | Loss: 0.00000934
Iteration 368/1000 | Loss: 0.00000934
Iteration 369/1000 | Loss: 0.00000934
Iteration 370/1000 | Loss: 0.00000934
Iteration 371/1000 | Loss: 0.00000933
Iteration 372/1000 | Loss: 0.00000933
Iteration 373/1000 | Loss: 0.00000933
Iteration 374/1000 | Loss: 0.00000933
Iteration 375/1000 | Loss: 0.00000933
Iteration 376/1000 | Loss: 0.00000933
Iteration 377/1000 | Loss: 0.00000933
Iteration 378/1000 | Loss: 0.00000933
Iteration 379/1000 | Loss: 0.00000933
Iteration 380/1000 | Loss: 0.00000933
Iteration 381/1000 | Loss: 0.00000933
Iteration 382/1000 | Loss: 0.00000933
Iteration 383/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 383. Stopping optimization.
Last 5 losses: [9.332692570751533e-06, 9.332692570751533e-06, 9.332692570751533e-06, 9.332692570751533e-06, 9.332692570751533e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.332692570751533e-06

Optimization complete. Final v2v error: 2.500927209854126 mm

Highest mean error: 3.0751724243164062 mm for frame 70

Lowest mean error: 2.3110742568969727 mm for frame 114

Saving results

Total time: 46.64286398887634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060617
Iteration 2/25 | Loss: 0.00228478
Iteration 3/25 | Loss: 0.00139375
Iteration 4/25 | Loss: 0.00129935
Iteration 5/25 | Loss: 0.00126879
Iteration 6/25 | Loss: 0.00122437
Iteration 7/25 | Loss: 0.00119099
Iteration 8/25 | Loss: 0.00119242
Iteration 9/25 | Loss: 0.00115677
Iteration 10/25 | Loss: 0.00114782
Iteration 11/25 | Loss: 0.00114942
Iteration 12/25 | Loss: 0.00115975
Iteration 13/25 | Loss: 0.00114154
Iteration 14/25 | Loss: 0.00114147
Iteration 15/25 | Loss: 0.00114215
Iteration 16/25 | Loss: 0.00114003
Iteration 17/25 | Loss: 0.00113929
Iteration 18/25 | Loss: 0.00113925
Iteration 19/25 | Loss: 0.00113925
Iteration 20/25 | Loss: 0.00113925
Iteration 21/25 | Loss: 0.00113925
Iteration 22/25 | Loss: 0.00113925
Iteration 23/25 | Loss: 0.00113925
Iteration 24/25 | Loss: 0.00113925
Iteration 25/25 | Loss: 0.00113925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29192138
Iteration 2/25 | Loss: 0.00079876
Iteration 3/25 | Loss: 0.00079876
Iteration 4/25 | Loss: 0.00079876
Iteration 5/25 | Loss: 0.00079876
Iteration 6/25 | Loss: 0.00079876
Iteration 7/25 | Loss: 0.00079876
Iteration 8/25 | Loss: 0.00079876
Iteration 9/25 | Loss: 0.00079876
Iteration 10/25 | Loss: 0.00079876
Iteration 11/25 | Loss: 0.00079876
Iteration 12/25 | Loss: 0.00079876
Iteration 13/25 | Loss: 0.00079876
Iteration 14/25 | Loss: 0.00079876
Iteration 15/25 | Loss: 0.00079876
Iteration 16/25 | Loss: 0.00079876
Iteration 17/25 | Loss: 0.00079876
Iteration 18/25 | Loss: 0.00079876
Iteration 19/25 | Loss: 0.00079876
Iteration 20/25 | Loss: 0.00079876
Iteration 21/25 | Loss: 0.00079876
Iteration 22/25 | Loss: 0.00079876
Iteration 23/25 | Loss: 0.00079876
Iteration 24/25 | Loss: 0.00079876
Iteration 25/25 | Loss: 0.00079876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079876
Iteration 2/1000 | Loss: 0.00004640
Iteration 3/1000 | Loss: 0.00002838
Iteration 4/1000 | Loss: 0.00006126
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00006283
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00006442
Iteration 9/1000 | Loss: 0.00001784
Iteration 10/1000 | Loss: 0.00004239
Iteration 11/1000 | Loss: 0.00001725
Iteration 12/1000 | Loss: 0.00002300
Iteration 13/1000 | Loss: 0.00001696
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001658
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001658
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001656
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001652
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00003767
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002849
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001629
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001629
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001628
Iteration 47/1000 | Loss: 0.00001628
Iteration 48/1000 | Loss: 0.00001628
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001628
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001670
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00002518
Iteration 60/1000 | Loss: 0.00001696
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001623
Iteration 63/1000 | Loss: 0.00001623
Iteration 64/1000 | Loss: 0.00001623
Iteration 65/1000 | Loss: 0.00001623
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001621
Iteration 86/1000 | Loss: 0.00001621
Iteration 87/1000 | Loss: 0.00001621
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001621
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001621
Iteration 93/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.6213369235629216e-05, 1.6213369235629216e-05, 1.6213369235629216e-05, 1.6213369235629216e-05, 1.6213369235629216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6213369235629216e-05

Optimization complete. Final v2v error: 3.1530983448028564 mm

Highest mean error: 5.352431297302246 mm for frame 67

Lowest mean error: 2.3697891235351562 mm for frame 143

Saving results

Total time: 67.63406348228455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457210
Iteration 2/25 | Loss: 0.00127191
Iteration 3/25 | Loss: 0.00113751
Iteration 4/25 | Loss: 0.00112590
Iteration 5/25 | Loss: 0.00112266
Iteration 6/25 | Loss: 0.00112198
Iteration 7/25 | Loss: 0.00112198
Iteration 8/25 | Loss: 0.00112198
Iteration 9/25 | Loss: 0.00112198
Iteration 10/25 | Loss: 0.00112198
Iteration 11/25 | Loss: 0.00112198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011219814186915755, 0.0011219814186915755, 0.0011219814186915755, 0.0011219814186915755, 0.0011219814186915755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011219814186915755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47116756
Iteration 2/25 | Loss: 0.00071347
Iteration 3/25 | Loss: 0.00071346
Iteration 4/25 | Loss: 0.00071346
Iteration 5/25 | Loss: 0.00071346
Iteration 6/25 | Loss: 0.00071346
Iteration 7/25 | Loss: 0.00071346
Iteration 8/25 | Loss: 0.00071346
Iteration 9/25 | Loss: 0.00071346
Iteration 10/25 | Loss: 0.00071346
Iteration 11/25 | Loss: 0.00071346
Iteration 12/25 | Loss: 0.00071346
Iteration 13/25 | Loss: 0.00071346
Iteration 14/25 | Loss: 0.00071346
Iteration 15/25 | Loss: 0.00071346
Iteration 16/25 | Loss: 0.00071346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007134588086046278, 0.0007134588086046278, 0.0007134588086046278, 0.0007134588086046278, 0.0007134588086046278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007134588086046278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071346
Iteration 2/1000 | Loss: 0.00002931
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001885
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001720
Iteration 8/1000 | Loss: 0.00001679
Iteration 9/1000 | Loss: 0.00001653
Iteration 10/1000 | Loss: 0.00001629
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001606
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001600
Iteration 19/1000 | Loss: 0.00001599
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001597
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001596
Iteration 25/1000 | Loss: 0.00001596
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001589
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001587
Iteration 32/1000 | Loss: 0.00001587
Iteration 33/1000 | Loss: 0.00001586
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001586
Iteration 36/1000 | Loss: 0.00001585
Iteration 37/1000 | Loss: 0.00001585
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001585
Iteration 41/1000 | Loss: 0.00001584
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001583
Iteration 44/1000 | Loss: 0.00001583
Iteration 45/1000 | Loss: 0.00001583
Iteration 46/1000 | Loss: 0.00001582
Iteration 47/1000 | Loss: 0.00001582
Iteration 48/1000 | Loss: 0.00001581
Iteration 49/1000 | Loss: 0.00001580
Iteration 50/1000 | Loss: 0.00001580
Iteration 51/1000 | Loss: 0.00001580
Iteration 52/1000 | Loss: 0.00001579
Iteration 53/1000 | Loss: 0.00001579
Iteration 54/1000 | Loss: 0.00001579
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00001578
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001577
Iteration 59/1000 | Loss: 0.00001577
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001577
Iteration 62/1000 | Loss: 0.00001577
Iteration 63/1000 | Loss: 0.00001577
Iteration 64/1000 | Loss: 0.00001577
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001571
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001569
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001560
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001555
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001552
Iteration 138/1000 | Loss: 0.00001552
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001552
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001551
Iteration 145/1000 | Loss: 0.00001551
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001551
Iteration 148/1000 | Loss: 0.00001551
Iteration 149/1000 | Loss: 0.00001551
Iteration 150/1000 | Loss: 0.00001551
Iteration 151/1000 | Loss: 0.00001551
Iteration 152/1000 | Loss: 0.00001551
Iteration 153/1000 | Loss: 0.00001551
Iteration 154/1000 | Loss: 0.00001551
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001550
Iteration 158/1000 | Loss: 0.00001550
Iteration 159/1000 | Loss: 0.00001550
Iteration 160/1000 | Loss: 0.00001550
Iteration 161/1000 | Loss: 0.00001549
Iteration 162/1000 | Loss: 0.00001549
Iteration 163/1000 | Loss: 0.00001549
Iteration 164/1000 | Loss: 0.00001549
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001549
Iteration 167/1000 | Loss: 0.00001549
Iteration 168/1000 | Loss: 0.00001549
Iteration 169/1000 | Loss: 0.00001549
Iteration 170/1000 | Loss: 0.00001549
Iteration 171/1000 | Loss: 0.00001549
Iteration 172/1000 | Loss: 0.00001549
Iteration 173/1000 | Loss: 0.00001549
Iteration 174/1000 | Loss: 0.00001549
Iteration 175/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.549273292766884e-05, 1.549273292766884e-05, 1.549273292766884e-05, 1.549273292766884e-05, 1.549273292766884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.549273292766884e-05

Optimization complete. Final v2v error: 3.2313523292541504 mm

Highest mean error: 4.09626579284668 mm for frame 147

Lowest mean error: 2.4425785541534424 mm for frame 1

Saving results

Total time: 38.86313819885254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999070
Iteration 2/25 | Loss: 0.00999070
Iteration 3/25 | Loss: 0.00999069
Iteration 4/25 | Loss: 0.00999069
Iteration 5/25 | Loss: 0.00999069
Iteration 6/25 | Loss: 0.00999069
Iteration 7/25 | Loss: 0.00999069
Iteration 8/25 | Loss: 0.00999069
Iteration 9/25 | Loss: 0.00999068
Iteration 10/25 | Loss: 0.00999068
Iteration 11/25 | Loss: 0.00999068
Iteration 12/25 | Loss: 0.00999068
Iteration 13/25 | Loss: 0.00999068
Iteration 14/25 | Loss: 0.00999068
Iteration 15/25 | Loss: 0.00999067
Iteration 16/25 | Loss: 0.00999067
Iteration 17/25 | Loss: 0.00999067
Iteration 18/25 | Loss: 0.00999067
Iteration 19/25 | Loss: 0.00999067
Iteration 20/25 | Loss: 0.00999066
Iteration 21/25 | Loss: 0.00999066
Iteration 22/25 | Loss: 0.00999066
Iteration 23/25 | Loss: 0.00999066
Iteration 24/25 | Loss: 0.00999066
Iteration 25/25 | Loss: 0.00999065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65142024
Iteration 2/25 | Loss: 0.13138530
Iteration 3/25 | Loss: 0.13065203
Iteration 4/25 | Loss: 0.13029717
Iteration 5/25 | Loss: 0.12990962
Iteration 6/25 | Loss: 0.12990957
Iteration 7/25 | Loss: 0.12990957
Iteration 8/25 | Loss: 0.12990955
Iteration 9/25 | Loss: 0.12990955
Iteration 10/25 | Loss: 0.12990955
Iteration 11/25 | Loss: 0.12990955
Iteration 12/25 | Loss: 0.12990953
Iteration 13/25 | Loss: 0.12990953
Iteration 14/25 | Loss: 0.12990953
Iteration 15/25 | Loss: 0.12990955
Iteration 16/25 | Loss: 0.12990955
Iteration 17/25 | Loss: 0.12990953
Iteration 18/25 | Loss: 0.12990953
Iteration 19/25 | Loss: 0.12990953
Iteration 20/25 | Loss: 0.12990953
Iteration 21/25 | Loss: 0.12990953
Iteration 22/25 | Loss: 0.12990953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.12990953028202057, 0.12990953028202057, 0.12990953028202057, 0.12990953028202057, 0.12990953028202057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12990953028202057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12990953
Iteration 2/1000 | Loss: 0.00229550
Iteration 3/1000 | Loss: 0.00060667
Iteration 4/1000 | Loss: 0.00062341
Iteration 5/1000 | Loss: 0.00829594
Iteration 6/1000 | Loss: 0.00021012
Iteration 7/1000 | Loss: 0.00143447
Iteration 8/1000 | Loss: 0.00034503
Iteration 9/1000 | Loss: 0.00025453
Iteration 10/1000 | Loss: 0.00012464
Iteration 11/1000 | Loss: 0.00040341
Iteration 12/1000 | Loss: 0.00010135
Iteration 13/1000 | Loss: 0.00023309
Iteration 14/1000 | Loss: 0.00010941
Iteration 15/1000 | Loss: 0.00277477
Iteration 16/1000 | Loss: 0.00357388
Iteration 17/1000 | Loss: 0.00690698
Iteration 18/1000 | Loss: 0.00016208
Iteration 19/1000 | Loss: 0.00008594
Iteration 20/1000 | Loss: 0.00009564
Iteration 21/1000 | Loss: 0.00119295
Iteration 22/1000 | Loss: 0.00116169
Iteration 23/1000 | Loss: 0.00025421
Iteration 24/1000 | Loss: 0.00004682
Iteration 25/1000 | Loss: 0.00002637
Iteration 26/1000 | Loss: 0.00002484
Iteration 27/1000 | Loss: 0.00027413
Iteration 28/1000 | Loss: 0.00029496
Iteration 29/1000 | Loss: 0.00009676
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00015112
Iteration 32/1000 | Loss: 0.00092759
Iteration 33/1000 | Loss: 0.00002725
Iteration 34/1000 | Loss: 0.00017323
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00028386
Iteration 37/1000 | Loss: 0.00012767
Iteration 38/1000 | Loss: 0.00001985
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001792
Iteration 41/1000 | Loss: 0.00015723
Iteration 42/1000 | Loss: 0.00008944
Iteration 43/1000 | Loss: 0.00006212
Iteration 44/1000 | Loss: 0.00010017
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00022683
Iteration 49/1000 | Loss: 0.00002206
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001580
Iteration 53/1000 | Loss: 0.00005260
Iteration 54/1000 | Loss: 0.00016770
Iteration 55/1000 | Loss: 0.00005288
Iteration 56/1000 | Loss: 0.00003970
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00004080
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001540
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001535
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001535
Iteration 69/1000 | Loss: 0.00001535
Iteration 70/1000 | Loss: 0.00001535
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001533
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001531
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001528
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001526
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001525
Iteration 90/1000 | Loss: 0.00001525
Iteration 91/1000 | Loss: 0.00001525
Iteration 92/1000 | Loss: 0.00001525
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001524
Iteration 98/1000 | Loss: 0.00001524
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001522
Iteration 105/1000 | Loss: 0.00001522
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001522
Iteration 108/1000 | Loss: 0.00001522
Iteration 109/1000 | Loss: 0.00001522
Iteration 110/1000 | Loss: 0.00001522
Iteration 111/1000 | Loss: 0.00001522
Iteration 112/1000 | Loss: 0.00001521
Iteration 113/1000 | Loss: 0.00001521
Iteration 114/1000 | Loss: 0.00001521
Iteration 115/1000 | Loss: 0.00001521
Iteration 116/1000 | Loss: 0.00001521
Iteration 117/1000 | Loss: 0.00001521
Iteration 118/1000 | Loss: 0.00001521
Iteration 119/1000 | Loss: 0.00001520
Iteration 120/1000 | Loss: 0.00001520
Iteration 121/1000 | Loss: 0.00001520
Iteration 122/1000 | Loss: 0.00001520
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001520
Iteration 125/1000 | Loss: 0.00001520
Iteration 126/1000 | Loss: 0.00001520
Iteration 127/1000 | Loss: 0.00001519
Iteration 128/1000 | Loss: 0.00001519
Iteration 129/1000 | Loss: 0.00001519
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001519
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001519
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001519
Iteration 136/1000 | Loss: 0.00001519
Iteration 137/1000 | Loss: 0.00001519
Iteration 138/1000 | Loss: 0.00001519
Iteration 139/1000 | Loss: 0.00001519
Iteration 140/1000 | Loss: 0.00001519
Iteration 141/1000 | Loss: 0.00001519
Iteration 142/1000 | Loss: 0.00001519
Iteration 143/1000 | Loss: 0.00001519
Iteration 144/1000 | Loss: 0.00001519
Iteration 145/1000 | Loss: 0.00001519
Iteration 146/1000 | Loss: 0.00001519
Iteration 147/1000 | Loss: 0.00001519
Iteration 148/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.518735552963335e-05, 1.518735552963335e-05, 1.518735552963335e-05, 1.518735552963335e-05, 1.518735552963335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.518735552963335e-05

Optimization complete. Final v2v error: 3.3201494216918945 mm

Highest mean error: 4.545246601104736 mm for frame 234

Lowest mean error: 2.654325246810913 mm for frame 0

Saving results

Total time: 113.09587740898132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025277
Iteration 2/25 | Loss: 0.00189222
Iteration 3/25 | Loss: 0.00141604
Iteration 4/25 | Loss: 0.00113406
Iteration 5/25 | Loss: 0.00110891
Iteration 6/25 | Loss: 0.00110157
Iteration 7/25 | Loss: 0.00110162
Iteration 8/25 | Loss: 0.00107931
Iteration 9/25 | Loss: 0.00108142
Iteration 10/25 | Loss: 0.00107379
Iteration 11/25 | Loss: 0.00107343
Iteration 12/25 | Loss: 0.00107323
Iteration 13/25 | Loss: 0.00107320
Iteration 14/25 | Loss: 0.00107319
Iteration 15/25 | Loss: 0.00107319
Iteration 16/25 | Loss: 0.00107319
Iteration 17/25 | Loss: 0.00107319
Iteration 18/25 | Loss: 0.00107319
Iteration 19/25 | Loss: 0.00107319
Iteration 20/25 | Loss: 0.00107319
Iteration 21/25 | Loss: 0.00107319
Iteration 22/25 | Loss: 0.00107319
Iteration 23/25 | Loss: 0.00107319
Iteration 24/25 | Loss: 0.00107319
Iteration 25/25 | Loss: 0.00107318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38642943
Iteration 2/25 | Loss: 0.00096376
Iteration 3/25 | Loss: 0.00092974
Iteration 4/25 | Loss: 0.00092973
Iteration 5/25 | Loss: 0.00092973
Iteration 6/25 | Loss: 0.00092973
Iteration 7/25 | Loss: 0.00092973
Iteration 8/25 | Loss: 0.00092973
Iteration 9/25 | Loss: 0.00092973
Iteration 10/25 | Loss: 0.00092973
Iteration 11/25 | Loss: 0.00092973
Iteration 12/25 | Loss: 0.00092973
Iteration 13/25 | Loss: 0.00092973
Iteration 14/25 | Loss: 0.00092973
Iteration 15/25 | Loss: 0.00092973
Iteration 16/25 | Loss: 0.00092973
Iteration 17/25 | Loss: 0.00092973
Iteration 18/25 | Loss: 0.00092973
Iteration 19/25 | Loss: 0.00092973
Iteration 20/25 | Loss: 0.00092973
Iteration 21/25 | Loss: 0.00092973
Iteration 22/25 | Loss: 0.00092973
Iteration 23/25 | Loss: 0.00092973
Iteration 24/25 | Loss: 0.00092973
Iteration 25/25 | Loss: 0.00092973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092973
Iteration 2/1000 | Loss: 0.00004883
Iteration 3/1000 | Loss: 0.00010453
Iteration 4/1000 | Loss: 0.00012084
Iteration 5/1000 | Loss: 0.00003762
Iteration 6/1000 | Loss: 0.00008866
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00003448
Iteration 10/1000 | Loss: 0.00008112
Iteration 11/1000 | Loss: 0.00001796
Iteration 12/1000 | Loss: 0.00001102
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001075
Iteration 16/1000 | Loss: 0.00001063
Iteration 17/1000 | Loss: 0.00003241
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001026
Iteration 21/1000 | Loss: 0.00001021
Iteration 22/1000 | Loss: 0.00001020
Iteration 23/1000 | Loss: 0.00001020
Iteration 24/1000 | Loss: 0.00001020
Iteration 25/1000 | Loss: 0.00001020
Iteration 26/1000 | Loss: 0.00001018
Iteration 27/1000 | Loss: 0.00001018
Iteration 28/1000 | Loss: 0.00001018
Iteration 29/1000 | Loss: 0.00001016
Iteration 30/1000 | Loss: 0.00001016
Iteration 31/1000 | Loss: 0.00001015
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001014
Iteration 34/1000 | Loss: 0.00001014
Iteration 35/1000 | Loss: 0.00001014
Iteration 36/1000 | Loss: 0.00001014
Iteration 37/1000 | Loss: 0.00001013
Iteration 38/1000 | Loss: 0.00001013
Iteration 39/1000 | Loss: 0.00001013
Iteration 40/1000 | Loss: 0.00001012
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001009
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001006
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00003388
Iteration 59/1000 | Loss: 0.00003090
Iteration 60/1000 | Loss: 0.00005317
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00003413
Iteration 68/1000 | Loss: 0.00002663
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000994
Iteration 71/1000 | Loss: 0.00000994
Iteration 72/1000 | Loss: 0.00000994
Iteration 73/1000 | Loss: 0.00000994
Iteration 74/1000 | Loss: 0.00000994
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000994
Iteration 85/1000 | Loss: 0.00000994
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000994
Iteration 90/1000 | Loss: 0.00000994
Iteration 91/1000 | Loss: 0.00000994
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [9.936487003869843e-06, 9.936487003869843e-06, 9.936487003869843e-06, 9.936487003869843e-06, 9.936487003869843e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.936487003869843e-06

Optimization complete. Final v2v error: 2.7090015411376953 mm

Highest mean error: 3.322543144226074 mm for frame 70

Lowest mean error: 2.47676420211792 mm for frame 22

Saving results

Total time: 61.34438443183899
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787816
Iteration 2/25 | Loss: 0.00129882
Iteration 3/25 | Loss: 0.00118334
Iteration 4/25 | Loss: 0.00116224
Iteration 5/25 | Loss: 0.00115538
Iteration 6/25 | Loss: 0.00115315
Iteration 7/25 | Loss: 0.00115286
Iteration 8/25 | Loss: 0.00115286
Iteration 9/25 | Loss: 0.00115286
Iteration 10/25 | Loss: 0.00115286
Iteration 11/25 | Loss: 0.00115286
Iteration 12/25 | Loss: 0.00115286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001152864540927112, 0.001152864540927112, 0.001152864540927112, 0.001152864540927112, 0.001152864540927112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001152864540927112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33644474
Iteration 2/25 | Loss: 0.00106163
Iteration 3/25 | Loss: 0.00106162
Iteration 4/25 | Loss: 0.00106162
Iteration 5/25 | Loss: 0.00106162
Iteration 6/25 | Loss: 0.00106162
Iteration 7/25 | Loss: 0.00106162
Iteration 8/25 | Loss: 0.00106162
Iteration 9/25 | Loss: 0.00106162
Iteration 10/25 | Loss: 0.00106162
Iteration 11/25 | Loss: 0.00106162
Iteration 12/25 | Loss: 0.00106162
Iteration 13/25 | Loss: 0.00106162
Iteration 14/25 | Loss: 0.00106162
Iteration 15/25 | Loss: 0.00106162
Iteration 16/25 | Loss: 0.00106162
Iteration 17/25 | Loss: 0.00106162
Iteration 18/25 | Loss: 0.00106162
Iteration 19/25 | Loss: 0.00106162
Iteration 20/25 | Loss: 0.00106162
Iteration 21/25 | Loss: 0.00106162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010616155341267586, 0.0010616155341267586, 0.0010616155341267586, 0.0010616155341267586, 0.0010616155341267586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010616155341267586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106162
Iteration 2/1000 | Loss: 0.00004905
Iteration 3/1000 | Loss: 0.00003416
Iteration 4/1000 | Loss: 0.00002633
Iteration 5/1000 | Loss: 0.00002409
Iteration 6/1000 | Loss: 0.00002300
Iteration 7/1000 | Loss: 0.00002197
Iteration 8/1000 | Loss: 0.00002142
Iteration 9/1000 | Loss: 0.00002096
Iteration 10/1000 | Loss: 0.00002066
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002044
Iteration 13/1000 | Loss: 0.00002027
Iteration 14/1000 | Loss: 0.00002019
Iteration 15/1000 | Loss: 0.00002013
Iteration 16/1000 | Loss: 0.00002012
Iteration 17/1000 | Loss: 0.00002005
Iteration 18/1000 | Loss: 0.00002004
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00001997
Iteration 21/1000 | Loss: 0.00001997
Iteration 22/1000 | Loss: 0.00001995
Iteration 23/1000 | Loss: 0.00001995
Iteration 24/1000 | Loss: 0.00001993
Iteration 25/1000 | Loss: 0.00001993
Iteration 26/1000 | Loss: 0.00001992
Iteration 27/1000 | Loss: 0.00001992
Iteration 28/1000 | Loss: 0.00001992
Iteration 29/1000 | Loss: 0.00001992
Iteration 30/1000 | Loss: 0.00001991
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001990
Iteration 34/1000 | Loss: 0.00001990
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001986
Iteration 42/1000 | Loss: 0.00001985
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001981
Iteration 45/1000 | Loss: 0.00001980
Iteration 46/1000 | Loss: 0.00001980
Iteration 47/1000 | Loss: 0.00001978
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001974
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001974
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001973
Iteration 58/1000 | Loss: 0.00001973
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001973
Iteration 62/1000 | Loss: 0.00001972
Iteration 63/1000 | Loss: 0.00001972
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001971
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001971
Iteration 70/1000 | Loss: 0.00001971
Iteration 71/1000 | Loss: 0.00001970
Iteration 72/1000 | Loss: 0.00001969
Iteration 73/1000 | Loss: 0.00001969
Iteration 74/1000 | Loss: 0.00001969
Iteration 75/1000 | Loss: 0.00001968
Iteration 76/1000 | Loss: 0.00001968
Iteration 77/1000 | Loss: 0.00001968
Iteration 78/1000 | Loss: 0.00001968
Iteration 79/1000 | Loss: 0.00001968
Iteration 80/1000 | Loss: 0.00001968
Iteration 81/1000 | Loss: 0.00001967
Iteration 82/1000 | Loss: 0.00001967
Iteration 83/1000 | Loss: 0.00001967
Iteration 84/1000 | Loss: 0.00001967
Iteration 85/1000 | Loss: 0.00001967
Iteration 86/1000 | Loss: 0.00001967
Iteration 87/1000 | Loss: 0.00001966
Iteration 88/1000 | Loss: 0.00001966
Iteration 89/1000 | Loss: 0.00001966
Iteration 90/1000 | Loss: 0.00001966
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001964
Iteration 99/1000 | Loss: 0.00001964
Iteration 100/1000 | Loss: 0.00001964
Iteration 101/1000 | Loss: 0.00001964
Iteration 102/1000 | Loss: 0.00001964
Iteration 103/1000 | Loss: 0.00001964
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001964
Iteration 106/1000 | Loss: 0.00001964
Iteration 107/1000 | Loss: 0.00001964
Iteration 108/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.9637025616248138e-05, 1.9637025616248138e-05, 1.9637025616248138e-05, 1.9637025616248138e-05, 1.9637025616248138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9637025616248138e-05

Optimization complete. Final v2v error: 3.6400389671325684 mm

Highest mean error: 4.529545783996582 mm for frame 96

Lowest mean error: 2.831467628479004 mm for frame 2

Saving results

Total time: 37.442333936691284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379796
Iteration 2/25 | Loss: 0.00124568
Iteration 3/25 | Loss: 0.00110633
Iteration 4/25 | Loss: 0.00108162
Iteration 5/25 | Loss: 0.00107483
Iteration 6/25 | Loss: 0.00107316
Iteration 7/25 | Loss: 0.00107282
Iteration 8/25 | Loss: 0.00107282
Iteration 9/25 | Loss: 0.00107282
Iteration 10/25 | Loss: 0.00107282
Iteration 11/25 | Loss: 0.00107282
Iteration 12/25 | Loss: 0.00107282
Iteration 13/25 | Loss: 0.00107282
Iteration 14/25 | Loss: 0.00107282
Iteration 15/25 | Loss: 0.00107282
Iteration 16/25 | Loss: 0.00107282
Iteration 17/25 | Loss: 0.00107282
Iteration 18/25 | Loss: 0.00107282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010728248162195086, 0.0010728248162195086, 0.0010728248162195086, 0.0010728248162195086, 0.0010728248162195086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010728248162195086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31708372
Iteration 2/25 | Loss: 0.00081596
Iteration 3/25 | Loss: 0.00081596
Iteration 4/25 | Loss: 0.00081596
Iteration 5/25 | Loss: 0.00081596
Iteration 6/25 | Loss: 0.00081596
Iteration 7/25 | Loss: 0.00081596
Iteration 8/25 | Loss: 0.00081596
Iteration 9/25 | Loss: 0.00081595
Iteration 10/25 | Loss: 0.00081595
Iteration 11/25 | Loss: 0.00081595
Iteration 12/25 | Loss: 0.00081595
Iteration 13/25 | Loss: 0.00081595
Iteration 14/25 | Loss: 0.00081595
Iteration 15/25 | Loss: 0.00081595
Iteration 16/25 | Loss: 0.00081595
Iteration 17/25 | Loss: 0.00081595
Iteration 18/25 | Loss: 0.00081595
Iteration 19/25 | Loss: 0.00081595
Iteration 20/25 | Loss: 0.00081595
Iteration 21/25 | Loss: 0.00081595
Iteration 22/25 | Loss: 0.00081595
Iteration 23/25 | Loss: 0.00081595
Iteration 24/25 | Loss: 0.00081595
Iteration 25/25 | Loss: 0.00081595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081595
Iteration 2/1000 | Loss: 0.00004358
Iteration 3/1000 | Loss: 0.00002766
Iteration 4/1000 | Loss: 0.00002281
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001908
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001668
Iteration 10/1000 | Loss: 0.00001638
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001558
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001556
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001552
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00001548
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001532
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001527
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001526
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001521
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001520
Iteration 78/1000 | Loss: 0.00001520
Iteration 79/1000 | Loss: 0.00001520
Iteration 80/1000 | Loss: 0.00001520
Iteration 81/1000 | Loss: 0.00001520
Iteration 82/1000 | Loss: 0.00001520
Iteration 83/1000 | Loss: 0.00001520
Iteration 84/1000 | Loss: 0.00001520
Iteration 85/1000 | Loss: 0.00001519
Iteration 86/1000 | Loss: 0.00001519
Iteration 87/1000 | Loss: 0.00001519
Iteration 88/1000 | Loss: 0.00001519
Iteration 89/1000 | Loss: 0.00001519
Iteration 90/1000 | Loss: 0.00001519
Iteration 91/1000 | Loss: 0.00001519
Iteration 92/1000 | Loss: 0.00001519
Iteration 93/1000 | Loss: 0.00001518
Iteration 94/1000 | Loss: 0.00001518
Iteration 95/1000 | Loss: 0.00001518
Iteration 96/1000 | Loss: 0.00001518
Iteration 97/1000 | Loss: 0.00001518
Iteration 98/1000 | Loss: 0.00001518
Iteration 99/1000 | Loss: 0.00001518
Iteration 100/1000 | Loss: 0.00001518
Iteration 101/1000 | Loss: 0.00001518
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001517
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001516
Iteration 110/1000 | Loss: 0.00001516
Iteration 111/1000 | Loss: 0.00001516
Iteration 112/1000 | Loss: 0.00001516
Iteration 113/1000 | Loss: 0.00001516
Iteration 114/1000 | Loss: 0.00001515
Iteration 115/1000 | Loss: 0.00001515
Iteration 116/1000 | Loss: 0.00001515
Iteration 117/1000 | Loss: 0.00001515
Iteration 118/1000 | Loss: 0.00001515
Iteration 119/1000 | Loss: 0.00001515
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001514
Iteration 124/1000 | Loss: 0.00001514
Iteration 125/1000 | Loss: 0.00001514
Iteration 126/1000 | Loss: 0.00001514
Iteration 127/1000 | Loss: 0.00001514
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001514
Iteration 130/1000 | Loss: 0.00001514
Iteration 131/1000 | Loss: 0.00001514
Iteration 132/1000 | Loss: 0.00001514
Iteration 133/1000 | Loss: 0.00001513
Iteration 134/1000 | Loss: 0.00001513
Iteration 135/1000 | Loss: 0.00001513
Iteration 136/1000 | Loss: 0.00001513
Iteration 137/1000 | Loss: 0.00001513
Iteration 138/1000 | Loss: 0.00001513
Iteration 139/1000 | Loss: 0.00001513
Iteration 140/1000 | Loss: 0.00001513
Iteration 141/1000 | Loss: 0.00001513
Iteration 142/1000 | Loss: 0.00001513
Iteration 143/1000 | Loss: 0.00001513
Iteration 144/1000 | Loss: 0.00001513
Iteration 145/1000 | Loss: 0.00001512
Iteration 146/1000 | Loss: 0.00001512
Iteration 147/1000 | Loss: 0.00001512
Iteration 148/1000 | Loss: 0.00001512
Iteration 149/1000 | Loss: 0.00001512
Iteration 150/1000 | Loss: 0.00001512
Iteration 151/1000 | Loss: 0.00001512
Iteration 152/1000 | Loss: 0.00001512
Iteration 153/1000 | Loss: 0.00001512
Iteration 154/1000 | Loss: 0.00001512
Iteration 155/1000 | Loss: 0.00001512
Iteration 156/1000 | Loss: 0.00001512
Iteration 157/1000 | Loss: 0.00001511
Iteration 158/1000 | Loss: 0.00001511
Iteration 159/1000 | Loss: 0.00001511
Iteration 160/1000 | Loss: 0.00001511
Iteration 161/1000 | Loss: 0.00001511
Iteration 162/1000 | Loss: 0.00001511
Iteration 163/1000 | Loss: 0.00001511
Iteration 164/1000 | Loss: 0.00001511
Iteration 165/1000 | Loss: 0.00001511
Iteration 166/1000 | Loss: 0.00001511
Iteration 167/1000 | Loss: 0.00001510
Iteration 168/1000 | Loss: 0.00001510
Iteration 169/1000 | Loss: 0.00001510
Iteration 170/1000 | Loss: 0.00001510
Iteration 171/1000 | Loss: 0.00001510
Iteration 172/1000 | Loss: 0.00001509
Iteration 173/1000 | Loss: 0.00001509
Iteration 174/1000 | Loss: 0.00001509
Iteration 175/1000 | Loss: 0.00001509
Iteration 176/1000 | Loss: 0.00001509
Iteration 177/1000 | Loss: 0.00001509
Iteration 178/1000 | Loss: 0.00001508
Iteration 179/1000 | Loss: 0.00001508
Iteration 180/1000 | Loss: 0.00001508
Iteration 181/1000 | Loss: 0.00001508
Iteration 182/1000 | Loss: 0.00001508
Iteration 183/1000 | Loss: 0.00001508
Iteration 184/1000 | Loss: 0.00001507
Iteration 185/1000 | Loss: 0.00001507
Iteration 186/1000 | Loss: 0.00001507
Iteration 187/1000 | Loss: 0.00001507
Iteration 188/1000 | Loss: 0.00001507
Iteration 189/1000 | Loss: 0.00001507
Iteration 190/1000 | Loss: 0.00001507
Iteration 191/1000 | Loss: 0.00001507
Iteration 192/1000 | Loss: 0.00001507
Iteration 193/1000 | Loss: 0.00001507
Iteration 194/1000 | Loss: 0.00001507
Iteration 195/1000 | Loss: 0.00001507
Iteration 196/1000 | Loss: 0.00001507
Iteration 197/1000 | Loss: 0.00001507
Iteration 198/1000 | Loss: 0.00001507
Iteration 199/1000 | Loss: 0.00001507
Iteration 200/1000 | Loss: 0.00001506
Iteration 201/1000 | Loss: 0.00001506
Iteration 202/1000 | Loss: 0.00001506
Iteration 203/1000 | Loss: 0.00001506
Iteration 204/1000 | Loss: 0.00001506
Iteration 205/1000 | Loss: 0.00001506
Iteration 206/1000 | Loss: 0.00001506
Iteration 207/1000 | Loss: 0.00001506
Iteration 208/1000 | Loss: 0.00001506
Iteration 209/1000 | Loss: 0.00001506
Iteration 210/1000 | Loss: 0.00001506
Iteration 211/1000 | Loss: 0.00001506
Iteration 212/1000 | Loss: 0.00001506
Iteration 213/1000 | Loss: 0.00001506
Iteration 214/1000 | Loss: 0.00001506
Iteration 215/1000 | Loss: 0.00001505
Iteration 216/1000 | Loss: 0.00001505
Iteration 217/1000 | Loss: 0.00001505
Iteration 218/1000 | Loss: 0.00001505
Iteration 219/1000 | Loss: 0.00001505
Iteration 220/1000 | Loss: 0.00001505
Iteration 221/1000 | Loss: 0.00001505
Iteration 222/1000 | Loss: 0.00001505
Iteration 223/1000 | Loss: 0.00001505
Iteration 224/1000 | Loss: 0.00001505
Iteration 225/1000 | Loss: 0.00001505
Iteration 226/1000 | Loss: 0.00001505
Iteration 227/1000 | Loss: 0.00001505
Iteration 228/1000 | Loss: 0.00001505
Iteration 229/1000 | Loss: 0.00001505
Iteration 230/1000 | Loss: 0.00001505
Iteration 231/1000 | Loss: 0.00001505
Iteration 232/1000 | Loss: 0.00001504
Iteration 233/1000 | Loss: 0.00001504
Iteration 234/1000 | Loss: 0.00001504
Iteration 235/1000 | Loss: 0.00001504
Iteration 236/1000 | Loss: 0.00001504
Iteration 237/1000 | Loss: 0.00001504
Iteration 238/1000 | Loss: 0.00001504
Iteration 239/1000 | Loss: 0.00001504
Iteration 240/1000 | Loss: 0.00001504
Iteration 241/1000 | Loss: 0.00001504
Iteration 242/1000 | Loss: 0.00001504
Iteration 243/1000 | Loss: 0.00001504
Iteration 244/1000 | Loss: 0.00001503
Iteration 245/1000 | Loss: 0.00001503
Iteration 246/1000 | Loss: 0.00001503
Iteration 247/1000 | Loss: 0.00001503
Iteration 248/1000 | Loss: 0.00001503
Iteration 249/1000 | Loss: 0.00001503
Iteration 250/1000 | Loss: 0.00001503
Iteration 251/1000 | Loss: 0.00001503
Iteration 252/1000 | Loss: 0.00001503
Iteration 253/1000 | Loss: 0.00001503
Iteration 254/1000 | Loss: 0.00001503
Iteration 255/1000 | Loss: 0.00001503
Iteration 256/1000 | Loss: 0.00001503
Iteration 257/1000 | Loss: 0.00001503
Iteration 258/1000 | Loss: 0.00001503
Iteration 259/1000 | Loss: 0.00001503
Iteration 260/1000 | Loss: 0.00001503
Iteration 261/1000 | Loss: 0.00001503
Iteration 262/1000 | Loss: 0.00001503
Iteration 263/1000 | Loss: 0.00001503
Iteration 264/1000 | Loss: 0.00001503
Iteration 265/1000 | Loss: 0.00001503
Iteration 266/1000 | Loss: 0.00001503
Iteration 267/1000 | Loss: 0.00001503
Iteration 268/1000 | Loss: 0.00001503
Iteration 269/1000 | Loss: 0.00001503
Iteration 270/1000 | Loss: 0.00001503
Iteration 271/1000 | Loss: 0.00001503
Iteration 272/1000 | Loss: 0.00001503
Iteration 273/1000 | Loss: 0.00001503
Iteration 274/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [1.5029479982331395e-05, 1.5029479982331395e-05, 1.5029479982331395e-05, 1.5029479982331395e-05, 1.5029479982331395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5029479982331395e-05

Optimization complete. Final v2v error: 3.2649126052856445 mm

Highest mean error: 3.877687454223633 mm for frame 73

Lowest mean error: 2.701592206954956 mm for frame 83

Saving results

Total time: 47.02773118019104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00685227
Iteration 2/25 | Loss: 0.00148334
Iteration 3/25 | Loss: 0.00121192
Iteration 4/25 | Loss: 0.00117071
Iteration 5/25 | Loss: 0.00116344
Iteration 6/25 | Loss: 0.00115703
Iteration 7/25 | Loss: 0.00115424
Iteration 8/25 | Loss: 0.00114508
Iteration 9/25 | Loss: 0.00113894
Iteration 10/25 | Loss: 0.00113397
Iteration 11/25 | Loss: 0.00113228
Iteration 12/25 | Loss: 0.00113212
Iteration 13/25 | Loss: 0.00113212
Iteration 14/25 | Loss: 0.00113212
Iteration 15/25 | Loss: 0.00113212
Iteration 16/25 | Loss: 0.00113211
Iteration 17/25 | Loss: 0.00113211
Iteration 18/25 | Loss: 0.00113211
Iteration 19/25 | Loss: 0.00113211
Iteration 20/25 | Loss: 0.00113211
Iteration 21/25 | Loss: 0.00113211
Iteration 22/25 | Loss: 0.00113211
Iteration 23/25 | Loss: 0.00113211
Iteration 24/25 | Loss: 0.00113211
Iteration 25/25 | Loss: 0.00113211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06986022
Iteration 2/25 | Loss: 0.00099028
Iteration 3/25 | Loss: 0.00099008
Iteration 4/25 | Loss: 0.00099008
Iteration 5/25 | Loss: 0.00099008
Iteration 6/25 | Loss: 0.00099008
Iteration 7/25 | Loss: 0.00099007
Iteration 8/25 | Loss: 0.00099007
Iteration 9/25 | Loss: 0.00099007
Iteration 10/25 | Loss: 0.00099007
Iteration 11/25 | Loss: 0.00099007
Iteration 12/25 | Loss: 0.00099007
Iteration 13/25 | Loss: 0.00099007
Iteration 14/25 | Loss: 0.00099007
Iteration 15/25 | Loss: 0.00099007
Iteration 16/25 | Loss: 0.00099007
Iteration 17/25 | Loss: 0.00099007
Iteration 18/25 | Loss: 0.00099007
Iteration 19/25 | Loss: 0.00099007
Iteration 20/25 | Loss: 0.00099007
Iteration 21/25 | Loss: 0.00099007
Iteration 22/25 | Loss: 0.00099007
Iteration 23/25 | Loss: 0.00099007
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009900726145133376, 0.0009900726145133376, 0.0009900726145133376, 0.0009900726145133376, 0.0009900726145133376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009900726145133376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099007
Iteration 2/1000 | Loss: 0.00005935
Iteration 3/1000 | Loss: 0.00003598
Iteration 4/1000 | Loss: 0.00003003
Iteration 5/1000 | Loss: 0.00002739
Iteration 6/1000 | Loss: 0.00002570
Iteration 7/1000 | Loss: 0.00002461
Iteration 8/1000 | Loss: 0.00002389
Iteration 9/1000 | Loss: 0.00002323
Iteration 10/1000 | Loss: 0.00002278
Iteration 11/1000 | Loss: 0.00002234
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00002180
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002148
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002126
Iteration 19/1000 | Loss: 0.00002126
Iteration 20/1000 | Loss: 0.00002113
Iteration 21/1000 | Loss: 0.00002112
Iteration 22/1000 | Loss: 0.00002109
Iteration 23/1000 | Loss: 0.00002108
Iteration 24/1000 | Loss: 0.00002107
Iteration 25/1000 | Loss: 0.00002106
Iteration 26/1000 | Loss: 0.00002101
Iteration 27/1000 | Loss: 0.00002101
Iteration 28/1000 | Loss: 0.00002098
Iteration 29/1000 | Loss: 0.00002094
Iteration 30/1000 | Loss: 0.00002094
Iteration 31/1000 | Loss: 0.00002093
Iteration 32/1000 | Loss: 0.00002093
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002092
Iteration 35/1000 | Loss: 0.00002092
Iteration 36/1000 | Loss: 0.00002091
Iteration 37/1000 | Loss: 0.00002090
Iteration 38/1000 | Loss: 0.00002090
Iteration 39/1000 | Loss: 0.00002089
Iteration 40/1000 | Loss: 0.00002089
Iteration 41/1000 | Loss: 0.00002089
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002087
Iteration 44/1000 | Loss: 0.00002086
Iteration 45/1000 | Loss: 0.00002086
Iteration 46/1000 | Loss: 0.00002085
Iteration 47/1000 | Loss: 0.00002085
Iteration 48/1000 | Loss: 0.00002084
Iteration 49/1000 | Loss: 0.00002084
Iteration 50/1000 | Loss: 0.00002084
Iteration 51/1000 | Loss: 0.00002083
Iteration 52/1000 | Loss: 0.00002083
Iteration 53/1000 | Loss: 0.00002082
Iteration 54/1000 | Loss: 0.00002081
Iteration 55/1000 | Loss: 0.00002080
Iteration 56/1000 | Loss: 0.00002080
Iteration 57/1000 | Loss: 0.00002079
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002079
Iteration 60/1000 | Loss: 0.00002078
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002077
Iteration 64/1000 | Loss: 0.00002076
Iteration 65/1000 | Loss: 0.00002076
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002074
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002073
Iteration 74/1000 | Loss: 0.00002073
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002072
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002069
Iteration 82/1000 | Loss: 0.00002068
Iteration 83/1000 | Loss: 0.00002068
Iteration 84/1000 | Loss: 0.00002068
Iteration 85/1000 | Loss: 0.00002068
Iteration 86/1000 | Loss: 0.00002068
Iteration 87/1000 | Loss: 0.00002068
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002067
Iteration 91/1000 | Loss: 0.00002067
Iteration 92/1000 | Loss: 0.00002067
Iteration 93/1000 | Loss: 0.00002067
Iteration 94/1000 | Loss: 0.00002067
Iteration 95/1000 | Loss: 0.00002067
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002066
Iteration 98/1000 | Loss: 0.00002066
Iteration 99/1000 | Loss: 0.00002066
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002065
Iteration 104/1000 | Loss: 0.00002065
Iteration 105/1000 | Loss: 0.00002064
Iteration 106/1000 | Loss: 0.00002064
Iteration 107/1000 | Loss: 0.00002064
Iteration 108/1000 | Loss: 0.00002064
Iteration 109/1000 | Loss: 0.00002064
Iteration 110/1000 | Loss: 0.00002064
Iteration 111/1000 | Loss: 0.00002064
Iteration 112/1000 | Loss: 0.00002064
Iteration 113/1000 | Loss: 0.00002064
Iteration 114/1000 | Loss: 0.00002064
Iteration 115/1000 | Loss: 0.00002064
Iteration 116/1000 | Loss: 0.00002064
Iteration 117/1000 | Loss: 0.00002064
Iteration 118/1000 | Loss: 0.00002064
Iteration 119/1000 | Loss: 0.00002064
Iteration 120/1000 | Loss: 0.00002064
Iteration 121/1000 | Loss: 0.00002064
Iteration 122/1000 | Loss: 0.00002064
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002064
Iteration 128/1000 | Loss: 0.00002064
Iteration 129/1000 | Loss: 0.00002064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.0639925423893146e-05, 2.0639925423893146e-05, 2.0639925423893146e-05, 2.0639925423893146e-05, 2.0639925423893146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0639925423893146e-05

Optimization complete. Final v2v error: 3.6304879188537598 mm

Highest mean error: 5.932679176330566 mm for frame 132

Lowest mean error: 2.712538719177246 mm for frame 194

Saving results

Total time: 62.01442098617554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389768
Iteration 2/25 | Loss: 0.00111231
Iteration 3/25 | Loss: 0.00105148
Iteration 4/25 | Loss: 0.00104548
Iteration 5/25 | Loss: 0.00104398
Iteration 6/25 | Loss: 0.00104388
Iteration 7/25 | Loss: 0.00104388
Iteration 8/25 | Loss: 0.00104388
Iteration 9/25 | Loss: 0.00104388
Iteration 10/25 | Loss: 0.00104388
Iteration 11/25 | Loss: 0.00104388
Iteration 12/25 | Loss: 0.00104388
Iteration 13/25 | Loss: 0.00104388
Iteration 14/25 | Loss: 0.00104388
Iteration 15/25 | Loss: 0.00104388
Iteration 16/25 | Loss: 0.00104388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010438752360641956, 0.0010438752360641956, 0.0010438752360641956, 0.0010438752360641956, 0.0010438752360641956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010438752360641956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35122633
Iteration 2/25 | Loss: 0.00090615
Iteration 3/25 | Loss: 0.00090614
Iteration 4/25 | Loss: 0.00090614
Iteration 5/25 | Loss: 0.00090614
Iteration 6/25 | Loss: 0.00090614
Iteration 7/25 | Loss: 0.00090614
Iteration 8/25 | Loss: 0.00090614
Iteration 9/25 | Loss: 0.00090614
Iteration 10/25 | Loss: 0.00090614
Iteration 11/25 | Loss: 0.00090614
Iteration 12/25 | Loss: 0.00090614
Iteration 13/25 | Loss: 0.00090614
Iteration 14/25 | Loss: 0.00090614
Iteration 15/25 | Loss: 0.00090614
Iteration 16/25 | Loss: 0.00090614
Iteration 17/25 | Loss: 0.00090614
Iteration 18/25 | Loss: 0.00090614
Iteration 19/25 | Loss: 0.00090614
Iteration 20/25 | Loss: 0.00090614
Iteration 21/25 | Loss: 0.00090614
Iteration 22/25 | Loss: 0.00090614
Iteration 23/25 | Loss: 0.00090614
Iteration 24/25 | Loss: 0.00090614
Iteration 25/25 | Loss: 0.00090614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090614
Iteration 2/1000 | Loss: 0.00002277
Iteration 3/1000 | Loss: 0.00001381
Iteration 4/1000 | Loss: 0.00001089
Iteration 5/1000 | Loss: 0.00000974
Iteration 6/1000 | Loss: 0.00000907
Iteration 7/1000 | Loss: 0.00000869
Iteration 8/1000 | Loss: 0.00000835
Iteration 9/1000 | Loss: 0.00000821
Iteration 10/1000 | Loss: 0.00000812
Iteration 11/1000 | Loss: 0.00000798
Iteration 12/1000 | Loss: 0.00000797
Iteration 13/1000 | Loss: 0.00000797
Iteration 14/1000 | Loss: 0.00000795
Iteration 15/1000 | Loss: 0.00000795
Iteration 16/1000 | Loss: 0.00000794
Iteration 17/1000 | Loss: 0.00000793
Iteration 18/1000 | Loss: 0.00000792
Iteration 19/1000 | Loss: 0.00000791
Iteration 20/1000 | Loss: 0.00000791
Iteration 21/1000 | Loss: 0.00000791
Iteration 22/1000 | Loss: 0.00000790
Iteration 23/1000 | Loss: 0.00000787
Iteration 24/1000 | Loss: 0.00000786
Iteration 25/1000 | Loss: 0.00000786
Iteration 26/1000 | Loss: 0.00000786
Iteration 27/1000 | Loss: 0.00000786
Iteration 28/1000 | Loss: 0.00000785
Iteration 29/1000 | Loss: 0.00000785
Iteration 30/1000 | Loss: 0.00000785
Iteration 31/1000 | Loss: 0.00000784
Iteration 32/1000 | Loss: 0.00000782
Iteration 33/1000 | Loss: 0.00000781
Iteration 34/1000 | Loss: 0.00000780
Iteration 35/1000 | Loss: 0.00000774
Iteration 36/1000 | Loss: 0.00000774
Iteration 37/1000 | Loss: 0.00000774
Iteration 38/1000 | Loss: 0.00000774
Iteration 39/1000 | Loss: 0.00000773
Iteration 40/1000 | Loss: 0.00000772
Iteration 41/1000 | Loss: 0.00000772
Iteration 42/1000 | Loss: 0.00000771
Iteration 43/1000 | Loss: 0.00000771
Iteration 44/1000 | Loss: 0.00000770
Iteration 45/1000 | Loss: 0.00000770
Iteration 46/1000 | Loss: 0.00000769
Iteration 47/1000 | Loss: 0.00000769
Iteration 48/1000 | Loss: 0.00000769
Iteration 49/1000 | Loss: 0.00000768
Iteration 50/1000 | Loss: 0.00000768
Iteration 51/1000 | Loss: 0.00000767
Iteration 52/1000 | Loss: 0.00000767
Iteration 53/1000 | Loss: 0.00000766
Iteration 54/1000 | Loss: 0.00000766
Iteration 55/1000 | Loss: 0.00000766
Iteration 56/1000 | Loss: 0.00000766
Iteration 57/1000 | Loss: 0.00000766
Iteration 58/1000 | Loss: 0.00000765
Iteration 59/1000 | Loss: 0.00000765
Iteration 60/1000 | Loss: 0.00000765
Iteration 61/1000 | Loss: 0.00000765
Iteration 62/1000 | Loss: 0.00000765
Iteration 63/1000 | Loss: 0.00000765
Iteration 64/1000 | Loss: 0.00000764
Iteration 65/1000 | Loss: 0.00000764
Iteration 66/1000 | Loss: 0.00000764
Iteration 67/1000 | Loss: 0.00000764
Iteration 68/1000 | Loss: 0.00000763
Iteration 69/1000 | Loss: 0.00000763
Iteration 70/1000 | Loss: 0.00000762
Iteration 71/1000 | Loss: 0.00000762
Iteration 72/1000 | Loss: 0.00000761
Iteration 73/1000 | Loss: 0.00000761
Iteration 74/1000 | Loss: 0.00000761
Iteration 75/1000 | Loss: 0.00000761
Iteration 76/1000 | Loss: 0.00000761
Iteration 77/1000 | Loss: 0.00000761
Iteration 78/1000 | Loss: 0.00000761
Iteration 79/1000 | Loss: 0.00000760
Iteration 80/1000 | Loss: 0.00000760
Iteration 81/1000 | Loss: 0.00000760
Iteration 82/1000 | Loss: 0.00000760
Iteration 83/1000 | Loss: 0.00000759
Iteration 84/1000 | Loss: 0.00000759
Iteration 85/1000 | Loss: 0.00000759
Iteration 86/1000 | Loss: 0.00000758
Iteration 87/1000 | Loss: 0.00000758
Iteration 88/1000 | Loss: 0.00000758
Iteration 89/1000 | Loss: 0.00000758
Iteration 90/1000 | Loss: 0.00000758
Iteration 91/1000 | Loss: 0.00000758
Iteration 92/1000 | Loss: 0.00000757
Iteration 93/1000 | Loss: 0.00000757
Iteration 94/1000 | Loss: 0.00000756
Iteration 95/1000 | Loss: 0.00000756
Iteration 96/1000 | Loss: 0.00000756
Iteration 97/1000 | Loss: 0.00000756
Iteration 98/1000 | Loss: 0.00000756
Iteration 99/1000 | Loss: 0.00000756
Iteration 100/1000 | Loss: 0.00000756
Iteration 101/1000 | Loss: 0.00000756
Iteration 102/1000 | Loss: 0.00000755
Iteration 103/1000 | Loss: 0.00000755
Iteration 104/1000 | Loss: 0.00000755
Iteration 105/1000 | Loss: 0.00000755
Iteration 106/1000 | Loss: 0.00000755
Iteration 107/1000 | Loss: 0.00000755
Iteration 108/1000 | Loss: 0.00000755
Iteration 109/1000 | Loss: 0.00000755
Iteration 110/1000 | Loss: 0.00000755
Iteration 111/1000 | Loss: 0.00000755
Iteration 112/1000 | Loss: 0.00000755
Iteration 113/1000 | Loss: 0.00000755
Iteration 114/1000 | Loss: 0.00000754
Iteration 115/1000 | Loss: 0.00000754
Iteration 116/1000 | Loss: 0.00000754
Iteration 117/1000 | Loss: 0.00000754
Iteration 118/1000 | Loss: 0.00000754
Iteration 119/1000 | Loss: 0.00000754
Iteration 120/1000 | Loss: 0.00000754
Iteration 121/1000 | Loss: 0.00000754
Iteration 122/1000 | Loss: 0.00000754
Iteration 123/1000 | Loss: 0.00000753
Iteration 124/1000 | Loss: 0.00000753
Iteration 125/1000 | Loss: 0.00000753
Iteration 126/1000 | Loss: 0.00000752
Iteration 127/1000 | Loss: 0.00000752
Iteration 128/1000 | Loss: 0.00000752
Iteration 129/1000 | Loss: 0.00000752
Iteration 130/1000 | Loss: 0.00000752
Iteration 131/1000 | Loss: 0.00000752
Iteration 132/1000 | Loss: 0.00000752
Iteration 133/1000 | Loss: 0.00000752
Iteration 134/1000 | Loss: 0.00000752
Iteration 135/1000 | Loss: 0.00000752
Iteration 136/1000 | Loss: 0.00000751
Iteration 137/1000 | Loss: 0.00000751
Iteration 138/1000 | Loss: 0.00000751
Iteration 139/1000 | Loss: 0.00000750
Iteration 140/1000 | Loss: 0.00000750
Iteration 141/1000 | Loss: 0.00000750
Iteration 142/1000 | Loss: 0.00000750
Iteration 143/1000 | Loss: 0.00000749
Iteration 144/1000 | Loss: 0.00000749
Iteration 145/1000 | Loss: 0.00000749
Iteration 146/1000 | Loss: 0.00000749
Iteration 147/1000 | Loss: 0.00000748
Iteration 148/1000 | Loss: 0.00000748
Iteration 149/1000 | Loss: 0.00000748
Iteration 150/1000 | Loss: 0.00000748
Iteration 151/1000 | Loss: 0.00000748
Iteration 152/1000 | Loss: 0.00000748
Iteration 153/1000 | Loss: 0.00000748
Iteration 154/1000 | Loss: 0.00000747
Iteration 155/1000 | Loss: 0.00000747
Iteration 156/1000 | Loss: 0.00000747
Iteration 157/1000 | Loss: 0.00000747
Iteration 158/1000 | Loss: 0.00000746
Iteration 159/1000 | Loss: 0.00000746
Iteration 160/1000 | Loss: 0.00000746
Iteration 161/1000 | Loss: 0.00000745
Iteration 162/1000 | Loss: 0.00000745
Iteration 163/1000 | Loss: 0.00000745
Iteration 164/1000 | Loss: 0.00000745
Iteration 165/1000 | Loss: 0.00000745
Iteration 166/1000 | Loss: 0.00000745
Iteration 167/1000 | Loss: 0.00000745
Iteration 168/1000 | Loss: 0.00000745
Iteration 169/1000 | Loss: 0.00000745
Iteration 170/1000 | Loss: 0.00000745
Iteration 171/1000 | Loss: 0.00000745
Iteration 172/1000 | Loss: 0.00000745
Iteration 173/1000 | Loss: 0.00000744
Iteration 174/1000 | Loss: 0.00000744
Iteration 175/1000 | Loss: 0.00000744
Iteration 176/1000 | Loss: 0.00000744
Iteration 177/1000 | Loss: 0.00000744
Iteration 178/1000 | Loss: 0.00000744
Iteration 179/1000 | Loss: 0.00000743
Iteration 180/1000 | Loss: 0.00000743
Iteration 181/1000 | Loss: 0.00000743
Iteration 182/1000 | Loss: 0.00000743
Iteration 183/1000 | Loss: 0.00000743
Iteration 184/1000 | Loss: 0.00000743
Iteration 185/1000 | Loss: 0.00000743
Iteration 186/1000 | Loss: 0.00000743
Iteration 187/1000 | Loss: 0.00000743
Iteration 188/1000 | Loss: 0.00000742
Iteration 189/1000 | Loss: 0.00000742
Iteration 190/1000 | Loss: 0.00000742
Iteration 191/1000 | Loss: 0.00000742
Iteration 192/1000 | Loss: 0.00000742
Iteration 193/1000 | Loss: 0.00000742
Iteration 194/1000 | Loss: 0.00000742
Iteration 195/1000 | Loss: 0.00000742
Iteration 196/1000 | Loss: 0.00000742
Iteration 197/1000 | Loss: 0.00000742
Iteration 198/1000 | Loss: 0.00000742
Iteration 199/1000 | Loss: 0.00000742
Iteration 200/1000 | Loss: 0.00000741
Iteration 201/1000 | Loss: 0.00000741
Iteration 202/1000 | Loss: 0.00000741
Iteration 203/1000 | Loss: 0.00000741
Iteration 204/1000 | Loss: 0.00000741
Iteration 205/1000 | Loss: 0.00000741
Iteration 206/1000 | Loss: 0.00000741
Iteration 207/1000 | Loss: 0.00000741
Iteration 208/1000 | Loss: 0.00000741
Iteration 209/1000 | Loss: 0.00000741
Iteration 210/1000 | Loss: 0.00000741
Iteration 211/1000 | Loss: 0.00000741
Iteration 212/1000 | Loss: 0.00000741
Iteration 213/1000 | Loss: 0.00000740
Iteration 214/1000 | Loss: 0.00000740
Iteration 215/1000 | Loss: 0.00000740
Iteration 216/1000 | Loss: 0.00000740
Iteration 217/1000 | Loss: 0.00000740
Iteration 218/1000 | Loss: 0.00000740
Iteration 219/1000 | Loss: 0.00000740
Iteration 220/1000 | Loss: 0.00000740
Iteration 221/1000 | Loss: 0.00000740
Iteration 222/1000 | Loss: 0.00000740
Iteration 223/1000 | Loss: 0.00000740
Iteration 224/1000 | Loss: 0.00000740
Iteration 225/1000 | Loss: 0.00000740
Iteration 226/1000 | Loss: 0.00000740
Iteration 227/1000 | Loss: 0.00000740
Iteration 228/1000 | Loss: 0.00000740
Iteration 229/1000 | Loss: 0.00000740
Iteration 230/1000 | Loss: 0.00000740
Iteration 231/1000 | Loss: 0.00000740
Iteration 232/1000 | Loss: 0.00000740
Iteration 233/1000 | Loss: 0.00000740
Iteration 234/1000 | Loss: 0.00000740
Iteration 235/1000 | Loss: 0.00000740
Iteration 236/1000 | Loss: 0.00000740
Iteration 237/1000 | Loss: 0.00000740
Iteration 238/1000 | Loss: 0.00000740
Iteration 239/1000 | Loss: 0.00000740
Iteration 240/1000 | Loss: 0.00000740
Iteration 241/1000 | Loss: 0.00000740
Iteration 242/1000 | Loss: 0.00000739
Iteration 243/1000 | Loss: 0.00000739
Iteration 244/1000 | Loss: 0.00000739
Iteration 245/1000 | Loss: 0.00000739
Iteration 246/1000 | Loss: 0.00000739
Iteration 247/1000 | Loss: 0.00000739
Iteration 248/1000 | Loss: 0.00000739
Iteration 249/1000 | Loss: 0.00000739
Iteration 250/1000 | Loss: 0.00000739
Iteration 251/1000 | Loss: 0.00000739
Iteration 252/1000 | Loss: 0.00000739
Iteration 253/1000 | Loss: 0.00000739
Iteration 254/1000 | Loss: 0.00000739
Iteration 255/1000 | Loss: 0.00000739
Iteration 256/1000 | Loss: 0.00000739
Iteration 257/1000 | Loss: 0.00000739
Iteration 258/1000 | Loss: 0.00000739
Iteration 259/1000 | Loss: 0.00000739
Iteration 260/1000 | Loss: 0.00000739
Iteration 261/1000 | Loss: 0.00000739
Iteration 262/1000 | Loss: 0.00000739
Iteration 263/1000 | Loss: 0.00000739
Iteration 264/1000 | Loss: 0.00000739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [7.3949931902461685e-06, 7.3949931902461685e-06, 7.3949931902461685e-06, 7.3949931902461685e-06, 7.3949931902461685e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.3949931902461685e-06

Optimization complete. Final v2v error: 2.3448755741119385 mm

Highest mean error: 2.4476852416992188 mm for frame 82

Lowest mean error: 2.296299934387207 mm for frame 129

Saving results

Total time: 41.140708684921265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800413
Iteration 2/25 | Loss: 0.00114503
Iteration 3/25 | Loss: 0.00106897
Iteration 4/25 | Loss: 0.00105909
Iteration 5/25 | Loss: 0.00105560
Iteration 6/25 | Loss: 0.00105480
Iteration 7/25 | Loss: 0.00105480
Iteration 8/25 | Loss: 0.00105480
Iteration 9/25 | Loss: 0.00105480
Iteration 10/25 | Loss: 0.00105480
Iteration 11/25 | Loss: 0.00105480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010547982528805733, 0.0010547982528805733, 0.0010547982528805733, 0.0010547982528805733, 0.0010547982528805733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010547982528805733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36142170
Iteration 2/25 | Loss: 0.00084226
Iteration 3/25 | Loss: 0.00084226
Iteration 4/25 | Loss: 0.00084226
Iteration 5/25 | Loss: 0.00084226
Iteration 6/25 | Loss: 0.00084226
Iteration 7/25 | Loss: 0.00084226
Iteration 8/25 | Loss: 0.00084226
Iteration 9/25 | Loss: 0.00084226
Iteration 10/25 | Loss: 0.00084226
Iteration 11/25 | Loss: 0.00084226
Iteration 12/25 | Loss: 0.00084226
Iteration 13/25 | Loss: 0.00084226
Iteration 14/25 | Loss: 0.00084226
Iteration 15/25 | Loss: 0.00084226
Iteration 16/25 | Loss: 0.00084226
Iteration 17/25 | Loss: 0.00084226
Iteration 18/25 | Loss: 0.00084226
Iteration 19/25 | Loss: 0.00084226
Iteration 20/25 | Loss: 0.00084226
Iteration 21/25 | Loss: 0.00084226
Iteration 22/25 | Loss: 0.00084226
Iteration 23/25 | Loss: 0.00084226
Iteration 24/25 | Loss: 0.00084226
Iteration 25/25 | Loss: 0.00084226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084226
Iteration 2/1000 | Loss: 0.00002346
Iteration 3/1000 | Loss: 0.00001467
Iteration 4/1000 | Loss: 0.00001169
Iteration 5/1000 | Loss: 0.00001068
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000966
Iteration 8/1000 | Loss: 0.00000934
Iteration 9/1000 | Loss: 0.00000910
Iteration 10/1000 | Loss: 0.00000904
Iteration 11/1000 | Loss: 0.00000899
Iteration 12/1000 | Loss: 0.00000897
Iteration 13/1000 | Loss: 0.00000897
Iteration 14/1000 | Loss: 0.00000896
Iteration 15/1000 | Loss: 0.00000888
Iteration 16/1000 | Loss: 0.00000880
Iteration 17/1000 | Loss: 0.00000879
Iteration 18/1000 | Loss: 0.00000878
Iteration 19/1000 | Loss: 0.00000878
Iteration 20/1000 | Loss: 0.00000877
Iteration 21/1000 | Loss: 0.00000877
Iteration 22/1000 | Loss: 0.00000876
Iteration 23/1000 | Loss: 0.00000876
Iteration 24/1000 | Loss: 0.00000874
Iteration 25/1000 | Loss: 0.00000874
Iteration 26/1000 | Loss: 0.00000873
Iteration 27/1000 | Loss: 0.00000868
Iteration 28/1000 | Loss: 0.00000867
Iteration 29/1000 | Loss: 0.00000867
Iteration 30/1000 | Loss: 0.00000867
Iteration 31/1000 | Loss: 0.00000866
Iteration 32/1000 | Loss: 0.00000864
Iteration 33/1000 | Loss: 0.00000864
Iteration 34/1000 | Loss: 0.00000862
Iteration 35/1000 | Loss: 0.00000862
Iteration 36/1000 | Loss: 0.00000862
Iteration 37/1000 | Loss: 0.00000862
Iteration 38/1000 | Loss: 0.00000862
Iteration 39/1000 | Loss: 0.00000862
Iteration 40/1000 | Loss: 0.00000862
Iteration 41/1000 | Loss: 0.00000861
Iteration 42/1000 | Loss: 0.00000861
Iteration 43/1000 | Loss: 0.00000861
Iteration 44/1000 | Loss: 0.00000860
Iteration 45/1000 | Loss: 0.00000860
Iteration 46/1000 | Loss: 0.00000859
Iteration 47/1000 | Loss: 0.00000857
Iteration 48/1000 | Loss: 0.00000856
Iteration 49/1000 | Loss: 0.00000856
Iteration 50/1000 | Loss: 0.00000856
Iteration 51/1000 | Loss: 0.00000856
Iteration 52/1000 | Loss: 0.00000856
Iteration 53/1000 | Loss: 0.00000856
Iteration 54/1000 | Loss: 0.00000856
Iteration 55/1000 | Loss: 0.00000856
Iteration 56/1000 | Loss: 0.00000856
Iteration 57/1000 | Loss: 0.00000855
Iteration 58/1000 | Loss: 0.00000855
Iteration 59/1000 | Loss: 0.00000855
Iteration 60/1000 | Loss: 0.00000854
Iteration 61/1000 | Loss: 0.00000854
Iteration 62/1000 | Loss: 0.00000853
Iteration 63/1000 | Loss: 0.00000853
Iteration 64/1000 | Loss: 0.00000852
Iteration 65/1000 | Loss: 0.00000852
Iteration 66/1000 | Loss: 0.00000852
Iteration 67/1000 | Loss: 0.00000852
Iteration 68/1000 | Loss: 0.00000852
Iteration 69/1000 | Loss: 0.00000852
Iteration 70/1000 | Loss: 0.00000852
Iteration 71/1000 | Loss: 0.00000852
Iteration 72/1000 | Loss: 0.00000852
Iteration 73/1000 | Loss: 0.00000851
Iteration 74/1000 | Loss: 0.00000851
Iteration 75/1000 | Loss: 0.00000851
Iteration 76/1000 | Loss: 0.00000850
Iteration 77/1000 | Loss: 0.00000849
Iteration 78/1000 | Loss: 0.00000849
Iteration 79/1000 | Loss: 0.00000848
Iteration 80/1000 | Loss: 0.00000848
Iteration 81/1000 | Loss: 0.00000848
Iteration 82/1000 | Loss: 0.00000847
Iteration 83/1000 | Loss: 0.00000847
Iteration 84/1000 | Loss: 0.00000847
Iteration 85/1000 | Loss: 0.00000847
Iteration 86/1000 | Loss: 0.00000847
Iteration 87/1000 | Loss: 0.00000846
Iteration 88/1000 | Loss: 0.00000846
Iteration 89/1000 | Loss: 0.00000846
Iteration 90/1000 | Loss: 0.00000846
Iteration 91/1000 | Loss: 0.00000846
Iteration 92/1000 | Loss: 0.00000846
Iteration 93/1000 | Loss: 0.00000846
Iteration 94/1000 | Loss: 0.00000846
Iteration 95/1000 | Loss: 0.00000846
Iteration 96/1000 | Loss: 0.00000846
Iteration 97/1000 | Loss: 0.00000846
Iteration 98/1000 | Loss: 0.00000845
Iteration 99/1000 | Loss: 0.00000845
Iteration 100/1000 | Loss: 0.00000844
Iteration 101/1000 | Loss: 0.00000844
Iteration 102/1000 | Loss: 0.00000844
Iteration 103/1000 | Loss: 0.00000844
Iteration 104/1000 | Loss: 0.00000843
Iteration 105/1000 | Loss: 0.00000843
Iteration 106/1000 | Loss: 0.00000843
Iteration 107/1000 | Loss: 0.00000843
Iteration 108/1000 | Loss: 0.00000843
Iteration 109/1000 | Loss: 0.00000842
Iteration 110/1000 | Loss: 0.00000842
Iteration 111/1000 | Loss: 0.00000842
Iteration 112/1000 | Loss: 0.00000842
Iteration 113/1000 | Loss: 0.00000841
Iteration 114/1000 | Loss: 0.00000841
Iteration 115/1000 | Loss: 0.00000841
Iteration 116/1000 | Loss: 0.00000840
Iteration 117/1000 | Loss: 0.00000840
Iteration 118/1000 | Loss: 0.00000840
Iteration 119/1000 | Loss: 0.00000840
Iteration 120/1000 | Loss: 0.00000840
Iteration 121/1000 | Loss: 0.00000840
Iteration 122/1000 | Loss: 0.00000840
Iteration 123/1000 | Loss: 0.00000839
Iteration 124/1000 | Loss: 0.00000839
Iteration 125/1000 | Loss: 0.00000839
Iteration 126/1000 | Loss: 0.00000839
Iteration 127/1000 | Loss: 0.00000839
Iteration 128/1000 | Loss: 0.00000839
Iteration 129/1000 | Loss: 0.00000839
Iteration 130/1000 | Loss: 0.00000839
Iteration 131/1000 | Loss: 0.00000839
Iteration 132/1000 | Loss: 0.00000839
Iteration 133/1000 | Loss: 0.00000839
Iteration 134/1000 | Loss: 0.00000839
Iteration 135/1000 | Loss: 0.00000839
Iteration 136/1000 | Loss: 0.00000839
Iteration 137/1000 | Loss: 0.00000838
Iteration 138/1000 | Loss: 0.00000838
Iteration 139/1000 | Loss: 0.00000838
Iteration 140/1000 | Loss: 0.00000838
Iteration 141/1000 | Loss: 0.00000838
Iteration 142/1000 | Loss: 0.00000838
Iteration 143/1000 | Loss: 0.00000838
Iteration 144/1000 | Loss: 0.00000838
Iteration 145/1000 | Loss: 0.00000838
Iteration 146/1000 | Loss: 0.00000838
Iteration 147/1000 | Loss: 0.00000838
Iteration 148/1000 | Loss: 0.00000838
Iteration 149/1000 | Loss: 0.00000837
Iteration 150/1000 | Loss: 0.00000837
Iteration 151/1000 | Loss: 0.00000836
Iteration 152/1000 | Loss: 0.00000836
Iteration 153/1000 | Loss: 0.00000836
Iteration 154/1000 | Loss: 0.00000836
Iteration 155/1000 | Loss: 0.00000836
Iteration 156/1000 | Loss: 0.00000836
Iteration 157/1000 | Loss: 0.00000836
Iteration 158/1000 | Loss: 0.00000836
Iteration 159/1000 | Loss: 0.00000836
Iteration 160/1000 | Loss: 0.00000836
Iteration 161/1000 | Loss: 0.00000836
Iteration 162/1000 | Loss: 0.00000836
Iteration 163/1000 | Loss: 0.00000835
Iteration 164/1000 | Loss: 0.00000835
Iteration 165/1000 | Loss: 0.00000835
Iteration 166/1000 | Loss: 0.00000835
Iteration 167/1000 | Loss: 0.00000835
Iteration 168/1000 | Loss: 0.00000835
Iteration 169/1000 | Loss: 0.00000835
Iteration 170/1000 | Loss: 0.00000834
Iteration 171/1000 | Loss: 0.00000834
Iteration 172/1000 | Loss: 0.00000834
Iteration 173/1000 | Loss: 0.00000834
Iteration 174/1000 | Loss: 0.00000834
Iteration 175/1000 | Loss: 0.00000834
Iteration 176/1000 | Loss: 0.00000834
Iteration 177/1000 | Loss: 0.00000834
Iteration 178/1000 | Loss: 0.00000834
Iteration 179/1000 | Loss: 0.00000834
Iteration 180/1000 | Loss: 0.00000834
Iteration 181/1000 | Loss: 0.00000834
Iteration 182/1000 | Loss: 0.00000834
Iteration 183/1000 | Loss: 0.00000834
Iteration 184/1000 | Loss: 0.00000834
Iteration 185/1000 | Loss: 0.00000834
Iteration 186/1000 | Loss: 0.00000834
Iteration 187/1000 | Loss: 0.00000834
Iteration 188/1000 | Loss: 0.00000834
Iteration 189/1000 | Loss: 0.00000833
Iteration 190/1000 | Loss: 0.00000833
Iteration 191/1000 | Loss: 0.00000833
Iteration 192/1000 | Loss: 0.00000833
Iteration 193/1000 | Loss: 0.00000833
Iteration 194/1000 | Loss: 0.00000833
Iteration 195/1000 | Loss: 0.00000833
Iteration 196/1000 | Loss: 0.00000833
Iteration 197/1000 | Loss: 0.00000833
Iteration 198/1000 | Loss: 0.00000833
Iteration 199/1000 | Loss: 0.00000833
Iteration 200/1000 | Loss: 0.00000833
Iteration 201/1000 | Loss: 0.00000832
Iteration 202/1000 | Loss: 0.00000832
Iteration 203/1000 | Loss: 0.00000832
Iteration 204/1000 | Loss: 0.00000832
Iteration 205/1000 | Loss: 0.00000832
Iteration 206/1000 | Loss: 0.00000832
Iteration 207/1000 | Loss: 0.00000832
Iteration 208/1000 | Loss: 0.00000832
Iteration 209/1000 | Loss: 0.00000832
Iteration 210/1000 | Loss: 0.00000832
Iteration 211/1000 | Loss: 0.00000832
Iteration 212/1000 | Loss: 0.00000832
Iteration 213/1000 | Loss: 0.00000832
Iteration 214/1000 | Loss: 0.00000832
Iteration 215/1000 | Loss: 0.00000832
Iteration 216/1000 | Loss: 0.00000832
Iteration 217/1000 | Loss: 0.00000832
Iteration 218/1000 | Loss: 0.00000832
Iteration 219/1000 | Loss: 0.00000832
Iteration 220/1000 | Loss: 0.00000832
Iteration 221/1000 | Loss: 0.00000831
Iteration 222/1000 | Loss: 0.00000831
Iteration 223/1000 | Loss: 0.00000831
Iteration 224/1000 | Loss: 0.00000831
Iteration 225/1000 | Loss: 0.00000831
Iteration 226/1000 | Loss: 0.00000831
Iteration 227/1000 | Loss: 0.00000831
Iteration 228/1000 | Loss: 0.00000831
Iteration 229/1000 | Loss: 0.00000831
Iteration 230/1000 | Loss: 0.00000831
Iteration 231/1000 | Loss: 0.00000831
Iteration 232/1000 | Loss: 0.00000830
Iteration 233/1000 | Loss: 0.00000830
Iteration 234/1000 | Loss: 0.00000830
Iteration 235/1000 | Loss: 0.00000830
Iteration 236/1000 | Loss: 0.00000830
Iteration 237/1000 | Loss: 0.00000830
Iteration 238/1000 | Loss: 0.00000830
Iteration 239/1000 | Loss: 0.00000829
Iteration 240/1000 | Loss: 0.00000829
Iteration 241/1000 | Loss: 0.00000829
Iteration 242/1000 | Loss: 0.00000829
Iteration 243/1000 | Loss: 0.00000829
Iteration 244/1000 | Loss: 0.00000829
Iteration 245/1000 | Loss: 0.00000829
Iteration 246/1000 | Loss: 0.00000829
Iteration 247/1000 | Loss: 0.00000829
Iteration 248/1000 | Loss: 0.00000829
Iteration 249/1000 | Loss: 0.00000829
Iteration 250/1000 | Loss: 0.00000829
Iteration 251/1000 | Loss: 0.00000829
Iteration 252/1000 | Loss: 0.00000829
Iteration 253/1000 | Loss: 0.00000829
Iteration 254/1000 | Loss: 0.00000829
Iteration 255/1000 | Loss: 0.00000829
Iteration 256/1000 | Loss: 0.00000828
Iteration 257/1000 | Loss: 0.00000828
Iteration 258/1000 | Loss: 0.00000828
Iteration 259/1000 | Loss: 0.00000828
Iteration 260/1000 | Loss: 0.00000828
Iteration 261/1000 | Loss: 0.00000828
Iteration 262/1000 | Loss: 0.00000828
Iteration 263/1000 | Loss: 0.00000828
Iteration 264/1000 | Loss: 0.00000828
Iteration 265/1000 | Loss: 0.00000828
Iteration 266/1000 | Loss: 0.00000828
Iteration 267/1000 | Loss: 0.00000828
Iteration 268/1000 | Loss: 0.00000828
Iteration 269/1000 | Loss: 0.00000828
Iteration 270/1000 | Loss: 0.00000828
Iteration 271/1000 | Loss: 0.00000828
Iteration 272/1000 | Loss: 0.00000828
Iteration 273/1000 | Loss: 0.00000828
Iteration 274/1000 | Loss: 0.00000828
Iteration 275/1000 | Loss: 0.00000827
Iteration 276/1000 | Loss: 0.00000827
Iteration 277/1000 | Loss: 0.00000827
Iteration 278/1000 | Loss: 0.00000827
Iteration 279/1000 | Loss: 0.00000827
Iteration 280/1000 | Loss: 0.00000827
Iteration 281/1000 | Loss: 0.00000827
Iteration 282/1000 | Loss: 0.00000827
Iteration 283/1000 | Loss: 0.00000827
Iteration 284/1000 | Loss: 0.00000827
Iteration 285/1000 | Loss: 0.00000827
Iteration 286/1000 | Loss: 0.00000827
Iteration 287/1000 | Loss: 0.00000827
Iteration 288/1000 | Loss: 0.00000827
Iteration 289/1000 | Loss: 0.00000827
Iteration 290/1000 | Loss: 0.00000827
Iteration 291/1000 | Loss: 0.00000827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [8.270407306554262e-06, 8.270407306554262e-06, 8.270407306554262e-06, 8.270407306554262e-06, 8.270407306554262e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.270407306554262e-06

Optimization complete. Final v2v error: 2.474147081375122 mm

Highest mean error: 2.669360637664795 mm for frame 108

Lowest mean error: 2.3623366355895996 mm for frame 87

Saving results

Total time: 40.51624321937561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427139
Iteration 2/25 | Loss: 0.00124410
Iteration 3/25 | Loss: 0.00111539
Iteration 4/25 | Loss: 0.00109794
Iteration 5/25 | Loss: 0.00109359
Iteration 6/25 | Loss: 0.00109235
Iteration 7/25 | Loss: 0.00109197
Iteration 8/25 | Loss: 0.00109197
Iteration 9/25 | Loss: 0.00109197
Iteration 10/25 | Loss: 0.00109197
Iteration 11/25 | Loss: 0.00109197
Iteration 12/25 | Loss: 0.00109197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010919729247689247, 0.0010919729247689247, 0.0010919729247689247, 0.0010919729247689247, 0.0010919729247689247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010919729247689247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46048379
Iteration 2/25 | Loss: 0.00083651
Iteration 3/25 | Loss: 0.00083651
Iteration 4/25 | Loss: 0.00083651
Iteration 5/25 | Loss: 0.00083651
Iteration 6/25 | Loss: 0.00083651
Iteration 7/25 | Loss: 0.00083650
Iteration 8/25 | Loss: 0.00083650
Iteration 9/25 | Loss: 0.00083650
Iteration 10/25 | Loss: 0.00083650
Iteration 11/25 | Loss: 0.00083650
Iteration 12/25 | Loss: 0.00083650
Iteration 13/25 | Loss: 0.00083650
Iteration 14/25 | Loss: 0.00083650
Iteration 15/25 | Loss: 0.00083650
Iteration 16/25 | Loss: 0.00083650
Iteration 17/25 | Loss: 0.00083650
Iteration 18/25 | Loss: 0.00083650
Iteration 19/25 | Loss: 0.00083650
Iteration 20/25 | Loss: 0.00083650
Iteration 21/25 | Loss: 0.00083650
Iteration 22/25 | Loss: 0.00083650
Iteration 23/25 | Loss: 0.00083650
Iteration 24/25 | Loss: 0.00083650
Iteration 25/25 | Loss: 0.00083650

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083650
Iteration 2/1000 | Loss: 0.00002793
Iteration 3/1000 | Loss: 0.00001955
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001547
Iteration 6/1000 | Loss: 0.00001488
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001376
Iteration 10/1000 | Loss: 0.00001361
Iteration 11/1000 | Loss: 0.00001358
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001347
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001321
Iteration 18/1000 | Loss: 0.00001321
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001294
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001292
Iteration 27/1000 | Loss: 0.00001292
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001282
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001280
Iteration 36/1000 | Loss: 0.00001280
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001279
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001275
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001273
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001271
Iteration 48/1000 | Loss: 0.00001271
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001270
Iteration 51/1000 | Loss: 0.00001270
Iteration 52/1000 | Loss: 0.00001270
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001268
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001268
Iteration 60/1000 | Loss: 0.00001267
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001267
Iteration 64/1000 | Loss: 0.00001267
Iteration 65/1000 | Loss: 0.00001267
Iteration 66/1000 | Loss: 0.00001267
Iteration 67/1000 | Loss: 0.00001267
Iteration 68/1000 | Loss: 0.00001267
Iteration 69/1000 | Loss: 0.00001267
Iteration 70/1000 | Loss: 0.00001267
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001266
Iteration 73/1000 | Loss: 0.00001266
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001264
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001264
Iteration 81/1000 | Loss: 0.00001264
Iteration 82/1000 | Loss: 0.00001264
Iteration 83/1000 | Loss: 0.00001264
Iteration 84/1000 | Loss: 0.00001263
Iteration 85/1000 | Loss: 0.00001263
Iteration 86/1000 | Loss: 0.00001263
Iteration 87/1000 | Loss: 0.00001263
Iteration 88/1000 | Loss: 0.00001263
Iteration 89/1000 | Loss: 0.00001262
Iteration 90/1000 | Loss: 0.00001262
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001261
Iteration 94/1000 | Loss: 0.00001260
Iteration 95/1000 | Loss: 0.00001260
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001256
Iteration 116/1000 | Loss: 0.00001256
Iteration 117/1000 | Loss: 0.00001256
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001255
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001255
Iteration 128/1000 | Loss: 0.00001255
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001254
Iteration 135/1000 | Loss: 0.00001254
Iteration 136/1000 | Loss: 0.00001254
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001252
Iteration 144/1000 | Loss: 0.00001252
Iteration 145/1000 | Loss: 0.00001252
Iteration 146/1000 | Loss: 0.00001252
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001252
Iteration 149/1000 | Loss: 0.00001252
Iteration 150/1000 | Loss: 0.00001252
Iteration 151/1000 | Loss: 0.00001251
Iteration 152/1000 | Loss: 0.00001251
Iteration 153/1000 | Loss: 0.00001251
Iteration 154/1000 | Loss: 0.00001250
Iteration 155/1000 | Loss: 0.00001250
Iteration 156/1000 | Loss: 0.00001250
Iteration 157/1000 | Loss: 0.00001250
Iteration 158/1000 | Loss: 0.00001250
Iteration 159/1000 | Loss: 0.00001249
Iteration 160/1000 | Loss: 0.00001249
Iteration 161/1000 | Loss: 0.00001249
Iteration 162/1000 | Loss: 0.00001249
Iteration 163/1000 | Loss: 0.00001249
Iteration 164/1000 | Loss: 0.00001249
Iteration 165/1000 | Loss: 0.00001249
Iteration 166/1000 | Loss: 0.00001249
Iteration 167/1000 | Loss: 0.00001248
Iteration 168/1000 | Loss: 0.00001248
Iteration 169/1000 | Loss: 0.00001248
Iteration 170/1000 | Loss: 0.00001248
Iteration 171/1000 | Loss: 0.00001248
Iteration 172/1000 | Loss: 0.00001248
Iteration 173/1000 | Loss: 0.00001248
Iteration 174/1000 | Loss: 0.00001248
Iteration 175/1000 | Loss: 0.00001247
Iteration 176/1000 | Loss: 0.00001247
Iteration 177/1000 | Loss: 0.00001247
Iteration 178/1000 | Loss: 0.00001247
Iteration 179/1000 | Loss: 0.00001247
Iteration 180/1000 | Loss: 0.00001247
Iteration 181/1000 | Loss: 0.00001246
Iteration 182/1000 | Loss: 0.00001246
Iteration 183/1000 | Loss: 0.00001246
Iteration 184/1000 | Loss: 0.00001246
Iteration 185/1000 | Loss: 0.00001246
Iteration 186/1000 | Loss: 0.00001246
Iteration 187/1000 | Loss: 0.00001246
Iteration 188/1000 | Loss: 0.00001246
Iteration 189/1000 | Loss: 0.00001246
Iteration 190/1000 | Loss: 0.00001246
Iteration 191/1000 | Loss: 0.00001246
Iteration 192/1000 | Loss: 0.00001246
Iteration 193/1000 | Loss: 0.00001246
Iteration 194/1000 | Loss: 0.00001246
Iteration 195/1000 | Loss: 0.00001246
Iteration 196/1000 | Loss: 0.00001246
Iteration 197/1000 | Loss: 0.00001246
Iteration 198/1000 | Loss: 0.00001246
Iteration 199/1000 | Loss: 0.00001246
Iteration 200/1000 | Loss: 0.00001246
Iteration 201/1000 | Loss: 0.00001245
Iteration 202/1000 | Loss: 0.00001245
Iteration 203/1000 | Loss: 0.00001245
Iteration 204/1000 | Loss: 0.00001245
Iteration 205/1000 | Loss: 0.00001245
Iteration 206/1000 | Loss: 0.00001245
Iteration 207/1000 | Loss: 0.00001245
Iteration 208/1000 | Loss: 0.00001245
Iteration 209/1000 | Loss: 0.00001245
Iteration 210/1000 | Loss: 0.00001245
Iteration 211/1000 | Loss: 0.00001245
Iteration 212/1000 | Loss: 0.00001245
Iteration 213/1000 | Loss: 0.00001245
Iteration 214/1000 | Loss: 0.00001245
Iteration 215/1000 | Loss: 0.00001245
Iteration 216/1000 | Loss: 0.00001245
Iteration 217/1000 | Loss: 0.00001245
Iteration 218/1000 | Loss: 0.00001245
Iteration 219/1000 | Loss: 0.00001245
Iteration 220/1000 | Loss: 0.00001245
Iteration 221/1000 | Loss: 0.00001244
Iteration 222/1000 | Loss: 0.00001244
Iteration 223/1000 | Loss: 0.00001244
Iteration 224/1000 | Loss: 0.00001244
Iteration 225/1000 | Loss: 0.00001244
Iteration 226/1000 | Loss: 0.00001244
Iteration 227/1000 | Loss: 0.00001244
Iteration 228/1000 | Loss: 0.00001244
Iteration 229/1000 | Loss: 0.00001244
Iteration 230/1000 | Loss: 0.00001244
Iteration 231/1000 | Loss: 0.00001244
Iteration 232/1000 | Loss: 0.00001244
Iteration 233/1000 | Loss: 0.00001244
Iteration 234/1000 | Loss: 0.00001244
Iteration 235/1000 | Loss: 0.00001244
Iteration 236/1000 | Loss: 0.00001244
Iteration 237/1000 | Loss: 0.00001244
Iteration 238/1000 | Loss: 0.00001244
Iteration 239/1000 | Loss: 0.00001244
Iteration 240/1000 | Loss: 0.00001244
Iteration 241/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.2441362741810735e-05, 1.2441362741810735e-05, 1.2441362741810735e-05, 1.2441362741810735e-05, 1.2441362741810735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2441362741810735e-05

Optimization complete. Final v2v error: 2.9715120792388916 mm

Highest mean error: 4.373656272888184 mm for frame 46

Lowest mean error: 2.6065053939819336 mm for frame 86

Saving results

Total time: 45.609535455703735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848444
Iteration 2/25 | Loss: 0.00260669
Iteration 3/25 | Loss: 0.00188166
Iteration 4/25 | Loss: 0.00186799
Iteration 5/25 | Loss: 0.00164272
Iteration 6/25 | Loss: 0.00143411
Iteration 7/25 | Loss: 0.00134016
Iteration 8/25 | Loss: 0.00129812
Iteration 9/25 | Loss: 0.00128642
Iteration 10/25 | Loss: 0.00128362
Iteration 11/25 | Loss: 0.00128012
Iteration 12/25 | Loss: 0.00127613
Iteration 13/25 | Loss: 0.00127424
Iteration 14/25 | Loss: 0.00127253
Iteration 15/25 | Loss: 0.00127154
Iteration 16/25 | Loss: 0.00127019
Iteration 17/25 | Loss: 0.00126977
Iteration 18/25 | Loss: 0.00126962
Iteration 19/25 | Loss: 0.00126960
Iteration 20/25 | Loss: 0.00126960
Iteration 21/25 | Loss: 0.00126960
Iteration 22/25 | Loss: 0.00126960
Iteration 23/25 | Loss: 0.00126959
Iteration 24/25 | Loss: 0.00126959
Iteration 25/25 | Loss: 0.00126959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28523624
Iteration 2/25 | Loss: 0.00069994
Iteration 3/25 | Loss: 0.00069994
Iteration 4/25 | Loss: 0.00069994
Iteration 5/25 | Loss: 0.00069994
Iteration 6/25 | Loss: 0.00069994
Iteration 7/25 | Loss: 0.00069994
Iteration 8/25 | Loss: 0.00069994
Iteration 9/25 | Loss: 0.00069994
Iteration 10/25 | Loss: 0.00069994
Iteration 11/25 | Loss: 0.00069994
Iteration 12/25 | Loss: 0.00069994
Iteration 13/25 | Loss: 0.00069994
Iteration 14/25 | Loss: 0.00069994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006999384495429695, 0.0006999384495429695, 0.0006999384495429695, 0.0006999384495429695, 0.0006999384495429695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006999384495429695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069994
Iteration 2/1000 | Loss: 0.00003950
Iteration 3/1000 | Loss: 0.00002362
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00002024
Iteration 9/1000 | Loss: 0.00002017
Iteration 10/1000 | Loss: 0.00002010
Iteration 11/1000 | Loss: 0.00002009
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00002003
Iteration 17/1000 | Loss: 0.00002000
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00002000
Iteration 21/1000 | Loss: 0.00002000
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00001999
Iteration 24/1000 | Loss: 0.00001999
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001996
Iteration 27/1000 | Loss: 0.00001996
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001996
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001995
Iteration 34/1000 | Loss: 0.00001995
Iteration 35/1000 | Loss: 0.00001995
Iteration 36/1000 | Loss: 0.00001994
Iteration 37/1000 | Loss: 0.00001994
Iteration 38/1000 | Loss: 0.00001994
Iteration 39/1000 | Loss: 0.00001994
Iteration 40/1000 | Loss: 0.00001993
Iteration 41/1000 | Loss: 0.00001992
Iteration 42/1000 | Loss: 0.00001992
Iteration 43/1000 | Loss: 0.00001992
Iteration 44/1000 | Loss: 0.00001992
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001992
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001991
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001983
Iteration 55/1000 | Loss: 0.00001983
Iteration 56/1000 | Loss: 0.00001983
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001983
Iteration 60/1000 | Loss: 0.00001983
Iteration 61/1000 | Loss: 0.00001983
Iteration 62/1000 | Loss: 0.00001983
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001982
Iteration 65/1000 | Loss: 0.00001982
Iteration 66/1000 | Loss: 0.00001982
Iteration 67/1000 | Loss: 0.00001982
Iteration 68/1000 | Loss: 0.00001982
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001979
Iteration 71/1000 | Loss: 0.00001978
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001978
Iteration 74/1000 | Loss: 0.00001978
Iteration 75/1000 | Loss: 0.00001978
Iteration 76/1000 | Loss: 0.00001978
Iteration 77/1000 | Loss: 0.00001978
Iteration 78/1000 | Loss: 0.00001978
Iteration 79/1000 | Loss: 0.00001978
Iteration 80/1000 | Loss: 0.00001977
Iteration 81/1000 | Loss: 0.00001977
Iteration 82/1000 | Loss: 0.00001977
Iteration 83/1000 | Loss: 0.00001977
Iteration 84/1000 | Loss: 0.00001976
Iteration 85/1000 | Loss: 0.00001975
Iteration 86/1000 | Loss: 0.00001975
Iteration 87/1000 | Loss: 0.00001975
Iteration 88/1000 | Loss: 0.00001975
Iteration 89/1000 | Loss: 0.00001974
Iteration 90/1000 | Loss: 0.00001974
Iteration 91/1000 | Loss: 0.00001974
Iteration 92/1000 | Loss: 0.00001974
Iteration 93/1000 | Loss: 0.00001973
Iteration 94/1000 | Loss: 0.00001973
Iteration 95/1000 | Loss: 0.00001973
Iteration 96/1000 | Loss: 0.00001973
Iteration 97/1000 | Loss: 0.00001973
Iteration 98/1000 | Loss: 0.00001973
Iteration 99/1000 | Loss: 0.00001973
Iteration 100/1000 | Loss: 0.00001972
Iteration 101/1000 | Loss: 0.00001972
Iteration 102/1000 | Loss: 0.00001972
Iteration 103/1000 | Loss: 0.00001972
Iteration 104/1000 | Loss: 0.00001972
Iteration 105/1000 | Loss: 0.00001972
Iteration 106/1000 | Loss: 0.00001972
Iteration 107/1000 | Loss: 0.00001972
Iteration 108/1000 | Loss: 0.00001972
Iteration 109/1000 | Loss: 0.00001972
Iteration 110/1000 | Loss: 0.00001972
Iteration 111/1000 | Loss: 0.00001971
Iteration 112/1000 | Loss: 0.00001971
Iteration 113/1000 | Loss: 0.00001971
Iteration 114/1000 | Loss: 0.00001971
Iteration 115/1000 | Loss: 0.00001970
Iteration 116/1000 | Loss: 0.00001970
Iteration 117/1000 | Loss: 0.00001970
Iteration 118/1000 | Loss: 0.00001970
Iteration 119/1000 | Loss: 0.00001970
Iteration 120/1000 | Loss: 0.00001970
Iteration 121/1000 | Loss: 0.00001970
Iteration 122/1000 | Loss: 0.00001970
Iteration 123/1000 | Loss: 0.00001970
Iteration 124/1000 | Loss: 0.00001970
Iteration 125/1000 | Loss: 0.00001970
Iteration 126/1000 | Loss: 0.00001970
Iteration 127/1000 | Loss: 0.00001970
Iteration 128/1000 | Loss: 0.00001969
Iteration 129/1000 | Loss: 0.00001969
Iteration 130/1000 | Loss: 0.00001968
Iteration 131/1000 | Loss: 0.00001968
Iteration 132/1000 | Loss: 0.00001968
Iteration 133/1000 | Loss: 0.00001968
Iteration 134/1000 | Loss: 0.00001968
Iteration 135/1000 | Loss: 0.00001968
Iteration 136/1000 | Loss: 0.00001968
Iteration 137/1000 | Loss: 0.00001968
Iteration 138/1000 | Loss: 0.00001968
Iteration 139/1000 | Loss: 0.00001968
Iteration 140/1000 | Loss: 0.00001968
Iteration 141/1000 | Loss: 0.00001968
Iteration 142/1000 | Loss: 0.00001968
Iteration 143/1000 | Loss: 0.00001968
Iteration 144/1000 | Loss: 0.00001968
Iteration 145/1000 | Loss: 0.00001968
Iteration 146/1000 | Loss: 0.00001968
Iteration 147/1000 | Loss: 0.00001968
Iteration 148/1000 | Loss: 0.00001968
Iteration 149/1000 | Loss: 0.00001968
Iteration 150/1000 | Loss: 0.00001968
Iteration 151/1000 | Loss: 0.00001968
Iteration 152/1000 | Loss: 0.00001968
Iteration 153/1000 | Loss: 0.00001968
Iteration 154/1000 | Loss: 0.00001968
Iteration 155/1000 | Loss: 0.00001968
Iteration 156/1000 | Loss: 0.00001968
Iteration 157/1000 | Loss: 0.00001968
Iteration 158/1000 | Loss: 0.00001968
Iteration 159/1000 | Loss: 0.00001968
Iteration 160/1000 | Loss: 0.00001968
Iteration 161/1000 | Loss: 0.00001968
Iteration 162/1000 | Loss: 0.00001968
Iteration 163/1000 | Loss: 0.00001968
Iteration 164/1000 | Loss: 0.00001968
Iteration 165/1000 | Loss: 0.00001968
Iteration 166/1000 | Loss: 0.00001968
Iteration 167/1000 | Loss: 0.00001968
Iteration 168/1000 | Loss: 0.00001968
Iteration 169/1000 | Loss: 0.00001968
Iteration 170/1000 | Loss: 0.00001968
Iteration 171/1000 | Loss: 0.00001968
Iteration 172/1000 | Loss: 0.00001968
Iteration 173/1000 | Loss: 0.00001968
Iteration 174/1000 | Loss: 0.00001968
Iteration 175/1000 | Loss: 0.00001968
Iteration 176/1000 | Loss: 0.00001968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.9676232113852166e-05, 1.9676232113852166e-05, 1.9676232113852166e-05, 1.9676232113852166e-05, 1.9676232113852166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9676232113852166e-05

Optimization complete. Final v2v error: 3.7150778770446777 mm

Highest mean error: 4.042923927307129 mm for frame 203

Lowest mean error: 3.429399013519287 mm for frame 0

Saving results

Total time: 63.7866792678833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482572
Iteration 2/25 | Loss: 0.00125428
Iteration 3/25 | Loss: 0.00115095
Iteration 4/25 | Loss: 0.00113363
Iteration 5/25 | Loss: 0.00112864
Iteration 6/25 | Loss: 0.00112790
Iteration 7/25 | Loss: 0.00112790
Iteration 8/25 | Loss: 0.00112790
Iteration 9/25 | Loss: 0.00112790
Iteration 10/25 | Loss: 0.00112790
Iteration 11/25 | Loss: 0.00112790
Iteration 12/25 | Loss: 0.00112790
Iteration 13/25 | Loss: 0.00112790
Iteration 14/25 | Loss: 0.00112790
Iteration 15/25 | Loss: 0.00112790
Iteration 16/25 | Loss: 0.00112790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011278982274234295, 0.0011278982274234295, 0.0011278982274234295, 0.0011278982274234295, 0.0011278982274234295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011278982274234295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37086773
Iteration 2/25 | Loss: 0.00076763
Iteration 3/25 | Loss: 0.00076763
Iteration 4/25 | Loss: 0.00076763
Iteration 5/25 | Loss: 0.00076763
Iteration 6/25 | Loss: 0.00076763
Iteration 7/25 | Loss: 0.00076763
Iteration 8/25 | Loss: 0.00076763
Iteration 9/25 | Loss: 0.00076763
Iteration 10/25 | Loss: 0.00076763
Iteration 11/25 | Loss: 0.00076763
Iteration 12/25 | Loss: 0.00076763
Iteration 13/25 | Loss: 0.00076763
Iteration 14/25 | Loss: 0.00076763
Iteration 15/25 | Loss: 0.00076763
Iteration 16/25 | Loss: 0.00076763
Iteration 17/25 | Loss: 0.00076763
Iteration 18/25 | Loss: 0.00076763
Iteration 19/25 | Loss: 0.00076763
Iteration 20/25 | Loss: 0.00076763
Iteration 21/25 | Loss: 0.00076763
Iteration 22/25 | Loss: 0.00076763
Iteration 23/25 | Loss: 0.00076763
Iteration 24/25 | Loss: 0.00076763
Iteration 25/25 | Loss: 0.00076763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076763
Iteration 2/1000 | Loss: 0.00003725
Iteration 3/1000 | Loss: 0.00002690
Iteration 4/1000 | Loss: 0.00002340
Iteration 5/1000 | Loss: 0.00002252
Iteration 6/1000 | Loss: 0.00002183
Iteration 7/1000 | Loss: 0.00002130
Iteration 8/1000 | Loss: 0.00002081
Iteration 9/1000 | Loss: 0.00002053
Iteration 10/1000 | Loss: 0.00002024
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00001984
Iteration 13/1000 | Loss: 0.00001968
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001960
Iteration 17/1000 | Loss: 0.00001958
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001954
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001940
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001926
Iteration 31/1000 | Loss: 0.00001926
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001918
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001916
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001916
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001913
Iteration 75/1000 | Loss: 0.00001913
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001913
Iteration 78/1000 | Loss: 0.00001912
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001912
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001911
Iteration 89/1000 | Loss: 0.00001911
Iteration 90/1000 | Loss: 0.00001911
Iteration 91/1000 | Loss: 0.00001910
Iteration 92/1000 | Loss: 0.00001910
Iteration 93/1000 | Loss: 0.00001910
Iteration 94/1000 | Loss: 0.00001910
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001909
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001909
Iteration 99/1000 | Loss: 0.00001909
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001908
Iteration 105/1000 | Loss: 0.00001908
Iteration 106/1000 | Loss: 0.00001908
Iteration 107/1000 | Loss: 0.00001908
Iteration 108/1000 | Loss: 0.00001908
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001908
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001908
Iteration 118/1000 | Loss: 0.00001908
Iteration 119/1000 | Loss: 0.00001908
Iteration 120/1000 | Loss: 0.00001908
Iteration 121/1000 | Loss: 0.00001908
Iteration 122/1000 | Loss: 0.00001908
Iteration 123/1000 | Loss: 0.00001908
Iteration 124/1000 | Loss: 0.00001908
Iteration 125/1000 | Loss: 0.00001908
Iteration 126/1000 | Loss: 0.00001908
Iteration 127/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.9078594050370157e-05, 1.9078594050370157e-05, 1.9078594050370157e-05, 1.9078594050370157e-05, 1.9078594050370157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9078594050370157e-05

Optimization complete. Final v2v error: 3.6162261962890625 mm

Highest mean error: 5.184783935546875 mm for frame 39

Lowest mean error: 3.3082664012908936 mm for frame 21

Saving results

Total time: 38.42556881904602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507537
Iteration 2/25 | Loss: 0.00131142
Iteration 3/25 | Loss: 0.00113897
Iteration 4/25 | Loss: 0.00111749
Iteration 5/25 | Loss: 0.00111083
Iteration 6/25 | Loss: 0.00110896
Iteration 7/25 | Loss: 0.00110872
Iteration 8/25 | Loss: 0.00110872
Iteration 9/25 | Loss: 0.00110872
Iteration 10/25 | Loss: 0.00110872
Iteration 11/25 | Loss: 0.00110872
Iteration 12/25 | Loss: 0.00110872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011087217135354877, 0.0011087217135354877, 0.0011087217135354877, 0.0011087217135354877, 0.0011087217135354877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011087217135354877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.34868813
Iteration 2/25 | Loss: 0.00106442
Iteration 3/25 | Loss: 0.00106428
Iteration 4/25 | Loss: 0.00106428
Iteration 5/25 | Loss: 0.00106428
Iteration 6/25 | Loss: 0.00106428
Iteration 7/25 | Loss: 0.00106428
Iteration 8/25 | Loss: 0.00106428
Iteration 9/25 | Loss: 0.00106428
Iteration 10/25 | Loss: 0.00106428
Iteration 11/25 | Loss: 0.00106428
Iteration 12/25 | Loss: 0.00106428
Iteration 13/25 | Loss: 0.00106428
Iteration 14/25 | Loss: 0.00106428
Iteration 15/25 | Loss: 0.00106428
Iteration 16/25 | Loss: 0.00106428
Iteration 17/25 | Loss: 0.00106428
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010642787674441934, 0.0010642787674441934, 0.0010642787674441934, 0.0010642787674441934, 0.0010642787674441934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010642787674441934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106428
Iteration 2/1000 | Loss: 0.00004304
Iteration 3/1000 | Loss: 0.00002697
Iteration 4/1000 | Loss: 0.00001820
Iteration 5/1000 | Loss: 0.00001599
Iteration 6/1000 | Loss: 0.00001517
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001319
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001312
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001300
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001297
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001291
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001288
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001287
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001287
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001286
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001282
Iteration 73/1000 | Loss: 0.00001282
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001280
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001280
Iteration 82/1000 | Loss: 0.00001280
Iteration 83/1000 | Loss: 0.00001280
Iteration 84/1000 | Loss: 0.00001280
Iteration 85/1000 | Loss: 0.00001279
Iteration 86/1000 | Loss: 0.00001279
Iteration 87/1000 | Loss: 0.00001279
Iteration 88/1000 | Loss: 0.00001279
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001278
Iteration 93/1000 | Loss: 0.00001278
Iteration 94/1000 | Loss: 0.00001278
Iteration 95/1000 | Loss: 0.00001278
Iteration 96/1000 | Loss: 0.00001278
Iteration 97/1000 | Loss: 0.00001278
Iteration 98/1000 | Loss: 0.00001277
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001277
Iteration 101/1000 | Loss: 0.00001277
Iteration 102/1000 | Loss: 0.00001277
Iteration 103/1000 | Loss: 0.00001277
Iteration 104/1000 | Loss: 0.00001277
Iteration 105/1000 | Loss: 0.00001277
Iteration 106/1000 | Loss: 0.00001277
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001277
Iteration 110/1000 | Loss: 0.00001277
Iteration 111/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.2774067727150396e-05, 1.2774067727150396e-05, 1.2774067727150396e-05, 1.2774067727150396e-05, 1.2774067727150396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2774067727150396e-05

Optimization complete. Final v2v error: 3.0159099102020264 mm

Highest mean error: 3.3878893852233887 mm for frame 107

Lowest mean error: 2.704704523086548 mm for frame 116

Saving results

Total time: 35.25876331329346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896953
Iteration 2/25 | Loss: 0.00132313
Iteration 3/25 | Loss: 0.00113865
Iteration 4/25 | Loss: 0.00111772
Iteration 5/25 | Loss: 0.00111040
Iteration 6/25 | Loss: 0.00110786
Iteration 7/25 | Loss: 0.00110794
Iteration 8/25 | Loss: 0.00110615
Iteration 9/25 | Loss: 0.00110547
Iteration 10/25 | Loss: 0.00110512
Iteration 11/25 | Loss: 0.00110508
Iteration 12/25 | Loss: 0.00110507
Iteration 13/25 | Loss: 0.00110507
Iteration 14/25 | Loss: 0.00110507
Iteration 15/25 | Loss: 0.00110507
Iteration 16/25 | Loss: 0.00110507
Iteration 17/25 | Loss: 0.00110507
Iteration 18/25 | Loss: 0.00110507
Iteration 19/25 | Loss: 0.00110507
Iteration 20/25 | Loss: 0.00110507
Iteration 21/25 | Loss: 0.00110507
Iteration 22/25 | Loss: 0.00110507
Iteration 23/25 | Loss: 0.00110507
Iteration 24/25 | Loss: 0.00110507
Iteration 25/25 | Loss: 0.00110507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35067439
Iteration 2/25 | Loss: 0.00068745
Iteration 3/25 | Loss: 0.00068742
Iteration 4/25 | Loss: 0.00068742
Iteration 5/25 | Loss: 0.00068742
Iteration 6/25 | Loss: 0.00068742
Iteration 7/25 | Loss: 0.00068742
Iteration 8/25 | Loss: 0.00068742
Iteration 9/25 | Loss: 0.00068742
Iteration 10/25 | Loss: 0.00068742
Iteration 11/25 | Loss: 0.00068742
Iteration 12/25 | Loss: 0.00068742
Iteration 13/25 | Loss: 0.00068742
Iteration 14/25 | Loss: 0.00068742
Iteration 15/25 | Loss: 0.00068742
Iteration 16/25 | Loss: 0.00068742
Iteration 17/25 | Loss: 0.00068742
Iteration 18/25 | Loss: 0.00068742
Iteration 19/25 | Loss: 0.00068742
Iteration 20/25 | Loss: 0.00068742
Iteration 21/25 | Loss: 0.00068742
Iteration 22/25 | Loss: 0.00068742
Iteration 23/25 | Loss: 0.00068742
Iteration 24/25 | Loss: 0.00068742
Iteration 25/25 | Loss: 0.00068742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068742
Iteration 2/1000 | Loss: 0.00002415
Iteration 3/1000 | Loss: 0.00001511
Iteration 4/1000 | Loss: 0.00001338
Iteration 5/1000 | Loss: 0.00001248
Iteration 6/1000 | Loss: 0.00001198
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001094
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001056
Iteration 12/1000 | Loss: 0.00001050
Iteration 13/1000 | Loss: 0.00001049
Iteration 14/1000 | Loss: 0.00001046
Iteration 15/1000 | Loss: 0.00001045
Iteration 16/1000 | Loss: 0.00001043
Iteration 17/1000 | Loss: 0.00001042
Iteration 18/1000 | Loss: 0.00001035
Iteration 19/1000 | Loss: 0.00001034
Iteration 20/1000 | Loss: 0.00001030
Iteration 21/1000 | Loss: 0.00001029
Iteration 22/1000 | Loss: 0.00001028
Iteration 23/1000 | Loss: 0.00001028
Iteration 24/1000 | Loss: 0.00001027
Iteration 25/1000 | Loss: 0.00001026
Iteration 26/1000 | Loss: 0.00001025
Iteration 27/1000 | Loss: 0.00001025
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001023
Iteration 30/1000 | Loss: 0.00001022
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001021
Iteration 33/1000 | Loss: 0.00001021
Iteration 34/1000 | Loss: 0.00001020
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001020
Iteration 37/1000 | Loss: 0.00001020
Iteration 38/1000 | Loss: 0.00001019
Iteration 39/1000 | Loss: 0.00001019
Iteration 40/1000 | Loss: 0.00001018
Iteration 41/1000 | Loss: 0.00001018
Iteration 42/1000 | Loss: 0.00001017
Iteration 43/1000 | Loss: 0.00001017
Iteration 44/1000 | Loss: 0.00001017
Iteration 45/1000 | Loss: 0.00001017
Iteration 46/1000 | Loss: 0.00001017
Iteration 47/1000 | Loss: 0.00001017
Iteration 48/1000 | Loss: 0.00001017
Iteration 49/1000 | Loss: 0.00001016
Iteration 50/1000 | Loss: 0.00001016
Iteration 51/1000 | Loss: 0.00001016
Iteration 52/1000 | Loss: 0.00001016
Iteration 53/1000 | Loss: 0.00001016
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001016
Iteration 56/1000 | Loss: 0.00001015
Iteration 57/1000 | Loss: 0.00001015
Iteration 58/1000 | Loss: 0.00001015
Iteration 59/1000 | Loss: 0.00001015
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001013
Iteration 69/1000 | Loss: 0.00001012
Iteration 70/1000 | Loss: 0.00001012
Iteration 71/1000 | Loss: 0.00001011
Iteration 72/1000 | Loss: 0.00001011
Iteration 73/1000 | Loss: 0.00001011
Iteration 74/1000 | Loss: 0.00001011
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001010
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001010
Iteration 82/1000 | Loss: 0.00001010
Iteration 83/1000 | Loss: 0.00001010
Iteration 84/1000 | Loss: 0.00001010
Iteration 85/1000 | Loss: 0.00001010
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001009
Iteration 88/1000 | Loss: 0.00001009
Iteration 89/1000 | Loss: 0.00001009
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001009
Iteration 92/1000 | Loss: 0.00001009
Iteration 93/1000 | Loss: 0.00001009
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001008
Iteration 100/1000 | Loss: 0.00001008
Iteration 101/1000 | Loss: 0.00001008
Iteration 102/1000 | Loss: 0.00001008
Iteration 103/1000 | Loss: 0.00001008
Iteration 104/1000 | Loss: 0.00001007
Iteration 105/1000 | Loss: 0.00001007
Iteration 106/1000 | Loss: 0.00001007
Iteration 107/1000 | Loss: 0.00001007
Iteration 108/1000 | Loss: 0.00001007
Iteration 109/1000 | Loss: 0.00001007
Iteration 110/1000 | Loss: 0.00001007
Iteration 111/1000 | Loss: 0.00001007
Iteration 112/1000 | Loss: 0.00001007
Iteration 113/1000 | Loss: 0.00001007
Iteration 114/1000 | Loss: 0.00001007
Iteration 115/1000 | Loss: 0.00001006
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001006
Iteration 118/1000 | Loss: 0.00001006
Iteration 119/1000 | Loss: 0.00001006
Iteration 120/1000 | Loss: 0.00001006
Iteration 121/1000 | Loss: 0.00001006
Iteration 122/1000 | Loss: 0.00001006
Iteration 123/1000 | Loss: 0.00001006
Iteration 124/1000 | Loss: 0.00001006
Iteration 125/1000 | Loss: 0.00001006
Iteration 126/1000 | Loss: 0.00001006
Iteration 127/1000 | Loss: 0.00001006
Iteration 128/1000 | Loss: 0.00001006
Iteration 129/1000 | Loss: 0.00001006
Iteration 130/1000 | Loss: 0.00001006
Iteration 131/1000 | Loss: 0.00001006
Iteration 132/1000 | Loss: 0.00001006
Iteration 133/1000 | Loss: 0.00001006
Iteration 134/1000 | Loss: 0.00001006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.0064713933388703e-05, 1.0064713933388703e-05, 1.0064713933388703e-05, 1.0064713933388703e-05, 1.0064713933388703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0064713933388703e-05

Optimization complete. Final v2v error: 2.715763568878174 mm

Highest mean error: 2.9639177322387695 mm for frame 51

Lowest mean error: 2.505587100982666 mm for frame 138

Saving results

Total time: 50.11926293373108
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021089
Iteration 2/25 | Loss: 0.01021089
Iteration 3/25 | Loss: 0.01021089
Iteration 4/25 | Loss: 0.01021088
Iteration 5/25 | Loss: 0.01021088
Iteration 6/25 | Loss: 0.01021088
Iteration 7/25 | Loss: 0.01021088
Iteration 8/25 | Loss: 0.01021088
Iteration 9/25 | Loss: 0.01021088
Iteration 10/25 | Loss: 0.01021087
Iteration 11/25 | Loss: 0.01021087
Iteration 12/25 | Loss: 0.01021087
Iteration 13/25 | Loss: 0.01021087
Iteration 14/25 | Loss: 0.01021087
Iteration 15/25 | Loss: 0.01021087
Iteration 16/25 | Loss: 0.01021086
Iteration 17/25 | Loss: 0.01021086
Iteration 18/25 | Loss: 0.01021086
Iteration 19/25 | Loss: 0.01021086
Iteration 20/25 | Loss: 0.01021086
Iteration 21/25 | Loss: 0.01021086
Iteration 22/25 | Loss: 0.01021086
Iteration 23/25 | Loss: 0.01021086
Iteration 24/25 | Loss: 0.01021085
Iteration 25/25 | Loss: 0.01021085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45272255
Iteration 2/25 | Loss: 0.17391606
Iteration 3/25 | Loss: 0.17295499
Iteration 4/25 | Loss: 0.17243932
Iteration 5/25 | Loss: 0.17242898
Iteration 6/25 | Loss: 0.17242900
Iteration 7/25 | Loss: 0.17242900
Iteration 8/25 | Loss: 0.17242900
Iteration 9/25 | Loss: 0.17242900
Iteration 10/25 | Loss: 0.17242900
Iteration 11/25 | Loss: 0.17242900
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.17242899537086487, 0.17242899537086487, 0.17242899537086487, 0.17242899537086487, 0.17242899537086487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17242899537086487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17242900
Iteration 2/1000 | Loss: 0.01467965
Iteration 3/1000 | Loss: 0.00203127
Iteration 4/1000 | Loss: 0.00058386
Iteration 5/1000 | Loss: 0.00043019
Iteration 6/1000 | Loss: 0.00016437
Iteration 7/1000 | Loss: 0.00009878
Iteration 8/1000 | Loss: 0.00006827
Iteration 9/1000 | Loss: 0.00037837
Iteration 10/1000 | Loss: 0.00004962
Iteration 11/1000 | Loss: 0.00004042
Iteration 12/1000 | Loss: 0.00005671
Iteration 13/1000 | Loss: 0.00003253
Iteration 14/1000 | Loss: 0.00004705
Iteration 15/1000 | Loss: 0.00013583
Iteration 16/1000 | Loss: 0.00002602
Iteration 17/1000 | Loss: 0.00002937
Iteration 18/1000 | Loss: 0.00002789
Iteration 19/1000 | Loss: 0.00002260
Iteration 20/1000 | Loss: 0.00002835
Iteration 21/1000 | Loss: 0.00002441
Iteration 22/1000 | Loss: 0.00025027
Iteration 23/1000 | Loss: 0.00002320
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00011649
Iteration 26/1000 | Loss: 0.00001979
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00002984
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001747
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001717
Iteration 39/1000 | Loss: 0.00001714
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001704
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001700
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001698
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001688
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001684
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001682
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00001682
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001681
Iteration 96/1000 | Loss: 0.00001681
Iteration 97/1000 | Loss: 0.00001681
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001680
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001679
Iteration 103/1000 | Loss: 0.00001679
Iteration 104/1000 | Loss: 0.00001679
Iteration 105/1000 | Loss: 0.00001679
Iteration 106/1000 | Loss: 0.00001679
Iteration 107/1000 | Loss: 0.00001679
Iteration 108/1000 | Loss: 0.00003347
Iteration 109/1000 | Loss: 0.00002043
Iteration 110/1000 | Loss: 0.00001678
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001677
Iteration 113/1000 | Loss: 0.00001677
Iteration 114/1000 | Loss: 0.00001677
Iteration 115/1000 | Loss: 0.00001677
Iteration 116/1000 | Loss: 0.00001677
Iteration 117/1000 | Loss: 0.00001677
Iteration 118/1000 | Loss: 0.00001677
Iteration 119/1000 | Loss: 0.00001677
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00002276
Iteration 124/1000 | Loss: 0.00001678
Iteration 125/1000 | Loss: 0.00001677
Iteration 126/1000 | Loss: 0.00001677
Iteration 127/1000 | Loss: 0.00001677
Iteration 128/1000 | Loss: 0.00001677
Iteration 129/1000 | Loss: 0.00001677
Iteration 130/1000 | Loss: 0.00001677
Iteration 131/1000 | Loss: 0.00001677
Iteration 132/1000 | Loss: 0.00001677
Iteration 133/1000 | Loss: 0.00001677
Iteration 134/1000 | Loss: 0.00001677
Iteration 135/1000 | Loss: 0.00001677
Iteration 136/1000 | Loss: 0.00001676
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001676
Iteration 140/1000 | Loss: 0.00001676
Iteration 141/1000 | Loss: 0.00001676
Iteration 142/1000 | Loss: 0.00001676
Iteration 143/1000 | Loss: 0.00001676
Iteration 144/1000 | Loss: 0.00001676
Iteration 145/1000 | Loss: 0.00001676
Iteration 146/1000 | Loss: 0.00001676
Iteration 147/1000 | Loss: 0.00001676
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00001675
Iteration 154/1000 | Loss: 0.00001675
Iteration 155/1000 | Loss: 0.00001675
Iteration 156/1000 | Loss: 0.00001674
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001674
Iteration 159/1000 | Loss: 0.00001674
Iteration 160/1000 | Loss: 0.00001674
Iteration 161/1000 | Loss: 0.00001674
Iteration 162/1000 | Loss: 0.00001674
Iteration 163/1000 | Loss: 0.00001674
Iteration 164/1000 | Loss: 0.00001674
Iteration 165/1000 | Loss: 0.00001674
Iteration 166/1000 | Loss: 0.00001674
Iteration 167/1000 | Loss: 0.00001674
Iteration 168/1000 | Loss: 0.00001674
Iteration 169/1000 | Loss: 0.00001674
Iteration 170/1000 | Loss: 0.00001674
Iteration 171/1000 | Loss: 0.00001674
Iteration 172/1000 | Loss: 0.00001674
Iteration 173/1000 | Loss: 0.00001674
Iteration 174/1000 | Loss: 0.00001674
Iteration 175/1000 | Loss: 0.00001674
Iteration 176/1000 | Loss: 0.00001674
Iteration 177/1000 | Loss: 0.00001674
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001673
Iteration 180/1000 | Loss: 0.00001673
Iteration 181/1000 | Loss: 0.00001673
Iteration 182/1000 | Loss: 0.00001673
Iteration 183/1000 | Loss: 0.00001673
Iteration 184/1000 | Loss: 0.00001673
Iteration 185/1000 | Loss: 0.00001673
Iteration 186/1000 | Loss: 0.00001673
Iteration 187/1000 | Loss: 0.00001673
Iteration 188/1000 | Loss: 0.00001673
Iteration 189/1000 | Loss: 0.00001673
Iteration 190/1000 | Loss: 0.00001673
Iteration 191/1000 | Loss: 0.00001673
Iteration 192/1000 | Loss: 0.00001673
Iteration 193/1000 | Loss: 0.00001673
Iteration 194/1000 | Loss: 0.00001673
Iteration 195/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.672576581768226e-05, 1.672576581768226e-05, 1.672576581768226e-05, 1.672576581768226e-05, 1.672576581768226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.672576581768226e-05

Optimization complete. Final v2v error: 3.4326651096343994 mm

Highest mean error: 3.886472702026367 mm for frame 59

Lowest mean error: 3.0091867446899414 mm for frame 223

Saving results

Total time: 81.41995596885681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806763
Iteration 2/25 | Loss: 0.00121856
Iteration 3/25 | Loss: 0.00112439
Iteration 4/25 | Loss: 0.00110833
Iteration 5/25 | Loss: 0.00110363
Iteration 6/25 | Loss: 0.00110312
Iteration 7/25 | Loss: 0.00110312
Iteration 8/25 | Loss: 0.00110312
Iteration 9/25 | Loss: 0.00110312
Iteration 10/25 | Loss: 0.00110312
Iteration 11/25 | Loss: 0.00110312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011031226022168994, 0.0011031226022168994, 0.0011031226022168994, 0.0011031226022168994, 0.0011031226022168994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011031226022168994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.07216835
Iteration 2/25 | Loss: 0.00082864
Iteration 3/25 | Loss: 0.00082864
Iteration 4/25 | Loss: 0.00082864
Iteration 5/25 | Loss: 0.00082864
Iteration 6/25 | Loss: 0.00082863
Iteration 7/25 | Loss: 0.00082863
Iteration 8/25 | Loss: 0.00082863
Iteration 9/25 | Loss: 0.00082863
Iteration 10/25 | Loss: 0.00082863
Iteration 11/25 | Loss: 0.00082863
Iteration 12/25 | Loss: 0.00082863
Iteration 13/25 | Loss: 0.00082863
Iteration 14/25 | Loss: 0.00082863
Iteration 15/25 | Loss: 0.00082863
Iteration 16/25 | Loss: 0.00082863
Iteration 17/25 | Loss: 0.00082863
Iteration 18/25 | Loss: 0.00082863
Iteration 19/25 | Loss: 0.00082863
Iteration 20/25 | Loss: 0.00082863
Iteration 21/25 | Loss: 0.00082863
Iteration 22/25 | Loss: 0.00082863
Iteration 23/25 | Loss: 0.00082863
Iteration 24/25 | Loss: 0.00082863
Iteration 25/25 | Loss: 0.00082863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082863
Iteration 2/1000 | Loss: 0.00002811
Iteration 3/1000 | Loss: 0.00002054
Iteration 4/1000 | Loss: 0.00001839
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001622
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001568
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001474
Iteration 14/1000 | Loss: 0.00001474
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001456
Iteration 17/1000 | Loss: 0.00001451
Iteration 18/1000 | Loss: 0.00001446
Iteration 19/1000 | Loss: 0.00001445
Iteration 20/1000 | Loss: 0.00001445
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001434
Iteration 42/1000 | Loss: 0.00001434
Iteration 43/1000 | Loss: 0.00001434
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001433
Iteration 51/1000 | Loss: 0.00001433
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001432
Iteration 54/1000 | Loss: 0.00001432
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001431
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001431
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001431
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001429
Iteration 66/1000 | Loss: 0.00001429
Iteration 67/1000 | Loss: 0.00001429
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001428
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001426
Iteration 76/1000 | Loss: 0.00001426
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001425
Iteration 79/1000 | Loss: 0.00001425
Iteration 80/1000 | Loss: 0.00001425
Iteration 81/1000 | Loss: 0.00001425
Iteration 82/1000 | Loss: 0.00001424
Iteration 83/1000 | Loss: 0.00001424
Iteration 84/1000 | Loss: 0.00001424
Iteration 85/1000 | Loss: 0.00001424
Iteration 86/1000 | Loss: 0.00001423
Iteration 87/1000 | Loss: 0.00001423
Iteration 88/1000 | Loss: 0.00001423
Iteration 89/1000 | Loss: 0.00001422
Iteration 90/1000 | Loss: 0.00001422
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001421
Iteration 95/1000 | Loss: 0.00001421
Iteration 96/1000 | Loss: 0.00001421
Iteration 97/1000 | Loss: 0.00001421
Iteration 98/1000 | Loss: 0.00001421
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001415
Iteration 111/1000 | Loss: 0.00001415
Iteration 112/1000 | Loss: 0.00001414
Iteration 113/1000 | Loss: 0.00001414
Iteration 114/1000 | Loss: 0.00001413
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001411
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001410
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001410
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001409
Iteration 134/1000 | Loss: 0.00001409
Iteration 135/1000 | Loss: 0.00001409
Iteration 136/1000 | Loss: 0.00001408
Iteration 137/1000 | Loss: 0.00001408
Iteration 138/1000 | Loss: 0.00001408
Iteration 139/1000 | Loss: 0.00001408
Iteration 140/1000 | Loss: 0.00001408
Iteration 141/1000 | Loss: 0.00001408
Iteration 142/1000 | Loss: 0.00001408
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001407
Iteration 146/1000 | Loss: 0.00001407
Iteration 147/1000 | Loss: 0.00001407
Iteration 148/1000 | Loss: 0.00001407
Iteration 149/1000 | Loss: 0.00001407
Iteration 150/1000 | Loss: 0.00001407
Iteration 151/1000 | Loss: 0.00001407
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001407
Iteration 155/1000 | Loss: 0.00001407
Iteration 156/1000 | Loss: 0.00001407
Iteration 157/1000 | Loss: 0.00001407
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001407
Iteration 160/1000 | Loss: 0.00001407
Iteration 161/1000 | Loss: 0.00001407
Iteration 162/1000 | Loss: 0.00001407
Iteration 163/1000 | Loss: 0.00001407
Iteration 164/1000 | Loss: 0.00001407
Iteration 165/1000 | Loss: 0.00001407
Iteration 166/1000 | Loss: 0.00001407
Iteration 167/1000 | Loss: 0.00001407
Iteration 168/1000 | Loss: 0.00001407
Iteration 169/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4068495147512294e-05, 1.4068495147512294e-05, 1.4068495147512294e-05, 1.4068495147512294e-05, 1.4068495147512294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4068495147512294e-05

Optimization complete. Final v2v error: 3.180750608444214 mm

Highest mean error: 3.4304020404815674 mm for frame 128

Lowest mean error: 2.9491045475006104 mm for frame 206

Saving results

Total time: 43.42467427253723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963642
Iteration 2/25 | Loss: 0.00323729
Iteration 3/25 | Loss: 0.00161825
Iteration 4/25 | Loss: 0.00140314
Iteration 5/25 | Loss: 0.00137201
Iteration 6/25 | Loss: 0.00134763
Iteration 7/25 | Loss: 0.00124206
Iteration 8/25 | Loss: 0.00119965
Iteration 9/25 | Loss: 0.00118175
Iteration 10/25 | Loss: 0.00118003
Iteration 11/25 | Loss: 0.00118103
Iteration 12/25 | Loss: 0.00117941
Iteration 13/25 | Loss: 0.00117486
Iteration 14/25 | Loss: 0.00117182
Iteration 15/25 | Loss: 0.00117143
Iteration 16/25 | Loss: 0.00117131
Iteration 17/25 | Loss: 0.00117122
Iteration 18/25 | Loss: 0.00117349
Iteration 19/25 | Loss: 0.00116933
Iteration 20/25 | Loss: 0.00116774
Iteration 21/25 | Loss: 0.00116714
Iteration 22/25 | Loss: 0.00116693
Iteration 23/25 | Loss: 0.00116681
Iteration 24/25 | Loss: 0.00116679
Iteration 25/25 | Loss: 0.00116679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33647752
Iteration 2/25 | Loss: 0.00075486
Iteration 3/25 | Loss: 0.00075486
Iteration 4/25 | Loss: 0.00075486
Iteration 5/25 | Loss: 0.00075486
Iteration 6/25 | Loss: 0.00075486
Iteration 7/25 | Loss: 0.00075486
Iteration 8/25 | Loss: 0.00075486
Iteration 9/25 | Loss: 0.00075486
Iteration 10/25 | Loss: 0.00075486
Iteration 11/25 | Loss: 0.00075486
Iteration 12/25 | Loss: 0.00075486
Iteration 13/25 | Loss: 0.00075486
Iteration 14/25 | Loss: 0.00075486
Iteration 15/25 | Loss: 0.00075486
Iteration 16/25 | Loss: 0.00075486
Iteration 17/25 | Loss: 0.00075486
Iteration 18/25 | Loss: 0.00075486
Iteration 19/25 | Loss: 0.00075486
Iteration 20/25 | Loss: 0.00075486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007548553985543549, 0.0007548553985543549, 0.0007548553985543549, 0.0007548553985543549, 0.0007548553985543549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007548553985543549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075486
Iteration 2/1000 | Loss: 0.00005507
Iteration 3/1000 | Loss: 0.00003232
Iteration 4/1000 | Loss: 0.00002816
Iteration 5/1000 | Loss: 0.00002625
Iteration 6/1000 | Loss: 0.00002489
Iteration 7/1000 | Loss: 0.00002415
Iteration 8/1000 | Loss: 0.00002365
Iteration 9/1000 | Loss: 0.00002317
Iteration 10/1000 | Loss: 0.00002279
Iteration 11/1000 | Loss: 0.00002241
Iteration 12/1000 | Loss: 0.00002202
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002139
Iteration 15/1000 | Loss: 0.00002121
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002077
Iteration 18/1000 | Loss: 0.00053384
Iteration 19/1000 | Loss: 0.00044612
Iteration 20/1000 | Loss: 0.00013618
Iteration 21/1000 | Loss: 0.00002215
Iteration 22/1000 | Loss: 0.00002036
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001972
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00040316
Iteration 28/1000 | Loss: 0.00011113
Iteration 29/1000 | Loss: 0.00001992
Iteration 30/1000 | Loss: 0.00001944
Iteration 31/1000 | Loss: 0.00041708
Iteration 32/1000 | Loss: 0.00005643
Iteration 33/1000 | Loss: 0.00002029
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00044446
Iteration 37/1000 | Loss: 0.00010123
Iteration 38/1000 | Loss: 0.00002815
Iteration 39/1000 | Loss: 0.00003890
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002571
Iteration 42/1000 | Loss: 0.00002459
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002743
Iteration 45/1000 | Loss: 0.00002237
Iteration 46/1000 | Loss: 0.00002132
Iteration 47/1000 | Loss: 0.00002059
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00001958
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001916
Iteration 53/1000 | Loss: 0.00001915
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001914
Iteration 58/1000 | Loss: 0.00001913
Iteration 59/1000 | Loss: 0.00001913
Iteration 60/1000 | Loss: 0.00001913
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001912
Iteration 63/1000 | Loss: 0.00001912
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001910
Iteration 67/1000 | Loss: 0.00001910
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001909
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001907
Iteration 78/1000 | Loss: 0.00001906
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001905
Iteration 84/1000 | Loss: 0.00001905
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001904
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Iteration 91/1000 | Loss: 0.00001903
Iteration 92/1000 | Loss: 0.00001903
Iteration 93/1000 | Loss: 0.00001902
Iteration 94/1000 | Loss: 0.00001902
Iteration 95/1000 | Loss: 0.00001902
Iteration 96/1000 | Loss: 0.00001902
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001901
Iteration 99/1000 | Loss: 0.00001901
Iteration 100/1000 | Loss: 0.00001901
Iteration 101/1000 | Loss: 0.00001901
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001900
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001899
Iteration 111/1000 | Loss: 0.00001899
Iteration 112/1000 | Loss: 0.00001899
Iteration 113/1000 | Loss: 0.00001899
Iteration 114/1000 | Loss: 0.00001899
Iteration 115/1000 | Loss: 0.00001899
Iteration 116/1000 | Loss: 0.00001899
Iteration 117/1000 | Loss: 0.00001899
Iteration 118/1000 | Loss: 0.00001899
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001898
Iteration 121/1000 | Loss: 0.00001898
Iteration 122/1000 | Loss: 0.00001898
Iteration 123/1000 | Loss: 0.00001898
Iteration 124/1000 | Loss: 0.00001898
Iteration 125/1000 | Loss: 0.00001898
Iteration 126/1000 | Loss: 0.00001898
Iteration 127/1000 | Loss: 0.00001897
Iteration 128/1000 | Loss: 0.00001897
Iteration 129/1000 | Loss: 0.00001897
Iteration 130/1000 | Loss: 0.00001897
Iteration 131/1000 | Loss: 0.00001896
Iteration 132/1000 | Loss: 0.00001896
Iteration 133/1000 | Loss: 0.00001896
Iteration 134/1000 | Loss: 0.00001896
Iteration 135/1000 | Loss: 0.00001896
Iteration 136/1000 | Loss: 0.00001896
Iteration 137/1000 | Loss: 0.00001896
Iteration 138/1000 | Loss: 0.00001896
Iteration 139/1000 | Loss: 0.00001896
Iteration 140/1000 | Loss: 0.00001896
Iteration 141/1000 | Loss: 0.00001896
Iteration 142/1000 | Loss: 0.00001896
Iteration 143/1000 | Loss: 0.00001896
Iteration 144/1000 | Loss: 0.00001896
Iteration 145/1000 | Loss: 0.00001896
Iteration 146/1000 | Loss: 0.00001895
Iteration 147/1000 | Loss: 0.00001895
Iteration 148/1000 | Loss: 0.00001895
Iteration 149/1000 | Loss: 0.00001895
Iteration 150/1000 | Loss: 0.00001895
Iteration 151/1000 | Loss: 0.00001895
Iteration 152/1000 | Loss: 0.00001895
Iteration 153/1000 | Loss: 0.00001895
Iteration 154/1000 | Loss: 0.00001895
Iteration 155/1000 | Loss: 0.00001895
Iteration 156/1000 | Loss: 0.00001895
Iteration 157/1000 | Loss: 0.00001895
Iteration 158/1000 | Loss: 0.00001894
Iteration 159/1000 | Loss: 0.00001894
Iteration 160/1000 | Loss: 0.00001894
Iteration 161/1000 | Loss: 0.00001894
Iteration 162/1000 | Loss: 0.00001894
Iteration 163/1000 | Loss: 0.00001894
Iteration 164/1000 | Loss: 0.00001894
Iteration 165/1000 | Loss: 0.00001894
Iteration 166/1000 | Loss: 0.00001894
Iteration 167/1000 | Loss: 0.00001893
Iteration 168/1000 | Loss: 0.00001893
Iteration 169/1000 | Loss: 0.00001893
Iteration 170/1000 | Loss: 0.00001893
Iteration 171/1000 | Loss: 0.00001893
Iteration 172/1000 | Loss: 0.00001893
Iteration 173/1000 | Loss: 0.00001893
Iteration 174/1000 | Loss: 0.00001893
Iteration 175/1000 | Loss: 0.00001893
Iteration 176/1000 | Loss: 0.00001893
Iteration 177/1000 | Loss: 0.00001893
Iteration 178/1000 | Loss: 0.00001893
Iteration 179/1000 | Loss: 0.00001893
Iteration 180/1000 | Loss: 0.00001893
Iteration 181/1000 | Loss: 0.00001893
Iteration 182/1000 | Loss: 0.00001892
Iteration 183/1000 | Loss: 0.00001892
Iteration 184/1000 | Loss: 0.00001892
Iteration 185/1000 | Loss: 0.00001892
Iteration 186/1000 | Loss: 0.00001892
Iteration 187/1000 | Loss: 0.00001892
Iteration 188/1000 | Loss: 0.00001892
Iteration 189/1000 | Loss: 0.00001892
Iteration 190/1000 | Loss: 0.00001892
Iteration 191/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.892336513265036e-05, 1.892336513265036e-05, 1.892336513265036e-05, 1.892336513265036e-05, 1.892336513265036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.892336513265036e-05

Optimization complete. Final v2v error: 3.198721408843994 mm

Highest mean error: 12.45654010772705 mm for frame 13

Lowest mean error: 2.625722646713257 mm for frame 132

Saving results

Total time: 138.27361011505127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010130
Iteration 2/25 | Loss: 0.00207670
Iteration 3/25 | Loss: 0.00144542
Iteration 4/25 | Loss: 0.00128540
Iteration 5/25 | Loss: 0.00125320
Iteration 6/25 | Loss: 0.00124084
Iteration 7/25 | Loss: 0.00122760
Iteration 8/25 | Loss: 0.00123148
Iteration 9/25 | Loss: 0.00123171
Iteration 10/25 | Loss: 0.00121810
Iteration 11/25 | Loss: 0.00121232
Iteration 12/25 | Loss: 0.00121075
Iteration 13/25 | Loss: 0.00121043
Iteration 14/25 | Loss: 0.00121032
Iteration 15/25 | Loss: 0.00121030
Iteration 16/25 | Loss: 0.00121030
Iteration 17/25 | Loss: 0.00121029
Iteration 18/25 | Loss: 0.00121029
Iteration 19/25 | Loss: 0.00121029
Iteration 20/25 | Loss: 0.00121029
Iteration 21/25 | Loss: 0.00121029
Iteration 22/25 | Loss: 0.00121029
Iteration 23/25 | Loss: 0.00121029
Iteration 24/25 | Loss: 0.00121029
Iteration 25/25 | Loss: 0.00121029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30728459
Iteration 2/25 | Loss: 0.00085456
Iteration 3/25 | Loss: 0.00085456
Iteration 4/25 | Loss: 0.00085456
Iteration 5/25 | Loss: 0.00085456
Iteration 6/25 | Loss: 0.00085456
Iteration 7/25 | Loss: 0.00085456
Iteration 8/25 | Loss: 0.00085456
Iteration 9/25 | Loss: 0.00085456
Iteration 10/25 | Loss: 0.00085456
Iteration 11/25 | Loss: 0.00085456
Iteration 12/25 | Loss: 0.00085456
Iteration 13/25 | Loss: 0.00085456
Iteration 14/25 | Loss: 0.00085456
Iteration 15/25 | Loss: 0.00085456
Iteration 16/25 | Loss: 0.00085456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008545588934794068, 0.0008545588934794068, 0.0008545588934794068, 0.0008545588934794068, 0.0008545588934794068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008545588934794068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085456
Iteration 2/1000 | Loss: 0.00003696
Iteration 3/1000 | Loss: 0.00002496
Iteration 4/1000 | Loss: 0.00002293
Iteration 5/1000 | Loss: 0.00002210
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002113
Iteration 8/1000 | Loss: 0.00002083
Iteration 9/1000 | Loss: 0.00002049
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00001977
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001970
Iteration 17/1000 | Loss: 0.00001970
Iteration 18/1000 | Loss: 0.00001962
Iteration 19/1000 | Loss: 0.00004592
Iteration 20/1000 | Loss: 0.00003267
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00003750
Iteration 23/1000 | Loss: 0.00005259
Iteration 24/1000 | Loss: 0.00002778
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001848
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001844
Iteration 35/1000 | Loss: 0.00001844
Iteration 36/1000 | Loss: 0.00001844
Iteration 37/1000 | Loss: 0.00001843
Iteration 38/1000 | Loss: 0.00001843
Iteration 39/1000 | Loss: 0.00001843
Iteration 40/1000 | Loss: 0.00001843
Iteration 41/1000 | Loss: 0.00001842
Iteration 42/1000 | Loss: 0.00001842
Iteration 43/1000 | Loss: 0.00001842
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001842
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001841
Iteration 50/1000 | Loss: 0.00001841
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001841
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001841
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.8414091755403206e-05, 1.8414091755403206e-05, 1.8414091755403206e-05, 1.8414091755403206e-05, 1.8414091755403206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8414091755403206e-05

Optimization complete. Final v2v error: 3.632767677307129 mm

Highest mean error: 10.772711753845215 mm for frame 234

Lowest mean error: 3.4277331829071045 mm for frame 12

Saving results

Total time: 66.13047504425049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969415
Iteration 2/25 | Loss: 0.00248537
Iteration 3/25 | Loss: 0.00161949
Iteration 4/25 | Loss: 0.00152832
Iteration 5/25 | Loss: 0.00156664
Iteration 6/25 | Loss: 0.00150532
Iteration 7/25 | Loss: 0.00145313
Iteration 8/25 | Loss: 0.00143844
Iteration 9/25 | Loss: 0.00144375
Iteration 10/25 | Loss: 0.00143896
Iteration 11/25 | Loss: 0.00142656
Iteration 12/25 | Loss: 0.00143131
Iteration 13/25 | Loss: 0.00141176
Iteration 14/25 | Loss: 0.00140218
Iteration 15/25 | Loss: 0.00139962
Iteration 16/25 | Loss: 0.00139806
Iteration 17/25 | Loss: 0.00139712
Iteration 18/25 | Loss: 0.00139648
Iteration 19/25 | Loss: 0.00139564
Iteration 20/25 | Loss: 0.00139607
Iteration 21/25 | Loss: 0.00140263
Iteration 22/25 | Loss: 0.00139589
Iteration 23/25 | Loss: 0.00139068
Iteration 24/25 | Loss: 0.00139177
Iteration 25/25 | Loss: 0.00138835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36125684
Iteration 2/25 | Loss: 0.00250812
Iteration 3/25 | Loss: 0.00250812
Iteration 4/25 | Loss: 0.00250812
Iteration 5/25 | Loss: 0.00250812
Iteration 6/25 | Loss: 0.00250812
Iteration 7/25 | Loss: 0.00250812
Iteration 8/25 | Loss: 0.00250812
Iteration 9/25 | Loss: 0.00250812
Iteration 10/25 | Loss: 0.00250812
Iteration 11/25 | Loss: 0.00250812
Iteration 12/25 | Loss: 0.00250812
Iteration 13/25 | Loss: 0.00250812
Iteration 14/25 | Loss: 0.00250812
Iteration 15/25 | Loss: 0.00250812
Iteration 16/25 | Loss: 0.00250812
Iteration 17/25 | Loss: 0.00250812
Iteration 18/25 | Loss: 0.00250812
Iteration 19/25 | Loss: 0.00250812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0025081182830035686, 0.0025081182830035686, 0.0025081182830035686, 0.0025081182830035686, 0.0025081182830035686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025081182830035686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250812
Iteration 2/1000 | Loss: 0.00031872
Iteration 3/1000 | Loss: 0.00025367
Iteration 4/1000 | Loss: 0.00021404
Iteration 5/1000 | Loss: 0.00019599
Iteration 6/1000 | Loss: 0.00017809
Iteration 7/1000 | Loss: 0.00047125
Iteration 8/1000 | Loss: 0.00020880
Iteration 9/1000 | Loss: 0.00015212
Iteration 10/1000 | Loss: 0.00022371
Iteration 11/1000 | Loss: 0.00017598
Iteration 12/1000 | Loss: 0.00013971
Iteration 13/1000 | Loss: 0.00013599
Iteration 14/1000 | Loss: 0.00033750
Iteration 15/1000 | Loss: 0.00014042
Iteration 16/1000 | Loss: 0.00075036
Iteration 17/1000 | Loss: 0.00852147
Iteration 18/1000 | Loss: 0.00713643
Iteration 19/1000 | Loss: 0.00200323
Iteration 20/1000 | Loss: 0.00115796
Iteration 21/1000 | Loss: 0.00035252
Iteration 22/1000 | Loss: 0.00025743
Iteration 23/1000 | Loss: 0.00019540
Iteration 24/1000 | Loss: 0.00014232
Iteration 25/1000 | Loss: 0.00029945
Iteration 26/1000 | Loss: 0.00008658
Iteration 27/1000 | Loss: 0.00005620
Iteration 28/1000 | Loss: 0.00009403
Iteration 29/1000 | Loss: 0.00014743
Iteration 30/1000 | Loss: 0.00004378
Iteration 31/1000 | Loss: 0.00003640
Iteration 32/1000 | Loss: 0.00021876
Iteration 33/1000 | Loss: 0.00012933
Iteration 34/1000 | Loss: 0.00016278
Iteration 35/1000 | Loss: 0.00016533
Iteration 36/1000 | Loss: 0.00005134
Iteration 37/1000 | Loss: 0.00003249
Iteration 38/1000 | Loss: 0.00002901
Iteration 39/1000 | Loss: 0.00002649
Iteration 40/1000 | Loss: 0.00002477
Iteration 41/1000 | Loss: 0.00002359
Iteration 42/1000 | Loss: 0.00002296
Iteration 43/1000 | Loss: 0.00002232
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002092
Iteration 48/1000 | Loss: 0.00002077
Iteration 49/1000 | Loss: 0.00002055
Iteration 50/1000 | Loss: 0.00002046
Iteration 51/1000 | Loss: 0.00002019
Iteration 52/1000 | Loss: 0.00004240
Iteration 53/1000 | Loss: 0.00009040
Iteration 54/1000 | Loss: 0.00019201
Iteration 55/1000 | Loss: 0.00009157
Iteration 56/1000 | Loss: 0.00012878
Iteration 57/1000 | Loss: 0.00018942
Iteration 58/1000 | Loss: 0.00026850
Iteration 59/1000 | Loss: 0.00018919
Iteration 60/1000 | Loss: 0.00058164
Iteration 61/1000 | Loss: 0.00009920
Iteration 62/1000 | Loss: 0.00015677
Iteration 63/1000 | Loss: 0.00005964
Iteration 64/1000 | Loss: 0.00021013
Iteration 65/1000 | Loss: 0.00002493
Iteration 66/1000 | Loss: 0.00002283
Iteration 67/1000 | Loss: 0.00086200
Iteration 68/1000 | Loss: 0.00005251
Iteration 69/1000 | Loss: 0.00003717
Iteration 70/1000 | Loss: 0.00002370
Iteration 71/1000 | Loss: 0.00002181
Iteration 72/1000 | Loss: 0.00002098
Iteration 73/1000 | Loss: 0.00002018
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00001943
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001839
Iteration 79/1000 | Loss: 0.00001780
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001679
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001664
Iteration 87/1000 | Loss: 0.00001664
Iteration 88/1000 | Loss: 0.00001664
Iteration 89/1000 | Loss: 0.00001664
Iteration 90/1000 | Loss: 0.00001664
Iteration 91/1000 | Loss: 0.00001664
Iteration 92/1000 | Loss: 0.00001664
Iteration 93/1000 | Loss: 0.00001664
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001663
Iteration 104/1000 | Loss: 0.00001663
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001662
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001659
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001658
Iteration 123/1000 | Loss: 0.00001658
Iteration 124/1000 | Loss: 0.00001657
Iteration 125/1000 | Loss: 0.00001657
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001656
Iteration 130/1000 | Loss: 0.00001656
Iteration 131/1000 | Loss: 0.00001656
Iteration 132/1000 | Loss: 0.00001656
Iteration 133/1000 | Loss: 0.00001656
Iteration 134/1000 | Loss: 0.00001656
Iteration 135/1000 | Loss: 0.00001656
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001655
Iteration 143/1000 | Loss: 0.00001655
Iteration 144/1000 | Loss: 0.00001655
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001655
Iteration 151/1000 | Loss: 0.00001655
Iteration 152/1000 | Loss: 0.00001655
Iteration 153/1000 | Loss: 0.00001655
Iteration 154/1000 | Loss: 0.00001655
Iteration 155/1000 | Loss: 0.00001655
Iteration 156/1000 | Loss: 0.00001655
Iteration 157/1000 | Loss: 0.00001655
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001654
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001654
Iteration 165/1000 | Loss: 0.00001654
Iteration 166/1000 | Loss: 0.00001654
Iteration 167/1000 | Loss: 0.00001654
Iteration 168/1000 | Loss: 0.00001654
Iteration 169/1000 | Loss: 0.00001654
Iteration 170/1000 | Loss: 0.00001654
Iteration 171/1000 | Loss: 0.00001654
Iteration 172/1000 | Loss: 0.00001654
Iteration 173/1000 | Loss: 0.00001654
Iteration 174/1000 | Loss: 0.00001654
Iteration 175/1000 | Loss: 0.00001654
Iteration 176/1000 | Loss: 0.00001654
Iteration 177/1000 | Loss: 0.00001654
Iteration 178/1000 | Loss: 0.00001654
Iteration 179/1000 | Loss: 0.00001654
Iteration 180/1000 | Loss: 0.00001654
Iteration 181/1000 | Loss: 0.00001654
Iteration 182/1000 | Loss: 0.00001654
Iteration 183/1000 | Loss: 0.00001654
Iteration 184/1000 | Loss: 0.00001654
Iteration 185/1000 | Loss: 0.00001654
Iteration 186/1000 | Loss: 0.00001654
Iteration 187/1000 | Loss: 0.00001654
Iteration 188/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.6538631825824268e-05, 1.6538631825824268e-05, 1.6538631825824268e-05, 1.6538631825824268e-05, 1.6538631825824268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6538631825824268e-05

Optimization complete. Final v2v error: 3.3530585765838623 mm

Highest mean error: 5.1747636795043945 mm for frame 5

Lowest mean error: 2.989833354949951 mm for frame 103

Saving results

Total time: 163.71477007865906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391713
Iteration 2/25 | Loss: 0.00114208
Iteration 3/25 | Loss: 0.00108491
Iteration 4/25 | Loss: 0.00107776
Iteration 5/25 | Loss: 0.00107549
Iteration 6/25 | Loss: 0.00107549
Iteration 7/25 | Loss: 0.00107549
Iteration 8/25 | Loss: 0.00107549
Iteration 9/25 | Loss: 0.00107549
Iteration 10/25 | Loss: 0.00107549
Iteration 11/25 | Loss: 0.00107549
Iteration 12/25 | Loss: 0.00107549
Iteration 13/25 | Loss: 0.00107549
Iteration 14/25 | Loss: 0.00107549
Iteration 15/25 | Loss: 0.00107549
Iteration 16/25 | Loss: 0.00107549
Iteration 17/25 | Loss: 0.00107549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010754887480288744, 0.0010754887480288744, 0.0010754887480288744, 0.0010754887480288744, 0.0010754887480288744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010754887480288744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35859942
Iteration 2/25 | Loss: 0.00081853
Iteration 3/25 | Loss: 0.00081853
Iteration 4/25 | Loss: 0.00081853
Iteration 5/25 | Loss: 0.00081853
Iteration 6/25 | Loss: 0.00081853
Iteration 7/25 | Loss: 0.00081853
Iteration 8/25 | Loss: 0.00081853
Iteration 9/25 | Loss: 0.00081853
Iteration 10/25 | Loss: 0.00081852
Iteration 11/25 | Loss: 0.00081852
Iteration 12/25 | Loss: 0.00081852
Iteration 13/25 | Loss: 0.00081852
Iteration 14/25 | Loss: 0.00081852
Iteration 15/25 | Loss: 0.00081852
Iteration 16/25 | Loss: 0.00081852
Iteration 17/25 | Loss: 0.00081852
Iteration 18/25 | Loss: 0.00081852
Iteration 19/25 | Loss: 0.00081852
Iteration 20/25 | Loss: 0.00081852
Iteration 21/25 | Loss: 0.00081852
Iteration 22/25 | Loss: 0.00081852
Iteration 23/25 | Loss: 0.00081852
Iteration 24/25 | Loss: 0.00081852
Iteration 25/25 | Loss: 0.00081852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081852
Iteration 2/1000 | Loss: 0.00002092
Iteration 3/1000 | Loss: 0.00001390
Iteration 4/1000 | Loss: 0.00001184
Iteration 5/1000 | Loss: 0.00001128
Iteration 6/1000 | Loss: 0.00001108
Iteration 7/1000 | Loss: 0.00001076
Iteration 8/1000 | Loss: 0.00001040
Iteration 9/1000 | Loss: 0.00001023
Iteration 10/1000 | Loss: 0.00001015
Iteration 11/1000 | Loss: 0.00001014
Iteration 12/1000 | Loss: 0.00000986
Iteration 13/1000 | Loss: 0.00000986
Iteration 14/1000 | Loss: 0.00000985
Iteration 15/1000 | Loss: 0.00000985
Iteration 16/1000 | Loss: 0.00000984
Iteration 17/1000 | Loss: 0.00000983
Iteration 18/1000 | Loss: 0.00000983
Iteration 19/1000 | Loss: 0.00000978
Iteration 20/1000 | Loss: 0.00000974
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000967
Iteration 23/1000 | Loss: 0.00000967
Iteration 24/1000 | Loss: 0.00000966
Iteration 25/1000 | Loss: 0.00000960
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000955
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000953
Iteration 30/1000 | Loss: 0.00000953
Iteration 31/1000 | Loss: 0.00000944
Iteration 32/1000 | Loss: 0.00000941
Iteration 33/1000 | Loss: 0.00000940
Iteration 34/1000 | Loss: 0.00000940
Iteration 35/1000 | Loss: 0.00000938
Iteration 36/1000 | Loss: 0.00000938
Iteration 37/1000 | Loss: 0.00000938
Iteration 38/1000 | Loss: 0.00000937
Iteration 39/1000 | Loss: 0.00000936
Iteration 40/1000 | Loss: 0.00000934
Iteration 41/1000 | Loss: 0.00000934
Iteration 42/1000 | Loss: 0.00000934
Iteration 43/1000 | Loss: 0.00000934
Iteration 44/1000 | Loss: 0.00000933
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000933
Iteration 47/1000 | Loss: 0.00000933
Iteration 48/1000 | Loss: 0.00000933
Iteration 49/1000 | Loss: 0.00000933
Iteration 50/1000 | Loss: 0.00000933
Iteration 51/1000 | Loss: 0.00000933
Iteration 52/1000 | Loss: 0.00000933
Iteration 53/1000 | Loss: 0.00000933
Iteration 54/1000 | Loss: 0.00000933
Iteration 55/1000 | Loss: 0.00000932
Iteration 56/1000 | Loss: 0.00000931
Iteration 57/1000 | Loss: 0.00000931
Iteration 58/1000 | Loss: 0.00000931
Iteration 59/1000 | Loss: 0.00000931
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000930
Iteration 62/1000 | Loss: 0.00000930
Iteration 63/1000 | Loss: 0.00000930
Iteration 64/1000 | Loss: 0.00000929
Iteration 65/1000 | Loss: 0.00000929
Iteration 66/1000 | Loss: 0.00000929
Iteration 67/1000 | Loss: 0.00000929
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000929
Iteration 70/1000 | Loss: 0.00000929
Iteration 71/1000 | Loss: 0.00000928
Iteration 72/1000 | Loss: 0.00000928
Iteration 73/1000 | Loss: 0.00000925
Iteration 74/1000 | Loss: 0.00000924
Iteration 75/1000 | Loss: 0.00000924
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000924
Iteration 81/1000 | Loss: 0.00000924
Iteration 82/1000 | Loss: 0.00000924
Iteration 83/1000 | Loss: 0.00000924
Iteration 84/1000 | Loss: 0.00000924
Iteration 85/1000 | Loss: 0.00000923
Iteration 86/1000 | Loss: 0.00000923
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000920
Iteration 89/1000 | Loss: 0.00000920
Iteration 90/1000 | Loss: 0.00000920
Iteration 91/1000 | Loss: 0.00000920
Iteration 92/1000 | Loss: 0.00000920
Iteration 93/1000 | Loss: 0.00000920
Iteration 94/1000 | Loss: 0.00000920
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000919
Iteration 97/1000 | Loss: 0.00000919
Iteration 98/1000 | Loss: 0.00000918
Iteration 99/1000 | Loss: 0.00000918
Iteration 100/1000 | Loss: 0.00000917
Iteration 101/1000 | Loss: 0.00000917
Iteration 102/1000 | Loss: 0.00000917
Iteration 103/1000 | Loss: 0.00000916
Iteration 104/1000 | Loss: 0.00000916
Iteration 105/1000 | Loss: 0.00000916
Iteration 106/1000 | Loss: 0.00000916
Iteration 107/1000 | Loss: 0.00000916
Iteration 108/1000 | Loss: 0.00000916
Iteration 109/1000 | Loss: 0.00000916
Iteration 110/1000 | Loss: 0.00000916
Iteration 111/1000 | Loss: 0.00000915
Iteration 112/1000 | Loss: 0.00000915
Iteration 113/1000 | Loss: 0.00000915
Iteration 114/1000 | Loss: 0.00000915
Iteration 115/1000 | Loss: 0.00000915
Iteration 116/1000 | Loss: 0.00000915
Iteration 117/1000 | Loss: 0.00000914
Iteration 118/1000 | Loss: 0.00000914
Iteration 119/1000 | Loss: 0.00000914
Iteration 120/1000 | Loss: 0.00000913
Iteration 121/1000 | Loss: 0.00000913
Iteration 122/1000 | Loss: 0.00000913
Iteration 123/1000 | Loss: 0.00000913
Iteration 124/1000 | Loss: 0.00000913
Iteration 125/1000 | Loss: 0.00000913
Iteration 126/1000 | Loss: 0.00000912
Iteration 127/1000 | Loss: 0.00000912
Iteration 128/1000 | Loss: 0.00000912
Iteration 129/1000 | Loss: 0.00000912
Iteration 130/1000 | Loss: 0.00000912
Iteration 131/1000 | Loss: 0.00000912
Iteration 132/1000 | Loss: 0.00000912
Iteration 133/1000 | Loss: 0.00000912
Iteration 134/1000 | Loss: 0.00000912
Iteration 135/1000 | Loss: 0.00000912
Iteration 136/1000 | Loss: 0.00000912
Iteration 137/1000 | Loss: 0.00000911
Iteration 138/1000 | Loss: 0.00000911
Iteration 139/1000 | Loss: 0.00000911
Iteration 140/1000 | Loss: 0.00000911
Iteration 141/1000 | Loss: 0.00000911
Iteration 142/1000 | Loss: 0.00000911
Iteration 143/1000 | Loss: 0.00000911
Iteration 144/1000 | Loss: 0.00000911
Iteration 145/1000 | Loss: 0.00000911
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000909
Iteration 149/1000 | Loss: 0.00000909
Iteration 150/1000 | Loss: 0.00000909
Iteration 151/1000 | Loss: 0.00000909
Iteration 152/1000 | Loss: 0.00000909
Iteration 153/1000 | Loss: 0.00000909
Iteration 154/1000 | Loss: 0.00000909
Iteration 155/1000 | Loss: 0.00000909
Iteration 156/1000 | Loss: 0.00000908
Iteration 157/1000 | Loss: 0.00000908
Iteration 158/1000 | Loss: 0.00000908
Iteration 159/1000 | Loss: 0.00000908
Iteration 160/1000 | Loss: 0.00000908
Iteration 161/1000 | Loss: 0.00000908
Iteration 162/1000 | Loss: 0.00000908
Iteration 163/1000 | Loss: 0.00000908
Iteration 164/1000 | Loss: 0.00000908
Iteration 165/1000 | Loss: 0.00000908
Iteration 166/1000 | Loss: 0.00000908
Iteration 167/1000 | Loss: 0.00000908
Iteration 168/1000 | Loss: 0.00000908
Iteration 169/1000 | Loss: 0.00000907
Iteration 170/1000 | Loss: 0.00000907
Iteration 171/1000 | Loss: 0.00000907
Iteration 172/1000 | Loss: 0.00000907
Iteration 173/1000 | Loss: 0.00000907
Iteration 174/1000 | Loss: 0.00000907
Iteration 175/1000 | Loss: 0.00000907
Iteration 176/1000 | Loss: 0.00000907
Iteration 177/1000 | Loss: 0.00000906
Iteration 178/1000 | Loss: 0.00000906
Iteration 179/1000 | Loss: 0.00000906
Iteration 180/1000 | Loss: 0.00000906
Iteration 181/1000 | Loss: 0.00000905
Iteration 182/1000 | Loss: 0.00000905
Iteration 183/1000 | Loss: 0.00000905
Iteration 184/1000 | Loss: 0.00000905
Iteration 185/1000 | Loss: 0.00000905
Iteration 186/1000 | Loss: 0.00000905
Iteration 187/1000 | Loss: 0.00000905
Iteration 188/1000 | Loss: 0.00000905
Iteration 189/1000 | Loss: 0.00000905
Iteration 190/1000 | Loss: 0.00000905
Iteration 191/1000 | Loss: 0.00000905
Iteration 192/1000 | Loss: 0.00000905
Iteration 193/1000 | Loss: 0.00000904
Iteration 194/1000 | Loss: 0.00000904
Iteration 195/1000 | Loss: 0.00000904
Iteration 196/1000 | Loss: 0.00000904
Iteration 197/1000 | Loss: 0.00000904
Iteration 198/1000 | Loss: 0.00000904
Iteration 199/1000 | Loss: 0.00000904
Iteration 200/1000 | Loss: 0.00000904
Iteration 201/1000 | Loss: 0.00000904
Iteration 202/1000 | Loss: 0.00000904
Iteration 203/1000 | Loss: 0.00000904
Iteration 204/1000 | Loss: 0.00000904
Iteration 205/1000 | Loss: 0.00000904
Iteration 206/1000 | Loss: 0.00000904
Iteration 207/1000 | Loss: 0.00000904
Iteration 208/1000 | Loss: 0.00000904
Iteration 209/1000 | Loss: 0.00000904
Iteration 210/1000 | Loss: 0.00000904
Iteration 211/1000 | Loss: 0.00000904
Iteration 212/1000 | Loss: 0.00000904
Iteration 213/1000 | Loss: 0.00000904
Iteration 214/1000 | Loss: 0.00000904
Iteration 215/1000 | Loss: 0.00000904
Iteration 216/1000 | Loss: 0.00000904
Iteration 217/1000 | Loss: 0.00000904
Iteration 218/1000 | Loss: 0.00000904
Iteration 219/1000 | Loss: 0.00000904
Iteration 220/1000 | Loss: 0.00000904
Iteration 221/1000 | Loss: 0.00000904
Iteration 222/1000 | Loss: 0.00000904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [9.035446964844596e-06, 9.035446964844596e-06, 9.035446964844596e-06, 9.035446964844596e-06, 9.035446964844596e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.035446964844596e-06

Optimization complete. Final v2v error: 2.6281139850616455 mm

Highest mean error: 2.7137064933776855 mm for frame 102

Lowest mean error: 2.5800557136535645 mm for frame 1

Saving results

Total time: 39.91663217544556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542106
Iteration 2/25 | Loss: 0.00138373
Iteration 3/25 | Loss: 0.00117530
Iteration 4/25 | Loss: 0.00111132
Iteration 5/25 | Loss: 0.00110331
Iteration 6/25 | Loss: 0.00110230
Iteration 7/25 | Loss: 0.00110206
Iteration 8/25 | Loss: 0.00110206
Iteration 9/25 | Loss: 0.00110206
Iteration 10/25 | Loss: 0.00110206
Iteration 11/25 | Loss: 0.00110206
Iteration 12/25 | Loss: 0.00110206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011020645033568144, 0.0011020645033568144, 0.0011020645033568144, 0.0011020645033568144, 0.0011020645033568144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011020645033568144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22542095
Iteration 2/25 | Loss: 0.00089193
Iteration 3/25 | Loss: 0.00089191
Iteration 4/25 | Loss: 0.00089190
Iteration 5/25 | Loss: 0.00089190
Iteration 6/25 | Loss: 0.00089190
Iteration 7/25 | Loss: 0.00089190
Iteration 8/25 | Loss: 0.00089190
Iteration 9/25 | Loss: 0.00089190
Iteration 10/25 | Loss: 0.00089190
Iteration 11/25 | Loss: 0.00089190
Iteration 12/25 | Loss: 0.00089190
Iteration 13/25 | Loss: 0.00089190
Iteration 14/25 | Loss: 0.00089190
Iteration 15/25 | Loss: 0.00089190
Iteration 16/25 | Loss: 0.00089190
Iteration 17/25 | Loss: 0.00089190
Iteration 18/25 | Loss: 0.00089190
Iteration 19/25 | Loss: 0.00089190
Iteration 20/25 | Loss: 0.00089190
Iteration 21/25 | Loss: 0.00089190
Iteration 22/25 | Loss: 0.00089190
Iteration 23/25 | Loss: 0.00089190
Iteration 24/25 | Loss: 0.00089190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008919014362618327, 0.0008919014362618327, 0.0008919014362618327, 0.0008919014362618327, 0.0008919014362618327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008919014362618327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089190
Iteration 2/1000 | Loss: 0.00002280
Iteration 3/1000 | Loss: 0.00001488
Iteration 4/1000 | Loss: 0.00001310
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001203
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001156
Iteration 9/1000 | Loss: 0.00001148
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001124
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001104
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001096
Iteration 17/1000 | Loss: 0.00001094
Iteration 18/1000 | Loss: 0.00001091
Iteration 19/1000 | Loss: 0.00001090
Iteration 20/1000 | Loss: 0.00001090
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001085
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001085
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001074
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001072
Iteration 34/1000 | Loss: 0.00001072
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001071
Iteration 37/1000 | Loss: 0.00001071
Iteration 38/1000 | Loss: 0.00001071
Iteration 39/1000 | Loss: 0.00001071
Iteration 40/1000 | Loss: 0.00001071
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001071
Iteration 43/1000 | Loss: 0.00001071
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001071
Iteration 47/1000 | Loss: 0.00001071
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001070
Iteration 50/1000 | Loss: 0.00001070
Iteration 51/1000 | Loss: 0.00001070
Iteration 52/1000 | Loss: 0.00001070
Iteration 53/1000 | Loss: 0.00001070
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001069
Iteration 56/1000 | Loss: 0.00001069
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001068
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001068
Iteration 62/1000 | Loss: 0.00001068
Iteration 63/1000 | Loss: 0.00001067
Iteration 64/1000 | Loss: 0.00001067
Iteration 65/1000 | Loss: 0.00001067
Iteration 66/1000 | Loss: 0.00001067
Iteration 67/1000 | Loss: 0.00001067
Iteration 68/1000 | Loss: 0.00001066
Iteration 69/1000 | Loss: 0.00001066
Iteration 70/1000 | Loss: 0.00001066
Iteration 71/1000 | Loss: 0.00001065
Iteration 72/1000 | Loss: 0.00001065
Iteration 73/1000 | Loss: 0.00001065
Iteration 74/1000 | Loss: 0.00001064
Iteration 75/1000 | Loss: 0.00001064
Iteration 76/1000 | Loss: 0.00001064
Iteration 77/1000 | Loss: 0.00001064
Iteration 78/1000 | Loss: 0.00001064
Iteration 79/1000 | Loss: 0.00001064
Iteration 80/1000 | Loss: 0.00001064
Iteration 81/1000 | Loss: 0.00001063
Iteration 82/1000 | Loss: 0.00001063
Iteration 83/1000 | Loss: 0.00001063
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001063
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001062
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001061
Iteration 96/1000 | Loss: 0.00001061
Iteration 97/1000 | Loss: 0.00001061
Iteration 98/1000 | Loss: 0.00001061
Iteration 99/1000 | Loss: 0.00001060
Iteration 100/1000 | Loss: 0.00001060
Iteration 101/1000 | Loss: 0.00001060
Iteration 102/1000 | Loss: 0.00001059
Iteration 103/1000 | Loss: 0.00001059
Iteration 104/1000 | Loss: 0.00001059
Iteration 105/1000 | Loss: 0.00001058
Iteration 106/1000 | Loss: 0.00001058
Iteration 107/1000 | Loss: 0.00001058
Iteration 108/1000 | Loss: 0.00001058
Iteration 109/1000 | Loss: 0.00001058
Iteration 110/1000 | Loss: 0.00001058
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001057
Iteration 114/1000 | Loss: 0.00001056
Iteration 115/1000 | Loss: 0.00001056
Iteration 116/1000 | Loss: 0.00001056
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001056
Iteration 119/1000 | Loss: 0.00001056
Iteration 120/1000 | Loss: 0.00001056
Iteration 121/1000 | Loss: 0.00001056
Iteration 122/1000 | Loss: 0.00001056
Iteration 123/1000 | Loss: 0.00001056
Iteration 124/1000 | Loss: 0.00001056
Iteration 125/1000 | Loss: 0.00001056
Iteration 126/1000 | Loss: 0.00001055
Iteration 127/1000 | Loss: 0.00001055
Iteration 128/1000 | Loss: 0.00001055
Iteration 129/1000 | Loss: 0.00001055
Iteration 130/1000 | Loss: 0.00001055
Iteration 131/1000 | Loss: 0.00001055
Iteration 132/1000 | Loss: 0.00001055
Iteration 133/1000 | Loss: 0.00001055
Iteration 134/1000 | Loss: 0.00001055
Iteration 135/1000 | Loss: 0.00001055
Iteration 136/1000 | Loss: 0.00001055
Iteration 137/1000 | Loss: 0.00001055
Iteration 138/1000 | Loss: 0.00001054
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001054
Iteration 141/1000 | Loss: 0.00001054
Iteration 142/1000 | Loss: 0.00001054
Iteration 143/1000 | Loss: 0.00001054
Iteration 144/1000 | Loss: 0.00001054
Iteration 145/1000 | Loss: 0.00001054
Iteration 146/1000 | Loss: 0.00001053
Iteration 147/1000 | Loss: 0.00001053
Iteration 148/1000 | Loss: 0.00001053
Iteration 149/1000 | Loss: 0.00001053
Iteration 150/1000 | Loss: 0.00001052
Iteration 151/1000 | Loss: 0.00001052
Iteration 152/1000 | Loss: 0.00001052
Iteration 153/1000 | Loss: 0.00001052
Iteration 154/1000 | Loss: 0.00001052
Iteration 155/1000 | Loss: 0.00001052
Iteration 156/1000 | Loss: 0.00001052
Iteration 157/1000 | Loss: 0.00001052
Iteration 158/1000 | Loss: 0.00001052
Iteration 159/1000 | Loss: 0.00001052
Iteration 160/1000 | Loss: 0.00001052
Iteration 161/1000 | Loss: 0.00001052
Iteration 162/1000 | Loss: 0.00001052
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001051
Iteration 165/1000 | Loss: 0.00001051
Iteration 166/1000 | Loss: 0.00001051
Iteration 167/1000 | Loss: 0.00001051
Iteration 168/1000 | Loss: 0.00001051
Iteration 169/1000 | Loss: 0.00001051
Iteration 170/1000 | Loss: 0.00001051
Iteration 171/1000 | Loss: 0.00001050
Iteration 172/1000 | Loss: 0.00001050
Iteration 173/1000 | Loss: 0.00001050
Iteration 174/1000 | Loss: 0.00001050
Iteration 175/1000 | Loss: 0.00001050
Iteration 176/1000 | Loss: 0.00001050
Iteration 177/1000 | Loss: 0.00001050
Iteration 178/1000 | Loss: 0.00001050
Iteration 179/1000 | Loss: 0.00001049
Iteration 180/1000 | Loss: 0.00001049
Iteration 181/1000 | Loss: 0.00001049
Iteration 182/1000 | Loss: 0.00001049
Iteration 183/1000 | Loss: 0.00001049
Iteration 184/1000 | Loss: 0.00001049
Iteration 185/1000 | Loss: 0.00001049
Iteration 186/1000 | Loss: 0.00001049
Iteration 187/1000 | Loss: 0.00001049
Iteration 188/1000 | Loss: 0.00001049
Iteration 189/1000 | Loss: 0.00001049
Iteration 190/1000 | Loss: 0.00001049
Iteration 191/1000 | Loss: 0.00001049
Iteration 192/1000 | Loss: 0.00001049
Iteration 193/1000 | Loss: 0.00001049
Iteration 194/1000 | Loss: 0.00001048
Iteration 195/1000 | Loss: 0.00001048
Iteration 196/1000 | Loss: 0.00001048
Iteration 197/1000 | Loss: 0.00001048
Iteration 198/1000 | Loss: 0.00001048
Iteration 199/1000 | Loss: 0.00001048
Iteration 200/1000 | Loss: 0.00001048
Iteration 201/1000 | Loss: 0.00001048
Iteration 202/1000 | Loss: 0.00001048
Iteration 203/1000 | Loss: 0.00001048
Iteration 204/1000 | Loss: 0.00001048
Iteration 205/1000 | Loss: 0.00001048
Iteration 206/1000 | Loss: 0.00001048
Iteration 207/1000 | Loss: 0.00001048
Iteration 208/1000 | Loss: 0.00001048
Iteration 209/1000 | Loss: 0.00001047
Iteration 210/1000 | Loss: 0.00001047
Iteration 211/1000 | Loss: 0.00001047
Iteration 212/1000 | Loss: 0.00001047
Iteration 213/1000 | Loss: 0.00001047
Iteration 214/1000 | Loss: 0.00001047
Iteration 215/1000 | Loss: 0.00001047
Iteration 216/1000 | Loss: 0.00001047
Iteration 217/1000 | Loss: 0.00001047
Iteration 218/1000 | Loss: 0.00001047
Iteration 219/1000 | Loss: 0.00001047
Iteration 220/1000 | Loss: 0.00001047
Iteration 221/1000 | Loss: 0.00001047
Iteration 222/1000 | Loss: 0.00001047
Iteration 223/1000 | Loss: 0.00001047
Iteration 224/1000 | Loss: 0.00001047
Iteration 225/1000 | Loss: 0.00001047
Iteration 226/1000 | Loss: 0.00001047
Iteration 227/1000 | Loss: 0.00001047
Iteration 228/1000 | Loss: 0.00001047
Iteration 229/1000 | Loss: 0.00001047
Iteration 230/1000 | Loss: 0.00001047
Iteration 231/1000 | Loss: 0.00001047
Iteration 232/1000 | Loss: 0.00001047
Iteration 233/1000 | Loss: 0.00001047
Iteration 234/1000 | Loss: 0.00001047
Iteration 235/1000 | Loss: 0.00001047
Iteration 236/1000 | Loss: 0.00001047
Iteration 237/1000 | Loss: 0.00001047
Iteration 238/1000 | Loss: 0.00001047
Iteration 239/1000 | Loss: 0.00001047
Iteration 240/1000 | Loss: 0.00001047
Iteration 241/1000 | Loss: 0.00001047
Iteration 242/1000 | Loss: 0.00001047
Iteration 243/1000 | Loss: 0.00001047
Iteration 244/1000 | Loss: 0.00001047
Iteration 245/1000 | Loss: 0.00001047
Iteration 246/1000 | Loss: 0.00001047
Iteration 247/1000 | Loss: 0.00001047
Iteration 248/1000 | Loss: 0.00001047
Iteration 249/1000 | Loss: 0.00001047
Iteration 250/1000 | Loss: 0.00001047
Iteration 251/1000 | Loss: 0.00001047
Iteration 252/1000 | Loss: 0.00001047
Iteration 253/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.0467817446624395e-05, 1.0467817446624395e-05, 1.0467817446624395e-05, 1.0467817446624395e-05, 1.0467817446624395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0467817446624395e-05

Optimization complete. Final v2v error: 2.712855577468872 mm

Highest mean error: 3.382750988006592 mm for frame 166

Lowest mean error: 2.394150972366333 mm for frame 90

Saving results

Total time: 48.548189878463745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433571
Iteration 2/25 | Loss: 0.00114149
Iteration 3/25 | Loss: 0.00106698
Iteration 4/25 | Loss: 0.00105491
Iteration 5/25 | Loss: 0.00105114
Iteration 6/25 | Loss: 0.00105099
Iteration 7/25 | Loss: 0.00105099
Iteration 8/25 | Loss: 0.00105099
Iteration 9/25 | Loss: 0.00105099
Iteration 10/25 | Loss: 0.00105099
Iteration 11/25 | Loss: 0.00105099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010509865824133158, 0.0010509865824133158, 0.0010509865824133158, 0.0010509865824133158, 0.0010509865824133158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010509865824133158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.61091614
Iteration 2/25 | Loss: 0.00079699
Iteration 3/25 | Loss: 0.00079699
Iteration 4/25 | Loss: 0.00079699
Iteration 5/25 | Loss: 0.00079699
Iteration 6/25 | Loss: 0.00079698
Iteration 7/25 | Loss: 0.00079698
Iteration 8/25 | Loss: 0.00079698
Iteration 9/25 | Loss: 0.00079698
Iteration 10/25 | Loss: 0.00079698
Iteration 11/25 | Loss: 0.00079698
Iteration 12/25 | Loss: 0.00079698
Iteration 13/25 | Loss: 0.00079698
Iteration 14/25 | Loss: 0.00079698
Iteration 15/25 | Loss: 0.00079698
Iteration 16/25 | Loss: 0.00079698
Iteration 17/25 | Loss: 0.00079698
Iteration 18/25 | Loss: 0.00079698
Iteration 19/25 | Loss: 0.00079698
Iteration 20/25 | Loss: 0.00079698
Iteration 21/25 | Loss: 0.00079698
Iteration 22/25 | Loss: 0.00079698
Iteration 23/25 | Loss: 0.00079698
Iteration 24/25 | Loss: 0.00079698
Iteration 25/25 | Loss: 0.00079698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079698
Iteration 2/1000 | Loss: 0.00001669
Iteration 3/1000 | Loss: 0.00001289
Iteration 4/1000 | Loss: 0.00001193
Iteration 5/1000 | Loss: 0.00001139
Iteration 6/1000 | Loss: 0.00001097
Iteration 7/1000 | Loss: 0.00001073
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001035
Iteration 10/1000 | Loss: 0.00001028
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001023
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001017
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001010
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000995
Iteration 19/1000 | Loss: 0.00000993
Iteration 20/1000 | Loss: 0.00000988
Iteration 21/1000 | Loss: 0.00000982
Iteration 22/1000 | Loss: 0.00000980
Iteration 23/1000 | Loss: 0.00000980
Iteration 24/1000 | Loss: 0.00000980
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000979
Iteration 28/1000 | Loss: 0.00000979
Iteration 29/1000 | Loss: 0.00000978
Iteration 30/1000 | Loss: 0.00000978
Iteration 31/1000 | Loss: 0.00000977
Iteration 32/1000 | Loss: 0.00000973
Iteration 33/1000 | Loss: 0.00000973
Iteration 34/1000 | Loss: 0.00000972
Iteration 35/1000 | Loss: 0.00000972
Iteration 36/1000 | Loss: 0.00000972
Iteration 37/1000 | Loss: 0.00000971
Iteration 38/1000 | Loss: 0.00000964
Iteration 39/1000 | Loss: 0.00000959
Iteration 40/1000 | Loss: 0.00000958
Iteration 41/1000 | Loss: 0.00000958
Iteration 42/1000 | Loss: 0.00000955
Iteration 43/1000 | Loss: 0.00000955
Iteration 44/1000 | Loss: 0.00000955
Iteration 45/1000 | Loss: 0.00000954
Iteration 46/1000 | Loss: 0.00000954
Iteration 47/1000 | Loss: 0.00000953
Iteration 48/1000 | Loss: 0.00000953
Iteration 49/1000 | Loss: 0.00000952
Iteration 50/1000 | Loss: 0.00000952
Iteration 51/1000 | Loss: 0.00000951
Iteration 52/1000 | Loss: 0.00000951
Iteration 53/1000 | Loss: 0.00000950
Iteration 54/1000 | Loss: 0.00000950
Iteration 55/1000 | Loss: 0.00000950
Iteration 56/1000 | Loss: 0.00000950
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000949
Iteration 59/1000 | Loss: 0.00000948
Iteration 60/1000 | Loss: 0.00000947
Iteration 61/1000 | Loss: 0.00000947
Iteration 62/1000 | Loss: 0.00000946
Iteration 63/1000 | Loss: 0.00000945
Iteration 64/1000 | Loss: 0.00000945
Iteration 65/1000 | Loss: 0.00000945
Iteration 66/1000 | Loss: 0.00000945
Iteration 67/1000 | Loss: 0.00000945
Iteration 68/1000 | Loss: 0.00000944
Iteration 69/1000 | Loss: 0.00000944
Iteration 70/1000 | Loss: 0.00000943
Iteration 71/1000 | Loss: 0.00000943
Iteration 72/1000 | Loss: 0.00000943
Iteration 73/1000 | Loss: 0.00000942
Iteration 74/1000 | Loss: 0.00000942
Iteration 75/1000 | Loss: 0.00000942
Iteration 76/1000 | Loss: 0.00000942
Iteration 77/1000 | Loss: 0.00000942
Iteration 78/1000 | Loss: 0.00000942
Iteration 79/1000 | Loss: 0.00000939
Iteration 80/1000 | Loss: 0.00000939
Iteration 81/1000 | Loss: 0.00000938
Iteration 82/1000 | Loss: 0.00000938
Iteration 83/1000 | Loss: 0.00000938
Iteration 84/1000 | Loss: 0.00000937
Iteration 85/1000 | Loss: 0.00000937
Iteration 86/1000 | Loss: 0.00000937
Iteration 87/1000 | Loss: 0.00000935
Iteration 88/1000 | Loss: 0.00000934
Iteration 89/1000 | Loss: 0.00000934
Iteration 90/1000 | Loss: 0.00000934
Iteration 91/1000 | Loss: 0.00000934
Iteration 92/1000 | Loss: 0.00000933
Iteration 93/1000 | Loss: 0.00000933
Iteration 94/1000 | Loss: 0.00000932
Iteration 95/1000 | Loss: 0.00000931
Iteration 96/1000 | Loss: 0.00000931
Iteration 97/1000 | Loss: 0.00000929
Iteration 98/1000 | Loss: 0.00000929
Iteration 99/1000 | Loss: 0.00000929
Iteration 100/1000 | Loss: 0.00000929
Iteration 101/1000 | Loss: 0.00000929
Iteration 102/1000 | Loss: 0.00000929
Iteration 103/1000 | Loss: 0.00000929
Iteration 104/1000 | Loss: 0.00000929
Iteration 105/1000 | Loss: 0.00000929
Iteration 106/1000 | Loss: 0.00000929
Iteration 107/1000 | Loss: 0.00000928
Iteration 108/1000 | Loss: 0.00000928
Iteration 109/1000 | Loss: 0.00000927
Iteration 110/1000 | Loss: 0.00000926
Iteration 111/1000 | Loss: 0.00000926
Iteration 112/1000 | Loss: 0.00000926
Iteration 113/1000 | Loss: 0.00000925
Iteration 114/1000 | Loss: 0.00000925
Iteration 115/1000 | Loss: 0.00000925
Iteration 116/1000 | Loss: 0.00000925
Iteration 117/1000 | Loss: 0.00000925
Iteration 118/1000 | Loss: 0.00000925
Iteration 119/1000 | Loss: 0.00000925
Iteration 120/1000 | Loss: 0.00000925
Iteration 121/1000 | Loss: 0.00000925
Iteration 122/1000 | Loss: 0.00000925
Iteration 123/1000 | Loss: 0.00000925
Iteration 124/1000 | Loss: 0.00000925
Iteration 125/1000 | Loss: 0.00000924
Iteration 126/1000 | Loss: 0.00000924
Iteration 127/1000 | Loss: 0.00000924
Iteration 128/1000 | Loss: 0.00000924
Iteration 129/1000 | Loss: 0.00000923
Iteration 130/1000 | Loss: 0.00000923
Iteration 131/1000 | Loss: 0.00000923
Iteration 132/1000 | Loss: 0.00000922
Iteration 133/1000 | Loss: 0.00000922
Iteration 134/1000 | Loss: 0.00000922
Iteration 135/1000 | Loss: 0.00000922
Iteration 136/1000 | Loss: 0.00000922
Iteration 137/1000 | Loss: 0.00000922
Iteration 138/1000 | Loss: 0.00000922
Iteration 139/1000 | Loss: 0.00000922
Iteration 140/1000 | Loss: 0.00000921
Iteration 141/1000 | Loss: 0.00000921
Iteration 142/1000 | Loss: 0.00000921
Iteration 143/1000 | Loss: 0.00000921
Iteration 144/1000 | Loss: 0.00000921
Iteration 145/1000 | Loss: 0.00000921
Iteration 146/1000 | Loss: 0.00000921
Iteration 147/1000 | Loss: 0.00000921
Iteration 148/1000 | Loss: 0.00000920
Iteration 149/1000 | Loss: 0.00000920
Iteration 150/1000 | Loss: 0.00000920
Iteration 151/1000 | Loss: 0.00000920
Iteration 152/1000 | Loss: 0.00000920
Iteration 153/1000 | Loss: 0.00000920
Iteration 154/1000 | Loss: 0.00000920
Iteration 155/1000 | Loss: 0.00000920
Iteration 156/1000 | Loss: 0.00000920
Iteration 157/1000 | Loss: 0.00000920
Iteration 158/1000 | Loss: 0.00000920
Iteration 159/1000 | Loss: 0.00000920
Iteration 160/1000 | Loss: 0.00000920
Iteration 161/1000 | Loss: 0.00000920
Iteration 162/1000 | Loss: 0.00000919
Iteration 163/1000 | Loss: 0.00000919
Iteration 164/1000 | Loss: 0.00000919
Iteration 165/1000 | Loss: 0.00000919
Iteration 166/1000 | Loss: 0.00000919
Iteration 167/1000 | Loss: 0.00000919
Iteration 168/1000 | Loss: 0.00000919
Iteration 169/1000 | Loss: 0.00000919
Iteration 170/1000 | Loss: 0.00000919
Iteration 171/1000 | Loss: 0.00000918
Iteration 172/1000 | Loss: 0.00000918
Iteration 173/1000 | Loss: 0.00000918
Iteration 174/1000 | Loss: 0.00000918
Iteration 175/1000 | Loss: 0.00000918
Iteration 176/1000 | Loss: 0.00000918
Iteration 177/1000 | Loss: 0.00000918
Iteration 178/1000 | Loss: 0.00000918
Iteration 179/1000 | Loss: 0.00000918
Iteration 180/1000 | Loss: 0.00000918
Iteration 181/1000 | Loss: 0.00000918
Iteration 182/1000 | Loss: 0.00000918
Iteration 183/1000 | Loss: 0.00000918
Iteration 184/1000 | Loss: 0.00000918
Iteration 185/1000 | Loss: 0.00000918
Iteration 186/1000 | Loss: 0.00000918
Iteration 187/1000 | Loss: 0.00000917
Iteration 188/1000 | Loss: 0.00000917
Iteration 189/1000 | Loss: 0.00000917
Iteration 190/1000 | Loss: 0.00000917
Iteration 191/1000 | Loss: 0.00000917
Iteration 192/1000 | Loss: 0.00000917
Iteration 193/1000 | Loss: 0.00000917
Iteration 194/1000 | Loss: 0.00000917
Iteration 195/1000 | Loss: 0.00000917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [9.171451893053018e-06, 9.171451893053018e-06, 9.171451893053018e-06, 9.171451893053018e-06, 9.171451893053018e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.171451893053018e-06

Optimization complete. Final v2v error: 2.6348538398742676 mm

Highest mean error: 2.9578287601470947 mm for frame 46

Lowest mean error: 2.411748170852661 mm for frame 25

Saving results

Total time: 46.314146995544434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483606
Iteration 2/25 | Loss: 0.00129661
Iteration 3/25 | Loss: 0.00116700
Iteration 4/25 | Loss: 0.00115775
Iteration 5/25 | Loss: 0.00115570
Iteration 6/25 | Loss: 0.00115563
Iteration 7/25 | Loss: 0.00115563
Iteration 8/25 | Loss: 0.00115563
Iteration 9/25 | Loss: 0.00115563
Iteration 10/25 | Loss: 0.00115563
Iteration 11/25 | Loss: 0.00115563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011556309182196856, 0.0011556309182196856, 0.0011556309182196856, 0.0011556309182196856, 0.0011556309182196856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011556309182196856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.73617077
Iteration 2/25 | Loss: 0.00063010
Iteration 3/25 | Loss: 0.00063010
Iteration 4/25 | Loss: 0.00063010
Iteration 5/25 | Loss: 0.00063010
Iteration 6/25 | Loss: 0.00063010
Iteration 7/25 | Loss: 0.00063010
Iteration 8/25 | Loss: 0.00063009
Iteration 9/25 | Loss: 0.00063009
Iteration 10/25 | Loss: 0.00063009
Iteration 11/25 | Loss: 0.00063009
Iteration 12/25 | Loss: 0.00063009
Iteration 13/25 | Loss: 0.00063009
Iteration 14/25 | Loss: 0.00063009
Iteration 15/25 | Loss: 0.00063009
Iteration 16/25 | Loss: 0.00063009
Iteration 17/25 | Loss: 0.00063009
Iteration 18/25 | Loss: 0.00063009
Iteration 19/25 | Loss: 0.00063009
Iteration 20/25 | Loss: 0.00063009
Iteration 21/25 | Loss: 0.00063009
Iteration 22/25 | Loss: 0.00063009
Iteration 23/25 | Loss: 0.00063009
Iteration 24/25 | Loss: 0.00063009
Iteration 25/25 | Loss: 0.00063009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063009
Iteration 2/1000 | Loss: 0.00003291
Iteration 3/1000 | Loss: 0.00002108
Iteration 4/1000 | Loss: 0.00001885
Iteration 5/1000 | Loss: 0.00001808
Iteration 6/1000 | Loss: 0.00001765
Iteration 7/1000 | Loss: 0.00001723
Iteration 8/1000 | Loss: 0.00001684
Iteration 9/1000 | Loss: 0.00001652
Iteration 10/1000 | Loss: 0.00001630
Iteration 11/1000 | Loss: 0.00001620
Iteration 12/1000 | Loss: 0.00001608
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00001596
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001595
Iteration 17/1000 | Loss: 0.00001590
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001585
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001583
Iteration 26/1000 | Loss: 0.00001583
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001576
Iteration 35/1000 | Loss: 0.00001576
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001566
Iteration 44/1000 | Loss: 0.00001566
Iteration 45/1000 | Loss: 0.00001566
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001552
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001551
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001550
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001550
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001548
Iteration 97/1000 | Loss: 0.00001548
Iteration 98/1000 | Loss: 0.00001548
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001547
Iteration 105/1000 | Loss: 0.00001547
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001547
Iteration 110/1000 | Loss: 0.00001547
Iteration 111/1000 | Loss: 0.00001547
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001547
Iteration 116/1000 | Loss: 0.00001547
Iteration 117/1000 | Loss: 0.00001547
Iteration 118/1000 | Loss: 0.00001547
Iteration 119/1000 | Loss: 0.00001547
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001547
Iteration 141/1000 | Loss: 0.00001547
Iteration 142/1000 | Loss: 0.00001547
Iteration 143/1000 | Loss: 0.00001547
Iteration 144/1000 | Loss: 0.00001547
Iteration 145/1000 | Loss: 0.00001547
Iteration 146/1000 | Loss: 0.00001547
Iteration 147/1000 | Loss: 0.00001547
Iteration 148/1000 | Loss: 0.00001547
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001547
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001547
Iteration 155/1000 | Loss: 0.00001547
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001547
Iteration 159/1000 | Loss: 0.00001547
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.546566272736527e-05, 1.546566272736527e-05, 1.546566272736527e-05, 1.546566272736527e-05, 1.546566272736527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.546566272736527e-05

Optimization complete. Final v2v error: 3.2875077724456787 mm

Highest mean error: 3.8344244956970215 mm for frame 39

Lowest mean error: 2.8700525760650635 mm for frame 0

Saving results

Total time: 37.196773290634155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00680727
Iteration 2/25 | Loss: 0.00127262
Iteration 3/25 | Loss: 0.00115219
Iteration 4/25 | Loss: 0.00108590
Iteration 5/25 | Loss: 0.00108886
Iteration 6/25 | Loss: 0.00107989
Iteration 7/25 | Loss: 0.00108029
Iteration 8/25 | Loss: 0.00108225
Iteration 9/25 | Loss: 0.00106951
Iteration 10/25 | Loss: 0.00106922
Iteration 11/25 | Loss: 0.00106918
Iteration 12/25 | Loss: 0.00106918
Iteration 13/25 | Loss: 0.00106917
Iteration 14/25 | Loss: 0.00106917
Iteration 15/25 | Loss: 0.00106917
Iteration 16/25 | Loss: 0.00106917
Iteration 17/25 | Loss: 0.00106917
Iteration 18/25 | Loss: 0.00106917
Iteration 19/25 | Loss: 0.00106917
Iteration 20/25 | Loss: 0.00106917
Iteration 21/25 | Loss: 0.00106917
Iteration 22/25 | Loss: 0.00106917
Iteration 23/25 | Loss: 0.00106917
Iteration 24/25 | Loss: 0.00106917
Iteration 25/25 | Loss: 0.00106916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61960506
Iteration 2/25 | Loss: 0.00087396
Iteration 3/25 | Loss: 0.00087396
Iteration 4/25 | Loss: 0.00087396
Iteration 5/25 | Loss: 0.00087396
Iteration 6/25 | Loss: 0.00087396
Iteration 7/25 | Loss: 0.00087396
Iteration 8/25 | Loss: 0.00087396
Iteration 9/25 | Loss: 0.00087396
Iteration 10/25 | Loss: 0.00087396
Iteration 11/25 | Loss: 0.00087396
Iteration 12/25 | Loss: 0.00087396
Iteration 13/25 | Loss: 0.00087396
Iteration 14/25 | Loss: 0.00087396
Iteration 15/25 | Loss: 0.00087396
Iteration 16/25 | Loss: 0.00087396
Iteration 17/25 | Loss: 0.00087396
Iteration 18/25 | Loss: 0.00087396
Iteration 19/25 | Loss: 0.00087396
Iteration 20/25 | Loss: 0.00087396
Iteration 21/25 | Loss: 0.00087396
Iteration 22/25 | Loss: 0.00087395
Iteration 23/25 | Loss: 0.00087395
Iteration 24/25 | Loss: 0.00087395
Iteration 25/25 | Loss: 0.00087395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087395
Iteration 2/1000 | Loss: 0.00001693
Iteration 3/1000 | Loss: 0.00001278
Iteration 4/1000 | Loss: 0.00001172
Iteration 5/1000 | Loss: 0.00001118
Iteration 6/1000 | Loss: 0.00001080
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001029
Iteration 10/1000 | Loss: 0.00001013
Iteration 11/1000 | Loss: 0.00001008
Iteration 12/1000 | Loss: 0.00001007
Iteration 13/1000 | Loss: 0.00001002
Iteration 14/1000 | Loss: 0.00000997
Iteration 15/1000 | Loss: 0.00000993
Iteration 16/1000 | Loss: 0.00000993
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000992
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000990
Iteration 24/1000 | Loss: 0.00000989
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000988
Iteration 27/1000 | Loss: 0.00000988
Iteration 28/1000 | Loss: 0.00000987
Iteration 29/1000 | Loss: 0.00000985
Iteration 30/1000 | Loss: 0.00000985
Iteration 31/1000 | Loss: 0.00000984
Iteration 32/1000 | Loss: 0.00000984
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000980
Iteration 37/1000 | Loss: 0.00000979
Iteration 38/1000 | Loss: 0.00000978
Iteration 39/1000 | Loss: 0.00000978
Iteration 40/1000 | Loss: 0.00000978
Iteration 41/1000 | Loss: 0.00000978
Iteration 42/1000 | Loss: 0.00000978
Iteration 43/1000 | Loss: 0.00000976
Iteration 44/1000 | Loss: 0.00000976
Iteration 45/1000 | Loss: 0.00000975
Iteration 46/1000 | Loss: 0.00000975
Iteration 47/1000 | Loss: 0.00000975
Iteration 48/1000 | Loss: 0.00000974
Iteration 49/1000 | Loss: 0.00000973
Iteration 50/1000 | Loss: 0.00000971
Iteration 51/1000 | Loss: 0.00000970
Iteration 52/1000 | Loss: 0.00000969
Iteration 53/1000 | Loss: 0.00000969
Iteration 54/1000 | Loss: 0.00000969
Iteration 55/1000 | Loss: 0.00000968
Iteration 56/1000 | Loss: 0.00000968
Iteration 57/1000 | Loss: 0.00000967
Iteration 58/1000 | Loss: 0.00000966
Iteration 59/1000 | Loss: 0.00000966
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000965
Iteration 63/1000 | Loss: 0.00000965
Iteration 64/1000 | Loss: 0.00000965
Iteration 65/1000 | Loss: 0.00000965
Iteration 66/1000 | Loss: 0.00000965
Iteration 67/1000 | Loss: 0.00000965
Iteration 68/1000 | Loss: 0.00000965
Iteration 69/1000 | Loss: 0.00000964
Iteration 70/1000 | Loss: 0.00000964
Iteration 71/1000 | Loss: 0.00000964
Iteration 72/1000 | Loss: 0.00000964
Iteration 73/1000 | Loss: 0.00000963
Iteration 74/1000 | Loss: 0.00000963
Iteration 75/1000 | Loss: 0.00000962
Iteration 76/1000 | Loss: 0.00000962
Iteration 77/1000 | Loss: 0.00000962
Iteration 78/1000 | Loss: 0.00000962
Iteration 79/1000 | Loss: 0.00000962
Iteration 80/1000 | Loss: 0.00000962
Iteration 81/1000 | Loss: 0.00000961
Iteration 82/1000 | Loss: 0.00000961
Iteration 83/1000 | Loss: 0.00000961
Iteration 84/1000 | Loss: 0.00000961
Iteration 85/1000 | Loss: 0.00000960
Iteration 86/1000 | Loss: 0.00000960
Iteration 87/1000 | Loss: 0.00000960
Iteration 88/1000 | Loss: 0.00000960
Iteration 89/1000 | Loss: 0.00000960
Iteration 90/1000 | Loss: 0.00000960
Iteration 91/1000 | Loss: 0.00000960
Iteration 92/1000 | Loss: 0.00000960
Iteration 93/1000 | Loss: 0.00000960
Iteration 94/1000 | Loss: 0.00000960
Iteration 95/1000 | Loss: 0.00000960
Iteration 96/1000 | Loss: 0.00000960
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000960
Iteration 99/1000 | Loss: 0.00000960
Iteration 100/1000 | Loss: 0.00000959
Iteration 101/1000 | Loss: 0.00000959
Iteration 102/1000 | Loss: 0.00000959
Iteration 103/1000 | Loss: 0.00000959
Iteration 104/1000 | Loss: 0.00000959
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000959
Iteration 118/1000 | Loss: 0.00000959
Iteration 119/1000 | Loss: 0.00000959
Iteration 120/1000 | Loss: 0.00000959
Iteration 121/1000 | Loss: 0.00000959
Iteration 122/1000 | Loss: 0.00000959
Iteration 123/1000 | Loss: 0.00000959
Iteration 124/1000 | Loss: 0.00000959
Iteration 125/1000 | Loss: 0.00000959
Iteration 126/1000 | Loss: 0.00000959
Iteration 127/1000 | Loss: 0.00000959
Iteration 128/1000 | Loss: 0.00000959
Iteration 129/1000 | Loss: 0.00000959
Iteration 130/1000 | Loss: 0.00000959
Iteration 131/1000 | Loss: 0.00000959
Iteration 132/1000 | Loss: 0.00000959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [9.591978596290573e-06, 9.591978596290573e-06, 9.591978596290573e-06, 9.591978596290573e-06, 9.591978596290573e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.591978596290573e-06

Optimization complete. Final v2v error: 2.655954122543335 mm

Highest mean error: 2.997022867202759 mm for frame 159

Lowest mean error: 2.4144980907440186 mm for frame 40

Saving results

Total time: 48.21664524078369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489969
Iteration 2/25 | Loss: 0.00119921
Iteration 3/25 | Loss: 0.00110937
Iteration 4/25 | Loss: 0.00109891
Iteration 5/25 | Loss: 0.00109719
Iteration 6/25 | Loss: 0.00109719
Iteration 7/25 | Loss: 0.00109719
Iteration 8/25 | Loss: 0.00109719
Iteration 9/25 | Loss: 0.00109719
Iteration 10/25 | Loss: 0.00109719
Iteration 11/25 | Loss: 0.00109719
Iteration 12/25 | Loss: 0.00109719
Iteration 13/25 | Loss: 0.00109719
Iteration 14/25 | Loss: 0.00109719
Iteration 15/25 | Loss: 0.00109719
Iteration 16/25 | Loss: 0.00109719
Iteration 17/25 | Loss: 0.00109719
Iteration 18/25 | Loss: 0.00109719
Iteration 19/25 | Loss: 0.00109719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010971883311867714, 0.0010971883311867714, 0.0010971883311867714, 0.0010971883311867714, 0.0010971883311867714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010971883311867714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61079335
Iteration 2/25 | Loss: 0.00084868
Iteration 3/25 | Loss: 0.00084867
Iteration 4/25 | Loss: 0.00084867
Iteration 5/25 | Loss: 0.00084867
Iteration 6/25 | Loss: 0.00084867
Iteration 7/25 | Loss: 0.00084867
Iteration 8/25 | Loss: 0.00084867
Iteration 9/25 | Loss: 0.00084867
Iteration 10/25 | Loss: 0.00084867
Iteration 11/25 | Loss: 0.00084867
Iteration 12/25 | Loss: 0.00084867
Iteration 13/25 | Loss: 0.00084867
Iteration 14/25 | Loss: 0.00084867
Iteration 15/25 | Loss: 0.00084867
Iteration 16/25 | Loss: 0.00084867
Iteration 17/25 | Loss: 0.00084867
Iteration 18/25 | Loss: 0.00084867
Iteration 19/25 | Loss: 0.00084867
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008486693841405213, 0.0008486693841405213, 0.0008486693841405213, 0.0008486693841405213, 0.0008486693841405213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008486693841405213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084867
Iteration 2/1000 | Loss: 0.00002342
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001501
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001374
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001323
Iteration 11/1000 | Loss: 0.00001303
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001293
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001273
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001268
Iteration 31/1000 | Loss: 0.00001268
Iteration 32/1000 | Loss: 0.00001267
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001264
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001264
Iteration 39/1000 | Loss: 0.00001264
Iteration 40/1000 | Loss: 0.00001264
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001263
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001259
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001257
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001257
Iteration 66/1000 | Loss: 0.00001257
Iteration 67/1000 | Loss: 0.00001257
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001256
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001256
Iteration 74/1000 | Loss: 0.00001256
Iteration 75/1000 | Loss: 0.00001256
Iteration 76/1000 | Loss: 0.00001256
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001253
Iteration 88/1000 | Loss: 0.00001253
Iteration 89/1000 | Loss: 0.00001253
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001250
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001248
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001247
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001245
Iteration 125/1000 | Loss: 0.00001245
Iteration 126/1000 | Loss: 0.00001245
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001244
Iteration 132/1000 | Loss: 0.00001244
Iteration 133/1000 | Loss: 0.00001244
Iteration 134/1000 | Loss: 0.00001244
Iteration 135/1000 | Loss: 0.00001244
Iteration 136/1000 | Loss: 0.00001244
Iteration 137/1000 | Loss: 0.00001244
Iteration 138/1000 | Loss: 0.00001244
Iteration 139/1000 | Loss: 0.00001243
Iteration 140/1000 | Loss: 0.00001243
Iteration 141/1000 | Loss: 0.00001243
Iteration 142/1000 | Loss: 0.00001243
Iteration 143/1000 | Loss: 0.00001243
Iteration 144/1000 | Loss: 0.00001243
Iteration 145/1000 | Loss: 0.00001243
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001243
Iteration 148/1000 | Loss: 0.00001243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2431144568836316e-05, 1.2431144568836316e-05, 1.2431144568836316e-05, 1.2431144568836316e-05, 1.2431144568836316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2431144568836316e-05

Optimization complete. Final v2v error: 2.970654010772705 mm

Highest mean error: 3.2067043781280518 mm for frame 266

Lowest mean error: 2.7514607906341553 mm for frame 16

Saving results

Total time: 41.48191857337952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_038/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_038/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797095
Iteration 2/25 | Loss: 0.00127799
Iteration 3/25 | Loss: 0.00110885
Iteration 4/25 | Loss: 0.00108819
Iteration 5/25 | Loss: 0.00108398
Iteration 6/25 | Loss: 0.00108305
Iteration 7/25 | Loss: 0.00108305
Iteration 8/25 | Loss: 0.00108305
Iteration 9/25 | Loss: 0.00108305
Iteration 10/25 | Loss: 0.00108305
Iteration 11/25 | Loss: 0.00108305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001083045732229948, 0.001083045732229948, 0.001083045732229948, 0.001083045732229948, 0.001083045732229948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001083045732229948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27685976
Iteration 2/25 | Loss: 0.00074683
Iteration 3/25 | Loss: 0.00074679
Iteration 4/25 | Loss: 0.00074678
Iteration 5/25 | Loss: 0.00074678
Iteration 6/25 | Loss: 0.00074678
Iteration 7/25 | Loss: 0.00074678
Iteration 8/25 | Loss: 0.00074678
Iteration 9/25 | Loss: 0.00074678
Iteration 10/25 | Loss: 0.00074678
Iteration 11/25 | Loss: 0.00074678
Iteration 12/25 | Loss: 0.00074678
Iteration 13/25 | Loss: 0.00074678
Iteration 14/25 | Loss: 0.00074678
Iteration 15/25 | Loss: 0.00074678
Iteration 16/25 | Loss: 0.00074678
Iteration 17/25 | Loss: 0.00074678
Iteration 18/25 | Loss: 0.00074678
Iteration 19/25 | Loss: 0.00074678
Iteration 20/25 | Loss: 0.00074678
Iteration 21/25 | Loss: 0.00074678
Iteration 22/25 | Loss: 0.00074678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007467804825864732, 0.0007467804825864732, 0.0007467804825864732, 0.0007467804825864732, 0.0007467804825864732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007467804825864732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074678
Iteration 2/1000 | Loss: 0.00003270
Iteration 3/1000 | Loss: 0.00002286
Iteration 4/1000 | Loss: 0.00002012
Iteration 5/1000 | Loss: 0.00001884
Iteration 6/1000 | Loss: 0.00001802
Iteration 7/1000 | Loss: 0.00001755
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001679
Iteration 11/1000 | Loss: 0.00001651
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001625
Iteration 14/1000 | Loss: 0.00001621
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001603
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001593
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001591
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001589
Iteration 31/1000 | Loss: 0.00001589
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001586
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001583
Iteration 39/1000 | Loss: 0.00001581
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001579
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001568
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001564
Iteration 68/1000 | Loss: 0.00001564
Iteration 69/1000 | Loss: 0.00001564
Iteration 70/1000 | Loss: 0.00001563
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001561
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001561
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.5611194612574764e-05, 1.5611194612574764e-05, 1.5611194612574764e-05, 1.5611194612574764e-05, 1.5611194612574764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5611194612574764e-05

Optimization complete. Final v2v error: 3.250474452972412 mm

Highest mean error: 4.60670804977417 mm for frame 159

Lowest mean error: 2.4017364978790283 mm for frame 181

Saving results

Total time: 40.34915280342102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003067
Iteration 2/25 | Loss: 0.01003067
Iteration 3/25 | Loss: 0.00429535
Iteration 4/25 | Loss: 0.00315148
Iteration 5/25 | Loss: 0.00252331
Iteration 6/25 | Loss: 0.00222686
Iteration 7/25 | Loss: 0.00218310
Iteration 8/25 | Loss: 0.00200947
Iteration 9/25 | Loss: 0.00198203
Iteration 10/25 | Loss: 0.00197565
Iteration 11/25 | Loss: 0.00190543
Iteration 12/25 | Loss: 0.00178920
Iteration 13/25 | Loss: 0.00167650
Iteration 14/25 | Loss: 0.00165768
Iteration 15/25 | Loss: 0.00162653
Iteration 16/25 | Loss: 0.00161584
Iteration 17/25 | Loss: 0.00162289
Iteration 18/25 | Loss: 0.00161789
Iteration 19/25 | Loss: 0.00161484
Iteration 20/25 | Loss: 0.00160461
Iteration 21/25 | Loss: 0.00159872
Iteration 22/25 | Loss: 0.00159521
Iteration 23/25 | Loss: 0.00159412
Iteration 24/25 | Loss: 0.00158399
Iteration 25/25 | Loss: 0.00158237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25788367
Iteration 2/25 | Loss: 0.00617452
Iteration 3/25 | Loss: 0.00548619
Iteration 4/25 | Loss: 0.00543705
Iteration 5/25 | Loss: 0.00543705
Iteration 6/25 | Loss: 0.00543704
Iteration 7/25 | Loss: 0.00543704
Iteration 8/25 | Loss: 0.00543704
Iteration 9/25 | Loss: 0.00543704
Iteration 10/25 | Loss: 0.00543704
Iteration 11/25 | Loss: 0.00543704
Iteration 12/25 | Loss: 0.00543704
Iteration 13/25 | Loss: 0.00543704
Iteration 14/25 | Loss: 0.00543704
Iteration 15/25 | Loss: 0.00543704
Iteration 16/25 | Loss: 0.00543704
Iteration 17/25 | Loss: 0.00543704
Iteration 18/25 | Loss: 0.00543704
Iteration 19/25 | Loss: 0.00543704
Iteration 20/25 | Loss: 0.00543704
Iteration 21/25 | Loss: 0.00543704
Iteration 22/25 | Loss: 0.00543704
Iteration 23/25 | Loss: 0.00543704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.005437041167169809, 0.005437041167169809, 0.005437041167169809, 0.005437041167169809, 0.005437041167169809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005437041167169809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00543704
Iteration 2/1000 | Loss: 0.00281434
Iteration 3/1000 | Loss: 0.00432348
Iteration 4/1000 | Loss: 0.00136358
Iteration 5/1000 | Loss: 0.00051274
Iteration 6/1000 | Loss: 0.00062161
Iteration 7/1000 | Loss: 0.00093309
Iteration 8/1000 | Loss: 0.00185287
Iteration 9/1000 | Loss: 0.00090000
Iteration 10/1000 | Loss: 0.00100593
Iteration 11/1000 | Loss: 0.00055085
Iteration 12/1000 | Loss: 0.00044348
Iteration 13/1000 | Loss: 0.00047851
Iteration 14/1000 | Loss: 0.00039791
Iteration 15/1000 | Loss: 0.00036214
Iteration 16/1000 | Loss: 0.00078534
Iteration 17/1000 | Loss: 0.00040028
Iteration 18/1000 | Loss: 0.00050808
Iteration 19/1000 | Loss: 0.00085892
Iteration 20/1000 | Loss: 0.00113037
Iteration 21/1000 | Loss: 0.00052815
Iteration 22/1000 | Loss: 0.00027678
Iteration 23/1000 | Loss: 0.00028399
Iteration 24/1000 | Loss: 0.00025585
Iteration 25/1000 | Loss: 0.00027493
Iteration 26/1000 | Loss: 0.00022234
Iteration 27/1000 | Loss: 0.00050427
Iteration 28/1000 | Loss: 0.00232262
Iteration 29/1000 | Loss: 0.00026027
Iteration 30/1000 | Loss: 0.00024863
Iteration 31/1000 | Loss: 0.00021517
Iteration 32/1000 | Loss: 0.00033746
Iteration 33/1000 | Loss: 0.00052193
Iteration 34/1000 | Loss: 0.00048322
Iteration 35/1000 | Loss: 0.00074593
Iteration 36/1000 | Loss: 0.00058371
Iteration 37/1000 | Loss: 0.00061729
Iteration 38/1000 | Loss: 0.00141557
Iteration 39/1000 | Loss: 0.00030132
Iteration 40/1000 | Loss: 0.00021511
Iteration 41/1000 | Loss: 0.00020928
Iteration 42/1000 | Loss: 0.00020621
Iteration 43/1000 | Loss: 0.00020277
Iteration 44/1000 | Loss: 0.00035602
Iteration 45/1000 | Loss: 0.00031904
Iteration 46/1000 | Loss: 0.00033560
Iteration 47/1000 | Loss: 0.00023744
Iteration 48/1000 | Loss: 0.00030461
Iteration 49/1000 | Loss: 0.00021198
Iteration 50/1000 | Loss: 0.00035438
Iteration 51/1000 | Loss: 0.00037741
Iteration 52/1000 | Loss: 0.00030170
Iteration 53/1000 | Loss: 0.00019817
Iteration 54/1000 | Loss: 0.00033059
Iteration 55/1000 | Loss: 0.00022022
Iteration 56/1000 | Loss: 0.00029001
Iteration 57/1000 | Loss: 0.00027317
Iteration 58/1000 | Loss: 0.00026888
Iteration 59/1000 | Loss: 0.00023555
Iteration 60/1000 | Loss: 0.00026191
Iteration 61/1000 | Loss: 0.00031056
Iteration 62/1000 | Loss: 0.00021705
Iteration 63/1000 | Loss: 0.00019343
Iteration 64/1000 | Loss: 0.00021238
Iteration 65/1000 | Loss: 0.00019029
Iteration 66/1000 | Loss: 0.00024690
Iteration 67/1000 | Loss: 0.00034500
Iteration 68/1000 | Loss: 0.00022842
Iteration 69/1000 | Loss: 0.00019218
Iteration 70/1000 | Loss: 0.00018971
Iteration 71/1000 | Loss: 0.00018722
Iteration 72/1000 | Loss: 0.00018587
Iteration 73/1000 | Loss: 0.00031172
Iteration 74/1000 | Loss: 0.00019045
Iteration 75/1000 | Loss: 0.00018709
Iteration 76/1000 | Loss: 0.00018572
Iteration 77/1000 | Loss: 0.00018411
Iteration 78/1000 | Loss: 0.00031168
Iteration 79/1000 | Loss: 0.00019021
Iteration 80/1000 | Loss: 0.00018511
Iteration 81/1000 | Loss: 0.00018344
Iteration 82/1000 | Loss: 0.00018188
Iteration 83/1000 | Loss: 0.00018115
Iteration 84/1000 | Loss: 0.00018080
Iteration 85/1000 | Loss: 0.00018077
Iteration 86/1000 | Loss: 0.00018073
Iteration 87/1000 | Loss: 0.00018058
Iteration 88/1000 | Loss: 0.00018045
Iteration 89/1000 | Loss: 0.00018039
Iteration 90/1000 | Loss: 0.00018039
Iteration 91/1000 | Loss: 0.00018038
Iteration 92/1000 | Loss: 0.00018038
Iteration 93/1000 | Loss: 0.00018036
Iteration 94/1000 | Loss: 0.00018036
Iteration 95/1000 | Loss: 0.00018035
Iteration 96/1000 | Loss: 0.00018033
Iteration 97/1000 | Loss: 0.00018030
Iteration 98/1000 | Loss: 0.00018030
Iteration 99/1000 | Loss: 0.00018030
Iteration 100/1000 | Loss: 0.00018029
Iteration 101/1000 | Loss: 0.00018029
Iteration 102/1000 | Loss: 0.00018029
Iteration 103/1000 | Loss: 0.00018029
Iteration 104/1000 | Loss: 0.00018029
Iteration 105/1000 | Loss: 0.00018029
Iteration 106/1000 | Loss: 0.00018029
Iteration 107/1000 | Loss: 0.00018029
Iteration 108/1000 | Loss: 0.00018029
Iteration 109/1000 | Loss: 0.00018029
Iteration 110/1000 | Loss: 0.00018029
Iteration 111/1000 | Loss: 0.00018028
Iteration 112/1000 | Loss: 0.00018028
Iteration 113/1000 | Loss: 0.00018028
Iteration 114/1000 | Loss: 0.00018028
Iteration 115/1000 | Loss: 0.00018028
Iteration 116/1000 | Loss: 0.00018027
Iteration 117/1000 | Loss: 0.00018027
Iteration 118/1000 | Loss: 0.00018027
Iteration 119/1000 | Loss: 0.00018027
Iteration 120/1000 | Loss: 0.00018026
Iteration 121/1000 | Loss: 0.00018026
Iteration 122/1000 | Loss: 0.00018025
Iteration 123/1000 | Loss: 0.00018025
Iteration 124/1000 | Loss: 0.00018025
Iteration 125/1000 | Loss: 0.00018025
Iteration 126/1000 | Loss: 0.00018024
Iteration 127/1000 | Loss: 0.00018023
Iteration 128/1000 | Loss: 0.00018022
Iteration 129/1000 | Loss: 0.00018022
Iteration 130/1000 | Loss: 0.00018021
Iteration 131/1000 | Loss: 0.00018021
Iteration 132/1000 | Loss: 0.00018021
Iteration 133/1000 | Loss: 0.00018021
Iteration 134/1000 | Loss: 0.00018021
Iteration 135/1000 | Loss: 0.00018021
Iteration 136/1000 | Loss: 0.00018020
Iteration 137/1000 | Loss: 0.00018020
Iteration 138/1000 | Loss: 0.00018020
Iteration 139/1000 | Loss: 0.00018020
Iteration 140/1000 | Loss: 0.00018020
Iteration 141/1000 | Loss: 0.00018020
Iteration 142/1000 | Loss: 0.00018020
Iteration 143/1000 | Loss: 0.00018020
Iteration 144/1000 | Loss: 0.00018020
Iteration 145/1000 | Loss: 0.00018019
Iteration 146/1000 | Loss: 0.00018019
Iteration 147/1000 | Loss: 0.00018019
Iteration 148/1000 | Loss: 0.00018019
Iteration 149/1000 | Loss: 0.00018019
Iteration 150/1000 | Loss: 0.00018019
Iteration 151/1000 | Loss: 0.00018019
Iteration 152/1000 | Loss: 0.00018018
Iteration 153/1000 | Loss: 0.00018018
Iteration 154/1000 | Loss: 0.00018017
Iteration 155/1000 | Loss: 0.00018017
Iteration 156/1000 | Loss: 0.00018017
Iteration 157/1000 | Loss: 0.00018017
Iteration 158/1000 | Loss: 0.00018017
Iteration 159/1000 | Loss: 0.00018017
Iteration 160/1000 | Loss: 0.00018017
Iteration 161/1000 | Loss: 0.00018017
Iteration 162/1000 | Loss: 0.00018017
Iteration 163/1000 | Loss: 0.00018017
Iteration 164/1000 | Loss: 0.00018016
Iteration 165/1000 | Loss: 0.00018016
Iteration 166/1000 | Loss: 0.00018016
Iteration 167/1000 | Loss: 0.00018016
Iteration 168/1000 | Loss: 0.00018015
Iteration 169/1000 | Loss: 0.00018015
Iteration 170/1000 | Loss: 0.00018015
Iteration 171/1000 | Loss: 0.00018015
Iteration 172/1000 | Loss: 0.00018015
Iteration 173/1000 | Loss: 0.00018015
Iteration 174/1000 | Loss: 0.00018015
Iteration 175/1000 | Loss: 0.00018015
Iteration 176/1000 | Loss: 0.00018015
Iteration 177/1000 | Loss: 0.00018015
Iteration 178/1000 | Loss: 0.00018015
Iteration 179/1000 | Loss: 0.00018014
Iteration 180/1000 | Loss: 0.00018014
Iteration 181/1000 | Loss: 0.00018014
Iteration 182/1000 | Loss: 0.00018014
Iteration 183/1000 | Loss: 0.00018014
Iteration 184/1000 | Loss: 0.00018014
Iteration 185/1000 | Loss: 0.00018014
Iteration 186/1000 | Loss: 0.00018013
Iteration 187/1000 | Loss: 0.00018013
Iteration 188/1000 | Loss: 0.00018013
Iteration 189/1000 | Loss: 0.00018013
Iteration 190/1000 | Loss: 0.00018013
Iteration 191/1000 | Loss: 0.00018013
Iteration 192/1000 | Loss: 0.00018013
Iteration 193/1000 | Loss: 0.00018013
Iteration 194/1000 | Loss: 0.00018013
Iteration 195/1000 | Loss: 0.00018013
Iteration 196/1000 | Loss: 0.00018013
Iteration 197/1000 | Loss: 0.00018013
Iteration 198/1000 | Loss: 0.00018013
Iteration 199/1000 | Loss: 0.00018012
Iteration 200/1000 | Loss: 0.00018012
Iteration 201/1000 | Loss: 0.00018012
Iteration 202/1000 | Loss: 0.00018012
Iteration 203/1000 | Loss: 0.00018012
Iteration 204/1000 | Loss: 0.00018012
Iteration 205/1000 | Loss: 0.00018012
Iteration 206/1000 | Loss: 0.00018012
Iteration 207/1000 | Loss: 0.00018012
Iteration 208/1000 | Loss: 0.00018011
Iteration 209/1000 | Loss: 0.00018011
Iteration 210/1000 | Loss: 0.00018011
Iteration 211/1000 | Loss: 0.00018011
Iteration 212/1000 | Loss: 0.00018011
Iteration 213/1000 | Loss: 0.00018011
Iteration 214/1000 | Loss: 0.00018011
Iteration 215/1000 | Loss: 0.00018011
Iteration 216/1000 | Loss: 0.00018011
Iteration 217/1000 | Loss: 0.00018011
Iteration 218/1000 | Loss: 0.00018011
Iteration 219/1000 | Loss: 0.00018011
Iteration 220/1000 | Loss: 0.00018011
Iteration 221/1000 | Loss: 0.00018011
Iteration 222/1000 | Loss: 0.00018010
Iteration 223/1000 | Loss: 0.00018010
Iteration 224/1000 | Loss: 0.00018010
Iteration 225/1000 | Loss: 0.00018010
Iteration 226/1000 | Loss: 0.00018010
Iteration 227/1000 | Loss: 0.00018010
Iteration 228/1000 | Loss: 0.00018010
Iteration 229/1000 | Loss: 0.00018010
Iteration 230/1000 | Loss: 0.00018010
Iteration 231/1000 | Loss: 0.00018010
Iteration 232/1000 | Loss: 0.00018010
Iteration 233/1000 | Loss: 0.00018009
Iteration 234/1000 | Loss: 0.00018009
Iteration 235/1000 | Loss: 0.00018009
Iteration 236/1000 | Loss: 0.00018009
Iteration 237/1000 | Loss: 0.00018009
Iteration 238/1000 | Loss: 0.00018009
Iteration 239/1000 | Loss: 0.00018009
Iteration 240/1000 | Loss: 0.00018009
Iteration 241/1000 | Loss: 0.00018009
Iteration 242/1000 | Loss: 0.00018008
Iteration 243/1000 | Loss: 0.00018008
Iteration 244/1000 | Loss: 0.00018008
Iteration 245/1000 | Loss: 0.00018008
Iteration 246/1000 | Loss: 0.00018007
Iteration 247/1000 | Loss: 0.00018007
Iteration 248/1000 | Loss: 0.00018007
Iteration 249/1000 | Loss: 0.00018007
Iteration 250/1000 | Loss: 0.00018007
Iteration 251/1000 | Loss: 0.00018007
Iteration 252/1000 | Loss: 0.00018007
Iteration 253/1000 | Loss: 0.00018006
Iteration 254/1000 | Loss: 0.00018006
Iteration 255/1000 | Loss: 0.00018006
Iteration 256/1000 | Loss: 0.00018006
Iteration 257/1000 | Loss: 0.00018006
Iteration 258/1000 | Loss: 0.00018006
Iteration 259/1000 | Loss: 0.00018006
Iteration 260/1000 | Loss: 0.00018006
Iteration 261/1000 | Loss: 0.00018006
Iteration 262/1000 | Loss: 0.00018006
Iteration 263/1000 | Loss: 0.00018006
Iteration 264/1000 | Loss: 0.00018006
Iteration 265/1000 | Loss: 0.00018005
Iteration 266/1000 | Loss: 0.00018005
Iteration 267/1000 | Loss: 0.00018005
Iteration 268/1000 | Loss: 0.00018005
Iteration 269/1000 | Loss: 0.00018005
Iteration 270/1000 | Loss: 0.00018005
Iteration 271/1000 | Loss: 0.00018005
Iteration 272/1000 | Loss: 0.00018005
Iteration 273/1000 | Loss: 0.00018005
Iteration 274/1000 | Loss: 0.00018005
Iteration 275/1000 | Loss: 0.00018005
Iteration 276/1000 | Loss: 0.00018005
Iteration 277/1000 | Loss: 0.00018005
Iteration 278/1000 | Loss: 0.00018005
Iteration 279/1000 | Loss: 0.00018005
Iteration 280/1000 | Loss: 0.00018005
Iteration 281/1000 | Loss: 0.00018005
Iteration 282/1000 | Loss: 0.00018005
Iteration 283/1000 | Loss: 0.00018005
Iteration 284/1000 | Loss: 0.00018005
Iteration 285/1000 | Loss: 0.00018005
Iteration 286/1000 | Loss: 0.00018005
Iteration 287/1000 | Loss: 0.00018005
Iteration 288/1000 | Loss: 0.00018005
Iteration 289/1000 | Loss: 0.00018005
Iteration 290/1000 | Loss: 0.00018005
Iteration 291/1000 | Loss: 0.00018005
Iteration 292/1000 | Loss: 0.00018005
Iteration 293/1000 | Loss: 0.00018005
Iteration 294/1000 | Loss: 0.00018005
Iteration 295/1000 | Loss: 0.00018005
Iteration 296/1000 | Loss: 0.00018005
Iteration 297/1000 | Loss: 0.00018005
Iteration 298/1000 | Loss: 0.00018005
Iteration 299/1000 | Loss: 0.00018005
Iteration 300/1000 | Loss: 0.00018005
Iteration 301/1000 | Loss: 0.00018005
Iteration 302/1000 | Loss: 0.00018005
Iteration 303/1000 | Loss: 0.00018005
Iteration 304/1000 | Loss: 0.00018005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [0.00018004864978138357, 0.00018004864978138357, 0.00018004864978138357, 0.00018004864978138357, 0.00018004864978138357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018004864978138357

Optimization complete. Final v2v error: 6.721126079559326 mm

Highest mean error: 13.262818336486816 mm for frame 129

Lowest mean error: 3.5696909427642822 mm for frame 14

Saving results

Total time: 205.73116755485535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399692
Iteration 2/25 | Loss: 0.00110529
Iteration 3/25 | Loss: 0.00096045
Iteration 4/25 | Loss: 0.00094296
Iteration 5/25 | Loss: 0.00093782
Iteration 6/25 | Loss: 0.00093594
Iteration 7/25 | Loss: 0.00093551
Iteration 8/25 | Loss: 0.00093551
Iteration 9/25 | Loss: 0.00093551
Iteration 10/25 | Loss: 0.00093551
Iteration 11/25 | Loss: 0.00093551
Iteration 12/25 | Loss: 0.00093551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009355093934573233, 0.0009355093934573233, 0.0009355093934573233, 0.0009355093934573233, 0.0009355093934573233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009355093934573233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32650125
Iteration 2/25 | Loss: 0.00084960
Iteration 3/25 | Loss: 0.00084958
Iteration 4/25 | Loss: 0.00084958
Iteration 5/25 | Loss: 0.00084958
Iteration 6/25 | Loss: 0.00084958
Iteration 7/25 | Loss: 0.00084958
Iteration 8/25 | Loss: 0.00084958
Iteration 9/25 | Loss: 0.00084958
Iteration 10/25 | Loss: 0.00084958
Iteration 11/25 | Loss: 0.00084958
Iteration 12/25 | Loss: 0.00084958
Iteration 13/25 | Loss: 0.00084958
Iteration 14/25 | Loss: 0.00084958
Iteration 15/25 | Loss: 0.00084958
Iteration 16/25 | Loss: 0.00084958
Iteration 17/25 | Loss: 0.00084958
Iteration 18/25 | Loss: 0.00084958
Iteration 19/25 | Loss: 0.00084958
Iteration 20/25 | Loss: 0.00084958
Iteration 21/25 | Loss: 0.00084958
Iteration 22/25 | Loss: 0.00084958
Iteration 23/25 | Loss: 0.00084958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008495798683725297, 0.0008495798683725297, 0.0008495798683725297, 0.0008495798683725297, 0.0008495798683725297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008495798683725297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084958
Iteration 2/1000 | Loss: 0.00004557
Iteration 3/1000 | Loss: 0.00002280
Iteration 4/1000 | Loss: 0.00001683
Iteration 5/1000 | Loss: 0.00001533
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001337
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001270
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001237
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001234
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001230
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001226
Iteration 78/1000 | Loss: 0.00001226
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001224
Iteration 88/1000 | Loss: 0.00001224
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001223
Iteration 91/1000 | Loss: 0.00001223
Iteration 92/1000 | Loss: 0.00001223
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001223
Iteration 95/1000 | Loss: 0.00001222
Iteration 96/1000 | Loss: 0.00001222
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001220
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001219
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001217
Iteration 110/1000 | Loss: 0.00001217
Iteration 111/1000 | Loss: 0.00001217
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001215
Iteration 119/1000 | Loss: 0.00001215
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001210
Iteration 137/1000 | Loss: 0.00001210
Iteration 138/1000 | Loss: 0.00001210
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001210
Iteration 142/1000 | Loss: 0.00001210
Iteration 143/1000 | Loss: 0.00001210
Iteration 144/1000 | Loss: 0.00001210
Iteration 145/1000 | Loss: 0.00001210
Iteration 146/1000 | Loss: 0.00001210
Iteration 147/1000 | Loss: 0.00001209
Iteration 148/1000 | Loss: 0.00001209
Iteration 149/1000 | Loss: 0.00001209
Iteration 150/1000 | Loss: 0.00001209
Iteration 151/1000 | Loss: 0.00001209
Iteration 152/1000 | Loss: 0.00001209
Iteration 153/1000 | Loss: 0.00001209
Iteration 154/1000 | Loss: 0.00001209
Iteration 155/1000 | Loss: 0.00001209
Iteration 156/1000 | Loss: 0.00001209
Iteration 157/1000 | Loss: 0.00001209
Iteration 158/1000 | Loss: 0.00001209
Iteration 159/1000 | Loss: 0.00001209
Iteration 160/1000 | Loss: 0.00001209
Iteration 161/1000 | Loss: 0.00001209
Iteration 162/1000 | Loss: 0.00001209
Iteration 163/1000 | Loss: 0.00001209
Iteration 164/1000 | Loss: 0.00001209
Iteration 165/1000 | Loss: 0.00001209
Iteration 166/1000 | Loss: 0.00001209
Iteration 167/1000 | Loss: 0.00001209
Iteration 168/1000 | Loss: 0.00001209
Iteration 169/1000 | Loss: 0.00001209
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001209
Iteration 172/1000 | Loss: 0.00001209
Iteration 173/1000 | Loss: 0.00001209
Iteration 174/1000 | Loss: 0.00001209
Iteration 175/1000 | Loss: 0.00001209
Iteration 176/1000 | Loss: 0.00001209
Iteration 177/1000 | Loss: 0.00001209
Iteration 178/1000 | Loss: 0.00001209
Iteration 179/1000 | Loss: 0.00001209
Iteration 180/1000 | Loss: 0.00001209
Iteration 181/1000 | Loss: 0.00001209
Iteration 182/1000 | Loss: 0.00001209
Iteration 183/1000 | Loss: 0.00001209
Iteration 184/1000 | Loss: 0.00001209
Iteration 185/1000 | Loss: 0.00001209
Iteration 186/1000 | Loss: 0.00001209
Iteration 187/1000 | Loss: 0.00001209
Iteration 188/1000 | Loss: 0.00001209
Iteration 189/1000 | Loss: 0.00001209
Iteration 190/1000 | Loss: 0.00001209
Iteration 191/1000 | Loss: 0.00001209
Iteration 192/1000 | Loss: 0.00001209
Iteration 193/1000 | Loss: 0.00001209
Iteration 194/1000 | Loss: 0.00001209
Iteration 195/1000 | Loss: 0.00001209
Iteration 196/1000 | Loss: 0.00001209
Iteration 197/1000 | Loss: 0.00001209
Iteration 198/1000 | Loss: 0.00001209
Iteration 199/1000 | Loss: 0.00001209
Iteration 200/1000 | Loss: 0.00001209
Iteration 201/1000 | Loss: 0.00001209
Iteration 202/1000 | Loss: 0.00001209
Iteration 203/1000 | Loss: 0.00001209
Iteration 204/1000 | Loss: 0.00001209
Iteration 205/1000 | Loss: 0.00001209
Iteration 206/1000 | Loss: 0.00001209
Iteration 207/1000 | Loss: 0.00001209
Iteration 208/1000 | Loss: 0.00001209
Iteration 209/1000 | Loss: 0.00001209
Iteration 210/1000 | Loss: 0.00001209
Iteration 211/1000 | Loss: 0.00001209
Iteration 212/1000 | Loss: 0.00001209
Iteration 213/1000 | Loss: 0.00001209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.2085217349522281e-05, 1.2085217349522281e-05, 1.2085217349522281e-05, 1.2085217349522281e-05, 1.2085217349522281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2085217349522281e-05

Optimization complete. Final v2v error: 2.8054261207580566 mm

Highest mean error: 4.585077285766602 mm for frame 83

Lowest mean error: 2.2714109420776367 mm for frame 172

Saving results

Total time: 40.19292902946472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951327
Iteration 2/25 | Loss: 0.00182882
Iteration 3/25 | Loss: 0.00141612
Iteration 4/25 | Loss: 0.00132797
Iteration 5/25 | Loss: 0.00136726
Iteration 6/25 | Loss: 0.00127024
Iteration 7/25 | Loss: 0.00123180
Iteration 8/25 | Loss: 0.00123986
Iteration 9/25 | Loss: 0.00121888
Iteration 10/25 | Loss: 0.00121219
Iteration 11/25 | Loss: 0.00121543
Iteration 12/25 | Loss: 0.00121130
Iteration 13/25 | Loss: 0.00121106
Iteration 14/25 | Loss: 0.00119815
Iteration 15/25 | Loss: 0.00119003
Iteration 16/25 | Loss: 0.00119287
Iteration 17/25 | Loss: 0.00118692
Iteration 18/25 | Loss: 0.00118270
Iteration 19/25 | Loss: 0.00118036
Iteration 20/25 | Loss: 0.00118592
Iteration 21/25 | Loss: 0.00118314
Iteration 22/25 | Loss: 0.00118291
Iteration 23/25 | Loss: 0.00117942
Iteration 24/25 | Loss: 0.00118070
Iteration 25/25 | Loss: 0.00118021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.61823106
Iteration 2/25 | Loss: 0.00171281
Iteration 3/25 | Loss: 0.00171247
Iteration 4/25 | Loss: 0.00171247
Iteration 5/25 | Loss: 0.00171247
Iteration 6/25 | Loss: 0.00171247
Iteration 7/25 | Loss: 0.00171247
Iteration 8/25 | Loss: 0.00171247
Iteration 9/25 | Loss: 0.00171247
Iteration 10/25 | Loss: 0.00171247
Iteration 11/25 | Loss: 0.00171247
Iteration 12/25 | Loss: 0.00171247
Iteration 13/25 | Loss: 0.00171247
Iteration 14/25 | Loss: 0.00171247
Iteration 15/25 | Loss: 0.00171247
Iteration 16/25 | Loss: 0.00171247
Iteration 17/25 | Loss: 0.00171247
Iteration 18/25 | Loss: 0.00171247
Iteration 19/25 | Loss: 0.00171247
Iteration 20/25 | Loss: 0.00171247
Iteration 21/25 | Loss: 0.00171247
Iteration 22/25 | Loss: 0.00171247
Iteration 23/25 | Loss: 0.00171247
Iteration 24/25 | Loss: 0.00171247
Iteration 25/25 | Loss: 0.00171247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017124692676588893, 0.0017124692676588893, 0.0017124692676588893, 0.0017124692676588893, 0.0017124692676588893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017124692676588893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171247
Iteration 2/1000 | Loss: 0.00734097
Iteration 3/1000 | Loss: 0.00192073
Iteration 4/1000 | Loss: 0.00371802
Iteration 5/1000 | Loss: 0.00030938
Iteration 6/1000 | Loss: 0.00020800
Iteration 7/1000 | Loss: 0.00025056
Iteration 8/1000 | Loss: 0.00015787
Iteration 9/1000 | Loss: 0.00014743
Iteration 10/1000 | Loss: 0.00012464
Iteration 11/1000 | Loss: 0.00014455
Iteration 12/1000 | Loss: 0.00015889
Iteration 13/1000 | Loss: 0.00013593
Iteration 14/1000 | Loss: 0.00011816
Iteration 15/1000 | Loss: 0.00012059
Iteration 16/1000 | Loss: 0.00011935
Iteration 17/1000 | Loss: 0.00012466
Iteration 18/1000 | Loss: 0.00048102
Iteration 19/1000 | Loss: 0.00039287
Iteration 20/1000 | Loss: 0.00039514
Iteration 21/1000 | Loss: 0.00042085
Iteration 22/1000 | Loss: 0.00034527
Iteration 23/1000 | Loss: 0.00037374
Iteration 24/1000 | Loss: 0.00013073
Iteration 25/1000 | Loss: 0.00015694
Iteration 26/1000 | Loss: 0.00012883
Iteration 27/1000 | Loss: 0.00013616
Iteration 28/1000 | Loss: 0.00009228
Iteration 29/1000 | Loss: 0.00011172
Iteration 30/1000 | Loss: 0.00014829
Iteration 31/1000 | Loss: 0.00010854
Iteration 32/1000 | Loss: 0.00011596
Iteration 33/1000 | Loss: 0.00008894
Iteration 34/1000 | Loss: 0.00008186
Iteration 35/1000 | Loss: 0.00008143
Iteration 36/1000 | Loss: 0.00007857
Iteration 37/1000 | Loss: 0.00007601
Iteration 38/1000 | Loss: 0.00008577
Iteration 39/1000 | Loss: 0.00007247
Iteration 40/1000 | Loss: 0.00011154
Iteration 41/1000 | Loss: 0.00008217
Iteration 42/1000 | Loss: 0.00009307
Iteration 43/1000 | Loss: 0.00007812
Iteration 44/1000 | Loss: 0.00004816
Iteration 45/1000 | Loss: 0.00008280
Iteration 46/1000 | Loss: 0.00008432
Iteration 47/1000 | Loss: 0.00007358
Iteration 48/1000 | Loss: 0.00007842
Iteration 49/1000 | Loss: 0.00006878
Iteration 50/1000 | Loss: 0.00006868
Iteration 51/1000 | Loss: 0.00006342
Iteration 52/1000 | Loss: 0.00006425
Iteration 53/1000 | Loss: 0.00007153
Iteration 54/1000 | Loss: 0.00008854
Iteration 55/1000 | Loss: 0.00007202
Iteration 56/1000 | Loss: 0.00008999
Iteration 57/1000 | Loss: 0.00008168
Iteration 58/1000 | Loss: 0.00008467
Iteration 59/1000 | Loss: 0.00007097
Iteration 60/1000 | Loss: 0.00008470
Iteration 61/1000 | Loss: 0.00008686
Iteration 62/1000 | Loss: 0.00009039
Iteration 63/1000 | Loss: 0.00009354
Iteration 64/1000 | Loss: 0.00008854
Iteration 65/1000 | Loss: 0.00007873
Iteration 66/1000 | Loss: 0.00011877
Iteration 67/1000 | Loss: 0.00006657
Iteration 68/1000 | Loss: 0.00009430
Iteration 69/1000 | Loss: 0.00006495
Iteration 70/1000 | Loss: 0.00004410
Iteration 71/1000 | Loss: 0.00004215
Iteration 72/1000 | Loss: 0.00004117
Iteration 73/1000 | Loss: 0.00004061
Iteration 74/1000 | Loss: 0.00004026
Iteration 75/1000 | Loss: 0.00003976
Iteration 76/1000 | Loss: 0.00003929
Iteration 77/1000 | Loss: 0.00003897
Iteration 78/1000 | Loss: 0.00003868
Iteration 79/1000 | Loss: 0.00003844
Iteration 80/1000 | Loss: 0.00003830
Iteration 81/1000 | Loss: 0.00003825
Iteration 82/1000 | Loss: 0.00003823
Iteration 83/1000 | Loss: 0.00003822
Iteration 84/1000 | Loss: 0.00003818
Iteration 85/1000 | Loss: 0.00003807
Iteration 86/1000 | Loss: 0.00003795
Iteration 87/1000 | Loss: 0.00003794
Iteration 88/1000 | Loss: 0.00003793
Iteration 89/1000 | Loss: 0.00003793
Iteration 90/1000 | Loss: 0.00003790
Iteration 91/1000 | Loss: 0.00003790
Iteration 92/1000 | Loss: 0.00003790
Iteration 93/1000 | Loss: 0.00003790
Iteration 94/1000 | Loss: 0.00003790
Iteration 95/1000 | Loss: 0.00003790
Iteration 96/1000 | Loss: 0.00003790
Iteration 97/1000 | Loss: 0.00003789
Iteration 98/1000 | Loss: 0.00003789
Iteration 99/1000 | Loss: 0.00003789
Iteration 100/1000 | Loss: 0.00003786
Iteration 101/1000 | Loss: 0.00083913
Iteration 102/1000 | Loss: 0.00014104
Iteration 103/1000 | Loss: 0.00003918
Iteration 104/1000 | Loss: 0.00003808
Iteration 105/1000 | Loss: 0.00003787
Iteration 106/1000 | Loss: 0.00003780
Iteration 107/1000 | Loss: 0.00003780
Iteration 108/1000 | Loss: 0.00003779
Iteration 109/1000 | Loss: 0.00003779
Iteration 110/1000 | Loss: 0.00003779
Iteration 111/1000 | Loss: 0.00003778
Iteration 112/1000 | Loss: 0.00003778
Iteration 113/1000 | Loss: 0.00003777
Iteration 114/1000 | Loss: 0.00003775
Iteration 115/1000 | Loss: 0.00003774
Iteration 116/1000 | Loss: 0.00003774
Iteration 117/1000 | Loss: 0.00003774
Iteration 118/1000 | Loss: 0.00003773
Iteration 119/1000 | Loss: 0.00003773
Iteration 120/1000 | Loss: 0.00003773
Iteration 121/1000 | Loss: 0.00003772
Iteration 122/1000 | Loss: 0.00003772
Iteration 123/1000 | Loss: 0.00003772
Iteration 124/1000 | Loss: 0.00003772
Iteration 125/1000 | Loss: 0.00003772
Iteration 126/1000 | Loss: 0.00003772
Iteration 127/1000 | Loss: 0.00003771
Iteration 128/1000 | Loss: 0.00003771
Iteration 129/1000 | Loss: 0.00003771
Iteration 130/1000 | Loss: 0.00003771
Iteration 131/1000 | Loss: 0.00003771
Iteration 132/1000 | Loss: 0.00003771
Iteration 133/1000 | Loss: 0.00003771
Iteration 134/1000 | Loss: 0.00003771
Iteration 135/1000 | Loss: 0.00003771
Iteration 136/1000 | Loss: 0.00003771
Iteration 137/1000 | Loss: 0.00003771
Iteration 138/1000 | Loss: 0.00003771
Iteration 139/1000 | Loss: 0.00003771
Iteration 140/1000 | Loss: 0.00003771
Iteration 141/1000 | Loss: 0.00003771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.770750845433213e-05, 3.770750845433213e-05, 3.770750845433213e-05, 3.770750845433213e-05, 3.770750845433213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.770750845433213e-05

Optimization complete. Final v2v error: 4.432707786560059 mm

Highest mean error: 13.397125244140625 mm for frame 21

Lowest mean error: 3.1140077114105225 mm for frame 140

Saving results

Total time: 198.4790735244751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584919
Iteration 2/25 | Loss: 0.00100526
Iteration 3/25 | Loss: 0.00091007
Iteration 4/25 | Loss: 0.00089471
Iteration 5/25 | Loss: 0.00089031
Iteration 6/25 | Loss: 0.00088885
Iteration 7/25 | Loss: 0.00088877
Iteration 8/25 | Loss: 0.00088877
Iteration 9/25 | Loss: 0.00088877
Iteration 10/25 | Loss: 0.00088877
Iteration 11/25 | Loss: 0.00088877
Iteration 12/25 | Loss: 0.00088877
Iteration 13/25 | Loss: 0.00088877
Iteration 14/25 | Loss: 0.00088877
Iteration 15/25 | Loss: 0.00088877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008887678850442171, 0.0008887678850442171, 0.0008887678850442171, 0.0008887678850442171, 0.0008887678850442171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008887678850442171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00201845
Iteration 2/25 | Loss: 0.00070297
Iteration 3/25 | Loss: 0.00070297
Iteration 4/25 | Loss: 0.00070297
Iteration 5/25 | Loss: 0.00070297
Iteration 6/25 | Loss: 0.00070297
Iteration 7/25 | Loss: 0.00070297
Iteration 8/25 | Loss: 0.00070297
Iteration 9/25 | Loss: 0.00070297
Iteration 10/25 | Loss: 0.00070297
Iteration 11/25 | Loss: 0.00070297
Iteration 12/25 | Loss: 0.00070297
Iteration 13/25 | Loss: 0.00070297
Iteration 14/25 | Loss: 0.00070297
Iteration 15/25 | Loss: 0.00070297
Iteration 16/25 | Loss: 0.00070297
Iteration 17/25 | Loss: 0.00070297
Iteration 18/25 | Loss: 0.00070297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000702971126884222, 0.000702971126884222, 0.000702971126884222, 0.000702971126884222, 0.000702971126884222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000702971126884222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070297
Iteration 2/1000 | Loss: 0.00002284
Iteration 3/1000 | Loss: 0.00001287
Iteration 4/1000 | Loss: 0.00001170
Iteration 5/1000 | Loss: 0.00001124
Iteration 6/1000 | Loss: 0.00001075
Iteration 7/1000 | Loss: 0.00001041
Iteration 8/1000 | Loss: 0.00001013
Iteration 9/1000 | Loss: 0.00001002
Iteration 10/1000 | Loss: 0.00001000
Iteration 11/1000 | Loss: 0.00001000
Iteration 12/1000 | Loss: 0.00000994
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000982
Iteration 15/1000 | Loss: 0.00000977
Iteration 16/1000 | Loss: 0.00000976
Iteration 17/1000 | Loss: 0.00000972
Iteration 18/1000 | Loss: 0.00000972
Iteration 19/1000 | Loss: 0.00000969
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000967
Iteration 22/1000 | Loss: 0.00000966
Iteration 23/1000 | Loss: 0.00000966
Iteration 24/1000 | Loss: 0.00000966
Iteration 25/1000 | Loss: 0.00000965
Iteration 26/1000 | Loss: 0.00000965
Iteration 27/1000 | Loss: 0.00000964
Iteration 28/1000 | Loss: 0.00000964
Iteration 29/1000 | Loss: 0.00000963
Iteration 30/1000 | Loss: 0.00000963
Iteration 31/1000 | Loss: 0.00000963
Iteration 32/1000 | Loss: 0.00000962
Iteration 33/1000 | Loss: 0.00000962
Iteration 34/1000 | Loss: 0.00000960
Iteration 35/1000 | Loss: 0.00000960
Iteration 36/1000 | Loss: 0.00000958
Iteration 37/1000 | Loss: 0.00000958
Iteration 38/1000 | Loss: 0.00000958
Iteration 39/1000 | Loss: 0.00000958
Iteration 40/1000 | Loss: 0.00000957
Iteration 41/1000 | Loss: 0.00000957
Iteration 42/1000 | Loss: 0.00000957
Iteration 43/1000 | Loss: 0.00000957
Iteration 44/1000 | Loss: 0.00000957
Iteration 45/1000 | Loss: 0.00000957
Iteration 46/1000 | Loss: 0.00000957
Iteration 47/1000 | Loss: 0.00000955
Iteration 48/1000 | Loss: 0.00000954
Iteration 49/1000 | Loss: 0.00000954
Iteration 50/1000 | Loss: 0.00000954
Iteration 51/1000 | Loss: 0.00000954
Iteration 52/1000 | Loss: 0.00000953
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000952
Iteration 55/1000 | Loss: 0.00000951
Iteration 56/1000 | Loss: 0.00000951
Iteration 57/1000 | Loss: 0.00000951
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000950
Iteration 60/1000 | Loss: 0.00000950
Iteration 61/1000 | Loss: 0.00000950
Iteration 62/1000 | Loss: 0.00000950
Iteration 63/1000 | Loss: 0.00000950
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000949
Iteration 70/1000 | Loss: 0.00000949
Iteration 71/1000 | Loss: 0.00000949
Iteration 72/1000 | Loss: 0.00000948
Iteration 73/1000 | Loss: 0.00000948
Iteration 74/1000 | Loss: 0.00000948
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000948
Iteration 77/1000 | Loss: 0.00000947
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000947
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000946
Iteration 85/1000 | Loss: 0.00000946
Iteration 86/1000 | Loss: 0.00000946
Iteration 87/1000 | Loss: 0.00000946
Iteration 88/1000 | Loss: 0.00000946
Iteration 89/1000 | Loss: 0.00000945
Iteration 90/1000 | Loss: 0.00000945
Iteration 91/1000 | Loss: 0.00000945
Iteration 92/1000 | Loss: 0.00000945
Iteration 93/1000 | Loss: 0.00000945
Iteration 94/1000 | Loss: 0.00000945
Iteration 95/1000 | Loss: 0.00000945
Iteration 96/1000 | Loss: 0.00000944
Iteration 97/1000 | Loss: 0.00000944
Iteration 98/1000 | Loss: 0.00000944
Iteration 99/1000 | Loss: 0.00000944
Iteration 100/1000 | Loss: 0.00000944
Iteration 101/1000 | Loss: 0.00000943
Iteration 102/1000 | Loss: 0.00000943
Iteration 103/1000 | Loss: 0.00000943
Iteration 104/1000 | Loss: 0.00000943
Iteration 105/1000 | Loss: 0.00000943
Iteration 106/1000 | Loss: 0.00000943
Iteration 107/1000 | Loss: 0.00000943
Iteration 108/1000 | Loss: 0.00000943
Iteration 109/1000 | Loss: 0.00000943
Iteration 110/1000 | Loss: 0.00000943
Iteration 111/1000 | Loss: 0.00000942
Iteration 112/1000 | Loss: 0.00000942
Iteration 113/1000 | Loss: 0.00000942
Iteration 114/1000 | Loss: 0.00000942
Iteration 115/1000 | Loss: 0.00000942
Iteration 116/1000 | Loss: 0.00000942
Iteration 117/1000 | Loss: 0.00000942
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000941
Iteration 126/1000 | Loss: 0.00000941
Iteration 127/1000 | Loss: 0.00000941
Iteration 128/1000 | Loss: 0.00000941
Iteration 129/1000 | Loss: 0.00000941
Iteration 130/1000 | Loss: 0.00000941
Iteration 131/1000 | Loss: 0.00000941
Iteration 132/1000 | Loss: 0.00000940
Iteration 133/1000 | Loss: 0.00000940
Iteration 134/1000 | Loss: 0.00000940
Iteration 135/1000 | Loss: 0.00000940
Iteration 136/1000 | Loss: 0.00000940
Iteration 137/1000 | Loss: 0.00000940
Iteration 138/1000 | Loss: 0.00000940
Iteration 139/1000 | Loss: 0.00000940
Iteration 140/1000 | Loss: 0.00000940
Iteration 141/1000 | Loss: 0.00000940
Iteration 142/1000 | Loss: 0.00000940
Iteration 143/1000 | Loss: 0.00000939
Iteration 144/1000 | Loss: 0.00000939
Iteration 145/1000 | Loss: 0.00000939
Iteration 146/1000 | Loss: 0.00000939
Iteration 147/1000 | Loss: 0.00000939
Iteration 148/1000 | Loss: 0.00000939
Iteration 149/1000 | Loss: 0.00000938
Iteration 150/1000 | Loss: 0.00000938
Iteration 151/1000 | Loss: 0.00000938
Iteration 152/1000 | Loss: 0.00000938
Iteration 153/1000 | Loss: 0.00000938
Iteration 154/1000 | Loss: 0.00000938
Iteration 155/1000 | Loss: 0.00000938
Iteration 156/1000 | Loss: 0.00000938
Iteration 157/1000 | Loss: 0.00000938
Iteration 158/1000 | Loss: 0.00000938
Iteration 159/1000 | Loss: 0.00000938
Iteration 160/1000 | Loss: 0.00000938
Iteration 161/1000 | Loss: 0.00000938
Iteration 162/1000 | Loss: 0.00000938
Iteration 163/1000 | Loss: 0.00000937
Iteration 164/1000 | Loss: 0.00000937
Iteration 165/1000 | Loss: 0.00000937
Iteration 166/1000 | Loss: 0.00000937
Iteration 167/1000 | Loss: 0.00000937
Iteration 168/1000 | Loss: 0.00000937
Iteration 169/1000 | Loss: 0.00000937
Iteration 170/1000 | Loss: 0.00000937
Iteration 171/1000 | Loss: 0.00000937
Iteration 172/1000 | Loss: 0.00000937
Iteration 173/1000 | Loss: 0.00000937
Iteration 174/1000 | Loss: 0.00000936
Iteration 175/1000 | Loss: 0.00000936
Iteration 176/1000 | Loss: 0.00000936
Iteration 177/1000 | Loss: 0.00000936
Iteration 178/1000 | Loss: 0.00000936
Iteration 179/1000 | Loss: 0.00000936
Iteration 180/1000 | Loss: 0.00000936
Iteration 181/1000 | Loss: 0.00000936
Iteration 182/1000 | Loss: 0.00000936
Iteration 183/1000 | Loss: 0.00000936
Iteration 184/1000 | Loss: 0.00000936
Iteration 185/1000 | Loss: 0.00000936
Iteration 186/1000 | Loss: 0.00000936
Iteration 187/1000 | Loss: 0.00000936
Iteration 188/1000 | Loss: 0.00000935
Iteration 189/1000 | Loss: 0.00000935
Iteration 190/1000 | Loss: 0.00000935
Iteration 191/1000 | Loss: 0.00000935
Iteration 192/1000 | Loss: 0.00000935
Iteration 193/1000 | Loss: 0.00000935
Iteration 194/1000 | Loss: 0.00000935
Iteration 195/1000 | Loss: 0.00000935
Iteration 196/1000 | Loss: 0.00000935
Iteration 197/1000 | Loss: 0.00000935
Iteration 198/1000 | Loss: 0.00000935
Iteration 199/1000 | Loss: 0.00000935
Iteration 200/1000 | Loss: 0.00000935
Iteration 201/1000 | Loss: 0.00000935
Iteration 202/1000 | Loss: 0.00000935
Iteration 203/1000 | Loss: 0.00000934
Iteration 204/1000 | Loss: 0.00000934
Iteration 205/1000 | Loss: 0.00000934
Iteration 206/1000 | Loss: 0.00000934
Iteration 207/1000 | Loss: 0.00000934
Iteration 208/1000 | Loss: 0.00000934
Iteration 209/1000 | Loss: 0.00000934
Iteration 210/1000 | Loss: 0.00000934
Iteration 211/1000 | Loss: 0.00000934
Iteration 212/1000 | Loss: 0.00000934
Iteration 213/1000 | Loss: 0.00000934
Iteration 214/1000 | Loss: 0.00000934
Iteration 215/1000 | Loss: 0.00000934
Iteration 216/1000 | Loss: 0.00000934
Iteration 217/1000 | Loss: 0.00000934
Iteration 218/1000 | Loss: 0.00000934
Iteration 219/1000 | Loss: 0.00000934
Iteration 220/1000 | Loss: 0.00000934
Iteration 221/1000 | Loss: 0.00000934
Iteration 222/1000 | Loss: 0.00000934
Iteration 223/1000 | Loss: 0.00000934
Iteration 224/1000 | Loss: 0.00000934
Iteration 225/1000 | Loss: 0.00000934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [9.33883711695671e-06, 9.33883711695671e-06, 9.33883711695671e-06, 9.33883711695671e-06, 9.33883711695671e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.33883711695671e-06

Optimization complete. Final v2v error: 2.614659547805786 mm

Highest mean error: 2.8365750312805176 mm for frame 100

Lowest mean error: 2.351261854171753 mm for frame 0

Saving results

Total time: 37.5137038230896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626848
Iteration 2/25 | Loss: 0.00100954
Iteration 3/25 | Loss: 0.00092830
Iteration 4/25 | Loss: 0.00091485
Iteration 5/25 | Loss: 0.00091064
Iteration 6/25 | Loss: 0.00090919
Iteration 7/25 | Loss: 0.00090919
Iteration 8/25 | Loss: 0.00090919
Iteration 9/25 | Loss: 0.00090919
Iteration 10/25 | Loss: 0.00090919
Iteration 11/25 | Loss: 0.00090919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009091886458918452, 0.0009091886458918452, 0.0009091886458918452, 0.0009091886458918452, 0.0009091886458918452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009091886458918452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62675834
Iteration 2/25 | Loss: 0.00071487
Iteration 3/25 | Loss: 0.00071487
Iteration 4/25 | Loss: 0.00071487
Iteration 5/25 | Loss: 0.00071487
Iteration 6/25 | Loss: 0.00071487
Iteration 7/25 | Loss: 0.00071487
Iteration 8/25 | Loss: 0.00071487
Iteration 9/25 | Loss: 0.00071487
Iteration 10/25 | Loss: 0.00071487
Iteration 11/25 | Loss: 0.00071487
Iteration 12/25 | Loss: 0.00071487
Iteration 13/25 | Loss: 0.00071487
Iteration 14/25 | Loss: 0.00071487
Iteration 15/25 | Loss: 0.00071487
Iteration 16/25 | Loss: 0.00071487
Iteration 17/25 | Loss: 0.00071487
Iteration 18/25 | Loss: 0.00071487
Iteration 19/25 | Loss: 0.00071487
Iteration 20/25 | Loss: 0.00071487
Iteration 21/25 | Loss: 0.00071487
Iteration 22/25 | Loss: 0.00071487
Iteration 23/25 | Loss: 0.00071487
Iteration 24/25 | Loss: 0.00071487
Iteration 25/25 | Loss: 0.00071487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071487
Iteration 2/1000 | Loss: 0.00002016
Iteration 3/1000 | Loss: 0.00001405
Iteration 4/1000 | Loss: 0.00001314
Iteration 5/1000 | Loss: 0.00001262
Iteration 6/1000 | Loss: 0.00001216
Iteration 7/1000 | Loss: 0.00001177
Iteration 8/1000 | Loss: 0.00001157
Iteration 9/1000 | Loss: 0.00001133
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001121
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001113
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001112
Iteration 16/1000 | Loss: 0.00001112
Iteration 17/1000 | Loss: 0.00001112
Iteration 18/1000 | Loss: 0.00001111
Iteration 19/1000 | Loss: 0.00001109
Iteration 20/1000 | Loss: 0.00001104
Iteration 21/1000 | Loss: 0.00001097
Iteration 22/1000 | Loss: 0.00001090
Iteration 23/1000 | Loss: 0.00001088
Iteration 24/1000 | Loss: 0.00001088
Iteration 25/1000 | Loss: 0.00001087
Iteration 26/1000 | Loss: 0.00001085
Iteration 27/1000 | Loss: 0.00001085
Iteration 28/1000 | Loss: 0.00001085
Iteration 29/1000 | Loss: 0.00001085
Iteration 30/1000 | Loss: 0.00001085
Iteration 31/1000 | Loss: 0.00001085
Iteration 32/1000 | Loss: 0.00001084
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001083
Iteration 37/1000 | Loss: 0.00001083
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001082
Iteration 41/1000 | Loss: 0.00001081
Iteration 42/1000 | Loss: 0.00001081
Iteration 43/1000 | Loss: 0.00001080
Iteration 44/1000 | Loss: 0.00001080
Iteration 45/1000 | Loss: 0.00001079
Iteration 46/1000 | Loss: 0.00001079
Iteration 47/1000 | Loss: 0.00001079
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001078
Iteration 51/1000 | Loss: 0.00001078
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001076
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001075
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001073
Iteration 72/1000 | Loss: 0.00001073
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001072
Iteration 76/1000 | Loss: 0.00001072
Iteration 77/1000 | Loss: 0.00001072
Iteration 78/1000 | Loss: 0.00001072
Iteration 79/1000 | Loss: 0.00001072
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001070
Iteration 83/1000 | Loss: 0.00001070
Iteration 84/1000 | Loss: 0.00001070
Iteration 85/1000 | Loss: 0.00001070
Iteration 86/1000 | Loss: 0.00001070
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001070
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001069
Iteration 92/1000 | Loss: 0.00001069
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001069
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001069
Iteration 97/1000 | Loss: 0.00001068
Iteration 98/1000 | Loss: 0.00001068
Iteration 99/1000 | Loss: 0.00001068
Iteration 100/1000 | Loss: 0.00001068
Iteration 101/1000 | Loss: 0.00001068
Iteration 102/1000 | Loss: 0.00001068
Iteration 103/1000 | Loss: 0.00001068
Iteration 104/1000 | Loss: 0.00001068
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001067
Iteration 108/1000 | Loss: 0.00001067
Iteration 109/1000 | Loss: 0.00001067
Iteration 110/1000 | Loss: 0.00001067
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001067
Iteration 121/1000 | Loss: 0.00001067
Iteration 122/1000 | Loss: 0.00001067
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001066
Iteration 125/1000 | Loss: 0.00001066
Iteration 126/1000 | Loss: 0.00001066
Iteration 127/1000 | Loss: 0.00001066
Iteration 128/1000 | Loss: 0.00001066
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001065
Iteration 134/1000 | Loss: 0.00001065
Iteration 135/1000 | Loss: 0.00001065
Iteration 136/1000 | Loss: 0.00001065
Iteration 137/1000 | Loss: 0.00001065
Iteration 138/1000 | Loss: 0.00001065
Iteration 139/1000 | Loss: 0.00001065
Iteration 140/1000 | Loss: 0.00001065
Iteration 141/1000 | Loss: 0.00001065
Iteration 142/1000 | Loss: 0.00001064
Iteration 143/1000 | Loss: 0.00001064
Iteration 144/1000 | Loss: 0.00001064
Iteration 145/1000 | Loss: 0.00001064
Iteration 146/1000 | Loss: 0.00001064
Iteration 147/1000 | Loss: 0.00001064
Iteration 148/1000 | Loss: 0.00001064
Iteration 149/1000 | Loss: 0.00001064
Iteration 150/1000 | Loss: 0.00001064
Iteration 151/1000 | Loss: 0.00001064
Iteration 152/1000 | Loss: 0.00001064
Iteration 153/1000 | Loss: 0.00001064
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001063
Iteration 157/1000 | Loss: 0.00001063
Iteration 158/1000 | Loss: 0.00001063
Iteration 159/1000 | Loss: 0.00001063
Iteration 160/1000 | Loss: 0.00001063
Iteration 161/1000 | Loss: 0.00001063
Iteration 162/1000 | Loss: 0.00001063
Iteration 163/1000 | Loss: 0.00001063
Iteration 164/1000 | Loss: 0.00001063
Iteration 165/1000 | Loss: 0.00001063
Iteration 166/1000 | Loss: 0.00001063
Iteration 167/1000 | Loss: 0.00001063
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001062
Iteration 172/1000 | Loss: 0.00001062
Iteration 173/1000 | Loss: 0.00001062
Iteration 174/1000 | Loss: 0.00001062
Iteration 175/1000 | Loss: 0.00001062
Iteration 176/1000 | Loss: 0.00001062
Iteration 177/1000 | Loss: 0.00001061
Iteration 178/1000 | Loss: 0.00001061
Iteration 179/1000 | Loss: 0.00001061
Iteration 180/1000 | Loss: 0.00001061
Iteration 181/1000 | Loss: 0.00001061
Iteration 182/1000 | Loss: 0.00001061
Iteration 183/1000 | Loss: 0.00001060
Iteration 184/1000 | Loss: 0.00001060
Iteration 185/1000 | Loss: 0.00001060
Iteration 186/1000 | Loss: 0.00001060
Iteration 187/1000 | Loss: 0.00001060
Iteration 188/1000 | Loss: 0.00001060
Iteration 189/1000 | Loss: 0.00001060
Iteration 190/1000 | Loss: 0.00001060
Iteration 191/1000 | Loss: 0.00001060
Iteration 192/1000 | Loss: 0.00001060
Iteration 193/1000 | Loss: 0.00001060
Iteration 194/1000 | Loss: 0.00001060
Iteration 195/1000 | Loss: 0.00001060
Iteration 196/1000 | Loss: 0.00001060
Iteration 197/1000 | Loss: 0.00001060
Iteration 198/1000 | Loss: 0.00001060
Iteration 199/1000 | Loss: 0.00001060
Iteration 200/1000 | Loss: 0.00001059
Iteration 201/1000 | Loss: 0.00001059
Iteration 202/1000 | Loss: 0.00001059
Iteration 203/1000 | Loss: 0.00001059
Iteration 204/1000 | Loss: 0.00001059
Iteration 205/1000 | Loss: 0.00001059
Iteration 206/1000 | Loss: 0.00001058
Iteration 207/1000 | Loss: 0.00001058
Iteration 208/1000 | Loss: 0.00001058
Iteration 209/1000 | Loss: 0.00001058
Iteration 210/1000 | Loss: 0.00001058
Iteration 211/1000 | Loss: 0.00001058
Iteration 212/1000 | Loss: 0.00001058
Iteration 213/1000 | Loss: 0.00001058
Iteration 214/1000 | Loss: 0.00001058
Iteration 215/1000 | Loss: 0.00001058
Iteration 216/1000 | Loss: 0.00001058
Iteration 217/1000 | Loss: 0.00001058
Iteration 218/1000 | Loss: 0.00001058
Iteration 219/1000 | Loss: 0.00001057
Iteration 220/1000 | Loss: 0.00001057
Iteration 221/1000 | Loss: 0.00001057
Iteration 222/1000 | Loss: 0.00001057
Iteration 223/1000 | Loss: 0.00001057
Iteration 224/1000 | Loss: 0.00001057
Iteration 225/1000 | Loss: 0.00001057
Iteration 226/1000 | Loss: 0.00001057
Iteration 227/1000 | Loss: 0.00001057
Iteration 228/1000 | Loss: 0.00001057
Iteration 229/1000 | Loss: 0.00001057
Iteration 230/1000 | Loss: 0.00001057
Iteration 231/1000 | Loss: 0.00001057
Iteration 232/1000 | Loss: 0.00001057
Iteration 233/1000 | Loss: 0.00001057
Iteration 234/1000 | Loss: 0.00001057
Iteration 235/1000 | Loss: 0.00001057
Iteration 236/1000 | Loss: 0.00001057
Iteration 237/1000 | Loss: 0.00001057
Iteration 238/1000 | Loss: 0.00001057
Iteration 239/1000 | Loss: 0.00001057
Iteration 240/1000 | Loss: 0.00001057
Iteration 241/1000 | Loss: 0.00001057
Iteration 242/1000 | Loss: 0.00001057
Iteration 243/1000 | Loss: 0.00001057
Iteration 244/1000 | Loss: 0.00001057
Iteration 245/1000 | Loss: 0.00001057
Iteration 246/1000 | Loss: 0.00001057
Iteration 247/1000 | Loss: 0.00001057
Iteration 248/1000 | Loss: 0.00001057
Iteration 249/1000 | Loss: 0.00001057
Iteration 250/1000 | Loss: 0.00001057
Iteration 251/1000 | Loss: 0.00001057
Iteration 252/1000 | Loss: 0.00001057
Iteration 253/1000 | Loss: 0.00001057
Iteration 254/1000 | Loss: 0.00001057
Iteration 255/1000 | Loss: 0.00001057
Iteration 256/1000 | Loss: 0.00001057
Iteration 257/1000 | Loss: 0.00001057
Iteration 258/1000 | Loss: 0.00001057
Iteration 259/1000 | Loss: 0.00001057
Iteration 260/1000 | Loss: 0.00001057
Iteration 261/1000 | Loss: 0.00001057
Iteration 262/1000 | Loss: 0.00001057
Iteration 263/1000 | Loss: 0.00001057
Iteration 264/1000 | Loss: 0.00001057
Iteration 265/1000 | Loss: 0.00001057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.0565501725068316e-05, 1.0565501725068316e-05, 1.0565501725068316e-05, 1.0565501725068316e-05, 1.0565501725068316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0565501725068316e-05

Optimization complete. Final v2v error: 2.789797782897949 mm

Highest mean error: 3.1666758060455322 mm for frame 59

Lowest mean error: 2.1711413860321045 mm for frame 1

Saving results

Total time: 42.4352753162384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446548
Iteration 2/25 | Loss: 0.00123084
Iteration 3/25 | Loss: 0.00104627
Iteration 4/25 | Loss: 0.00102637
Iteration 5/25 | Loss: 0.00102271
Iteration 6/25 | Loss: 0.00102164
Iteration 7/25 | Loss: 0.00102140
Iteration 8/25 | Loss: 0.00102140
Iteration 9/25 | Loss: 0.00102140
Iteration 10/25 | Loss: 0.00102140
Iteration 11/25 | Loss: 0.00102140
Iteration 12/25 | Loss: 0.00102140
Iteration 13/25 | Loss: 0.00102140
Iteration 14/25 | Loss: 0.00102140
Iteration 15/25 | Loss: 0.00102140
Iteration 16/25 | Loss: 0.00102140
Iteration 17/25 | Loss: 0.00102140
Iteration 18/25 | Loss: 0.00102140
Iteration 19/25 | Loss: 0.00102140
Iteration 20/25 | Loss: 0.00102140
Iteration 21/25 | Loss: 0.00102140
Iteration 22/25 | Loss: 0.00102140
Iteration 23/25 | Loss: 0.00102140
Iteration 24/25 | Loss: 0.00102140
Iteration 25/25 | Loss: 0.00102140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44555187
Iteration 2/25 | Loss: 0.00067135
Iteration 3/25 | Loss: 0.00067135
Iteration 4/25 | Loss: 0.00067135
Iteration 5/25 | Loss: 0.00067135
Iteration 6/25 | Loss: 0.00067135
Iteration 7/25 | Loss: 0.00067135
Iteration 8/25 | Loss: 0.00067134
Iteration 9/25 | Loss: 0.00067134
Iteration 10/25 | Loss: 0.00067134
Iteration 11/25 | Loss: 0.00067134
Iteration 12/25 | Loss: 0.00067134
Iteration 13/25 | Loss: 0.00067134
Iteration 14/25 | Loss: 0.00067134
Iteration 15/25 | Loss: 0.00067134
Iteration 16/25 | Loss: 0.00067134
Iteration 17/25 | Loss: 0.00067134
Iteration 18/25 | Loss: 0.00067134
Iteration 19/25 | Loss: 0.00067134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006713442271575332, 0.0006713442271575332, 0.0006713442271575332, 0.0006713442271575332, 0.0006713442271575332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006713442271575332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067134
Iteration 2/1000 | Loss: 0.00006461
Iteration 3/1000 | Loss: 0.00002681
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00001772
Iteration 8/1000 | Loss: 0.00001726
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001662
Iteration 11/1000 | Loss: 0.00001659
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001620
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001605
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001577
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001576
Iteration 42/1000 | Loss: 0.00001576
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001575
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001574
Iteration 47/1000 | Loss: 0.00001574
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001572
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001571
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001571
Iteration 61/1000 | Loss: 0.00001571
Iteration 62/1000 | Loss: 0.00001571
Iteration 63/1000 | Loss: 0.00001571
Iteration 64/1000 | Loss: 0.00001571
Iteration 65/1000 | Loss: 0.00001570
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001569
Iteration 68/1000 | Loss: 0.00001569
Iteration 69/1000 | Loss: 0.00001569
Iteration 70/1000 | Loss: 0.00001568
Iteration 71/1000 | Loss: 0.00001568
Iteration 72/1000 | Loss: 0.00001568
Iteration 73/1000 | Loss: 0.00001568
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001567
Iteration 76/1000 | Loss: 0.00001567
Iteration 77/1000 | Loss: 0.00001567
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001562
Iteration 91/1000 | Loss: 0.00001562
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001556
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001556
Iteration 120/1000 | Loss: 0.00001556
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001555
Iteration 125/1000 | Loss: 0.00001555
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001552
Iteration 134/1000 | Loss: 0.00001552
Iteration 135/1000 | Loss: 0.00001552
Iteration 136/1000 | Loss: 0.00001551
Iteration 137/1000 | Loss: 0.00001551
Iteration 138/1000 | Loss: 0.00001551
Iteration 139/1000 | Loss: 0.00001551
Iteration 140/1000 | Loss: 0.00001551
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001550
Iteration 145/1000 | Loss: 0.00001550
Iteration 146/1000 | Loss: 0.00001550
Iteration 147/1000 | Loss: 0.00001550
Iteration 148/1000 | Loss: 0.00001550
Iteration 149/1000 | Loss: 0.00001549
Iteration 150/1000 | Loss: 0.00001549
Iteration 151/1000 | Loss: 0.00001549
Iteration 152/1000 | Loss: 0.00001549
Iteration 153/1000 | Loss: 0.00001549
Iteration 154/1000 | Loss: 0.00001549
Iteration 155/1000 | Loss: 0.00001549
Iteration 156/1000 | Loss: 0.00001549
Iteration 157/1000 | Loss: 0.00001549
Iteration 158/1000 | Loss: 0.00001549
Iteration 159/1000 | Loss: 0.00001548
Iteration 160/1000 | Loss: 0.00001548
Iteration 161/1000 | Loss: 0.00001548
Iteration 162/1000 | Loss: 0.00001548
Iteration 163/1000 | Loss: 0.00001548
Iteration 164/1000 | Loss: 0.00001548
Iteration 165/1000 | Loss: 0.00001548
Iteration 166/1000 | Loss: 0.00001548
Iteration 167/1000 | Loss: 0.00001547
Iteration 168/1000 | Loss: 0.00001547
Iteration 169/1000 | Loss: 0.00001547
Iteration 170/1000 | Loss: 0.00001547
Iteration 171/1000 | Loss: 0.00001547
Iteration 172/1000 | Loss: 0.00001547
Iteration 173/1000 | Loss: 0.00001547
Iteration 174/1000 | Loss: 0.00001547
Iteration 175/1000 | Loss: 0.00001547
Iteration 176/1000 | Loss: 0.00001547
Iteration 177/1000 | Loss: 0.00001547
Iteration 178/1000 | Loss: 0.00001547
Iteration 179/1000 | Loss: 0.00001547
Iteration 180/1000 | Loss: 0.00001547
Iteration 181/1000 | Loss: 0.00001547
Iteration 182/1000 | Loss: 0.00001547
Iteration 183/1000 | Loss: 0.00001547
Iteration 184/1000 | Loss: 0.00001547
Iteration 185/1000 | Loss: 0.00001547
Iteration 186/1000 | Loss: 0.00001547
Iteration 187/1000 | Loss: 0.00001547
Iteration 188/1000 | Loss: 0.00001547
Iteration 189/1000 | Loss: 0.00001547
Iteration 190/1000 | Loss: 0.00001547
Iteration 191/1000 | Loss: 0.00001547
Iteration 192/1000 | Loss: 0.00001547
Iteration 193/1000 | Loss: 0.00001547
Iteration 194/1000 | Loss: 0.00001547
Iteration 195/1000 | Loss: 0.00001547
Iteration 196/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.546715066069737e-05, 1.546715066069737e-05, 1.546715066069737e-05, 1.546715066069737e-05, 1.546715066069737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.546715066069737e-05

Optimization complete. Final v2v error: 3.322441339492798 mm

Highest mean error: 4.411396026611328 mm for frame 56

Lowest mean error: 2.838749885559082 mm for frame 37

Saving results

Total time: 42.35776400566101
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926004
Iteration 2/25 | Loss: 0.00129663
Iteration 3/25 | Loss: 0.00109160
Iteration 4/25 | Loss: 0.00105589
Iteration 5/25 | Loss: 0.00105698
Iteration 6/25 | Loss: 0.00104761
Iteration 7/25 | Loss: 0.00103330
Iteration 8/25 | Loss: 0.00102704
Iteration 9/25 | Loss: 0.00102528
Iteration 10/25 | Loss: 0.00102430
Iteration 11/25 | Loss: 0.00102377
Iteration 12/25 | Loss: 0.00102355
Iteration 13/25 | Loss: 0.00102351
Iteration 14/25 | Loss: 0.00102351
Iteration 15/25 | Loss: 0.00102351
Iteration 16/25 | Loss: 0.00102351
Iteration 17/25 | Loss: 0.00102351
Iteration 18/25 | Loss: 0.00102351
Iteration 19/25 | Loss: 0.00102351
Iteration 20/25 | Loss: 0.00102351
Iteration 21/25 | Loss: 0.00102351
Iteration 22/25 | Loss: 0.00102351
Iteration 23/25 | Loss: 0.00102351
Iteration 24/25 | Loss: 0.00102351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010235091904178262, 0.0010235091904178262, 0.0010235091904178262, 0.0010235091904178262, 0.0010235091904178262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010235091904178262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23909307
Iteration 2/25 | Loss: 0.00070380
Iteration 3/25 | Loss: 0.00070380
Iteration 4/25 | Loss: 0.00070380
Iteration 5/25 | Loss: 0.00070380
Iteration 6/25 | Loss: 0.00070380
Iteration 7/25 | Loss: 0.00070380
Iteration 8/25 | Loss: 0.00070380
Iteration 9/25 | Loss: 0.00070380
Iteration 10/25 | Loss: 0.00070380
Iteration 11/25 | Loss: 0.00070380
Iteration 12/25 | Loss: 0.00070380
Iteration 13/25 | Loss: 0.00070380
Iteration 14/25 | Loss: 0.00070380
Iteration 15/25 | Loss: 0.00070380
Iteration 16/25 | Loss: 0.00070380
Iteration 17/25 | Loss: 0.00070380
Iteration 18/25 | Loss: 0.00070380
Iteration 19/25 | Loss: 0.00070380
Iteration 20/25 | Loss: 0.00070380
Iteration 21/25 | Loss: 0.00070380
Iteration 22/25 | Loss: 0.00070380
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007037980831228197, 0.0007037980831228197, 0.0007037980831228197, 0.0007037980831228197, 0.0007037980831228197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007037980831228197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070380
Iteration 2/1000 | Loss: 0.00003688
Iteration 3/1000 | Loss: 0.00002883
Iteration 4/1000 | Loss: 0.00002728
Iteration 5/1000 | Loss: 0.00002599
Iteration 6/1000 | Loss: 0.00002517
Iteration 7/1000 | Loss: 0.00002469
Iteration 8/1000 | Loss: 0.00002416
Iteration 9/1000 | Loss: 0.00002380
Iteration 10/1000 | Loss: 0.00002346
Iteration 11/1000 | Loss: 0.00002325
Iteration 12/1000 | Loss: 0.00002310
Iteration 13/1000 | Loss: 0.00002294
Iteration 14/1000 | Loss: 0.00002293
Iteration 15/1000 | Loss: 0.00002293
Iteration 16/1000 | Loss: 0.00002293
Iteration 17/1000 | Loss: 0.00002288
Iteration 18/1000 | Loss: 0.00002288
Iteration 19/1000 | Loss: 0.00002287
Iteration 20/1000 | Loss: 0.00002287
Iteration 21/1000 | Loss: 0.00002282
Iteration 22/1000 | Loss: 0.00002281
Iteration 23/1000 | Loss: 0.00002281
Iteration 24/1000 | Loss: 0.00002280
Iteration 25/1000 | Loss: 0.00002280
Iteration 26/1000 | Loss: 0.00002279
Iteration 27/1000 | Loss: 0.00002279
Iteration 28/1000 | Loss: 0.00002278
Iteration 29/1000 | Loss: 0.00002278
Iteration 30/1000 | Loss: 0.00002277
Iteration 31/1000 | Loss: 0.00002277
Iteration 32/1000 | Loss: 0.00002276
Iteration 33/1000 | Loss: 0.00002276
Iteration 34/1000 | Loss: 0.00002276
Iteration 35/1000 | Loss: 0.00002275
Iteration 36/1000 | Loss: 0.00002275
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002273
Iteration 39/1000 | Loss: 0.00002273
Iteration 40/1000 | Loss: 0.00002272
Iteration 41/1000 | Loss: 0.00002272
Iteration 42/1000 | Loss: 0.00002272
Iteration 43/1000 | Loss: 0.00002271
Iteration 44/1000 | Loss: 0.00002271
Iteration 45/1000 | Loss: 0.00002271
Iteration 46/1000 | Loss: 0.00002271
Iteration 47/1000 | Loss: 0.00002270
Iteration 48/1000 | Loss: 0.00002270
Iteration 49/1000 | Loss: 0.00002269
Iteration 50/1000 | Loss: 0.00002269
Iteration 51/1000 | Loss: 0.00002268
Iteration 52/1000 | Loss: 0.00002268
Iteration 53/1000 | Loss: 0.00002268
Iteration 54/1000 | Loss: 0.00002268
Iteration 55/1000 | Loss: 0.00002268
Iteration 56/1000 | Loss: 0.00002268
Iteration 57/1000 | Loss: 0.00002268
Iteration 58/1000 | Loss: 0.00002268
Iteration 59/1000 | Loss: 0.00002268
Iteration 60/1000 | Loss: 0.00002268
Iteration 61/1000 | Loss: 0.00002268
Iteration 62/1000 | Loss: 0.00002268
Iteration 63/1000 | Loss: 0.00002268
Iteration 64/1000 | Loss: 0.00002267
Iteration 65/1000 | Loss: 0.00002267
Iteration 66/1000 | Loss: 0.00002267
Iteration 67/1000 | Loss: 0.00002267
Iteration 68/1000 | Loss: 0.00002266
Iteration 69/1000 | Loss: 0.00002266
Iteration 70/1000 | Loss: 0.00002265
Iteration 71/1000 | Loss: 0.00002265
Iteration 72/1000 | Loss: 0.00002265
Iteration 73/1000 | Loss: 0.00002264
Iteration 74/1000 | Loss: 0.00002264
Iteration 75/1000 | Loss: 0.00002264
Iteration 76/1000 | Loss: 0.00002264
Iteration 77/1000 | Loss: 0.00002264
Iteration 78/1000 | Loss: 0.00002263
Iteration 79/1000 | Loss: 0.00002263
Iteration 80/1000 | Loss: 0.00002263
Iteration 81/1000 | Loss: 0.00002263
Iteration 82/1000 | Loss: 0.00002263
Iteration 83/1000 | Loss: 0.00002262
Iteration 84/1000 | Loss: 0.00002262
Iteration 85/1000 | Loss: 0.00002262
Iteration 86/1000 | Loss: 0.00002262
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002262
Iteration 92/1000 | Loss: 0.00002262
Iteration 93/1000 | Loss: 0.00002262
Iteration 94/1000 | Loss: 0.00002262
Iteration 95/1000 | Loss: 0.00002261
Iteration 96/1000 | Loss: 0.00002261
Iteration 97/1000 | Loss: 0.00002261
Iteration 98/1000 | Loss: 0.00002261
Iteration 99/1000 | Loss: 0.00002261
Iteration 100/1000 | Loss: 0.00002261
Iteration 101/1000 | Loss: 0.00002261
Iteration 102/1000 | Loss: 0.00002261
Iteration 103/1000 | Loss: 0.00002261
Iteration 104/1000 | Loss: 0.00002261
Iteration 105/1000 | Loss: 0.00002261
Iteration 106/1000 | Loss: 0.00002261
Iteration 107/1000 | Loss: 0.00002261
Iteration 108/1000 | Loss: 0.00002261
Iteration 109/1000 | Loss: 0.00002260
Iteration 110/1000 | Loss: 0.00002260
Iteration 111/1000 | Loss: 0.00002260
Iteration 112/1000 | Loss: 0.00002260
Iteration 113/1000 | Loss: 0.00002260
Iteration 114/1000 | Loss: 0.00002260
Iteration 115/1000 | Loss: 0.00002260
Iteration 116/1000 | Loss: 0.00002259
Iteration 117/1000 | Loss: 0.00002259
Iteration 118/1000 | Loss: 0.00002259
Iteration 119/1000 | Loss: 0.00002259
Iteration 120/1000 | Loss: 0.00002259
Iteration 121/1000 | Loss: 0.00002259
Iteration 122/1000 | Loss: 0.00002259
Iteration 123/1000 | Loss: 0.00002259
Iteration 124/1000 | Loss: 0.00002258
Iteration 125/1000 | Loss: 0.00002258
Iteration 126/1000 | Loss: 0.00002258
Iteration 127/1000 | Loss: 0.00002258
Iteration 128/1000 | Loss: 0.00002258
Iteration 129/1000 | Loss: 0.00002257
Iteration 130/1000 | Loss: 0.00002257
Iteration 131/1000 | Loss: 0.00002257
Iteration 132/1000 | Loss: 0.00002257
Iteration 133/1000 | Loss: 0.00002257
Iteration 134/1000 | Loss: 0.00002256
Iteration 135/1000 | Loss: 0.00002256
Iteration 136/1000 | Loss: 0.00002256
Iteration 137/1000 | Loss: 0.00002256
Iteration 138/1000 | Loss: 0.00002256
Iteration 139/1000 | Loss: 0.00002256
Iteration 140/1000 | Loss: 0.00002256
Iteration 141/1000 | Loss: 0.00002255
Iteration 142/1000 | Loss: 0.00002255
Iteration 143/1000 | Loss: 0.00002255
Iteration 144/1000 | Loss: 0.00002255
Iteration 145/1000 | Loss: 0.00002255
Iteration 146/1000 | Loss: 0.00002255
Iteration 147/1000 | Loss: 0.00002255
Iteration 148/1000 | Loss: 0.00002255
Iteration 149/1000 | Loss: 0.00002255
Iteration 150/1000 | Loss: 0.00002254
Iteration 151/1000 | Loss: 0.00002254
Iteration 152/1000 | Loss: 0.00002254
Iteration 153/1000 | Loss: 0.00002254
Iteration 154/1000 | Loss: 0.00002254
Iteration 155/1000 | Loss: 0.00002254
Iteration 156/1000 | Loss: 0.00002254
Iteration 157/1000 | Loss: 0.00002254
Iteration 158/1000 | Loss: 0.00002254
Iteration 159/1000 | Loss: 0.00002254
Iteration 160/1000 | Loss: 0.00002254
Iteration 161/1000 | Loss: 0.00002253
Iteration 162/1000 | Loss: 0.00002253
Iteration 163/1000 | Loss: 0.00002253
Iteration 164/1000 | Loss: 0.00002253
Iteration 165/1000 | Loss: 0.00002253
Iteration 166/1000 | Loss: 0.00002253
Iteration 167/1000 | Loss: 0.00002253
Iteration 168/1000 | Loss: 0.00002253
Iteration 169/1000 | Loss: 0.00002253
Iteration 170/1000 | Loss: 0.00002253
Iteration 171/1000 | Loss: 0.00002253
Iteration 172/1000 | Loss: 0.00002253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.2532878574565984e-05, 2.2532878574565984e-05, 2.2532878574565984e-05, 2.2532878574565984e-05, 2.2532878574565984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2532878574565984e-05

Optimization complete. Final v2v error: 3.9544677734375 mm

Highest mean error: 4.43051290512085 mm for frame 236

Lowest mean error: 3.4437899589538574 mm for frame 53

Saving results

Total time: 58.58764934539795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913481
Iteration 2/25 | Loss: 0.00202612
Iteration 3/25 | Loss: 0.00131669
Iteration 4/25 | Loss: 0.00121864
Iteration 5/25 | Loss: 0.00121005
Iteration 6/25 | Loss: 0.00115179
Iteration 7/25 | Loss: 0.00113439
Iteration 8/25 | Loss: 0.00111045
Iteration 9/25 | Loss: 0.00110386
Iteration 10/25 | Loss: 0.00109500
Iteration 11/25 | Loss: 0.00108916
Iteration 12/25 | Loss: 0.00108462
Iteration 13/25 | Loss: 0.00108190
Iteration 14/25 | Loss: 0.00108149
Iteration 15/25 | Loss: 0.00108736
Iteration 16/25 | Loss: 0.00109034
Iteration 17/25 | Loss: 0.00108620
Iteration 18/25 | Loss: 0.00109326
Iteration 19/25 | Loss: 0.00108993
Iteration 20/25 | Loss: 0.00108756
Iteration 21/25 | Loss: 0.00108519
Iteration 22/25 | Loss: 0.00109214
Iteration 23/25 | Loss: 0.00108351
Iteration 24/25 | Loss: 0.00108635
Iteration 25/25 | Loss: 0.00108718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64052784
Iteration 2/25 | Loss: 0.00598229
Iteration 3/25 | Loss: 0.00454027
Iteration 4/25 | Loss: 0.00298892
Iteration 5/25 | Loss: 0.00298889
Iteration 6/25 | Loss: 0.00298889
Iteration 7/25 | Loss: 0.00298889
Iteration 8/25 | Loss: 0.00298889
Iteration 9/25 | Loss: 0.00298889
Iteration 10/25 | Loss: 0.00298889
Iteration 11/25 | Loss: 0.00298889
Iteration 12/25 | Loss: 0.00298889
Iteration 13/25 | Loss: 0.00298889
Iteration 14/25 | Loss: 0.00298889
Iteration 15/25 | Loss: 0.00298889
Iteration 16/25 | Loss: 0.00298889
Iteration 17/25 | Loss: 0.00298889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0029888858553022146, 0.0029888858553022146, 0.0029888858553022146, 0.0029888858553022146, 0.0029888858553022146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029888858553022146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298889
Iteration 2/1000 | Loss: 0.00237431
Iteration 3/1000 | Loss: 0.00820680
Iteration 4/1000 | Loss: 0.00152837
Iteration 5/1000 | Loss: 0.00095761
Iteration 6/1000 | Loss: 0.00408004
Iteration 7/1000 | Loss: 0.00116812
Iteration 8/1000 | Loss: 0.00090920
Iteration 9/1000 | Loss: 0.00060978
Iteration 10/1000 | Loss: 0.00149244
Iteration 11/1000 | Loss: 0.00158893
Iteration 12/1000 | Loss: 0.00097376
Iteration 13/1000 | Loss: 0.00532707
Iteration 14/1000 | Loss: 0.00616611
Iteration 15/1000 | Loss: 0.00187744
Iteration 16/1000 | Loss: 0.00118102
Iteration 17/1000 | Loss: 0.00087516
Iteration 18/1000 | Loss: 0.00094361
Iteration 19/1000 | Loss: 0.00096375
Iteration 20/1000 | Loss: 0.00111467
Iteration 21/1000 | Loss: 0.00067286
Iteration 22/1000 | Loss: 0.00295503
Iteration 23/1000 | Loss: 0.00061490
Iteration 24/1000 | Loss: 0.00008831
Iteration 25/1000 | Loss: 0.00021788
Iteration 26/1000 | Loss: 0.00054566
Iteration 27/1000 | Loss: 0.00011881
Iteration 28/1000 | Loss: 0.00011703
Iteration 29/1000 | Loss: 0.00004941
Iteration 30/1000 | Loss: 0.00004324
Iteration 31/1000 | Loss: 0.00003887
Iteration 32/1000 | Loss: 0.00003635
Iteration 33/1000 | Loss: 0.00010669
Iteration 34/1000 | Loss: 0.00003585
Iteration 35/1000 | Loss: 0.00003557
Iteration 36/1000 | Loss: 0.00003226
Iteration 37/1000 | Loss: 0.00003109
Iteration 38/1000 | Loss: 0.00002958
Iteration 39/1000 | Loss: 0.00002813
Iteration 40/1000 | Loss: 0.00002722
Iteration 41/1000 | Loss: 0.00002667
Iteration 42/1000 | Loss: 0.00002637
Iteration 43/1000 | Loss: 0.00002613
Iteration 44/1000 | Loss: 0.00002595
Iteration 45/1000 | Loss: 0.00002578
Iteration 46/1000 | Loss: 0.00002577
Iteration 47/1000 | Loss: 0.00002573
Iteration 48/1000 | Loss: 0.00002571
Iteration 49/1000 | Loss: 0.00002570
Iteration 50/1000 | Loss: 0.00002561
Iteration 51/1000 | Loss: 0.00002558
Iteration 52/1000 | Loss: 0.00002555
Iteration 53/1000 | Loss: 0.00002554
Iteration 54/1000 | Loss: 0.00002553
Iteration 55/1000 | Loss: 0.00002552
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002550
Iteration 58/1000 | Loss: 0.00002547
Iteration 59/1000 | Loss: 0.00002546
Iteration 60/1000 | Loss: 0.00002546
Iteration 61/1000 | Loss: 0.00002545
Iteration 62/1000 | Loss: 0.00002541
Iteration 63/1000 | Loss: 0.00002541
Iteration 64/1000 | Loss: 0.00002541
Iteration 65/1000 | Loss: 0.00002541
Iteration 66/1000 | Loss: 0.00002541
Iteration 67/1000 | Loss: 0.00002540
Iteration 68/1000 | Loss: 0.00002540
Iteration 69/1000 | Loss: 0.00002539
Iteration 70/1000 | Loss: 0.00002539
Iteration 71/1000 | Loss: 0.00002539
Iteration 72/1000 | Loss: 0.00002538
Iteration 73/1000 | Loss: 0.00002537
Iteration 74/1000 | Loss: 0.00002535
Iteration 75/1000 | Loss: 0.00002534
Iteration 76/1000 | Loss: 0.00002533
Iteration 77/1000 | Loss: 0.00002532
Iteration 78/1000 | Loss: 0.00002532
Iteration 79/1000 | Loss: 0.00002532
Iteration 80/1000 | Loss: 0.00002531
Iteration 81/1000 | Loss: 0.00002531
Iteration 82/1000 | Loss: 0.00002530
Iteration 83/1000 | Loss: 0.00002530
Iteration 84/1000 | Loss: 0.00002530
Iteration 85/1000 | Loss: 0.00002529
Iteration 86/1000 | Loss: 0.00002529
Iteration 87/1000 | Loss: 0.00002529
Iteration 88/1000 | Loss: 0.00002528
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002527
Iteration 91/1000 | Loss: 0.00002527
Iteration 92/1000 | Loss: 0.00002526
Iteration 93/1000 | Loss: 0.00002526
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002524
Iteration 96/1000 | Loss: 0.00002524
Iteration 97/1000 | Loss: 0.00002523
Iteration 98/1000 | Loss: 0.00002522
Iteration 99/1000 | Loss: 0.00002521
Iteration 100/1000 | Loss: 0.00002521
Iteration 101/1000 | Loss: 0.00002518
Iteration 102/1000 | Loss: 0.00002514
Iteration 103/1000 | Loss: 0.00002513
Iteration 104/1000 | Loss: 0.00002512
Iteration 105/1000 | Loss: 0.00002512
Iteration 106/1000 | Loss: 0.00002511
Iteration 107/1000 | Loss: 0.00002511
Iteration 108/1000 | Loss: 0.00002511
Iteration 109/1000 | Loss: 0.00002511
Iteration 110/1000 | Loss: 0.00002511
Iteration 111/1000 | Loss: 0.00002511
Iteration 112/1000 | Loss: 0.00002511
Iteration 113/1000 | Loss: 0.00002511
Iteration 114/1000 | Loss: 0.00002510
Iteration 115/1000 | Loss: 0.00002510
Iteration 116/1000 | Loss: 0.00002510
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002509
Iteration 119/1000 | Loss: 0.00002509
Iteration 120/1000 | Loss: 0.00002509
Iteration 121/1000 | Loss: 0.00002509
Iteration 122/1000 | Loss: 0.00002508
Iteration 123/1000 | Loss: 0.00002508
Iteration 124/1000 | Loss: 0.00002507
Iteration 125/1000 | Loss: 0.00002507
Iteration 126/1000 | Loss: 0.00002507
Iteration 127/1000 | Loss: 0.00002506
Iteration 128/1000 | Loss: 0.00002506
Iteration 129/1000 | Loss: 0.00002506
Iteration 130/1000 | Loss: 0.00002506
Iteration 131/1000 | Loss: 0.00002505
Iteration 132/1000 | Loss: 0.00002505
Iteration 133/1000 | Loss: 0.00002505
Iteration 134/1000 | Loss: 0.00002505
Iteration 135/1000 | Loss: 0.00002504
Iteration 136/1000 | Loss: 0.00002504
Iteration 137/1000 | Loss: 0.00002504
Iteration 138/1000 | Loss: 0.00002504
Iteration 139/1000 | Loss: 0.00002504
Iteration 140/1000 | Loss: 0.00002504
Iteration 141/1000 | Loss: 0.00002504
Iteration 142/1000 | Loss: 0.00002504
Iteration 143/1000 | Loss: 0.00002503
Iteration 144/1000 | Loss: 0.00002503
Iteration 145/1000 | Loss: 0.00002503
Iteration 146/1000 | Loss: 0.00002503
Iteration 147/1000 | Loss: 0.00002503
Iteration 148/1000 | Loss: 0.00002503
Iteration 149/1000 | Loss: 0.00002503
Iteration 150/1000 | Loss: 0.00002503
Iteration 151/1000 | Loss: 0.00002502
Iteration 152/1000 | Loss: 0.00002502
Iteration 153/1000 | Loss: 0.00002502
Iteration 154/1000 | Loss: 0.00002501
Iteration 155/1000 | Loss: 0.00002501
Iteration 156/1000 | Loss: 0.00002501
Iteration 157/1000 | Loss: 0.00002501
Iteration 158/1000 | Loss: 0.00002501
Iteration 159/1000 | Loss: 0.00002501
Iteration 160/1000 | Loss: 0.00002501
Iteration 161/1000 | Loss: 0.00002501
Iteration 162/1000 | Loss: 0.00002501
Iteration 163/1000 | Loss: 0.00002501
Iteration 164/1000 | Loss: 0.00002501
Iteration 165/1000 | Loss: 0.00002501
Iteration 166/1000 | Loss: 0.00002501
Iteration 167/1000 | Loss: 0.00002501
Iteration 168/1000 | Loss: 0.00002501
Iteration 169/1000 | Loss: 0.00002501
Iteration 170/1000 | Loss: 0.00002501
Iteration 171/1000 | Loss: 0.00002500
Iteration 172/1000 | Loss: 0.00002500
Iteration 173/1000 | Loss: 0.00002500
Iteration 174/1000 | Loss: 0.00002500
Iteration 175/1000 | Loss: 0.00002500
Iteration 176/1000 | Loss: 0.00002500
Iteration 177/1000 | Loss: 0.00002500
Iteration 178/1000 | Loss: 0.00002500
Iteration 179/1000 | Loss: 0.00002500
Iteration 180/1000 | Loss: 0.00002500
Iteration 181/1000 | Loss: 0.00002500
Iteration 182/1000 | Loss: 0.00002500
Iteration 183/1000 | Loss: 0.00002500
Iteration 184/1000 | Loss: 0.00002500
Iteration 185/1000 | Loss: 0.00002499
Iteration 186/1000 | Loss: 0.00002499
Iteration 187/1000 | Loss: 0.00002499
Iteration 188/1000 | Loss: 0.00002499
Iteration 189/1000 | Loss: 0.00002499
Iteration 190/1000 | Loss: 0.00002499
Iteration 191/1000 | Loss: 0.00002499
Iteration 192/1000 | Loss: 0.00002498
Iteration 193/1000 | Loss: 0.00002498
Iteration 194/1000 | Loss: 0.00002498
Iteration 195/1000 | Loss: 0.00002498
Iteration 196/1000 | Loss: 0.00002498
Iteration 197/1000 | Loss: 0.00002498
Iteration 198/1000 | Loss: 0.00002498
Iteration 199/1000 | Loss: 0.00002497
Iteration 200/1000 | Loss: 0.00002497
Iteration 201/1000 | Loss: 0.00002497
Iteration 202/1000 | Loss: 0.00002497
Iteration 203/1000 | Loss: 0.00002497
Iteration 204/1000 | Loss: 0.00002497
Iteration 205/1000 | Loss: 0.00002497
Iteration 206/1000 | Loss: 0.00002497
Iteration 207/1000 | Loss: 0.00002497
Iteration 208/1000 | Loss: 0.00002497
Iteration 209/1000 | Loss: 0.00002497
Iteration 210/1000 | Loss: 0.00002497
Iteration 211/1000 | Loss: 0.00002497
Iteration 212/1000 | Loss: 0.00002496
Iteration 213/1000 | Loss: 0.00002496
Iteration 214/1000 | Loss: 0.00002496
Iteration 215/1000 | Loss: 0.00002496
Iteration 216/1000 | Loss: 0.00002496
Iteration 217/1000 | Loss: 0.00002496
Iteration 218/1000 | Loss: 0.00002496
Iteration 219/1000 | Loss: 0.00002496
Iteration 220/1000 | Loss: 0.00002496
Iteration 221/1000 | Loss: 0.00002496
Iteration 222/1000 | Loss: 0.00002496
Iteration 223/1000 | Loss: 0.00002496
Iteration 224/1000 | Loss: 0.00002496
Iteration 225/1000 | Loss: 0.00002496
Iteration 226/1000 | Loss: 0.00002496
Iteration 227/1000 | Loss: 0.00002496
Iteration 228/1000 | Loss: 0.00002496
Iteration 229/1000 | Loss: 0.00002496
Iteration 230/1000 | Loss: 0.00002495
Iteration 231/1000 | Loss: 0.00002495
Iteration 232/1000 | Loss: 0.00002495
Iteration 233/1000 | Loss: 0.00002495
Iteration 234/1000 | Loss: 0.00002495
Iteration 235/1000 | Loss: 0.00002495
Iteration 236/1000 | Loss: 0.00002495
Iteration 237/1000 | Loss: 0.00002495
Iteration 238/1000 | Loss: 0.00002495
Iteration 239/1000 | Loss: 0.00002495
Iteration 240/1000 | Loss: 0.00002495
Iteration 241/1000 | Loss: 0.00002495
Iteration 242/1000 | Loss: 0.00002495
Iteration 243/1000 | Loss: 0.00002495
Iteration 244/1000 | Loss: 0.00002495
Iteration 245/1000 | Loss: 0.00002495
Iteration 246/1000 | Loss: 0.00002495
Iteration 247/1000 | Loss: 0.00002495
Iteration 248/1000 | Loss: 0.00002494
Iteration 249/1000 | Loss: 0.00002494
Iteration 250/1000 | Loss: 0.00002494
Iteration 251/1000 | Loss: 0.00002494
Iteration 252/1000 | Loss: 0.00002494
Iteration 253/1000 | Loss: 0.00002494
Iteration 254/1000 | Loss: 0.00002494
Iteration 255/1000 | Loss: 0.00002494
Iteration 256/1000 | Loss: 0.00002494
Iteration 257/1000 | Loss: 0.00002494
Iteration 258/1000 | Loss: 0.00002494
Iteration 259/1000 | Loss: 0.00002494
Iteration 260/1000 | Loss: 0.00002494
Iteration 261/1000 | Loss: 0.00002494
Iteration 262/1000 | Loss: 0.00002494
Iteration 263/1000 | Loss: 0.00002494
Iteration 264/1000 | Loss: 0.00002494
Iteration 265/1000 | Loss: 0.00002494
Iteration 266/1000 | Loss: 0.00002493
Iteration 267/1000 | Loss: 0.00002493
Iteration 268/1000 | Loss: 0.00002493
Iteration 269/1000 | Loss: 0.00002493
Iteration 270/1000 | Loss: 0.00002493
Iteration 271/1000 | Loss: 0.00002493
Iteration 272/1000 | Loss: 0.00002493
Iteration 273/1000 | Loss: 0.00002493
Iteration 274/1000 | Loss: 0.00002493
Iteration 275/1000 | Loss: 0.00002493
Iteration 276/1000 | Loss: 0.00002493
Iteration 277/1000 | Loss: 0.00002493
Iteration 278/1000 | Loss: 0.00002493
Iteration 279/1000 | Loss: 0.00002493
Iteration 280/1000 | Loss: 0.00002493
Iteration 281/1000 | Loss: 0.00002493
Iteration 282/1000 | Loss: 0.00002493
Iteration 283/1000 | Loss: 0.00002493
Iteration 284/1000 | Loss: 0.00002493
Iteration 285/1000 | Loss: 0.00002493
Iteration 286/1000 | Loss: 0.00002493
Iteration 287/1000 | Loss: 0.00002493
Iteration 288/1000 | Loss: 0.00002493
Iteration 289/1000 | Loss: 0.00002493
Iteration 290/1000 | Loss: 0.00002493
Iteration 291/1000 | Loss: 0.00002493
Iteration 292/1000 | Loss: 0.00002493
Iteration 293/1000 | Loss: 0.00002493
Iteration 294/1000 | Loss: 0.00002493
Iteration 295/1000 | Loss: 0.00002493
Iteration 296/1000 | Loss: 0.00002493
Iteration 297/1000 | Loss: 0.00002493
Iteration 298/1000 | Loss: 0.00002493
Iteration 299/1000 | Loss: 0.00002493
Iteration 300/1000 | Loss: 0.00002493
Iteration 301/1000 | Loss: 0.00002493
Iteration 302/1000 | Loss: 0.00002493
Iteration 303/1000 | Loss: 0.00002493
Iteration 304/1000 | Loss: 0.00002493
Iteration 305/1000 | Loss: 0.00002493
Iteration 306/1000 | Loss: 0.00002493
Iteration 307/1000 | Loss: 0.00002493
Iteration 308/1000 | Loss: 0.00002493
Iteration 309/1000 | Loss: 0.00002493
Iteration 310/1000 | Loss: 0.00002493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [2.4929957362473942e-05, 2.4929957362473942e-05, 2.4929957362473942e-05, 2.4929957362473942e-05, 2.4929957362473942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4929957362473942e-05

Optimization complete. Final v2v error: 3.9937515258789062 mm

Highest mean error: 6.239488124847412 mm for frame 139

Lowest mean error: 2.68636155128479 mm for frame 47

Saving results

Total time: 152.03259658813477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384448
Iteration 2/25 | Loss: 0.00111449
Iteration 3/25 | Loss: 0.00102261
Iteration 4/25 | Loss: 0.00100235
Iteration 5/25 | Loss: 0.00099445
Iteration 6/25 | Loss: 0.00099129
Iteration 7/25 | Loss: 0.00099121
Iteration 8/25 | Loss: 0.00099121
Iteration 9/25 | Loss: 0.00099121
Iteration 10/25 | Loss: 0.00099121
Iteration 11/25 | Loss: 0.00099121
Iteration 12/25 | Loss: 0.00099121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000991211156360805, 0.000991211156360805, 0.000991211156360805, 0.000991211156360805, 0.000991211156360805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991211156360805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22928452
Iteration 2/25 | Loss: 0.00079416
Iteration 3/25 | Loss: 0.00079416
Iteration 4/25 | Loss: 0.00079416
Iteration 5/25 | Loss: 0.00079416
Iteration 6/25 | Loss: 0.00079416
Iteration 7/25 | Loss: 0.00079416
Iteration 8/25 | Loss: 0.00079416
Iteration 9/25 | Loss: 0.00079416
Iteration 10/25 | Loss: 0.00079416
Iteration 11/25 | Loss: 0.00079416
Iteration 12/25 | Loss: 0.00079416
Iteration 13/25 | Loss: 0.00079416
Iteration 14/25 | Loss: 0.00079416
Iteration 15/25 | Loss: 0.00079416
Iteration 16/25 | Loss: 0.00079416
Iteration 17/25 | Loss: 0.00079416
Iteration 18/25 | Loss: 0.00079416
Iteration 19/25 | Loss: 0.00079416
Iteration 20/25 | Loss: 0.00079416
Iteration 21/25 | Loss: 0.00079416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007941557560116053, 0.0007941557560116053, 0.0007941557560116053, 0.0007941557560116053, 0.0007941557560116053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007941557560116053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079416
Iteration 2/1000 | Loss: 0.00004398
Iteration 3/1000 | Loss: 0.00002790
Iteration 4/1000 | Loss: 0.00002602
Iteration 5/1000 | Loss: 0.00002494
Iteration 6/1000 | Loss: 0.00002425
Iteration 7/1000 | Loss: 0.00002377
Iteration 8/1000 | Loss: 0.00002347
Iteration 9/1000 | Loss: 0.00002318
Iteration 10/1000 | Loss: 0.00002299
Iteration 11/1000 | Loss: 0.00002277
Iteration 12/1000 | Loss: 0.00002268
Iteration 13/1000 | Loss: 0.00002258
Iteration 14/1000 | Loss: 0.00002258
Iteration 15/1000 | Loss: 0.00002257
Iteration 16/1000 | Loss: 0.00002257
Iteration 17/1000 | Loss: 0.00002254
Iteration 18/1000 | Loss: 0.00002253
Iteration 19/1000 | Loss: 0.00002252
Iteration 20/1000 | Loss: 0.00002252
Iteration 21/1000 | Loss: 0.00002252
Iteration 22/1000 | Loss: 0.00002251
Iteration 23/1000 | Loss: 0.00002248
Iteration 24/1000 | Loss: 0.00002248
Iteration 25/1000 | Loss: 0.00002248
Iteration 26/1000 | Loss: 0.00002247
Iteration 27/1000 | Loss: 0.00002244
Iteration 28/1000 | Loss: 0.00002243
Iteration 29/1000 | Loss: 0.00002242
Iteration 30/1000 | Loss: 0.00002241
Iteration 31/1000 | Loss: 0.00002241
Iteration 32/1000 | Loss: 0.00002241
Iteration 33/1000 | Loss: 0.00002241
Iteration 34/1000 | Loss: 0.00002240
Iteration 35/1000 | Loss: 0.00002240
Iteration 36/1000 | Loss: 0.00002240
Iteration 37/1000 | Loss: 0.00002239
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002239
Iteration 41/1000 | Loss: 0.00002239
Iteration 42/1000 | Loss: 0.00002239
Iteration 43/1000 | Loss: 0.00002239
Iteration 44/1000 | Loss: 0.00002239
Iteration 45/1000 | Loss: 0.00002238
Iteration 46/1000 | Loss: 0.00002238
Iteration 47/1000 | Loss: 0.00002237
Iteration 48/1000 | Loss: 0.00002237
Iteration 49/1000 | Loss: 0.00002237
Iteration 50/1000 | Loss: 0.00002237
Iteration 51/1000 | Loss: 0.00002237
Iteration 52/1000 | Loss: 0.00002237
Iteration 53/1000 | Loss: 0.00002237
Iteration 54/1000 | Loss: 0.00002237
Iteration 55/1000 | Loss: 0.00002237
Iteration 56/1000 | Loss: 0.00002236
Iteration 57/1000 | Loss: 0.00002236
Iteration 58/1000 | Loss: 0.00002236
Iteration 59/1000 | Loss: 0.00002236
Iteration 60/1000 | Loss: 0.00002236
Iteration 61/1000 | Loss: 0.00002236
Iteration 62/1000 | Loss: 0.00002236
Iteration 63/1000 | Loss: 0.00002236
Iteration 64/1000 | Loss: 0.00002235
Iteration 65/1000 | Loss: 0.00002235
Iteration 66/1000 | Loss: 0.00002235
Iteration 67/1000 | Loss: 0.00002234
Iteration 68/1000 | Loss: 0.00002234
Iteration 69/1000 | Loss: 0.00002234
Iteration 70/1000 | Loss: 0.00002234
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00002233
Iteration 73/1000 | Loss: 0.00002233
Iteration 74/1000 | Loss: 0.00002233
Iteration 75/1000 | Loss: 0.00002233
Iteration 76/1000 | Loss: 0.00002233
Iteration 77/1000 | Loss: 0.00002233
Iteration 78/1000 | Loss: 0.00002233
Iteration 79/1000 | Loss: 0.00002233
Iteration 80/1000 | Loss: 0.00002233
Iteration 81/1000 | Loss: 0.00002233
Iteration 82/1000 | Loss: 0.00002233
Iteration 83/1000 | Loss: 0.00002233
Iteration 84/1000 | Loss: 0.00002233
Iteration 85/1000 | Loss: 0.00002233
Iteration 86/1000 | Loss: 0.00002233
Iteration 87/1000 | Loss: 0.00002233
Iteration 88/1000 | Loss: 0.00002232
Iteration 89/1000 | Loss: 0.00002232
Iteration 90/1000 | Loss: 0.00002232
Iteration 91/1000 | Loss: 0.00002232
Iteration 92/1000 | Loss: 0.00002232
Iteration 93/1000 | Loss: 0.00002232
Iteration 94/1000 | Loss: 0.00002232
Iteration 95/1000 | Loss: 0.00002232
Iteration 96/1000 | Loss: 0.00002231
Iteration 97/1000 | Loss: 0.00002231
Iteration 98/1000 | Loss: 0.00002231
Iteration 99/1000 | Loss: 0.00002231
Iteration 100/1000 | Loss: 0.00002231
Iteration 101/1000 | Loss: 0.00002231
Iteration 102/1000 | Loss: 0.00002231
Iteration 103/1000 | Loss: 0.00002231
Iteration 104/1000 | Loss: 0.00002230
Iteration 105/1000 | Loss: 0.00002230
Iteration 106/1000 | Loss: 0.00002230
Iteration 107/1000 | Loss: 0.00002230
Iteration 108/1000 | Loss: 0.00002230
Iteration 109/1000 | Loss: 0.00002230
Iteration 110/1000 | Loss: 0.00002230
Iteration 111/1000 | Loss: 0.00002230
Iteration 112/1000 | Loss: 0.00002230
Iteration 113/1000 | Loss: 0.00002230
Iteration 114/1000 | Loss: 0.00002230
Iteration 115/1000 | Loss: 0.00002229
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002229
Iteration 118/1000 | Loss: 0.00002229
Iteration 119/1000 | Loss: 0.00002229
Iteration 120/1000 | Loss: 0.00002229
Iteration 121/1000 | Loss: 0.00002229
Iteration 122/1000 | Loss: 0.00002229
Iteration 123/1000 | Loss: 0.00002229
Iteration 124/1000 | Loss: 0.00002228
Iteration 125/1000 | Loss: 0.00002228
Iteration 126/1000 | Loss: 0.00002228
Iteration 127/1000 | Loss: 0.00002228
Iteration 128/1000 | Loss: 0.00002228
Iteration 129/1000 | Loss: 0.00002227
Iteration 130/1000 | Loss: 0.00002227
Iteration 131/1000 | Loss: 0.00002227
Iteration 132/1000 | Loss: 0.00002227
Iteration 133/1000 | Loss: 0.00002227
Iteration 134/1000 | Loss: 0.00002226
Iteration 135/1000 | Loss: 0.00002226
Iteration 136/1000 | Loss: 0.00002226
Iteration 137/1000 | Loss: 0.00002226
Iteration 138/1000 | Loss: 0.00002226
Iteration 139/1000 | Loss: 0.00002226
Iteration 140/1000 | Loss: 0.00002226
Iteration 141/1000 | Loss: 0.00002226
Iteration 142/1000 | Loss: 0.00002226
Iteration 143/1000 | Loss: 0.00002225
Iteration 144/1000 | Loss: 0.00002225
Iteration 145/1000 | Loss: 0.00002225
Iteration 146/1000 | Loss: 0.00002225
Iteration 147/1000 | Loss: 0.00002225
Iteration 148/1000 | Loss: 0.00002225
Iteration 149/1000 | Loss: 0.00002225
Iteration 150/1000 | Loss: 0.00002225
Iteration 151/1000 | Loss: 0.00002225
Iteration 152/1000 | Loss: 0.00002225
Iteration 153/1000 | Loss: 0.00002225
Iteration 154/1000 | Loss: 0.00002224
Iteration 155/1000 | Loss: 0.00002224
Iteration 156/1000 | Loss: 0.00002224
Iteration 157/1000 | Loss: 0.00002224
Iteration 158/1000 | Loss: 0.00002224
Iteration 159/1000 | Loss: 0.00002224
Iteration 160/1000 | Loss: 0.00002224
Iteration 161/1000 | Loss: 0.00002224
Iteration 162/1000 | Loss: 0.00002224
Iteration 163/1000 | Loss: 0.00002224
Iteration 164/1000 | Loss: 0.00002224
Iteration 165/1000 | Loss: 0.00002224
Iteration 166/1000 | Loss: 0.00002224
Iteration 167/1000 | Loss: 0.00002224
Iteration 168/1000 | Loss: 0.00002224
Iteration 169/1000 | Loss: 0.00002224
Iteration 170/1000 | Loss: 0.00002224
Iteration 171/1000 | Loss: 0.00002224
Iteration 172/1000 | Loss: 0.00002224
Iteration 173/1000 | Loss: 0.00002223
Iteration 174/1000 | Loss: 0.00002223
Iteration 175/1000 | Loss: 0.00002223
Iteration 176/1000 | Loss: 0.00002223
Iteration 177/1000 | Loss: 0.00002223
Iteration 178/1000 | Loss: 0.00002223
Iteration 179/1000 | Loss: 0.00002223
Iteration 180/1000 | Loss: 0.00002223
Iteration 181/1000 | Loss: 0.00002223
Iteration 182/1000 | Loss: 0.00002223
Iteration 183/1000 | Loss: 0.00002223
Iteration 184/1000 | Loss: 0.00002222
Iteration 185/1000 | Loss: 0.00002222
Iteration 186/1000 | Loss: 0.00002222
Iteration 187/1000 | Loss: 0.00002222
Iteration 188/1000 | Loss: 0.00002222
Iteration 189/1000 | Loss: 0.00002222
Iteration 190/1000 | Loss: 0.00002222
Iteration 191/1000 | Loss: 0.00002222
Iteration 192/1000 | Loss: 0.00002222
Iteration 193/1000 | Loss: 0.00002222
Iteration 194/1000 | Loss: 0.00002222
Iteration 195/1000 | Loss: 0.00002222
Iteration 196/1000 | Loss: 0.00002222
Iteration 197/1000 | Loss: 0.00002222
Iteration 198/1000 | Loss: 0.00002222
Iteration 199/1000 | Loss: 0.00002222
Iteration 200/1000 | Loss: 0.00002222
Iteration 201/1000 | Loss: 0.00002222
Iteration 202/1000 | Loss: 0.00002221
Iteration 203/1000 | Loss: 0.00002221
Iteration 204/1000 | Loss: 0.00002221
Iteration 205/1000 | Loss: 0.00002221
Iteration 206/1000 | Loss: 0.00002221
Iteration 207/1000 | Loss: 0.00002221
Iteration 208/1000 | Loss: 0.00002221
Iteration 209/1000 | Loss: 0.00002220
Iteration 210/1000 | Loss: 0.00002220
Iteration 211/1000 | Loss: 0.00002220
Iteration 212/1000 | Loss: 0.00002220
Iteration 213/1000 | Loss: 0.00002220
Iteration 214/1000 | Loss: 0.00002220
Iteration 215/1000 | Loss: 0.00002220
Iteration 216/1000 | Loss: 0.00002220
Iteration 217/1000 | Loss: 0.00002220
Iteration 218/1000 | Loss: 0.00002220
Iteration 219/1000 | Loss: 0.00002220
Iteration 220/1000 | Loss: 0.00002220
Iteration 221/1000 | Loss: 0.00002219
Iteration 222/1000 | Loss: 0.00002219
Iteration 223/1000 | Loss: 0.00002219
Iteration 224/1000 | Loss: 0.00002219
Iteration 225/1000 | Loss: 0.00002219
Iteration 226/1000 | Loss: 0.00002219
Iteration 227/1000 | Loss: 0.00002219
Iteration 228/1000 | Loss: 0.00002219
Iteration 229/1000 | Loss: 0.00002219
Iteration 230/1000 | Loss: 0.00002219
Iteration 231/1000 | Loss: 0.00002218
Iteration 232/1000 | Loss: 0.00002218
Iteration 233/1000 | Loss: 0.00002218
Iteration 234/1000 | Loss: 0.00002218
Iteration 235/1000 | Loss: 0.00002218
Iteration 236/1000 | Loss: 0.00002218
Iteration 237/1000 | Loss: 0.00002218
Iteration 238/1000 | Loss: 0.00002218
Iteration 239/1000 | Loss: 0.00002218
Iteration 240/1000 | Loss: 0.00002218
Iteration 241/1000 | Loss: 0.00002218
Iteration 242/1000 | Loss: 0.00002218
Iteration 243/1000 | Loss: 0.00002218
Iteration 244/1000 | Loss: 0.00002218
Iteration 245/1000 | Loss: 0.00002218
Iteration 246/1000 | Loss: 0.00002218
Iteration 247/1000 | Loss: 0.00002218
Iteration 248/1000 | Loss: 0.00002218
Iteration 249/1000 | Loss: 0.00002218
Iteration 250/1000 | Loss: 0.00002218
Iteration 251/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.2183137843967415e-05, 2.2183137843967415e-05, 2.2183137843967415e-05, 2.2183137843967415e-05, 2.2183137843967415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2183137843967415e-05

Optimization complete. Final v2v error: 3.9850552082061768 mm

Highest mean error: 4.3426313400268555 mm for frame 111

Lowest mean error: 3.2717466354370117 mm for frame 0

Saving results

Total time: 46.35344433784485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_nl_5445/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_nl_5445/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755126
Iteration 2/25 | Loss: 0.00142285
Iteration 3/25 | Loss: 0.00116914
Iteration 4/25 | Loss: 0.00113457
Iteration 5/25 | Loss: 0.00112959
Iteration 6/25 | Loss: 0.00112075
Iteration 7/25 | Loss: 0.00110889
Iteration 8/25 | Loss: 0.00110491
Iteration 9/25 | Loss: 0.00110219
Iteration 10/25 | Loss: 0.00110016
Iteration 11/25 | Loss: 0.00109979
Iteration 12/25 | Loss: 0.00109941
Iteration 13/25 | Loss: 0.00109919
Iteration 14/25 | Loss: 0.00109900
Iteration 15/25 | Loss: 0.00109897
Iteration 16/25 | Loss: 0.00109897
Iteration 17/25 | Loss: 0.00109897
Iteration 18/25 | Loss: 0.00109896
Iteration 19/25 | Loss: 0.00109896
Iteration 20/25 | Loss: 0.00109896
Iteration 21/25 | Loss: 0.00109896
Iteration 22/25 | Loss: 0.00109896
Iteration 23/25 | Loss: 0.00109896
Iteration 24/25 | Loss: 0.00109896
Iteration 25/25 | Loss: 0.00109896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42508769
Iteration 2/25 | Loss: 0.00147829
Iteration 3/25 | Loss: 0.00147824
Iteration 4/25 | Loss: 0.00147824
Iteration 5/25 | Loss: 0.00147824
Iteration 6/25 | Loss: 0.00147823
Iteration 7/25 | Loss: 0.00147823
Iteration 8/25 | Loss: 0.00147823
Iteration 9/25 | Loss: 0.00147823
Iteration 10/25 | Loss: 0.00147823
Iteration 11/25 | Loss: 0.00147823
Iteration 12/25 | Loss: 0.00147823
Iteration 13/25 | Loss: 0.00147823
Iteration 14/25 | Loss: 0.00147823
Iteration 15/25 | Loss: 0.00147823
Iteration 16/25 | Loss: 0.00147823
Iteration 17/25 | Loss: 0.00147823
Iteration 18/25 | Loss: 0.00147823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014782330254092813, 0.0014782330254092813, 0.0014782330254092813, 0.0014782330254092813, 0.0014782330254092813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014782330254092813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147823
Iteration 2/1000 | Loss: 0.00018347
Iteration 3/1000 | Loss: 0.00033152
Iteration 4/1000 | Loss: 0.00015540
Iteration 5/1000 | Loss: 0.00004589
Iteration 6/1000 | Loss: 0.00003368
Iteration 7/1000 | Loss: 0.00002745
Iteration 8/1000 | Loss: 0.00002496
Iteration 9/1000 | Loss: 0.00002371
Iteration 10/1000 | Loss: 0.00002305
Iteration 11/1000 | Loss: 0.00002261
Iteration 12/1000 | Loss: 0.00002232
Iteration 13/1000 | Loss: 0.00002201
Iteration 14/1000 | Loss: 0.00002168
Iteration 15/1000 | Loss: 0.00002148
Iteration 16/1000 | Loss: 0.00002136
Iteration 17/1000 | Loss: 0.00002134
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002120
Iteration 20/1000 | Loss: 0.00002109
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002105
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002100
Iteration 25/1000 | Loss: 0.00002099
Iteration 26/1000 | Loss: 0.00002098
Iteration 27/1000 | Loss: 0.00002092
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002079
Iteration 33/1000 | Loss: 0.00002078
Iteration 34/1000 | Loss: 0.00002078
Iteration 35/1000 | Loss: 0.00002075
Iteration 36/1000 | Loss: 0.00002071
Iteration 37/1000 | Loss: 0.00002070
Iteration 38/1000 | Loss: 0.00002069
Iteration 39/1000 | Loss: 0.00002069
Iteration 40/1000 | Loss: 0.00002065
Iteration 41/1000 | Loss: 0.00002063
Iteration 42/1000 | Loss: 0.00002063
Iteration 43/1000 | Loss: 0.00002062
Iteration 44/1000 | Loss: 0.00002062
Iteration 45/1000 | Loss: 0.00002061
Iteration 46/1000 | Loss: 0.00002061
Iteration 47/1000 | Loss: 0.00002060
Iteration 48/1000 | Loss: 0.00002060
Iteration 49/1000 | Loss: 0.00002059
Iteration 50/1000 | Loss: 0.00002059
Iteration 51/1000 | Loss: 0.00002058
Iteration 52/1000 | Loss: 0.00002058
Iteration 53/1000 | Loss: 0.00002058
Iteration 54/1000 | Loss: 0.00002058
Iteration 55/1000 | Loss: 0.00002057
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002055
Iteration 62/1000 | Loss: 0.00002055
Iteration 63/1000 | Loss: 0.00002054
Iteration 64/1000 | Loss: 0.00002054
Iteration 65/1000 | Loss: 0.00002054
Iteration 66/1000 | Loss: 0.00002054
Iteration 67/1000 | Loss: 0.00002053
Iteration 68/1000 | Loss: 0.00002053
Iteration 69/1000 | Loss: 0.00002053
Iteration 70/1000 | Loss: 0.00002053
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00002052
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002050
Iteration 79/1000 | Loss: 0.00002050
Iteration 80/1000 | Loss: 0.00002050
Iteration 81/1000 | Loss: 0.00002050
Iteration 82/1000 | Loss: 0.00002049
Iteration 83/1000 | Loss: 0.00002049
Iteration 84/1000 | Loss: 0.00002049
Iteration 85/1000 | Loss: 0.00002049
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002048
Iteration 90/1000 | Loss: 0.00002048
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002047
Iteration 96/1000 | Loss: 0.00002047
Iteration 97/1000 | Loss: 0.00002047
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002046
Iteration 102/1000 | Loss: 0.00002046
Iteration 103/1000 | Loss: 0.00002046
Iteration 104/1000 | Loss: 0.00002046
Iteration 105/1000 | Loss: 0.00002046
Iteration 106/1000 | Loss: 0.00002045
Iteration 107/1000 | Loss: 0.00002045
Iteration 108/1000 | Loss: 0.00002045
Iteration 109/1000 | Loss: 0.00002045
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00002044
Iteration 113/1000 | Loss: 0.00002044
Iteration 114/1000 | Loss: 0.00002044
Iteration 115/1000 | Loss: 0.00002043
Iteration 116/1000 | Loss: 0.00002043
Iteration 117/1000 | Loss: 0.00002043
Iteration 118/1000 | Loss: 0.00002043
Iteration 119/1000 | Loss: 0.00002042
Iteration 120/1000 | Loss: 0.00002042
Iteration 121/1000 | Loss: 0.00002042
Iteration 122/1000 | Loss: 0.00002042
Iteration 123/1000 | Loss: 0.00002042
Iteration 124/1000 | Loss: 0.00002042
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002041
Iteration 127/1000 | Loss: 0.00002041
Iteration 128/1000 | Loss: 0.00002041
Iteration 129/1000 | Loss: 0.00002041
Iteration 130/1000 | Loss: 0.00002041
Iteration 131/1000 | Loss: 0.00002041
Iteration 132/1000 | Loss: 0.00002040
Iteration 133/1000 | Loss: 0.00002040
Iteration 134/1000 | Loss: 0.00002040
Iteration 135/1000 | Loss: 0.00002040
Iteration 136/1000 | Loss: 0.00002040
Iteration 137/1000 | Loss: 0.00002040
Iteration 138/1000 | Loss: 0.00002040
Iteration 139/1000 | Loss: 0.00002039
Iteration 140/1000 | Loss: 0.00002039
Iteration 141/1000 | Loss: 0.00002039
Iteration 142/1000 | Loss: 0.00002039
Iteration 143/1000 | Loss: 0.00002039
Iteration 144/1000 | Loss: 0.00002039
Iteration 145/1000 | Loss: 0.00002039
Iteration 146/1000 | Loss: 0.00002039
Iteration 147/1000 | Loss: 0.00002039
Iteration 148/1000 | Loss: 0.00002039
Iteration 149/1000 | Loss: 0.00002039
Iteration 150/1000 | Loss: 0.00002039
Iteration 151/1000 | Loss: 0.00002039
Iteration 152/1000 | Loss: 0.00002039
Iteration 153/1000 | Loss: 0.00002039
Iteration 154/1000 | Loss: 0.00002039
Iteration 155/1000 | Loss: 0.00002039
Iteration 156/1000 | Loss: 0.00002039
Iteration 157/1000 | Loss: 0.00002038
Iteration 158/1000 | Loss: 0.00002038
Iteration 159/1000 | Loss: 0.00002038
Iteration 160/1000 | Loss: 0.00002038
Iteration 161/1000 | Loss: 0.00002038
Iteration 162/1000 | Loss: 0.00002038
Iteration 163/1000 | Loss: 0.00002038
Iteration 164/1000 | Loss: 0.00002038
Iteration 165/1000 | Loss: 0.00002038
Iteration 166/1000 | Loss: 0.00002038
Iteration 167/1000 | Loss: 0.00002038
Iteration 168/1000 | Loss: 0.00002038
Iteration 169/1000 | Loss: 0.00002038
Iteration 170/1000 | Loss: 0.00002038
Iteration 171/1000 | Loss: 0.00002038
Iteration 172/1000 | Loss: 0.00002038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.03838681045454e-05, 2.03838681045454e-05, 2.03838681045454e-05, 2.03838681045454e-05, 2.03838681045454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.03838681045454e-05

Optimization complete. Final v2v error: 3.63913893699646 mm

Highest mean error: 6.488275527954102 mm for frame 103

Lowest mean error: 2.5338611602783203 mm for frame 203

Saving results

Total time: 74.3397479057312
