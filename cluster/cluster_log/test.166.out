Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=166, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9296-9351
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795149
Iteration 2/25 | Loss: 0.00214432
Iteration 3/25 | Loss: 0.00170326
Iteration 4/25 | Loss: 0.00156943
Iteration 5/25 | Loss: 0.00154964
Iteration 6/25 | Loss: 0.00146273
Iteration 7/25 | Loss: 0.00145284
Iteration 8/25 | Loss: 0.00145604
Iteration 9/25 | Loss: 0.00144928
Iteration 10/25 | Loss: 0.00144869
Iteration 11/25 | Loss: 0.00144863
Iteration 12/25 | Loss: 0.00144863
Iteration 13/25 | Loss: 0.00144863
Iteration 14/25 | Loss: 0.00144862
Iteration 15/25 | Loss: 0.00144862
Iteration 16/25 | Loss: 0.00144862
Iteration 17/25 | Loss: 0.00144862
Iteration 18/25 | Loss: 0.00144862
Iteration 19/25 | Loss: 0.00144862
Iteration 20/25 | Loss: 0.00144862
Iteration 21/25 | Loss: 0.00144862
Iteration 22/25 | Loss: 0.00144862
Iteration 23/25 | Loss: 0.00144862
Iteration 24/25 | Loss: 0.00144862
Iteration 25/25 | Loss: 0.00144862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17042553
Iteration 2/25 | Loss: 0.00152355
Iteration 3/25 | Loss: 0.00152353
Iteration 4/25 | Loss: 0.00152353
Iteration 5/25 | Loss: 0.00152353
Iteration 6/25 | Loss: 0.00152353
Iteration 7/25 | Loss: 0.00152353
Iteration 8/25 | Loss: 0.00152353
Iteration 9/25 | Loss: 0.00152353
Iteration 10/25 | Loss: 0.00152353
Iteration 11/25 | Loss: 0.00152353
Iteration 12/25 | Loss: 0.00152353
Iteration 13/25 | Loss: 0.00152353
Iteration 14/25 | Loss: 0.00152353
Iteration 15/25 | Loss: 0.00152353
Iteration 16/25 | Loss: 0.00152353
Iteration 17/25 | Loss: 0.00152353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001523526618257165, 0.001523526618257165, 0.001523526618257165, 0.001523526618257165, 0.001523526618257165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001523526618257165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152353
Iteration 2/1000 | Loss: 0.00004159
Iteration 3/1000 | Loss: 0.00002503
Iteration 4/1000 | Loss: 0.00002123
Iteration 5/1000 | Loss: 0.00002013
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001883
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001809
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001765
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001757
Iteration 19/1000 | Loss: 0.00001756
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001728
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001721
Iteration 49/1000 | Loss: 0.00001721
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001718
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001718
Iteration 63/1000 | Loss: 0.00001718
Iteration 64/1000 | Loss: 0.00001718
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001716
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001715
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001713
Iteration 88/1000 | Loss: 0.00001713
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001713
Iteration 93/1000 | Loss: 0.00001713
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001713
Iteration 97/1000 | Loss: 0.00001713
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001713
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001711
Iteration 112/1000 | Loss: 0.00001711
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001710
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001710
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001709
Iteration 121/1000 | Loss: 0.00001709
Iteration 122/1000 | Loss: 0.00001709
Iteration 123/1000 | Loss: 0.00001709
Iteration 124/1000 | Loss: 0.00001709
Iteration 125/1000 | Loss: 0.00001709
Iteration 126/1000 | Loss: 0.00001708
Iteration 127/1000 | Loss: 0.00001708
Iteration 128/1000 | Loss: 0.00001708
Iteration 129/1000 | Loss: 0.00001708
Iteration 130/1000 | Loss: 0.00001708
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001707
Iteration 137/1000 | Loss: 0.00001707
Iteration 138/1000 | Loss: 0.00001707
Iteration 139/1000 | Loss: 0.00001707
Iteration 140/1000 | Loss: 0.00001707
Iteration 141/1000 | Loss: 0.00001707
Iteration 142/1000 | Loss: 0.00001707
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001706
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00001706
Iteration 147/1000 | Loss: 0.00001706
Iteration 148/1000 | Loss: 0.00001706
Iteration 149/1000 | Loss: 0.00001706
Iteration 150/1000 | Loss: 0.00001706
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001706
Iteration 155/1000 | Loss: 0.00001706
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001705
Iteration 160/1000 | Loss: 0.00001705
Iteration 161/1000 | Loss: 0.00001705
Iteration 162/1000 | Loss: 0.00001705
Iteration 163/1000 | Loss: 0.00001705
Iteration 164/1000 | Loss: 0.00001705
Iteration 165/1000 | Loss: 0.00001704
Iteration 166/1000 | Loss: 0.00001704
Iteration 167/1000 | Loss: 0.00001704
Iteration 168/1000 | Loss: 0.00001704
Iteration 169/1000 | Loss: 0.00001704
Iteration 170/1000 | Loss: 0.00001704
Iteration 171/1000 | Loss: 0.00001704
Iteration 172/1000 | Loss: 0.00001704
Iteration 173/1000 | Loss: 0.00001704
Iteration 174/1000 | Loss: 0.00001704
Iteration 175/1000 | Loss: 0.00001704
Iteration 176/1000 | Loss: 0.00001704
Iteration 177/1000 | Loss: 0.00001704
Iteration 178/1000 | Loss: 0.00001704
Iteration 179/1000 | Loss: 0.00001704
Iteration 180/1000 | Loss: 0.00001704
Iteration 181/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.7040109014487825e-05, 1.7040109014487825e-05, 1.7040109014487825e-05, 1.7040109014487825e-05, 1.7040109014487825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7040109014487825e-05

Optimization complete. Final v2v error: 3.475600242614746 mm

Highest mean error: 3.7566497325897217 mm for frame 8

Lowest mean error: 3.3588321208953857 mm for frame 67

Saving results

Total time: 51.12350916862488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604979
Iteration 2/25 | Loss: 0.00139118
Iteration 3/25 | Loss: 0.00133021
Iteration 4/25 | Loss: 0.00132174
Iteration 5/25 | Loss: 0.00132057
Iteration 6/25 | Loss: 0.00132057
Iteration 7/25 | Loss: 0.00132057
Iteration 8/25 | Loss: 0.00132057
Iteration 9/25 | Loss: 0.00132057
Iteration 10/25 | Loss: 0.00132057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013205731520429254, 0.0013205731520429254, 0.0013205731520429254, 0.0013205731520429254, 0.0013205731520429254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013205731520429254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.60066891
Iteration 2/25 | Loss: 0.00199454
Iteration 3/25 | Loss: 0.00199451
Iteration 4/25 | Loss: 0.00199451
Iteration 5/25 | Loss: 0.00199451
Iteration 6/25 | Loss: 0.00199451
Iteration 7/25 | Loss: 0.00199451
Iteration 8/25 | Loss: 0.00199451
Iteration 9/25 | Loss: 0.00199451
Iteration 10/25 | Loss: 0.00199451
Iteration 11/25 | Loss: 0.00199451
Iteration 12/25 | Loss: 0.00199451
Iteration 13/25 | Loss: 0.00199451
Iteration 14/25 | Loss: 0.00199451
Iteration 15/25 | Loss: 0.00199451
Iteration 16/25 | Loss: 0.00199451
Iteration 17/25 | Loss: 0.00199451
Iteration 18/25 | Loss: 0.00199451
Iteration 19/25 | Loss: 0.00199451
Iteration 20/25 | Loss: 0.00199451
Iteration 21/25 | Loss: 0.00199451
Iteration 22/25 | Loss: 0.00199451
Iteration 23/25 | Loss: 0.00199451
Iteration 24/25 | Loss: 0.00199451
Iteration 25/25 | Loss: 0.00199451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199451
Iteration 2/1000 | Loss: 0.00002063
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00001250
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001151
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001137
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001133
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001125
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001119
Iteration 26/1000 | Loss: 0.00001117
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001112
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001110
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001104
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001102
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001100
Iteration 44/1000 | Loss: 0.00001100
Iteration 45/1000 | Loss: 0.00001100
Iteration 46/1000 | Loss: 0.00001099
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001096
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001095
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001095
Iteration 59/1000 | Loss: 0.00001094
Iteration 60/1000 | Loss: 0.00001094
Iteration 61/1000 | Loss: 0.00001094
Iteration 62/1000 | Loss: 0.00001093
Iteration 63/1000 | Loss: 0.00001092
Iteration 64/1000 | Loss: 0.00001091
Iteration 65/1000 | Loss: 0.00001090
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001090
Iteration 70/1000 | Loss: 0.00001090
Iteration 71/1000 | Loss: 0.00001090
Iteration 72/1000 | Loss: 0.00001090
Iteration 73/1000 | Loss: 0.00001090
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001086
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001085
Iteration 84/1000 | Loss: 0.00001084
Iteration 85/1000 | Loss: 0.00001083
Iteration 86/1000 | Loss: 0.00001083
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001080
Iteration 100/1000 | Loss: 0.00001080
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001079
Iteration 103/1000 | Loss: 0.00001079
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001079
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001078
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001078
Iteration 114/1000 | Loss: 0.00001078
Iteration 115/1000 | Loss: 0.00001078
Iteration 116/1000 | Loss: 0.00001078
Iteration 117/1000 | Loss: 0.00001078
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001077
Iteration 121/1000 | Loss: 0.00001077
Iteration 122/1000 | Loss: 0.00001077
Iteration 123/1000 | Loss: 0.00001077
Iteration 124/1000 | Loss: 0.00001077
Iteration 125/1000 | Loss: 0.00001077
Iteration 126/1000 | Loss: 0.00001077
Iteration 127/1000 | Loss: 0.00001077
Iteration 128/1000 | Loss: 0.00001077
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001076
Iteration 132/1000 | Loss: 0.00001076
Iteration 133/1000 | Loss: 0.00001076
Iteration 134/1000 | Loss: 0.00001076
Iteration 135/1000 | Loss: 0.00001076
Iteration 136/1000 | Loss: 0.00001076
Iteration 137/1000 | Loss: 0.00001076
Iteration 138/1000 | Loss: 0.00001076
Iteration 139/1000 | Loss: 0.00001076
Iteration 140/1000 | Loss: 0.00001076
Iteration 141/1000 | Loss: 0.00001076
Iteration 142/1000 | Loss: 0.00001076
Iteration 143/1000 | Loss: 0.00001076
Iteration 144/1000 | Loss: 0.00001076
Iteration 145/1000 | Loss: 0.00001076
Iteration 146/1000 | Loss: 0.00001076
Iteration 147/1000 | Loss: 0.00001076
Iteration 148/1000 | Loss: 0.00001076
Iteration 149/1000 | Loss: 0.00001076
Iteration 150/1000 | Loss: 0.00001076
Iteration 151/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.076301032298943e-05, 1.076301032298943e-05, 1.076301032298943e-05, 1.076301032298943e-05, 1.076301032298943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.076301032298943e-05

Optimization complete. Final v2v error: 2.8383216857910156 mm

Highest mean error: 3.0462887287139893 mm for frame 117

Lowest mean error: 2.670022964477539 mm for frame 18

Saving results

Total time: 45.3731427192688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942589
Iteration 2/25 | Loss: 0.00183045
Iteration 3/25 | Loss: 0.00158880
Iteration 4/25 | Loss: 0.00156704
Iteration 5/25 | Loss: 0.00156476
Iteration 6/25 | Loss: 0.00156476
Iteration 7/25 | Loss: 0.00156476
Iteration 8/25 | Loss: 0.00156476
Iteration 9/25 | Loss: 0.00156476
Iteration 10/25 | Loss: 0.00156476
Iteration 11/25 | Loss: 0.00156476
Iteration 12/25 | Loss: 0.00156476
Iteration 13/25 | Loss: 0.00156476
Iteration 14/25 | Loss: 0.00156476
Iteration 15/25 | Loss: 0.00156476
Iteration 16/25 | Loss: 0.00156476
Iteration 17/25 | Loss: 0.00156476
Iteration 18/25 | Loss: 0.00156476
Iteration 19/25 | Loss: 0.00156476
Iteration 20/25 | Loss: 0.00156476
Iteration 21/25 | Loss: 0.00156476
Iteration 22/25 | Loss: 0.00156476
Iteration 23/25 | Loss: 0.00156476
Iteration 24/25 | Loss: 0.00156476
Iteration 25/25 | Loss: 0.00156476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39944059
Iteration 2/25 | Loss: 0.00127342
Iteration 3/25 | Loss: 0.00127342
Iteration 4/25 | Loss: 0.00127342
Iteration 5/25 | Loss: 0.00127342
Iteration 6/25 | Loss: 0.00127342
Iteration 7/25 | Loss: 0.00127342
Iteration 8/25 | Loss: 0.00127342
Iteration 9/25 | Loss: 0.00127342
Iteration 10/25 | Loss: 0.00127342
Iteration 11/25 | Loss: 0.00127342
Iteration 12/25 | Loss: 0.00127342
Iteration 13/25 | Loss: 0.00127342
Iteration 14/25 | Loss: 0.00127342
Iteration 15/25 | Loss: 0.00127342
Iteration 16/25 | Loss: 0.00127342
Iteration 17/25 | Loss: 0.00127342
Iteration 18/25 | Loss: 0.00127342
Iteration 19/25 | Loss: 0.00127342
Iteration 20/25 | Loss: 0.00127342
Iteration 21/25 | Loss: 0.00127342
Iteration 22/25 | Loss: 0.00127342
Iteration 23/25 | Loss: 0.00127342
Iteration 24/25 | Loss: 0.00127342
Iteration 25/25 | Loss: 0.00127342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127342
Iteration 2/1000 | Loss: 0.00005187
Iteration 3/1000 | Loss: 0.00003785
Iteration 4/1000 | Loss: 0.00003416
Iteration 5/1000 | Loss: 0.00003334
Iteration 6/1000 | Loss: 0.00003268
Iteration 7/1000 | Loss: 0.00003211
Iteration 8/1000 | Loss: 0.00003168
Iteration 9/1000 | Loss: 0.00003135
Iteration 10/1000 | Loss: 0.00003097
Iteration 11/1000 | Loss: 0.00003068
Iteration 12/1000 | Loss: 0.00003041
Iteration 13/1000 | Loss: 0.00003015
Iteration 14/1000 | Loss: 0.00002992
Iteration 15/1000 | Loss: 0.00002973
Iteration 16/1000 | Loss: 0.00002961
Iteration 17/1000 | Loss: 0.00002960
Iteration 18/1000 | Loss: 0.00002946
Iteration 19/1000 | Loss: 0.00002945
Iteration 20/1000 | Loss: 0.00002931
Iteration 21/1000 | Loss: 0.00002913
Iteration 22/1000 | Loss: 0.00002907
Iteration 23/1000 | Loss: 0.00002906
Iteration 24/1000 | Loss: 0.00002905
Iteration 25/1000 | Loss: 0.00002905
Iteration 26/1000 | Loss: 0.00002905
Iteration 27/1000 | Loss: 0.00002905
Iteration 28/1000 | Loss: 0.00002904
Iteration 29/1000 | Loss: 0.00002904
Iteration 30/1000 | Loss: 0.00002904
Iteration 31/1000 | Loss: 0.00002903
Iteration 32/1000 | Loss: 0.00002897
Iteration 33/1000 | Loss: 0.00002896
Iteration 34/1000 | Loss: 0.00002894
Iteration 35/1000 | Loss: 0.00002882
Iteration 36/1000 | Loss: 0.00002880
Iteration 37/1000 | Loss: 0.00002880
Iteration 38/1000 | Loss: 0.00002879
Iteration 39/1000 | Loss: 0.00002876
Iteration 40/1000 | Loss: 0.00002869
Iteration 41/1000 | Loss: 0.00002867
Iteration 42/1000 | Loss: 0.00002867
Iteration 43/1000 | Loss: 0.00002867
Iteration 44/1000 | Loss: 0.00002866
Iteration 45/1000 | Loss: 0.00002866
Iteration 46/1000 | Loss: 0.00002866
Iteration 47/1000 | Loss: 0.00002862
Iteration 48/1000 | Loss: 0.00002862
Iteration 49/1000 | Loss: 0.00002862
Iteration 50/1000 | Loss: 0.00002862
Iteration 51/1000 | Loss: 0.00002862
Iteration 52/1000 | Loss: 0.00002860
Iteration 53/1000 | Loss: 0.00002860
Iteration 54/1000 | Loss: 0.00002858
Iteration 55/1000 | Loss: 0.00002858
Iteration 56/1000 | Loss: 0.00002858
Iteration 57/1000 | Loss: 0.00002858
Iteration 58/1000 | Loss: 0.00002858
Iteration 59/1000 | Loss: 0.00002858
Iteration 60/1000 | Loss: 0.00002858
Iteration 61/1000 | Loss: 0.00002858
Iteration 62/1000 | Loss: 0.00002858
Iteration 63/1000 | Loss: 0.00002858
Iteration 64/1000 | Loss: 0.00002858
Iteration 65/1000 | Loss: 0.00002857
Iteration 66/1000 | Loss: 0.00002857
Iteration 67/1000 | Loss: 0.00002857
Iteration 68/1000 | Loss: 0.00002856
Iteration 69/1000 | Loss: 0.00002856
Iteration 70/1000 | Loss: 0.00002856
Iteration 71/1000 | Loss: 0.00002856
Iteration 72/1000 | Loss: 0.00002856
Iteration 73/1000 | Loss: 0.00002856
Iteration 74/1000 | Loss: 0.00002856
Iteration 75/1000 | Loss: 0.00002856
Iteration 76/1000 | Loss: 0.00002856
Iteration 77/1000 | Loss: 0.00002855
Iteration 78/1000 | Loss: 0.00002855
Iteration 79/1000 | Loss: 0.00002855
Iteration 80/1000 | Loss: 0.00002855
Iteration 81/1000 | Loss: 0.00002855
Iteration 82/1000 | Loss: 0.00002855
Iteration 83/1000 | Loss: 0.00002855
Iteration 84/1000 | Loss: 0.00002855
Iteration 85/1000 | Loss: 0.00002855
Iteration 86/1000 | Loss: 0.00002855
Iteration 87/1000 | Loss: 0.00002854
Iteration 88/1000 | Loss: 0.00002854
Iteration 89/1000 | Loss: 0.00002854
Iteration 90/1000 | Loss: 0.00002853
Iteration 91/1000 | Loss: 0.00002853
Iteration 92/1000 | Loss: 0.00002853
Iteration 93/1000 | Loss: 0.00002853
Iteration 94/1000 | Loss: 0.00002853
Iteration 95/1000 | Loss: 0.00002853
Iteration 96/1000 | Loss: 0.00002853
Iteration 97/1000 | Loss: 0.00002853
Iteration 98/1000 | Loss: 0.00002853
Iteration 99/1000 | Loss: 0.00002852
Iteration 100/1000 | Loss: 0.00002852
Iteration 101/1000 | Loss: 0.00002852
Iteration 102/1000 | Loss: 0.00002852
Iteration 103/1000 | Loss: 0.00002852
Iteration 104/1000 | Loss: 0.00002852
Iteration 105/1000 | Loss: 0.00002852
Iteration 106/1000 | Loss: 0.00002852
Iteration 107/1000 | Loss: 0.00002852
Iteration 108/1000 | Loss: 0.00002852
Iteration 109/1000 | Loss: 0.00002852
Iteration 110/1000 | Loss: 0.00002852
Iteration 111/1000 | Loss: 0.00002852
Iteration 112/1000 | Loss: 0.00002852
Iteration 113/1000 | Loss: 0.00002852
Iteration 114/1000 | Loss: 0.00002852
Iteration 115/1000 | Loss: 0.00002852
Iteration 116/1000 | Loss: 0.00002852
Iteration 117/1000 | Loss: 0.00002852
Iteration 118/1000 | Loss: 0.00002852
Iteration 119/1000 | Loss: 0.00002852
Iteration 120/1000 | Loss: 0.00002852
Iteration 121/1000 | Loss: 0.00002852
Iteration 122/1000 | Loss: 0.00002852
Iteration 123/1000 | Loss: 0.00002852
Iteration 124/1000 | Loss: 0.00002852
Iteration 125/1000 | Loss: 0.00002852
Iteration 126/1000 | Loss: 0.00002852
Iteration 127/1000 | Loss: 0.00002852
Iteration 128/1000 | Loss: 0.00002852
Iteration 129/1000 | Loss: 0.00002852
Iteration 130/1000 | Loss: 0.00002852
Iteration 131/1000 | Loss: 0.00002852
Iteration 132/1000 | Loss: 0.00002852
Iteration 133/1000 | Loss: 0.00002852
Iteration 134/1000 | Loss: 0.00002852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.8516833481262438e-05, 2.8516833481262438e-05, 2.8516833481262438e-05, 2.8516833481262438e-05, 2.8516833481262438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8516833481262438e-05

Optimization complete. Final v2v error: 4.48559045791626 mm

Highest mean error: 4.667397975921631 mm for frame 135

Lowest mean error: 4.134607791900635 mm for frame 14

Saving results

Total time: 48.273276805877686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00702317
Iteration 2/25 | Loss: 0.00155224
Iteration 3/25 | Loss: 0.00145453
Iteration 4/25 | Loss: 0.00144762
Iteration 5/25 | Loss: 0.00144728
Iteration 6/25 | Loss: 0.00144728
Iteration 7/25 | Loss: 0.00144728
Iteration 8/25 | Loss: 0.00144728
Iteration 9/25 | Loss: 0.00144728
Iteration 10/25 | Loss: 0.00144728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001447281683795154, 0.001447281683795154, 0.001447281683795154, 0.001447281683795154, 0.001447281683795154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001447281683795154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39642370
Iteration 2/25 | Loss: 0.00168883
Iteration 3/25 | Loss: 0.00168882
Iteration 4/25 | Loss: 0.00168882
Iteration 5/25 | Loss: 0.00168882
Iteration 6/25 | Loss: 0.00168882
Iteration 7/25 | Loss: 0.00168882
Iteration 8/25 | Loss: 0.00168882
Iteration 9/25 | Loss: 0.00168882
Iteration 10/25 | Loss: 0.00168882
Iteration 11/25 | Loss: 0.00168882
Iteration 12/25 | Loss: 0.00168882
Iteration 13/25 | Loss: 0.00168882
Iteration 14/25 | Loss: 0.00168882
Iteration 15/25 | Loss: 0.00168882
Iteration 16/25 | Loss: 0.00168882
Iteration 17/25 | Loss: 0.00168882
Iteration 18/25 | Loss: 0.00168882
Iteration 19/25 | Loss: 0.00168882
Iteration 20/25 | Loss: 0.00168882
Iteration 21/25 | Loss: 0.00168882
Iteration 22/25 | Loss: 0.00168882
Iteration 23/25 | Loss: 0.00168882
Iteration 24/25 | Loss: 0.00168882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016888165846467018, 0.0016888165846467018, 0.0016888165846467018, 0.0016888165846467018, 0.0016888165846467018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016888165846467018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168882
Iteration 2/1000 | Loss: 0.00003969
Iteration 3/1000 | Loss: 0.00002939
Iteration 4/1000 | Loss: 0.00002719
Iteration 5/1000 | Loss: 0.00002615
Iteration 6/1000 | Loss: 0.00002560
Iteration 7/1000 | Loss: 0.00002526
Iteration 8/1000 | Loss: 0.00002490
Iteration 9/1000 | Loss: 0.00002462
Iteration 10/1000 | Loss: 0.00002437
Iteration 11/1000 | Loss: 0.00002415
Iteration 12/1000 | Loss: 0.00002392
Iteration 13/1000 | Loss: 0.00002377
Iteration 14/1000 | Loss: 0.00002377
Iteration 15/1000 | Loss: 0.00002376
Iteration 16/1000 | Loss: 0.00002358
Iteration 17/1000 | Loss: 0.00002337
Iteration 18/1000 | Loss: 0.00002318
Iteration 19/1000 | Loss: 0.00002299
Iteration 20/1000 | Loss: 0.00002299
Iteration 21/1000 | Loss: 0.00002282
Iteration 22/1000 | Loss: 0.00002275
Iteration 23/1000 | Loss: 0.00002275
Iteration 24/1000 | Loss: 0.00002273
Iteration 25/1000 | Loss: 0.00002273
Iteration 26/1000 | Loss: 0.00002273
Iteration 27/1000 | Loss: 0.00002273
Iteration 28/1000 | Loss: 0.00002273
Iteration 29/1000 | Loss: 0.00002273
Iteration 30/1000 | Loss: 0.00002273
Iteration 31/1000 | Loss: 0.00002273
Iteration 32/1000 | Loss: 0.00002272
Iteration 33/1000 | Loss: 0.00002272
Iteration 34/1000 | Loss: 0.00002272
Iteration 35/1000 | Loss: 0.00002270
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002270
Iteration 38/1000 | Loss: 0.00002270
Iteration 39/1000 | Loss: 0.00002269
Iteration 40/1000 | Loss: 0.00002269
Iteration 41/1000 | Loss: 0.00002269
Iteration 42/1000 | Loss: 0.00002269
Iteration 43/1000 | Loss: 0.00002268
Iteration 44/1000 | Loss: 0.00002268
Iteration 45/1000 | Loss: 0.00002268
Iteration 46/1000 | Loss: 0.00002267
Iteration 47/1000 | Loss: 0.00002267
Iteration 48/1000 | Loss: 0.00002267
Iteration 49/1000 | Loss: 0.00002267
Iteration 50/1000 | Loss: 0.00002267
Iteration 51/1000 | Loss: 0.00002267
Iteration 52/1000 | Loss: 0.00002267
Iteration 53/1000 | Loss: 0.00002267
Iteration 54/1000 | Loss: 0.00002266
Iteration 55/1000 | Loss: 0.00002266
Iteration 56/1000 | Loss: 0.00002266
Iteration 57/1000 | Loss: 0.00002266
Iteration 58/1000 | Loss: 0.00002266
Iteration 59/1000 | Loss: 0.00002265
Iteration 60/1000 | Loss: 0.00002265
Iteration 61/1000 | Loss: 0.00002265
Iteration 62/1000 | Loss: 0.00002264
Iteration 63/1000 | Loss: 0.00002264
Iteration 64/1000 | Loss: 0.00002264
Iteration 65/1000 | Loss: 0.00002264
Iteration 66/1000 | Loss: 0.00002264
Iteration 67/1000 | Loss: 0.00002264
Iteration 68/1000 | Loss: 0.00002264
Iteration 69/1000 | Loss: 0.00002264
Iteration 70/1000 | Loss: 0.00002264
Iteration 71/1000 | Loss: 0.00002263
Iteration 72/1000 | Loss: 0.00002263
Iteration 73/1000 | Loss: 0.00002263
Iteration 74/1000 | Loss: 0.00002263
Iteration 75/1000 | Loss: 0.00002263
Iteration 76/1000 | Loss: 0.00002263
Iteration 77/1000 | Loss: 0.00002263
Iteration 78/1000 | Loss: 0.00002263
Iteration 79/1000 | Loss: 0.00002263
Iteration 80/1000 | Loss: 0.00002263
Iteration 81/1000 | Loss: 0.00002263
Iteration 82/1000 | Loss: 0.00002262
Iteration 83/1000 | Loss: 0.00002262
Iteration 84/1000 | Loss: 0.00002262
Iteration 85/1000 | Loss: 0.00002262
Iteration 86/1000 | Loss: 0.00002262
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002262
Iteration 92/1000 | Loss: 0.00002262
Iteration 93/1000 | Loss: 0.00002262
Iteration 94/1000 | Loss: 0.00002262
Iteration 95/1000 | Loss: 0.00002261
Iteration 96/1000 | Loss: 0.00002261
Iteration 97/1000 | Loss: 0.00002261
Iteration 98/1000 | Loss: 0.00002261
Iteration 99/1000 | Loss: 0.00002261
Iteration 100/1000 | Loss: 0.00002261
Iteration 101/1000 | Loss: 0.00002260
Iteration 102/1000 | Loss: 0.00002260
Iteration 103/1000 | Loss: 0.00002260
Iteration 104/1000 | Loss: 0.00002260
Iteration 105/1000 | Loss: 0.00002260
Iteration 106/1000 | Loss: 0.00002260
Iteration 107/1000 | Loss: 0.00002260
Iteration 108/1000 | Loss: 0.00002260
Iteration 109/1000 | Loss: 0.00002260
Iteration 110/1000 | Loss: 0.00002259
Iteration 111/1000 | Loss: 0.00002259
Iteration 112/1000 | Loss: 0.00002259
Iteration 113/1000 | Loss: 0.00002259
Iteration 114/1000 | Loss: 0.00002259
Iteration 115/1000 | Loss: 0.00002259
Iteration 116/1000 | Loss: 0.00002258
Iteration 117/1000 | Loss: 0.00002258
Iteration 118/1000 | Loss: 0.00002258
Iteration 119/1000 | Loss: 0.00002258
Iteration 120/1000 | Loss: 0.00002258
Iteration 121/1000 | Loss: 0.00002258
Iteration 122/1000 | Loss: 0.00002258
Iteration 123/1000 | Loss: 0.00002258
Iteration 124/1000 | Loss: 0.00002258
Iteration 125/1000 | Loss: 0.00002257
Iteration 126/1000 | Loss: 0.00002257
Iteration 127/1000 | Loss: 0.00002257
Iteration 128/1000 | Loss: 0.00002256
Iteration 129/1000 | Loss: 0.00002256
Iteration 130/1000 | Loss: 0.00002255
Iteration 131/1000 | Loss: 0.00002255
Iteration 132/1000 | Loss: 0.00002255
Iteration 133/1000 | Loss: 0.00002255
Iteration 134/1000 | Loss: 0.00002255
Iteration 135/1000 | Loss: 0.00002255
Iteration 136/1000 | Loss: 0.00002255
Iteration 137/1000 | Loss: 0.00002255
Iteration 138/1000 | Loss: 0.00002255
Iteration 139/1000 | Loss: 0.00002255
Iteration 140/1000 | Loss: 0.00002255
Iteration 141/1000 | Loss: 0.00002255
Iteration 142/1000 | Loss: 0.00002255
Iteration 143/1000 | Loss: 0.00002254
Iteration 144/1000 | Loss: 0.00002254
Iteration 145/1000 | Loss: 0.00002254
Iteration 146/1000 | Loss: 0.00002254
Iteration 147/1000 | Loss: 0.00002254
Iteration 148/1000 | Loss: 0.00002254
Iteration 149/1000 | Loss: 0.00002254
Iteration 150/1000 | Loss: 0.00002254
Iteration 151/1000 | Loss: 0.00002254
Iteration 152/1000 | Loss: 0.00002254
Iteration 153/1000 | Loss: 0.00002254
Iteration 154/1000 | Loss: 0.00002253
Iteration 155/1000 | Loss: 0.00002253
Iteration 156/1000 | Loss: 0.00002253
Iteration 157/1000 | Loss: 0.00002253
Iteration 158/1000 | Loss: 0.00002253
Iteration 159/1000 | Loss: 0.00002253
Iteration 160/1000 | Loss: 0.00002253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.253408638352994e-05, 2.253408638352994e-05, 2.253408638352994e-05, 2.253408638352994e-05, 2.253408638352994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.253408638352994e-05

Optimization complete. Final v2v error: 3.7324516773223877 mm

Highest mean error: 4.148199081420898 mm for frame 98

Lowest mean error: 3.3098044395446777 mm for frame 158

Saving results

Total time: 47.57865786552429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423151
Iteration 2/25 | Loss: 0.00138329
Iteration 3/25 | Loss: 0.00131971
Iteration 4/25 | Loss: 0.00131206
Iteration 5/25 | Loss: 0.00130959
Iteration 6/25 | Loss: 0.00130951
Iteration 7/25 | Loss: 0.00130951
Iteration 8/25 | Loss: 0.00130951
Iteration 9/25 | Loss: 0.00130951
Iteration 10/25 | Loss: 0.00130951
Iteration 11/25 | Loss: 0.00130951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013095051981508732, 0.0013095051981508732, 0.0013095051981508732, 0.0013095051981508732, 0.0013095051981508732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013095051981508732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02374661
Iteration 2/25 | Loss: 0.00198255
Iteration 3/25 | Loss: 0.00198255
Iteration 4/25 | Loss: 0.00198255
Iteration 5/25 | Loss: 0.00198255
Iteration 6/25 | Loss: 0.00198255
Iteration 7/25 | Loss: 0.00198255
Iteration 8/25 | Loss: 0.00198255
Iteration 9/25 | Loss: 0.00198255
Iteration 10/25 | Loss: 0.00198255
Iteration 11/25 | Loss: 0.00198255
Iteration 12/25 | Loss: 0.00198255
Iteration 13/25 | Loss: 0.00198255
Iteration 14/25 | Loss: 0.00198255
Iteration 15/25 | Loss: 0.00198255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019825471099466085, 0.0019825471099466085, 0.0019825471099466085, 0.0019825471099466085, 0.0019825471099466085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019825471099466085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198255
Iteration 2/1000 | Loss: 0.00003802
Iteration 3/1000 | Loss: 0.00002326
Iteration 4/1000 | Loss: 0.00001970
Iteration 5/1000 | Loss: 0.00001791
Iteration 6/1000 | Loss: 0.00001666
Iteration 7/1000 | Loss: 0.00001609
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001495
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001345
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001297
Iteration 16/1000 | Loss: 0.00001288
Iteration 17/1000 | Loss: 0.00001288
Iteration 18/1000 | Loss: 0.00001288
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001285
Iteration 26/1000 | Loss: 0.00001285
Iteration 27/1000 | Loss: 0.00001279
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001272
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001269
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001265
Iteration 48/1000 | Loss: 0.00001265
Iteration 49/1000 | Loss: 0.00001265
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001262
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001257
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001257
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001256
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001256
Iteration 70/1000 | Loss: 0.00001252
Iteration 71/1000 | Loss: 0.00001252
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001251
Iteration 83/1000 | Loss: 0.00001251
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001245
Iteration 96/1000 | Loss: 0.00001244
Iteration 97/1000 | Loss: 0.00001244
Iteration 98/1000 | Loss: 0.00001244
Iteration 99/1000 | Loss: 0.00001244
Iteration 100/1000 | Loss: 0.00001244
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001243
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001242
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001240
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001238
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001238
Iteration 124/1000 | Loss: 0.00001238
Iteration 125/1000 | Loss: 0.00001238
Iteration 126/1000 | Loss: 0.00001238
Iteration 127/1000 | Loss: 0.00001238
Iteration 128/1000 | Loss: 0.00001238
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001236
Iteration 134/1000 | Loss: 0.00001236
Iteration 135/1000 | Loss: 0.00001236
Iteration 136/1000 | Loss: 0.00001235
Iteration 137/1000 | Loss: 0.00001235
Iteration 138/1000 | Loss: 0.00001235
Iteration 139/1000 | Loss: 0.00001234
Iteration 140/1000 | Loss: 0.00001234
Iteration 141/1000 | Loss: 0.00001234
Iteration 142/1000 | Loss: 0.00001234
Iteration 143/1000 | Loss: 0.00001234
Iteration 144/1000 | Loss: 0.00001234
Iteration 145/1000 | Loss: 0.00001234
Iteration 146/1000 | Loss: 0.00001234
Iteration 147/1000 | Loss: 0.00001234
Iteration 148/1000 | Loss: 0.00001234
Iteration 149/1000 | Loss: 0.00001234
Iteration 150/1000 | Loss: 0.00001234
Iteration 151/1000 | Loss: 0.00001234
Iteration 152/1000 | Loss: 0.00001234
Iteration 153/1000 | Loss: 0.00001234
Iteration 154/1000 | Loss: 0.00001234
Iteration 155/1000 | Loss: 0.00001234
Iteration 156/1000 | Loss: 0.00001234
Iteration 157/1000 | Loss: 0.00001234
Iteration 158/1000 | Loss: 0.00001234
Iteration 159/1000 | Loss: 0.00001234
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001234
Iteration 168/1000 | Loss: 0.00001234
Iteration 169/1000 | Loss: 0.00001234
Iteration 170/1000 | Loss: 0.00001234
Iteration 171/1000 | Loss: 0.00001234
Iteration 172/1000 | Loss: 0.00001234
Iteration 173/1000 | Loss: 0.00001234
Iteration 174/1000 | Loss: 0.00001234
Iteration 175/1000 | Loss: 0.00001234
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Iteration 180/1000 | Loss: 0.00001234
Iteration 181/1000 | Loss: 0.00001234
Iteration 182/1000 | Loss: 0.00001234
Iteration 183/1000 | Loss: 0.00001234
Iteration 184/1000 | Loss: 0.00001234
Iteration 185/1000 | Loss: 0.00001234
Iteration 186/1000 | Loss: 0.00001234
Iteration 187/1000 | Loss: 0.00001234
Iteration 188/1000 | Loss: 0.00001234
Iteration 189/1000 | Loss: 0.00001234
Iteration 190/1000 | Loss: 0.00001234
Iteration 191/1000 | Loss: 0.00001234
Iteration 192/1000 | Loss: 0.00001234
Iteration 193/1000 | Loss: 0.00001234
Iteration 194/1000 | Loss: 0.00001234
Iteration 195/1000 | Loss: 0.00001234
Iteration 196/1000 | Loss: 0.00001234
Iteration 197/1000 | Loss: 0.00001234
Iteration 198/1000 | Loss: 0.00001234
Iteration 199/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.2340695320744999e-05, 1.2340695320744999e-05, 1.2340695320744999e-05, 1.2340695320744999e-05, 1.2340695320744999e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2340695320744999e-05

Optimization complete. Final v2v error: 3.055570602416992 mm

Highest mean error: 3.0747082233428955 mm for frame 52

Lowest mean error: 3.0450994968414307 mm for frame 80

Saving results

Total time: 40.96840763092041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832613
Iteration 2/25 | Loss: 0.00152792
Iteration 3/25 | Loss: 0.00137318
Iteration 4/25 | Loss: 0.00135637
Iteration 5/25 | Loss: 0.00135116
Iteration 6/25 | Loss: 0.00135084
Iteration 7/25 | Loss: 0.00135084
Iteration 8/25 | Loss: 0.00135084
Iteration 9/25 | Loss: 0.00135084
Iteration 10/25 | Loss: 0.00135084
Iteration 11/25 | Loss: 0.00135084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001350843464024365, 0.001350843464024365, 0.001350843464024365, 0.001350843464024365, 0.001350843464024365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001350843464024365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82487881
Iteration 2/25 | Loss: 0.00127761
Iteration 3/25 | Loss: 0.00127760
Iteration 4/25 | Loss: 0.00127760
Iteration 5/25 | Loss: 0.00127760
Iteration 6/25 | Loss: 0.00127760
Iteration 7/25 | Loss: 0.00127760
Iteration 8/25 | Loss: 0.00127760
Iteration 9/25 | Loss: 0.00127760
Iteration 10/25 | Loss: 0.00127760
Iteration 11/25 | Loss: 0.00127760
Iteration 12/25 | Loss: 0.00127760
Iteration 13/25 | Loss: 0.00127760
Iteration 14/25 | Loss: 0.00127760
Iteration 15/25 | Loss: 0.00127760
Iteration 16/25 | Loss: 0.00127760
Iteration 17/25 | Loss: 0.00127760
Iteration 18/25 | Loss: 0.00127760
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012776008807122707, 0.0012776008807122707, 0.0012776008807122707, 0.0012776008807122707, 0.0012776008807122707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012776008807122707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127760
Iteration 2/1000 | Loss: 0.00003949
Iteration 3/1000 | Loss: 0.00003132
Iteration 4/1000 | Loss: 0.00002914
Iteration 5/1000 | Loss: 0.00002771
Iteration 6/1000 | Loss: 0.00002696
Iteration 7/1000 | Loss: 0.00002655
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00002601
Iteration 10/1000 | Loss: 0.00002584
Iteration 11/1000 | Loss: 0.00002564
Iteration 12/1000 | Loss: 0.00002560
Iteration 13/1000 | Loss: 0.00002549
Iteration 14/1000 | Loss: 0.00002546
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00002535
Iteration 17/1000 | Loss: 0.00002535
Iteration 18/1000 | Loss: 0.00002535
Iteration 19/1000 | Loss: 0.00002535
Iteration 20/1000 | Loss: 0.00002534
Iteration 21/1000 | Loss: 0.00002534
Iteration 22/1000 | Loss: 0.00002534
Iteration 23/1000 | Loss: 0.00002534
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002534
Iteration 26/1000 | Loss: 0.00002534
Iteration 27/1000 | Loss: 0.00002534
Iteration 28/1000 | Loss: 0.00002534
Iteration 29/1000 | Loss: 0.00002534
Iteration 30/1000 | Loss: 0.00002534
Iteration 31/1000 | Loss: 0.00002534
Iteration 32/1000 | Loss: 0.00002534
Iteration 33/1000 | Loss: 0.00002534
Iteration 34/1000 | Loss: 0.00002534
Iteration 35/1000 | Loss: 0.00002534
Iteration 36/1000 | Loss: 0.00002533
Iteration 37/1000 | Loss: 0.00002533
Iteration 38/1000 | Loss: 0.00002533
Iteration 39/1000 | Loss: 0.00002533
Iteration 40/1000 | Loss: 0.00002533
Iteration 41/1000 | Loss: 0.00002533
Iteration 42/1000 | Loss: 0.00002533
Iteration 43/1000 | Loss: 0.00002533
Iteration 44/1000 | Loss: 0.00002533
Iteration 45/1000 | Loss: 0.00002533
Iteration 46/1000 | Loss: 0.00002533
Iteration 47/1000 | Loss: 0.00002533
Iteration 48/1000 | Loss: 0.00002531
Iteration 49/1000 | Loss: 0.00002531
Iteration 50/1000 | Loss: 0.00002530
Iteration 51/1000 | Loss: 0.00002530
Iteration 52/1000 | Loss: 0.00002530
Iteration 53/1000 | Loss: 0.00002530
Iteration 54/1000 | Loss: 0.00002529
Iteration 55/1000 | Loss: 0.00002529
Iteration 56/1000 | Loss: 0.00002529
Iteration 57/1000 | Loss: 0.00002529
Iteration 58/1000 | Loss: 0.00002529
Iteration 59/1000 | Loss: 0.00002529
Iteration 60/1000 | Loss: 0.00002528
Iteration 61/1000 | Loss: 0.00002528
Iteration 62/1000 | Loss: 0.00002528
Iteration 63/1000 | Loss: 0.00002528
Iteration 64/1000 | Loss: 0.00002528
Iteration 65/1000 | Loss: 0.00002528
Iteration 66/1000 | Loss: 0.00002528
Iteration 67/1000 | Loss: 0.00002528
Iteration 68/1000 | Loss: 0.00002528
Iteration 69/1000 | Loss: 0.00002528
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002527
Iteration 72/1000 | Loss: 0.00002527
Iteration 73/1000 | Loss: 0.00002527
Iteration 74/1000 | Loss: 0.00002527
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00002527
Iteration 77/1000 | Loss: 0.00002527
Iteration 78/1000 | Loss: 0.00002526
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002526
Iteration 81/1000 | Loss: 0.00002526
Iteration 82/1000 | Loss: 0.00002526
Iteration 83/1000 | Loss: 0.00002525
Iteration 84/1000 | Loss: 0.00002525
Iteration 85/1000 | Loss: 0.00002525
Iteration 86/1000 | Loss: 0.00002525
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002525
Iteration 89/1000 | Loss: 0.00002525
Iteration 90/1000 | Loss: 0.00002525
Iteration 91/1000 | Loss: 0.00002525
Iteration 92/1000 | Loss: 0.00002522
Iteration 93/1000 | Loss: 0.00002522
Iteration 94/1000 | Loss: 0.00002522
Iteration 95/1000 | Loss: 0.00002522
Iteration 96/1000 | Loss: 0.00002522
Iteration 97/1000 | Loss: 0.00002522
Iteration 98/1000 | Loss: 0.00002522
Iteration 99/1000 | Loss: 0.00002522
Iteration 100/1000 | Loss: 0.00002522
Iteration 101/1000 | Loss: 0.00002522
Iteration 102/1000 | Loss: 0.00002522
Iteration 103/1000 | Loss: 0.00002522
Iteration 104/1000 | Loss: 0.00002521
Iteration 105/1000 | Loss: 0.00002521
Iteration 106/1000 | Loss: 0.00002521
Iteration 107/1000 | Loss: 0.00002521
Iteration 108/1000 | Loss: 0.00002521
Iteration 109/1000 | Loss: 0.00002520
Iteration 110/1000 | Loss: 0.00002520
Iteration 111/1000 | Loss: 0.00002520
Iteration 112/1000 | Loss: 0.00002520
Iteration 113/1000 | Loss: 0.00002520
Iteration 114/1000 | Loss: 0.00002520
Iteration 115/1000 | Loss: 0.00002520
Iteration 116/1000 | Loss: 0.00002519
Iteration 117/1000 | Loss: 0.00002519
Iteration 118/1000 | Loss: 0.00002518
Iteration 119/1000 | Loss: 0.00002518
Iteration 120/1000 | Loss: 0.00002518
Iteration 121/1000 | Loss: 0.00002518
Iteration 122/1000 | Loss: 0.00002518
Iteration 123/1000 | Loss: 0.00002518
Iteration 124/1000 | Loss: 0.00002518
Iteration 125/1000 | Loss: 0.00002518
Iteration 126/1000 | Loss: 0.00002518
Iteration 127/1000 | Loss: 0.00002518
Iteration 128/1000 | Loss: 0.00002518
Iteration 129/1000 | Loss: 0.00002518
Iteration 130/1000 | Loss: 0.00002518
Iteration 131/1000 | Loss: 0.00002518
Iteration 132/1000 | Loss: 0.00002518
Iteration 133/1000 | Loss: 0.00002518
Iteration 134/1000 | Loss: 0.00002518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.518341898394283e-05, 2.518341898394283e-05, 2.518341898394283e-05, 2.518341898394283e-05, 2.518341898394283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.518341898394283e-05

Optimization complete. Final v2v error: 4.284040451049805 mm

Highest mean error: 4.367530822753906 mm for frame 56

Lowest mean error: 4.153164863586426 mm for frame 170

Saving results

Total time: 35.24233341217041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479561
Iteration 2/25 | Loss: 0.00147334
Iteration 3/25 | Loss: 0.00138694
Iteration 4/25 | Loss: 0.00137934
Iteration 5/25 | Loss: 0.00137769
Iteration 6/25 | Loss: 0.00137769
Iteration 7/25 | Loss: 0.00137769
Iteration 8/25 | Loss: 0.00137769
Iteration 9/25 | Loss: 0.00137769
Iteration 10/25 | Loss: 0.00137769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013776897685602307, 0.0013776897685602307, 0.0013776897685602307, 0.0013776897685602307, 0.0013776897685602307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013776897685602307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23531938
Iteration 2/25 | Loss: 0.00214137
Iteration 3/25 | Loss: 0.00214136
Iteration 4/25 | Loss: 0.00214136
Iteration 5/25 | Loss: 0.00214136
Iteration 6/25 | Loss: 0.00214136
Iteration 7/25 | Loss: 0.00214136
Iteration 8/25 | Loss: 0.00214136
Iteration 9/25 | Loss: 0.00214136
Iteration 10/25 | Loss: 0.00214136
Iteration 11/25 | Loss: 0.00214136
Iteration 12/25 | Loss: 0.00214136
Iteration 13/25 | Loss: 0.00214136
Iteration 14/25 | Loss: 0.00214136
Iteration 15/25 | Loss: 0.00214136
Iteration 16/25 | Loss: 0.00214136
Iteration 17/25 | Loss: 0.00214136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021413557697087526, 0.0021413557697087526, 0.0021413557697087526, 0.0021413557697087526, 0.0021413557697087526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021413557697087526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214136
Iteration 2/1000 | Loss: 0.00003258
Iteration 3/1000 | Loss: 0.00002300
Iteration 4/1000 | Loss: 0.00002039
Iteration 5/1000 | Loss: 0.00001895
Iteration 6/1000 | Loss: 0.00001806
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001718
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001626
Iteration 13/1000 | Loss: 0.00001618
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001582
Iteration 21/1000 | Loss: 0.00001581
Iteration 22/1000 | Loss: 0.00001580
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001564
Iteration 26/1000 | Loss: 0.00001564
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001563
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001562
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001558
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001554
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001548
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001547
Iteration 52/1000 | Loss: 0.00001547
Iteration 53/1000 | Loss: 0.00001547
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001545
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001543
Iteration 66/1000 | Loss: 0.00001543
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001539
Iteration 71/1000 | Loss: 0.00001539
Iteration 72/1000 | Loss: 0.00001539
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001539
Iteration 77/1000 | Loss: 0.00001539
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001535
Iteration 82/1000 | Loss: 0.00001535
Iteration 83/1000 | Loss: 0.00001534
Iteration 84/1000 | Loss: 0.00001534
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001533
Iteration 90/1000 | Loss: 0.00001533
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001531
Iteration 94/1000 | Loss: 0.00001531
Iteration 95/1000 | Loss: 0.00001531
Iteration 96/1000 | Loss: 0.00001531
Iteration 97/1000 | Loss: 0.00001531
Iteration 98/1000 | Loss: 0.00001530
Iteration 99/1000 | Loss: 0.00001530
Iteration 100/1000 | Loss: 0.00001530
Iteration 101/1000 | Loss: 0.00001529
Iteration 102/1000 | Loss: 0.00001529
Iteration 103/1000 | Loss: 0.00001528
Iteration 104/1000 | Loss: 0.00001528
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001527
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001525
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001524
Iteration 120/1000 | Loss: 0.00001524
Iteration 121/1000 | Loss: 0.00001524
Iteration 122/1000 | Loss: 0.00001524
Iteration 123/1000 | Loss: 0.00001524
Iteration 124/1000 | Loss: 0.00001524
Iteration 125/1000 | Loss: 0.00001524
Iteration 126/1000 | Loss: 0.00001524
Iteration 127/1000 | Loss: 0.00001523
Iteration 128/1000 | Loss: 0.00001523
Iteration 129/1000 | Loss: 0.00001523
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001523
Iteration 133/1000 | Loss: 0.00001523
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001523
Iteration 140/1000 | Loss: 0.00001523
Iteration 141/1000 | Loss: 0.00001523
Iteration 142/1000 | Loss: 0.00001523
Iteration 143/1000 | Loss: 0.00001522
Iteration 144/1000 | Loss: 0.00001522
Iteration 145/1000 | Loss: 0.00001522
Iteration 146/1000 | Loss: 0.00001522
Iteration 147/1000 | Loss: 0.00001522
Iteration 148/1000 | Loss: 0.00001522
Iteration 149/1000 | Loss: 0.00001522
Iteration 150/1000 | Loss: 0.00001522
Iteration 151/1000 | Loss: 0.00001522
Iteration 152/1000 | Loss: 0.00001521
Iteration 153/1000 | Loss: 0.00001521
Iteration 154/1000 | Loss: 0.00001521
Iteration 155/1000 | Loss: 0.00001521
Iteration 156/1000 | Loss: 0.00001521
Iteration 157/1000 | Loss: 0.00001521
Iteration 158/1000 | Loss: 0.00001521
Iteration 159/1000 | Loss: 0.00001521
Iteration 160/1000 | Loss: 0.00001521
Iteration 161/1000 | Loss: 0.00001521
Iteration 162/1000 | Loss: 0.00001521
Iteration 163/1000 | Loss: 0.00001521
Iteration 164/1000 | Loss: 0.00001521
Iteration 165/1000 | Loss: 0.00001521
Iteration 166/1000 | Loss: 0.00001521
Iteration 167/1000 | Loss: 0.00001521
Iteration 168/1000 | Loss: 0.00001521
Iteration 169/1000 | Loss: 0.00001520
Iteration 170/1000 | Loss: 0.00001520
Iteration 171/1000 | Loss: 0.00001520
Iteration 172/1000 | Loss: 0.00001520
Iteration 173/1000 | Loss: 0.00001520
Iteration 174/1000 | Loss: 0.00001519
Iteration 175/1000 | Loss: 0.00001519
Iteration 176/1000 | Loss: 0.00001519
Iteration 177/1000 | Loss: 0.00001519
Iteration 178/1000 | Loss: 0.00001519
Iteration 179/1000 | Loss: 0.00001519
Iteration 180/1000 | Loss: 0.00001519
Iteration 181/1000 | Loss: 0.00001519
Iteration 182/1000 | Loss: 0.00001519
Iteration 183/1000 | Loss: 0.00001519
Iteration 184/1000 | Loss: 0.00001519
Iteration 185/1000 | Loss: 0.00001518
Iteration 186/1000 | Loss: 0.00001518
Iteration 187/1000 | Loss: 0.00001518
Iteration 188/1000 | Loss: 0.00001518
Iteration 189/1000 | Loss: 0.00001518
Iteration 190/1000 | Loss: 0.00001518
Iteration 191/1000 | Loss: 0.00001518
Iteration 192/1000 | Loss: 0.00001518
Iteration 193/1000 | Loss: 0.00001518
Iteration 194/1000 | Loss: 0.00001518
Iteration 195/1000 | Loss: 0.00001518
Iteration 196/1000 | Loss: 0.00001518
Iteration 197/1000 | Loss: 0.00001518
Iteration 198/1000 | Loss: 0.00001518
Iteration 199/1000 | Loss: 0.00001518
Iteration 200/1000 | Loss: 0.00001518
Iteration 201/1000 | Loss: 0.00001518
Iteration 202/1000 | Loss: 0.00001517
Iteration 203/1000 | Loss: 0.00001517
Iteration 204/1000 | Loss: 0.00001517
Iteration 205/1000 | Loss: 0.00001517
Iteration 206/1000 | Loss: 0.00001517
Iteration 207/1000 | Loss: 0.00001517
Iteration 208/1000 | Loss: 0.00001517
Iteration 209/1000 | Loss: 0.00001517
Iteration 210/1000 | Loss: 0.00001517
Iteration 211/1000 | Loss: 0.00001517
Iteration 212/1000 | Loss: 0.00001517
Iteration 213/1000 | Loss: 0.00001517
Iteration 214/1000 | Loss: 0.00001517
Iteration 215/1000 | Loss: 0.00001516
Iteration 216/1000 | Loss: 0.00001516
Iteration 217/1000 | Loss: 0.00001516
Iteration 218/1000 | Loss: 0.00001516
Iteration 219/1000 | Loss: 0.00001516
Iteration 220/1000 | Loss: 0.00001516
Iteration 221/1000 | Loss: 0.00001516
Iteration 222/1000 | Loss: 0.00001516
Iteration 223/1000 | Loss: 0.00001516
Iteration 224/1000 | Loss: 0.00001516
Iteration 225/1000 | Loss: 0.00001516
Iteration 226/1000 | Loss: 0.00001516
Iteration 227/1000 | Loss: 0.00001516
Iteration 228/1000 | Loss: 0.00001516
Iteration 229/1000 | Loss: 0.00001516
Iteration 230/1000 | Loss: 0.00001516
Iteration 231/1000 | Loss: 0.00001516
Iteration 232/1000 | Loss: 0.00001516
Iteration 233/1000 | Loss: 0.00001516
Iteration 234/1000 | Loss: 0.00001516
Iteration 235/1000 | Loss: 0.00001516
Iteration 236/1000 | Loss: 0.00001515
Iteration 237/1000 | Loss: 0.00001515
Iteration 238/1000 | Loss: 0.00001515
Iteration 239/1000 | Loss: 0.00001515
Iteration 240/1000 | Loss: 0.00001515
Iteration 241/1000 | Loss: 0.00001515
Iteration 242/1000 | Loss: 0.00001515
Iteration 243/1000 | Loss: 0.00001515
Iteration 244/1000 | Loss: 0.00001515
Iteration 245/1000 | Loss: 0.00001515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.5149365026445594e-05, 1.5149365026445594e-05, 1.5149365026445594e-05, 1.5149365026445594e-05, 1.5149365026445594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5149365026445594e-05

Optimization complete. Final v2v error: 3.2346246242523193 mm

Highest mean error: 3.9289588928222656 mm for frame 58

Lowest mean error: 2.9577317237854004 mm for frame 12

Saving results

Total time: 53.25670909881592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818309
Iteration 2/25 | Loss: 0.00137901
Iteration 3/25 | Loss: 0.00131679
Iteration 4/25 | Loss: 0.00130900
Iteration 5/25 | Loss: 0.00130785
Iteration 6/25 | Loss: 0.00130785
Iteration 7/25 | Loss: 0.00130785
Iteration 8/25 | Loss: 0.00130785
Iteration 9/25 | Loss: 0.00130785
Iteration 10/25 | Loss: 0.00130785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013078470947220922, 0.0013078470947220922, 0.0013078470947220922, 0.0013078470947220922, 0.0013078470947220922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013078470947220922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39790702
Iteration 2/25 | Loss: 0.00195726
Iteration 3/25 | Loss: 0.00195726
Iteration 4/25 | Loss: 0.00195726
Iteration 5/25 | Loss: 0.00195726
Iteration 6/25 | Loss: 0.00195726
Iteration 7/25 | Loss: 0.00195726
Iteration 8/25 | Loss: 0.00195726
Iteration 9/25 | Loss: 0.00195726
Iteration 10/25 | Loss: 0.00195726
Iteration 11/25 | Loss: 0.00195726
Iteration 12/25 | Loss: 0.00195726
Iteration 13/25 | Loss: 0.00195726
Iteration 14/25 | Loss: 0.00195726
Iteration 15/25 | Loss: 0.00195726
Iteration 16/25 | Loss: 0.00195726
Iteration 17/25 | Loss: 0.00195726
Iteration 18/25 | Loss: 0.00195726
Iteration 19/25 | Loss: 0.00195726
Iteration 20/25 | Loss: 0.00195726
Iteration 21/25 | Loss: 0.00195726
Iteration 22/25 | Loss: 0.00195726
Iteration 23/25 | Loss: 0.00195726
Iteration 24/25 | Loss: 0.00195726
Iteration 25/25 | Loss: 0.00195726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001957257743924856, 0.001957257743924856, 0.001957257743924856, 0.001957257743924856, 0.001957257743924856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001957257743924856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195726
Iteration 2/1000 | Loss: 0.00002032
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001476
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001280
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001189
Iteration 9/1000 | Loss: 0.00001149
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00001110
Iteration 16/1000 | Loss: 0.00001089
Iteration 17/1000 | Loss: 0.00001069
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001048
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001046
Iteration 24/1000 | Loss: 0.00001046
Iteration 25/1000 | Loss: 0.00001046
Iteration 26/1000 | Loss: 0.00001045
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001045
Iteration 31/1000 | Loss: 0.00001045
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001040
Iteration 36/1000 | Loss: 0.00001040
Iteration 37/1000 | Loss: 0.00001038
Iteration 38/1000 | Loss: 0.00001037
Iteration 39/1000 | Loss: 0.00001037
Iteration 40/1000 | Loss: 0.00001035
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001034
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001032
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001026
Iteration 51/1000 | Loss: 0.00001026
Iteration 52/1000 | Loss: 0.00001026
Iteration 53/1000 | Loss: 0.00001025
Iteration 54/1000 | Loss: 0.00001025
Iteration 55/1000 | Loss: 0.00001024
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001021
Iteration 58/1000 | Loss: 0.00001021
Iteration 59/1000 | Loss: 0.00001020
Iteration 60/1000 | Loss: 0.00001020
Iteration 61/1000 | Loss: 0.00001020
Iteration 62/1000 | Loss: 0.00001020
Iteration 63/1000 | Loss: 0.00001020
Iteration 64/1000 | Loss: 0.00001020
Iteration 65/1000 | Loss: 0.00001020
Iteration 66/1000 | Loss: 0.00001020
Iteration 67/1000 | Loss: 0.00001020
Iteration 68/1000 | Loss: 0.00001020
Iteration 69/1000 | Loss: 0.00001019
Iteration 70/1000 | Loss: 0.00001018
Iteration 71/1000 | Loss: 0.00001018
Iteration 72/1000 | Loss: 0.00001017
Iteration 73/1000 | Loss: 0.00001017
Iteration 74/1000 | Loss: 0.00001017
Iteration 75/1000 | Loss: 0.00001017
Iteration 76/1000 | Loss: 0.00001017
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001016
Iteration 79/1000 | Loss: 0.00001016
Iteration 80/1000 | Loss: 0.00001016
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001012
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001010
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001008
Iteration 100/1000 | Loss: 0.00001008
Iteration 101/1000 | Loss: 0.00001008
Iteration 102/1000 | Loss: 0.00001008
Iteration 103/1000 | Loss: 0.00001008
Iteration 104/1000 | Loss: 0.00001008
Iteration 105/1000 | Loss: 0.00001008
Iteration 106/1000 | Loss: 0.00001008
Iteration 107/1000 | Loss: 0.00001008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.0076853868667968e-05, 1.0076853868667968e-05, 1.0076853868667968e-05, 1.0076853868667968e-05, 1.0076853868667968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0076853868667968e-05

Optimization complete. Final v2v error: 2.7563488483428955 mm

Highest mean error: 3.0507264137268066 mm for frame 197

Lowest mean error: 2.5979161262512207 mm for frame 36

Saving results

Total time: 39.858962297439575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789324
Iteration 2/25 | Loss: 0.00143090
Iteration 3/25 | Loss: 0.00133457
Iteration 4/25 | Loss: 0.00132828
Iteration 5/25 | Loss: 0.00132700
Iteration 6/25 | Loss: 0.00132700
Iteration 7/25 | Loss: 0.00132700
Iteration 8/25 | Loss: 0.00132700
Iteration 9/25 | Loss: 0.00132700
Iteration 10/25 | Loss: 0.00132700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001326997298747301, 0.001326997298747301, 0.001326997298747301, 0.001326997298747301, 0.001326997298747301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001326997298747301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27571356
Iteration 2/25 | Loss: 0.00232334
Iteration 3/25 | Loss: 0.00232333
Iteration 4/25 | Loss: 0.00232333
Iteration 5/25 | Loss: 0.00232333
Iteration 6/25 | Loss: 0.00232333
Iteration 7/25 | Loss: 0.00232333
Iteration 8/25 | Loss: 0.00232333
Iteration 9/25 | Loss: 0.00232333
Iteration 10/25 | Loss: 0.00232333
Iteration 11/25 | Loss: 0.00232333
Iteration 12/25 | Loss: 0.00232333
Iteration 13/25 | Loss: 0.00232333
Iteration 14/25 | Loss: 0.00232333
Iteration 15/25 | Loss: 0.00232333
Iteration 16/25 | Loss: 0.00232333
Iteration 17/25 | Loss: 0.00232333
Iteration 18/25 | Loss: 0.00232333
Iteration 19/25 | Loss: 0.00232333
Iteration 20/25 | Loss: 0.00232333
Iteration 21/25 | Loss: 0.00232333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002323329448699951, 0.002323329448699951, 0.002323329448699951, 0.002323329448699951, 0.002323329448699951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002323329448699951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232333
Iteration 2/1000 | Loss: 0.00003404
Iteration 3/1000 | Loss: 0.00002317
Iteration 4/1000 | Loss: 0.00001932
Iteration 5/1000 | Loss: 0.00001690
Iteration 6/1000 | Loss: 0.00001584
Iteration 7/1000 | Loss: 0.00001495
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001359
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001298
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001289
Iteration 17/1000 | Loss: 0.00001279
Iteration 18/1000 | Loss: 0.00001274
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001233
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001230
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001213
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001209
Iteration 71/1000 | Loss: 0.00001209
Iteration 72/1000 | Loss: 0.00001209
Iteration 73/1000 | Loss: 0.00001209
Iteration 74/1000 | Loss: 0.00001209
Iteration 75/1000 | Loss: 0.00001208
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001198
Iteration 101/1000 | Loss: 0.00001198
Iteration 102/1000 | Loss: 0.00001198
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001190
Iteration 159/1000 | Loss: 0.00001190
Iteration 160/1000 | Loss: 0.00001190
Iteration 161/1000 | Loss: 0.00001190
Iteration 162/1000 | Loss: 0.00001190
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001189
Iteration 166/1000 | Loss: 0.00001189
Iteration 167/1000 | Loss: 0.00001189
Iteration 168/1000 | Loss: 0.00001189
Iteration 169/1000 | Loss: 0.00001189
Iteration 170/1000 | Loss: 0.00001189
Iteration 171/1000 | Loss: 0.00001189
Iteration 172/1000 | Loss: 0.00001189
Iteration 173/1000 | Loss: 0.00001189
Iteration 174/1000 | Loss: 0.00001188
Iteration 175/1000 | Loss: 0.00001188
Iteration 176/1000 | Loss: 0.00001188
Iteration 177/1000 | Loss: 0.00001188
Iteration 178/1000 | Loss: 0.00001188
Iteration 179/1000 | Loss: 0.00001188
Iteration 180/1000 | Loss: 0.00001188
Iteration 181/1000 | Loss: 0.00001187
Iteration 182/1000 | Loss: 0.00001187
Iteration 183/1000 | Loss: 0.00001187
Iteration 184/1000 | Loss: 0.00001187
Iteration 185/1000 | Loss: 0.00001187
Iteration 186/1000 | Loss: 0.00001187
Iteration 187/1000 | Loss: 0.00001187
Iteration 188/1000 | Loss: 0.00001187
Iteration 189/1000 | Loss: 0.00001187
Iteration 190/1000 | Loss: 0.00001187
Iteration 191/1000 | Loss: 0.00001186
Iteration 192/1000 | Loss: 0.00001186
Iteration 193/1000 | Loss: 0.00001186
Iteration 194/1000 | Loss: 0.00001185
Iteration 195/1000 | Loss: 0.00001185
Iteration 196/1000 | Loss: 0.00001185
Iteration 197/1000 | Loss: 0.00001185
Iteration 198/1000 | Loss: 0.00001185
Iteration 199/1000 | Loss: 0.00001185
Iteration 200/1000 | Loss: 0.00001185
Iteration 201/1000 | Loss: 0.00001184
Iteration 202/1000 | Loss: 0.00001184
Iteration 203/1000 | Loss: 0.00001184
Iteration 204/1000 | Loss: 0.00001184
Iteration 205/1000 | Loss: 0.00001183
Iteration 206/1000 | Loss: 0.00001183
Iteration 207/1000 | Loss: 0.00001183
Iteration 208/1000 | Loss: 0.00001183
Iteration 209/1000 | Loss: 0.00001183
Iteration 210/1000 | Loss: 0.00001182
Iteration 211/1000 | Loss: 0.00001182
Iteration 212/1000 | Loss: 0.00001182
Iteration 213/1000 | Loss: 0.00001182
Iteration 214/1000 | Loss: 0.00001182
Iteration 215/1000 | Loss: 0.00001182
Iteration 216/1000 | Loss: 0.00001182
Iteration 217/1000 | Loss: 0.00001182
Iteration 218/1000 | Loss: 0.00001182
Iteration 219/1000 | Loss: 0.00001182
Iteration 220/1000 | Loss: 0.00001182
Iteration 221/1000 | Loss: 0.00001182
Iteration 222/1000 | Loss: 0.00001182
Iteration 223/1000 | Loss: 0.00001181
Iteration 224/1000 | Loss: 0.00001181
Iteration 225/1000 | Loss: 0.00001181
Iteration 226/1000 | Loss: 0.00001181
Iteration 227/1000 | Loss: 0.00001181
Iteration 228/1000 | Loss: 0.00001181
Iteration 229/1000 | Loss: 0.00001180
Iteration 230/1000 | Loss: 0.00001180
Iteration 231/1000 | Loss: 0.00001180
Iteration 232/1000 | Loss: 0.00001179
Iteration 233/1000 | Loss: 0.00001179
Iteration 234/1000 | Loss: 0.00001179
Iteration 235/1000 | Loss: 0.00001179
Iteration 236/1000 | Loss: 0.00001178
Iteration 237/1000 | Loss: 0.00001178
Iteration 238/1000 | Loss: 0.00001178
Iteration 239/1000 | Loss: 0.00001178
Iteration 240/1000 | Loss: 0.00001177
Iteration 241/1000 | Loss: 0.00001177
Iteration 242/1000 | Loss: 0.00001177
Iteration 243/1000 | Loss: 0.00001177
Iteration 244/1000 | Loss: 0.00001177
Iteration 245/1000 | Loss: 0.00001177
Iteration 246/1000 | Loss: 0.00001176
Iteration 247/1000 | Loss: 0.00001176
Iteration 248/1000 | Loss: 0.00001176
Iteration 249/1000 | Loss: 0.00001176
Iteration 250/1000 | Loss: 0.00001176
Iteration 251/1000 | Loss: 0.00001176
Iteration 252/1000 | Loss: 0.00001176
Iteration 253/1000 | Loss: 0.00001175
Iteration 254/1000 | Loss: 0.00001175
Iteration 255/1000 | Loss: 0.00001175
Iteration 256/1000 | Loss: 0.00001175
Iteration 257/1000 | Loss: 0.00001174
Iteration 258/1000 | Loss: 0.00001174
Iteration 259/1000 | Loss: 0.00001174
Iteration 260/1000 | Loss: 0.00001174
Iteration 261/1000 | Loss: 0.00001174
Iteration 262/1000 | Loss: 0.00001174
Iteration 263/1000 | Loss: 0.00001174
Iteration 264/1000 | Loss: 0.00001173
Iteration 265/1000 | Loss: 0.00001173
Iteration 266/1000 | Loss: 0.00001173
Iteration 267/1000 | Loss: 0.00001173
Iteration 268/1000 | Loss: 0.00001173
Iteration 269/1000 | Loss: 0.00001173
Iteration 270/1000 | Loss: 0.00001173
Iteration 271/1000 | Loss: 0.00001173
Iteration 272/1000 | Loss: 0.00001173
Iteration 273/1000 | Loss: 0.00001173
Iteration 274/1000 | Loss: 0.00001172
Iteration 275/1000 | Loss: 0.00001172
Iteration 276/1000 | Loss: 0.00001172
Iteration 277/1000 | Loss: 0.00001172
Iteration 278/1000 | Loss: 0.00001172
Iteration 279/1000 | Loss: 0.00001172
Iteration 280/1000 | Loss: 0.00001172
Iteration 281/1000 | Loss: 0.00001172
Iteration 282/1000 | Loss: 0.00001172
Iteration 283/1000 | Loss: 0.00001172
Iteration 284/1000 | Loss: 0.00001171
Iteration 285/1000 | Loss: 0.00001171
Iteration 286/1000 | Loss: 0.00001171
Iteration 287/1000 | Loss: 0.00001171
Iteration 288/1000 | Loss: 0.00001171
Iteration 289/1000 | Loss: 0.00001171
Iteration 290/1000 | Loss: 0.00001171
Iteration 291/1000 | Loss: 0.00001171
Iteration 292/1000 | Loss: 0.00001171
Iteration 293/1000 | Loss: 0.00001171
Iteration 294/1000 | Loss: 0.00001171
Iteration 295/1000 | Loss: 0.00001171
Iteration 296/1000 | Loss: 0.00001171
Iteration 297/1000 | Loss: 0.00001170
Iteration 298/1000 | Loss: 0.00001170
Iteration 299/1000 | Loss: 0.00001170
Iteration 300/1000 | Loss: 0.00001170
Iteration 301/1000 | Loss: 0.00001170
Iteration 302/1000 | Loss: 0.00001170
Iteration 303/1000 | Loss: 0.00001170
Iteration 304/1000 | Loss: 0.00001170
Iteration 305/1000 | Loss: 0.00001170
Iteration 306/1000 | Loss: 0.00001170
Iteration 307/1000 | Loss: 0.00001170
Iteration 308/1000 | Loss: 0.00001170
Iteration 309/1000 | Loss: 0.00001170
Iteration 310/1000 | Loss: 0.00001170
Iteration 311/1000 | Loss: 0.00001170
Iteration 312/1000 | Loss: 0.00001170
Iteration 313/1000 | Loss: 0.00001170
Iteration 314/1000 | Loss: 0.00001170
Iteration 315/1000 | Loss: 0.00001170
Iteration 316/1000 | Loss: 0.00001170
Iteration 317/1000 | Loss: 0.00001169
Iteration 318/1000 | Loss: 0.00001169
Iteration 319/1000 | Loss: 0.00001169
Iteration 320/1000 | Loss: 0.00001169
Iteration 321/1000 | Loss: 0.00001169
Iteration 322/1000 | Loss: 0.00001169
Iteration 323/1000 | Loss: 0.00001169
Iteration 324/1000 | Loss: 0.00001169
Iteration 325/1000 | Loss: 0.00001169
Iteration 326/1000 | Loss: 0.00001169
Iteration 327/1000 | Loss: 0.00001169
Iteration 328/1000 | Loss: 0.00001169
Iteration 329/1000 | Loss: 0.00001169
Iteration 330/1000 | Loss: 0.00001169
Iteration 331/1000 | Loss: 0.00001169
Iteration 332/1000 | Loss: 0.00001169
Iteration 333/1000 | Loss: 0.00001169
Iteration 334/1000 | Loss: 0.00001169
Iteration 335/1000 | Loss: 0.00001169
Iteration 336/1000 | Loss: 0.00001169
Iteration 337/1000 | Loss: 0.00001169
Iteration 338/1000 | Loss: 0.00001169
Iteration 339/1000 | Loss: 0.00001169
Iteration 340/1000 | Loss: 0.00001169
Iteration 341/1000 | Loss: 0.00001169
Iteration 342/1000 | Loss: 0.00001169
Iteration 343/1000 | Loss: 0.00001169
Iteration 344/1000 | Loss: 0.00001169
Iteration 345/1000 | Loss: 0.00001169
Iteration 346/1000 | Loss: 0.00001169
Iteration 347/1000 | Loss: 0.00001169
Iteration 348/1000 | Loss: 0.00001169
Iteration 349/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 349. Stopping optimization.
Last 5 losses: [1.168977814813843e-05, 1.168977814813843e-05, 1.168977814813843e-05, 1.168977814813843e-05, 1.168977814813843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.168977814813843e-05

Optimization complete. Final v2v error: 2.864790201187134 mm

Highest mean error: 4.430999755859375 mm for frame 78

Lowest mean error: 2.4932565689086914 mm for frame 2

Saving results

Total time: 63.280444622039795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996864
Iteration 2/25 | Loss: 0.00300234
Iteration 3/25 | Loss: 0.00246084
Iteration 4/25 | Loss: 0.00201488
Iteration 5/25 | Loss: 0.00202697
Iteration 6/25 | Loss: 0.00181403
Iteration 7/25 | Loss: 0.00166312
Iteration 8/25 | Loss: 0.00163050
Iteration 9/25 | Loss: 0.00155319
Iteration 10/25 | Loss: 0.00152117
Iteration 11/25 | Loss: 0.00148228
Iteration 12/25 | Loss: 0.00147607
Iteration 13/25 | Loss: 0.00146828
Iteration 14/25 | Loss: 0.00147262
Iteration 15/25 | Loss: 0.00146466
Iteration 16/25 | Loss: 0.00146860
Iteration 17/25 | Loss: 0.00145844
Iteration 18/25 | Loss: 0.00146475
Iteration 19/25 | Loss: 0.00145575
Iteration 20/25 | Loss: 0.00144304
Iteration 21/25 | Loss: 0.00144171
Iteration 22/25 | Loss: 0.00144492
Iteration 23/25 | Loss: 0.00144609
Iteration 24/25 | Loss: 0.00144053
Iteration 25/25 | Loss: 0.00143999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30850840
Iteration 2/25 | Loss: 0.00260267
Iteration 3/25 | Loss: 0.00236697
Iteration 4/25 | Loss: 0.00236660
Iteration 5/25 | Loss: 0.00233797
Iteration 6/25 | Loss: 0.00233797
Iteration 7/25 | Loss: 0.00233797
Iteration 8/25 | Loss: 0.00233797
Iteration 9/25 | Loss: 0.00233797
Iteration 10/25 | Loss: 0.00233797
Iteration 11/25 | Loss: 0.00233797
Iteration 12/25 | Loss: 0.00233797
Iteration 13/25 | Loss: 0.00233797
Iteration 14/25 | Loss: 0.00233797
Iteration 15/25 | Loss: 0.00233797
Iteration 16/25 | Loss: 0.00233797
Iteration 17/25 | Loss: 0.00233797
Iteration 18/25 | Loss: 0.00233797
Iteration 19/25 | Loss: 0.00233797
Iteration 20/25 | Loss: 0.00233797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023379665799438953, 0.0023379665799438953, 0.0023379665799438953, 0.0023379665799438953, 0.0023379665799438953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023379665799438953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233797
Iteration 2/1000 | Loss: 0.00043949
Iteration 3/1000 | Loss: 0.00037505
Iteration 4/1000 | Loss: 0.00128988
Iteration 5/1000 | Loss: 0.00018907
Iteration 6/1000 | Loss: 0.00043027
Iteration 7/1000 | Loss: 0.00018051
Iteration 8/1000 | Loss: 0.00024010
Iteration 9/1000 | Loss: 0.00060626
Iteration 10/1000 | Loss: 0.00100491
Iteration 11/1000 | Loss: 0.00010084
Iteration 12/1000 | Loss: 0.00009044
Iteration 13/1000 | Loss: 0.00026812
Iteration 14/1000 | Loss: 0.00007107
Iteration 15/1000 | Loss: 0.00023923
Iteration 16/1000 | Loss: 0.00005601
Iteration 17/1000 | Loss: 0.00021793
Iteration 18/1000 | Loss: 0.00068629
Iteration 19/1000 | Loss: 0.00006688
Iteration 20/1000 | Loss: 0.00012576
Iteration 21/1000 | Loss: 0.00011844
Iteration 22/1000 | Loss: 0.00012790
Iteration 23/1000 | Loss: 0.00035958
Iteration 24/1000 | Loss: 0.00020242
Iteration 25/1000 | Loss: 0.00008253
Iteration 26/1000 | Loss: 0.00005257
Iteration 27/1000 | Loss: 0.00007296
Iteration 28/1000 | Loss: 0.00004934
Iteration 29/1000 | Loss: 0.00004791
Iteration 30/1000 | Loss: 0.00008019
Iteration 31/1000 | Loss: 0.00010186
Iteration 32/1000 | Loss: 0.00030807
Iteration 33/1000 | Loss: 0.00046997
Iteration 34/1000 | Loss: 0.00005312
Iteration 35/1000 | Loss: 0.00020795
Iteration 36/1000 | Loss: 0.00004461
Iteration 37/1000 | Loss: 0.00039735
Iteration 38/1000 | Loss: 0.00117449
Iteration 39/1000 | Loss: 0.00010691
Iteration 40/1000 | Loss: 0.00006656
Iteration 41/1000 | Loss: 0.00005350
Iteration 42/1000 | Loss: 0.00004154
Iteration 43/1000 | Loss: 0.00004269
Iteration 44/1000 | Loss: 0.00003473
Iteration 45/1000 | Loss: 0.00012191
Iteration 46/1000 | Loss: 0.00003274
Iteration 47/1000 | Loss: 0.00003693
Iteration 48/1000 | Loss: 0.00003260
Iteration 49/1000 | Loss: 0.00015262
Iteration 50/1000 | Loss: 0.00003853
Iteration 51/1000 | Loss: 0.00002859
Iteration 52/1000 | Loss: 0.00002797
Iteration 53/1000 | Loss: 0.00002747
Iteration 54/1000 | Loss: 0.00002713
Iteration 55/1000 | Loss: 0.00002684
Iteration 56/1000 | Loss: 0.00002682
Iteration 57/1000 | Loss: 0.00006025
Iteration 58/1000 | Loss: 0.00002951
Iteration 59/1000 | Loss: 0.00002738
Iteration 60/1000 | Loss: 0.00002654
Iteration 61/1000 | Loss: 0.00002633
Iteration 62/1000 | Loss: 0.00002626
Iteration 63/1000 | Loss: 0.00002605
Iteration 64/1000 | Loss: 0.00014397
Iteration 65/1000 | Loss: 0.00073680
Iteration 66/1000 | Loss: 0.00074835
Iteration 67/1000 | Loss: 0.00009626
Iteration 68/1000 | Loss: 0.00011632
Iteration 69/1000 | Loss: 0.00002620
Iteration 70/1000 | Loss: 0.00002589
Iteration 71/1000 | Loss: 0.00002584
Iteration 72/1000 | Loss: 0.00002579
Iteration 73/1000 | Loss: 0.00002577
Iteration 74/1000 | Loss: 0.00002575
Iteration 75/1000 | Loss: 0.00006258
Iteration 76/1000 | Loss: 0.00002580
Iteration 77/1000 | Loss: 0.00002559
Iteration 78/1000 | Loss: 0.00002559
Iteration 79/1000 | Loss: 0.00002558
Iteration 80/1000 | Loss: 0.00002558
Iteration 81/1000 | Loss: 0.00002558
Iteration 82/1000 | Loss: 0.00002558
Iteration 83/1000 | Loss: 0.00002558
Iteration 84/1000 | Loss: 0.00002558
Iteration 85/1000 | Loss: 0.00002557
Iteration 86/1000 | Loss: 0.00002557
Iteration 87/1000 | Loss: 0.00002556
Iteration 88/1000 | Loss: 0.00002556
Iteration 89/1000 | Loss: 0.00002555
Iteration 90/1000 | Loss: 0.00002555
Iteration 91/1000 | Loss: 0.00002555
Iteration 92/1000 | Loss: 0.00002555
Iteration 93/1000 | Loss: 0.00002555
Iteration 94/1000 | Loss: 0.00002555
Iteration 95/1000 | Loss: 0.00004970
Iteration 96/1000 | Loss: 0.00003035
Iteration 97/1000 | Loss: 0.00002556
Iteration 98/1000 | Loss: 0.00002555
Iteration 99/1000 | Loss: 0.00002555
Iteration 100/1000 | Loss: 0.00002555
Iteration 101/1000 | Loss: 0.00002555
Iteration 102/1000 | Loss: 0.00002555
Iteration 103/1000 | Loss: 0.00002555
Iteration 104/1000 | Loss: 0.00002554
Iteration 105/1000 | Loss: 0.00002554
Iteration 106/1000 | Loss: 0.00002554
Iteration 107/1000 | Loss: 0.00002553
Iteration 108/1000 | Loss: 0.00002553
Iteration 109/1000 | Loss: 0.00002553
Iteration 110/1000 | Loss: 0.00002553
Iteration 111/1000 | Loss: 0.00002553
Iteration 112/1000 | Loss: 0.00002552
Iteration 113/1000 | Loss: 0.00002552
Iteration 114/1000 | Loss: 0.00002552
Iteration 115/1000 | Loss: 0.00002552
Iteration 116/1000 | Loss: 0.00002552
Iteration 117/1000 | Loss: 0.00002552
Iteration 118/1000 | Loss: 0.00002552
Iteration 119/1000 | Loss: 0.00002552
Iteration 120/1000 | Loss: 0.00002552
Iteration 121/1000 | Loss: 0.00002552
Iteration 122/1000 | Loss: 0.00002552
Iteration 123/1000 | Loss: 0.00002552
Iteration 124/1000 | Loss: 0.00002551
Iteration 125/1000 | Loss: 0.00002551
Iteration 126/1000 | Loss: 0.00002550
Iteration 127/1000 | Loss: 0.00002550
Iteration 128/1000 | Loss: 0.00002549
Iteration 129/1000 | Loss: 0.00002549
Iteration 130/1000 | Loss: 0.00002549
Iteration 131/1000 | Loss: 0.00002548
Iteration 132/1000 | Loss: 0.00002548
Iteration 133/1000 | Loss: 0.00002547
Iteration 134/1000 | Loss: 0.00002547
Iteration 135/1000 | Loss: 0.00002546
Iteration 136/1000 | Loss: 0.00002546
Iteration 137/1000 | Loss: 0.00002545
Iteration 138/1000 | Loss: 0.00002545
Iteration 139/1000 | Loss: 0.00002545
Iteration 140/1000 | Loss: 0.00002544
Iteration 141/1000 | Loss: 0.00002544
Iteration 142/1000 | Loss: 0.00002544
Iteration 143/1000 | Loss: 0.00002543
Iteration 144/1000 | Loss: 0.00002543
Iteration 145/1000 | Loss: 0.00002543
Iteration 146/1000 | Loss: 0.00002543
Iteration 147/1000 | Loss: 0.00002543
Iteration 148/1000 | Loss: 0.00002543
Iteration 149/1000 | Loss: 0.00002543
Iteration 150/1000 | Loss: 0.00002542
Iteration 151/1000 | Loss: 0.00002542
Iteration 152/1000 | Loss: 0.00002542
Iteration 153/1000 | Loss: 0.00002541
Iteration 154/1000 | Loss: 0.00002541
Iteration 155/1000 | Loss: 0.00002541
Iteration 156/1000 | Loss: 0.00002541
Iteration 157/1000 | Loss: 0.00002541
Iteration 158/1000 | Loss: 0.00002540
Iteration 159/1000 | Loss: 0.00002540
Iteration 160/1000 | Loss: 0.00002540
Iteration 161/1000 | Loss: 0.00002540
Iteration 162/1000 | Loss: 0.00002540
Iteration 163/1000 | Loss: 0.00002540
Iteration 164/1000 | Loss: 0.00002540
Iteration 165/1000 | Loss: 0.00002540
Iteration 166/1000 | Loss: 0.00002540
Iteration 167/1000 | Loss: 0.00002540
Iteration 168/1000 | Loss: 0.00002540
Iteration 169/1000 | Loss: 0.00002540
Iteration 170/1000 | Loss: 0.00002540
Iteration 171/1000 | Loss: 0.00002540
Iteration 172/1000 | Loss: 0.00002540
Iteration 173/1000 | Loss: 0.00002540
Iteration 174/1000 | Loss: 0.00002540
Iteration 175/1000 | Loss: 0.00002540
Iteration 176/1000 | Loss: 0.00002540
Iteration 177/1000 | Loss: 0.00002540
Iteration 178/1000 | Loss: 0.00002540
Iteration 179/1000 | Loss: 0.00002540
Iteration 180/1000 | Loss: 0.00002540
Iteration 181/1000 | Loss: 0.00002540
Iteration 182/1000 | Loss: 0.00002540
Iteration 183/1000 | Loss: 0.00002540
Iteration 184/1000 | Loss: 0.00002540
Iteration 185/1000 | Loss: 0.00002540
Iteration 186/1000 | Loss: 0.00002540
Iteration 187/1000 | Loss: 0.00002540
Iteration 188/1000 | Loss: 0.00002540
Iteration 189/1000 | Loss: 0.00002540
Iteration 190/1000 | Loss: 0.00002540
Iteration 191/1000 | Loss: 0.00002540
Iteration 192/1000 | Loss: 0.00002540
Iteration 193/1000 | Loss: 0.00002540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.5400939193787053e-05, 2.5400939193787053e-05, 2.5400939193787053e-05, 2.5400939193787053e-05, 2.5400939193787053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5400939193787053e-05

Optimization complete. Final v2v error: 3.846734046936035 mm

Highest mean error: 11.303799629211426 mm for frame 195

Lowest mean error: 3.3136730194091797 mm for frame 133

Saving results

Total time: 181.6679172515869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486804
Iteration 2/25 | Loss: 0.00148543
Iteration 3/25 | Loss: 0.00139477
Iteration 4/25 | Loss: 0.00138496
Iteration 5/25 | Loss: 0.00138262
Iteration 6/25 | Loss: 0.00138262
Iteration 7/25 | Loss: 0.00138262
Iteration 8/25 | Loss: 0.00138262
Iteration 9/25 | Loss: 0.00138262
Iteration 10/25 | Loss: 0.00138262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013826225185766816, 0.0013826225185766816, 0.0013826225185766816, 0.0013826225185766816, 0.0013826225185766816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013826225185766816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23103678
Iteration 2/25 | Loss: 0.00215121
Iteration 3/25 | Loss: 0.00215119
Iteration 4/25 | Loss: 0.00215119
Iteration 5/25 | Loss: 0.00215119
Iteration 6/25 | Loss: 0.00215119
Iteration 7/25 | Loss: 0.00215119
Iteration 8/25 | Loss: 0.00215119
Iteration 9/25 | Loss: 0.00215119
Iteration 10/25 | Loss: 0.00215119
Iteration 11/25 | Loss: 0.00215119
Iteration 12/25 | Loss: 0.00215119
Iteration 13/25 | Loss: 0.00215118
Iteration 14/25 | Loss: 0.00215118
Iteration 15/25 | Loss: 0.00215118
Iteration 16/25 | Loss: 0.00215118
Iteration 17/25 | Loss: 0.00215118
Iteration 18/25 | Loss: 0.00215118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021511847153306007, 0.0021511847153306007, 0.0021511847153306007, 0.0021511847153306007, 0.0021511847153306007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021511847153306007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215118
Iteration 2/1000 | Loss: 0.00003924
Iteration 3/1000 | Loss: 0.00002680
Iteration 4/1000 | Loss: 0.00002288
Iteration 5/1000 | Loss: 0.00002096
Iteration 6/1000 | Loss: 0.00001972
Iteration 7/1000 | Loss: 0.00001895
Iteration 8/1000 | Loss: 0.00001849
Iteration 9/1000 | Loss: 0.00001817
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001731
Iteration 13/1000 | Loss: 0.00001725
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001704
Iteration 16/1000 | Loss: 0.00001703
Iteration 17/1000 | Loss: 0.00001702
Iteration 18/1000 | Loss: 0.00001688
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001677
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001656
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001655
Iteration 30/1000 | Loss: 0.00001651
Iteration 31/1000 | Loss: 0.00001650
Iteration 32/1000 | Loss: 0.00001650
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001645
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001645
Iteration 38/1000 | Loss: 0.00001645
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001643
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001642
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001641
Iteration 49/1000 | Loss: 0.00001641
Iteration 50/1000 | Loss: 0.00001641
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001630
Iteration 80/1000 | Loss: 0.00001630
Iteration 81/1000 | Loss: 0.00001629
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001628
Iteration 85/1000 | Loss: 0.00001628
Iteration 86/1000 | Loss: 0.00001628
Iteration 87/1000 | Loss: 0.00001628
Iteration 88/1000 | Loss: 0.00001628
Iteration 89/1000 | Loss: 0.00001628
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001628
Iteration 93/1000 | Loss: 0.00001627
Iteration 94/1000 | Loss: 0.00001627
Iteration 95/1000 | Loss: 0.00001627
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001627
Iteration 99/1000 | Loss: 0.00001627
Iteration 100/1000 | Loss: 0.00001627
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001627
Iteration 109/1000 | Loss: 0.00001627
Iteration 110/1000 | Loss: 0.00001627
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001624
Iteration 117/1000 | Loss: 0.00001624
Iteration 118/1000 | Loss: 0.00001624
Iteration 119/1000 | Loss: 0.00001624
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001623
Iteration 122/1000 | Loss: 0.00001623
Iteration 123/1000 | Loss: 0.00001623
Iteration 124/1000 | Loss: 0.00001623
Iteration 125/1000 | Loss: 0.00001623
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001622
Iteration 129/1000 | Loss: 0.00001622
Iteration 130/1000 | Loss: 0.00001622
Iteration 131/1000 | Loss: 0.00001621
Iteration 132/1000 | Loss: 0.00001621
Iteration 133/1000 | Loss: 0.00001621
Iteration 134/1000 | Loss: 0.00001621
Iteration 135/1000 | Loss: 0.00001621
Iteration 136/1000 | Loss: 0.00001621
Iteration 137/1000 | Loss: 0.00001621
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001620
Iteration 142/1000 | Loss: 0.00001620
Iteration 143/1000 | Loss: 0.00001620
Iteration 144/1000 | Loss: 0.00001620
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001619
Iteration 159/1000 | Loss: 0.00001619
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001619
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001618
Iteration 174/1000 | Loss: 0.00001618
Iteration 175/1000 | Loss: 0.00001618
Iteration 176/1000 | Loss: 0.00001618
Iteration 177/1000 | Loss: 0.00001618
Iteration 178/1000 | Loss: 0.00001618
Iteration 179/1000 | Loss: 0.00001618
Iteration 180/1000 | Loss: 0.00001618
Iteration 181/1000 | Loss: 0.00001618
Iteration 182/1000 | Loss: 0.00001618
Iteration 183/1000 | Loss: 0.00001618
Iteration 184/1000 | Loss: 0.00001618
Iteration 185/1000 | Loss: 0.00001618
Iteration 186/1000 | Loss: 0.00001618
Iteration 187/1000 | Loss: 0.00001618
Iteration 188/1000 | Loss: 0.00001618
Iteration 189/1000 | Loss: 0.00001618
Iteration 190/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.6178499208763242e-05, 1.6178499208763242e-05, 1.6178499208763242e-05, 1.6178499208763242e-05, 1.6178499208763242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6178499208763242e-05

Optimization complete. Final v2v error: 3.327183723449707 mm

Highest mean error: 4.046514987945557 mm for frame 2

Lowest mean error: 2.9871745109558105 mm for frame 146

Saving results

Total time: 51.46430015563965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988274
Iteration 2/25 | Loss: 0.00226514
Iteration 3/25 | Loss: 0.00181973
Iteration 4/25 | Loss: 0.00173122
Iteration 5/25 | Loss: 0.00182344
Iteration 6/25 | Loss: 0.00164776
Iteration 7/25 | Loss: 0.00151282
Iteration 8/25 | Loss: 0.00147554
Iteration 9/25 | Loss: 0.00142672
Iteration 10/25 | Loss: 0.00140468
Iteration 11/25 | Loss: 0.00138776
Iteration 12/25 | Loss: 0.00137663
Iteration 13/25 | Loss: 0.00137036
Iteration 14/25 | Loss: 0.00136774
Iteration 15/25 | Loss: 0.00136609
Iteration 16/25 | Loss: 0.00136321
Iteration 17/25 | Loss: 0.00136053
Iteration 18/25 | Loss: 0.00135768
Iteration 19/25 | Loss: 0.00135642
Iteration 20/25 | Loss: 0.00135548
Iteration 21/25 | Loss: 0.00135503
Iteration 22/25 | Loss: 0.00135486
Iteration 23/25 | Loss: 0.00135481
Iteration 24/25 | Loss: 0.00135480
Iteration 25/25 | Loss: 0.00135479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30048335
Iteration 2/25 | Loss: 0.00176514
Iteration 3/25 | Loss: 0.00176513
Iteration 4/25 | Loss: 0.00176513
Iteration 5/25 | Loss: 0.00176513
Iteration 6/25 | Loss: 0.00176513
Iteration 7/25 | Loss: 0.00176513
Iteration 8/25 | Loss: 0.00176513
Iteration 9/25 | Loss: 0.00176513
Iteration 10/25 | Loss: 0.00176513
Iteration 11/25 | Loss: 0.00176513
Iteration 12/25 | Loss: 0.00176513
Iteration 13/25 | Loss: 0.00176513
Iteration 14/25 | Loss: 0.00176513
Iteration 15/25 | Loss: 0.00176513
Iteration 16/25 | Loss: 0.00176513
Iteration 17/25 | Loss: 0.00176513
Iteration 18/25 | Loss: 0.00176513
Iteration 19/25 | Loss: 0.00176513
Iteration 20/25 | Loss: 0.00176513
Iteration 21/25 | Loss: 0.00176513
Iteration 22/25 | Loss: 0.00176513
Iteration 23/25 | Loss: 0.00176513
Iteration 24/25 | Loss: 0.00176513
Iteration 25/25 | Loss: 0.00176513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176513
Iteration 2/1000 | Loss: 0.00003230
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00002246
Iteration 5/1000 | Loss: 0.00002146
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00031429
Iteration 8/1000 | Loss: 0.00031194
Iteration 9/1000 | Loss: 0.00037330
Iteration 10/1000 | Loss: 0.00031441
Iteration 11/1000 | Loss: 0.00038117
Iteration 12/1000 | Loss: 0.00026343
Iteration 13/1000 | Loss: 0.00026386
Iteration 14/1000 | Loss: 0.00003167
Iteration 15/1000 | Loss: 0.00002370
Iteration 16/1000 | Loss: 0.00002173
Iteration 17/1000 | Loss: 0.00002019
Iteration 18/1000 | Loss: 0.00002165
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001806
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001724
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001696
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001647
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001633
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001622
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00014531
Iteration 40/1000 | Loss: 0.00003305
Iteration 41/1000 | Loss: 0.00001986
Iteration 42/1000 | Loss: 0.00004362
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001588
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001554
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001552
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001551
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001544
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001543
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001543
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001542
Iteration 106/1000 | Loss: 0.00001542
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001538
Iteration 128/1000 | Loss: 0.00001537
Iteration 129/1000 | Loss: 0.00001537
Iteration 130/1000 | Loss: 0.00001537
Iteration 131/1000 | Loss: 0.00001537
Iteration 132/1000 | Loss: 0.00001537
Iteration 133/1000 | Loss: 0.00001536
Iteration 134/1000 | Loss: 0.00001536
Iteration 135/1000 | Loss: 0.00001536
Iteration 136/1000 | Loss: 0.00001536
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001535
Iteration 139/1000 | Loss: 0.00001535
Iteration 140/1000 | Loss: 0.00001535
Iteration 141/1000 | Loss: 0.00001535
Iteration 142/1000 | Loss: 0.00001535
Iteration 143/1000 | Loss: 0.00001535
Iteration 144/1000 | Loss: 0.00001535
Iteration 145/1000 | Loss: 0.00001535
Iteration 146/1000 | Loss: 0.00001535
Iteration 147/1000 | Loss: 0.00001535
Iteration 148/1000 | Loss: 0.00001534
Iteration 149/1000 | Loss: 0.00001534
Iteration 150/1000 | Loss: 0.00001534
Iteration 151/1000 | Loss: 0.00001534
Iteration 152/1000 | Loss: 0.00001534
Iteration 153/1000 | Loss: 0.00001534
Iteration 154/1000 | Loss: 0.00001534
Iteration 155/1000 | Loss: 0.00001534
Iteration 156/1000 | Loss: 0.00001534
Iteration 157/1000 | Loss: 0.00001534
Iteration 158/1000 | Loss: 0.00001533
Iteration 159/1000 | Loss: 0.00001533
Iteration 160/1000 | Loss: 0.00001533
Iteration 161/1000 | Loss: 0.00001533
Iteration 162/1000 | Loss: 0.00001533
Iteration 163/1000 | Loss: 0.00001533
Iteration 164/1000 | Loss: 0.00001533
Iteration 165/1000 | Loss: 0.00001533
Iteration 166/1000 | Loss: 0.00001533
Iteration 167/1000 | Loss: 0.00001532
Iteration 168/1000 | Loss: 0.00001532
Iteration 169/1000 | Loss: 0.00001532
Iteration 170/1000 | Loss: 0.00001532
Iteration 171/1000 | Loss: 0.00001532
Iteration 172/1000 | Loss: 0.00001532
Iteration 173/1000 | Loss: 0.00001532
Iteration 174/1000 | Loss: 0.00001532
Iteration 175/1000 | Loss: 0.00001532
Iteration 176/1000 | Loss: 0.00001531
Iteration 177/1000 | Loss: 0.00001531
Iteration 178/1000 | Loss: 0.00001531
Iteration 179/1000 | Loss: 0.00001531
Iteration 180/1000 | Loss: 0.00001531
Iteration 181/1000 | Loss: 0.00001531
Iteration 182/1000 | Loss: 0.00001531
Iteration 183/1000 | Loss: 0.00001531
Iteration 184/1000 | Loss: 0.00001531
Iteration 185/1000 | Loss: 0.00001531
Iteration 186/1000 | Loss: 0.00001531
Iteration 187/1000 | Loss: 0.00001531
Iteration 188/1000 | Loss: 0.00001531
Iteration 189/1000 | Loss: 0.00001531
Iteration 190/1000 | Loss: 0.00001531
Iteration 191/1000 | Loss: 0.00001531
Iteration 192/1000 | Loss: 0.00001531
Iteration 193/1000 | Loss: 0.00001531
Iteration 194/1000 | Loss: 0.00001531
Iteration 195/1000 | Loss: 0.00001531
Iteration 196/1000 | Loss: 0.00001531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.531302223156672e-05, 1.531302223156672e-05, 1.531302223156672e-05, 1.531302223156672e-05, 1.531302223156672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.531302223156672e-05

Optimization complete. Final v2v error: 3.3328309059143066 mm

Highest mean error: 4.2747697830200195 mm for frame 153

Lowest mean error: 3.1411726474761963 mm for frame 44

Saving results

Total time: 105.75161981582642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684498
Iteration 2/25 | Loss: 0.00170580
Iteration 3/25 | Loss: 0.00150518
Iteration 4/25 | Loss: 0.00147539
Iteration 5/25 | Loss: 0.00146495
Iteration 6/25 | Loss: 0.00146305
Iteration 7/25 | Loss: 0.00146305
Iteration 8/25 | Loss: 0.00146305
Iteration 9/25 | Loss: 0.00146305
Iteration 10/25 | Loss: 0.00146305
Iteration 11/25 | Loss: 0.00146305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014630465302616358, 0.0014630465302616358, 0.0014630465302616358, 0.0014630465302616358, 0.0014630465302616358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014630465302616358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53183317
Iteration 2/25 | Loss: 0.00252221
Iteration 3/25 | Loss: 0.00252220
Iteration 4/25 | Loss: 0.00252220
Iteration 5/25 | Loss: 0.00252219
Iteration 6/25 | Loss: 0.00252219
Iteration 7/25 | Loss: 0.00252219
Iteration 8/25 | Loss: 0.00252219
Iteration 9/25 | Loss: 0.00252219
Iteration 10/25 | Loss: 0.00252219
Iteration 11/25 | Loss: 0.00252219
Iteration 12/25 | Loss: 0.00252219
Iteration 13/25 | Loss: 0.00252219
Iteration 14/25 | Loss: 0.00252219
Iteration 15/25 | Loss: 0.00252219
Iteration 16/25 | Loss: 0.00252219
Iteration 17/25 | Loss: 0.00252219
Iteration 18/25 | Loss: 0.00252219
Iteration 19/25 | Loss: 0.00252219
Iteration 20/25 | Loss: 0.00252219
Iteration 21/25 | Loss: 0.00252219
Iteration 22/25 | Loss: 0.00252219
Iteration 23/25 | Loss: 0.00252219
Iteration 24/25 | Loss: 0.00252219
Iteration 25/25 | Loss: 0.00252219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252219
Iteration 2/1000 | Loss: 0.00008350
Iteration 3/1000 | Loss: 0.00005076
Iteration 4/1000 | Loss: 0.00003885
Iteration 5/1000 | Loss: 0.00003583
Iteration 6/1000 | Loss: 0.00003397
Iteration 7/1000 | Loss: 0.00003292
Iteration 8/1000 | Loss: 0.00003201
Iteration 9/1000 | Loss: 0.00003143
Iteration 10/1000 | Loss: 0.00003098
Iteration 11/1000 | Loss: 0.00003066
Iteration 12/1000 | Loss: 0.00003038
Iteration 13/1000 | Loss: 0.00003038
Iteration 14/1000 | Loss: 0.00003017
Iteration 15/1000 | Loss: 0.00002998
Iteration 16/1000 | Loss: 0.00002985
Iteration 17/1000 | Loss: 0.00002974
Iteration 18/1000 | Loss: 0.00002964
Iteration 19/1000 | Loss: 0.00002955
Iteration 20/1000 | Loss: 0.00002951
Iteration 21/1000 | Loss: 0.00002948
Iteration 22/1000 | Loss: 0.00002943
Iteration 23/1000 | Loss: 0.00002942
Iteration 24/1000 | Loss: 0.00002942
Iteration 25/1000 | Loss: 0.00002941
Iteration 26/1000 | Loss: 0.00002941
Iteration 27/1000 | Loss: 0.00002940
Iteration 28/1000 | Loss: 0.00002940
Iteration 29/1000 | Loss: 0.00002933
Iteration 30/1000 | Loss: 0.00002932
Iteration 31/1000 | Loss: 0.00002931
Iteration 32/1000 | Loss: 0.00002931
Iteration 33/1000 | Loss: 0.00002930
Iteration 34/1000 | Loss: 0.00002930
Iteration 35/1000 | Loss: 0.00002930
Iteration 36/1000 | Loss: 0.00002929
Iteration 37/1000 | Loss: 0.00002928
Iteration 38/1000 | Loss: 0.00002928
Iteration 39/1000 | Loss: 0.00002928
Iteration 40/1000 | Loss: 0.00002926
Iteration 41/1000 | Loss: 0.00002926
Iteration 42/1000 | Loss: 0.00002926
Iteration 43/1000 | Loss: 0.00002926
Iteration 44/1000 | Loss: 0.00002925
Iteration 45/1000 | Loss: 0.00002925
Iteration 46/1000 | Loss: 0.00002924
Iteration 47/1000 | Loss: 0.00002924
Iteration 48/1000 | Loss: 0.00002923
Iteration 49/1000 | Loss: 0.00002923
Iteration 50/1000 | Loss: 0.00002922
Iteration 51/1000 | Loss: 0.00002922
Iteration 52/1000 | Loss: 0.00002921
Iteration 53/1000 | Loss: 0.00002921
Iteration 54/1000 | Loss: 0.00002921
Iteration 55/1000 | Loss: 0.00002920
Iteration 56/1000 | Loss: 0.00002920
Iteration 57/1000 | Loss: 0.00002920
Iteration 58/1000 | Loss: 0.00002919
Iteration 59/1000 | Loss: 0.00002919
Iteration 60/1000 | Loss: 0.00002918
Iteration 61/1000 | Loss: 0.00002918
Iteration 62/1000 | Loss: 0.00002918
Iteration 63/1000 | Loss: 0.00002918
Iteration 64/1000 | Loss: 0.00002918
Iteration 65/1000 | Loss: 0.00002918
Iteration 66/1000 | Loss: 0.00002918
Iteration 67/1000 | Loss: 0.00002918
Iteration 68/1000 | Loss: 0.00002917
Iteration 69/1000 | Loss: 0.00002916
Iteration 70/1000 | Loss: 0.00002916
Iteration 71/1000 | Loss: 0.00002916
Iteration 72/1000 | Loss: 0.00002915
Iteration 73/1000 | Loss: 0.00002915
Iteration 74/1000 | Loss: 0.00002915
Iteration 75/1000 | Loss: 0.00002914
Iteration 76/1000 | Loss: 0.00002914
Iteration 77/1000 | Loss: 0.00002914
Iteration 78/1000 | Loss: 0.00002913
Iteration 79/1000 | Loss: 0.00002913
Iteration 80/1000 | Loss: 0.00002913
Iteration 81/1000 | Loss: 0.00002913
Iteration 82/1000 | Loss: 0.00002913
Iteration 83/1000 | Loss: 0.00002912
Iteration 84/1000 | Loss: 0.00002912
Iteration 85/1000 | Loss: 0.00002912
Iteration 86/1000 | Loss: 0.00002911
Iteration 87/1000 | Loss: 0.00002911
Iteration 88/1000 | Loss: 0.00002911
Iteration 89/1000 | Loss: 0.00002911
Iteration 90/1000 | Loss: 0.00002910
Iteration 91/1000 | Loss: 0.00002910
Iteration 92/1000 | Loss: 0.00002910
Iteration 93/1000 | Loss: 0.00002910
Iteration 94/1000 | Loss: 0.00002909
Iteration 95/1000 | Loss: 0.00002909
Iteration 96/1000 | Loss: 0.00002909
Iteration 97/1000 | Loss: 0.00002909
Iteration 98/1000 | Loss: 0.00002908
Iteration 99/1000 | Loss: 0.00002908
Iteration 100/1000 | Loss: 0.00002908
Iteration 101/1000 | Loss: 0.00002907
Iteration 102/1000 | Loss: 0.00002907
Iteration 103/1000 | Loss: 0.00002907
Iteration 104/1000 | Loss: 0.00002907
Iteration 105/1000 | Loss: 0.00002907
Iteration 106/1000 | Loss: 0.00002907
Iteration 107/1000 | Loss: 0.00002907
Iteration 108/1000 | Loss: 0.00002907
Iteration 109/1000 | Loss: 0.00002906
Iteration 110/1000 | Loss: 0.00002906
Iteration 111/1000 | Loss: 0.00002906
Iteration 112/1000 | Loss: 0.00002906
Iteration 113/1000 | Loss: 0.00002906
Iteration 114/1000 | Loss: 0.00002906
Iteration 115/1000 | Loss: 0.00002906
Iteration 116/1000 | Loss: 0.00002906
Iteration 117/1000 | Loss: 0.00002906
Iteration 118/1000 | Loss: 0.00002906
Iteration 119/1000 | Loss: 0.00002906
Iteration 120/1000 | Loss: 0.00002906
Iteration 121/1000 | Loss: 0.00002905
Iteration 122/1000 | Loss: 0.00002905
Iteration 123/1000 | Loss: 0.00002905
Iteration 124/1000 | Loss: 0.00002905
Iteration 125/1000 | Loss: 0.00002905
Iteration 126/1000 | Loss: 0.00002905
Iteration 127/1000 | Loss: 0.00002905
Iteration 128/1000 | Loss: 0.00002905
Iteration 129/1000 | Loss: 0.00002905
Iteration 130/1000 | Loss: 0.00002904
Iteration 131/1000 | Loss: 0.00002904
Iteration 132/1000 | Loss: 0.00002904
Iteration 133/1000 | Loss: 0.00002904
Iteration 134/1000 | Loss: 0.00002903
Iteration 135/1000 | Loss: 0.00002903
Iteration 136/1000 | Loss: 0.00002903
Iteration 137/1000 | Loss: 0.00002903
Iteration 138/1000 | Loss: 0.00002902
Iteration 139/1000 | Loss: 0.00002902
Iteration 140/1000 | Loss: 0.00002902
Iteration 141/1000 | Loss: 0.00002902
Iteration 142/1000 | Loss: 0.00002902
Iteration 143/1000 | Loss: 0.00002901
Iteration 144/1000 | Loss: 0.00002901
Iteration 145/1000 | Loss: 0.00002901
Iteration 146/1000 | Loss: 0.00002901
Iteration 147/1000 | Loss: 0.00002901
Iteration 148/1000 | Loss: 0.00002901
Iteration 149/1000 | Loss: 0.00002901
Iteration 150/1000 | Loss: 0.00002901
Iteration 151/1000 | Loss: 0.00002901
Iteration 152/1000 | Loss: 0.00002900
Iteration 153/1000 | Loss: 0.00002900
Iteration 154/1000 | Loss: 0.00002900
Iteration 155/1000 | Loss: 0.00002900
Iteration 156/1000 | Loss: 0.00002899
Iteration 157/1000 | Loss: 0.00002899
Iteration 158/1000 | Loss: 0.00002899
Iteration 159/1000 | Loss: 0.00002899
Iteration 160/1000 | Loss: 0.00002899
Iteration 161/1000 | Loss: 0.00002899
Iteration 162/1000 | Loss: 0.00002899
Iteration 163/1000 | Loss: 0.00002899
Iteration 164/1000 | Loss: 0.00002899
Iteration 165/1000 | Loss: 0.00002899
Iteration 166/1000 | Loss: 0.00002899
Iteration 167/1000 | Loss: 0.00002899
Iteration 168/1000 | Loss: 0.00002898
Iteration 169/1000 | Loss: 0.00002898
Iteration 170/1000 | Loss: 0.00002898
Iteration 171/1000 | Loss: 0.00002898
Iteration 172/1000 | Loss: 0.00002898
Iteration 173/1000 | Loss: 0.00002898
Iteration 174/1000 | Loss: 0.00002898
Iteration 175/1000 | Loss: 0.00002898
Iteration 176/1000 | Loss: 0.00002898
Iteration 177/1000 | Loss: 0.00002898
Iteration 178/1000 | Loss: 0.00002898
Iteration 179/1000 | Loss: 0.00002898
Iteration 180/1000 | Loss: 0.00002898
Iteration 181/1000 | Loss: 0.00002898
Iteration 182/1000 | Loss: 0.00002898
Iteration 183/1000 | Loss: 0.00002898
Iteration 184/1000 | Loss: 0.00002898
Iteration 185/1000 | Loss: 0.00002898
Iteration 186/1000 | Loss: 0.00002897
Iteration 187/1000 | Loss: 0.00002897
Iteration 188/1000 | Loss: 0.00002897
Iteration 189/1000 | Loss: 0.00002897
Iteration 190/1000 | Loss: 0.00002897
Iteration 191/1000 | Loss: 0.00002897
Iteration 192/1000 | Loss: 0.00002897
Iteration 193/1000 | Loss: 0.00002897
Iteration 194/1000 | Loss: 0.00002897
Iteration 195/1000 | Loss: 0.00002897
Iteration 196/1000 | Loss: 0.00002897
Iteration 197/1000 | Loss: 0.00002897
Iteration 198/1000 | Loss: 0.00002897
Iteration 199/1000 | Loss: 0.00002896
Iteration 200/1000 | Loss: 0.00002896
Iteration 201/1000 | Loss: 0.00002896
Iteration 202/1000 | Loss: 0.00002896
Iteration 203/1000 | Loss: 0.00002896
Iteration 204/1000 | Loss: 0.00002896
Iteration 205/1000 | Loss: 0.00002896
Iteration 206/1000 | Loss: 0.00002896
Iteration 207/1000 | Loss: 0.00002896
Iteration 208/1000 | Loss: 0.00002896
Iteration 209/1000 | Loss: 0.00002896
Iteration 210/1000 | Loss: 0.00002896
Iteration 211/1000 | Loss: 0.00002896
Iteration 212/1000 | Loss: 0.00002896
Iteration 213/1000 | Loss: 0.00002896
Iteration 214/1000 | Loss: 0.00002896
Iteration 215/1000 | Loss: 0.00002896
Iteration 216/1000 | Loss: 0.00002896
Iteration 217/1000 | Loss: 0.00002896
Iteration 218/1000 | Loss: 0.00002896
Iteration 219/1000 | Loss: 0.00002896
Iteration 220/1000 | Loss: 0.00002896
Iteration 221/1000 | Loss: 0.00002896
Iteration 222/1000 | Loss: 0.00002896
Iteration 223/1000 | Loss: 0.00002896
Iteration 224/1000 | Loss: 0.00002896
Iteration 225/1000 | Loss: 0.00002896
Iteration 226/1000 | Loss: 0.00002896
Iteration 227/1000 | Loss: 0.00002896
Iteration 228/1000 | Loss: 0.00002896
Iteration 229/1000 | Loss: 0.00002896
Iteration 230/1000 | Loss: 0.00002896
Iteration 231/1000 | Loss: 0.00002896
Iteration 232/1000 | Loss: 0.00002896
Iteration 233/1000 | Loss: 0.00002896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.895846773753874e-05, 2.895846773753874e-05, 2.895846773753874e-05, 2.895846773753874e-05, 2.895846773753874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.895846773753874e-05

Optimization complete. Final v2v error: 4.352977752685547 mm

Highest mean error: 6.126803874969482 mm for frame 157

Lowest mean error: 2.846435070037842 mm for frame 233

Saving results

Total time: 58.13280391693115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892860
Iteration 2/25 | Loss: 0.00197271
Iteration 3/25 | Loss: 0.00164713
Iteration 4/25 | Loss: 0.00158842
Iteration 5/25 | Loss: 0.00157164
Iteration 6/25 | Loss: 0.00149540
Iteration 7/25 | Loss: 0.00146950
Iteration 8/25 | Loss: 0.00145860
Iteration 9/25 | Loss: 0.00146273
Iteration 10/25 | Loss: 0.00146946
Iteration 11/25 | Loss: 0.00146342
Iteration 12/25 | Loss: 0.00146594
Iteration 13/25 | Loss: 0.00145347
Iteration 14/25 | Loss: 0.00144135
Iteration 15/25 | Loss: 0.00143267
Iteration 16/25 | Loss: 0.00143332
Iteration 17/25 | Loss: 0.00143189
Iteration 18/25 | Loss: 0.00142952
Iteration 19/25 | Loss: 0.00142739
Iteration 20/25 | Loss: 0.00142674
Iteration 21/25 | Loss: 0.00142646
Iteration 22/25 | Loss: 0.00142644
Iteration 23/25 | Loss: 0.00142644
Iteration 24/25 | Loss: 0.00142643
Iteration 25/25 | Loss: 0.00142643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33870053
Iteration 2/25 | Loss: 0.00200963
Iteration 3/25 | Loss: 0.00200956
Iteration 4/25 | Loss: 0.00200956
Iteration 5/25 | Loss: 0.00200956
Iteration 6/25 | Loss: 0.00200956
Iteration 7/25 | Loss: 0.00200956
Iteration 8/25 | Loss: 0.00200956
Iteration 9/25 | Loss: 0.00200956
Iteration 10/25 | Loss: 0.00200956
Iteration 11/25 | Loss: 0.00200956
Iteration 12/25 | Loss: 0.00200956
Iteration 13/25 | Loss: 0.00200956
Iteration 14/25 | Loss: 0.00200956
Iteration 15/25 | Loss: 0.00200956
Iteration 16/25 | Loss: 0.00200956
Iteration 17/25 | Loss: 0.00200956
Iteration 18/25 | Loss: 0.00200956
Iteration 19/25 | Loss: 0.00200956
Iteration 20/25 | Loss: 0.00200956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0020095575600862503, 0.0020095575600862503, 0.0020095575600862503, 0.0020095575600862503, 0.0020095575600862503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020095575600862503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200956
Iteration 2/1000 | Loss: 0.00021455
Iteration 3/1000 | Loss: 0.00030009
Iteration 4/1000 | Loss: 0.00014898
Iteration 5/1000 | Loss: 0.00011236
Iteration 6/1000 | Loss: 0.00008453
Iteration 7/1000 | Loss: 0.00007132
Iteration 8/1000 | Loss: 0.00006459
Iteration 9/1000 | Loss: 0.00005868
Iteration 10/1000 | Loss: 0.00017222
Iteration 11/1000 | Loss: 0.00018974
Iteration 12/1000 | Loss: 0.00010201
Iteration 13/1000 | Loss: 0.00008876
Iteration 14/1000 | Loss: 0.00007628
Iteration 15/1000 | Loss: 0.00006463
Iteration 16/1000 | Loss: 0.00005638
Iteration 17/1000 | Loss: 0.00005124
Iteration 18/1000 | Loss: 0.00004745
Iteration 19/1000 | Loss: 0.00004414
Iteration 20/1000 | Loss: 0.00004149
Iteration 21/1000 | Loss: 0.00003887
Iteration 22/1000 | Loss: 0.00003666
Iteration 23/1000 | Loss: 0.00003498
Iteration 24/1000 | Loss: 0.00005329
Iteration 25/1000 | Loss: 0.00006055
Iteration 26/1000 | Loss: 0.00005877
Iteration 27/1000 | Loss: 0.00005234
Iteration 28/1000 | Loss: 0.00004546
Iteration 29/1000 | Loss: 0.00004056
Iteration 30/1000 | Loss: 0.00003648
Iteration 31/1000 | Loss: 0.00003330
Iteration 32/1000 | Loss: 0.00003649
Iteration 33/1000 | Loss: 0.00003403
Iteration 34/1000 | Loss: 0.00003260
Iteration 35/1000 | Loss: 0.00003159
Iteration 36/1000 | Loss: 0.00003057
Iteration 37/1000 | Loss: 0.00004959
Iteration 38/1000 | Loss: 0.00004246
Iteration 39/1000 | Loss: 0.00003854
Iteration 40/1000 | Loss: 0.00003446
Iteration 41/1000 | Loss: 0.00002996
Iteration 42/1000 | Loss: 0.00002786
Iteration 43/1000 | Loss: 0.00002648
Iteration 44/1000 | Loss: 0.00002605
Iteration 45/1000 | Loss: 0.00002577
Iteration 46/1000 | Loss: 0.00002555
Iteration 47/1000 | Loss: 0.00002528
Iteration 48/1000 | Loss: 0.00002498
Iteration 49/1000 | Loss: 0.00002470
Iteration 50/1000 | Loss: 0.00002450
Iteration 51/1000 | Loss: 0.00002423
Iteration 52/1000 | Loss: 0.00002403
Iteration 53/1000 | Loss: 0.00004532
Iteration 54/1000 | Loss: 0.00003287
Iteration 55/1000 | Loss: 0.00002744
Iteration 56/1000 | Loss: 0.00003804
Iteration 57/1000 | Loss: 0.00004061
Iteration 58/1000 | Loss: 0.00003092
Iteration 59/1000 | Loss: 0.00002945
Iteration 60/1000 | Loss: 0.00004306
Iteration 61/1000 | Loss: 0.00003605
Iteration 62/1000 | Loss: 0.00004544
Iteration 63/1000 | Loss: 0.00004094
Iteration 64/1000 | Loss: 0.00003397
Iteration 65/1000 | Loss: 0.00004027
Iteration 66/1000 | Loss: 0.00003237
Iteration 67/1000 | Loss: 0.00002393
Iteration 68/1000 | Loss: 0.00002374
Iteration 69/1000 | Loss: 0.00002374
Iteration 70/1000 | Loss: 0.00002374
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00003898
Iteration 73/1000 | Loss: 0.00003123
Iteration 74/1000 | Loss: 0.00003866
Iteration 75/1000 | Loss: 0.00003050
Iteration 76/1000 | Loss: 0.00003844
Iteration 77/1000 | Loss: 0.00006647
Iteration 78/1000 | Loss: 0.00004552
Iteration 79/1000 | Loss: 0.00006423
Iteration 80/1000 | Loss: 0.00006707
Iteration 81/1000 | Loss: 0.00003754
Iteration 82/1000 | Loss: 0.00002912
Iteration 83/1000 | Loss: 0.00002850
Iteration 84/1000 | Loss: 0.00003228
Iteration 85/1000 | Loss: 0.00002664
Iteration 86/1000 | Loss: 0.00003125
Iteration 87/1000 | Loss: 0.00003694
Iteration 88/1000 | Loss: 0.00002965
Iteration 89/1000 | Loss: 0.00003705
Iteration 90/1000 | Loss: 0.00003704
Iteration 91/1000 | Loss: 0.00003059
Iteration 92/1000 | Loss: 0.00003696
Iteration 93/1000 | Loss: 0.00002583
Iteration 94/1000 | Loss: 0.00002465
Iteration 95/1000 | Loss: 0.00002729
Iteration 96/1000 | Loss: 0.00003582
Iteration 97/1000 | Loss: 0.00005846
Iteration 98/1000 | Loss: 0.00002945
Iteration 99/1000 | Loss: 0.00002747
Iteration 100/1000 | Loss: 0.00002684
Iteration 101/1000 | Loss: 0.00002584
Iteration 102/1000 | Loss: 0.00002509
Iteration 103/1000 | Loss: 0.00002464
Iteration 104/1000 | Loss: 0.00002433
Iteration 105/1000 | Loss: 0.00002412
Iteration 106/1000 | Loss: 0.00011916
Iteration 107/1000 | Loss: 0.00005296
Iteration 108/1000 | Loss: 0.00008334
Iteration 109/1000 | Loss: 0.00002537
Iteration 110/1000 | Loss: 0.00011351
Iteration 111/1000 | Loss: 0.00006960
Iteration 112/1000 | Loss: 0.00002722
Iteration 113/1000 | Loss: 0.00002395
Iteration 114/1000 | Loss: 0.00012234
Iteration 115/1000 | Loss: 0.00004257
Iteration 116/1000 | Loss: 0.00002645
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002398
Iteration 119/1000 | Loss: 0.00002369
Iteration 120/1000 | Loss: 0.00002355
Iteration 121/1000 | Loss: 0.00002348
Iteration 122/1000 | Loss: 0.00002341
Iteration 123/1000 | Loss: 0.00002341
Iteration 124/1000 | Loss: 0.00002340
Iteration 125/1000 | Loss: 0.00002339
Iteration 126/1000 | Loss: 0.00002338
Iteration 127/1000 | Loss: 0.00002338
Iteration 128/1000 | Loss: 0.00002338
Iteration 129/1000 | Loss: 0.00002337
Iteration 130/1000 | Loss: 0.00002337
Iteration 131/1000 | Loss: 0.00002337
Iteration 132/1000 | Loss: 0.00002336
Iteration 133/1000 | Loss: 0.00002336
Iteration 134/1000 | Loss: 0.00002336
Iteration 135/1000 | Loss: 0.00002336
Iteration 136/1000 | Loss: 0.00002335
Iteration 137/1000 | Loss: 0.00002335
Iteration 138/1000 | Loss: 0.00002335
Iteration 139/1000 | Loss: 0.00002335
Iteration 140/1000 | Loss: 0.00002334
Iteration 141/1000 | Loss: 0.00002334
Iteration 142/1000 | Loss: 0.00002334
Iteration 143/1000 | Loss: 0.00002334
Iteration 144/1000 | Loss: 0.00002333
Iteration 145/1000 | Loss: 0.00002333
Iteration 146/1000 | Loss: 0.00002333
Iteration 147/1000 | Loss: 0.00002333
Iteration 148/1000 | Loss: 0.00002332
Iteration 149/1000 | Loss: 0.00002332
Iteration 150/1000 | Loss: 0.00002332
Iteration 151/1000 | Loss: 0.00002332
Iteration 152/1000 | Loss: 0.00002332
Iteration 153/1000 | Loss: 0.00002331
Iteration 154/1000 | Loss: 0.00002331
Iteration 155/1000 | Loss: 0.00002331
Iteration 156/1000 | Loss: 0.00002331
Iteration 157/1000 | Loss: 0.00002331
Iteration 158/1000 | Loss: 0.00002330
Iteration 159/1000 | Loss: 0.00002330
Iteration 160/1000 | Loss: 0.00002330
Iteration 161/1000 | Loss: 0.00002330
Iteration 162/1000 | Loss: 0.00002330
Iteration 163/1000 | Loss: 0.00002330
Iteration 164/1000 | Loss: 0.00002330
Iteration 165/1000 | Loss: 0.00002330
Iteration 166/1000 | Loss: 0.00002329
Iteration 167/1000 | Loss: 0.00002329
Iteration 168/1000 | Loss: 0.00002329
Iteration 169/1000 | Loss: 0.00002329
Iteration 170/1000 | Loss: 0.00002329
Iteration 171/1000 | Loss: 0.00002329
Iteration 172/1000 | Loss: 0.00002329
Iteration 173/1000 | Loss: 0.00002329
Iteration 174/1000 | Loss: 0.00002329
Iteration 175/1000 | Loss: 0.00002329
Iteration 176/1000 | Loss: 0.00002329
Iteration 177/1000 | Loss: 0.00002329
Iteration 178/1000 | Loss: 0.00002329
Iteration 179/1000 | Loss: 0.00002329
Iteration 180/1000 | Loss: 0.00002329
Iteration 181/1000 | Loss: 0.00002329
Iteration 182/1000 | Loss: 0.00002329
Iteration 183/1000 | Loss: 0.00002329
Iteration 184/1000 | Loss: 0.00002329
Iteration 185/1000 | Loss: 0.00002329
Iteration 186/1000 | Loss: 0.00002329
Iteration 187/1000 | Loss: 0.00002329
Iteration 188/1000 | Loss: 0.00002329
Iteration 189/1000 | Loss: 0.00002329
Iteration 190/1000 | Loss: 0.00002329
Iteration 191/1000 | Loss: 0.00002329
Iteration 192/1000 | Loss: 0.00002329
Iteration 193/1000 | Loss: 0.00002329
Iteration 194/1000 | Loss: 0.00002329
Iteration 195/1000 | Loss: 0.00002329
Iteration 196/1000 | Loss: 0.00002329
Iteration 197/1000 | Loss: 0.00002329
Iteration 198/1000 | Loss: 0.00002329
Iteration 199/1000 | Loss: 0.00002329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.3292242985917255e-05, 2.3292242985917255e-05, 2.3292242985917255e-05, 2.3292242985917255e-05, 2.3292242985917255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3292242985917255e-05

Optimization complete. Final v2v error: 3.9360415935516357 mm

Highest mean error: 6.252769470214844 mm for frame 103

Lowest mean error: 3.0046510696411133 mm for frame 74

Saving results

Total time: 216.56959414482117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989961
Iteration 2/25 | Loss: 0.00220059
Iteration 3/25 | Loss: 0.00199633
Iteration 4/25 | Loss: 0.00188704
Iteration 5/25 | Loss: 0.00179663
Iteration 6/25 | Loss: 0.00167672
Iteration 7/25 | Loss: 0.00166015
Iteration 8/25 | Loss: 0.00148677
Iteration 9/25 | Loss: 0.00147310
Iteration 10/25 | Loss: 0.00147355
Iteration 11/25 | Loss: 0.00145804
Iteration 12/25 | Loss: 0.00143877
Iteration 13/25 | Loss: 0.00141414
Iteration 14/25 | Loss: 0.00140496
Iteration 15/25 | Loss: 0.00139742
Iteration 16/25 | Loss: 0.00139748
Iteration 17/25 | Loss: 0.00139324
Iteration 18/25 | Loss: 0.00139289
Iteration 19/25 | Loss: 0.00139015
Iteration 20/25 | Loss: 0.00138932
Iteration 21/25 | Loss: 0.00138207
Iteration 22/25 | Loss: 0.00138050
Iteration 23/25 | Loss: 0.00137952
Iteration 24/25 | Loss: 0.00137907
Iteration 25/25 | Loss: 0.00137877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33686531
Iteration 2/25 | Loss: 0.00262738
Iteration 3/25 | Loss: 0.00262737
Iteration 4/25 | Loss: 0.00262737
Iteration 5/25 | Loss: 0.00262737
Iteration 6/25 | Loss: 0.00262737
Iteration 7/25 | Loss: 0.00262737
Iteration 8/25 | Loss: 0.00262737
Iteration 9/25 | Loss: 0.00262737
Iteration 10/25 | Loss: 0.00262737
Iteration 11/25 | Loss: 0.00262737
Iteration 12/25 | Loss: 0.00262737
Iteration 13/25 | Loss: 0.00262737
Iteration 14/25 | Loss: 0.00262737
Iteration 15/25 | Loss: 0.00262737
Iteration 16/25 | Loss: 0.00262737
Iteration 17/25 | Loss: 0.00262737
Iteration 18/25 | Loss: 0.00262737
Iteration 19/25 | Loss: 0.00262737
Iteration 20/25 | Loss: 0.00262737
Iteration 21/25 | Loss: 0.00262737
Iteration 22/25 | Loss: 0.00262737
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0026273676194250584, 0.0026273676194250584, 0.0026273676194250584, 0.0026273676194250584, 0.0026273676194250584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026273676194250584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262737
Iteration 2/1000 | Loss: 0.00042766
Iteration 3/1000 | Loss: 0.00033236
Iteration 4/1000 | Loss: 0.00014596
Iteration 5/1000 | Loss: 0.00112465
Iteration 6/1000 | Loss: 0.00068421
Iteration 7/1000 | Loss: 0.00009860
Iteration 8/1000 | Loss: 0.00045758
Iteration 9/1000 | Loss: 0.00033894
Iteration 10/1000 | Loss: 0.00030657
Iteration 11/1000 | Loss: 0.00032539
Iteration 12/1000 | Loss: 0.00048409
Iteration 13/1000 | Loss: 0.00028549
Iteration 14/1000 | Loss: 0.00019569
Iteration 15/1000 | Loss: 0.00028218
Iteration 16/1000 | Loss: 0.00024951
Iteration 17/1000 | Loss: 0.00013712
Iteration 18/1000 | Loss: 0.00005959
Iteration 19/1000 | Loss: 0.00003819
Iteration 20/1000 | Loss: 0.00325033
Iteration 21/1000 | Loss: 0.00259548
Iteration 22/1000 | Loss: 0.00006826
Iteration 23/1000 | Loss: 0.00100059
Iteration 24/1000 | Loss: 0.00045857
Iteration 25/1000 | Loss: 0.00024523
Iteration 26/1000 | Loss: 0.00030495
Iteration 27/1000 | Loss: 0.00043455
Iteration 28/1000 | Loss: 0.00021660
Iteration 29/1000 | Loss: 0.00019691
Iteration 30/1000 | Loss: 0.00030549
Iteration 31/1000 | Loss: 0.00032928
Iteration 32/1000 | Loss: 0.00043169
Iteration 33/1000 | Loss: 0.00025905
Iteration 34/1000 | Loss: 0.00004645
Iteration 35/1000 | Loss: 0.00019295
Iteration 36/1000 | Loss: 0.00025555
Iteration 37/1000 | Loss: 0.00097488
Iteration 38/1000 | Loss: 0.00120644
Iteration 39/1000 | Loss: 0.00032704
Iteration 40/1000 | Loss: 0.00007069
Iteration 41/1000 | Loss: 0.00014202
Iteration 42/1000 | Loss: 0.00096707
Iteration 43/1000 | Loss: 0.00106010
Iteration 44/1000 | Loss: 0.00081443
Iteration 45/1000 | Loss: 0.00076662
Iteration 46/1000 | Loss: 0.00017657
Iteration 47/1000 | Loss: 0.00007757
Iteration 48/1000 | Loss: 0.00040138
Iteration 49/1000 | Loss: 0.00031619
Iteration 50/1000 | Loss: 0.00010002
Iteration 51/1000 | Loss: 0.00025547
Iteration 52/1000 | Loss: 0.00004485
Iteration 53/1000 | Loss: 0.00003519
Iteration 54/1000 | Loss: 0.00012130
Iteration 55/1000 | Loss: 0.00002863
Iteration 56/1000 | Loss: 0.00024265
Iteration 57/1000 | Loss: 0.00003448
Iteration 58/1000 | Loss: 0.00037942
Iteration 59/1000 | Loss: 0.00042610
Iteration 60/1000 | Loss: 0.00010050
Iteration 61/1000 | Loss: 0.00005736
Iteration 62/1000 | Loss: 0.00014003
Iteration 63/1000 | Loss: 0.00002481
Iteration 64/1000 | Loss: 0.00002356
Iteration 65/1000 | Loss: 0.00002157
Iteration 66/1000 | Loss: 0.00001981
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001782
Iteration 71/1000 | Loss: 0.00008704
Iteration 72/1000 | Loss: 0.00025444
Iteration 73/1000 | Loss: 0.00028645
Iteration 74/1000 | Loss: 0.00015185
Iteration 75/1000 | Loss: 0.00003924
Iteration 76/1000 | Loss: 0.00003454
Iteration 77/1000 | Loss: 0.00002648
Iteration 78/1000 | Loss: 0.00004607
Iteration 79/1000 | Loss: 0.00002587
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001764
Iteration 83/1000 | Loss: 0.00014837
Iteration 84/1000 | Loss: 0.00009029
Iteration 85/1000 | Loss: 0.00002387
Iteration 86/1000 | Loss: 0.00002548
Iteration 87/1000 | Loss: 0.00019918
Iteration 88/1000 | Loss: 0.00002591
Iteration 89/1000 | Loss: 0.00003096
Iteration 90/1000 | Loss: 0.00001877
Iteration 91/1000 | Loss: 0.00005380
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001646
Iteration 95/1000 | Loss: 0.00001633
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00006653
Iteration 98/1000 | Loss: 0.00006653
Iteration 99/1000 | Loss: 0.00028049
Iteration 100/1000 | Loss: 0.00001634
Iteration 101/1000 | Loss: 0.00001607
Iteration 102/1000 | Loss: 0.00006959
Iteration 103/1000 | Loss: 0.00002350
Iteration 104/1000 | Loss: 0.00003391
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001576
Iteration 107/1000 | Loss: 0.00003867
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001545
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001539
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001536
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001536
Iteration 121/1000 | Loss: 0.00001536
Iteration 122/1000 | Loss: 0.00001536
Iteration 123/1000 | Loss: 0.00001535
Iteration 124/1000 | Loss: 0.00001535
Iteration 125/1000 | Loss: 0.00001535
Iteration 126/1000 | Loss: 0.00001535
Iteration 127/1000 | Loss: 0.00001535
Iteration 128/1000 | Loss: 0.00001535
Iteration 129/1000 | Loss: 0.00001535
Iteration 130/1000 | Loss: 0.00001535
Iteration 131/1000 | Loss: 0.00001535
Iteration 132/1000 | Loss: 0.00001535
Iteration 133/1000 | Loss: 0.00001535
Iteration 134/1000 | Loss: 0.00001534
Iteration 135/1000 | Loss: 0.00001534
Iteration 136/1000 | Loss: 0.00001534
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001533
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00001533
Iteration 141/1000 | Loss: 0.00001532
Iteration 142/1000 | Loss: 0.00001532
Iteration 143/1000 | Loss: 0.00001532
Iteration 144/1000 | Loss: 0.00001532
Iteration 145/1000 | Loss: 0.00001531
Iteration 146/1000 | Loss: 0.00001531
Iteration 147/1000 | Loss: 0.00001531
Iteration 148/1000 | Loss: 0.00001531
Iteration 149/1000 | Loss: 0.00001531
Iteration 150/1000 | Loss: 0.00001530
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001529
Iteration 154/1000 | Loss: 0.00001529
Iteration 155/1000 | Loss: 0.00001529
Iteration 156/1000 | Loss: 0.00001529
Iteration 157/1000 | Loss: 0.00001529
Iteration 158/1000 | Loss: 0.00001529
Iteration 159/1000 | Loss: 0.00001529
Iteration 160/1000 | Loss: 0.00001529
Iteration 161/1000 | Loss: 0.00001529
Iteration 162/1000 | Loss: 0.00001528
Iteration 163/1000 | Loss: 0.00001528
Iteration 164/1000 | Loss: 0.00001528
Iteration 165/1000 | Loss: 0.00001528
Iteration 166/1000 | Loss: 0.00001528
Iteration 167/1000 | Loss: 0.00001528
Iteration 168/1000 | Loss: 0.00001528
Iteration 169/1000 | Loss: 0.00001527
Iteration 170/1000 | Loss: 0.00001527
Iteration 171/1000 | Loss: 0.00001527
Iteration 172/1000 | Loss: 0.00001527
Iteration 173/1000 | Loss: 0.00001527
Iteration 174/1000 | Loss: 0.00001527
Iteration 175/1000 | Loss: 0.00001526
Iteration 176/1000 | Loss: 0.00001526
Iteration 177/1000 | Loss: 0.00001526
Iteration 178/1000 | Loss: 0.00001526
Iteration 179/1000 | Loss: 0.00001526
Iteration 180/1000 | Loss: 0.00001526
Iteration 181/1000 | Loss: 0.00001525
Iteration 182/1000 | Loss: 0.00001525
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001525
Iteration 185/1000 | Loss: 0.00001525
Iteration 186/1000 | Loss: 0.00001525
Iteration 187/1000 | Loss: 0.00001525
Iteration 188/1000 | Loss: 0.00001525
Iteration 189/1000 | Loss: 0.00001525
Iteration 190/1000 | Loss: 0.00001524
Iteration 191/1000 | Loss: 0.00001524
Iteration 192/1000 | Loss: 0.00001524
Iteration 193/1000 | Loss: 0.00001524
Iteration 194/1000 | Loss: 0.00001524
Iteration 195/1000 | Loss: 0.00001524
Iteration 196/1000 | Loss: 0.00001524
Iteration 197/1000 | Loss: 0.00001524
Iteration 198/1000 | Loss: 0.00001524
Iteration 199/1000 | Loss: 0.00001524
Iteration 200/1000 | Loss: 0.00001524
Iteration 201/1000 | Loss: 0.00001524
Iteration 202/1000 | Loss: 0.00001524
Iteration 203/1000 | Loss: 0.00001524
Iteration 204/1000 | Loss: 0.00001524
Iteration 205/1000 | Loss: 0.00001524
Iteration 206/1000 | Loss: 0.00001524
Iteration 207/1000 | Loss: 0.00001523
Iteration 208/1000 | Loss: 0.00001523
Iteration 209/1000 | Loss: 0.00001523
Iteration 210/1000 | Loss: 0.00001523
Iteration 211/1000 | Loss: 0.00001523
Iteration 212/1000 | Loss: 0.00001523
Iteration 213/1000 | Loss: 0.00001523
Iteration 214/1000 | Loss: 0.00001523
Iteration 215/1000 | Loss: 0.00001523
Iteration 216/1000 | Loss: 0.00001522
Iteration 217/1000 | Loss: 0.00001522
Iteration 218/1000 | Loss: 0.00001522
Iteration 219/1000 | Loss: 0.00001522
Iteration 220/1000 | Loss: 0.00001521
Iteration 221/1000 | Loss: 0.00001521
Iteration 222/1000 | Loss: 0.00001521
Iteration 223/1000 | Loss: 0.00001521
Iteration 224/1000 | Loss: 0.00001521
Iteration 225/1000 | Loss: 0.00001521
Iteration 226/1000 | Loss: 0.00001521
Iteration 227/1000 | Loss: 0.00001521
Iteration 228/1000 | Loss: 0.00001520
Iteration 229/1000 | Loss: 0.00001520
Iteration 230/1000 | Loss: 0.00001520
Iteration 231/1000 | Loss: 0.00001520
Iteration 232/1000 | Loss: 0.00001520
Iteration 233/1000 | Loss: 0.00001520
Iteration 234/1000 | Loss: 0.00001520
Iteration 235/1000 | Loss: 0.00001520
Iteration 236/1000 | Loss: 0.00001520
Iteration 237/1000 | Loss: 0.00001520
Iteration 238/1000 | Loss: 0.00001520
Iteration 239/1000 | Loss: 0.00001520
Iteration 240/1000 | Loss: 0.00001520
Iteration 241/1000 | Loss: 0.00001520
Iteration 242/1000 | Loss: 0.00001520
Iteration 243/1000 | Loss: 0.00001520
Iteration 244/1000 | Loss: 0.00001520
Iteration 245/1000 | Loss: 0.00001520
Iteration 246/1000 | Loss: 0.00001520
Iteration 247/1000 | Loss: 0.00001520
Iteration 248/1000 | Loss: 0.00001520
Iteration 249/1000 | Loss: 0.00001520
Iteration 250/1000 | Loss: 0.00001520
Iteration 251/1000 | Loss: 0.00001520
Iteration 252/1000 | Loss: 0.00001520
Iteration 253/1000 | Loss: 0.00001520
Iteration 254/1000 | Loss: 0.00001520
Iteration 255/1000 | Loss: 0.00001520
Iteration 256/1000 | Loss: 0.00001520
Iteration 257/1000 | Loss: 0.00001520
Iteration 258/1000 | Loss: 0.00001520
Iteration 259/1000 | Loss: 0.00001520
Iteration 260/1000 | Loss: 0.00001520
Iteration 261/1000 | Loss: 0.00001520
Iteration 262/1000 | Loss: 0.00001520
Iteration 263/1000 | Loss: 0.00001520
Iteration 264/1000 | Loss: 0.00001520
Iteration 265/1000 | Loss: 0.00001520
Iteration 266/1000 | Loss: 0.00001520
Iteration 267/1000 | Loss: 0.00001520
Iteration 268/1000 | Loss: 0.00001520
Iteration 269/1000 | Loss: 0.00001520
Iteration 270/1000 | Loss: 0.00001520
Iteration 271/1000 | Loss: 0.00001520
Iteration 272/1000 | Loss: 0.00001520
Iteration 273/1000 | Loss: 0.00001520
Iteration 274/1000 | Loss: 0.00001520
Iteration 275/1000 | Loss: 0.00001520
Iteration 276/1000 | Loss: 0.00001520
Iteration 277/1000 | Loss: 0.00001520
Iteration 278/1000 | Loss: 0.00001520
Iteration 279/1000 | Loss: 0.00001520
Iteration 280/1000 | Loss: 0.00001520
Iteration 281/1000 | Loss: 0.00001520
Iteration 282/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [1.5196488675428554e-05, 1.5196488675428554e-05, 1.5196488675428554e-05, 1.5196488675428554e-05, 1.5196488675428554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5196488675428554e-05

Optimization complete. Final v2v error: 3.2849535942077637 mm

Highest mean error: 4.253047466278076 mm for frame 84

Lowest mean error: 2.7757556438446045 mm for frame 31

Saving results

Total time: 202.05235528945923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003980
Iteration 2/25 | Loss: 0.00268126
Iteration 3/25 | Loss: 0.00214670
Iteration 4/25 | Loss: 0.00194318
Iteration 5/25 | Loss: 0.00190471
Iteration 6/25 | Loss: 0.00180605
Iteration 7/25 | Loss: 0.00180037
Iteration 8/25 | Loss: 0.00175896
Iteration 9/25 | Loss: 0.00175087
Iteration 10/25 | Loss: 0.00171915
Iteration 11/25 | Loss: 0.00170498
Iteration 12/25 | Loss: 0.00169060
Iteration 13/25 | Loss: 0.00168051
Iteration 14/25 | Loss: 0.00167428
Iteration 15/25 | Loss: 0.00168273
Iteration 16/25 | Loss: 0.00167552
Iteration 17/25 | Loss: 0.00167118
Iteration 18/25 | Loss: 0.00167668
Iteration 19/25 | Loss: 0.00166914
Iteration 20/25 | Loss: 0.00166536
Iteration 21/25 | Loss: 0.00166306
Iteration 22/25 | Loss: 0.00166141
Iteration 23/25 | Loss: 0.00166083
Iteration 24/25 | Loss: 0.00166069
Iteration 25/25 | Loss: 0.00166066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19255376
Iteration 2/25 | Loss: 0.00725787
Iteration 3/25 | Loss: 0.00464012
Iteration 4/25 | Loss: 0.00464012
Iteration 5/25 | Loss: 0.00464012
Iteration 6/25 | Loss: 0.00464012
Iteration 7/25 | Loss: 0.00464012
Iteration 8/25 | Loss: 0.00464012
Iteration 9/25 | Loss: 0.00464012
Iteration 10/25 | Loss: 0.00464012
Iteration 11/25 | Loss: 0.00464012
Iteration 12/25 | Loss: 0.00464012
Iteration 13/25 | Loss: 0.00464012
Iteration 14/25 | Loss: 0.00464012
Iteration 15/25 | Loss: 0.00464012
Iteration 16/25 | Loss: 0.00464012
Iteration 17/25 | Loss: 0.00464012
Iteration 18/25 | Loss: 0.00464012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0046401191502809525, 0.0046401191502809525, 0.0046401191502809525, 0.0046401191502809525, 0.0046401191502809525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0046401191502809525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00464012
Iteration 2/1000 | Loss: 0.00098799
Iteration 3/1000 | Loss: 0.00278058
Iteration 4/1000 | Loss: 0.00298259
Iteration 5/1000 | Loss: 0.00201913
Iteration 6/1000 | Loss: 0.00183843
Iteration 7/1000 | Loss: 0.00243699
Iteration 8/1000 | Loss: 0.00172612
Iteration 9/1000 | Loss: 0.00113177
Iteration 10/1000 | Loss: 0.00221444
Iteration 11/1000 | Loss: 0.00239045
Iteration 12/1000 | Loss: 0.00165470
Iteration 13/1000 | Loss: 0.00034505
Iteration 14/1000 | Loss: 0.00122129
Iteration 15/1000 | Loss: 0.00078340
Iteration 16/1000 | Loss: 0.00092569
Iteration 17/1000 | Loss: 0.00127849
Iteration 18/1000 | Loss: 0.00069152
Iteration 19/1000 | Loss: 0.00086168
Iteration 20/1000 | Loss: 0.00024996
Iteration 21/1000 | Loss: 0.00024056
Iteration 22/1000 | Loss: 0.00028186
Iteration 23/1000 | Loss: 0.00083074
Iteration 24/1000 | Loss: 0.00040339
Iteration 25/1000 | Loss: 0.00051087
Iteration 26/1000 | Loss: 0.00038881
Iteration 27/1000 | Loss: 0.00029696
Iteration 28/1000 | Loss: 0.00031134
Iteration 29/1000 | Loss: 0.00021841
Iteration 30/1000 | Loss: 0.00039311
Iteration 31/1000 | Loss: 0.00026270
Iteration 32/1000 | Loss: 0.00055939
Iteration 33/1000 | Loss: 0.00096545
Iteration 34/1000 | Loss: 0.00095117
Iteration 35/1000 | Loss: 0.00155603
Iteration 36/1000 | Loss: 0.00121486
Iteration 37/1000 | Loss: 0.00160149
Iteration 38/1000 | Loss: 0.00157697
Iteration 39/1000 | Loss: 0.00100147
Iteration 40/1000 | Loss: 0.00165443
Iteration 41/1000 | Loss: 0.00106467
Iteration 42/1000 | Loss: 0.00106056
Iteration 43/1000 | Loss: 0.00165799
Iteration 44/1000 | Loss: 0.00111885
Iteration 45/1000 | Loss: 0.00099464
Iteration 46/1000 | Loss: 0.00303004
Iteration 47/1000 | Loss: 0.00104788
Iteration 48/1000 | Loss: 0.00111787
Iteration 49/1000 | Loss: 0.00051301
Iteration 50/1000 | Loss: 0.00076093
Iteration 51/1000 | Loss: 0.00045575
Iteration 52/1000 | Loss: 0.00084149
Iteration 53/1000 | Loss: 0.00035661
Iteration 54/1000 | Loss: 0.00046128
Iteration 55/1000 | Loss: 0.00088660
Iteration 56/1000 | Loss: 0.00161862
Iteration 57/1000 | Loss: 0.00162220
Iteration 58/1000 | Loss: 0.00430731
Iteration 59/1000 | Loss: 0.00365754
Iteration 60/1000 | Loss: 0.00031590
Iteration 61/1000 | Loss: 0.00060179
Iteration 62/1000 | Loss: 0.00059468
Iteration 63/1000 | Loss: 0.00029879
Iteration 64/1000 | Loss: 0.00069580
Iteration 65/1000 | Loss: 0.00027167
Iteration 66/1000 | Loss: 0.00066971
Iteration 67/1000 | Loss: 0.00043010
Iteration 68/1000 | Loss: 0.00026241
Iteration 69/1000 | Loss: 0.00026332
Iteration 70/1000 | Loss: 0.00059900
Iteration 71/1000 | Loss: 0.00044354
Iteration 72/1000 | Loss: 0.00038982
Iteration 73/1000 | Loss: 0.00043744
Iteration 74/1000 | Loss: 0.00028707
Iteration 75/1000 | Loss: 0.00020873
Iteration 76/1000 | Loss: 0.00019023
Iteration 77/1000 | Loss: 0.00011247
Iteration 78/1000 | Loss: 0.00011479
Iteration 79/1000 | Loss: 0.00016068
Iteration 80/1000 | Loss: 0.00011140
Iteration 81/1000 | Loss: 0.00010544
Iteration 82/1000 | Loss: 0.00068400
Iteration 83/1000 | Loss: 0.00073760
Iteration 84/1000 | Loss: 0.00132843
Iteration 85/1000 | Loss: 0.00061298
Iteration 86/1000 | Loss: 0.00058711
Iteration 87/1000 | Loss: 0.00031529
Iteration 88/1000 | Loss: 0.00021395
Iteration 89/1000 | Loss: 0.00032476
Iteration 90/1000 | Loss: 0.00012470
Iteration 91/1000 | Loss: 0.00012668
Iteration 92/1000 | Loss: 0.00018575
Iteration 93/1000 | Loss: 0.00009049
Iteration 94/1000 | Loss: 0.00062825
Iteration 95/1000 | Loss: 0.00014896
Iteration 96/1000 | Loss: 0.00050072
Iteration 97/1000 | Loss: 0.00011138
Iteration 98/1000 | Loss: 0.00012587
Iteration 99/1000 | Loss: 0.00009860
Iteration 100/1000 | Loss: 0.00010883
Iteration 101/1000 | Loss: 0.00019017
Iteration 102/1000 | Loss: 0.00018601
Iteration 103/1000 | Loss: 0.00008494
Iteration 104/1000 | Loss: 0.00019392
Iteration 105/1000 | Loss: 0.00008809
Iteration 106/1000 | Loss: 0.00011453
Iteration 107/1000 | Loss: 0.00008344
Iteration 108/1000 | Loss: 0.00008173
Iteration 109/1000 | Loss: 0.00023723
Iteration 110/1000 | Loss: 0.00008685
Iteration 111/1000 | Loss: 0.00008922
Iteration 112/1000 | Loss: 0.00008106
Iteration 113/1000 | Loss: 0.00008077
Iteration 114/1000 | Loss: 0.00008047
Iteration 115/1000 | Loss: 0.00023180
Iteration 116/1000 | Loss: 0.00010848
Iteration 117/1000 | Loss: 0.00009408
Iteration 118/1000 | Loss: 0.00008021
Iteration 119/1000 | Loss: 0.00008823
Iteration 120/1000 | Loss: 0.00007990
Iteration 121/1000 | Loss: 0.00007989
Iteration 122/1000 | Loss: 0.00007963
Iteration 123/1000 | Loss: 0.00019470
Iteration 124/1000 | Loss: 0.00019468
Iteration 125/1000 | Loss: 0.00041396
Iteration 126/1000 | Loss: 0.00007918
Iteration 127/1000 | Loss: 0.00007883
Iteration 128/1000 | Loss: 0.00014800
Iteration 129/1000 | Loss: 0.00007901
Iteration 130/1000 | Loss: 0.00007725
Iteration 131/1000 | Loss: 0.00016072
Iteration 132/1000 | Loss: 0.00007596
Iteration 133/1000 | Loss: 0.00041098
Iteration 134/1000 | Loss: 0.00132380
Iteration 135/1000 | Loss: 0.00007723
Iteration 136/1000 | Loss: 0.00018996
Iteration 137/1000 | Loss: 0.00034192
Iteration 138/1000 | Loss: 0.00031785
Iteration 139/1000 | Loss: 0.00065005
Iteration 140/1000 | Loss: 0.00248239
Iteration 141/1000 | Loss: 0.00143327
Iteration 142/1000 | Loss: 0.00237290
Iteration 143/1000 | Loss: 0.00018463
Iteration 144/1000 | Loss: 0.00011087
Iteration 145/1000 | Loss: 0.00017501
Iteration 146/1000 | Loss: 0.00007407
Iteration 147/1000 | Loss: 0.00066867
Iteration 148/1000 | Loss: 0.00006076
Iteration 149/1000 | Loss: 0.00005565
Iteration 150/1000 | Loss: 0.00005313
Iteration 151/1000 | Loss: 0.00021153
Iteration 152/1000 | Loss: 0.00005588
Iteration 153/1000 | Loss: 0.00005754
Iteration 154/1000 | Loss: 0.00004965
Iteration 155/1000 | Loss: 0.00004872
Iteration 156/1000 | Loss: 0.00023825
Iteration 157/1000 | Loss: 0.00004757
Iteration 158/1000 | Loss: 0.00015240
Iteration 159/1000 | Loss: 0.00004685
Iteration 160/1000 | Loss: 0.00004638
Iteration 161/1000 | Loss: 0.00004595
Iteration 162/1000 | Loss: 0.00023234
Iteration 163/1000 | Loss: 0.00016505
Iteration 164/1000 | Loss: 0.00005268
Iteration 165/1000 | Loss: 0.00021760
Iteration 166/1000 | Loss: 0.00005297
Iteration 167/1000 | Loss: 0.00004558
Iteration 168/1000 | Loss: 0.00005952
Iteration 169/1000 | Loss: 0.00004515
Iteration 170/1000 | Loss: 0.00004480
Iteration 171/1000 | Loss: 0.00025810
Iteration 172/1000 | Loss: 0.00005869
Iteration 173/1000 | Loss: 0.00005383
Iteration 174/1000 | Loss: 0.00006377
Iteration 175/1000 | Loss: 0.00004448
Iteration 176/1000 | Loss: 0.00004412
Iteration 177/1000 | Loss: 0.00004390
Iteration 178/1000 | Loss: 0.00004374
Iteration 179/1000 | Loss: 0.00013002
Iteration 180/1000 | Loss: 0.00005926
Iteration 181/1000 | Loss: 0.00006510
Iteration 182/1000 | Loss: 0.00004354
Iteration 183/1000 | Loss: 0.00004344
Iteration 184/1000 | Loss: 0.00004343
Iteration 185/1000 | Loss: 0.00004341
Iteration 186/1000 | Loss: 0.00004340
Iteration 187/1000 | Loss: 0.00004339
Iteration 188/1000 | Loss: 0.00004335
Iteration 189/1000 | Loss: 0.00004318
Iteration 190/1000 | Loss: 0.00014150
Iteration 191/1000 | Loss: 0.00004361
Iteration 192/1000 | Loss: 0.00004319
Iteration 193/1000 | Loss: 0.00004297
Iteration 194/1000 | Loss: 0.00004291
Iteration 195/1000 | Loss: 0.00004291
Iteration 196/1000 | Loss: 0.00004291
Iteration 197/1000 | Loss: 0.00004291
Iteration 198/1000 | Loss: 0.00004290
Iteration 199/1000 | Loss: 0.00004290
Iteration 200/1000 | Loss: 0.00004290
Iteration 201/1000 | Loss: 0.00004290
Iteration 202/1000 | Loss: 0.00004290
Iteration 203/1000 | Loss: 0.00004290
Iteration 204/1000 | Loss: 0.00004290
Iteration 205/1000 | Loss: 0.00004290
Iteration 206/1000 | Loss: 0.00004289
Iteration 207/1000 | Loss: 0.00004288
Iteration 208/1000 | Loss: 0.00004288
Iteration 209/1000 | Loss: 0.00004288
Iteration 210/1000 | Loss: 0.00004287
Iteration 211/1000 | Loss: 0.00004287
Iteration 212/1000 | Loss: 0.00004286
Iteration 213/1000 | Loss: 0.00004285
Iteration 214/1000 | Loss: 0.00004284
Iteration 215/1000 | Loss: 0.00004284
Iteration 216/1000 | Loss: 0.00004283
Iteration 217/1000 | Loss: 0.00004282
Iteration 218/1000 | Loss: 0.00004280
Iteration 219/1000 | Loss: 0.00004273
Iteration 220/1000 | Loss: 0.00004263
Iteration 221/1000 | Loss: 0.00004262
Iteration 222/1000 | Loss: 0.00004259
Iteration 223/1000 | Loss: 0.00004256
Iteration 224/1000 | Loss: 0.00004237
Iteration 225/1000 | Loss: 0.00019671
Iteration 226/1000 | Loss: 0.00089704
Iteration 227/1000 | Loss: 0.00094317
Iteration 228/1000 | Loss: 0.00004596
Iteration 229/1000 | Loss: 0.00004245
Iteration 230/1000 | Loss: 0.00004204
Iteration 231/1000 | Loss: 0.00024689
Iteration 232/1000 | Loss: 0.00074817
Iteration 233/1000 | Loss: 0.00052005
Iteration 234/1000 | Loss: 0.00022612
Iteration 235/1000 | Loss: 0.00070288
Iteration 236/1000 | Loss: 0.00066872
Iteration 237/1000 | Loss: 0.00069427
Iteration 238/1000 | Loss: 0.00006396
Iteration 239/1000 | Loss: 0.00004598
Iteration 240/1000 | Loss: 0.00016612
Iteration 241/1000 | Loss: 0.00078264
Iteration 242/1000 | Loss: 0.00056394
Iteration 243/1000 | Loss: 0.00054812
Iteration 244/1000 | Loss: 0.00052177
Iteration 245/1000 | Loss: 0.00018192
Iteration 246/1000 | Loss: 0.00018773
Iteration 247/1000 | Loss: 0.00012036
Iteration 248/1000 | Loss: 0.00007755
Iteration 249/1000 | Loss: 0.00014517
Iteration 250/1000 | Loss: 0.00012715
Iteration 251/1000 | Loss: 0.00085227
Iteration 252/1000 | Loss: 0.00062293
Iteration 253/1000 | Loss: 0.00070591
Iteration 254/1000 | Loss: 0.00065638
Iteration 255/1000 | Loss: 0.00046855
Iteration 256/1000 | Loss: 0.00046209
Iteration 257/1000 | Loss: 0.00038314
Iteration 258/1000 | Loss: 0.00040626
Iteration 259/1000 | Loss: 0.00030917
Iteration 260/1000 | Loss: 0.00015850
Iteration 261/1000 | Loss: 0.00010516
Iteration 262/1000 | Loss: 0.00006052
Iteration 263/1000 | Loss: 0.00003451
Iteration 264/1000 | Loss: 0.00008676
Iteration 265/1000 | Loss: 0.00008732
Iteration 266/1000 | Loss: 0.00003375
Iteration 267/1000 | Loss: 0.00005513
Iteration 268/1000 | Loss: 0.00002704
Iteration 269/1000 | Loss: 0.00011759
Iteration 270/1000 | Loss: 0.00010062
Iteration 271/1000 | Loss: 0.00009721
Iteration 272/1000 | Loss: 0.00003055
Iteration 273/1000 | Loss: 0.00002529
Iteration 274/1000 | Loss: 0.00002195
Iteration 275/1000 | Loss: 0.00002071
Iteration 276/1000 | Loss: 0.00001988
Iteration 277/1000 | Loss: 0.00012671
Iteration 278/1000 | Loss: 0.00002471
Iteration 279/1000 | Loss: 0.00002043
Iteration 280/1000 | Loss: 0.00001912
Iteration 281/1000 | Loss: 0.00016483
Iteration 282/1000 | Loss: 0.00002687
Iteration 283/1000 | Loss: 0.00002027
Iteration 284/1000 | Loss: 0.00001727
Iteration 285/1000 | Loss: 0.00001639
Iteration 286/1000 | Loss: 0.00001569
Iteration 287/1000 | Loss: 0.00001525
Iteration 288/1000 | Loss: 0.00001523
Iteration 289/1000 | Loss: 0.00002497
Iteration 290/1000 | Loss: 0.00001631
Iteration 291/1000 | Loss: 0.00001557
Iteration 292/1000 | Loss: 0.00001511
Iteration 293/1000 | Loss: 0.00002222
Iteration 294/1000 | Loss: 0.00001567
Iteration 295/1000 | Loss: 0.00001495
Iteration 296/1000 | Loss: 0.00001466
Iteration 297/1000 | Loss: 0.00001443
Iteration 298/1000 | Loss: 0.00001433
Iteration 299/1000 | Loss: 0.00001430
Iteration 300/1000 | Loss: 0.00001429
Iteration 301/1000 | Loss: 0.00001429
Iteration 302/1000 | Loss: 0.00001428
Iteration 303/1000 | Loss: 0.00001428
Iteration 304/1000 | Loss: 0.00001426
Iteration 305/1000 | Loss: 0.00001425
Iteration 306/1000 | Loss: 0.00001424
Iteration 307/1000 | Loss: 0.00001423
Iteration 308/1000 | Loss: 0.00001418
Iteration 309/1000 | Loss: 0.00001418
Iteration 310/1000 | Loss: 0.00001418
Iteration 311/1000 | Loss: 0.00001418
Iteration 312/1000 | Loss: 0.00001418
Iteration 313/1000 | Loss: 0.00001418
Iteration 314/1000 | Loss: 0.00001418
Iteration 315/1000 | Loss: 0.00001418
Iteration 316/1000 | Loss: 0.00001418
Iteration 317/1000 | Loss: 0.00001418
Iteration 318/1000 | Loss: 0.00001418
Iteration 319/1000 | Loss: 0.00001418
Iteration 320/1000 | Loss: 0.00001418
Iteration 321/1000 | Loss: 0.00001417
Iteration 322/1000 | Loss: 0.00001416
Iteration 323/1000 | Loss: 0.00001416
Iteration 324/1000 | Loss: 0.00001416
Iteration 325/1000 | Loss: 0.00001416
Iteration 326/1000 | Loss: 0.00001416
Iteration 327/1000 | Loss: 0.00001416
Iteration 328/1000 | Loss: 0.00001416
Iteration 329/1000 | Loss: 0.00001416
Iteration 330/1000 | Loss: 0.00001415
Iteration 331/1000 | Loss: 0.00001415
Iteration 332/1000 | Loss: 0.00001415
Iteration 333/1000 | Loss: 0.00001415
Iteration 334/1000 | Loss: 0.00001409
Iteration 335/1000 | Loss: 0.00001409
Iteration 336/1000 | Loss: 0.00001408
Iteration 337/1000 | Loss: 0.00001408
Iteration 338/1000 | Loss: 0.00001407
Iteration 339/1000 | Loss: 0.00001407
Iteration 340/1000 | Loss: 0.00001407
Iteration 341/1000 | Loss: 0.00001407
Iteration 342/1000 | Loss: 0.00001407
Iteration 343/1000 | Loss: 0.00001407
Iteration 344/1000 | Loss: 0.00001406
Iteration 345/1000 | Loss: 0.00001406
Iteration 346/1000 | Loss: 0.00001406
Iteration 347/1000 | Loss: 0.00001406
Iteration 348/1000 | Loss: 0.00001406
Iteration 349/1000 | Loss: 0.00001406
Iteration 350/1000 | Loss: 0.00001406
Iteration 351/1000 | Loss: 0.00001406
Iteration 352/1000 | Loss: 0.00001406
Iteration 353/1000 | Loss: 0.00001406
Iteration 354/1000 | Loss: 0.00001406
Iteration 355/1000 | Loss: 0.00001406
Iteration 356/1000 | Loss: 0.00001406
Iteration 357/1000 | Loss: 0.00001406
Iteration 358/1000 | Loss: 0.00001406
Iteration 359/1000 | Loss: 0.00001406
Iteration 360/1000 | Loss: 0.00001406
Iteration 361/1000 | Loss: 0.00001406
Iteration 362/1000 | Loss: 0.00001406
Iteration 363/1000 | Loss: 0.00001406
Iteration 364/1000 | Loss: 0.00001406
Iteration 365/1000 | Loss: 0.00001406
Iteration 366/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 366. Stopping optimization.
Last 5 losses: [1.4055814062885474e-05, 1.4055814062885474e-05, 1.4055814062885474e-05, 1.4055814062885474e-05, 1.4055814062885474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4055814062885474e-05

Optimization complete. Final v2v error: 3.257448434829712 mm

Highest mean error: 4.82838773727417 mm for frame 118

Lowest mean error: 2.9387614727020264 mm for frame 103

Saving results

Total time: 485.344482421875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786048
Iteration 2/25 | Loss: 0.00216004
Iteration 3/25 | Loss: 0.00162226
Iteration 4/25 | Loss: 0.00157887
Iteration 5/25 | Loss: 0.00155325
Iteration 6/25 | Loss: 0.00154831
Iteration 7/25 | Loss: 0.00154725
Iteration 8/25 | Loss: 0.00154705
Iteration 9/25 | Loss: 0.00154705
Iteration 10/25 | Loss: 0.00154705
Iteration 11/25 | Loss: 0.00154705
Iteration 12/25 | Loss: 0.00154705
Iteration 13/25 | Loss: 0.00154705
Iteration 14/25 | Loss: 0.00154705
Iteration 15/25 | Loss: 0.00154705
Iteration 16/25 | Loss: 0.00154705
Iteration 17/25 | Loss: 0.00154705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015470515936613083, 0.0015470515936613083, 0.0015470515936613083, 0.0015470515936613083, 0.0015470515936613083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015470515936613083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20611584
Iteration 2/25 | Loss: 0.00195269
Iteration 3/25 | Loss: 0.00195268
Iteration 4/25 | Loss: 0.00195268
Iteration 5/25 | Loss: 0.00195268
Iteration 6/25 | Loss: 0.00195268
Iteration 7/25 | Loss: 0.00195268
Iteration 8/25 | Loss: 0.00195268
Iteration 9/25 | Loss: 0.00195268
Iteration 10/25 | Loss: 0.00195268
Iteration 11/25 | Loss: 0.00195268
Iteration 12/25 | Loss: 0.00195268
Iteration 13/25 | Loss: 0.00195268
Iteration 14/25 | Loss: 0.00195268
Iteration 15/25 | Loss: 0.00195268
Iteration 16/25 | Loss: 0.00195268
Iteration 17/25 | Loss: 0.00195268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019526802934706211, 0.0019526802934706211, 0.0019526802934706211, 0.0019526802934706211, 0.0019526802934706211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019526802934706211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195268
Iteration 2/1000 | Loss: 0.00006540
Iteration 3/1000 | Loss: 0.00003973
Iteration 4/1000 | Loss: 0.00003488
Iteration 5/1000 | Loss: 0.00003282
Iteration 6/1000 | Loss: 0.00003121
Iteration 7/1000 | Loss: 0.00003054
Iteration 8/1000 | Loss: 0.00002995
Iteration 9/1000 | Loss: 0.00002942
Iteration 10/1000 | Loss: 0.00002914
Iteration 11/1000 | Loss: 0.00002891
Iteration 12/1000 | Loss: 0.00002882
Iteration 13/1000 | Loss: 0.00002875
Iteration 14/1000 | Loss: 0.00002869
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00002862
Iteration 17/1000 | Loss: 0.00002862
Iteration 18/1000 | Loss: 0.00002861
Iteration 19/1000 | Loss: 0.00002849
Iteration 20/1000 | Loss: 0.00002846
Iteration 21/1000 | Loss: 0.00002845
Iteration 22/1000 | Loss: 0.00002842
Iteration 23/1000 | Loss: 0.00002842
Iteration 24/1000 | Loss: 0.00002842
Iteration 25/1000 | Loss: 0.00002842
Iteration 26/1000 | Loss: 0.00002842
Iteration 27/1000 | Loss: 0.00002841
Iteration 28/1000 | Loss: 0.00002840
Iteration 29/1000 | Loss: 0.00002839
Iteration 30/1000 | Loss: 0.00002838
Iteration 31/1000 | Loss: 0.00002837
Iteration 32/1000 | Loss: 0.00002836
Iteration 33/1000 | Loss: 0.00002832
Iteration 34/1000 | Loss: 0.00002831
Iteration 35/1000 | Loss: 0.00002831
Iteration 36/1000 | Loss: 0.00002829
Iteration 37/1000 | Loss: 0.00002828
Iteration 38/1000 | Loss: 0.00002828
Iteration 39/1000 | Loss: 0.00002828
Iteration 40/1000 | Loss: 0.00002828
Iteration 41/1000 | Loss: 0.00002828
Iteration 42/1000 | Loss: 0.00002828
Iteration 43/1000 | Loss: 0.00002828
Iteration 44/1000 | Loss: 0.00002828
Iteration 45/1000 | Loss: 0.00002828
Iteration 46/1000 | Loss: 0.00002828
Iteration 47/1000 | Loss: 0.00002827
Iteration 48/1000 | Loss: 0.00002827
Iteration 49/1000 | Loss: 0.00002827
Iteration 50/1000 | Loss: 0.00002827
Iteration 51/1000 | Loss: 0.00002826
Iteration 52/1000 | Loss: 0.00002826
Iteration 53/1000 | Loss: 0.00002825
Iteration 54/1000 | Loss: 0.00002825
Iteration 55/1000 | Loss: 0.00002824
Iteration 56/1000 | Loss: 0.00002824
Iteration 57/1000 | Loss: 0.00002824
Iteration 58/1000 | Loss: 0.00002824
Iteration 59/1000 | Loss: 0.00002823
Iteration 60/1000 | Loss: 0.00002823
Iteration 61/1000 | Loss: 0.00002823
Iteration 62/1000 | Loss: 0.00002823
Iteration 63/1000 | Loss: 0.00002823
Iteration 64/1000 | Loss: 0.00002822
Iteration 65/1000 | Loss: 0.00002822
Iteration 66/1000 | Loss: 0.00002822
Iteration 67/1000 | Loss: 0.00002822
Iteration 68/1000 | Loss: 0.00002822
Iteration 69/1000 | Loss: 0.00002822
Iteration 70/1000 | Loss: 0.00002822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.8219365049153566e-05, 2.8219365049153566e-05, 2.8219365049153566e-05, 2.8219365049153566e-05, 2.8219365049153566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8219365049153566e-05

Optimization complete. Final v2v error: 4.622512340545654 mm

Highest mean error: 4.9757914543151855 mm for frame 223

Lowest mean error: 4.091274261474609 mm for frame 43

Saving results

Total time: 43.158483028411865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957920
Iteration 2/25 | Loss: 0.00195208
Iteration 3/25 | Loss: 0.00147964
Iteration 4/25 | Loss: 0.00143787
Iteration 5/25 | Loss: 0.00142809
Iteration 6/25 | Loss: 0.00140658
Iteration 7/25 | Loss: 0.00140660
Iteration 8/25 | Loss: 0.00140590
Iteration 9/25 | Loss: 0.00139406
Iteration 10/25 | Loss: 0.00138354
Iteration 11/25 | Loss: 0.00137866
Iteration 12/25 | Loss: 0.00136520
Iteration 13/25 | Loss: 0.00135709
Iteration 14/25 | Loss: 0.00135082
Iteration 15/25 | Loss: 0.00134885
Iteration 16/25 | Loss: 0.00134739
Iteration 17/25 | Loss: 0.00134722
Iteration 18/25 | Loss: 0.00134719
Iteration 19/25 | Loss: 0.00134719
Iteration 20/25 | Loss: 0.00134719
Iteration 21/25 | Loss: 0.00134719
Iteration 22/25 | Loss: 0.00134719
Iteration 23/25 | Loss: 0.00134719
Iteration 24/25 | Loss: 0.00134719
Iteration 25/25 | Loss: 0.00134719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34670019
Iteration 2/25 | Loss: 0.00254630
Iteration 3/25 | Loss: 0.00254630
Iteration 4/25 | Loss: 0.00254630
Iteration 5/25 | Loss: 0.00254630
Iteration 6/25 | Loss: 0.00254630
Iteration 7/25 | Loss: 0.00254630
Iteration 8/25 | Loss: 0.00254630
Iteration 9/25 | Loss: 0.00254630
Iteration 10/25 | Loss: 0.00254630
Iteration 11/25 | Loss: 0.00254630
Iteration 12/25 | Loss: 0.00254630
Iteration 13/25 | Loss: 0.00254630
Iteration 14/25 | Loss: 0.00254630
Iteration 15/25 | Loss: 0.00254630
Iteration 16/25 | Loss: 0.00254630
Iteration 17/25 | Loss: 0.00254630
Iteration 18/25 | Loss: 0.00254630
Iteration 19/25 | Loss: 0.00254630
Iteration 20/25 | Loss: 0.00254630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0025462950579822063, 0.0025462950579822063, 0.0025462950579822063, 0.0025462950579822063, 0.0025462950579822063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025462950579822063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254630
Iteration 2/1000 | Loss: 0.00005861
Iteration 3/1000 | Loss: 0.00009459
Iteration 4/1000 | Loss: 0.00003210
Iteration 5/1000 | Loss: 0.00002644
Iteration 6/1000 | Loss: 0.00006883
Iteration 7/1000 | Loss: 0.00004796
Iteration 8/1000 | Loss: 0.00027695
Iteration 9/1000 | Loss: 0.00005459
Iteration 10/1000 | Loss: 0.00018624
Iteration 11/1000 | Loss: 0.00006720
Iteration 12/1000 | Loss: 0.00002538
Iteration 13/1000 | Loss: 0.00004146
Iteration 14/1000 | Loss: 0.00004831
Iteration 15/1000 | Loss: 0.00009062
Iteration 16/1000 | Loss: 0.00002824
Iteration 17/1000 | Loss: 0.00003632
Iteration 18/1000 | Loss: 0.00002079
Iteration 19/1000 | Loss: 0.00006909
Iteration 20/1000 | Loss: 0.00002051
Iteration 21/1000 | Loss: 0.00010552
Iteration 22/1000 | Loss: 0.00009167
Iteration 23/1000 | Loss: 0.00006074
Iteration 24/1000 | Loss: 0.00014953
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00001814
Iteration 27/1000 | Loss: 0.00014318
Iteration 28/1000 | Loss: 0.00008871
Iteration 29/1000 | Loss: 0.00006158
Iteration 30/1000 | Loss: 0.00008464
Iteration 31/1000 | Loss: 0.00002542
Iteration 32/1000 | Loss: 0.00004639
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001518
Iteration 35/1000 | Loss: 0.00004738
Iteration 36/1000 | Loss: 0.00008073
Iteration 37/1000 | Loss: 0.00003135
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001586
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001385
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001366
Iteration 45/1000 | Loss: 0.00006149
Iteration 46/1000 | Loss: 0.00013201
Iteration 47/1000 | Loss: 0.00012056
Iteration 48/1000 | Loss: 0.00005872
Iteration 49/1000 | Loss: 0.00001543
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00003239
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00003222
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001332
Iteration 108/1000 | Loss: 0.00001332
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001332
Iteration 112/1000 | Loss: 0.00001332
Iteration 113/1000 | Loss: 0.00001332
Iteration 114/1000 | Loss: 0.00001332
Iteration 115/1000 | Loss: 0.00001332
Iteration 116/1000 | Loss: 0.00001332
Iteration 117/1000 | Loss: 0.00001332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.3320236575964373e-05, 1.3320236575964373e-05, 1.3320236575964373e-05, 1.3320236575964373e-05, 1.3320236575964373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3320236575964373e-05

Optimization complete. Final v2v error: 3.1303484439849854 mm

Highest mean error: 4.8598432540893555 mm for frame 58

Lowest mean error: 2.5530200004577637 mm for frame 97

Saving results

Total time: 109.09604501724243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676669
Iteration 2/25 | Loss: 0.00149699
Iteration 3/25 | Loss: 0.00139338
Iteration 4/25 | Loss: 0.00133397
Iteration 5/25 | Loss: 0.00134784
Iteration 6/25 | Loss: 0.00132261
Iteration 7/25 | Loss: 0.00132716
Iteration 8/25 | Loss: 0.00132449
Iteration 9/25 | Loss: 0.00132706
Iteration 10/25 | Loss: 0.00132074
Iteration 11/25 | Loss: 0.00131985
Iteration 12/25 | Loss: 0.00131853
Iteration 13/25 | Loss: 0.00131853
Iteration 14/25 | Loss: 0.00131853
Iteration 15/25 | Loss: 0.00131853
Iteration 16/25 | Loss: 0.00131853
Iteration 17/25 | Loss: 0.00131853
Iteration 18/25 | Loss: 0.00131853
Iteration 19/25 | Loss: 0.00131852
Iteration 20/25 | Loss: 0.00131852
Iteration 21/25 | Loss: 0.00131852
Iteration 22/25 | Loss: 0.00131852
Iteration 23/25 | Loss: 0.00131852
Iteration 24/25 | Loss: 0.00131852
Iteration 25/25 | Loss: 0.00131852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50889826
Iteration 2/25 | Loss: 0.00218362
Iteration 3/25 | Loss: 0.00218362
Iteration 4/25 | Loss: 0.00218362
Iteration 5/25 | Loss: 0.00218362
Iteration 6/25 | Loss: 0.00218362
Iteration 7/25 | Loss: 0.00218362
Iteration 8/25 | Loss: 0.00218362
Iteration 9/25 | Loss: 0.00218362
Iteration 10/25 | Loss: 0.00218362
Iteration 11/25 | Loss: 0.00218362
Iteration 12/25 | Loss: 0.00218362
Iteration 13/25 | Loss: 0.00218362
Iteration 14/25 | Loss: 0.00218362
Iteration 15/25 | Loss: 0.00218362
Iteration 16/25 | Loss: 0.00218362
Iteration 17/25 | Loss: 0.00218362
Iteration 18/25 | Loss: 0.00218362
Iteration 19/25 | Loss: 0.00218362
Iteration 20/25 | Loss: 0.00218362
Iteration 21/25 | Loss: 0.00218362
Iteration 22/25 | Loss: 0.00218362
Iteration 23/25 | Loss: 0.00218362
Iteration 24/25 | Loss: 0.00218362
Iteration 25/25 | Loss: 0.00218362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218362
Iteration 2/1000 | Loss: 0.00001961
Iteration 3/1000 | Loss: 0.00001610
Iteration 4/1000 | Loss: 0.00001467
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001208
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001182
Iteration 13/1000 | Loss: 0.00001167
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001134
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001134
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001133
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001129
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001126
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001123
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001122
Iteration 47/1000 | Loss: 0.00001121
Iteration 48/1000 | Loss: 0.00001121
Iteration 49/1000 | Loss: 0.00001121
Iteration 50/1000 | Loss: 0.00001120
Iteration 51/1000 | Loss: 0.00001120
Iteration 52/1000 | Loss: 0.00001120
Iteration 53/1000 | Loss: 0.00001120
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001119
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001118
Iteration 60/1000 | Loss: 0.00001118
Iteration 61/1000 | Loss: 0.00001118
Iteration 62/1000 | Loss: 0.00001118
Iteration 63/1000 | Loss: 0.00001118
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001107
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001106
Iteration 109/1000 | Loss: 0.00001106
Iteration 110/1000 | Loss: 0.00001105
Iteration 111/1000 | Loss: 0.00001105
Iteration 112/1000 | Loss: 0.00001105
Iteration 113/1000 | Loss: 0.00001105
Iteration 114/1000 | Loss: 0.00001105
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001104
Iteration 121/1000 | Loss: 0.00001104
Iteration 122/1000 | Loss: 0.00001104
Iteration 123/1000 | Loss: 0.00001104
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001103
Iteration 126/1000 | Loss: 0.00001103
Iteration 127/1000 | Loss: 0.00001103
Iteration 128/1000 | Loss: 0.00001103
Iteration 129/1000 | Loss: 0.00001103
Iteration 130/1000 | Loss: 0.00001103
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001102
Iteration 135/1000 | Loss: 0.00001102
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001101
Iteration 138/1000 | Loss: 0.00001101
Iteration 139/1000 | Loss: 0.00001101
Iteration 140/1000 | Loss: 0.00001101
Iteration 141/1000 | Loss: 0.00001101
Iteration 142/1000 | Loss: 0.00001101
Iteration 143/1000 | Loss: 0.00001101
Iteration 144/1000 | Loss: 0.00001101
Iteration 145/1000 | Loss: 0.00001101
Iteration 146/1000 | Loss: 0.00001101
Iteration 147/1000 | Loss: 0.00001101
Iteration 148/1000 | Loss: 0.00001101
Iteration 149/1000 | Loss: 0.00001101
Iteration 150/1000 | Loss: 0.00001101
Iteration 151/1000 | Loss: 0.00001101
Iteration 152/1000 | Loss: 0.00001101
Iteration 153/1000 | Loss: 0.00001101
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.1013700714102015e-05, 1.1013700714102015e-05, 1.1013700714102015e-05, 1.1013700714102015e-05, 1.1013700714102015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1013700714102015e-05

Optimization complete. Final v2v error: 2.8871209621429443 mm

Highest mean error: 3.1574270725250244 mm for frame 201

Lowest mean error: 2.7106385231018066 mm for frame 41

Saving results

Total time: 59.32684588432312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000826
Iteration 2/25 | Loss: 0.00273745
Iteration 3/25 | Loss: 0.00221702
Iteration 4/25 | Loss: 0.00219822
Iteration 5/25 | Loss: 0.00233041
Iteration 6/25 | Loss: 0.00216595
Iteration 7/25 | Loss: 0.00188854
Iteration 8/25 | Loss: 0.00177317
Iteration 9/25 | Loss: 0.00169103
Iteration 10/25 | Loss: 0.00166612
Iteration 11/25 | Loss: 0.00165847
Iteration 12/25 | Loss: 0.00165798
Iteration 13/25 | Loss: 0.00165994
Iteration 14/25 | Loss: 0.00164845
Iteration 15/25 | Loss: 0.00169066
Iteration 16/25 | Loss: 0.00166886
Iteration 17/25 | Loss: 0.00162406
Iteration 18/25 | Loss: 0.00160377
Iteration 19/25 | Loss: 0.00157301
Iteration 20/25 | Loss: 0.00157239
Iteration 21/25 | Loss: 0.00156070
Iteration 22/25 | Loss: 0.00152383
Iteration 23/25 | Loss: 0.00153413
Iteration 24/25 | Loss: 0.00153973
Iteration 25/25 | Loss: 0.00154900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14056396
Iteration 2/25 | Loss: 0.00515352
Iteration 3/25 | Loss: 0.00344146
Iteration 4/25 | Loss: 0.00344146
Iteration 5/25 | Loss: 0.00344145
Iteration 6/25 | Loss: 0.00344145
Iteration 7/25 | Loss: 0.00344145
Iteration 8/25 | Loss: 0.00344145
Iteration 9/25 | Loss: 0.00344145
Iteration 10/25 | Loss: 0.00344145
Iteration 11/25 | Loss: 0.00344145
Iteration 12/25 | Loss: 0.00344145
Iteration 13/25 | Loss: 0.00344145
Iteration 14/25 | Loss: 0.00344145
Iteration 15/25 | Loss: 0.00344145
Iteration 16/25 | Loss: 0.00344145
Iteration 17/25 | Loss: 0.00344145
Iteration 18/25 | Loss: 0.00344145
Iteration 19/25 | Loss: 0.00344145
Iteration 20/25 | Loss: 0.00344145
Iteration 21/25 | Loss: 0.00344145
Iteration 22/25 | Loss: 0.00344145
Iteration 23/25 | Loss: 0.00344145
Iteration 24/25 | Loss: 0.00344145
Iteration 25/25 | Loss: 0.00344145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00344145
Iteration 2/1000 | Loss: 0.00173790
Iteration 3/1000 | Loss: 0.00042959
Iteration 4/1000 | Loss: 0.00107955
Iteration 5/1000 | Loss: 0.00054596
Iteration 6/1000 | Loss: 0.00040522
Iteration 7/1000 | Loss: 0.00030643
Iteration 8/1000 | Loss: 0.00027661
Iteration 9/1000 | Loss: 0.00009278
Iteration 10/1000 | Loss: 0.00009237
Iteration 11/1000 | Loss: 0.00027682
Iteration 12/1000 | Loss: 0.00086434
Iteration 13/1000 | Loss: 0.00081051
Iteration 14/1000 | Loss: 0.00059906
Iteration 15/1000 | Loss: 0.00105290
Iteration 16/1000 | Loss: 0.00051801
Iteration 17/1000 | Loss: 0.00032006
Iteration 18/1000 | Loss: 0.00043301
Iteration 19/1000 | Loss: 0.00029120
Iteration 20/1000 | Loss: 0.00035318
Iteration 21/1000 | Loss: 0.00032383
Iteration 22/1000 | Loss: 0.00032320
Iteration 23/1000 | Loss: 0.00019460
Iteration 24/1000 | Loss: 0.00006689
Iteration 25/1000 | Loss: 0.00037171
Iteration 26/1000 | Loss: 0.00028586
Iteration 27/1000 | Loss: 0.00005658
Iteration 28/1000 | Loss: 0.00005248
Iteration 29/1000 | Loss: 0.00005653
Iteration 30/1000 | Loss: 0.00010056
Iteration 31/1000 | Loss: 0.00010319
Iteration 32/1000 | Loss: 0.00004639
Iteration 33/1000 | Loss: 0.00043934
Iteration 34/1000 | Loss: 0.00040540
Iteration 35/1000 | Loss: 0.00018498
Iteration 36/1000 | Loss: 0.00041106
Iteration 37/1000 | Loss: 0.00008055
Iteration 38/1000 | Loss: 0.00005167
Iteration 39/1000 | Loss: 0.00004432
Iteration 40/1000 | Loss: 0.00004101
Iteration 41/1000 | Loss: 0.00007145
Iteration 42/1000 | Loss: 0.00004537
Iteration 43/1000 | Loss: 0.00003791
Iteration 44/1000 | Loss: 0.00007389
Iteration 45/1000 | Loss: 0.00003691
Iteration 46/1000 | Loss: 0.00003632
Iteration 47/1000 | Loss: 0.00004489
Iteration 48/1000 | Loss: 0.00014900
Iteration 49/1000 | Loss: 0.00004613
Iteration 50/1000 | Loss: 0.00003525
Iteration 51/1000 | Loss: 0.00003458
Iteration 52/1000 | Loss: 0.00003397
Iteration 53/1000 | Loss: 0.00009969
Iteration 54/1000 | Loss: 0.00003392
Iteration 55/1000 | Loss: 0.00003351
Iteration 56/1000 | Loss: 0.00006983
Iteration 57/1000 | Loss: 0.00003412
Iteration 58/1000 | Loss: 0.00003349
Iteration 59/1000 | Loss: 0.00003286
Iteration 60/1000 | Loss: 0.00003256
Iteration 61/1000 | Loss: 0.00003253
Iteration 62/1000 | Loss: 0.00003247
Iteration 63/1000 | Loss: 0.00003243
Iteration 64/1000 | Loss: 0.00003242
Iteration 65/1000 | Loss: 0.00003241
Iteration 66/1000 | Loss: 0.00003239
Iteration 67/1000 | Loss: 0.00003227
Iteration 68/1000 | Loss: 0.00003226
Iteration 69/1000 | Loss: 0.00003216
Iteration 70/1000 | Loss: 0.00003211
Iteration 71/1000 | Loss: 0.00003207
Iteration 72/1000 | Loss: 0.00003206
Iteration 73/1000 | Loss: 0.00003206
Iteration 74/1000 | Loss: 0.00003205
Iteration 75/1000 | Loss: 0.00003202
Iteration 76/1000 | Loss: 0.00003200
Iteration 77/1000 | Loss: 0.00003200
Iteration 78/1000 | Loss: 0.00003193
Iteration 79/1000 | Loss: 0.00003192
Iteration 80/1000 | Loss: 0.00003191
Iteration 81/1000 | Loss: 0.00003190
Iteration 82/1000 | Loss: 0.00003189
Iteration 83/1000 | Loss: 0.00003188
Iteration 84/1000 | Loss: 0.00003188
Iteration 85/1000 | Loss: 0.00003188
Iteration 86/1000 | Loss: 0.00003187
Iteration 87/1000 | Loss: 0.00003187
Iteration 88/1000 | Loss: 0.00003187
Iteration 89/1000 | Loss: 0.00003187
Iteration 90/1000 | Loss: 0.00003187
Iteration 91/1000 | Loss: 0.00003187
Iteration 92/1000 | Loss: 0.00003187
Iteration 93/1000 | Loss: 0.00003186
Iteration 94/1000 | Loss: 0.00003186
Iteration 95/1000 | Loss: 0.00003186
Iteration 96/1000 | Loss: 0.00003186
Iteration 97/1000 | Loss: 0.00003186
Iteration 98/1000 | Loss: 0.00003186
Iteration 99/1000 | Loss: 0.00003186
Iteration 100/1000 | Loss: 0.00003186
Iteration 101/1000 | Loss: 0.00003186
Iteration 102/1000 | Loss: 0.00003186
Iteration 103/1000 | Loss: 0.00003186
Iteration 104/1000 | Loss: 0.00003186
Iteration 105/1000 | Loss: 0.00003186
Iteration 106/1000 | Loss: 0.00003186
Iteration 107/1000 | Loss: 0.00003185
Iteration 108/1000 | Loss: 0.00003185
Iteration 109/1000 | Loss: 0.00003185
Iteration 110/1000 | Loss: 0.00003185
Iteration 111/1000 | Loss: 0.00007135
Iteration 112/1000 | Loss: 0.00003188
Iteration 113/1000 | Loss: 0.00003183
Iteration 114/1000 | Loss: 0.00003183
Iteration 115/1000 | Loss: 0.00003182
Iteration 116/1000 | Loss: 0.00003182
Iteration 117/1000 | Loss: 0.00003182
Iteration 118/1000 | Loss: 0.00003182
Iteration 119/1000 | Loss: 0.00003182
Iteration 120/1000 | Loss: 0.00003181
Iteration 121/1000 | Loss: 0.00003181
Iteration 122/1000 | Loss: 0.00003181
Iteration 123/1000 | Loss: 0.00003181
Iteration 124/1000 | Loss: 0.00003180
Iteration 125/1000 | Loss: 0.00003179
Iteration 126/1000 | Loss: 0.00003179
Iteration 127/1000 | Loss: 0.00003179
Iteration 128/1000 | Loss: 0.00003179
Iteration 129/1000 | Loss: 0.00003179
Iteration 130/1000 | Loss: 0.00003179
Iteration 131/1000 | Loss: 0.00003179
Iteration 132/1000 | Loss: 0.00003179
Iteration 133/1000 | Loss: 0.00003179
Iteration 134/1000 | Loss: 0.00003179
Iteration 135/1000 | Loss: 0.00003178
Iteration 136/1000 | Loss: 0.00003178
Iteration 137/1000 | Loss: 0.00003178
Iteration 138/1000 | Loss: 0.00003178
Iteration 139/1000 | Loss: 0.00003177
Iteration 140/1000 | Loss: 0.00003177
Iteration 141/1000 | Loss: 0.00003177
Iteration 142/1000 | Loss: 0.00003177
Iteration 143/1000 | Loss: 0.00003177
Iteration 144/1000 | Loss: 0.00003177
Iteration 145/1000 | Loss: 0.00003176
Iteration 146/1000 | Loss: 0.00003176
Iteration 147/1000 | Loss: 0.00003176
Iteration 148/1000 | Loss: 0.00003176
Iteration 149/1000 | Loss: 0.00003176
Iteration 150/1000 | Loss: 0.00003176
Iteration 151/1000 | Loss: 0.00003175
Iteration 152/1000 | Loss: 0.00003175
Iteration 153/1000 | Loss: 0.00003175
Iteration 154/1000 | Loss: 0.00003175
Iteration 155/1000 | Loss: 0.00003175
Iteration 156/1000 | Loss: 0.00003175
Iteration 157/1000 | Loss: 0.00003175
Iteration 158/1000 | Loss: 0.00003174
Iteration 159/1000 | Loss: 0.00003174
Iteration 160/1000 | Loss: 0.00003174
Iteration 161/1000 | Loss: 0.00003174
Iteration 162/1000 | Loss: 0.00003173
Iteration 163/1000 | Loss: 0.00003173
Iteration 164/1000 | Loss: 0.00003173
Iteration 165/1000 | Loss: 0.00003173
Iteration 166/1000 | Loss: 0.00003172
Iteration 167/1000 | Loss: 0.00003172
Iteration 168/1000 | Loss: 0.00003171
Iteration 169/1000 | Loss: 0.00003171
Iteration 170/1000 | Loss: 0.00003170
Iteration 171/1000 | Loss: 0.00003169
Iteration 172/1000 | Loss: 0.00003169
Iteration 173/1000 | Loss: 0.00003169
Iteration 174/1000 | Loss: 0.00003169
Iteration 175/1000 | Loss: 0.00003169
Iteration 176/1000 | Loss: 0.00003168
Iteration 177/1000 | Loss: 0.00003168
Iteration 178/1000 | Loss: 0.00003168
Iteration 179/1000 | Loss: 0.00003168
Iteration 180/1000 | Loss: 0.00003168
Iteration 181/1000 | Loss: 0.00003168
Iteration 182/1000 | Loss: 0.00003168
Iteration 183/1000 | Loss: 0.00003168
Iteration 184/1000 | Loss: 0.00003168
Iteration 185/1000 | Loss: 0.00003168
Iteration 186/1000 | Loss: 0.00003167
Iteration 187/1000 | Loss: 0.00003167
Iteration 188/1000 | Loss: 0.00003167
Iteration 189/1000 | Loss: 0.00003166
Iteration 190/1000 | Loss: 0.00003166
Iteration 191/1000 | Loss: 0.00003166
Iteration 192/1000 | Loss: 0.00003165
Iteration 193/1000 | Loss: 0.00003165
Iteration 194/1000 | Loss: 0.00003165
Iteration 195/1000 | Loss: 0.00003165
Iteration 196/1000 | Loss: 0.00003164
Iteration 197/1000 | Loss: 0.00003164
Iteration 198/1000 | Loss: 0.00003164
Iteration 199/1000 | Loss: 0.00003164
Iteration 200/1000 | Loss: 0.00003164
Iteration 201/1000 | Loss: 0.00003163
Iteration 202/1000 | Loss: 0.00003162
Iteration 203/1000 | Loss: 0.00003162
Iteration 204/1000 | Loss: 0.00003162
Iteration 205/1000 | Loss: 0.00003161
Iteration 206/1000 | Loss: 0.00003161
Iteration 207/1000 | Loss: 0.00003161
Iteration 208/1000 | Loss: 0.00003160
Iteration 209/1000 | Loss: 0.00003160
Iteration 210/1000 | Loss: 0.00003160
Iteration 211/1000 | Loss: 0.00003160
Iteration 212/1000 | Loss: 0.00003159
Iteration 213/1000 | Loss: 0.00003159
Iteration 214/1000 | Loss: 0.00003159
Iteration 215/1000 | Loss: 0.00003159
Iteration 216/1000 | Loss: 0.00007344
Iteration 217/1000 | Loss: 0.00003162
Iteration 218/1000 | Loss: 0.00003155
Iteration 219/1000 | Loss: 0.00003154
Iteration 220/1000 | Loss: 0.00003154
Iteration 221/1000 | Loss: 0.00003153
Iteration 222/1000 | Loss: 0.00003153
Iteration 223/1000 | Loss: 0.00003153
Iteration 224/1000 | Loss: 0.00003153
Iteration 225/1000 | Loss: 0.00003153
Iteration 226/1000 | Loss: 0.00003153
Iteration 227/1000 | Loss: 0.00003153
Iteration 228/1000 | Loss: 0.00003153
Iteration 229/1000 | Loss: 0.00003153
Iteration 230/1000 | Loss: 0.00003153
Iteration 231/1000 | Loss: 0.00003152
Iteration 232/1000 | Loss: 0.00003152
Iteration 233/1000 | Loss: 0.00003152
Iteration 234/1000 | Loss: 0.00003152
Iteration 235/1000 | Loss: 0.00003152
Iteration 236/1000 | Loss: 0.00003152
Iteration 237/1000 | Loss: 0.00003152
Iteration 238/1000 | Loss: 0.00003152
Iteration 239/1000 | Loss: 0.00003152
Iteration 240/1000 | Loss: 0.00003152
Iteration 241/1000 | Loss: 0.00003152
Iteration 242/1000 | Loss: 0.00003152
Iteration 243/1000 | Loss: 0.00003152
Iteration 244/1000 | Loss: 0.00003152
Iteration 245/1000 | Loss: 0.00003152
Iteration 246/1000 | Loss: 0.00003152
Iteration 247/1000 | Loss: 0.00003152
Iteration 248/1000 | Loss: 0.00003152
Iteration 249/1000 | Loss: 0.00003152
Iteration 250/1000 | Loss: 0.00003152
Iteration 251/1000 | Loss: 0.00003152
Iteration 252/1000 | Loss: 0.00003152
Iteration 253/1000 | Loss: 0.00003152
Iteration 254/1000 | Loss: 0.00003152
Iteration 255/1000 | Loss: 0.00003152
Iteration 256/1000 | Loss: 0.00003152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [3.151605051243678e-05, 3.151605051243678e-05, 3.151605051243678e-05, 3.151605051243678e-05, 3.151605051243678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.151605051243678e-05

Optimization complete. Final v2v error: 4.005589008331299 mm

Highest mean error: 5.776452541351318 mm for frame 91

Lowest mean error: 3.5595715045928955 mm for frame 128

Saving results

Total time: 151.88690900802612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008691
Iteration 2/25 | Loss: 0.00149584
Iteration 3/25 | Loss: 0.00135649
Iteration 4/25 | Loss: 0.00134017
Iteration 5/25 | Loss: 0.00133606
Iteration 6/25 | Loss: 0.00133301
Iteration 7/25 | Loss: 0.00133338
Iteration 8/25 | Loss: 0.00133259
Iteration 9/25 | Loss: 0.00133258
Iteration 10/25 | Loss: 0.00133257
Iteration 11/25 | Loss: 0.00133257
Iteration 12/25 | Loss: 0.00133257
Iteration 13/25 | Loss: 0.00133256
Iteration 14/25 | Loss: 0.00133256
Iteration 15/25 | Loss: 0.00133256
Iteration 16/25 | Loss: 0.00133256
Iteration 17/25 | Loss: 0.00133256
Iteration 18/25 | Loss: 0.00133256
Iteration 19/25 | Loss: 0.00133256
Iteration 20/25 | Loss: 0.00133256
Iteration 21/25 | Loss: 0.00133256
Iteration 22/25 | Loss: 0.00133256
Iteration 23/25 | Loss: 0.00133255
Iteration 24/25 | Loss: 0.00133255
Iteration 25/25 | Loss: 0.00133255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23776114
Iteration 2/25 | Loss: 0.00232782
Iteration 3/25 | Loss: 0.00231210
Iteration 4/25 | Loss: 0.00231209
Iteration 5/25 | Loss: 0.00231209
Iteration 6/25 | Loss: 0.00231209
Iteration 7/25 | Loss: 0.00231209
Iteration 8/25 | Loss: 0.00231209
Iteration 9/25 | Loss: 0.00231209
Iteration 10/25 | Loss: 0.00231209
Iteration 11/25 | Loss: 0.00231209
Iteration 12/25 | Loss: 0.00231209
Iteration 13/25 | Loss: 0.00231209
Iteration 14/25 | Loss: 0.00231209
Iteration 15/25 | Loss: 0.00231209
Iteration 16/25 | Loss: 0.00231209
Iteration 17/25 | Loss: 0.00231209
Iteration 18/25 | Loss: 0.00231209
Iteration 19/25 | Loss: 0.00231209
Iteration 20/25 | Loss: 0.00231209
Iteration 21/25 | Loss: 0.00231209
Iteration 22/25 | Loss: 0.00231209
Iteration 23/25 | Loss: 0.00231209
Iteration 24/25 | Loss: 0.00231209
Iteration 25/25 | Loss: 0.00231209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231209
Iteration 2/1000 | Loss: 0.00004114
Iteration 3/1000 | Loss: 0.00003512
Iteration 4/1000 | Loss: 0.00001631
Iteration 5/1000 | Loss: 0.00003568
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001457
Iteration 8/1000 | Loss: 0.00005760
Iteration 9/1000 | Loss: 0.00006737
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001364
Iteration 13/1000 | Loss: 0.00001333
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001289
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001251
Iteration 25/1000 | Loss: 0.00001251
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001223
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001215
Iteration 65/1000 | Loss: 0.00001215
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001213
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001212
Iteration 71/1000 | Loss: 0.00001212
Iteration 72/1000 | Loss: 0.00001212
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001211
Iteration 75/1000 | Loss: 0.00003517
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001209
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001209
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001208
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001206
Iteration 110/1000 | Loss: 0.00001206
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001203
Iteration 131/1000 | Loss: 0.00001203
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001201
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001200
Iteration 149/1000 | Loss: 0.00001200
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001199
Iteration 155/1000 | Loss: 0.00001199
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001198
Iteration 166/1000 | Loss: 0.00001198
Iteration 167/1000 | Loss: 0.00001198
Iteration 168/1000 | Loss: 0.00001198
Iteration 169/1000 | Loss: 0.00001198
Iteration 170/1000 | Loss: 0.00001198
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001522
Iteration 173/1000 | Loss: 0.00001199
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001197
Iteration 177/1000 | Loss: 0.00001197
Iteration 178/1000 | Loss: 0.00001197
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001197
Iteration 181/1000 | Loss: 0.00001197
Iteration 182/1000 | Loss: 0.00001197
Iteration 183/1000 | Loss: 0.00001197
Iteration 184/1000 | Loss: 0.00001197
Iteration 185/1000 | Loss: 0.00001197
Iteration 186/1000 | Loss: 0.00001197
Iteration 187/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.1967506907240022e-05, 1.1967506907240022e-05, 1.1967506907240022e-05, 1.1967506907240022e-05, 1.1967506907240022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1967506907240022e-05

Optimization complete. Final v2v error: 2.988511562347412 mm

Highest mean error: 3.10310959815979 mm for frame 56

Lowest mean error: 2.6082992553710938 mm for frame 0

Saving results

Total time: 55.18032693862915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00733847
Iteration 2/25 | Loss: 0.00165562
Iteration 3/25 | Loss: 0.00142045
Iteration 4/25 | Loss: 0.00139777
Iteration 5/25 | Loss: 0.00139315
Iteration 6/25 | Loss: 0.00140068
Iteration 7/25 | Loss: 0.00137584
Iteration 8/25 | Loss: 0.00136685
Iteration 9/25 | Loss: 0.00136586
Iteration 10/25 | Loss: 0.00136566
Iteration 11/25 | Loss: 0.00136562
Iteration 12/25 | Loss: 0.00136562
Iteration 13/25 | Loss: 0.00136562
Iteration 14/25 | Loss: 0.00136562
Iteration 15/25 | Loss: 0.00136562
Iteration 16/25 | Loss: 0.00136562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001365623902529478, 0.001365623902529478, 0.001365623902529478, 0.001365623902529478, 0.001365623902529478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001365623902529478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.37536621
Iteration 2/25 | Loss: 0.00196886
Iteration 3/25 | Loss: 0.00196875
Iteration 4/25 | Loss: 0.00196875
Iteration 5/25 | Loss: 0.00196875
Iteration 6/25 | Loss: 0.00196875
Iteration 7/25 | Loss: 0.00196875
Iteration 8/25 | Loss: 0.00196874
Iteration 9/25 | Loss: 0.00196874
Iteration 10/25 | Loss: 0.00196874
Iteration 11/25 | Loss: 0.00196874
Iteration 12/25 | Loss: 0.00196874
Iteration 13/25 | Loss: 0.00196874
Iteration 14/25 | Loss: 0.00196874
Iteration 15/25 | Loss: 0.00196874
Iteration 16/25 | Loss: 0.00196874
Iteration 17/25 | Loss: 0.00196874
Iteration 18/25 | Loss: 0.00196874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019687435124069452, 0.0019687435124069452, 0.0019687435124069452, 0.0019687435124069452, 0.0019687435124069452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019687435124069452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196874
Iteration 2/1000 | Loss: 0.00002894
Iteration 3/1000 | Loss: 0.00002090
Iteration 4/1000 | Loss: 0.00001873
Iteration 5/1000 | Loss: 0.00001777
Iteration 6/1000 | Loss: 0.00001716
Iteration 7/1000 | Loss: 0.00001658
Iteration 8/1000 | Loss: 0.00001628
Iteration 9/1000 | Loss: 0.00001576
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001513
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001476
Iteration 14/1000 | Loss: 0.00001468
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001448
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001442
Iteration 21/1000 | Loss: 0.00001441
Iteration 22/1000 | Loss: 0.00001438
Iteration 23/1000 | Loss: 0.00001437
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001423
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001419
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001418
Iteration 39/1000 | Loss: 0.00001418
Iteration 40/1000 | Loss: 0.00001417
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001414
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001413
Iteration 50/1000 | Loss: 0.00001413
Iteration 51/1000 | Loss: 0.00001413
Iteration 52/1000 | Loss: 0.00001412
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001408
Iteration 60/1000 | Loss: 0.00001408
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001405
Iteration 67/1000 | Loss: 0.00001405
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001405
Iteration 73/1000 | Loss: 0.00001404
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001403
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001402
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001402
Iteration 83/1000 | Loss: 0.00001402
Iteration 84/1000 | Loss: 0.00001402
Iteration 85/1000 | Loss: 0.00001402
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001401
Iteration 90/1000 | Loss: 0.00001401
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001398
Iteration 101/1000 | Loss: 0.00001398
Iteration 102/1000 | Loss: 0.00001398
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001396
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001396
Iteration 120/1000 | Loss: 0.00001396
Iteration 121/1000 | Loss: 0.00001396
Iteration 122/1000 | Loss: 0.00001396
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001395
Iteration 125/1000 | Loss: 0.00001395
Iteration 126/1000 | Loss: 0.00001395
Iteration 127/1000 | Loss: 0.00001395
Iteration 128/1000 | Loss: 0.00001395
Iteration 129/1000 | Loss: 0.00001395
Iteration 130/1000 | Loss: 0.00001395
Iteration 131/1000 | Loss: 0.00001395
Iteration 132/1000 | Loss: 0.00001395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.3954298992757685e-05, 1.3954298992757685e-05, 1.3954298992757685e-05, 1.3954298992757685e-05, 1.3954298992757685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3954298992757685e-05

Optimization complete. Final v2v error: 3.2150065898895264 mm

Highest mean error: 3.8999085426330566 mm for frame 27

Lowest mean error: 2.827241897583008 mm for frame 10

Saving results

Total time: 55.99115538597107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030137
Iteration 2/25 | Loss: 0.01030137
Iteration 3/25 | Loss: 0.01030136
Iteration 4/25 | Loss: 0.01030136
Iteration 5/25 | Loss: 0.01030136
Iteration 6/25 | Loss: 0.01030136
Iteration 7/25 | Loss: 0.01030135
Iteration 8/25 | Loss: 0.01030135
Iteration 9/25 | Loss: 0.01030135
Iteration 10/25 | Loss: 0.01030135
Iteration 11/25 | Loss: 0.01030134
Iteration 12/25 | Loss: 0.01030134
Iteration 13/25 | Loss: 0.01030134
Iteration 14/25 | Loss: 0.01030133
Iteration 15/25 | Loss: 0.01030133
Iteration 16/25 | Loss: 0.01030133
Iteration 17/25 | Loss: 0.01030133
Iteration 18/25 | Loss: 0.01030133
Iteration 19/25 | Loss: 0.01030133
Iteration 20/25 | Loss: 0.01030132
Iteration 21/25 | Loss: 0.01030132
Iteration 22/25 | Loss: 0.01030132
Iteration 23/25 | Loss: 0.01030132
Iteration 24/25 | Loss: 0.01030132
Iteration 25/25 | Loss: 0.01030131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59493113
Iteration 2/25 | Loss: 0.05386755
Iteration 3/25 | Loss: 0.05101962
Iteration 4/25 | Loss: 0.05101962
Iteration 5/25 | Loss: 0.05101962
Iteration 6/25 | Loss: 0.05101962
Iteration 7/25 | Loss: 0.05101962
Iteration 8/25 | Loss: 0.05101962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.051019616425037384, 0.051019616425037384, 0.051019616425037384, 0.051019616425037384, 0.051019616425037384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.051019616425037384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05101962
Iteration 2/1000 | Loss: 0.00084946
Iteration 3/1000 | Loss: 0.00042348
Iteration 4/1000 | Loss: 0.00007736
Iteration 5/1000 | Loss: 0.00004431
Iteration 6/1000 | Loss: 0.00003100
Iteration 7/1000 | Loss: 0.00002578
Iteration 8/1000 | Loss: 0.00002174
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001647
Iteration 11/1000 | Loss: 0.00001547
Iteration 12/1000 | Loss: 0.00001431
Iteration 13/1000 | Loss: 0.00001365
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001163
Iteration 17/1000 | Loss: 0.00001114
Iteration 18/1000 | Loss: 0.00001091
Iteration 19/1000 | Loss: 0.00001058
Iteration 20/1000 | Loss: 0.00001023
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000964
Iteration 23/1000 | Loss: 0.00000962
Iteration 24/1000 | Loss: 0.00000935
Iteration 25/1000 | Loss: 0.00000923
Iteration 26/1000 | Loss: 0.00000918
Iteration 27/1000 | Loss: 0.00000906
Iteration 28/1000 | Loss: 0.00000894
Iteration 29/1000 | Loss: 0.00000887
Iteration 30/1000 | Loss: 0.00000886
Iteration 31/1000 | Loss: 0.00000886
Iteration 32/1000 | Loss: 0.00000874
Iteration 33/1000 | Loss: 0.00000873
Iteration 34/1000 | Loss: 0.00000869
Iteration 35/1000 | Loss: 0.00000869
Iteration 36/1000 | Loss: 0.00000867
Iteration 37/1000 | Loss: 0.00000866
Iteration 38/1000 | Loss: 0.00000865
Iteration 39/1000 | Loss: 0.00000859
Iteration 40/1000 | Loss: 0.00000855
Iteration 41/1000 | Loss: 0.00000853
Iteration 42/1000 | Loss: 0.00000853
Iteration 43/1000 | Loss: 0.00000852
Iteration 44/1000 | Loss: 0.00000852
Iteration 45/1000 | Loss: 0.00000851
Iteration 46/1000 | Loss: 0.00000851
Iteration 47/1000 | Loss: 0.00000846
Iteration 48/1000 | Loss: 0.00000846
Iteration 49/1000 | Loss: 0.00000843
Iteration 50/1000 | Loss: 0.00000840
Iteration 51/1000 | Loss: 0.00000840
Iteration 52/1000 | Loss: 0.00000840
Iteration 53/1000 | Loss: 0.00000839
Iteration 54/1000 | Loss: 0.00000839
Iteration 55/1000 | Loss: 0.00000839
Iteration 56/1000 | Loss: 0.00000838
Iteration 57/1000 | Loss: 0.00000838
Iteration 58/1000 | Loss: 0.00000838
Iteration 59/1000 | Loss: 0.00000837
Iteration 60/1000 | Loss: 0.00000837
Iteration 61/1000 | Loss: 0.00000836
Iteration 62/1000 | Loss: 0.00000836
Iteration 63/1000 | Loss: 0.00000836
Iteration 64/1000 | Loss: 0.00000835
Iteration 65/1000 | Loss: 0.00000835
Iteration 66/1000 | Loss: 0.00000834
Iteration 67/1000 | Loss: 0.00000834
Iteration 68/1000 | Loss: 0.00000833
Iteration 69/1000 | Loss: 0.00000833
Iteration 70/1000 | Loss: 0.00000832
Iteration 71/1000 | Loss: 0.00000832
Iteration 72/1000 | Loss: 0.00000831
Iteration 73/1000 | Loss: 0.00000831
Iteration 74/1000 | Loss: 0.00000830
Iteration 75/1000 | Loss: 0.00000830
Iteration 76/1000 | Loss: 0.00000830
Iteration 77/1000 | Loss: 0.00000829
Iteration 78/1000 | Loss: 0.00000829
Iteration 79/1000 | Loss: 0.00000829
Iteration 80/1000 | Loss: 0.00000829
Iteration 81/1000 | Loss: 0.00000828
Iteration 82/1000 | Loss: 0.00000828
Iteration 83/1000 | Loss: 0.00000828
Iteration 84/1000 | Loss: 0.00000828
Iteration 85/1000 | Loss: 0.00000828
Iteration 86/1000 | Loss: 0.00000828
Iteration 87/1000 | Loss: 0.00000827
Iteration 88/1000 | Loss: 0.00000827
Iteration 89/1000 | Loss: 0.00000827
Iteration 90/1000 | Loss: 0.00000827
Iteration 91/1000 | Loss: 0.00000827
Iteration 92/1000 | Loss: 0.00000827
Iteration 93/1000 | Loss: 0.00000826
Iteration 94/1000 | Loss: 0.00000826
Iteration 95/1000 | Loss: 0.00000826
Iteration 96/1000 | Loss: 0.00000825
Iteration 97/1000 | Loss: 0.00000825
Iteration 98/1000 | Loss: 0.00000825
Iteration 99/1000 | Loss: 0.00000825
Iteration 100/1000 | Loss: 0.00000825
Iteration 101/1000 | Loss: 0.00000824
Iteration 102/1000 | Loss: 0.00000824
Iteration 103/1000 | Loss: 0.00000824
Iteration 104/1000 | Loss: 0.00000824
Iteration 105/1000 | Loss: 0.00000824
Iteration 106/1000 | Loss: 0.00000824
Iteration 107/1000 | Loss: 0.00000824
Iteration 108/1000 | Loss: 0.00000824
Iteration 109/1000 | Loss: 0.00000823
Iteration 110/1000 | Loss: 0.00000823
Iteration 111/1000 | Loss: 0.00000823
Iteration 112/1000 | Loss: 0.00000823
Iteration 113/1000 | Loss: 0.00000823
Iteration 114/1000 | Loss: 0.00000823
Iteration 115/1000 | Loss: 0.00000823
Iteration 116/1000 | Loss: 0.00000823
Iteration 117/1000 | Loss: 0.00000823
Iteration 118/1000 | Loss: 0.00000822
Iteration 119/1000 | Loss: 0.00000822
Iteration 120/1000 | Loss: 0.00000822
Iteration 121/1000 | Loss: 0.00000821
Iteration 122/1000 | Loss: 0.00000821
Iteration 123/1000 | Loss: 0.00000821
Iteration 124/1000 | Loss: 0.00000821
Iteration 125/1000 | Loss: 0.00000821
Iteration 126/1000 | Loss: 0.00000821
Iteration 127/1000 | Loss: 0.00000820
Iteration 128/1000 | Loss: 0.00000820
Iteration 129/1000 | Loss: 0.00000820
Iteration 130/1000 | Loss: 0.00000820
Iteration 131/1000 | Loss: 0.00000820
Iteration 132/1000 | Loss: 0.00000820
Iteration 133/1000 | Loss: 0.00000820
Iteration 134/1000 | Loss: 0.00000820
Iteration 135/1000 | Loss: 0.00000820
Iteration 136/1000 | Loss: 0.00000819
Iteration 137/1000 | Loss: 0.00000819
Iteration 138/1000 | Loss: 0.00000819
Iteration 139/1000 | Loss: 0.00000819
Iteration 140/1000 | Loss: 0.00000819
Iteration 141/1000 | Loss: 0.00000819
Iteration 142/1000 | Loss: 0.00000818
Iteration 143/1000 | Loss: 0.00000818
Iteration 144/1000 | Loss: 0.00000818
Iteration 145/1000 | Loss: 0.00000818
Iteration 146/1000 | Loss: 0.00000817
Iteration 147/1000 | Loss: 0.00000817
Iteration 148/1000 | Loss: 0.00000817
Iteration 149/1000 | Loss: 0.00000817
Iteration 150/1000 | Loss: 0.00000817
Iteration 151/1000 | Loss: 0.00000817
Iteration 152/1000 | Loss: 0.00000817
Iteration 153/1000 | Loss: 0.00000817
Iteration 154/1000 | Loss: 0.00000817
Iteration 155/1000 | Loss: 0.00000817
Iteration 156/1000 | Loss: 0.00000817
Iteration 157/1000 | Loss: 0.00000817
Iteration 158/1000 | Loss: 0.00000817
Iteration 159/1000 | Loss: 0.00000817
Iteration 160/1000 | Loss: 0.00000817
Iteration 161/1000 | Loss: 0.00000817
Iteration 162/1000 | Loss: 0.00000817
Iteration 163/1000 | Loss: 0.00000817
Iteration 164/1000 | Loss: 0.00000817
Iteration 165/1000 | Loss: 0.00000817
Iteration 166/1000 | Loss: 0.00000817
Iteration 167/1000 | Loss: 0.00000817
Iteration 168/1000 | Loss: 0.00000817
Iteration 169/1000 | Loss: 0.00000817
Iteration 170/1000 | Loss: 0.00000817
Iteration 171/1000 | Loss: 0.00000817
Iteration 172/1000 | Loss: 0.00000817
Iteration 173/1000 | Loss: 0.00000817
Iteration 174/1000 | Loss: 0.00000817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [8.16519695945317e-06, 8.16519695945317e-06, 8.16519695945317e-06, 8.16519695945317e-06, 8.16519695945317e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.16519695945317e-06

Optimization complete. Final v2v error: 2.504091262817383 mm

Highest mean error: 2.6377146244049072 mm for frame 42

Lowest mean error: 2.4084312915802 mm for frame 0

Saving results

Total time: 61.98157739639282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00672274
Iteration 2/25 | Loss: 0.00142835
Iteration 3/25 | Loss: 0.00133241
Iteration 4/25 | Loss: 0.00131945
Iteration 5/25 | Loss: 0.00131597
Iteration 6/25 | Loss: 0.00131492
Iteration 7/25 | Loss: 0.00131488
Iteration 8/25 | Loss: 0.00131488
Iteration 9/25 | Loss: 0.00131488
Iteration 10/25 | Loss: 0.00131488
Iteration 11/25 | Loss: 0.00131488
Iteration 12/25 | Loss: 0.00131488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013148795114830136, 0.0013148795114830136, 0.0013148795114830136, 0.0013148795114830136, 0.0013148795114830136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013148795114830136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30815196
Iteration 2/25 | Loss: 0.00208610
Iteration 3/25 | Loss: 0.00208609
Iteration 4/25 | Loss: 0.00208609
Iteration 5/25 | Loss: 0.00208609
Iteration 6/25 | Loss: 0.00208609
Iteration 7/25 | Loss: 0.00208609
Iteration 8/25 | Loss: 0.00208609
Iteration 9/25 | Loss: 0.00208609
Iteration 10/25 | Loss: 0.00208609
Iteration 11/25 | Loss: 0.00208609
Iteration 12/25 | Loss: 0.00208609
Iteration 13/25 | Loss: 0.00208609
Iteration 14/25 | Loss: 0.00208609
Iteration 15/25 | Loss: 0.00208609
Iteration 16/25 | Loss: 0.00208609
Iteration 17/25 | Loss: 0.00208609
Iteration 18/25 | Loss: 0.00208609
Iteration 19/25 | Loss: 0.00208609
Iteration 20/25 | Loss: 0.00208609
Iteration 21/25 | Loss: 0.00208609
Iteration 22/25 | Loss: 0.00208609
Iteration 23/25 | Loss: 0.00208609
Iteration 24/25 | Loss: 0.00208609
Iteration 25/25 | Loss: 0.00208609

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208609
Iteration 2/1000 | Loss: 0.00002252
Iteration 3/1000 | Loss: 0.00001564
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001212
Iteration 8/1000 | Loss: 0.00001176
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001094
Iteration 12/1000 | Loss: 0.00001076
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001057
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001056
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001041
Iteration 23/1000 | Loss: 0.00001041
Iteration 24/1000 | Loss: 0.00001035
Iteration 25/1000 | Loss: 0.00001034
Iteration 26/1000 | Loss: 0.00001034
Iteration 27/1000 | Loss: 0.00001034
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001034
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001031
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001029
Iteration 34/1000 | Loss: 0.00001028
Iteration 35/1000 | Loss: 0.00001028
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001026
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001024
Iteration 42/1000 | Loss: 0.00001024
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001022
Iteration 47/1000 | Loss: 0.00001021
Iteration 48/1000 | Loss: 0.00001019
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001019
Iteration 53/1000 | Loss: 0.00001019
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001018
Iteration 56/1000 | Loss: 0.00001018
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001018
Iteration 62/1000 | Loss: 0.00001018
Iteration 63/1000 | Loss: 0.00001018
Iteration 64/1000 | Loss: 0.00001017
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001017
Iteration 67/1000 | Loss: 0.00001015
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001014
Iteration 70/1000 | Loss: 0.00001014
Iteration 71/1000 | Loss: 0.00001014
Iteration 72/1000 | Loss: 0.00001013
Iteration 73/1000 | Loss: 0.00001013
Iteration 74/1000 | Loss: 0.00001013
Iteration 75/1000 | Loss: 0.00001013
Iteration 76/1000 | Loss: 0.00001013
Iteration 77/1000 | Loss: 0.00001012
Iteration 78/1000 | Loss: 0.00001012
Iteration 79/1000 | Loss: 0.00001012
Iteration 80/1000 | Loss: 0.00001012
Iteration 81/1000 | Loss: 0.00001011
Iteration 82/1000 | Loss: 0.00001011
Iteration 83/1000 | Loss: 0.00001011
Iteration 84/1000 | Loss: 0.00001011
Iteration 85/1000 | Loss: 0.00001010
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001010
Iteration 88/1000 | Loss: 0.00001010
Iteration 89/1000 | Loss: 0.00001010
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001009
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001008
Iteration 94/1000 | Loss: 0.00001008
Iteration 95/1000 | Loss: 0.00001008
Iteration 96/1000 | Loss: 0.00001007
Iteration 97/1000 | Loss: 0.00001007
Iteration 98/1000 | Loss: 0.00001007
Iteration 99/1000 | Loss: 0.00001007
Iteration 100/1000 | Loss: 0.00001007
Iteration 101/1000 | Loss: 0.00001007
Iteration 102/1000 | Loss: 0.00001007
Iteration 103/1000 | Loss: 0.00001007
Iteration 104/1000 | Loss: 0.00001007
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001006
Iteration 107/1000 | Loss: 0.00001006
Iteration 108/1000 | Loss: 0.00001006
Iteration 109/1000 | Loss: 0.00001006
Iteration 110/1000 | Loss: 0.00001006
Iteration 111/1000 | Loss: 0.00001006
Iteration 112/1000 | Loss: 0.00001006
Iteration 113/1000 | Loss: 0.00001006
Iteration 114/1000 | Loss: 0.00001006
Iteration 115/1000 | Loss: 0.00001006
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001005
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001005
Iteration 121/1000 | Loss: 0.00001005
Iteration 122/1000 | Loss: 0.00001005
Iteration 123/1000 | Loss: 0.00001005
Iteration 124/1000 | Loss: 0.00001005
Iteration 125/1000 | Loss: 0.00001005
Iteration 126/1000 | Loss: 0.00001005
Iteration 127/1000 | Loss: 0.00001005
Iteration 128/1000 | Loss: 0.00001005
Iteration 129/1000 | Loss: 0.00001005
Iteration 130/1000 | Loss: 0.00001005
Iteration 131/1000 | Loss: 0.00001005
Iteration 132/1000 | Loss: 0.00001004
Iteration 133/1000 | Loss: 0.00001004
Iteration 134/1000 | Loss: 0.00001004
Iteration 135/1000 | Loss: 0.00001004
Iteration 136/1000 | Loss: 0.00001004
Iteration 137/1000 | Loss: 0.00001004
Iteration 138/1000 | Loss: 0.00001004
Iteration 139/1000 | Loss: 0.00001004
Iteration 140/1000 | Loss: 0.00001004
Iteration 141/1000 | Loss: 0.00001004
Iteration 142/1000 | Loss: 0.00001004
Iteration 143/1000 | Loss: 0.00001003
Iteration 144/1000 | Loss: 0.00001003
Iteration 145/1000 | Loss: 0.00001003
Iteration 146/1000 | Loss: 0.00001003
Iteration 147/1000 | Loss: 0.00001003
Iteration 148/1000 | Loss: 0.00001003
Iteration 149/1000 | Loss: 0.00001003
Iteration 150/1000 | Loss: 0.00001003
Iteration 151/1000 | Loss: 0.00001003
Iteration 152/1000 | Loss: 0.00001003
Iteration 153/1000 | Loss: 0.00001003
Iteration 154/1000 | Loss: 0.00001003
Iteration 155/1000 | Loss: 0.00001002
Iteration 156/1000 | Loss: 0.00001002
Iteration 157/1000 | Loss: 0.00001002
Iteration 158/1000 | Loss: 0.00001002
Iteration 159/1000 | Loss: 0.00001002
Iteration 160/1000 | Loss: 0.00001002
Iteration 161/1000 | Loss: 0.00001002
Iteration 162/1000 | Loss: 0.00001002
Iteration 163/1000 | Loss: 0.00001002
Iteration 164/1000 | Loss: 0.00001002
Iteration 165/1000 | Loss: 0.00001001
Iteration 166/1000 | Loss: 0.00001001
Iteration 167/1000 | Loss: 0.00001001
Iteration 168/1000 | Loss: 0.00001001
Iteration 169/1000 | Loss: 0.00001001
Iteration 170/1000 | Loss: 0.00001001
Iteration 171/1000 | Loss: 0.00001001
Iteration 172/1000 | Loss: 0.00001001
Iteration 173/1000 | Loss: 0.00001001
Iteration 174/1000 | Loss: 0.00001001
Iteration 175/1000 | Loss: 0.00001001
Iteration 176/1000 | Loss: 0.00001001
Iteration 177/1000 | Loss: 0.00001001
Iteration 178/1000 | Loss: 0.00001001
Iteration 179/1000 | Loss: 0.00001001
Iteration 180/1000 | Loss: 0.00001001
Iteration 181/1000 | Loss: 0.00001000
Iteration 182/1000 | Loss: 0.00001000
Iteration 183/1000 | Loss: 0.00001000
Iteration 184/1000 | Loss: 0.00001000
Iteration 185/1000 | Loss: 0.00001000
Iteration 186/1000 | Loss: 0.00001000
Iteration 187/1000 | Loss: 0.00001000
Iteration 188/1000 | Loss: 0.00001000
Iteration 189/1000 | Loss: 0.00001000
Iteration 190/1000 | Loss: 0.00001000
Iteration 191/1000 | Loss: 0.00001000
Iteration 192/1000 | Loss: 0.00001000
Iteration 193/1000 | Loss: 0.00001000
Iteration 194/1000 | Loss: 0.00001000
Iteration 195/1000 | Loss: 0.00001000
Iteration 196/1000 | Loss: 0.00001000
Iteration 197/1000 | Loss: 0.00000999
Iteration 198/1000 | Loss: 0.00000999
Iteration 199/1000 | Loss: 0.00000999
Iteration 200/1000 | Loss: 0.00000999
Iteration 201/1000 | Loss: 0.00000999
Iteration 202/1000 | Loss: 0.00000999
Iteration 203/1000 | Loss: 0.00000999
Iteration 204/1000 | Loss: 0.00000999
Iteration 205/1000 | Loss: 0.00000998
Iteration 206/1000 | Loss: 0.00000998
Iteration 207/1000 | Loss: 0.00000998
Iteration 208/1000 | Loss: 0.00000998
Iteration 209/1000 | Loss: 0.00000998
Iteration 210/1000 | Loss: 0.00000998
Iteration 211/1000 | Loss: 0.00000998
Iteration 212/1000 | Loss: 0.00000998
Iteration 213/1000 | Loss: 0.00000998
Iteration 214/1000 | Loss: 0.00000998
Iteration 215/1000 | Loss: 0.00000998
Iteration 216/1000 | Loss: 0.00000998
Iteration 217/1000 | Loss: 0.00000998
Iteration 218/1000 | Loss: 0.00000998
Iteration 219/1000 | Loss: 0.00000997
Iteration 220/1000 | Loss: 0.00000997
Iteration 221/1000 | Loss: 0.00000997
Iteration 222/1000 | Loss: 0.00000997
Iteration 223/1000 | Loss: 0.00000997
Iteration 224/1000 | Loss: 0.00000997
Iteration 225/1000 | Loss: 0.00000997
Iteration 226/1000 | Loss: 0.00000997
Iteration 227/1000 | Loss: 0.00000997
Iteration 228/1000 | Loss: 0.00000997
Iteration 229/1000 | Loss: 0.00000997
Iteration 230/1000 | Loss: 0.00000996
Iteration 231/1000 | Loss: 0.00000996
Iteration 232/1000 | Loss: 0.00000996
Iteration 233/1000 | Loss: 0.00000996
Iteration 234/1000 | Loss: 0.00000996
Iteration 235/1000 | Loss: 0.00000996
Iteration 236/1000 | Loss: 0.00000996
Iteration 237/1000 | Loss: 0.00000996
Iteration 238/1000 | Loss: 0.00000995
Iteration 239/1000 | Loss: 0.00000995
Iteration 240/1000 | Loss: 0.00000995
Iteration 241/1000 | Loss: 0.00000995
Iteration 242/1000 | Loss: 0.00000995
Iteration 243/1000 | Loss: 0.00000995
Iteration 244/1000 | Loss: 0.00000995
Iteration 245/1000 | Loss: 0.00000995
Iteration 246/1000 | Loss: 0.00000995
Iteration 247/1000 | Loss: 0.00000995
Iteration 248/1000 | Loss: 0.00000995
Iteration 249/1000 | Loss: 0.00000995
Iteration 250/1000 | Loss: 0.00000995
Iteration 251/1000 | Loss: 0.00000994
Iteration 252/1000 | Loss: 0.00000994
Iteration 253/1000 | Loss: 0.00000994
Iteration 254/1000 | Loss: 0.00000994
Iteration 255/1000 | Loss: 0.00000994
Iteration 256/1000 | Loss: 0.00000994
Iteration 257/1000 | Loss: 0.00000994
Iteration 258/1000 | Loss: 0.00000994
Iteration 259/1000 | Loss: 0.00000994
Iteration 260/1000 | Loss: 0.00000994
Iteration 261/1000 | Loss: 0.00000994
Iteration 262/1000 | Loss: 0.00000994
Iteration 263/1000 | Loss: 0.00000994
Iteration 264/1000 | Loss: 0.00000994
Iteration 265/1000 | Loss: 0.00000994
Iteration 266/1000 | Loss: 0.00000994
Iteration 267/1000 | Loss: 0.00000994
Iteration 268/1000 | Loss: 0.00000994
Iteration 269/1000 | Loss: 0.00000994
Iteration 270/1000 | Loss: 0.00000994
Iteration 271/1000 | Loss: 0.00000994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [9.943966688297223e-06, 9.943966688297223e-06, 9.943966688297223e-06, 9.943966688297223e-06, 9.943966688297223e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.943966688297223e-06

Optimization complete. Final v2v error: 2.717979669570923 mm

Highest mean error: 3.393711805343628 mm for frame 73

Lowest mean error: 2.5045762062072754 mm for frame 1

Saving results

Total time: 45.010019302368164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777844
Iteration 2/25 | Loss: 0.00151962
Iteration 3/25 | Loss: 0.00141140
Iteration 4/25 | Loss: 0.00139289
Iteration 5/25 | Loss: 0.00138723
Iteration 6/25 | Loss: 0.00138556
Iteration 7/25 | Loss: 0.00138556
Iteration 8/25 | Loss: 0.00138556
Iteration 9/25 | Loss: 0.00138556
Iteration 10/25 | Loss: 0.00138556
Iteration 11/25 | Loss: 0.00138556
Iteration 12/25 | Loss: 0.00138556
Iteration 13/25 | Loss: 0.00138556
Iteration 14/25 | Loss: 0.00138556
Iteration 15/25 | Loss: 0.00138556
Iteration 16/25 | Loss: 0.00138556
Iteration 17/25 | Loss: 0.00138556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013855602592229843, 0.0013855602592229843, 0.0013855602592229843, 0.0013855602592229843, 0.0013855602592229843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013855602592229843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21898139
Iteration 2/25 | Loss: 0.00274689
Iteration 3/25 | Loss: 0.00274689
Iteration 4/25 | Loss: 0.00274689
Iteration 5/25 | Loss: 0.00274689
Iteration 6/25 | Loss: 0.00274689
Iteration 7/25 | Loss: 0.00274689
Iteration 8/25 | Loss: 0.00274689
Iteration 9/25 | Loss: 0.00274689
Iteration 10/25 | Loss: 0.00274688
Iteration 11/25 | Loss: 0.00274688
Iteration 12/25 | Loss: 0.00274688
Iteration 13/25 | Loss: 0.00274688
Iteration 14/25 | Loss: 0.00274688
Iteration 15/25 | Loss: 0.00274688
Iteration 16/25 | Loss: 0.00274688
Iteration 17/25 | Loss: 0.00274688
Iteration 18/25 | Loss: 0.00274688
Iteration 19/25 | Loss: 0.00274688
Iteration 20/25 | Loss: 0.00274688
Iteration 21/25 | Loss: 0.00274688
Iteration 22/25 | Loss: 0.00274688
Iteration 23/25 | Loss: 0.00274688
Iteration 24/25 | Loss: 0.00274688
Iteration 25/25 | Loss: 0.00274688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00274688
Iteration 2/1000 | Loss: 0.00005266
Iteration 3/1000 | Loss: 0.00003691
Iteration 4/1000 | Loss: 0.00002901
Iteration 5/1000 | Loss: 0.00002632
Iteration 6/1000 | Loss: 0.00002479
Iteration 7/1000 | Loss: 0.00002394
Iteration 8/1000 | Loss: 0.00002308
Iteration 9/1000 | Loss: 0.00002251
Iteration 10/1000 | Loss: 0.00002219
Iteration 11/1000 | Loss: 0.00002188
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00002168
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002134
Iteration 16/1000 | Loss: 0.00002119
Iteration 17/1000 | Loss: 0.00002113
Iteration 18/1000 | Loss: 0.00002109
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002102
Iteration 21/1000 | Loss: 0.00002101
Iteration 22/1000 | Loss: 0.00002094
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002090
Iteration 25/1000 | Loss: 0.00002090
Iteration 26/1000 | Loss: 0.00002090
Iteration 27/1000 | Loss: 0.00002089
Iteration 28/1000 | Loss: 0.00002089
Iteration 29/1000 | Loss: 0.00002088
Iteration 30/1000 | Loss: 0.00002088
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002087
Iteration 33/1000 | Loss: 0.00002087
Iteration 34/1000 | Loss: 0.00002084
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002084
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002081
Iteration 43/1000 | Loss: 0.00002081
Iteration 44/1000 | Loss: 0.00002081
Iteration 45/1000 | Loss: 0.00002081
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002080
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002078
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002077
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002076
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002075
Iteration 62/1000 | Loss: 0.00002074
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002073
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002070
Iteration 73/1000 | Loss: 0.00002070
Iteration 74/1000 | Loss: 0.00002070
Iteration 75/1000 | Loss: 0.00002070
Iteration 76/1000 | Loss: 0.00002070
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002069
Iteration 83/1000 | Loss: 0.00002069
Iteration 84/1000 | Loss: 0.00002069
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002068
Iteration 91/1000 | Loss: 0.00002067
Iteration 92/1000 | Loss: 0.00002067
Iteration 93/1000 | Loss: 0.00002067
Iteration 94/1000 | Loss: 0.00002067
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002066
Iteration 98/1000 | Loss: 0.00002066
Iteration 99/1000 | Loss: 0.00002066
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002066
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002066
Iteration 108/1000 | Loss: 0.00002065
Iteration 109/1000 | Loss: 0.00002065
Iteration 110/1000 | Loss: 0.00002065
Iteration 111/1000 | Loss: 0.00002065
Iteration 112/1000 | Loss: 0.00002064
Iteration 113/1000 | Loss: 0.00002064
Iteration 114/1000 | Loss: 0.00002064
Iteration 115/1000 | Loss: 0.00002064
Iteration 116/1000 | Loss: 0.00002064
Iteration 117/1000 | Loss: 0.00002063
Iteration 118/1000 | Loss: 0.00002063
Iteration 119/1000 | Loss: 0.00002063
Iteration 120/1000 | Loss: 0.00002063
Iteration 121/1000 | Loss: 0.00002063
Iteration 122/1000 | Loss: 0.00002063
Iteration 123/1000 | Loss: 0.00002063
Iteration 124/1000 | Loss: 0.00002063
Iteration 125/1000 | Loss: 0.00002063
Iteration 126/1000 | Loss: 0.00002063
Iteration 127/1000 | Loss: 0.00002063
Iteration 128/1000 | Loss: 0.00002062
Iteration 129/1000 | Loss: 0.00002062
Iteration 130/1000 | Loss: 0.00002062
Iteration 131/1000 | Loss: 0.00002062
Iteration 132/1000 | Loss: 0.00002062
Iteration 133/1000 | Loss: 0.00002062
Iteration 134/1000 | Loss: 0.00002062
Iteration 135/1000 | Loss: 0.00002062
Iteration 136/1000 | Loss: 0.00002062
Iteration 137/1000 | Loss: 0.00002062
Iteration 138/1000 | Loss: 0.00002062
Iteration 139/1000 | Loss: 0.00002062
Iteration 140/1000 | Loss: 0.00002062
Iteration 141/1000 | Loss: 0.00002062
Iteration 142/1000 | Loss: 0.00002062
Iteration 143/1000 | Loss: 0.00002062
Iteration 144/1000 | Loss: 0.00002061
Iteration 145/1000 | Loss: 0.00002061
Iteration 146/1000 | Loss: 0.00002061
Iteration 147/1000 | Loss: 0.00002061
Iteration 148/1000 | Loss: 0.00002061
Iteration 149/1000 | Loss: 0.00002061
Iteration 150/1000 | Loss: 0.00002061
Iteration 151/1000 | Loss: 0.00002061
Iteration 152/1000 | Loss: 0.00002061
Iteration 153/1000 | Loss: 0.00002061
Iteration 154/1000 | Loss: 0.00002061
Iteration 155/1000 | Loss: 0.00002061
Iteration 156/1000 | Loss: 0.00002061
Iteration 157/1000 | Loss: 0.00002060
Iteration 158/1000 | Loss: 0.00002060
Iteration 159/1000 | Loss: 0.00002060
Iteration 160/1000 | Loss: 0.00002060
Iteration 161/1000 | Loss: 0.00002060
Iteration 162/1000 | Loss: 0.00002060
Iteration 163/1000 | Loss: 0.00002060
Iteration 164/1000 | Loss: 0.00002060
Iteration 165/1000 | Loss: 0.00002060
Iteration 166/1000 | Loss: 0.00002060
Iteration 167/1000 | Loss: 0.00002060
Iteration 168/1000 | Loss: 0.00002060
Iteration 169/1000 | Loss: 0.00002060
Iteration 170/1000 | Loss: 0.00002060
Iteration 171/1000 | Loss: 0.00002060
Iteration 172/1000 | Loss: 0.00002060
Iteration 173/1000 | Loss: 0.00002059
Iteration 174/1000 | Loss: 0.00002059
Iteration 175/1000 | Loss: 0.00002059
Iteration 176/1000 | Loss: 0.00002059
Iteration 177/1000 | Loss: 0.00002059
Iteration 178/1000 | Loss: 0.00002059
Iteration 179/1000 | Loss: 0.00002059
Iteration 180/1000 | Loss: 0.00002059
Iteration 181/1000 | Loss: 0.00002059
Iteration 182/1000 | Loss: 0.00002058
Iteration 183/1000 | Loss: 0.00002058
Iteration 184/1000 | Loss: 0.00002058
Iteration 185/1000 | Loss: 0.00002058
Iteration 186/1000 | Loss: 0.00002058
Iteration 187/1000 | Loss: 0.00002058
Iteration 188/1000 | Loss: 0.00002058
Iteration 189/1000 | Loss: 0.00002058
Iteration 190/1000 | Loss: 0.00002057
Iteration 191/1000 | Loss: 0.00002057
Iteration 192/1000 | Loss: 0.00002057
Iteration 193/1000 | Loss: 0.00002057
Iteration 194/1000 | Loss: 0.00002057
Iteration 195/1000 | Loss: 0.00002057
Iteration 196/1000 | Loss: 0.00002057
Iteration 197/1000 | Loss: 0.00002057
Iteration 198/1000 | Loss: 0.00002057
Iteration 199/1000 | Loss: 0.00002057
Iteration 200/1000 | Loss: 0.00002057
Iteration 201/1000 | Loss: 0.00002057
Iteration 202/1000 | Loss: 0.00002057
Iteration 203/1000 | Loss: 0.00002057
Iteration 204/1000 | Loss: 0.00002057
Iteration 205/1000 | Loss: 0.00002057
Iteration 206/1000 | Loss: 0.00002057
Iteration 207/1000 | Loss: 0.00002057
Iteration 208/1000 | Loss: 0.00002057
Iteration 209/1000 | Loss: 0.00002057
Iteration 210/1000 | Loss: 0.00002057
Iteration 211/1000 | Loss: 0.00002057
Iteration 212/1000 | Loss: 0.00002057
Iteration 213/1000 | Loss: 0.00002057
Iteration 214/1000 | Loss: 0.00002057
Iteration 215/1000 | Loss: 0.00002057
Iteration 216/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.056563607766293e-05, 2.056563607766293e-05, 2.056563607766293e-05, 2.056563607766293e-05, 2.056563607766293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.056563607766293e-05

Optimization complete. Final v2v error: 3.754103183746338 mm

Highest mean error: 4.697268009185791 mm for frame 96

Lowest mean error: 2.9473519325256348 mm for frame 2

Saving results

Total time: 46.83313035964966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511632
Iteration 2/25 | Loss: 0.00145232
Iteration 3/25 | Loss: 0.00136106
Iteration 4/25 | Loss: 0.00134527
Iteration 5/25 | Loss: 0.00133875
Iteration 6/25 | Loss: 0.00133812
Iteration 7/25 | Loss: 0.00133812
Iteration 8/25 | Loss: 0.00133812
Iteration 9/25 | Loss: 0.00133812
Iteration 10/25 | Loss: 0.00133812
Iteration 11/25 | Loss: 0.00133812
Iteration 12/25 | Loss: 0.00133812
Iteration 13/25 | Loss: 0.00133812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00133811729028821, 0.00133811729028821, 0.00133811729028821, 0.00133811729028821, 0.00133811729028821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133811729028821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72192222
Iteration 2/25 | Loss: 0.00180024
Iteration 3/25 | Loss: 0.00180024
Iteration 4/25 | Loss: 0.00180023
Iteration 5/25 | Loss: 0.00180023
Iteration 6/25 | Loss: 0.00180023
Iteration 7/25 | Loss: 0.00180023
Iteration 8/25 | Loss: 0.00180023
Iteration 9/25 | Loss: 0.00180023
Iteration 10/25 | Loss: 0.00180023
Iteration 11/25 | Loss: 0.00180023
Iteration 12/25 | Loss: 0.00180023
Iteration 13/25 | Loss: 0.00180023
Iteration 14/25 | Loss: 0.00180023
Iteration 15/25 | Loss: 0.00180023
Iteration 16/25 | Loss: 0.00180023
Iteration 17/25 | Loss: 0.00180023
Iteration 18/25 | Loss: 0.00180023
Iteration 19/25 | Loss: 0.00180023
Iteration 20/25 | Loss: 0.00180023
Iteration 21/25 | Loss: 0.00180023
Iteration 22/25 | Loss: 0.00180023
Iteration 23/25 | Loss: 0.00180023
Iteration 24/25 | Loss: 0.00180023
Iteration 25/25 | Loss: 0.00180023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180023
Iteration 2/1000 | Loss: 0.00003723
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00002369
Iteration 5/1000 | Loss: 0.00002232
Iteration 6/1000 | Loss: 0.00002155
Iteration 7/1000 | Loss: 0.00002097
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001942
Iteration 12/1000 | Loss: 0.00001914
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001875
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001825
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001779
Iteration 23/1000 | Loss: 0.00001775
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001772
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001769
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001756
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001755
Iteration 44/1000 | Loss: 0.00001755
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001753
Iteration 50/1000 | Loss: 0.00001753
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001753
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001753
Iteration 58/1000 | Loss: 0.00001752
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001751
Iteration 61/1000 | Loss: 0.00001751
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001748
Iteration 91/1000 | Loss: 0.00001748
Iteration 92/1000 | Loss: 0.00001748
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001747
Iteration 96/1000 | Loss: 0.00001747
Iteration 97/1000 | Loss: 0.00001747
Iteration 98/1000 | Loss: 0.00001747
Iteration 99/1000 | Loss: 0.00001747
Iteration 100/1000 | Loss: 0.00001747
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001746
Iteration 104/1000 | Loss: 0.00001746
Iteration 105/1000 | Loss: 0.00001746
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001745
Iteration 108/1000 | Loss: 0.00001745
Iteration 109/1000 | Loss: 0.00001745
Iteration 110/1000 | Loss: 0.00001745
Iteration 111/1000 | Loss: 0.00001745
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.745133886288386e-05, 1.745133886288386e-05, 1.745133886288386e-05, 1.745133886288386e-05, 1.745133886288386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.745133886288386e-05

Optimization complete. Final v2v error: 3.5958127975463867 mm

Highest mean error: 4.275460720062256 mm for frame 265

Lowest mean error: 3.5176374912261963 mm for frame 26

Saving results

Total time: 52.92431879043579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897058
Iteration 2/25 | Loss: 0.00148223
Iteration 3/25 | Loss: 0.00138038
Iteration 4/25 | Loss: 0.00136080
Iteration 5/25 | Loss: 0.00135505
Iteration 6/25 | Loss: 0.00135372
Iteration 7/25 | Loss: 0.00135372
Iteration 8/25 | Loss: 0.00135372
Iteration 9/25 | Loss: 0.00135372
Iteration 10/25 | Loss: 0.00135372
Iteration 11/25 | Loss: 0.00135372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013537203194573522, 0.0013537203194573522, 0.0013537203194573522, 0.0013537203194573522, 0.0013537203194573522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013537203194573522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25016534
Iteration 2/25 | Loss: 0.00209967
Iteration 3/25 | Loss: 0.00209967
Iteration 4/25 | Loss: 0.00209967
Iteration 5/25 | Loss: 0.00209967
Iteration 6/25 | Loss: 0.00209967
Iteration 7/25 | Loss: 0.00209967
Iteration 8/25 | Loss: 0.00209967
Iteration 9/25 | Loss: 0.00209967
Iteration 10/25 | Loss: 0.00209967
Iteration 11/25 | Loss: 0.00209967
Iteration 12/25 | Loss: 0.00209967
Iteration 13/25 | Loss: 0.00209967
Iteration 14/25 | Loss: 0.00209967
Iteration 15/25 | Loss: 0.00209967
Iteration 16/25 | Loss: 0.00209967
Iteration 17/25 | Loss: 0.00209967
Iteration 18/25 | Loss: 0.00209967
Iteration 19/25 | Loss: 0.00209967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020996651146560907, 0.0020996651146560907, 0.0020996651146560907, 0.0020996651146560907, 0.0020996651146560907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020996651146560907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209967
Iteration 2/1000 | Loss: 0.00004308
Iteration 3/1000 | Loss: 0.00003062
Iteration 4/1000 | Loss: 0.00002514
Iteration 5/1000 | Loss: 0.00002346
Iteration 6/1000 | Loss: 0.00002217
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002087
Iteration 9/1000 | Loss: 0.00002026
Iteration 10/1000 | Loss: 0.00001994
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001939
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00001920
Iteration 15/1000 | Loss: 0.00001911
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001887
Iteration 18/1000 | Loss: 0.00001880
Iteration 19/1000 | Loss: 0.00001878
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001868
Iteration 23/1000 | Loss: 0.00001863
Iteration 24/1000 | Loss: 0.00001862
Iteration 25/1000 | Loss: 0.00001862
Iteration 26/1000 | Loss: 0.00001862
Iteration 27/1000 | Loss: 0.00001861
Iteration 28/1000 | Loss: 0.00001861
Iteration 29/1000 | Loss: 0.00001860
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001858
Iteration 32/1000 | Loss: 0.00001853
Iteration 33/1000 | Loss: 0.00001850
Iteration 34/1000 | Loss: 0.00001850
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001844
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001840
Iteration 49/1000 | Loss: 0.00001839
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00001838
Iteration 52/1000 | Loss: 0.00001838
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001835
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001835
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001833
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001827
Iteration 84/1000 | Loss: 0.00001827
Iteration 85/1000 | Loss: 0.00001827
Iteration 86/1000 | Loss: 0.00001827
Iteration 87/1000 | Loss: 0.00001826
Iteration 88/1000 | Loss: 0.00001826
Iteration 89/1000 | Loss: 0.00001825
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001824
Iteration 93/1000 | Loss: 0.00001824
Iteration 94/1000 | Loss: 0.00001824
Iteration 95/1000 | Loss: 0.00001824
Iteration 96/1000 | Loss: 0.00001823
Iteration 97/1000 | Loss: 0.00001823
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001821
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001821
Iteration 106/1000 | Loss: 0.00001821
Iteration 107/1000 | Loss: 0.00001821
Iteration 108/1000 | Loss: 0.00001821
Iteration 109/1000 | Loss: 0.00001821
Iteration 110/1000 | Loss: 0.00001821
Iteration 111/1000 | Loss: 0.00001821
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001819
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001818
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001818
Iteration 126/1000 | Loss: 0.00001818
Iteration 127/1000 | Loss: 0.00001818
Iteration 128/1000 | Loss: 0.00001818
Iteration 129/1000 | Loss: 0.00001818
Iteration 130/1000 | Loss: 0.00001818
Iteration 131/1000 | Loss: 0.00001818
Iteration 132/1000 | Loss: 0.00001818
Iteration 133/1000 | Loss: 0.00001817
Iteration 134/1000 | Loss: 0.00001817
Iteration 135/1000 | Loss: 0.00001817
Iteration 136/1000 | Loss: 0.00001817
Iteration 137/1000 | Loss: 0.00001817
Iteration 138/1000 | Loss: 0.00001816
Iteration 139/1000 | Loss: 0.00001816
Iteration 140/1000 | Loss: 0.00001816
Iteration 141/1000 | Loss: 0.00001816
Iteration 142/1000 | Loss: 0.00001816
Iteration 143/1000 | Loss: 0.00001816
Iteration 144/1000 | Loss: 0.00001816
Iteration 145/1000 | Loss: 0.00001816
Iteration 146/1000 | Loss: 0.00001816
Iteration 147/1000 | Loss: 0.00001815
Iteration 148/1000 | Loss: 0.00001815
Iteration 149/1000 | Loss: 0.00001815
Iteration 150/1000 | Loss: 0.00001815
Iteration 151/1000 | Loss: 0.00001815
Iteration 152/1000 | Loss: 0.00001815
Iteration 153/1000 | Loss: 0.00001815
Iteration 154/1000 | Loss: 0.00001815
Iteration 155/1000 | Loss: 0.00001815
Iteration 156/1000 | Loss: 0.00001815
Iteration 157/1000 | Loss: 0.00001815
Iteration 158/1000 | Loss: 0.00001814
Iteration 159/1000 | Loss: 0.00001814
Iteration 160/1000 | Loss: 0.00001814
Iteration 161/1000 | Loss: 0.00001814
Iteration 162/1000 | Loss: 0.00001814
Iteration 163/1000 | Loss: 0.00001814
Iteration 164/1000 | Loss: 0.00001814
Iteration 165/1000 | Loss: 0.00001814
Iteration 166/1000 | Loss: 0.00001814
Iteration 167/1000 | Loss: 0.00001813
Iteration 168/1000 | Loss: 0.00001813
Iteration 169/1000 | Loss: 0.00001813
Iteration 170/1000 | Loss: 0.00001813
Iteration 171/1000 | Loss: 0.00001813
Iteration 172/1000 | Loss: 0.00001813
Iteration 173/1000 | Loss: 0.00001813
Iteration 174/1000 | Loss: 0.00001813
Iteration 175/1000 | Loss: 0.00001813
Iteration 176/1000 | Loss: 0.00001813
Iteration 177/1000 | Loss: 0.00001813
Iteration 178/1000 | Loss: 0.00001813
Iteration 179/1000 | Loss: 0.00001813
Iteration 180/1000 | Loss: 0.00001813
Iteration 181/1000 | Loss: 0.00001813
Iteration 182/1000 | Loss: 0.00001813
Iteration 183/1000 | Loss: 0.00001813
Iteration 184/1000 | Loss: 0.00001813
Iteration 185/1000 | Loss: 0.00001813
Iteration 186/1000 | Loss: 0.00001813
Iteration 187/1000 | Loss: 0.00001813
Iteration 188/1000 | Loss: 0.00001813
Iteration 189/1000 | Loss: 0.00001813
Iteration 190/1000 | Loss: 0.00001813
Iteration 191/1000 | Loss: 0.00001813
Iteration 192/1000 | Loss: 0.00001813
Iteration 193/1000 | Loss: 0.00001813
Iteration 194/1000 | Loss: 0.00001813
Iteration 195/1000 | Loss: 0.00001813
Iteration 196/1000 | Loss: 0.00001813
Iteration 197/1000 | Loss: 0.00001813
Iteration 198/1000 | Loss: 0.00001813
Iteration 199/1000 | Loss: 0.00001813
Iteration 200/1000 | Loss: 0.00001813
Iteration 201/1000 | Loss: 0.00001813
Iteration 202/1000 | Loss: 0.00001813
Iteration 203/1000 | Loss: 0.00001813
Iteration 204/1000 | Loss: 0.00001813
Iteration 205/1000 | Loss: 0.00001813
Iteration 206/1000 | Loss: 0.00001813
Iteration 207/1000 | Loss: 0.00001813
Iteration 208/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.813058042898774e-05, 1.813058042898774e-05, 1.813058042898774e-05, 1.813058042898774e-05, 1.813058042898774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.813058042898774e-05

Optimization complete. Final v2v error: 3.5803606510162354 mm

Highest mean error: 5.562533855438232 mm for frame 70

Lowest mean error: 3.1198341846466064 mm for frame 95

Saving results

Total time: 45.19151496887207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410252
Iteration 2/25 | Loss: 0.00140981
Iteration 3/25 | Loss: 0.00133209
Iteration 4/25 | Loss: 0.00132752
Iteration 5/25 | Loss: 0.00132752
Iteration 6/25 | Loss: 0.00132752
Iteration 7/25 | Loss: 0.00132752
Iteration 8/25 | Loss: 0.00132752
Iteration 9/25 | Loss: 0.00132752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0013275162782520056, 0.0013275162782520056, 0.0013275162782520056, 0.0013275162782520056, 0.0013275162782520056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013275162782520056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50966537
Iteration 2/25 | Loss: 0.00197858
Iteration 3/25 | Loss: 0.00197858
Iteration 4/25 | Loss: 0.00197858
Iteration 5/25 | Loss: 0.00197858
Iteration 6/25 | Loss: 0.00197858
Iteration 7/25 | Loss: 0.00197858
Iteration 8/25 | Loss: 0.00197858
Iteration 9/25 | Loss: 0.00197858
Iteration 10/25 | Loss: 0.00197858
Iteration 11/25 | Loss: 0.00197858
Iteration 12/25 | Loss: 0.00197858
Iteration 13/25 | Loss: 0.00197858
Iteration 14/25 | Loss: 0.00197858
Iteration 15/25 | Loss: 0.00197858
Iteration 16/25 | Loss: 0.00197858
Iteration 17/25 | Loss: 0.00197858
Iteration 18/25 | Loss: 0.00197858
Iteration 19/25 | Loss: 0.00197858
Iteration 20/25 | Loss: 0.00197858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019785792101174593, 0.0019785792101174593, 0.0019785792101174593, 0.0019785792101174593, 0.0019785792101174593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019785792101174593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197858
Iteration 2/1000 | Loss: 0.00002907
Iteration 3/1000 | Loss: 0.00001912
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001319
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001182
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00001122
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001101
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001100
Iteration 18/1000 | Loss: 0.00001088
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001065
Iteration 21/1000 | Loss: 0.00001064
Iteration 22/1000 | Loss: 0.00001058
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001056
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001051
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001047
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001047
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001036
Iteration 38/1000 | Loss: 0.00001034
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001033
Iteration 41/1000 | Loss: 0.00001033
Iteration 42/1000 | Loss: 0.00001031
Iteration 43/1000 | Loss: 0.00001030
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001027
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001026
Iteration 51/1000 | Loss: 0.00001026
Iteration 52/1000 | Loss: 0.00001025
Iteration 53/1000 | Loss: 0.00001025
Iteration 54/1000 | Loss: 0.00001024
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001022
Iteration 57/1000 | Loss: 0.00001022
Iteration 58/1000 | Loss: 0.00001021
Iteration 59/1000 | Loss: 0.00001021
Iteration 60/1000 | Loss: 0.00001020
Iteration 61/1000 | Loss: 0.00001019
Iteration 62/1000 | Loss: 0.00001018
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001015
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001012
Iteration 69/1000 | Loss: 0.00001009
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001009
Iteration 75/1000 | Loss: 0.00001009
Iteration 76/1000 | Loss: 0.00001009
Iteration 77/1000 | Loss: 0.00001009
Iteration 78/1000 | Loss: 0.00001009
Iteration 79/1000 | Loss: 0.00001009
Iteration 80/1000 | Loss: 0.00001008
Iteration 81/1000 | Loss: 0.00001007
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001005
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001005
Iteration 87/1000 | Loss: 0.00001005
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001004
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001001
Iteration 92/1000 | Loss: 0.00001001
Iteration 93/1000 | Loss: 0.00001001
Iteration 94/1000 | Loss: 0.00001001
Iteration 95/1000 | Loss: 0.00001001
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001000
Iteration 99/1000 | Loss: 0.00001000
Iteration 100/1000 | Loss: 0.00001000
Iteration 101/1000 | Loss: 0.00001000
Iteration 102/1000 | Loss: 0.00001000
Iteration 103/1000 | Loss: 0.00001000
Iteration 104/1000 | Loss: 0.00001000
Iteration 105/1000 | Loss: 0.00001000
Iteration 106/1000 | Loss: 0.00001000
Iteration 107/1000 | Loss: 0.00001000
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00001000
Iteration 112/1000 | Loss: 0.00001000
Iteration 113/1000 | Loss: 0.00001000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.0002207091019955e-05, 1.0002207091019955e-05, 1.0002207091019955e-05, 1.0002207091019955e-05, 1.0002207091019955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0002207091019955e-05

Optimization complete. Final v2v error: 2.734210968017578 mm

Highest mean error: 2.898463726043701 mm for frame 89

Lowest mean error: 2.6065194606781006 mm for frame 82

Saving results

Total time: 42.916268825531006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_001/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_001/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803376
Iteration 2/25 | Loss: 0.00141004
Iteration 3/25 | Loss: 0.00133168
Iteration 4/25 | Loss: 0.00132253
Iteration 5/25 | Loss: 0.00132050
Iteration 6/25 | Loss: 0.00132050
Iteration 7/25 | Loss: 0.00132050
Iteration 8/25 | Loss: 0.00132050
Iteration 9/25 | Loss: 0.00132050
Iteration 10/25 | Loss: 0.00132050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013205043505877256, 0.0013205043505877256, 0.0013205043505877256, 0.0013205043505877256, 0.0013205043505877256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013205043505877256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23695266
Iteration 2/25 | Loss: 0.00233832
Iteration 3/25 | Loss: 0.00233832
Iteration 4/25 | Loss: 0.00233832
Iteration 5/25 | Loss: 0.00233832
Iteration 6/25 | Loss: 0.00233831
Iteration 7/25 | Loss: 0.00233831
Iteration 8/25 | Loss: 0.00233831
Iteration 9/25 | Loss: 0.00233831
Iteration 10/25 | Loss: 0.00233831
Iteration 11/25 | Loss: 0.00233831
Iteration 12/25 | Loss: 0.00233831
Iteration 13/25 | Loss: 0.00233831
Iteration 14/25 | Loss: 0.00233831
Iteration 15/25 | Loss: 0.00233831
Iteration 16/25 | Loss: 0.00233831
Iteration 17/25 | Loss: 0.00233831
Iteration 18/25 | Loss: 0.00233831
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023383123334497213, 0.0023383123334497213, 0.0023383123334497213, 0.0023383123334497213, 0.0023383123334497213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023383123334497213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233831
Iteration 2/1000 | Loss: 0.00003036
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001722
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001151
Iteration 16/1000 | Loss: 0.00001145
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001136
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001107
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001100
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001096
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001095
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001094
Iteration 42/1000 | Loss: 0.00001093
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001081
Iteration 60/1000 | Loss: 0.00001081
Iteration 61/1000 | Loss: 0.00001081
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001079
Iteration 65/1000 | Loss: 0.00001079
Iteration 66/1000 | Loss: 0.00001078
Iteration 67/1000 | Loss: 0.00001078
Iteration 68/1000 | Loss: 0.00001078
Iteration 69/1000 | Loss: 0.00001077
Iteration 70/1000 | Loss: 0.00001077
Iteration 71/1000 | Loss: 0.00001077
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001076
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001073
Iteration 87/1000 | Loss: 0.00001073
Iteration 88/1000 | Loss: 0.00001073
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001072
Iteration 92/1000 | Loss: 0.00001072
Iteration 93/1000 | Loss: 0.00001072
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001071
Iteration 100/1000 | Loss: 0.00001071
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001070
Iteration 105/1000 | Loss: 0.00001069
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001068
Iteration 111/1000 | Loss: 0.00001068
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001067
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001066
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001066
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001065
Iteration 128/1000 | Loss: 0.00001065
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001065
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001064
Iteration 141/1000 | Loss: 0.00001064
Iteration 142/1000 | Loss: 0.00001064
Iteration 143/1000 | Loss: 0.00001064
Iteration 144/1000 | Loss: 0.00001064
Iteration 145/1000 | Loss: 0.00001064
Iteration 146/1000 | Loss: 0.00001064
Iteration 147/1000 | Loss: 0.00001064
Iteration 148/1000 | Loss: 0.00001064
Iteration 149/1000 | Loss: 0.00001064
Iteration 150/1000 | Loss: 0.00001064
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001063
Iteration 157/1000 | Loss: 0.00001062
Iteration 158/1000 | Loss: 0.00001062
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001062
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001061
Iteration 168/1000 | Loss: 0.00001061
Iteration 169/1000 | Loss: 0.00001061
Iteration 170/1000 | Loss: 0.00001061
Iteration 171/1000 | Loss: 0.00001061
Iteration 172/1000 | Loss: 0.00001061
Iteration 173/1000 | Loss: 0.00001061
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001061
Iteration 177/1000 | Loss: 0.00001061
Iteration 178/1000 | Loss: 0.00001061
Iteration 179/1000 | Loss: 0.00001061
Iteration 180/1000 | Loss: 0.00001061
Iteration 181/1000 | Loss: 0.00001061
Iteration 182/1000 | Loss: 0.00001061
Iteration 183/1000 | Loss: 0.00001060
Iteration 184/1000 | Loss: 0.00001060
Iteration 185/1000 | Loss: 0.00001060
Iteration 186/1000 | Loss: 0.00001060
Iteration 187/1000 | Loss: 0.00001060
Iteration 188/1000 | Loss: 0.00001060
Iteration 189/1000 | Loss: 0.00001060
Iteration 190/1000 | Loss: 0.00001059
Iteration 191/1000 | Loss: 0.00001059
Iteration 192/1000 | Loss: 0.00001059
Iteration 193/1000 | Loss: 0.00001059
Iteration 194/1000 | Loss: 0.00001059
Iteration 195/1000 | Loss: 0.00001059
Iteration 196/1000 | Loss: 0.00001059
Iteration 197/1000 | Loss: 0.00001059
Iteration 198/1000 | Loss: 0.00001059
Iteration 199/1000 | Loss: 0.00001059
Iteration 200/1000 | Loss: 0.00001059
Iteration 201/1000 | Loss: 0.00001059
Iteration 202/1000 | Loss: 0.00001059
Iteration 203/1000 | Loss: 0.00001059
Iteration 204/1000 | Loss: 0.00001059
Iteration 205/1000 | Loss: 0.00001059
Iteration 206/1000 | Loss: 0.00001059
Iteration 207/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.0587858014332596e-05, 1.0587858014332596e-05, 1.0587858014332596e-05, 1.0587858014332596e-05, 1.0587858014332596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0587858014332596e-05

Optimization complete. Final v2v error: 2.773048162460327 mm

Highest mean error: 3.7685604095458984 mm for frame 65

Lowest mean error: 2.4668564796447754 mm for frame 142

Saving results

Total time: 48.351733446121216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830001
Iteration 2/25 | Loss: 0.00144315
Iteration 3/25 | Loss: 0.00133654
Iteration 4/25 | Loss: 0.00132518
Iteration 5/25 | Loss: 0.00132230
Iteration 6/25 | Loss: 0.00132204
Iteration 7/25 | Loss: 0.00132204
Iteration 8/25 | Loss: 0.00132204
Iteration 9/25 | Loss: 0.00132204
Iteration 10/25 | Loss: 0.00132204
Iteration 11/25 | Loss: 0.00132204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013220430118963122, 0.0013220430118963122, 0.0013220430118963122, 0.0013220430118963122, 0.0013220430118963122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013220430118963122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21100128
Iteration 2/25 | Loss: 0.00232166
Iteration 3/25 | Loss: 0.00232165
Iteration 4/25 | Loss: 0.00232165
Iteration 5/25 | Loss: 0.00232165
Iteration 6/25 | Loss: 0.00232165
Iteration 7/25 | Loss: 0.00232165
Iteration 8/25 | Loss: 0.00232165
Iteration 9/25 | Loss: 0.00232165
Iteration 10/25 | Loss: 0.00232165
Iteration 11/25 | Loss: 0.00232165
Iteration 12/25 | Loss: 0.00232165
Iteration 13/25 | Loss: 0.00232165
Iteration 14/25 | Loss: 0.00232165
Iteration 15/25 | Loss: 0.00232165
Iteration 16/25 | Loss: 0.00232165
Iteration 17/25 | Loss: 0.00232165
Iteration 18/25 | Loss: 0.00232165
Iteration 19/25 | Loss: 0.00232165
Iteration 20/25 | Loss: 0.00232165
Iteration 21/25 | Loss: 0.00232165
Iteration 22/25 | Loss: 0.00232165
Iteration 23/25 | Loss: 0.00232165
Iteration 24/25 | Loss: 0.00232165
Iteration 25/25 | Loss: 0.00232165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232165
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001714
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001661
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001657
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001633
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001632
Iteration 38/1000 | Loss: 0.00001631
Iteration 39/1000 | Loss: 0.00001630
Iteration 40/1000 | Loss: 0.00001630
Iteration 41/1000 | Loss: 0.00001629
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001620
Iteration 48/1000 | Loss: 0.00001620
Iteration 49/1000 | Loss: 0.00001619
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001619
Iteration 52/1000 | Loss: 0.00001617
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001614
Iteration 58/1000 | Loss: 0.00001614
Iteration 59/1000 | Loss: 0.00001614
Iteration 60/1000 | Loss: 0.00001613
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001613
Iteration 63/1000 | Loss: 0.00001612
Iteration 64/1000 | Loss: 0.00001612
Iteration 65/1000 | Loss: 0.00001612
Iteration 66/1000 | Loss: 0.00001612
Iteration 67/1000 | Loss: 0.00001611
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001610
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001609
Iteration 78/1000 | Loss: 0.00001609
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001609
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001608
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001607
Iteration 91/1000 | Loss: 0.00001607
Iteration 92/1000 | Loss: 0.00001607
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001606
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001606
Iteration 98/1000 | Loss: 0.00001606
Iteration 99/1000 | Loss: 0.00001606
Iteration 100/1000 | Loss: 0.00001606
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001606
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001606
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001604
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001604
Iteration 123/1000 | Loss: 0.00001604
Iteration 124/1000 | Loss: 0.00001604
Iteration 125/1000 | Loss: 0.00001604
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001603
Iteration 128/1000 | Loss: 0.00001603
Iteration 129/1000 | Loss: 0.00001603
Iteration 130/1000 | Loss: 0.00001603
Iteration 131/1000 | Loss: 0.00001603
Iteration 132/1000 | Loss: 0.00001603
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001602
Iteration 142/1000 | Loss: 0.00001602
Iteration 143/1000 | Loss: 0.00001602
Iteration 144/1000 | Loss: 0.00001602
Iteration 145/1000 | Loss: 0.00001602
Iteration 146/1000 | Loss: 0.00001601
Iteration 147/1000 | Loss: 0.00001601
Iteration 148/1000 | Loss: 0.00001601
Iteration 149/1000 | Loss: 0.00001601
Iteration 150/1000 | Loss: 0.00001600
Iteration 151/1000 | Loss: 0.00001600
Iteration 152/1000 | Loss: 0.00001599
Iteration 153/1000 | Loss: 0.00001599
Iteration 154/1000 | Loss: 0.00001599
Iteration 155/1000 | Loss: 0.00001599
Iteration 156/1000 | Loss: 0.00001599
Iteration 157/1000 | Loss: 0.00001599
Iteration 158/1000 | Loss: 0.00001599
Iteration 159/1000 | Loss: 0.00001599
Iteration 160/1000 | Loss: 0.00001599
Iteration 161/1000 | Loss: 0.00001599
Iteration 162/1000 | Loss: 0.00001599
Iteration 163/1000 | Loss: 0.00001599
Iteration 164/1000 | Loss: 0.00001599
Iteration 165/1000 | Loss: 0.00001598
Iteration 166/1000 | Loss: 0.00001598
Iteration 167/1000 | Loss: 0.00001598
Iteration 168/1000 | Loss: 0.00001598
Iteration 169/1000 | Loss: 0.00001598
Iteration 170/1000 | Loss: 0.00001598
Iteration 171/1000 | Loss: 0.00001598
Iteration 172/1000 | Loss: 0.00001598
Iteration 173/1000 | Loss: 0.00001597
Iteration 174/1000 | Loss: 0.00001597
Iteration 175/1000 | Loss: 0.00001597
Iteration 176/1000 | Loss: 0.00001597
Iteration 177/1000 | Loss: 0.00001597
Iteration 178/1000 | Loss: 0.00001597
Iteration 179/1000 | Loss: 0.00001597
Iteration 180/1000 | Loss: 0.00001597
Iteration 181/1000 | Loss: 0.00001597
Iteration 182/1000 | Loss: 0.00001597
Iteration 183/1000 | Loss: 0.00001597
Iteration 184/1000 | Loss: 0.00001596
Iteration 185/1000 | Loss: 0.00001596
Iteration 186/1000 | Loss: 0.00001596
Iteration 187/1000 | Loss: 0.00001596
Iteration 188/1000 | Loss: 0.00001596
Iteration 189/1000 | Loss: 0.00001596
Iteration 190/1000 | Loss: 0.00001595
Iteration 191/1000 | Loss: 0.00001595
Iteration 192/1000 | Loss: 0.00001595
Iteration 193/1000 | Loss: 0.00001595
Iteration 194/1000 | Loss: 0.00001595
Iteration 195/1000 | Loss: 0.00001595
Iteration 196/1000 | Loss: 0.00001595
Iteration 197/1000 | Loss: 0.00001595
Iteration 198/1000 | Loss: 0.00001595
Iteration 199/1000 | Loss: 0.00001595
Iteration 200/1000 | Loss: 0.00001595
Iteration 201/1000 | Loss: 0.00001595
Iteration 202/1000 | Loss: 0.00001595
Iteration 203/1000 | Loss: 0.00001595
Iteration 204/1000 | Loss: 0.00001595
Iteration 205/1000 | Loss: 0.00001595
Iteration 206/1000 | Loss: 0.00001595
Iteration 207/1000 | Loss: 0.00001595
Iteration 208/1000 | Loss: 0.00001595
Iteration 209/1000 | Loss: 0.00001595
Iteration 210/1000 | Loss: 0.00001595
Iteration 211/1000 | Loss: 0.00001595
Iteration 212/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.594636523805093e-05, 1.594636523805093e-05, 1.594636523805093e-05, 1.594636523805093e-05, 1.594636523805093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.594636523805093e-05

Optimization complete. Final v2v error: 3.3743181228637695 mm

Highest mean error: 3.6035406589508057 mm for frame 85

Lowest mean error: 3.022651433944702 mm for frame 20

Saving results

Total time: 41.5301730632782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998588
Iteration 2/25 | Loss: 0.00220978
Iteration 3/25 | Loss: 0.00176754
Iteration 4/25 | Loss: 0.00157504
Iteration 5/25 | Loss: 0.00158434
Iteration 6/25 | Loss: 0.00137808
Iteration 7/25 | Loss: 0.00134496
Iteration 8/25 | Loss: 0.00133955
Iteration 9/25 | Loss: 0.00133920
Iteration 10/25 | Loss: 0.00133920
Iteration 11/25 | Loss: 0.00133920
Iteration 12/25 | Loss: 0.00133920
Iteration 13/25 | Loss: 0.00133920
Iteration 14/25 | Loss: 0.00133920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001339199603535235, 0.001339199603535235, 0.001339199603535235, 0.001339199603535235, 0.001339199603535235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001339199603535235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19310629
Iteration 2/25 | Loss: 0.00181137
Iteration 3/25 | Loss: 0.00181137
Iteration 4/25 | Loss: 0.00181137
Iteration 5/25 | Loss: 0.00181137
Iteration 6/25 | Loss: 0.00181137
Iteration 7/25 | Loss: 0.00181137
Iteration 8/25 | Loss: 0.00181137
Iteration 9/25 | Loss: 0.00181137
Iteration 10/25 | Loss: 0.00181137
Iteration 11/25 | Loss: 0.00181137
Iteration 12/25 | Loss: 0.00181137
Iteration 13/25 | Loss: 0.00181137
Iteration 14/25 | Loss: 0.00181137
Iteration 15/25 | Loss: 0.00181137
Iteration 16/25 | Loss: 0.00181137
Iteration 17/25 | Loss: 0.00181137
Iteration 18/25 | Loss: 0.00181137
Iteration 19/25 | Loss: 0.00181137
Iteration 20/25 | Loss: 0.00181137
Iteration 21/25 | Loss: 0.00181137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001811370486393571, 0.001811370486393571, 0.001811370486393571, 0.001811370486393571, 0.001811370486393571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001811370486393571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181137
Iteration 2/1000 | Loss: 0.00003980
Iteration 3/1000 | Loss: 0.00002689
Iteration 4/1000 | Loss: 0.00002519
Iteration 5/1000 | Loss: 0.00002436
Iteration 6/1000 | Loss: 0.00002363
Iteration 7/1000 | Loss: 0.00002290
Iteration 8/1000 | Loss: 0.00002262
Iteration 9/1000 | Loss: 0.00002209
Iteration 10/1000 | Loss: 0.00002181
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002152
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002126
Iteration 15/1000 | Loss: 0.00002126
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002122
Iteration 18/1000 | Loss: 0.00002122
Iteration 19/1000 | Loss: 0.00002121
Iteration 20/1000 | Loss: 0.00002121
Iteration 21/1000 | Loss: 0.00002121
Iteration 22/1000 | Loss: 0.00002120
Iteration 23/1000 | Loss: 0.00002114
Iteration 24/1000 | Loss: 0.00002114
Iteration 25/1000 | Loss: 0.00002114
Iteration 26/1000 | Loss: 0.00002113
Iteration 27/1000 | Loss: 0.00002113
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002113
Iteration 30/1000 | Loss: 0.00002113
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002113
Iteration 34/1000 | Loss: 0.00002112
Iteration 35/1000 | Loss: 0.00002112
Iteration 36/1000 | Loss: 0.00002112
Iteration 37/1000 | Loss: 0.00002111
Iteration 38/1000 | Loss: 0.00002111
Iteration 39/1000 | Loss: 0.00002110
Iteration 40/1000 | Loss: 0.00002110
Iteration 41/1000 | Loss: 0.00002110
Iteration 42/1000 | Loss: 0.00002109
Iteration 43/1000 | Loss: 0.00002109
Iteration 44/1000 | Loss: 0.00002108
Iteration 45/1000 | Loss: 0.00002108
Iteration 46/1000 | Loss: 0.00002108
Iteration 47/1000 | Loss: 0.00002107
Iteration 48/1000 | Loss: 0.00002107
Iteration 49/1000 | Loss: 0.00002107
Iteration 50/1000 | Loss: 0.00002107
Iteration 51/1000 | Loss: 0.00002106
Iteration 52/1000 | Loss: 0.00002106
Iteration 53/1000 | Loss: 0.00002106
Iteration 54/1000 | Loss: 0.00002106
Iteration 55/1000 | Loss: 0.00002106
Iteration 56/1000 | Loss: 0.00002106
Iteration 57/1000 | Loss: 0.00002106
Iteration 58/1000 | Loss: 0.00002106
Iteration 59/1000 | Loss: 0.00002106
Iteration 60/1000 | Loss: 0.00002106
Iteration 61/1000 | Loss: 0.00002106
Iteration 62/1000 | Loss: 0.00002106
Iteration 63/1000 | Loss: 0.00002106
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002106
Iteration 67/1000 | Loss: 0.00002106
Iteration 68/1000 | Loss: 0.00002106
Iteration 69/1000 | Loss: 0.00002106
Iteration 70/1000 | Loss: 0.00002106
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002106
Iteration 73/1000 | Loss: 0.00002106
Iteration 74/1000 | Loss: 0.00002106
Iteration 75/1000 | Loss: 0.00002106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.1055349861853756e-05, 2.1055349861853756e-05, 2.1055349861853756e-05, 2.1055349861853756e-05, 2.1055349861853756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1055349861853756e-05

Optimization complete. Final v2v error: 3.9595115184783936 mm

Highest mean error: 4.05926513671875 mm for frame 13

Lowest mean error: 3.9020299911499023 mm for frame 84

Saving results

Total time: 40.35821890830994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831480
Iteration 2/25 | Loss: 0.00155160
Iteration 3/25 | Loss: 0.00137931
Iteration 4/25 | Loss: 0.00137173
Iteration 5/25 | Loss: 0.00137070
Iteration 6/25 | Loss: 0.00137070
Iteration 7/25 | Loss: 0.00137070
Iteration 8/25 | Loss: 0.00137070
Iteration 9/25 | Loss: 0.00137070
Iteration 10/25 | Loss: 0.00137070
Iteration 11/25 | Loss: 0.00137070
Iteration 12/25 | Loss: 0.00137070
Iteration 13/25 | Loss: 0.00137070
Iteration 14/25 | Loss: 0.00137070
Iteration 15/25 | Loss: 0.00137070
Iteration 16/25 | Loss: 0.00137070
Iteration 17/25 | Loss: 0.00137070
Iteration 18/25 | Loss: 0.00137070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001370700541883707, 0.001370700541883707, 0.001370700541883707, 0.001370700541883707, 0.001370700541883707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001370700541883707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87463391
Iteration 2/25 | Loss: 0.00140015
Iteration 3/25 | Loss: 0.00140014
Iteration 4/25 | Loss: 0.00140014
Iteration 5/25 | Loss: 0.00140014
Iteration 6/25 | Loss: 0.00140014
Iteration 7/25 | Loss: 0.00140014
Iteration 8/25 | Loss: 0.00140014
Iteration 9/25 | Loss: 0.00140014
Iteration 10/25 | Loss: 0.00140014
Iteration 11/25 | Loss: 0.00140014
Iteration 12/25 | Loss: 0.00140014
Iteration 13/25 | Loss: 0.00140014
Iteration 14/25 | Loss: 0.00140014
Iteration 15/25 | Loss: 0.00140014
Iteration 16/25 | Loss: 0.00140014
Iteration 17/25 | Loss: 0.00140014
Iteration 18/25 | Loss: 0.00140014
Iteration 19/25 | Loss: 0.00140014
Iteration 20/25 | Loss: 0.00140014
Iteration 21/25 | Loss: 0.00140014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014001402305439115, 0.0014001402305439115, 0.0014001402305439115, 0.0014001402305439115, 0.0014001402305439115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014001402305439115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140014
Iteration 2/1000 | Loss: 0.00004425
Iteration 3/1000 | Loss: 0.00003042
Iteration 4/1000 | Loss: 0.00002723
Iteration 5/1000 | Loss: 0.00002578
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002449
Iteration 8/1000 | Loss: 0.00002401
Iteration 9/1000 | Loss: 0.00002363
Iteration 10/1000 | Loss: 0.00002328
Iteration 11/1000 | Loss: 0.00002302
Iteration 12/1000 | Loss: 0.00002272
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002228
Iteration 15/1000 | Loss: 0.00002221
Iteration 16/1000 | Loss: 0.00002208
Iteration 17/1000 | Loss: 0.00002205
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002189
Iteration 20/1000 | Loss: 0.00002187
Iteration 21/1000 | Loss: 0.00002186
Iteration 22/1000 | Loss: 0.00002186
Iteration 23/1000 | Loss: 0.00002186
Iteration 24/1000 | Loss: 0.00002185
Iteration 25/1000 | Loss: 0.00002185
Iteration 26/1000 | Loss: 0.00002185
Iteration 27/1000 | Loss: 0.00002183
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002176
Iteration 30/1000 | Loss: 0.00002172
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002170
Iteration 33/1000 | Loss: 0.00002169
Iteration 34/1000 | Loss: 0.00002169
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002164
Iteration 37/1000 | Loss: 0.00002163
Iteration 38/1000 | Loss: 0.00002163
Iteration 39/1000 | Loss: 0.00002163
Iteration 40/1000 | Loss: 0.00002163
Iteration 41/1000 | Loss: 0.00002163
Iteration 42/1000 | Loss: 0.00002163
Iteration 43/1000 | Loss: 0.00002163
Iteration 44/1000 | Loss: 0.00002163
Iteration 45/1000 | Loss: 0.00002163
Iteration 46/1000 | Loss: 0.00002163
Iteration 47/1000 | Loss: 0.00002163
Iteration 48/1000 | Loss: 0.00002163
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002162
Iteration 51/1000 | Loss: 0.00002162
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002162
Iteration 55/1000 | Loss: 0.00002162
Iteration 56/1000 | Loss: 0.00002161
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002161
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002161
Iteration 67/1000 | Loss: 0.00002161
Iteration 68/1000 | Loss: 0.00002161
Iteration 69/1000 | Loss: 0.00002161
Iteration 70/1000 | Loss: 0.00002160
Iteration 71/1000 | Loss: 0.00002160
Iteration 72/1000 | Loss: 0.00002160
Iteration 73/1000 | Loss: 0.00002160
Iteration 74/1000 | Loss: 0.00002160
Iteration 75/1000 | Loss: 0.00002160
Iteration 76/1000 | Loss: 0.00002160
Iteration 77/1000 | Loss: 0.00002160
Iteration 78/1000 | Loss: 0.00002160
Iteration 79/1000 | Loss: 0.00002160
Iteration 80/1000 | Loss: 0.00002160
Iteration 81/1000 | Loss: 0.00002160
Iteration 82/1000 | Loss: 0.00002160
Iteration 83/1000 | Loss: 0.00002160
Iteration 84/1000 | Loss: 0.00002160
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002160
Iteration 87/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.1596801161649637e-05, 2.1596801161649637e-05, 2.1596801161649637e-05, 2.1596801161649637e-05, 2.1596801161649637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1596801161649637e-05

Optimization complete. Final v2v error: 3.97151517868042 mm

Highest mean error: 4.0298871994018555 mm for frame 125

Lowest mean error: 3.897160291671753 mm for frame 150

Saving results

Total time: 36.86050820350647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367581
Iteration 2/25 | Loss: 0.00138540
Iteration 3/25 | Loss: 0.00126571
Iteration 4/25 | Loss: 0.00125426
Iteration 5/25 | Loss: 0.00124973
Iteration 6/25 | Loss: 0.00124973
Iteration 7/25 | Loss: 0.00124973
Iteration 8/25 | Loss: 0.00124973
Iteration 9/25 | Loss: 0.00124973
Iteration 10/25 | Loss: 0.00124973
Iteration 11/25 | Loss: 0.00124973
Iteration 12/25 | Loss: 0.00124973
Iteration 13/25 | Loss: 0.00124973
Iteration 14/25 | Loss: 0.00124973
Iteration 15/25 | Loss: 0.00124973
Iteration 16/25 | Loss: 0.00124973
Iteration 17/25 | Loss: 0.00124973
Iteration 18/25 | Loss: 0.00124973
Iteration 19/25 | Loss: 0.00124973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012497272109612823, 0.0012497272109612823, 0.0012497272109612823, 0.0012497272109612823, 0.0012497272109612823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012497272109612823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76421303
Iteration 2/25 | Loss: 0.00263586
Iteration 3/25 | Loss: 0.00263586
Iteration 4/25 | Loss: 0.00263586
Iteration 5/25 | Loss: 0.00263586
Iteration 6/25 | Loss: 0.00263586
Iteration 7/25 | Loss: 0.00263586
Iteration 8/25 | Loss: 0.00263586
Iteration 9/25 | Loss: 0.00263586
Iteration 10/25 | Loss: 0.00263586
Iteration 11/25 | Loss: 0.00263586
Iteration 12/25 | Loss: 0.00263586
Iteration 13/25 | Loss: 0.00263586
Iteration 14/25 | Loss: 0.00263586
Iteration 15/25 | Loss: 0.00263586
Iteration 16/25 | Loss: 0.00263586
Iteration 17/25 | Loss: 0.00263586
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026358584873378277, 0.0026358584873378277, 0.0026358584873378277, 0.0026358584873378277, 0.0026358584873378277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026358584873378277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263586
Iteration 2/1000 | Loss: 0.00003877
Iteration 3/1000 | Loss: 0.00002357
Iteration 4/1000 | Loss: 0.00002081
Iteration 5/1000 | Loss: 0.00001934
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001690
Iteration 9/1000 | Loss: 0.00001657
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001589
Iteration 12/1000 | Loss: 0.00001562
Iteration 13/1000 | Loss: 0.00001537
Iteration 14/1000 | Loss: 0.00001535
Iteration 15/1000 | Loss: 0.00001528
Iteration 16/1000 | Loss: 0.00001511
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001466
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001448
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001432
Iteration 29/1000 | Loss: 0.00001423
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001422
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001421
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001420
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001416
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001412
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001412
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001411
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001405
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001404
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001398
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001397
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001396
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001391
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001391
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001391
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001390
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001390
Iteration 115/1000 | Loss: 0.00001390
Iteration 116/1000 | Loss: 0.00001390
Iteration 117/1000 | Loss: 0.00001390
Iteration 118/1000 | Loss: 0.00001390
Iteration 119/1000 | Loss: 0.00001390
Iteration 120/1000 | Loss: 0.00001389
Iteration 121/1000 | Loss: 0.00001389
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001388
Iteration 126/1000 | Loss: 0.00001388
Iteration 127/1000 | Loss: 0.00001388
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001387
Iteration 131/1000 | Loss: 0.00001387
Iteration 132/1000 | Loss: 0.00001387
Iteration 133/1000 | Loss: 0.00001387
Iteration 134/1000 | Loss: 0.00001387
Iteration 135/1000 | Loss: 0.00001387
Iteration 136/1000 | Loss: 0.00001387
Iteration 137/1000 | Loss: 0.00001387
Iteration 138/1000 | Loss: 0.00001387
Iteration 139/1000 | Loss: 0.00001387
Iteration 140/1000 | Loss: 0.00001387
Iteration 141/1000 | Loss: 0.00001387
Iteration 142/1000 | Loss: 0.00001387
Iteration 143/1000 | Loss: 0.00001387
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001387
Iteration 150/1000 | Loss: 0.00001387
Iteration 151/1000 | Loss: 0.00001387
Iteration 152/1000 | Loss: 0.00001387
Iteration 153/1000 | Loss: 0.00001387
Iteration 154/1000 | Loss: 0.00001387
Iteration 155/1000 | Loss: 0.00001387
Iteration 156/1000 | Loss: 0.00001387
Iteration 157/1000 | Loss: 0.00001387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.3867657798982691e-05, 1.3867657798982691e-05, 1.3867657798982691e-05, 1.3867657798982691e-05, 1.3867657798982691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3867657798982691e-05

Optimization complete. Final v2v error: 3.1661534309387207 mm

Highest mean error: 3.302865743637085 mm for frame 249

Lowest mean error: 3.099424362182617 mm for frame 201

Saving results

Total time: 54.46115708351135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906576
Iteration 2/25 | Loss: 0.00156274
Iteration 3/25 | Loss: 0.00135490
Iteration 4/25 | Loss: 0.00133634
Iteration 5/25 | Loss: 0.00133104
Iteration 6/25 | Loss: 0.00132942
Iteration 7/25 | Loss: 0.00132942
Iteration 8/25 | Loss: 0.00132942
Iteration 9/25 | Loss: 0.00132942
Iteration 10/25 | Loss: 0.00132942
Iteration 11/25 | Loss: 0.00132942
Iteration 12/25 | Loss: 0.00132942
Iteration 13/25 | Loss: 0.00132942
Iteration 14/25 | Loss: 0.00132942
Iteration 15/25 | Loss: 0.00132942
Iteration 16/25 | Loss: 0.00132942
Iteration 17/25 | Loss: 0.00132942
Iteration 18/25 | Loss: 0.00132942
Iteration 19/25 | Loss: 0.00132942
Iteration 20/25 | Loss: 0.00132942
Iteration 21/25 | Loss: 0.00132942
Iteration 22/25 | Loss: 0.00132942
Iteration 23/25 | Loss: 0.00132942
Iteration 24/25 | Loss: 0.00132942
Iteration 25/25 | Loss: 0.00132942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07972491
Iteration 2/25 | Loss: 0.00171685
Iteration 3/25 | Loss: 0.00171684
Iteration 4/25 | Loss: 0.00171684
Iteration 5/25 | Loss: 0.00171684
Iteration 6/25 | Loss: 0.00171684
Iteration 7/25 | Loss: 0.00171684
Iteration 8/25 | Loss: 0.00171684
Iteration 9/25 | Loss: 0.00171684
Iteration 10/25 | Loss: 0.00171684
Iteration 11/25 | Loss: 0.00171684
Iteration 12/25 | Loss: 0.00171684
Iteration 13/25 | Loss: 0.00171684
Iteration 14/25 | Loss: 0.00171684
Iteration 15/25 | Loss: 0.00171684
Iteration 16/25 | Loss: 0.00171684
Iteration 17/25 | Loss: 0.00171684
Iteration 18/25 | Loss: 0.00171684
Iteration 19/25 | Loss: 0.00171684
Iteration 20/25 | Loss: 0.00171684
Iteration 21/25 | Loss: 0.00171684
Iteration 22/25 | Loss: 0.00171684
Iteration 23/25 | Loss: 0.00171684
Iteration 24/25 | Loss: 0.00171684
Iteration 25/25 | Loss: 0.00171684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171684
Iteration 2/1000 | Loss: 0.00004779
Iteration 3/1000 | Loss: 0.00003673
Iteration 4/1000 | Loss: 0.00003038
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002773
Iteration 7/1000 | Loss: 0.00002711
Iteration 8/1000 | Loss: 0.00002638
Iteration 9/1000 | Loss: 0.00002590
Iteration 10/1000 | Loss: 0.00002553
Iteration 11/1000 | Loss: 0.00002524
Iteration 12/1000 | Loss: 0.00002500
Iteration 13/1000 | Loss: 0.00002479
Iteration 14/1000 | Loss: 0.00002455
Iteration 15/1000 | Loss: 0.00002439
Iteration 16/1000 | Loss: 0.00002423
Iteration 17/1000 | Loss: 0.00002418
Iteration 18/1000 | Loss: 0.00002407
Iteration 19/1000 | Loss: 0.00002401
Iteration 20/1000 | Loss: 0.00002398
Iteration 21/1000 | Loss: 0.00002394
Iteration 22/1000 | Loss: 0.00002392
Iteration 23/1000 | Loss: 0.00002391
Iteration 24/1000 | Loss: 0.00002389
Iteration 25/1000 | Loss: 0.00002382
Iteration 26/1000 | Loss: 0.00002373
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00002369
Iteration 29/1000 | Loss: 0.00002368
Iteration 30/1000 | Loss: 0.00002368
Iteration 31/1000 | Loss: 0.00002367
Iteration 32/1000 | Loss: 0.00002367
Iteration 33/1000 | Loss: 0.00002366
Iteration 34/1000 | Loss: 0.00002365
Iteration 35/1000 | Loss: 0.00002365
Iteration 36/1000 | Loss: 0.00002365
Iteration 37/1000 | Loss: 0.00002365
Iteration 38/1000 | Loss: 0.00002365
Iteration 39/1000 | Loss: 0.00002365
Iteration 40/1000 | Loss: 0.00002365
Iteration 41/1000 | Loss: 0.00002365
Iteration 42/1000 | Loss: 0.00002365
Iteration 43/1000 | Loss: 0.00002365
Iteration 44/1000 | Loss: 0.00002364
Iteration 45/1000 | Loss: 0.00002362
Iteration 46/1000 | Loss: 0.00002362
Iteration 47/1000 | Loss: 0.00002362
Iteration 48/1000 | Loss: 0.00002362
Iteration 49/1000 | Loss: 0.00002362
Iteration 50/1000 | Loss: 0.00002362
Iteration 51/1000 | Loss: 0.00002361
Iteration 52/1000 | Loss: 0.00002361
Iteration 53/1000 | Loss: 0.00002361
Iteration 54/1000 | Loss: 0.00002361
Iteration 55/1000 | Loss: 0.00002361
Iteration 56/1000 | Loss: 0.00002361
Iteration 57/1000 | Loss: 0.00002361
Iteration 58/1000 | Loss: 0.00002360
Iteration 59/1000 | Loss: 0.00002360
Iteration 60/1000 | Loss: 0.00002360
Iteration 61/1000 | Loss: 0.00002360
Iteration 62/1000 | Loss: 0.00002360
Iteration 63/1000 | Loss: 0.00002360
Iteration 64/1000 | Loss: 0.00002360
Iteration 65/1000 | Loss: 0.00002360
Iteration 66/1000 | Loss: 0.00002360
Iteration 67/1000 | Loss: 0.00002359
Iteration 68/1000 | Loss: 0.00002359
Iteration 69/1000 | Loss: 0.00002359
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002358
Iteration 73/1000 | Loss: 0.00002358
Iteration 74/1000 | Loss: 0.00002358
Iteration 75/1000 | Loss: 0.00002358
Iteration 76/1000 | Loss: 0.00002358
Iteration 77/1000 | Loss: 0.00002358
Iteration 78/1000 | Loss: 0.00002358
Iteration 79/1000 | Loss: 0.00002357
Iteration 80/1000 | Loss: 0.00002357
Iteration 81/1000 | Loss: 0.00002357
Iteration 82/1000 | Loss: 0.00002357
Iteration 83/1000 | Loss: 0.00002357
Iteration 84/1000 | Loss: 0.00002356
Iteration 85/1000 | Loss: 0.00002356
Iteration 86/1000 | Loss: 0.00002356
Iteration 87/1000 | Loss: 0.00002356
Iteration 88/1000 | Loss: 0.00002356
Iteration 89/1000 | Loss: 0.00002356
Iteration 90/1000 | Loss: 0.00002356
Iteration 91/1000 | Loss: 0.00002356
Iteration 92/1000 | Loss: 0.00002356
Iteration 93/1000 | Loss: 0.00002356
Iteration 94/1000 | Loss: 0.00002356
Iteration 95/1000 | Loss: 0.00002356
Iteration 96/1000 | Loss: 0.00002356
Iteration 97/1000 | Loss: 0.00002356
Iteration 98/1000 | Loss: 0.00002356
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00002356
Iteration 102/1000 | Loss: 0.00002356
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002356
Iteration 107/1000 | Loss: 0.00002356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.3557484382763505e-05, 2.3557484382763505e-05, 2.3557484382763505e-05, 2.3557484382763505e-05, 2.3557484382763505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3557484382763505e-05

Optimization complete. Final v2v error: 3.9763362407684326 mm

Highest mean error: 5.303537368774414 mm for frame 85

Lowest mean error: 3.1109986305236816 mm for frame 121

Saving results

Total time: 44.62215995788574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395451
Iteration 2/25 | Loss: 0.00129855
Iteration 3/25 | Loss: 0.00123661
Iteration 4/25 | Loss: 0.00122734
Iteration 5/25 | Loss: 0.00122412
Iteration 6/25 | Loss: 0.00122398
Iteration 7/25 | Loss: 0.00122398
Iteration 8/25 | Loss: 0.00122398
Iteration 9/25 | Loss: 0.00122398
Iteration 10/25 | Loss: 0.00122398
Iteration 11/25 | Loss: 0.00122398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012239828938618302, 0.0012239828938618302, 0.0012239828938618302, 0.0012239828938618302, 0.0012239828938618302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012239828938618302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22283792
Iteration 2/25 | Loss: 0.00336449
Iteration 3/25 | Loss: 0.00336449
Iteration 4/25 | Loss: 0.00336449
Iteration 5/25 | Loss: 0.00336449
Iteration 6/25 | Loss: 0.00336449
Iteration 7/25 | Loss: 0.00336449
Iteration 8/25 | Loss: 0.00336449
Iteration 9/25 | Loss: 0.00336449
Iteration 10/25 | Loss: 0.00336449
Iteration 11/25 | Loss: 0.00336449
Iteration 12/25 | Loss: 0.00336449
Iteration 13/25 | Loss: 0.00336449
Iteration 14/25 | Loss: 0.00336449
Iteration 15/25 | Loss: 0.00336449
Iteration 16/25 | Loss: 0.00336449
Iteration 17/25 | Loss: 0.00336449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00336448778398335, 0.00336448778398335, 0.00336448778398335, 0.00336448778398335, 0.00336448778398335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00336448778398335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00336449
Iteration 2/1000 | Loss: 0.00003523
Iteration 3/1000 | Loss: 0.00002487
Iteration 4/1000 | Loss: 0.00002092
Iteration 5/1000 | Loss: 0.00001918
Iteration 6/1000 | Loss: 0.00001776
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001591
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001487
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001469
Iteration 16/1000 | Loss: 0.00001467
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001458
Iteration 19/1000 | Loss: 0.00001458
Iteration 20/1000 | Loss: 0.00001454
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001451
Iteration 23/1000 | Loss: 0.00001448
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001427
Iteration 33/1000 | Loss: 0.00001427
Iteration 34/1000 | Loss: 0.00001427
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001426
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001425
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001420
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001419
Iteration 57/1000 | Loss: 0.00001419
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001419
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001417
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00001416
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001415
Iteration 71/1000 | Loss: 0.00001415
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001414
Iteration 75/1000 | Loss: 0.00001414
Iteration 76/1000 | Loss: 0.00001414
Iteration 77/1000 | Loss: 0.00001413
Iteration 78/1000 | Loss: 0.00001413
Iteration 79/1000 | Loss: 0.00001413
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001410
Iteration 88/1000 | Loss: 0.00001410
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001410
Iteration 95/1000 | Loss: 0.00001409
Iteration 96/1000 | Loss: 0.00001409
Iteration 97/1000 | Loss: 0.00001409
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001408
Iteration 101/1000 | Loss: 0.00001408
Iteration 102/1000 | Loss: 0.00001408
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001407
Iteration 105/1000 | Loss: 0.00001407
Iteration 106/1000 | Loss: 0.00001407
Iteration 107/1000 | Loss: 0.00001407
Iteration 108/1000 | Loss: 0.00001407
Iteration 109/1000 | Loss: 0.00001407
Iteration 110/1000 | Loss: 0.00001407
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00001407
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001406
Iteration 115/1000 | Loss: 0.00001406
Iteration 116/1000 | Loss: 0.00001406
Iteration 117/1000 | Loss: 0.00001406
Iteration 118/1000 | Loss: 0.00001406
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001404
Iteration 129/1000 | Loss: 0.00001404
Iteration 130/1000 | Loss: 0.00001403
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001403
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001401
Iteration 135/1000 | Loss: 0.00001401
Iteration 136/1000 | Loss: 0.00001401
Iteration 137/1000 | Loss: 0.00001401
Iteration 138/1000 | Loss: 0.00001401
Iteration 139/1000 | Loss: 0.00001400
Iteration 140/1000 | Loss: 0.00001400
Iteration 141/1000 | Loss: 0.00001400
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001398
Iteration 144/1000 | Loss: 0.00001398
Iteration 145/1000 | Loss: 0.00001397
Iteration 146/1000 | Loss: 0.00001397
Iteration 147/1000 | Loss: 0.00001396
Iteration 148/1000 | Loss: 0.00001396
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001395
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001393
Iteration 156/1000 | Loss: 0.00001393
Iteration 157/1000 | Loss: 0.00001393
Iteration 158/1000 | Loss: 0.00001392
Iteration 159/1000 | Loss: 0.00001392
Iteration 160/1000 | Loss: 0.00001391
Iteration 161/1000 | Loss: 0.00001391
Iteration 162/1000 | Loss: 0.00001391
Iteration 163/1000 | Loss: 0.00001391
Iteration 164/1000 | Loss: 0.00001391
Iteration 165/1000 | Loss: 0.00001391
Iteration 166/1000 | Loss: 0.00001391
Iteration 167/1000 | Loss: 0.00001391
Iteration 168/1000 | Loss: 0.00001391
Iteration 169/1000 | Loss: 0.00001391
Iteration 170/1000 | Loss: 0.00001391
Iteration 171/1000 | Loss: 0.00001390
Iteration 172/1000 | Loss: 0.00001390
Iteration 173/1000 | Loss: 0.00001390
Iteration 174/1000 | Loss: 0.00001390
Iteration 175/1000 | Loss: 0.00001390
Iteration 176/1000 | Loss: 0.00001390
Iteration 177/1000 | Loss: 0.00001390
Iteration 178/1000 | Loss: 0.00001390
Iteration 179/1000 | Loss: 0.00001390
Iteration 180/1000 | Loss: 0.00001390
Iteration 181/1000 | Loss: 0.00001390
Iteration 182/1000 | Loss: 0.00001390
Iteration 183/1000 | Loss: 0.00001390
Iteration 184/1000 | Loss: 0.00001390
Iteration 185/1000 | Loss: 0.00001390
Iteration 186/1000 | Loss: 0.00001389
Iteration 187/1000 | Loss: 0.00001389
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001387
Iteration 192/1000 | Loss: 0.00001387
Iteration 193/1000 | Loss: 0.00001386
Iteration 194/1000 | Loss: 0.00001386
Iteration 195/1000 | Loss: 0.00001386
Iteration 196/1000 | Loss: 0.00001386
Iteration 197/1000 | Loss: 0.00001386
Iteration 198/1000 | Loss: 0.00001386
Iteration 199/1000 | Loss: 0.00001386
Iteration 200/1000 | Loss: 0.00001386
Iteration 201/1000 | Loss: 0.00001386
Iteration 202/1000 | Loss: 0.00001386
Iteration 203/1000 | Loss: 0.00001386
Iteration 204/1000 | Loss: 0.00001385
Iteration 205/1000 | Loss: 0.00001385
Iteration 206/1000 | Loss: 0.00001385
Iteration 207/1000 | Loss: 0.00001385
Iteration 208/1000 | Loss: 0.00001385
Iteration 209/1000 | Loss: 0.00001385
Iteration 210/1000 | Loss: 0.00001385
Iteration 211/1000 | Loss: 0.00001385
Iteration 212/1000 | Loss: 0.00001385
Iteration 213/1000 | Loss: 0.00001385
Iteration 214/1000 | Loss: 0.00001385
Iteration 215/1000 | Loss: 0.00001384
Iteration 216/1000 | Loss: 0.00001384
Iteration 217/1000 | Loss: 0.00001384
Iteration 218/1000 | Loss: 0.00001384
Iteration 219/1000 | Loss: 0.00001383
Iteration 220/1000 | Loss: 0.00001383
Iteration 221/1000 | Loss: 0.00001383
Iteration 222/1000 | Loss: 0.00001383
Iteration 223/1000 | Loss: 0.00001383
Iteration 224/1000 | Loss: 0.00001383
Iteration 225/1000 | Loss: 0.00001383
Iteration 226/1000 | Loss: 0.00001382
Iteration 227/1000 | Loss: 0.00001382
Iteration 228/1000 | Loss: 0.00001382
Iteration 229/1000 | Loss: 0.00001382
Iteration 230/1000 | Loss: 0.00001382
Iteration 231/1000 | Loss: 0.00001382
Iteration 232/1000 | Loss: 0.00001382
Iteration 233/1000 | Loss: 0.00001382
Iteration 234/1000 | Loss: 0.00001382
Iteration 235/1000 | Loss: 0.00001382
Iteration 236/1000 | Loss: 0.00001381
Iteration 237/1000 | Loss: 0.00001381
Iteration 238/1000 | Loss: 0.00001381
Iteration 239/1000 | Loss: 0.00001381
Iteration 240/1000 | Loss: 0.00001381
Iteration 241/1000 | Loss: 0.00001381
Iteration 242/1000 | Loss: 0.00001381
Iteration 243/1000 | Loss: 0.00001381
Iteration 244/1000 | Loss: 0.00001381
Iteration 245/1000 | Loss: 0.00001381
Iteration 246/1000 | Loss: 0.00001381
Iteration 247/1000 | Loss: 0.00001380
Iteration 248/1000 | Loss: 0.00001380
Iteration 249/1000 | Loss: 0.00001380
Iteration 250/1000 | Loss: 0.00001380
Iteration 251/1000 | Loss: 0.00001380
Iteration 252/1000 | Loss: 0.00001379
Iteration 253/1000 | Loss: 0.00001379
Iteration 254/1000 | Loss: 0.00001379
Iteration 255/1000 | Loss: 0.00001379
Iteration 256/1000 | Loss: 0.00001379
Iteration 257/1000 | Loss: 0.00001379
Iteration 258/1000 | Loss: 0.00001379
Iteration 259/1000 | Loss: 0.00001379
Iteration 260/1000 | Loss: 0.00001379
Iteration 261/1000 | Loss: 0.00001379
Iteration 262/1000 | Loss: 0.00001379
Iteration 263/1000 | Loss: 0.00001379
Iteration 264/1000 | Loss: 0.00001379
Iteration 265/1000 | Loss: 0.00001379
Iteration 266/1000 | Loss: 0.00001379
Iteration 267/1000 | Loss: 0.00001379
Iteration 268/1000 | Loss: 0.00001379
Iteration 269/1000 | Loss: 0.00001379
Iteration 270/1000 | Loss: 0.00001379
Iteration 271/1000 | Loss: 0.00001379
Iteration 272/1000 | Loss: 0.00001379
Iteration 273/1000 | Loss: 0.00001379
Iteration 274/1000 | Loss: 0.00001379
Iteration 275/1000 | Loss: 0.00001379
Iteration 276/1000 | Loss: 0.00001379
Iteration 277/1000 | Loss: 0.00001379
Iteration 278/1000 | Loss: 0.00001379
Iteration 279/1000 | Loss: 0.00001379
Iteration 280/1000 | Loss: 0.00001379
Iteration 281/1000 | Loss: 0.00001379
Iteration 282/1000 | Loss: 0.00001379
Iteration 283/1000 | Loss: 0.00001379
Iteration 284/1000 | Loss: 0.00001379
Iteration 285/1000 | Loss: 0.00001379
Iteration 286/1000 | Loss: 0.00001379
Iteration 287/1000 | Loss: 0.00001379
Iteration 288/1000 | Loss: 0.00001379
Iteration 289/1000 | Loss: 0.00001379
Iteration 290/1000 | Loss: 0.00001379
Iteration 291/1000 | Loss: 0.00001379
Iteration 292/1000 | Loss: 0.00001379
Iteration 293/1000 | Loss: 0.00001379
Iteration 294/1000 | Loss: 0.00001379
Iteration 295/1000 | Loss: 0.00001379
Iteration 296/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.3786566341877915e-05, 1.3786566341877915e-05, 1.3786566341877915e-05, 1.3786566341877915e-05, 1.3786566341877915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3786566341877915e-05

Optimization complete. Final v2v error: 3.082005500793457 mm

Highest mean error: 3.868717908859253 mm for frame 120

Lowest mean error: 2.4784772396087646 mm for frame 27

Saving results

Total time: 55.172048807144165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015245
Iteration 2/25 | Loss: 0.00230907
Iteration 3/25 | Loss: 0.00178418
Iteration 4/25 | Loss: 0.00161064
Iteration 5/25 | Loss: 0.00153560
Iteration 6/25 | Loss: 0.00146518
Iteration 7/25 | Loss: 0.00141075
Iteration 8/25 | Loss: 0.00139390
Iteration 9/25 | Loss: 0.00138277
Iteration 10/25 | Loss: 0.00137616
Iteration 11/25 | Loss: 0.00137316
Iteration 12/25 | Loss: 0.00137236
Iteration 13/25 | Loss: 0.00137209
Iteration 14/25 | Loss: 0.00137203
Iteration 15/25 | Loss: 0.00137203
Iteration 16/25 | Loss: 0.00137203
Iteration 17/25 | Loss: 0.00137203
Iteration 18/25 | Loss: 0.00137203
Iteration 19/25 | Loss: 0.00137203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001372027094475925, 0.001372027094475925, 0.001372027094475925, 0.001372027094475925, 0.001372027094475925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001372027094475925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20657527
Iteration 2/25 | Loss: 0.00233690
Iteration 3/25 | Loss: 0.00233689
Iteration 4/25 | Loss: 0.00233689
Iteration 5/25 | Loss: 0.00233689
Iteration 6/25 | Loss: 0.00233689
Iteration 7/25 | Loss: 0.00233689
Iteration 8/25 | Loss: 0.00233689
Iteration 9/25 | Loss: 0.00233689
Iteration 10/25 | Loss: 0.00233689
Iteration 11/25 | Loss: 0.00233689
Iteration 12/25 | Loss: 0.00233689
Iteration 13/25 | Loss: 0.00233689
Iteration 14/25 | Loss: 0.00233689
Iteration 15/25 | Loss: 0.00233689
Iteration 16/25 | Loss: 0.00233689
Iteration 17/25 | Loss: 0.00233689
Iteration 18/25 | Loss: 0.00233689
Iteration 19/25 | Loss: 0.00233689
Iteration 20/25 | Loss: 0.00233689
Iteration 21/25 | Loss: 0.00233689
Iteration 22/25 | Loss: 0.00233689
Iteration 23/25 | Loss: 0.00233689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0023368860129266977, 0.0023368860129266977, 0.0023368860129266977, 0.0023368860129266977, 0.0023368860129266977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023368860129266977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233689
Iteration 2/1000 | Loss: 0.00028603
Iteration 3/1000 | Loss: 0.00024769
Iteration 4/1000 | Loss: 0.00022086
Iteration 5/1000 | Loss: 0.00015382
Iteration 6/1000 | Loss: 0.00014437
Iteration 7/1000 | Loss: 0.00006310
Iteration 8/1000 | Loss: 0.00005554
Iteration 9/1000 | Loss: 0.00005155
Iteration 10/1000 | Loss: 0.00006181
Iteration 11/1000 | Loss: 0.00004887
Iteration 12/1000 | Loss: 0.00004761
Iteration 13/1000 | Loss: 0.00004604
Iteration 14/1000 | Loss: 0.00004508
Iteration 15/1000 | Loss: 0.00016904
Iteration 16/1000 | Loss: 0.00051770
Iteration 17/1000 | Loss: 0.00018288
Iteration 18/1000 | Loss: 0.00006335
Iteration 19/1000 | Loss: 0.00004637
Iteration 20/1000 | Loss: 0.00003930
Iteration 21/1000 | Loss: 0.00003402
Iteration 22/1000 | Loss: 0.00003048
Iteration 23/1000 | Loss: 0.00002787
Iteration 24/1000 | Loss: 0.00002500
Iteration 25/1000 | Loss: 0.00002349
Iteration 26/1000 | Loss: 0.00002242
Iteration 27/1000 | Loss: 0.00002184
Iteration 28/1000 | Loss: 0.00002115
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002011
Iteration 31/1000 | Loss: 0.00001984
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001951
Iteration 34/1000 | Loss: 0.00003273
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002049
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001934
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001923
Iteration 46/1000 | Loss: 0.00001923
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001914
Iteration 51/1000 | Loss: 0.00001905
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001899
Iteration 55/1000 | Loss: 0.00001899
Iteration 56/1000 | Loss: 0.00001899
Iteration 57/1000 | Loss: 0.00001898
Iteration 58/1000 | Loss: 0.00001898
Iteration 59/1000 | Loss: 0.00001898
Iteration 60/1000 | Loss: 0.00001898
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001898
Iteration 63/1000 | Loss: 0.00001898
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001897
Iteration 66/1000 | Loss: 0.00001897
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001896
Iteration 69/1000 | Loss: 0.00001896
Iteration 70/1000 | Loss: 0.00001896
Iteration 71/1000 | Loss: 0.00001896
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001896
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001895
Iteration 77/1000 | Loss: 0.00001895
Iteration 78/1000 | Loss: 0.00001895
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001895
Iteration 81/1000 | Loss: 0.00001895
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001894
Iteration 85/1000 | Loss: 0.00001894
Iteration 86/1000 | Loss: 0.00001894
Iteration 87/1000 | Loss: 0.00001894
Iteration 88/1000 | Loss: 0.00001894
Iteration 89/1000 | Loss: 0.00001894
Iteration 90/1000 | Loss: 0.00001894
Iteration 91/1000 | Loss: 0.00001894
Iteration 92/1000 | Loss: 0.00001894
Iteration 93/1000 | Loss: 0.00001894
Iteration 94/1000 | Loss: 0.00001894
Iteration 95/1000 | Loss: 0.00001894
Iteration 96/1000 | Loss: 0.00001894
Iteration 97/1000 | Loss: 0.00001893
Iteration 98/1000 | Loss: 0.00001893
Iteration 99/1000 | Loss: 0.00001893
Iteration 100/1000 | Loss: 0.00001893
Iteration 101/1000 | Loss: 0.00001893
Iteration 102/1000 | Loss: 0.00001893
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001893
Iteration 105/1000 | Loss: 0.00001892
Iteration 106/1000 | Loss: 0.00001892
Iteration 107/1000 | Loss: 0.00001892
Iteration 108/1000 | Loss: 0.00001892
Iteration 109/1000 | Loss: 0.00001892
Iteration 110/1000 | Loss: 0.00001892
Iteration 111/1000 | Loss: 0.00001892
Iteration 112/1000 | Loss: 0.00001892
Iteration 113/1000 | Loss: 0.00001892
Iteration 114/1000 | Loss: 0.00001892
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001892
Iteration 120/1000 | Loss: 0.00001892
Iteration 121/1000 | Loss: 0.00001892
Iteration 122/1000 | Loss: 0.00001892
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001892
Iteration 125/1000 | Loss: 0.00001892
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.891731153591536e-05, 1.891731153591536e-05, 1.891731153591536e-05, 1.891731153591536e-05, 1.891731153591536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.891731153591536e-05

Optimization complete. Final v2v error: 3.6771132946014404 mm

Highest mean error: 4.335029602050781 mm for frame 239

Lowest mean error: 3.329953193664551 mm for frame 1

Saving results

Total time: 98.70598077774048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360409
Iteration 2/25 | Loss: 0.00136168
Iteration 3/25 | Loss: 0.00125258
Iteration 4/25 | Loss: 0.00123171
Iteration 5/25 | Loss: 0.00122407
Iteration 6/25 | Loss: 0.00122199
Iteration 7/25 | Loss: 0.00122191
Iteration 8/25 | Loss: 0.00122191
Iteration 9/25 | Loss: 0.00122191
Iteration 10/25 | Loss: 0.00122191
Iteration 11/25 | Loss: 0.00122191
Iteration 12/25 | Loss: 0.00122191
Iteration 13/25 | Loss: 0.00122191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012219087220728397, 0.0012219087220728397, 0.0012219087220728397, 0.0012219087220728397, 0.0012219087220728397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012219087220728397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23463655
Iteration 2/25 | Loss: 0.00306085
Iteration 3/25 | Loss: 0.00306085
Iteration 4/25 | Loss: 0.00306085
Iteration 5/25 | Loss: 0.00306085
Iteration 6/25 | Loss: 0.00306085
Iteration 7/25 | Loss: 0.00306085
Iteration 8/25 | Loss: 0.00306085
Iteration 9/25 | Loss: 0.00306085
Iteration 10/25 | Loss: 0.00306085
Iteration 11/25 | Loss: 0.00306084
Iteration 12/25 | Loss: 0.00306084
Iteration 13/25 | Loss: 0.00306084
Iteration 14/25 | Loss: 0.00306084
Iteration 15/25 | Loss: 0.00306084
Iteration 16/25 | Loss: 0.00306084
Iteration 17/25 | Loss: 0.00306084
Iteration 18/25 | Loss: 0.00306084
Iteration 19/25 | Loss: 0.00306084
Iteration 20/25 | Loss: 0.00306084
Iteration 21/25 | Loss: 0.00306084
Iteration 22/25 | Loss: 0.00306084
Iteration 23/25 | Loss: 0.00306084
Iteration 24/25 | Loss: 0.00306084
Iteration 25/25 | Loss: 0.00306084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306084
Iteration 2/1000 | Loss: 0.00004950
Iteration 3/1000 | Loss: 0.00003449
Iteration 4/1000 | Loss: 0.00002552
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001725
Iteration 9/1000 | Loss: 0.00001668
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001557
Iteration 13/1000 | Loss: 0.00001536
Iteration 14/1000 | Loss: 0.00001529
Iteration 15/1000 | Loss: 0.00001523
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001519
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001517
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001487
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001486
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001481
Iteration 36/1000 | Loss: 0.00001481
Iteration 37/1000 | Loss: 0.00001480
Iteration 38/1000 | Loss: 0.00001480
Iteration 39/1000 | Loss: 0.00001480
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001478
Iteration 42/1000 | Loss: 0.00001477
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001472
Iteration 49/1000 | Loss: 0.00001471
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001470
Iteration 52/1000 | Loss: 0.00001470
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001467
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001465
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001463
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001463
Iteration 73/1000 | Loss: 0.00001463
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001462
Iteration 76/1000 | Loss: 0.00001462
Iteration 77/1000 | Loss: 0.00001462
Iteration 78/1000 | Loss: 0.00001462
Iteration 79/1000 | Loss: 0.00001461
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001461
Iteration 84/1000 | Loss: 0.00001460
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001460
Iteration 87/1000 | Loss: 0.00001460
Iteration 88/1000 | Loss: 0.00001460
Iteration 89/1000 | Loss: 0.00001460
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001459
Iteration 92/1000 | Loss: 0.00001459
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001453
Iteration 124/1000 | Loss: 0.00001453
Iteration 125/1000 | Loss: 0.00001453
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001452
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001452
Iteration 132/1000 | Loss: 0.00001452
Iteration 133/1000 | Loss: 0.00001451
Iteration 134/1000 | Loss: 0.00001451
Iteration 135/1000 | Loss: 0.00001451
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001450
Iteration 139/1000 | Loss: 0.00001450
Iteration 140/1000 | Loss: 0.00001450
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00001450
Iteration 143/1000 | Loss: 0.00001450
Iteration 144/1000 | Loss: 0.00001450
Iteration 145/1000 | Loss: 0.00001449
Iteration 146/1000 | Loss: 0.00001449
Iteration 147/1000 | Loss: 0.00001449
Iteration 148/1000 | Loss: 0.00001449
Iteration 149/1000 | Loss: 0.00001449
Iteration 150/1000 | Loss: 0.00001449
Iteration 151/1000 | Loss: 0.00001449
Iteration 152/1000 | Loss: 0.00001449
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001448
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001448
Iteration 158/1000 | Loss: 0.00001448
Iteration 159/1000 | Loss: 0.00001448
Iteration 160/1000 | Loss: 0.00001448
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001447
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001447
Iteration 166/1000 | Loss: 0.00001447
Iteration 167/1000 | Loss: 0.00001447
Iteration 168/1000 | Loss: 0.00001446
Iteration 169/1000 | Loss: 0.00001446
Iteration 170/1000 | Loss: 0.00001446
Iteration 171/1000 | Loss: 0.00001445
Iteration 172/1000 | Loss: 0.00001445
Iteration 173/1000 | Loss: 0.00001445
Iteration 174/1000 | Loss: 0.00001445
Iteration 175/1000 | Loss: 0.00001445
Iteration 176/1000 | Loss: 0.00001445
Iteration 177/1000 | Loss: 0.00001445
Iteration 178/1000 | Loss: 0.00001445
Iteration 179/1000 | Loss: 0.00001445
Iteration 180/1000 | Loss: 0.00001445
Iteration 181/1000 | Loss: 0.00001445
Iteration 182/1000 | Loss: 0.00001444
Iteration 183/1000 | Loss: 0.00001444
Iteration 184/1000 | Loss: 0.00001444
Iteration 185/1000 | Loss: 0.00001444
Iteration 186/1000 | Loss: 0.00001444
Iteration 187/1000 | Loss: 0.00001444
Iteration 188/1000 | Loss: 0.00001444
Iteration 189/1000 | Loss: 0.00001444
Iteration 190/1000 | Loss: 0.00001444
Iteration 191/1000 | Loss: 0.00001444
Iteration 192/1000 | Loss: 0.00001444
Iteration 193/1000 | Loss: 0.00001443
Iteration 194/1000 | Loss: 0.00001443
Iteration 195/1000 | Loss: 0.00001443
Iteration 196/1000 | Loss: 0.00001443
Iteration 197/1000 | Loss: 0.00001443
Iteration 198/1000 | Loss: 0.00001443
Iteration 199/1000 | Loss: 0.00001443
Iteration 200/1000 | Loss: 0.00001443
Iteration 201/1000 | Loss: 0.00001443
Iteration 202/1000 | Loss: 0.00001443
Iteration 203/1000 | Loss: 0.00001443
Iteration 204/1000 | Loss: 0.00001443
Iteration 205/1000 | Loss: 0.00001443
Iteration 206/1000 | Loss: 0.00001443
Iteration 207/1000 | Loss: 0.00001442
Iteration 208/1000 | Loss: 0.00001442
Iteration 209/1000 | Loss: 0.00001442
Iteration 210/1000 | Loss: 0.00001442
Iteration 211/1000 | Loss: 0.00001442
Iteration 212/1000 | Loss: 0.00001442
Iteration 213/1000 | Loss: 0.00001442
Iteration 214/1000 | Loss: 0.00001441
Iteration 215/1000 | Loss: 0.00001441
Iteration 216/1000 | Loss: 0.00001441
Iteration 217/1000 | Loss: 0.00001441
Iteration 218/1000 | Loss: 0.00001441
Iteration 219/1000 | Loss: 0.00001440
Iteration 220/1000 | Loss: 0.00001440
Iteration 221/1000 | Loss: 0.00001440
Iteration 222/1000 | Loss: 0.00001440
Iteration 223/1000 | Loss: 0.00001440
Iteration 224/1000 | Loss: 0.00001440
Iteration 225/1000 | Loss: 0.00001439
Iteration 226/1000 | Loss: 0.00001439
Iteration 227/1000 | Loss: 0.00001439
Iteration 228/1000 | Loss: 0.00001439
Iteration 229/1000 | Loss: 0.00001439
Iteration 230/1000 | Loss: 0.00001438
Iteration 231/1000 | Loss: 0.00001438
Iteration 232/1000 | Loss: 0.00001438
Iteration 233/1000 | Loss: 0.00001438
Iteration 234/1000 | Loss: 0.00001438
Iteration 235/1000 | Loss: 0.00001438
Iteration 236/1000 | Loss: 0.00001438
Iteration 237/1000 | Loss: 0.00001438
Iteration 238/1000 | Loss: 0.00001437
Iteration 239/1000 | Loss: 0.00001437
Iteration 240/1000 | Loss: 0.00001437
Iteration 241/1000 | Loss: 0.00001437
Iteration 242/1000 | Loss: 0.00001437
Iteration 243/1000 | Loss: 0.00001437
Iteration 244/1000 | Loss: 0.00001437
Iteration 245/1000 | Loss: 0.00001437
Iteration 246/1000 | Loss: 0.00001437
Iteration 247/1000 | Loss: 0.00001437
Iteration 248/1000 | Loss: 0.00001437
Iteration 249/1000 | Loss: 0.00001437
Iteration 250/1000 | Loss: 0.00001437
Iteration 251/1000 | Loss: 0.00001437
Iteration 252/1000 | Loss: 0.00001436
Iteration 253/1000 | Loss: 0.00001436
Iteration 254/1000 | Loss: 0.00001436
Iteration 255/1000 | Loss: 0.00001436
Iteration 256/1000 | Loss: 0.00001436
Iteration 257/1000 | Loss: 0.00001436
Iteration 258/1000 | Loss: 0.00001436
Iteration 259/1000 | Loss: 0.00001436
Iteration 260/1000 | Loss: 0.00001436
Iteration 261/1000 | Loss: 0.00001436
Iteration 262/1000 | Loss: 0.00001436
Iteration 263/1000 | Loss: 0.00001436
Iteration 264/1000 | Loss: 0.00001436
Iteration 265/1000 | Loss: 0.00001436
Iteration 266/1000 | Loss: 0.00001436
Iteration 267/1000 | Loss: 0.00001435
Iteration 268/1000 | Loss: 0.00001435
Iteration 269/1000 | Loss: 0.00001435
Iteration 270/1000 | Loss: 0.00001435
Iteration 271/1000 | Loss: 0.00001435
Iteration 272/1000 | Loss: 0.00001435
Iteration 273/1000 | Loss: 0.00001435
Iteration 274/1000 | Loss: 0.00001435
Iteration 275/1000 | Loss: 0.00001435
Iteration 276/1000 | Loss: 0.00001435
Iteration 277/1000 | Loss: 0.00001435
Iteration 278/1000 | Loss: 0.00001435
Iteration 279/1000 | Loss: 0.00001435
Iteration 280/1000 | Loss: 0.00001435
Iteration 281/1000 | Loss: 0.00001435
Iteration 282/1000 | Loss: 0.00001435
Iteration 283/1000 | Loss: 0.00001435
Iteration 284/1000 | Loss: 0.00001435
Iteration 285/1000 | Loss: 0.00001435
Iteration 286/1000 | Loss: 0.00001435
Iteration 287/1000 | Loss: 0.00001435
Iteration 288/1000 | Loss: 0.00001435
Iteration 289/1000 | Loss: 0.00001435
Iteration 290/1000 | Loss: 0.00001435
Iteration 291/1000 | Loss: 0.00001435
Iteration 292/1000 | Loss: 0.00001435
Iteration 293/1000 | Loss: 0.00001435
Iteration 294/1000 | Loss: 0.00001435
Iteration 295/1000 | Loss: 0.00001435
Iteration 296/1000 | Loss: 0.00001435
Iteration 297/1000 | Loss: 0.00001435
Iteration 298/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [1.4348232070915401e-05, 1.4348232070915401e-05, 1.4348232070915401e-05, 1.4348232070915401e-05, 1.4348232070915401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4348232070915401e-05

Optimization complete. Final v2v error: 3.208493232727051 mm

Highest mean error: 4.749592304229736 mm for frame 151

Lowest mean error: 2.3034589290618896 mm for frame 166

Saving results

Total time: 54.527037143707275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008677
Iteration 2/25 | Loss: 0.00241906
Iteration 3/25 | Loss: 0.00309482
Iteration 4/25 | Loss: 0.00234054
Iteration 5/25 | Loss: 0.00194286
Iteration 6/25 | Loss: 0.00178183
Iteration 7/25 | Loss: 0.00146256
Iteration 8/25 | Loss: 0.00144374
Iteration 9/25 | Loss: 0.00141529
Iteration 10/25 | Loss: 0.00136763
Iteration 11/25 | Loss: 0.00133649
Iteration 12/25 | Loss: 0.00132256
Iteration 13/25 | Loss: 0.00131646
Iteration 14/25 | Loss: 0.00131574
Iteration 15/25 | Loss: 0.00131249
Iteration 16/25 | Loss: 0.00131143
Iteration 17/25 | Loss: 0.00130468
Iteration 18/25 | Loss: 0.00130342
Iteration 19/25 | Loss: 0.00130335
Iteration 20/25 | Loss: 0.00130335
Iteration 21/25 | Loss: 0.00130335
Iteration 22/25 | Loss: 0.00130334
Iteration 23/25 | Loss: 0.00130334
Iteration 24/25 | Loss: 0.00130334
Iteration 25/25 | Loss: 0.00130334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30661070
Iteration 2/25 | Loss: 0.00221743
Iteration 3/25 | Loss: 0.00221743
Iteration 4/25 | Loss: 0.00221743
Iteration 5/25 | Loss: 0.00221743
Iteration 6/25 | Loss: 0.00221743
Iteration 7/25 | Loss: 0.00221743
Iteration 8/25 | Loss: 0.00221743
Iteration 9/25 | Loss: 0.00221743
Iteration 10/25 | Loss: 0.00221743
Iteration 11/25 | Loss: 0.00221743
Iteration 12/25 | Loss: 0.00221743
Iteration 13/25 | Loss: 0.00221743
Iteration 14/25 | Loss: 0.00221743
Iteration 15/25 | Loss: 0.00221743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022174252662807703, 0.0022174252662807703, 0.0022174252662807703, 0.0022174252662807703, 0.0022174252662807703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022174252662807703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221743
Iteration 2/1000 | Loss: 0.00005466
Iteration 3/1000 | Loss: 0.00077192
Iteration 4/1000 | Loss: 0.00011544
Iteration 5/1000 | Loss: 0.00012253
Iteration 6/1000 | Loss: 0.00011671
Iteration 7/1000 | Loss: 0.00012362
Iteration 8/1000 | Loss: 0.00005047
Iteration 9/1000 | Loss: 0.00011453
Iteration 10/1000 | Loss: 0.00012706
Iteration 11/1000 | Loss: 0.00012946
Iteration 12/1000 | Loss: 0.00012630
Iteration 13/1000 | Loss: 0.00011858
Iteration 14/1000 | Loss: 0.00006087
Iteration 15/1000 | Loss: 0.00007136
Iteration 16/1000 | Loss: 0.00009107
Iteration 17/1000 | Loss: 0.00003375
Iteration 18/1000 | Loss: 0.00011394
Iteration 19/1000 | Loss: 0.00006412
Iteration 20/1000 | Loss: 0.00008052
Iteration 21/1000 | Loss: 0.00006461
Iteration 22/1000 | Loss: 0.00007125
Iteration 23/1000 | Loss: 0.00009708
Iteration 24/1000 | Loss: 0.00008206
Iteration 25/1000 | Loss: 0.00150325
Iteration 26/1000 | Loss: 0.00025234
Iteration 27/1000 | Loss: 0.00014683
Iteration 28/1000 | Loss: 0.00010157
Iteration 29/1000 | Loss: 0.00009443
Iteration 30/1000 | Loss: 0.00005576
Iteration 31/1000 | Loss: 0.00004768
Iteration 32/1000 | Loss: 0.00004019
Iteration 33/1000 | Loss: 0.00006459
Iteration 34/1000 | Loss: 0.00011641
Iteration 35/1000 | Loss: 0.00009730
Iteration 36/1000 | Loss: 0.00011413
Iteration 37/1000 | Loss: 0.00008020
Iteration 38/1000 | Loss: 0.00003365
Iteration 39/1000 | Loss: 0.00003980
Iteration 40/1000 | Loss: 0.00011809
Iteration 41/1000 | Loss: 0.00012524
Iteration 42/1000 | Loss: 0.00017210
Iteration 43/1000 | Loss: 0.00004423
Iteration 44/1000 | Loss: 0.00011950
Iteration 45/1000 | Loss: 0.00013279
Iteration 46/1000 | Loss: 0.00009863
Iteration 47/1000 | Loss: 0.00013163
Iteration 48/1000 | Loss: 0.00009271
Iteration 49/1000 | Loss: 0.00007769
Iteration 50/1000 | Loss: 0.00015021
Iteration 51/1000 | Loss: 0.00006227
Iteration 52/1000 | Loss: 0.00006405
Iteration 53/1000 | Loss: 0.00007002
Iteration 54/1000 | Loss: 0.00020396
Iteration 55/1000 | Loss: 0.00007616
Iteration 56/1000 | Loss: 0.00022240
Iteration 57/1000 | Loss: 0.00013539
Iteration 58/1000 | Loss: 0.00018385
Iteration 59/1000 | Loss: 0.00015054
Iteration 60/1000 | Loss: 0.00017477
Iteration 61/1000 | Loss: 0.00014149
Iteration 62/1000 | Loss: 0.00003479
Iteration 63/1000 | Loss: 0.00002753
Iteration 64/1000 | Loss: 0.00003598
Iteration 65/1000 | Loss: 0.00003291
Iteration 66/1000 | Loss: 0.00002926
Iteration 67/1000 | Loss: 0.00003561
Iteration 68/1000 | Loss: 0.00003010
Iteration 69/1000 | Loss: 0.00002831
Iteration 70/1000 | Loss: 0.00002443
Iteration 71/1000 | Loss: 0.00002863
Iteration 72/1000 | Loss: 0.00002196
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002066
Iteration 75/1000 | Loss: 0.00002042
Iteration 76/1000 | Loss: 0.00002028
Iteration 77/1000 | Loss: 0.00002025
Iteration 78/1000 | Loss: 0.00002007
Iteration 79/1000 | Loss: 0.00001988
Iteration 80/1000 | Loss: 0.00001970
Iteration 81/1000 | Loss: 0.00001966
Iteration 82/1000 | Loss: 0.00001965
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001960
Iteration 93/1000 | Loss: 0.00001960
Iteration 94/1000 | Loss: 0.00001959
Iteration 95/1000 | Loss: 0.00001959
Iteration 96/1000 | Loss: 0.00001954
Iteration 97/1000 | Loss: 0.00001954
Iteration 98/1000 | Loss: 0.00001953
Iteration 99/1000 | Loss: 0.00001953
Iteration 100/1000 | Loss: 0.00001952
Iteration 101/1000 | Loss: 0.00001952
Iteration 102/1000 | Loss: 0.00001952
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00001951
Iteration 105/1000 | Loss: 0.00001951
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00003526
Iteration 108/1000 | Loss: 0.00002262
Iteration 109/1000 | Loss: 0.00002879
Iteration 110/1000 | Loss: 0.00002040
Iteration 111/1000 | Loss: 0.00003438
Iteration 112/1000 | Loss: 0.00002725
Iteration 113/1000 | Loss: 0.00003459
Iteration 114/1000 | Loss: 0.00002143
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002008
Iteration 117/1000 | Loss: 0.00001954
Iteration 118/1000 | Loss: 0.00001933
Iteration 119/1000 | Loss: 0.00001922
Iteration 120/1000 | Loss: 0.00001920
Iteration 121/1000 | Loss: 0.00001919
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001915
Iteration 124/1000 | Loss: 0.00001914
Iteration 125/1000 | Loss: 0.00001913
Iteration 126/1000 | Loss: 0.00001912
Iteration 127/1000 | Loss: 0.00001912
Iteration 128/1000 | Loss: 0.00001912
Iteration 129/1000 | Loss: 0.00001912
Iteration 130/1000 | Loss: 0.00001912
Iteration 131/1000 | Loss: 0.00001912
Iteration 132/1000 | Loss: 0.00001912
Iteration 133/1000 | Loss: 0.00001911
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001911
Iteration 136/1000 | Loss: 0.00001911
Iteration 137/1000 | Loss: 0.00001911
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001910
Iteration 140/1000 | Loss: 0.00001910
Iteration 141/1000 | Loss: 0.00001910
Iteration 142/1000 | Loss: 0.00001910
Iteration 143/1000 | Loss: 0.00001910
Iteration 144/1000 | Loss: 0.00001910
Iteration 145/1000 | Loss: 0.00001909
Iteration 146/1000 | Loss: 0.00001909
Iteration 147/1000 | Loss: 0.00001909
Iteration 148/1000 | Loss: 0.00001908
Iteration 149/1000 | Loss: 0.00001908
Iteration 150/1000 | Loss: 0.00001908
Iteration 151/1000 | Loss: 0.00001908
Iteration 152/1000 | Loss: 0.00001908
Iteration 153/1000 | Loss: 0.00001908
Iteration 154/1000 | Loss: 0.00001908
Iteration 155/1000 | Loss: 0.00001908
Iteration 156/1000 | Loss: 0.00001908
Iteration 157/1000 | Loss: 0.00001908
Iteration 158/1000 | Loss: 0.00001907
Iteration 159/1000 | Loss: 0.00001907
Iteration 160/1000 | Loss: 0.00001907
Iteration 161/1000 | Loss: 0.00001907
Iteration 162/1000 | Loss: 0.00001907
Iteration 163/1000 | Loss: 0.00001907
Iteration 164/1000 | Loss: 0.00001907
Iteration 165/1000 | Loss: 0.00001907
Iteration 166/1000 | Loss: 0.00001906
Iteration 167/1000 | Loss: 0.00001906
Iteration 168/1000 | Loss: 0.00001906
Iteration 169/1000 | Loss: 0.00001906
Iteration 170/1000 | Loss: 0.00001906
Iteration 171/1000 | Loss: 0.00001906
Iteration 172/1000 | Loss: 0.00001905
Iteration 173/1000 | Loss: 0.00001905
Iteration 174/1000 | Loss: 0.00001905
Iteration 175/1000 | Loss: 0.00001905
Iteration 176/1000 | Loss: 0.00001905
Iteration 177/1000 | Loss: 0.00001905
Iteration 178/1000 | Loss: 0.00001905
Iteration 179/1000 | Loss: 0.00001905
Iteration 180/1000 | Loss: 0.00001905
Iteration 181/1000 | Loss: 0.00001905
Iteration 182/1000 | Loss: 0.00001905
Iteration 183/1000 | Loss: 0.00001905
Iteration 184/1000 | Loss: 0.00001905
Iteration 185/1000 | Loss: 0.00001905
Iteration 186/1000 | Loss: 0.00001905
Iteration 187/1000 | Loss: 0.00001905
Iteration 188/1000 | Loss: 0.00001905
Iteration 189/1000 | Loss: 0.00001905
Iteration 190/1000 | Loss: 0.00001905
Iteration 191/1000 | Loss: 0.00001905
Iteration 192/1000 | Loss: 0.00001905
Iteration 193/1000 | Loss: 0.00001905
Iteration 194/1000 | Loss: 0.00001905
Iteration 195/1000 | Loss: 0.00001905
Iteration 196/1000 | Loss: 0.00001905
Iteration 197/1000 | Loss: 0.00001905
Iteration 198/1000 | Loss: 0.00001905
Iteration 199/1000 | Loss: 0.00001905
Iteration 200/1000 | Loss: 0.00001905
Iteration 201/1000 | Loss: 0.00001905
Iteration 202/1000 | Loss: 0.00001905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.904714372358285e-05, 1.904714372358285e-05, 1.904714372358285e-05, 1.904714372358285e-05, 1.904714372358285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.904714372358285e-05

Optimization complete. Final v2v error: 3.5522446632385254 mm

Highest mean error: 10.462526321411133 mm for frame 57

Lowest mean error: 2.9779820442199707 mm for frame 170

Saving results

Total time: 194.9903485774994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388641
Iteration 2/25 | Loss: 0.00141296
Iteration 3/25 | Loss: 0.00126235
Iteration 4/25 | Loss: 0.00124191
Iteration 5/25 | Loss: 0.00123499
Iteration 6/25 | Loss: 0.00123348
Iteration 7/25 | Loss: 0.00123320
Iteration 8/25 | Loss: 0.00123320
Iteration 9/25 | Loss: 0.00123320
Iteration 10/25 | Loss: 0.00123320
Iteration 11/25 | Loss: 0.00123320
Iteration 12/25 | Loss: 0.00123320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012332011247053742, 0.0012332011247053742, 0.0012332011247053742, 0.0012332011247053742, 0.0012332011247053742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012332011247053742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32063615
Iteration 2/25 | Loss: 0.00268487
Iteration 3/25 | Loss: 0.00268487
Iteration 4/25 | Loss: 0.00268487
Iteration 5/25 | Loss: 0.00268487
Iteration 6/25 | Loss: 0.00268487
Iteration 7/25 | Loss: 0.00268487
Iteration 8/25 | Loss: 0.00268487
Iteration 9/25 | Loss: 0.00268487
Iteration 10/25 | Loss: 0.00268487
Iteration 11/25 | Loss: 0.00268487
Iteration 12/25 | Loss: 0.00268487
Iteration 13/25 | Loss: 0.00268487
Iteration 14/25 | Loss: 0.00268487
Iteration 15/25 | Loss: 0.00268487
Iteration 16/25 | Loss: 0.00268487
Iteration 17/25 | Loss: 0.00268486
Iteration 18/25 | Loss: 0.00268486
Iteration 19/25 | Loss: 0.00268486
Iteration 20/25 | Loss: 0.00268486
Iteration 21/25 | Loss: 0.00268486
Iteration 22/25 | Loss: 0.00268486
Iteration 23/25 | Loss: 0.00268486
Iteration 24/25 | Loss: 0.00268486
Iteration 25/25 | Loss: 0.00268486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268486
Iteration 2/1000 | Loss: 0.00002706
Iteration 3/1000 | Loss: 0.00002021
Iteration 4/1000 | Loss: 0.00001829
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001626
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001587
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001578
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001575
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001573
Iteration 23/1000 | Loss: 0.00001572
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001568
Iteration 26/1000 | Loss: 0.00001568
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001559
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001559
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001554
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001552
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001549
Iteration 55/1000 | Loss: 0.00001549
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001541
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001540
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001535
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001535
Iteration 90/1000 | Loss: 0.00001535
Iteration 91/1000 | Loss: 0.00001535
Iteration 92/1000 | Loss: 0.00001535
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001534
Iteration 98/1000 | Loss: 0.00001534
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001531
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001528
Iteration 114/1000 | Loss: 0.00001528
Iteration 115/1000 | Loss: 0.00001527
Iteration 116/1000 | Loss: 0.00001527
Iteration 117/1000 | Loss: 0.00001527
Iteration 118/1000 | Loss: 0.00001527
Iteration 119/1000 | Loss: 0.00001527
Iteration 120/1000 | Loss: 0.00001526
Iteration 121/1000 | Loss: 0.00001526
Iteration 122/1000 | Loss: 0.00001526
Iteration 123/1000 | Loss: 0.00001526
Iteration 124/1000 | Loss: 0.00001526
Iteration 125/1000 | Loss: 0.00001525
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001524
Iteration 131/1000 | Loss: 0.00001524
Iteration 132/1000 | Loss: 0.00001524
Iteration 133/1000 | Loss: 0.00001524
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001522
Iteration 139/1000 | Loss: 0.00001522
Iteration 140/1000 | Loss: 0.00001522
Iteration 141/1000 | Loss: 0.00001522
Iteration 142/1000 | Loss: 0.00001522
Iteration 143/1000 | Loss: 0.00001522
Iteration 144/1000 | Loss: 0.00001522
Iteration 145/1000 | Loss: 0.00001522
Iteration 146/1000 | Loss: 0.00001521
Iteration 147/1000 | Loss: 0.00001521
Iteration 148/1000 | Loss: 0.00001521
Iteration 149/1000 | Loss: 0.00001521
Iteration 150/1000 | Loss: 0.00001521
Iteration 151/1000 | Loss: 0.00001520
Iteration 152/1000 | Loss: 0.00001520
Iteration 153/1000 | Loss: 0.00001520
Iteration 154/1000 | Loss: 0.00001520
Iteration 155/1000 | Loss: 0.00001520
Iteration 156/1000 | Loss: 0.00001520
Iteration 157/1000 | Loss: 0.00001520
Iteration 158/1000 | Loss: 0.00001520
Iteration 159/1000 | Loss: 0.00001519
Iteration 160/1000 | Loss: 0.00001519
Iteration 161/1000 | Loss: 0.00001519
Iteration 162/1000 | Loss: 0.00001519
Iteration 163/1000 | Loss: 0.00001519
Iteration 164/1000 | Loss: 0.00001519
Iteration 165/1000 | Loss: 0.00001519
Iteration 166/1000 | Loss: 0.00001519
Iteration 167/1000 | Loss: 0.00001519
Iteration 168/1000 | Loss: 0.00001519
Iteration 169/1000 | Loss: 0.00001519
Iteration 170/1000 | Loss: 0.00001518
Iteration 171/1000 | Loss: 0.00001518
Iteration 172/1000 | Loss: 0.00001518
Iteration 173/1000 | Loss: 0.00001518
Iteration 174/1000 | Loss: 0.00001518
Iteration 175/1000 | Loss: 0.00001518
Iteration 176/1000 | Loss: 0.00001518
Iteration 177/1000 | Loss: 0.00001518
Iteration 178/1000 | Loss: 0.00001518
Iteration 179/1000 | Loss: 0.00001518
Iteration 180/1000 | Loss: 0.00001518
Iteration 181/1000 | Loss: 0.00001517
Iteration 182/1000 | Loss: 0.00001517
Iteration 183/1000 | Loss: 0.00001517
Iteration 184/1000 | Loss: 0.00001517
Iteration 185/1000 | Loss: 0.00001517
Iteration 186/1000 | Loss: 0.00001517
Iteration 187/1000 | Loss: 0.00001517
Iteration 188/1000 | Loss: 0.00001517
Iteration 189/1000 | Loss: 0.00001517
Iteration 190/1000 | Loss: 0.00001517
Iteration 191/1000 | Loss: 0.00001516
Iteration 192/1000 | Loss: 0.00001516
Iteration 193/1000 | Loss: 0.00001516
Iteration 194/1000 | Loss: 0.00001516
Iteration 195/1000 | Loss: 0.00001516
Iteration 196/1000 | Loss: 0.00001516
Iteration 197/1000 | Loss: 0.00001516
Iteration 198/1000 | Loss: 0.00001516
Iteration 199/1000 | Loss: 0.00001516
Iteration 200/1000 | Loss: 0.00001516
Iteration 201/1000 | Loss: 0.00001516
Iteration 202/1000 | Loss: 0.00001516
Iteration 203/1000 | Loss: 0.00001516
Iteration 204/1000 | Loss: 0.00001516
Iteration 205/1000 | Loss: 0.00001516
Iteration 206/1000 | Loss: 0.00001516
Iteration 207/1000 | Loss: 0.00001516
Iteration 208/1000 | Loss: 0.00001516
Iteration 209/1000 | Loss: 0.00001516
Iteration 210/1000 | Loss: 0.00001516
Iteration 211/1000 | Loss: 0.00001516
Iteration 212/1000 | Loss: 0.00001516
Iteration 213/1000 | Loss: 0.00001516
Iteration 214/1000 | Loss: 0.00001516
Iteration 215/1000 | Loss: 0.00001516
Iteration 216/1000 | Loss: 0.00001516
Iteration 217/1000 | Loss: 0.00001516
Iteration 218/1000 | Loss: 0.00001516
Iteration 219/1000 | Loss: 0.00001516
Iteration 220/1000 | Loss: 0.00001516
Iteration 221/1000 | Loss: 0.00001516
Iteration 222/1000 | Loss: 0.00001516
Iteration 223/1000 | Loss: 0.00001516
Iteration 224/1000 | Loss: 0.00001516
Iteration 225/1000 | Loss: 0.00001516
Iteration 226/1000 | Loss: 0.00001516
Iteration 227/1000 | Loss: 0.00001516
Iteration 228/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.5157541383814532e-05, 1.5157541383814532e-05, 1.5157541383814532e-05, 1.5157541383814532e-05, 1.5157541383814532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5157541383814532e-05

Optimization complete. Final v2v error: 3.2908401489257812 mm

Highest mean error: 3.6575405597686768 mm for frame 110

Lowest mean error: 2.7149171829223633 mm for frame 4

Saving results

Total time: 41.37246823310852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591349
Iteration 2/25 | Loss: 0.00157620
Iteration 3/25 | Loss: 0.00133405
Iteration 4/25 | Loss: 0.00130566
Iteration 5/25 | Loss: 0.00130196
Iteration 6/25 | Loss: 0.00130094
Iteration 7/25 | Loss: 0.00130078
Iteration 8/25 | Loss: 0.00130078
Iteration 9/25 | Loss: 0.00130078
Iteration 10/25 | Loss: 0.00130078
Iteration 11/25 | Loss: 0.00130078
Iteration 12/25 | Loss: 0.00130078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013007819652557373, 0.0013007819652557373, 0.0013007819652557373, 0.0013007819652557373, 0.0013007819652557373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013007819652557373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06575155
Iteration 2/25 | Loss: 0.00176125
Iteration 3/25 | Loss: 0.00176123
Iteration 4/25 | Loss: 0.00176123
Iteration 5/25 | Loss: 0.00176123
Iteration 6/25 | Loss: 0.00176123
Iteration 7/25 | Loss: 0.00176123
Iteration 8/25 | Loss: 0.00176123
Iteration 9/25 | Loss: 0.00176123
Iteration 10/25 | Loss: 0.00176123
Iteration 11/25 | Loss: 0.00176123
Iteration 12/25 | Loss: 0.00176123
Iteration 13/25 | Loss: 0.00176123
Iteration 14/25 | Loss: 0.00176123
Iteration 15/25 | Loss: 0.00176123
Iteration 16/25 | Loss: 0.00176123
Iteration 17/25 | Loss: 0.00176123
Iteration 18/25 | Loss: 0.00176123
Iteration 19/25 | Loss: 0.00176123
Iteration 20/25 | Loss: 0.00176123
Iteration 21/25 | Loss: 0.00176123
Iteration 22/25 | Loss: 0.00176123
Iteration 23/25 | Loss: 0.00176123
Iteration 24/25 | Loss: 0.00176123
Iteration 25/25 | Loss: 0.00176123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176123
Iteration 2/1000 | Loss: 0.00004293
Iteration 3/1000 | Loss: 0.00002644
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001605
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001561
Iteration 12/1000 | Loss: 0.00001548
Iteration 13/1000 | Loss: 0.00001545
Iteration 14/1000 | Loss: 0.00001541
Iteration 15/1000 | Loss: 0.00001541
Iteration 16/1000 | Loss: 0.00001539
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001534
Iteration 21/1000 | Loss: 0.00001533
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001521
Iteration 24/1000 | Loss: 0.00001519
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001514
Iteration 27/1000 | Loss: 0.00001511
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001505
Iteration 30/1000 | Loss: 0.00001505
Iteration 31/1000 | Loss: 0.00001501
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001494
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001487
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001487
Iteration 48/1000 | Loss: 0.00001487
Iteration 49/1000 | Loss: 0.00001486
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001484
Iteration 55/1000 | Loss: 0.00001484
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001483
Iteration 58/1000 | Loss: 0.00001483
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001481
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001478
Iteration 96/1000 | Loss: 0.00001478
Iteration 97/1000 | Loss: 0.00001478
Iteration 98/1000 | Loss: 0.00001478
Iteration 99/1000 | Loss: 0.00001478
Iteration 100/1000 | Loss: 0.00001478
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001478
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001478
Iteration 108/1000 | Loss: 0.00001478
Iteration 109/1000 | Loss: 0.00001478
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001478
Iteration 112/1000 | Loss: 0.00001478
Iteration 113/1000 | Loss: 0.00001478
Iteration 114/1000 | Loss: 0.00001478
Iteration 115/1000 | Loss: 0.00001478
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001478
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001478
Iteration 125/1000 | Loss: 0.00001478
Iteration 126/1000 | Loss: 0.00001478
Iteration 127/1000 | Loss: 0.00001478
Iteration 128/1000 | Loss: 0.00001478
Iteration 129/1000 | Loss: 0.00001478
Iteration 130/1000 | Loss: 0.00001478
Iteration 131/1000 | Loss: 0.00001478
Iteration 132/1000 | Loss: 0.00001478
Iteration 133/1000 | Loss: 0.00001478
Iteration 134/1000 | Loss: 0.00001478
Iteration 135/1000 | Loss: 0.00001478
Iteration 136/1000 | Loss: 0.00001478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.4776348507439252e-05, 1.4776348507439252e-05, 1.4776348507439252e-05, 1.4776348507439252e-05, 1.4776348507439252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4776348507439252e-05

Optimization complete. Final v2v error: 3.1363468170166016 mm

Highest mean error: 4.735004425048828 mm for frame 54

Lowest mean error: 2.7591588497161865 mm for frame 142

Saving results

Total time: 39.737175941467285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752290
Iteration 2/25 | Loss: 0.00173808
Iteration 3/25 | Loss: 0.00137909
Iteration 4/25 | Loss: 0.00126794
Iteration 5/25 | Loss: 0.00125361
Iteration 6/25 | Loss: 0.00123952
Iteration 7/25 | Loss: 0.00123363
Iteration 8/25 | Loss: 0.00123012
Iteration 9/25 | Loss: 0.00122864
Iteration 10/25 | Loss: 0.00122833
Iteration 11/25 | Loss: 0.00122815
Iteration 12/25 | Loss: 0.00122810
Iteration 13/25 | Loss: 0.00122810
Iteration 14/25 | Loss: 0.00122809
Iteration 15/25 | Loss: 0.00122809
Iteration 16/25 | Loss: 0.00122809
Iteration 17/25 | Loss: 0.00122809
Iteration 18/25 | Loss: 0.00122809
Iteration 19/25 | Loss: 0.00122809
Iteration 20/25 | Loss: 0.00122809
Iteration 21/25 | Loss: 0.00122809
Iteration 22/25 | Loss: 0.00122809
Iteration 23/25 | Loss: 0.00122809
Iteration 24/25 | Loss: 0.00122808
Iteration 25/25 | Loss: 0.00122808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06098604
Iteration 2/25 | Loss: 0.00204479
Iteration 3/25 | Loss: 0.00204479
Iteration 4/25 | Loss: 0.00204478
Iteration 5/25 | Loss: 0.00204318
Iteration 6/25 | Loss: 0.00204317
Iteration 7/25 | Loss: 0.00204317
Iteration 8/25 | Loss: 0.00204317
Iteration 9/25 | Loss: 0.00204317
Iteration 10/25 | Loss: 0.00204317
Iteration 11/25 | Loss: 0.00204317
Iteration 12/25 | Loss: 0.00204317
Iteration 13/25 | Loss: 0.00204317
Iteration 14/25 | Loss: 0.00204317
Iteration 15/25 | Loss: 0.00204317
Iteration 16/25 | Loss: 0.00204317
Iteration 17/25 | Loss: 0.00204317
Iteration 18/25 | Loss: 0.00204317
Iteration 19/25 | Loss: 0.00204317
Iteration 20/25 | Loss: 0.00204317
Iteration 21/25 | Loss: 0.00204317
Iteration 22/25 | Loss: 0.00204317
Iteration 23/25 | Loss: 0.00204317
Iteration 24/25 | Loss: 0.00204317
Iteration 25/25 | Loss: 0.00204317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204317
Iteration 2/1000 | Loss: 0.00035445
Iteration 3/1000 | Loss: 0.00023926
Iteration 4/1000 | Loss: 0.00002761
Iteration 5/1000 | Loss: 0.00002502
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00001949
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001673
Iteration 10/1000 | Loss: 0.00001597
Iteration 11/1000 | Loss: 0.00001542
Iteration 12/1000 | Loss: 0.00001488
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001443
Iteration 15/1000 | Loss: 0.00001424
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001411
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001368
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00017878
Iteration 25/1000 | Loss: 0.00001860
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001521
Iteration 28/1000 | Loss: 0.00001445
Iteration 29/1000 | Loss: 0.00018273
Iteration 30/1000 | Loss: 0.00018271
Iteration 31/1000 | Loss: 0.00002387
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001601
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001246
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00018445
Iteration 49/1000 | Loss: 0.00002198
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001519
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001390
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001274
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001160
Iteration 76/1000 | Loss: 0.00001159
Iteration 77/1000 | Loss: 0.00001159
Iteration 78/1000 | Loss: 0.00001159
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001158
Iteration 81/1000 | Loss: 0.00001158
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001156
Iteration 88/1000 | Loss: 0.00001156
Iteration 89/1000 | Loss: 0.00001156
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001155
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001149
Iteration 115/1000 | Loss: 0.00001149
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001146
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001145
Iteration 131/1000 | Loss: 0.00001145
Iteration 132/1000 | Loss: 0.00001145
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001144
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001143
Iteration 139/1000 | Loss: 0.00001143
Iteration 140/1000 | Loss: 0.00001143
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001142
Iteration 147/1000 | Loss: 0.00001142
Iteration 148/1000 | Loss: 0.00001142
Iteration 149/1000 | Loss: 0.00001142
Iteration 150/1000 | Loss: 0.00001141
Iteration 151/1000 | Loss: 0.00001141
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001140
Iteration 154/1000 | Loss: 0.00001140
Iteration 155/1000 | Loss: 0.00001140
Iteration 156/1000 | Loss: 0.00001140
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001138
Iteration 162/1000 | Loss: 0.00001138
Iteration 163/1000 | Loss: 0.00001138
Iteration 164/1000 | Loss: 0.00001138
Iteration 165/1000 | Loss: 0.00001138
Iteration 166/1000 | Loss: 0.00001138
Iteration 167/1000 | Loss: 0.00001138
Iteration 168/1000 | Loss: 0.00001138
Iteration 169/1000 | Loss: 0.00001137
Iteration 170/1000 | Loss: 0.00001137
Iteration 171/1000 | Loss: 0.00001137
Iteration 172/1000 | Loss: 0.00001137
Iteration 173/1000 | Loss: 0.00001137
Iteration 174/1000 | Loss: 0.00001137
Iteration 175/1000 | Loss: 0.00001137
Iteration 176/1000 | Loss: 0.00001136
Iteration 177/1000 | Loss: 0.00001136
Iteration 178/1000 | Loss: 0.00001135
Iteration 179/1000 | Loss: 0.00001135
Iteration 180/1000 | Loss: 0.00001135
Iteration 181/1000 | Loss: 0.00001134
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001124
Iteration 184/1000 | Loss: 0.00001280
Iteration 185/1000 | Loss: 0.00001118
Iteration 186/1000 | Loss: 0.00001115
Iteration 187/1000 | Loss: 0.00001115
Iteration 188/1000 | Loss: 0.00001115
Iteration 189/1000 | Loss: 0.00001114
Iteration 190/1000 | Loss: 0.00001114
Iteration 191/1000 | Loss: 0.00001114
Iteration 192/1000 | Loss: 0.00001114
Iteration 193/1000 | Loss: 0.00001114
Iteration 194/1000 | Loss: 0.00001114
Iteration 195/1000 | Loss: 0.00001114
Iteration 196/1000 | Loss: 0.00001114
Iteration 197/1000 | Loss: 0.00001114
Iteration 198/1000 | Loss: 0.00001114
Iteration 199/1000 | Loss: 0.00001114
Iteration 200/1000 | Loss: 0.00001113
Iteration 201/1000 | Loss: 0.00001113
Iteration 202/1000 | Loss: 0.00001112
Iteration 203/1000 | Loss: 0.00001112
Iteration 204/1000 | Loss: 0.00001112
Iteration 205/1000 | Loss: 0.00001112
Iteration 206/1000 | Loss: 0.00001111
Iteration 207/1000 | Loss: 0.00001103
Iteration 208/1000 | Loss: 0.00001101
Iteration 209/1000 | Loss: 0.00001100
Iteration 210/1000 | Loss: 0.00001099
Iteration 211/1000 | Loss: 0.00001099
Iteration 212/1000 | Loss: 0.00001099
Iteration 213/1000 | Loss: 0.00001099
Iteration 214/1000 | Loss: 0.00001098
Iteration 215/1000 | Loss: 0.00001098
Iteration 216/1000 | Loss: 0.00001098
Iteration 217/1000 | Loss: 0.00001097
Iteration 218/1000 | Loss: 0.00001097
Iteration 219/1000 | Loss: 0.00001096
Iteration 220/1000 | Loss: 0.00001096
Iteration 221/1000 | Loss: 0.00001095
Iteration 222/1000 | Loss: 0.00001095
Iteration 223/1000 | Loss: 0.00001094
Iteration 224/1000 | Loss: 0.00001094
Iteration 225/1000 | Loss: 0.00001094
Iteration 226/1000 | Loss: 0.00001094
Iteration 227/1000 | Loss: 0.00001093
Iteration 228/1000 | Loss: 0.00001093
Iteration 229/1000 | Loss: 0.00001092
Iteration 230/1000 | Loss: 0.00001091
Iteration 231/1000 | Loss: 0.00001091
Iteration 232/1000 | Loss: 0.00001090
Iteration 233/1000 | Loss: 0.00001090
Iteration 234/1000 | Loss: 0.00001089
Iteration 235/1000 | Loss: 0.00001089
Iteration 236/1000 | Loss: 0.00001089
Iteration 237/1000 | Loss: 0.00001089
Iteration 238/1000 | Loss: 0.00001089
Iteration 239/1000 | Loss: 0.00001089
Iteration 240/1000 | Loss: 0.00001089
Iteration 241/1000 | Loss: 0.00001089
Iteration 242/1000 | Loss: 0.00001088
Iteration 243/1000 | Loss: 0.00001088
Iteration 244/1000 | Loss: 0.00001088
Iteration 245/1000 | Loss: 0.00001088
Iteration 246/1000 | Loss: 0.00001088
Iteration 247/1000 | Loss: 0.00001088
Iteration 248/1000 | Loss: 0.00001088
Iteration 249/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.088388125936035e-05, 1.088388125936035e-05, 1.088388125936035e-05, 1.088388125936035e-05, 1.088388125936035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.088388125936035e-05

Optimization complete. Final v2v error: 2.815244436264038 mm

Highest mean error: 5.34209680557251 mm for frame 124

Lowest mean error: 2.512251138687134 mm for frame 207

Saving results

Total time: 125.36217832565308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998176
Iteration 2/25 | Loss: 0.00383520
Iteration 3/25 | Loss: 0.00231298
Iteration 4/25 | Loss: 0.00181292
Iteration 5/25 | Loss: 0.00177623
Iteration 6/25 | Loss: 0.00174704
Iteration 7/25 | Loss: 0.00173203
Iteration 8/25 | Loss: 0.00169334
Iteration 9/25 | Loss: 0.00166729
Iteration 10/25 | Loss: 0.00166224
Iteration 11/25 | Loss: 0.00166923
Iteration 12/25 | Loss: 0.00166043
Iteration 13/25 | Loss: 0.00165310
Iteration 14/25 | Loss: 0.00165169
Iteration 15/25 | Loss: 0.00164710
Iteration 16/25 | Loss: 0.00165081
Iteration 17/25 | Loss: 0.00164480
Iteration 18/25 | Loss: 0.00164195
Iteration 19/25 | Loss: 0.00163903
Iteration 20/25 | Loss: 0.00163896
Iteration 21/25 | Loss: 0.00163454
Iteration 22/25 | Loss: 0.00163371
Iteration 23/25 | Loss: 0.00163962
Iteration 24/25 | Loss: 0.00163727
Iteration 25/25 | Loss: 0.00163971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.85394144
Iteration 2/25 | Loss: 0.00630882
Iteration 3/25 | Loss: 0.00628354
Iteration 4/25 | Loss: 0.00628351
Iteration 5/25 | Loss: 0.00628350
Iteration 6/25 | Loss: 0.00628350
Iteration 7/25 | Loss: 0.00628350
Iteration 8/25 | Loss: 0.00628350
Iteration 9/25 | Loss: 0.00628350
Iteration 10/25 | Loss: 0.00628350
Iteration 11/25 | Loss: 0.00628350
Iteration 12/25 | Loss: 0.00628350
Iteration 13/25 | Loss: 0.00628350
Iteration 14/25 | Loss: 0.00628350
Iteration 15/25 | Loss: 0.00628350
Iteration 16/25 | Loss: 0.00628350
Iteration 17/25 | Loss: 0.00628350
Iteration 18/25 | Loss: 0.00628350
Iteration 19/25 | Loss: 0.00628350
Iteration 20/25 | Loss: 0.00628350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00628349743783474, 0.00628349743783474, 0.00628349743783474, 0.00628349743783474, 0.00628349743783474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00628349743783474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00628350
Iteration 2/1000 | Loss: 0.00146501
Iteration 3/1000 | Loss: 0.00089158
Iteration 4/1000 | Loss: 0.00191230
Iteration 5/1000 | Loss: 0.00223736
Iteration 6/1000 | Loss: 0.00111022
Iteration 7/1000 | Loss: 0.00021624
Iteration 8/1000 | Loss: 0.00185231
Iteration 9/1000 | Loss: 0.00028740
Iteration 10/1000 | Loss: 0.00080943
Iteration 11/1000 | Loss: 0.00044751
Iteration 12/1000 | Loss: 0.00029846
Iteration 13/1000 | Loss: 0.00042669
Iteration 14/1000 | Loss: 0.00009308
Iteration 15/1000 | Loss: 0.00060164
Iteration 16/1000 | Loss: 0.00008679
Iteration 17/1000 | Loss: 0.00007362
Iteration 18/1000 | Loss: 0.00006327
Iteration 19/1000 | Loss: 0.00033499
Iteration 20/1000 | Loss: 0.00017626
Iteration 21/1000 | Loss: 0.00024528
Iteration 22/1000 | Loss: 0.00006002
Iteration 23/1000 | Loss: 0.00005125
Iteration 24/1000 | Loss: 0.00004700
Iteration 25/1000 | Loss: 0.00004384
Iteration 26/1000 | Loss: 0.00023879
Iteration 27/1000 | Loss: 0.00049721
Iteration 28/1000 | Loss: 0.00029740
Iteration 29/1000 | Loss: 0.00025974
Iteration 30/1000 | Loss: 0.00013702
Iteration 31/1000 | Loss: 0.00004414
Iteration 32/1000 | Loss: 0.00003990
Iteration 33/1000 | Loss: 0.00003620
Iteration 34/1000 | Loss: 0.00003330
Iteration 35/1000 | Loss: 0.00003156
Iteration 36/1000 | Loss: 0.00019990
Iteration 37/1000 | Loss: 0.00027251
Iteration 38/1000 | Loss: 0.00034939
Iteration 39/1000 | Loss: 0.00018636
Iteration 40/1000 | Loss: 0.00011139
Iteration 41/1000 | Loss: 0.00003279
Iteration 42/1000 | Loss: 0.00023114
Iteration 43/1000 | Loss: 0.00010693
Iteration 44/1000 | Loss: 0.00012715
Iteration 45/1000 | Loss: 0.00004441
Iteration 46/1000 | Loss: 0.00009126
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002232
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00020211
Iteration 51/1000 | Loss: 0.00008884
Iteration 52/1000 | Loss: 0.00011758
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001708
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001697
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001694
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001693
Iteration 109/1000 | Loss: 0.00001692
Iteration 110/1000 | Loss: 0.00001692
Iteration 111/1000 | Loss: 0.00001692
Iteration 112/1000 | Loss: 0.00001692
Iteration 113/1000 | Loss: 0.00001691
Iteration 114/1000 | Loss: 0.00001691
Iteration 115/1000 | Loss: 0.00001691
Iteration 116/1000 | Loss: 0.00001691
Iteration 117/1000 | Loss: 0.00001691
Iteration 118/1000 | Loss: 0.00001691
Iteration 119/1000 | Loss: 0.00001691
Iteration 120/1000 | Loss: 0.00001691
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001690
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001689
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001688
Iteration 137/1000 | Loss: 0.00001688
Iteration 138/1000 | Loss: 0.00001688
Iteration 139/1000 | Loss: 0.00001688
Iteration 140/1000 | Loss: 0.00001688
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001687
Iteration 145/1000 | Loss: 0.00001687
Iteration 146/1000 | Loss: 0.00001687
Iteration 147/1000 | Loss: 0.00001687
Iteration 148/1000 | Loss: 0.00001687
Iteration 149/1000 | Loss: 0.00001687
Iteration 150/1000 | Loss: 0.00001687
Iteration 151/1000 | Loss: 0.00001687
Iteration 152/1000 | Loss: 0.00001687
Iteration 153/1000 | Loss: 0.00001687
Iteration 154/1000 | Loss: 0.00001687
Iteration 155/1000 | Loss: 0.00001686
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Iteration 160/1000 | Loss: 0.00001686
Iteration 161/1000 | Loss: 0.00001686
Iteration 162/1000 | Loss: 0.00001686
Iteration 163/1000 | Loss: 0.00001686
Iteration 164/1000 | Loss: 0.00001686
Iteration 165/1000 | Loss: 0.00001686
Iteration 166/1000 | Loss: 0.00001686
Iteration 167/1000 | Loss: 0.00001686
Iteration 168/1000 | Loss: 0.00001686
Iteration 169/1000 | Loss: 0.00001686
Iteration 170/1000 | Loss: 0.00001686
Iteration 171/1000 | Loss: 0.00001686
Iteration 172/1000 | Loss: 0.00001686
Iteration 173/1000 | Loss: 0.00001686
Iteration 174/1000 | Loss: 0.00001686
Iteration 175/1000 | Loss: 0.00001686
Iteration 176/1000 | Loss: 0.00001686
Iteration 177/1000 | Loss: 0.00001686
Iteration 178/1000 | Loss: 0.00001686
Iteration 179/1000 | Loss: 0.00001686
Iteration 180/1000 | Loss: 0.00001686
Iteration 181/1000 | Loss: 0.00001686
Iteration 182/1000 | Loss: 0.00001686
Iteration 183/1000 | Loss: 0.00001686
Iteration 184/1000 | Loss: 0.00001686
Iteration 185/1000 | Loss: 0.00001686
Iteration 186/1000 | Loss: 0.00001686
Iteration 187/1000 | Loss: 0.00001686
Iteration 188/1000 | Loss: 0.00001686
Iteration 189/1000 | Loss: 0.00001686
Iteration 190/1000 | Loss: 0.00001686
Iteration 191/1000 | Loss: 0.00001686
Iteration 192/1000 | Loss: 0.00001686
Iteration 193/1000 | Loss: 0.00001686
Iteration 194/1000 | Loss: 0.00001686
Iteration 195/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.6855792637215927e-05, 1.6855792637215927e-05, 1.6855792637215927e-05, 1.6855792637215927e-05, 1.6855792637215927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6855792637215927e-05

Optimization complete. Final v2v error: 3.298718214035034 mm

Highest mean error: 5.226853370666504 mm for frame 78

Lowest mean error: 2.578709840774536 mm for frame 4

Saving results

Total time: 144.86061000823975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513653
Iteration 2/25 | Loss: 0.00141552
Iteration 3/25 | Loss: 0.00128839
Iteration 4/25 | Loss: 0.00128348
Iteration 5/25 | Loss: 0.00128348
Iteration 6/25 | Loss: 0.00128348
Iteration 7/25 | Loss: 0.00128348
Iteration 8/25 | Loss: 0.00128348
Iteration 9/25 | Loss: 0.00128348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012834807857871056, 0.0012834807857871056, 0.0012834807857871056, 0.0012834807857871056, 0.0012834807857871056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012834807857871056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74375677
Iteration 2/25 | Loss: 0.00131181
Iteration 3/25 | Loss: 0.00131181
Iteration 4/25 | Loss: 0.00131181
Iteration 5/25 | Loss: 0.00131181
Iteration 6/25 | Loss: 0.00131181
Iteration 7/25 | Loss: 0.00131181
Iteration 8/25 | Loss: 0.00131181
Iteration 9/25 | Loss: 0.00131181
Iteration 10/25 | Loss: 0.00131181
Iteration 11/25 | Loss: 0.00131181
Iteration 12/25 | Loss: 0.00131181
Iteration 13/25 | Loss: 0.00131181
Iteration 14/25 | Loss: 0.00131181
Iteration 15/25 | Loss: 0.00131181
Iteration 16/25 | Loss: 0.00131181
Iteration 17/25 | Loss: 0.00131181
Iteration 18/25 | Loss: 0.00131181
Iteration 19/25 | Loss: 0.00131181
Iteration 20/25 | Loss: 0.00131181
Iteration 21/25 | Loss: 0.00131181
Iteration 22/25 | Loss: 0.00131181
Iteration 23/25 | Loss: 0.00131181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013118092902004719, 0.0013118092902004719, 0.0013118092902004719, 0.0013118092902004719, 0.0013118092902004719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013118092902004719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131181
Iteration 2/1000 | Loss: 0.00002223
Iteration 3/1000 | Loss: 0.00001666
Iteration 4/1000 | Loss: 0.00001531
Iteration 5/1000 | Loss: 0.00001476
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001424
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001362
Iteration 10/1000 | Loss: 0.00001328
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001302
Iteration 13/1000 | Loss: 0.00001282
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001235
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001233
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001232
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001227
Iteration 60/1000 | Loss: 0.00001226
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.2159806829004083e-05, 1.2159806829004083e-05, 1.2159806829004083e-05, 1.2159806829004083e-05, 1.2159806829004083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2159806829004083e-05

Optimization complete. Final v2v error: 2.9553606510162354 mm

Highest mean error: 3.082073211669922 mm for frame 0

Lowest mean error: 2.850126266479492 mm for frame 168

Saving results

Total time: 33.88629937171936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965221
Iteration 2/25 | Loss: 0.00428411
Iteration 3/25 | Loss: 0.00299082
Iteration 4/25 | Loss: 0.00241012
Iteration 5/25 | Loss: 0.00210551
Iteration 6/25 | Loss: 0.00200291
Iteration 7/25 | Loss: 0.00197331
Iteration 8/25 | Loss: 0.00195694
Iteration 9/25 | Loss: 0.00195348
Iteration 10/25 | Loss: 0.00195315
Iteration 11/25 | Loss: 0.00195327
Iteration 12/25 | Loss: 0.00194958
Iteration 13/25 | Loss: 0.00194718
Iteration 14/25 | Loss: 0.00194921
Iteration 15/25 | Loss: 0.00194172
Iteration 16/25 | Loss: 0.00194108
Iteration 17/25 | Loss: 0.00194035
Iteration 18/25 | Loss: 0.00193967
Iteration 19/25 | Loss: 0.00193967
Iteration 20/25 | Loss: 0.00193967
Iteration 21/25 | Loss: 0.00193967
Iteration 22/25 | Loss: 0.00193966
Iteration 23/25 | Loss: 0.00193966
Iteration 24/25 | Loss: 0.00193966
Iteration 25/25 | Loss: 0.00193966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16539323
Iteration 2/25 | Loss: 0.00592267
Iteration 3/25 | Loss: 0.00585232
Iteration 4/25 | Loss: 0.00585173
Iteration 5/25 | Loss: 0.00584395
Iteration 6/25 | Loss: 0.00584395
Iteration 7/25 | Loss: 0.00584395
Iteration 8/25 | Loss: 0.00584395
Iteration 9/25 | Loss: 0.00584395
Iteration 10/25 | Loss: 0.00584395
Iteration 11/25 | Loss: 0.00584395
Iteration 12/25 | Loss: 0.00584395
Iteration 13/25 | Loss: 0.00584395
Iteration 14/25 | Loss: 0.00584395
Iteration 15/25 | Loss: 0.00584395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005843950901180506, 0.005843950901180506, 0.005843950901180506, 0.005843950901180506, 0.005843950901180506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005843950901180506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00584395
Iteration 2/1000 | Loss: 0.00062725
Iteration 3/1000 | Loss: 0.00048191
Iteration 4/1000 | Loss: 0.00049997
Iteration 5/1000 | Loss: 0.00099784
Iteration 6/1000 | Loss: 0.00037700
Iteration 7/1000 | Loss: 0.00035641
Iteration 8/1000 | Loss: 0.00033755
Iteration 9/1000 | Loss: 0.00038039
Iteration 10/1000 | Loss: 0.00354671
Iteration 11/1000 | Loss: 0.00037500
Iteration 12/1000 | Loss: 0.00053086
Iteration 13/1000 | Loss: 0.00030073
Iteration 14/1000 | Loss: 0.00039459
Iteration 15/1000 | Loss: 0.02775470
Iteration 16/1000 | Loss: 0.00338809
Iteration 17/1000 | Loss: 0.00115616
Iteration 18/1000 | Loss: 0.00044055
Iteration 19/1000 | Loss: 0.00034188
Iteration 20/1000 | Loss: 0.00025707
Iteration 21/1000 | Loss: 0.00017020
Iteration 22/1000 | Loss: 0.00012475
Iteration 23/1000 | Loss: 0.00015387
Iteration 24/1000 | Loss: 0.00022206
Iteration 25/1000 | Loss: 0.00014620
Iteration 26/1000 | Loss: 0.00005863
Iteration 27/1000 | Loss: 0.00006586
Iteration 28/1000 | Loss: 0.00008979
Iteration 29/1000 | Loss: 0.00009479
Iteration 30/1000 | Loss: 0.00007272
Iteration 31/1000 | Loss: 0.00006769
Iteration 32/1000 | Loss: 0.00007025
Iteration 33/1000 | Loss: 0.00009326
Iteration 34/1000 | Loss: 0.00005288
Iteration 35/1000 | Loss: 0.00002644
Iteration 36/1000 | Loss: 0.00002192
Iteration 37/1000 | Loss: 0.00005287
Iteration 38/1000 | Loss: 0.00002102
Iteration 39/1000 | Loss: 0.00004334
Iteration 40/1000 | Loss: 0.00004261
Iteration 41/1000 | Loss: 0.00002197
Iteration 42/1000 | Loss: 0.00002096
Iteration 43/1000 | Loss: 0.00002786
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001803
Iteration 46/1000 | Loss: 0.00002250
Iteration 47/1000 | Loss: 0.00001578
Iteration 48/1000 | Loss: 0.00001554
Iteration 49/1000 | Loss: 0.00002644
Iteration 50/1000 | Loss: 0.00002120
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001534
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001595
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001545
Iteration 61/1000 | Loss: 0.00001532
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001519
Iteration 81/1000 | Loss: 0.00001519
Iteration 82/1000 | Loss: 0.00001519
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001517
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001517
Iteration 94/1000 | Loss: 0.00001517
Iteration 95/1000 | Loss: 0.00001517
Iteration 96/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.5169584003160708e-05, 1.5169584003160708e-05, 1.5169584003160708e-05, 1.5169584003160708e-05, 1.5169584003160708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5169584003160708e-05

Optimization complete. Final v2v error: 3.349173069000244 mm

Highest mean error: 3.5003435611724854 mm for frame 27

Lowest mean error: 2.874743938446045 mm for frame 137

Saving results

Total time: 112.39704370498657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377081
Iteration 2/25 | Loss: 0.00128970
Iteration 3/25 | Loss: 0.00124048
Iteration 4/25 | Loss: 0.00123340
Iteration 5/25 | Loss: 0.00123038
Iteration 6/25 | Loss: 0.00123035
Iteration 7/25 | Loss: 0.00123035
Iteration 8/25 | Loss: 0.00123035
Iteration 9/25 | Loss: 0.00123035
Iteration 10/25 | Loss: 0.00123035
Iteration 11/25 | Loss: 0.00123035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012303481344133615, 0.0012303481344133615, 0.0012303481344133615, 0.0012303481344133615, 0.0012303481344133615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012303481344133615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39483786
Iteration 2/25 | Loss: 0.00211144
Iteration 3/25 | Loss: 0.00211140
Iteration 4/25 | Loss: 0.00211140
Iteration 5/25 | Loss: 0.00211140
Iteration 6/25 | Loss: 0.00211140
Iteration 7/25 | Loss: 0.00211140
Iteration 8/25 | Loss: 0.00211140
Iteration 9/25 | Loss: 0.00211140
Iteration 10/25 | Loss: 0.00211140
Iteration 11/25 | Loss: 0.00211140
Iteration 12/25 | Loss: 0.00211140
Iteration 13/25 | Loss: 0.00211140
Iteration 14/25 | Loss: 0.00211140
Iteration 15/25 | Loss: 0.00211140
Iteration 16/25 | Loss: 0.00211140
Iteration 17/25 | Loss: 0.00211140
Iteration 18/25 | Loss: 0.00211140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00211139814928174, 0.00211139814928174, 0.00211139814928174, 0.00211139814928174, 0.00211139814928174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00211139814928174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211140
Iteration 2/1000 | Loss: 0.00002715
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001389
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001200
Iteration 7/1000 | Loss: 0.00001170
Iteration 8/1000 | Loss: 0.00001145
Iteration 9/1000 | Loss: 0.00001097
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001056
Iteration 12/1000 | Loss: 0.00001041
Iteration 13/1000 | Loss: 0.00001029
Iteration 14/1000 | Loss: 0.00001026
Iteration 15/1000 | Loss: 0.00001025
Iteration 16/1000 | Loss: 0.00001023
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001020
Iteration 19/1000 | Loss: 0.00001015
Iteration 20/1000 | Loss: 0.00001014
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001011
Iteration 23/1000 | Loss: 0.00001010
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001001
Iteration 27/1000 | Loss: 0.00001001
Iteration 28/1000 | Loss: 0.00001001
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00000996
Iteration 31/1000 | Loss: 0.00000996
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000995
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000994
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000992
Iteration 43/1000 | Loss: 0.00000992
Iteration 44/1000 | Loss: 0.00000992
Iteration 45/1000 | Loss: 0.00000991
Iteration 46/1000 | Loss: 0.00000991
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000991
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000991
Iteration 51/1000 | Loss: 0.00000991
Iteration 52/1000 | Loss: 0.00000991
Iteration 53/1000 | Loss: 0.00000990
Iteration 54/1000 | Loss: 0.00000990
Iteration 55/1000 | Loss: 0.00000990
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000989
Iteration 59/1000 | Loss: 0.00000989
Iteration 60/1000 | Loss: 0.00000988
Iteration 61/1000 | Loss: 0.00000988
Iteration 62/1000 | Loss: 0.00000986
Iteration 63/1000 | Loss: 0.00000986
Iteration 64/1000 | Loss: 0.00000986
Iteration 65/1000 | Loss: 0.00000986
Iteration 66/1000 | Loss: 0.00000985
Iteration 67/1000 | Loss: 0.00000985
Iteration 68/1000 | Loss: 0.00000984
Iteration 69/1000 | Loss: 0.00000984
Iteration 70/1000 | Loss: 0.00000984
Iteration 71/1000 | Loss: 0.00000983
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000983
Iteration 74/1000 | Loss: 0.00000982
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000981
Iteration 77/1000 | Loss: 0.00000981
Iteration 78/1000 | Loss: 0.00000981
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000979
Iteration 82/1000 | Loss: 0.00000979
Iteration 83/1000 | Loss: 0.00000979
Iteration 84/1000 | Loss: 0.00000978
Iteration 85/1000 | Loss: 0.00000978
Iteration 86/1000 | Loss: 0.00000978
Iteration 87/1000 | Loss: 0.00000978
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000977
Iteration 90/1000 | Loss: 0.00000976
Iteration 91/1000 | Loss: 0.00000975
Iteration 92/1000 | Loss: 0.00000975
Iteration 93/1000 | Loss: 0.00000975
Iteration 94/1000 | Loss: 0.00000975
Iteration 95/1000 | Loss: 0.00000975
Iteration 96/1000 | Loss: 0.00000974
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000973
Iteration 99/1000 | Loss: 0.00000973
Iteration 100/1000 | Loss: 0.00000973
Iteration 101/1000 | Loss: 0.00000972
Iteration 102/1000 | Loss: 0.00000972
Iteration 103/1000 | Loss: 0.00000971
Iteration 104/1000 | Loss: 0.00000971
Iteration 105/1000 | Loss: 0.00000971
Iteration 106/1000 | Loss: 0.00000970
Iteration 107/1000 | Loss: 0.00000970
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000969
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000968
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000966
Iteration 121/1000 | Loss: 0.00000966
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000965
Iteration 126/1000 | Loss: 0.00000965
Iteration 127/1000 | Loss: 0.00000965
Iteration 128/1000 | Loss: 0.00000964
Iteration 129/1000 | Loss: 0.00000964
Iteration 130/1000 | Loss: 0.00000963
Iteration 131/1000 | Loss: 0.00000963
Iteration 132/1000 | Loss: 0.00000962
Iteration 133/1000 | Loss: 0.00000962
Iteration 134/1000 | Loss: 0.00000962
Iteration 135/1000 | Loss: 0.00000962
Iteration 136/1000 | Loss: 0.00000962
Iteration 137/1000 | Loss: 0.00000961
Iteration 138/1000 | Loss: 0.00000961
Iteration 139/1000 | Loss: 0.00000961
Iteration 140/1000 | Loss: 0.00000961
Iteration 141/1000 | Loss: 0.00000960
Iteration 142/1000 | Loss: 0.00000960
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000958
Iteration 149/1000 | Loss: 0.00000958
Iteration 150/1000 | Loss: 0.00000958
Iteration 151/1000 | Loss: 0.00000958
Iteration 152/1000 | Loss: 0.00000958
Iteration 153/1000 | Loss: 0.00000957
Iteration 154/1000 | Loss: 0.00000957
Iteration 155/1000 | Loss: 0.00000956
Iteration 156/1000 | Loss: 0.00000956
Iteration 157/1000 | Loss: 0.00000956
Iteration 158/1000 | Loss: 0.00000956
Iteration 159/1000 | Loss: 0.00000956
Iteration 160/1000 | Loss: 0.00000955
Iteration 161/1000 | Loss: 0.00000955
Iteration 162/1000 | Loss: 0.00000955
Iteration 163/1000 | Loss: 0.00000955
Iteration 164/1000 | Loss: 0.00000955
Iteration 165/1000 | Loss: 0.00000955
Iteration 166/1000 | Loss: 0.00000955
Iteration 167/1000 | Loss: 0.00000955
Iteration 168/1000 | Loss: 0.00000955
Iteration 169/1000 | Loss: 0.00000954
Iteration 170/1000 | Loss: 0.00000954
Iteration 171/1000 | Loss: 0.00000954
Iteration 172/1000 | Loss: 0.00000954
Iteration 173/1000 | Loss: 0.00000954
Iteration 174/1000 | Loss: 0.00000954
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000953
Iteration 178/1000 | Loss: 0.00000953
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000953
Iteration 181/1000 | Loss: 0.00000953
Iteration 182/1000 | Loss: 0.00000953
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000953
Iteration 185/1000 | Loss: 0.00000953
Iteration 186/1000 | Loss: 0.00000952
Iteration 187/1000 | Loss: 0.00000952
Iteration 188/1000 | Loss: 0.00000952
Iteration 189/1000 | Loss: 0.00000952
Iteration 190/1000 | Loss: 0.00000952
Iteration 191/1000 | Loss: 0.00000952
Iteration 192/1000 | Loss: 0.00000952
Iteration 193/1000 | Loss: 0.00000952
Iteration 194/1000 | Loss: 0.00000952
Iteration 195/1000 | Loss: 0.00000952
Iteration 196/1000 | Loss: 0.00000952
Iteration 197/1000 | Loss: 0.00000952
Iteration 198/1000 | Loss: 0.00000952
Iteration 199/1000 | Loss: 0.00000952
Iteration 200/1000 | Loss: 0.00000952
Iteration 201/1000 | Loss: 0.00000952
Iteration 202/1000 | Loss: 0.00000951
Iteration 203/1000 | Loss: 0.00000951
Iteration 204/1000 | Loss: 0.00000951
Iteration 205/1000 | Loss: 0.00000951
Iteration 206/1000 | Loss: 0.00000951
Iteration 207/1000 | Loss: 0.00000950
Iteration 208/1000 | Loss: 0.00000950
Iteration 209/1000 | Loss: 0.00000950
Iteration 210/1000 | Loss: 0.00000950
Iteration 211/1000 | Loss: 0.00000950
Iteration 212/1000 | Loss: 0.00000950
Iteration 213/1000 | Loss: 0.00000950
Iteration 214/1000 | Loss: 0.00000950
Iteration 215/1000 | Loss: 0.00000950
Iteration 216/1000 | Loss: 0.00000950
Iteration 217/1000 | Loss: 0.00000950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.501077329332475e-06, 9.501077329332475e-06, 9.501077329332475e-06, 9.501077329332475e-06, 9.501077329332475e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.501077329332475e-06

Optimization complete. Final v2v error: 2.6482675075531006 mm

Highest mean error: 3.0738308429718018 mm for frame 129

Lowest mean error: 2.3380815982818604 mm for frame 154

Saving results

Total time: 50.799829959869385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400612
Iteration 2/25 | Loss: 0.00130913
Iteration 3/25 | Loss: 0.00123517
Iteration 4/25 | Loss: 0.00122459
Iteration 5/25 | Loss: 0.00122111
Iteration 6/25 | Loss: 0.00122022
Iteration 7/25 | Loss: 0.00122022
Iteration 8/25 | Loss: 0.00122022
Iteration 9/25 | Loss: 0.00122022
Iteration 10/25 | Loss: 0.00122022
Iteration 11/25 | Loss: 0.00122022
Iteration 12/25 | Loss: 0.00122022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012202202342450619, 0.0012202202342450619, 0.0012202202342450619, 0.0012202202342450619, 0.0012202202342450619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012202202342450619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74456608
Iteration 2/25 | Loss: 0.00216436
Iteration 3/25 | Loss: 0.00216436
Iteration 4/25 | Loss: 0.00216436
Iteration 5/25 | Loss: 0.00216436
Iteration 6/25 | Loss: 0.00216436
Iteration 7/25 | Loss: 0.00216436
Iteration 8/25 | Loss: 0.00216435
Iteration 9/25 | Loss: 0.00216435
Iteration 10/25 | Loss: 0.00216435
Iteration 11/25 | Loss: 0.00216435
Iteration 12/25 | Loss: 0.00216435
Iteration 13/25 | Loss: 0.00216435
Iteration 14/25 | Loss: 0.00216435
Iteration 15/25 | Loss: 0.00216435
Iteration 16/25 | Loss: 0.00216435
Iteration 17/25 | Loss: 0.00216435
Iteration 18/25 | Loss: 0.00216435
Iteration 19/25 | Loss: 0.00216435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021643538493663073, 0.0021643538493663073, 0.0021643538493663073, 0.0021643538493663073, 0.0021643538493663073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021643538493663073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216435
Iteration 2/1000 | Loss: 0.00002214
Iteration 3/1000 | Loss: 0.00001627
Iteration 4/1000 | Loss: 0.00001353
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001191
Iteration 7/1000 | Loss: 0.00001138
Iteration 8/1000 | Loss: 0.00001093
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001067
Iteration 11/1000 | Loss: 0.00001044
Iteration 12/1000 | Loss: 0.00001023
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001021
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001019
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001000
Iteration 20/1000 | Loss: 0.00000995
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000994
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000993
Iteration 25/1000 | Loss: 0.00000992
Iteration 26/1000 | Loss: 0.00000992
Iteration 27/1000 | Loss: 0.00000991
Iteration 28/1000 | Loss: 0.00000991
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000987
Iteration 32/1000 | Loss: 0.00000987
Iteration 33/1000 | Loss: 0.00000986
Iteration 34/1000 | Loss: 0.00000986
Iteration 35/1000 | Loss: 0.00000986
Iteration 36/1000 | Loss: 0.00000984
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000978
Iteration 40/1000 | Loss: 0.00000975
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000967
Iteration 43/1000 | Loss: 0.00000966
Iteration 44/1000 | Loss: 0.00000965
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000965
Iteration 47/1000 | Loss: 0.00000965
Iteration 48/1000 | Loss: 0.00000965
Iteration 49/1000 | Loss: 0.00000964
Iteration 50/1000 | Loss: 0.00000964
Iteration 51/1000 | Loss: 0.00000964
Iteration 52/1000 | Loss: 0.00000963
Iteration 53/1000 | Loss: 0.00000963
Iteration 54/1000 | Loss: 0.00000963
Iteration 55/1000 | Loss: 0.00000963
Iteration 56/1000 | Loss: 0.00000962
Iteration 57/1000 | Loss: 0.00000962
Iteration 58/1000 | Loss: 0.00000962
Iteration 59/1000 | Loss: 0.00000961
Iteration 60/1000 | Loss: 0.00000961
Iteration 61/1000 | Loss: 0.00000960
Iteration 62/1000 | Loss: 0.00000960
Iteration 63/1000 | Loss: 0.00000960
Iteration 64/1000 | Loss: 0.00000959
Iteration 65/1000 | Loss: 0.00000959
Iteration 66/1000 | Loss: 0.00000959
Iteration 67/1000 | Loss: 0.00000959
Iteration 68/1000 | Loss: 0.00000959
Iteration 69/1000 | Loss: 0.00000958
Iteration 70/1000 | Loss: 0.00000958
Iteration 71/1000 | Loss: 0.00000958
Iteration 72/1000 | Loss: 0.00000957
Iteration 73/1000 | Loss: 0.00000957
Iteration 74/1000 | Loss: 0.00000957
Iteration 75/1000 | Loss: 0.00000957
Iteration 76/1000 | Loss: 0.00000957
Iteration 77/1000 | Loss: 0.00000956
Iteration 78/1000 | Loss: 0.00000956
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000953
Iteration 84/1000 | Loss: 0.00000953
Iteration 85/1000 | Loss: 0.00000953
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000953
Iteration 88/1000 | Loss: 0.00000953
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000952
Iteration 91/1000 | Loss: 0.00000952
Iteration 92/1000 | Loss: 0.00000952
Iteration 93/1000 | Loss: 0.00000952
Iteration 94/1000 | Loss: 0.00000952
Iteration 95/1000 | Loss: 0.00000952
Iteration 96/1000 | Loss: 0.00000952
Iteration 97/1000 | Loss: 0.00000952
Iteration 98/1000 | Loss: 0.00000952
Iteration 99/1000 | Loss: 0.00000952
Iteration 100/1000 | Loss: 0.00000951
Iteration 101/1000 | Loss: 0.00000951
Iteration 102/1000 | Loss: 0.00000951
Iteration 103/1000 | Loss: 0.00000951
Iteration 104/1000 | Loss: 0.00000951
Iteration 105/1000 | Loss: 0.00000951
Iteration 106/1000 | Loss: 0.00000951
Iteration 107/1000 | Loss: 0.00000951
Iteration 108/1000 | Loss: 0.00000951
Iteration 109/1000 | Loss: 0.00000950
Iteration 110/1000 | Loss: 0.00000950
Iteration 111/1000 | Loss: 0.00000950
Iteration 112/1000 | Loss: 0.00000950
Iteration 113/1000 | Loss: 0.00000950
Iteration 114/1000 | Loss: 0.00000949
Iteration 115/1000 | Loss: 0.00000949
Iteration 116/1000 | Loss: 0.00000949
Iteration 117/1000 | Loss: 0.00000949
Iteration 118/1000 | Loss: 0.00000949
Iteration 119/1000 | Loss: 0.00000949
Iteration 120/1000 | Loss: 0.00000949
Iteration 121/1000 | Loss: 0.00000949
Iteration 122/1000 | Loss: 0.00000949
Iteration 123/1000 | Loss: 0.00000949
Iteration 124/1000 | Loss: 0.00000949
Iteration 125/1000 | Loss: 0.00000949
Iteration 126/1000 | Loss: 0.00000949
Iteration 127/1000 | Loss: 0.00000949
Iteration 128/1000 | Loss: 0.00000949
Iteration 129/1000 | Loss: 0.00000949
Iteration 130/1000 | Loss: 0.00000948
Iteration 131/1000 | Loss: 0.00000948
Iteration 132/1000 | Loss: 0.00000948
Iteration 133/1000 | Loss: 0.00000948
Iteration 134/1000 | Loss: 0.00000948
Iteration 135/1000 | Loss: 0.00000948
Iteration 136/1000 | Loss: 0.00000948
Iteration 137/1000 | Loss: 0.00000948
Iteration 138/1000 | Loss: 0.00000948
Iteration 139/1000 | Loss: 0.00000948
Iteration 140/1000 | Loss: 0.00000948
Iteration 141/1000 | Loss: 0.00000948
Iteration 142/1000 | Loss: 0.00000948
Iteration 143/1000 | Loss: 0.00000948
Iteration 144/1000 | Loss: 0.00000948
Iteration 145/1000 | Loss: 0.00000948
Iteration 146/1000 | Loss: 0.00000948
Iteration 147/1000 | Loss: 0.00000947
Iteration 148/1000 | Loss: 0.00000947
Iteration 149/1000 | Loss: 0.00000947
Iteration 150/1000 | Loss: 0.00000947
Iteration 151/1000 | Loss: 0.00000947
Iteration 152/1000 | Loss: 0.00000947
Iteration 153/1000 | Loss: 0.00000947
Iteration 154/1000 | Loss: 0.00000947
Iteration 155/1000 | Loss: 0.00000947
Iteration 156/1000 | Loss: 0.00000947
Iteration 157/1000 | Loss: 0.00000947
Iteration 158/1000 | Loss: 0.00000947
Iteration 159/1000 | Loss: 0.00000947
Iteration 160/1000 | Loss: 0.00000947
Iteration 161/1000 | Loss: 0.00000946
Iteration 162/1000 | Loss: 0.00000946
Iteration 163/1000 | Loss: 0.00000946
Iteration 164/1000 | Loss: 0.00000946
Iteration 165/1000 | Loss: 0.00000946
Iteration 166/1000 | Loss: 0.00000946
Iteration 167/1000 | Loss: 0.00000946
Iteration 168/1000 | Loss: 0.00000946
Iteration 169/1000 | Loss: 0.00000946
Iteration 170/1000 | Loss: 0.00000946
Iteration 171/1000 | Loss: 0.00000946
Iteration 172/1000 | Loss: 0.00000946
Iteration 173/1000 | Loss: 0.00000946
Iteration 174/1000 | Loss: 0.00000945
Iteration 175/1000 | Loss: 0.00000945
Iteration 176/1000 | Loss: 0.00000945
Iteration 177/1000 | Loss: 0.00000945
Iteration 178/1000 | Loss: 0.00000945
Iteration 179/1000 | Loss: 0.00000945
Iteration 180/1000 | Loss: 0.00000945
Iteration 181/1000 | Loss: 0.00000945
Iteration 182/1000 | Loss: 0.00000945
Iteration 183/1000 | Loss: 0.00000945
Iteration 184/1000 | Loss: 0.00000945
Iteration 185/1000 | Loss: 0.00000945
Iteration 186/1000 | Loss: 0.00000945
Iteration 187/1000 | Loss: 0.00000944
Iteration 188/1000 | Loss: 0.00000944
Iteration 189/1000 | Loss: 0.00000944
Iteration 190/1000 | Loss: 0.00000944
Iteration 191/1000 | Loss: 0.00000944
Iteration 192/1000 | Loss: 0.00000944
Iteration 193/1000 | Loss: 0.00000944
Iteration 194/1000 | Loss: 0.00000944
Iteration 195/1000 | Loss: 0.00000944
Iteration 196/1000 | Loss: 0.00000944
Iteration 197/1000 | Loss: 0.00000944
Iteration 198/1000 | Loss: 0.00000944
Iteration 199/1000 | Loss: 0.00000943
Iteration 200/1000 | Loss: 0.00000943
Iteration 201/1000 | Loss: 0.00000943
Iteration 202/1000 | Loss: 0.00000943
Iteration 203/1000 | Loss: 0.00000943
Iteration 204/1000 | Loss: 0.00000943
Iteration 205/1000 | Loss: 0.00000943
Iteration 206/1000 | Loss: 0.00000943
Iteration 207/1000 | Loss: 0.00000942
Iteration 208/1000 | Loss: 0.00000942
Iteration 209/1000 | Loss: 0.00000942
Iteration 210/1000 | Loss: 0.00000942
Iteration 211/1000 | Loss: 0.00000942
Iteration 212/1000 | Loss: 0.00000942
Iteration 213/1000 | Loss: 0.00000942
Iteration 214/1000 | Loss: 0.00000942
Iteration 215/1000 | Loss: 0.00000942
Iteration 216/1000 | Loss: 0.00000941
Iteration 217/1000 | Loss: 0.00000941
Iteration 218/1000 | Loss: 0.00000941
Iteration 219/1000 | Loss: 0.00000941
Iteration 220/1000 | Loss: 0.00000941
Iteration 221/1000 | Loss: 0.00000941
Iteration 222/1000 | Loss: 0.00000941
Iteration 223/1000 | Loss: 0.00000941
Iteration 224/1000 | Loss: 0.00000941
Iteration 225/1000 | Loss: 0.00000941
Iteration 226/1000 | Loss: 0.00000941
Iteration 227/1000 | Loss: 0.00000941
Iteration 228/1000 | Loss: 0.00000940
Iteration 229/1000 | Loss: 0.00000940
Iteration 230/1000 | Loss: 0.00000940
Iteration 231/1000 | Loss: 0.00000940
Iteration 232/1000 | Loss: 0.00000940
Iteration 233/1000 | Loss: 0.00000940
Iteration 234/1000 | Loss: 0.00000940
Iteration 235/1000 | Loss: 0.00000940
Iteration 236/1000 | Loss: 0.00000940
Iteration 237/1000 | Loss: 0.00000940
Iteration 238/1000 | Loss: 0.00000940
Iteration 239/1000 | Loss: 0.00000940
Iteration 240/1000 | Loss: 0.00000940
Iteration 241/1000 | Loss: 0.00000940
Iteration 242/1000 | Loss: 0.00000940
Iteration 243/1000 | Loss: 0.00000939
Iteration 244/1000 | Loss: 0.00000939
Iteration 245/1000 | Loss: 0.00000939
Iteration 246/1000 | Loss: 0.00000939
Iteration 247/1000 | Loss: 0.00000939
Iteration 248/1000 | Loss: 0.00000939
Iteration 249/1000 | Loss: 0.00000939
Iteration 250/1000 | Loss: 0.00000939
Iteration 251/1000 | Loss: 0.00000939
Iteration 252/1000 | Loss: 0.00000939
Iteration 253/1000 | Loss: 0.00000939
Iteration 254/1000 | Loss: 0.00000939
Iteration 255/1000 | Loss: 0.00000939
Iteration 256/1000 | Loss: 0.00000938
Iteration 257/1000 | Loss: 0.00000938
Iteration 258/1000 | Loss: 0.00000938
Iteration 259/1000 | Loss: 0.00000938
Iteration 260/1000 | Loss: 0.00000938
Iteration 261/1000 | Loss: 0.00000938
Iteration 262/1000 | Loss: 0.00000938
Iteration 263/1000 | Loss: 0.00000938
Iteration 264/1000 | Loss: 0.00000938
Iteration 265/1000 | Loss: 0.00000938
Iteration 266/1000 | Loss: 0.00000938
Iteration 267/1000 | Loss: 0.00000938
Iteration 268/1000 | Loss: 0.00000938
Iteration 269/1000 | Loss: 0.00000938
Iteration 270/1000 | Loss: 0.00000938
Iteration 271/1000 | Loss: 0.00000938
Iteration 272/1000 | Loss: 0.00000938
Iteration 273/1000 | Loss: 0.00000938
Iteration 274/1000 | Loss: 0.00000938
Iteration 275/1000 | Loss: 0.00000938
Iteration 276/1000 | Loss: 0.00000938
Iteration 277/1000 | Loss: 0.00000938
Iteration 278/1000 | Loss: 0.00000938
Iteration 279/1000 | Loss: 0.00000938
Iteration 280/1000 | Loss: 0.00000938
Iteration 281/1000 | Loss: 0.00000938
Iteration 282/1000 | Loss: 0.00000938
Iteration 283/1000 | Loss: 0.00000938
Iteration 284/1000 | Loss: 0.00000938
Iteration 285/1000 | Loss: 0.00000938
Iteration 286/1000 | Loss: 0.00000938
Iteration 287/1000 | Loss: 0.00000938
Iteration 288/1000 | Loss: 0.00000938
Iteration 289/1000 | Loss: 0.00000938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [9.379902621731162e-06, 9.379902621731162e-06, 9.379902621731162e-06, 9.379902621731162e-06, 9.379902621731162e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.379902621731162e-06

Optimization complete. Final v2v error: 2.654905319213867 mm

Highest mean error: 3.1538476943969727 mm for frame 55

Lowest mean error: 2.458979606628418 mm for frame 84

Saving results

Total time: 46.221853733062744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833621
Iteration 2/25 | Loss: 0.00133864
Iteration 3/25 | Loss: 0.00124311
Iteration 4/25 | Loss: 0.00123199
Iteration 5/25 | Loss: 0.00123053
Iteration 6/25 | Loss: 0.00123053
Iteration 7/25 | Loss: 0.00123053
Iteration 8/25 | Loss: 0.00123053
Iteration 9/25 | Loss: 0.00123053
Iteration 10/25 | Loss: 0.00123053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012305290438234806, 0.0012305290438234806, 0.0012305290438234806, 0.0012305290438234806, 0.0012305290438234806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012305290438234806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.85548735
Iteration 2/25 | Loss: 0.00199835
Iteration 3/25 | Loss: 0.00199835
Iteration 4/25 | Loss: 0.00199835
Iteration 5/25 | Loss: 0.00199835
Iteration 6/25 | Loss: 0.00199835
Iteration 7/25 | Loss: 0.00199835
Iteration 8/25 | Loss: 0.00199835
Iteration 9/25 | Loss: 0.00199835
Iteration 10/25 | Loss: 0.00199835
Iteration 11/25 | Loss: 0.00199835
Iteration 12/25 | Loss: 0.00199835
Iteration 13/25 | Loss: 0.00199835
Iteration 14/25 | Loss: 0.00199835
Iteration 15/25 | Loss: 0.00199835
Iteration 16/25 | Loss: 0.00199835
Iteration 17/25 | Loss: 0.00199835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019983467645943165, 0.0019983467645943165, 0.0019983467645943165, 0.0019983467645943165, 0.0019983467645943165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019983467645943165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199835
Iteration 2/1000 | Loss: 0.00002159
Iteration 3/1000 | Loss: 0.00001406
Iteration 4/1000 | Loss: 0.00001236
Iteration 5/1000 | Loss: 0.00001163
Iteration 6/1000 | Loss: 0.00001102
Iteration 7/1000 | Loss: 0.00001101
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001043
Iteration 10/1000 | Loss: 0.00001010
Iteration 11/1000 | Loss: 0.00000995
Iteration 12/1000 | Loss: 0.00000978
Iteration 13/1000 | Loss: 0.00000977
Iteration 14/1000 | Loss: 0.00000966
Iteration 15/1000 | Loss: 0.00000965
Iteration 16/1000 | Loss: 0.00000950
Iteration 17/1000 | Loss: 0.00000934
Iteration 18/1000 | Loss: 0.00000931
Iteration 19/1000 | Loss: 0.00000916
Iteration 20/1000 | Loss: 0.00000905
Iteration 21/1000 | Loss: 0.00000905
Iteration 22/1000 | Loss: 0.00000904
Iteration 23/1000 | Loss: 0.00000904
Iteration 24/1000 | Loss: 0.00000904
Iteration 25/1000 | Loss: 0.00000903
Iteration 26/1000 | Loss: 0.00000903
Iteration 27/1000 | Loss: 0.00000902
Iteration 28/1000 | Loss: 0.00000902
Iteration 29/1000 | Loss: 0.00000901
Iteration 30/1000 | Loss: 0.00000900
Iteration 31/1000 | Loss: 0.00000899
Iteration 32/1000 | Loss: 0.00000898
Iteration 33/1000 | Loss: 0.00000897
Iteration 34/1000 | Loss: 0.00000897
Iteration 35/1000 | Loss: 0.00000894
Iteration 36/1000 | Loss: 0.00000893
Iteration 37/1000 | Loss: 0.00000892
Iteration 38/1000 | Loss: 0.00000892
Iteration 39/1000 | Loss: 0.00000892
Iteration 40/1000 | Loss: 0.00000892
Iteration 41/1000 | Loss: 0.00000892
Iteration 42/1000 | Loss: 0.00000892
Iteration 43/1000 | Loss: 0.00000891
Iteration 44/1000 | Loss: 0.00000891
Iteration 45/1000 | Loss: 0.00000891
Iteration 46/1000 | Loss: 0.00000890
Iteration 47/1000 | Loss: 0.00000889
Iteration 48/1000 | Loss: 0.00000888
Iteration 49/1000 | Loss: 0.00000888
Iteration 50/1000 | Loss: 0.00000887
Iteration 51/1000 | Loss: 0.00000887
Iteration 52/1000 | Loss: 0.00000886
Iteration 53/1000 | Loss: 0.00000885
Iteration 54/1000 | Loss: 0.00000884
Iteration 55/1000 | Loss: 0.00000882
Iteration 56/1000 | Loss: 0.00000882
Iteration 57/1000 | Loss: 0.00000882
Iteration 58/1000 | Loss: 0.00000882
Iteration 59/1000 | Loss: 0.00000882
Iteration 60/1000 | Loss: 0.00000882
Iteration 61/1000 | Loss: 0.00000882
Iteration 62/1000 | Loss: 0.00000882
Iteration 63/1000 | Loss: 0.00000882
Iteration 64/1000 | Loss: 0.00000882
Iteration 65/1000 | Loss: 0.00000882
Iteration 66/1000 | Loss: 0.00000881
Iteration 67/1000 | Loss: 0.00000881
Iteration 68/1000 | Loss: 0.00000880
Iteration 69/1000 | Loss: 0.00000878
Iteration 70/1000 | Loss: 0.00000878
Iteration 71/1000 | Loss: 0.00000877
Iteration 72/1000 | Loss: 0.00000876
Iteration 73/1000 | Loss: 0.00000875
Iteration 74/1000 | Loss: 0.00000875
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000875
Iteration 77/1000 | Loss: 0.00000875
Iteration 78/1000 | Loss: 0.00000874
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000874
Iteration 81/1000 | Loss: 0.00000873
Iteration 82/1000 | Loss: 0.00000873
Iteration 83/1000 | Loss: 0.00000872
Iteration 84/1000 | Loss: 0.00000871
Iteration 85/1000 | Loss: 0.00000870
Iteration 86/1000 | Loss: 0.00000869
Iteration 87/1000 | Loss: 0.00000869
Iteration 88/1000 | Loss: 0.00000869
Iteration 89/1000 | Loss: 0.00000868
Iteration 90/1000 | Loss: 0.00000868
Iteration 91/1000 | Loss: 0.00000868
Iteration 92/1000 | Loss: 0.00000868
Iteration 93/1000 | Loss: 0.00000868
Iteration 94/1000 | Loss: 0.00000867
Iteration 95/1000 | Loss: 0.00000867
Iteration 96/1000 | Loss: 0.00000867
Iteration 97/1000 | Loss: 0.00000866
Iteration 98/1000 | Loss: 0.00000865
Iteration 99/1000 | Loss: 0.00000865
Iteration 100/1000 | Loss: 0.00000865
Iteration 101/1000 | Loss: 0.00000864
Iteration 102/1000 | Loss: 0.00000864
Iteration 103/1000 | Loss: 0.00000864
Iteration 104/1000 | Loss: 0.00000864
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000864
Iteration 107/1000 | Loss: 0.00000864
Iteration 108/1000 | Loss: 0.00000864
Iteration 109/1000 | Loss: 0.00000864
Iteration 110/1000 | Loss: 0.00000864
Iteration 111/1000 | Loss: 0.00000864
Iteration 112/1000 | Loss: 0.00000863
Iteration 113/1000 | Loss: 0.00000863
Iteration 114/1000 | Loss: 0.00000863
Iteration 115/1000 | Loss: 0.00000863
Iteration 116/1000 | Loss: 0.00000862
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000861
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000861
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000860
Iteration 131/1000 | Loss: 0.00000860
Iteration 132/1000 | Loss: 0.00000860
Iteration 133/1000 | Loss: 0.00000860
Iteration 134/1000 | Loss: 0.00000860
Iteration 135/1000 | Loss: 0.00000860
Iteration 136/1000 | Loss: 0.00000860
Iteration 137/1000 | Loss: 0.00000860
Iteration 138/1000 | Loss: 0.00000860
Iteration 139/1000 | Loss: 0.00000860
Iteration 140/1000 | Loss: 0.00000860
Iteration 141/1000 | Loss: 0.00000860
Iteration 142/1000 | Loss: 0.00000860
Iteration 143/1000 | Loss: 0.00000860
Iteration 144/1000 | Loss: 0.00000860
Iteration 145/1000 | Loss: 0.00000860
Iteration 146/1000 | Loss: 0.00000860
Iteration 147/1000 | Loss: 0.00000860
Iteration 148/1000 | Loss: 0.00000860
Iteration 149/1000 | Loss: 0.00000860
Iteration 150/1000 | Loss: 0.00000860
Iteration 151/1000 | Loss: 0.00000860
Iteration 152/1000 | Loss: 0.00000860
Iteration 153/1000 | Loss: 0.00000860
Iteration 154/1000 | Loss: 0.00000860
Iteration 155/1000 | Loss: 0.00000860
Iteration 156/1000 | Loss: 0.00000860
Iteration 157/1000 | Loss: 0.00000860
Iteration 158/1000 | Loss: 0.00000860
Iteration 159/1000 | Loss: 0.00000860
Iteration 160/1000 | Loss: 0.00000860
Iteration 161/1000 | Loss: 0.00000860
Iteration 162/1000 | Loss: 0.00000860
Iteration 163/1000 | Loss: 0.00000860
Iteration 164/1000 | Loss: 0.00000860
Iteration 165/1000 | Loss: 0.00000860
Iteration 166/1000 | Loss: 0.00000860
Iteration 167/1000 | Loss: 0.00000860
Iteration 168/1000 | Loss: 0.00000860
Iteration 169/1000 | Loss: 0.00000860
Iteration 170/1000 | Loss: 0.00000860
Iteration 171/1000 | Loss: 0.00000860
Iteration 172/1000 | Loss: 0.00000860
Iteration 173/1000 | Loss: 0.00000860
Iteration 174/1000 | Loss: 0.00000860
Iteration 175/1000 | Loss: 0.00000860
Iteration 176/1000 | Loss: 0.00000860
Iteration 177/1000 | Loss: 0.00000860
Iteration 178/1000 | Loss: 0.00000860
Iteration 179/1000 | Loss: 0.00000860
Iteration 180/1000 | Loss: 0.00000860
Iteration 181/1000 | Loss: 0.00000860
Iteration 182/1000 | Loss: 0.00000860
Iteration 183/1000 | Loss: 0.00000860
Iteration 184/1000 | Loss: 0.00000860
Iteration 185/1000 | Loss: 0.00000860
Iteration 186/1000 | Loss: 0.00000860
Iteration 187/1000 | Loss: 0.00000860
Iteration 188/1000 | Loss: 0.00000860
Iteration 189/1000 | Loss: 0.00000860
Iteration 190/1000 | Loss: 0.00000860
Iteration 191/1000 | Loss: 0.00000860
Iteration 192/1000 | Loss: 0.00000860
Iteration 193/1000 | Loss: 0.00000860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [8.603656169725582e-06, 8.603656169725582e-06, 8.603656169725582e-06, 8.603656169725582e-06, 8.603656169725582e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.603656169725582e-06

Optimization complete. Final v2v error: 2.5550098419189453 mm

Highest mean error: 2.813922643661499 mm for frame 234

Lowest mean error: 2.3490188121795654 mm for frame 94

Saving results

Total time: 42.572776794433594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603947
Iteration 2/25 | Loss: 0.00138538
Iteration 3/25 | Loss: 0.00127650
Iteration 4/25 | Loss: 0.00127003
Iteration 5/25 | Loss: 0.00126847
Iteration 6/25 | Loss: 0.00126847
Iteration 7/25 | Loss: 0.00126847
Iteration 8/25 | Loss: 0.00126847
Iteration 9/25 | Loss: 0.00126847
Iteration 10/25 | Loss: 0.00126847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001268472638912499, 0.001268472638912499, 0.001268472638912499, 0.001268472638912499, 0.001268472638912499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001268472638912499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96885419
Iteration 2/25 | Loss: 0.00206402
Iteration 3/25 | Loss: 0.00206396
Iteration 4/25 | Loss: 0.00206396
Iteration 5/25 | Loss: 0.00206396
Iteration 6/25 | Loss: 0.00206396
Iteration 7/25 | Loss: 0.00206396
Iteration 8/25 | Loss: 0.00206396
Iteration 9/25 | Loss: 0.00206396
Iteration 10/25 | Loss: 0.00206396
Iteration 11/25 | Loss: 0.00206396
Iteration 12/25 | Loss: 0.00206396
Iteration 13/25 | Loss: 0.00206396
Iteration 14/25 | Loss: 0.00206396
Iteration 15/25 | Loss: 0.00206396
Iteration 16/25 | Loss: 0.00206396
Iteration 17/25 | Loss: 0.00206396
Iteration 18/25 | Loss: 0.00206396
Iteration 19/25 | Loss: 0.00206396
Iteration 20/25 | Loss: 0.00206396
Iteration 21/25 | Loss: 0.00206396
Iteration 22/25 | Loss: 0.00206396
Iteration 23/25 | Loss: 0.00206396
Iteration 24/25 | Loss: 0.00206396
Iteration 25/25 | Loss: 0.00206396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206396
Iteration 2/1000 | Loss: 0.00002833
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001493
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001347
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001313
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001266
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001242
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001235
Iteration 20/1000 | Loss: 0.00001234
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001220
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001208
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001207
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001205
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001203
Iteration 64/1000 | Loss: 0.00001203
Iteration 65/1000 | Loss: 0.00001202
Iteration 66/1000 | Loss: 0.00001202
Iteration 67/1000 | Loss: 0.00001202
Iteration 68/1000 | Loss: 0.00001202
Iteration 69/1000 | Loss: 0.00001202
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001201
Iteration 74/1000 | Loss: 0.00001200
Iteration 75/1000 | Loss: 0.00001200
Iteration 76/1000 | Loss: 0.00001199
Iteration 77/1000 | Loss: 0.00001199
Iteration 78/1000 | Loss: 0.00001199
Iteration 79/1000 | Loss: 0.00001199
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001198
Iteration 82/1000 | Loss: 0.00001198
Iteration 83/1000 | Loss: 0.00001198
Iteration 84/1000 | Loss: 0.00001198
Iteration 85/1000 | Loss: 0.00001197
Iteration 86/1000 | Loss: 0.00001197
Iteration 87/1000 | Loss: 0.00001197
Iteration 88/1000 | Loss: 0.00001197
Iteration 89/1000 | Loss: 0.00001196
Iteration 90/1000 | Loss: 0.00001196
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001194
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001191
Iteration 102/1000 | Loss: 0.00001191
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001190
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001189
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001189
Iteration 120/1000 | Loss: 0.00001189
Iteration 121/1000 | Loss: 0.00001189
Iteration 122/1000 | Loss: 0.00001189
Iteration 123/1000 | Loss: 0.00001189
Iteration 124/1000 | Loss: 0.00001189
Iteration 125/1000 | Loss: 0.00001189
Iteration 126/1000 | Loss: 0.00001188
Iteration 127/1000 | Loss: 0.00001188
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001187
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Iteration 144/1000 | Loss: 0.00001186
Iteration 145/1000 | Loss: 0.00001186
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001186
Iteration 149/1000 | Loss: 0.00001186
Iteration 150/1000 | Loss: 0.00001186
Iteration 151/1000 | Loss: 0.00001186
Iteration 152/1000 | Loss: 0.00001186
Iteration 153/1000 | Loss: 0.00001186
Iteration 154/1000 | Loss: 0.00001186
Iteration 155/1000 | Loss: 0.00001186
Iteration 156/1000 | Loss: 0.00001186
Iteration 157/1000 | Loss: 0.00001186
Iteration 158/1000 | Loss: 0.00001186
Iteration 159/1000 | Loss: 0.00001186
Iteration 160/1000 | Loss: 0.00001186
Iteration 161/1000 | Loss: 0.00001186
Iteration 162/1000 | Loss: 0.00001186
Iteration 163/1000 | Loss: 0.00001186
Iteration 164/1000 | Loss: 0.00001186
Iteration 165/1000 | Loss: 0.00001186
Iteration 166/1000 | Loss: 0.00001186
Iteration 167/1000 | Loss: 0.00001186
Iteration 168/1000 | Loss: 0.00001186
Iteration 169/1000 | Loss: 0.00001186
Iteration 170/1000 | Loss: 0.00001186
Iteration 171/1000 | Loss: 0.00001186
Iteration 172/1000 | Loss: 0.00001186
Iteration 173/1000 | Loss: 0.00001186
Iteration 174/1000 | Loss: 0.00001186
Iteration 175/1000 | Loss: 0.00001186
Iteration 176/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.1855939192173537e-05, 1.1855939192173537e-05, 1.1855939192173537e-05, 1.1855939192173537e-05, 1.1855939192173537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1855939192173537e-05

Optimization complete. Final v2v error: 2.9357352256774902 mm

Highest mean error: 3.920769214630127 mm for frame 33

Lowest mean error: 2.5682456493377686 mm for frame 154

Saving results

Total time: 39.01728367805481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758947
Iteration 2/25 | Loss: 0.00189796
Iteration 3/25 | Loss: 0.00148212
Iteration 4/25 | Loss: 0.00141358
Iteration 5/25 | Loss: 0.00140030
Iteration 6/25 | Loss: 0.00139321
Iteration 7/25 | Loss: 0.00137158
Iteration 8/25 | Loss: 0.00135219
Iteration 9/25 | Loss: 0.00133574
Iteration 10/25 | Loss: 0.00133139
Iteration 11/25 | Loss: 0.00133035
Iteration 12/25 | Loss: 0.00132975
Iteration 13/25 | Loss: 0.00132811
Iteration 14/25 | Loss: 0.00132981
Iteration 15/25 | Loss: 0.00132976
Iteration 16/25 | Loss: 0.00132935
Iteration 17/25 | Loss: 0.00132979
Iteration 18/25 | Loss: 0.00132812
Iteration 19/25 | Loss: 0.00132681
Iteration 20/25 | Loss: 0.00132874
Iteration 21/25 | Loss: 0.00132710
Iteration 22/25 | Loss: 0.00132599
Iteration 23/25 | Loss: 0.00132546
Iteration 24/25 | Loss: 0.00132531
Iteration 25/25 | Loss: 0.00132530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19592595
Iteration 2/25 | Loss: 0.00195644
Iteration 3/25 | Loss: 0.00195643
Iteration 4/25 | Loss: 0.00195643
Iteration 5/25 | Loss: 0.00195643
Iteration 6/25 | Loss: 0.00195643
Iteration 7/25 | Loss: 0.00195643
Iteration 8/25 | Loss: 0.00195643
Iteration 9/25 | Loss: 0.00195643
Iteration 10/25 | Loss: 0.00195643
Iteration 11/25 | Loss: 0.00195643
Iteration 12/25 | Loss: 0.00195643
Iteration 13/25 | Loss: 0.00195643
Iteration 14/25 | Loss: 0.00195643
Iteration 15/25 | Loss: 0.00195643
Iteration 16/25 | Loss: 0.00195643
Iteration 17/25 | Loss: 0.00195643
Iteration 18/25 | Loss: 0.00195643
Iteration 19/25 | Loss: 0.00195643
Iteration 20/25 | Loss: 0.00195643
Iteration 21/25 | Loss: 0.00195643
Iteration 22/25 | Loss: 0.00195643
Iteration 23/25 | Loss: 0.00195643
Iteration 24/25 | Loss: 0.00195643
Iteration 25/25 | Loss: 0.00195643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195643
Iteration 2/1000 | Loss: 0.00079239
Iteration 3/1000 | Loss: 0.00036507
Iteration 4/1000 | Loss: 0.00009768
Iteration 5/1000 | Loss: 0.00006909
Iteration 6/1000 | Loss: 0.00005263
Iteration 7/1000 | Loss: 0.00004486
Iteration 8/1000 | Loss: 0.00003783
Iteration 9/1000 | Loss: 0.00003315
Iteration 10/1000 | Loss: 0.00003052
Iteration 11/1000 | Loss: 0.00002784
Iteration 12/1000 | Loss: 0.00002598
Iteration 13/1000 | Loss: 0.00009608
Iteration 14/1000 | Loss: 0.00010165
Iteration 15/1000 | Loss: 0.00002794
Iteration 16/1000 | Loss: 0.00002548
Iteration 17/1000 | Loss: 0.00002260
Iteration 18/1000 | Loss: 0.00002090
Iteration 19/1000 | Loss: 0.00002350
Iteration 20/1000 | Loss: 0.00002899
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00003179
Iteration 23/1000 | Loss: 0.00002001
Iteration 24/1000 | Loss: 0.00001915
Iteration 25/1000 | Loss: 0.00001819
Iteration 26/1000 | Loss: 0.00001766
Iteration 27/1000 | Loss: 0.00001728
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001625
Iteration 34/1000 | Loss: 0.00001600
Iteration 35/1000 | Loss: 0.00001589
Iteration 36/1000 | Loss: 0.00001577
Iteration 37/1000 | Loss: 0.00001566
Iteration 38/1000 | Loss: 0.00001565
Iteration 39/1000 | Loss: 0.00001560
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001556
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001555
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001551
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001542
Iteration 48/1000 | Loss: 0.00001542
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001540
Iteration 51/1000 | Loss: 0.00001540
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00001540
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00001539
Iteration 57/1000 | Loss: 0.00001539
Iteration 58/1000 | Loss: 0.00001538
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001538
Iteration 61/1000 | Loss: 0.00001537
Iteration 62/1000 | Loss: 0.00001537
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001537
Iteration 71/1000 | Loss: 0.00001537
Iteration 72/1000 | Loss: 0.00001537
Iteration 73/1000 | Loss: 0.00001536
Iteration 74/1000 | Loss: 0.00001536
Iteration 75/1000 | Loss: 0.00001536
Iteration 76/1000 | Loss: 0.00001536
Iteration 77/1000 | Loss: 0.00001536
Iteration 78/1000 | Loss: 0.00001536
Iteration 79/1000 | Loss: 0.00001536
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001535
Iteration 83/1000 | Loss: 0.00001535
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001535
Iteration 90/1000 | Loss: 0.00001535
Iteration 91/1000 | Loss: 0.00001535
Iteration 92/1000 | Loss: 0.00001535
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001534
Iteration 98/1000 | Loss: 0.00001534
Iteration 99/1000 | Loss: 0.00001534
Iteration 100/1000 | Loss: 0.00001534
Iteration 101/1000 | Loss: 0.00001534
Iteration 102/1000 | Loss: 0.00001534
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001533
Iteration 106/1000 | Loss: 0.00001533
Iteration 107/1000 | Loss: 0.00001533
Iteration 108/1000 | Loss: 0.00001533
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001532
Iteration 115/1000 | Loss: 0.00001532
Iteration 116/1000 | Loss: 0.00001532
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001531
Iteration 119/1000 | Loss: 0.00001531
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001531
Iteration 122/1000 | Loss: 0.00001531
Iteration 123/1000 | Loss: 0.00001531
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00001530
Iteration 129/1000 | Loss: 0.00001530
Iteration 130/1000 | Loss: 0.00001530
Iteration 131/1000 | Loss: 0.00001530
Iteration 132/1000 | Loss: 0.00001530
Iteration 133/1000 | Loss: 0.00001530
Iteration 134/1000 | Loss: 0.00001529
Iteration 135/1000 | Loss: 0.00001529
Iteration 136/1000 | Loss: 0.00001529
Iteration 137/1000 | Loss: 0.00001529
Iteration 138/1000 | Loss: 0.00001529
Iteration 139/1000 | Loss: 0.00001529
Iteration 140/1000 | Loss: 0.00001529
Iteration 141/1000 | Loss: 0.00001529
Iteration 142/1000 | Loss: 0.00001529
Iteration 143/1000 | Loss: 0.00001529
Iteration 144/1000 | Loss: 0.00001529
Iteration 145/1000 | Loss: 0.00001529
Iteration 146/1000 | Loss: 0.00001529
Iteration 147/1000 | Loss: 0.00001529
Iteration 148/1000 | Loss: 0.00001528
Iteration 149/1000 | Loss: 0.00001528
Iteration 150/1000 | Loss: 0.00001528
Iteration 151/1000 | Loss: 0.00001528
Iteration 152/1000 | Loss: 0.00001528
Iteration 153/1000 | Loss: 0.00001528
Iteration 154/1000 | Loss: 0.00001528
Iteration 155/1000 | Loss: 0.00001528
Iteration 156/1000 | Loss: 0.00001528
Iteration 157/1000 | Loss: 0.00001528
Iteration 158/1000 | Loss: 0.00001527
Iteration 159/1000 | Loss: 0.00001527
Iteration 160/1000 | Loss: 0.00001527
Iteration 161/1000 | Loss: 0.00001527
Iteration 162/1000 | Loss: 0.00001527
Iteration 163/1000 | Loss: 0.00001527
Iteration 164/1000 | Loss: 0.00001527
Iteration 165/1000 | Loss: 0.00001527
Iteration 166/1000 | Loss: 0.00001527
Iteration 167/1000 | Loss: 0.00001527
Iteration 168/1000 | Loss: 0.00001527
Iteration 169/1000 | Loss: 0.00001527
Iteration 170/1000 | Loss: 0.00001527
Iteration 171/1000 | Loss: 0.00001527
Iteration 172/1000 | Loss: 0.00001526
Iteration 173/1000 | Loss: 0.00001526
Iteration 174/1000 | Loss: 0.00001526
Iteration 175/1000 | Loss: 0.00001526
Iteration 176/1000 | Loss: 0.00001526
Iteration 177/1000 | Loss: 0.00001526
Iteration 178/1000 | Loss: 0.00001526
Iteration 179/1000 | Loss: 0.00001526
Iteration 180/1000 | Loss: 0.00001526
Iteration 181/1000 | Loss: 0.00001526
Iteration 182/1000 | Loss: 0.00001526
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001525
Iteration 185/1000 | Loss: 0.00001524
Iteration 186/1000 | Loss: 0.00001524
Iteration 187/1000 | Loss: 0.00001524
Iteration 188/1000 | Loss: 0.00001524
Iteration 189/1000 | Loss: 0.00001523
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001523
Iteration 193/1000 | Loss: 0.00001523
Iteration 194/1000 | Loss: 0.00001523
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001522
Iteration 197/1000 | Loss: 0.00001522
Iteration 198/1000 | Loss: 0.00001522
Iteration 199/1000 | Loss: 0.00001522
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001522
Iteration 204/1000 | Loss: 0.00001522
Iteration 205/1000 | Loss: 0.00001522
Iteration 206/1000 | Loss: 0.00001522
Iteration 207/1000 | Loss: 0.00001522
Iteration 208/1000 | Loss: 0.00001522
Iteration 209/1000 | Loss: 0.00001522
Iteration 210/1000 | Loss: 0.00001522
Iteration 211/1000 | Loss: 0.00001521
Iteration 212/1000 | Loss: 0.00001521
Iteration 213/1000 | Loss: 0.00001521
Iteration 214/1000 | Loss: 0.00001521
Iteration 215/1000 | Loss: 0.00001521
Iteration 216/1000 | Loss: 0.00001521
Iteration 217/1000 | Loss: 0.00001521
Iteration 218/1000 | Loss: 0.00001521
Iteration 219/1000 | Loss: 0.00001521
Iteration 220/1000 | Loss: 0.00001521
Iteration 221/1000 | Loss: 0.00001521
Iteration 222/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.5210113815555815e-05, 1.5210113815555815e-05, 1.5210113815555815e-05, 1.5210113815555815e-05, 1.5210113815555815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5210113815555815e-05

Optimization complete. Final v2v error: 3.282370090484619 mm

Highest mean error: 4.033534526824951 mm for frame 142

Lowest mean error: 2.563795328140259 mm for frame 78

Saving results

Total time: 120.45560884475708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471234
Iteration 2/25 | Loss: 0.00133882
Iteration 3/25 | Loss: 0.00126051
Iteration 4/25 | Loss: 0.00125082
Iteration 5/25 | Loss: 0.00124952
Iteration 6/25 | Loss: 0.00124952
Iteration 7/25 | Loss: 0.00124952
Iteration 8/25 | Loss: 0.00124952
Iteration 9/25 | Loss: 0.00124952
Iteration 10/25 | Loss: 0.00124952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012495217379182577, 0.0012495217379182577, 0.0012495217379182577, 0.0012495217379182577, 0.0012495217379182577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012495217379182577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.89714050
Iteration 2/25 | Loss: 0.00182642
Iteration 3/25 | Loss: 0.00182642
Iteration 4/25 | Loss: 0.00182642
Iteration 5/25 | Loss: 0.00182642
Iteration 6/25 | Loss: 0.00182642
Iteration 7/25 | Loss: 0.00182642
Iteration 8/25 | Loss: 0.00182642
Iteration 9/25 | Loss: 0.00182642
Iteration 10/25 | Loss: 0.00182642
Iteration 11/25 | Loss: 0.00182642
Iteration 12/25 | Loss: 0.00182642
Iteration 13/25 | Loss: 0.00182641
Iteration 14/25 | Loss: 0.00182641
Iteration 15/25 | Loss: 0.00182641
Iteration 16/25 | Loss: 0.00182641
Iteration 17/25 | Loss: 0.00182641
Iteration 18/25 | Loss: 0.00182641
Iteration 19/25 | Loss: 0.00182641
Iteration 20/25 | Loss: 0.00182641
Iteration 21/25 | Loss: 0.00182641
Iteration 22/25 | Loss: 0.00182641
Iteration 23/25 | Loss: 0.00182641
Iteration 24/25 | Loss: 0.00182641
Iteration 25/25 | Loss: 0.00182641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182641
Iteration 2/1000 | Loss: 0.00002204
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001475
Iteration 6/1000 | Loss: 0.00001411
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001328
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001243
Iteration 14/1000 | Loss: 0.00001241
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001179
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001175
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001174
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001171
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001166
Iteration 36/1000 | Loss: 0.00001165
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001151
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001150
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001147
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001146
Iteration 70/1000 | Loss: 0.00001146
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001144
Iteration 74/1000 | Loss: 0.00001144
Iteration 75/1000 | Loss: 0.00001144
Iteration 76/1000 | Loss: 0.00001144
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001144
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001144
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001142
Iteration 93/1000 | Loss: 0.00001142
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001141
Iteration 102/1000 | Loss: 0.00001141
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001141
Iteration 106/1000 | Loss: 0.00001141
Iteration 107/1000 | Loss: 0.00001141
Iteration 108/1000 | Loss: 0.00001141
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001141
Iteration 113/1000 | Loss: 0.00001141
Iteration 114/1000 | Loss: 0.00001141
Iteration 115/1000 | Loss: 0.00001141
Iteration 116/1000 | Loss: 0.00001141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.1411827472329605e-05, 1.1411827472329605e-05, 1.1411827472329605e-05, 1.1411827472329605e-05, 1.1411827472329605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1411827472329605e-05

Optimization complete. Final v2v error: 2.9259512424468994 mm

Highest mean error: 3.17913556098938 mm for frame 228

Lowest mean error: 2.7795004844665527 mm for frame 121

Saving results

Total time: 41.16636323928833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813365
Iteration 2/25 | Loss: 0.00138954
Iteration 3/25 | Loss: 0.00127815
Iteration 4/25 | Loss: 0.00126015
Iteration 5/25 | Loss: 0.00125257
Iteration 6/25 | Loss: 0.00125137
Iteration 7/25 | Loss: 0.00125069
Iteration 8/25 | Loss: 0.00125119
Iteration 9/25 | Loss: 0.00124967
Iteration 10/25 | Loss: 0.00124893
Iteration 11/25 | Loss: 0.00124835
Iteration 12/25 | Loss: 0.00124806
Iteration 13/25 | Loss: 0.00124799
Iteration 14/25 | Loss: 0.00124799
Iteration 15/25 | Loss: 0.00124799
Iteration 16/25 | Loss: 0.00124798
Iteration 17/25 | Loss: 0.00124798
Iteration 18/25 | Loss: 0.00124798
Iteration 19/25 | Loss: 0.00124798
Iteration 20/25 | Loss: 0.00124798
Iteration 21/25 | Loss: 0.00124798
Iteration 22/25 | Loss: 0.00124798
Iteration 23/25 | Loss: 0.00124798
Iteration 24/25 | Loss: 0.00124798
Iteration 25/25 | Loss: 0.00124798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50562429
Iteration 2/25 | Loss: 0.00186978
Iteration 3/25 | Loss: 0.00186978
Iteration 4/25 | Loss: 0.00186978
Iteration 5/25 | Loss: 0.00186978
Iteration 6/25 | Loss: 0.00186978
Iteration 7/25 | Loss: 0.00186978
Iteration 8/25 | Loss: 0.00186978
Iteration 9/25 | Loss: 0.00186978
Iteration 10/25 | Loss: 0.00186978
Iteration 11/25 | Loss: 0.00186978
Iteration 12/25 | Loss: 0.00186978
Iteration 13/25 | Loss: 0.00186978
Iteration 14/25 | Loss: 0.00186978
Iteration 15/25 | Loss: 0.00186978
Iteration 16/25 | Loss: 0.00186978
Iteration 17/25 | Loss: 0.00186978
Iteration 18/25 | Loss: 0.00186978
Iteration 19/25 | Loss: 0.00186978
Iteration 20/25 | Loss: 0.00186977
Iteration 21/25 | Loss: 0.00186977
Iteration 22/25 | Loss: 0.00186977
Iteration 23/25 | Loss: 0.00186977
Iteration 24/25 | Loss: 0.00186977
Iteration 25/25 | Loss: 0.00186977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186977
Iteration 2/1000 | Loss: 0.00002401
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001702
Iteration 5/1000 | Loss: 0.00001621
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001412
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001369
Iteration 17/1000 | Loss: 0.00001368
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001348
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001338
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001334
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001333
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.3293431038619019e-05, 1.3293431038619019e-05, 1.3293431038619019e-05, 1.3293431038619019e-05, 1.3293431038619019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3293431038619019e-05

Optimization complete. Final v2v error: 3.1378843784332275 mm

Highest mean error: 3.5270750522613525 mm for frame 204

Lowest mean error: 2.8627421855926514 mm for frame 107

Saving results

Total time: 53.53917670249939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747429
Iteration 2/25 | Loss: 0.00148007
Iteration 3/25 | Loss: 0.00133589
Iteration 4/25 | Loss: 0.00130616
Iteration 5/25 | Loss: 0.00128370
Iteration 6/25 | Loss: 0.00127560
Iteration 7/25 | Loss: 0.00127115
Iteration 8/25 | Loss: 0.00127039
Iteration 9/25 | Loss: 0.00127010
Iteration 10/25 | Loss: 0.00126992
Iteration 11/25 | Loss: 0.00126961
Iteration 12/25 | Loss: 0.00126910
Iteration 13/25 | Loss: 0.00126873
Iteration 14/25 | Loss: 0.00126793
Iteration 15/25 | Loss: 0.00126847
Iteration 16/25 | Loss: 0.00126946
Iteration 17/25 | Loss: 0.00126727
Iteration 18/25 | Loss: 0.00126671
Iteration 19/25 | Loss: 0.00126584
Iteration 20/25 | Loss: 0.00126549
Iteration 21/25 | Loss: 0.00126570
Iteration 22/25 | Loss: 0.00126606
Iteration 23/25 | Loss: 0.00126565
Iteration 24/25 | Loss: 0.00126503
Iteration 25/25 | Loss: 0.00126584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77213657
Iteration 2/25 | Loss: 0.00335013
Iteration 3/25 | Loss: 0.00335013
Iteration 4/25 | Loss: 0.00335013
Iteration 5/25 | Loss: 0.00335013
Iteration 6/25 | Loss: 0.00335013
Iteration 7/25 | Loss: 0.00335013
Iteration 8/25 | Loss: 0.00335012
Iteration 9/25 | Loss: 0.00335012
Iteration 10/25 | Loss: 0.00335012
Iteration 11/25 | Loss: 0.00335012
Iteration 12/25 | Loss: 0.00335012
Iteration 13/25 | Loss: 0.00335012
Iteration 14/25 | Loss: 0.00335012
Iteration 15/25 | Loss: 0.00335012
Iteration 16/25 | Loss: 0.00335012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0033501237630844116, 0.0033501237630844116, 0.0033501237630844116, 0.0033501237630844116, 0.0033501237630844116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033501237630844116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00335012
Iteration 2/1000 | Loss: 0.00017119
Iteration 3/1000 | Loss: 0.00023470
Iteration 4/1000 | Loss: 0.00009487
Iteration 5/1000 | Loss: 0.00009138
Iteration 6/1000 | Loss: 0.00007380
Iteration 7/1000 | Loss: 0.00010266
Iteration 8/1000 | Loss: 0.00010028
Iteration 9/1000 | Loss: 0.00009083
Iteration 10/1000 | Loss: 0.00008093
Iteration 11/1000 | Loss: 0.00016992
Iteration 12/1000 | Loss: 0.00014531
Iteration 13/1000 | Loss: 0.00014474
Iteration 14/1000 | Loss: 0.00013235
Iteration 15/1000 | Loss: 0.00002773
Iteration 16/1000 | Loss: 0.00003861
Iteration 17/1000 | Loss: 0.00018212
Iteration 18/1000 | Loss: 0.00013570
Iteration 19/1000 | Loss: 0.00016409
Iteration 20/1000 | Loss: 0.00010107
Iteration 21/1000 | Loss: 0.00012818
Iteration 22/1000 | Loss: 0.00010528
Iteration 23/1000 | Loss: 0.00011032
Iteration 24/1000 | Loss: 0.00007946
Iteration 25/1000 | Loss: 0.00010322
Iteration 26/1000 | Loss: 0.00010822
Iteration 27/1000 | Loss: 0.00010662
Iteration 28/1000 | Loss: 0.00005359
Iteration 29/1000 | Loss: 0.00008505
Iteration 30/1000 | Loss: 0.00008678
Iteration 31/1000 | Loss: 0.00009930
Iteration 32/1000 | Loss: 0.00010325
Iteration 33/1000 | Loss: 0.00030659
Iteration 34/1000 | Loss: 0.00019047
Iteration 35/1000 | Loss: 0.00004180
Iteration 36/1000 | Loss: 0.00017313
Iteration 37/1000 | Loss: 0.00003353
Iteration 38/1000 | Loss: 0.00002690
Iteration 39/1000 | Loss: 0.00002526
Iteration 40/1000 | Loss: 0.00002351
Iteration 41/1000 | Loss: 0.00002216
Iteration 42/1000 | Loss: 0.00002139
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002054
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002017
Iteration 47/1000 | Loss: 0.00002011
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001997
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001992
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001972
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001964
Iteration 58/1000 | Loss: 0.00032478
Iteration 59/1000 | Loss: 0.00002053
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001826
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001749
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001742
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001731
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001727
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001722
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001716
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001714
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001714
Iteration 112/1000 | Loss: 0.00001713
Iteration 113/1000 | Loss: 0.00001713
Iteration 114/1000 | Loss: 0.00001713
Iteration 115/1000 | Loss: 0.00001713
Iteration 116/1000 | Loss: 0.00001713
Iteration 117/1000 | Loss: 0.00001713
Iteration 118/1000 | Loss: 0.00001713
Iteration 119/1000 | Loss: 0.00001713
Iteration 120/1000 | Loss: 0.00001713
Iteration 121/1000 | Loss: 0.00001713
Iteration 122/1000 | Loss: 0.00001713
Iteration 123/1000 | Loss: 0.00001713
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001712
Iteration 127/1000 | Loss: 0.00001712
Iteration 128/1000 | Loss: 0.00001712
Iteration 129/1000 | Loss: 0.00001712
Iteration 130/1000 | Loss: 0.00001712
Iteration 131/1000 | Loss: 0.00001712
Iteration 132/1000 | Loss: 0.00001712
Iteration 133/1000 | Loss: 0.00001712
Iteration 134/1000 | Loss: 0.00001711
Iteration 135/1000 | Loss: 0.00001711
Iteration 136/1000 | Loss: 0.00001710
Iteration 137/1000 | Loss: 0.00001710
Iteration 138/1000 | Loss: 0.00001710
Iteration 139/1000 | Loss: 0.00001710
Iteration 140/1000 | Loss: 0.00001710
Iteration 141/1000 | Loss: 0.00001710
Iteration 142/1000 | Loss: 0.00001710
Iteration 143/1000 | Loss: 0.00001710
Iteration 144/1000 | Loss: 0.00001710
Iteration 145/1000 | Loss: 0.00001710
Iteration 146/1000 | Loss: 0.00001710
Iteration 147/1000 | Loss: 0.00001710
Iteration 148/1000 | Loss: 0.00001710
Iteration 149/1000 | Loss: 0.00001710
Iteration 150/1000 | Loss: 0.00001710
Iteration 151/1000 | Loss: 0.00001710
Iteration 152/1000 | Loss: 0.00001710
Iteration 153/1000 | Loss: 0.00001710
Iteration 154/1000 | Loss: 0.00001710
Iteration 155/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7101641788030975e-05, 1.7101641788030975e-05, 1.7101641788030975e-05, 1.7101641788030975e-05, 1.7101641788030975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7101641788030975e-05

Optimization complete. Final v2v error: 3.515806198120117 mm

Highest mean error: 4.6859636306762695 mm for frame 233

Lowest mean error: 3.1875901222229004 mm for frame 94

Saving results

Total time: 152.33635926246643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393058
Iteration 2/25 | Loss: 0.00134683
Iteration 3/25 | Loss: 0.00124431
Iteration 4/25 | Loss: 0.00123589
Iteration 5/25 | Loss: 0.00123473
Iteration 6/25 | Loss: 0.00123473
Iteration 7/25 | Loss: 0.00123473
Iteration 8/25 | Loss: 0.00123473
Iteration 9/25 | Loss: 0.00123473
Iteration 10/25 | Loss: 0.00123473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012347251176834106, 0.0012347251176834106, 0.0012347251176834106, 0.0012347251176834106, 0.0012347251176834106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012347251176834106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22899687
Iteration 2/25 | Loss: 0.00190825
Iteration 3/25 | Loss: 0.00190825
Iteration 4/25 | Loss: 0.00190825
Iteration 5/25 | Loss: 0.00190825
Iteration 6/25 | Loss: 0.00190825
Iteration 7/25 | Loss: 0.00190825
Iteration 8/25 | Loss: 0.00190825
Iteration 9/25 | Loss: 0.00190825
Iteration 10/25 | Loss: 0.00190825
Iteration 11/25 | Loss: 0.00190825
Iteration 12/25 | Loss: 0.00190825
Iteration 13/25 | Loss: 0.00190825
Iteration 14/25 | Loss: 0.00190825
Iteration 15/25 | Loss: 0.00190825
Iteration 16/25 | Loss: 0.00190825
Iteration 17/25 | Loss: 0.00190825
Iteration 18/25 | Loss: 0.00190825
Iteration 19/25 | Loss: 0.00190825
Iteration 20/25 | Loss: 0.00190825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019082474755123258, 0.0019082474755123258, 0.0019082474755123258, 0.0019082474755123258, 0.0019082474755123258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019082474755123258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190825
Iteration 2/1000 | Loss: 0.00002027
Iteration 3/1000 | Loss: 0.00001608
Iteration 4/1000 | Loss: 0.00001472
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001311
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001182
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001157
Iteration 13/1000 | Loss: 0.00001156
Iteration 14/1000 | Loss: 0.00001148
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001127
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001112
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001102
Iteration 28/1000 | Loss: 0.00001098
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001094
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001089
Iteration 34/1000 | Loss: 0.00001089
Iteration 35/1000 | Loss: 0.00001085
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001084
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001080
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001075
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001070
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001070
Iteration 73/1000 | Loss: 0.00001070
Iteration 74/1000 | Loss: 0.00001070
Iteration 75/1000 | Loss: 0.00001070
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001069
Iteration 78/1000 | Loss: 0.00001069
Iteration 79/1000 | Loss: 0.00001069
Iteration 80/1000 | Loss: 0.00001069
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001067
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001066
Iteration 85/1000 | Loss: 0.00001066
Iteration 86/1000 | Loss: 0.00001066
Iteration 87/1000 | Loss: 0.00001065
Iteration 88/1000 | Loss: 0.00001065
Iteration 89/1000 | Loss: 0.00001065
Iteration 90/1000 | Loss: 0.00001064
Iteration 91/1000 | Loss: 0.00001064
Iteration 92/1000 | Loss: 0.00001064
Iteration 93/1000 | Loss: 0.00001064
Iteration 94/1000 | Loss: 0.00001063
Iteration 95/1000 | Loss: 0.00001062
Iteration 96/1000 | Loss: 0.00001062
Iteration 97/1000 | Loss: 0.00001062
Iteration 98/1000 | Loss: 0.00001062
Iteration 99/1000 | Loss: 0.00001061
Iteration 100/1000 | Loss: 0.00001061
Iteration 101/1000 | Loss: 0.00001061
Iteration 102/1000 | Loss: 0.00001060
Iteration 103/1000 | Loss: 0.00001060
Iteration 104/1000 | Loss: 0.00001060
Iteration 105/1000 | Loss: 0.00001060
Iteration 106/1000 | Loss: 0.00001060
Iteration 107/1000 | Loss: 0.00001059
Iteration 108/1000 | Loss: 0.00001059
Iteration 109/1000 | Loss: 0.00001059
Iteration 110/1000 | Loss: 0.00001058
Iteration 111/1000 | Loss: 0.00001058
Iteration 112/1000 | Loss: 0.00001058
Iteration 113/1000 | Loss: 0.00001058
Iteration 114/1000 | Loss: 0.00001058
Iteration 115/1000 | Loss: 0.00001058
Iteration 116/1000 | Loss: 0.00001058
Iteration 117/1000 | Loss: 0.00001058
Iteration 118/1000 | Loss: 0.00001057
Iteration 119/1000 | Loss: 0.00001057
Iteration 120/1000 | Loss: 0.00001057
Iteration 121/1000 | Loss: 0.00001057
Iteration 122/1000 | Loss: 0.00001057
Iteration 123/1000 | Loss: 0.00001057
Iteration 124/1000 | Loss: 0.00001057
Iteration 125/1000 | Loss: 0.00001057
Iteration 126/1000 | Loss: 0.00001057
Iteration 127/1000 | Loss: 0.00001056
Iteration 128/1000 | Loss: 0.00001056
Iteration 129/1000 | Loss: 0.00001056
Iteration 130/1000 | Loss: 0.00001056
Iteration 131/1000 | Loss: 0.00001056
Iteration 132/1000 | Loss: 0.00001056
Iteration 133/1000 | Loss: 0.00001056
Iteration 134/1000 | Loss: 0.00001056
Iteration 135/1000 | Loss: 0.00001056
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001055
Iteration 141/1000 | Loss: 0.00001055
Iteration 142/1000 | Loss: 0.00001055
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001055
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001055
Iteration 154/1000 | Loss: 0.00001055
Iteration 155/1000 | Loss: 0.00001055
Iteration 156/1000 | Loss: 0.00001055
Iteration 157/1000 | Loss: 0.00001055
Iteration 158/1000 | Loss: 0.00001055
Iteration 159/1000 | Loss: 0.00001055
Iteration 160/1000 | Loss: 0.00001055
Iteration 161/1000 | Loss: 0.00001055
Iteration 162/1000 | Loss: 0.00001055
Iteration 163/1000 | Loss: 0.00001055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.0546852536208462e-05, 1.0546852536208462e-05, 1.0546852536208462e-05, 1.0546852536208462e-05, 1.0546852536208462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0546852536208462e-05

Optimization complete. Final v2v error: 2.7990760803222656 mm

Highest mean error: 3.107121229171753 mm for frame 107

Lowest mean error: 2.5723936557769775 mm for frame 10

Saving results

Total time: 41.976237058639526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944213
Iteration 2/25 | Loss: 0.00219815
Iteration 3/25 | Loss: 0.00167989
Iteration 4/25 | Loss: 0.00158735
Iteration 5/25 | Loss: 0.00155189
Iteration 6/25 | Loss: 0.00148962
Iteration 7/25 | Loss: 0.00139520
Iteration 8/25 | Loss: 0.00136826
Iteration 9/25 | Loss: 0.00137274
Iteration 10/25 | Loss: 0.00140494
Iteration 11/25 | Loss: 0.00139839
Iteration 12/25 | Loss: 0.00137875
Iteration 13/25 | Loss: 0.00137499
Iteration 14/25 | Loss: 0.00137849
Iteration 15/25 | Loss: 0.00137501
Iteration 16/25 | Loss: 0.00137575
Iteration 17/25 | Loss: 0.00137805
Iteration 18/25 | Loss: 0.00137184
Iteration 19/25 | Loss: 0.00137616
Iteration 20/25 | Loss: 0.00137836
Iteration 21/25 | Loss: 0.00136613
Iteration 22/25 | Loss: 0.00136512
Iteration 23/25 | Loss: 0.00137481
Iteration 24/25 | Loss: 0.00139498
Iteration 25/25 | Loss: 0.00138483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22313809
Iteration 2/25 | Loss: 0.00377836
Iteration 3/25 | Loss: 0.00377836
Iteration 4/25 | Loss: 0.00377836
Iteration 5/25 | Loss: 0.00377836
Iteration 6/25 | Loss: 0.00377835
Iteration 7/25 | Loss: 0.00377835
Iteration 8/25 | Loss: 0.00377835
Iteration 9/25 | Loss: 0.00377835
Iteration 10/25 | Loss: 0.00377835
Iteration 11/25 | Loss: 0.00377835
Iteration 12/25 | Loss: 0.00377835
Iteration 13/25 | Loss: 0.00377835
Iteration 14/25 | Loss: 0.00377835
Iteration 15/25 | Loss: 0.00377835
Iteration 16/25 | Loss: 0.00377835
Iteration 17/25 | Loss: 0.00377835
Iteration 18/25 | Loss: 0.00377835
Iteration 19/25 | Loss: 0.00377835
Iteration 20/25 | Loss: 0.00377835
Iteration 21/25 | Loss: 0.00377835
Iteration 22/25 | Loss: 0.00377835
Iteration 23/25 | Loss: 0.00377835
Iteration 24/25 | Loss: 0.00377835
Iteration 25/25 | Loss: 0.00377835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00377835
Iteration 2/1000 | Loss: 0.00058071
Iteration 3/1000 | Loss: 0.00116935
Iteration 4/1000 | Loss: 0.00073420
Iteration 5/1000 | Loss: 0.00078187
Iteration 6/1000 | Loss: 0.00054239
Iteration 7/1000 | Loss: 0.00067917
Iteration 8/1000 | Loss: 0.00054544
Iteration 9/1000 | Loss: 0.00068270
Iteration 10/1000 | Loss: 0.00049465
Iteration 11/1000 | Loss: 0.00027421
Iteration 12/1000 | Loss: 0.00007777
Iteration 13/1000 | Loss: 0.00006571
Iteration 14/1000 | Loss: 0.00013433
Iteration 15/1000 | Loss: 0.00005814
Iteration 16/1000 | Loss: 0.00008062
Iteration 17/1000 | Loss: 0.00007325
Iteration 18/1000 | Loss: 0.00004770
Iteration 19/1000 | Loss: 0.00004467
Iteration 20/1000 | Loss: 0.00004215
Iteration 21/1000 | Loss: 0.00009972
Iteration 22/1000 | Loss: 0.00004027
Iteration 23/1000 | Loss: 0.00003693
Iteration 24/1000 | Loss: 0.00003523
Iteration 25/1000 | Loss: 0.00043694
Iteration 26/1000 | Loss: 0.00030934
Iteration 27/1000 | Loss: 0.00047103
Iteration 28/1000 | Loss: 0.00038951
Iteration 29/1000 | Loss: 0.00045481
Iteration 30/1000 | Loss: 0.00026818
Iteration 31/1000 | Loss: 0.00040893
Iteration 32/1000 | Loss: 0.00005960
Iteration 33/1000 | Loss: 0.00004354
Iteration 34/1000 | Loss: 0.00004033
Iteration 35/1000 | Loss: 0.00005609
Iteration 36/1000 | Loss: 0.00059203
Iteration 37/1000 | Loss: 0.00046311
Iteration 38/1000 | Loss: 0.00063710
Iteration 39/1000 | Loss: 0.00006106
Iteration 40/1000 | Loss: 0.00005056
Iteration 41/1000 | Loss: 0.00004592
Iteration 42/1000 | Loss: 0.00004304
Iteration 43/1000 | Loss: 0.00004163
Iteration 44/1000 | Loss: 0.00003991
Iteration 45/1000 | Loss: 0.00003819
Iteration 46/1000 | Loss: 0.00050014
Iteration 47/1000 | Loss: 0.00057543
Iteration 48/1000 | Loss: 0.00004686
Iteration 49/1000 | Loss: 0.00004028
Iteration 50/1000 | Loss: 0.00003520
Iteration 51/1000 | Loss: 0.00003310
Iteration 52/1000 | Loss: 0.00003157
Iteration 53/1000 | Loss: 0.00003046
Iteration 54/1000 | Loss: 0.00002969
Iteration 55/1000 | Loss: 0.00002916
Iteration 56/1000 | Loss: 0.00002880
Iteration 57/1000 | Loss: 0.00002847
Iteration 58/1000 | Loss: 0.00002816
Iteration 59/1000 | Loss: 0.00002803
Iteration 60/1000 | Loss: 0.00002796
Iteration 61/1000 | Loss: 0.00002793
Iteration 62/1000 | Loss: 0.00002788
Iteration 63/1000 | Loss: 0.00002787
Iteration 64/1000 | Loss: 0.00002784
Iteration 65/1000 | Loss: 0.00002774
Iteration 66/1000 | Loss: 0.00002771
Iteration 67/1000 | Loss: 0.00002771
Iteration 68/1000 | Loss: 0.00002769
Iteration 69/1000 | Loss: 0.00004272
Iteration 70/1000 | Loss: 0.00065479
Iteration 71/1000 | Loss: 0.00059500
Iteration 72/1000 | Loss: 0.00055519
Iteration 73/1000 | Loss: 0.00050342
Iteration 74/1000 | Loss: 0.00050683
Iteration 75/1000 | Loss: 0.00024759
Iteration 76/1000 | Loss: 0.00004606
Iteration 77/1000 | Loss: 0.00004734
Iteration 78/1000 | Loss: 0.00002912
Iteration 79/1000 | Loss: 0.00002777
Iteration 80/1000 | Loss: 0.00002686
Iteration 81/1000 | Loss: 0.00002619
Iteration 82/1000 | Loss: 0.00002569
Iteration 83/1000 | Loss: 0.00002542
Iteration 84/1000 | Loss: 0.00002536
Iteration 85/1000 | Loss: 0.00002527
Iteration 86/1000 | Loss: 0.00002526
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002519
Iteration 89/1000 | Loss: 0.00059962
Iteration 90/1000 | Loss: 0.00008894
Iteration 91/1000 | Loss: 0.00058064
Iteration 92/1000 | Loss: 0.00002823
Iteration 93/1000 | Loss: 0.00059915
Iteration 94/1000 | Loss: 0.00057376
Iteration 95/1000 | Loss: 0.00004199
Iteration 96/1000 | Loss: 0.00003023
Iteration 97/1000 | Loss: 0.00002717
Iteration 98/1000 | Loss: 0.00002522
Iteration 99/1000 | Loss: 0.00002501
Iteration 100/1000 | Loss: 0.00002500
Iteration 101/1000 | Loss: 0.00002500
Iteration 102/1000 | Loss: 0.00002499
Iteration 103/1000 | Loss: 0.00002496
Iteration 104/1000 | Loss: 0.00002493
Iteration 105/1000 | Loss: 0.00002493
Iteration 106/1000 | Loss: 0.00002493
Iteration 107/1000 | Loss: 0.00002493
Iteration 108/1000 | Loss: 0.00002493
Iteration 109/1000 | Loss: 0.00002493
Iteration 110/1000 | Loss: 0.00002493
Iteration 111/1000 | Loss: 0.00002493
Iteration 112/1000 | Loss: 0.00002493
Iteration 113/1000 | Loss: 0.00002493
Iteration 114/1000 | Loss: 0.00002493
Iteration 115/1000 | Loss: 0.00002492
Iteration 116/1000 | Loss: 0.00002492
Iteration 117/1000 | Loss: 0.00002492
Iteration 118/1000 | Loss: 0.00002492
Iteration 119/1000 | Loss: 0.00002492
Iteration 120/1000 | Loss: 0.00002492
Iteration 121/1000 | Loss: 0.00002491
Iteration 122/1000 | Loss: 0.00002491
Iteration 123/1000 | Loss: 0.00002491
Iteration 124/1000 | Loss: 0.00002491
Iteration 125/1000 | Loss: 0.00002491
Iteration 126/1000 | Loss: 0.00002491
Iteration 127/1000 | Loss: 0.00002491
Iteration 128/1000 | Loss: 0.00002491
Iteration 129/1000 | Loss: 0.00002491
Iteration 130/1000 | Loss: 0.00002491
Iteration 131/1000 | Loss: 0.00002491
Iteration 132/1000 | Loss: 0.00002491
Iteration 133/1000 | Loss: 0.00002490
Iteration 134/1000 | Loss: 0.00002490
Iteration 135/1000 | Loss: 0.00002489
Iteration 136/1000 | Loss: 0.00002489
Iteration 137/1000 | Loss: 0.00002488
Iteration 138/1000 | Loss: 0.00002488
Iteration 139/1000 | Loss: 0.00002488
Iteration 140/1000 | Loss: 0.00002487
Iteration 141/1000 | Loss: 0.00002486
Iteration 142/1000 | Loss: 0.00002486
Iteration 143/1000 | Loss: 0.00002486
Iteration 144/1000 | Loss: 0.00002485
Iteration 145/1000 | Loss: 0.00002832
Iteration 146/1000 | Loss: 0.00002483
Iteration 147/1000 | Loss: 0.00002483
Iteration 148/1000 | Loss: 0.00002482
Iteration 149/1000 | Loss: 0.00002482
Iteration 150/1000 | Loss: 0.00002482
Iteration 151/1000 | Loss: 0.00002482
Iteration 152/1000 | Loss: 0.00002482
Iteration 153/1000 | Loss: 0.00002482
Iteration 154/1000 | Loss: 0.00002482
Iteration 155/1000 | Loss: 0.00002482
Iteration 156/1000 | Loss: 0.00002482
Iteration 157/1000 | Loss: 0.00002482
Iteration 158/1000 | Loss: 0.00002482
Iteration 159/1000 | Loss: 0.00002482
Iteration 160/1000 | Loss: 0.00002481
Iteration 161/1000 | Loss: 0.00002481
Iteration 162/1000 | Loss: 0.00002481
Iteration 163/1000 | Loss: 0.00002481
Iteration 164/1000 | Loss: 0.00002481
Iteration 165/1000 | Loss: 0.00002481
Iteration 166/1000 | Loss: 0.00002481
Iteration 167/1000 | Loss: 0.00002481
Iteration 168/1000 | Loss: 0.00002481
Iteration 169/1000 | Loss: 0.00002481
Iteration 170/1000 | Loss: 0.00002481
Iteration 171/1000 | Loss: 0.00002481
Iteration 172/1000 | Loss: 0.00002481
Iteration 173/1000 | Loss: 0.00002481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.480719194863923e-05, 2.480719194863923e-05, 2.480719194863923e-05, 2.480719194863923e-05, 2.480719194863923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.480719194863923e-05

Optimization complete. Final v2v error: 3.8811662197113037 mm

Highest mean error: 10.427244186401367 mm for frame 21

Lowest mean error: 2.569638252258301 mm for frame 7

Saving results

Total time: 173.4231379032135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471497
Iteration 2/25 | Loss: 0.00131780
Iteration 3/25 | Loss: 0.00123983
Iteration 4/25 | Loss: 0.00122973
Iteration 5/25 | Loss: 0.00122752
Iteration 6/25 | Loss: 0.00122748
Iteration 7/25 | Loss: 0.00122748
Iteration 8/25 | Loss: 0.00122748
Iteration 9/25 | Loss: 0.00122748
Iteration 10/25 | Loss: 0.00122748
Iteration 11/25 | Loss: 0.00122748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012274831533432007, 0.0012274831533432007, 0.0012274831533432007, 0.0012274831533432007, 0.0012274831533432007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012274831533432007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.98562479
Iteration 2/25 | Loss: 0.00199480
Iteration 3/25 | Loss: 0.00199480
Iteration 4/25 | Loss: 0.00199480
Iteration 5/25 | Loss: 0.00199480
Iteration 6/25 | Loss: 0.00199480
Iteration 7/25 | Loss: 0.00199480
Iteration 8/25 | Loss: 0.00199479
Iteration 9/25 | Loss: 0.00199479
Iteration 10/25 | Loss: 0.00199479
Iteration 11/25 | Loss: 0.00199479
Iteration 12/25 | Loss: 0.00199479
Iteration 13/25 | Loss: 0.00199479
Iteration 14/25 | Loss: 0.00199479
Iteration 15/25 | Loss: 0.00199479
Iteration 16/25 | Loss: 0.00199479
Iteration 17/25 | Loss: 0.00199479
Iteration 18/25 | Loss: 0.00199479
Iteration 19/25 | Loss: 0.00199479
Iteration 20/25 | Loss: 0.00199479
Iteration 21/25 | Loss: 0.00199479
Iteration 22/25 | Loss: 0.00199479
Iteration 23/25 | Loss: 0.00199479
Iteration 24/25 | Loss: 0.00199479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019947935361415148, 0.0019947935361415148, 0.0019947935361415148, 0.0019947935361415148, 0.0019947935361415148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019947935361415148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199479
Iteration 2/1000 | Loss: 0.00001981
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001376
Iteration 5/1000 | Loss: 0.00001289
Iteration 6/1000 | Loss: 0.00001222
Iteration 7/1000 | Loss: 0.00001164
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001096
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001067
Iteration 12/1000 | Loss: 0.00001063
Iteration 13/1000 | Loss: 0.00001060
Iteration 14/1000 | Loss: 0.00001048
Iteration 15/1000 | Loss: 0.00001044
Iteration 16/1000 | Loss: 0.00001035
Iteration 17/1000 | Loss: 0.00001031
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001029
Iteration 20/1000 | Loss: 0.00001028
Iteration 21/1000 | Loss: 0.00001028
Iteration 22/1000 | Loss: 0.00001027
Iteration 23/1000 | Loss: 0.00001027
Iteration 24/1000 | Loss: 0.00001026
Iteration 25/1000 | Loss: 0.00001026
Iteration 26/1000 | Loss: 0.00001024
Iteration 27/1000 | Loss: 0.00001018
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001008
Iteration 30/1000 | Loss: 0.00001006
Iteration 31/1000 | Loss: 0.00001005
Iteration 32/1000 | Loss: 0.00001005
Iteration 33/1000 | Loss: 0.00001004
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001003
Iteration 36/1000 | Loss: 0.00001003
Iteration 37/1000 | Loss: 0.00001002
Iteration 38/1000 | Loss: 0.00001002
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000999
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000998
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000996
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000989
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000988
Iteration 54/1000 | Loss: 0.00000988
Iteration 55/1000 | Loss: 0.00000987
Iteration 56/1000 | Loss: 0.00000987
Iteration 57/1000 | Loss: 0.00000987
Iteration 58/1000 | Loss: 0.00000987
Iteration 59/1000 | Loss: 0.00000987
Iteration 60/1000 | Loss: 0.00000987
Iteration 61/1000 | Loss: 0.00000987
Iteration 62/1000 | Loss: 0.00000987
Iteration 63/1000 | Loss: 0.00000987
Iteration 64/1000 | Loss: 0.00000987
Iteration 65/1000 | Loss: 0.00000987
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000983
Iteration 74/1000 | Loss: 0.00000983
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000981
Iteration 78/1000 | Loss: 0.00000981
Iteration 79/1000 | Loss: 0.00000981
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000979
Iteration 83/1000 | Loss: 0.00000979
Iteration 84/1000 | Loss: 0.00000978
Iteration 85/1000 | Loss: 0.00000978
Iteration 86/1000 | Loss: 0.00000978
Iteration 87/1000 | Loss: 0.00000978
Iteration 88/1000 | Loss: 0.00000978
Iteration 89/1000 | Loss: 0.00000978
Iteration 90/1000 | Loss: 0.00000977
Iteration 91/1000 | Loss: 0.00000977
Iteration 92/1000 | Loss: 0.00000977
Iteration 93/1000 | Loss: 0.00000977
Iteration 94/1000 | Loss: 0.00000977
Iteration 95/1000 | Loss: 0.00000977
Iteration 96/1000 | Loss: 0.00000976
Iteration 97/1000 | Loss: 0.00000976
Iteration 98/1000 | Loss: 0.00000976
Iteration 99/1000 | Loss: 0.00000976
Iteration 100/1000 | Loss: 0.00000976
Iteration 101/1000 | Loss: 0.00000976
Iteration 102/1000 | Loss: 0.00000976
Iteration 103/1000 | Loss: 0.00000975
Iteration 104/1000 | Loss: 0.00000975
Iteration 105/1000 | Loss: 0.00000975
Iteration 106/1000 | Loss: 0.00000975
Iteration 107/1000 | Loss: 0.00000975
Iteration 108/1000 | Loss: 0.00000975
Iteration 109/1000 | Loss: 0.00000975
Iteration 110/1000 | Loss: 0.00000975
Iteration 111/1000 | Loss: 0.00000975
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000974
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000974
Iteration 118/1000 | Loss: 0.00000974
Iteration 119/1000 | Loss: 0.00000974
Iteration 120/1000 | Loss: 0.00000974
Iteration 121/1000 | Loss: 0.00000974
Iteration 122/1000 | Loss: 0.00000974
Iteration 123/1000 | Loss: 0.00000974
Iteration 124/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [9.742640031618066e-06, 9.742640031618066e-06, 9.742640031618066e-06, 9.742640031618066e-06, 9.742640031618066e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.742640031618066e-06

Optimization complete. Final v2v error: 2.719322681427002 mm

Highest mean error: 3.0869526863098145 mm for frame 75

Lowest mean error: 2.5208582878112793 mm for frame 30

Saving results

Total time: 36.1780059337616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514700
Iteration 2/25 | Loss: 0.00165670
Iteration 3/25 | Loss: 0.00134103
Iteration 4/25 | Loss: 0.00132147
Iteration 5/25 | Loss: 0.00131587
Iteration 6/25 | Loss: 0.00131392
Iteration 7/25 | Loss: 0.00131335
Iteration 8/25 | Loss: 0.00131331
Iteration 9/25 | Loss: 0.00131331
Iteration 10/25 | Loss: 0.00131331
Iteration 11/25 | Loss: 0.00131331
Iteration 12/25 | Loss: 0.00131331
Iteration 13/25 | Loss: 0.00131331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013133096508681774, 0.0013133096508681774, 0.0013133096508681774, 0.0013133096508681774, 0.0013133096508681774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013133096508681774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22539771
Iteration 2/25 | Loss: 0.00184979
Iteration 3/25 | Loss: 0.00184978
Iteration 4/25 | Loss: 0.00184977
Iteration 5/25 | Loss: 0.00184977
Iteration 6/25 | Loss: 0.00184977
Iteration 7/25 | Loss: 0.00184977
Iteration 8/25 | Loss: 0.00184977
Iteration 9/25 | Loss: 0.00184977
Iteration 10/25 | Loss: 0.00184977
Iteration 11/25 | Loss: 0.00184977
Iteration 12/25 | Loss: 0.00184977
Iteration 13/25 | Loss: 0.00184977
Iteration 14/25 | Loss: 0.00184977
Iteration 15/25 | Loss: 0.00184977
Iteration 16/25 | Loss: 0.00184977
Iteration 17/25 | Loss: 0.00184977
Iteration 18/25 | Loss: 0.00184977
Iteration 19/25 | Loss: 0.00184977
Iteration 20/25 | Loss: 0.00184977
Iteration 21/25 | Loss: 0.00184977
Iteration 22/25 | Loss: 0.00184977
Iteration 23/25 | Loss: 0.00184977
Iteration 24/25 | Loss: 0.00184977
Iteration 25/25 | Loss: 0.00184977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184977
Iteration 2/1000 | Loss: 0.00005586
Iteration 3/1000 | Loss: 0.00003576
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002405
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002209
Iteration 8/1000 | Loss: 0.00002156
Iteration 9/1000 | Loss: 0.00002109
Iteration 10/1000 | Loss: 0.00002075
Iteration 11/1000 | Loss: 0.00002044
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00001997
Iteration 14/1000 | Loss: 0.00001991
Iteration 15/1000 | Loss: 0.00001973
Iteration 16/1000 | Loss: 0.00001956
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001926
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001904
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001896
Iteration 34/1000 | Loss: 0.00001896
Iteration 35/1000 | Loss: 0.00001896
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001896
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001895
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001895
Iteration 42/1000 | Loss: 0.00001895
Iteration 43/1000 | Loss: 0.00001895
Iteration 44/1000 | Loss: 0.00001895
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001894
Iteration 47/1000 | Loss: 0.00001894
Iteration 48/1000 | Loss: 0.00001894
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001890
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001889
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001887
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001887
Iteration 58/1000 | Loss: 0.00001887
Iteration 59/1000 | Loss: 0.00001887
Iteration 60/1000 | Loss: 0.00001887
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001886
Iteration 63/1000 | Loss: 0.00001886
Iteration 64/1000 | Loss: 0.00001885
Iteration 65/1000 | Loss: 0.00001885
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001884
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001882
Iteration 77/1000 | Loss: 0.00001882
Iteration 78/1000 | Loss: 0.00001882
Iteration 79/1000 | Loss: 0.00001882
Iteration 80/1000 | Loss: 0.00001882
Iteration 81/1000 | Loss: 0.00001882
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001881
Iteration 84/1000 | Loss: 0.00001881
Iteration 85/1000 | Loss: 0.00001881
Iteration 86/1000 | Loss: 0.00001881
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001880
Iteration 96/1000 | Loss: 0.00001880
Iteration 97/1000 | Loss: 0.00001880
Iteration 98/1000 | Loss: 0.00001880
Iteration 99/1000 | Loss: 0.00001880
Iteration 100/1000 | Loss: 0.00001880
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001879
Iteration 109/1000 | Loss: 0.00001879
Iteration 110/1000 | Loss: 0.00001879
Iteration 111/1000 | Loss: 0.00001879
Iteration 112/1000 | Loss: 0.00001879
Iteration 113/1000 | Loss: 0.00001879
Iteration 114/1000 | Loss: 0.00001879
Iteration 115/1000 | Loss: 0.00001879
Iteration 116/1000 | Loss: 0.00001878
Iteration 117/1000 | Loss: 0.00001878
Iteration 118/1000 | Loss: 0.00001878
Iteration 119/1000 | Loss: 0.00001878
Iteration 120/1000 | Loss: 0.00001878
Iteration 121/1000 | Loss: 0.00001878
Iteration 122/1000 | Loss: 0.00001878
Iteration 123/1000 | Loss: 0.00001878
Iteration 124/1000 | Loss: 0.00001878
Iteration 125/1000 | Loss: 0.00001878
Iteration 126/1000 | Loss: 0.00001878
Iteration 127/1000 | Loss: 0.00001878
Iteration 128/1000 | Loss: 0.00001878
Iteration 129/1000 | Loss: 0.00001878
Iteration 130/1000 | Loss: 0.00001878
Iteration 131/1000 | Loss: 0.00001878
Iteration 132/1000 | Loss: 0.00001878
Iteration 133/1000 | Loss: 0.00001878
Iteration 134/1000 | Loss: 0.00001878
Iteration 135/1000 | Loss: 0.00001878
Iteration 136/1000 | Loss: 0.00001878
Iteration 137/1000 | Loss: 0.00001878
Iteration 138/1000 | Loss: 0.00001878
Iteration 139/1000 | Loss: 0.00001878
Iteration 140/1000 | Loss: 0.00001878
Iteration 141/1000 | Loss: 0.00001878
Iteration 142/1000 | Loss: 0.00001878
Iteration 143/1000 | Loss: 0.00001878
Iteration 144/1000 | Loss: 0.00001878
Iteration 145/1000 | Loss: 0.00001878
Iteration 146/1000 | Loss: 0.00001878
Iteration 147/1000 | Loss: 0.00001878
Iteration 148/1000 | Loss: 0.00001878
Iteration 149/1000 | Loss: 0.00001878
Iteration 150/1000 | Loss: 0.00001878
Iteration 151/1000 | Loss: 0.00001878
Iteration 152/1000 | Loss: 0.00001878
Iteration 153/1000 | Loss: 0.00001878
Iteration 154/1000 | Loss: 0.00001878
Iteration 155/1000 | Loss: 0.00001878
Iteration 156/1000 | Loss: 0.00001878
Iteration 157/1000 | Loss: 0.00001878
Iteration 158/1000 | Loss: 0.00001878
Iteration 159/1000 | Loss: 0.00001878
Iteration 160/1000 | Loss: 0.00001878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.8783362975227647e-05, 1.8783362975227647e-05, 1.8783362975227647e-05, 1.8783362975227647e-05, 1.8783362975227647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8783362975227647e-05

Optimization complete. Final v2v error: 3.5308690071105957 mm

Highest mean error: 5.536798477172852 mm for frame 57

Lowest mean error: 2.6501076221466064 mm for frame 2

Saving results

Total time: 47.40789318084717
