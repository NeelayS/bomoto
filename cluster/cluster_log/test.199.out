Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=199, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11144-11199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467999
Iteration 2/25 | Loss: 0.00118818
Iteration 3/25 | Loss: 0.00095837
Iteration 4/25 | Loss: 0.00092290
Iteration 5/25 | Loss: 0.00091486
Iteration 6/25 | Loss: 0.00091294
Iteration 7/25 | Loss: 0.00091221
Iteration 8/25 | Loss: 0.00091215
Iteration 9/25 | Loss: 0.00091215
Iteration 10/25 | Loss: 0.00091215
Iteration 11/25 | Loss: 0.00091215
Iteration 12/25 | Loss: 0.00091215
Iteration 13/25 | Loss: 0.00091215
Iteration 14/25 | Loss: 0.00091215
Iteration 15/25 | Loss: 0.00091215
Iteration 16/25 | Loss: 0.00091215
Iteration 17/25 | Loss: 0.00091215
Iteration 18/25 | Loss: 0.00091215
Iteration 19/25 | Loss: 0.00091215
Iteration 20/25 | Loss: 0.00091215
Iteration 21/25 | Loss: 0.00091215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009121457696892321, 0.0009121457696892321, 0.0009121457696892321, 0.0009121457696892321, 0.0009121457696892321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009121457696892321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27069044
Iteration 2/25 | Loss: 0.00052637
Iteration 3/25 | Loss: 0.00052636
Iteration 4/25 | Loss: 0.00052636
Iteration 5/25 | Loss: 0.00052636
Iteration 6/25 | Loss: 0.00052636
Iteration 7/25 | Loss: 0.00052636
Iteration 8/25 | Loss: 0.00052636
Iteration 9/25 | Loss: 0.00052636
Iteration 10/25 | Loss: 0.00052636
Iteration 11/25 | Loss: 0.00052636
Iteration 12/25 | Loss: 0.00052636
Iteration 13/25 | Loss: 0.00052636
Iteration 14/25 | Loss: 0.00052636
Iteration 15/25 | Loss: 0.00052636
Iteration 16/25 | Loss: 0.00052636
Iteration 17/25 | Loss: 0.00052636
Iteration 18/25 | Loss: 0.00052636
Iteration 19/25 | Loss: 0.00052636
Iteration 20/25 | Loss: 0.00052636
Iteration 21/25 | Loss: 0.00052636
Iteration 22/25 | Loss: 0.00052636
Iteration 23/25 | Loss: 0.00052636
Iteration 24/25 | Loss: 0.00052636
Iteration 25/25 | Loss: 0.00052636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005263552302494645, 0.0005263552302494645, 0.0005263552302494645, 0.0005263552302494645, 0.0005263552302494645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005263552302494645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052636
Iteration 2/1000 | Loss: 0.00007242
Iteration 3/1000 | Loss: 0.00004685
Iteration 4/1000 | Loss: 0.00003936
Iteration 5/1000 | Loss: 0.00003701
Iteration 6/1000 | Loss: 0.00003562
Iteration 7/1000 | Loss: 0.00003460
Iteration 8/1000 | Loss: 0.00003399
Iteration 9/1000 | Loss: 0.00003340
Iteration 10/1000 | Loss: 0.00003292
Iteration 11/1000 | Loss: 0.00003261
Iteration 12/1000 | Loss: 0.00003233
Iteration 13/1000 | Loss: 0.00003210
Iteration 14/1000 | Loss: 0.00003187
Iteration 15/1000 | Loss: 0.00003167
Iteration 16/1000 | Loss: 0.00003148
Iteration 17/1000 | Loss: 0.00003147
Iteration 18/1000 | Loss: 0.00003134
Iteration 19/1000 | Loss: 0.00003132
Iteration 20/1000 | Loss: 0.00003128
Iteration 21/1000 | Loss: 0.00003121
Iteration 22/1000 | Loss: 0.00003118
Iteration 23/1000 | Loss: 0.00003117
Iteration 24/1000 | Loss: 0.00003117
Iteration 25/1000 | Loss: 0.00003116
Iteration 26/1000 | Loss: 0.00003116
Iteration 27/1000 | Loss: 0.00003115
Iteration 28/1000 | Loss: 0.00003115
Iteration 29/1000 | Loss: 0.00003115
Iteration 30/1000 | Loss: 0.00003114
Iteration 31/1000 | Loss: 0.00003109
Iteration 32/1000 | Loss: 0.00003106
Iteration 33/1000 | Loss: 0.00003104
Iteration 34/1000 | Loss: 0.00003100
Iteration 35/1000 | Loss: 0.00003097
Iteration 36/1000 | Loss: 0.00003093
Iteration 37/1000 | Loss: 0.00003092
Iteration 38/1000 | Loss: 0.00003091
Iteration 39/1000 | Loss: 0.00003088
Iteration 40/1000 | Loss: 0.00003087
Iteration 41/1000 | Loss: 0.00003086
Iteration 42/1000 | Loss: 0.00003085
Iteration 43/1000 | Loss: 0.00003085
Iteration 44/1000 | Loss: 0.00003085
Iteration 45/1000 | Loss: 0.00003084
Iteration 46/1000 | Loss: 0.00003084
Iteration 47/1000 | Loss: 0.00003084
Iteration 48/1000 | Loss: 0.00003083
Iteration 49/1000 | Loss: 0.00003083
Iteration 50/1000 | Loss: 0.00003083
Iteration 51/1000 | Loss: 0.00003082
Iteration 52/1000 | Loss: 0.00003082
Iteration 53/1000 | Loss: 0.00003082
Iteration 54/1000 | Loss: 0.00003081
Iteration 55/1000 | Loss: 0.00003081
Iteration 56/1000 | Loss: 0.00003080
Iteration 57/1000 | Loss: 0.00003080
Iteration 58/1000 | Loss: 0.00003080
Iteration 59/1000 | Loss: 0.00003079
Iteration 60/1000 | Loss: 0.00003079
Iteration 61/1000 | Loss: 0.00003079
Iteration 62/1000 | Loss: 0.00003078
Iteration 63/1000 | Loss: 0.00003078
Iteration 64/1000 | Loss: 0.00003078
Iteration 65/1000 | Loss: 0.00003077
Iteration 66/1000 | Loss: 0.00003077
Iteration 67/1000 | Loss: 0.00003077
Iteration 68/1000 | Loss: 0.00003077
Iteration 69/1000 | Loss: 0.00003076
Iteration 70/1000 | Loss: 0.00003076
Iteration 71/1000 | Loss: 0.00003076
Iteration 72/1000 | Loss: 0.00003076
Iteration 73/1000 | Loss: 0.00003076
Iteration 74/1000 | Loss: 0.00003075
Iteration 75/1000 | Loss: 0.00003075
Iteration 76/1000 | Loss: 0.00003075
Iteration 77/1000 | Loss: 0.00003074
Iteration 78/1000 | Loss: 0.00003074
Iteration 79/1000 | Loss: 0.00003074
Iteration 80/1000 | Loss: 0.00003074
Iteration 81/1000 | Loss: 0.00003074
Iteration 82/1000 | Loss: 0.00003074
Iteration 83/1000 | Loss: 0.00003074
Iteration 84/1000 | Loss: 0.00003074
Iteration 85/1000 | Loss: 0.00003073
Iteration 86/1000 | Loss: 0.00003073
Iteration 87/1000 | Loss: 0.00003073
Iteration 88/1000 | Loss: 0.00003072
Iteration 89/1000 | Loss: 0.00003072
Iteration 90/1000 | Loss: 0.00003072
Iteration 91/1000 | Loss: 0.00003072
Iteration 92/1000 | Loss: 0.00003072
Iteration 93/1000 | Loss: 0.00003072
Iteration 94/1000 | Loss: 0.00003071
Iteration 95/1000 | Loss: 0.00003071
Iteration 96/1000 | Loss: 0.00003071
Iteration 97/1000 | Loss: 0.00003070
Iteration 98/1000 | Loss: 0.00003070
Iteration 99/1000 | Loss: 0.00003070
Iteration 100/1000 | Loss: 0.00003070
Iteration 101/1000 | Loss: 0.00003069
Iteration 102/1000 | Loss: 0.00003069
Iteration 103/1000 | Loss: 0.00003069
Iteration 104/1000 | Loss: 0.00003069
Iteration 105/1000 | Loss: 0.00003069
Iteration 106/1000 | Loss: 0.00003069
Iteration 107/1000 | Loss: 0.00003068
Iteration 108/1000 | Loss: 0.00003068
Iteration 109/1000 | Loss: 0.00003068
Iteration 110/1000 | Loss: 0.00003068
Iteration 111/1000 | Loss: 0.00003068
Iteration 112/1000 | Loss: 0.00003067
Iteration 113/1000 | Loss: 0.00003067
Iteration 114/1000 | Loss: 0.00003067
Iteration 115/1000 | Loss: 0.00003067
Iteration 116/1000 | Loss: 0.00003066
Iteration 117/1000 | Loss: 0.00003066
Iteration 118/1000 | Loss: 0.00003066
Iteration 119/1000 | Loss: 0.00003066
Iteration 120/1000 | Loss: 0.00003066
Iteration 121/1000 | Loss: 0.00003065
Iteration 122/1000 | Loss: 0.00003065
Iteration 123/1000 | Loss: 0.00003065
Iteration 124/1000 | Loss: 0.00003065
Iteration 125/1000 | Loss: 0.00003065
Iteration 126/1000 | Loss: 0.00003065
Iteration 127/1000 | Loss: 0.00003065
Iteration 128/1000 | Loss: 0.00003065
Iteration 129/1000 | Loss: 0.00003064
Iteration 130/1000 | Loss: 0.00003064
Iteration 131/1000 | Loss: 0.00003064
Iteration 132/1000 | Loss: 0.00003064
Iteration 133/1000 | Loss: 0.00003064
Iteration 134/1000 | Loss: 0.00003064
Iteration 135/1000 | Loss: 0.00003064
Iteration 136/1000 | Loss: 0.00003064
Iteration 137/1000 | Loss: 0.00003064
Iteration 138/1000 | Loss: 0.00003064
Iteration 139/1000 | Loss: 0.00003063
Iteration 140/1000 | Loss: 0.00003063
Iteration 141/1000 | Loss: 0.00003063
Iteration 142/1000 | Loss: 0.00003063
Iteration 143/1000 | Loss: 0.00003063
Iteration 144/1000 | Loss: 0.00003063
Iteration 145/1000 | Loss: 0.00003063
Iteration 146/1000 | Loss: 0.00003063
Iteration 147/1000 | Loss: 0.00003062
Iteration 148/1000 | Loss: 0.00003062
Iteration 149/1000 | Loss: 0.00003062
Iteration 150/1000 | Loss: 0.00003062
Iteration 151/1000 | Loss: 0.00003062
Iteration 152/1000 | Loss: 0.00003062
Iteration 153/1000 | Loss: 0.00003062
Iteration 154/1000 | Loss: 0.00003062
Iteration 155/1000 | Loss: 0.00003061
Iteration 156/1000 | Loss: 0.00003061
Iteration 157/1000 | Loss: 0.00003061
Iteration 158/1000 | Loss: 0.00003061
Iteration 159/1000 | Loss: 0.00003061
Iteration 160/1000 | Loss: 0.00003061
Iteration 161/1000 | Loss: 0.00003061
Iteration 162/1000 | Loss: 0.00003061
Iteration 163/1000 | Loss: 0.00003061
Iteration 164/1000 | Loss: 0.00003061
Iteration 165/1000 | Loss: 0.00003060
Iteration 166/1000 | Loss: 0.00003060
Iteration 167/1000 | Loss: 0.00003060
Iteration 168/1000 | Loss: 0.00003060
Iteration 169/1000 | Loss: 0.00003060
Iteration 170/1000 | Loss: 0.00003060
Iteration 171/1000 | Loss: 0.00003060
Iteration 172/1000 | Loss: 0.00003060
Iteration 173/1000 | Loss: 0.00003060
Iteration 174/1000 | Loss: 0.00003060
Iteration 175/1000 | Loss: 0.00003060
Iteration 176/1000 | Loss: 0.00003060
Iteration 177/1000 | Loss: 0.00003060
Iteration 178/1000 | Loss: 0.00003060
Iteration 179/1000 | Loss: 0.00003060
Iteration 180/1000 | Loss: 0.00003060
Iteration 181/1000 | Loss: 0.00003060
Iteration 182/1000 | Loss: 0.00003060
Iteration 183/1000 | Loss: 0.00003060
Iteration 184/1000 | Loss: 0.00003060
Iteration 185/1000 | Loss: 0.00003060
Iteration 186/1000 | Loss: 0.00003060
Iteration 187/1000 | Loss: 0.00003060
Iteration 188/1000 | Loss: 0.00003060
Iteration 189/1000 | Loss: 0.00003060
Iteration 190/1000 | Loss: 0.00003060
Iteration 191/1000 | Loss: 0.00003060
Iteration 192/1000 | Loss: 0.00003060
Iteration 193/1000 | Loss: 0.00003060
Iteration 194/1000 | Loss: 0.00003060
Iteration 195/1000 | Loss: 0.00003060
Iteration 196/1000 | Loss: 0.00003060
Iteration 197/1000 | Loss: 0.00003060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [3.060320159420371e-05, 3.060320159420371e-05, 3.060320159420371e-05, 3.060320159420371e-05, 3.060320159420371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.060320159420371e-05

Optimization complete. Final v2v error: 4.471438884735107 mm

Highest mean error: 6.096134185791016 mm for frame 74

Lowest mean error: 3.4388587474823 mm for frame 35

Saving results

Total time: 53.33975863456726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368441
Iteration 2/25 | Loss: 0.00102725
Iteration 3/25 | Loss: 0.00086417
Iteration 4/25 | Loss: 0.00084085
Iteration 5/25 | Loss: 0.00083610
Iteration 6/25 | Loss: 0.00083507
Iteration 7/25 | Loss: 0.00083505
Iteration 8/25 | Loss: 0.00083505
Iteration 9/25 | Loss: 0.00083505
Iteration 10/25 | Loss: 0.00083505
Iteration 11/25 | Loss: 0.00083505
Iteration 12/25 | Loss: 0.00083505
Iteration 13/25 | Loss: 0.00083505
Iteration 14/25 | Loss: 0.00083505
Iteration 15/25 | Loss: 0.00083505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008350479765795171, 0.0008350479765795171, 0.0008350479765795171, 0.0008350479765795171, 0.0008350479765795171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008350479765795171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91576874
Iteration 2/25 | Loss: 0.00065581
Iteration 3/25 | Loss: 0.00065581
Iteration 4/25 | Loss: 0.00065581
Iteration 5/25 | Loss: 0.00065581
Iteration 6/25 | Loss: 0.00065581
Iteration 7/25 | Loss: 0.00065581
Iteration 8/25 | Loss: 0.00065581
Iteration 9/25 | Loss: 0.00065581
Iteration 10/25 | Loss: 0.00065581
Iteration 11/25 | Loss: 0.00065581
Iteration 12/25 | Loss: 0.00065581
Iteration 13/25 | Loss: 0.00065581
Iteration 14/25 | Loss: 0.00065581
Iteration 15/25 | Loss: 0.00065581
Iteration 16/25 | Loss: 0.00065581
Iteration 17/25 | Loss: 0.00065581
Iteration 18/25 | Loss: 0.00065581
Iteration 19/25 | Loss: 0.00065581
Iteration 20/25 | Loss: 0.00065581
Iteration 21/25 | Loss: 0.00065581
Iteration 22/25 | Loss: 0.00065581
Iteration 23/25 | Loss: 0.00065581
Iteration 24/25 | Loss: 0.00065581
Iteration 25/25 | Loss: 0.00065581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065581
Iteration 2/1000 | Loss: 0.00002676
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001877
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001614
Iteration 9/1000 | Loss: 0.00001586
Iteration 10/1000 | Loss: 0.00001575
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001502
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001498
Iteration 23/1000 | Loss: 0.00001498
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001496
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001490
Iteration 49/1000 | Loss: 0.00001490
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001487
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001486
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001482
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001479
Iteration 92/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.4789186025154777e-05, 1.4789186025154777e-05, 1.4789186025154777e-05, 1.4789186025154777e-05, 1.4789186025154777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4789186025154777e-05

Optimization complete. Final v2v error: 3.2854881286621094 mm

Highest mean error: 3.829275131225586 mm for frame 75

Lowest mean error: 2.8915157318115234 mm for frame 230

Saving results

Total time: 39.07719683647156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050193
Iteration 2/25 | Loss: 0.00233038
Iteration 3/25 | Loss: 0.00139432
Iteration 4/25 | Loss: 0.00115644
Iteration 5/25 | Loss: 0.00106448
Iteration 6/25 | Loss: 0.00103550
Iteration 7/25 | Loss: 0.00101113
Iteration 8/25 | Loss: 0.00100782
Iteration 9/25 | Loss: 0.00100354
Iteration 10/25 | Loss: 0.00099625
Iteration 11/25 | Loss: 0.00099397
Iteration 12/25 | Loss: 0.00099508
Iteration 13/25 | Loss: 0.00099434
Iteration 14/25 | Loss: 0.00098859
Iteration 15/25 | Loss: 0.00098518
Iteration 16/25 | Loss: 0.00097716
Iteration 17/25 | Loss: 0.00098099
Iteration 18/25 | Loss: 0.00100059
Iteration 19/25 | Loss: 0.00099635
Iteration 20/25 | Loss: 0.00097741
Iteration 21/25 | Loss: 0.00097314
Iteration 22/25 | Loss: 0.00096350
Iteration 23/25 | Loss: 0.00095104
Iteration 24/25 | Loss: 0.00095283
Iteration 25/25 | Loss: 0.00094784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55745459
Iteration 2/25 | Loss: 0.00337923
Iteration 3/25 | Loss: 0.00131329
Iteration 4/25 | Loss: 0.00131329
Iteration 5/25 | Loss: 0.00131329
Iteration 6/25 | Loss: 0.00131329
Iteration 7/25 | Loss: 0.00131329
Iteration 8/25 | Loss: 0.00131329
Iteration 9/25 | Loss: 0.00131329
Iteration 10/25 | Loss: 0.00131329
Iteration 11/25 | Loss: 0.00131329
Iteration 12/25 | Loss: 0.00131329
Iteration 13/25 | Loss: 0.00131329
Iteration 14/25 | Loss: 0.00131329
Iteration 15/25 | Loss: 0.00131329
Iteration 16/25 | Loss: 0.00131329
Iteration 17/25 | Loss: 0.00131329
Iteration 18/25 | Loss: 0.00131329
Iteration 19/25 | Loss: 0.00131329
Iteration 20/25 | Loss: 0.00131329
Iteration 21/25 | Loss: 0.00131329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013132881140336394, 0.0013132881140336394, 0.0013132881140336394, 0.0013132881140336394, 0.0013132881140336394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013132881140336394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131329
Iteration 2/1000 | Loss: 0.00045469
Iteration 3/1000 | Loss: 0.00044191
Iteration 4/1000 | Loss: 0.00054211
Iteration 5/1000 | Loss: 0.00052141
Iteration 6/1000 | Loss: 0.00032907
Iteration 7/1000 | Loss: 0.00040627
Iteration 8/1000 | Loss: 0.00028540
Iteration 9/1000 | Loss: 0.00037086
Iteration 10/1000 | Loss: 0.00044628
Iteration 11/1000 | Loss: 0.00046435
Iteration 12/1000 | Loss: 0.00096906
Iteration 13/1000 | Loss: 0.00108295
Iteration 14/1000 | Loss: 0.00099843
Iteration 15/1000 | Loss: 0.00109529
Iteration 16/1000 | Loss: 0.00046626
Iteration 17/1000 | Loss: 0.00058151
Iteration 18/1000 | Loss: 0.00026264
Iteration 19/1000 | Loss: 0.00104324
Iteration 20/1000 | Loss: 0.00114312
Iteration 21/1000 | Loss: 0.00070406
Iteration 22/1000 | Loss: 0.00021648
Iteration 23/1000 | Loss: 0.00236267
Iteration 24/1000 | Loss: 0.00139932
Iteration 25/1000 | Loss: 0.00138991
Iteration 26/1000 | Loss: 0.00058834
Iteration 27/1000 | Loss: 0.00102447
Iteration 28/1000 | Loss: 0.00110639
Iteration 29/1000 | Loss: 0.00258248
Iteration 30/1000 | Loss: 0.00159748
Iteration 31/1000 | Loss: 0.00279177
Iteration 32/1000 | Loss: 0.00191352
Iteration 33/1000 | Loss: 0.00208151
Iteration 34/1000 | Loss: 0.00227134
Iteration 35/1000 | Loss: 0.00237702
Iteration 36/1000 | Loss: 0.00213141
Iteration 37/1000 | Loss: 0.00191983
Iteration 38/1000 | Loss: 0.00319639
Iteration 39/1000 | Loss: 0.00211421
Iteration 40/1000 | Loss: 0.00241590
Iteration 41/1000 | Loss: 0.00218249
Iteration 42/1000 | Loss: 0.00174550
Iteration 43/1000 | Loss: 0.00164394
Iteration 44/1000 | Loss: 0.00205861
Iteration 45/1000 | Loss: 0.00167290
Iteration 46/1000 | Loss: 0.00168332
Iteration 47/1000 | Loss: 0.00143379
Iteration 48/1000 | Loss: 0.00126243
Iteration 49/1000 | Loss: 0.00059319
Iteration 50/1000 | Loss: 0.00157693
Iteration 51/1000 | Loss: 0.00523296
Iteration 52/1000 | Loss: 0.00266412
Iteration 53/1000 | Loss: 0.00151565
Iteration 54/1000 | Loss: 0.00171473
Iteration 55/1000 | Loss: 0.00139840
Iteration 56/1000 | Loss: 0.00113152
Iteration 57/1000 | Loss: 0.00073617
Iteration 58/1000 | Loss: 0.00080018
Iteration 59/1000 | Loss: 0.00047680
Iteration 60/1000 | Loss: 0.00024762
Iteration 61/1000 | Loss: 0.00022271
Iteration 62/1000 | Loss: 0.00035316
Iteration 63/1000 | Loss: 0.00127423
Iteration 64/1000 | Loss: 0.00120102
Iteration 65/1000 | Loss: 0.00494172
Iteration 66/1000 | Loss: 0.00189344
Iteration 67/1000 | Loss: 0.00105949
Iteration 68/1000 | Loss: 0.00059440
Iteration 69/1000 | Loss: 0.00032335
Iteration 70/1000 | Loss: 0.00134185
Iteration 71/1000 | Loss: 0.00134878
Iteration 72/1000 | Loss: 0.00081540
Iteration 73/1000 | Loss: 0.00077054
Iteration 74/1000 | Loss: 0.00070774
Iteration 75/1000 | Loss: 0.00065669
Iteration 76/1000 | Loss: 0.00193921
Iteration 77/1000 | Loss: 0.00111639
Iteration 78/1000 | Loss: 0.00139270
Iteration 79/1000 | Loss: 0.00108897
Iteration 80/1000 | Loss: 0.00118238
Iteration 81/1000 | Loss: 0.00103832
Iteration 82/1000 | Loss: 0.00160963
Iteration 83/1000 | Loss: 0.00116213
Iteration 84/1000 | Loss: 0.00040704
Iteration 85/1000 | Loss: 0.00021690
Iteration 86/1000 | Loss: 0.00017920
Iteration 87/1000 | Loss: 0.00020323
Iteration 88/1000 | Loss: 0.00027932
Iteration 89/1000 | Loss: 0.00020104
Iteration 90/1000 | Loss: 0.00032249
Iteration 91/1000 | Loss: 0.00018015
Iteration 92/1000 | Loss: 0.00034359
Iteration 93/1000 | Loss: 0.00041522
Iteration 94/1000 | Loss: 0.00037576
Iteration 95/1000 | Loss: 0.00025056
Iteration 96/1000 | Loss: 0.00038304
Iteration 97/1000 | Loss: 0.00061192
Iteration 98/1000 | Loss: 0.00045828
Iteration 99/1000 | Loss: 0.00060730
Iteration 100/1000 | Loss: 0.00052098
Iteration 101/1000 | Loss: 0.00042921
Iteration 102/1000 | Loss: 0.00055455
Iteration 103/1000 | Loss: 0.00046158
Iteration 104/1000 | Loss: 0.00036739
Iteration 105/1000 | Loss: 0.00058549
Iteration 106/1000 | Loss: 0.00037655
Iteration 107/1000 | Loss: 0.00042884
Iteration 108/1000 | Loss: 0.00059047
Iteration 109/1000 | Loss: 0.00103455
Iteration 110/1000 | Loss: 0.00025181
Iteration 111/1000 | Loss: 0.00031892
Iteration 112/1000 | Loss: 0.00016730
Iteration 113/1000 | Loss: 0.00016789
Iteration 114/1000 | Loss: 0.00017029
Iteration 115/1000 | Loss: 0.00019936
Iteration 116/1000 | Loss: 0.00017354
Iteration 117/1000 | Loss: 0.00018245
Iteration 118/1000 | Loss: 0.00020229
Iteration 119/1000 | Loss: 0.00016572
Iteration 120/1000 | Loss: 0.00016819
Iteration 121/1000 | Loss: 0.00015859
Iteration 122/1000 | Loss: 0.00015857
Iteration 123/1000 | Loss: 0.00015748
Iteration 124/1000 | Loss: 0.00016553
Iteration 125/1000 | Loss: 0.00016540
Iteration 126/1000 | Loss: 0.00015465
Iteration 127/1000 | Loss: 0.00008600
Iteration 128/1000 | Loss: 0.00014748
Iteration 129/1000 | Loss: 0.00064454
Iteration 130/1000 | Loss: 0.00115395
Iteration 131/1000 | Loss: 0.00086541
Iteration 132/1000 | Loss: 0.00057572
Iteration 133/1000 | Loss: 0.00039980
Iteration 134/1000 | Loss: 0.00031456
Iteration 135/1000 | Loss: 0.00015480
Iteration 136/1000 | Loss: 0.00022423
Iteration 137/1000 | Loss: 0.00003808
Iteration 138/1000 | Loss: 0.00003155
Iteration 139/1000 | Loss: 0.00002889
Iteration 140/1000 | Loss: 0.00002774
Iteration 141/1000 | Loss: 0.00002671
Iteration 142/1000 | Loss: 0.00002600
Iteration 143/1000 | Loss: 0.00002518
Iteration 144/1000 | Loss: 0.00002471
Iteration 145/1000 | Loss: 0.00002432
Iteration 146/1000 | Loss: 0.00002404
Iteration 147/1000 | Loss: 0.00002398
Iteration 148/1000 | Loss: 0.00002377
Iteration 149/1000 | Loss: 0.00002365
Iteration 150/1000 | Loss: 0.00002349
Iteration 151/1000 | Loss: 0.00002348
Iteration 152/1000 | Loss: 0.00002348
Iteration 153/1000 | Loss: 0.00002347
Iteration 154/1000 | Loss: 0.00002346
Iteration 155/1000 | Loss: 0.00002346
Iteration 156/1000 | Loss: 0.00002346
Iteration 157/1000 | Loss: 0.00002345
Iteration 158/1000 | Loss: 0.00002344
Iteration 159/1000 | Loss: 0.00002338
Iteration 160/1000 | Loss: 0.00002338
Iteration 161/1000 | Loss: 0.00002336
Iteration 162/1000 | Loss: 0.00002335
Iteration 163/1000 | Loss: 0.00002335
Iteration 164/1000 | Loss: 0.00002334
Iteration 165/1000 | Loss: 0.00002334
Iteration 166/1000 | Loss: 0.00002334
Iteration 167/1000 | Loss: 0.00002333
Iteration 168/1000 | Loss: 0.00002333
Iteration 169/1000 | Loss: 0.00002332
Iteration 170/1000 | Loss: 0.00002332
Iteration 171/1000 | Loss: 0.00002331
Iteration 172/1000 | Loss: 0.00002330
Iteration 173/1000 | Loss: 0.00002330
Iteration 174/1000 | Loss: 0.00002329
Iteration 175/1000 | Loss: 0.00002329
Iteration 176/1000 | Loss: 0.00002329
Iteration 177/1000 | Loss: 0.00002329
Iteration 178/1000 | Loss: 0.00002328
Iteration 179/1000 | Loss: 0.00002328
Iteration 180/1000 | Loss: 0.00002326
Iteration 181/1000 | Loss: 0.00002326
Iteration 182/1000 | Loss: 0.00002326
Iteration 183/1000 | Loss: 0.00002326
Iteration 184/1000 | Loss: 0.00002326
Iteration 185/1000 | Loss: 0.00002325
Iteration 186/1000 | Loss: 0.00002325
Iteration 187/1000 | Loss: 0.00002325
Iteration 188/1000 | Loss: 0.00002325
Iteration 189/1000 | Loss: 0.00002324
Iteration 190/1000 | Loss: 0.00002324
Iteration 191/1000 | Loss: 0.00002323
Iteration 192/1000 | Loss: 0.00002323
Iteration 193/1000 | Loss: 0.00002323
Iteration 194/1000 | Loss: 0.00002323
Iteration 195/1000 | Loss: 0.00002322
Iteration 196/1000 | Loss: 0.00002322
Iteration 197/1000 | Loss: 0.00002322
Iteration 198/1000 | Loss: 0.00002321
Iteration 199/1000 | Loss: 0.00002321
Iteration 200/1000 | Loss: 0.00002321
Iteration 201/1000 | Loss: 0.00002321
Iteration 202/1000 | Loss: 0.00002320
Iteration 203/1000 | Loss: 0.00002320
Iteration 204/1000 | Loss: 0.00002320
Iteration 205/1000 | Loss: 0.00002320
Iteration 206/1000 | Loss: 0.00002319
Iteration 207/1000 | Loss: 0.00002319
Iteration 208/1000 | Loss: 0.00002319
Iteration 209/1000 | Loss: 0.00002318
Iteration 210/1000 | Loss: 0.00002318
Iteration 211/1000 | Loss: 0.00002318
Iteration 212/1000 | Loss: 0.00002317
Iteration 213/1000 | Loss: 0.00002317
Iteration 214/1000 | Loss: 0.00002317
Iteration 215/1000 | Loss: 0.00002317
Iteration 216/1000 | Loss: 0.00002316
Iteration 217/1000 | Loss: 0.00002316
Iteration 218/1000 | Loss: 0.00002315
Iteration 219/1000 | Loss: 0.00002315
Iteration 220/1000 | Loss: 0.00002314
Iteration 221/1000 | Loss: 0.00002314
Iteration 222/1000 | Loss: 0.00002313
Iteration 223/1000 | Loss: 0.00002312
Iteration 224/1000 | Loss: 0.00002312
Iteration 225/1000 | Loss: 0.00002312
Iteration 226/1000 | Loss: 0.00002312
Iteration 227/1000 | Loss: 0.00002312
Iteration 228/1000 | Loss: 0.00002312
Iteration 229/1000 | Loss: 0.00002311
Iteration 230/1000 | Loss: 0.00002311
Iteration 231/1000 | Loss: 0.00002311
Iteration 232/1000 | Loss: 0.00002311
Iteration 233/1000 | Loss: 0.00002310
Iteration 234/1000 | Loss: 0.00002310
Iteration 235/1000 | Loss: 0.00002309
Iteration 236/1000 | Loss: 0.00002309
Iteration 237/1000 | Loss: 0.00002308
Iteration 238/1000 | Loss: 0.00002308
Iteration 239/1000 | Loss: 0.00002308
Iteration 240/1000 | Loss: 0.00002307
Iteration 241/1000 | Loss: 0.00002307
Iteration 242/1000 | Loss: 0.00002307
Iteration 243/1000 | Loss: 0.00002306
Iteration 244/1000 | Loss: 0.00002306
Iteration 245/1000 | Loss: 0.00002306
Iteration 246/1000 | Loss: 0.00002306
Iteration 247/1000 | Loss: 0.00002306
Iteration 248/1000 | Loss: 0.00002305
Iteration 249/1000 | Loss: 0.00002305
Iteration 250/1000 | Loss: 0.00002305
Iteration 251/1000 | Loss: 0.00002304
Iteration 252/1000 | Loss: 0.00002304
Iteration 253/1000 | Loss: 0.00002304
Iteration 254/1000 | Loss: 0.00002304
Iteration 255/1000 | Loss: 0.00002303
Iteration 256/1000 | Loss: 0.00002303
Iteration 257/1000 | Loss: 0.00002303
Iteration 258/1000 | Loss: 0.00002303
Iteration 259/1000 | Loss: 0.00002303
Iteration 260/1000 | Loss: 0.00002303
Iteration 261/1000 | Loss: 0.00002303
Iteration 262/1000 | Loss: 0.00002303
Iteration 263/1000 | Loss: 0.00002302
Iteration 264/1000 | Loss: 0.00002302
Iteration 265/1000 | Loss: 0.00002302
Iteration 266/1000 | Loss: 0.00002302
Iteration 267/1000 | Loss: 0.00002302
Iteration 268/1000 | Loss: 0.00002302
Iteration 269/1000 | Loss: 0.00002302
Iteration 270/1000 | Loss: 0.00002302
Iteration 271/1000 | Loss: 0.00002302
Iteration 272/1000 | Loss: 0.00002302
Iteration 273/1000 | Loss: 0.00002302
Iteration 274/1000 | Loss: 0.00002302
Iteration 275/1000 | Loss: 0.00002302
Iteration 276/1000 | Loss: 0.00002301
Iteration 277/1000 | Loss: 0.00002301
Iteration 278/1000 | Loss: 0.00002301
Iteration 279/1000 | Loss: 0.00002301
Iteration 280/1000 | Loss: 0.00002301
Iteration 281/1000 | Loss: 0.00002301
Iteration 282/1000 | Loss: 0.00002301
Iteration 283/1000 | Loss: 0.00002301
Iteration 284/1000 | Loss: 0.00002301
Iteration 285/1000 | Loss: 0.00002301
Iteration 286/1000 | Loss: 0.00002301
Iteration 287/1000 | Loss: 0.00002300
Iteration 288/1000 | Loss: 0.00002300
Iteration 289/1000 | Loss: 0.00002300
Iteration 290/1000 | Loss: 0.00002300
Iteration 291/1000 | Loss: 0.00002300
Iteration 292/1000 | Loss: 0.00002300
Iteration 293/1000 | Loss: 0.00002300
Iteration 294/1000 | Loss: 0.00002300
Iteration 295/1000 | Loss: 0.00002300
Iteration 296/1000 | Loss: 0.00002300
Iteration 297/1000 | Loss: 0.00002300
Iteration 298/1000 | Loss: 0.00002300
Iteration 299/1000 | Loss: 0.00002300
Iteration 300/1000 | Loss: 0.00002300
Iteration 301/1000 | Loss: 0.00002299
Iteration 302/1000 | Loss: 0.00002299
Iteration 303/1000 | Loss: 0.00002299
Iteration 304/1000 | Loss: 0.00002299
Iteration 305/1000 | Loss: 0.00002299
Iteration 306/1000 | Loss: 0.00002299
Iteration 307/1000 | Loss: 0.00002299
Iteration 308/1000 | Loss: 0.00002299
Iteration 309/1000 | Loss: 0.00002298
Iteration 310/1000 | Loss: 0.00002298
Iteration 311/1000 | Loss: 0.00002298
Iteration 312/1000 | Loss: 0.00002298
Iteration 313/1000 | Loss: 0.00002298
Iteration 314/1000 | Loss: 0.00002298
Iteration 315/1000 | Loss: 0.00002298
Iteration 316/1000 | Loss: 0.00002298
Iteration 317/1000 | Loss: 0.00002298
Iteration 318/1000 | Loss: 0.00002298
Iteration 319/1000 | Loss: 0.00002298
Iteration 320/1000 | Loss: 0.00002298
Iteration 321/1000 | Loss: 0.00002298
Iteration 322/1000 | Loss: 0.00002297
Iteration 323/1000 | Loss: 0.00002297
Iteration 324/1000 | Loss: 0.00002297
Iteration 325/1000 | Loss: 0.00002297
Iteration 326/1000 | Loss: 0.00002297
Iteration 327/1000 | Loss: 0.00002297
Iteration 328/1000 | Loss: 0.00002297
Iteration 329/1000 | Loss: 0.00002297
Iteration 330/1000 | Loss: 0.00002296
Iteration 331/1000 | Loss: 0.00002296
Iteration 332/1000 | Loss: 0.00002296
Iteration 333/1000 | Loss: 0.00002296
Iteration 334/1000 | Loss: 0.00002296
Iteration 335/1000 | Loss: 0.00002296
Iteration 336/1000 | Loss: 0.00002296
Iteration 337/1000 | Loss: 0.00002296
Iteration 338/1000 | Loss: 0.00002296
Iteration 339/1000 | Loss: 0.00002296
Iteration 340/1000 | Loss: 0.00002296
Iteration 341/1000 | Loss: 0.00002295
Iteration 342/1000 | Loss: 0.00002295
Iteration 343/1000 | Loss: 0.00002295
Iteration 344/1000 | Loss: 0.00002295
Iteration 345/1000 | Loss: 0.00002295
Iteration 346/1000 | Loss: 0.00002295
Iteration 347/1000 | Loss: 0.00002295
Iteration 348/1000 | Loss: 0.00002295
Iteration 349/1000 | Loss: 0.00002295
Iteration 350/1000 | Loss: 0.00002295
Iteration 351/1000 | Loss: 0.00002295
Iteration 352/1000 | Loss: 0.00002295
Iteration 353/1000 | Loss: 0.00002294
Iteration 354/1000 | Loss: 0.00002294
Iteration 355/1000 | Loss: 0.00002294
Iteration 356/1000 | Loss: 0.00002294
Iteration 357/1000 | Loss: 0.00002294
Iteration 358/1000 | Loss: 0.00002294
Iteration 359/1000 | Loss: 0.00002294
Iteration 360/1000 | Loss: 0.00002294
Iteration 361/1000 | Loss: 0.00002294
Iteration 362/1000 | Loss: 0.00002294
Iteration 363/1000 | Loss: 0.00002294
Iteration 364/1000 | Loss: 0.00002294
Iteration 365/1000 | Loss: 0.00002294
Iteration 366/1000 | Loss: 0.00002294
Iteration 367/1000 | Loss: 0.00002294
Iteration 368/1000 | Loss: 0.00002293
Iteration 369/1000 | Loss: 0.00002293
Iteration 370/1000 | Loss: 0.00002293
Iteration 371/1000 | Loss: 0.00002293
Iteration 372/1000 | Loss: 0.00002293
Iteration 373/1000 | Loss: 0.00002293
Iteration 374/1000 | Loss: 0.00002293
Iteration 375/1000 | Loss: 0.00002293
Iteration 376/1000 | Loss: 0.00002293
Iteration 377/1000 | Loss: 0.00002293
Iteration 378/1000 | Loss: 0.00002293
Iteration 379/1000 | Loss: 0.00002293
Iteration 380/1000 | Loss: 0.00002293
Iteration 381/1000 | Loss: 0.00002293
Iteration 382/1000 | Loss: 0.00002292
Iteration 383/1000 | Loss: 0.00002292
Iteration 384/1000 | Loss: 0.00002292
Iteration 385/1000 | Loss: 0.00002292
Iteration 386/1000 | Loss: 0.00002292
Iteration 387/1000 | Loss: 0.00002292
Iteration 388/1000 | Loss: 0.00002292
Iteration 389/1000 | Loss: 0.00002292
Iteration 390/1000 | Loss: 0.00002292
Iteration 391/1000 | Loss: 0.00002292
Iteration 392/1000 | Loss: 0.00002292
Iteration 393/1000 | Loss: 0.00002292
Iteration 394/1000 | Loss: 0.00002292
Iteration 395/1000 | Loss: 0.00002292
Iteration 396/1000 | Loss: 0.00002292
Iteration 397/1000 | Loss: 0.00002292
Iteration 398/1000 | Loss: 0.00002292
Iteration 399/1000 | Loss: 0.00002292
Iteration 400/1000 | Loss: 0.00002291
Iteration 401/1000 | Loss: 0.00002291
Iteration 402/1000 | Loss: 0.00002291
Iteration 403/1000 | Loss: 0.00002291
Iteration 404/1000 | Loss: 0.00002291
Iteration 405/1000 | Loss: 0.00002291
Iteration 406/1000 | Loss: 0.00002291
Iteration 407/1000 | Loss: 0.00002291
Iteration 408/1000 | Loss: 0.00002291
Iteration 409/1000 | Loss: 0.00002291
Iteration 410/1000 | Loss: 0.00002291
Iteration 411/1000 | Loss: 0.00002291
Iteration 412/1000 | Loss: 0.00002291
Iteration 413/1000 | Loss: 0.00002291
Iteration 414/1000 | Loss: 0.00002291
Iteration 415/1000 | Loss: 0.00002291
Iteration 416/1000 | Loss: 0.00002291
Iteration 417/1000 | Loss: 0.00002291
Iteration 418/1000 | Loss: 0.00002291
Iteration 419/1000 | Loss: 0.00002291
Iteration 420/1000 | Loss: 0.00002291
Iteration 421/1000 | Loss: 0.00002291
Iteration 422/1000 | Loss: 0.00002290
Iteration 423/1000 | Loss: 0.00002290
Iteration 424/1000 | Loss: 0.00002290
Iteration 425/1000 | Loss: 0.00002290
Iteration 426/1000 | Loss: 0.00002290
Iteration 427/1000 | Loss: 0.00002290
Iteration 428/1000 | Loss: 0.00002290
Iteration 429/1000 | Loss: 0.00002290
Iteration 430/1000 | Loss: 0.00002290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 430. Stopping optimization.
Last 5 losses: [2.290418342454359e-05, 2.290418342454359e-05, 2.290418342454359e-05, 2.290418342454359e-05, 2.290418342454359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.290418342454359e-05

Optimization complete. Final v2v error: 4.011983871459961 mm

Highest mean error: 5.460511207580566 mm for frame 236

Lowest mean error: 3.40262508392334 mm for frame 215

Saving results

Total time: 316.0810239315033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804714
Iteration 2/25 | Loss: 0.00127573
Iteration 3/25 | Loss: 0.00104742
Iteration 4/25 | Loss: 0.00098381
Iteration 5/25 | Loss: 0.00097039
Iteration 6/25 | Loss: 0.00094612
Iteration 7/25 | Loss: 0.00094084
Iteration 8/25 | Loss: 0.00093750
Iteration 9/25 | Loss: 0.00093685
Iteration 10/25 | Loss: 0.00093435
Iteration 11/25 | Loss: 0.00093011
Iteration 12/25 | Loss: 0.00092926
Iteration 13/25 | Loss: 0.00092914
Iteration 14/25 | Loss: 0.00092913
Iteration 15/25 | Loss: 0.00092913
Iteration 16/25 | Loss: 0.00092913
Iteration 17/25 | Loss: 0.00092913
Iteration 18/25 | Loss: 0.00092913
Iteration 19/25 | Loss: 0.00092913
Iteration 20/25 | Loss: 0.00092912
Iteration 21/25 | Loss: 0.00092912
Iteration 22/25 | Loss: 0.00092912
Iteration 23/25 | Loss: 0.00092910
Iteration 24/25 | Loss: 0.00092910
Iteration 25/25 | Loss: 0.00092910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50315237
Iteration 2/25 | Loss: 0.00151901
Iteration 3/25 | Loss: 0.00151901
Iteration 4/25 | Loss: 0.00151901
Iteration 5/25 | Loss: 0.00151901
Iteration 6/25 | Loss: 0.00151901
Iteration 7/25 | Loss: 0.00151901
Iteration 8/25 | Loss: 0.00151901
Iteration 9/25 | Loss: 0.00151901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.001519010285846889, 0.001519010285846889, 0.001519010285846889, 0.001519010285846889, 0.001519010285846889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001519010285846889

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151901
Iteration 2/1000 | Loss: 0.00013383
Iteration 3/1000 | Loss: 0.00064319
Iteration 4/1000 | Loss: 0.00042831
Iteration 5/1000 | Loss: 0.00010063
Iteration 6/1000 | Loss: 0.00007887
Iteration 7/1000 | Loss: 0.00022938
Iteration 8/1000 | Loss: 0.00016093
Iteration 9/1000 | Loss: 0.00006765
Iteration 10/1000 | Loss: 0.00024864
Iteration 11/1000 | Loss: 0.00009399
Iteration 12/1000 | Loss: 0.00004399
Iteration 13/1000 | Loss: 0.00004124
Iteration 14/1000 | Loss: 0.00003854
Iteration 15/1000 | Loss: 0.00003710
Iteration 16/1000 | Loss: 0.00003635
Iteration 17/1000 | Loss: 0.00003528
Iteration 18/1000 | Loss: 0.00003476
Iteration 19/1000 | Loss: 0.00003425
Iteration 20/1000 | Loss: 0.00045290
Iteration 21/1000 | Loss: 0.00004023
Iteration 22/1000 | Loss: 0.00003596
Iteration 23/1000 | Loss: 0.00003451
Iteration 24/1000 | Loss: 0.00003345
Iteration 25/1000 | Loss: 0.00003226
Iteration 26/1000 | Loss: 0.00003173
Iteration 27/1000 | Loss: 0.00003134
Iteration 28/1000 | Loss: 0.00016500
Iteration 29/1000 | Loss: 0.00003752
Iteration 30/1000 | Loss: 0.00003405
Iteration 31/1000 | Loss: 0.00003307
Iteration 32/1000 | Loss: 0.00016089
Iteration 33/1000 | Loss: 0.00020061
Iteration 34/1000 | Loss: 0.00015226
Iteration 35/1000 | Loss: 0.00003668
Iteration 36/1000 | Loss: 0.00003462
Iteration 37/1000 | Loss: 0.00003359
Iteration 38/1000 | Loss: 0.00003299
Iteration 39/1000 | Loss: 0.00003212
Iteration 40/1000 | Loss: 0.00003153
Iteration 41/1000 | Loss: 0.00004153
Iteration 42/1000 | Loss: 0.00003166
Iteration 43/1000 | Loss: 0.00003114
Iteration 44/1000 | Loss: 0.00003079
Iteration 45/1000 | Loss: 0.00003076
Iteration 46/1000 | Loss: 0.00003075
Iteration 47/1000 | Loss: 0.00003073
Iteration 48/1000 | Loss: 0.00003073
Iteration 49/1000 | Loss: 0.00003071
Iteration 50/1000 | Loss: 0.00003070
Iteration 51/1000 | Loss: 0.00003066
Iteration 52/1000 | Loss: 0.00003065
Iteration 53/1000 | Loss: 0.00003065
Iteration 54/1000 | Loss: 0.00003064
Iteration 55/1000 | Loss: 0.00003064
Iteration 56/1000 | Loss: 0.00003052
Iteration 57/1000 | Loss: 0.00003044
Iteration 58/1000 | Loss: 0.00003044
Iteration 59/1000 | Loss: 0.00003043
Iteration 60/1000 | Loss: 0.00003043
Iteration 61/1000 | Loss: 0.00003043
Iteration 62/1000 | Loss: 0.00003042
Iteration 63/1000 | Loss: 0.00003042
Iteration 64/1000 | Loss: 0.00003038
Iteration 65/1000 | Loss: 0.00003035
Iteration 66/1000 | Loss: 0.00003035
Iteration 67/1000 | Loss: 0.00003034
Iteration 68/1000 | Loss: 0.00003034
Iteration 69/1000 | Loss: 0.00003034
Iteration 70/1000 | Loss: 0.00003034
Iteration 71/1000 | Loss: 0.00003033
Iteration 72/1000 | Loss: 0.00003033
Iteration 73/1000 | Loss: 0.00003032
Iteration 74/1000 | Loss: 0.00003032
Iteration 75/1000 | Loss: 0.00003032
Iteration 76/1000 | Loss: 0.00003030
Iteration 77/1000 | Loss: 0.00003030
Iteration 78/1000 | Loss: 0.00003029
Iteration 79/1000 | Loss: 0.00003029
Iteration 80/1000 | Loss: 0.00003029
Iteration 81/1000 | Loss: 0.00003028
Iteration 82/1000 | Loss: 0.00003028
Iteration 83/1000 | Loss: 0.00003028
Iteration 84/1000 | Loss: 0.00003027
Iteration 85/1000 | Loss: 0.00003027
Iteration 86/1000 | Loss: 0.00003027
Iteration 87/1000 | Loss: 0.00003027
Iteration 88/1000 | Loss: 0.00003027
Iteration 89/1000 | Loss: 0.00003027
Iteration 90/1000 | Loss: 0.00003027
Iteration 91/1000 | Loss: 0.00003027
Iteration 92/1000 | Loss: 0.00003026
Iteration 93/1000 | Loss: 0.00003026
Iteration 94/1000 | Loss: 0.00003026
Iteration 95/1000 | Loss: 0.00003026
Iteration 96/1000 | Loss: 0.00003026
Iteration 97/1000 | Loss: 0.00003026
Iteration 98/1000 | Loss: 0.00003026
Iteration 99/1000 | Loss: 0.00003026
Iteration 100/1000 | Loss: 0.00003026
Iteration 101/1000 | Loss: 0.00003026
Iteration 102/1000 | Loss: 0.00003026
Iteration 103/1000 | Loss: 0.00003026
Iteration 104/1000 | Loss: 0.00003025
Iteration 105/1000 | Loss: 0.00003025
Iteration 106/1000 | Loss: 0.00003025
Iteration 107/1000 | Loss: 0.00003025
Iteration 108/1000 | Loss: 0.00003025
Iteration 109/1000 | Loss: 0.00003025
Iteration 110/1000 | Loss: 0.00003025
Iteration 111/1000 | Loss: 0.00003025
Iteration 112/1000 | Loss: 0.00003024
Iteration 113/1000 | Loss: 0.00003024
Iteration 114/1000 | Loss: 0.00003024
Iteration 115/1000 | Loss: 0.00003024
Iteration 116/1000 | Loss: 0.00003024
Iteration 117/1000 | Loss: 0.00003023
Iteration 118/1000 | Loss: 0.00003023
Iteration 119/1000 | Loss: 0.00003023
Iteration 120/1000 | Loss: 0.00003023
Iteration 121/1000 | Loss: 0.00003023
Iteration 122/1000 | Loss: 0.00003023
Iteration 123/1000 | Loss: 0.00003023
Iteration 124/1000 | Loss: 0.00003023
Iteration 125/1000 | Loss: 0.00003022
Iteration 126/1000 | Loss: 0.00003022
Iteration 127/1000 | Loss: 0.00003022
Iteration 128/1000 | Loss: 0.00003022
Iteration 129/1000 | Loss: 0.00003022
Iteration 130/1000 | Loss: 0.00003022
Iteration 131/1000 | Loss: 0.00003022
Iteration 132/1000 | Loss: 0.00003022
Iteration 133/1000 | Loss: 0.00003021
Iteration 134/1000 | Loss: 0.00003021
Iteration 135/1000 | Loss: 0.00003021
Iteration 136/1000 | Loss: 0.00003021
Iteration 137/1000 | Loss: 0.00003021
Iteration 138/1000 | Loss: 0.00003021
Iteration 139/1000 | Loss: 0.00003021
Iteration 140/1000 | Loss: 0.00003021
Iteration 141/1000 | Loss: 0.00003021
Iteration 142/1000 | Loss: 0.00003021
Iteration 143/1000 | Loss: 0.00003021
Iteration 144/1000 | Loss: 0.00003020
Iteration 145/1000 | Loss: 0.00003020
Iteration 146/1000 | Loss: 0.00003020
Iteration 147/1000 | Loss: 0.00003020
Iteration 148/1000 | Loss: 0.00003020
Iteration 149/1000 | Loss: 0.00003020
Iteration 150/1000 | Loss: 0.00003020
Iteration 151/1000 | Loss: 0.00003020
Iteration 152/1000 | Loss: 0.00003019
Iteration 153/1000 | Loss: 0.00003019
Iteration 154/1000 | Loss: 0.00003019
Iteration 155/1000 | Loss: 0.00003019
Iteration 156/1000 | Loss: 0.00003019
Iteration 157/1000 | Loss: 0.00003019
Iteration 158/1000 | Loss: 0.00003019
Iteration 159/1000 | Loss: 0.00003019
Iteration 160/1000 | Loss: 0.00003019
Iteration 161/1000 | Loss: 0.00003019
Iteration 162/1000 | Loss: 0.00003018
Iteration 163/1000 | Loss: 0.00003018
Iteration 164/1000 | Loss: 0.00003018
Iteration 165/1000 | Loss: 0.00003018
Iteration 166/1000 | Loss: 0.00003018
Iteration 167/1000 | Loss: 0.00003018
Iteration 168/1000 | Loss: 0.00003018
Iteration 169/1000 | Loss: 0.00003018
Iteration 170/1000 | Loss: 0.00003018
Iteration 171/1000 | Loss: 0.00003018
Iteration 172/1000 | Loss: 0.00003018
Iteration 173/1000 | Loss: 0.00003018
Iteration 174/1000 | Loss: 0.00003018
Iteration 175/1000 | Loss: 0.00003018
Iteration 176/1000 | Loss: 0.00003018
Iteration 177/1000 | Loss: 0.00003017
Iteration 178/1000 | Loss: 0.00003017
Iteration 179/1000 | Loss: 0.00003017
Iteration 180/1000 | Loss: 0.00003017
Iteration 181/1000 | Loss: 0.00003017
Iteration 182/1000 | Loss: 0.00003017
Iteration 183/1000 | Loss: 0.00003017
Iteration 184/1000 | Loss: 0.00003017
Iteration 185/1000 | Loss: 0.00003017
Iteration 186/1000 | Loss: 0.00003017
Iteration 187/1000 | Loss: 0.00003017
Iteration 188/1000 | Loss: 0.00003017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [3.0173921913956292e-05, 3.0173921913956292e-05, 3.0173921913956292e-05, 3.0173921913956292e-05, 3.0173921913956292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0173921913956292e-05

Optimization complete. Final v2v error: 3.954418897628784 mm

Highest mean error: 12.1173734664917 mm for frame 26

Lowest mean error: 3.257018566131592 mm for frame 152

Saving results

Total time: 110.7120668888092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398238
Iteration 2/25 | Loss: 0.00097071
Iteration 3/25 | Loss: 0.00083810
Iteration 4/25 | Loss: 0.00082552
Iteration 5/25 | Loss: 0.00082173
Iteration 6/25 | Loss: 0.00082060
Iteration 7/25 | Loss: 0.00082015
Iteration 8/25 | Loss: 0.00082015
Iteration 9/25 | Loss: 0.00082015
Iteration 10/25 | Loss: 0.00082015
Iteration 11/25 | Loss: 0.00082015
Iteration 12/25 | Loss: 0.00082015
Iteration 13/25 | Loss: 0.00082015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008201465825550258, 0.0008201465825550258, 0.0008201465825550258, 0.0008201465825550258, 0.0008201465825550258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008201465825550258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57841611
Iteration 2/25 | Loss: 0.00063664
Iteration 3/25 | Loss: 0.00063664
Iteration 4/25 | Loss: 0.00063664
Iteration 5/25 | Loss: 0.00063664
Iteration 6/25 | Loss: 0.00063664
Iteration 7/25 | Loss: 0.00063664
Iteration 8/25 | Loss: 0.00063664
Iteration 9/25 | Loss: 0.00063664
Iteration 10/25 | Loss: 0.00063664
Iteration 11/25 | Loss: 0.00063664
Iteration 12/25 | Loss: 0.00063664
Iteration 13/25 | Loss: 0.00063664
Iteration 14/25 | Loss: 0.00063664
Iteration 15/25 | Loss: 0.00063664
Iteration 16/25 | Loss: 0.00063664
Iteration 17/25 | Loss: 0.00063664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006366410525515676, 0.0006366410525515676, 0.0006366410525515676, 0.0006366410525515676, 0.0006366410525515676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006366410525515676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063664
Iteration 2/1000 | Loss: 0.00002526
Iteration 3/1000 | Loss: 0.00001646
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001417
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001233
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001233
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001232
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001226
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001225
Iteration 70/1000 | Loss: 0.00001225
Iteration 71/1000 | Loss: 0.00001225
Iteration 72/1000 | Loss: 0.00001225
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001221
Iteration 82/1000 | Loss: 0.00001221
Iteration 83/1000 | Loss: 0.00001220
Iteration 84/1000 | Loss: 0.00001220
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001219
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001218
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001218
Iteration 94/1000 | Loss: 0.00001218
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001215
Iteration 113/1000 | Loss: 0.00001215
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001215
Iteration 119/1000 | Loss: 0.00001215
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001214
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001214
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001214
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001214
Iteration 143/1000 | Loss: 0.00001214
Iteration 144/1000 | Loss: 0.00001214
Iteration 145/1000 | Loss: 0.00001214
Iteration 146/1000 | Loss: 0.00001214
Iteration 147/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.2139632417529356e-05, 1.2139632417529356e-05, 1.2139632417529356e-05, 1.2139632417529356e-05, 1.2139632417529356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2139632417529356e-05

Optimization complete. Final v2v error: 2.960926055908203 mm

Highest mean error: 3.8484532833099365 mm for frame 36

Lowest mean error: 2.734687089920044 mm for frame 118

Saving results

Total time: 37.215630769729614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880103
Iteration 2/25 | Loss: 0.00147423
Iteration 3/25 | Loss: 0.00115076
Iteration 4/25 | Loss: 0.00106572
Iteration 5/25 | Loss: 0.00103271
Iteration 6/25 | Loss: 0.00102529
Iteration 7/25 | Loss: 0.00102646
Iteration 8/25 | Loss: 0.00102775
Iteration 9/25 | Loss: 0.00102219
Iteration 10/25 | Loss: 0.00101973
Iteration 11/25 | Loss: 0.00101862
Iteration 12/25 | Loss: 0.00101800
Iteration 13/25 | Loss: 0.00101782
Iteration 14/25 | Loss: 0.00101773
Iteration 15/25 | Loss: 0.00101773
Iteration 16/25 | Loss: 0.00101773
Iteration 17/25 | Loss: 0.00101773
Iteration 18/25 | Loss: 0.00101772
Iteration 19/25 | Loss: 0.00101772
Iteration 20/25 | Loss: 0.00101772
Iteration 21/25 | Loss: 0.00101772
Iteration 22/25 | Loss: 0.00101771
Iteration 23/25 | Loss: 0.00101771
Iteration 24/25 | Loss: 0.00101771
Iteration 25/25 | Loss: 0.00101771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48107398
Iteration 2/25 | Loss: 0.00137268
Iteration 3/25 | Loss: 0.00137263
Iteration 4/25 | Loss: 0.00137263
Iteration 5/25 | Loss: 0.00137263
Iteration 6/25 | Loss: 0.00137263
Iteration 7/25 | Loss: 0.00137262
Iteration 8/25 | Loss: 0.00137262
Iteration 9/25 | Loss: 0.00137262
Iteration 10/25 | Loss: 0.00137262
Iteration 11/25 | Loss: 0.00137262
Iteration 12/25 | Loss: 0.00137262
Iteration 13/25 | Loss: 0.00137262
Iteration 14/25 | Loss: 0.00137262
Iteration 15/25 | Loss: 0.00137262
Iteration 16/25 | Loss: 0.00137262
Iteration 17/25 | Loss: 0.00137262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013726238394156098, 0.0013726238394156098, 0.0013726238394156098, 0.0013726238394156098, 0.0013726238394156098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013726238394156098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137262
Iteration 2/1000 | Loss: 0.00021662
Iteration 3/1000 | Loss: 0.00015390
Iteration 4/1000 | Loss: 0.00025822
Iteration 5/1000 | Loss: 0.00061917
Iteration 6/1000 | Loss: 0.00014414
Iteration 7/1000 | Loss: 0.00009976
Iteration 8/1000 | Loss: 0.00007455
Iteration 9/1000 | Loss: 0.00087729
Iteration 10/1000 | Loss: 0.00049307
Iteration 11/1000 | Loss: 0.00073333
Iteration 12/1000 | Loss: 0.00068486
Iteration 13/1000 | Loss: 0.00017389
Iteration 14/1000 | Loss: 0.00008036
Iteration 15/1000 | Loss: 0.00004974
Iteration 16/1000 | Loss: 0.00004446
Iteration 17/1000 | Loss: 0.00004202
Iteration 18/1000 | Loss: 0.00003972
Iteration 19/1000 | Loss: 0.00024550
Iteration 20/1000 | Loss: 0.00055427
Iteration 21/1000 | Loss: 0.00013008
Iteration 22/1000 | Loss: 0.00003629
Iteration 23/1000 | Loss: 0.00003486
Iteration 24/1000 | Loss: 0.00048251
Iteration 25/1000 | Loss: 0.00015194
Iteration 26/1000 | Loss: 0.00004254
Iteration 27/1000 | Loss: 0.00026088
Iteration 28/1000 | Loss: 0.00018191
Iteration 29/1000 | Loss: 0.00003549
Iteration 30/1000 | Loss: 0.00003393
Iteration 31/1000 | Loss: 0.00003317
Iteration 32/1000 | Loss: 0.00003292
Iteration 33/1000 | Loss: 0.00003273
Iteration 34/1000 | Loss: 0.00046920
Iteration 35/1000 | Loss: 0.00016639
Iteration 36/1000 | Loss: 0.00003696
Iteration 37/1000 | Loss: 0.00051299
Iteration 38/1000 | Loss: 0.00030258
Iteration 39/1000 | Loss: 0.00004124
Iteration 40/1000 | Loss: 0.00036886
Iteration 41/1000 | Loss: 0.00042998
Iteration 42/1000 | Loss: 0.00020116
Iteration 43/1000 | Loss: 0.00040172
Iteration 44/1000 | Loss: 0.00027395
Iteration 45/1000 | Loss: 0.00017894
Iteration 46/1000 | Loss: 0.00004572
Iteration 47/1000 | Loss: 0.00083052
Iteration 48/1000 | Loss: 0.00004854
Iteration 49/1000 | Loss: 0.00089533
Iteration 50/1000 | Loss: 0.00029143
Iteration 51/1000 | Loss: 0.00035500
Iteration 52/1000 | Loss: 0.00044987
Iteration 53/1000 | Loss: 0.00026846
Iteration 54/1000 | Loss: 0.00043745
Iteration 55/1000 | Loss: 0.00004641
Iteration 56/1000 | Loss: 0.00004016
Iteration 57/1000 | Loss: 0.00003736
Iteration 58/1000 | Loss: 0.00003402
Iteration 59/1000 | Loss: 0.00003244
Iteration 60/1000 | Loss: 0.00003175
Iteration 61/1000 | Loss: 0.00003098
Iteration 62/1000 | Loss: 0.00003070
Iteration 63/1000 | Loss: 0.00003064
Iteration 64/1000 | Loss: 0.00003045
Iteration 65/1000 | Loss: 0.00003021
Iteration 66/1000 | Loss: 0.00003017
Iteration 67/1000 | Loss: 0.00003011
Iteration 68/1000 | Loss: 0.00003011
Iteration 69/1000 | Loss: 0.00003008
Iteration 70/1000 | Loss: 0.00003003
Iteration 71/1000 | Loss: 0.00003003
Iteration 72/1000 | Loss: 0.00003003
Iteration 73/1000 | Loss: 0.00003003
Iteration 74/1000 | Loss: 0.00003003
Iteration 75/1000 | Loss: 0.00003003
Iteration 76/1000 | Loss: 0.00003003
Iteration 77/1000 | Loss: 0.00003003
Iteration 78/1000 | Loss: 0.00003003
Iteration 79/1000 | Loss: 0.00003002
Iteration 80/1000 | Loss: 0.00003002
Iteration 81/1000 | Loss: 0.00003002
Iteration 82/1000 | Loss: 0.00003001
Iteration 83/1000 | Loss: 0.00003001
Iteration 84/1000 | Loss: 0.00003001
Iteration 85/1000 | Loss: 0.00003000
Iteration 86/1000 | Loss: 0.00003000
Iteration 87/1000 | Loss: 0.00003000
Iteration 88/1000 | Loss: 0.00002999
Iteration 89/1000 | Loss: 0.00002999
Iteration 90/1000 | Loss: 0.00002999
Iteration 91/1000 | Loss: 0.00002998
Iteration 92/1000 | Loss: 0.00002998
Iteration 93/1000 | Loss: 0.00002998
Iteration 94/1000 | Loss: 0.00002998
Iteration 95/1000 | Loss: 0.00002998
Iteration 96/1000 | Loss: 0.00002998
Iteration 97/1000 | Loss: 0.00002998
Iteration 98/1000 | Loss: 0.00002998
Iteration 99/1000 | Loss: 0.00002997
Iteration 100/1000 | Loss: 0.00002997
Iteration 101/1000 | Loss: 0.00002997
Iteration 102/1000 | Loss: 0.00002997
Iteration 103/1000 | Loss: 0.00002997
Iteration 104/1000 | Loss: 0.00002997
Iteration 105/1000 | Loss: 0.00002997
Iteration 106/1000 | Loss: 0.00002997
Iteration 107/1000 | Loss: 0.00002997
Iteration 108/1000 | Loss: 0.00002997
Iteration 109/1000 | Loss: 0.00002997
Iteration 110/1000 | Loss: 0.00002996
Iteration 111/1000 | Loss: 0.00002996
Iteration 112/1000 | Loss: 0.00002996
Iteration 113/1000 | Loss: 0.00002996
Iteration 114/1000 | Loss: 0.00002996
Iteration 115/1000 | Loss: 0.00002996
Iteration 116/1000 | Loss: 0.00002995
Iteration 117/1000 | Loss: 0.00002995
Iteration 118/1000 | Loss: 0.00002995
Iteration 119/1000 | Loss: 0.00002995
Iteration 120/1000 | Loss: 0.00002995
Iteration 121/1000 | Loss: 0.00002995
Iteration 122/1000 | Loss: 0.00002995
Iteration 123/1000 | Loss: 0.00002995
Iteration 124/1000 | Loss: 0.00002995
Iteration 125/1000 | Loss: 0.00002994
Iteration 126/1000 | Loss: 0.00002994
Iteration 127/1000 | Loss: 0.00002994
Iteration 128/1000 | Loss: 0.00002994
Iteration 129/1000 | Loss: 0.00002994
Iteration 130/1000 | Loss: 0.00002993
Iteration 131/1000 | Loss: 0.00002993
Iteration 132/1000 | Loss: 0.00002993
Iteration 133/1000 | Loss: 0.00002992
Iteration 134/1000 | Loss: 0.00002992
Iteration 135/1000 | Loss: 0.00002992
Iteration 136/1000 | Loss: 0.00002992
Iteration 137/1000 | Loss: 0.00002992
Iteration 138/1000 | Loss: 0.00002991
Iteration 139/1000 | Loss: 0.00002991
Iteration 140/1000 | Loss: 0.00002991
Iteration 141/1000 | Loss: 0.00002991
Iteration 142/1000 | Loss: 0.00002991
Iteration 143/1000 | Loss: 0.00002991
Iteration 144/1000 | Loss: 0.00002991
Iteration 145/1000 | Loss: 0.00002991
Iteration 146/1000 | Loss: 0.00002991
Iteration 147/1000 | Loss: 0.00002991
Iteration 148/1000 | Loss: 0.00002991
Iteration 149/1000 | Loss: 0.00002991
Iteration 150/1000 | Loss: 0.00002991
Iteration 151/1000 | Loss: 0.00002991
Iteration 152/1000 | Loss: 0.00002991
Iteration 153/1000 | Loss: 0.00002991
Iteration 154/1000 | Loss: 0.00002991
Iteration 155/1000 | Loss: 0.00002991
Iteration 156/1000 | Loss: 0.00002991
Iteration 157/1000 | Loss: 0.00002991
Iteration 158/1000 | Loss: 0.00002991
Iteration 159/1000 | Loss: 0.00002991
Iteration 160/1000 | Loss: 0.00002991
Iteration 161/1000 | Loss: 0.00002991
Iteration 162/1000 | Loss: 0.00002991
Iteration 163/1000 | Loss: 0.00002991
Iteration 164/1000 | Loss: 0.00002991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.9907267162343487e-05, 2.9907267162343487e-05, 2.9907267162343487e-05, 2.9907267162343487e-05, 2.9907267162343487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9907267162343487e-05

Optimization complete. Final v2v error: 4.390599727630615 mm

Highest mean error: 7.049590110778809 mm for frame 59

Lowest mean error: 3.801504135131836 mm for frame 117

Saving results

Total time: 140.58927702903748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677951
Iteration 2/25 | Loss: 0.00109250
Iteration 3/25 | Loss: 0.00094116
Iteration 4/25 | Loss: 0.00090440
Iteration 5/25 | Loss: 0.00089247
Iteration 6/25 | Loss: 0.00089065
Iteration 7/25 | Loss: 0.00089058
Iteration 8/25 | Loss: 0.00089058
Iteration 9/25 | Loss: 0.00089058
Iteration 10/25 | Loss: 0.00089058
Iteration 11/25 | Loss: 0.00089058
Iteration 12/25 | Loss: 0.00089058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000890581461135298, 0.000890581461135298, 0.000890581461135298, 0.000890581461135298, 0.000890581461135298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000890581461135298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51568043
Iteration 2/25 | Loss: 0.00070941
Iteration 3/25 | Loss: 0.00070940
Iteration 4/25 | Loss: 0.00070940
Iteration 5/25 | Loss: 0.00070940
Iteration 6/25 | Loss: 0.00070940
Iteration 7/25 | Loss: 0.00070940
Iteration 8/25 | Loss: 0.00070940
Iteration 9/25 | Loss: 0.00070940
Iteration 10/25 | Loss: 0.00070940
Iteration 11/25 | Loss: 0.00070940
Iteration 12/25 | Loss: 0.00070940
Iteration 13/25 | Loss: 0.00070940
Iteration 14/25 | Loss: 0.00070940
Iteration 15/25 | Loss: 0.00070940
Iteration 16/25 | Loss: 0.00070940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007093974854797125, 0.0007093974854797125, 0.0007093974854797125, 0.0007093974854797125, 0.0007093974854797125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007093974854797125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070940
Iteration 2/1000 | Loss: 0.00005286
Iteration 3/1000 | Loss: 0.00003526
Iteration 4/1000 | Loss: 0.00003240
Iteration 5/1000 | Loss: 0.00003096
Iteration 6/1000 | Loss: 0.00003011
Iteration 7/1000 | Loss: 0.00002936
Iteration 8/1000 | Loss: 0.00002876
Iteration 9/1000 | Loss: 0.00002833
Iteration 10/1000 | Loss: 0.00002802
Iteration 11/1000 | Loss: 0.00002775
Iteration 12/1000 | Loss: 0.00002764
Iteration 13/1000 | Loss: 0.00002751
Iteration 14/1000 | Loss: 0.00002749
Iteration 15/1000 | Loss: 0.00002742
Iteration 16/1000 | Loss: 0.00002740
Iteration 17/1000 | Loss: 0.00002739
Iteration 18/1000 | Loss: 0.00002739
Iteration 19/1000 | Loss: 0.00002739
Iteration 20/1000 | Loss: 0.00002738
Iteration 21/1000 | Loss: 0.00002737
Iteration 22/1000 | Loss: 0.00002737
Iteration 23/1000 | Loss: 0.00002737
Iteration 24/1000 | Loss: 0.00002736
Iteration 25/1000 | Loss: 0.00002736
Iteration 26/1000 | Loss: 0.00002735
Iteration 27/1000 | Loss: 0.00002735
Iteration 28/1000 | Loss: 0.00002734
Iteration 29/1000 | Loss: 0.00002734
Iteration 30/1000 | Loss: 0.00002734
Iteration 31/1000 | Loss: 0.00002733
Iteration 32/1000 | Loss: 0.00002733
Iteration 33/1000 | Loss: 0.00002733
Iteration 34/1000 | Loss: 0.00002733
Iteration 35/1000 | Loss: 0.00002732
Iteration 36/1000 | Loss: 0.00002732
Iteration 37/1000 | Loss: 0.00002732
Iteration 38/1000 | Loss: 0.00002732
Iteration 39/1000 | Loss: 0.00002731
Iteration 40/1000 | Loss: 0.00002731
Iteration 41/1000 | Loss: 0.00002731
Iteration 42/1000 | Loss: 0.00002731
Iteration 43/1000 | Loss: 0.00002731
Iteration 44/1000 | Loss: 0.00002731
Iteration 45/1000 | Loss: 0.00002731
Iteration 46/1000 | Loss: 0.00002731
Iteration 47/1000 | Loss: 0.00002731
Iteration 48/1000 | Loss: 0.00002730
Iteration 49/1000 | Loss: 0.00002730
Iteration 50/1000 | Loss: 0.00002730
Iteration 51/1000 | Loss: 0.00002730
Iteration 52/1000 | Loss: 0.00002729
Iteration 53/1000 | Loss: 0.00002729
Iteration 54/1000 | Loss: 0.00002729
Iteration 55/1000 | Loss: 0.00002729
Iteration 56/1000 | Loss: 0.00002729
Iteration 57/1000 | Loss: 0.00002729
Iteration 58/1000 | Loss: 0.00002728
Iteration 59/1000 | Loss: 0.00002728
Iteration 60/1000 | Loss: 0.00002728
Iteration 61/1000 | Loss: 0.00002728
Iteration 62/1000 | Loss: 0.00002727
Iteration 63/1000 | Loss: 0.00002727
Iteration 64/1000 | Loss: 0.00002727
Iteration 65/1000 | Loss: 0.00002726
Iteration 66/1000 | Loss: 0.00002726
Iteration 67/1000 | Loss: 0.00002726
Iteration 68/1000 | Loss: 0.00002726
Iteration 69/1000 | Loss: 0.00002726
Iteration 70/1000 | Loss: 0.00002725
Iteration 71/1000 | Loss: 0.00002724
Iteration 72/1000 | Loss: 0.00002724
Iteration 73/1000 | Loss: 0.00002723
Iteration 74/1000 | Loss: 0.00002723
Iteration 75/1000 | Loss: 0.00002723
Iteration 76/1000 | Loss: 0.00002722
Iteration 77/1000 | Loss: 0.00002722
Iteration 78/1000 | Loss: 0.00002722
Iteration 79/1000 | Loss: 0.00002721
Iteration 80/1000 | Loss: 0.00002721
Iteration 81/1000 | Loss: 0.00002721
Iteration 82/1000 | Loss: 0.00002720
Iteration 83/1000 | Loss: 0.00002720
Iteration 84/1000 | Loss: 0.00002720
Iteration 85/1000 | Loss: 0.00002720
Iteration 86/1000 | Loss: 0.00002719
Iteration 87/1000 | Loss: 0.00002719
Iteration 88/1000 | Loss: 0.00002718
Iteration 89/1000 | Loss: 0.00002718
Iteration 90/1000 | Loss: 0.00002718
Iteration 91/1000 | Loss: 0.00002718
Iteration 92/1000 | Loss: 0.00002718
Iteration 93/1000 | Loss: 0.00002718
Iteration 94/1000 | Loss: 0.00002718
Iteration 95/1000 | Loss: 0.00002718
Iteration 96/1000 | Loss: 0.00002718
Iteration 97/1000 | Loss: 0.00002717
Iteration 98/1000 | Loss: 0.00002717
Iteration 99/1000 | Loss: 0.00002717
Iteration 100/1000 | Loss: 0.00002717
Iteration 101/1000 | Loss: 0.00002717
Iteration 102/1000 | Loss: 0.00002717
Iteration 103/1000 | Loss: 0.00002717
Iteration 104/1000 | Loss: 0.00002717
Iteration 105/1000 | Loss: 0.00002716
Iteration 106/1000 | Loss: 0.00002716
Iteration 107/1000 | Loss: 0.00002716
Iteration 108/1000 | Loss: 0.00002716
Iteration 109/1000 | Loss: 0.00002716
Iteration 110/1000 | Loss: 0.00002716
Iteration 111/1000 | Loss: 0.00002716
Iteration 112/1000 | Loss: 0.00002716
Iteration 113/1000 | Loss: 0.00002716
Iteration 114/1000 | Loss: 0.00002716
Iteration 115/1000 | Loss: 0.00002716
Iteration 116/1000 | Loss: 0.00002716
Iteration 117/1000 | Loss: 0.00002716
Iteration 118/1000 | Loss: 0.00002716
Iteration 119/1000 | Loss: 0.00002716
Iteration 120/1000 | Loss: 0.00002715
Iteration 121/1000 | Loss: 0.00002715
Iteration 122/1000 | Loss: 0.00002715
Iteration 123/1000 | Loss: 0.00002715
Iteration 124/1000 | Loss: 0.00002715
Iteration 125/1000 | Loss: 0.00002715
Iteration 126/1000 | Loss: 0.00002715
Iteration 127/1000 | Loss: 0.00002715
Iteration 128/1000 | Loss: 0.00002714
Iteration 129/1000 | Loss: 0.00002714
Iteration 130/1000 | Loss: 0.00002714
Iteration 131/1000 | Loss: 0.00002714
Iteration 132/1000 | Loss: 0.00002714
Iteration 133/1000 | Loss: 0.00002713
Iteration 134/1000 | Loss: 0.00002713
Iteration 135/1000 | Loss: 0.00002713
Iteration 136/1000 | Loss: 0.00002713
Iteration 137/1000 | Loss: 0.00002713
Iteration 138/1000 | Loss: 0.00002713
Iteration 139/1000 | Loss: 0.00002713
Iteration 140/1000 | Loss: 0.00002713
Iteration 141/1000 | Loss: 0.00002713
Iteration 142/1000 | Loss: 0.00002713
Iteration 143/1000 | Loss: 0.00002713
Iteration 144/1000 | Loss: 0.00002712
Iteration 145/1000 | Loss: 0.00002712
Iteration 146/1000 | Loss: 0.00002712
Iteration 147/1000 | Loss: 0.00002712
Iteration 148/1000 | Loss: 0.00002712
Iteration 149/1000 | Loss: 0.00002712
Iteration 150/1000 | Loss: 0.00002712
Iteration 151/1000 | Loss: 0.00002712
Iteration 152/1000 | Loss: 0.00002712
Iteration 153/1000 | Loss: 0.00002712
Iteration 154/1000 | Loss: 0.00002712
Iteration 155/1000 | Loss: 0.00002711
Iteration 156/1000 | Loss: 0.00002711
Iteration 157/1000 | Loss: 0.00002711
Iteration 158/1000 | Loss: 0.00002711
Iteration 159/1000 | Loss: 0.00002711
Iteration 160/1000 | Loss: 0.00002711
Iteration 161/1000 | Loss: 0.00002711
Iteration 162/1000 | Loss: 0.00002711
Iteration 163/1000 | Loss: 0.00002711
Iteration 164/1000 | Loss: 0.00002711
Iteration 165/1000 | Loss: 0.00002711
Iteration 166/1000 | Loss: 0.00002711
Iteration 167/1000 | Loss: 0.00002711
Iteration 168/1000 | Loss: 0.00002711
Iteration 169/1000 | Loss: 0.00002711
Iteration 170/1000 | Loss: 0.00002711
Iteration 171/1000 | Loss: 0.00002711
Iteration 172/1000 | Loss: 0.00002710
Iteration 173/1000 | Loss: 0.00002710
Iteration 174/1000 | Loss: 0.00002710
Iteration 175/1000 | Loss: 0.00002710
Iteration 176/1000 | Loss: 0.00002710
Iteration 177/1000 | Loss: 0.00002710
Iteration 178/1000 | Loss: 0.00002710
Iteration 179/1000 | Loss: 0.00002710
Iteration 180/1000 | Loss: 0.00002710
Iteration 181/1000 | Loss: 0.00002710
Iteration 182/1000 | Loss: 0.00002710
Iteration 183/1000 | Loss: 0.00002710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.7100328225060366e-05, 2.7100328225060366e-05, 2.7100328225060366e-05, 2.7100328225060366e-05, 2.7100328225060366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7100328225060366e-05

Optimization complete. Final v2v error: 4.2904372215271 mm

Highest mean error: 4.749486923217773 mm for frame 15

Lowest mean error: 3.7242043018341064 mm for frame 235

Saving results

Total time: 44.44740676879883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396797
Iteration 2/25 | Loss: 0.00088102
Iteration 3/25 | Loss: 0.00080395
Iteration 4/25 | Loss: 0.00079147
Iteration 5/25 | Loss: 0.00078936
Iteration 6/25 | Loss: 0.00078892
Iteration 7/25 | Loss: 0.00078892
Iteration 8/25 | Loss: 0.00078892
Iteration 9/25 | Loss: 0.00078892
Iteration 10/25 | Loss: 0.00078892
Iteration 11/25 | Loss: 0.00078892
Iteration 12/25 | Loss: 0.00078892
Iteration 13/25 | Loss: 0.00078892
Iteration 14/25 | Loss: 0.00078867
Iteration 15/25 | Loss: 0.00078867
Iteration 16/25 | Loss: 0.00078867
Iteration 17/25 | Loss: 0.00078867
Iteration 18/25 | Loss: 0.00078867
Iteration 19/25 | Loss: 0.00078867
Iteration 20/25 | Loss: 0.00078867
Iteration 21/25 | Loss: 0.00078867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007886716048233211, 0.0007886716048233211, 0.0007886716048233211, 0.0007886716048233211, 0.0007886716048233211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007886716048233211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42467403
Iteration 2/25 | Loss: 0.00058972
Iteration 3/25 | Loss: 0.00058972
Iteration 4/25 | Loss: 0.00058972
Iteration 5/25 | Loss: 0.00058972
Iteration 6/25 | Loss: 0.00058972
Iteration 7/25 | Loss: 0.00058971
Iteration 8/25 | Loss: 0.00058971
Iteration 9/25 | Loss: 0.00058971
Iteration 10/25 | Loss: 0.00058971
Iteration 11/25 | Loss: 0.00058971
Iteration 12/25 | Loss: 0.00058971
Iteration 13/25 | Loss: 0.00058971
Iteration 14/25 | Loss: 0.00058971
Iteration 15/25 | Loss: 0.00058971
Iteration 16/25 | Loss: 0.00058971
Iteration 17/25 | Loss: 0.00058971
Iteration 18/25 | Loss: 0.00058971
Iteration 19/25 | Loss: 0.00058971
Iteration 20/25 | Loss: 0.00058971
Iteration 21/25 | Loss: 0.00058971
Iteration 22/25 | Loss: 0.00058971
Iteration 23/25 | Loss: 0.00058971
Iteration 24/25 | Loss: 0.00058971
Iteration 25/25 | Loss: 0.00058971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058971
Iteration 2/1000 | Loss: 0.00002647
Iteration 3/1000 | Loss: 0.00001621
Iteration 4/1000 | Loss: 0.00001503
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001368
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001324
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001309
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001302
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001278
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001263
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001233
Iteration 42/1000 | Loss: 0.00001233
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.2153320312791038e-05, 1.2153320312791038e-05, 1.2153320312791038e-05, 1.2153320312791038e-05, 1.2153320312791038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2153320312791038e-05

Optimization complete. Final v2v error: 3.0083329677581787 mm

Highest mean error: 3.2009737491607666 mm for frame 81

Lowest mean error: 2.8767828941345215 mm for frame 2

Saving results

Total time: 34.90329051017761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128705
Iteration 2/25 | Loss: 0.00184217
Iteration 3/25 | Loss: 0.00114670
Iteration 4/25 | Loss: 0.00106118
Iteration 5/25 | Loss: 0.00103517
Iteration 6/25 | Loss: 0.00105282
Iteration 7/25 | Loss: 0.00102959
Iteration 8/25 | Loss: 0.00102720
Iteration 9/25 | Loss: 0.00102814
Iteration 10/25 | Loss: 0.00102042
Iteration 11/25 | Loss: 0.00101753
Iteration 12/25 | Loss: 0.00102179
Iteration 13/25 | Loss: 0.00102028
Iteration 14/25 | Loss: 0.00101522
Iteration 15/25 | Loss: 0.00101635
Iteration 16/25 | Loss: 0.00102087
Iteration 17/25 | Loss: 0.00101374
Iteration 18/25 | Loss: 0.00101201
Iteration 19/25 | Loss: 0.00101246
Iteration 20/25 | Loss: 0.00100829
Iteration 21/25 | Loss: 0.00100733
Iteration 22/25 | Loss: 0.00100534
Iteration 23/25 | Loss: 0.00100462
Iteration 24/25 | Loss: 0.00100024
Iteration 25/25 | Loss: 0.00100579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07253015
Iteration 2/25 | Loss: 0.00079541
Iteration 3/25 | Loss: 0.00079541
Iteration 4/25 | Loss: 0.00079540
Iteration 5/25 | Loss: 0.00079540
Iteration 6/25 | Loss: 0.00079540
Iteration 7/25 | Loss: 0.00079540
Iteration 8/25 | Loss: 0.00079540
Iteration 9/25 | Loss: 0.00079540
Iteration 10/25 | Loss: 0.00079540
Iteration 11/25 | Loss: 0.00079540
Iteration 12/25 | Loss: 0.00079540
Iteration 13/25 | Loss: 0.00079540
Iteration 14/25 | Loss: 0.00079540
Iteration 15/25 | Loss: 0.00079540
Iteration 16/25 | Loss: 0.00079540
Iteration 17/25 | Loss: 0.00079540
Iteration 18/25 | Loss: 0.00079540
Iteration 19/25 | Loss: 0.00079540
Iteration 20/25 | Loss: 0.00079540
Iteration 21/25 | Loss: 0.00079540
Iteration 22/25 | Loss: 0.00079540
Iteration 23/25 | Loss: 0.00079540
Iteration 24/25 | Loss: 0.00079540
Iteration 25/25 | Loss: 0.00079540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079540
Iteration 2/1000 | Loss: 0.00065403
Iteration 3/1000 | Loss: 0.00030150
Iteration 4/1000 | Loss: 0.00061822
Iteration 5/1000 | Loss: 0.00053305
Iteration 6/1000 | Loss: 0.00012867
Iteration 7/1000 | Loss: 0.00039624
Iteration 8/1000 | Loss: 0.00028722
Iteration 9/1000 | Loss: 0.00030234
Iteration 10/1000 | Loss: 0.00025541
Iteration 11/1000 | Loss: 0.00038925
Iteration 12/1000 | Loss: 0.00031808
Iteration 13/1000 | Loss: 0.00047048
Iteration 14/1000 | Loss: 0.00029461
Iteration 15/1000 | Loss: 0.00058526
Iteration 16/1000 | Loss: 0.00043227
Iteration 17/1000 | Loss: 0.00061634
Iteration 18/1000 | Loss: 0.00058055
Iteration 19/1000 | Loss: 0.00022799
Iteration 20/1000 | Loss: 0.00015284
Iteration 21/1000 | Loss: 0.00035167
Iteration 22/1000 | Loss: 0.00031542
Iteration 23/1000 | Loss: 0.00025602
Iteration 24/1000 | Loss: 0.00032119
Iteration 25/1000 | Loss: 0.00033859
Iteration 26/1000 | Loss: 0.00045445
Iteration 27/1000 | Loss: 0.00050045
Iteration 28/1000 | Loss: 0.00041664
Iteration 29/1000 | Loss: 0.00095892
Iteration 30/1000 | Loss: 0.00059387
Iteration 31/1000 | Loss: 0.00025343
Iteration 32/1000 | Loss: 0.00029659
Iteration 33/1000 | Loss: 0.00031696
Iteration 34/1000 | Loss: 0.00029016
Iteration 35/1000 | Loss: 0.00030730
Iteration 36/1000 | Loss: 0.00045678
Iteration 37/1000 | Loss: 0.00046268
Iteration 38/1000 | Loss: 0.00039480
Iteration 39/1000 | Loss: 0.00046658
Iteration 40/1000 | Loss: 0.00042947
Iteration 41/1000 | Loss: 0.00039165
Iteration 42/1000 | Loss: 0.00021720
Iteration 43/1000 | Loss: 0.00032497
Iteration 44/1000 | Loss: 0.00039198
Iteration 45/1000 | Loss: 0.00041847
Iteration 46/1000 | Loss: 0.00062077
Iteration 47/1000 | Loss: 0.00051804
Iteration 48/1000 | Loss: 0.00038520
Iteration 49/1000 | Loss: 0.00033001
Iteration 50/1000 | Loss: 0.00079965
Iteration 51/1000 | Loss: 0.00125401
Iteration 52/1000 | Loss: 0.00140555
Iteration 53/1000 | Loss: 0.00045151
Iteration 54/1000 | Loss: 0.00069761
Iteration 55/1000 | Loss: 0.00041984
Iteration 56/1000 | Loss: 0.00044097
Iteration 57/1000 | Loss: 0.00060348
Iteration 58/1000 | Loss: 0.00075790
Iteration 59/1000 | Loss: 0.00050271
Iteration 60/1000 | Loss: 0.00094127
Iteration 61/1000 | Loss: 0.00049797
Iteration 62/1000 | Loss: 0.00008363
Iteration 63/1000 | Loss: 0.00034575
Iteration 64/1000 | Loss: 0.00118309
Iteration 65/1000 | Loss: 0.00056733
Iteration 66/1000 | Loss: 0.00059539
Iteration 67/1000 | Loss: 0.00039821
Iteration 68/1000 | Loss: 0.00039022
Iteration 69/1000 | Loss: 0.00036030
Iteration 70/1000 | Loss: 0.00046328
Iteration 71/1000 | Loss: 0.00053373
Iteration 72/1000 | Loss: 0.00045636
Iteration 73/1000 | Loss: 0.00037087
Iteration 74/1000 | Loss: 0.00050535
Iteration 75/1000 | Loss: 0.00043299
Iteration 76/1000 | Loss: 0.00046782
Iteration 77/1000 | Loss: 0.00023038
Iteration 78/1000 | Loss: 0.00059612
Iteration 79/1000 | Loss: 0.00096747
Iteration 80/1000 | Loss: 0.00053169
Iteration 81/1000 | Loss: 0.00043767
Iteration 82/1000 | Loss: 0.00029443
Iteration 83/1000 | Loss: 0.00033386
Iteration 84/1000 | Loss: 0.00028632
Iteration 85/1000 | Loss: 0.00026360
Iteration 86/1000 | Loss: 0.00034566
Iteration 87/1000 | Loss: 0.00043630
Iteration 88/1000 | Loss: 0.00026212
Iteration 89/1000 | Loss: 0.00009833
Iteration 90/1000 | Loss: 0.00012751
Iteration 91/1000 | Loss: 0.00007295
Iteration 92/1000 | Loss: 0.00014795
Iteration 93/1000 | Loss: 0.00015394
Iteration 94/1000 | Loss: 0.00010215
Iteration 95/1000 | Loss: 0.00013532
Iteration 96/1000 | Loss: 0.00012981
Iteration 97/1000 | Loss: 0.00011683
Iteration 98/1000 | Loss: 0.00022296
Iteration 99/1000 | Loss: 0.00017626
Iteration 100/1000 | Loss: 0.00013274
Iteration 101/1000 | Loss: 0.00015916
Iteration 102/1000 | Loss: 0.00015611
Iteration 103/1000 | Loss: 0.00023836
Iteration 104/1000 | Loss: 0.00021722
Iteration 105/1000 | Loss: 0.00021096
Iteration 106/1000 | Loss: 0.00021761
Iteration 107/1000 | Loss: 0.00022612
Iteration 108/1000 | Loss: 0.00013053
Iteration 109/1000 | Loss: 0.00026358
Iteration 110/1000 | Loss: 0.00027262
Iteration 111/1000 | Loss: 0.00029488
Iteration 112/1000 | Loss: 0.00020543
Iteration 113/1000 | Loss: 0.00013298
Iteration 114/1000 | Loss: 0.00029340
Iteration 115/1000 | Loss: 0.00023825
Iteration 116/1000 | Loss: 0.00013481
Iteration 117/1000 | Loss: 0.00041857
Iteration 118/1000 | Loss: 0.00009712
Iteration 119/1000 | Loss: 0.00005258
Iteration 120/1000 | Loss: 0.00016920
Iteration 121/1000 | Loss: 0.00006134
Iteration 122/1000 | Loss: 0.00006231
Iteration 123/1000 | Loss: 0.00014590
Iteration 124/1000 | Loss: 0.00023384
Iteration 125/1000 | Loss: 0.00015744
Iteration 126/1000 | Loss: 0.00005380
Iteration 127/1000 | Loss: 0.00005377
Iteration 128/1000 | Loss: 0.00007851
Iteration 129/1000 | Loss: 0.00010140
Iteration 130/1000 | Loss: 0.00017363
Iteration 131/1000 | Loss: 0.00010611
Iteration 132/1000 | Loss: 0.00018343
Iteration 133/1000 | Loss: 0.00018924
Iteration 134/1000 | Loss: 0.00013486
Iteration 135/1000 | Loss: 0.00017198
Iteration 136/1000 | Loss: 0.00012778
Iteration 137/1000 | Loss: 0.00012875
Iteration 138/1000 | Loss: 0.00019853
Iteration 139/1000 | Loss: 0.00013062
Iteration 140/1000 | Loss: 0.00010852
Iteration 141/1000 | Loss: 0.00020046
Iteration 142/1000 | Loss: 0.00006898
Iteration 143/1000 | Loss: 0.00016322
Iteration 144/1000 | Loss: 0.00014474
Iteration 145/1000 | Loss: 0.00017101
Iteration 146/1000 | Loss: 0.00007930
Iteration 147/1000 | Loss: 0.00021380
Iteration 148/1000 | Loss: 0.00017609
Iteration 149/1000 | Loss: 0.00017631
Iteration 150/1000 | Loss: 0.00014738
Iteration 151/1000 | Loss: 0.00003656
Iteration 152/1000 | Loss: 0.00003254
Iteration 153/1000 | Loss: 0.00003113
Iteration 154/1000 | Loss: 0.00003065
Iteration 155/1000 | Loss: 0.00002991
Iteration 156/1000 | Loss: 0.00002914
Iteration 157/1000 | Loss: 0.00002862
Iteration 158/1000 | Loss: 0.00002824
Iteration 159/1000 | Loss: 0.00002797
Iteration 160/1000 | Loss: 0.00002779
Iteration 161/1000 | Loss: 0.00002760
Iteration 162/1000 | Loss: 0.00002758
Iteration 163/1000 | Loss: 0.00002740
Iteration 164/1000 | Loss: 0.00002735
Iteration 165/1000 | Loss: 0.00002720
Iteration 166/1000 | Loss: 0.00002716
Iteration 167/1000 | Loss: 0.00002714
Iteration 168/1000 | Loss: 0.00002713
Iteration 169/1000 | Loss: 0.00002710
Iteration 170/1000 | Loss: 0.00002704
Iteration 171/1000 | Loss: 0.00002703
Iteration 172/1000 | Loss: 0.00002703
Iteration 173/1000 | Loss: 0.00002703
Iteration 174/1000 | Loss: 0.00002703
Iteration 175/1000 | Loss: 0.00002700
Iteration 176/1000 | Loss: 0.00002700
Iteration 177/1000 | Loss: 0.00002700
Iteration 178/1000 | Loss: 0.00002700
Iteration 179/1000 | Loss: 0.00002700
Iteration 180/1000 | Loss: 0.00002699
Iteration 181/1000 | Loss: 0.00002699
Iteration 182/1000 | Loss: 0.00002699
Iteration 183/1000 | Loss: 0.00002697
Iteration 184/1000 | Loss: 0.00002697
Iteration 185/1000 | Loss: 0.00002697
Iteration 186/1000 | Loss: 0.00002697
Iteration 187/1000 | Loss: 0.00002697
Iteration 188/1000 | Loss: 0.00002697
Iteration 189/1000 | Loss: 0.00002697
Iteration 190/1000 | Loss: 0.00002697
Iteration 191/1000 | Loss: 0.00002697
Iteration 192/1000 | Loss: 0.00002697
Iteration 193/1000 | Loss: 0.00002697
Iteration 194/1000 | Loss: 0.00002697
Iteration 195/1000 | Loss: 0.00002696
Iteration 196/1000 | Loss: 0.00002696
Iteration 197/1000 | Loss: 0.00002696
Iteration 198/1000 | Loss: 0.00002696
Iteration 199/1000 | Loss: 0.00002696
Iteration 200/1000 | Loss: 0.00002696
Iteration 201/1000 | Loss: 0.00002696
Iteration 202/1000 | Loss: 0.00002696
Iteration 203/1000 | Loss: 0.00002696
Iteration 204/1000 | Loss: 0.00002696
Iteration 205/1000 | Loss: 0.00002693
Iteration 206/1000 | Loss: 0.00002693
Iteration 207/1000 | Loss: 0.00002693
Iteration 208/1000 | Loss: 0.00002692
Iteration 209/1000 | Loss: 0.00002692
Iteration 210/1000 | Loss: 0.00002689
Iteration 211/1000 | Loss: 0.00002689
Iteration 212/1000 | Loss: 0.00002688
Iteration 213/1000 | Loss: 0.00002688
Iteration 214/1000 | Loss: 0.00002688
Iteration 215/1000 | Loss: 0.00002687
Iteration 216/1000 | Loss: 0.00002687
Iteration 217/1000 | Loss: 0.00002687
Iteration 218/1000 | Loss: 0.00002686
Iteration 219/1000 | Loss: 0.00002686
Iteration 220/1000 | Loss: 0.00002686
Iteration 221/1000 | Loss: 0.00002686
Iteration 222/1000 | Loss: 0.00002686
Iteration 223/1000 | Loss: 0.00002686
Iteration 224/1000 | Loss: 0.00002686
Iteration 225/1000 | Loss: 0.00002686
Iteration 226/1000 | Loss: 0.00002685
Iteration 227/1000 | Loss: 0.00002685
Iteration 228/1000 | Loss: 0.00002685
Iteration 229/1000 | Loss: 0.00002685
Iteration 230/1000 | Loss: 0.00002685
Iteration 231/1000 | Loss: 0.00002684
Iteration 232/1000 | Loss: 0.00002684
Iteration 233/1000 | Loss: 0.00002684
Iteration 234/1000 | Loss: 0.00002684
Iteration 235/1000 | Loss: 0.00002683
Iteration 236/1000 | Loss: 0.00002683
Iteration 237/1000 | Loss: 0.00002683
Iteration 238/1000 | Loss: 0.00002681
Iteration 239/1000 | Loss: 0.00002681
Iteration 240/1000 | Loss: 0.00002681
Iteration 241/1000 | Loss: 0.00002681
Iteration 242/1000 | Loss: 0.00002681
Iteration 243/1000 | Loss: 0.00002681
Iteration 244/1000 | Loss: 0.00002681
Iteration 245/1000 | Loss: 0.00002681
Iteration 246/1000 | Loss: 0.00002681
Iteration 247/1000 | Loss: 0.00002681
Iteration 248/1000 | Loss: 0.00002680
Iteration 249/1000 | Loss: 0.00002680
Iteration 250/1000 | Loss: 0.00002680
Iteration 251/1000 | Loss: 0.00002680
Iteration 252/1000 | Loss: 0.00002680
Iteration 253/1000 | Loss: 0.00002680
Iteration 254/1000 | Loss: 0.00002679
Iteration 255/1000 | Loss: 0.00002679
Iteration 256/1000 | Loss: 0.00002679
Iteration 257/1000 | Loss: 0.00002679
Iteration 258/1000 | Loss: 0.00002678
Iteration 259/1000 | Loss: 0.00002678
Iteration 260/1000 | Loss: 0.00002678
Iteration 261/1000 | Loss: 0.00002678
Iteration 262/1000 | Loss: 0.00002678
Iteration 263/1000 | Loss: 0.00002677
Iteration 264/1000 | Loss: 0.00002677
Iteration 265/1000 | Loss: 0.00002677
Iteration 266/1000 | Loss: 0.00002676
Iteration 267/1000 | Loss: 0.00002676
Iteration 268/1000 | Loss: 0.00002676
Iteration 269/1000 | Loss: 0.00002676
Iteration 270/1000 | Loss: 0.00002676
Iteration 271/1000 | Loss: 0.00002676
Iteration 272/1000 | Loss: 0.00002676
Iteration 273/1000 | Loss: 0.00002675
Iteration 274/1000 | Loss: 0.00002675
Iteration 275/1000 | Loss: 0.00002675
Iteration 276/1000 | Loss: 0.00002675
Iteration 277/1000 | Loss: 0.00002674
Iteration 278/1000 | Loss: 0.00002674
Iteration 279/1000 | Loss: 0.00002674
Iteration 280/1000 | Loss: 0.00002674
Iteration 281/1000 | Loss: 0.00002674
Iteration 282/1000 | Loss: 0.00002674
Iteration 283/1000 | Loss: 0.00002673
Iteration 284/1000 | Loss: 0.00002673
Iteration 285/1000 | Loss: 0.00002672
Iteration 286/1000 | Loss: 0.00002672
Iteration 287/1000 | Loss: 0.00002672
Iteration 288/1000 | Loss: 0.00002672
Iteration 289/1000 | Loss: 0.00002671
Iteration 290/1000 | Loss: 0.00002671
Iteration 291/1000 | Loss: 0.00002671
Iteration 292/1000 | Loss: 0.00002670
Iteration 293/1000 | Loss: 0.00002670
Iteration 294/1000 | Loss: 0.00002670
Iteration 295/1000 | Loss: 0.00002670
Iteration 296/1000 | Loss: 0.00002670
Iteration 297/1000 | Loss: 0.00002670
Iteration 298/1000 | Loss: 0.00002670
Iteration 299/1000 | Loss: 0.00002670
Iteration 300/1000 | Loss: 0.00002670
Iteration 301/1000 | Loss: 0.00002670
Iteration 302/1000 | Loss: 0.00002670
Iteration 303/1000 | Loss: 0.00002670
Iteration 304/1000 | Loss: 0.00002670
Iteration 305/1000 | Loss: 0.00002670
Iteration 306/1000 | Loss: 0.00002670
Iteration 307/1000 | Loss: 0.00002670
Iteration 308/1000 | Loss: 0.00002669
Iteration 309/1000 | Loss: 0.00002669
Iteration 310/1000 | Loss: 0.00002669
Iteration 311/1000 | Loss: 0.00002669
Iteration 312/1000 | Loss: 0.00002669
Iteration 313/1000 | Loss: 0.00002669
Iteration 314/1000 | Loss: 0.00002669
Iteration 315/1000 | Loss: 0.00002669
Iteration 316/1000 | Loss: 0.00002669
Iteration 317/1000 | Loss: 0.00002669
Iteration 318/1000 | Loss: 0.00002669
Iteration 319/1000 | Loss: 0.00002669
Iteration 320/1000 | Loss: 0.00002669
Iteration 321/1000 | Loss: 0.00002669
Iteration 322/1000 | Loss: 0.00002669
Iteration 323/1000 | Loss: 0.00002669
Iteration 324/1000 | Loss: 0.00002669
Iteration 325/1000 | Loss: 0.00002669
Iteration 326/1000 | Loss: 0.00002669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 326. Stopping optimization.
Last 5 losses: [2.669392415555194e-05, 2.669392415555194e-05, 2.669392415555194e-05, 2.669392415555194e-05, 2.669392415555194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.669392415555194e-05

Optimization complete. Final v2v error: 4.218389987945557 mm

Highest mean error: 5.4623212814331055 mm for frame 154

Lowest mean error: 3.8407516479492188 mm for frame 130

Saving results

Total time: 306.3054130077362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770612
Iteration 2/25 | Loss: 0.00105371
Iteration 3/25 | Loss: 0.00089447
Iteration 4/25 | Loss: 0.00085218
Iteration 5/25 | Loss: 0.00084093
Iteration 6/25 | Loss: 0.00083916
Iteration 7/25 | Loss: 0.00083901
Iteration 8/25 | Loss: 0.00083901
Iteration 9/25 | Loss: 0.00083901
Iteration 10/25 | Loss: 0.00083901
Iteration 11/25 | Loss: 0.00083901
Iteration 12/25 | Loss: 0.00083901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008390147122554481, 0.0008390147122554481, 0.0008390147122554481, 0.0008390147122554481, 0.0008390147122554481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008390147122554481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47369945
Iteration 2/25 | Loss: 0.00059848
Iteration 3/25 | Loss: 0.00059845
Iteration 4/25 | Loss: 0.00059845
Iteration 5/25 | Loss: 0.00059845
Iteration 6/25 | Loss: 0.00059845
Iteration 7/25 | Loss: 0.00059845
Iteration 8/25 | Loss: 0.00059845
Iteration 9/25 | Loss: 0.00059845
Iteration 10/25 | Loss: 0.00059845
Iteration 11/25 | Loss: 0.00059845
Iteration 12/25 | Loss: 0.00059845
Iteration 13/25 | Loss: 0.00059845
Iteration 14/25 | Loss: 0.00059845
Iteration 15/25 | Loss: 0.00059845
Iteration 16/25 | Loss: 0.00059845
Iteration 17/25 | Loss: 0.00059845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005984474555589259, 0.0005984474555589259, 0.0005984474555589259, 0.0005984474555589259, 0.0005984474555589259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005984474555589259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059845
Iteration 2/1000 | Loss: 0.00002986
Iteration 3/1000 | Loss: 0.00002399
Iteration 4/1000 | Loss: 0.00002185
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001930
Iteration 8/1000 | Loss: 0.00001908
Iteration 9/1000 | Loss: 0.00001896
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001876
Iteration 13/1000 | Loss: 0.00001875
Iteration 14/1000 | Loss: 0.00001875
Iteration 15/1000 | Loss: 0.00001868
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001862
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001845
Iteration 20/1000 | Loss: 0.00001841
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001839
Iteration 23/1000 | Loss: 0.00001836
Iteration 24/1000 | Loss: 0.00001833
Iteration 25/1000 | Loss: 0.00001832
Iteration 26/1000 | Loss: 0.00001830
Iteration 27/1000 | Loss: 0.00001830
Iteration 28/1000 | Loss: 0.00001829
Iteration 29/1000 | Loss: 0.00001828
Iteration 30/1000 | Loss: 0.00001828
Iteration 31/1000 | Loss: 0.00001827
Iteration 32/1000 | Loss: 0.00001826
Iteration 33/1000 | Loss: 0.00001826
Iteration 34/1000 | Loss: 0.00001826
Iteration 35/1000 | Loss: 0.00001826
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001825
Iteration 38/1000 | Loss: 0.00001825
Iteration 39/1000 | Loss: 0.00001825
Iteration 40/1000 | Loss: 0.00001825
Iteration 41/1000 | Loss: 0.00001825
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001825
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001824
Iteration 46/1000 | Loss: 0.00001824
Iteration 47/1000 | Loss: 0.00001824
Iteration 48/1000 | Loss: 0.00001824
Iteration 49/1000 | Loss: 0.00001824
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001822
Iteration 54/1000 | Loss: 0.00001822
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001822
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001821
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001816
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001815
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001814
Iteration 81/1000 | Loss: 0.00001814
Iteration 82/1000 | Loss: 0.00001814
Iteration 83/1000 | Loss: 0.00001814
Iteration 84/1000 | Loss: 0.00001814
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001813
Iteration 88/1000 | Loss: 0.00001812
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001811
Iteration 91/1000 | Loss: 0.00001811
Iteration 92/1000 | Loss: 0.00001810
Iteration 93/1000 | Loss: 0.00001810
Iteration 94/1000 | Loss: 0.00001810
Iteration 95/1000 | Loss: 0.00001810
Iteration 96/1000 | Loss: 0.00001810
Iteration 97/1000 | Loss: 0.00001810
Iteration 98/1000 | Loss: 0.00001810
Iteration 99/1000 | Loss: 0.00001810
Iteration 100/1000 | Loss: 0.00001810
Iteration 101/1000 | Loss: 0.00001810
Iteration 102/1000 | Loss: 0.00001810
Iteration 103/1000 | Loss: 0.00001810
Iteration 104/1000 | Loss: 0.00001809
Iteration 105/1000 | Loss: 0.00001809
Iteration 106/1000 | Loss: 0.00001809
Iteration 107/1000 | Loss: 0.00001809
Iteration 108/1000 | Loss: 0.00001808
Iteration 109/1000 | Loss: 0.00001808
Iteration 110/1000 | Loss: 0.00001808
Iteration 111/1000 | Loss: 0.00001808
Iteration 112/1000 | Loss: 0.00001808
Iteration 113/1000 | Loss: 0.00001808
Iteration 114/1000 | Loss: 0.00001808
Iteration 115/1000 | Loss: 0.00001808
Iteration 116/1000 | Loss: 0.00001808
Iteration 117/1000 | Loss: 0.00001808
Iteration 118/1000 | Loss: 0.00001808
Iteration 119/1000 | Loss: 0.00001808
Iteration 120/1000 | Loss: 0.00001807
Iteration 121/1000 | Loss: 0.00001807
Iteration 122/1000 | Loss: 0.00001807
Iteration 123/1000 | Loss: 0.00001807
Iteration 124/1000 | Loss: 0.00001807
Iteration 125/1000 | Loss: 0.00001807
Iteration 126/1000 | Loss: 0.00001807
Iteration 127/1000 | Loss: 0.00001807
Iteration 128/1000 | Loss: 0.00001807
Iteration 129/1000 | Loss: 0.00001807
Iteration 130/1000 | Loss: 0.00001807
Iteration 131/1000 | Loss: 0.00001807
Iteration 132/1000 | Loss: 0.00001807
Iteration 133/1000 | Loss: 0.00001806
Iteration 134/1000 | Loss: 0.00001806
Iteration 135/1000 | Loss: 0.00001806
Iteration 136/1000 | Loss: 0.00001806
Iteration 137/1000 | Loss: 0.00001806
Iteration 138/1000 | Loss: 0.00001806
Iteration 139/1000 | Loss: 0.00001806
Iteration 140/1000 | Loss: 0.00001806
Iteration 141/1000 | Loss: 0.00001806
Iteration 142/1000 | Loss: 0.00001806
Iteration 143/1000 | Loss: 0.00001806
Iteration 144/1000 | Loss: 0.00001806
Iteration 145/1000 | Loss: 0.00001806
Iteration 146/1000 | Loss: 0.00001806
Iteration 147/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.80567403731402e-05, 1.80567403731402e-05, 1.80567403731402e-05, 1.80567403731402e-05, 1.80567403731402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.80567403731402e-05

Optimization complete. Final v2v error: 3.610645055770874 mm

Highest mean error: 4.080139636993408 mm for frame 211

Lowest mean error: 3.3813021183013916 mm for frame 62

Saving results

Total time: 41.89848613739014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403239
Iteration 2/25 | Loss: 0.00094138
Iteration 3/25 | Loss: 0.00081562
Iteration 4/25 | Loss: 0.00079957
Iteration 5/25 | Loss: 0.00079552
Iteration 6/25 | Loss: 0.00079409
Iteration 7/25 | Loss: 0.00079379
Iteration 8/25 | Loss: 0.00079379
Iteration 9/25 | Loss: 0.00079379
Iteration 10/25 | Loss: 0.00079379
Iteration 11/25 | Loss: 0.00079379
Iteration 12/25 | Loss: 0.00079379
Iteration 13/25 | Loss: 0.00079379
Iteration 14/25 | Loss: 0.00079379
Iteration 15/25 | Loss: 0.00079379
Iteration 16/25 | Loss: 0.00079379
Iteration 17/25 | Loss: 0.00079379
Iteration 18/25 | Loss: 0.00079379
Iteration 19/25 | Loss: 0.00079379
Iteration 20/25 | Loss: 0.00079379
Iteration 21/25 | Loss: 0.00079379
Iteration 22/25 | Loss: 0.00079379
Iteration 23/25 | Loss: 0.00079379
Iteration 24/25 | Loss: 0.00079379
Iteration 25/25 | Loss: 0.00079379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51760435
Iteration 2/25 | Loss: 0.00052123
Iteration 3/25 | Loss: 0.00052123
Iteration 4/25 | Loss: 0.00052123
Iteration 5/25 | Loss: 0.00052123
Iteration 6/25 | Loss: 0.00052123
Iteration 7/25 | Loss: 0.00052123
Iteration 8/25 | Loss: 0.00052123
Iteration 9/25 | Loss: 0.00052123
Iteration 10/25 | Loss: 0.00052123
Iteration 11/25 | Loss: 0.00052123
Iteration 12/25 | Loss: 0.00052123
Iteration 13/25 | Loss: 0.00052123
Iteration 14/25 | Loss: 0.00052122
Iteration 15/25 | Loss: 0.00052122
Iteration 16/25 | Loss: 0.00052122
Iteration 17/25 | Loss: 0.00052122
Iteration 18/25 | Loss: 0.00052122
Iteration 19/25 | Loss: 0.00052122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005212249816395342, 0.0005212249816395342, 0.0005212249816395342, 0.0005212249816395342, 0.0005212249816395342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005212249816395342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052122
Iteration 2/1000 | Loss: 0.00002050
Iteration 3/1000 | Loss: 0.00001426
Iteration 4/1000 | Loss: 0.00001239
Iteration 5/1000 | Loss: 0.00001158
Iteration 6/1000 | Loss: 0.00001100
Iteration 7/1000 | Loss: 0.00001073
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001062
Iteration 10/1000 | Loss: 0.00001059
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001052
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001051
Iteration 15/1000 | Loss: 0.00001051
Iteration 16/1000 | Loss: 0.00001048
Iteration 17/1000 | Loss: 0.00001048
Iteration 18/1000 | Loss: 0.00001045
Iteration 19/1000 | Loss: 0.00001044
Iteration 20/1000 | Loss: 0.00001044
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001042
Iteration 23/1000 | Loss: 0.00001041
Iteration 24/1000 | Loss: 0.00001041
Iteration 25/1000 | Loss: 0.00001041
Iteration 26/1000 | Loss: 0.00001040
Iteration 27/1000 | Loss: 0.00001039
Iteration 28/1000 | Loss: 0.00001038
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001035
Iteration 31/1000 | Loss: 0.00001035
Iteration 32/1000 | Loss: 0.00001035
Iteration 33/1000 | Loss: 0.00001035
Iteration 34/1000 | Loss: 0.00001035
Iteration 35/1000 | Loss: 0.00001035
Iteration 36/1000 | Loss: 0.00001035
Iteration 37/1000 | Loss: 0.00001035
Iteration 38/1000 | Loss: 0.00001034
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001033
Iteration 41/1000 | Loss: 0.00001031
Iteration 42/1000 | Loss: 0.00001031
Iteration 43/1000 | Loss: 0.00001030
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001030
Iteration 46/1000 | Loss: 0.00001028
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001026
Iteration 53/1000 | Loss: 0.00001026
Iteration 54/1000 | Loss: 0.00001026
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001025
Iteration 57/1000 | Loss: 0.00001025
Iteration 58/1000 | Loss: 0.00001024
Iteration 59/1000 | Loss: 0.00001024
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001022
Iteration 62/1000 | Loss: 0.00001022
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001020
Iteration 65/1000 | Loss: 0.00001020
Iteration 66/1000 | Loss: 0.00001019
Iteration 67/1000 | Loss: 0.00001018
Iteration 68/1000 | Loss: 0.00001018
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001016
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001013
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001013
Iteration 80/1000 | Loss: 0.00001013
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001013
Iteration 84/1000 | Loss: 0.00001013
Iteration 85/1000 | Loss: 0.00001013
Iteration 86/1000 | Loss: 0.00001013
Iteration 87/1000 | Loss: 0.00001013
Iteration 88/1000 | Loss: 0.00001013
Iteration 89/1000 | Loss: 0.00001013
Iteration 90/1000 | Loss: 0.00001013
Iteration 91/1000 | Loss: 0.00001013
Iteration 92/1000 | Loss: 0.00001013
Iteration 93/1000 | Loss: 0.00001013
Iteration 94/1000 | Loss: 0.00001013
Iteration 95/1000 | Loss: 0.00001012
Iteration 96/1000 | Loss: 0.00001012
Iteration 97/1000 | Loss: 0.00001012
Iteration 98/1000 | Loss: 0.00001012
Iteration 99/1000 | Loss: 0.00001012
Iteration 100/1000 | Loss: 0.00001012
Iteration 101/1000 | Loss: 0.00001012
Iteration 102/1000 | Loss: 0.00001012
Iteration 103/1000 | Loss: 0.00001012
Iteration 104/1000 | Loss: 0.00001012
Iteration 105/1000 | Loss: 0.00001012
Iteration 106/1000 | Loss: 0.00001012
Iteration 107/1000 | Loss: 0.00001012
Iteration 108/1000 | Loss: 0.00001012
Iteration 109/1000 | Loss: 0.00001012
Iteration 110/1000 | Loss: 0.00001012
Iteration 111/1000 | Loss: 0.00001012
Iteration 112/1000 | Loss: 0.00001012
Iteration 113/1000 | Loss: 0.00001012
Iteration 114/1000 | Loss: 0.00001012
Iteration 115/1000 | Loss: 0.00001012
Iteration 116/1000 | Loss: 0.00001012
Iteration 117/1000 | Loss: 0.00001012
Iteration 118/1000 | Loss: 0.00001012
Iteration 119/1000 | Loss: 0.00001012
Iteration 120/1000 | Loss: 0.00001012
Iteration 121/1000 | Loss: 0.00001012
Iteration 122/1000 | Loss: 0.00001012
Iteration 123/1000 | Loss: 0.00001012
Iteration 124/1000 | Loss: 0.00001012
Iteration 125/1000 | Loss: 0.00001012
Iteration 126/1000 | Loss: 0.00001012
Iteration 127/1000 | Loss: 0.00001012
Iteration 128/1000 | Loss: 0.00001012
Iteration 129/1000 | Loss: 0.00001012
Iteration 130/1000 | Loss: 0.00001012
Iteration 131/1000 | Loss: 0.00001012
Iteration 132/1000 | Loss: 0.00001012
Iteration 133/1000 | Loss: 0.00001012
Iteration 134/1000 | Loss: 0.00001012
Iteration 135/1000 | Loss: 0.00001012
Iteration 136/1000 | Loss: 0.00001012
Iteration 137/1000 | Loss: 0.00001012
Iteration 138/1000 | Loss: 0.00001012
Iteration 139/1000 | Loss: 0.00001012
Iteration 140/1000 | Loss: 0.00001012
Iteration 141/1000 | Loss: 0.00001012
Iteration 142/1000 | Loss: 0.00001012
Iteration 143/1000 | Loss: 0.00001012
Iteration 144/1000 | Loss: 0.00001012
Iteration 145/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.0121832019649446e-05, 1.0121832019649446e-05, 1.0121832019649446e-05, 1.0121832019649446e-05, 1.0121832019649446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0121832019649446e-05

Optimization complete. Final v2v error: 2.7030258178710938 mm

Highest mean error: 3.521263360977173 mm for frame 61

Lowest mean error: 2.517072916030884 mm for frame 85

Saving results

Total time: 31.392658710479736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081507
Iteration 2/25 | Loss: 0.01081507
Iteration 3/25 | Loss: 0.01081507
Iteration 4/25 | Loss: 0.01081507
Iteration 5/25 | Loss: 0.01081507
Iteration 6/25 | Loss: 0.01081506
Iteration 7/25 | Loss: 0.01081506
Iteration 8/25 | Loss: 0.01081506
Iteration 9/25 | Loss: 0.01081506
Iteration 10/25 | Loss: 0.01081506
Iteration 11/25 | Loss: 0.01081506
Iteration 12/25 | Loss: 0.01081506
Iteration 13/25 | Loss: 0.01081506
Iteration 14/25 | Loss: 0.01081505
Iteration 15/25 | Loss: 0.01081505
Iteration 16/25 | Loss: 0.01081505
Iteration 17/25 | Loss: 0.01081505
Iteration 18/25 | Loss: 0.01081505
Iteration 19/25 | Loss: 0.01081505
Iteration 20/25 | Loss: 0.01081505
Iteration 21/25 | Loss: 0.01081504
Iteration 22/25 | Loss: 0.01081504
Iteration 23/25 | Loss: 0.01081504
Iteration 24/25 | Loss: 0.01081504
Iteration 25/25 | Loss: 0.01081504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50348222
Iteration 2/25 | Loss: 0.16117109
Iteration 3/25 | Loss: 0.16049567
Iteration 4/25 | Loss: 0.16049567
Iteration 5/25 | Loss: 0.16049567
Iteration 6/25 | Loss: 0.16049564
Iteration 7/25 | Loss: 0.16049564
Iteration 8/25 | Loss: 0.16049564
Iteration 9/25 | Loss: 0.16049564
Iteration 10/25 | Loss: 0.16049564
Iteration 11/25 | Loss: 0.16049564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.16049563884735107, 0.16049563884735107, 0.16049563884735107, 0.16049563884735107, 0.16049563884735107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16049563884735107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16049564
Iteration 2/1000 | Loss: 0.00204586
Iteration 3/1000 | Loss: 0.00223458
Iteration 4/1000 | Loss: 0.00145539
Iteration 5/1000 | Loss: 0.00027770
Iteration 6/1000 | Loss: 0.00155069
Iteration 7/1000 | Loss: 0.00018128
Iteration 8/1000 | Loss: 0.00011478
Iteration 9/1000 | Loss: 0.00004224
Iteration 10/1000 | Loss: 0.00003597
Iteration 11/1000 | Loss: 0.00045060
Iteration 12/1000 | Loss: 0.00011872
Iteration 13/1000 | Loss: 0.00179951
Iteration 14/1000 | Loss: 0.00074255
Iteration 15/1000 | Loss: 0.00030919
Iteration 16/1000 | Loss: 0.00003013
Iteration 17/1000 | Loss: 0.00002644
Iteration 18/1000 | Loss: 0.00028608
Iteration 19/1000 | Loss: 0.00002382
Iteration 20/1000 | Loss: 0.00021007
Iteration 21/1000 | Loss: 0.00027342
Iteration 22/1000 | Loss: 0.00004827
Iteration 23/1000 | Loss: 0.00009126
Iteration 24/1000 | Loss: 0.00002109
Iteration 25/1000 | Loss: 0.00002002
Iteration 26/1000 | Loss: 0.00001904
Iteration 27/1000 | Loss: 0.00013911
Iteration 28/1000 | Loss: 0.00001831
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001654
Iteration 32/1000 | Loss: 0.00001628
Iteration 33/1000 | Loss: 0.00001600
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001575
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001556
Iteration 46/1000 | Loss: 0.00001556
Iteration 47/1000 | Loss: 0.00001556
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001555
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001554
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001553
Iteration 58/1000 | Loss: 0.00001553
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001552
Iteration 62/1000 | Loss: 0.00001552
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001551
Iteration 66/1000 | Loss: 0.00001551
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001548
Iteration 72/1000 | Loss: 0.00001548
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001547
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001544
Iteration 89/1000 | Loss: 0.00001544
Iteration 90/1000 | Loss: 0.00001544
Iteration 91/1000 | Loss: 0.00001544
Iteration 92/1000 | Loss: 0.00001544
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001543
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001543
Iteration 100/1000 | Loss: 0.00001543
Iteration 101/1000 | Loss: 0.00001543
Iteration 102/1000 | Loss: 0.00001543
Iteration 103/1000 | Loss: 0.00001543
Iteration 104/1000 | Loss: 0.00001543
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001542
Iteration 108/1000 | Loss: 0.00001542
Iteration 109/1000 | Loss: 0.00001542
Iteration 110/1000 | Loss: 0.00001542
Iteration 111/1000 | Loss: 0.00001542
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001538
Iteration 134/1000 | Loss: 0.00001538
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001538
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001536
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001536
Iteration 156/1000 | Loss: 0.00001536
Iteration 157/1000 | Loss: 0.00001536
Iteration 158/1000 | Loss: 0.00001536
Iteration 159/1000 | Loss: 0.00001536
Iteration 160/1000 | Loss: 0.00001536
Iteration 161/1000 | Loss: 0.00001536
Iteration 162/1000 | Loss: 0.00001536
Iteration 163/1000 | Loss: 0.00001536
Iteration 164/1000 | Loss: 0.00001536
Iteration 165/1000 | Loss: 0.00001536
Iteration 166/1000 | Loss: 0.00001536
Iteration 167/1000 | Loss: 0.00001536
Iteration 168/1000 | Loss: 0.00001536
Iteration 169/1000 | Loss: 0.00001536
Iteration 170/1000 | Loss: 0.00001536
Iteration 171/1000 | Loss: 0.00001536
Iteration 172/1000 | Loss: 0.00001536
Iteration 173/1000 | Loss: 0.00001536
Iteration 174/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.536166382720694e-05, 1.536166382720694e-05, 1.536166382720694e-05, 1.536166382720694e-05, 1.536166382720694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536166382720694e-05

Optimization complete. Final v2v error: 3.310656785964966 mm

Highest mean error: 3.584174871444702 mm for frame 127

Lowest mean error: 2.942017078399658 mm for frame 1

Saving results

Total time: 70.36644864082336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111065
Iteration 2/25 | Loss: 0.00372378
Iteration 3/25 | Loss: 0.00260429
Iteration 4/25 | Loss: 0.00256425
Iteration 5/25 | Loss: 0.00210049
Iteration 6/25 | Loss: 0.00237320
Iteration 7/25 | Loss: 0.00209796
Iteration 8/25 | Loss: 0.00186367
Iteration 9/25 | Loss: 0.00166237
Iteration 10/25 | Loss: 0.00151910
Iteration 11/25 | Loss: 0.00139057
Iteration 12/25 | Loss: 0.00135073
Iteration 13/25 | Loss: 0.00131509
Iteration 14/25 | Loss: 0.00131765
Iteration 15/25 | Loss: 0.00129602
Iteration 16/25 | Loss: 0.00129337
Iteration 17/25 | Loss: 0.00127975
Iteration 18/25 | Loss: 0.00127490
Iteration 19/25 | Loss: 0.00127447
Iteration 20/25 | Loss: 0.00126706
Iteration 21/25 | Loss: 0.00125956
Iteration 22/25 | Loss: 0.00125421
Iteration 23/25 | Loss: 0.00125143
Iteration 24/25 | Loss: 0.00124782
Iteration 25/25 | Loss: 0.00124999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69043881
Iteration 2/25 | Loss: 0.00152005
Iteration 3/25 | Loss: 0.00121502
Iteration 4/25 | Loss: 0.00121502
Iteration 5/25 | Loss: 0.00121502
Iteration 6/25 | Loss: 0.00121502
Iteration 7/25 | Loss: 0.00121502
Iteration 8/25 | Loss: 0.00121502
Iteration 9/25 | Loss: 0.00121502
Iteration 10/25 | Loss: 0.00121502
Iteration 11/25 | Loss: 0.00121502
Iteration 12/25 | Loss: 0.00121502
Iteration 13/25 | Loss: 0.00121502
Iteration 14/25 | Loss: 0.00121502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012150161201134324, 0.0012150161201134324, 0.0012150161201134324, 0.0012150161201134324, 0.0012150161201134324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012150161201134324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121502
Iteration 2/1000 | Loss: 0.00059043
Iteration 3/1000 | Loss: 0.00050540
Iteration 4/1000 | Loss: 0.00038128
Iteration 5/1000 | Loss: 0.00059400
Iteration 6/1000 | Loss: 0.00032580
Iteration 7/1000 | Loss: 0.00030002
Iteration 8/1000 | Loss: 0.00060800
Iteration 9/1000 | Loss: 0.00050352
Iteration 10/1000 | Loss: 0.00027082
Iteration 11/1000 | Loss: 0.00035207
Iteration 12/1000 | Loss: 0.00019248
Iteration 13/1000 | Loss: 0.00017177
Iteration 14/1000 | Loss: 0.00023337
Iteration 15/1000 | Loss: 0.00032836
Iteration 16/1000 | Loss: 0.00019592
Iteration 17/1000 | Loss: 0.00020916
Iteration 18/1000 | Loss: 0.00021383
Iteration 19/1000 | Loss: 0.00025567
Iteration 20/1000 | Loss: 0.00024158
Iteration 21/1000 | Loss: 0.00018564
Iteration 22/1000 | Loss: 0.00008793
Iteration 23/1000 | Loss: 0.00021350
Iteration 24/1000 | Loss: 0.00018762
Iteration 25/1000 | Loss: 0.00021957
Iteration 26/1000 | Loss: 0.00019956
Iteration 27/1000 | Loss: 0.00021076
Iteration 28/1000 | Loss: 0.00009098
Iteration 29/1000 | Loss: 0.00019004
Iteration 30/1000 | Loss: 0.00018244
Iteration 31/1000 | Loss: 0.00008709
Iteration 32/1000 | Loss: 0.00008329
Iteration 33/1000 | Loss: 0.00020551
Iteration 34/1000 | Loss: 0.00029605
Iteration 35/1000 | Loss: 0.00010808
Iteration 36/1000 | Loss: 0.00022182
Iteration 37/1000 | Loss: 0.00008045
Iteration 38/1000 | Loss: 0.00007711
Iteration 39/1000 | Loss: 0.00007496
Iteration 40/1000 | Loss: 0.00018215
Iteration 41/1000 | Loss: 0.00018526
Iteration 42/1000 | Loss: 0.00016235
Iteration 43/1000 | Loss: 0.00007861
Iteration 44/1000 | Loss: 0.00007371
Iteration 45/1000 | Loss: 0.00007220
Iteration 46/1000 | Loss: 0.00023569
Iteration 47/1000 | Loss: 0.00019843
Iteration 48/1000 | Loss: 0.00011373
Iteration 49/1000 | Loss: 0.00025468
Iteration 50/1000 | Loss: 0.00007989
Iteration 51/1000 | Loss: 0.00007317
Iteration 52/1000 | Loss: 0.00023588
Iteration 53/1000 | Loss: 0.00007062
Iteration 54/1000 | Loss: 0.00006904
Iteration 55/1000 | Loss: 0.00006815
Iteration 56/1000 | Loss: 0.00006754
Iteration 57/1000 | Loss: 0.00006655
Iteration 58/1000 | Loss: 0.00006597
Iteration 59/1000 | Loss: 0.00006569
Iteration 60/1000 | Loss: 0.00015155
Iteration 61/1000 | Loss: 0.00024235
Iteration 62/1000 | Loss: 0.00025843
Iteration 63/1000 | Loss: 0.00013498
Iteration 64/1000 | Loss: 0.00014796
Iteration 65/1000 | Loss: 0.00015117
Iteration 66/1000 | Loss: 0.00007018
Iteration 67/1000 | Loss: 0.00006801
Iteration 68/1000 | Loss: 0.00015792
Iteration 69/1000 | Loss: 0.00007073
Iteration 70/1000 | Loss: 0.00006947
Iteration 71/1000 | Loss: 0.00006847
Iteration 72/1000 | Loss: 0.00017986
Iteration 73/1000 | Loss: 0.00015764
Iteration 74/1000 | Loss: 0.00006937
Iteration 75/1000 | Loss: 0.00006773
Iteration 76/1000 | Loss: 0.00006688
Iteration 77/1000 | Loss: 0.00019172
Iteration 78/1000 | Loss: 0.00011119
Iteration 79/1000 | Loss: 0.00007985
Iteration 80/1000 | Loss: 0.00013799
Iteration 81/1000 | Loss: 0.00007020
Iteration 82/1000 | Loss: 0.00006668
Iteration 83/1000 | Loss: 0.00006555
Iteration 84/1000 | Loss: 0.00006498
Iteration 85/1000 | Loss: 0.00006474
Iteration 86/1000 | Loss: 0.00006463
Iteration 87/1000 | Loss: 0.00006456
Iteration 88/1000 | Loss: 0.00006445
Iteration 89/1000 | Loss: 0.00006434
Iteration 90/1000 | Loss: 0.00006421
Iteration 91/1000 | Loss: 0.00006415
Iteration 92/1000 | Loss: 0.00006415
Iteration 93/1000 | Loss: 0.00006411
Iteration 94/1000 | Loss: 0.00006410
Iteration 95/1000 | Loss: 0.00006400
Iteration 96/1000 | Loss: 0.00006400
Iteration 97/1000 | Loss: 0.00006400
Iteration 98/1000 | Loss: 0.00006400
Iteration 99/1000 | Loss: 0.00006400
Iteration 100/1000 | Loss: 0.00006400
Iteration 101/1000 | Loss: 0.00006400
Iteration 102/1000 | Loss: 0.00006400
Iteration 103/1000 | Loss: 0.00006399
Iteration 104/1000 | Loss: 0.00006399
Iteration 105/1000 | Loss: 0.00006399
Iteration 106/1000 | Loss: 0.00006399
Iteration 107/1000 | Loss: 0.00006399
Iteration 108/1000 | Loss: 0.00006399
Iteration 109/1000 | Loss: 0.00006397
Iteration 110/1000 | Loss: 0.00006396
Iteration 111/1000 | Loss: 0.00006396
Iteration 112/1000 | Loss: 0.00006396
Iteration 113/1000 | Loss: 0.00006395
Iteration 114/1000 | Loss: 0.00006395
Iteration 115/1000 | Loss: 0.00006394
Iteration 116/1000 | Loss: 0.00006394
Iteration 117/1000 | Loss: 0.00006393
Iteration 118/1000 | Loss: 0.00006393
Iteration 119/1000 | Loss: 0.00006393
Iteration 120/1000 | Loss: 0.00006392
Iteration 121/1000 | Loss: 0.00006392
Iteration 122/1000 | Loss: 0.00006392
Iteration 123/1000 | Loss: 0.00006391
Iteration 124/1000 | Loss: 0.00006391
Iteration 125/1000 | Loss: 0.00006391
Iteration 126/1000 | Loss: 0.00006391
Iteration 127/1000 | Loss: 0.00006391
Iteration 128/1000 | Loss: 0.00006391
Iteration 129/1000 | Loss: 0.00006391
Iteration 130/1000 | Loss: 0.00006391
Iteration 131/1000 | Loss: 0.00006390
Iteration 132/1000 | Loss: 0.00006390
Iteration 133/1000 | Loss: 0.00006390
Iteration 134/1000 | Loss: 0.00006390
Iteration 135/1000 | Loss: 0.00006390
Iteration 136/1000 | Loss: 0.00006390
Iteration 137/1000 | Loss: 0.00006390
Iteration 138/1000 | Loss: 0.00006390
Iteration 139/1000 | Loss: 0.00006390
Iteration 140/1000 | Loss: 0.00006390
Iteration 141/1000 | Loss: 0.00006390
Iteration 142/1000 | Loss: 0.00006390
Iteration 143/1000 | Loss: 0.00006389
Iteration 144/1000 | Loss: 0.00006389
Iteration 145/1000 | Loss: 0.00006389
Iteration 146/1000 | Loss: 0.00006389
Iteration 147/1000 | Loss: 0.00006389
Iteration 148/1000 | Loss: 0.00006389
Iteration 149/1000 | Loss: 0.00006388
Iteration 150/1000 | Loss: 0.00006388
Iteration 151/1000 | Loss: 0.00006388
Iteration 152/1000 | Loss: 0.00006388
Iteration 153/1000 | Loss: 0.00006388
Iteration 154/1000 | Loss: 0.00006388
Iteration 155/1000 | Loss: 0.00006388
Iteration 156/1000 | Loss: 0.00006388
Iteration 157/1000 | Loss: 0.00006388
Iteration 158/1000 | Loss: 0.00006388
Iteration 159/1000 | Loss: 0.00006388
Iteration 160/1000 | Loss: 0.00006387
Iteration 161/1000 | Loss: 0.00006387
Iteration 162/1000 | Loss: 0.00006386
Iteration 163/1000 | Loss: 0.00006386
Iteration 164/1000 | Loss: 0.00006386
Iteration 165/1000 | Loss: 0.00006386
Iteration 166/1000 | Loss: 0.00006385
Iteration 167/1000 | Loss: 0.00006385
Iteration 168/1000 | Loss: 0.00006385
Iteration 169/1000 | Loss: 0.00006384
Iteration 170/1000 | Loss: 0.00006384
Iteration 171/1000 | Loss: 0.00006384
Iteration 172/1000 | Loss: 0.00006384
Iteration 173/1000 | Loss: 0.00006384
Iteration 174/1000 | Loss: 0.00006383
Iteration 175/1000 | Loss: 0.00006383
Iteration 176/1000 | Loss: 0.00006383
Iteration 177/1000 | Loss: 0.00006383
Iteration 178/1000 | Loss: 0.00006383
Iteration 179/1000 | Loss: 0.00006383
Iteration 180/1000 | Loss: 0.00006383
Iteration 181/1000 | Loss: 0.00006383
Iteration 182/1000 | Loss: 0.00006383
Iteration 183/1000 | Loss: 0.00006383
Iteration 184/1000 | Loss: 0.00006382
Iteration 185/1000 | Loss: 0.00006382
Iteration 186/1000 | Loss: 0.00006382
Iteration 187/1000 | Loss: 0.00006382
Iteration 188/1000 | Loss: 0.00006382
Iteration 189/1000 | Loss: 0.00006382
Iteration 190/1000 | Loss: 0.00006382
Iteration 191/1000 | Loss: 0.00006382
Iteration 192/1000 | Loss: 0.00006382
Iteration 193/1000 | Loss: 0.00006381
Iteration 194/1000 | Loss: 0.00006381
Iteration 195/1000 | Loss: 0.00006381
Iteration 196/1000 | Loss: 0.00006381
Iteration 197/1000 | Loss: 0.00006381
Iteration 198/1000 | Loss: 0.00006381
Iteration 199/1000 | Loss: 0.00006381
Iteration 200/1000 | Loss: 0.00006381
Iteration 201/1000 | Loss: 0.00006381
Iteration 202/1000 | Loss: 0.00006381
Iteration 203/1000 | Loss: 0.00006380
Iteration 204/1000 | Loss: 0.00006380
Iteration 205/1000 | Loss: 0.00006380
Iteration 206/1000 | Loss: 0.00006379
Iteration 207/1000 | Loss: 0.00006379
Iteration 208/1000 | Loss: 0.00006379
Iteration 209/1000 | Loss: 0.00006379
Iteration 210/1000 | Loss: 0.00006379
Iteration 211/1000 | Loss: 0.00006379
Iteration 212/1000 | Loss: 0.00006379
Iteration 213/1000 | Loss: 0.00006378
Iteration 214/1000 | Loss: 0.00006378
Iteration 215/1000 | Loss: 0.00006378
Iteration 216/1000 | Loss: 0.00006378
Iteration 217/1000 | Loss: 0.00006378
Iteration 218/1000 | Loss: 0.00006378
Iteration 219/1000 | Loss: 0.00006378
Iteration 220/1000 | Loss: 0.00006378
Iteration 221/1000 | Loss: 0.00006378
Iteration 222/1000 | Loss: 0.00006378
Iteration 223/1000 | Loss: 0.00006378
Iteration 224/1000 | Loss: 0.00006377
Iteration 225/1000 | Loss: 0.00006377
Iteration 226/1000 | Loss: 0.00006374
Iteration 227/1000 | Loss: 0.00006374
Iteration 228/1000 | Loss: 0.00006373
Iteration 229/1000 | Loss: 0.00006373
Iteration 230/1000 | Loss: 0.00006372
Iteration 231/1000 | Loss: 0.00006371
Iteration 232/1000 | Loss: 0.00006371
Iteration 233/1000 | Loss: 0.00006369
Iteration 234/1000 | Loss: 0.00006368
Iteration 235/1000 | Loss: 0.00006366
Iteration 236/1000 | Loss: 0.00006358
Iteration 237/1000 | Loss: 0.00007444
Iteration 238/1000 | Loss: 0.00006597
Iteration 239/1000 | Loss: 0.00006459
Iteration 240/1000 | Loss: 0.00006391
Iteration 241/1000 | Loss: 0.00006364
Iteration 242/1000 | Loss: 0.00006349
Iteration 243/1000 | Loss: 0.00006348
Iteration 244/1000 | Loss: 0.00006344
Iteration 245/1000 | Loss: 0.00006341
Iteration 246/1000 | Loss: 0.00006341
Iteration 247/1000 | Loss: 0.00006340
Iteration 248/1000 | Loss: 0.00006340
Iteration 249/1000 | Loss: 0.00006338
Iteration 250/1000 | Loss: 0.00006338
Iteration 251/1000 | Loss: 0.00006337
Iteration 252/1000 | Loss: 0.00006337
Iteration 253/1000 | Loss: 0.00006336
Iteration 254/1000 | Loss: 0.00006336
Iteration 255/1000 | Loss: 0.00006336
Iteration 256/1000 | Loss: 0.00006336
Iteration 257/1000 | Loss: 0.00006335
Iteration 258/1000 | Loss: 0.00006335
Iteration 259/1000 | Loss: 0.00006335
Iteration 260/1000 | Loss: 0.00006335
Iteration 261/1000 | Loss: 0.00006335
Iteration 262/1000 | Loss: 0.00006335
Iteration 263/1000 | Loss: 0.00006335
Iteration 264/1000 | Loss: 0.00006335
Iteration 265/1000 | Loss: 0.00006335
Iteration 266/1000 | Loss: 0.00006335
Iteration 267/1000 | Loss: 0.00006335
Iteration 268/1000 | Loss: 0.00006335
Iteration 269/1000 | Loss: 0.00006335
Iteration 270/1000 | Loss: 0.00006334
Iteration 271/1000 | Loss: 0.00006334
Iteration 272/1000 | Loss: 0.00006334
Iteration 273/1000 | Loss: 0.00006334
Iteration 274/1000 | Loss: 0.00006334
Iteration 275/1000 | Loss: 0.00006334
Iteration 276/1000 | Loss: 0.00006334
Iteration 277/1000 | Loss: 0.00006334
Iteration 278/1000 | Loss: 0.00006334
Iteration 279/1000 | Loss: 0.00006334
Iteration 280/1000 | Loss: 0.00006333
Iteration 281/1000 | Loss: 0.00006333
Iteration 282/1000 | Loss: 0.00006333
Iteration 283/1000 | Loss: 0.00006333
Iteration 284/1000 | Loss: 0.00006333
Iteration 285/1000 | Loss: 0.00006332
Iteration 286/1000 | Loss: 0.00006332
Iteration 287/1000 | Loss: 0.00006332
Iteration 288/1000 | Loss: 0.00006332
Iteration 289/1000 | Loss: 0.00006331
Iteration 290/1000 | Loss: 0.00006331
Iteration 291/1000 | Loss: 0.00006331
Iteration 292/1000 | Loss: 0.00006331
Iteration 293/1000 | Loss: 0.00006330
Iteration 294/1000 | Loss: 0.00006330
Iteration 295/1000 | Loss: 0.00006330
Iteration 296/1000 | Loss: 0.00006330
Iteration 297/1000 | Loss: 0.00006329
Iteration 298/1000 | Loss: 0.00006329
Iteration 299/1000 | Loss: 0.00006329
Iteration 300/1000 | Loss: 0.00006327
Iteration 301/1000 | Loss: 0.00006327
Iteration 302/1000 | Loss: 0.00006326
Iteration 303/1000 | Loss: 0.00006325
Iteration 304/1000 | Loss: 0.00006325
Iteration 305/1000 | Loss: 0.00006325
Iteration 306/1000 | Loss: 0.00006325
Iteration 307/1000 | Loss: 0.00006325
Iteration 308/1000 | Loss: 0.00006324
Iteration 309/1000 | Loss: 0.00006324
Iteration 310/1000 | Loss: 0.00006324
Iteration 311/1000 | Loss: 0.00006324
Iteration 312/1000 | Loss: 0.00006324
Iteration 313/1000 | Loss: 0.00006324
Iteration 314/1000 | Loss: 0.00006324
Iteration 315/1000 | Loss: 0.00006323
Iteration 316/1000 | Loss: 0.00006323
Iteration 317/1000 | Loss: 0.00006323
Iteration 318/1000 | Loss: 0.00006323
Iteration 319/1000 | Loss: 0.00006322
Iteration 320/1000 | Loss: 0.00006322
Iteration 321/1000 | Loss: 0.00006322
Iteration 322/1000 | Loss: 0.00006322
Iteration 323/1000 | Loss: 0.00006321
Iteration 324/1000 | Loss: 0.00006321
Iteration 325/1000 | Loss: 0.00006321
Iteration 326/1000 | Loss: 0.00006321
Iteration 327/1000 | Loss: 0.00006321
Iteration 328/1000 | Loss: 0.00006321
Iteration 329/1000 | Loss: 0.00006321
Iteration 330/1000 | Loss: 0.00006321
Iteration 331/1000 | Loss: 0.00006321
Iteration 332/1000 | Loss: 0.00006321
Iteration 333/1000 | Loss: 0.00006321
Iteration 334/1000 | Loss: 0.00006321
Iteration 335/1000 | Loss: 0.00006321
Iteration 336/1000 | Loss: 0.00006321
Iteration 337/1000 | Loss: 0.00006320
Iteration 338/1000 | Loss: 0.00006320
Iteration 339/1000 | Loss: 0.00006320
Iteration 340/1000 | Loss: 0.00006320
Iteration 341/1000 | Loss: 0.00006320
Iteration 342/1000 | Loss: 0.00006320
Iteration 343/1000 | Loss: 0.00006320
Iteration 344/1000 | Loss: 0.00006320
Iteration 345/1000 | Loss: 0.00006320
Iteration 346/1000 | Loss: 0.00006319
Iteration 347/1000 | Loss: 0.00006319
Iteration 348/1000 | Loss: 0.00006319
Iteration 349/1000 | Loss: 0.00006319
Iteration 350/1000 | Loss: 0.00006319
Iteration 351/1000 | Loss: 0.00006318
Iteration 352/1000 | Loss: 0.00006318
Iteration 353/1000 | Loss: 0.00006318
Iteration 354/1000 | Loss: 0.00006318
Iteration 355/1000 | Loss: 0.00006318
Iteration 356/1000 | Loss: 0.00006318
Iteration 357/1000 | Loss: 0.00006318
Iteration 358/1000 | Loss: 0.00006318
Iteration 359/1000 | Loss: 0.00006318
Iteration 360/1000 | Loss: 0.00006318
Iteration 361/1000 | Loss: 0.00006318
Iteration 362/1000 | Loss: 0.00006318
Iteration 363/1000 | Loss: 0.00006318
Iteration 364/1000 | Loss: 0.00006318
Iteration 365/1000 | Loss: 0.00006318
Iteration 366/1000 | Loss: 0.00006318
Iteration 367/1000 | Loss: 0.00006318
Iteration 368/1000 | Loss: 0.00006318
Iteration 369/1000 | Loss: 0.00006318
Iteration 370/1000 | Loss: 0.00006318
Iteration 371/1000 | Loss: 0.00006318
Iteration 372/1000 | Loss: 0.00006318
Iteration 373/1000 | Loss: 0.00006318
Iteration 374/1000 | Loss: 0.00006318
Iteration 375/1000 | Loss: 0.00006318
Iteration 376/1000 | Loss: 0.00006318
Iteration 377/1000 | Loss: 0.00006318
Iteration 378/1000 | Loss: 0.00006318
Iteration 379/1000 | Loss: 0.00006318
Iteration 380/1000 | Loss: 0.00006318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 380. Stopping optimization.
Last 5 losses: [6.317860970739275e-05, 6.317860970739275e-05, 6.317860970739275e-05, 6.317860970739275e-05, 6.317860970739275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.317860970739275e-05

Optimization complete. Final v2v error: 5.650635242462158 mm

Highest mean error: 8.56116771697998 mm for frame 92

Lowest mean error: 3.4991350173950195 mm for frame 57

Saving results

Total time: 232.38531494140625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036590
Iteration 2/25 | Loss: 0.00238330
Iteration 3/25 | Loss: 0.00138491
Iteration 4/25 | Loss: 0.00133540
Iteration 5/25 | Loss: 0.00097486
Iteration 6/25 | Loss: 0.00094027
Iteration 7/25 | Loss: 0.00093379
Iteration 8/25 | Loss: 0.00092730
Iteration 9/25 | Loss: 0.00093378
Iteration 10/25 | Loss: 0.00093283
Iteration 11/25 | Loss: 0.00093010
Iteration 12/25 | Loss: 0.00092608
Iteration 13/25 | Loss: 0.00092250
Iteration 14/25 | Loss: 0.00091439
Iteration 15/25 | Loss: 0.00091275
Iteration 16/25 | Loss: 0.00090098
Iteration 17/25 | Loss: 0.00090410
Iteration 18/25 | Loss: 0.00091058
Iteration 19/25 | Loss: 0.00090013
Iteration 20/25 | Loss: 0.00088993
Iteration 21/25 | Loss: 0.00087979
Iteration 22/25 | Loss: 0.00087680
Iteration 23/25 | Loss: 0.00087213
Iteration 24/25 | Loss: 0.00087115
Iteration 25/25 | Loss: 0.00086519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52280748
Iteration 2/25 | Loss: 0.00060748
Iteration 3/25 | Loss: 0.00060748
Iteration 4/25 | Loss: 0.00060748
Iteration 5/25 | Loss: 0.00060748
Iteration 6/25 | Loss: 0.00060748
Iteration 7/25 | Loss: 0.00060748
Iteration 8/25 | Loss: 0.00060748
Iteration 9/25 | Loss: 0.00060748
Iteration 10/25 | Loss: 0.00060748
Iteration 11/25 | Loss: 0.00060748
Iteration 12/25 | Loss: 0.00060748
Iteration 13/25 | Loss: 0.00060748
Iteration 14/25 | Loss: 0.00060748
Iteration 15/25 | Loss: 0.00060748
Iteration 16/25 | Loss: 0.00060748
Iteration 17/25 | Loss: 0.00060748
Iteration 18/25 | Loss: 0.00060748
Iteration 19/25 | Loss: 0.00060748
Iteration 20/25 | Loss: 0.00060748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006074760458432138, 0.0006074760458432138, 0.0006074760458432138, 0.0006074760458432138, 0.0006074760458432138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006074760458432138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060748
Iteration 2/1000 | Loss: 0.00003860
Iteration 3/1000 | Loss: 0.00006305
Iteration 4/1000 | Loss: 0.00010579
Iteration 5/1000 | Loss: 0.00003382
Iteration 6/1000 | Loss: 0.00003141
Iteration 7/1000 | Loss: 0.00010066
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002014
Iteration 10/1000 | Loss: 0.00001966
Iteration 11/1000 | Loss: 0.00009164
Iteration 12/1000 | Loss: 0.00004277
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001860
Iteration 15/1000 | Loss: 0.00002243
Iteration 16/1000 | Loss: 0.00007495
Iteration 17/1000 | Loss: 0.00001961
Iteration 18/1000 | Loss: 0.00001807
Iteration 19/1000 | Loss: 0.00001870
Iteration 20/1000 | Loss: 0.00001799
Iteration 21/1000 | Loss: 0.00001798
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001790
Iteration 24/1000 | Loss: 0.00004576
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001776
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001775
Iteration 34/1000 | Loss: 0.00001775
Iteration 35/1000 | Loss: 0.00001775
Iteration 36/1000 | Loss: 0.00001774
Iteration 37/1000 | Loss: 0.00001773
Iteration 38/1000 | Loss: 0.00001773
Iteration 39/1000 | Loss: 0.00001772
Iteration 40/1000 | Loss: 0.00001772
Iteration 41/1000 | Loss: 0.00001772
Iteration 42/1000 | Loss: 0.00001772
Iteration 43/1000 | Loss: 0.00001771
Iteration 44/1000 | Loss: 0.00001771
Iteration 45/1000 | Loss: 0.00001771
Iteration 46/1000 | Loss: 0.00001770
Iteration 47/1000 | Loss: 0.00001770
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001769
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00001765
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001763
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001762
Iteration 77/1000 | Loss: 0.00001762
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001762
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00001761
Iteration 82/1000 | Loss: 0.00001761
Iteration 83/1000 | Loss: 0.00001761
Iteration 84/1000 | Loss: 0.00001761
Iteration 85/1000 | Loss: 0.00001761
Iteration 86/1000 | Loss: 0.00001760
Iteration 87/1000 | Loss: 0.00001760
Iteration 88/1000 | Loss: 0.00001760
Iteration 89/1000 | Loss: 0.00001759
Iteration 90/1000 | Loss: 0.00001759
Iteration 91/1000 | Loss: 0.00001759
Iteration 92/1000 | Loss: 0.00001758
Iteration 93/1000 | Loss: 0.00001758
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001758
Iteration 96/1000 | Loss: 0.00001758
Iteration 97/1000 | Loss: 0.00001758
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001757
Iteration 103/1000 | Loss: 0.00001757
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001757
Iteration 107/1000 | Loss: 0.00001757
Iteration 108/1000 | Loss: 0.00001757
Iteration 109/1000 | Loss: 0.00001757
Iteration 110/1000 | Loss: 0.00001757
Iteration 111/1000 | Loss: 0.00001756
Iteration 112/1000 | Loss: 0.00001756
Iteration 113/1000 | Loss: 0.00001756
Iteration 114/1000 | Loss: 0.00001756
Iteration 115/1000 | Loss: 0.00001756
Iteration 116/1000 | Loss: 0.00001756
Iteration 117/1000 | Loss: 0.00001756
Iteration 118/1000 | Loss: 0.00001756
Iteration 119/1000 | Loss: 0.00001756
Iteration 120/1000 | Loss: 0.00001756
Iteration 121/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.756398705765605e-05, 1.756398705765605e-05, 1.756398705765605e-05, 1.756398705765605e-05, 1.756398705765605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.756398705765605e-05

Optimization complete. Final v2v error: 3.53725004196167 mm

Highest mean error: 4.136145114898682 mm for frame 154

Lowest mean error: 3.04142165184021 mm for frame 177

Saving results

Total time: 91.4903724193573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994815
Iteration 2/25 | Loss: 0.00185696
Iteration 3/25 | Loss: 0.00115788
Iteration 4/25 | Loss: 0.00104043
Iteration 5/25 | Loss: 0.00098809
Iteration 6/25 | Loss: 0.00096499
Iteration 7/25 | Loss: 0.00096718
Iteration 8/25 | Loss: 0.00096243
Iteration 9/25 | Loss: 0.00095180
Iteration 10/25 | Loss: 0.00093961
Iteration 11/25 | Loss: 0.00092872
Iteration 12/25 | Loss: 0.00091994
Iteration 13/25 | Loss: 0.00091602
Iteration 14/25 | Loss: 0.00091474
Iteration 15/25 | Loss: 0.00091489
Iteration 16/25 | Loss: 0.00091341
Iteration 17/25 | Loss: 0.00091259
Iteration 18/25 | Loss: 0.00091147
Iteration 19/25 | Loss: 0.00091123
Iteration 20/25 | Loss: 0.00091064
Iteration 21/25 | Loss: 0.00090977
Iteration 22/25 | Loss: 0.00090934
Iteration 23/25 | Loss: 0.00090934
Iteration 24/25 | Loss: 0.00090933
Iteration 25/25 | Loss: 0.00090933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53243530
Iteration 2/25 | Loss: 0.00066269
Iteration 3/25 | Loss: 0.00066269
Iteration 4/25 | Loss: 0.00066268
Iteration 5/25 | Loss: 0.00066268
Iteration 6/25 | Loss: 0.00066268
Iteration 7/25 | Loss: 0.00066268
Iteration 8/25 | Loss: 0.00066268
Iteration 9/25 | Loss: 0.00066268
Iteration 10/25 | Loss: 0.00066268
Iteration 11/25 | Loss: 0.00066268
Iteration 12/25 | Loss: 0.00066268
Iteration 13/25 | Loss: 0.00066268
Iteration 14/25 | Loss: 0.00066268
Iteration 15/25 | Loss: 0.00066268
Iteration 16/25 | Loss: 0.00066268
Iteration 17/25 | Loss: 0.00066268
Iteration 18/25 | Loss: 0.00066268
Iteration 19/25 | Loss: 0.00066268
Iteration 20/25 | Loss: 0.00066268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006626823451370001, 0.0006626823451370001, 0.0006626823451370001, 0.0006626823451370001, 0.0006626823451370001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006626823451370001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066268
Iteration 2/1000 | Loss: 0.00003858
Iteration 3/1000 | Loss: 0.00163233
Iteration 4/1000 | Loss: 0.00091213
Iteration 5/1000 | Loss: 0.00141671
Iteration 6/1000 | Loss: 0.00076766
Iteration 7/1000 | Loss: 0.00016049
Iteration 8/1000 | Loss: 0.00176380
Iteration 9/1000 | Loss: 0.00039061
Iteration 10/1000 | Loss: 0.00047865
Iteration 11/1000 | Loss: 0.00003615
Iteration 12/1000 | Loss: 0.00009866
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00015040
Iteration 15/1000 | Loss: 0.00008009
Iteration 16/1000 | Loss: 0.00002258
Iteration 17/1000 | Loss: 0.00002159
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00001987
Iteration 21/1000 | Loss: 0.00005316
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001915
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00002711
Iteration 27/1000 | Loss: 0.00002711
Iteration 28/1000 | Loss: 0.00005973
Iteration 29/1000 | Loss: 0.00003184
Iteration 30/1000 | Loss: 0.00001892
Iteration 31/1000 | Loss: 0.00001885
Iteration 32/1000 | Loss: 0.00001885
Iteration 33/1000 | Loss: 0.00001885
Iteration 34/1000 | Loss: 0.00001885
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001884
Iteration 37/1000 | Loss: 0.00001884
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001884
Iteration 40/1000 | Loss: 0.00001884
Iteration 41/1000 | Loss: 0.00001884
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001882
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001882
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001878
Iteration 64/1000 | Loss: 0.00001878
Iteration 65/1000 | Loss: 0.00001877
Iteration 66/1000 | Loss: 0.00001877
Iteration 67/1000 | Loss: 0.00001877
Iteration 68/1000 | Loss: 0.00001877
Iteration 69/1000 | Loss: 0.00001877
Iteration 70/1000 | Loss: 0.00001877
Iteration 71/1000 | Loss: 0.00001877
Iteration 72/1000 | Loss: 0.00001877
Iteration 73/1000 | Loss: 0.00001876
Iteration 74/1000 | Loss: 0.00001876
Iteration 75/1000 | Loss: 0.00002930
Iteration 76/1000 | Loss: 0.00002930
Iteration 77/1000 | Loss: 0.00008167
Iteration 78/1000 | Loss: 0.00002434
Iteration 79/1000 | Loss: 0.00001877
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001873
Iteration 84/1000 | Loss: 0.00001873
Iteration 85/1000 | Loss: 0.00001873
Iteration 86/1000 | Loss: 0.00001873
Iteration 87/1000 | Loss: 0.00001873
Iteration 88/1000 | Loss: 0.00001872
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001872
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001871
Iteration 93/1000 | Loss: 0.00001871
Iteration 94/1000 | Loss: 0.00001871
Iteration 95/1000 | Loss: 0.00001871
Iteration 96/1000 | Loss: 0.00001871
Iteration 97/1000 | Loss: 0.00001871
Iteration 98/1000 | Loss: 0.00001871
Iteration 99/1000 | Loss: 0.00001871
Iteration 100/1000 | Loss: 0.00001871
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001870
Iteration 103/1000 | Loss: 0.00001870
Iteration 104/1000 | Loss: 0.00001870
Iteration 105/1000 | Loss: 0.00001870
Iteration 106/1000 | Loss: 0.00001870
Iteration 107/1000 | Loss: 0.00001870
Iteration 108/1000 | Loss: 0.00001870
Iteration 109/1000 | Loss: 0.00001870
Iteration 110/1000 | Loss: 0.00001870
Iteration 111/1000 | Loss: 0.00001870
Iteration 112/1000 | Loss: 0.00001869
Iteration 113/1000 | Loss: 0.00001869
Iteration 114/1000 | Loss: 0.00001869
Iteration 115/1000 | Loss: 0.00001869
Iteration 116/1000 | Loss: 0.00001869
Iteration 117/1000 | Loss: 0.00002606
Iteration 118/1000 | Loss: 0.00002606
Iteration 119/1000 | Loss: 0.00005042
Iteration 120/1000 | Loss: 0.00003551
Iteration 121/1000 | Loss: 0.00002090
Iteration 122/1000 | Loss: 0.00001896
Iteration 123/1000 | Loss: 0.00001886
Iteration 124/1000 | Loss: 0.00003062
Iteration 125/1000 | Loss: 0.00002311
Iteration 126/1000 | Loss: 0.00001878
Iteration 127/1000 | Loss: 0.00002627
Iteration 128/1000 | Loss: 0.00002929
Iteration 129/1000 | Loss: 0.00001905
Iteration 130/1000 | Loss: 0.00001865
Iteration 131/1000 | Loss: 0.00001865
Iteration 132/1000 | Loss: 0.00001864
Iteration 133/1000 | Loss: 0.00001864
Iteration 134/1000 | Loss: 0.00001864
Iteration 135/1000 | Loss: 0.00001864
Iteration 136/1000 | Loss: 0.00001864
Iteration 137/1000 | Loss: 0.00001864
Iteration 138/1000 | Loss: 0.00001864
Iteration 139/1000 | Loss: 0.00001864
Iteration 140/1000 | Loss: 0.00001864
Iteration 141/1000 | Loss: 0.00001864
Iteration 142/1000 | Loss: 0.00001864
Iteration 143/1000 | Loss: 0.00001864
Iteration 144/1000 | Loss: 0.00001864
Iteration 145/1000 | Loss: 0.00001864
Iteration 146/1000 | Loss: 0.00001864
Iteration 147/1000 | Loss: 0.00001864
Iteration 148/1000 | Loss: 0.00001864
Iteration 149/1000 | Loss: 0.00001864
Iteration 150/1000 | Loss: 0.00001864
Iteration 151/1000 | Loss: 0.00001864
Iteration 152/1000 | Loss: 0.00001864
Iteration 153/1000 | Loss: 0.00001864
Iteration 154/1000 | Loss: 0.00001864
Iteration 155/1000 | Loss: 0.00001864
Iteration 156/1000 | Loss: 0.00001864
Iteration 157/1000 | Loss: 0.00001864
Iteration 158/1000 | Loss: 0.00001864
Iteration 159/1000 | Loss: 0.00001864
Iteration 160/1000 | Loss: 0.00001864
Iteration 161/1000 | Loss: 0.00001864
Iteration 162/1000 | Loss: 0.00001864
Iteration 163/1000 | Loss: 0.00001864
Iteration 164/1000 | Loss: 0.00001864
Iteration 165/1000 | Loss: 0.00001864
Iteration 166/1000 | Loss: 0.00001864
Iteration 167/1000 | Loss: 0.00001864
Iteration 168/1000 | Loss: 0.00001864
Iteration 169/1000 | Loss: 0.00001864
Iteration 170/1000 | Loss: 0.00001864
Iteration 171/1000 | Loss: 0.00001864
Iteration 172/1000 | Loss: 0.00001864
Iteration 173/1000 | Loss: 0.00001864
Iteration 174/1000 | Loss: 0.00001864
Iteration 175/1000 | Loss: 0.00001864
Iteration 176/1000 | Loss: 0.00001864
Iteration 177/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.864066143753007e-05, 1.864066143753007e-05, 1.864066143753007e-05, 1.864066143753007e-05, 1.864066143753007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.864066143753007e-05

Optimization complete. Final v2v error: 3.5516867637634277 mm

Highest mean error: 5.513612747192383 mm for frame 72

Lowest mean error: 2.804368495941162 mm for frame 130

Saving results

Total time: 115.89786720275879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479987
Iteration 2/25 | Loss: 0.00094283
Iteration 3/25 | Loss: 0.00085263
Iteration 4/25 | Loss: 0.00083926
Iteration 5/25 | Loss: 0.00083516
Iteration 6/25 | Loss: 0.00083384
Iteration 7/25 | Loss: 0.00083378
Iteration 8/25 | Loss: 0.00083378
Iteration 9/25 | Loss: 0.00083378
Iteration 10/25 | Loss: 0.00083378
Iteration 11/25 | Loss: 0.00083378
Iteration 12/25 | Loss: 0.00083378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000833779398817569, 0.000833779398817569, 0.000833779398817569, 0.000833779398817569, 0.000833779398817569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000833779398817569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54487741
Iteration 2/25 | Loss: 0.00061159
Iteration 3/25 | Loss: 0.00061159
Iteration 4/25 | Loss: 0.00061159
Iteration 5/25 | Loss: 0.00061158
Iteration 6/25 | Loss: 0.00061158
Iteration 7/25 | Loss: 0.00061158
Iteration 8/25 | Loss: 0.00061158
Iteration 9/25 | Loss: 0.00061158
Iteration 10/25 | Loss: 0.00061158
Iteration 11/25 | Loss: 0.00061158
Iteration 12/25 | Loss: 0.00061158
Iteration 13/25 | Loss: 0.00061158
Iteration 14/25 | Loss: 0.00061158
Iteration 15/25 | Loss: 0.00061158
Iteration 16/25 | Loss: 0.00061158
Iteration 17/25 | Loss: 0.00061158
Iteration 18/25 | Loss: 0.00061158
Iteration 19/25 | Loss: 0.00061158
Iteration 20/25 | Loss: 0.00061158
Iteration 21/25 | Loss: 0.00061158
Iteration 22/25 | Loss: 0.00061158
Iteration 23/25 | Loss: 0.00061158
Iteration 24/25 | Loss: 0.00061158
Iteration 25/25 | Loss: 0.00061158
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006115824216976762, 0.0006115824216976762, 0.0006115824216976762, 0.0006115824216976762, 0.0006115824216976762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006115824216976762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061158
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00002059
Iteration 4/1000 | Loss: 0.00001855
Iteration 5/1000 | Loss: 0.00001742
Iteration 6/1000 | Loss: 0.00001689
Iteration 7/1000 | Loss: 0.00001653
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001555
Iteration 19/1000 | Loss: 0.00001551
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001548
Iteration 22/1000 | Loss: 0.00001547
Iteration 23/1000 | Loss: 0.00001546
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001545
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001523
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001521
Iteration 48/1000 | Loss: 0.00001521
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001520
Iteration 52/1000 | Loss: 0.00001520
Iteration 53/1000 | Loss: 0.00001520
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001518
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001517
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001517
Iteration 94/1000 | Loss: 0.00001517
Iteration 95/1000 | Loss: 0.00001517
Iteration 96/1000 | Loss: 0.00001516
Iteration 97/1000 | Loss: 0.00001516
Iteration 98/1000 | Loss: 0.00001516
Iteration 99/1000 | Loss: 0.00001516
Iteration 100/1000 | Loss: 0.00001516
Iteration 101/1000 | Loss: 0.00001516
Iteration 102/1000 | Loss: 0.00001516
Iteration 103/1000 | Loss: 0.00001516
Iteration 104/1000 | Loss: 0.00001516
Iteration 105/1000 | Loss: 0.00001516
Iteration 106/1000 | Loss: 0.00001516
Iteration 107/1000 | Loss: 0.00001516
Iteration 108/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.5160427210503258e-05, 1.5160427210503258e-05, 1.5160427210503258e-05, 1.5160427210503258e-05, 1.5160427210503258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5160427210503258e-05

Optimization complete. Final v2v error: 3.2619476318359375 mm

Highest mean error: 3.497096061706543 mm for frame 197

Lowest mean error: 3.06358003616333 mm for frame 17

Saving results

Total time: 39.30166482925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00666328
Iteration 2/25 | Loss: 0.00116688
Iteration 3/25 | Loss: 0.00096056
Iteration 4/25 | Loss: 0.00092877
Iteration 5/25 | Loss: 0.00091842
Iteration 6/25 | Loss: 0.00091735
Iteration 7/25 | Loss: 0.00091722
Iteration 8/25 | Loss: 0.00091722
Iteration 9/25 | Loss: 0.00091722
Iteration 10/25 | Loss: 0.00091722
Iteration 11/25 | Loss: 0.00091722
Iteration 12/25 | Loss: 0.00091722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009172181598842144, 0.0009172181598842144, 0.0009172181598842144, 0.0009172181598842144, 0.0009172181598842144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009172181598842144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18959844
Iteration 2/25 | Loss: 0.00055188
Iteration 3/25 | Loss: 0.00055185
Iteration 4/25 | Loss: 0.00055185
Iteration 5/25 | Loss: 0.00055185
Iteration 6/25 | Loss: 0.00055184
Iteration 7/25 | Loss: 0.00055184
Iteration 8/25 | Loss: 0.00055184
Iteration 9/25 | Loss: 0.00055184
Iteration 10/25 | Loss: 0.00055184
Iteration 11/25 | Loss: 0.00055184
Iteration 12/25 | Loss: 0.00055184
Iteration 13/25 | Loss: 0.00055184
Iteration 14/25 | Loss: 0.00055184
Iteration 15/25 | Loss: 0.00055184
Iteration 16/25 | Loss: 0.00055184
Iteration 17/25 | Loss: 0.00055184
Iteration 18/25 | Loss: 0.00055184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005518434336408973, 0.0005518434336408973, 0.0005518434336408973, 0.0005518434336408973, 0.0005518434336408973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005518434336408973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055184
Iteration 2/1000 | Loss: 0.00004410
Iteration 3/1000 | Loss: 0.00002912
Iteration 4/1000 | Loss: 0.00002657
Iteration 5/1000 | Loss: 0.00002544
Iteration 6/1000 | Loss: 0.00002459
Iteration 7/1000 | Loss: 0.00002387
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00002280
Iteration 11/1000 | Loss: 0.00002266
Iteration 12/1000 | Loss: 0.00002250
Iteration 13/1000 | Loss: 0.00002246
Iteration 14/1000 | Loss: 0.00002241
Iteration 15/1000 | Loss: 0.00002240
Iteration 16/1000 | Loss: 0.00002240
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002238
Iteration 19/1000 | Loss: 0.00002238
Iteration 20/1000 | Loss: 0.00002238
Iteration 21/1000 | Loss: 0.00002238
Iteration 22/1000 | Loss: 0.00002237
Iteration 23/1000 | Loss: 0.00002236
Iteration 24/1000 | Loss: 0.00002234
Iteration 25/1000 | Loss: 0.00002233
Iteration 26/1000 | Loss: 0.00002233
Iteration 27/1000 | Loss: 0.00002232
Iteration 28/1000 | Loss: 0.00002232
Iteration 29/1000 | Loss: 0.00002231
Iteration 30/1000 | Loss: 0.00002230
Iteration 31/1000 | Loss: 0.00002230
Iteration 32/1000 | Loss: 0.00002230
Iteration 33/1000 | Loss: 0.00002229
Iteration 34/1000 | Loss: 0.00002228
Iteration 35/1000 | Loss: 0.00002228
Iteration 36/1000 | Loss: 0.00002228
Iteration 37/1000 | Loss: 0.00002227
Iteration 38/1000 | Loss: 0.00002227
Iteration 39/1000 | Loss: 0.00002227
Iteration 40/1000 | Loss: 0.00002227
Iteration 41/1000 | Loss: 0.00002227
Iteration 42/1000 | Loss: 0.00002227
Iteration 43/1000 | Loss: 0.00002227
Iteration 44/1000 | Loss: 0.00002227
Iteration 45/1000 | Loss: 0.00002226
Iteration 46/1000 | Loss: 0.00002226
Iteration 47/1000 | Loss: 0.00002226
Iteration 48/1000 | Loss: 0.00002226
Iteration 49/1000 | Loss: 0.00002225
Iteration 50/1000 | Loss: 0.00002225
Iteration 51/1000 | Loss: 0.00002225
Iteration 52/1000 | Loss: 0.00002225
Iteration 53/1000 | Loss: 0.00002225
Iteration 54/1000 | Loss: 0.00002225
Iteration 55/1000 | Loss: 0.00002225
Iteration 56/1000 | Loss: 0.00002225
Iteration 57/1000 | Loss: 0.00002225
Iteration 58/1000 | Loss: 0.00002225
Iteration 59/1000 | Loss: 0.00002225
Iteration 60/1000 | Loss: 0.00002224
Iteration 61/1000 | Loss: 0.00002224
Iteration 62/1000 | Loss: 0.00002224
Iteration 63/1000 | Loss: 0.00002224
Iteration 64/1000 | Loss: 0.00002224
Iteration 65/1000 | Loss: 0.00002224
Iteration 66/1000 | Loss: 0.00002224
Iteration 67/1000 | Loss: 0.00002223
Iteration 68/1000 | Loss: 0.00002223
Iteration 69/1000 | Loss: 0.00002222
Iteration 70/1000 | Loss: 0.00002222
Iteration 71/1000 | Loss: 0.00002222
Iteration 72/1000 | Loss: 0.00002222
Iteration 73/1000 | Loss: 0.00002222
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002222
Iteration 76/1000 | Loss: 0.00002222
Iteration 77/1000 | Loss: 0.00002222
Iteration 78/1000 | Loss: 0.00002222
Iteration 79/1000 | Loss: 0.00002222
Iteration 80/1000 | Loss: 0.00002221
Iteration 81/1000 | Loss: 0.00002221
Iteration 82/1000 | Loss: 0.00002221
Iteration 83/1000 | Loss: 0.00002221
Iteration 84/1000 | Loss: 0.00002220
Iteration 85/1000 | Loss: 0.00002220
Iteration 86/1000 | Loss: 0.00002220
Iteration 87/1000 | Loss: 0.00002219
Iteration 88/1000 | Loss: 0.00002219
Iteration 89/1000 | Loss: 0.00002218
Iteration 90/1000 | Loss: 0.00002218
Iteration 91/1000 | Loss: 0.00002218
Iteration 92/1000 | Loss: 0.00002218
Iteration 93/1000 | Loss: 0.00002217
Iteration 94/1000 | Loss: 0.00002217
Iteration 95/1000 | Loss: 0.00002217
Iteration 96/1000 | Loss: 0.00002217
Iteration 97/1000 | Loss: 0.00002217
Iteration 98/1000 | Loss: 0.00002217
Iteration 99/1000 | Loss: 0.00002217
Iteration 100/1000 | Loss: 0.00002217
Iteration 101/1000 | Loss: 0.00002217
Iteration 102/1000 | Loss: 0.00002217
Iteration 103/1000 | Loss: 0.00002217
Iteration 104/1000 | Loss: 0.00002215
Iteration 105/1000 | Loss: 0.00002215
Iteration 106/1000 | Loss: 0.00002215
Iteration 107/1000 | Loss: 0.00002215
Iteration 108/1000 | Loss: 0.00002215
Iteration 109/1000 | Loss: 0.00002215
Iteration 110/1000 | Loss: 0.00002215
Iteration 111/1000 | Loss: 0.00002214
Iteration 112/1000 | Loss: 0.00002214
Iteration 113/1000 | Loss: 0.00002214
Iteration 114/1000 | Loss: 0.00002214
Iteration 115/1000 | Loss: 0.00002214
Iteration 116/1000 | Loss: 0.00002213
Iteration 117/1000 | Loss: 0.00002213
Iteration 118/1000 | Loss: 0.00002213
Iteration 119/1000 | Loss: 0.00002213
Iteration 120/1000 | Loss: 0.00002213
Iteration 121/1000 | Loss: 0.00002212
Iteration 122/1000 | Loss: 0.00002212
Iteration 123/1000 | Loss: 0.00002212
Iteration 124/1000 | Loss: 0.00002212
Iteration 125/1000 | Loss: 0.00002211
Iteration 126/1000 | Loss: 0.00002211
Iteration 127/1000 | Loss: 0.00002211
Iteration 128/1000 | Loss: 0.00002211
Iteration 129/1000 | Loss: 0.00002211
Iteration 130/1000 | Loss: 0.00002210
Iteration 131/1000 | Loss: 0.00002210
Iteration 132/1000 | Loss: 0.00002210
Iteration 133/1000 | Loss: 0.00002209
Iteration 134/1000 | Loss: 0.00002209
Iteration 135/1000 | Loss: 0.00002209
Iteration 136/1000 | Loss: 0.00002209
Iteration 137/1000 | Loss: 0.00002209
Iteration 138/1000 | Loss: 0.00002209
Iteration 139/1000 | Loss: 0.00002209
Iteration 140/1000 | Loss: 0.00002209
Iteration 141/1000 | Loss: 0.00002208
Iteration 142/1000 | Loss: 0.00002208
Iteration 143/1000 | Loss: 0.00002208
Iteration 144/1000 | Loss: 0.00002208
Iteration 145/1000 | Loss: 0.00002208
Iteration 146/1000 | Loss: 0.00002208
Iteration 147/1000 | Loss: 0.00002207
Iteration 148/1000 | Loss: 0.00002207
Iteration 149/1000 | Loss: 0.00002207
Iteration 150/1000 | Loss: 0.00002207
Iteration 151/1000 | Loss: 0.00002206
Iteration 152/1000 | Loss: 0.00002206
Iteration 153/1000 | Loss: 0.00002206
Iteration 154/1000 | Loss: 0.00002206
Iteration 155/1000 | Loss: 0.00002206
Iteration 156/1000 | Loss: 0.00002206
Iteration 157/1000 | Loss: 0.00002206
Iteration 158/1000 | Loss: 0.00002206
Iteration 159/1000 | Loss: 0.00002206
Iteration 160/1000 | Loss: 0.00002206
Iteration 161/1000 | Loss: 0.00002206
Iteration 162/1000 | Loss: 0.00002206
Iteration 163/1000 | Loss: 0.00002206
Iteration 164/1000 | Loss: 0.00002206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.2055439330870286e-05, 2.2055439330870286e-05, 2.2055439330870286e-05, 2.2055439330870286e-05, 2.2055439330870286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2055439330870286e-05

Optimization complete. Final v2v error: 3.974796772003174 mm

Highest mean error: 4.480038166046143 mm for frame 86

Lowest mean error: 3.6303181648254395 mm for frame 36

Saving results

Total time: 41.65013122558594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098152
Iteration 2/25 | Loss: 0.01098152
Iteration 3/25 | Loss: 0.01098152
Iteration 4/25 | Loss: 0.01098152
Iteration 5/25 | Loss: 0.01098151
Iteration 6/25 | Loss: 0.01098151
Iteration 7/25 | Loss: 0.01098151
Iteration 8/25 | Loss: 0.01098151
Iteration 9/25 | Loss: 0.01098151
Iteration 10/25 | Loss: 0.01098151
Iteration 11/25 | Loss: 0.01098151
Iteration 12/25 | Loss: 0.01098151
Iteration 13/25 | Loss: 0.01098151
Iteration 14/25 | Loss: 0.01098151
Iteration 15/25 | Loss: 0.01098151
Iteration 16/25 | Loss: 0.01098151
Iteration 17/25 | Loss: 0.01098151
Iteration 18/25 | Loss: 0.01098151
Iteration 19/25 | Loss: 0.01098151
Iteration 20/25 | Loss: 0.01098151
Iteration 21/25 | Loss: 0.01098151
Iteration 22/25 | Loss: 0.01098151
Iteration 23/25 | Loss: 0.01098151
Iteration 24/25 | Loss: 0.01098150
Iteration 25/25 | Loss: 0.01098150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98630726
Iteration 2/25 | Loss: 0.05263484
Iteration 3/25 | Loss: 0.05201315
Iteration 4/25 | Loss: 0.05156249
Iteration 5/25 | Loss: 0.05156248
Iteration 6/25 | Loss: 0.05156247
Iteration 7/25 | Loss: 0.05156247
Iteration 8/25 | Loss: 0.05156247
Iteration 9/25 | Loss: 0.05156247
Iteration 10/25 | Loss: 0.05156247
Iteration 11/25 | Loss: 0.05156247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.05156246945261955, 0.05156246945261955, 0.05156246945261955, 0.05156246945261955, 0.05156246945261955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05156246945261955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05156247
Iteration 2/1000 | Loss: 0.00311280
Iteration 3/1000 | Loss: 0.00089779
Iteration 4/1000 | Loss: 0.00055736
Iteration 5/1000 | Loss: 0.00032521
Iteration 6/1000 | Loss: 0.00059077
Iteration 7/1000 | Loss: 0.00009374
Iteration 8/1000 | Loss: 0.00040941
Iteration 9/1000 | Loss: 0.00031908
Iteration 10/1000 | Loss: 0.00042528
Iteration 11/1000 | Loss: 0.00007476
Iteration 12/1000 | Loss: 0.00043358
Iteration 13/1000 | Loss: 0.00004841
Iteration 14/1000 | Loss: 0.00004447
Iteration 15/1000 | Loss: 0.00014159
Iteration 16/1000 | Loss: 0.00014974
Iteration 17/1000 | Loss: 0.00003687
Iteration 18/1000 | Loss: 0.00021796
Iteration 19/1000 | Loss: 0.00016076
Iteration 20/1000 | Loss: 0.00009853
Iteration 21/1000 | Loss: 0.00003208
Iteration 22/1000 | Loss: 0.00010421
Iteration 23/1000 | Loss: 0.00012147
Iteration 24/1000 | Loss: 0.00003329
Iteration 25/1000 | Loss: 0.00002793
Iteration 26/1000 | Loss: 0.00011266
Iteration 27/1000 | Loss: 0.00009741
Iteration 28/1000 | Loss: 0.00007120
Iteration 29/1000 | Loss: 0.00033148
Iteration 30/1000 | Loss: 0.00058199
Iteration 31/1000 | Loss: 0.00009823
Iteration 32/1000 | Loss: 0.00074990
Iteration 33/1000 | Loss: 0.00095424
Iteration 34/1000 | Loss: 0.00021790
Iteration 35/1000 | Loss: 0.00006358
Iteration 36/1000 | Loss: 0.00038102
Iteration 37/1000 | Loss: 0.00002788
Iteration 38/1000 | Loss: 0.00002654
Iteration 39/1000 | Loss: 0.00004382
Iteration 40/1000 | Loss: 0.00003596
Iteration 41/1000 | Loss: 0.00011131
Iteration 42/1000 | Loss: 0.00003177
Iteration 43/1000 | Loss: 0.00005362
Iteration 44/1000 | Loss: 0.00025877
Iteration 45/1000 | Loss: 0.00007680
Iteration 46/1000 | Loss: 0.00009676
Iteration 47/1000 | Loss: 0.00003427
Iteration 48/1000 | Loss: 0.00002849
Iteration 49/1000 | Loss: 0.00002981
Iteration 50/1000 | Loss: 0.00002432
Iteration 51/1000 | Loss: 0.00002431
Iteration 52/1000 | Loss: 0.00002421
Iteration 53/1000 | Loss: 0.00002974
Iteration 54/1000 | Loss: 0.00015123
Iteration 55/1000 | Loss: 0.00004487
Iteration 56/1000 | Loss: 0.00002881
Iteration 57/1000 | Loss: 0.00003049
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002366
Iteration 60/1000 | Loss: 0.00002366
Iteration 61/1000 | Loss: 0.00002366
Iteration 62/1000 | Loss: 0.00002366
Iteration 63/1000 | Loss: 0.00002366
Iteration 64/1000 | Loss: 0.00002365
Iteration 65/1000 | Loss: 0.00002365
Iteration 66/1000 | Loss: 0.00002365
Iteration 67/1000 | Loss: 0.00002365
Iteration 68/1000 | Loss: 0.00002365
Iteration 69/1000 | Loss: 0.00002364
Iteration 70/1000 | Loss: 0.00003118
Iteration 71/1000 | Loss: 0.00002355
Iteration 72/1000 | Loss: 0.00002354
Iteration 73/1000 | Loss: 0.00002353
Iteration 74/1000 | Loss: 0.00002353
Iteration 75/1000 | Loss: 0.00002352
Iteration 76/1000 | Loss: 0.00002352
Iteration 77/1000 | Loss: 0.00002351
Iteration 78/1000 | Loss: 0.00002351
Iteration 79/1000 | Loss: 0.00002351
Iteration 80/1000 | Loss: 0.00002350
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002348
Iteration 84/1000 | Loss: 0.00005963
Iteration 85/1000 | Loss: 0.00003516
Iteration 86/1000 | Loss: 0.00002343
Iteration 87/1000 | Loss: 0.00002343
Iteration 88/1000 | Loss: 0.00002342
Iteration 89/1000 | Loss: 0.00002342
Iteration 90/1000 | Loss: 0.00002342
Iteration 91/1000 | Loss: 0.00002342
Iteration 92/1000 | Loss: 0.00002342
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002341
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002341
Iteration 97/1000 | Loss: 0.00002340
Iteration 98/1000 | Loss: 0.00002340
Iteration 99/1000 | Loss: 0.00002340
Iteration 100/1000 | Loss: 0.00002340
Iteration 101/1000 | Loss: 0.00002339
Iteration 102/1000 | Loss: 0.00002339
Iteration 103/1000 | Loss: 0.00002339
Iteration 104/1000 | Loss: 0.00002339
Iteration 105/1000 | Loss: 0.00002339
Iteration 106/1000 | Loss: 0.00002338
Iteration 107/1000 | Loss: 0.00002338
Iteration 108/1000 | Loss: 0.00002337
Iteration 109/1000 | Loss: 0.00002337
Iteration 110/1000 | Loss: 0.00004389
Iteration 111/1000 | Loss: 0.00002339
Iteration 112/1000 | Loss: 0.00002333
Iteration 113/1000 | Loss: 0.00002332
Iteration 114/1000 | Loss: 0.00002331
Iteration 115/1000 | Loss: 0.00002331
Iteration 116/1000 | Loss: 0.00002331
Iteration 117/1000 | Loss: 0.00002331
Iteration 118/1000 | Loss: 0.00002331
Iteration 119/1000 | Loss: 0.00002331
Iteration 120/1000 | Loss: 0.00002331
Iteration 121/1000 | Loss: 0.00002331
Iteration 122/1000 | Loss: 0.00002331
Iteration 123/1000 | Loss: 0.00002331
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002330
Iteration 126/1000 | Loss: 0.00002330
Iteration 127/1000 | Loss: 0.00002330
Iteration 128/1000 | Loss: 0.00002330
Iteration 129/1000 | Loss: 0.00002330
Iteration 130/1000 | Loss: 0.00002330
Iteration 131/1000 | Loss: 0.00002330
Iteration 132/1000 | Loss: 0.00002330
Iteration 133/1000 | Loss: 0.00002329
Iteration 134/1000 | Loss: 0.00002329
Iteration 135/1000 | Loss: 0.00002328
Iteration 136/1000 | Loss: 0.00002328
Iteration 137/1000 | Loss: 0.00002327
Iteration 138/1000 | Loss: 0.00002327
Iteration 139/1000 | Loss: 0.00002327
Iteration 140/1000 | Loss: 0.00002326
Iteration 141/1000 | Loss: 0.00002326
Iteration 142/1000 | Loss: 0.00002326
Iteration 143/1000 | Loss: 0.00002326
Iteration 144/1000 | Loss: 0.00002326
Iteration 145/1000 | Loss: 0.00002326
Iteration 146/1000 | Loss: 0.00002326
Iteration 147/1000 | Loss: 0.00002326
Iteration 148/1000 | Loss: 0.00002326
Iteration 149/1000 | Loss: 0.00002326
Iteration 150/1000 | Loss: 0.00002326
Iteration 151/1000 | Loss: 0.00002326
Iteration 152/1000 | Loss: 0.00002325
Iteration 153/1000 | Loss: 0.00002325
Iteration 154/1000 | Loss: 0.00002325
Iteration 155/1000 | Loss: 0.00002325
Iteration 156/1000 | Loss: 0.00002325
Iteration 157/1000 | Loss: 0.00002325
Iteration 158/1000 | Loss: 0.00002325
Iteration 159/1000 | Loss: 0.00002325
Iteration 160/1000 | Loss: 0.00002325
Iteration 161/1000 | Loss: 0.00002324
Iteration 162/1000 | Loss: 0.00002324
Iteration 163/1000 | Loss: 0.00002324
Iteration 164/1000 | Loss: 0.00002324
Iteration 165/1000 | Loss: 0.00002324
Iteration 166/1000 | Loss: 0.00002324
Iteration 167/1000 | Loss: 0.00002324
Iteration 168/1000 | Loss: 0.00003134
Iteration 169/1000 | Loss: 0.00003134
Iteration 170/1000 | Loss: 0.00002794
Iteration 171/1000 | Loss: 0.00002383
Iteration 172/1000 | Loss: 0.00002319
Iteration 173/1000 | Loss: 0.00002319
Iteration 174/1000 | Loss: 0.00002406
Iteration 175/1000 | Loss: 0.00002319
Iteration 176/1000 | Loss: 0.00002319
Iteration 177/1000 | Loss: 0.00002319
Iteration 178/1000 | Loss: 0.00002319
Iteration 179/1000 | Loss: 0.00002319
Iteration 180/1000 | Loss: 0.00002318
Iteration 181/1000 | Loss: 0.00002318
Iteration 182/1000 | Loss: 0.00002318
Iteration 183/1000 | Loss: 0.00002318
Iteration 184/1000 | Loss: 0.00002318
Iteration 185/1000 | Loss: 0.00002318
Iteration 186/1000 | Loss: 0.00002318
Iteration 187/1000 | Loss: 0.00002318
Iteration 188/1000 | Loss: 0.00002318
Iteration 189/1000 | Loss: 0.00002318
Iteration 190/1000 | Loss: 0.00002318
Iteration 191/1000 | Loss: 0.00002318
Iteration 192/1000 | Loss: 0.00002318
Iteration 193/1000 | Loss: 0.00002318
Iteration 194/1000 | Loss: 0.00002318
Iteration 195/1000 | Loss: 0.00002318
Iteration 196/1000 | Loss: 0.00002318
Iteration 197/1000 | Loss: 0.00002318
Iteration 198/1000 | Loss: 0.00002318
Iteration 199/1000 | Loss: 0.00002318
Iteration 200/1000 | Loss: 0.00002318
Iteration 201/1000 | Loss: 0.00002318
Iteration 202/1000 | Loss: 0.00002318
Iteration 203/1000 | Loss: 0.00002318
Iteration 204/1000 | Loss: 0.00002318
Iteration 205/1000 | Loss: 0.00002318
Iteration 206/1000 | Loss: 0.00002318
Iteration 207/1000 | Loss: 0.00002318
Iteration 208/1000 | Loss: 0.00002318
Iteration 209/1000 | Loss: 0.00002318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.3179216441349126e-05, 2.3179216441349126e-05, 2.3179216441349126e-05, 2.3179216441349126e-05, 2.3179216441349126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3179216441349126e-05

Optimization complete. Final v2v error: 4.1118316650390625 mm

Highest mean error: 4.599963188171387 mm for frame 52

Lowest mean error: 3.6564409732818604 mm for frame 167

Saving results

Total time: 119.34864091873169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738627
Iteration 2/25 | Loss: 0.00113543
Iteration 3/25 | Loss: 0.00088702
Iteration 4/25 | Loss: 0.00086626
Iteration 5/25 | Loss: 0.00084483
Iteration 6/25 | Loss: 0.00083509
Iteration 7/25 | Loss: 0.00083313
Iteration 8/25 | Loss: 0.00084027
Iteration 9/25 | Loss: 0.00082890
Iteration 10/25 | Loss: 0.00082750
Iteration 11/25 | Loss: 0.00082747
Iteration 12/25 | Loss: 0.00082747
Iteration 13/25 | Loss: 0.00082747
Iteration 14/25 | Loss: 0.00082746
Iteration 15/25 | Loss: 0.00082746
Iteration 16/25 | Loss: 0.00082746
Iteration 17/25 | Loss: 0.00082746
Iteration 18/25 | Loss: 0.00082746
Iteration 19/25 | Loss: 0.00082746
Iteration 20/25 | Loss: 0.00082746
Iteration 21/25 | Loss: 0.00082746
Iteration 22/25 | Loss: 0.00082746
Iteration 23/25 | Loss: 0.00082746
Iteration 24/25 | Loss: 0.00082746
Iteration 25/25 | Loss: 0.00082746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08033466
Iteration 2/25 | Loss: 0.00059939
Iteration 3/25 | Loss: 0.00059939
Iteration 4/25 | Loss: 0.00059939
Iteration 5/25 | Loss: 0.00059939
Iteration 6/25 | Loss: 0.00059939
Iteration 7/25 | Loss: 0.00059939
Iteration 8/25 | Loss: 0.00059939
Iteration 9/25 | Loss: 0.00059939
Iteration 10/25 | Loss: 0.00059939
Iteration 11/25 | Loss: 0.00059939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005993865197524428, 0.0005993865197524428, 0.0005993865197524428, 0.0005993865197524428, 0.0005993865197524428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005993865197524428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059939
Iteration 2/1000 | Loss: 0.00002545
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001788
Iteration 5/1000 | Loss: 0.00008020
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001573
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001500
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001494
Iteration 20/1000 | Loss: 0.00001494
Iteration 21/1000 | Loss: 0.00001494
Iteration 22/1000 | Loss: 0.00001494
Iteration 23/1000 | Loss: 0.00001494
Iteration 24/1000 | Loss: 0.00001494
Iteration 25/1000 | Loss: 0.00001493
Iteration 26/1000 | Loss: 0.00001492
Iteration 27/1000 | Loss: 0.00001492
Iteration 28/1000 | Loss: 0.00001490
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001468
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001455
Iteration 59/1000 | Loss: 0.00001455
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001455
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001452
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001451
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001451
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001450
Iteration 89/1000 | Loss: 0.00001450
Iteration 90/1000 | Loss: 0.00001450
Iteration 91/1000 | Loss: 0.00001449
Iteration 92/1000 | Loss: 0.00001449
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001448
Iteration 99/1000 | Loss: 0.00001448
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001448
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001448
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001448
Iteration 109/1000 | Loss: 0.00001448
Iteration 110/1000 | Loss: 0.00001448
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Iteration 115/1000 | Loss: 0.00001447
Iteration 116/1000 | Loss: 0.00001447
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00001447
Iteration 119/1000 | Loss: 0.00001447
Iteration 120/1000 | Loss: 0.00001447
Iteration 121/1000 | Loss: 0.00001447
Iteration 122/1000 | Loss: 0.00001447
Iteration 123/1000 | Loss: 0.00001447
Iteration 124/1000 | Loss: 0.00001446
Iteration 125/1000 | Loss: 0.00001446
Iteration 126/1000 | Loss: 0.00001446
Iteration 127/1000 | Loss: 0.00001446
Iteration 128/1000 | Loss: 0.00001446
Iteration 129/1000 | Loss: 0.00001446
Iteration 130/1000 | Loss: 0.00001446
Iteration 131/1000 | Loss: 0.00001446
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001446
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00001446
Iteration 138/1000 | Loss: 0.00001446
Iteration 139/1000 | Loss: 0.00001446
Iteration 140/1000 | Loss: 0.00001446
Iteration 141/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.4464662854152266e-05, 1.4464662854152266e-05, 1.4464662854152266e-05, 1.4464662854152266e-05, 1.4464662854152266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4464662854152266e-05

Optimization complete. Final v2v error: 3.24957275390625 mm

Highest mean error: 3.5761630535125732 mm for frame 149

Lowest mean error: 3.115713119506836 mm for frame 122

Saving results

Total time: 53.06680345535278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905600
Iteration 2/25 | Loss: 0.00095943
Iteration 3/25 | Loss: 0.00082217
Iteration 4/25 | Loss: 0.00079778
Iteration 5/25 | Loss: 0.00079390
Iteration 6/25 | Loss: 0.00079264
Iteration 7/25 | Loss: 0.00079264
Iteration 8/25 | Loss: 0.00079264
Iteration 9/25 | Loss: 0.00079264
Iteration 10/25 | Loss: 0.00079264
Iteration 11/25 | Loss: 0.00079264
Iteration 12/25 | Loss: 0.00079264
Iteration 13/25 | Loss: 0.00079264
Iteration 14/25 | Loss: 0.00079264
Iteration 15/25 | Loss: 0.00079264
Iteration 16/25 | Loss: 0.00079264
Iteration 17/25 | Loss: 0.00079264
Iteration 18/25 | Loss: 0.00079264
Iteration 19/25 | Loss: 0.00079264
Iteration 20/25 | Loss: 0.00079264
Iteration 21/25 | Loss: 0.00079264
Iteration 22/25 | Loss: 0.00079264
Iteration 23/25 | Loss: 0.00079264
Iteration 24/25 | Loss: 0.00079264
Iteration 25/25 | Loss: 0.00079264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.07019711
Iteration 2/25 | Loss: 0.00051442
Iteration 3/25 | Loss: 0.00051441
Iteration 4/25 | Loss: 0.00051441
Iteration 5/25 | Loss: 0.00051441
Iteration 6/25 | Loss: 0.00051441
Iteration 7/25 | Loss: 0.00051441
Iteration 8/25 | Loss: 0.00051441
Iteration 9/25 | Loss: 0.00051441
Iteration 10/25 | Loss: 0.00051441
Iteration 11/25 | Loss: 0.00051441
Iteration 12/25 | Loss: 0.00051441
Iteration 13/25 | Loss: 0.00051441
Iteration 14/25 | Loss: 0.00051441
Iteration 15/25 | Loss: 0.00051441
Iteration 16/25 | Loss: 0.00051441
Iteration 17/25 | Loss: 0.00051441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005144050228409469, 0.0005144050228409469, 0.0005144050228409469, 0.0005144050228409469, 0.0005144050228409469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005144050228409469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051441
Iteration 2/1000 | Loss: 0.00002298
Iteration 3/1000 | Loss: 0.00001544
Iteration 4/1000 | Loss: 0.00001387
Iteration 5/1000 | Loss: 0.00001295
Iteration 6/1000 | Loss: 0.00001247
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001184
Iteration 10/1000 | Loss: 0.00001179
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001163
Iteration 13/1000 | Loss: 0.00001161
Iteration 14/1000 | Loss: 0.00001158
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001157
Iteration 17/1000 | Loss: 0.00001155
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001146
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001145
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001137
Iteration 31/1000 | Loss: 0.00001137
Iteration 32/1000 | Loss: 0.00001136
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001133
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001133
Iteration 43/1000 | Loss: 0.00001133
Iteration 44/1000 | Loss: 0.00001133
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001132
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001131
Iteration 51/1000 | Loss: 0.00001130
Iteration 52/1000 | Loss: 0.00001130
Iteration 53/1000 | Loss: 0.00001130
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001129
Iteration 57/1000 | Loss: 0.00001129
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001123
Iteration 72/1000 | Loss: 0.00001122
Iteration 73/1000 | Loss: 0.00001122
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001122
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001121
Iteration 78/1000 | Loss: 0.00001121
Iteration 79/1000 | Loss: 0.00001121
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001121
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001120
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001119
Iteration 90/1000 | Loss: 0.00001119
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001116
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001116
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001115
Iteration 110/1000 | Loss: 0.00001115
Iteration 111/1000 | Loss: 0.00001115
Iteration 112/1000 | Loss: 0.00001115
Iteration 113/1000 | Loss: 0.00001114
Iteration 114/1000 | Loss: 0.00001114
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001114
Iteration 117/1000 | Loss: 0.00001114
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001113
Iteration 121/1000 | Loss: 0.00001113
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001113
Iteration 128/1000 | Loss: 0.00001113
Iteration 129/1000 | Loss: 0.00001113
Iteration 130/1000 | Loss: 0.00001112
Iteration 131/1000 | Loss: 0.00001112
Iteration 132/1000 | Loss: 0.00001112
Iteration 133/1000 | Loss: 0.00001112
Iteration 134/1000 | Loss: 0.00001112
Iteration 135/1000 | Loss: 0.00001112
Iteration 136/1000 | Loss: 0.00001112
Iteration 137/1000 | Loss: 0.00001111
Iteration 138/1000 | Loss: 0.00001111
Iteration 139/1000 | Loss: 0.00001111
Iteration 140/1000 | Loss: 0.00001111
Iteration 141/1000 | Loss: 0.00001111
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Iteration 145/1000 | Loss: 0.00001110
Iteration 146/1000 | Loss: 0.00001110
Iteration 147/1000 | Loss: 0.00001110
Iteration 148/1000 | Loss: 0.00001110
Iteration 149/1000 | Loss: 0.00001110
Iteration 150/1000 | Loss: 0.00001110
Iteration 151/1000 | Loss: 0.00001110
Iteration 152/1000 | Loss: 0.00001110
Iteration 153/1000 | Loss: 0.00001110
Iteration 154/1000 | Loss: 0.00001110
Iteration 155/1000 | Loss: 0.00001110
Iteration 156/1000 | Loss: 0.00001109
Iteration 157/1000 | Loss: 0.00001109
Iteration 158/1000 | Loss: 0.00001109
Iteration 159/1000 | Loss: 0.00001109
Iteration 160/1000 | Loss: 0.00001109
Iteration 161/1000 | Loss: 0.00001109
Iteration 162/1000 | Loss: 0.00001109
Iteration 163/1000 | Loss: 0.00001109
Iteration 164/1000 | Loss: 0.00001109
Iteration 165/1000 | Loss: 0.00001109
Iteration 166/1000 | Loss: 0.00001109
Iteration 167/1000 | Loss: 0.00001109
Iteration 168/1000 | Loss: 0.00001109
Iteration 169/1000 | Loss: 0.00001109
Iteration 170/1000 | Loss: 0.00001109
Iteration 171/1000 | Loss: 0.00001109
Iteration 172/1000 | Loss: 0.00001109
Iteration 173/1000 | Loss: 0.00001109
Iteration 174/1000 | Loss: 0.00001109
Iteration 175/1000 | Loss: 0.00001109
Iteration 176/1000 | Loss: 0.00001109
Iteration 177/1000 | Loss: 0.00001109
Iteration 178/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.1086964150308631e-05, 1.1086964150308631e-05, 1.1086964150308631e-05, 1.1086964150308631e-05, 1.1086964150308631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1086964150308631e-05

Optimization complete. Final v2v error: 2.8653390407562256 mm

Highest mean error: 3.099459648132324 mm for frame 117

Lowest mean error: 2.717478036880493 mm for frame 57

Saving results

Total time: 36.752145528793335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541419
Iteration 2/25 | Loss: 0.00116192
Iteration 3/25 | Loss: 0.00089057
Iteration 4/25 | Loss: 0.00086945
Iteration 5/25 | Loss: 0.00086443
Iteration 6/25 | Loss: 0.00086219
Iteration 7/25 | Loss: 0.00086135
Iteration 8/25 | Loss: 0.00086127
Iteration 9/25 | Loss: 0.00086127
Iteration 10/25 | Loss: 0.00086127
Iteration 11/25 | Loss: 0.00086127
Iteration 12/25 | Loss: 0.00086127
Iteration 13/25 | Loss: 0.00086127
Iteration 14/25 | Loss: 0.00086127
Iteration 15/25 | Loss: 0.00086127
Iteration 16/25 | Loss: 0.00086127
Iteration 17/25 | Loss: 0.00086127
Iteration 18/25 | Loss: 0.00086127
Iteration 19/25 | Loss: 0.00086127
Iteration 20/25 | Loss: 0.00086127
Iteration 21/25 | Loss: 0.00086127
Iteration 22/25 | Loss: 0.00086127
Iteration 23/25 | Loss: 0.00086127
Iteration 24/25 | Loss: 0.00086127
Iteration 25/25 | Loss: 0.00086127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84417564
Iteration 2/25 | Loss: 0.00050631
Iteration 3/25 | Loss: 0.00050630
Iteration 4/25 | Loss: 0.00050630
Iteration 5/25 | Loss: 0.00050630
Iteration 6/25 | Loss: 0.00050630
Iteration 7/25 | Loss: 0.00050630
Iteration 8/25 | Loss: 0.00050630
Iteration 9/25 | Loss: 0.00050630
Iteration 10/25 | Loss: 0.00050630
Iteration 11/25 | Loss: 0.00050630
Iteration 12/25 | Loss: 0.00050630
Iteration 13/25 | Loss: 0.00050630
Iteration 14/25 | Loss: 0.00050630
Iteration 15/25 | Loss: 0.00050630
Iteration 16/25 | Loss: 0.00050630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005063007120043039, 0.0005063007120043039, 0.0005063007120043039, 0.0005063007120043039, 0.0005063007120043039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005063007120043039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050630
Iteration 2/1000 | Loss: 0.00003373
Iteration 3/1000 | Loss: 0.00001799
Iteration 4/1000 | Loss: 0.00001549
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001334
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001267
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001261
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001260
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001233
Iteration 42/1000 | Loss: 0.00001233
Iteration 43/1000 | Loss: 0.00001232
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001228
Iteration 55/1000 | Loss: 0.00001228
Iteration 56/1000 | Loss: 0.00001228
Iteration 57/1000 | Loss: 0.00001228
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001226
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001223
Iteration 91/1000 | Loss: 0.00001222
Iteration 92/1000 | Loss: 0.00001222
Iteration 93/1000 | Loss: 0.00001222
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001220
Iteration 102/1000 | Loss: 0.00001220
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001219
Iteration 107/1000 | Loss: 0.00001218
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001218
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001217
Iteration 118/1000 | Loss: 0.00001217
Iteration 119/1000 | Loss: 0.00001217
Iteration 120/1000 | Loss: 0.00001217
Iteration 121/1000 | Loss: 0.00001217
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001217
Iteration 124/1000 | Loss: 0.00001217
Iteration 125/1000 | Loss: 0.00001217
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001216
Iteration 132/1000 | Loss: 0.00001216
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001215
Iteration 135/1000 | Loss: 0.00001215
Iteration 136/1000 | Loss: 0.00001215
Iteration 137/1000 | Loss: 0.00001215
Iteration 138/1000 | Loss: 0.00001215
Iteration 139/1000 | Loss: 0.00001215
Iteration 140/1000 | Loss: 0.00001215
Iteration 141/1000 | Loss: 0.00001215
Iteration 142/1000 | Loss: 0.00001215
Iteration 143/1000 | Loss: 0.00001215
Iteration 144/1000 | Loss: 0.00001214
Iteration 145/1000 | Loss: 0.00001214
Iteration 146/1000 | Loss: 0.00001214
Iteration 147/1000 | Loss: 0.00001214
Iteration 148/1000 | Loss: 0.00001214
Iteration 149/1000 | Loss: 0.00001214
Iteration 150/1000 | Loss: 0.00001214
Iteration 151/1000 | Loss: 0.00001213
Iteration 152/1000 | Loss: 0.00001213
Iteration 153/1000 | Loss: 0.00001213
Iteration 154/1000 | Loss: 0.00001213
Iteration 155/1000 | Loss: 0.00001213
Iteration 156/1000 | Loss: 0.00001213
Iteration 157/1000 | Loss: 0.00001213
Iteration 158/1000 | Loss: 0.00001213
Iteration 159/1000 | Loss: 0.00001213
Iteration 160/1000 | Loss: 0.00001213
Iteration 161/1000 | Loss: 0.00001213
Iteration 162/1000 | Loss: 0.00001212
Iteration 163/1000 | Loss: 0.00001212
Iteration 164/1000 | Loss: 0.00001212
Iteration 165/1000 | Loss: 0.00001212
Iteration 166/1000 | Loss: 0.00001211
Iteration 167/1000 | Loss: 0.00001211
Iteration 168/1000 | Loss: 0.00001211
Iteration 169/1000 | Loss: 0.00001211
Iteration 170/1000 | Loss: 0.00001211
Iteration 171/1000 | Loss: 0.00001211
Iteration 172/1000 | Loss: 0.00001211
Iteration 173/1000 | Loss: 0.00001211
Iteration 174/1000 | Loss: 0.00001211
Iteration 175/1000 | Loss: 0.00001211
Iteration 176/1000 | Loss: 0.00001211
Iteration 177/1000 | Loss: 0.00001211
Iteration 178/1000 | Loss: 0.00001211
Iteration 179/1000 | Loss: 0.00001211
Iteration 180/1000 | Loss: 0.00001210
Iteration 181/1000 | Loss: 0.00001210
Iteration 182/1000 | Loss: 0.00001210
Iteration 183/1000 | Loss: 0.00001210
Iteration 184/1000 | Loss: 0.00001210
Iteration 185/1000 | Loss: 0.00001209
Iteration 186/1000 | Loss: 0.00001209
Iteration 187/1000 | Loss: 0.00001209
Iteration 188/1000 | Loss: 0.00001209
Iteration 189/1000 | Loss: 0.00001209
Iteration 190/1000 | Loss: 0.00001209
Iteration 191/1000 | Loss: 0.00001209
Iteration 192/1000 | Loss: 0.00001209
Iteration 193/1000 | Loss: 0.00001208
Iteration 194/1000 | Loss: 0.00001208
Iteration 195/1000 | Loss: 0.00001208
Iteration 196/1000 | Loss: 0.00001208
Iteration 197/1000 | Loss: 0.00001208
Iteration 198/1000 | Loss: 0.00001208
Iteration 199/1000 | Loss: 0.00001208
Iteration 200/1000 | Loss: 0.00001208
Iteration 201/1000 | Loss: 0.00001207
Iteration 202/1000 | Loss: 0.00001207
Iteration 203/1000 | Loss: 0.00001207
Iteration 204/1000 | Loss: 0.00001207
Iteration 205/1000 | Loss: 0.00001207
Iteration 206/1000 | Loss: 0.00001207
Iteration 207/1000 | Loss: 0.00001207
Iteration 208/1000 | Loss: 0.00001207
Iteration 209/1000 | Loss: 0.00001207
Iteration 210/1000 | Loss: 0.00001207
Iteration 211/1000 | Loss: 0.00001207
Iteration 212/1000 | Loss: 0.00001207
Iteration 213/1000 | Loss: 0.00001207
Iteration 214/1000 | Loss: 0.00001207
Iteration 215/1000 | Loss: 0.00001207
Iteration 216/1000 | Loss: 0.00001206
Iteration 217/1000 | Loss: 0.00001206
Iteration 218/1000 | Loss: 0.00001206
Iteration 219/1000 | Loss: 0.00001206
Iteration 220/1000 | Loss: 0.00001206
Iteration 221/1000 | Loss: 0.00001206
Iteration 222/1000 | Loss: 0.00001206
Iteration 223/1000 | Loss: 0.00001206
Iteration 224/1000 | Loss: 0.00001206
Iteration 225/1000 | Loss: 0.00001206
Iteration 226/1000 | Loss: 0.00001206
Iteration 227/1000 | Loss: 0.00001206
Iteration 228/1000 | Loss: 0.00001206
Iteration 229/1000 | Loss: 0.00001206
Iteration 230/1000 | Loss: 0.00001206
Iteration 231/1000 | Loss: 0.00001205
Iteration 232/1000 | Loss: 0.00001205
Iteration 233/1000 | Loss: 0.00001205
Iteration 234/1000 | Loss: 0.00001205
Iteration 235/1000 | Loss: 0.00001205
Iteration 236/1000 | Loss: 0.00001205
Iteration 237/1000 | Loss: 0.00001205
Iteration 238/1000 | Loss: 0.00001205
Iteration 239/1000 | Loss: 0.00001205
Iteration 240/1000 | Loss: 0.00001205
Iteration 241/1000 | Loss: 0.00001205
Iteration 242/1000 | Loss: 0.00001205
Iteration 243/1000 | Loss: 0.00001205
Iteration 244/1000 | Loss: 0.00001204
Iteration 245/1000 | Loss: 0.00001204
Iteration 246/1000 | Loss: 0.00001204
Iteration 247/1000 | Loss: 0.00001204
Iteration 248/1000 | Loss: 0.00001204
Iteration 249/1000 | Loss: 0.00001204
Iteration 250/1000 | Loss: 0.00001204
Iteration 251/1000 | Loss: 0.00001204
Iteration 252/1000 | Loss: 0.00001204
Iteration 253/1000 | Loss: 0.00001204
Iteration 254/1000 | Loss: 0.00001204
Iteration 255/1000 | Loss: 0.00001204
Iteration 256/1000 | Loss: 0.00001204
Iteration 257/1000 | Loss: 0.00001204
Iteration 258/1000 | Loss: 0.00001203
Iteration 259/1000 | Loss: 0.00001203
Iteration 260/1000 | Loss: 0.00001203
Iteration 261/1000 | Loss: 0.00001203
Iteration 262/1000 | Loss: 0.00001203
Iteration 263/1000 | Loss: 0.00001203
Iteration 264/1000 | Loss: 0.00001203
Iteration 265/1000 | Loss: 0.00001203
Iteration 266/1000 | Loss: 0.00001203
Iteration 267/1000 | Loss: 0.00001203
Iteration 268/1000 | Loss: 0.00001203
Iteration 269/1000 | Loss: 0.00001203
Iteration 270/1000 | Loss: 0.00001203
Iteration 271/1000 | Loss: 0.00001203
Iteration 272/1000 | Loss: 0.00001203
Iteration 273/1000 | Loss: 0.00001203
Iteration 274/1000 | Loss: 0.00001203
Iteration 275/1000 | Loss: 0.00001203
Iteration 276/1000 | Loss: 0.00001203
Iteration 277/1000 | Loss: 0.00001203
Iteration 278/1000 | Loss: 0.00001203
Iteration 279/1000 | Loss: 0.00001203
Iteration 280/1000 | Loss: 0.00001203
Iteration 281/1000 | Loss: 0.00001203
Iteration 282/1000 | Loss: 0.00001203
Iteration 283/1000 | Loss: 0.00001203
Iteration 284/1000 | Loss: 0.00001203
Iteration 285/1000 | Loss: 0.00001203
Iteration 286/1000 | Loss: 0.00001203
Iteration 287/1000 | Loss: 0.00001203
Iteration 288/1000 | Loss: 0.00001203
Iteration 289/1000 | Loss: 0.00001203
Iteration 290/1000 | Loss: 0.00001203
Iteration 291/1000 | Loss: 0.00001203
Iteration 292/1000 | Loss: 0.00001203
Iteration 293/1000 | Loss: 0.00001203
Iteration 294/1000 | Loss: 0.00001203
Iteration 295/1000 | Loss: 0.00001203
Iteration 296/1000 | Loss: 0.00001203
Iteration 297/1000 | Loss: 0.00001203
Iteration 298/1000 | Loss: 0.00001203
Iteration 299/1000 | Loss: 0.00001203
Iteration 300/1000 | Loss: 0.00001203
Iteration 301/1000 | Loss: 0.00001203
Iteration 302/1000 | Loss: 0.00001203
Iteration 303/1000 | Loss: 0.00001203
Iteration 304/1000 | Loss: 0.00001203
Iteration 305/1000 | Loss: 0.00001203
Iteration 306/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [1.2032392078253906e-05, 1.2032392078253906e-05, 1.2032392078253906e-05, 1.2032392078253906e-05, 1.2032392078253906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2032392078253906e-05

Optimization complete. Final v2v error: 2.891610622406006 mm

Highest mean error: 3.4871039390563965 mm for frame 15

Lowest mean error: 2.6297404766082764 mm for frame 59

Saving results

Total time: 49.48485708236694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869507
Iteration 2/25 | Loss: 0.00140769
Iteration 3/25 | Loss: 0.00095003
Iteration 4/25 | Loss: 0.00086763
Iteration 5/25 | Loss: 0.00084653
Iteration 6/25 | Loss: 0.00084192
Iteration 7/25 | Loss: 0.00084082
Iteration 8/25 | Loss: 0.00084082
Iteration 9/25 | Loss: 0.00084082
Iteration 10/25 | Loss: 0.00084082
Iteration 11/25 | Loss: 0.00084082
Iteration 12/25 | Loss: 0.00084082
Iteration 13/25 | Loss: 0.00084082
Iteration 14/25 | Loss: 0.00084082
Iteration 15/25 | Loss: 0.00084082
Iteration 16/25 | Loss: 0.00084082
Iteration 17/25 | Loss: 0.00084082
Iteration 18/25 | Loss: 0.00084082
Iteration 19/25 | Loss: 0.00084082
Iteration 20/25 | Loss: 0.00084082
Iteration 21/25 | Loss: 0.00084082
Iteration 22/25 | Loss: 0.00084082
Iteration 23/25 | Loss: 0.00084082
Iteration 24/25 | Loss: 0.00084082
Iteration 25/25 | Loss: 0.00084082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48431361
Iteration 2/25 | Loss: 0.00065556
Iteration 3/25 | Loss: 0.00065555
Iteration 4/25 | Loss: 0.00065554
Iteration 5/25 | Loss: 0.00065554
Iteration 6/25 | Loss: 0.00065554
Iteration 7/25 | Loss: 0.00065554
Iteration 8/25 | Loss: 0.00065554
Iteration 9/25 | Loss: 0.00065554
Iteration 10/25 | Loss: 0.00065554
Iteration 11/25 | Loss: 0.00065554
Iteration 12/25 | Loss: 0.00065554
Iteration 13/25 | Loss: 0.00065554
Iteration 14/25 | Loss: 0.00065554
Iteration 15/25 | Loss: 0.00065554
Iteration 16/25 | Loss: 0.00065554
Iteration 17/25 | Loss: 0.00065554
Iteration 18/25 | Loss: 0.00065554
Iteration 19/25 | Loss: 0.00065554
Iteration 20/25 | Loss: 0.00065554
Iteration 21/25 | Loss: 0.00065554
Iteration 22/25 | Loss: 0.00065554
Iteration 23/25 | Loss: 0.00065554
Iteration 24/25 | Loss: 0.00065554
Iteration 25/25 | Loss: 0.00065554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065554
Iteration 2/1000 | Loss: 0.00002872
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001839
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001628
Iteration 7/1000 | Loss: 0.00001590
Iteration 8/1000 | Loss: 0.00001549
Iteration 9/1000 | Loss: 0.00001534
Iteration 10/1000 | Loss: 0.00001514
Iteration 11/1000 | Loss: 0.00001495
Iteration 12/1000 | Loss: 0.00001482
Iteration 13/1000 | Loss: 0.00001482
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001472
Iteration 18/1000 | Loss: 0.00001469
Iteration 19/1000 | Loss: 0.00001468
Iteration 20/1000 | Loss: 0.00001468
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001459
Iteration 31/1000 | Loss: 0.00001459
Iteration 32/1000 | Loss: 0.00001458
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001457
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001456
Iteration 43/1000 | Loss: 0.00001456
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001455
Iteration 47/1000 | Loss: 0.00001455
Iteration 48/1000 | Loss: 0.00001455
Iteration 49/1000 | Loss: 0.00001455
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001454
Iteration 52/1000 | Loss: 0.00001454
Iteration 53/1000 | Loss: 0.00001454
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001454
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001453
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001452
Iteration 60/1000 | Loss: 0.00001452
Iteration 61/1000 | Loss: 0.00001452
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001450
Iteration 70/1000 | Loss: 0.00001450
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001449
Iteration 74/1000 | Loss: 0.00001449
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001448
Iteration 86/1000 | Loss: 0.00001448
Iteration 87/1000 | Loss: 0.00001448
Iteration 88/1000 | Loss: 0.00001448
Iteration 89/1000 | Loss: 0.00001448
Iteration 90/1000 | Loss: 0.00001448
Iteration 91/1000 | Loss: 0.00001448
Iteration 92/1000 | Loss: 0.00001448
Iteration 93/1000 | Loss: 0.00001448
Iteration 94/1000 | Loss: 0.00001448
Iteration 95/1000 | Loss: 0.00001448
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.4477272088697646e-05, 1.4477272088697646e-05, 1.4477272088697646e-05, 1.4477272088697646e-05, 1.4477272088697646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4477272088697646e-05

Optimization complete. Final v2v error: 3.228419065475464 mm

Highest mean error: 3.9008607864379883 mm for frame 53

Lowest mean error: 2.7629783153533936 mm for frame 239

Saving results

Total time: 39.3601610660553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838437
Iteration 2/25 | Loss: 0.00160960
Iteration 3/25 | Loss: 0.00096940
Iteration 4/25 | Loss: 0.00089839
Iteration 5/25 | Loss: 0.00088371
Iteration 6/25 | Loss: 0.00087937
Iteration 7/25 | Loss: 0.00087845
Iteration 8/25 | Loss: 0.00087740
Iteration 9/25 | Loss: 0.00087681
Iteration 10/25 | Loss: 0.00087642
Iteration 11/25 | Loss: 0.00087624
Iteration 12/25 | Loss: 0.00087618
Iteration 13/25 | Loss: 0.00087618
Iteration 14/25 | Loss: 0.00087618
Iteration 15/25 | Loss: 0.00087618
Iteration 16/25 | Loss: 0.00087618
Iteration 17/25 | Loss: 0.00087618
Iteration 18/25 | Loss: 0.00087618
Iteration 19/25 | Loss: 0.00087618
Iteration 20/25 | Loss: 0.00087618
Iteration 21/25 | Loss: 0.00087618
Iteration 22/25 | Loss: 0.00087618
Iteration 23/25 | Loss: 0.00087618
Iteration 24/25 | Loss: 0.00087618
Iteration 25/25 | Loss: 0.00087618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52160072
Iteration 2/25 | Loss: 0.00064920
Iteration 3/25 | Loss: 0.00064919
Iteration 4/25 | Loss: 0.00064919
Iteration 5/25 | Loss: 0.00064919
Iteration 6/25 | Loss: 0.00064919
Iteration 7/25 | Loss: 0.00064919
Iteration 8/25 | Loss: 0.00064919
Iteration 9/25 | Loss: 0.00064919
Iteration 10/25 | Loss: 0.00064919
Iteration 11/25 | Loss: 0.00064919
Iteration 12/25 | Loss: 0.00064919
Iteration 13/25 | Loss: 0.00064919
Iteration 14/25 | Loss: 0.00064919
Iteration 15/25 | Loss: 0.00064919
Iteration 16/25 | Loss: 0.00064919
Iteration 17/25 | Loss: 0.00064919
Iteration 18/25 | Loss: 0.00064919
Iteration 19/25 | Loss: 0.00064919
Iteration 20/25 | Loss: 0.00064919
Iteration 21/25 | Loss: 0.00064919
Iteration 22/25 | Loss: 0.00064919
Iteration 23/25 | Loss: 0.00064919
Iteration 24/25 | Loss: 0.00064919
Iteration 25/25 | Loss: 0.00064919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064919
Iteration 2/1000 | Loss: 0.00003206
Iteration 3/1000 | Loss: 0.00002482
Iteration 4/1000 | Loss: 0.00002294
Iteration 5/1000 | Loss: 0.00002170
Iteration 6/1000 | Loss: 0.00002081
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001926
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001892
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001884
Iteration 23/1000 | Loss: 0.00001884
Iteration 24/1000 | Loss: 0.00001883
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001879
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001878
Iteration 34/1000 | Loss: 0.00001878
Iteration 35/1000 | Loss: 0.00001878
Iteration 36/1000 | Loss: 0.00001878
Iteration 37/1000 | Loss: 0.00001877
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001877
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001874
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001873
Iteration 51/1000 | Loss: 0.00001873
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001872
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001871
Iteration 61/1000 | Loss: 0.00001871
Iteration 62/1000 | Loss: 0.00001871
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001869
Iteration 69/1000 | Loss: 0.00001869
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001868
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001868
Iteration 88/1000 | Loss: 0.00001868
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001867
Iteration 95/1000 | Loss: 0.00001867
Iteration 96/1000 | Loss: 0.00001867
Iteration 97/1000 | Loss: 0.00001867
Iteration 98/1000 | Loss: 0.00001867
Iteration 99/1000 | Loss: 0.00001867
Iteration 100/1000 | Loss: 0.00001867
Iteration 101/1000 | Loss: 0.00001866
Iteration 102/1000 | Loss: 0.00001866
Iteration 103/1000 | Loss: 0.00001866
Iteration 104/1000 | Loss: 0.00001866
Iteration 105/1000 | Loss: 0.00001866
Iteration 106/1000 | Loss: 0.00001866
Iteration 107/1000 | Loss: 0.00001866
Iteration 108/1000 | Loss: 0.00001866
Iteration 109/1000 | Loss: 0.00001866
Iteration 110/1000 | Loss: 0.00001866
Iteration 111/1000 | Loss: 0.00001866
Iteration 112/1000 | Loss: 0.00001866
Iteration 113/1000 | Loss: 0.00001866
Iteration 114/1000 | Loss: 0.00001866
Iteration 115/1000 | Loss: 0.00001866
Iteration 116/1000 | Loss: 0.00001866
Iteration 117/1000 | Loss: 0.00001866
Iteration 118/1000 | Loss: 0.00001866
Iteration 119/1000 | Loss: 0.00001866
Iteration 120/1000 | Loss: 0.00001866
Iteration 121/1000 | Loss: 0.00001866
Iteration 122/1000 | Loss: 0.00001866
Iteration 123/1000 | Loss: 0.00001866
Iteration 124/1000 | Loss: 0.00001866
Iteration 125/1000 | Loss: 0.00001866
Iteration 126/1000 | Loss: 0.00001866
Iteration 127/1000 | Loss: 0.00001866
Iteration 128/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.8661459762370214e-05, 1.8661459762370214e-05, 1.8661459762370214e-05, 1.8661459762370214e-05, 1.8661459762370214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8661459762370214e-05

Optimization complete. Final v2v error: 3.6304571628570557 mm

Highest mean error: 4.3907623291015625 mm for frame 171

Lowest mean error: 2.891115665435791 mm for frame 62

Saving results

Total time: 49.654781341552734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134270
Iteration 2/25 | Loss: 0.00161287
Iteration 3/25 | Loss: 0.00107163
Iteration 4/25 | Loss: 0.00101529
Iteration 5/25 | Loss: 0.00100174
Iteration 6/25 | Loss: 0.00099852
Iteration 7/25 | Loss: 0.00099797
Iteration 8/25 | Loss: 0.00099797
Iteration 9/25 | Loss: 0.00099797
Iteration 10/25 | Loss: 0.00099797
Iteration 11/25 | Loss: 0.00099797
Iteration 12/25 | Loss: 0.00099797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009979686001315713, 0.0009979686001315713, 0.0009979686001315713, 0.0009979686001315713, 0.0009979686001315713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009979686001315713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25085557
Iteration 2/25 | Loss: 0.00050392
Iteration 3/25 | Loss: 0.00050389
Iteration 4/25 | Loss: 0.00050389
Iteration 5/25 | Loss: 0.00050389
Iteration 6/25 | Loss: 0.00050389
Iteration 7/25 | Loss: 0.00050389
Iteration 8/25 | Loss: 0.00050389
Iteration 9/25 | Loss: 0.00050389
Iteration 10/25 | Loss: 0.00050389
Iteration 11/25 | Loss: 0.00050389
Iteration 12/25 | Loss: 0.00050389
Iteration 13/25 | Loss: 0.00050389
Iteration 14/25 | Loss: 0.00050389
Iteration 15/25 | Loss: 0.00050389
Iteration 16/25 | Loss: 0.00050389
Iteration 17/25 | Loss: 0.00050389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005038913222961128, 0.0005038913222961128, 0.0005038913222961128, 0.0005038913222961128, 0.0005038913222961128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005038913222961128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050389
Iteration 2/1000 | Loss: 0.00006197
Iteration 3/1000 | Loss: 0.00004134
Iteration 4/1000 | Loss: 0.00003646
Iteration 5/1000 | Loss: 0.00003461
Iteration 6/1000 | Loss: 0.00003367
Iteration 7/1000 | Loss: 0.00003310
Iteration 8/1000 | Loss: 0.00003262
Iteration 9/1000 | Loss: 0.00003231
Iteration 10/1000 | Loss: 0.00003205
Iteration 11/1000 | Loss: 0.00003186
Iteration 12/1000 | Loss: 0.00003184
Iteration 13/1000 | Loss: 0.00003169
Iteration 14/1000 | Loss: 0.00003152
Iteration 15/1000 | Loss: 0.00003136
Iteration 16/1000 | Loss: 0.00003134
Iteration 17/1000 | Loss: 0.00003128
Iteration 18/1000 | Loss: 0.00003127
Iteration 19/1000 | Loss: 0.00003125
Iteration 20/1000 | Loss: 0.00003123
Iteration 21/1000 | Loss: 0.00003122
Iteration 22/1000 | Loss: 0.00003118
Iteration 23/1000 | Loss: 0.00003112
Iteration 24/1000 | Loss: 0.00003109
Iteration 25/1000 | Loss: 0.00003108
Iteration 26/1000 | Loss: 0.00003103
Iteration 27/1000 | Loss: 0.00003103
Iteration 28/1000 | Loss: 0.00003102
Iteration 29/1000 | Loss: 0.00003101
Iteration 30/1000 | Loss: 0.00003100
Iteration 31/1000 | Loss: 0.00003100
Iteration 32/1000 | Loss: 0.00003100
Iteration 33/1000 | Loss: 0.00003099
Iteration 34/1000 | Loss: 0.00003098
Iteration 35/1000 | Loss: 0.00003097
Iteration 36/1000 | Loss: 0.00003097
Iteration 37/1000 | Loss: 0.00003096
Iteration 38/1000 | Loss: 0.00003096
Iteration 39/1000 | Loss: 0.00003096
Iteration 40/1000 | Loss: 0.00003095
Iteration 41/1000 | Loss: 0.00003095
Iteration 42/1000 | Loss: 0.00003095
Iteration 43/1000 | Loss: 0.00003094
Iteration 44/1000 | Loss: 0.00003094
Iteration 45/1000 | Loss: 0.00003093
Iteration 46/1000 | Loss: 0.00003093
Iteration 47/1000 | Loss: 0.00003093
Iteration 48/1000 | Loss: 0.00003092
Iteration 49/1000 | Loss: 0.00003092
Iteration 50/1000 | Loss: 0.00003091
Iteration 51/1000 | Loss: 0.00003091
Iteration 52/1000 | Loss: 0.00003090
Iteration 53/1000 | Loss: 0.00003090
Iteration 54/1000 | Loss: 0.00003089
Iteration 55/1000 | Loss: 0.00003089
Iteration 56/1000 | Loss: 0.00003089
Iteration 57/1000 | Loss: 0.00003088
Iteration 58/1000 | Loss: 0.00003088
Iteration 59/1000 | Loss: 0.00003088
Iteration 60/1000 | Loss: 0.00003087
Iteration 61/1000 | Loss: 0.00003087
Iteration 62/1000 | Loss: 0.00003086
Iteration 63/1000 | Loss: 0.00003086
Iteration 64/1000 | Loss: 0.00003086
Iteration 65/1000 | Loss: 0.00003086
Iteration 66/1000 | Loss: 0.00003085
Iteration 67/1000 | Loss: 0.00003085
Iteration 68/1000 | Loss: 0.00003084
Iteration 69/1000 | Loss: 0.00003084
Iteration 70/1000 | Loss: 0.00003084
Iteration 71/1000 | Loss: 0.00003084
Iteration 72/1000 | Loss: 0.00003084
Iteration 73/1000 | Loss: 0.00003084
Iteration 74/1000 | Loss: 0.00003083
Iteration 75/1000 | Loss: 0.00003083
Iteration 76/1000 | Loss: 0.00003083
Iteration 77/1000 | Loss: 0.00003082
Iteration 78/1000 | Loss: 0.00003082
Iteration 79/1000 | Loss: 0.00003082
Iteration 80/1000 | Loss: 0.00003082
Iteration 81/1000 | Loss: 0.00003082
Iteration 82/1000 | Loss: 0.00003082
Iteration 83/1000 | Loss: 0.00003082
Iteration 84/1000 | Loss: 0.00003081
Iteration 85/1000 | Loss: 0.00003081
Iteration 86/1000 | Loss: 0.00003081
Iteration 87/1000 | Loss: 0.00003081
Iteration 88/1000 | Loss: 0.00003081
Iteration 89/1000 | Loss: 0.00003080
Iteration 90/1000 | Loss: 0.00003080
Iteration 91/1000 | Loss: 0.00003080
Iteration 92/1000 | Loss: 0.00003080
Iteration 93/1000 | Loss: 0.00003080
Iteration 94/1000 | Loss: 0.00003080
Iteration 95/1000 | Loss: 0.00003080
Iteration 96/1000 | Loss: 0.00003079
Iteration 97/1000 | Loss: 0.00003079
Iteration 98/1000 | Loss: 0.00003079
Iteration 99/1000 | Loss: 0.00003079
Iteration 100/1000 | Loss: 0.00003079
Iteration 101/1000 | Loss: 0.00003079
Iteration 102/1000 | Loss: 0.00003079
Iteration 103/1000 | Loss: 0.00003079
Iteration 104/1000 | Loss: 0.00003078
Iteration 105/1000 | Loss: 0.00003078
Iteration 106/1000 | Loss: 0.00003078
Iteration 107/1000 | Loss: 0.00003078
Iteration 108/1000 | Loss: 0.00003077
Iteration 109/1000 | Loss: 0.00003077
Iteration 110/1000 | Loss: 0.00003077
Iteration 111/1000 | Loss: 0.00003077
Iteration 112/1000 | Loss: 0.00003076
Iteration 113/1000 | Loss: 0.00003076
Iteration 114/1000 | Loss: 0.00003076
Iteration 115/1000 | Loss: 0.00003076
Iteration 116/1000 | Loss: 0.00003076
Iteration 117/1000 | Loss: 0.00003075
Iteration 118/1000 | Loss: 0.00003075
Iteration 119/1000 | Loss: 0.00003075
Iteration 120/1000 | Loss: 0.00003075
Iteration 121/1000 | Loss: 0.00003074
Iteration 122/1000 | Loss: 0.00003074
Iteration 123/1000 | Loss: 0.00003074
Iteration 124/1000 | Loss: 0.00003074
Iteration 125/1000 | Loss: 0.00003074
Iteration 126/1000 | Loss: 0.00003074
Iteration 127/1000 | Loss: 0.00003074
Iteration 128/1000 | Loss: 0.00003073
Iteration 129/1000 | Loss: 0.00003073
Iteration 130/1000 | Loss: 0.00003073
Iteration 131/1000 | Loss: 0.00003073
Iteration 132/1000 | Loss: 0.00003073
Iteration 133/1000 | Loss: 0.00003073
Iteration 134/1000 | Loss: 0.00003073
Iteration 135/1000 | Loss: 0.00003073
Iteration 136/1000 | Loss: 0.00003073
Iteration 137/1000 | Loss: 0.00003073
Iteration 138/1000 | Loss: 0.00003073
Iteration 139/1000 | Loss: 0.00003073
Iteration 140/1000 | Loss: 0.00003072
Iteration 141/1000 | Loss: 0.00003072
Iteration 142/1000 | Loss: 0.00003072
Iteration 143/1000 | Loss: 0.00003072
Iteration 144/1000 | Loss: 0.00003072
Iteration 145/1000 | Loss: 0.00003072
Iteration 146/1000 | Loss: 0.00003072
Iteration 147/1000 | Loss: 0.00003072
Iteration 148/1000 | Loss: 0.00003072
Iteration 149/1000 | Loss: 0.00003072
Iteration 150/1000 | Loss: 0.00003072
Iteration 151/1000 | Loss: 0.00003071
Iteration 152/1000 | Loss: 0.00003071
Iteration 153/1000 | Loss: 0.00003071
Iteration 154/1000 | Loss: 0.00003071
Iteration 155/1000 | Loss: 0.00003071
Iteration 156/1000 | Loss: 0.00003071
Iteration 157/1000 | Loss: 0.00003071
Iteration 158/1000 | Loss: 0.00003071
Iteration 159/1000 | Loss: 0.00003070
Iteration 160/1000 | Loss: 0.00003070
Iteration 161/1000 | Loss: 0.00003070
Iteration 162/1000 | Loss: 0.00003070
Iteration 163/1000 | Loss: 0.00003070
Iteration 164/1000 | Loss: 0.00003070
Iteration 165/1000 | Loss: 0.00003070
Iteration 166/1000 | Loss: 0.00003070
Iteration 167/1000 | Loss: 0.00003070
Iteration 168/1000 | Loss: 0.00003070
Iteration 169/1000 | Loss: 0.00003069
Iteration 170/1000 | Loss: 0.00003069
Iteration 171/1000 | Loss: 0.00003069
Iteration 172/1000 | Loss: 0.00003069
Iteration 173/1000 | Loss: 0.00003069
Iteration 174/1000 | Loss: 0.00003068
Iteration 175/1000 | Loss: 0.00003068
Iteration 176/1000 | Loss: 0.00003068
Iteration 177/1000 | Loss: 0.00003068
Iteration 178/1000 | Loss: 0.00003068
Iteration 179/1000 | Loss: 0.00003068
Iteration 180/1000 | Loss: 0.00003068
Iteration 181/1000 | Loss: 0.00003068
Iteration 182/1000 | Loss: 0.00003068
Iteration 183/1000 | Loss: 0.00003068
Iteration 184/1000 | Loss: 0.00003068
Iteration 185/1000 | Loss: 0.00003068
Iteration 186/1000 | Loss: 0.00003067
Iteration 187/1000 | Loss: 0.00003067
Iteration 188/1000 | Loss: 0.00003067
Iteration 189/1000 | Loss: 0.00003067
Iteration 190/1000 | Loss: 0.00003067
Iteration 191/1000 | Loss: 0.00003067
Iteration 192/1000 | Loss: 0.00003067
Iteration 193/1000 | Loss: 0.00003067
Iteration 194/1000 | Loss: 0.00003067
Iteration 195/1000 | Loss: 0.00003067
Iteration 196/1000 | Loss: 0.00003067
Iteration 197/1000 | Loss: 0.00003067
Iteration 198/1000 | Loss: 0.00003067
Iteration 199/1000 | Loss: 0.00003067
Iteration 200/1000 | Loss: 0.00003067
Iteration 201/1000 | Loss: 0.00003066
Iteration 202/1000 | Loss: 0.00003066
Iteration 203/1000 | Loss: 0.00003066
Iteration 204/1000 | Loss: 0.00003066
Iteration 205/1000 | Loss: 0.00003066
Iteration 206/1000 | Loss: 0.00003066
Iteration 207/1000 | Loss: 0.00003066
Iteration 208/1000 | Loss: 0.00003066
Iteration 209/1000 | Loss: 0.00003066
Iteration 210/1000 | Loss: 0.00003066
Iteration 211/1000 | Loss: 0.00003066
Iteration 212/1000 | Loss: 0.00003066
Iteration 213/1000 | Loss: 0.00003066
Iteration 214/1000 | Loss: 0.00003066
Iteration 215/1000 | Loss: 0.00003066
Iteration 216/1000 | Loss: 0.00003065
Iteration 217/1000 | Loss: 0.00003065
Iteration 218/1000 | Loss: 0.00003065
Iteration 219/1000 | Loss: 0.00003065
Iteration 220/1000 | Loss: 0.00003065
Iteration 221/1000 | Loss: 0.00003065
Iteration 222/1000 | Loss: 0.00003065
Iteration 223/1000 | Loss: 0.00003065
Iteration 224/1000 | Loss: 0.00003065
Iteration 225/1000 | Loss: 0.00003065
Iteration 226/1000 | Loss: 0.00003065
Iteration 227/1000 | Loss: 0.00003065
Iteration 228/1000 | Loss: 0.00003065
Iteration 229/1000 | Loss: 0.00003065
Iteration 230/1000 | Loss: 0.00003065
Iteration 231/1000 | Loss: 0.00003065
Iteration 232/1000 | Loss: 0.00003065
Iteration 233/1000 | Loss: 0.00003065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [3.065078999497928e-05, 3.065078999497928e-05, 3.065078999497928e-05, 3.065078999497928e-05, 3.065078999497928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.065078999497928e-05

Optimization complete. Final v2v error: 4.418741226196289 mm

Highest mean error: 5.6650614738464355 mm for frame 75

Lowest mean error: 3.677489995956421 mm for frame 55

Saving results

Total time: 54.80685234069824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771750
Iteration 2/25 | Loss: 0.00124262
Iteration 3/25 | Loss: 0.00105645
Iteration 4/25 | Loss: 0.00103629
Iteration 5/25 | Loss: 0.00102977
Iteration 6/25 | Loss: 0.00102851
Iteration 7/25 | Loss: 0.00102851
Iteration 8/25 | Loss: 0.00102851
Iteration 9/25 | Loss: 0.00102851
Iteration 10/25 | Loss: 0.00102851
Iteration 11/25 | Loss: 0.00102851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001028507947921753, 0.001028507947921753, 0.001028507947921753, 0.001028507947921753, 0.001028507947921753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001028507947921753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.43336439
Iteration 2/25 | Loss: 0.00063855
Iteration 3/25 | Loss: 0.00063855
Iteration 4/25 | Loss: 0.00063855
Iteration 5/25 | Loss: 0.00063855
Iteration 6/25 | Loss: 0.00063855
Iteration 7/25 | Loss: 0.00063855
Iteration 8/25 | Loss: 0.00063855
Iteration 9/25 | Loss: 0.00063855
Iteration 10/25 | Loss: 0.00063855
Iteration 11/25 | Loss: 0.00063855
Iteration 12/25 | Loss: 0.00063855
Iteration 13/25 | Loss: 0.00063855
Iteration 14/25 | Loss: 0.00063855
Iteration 15/25 | Loss: 0.00063855
Iteration 16/25 | Loss: 0.00063855
Iteration 17/25 | Loss: 0.00063855
Iteration 18/25 | Loss: 0.00063855
Iteration 19/25 | Loss: 0.00063855
Iteration 20/25 | Loss: 0.00063855
Iteration 21/25 | Loss: 0.00063855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006385460146702826, 0.0006385460146702826, 0.0006385460146702826, 0.0006385460146702826, 0.0006385460146702826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006385460146702826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063855
Iteration 2/1000 | Loss: 0.00005304
Iteration 3/1000 | Loss: 0.00003384
Iteration 4/1000 | Loss: 0.00003044
Iteration 5/1000 | Loss: 0.00002928
Iteration 6/1000 | Loss: 0.00002825
Iteration 7/1000 | Loss: 0.00002769
Iteration 8/1000 | Loss: 0.00002738
Iteration 9/1000 | Loss: 0.00002708
Iteration 10/1000 | Loss: 0.00002695
Iteration 11/1000 | Loss: 0.00002684
Iteration 12/1000 | Loss: 0.00002673
Iteration 13/1000 | Loss: 0.00002670
Iteration 14/1000 | Loss: 0.00002655
Iteration 15/1000 | Loss: 0.00002651
Iteration 16/1000 | Loss: 0.00002651
Iteration 17/1000 | Loss: 0.00002650
Iteration 18/1000 | Loss: 0.00002645
Iteration 19/1000 | Loss: 0.00002638
Iteration 20/1000 | Loss: 0.00002634
Iteration 21/1000 | Loss: 0.00002633
Iteration 22/1000 | Loss: 0.00002633
Iteration 23/1000 | Loss: 0.00002633
Iteration 24/1000 | Loss: 0.00002633
Iteration 25/1000 | Loss: 0.00002633
Iteration 26/1000 | Loss: 0.00002633
Iteration 27/1000 | Loss: 0.00002632
Iteration 28/1000 | Loss: 0.00002631
Iteration 29/1000 | Loss: 0.00002631
Iteration 30/1000 | Loss: 0.00002630
Iteration 31/1000 | Loss: 0.00002630
Iteration 32/1000 | Loss: 0.00002629
Iteration 33/1000 | Loss: 0.00002629
Iteration 34/1000 | Loss: 0.00002629
Iteration 35/1000 | Loss: 0.00002629
Iteration 36/1000 | Loss: 0.00002629
Iteration 37/1000 | Loss: 0.00002628
Iteration 38/1000 | Loss: 0.00002628
Iteration 39/1000 | Loss: 0.00002628
Iteration 40/1000 | Loss: 0.00002628
Iteration 41/1000 | Loss: 0.00002627
Iteration 42/1000 | Loss: 0.00002627
Iteration 43/1000 | Loss: 0.00002627
Iteration 44/1000 | Loss: 0.00002627
Iteration 45/1000 | Loss: 0.00002627
Iteration 46/1000 | Loss: 0.00002627
Iteration 47/1000 | Loss: 0.00002627
Iteration 48/1000 | Loss: 0.00002627
Iteration 49/1000 | Loss: 0.00002627
Iteration 50/1000 | Loss: 0.00002627
Iteration 51/1000 | Loss: 0.00002627
Iteration 52/1000 | Loss: 0.00002627
Iteration 53/1000 | Loss: 0.00002627
Iteration 54/1000 | Loss: 0.00002626
Iteration 55/1000 | Loss: 0.00002626
Iteration 56/1000 | Loss: 0.00002626
Iteration 57/1000 | Loss: 0.00002626
Iteration 58/1000 | Loss: 0.00002626
Iteration 59/1000 | Loss: 0.00002626
Iteration 60/1000 | Loss: 0.00002626
Iteration 61/1000 | Loss: 0.00002626
Iteration 62/1000 | Loss: 0.00002626
Iteration 63/1000 | Loss: 0.00002626
Iteration 64/1000 | Loss: 0.00002625
Iteration 65/1000 | Loss: 0.00002625
Iteration 66/1000 | Loss: 0.00002625
Iteration 67/1000 | Loss: 0.00002625
Iteration 68/1000 | Loss: 0.00002625
Iteration 69/1000 | Loss: 0.00002625
Iteration 70/1000 | Loss: 0.00002625
Iteration 71/1000 | Loss: 0.00002625
Iteration 72/1000 | Loss: 0.00002625
Iteration 73/1000 | Loss: 0.00002625
Iteration 74/1000 | Loss: 0.00002625
Iteration 75/1000 | Loss: 0.00002625
Iteration 76/1000 | Loss: 0.00002625
Iteration 77/1000 | Loss: 0.00002625
Iteration 78/1000 | Loss: 0.00002625
Iteration 79/1000 | Loss: 0.00002625
Iteration 80/1000 | Loss: 0.00002625
Iteration 81/1000 | Loss: 0.00002625
Iteration 82/1000 | Loss: 0.00002625
Iteration 83/1000 | Loss: 0.00002625
Iteration 84/1000 | Loss: 0.00002625
Iteration 85/1000 | Loss: 0.00002625
Iteration 86/1000 | Loss: 0.00002625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.6247234927723184e-05, 2.6247234927723184e-05, 2.6247234927723184e-05, 2.6247234927723184e-05, 2.6247234927723184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6247234927723184e-05

Optimization complete. Final v2v error: 4.125908374786377 mm

Highest mean error: 4.760807514190674 mm for frame 94

Lowest mean error: 3.3942432403564453 mm for frame 82

Saving results

Total time: 32.54551911354065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059923
Iteration 2/25 | Loss: 0.00274069
Iteration 3/25 | Loss: 0.00208645
Iteration 4/25 | Loss: 0.00186005
Iteration 5/25 | Loss: 0.00188909
Iteration 6/25 | Loss: 0.00186799
Iteration 7/25 | Loss: 0.00164976
Iteration 8/25 | Loss: 0.00155770
Iteration 9/25 | Loss: 0.00141006
Iteration 10/25 | Loss: 0.00130671
Iteration 11/25 | Loss: 0.00125871
Iteration 12/25 | Loss: 0.00122111
Iteration 13/25 | Loss: 0.00123392
Iteration 14/25 | Loss: 0.00121938
Iteration 15/25 | Loss: 0.00116640
Iteration 16/25 | Loss: 0.00114754
Iteration 17/25 | Loss: 0.00114669
Iteration 18/25 | Loss: 0.00112931
Iteration 19/25 | Loss: 0.00112646
Iteration 20/25 | Loss: 0.00112663
Iteration 21/25 | Loss: 0.00112618
Iteration 22/25 | Loss: 0.00112535
Iteration 23/25 | Loss: 0.00112520
Iteration 24/25 | Loss: 0.00112134
Iteration 25/25 | Loss: 0.00112274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61812413
Iteration 2/25 | Loss: 0.00368225
Iteration 3/25 | Loss: 0.00238146
Iteration 4/25 | Loss: 0.00238146
Iteration 5/25 | Loss: 0.00238146
Iteration 6/25 | Loss: 0.00238146
Iteration 7/25 | Loss: 0.00238146
Iteration 8/25 | Loss: 0.00238146
Iteration 9/25 | Loss: 0.00238146
Iteration 10/25 | Loss: 0.00238146
Iteration 11/25 | Loss: 0.00238146
Iteration 12/25 | Loss: 0.00238146
Iteration 13/25 | Loss: 0.00238146
Iteration 14/25 | Loss: 0.00238146
Iteration 15/25 | Loss: 0.00238146
Iteration 16/25 | Loss: 0.00238146
Iteration 17/25 | Loss: 0.00238146
Iteration 18/25 | Loss: 0.00238146
Iteration 19/25 | Loss: 0.00238146
Iteration 20/25 | Loss: 0.00238146
Iteration 21/25 | Loss: 0.00238146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0023814591113477945, 0.0023814591113477945, 0.0023814591113477945, 0.0023814591113477945, 0.0023814591113477945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023814591113477945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238146
Iteration 2/1000 | Loss: 0.00276156
Iteration 3/1000 | Loss: 0.00082391
Iteration 4/1000 | Loss: 0.00034416
Iteration 5/1000 | Loss: 0.00133793
Iteration 6/1000 | Loss: 0.00043666
Iteration 7/1000 | Loss: 0.00024865
Iteration 8/1000 | Loss: 0.00149964
Iteration 9/1000 | Loss: 0.00015671
Iteration 10/1000 | Loss: 0.00046426
Iteration 11/1000 | Loss: 0.00212970
Iteration 12/1000 | Loss: 0.00130927
Iteration 13/1000 | Loss: 0.00095940
Iteration 14/1000 | Loss: 0.00074765
Iteration 15/1000 | Loss: 0.00139027
Iteration 16/1000 | Loss: 0.00100268
Iteration 17/1000 | Loss: 0.00108329
Iteration 18/1000 | Loss: 0.00022771
Iteration 19/1000 | Loss: 0.00015634
Iteration 20/1000 | Loss: 0.00088921
Iteration 21/1000 | Loss: 0.00018398
Iteration 22/1000 | Loss: 0.00009753
Iteration 23/1000 | Loss: 0.00059043
Iteration 24/1000 | Loss: 0.00328827
Iteration 25/1000 | Loss: 0.00139544
Iteration 26/1000 | Loss: 0.00109692
Iteration 27/1000 | Loss: 0.00010086
Iteration 28/1000 | Loss: 0.00008027
Iteration 29/1000 | Loss: 0.00034361
Iteration 30/1000 | Loss: 0.00042355
Iteration 31/1000 | Loss: 0.00009297
Iteration 32/1000 | Loss: 0.00007877
Iteration 33/1000 | Loss: 0.00006012
Iteration 34/1000 | Loss: 0.00006837
Iteration 35/1000 | Loss: 0.00006336
Iteration 36/1000 | Loss: 0.00005309
Iteration 37/1000 | Loss: 0.00022239
Iteration 38/1000 | Loss: 0.00007870
Iteration 39/1000 | Loss: 0.00027535
Iteration 40/1000 | Loss: 0.00006644
Iteration 41/1000 | Loss: 0.00006437
Iteration 42/1000 | Loss: 0.00005229
Iteration 43/1000 | Loss: 0.00037466
Iteration 44/1000 | Loss: 0.00206925
Iteration 45/1000 | Loss: 0.00059018
Iteration 46/1000 | Loss: 0.00173058
Iteration 47/1000 | Loss: 0.00010998
Iteration 48/1000 | Loss: 0.00007709
Iteration 49/1000 | Loss: 0.00006056
Iteration 50/1000 | Loss: 0.00044486
Iteration 51/1000 | Loss: 0.00004624
Iteration 52/1000 | Loss: 0.00032105
Iteration 53/1000 | Loss: 0.00003242
Iteration 54/1000 | Loss: 0.00037175
Iteration 55/1000 | Loss: 0.00005897
Iteration 56/1000 | Loss: 0.00053702
Iteration 57/1000 | Loss: 0.00006374
Iteration 58/1000 | Loss: 0.00010800
Iteration 59/1000 | Loss: 0.00002763
Iteration 60/1000 | Loss: 0.00002658
Iteration 61/1000 | Loss: 0.00002563
Iteration 62/1000 | Loss: 0.00002472
Iteration 63/1000 | Loss: 0.00002394
Iteration 64/1000 | Loss: 0.00002343
Iteration 65/1000 | Loss: 0.00002304
Iteration 66/1000 | Loss: 0.00002286
Iteration 67/1000 | Loss: 0.00002272
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002269
Iteration 70/1000 | Loss: 0.00002268
Iteration 71/1000 | Loss: 0.00002263
Iteration 72/1000 | Loss: 0.00002261
Iteration 73/1000 | Loss: 0.00002261
Iteration 74/1000 | Loss: 0.00002261
Iteration 75/1000 | Loss: 0.00002261
Iteration 76/1000 | Loss: 0.00002260
Iteration 77/1000 | Loss: 0.00002260
Iteration 78/1000 | Loss: 0.00002260
Iteration 79/1000 | Loss: 0.00002260
Iteration 80/1000 | Loss: 0.00002260
Iteration 81/1000 | Loss: 0.00002260
Iteration 82/1000 | Loss: 0.00002260
Iteration 83/1000 | Loss: 0.00002259
Iteration 84/1000 | Loss: 0.00002259
Iteration 85/1000 | Loss: 0.00002259
Iteration 86/1000 | Loss: 0.00002258
Iteration 87/1000 | Loss: 0.00002258
Iteration 88/1000 | Loss: 0.00002258
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002257
Iteration 91/1000 | Loss: 0.00002255
Iteration 92/1000 | Loss: 0.00002255
Iteration 93/1000 | Loss: 0.00002255
Iteration 94/1000 | Loss: 0.00002254
Iteration 95/1000 | Loss: 0.00002254
Iteration 96/1000 | Loss: 0.00002253
Iteration 97/1000 | Loss: 0.00002253
Iteration 98/1000 | Loss: 0.00002253
Iteration 99/1000 | Loss: 0.00002252
Iteration 100/1000 | Loss: 0.00002252
Iteration 101/1000 | Loss: 0.00002251
Iteration 102/1000 | Loss: 0.00002251
Iteration 103/1000 | Loss: 0.00002251
Iteration 104/1000 | Loss: 0.00002251
Iteration 105/1000 | Loss: 0.00002250
Iteration 106/1000 | Loss: 0.00002250
Iteration 107/1000 | Loss: 0.00002250
Iteration 108/1000 | Loss: 0.00002250
Iteration 109/1000 | Loss: 0.00002250
Iteration 110/1000 | Loss: 0.00002250
Iteration 111/1000 | Loss: 0.00002250
Iteration 112/1000 | Loss: 0.00002250
Iteration 113/1000 | Loss: 0.00002249
Iteration 114/1000 | Loss: 0.00002249
Iteration 115/1000 | Loss: 0.00002249
Iteration 116/1000 | Loss: 0.00002249
Iteration 117/1000 | Loss: 0.00002249
Iteration 118/1000 | Loss: 0.00002249
Iteration 119/1000 | Loss: 0.00002249
Iteration 120/1000 | Loss: 0.00002249
Iteration 121/1000 | Loss: 0.00002249
Iteration 122/1000 | Loss: 0.00002249
Iteration 123/1000 | Loss: 0.00002248
Iteration 124/1000 | Loss: 0.00002248
Iteration 125/1000 | Loss: 0.00002248
Iteration 126/1000 | Loss: 0.00002248
Iteration 127/1000 | Loss: 0.00002248
Iteration 128/1000 | Loss: 0.00002248
Iteration 129/1000 | Loss: 0.00002248
Iteration 130/1000 | Loss: 0.00002248
Iteration 131/1000 | Loss: 0.00002248
Iteration 132/1000 | Loss: 0.00002248
Iteration 133/1000 | Loss: 0.00002248
Iteration 134/1000 | Loss: 0.00002248
Iteration 135/1000 | Loss: 0.00002248
Iteration 136/1000 | Loss: 0.00002248
Iteration 137/1000 | Loss: 0.00002248
Iteration 138/1000 | Loss: 0.00002248
Iteration 139/1000 | Loss: 0.00002248
Iteration 140/1000 | Loss: 0.00002248
Iteration 141/1000 | Loss: 0.00002248
Iteration 142/1000 | Loss: 0.00002248
Iteration 143/1000 | Loss: 0.00002248
Iteration 144/1000 | Loss: 0.00002248
Iteration 145/1000 | Loss: 0.00002248
Iteration 146/1000 | Loss: 0.00002248
Iteration 147/1000 | Loss: 0.00002248
Iteration 148/1000 | Loss: 0.00002248
Iteration 149/1000 | Loss: 0.00002248
Iteration 150/1000 | Loss: 0.00002248
Iteration 151/1000 | Loss: 0.00002248
Iteration 152/1000 | Loss: 0.00002248
Iteration 153/1000 | Loss: 0.00002248
Iteration 154/1000 | Loss: 0.00002248
Iteration 155/1000 | Loss: 0.00002248
Iteration 156/1000 | Loss: 0.00002248
Iteration 157/1000 | Loss: 0.00002248
Iteration 158/1000 | Loss: 0.00002248
Iteration 159/1000 | Loss: 0.00002248
Iteration 160/1000 | Loss: 0.00002248
Iteration 161/1000 | Loss: 0.00002248
Iteration 162/1000 | Loss: 0.00002248
Iteration 163/1000 | Loss: 0.00002248
Iteration 164/1000 | Loss: 0.00002248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.2478907339973375e-05, 2.2478907339973375e-05, 2.2478907339973375e-05, 2.2478907339973375e-05, 2.2478907339973375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2478907339973375e-05

Optimization complete. Final v2v error: 3.95719838142395 mm

Highest mean error: 4.56492280960083 mm for frame 51

Lowest mean error: 3.5317466259002686 mm for frame 15

Saving results

Total time: 150.36146306991577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621641
Iteration 2/25 | Loss: 0.00144164
Iteration 3/25 | Loss: 0.00103666
Iteration 4/25 | Loss: 0.00100331
Iteration 5/25 | Loss: 0.00099218
Iteration 6/25 | Loss: 0.00098918
Iteration 7/25 | Loss: 0.00098859
Iteration 8/25 | Loss: 0.00098856
Iteration 9/25 | Loss: 0.00098856
Iteration 10/25 | Loss: 0.00098856
Iteration 11/25 | Loss: 0.00098856
Iteration 12/25 | Loss: 0.00098856
Iteration 13/25 | Loss: 0.00098856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000988558167591691, 0.000988558167591691, 0.000988558167591691, 0.000988558167591691, 0.000988558167591691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000988558167591691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96990830
Iteration 2/25 | Loss: 0.00058707
Iteration 3/25 | Loss: 0.00058706
Iteration 4/25 | Loss: 0.00058706
Iteration 5/25 | Loss: 0.00058706
Iteration 6/25 | Loss: 0.00058706
Iteration 7/25 | Loss: 0.00058706
Iteration 8/25 | Loss: 0.00058706
Iteration 9/25 | Loss: 0.00058706
Iteration 10/25 | Loss: 0.00058706
Iteration 11/25 | Loss: 0.00058706
Iteration 12/25 | Loss: 0.00058706
Iteration 13/25 | Loss: 0.00058706
Iteration 14/25 | Loss: 0.00058706
Iteration 15/25 | Loss: 0.00058706
Iteration 16/25 | Loss: 0.00058706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005870562745258212, 0.0005870562745258212, 0.0005870562745258212, 0.0005870562745258212, 0.0005870562745258212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005870562745258212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058706
Iteration 2/1000 | Loss: 0.00006487
Iteration 3/1000 | Loss: 0.00003879
Iteration 4/1000 | Loss: 0.00003521
Iteration 5/1000 | Loss: 0.00003355
Iteration 6/1000 | Loss: 0.00003271
Iteration 7/1000 | Loss: 0.00003190
Iteration 8/1000 | Loss: 0.00003141
Iteration 9/1000 | Loss: 0.00003097
Iteration 10/1000 | Loss: 0.00003066
Iteration 11/1000 | Loss: 0.00003032
Iteration 12/1000 | Loss: 0.00003008
Iteration 13/1000 | Loss: 0.00002989
Iteration 14/1000 | Loss: 0.00002971
Iteration 15/1000 | Loss: 0.00002950
Iteration 16/1000 | Loss: 0.00002932
Iteration 17/1000 | Loss: 0.00002919
Iteration 18/1000 | Loss: 0.00002913
Iteration 19/1000 | Loss: 0.00002906
Iteration 20/1000 | Loss: 0.00002906
Iteration 21/1000 | Loss: 0.00002905
Iteration 22/1000 | Loss: 0.00002904
Iteration 23/1000 | Loss: 0.00002902
Iteration 24/1000 | Loss: 0.00002902
Iteration 25/1000 | Loss: 0.00002902
Iteration 26/1000 | Loss: 0.00002901
Iteration 27/1000 | Loss: 0.00002900
Iteration 28/1000 | Loss: 0.00002898
Iteration 29/1000 | Loss: 0.00002898
Iteration 30/1000 | Loss: 0.00002898
Iteration 31/1000 | Loss: 0.00002898
Iteration 32/1000 | Loss: 0.00002898
Iteration 33/1000 | Loss: 0.00002898
Iteration 34/1000 | Loss: 0.00002897
Iteration 35/1000 | Loss: 0.00002897
Iteration 36/1000 | Loss: 0.00002896
Iteration 37/1000 | Loss: 0.00002896
Iteration 38/1000 | Loss: 0.00002895
Iteration 39/1000 | Loss: 0.00002895
Iteration 40/1000 | Loss: 0.00002895
Iteration 41/1000 | Loss: 0.00002894
Iteration 42/1000 | Loss: 0.00002894
Iteration 43/1000 | Loss: 0.00002894
Iteration 44/1000 | Loss: 0.00002894
Iteration 45/1000 | Loss: 0.00002894
Iteration 46/1000 | Loss: 0.00002894
Iteration 47/1000 | Loss: 0.00002893
Iteration 48/1000 | Loss: 0.00002893
Iteration 49/1000 | Loss: 0.00002893
Iteration 50/1000 | Loss: 0.00002892
Iteration 51/1000 | Loss: 0.00002892
Iteration 52/1000 | Loss: 0.00002892
Iteration 53/1000 | Loss: 0.00002892
Iteration 54/1000 | Loss: 0.00002891
Iteration 55/1000 | Loss: 0.00002891
Iteration 56/1000 | Loss: 0.00002891
Iteration 57/1000 | Loss: 0.00002891
Iteration 58/1000 | Loss: 0.00002891
Iteration 59/1000 | Loss: 0.00002891
Iteration 60/1000 | Loss: 0.00002890
Iteration 61/1000 | Loss: 0.00002890
Iteration 62/1000 | Loss: 0.00002890
Iteration 63/1000 | Loss: 0.00002890
Iteration 64/1000 | Loss: 0.00002890
Iteration 65/1000 | Loss: 0.00002890
Iteration 66/1000 | Loss: 0.00002890
Iteration 67/1000 | Loss: 0.00002890
Iteration 68/1000 | Loss: 0.00002890
Iteration 69/1000 | Loss: 0.00002890
Iteration 70/1000 | Loss: 0.00002889
Iteration 71/1000 | Loss: 0.00002889
Iteration 72/1000 | Loss: 0.00002889
Iteration 73/1000 | Loss: 0.00002889
Iteration 74/1000 | Loss: 0.00002889
Iteration 75/1000 | Loss: 0.00002889
Iteration 76/1000 | Loss: 0.00002888
Iteration 77/1000 | Loss: 0.00002888
Iteration 78/1000 | Loss: 0.00002888
Iteration 79/1000 | Loss: 0.00002888
Iteration 80/1000 | Loss: 0.00002888
Iteration 81/1000 | Loss: 0.00002888
Iteration 82/1000 | Loss: 0.00002888
Iteration 83/1000 | Loss: 0.00002888
Iteration 84/1000 | Loss: 0.00002887
Iteration 85/1000 | Loss: 0.00002887
Iteration 86/1000 | Loss: 0.00002887
Iteration 87/1000 | Loss: 0.00002887
Iteration 88/1000 | Loss: 0.00002887
Iteration 89/1000 | Loss: 0.00002887
Iteration 90/1000 | Loss: 0.00002887
Iteration 91/1000 | Loss: 0.00002886
Iteration 92/1000 | Loss: 0.00002886
Iteration 93/1000 | Loss: 0.00002886
Iteration 94/1000 | Loss: 0.00002886
Iteration 95/1000 | Loss: 0.00002886
Iteration 96/1000 | Loss: 0.00002886
Iteration 97/1000 | Loss: 0.00002886
Iteration 98/1000 | Loss: 0.00002886
Iteration 99/1000 | Loss: 0.00002886
Iteration 100/1000 | Loss: 0.00002886
Iteration 101/1000 | Loss: 0.00002885
Iteration 102/1000 | Loss: 0.00002885
Iteration 103/1000 | Loss: 0.00002885
Iteration 104/1000 | Loss: 0.00002885
Iteration 105/1000 | Loss: 0.00002885
Iteration 106/1000 | Loss: 0.00002885
Iteration 107/1000 | Loss: 0.00002884
Iteration 108/1000 | Loss: 0.00002884
Iteration 109/1000 | Loss: 0.00002884
Iteration 110/1000 | Loss: 0.00002884
Iteration 111/1000 | Loss: 0.00002884
Iteration 112/1000 | Loss: 0.00002884
Iteration 113/1000 | Loss: 0.00002884
Iteration 114/1000 | Loss: 0.00002884
Iteration 115/1000 | Loss: 0.00002884
Iteration 116/1000 | Loss: 0.00002884
Iteration 117/1000 | Loss: 0.00002884
Iteration 118/1000 | Loss: 0.00002884
Iteration 119/1000 | Loss: 0.00002884
Iteration 120/1000 | Loss: 0.00002883
Iteration 121/1000 | Loss: 0.00002883
Iteration 122/1000 | Loss: 0.00002883
Iteration 123/1000 | Loss: 0.00002883
Iteration 124/1000 | Loss: 0.00002883
Iteration 125/1000 | Loss: 0.00002883
Iteration 126/1000 | Loss: 0.00002883
Iteration 127/1000 | Loss: 0.00002883
Iteration 128/1000 | Loss: 0.00002883
Iteration 129/1000 | Loss: 0.00002883
Iteration 130/1000 | Loss: 0.00002883
Iteration 131/1000 | Loss: 0.00002882
Iteration 132/1000 | Loss: 0.00002882
Iteration 133/1000 | Loss: 0.00002882
Iteration 134/1000 | Loss: 0.00002882
Iteration 135/1000 | Loss: 0.00002882
Iteration 136/1000 | Loss: 0.00002882
Iteration 137/1000 | Loss: 0.00002882
Iteration 138/1000 | Loss: 0.00002882
Iteration 139/1000 | Loss: 0.00002882
Iteration 140/1000 | Loss: 0.00002882
Iteration 141/1000 | Loss: 0.00002882
Iteration 142/1000 | Loss: 0.00002882
Iteration 143/1000 | Loss: 0.00002882
Iteration 144/1000 | Loss: 0.00002882
Iteration 145/1000 | Loss: 0.00002882
Iteration 146/1000 | Loss: 0.00002882
Iteration 147/1000 | Loss: 0.00002882
Iteration 148/1000 | Loss: 0.00002881
Iteration 149/1000 | Loss: 0.00002881
Iteration 150/1000 | Loss: 0.00002881
Iteration 151/1000 | Loss: 0.00002881
Iteration 152/1000 | Loss: 0.00002881
Iteration 153/1000 | Loss: 0.00002881
Iteration 154/1000 | Loss: 0.00002881
Iteration 155/1000 | Loss: 0.00002881
Iteration 156/1000 | Loss: 0.00002881
Iteration 157/1000 | Loss: 0.00002881
Iteration 158/1000 | Loss: 0.00002881
Iteration 159/1000 | Loss: 0.00002881
Iteration 160/1000 | Loss: 0.00002881
Iteration 161/1000 | Loss: 0.00002881
Iteration 162/1000 | Loss: 0.00002881
Iteration 163/1000 | Loss: 0.00002881
Iteration 164/1000 | Loss: 0.00002881
Iteration 165/1000 | Loss: 0.00002881
Iteration 166/1000 | Loss: 0.00002881
Iteration 167/1000 | Loss: 0.00002881
Iteration 168/1000 | Loss: 0.00002881
Iteration 169/1000 | Loss: 0.00002881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.8814914912800305e-05, 2.8814914912800305e-05, 2.8814914912800305e-05, 2.8814914912800305e-05, 2.8814914912800305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8814914912800305e-05

Optimization complete. Final v2v error: 4.1764702796936035 mm

Highest mean error: 4.921998500823975 mm for frame 98

Lowest mean error: 3.3119757175445557 mm for frame 47

Saving results

Total time: 45.2030770778656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996630
Iteration 2/25 | Loss: 0.00163645
Iteration 3/25 | Loss: 0.00110580
Iteration 4/25 | Loss: 0.00105205
Iteration 5/25 | Loss: 0.00104147
Iteration 6/25 | Loss: 0.00103945
Iteration 7/25 | Loss: 0.00103942
Iteration 8/25 | Loss: 0.00103942
Iteration 9/25 | Loss: 0.00103942
Iteration 10/25 | Loss: 0.00103942
Iteration 11/25 | Loss: 0.00103942
Iteration 12/25 | Loss: 0.00103942
Iteration 13/25 | Loss: 0.00103942
Iteration 14/25 | Loss: 0.00103942
Iteration 15/25 | Loss: 0.00103942
Iteration 16/25 | Loss: 0.00103942
Iteration 17/25 | Loss: 0.00103942
Iteration 18/25 | Loss: 0.00103942
Iteration 19/25 | Loss: 0.00103942
Iteration 20/25 | Loss: 0.00103942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010394183918833733, 0.0010394183918833733, 0.0010394183918833733, 0.0010394183918833733, 0.0010394183918833733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010394183918833733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95649409
Iteration 2/25 | Loss: 0.00056514
Iteration 3/25 | Loss: 0.00056514
Iteration 4/25 | Loss: 0.00056514
Iteration 5/25 | Loss: 0.00056514
Iteration 6/25 | Loss: 0.00056513
Iteration 7/25 | Loss: 0.00056513
Iteration 8/25 | Loss: 0.00056513
Iteration 9/25 | Loss: 0.00056513
Iteration 10/25 | Loss: 0.00056513
Iteration 11/25 | Loss: 0.00056513
Iteration 12/25 | Loss: 0.00056513
Iteration 13/25 | Loss: 0.00056513
Iteration 14/25 | Loss: 0.00056513
Iteration 15/25 | Loss: 0.00056513
Iteration 16/25 | Loss: 0.00056513
Iteration 17/25 | Loss: 0.00056513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005651337560266256, 0.0005651337560266256, 0.0005651337560266256, 0.0005651337560266256, 0.0005651337560266256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005651337560266256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056513
Iteration 2/1000 | Loss: 0.00006940
Iteration 3/1000 | Loss: 0.00005226
Iteration 4/1000 | Loss: 0.00004901
Iteration 5/1000 | Loss: 0.00004701
Iteration 6/1000 | Loss: 0.00004575
Iteration 7/1000 | Loss: 0.00004463
Iteration 8/1000 | Loss: 0.00004392
Iteration 9/1000 | Loss: 0.00004354
Iteration 10/1000 | Loss: 0.00004314
Iteration 11/1000 | Loss: 0.00004280
Iteration 12/1000 | Loss: 0.00004248
Iteration 13/1000 | Loss: 0.00004219
Iteration 14/1000 | Loss: 0.00004193
Iteration 15/1000 | Loss: 0.00004170
Iteration 16/1000 | Loss: 0.00004152
Iteration 17/1000 | Loss: 0.00004139
Iteration 18/1000 | Loss: 0.00004137
Iteration 19/1000 | Loss: 0.00004137
Iteration 20/1000 | Loss: 0.00004130
Iteration 21/1000 | Loss: 0.00004125
Iteration 22/1000 | Loss: 0.00004124
Iteration 23/1000 | Loss: 0.00004124
Iteration 24/1000 | Loss: 0.00004123
Iteration 25/1000 | Loss: 0.00004121
Iteration 26/1000 | Loss: 0.00004120
Iteration 27/1000 | Loss: 0.00004119
Iteration 28/1000 | Loss: 0.00004119
Iteration 29/1000 | Loss: 0.00004118
Iteration 30/1000 | Loss: 0.00004118
Iteration 31/1000 | Loss: 0.00004116
Iteration 32/1000 | Loss: 0.00004115
Iteration 33/1000 | Loss: 0.00004111
Iteration 34/1000 | Loss: 0.00004109
Iteration 35/1000 | Loss: 0.00004109
Iteration 36/1000 | Loss: 0.00004108
Iteration 37/1000 | Loss: 0.00004108
Iteration 38/1000 | Loss: 0.00004107
Iteration 39/1000 | Loss: 0.00004107
Iteration 40/1000 | Loss: 0.00004106
Iteration 41/1000 | Loss: 0.00004106
Iteration 42/1000 | Loss: 0.00004105
Iteration 43/1000 | Loss: 0.00004105
Iteration 44/1000 | Loss: 0.00004105
Iteration 45/1000 | Loss: 0.00004104
Iteration 46/1000 | Loss: 0.00004104
Iteration 47/1000 | Loss: 0.00004104
Iteration 48/1000 | Loss: 0.00004104
Iteration 49/1000 | Loss: 0.00004103
Iteration 50/1000 | Loss: 0.00004103
Iteration 51/1000 | Loss: 0.00004103
Iteration 52/1000 | Loss: 0.00004102
Iteration 53/1000 | Loss: 0.00004102
Iteration 54/1000 | Loss: 0.00004102
Iteration 55/1000 | Loss: 0.00004101
Iteration 56/1000 | Loss: 0.00004101
Iteration 57/1000 | Loss: 0.00004101
Iteration 58/1000 | Loss: 0.00004101
Iteration 59/1000 | Loss: 0.00004101
Iteration 60/1000 | Loss: 0.00004101
Iteration 61/1000 | Loss: 0.00004101
Iteration 62/1000 | Loss: 0.00004101
Iteration 63/1000 | Loss: 0.00004101
Iteration 64/1000 | Loss: 0.00004101
Iteration 65/1000 | Loss: 0.00004101
Iteration 66/1000 | Loss: 0.00004100
Iteration 67/1000 | Loss: 0.00004100
Iteration 68/1000 | Loss: 0.00004100
Iteration 69/1000 | Loss: 0.00004100
Iteration 70/1000 | Loss: 0.00004100
Iteration 71/1000 | Loss: 0.00004100
Iteration 72/1000 | Loss: 0.00004100
Iteration 73/1000 | Loss: 0.00004099
Iteration 74/1000 | Loss: 0.00004099
Iteration 75/1000 | Loss: 0.00004099
Iteration 76/1000 | Loss: 0.00004099
Iteration 77/1000 | Loss: 0.00004099
Iteration 78/1000 | Loss: 0.00004099
Iteration 79/1000 | Loss: 0.00004098
Iteration 80/1000 | Loss: 0.00004098
Iteration 81/1000 | Loss: 0.00004098
Iteration 82/1000 | Loss: 0.00004098
Iteration 83/1000 | Loss: 0.00004098
Iteration 84/1000 | Loss: 0.00004098
Iteration 85/1000 | Loss: 0.00004098
Iteration 86/1000 | Loss: 0.00004097
Iteration 87/1000 | Loss: 0.00004097
Iteration 88/1000 | Loss: 0.00004097
Iteration 89/1000 | Loss: 0.00004097
Iteration 90/1000 | Loss: 0.00004097
Iteration 91/1000 | Loss: 0.00004097
Iteration 92/1000 | Loss: 0.00004097
Iteration 93/1000 | Loss: 0.00004097
Iteration 94/1000 | Loss: 0.00004096
Iteration 95/1000 | Loss: 0.00004096
Iteration 96/1000 | Loss: 0.00004096
Iteration 97/1000 | Loss: 0.00004096
Iteration 98/1000 | Loss: 0.00004096
Iteration 99/1000 | Loss: 0.00004096
Iteration 100/1000 | Loss: 0.00004096
Iteration 101/1000 | Loss: 0.00004096
Iteration 102/1000 | Loss: 0.00004096
Iteration 103/1000 | Loss: 0.00004096
Iteration 104/1000 | Loss: 0.00004096
Iteration 105/1000 | Loss: 0.00004096
Iteration 106/1000 | Loss: 0.00004096
Iteration 107/1000 | Loss: 0.00004096
Iteration 108/1000 | Loss: 0.00004096
Iteration 109/1000 | Loss: 0.00004096
Iteration 110/1000 | Loss: 0.00004096
Iteration 111/1000 | Loss: 0.00004096
Iteration 112/1000 | Loss: 0.00004096
Iteration 113/1000 | Loss: 0.00004096
Iteration 114/1000 | Loss: 0.00004096
Iteration 115/1000 | Loss: 0.00004096
Iteration 116/1000 | Loss: 0.00004096
Iteration 117/1000 | Loss: 0.00004096
Iteration 118/1000 | Loss: 0.00004096
Iteration 119/1000 | Loss: 0.00004096
Iteration 120/1000 | Loss: 0.00004096
Iteration 121/1000 | Loss: 0.00004096
Iteration 122/1000 | Loss: 0.00004096
Iteration 123/1000 | Loss: 0.00004096
Iteration 124/1000 | Loss: 0.00004096
Iteration 125/1000 | Loss: 0.00004096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [4.096020711585879e-05, 4.096020711585879e-05, 4.096020711585879e-05, 4.096020711585879e-05, 4.096020711585879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.096020711585879e-05

Optimization complete. Final v2v error: 5.335506439208984 mm

Highest mean error: 5.916379451751709 mm for frame 96

Lowest mean error: 4.793758869171143 mm for frame 121

Saving results

Total time: 49.17865228652954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053183
Iteration 2/25 | Loss: 0.00387076
Iteration 3/25 | Loss: 0.00255497
Iteration 4/25 | Loss: 0.00237178
Iteration 5/25 | Loss: 0.00202959
Iteration 6/25 | Loss: 0.00175761
Iteration 7/25 | Loss: 0.00156745
Iteration 8/25 | Loss: 0.00153296
Iteration 9/25 | Loss: 0.00144742
Iteration 10/25 | Loss: 0.00136779
Iteration 11/25 | Loss: 0.00133810
Iteration 12/25 | Loss: 0.00132575
Iteration 13/25 | Loss: 0.00128891
Iteration 14/25 | Loss: 0.00127283
Iteration 15/25 | Loss: 0.00125968
Iteration 16/25 | Loss: 0.00125453
Iteration 17/25 | Loss: 0.00123358
Iteration 18/25 | Loss: 0.00123611
Iteration 19/25 | Loss: 0.00122135
Iteration 20/25 | Loss: 0.00121322
Iteration 21/25 | Loss: 0.00121178
Iteration 22/25 | Loss: 0.00120451
Iteration 23/25 | Loss: 0.00120986
Iteration 24/25 | Loss: 0.00120304
Iteration 25/25 | Loss: 0.00119639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48822522
Iteration 2/25 | Loss: 0.00552570
Iteration 3/25 | Loss: 0.00402348
Iteration 4/25 | Loss: 0.00402347
Iteration 5/25 | Loss: 0.00402347
Iteration 6/25 | Loss: 0.00402347
Iteration 7/25 | Loss: 0.00402347
Iteration 8/25 | Loss: 0.00402347
Iteration 9/25 | Loss: 0.00402347
Iteration 10/25 | Loss: 0.00402347
Iteration 11/25 | Loss: 0.00402347
Iteration 12/25 | Loss: 0.00402347
Iteration 13/25 | Loss: 0.00402347
Iteration 14/25 | Loss: 0.00402347
Iteration 15/25 | Loss: 0.00402347
Iteration 16/25 | Loss: 0.00402347
Iteration 17/25 | Loss: 0.00402347
Iteration 18/25 | Loss: 0.00402347
Iteration 19/25 | Loss: 0.00402347
Iteration 20/25 | Loss: 0.00402347
Iteration 21/25 | Loss: 0.00402347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004023469518870115, 0.004023469518870115, 0.004023469518870115, 0.004023469518870115, 0.004023469518870115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004023469518870115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00402347
Iteration 2/1000 | Loss: 0.00112368
Iteration 3/1000 | Loss: 0.00078901
Iteration 4/1000 | Loss: 0.00105840
Iteration 5/1000 | Loss: 0.00184562
Iteration 6/1000 | Loss: 0.00032948
Iteration 7/1000 | Loss: 0.00060997
Iteration 8/1000 | Loss: 0.00085918
Iteration 9/1000 | Loss: 0.00042456
Iteration 10/1000 | Loss: 0.00072988
Iteration 11/1000 | Loss: 0.00160542
Iteration 12/1000 | Loss: 0.00150503
Iteration 13/1000 | Loss: 0.00028641
Iteration 14/1000 | Loss: 0.00026681
Iteration 15/1000 | Loss: 0.00075146
Iteration 16/1000 | Loss: 0.00154550
Iteration 17/1000 | Loss: 0.00134194
Iteration 18/1000 | Loss: 0.00097629
Iteration 19/1000 | Loss: 0.00030538
Iteration 20/1000 | Loss: 0.00024109
Iteration 21/1000 | Loss: 0.00029025
Iteration 22/1000 | Loss: 0.00067778
Iteration 23/1000 | Loss: 0.00534505
Iteration 24/1000 | Loss: 0.00116337
Iteration 25/1000 | Loss: 0.00284688
Iteration 26/1000 | Loss: 0.00118531
Iteration 27/1000 | Loss: 0.00095501
Iteration 28/1000 | Loss: 0.00025530
Iteration 29/1000 | Loss: 0.00056358
Iteration 30/1000 | Loss: 0.00036461
Iteration 31/1000 | Loss: 0.00034990
Iteration 32/1000 | Loss: 0.00041779
Iteration 33/1000 | Loss: 0.00081500
Iteration 34/1000 | Loss: 0.00021967
Iteration 35/1000 | Loss: 0.00125483
Iteration 36/1000 | Loss: 0.00068662
Iteration 37/1000 | Loss: 0.00056413
Iteration 38/1000 | Loss: 0.00073297
Iteration 39/1000 | Loss: 0.00022132
Iteration 40/1000 | Loss: 0.00021176
Iteration 41/1000 | Loss: 0.00092451
Iteration 42/1000 | Loss: 0.00021211
Iteration 43/1000 | Loss: 0.00025777
Iteration 44/1000 | Loss: 0.00024967
Iteration 45/1000 | Loss: 0.00019660
Iteration 46/1000 | Loss: 0.00019317
Iteration 47/1000 | Loss: 0.00018984
Iteration 48/1000 | Loss: 0.00018794
Iteration 49/1000 | Loss: 0.00018559
Iteration 50/1000 | Loss: 0.00114959
Iteration 51/1000 | Loss: 0.00026038
Iteration 52/1000 | Loss: 0.00025948
Iteration 53/1000 | Loss: 0.00024140
Iteration 54/1000 | Loss: 0.00018062
Iteration 55/1000 | Loss: 0.00052508
Iteration 56/1000 | Loss: 0.00031868
Iteration 57/1000 | Loss: 0.00068712
Iteration 58/1000 | Loss: 0.00029893
Iteration 59/1000 | Loss: 0.00067805
Iteration 60/1000 | Loss: 0.00166851
Iteration 61/1000 | Loss: 0.00078166
Iteration 62/1000 | Loss: 0.00044504
Iteration 63/1000 | Loss: 0.00057883
Iteration 64/1000 | Loss: 0.00021318
Iteration 65/1000 | Loss: 0.00017023
Iteration 66/1000 | Loss: 0.00058952
Iteration 67/1000 | Loss: 0.00033313
Iteration 68/1000 | Loss: 0.00036156
Iteration 69/1000 | Loss: 0.00016187
Iteration 70/1000 | Loss: 0.00061711
Iteration 71/1000 | Loss: 0.00084038
Iteration 72/1000 | Loss: 0.00017151
Iteration 73/1000 | Loss: 0.00015480
Iteration 74/1000 | Loss: 0.00076990
Iteration 75/1000 | Loss: 0.00342372
Iteration 76/1000 | Loss: 0.00085372
Iteration 77/1000 | Loss: 0.00041492
Iteration 78/1000 | Loss: 0.00015003
Iteration 79/1000 | Loss: 0.00014263
Iteration 80/1000 | Loss: 0.00013656
Iteration 81/1000 | Loss: 0.00057291
Iteration 82/1000 | Loss: 0.00013227
Iteration 83/1000 | Loss: 0.00012887
Iteration 84/1000 | Loss: 0.00059660
Iteration 85/1000 | Loss: 0.00110392
Iteration 86/1000 | Loss: 0.00186501
Iteration 87/1000 | Loss: 0.00607411
Iteration 88/1000 | Loss: 0.00082667
Iteration 89/1000 | Loss: 0.00119840
Iteration 90/1000 | Loss: 0.00163167
Iteration 91/1000 | Loss: 0.00224848
Iteration 92/1000 | Loss: 0.00133933
Iteration 93/1000 | Loss: 0.00028176
Iteration 94/1000 | Loss: 0.00069566
Iteration 95/1000 | Loss: 0.00073771
Iteration 96/1000 | Loss: 0.00033832
Iteration 97/1000 | Loss: 0.00012046
Iteration 98/1000 | Loss: 0.00014805
Iteration 99/1000 | Loss: 0.00047555
Iteration 100/1000 | Loss: 0.00034461
Iteration 101/1000 | Loss: 0.00018813
Iteration 102/1000 | Loss: 0.00011560
Iteration 103/1000 | Loss: 0.00028020
Iteration 104/1000 | Loss: 0.00043517
Iteration 105/1000 | Loss: 0.00086957
Iteration 106/1000 | Loss: 0.00093196
Iteration 107/1000 | Loss: 0.00012128
Iteration 108/1000 | Loss: 0.00105723
Iteration 109/1000 | Loss: 0.00013930
Iteration 110/1000 | Loss: 0.00015113
Iteration 111/1000 | Loss: 0.00009899
Iteration 112/1000 | Loss: 0.00009609
Iteration 113/1000 | Loss: 0.00074037
Iteration 114/1000 | Loss: 0.00048440
Iteration 115/1000 | Loss: 0.00029708
Iteration 116/1000 | Loss: 0.00011308
Iteration 117/1000 | Loss: 0.00066998
Iteration 118/1000 | Loss: 0.00035430
Iteration 119/1000 | Loss: 0.00010362
Iteration 120/1000 | Loss: 0.00052924
Iteration 121/1000 | Loss: 0.00031293
Iteration 122/1000 | Loss: 0.00033224
Iteration 123/1000 | Loss: 0.00016148
Iteration 124/1000 | Loss: 0.00009534
Iteration 125/1000 | Loss: 0.00013154
Iteration 126/1000 | Loss: 0.00009365
Iteration 127/1000 | Loss: 0.00036055
Iteration 128/1000 | Loss: 0.00026866
Iteration 129/1000 | Loss: 0.00009461
Iteration 130/1000 | Loss: 0.00009178
Iteration 131/1000 | Loss: 0.00037251
Iteration 132/1000 | Loss: 0.00060457
Iteration 133/1000 | Loss: 0.00043350
Iteration 134/1000 | Loss: 0.00009997
Iteration 135/1000 | Loss: 0.00009281
Iteration 136/1000 | Loss: 0.00029107
Iteration 137/1000 | Loss: 0.00009085
Iteration 138/1000 | Loss: 0.00008816
Iteration 139/1000 | Loss: 0.00008672
Iteration 140/1000 | Loss: 0.00008553
Iteration 141/1000 | Loss: 0.00008506
Iteration 142/1000 | Loss: 0.00008480
Iteration 143/1000 | Loss: 0.00008458
Iteration 144/1000 | Loss: 0.00008453
Iteration 145/1000 | Loss: 0.00008444
Iteration 146/1000 | Loss: 0.00008444
Iteration 147/1000 | Loss: 0.00008442
Iteration 148/1000 | Loss: 0.00008441
Iteration 149/1000 | Loss: 0.00008440
Iteration 150/1000 | Loss: 0.00008439
Iteration 151/1000 | Loss: 0.00008438
Iteration 152/1000 | Loss: 0.00008438
Iteration 153/1000 | Loss: 0.00008436
Iteration 154/1000 | Loss: 0.00008436
Iteration 155/1000 | Loss: 0.00008435
Iteration 156/1000 | Loss: 0.00008435
Iteration 157/1000 | Loss: 0.00008434
Iteration 158/1000 | Loss: 0.00008433
Iteration 159/1000 | Loss: 0.00008430
Iteration 160/1000 | Loss: 0.00008422
Iteration 161/1000 | Loss: 0.00008422
Iteration 162/1000 | Loss: 0.00008422
Iteration 163/1000 | Loss: 0.00008421
Iteration 164/1000 | Loss: 0.00008421
Iteration 165/1000 | Loss: 0.00008421
Iteration 166/1000 | Loss: 0.00008421
Iteration 167/1000 | Loss: 0.00008421
Iteration 168/1000 | Loss: 0.00008421
Iteration 169/1000 | Loss: 0.00008420
Iteration 170/1000 | Loss: 0.00008420
Iteration 171/1000 | Loss: 0.00008420
Iteration 172/1000 | Loss: 0.00008420
Iteration 173/1000 | Loss: 0.00008420
Iteration 174/1000 | Loss: 0.00008420
Iteration 175/1000 | Loss: 0.00008420
Iteration 176/1000 | Loss: 0.00008420
Iteration 177/1000 | Loss: 0.00008420
Iteration 178/1000 | Loss: 0.00008420
Iteration 179/1000 | Loss: 0.00008419
Iteration 180/1000 | Loss: 0.00008419
Iteration 181/1000 | Loss: 0.00008418
Iteration 182/1000 | Loss: 0.00008418
Iteration 183/1000 | Loss: 0.00008418
Iteration 184/1000 | Loss: 0.00008417
Iteration 185/1000 | Loss: 0.00008417
Iteration 186/1000 | Loss: 0.00008417
Iteration 187/1000 | Loss: 0.00008416
Iteration 188/1000 | Loss: 0.00008416
Iteration 189/1000 | Loss: 0.00008416
Iteration 190/1000 | Loss: 0.00008416
Iteration 191/1000 | Loss: 0.00008416
Iteration 192/1000 | Loss: 0.00008415
Iteration 193/1000 | Loss: 0.00008415
Iteration 194/1000 | Loss: 0.00008415
Iteration 195/1000 | Loss: 0.00008415
Iteration 196/1000 | Loss: 0.00008415
Iteration 197/1000 | Loss: 0.00008415
Iteration 198/1000 | Loss: 0.00008415
Iteration 199/1000 | Loss: 0.00008415
Iteration 200/1000 | Loss: 0.00008415
Iteration 201/1000 | Loss: 0.00008415
Iteration 202/1000 | Loss: 0.00008414
Iteration 203/1000 | Loss: 0.00008414
Iteration 204/1000 | Loss: 0.00008414
Iteration 205/1000 | Loss: 0.00008414
Iteration 206/1000 | Loss: 0.00008414
Iteration 207/1000 | Loss: 0.00008414
Iteration 208/1000 | Loss: 0.00008414
Iteration 209/1000 | Loss: 0.00008414
Iteration 210/1000 | Loss: 0.00008414
Iteration 211/1000 | Loss: 0.00008414
Iteration 212/1000 | Loss: 0.00008414
Iteration 213/1000 | Loss: 0.00008414
Iteration 214/1000 | Loss: 0.00008414
Iteration 215/1000 | Loss: 0.00008414
Iteration 216/1000 | Loss: 0.00008414
Iteration 217/1000 | Loss: 0.00008414
Iteration 218/1000 | Loss: 0.00008414
Iteration 219/1000 | Loss: 0.00008414
Iteration 220/1000 | Loss: 0.00008414
Iteration 221/1000 | Loss: 0.00008414
Iteration 222/1000 | Loss: 0.00008414
Iteration 223/1000 | Loss: 0.00008414
Iteration 224/1000 | Loss: 0.00008414
Iteration 225/1000 | Loss: 0.00008414
Iteration 226/1000 | Loss: 0.00008414
Iteration 227/1000 | Loss: 0.00008414
Iteration 228/1000 | Loss: 0.00008414
Iteration 229/1000 | Loss: 0.00008414
Iteration 230/1000 | Loss: 0.00008414
Iteration 231/1000 | Loss: 0.00008414
Iteration 232/1000 | Loss: 0.00008414
Iteration 233/1000 | Loss: 0.00008414
Iteration 234/1000 | Loss: 0.00008414
Iteration 235/1000 | Loss: 0.00008414
Iteration 236/1000 | Loss: 0.00008414
Iteration 237/1000 | Loss: 0.00008414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [8.413750765612349e-05, 8.413750765612349e-05, 8.413750765612349e-05, 8.413750765612349e-05, 8.413750765612349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.413750765612349e-05

Optimization complete. Final v2v error: 5.027408599853516 mm

Highest mean error: 13.83283805847168 mm for frame 50

Lowest mean error: 3.3541290760040283 mm for frame 149

Saving results

Total time: 252.68507480621338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826285
Iteration 2/25 | Loss: 0.00096984
Iteration 3/25 | Loss: 0.00087746
Iteration 4/25 | Loss: 0.00084483
Iteration 5/25 | Loss: 0.00084103
Iteration 6/25 | Loss: 0.00084022
Iteration 7/25 | Loss: 0.00084022
Iteration 8/25 | Loss: 0.00084022
Iteration 9/25 | Loss: 0.00084022
Iteration 10/25 | Loss: 0.00084022
Iteration 11/25 | Loss: 0.00084022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000840215478092432, 0.000840215478092432, 0.000840215478092432, 0.000840215478092432, 0.000840215478092432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000840215478092432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.51004410
Iteration 2/25 | Loss: 0.00065544
Iteration 3/25 | Loss: 0.00065539
Iteration 4/25 | Loss: 0.00065538
Iteration 5/25 | Loss: 0.00065538
Iteration 6/25 | Loss: 0.00065538
Iteration 7/25 | Loss: 0.00065538
Iteration 8/25 | Loss: 0.00065538
Iteration 9/25 | Loss: 0.00065538
Iteration 10/25 | Loss: 0.00065538
Iteration 11/25 | Loss: 0.00065538
Iteration 12/25 | Loss: 0.00065538
Iteration 13/25 | Loss: 0.00065538
Iteration 14/25 | Loss: 0.00065538
Iteration 15/25 | Loss: 0.00065538
Iteration 16/25 | Loss: 0.00065538
Iteration 17/25 | Loss: 0.00065538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006553818820975721, 0.0006553818820975721, 0.0006553818820975721, 0.0006553818820975721, 0.0006553818820975721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006553818820975721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065538
Iteration 2/1000 | Loss: 0.00003215
Iteration 3/1000 | Loss: 0.00002256
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00001988
Iteration 6/1000 | Loss: 0.00001940
Iteration 7/1000 | Loss: 0.00001889
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001844
Iteration 10/1000 | Loss: 0.00001820
Iteration 11/1000 | Loss: 0.00001818
Iteration 12/1000 | Loss: 0.00001803
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001797
Iteration 15/1000 | Loss: 0.00001788
Iteration 16/1000 | Loss: 0.00001788
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001780
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001775
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001773
Iteration 23/1000 | Loss: 0.00001773
Iteration 24/1000 | Loss: 0.00001772
Iteration 25/1000 | Loss: 0.00001772
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001767
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001767
Iteration 34/1000 | Loss: 0.00001767
Iteration 35/1000 | Loss: 0.00001766
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001765
Iteration 38/1000 | Loss: 0.00001765
Iteration 39/1000 | Loss: 0.00001764
Iteration 40/1000 | Loss: 0.00001764
Iteration 41/1000 | Loss: 0.00001764
Iteration 42/1000 | Loss: 0.00001764
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001760
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001757
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001756
Iteration 85/1000 | Loss: 0.00001756
Iteration 86/1000 | Loss: 0.00001756
Iteration 87/1000 | Loss: 0.00001755
Iteration 88/1000 | Loss: 0.00001755
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001755
Iteration 93/1000 | Loss: 0.00001755
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001753
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001752
Iteration 118/1000 | Loss: 0.00001752
Iteration 119/1000 | Loss: 0.00001752
Iteration 120/1000 | Loss: 0.00001752
Iteration 121/1000 | Loss: 0.00001752
Iteration 122/1000 | Loss: 0.00001752
Iteration 123/1000 | Loss: 0.00001752
Iteration 124/1000 | Loss: 0.00001752
Iteration 125/1000 | Loss: 0.00001752
Iteration 126/1000 | Loss: 0.00001752
Iteration 127/1000 | Loss: 0.00001752
Iteration 128/1000 | Loss: 0.00001752
Iteration 129/1000 | Loss: 0.00001752
Iteration 130/1000 | Loss: 0.00001752
Iteration 131/1000 | Loss: 0.00001752
Iteration 132/1000 | Loss: 0.00001752
Iteration 133/1000 | Loss: 0.00001752
Iteration 134/1000 | Loss: 0.00001752
Iteration 135/1000 | Loss: 0.00001752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.7524909708299674e-05, 1.7524909708299674e-05, 1.7524909708299674e-05, 1.7524909708299674e-05, 1.7524909708299674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7524909708299674e-05

Optimization complete. Final v2v error: 3.5749995708465576 mm

Highest mean error: 4.000048637390137 mm for frame 167

Lowest mean error: 3.2948153018951416 mm for frame 53

Saving results

Total time: 38.6443510055542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895953
Iteration 2/25 | Loss: 0.00126933
Iteration 3/25 | Loss: 0.00093166
Iteration 4/25 | Loss: 0.00089603
Iteration 5/25 | Loss: 0.00088879
Iteration 6/25 | Loss: 0.00088742
Iteration 7/25 | Loss: 0.00088739
Iteration 8/25 | Loss: 0.00088739
Iteration 9/25 | Loss: 0.00088739
Iteration 10/25 | Loss: 0.00088739
Iteration 11/25 | Loss: 0.00088739
Iteration 12/25 | Loss: 0.00088739
Iteration 13/25 | Loss: 0.00088739
Iteration 14/25 | Loss: 0.00088739
Iteration 15/25 | Loss: 0.00088739
Iteration 16/25 | Loss: 0.00088739
Iteration 17/25 | Loss: 0.00088739
Iteration 18/25 | Loss: 0.00088739
Iteration 19/25 | Loss: 0.00088739
Iteration 20/25 | Loss: 0.00088739
Iteration 21/25 | Loss: 0.00088739
Iteration 22/25 | Loss: 0.00088739
Iteration 23/25 | Loss: 0.00088739
Iteration 24/25 | Loss: 0.00088739
Iteration 25/25 | Loss: 0.00088739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50729990
Iteration 2/25 | Loss: 0.00064788
Iteration 3/25 | Loss: 0.00064784
Iteration 4/25 | Loss: 0.00064784
Iteration 5/25 | Loss: 0.00064784
Iteration 6/25 | Loss: 0.00064784
Iteration 7/25 | Loss: 0.00064784
Iteration 8/25 | Loss: 0.00064784
Iteration 9/25 | Loss: 0.00064784
Iteration 10/25 | Loss: 0.00064784
Iteration 11/25 | Loss: 0.00064784
Iteration 12/25 | Loss: 0.00064784
Iteration 13/25 | Loss: 0.00064784
Iteration 14/25 | Loss: 0.00064784
Iteration 15/25 | Loss: 0.00064784
Iteration 16/25 | Loss: 0.00064784
Iteration 17/25 | Loss: 0.00064784
Iteration 18/25 | Loss: 0.00064784
Iteration 19/25 | Loss: 0.00064784
Iteration 20/25 | Loss: 0.00064784
Iteration 21/25 | Loss: 0.00064784
Iteration 22/25 | Loss: 0.00064784
Iteration 23/25 | Loss: 0.00064784
Iteration 24/25 | Loss: 0.00064784
Iteration 25/25 | Loss: 0.00064784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006478397990576923, 0.0006478397990576923, 0.0006478397990576923, 0.0006478397990576923, 0.0006478397990576923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006478397990576923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064784
Iteration 2/1000 | Loss: 0.00002924
Iteration 3/1000 | Loss: 0.00002254
Iteration 4/1000 | Loss: 0.00001995
Iteration 5/1000 | Loss: 0.00001902
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001811
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001774
Iteration 10/1000 | Loss: 0.00001765
Iteration 11/1000 | Loss: 0.00001745
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001724
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001716
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001706
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001706
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001704
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001703
Iteration 62/1000 | Loss: 0.00001703
Iteration 63/1000 | Loss: 0.00001703
Iteration 64/1000 | Loss: 0.00001703
Iteration 65/1000 | Loss: 0.00001703
Iteration 66/1000 | Loss: 0.00001703
Iteration 67/1000 | Loss: 0.00001703
Iteration 68/1000 | Loss: 0.00001703
Iteration 69/1000 | Loss: 0.00001703
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001702
Iteration 84/1000 | Loss: 0.00001702
Iteration 85/1000 | Loss: 0.00001702
Iteration 86/1000 | Loss: 0.00001702
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001702
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001701
Iteration 105/1000 | Loss: 0.00001701
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001701
Iteration 108/1000 | Loss: 0.00001701
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001700
Iteration 115/1000 | Loss: 0.00001700
Iteration 116/1000 | Loss: 0.00001700
Iteration 117/1000 | Loss: 0.00001700
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001699
Iteration 124/1000 | Loss: 0.00001699
Iteration 125/1000 | Loss: 0.00001699
Iteration 126/1000 | Loss: 0.00001699
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001698
Iteration 137/1000 | Loss: 0.00001698
Iteration 138/1000 | Loss: 0.00001698
Iteration 139/1000 | Loss: 0.00001697
Iteration 140/1000 | Loss: 0.00001697
Iteration 141/1000 | Loss: 0.00001697
Iteration 142/1000 | Loss: 0.00001697
Iteration 143/1000 | Loss: 0.00001697
Iteration 144/1000 | Loss: 0.00001697
Iteration 145/1000 | Loss: 0.00001697
Iteration 146/1000 | Loss: 0.00001697
Iteration 147/1000 | Loss: 0.00001697
Iteration 148/1000 | Loss: 0.00001697
Iteration 149/1000 | Loss: 0.00001697
Iteration 150/1000 | Loss: 0.00001696
Iteration 151/1000 | Loss: 0.00001696
Iteration 152/1000 | Loss: 0.00001696
Iteration 153/1000 | Loss: 0.00001696
Iteration 154/1000 | Loss: 0.00001696
Iteration 155/1000 | Loss: 0.00001696
Iteration 156/1000 | Loss: 0.00001696
Iteration 157/1000 | Loss: 0.00001696
Iteration 158/1000 | Loss: 0.00001696
Iteration 159/1000 | Loss: 0.00001696
Iteration 160/1000 | Loss: 0.00001696
Iteration 161/1000 | Loss: 0.00001696
Iteration 162/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.6960191715043038e-05, 1.6960191715043038e-05, 1.6960191715043038e-05, 1.6960191715043038e-05, 1.6960191715043038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6960191715043038e-05

Optimization complete. Final v2v error: 3.4754927158355713 mm

Highest mean error: 3.5554637908935547 mm for frame 33

Lowest mean error: 3.235691547393799 mm for frame 0

Saving results

Total time: 36.081727266311646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839904
Iteration 2/25 | Loss: 0.00212543
Iteration 3/25 | Loss: 0.00155510
Iteration 4/25 | Loss: 0.00140839
Iteration 5/25 | Loss: 0.00129018
Iteration 6/25 | Loss: 0.00097886
Iteration 7/25 | Loss: 0.00095516
Iteration 8/25 | Loss: 0.00095357
Iteration 9/25 | Loss: 0.00095316
Iteration 10/25 | Loss: 0.00095314
Iteration 11/25 | Loss: 0.00095314
Iteration 12/25 | Loss: 0.00095314
Iteration 13/25 | Loss: 0.00095314
Iteration 14/25 | Loss: 0.00095314
Iteration 15/25 | Loss: 0.00095314
Iteration 16/25 | Loss: 0.00095314
Iteration 17/25 | Loss: 0.00095314
Iteration 18/25 | Loss: 0.00095314
Iteration 19/25 | Loss: 0.00095314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009531400282867253, 0.0009531400282867253, 0.0009531400282867253, 0.0009531400282867253, 0.0009531400282867253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009531400282867253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53074837
Iteration 2/25 | Loss: 0.00063060
Iteration 3/25 | Loss: 0.00063059
Iteration 4/25 | Loss: 0.00063059
Iteration 5/25 | Loss: 0.00063059
Iteration 6/25 | Loss: 0.00063059
Iteration 7/25 | Loss: 0.00063059
Iteration 8/25 | Loss: 0.00063059
Iteration 9/25 | Loss: 0.00063059
Iteration 10/25 | Loss: 0.00063059
Iteration 11/25 | Loss: 0.00063059
Iteration 12/25 | Loss: 0.00063059
Iteration 13/25 | Loss: 0.00063059
Iteration 14/25 | Loss: 0.00063059
Iteration 15/25 | Loss: 0.00063059
Iteration 16/25 | Loss: 0.00063059
Iteration 17/25 | Loss: 0.00063059
Iteration 18/25 | Loss: 0.00063059
Iteration 19/25 | Loss: 0.00063059
Iteration 20/25 | Loss: 0.00063059
Iteration 21/25 | Loss: 0.00063059
Iteration 22/25 | Loss: 0.00063059
Iteration 23/25 | Loss: 0.00063059
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006305909482762218, 0.0006305909482762218, 0.0006305909482762218, 0.0006305909482762218, 0.0006305909482762218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006305909482762218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063059
Iteration 2/1000 | Loss: 0.00003157
Iteration 3/1000 | Loss: 0.00002270
Iteration 4/1000 | Loss: 0.00002092
Iteration 5/1000 | Loss: 0.00002017
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001932
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001890
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001875
Iteration 12/1000 | Loss: 0.00001872
Iteration 13/1000 | Loss: 0.00001864
Iteration 14/1000 | Loss: 0.00001862
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001851
Iteration 17/1000 | Loss: 0.00001851
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001836
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001835
Iteration 22/1000 | Loss: 0.00001835
Iteration 23/1000 | Loss: 0.00001835
Iteration 24/1000 | Loss: 0.00001834
Iteration 25/1000 | Loss: 0.00001833
Iteration 26/1000 | Loss: 0.00001833
Iteration 27/1000 | Loss: 0.00001832
Iteration 28/1000 | Loss: 0.00001831
Iteration 29/1000 | Loss: 0.00001831
Iteration 30/1000 | Loss: 0.00001828
Iteration 31/1000 | Loss: 0.00001828
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001828
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001825
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001823
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001823
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001820
Iteration 49/1000 | Loss: 0.00001820
Iteration 50/1000 | Loss: 0.00001820
Iteration 51/1000 | Loss: 0.00001820
Iteration 52/1000 | Loss: 0.00001820
Iteration 53/1000 | Loss: 0.00001820
Iteration 54/1000 | Loss: 0.00001820
Iteration 55/1000 | Loss: 0.00001819
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001818
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001817
Iteration 65/1000 | Loss: 0.00001817
Iteration 66/1000 | Loss: 0.00001817
Iteration 67/1000 | Loss: 0.00001817
Iteration 68/1000 | Loss: 0.00001816
Iteration 69/1000 | Loss: 0.00001816
Iteration 70/1000 | Loss: 0.00001816
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00001816
Iteration 73/1000 | Loss: 0.00001815
Iteration 74/1000 | Loss: 0.00001815
Iteration 75/1000 | Loss: 0.00001815
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001815
Iteration 79/1000 | Loss: 0.00001815
Iteration 80/1000 | Loss: 0.00001815
Iteration 81/1000 | Loss: 0.00001815
Iteration 82/1000 | Loss: 0.00001814
Iteration 83/1000 | Loss: 0.00001814
Iteration 84/1000 | Loss: 0.00001814
Iteration 85/1000 | Loss: 0.00001814
Iteration 86/1000 | Loss: 0.00001814
Iteration 87/1000 | Loss: 0.00001814
Iteration 88/1000 | Loss: 0.00001814
Iteration 89/1000 | Loss: 0.00001814
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001814
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001814
Iteration 94/1000 | Loss: 0.00001814
Iteration 95/1000 | Loss: 0.00001814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.8143828128813766e-05, 1.8143828128813766e-05, 1.8143828128813766e-05, 1.8143828128813766e-05, 1.8143828128813766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8143828128813766e-05

Optimization complete. Final v2v error: 3.6121060848236084 mm

Highest mean error: 3.7994401454925537 mm for frame 86

Lowest mean error: 3.4216692447662354 mm for frame 10

Saving results

Total time: 42.139448404312134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999749
Iteration 2/25 | Loss: 0.00999749
Iteration 3/25 | Loss: 0.00999748
Iteration 4/25 | Loss: 0.00999748
Iteration 5/25 | Loss: 0.00999748
Iteration 6/25 | Loss: 0.00999748
Iteration 7/25 | Loss: 0.00999748
Iteration 8/25 | Loss: 0.00999747
Iteration 9/25 | Loss: 0.00999747
Iteration 10/25 | Loss: 0.00999747
Iteration 11/25 | Loss: 0.00999747
Iteration 12/25 | Loss: 0.00999747
Iteration 13/25 | Loss: 0.00232703
Iteration 14/25 | Loss: 0.00180922
Iteration 15/25 | Loss: 0.00174284
Iteration 16/25 | Loss: 0.00134711
Iteration 17/25 | Loss: 0.00122316
Iteration 18/25 | Loss: 0.00115258
Iteration 19/25 | Loss: 0.00109189
Iteration 20/25 | Loss: 0.00110031
Iteration 21/25 | Loss: 0.00107533
Iteration 22/25 | Loss: 0.00106501
Iteration 23/25 | Loss: 0.00103841
Iteration 24/25 | Loss: 0.00103447
Iteration 25/25 | Loss: 0.00109649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52435100
Iteration 2/25 | Loss: 0.00285163
Iteration 3/25 | Loss: 0.00273063
Iteration 4/25 | Loss: 0.00272895
Iteration 5/25 | Loss: 0.00272895
Iteration 6/25 | Loss: 0.00272895
Iteration 7/25 | Loss: 0.00272894
Iteration 8/25 | Loss: 0.00272894
Iteration 9/25 | Loss: 0.00272894
Iteration 10/25 | Loss: 0.00272894
Iteration 11/25 | Loss: 0.00272894
Iteration 12/25 | Loss: 0.00272894
Iteration 13/25 | Loss: 0.00272894
Iteration 14/25 | Loss: 0.00272894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002728943480178714, 0.002728943480178714, 0.002728943480178714, 0.002728943480178714, 0.002728943480178714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002728943480178714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272894
Iteration 2/1000 | Loss: 0.00372683
Iteration 3/1000 | Loss: 0.00503023
Iteration 4/1000 | Loss: 0.00187612
Iteration 5/1000 | Loss: 0.00164021
Iteration 6/1000 | Loss: 0.00179739
Iteration 7/1000 | Loss: 0.00163092
Iteration 8/1000 | Loss: 0.00082701
Iteration 9/1000 | Loss: 0.00200047
Iteration 10/1000 | Loss: 0.00047665
Iteration 11/1000 | Loss: 0.00107056
Iteration 12/1000 | Loss: 0.00090906
Iteration 13/1000 | Loss: 0.00135249
Iteration 14/1000 | Loss: 0.00090406
Iteration 15/1000 | Loss: 0.00120080
Iteration 16/1000 | Loss: 0.00139133
Iteration 17/1000 | Loss: 0.00159358
Iteration 18/1000 | Loss: 0.00154000
Iteration 19/1000 | Loss: 0.00121927
Iteration 20/1000 | Loss: 0.00171103
Iteration 21/1000 | Loss: 0.00073095
Iteration 22/1000 | Loss: 0.00137149
Iteration 23/1000 | Loss: 0.00072810
Iteration 24/1000 | Loss: 0.00054121
Iteration 25/1000 | Loss: 0.00061706
Iteration 26/1000 | Loss: 0.00038974
Iteration 27/1000 | Loss: 0.00138805
Iteration 28/1000 | Loss: 0.00063557
Iteration 29/1000 | Loss: 0.00005237
Iteration 30/1000 | Loss: 0.00070305
Iteration 31/1000 | Loss: 0.00179320
Iteration 32/1000 | Loss: 0.00064142
Iteration 33/1000 | Loss: 0.00099209
Iteration 34/1000 | Loss: 0.00062294
Iteration 35/1000 | Loss: 0.00014529
Iteration 36/1000 | Loss: 0.00070922
Iteration 37/1000 | Loss: 0.00130863
Iteration 38/1000 | Loss: 0.00053750
Iteration 39/1000 | Loss: 0.00071024
Iteration 40/1000 | Loss: 0.00141743
Iteration 41/1000 | Loss: 0.00153128
Iteration 42/1000 | Loss: 0.00088571
Iteration 43/1000 | Loss: 0.00095231
Iteration 44/1000 | Loss: 0.00075757
Iteration 45/1000 | Loss: 0.00106960
Iteration 46/1000 | Loss: 0.00106364
Iteration 47/1000 | Loss: 0.00086972
Iteration 48/1000 | Loss: 0.00067867
Iteration 49/1000 | Loss: 0.00098374
Iteration 50/1000 | Loss: 0.00071062
Iteration 51/1000 | Loss: 0.00123498
Iteration 52/1000 | Loss: 0.00044929
Iteration 53/1000 | Loss: 0.00008281
Iteration 54/1000 | Loss: 0.00039425
Iteration 55/1000 | Loss: 0.00079638
Iteration 56/1000 | Loss: 0.00106051
Iteration 57/1000 | Loss: 0.00004224
Iteration 58/1000 | Loss: 0.00003752
Iteration 59/1000 | Loss: 0.00011339
Iteration 60/1000 | Loss: 0.00003719
Iteration 61/1000 | Loss: 0.00016034
Iteration 62/1000 | Loss: 0.00083530
Iteration 63/1000 | Loss: 0.00011285
Iteration 64/1000 | Loss: 0.00004521
Iteration 65/1000 | Loss: 0.00003549
Iteration 66/1000 | Loss: 0.00002930
Iteration 67/1000 | Loss: 0.00002447
Iteration 68/1000 | Loss: 0.00002243
Iteration 69/1000 | Loss: 0.00002162
Iteration 70/1000 | Loss: 0.00002119
Iteration 71/1000 | Loss: 0.00002069
Iteration 72/1000 | Loss: 0.00002032
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00001987
Iteration 75/1000 | Loss: 0.00001982
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001979
Iteration 78/1000 | Loss: 0.00001978
Iteration 79/1000 | Loss: 0.00001977
Iteration 80/1000 | Loss: 0.00001976
Iteration 81/1000 | Loss: 0.00001976
Iteration 82/1000 | Loss: 0.00001968
Iteration 83/1000 | Loss: 0.00001968
Iteration 84/1000 | Loss: 0.00001967
Iteration 85/1000 | Loss: 0.00001966
Iteration 86/1000 | Loss: 0.00001966
Iteration 87/1000 | Loss: 0.00001966
Iteration 88/1000 | Loss: 0.00001966
Iteration 89/1000 | Loss: 0.00001966
Iteration 90/1000 | Loss: 0.00001966
Iteration 91/1000 | Loss: 0.00001966
Iteration 92/1000 | Loss: 0.00001966
Iteration 93/1000 | Loss: 0.00001966
Iteration 94/1000 | Loss: 0.00001966
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001961
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001959
Iteration 108/1000 | Loss: 0.00001959
Iteration 109/1000 | Loss: 0.00001959
Iteration 110/1000 | Loss: 0.00001959
Iteration 111/1000 | Loss: 0.00001958
Iteration 112/1000 | Loss: 0.00001958
Iteration 113/1000 | Loss: 0.00001957
Iteration 114/1000 | Loss: 0.00001957
Iteration 115/1000 | Loss: 0.00001957
Iteration 116/1000 | Loss: 0.00001957
Iteration 117/1000 | Loss: 0.00001957
Iteration 118/1000 | Loss: 0.00001957
Iteration 119/1000 | Loss: 0.00001957
Iteration 120/1000 | Loss: 0.00001957
Iteration 121/1000 | Loss: 0.00001957
Iteration 122/1000 | Loss: 0.00001957
Iteration 123/1000 | Loss: 0.00001957
Iteration 124/1000 | Loss: 0.00001957
Iteration 125/1000 | Loss: 0.00001956
Iteration 126/1000 | Loss: 0.00001956
Iteration 127/1000 | Loss: 0.00001956
Iteration 128/1000 | Loss: 0.00001956
Iteration 129/1000 | Loss: 0.00001956
Iteration 130/1000 | Loss: 0.00001956
Iteration 131/1000 | Loss: 0.00001955
Iteration 132/1000 | Loss: 0.00001955
Iteration 133/1000 | Loss: 0.00001955
Iteration 134/1000 | Loss: 0.00001955
Iteration 135/1000 | Loss: 0.00001954
Iteration 136/1000 | Loss: 0.00001954
Iteration 137/1000 | Loss: 0.00001954
Iteration 138/1000 | Loss: 0.00001954
Iteration 139/1000 | Loss: 0.00001954
Iteration 140/1000 | Loss: 0.00001954
Iteration 141/1000 | Loss: 0.00001953
Iteration 142/1000 | Loss: 0.00001953
Iteration 143/1000 | Loss: 0.00001953
Iteration 144/1000 | Loss: 0.00001953
Iteration 145/1000 | Loss: 0.00001953
Iteration 146/1000 | Loss: 0.00001953
Iteration 147/1000 | Loss: 0.00001953
Iteration 148/1000 | Loss: 0.00001952
Iteration 149/1000 | Loss: 0.00001952
Iteration 150/1000 | Loss: 0.00001952
Iteration 151/1000 | Loss: 0.00001952
Iteration 152/1000 | Loss: 0.00001952
Iteration 153/1000 | Loss: 0.00001952
Iteration 154/1000 | Loss: 0.00001951
Iteration 155/1000 | Loss: 0.00001951
Iteration 156/1000 | Loss: 0.00001951
Iteration 157/1000 | Loss: 0.00001951
Iteration 158/1000 | Loss: 0.00001951
Iteration 159/1000 | Loss: 0.00001951
Iteration 160/1000 | Loss: 0.00001951
Iteration 161/1000 | Loss: 0.00001951
Iteration 162/1000 | Loss: 0.00001951
Iteration 163/1000 | Loss: 0.00001951
Iteration 164/1000 | Loss: 0.00001951
Iteration 165/1000 | Loss: 0.00001951
Iteration 166/1000 | Loss: 0.00001951
Iteration 167/1000 | Loss: 0.00001951
Iteration 168/1000 | Loss: 0.00001951
Iteration 169/1000 | Loss: 0.00001951
Iteration 170/1000 | Loss: 0.00001951
Iteration 171/1000 | Loss: 0.00001951
Iteration 172/1000 | Loss: 0.00001951
Iteration 173/1000 | Loss: 0.00001951
Iteration 174/1000 | Loss: 0.00001951
Iteration 175/1000 | Loss: 0.00001951
Iteration 176/1000 | Loss: 0.00001951
Iteration 177/1000 | Loss: 0.00001951
Iteration 178/1000 | Loss: 0.00001951
Iteration 179/1000 | Loss: 0.00001951
Iteration 180/1000 | Loss: 0.00001951
Iteration 181/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.950775731529575e-05, 1.950775731529575e-05, 1.950775731529575e-05, 1.950775731529575e-05, 1.950775731529575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.950775731529575e-05

Optimization complete. Final v2v error: 3.5597128868103027 mm

Highest mean error: 11.015007019042969 mm for frame 7

Lowest mean error: 3.324108839035034 mm for frame 55

Saving results

Total time: 161.1716594696045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372834
Iteration 2/25 | Loss: 0.00090794
Iteration 3/25 | Loss: 0.00081915
Iteration 4/25 | Loss: 0.00080196
Iteration 5/25 | Loss: 0.00079561
Iteration 6/25 | Loss: 0.00079425
Iteration 7/25 | Loss: 0.00079400
Iteration 8/25 | Loss: 0.00079400
Iteration 9/25 | Loss: 0.00079400
Iteration 10/25 | Loss: 0.00079400
Iteration 11/25 | Loss: 0.00079400
Iteration 12/25 | Loss: 0.00079400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007939988863654435, 0.0007939988863654435, 0.0007939988863654435, 0.0007939988863654435, 0.0007939988863654435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007939988863654435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02169180
Iteration 2/25 | Loss: 0.00060091
Iteration 3/25 | Loss: 0.00060090
Iteration 4/25 | Loss: 0.00060090
Iteration 5/25 | Loss: 0.00060090
Iteration 6/25 | Loss: 0.00060090
Iteration 7/25 | Loss: 0.00060090
Iteration 8/25 | Loss: 0.00060090
Iteration 9/25 | Loss: 0.00060090
Iteration 10/25 | Loss: 0.00060090
Iteration 11/25 | Loss: 0.00060090
Iteration 12/25 | Loss: 0.00060090
Iteration 13/25 | Loss: 0.00060090
Iteration 14/25 | Loss: 0.00060090
Iteration 15/25 | Loss: 0.00060090
Iteration 16/25 | Loss: 0.00060090
Iteration 17/25 | Loss: 0.00060090
Iteration 18/25 | Loss: 0.00060090
Iteration 19/25 | Loss: 0.00060090
Iteration 20/25 | Loss: 0.00060090
Iteration 21/25 | Loss: 0.00060090
Iteration 22/25 | Loss: 0.00060090
Iteration 23/25 | Loss: 0.00060090
Iteration 24/25 | Loss: 0.00060090
Iteration 25/25 | Loss: 0.00060090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060090
Iteration 2/1000 | Loss: 0.00002620
Iteration 3/1000 | Loss: 0.00001590
Iteration 4/1000 | Loss: 0.00001474
Iteration 5/1000 | Loss: 0.00001389
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001304
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001277
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001271
Iteration 13/1000 | Loss: 0.00001271
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001249
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001239
Iteration 34/1000 | Loss: 0.00001239
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001237
Iteration 38/1000 | Loss: 0.00001237
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001236
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001233
Iteration 52/1000 | Loss: 0.00001233
Iteration 53/1000 | Loss: 0.00001233
Iteration 54/1000 | Loss: 0.00001233
Iteration 55/1000 | Loss: 0.00001233
Iteration 56/1000 | Loss: 0.00001233
Iteration 57/1000 | Loss: 0.00001233
Iteration 58/1000 | Loss: 0.00001233
Iteration 59/1000 | Loss: 0.00001233
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001232
Iteration 62/1000 | Loss: 0.00001232
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001232
Iteration 65/1000 | Loss: 0.00001232
Iteration 66/1000 | Loss: 0.00001231
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00001217
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001213
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001212
Iteration 101/1000 | Loss: 0.00001212
Iteration 102/1000 | Loss: 0.00001212
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001211
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001210
Iteration 116/1000 | Loss: 0.00001210
Iteration 117/1000 | Loss: 0.00001210
Iteration 118/1000 | Loss: 0.00001210
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001209
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001209
Iteration 124/1000 | Loss: 0.00001209
Iteration 125/1000 | Loss: 0.00001209
Iteration 126/1000 | Loss: 0.00001209
Iteration 127/1000 | Loss: 0.00001209
Iteration 128/1000 | Loss: 0.00001209
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001208
Iteration 136/1000 | Loss: 0.00001208
Iteration 137/1000 | Loss: 0.00001208
Iteration 138/1000 | Loss: 0.00001208
Iteration 139/1000 | Loss: 0.00001208
Iteration 140/1000 | Loss: 0.00001208
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001207
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001206
Iteration 149/1000 | Loss: 0.00001206
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001206
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001206
Iteration 157/1000 | Loss: 0.00001206
Iteration 158/1000 | Loss: 0.00001206
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001205
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001205
Iteration 164/1000 | Loss: 0.00001205
Iteration 165/1000 | Loss: 0.00001205
Iteration 166/1000 | Loss: 0.00001205
Iteration 167/1000 | Loss: 0.00001205
Iteration 168/1000 | Loss: 0.00001205
Iteration 169/1000 | Loss: 0.00001205
Iteration 170/1000 | Loss: 0.00001205
Iteration 171/1000 | Loss: 0.00001205
Iteration 172/1000 | Loss: 0.00001205
Iteration 173/1000 | Loss: 0.00001205
Iteration 174/1000 | Loss: 0.00001205
Iteration 175/1000 | Loss: 0.00001205
Iteration 176/1000 | Loss: 0.00001205
Iteration 177/1000 | Loss: 0.00001205
Iteration 178/1000 | Loss: 0.00001205
Iteration 179/1000 | Loss: 0.00001205
Iteration 180/1000 | Loss: 0.00001205
Iteration 181/1000 | Loss: 0.00001205
Iteration 182/1000 | Loss: 0.00001205
Iteration 183/1000 | Loss: 0.00001205
Iteration 184/1000 | Loss: 0.00001205
Iteration 185/1000 | Loss: 0.00001205
Iteration 186/1000 | Loss: 0.00001205
Iteration 187/1000 | Loss: 0.00001205
Iteration 188/1000 | Loss: 0.00001205
Iteration 189/1000 | Loss: 0.00001205
Iteration 190/1000 | Loss: 0.00001205
Iteration 191/1000 | Loss: 0.00001205
Iteration 192/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.2047816198901273e-05, 1.2047816198901273e-05, 1.2047816198901273e-05, 1.2047816198901273e-05, 1.2047816198901273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2047816198901273e-05

Optimization complete. Final v2v error: 2.9585554599761963 mm

Highest mean error: 3.349538803100586 mm for frame 82

Lowest mean error: 2.8540618419647217 mm for frame 26

Saving results

Total time: 38.14119791984558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484113
Iteration 2/25 | Loss: 0.00100250
Iteration 3/25 | Loss: 0.00090058
Iteration 4/25 | Loss: 0.00086615
Iteration 5/25 | Loss: 0.00085655
Iteration 6/25 | Loss: 0.00085450
Iteration 7/25 | Loss: 0.00085393
Iteration 8/25 | Loss: 0.00085393
Iteration 9/25 | Loss: 0.00085393
Iteration 10/25 | Loss: 0.00085393
Iteration 11/25 | Loss: 0.00085393
Iteration 12/25 | Loss: 0.00085393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008539344416931272, 0.0008539344416931272, 0.0008539344416931272, 0.0008539344416931272, 0.0008539344416931272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008539344416931272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65446520
Iteration 2/25 | Loss: 0.00060426
Iteration 3/25 | Loss: 0.00060426
Iteration 4/25 | Loss: 0.00060426
Iteration 5/25 | Loss: 0.00060426
Iteration 6/25 | Loss: 0.00060426
Iteration 7/25 | Loss: 0.00060426
Iteration 8/25 | Loss: 0.00060426
Iteration 9/25 | Loss: 0.00060426
Iteration 10/25 | Loss: 0.00060426
Iteration 11/25 | Loss: 0.00060426
Iteration 12/25 | Loss: 0.00060426
Iteration 13/25 | Loss: 0.00060426
Iteration 14/25 | Loss: 0.00060426
Iteration 15/25 | Loss: 0.00060426
Iteration 16/25 | Loss: 0.00060426
Iteration 17/25 | Loss: 0.00060426
Iteration 18/25 | Loss: 0.00060426
Iteration 19/25 | Loss: 0.00060426
Iteration 20/25 | Loss: 0.00060426
Iteration 21/25 | Loss: 0.00060426
Iteration 22/25 | Loss: 0.00060426
Iteration 23/25 | Loss: 0.00060426
Iteration 24/25 | Loss: 0.00060426
Iteration 25/25 | Loss: 0.00060426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060426
Iteration 2/1000 | Loss: 0.00003873
Iteration 3/1000 | Loss: 0.00002780
Iteration 4/1000 | Loss: 0.00002562
Iteration 5/1000 | Loss: 0.00002448
Iteration 6/1000 | Loss: 0.00002373
Iteration 7/1000 | Loss: 0.00002319
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00002250
Iteration 10/1000 | Loss: 0.00002223
Iteration 11/1000 | Loss: 0.00002223
Iteration 12/1000 | Loss: 0.00002210
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002202
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002200
Iteration 17/1000 | Loss: 0.00002195
Iteration 18/1000 | Loss: 0.00002187
Iteration 19/1000 | Loss: 0.00002187
Iteration 20/1000 | Loss: 0.00002185
Iteration 21/1000 | Loss: 0.00002185
Iteration 22/1000 | Loss: 0.00002183
Iteration 23/1000 | Loss: 0.00002183
Iteration 24/1000 | Loss: 0.00002182
Iteration 25/1000 | Loss: 0.00002182
Iteration 26/1000 | Loss: 0.00002182
Iteration 27/1000 | Loss: 0.00002181
Iteration 28/1000 | Loss: 0.00002181
Iteration 29/1000 | Loss: 0.00002181
Iteration 30/1000 | Loss: 0.00002181
Iteration 31/1000 | Loss: 0.00002181
Iteration 32/1000 | Loss: 0.00002181
Iteration 33/1000 | Loss: 0.00002181
Iteration 34/1000 | Loss: 0.00002181
Iteration 35/1000 | Loss: 0.00002181
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002180
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002180
Iteration 43/1000 | Loss: 0.00002180
Iteration 44/1000 | Loss: 0.00002180
Iteration 45/1000 | Loss: 0.00002179
Iteration 46/1000 | Loss: 0.00002178
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002177
Iteration 49/1000 | Loss: 0.00002176
Iteration 50/1000 | Loss: 0.00002176
Iteration 51/1000 | Loss: 0.00002175
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002174
Iteration 55/1000 | Loss: 0.00002174
Iteration 56/1000 | Loss: 0.00002174
Iteration 57/1000 | Loss: 0.00002173
Iteration 58/1000 | Loss: 0.00002173
Iteration 59/1000 | Loss: 0.00002173
Iteration 60/1000 | Loss: 0.00002172
Iteration 61/1000 | Loss: 0.00002172
Iteration 62/1000 | Loss: 0.00002171
Iteration 63/1000 | Loss: 0.00002171
Iteration 64/1000 | Loss: 0.00002170
Iteration 65/1000 | Loss: 0.00002169
Iteration 66/1000 | Loss: 0.00002169
Iteration 67/1000 | Loss: 0.00002169
Iteration 68/1000 | Loss: 0.00002168
Iteration 69/1000 | Loss: 0.00002168
Iteration 70/1000 | Loss: 0.00002167
Iteration 71/1000 | Loss: 0.00002167
Iteration 72/1000 | Loss: 0.00002166
Iteration 73/1000 | Loss: 0.00002165
Iteration 74/1000 | Loss: 0.00002165
Iteration 75/1000 | Loss: 0.00002165
Iteration 76/1000 | Loss: 0.00002164
Iteration 77/1000 | Loss: 0.00002164
Iteration 78/1000 | Loss: 0.00002164
Iteration 79/1000 | Loss: 0.00002164
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002164
Iteration 85/1000 | Loss: 0.00002164
Iteration 86/1000 | Loss: 0.00002164
Iteration 87/1000 | Loss: 0.00002164
Iteration 88/1000 | Loss: 0.00002164
Iteration 89/1000 | Loss: 0.00002163
Iteration 90/1000 | Loss: 0.00002163
Iteration 91/1000 | Loss: 0.00002162
Iteration 92/1000 | Loss: 0.00002161
Iteration 93/1000 | Loss: 0.00002161
Iteration 94/1000 | Loss: 0.00002161
Iteration 95/1000 | Loss: 0.00002161
Iteration 96/1000 | Loss: 0.00002161
Iteration 97/1000 | Loss: 0.00002161
Iteration 98/1000 | Loss: 0.00002161
Iteration 99/1000 | Loss: 0.00002161
Iteration 100/1000 | Loss: 0.00002160
Iteration 101/1000 | Loss: 0.00002160
Iteration 102/1000 | Loss: 0.00002160
Iteration 103/1000 | Loss: 0.00002160
Iteration 104/1000 | Loss: 0.00002160
Iteration 105/1000 | Loss: 0.00002160
Iteration 106/1000 | Loss: 0.00002160
Iteration 107/1000 | Loss: 0.00002160
Iteration 108/1000 | Loss: 0.00002159
Iteration 109/1000 | Loss: 0.00002159
Iteration 110/1000 | Loss: 0.00002159
Iteration 111/1000 | Loss: 0.00002159
Iteration 112/1000 | Loss: 0.00002159
Iteration 113/1000 | Loss: 0.00002158
Iteration 114/1000 | Loss: 0.00002158
Iteration 115/1000 | Loss: 0.00002158
Iteration 116/1000 | Loss: 0.00002158
Iteration 117/1000 | Loss: 0.00002158
Iteration 118/1000 | Loss: 0.00002158
Iteration 119/1000 | Loss: 0.00002158
Iteration 120/1000 | Loss: 0.00002158
Iteration 121/1000 | Loss: 0.00002158
Iteration 122/1000 | Loss: 0.00002157
Iteration 123/1000 | Loss: 0.00002157
Iteration 124/1000 | Loss: 0.00002157
Iteration 125/1000 | Loss: 0.00002157
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002156
Iteration 128/1000 | Loss: 0.00002156
Iteration 129/1000 | Loss: 0.00002156
Iteration 130/1000 | Loss: 0.00002156
Iteration 131/1000 | Loss: 0.00002156
Iteration 132/1000 | Loss: 0.00002155
Iteration 133/1000 | Loss: 0.00002155
Iteration 134/1000 | Loss: 0.00002155
Iteration 135/1000 | Loss: 0.00002155
Iteration 136/1000 | Loss: 0.00002155
Iteration 137/1000 | Loss: 0.00002155
Iteration 138/1000 | Loss: 0.00002155
Iteration 139/1000 | Loss: 0.00002155
Iteration 140/1000 | Loss: 0.00002155
Iteration 141/1000 | Loss: 0.00002155
Iteration 142/1000 | Loss: 0.00002155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.1553545593633316e-05, 2.1553545593633316e-05, 2.1553545593633316e-05, 2.1553545593633316e-05, 2.1553545593633316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1553545593633316e-05

Optimization complete. Final v2v error: 3.8794591426849365 mm

Highest mean error: 4.315974712371826 mm for frame 20

Lowest mean error: 3.330256462097168 mm for frame 45

Saving results

Total time: 35.37209129333496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00288654
Iteration 2/25 | Loss: 0.00105272
Iteration 3/25 | Loss: 0.00086999
Iteration 4/25 | Loss: 0.00083475
Iteration 5/25 | Loss: 0.00082440
Iteration 6/25 | Loss: 0.00082204
Iteration 7/25 | Loss: 0.00082121
Iteration 8/25 | Loss: 0.00082101
Iteration 9/25 | Loss: 0.00082101
Iteration 10/25 | Loss: 0.00082101
Iteration 11/25 | Loss: 0.00082101
Iteration 12/25 | Loss: 0.00082101
Iteration 13/25 | Loss: 0.00082101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008210069499909878, 0.0008210069499909878, 0.0008210069499909878, 0.0008210069499909878, 0.0008210069499909878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008210069499909878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52898145
Iteration 2/25 | Loss: 0.00064978
Iteration 3/25 | Loss: 0.00064978
Iteration 4/25 | Loss: 0.00064978
Iteration 5/25 | Loss: 0.00064978
Iteration 6/25 | Loss: 0.00064978
Iteration 7/25 | Loss: 0.00064978
Iteration 8/25 | Loss: 0.00064978
Iteration 9/25 | Loss: 0.00064978
Iteration 10/25 | Loss: 0.00064978
Iteration 11/25 | Loss: 0.00064978
Iteration 12/25 | Loss: 0.00064978
Iteration 13/25 | Loss: 0.00064978
Iteration 14/25 | Loss: 0.00064978
Iteration 15/25 | Loss: 0.00064978
Iteration 16/25 | Loss: 0.00064978
Iteration 17/25 | Loss: 0.00064978
Iteration 18/25 | Loss: 0.00064978
Iteration 19/25 | Loss: 0.00064978
Iteration 20/25 | Loss: 0.00064978
Iteration 21/25 | Loss: 0.00064978
Iteration 22/25 | Loss: 0.00064978
Iteration 23/25 | Loss: 0.00064978
Iteration 24/25 | Loss: 0.00064978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006497760186903179, 0.0006497760186903179, 0.0006497760186903179, 0.0006497760186903179, 0.0006497760186903179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006497760186903179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064978
Iteration 2/1000 | Loss: 0.00003739
Iteration 3/1000 | Loss: 0.00002392
Iteration 4/1000 | Loss: 0.00002080
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00001857
Iteration 7/1000 | Loss: 0.00001809
Iteration 8/1000 | Loss: 0.00001760
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001702
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001657
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001637
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001631
Iteration 21/1000 | Loss: 0.00001631
Iteration 22/1000 | Loss: 0.00001630
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001619
Iteration 34/1000 | Loss: 0.00001619
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001618
Iteration 37/1000 | Loss: 0.00001618
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00001617
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001615
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001614
Iteration 48/1000 | Loss: 0.00001614
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001613
Iteration 51/1000 | Loss: 0.00001613
Iteration 52/1000 | Loss: 0.00001613
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001612
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001611
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001611
Iteration 67/1000 | Loss: 0.00001611
Iteration 68/1000 | Loss: 0.00001611
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001609
Iteration 74/1000 | Loss: 0.00001609
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001609
Iteration 78/1000 | Loss: 0.00001609
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001608
Iteration 83/1000 | Loss: 0.00001608
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001607
Iteration 87/1000 | Loss: 0.00001607
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001606
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.605018951522652e-05, 1.605018951522652e-05, 1.605018951522652e-05, 1.605018951522652e-05, 1.605018951522652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.605018951522652e-05

Optimization complete. Final v2v error: 3.412576913833618 mm

Highest mean error: 3.5549662113189697 mm for frame 16

Lowest mean error: 3.3134255409240723 mm for frame 27

Saving results

Total time: 41.14882206916809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066565
Iteration 2/25 | Loss: 0.00239474
Iteration 3/25 | Loss: 0.00200751
Iteration 4/25 | Loss: 0.00187436
Iteration 5/25 | Loss: 0.00178143
Iteration 6/25 | Loss: 0.00182894
Iteration 7/25 | Loss: 0.00197323
Iteration 8/25 | Loss: 0.00191028
Iteration 9/25 | Loss: 0.00172179
Iteration 10/25 | Loss: 0.00157813
Iteration 11/25 | Loss: 0.00149995
Iteration 12/25 | Loss: 0.00142888
Iteration 13/25 | Loss: 0.00138026
Iteration 14/25 | Loss: 0.00135799
Iteration 15/25 | Loss: 0.00131257
Iteration 16/25 | Loss: 0.00130730
Iteration 17/25 | Loss: 0.00126910
Iteration 18/25 | Loss: 0.00123489
Iteration 19/25 | Loss: 0.00122350
Iteration 20/25 | Loss: 0.00122516
Iteration 21/25 | Loss: 0.00121093
Iteration 22/25 | Loss: 0.00120640
Iteration 23/25 | Loss: 0.00119445
Iteration 24/25 | Loss: 0.00118502
Iteration 25/25 | Loss: 0.00119874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31051552
Iteration 2/25 | Loss: 0.00410238
Iteration 3/25 | Loss: 0.00263585
Iteration 4/25 | Loss: 0.00263582
Iteration 5/25 | Loss: 0.00263582
Iteration 6/25 | Loss: 0.00263582
Iteration 7/25 | Loss: 0.00263582
Iteration 8/25 | Loss: 0.00263582
Iteration 9/25 | Loss: 0.00263582
Iteration 10/25 | Loss: 0.00263582
Iteration 11/25 | Loss: 0.00263582
Iteration 12/25 | Loss: 0.00263582
Iteration 13/25 | Loss: 0.00263582
Iteration 14/25 | Loss: 0.00263582
Iteration 15/25 | Loss: 0.00263582
Iteration 16/25 | Loss: 0.00263582
Iteration 17/25 | Loss: 0.00263582
Iteration 18/25 | Loss: 0.00263582
Iteration 19/25 | Loss: 0.00263582
Iteration 20/25 | Loss: 0.00263582
Iteration 21/25 | Loss: 0.00263582
Iteration 22/25 | Loss: 0.00263582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0026358170434832573, 0.0026358170434832573, 0.0026358170434832573, 0.0026358170434832573, 0.0026358170434832573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026358170434832573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263582
Iteration 2/1000 | Loss: 0.01537099
Iteration 3/1000 | Loss: 0.01648535
Iteration 4/1000 | Loss: 0.01735718
Iteration 5/1000 | Loss: 0.01243634
Iteration 6/1000 | Loss: 0.01619272
Iteration 7/1000 | Loss: 0.01234267
Iteration 8/1000 | Loss: 0.00622789
Iteration 9/1000 | Loss: 0.00668394
Iteration 10/1000 | Loss: 0.00589503
Iteration 11/1000 | Loss: 0.00550745
Iteration 12/1000 | Loss: 0.00599219
Iteration 13/1000 | Loss: 0.00407264
Iteration 14/1000 | Loss: 0.00401801
Iteration 15/1000 | Loss: 0.00094113
Iteration 16/1000 | Loss: 0.00195018
Iteration 17/1000 | Loss: 0.00191140
Iteration 18/1000 | Loss: 0.00228421
Iteration 19/1000 | Loss: 0.00179609
Iteration 20/1000 | Loss: 0.00227199
Iteration 21/1000 | Loss: 0.00107578
Iteration 22/1000 | Loss: 0.00147203
Iteration 23/1000 | Loss: 0.00068230
Iteration 24/1000 | Loss: 0.00259076
Iteration 25/1000 | Loss: 0.00137517
Iteration 26/1000 | Loss: 0.00094984
Iteration 27/1000 | Loss: 0.00143740
Iteration 28/1000 | Loss: 0.00116967
Iteration 29/1000 | Loss: 0.00092402
Iteration 30/1000 | Loss: 0.00081686
Iteration 31/1000 | Loss: 0.00063413
Iteration 32/1000 | Loss: 0.00205961
Iteration 33/1000 | Loss: 0.00106700
Iteration 34/1000 | Loss: 0.00151376
Iteration 35/1000 | Loss: 0.00182252
Iteration 36/1000 | Loss: 0.00220114
Iteration 37/1000 | Loss: 0.00091335
Iteration 38/1000 | Loss: 0.00175002
Iteration 39/1000 | Loss: 0.00174899
Iteration 40/1000 | Loss: 0.00604186
Iteration 41/1000 | Loss: 0.00574269
Iteration 42/1000 | Loss: 0.00434443
Iteration 43/1000 | Loss: 0.00297787
Iteration 44/1000 | Loss: 0.00170375
Iteration 45/1000 | Loss: 0.00208682
Iteration 46/1000 | Loss: 0.00391858
Iteration 47/1000 | Loss: 0.00255521
Iteration 48/1000 | Loss: 0.00208103
Iteration 49/1000 | Loss: 0.00162673
Iteration 50/1000 | Loss: 0.00168368
Iteration 51/1000 | Loss: 0.00184026
Iteration 52/1000 | Loss: 0.00067408
Iteration 53/1000 | Loss: 0.00130066
Iteration 54/1000 | Loss: 0.00135608
Iteration 55/1000 | Loss: 0.00140114
Iteration 56/1000 | Loss: 0.00413007
Iteration 57/1000 | Loss: 0.00282785
Iteration 58/1000 | Loss: 0.00319434
Iteration 59/1000 | Loss: 0.00203822
Iteration 60/1000 | Loss: 0.00254200
Iteration 61/1000 | Loss: 0.00183341
Iteration 62/1000 | Loss: 0.00119292
Iteration 63/1000 | Loss: 0.00099677
Iteration 64/1000 | Loss: 0.00221886
Iteration 65/1000 | Loss: 0.00132333
Iteration 66/1000 | Loss: 0.00067260
Iteration 67/1000 | Loss: 0.00102239
Iteration 68/1000 | Loss: 0.00148562
Iteration 69/1000 | Loss: 0.00102453
Iteration 70/1000 | Loss: 0.00122321
Iteration 71/1000 | Loss: 0.00098892
Iteration 72/1000 | Loss: 0.00137008
Iteration 73/1000 | Loss: 0.00076349
Iteration 74/1000 | Loss: 0.00068614
Iteration 75/1000 | Loss: 0.00109224
Iteration 76/1000 | Loss: 0.00130287
Iteration 77/1000 | Loss: 0.00101718
Iteration 78/1000 | Loss: 0.00156322
Iteration 79/1000 | Loss: 0.00061659
Iteration 80/1000 | Loss: 0.00107839
Iteration 81/1000 | Loss: 0.00110977
Iteration 82/1000 | Loss: 0.00162891
Iteration 83/1000 | Loss: 0.00047390
Iteration 84/1000 | Loss: 0.00067907
Iteration 85/1000 | Loss: 0.00094814
Iteration 86/1000 | Loss: 0.00205266
Iteration 87/1000 | Loss: 0.00072560
Iteration 88/1000 | Loss: 0.00088247
Iteration 89/1000 | Loss: 0.00038152
Iteration 90/1000 | Loss: 0.00036893
Iteration 91/1000 | Loss: 0.00043542
Iteration 92/1000 | Loss: 0.00028492
Iteration 93/1000 | Loss: 0.00028796
Iteration 94/1000 | Loss: 0.00018351
Iteration 95/1000 | Loss: 0.00079910
Iteration 96/1000 | Loss: 0.00019704
Iteration 97/1000 | Loss: 0.00020466
Iteration 98/1000 | Loss: 0.00222415
Iteration 99/1000 | Loss: 0.00019095
Iteration 100/1000 | Loss: 0.00022224
Iteration 101/1000 | Loss: 0.00023635
Iteration 102/1000 | Loss: 0.00040247
Iteration 103/1000 | Loss: 0.00020630
Iteration 104/1000 | Loss: 0.00020230
Iteration 105/1000 | Loss: 0.00021452
Iteration 106/1000 | Loss: 0.00137209
Iteration 107/1000 | Loss: 0.00068591
Iteration 108/1000 | Loss: 0.00095405
Iteration 109/1000 | Loss: 0.00137158
Iteration 110/1000 | Loss: 0.00057240
Iteration 111/1000 | Loss: 0.00072520
Iteration 112/1000 | Loss: 0.00082740
Iteration 113/1000 | Loss: 0.00086993
Iteration 114/1000 | Loss: 0.00041474
Iteration 115/1000 | Loss: 0.00071022
Iteration 116/1000 | Loss: 0.00063992
Iteration 117/1000 | Loss: 0.00068116
Iteration 118/1000 | Loss: 0.00033232
Iteration 119/1000 | Loss: 0.00032005
Iteration 120/1000 | Loss: 0.00027932
Iteration 121/1000 | Loss: 0.00018464
Iteration 122/1000 | Loss: 0.00015121
Iteration 123/1000 | Loss: 0.00025687
Iteration 124/1000 | Loss: 0.00222406
Iteration 125/1000 | Loss: 0.00025791
Iteration 126/1000 | Loss: 0.00016674
Iteration 127/1000 | Loss: 0.00262506
Iteration 128/1000 | Loss: 0.00254178
Iteration 129/1000 | Loss: 0.00124723
Iteration 130/1000 | Loss: 0.00037168
Iteration 131/1000 | Loss: 0.00011138
Iteration 132/1000 | Loss: 0.00113432
Iteration 133/1000 | Loss: 0.00010530
Iteration 134/1000 | Loss: 0.00011211
Iteration 135/1000 | Loss: 0.00025673
Iteration 136/1000 | Loss: 0.00021740
Iteration 137/1000 | Loss: 0.00017874
Iteration 138/1000 | Loss: 0.00012493
Iteration 139/1000 | Loss: 0.00038148
Iteration 140/1000 | Loss: 0.00011661
Iteration 141/1000 | Loss: 0.00024416
Iteration 142/1000 | Loss: 0.00027045
Iteration 143/1000 | Loss: 0.00023883
Iteration 144/1000 | Loss: 0.00012754
Iteration 145/1000 | Loss: 0.00008706
Iteration 146/1000 | Loss: 0.00142184
Iteration 147/1000 | Loss: 0.00090779
Iteration 148/1000 | Loss: 0.00125856
Iteration 149/1000 | Loss: 0.00101563
Iteration 150/1000 | Loss: 0.00112287
Iteration 151/1000 | Loss: 0.00172928
Iteration 152/1000 | Loss: 0.00025129
Iteration 153/1000 | Loss: 0.00012248
Iteration 154/1000 | Loss: 0.00010877
Iteration 155/1000 | Loss: 0.00011443
Iteration 156/1000 | Loss: 0.00010826
Iteration 157/1000 | Loss: 0.00040927
Iteration 158/1000 | Loss: 0.00013101
Iteration 159/1000 | Loss: 0.00036836
Iteration 160/1000 | Loss: 0.00036626
Iteration 161/1000 | Loss: 0.00073945
Iteration 162/1000 | Loss: 0.00057266
Iteration 163/1000 | Loss: 0.00094670
Iteration 164/1000 | Loss: 0.00022095
Iteration 165/1000 | Loss: 0.00026924
Iteration 166/1000 | Loss: 0.00067734
Iteration 167/1000 | Loss: 0.00027446
Iteration 168/1000 | Loss: 0.00016191
Iteration 169/1000 | Loss: 0.00011759
Iteration 170/1000 | Loss: 0.00011208
Iteration 171/1000 | Loss: 0.00018279
Iteration 172/1000 | Loss: 0.00019101
Iteration 173/1000 | Loss: 0.00026093
Iteration 174/1000 | Loss: 0.00010532
Iteration 175/1000 | Loss: 0.00011090
Iteration 176/1000 | Loss: 0.00010455
Iteration 177/1000 | Loss: 0.00009558
Iteration 178/1000 | Loss: 0.00009695
Iteration 179/1000 | Loss: 0.00007961
Iteration 180/1000 | Loss: 0.00024770
Iteration 181/1000 | Loss: 0.00182584
Iteration 182/1000 | Loss: 0.00148619
Iteration 183/1000 | Loss: 0.00143243
Iteration 184/1000 | Loss: 0.00025685
Iteration 185/1000 | Loss: 0.00043079
Iteration 186/1000 | Loss: 0.00043144
Iteration 187/1000 | Loss: 0.00013610
Iteration 188/1000 | Loss: 0.00006930
Iteration 189/1000 | Loss: 0.00007328
Iteration 190/1000 | Loss: 0.00007592
Iteration 191/1000 | Loss: 0.00007682
Iteration 192/1000 | Loss: 0.00024817
Iteration 193/1000 | Loss: 0.00016292
Iteration 194/1000 | Loss: 0.00013872
Iteration 195/1000 | Loss: 0.00008980
Iteration 196/1000 | Loss: 0.00023010
Iteration 197/1000 | Loss: 0.00040240
Iteration 198/1000 | Loss: 0.00021461
Iteration 199/1000 | Loss: 0.00019549
Iteration 200/1000 | Loss: 0.00019524
Iteration 201/1000 | Loss: 0.00020062
Iteration 202/1000 | Loss: 0.00037574
Iteration 203/1000 | Loss: 0.00024705
Iteration 204/1000 | Loss: 0.00029404
Iteration 205/1000 | Loss: 0.00016945
Iteration 206/1000 | Loss: 0.00041866
Iteration 207/1000 | Loss: 0.00025074
Iteration 208/1000 | Loss: 0.00022283
Iteration 209/1000 | Loss: 0.00008189
Iteration 210/1000 | Loss: 0.00006830
Iteration 211/1000 | Loss: 0.00037436
Iteration 212/1000 | Loss: 0.00030627
Iteration 213/1000 | Loss: 0.00005748
Iteration 214/1000 | Loss: 0.00005929
Iteration 215/1000 | Loss: 0.00025912
Iteration 216/1000 | Loss: 0.00007510
Iteration 217/1000 | Loss: 0.00005493
Iteration 218/1000 | Loss: 0.00017894
Iteration 219/1000 | Loss: 0.00004838
Iteration 220/1000 | Loss: 0.00004202
Iteration 221/1000 | Loss: 0.00003897
Iteration 222/1000 | Loss: 0.00020547
Iteration 223/1000 | Loss: 0.00031308
Iteration 224/1000 | Loss: 0.00016354
Iteration 225/1000 | Loss: 0.00017734
Iteration 226/1000 | Loss: 0.00044129
Iteration 227/1000 | Loss: 0.00019301
Iteration 228/1000 | Loss: 0.00004210
Iteration 229/1000 | Loss: 0.00003894
Iteration 230/1000 | Loss: 0.00003555
Iteration 231/1000 | Loss: 0.00003334
Iteration 232/1000 | Loss: 0.00003126
Iteration 233/1000 | Loss: 0.00003052
Iteration 234/1000 | Loss: 0.00002984
Iteration 235/1000 | Loss: 0.00002932
Iteration 236/1000 | Loss: 0.00015503
Iteration 237/1000 | Loss: 0.00003618
Iteration 238/1000 | Loss: 0.00004561
Iteration 239/1000 | Loss: 0.00003275
Iteration 240/1000 | Loss: 0.00003126
Iteration 241/1000 | Loss: 0.00033733
Iteration 242/1000 | Loss: 0.00008892
Iteration 243/1000 | Loss: 0.00008709
Iteration 244/1000 | Loss: 0.00006206
Iteration 245/1000 | Loss: 0.00004995
Iteration 246/1000 | Loss: 0.00003238
Iteration 247/1000 | Loss: 0.00018589
Iteration 248/1000 | Loss: 0.00003592
Iteration 249/1000 | Loss: 0.00005159
Iteration 250/1000 | Loss: 0.00003447
Iteration 251/1000 | Loss: 0.00004476
Iteration 252/1000 | Loss: 0.00003297
Iteration 253/1000 | Loss: 0.00003232
Iteration 254/1000 | Loss: 0.00003095
Iteration 255/1000 | Loss: 0.00002892
Iteration 256/1000 | Loss: 0.00003473
Iteration 257/1000 | Loss: 0.00003063
Iteration 258/1000 | Loss: 0.00002743
Iteration 259/1000 | Loss: 0.00015754
Iteration 260/1000 | Loss: 0.00021506
Iteration 261/1000 | Loss: 0.00024834
Iteration 262/1000 | Loss: 0.00004194
Iteration 263/1000 | Loss: 0.00004078
Iteration 264/1000 | Loss: 0.00003110
Iteration 265/1000 | Loss: 0.00020754
Iteration 266/1000 | Loss: 0.00003264
Iteration 267/1000 | Loss: 0.00003841
Iteration 268/1000 | Loss: 0.00003552
Iteration 269/1000 | Loss: 0.00009014
Iteration 270/1000 | Loss: 0.00003073
Iteration 271/1000 | Loss: 0.00002789
Iteration 272/1000 | Loss: 0.00002787
Iteration 273/1000 | Loss: 0.00002800
Iteration 274/1000 | Loss: 0.00002691
Iteration 275/1000 | Loss: 0.00002693
Iteration 276/1000 | Loss: 0.00005282
Iteration 277/1000 | Loss: 0.00004131
Iteration 278/1000 | Loss: 0.00002549
Iteration 279/1000 | Loss: 0.00004301
Iteration 280/1000 | Loss: 0.00003989
Iteration 281/1000 | Loss: 0.00004480
Iteration 282/1000 | Loss: 0.00003652
Iteration 283/1000 | Loss: 0.00022315
Iteration 284/1000 | Loss: 0.00014854
Iteration 285/1000 | Loss: 0.00013106
Iteration 286/1000 | Loss: 0.00003409
Iteration 287/1000 | Loss: 0.00002770
Iteration 288/1000 | Loss: 0.00002616
Iteration 289/1000 | Loss: 0.00002567
Iteration 290/1000 | Loss: 0.00012816
Iteration 291/1000 | Loss: 0.00003201
Iteration 292/1000 | Loss: 0.00004380
Iteration 293/1000 | Loss: 0.00003652
Iteration 294/1000 | Loss: 0.00002879
Iteration 295/1000 | Loss: 0.00003479
Iteration 296/1000 | Loss: 0.00013597
Iteration 297/1000 | Loss: 0.00014174
Iteration 298/1000 | Loss: 0.00004088
Iteration 299/1000 | Loss: 0.00007966
Iteration 300/1000 | Loss: 0.00013500
Iteration 301/1000 | Loss: 0.00009147
Iteration 302/1000 | Loss: 0.00003800
Iteration 303/1000 | Loss: 0.00005082
Iteration 304/1000 | Loss: 0.00018463
Iteration 305/1000 | Loss: 0.00020144
Iteration 306/1000 | Loss: 0.00025562
Iteration 307/1000 | Loss: 0.00012498
Iteration 308/1000 | Loss: 0.00018870
Iteration 309/1000 | Loss: 0.00025259
Iteration 310/1000 | Loss: 0.00009573
Iteration 311/1000 | Loss: 0.00018541
Iteration 312/1000 | Loss: 0.00021024
Iteration 313/1000 | Loss: 0.00023985
Iteration 314/1000 | Loss: 0.00012143
Iteration 315/1000 | Loss: 0.00023331
Iteration 316/1000 | Loss: 0.00020970
Iteration 317/1000 | Loss: 0.00003452
Iteration 318/1000 | Loss: 0.00005650
Iteration 319/1000 | Loss: 0.00004124
Iteration 320/1000 | Loss: 0.00002752
Iteration 321/1000 | Loss: 0.00002641
Iteration 322/1000 | Loss: 0.00002609
Iteration 323/1000 | Loss: 0.00002602
Iteration 324/1000 | Loss: 0.00003255
Iteration 325/1000 | Loss: 0.00002908
Iteration 326/1000 | Loss: 0.00003224
Iteration 327/1000 | Loss: 0.00008869
Iteration 328/1000 | Loss: 0.00003818
Iteration 329/1000 | Loss: 0.00004468
Iteration 330/1000 | Loss: 0.00003701
Iteration 331/1000 | Loss: 0.00002857
Iteration 332/1000 | Loss: 0.00003643
Iteration 333/1000 | Loss: 0.00002894
Iteration 334/1000 | Loss: 0.00002729
Iteration 335/1000 | Loss: 0.00003488
Iteration 336/1000 | Loss: 0.00005790
Iteration 337/1000 | Loss: 0.00003758
Iteration 338/1000 | Loss: 0.00004992
Iteration 339/1000 | Loss: 0.00004732
Iteration 340/1000 | Loss: 0.00003835
Iteration 341/1000 | Loss: 0.00005533
Iteration 342/1000 | Loss: 0.00004763
Iteration 343/1000 | Loss: 0.00003875
Iteration 344/1000 | Loss: 0.00002787
Iteration 345/1000 | Loss: 0.00003844
Iteration 346/1000 | Loss: 0.00003437
Iteration 347/1000 | Loss: 0.00003489
Iteration 348/1000 | Loss: 0.00003518
Iteration 349/1000 | Loss: 0.00003676
Iteration 350/1000 | Loss: 0.00003478
Iteration 351/1000 | Loss: 0.00002970
Iteration 352/1000 | Loss: 0.00002826
Iteration 353/1000 | Loss: 0.00003216
Iteration 354/1000 | Loss: 0.00004718
Iteration 355/1000 | Loss: 0.00003601
Iteration 356/1000 | Loss: 0.00017102
Iteration 357/1000 | Loss: 0.00016740
Iteration 358/1000 | Loss: 0.00008342
Iteration 359/1000 | Loss: 0.00004557
Iteration 360/1000 | Loss: 0.00003714
Iteration 361/1000 | Loss: 0.00020901
Iteration 362/1000 | Loss: 0.00019975
Iteration 363/1000 | Loss: 0.00013161
Iteration 364/1000 | Loss: 0.00009262
Iteration 365/1000 | Loss: 0.00021134
Iteration 366/1000 | Loss: 0.00012619
Iteration 367/1000 | Loss: 0.00003435
Iteration 368/1000 | Loss: 0.00013803
Iteration 369/1000 | Loss: 0.00004924
Iteration 370/1000 | Loss: 0.00002841
Iteration 371/1000 | Loss: 0.00002673
Iteration 372/1000 | Loss: 0.00002576
Iteration 373/1000 | Loss: 0.00002471
Iteration 374/1000 | Loss: 0.00002433
Iteration 375/1000 | Loss: 0.00002408
Iteration 376/1000 | Loss: 0.00002389
Iteration 377/1000 | Loss: 0.00002389
Iteration 378/1000 | Loss: 0.00002388
Iteration 379/1000 | Loss: 0.00002387
Iteration 380/1000 | Loss: 0.00002373
Iteration 381/1000 | Loss: 0.00002372
Iteration 382/1000 | Loss: 0.00002372
Iteration 383/1000 | Loss: 0.00002372
Iteration 384/1000 | Loss: 0.00002371
Iteration 385/1000 | Loss: 0.00002371
Iteration 386/1000 | Loss: 0.00002369
Iteration 387/1000 | Loss: 0.00002369
Iteration 388/1000 | Loss: 0.00002368
Iteration 389/1000 | Loss: 0.00002368
Iteration 390/1000 | Loss: 0.00002368
Iteration 391/1000 | Loss: 0.00002368
Iteration 392/1000 | Loss: 0.00002368
Iteration 393/1000 | Loss: 0.00002367
Iteration 394/1000 | Loss: 0.00002367
Iteration 395/1000 | Loss: 0.00002367
Iteration 396/1000 | Loss: 0.00002367
Iteration 397/1000 | Loss: 0.00002367
Iteration 398/1000 | Loss: 0.00002367
Iteration 399/1000 | Loss: 0.00002367
Iteration 400/1000 | Loss: 0.00002367
Iteration 401/1000 | Loss: 0.00002366
Iteration 402/1000 | Loss: 0.00002366
Iteration 403/1000 | Loss: 0.00002365
Iteration 404/1000 | Loss: 0.00002365
Iteration 405/1000 | Loss: 0.00002364
Iteration 406/1000 | Loss: 0.00002364
Iteration 407/1000 | Loss: 0.00002363
Iteration 408/1000 | Loss: 0.00002363
Iteration 409/1000 | Loss: 0.00002363
Iteration 410/1000 | Loss: 0.00002362
Iteration 411/1000 | Loss: 0.00002361
Iteration 412/1000 | Loss: 0.00002352
Iteration 413/1000 | Loss: 0.00002351
Iteration 414/1000 | Loss: 0.00002348
Iteration 415/1000 | Loss: 0.00002346
Iteration 416/1000 | Loss: 0.00002344
Iteration 417/1000 | Loss: 0.00002344
Iteration 418/1000 | Loss: 0.00002344
Iteration 419/1000 | Loss: 0.00002342
Iteration 420/1000 | Loss: 0.00002342
Iteration 421/1000 | Loss: 0.00002341
Iteration 422/1000 | Loss: 0.00002341
Iteration 423/1000 | Loss: 0.00002341
Iteration 424/1000 | Loss: 0.00002340
Iteration 425/1000 | Loss: 0.00002340
Iteration 426/1000 | Loss: 0.00002340
Iteration 427/1000 | Loss: 0.00002340
Iteration 428/1000 | Loss: 0.00002339
Iteration 429/1000 | Loss: 0.00002339
Iteration 430/1000 | Loss: 0.00002338
Iteration 431/1000 | Loss: 0.00002338
Iteration 432/1000 | Loss: 0.00002338
Iteration 433/1000 | Loss: 0.00002337
Iteration 434/1000 | Loss: 0.00002337
Iteration 435/1000 | Loss: 0.00002336
Iteration 436/1000 | Loss: 0.00002336
Iteration 437/1000 | Loss: 0.00002335
Iteration 438/1000 | Loss: 0.00002335
Iteration 439/1000 | Loss: 0.00002335
Iteration 440/1000 | Loss: 0.00002334
Iteration 441/1000 | Loss: 0.00002332
Iteration 442/1000 | Loss: 0.00002332
Iteration 443/1000 | Loss: 0.00002332
Iteration 444/1000 | Loss: 0.00002332
Iteration 445/1000 | Loss: 0.00002332
Iteration 446/1000 | Loss: 0.00002332
Iteration 447/1000 | Loss: 0.00002332
Iteration 448/1000 | Loss: 0.00002332
Iteration 449/1000 | Loss: 0.00002331
Iteration 450/1000 | Loss: 0.00002331
Iteration 451/1000 | Loss: 0.00002331
Iteration 452/1000 | Loss: 0.00002331
Iteration 453/1000 | Loss: 0.00002331
Iteration 454/1000 | Loss: 0.00002330
Iteration 455/1000 | Loss: 0.00002330
Iteration 456/1000 | Loss: 0.00002330
Iteration 457/1000 | Loss: 0.00002330
Iteration 458/1000 | Loss: 0.00002330
Iteration 459/1000 | Loss: 0.00002329
Iteration 460/1000 | Loss: 0.00002329
Iteration 461/1000 | Loss: 0.00002329
Iteration 462/1000 | Loss: 0.00002329
Iteration 463/1000 | Loss: 0.00002328
Iteration 464/1000 | Loss: 0.00002328
Iteration 465/1000 | Loss: 0.00002328
Iteration 466/1000 | Loss: 0.00002328
Iteration 467/1000 | Loss: 0.00002328
Iteration 468/1000 | Loss: 0.00002328
Iteration 469/1000 | Loss: 0.00002328
Iteration 470/1000 | Loss: 0.00002327
Iteration 471/1000 | Loss: 0.00002327
Iteration 472/1000 | Loss: 0.00002327
Iteration 473/1000 | Loss: 0.00002327
Iteration 474/1000 | Loss: 0.00002327
Iteration 475/1000 | Loss: 0.00002327
Iteration 476/1000 | Loss: 0.00002327
Iteration 477/1000 | Loss: 0.00002327
Iteration 478/1000 | Loss: 0.00002327
Iteration 479/1000 | Loss: 0.00002327
Iteration 480/1000 | Loss: 0.00002327
Iteration 481/1000 | Loss: 0.00002327
Iteration 482/1000 | Loss: 0.00002327
Iteration 483/1000 | Loss: 0.00002327
Iteration 484/1000 | Loss: 0.00002327
Iteration 485/1000 | Loss: 0.00002327
Iteration 486/1000 | Loss: 0.00002327
Iteration 487/1000 | Loss: 0.00002327
Iteration 488/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 488. Stopping optimization.
Last 5 losses: [2.3267635697266087e-05, 2.3267635697266087e-05, 2.3267635697266087e-05, 2.3267635697266087e-05, 2.3267635697266087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3267635697266087e-05

Optimization complete. Final v2v error: 4.023850440979004 mm

Highest mean error: 6.910457134246826 mm for frame 192

Lowest mean error: 3.4294004440307617 mm for frame 171

Saving results

Total time: 675.6029090881348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804042
Iteration 2/25 | Loss: 0.00121524
Iteration 3/25 | Loss: 0.00088112
Iteration 4/25 | Loss: 0.00084154
Iteration 5/25 | Loss: 0.00083463
Iteration 6/25 | Loss: 0.00083302
Iteration 7/25 | Loss: 0.00083281
Iteration 8/25 | Loss: 0.00083281
Iteration 9/25 | Loss: 0.00083281
Iteration 10/25 | Loss: 0.00083281
Iteration 11/25 | Loss: 0.00083281
Iteration 12/25 | Loss: 0.00083281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008328062831424177, 0.0008328062831424177, 0.0008328062831424177, 0.0008328062831424177, 0.0008328062831424177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008328062831424177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50340629
Iteration 2/25 | Loss: 0.00057241
Iteration 3/25 | Loss: 0.00057241
Iteration 4/25 | Loss: 0.00057241
Iteration 5/25 | Loss: 0.00057241
Iteration 6/25 | Loss: 0.00057241
Iteration 7/25 | Loss: 0.00057241
Iteration 8/25 | Loss: 0.00057240
Iteration 9/25 | Loss: 0.00057240
Iteration 10/25 | Loss: 0.00057240
Iteration 11/25 | Loss: 0.00057240
Iteration 12/25 | Loss: 0.00057240
Iteration 13/25 | Loss: 0.00057240
Iteration 14/25 | Loss: 0.00057240
Iteration 15/25 | Loss: 0.00057240
Iteration 16/25 | Loss: 0.00057240
Iteration 17/25 | Loss: 0.00057240
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005724041839130223, 0.0005724041839130223, 0.0005724041839130223, 0.0005724041839130223, 0.0005724041839130223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005724041839130223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057240
Iteration 2/1000 | Loss: 0.00002452
Iteration 3/1000 | Loss: 0.00001877
Iteration 4/1000 | Loss: 0.00001726
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001479
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001397
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001358
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001339
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001336
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001335
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001333
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001332
Iteration 33/1000 | Loss: 0.00001332
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001329
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001326
Iteration 56/1000 | Loss: 0.00001326
Iteration 57/1000 | Loss: 0.00001326
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001325
Iteration 65/1000 | Loss: 0.00001325
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001325
Iteration 74/1000 | Loss: 0.00001325
Iteration 75/1000 | Loss: 0.00001325
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001324
Iteration 78/1000 | Loss: 0.00001324
Iteration 79/1000 | Loss: 0.00001324
Iteration 80/1000 | Loss: 0.00001324
Iteration 81/1000 | Loss: 0.00001324
Iteration 82/1000 | Loss: 0.00001324
Iteration 83/1000 | Loss: 0.00001324
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001323
Iteration 87/1000 | Loss: 0.00001323
Iteration 88/1000 | Loss: 0.00001323
Iteration 89/1000 | Loss: 0.00001322
Iteration 90/1000 | Loss: 0.00001322
Iteration 91/1000 | Loss: 0.00001322
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001321
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001321
Iteration 96/1000 | Loss: 0.00001321
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001320
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001319
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001319
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001316
Iteration 144/1000 | Loss: 0.00001315
Iteration 145/1000 | Loss: 0.00001315
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001315
Iteration 150/1000 | Loss: 0.00001315
Iteration 151/1000 | Loss: 0.00001315
Iteration 152/1000 | Loss: 0.00001315
Iteration 153/1000 | Loss: 0.00001315
Iteration 154/1000 | Loss: 0.00001315
Iteration 155/1000 | Loss: 0.00001315
Iteration 156/1000 | Loss: 0.00001315
Iteration 157/1000 | Loss: 0.00001315
Iteration 158/1000 | Loss: 0.00001315
Iteration 159/1000 | Loss: 0.00001315
Iteration 160/1000 | Loss: 0.00001315
Iteration 161/1000 | Loss: 0.00001315
Iteration 162/1000 | Loss: 0.00001315
Iteration 163/1000 | Loss: 0.00001315
Iteration 164/1000 | Loss: 0.00001315
Iteration 165/1000 | Loss: 0.00001315
Iteration 166/1000 | Loss: 0.00001315
Iteration 167/1000 | Loss: 0.00001315
Iteration 168/1000 | Loss: 0.00001315
Iteration 169/1000 | Loss: 0.00001315
Iteration 170/1000 | Loss: 0.00001315
Iteration 171/1000 | Loss: 0.00001315
Iteration 172/1000 | Loss: 0.00001315
Iteration 173/1000 | Loss: 0.00001315
Iteration 174/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.3148885045666248e-05, 1.3148885045666248e-05, 1.3148885045666248e-05, 1.3148885045666248e-05, 1.3148885045666248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3148885045666248e-05

Optimization complete. Final v2v error: 3.047161340713501 mm

Highest mean error: 3.3582186698913574 mm for frame 122

Lowest mean error: 2.7224831581115723 mm for frame 55

Saving results

Total time: 37.193419218063354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435708
Iteration 2/25 | Loss: 0.00098954
Iteration 3/25 | Loss: 0.00086203
Iteration 4/25 | Loss: 0.00084511
Iteration 5/25 | Loss: 0.00084001
Iteration 6/25 | Loss: 0.00083869
Iteration 7/25 | Loss: 0.00083848
Iteration 8/25 | Loss: 0.00083849
Iteration 9/25 | Loss: 0.00083849
Iteration 10/25 | Loss: 0.00083848
Iteration 11/25 | Loss: 0.00083849
Iteration 12/25 | Loss: 0.00083849
Iteration 13/25 | Loss: 0.00083848
Iteration 14/25 | Loss: 0.00083848
Iteration 15/25 | Loss: 0.00083848
Iteration 16/25 | Loss: 0.00083849
Iteration 17/25 | Loss: 0.00083849
Iteration 18/25 | Loss: 0.00083849
Iteration 19/25 | Loss: 0.00083849
Iteration 20/25 | Loss: 0.00083848
Iteration 21/25 | Loss: 0.00083849
Iteration 22/25 | Loss: 0.00083849
Iteration 23/25 | Loss: 0.00083849
Iteration 24/25 | Loss: 0.00083849
Iteration 25/25 | Loss: 0.00083849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50240004
Iteration 2/25 | Loss: 0.00061960
Iteration 3/25 | Loss: 0.00061960
Iteration 4/25 | Loss: 0.00061960
Iteration 5/25 | Loss: 0.00061960
Iteration 6/25 | Loss: 0.00061960
Iteration 7/25 | Loss: 0.00061960
Iteration 8/25 | Loss: 0.00061960
Iteration 9/25 | Loss: 0.00061960
Iteration 10/25 | Loss: 0.00061960
Iteration 11/25 | Loss: 0.00061960
Iteration 12/25 | Loss: 0.00061960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006195973255671561, 0.0006195973255671561, 0.0006195973255671561, 0.0006195973255671561, 0.0006195973255671561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006195973255671561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061960
Iteration 2/1000 | Loss: 0.00002445
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001474
Iteration 8/1000 | Loss: 0.00001461
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001411
Iteration 11/1000 | Loss: 0.00001393
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001385
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001367
Iteration 40/1000 | Loss: 0.00001366
Iteration 41/1000 | Loss: 0.00001366
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001365
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001363
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001362
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001355
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001351
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001345
Iteration 79/1000 | Loss: 0.00001345
Iteration 80/1000 | Loss: 0.00001345
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00001345
Iteration 84/1000 | Loss: 0.00001345
Iteration 85/1000 | Loss: 0.00001345
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001344
Iteration 89/1000 | Loss: 0.00001344
Iteration 90/1000 | Loss: 0.00001344
Iteration 91/1000 | Loss: 0.00001344
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001343
Iteration 94/1000 | Loss: 0.00001343
Iteration 95/1000 | Loss: 0.00001343
Iteration 96/1000 | Loss: 0.00001343
Iteration 97/1000 | Loss: 0.00001343
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001342
Iteration 102/1000 | Loss: 0.00001342
Iteration 103/1000 | Loss: 0.00001342
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.3422763004200533e-05, 1.3422763004200533e-05, 1.3422763004200533e-05, 1.3422763004200533e-05, 1.3422763004200533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3422763004200533e-05

Optimization complete. Final v2v error: 3.0950539112091064 mm

Highest mean error: 3.3596386909484863 mm for frame 47

Lowest mean error: 2.805509328842163 mm for frame 10

Saving results

Total time: 38.470945596694946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384370
Iteration 2/25 | Loss: 0.00108897
Iteration 3/25 | Loss: 0.00086339
Iteration 4/25 | Loss: 0.00082575
Iteration 5/25 | Loss: 0.00081977
Iteration 6/25 | Loss: 0.00081751
Iteration 7/25 | Loss: 0.00081702
Iteration 8/25 | Loss: 0.00081702
Iteration 9/25 | Loss: 0.00081702
Iteration 10/25 | Loss: 0.00081702
Iteration 11/25 | Loss: 0.00081702
Iteration 12/25 | Loss: 0.00081702
Iteration 13/25 | Loss: 0.00081702
Iteration 14/25 | Loss: 0.00081702
Iteration 15/25 | Loss: 0.00081702
Iteration 16/25 | Loss: 0.00081702
Iteration 17/25 | Loss: 0.00081702
Iteration 18/25 | Loss: 0.00081702
Iteration 19/25 | Loss: 0.00081702
Iteration 20/25 | Loss: 0.00081702
Iteration 21/25 | Loss: 0.00081702
Iteration 22/25 | Loss: 0.00081702
Iteration 23/25 | Loss: 0.00081702
Iteration 24/25 | Loss: 0.00081702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008170153014361858, 0.0008170153014361858, 0.0008170153014361858, 0.0008170153014361858, 0.0008170153014361858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008170153014361858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60920835
Iteration 2/25 | Loss: 0.00052116
Iteration 3/25 | Loss: 0.00052116
Iteration 4/25 | Loss: 0.00052116
Iteration 5/25 | Loss: 0.00052116
Iteration 6/25 | Loss: 0.00052116
Iteration 7/25 | Loss: 0.00052116
Iteration 8/25 | Loss: 0.00052116
Iteration 9/25 | Loss: 0.00052115
Iteration 10/25 | Loss: 0.00052115
Iteration 11/25 | Loss: 0.00052115
Iteration 12/25 | Loss: 0.00052115
Iteration 13/25 | Loss: 0.00052115
Iteration 14/25 | Loss: 0.00052115
Iteration 15/25 | Loss: 0.00052115
Iteration 16/25 | Loss: 0.00052115
Iteration 17/25 | Loss: 0.00052115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005211547832004726, 0.0005211547832004726, 0.0005211547832004726, 0.0005211547832004726, 0.0005211547832004726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005211547832004726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052115
Iteration 2/1000 | Loss: 0.00003479
Iteration 3/1000 | Loss: 0.00002215
Iteration 4/1000 | Loss: 0.00002069
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001870
Iteration 8/1000 | Loss: 0.00001830
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001778
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001748
Iteration 15/1000 | Loss: 0.00001739
Iteration 16/1000 | Loss: 0.00001736
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001713
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001713
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001712
Iteration 32/1000 | Loss: 0.00001711
Iteration 33/1000 | Loss: 0.00001711
Iteration 34/1000 | Loss: 0.00001710
Iteration 35/1000 | Loss: 0.00001710
Iteration 36/1000 | Loss: 0.00001709
Iteration 37/1000 | Loss: 0.00001709
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001708
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001700
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001699
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001696
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001695
Iteration 98/1000 | Loss: 0.00001695
Iteration 99/1000 | Loss: 0.00001695
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.694712227617856e-05, 1.694712227617856e-05, 1.694712227617856e-05, 1.694712227617856e-05, 1.694712227617856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.694712227617856e-05

Optimization complete. Final v2v error: 3.484489679336548 mm

Highest mean error: 3.759420871734619 mm for frame 111

Lowest mean error: 3.201688766479492 mm for frame 14

Saving results

Total time: 37.27205300331116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854098
Iteration 2/25 | Loss: 0.00101433
Iteration 3/25 | Loss: 0.00086363
Iteration 4/25 | Loss: 0.00083562
Iteration 5/25 | Loss: 0.00083124
Iteration 6/25 | Loss: 0.00083010
Iteration 7/25 | Loss: 0.00083008
Iteration 8/25 | Loss: 0.00083008
Iteration 9/25 | Loss: 0.00083008
Iteration 10/25 | Loss: 0.00083008
Iteration 11/25 | Loss: 0.00083008
Iteration 12/25 | Loss: 0.00083008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008300848421640694, 0.0008300848421640694, 0.0008300848421640694, 0.0008300848421640694, 0.0008300848421640694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008300848421640694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45876801
Iteration 2/25 | Loss: 0.00050461
Iteration 3/25 | Loss: 0.00050453
Iteration 4/25 | Loss: 0.00050453
Iteration 5/25 | Loss: 0.00050453
Iteration 6/25 | Loss: 0.00050453
Iteration 7/25 | Loss: 0.00050453
Iteration 8/25 | Loss: 0.00050453
Iteration 9/25 | Loss: 0.00050452
Iteration 10/25 | Loss: 0.00050452
Iteration 11/25 | Loss: 0.00050452
Iteration 12/25 | Loss: 0.00050452
Iteration 13/25 | Loss: 0.00050452
Iteration 14/25 | Loss: 0.00050452
Iteration 15/25 | Loss: 0.00050452
Iteration 16/25 | Loss: 0.00050452
Iteration 17/25 | Loss: 0.00050452
Iteration 18/25 | Loss: 0.00050452
Iteration 19/25 | Loss: 0.00050452
Iteration 20/25 | Loss: 0.00050452
Iteration 21/25 | Loss: 0.00050452
Iteration 22/25 | Loss: 0.00050452
Iteration 23/25 | Loss: 0.00050452
Iteration 24/25 | Loss: 0.00050452
Iteration 25/25 | Loss: 0.00050452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050452
Iteration 2/1000 | Loss: 0.00003407
Iteration 3/1000 | Loss: 0.00002195
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001675
Iteration 6/1000 | Loss: 0.00001565
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001456
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001373
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001366
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001364
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001353
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001345
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001344
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001331
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001330
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001328
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001326
Iteration 154/1000 | Loss: 0.00001326
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001326
Iteration 158/1000 | Loss: 0.00001326
Iteration 159/1000 | Loss: 0.00001326
Iteration 160/1000 | Loss: 0.00001326
Iteration 161/1000 | Loss: 0.00001326
Iteration 162/1000 | Loss: 0.00001326
Iteration 163/1000 | Loss: 0.00001326
Iteration 164/1000 | Loss: 0.00001326
Iteration 165/1000 | Loss: 0.00001326
Iteration 166/1000 | Loss: 0.00001326
Iteration 167/1000 | Loss: 0.00001326
Iteration 168/1000 | Loss: 0.00001326
Iteration 169/1000 | Loss: 0.00001326
Iteration 170/1000 | Loss: 0.00001326
Iteration 171/1000 | Loss: 0.00001326
Iteration 172/1000 | Loss: 0.00001326
Iteration 173/1000 | Loss: 0.00001326
Iteration 174/1000 | Loss: 0.00001326
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.3255025805847254e-05, 1.3255025805847254e-05, 1.3255025805847254e-05, 1.3255025805847254e-05, 1.3255025805847254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3255025805847254e-05

Optimization complete. Final v2v error: 3.166823625564575 mm

Highest mean error: 3.5204014778137207 mm for frame 124

Lowest mean error: 2.984450578689575 mm for frame 100

Saving results

Total time: 38.24336624145508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434499
Iteration 2/25 | Loss: 0.00117727
Iteration 3/25 | Loss: 0.00088684
Iteration 4/25 | Loss: 0.00085297
Iteration 5/25 | Loss: 0.00084693
Iteration 6/25 | Loss: 0.00084465
Iteration 7/25 | Loss: 0.00084384
Iteration 8/25 | Loss: 0.00084372
Iteration 9/25 | Loss: 0.00084372
Iteration 10/25 | Loss: 0.00084372
Iteration 11/25 | Loss: 0.00084372
Iteration 12/25 | Loss: 0.00084372
Iteration 13/25 | Loss: 0.00084372
Iteration 14/25 | Loss: 0.00084372
Iteration 15/25 | Loss: 0.00084372
Iteration 16/25 | Loss: 0.00084372
Iteration 17/25 | Loss: 0.00084372
Iteration 18/25 | Loss: 0.00084372
Iteration 19/25 | Loss: 0.00084372
Iteration 20/25 | Loss: 0.00084372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008437206270173192, 0.0008437206270173192, 0.0008437206270173192, 0.0008437206270173192, 0.0008437206270173192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008437206270173192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.40473270
Iteration 2/25 | Loss: 0.00061968
Iteration 3/25 | Loss: 0.00061963
Iteration 4/25 | Loss: 0.00061963
Iteration 5/25 | Loss: 0.00061963
Iteration 6/25 | Loss: 0.00061963
Iteration 7/25 | Loss: 0.00061963
Iteration 8/25 | Loss: 0.00061963
Iteration 9/25 | Loss: 0.00061963
Iteration 10/25 | Loss: 0.00061962
Iteration 11/25 | Loss: 0.00061962
Iteration 12/25 | Loss: 0.00061962
Iteration 13/25 | Loss: 0.00061962
Iteration 14/25 | Loss: 0.00061962
Iteration 15/25 | Loss: 0.00061962
Iteration 16/25 | Loss: 0.00061962
Iteration 17/25 | Loss: 0.00061962
Iteration 18/25 | Loss: 0.00061962
Iteration 19/25 | Loss: 0.00061962
Iteration 20/25 | Loss: 0.00061962
Iteration 21/25 | Loss: 0.00061962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006196245667524636, 0.0006196245667524636, 0.0006196245667524636, 0.0006196245667524636, 0.0006196245667524636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006196245667524636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061962
Iteration 2/1000 | Loss: 0.00002366
Iteration 3/1000 | Loss: 0.00001712
Iteration 4/1000 | Loss: 0.00001587
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001463
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001393
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001385
Iteration 12/1000 | Loss: 0.00001384
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001371
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001355
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001352
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001345
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001341
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001341
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001340
Iteration 60/1000 | Loss: 0.00001340
Iteration 61/1000 | Loss: 0.00001340
Iteration 62/1000 | Loss: 0.00001340
Iteration 63/1000 | Loss: 0.00001340
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001339
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.3393513654591516e-05, 1.3393513654591516e-05, 1.3393513654591516e-05, 1.3393513654591516e-05, 1.3393513654591516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3393513654591516e-05

Optimization complete. Final v2v error: 3.128098487854004 mm

Highest mean error: 3.6949312686920166 mm for frame 71

Lowest mean error: 2.813309907913208 mm for frame 10

Saving results

Total time: 31.727611780166626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403475
Iteration 2/25 | Loss: 0.00090176
Iteration 3/25 | Loss: 0.00080990
Iteration 4/25 | Loss: 0.00079646
Iteration 5/25 | Loss: 0.00079390
Iteration 6/25 | Loss: 0.00079288
Iteration 7/25 | Loss: 0.00079281
Iteration 8/25 | Loss: 0.00079281
Iteration 9/25 | Loss: 0.00079281
Iteration 10/25 | Loss: 0.00079281
Iteration 11/25 | Loss: 0.00079281
Iteration 12/25 | Loss: 0.00079281
Iteration 13/25 | Loss: 0.00079281
Iteration 14/25 | Loss: 0.00079281
Iteration 15/25 | Loss: 0.00079281
Iteration 16/25 | Loss: 0.00079281
Iteration 17/25 | Loss: 0.00079281
Iteration 18/25 | Loss: 0.00079265
Iteration 19/25 | Loss: 0.00079265
Iteration 20/25 | Loss: 0.00079265
Iteration 21/25 | Loss: 0.00079265
Iteration 22/25 | Loss: 0.00079265
Iteration 23/25 | Loss: 0.00079265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000792649865616113, 0.000792649865616113, 0.000792649865616113, 0.000792649865616113, 0.000792649865616113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000792649865616113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46504545
Iteration 2/25 | Loss: 0.00057677
Iteration 3/25 | Loss: 0.00057677
Iteration 4/25 | Loss: 0.00057677
Iteration 5/25 | Loss: 0.00057677
Iteration 6/25 | Loss: 0.00057677
Iteration 7/25 | Loss: 0.00057677
Iteration 8/25 | Loss: 0.00057677
Iteration 9/25 | Loss: 0.00057677
Iteration 10/25 | Loss: 0.00057677
Iteration 11/25 | Loss: 0.00057677
Iteration 12/25 | Loss: 0.00057677
Iteration 13/25 | Loss: 0.00057677
Iteration 14/25 | Loss: 0.00057677
Iteration 15/25 | Loss: 0.00057677
Iteration 16/25 | Loss: 0.00057677
Iteration 17/25 | Loss: 0.00057677
Iteration 18/25 | Loss: 0.00057677
Iteration 19/25 | Loss: 0.00057677
Iteration 20/25 | Loss: 0.00057677
Iteration 21/25 | Loss: 0.00057677
Iteration 22/25 | Loss: 0.00057677
Iteration 23/25 | Loss: 0.00057677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005767664406448603, 0.0005767664406448603, 0.0005767664406448603, 0.0005767664406448603, 0.0005767664406448603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005767664406448603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057677
Iteration 2/1000 | Loss: 0.00002529
Iteration 3/1000 | Loss: 0.00001535
Iteration 4/1000 | Loss: 0.00001428
Iteration 5/1000 | Loss: 0.00001359
Iteration 6/1000 | Loss: 0.00001311
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001271
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001246
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001233
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001219
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001205
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001201
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001199
Iteration 78/1000 | Loss: 0.00001199
Iteration 79/1000 | Loss: 0.00001198
Iteration 80/1000 | Loss: 0.00001198
Iteration 81/1000 | Loss: 0.00001197
Iteration 82/1000 | Loss: 0.00001197
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001192
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001191
Iteration 102/1000 | Loss: 0.00001191
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001191
Iteration 105/1000 | Loss: 0.00001191
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001190
Iteration 114/1000 | Loss: 0.00001190
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001190
Iteration 117/1000 | Loss: 0.00001190
Iteration 118/1000 | Loss: 0.00001190
Iteration 119/1000 | Loss: 0.00001190
Iteration 120/1000 | Loss: 0.00001190
Iteration 121/1000 | Loss: 0.00001190
Iteration 122/1000 | Loss: 0.00001190
Iteration 123/1000 | Loss: 0.00001190
Iteration 124/1000 | Loss: 0.00001189
Iteration 125/1000 | Loss: 0.00001189
Iteration 126/1000 | Loss: 0.00001189
Iteration 127/1000 | Loss: 0.00001189
Iteration 128/1000 | Loss: 0.00001189
Iteration 129/1000 | Loss: 0.00001189
Iteration 130/1000 | Loss: 0.00001189
Iteration 131/1000 | Loss: 0.00001189
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001188
Iteration 135/1000 | Loss: 0.00001188
Iteration 136/1000 | Loss: 0.00001188
Iteration 137/1000 | Loss: 0.00001188
Iteration 138/1000 | Loss: 0.00001188
Iteration 139/1000 | Loss: 0.00001188
Iteration 140/1000 | Loss: 0.00001188
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001188
Iteration 149/1000 | Loss: 0.00001188
Iteration 150/1000 | Loss: 0.00001188
Iteration 151/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.188079477287829e-05, 1.188079477287829e-05, 1.188079477287829e-05, 1.188079477287829e-05, 1.188079477287829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.188079477287829e-05

Optimization complete. Final v2v error: 2.94477915763855 mm

Highest mean error: 3.0953338146209717 mm for frame 77

Lowest mean error: 2.849785566329956 mm for frame 110

Saving results

Total time: 34.85388517379761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089233
Iteration 2/25 | Loss: 0.01089233
Iteration 3/25 | Loss: 0.01089233
Iteration 4/25 | Loss: 0.01089232
Iteration 5/25 | Loss: 0.01089232
Iteration 6/25 | Loss: 0.01089232
Iteration 7/25 | Loss: 0.01089232
Iteration 8/25 | Loss: 0.01089232
Iteration 9/25 | Loss: 0.01089232
Iteration 10/25 | Loss: 0.01089231
Iteration 11/25 | Loss: 0.01089231
Iteration 12/25 | Loss: 0.01089231
Iteration 13/25 | Loss: 0.01089231
Iteration 14/25 | Loss: 0.01089231
Iteration 15/25 | Loss: 0.01089230
Iteration 16/25 | Loss: 0.01089230
Iteration 17/25 | Loss: 0.01089230
Iteration 18/25 | Loss: 0.01089230
Iteration 19/25 | Loss: 0.01089230
Iteration 20/25 | Loss: 0.01089230
Iteration 21/25 | Loss: 0.01089229
Iteration 22/25 | Loss: 0.01089229
Iteration 23/25 | Loss: 0.01089229
Iteration 24/25 | Loss: 0.01089229
Iteration 25/25 | Loss: 0.01089229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87346458
Iteration 2/25 | Loss: 0.07730018
Iteration 3/25 | Loss: 0.07723214
Iteration 4/25 | Loss: 0.07723214
Iteration 5/25 | Loss: 0.07723212
Iteration 6/25 | Loss: 0.07723212
Iteration 7/25 | Loss: 0.07723212
Iteration 8/25 | Loss: 0.07723212
Iteration 9/25 | Loss: 0.07723212
Iteration 10/25 | Loss: 0.07723212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.07723212242126465, 0.07723212242126465, 0.07723212242126465, 0.07723212242126465, 0.07723212242126465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07723212242126465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07723212
Iteration 2/1000 | Loss: 0.00303588
Iteration 3/1000 | Loss: 0.00105814
Iteration 4/1000 | Loss: 0.00082289
Iteration 5/1000 | Loss: 0.00032604
Iteration 6/1000 | Loss: 0.00013413
Iteration 7/1000 | Loss: 0.00011266
Iteration 8/1000 | Loss: 0.00030473
Iteration 9/1000 | Loss: 0.00005383
Iteration 10/1000 | Loss: 0.00004905
Iteration 11/1000 | Loss: 0.00012414
Iteration 12/1000 | Loss: 0.00003432
Iteration 13/1000 | Loss: 0.00012716
Iteration 14/1000 | Loss: 0.00003001
Iteration 15/1000 | Loss: 0.00002834
Iteration 16/1000 | Loss: 0.00038214
Iteration 17/1000 | Loss: 0.00005851
Iteration 18/1000 | Loss: 0.00004131
Iteration 19/1000 | Loss: 0.00002427
Iteration 20/1000 | Loss: 0.00039456
Iteration 21/1000 | Loss: 0.00003404
Iteration 22/1000 | Loss: 0.00002209
Iteration 23/1000 | Loss: 0.00002431
Iteration 24/1000 | Loss: 0.00002073
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00002558
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00002672
Iteration 29/1000 | Loss: 0.00005825
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00004645
Iteration 32/1000 | Loss: 0.00047105
Iteration 33/1000 | Loss: 0.00005427
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001714
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00002056
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001687
Iteration 41/1000 | Loss: 0.00001686
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00002598
Iteration 44/1000 | Loss: 0.00004362
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001674
Iteration 47/1000 | Loss: 0.00001673
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00001672
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001977
Iteration 56/1000 | Loss: 0.00001672
Iteration 57/1000 | Loss: 0.00001672
Iteration 58/1000 | Loss: 0.00001672
Iteration 59/1000 | Loss: 0.00001672
Iteration 60/1000 | Loss: 0.00001672
Iteration 61/1000 | Loss: 0.00001672
Iteration 62/1000 | Loss: 0.00001672
Iteration 63/1000 | Loss: 0.00001672
Iteration 64/1000 | Loss: 0.00001671
Iteration 65/1000 | Loss: 0.00001671
Iteration 66/1000 | Loss: 0.00001671
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001671
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001670
Iteration 73/1000 | Loss: 0.00001670
Iteration 74/1000 | Loss: 0.00001670
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00002380
Iteration 86/1000 | Loss: 0.00001666
Iteration 87/1000 | Loss: 0.00001666
Iteration 88/1000 | Loss: 0.00001666
Iteration 89/1000 | Loss: 0.00001666
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001666
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001661
Iteration 106/1000 | Loss: 0.00001661
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001661
Iteration 113/1000 | Loss: 0.00005469
Iteration 114/1000 | Loss: 0.00003206
Iteration 115/1000 | Loss: 0.00034642
Iteration 116/1000 | Loss: 0.00002663
Iteration 117/1000 | Loss: 0.00002756
Iteration 118/1000 | Loss: 0.00002857
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001662
Iteration 123/1000 | Loss: 0.00001662
Iteration 124/1000 | Loss: 0.00001662
Iteration 125/1000 | Loss: 0.00001661
Iteration 126/1000 | Loss: 0.00001661
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001660
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001660
Iteration 136/1000 | Loss: 0.00001660
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001659
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00003664
Iteration 143/1000 | Loss: 0.00001843
Iteration 144/1000 | Loss: 0.00002193
Iteration 145/1000 | Loss: 0.00001658
Iteration 146/1000 | Loss: 0.00001658
Iteration 147/1000 | Loss: 0.00001658
Iteration 148/1000 | Loss: 0.00001658
Iteration 149/1000 | Loss: 0.00001658
Iteration 150/1000 | Loss: 0.00001658
Iteration 151/1000 | Loss: 0.00001658
Iteration 152/1000 | Loss: 0.00001658
Iteration 153/1000 | Loss: 0.00001658
Iteration 154/1000 | Loss: 0.00001658
Iteration 155/1000 | Loss: 0.00001658
Iteration 156/1000 | Loss: 0.00001658
Iteration 157/1000 | Loss: 0.00001658
Iteration 158/1000 | Loss: 0.00001658
Iteration 159/1000 | Loss: 0.00001658
Iteration 160/1000 | Loss: 0.00001658
Iteration 161/1000 | Loss: 0.00001658
Iteration 162/1000 | Loss: 0.00001658
Iteration 163/1000 | Loss: 0.00001658
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001658
Iteration 166/1000 | Loss: 0.00001658
Iteration 167/1000 | Loss: 0.00001658
Iteration 168/1000 | Loss: 0.00001658
Iteration 169/1000 | Loss: 0.00001658
Iteration 170/1000 | Loss: 0.00001658
Iteration 171/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.6575606423430145e-05, 1.6575606423430145e-05, 1.6575606423430145e-05, 1.6575606423430145e-05, 1.6575606423430145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6575606423430145e-05

Optimization complete. Final v2v error: 3.44596004486084 mm

Highest mean error: 3.9735958576202393 mm for frame 8

Lowest mean error: 3.1315464973449707 mm for frame 12

Saving results

Total time: 81.54221868515015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836292
Iteration 2/25 | Loss: 0.00100689
Iteration 3/25 | Loss: 0.00089684
Iteration 4/25 | Loss: 0.00084675
Iteration 5/25 | Loss: 0.00083761
Iteration 6/25 | Loss: 0.00083533
Iteration 7/25 | Loss: 0.00083437
Iteration 8/25 | Loss: 0.00083437
Iteration 9/25 | Loss: 0.00083437
Iteration 10/25 | Loss: 0.00083437
Iteration 11/25 | Loss: 0.00083437
Iteration 12/25 | Loss: 0.00083437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008343670633621514, 0.0008343670633621514, 0.0008343670633621514, 0.0008343670633621514, 0.0008343670633621514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008343670633621514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.55707788
Iteration 2/25 | Loss: 0.00065447
Iteration 3/25 | Loss: 0.00065441
Iteration 4/25 | Loss: 0.00065441
Iteration 5/25 | Loss: 0.00065441
Iteration 6/25 | Loss: 0.00065441
Iteration 7/25 | Loss: 0.00065441
Iteration 8/25 | Loss: 0.00065441
Iteration 9/25 | Loss: 0.00065441
Iteration 10/25 | Loss: 0.00065441
Iteration 11/25 | Loss: 0.00065441
Iteration 12/25 | Loss: 0.00065441
Iteration 13/25 | Loss: 0.00065441
Iteration 14/25 | Loss: 0.00065441
Iteration 15/25 | Loss: 0.00065441
Iteration 16/25 | Loss: 0.00065441
Iteration 17/25 | Loss: 0.00065441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006544080097228289, 0.0006544080097228289, 0.0006544080097228289, 0.0006544080097228289, 0.0006544080097228289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006544080097228289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065441
Iteration 2/1000 | Loss: 0.00004004
Iteration 3/1000 | Loss: 0.00002407
Iteration 4/1000 | Loss: 0.00002147
Iteration 5/1000 | Loss: 0.00002033
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001871
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001804
Iteration 10/1000 | Loss: 0.00001777
Iteration 11/1000 | Loss: 0.00001759
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001737
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001729
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001716
Iteration 68/1000 | Loss: 0.00001716
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001715
Iteration 71/1000 | Loss: 0.00001715
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001715
Iteration 77/1000 | Loss: 0.00001715
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001713
Iteration 82/1000 | Loss: 0.00001713
Iteration 83/1000 | Loss: 0.00001713
Iteration 84/1000 | Loss: 0.00001713
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001713
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001712
Iteration 90/1000 | Loss: 0.00001712
Iteration 91/1000 | Loss: 0.00001712
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001711
Iteration 98/1000 | Loss: 0.00001710
Iteration 99/1000 | Loss: 0.00001710
Iteration 100/1000 | Loss: 0.00001710
Iteration 101/1000 | Loss: 0.00001710
Iteration 102/1000 | Loss: 0.00001710
Iteration 103/1000 | Loss: 0.00001710
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001709
Iteration 110/1000 | Loss: 0.00001709
Iteration 111/1000 | Loss: 0.00001709
Iteration 112/1000 | Loss: 0.00001709
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001708
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00001708
Iteration 126/1000 | Loss: 0.00001708
Iteration 127/1000 | Loss: 0.00001708
Iteration 128/1000 | Loss: 0.00001708
Iteration 129/1000 | Loss: 0.00001708
Iteration 130/1000 | Loss: 0.00001708
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001708
Iteration 135/1000 | Loss: 0.00001708
Iteration 136/1000 | Loss: 0.00001708
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.707802221062593e-05, 1.707802221062593e-05, 1.707802221062593e-05, 1.707802221062593e-05, 1.707802221062593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.707802221062593e-05

Optimization complete. Final v2v error: 3.5152108669281006 mm

Highest mean error: 3.9160423278808594 mm for frame 58

Lowest mean error: 3.152627468109131 mm for frame 201

Saving results

Total time: 42.421815156936646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602151
Iteration 2/25 | Loss: 0.00139683
Iteration 3/25 | Loss: 0.00098282
Iteration 4/25 | Loss: 0.00090879
Iteration 5/25 | Loss: 0.00089501
Iteration 6/25 | Loss: 0.00089138
Iteration 7/25 | Loss: 0.00089062
Iteration 8/25 | Loss: 0.00089061
Iteration 9/25 | Loss: 0.00089061
Iteration 10/25 | Loss: 0.00089061
Iteration 11/25 | Loss: 0.00089061
Iteration 12/25 | Loss: 0.00089061
Iteration 13/25 | Loss: 0.00089061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008906093426048756, 0.0008906093426048756, 0.0008906093426048756, 0.0008906093426048756, 0.0008906093426048756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008906093426048756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52920818
Iteration 2/25 | Loss: 0.00060270
Iteration 3/25 | Loss: 0.00060268
Iteration 4/25 | Loss: 0.00060268
Iteration 5/25 | Loss: 0.00060268
Iteration 6/25 | Loss: 0.00060268
Iteration 7/25 | Loss: 0.00060268
Iteration 8/25 | Loss: 0.00060268
Iteration 9/25 | Loss: 0.00060268
Iteration 10/25 | Loss: 0.00060268
Iteration 11/25 | Loss: 0.00060268
Iteration 12/25 | Loss: 0.00060268
Iteration 13/25 | Loss: 0.00060268
Iteration 14/25 | Loss: 0.00060268
Iteration 15/25 | Loss: 0.00060268
Iteration 16/25 | Loss: 0.00060268
Iteration 17/25 | Loss: 0.00060268
Iteration 18/25 | Loss: 0.00060268
Iteration 19/25 | Loss: 0.00060268
Iteration 20/25 | Loss: 0.00060268
Iteration 21/25 | Loss: 0.00060268
Iteration 22/25 | Loss: 0.00060268
Iteration 23/25 | Loss: 0.00060268
Iteration 24/25 | Loss: 0.00060268
Iteration 25/25 | Loss: 0.00060268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060268
Iteration 2/1000 | Loss: 0.00003073
Iteration 3/1000 | Loss: 0.00002445
Iteration 4/1000 | Loss: 0.00002273
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002069
Iteration 7/1000 | Loss: 0.00002005
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001940
Iteration 10/1000 | Loss: 0.00001923
Iteration 11/1000 | Loss: 0.00001909
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001892
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001883
Iteration 18/1000 | Loss: 0.00001876
Iteration 19/1000 | Loss: 0.00001876
Iteration 20/1000 | Loss: 0.00001874
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001873
Iteration 23/1000 | Loss: 0.00001873
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001872
Iteration 26/1000 | Loss: 0.00001872
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001869
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001868
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001867
Iteration 41/1000 | Loss: 0.00001867
Iteration 42/1000 | Loss: 0.00001867
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001866
Iteration 46/1000 | Loss: 0.00001866
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001865
Iteration 49/1000 | Loss: 0.00001865
Iteration 50/1000 | Loss: 0.00001865
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001863
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001863
Iteration 79/1000 | Loss: 0.00001863
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001862
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001861
Iteration 93/1000 | Loss: 0.00001861
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001861
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001861
Iteration 112/1000 | Loss: 0.00001861
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00001861
Iteration 115/1000 | Loss: 0.00001861
Iteration 116/1000 | Loss: 0.00001861
Iteration 117/1000 | Loss: 0.00001861
Iteration 118/1000 | Loss: 0.00001861
Iteration 119/1000 | Loss: 0.00001861
Iteration 120/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.861026612459682e-05, 1.861026612459682e-05, 1.861026612459682e-05, 1.861026612459682e-05, 1.861026612459682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.861026612459682e-05

Optimization complete. Final v2v error: 3.6651759147644043 mm

Highest mean error: 4.349620342254639 mm for frame 56

Lowest mean error: 3.3032076358795166 mm for frame 131

Saving results

Total time: 35.36741590499878
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795946
Iteration 2/25 | Loss: 0.00152252
Iteration 3/25 | Loss: 0.00107851
Iteration 4/25 | Loss: 0.00095442
Iteration 5/25 | Loss: 0.00089746
Iteration 6/25 | Loss: 0.00090294
Iteration 7/25 | Loss: 0.00089564
Iteration 8/25 | Loss: 0.00087924
Iteration 9/25 | Loss: 0.00085306
Iteration 10/25 | Loss: 0.00085112
Iteration 11/25 | Loss: 0.00084702
Iteration 12/25 | Loss: 0.00083752
Iteration 13/25 | Loss: 0.00084101
Iteration 14/25 | Loss: 0.00083562
Iteration 15/25 | Loss: 0.00083491
Iteration 16/25 | Loss: 0.00083460
Iteration 17/25 | Loss: 0.00083456
Iteration 18/25 | Loss: 0.00083456
Iteration 19/25 | Loss: 0.00083456
Iteration 20/25 | Loss: 0.00083456
Iteration 21/25 | Loss: 0.00083456
Iteration 22/25 | Loss: 0.00083456
Iteration 23/25 | Loss: 0.00083456
Iteration 24/25 | Loss: 0.00083456
Iteration 25/25 | Loss: 0.00083456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.96356916
Iteration 2/25 | Loss: 0.00071564
Iteration 3/25 | Loss: 0.00066939
Iteration 4/25 | Loss: 0.00066939
Iteration 5/25 | Loss: 0.00066939
Iteration 6/25 | Loss: 0.00066939
Iteration 7/25 | Loss: 0.00066939
Iteration 8/25 | Loss: 0.00066939
Iteration 9/25 | Loss: 0.00066939
Iteration 10/25 | Loss: 0.00066939
Iteration 11/25 | Loss: 0.00066939
Iteration 12/25 | Loss: 0.00066939
Iteration 13/25 | Loss: 0.00066939
Iteration 14/25 | Loss: 0.00066939
Iteration 15/25 | Loss: 0.00066939
Iteration 16/25 | Loss: 0.00066939
Iteration 17/25 | Loss: 0.00066939
Iteration 18/25 | Loss: 0.00066939
Iteration 19/25 | Loss: 0.00066939
Iteration 20/25 | Loss: 0.00066939
Iteration 21/25 | Loss: 0.00066939
Iteration 22/25 | Loss: 0.00066939
Iteration 23/25 | Loss: 0.00066939
Iteration 24/25 | Loss: 0.00066939
Iteration 25/25 | Loss: 0.00066939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066939
Iteration 2/1000 | Loss: 0.00003904
Iteration 3/1000 | Loss: 0.00002426
Iteration 4/1000 | Loss: 0.00002193
Iteration 5/1000 | Loss: 0.00002051
Iteration 6/1000 | Loss: 0.00001997
Iteration 7/1000 | Loss: 0.00001940
Iteration 8/1000 | Loss: 0.00001907
Iteration 9/1000 | Loss: 0.00001881
Iteration 10/1000 | Loss: 0.00124225
Iteration 11/1000 | Loss: 0.00010925
Iteration 12/1000 | Loss: 0.00001976
Iteration 13/1000 | Loss: 0.00003942
Iteration 14/1000 | Loss: 0.00004962
Iteration 15/1000 | Loss: 0.00002832
Iteration 16/1000 | Loss: 0.00001566
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001435
Iteration 19/1000 | Loss: 0.00012081
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00001869
Iteration 22/1000 | Loss: 0.00001389
Iteration 23/1000 | Loss: 0.00001380
Iteration 24/1000 | Loss: 0.00006392
Iteration 25/1000 | Loss: 0.00001393
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001335
Iteration 38/1000 | Loss: 0.00001335
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001334
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001334
Iteration 43/1000 | Loss: 0.00001334
Iteration 44/1000 | Loss: 0.00001334
Iteration 45/1000 | Loss: 0.00001334
Iteration 46/1000 | Loss: 0.00001334
Iteration 47/1000 | Loss: 0.00001334
Iteration 48/1000 | Loss: 0.00001334
Iteration 49/1000 | Loss: 0.00001334
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001329
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001323
Iteration 82/1000 | Loss: 0.00001322
Iteration 83/1000 | Loss: 0.00001322
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001321
Iteration 91/1000 | Loss: 0.00001320
Iteration 92/1000 | Loss: 0.00001320
Iteration 93/1000 | Loss: 0.00001320
Iteration 94/1000 | Loss: 0.00001320
Iteration 95/1000 | Loss: 0.00001320
Iteration 96/1000 | Loss: 0.00001320
Iteration 97/1000 | Loss: 0.00001320
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001319
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001318
Iteration 102/1000 | Loss: 0.00001318
Iteration 103/1000 | Loss: 0.00001318
Iteration 104/1000 | Loss: 0.00001318
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001317
Iteration 109/1000 | Loss: 0.00001317
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00008198
Iteration 113/1000 | Loss: 0.00001340
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001315
Iteration 117/1000 | Loss: 0.00001315
Iteration 118/1000 | Loss: 0.00001314
Iteration 119/1000 | Loss: 0.00001314
Iteration 120/1000 | Loss: 0.00001314
Iteration 121/1000 | Loss: 0.00001314
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001313
Iteration 124/1000 | Loss: 0.00001313
Iteration 125/1000 | Loss: 0.00001313
Iteration 126/1000 | Loss: 0.00001313
Iteration 127/1000 | Loss: 0.00001313
Iteration 128/1000 | Loss: 0.00001313
Iteration 129/1000 | Loss: 0.00004498
Iteration 130/1000 | Loss: 0.00001943
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00002665
Iteration 137/1000 | Loss: 0.00001319
Iteration 138/1000 | Loss: 0.00001319
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001318
Iteration 148/1000 | Loss: 0.00001318
Iteration 149/1000 | Loss: 0.00001318
Iteration 150/1000 | Loss: 0.00001317
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001317
Iteration 153/1000 | Loss: 0.00001316
Iteration 154/1000 | Loss: 0.00001316
Iteration 155/1000 | Loss: 0.00001316
Iteration 156/1000 | Loss: 0.00001316
Iteration 157/1000 | Loss: 0.00001316
Iteration 158/1000 | Loss: 0.00001316
Iteration 159/1000 | Loss: 0.00001316
Iteration 160/1000 | Loss: 0.00001316
Iteration 161/1000 | Loss: 0.00001316
Iteration 162/1000 | Loss: 0.00001316
Iteration 163/1000 | Loss: 0.00001315
Iteration 164/1000 | Loss: 0.00001315
Iteration 165/1000 | Loss: 0.00001315
Iteration 166/1000 | Loss: 0.00001315
Iteration 167/1000 | Loss: 0.00001315
Iteration 168/1000 | Loss: 0.00001315
Iteration 169/1000 | Loss: 0.00001315
Iteration 170/1000 | Loss: 0.00001315
Iteration 171/1000 | Loss: 0.00001315
Iteration 172/1000 | Loss: 0.00001315
Iteration 173/1000 | Loss: 0.00001315
Iteration 174/1000 | Loss: 0.00001315
Iteration 175/1000 | Loss: 0.00001315
Iteration 176/1000 | Loss: 0.00001315
Iteration 177/1000 | Loss: 0.00001315
Iteration 178/1000 | Loss: 0.00001315
Iteration 179/1000 | Loss: 0.00001315
Iteration 180/1000 | Loss: 0.00001315
Iteration 181/1000 | Loss: 0.00001315
Iteration 182/1000 | Loss: 0.00001315
Iteration 183/1000 | Loss: 0.00001315
Iteration 184/1000 | Loss: 0.00001315
Iteration 185/1000 | Loss: 0.00001315
Iteration 186/1000 | Loss: 0.00001315
Iteration 187/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.3152553037798498e-05, 1.3152553037798498e-05, 1.3152553037798498e-05, 1.3152553037798498e-05, 1.3152553037798498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3152553037798498e-05

Optimization complete. Final v2v error: 3.084099292755127 mm

Highest mean error: 4.19197940826416 mm for frame 49

Lowest mean error: 2.803438425064087 mm for frame 38

Saving results

Total time: 96.4139051437378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744980
Iteration 2/25 | Loss: 0.00176659
Iteration 3/25 | Loss: 0.00116352
Iteration 4/25 | Loss: 0.00101564
Iteration 5/25 | Loss: 0.00101105
Iteration 6/25 | Loss: 0.00093219
Iteration 7/25 | Loss: 0.00092492
Iteration 8/25 | Loss: 0.00090887
Iteration 9/25 | Loss: 0.00090302
Iteration 10/25 | Loss: 0.00090102
Iteration 11/25 | Loss: 0.00090048
Iteration 12/25 | Loss: 0.00090118
Iteration 13/25 | Loss: 0.00090009
Iteration 14/25 | Loss: 0.00089919
Iteration 15/25 | Loss: 0.00089792
Iteration 16/25 | Loss: 0.00089770
Iteration 17/25 | Loss: 0.00089768
Iteration 18/25 | Loss: 0.00089767
Iteration 19/25 | Loss: 0.00089767
Iteration 20/25 | Loss: 0.00089767
Iteration 21/25 | Loss: 0.00089767
Iteration 22/25 | Loss: 0.00089767
Iteration 23/25 | Loss: 0.00089767
Iteration 24/25 | Loss: 0.00089767
Iteration 25/25 | Loss: 0.00089767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52674556
Iteration 2/25 | Loss: 0.00073021
Iteration 3/25 | Loss: 0.00073021
Iteration 4/25 | Loss: 0.00073021
Iteration 5/25 | Loss: 0.00073021
Iteration 6/25 | Loss: 0.00073021
Iteration 7/25 | Loss: 0.00073021
Iteration 8/25 | Loss: 0.00073021
Iteration 9/25 | Loss: 0.00073021
Iteration 10/25 | Loss: 0.00073021
Iteration 11/25 | Loss: 0.00073021
Iteration 12/25 | Loss: 0.00073021
Iteration 13/25 | Loss: 0.00073021
Iteration 14/25 | Loss: 0.00073021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007302075391635299, 0.0007302075391635299, 0.0007302075391635299, 0.0007302075391635299, 0.0007302075391635299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007302075391635299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073021
Iteration 2/1000 | Loss: 0.00004379
Iteration 3/1000 | Loss: 0.00003135
Iteration 4/1000 | Loss: 0.00002718
Iteration 5/1000 | Loss: 0.00002553
Iteration 6/1000 | Loss: 0.00002431
Iteration 7/1000 | Loss: 0.00002364
Iteration 8/1000 | Loss: 0.00002292
Iteration 9/1000 | Loss: 0.00002262
Iteration 10/1000 | Loss: 0.00002222
Iteration 11/1000 | Loss: 0.00002196
Iteration 12/1000 | Loss: 0.00002171
Iteration 13/1000 | Loss: 0.00002157
Iteration 14/1000 | Loss: 0.00002146
Iteration 15/1000 | Loss: 0.00002144
Iteration 16/1000 | Loss: 0.00002142
Iteration 17/1000 | Loss: 0.00002142
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002142
Iteration 20/1000 | Loss: 0.00002142
Iteration 21/1000 | Loss: 0.00002142
Iteration 22/1000 | Loss: 0.00002141
Iteration 23/1000 | Loss: 0.00002141
Iteration 24/1000 | Loss: 0.00002141
Iteration 25/1000 | Loss: 0.00002141
Iteration 26/1000 | Loss: 0.00002141
Iteration 27/1000 | Loss: 0.00002141
Iteration 28/1000 | Loss: 0.00002141
Iteration 29/1000 | Loss: 0.00002140
Iteration 30/1000 | Loss: 0.00002139
Iteration 31/1000 | Loss: 0.00002139
Iteration 32/1000 | Loss: 0.00002139
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002139
Iteration 35/1000 | Loss: 0.00002139
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00002138
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002138
Iteration 44/1000 | Loss: 0.00002138
Iteration 45/1000 | Loss: 0.00002138
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00002138
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002137
Iteration 54/1000 | Loss: 0.00002137
Iteration 55/1000 | Loss: 0.00002137
Iteration 56/1000 | Loss: 0.00002136
Iteration 57/1000 | Loss: 0.00002136
Iteration 58/1000 | Loss: 0.00002136
Iteration 59/1000 | Loss: 0.00002136
Iteration 60/1000 | Loss: 0.00002136
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002135
Iteration 63/1000 | Loss: 0.00002135
Iteration 64/1000 | Loss: 0.00002135
Iteration 65/1000 | Loss: 0.00002134
Iteration 66/1000 | Loss: 0.00002134
Iteration 67/1000 | Loss: 0.00002134
Iteration 68/1000 | Loss: 0.00002133
Iteration 69/1000 | Loss: 0.00002133
Iteration 70/1000 | Loss: 0.00002133
Iteration 71/1000 | Loss: 0.00002133
Iteration 72/1000 | Loss: 0.00002133
Iteration 73/1000 | Loss: 0.00002132
Iteration 74/1000 | Loss: 0.00002132
Iteration 75/1000 | Loss: 0.00002132
Iteration 76/1000 | Loss: 0.00002132
Iteration 77/1000 | Loss: 0.00002132
Iteration 78/1000 | Loss: 0.00002132
Iteration 79/1000 | Loss: 0.00002131
Iteration 80/1000 | Loss: 0.00002131
Iteration 81/1000 | Loss: 0.00002131
Iteration 82/1000 | Loss: 0.00002131
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002131
Iteration 85/1000 | Loss: 0.00002131
Iteration 86/1000 | Loss: 0.00002131
Iteration 87/1000 | Loss: 0.00002130
Iteration 88/1000 | Loss: 0.00002130
Iteration 89/1000 | Loss: 0.00002130
Iteration 90/1000 | Loss: 0.00002130
Iteration 91/1000 | Loss: 0.00002130
Iteration 92/1000 | Loss: 0.00002130
Iteration 93/1000 | Loss: 0.00002130
Iteration 94/1000 | Loss: 0.00002130
Iteration 95/1000 | Loss: 0.00002130
Iteration 96/1000 | Loss: 0.00002130
Iteration 97/1000 | Loss: 0.00002130
Iteration 98/1000 | Loss: 0.00002129
Iteration 99/1000 | Loss: 0.00002129
Iteration 100/1000 | Loss: 0.00002129
Iteration 101/1000 | Loss: 0.00002129
Iteration 102/1000 | Loss: 0.00002129
Iteration 103/1000 | Loss: 0.00002129
Iteration 104/1000 | Loss: 0.00002129
Iteration 105/1000 | Loss: 0.00002129
Iteration 106/1000 | Loss: 0.00002129
Iteration 107/1000 | Loss: 0.00002129
Iteration 108/1000 | Loss: 0.00002129
Iteration 109/1000 | Loss: 0.00002129
Iteration 110/1000 | Loss: 0.00002129
Iteration 111/1000 | Loss: 0.00002129
Iteration 112/1000 | Loss: 0.00002129
Iteration 113/1000 | Loss: 0.00002129
Iteration 114/1000 | Loss: 0.00002129
Iteration 115/1000 | Loss: 0.00002129
Iteration 116/1000 | Loss: 0.00002129
Iteration 117/1000 | Loss: 0.00002129
Iteration 118/1000 | Loss: 0.00002129
Iteration 119/1000 | Loss: 0.00002129
Iteration 120/1000 | Loss: 0.00002129
Iteration 121/1000 | Loss: 0.00002129
Iteration 122/1000 | Loss: 0.00002129
Iteration 123/1000 | Loss: 0.00002129
Iteration 124/1000 | Loss: 0.00002129
Iteration 125/1000 | Loss: 0.00002129
Iteration 126/1000 | Loss: 0.00002129
Iteration 127/1000 | Loss: 0.00002129
Iteration 128/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.1290037693688646e-05, 2.1290037693688646e-05, 2.1290037693688646e-05, 2.1290037693688646e-05, 2.1290037693688646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1290037693688646e-05

Optimization complete. Final v2v error: 3.868518590927124 mm

Highest mean error: 4.457653999328613 mm for frame 234

Lowest mean error: 3.606113910675049 mm for frame 192

Saving results

Total time: 61.625852823257446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789106
Iteration 2/25 | Loss: 0.00142912
Iteration 3/25 | Loss: 0.00103489
Iteration 4/25 | Loss: 0.00094125
Iteration 5/25 | Loss: 0.00092833
Iteration 6/25 | Loss: 0.00092597
Iteration 7/25 | Loss: 0.00092547
Iteration 8/25 | Loss: 0.00092547
Iteration 9/25 | Loss: 0.00092547
Iteration 10/25 | Loss: 0.00092547
Iteration 11/25 | Loss: 0.00092544
Iteration 12/25 | Loss: 0.00092544
Iteration 13/25 | Loss: 0.00092544
Iteration 14/25 | Loss: 0.00092542
Iteration 15/25 | Loss: 0.00092542
Iteration 16/25 | Loss: 0.00092542
Iteration 17/25 | Loss: 0.00092542
Iteration 18/25 | Loss: 0.00092542
Iteration 19/25 | Loss: 0.00092542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009254197939299047, 0.0009254197939299047, 0.0009254197939299047, 0.0009254197939299047, 0.0009254197939299047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009254197939299047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56705046
Iteration 2/25 | Loss: 0.00088376
Iteration 3/25 | Loss: 0.00088376
Iteration 4/25 | Loss: 0.00088376
Iteration 5/25 | Loss: 0.00088375
Iteration 6/25 | Loss: 0.00088375
Iteration 7/25 | Loss: 0.00088375
Iteration 8/25 | Loss: 0.00088375
Iteration 9/25 | Loss: 0.00088375
Iteration 10/25 | Loss: 0.00088375
Iteration 11/25 | Loss: 0.00088375
Iteration 12/25 | Loss: 0.00088375
Iteration 13/25 | Loss: 0.00088375
Iteration 14/25 | Loss: 0.00088375
Iteration 15/25 | Loss: 0.00088375
Iteration 16/25 | Loss: 0.00088375
Iteration 17/25 | Loss: 0.00088375
Iteration 18/25 | Loss: 0.00088375
Iteration 19/25 | Loss: 0.00088375
Iteration 20/25 | Loss: 0.00088375
Iteration 21/25 | Loss: 0.00088375
Iteration 22/25 | Loss: 0.00088375
Iteration 23/25 | Loss: 0.00088375
Iteration 24/25 | Loss: 0.00088375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008837532950565219, 0.0008837532950565219, 0.0008837532950565219, 0.0008837532950565219, 0.0008837532950565219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008837532950565219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088375
Iteration 2/1000 | Loss: 0.00004798
Iteration 3/1000 | Loss: 0.00003580
Iteration 4/1000 | Loss: 0.00003030
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002720
Iteration 7/1000 | Loss: 0.00002637
Iteration 8/1000 | Loss: 0.00002589
Iteration 9/1000 | Loss: 0.00002550
Iteration 10/1000 | Loss: 0.00002515
Iteration 11/1000 | Loss: 0.00002489
Iteration 12/1000 | Loss: 0.00002466
Iteration 13/1000 | Loss: 0.00002449
Iteration 14/1000 | Loss: 0.00002442
Iteration 15/1000 | Loss: 0.00002441
Iteration 16/1000 | Loss: 0.00002441
Iteration 17/1000 | Loss: 0.00002440
Iteration 18/1000 | Loss: 0.00002437
Iteration 19/1000 | Loss: 0.00002435
Iteration 20/1000 | Loss: 0.00002434
Iteration 21/1000 | Loss: 0.00002434
Iteration 22/1000 | Loss: 0.00002433
Iteration 23/1000 | Loss: 0.00002433
Iteration 24/1000 | Loss: 0.00002432
Iteration 25/1000 | Loss: 0.00002432
Iteration 26/1000 | Loss: 0.00002431
Iteration 27/1000 | Loss: 0.00002431
Iteration 28/1000 | Loss: 0.00002428
Iteration 29/1000 | Loss: 0.00002428
Iteration 30/1000 | Loss: 0.00002425
Iteration 31/1000 | Loss: 0.00002425
Iteration 32/1000 | Loss: 0.00002424
Iteration 33/1000 | Loss: 0.00002424
Iteration 34/1000 | Loss: 0.00002423
Iteration 35/1000 | Loss: 0.00002422
Iteration 36/1000 | Loss: 0.00002422
Iteration 37/1000 | Loss: 0.00002421
Iteration 38/1000 | Loss: 0.00002420
Iteration 39/1000 | Loss: 0.00002419
Iteration 40/1000 | Loss: 0.00002418
Iteration 41/1000 | Loss: 0.00002417
Iteration 42/1000 | Loss: 0.00002417
Iteration 43/1000 | Loss: 0.00002416
Iteration 44/1000 | Loss: 0.00002415
Iteration 45/1000 | Loss: 0.00002415
Iteration 46/1000 | Loss: 0.00002414
Iteration 47/1000 | Loss: 0.00002414
Iteration 48/1000 | Loss: 0.00002414
Iteration 49/1000 | Loss: 0.00002413
Iteration 50/1000 | Loss: 0.00002413
Iteration 51/1000 | Loss: 0.00002413
Iteration 52/1000 | Loss: 0.00002412
Iteration 53/1000 | Loss: 0.00002412
Iteration 54/1000 | Loss: 0.00002411
Iteration 55/1000 | Loss: 0.00002411
Iteration 56/1000 | Loss: 0.00002411
Iteration 57/1000 | Loss: 0.00002410
Iteration 58/1000 | Loss: 0.00002410
Iteration 59/1000 | Loss: 0.00002410
Iteration 60/1000 | Loss: 0.00002409
Iteration 61/1000 | Loss: 0.00002409
Iteration 62/1000 | Loss: 0.00002409
Iteration 63/1000 | Loss: 0.00002409
Iteration 64/1000 | Loss: 0.00002408
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002407
Iteration 68/1000 | Loss: 0.00002407
Iteration 69/1000 | Loss: 0.00002407
Iteration 70/1000 | Loss: 0.00002407
Iteration 71/1000 | Loss: 0.00002406
Iteration 72/1000 | Loss: 0.00002406
Iteration 73/1000 | Loss: 0.00002406
Iteration 74/1000 | Loss: 0.00002406
Iteration 75/1000 | Loss: 0.00002406
Iteration 76/1000 | Loss: 0.00002406
Iteration 77/1000 | Loss: 0.00002405
Iteration 78/1000 | Loss: 0.00002405
Iteration 79/1000 | Loss: 0.00002405
Iteration 80/1000 | Loss: 0.00002404
Iteration 81/1000 | Loss: 0.00002404
Iteration 82/1000 | Loss: 0.00002404
Iteration 83/1000 | Loss: 0.00002403
Iteration 84/1000 | Loss: 0.00002403
Iteration 85/1000 | Loss: 0.00002403
Iteration 86/1000 | Loss: 0.00002403
Iteration 87/1000 | Loss: 0.00002403
Iteration 88/1000 | Loss: 0.00002403
Iteration 89/1000 | Loss: 0.00002403
Iteration 90/1000 | Loss: 0.00002402
Iteration 91/1000 | Loss: 0.00002402
Iteration 92/1000 | Loss: 0.00002402
Iteration 93/1000 | Loss: 0.00002402
Iteration 94/1000 | Loss: 0.00002402
Iteration 95/1000 | Loss: 0.00002402
Iteration 96/1000 | Loss: 0.00002402
Iteration 97/1000 | Loss: 0.00002402
Iteration 98/1000 | Loss: 0.00002401
Iteration 99/1000 | Loss: 0.00002401
Iteration 100/1000 | Loss: 0.00002401
Iteration 101/1000 | Loss: 0.00002401
Iteration 102/1000 | Loss: 0.00002401
Iteration 103/1000 | Loss: 0.00002401
Iteration 104/1000 | Loss: 0.00002401
Iteration 105/1000 | Loss: 0.00002401
Iteration 106/1000 | Loss: 0.00002401
Iteration 107/1000 | Loss: 0.00002401
Iteration 108/1000 | Loss: 0.00002400
Iteration 109/1000 | Loss: 0.00002400
Iteration 110/1000 | Loss: 0.00002400
Iteration 111/1000 | Loss: 0.00002400
Iteration 112/1000 | Loss: 0.00002400
Iteration 113/1000 | Loss: 0.00002400
Iteration 114/1000 | Loss: 0.00002400
Iteration 115/1000 | Loss: 0.00002400
Iteration 116/1000 | Loss: 0.00002400
Iteration 117/1000 | Loss: 0.00002400
Iteration 118/1000 | Loss: 0.00002400
Iteration 119/1000 | Loss: 0.00002399
Iteration 120/1000 | Loss: 0.00002399
Iteration 121/1000 | Loss: 0.00002399
Iteration 122/1000 | Loss: 0.00002399
Iteration 123/1000 | Loss: 0.00002399
Iteration 124/1000 | Loss: 0.00002399
Iteration 125/1000 | Loss: 0.00002399
Iteration 126/1000 | Loss: 0.00002398
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002398
Iteration 129/1000 | Loss: 0.00002398
Iteration 130/1000 | Loss: 0.00002398
Iteration 131/1000 | Loss: 0.00002398
Iteration 132/1000 | Loss: 0.00002398
Iteration 133/1000 | Loss: 0.00002398
Iteration 134/1000 | Loss: 0.00002398
Iteration 135/1000 | Loss: 0.00002398
Iteration 136/1000 | Loss: 0.00002398
Iteration 137/1000 | Loss: 0.00002398
Iteration 138/1000 | Loss: 0.00002397
Iteration 139/1000 | Loss: 0.00002397
Iteration 140/1000 | Loss: 0.00002397
Iteration 141/1000 | Loss: 0.00002397
Iteration 142/1000 | Loss: 0.00002397
Iteration 143/1000 | Loss: 0.00002397
Iteration 144/1000 | Loss: 0.00002397
Iteration 145/1000 | Loss: 0.00002396
Iteration 146/1000 | Loss: 0.00002396
Iteration 147/1000 | Loss: 0.00002396
Iteration 148/1000 | Loss: 0.00002396
Iteration 149/1000 | Loss: 0.00002396
Iteration 150/1000 | Loss: 0.00002396
Iteration 151/1000 | Loss: 0.00002396
Iteration 152/1000 | Loss: 0.00002396
Iteration 153/1000 | Loss: 0.00002395
Iteration 154/1000 | Loss: 0.00002395
Iteration 155/1000 | Loss: 0.00002395
Iteration 156/1000 | Loss: 0.00002395
Iteration 157/1000 | Loss: 0.00002395
Iteration 158/1000 | Loss: 0.00002395
Iteration 159/1000 | Loss: 0.00002395
Iteration 160/1000 | Loss: 0.00002395
Iteration 161/1000 | Loss: 0.00002395
Iteration 162/1000 | Loss: 0.00002395
Iteration 163/1000 | Loss: 0.00002395
Iteration 164/1000 | Loss: 0.00002395
Iteration 165/1000 | Loss: 0.00002395
Iteration 166/1000 | Loss: 0.00002395
Iteration 167/1000 | Loss: 0.00002394
Iteration 168/1000 | Loss: 0.00002394
Iteration 169/1000 | Loss: 0.00002394
Iteration 170/1000 | Loss: 0.00002394
Iteration 171/1000 | Loss: 0.00002394
Iteration 172/1000 | Loss: 0.00002394
Iteration 173/1000 | Loss: 0.00002394
Iteration 174/1000 | Loss: 0.00002394
Iteration 175/1000 | Loss: 0.00002393
Iteration 176/1000 | Loss: 0.00002393
Iteration 177/1000 | Loss: 0.00002393
Iteration 178/1000 | Loss: 0.00002393
Iteration 179/1000 | Loss: 0.00002393
Iteration 180/1000 | Loss: 0.00002393
Iteration 181/1000 | Loss: 0.00002393
Iteration 182/1000 | Loss: 0.00002393
Iteration 183/1000 | Loss: 0.00002393
Iteration 184/1000 | Loss: 0.00002393
Iteration 185/1000 | Loss: 0.00002393
Iteration 186/1000 | Loss: 0.00002393
Iteration 187/1000 | Loss: 0.00002393
Iteration 188/1000 | Loss: 0.00002393
Iteration 189/1000 | Loss: 0.00002393
Iteration 190/1000 | Loss: 0.00002393
Iteration 191/1000 | Loss: 0.00002392
Iteration 192/1000 | Loss: 0.00002392
Iteration 193/1000 | Loss: 0.00002392
Iteration 194/1000 | Loss: 0.00002392
Iteration 195/1000 | Loss: 0.00002392
Iteration 196/1000 | Loss: 0.00002392
Iteration 197/1000 | Loss: 0.00002392
Iteration 198/1000 | Loss: 0.00002392
Iteration 199/1000 | Loss: 0.00002392
Iteration 200/1000 | Loss: 0.00002392
Iteration 201/1000 | Loss: 0.00002392
Iteration 202/1000 | Loss: 0.00002392
Iteration 203/1000 | Loss: 0.00002392
Iteration 204/1000 | Loss: 0.00002392
Iteration 205/1000 | Loss: 0.00002392
Iteration 206/1000 | Loss: 0.00002392
Iteration 207/1000 | Loss: 0.00002392
Iteration 208/1000 | Loss: 0.00002392
Iteration 209/1000 | Loss: 0.00002392
Iteration 210/1000 | Loss: 0.00002392
Iteration 211/1000 | Loss: 0.00002392
Iteration 212/1000 | Loss: 0.00002391
Iteration 213/1000 | Loss: 0.00002391
Iteration 214/1000 | Loss: 0.00002391
Iteration 215/1000 | Loss: 0.00002391
Iteration 216/1000 | Loss: 0.00002391
Iteration 217/1000 | Loss: 0.00002391
Iteration 218/1000 | Loss: 0.00002391
Iteration 219/1000 | Loss: 0.00002391
Iteration 220/1000 | Loss: 0.00002391
Iteration 221/1000 | Loss: 0.00002391
Iteration 222/1000 | Loss: 0.00002391
Iteration 223/1000 | Loss: 0.00002391
Iteration 224/1000 | Loss: 0.00002391
Iteration 225/1000 | Loss: 0.00002391
Iteration 226/1000 | Loss: 0.00002391
Iteration 227/1000 | Loss: 0.00002391
Iteration 228/1000 | Loss: 0.00002391
Iteration 229/1000 | Loss: 0.00002391
Iteration 230/1000 | Loss: 0.00002391
Iteration 231/1000 | Loss: 0.00002391
Iteration 232/1000 | Loss: 0.00002391
Iteration 233/1000 | Loss: 0.00002391
Iteration 234/1000 | Loss: 0.00002391
Iteration 235/1000 | Loss: 0.00002391
Iteration 236/1000 | Loss: 0.00002391
Iteration 237/1000 | Loss: 0.00002391
Iteration 238/1000 | Loss: 0.00002391
Iteration 239/1000 | Loss: 0.00002391
Iteration 240/1000 | Loss: 0.00002391
Iteration 241/1000 | Loss: 0.00002391
Iteration 242/1000 | Loss: 0.00002391
Iteration 243/1000 | Loss: 0.00002391
Iteration 244/1000 | Loss: 0.00002391
Iteration 245/1000 | Loss: 0.00002391
Iteration 246/1000 | Loss: 0.00002391
Iteration 247/1000 | Loss: 0.00002391
Iteration 248/1000 | Loss: 0.00002391
Iteration 249/1000 | Loss: 0.00002391
Iteration 250/1000 | Loss: 0.00002391
Iteration 251/1000 | Loss: 0.00002391
Iteration 252/1000 | Loss: 0.00002391
Iteration 253/1000 | Loss: 0.00002391
Iteration 254/1000 | Loss: 0.00002391
Iteration 255/1000 | Loss: 0.00002391
Iteration 256/1000 | Loss: 0.00002391
Iteration 257/1000 | Loss: 0.00002391
Iteration 258/1000 | Loss: 0.00002391
Iteration 259/1000 | Loss: 0.00002391
Iteration 260/1000 | Loss: 0.00002391
Iteration 261/1000 | Loss: 0.00002391
Iteration 262/1000 | Loss: 0.00002391
Iteration 263/1000 | Loss: 0.00002391
Iteration 264/1000 | Loss: 0.00002391
Iteration 265/1000 | Loss: 0.00002391
Iteration 266/1000 | Loss: 0.00002391
Iteration 267/1000 | Loss: 0.00002391
Iteration 268/1000 | Loss: 0.00002391
Iteration 269/1000 | Loss: 0.00002391
Iteration 270/1000 | Loss: 0.00002391
Iteration 271/1000 | Loss: 0.00002391
Iteration 272/1000 | Loss: 0.00002391
Iteration 273/1000 | Loss: 0.00002391
Iteration 274/1000 | Loss: 0.00002391
Iteration 275/1000 | Loss: 0.00002391
Iteration 276/1000 | Loss: 0.00002391
Iteration 277/1000 | Loss: 0.00002391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [2.391428643022664e-05, 2.391428643022664e-05, 2.391428643022664e-05, 2.391428643022664e-05, 2.391428643022664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.391428643022664e-05

Optimization complete. Final v2v error: 4.0322041511535645 mm

Highest mean error: 5.437495231628418 mm for frame 26

Lowest mean error: 3.408859968185425 mm for frame 179

Saving results

Total time: 45.66879677772522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445399
Iteration 2/25 | Loss: 0.00102087
Iteration 3/25 | Loss: 0.00091732
Iteration 4/25 | Loss: 0.00089010
Iteration 5/25 | Loss: 0.00088418
Iteration 6/25 | Loss: 0.00088302
Iteration 7/25 | Loss: 0.00088301
Iteration 8/25 | Loss: 0.00088301
Iteration 9/25 | Loss: 0.00088301
Iteration 10/25 | Loss: 0.00088301
Iteration 11/25 | Loss: 0.00088301
Iteration 12/25 | Loss: 0.00088301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008830056176520884, 0.0008830056176520884, 0.0008830056176520884, 0.0008830056176520884, 0.0008830056176520884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008830056176520884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48477459
Iteration 2/25 | Loss: 0.00060112
Iteration 3/25 | Loss: 0.00060112
Iteration 4/25 | Loss: 0.00060112
Iteration 5/25 | Loss: 0.00060112
Iteration 6/25 | Loss: 0.00060112
Iteration 7/25 | Loss: 0.00060112
Iteration 8/25 | Loss: 0.00060112
Iteration 9/25 | Loss: 0.00060112
Iteration 10/25 | Loss: 0.00060112
Iteration 11/25 | Loss: 0.00060112
Iteration 12/25 | Loss: 0.00060112
Iteration 13/25 | Loss: 0.00060111
Iteration 14/25 | Loss: 0.00060111
Iteration 15/25 | Loss: 0.00060111
Iteration 16/25 | Loss: 0.00060111
Iteration 17/25 | Loss: 0.00060111
Iteration 18/25 | Loss: 0.00060111
Iteration 19/25 | Loss: 0.00060111
Iteration 20/25 | Loss: 0.00060111
Iteration 21/25 | Loss: 0.00060111
Iteration 22/25 | Loss: 0.00060111
Iteration 23/25 | Loss: 0.00060111
Iteration 24/25 | Loss: 0.00060111
Iteration 25/25 | Loss: 0.00060111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060111
Iteration 2/1000 | Loss: 0.00005565
Iteration 3/1000 | Loss: 0.00003460
Iteration 4/1000 | Loss: 0.00003162
Iteration 5/1000 | Loss: 0.00002986
Iteration 6/1000 | Loss: 0.00002879
Iteration 7/1000 | Loss: 0.00002814
Iteration 8/1000 | Loss: 0.00002739
Iteration 9/1000 | Loss: 0.00002700
Iteration 10/1000 | Loss: 0.00002661
Iteration 11/1000 | Loss: 0.00002645
Iteration 12/1000 | Loss: 0.00002620
Iteration 13/1000 | Loss: 0.00002614
Iteration 14/1000 | Loss: 0.00002605
Iteration 15/1000 | Loss: 0.00002604
Iteration 16/1000 | Loss: 0.00002599
Iteration 17/1000 | Loss: 0.00002598
Iteration 18/1000 | Loss: 0.00002595
Iteration 19/1000 | Loss: 0.00002594
Iteration 20/1000 | Loss: 0.00002580
Iteration 21/1000 | Loss: 0.00002576
Iteration 22/1000 | Loss: 0.00002570
Iteration 23/1000 | Loss: 0.00002570
Iteration 24/1000 | Loss: 0.00002568
Iteration 25/1000 | Loss: 0.00002568
Iteration 26/1000 | Loss: 0.00002568
Iteration 27/1000 | Loss: 0.00002568
Iteration 28/1000 | Loss: 0.00002568
Iteration 29/1000 | Loss: 0.00002568
Iteration 30/1000 | Loss: 0.00002568
Iteration 31/1000 | Loss: 0.00002568
Iteration 32/1000 | Loss: 0.00002567
Iteration 33/1000 | Loss: 0.00002567
Iteration 34/1000 | Loss: 0.00002565
Iteration 35/1000 | Loss: 0.00002565
Iteration 36/1000 | Loss: 0.00002565
Iteration 37/1000 | Loss: 0.00002565
Iteration 38/1000 | Loss: 0.00002564
Iteration 39/1000 | Loss: 0.00002564
Iteration 40/1000 | Loss: 0.00002564
Iteration 41/1000 | Loss: 0.00002563
Iteration 42/1000 | Loss: 0.00002563
Iteration 43/1000 | Loss: 0.00002563
Iteration 44/1000 | Loss: 0.00002562
Iteration 45/1000 | Loss: 0.00002562
Iteration 46/1000 | Loss: 0.00002562
Iteration 47/1000 | Loss: 0.00002562
Iteration 48/1000 | Loss: 0.00002562
Iteration 49/1000 | Loss: 0.00002561
Iteration 50/1000 | Loss: 0.00002561
Iteration 51/1000 | Loss: 0.00002560
Iteration 52/1000 | Loss: 0.00002559
Iteration 53/1000 | Loss: 0.00002559
Iteration 54/1000 | Loss: 0.00002558
Iteration 55/1000 | Loss: 0.00002558
Iteration 56/1000 | Loss: 0.00002558
Iteration 57/1000 | Loss: 0.00002558
Iteration 58/1000 | Loss: 0.00002558
Iteration 59/1000 | Loss: 0.00002558
Iteration 60/1000 | Loss: 0.00002557
Iteration 61/1000 | Loss: 0.00002557
Iteration 62/1000 | Loss: 0.00002556
Iteration 63/1000 | Loss: 0.00002556
Iteration 64/1000 | Loss: 0.00002556
Iteration 65/1000 | Loss: 0.00002555
Iteration 66/1000 | Loss: 0.00002555
Iteration 67/1000 | Loss: 0.00002555
Iteration 68/1000 | Loss: 0.00002555
Iteration 69/1000 | Loss: 0.00002555
Iteration 70/1000 | Loss: 0.00002555
Iteration 71/1000 | Loss: 0.00002554
Iteration 72/1000 | Loss: 0.00002554
Iteration 73/1000 | Loss: 0.00002554
Iteration 74/1000 | Loss: 0.00002554
Iteration 75/1000 | Loss: 0.00002554
Iteration 76/1000 | Loss: 0.00002554
Iteration 77/1000 | Loss: 0.00002554
Iteration 78/1000 | Loss: 0.00002554
Iteration 79/1000 | Loss: 0.00002554
Iteration 80/1000 | Loss: 0.00002554
Iteration 81/1000 | Loss: 0.00002553
Iteration 82/1000 | Loss: 0.00002553
Iteration 83/1000 | Loss: 0.00002552
Iteration 84/1000 | Loss: 0.00002552
Iteration 85/1000 | Loss: 0.00002552
Iteration 86/1000 | Loss: 0.00002552
Iteration 87/1000 | Loss: 0.00002552
Iteration 88/1000 | Loss: 0.00002552
Iteration 89/1000 | Loss: 0.00002552
Iteration 90/1000 | Loss: 0.00002552
Iteration 91/1000 | Loss: 0.00002551
Iteration 92/1000 | Loss: 0.00002551
Iteration 93/1000 | Loss: 0.00002551
Iteration 94/1000 | Loss: 0.00002551
Iteration 95/1000 | Loss: 0.00002551
Iteration 96/1000 | Loss: 0.00002551
Iteration 97/1000 | Loss: 0.00002551
Iteration 98/1000 | Loss: 0.00002550
Iteration 99/1000 | Loss: 0.00002550
Iteration 100/1000 | Loss: 0.00002550
Iteration 101/1000 | Loss: 0.00002550
Iteration 102/1000 | Loss: 0.00002550
Iteration 103/1000 | Loss: 0.00002549
Iteration 104/1000 | Loss: 0.00002549
Iteration 105/1000 | Loss: 0.00002549
Iteration 106/1000 | Loss: 0.00002549
Iteration 107/1000 | Loss: 0.00002549
Iteration 108/1000 | Loss: 0.00002549
Iteration 109/1000 | Loss: 0.00002549
Iteration 110/1000 | Loss: 0.00002549
Iteration 111/1000 | Loss: 0.00002549
Iteration 112/1000 | Loss: 0.00002549
Iteration 113/1000 | Loss: 0.00002549
Iteration 114/1000 | Loss: 0.00002549
Iteration 115/1000 | Loss: 0.00002549
Iteration 116/1000 | Loss: 0.00002548
Iteration 117/1000 | Loss: 0.00002548
Iteration 118/1000 | Loss: 0.00002548
Iteration 119/1000 | Loss: 0.00002548
Iteration 120/1000 | Loss: 0.00002548
Iteration 121/1000 | Loss: 0.00002548
Iteration 122/1000 | Loss: 0.00002548
Iteration 123/1000 | Loss: 0.00002548
Iteration 124/1000 | Loss: 0.00002548
Iteration 125/1000 | Loss: 0.00002548
Iteration 126/1000 | Loss: 0.00002548
Iteration 127/1000 | Loss: 0.00002548
Iteration 128/1000 | Loss: 0.00002548
Iteration 129/1000 | Loss: 0.00002548
Iteration 130/1000 | Loss: 0.00002548
Iteration 131/1000 | Loss: 0.00002548
Iteration 132/1000 | Loss: 0.00002548
Iteration 133/1000 | Loss: 0.00002548
Iteration 134/1000 | Loss: 0.00002548
Iteration 135/1000 | Loss: 0.00002548
Iteration 136/1000 | Loss: 0.00002548
Iteration 137/1000 | Loss: 0.00002548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.5478771931375377e-05, 2.5478771931375377e-05, 2.5478771931375377e-05, 2.5478771931375377e-05, 2.5478771931375377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5478771931375377e-05

Optimization complete. Final v2v error: 4.2048845291137695 mm

Highest mean error: 4.444482326507568 mm for frame 125

Lowest mean error: 3.879411458969116 mm for frame 47

Saving results

Total time: 38.54343628883362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671928
Iteration 2/25 | Loss: 0.00121514
Iteration 3/25 | Loss: 0.00094131
Iteration 4/25 | Loss: 0.00090720
Iteration 5/25 | Loss: 0.00089904
Iteration 6/25 | Loss: 0.00089664
Iteration 7/25 | Loss: 0.00089610
Iteration 8/25 | Loss: 0.00089608
Iteration 9/25 | Loss: 0.00089608
Iteration 10/25 | Loss: 0.00089608
Iteration 11/25 | Loss: 0.00089608
Iteration 12/25 | Loss: 0.00089608
Iteration 13/25 | Loss: 0.00089608
Iteration 14/25 | Loss: 0.00089608
Iteration 15/25 | Loss: 0.00089608
Iteration 16/25 | Loss: 0.00089608
Iteration 17/25 | Loss: 0.00089608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008960783598013222, 0.0008960783598013222, 0.0008960783598013222, 0.0008960783598013222, 0.0008960783598013222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008960783598013222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.11275959
Iteration 2/25 | Loss: 0.00067248
Iteration 3/25 | Loss: 0.00067248
Iteration 4/25 | Loss: 0.00067248
Iteration 5/25 | Loss: 0.00067248
Iteration 6/25 | Loss: 0.00067248
Iteration 7/25 | Loss: 0.00067248
Iteration 8/25 | Loss: 0.00067248
Iteration 9/25 | Loss: 0.00067248
Iteration 10/25 | Loss: 0.00067248
Iteration 11/25 | Loss: 0.00067248
Iteration 12/25 | Loss: 0.00067248
Iteration 13/25 | Loss: 0.00067248
Iteration 14/25 | Loss: 0.00067248
Iteration 15/25 | Loss: 0.00067248
Iteration 16/25 | Loss: 0.00067248
Iteration 17/25 | Loss: 0.00067248
Iteration 18/25 | Loss: 0.00067248
Iteration 19/25 | Loss: 0.00067248
Iteration 20/25 | Loss: 0.00067248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006724757258780301, 0.0006724757258780301, 0.0006724757258780301, 0.0006724757258780301, 0.0006724757258780301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006724757258780301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067248
Iteration 2/1000 | Loss: 0.00004115
Iteration 3/1000 | Loss: 0.00002697
Iteration 4/1000 | Loss: 0.00002423
Iteration 5/1000 | Loss: 0.00002285
Iteration 6/1000 | Loss: 0.00002207
Iteration 7/1000 | Loss: 0.00002138
Iteration 8/1000 | Loss: 0.00002095
Iteration 9/1000 | Loss: 0.00002064
Iteration 10/1000 | Loss: 0.00002041
Iteration 11/1000 | Loss: 0.00002025
Iteration 12/1000 | Loss: 0.00002020
Iteration 13/1000 | Loss: 0.00002008
Iteration 14/1000 | Loss: 0.00002005
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001996
Iteration 17/1000 | Loss: 0.00001994
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001994
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001993
Iteration 22/1000 | Loss: 0.00001993
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001992
Iteration 26/1000 | Loss: 0.00001992
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001991
Iteration 30/1000 | Loss: 0.00001991
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001989
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001989
Iteration 38/1000 | Loss: 0.00001989
Iteration 39/1000 | Loss: 0.00001989
Iteration 40/1000 | Loss: 0.00001989
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001988
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001987
Iteration 47/1000 | Loss: 0.00001987
Iteration 48/1000 | Loss: 0.00001987
Iteration 49/1000 | Loss: 0.00001987
Iteration 50/1000 | Loss: 0.00001986
Iteration 51/1000 | Loss: 0.00001986
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00001986
Iteration 54/1000 | Loss: 0.00001986
Iteration 55/1000 | Loss: 0.00001985
Iteration 56/1000 | Loss: 0.00001985
Iteration 57/1000 | Loss: 0.00001985
Iteration 58/1000 | Loss: 0.00001985
Iteration 59/1000 | Loss: 0.00001984
Iteration 60/1000 | Loss: 0.00001984
Iteration 61/1000 | Loss: 0.00001984
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001984
Iteration 64/1000 | Loss: 0.00001984
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001983
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001983
Iteration 70/1000 | Loss: 0.00001983
Iteration 71/1000 | Loss: 0.00001983
Iteration 72/1000 | Loss: 0.00001983
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001983
Iteration 75/1000 | Loss: 0.00001982
Iteration 76/1000 | Loss: 0.00001982
Iteration 77/1000 | Loss: 0.00001982
Iteration 78/1000 | Loss: 0.00001982
Iteration 79/1000 | Loss: 0.00001981
Iteration 80/1000 | Loss: 0.00001981
Iteration 81/1000 | Loss: 0.00001981
Iteration 82/1000 | Loss: 0.00001981
Iteration 83/1000 | Loss: 0.00001980
Iteration 84/1000 | Loss: 0.00001980
Iteration 85/1000 | Loss: 0.00001980
Iteration 86/1000 | Loss: 0.00001980
Iteration 87/1000 | Loss: 0.00001979
Iteration 88/1000 | Loss: 0.00001979
Iteration 89/1000 | Loss: 0.00001979
Iteration 90/1000 | Loss: 0.00001979
Iteration 91/1000 | Loss: 0.00001979
Iteration 92/1000 | Loss: 0.00001979
Iteration 93/1000 | Loss: 0.00001978
Iteration 94/1000 | Loss: 0.00001978
Iteration 95/1000 | Loss: 0.00001978
Iteration 96/1000 | Loss: 0.00001977
Iteration 97/1000 | Loss: 0.00001977
Iteration 98/1000 | Loss: 0.00001977
Iteration 99/1000 | Loss: 0.00001977
Iteration 100/1000 | Loss: 0.00001976
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001975
Iteration 104/1000 | Loss: 0.00001975
Iteration 105/1000 | Loss: 0.00001975
Iteration 106/1000 | Loss: 0.00001975
Iteration 107/1000 | Loss: 0.00001974
Iteration 108/1000 | Loss: 0.00001974
Iteration 109/1000 | Loss: 0.00001974
Iteration 110/1000 | Loss: 0.00001974
Iteration 111/1000 | Loss: 0.00001974
Iteration 112/1000 | Loss: 0.00001974
Iteration 113/1000 | Loss: 0.00001974
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00001973
Iteration 116/1000 | Loss: 0.00001973
Iteration 117/1000 | Loss: 0.00001972
Iteration 118/1000 | Loss: 0.00001972
Iteration 119/1000 | Loss: 0.00001972
Iteration 120/1000 | Loss: 0.00001972
Iteration 121/1000 | Loss: 0.00001972
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001972
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001972
Iteration 128/1000 | Loss: 0.00001972
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001972
Iteration 131/1000 | Loss: 0.00001972
Iteration 132/1000 | Loss: 0.00001972
Iteration 133/1000 | Loss: 0.00001972
Iteration 134/1000 | Loss: 0.00001972
Iteration 135/1000 | Loss: 0.00001972
Iteration 136/1000 | Loss: 0.00001972
Iteration 137/1000 | Loss: 0.00001972
Iteration 138/1000 | Loss: 0.00001972
Iteration 139/1000 | Loss: 0.00001972
Iteration 140/1000 | Loss: 0.00001972
Iteration 141/1000 | Loss: 0.00001972
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001972
Iteration 148/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.9721463104360737e-05, 1.9721463104360737e-05, 1.9721463104360737e-05, 1.9721463104360737e-05, 1.9721463104360737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9721463104360737e-05

Optimization complete. Final v2v error: 3.687873363494873 mm

Highest mean error: 4.449130535125732 mm for frame 88

Lowest mean error: 2.874055862426758 mm for frame 22

Saving results

Total time: 37.99507284164429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021426
Iteration 2/25 | Loss: 0.00173043
Iteration 3/25 | Loss: 0.00124529
Iteration 4/25 | Loss: 0.00116373
Iteration 5/25 | Loss: 0.00111610
Iteration 6/25 | Loss: 0.00109742
Iteration 7/25 | Loss: 0.00108136
Iteration 8/25 | Loss: 0.00107634
Iteration 9/25 | Loss: 0.00105311
Iteration 10/25 | Loss: 0.00105901
Iteration 11/25 | Loss: 0.00106628
Iteration 12/25 | Loss: 0.00105083
Iteration 13/25 | Loss: 0.00104800
Iteration 14/25 | Loss: 0.00104490
Iteration 15/25 | Loss: 0.00104252
Iteration 16/25 | Loss: 0.00103633
Iteration 17/25 | Loss: 0.00102581
Iteration 18/25 | Loss: 0.00102424
Iteration 19/25 | Loss: 0.00102716
Iteration 20/25 | Loss: 0.00102538
Iteration 21/25 | Loss: 0.00102543
Iteration 22/25 | Loss: 0.00102189
Iteration 23/25 | Loss: 0.00102459
Iteration 24/25 | Loss: 0.00102290
Iteration 25/25 | Loss: 0.00101833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91180754
Iteration 2/25 | Loss: 0.00197977
Iteration 3/25 | Loss: 0.00197976
Iteration 4/25 | Loss: 0.00197976
Iteration 5/25 | Loss: 0.00197976
Iteration 6/25 | Loss: 0.00197976
Iteration 7/25 | Loss: 0.00197976
Iteration 8/25 | Loss: 0.00197976
Iteration 9/25 | Loss: 0.00197976
Iteration 10/25 | Loss: 0.00197976
Iteration 11/25 | Loss: 0.00197976
Iteration 12/25 | Loss: 0.00197976
Iteration 13/25 | Loss: 0.00197976
Iteration 14/25 | Loss: 0.00197976
Iteration 15/25 | Loss: 0.00197976
Iteration 16/25 | Loss: 0.00197976
Iteration 17/25 | Loss: 0.00197976
Iteration 18/25 | Loss: 0.00197976
Iteration 19/25 | Loss: 0.00197976
Iteration 20/25 | Loss: 0.00197976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019797622226178646, 0.0019797622226178646, 0.0019797622226178646, 0.0019797622226178646, 0.0019797622226178646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019797622226178646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197976
Iteration 2/1000 | Loss: 0.00050660
Iteration 3/1000 | Loss: 0.00027266
Iteration 4/1000 | Loss: 0.00081539
Iteration 5/1000 | Loss: 0.00035153
Iteration 6/1000 | Loss: 0.00073495
Iteration 7/1000 | Loss: 0.00056208
Iteration 8/1000 | Loss: 0.00018498
Iteration 9/1000 | Loss: 0.00068955
Iteration 10/1000 | Loss: 0.00082739
Iteration 11/1000 | Loss: 0.00234063
Iteration 12/1000 | Loss: 0.00214650
Iteration 13/1000 | Loss: 0.00094227
Iteration 14/1000 | Loss: 0.00050304
Iteration 15/1000 | Loss: 0.00019242
Iteration 16/1000 | Loss: 0.00021876
Iteration 17/1000 | Loss: 0.00028456
Iteration 18/1000 | Loss: 0.00027009
Iteration 19/1000 | Loss: 0.00044745
Iteration 20/1000 | Loss: 0.00024168
Iteration 21/1000 | Loss: 0.00030584
Iteration 22/1000 | Loss: 0.00020770
Iteration 23/1000 | Loss: 0.00023849
Iteration 24/1000 | Loss: 0.00020379
Iteration 25/1000 | Loss: 0.00065311
Iteration 26/1000 | Loss: 0.00094028
Iteration 27/1000 | Loss: 0.00066738
Iteration 28/1000 | Loss: 0.00061745
Iteration 29/1000 | Loss: 0.00089366
Iteration 30/1000 | Loss: 0.00020981
Iteration 31/1000 | Loss: 0.00024655
Iteration 32/1000 | Loss: 0.00018109
Iteration 33/1000 | Loss: 0.00023367
Iteration 34/1000 | Loss: 0.00038444
Iteration 35/1000 | Loss: 0.00024776
Iteration 36/1000 | Loss: 0.00028629
Iteration 37/1000 | Loss: 0.00022372
Iteration 38/1000 | Loss: 0.00019187
Iteration 39/1000 | Loss: 0.00026436
Iteration 40/1000 | Loss: 0.00020642
Iteration 41/1000 | Loss: 0.00039875
Iteration 42/1000 | Loss: 0.00016999
Iteration 43/1000 | Loss: 0.00017007
Iteration 44/1000 | Loss: 0.00013292
Iteration 45/1000 | Loss: 0.00017498
Iteration 46/1000 | Loss: 0.00015716
Iteration 47/1000 | Loss: 0.00068180
Iteration 48/1000 | Loss: 0.00036781
Iteration 49/1000 | Loss: 0.00060325
Iteration 50/1000 | Loss: 0.00009809
Iteration 51/1000 | Loss: 0.00006818
Iteration 52/1000 | Loss: 0.00112816
Iteration 53/1000 | Loss: 0.00009388
Iteration 54/1000 | Loss: 0.00008399
Iteration 55/1000 | Loss: 0.00007890
Iteration 56/1000 | Loss: 0.00007568
Iteration 57/1000 | Loss: 0.00006767
Iteration 58/1000 | Loss: 0.00099984
Iteration 59/1000 | Loss: 0.00053712
Iteration 60/1000 | Loss: 0.00011462
Iteration 61/1000 | Loss: 0.00005855
Iteration 62/1000 | Loss: 0.00052649
Iteration 63/1000 | Loss: 0.00005310
Iteration 64/1000 | Loss: 0.00004795
Iteration 65/1000 | Loss: 0.00055399
Iteration 66/1000 | Loss: 0.00005578
Iteration 67/1000 | Loss: 0.00058498
Iteration 68/1000 | Loss: 0.00005845
Iteration 69/1000 | Loss: 0.00004671
Iteration 70/1000 | Loss: 0.00006011
Iteration 71/1000 | Loss: 0.00004419
Iteration 72/1000 | Loss: 0.00004586
Iteration 73/1000 | Loss: 0.00005095
Iteration 74/1000 | Loss: 0.00003843
Iteration 75/1000 | Loss: 0.00003269
Iteration 76/1000 | Loss: 0.00003157
Iteration 77/1000 | Loss: 0.00003081
Iteration 78/1000 | Loss: 0.00003010
Iteration 79/1000 | Loss: 0.00002963
Iteration 80/1000 | Loss: 0.00002916
Iteration 81/1000 | Loss: 0.00002883
Iteration 82/1000 | Loss: 0.00002860
Iteration 83/1000 | Loss: 0.00002841
Iteration 84/1000 | Loss: 0.00002839
Iteration 85/1000 | Loss: 0.00002824
Iteration 86/1000 | Loss: 0.00002824
Iteration 87/1000 | Loss: 0.00002817
Iteration 88/1000 | Loss: 0.00002817
Iteration 89/1000 | Loss: 0.00002815
Iteration 90/1000 | Loss: 0.00002815
Iteration 91/1000 | Loss: 0.00002814
Iteration 92/1000 | Loss: 0.00002814
Iteration 93/1000 | Loss: 0.00002814
Iteration 94/1000 | Loss: 0.00002814
Iteration 95/1000 | Loss: 0.00002814
Iteration 96/1000 | Loss: 0.00002814
Iteration 97/1000 | Loss: 0.00002814
Iteration 98/1000 | Loss: 0.00002814
Iteration 99/1000 | Loss: 0.00002814
Iteration 100/1000 | Loss: 0.00002814
Iteration 101/1000 | Loss: 0.00002814
Iteration 102/1000 | Loss: 0.00002814
Iteration 103/1000 | Loss: 0.00002813
Iteration 104/1000 | Loss: 0.00002813
Iteration 105/1000 | Loss: 0.00002813
Iteration 106/1000 | Loss: 0.00002813
Iteration 107/1000 | Loss: 0.00002813
Iteration 108/1000 | Loss: 0.00002813
Iteration 109/1000 | Loss: 0.00002813
Iteration 110/1000 | Loss: 0.00002813
Iteration 111/1000 | Loss: 0.00002813
Iteration 112/1000 | Loss: 0.00002813
Iteration 113/1000 | Loss: 0.00002812
Iteration 114/1000 | Loss: 0.00002812
Iteration 115/1000 | Loss: 0.00002810
Iteration 116/1000 | Loss: 0.00002808
Iteration 117/1000 | Loss: 0.00002808
Iteration 118/1000 | Loss: 0.00002808
Iteration 119/1000 | Loss: 0.00002807
Iteration 120/1000 | Loss: 0.00002806
Iteration 121/1000 | Loss: 0.00002806
Iteration 122/1000 | Loss: 0.00002805
Iteration 123/1000 | Loss: 0.00002804
Iteration 124/1000 | Loss: 0.00002804
Iteration 125/1000 | Loss: 0.00002804
Iteration 126/1000 | Loss: 0.00002803
Iteration 127/1000 | Loss: 0.00002803
Iteration 128/1000 | Loss: 0.00002802
Iteration 129/1000 | Loss: 0.00002802
Iteration 130/1000 | Loss: 0.00002802
Iteration 131/1000 | Loss: 0.00002801
Iteration 132/1000 | Loss: 0.00002801
Iteration 133/1000 | Loss: 0.00002800
Iteration 134/1000 | Loss: 0.00002800
Iteration 135/1000 | Loss: 0.00002799
Iteration 136/1000 | Loss: 0.00002799
Iteration 137/1000 | Loss: 0.00020352
Iteration 138/1000 | Loss: 0.00003107
Iteration 139/1000 | Loss: 0.00002963
Iteration 140/1000 | Loss: 0.00002880
Iteration 141/1000 | Loss: 0.00002833
Iteration 142/1000 | Loss: 0.00002819
Iteration 143/1000 | Loss: 0.00002816
Iteration 144/1000 | Loss: 0.00002815
Iteration 145/1000 | Loss: 0.00002815
Iteration 146/1000 | Loss: 0.00002814
Iteration 147/1000 | Loss: 0.00002813
Iteration 148/1000 | Loss: 0.00002813
Iteration 149/1000 | Loss: 0.00002813
Iteration 150/1000 | Loss: 0.00002812
Iteration 151/1000 | Loss: 0.00002812
Iteration 152/1000 | Loss: 0.00002811
Iteration 153/1000 | Loss: 0.00002811
Iteration 154/1000 | Loss: 0.00002811
Iteration 155/1000 | Loss: 0.00002810
Iteration 156/1000 | Loss: 0.00002810
Iteration 157/1000 | Loss: 0.00002810
Iteration 158/1000 | Loss: 0.00002810
Iteration 159/1000 | Loss: 0.00002810
Iteration 160/1000 | Loss: 0.00002810
Iteration 161/1000 | Loss: 0.00002809
Iteration 162/1000 | Loss: 0.00002809
Iteration 163/1000 | Loss: 0.00002809
Iteration 164/1000 | Loss: 0.00002808
Iteration 165/1000 | Loss: 0.00002808
Iteration 166/1000 | Loss: 0.00002807
Iteration 167/1000 | Loss: 0.00002807
Iteration 168/1000 | Loss: 0.00002806
Iteration 169/1000 | Loss: 0.00002806
Iteration 170/1000 | Loss: 0.00002806
Iteration 171/1000 | Loss: 0.00002806
Iteration 172/1000 | Loss: 0.00002806
Iteration 173/1000 | Loss: 0.00002806
Iteration 174/1000 | Loss: 0.00002806
Iteration 175/1000 | Loss: 0.00002806
Iteration 176/1000 | Loss: 0.00002806
Iteration 177/1000 | Loss: 0.00002806
Iteration 178/1000 | Loss: 0.00002806
Iteration 179/1000 | Loss: 0.00002806
Iteration 180/1000 | Loss: 0.00002805
Iteration 181/1000 | Loss: 0.00002804
Iteration 182/1000 | Loss: 0.00002804
Iteration 183/1000 | Loss: 0.00002803
Iteration 184/1000 | Loss: 0.00002803
Iteration 185/1000 | Loss: 0.00002803
Iteration 186/1000 | Loss: 0.00002803
Iteration 187/1000 | Loss: 0.00002802
Iteration 188/1000 | Loss: 0.00002802
Iteration 189/1000 | Loss: 0.00002802
Iteration 190/1000 | Loss: 0.00002802
Iteration 191/1000 | Loss: 0.00002802
Iteration 192/1000 | Loss: 0.00002801
Iteration 193/1000 | Loss: 0.00002801
Iteration 194/1000 | Loss: 0.00002801
Iteration 195/1000 | Loss: 0.00002801
Iteration 196/1000 | Loss: 0.00002801
Iteration 197/1000 | Loss: 0.00002801
Iteration 198/1000 | Loss: 0.00002801
Iteration 199/1000 | Loss: 0.00002801
Iteration 200/1000 | Loss: 0.00002801
Iteration 201/1000 | Loss: 0.00002801
Iteration 202/1000 | Loss: 0.00002801
Iteration 203/1000 | Loss: 0.00002801
Iteration 204/1000 | Loss: 0.00002800
Iteration 205/1000 | Loss: 0.00002800
Iteration 206/1000 | Loss: 0.00002799
Iteration 207/1000 | Loss: 0.00002799
Iteration 208/1000 | Loss: 0.00002799
Iteration 209/1000 | Loss: 0.00002799
Iteration 210/1000 | Loss: 0.00002799
Iteration 211/1000 | Loss: 0.00002799
Iteration 212/1000 | Loss: 0.00002798
Iteration 213/1000 | Loss: 0.00002798
Iteration 214/1000 | Loss: 0.00002798
Iteration 215/1000 | Loss: 0.00002798
Iteration 216/1000 | Loss: 0.00002798
Iteration 217/1000 | Loss: 0.00002798
Iteration 218/1000 | Loss: 0.00002797
Iteration 219/1000 | Loss: 0.00002797
Iteration 220/1000 | Loss: 0.00002797
Iteration 221/1000 | Loss: 0.00002796
Iteration 222/1000 | Loss: 0.00019520
Iteration 223/1000 | Loss: 0.00006421
Iteration 224/1000 | Loss: 0.00013977
Iteration 225/1000 | Loss: 0.00008592
Iteration 226/1000 | Loss: 0.00002810
Iteration 227/1000 | Loss: 0.00002797
Iteration 228/1000 | Loss: 0.00019308
Iteration 229/1000 | Loss: 0.00013241
Iteration 230/1000 | Loss: 0.00011684
Iteration 231/1000 | Loss: 0.00010480
Iteration 232/1000 | Loss: 0.00014478
Iteration 233/1000 | Loss: 0.00006116
Iteration 234/1000 | Loss: 0.00003162
Iteration 235/1000 | Loss: 0.00016835
Iteration 236/1000 | Loss: 0.00003977
Iteration 237/1000 | Loss: 0.00003377
Iteration 238/1000 | Loss: 0.00003197
Iteration 239/1000 | Loss: 0.00003090
Iteration 240/1000 | Loss: 0.00003055
Iteration 241/1000 | Loss: 0.00002996
Iteration 242/1000 | Loss: 0.00004411
Iteration 243/1000 | Loss: 0.00003379
Iteration 244/1000 | Loss: 0.00003124
Iteration 245/1000 | Loss: 0.00002983
Iteration 246/1000 | Loss: 0.00002934
Iteration 247/1000 | Loss: 0.00002882
Iteration 248/1000 | Loss: 0.00002823
Iteration 249/1000 | Loss: 0.00002795
Iteration 250/1000 | Loss: 0.00002769
Iteration 251/1000 | Loss: 0.00002755
Iteration 252/1000 | Loss: 0.00002754
Iteration 253/1000 | Loss: 0.00002749
Iteration 254/1000 | Loss: 0.00002748
Iteration 255/1000 | Loss: 0.00002748
Iteration 256/1000 | Loss: 0.00002748
Iteration 257/1000 | Loss: 0.00002748
Iteration 258/1000 | Loss: 0.00002748
Iteration 259/1000 | Loss: 0.00002748
Iteration 260/1000 | Loss: 0.00002748
Iteration 261/1000 | Loss: 0.00002748
Iteration 262/1000 | Loss: 0.00002748
Iteration 263/1000 | Loss: 0.00002747
Iteration 264/1000 | Loss: 0.00002747
Iteration 265/1000 | Loss: 0.00002747
Iteration 266/1000 | Loss: 0.00002747
Iteration 267/1000 | Loss: 0.00002747
Iteration 268/1000 | Loss: 0.00002747
Iteration 269/1000 | Loss: 0.00002747
Iteration 270/1000 | Loss: 0.00002747
Iteration 271/1000 | Loss: 0.00002747
Iteration 272/1000 | Loss: 0.00002747
Iteration 273/1000 | Loss: 0.00002747
Iteration 274/1000 | Loss: 0.00002746
Iteration 275/1000 | Loss: 0.00002746
Iteration 276/1000 | Loss: 0.00002746
Iteration 277/1000 | Loss: 0.00002746
Iteration 278/1000 | Loss: 0.00002746
Iteration 279/1000 | Loss: 0.00002743
Iteration 280/1000 | Loss: 0.00002742
Iteration 281/1000 | Loss: 0.00002742
Iteration 282/1000 | Loss: 0.00002742
Iteration 283/1000 | Loss: 0.00002742
Iteration 284/1000 | Loss: 0.00002741
Iteration 285/1000 | Loss: 0.00002741
Iteration 286/1000 | Loss: 0.00002741
Iteration 287/1000 | Loss: 0.00002741
Iteration 288/1000 | Loss: 0.00002741
Iteration 289/1000 | Loss: 0.00002741
Iteration 290/1000 | Loss: 0.00002741
Iteration 291/1000 | Loss: 0.00002740
Iteration 292/1000 | Loss: 0.00002740
Iteration 293/1000 | Loss: 0.00002740
Iteration 294/1000 | Loss: 0.00002740
Iteration 295/1000 | Loss: 0.00002740
Iteration 296/1000 | Loss: 0.00002740
Iteration 297/1000 | Loss: 0.00002739
Iteration 298/1000 | Loss: 0.00002739
Iteration 299/1000 | Loss: 0.00002739
Iteration 300/1000 | Loss: 0.00002739
Iteration 301/1000 | Loss: 0.00002739
Iteration 302/1000 | Loss: 0.00002738
Iteration 303/1000 | Loss: 0.00002738
Iteration 304/1000 | Loss: 0.00002738
Iteration 305/1000 | Loss: 0.00002738
Iteration 306/1000 | Loss: 0.00002738
Iteration 307/1000 | Loss: 0.00002738
Iteration 308/1000 | Loss: 0.00002738
Iteration 309/1000 | Loss: 0.00002738
Iteration 310/1000 | Loss: 0.00002737
Iteration 311/1000 | Loss: 0.00002737
Iteration 312/1000 | Loss: 0.00002737
Iteration 313/1000 | Loss: 0.00002737
Iteration 314/1000 | Loss: 0.00002737
Iteration 315/1000 | Loss: 0.00002736
Iteration 316/1000 | Loss: 0.00002736
Iteration 317/1000 | Loss: 0.00002736
Iteration 318/1000 | Loss: 0.00002736
Iteration 319/1000 | Loss: 0.00002736
Iteration 320/1000 | Loss: 0.00002735
Iteration 321/1000 | Loss: 0.00002735
Iteration 322/1000 | Loss: 0.00002735
Iteration 323/1000 | Loss: 0.00002735
Iteration 324/1000 | Loss: 0.00002735
Iteration 325/1000 | Loss: 0.00002735
Iteration 326/1000 | Loss: 0.00002735
Iteration 327/1000 | Loss: 0.00002735
Iteration 328/1000 | Loss: 0.00002735
Iteration 329/1000 | Loss: 0.00002735
Iteration 330/1000 | Loss: 0.00002735
Iteration 331/1000 | Loss: 0.00002734
Iteration 332/1000 | Loss: 0.00002734
Iteration 333/1000 | Loss: 0.00002733
Iteration 334/1000 | Loss: 0.00002733
Iteration 335/1000 | Loss: 0.00002733
Iteration 336/1000 | Loss: 0.00002733
Iteration 337/1000 | Loss: 0.00002733
Iteration 338/1000 | Loss: 0.00002732
Iteration 339/1000 | Loss: 0.00002732
Iteration 340/1000 | Loss: 0.00002732
Iteration 341/1000 | Loss: 0.00002732
Iteration 342/1000 | Loss: 0.00002732
Iteration 343/1000 | Loss: 0.00002732
Iteration 344/1000 | Loss: 0.00002732
Iteration 345/1000 | Loss: 0.00002732
Iteration 346/1000 | Loss: 0.00002732
Iteration 347/1000 | Loss: 0.00002732
Iteration 348/1000 | Loss: 0.00002732
Iteration 349/1000 | Loss: 0.00002732
Iteration 350/1000 | Loss: 0.00002732
Iteration 351/1000 | Loss: 0.00002732
Iteration 352/1000 | Loss: 0.00002731
Iteration 353/1000 | Loss: 0.00002731
Iteration 354/1000 | Loss: 0.00002731
Iteration 355/1000 | Loss: 0.00002731
Iteration 356/1000 | Loss: 0.00002731
Iteration 357/1000 | Loss: 0.00002731
Iteration 358/1000 | Loss: 0.00002731
Iteration 359/1000 | Loss: 0.00002731
Iteration 360/1000 | Loss: 0.00002731
Iteration 361/1000 | Loss: 0.00002731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 361. Stopping optimization.
Last 5 losses: [2.7314788894727826e-05, 2.7314788894727826e-05, 2.7314788894727826e-05, 2.7314788894727826e-05, 2.7314788894727826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7314788894727826e-05

Optimization complete. Final v2v error: 4.376686096191406 mm

Highest mean error: 5.775304794311523 mm for frame 29

Lowest mean error: 3.775726556777954 mm for frame 186

Saving results

Total time: 265.492769241333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425750
Iteration 2/25 | Loss: 0.00114716
Iteration 3/25 | Loss: 0.00089887
Iteration 4/25 | Loss: 0.00087267
Iteration 5/25 | Loss: 0.00086777
Iteration 6/25 | Loss: 0.00086576
Iteration 7/25 | Loss: 0.00086539
Iteration 8/25 | Loss: 0.00086539
Iteration 9/25 | Loss: 0.00086539
Iteration 10/25 | Loss: 0.00086539
Iteration 11/25 | Loss: 0.00086539
Iteration 12/25 | Loss: 0.00086539
Iteration 13/25 | Loss: 0.00086539
Iteration 14/25 | Loss: 0.00086539
Iteration 15/25 | Loss: 0.00086539
Iteration 16/25 | Loss: 0.00086539
Iteration 17/25 | Loss: 0.00086539
Iteration 18/25 | Loss: 0.00086539
Iteration 19/25 | Loss: 0.00086539
Iteration 20/25 | Loss: 0.00086539
Iteration 21/25 | Loss: 0.00086539
Iteration 22/25 | Loss: 0.00086539
Iteration 23/25 | Loss: 0.00086539
Iteration 24/25 | Loss: 0.00086539
Iteration 25/25 | Loss: 0.00086539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.31539726
Iteration 2/25 | Loss: 0.00065263
Iteration 3/25 | Loss: 0.00065262
Iteration 4/25 | Loss: 0.00065262
Iteration 5/25 | Loss: 0.00065262
Iteration 6/25 | Loss: 0.00065262
Iteration 7/25 | Loss: 0.00065262
Iteration 8/25 | Loss: 0.00065262
Iteration 9/25 | Loss: 0.00065261
Iteration 10/25 | Loss: 0.00065261
Iteration 11/25 | Loss: 0.00065261
Iteration 12/25 | Loss: 0.00065261
Iteration 13/25 | Loss: 0.00065261
Iteration 14/25 | Loss: 0.00065261
Iteration 15/25 | Loss: 0.00065261
Iteration 16/25 | Loss: 0.00065261
Iteration 17/25 | Loss: 0.00065261
Iteration 18/25 | Loss: 0.00065261
Iteration 19/25 | Loss: 0.00065261
Iteration 20/25 | Loss: 0.00065261
Iteration 21/25 | Loss: 0.00065261
Iteration 22/25 | Loss: 0.00065261
Iteration 23/25 | Loss: 0.00065261
Iteration 24/25 | Loss: 0.00065261
Iteration 25/25 | Loss: 0.00065261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065261
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00002247
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00001846
Iteration 7/1000 | Loss: 0.00001789
Iteration 8/1000 | Loss: 0.00001758
Iteration 9/1000 | Loss: 0.00001746
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001722
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00001701
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00001697
Iteration 17/1000 | Loss: 0.00001696
Iteration 18/1000 | Loss: 0.00001692
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001691
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001689
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001688
Iteration 29/1000 | Loss: 0.00001688
Iteration 30/1000 | Loss: 0.00001687
Iteration 31/1000 | Loss: 0.00001687
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001684
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001682
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001681
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001679
Iteration 49/1000 | Loss: 0.00001679
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001674
Iteration 54/1000 | Loss: 0.00001673
Iteration 55/1000 | Loss: 0.00001672
Iteration 56/1000 | Loss: 0.00001671
Iteration 57/1000 | Loss: 0.00001671
Iteration 58/1000 | Loss: 0.00001671
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001667
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001667
Iteration 72/1000 | Loss: 0.00001667
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001665
Iteration 83/1000 | Loss: 0.00001665
Iteration 84/1000 | Loss: 0.00001665
Iteration 85/1000 | Loss: 0.00001665
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001665
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.664941373746842e-05, 1.664941373746842e-05, 1.664941373746842e-05, 1.664941373746842e-05, 1.664941373746842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.664941373746842e-05

Optimization complete. Final v2v error: 3.487849235534668 mm

Highest mean error: 3.9214985370635986 mm for frame 72

Lowest mean error: 3.207566261291504 mm for frame 104

Saving results

Total time: 34.10894060134888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00635329
Iteration 2/25 | Loss: 0.00117703
Iteration 3/25 | Loss: 0.00095939
Iteration 4/25 | Loss: 0.00092130
Iteration 5/25 | Loss: 0.00091332
Iteration 6/25 | Loss: 0.00091242
Iteration 7/25 | Loss: 0.00091242
Iteration 8/25 | Loss: 0.00091242
Iteration 9/25 | Loss: 0.00091242
Iteration 10/25 | Loss: 0.00091242
Iteration 11/25 | Loss: 0.00091242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000912420975510031, 0.000912420975510031, 0.000912420975510031, 0.000912420975510031, 0.000912420975510031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000912420975510031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26083148
Iteration 2/25 | Loss: 0.00057779
Iteration 3/25 | Loss: 0.00057776
Iteration 4/25 | Loss: 0.00057776
Iteration 5/25 | Loss: 0.00057775
Iteration 6/25 | Loss: 0.00057775
Iteration 7/25 | Loss: 0.00057775
Iteration 8/25 | Loss: 0.00057775
Iteration 9/25 | Loss: 0.00057775
Iteration 10/25 | Loss: 0.00057775
Iteration 11/25 | Loss: 0.00057775
Iteration 12/25 | Loss: 0.00057775
Iteration 13/25 | Loss: 0.00057775
Iteration 14/25 | Loss: 0.00057775
Iteration 15/25 | Loss: 0.00057775
Iteration 16/25 | Loss: 0.00057775
Iteration 17/25 | Loss: 0.00057775
Iteration 18/25 | Loss: 0.00057775
Iteration 19/25 | Loss: 0.00057775
Iteration 20/25 | Loss: 0.00057775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005777523620054126, 0.0005777523620054126, 0.0005777523620054126, 0.0005777523620054126, 0.0005777523620054126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005777523620054126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057775
Iteration 2/1000 | Loss: 0.00003908
Iteration 3/1000 | Loss: 0.00002888
Iteration 4/1000 | Loss: 0.00002722
Iteration 5/1000 | Loss: 0.00002618
Iteration 6/1000 | Loss: 0.00002519
Iteration 7/1000 | Loss: 0.00002439
Iteration 8/1000 | Loss: 0.00002394
Iteration 9/1000 | Loss: 0.00002356
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002310
Iteration 12/1000 | Loss: 0.00002306
Iteration 13/1000 | Loss: 0.00002306
Iteration 14/1000 | Loss: 0.00002306
Iteration 15/1000 | Loss: 0.00002306
Iteration 16/1000 | Loss: 0.00002301
Iteration 17/1000 | Loss: 0.00002301
Iteration 18/1000 | Loss: 0.00002292
Iteration 19/1000 | Loss: 0.00002291
Iteration 20/1000 | Loss: 0.00002282
Iteration 21/1000 | Loss: 0.00002280
Iteration 22/1000 | Loss: 0.00002279
Iteration 23/1000 | Loss: 0.00002278
Iteration 24/1000 | Loss: 0.00002278
Iteration 25/1000 | Loss: 0.00002277
Iteration 26/1000 | Loss: 0.00002277
Iteration 27/1000 | Loss: 0.00002277
Iteration 28/1000 | Loss: 0.00002277
Iteration 29/1000 | Loss: 0.00002277
Iteration 30/1000 | Loss: 0.00002277
Iteration 31/1000 | Loss: 0.00002277
Iteration 32/1000 | Loss: 0.00002277
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002276
Iteration 35/1000 | Loss: 0.00002276
Iteration 36/1000 | Loss: 0.00002276
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002275
Iteration 39/1000 | Loss: 0.00002275
Iteration 40/1000 | Loss: 0.00002274
Iteration 41/1000 | Loss: 0.00002274
Iteration 42/1000 | Loss: 0.00002274
Iteration 43/1000 | Loss: 0.00002274
Iteration 44/1000 | Loss: 0.00002273
Iteration 45/1000 | Loss: 0.00002273
Iteration 46/1000 | Loss: 0.00002273
Iteration 47/1000 | Loss: 0.00002273
Iteration 48/1000 | Loss: 0.00002272
Iteration 49/1000 | Loss: 0.00002272
Iteration 50/1000 | Loss: 0.00002272
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002271
Iteration 53/1000 | Loss: 0.00002271
Iteration 54/1000 | Loss: 0.00002271
Iteration 55/1000 | Loss: 0.00002270
Iteration 56/1000 | Loss: 0.00002270
Iteration 57/1000 | Loss: 0.00002270
Iteration 58/1000 | Loss: 0.00002269
Iteration 59/1000 | Loss: 0.00002269
Iteration 60/1000 | Loss: 0.00002269
Iteration 61/1000 | Loss: 0.00002269
Iteration 62/1000 | Loss: 0.00002268
Iteration 63/1000 | Loss: 0.00002268
Iteration 64/1000 | Loss: 0.00002268
Iteration 65/1000 | Loss: 0.00002268
Iteration 66/1000 | Loss: 0.00002268
Iteration 67/1000 | Loss: 0.00002268
Iteration 68/1000 | Loss: 0.00002268
Iteration 69/1000 | Loss: 0.00002268
Iteration 70/1000 | Loss: 0.00002268
Iteration 71/1000 | Loss: 0.00002268
Iteration 72/1000 | Loss: 0.00002267
Iteration 73/1000 | Loss: 0.00002267
Iteration 74/1000 | Loss: 0.00002267
Iteration 75/1000 | Loss: 0.00002266
Iteration 76/1000 | Loss: 0.00002266
Iteration 77/1000 | Loss: 0.00002266
Iteration 78/1000 | Loss: 0.00002266
Iteration 79/1000 | Loss: 0.00002265
Iteration 80/1000 | Loss: 0.00002265
Iteration 81/1000 | Loss: 0.00002265
Iteration 82/1000 | Loss: 0.00002265
Iteration 83/1000 | Loss: 0.00002265
Iteration 84/1000 | Loss: 0.00002265
Iteration 85/1000 | Loss: 0.00002265
Iteration 86/1000 | Loss: 0.00002265
Iteration 87/1000 | Loss: 0.00002264
Iteration 88/1000 | Loss: 0.00002264
Iteration 89/1000 | Loss: 0.00002264
Iteration 90/1000 | Loss: 0.00002263
Iteration 91/1000 | Loss: 0.00002263
Iteration 92/1000 | Loss: 0.00002263
Iteration 93/1000 | Loss: 0.00002263
Iteration 94/1000 | Loss: 0.00002263
Iteration 95/1000 | Loss: 0.00002263
Iteration 96/1000 | Loss: 0.00002263
Iteration 97/1000 | Loss: 0.00002262
Iteration 98/1000 | Loss: 0.00002262
Iteration 99/1000 | Loss: 0.00002262
Iteration 100/1000 | Loss: 0.00002262
Iteration 101/1000 | Loss: 0.00002262
Iteration 102/1000 | Loss: 0.00002262
Iteration 103/1000 | Loss: 0.00002262
Iteration 104/1000 | Loss: 0.00002261
Iteration 105/1000 | Loss: 0.00002261
Iteration 106/1000 | Loss: 0.00002261
Iteration 107/1000 | Loss: 0.00002261
Iteration 108/1000 | Loss: 0.00002261
Iteration 109/1000 | Loss: 0.00002261
Iteration 110/1000 | Loss: 0.00002261
Iteration 111/1000 | Loss: 0.00002261
Iteration 112/1000 | Loss: 0.00002260
Iteration 113/1000 | Loss: 0.00002260
Iteration 114/1000 | Loss: 0.00002260
Iteration 115/1000 | Loss: 0.00002260
Iteration 116/1000 | Loss: 0.00002260
Iteration 117/1000 | Loss: 0.00002259
Iteration 118/1000 | Loss: 0.00002259
Iteration 119/1000 | Loss: 0.00002259
Iteration 120/1000 | Loss: 0.00002259
Iteration 121/1000 | Loss: 0.00002259
Iteration 122/1000 | Loss: 0.00002259
Iteration 123/1000 | Loss: 0.00002259
Iteration 124/1000 | Loss: 0.00002259
Iteration 125/1000 | Loss: 0.00002259
Iteration 126/1000 | Loss: 0.00002259
Iteration 127/1000 | Loss: 0.00002259
Iteration 128/1000 | Loss: 0.00002259
Iteration 129/1000 | Loss: 0.00002259
Iteration 130/1000 | Loss: 0.00002259
Iteration 131/1000 | Loss: 0.00002259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.258608947158791e-05, 2.258608947158791e-05, 2.258608947158791e-05, 2.258608947158791e-05, 2.258608947158791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.258608947158791e-05

Optimization complete. Final v2v error: 4.015671730041504 mm

Highest mean error: 4.568700313568115 mm for frame 184

Lowest mean error: 3.6960301399230957 mm for frame 135

Saving results

Total time: 40.87429237365723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473384
Iteration 2/25 | Loss: 0.00107145
Iteration 3/25 | Loss: 0.00088658
Iteration 4/25 | Loss: 0.00086358
Iteration 5/25 | Loss: 0.00085699
Iteration 6/25 | Loss: 0.00085512
Iteration 7/25 | Loss: 0.00085491
Iteration 8/25 | Loss: 0.00085491
Iteration 9/25 | Loss: 0.00085491
Iteration 10/25 | Loss: 0.00085491
Iteration 11/25 | Loss: 0.00085491
Iteration 12/25 | Loss: 0.00085491
Iteration 13/25 | Loss: 0.00085491
Iteration 14/25 | Loss: 0.00085491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008549056947231293, 0.0008549056947231293, 0.0008549056947231293, 0.0008549056947231293, 0.0008549056947231293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008549056947231293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52598882
Iteration 2/25 | Loss: 0.00060196
Iteration 3/25 | Loss: 0.00060195
Iteration 4/25 | Loss: 0.00060195
Iteration 5/25 | Loss: 0.00060195
Iteration 6/25 | Loss: 0.00060195
Iteration 7/25 | Loss: 0.00060195
Iteration 8/25 | Loss: 0.00060195
Iteration 9/25 | Loss: 0.00060195
Iteration 10/25 | Loss: 0.00060195
Iteration 11/25 | Loss: 0.00060195
Iteration 12/25 | Loss: 0.00060195
Iteration 13/25 | Loss: 0.00060195
Iteration 14/25 | Loss: 0.00060195
Iteration 15/25 | Loss: 0.00060195
Iteration 16/25 | Loss: 0.00060195
Iteration 17/25 | Loss: 0.00060195
Iteration 18/25 | Loss: 0.00060195
Iteration 19/25 | Loss: 0.00060195
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006019502761773765, 0.0006019502761773765, 0.0006019502761773765, 0.0006019502761773765, 0.0006019502761773765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006019502761773765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060195
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002011
Iteration 4/1000 | Loss: 0.00001869
Iteration 5/1000 | Loss: 0.00001777
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001691
Iteration 8/1000 | Loss: 0.00001663
Iteration 9/1000 | Loss: 0.00001643
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001619
Iteration 12/1000 | Loss: 0.00001610
Iteration 13/1000 | Loss: 0.00001600
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001587
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001562
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001560
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001560
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001559
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001558
Iteration 51/1000 | Loss: 0.00001558
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001556
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001555
Iteration 101/1000 | Loss: 0.00001555
Iteration 102/1000 | Loss: 0.00001555
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001555
Iteration 107/1000 | Loss: 0.00001555
Iteration 108/1000 | Loss: 0.00001555
Iteration 109/1000 | Loss: 0.00001555
Iteration 110/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.554554000904318e-05, 1.554554000904318e-05, 1.554554000904318e-05, 1.554554000904318e-05, 1.554554000904318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.554554000904318e-05

Optimization complete. Final v2v error: 3.301603317260742 mm

Highest mean error: 3.6068480014801025 mm for frame 51

Lowest mean error: 2.9630706310272217 mm for frame 185

Saving results

Total time: 39.90317487716675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_006/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_006/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078000
Iteration 2/25 | Loss: 0.00406724
Iteration 3/25 | Loss: 0.00272318
Iteration 4/25 | Loss: 0.00249041
Iteration 5/25 | Loss: 0.00196385
Iteration 6/25 | Loss: 0.00176230
Iteration 7/25 | Loss: 0.00161152
Iteration 8/25 | Loss: 0.00152758
Iteration 9/25 | Loss: 0.00142926
Iteration 10/25 | Loss: 0.00137763
Iteration 11/25 | Loss: 0.00133834
Iteration 12/25 | Loss: 0.00132300
Iteration 13/25 | Loss: 0.00131962
Iteration 14/25 | Loss: 0.00132104
Iteration 15/25 | Loss: 0.00129309
Iteration 16/25 | Loss: 0.00129266
Iteration 17/25 | Loss: 0.00128511
Iteration 18/25 | Loss: 0.00127560
Iteration 19/25 | Loss: 0.00127548
Iteration 20/25 | Loss: 0.00127286
Iteration 21/25 | Loss: 0.00126177
Iteration 22/25 | Loss: 0.00125883
Iteration 23/25 | Loss: 0.00125689
Iteration 24/25 | Loss: 0.00125739
Iteration 25/25 | Loss: 0.00125432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34790254
Iteration 2/25 | Loss: 0.00478443
Iteration 3/25 | Loss: 0.00428864
Iteration 4/25 | Loss: 0.00428864
Iteration 5/25 | Loss: 0.00428863
Iteration 6/25 | Loss: 0.00428863
Iteration 7/25 | Loss: 0.00428863
Iteration 8/25 | Loss: 0.00428863
Iteration 9/25 | Loss: 0.00428863
Iteration 10/25 | Loss: 0.00428863
Iteration 11/25 | Loss: 0.00428863
Iteration 12/25 | Loss: 0.00428863
Iteration 13/25 | Loss: 0.00432126
Iteration 14/25 | Loss: 0.00432126
Iteration 15/25 | Loss: 0.00432126
Iteration 16/25 | Loss: 0.00432126
Iteration 17/25 | Loss: 0.00432126
Iteration 18/25 | Loss: 0.00432126
Iteration 19/25 | Loss: 0.00432126
Iteration 20/25 | Loss: 0.00432126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004321262706071138, 0.004321262706071138, 0.004321262706071138, 0.004321262706071138, 0.004321262706071138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004321262706071138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00432126
Iteration 2/1000 | Loss: 0.01582711
Iteration 3/1000 | Loss: 0.00228891
Iteration 4/1000 | Loss: 0.00094079
Iteration 5/1000 | Loss: 0.00097375
Iteration 6/1000 | Loss: 0.00085222
Iteration 7/1000 | Loss: 0.00035793
Iteration 8/1000 | Loss: 0.00098645
Iteration 9/1000 | Loss: 0.00061911
Iteration 10/1000 | Loss: 0.00034241
Iteration 11/1000 | Loss: 0.00153389
Iteration 12/1000 | Loss: 0.00204639
Iteration 13/1000 | Loss: 0.00242337
Iteration 14/1000 | Loss: 0.00105758
Iteration 15/1000 | Loss: 0.00115354
Iteration 16/1000 | Loss: 0.00146572
Iteration 17/1000 | Loss: 0.00134323
Iteration 18/1000 | Loss: 0.00063151
Iteration 19/1000 | Loss: 0.00158705
Iteration 20/1000 | Loss: 0.00053894
Iteration 21/1000 | Loss: 0.00048360
Iteration 22/1000 | Loss: 0.00031361
Iteration 23/1000 | Loss: 0.00140387
Iteration 24/1000 | Loss: 0.00165116
Iteration 25/1000 | Loss: 0.00142776
Iteration 26/1000 | Loss: 0.00080717
Iteration 27/1000 | Loss: 0.00025809
Iteration 28/1000 | Loss: 0.00055485
Iteration 29/1000 | Loss: 0.00031423
Iteration 30/1000 | Loss: 0.00031056
Iteration 31/1000 | Loss: 0.00025158
Iteration 32/1000 | Loss: 0.00046325
Iteration 33/1000 | Loss: 0.00052171
Iteration 34/1000 | Loss: 0.00022906
Iteration 35/1000 | Loss: 0.00098427
Iteration 36/1000 | Loss: 0.00036662
Iteration 37/1000 | Loss: 0.00008934
Iteration 38/1000 | Loss: 0.00022140
Iteration 39/1000 | Loss: 0.00007531
Iteration 40/1000 | Loss: 0.00021438
Iteration 41/1000 | Loss: 0.00057547
Iteration 42/1000 | Loss: 0.00037909
Iteration 43/1000 | Loss: 0.00006630
Iteration 44/1000 | Loss: 0.00004930
Iteration 45/1000 | Loss: 0.00008215
Iteration 46/1000 | Loss: 0.00006864
Iteration 47/1000 | Loss: 0.00005045
Iteration 48/1000 | Loss: 0.00008862
Iteration 49/1000 | Loss: 0.00076995
Iteration 50/1000 | Loss: 0.00359288
Iteration 51/1000 | Loss: 0.00324057
Iteration 52/1000 | Loss: 0.00180596
Iteration 53/1000 | Loss: 0.00010868
Iteration 54/1000 | Loss: 0.00014731
Iteration 55/1000 | Loss: 0.00009731
Iteration 56/1000 | Loss: 0.00005898
Iteration 57/1000 | Loss: 0.00040184
Iteration 58/1000 | Loss: 0.00355413
Iteration 59/1000 | Loss: 0.00403271
Iteration 60/1000 | Loss: 0.00363736
Iteration 61/1000 | Loss: 0.00179921
Iteration 62/1000 | Loss: 0.00281327
Iteration 63/1000 | Loss: 0.00020520
Iteration 64/1000 | Loss: 0.00023548
Iteration 65/1000 | Loss: 0.00004821
Iteration 66/1000 | Loss: 0.00011949
Iteration 67/1000 | Loss: 0.00040602
Iteration 68/1000 | Loss: 0.00010944
Iteration 69/1000 | Loss: 0.00004511
Iteration 70/1000 | Loss: 0.00031767
Iteration 71/1000 | Loss: 0.00025726
Iteration 72/1000 | Loss: 0.00021464
Iteration 73/1000 | Loss: 0.00011839
Iteration 74/1000 | Loss: 0.00011278
Iteration 75/1000 | Loss: 0.00004102
Iteration 76/1000 | Loss: 0.00003593
Iteration 77/1000 | Loss: 0.00005849
Iteration 78/1000 | Loss: 0.00004982
Iteration 79/1000 | Loss: 0.00005250
Iteration 80/1000 | Loss: 0.00004889
Iteration 81/1000 | Loss: 0.00003411
Iteration 82/1000 | Loss: 0.00004417
Iteration 83/1000 | Loss: 0.00063770
Iteration 84/1000 | Loss: 0.00011271
Iteration 85/1000 | Loss: 0.00011784
Iteration 86/1000 | Loss: 0.00004396
Iteration 87/1000 | Loss: 0.00003614
Iteration 88/1000 | Loss: 0.00014290
Iteration 89/1000 | Loss: 0.00003470
Iteration 90/1000 | Loss: 0.00002698
Iteration 91/1000 | Loss: 0.00008392
Iteration 92/1000 | Loss: 0.00008333
Iteration 93/1000 | Loss: 0.00002484
Iteration 94/1000 | Loss: 0.00006513
Iteration 95/1000 | Loss: 0.00002861
Iteration 96/1000 | Loss: 0.00003648
Iteration 97/1000 | Loss: 0.00003268
Iteration 98/1000 | Loss: 0.00002464
Iteration 99/1000 | Loss: 0.00002352
Iteration 100/1000 | Loss: 0.00003254
Iteration 101/1000 | Loss: 0.00004563
Iteration 102/1000 | Loss: 0.00002628
Iteration 103/1000 | Loss: 0.00002693
Iteration 104/1000 | Loss: 0.00002295
Iteration 105/1000 | Loss: 0.00002295
Iteration 106/1000 | Loss: 0.00002295
Iteration 107/1000 | Loss: 0.00002295
Iteration 108/1000 | Loss: 0.00002295
Iteration 109/1000 | Loss: 0.00002295
Iteration 110/1000 | Loss: 0.00002295
Iteration 111/1000 | Loss: 0.00002295
Iteration 112/1000 | Loss: 0.00002295
Iteration 113/1000 | Loss: 0.00002295
Iteration 114/1000 | Loss: 0.00004100
Iteration 115/1000 | Loss: 0.00002290
Iteration 116/1000 | Loss: 0.00002279
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002274
Iteration 120/1000 | Loss: 0.00002272
Iteration 121/1000 | Loss: 0.00002272
Iteration 122/1000 | Loss: 0.00002272
Iteration 123/1000 | Loss: 0.00002271
Iteration 124/1000 | Loss: 0.00002271
Iteration 125/1000 | Loss: 0.00002270
Iteration 126/1000 | Loss: 0.00002269
Iteration 127/1000 | Loss: 0.00002269
Iteration 128/1000 | Loss: 0.00002268
Iteration 129/1000 | Loss: 0.00002268
Iteration 130/1000 | Loss: 0.00002265
Iteration 131/1000 | Loss: 0.00002265
Iteration 132/1000 | Loss: 0.00002265
Iteration 133/1000 | Loss: 0.00002265
Iteration 134/1000 | Loss: 0.00002265
Iteration 135/1000 | Loss: 0.00002264
Iteration 136/1000 | Loss: 0.00002264
Iteration 137/1000 | Loss: 0.00002264
Iteration 138/1000 | Loss: 0.00002264
Iteration 139/1000 | Loss: 0.00002264
Iteration 140/1000 | Loss: 0.00002264
Iteration 141/1000 | Loss: 0.00002264
Iteration 142/1000 | Loss: 0.00002264
Iteration 143/1000 | Loss: 0.00002264
Iteration 144/1000 | Loss: 0.00002264
Iteration 145/1000 | Loss: 0.00002264
Iteration 146/1000 | Loss: 0.00002263
Iteration 147/1000 | Loss: 0.00002263
Iteration 148/1000 | Loss: 0.00002263
Iteration 149/1000 | Loss: 0.00002263
Iteration 150/1000 | Loss: 0.00002263
Iteration 151/1000 | Loss: 0.00002262
Iteration 152/1000 | Loss: 0.00002262
Iteration 153/1000 | Loss: 0.00002262
Iteration 154/1000 | Loss: 0.00002262
Iteration 155/1000 | Loss: 0.00002262
Iteration 156/1000 | Loss: 0.00002262
Iteration 157/1000 | Loss: 0.00002262
Iteration 158/1000 | Loss: 0.00002262
Iteration 159/1000 | Loss: 0.00002262
Iteration 160/1000 | Loss: 0.00002262
Iteration 161/1000 | Loss: 0.00002262
Iteration 162/1000 | Loss: 0.00002261
Iteration 163/1000 | Loss: 0.00002261
Iteration 164/1000 | Loss: 0.00002261
Iteration 165/1000 | Loss: 0.00002261
Iteration 166/1000 | Loss: 0.00002260
Iteration 167/1000 | Loss: 0.00002260
Iteration 168/1000 | Loss: 0.00002260
Iteration 169/1000 | Loss: 0.00002260
Iteration 170/1000 | Loss: 0.00002260
Iteration 171/1000 | Loss: 0.00002260
Iteration 172/1000 | Loss: 0.00002260
Iteration 173/1000 | Loss: 0.00002260
Iteration 174/1000 | Loss: 0.00002260
Iteration 175/1000 | Loss: 0.00002259
Iteration 176/1000 | Loss: 0.00002259
Iteration 177/1000 | Loss: 0.00002259
Iteration 178/1000 | Loss: 0.00002259
Iteration 179/1000 | Loss: 0.00002259
Iteration 180/1000 | Loss: 0.00002259
Iteration 181/1000 | Loss: 0.00002259
Iteration 182/1000 | Loss: 0.00002259
Iteration 183/1000 | Loss: 0.00002259
Iteration 184/1000 | Loss: 0.00002258
Iteration 185/1000 | Loss: 0.00002258
Iteration 186/1000 | Loss: 0.00002258
Iteration 187/1000 | Loss: 0.00002258
Iteration 188/1000 | Loss: 0.00002258
Iteration 189/1000 | Loss: 0.00002258
Iteration 190/1000 | Loss: 0.00002258
Iteration 191/1000 | Loss: 0.00002258
Iteration 192/1000 | Loss: 0.00002258
Iteration 193/1000 | Loss: 0.00002258
Iteration 194/1000 | Loss: 0.00002258
Iteration 195/1000 | Loss: 0.00002258
Iteration 196/1000 | Loss: 0.00002258
Iteration 197/1000 | Loss: 0.00002257
Iteration 198/1000 | Loss: 0.00002257
Iteration 199/1000 | Loss: 0.00002257
Iteration 200/1000 | Loss: 0.00002257
Iteration 201/1000 | Loss: 0.00002257
Iteration 202/1000 | Loss: 0.00002257
Iteration 203/1000 | Loss: 0.00002257
Iteration 204/1000 | Loss: 0.00002256
Iteration 205/1000 | Loss: 0.00002256
Iteration 206/1000 | Loss: 0.00002256
Iteration 207/1000 | Loss: 0.00002256
Iteration 208/1000 | Loss: 0.00002256
Iteration 209/1000 | Loss: 0.00002256
Iteration 210/1000 | Loss: 0.00002256
Iteration 211/1000 | Loss: 0.00002255
Iteration 212/1000 | Loss: 0.00002254
Iteration 213/1000 | Loss: 0.00002253
Iteration 214/1000 | Loss: 0.00002253
Iteration 215/1000 | Loss: 0.00002253
Iteration 216/1000 | Loss: 0.00002252
Iteration 217/1000 | Loss: 0.00002252
Iteration 218/1000 | Loss: 0.00002252
Iteration 219/1000 | Loss: 0.00002252
Iteration 220/1000 | Loss: 0.00002252
Iteration 221/1000 | Loss: 0.00002252
Iteration 222/1000 | Loss: 0.00002252
Iteration 223/1000 | Loss: 0.00002252
Iteration 224/1000 | Loss: 0.00002252
Iteration 225/1000 | Loss: 0.00002252
Iteration 226/1000 | Loss: 0.00002252
Iteration 227/1000 | Loss: 0.00002251
Iteration 228/1000 | Loss: 0.00002251
Iteration 229/1000 | Loss: 0.00002251
Iteration 230/1000 | Loss: 0.00002251
Iteration 231/1000 | Loss: 0.00002251
Iteration 232/1000 | Loss: 0.00002251
Iteration 233/1000 | Loss: 0.00002251
Iteration 234/1000 | Loss: 0.00002251
Iteration 235/1000 | Loss: 0.00002251
Iteration 236/1000 | Loss: 0.00002251
Iteration 237/1000 | Loss: 0.00002251
Iteration 238/1000 | Loss: 0.00002251
Iteration 239/1000 | Loss: 0.00002251
Iteration 240/1000 | Loss: 0.00002250
Iteration 241/1000 | Loss: 0.00002250
Iteration 242/1000 | Loss: 0.00002250
Iteration 243/1000 | Loss: 0.00002250
Iteration 244/1000 | Loss: 0.00002250
Iteration 245/1000 | Loss: 0.00002250
Iteration 246/1000 | Loss: 0.00002250
Iteration 247/1000 | Loss: 0.00002250
Iteration 248/1000 | Loss: 0.00002250
Iteration 249/1000 | Loss: 0.00002250
Iteration 250/1000 | Loss: 0.00002250
Iteration 251/1000 | Loss: 0.00002250
Iteration 252/1000 | Loss: 0.00003572
Iteration 253/1000 | Loss: 0.00004119
Iteration 254/1000 | Loss: 0.00004119
Iteration 255/1000 | Loss: 0.00002529
Iteration 256/1000 | Loss: 0.00002256
Iteration 257/1000 | Loss: 0.00002246
Iteration 258/1000 | Loss: 0.00002245
Iteration 259/1000 | Loss: 0.00002245
Iteration 260/1000 | Loss: 0.00002245
Iteration 261/1000 | Loss: 0.00002244
Iteration 262/1000 | Loss: 0.00002244
Iteration 263/1000 | Loss: 0.00002244
Iteration 264/1000 | Loss: 0.00002244
Iteration 265/1000 | Loss: 0.00002244
Iteration 266/1000 | Loss: 0.00002243
Iteration 267/1000 | Loss: 0.00002243
Iteration 268/1000 | Loss: 0.00002243
Iteration 269/1000 | Loss: 0.00002243
Iteration 270/1000 | Loss: 0.00002243
Iteration 271/1000 | Loss: 0.00002243
Iteration 272/1000 | Loss: 0.00002243
Iteration 273/1000 | Loss: 0.00002243
Iteration 274/1000 | Loss: 0.00002243
Iteration 275/1000 | Loss: 0.00002415
Iteration 276/1000 | Loss: 0.00002284
Iteration 277/1000 | Loss: 0.00002243
Iteration 278/1000 | Loss: 0.00002243
Iteration 279/1000 | Loss: 0.00002243
Iteration 280/1000 | Loss: 0.00002243
Iteration 281/1000 | Loss: 0.00002264
Iteration 282/1000 | Loss: 0.00002244
Iteration 283/1000 | Loss: 0.00002243
Iteration 284/1000 | Loss: 0.00002243
Iteration 285/1000 | Loss: 0.00002243
Iteration 286/1000 | Loss: 0.00002243
Iteration 287/1000 | Loss: 0.00002243
Iteration 288/1000 | Loss: 0.00002243
Iteration 289/1000 | Loss: 0.00002367
Iteration 290/1000 | Loss: 0.00002269
Iteration 291/1000 | Loss: 0.00002242
Iteration 292/1000 | Loss: 0.00002242
Iteration 293/1000 | Loss: 0.00002242
Iteration 294/1000 | Loss: 0.00002242
Iteration 295/1000 | Loss: 0.00002241
Iteration 296/1000 | Loss: 0.00002241
Iteration 297/1000 | Loss: 0.00002241
Iteration 298/1000 | Loss: 0.00002241
Iteration 299/1000 | Loss: 0.00002241
Iteration 300/1000 | Loss: 0.00002241
Iteration 301/1000 | Loss: 0.00002241
Iteration 302/1000 | Loss: 0.00002241
Iteration 303/1000 | Loss: 0.00002241
Iteration 304/1000 | Loss: 0.00002241
Iteration 305/1000 | Loss: 0.00002241
Iteration 306/1000 | Loss: 0.00002241
Iteration 307/1000 | Loss: 0.00002241
Iteration 308/1000 | Loss: 0.00002241
Iteration 309/1000 | Loss: 0.00002241
Iteration 310/1000 | Loss: 0.00002241
Iteration 311/1000 | Loss: 0.00002241
Iteration 312/1000 | Loss: 0.00002241
Iteration 313/1000 | Loss: 0.00002241
Iteration 314/1000 | Loss: 0.00002241
Iteration 315/1000 | Loss: 0.00002241
Iteration 316/1000 | Loss: 0.00002241
Iteration 317/1000 | Loss: 0.00002241
Iteration 318/1000 | Loss: 0.00002241
Iteration 319/1000 | Loss: 0.00002241
Iteration 320/1000 | Loss: 0.00002241
Iteration 321/1000 | Loss: 0.00002241
Iteration 322/1000 | Loss: 0.00002241
Iteration 323/1000 | Loss: 0.00002241
Iteration 324/1000 | Loss: 0.00002241
Iteration 325/1000 | Loss: 0.00002241
Iteration 326/1000 | Loss: 0.00002241
Iteration 327/1000 | Loss: 0.00002241
Iteration 328/1000 | Loss: 0.00002241
Iteration 329/1000 | Loss: 0.00002241
Iteration 330/1000 | Loss: 0.00002241
Iteration 331/1000 | Loss: 0.00002241
Iteration 332/1000 | Loss: 0.00002241
Iteration 333/1000 | Loss: 0.00002241
Iteration 334/1000 | Loss: 0.00002241
Iteration 335/1000 | Loss: 0.00002241
Iteration 336/1000 | Loss: 0.00002241
Iteration 337/1000 | Loss: 0.00002241
Iteration 338/1000 | Loss: 0.00002241
Iteration 339/1000 | Loss: 0.00002241
Iteration 340/1000 | Loss: 0.00002241
Iteration 341/1000 | Loss: 0.00002241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [2.2410142264561728e-05, 2.2410142264561728e-05, 2.2410142264561728e-05, 2.2410142264561728e-05, 2.2410142264561728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2410142264561728e-05

Optimization complete. Final v2v error: 3.842118978500366 mm

Highest mean error: 5.674333572387695 mm for frame 62

Lowest mean error: 2.9862141609191895 mm for frame 99

Saving results

Total time: 231.21106600761414
