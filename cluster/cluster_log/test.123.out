Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=123, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6888-6943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356122
Iteration 2/25 | Loss: 0.00125480
Iteration 3/25 | Loss: 0.00111531
Iteration 4/25 | Loss: 0.00108998
Iteration 5/25 | Loss: 0.00108152
Iteration 6/25 | Loss: 0.00107944
Iteration 7/25 | Loss: 0.00107944
Iteration 8/25 | Loss: 0.00107944
Iteration 9/25 | Loss: 0.00107944
Iteration 10/25 | Loss: 0.00107944
Iteration 11/25 | Loss: 0.00107944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010794353438541293, 0.0010794353438541293, 0.0010794353438541293, 0.0010794353438541293, 0.0010794353438541293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010794353438541293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39493620
Iteration 2/25 | Loss: 0.00069902
Iteration 3/25 | Loss: 0.00069902
Iteration 4/25 | Loss: 0.00069902
Iteration 5/25 | Loss: 0.00069902
Iteration 6/25 | Loss: 0.00069902
Iteration 7/25 | Loss: 0.00069902
Iteration 8/25 | Loss: 0.00069902
Iteration 9/25 | Loss: 0.00069902
Iteration 10/25 | Loss: 0.00069902
Iteration 11/25 | Loss: 0.00069902
Iteration 12/25 | Loss: 0.00069902
Iteration 13/25 | Loss: 0.00069902
Iteration 14/25 | Loss: 0.00069902
Iteration 15/25 | Loss: 0.00069902
Iteration 16/25 | Loss: 0.00069902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000699021213222295, 0.000699021213222295, 0.000699021213222295, 0.000699021213222295, 0.000699021213222295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000699021213222295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069902
Iteration 2/1000 | Loss: 0.00005174
Iteration 3/1000 | Loss: 0.00003286
Iteration 4/1000 | Loss: 0.00002508
Iteration 5/1000 | Loss: 0.00002324
Iteration 6/1000 | Loss: 0.00002207
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002089
Iteration 9/1000 | Loss: 0.00002054
Iteration 10/1000 | Loss: 0.00002027
Iteration 11/1000 | Loss: 0.00002002
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001976
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001964
Iteration 17/1000 | Loss: 0.00001961
Iteration 18/1000 | Loss: 0.00001957
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001956
Iteration 21/1000 | Loss: 0.00001955
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001953
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001943
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001936
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00001935
Iteration 33/1000 | Loss: 0.00001934
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001926
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001925
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001921
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001919
Iteration 70/1000 | Loss: 0.00001919
Iteration 71/1000 | Loss: 0.00001919
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001918
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001908
Iteration 105/1000 | Loss: 0.00001908
Iteration 106/1000 | Loss: 0.00001908
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001907
Iteration 110/1000 | Loss: 0.00001907
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001905
Iteration 117/1000 | Loss: 0.00001905
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001905
Iteration 120/1000 | Loss: 0.00001905
Iteration 121/1000 | Loss: 0.00001905
Iteration 122/1000 | Loss: 0.00001904
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001903
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001902
Iteration 149/1000 | Loss: 0.00001902
Iteration 150/1000 | Loss: 0.00001902
Iteration 151/1000 | Loss: 0.00001902
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001902
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001902
Iteration 159/1000 | Loss: 0.00001902
Iteration 160/1000 | Loss: 0.00001902
Iteration 161/1000 | Loss: 0.00001902
Iteration 162/1000 | Loss: 0.00001902
Iteration 163/1000 | Loss: 0.00001902
Iteration 164/1000 | Loss: 0.00001902
Iteration 165/1000 | Loss: 0.00001902
Iteration 166/1000 | Loss: 0.00001902
Iteration 167/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.9020370018552057e-05, 1.9020370018552057e-05, 1.9020370018552057e-05, 1.9020370018552057e-05, 1.9020370018552057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9020370018552057e-05

Optimization complete. Final v2v error: 3.5725908279418945 mm

Highest mean error: 4.679077625274658 mm for frame 189

Lowest mean error: 2.556767702102661 mm for frame 221

Saving results

Total time: 48.64380741119385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592240
Iteration 2/25 | Loss: 0.00116667
Iteration 3/25 | Loss: 0.00108814
Iteration 4/25 | Loss: 0.00107687
Iteration 5/25 | Loss: 0.00107373
Iteration 6/25 | Loss: 0.00107371
Iteration 7/25 | Loss: 0.00107371
Iteration 8/25 | Loss: 0.00107371
Iteration 9/25 | Loss: 0.00107371
Iteration 10/25 | Loss: 0.00107371
Iteration 11/25 | Loss: 0.00107371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010737103875726461, 0.0010737103875726461, 0.0010737103875726461, 0.0010737103875726461, 0.0010737103875726461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010737103875726461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.15870929
Iteration 2/25 | Loss: 0.00056367
Iteration 3/25 | Loss: 0.00056367
Iteration 4/25 | Loss: 0.00056367
Iteration 5/25 | Loss: 0.00056367
Iteration 6/25 | Loss: 0.00056367
Iteration 7/25 | Loss: 0.00056367
Iteration 8/25 | Loss: 0.00056367
Iteration 9/25 | Loss: 0.00056367
Iteration 10/25 | Loss: 0.00056367
Iteration 11/25 | Loss: 0.00056367
Iteration 12/25 | Loss: 0.00056367
Iteration 13/25 | Loss: 0.00056367
Iteration 14/25 | Loss: 0.00056367
Iteration 15/25 | Loss: 0.00056367
Iteration 16/25 | Loss: 0.00056367
Iteration 17/25 | Loss: 0.00056367
Iteration 18/25 | Loss: 0.00056367
Iteration 19/25 | Loss: 0.00056367
Iteration 20/25 | Loss: 0.00056367
Iteration 21/25 | Loss: 0.00056367
Iteration 22/25 | Loss: 0.00056367
Iteration 23/25 | Loss: 0.00056367
Iteration 24/25 | Loss: 0.00056367
Iteration 25/25 | Loss: 0.00056367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056367
Iteration 2/1000 | Loss: 0.00002092
Iteration 3/1000 | Loss: 0.00001637
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001336
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001324
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001292
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001261
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001242
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001239
Iteration 33/1000 | Loss: 0.00001239
Iteration 34/1000 | Loss: 0.00001239
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001238
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001237
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001237
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001236
Iteration 47/1000 | Loss: 0.00001236
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001234
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001232
Iteration 65/1000 | Loss: 0.00001232
Iteration 66/1000 | Loss: 0.00001232
Iteration 67/1000 | Loss: 0.00001232
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001230
Iteration 73/1000 | Loss: 0.00001229
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001223
Iteration 91/1000 | Loss: 0.00001223
Iteration 92/1000 | Loss: 0.00001223
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001222
Iteration 95/1000 | Loss: 0.00001222
Iteration 96/1000 | Loss: 0.00001222
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001222
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.2224862075527199e-05, 1.2224862075527199e-05, 1.2224862075527199e-05, 1.2224862075527199e-05, 1.2224862075527199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2224862075527199e-05

Optimization complete. Final v2v error: 2.989865779876709 mm

Highest mean error: 3.2611398696899414 mm for frame 140

Lowest mean error: 2.8129727840423584 mm for frame 37

Saving results

Total time: 36.01387667655945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814083
Iteration 2/25 | Loss: 0.00117010
Iteration 3/25 | Loss: 0.00107033
Iteration 4/25 | Loss: 0.00105968
Iteration 5/25 | Loss: 0.00105745
Iteration 6/25 | Loss: 0.00105745
Iteration 7/25 | Loss: 0.00105745
Iteration 8/25 | Loss: 0.00105745
Iteration 9/25 | Loss: 0.00105745
Iteration 10/25 | Loss: 0.00105745
Iteration 11/25 | Loss: 0.00105745
Iteration 12/25 | Loss: 0.00105745
Iteration 13/25 | Loss: 0.00105745
Iteration 14/25 | Loss: 0.00105745
Iteration 15/25 | Loss: 0.00105745
Iteration 16/25 | Loss: 0.00105745
Iteration 17/25 | Loss: 0.00105745
Iteration 18/25 | Loss: 0.00105745
Iteration 19/25 | Loss: 0.00105745
Iteration 20/25 | Loss: 0.00105745
Iteration 21/25 | Loss: 0.00105745
Iteration 22/25 | Loss: 0.00105745
Iteration 23/25 | Loss: 0.00105745
Iteration 24/25 | Loss: 0.00105745
Iteration 25/25 | Loss: 0.00105745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40407968
Iteration 2/25 | Loss: 0.00057572
Iteration 3/25 | Loss: 0.00057572
Iteration 4/25 | Loss: 0.00057572
Iteration 5/25 | Loss: 0.00057572
Iteration 6/25 | Loss: 0.00057572
Iteration 7/25 | Loss: 0.00057572
Iteration 8/25 | Loss: 0.00057572
Iteration 9/25 | Loss: 0.00057572
Iteration 10/25 | Loss: 0.00057572
Iteration 11/25 | Loss: 0.00057572
Iteration 12/25 | Loss: 0.00057572
Iteration 13/25 | Loss: 0.00057572
Iteration 14/25 | Loss: 0.00057572
Iteration 15/25 | Loss: 0.00057572
Iteration 16/25 | Loss: 0.00057572
Iteration 17/25 | Loss: 0.00057572
Iteration 18/25 | Loss: 0.00057572
Iteration 19/25 | Loss: 0.00057572
Iteration 20/25 | Loss: 0.00057572
Iteration 21/25 | Loss: 0.00057572
Iteration 22/25 | Loss: 0.00057572
Iteration 23/25 | Loss: 0.00057572
Iteration 24/25 | Loss: 0.00057572
Iteration 25/25 | Loss: 0.00057572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057572
Iteration 2/1000 | Loss: 0.00002127
Iteration 3/1000 | Loss: 0.00001395
Iteration 4/1000 | Loss: 0.00001205
Iteration 5/1000 | Loss: 0.00001124
Iteration 6/1000 | Loss: 0.00001062
Iteration 7/1000 | Loss: 0.00001026
Iteration 8/1000 | Loss: 0.00001005
Iteration 9/1000 | Loss: 0.00001003
Iteration 10/1000 | Loss: 0.00001003
Iteration 11/1000 | Loss: 0.00000991
Iteration 12/1000 | Loss: 0.00000987
Iteration 13/1000 | Loss: 0.00000969
Iteration 14/1000 | Loss: 0.00000966
Iteration 15/1000 | Loss: 0.00000965
Iteration 16/1000 | Loss: 0.00000964
Iteration 17/1000 | Loss: 0.00000964
Iteration 18/1000 | Loss: 0.00000958
Iteration 19/1000 | Loss: 0.00000958
Iteration 20/1000 | Loss: 0.00000955
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000954
Iteration 23/1000 | Loss: 0.00000953
Iteration 24/1000 | Loss: 0.00000953
Iteration 25/1000 | Loss: 0.00000952
Iteration 26/1000 | Loss: 0.00000952
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00000950
Iteration 29/1000 | Loss: 0.00000950
Iteration 30/1000 | Loss: 0.00000950
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000949
Iteration 33/1000 | Loss: 0.00000949
Iteration 34/1000 | Loss: 0.00000948
Iteration 35/1000 | Loss: 0.00000947
Iteration 36/1000 | Loss: 0.00000947
Iteration 37/1000 | Loss: 0.00000947
Iteration 38/1000 | Loss: 0.00000946
Iteration 39/1000 | Loss: 0.00000946
Iteration 40/1000 | Loss: 0.00000946
Iteration 41/1000 | Loss: 0.00000946
Iteration 42/1000 | Loss: 0.00000946
Iteration 43/1000 | Loss: 0.00000946
Iteration 44/1000 | Loss: 0.00000946
Iteration 45/1000 | Loss: 0.00000946
Iteration 46/1000 | Loss: 0.00000946
Iteration 47/1000 | Loss: 0.00000945
Iteration 48/1000 | Loss: 0.00000945
Iteration 49/1000 | Loss: 0.00000943
Iteration 50/1000 | Loss: 0.00000943
Iteration 51/1000 | Loss: 0.00000942
Iteration 52/1000 | Loss: 0.00000942
Iteration 53/1000 | Loss: 0.00000942
Iteration 54/1000 | Loss: 0.00000941
Iteration 55/1000 | Loss: 0.00000941
Iteration 56/1000 | Loss: 0.00000941
Iteration 57/1000 | Loss: 0.00000941
Iteration 58/1000 | Loss: 0.00000941
Iteration 59/1000 | Loss: 0.00000940
Iteration 60/1000 | Loss: 0.00000940
Iteration 61/1000 | Loss: 0.00000940
Iteration 62/1000 | Loss: 0.00000940
Iteration 63/1000 | Loss: 0.00000939
Iteration 64/1000 | Loss: 0.00000939
Iteration 65/1000 | Loss: 0.00000939
Iteration 66/1000 | Loss: 0.00000938
Iteration 67/1000 | Loss: 0.00000938
Iteration 68/1000 | Loss: 0.00000938
Iteration 69/1000 | Loss: 0.00000937
Iteration 70/1000 | Loss: 0.00000937
Iteration 71/1000 | Loss: 0.00000937
Iteration 72/1000 | Loss: 0.00000937
Iteration 73/1000 | Loss: 0.00000936
Iteration 74/1000 | Loss: 0.00000936
Iteration 75/1000 | Loss: 0.00000936
Iteration 76/1000 | Loss: 0.00000936
Iteration 77/1000 | Loss: 0.00000936
Iteration 78/1000 | Loss: 0.00000936
Iteration 79/1000 | Loss: 0.00000936
Iteration 80/1000 | Loss: 0.00000935
Iteration 81/1000 | Loss: 0.00000935
Iteration 82/1000 | Loss: 0.00000935
Iteration 83/1000 | Loss: 0.00000935
Iteration 84/1000 | Loss: 0.00000935
Iteration 85/1000 | Loss: 0.00000935
Iteration 86/1000 | Loss: 0.00000935
Iteration 87/1000 | Loss: 0.00000935
Iteration 88/1000 | Loss: 0.00000934
Iteration 89/1000 | Loss: 0.00000934
Iteration 90/1000 | Loss: 0.00000933
Iteration 91/1000 | Loss: 0.00000932
Iteration 92/1000 | Loss: 0.00000932
Iteration 93/1000 | Loss: 0.00000931
Iteration 94/1000 | Loss: 0.00000931
Iteration 95/1000 | Loss: 0.00000930
Iteration 96/1000 | Loss: 0.00000930
Iteration 97/1000 | Loss: 0.00000930
Iteration 98/1000 | Loss: 0.00000930
Iteration 99/1000 | Loss: 0.00000930
Iteration 100/1000 | Loss: 0.00000930
Iteration 101/1000 | Loss: 0.00000930
Iteration 102/1000 | Loss: 0.00000930
Iteration 103/1000 | Loss: 0.00000930
Iteration 104/1000 | Loss: 0.00000930
Iteration 105/1000 | Loss: 0.00000928
Iteration 106/1000 | Loss: 0.00000927
Iteration 107/1000 | Loss: 0.00000927
Iteration 108/1000 | Loss: 0.00000927
Iteration 109/1000 | Loss: 0.00000926
Iteration 110/1000 | Loss: 0.00000926
Iteration 111/1000 | Loss: 0.00000926
Iteration 112/1000 | Loss: 0.00000926
Iteration 113/1000 | Loss: 0.00000926
Iteration 114/1000 | Loss: 0.00000926
Iteration 115/1000 | Loss: 0.00000925
Iteration 116/1000 | Loss: 0.00000925
Iteration 117/1000 | Loss: 0.00000924
Iteration 118/1000 | Loss: 0.00000923
Iteration 119/1000 | Loss: 0.00000923
Iteration 120/1000 | Loss: 0.00000923
Iteration 121/1000 | Loss: 0.00000923
Iteration 122/1000 | Loss: 0.00000923
Iteration 123/1000 | Loss: 0.00000923
Iteration 124/1000 | Loss: 0.00000923
Iteration 125/1000 | Loss: 0.00000923
Iteration 126/1000 | Loss: 0.00000923
Iteration 127/1000 | Loss: 0.00000923
Iteration 128/1000 | Loss: 0.00000922
Iteration 129/1000 | Loss: 0.00000922
Iteration 130/1000 | Loss: 0.00000922
Iteration 131/1000 | Loss: 0.00000922
Iteration 132/1000 | Loss: 0.00000922
Iteration 133/1000 | Loss: 0.00000922
Iteration 134/1000 | Loss: 0.00000921
Iteration 135/1000 | Loss: 0.00000921
Iteration 136/1000 | Loss: 0.00000920
Iteration 137/1000 | Loss: 0.00000920
Iteration 138/1000 | Loss: 0.00000920
Iteration 139/1000 | Loss: 0.00000920
Iteration 140/1000 | Loss: 0.00000920
Iteration 141/1000 | Loss: 0.00000919
Iteration 142/1000 | Loss: 0.00000919
Iteration 143/1000 | Loss: 0.00000919
Iteration 144/1000 | Loss: 0.00000918
Iteration 145/1000 | Loss: 0.00000918
Iteration 146/1000 | Loss: 0.00000918
Iteration 147/1000 | Loss: 0.00000918
Iteration 148/1000 | Loss: 0.00000918
Iteration 149/1000 | Loss: 0.00000918
Iteration 150/1000 | Loss: 0.00000917
Iteration 151/1000 | Loss: 0.00000917
Iteration 152/1000 | Loss: 0.00000917
Iteration 153/1000 | Loss: 0.00000917
Iteration 154/1000 | Loss: 0.00000917
Iteration 155/1000 | Loss: 0.00000917
Iteration 156/1000 | Loss: 0.00000917
Iteration 157/1000 | Loss: 0.00000917
Iteration 158/1000 | Loss: 0.00000916
Iteration 159/1000 | Loss: 0.00000916
Iteration 160/1000 | Loss: 0.00000916
Iteration 161/1000 | Loss: 0.00000916
Iteration 162/1000 | Loss: 0.00000916
Iteration 163/1000 | Loss: 0.00000916
Iteration 164/1000 | Loss: 0.00000915
Iteration 165/1000 | Loss: 0.00000915
Iteration 166/1000 | Loss: 0.00000915
Iteration 167/1000 | Loss: 0.00000915
Iteration 168/1000 | Loss: 0.00000915
Iteration 169/1000 | Loss: 0.00000915
Iteration 170/1000 | Loss: 0.00000915
Iteration 171/1000 | Loss: 0.00000915
Iteration 172/1000 | Loss: 0.00000915
Iteration 173/1000 | Loss: 0.00000915
Iteration 174/1000 | Loss: 0.00000915
Iteration 175/1000 | Loss: 0.00000915
Iteration 176/1000 | Loss: 0.00000915
Iteration 177/1000 | Loss: 0.00000915
Iteration 178/1000 | Loss: 0.00000915
Iteration 179/1000 | Loss: 0.00000914
Iteration 180/1000 | Loss: 0.00000914
Iteration 181/1000 | Loss: 0.00000914
Iteration 182/1000 | Loss: 0.00000914
Iteration 183/1000 | Loss: 0.00000914
Iteration 184/1000 | Loss: 0.00000914
Iteration 185/1000 | Loss: 0.00000914
Iteration 186/1000 | Loss: 0.00000914
Iteration 187/1000 | Loss: 0.00000914
Iteration 188/1000 | Loss: 0.00000914
Iteration 189/1000 | Loss: 0.00000913
Iteration 190/1000 | Loss: 0.00000913
Iteration 191/1000 | Loss: 0.00000913
Iteration 192/1000 | Loss: 0.00000913
Iteration 193/1000 | Loss: 0.00000913
Iteration 194/1000 | Loss: 0.00000913
Iteration 195/1000 | Loss: 0.00000913
Iteration 196/1000 | Loss: 0.00000913
Iteration 197/1000 | Loss: 0.00000913
Iteration 198/1000 | Loss: 0.00000913
Iteration 199/1000 | Loss: 0.00000913
Iteration 200/1000 | Loss: 0.00000913
Iteration 201/1000 | Loss: 0.00000913
Iteration 202/1000 | Loss: 0.00000913
Iteration 203/1000 | Loss: 0.00000913
Iteration 204/1000 | Loss: 0.00000913
Iteration 205/1000 | Loss: 0.00000912
Iteration 206/1000 | Loss: 0.00000912
Iteration 207/1000 | Loss: 0.00000912
Iteration 208/1000 | Loss: 0.00000912
Iteration 209/1000 | Loss: 0.00000912
Iteration 210/1000 | Loss: 0.00000912
Iteration 211/1000 | Loss: 0.00000912
Iteration 212/1000 | Loss: 0.00000912
Iteration 213/1000 | Loss: 0.00000912
Iteration 214/1000 | Loss: 0.00000911
Iteration 215/1000 | Loss: 0.00000911
Iteration 216/1000 | Loss: 0.00000911
Iteration 217/1000 | Loss: 0.00000911
Iteration 218/1000 | Loss: 0.00000910
Iteration 219/1000 | Loss: 0.00000910
Iteration 220/1000 | Loss: 0.00000910
Iteration 221/1000 | Loss: 0.00000910
Iteration 222/1000 | Loss: 0.00000910
Iteration 223/1000 | Loss: 0.00000910
Iteration 224/1000 | Loss: 0.00000909
Iteration 225/1000 | Loss: 0.00000909
Iteration 226/1000 | Loss: 0.00000909
Iteration 227/1000 | Loss: 0.00000909
Iteration 228/1000 | Loss: 0.00000909
Iteration 229/1000 | Loss: 0.00000909
Iteration 230/1000 | Loss: 0.00000909
Iteration 231/1000 | Loss: 0.00000909
Iteration 232/1000 | Loss: 0.00000909
Iteration 233/1000 | Loss: 0.00000909
Iteration 234/1000 | Loss: 0.00000909
Iteration 235/1000 | Loss: 0.00000909
Iteration 236/1000 | Loss: 0.00000909
Iteration 237/1000 | Loss: 0.00000909
Iteration 238/1000 | Loss: 0.00000909
Iteration 239/1000 | Loss: 0.00000909
Iteration 240/1000 | Loss: 0.00000909
Iteration 241/1000 | Loss: 0.00000909
Iteration 242/1000 | Loss: 0.00000909
Iteration 243/1000 | Loss: 0.00000909
Iteration 244/1000 | Loss: 0.00000909
Iteration 245/1000 | Loss: 0.00000909
Iteration 246/1000 | Loss: 0.00000909
Iteration 247/1000 | Loss: 0.00000909
Iteration 248/1000 | Loss: 0.00000909
Iteration 249/1000 | Loss: 0.00000909
Iteration 250/1000 | Loss: 0.00000909
Iteration 251/1000 | Loss: 0.00000909
Iteration 252/1000 | Loss: 0.00000909
Iteration 253/1000 | Loss: 0.00000909
Iteration 254/1000 | Loss: 0.00000909
Iteration 255/1000 | Loss: 0.00000909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [9.087788384931628e-06, 9.087788384931628e-06, 9.087788384931628e-06, 9.087788384931628e-06, 9.087788384931628e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.087788384931628e-06

Optimization complete. Final v2v error: 2.5631539821624756 mm

Highest mean error: 2.781297206878662 mm for frame 68

Lowest mean error: 2.4223520755767822 mm for frame 164

Saving results

Total time: 41.320388317108154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451440
Iteration 2/25 | Loss: 0.00118556
Iteration 3/25 | Loss: 0.00111333
Iteration 4/25 | Loss: 0.00110118
Iteration 5/25 | Loss: 0.00109734
Iteration 6/25 | Loss: 0.00109701
Iteration 7/25 | Loss: 0.00109701
Iteration 8/25 | Loss: 0.00109701
Iteration 9/25 | Loss: 0.00109701
Iteration 10/25 | Loss: 0.00109701
Iteration 11/25 | Loss: 0.00109701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001097007654607296, 0.001097007654607296, 0.001097007654607296, 0.001097007654607296, 0.001097007654607296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001097007654607296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39137626
Iteration 2/25 | Loss: 0.00057588
Iteration 3/25 | Loss: 0.00057588
Iteration 4/25 | Loss: 0.00057588
Iteration 5/25 | Loss: 0.00057588
Iteration 6/25 | Loss: 0.00057587
Iteration 7/25 | Loss: 0.00057587
Iteration 8/25 | Loss: 0.00057587
Iteration 9/25 | Loss: 0.00057587
Iteration 10/25 | Loss: 0.00057587
Iteration 11/25 | Loss: 0.00057587
Iteration 12/25 | Loss: 0.00057587
Iteration 13/25 | Loss: 0.00057587
Iteration 14/25 | Loss: 0.00057587
Iteration 15/25 | Loss: 0.00057587
Iteration 16/25 | Loss: 0.00057587
Iteration 17/25 | Loss: 0.00057587
Iteration 18/25 | Loss: 0.00057587
Iteration 19/25 | Loss: 0.00057587
Iteration 20/25 | Loss: 0.00057587
Iteration 21/25 | Loss: 0.00057587
Iteration 22/25 | Loss: 0.00057587
Iteration 23/25 | Loss: 0.00057587
Iteration 24/25 | Loss: 0.00057587
Iteration 25/25 | Loss: 0.00057587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057587
Iteration 2/1000 | Loss: 0.00002621
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001902
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001666
Iteration 11/1000 | Loss: 0.00001642
Iteration 12/1000 | Loss: 0.00001631
Iteration 13/1000 | Loss: 0.00001630
Iteration 14/1000 | Loss: 0.00001621
Iteration 15/1000 | Loss: 0.00001607
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001605
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001595
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001594
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001591
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001583
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001576
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001573
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001571
Iteration 61/1000 | Loss: 0.00001570
Iteration 62/1000 | Loss: 0.00001570
Iteration 63/1000 | Loss: 0.00001570
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001569
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001569
Iteration 68/1000 | Loss: 0.00001568
Iteration 69/1000 | Loss: 0.00001568
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001565
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001563
Iteration 82/1000 | Loss: 0.00001563
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001562
Iteration 85/1000 | Loss: 0.00001562
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001558
Iteration 101/1000 | Loss: 0.00001558
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001557
Iteration 108/1000 | Loss: 0.00001557
Iteration 109/1000 | Loss: 0.00001557
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001556
Iteration 114/1000 | Loss: 0.00001556
Iteration 115/1000 | Loss: 0.00001556
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001556
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001553
Iteration 140/1000 | Loss: 0.00001553
Iteration 141/1000 | Loss: 0.00001553
Iteration 142/1000 | Loss: 0.00001553
Iteration 143/1000 | Loss: 0.00001553
Iteration 144/1000 | Loss: 0.00001553
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001552
Iteration 147/1000 | Loss: 0.00001552
Iteration 148/1000 | Loss: 0.00001552
Iteration 149/1000 | Loss: 0.00001552
Iteration 150/1000 | Loss: 0.00001552
Iteration 151/1000 | Loss: 0.00001551
Iteration 152/1000 | Loss: 0.00001551
Iteration 153/1000 | Loss: 0.00001551
Iteration 154/1000 | Loss: 0.00001551
Iteration 155/1000 | Loss: 0.00001551
Iteration 156/1000 | Loss: 0.00001551
Iteration 157/1000 | Loss: 0.00001551
Iteration 158/1000 | Loss: 0.00001551
Iteration 159/1000 | Loss: 0.00001551
Iteration 160/1000 | Loss: 0.00001551
Iteration 161/1000 | Loss: 0.00001551
Iteration 162/1000 | Loss: 0.00001551
Iteration 163/1000 | Loss: 0.00001551
Iteration 164/1000 | Loss: 0.00001551
Iteration 165/1000 | Loss: 0.00001551
Iteration 166/1000 | Loss: 0.00001551
Iteration 167/1000 | Loss: 0.00001551
Iteration 168/1000 | Loss: 0.00001551
Iteration 169/1000 | Loss: 0.00001551
Iteration 170/1000 | Loss: 0.00001551
Iteration 171/1000 | Loss: 0.00001551
Iteration 172/1000 | Loss: 0.00001551
Iteration 173/1000 | Loss: 0.00001551
Iteration 174/1000 | Loss: 0.00001551
Iteration 175/1000 | Loss: 0.00001551
Iteration 176/1000 | Loss: 0.00001551
Iteration 177/1000 | Loss: 0.00001550
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001550
Iteration 180/1000 | Loss: 0.00001550
Iteration 181/1000 | Loss: 0.00001550
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001550
Iteration 184/1000 | Loss: 0.00001550
Iteration 185/1000 | Loss: 0.00001550
Iteration 186/1000 | Loss: 0.00001550
Iteration 187/1000 | Loss: 0.00001550
Iteration 188/1000 | Loss: 0.00001550
Iteration 189/1000 | Loss: 0.00001550
Iteration 190/1000 | Loss: 0.00001550
Iteration 191/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.5498106222366914e-05, 1.5498106222366914e-05, 1.5498106222366914e-05, 1.5498106222366914e-05, 1.5498106222366914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5498106222366914e-05

Optimization complete. Final v2v error: 3.3418428897857666 mm

Highest mean error: 3.4684760570526123 mm for frame 116

Lowest mean error: 3.177814483642578 mm for frame 53

Saving results

Total time: 40.30386686325073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957864
Iteration 2/25 | Loss: 0.00272133
Iteration 3/25 | Loss: 0.00192660
Iteration 4/25 | Loss: 0.00185593
Iteration 5/25 | Loss: 0.00183049
Iteration 6/25 | Loss: 0.00176782
Iteration 7/25 | Loss: 0.00172071
Iteration 8/25 | Loss: 0.00161798
Iteration 9/25 | Loss: 0.00156940
Iteration 10/25 | Loss: 0.00155253
Iteration 11/25 | Loss: 0.00155712
Iteration 12/25 | Loss: 0.00153028
Iteration 13/25 | Loss: 0.00151972
Iteration 14/25 | Loss: 0.00151482
Iteration 15/25 | Loss: 0.00151100
Iteration 16/25 | Loss: 0.00151722
Iteration 17/25 | Loss: 0.00151407
Iteration 18/25 | Loss: 0.00151583
Iteration 19/25 | Loss: 0.00151020
Iteration 20/25 | Loss: 0.00150968
Iteration 21/25 | Loss: 0.00150936
Iteration 22/25 | Loss: 0.00150926
Iteration 23/25 | Loss: 0.00151020
Iteration 24/25 | Loss: 0.00150965
Iteration 25/25 | Loss: 0.00151005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81050730
Iteration 2/25 | Loss: 0.00173082
Iteration 3/25 | Loss: 0.00173081
Iteration 4/25 | Loss: 0.00173081
Iteration 5/25 | Loss: 0.00173080
Iteration 6/25 | Loss: 0.00173080
Iteration 7/25 | Loss: 0.00173080
Iteration 8/25 | Loss: 0.00173080
Iteration 9/25 | Loss: 0.00173080
Iteration 10/25 | Loss: 0.00173080
Iteration 11/25 | Loss: 0.00173080
Iteration 12/25 | Loss: 0.00173080
Iteration 13/25 | Loss: 0.00173080
Iteration 14/25 | Loss: 0.00173080
Iteration 15/25 | Loss: 0.00173080
Iteration 16/25 | Loss: 0.00173080
Iteration 17/25 | Loss: 0.00173080
Iteration 18/25 | Loss: 0.00173080
Iteration 19/25 | Loss: 0.00173080
Iteration 20/25 | Loss: 0.00173080
Iteration 21/25 | Loss: 0.00173080
Iteration 22/25 | Loss: 0.00173080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017308027017861605, 0.0017308027017861605, 0.0017308027017861605, 0.0017308027017861605, 0.0017308027017861605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017308027017861605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173080
Iteration 2/1000 | Loss: 0.00024308
Iteration 3/1000 | Loss: 0.00017116
Iteration 4/1000 | Loss: 0.00015007
Iteration 5/1000 | Loss: 0.00014402
Iteration 6/1000 | Loss: 0.00013666
Iteration 7/1000 | Loss: 0.00013115
Iteration 8/1000 | Loss: 0.00012183
Iteration 9/1000 | Loss: 0.00012230
Iteration 10/1000 | Loss: 0.00011722
Iteration 11/1000 | Loss: 0.00011306
Iteration 12/1000 | Loss: 0.00011217
Iteration 13/1000 | Loss: 0.00011283
Iteration 14/1000 | Loss: 0.00011192
Iteration 15/1000 | Loss: 0.00011077
Iteration 16/1000 | Loss: 0.00011083
Iteration 17/1000 | Loss: 0.00027681
Iteration 18/1000 | Loss: 0.00562361
Iteration 19/1000 | Loss: 0.00078327
Iteration 20/1000 | Loss: 0.00030582
Iteration 21/1000 | Loss: 0.00016315
Iteration 22/1000 | Loss: 0.00012671
Iteration 23/1000 | Loss: 0.00010651
Iteration 24/1000 | Loss: 0.00009451
Iteration 25/1000 | Loss: 0.00095631
Iteration 26/1000 | Loss: 0.00054111
Iteration 27/1000 | Loss: 0.00020981
Iteration 28/1000 | Loss: 0.00013115
Iteration 29/1000 | Loss: 0.00011574
Iteration 30/1000 | Loss: 0.00007540
Iteration 31/1000 | Loss: 0.00006973
Iteration 32/1000 | Loss: 0.00019066
Iteration 33/1000 | Loss: 0.00076149
Iteration 34/1000 | Loss: 0.00006750
Iteration 35/1000 | Loss: 0.00006434
Iteration 36/1000 | Loss: 0.00006118
Iteration 37/1000 | Loss: 0.00005829
Iteration 38/1000 | Loss: 0.00005674
Iteration 39/1000 | Loss: 0.00005552
Iteration 40/1000 | Loss: 0.00005410
Iteration 41/1000 | Loss: 0.00005314
Iteration 42/1000 | Loss: 0.00005187
Iteration 43/1000 | Loss: 0.00005139
Iteration 44/1000 | Loss: 0.00005099
Iteration 45/1000 | Loss: 0.00005073
Iteration 46/1000 | Loss: 0.00005049
Iteration 47/1000 | Loss: 0.00005037
Iteration 48/1000 | Loss: 0.00005035
Iteration 49/1000 | Loss: 0.00005033
Iteration 50/1000 | Loss: 0.00005033
Iteration 51/1000 | Loss: 0.00005032
Iteration 52/1000 | Loss: 0.00005032
Iteration 53/1000 | Loss: 0.00005031
Iteration 54/1000 | Loss: 0.00005031
Iteration 55/1000 | Loss: 0.00005031
Iteration 56/1000 | Loss: 0.00005031
Iteration 57/1000 | Loss: 0.00005031
Iteration 58/1000 | Loss: 0.00005031
Iteration 59/1000 | Loss: 0.00005031
Iteration 60/1000 | Loss: 0.00005031
Iteration 61/1000 | Loss: 0.00005031
Iteration 62/1000 | Loss: 0.00005030
Iteration 63/1000 | Loss: 0.00005030
Iteration 64/1000 | Loss: 0.00005030
Iteration 65/1000 | Loss: 0.00005030
Iteration 66/1000 | Loss: 0.00005030
Iteration 67/1000 | Loss: 0.00005030
Iteration 68/1000 | Loss: 0.00005029
Iteration 69/1000 | Loss: 0.00005029
Iteration 70/1000 | Loss: 0.00005029
Iteration 71/1000 | Loss: 0.00005028
Iteration 72/1000 | Loss: 0.00005028
Iteration 73/1000 | Loss: 0.00005027
Iteration 74/1000 | Loss: 0.00005027
Iteration 75/1000 | Loss: 0.00005027
Iteration 76/1000 | Loss: 0.00005027
Iteration 77/1000 | Loss: 0.00005027
Iteration 78/1000 | Loss: 0.00005027
Iteration 79/1000 | Loss: 0.00005027
Iteration 80/1000 | Loss: 0.00005027
Iteration 81/1000 | Loss: 0.00005026
Iteration 82/1000 | Loss: 0.00005026
Iteration 83/1000 | Loss: 0.00005026
Iteration 84/1000 | Loss: 0.00005026
Iteration 85/1000 | Loss: 0.00005025
Iteration 86/1000 | Loss: 0.00005025
Iteration 87/1000 | Loss: 0.00005025
Iteration 88/1000 | Loss: 0.00005025
Iteration 89/1000 | Loss: 0.00005024
Iteration 90/1000 | Loss: 0.00005024
Iteration 91/1000 | Loss: 0.00005024
Iteration 92/1000 | Loss: 0.00005024
Iteration 93/1000 | Loss: 0.00005024
Iteration 94/1000 | Loss: 0.00005024
Iteration 95/1000 | Loss: 0.00005024
Iteration 96/1000 | Loss: 0.00005023
Iteration 97/1000 | Loss: 0.00005023
Iteration 98/1000 | Loss: 0.00005022
Iteration 99/1000 | Loss: 0.00005022
Iteration 100/1000 | Loss: 0.00005022
Iteration 101/1000 | Loss: 0.00005022
Iteration 102/1000 | Loss: 0.00005022
Iteration 103/1000 | Loss: 0.00005022
Iteration 104/1000 | Loss: 0.00005022
Iteration 105/1000 | Loss: 0.00005022
Iteration 106/1000 | Loss: 0.00005021
Iteration 107/1000 | Loss: 0.00005021
Iteration 108/1000 | Loss: 0.00005021
Iteration 109/1000 | Loss: 0.00005021
Iteration 110/1000 | Loss: 0.00005021
Iteration 111/1000 | Loss: 0.00005021
Iteration 112/1000 | Loss: 0.00005021
Iteration 113/1000 | Loss: 0.00005021
Iteration 114/1000 | Loss: 0.00005021
Iteration 115/1000 | Loss: 0.00005021
Iteration 116/1000 | Loss: 0.00005019
Iteration 117/1000 | Loss: 0.00005019
Iteration 118/1000 | Loss: 0.00005019
Iteration 119/1000 | Loss: 0.00005019
Iteration 120/1000 | Loss: 0.00005018
Iteration 121/1000 | Loss: 0.00005018
Iteration 122/1000 | Loss: 0.00005017
Iteration 123/1000 | Loss: 0.00005017
Iteration 124/1000 | Loss: 0.00005017
Iteration 125/1000 | Loss: 0.00005016
Iteration 126/1000 | Loss: 0.00005016
Iteration 127/1000 | Loss: 0.00005016
Iteration 128/1000 | Loss: 0.00005015
Iteration 129/1000 | Loss: 0.00005015
Iteration 130/1000 | Loss: 0.00005015
Iteration 131/1000 | Loss: 0.00005015
Iteration 132/1000 | Loss: 0.00005015
Iteration 133/1000 | Loss: 0.00005015
Iteration 134/1000 | Loss: 0.00005015
Iteration 135/1000 | Loss: 0.00005015
Iteration 136/1000 | Loss: 0.00005015
Iteration 137/1000 | Loss: 0.00005015
Iteration 138/1000 | Loss: 0.00005014
Iteration 139/1000 | Loss: 0.00005014
Iteration 140/1000 | Loss: 0.00005014
Iteration 141/1000 | Loss: 0.00005014
Iteration 142/1000 | Loss: 0.00005014
Iteration 143/1000 | Loss: 0.00005014
Iteration 144/1000 | Loss: 0.00005014
Iteration 145/1000 | Loss: 0.00005014
Iteration 146/1000 | Loss: 0.00005014
Iteration 147/1000 | Loss: 0.00005013
Iteration 148/1000 | Loss: 0.00005013
Iteration 149/1000 | Loss: 0.00005013
Iteration 150/1000 | Loss: 0.00005013
Iteration 151/1000 | Loss: 0.00005013
Iteration 152/1000 | Loss: 0.00005013
Iteration 153/1000 | Loss: 0.00005013
Iteration 154/1000 | Loss: 0.00005013
Iteration 155/1000 | Loss: 0.00005013
Iteration 156/1000 | Loss: 0.00005013
Iteration 157/1000 | Loss: 0.00005013
Iteration 158/1000 | Loss: 0.00005012
Iteration 159/1000 | Loss: 0.00005012
Iteration 160/1000 | Loss: 0.00005012
Iteration 161/1000 | Loss: 0.00005012
Iteration 162/1000 | Loss: 0.00005012
Iteration 163/1000 | Loss: 0.00005012
Iteration 164/1000 | Loss: 0.00005012
Iteration 165/1000 | Loss: 0.00005012
Iteration 166/1000 | Loss: 0.00005011
Iteration 167/1000 | Loss: 0.00005011
Iteration 168/1000 | Loss: 0.00005011
Iteration 169/1000 | Loss: 0.00005011
Iteration 170/1000 | Loss: 0.00005011
Iteration 171/1000 | Loss: 0.00005011
Iteration 172/1000 | Loss: 0.00005011
Iteration 173/1000 | Loss: 0.00005011
Iteration 174/1000 | Loss: 0.00005010
Iteration 175/1000 | Loss: 0.00005010
Iteration 176/1000 | Loss: 0.00005010
Iteration 177/1000 | Loss: 0.00005010
Iteration 178/1000 | Loss: 0.00005009
Iteration 179/1000 | Loss: 0.00005009
Iteration 180/1000 | Loss: 0.00005009
Iteration 181/1000 | Loss: 0.00005009
Iteration 182/1000 | Loss: 0.00005009
Iteration 183/1000 | Loss: 0.00005009
Iteration 184/1000 | Loss: 0.00005009
Iteration 185/1000 | Loss: 0.00005009
Iteration 186/1000 | Loss: 0.00005009
Iteration 187/1000 | Loss: 0.00005009
Iteration 188/1000 | Loss: 0.00005009
Iteration 189/1000 | Loss: 0.00005009
Iteration 190/1000 | Loss: 0.00005009
Iteration 191/1000 | Loss: 0.00005009
Iteration 192/1000 | Loss: 0.00005008
Iteration 193/1000 | Loss: 0.00005008
Iteration 194/1000 | Loss: 0.00005008
Iteration 195/1000 | Loss: 0.00005008
Iteration 196/1000 | Loss: 0.00005008
Iteration 197/1000 | Loss: 0.00005008
Iteration 198/1000 | Loss: 0.00005008
Iteration 199/1000 | Loss: 0.00005008
Iteration 200/1000 | Loss: 0.00005008
Iteration 201/1000 | Loss: 0.00005008
Iteration 202/1000 | Loss: 0.00005008
Iteration 203/1000 | Loss: 0.00005008
Iteration 204/1000 | Loss: 0.00005008
Iteration 205/1000 | Loss: 0.00005008
Iteration 206/1000 | Loss: 0.00005008
Iteration 207/1000 | Loss: 0.00005008
Iteration 208/1000 | Loss: 0.00005008
Iteration 209/1000 | Loss: 0.00005008
Iteration 210/1000 | Loss: 0.00005008
Iteration 211/1000 | Loss: 0.00005008
Iteration 212/1000 | Loss: 0.00005008
Iteration 213/1000 | Loss: 0.00005008
Iteration 214/1000 | Loss: 0.00005008
Iteration 215/1000 | Loss: 0.00005008
Iteration 216/1000 | Loss: 0.00005008
Iteration 217/1000 | Loss: 0.00005008
Iteration 218/1000 | Loss: 0.00005008
Iteration 219/1000 | Loss: 0.00005008
Iteration 220/1000 | Loss: 0.00005008
Iteration 221/1000 | Loss: 0.00005008
Iteration 222/1000 | Loss: 0.00005008
Iteration 223/1000 | Loss: 0.00005008
Iteration 224/1000 | Loss: 0.00005008
Iteration 225/1000 | Loss: 0.00005008
Iteration 226/1000 | Loss: 0.00005008
Iteration 227/1000 | Loss: 0.00005008
Iteration 228/1000 | Loss: 0.00005008
Iteration 229/1000 | Loss: 0.00005008
Iteration 230/1000 | Loss: 0.00005008
Iteration 231/1000 | Loss: 0.00005008
Iteration 232/1000 | Loss: 0.00005008
Iteration 233/1000 | Loss: 0.00005008
Iteration 234/1000 | Loss: 0.00005008
Iteration 235/1000 | Loss: 0.00005008
Iteration 236/1000 | Loss: 0.00005008
Iteration 237/1000 | Loss: 0.00005008
Iteration 238/1000 | Loss: 0.00005008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [5.0080510845873505e-05, 5.0080510845873505e-05, 5.0080510845873505e-05, 5.0080510845873505e-05, 5.0080510845873505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.0080510845873505e-05

Optimization complete. Final v2v error: 5.125830173492432 mm

Highest mean error: 6.588537216186523 mm for frame 132

Lowest mean error: 3.3822832107543945 mm for frame 68

Saving results

Total time: 141.21759176254272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458833
Iteration 2/25 | Loss: 0.00120684
Iteration 3/25 | Loss: 0.00109968
Iteration 4/25 | Loss: 0.00109082
Iteration 5/25 | Loss: 0.00108884
Iteration 6/25 | Loss: 0.00108884
Iteration 7/25 | Loss: 0.00108884
Iteration 8/25 | Loss: 0.00108884
Iteration 9/25 | Loss: 0.00108884
Iteration 10/25 | Loss: 0.00108884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010888392571359873, 0.0010888392571359873, 0.0010888392571359873, 0.0010888392571359873, 0.0010888392571359873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010888392571359873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35403919
Iteration 2/25 | Loss: 0.00063201
Iteration 3/25 | Loss: 0.00063200
Iteration 4/25 | Loss: 0.00063200
Iteration 5/25 | Loss: 0.00063199
Iteration 6/25 | Loss: 0.00063199
Iteration 7/25 | Loss: 0.00063199
Iteration 8/25 | Loss: 0.00063199
Iteration 9/25 | Loss: 0.00063199
Iteration 10/25 | Loss: 0.00063199
Iteration 11/25 | Loss: 0.00063199
Iteration 12/25 | Loss: 0.00063199
Iteration 13/25 | Loss: 0.00063199
Iteration 14/25 | Loss: 0.00063199
Iteration 15/25 | Loss: 0.00063199
Iteration 16/25 | Loss: 0.00063199
Iteration 17/25 | Loss: 0.00063199
Iteration 18/25 | Loss: 0.00063199
Iteration 19/25 | Loss: 0.00063199
Iteration 20/25 | Loss: 0.00063199
Iteration 21/25 | Loss: 0.00063199
Iteration 22/25 | Loss: 0.00063199
Iteration 23/25 | Loss: 0.00063199
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006319916574284434, 0.0006319916574284434, 0.0006319916574284434, 0.0006319916574284434, 0.0006319916574284434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006319916574284434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063199
Iteration 2/1000 | Loss: 0.00002261
Iteration 3/1000 | Loss: 0.00001544
Iteration 4/1000 | Loss: 0.00001348
Iteration 5/1000 | Loss: 0.00001278
Iteration 6/1000 | Loss: 0.00001232
Iteration 7/1000 | Loss: 0.00001202
Iteration 8/1000 | Loss: 0.00001184
Iteration 9/1000 | Loss: 0.00001178
Iteration 10/1000 | Loss: 0.00001154
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001131
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001125
Iteration 19/1000 | Loss: 0.00001124
Iteration 20/1000 | Loss: 0.00001124
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001118
Iteration 23/1000 | Loss: 0.00001117
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001117
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001115
Iteration 28/1000 | Loss: 0.00001115
Iteration 29/1000 | Loss: 0.00001115
Iteration 30/1000 | Loss: 0.00001114
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001113
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001111
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001107
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001099
Iteration 66/1000 | Loss: 0.00001099
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001097
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001095
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001094
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001092
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001091
Iteration 86/1000 | Loss: 0.00001091
Iteration 87/1000 | Loss: 0.00001091
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001090
Iteration 90/1000 | Loss: 0.00001090
Iteration 91/1000 | Loss: 0.00001090
Iteration 92/1000 | Loss: 0.00001089
Iteration 93/1000 | Loss: 0.00001089
Iteration 94/1000 | Loss: 0.00001088
Iteration 95/1000 | Loss: 0.00001088
Iteration 96/1000 | Loss: 0.00001088
Iteration 97/1000 | Loss: 0.00001088
Iteration 98/1000 | Loss: 0.00001088
Iteration 99/1000 | Loss: 0.00001087
Iteration 100/1000 | Loss: 0.00001087
Iteration 101/1000 | Loss: 0.00001087
Iteration 102/1000 | Loss: 0.00001087
Iteration 103/1000 | Loss: 0.00001087
Iteration 104/1000 | Loss: 0.00001087
Iteration 105/1000 | Loss: 0.00001087
Iteration 106/1000 | Loss: 0.00001087
Iteration 107/1000 | Loss: 0.00001086
Iteration 108/1000 | Loss: 0.00001086
Iteration 109/1000 | Loss: 0.00001085
Iteration 110/1000 | Loss: 0.00001085
Iteration 111/1000 | Loss: 0.00001085
Iteration 112/1000 | Loss: 0.00001085
Iteration 113/1000 | Loss: 0.00001084
Iteration 114/1000 | Loss: 0.00001084
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Iteration 118/1000 | Loss: 0.00001083
Iteration 119/1000 | Loss: 0.00001083
Iteration 120/1000 | Loss: 0.00001083
Iteration 121/1000 | Loss: 0.00001083
Iteration 122/1000 | Loss: 0.00001083
Iteration 123/1000 | Loss: 0.00001083
Iteration 124/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.0833809938048944e-05, 1.0833809938048944e-05, 1.0833809938048944e-05, 1.0833809938048944e-05, 1.0833809938048944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0833809938048944e-05

Optimization complete. Final v2v error: 2.8211607933044434 mm

Highest mean error: 3.2018415927886963 mm for frame 184

Lowest mean error: 2.551600933074951 mm for frame 166

Saving results

Total time: 36.9352171421051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726726
Iteration 2/25 | Loss: 0.00143052
Iteration 3/25 | Loss: 0.00127210
Iteration 4/25 | Loss: 0.00125743
Iteration 5/25 | Loss: 0.00125168
Iteration 6/25 | Loss: 0.00125067
Iteration 7/25 | Loss: 0.00125067
Iteration 8/25 | Loss: 0.00125067
Iteration 9/25 | Loss: 0.00125067
Iteration 10/25 | Loss: 0.00125067
Iteration 11/25 | Loss: 0.00125067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001250673783943057, 0.001250673783943057, 0.001250673783943057, 0.001250673783943057, 0.001250673783943057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001250673783943057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81542140
Iteration 2/25 | Loss: 0.00079515
Iteration 3/25 | Loss: 0.00079510
Iteration 4/25 | Loss: 0.00079510
Iteration 5/25 | Loss: 0.00079510
Iteration 6/25 | Loss: 0.00079509
Iteration 7/25 | Loss: 0.00079509
Iteration 8/25 | Loss: 0.00079509
Iteration 9/25 | Loss: 0.00079509
Iteration 10/25 | Loss: 0.00079509
Iteration 11/25 | Loss: 0.00079509
Iteration 12/25 | Loss: 0.00079509
Iteration 13/25 | Loss: 0.00079509
Iteration 14/25 | Loss: 0.00079509
Iteration 15/25 | Loss: 0.00079509
Iteration 16/25 | Loss: 0.00079509
Iteration 17/25 | Loss: 0.00079509
Iteration 18/25 | Loss: 0.00079509
Iteration 19/25 | Loss: 0.00079509
Iteration 20/25 | Loss: 0.00079509
Iteration 21/25 | Loss: 0.00079509
Iteration 22/25 | Loss: 0.00079509
Iteration 23/25 | Loss: 0.00079509
Iteration 24/25 | Loss: 0.00079509
Iteration 25/25 | Loss: 0.00079509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079509
Iteration 2/1000 | Loss: 0.00007340
Iteration 3/1000 | Loss: 0.00004663
Iteration 4/1000 | Loss: 0.00003927
Iteration 5/1000 | Loss: 0.00003653
Iteration 6/1000 | Loss: 0.00003491
Iteration 7/1000 | Loss: 0.00003418
Iteration 8/1000 | Loss: 0.00003357
Iteration 9/1000 | Loss: 0.00003297
Iteration 10/1000 | Loss: 0.00003245
Iteration 11/1000 | Loss: 0.00003211
Iteration 12/1000 | Loss: 0.00003178
Iteration 13/1000 | Loss: 0.00003149
Iteration 14/1000 | Loss: 0.00003121
Iteration 15/1000 | Loss: 0.00003101
Iteration 16/1000 | Loss: 0.00003075
Iteration 17/1000 | Loss: 0.00003067
Iteration 18/1000 | Loss: 0.00003053
Iteration 19/1000 | Loss: 0.00003052
Iteration 20/1000 | Loss: 0.00003049
Iteration 21/1000 | Loss: 0.00003048
Iteration 22/1000 | Loss: 0.00003045
Iteration 23/1000 | Loss: 0.00003044
Iteration 24/1000 | Loss: 0.00003033
Iteration 25/1000 | Loss: 0.00003028
Iteration 26/1000 | Loss: 0.00003022
Iteration 27/1000 | Loss: 0.00003015
Iteration 28/1000 | Loss: 0.00003014
Iteration 29/1000 | Loss: 0.00003010
Iteration 30/1000 | Loss: 0.00003009
Iteration 31/1000 | Loss: 0.00003009
Iteration 32/1000 | Loss: 0.00003009
Iteration 33/1000 | Loss: 0.00003009
Iteration 34/1000 | Loss: 0.00003005
Iteration 35/1000 | Loss: 0.00003005
Iteration 36/1000 | Loss: 0.00003004
Iteration 37/1000 | Loss: 0.00003003
Iteration 38/1000 | Loss: 0.00002998
Iteration 39/1000 | Loss: 0.00002997
Iteration 40/1000 | Loss: 0.00002997
Iteration 41/1000 | Loss: 0.00002995
Iteration 42/1000 | Loss: 0.00002995
Iteration 43/1000 | Loss: 0.00002995
Iteration 44/1000 | Loss: 0.00002995
Iteration 45/1000 | Loss: 0.00002995
Iteration 46/1000 | Loss: 0.00002994
Iteration 47/1000 | Loss: 0.00002994
Iteration 48/1000 | Loss: 0.00002994
Iteration 49/1000 | Loss: 0.00002994
Iteration 50/1000 | Loss: 0.00002994
Iteration 51/1000 | Loss: 0.00002994
Iteration 52/1000 | Loss: 0.00002994
Iteration 53/1000 | Loss: 0.00002993
Iteration 54/1000 | Loss: 0.00002993
Iteration 55/1000 | Loss: 0.00002993
Iteration 56/1000 | Loss: 0.00002993
Iteration 57/1000 | Loss: 0.00002993
Iteration 58/1000 | Loss: 0.00002993
Iteration 59/1000 | Loss: 0.00002993
Iteration 60/1000 | Loss: 0.00002993
Iteration 61/1000 | Loss: 0.00002992
Iteration 62/1000 | Loss: 0.00002991
Iteration 63/1000 | Loss: 0.00002990
Iteration 64/1000 | Loss: 0.00002989
Iteration 65/1000 | Loss: 0.00002987
Iteration 66/1000 | Loss: 0.00002985
Iteration 67/1000 | Loss: 0.00002985
Iteration 68/1000 | Loss: 0.00002984
Iteration 69/1000 | Loss: 0.00002984
Iteration 70/1000 | Loss: 0.00002983
Iteration 71/1000 | Loss: 0.00002983
Iteration 72/1000 | Loss: 0.00002983
Iteration 73/1000 | Loss: 0.00002983
Iteration 74/1000 | Loss: 0.00002983
Iteration 75/1000 | Loss: 0.00002983
Iteration 76/1000 | Loss: 0.00002983
Iteration 77/1000 | Loss: 0.00002983
Iteration 78/1000 | Loss: 0.00002983
Iteration 79/1000 | Loss: 0.00002983
Iteration 80/1000 | Loss: 0.00002983
Iteration 81/1000 | Loss: 0.00002983
Iteration 82/1000 | Loss: 0.00002981
Iteration 83/1000 | Loss: 0.00002981
Iteration 84/1000 | Loss: 0.00002980
Iteration 85/1000 | Loss: 0.00002979
Iteration 86/1000 | Loss: 0.00002979
Iteration 87/1000 | Loss: 0.00002978
Iteration 88/1000 | Loss: 0.00002978
Iteration 89/1000 | Loss: 0.00002977
Iteration 90/1000 | Loss: 0.00002977
Iteration 91/1000 | Loss: 0.00002977
Iteration 92/1000 | Loss: 0.00002977
Iteration 93/1000 | Loss: 0.00002976
Iteration 94/1000 | Loss: 0.00002976
Iteration 95/1000 | Loss: 0.00002976
Iteration 96/1000 | Loss: 0.00002976
Iteration 97/1000 | Loss: 0.00002975
Iteration 98/1000 | Loss: 0.00002975
Iteration 99/1000 | Loss: 0.00002974
Iteration 100/1000 | Loss: 0.00002974
Iteration 101/1000 | Loss: 0.00002974
Iteration 102/1000 | Loss: 0.00002974
Iteration 103/1000 | Loss: 0.00002974
Iteration 104/1000 | Loss: 0.00002974
Iteration 105/1000 | Loss: 0.00002974
Iteration 106/1000 | Loss: 0.00002974
Iteration 107/1000 | Loss: 0.00002973
Iteration 108/1000 | Loss: 0.00002973
Iteration 109/1000 | Loss: 0.00002973
Iteration 110/1000 | Loss: 0.00002973
Iteration 111/1000 | Loss: 0.00002973
Iteration 112/1000 | Loss: 0.00002972
Iteration 113/1000 | Loss: 0.00002972
Iteration 114/1000 | Loss: 0.00002972
Iteration 115/1000 | Loss: 0.00002972
Iteration 116/1000 | Loss: 0.00002972
Iteration 117/1000 | Loss: 0.00002972
Iteration 118/1000 | Loss: 0.00002972
Iteration 119/1000 | Loss: 0.00002972
Iteration 120/1000 | Loss: 0.00002971
Iteration 121/1000 | Loss: 0.00002971
Iteration 122/1000 | Loss: 0.00002971
Iteration 123/1000 | Loss: 0.00002970
Iteration 124/1000 | Loss: 0.00002970
Iteration 125/1000 | Loss: 0.00002970
Iteration 126/1000 | Loss: 0.00002970
Iteration 127/1000 | Loss: 0.00002970
Iteration 128/1000 | Loss: 0.00002970
Iteration 129/1000 | Loss: 0.00002969
Iteration 130/1000 | Loss: 0.00002969
Iteration 131/1000 | Loss: 0.00002969
Iteration 132/1000 | Loss: 0.00002969
Iteration 133/1000 | Loss: 0.00002968
Iteration 134/1000 | Loss: 0.00002968
Iteration 135/1000 | Loss: 0.00002968
Iteration 136/1000 | Loss: 0.00002968
Iteration 137/1000 | Loss: 0.00002967
Iteration 138/1000 | Loss: 0.00002967
Iteration 139/1000 | Loss: 0.00002967
Iteration 140/1000 | Loss: 0.00002967
Iteration 141/1000 | Loss: 0.00002967
Iteration 142/1000 | Loss: 0.00002967
Iteration 143/1000 | Loss: 0.00002967
Iteration 144/1000 | Loss: 0.00002967
Iteration 145/1000 | Loss: 0.00002967
Iteration 146/1000 | Loss: 0.00002967
Iteration 147/1000 | Loss: 0.00002966
Iteration 148/1000 | Loss: 0.00002966
Iteration 149/1000 | Loss: 0.00002966
Iteration 150/1000 | Loss: 0.00002966
Iteration 151/1000 | Loss: 0.00002966
Iteration 152/1000 | Loss: 0.00002966
Iteration 153/1000 | Loss: 0.00002966
Iteration 154/1000 | Loss: 0.00002965
Iteration 155/1000 | Loss: 0.00002965
Iteration 156/1000 | Loss: 0.00002965
Iteration 157/1000 | Loss: 0.00002965
Iteration 158/1000 | Loss: 0.00002965
Iteration 159/1000 | Loss: 0.00002965
Iteration 160/1000 | Loss: 0.00002965
Iteration 161/1000 | Loss: 0.00002965
Iteration 162/1000 | Loss: 0.00002964
Iteration 163/1000 | Loss: 0.00002964
Iteration 164/1000 | Loss: 0.00002964
Iteration 165/1000 | Loss: 0.00002964
Iteration 166/1000 | Loss: 0.00002964
Iteration 167/1000 | Loss: 0.00002964
Iteration 168/1000 | Loss: 0.00002964
Iteration 169/1000 | Loss: 0.00002964
Iteration 170/1000 | Loss: 0.00002964
Iteration 171/1000 | Loss: 0.00002963
Iteration 172/1000 | Loss: 0.00002963
Iteration 173/1000 | Loss: 0.00002963
Iteration 174/1000 | Loss: 0.00002963
Iteration 175/1000 | Loss: 0.00002963
Iteration 176/1000 | Loss: 0.00002963
Iteration 177/1000 | Loss: 0.00002963
Iteration 178/1000 | Loss: 0.00002963
Iteration 179/1000 | Loss: 0.00002963
Iteration 180/1000 | Loss: 0.00002963
Iteration 181/1000 | Loss: 0.00002962
Iteration 182/1000 | Loss: 0.00002962
Iteration 183/1000 | Loss: 0.00002962
Iteration 184/1000 | Loss: 0.00002962
Iteration 185/1000 | Loss: 0.00002962
Iteration 186/1000 | Loss: 0.00002961
Iteration 187/1000 | Loss: 0.00002961
Iteration 188/1000 | Loss: 0.00002961
Iteration 189/1000 | Loss: 0.00002961
Iteration 190/1000 | Loss: 0.00002961
Iteration 191/1000 | Loss: 0.00002961
Iteration 192/1000 | Loss: 0.00002960
Iteration 193/1000 | Loss: 0.00002960
Iteration 194/1000 | Loss: 0.00002960
Iteration 195/1000 | Loss: 0.00002959
Iteration 196/1000 | Loss: 0.00002959
Iteration 197/1000 | Loss: 0.00002959
Iteration 198/1000 | Loss: 0.00002959
Iteration 199/1000 | Loss: 0.00002959
Iteration 200/1000 | Loss: 0.00002959
Iteration 201/1000 | Loss: 0.00002959
Iteration 202/1000 | Loss: 0.00002959
Iteration 203/1000 | Loss: 0.00002959
Iteration 204/1000 | Loss: 0.00002959
Iteration 205/1000 | Loss: 0.00002959
Iteration 206/1000 | Loss: 0.00002959
Iteration 207/1000 | Loss: 0.00002959
Iteration 208/1000 | Loss: 0.00002959
Iteration 209/1000 | Loss: 0.00002959
Iteration 210/1000 | Loss: 0.00002959
Iteration 211/1000 | Loss: 0.00002958
Iteration 212/1000 | Loss: 0.00002958
Iteration 213/1000 | Loss: 0.00002958
Iteration 214/1000 | Loss: 0.00002958
Iteration 215/1000 | Loss: 0.00002958
Iteration 216/1000 | Loss: 0.00002958
Iteration 217/1000 | Loss: 0.00002958
Iteration 218/1000 | Loss: 0.00002958
Iteration 219/1000 | Loss: 0.00002958
Iteration 220/1000 | Loss: 0.00002958
Iteration 221/1000 | Loss: 0.00002958
Iteration 222/1000 | Loss: 0.00002958
Iteration 223/1000 | Loss: 0.00002958
Iteration 224/1000 | Loss: 0.00002958
Iteration 225/1000 | Loss: 0.00002958
Iteration 226/1000 | Loss: 0.00002957
Iteration 227/1000 | Loss: 0.00002957
Iteration 228/1000 | Loss: 0.00002957
Iteration 229/1000 | Loss: 0.00002957
Iteration 230/1000 | Loss: 0.00002957
Iteration 231/1000 | Loss: 0.00002957
Iteration 232/1000 | Loss: 0.00002957
Iteration 233/1000 | Loss: 0.00002957
Iteration 234/1000 | Loss: 0.00002957
Iteration 235/1000 | Loss: 0.00002957
Iteration 236/1000 | Loss: 0.00002957
Iteration 237/1000 | Loss: 0.00002957
Iteration 238/1000 | Loss: 0.00002957
Iteration 239/1000 | Loss: 0.00002957
Iteration 240/1000 | Loss: 0.00002956
Iteration 241/1000 | Loss: 0.00002956
Iteration 242/1000 | Loss: 0.00002956
Iteration 243/1000 | Loss: 0.00002956
Iteration 244/1000 | Loss: 0.00002956
Iteration 245/1000 | Loss: 0.00002956
Iteration 246/1000 | Loss: 0.00002956
Iteration 247/1000 | Loss: 0.00002956
Iteration 248/1000 | Loss: 0.00002956
Iteration 249/1000 | Loss: 0.00002956
Iteration 250/1000 | Loss: 0.00002956
Iteration 251/1000 | Loss: 0.00002956
Iteration 252/1000 | Loss: 0.00002956
Iteration 253/1000 | Loss: 0.00002956
Iteration 254/1000 | Loss: 0.00002956
Iteration 255/1000 | Loss: 0.00002956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.9561177143477835e-05, 2.9561177143477835e-05, 2.9561177143477835e-05, 2.9561177143477835e-05, 2.9561177143477835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9561177143477835e-05

Optimization complete. Final v2v error: 4.282968997955322 mm

Highest mean error: 5.731912136077881 mm for frame 126

Lowest mean error: 3.3727426528930664 mm for frame 0

Saving results

Total time: 57.340697050094604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010124
Iteration 2/25 | Loss: 0.00184431
Iteration 3/25 | Loss: 0.00126868
Iteration 4/25 | Loss: 0.00124176
Iteration 5/25 | Loss: 0.00123734
Iteration 6/25 | Loss: 0.00123602
Iteration 7/25 | Loss: 0.00123602
Iteration 8/25 | Loss: 0.00123602
Iteration 9/25 | Loss: 0.00123602
Iteration 10/25 | Loss: 0.00123602
Iteration 11/25 | Loss: 0.00123602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012360162800177932, 0.0012360162800177932, 0.0012360162800177932, 0.0012360162800177932, 0.0012360162800177932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012360162800177932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.46659902
Iteration 2/25 | Loss: 0.00067931
Iteration 3/25 | Loss: 0.00067931
Iteration 4/25 | Loss: 0.00067931
Iteration 5/25 | Loss: 0.00067931
Iteration 6/25 | Loss: 0.00067931
Iteration 7/25 | Loss: 0.00067931
Iteration 8/25 | Loss: 0.00067931
Iteration 9/25 | Loss: 0.00067931
Iteration 10/25 | Loss: 0.00067931
Iteration 11/25 | Loss: 0.00067931
Iteration 12/25 | Loss: 0.00067931
Iteration 13/25 | Loss: 0.00067931
Iteration 14/25 | Loss: 0.00067931
Iteration 15/25 | Loss: 0.00067931
Iteration 16/25 | Loss: 0.00067931
Iteration 17/25 | Loss: 0.00067931
Iteration 18/25 | Loss: 0.00067931
Iteration 19/25 | Loss: 0.00067931
Iteration 20/25 | Loss: 0.00067931
Iteration 21/25 | Loss: 0.00067931
Iteration 22/25 | Loss: 0.00067931
Iteration 23/25 | Loss: 0.00067931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006793057546019554, 0.0006793057546019554, 0.0006793057546019554, 0.0006793057546019554, 0.0006793057546019554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006793057546019554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067931
Iteration 2/1000 | Loss: 0.00004561
Iteration 3/1000 | Loss: 0.00002931
Iteration 4/1000 | Loss: 0.00002534
Iteration 5/1000 | Loss: 0.00002350
Iteration 6/1000 | Loss: 0.00002269
Iteration 7/1000 | Loss: 0.00002199
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002124
Iteration 10/1000 | Loss: 0.00002097
Iteration 11/1000 | Loss: 0.00002078
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002057
Iteration 14/1000 | Loss: 0.00002047
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002039
Iteration 17/1000 | Loss: 0.00002038
Iteration 18/1000 | Loss: 0.00002037
Iteration 19/1000 | Loss: 0.00002035
Iteration 20/1000 | Loss: 0.00002035
Iteration 21/1000 | Loss: 0.00002035
Iteration 22/1000 | Loss: 0.00002035
Iteration 23/1000 | Loss: 0.00002035
Iteration 24/1000 | Loss: 0.00002035
Iteration 25/1000 | Loss: 0.00002034
Iteration 26/1000 | Loss: 0.00002034
Iteration 27/1000 | Loss: 0.00002031
Iteration 28/1000 | Loss: 0.00002031
Iteration 29/1000 | Loss: 0.00002030
Iteration 30/1000 | Loss: 0.00002030
Iteration 31/1000 | Loss: 0.00002028
Iteration 32/1000 | Loss: 0.00002028
Iteration 33/1000 | Loss: 0.00002028
Iteration 34/1000 | Loss: 0.00002028
Iteration 35/1000 | Loss: 0.00002028
Iteration 36/1000 | Loss: 0.00002028
Iteration 37/1000 | Loss: 0.00002028
Iteration 38/1000 | Loss: 0.00002028
Iteration 39/1000 | Loss: 0.00002027
Iteration 40/1000 | Loss: 0.00002026
Iteration 41/1000 | Loss: 0.00002026
Iteration 42/1000 | Loss: 0.00002026
Iteration 43/1000 | Loss: 0.00002026
Iteration 44/1000 | Loss: 0.00002025
Iteration 45/1000 | Loss: 0.00002025
Iteration 46/1000 | Loss: 0.00002024
Iteration 47/1000 | Loss: 0.00002024
Iteration 48/1000 | Loss: 0.00002024
Iteration 49/1000 | Loss: 0.00002024
Iteration 50/1000 | Loss: 0.00002024
Iteration 51/1000 | Loss: 0.00002023
Iteration 52/1000 | Loss: 0.00002023
Iteration 53/1000 | Loss: 0.00002023
Iteration 54/1000 | Loss: 0.00002023
Iteration 55/1000 | Loss: 0.00002023
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002022
Iteration 58/1000 | Loss: 0.00002021
Iteration 59/1000 | Loss: 0.00002021
Iteration 60/1000 | Loss: 0.00002021
Iteration 61/1000 | Loss: 0.00002020
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00002019
Iteration 66/1000 | Loss: 0.00002019
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002018
Iteration 72/1000 | Loss: 0.00002018
Iteration 73/1000 | Loss: 0.00002018
Iteration 74/1000 | Loss: 0.00002018
Iteration 75/1000 | Loss: 0.00002018
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002017
Iteration 78/1000 | Loss: 0.00002017
Iteration 79/1000 | Loss: 0.00002017
Iteration 80/1000 | Loss: 0.00002017
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002017
Iteration 83/1000 | Loss: 0.00002017
Iteration 84/1000 | Loss: 0.00002017
Iteration 85/1000 | Loss: 0.00002017
Iteration 86/1000 | Loss: 0.00002017
Iteration 87/1000 | Loss: 0.00002017
Iteration 88/1000 | Loss: 0.00002017
Iteration 89/1000 | Loss: 0.00002017
Iteration 90/1000 | Loss: 0.00002017
Iteration 91/1000 | Loss: 0.00002017
Iteration 92/1000 | Loss: 0.00002017
Iteration 93/1000 | Loss: 0.00002017
Iteration 94/1000 | Loss: 0.00002017
Iteration 95/1000 | Loss: 0.00002017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.0171286450931802e-05, 2.0171286450931802e-05, 2.0171286450931802e-05, 2.0171286450931802e-05, 2.0171286450931802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0171286450931802e-05

Optimization complete. Final v2v error: 3.650176525115967 mm

Highest mean error: 4.510605812072754 mm for frame 31

Lowest mean error: 3.0618364810943604 mm for frame 0

Saving results

Total time: 38.41557312011719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814093
Iteration 2/25 | Loss: 0.00113603
Iteration 3/25 | Loss: 0.00107082
Iteration 4/25 | Loss: 0.00104770
Iteration 5/25 | Loss: 0.00104107
Iteration 6/25 | Loss: 0.00104040
Iteration 7/25 | Loss: 0.00104040
Iteration 8/25 | Loss: 0.00104040
Iteration 9/25 | Loss: 0.00104040
Iteration 10/25 | Loss: 0.00104040
Iteration 11/25 | Loss: 0.00104040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010403960477560759, 0.0010403960477560759, 0.0010403960477560759, 0.0010403960477560759, 0.0010403960477560759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010403960477560759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.51355839
Iteration 2/25 | Loss: 0.00074344
Iteration 3/25 | Loss: 0.00074339
Iteration 4/25 | Loss: 0.00074339
Iteration 5/25 | Loss: 0.00074339
Iteration 6/25 | Loss: 0.00074339
Iteration 7/25 | Loss: 0.00074339
Iteration 8/25 | Loss: 0.00074339
Iteration 9/25 | Loss: 0.00074339
Iteration 10/25 | Loss: 0.00074339
Iteration 11/25 | Loss: 0.00074339
Iteration 12/25 | Loss: 0.00074339
Iteration 13/25 | Loss: 0.00074339
Iteration 14/25 | Loss: 0.00074339
Iteration 15/25 | Loss: 0.00074339
Iteration 16/25 | Loss: 0.00074339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007433880819007754, 0.0007433880819007754, 0.0007433880819007754, 0.0007433880819007754, 0.0007433880819007754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007433880819007754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074339
Iteration 2/1000 | Loss: 0.00002622
Iteration 3/1000 | Loss: 0.00001746
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001383
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001240
Iteration 22/1000 | Loss: 0.00001240
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001238
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001234
Iteration 39/1000 | Loss: 0.00001233
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001229
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001221
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001221
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001220
Iteration 84/1000 | Loss: 0.00001220
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001220
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001220
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001219
Iteration 99/1000 | Loss: 0.00001219
Iteration 100/1000 | Loss: 0.00001219
Iteration 101/1000 | Loss: 0.00001219
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001219
Iteration 107/1000 | Loss: 0.00001218
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001218
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001218
Iteration 120/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.2175491065136157e-05, 1.2175491065136157e-05, 1.2175491065136157e-05, 1.2175491065136157e-05, 1.2175491065136157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2175491065136157e-05

Optimization complete. Final v2v error: 2.9618752002716064 mm

Highest mean error: 3.47331166267395 mm for frame 48

Lowest mean error: 2.5852043628692627 mm for frame 219

Saving results

Total time: 38.75061392784119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607626
Iteration 2/25 | Loss: 0.00175456
Iteration 3/25 | Loss: 0.00129270
Iteration 4/25 | Loss: 0.00126683
Iteration 5/25 | Loss: 0.00126291
Iteration 6/25 | Loss: 0.00126093
Iteration 7/25 | Loss: 0.00126084
Iteration 8/25 | Loss: 0.00126084
Iteration 9/25 | Loss: 0.00126084
Iteration 10/25 | Loss: 0.00126084
Iteration 11/25 | Loss: 0.00126084
Iteration 12/25 | Loss: 0.00126084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012608433607965708, 0.0012608433607965708, 0.0012608433607965708, 0.0012608433607965708, 0.0012608433607965708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012608433607965708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04060233
Iteration 2/25 | Loss: 0.00084892
Iteration 3/25 | Loss: 0.00084890
Iteration 4/25 | Loss: 0.00084890
Iteration 5/25 | Loss: 0.00084890
Iteration 6/25 | Loss: 0.00084890
Iteration 7/25 | Loss: 0.00084890
Iteration 8/25 | Loss: 0.00084890
Iteration 9/25 | Loss: 0.00084890
Iteration 10/25 | Loss: 0.00084890
Iteration 11/25 | Loss: 0.00084890
Iteration 12/25 | Loss: 0.00084890
Iteration 13/25 | Loss: 0.00084890
Iteration 14/25 | Loss: 0.00084890
Iteration 15/25 | Loss: 0.00084890
Iteration 16/25 | Loss: 0.00084890
Iteration 17/25 | Loss: 0.00084890
Iteration 18/25 | Loss: 0.00084890
Iteration 19/25 | Loss: 0.00084890
Iteration 20/25 | Loss: 0.00084890
Iteration 21/25 | Loss: 0.00084890
Iteration 22/25 | Loss: 0.00084890
Iteration 23/25 | Loss: 0.00084890
Iteration 24/25 | Loss: 0.00084890
Iteration 25/25 | Loss: 0.00084890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084890
Iteration 2/1000 | Loss: 0.00007257
Iteration 3/1000 | Loss: 0.00004974
Iteration 4/1000 | Loss: 0.00004226
Iteration 5/1000 | Loss: 0.00004000
Iteration 6/1000 | Loss: 0.00003902
Iteration 7/1000 | Loss: 0.00003840
Iteration 8/1000 | Loss: 0.00003754
Iteration 9/1000 | Loss: 0.00003676
Iteration 10/1000 | Loss: 0.00003631
Iteration 11/1000 | Loss: 0.00003594
Iteration 12/1000 | Loss: 0.00003548
Iteration 13/1000 | Loss: 0.00003504
Iteration 14/1000 | Loss: 0.00003468
Iteration 15/1000 | Loss: 0.00003436
Iteration 16/1000 | Loss: 0.00003414
Iteration 17/1000 | Loss: 0.00003397
Iteration 18/1000 | Loss: 0.00003377
Iteration 19/1000 | Loss: 0.00003358
Iteration 20/1000 | Loss: 0.00003347
Iteration 21/1000 | Loss: 0.00003344
Iteration 22/1000 | Loss: 0.00003343
Iteration 23/1000 | Loss: 0.00003337
Iteration 24/1000 | Loss: 0.00003333
Iteration 25/1000 | Loss: 0.00003333
Iteration 26/1000 | Loss: 0.00003329
Iteration 27/1000 | Loss: 0.00003329
Iteration 28/1000 | Loss: 0.00003327
Iteration 29/1000 | Loss: 0.00003324
Iteration 30/1000 | Loss: 0.00003324
Iteration 31/1000 | Loss: 0.00003323
Iteration 32/1000 | Loss: 0.00003323
Iteration 33/1000 | Loss: 0.00003322
Iteration 34/1000 | Loss: 0.00003322
Iteration 35/1000 | Loss: 0.00003322
Iteration 36/1000 | Loss: 0.00003322
Iteration 37/1000 | Loss: 0.00003322
Iteration 38/1000 | Loss: 0.00003322
Iteration 39/1000 | Loss: 0.00003322
Iteration 40/1000 | Loss: 0.00003321
Iteration 41/1000 | Loss: 0.00003321
Iteration 42/1000 | Loss: 0.00003321
Iteration 43/1000 | Loss: 0.00003321
Iteration 44/1000 | Loss: 0.00003321
Iteration 45/1000 | Loss: 0.00003321
Iteration 46/1000 | Loss: 0.00003321
Iteration 47/1000 | Loss: 0.00003321
Iteration 48/1000 | Loss: 0.00003320
Iteration 49/1000 | Loss: 0.00003320
Iteration 50/1000 | Loss: 0.00003320
Iteration 51/1000 | Loss: 0.00003320
Iteration 52/1000 | Loss: 0.00003319
Iteration 53/1000 | Loss: 0.00003319
Iteration 54/1000 | Loss: 0.00003319
Iteration 55/1000 | Loss: 0.00003319
Iteration 56/1000 | Loss: 0.00003319
Iteration 57/1000 | Loss: 0.00003319
Iteration 58/1000 | Loss: 0.00003319
Iteration 59/1000 | Loss: 0.00003319
Iteration 60/1000 | Loss: 0.00003318
Iteration 61/1000 | Loss: 0.00003318
Iteration 62/1000 | Loss: 0.00003318
Iteration 63/1000 | Loss: 0.00003318
Iteration 64/1000 | Loss: 0.00003318
Iteration 65/1000 | Loss: 0.00003318
Iteration 66/1000 | Loss: 0.00003318
Iteration 67/1000 | Loss: 0.00003318
Iteration 68/1000 | Loss: 0.00003318
Iteration 69/1000 | Loss: 0.00003318
Iteration 70/1000 | Loss: 0.00003318
Iteration 71/1000 | Loss: 0.00003317
Iteration 72/1000 | Loss: 0.00003317
Iteration 73/1000 | Loss: 0.00003317
Iteration 74/1000 | Loss: 0.00003317
Iteration 75/1000 | Loss: 0.00003317
Iteration 76/1000 | Loss: 0.00003316
Iteration 77/1000 | Loss: 0.00003316
Iteration 78/1000 | Loss: 0.00003316
Iteration 79/1000 | Loss: 0.00003316
Iteration 80/1000 | Loss: 0.00003315
Iteration 81/1000 | Loss: 0.00003315
Iteration 82/1000 | Loss: 0.00003315
Iteration 83/1000 | Loss: 0.00003315
Iteration 84/1000 | Loss: 0.00003315
Iteration 85/1000 | Loss: 0.00003315
Iteration 86/1000 | Loss: 0.00003315
Iteration 87/1000 | Loss: 0.00003315
Iteration 88/1000 | Loss: 0.00003315
Iteration 89/1000 | Loss: 0.00003314
Iteration 90/1000 | Loss: 0.00003314
Iteration 91/1000 | Loss: 0.00003314
Iteration 92/1000 | Loss: 0.00003314
Iteration 93/1000 | Loss: 0.00003314
Iteration 94/1000 | Loss: 0.00003314
Iteration 95/1000 | Loss: 0.00003314
Iteration 96/1000 | Loss: 0.00003313
Iteration 97/1000 | Loss: 0.00003313
Iteration 98/1000 | Loss: 0.00003313
Iteration 99/1000 | Loss: 0.00003313
Iteration 100/1000 | Loss: 0.00003313
Iteration 101/1000 | Loss: 0.00003312
Iteration 102/1000 | Loss: 0.00003311
Iteration 103/1000 | Loss: 0.00003311
Iteration 104/1000 | Loss: 0.00003311
Iteration 105/1000 | Loss: 0.00003311
Iteration 106/1000 | Loss: 0.00003311
Iteration 107/1000 | Loss: 0.00003311
Iteration 108/1000 | Loss: 0.00003310
Iteration 109/1000 | Loss: 0.00003310
Iteration 110/1000 | Loss: 0.00003310
Iteration 111/1000 | Loss: 0.00003309
Iteration 112/1000 | Loss: 0.00003309
Iteration 113/1000 | Loss: 0.00003309
Iteration 114/1000 | Loss: 0.00003309
Iteration 115/1000 | Loss: 0.00003308
Iteration 116/1000 | Loss: 0.00003308
Iteration 117/1000 | Loss: 0.00003308
Iteration 118/1000 | Loss: 0.00003308
Iteration 119/1000 | Loss: 0.00003308
Iteration 120/1000 | Loss: 0.00003308
Iteration 121/1000 | Loss: 0.00003308
Iteration 122/1000 | Loss: 0.00003308
Iteration 123/1000 | Loss: 0.00003308
Iteration 124/1000 | Loss: 0.00003308
Iteration 125/1000 | Loss: 0.00003308
Iteration 126/1000 | Loss: 0.00003308
Iteration 127/1000 | Loss: 0.00003308
Iteration 128/1000 | Loss: 0.00003307
Iteration 129/1000 | Loss: 0.00003307
Iteration 130/1000 | Loss: 0.00003307
Iteration 131/1000 | Loss: 0.00003307
Iteration 132/1000 | Loss: 0.00003307
Iteration 133/1000 | Loss: 0.00003307
Iteration 134/1000 | Loss: 0.00003307
Iteration 135/1000 | Loss: 0.00003307
Iteration 136/1000 | Loss: 0.00003307
Iteration 137/1000 | Loss: 0.00003307
Iteration 138/1000 | Loss: 0.00003307
Iteration 139/1000 | Loss: 0.00003307
Iteration 140/1000 | Loss: 0.00003307
Iteration 141/1000 | Loss: 0.00003306
Iteration 142/1000 | Loss: 0.00003306
Iteration 143/1000 | Loss: 0.00003306
Iteration 144/1000 | Loss: 0.00003306
Iteration 145/1000 | Loss: 0.00003306
Iteration 146/1000 | Loss: 0.00003306
Iteration 147/1000 | Loss: 0.00003306
Iteration 148/1000 | Loss: 0.00003306
Iteration 149/1000 | Loss: 0.00003306
Iteration 150/1000 | Loss: 0.00003306
Iteration 151/1000 | Loss: 0.00003306
Iteration 152/1000 | Loss: 0.00003306
Iteration 153/1000 | Loss: 0.00003306
Iteration 154/1000 | Loss: 0.00003306
Iteration 155/1000 | Loss: 0.00003306
Iteration 156/1000 | Loss: 0.00003306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [3.3060365240089595e-05, 3.3060365240089595e-05, 3.3060365240089595e-05, 3.3060365240089595e-05, 3.3060365240089595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3060365240089595e-05

Optimization complete. Final v2v error: 4.308069229125977 mm

Highest mean error: 5.421041011810303 mm for frame 149

Lowest mean error: 3.2052972316741943 mm for frame 49

Saving results

Total time: 50.49215292930603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00704486
Iteration 2/25 | Loss: 0.00129651
Iteration 3/25 | Loss: 0.00113065
Iteration 4/25 | Loss: 0.00108893
Iteration 5/25 | Loss: 0.00107779
Iteration 6/25 | Loss: 0.00107541
Iteration 7/25 | Loss: 0.00107501
Iteration 8/25 | Loss: 0.00107468
Iteration 9/25 | Loss: 0.00107450
Iteration 10/25 | Loss: 0.00107462
Iteration 11/25 | Loss: 0.00107420
Iteration 12/25 | Loss: 0.00107365
Iteration 13/25 | Loss: 0.00107340
Iteration 14/25 | Loss: 0.00107337
Iteration 15/25 | Loss: 0.00107336
Iteration 16/25 | Loss: 0.00107336
Iteration 17/25 | Loss: 0.00107336
Iteration 18/25 | Loss: 0.00107335
Iteration 19/25 | Loss: 0.00107335
Iteration 20/25 | Loss: 0.00107335
Iteration 21/25 | Loss: 0.00107335
Iteration 22/25 | Loss: 0.00107335
Iteration 23/25 | Loss: 0.00107335
Iteration 24/25 | Loss: 0.00107335
Iteration 25/25 | Loss: 0.00107335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91936100
Iteration 2/25 | Loss: 0.00066345
Iteration 3/25 | Loss: 0.00066345
Iteration 4/25 | Loss: 0.00066345
Iteration 5/25 | Loss: 0.00066344
Iteration 6/25 | Loss: 0.00066344
Iteration 7/25 | Loss: 0.00066344
Iteration 8/25 | Loss: 0.00066344
Iteration 9/25 | Loss: 0.00066344
Iteration 10/25 | Loss: 0.00066344
Iteration 11/25 | Loss: 0.00066344
Iteration 12/25 | Loss: 0.00066344
Iteration 13/25 | Loss: 0.00066344
Iteration 14/25 | Loss: 0.00066344
Iteration 15/25 | Loss: 0.00066344
Iteration 16/25 | Loss: 0.00066344
Iteration 17/25 | Loss: 0.00066344
Iteration 18/25 | Loss: 0.00066344
Iteration 19/25 | Loss: 0.00066344
Iteration 20/25 | Loss: 0.00066344
Iteration 21/25 | Loss: 0.00066344
Iteration 22/25 | Loss: 0.00066344
Iteration 23/25 | Loss: 0.00066344
Iteration 24/25 | Loss: 0.00066344
Iteration 25/25 | Loss: 0.00066344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066344
Iteration 2/1000 | Loss: 0.00001967
Iteration 3/1000 | Loss: 0.00001471
Iteration 4/1000 | Loss: 0.00006990
Iteration 5/1000 | Loss: 0.00001296
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001184
Iteration 9/1000 | Loss: 0.00001169
Iteration 10/1000 | Loss: 0.00001149
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001134
Iteration 14/1000 | Loss: 0.00001121
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001101
Iteration 18/1000 | Loss: 0.00001100
Iteration 19/1000 | Loss: 0.00001090
Iteration 20/1000 | Loss: 0.00001090
Iteration 21/1000 | Loss: 0.00001089
Iteration 22/1000 | Loss: 0.00001085
Iteration 23/1000 | Loss: 0.00001081
Iteration 24/1000 | Loss: 0.00001078
Iteration 25/1000 | Loss: 0.00001078
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001075
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001074
Iteration 31/1000 | Loss: 0.00001073
Iteration 32/1000 | Loss: 0.00001073
Iteration 33/1000 | Loss: 0.00001072
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001066
Iteration 37/1000 | Loss: 0.00001062
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001060
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001059
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001057
Iteration 48/1000 | Loss: 0.00001056
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001055
Iteration 54/1000 | Loss: 0.00001055
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001055
Iteration 57/1000 | Loss: 0.00001055
Iteration 58/1000 | Loss: 0.00001055
Iteration 59/1000 | Loss: 0.00001053
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001051
Iteration 62/1000 | Loss: 0.00001051
Iteration 63/1000 | Loss: 0.00001051
Iteration 64/1000 | Loss: 0.00001051
Iteration 65/1000 | Loss: 0.00001050
Iteration 66/1000 | Loss: 0.00001050
Iteration 67/1000 | Loss: 0.00001050
Iteration 68/1000 | Loss: 0.00001049
Iteration 69/1000 | Loss: 0.00001049
Iteration 70/1000 | Loss: 0.00001049
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001048
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001047
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001045
Iteration 81/1000 | Loss: 0.00001045
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001044
Iteration 84/1000 | Loss: 0.00001044
Iteration 85/1000 | Loss: 0.00001044
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001041
Iteration 93/1000 | Loss: 0.00001041
Iteration 94/1000 | Loss: 0.00001040
Iteration 95/1000 | Loss: 0.00001040
Iteration 96/1000 | Loss: 0.00001040
Iteration 97/1000 | Loss: 0.00001040
Iteration 98/1000 | Loss: 0.00001039
Iteration 99/1000 | Loss: 0.00001039
Iteration 100/1000 | Loss: 0.00001039
Iteration 101/1000 | Loss: 0.00001039
Iteration 102/1000 | Loss: 0.00001039
Iteration 103/1000 | Loss: 0.00001039
Iteration 104/1000 | Loss: 0.00001039
Iteration 105/1000 | Loss: 0.00001039
Iteration 106/1000 | Loss: 0.00001039
Iteration 107/1000 | Loss: 0.00001038
Iteration 108/1000 | Loss: 0.00001038
Iteration 109/1000 | Loss: 0.00001038
Iteration 110/1000 | Loss: 0.00001038
Iteration 111/1000 | Loss: 0.00001037
Iteration 112/1000 | Loss: 0.00001037
Iteration 113/1000 | Loss: 0.00001037
Iteration 114/1000 | Loss: 0.00001037
Iteration 115/1000 | Loss: 0.00001036
Iteration 116/1000 | Loss: 0.00001036
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001035
Iteration 120/1000 | Loss: 0.00001035
Iteration 121/1000 | Loss: 0.00001035
Iteration 122/1000 | Loss: 0.00001035
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001034
Iteration 126/1000 | Loss: 0.00001034
Iteration 127/1000 | Loss: 0.00001034
Iteration 128/1000 | Loss: 0.00001034
Iteration 129/1000 | Loss: 0.00001034
Iteration 130/1000 | Loss: 0.00001034
Iteration 131/1000 | Loss: 0.00001034
Iteration 132/1000 | Loss: 0.00001034
Iteration 133/1000 | Loss: 0.00001034
Iteration 134/1000 | Loss: 0.00001034
Iteration 135/1000 | Loss: 0.00001034
Iteration 136/1000 | Loss: 0.00001034
Iteration 137/1000 | Loss: 0.00001034
Iteration 138/1000 | Loss: 0.00001034
Iteration 139/1000 | Loss: 0.00001034
Iteration 140/1000 | Loss: 0.00001033
Iteration 141/1000 | Loss: 0.00001033
Iteration 142/1000 | Loss: 0.00001033
Iteration 143/1000 | Loss: 0.00001033
Iteration 144/1000 | Loss: 0.00001033
Iteration 145/1000 | Loss: 0.00001033
Iteration 146/1000 | Loss: 0.00001033
Iteration 147/1000 | Loss: 0.00001033
Iteration 148/1000 | Loss: 0.00001033
Iteration 149/1000 | Loss: 0.00001033
Iteration 150/1000 | Loss: 0.00001033
Iteration 151/1000 | Loss: 0.00001033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.0334721991966944e-05, 1.0334721991966944e-05, 1.0334721991966944e-05, 1.0334721991966944e-05, 1.0334721991966944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0334721991966944e-05

Optimization complete. Final v2v error: 2.756648302078247 mm

Highest mean error: 3.0916361808776855 mm for frame 126

Lowest mean error: 2.549931764602661 mm for frame 261

Saving results

Total time: 62.50737237930298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00333101
Iteration 2/25 | Loss: 0.00123913
Iteration 3/25 | Loss: 0.00110446
Iteration 4/25 | Loss: 0.00108299
Iteration 5/25 | Loss: 0.00107697
Iteration 6/25 | Loss: 0.00107516
Iteration 7/25 | Loss: 0.00107516
Iteration 8/25 | Loss: 0.00107516
Iteration 9/25 | Loss: 0.00107516
Iteration 10/25 | Loss: 0.00107516
Iteration 11/25 | Loss: 0.00107516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010751602239906788, 0.0010751602239906788, 0.0010751602239906788, 0.0010751602239906788, 0.0010751602239906788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010751602239906788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34486425
Iteration 2/25 | Loss: 0.00063631
Iteration 3/25 | Loss: 0.00063631
Iteration 4/25 | Loss: 0.00063631
Iteration 5/25 | Loss: 0.00063631
Iteration 6/25 | Loss: 0.00063631
Iteration 7/25 | Loss: 0.00063631
Iteration 8/25 | Loss: 0.00063631
Iteration 9/25 | Loss: 0.00063631
Iteration 10/25 | Loss: 0.00063631
Iteration 11/25 | Loss: 0.00063631
Iteration 12/25 | Loss: 0.00063631
Iteration 13/25 | Loss: 0.00063631
Iteration 14/25 | Loss: 0.00063631
Iteration 15/25 | Loss: 0.00063631
Iteration 16/25 | Loss: 0.00063631
Iteration 17/25 | Loss: 0.00063631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006363075808621943, 0.0006363075808621943, 0.0006363075808621943, 0.0006363075808621943, 0.0006363075808621943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006363075808621943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063631
Iteration 2/1000 | Loss: 0.00003783
Iteration 3/1000 | Loss: 0.00002301
Iteration 4/1000 | Loss: 0.00002020
Iteration 5/1000 | Loss: 0.00001903
Iteration 6/1000 | Loss: 0.00001813
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001634
Iteration 12/1000 | Loss: 0.00001631
Iteration 13/1000 | Loss: 0.00001625
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001599
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001598
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001597
Iteration 39/1000 | Loss: 0.00001596
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001595
Iteration 44/1000 | Loss: 0.00001595
Iteration 45/1000 | Loss: 0.00001595
Iteration 46/1000 | Loss: 0.00001595
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [1.594629247847479e-05, 1.594629247847479e-05, 1.594629247847479e-05, 1.594629247847479e-05, 1.594629247847479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.594629247847479e-05

Optimization complete. Final v2v error: 3.3793230056762695 mm

Highest mean error: 4.108899116516113 mm for frame 210

Lowest mean error: 2.6248881816864014 mm for frame 49

Saving results

Total time: 34.37607932090759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387406
Iteration 2/25 | Loss: 0.00116309
Iteration 3/25 | Loss: 0.00108602
Iteration 4/25 | Loss: 0.00108043
Iteration 5/25 | Loss: 0.00107844
Iteration 6/25 | Loss: 0.00107844
Iteration 7/25 | Loss: 0.00107844
Iteration 8/25 | Loss: 0.00107844
Iteration 9/25 | Loss: 0.00107844
Iteration 10/25 | Loss: 0.00107844
Iteration 11/25 | Loss: 0.00107844
Iteration 12/25 | Loss: 0.00107844
Iteration 13/25 | Loss: 0.00107844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010784417390823364, 0.0010784417390823364, 0.0010784417390823364, 0.0010784417390823364, 0.0010784417390823364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010784417390823364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53847051
Iteration 2/25 | Loss: 0.00056529
Iteration 3/25 | Loss: 0.00056528
Iteration 4/25 | Loss: 0.00056528
Iteration 5/25 | Loss: 0.00056528
Iteration 6/25 | Loss: 0.00056528
Iteration 7/25 | Loss: 0.00056528
Iteration 8/25 | Loss: 0.00056528
Iteration 9/25 | Loss: 0.00056528
Iteration 10/25 | Loss: 0.00056528
Iteration 11/25 | Loss: 0.00056528
Iteration 12/25 | Loss: 0.00056528
Iteration 13/25 | Loss: 0.00056528
Iteration 14/25 | Loss: 0.00056528
Iteration 15/25 | Loss: 0.00056528
Iteration 16/25 | Loss: 0.00056528
Iteration 17/25 | Loss: 0.00056528
Iteration 18/25 | Loss: 0.00056528
Iteration 19/25 | Loss: 0.00056528
Iteration 20/25 | Loss: 0.00056528
Iteration 21/25 | Loss: 0.00056528
Iteration 22/25 | Loss: 0.00056528
Iteration 23/25 | Loss: 0.00056528
Iteration 24/25 | Loss: 0.00056528
Iteration 25/25 | Loss: 0.00056528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056528
Iteration 2/1000 | Loss: 0.00002028
Iteration 3/1000 | Loss: 0.00001391
Iteration 4/1000 | Loss: 0.00001248
Iteration 5/1000 | Loss: 0.00001185
Iteration 6/1000 | Loss: 0.00001138
Iteration 7/1000 | Loss: 0.00001102
Iteration 8/1000 | Loss: 0.00001075
Iteration 9/1000 | Loss: 0.00001040
Iteration 10/1000 | Loss: 0.00001024
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001018
Iteration 13/1000 | Loss: 0.00001017
Iteration 14/1000 | Loss: 0.00001015
Iteration 15/1000 | Loss: 0.00001014
Iteration 16/1000 | Loss: 0.00001005
Iteration 17/1000 | Loss: 0.00001002
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000993
Iteration 25/1000 | Loss: 0.00000993
Iteration 26/1000 | Loss: 0.00000993
Iteration 27/1000 | Loss: 0.00000992
Iteration 28/1000 | Loss: 0.00000992
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000992
Iteration 32/1000 | Loss: 0.00000991
Iteration 33/1000 | Loss: 0.00000990
Iteration 34/1000 | Loss: 0.00000989
Iteration 35/1000 | Loss: 0.00000989
Iteration 36/1000 | Loss: 0.00000988
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000988
Iteration 40/1000 | Loss: 0.00000988
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000988
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000987
Iteration 46/1000 | Loss: 0.00000987
Iteration 47/1000 | Loss: 0.00000986
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000985
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000985
Iteration 53/1000 | Loss: 0.00000985
Iteration 54/1000 | Loss: 0.00000985
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000982
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000982
Iteration 61/1000 | Loss: 0.00000982
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000982
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000982
Iteration 67/1000 | Loss: 0.00000981
Iteration 68/1000 | Loss: 0.00000981
Iteration 69/1000 | Loss: 0.00000981
Iteration 70/1000 | Loss: 0.00000980
Iteration 71/1000 | Loss: 0.00000980
Iteration 72/1000 | Loss: 0.00000979
Iteration 73/1000 | Loss: 0.00000978
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000976
Iteration 76/1000 | Loss: 0.00000976
Iteration 77/1000 | Loss: 0.00000975
Iteration 78/1000 | Loss: 0.00000975
Iteration 79/1000 | Loss: 0.00000975
Iteration 80/1000 | Loss: 0.00000974
Iteration 81/1000 | Loss: 0.00000974
Iteration 82/1000 | Loss: 0.00000974
Iteration 83/1000 | Loss: 0.00000973
Iteration 84/1000 | Loss: 0.00000973
Iteration 85/1000 | Loss: 0.00000973
Iteration 86/1000 | Loss: 0.00000973
Iteration 87/1000 | Loss: 0.00000972
Iteration 88/1000 | Loss: 0.00000972
Iteration 89/1000 | Loss: 0.00000972
Iteration 90/1000 | Loss: 0.00000972
Iteration 91/1000 | Loss: 0.00000972
Iteration 92/1000 | Loss: 0.00000971
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000970
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000970
Iteration 102/1000 | Loss: 0.00000969
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000969
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000969
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000968
Iteration 116/1000 | Loss: 0.00000968
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000967
Iteration 121/1000 | Loss: 0.00000967
Iteration 122/1000 | Loss: 0.00000967
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000966
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000966
Iteration 128/1000 | Loss: 0.00000965
Iteration 129/1000 | Loss: 0.00000965
Iteration 130/1000 | Loss: 0.00000965
Iteration 131/1000 | Loss: 0.00000965
Iteration 132/1000 | Loss: 0.00000964
Iteration 133/1000 | Loss: 0.00000964
Iteration 134/1000 | Loss: 0.00000964
Iteration 135/1000 | Loss: 0.00000964
Iteration 136/1000 | Loss: 0.00000964
Iteration 137/1000 | Loss: 0.00000964
Iteration 138/1000 | Loss: 0.00000964
Iteration 139/1000 | Loss: 0.00000964
Iteration 140/1000 | Loss: 0.00000964
Iteration 141/1000 | Loss: 0.00000964
Iteration 142/1000 | Loss: 0.00000964
Iteration 143/1000 | Loss: 0.00000964
Iteration 144/1000 | Loss: 0.00000964
Iteration 145/1000 | Loss: 0.00000964
Iteration 146/1000 | Loss: 0.00000964
Iteration 147/1000 | Loss: 0.00000964
Iteration 148/1000 | Loss: 0.00000964
Iteration 149/1000 | Loss: 0.00000964
Iteration 150/1000 | Loss: 0.00000964
Iteration 151/1000 | Loss: 0.00000964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [9.636818504077382e-06, 9.636818504077382e-06, 9.636818504077382e-06, 9.636818504077382e-06, 9.636818504077382e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.636818504077382e-06

Optimization complete. Final v2v error: 2.6654651165008545 mm

Highest mean error: 2.808870315551758 mm for frame 104

Lowest mean error: 2.5180201530456543 mm for frame 0

Saving results

Total time: 39.68755578994751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752644
Iteration 2/25 | Loss: 0.00165932
Iteration 3/25 | Loss: 0.00128647
Iteration 4/25 | Loss: 0.00116566
Iteration 5/25 | Loss: 0.00115411
Iteration 6/25 | Loss: 0.00113724
Iteration 7/25 | Loss: 0.00112704
Iteration 8/25 | Loss: 0.00111542
Iteration 9/25 | Loss: 0.00110786
Iteration 10/25 | Loss: 0.00110505
Iteration 11/25 | Loss: 0.00110380
Iteration 12/25 | Loss: 0.00109824
Iteration 13/25 | Loss: 0.00109520
Iteration 14/25 | Loss: 0.00109491
Iteration 15/25 | Loss: 0.00109488
Iteration 16/25 | Loss: 0.00109488
Iteration 17/25 | Loss: 0.00109488
Iteration 18/25 | Loss: 0.00109488
Iteration 19/25 | Loss: 0.00109488
Iteration 20/25 | Loss: 0.00109488
Iteration 21/25 | Loss: 0.00109488
Iteration 22/25 | Loss: 0.00109488
Iteration 23/25 | Loss: 0.00109488
Iteration 24/25 | Loss: 0.00109488
Iteration 25/25 | Loss: 0.00109488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.73584175
Iteration 2/25 | Loss: 0.00065870
Iteration 3/25 | Loss: 0.00065868
Iteration 4/25 | Loss: 0.00065868
Iteration 5/25 | Loss: 0.00065867
Iteration 6/25 | Loss: 0.00065867
Iteration 7/25 | Loss: 0.00065867
Iteration 8/25 | Loss: 0.00065867
Iteration 9/25 | Loss: 0.00065867
Iteration 10/25 | Loss: 0.00065867
Iteration 11/25 | Loss: 0.00065867
Iteration 12/25 | Loss: 0.00065867
Iteration 13/25 | Loss: 0.00065867
Iteration 14/25 | Loss: 0.00065867
Iteration 15/25 | Loss: 0.00065867
Iteration 16/25 | Loss: 0.00065867
Iteration 17/25 | Loss: 0.00065867
Iteration 18/25 | Loss: 0.00065867
Iteration 19/25 | Loss: 0.00065867
Iteration 20/25 | Loss: 0.00065867
Iteration 21/25 | Loss: 0.00065867
Iteration 22/25 | Loss: 0.00065867
Iteration 23/25 | Loss: 0.00065867
Iteration 24/25 | Loss: 0.00065867
Iteration 25/25 | Loss: 0.00065867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065867
Iteration 2/1000 | Loss: 0.00002171
Iteration 3/1000 | Loss: 0.00001646
Iteration 4/1000 | Loss: 0.00005148
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001461
Iteration 7/1000 | Loss: 0.00001426
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001376
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001324
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001287
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001286
Iteration 37/1000 | Loss: 0.00001285
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001283
Iteration 40/1000 | Loss: 0.00001282
Iteration 41/1000 | Loss: 0.00001281
Iteration 42/1000 | Loss: 0.00001280
Iteration 43/1000 | Loss: 0.00001280
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001276
Iteration 48/1000 | Loss: 0.00001276
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001276
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001276
Iteration 54/1000 | Loss: 0.00001276
Iteration 55/1000 | Loss: 0.00001276
Iteration 56/1000 | Loss: 0.00001276
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001275
Iteration 59/1000 | Loss: 0.00001275
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001272
Iteration 69/1000 | Loss: 0.00001272
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001270
Iteration 79/1000 | Loss: 0.00001270
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001269
Iteration 82/1000 | Loss: 0.00001269
Iteration 83/1000 | Loss: 0.00001269
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001268
Iteration 86/1000 | Loss: 0.00001268
Iteration 87/1000 | Loss: 0.00001268
Iteration 88/1000 | Loss: 0.00001268
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001266
Iteration 93/1000 | Loss: 0.00001266
Iteration 94/1000 | Loss: 0.00001266
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001263
Iteration 100/1000 | Loss: 0.00001263
Iteration 101/1000 | Loss: 0.00001263
Iteration 102/1000 | Loss: 0.00001263
Iteration 103/1000 | Loss: 0.00001263
Iteration 104/1000 | Loss: 0.00001263
Iteration 105/1000 | Loss: 0.00001263
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001262
Iteration 108/1000 | Loss: 0.00001262
Iteration 109/1000 | Loss: 0.00001262
Iteration 110/1000 | Loss: 0.00001262
Iteration 111/1000 | Loss: 0.00001262
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001261
Iteration 115/1000 | Loss: 0.00001261
Iteration 116/1000 | Loss: 0.00001261
Iteration 117/1000 | Loss: 0.00001261
Iteration 118/1000 | Loss: 0.00001261
Iteration 119/1000 | Loss: 0.00001260
Iteration 120/1000 | Loss: 0.00001260
Iteration 121/1000 | Loss: 0.00001260
Iteration 122/1000 | Loss: 0.00001260
Iteration 123/1000 | Loss: 0.00001260
Iteration 124/1000 | Loss: 0.00001259
Iteration 125/1000 | Loss: 0.00001259
Iteration 126/1000 | Loss: 0.00001259
Iteration 127/1000 | Loss: 0.00001259
Iteration 128/1000 | Loss: 0.00001259
Iteration 129/1000 | Loss: 0.00001259
Iteration 130/1000 | Loss: 0.00001259
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001259
Iteration 133/1000 | Loss: 0.00001258
Iteration 134/1000 | Loss: 0.00001258
Iteration 135/1000 | Loss: 0.00001258
Iteration 136/1000 | Loss: 0.00001258
Iteration 137/1000 | Loss: 0.00001258
Iteration 138/1000 | Loss: 0.00001258
Iteration 139/1000 | Loss: 0.00001258
Iteration 140/1000 | Loss: 0.00001257
Iteration 141/1000 | Loss: 0.00001257
Iteration 142/1000 | Loss: 0.00001257
Iteration 143/1000 | Loss: 0.00001257
Iteration 144/1000 | Loss: 0.00001257
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001257
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001256
Iteration 152/1000 | Loss: 0.00001256
Iteration 153/1000 | Loss: 0.00001256
Iteration 154/1000 | Loss: 0.00001256
Iteration 155/1000 | Loss: 0.00001255
Iteration 156/1000 | Loss: 0.00001255
Iteration 157/1000 | Loss: 0.00001255
Iteration 158/1000 | Loss: 0.00001255
Iteration 159/1000 | Loss: 0.00001255
Iteration 160/1000 | Loss: 0.00001255
Iteration 161/1000 | Loss: 0.00001255
Iteration 162/1000 | Loss: 0.00001255
Iteration 163/1000 | Loss: 0.00001255
Iteration 164/1000 | Loss: 0.00001255
Iteration 165/1000 | Loss: 0.00001255
Iteration 166/1000 | Loss: 0.00001255
Iteration 167/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.2547295227705035e-05, 1.2547295227705035e-05, 1.2547295227705035e-05, 1.2547295227705035e-05, 1.2547295227705035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2547295227705035e-05

Optimization complete. Final v2v error: 3.006153106689453 mm

Highest mean error: 3.5411322116851807 mm for frame 20

Lowest mean error: 2.618433713912964 mm for frame 199

Saving results

Total time: 64.56866002082825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060204
Iteration 2/25 | Loss: 0.01060204
Iteration 3/25 | Loss: 0.01060204
Iteration 4/25 | Loss: 0.01060203
Iteration 5/25 | Loss: 0.01060203
Iteration 6/25 | Loss: 0.01060203
Iteration 7/25 | Loss: 0.01060203
Iteration 8/25 | Loss: 0.01060203
Iteration 9/25 | Loss: 0.01060203
Iteration 10/25 | Loss: 0.01060203
Iteration 11/25 | Loss: 0.01060203
Iteration 12/25 | Loss: 0.01060203
Iteration 13/25 | Loss: 0.01060203
Iteration 14/25 | Loss: 0.01060203
Iteration 15/25 | Loss: 0.01060202
Iteration 16/25 | Loss: 0.01060202
Iteration 17/25 | Loss: 0.01060202
Iteration 18/25 | Loss: 0.01060202
Iteration 19/25 | Loss: 0.01060202
Iteration 20/25 | Loss: 0.01060202
Iteration 21/25 | Loss: 0.01060202
Iteration 22/25 | Loss: 0.01060202
Iteration 23/25 | Loss: 0.01060202
Iteration 24/25 | Loss: 0.01060202
Iteration 25/25 | Loss: 0.01060202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65003216
Iteration 2/25 | Loss: 0.08232995
Iteration 3/25 | Loss: 0.08232988
Iteration 4/25 | Loss: 0.08232987
Iteration 5/25 | Loss: 0.08232986
Iteration 6/25 | Loss: 0.08232986
Iteration 7/25 | Loss: 0.08232986
Iteration 8/25 | Loss: 0.08232985
Iteration 9/25 | Loss: 0.08232985
Iteration 10/25 | Loss: 0.08232985
Iteration 11/25 | Loss: 0.08232985
Iteration 12/25 | Loss: 0.08232985
Iteration 13/25 | Loss: 0.08232985
Iteration 14/25 | Loss: 0.08232985
Iteration 15/25 | Loss: 0.08232985
Iteration 16/25 | Loss: 0.08232985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.08232984691858292, 0.08232984691858292, 0.08232984691858292, 0.08232984691858292, 0.08232984691858292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08232984691858292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08232985
Iteration 2/1000 | Loss: 0.00046378
Iteration 3/1000 | Loss: 0.00014490
Iteration 4/1000 | Loss: 0.00006155
Iteration 5/1000 | Loss: 0.00003371
Iteration 6/1000 | Loss: 0.00002718
Iteration 7/1000 | Loss: 0.00002430
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00001940
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001635
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001183
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001091
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001045
Iteration 25/1000 | Loss: 0.00001033
Iteration 26/1000 | Loss: 0.00001027
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001014
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000995
Iteration 33/1000 | Loss: 0.00000993
Iteration 34/1000 | Loss: 0.00000992
Iteration 35/1000 | Loss: 0.00000991
Iteration 36/1000 | Loss: 0.00000990
Iteration 37/1000 | Loss: 0.00000990
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000988
Iteration 40/1000 | Loss: 0.00000988
Iteration 41/1000 | Loss: 0.00000987
Iteration 42/1000 | Loss: 0.00000986
Iteration 43/1000 | Loss: 0.00000986
Iteration 44/1000 | Loss: 0.00000986
Iteration 45/1000 | Loss: 0.00000986
Iteration 46/1000 | Loss: 0.00000986
Iteration 47/1000 | Loss: 0.00000986
Iteration 48/1000 | Loss: 0.00000986
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000984
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000984
Iteration 61/1000 | Loss: 0.00000984
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000983
Iteration 64/1000 | Loss: 0.00000983
Iteration 65/1000 | Loss: 0.00000983
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000983
Iteration 68/1000 | Loss: 0.00000982
Iteration 69/1000 | Loss: 0.00000982
Iteration 70/1000 | Loss: 0.00000982
Iteration 71/1000 | Loss: 0.00000981
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000980
Iteration 74/1000 | Loss: 0.00000980
Iteration 75/1000 | Loss: 0.00000980
Iteration 76/1000 | Loss: 0.00000979
Iteration 77/1000 | Loss: 0.00000979
Iteration 78/1000 | Loss: 0.00000979
Iteration 79/1000 | Loss: 0.00000979
Iteration 80/1000 | Loss: 0.00000978
Iteration 81/1000 | Loss: 0.00000978
Iteration 82/1000 | Loss: 0.00000978
Iteration 83/1000 | Loss: 0.00000978
Iteration 84/1000 | Loss: 0.00000977
Iteration 85/1000 | Loss: 0.00000977
Iteration 86/1000 | Loss: 0.00000977
Iteration 87/1000 | Loss: 0.00000977
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000977
Iteration 90/1000 | Loss: 0.00000977
Iteration 91/1000 | Loss: 0.00000977
Iteration 92/1000 | Loss: 0.00000977
Iteration 93/1000 | Loss: 0.00000976
Iteration 94/1000 | Loss: 0.00000976
Iteration 95/1000 | Loss: 0.00000976
Iteration 96/1000 | Loss: 0.00000975
Iteration 97/1000 | Loss: 0.00000975
Iteration 98/1000 | Loss: 0.00000975
Iteration 99/1000 | Loss: 0.00000975
Iteration 100/1000 | Loss: 0.00000974
Iteration 101/1000 | Loss: 0.00000974
Iteration 102/1000 | Loss: 0.00000974
Iteration 103/1000 | Loss: 0.00000974
Iteration 104/1000 | Loss: 0.00000974
Iteration 105/1000 | Loss: 0.00000974
Iteration 106/1000 | Loss: 0.00000974
Iteration 107/1000 | Loss: 0.00000973
Iteration 108/1000 | Loss: 0.00000973
Iteration 109/1000 | Loss: 0.00000972
Iteration 110/1000 | Loss: 0.00000972
Iteration 111/1000 | Loss: 0.00000972
Iteration 112/1000 | Loss: 0.00000972
Iteration 113/1000 | Loss: 0.00000972
Iteration 114/1000 | Loss: 0.00000971
Iteration 115/1000 | Loss: 0.00000971
Iteration 116/1000 | Loss: 0.00000971
Iteration 117/1000 | Loss: 0.00000971
Iteration 118/1000 | Loss: 0.00000971
Iteration 119/1000 | Loss: 0.00000971
Iteration 120/1000 | Loss: 0.00000971
Iteration 121/1000 | Loss: 0.00000971
Iteration 122/1000 | Loss: 0.00000971
Iteration 123/1000 | Loss: 0.00000971
Iteration 124/1000 | Loss: 0.00000971
Iteration 125/1000 | Loss: 0.00000971
Iteration 126/1000 | Loss: 0.00000971
Iteration 127/1000 | Loss: 0.00000971
Iteration 128/1000 | Loss: 0.00000971
Iteration 129/1000 | Loss: 0.00000971
Iteration 130/1000 | Loss: 0.00000970
Iteration 131/1000 | Loss: 0.00000970
Iteration 132/1000 | Loss: 0.00000970
Iteration 133/1000 | Loss: 0.00000970
Iteration 134/1000 | Loss: 0.00000970
Iteration 135/1000 | Loss: 0.00000970
Iteration 136/1000 | Loss: 0.00000970
Iteration 137/1000 | Loss: 0.00000969
Iteration 138/1000 | Loss: 0.00000969
Iteration 139/1000 | Loss: 0.00000969
Iteration 140/1000 | Loss: 0.00000969
Iteration 141/1000 | Loss: 0.00000969
Iteration 142/1000 | Loss: 0.00000969
Iteration 143/1000 | Loss: 0.00000969
Iteration 144/1000 | Loss: 0.00000969
Iteration 145/1000 | Loss: 0.00000969
Iteration 146/1000 | Loss: 0.00000969
Iteration 147/1000 | Loss: 0.00000968
Iteration 148/1000 | Loss: 0.00000968
Iteration 149/1000 | Loss: 0.00000968
Iteration 150/1000 | Loss: 0.00000968
Iteration 151/1000 | Loss: 0.00000968
Iteration 152/1000 | Loss: 0.00000968
Iteration 153/1000 | Loss: 0.00000968
Iteration 154/1000 | Loss: 0.00000968
Iteration 155/1000 | Loss: 0.00000968
Iteration 156/1000 | Loss: 0.00000968
Iteration 157/1000 | Loss: 0.00000968
Iteration 158/1000 | Loss: 0.00000968
Iteration 159/1000 | Loss: 0.00000968
Iteration 160/1000 | Loss: 0.00000968
Iteration 161/1000 | Loss: 0.00000968
Iteration 162/1000 | Loss: 0.00000968
Iteration 163/1000 | Loss: 0.00000968
Iteration 164/1000 | Loss: 0.00000967
Iteration 165/1000 | Loss: 0.00000967
Iteration 166/1000 | Loss: 0.00000967
Iteration 167/1000 | Loss: 0.00000967
Iteration 168/1000 | Loss: 0.00000967
Iteration 169/1000 | Loss: 0.00000967
Iteration 170/1000 | Loss: 0.00000967
Iteration 171/1000 | Loss: 0.00000967
Iteration 172/1000 | Loss: 0.00000967
Iteration 173/1000 | Loss: 0.00000967
Iteration 174/1000 | Loss: 0.00000967
Iteration 175/1000 | Loss: 0.00000966
Iteration 176/1000 | Loss: 0.00000966
Iteration 177/1000 | Loss: 0.00000966
Iteration 178/1000 | Loss: 0.00000966
Iteration 179/1000 | Loss: 0.00000966
Iteration 180/1000 | Loss: 0.00000966
Iteration 181/1000 | Loss: 0.00000966
Iteration 182/1000 | Loss: 0.00000966
Iteration 183/1000 | Loss: 0.00000966
Iteration 184/1000 | Loss: 0.00000966
Iteration 185/1000 | Loss: 0.00000966
Iteration 186/1000 | Loss: 0.00000966
Iteration 187/1000 | Loss: 0.00000966
Iteration 188/1000 | Loss: 0.00000966
Iteration 189/1000 | Loss: 0.00000966
Iteration 190/1000 | Loss: 0.00000966
Iteration 191/1000 | Loss: 0.00000966
Iteration 192/1000 | Loss: 0.00000966
Iteration 193/1000 | Loss: 0.00000966
Iteration 194/1000 | Loss: 0.00000966
Iteration 195/1000 | Loss: 0.00000966
Iteration 196/1000 | Loss: 0.00000966
Iteration 197/1000 | Loss: 0.00000966
Iteration 198/1000 | Loss: 0.00000966
Iteration 199/1000 | Loss: 0.00000966
Iteration 200/1000 | Loss: 0.00000966
Iteration 201/1000 | Loss: 0.00000966
Iteration 202/1000 | Loss: 0.00000966
Iteration 203/1000 | Loss: 0.00000966
Iteration 204/1000 | Loss: 0.00000966
Iteration 205/1000 | Loss: 0.00000966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [9.655127541918773e-06, 9.655127541918773e-06, 9.655127541918773e-06, 9.655127541918773e-06, 9.655127541918773e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.655127541918773e-06

Optimization complete. Final v2v error: 2.659113883972168 mm

Highest mean error: 2.8308470249176025 mm for frame 159

Lowest mean error: 2.4365954399108887 mm for frame 192

Saving results

Total time: 57.56415510177612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959348
Iteration 2/25 | Loss: 0.00380877
Iteration 3/25 | Loss: 0.00268525
Iteration 4/25 | Loss: 0.00225225
Iteration 5/25 | Loss: 0.00206580
Iteration 6/25 | Loss: 0.00213418
Iteration 7/25 | Loss: 0.00193952
Iteration 8/25 | Loss: 0.00174648
Iteration 9/25 | Loss: 0.00168242
Iteration 10/25 | Loss: 0.00163838
Iteration 11/25 | Loss: 0.00156269
Iteration 12/25 | Loss: 0.00150795
Iteration 13/25 | Loss: 0.00148878
Iteration 14/25 | Loss: 0.00147332
Iteration 15/25 | Loss: 0.00146725
Iteration 16/25 | Loss: 0.00146225
Iteration 17/25 | Loss: 0.00146040
Iteration 18/25 | Loss: 0.00146222
Iteration 19/25 | Loss: 0.00145948
Iteration 20/25 | Loss: 0.00145687
Iteration 21/25 | Loss: 0.00145524
Iteration 22/25 | Loss: 0.00145406
Iteration 23/25 | Loss: 0.00145477
Iteration 24/25 | Loss: 0.00145442
Iteration 25/25 | Loss: 0.00145554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38613570
Iteration 2/25 | Loss: 0.00211357
Iteration 3/25 | Loss: 0.00211356
Iteration 4/25 | Loss: 0.00211356
Iteration 5/25 | Loss: 0.00211356
Iteration 6/25 | Loss: 0.00211356
Iteration 7/25 | Loss: 0.00211356
Iteration 8/25 | Loss: 0.00211356
Iteration 9/25 | Loss: 0.00211356
Iteration 10/25 | Loss: 0.00211356
Iteration 11/25 | Loss: 0.00211356
Iteration 12/25 | Loss: 0.00211355
Iteration 13/25 | Loss: 0.00211355
Iteration 14/25 | Loss: 0.00211355
Iteration 15/25 | Loss: 0.00211355
Iteration 16/25 | Loss: 0.00211355
Iteration 17/25 | Loss: 0.00211355
Iteration 18/25 | Loss: 0.00211355
Iteration 19/25 | Loss: 0.00211355
Iteration 20/25 | Loss: 0.00211355
Iteration 21/25 | Loss: 0.00211355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021135546267032623, 0.0021135546267032623, 0.0021135546267032623, 0.0021135546267032623, 0.0021135546267032623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021135546267032623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211355
Iteration 2/1000 | Loss: 0.00165512
Iteration 3/1000 | Loss: 0.00080402
Iteration 4/1000 | Loss: 0.00060108
Iteration 5/1000 | Loss: 0.00050690
Iteration 6/1000 | Loss: 0.00038125
Iteration 7/1000 | Loss: 0.00023830
Iteration 8/1000 | Loss: 0.00019356
Iteration 9/1000 | Loss: 0.00018694
Iteration 10/1000 | Loss: 0.00098642
Iteration 11/1000 | Loss: 0.00098366
Iteration 12/1000 | Loss: 0.00177132
Iteration 13/1000 | Loss: 0.00346650
Iteration 14/1000 | Loss: 0.00118971
Iteration 15/1000 | Loss: 0.00045290
Iteration 16/1000 | Loss: 0.00075453
Iteration 17/1000 | Loss: 0.00181417
Iteration 18/1000 | Loss: 0.00107834
Iteration 19/1000 | Loss: 0.00122443
Iteration 20/1000 | Loss: 0.00123895
Iteration 21/1000 | Loss: 0.00061453
Iteration 22/1000 | Loss: 0.00061390
Iteration 23/1000 | Loss: 0.00077856
Iteration 24/1000 | Loss: 0.00044552
Iteration 25/1000 | Loss: 0.00044227
Iteration 26/1000 | Loss: 0.00055136
Iteration 27/1000 | Loss: 0.00030102
Iteration 28/1000 | Loss: 0.00015734
Iteration 29/1000 | Loss: 0.00044262
Iteration 30/1000 | Loss: 0.00019321
Iteration 31/1000 | Loss: 0.00033679
Iteration 32/1000 | Loss: 0.00045511
Iteration 33/1000 | Loss: 0.00015904
Iteration 34/1000 | Loss: 0.00026272
Iteration 35/1000 | Loss: 0.00035784
Iteration 36/1000 | Loss: 0.00033121
Iteration 37/1000 | Loss: 0.00053248
Iteration 38/1000 | Loss: 0.00008992
Iteration 39/1000 | Loss: 0.00006564
Iteration 40/1000 | Loss: 0.00005815
Iteration 41/1000 | Loss: 0.00043103
Iteration 42/1000 | Loss: 0.00005293
Iteration 43/1000 | Loss: 0.00022924
Iteration 44/1000 | Loss: 0.00005262
Iteration 45/1000 | Loss: 0.00004281
Iteration 46/1000 | Loss: 0.00005020
Iteration 47/1000 | Loss: 0.00003191
Iteration 48/1000 | Loss: 0.00004092
Iteration 49/1000 | Loss: 0.00013759
Iteration 50/1000 | Loss: 0.00014841
Iteration 51/1000 | Loss: 0.00013296
Iteration 52/1000 | Loss: 0.00004412
Iteration 53/1000 | Loss: 0.00004249
Iteration 54/1000 | Loss: 0.00008500
Iteration 55/1000 | Loss: 0.00058953
Iteration 56/1000 | Loss: 0.00006284
Iteration 57/1000 | Loss: 0.00004464
Iteration 58/1000 | Loss: 0.00014070
Iteration 59/1000 | Loss: 0.00011823
Iteration 60/1000 | Loss: 0.00011871
Iteration 61/1000 | Loss: 0.00013435
Iteration 62/1000 | Loss: 0.00003416
Iteration 63/1000 | Loss: 0.00002802
Iteration 64/1000 | Loss: 0.00016586
Iteration 65/1000 | Loss: 0.00003627
Iteration 66/1000 | Loss: 0.00003516
Iteration 67/1000 | Loss: 0.00021983
Iteration 68/1000 | Loss: 0.00032132
Iteration 69/1000 | Loss: 0.00017674
Iteration 70/1000 | Loss: 0.00029619
Iteration 71/1000 | Loss: 0.00016663
Iteration 72/1000 | Loss: 0.00013587
Iteration 73/1000 | Loss: 0.00003056
Iteration 74/1000 | Loss: 0.00003228
Iteration 75/1000 | Loss: 0.00024410
Iteration 76/1000 | Loss: 0.00022809
Iteration 77/1000 | Loss: 0.00002783
Iteration 78/1000 | Loss: 0.00027338
Iteration 79/1000 | Loss: 0.00017094
Iteration 80/1000 | Loss: 0.00015718
Iteration 81/1000 | Loss: 0.00012752
Iteration 82/1000 | Loss: 0.00002963
Iteration 83/1000 | Loss: 0.00030152
Iteration 84/1000 | Loss: 0.00029132
Iteration 85/1000 | Loss: 0.00004831
Iteration 86/1000 | Loss: 0.00003084
Iteration 87/1000 | Loss: 0.00002519
Iteration 88/1000 | Loss: 0.00003774
Iteration 89/1000 | Loss: 0.00002427
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001884
Iteration 93/1000 | Loss: 0.00001854
Iteration 94/1000 | Loss: 0.00001810
Iteration 95/1000 | Loss: 0.00001764
Iteration 96/1000 | Loss: 0.00001731
Iteration 97/1000 | Loss: 0.00001715
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001693
Iteration 107/1000 | Loss: 0.00001680
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001675
Iteration 112/1000 | Loss: 0.00001675
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001673
Iteration 115/1000 | Loss: 0.00001673
Iteration 116/1000 | Loss: 0.00001673
Iteration 117/1000 | Loss: 0.00001672
Iteration 118/1000 | Loss: 0.00001672
Iteration 119/1000 | Loss: 0.00001672
Iteration 120/1000 | Loss: 0.00001672
Iteration 121/1000 | Loss: 0.00001672
Iteration 122/1000 | Loss: 0.00001672
Iteration 123/1000 | Loss: 0.00001672
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001671
Iteration 126/1000 | Loss: 0.00001671
Iteration 127/1000 | Loss: 0.00001671
Iteration 128/1000 | Loss: 0.00001671
Iteration 129/1000 | Loss: 0.00001671
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001670
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001669
Iteration 135/1000 | Loss: 0.00001669
Iteration 136/1000 | Loss: 0.00001669
Iteration 137/1000 | Loss: 0.00001669
Iteration 138/1000 | Loss: 0.00001669
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001669
Iteration 141/1000 | Loss: 0.00001668
Iteration 142/1000 | Loss: 0.00001668
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001668
Iteration 145/1000 | Loss: 0.00001668
Iteration 146/1000 | Loss: 0.00001668
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001667
Iteration 149/1000 | Loss: 0.00001667
Iteration 150/1000 | Loss: 0.00001667
Iteration 151/1000 | Loss: 0.00001667
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001667
Iteration 154/1000 | Loss: 0.00001667
Iteration 155/1000 | Loss: 0.00001667
Iteration 156/1000 | Loss: 0.00001667
Iteration 157/1000 | Loss: 0.00001666
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001666
Iteration 160/1000 | Loss: 0.00001666
Iteration 161/1000 | Loss: 0.00001666
Iteration 162/1000 | Loss: 0.00001666
Iteration 163/1000 | Loss: 0.00001666
Iteration 164/1000 | Loss: 0.00001666
Iteration 165/1000 | Loss: 0.00001666
Iteration 166/1000 | Loss: 0.00001666
Iteration 167/1000 | Loss: 0.00001665
Iteration 168/1000 | Loss: 0.00001665
Iteration 169/1000 | Loss: 0.00001665
Iteration 170/1000 | Loss: 0.00001665
Iteration 171/1000 | Loss: 0.00001665
Iteration 172/1000 | Loss: 0.00001665
Iteration 173/1000 | Loss: 0.00001665
Iteration 174/1000 | Loss: 0.00001664
Iteration 175/1000 | Loss: 0.00001664
Iteration 176/1000 | Loss: 0.00001664
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001664
Iteration 179/1000 | Loss: 0.00001664
Iteration 180/1000 | Loss: 0.00001664
Iteration 181/1000 | Loss: 0.00001664
Iteration 182/1000 | Loss: 0.00001664
Iteration 183/1000 | Loss: 0.00001664
Iteration 184/1000 | Loss: 0.00001663
Iteration 185/1000 | Loss: 0.00001663
Iteration 186/1000 | Loss: 0.00001663
Iteration 187/1000 | Loss: 0.00001663
Iteration 188/1000 | Loss: 0.00001663
Iteration 189/1000 | Loss: 0.00001663
Iteration 190/1000 | Loss: 0.00001663
Iteration 191/1000 | Loss: 0.00001663
Iteration 192/1000 | Loss: 0.00001662
Iteration 193/1000 | Loss: 0.00001662
Iteration 194/1000 | Loss: 0.00001662
Iteration 195/1000 | Loss: 0.00001662
Iteration 196/1000 | Loss: 0.00001662
Iteration 197/1000 | Loss: 0.00001662
Iteration 198/1000 | Loss: 0.00001662
Iteration 199/1000 | Loss: 0.00001662
Iteration 200/1000 | Loss: 0.00001661
Iteration 201/1000 | Loss: 0.00001661
Iteration 202/1000 | Loss: 0.00001661
Iteration 203/1000 | Loss: 0.00001660
Iteration 204/1000 | Loss: 0.00001660
Iteration 205/1000 | Loss: 0.00001660
Iteration 206/1000 | Loss: 0.00001660
Iteration 207/1000 | Loss: 0.00001660
Iteration 208/1000 | Loss: 0.00001660
Iteration 209/1000 | Loss: 0.00001659
Iteration 210/1000 | Loss: 0.00001659
Iteration 211/1000 | Loss: 0.00001658
Iteration 212/1000 | Loss: 0.00001658
Iteration 213/1000 | Loss: 0.00001658
Iteration 214/1000 | Loss: 0.00001657
Iteration 215/1000 | Loss: 0.00001657
Iteration 216/1000 | Loss: 0.00001657
Iteration 217/1000 | Loss: 0.00001656
Iteration 218/1000 | Loss: 0.00001656
Iteration 219/1000 | Loss: 0.00001656
Iteration 220/1000 | Loss: 0.00001656
Iteration 221/1000 | Loss: 0.00001656
Iteration 222/1000 | Loss: 0.00001655
Iteration 223/1000 | Loss: 0.00001655
Iteration 224/1000 | Loss: 0.00001655
Iteration 225/1000 | Loss: 0.00001655
Iteration 226/1000 | Loss: 0.00001655
Iteration 227/1000 | Loss: 0.00001655
Iteration 228/1000 | Loss: 0.00001655
Iteration 229/1000 | Loss: 0.00001654
Iteration 230/1000 | Loss: 0.00001654
Iteration 231/1000 | Loss: 0.00001654
Iteration 232/1000 | Loss: 0.00001654
Iteration 233/1000 | Loss: 0.00001654
Iteration 234/1000 | Loss: 0.00001654
Iteration 235/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.6544754544156604e-05, 1.6544754544156604e-05, 1.6544754544156604e-05, 1.6544754544156604e-05, 1.6544754544156604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6544754544156604e-05

Optimization complete. Final v2v error: 3.257329225540161 mm

Highest mean error: 12.507567405700684 mm for frame 135

Lowest mean error: 3.0689775943756104 mm for frame 197

Saving results

Total time: 223.5660355091095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831795
Iteration 2/25 | Loss: 0.00128717
Iteration 3/25 | Loss: 0.00113997
Iteration 4/25 | Loss: 0.00111165
Iteration 5/25 | Loss: 0.00110369
Iteration 6/25 | Loss: 0.00110313
Iteration 7/25 | Loss: 0.00110313
Iteration 8/25 | Loss: 0.00110313
Iteration 9/25 | Loss: 0.00110313
Iteration 10/25 | Loss: 0.00110313
Iteration 11/25 | Loss: 0.00110313
Iteration 12/25 | Loss: 0.00110313
Iteration 13/25 | Loss: 0.00110313
Iteration 14/25 | Loss: 0.00110313
Iteration 15/25 | Loss: 0.00110313
Iteration 16/25 | Loss: 0.00110313
Iteration 17/25 | Loss: 0.00110313
Iteration 18/25 | Loss: 0.00110313
Iteration 19/25 | Loss: 0.00110313
Iteration 20/25 | Loss: 0.00110313
Iteration 21/25 | Loss: 0.00110313
Iteration 22/25 | Loss: 0.00110313
Iteration 23/25 | Loss: 0.00110313
Iteration 24/25 | Loss: 0.00110313
Iteration 25/25 | Loss: 0.00110313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00071454
Iteration 2/25 | Loss: 0.00035911
Iteration 3/25 | Loss: 0.00035910
Iteration 4/25 | Loss: 0.00035910
Iteration 5/25 | Loss: 0.00035910
Iteration 6/25 | Loss: 0.00035910
Iteration 7/25 | Loss: 0.00035910
Iteration 8/25 | Loss: 0.00035910
Iteration 9/25 | Loss: 0.00035910
Iteration 10/25 | Loss: 0.00035910
Iteration 11/25 | Loss: 0.00035910
Iteration 12/25 | Loss: 0.00035910
Iteration 13/25 | Loss: 0.00035910
Iteration 14/25 | Loss: 0.00035910
Iteration 15/25 | Loss: 0.00035910
Iteration 16/25 | Loss: 0.00035910
Iteration 17/25 | Loss: 0.00035910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003590996202547103, 0.0003590996202547103, 0.0003590996202547103, 0.0003590996202547103, 0.0003590996202547103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003590996202547103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035910
Iteration 2/1000 | Loss: 0.00003277
Iteration 3/1000 | Loss: 0.00002671
Iteration 4/1000 | Loss: 0.00002464
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002250
Iteration 7/1000 | Loss: 0.00002186
Iteration 8/1000 | Loss: 0.00002145
Iteration 9/1000 | Loss: 0.00002110
Iteration 10/1000 | Loss: 0.00002091
Iteration 11/1000 | Loss: 0.00002072
Iteration 12/1000 | Loss: 0.00002054
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002053
Iteration 15/1000 | Loss: 0.00002053
Iteration 16/1000 | Loss: 0.00002048
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002036
Iteration 22/1000 | Loss: 0.00002036
Iteration 23/1000 | Loss: 0.00002035
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00002033
Iteration 26/1000 | Loss: 0.00002032
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00002032
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00002031
Iteration 31/1000 | Loss: 0.00002029
Iteration 32/1000 | Loss: 0.00002029
Iteration 33/1000 | Loss: 0.00002029
Iteration 34/1000 | Loss: 0.00002029
Iteration 35/1000 | Loss: 0.00002029
Iteration 36/1000 | Loss: 0.00002029
Iteration 37/1000 | Loss: 0.00002028
Iteration 38/1000 | Loss: 0.00002028
Iteration 39/1000 | Loss: 0.00002028
Iteration 40/1000 | Loss: 0.00002028
Iteration 41/1000 | Loss: 0.00002028
Iteration 42/1000 | Loss: 0.00002028
Iteration 43/1000 | Loss: 0.00002028
Iteration 44/1000 | Loss: 0.00002028
Iteration 45/1000 | Loss: 0.00002028
Iteration 46/1000 | Loss: 0.00002028
Iteration 47/1000 | Loss: 0.00002028
Iteration 48/1000 | Loss: 0.00002028
Iteration 49/1000 | Loss: 0.00002028
Iteration 50/1000 | Loss: 0.00002027
Iteration 51/1000 | Loss: 0.00002027
Iteration 52/1000 | Loss: 0.00002027
Iteration 53/1000 | Loss: 0.00002027
Iteration 54/1000 | Loss: 0.00002027
Iteration 55/1000 | Loss: 0.00002027
Iteration 56/1000 | Loss: 0.00002027
Iteration 57/1000 | Loss: 0.00002027
Iteration 58/1000 | Loss: 0.00002026
Iteration 59/1000 | Loss: 0.00002026
Iteration 60/1000 | Loss: 0.00002026
Iteration 61/1000 | Loss: 0.00002025
Iteration 62/1000 | Loss: 0.00002025
Iteration 63/1000 | Loss: 0.00002025
Iteration 64/1000 | Loss: 0.00002024
Iteration 65/1000 | Loss: 0.00002024
Iteration 66/1000 | Loss: 0.00002023
Iteration 67/1000 | Loss: 0.00002023
Iteration 68/1000 | Loss: 0.00002023
Iteration 69/1000 | Loss: 0.00002023
Iteration 70/1000 | Loss: 0.00002023
Iteration 71/1000 | Loss: 0.00002023
Iteration 72/1000 | Loss: 0.00002023
Iteration 73/1000 | Loss: 0.00002023
Iteration 74/1000 | Loss: 0.00002022
Iteration 75/1000 | Loss: 0.00002022
Iteration 76/1000 | Loss: 0.00002022
Iteration 77/1000 | Loss: 0.00002022
Iteration 78/1000 | Loss: 0.00002022
Iteration 79/1000 | Loss: 0.00002022
Iteration 80/1000 | Loss: 0.00002022
Iteration 81/1000 | Loss: 0.00002022
Iteration 82/1000 | Loss: 0.00002022
Iteration 83/1000 | Loss: 0.00002022
Iteration 84/1000 | Loss: 0.00002022
Iteration 85/1000 | Loss: 0.00002022
Iteration 86/1000 | Loss: 0.00002022
Iteration 87/1000 | Loss: 0.00002022
Iteration 88/1000 | Loss: 0.00002022
Iteration 89/1000 | Loss: 0.00002022
Iteration 90/1000 | Loss: 0.00002022
Iteration 91/1000 | Loss: 0.00002021
Iteration 92/1000 | Loss: 0.00002021
Iteration 93/1000 | Loss: 0.00002021
Iteration 94/1000 | Loss: 0.00002021
Iteration 95/1000 | Loss: 0.00002021
Iteration 96/1000 | Loss: 0.00002021
Iteration 97/1000 | Loss: 0.00002021
Iteration 98/1000 | Loss: 0.00002021
Iteration 99/1000 | Loss: 0.00002021
Iteration 100/1000 | Loss: 0.00002021
Iteration 101/1000 | Loss: 0.00002021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.020982174144592e-05, 2.020982174144592e-05, 2.020982174144592e-05, 2.020982174144592e-05, 2.020982174144592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.020982174144592e-05

Optimization complete. Final v2v error: 3.8363661766052246 mm

Highest mean error: 4.421453952789307 mm for frame 239

Lowest mean error: 3.4340291023254395 mm for frame 4

Saving results

Total time: 37.488022327423096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00683262
Iteration 2/25 | Loss: 0.00177800
Iteration 3/25 | Loss: 0.00131573
Iteration 4/25 | Loss: 0.00125579
Iteration 5/25 | Loss: 0.00124351
Iteration 6/25 | Loss: 0.00123744
Iteration 7/25 | Loss: 0.00122266
Iteration 8/25 | Loss: 0.00119379
Iteration 9/25 | Loss: 0.00118518
Iteration 10/25 | Loss: 0.00116907
Iteration 11/25 | Loss: 0.00115896
Iteration 12/25 | Loss: 0.00116275
Iteration 13/25 | Loss: 0.00115553
Iteration 14/25 | Loss: 0.00115326
Iteration 15/25 | Loss: 0.00115102
Iteration 16/25 | Loss: 0.00115099
Iteration 17/25 | Loss: 0.00115128
Iteration 18/25 | Loss: 0.00115107
Iteration 19/25 | Loss: 0.00115071
Iteration 20/25 | Loss: 0.00115130
Iteration 21/25 | Loss: 0.00115126
Iteration 22/25 | Loss: 0.00115062
Iteration 23/25 | Loss: 0.00115108
Iteration 24/25 | Loss: 0.00115099
Iteration 25/25 | Loss: 0.00115099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30834651
Iteration 2/25 | Loss: 0.00069432
Iteration 3/25 | Loss: 0.00069429
Iteration 4/25 | Loss: 0.00069429
Iteration 5/25 | Loss: 0.00069429
Iteration 6/25 | Loss: 0.00069429
Iteration 7/25 | Loss: 0.00069429
Iteration 8/25 | Loss: 0.00069429
Iteration 9/25 | Loss: 0.00069429
Iteration 10/25 | Loss: 0.00069428
Iteration 11/25 | Loss: 0.00069428
Iteration 12/25 | Loss: 0.00069428
Iteration 13/25 | Loss: 0.00069428
Iteration 14/25 | Loss: 0.00069428
Iteration 15/25 | Loss: 0.00069428
Iteration 16/25 | Loss: 0.00069428
Iteration 17/25 | Loss: 0.00069428
Iteration 18/25 | Loss: 0.00069428
Iteration 19/25 | Loss: 0.00069428
Iteration 20/25 | Loss: 0.00069428
Iteration 21/25 | Loss: 0.00069428
Iteration 22/25 | Loss: 0.00069428
Iteration 23/25 | Loss: 0.00069428
Iteration 24/25 | Loss: 0.00069428
Iteration 25/25 | Loss: 0.00069428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069428
Iteration 2/1000 | Loss: 0.00004611
Iteration 3/1000 | Loss: 0.00003143
Iteration 4/1000 | Loss: 0.00002755
Iteration 5/1000 | Loss: 0.00004091
Iteration 6/1000 | Loss: 0.00003526
Iteration 7/1000 | Loss: 0.00004503
Iteration 8/1000 | Loss: 0.00004239
Iteration 9/1000 | Loss: 0.00004493
Iteration 10/1000 | Loss: 0.00002894
Iteration 11/1000 | Loss: 0.00002410
Iteration 12/1000 | Loss: 0.00002156
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002073
Iteration 15/1000 | Loss: 0.00002054
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002005
Iteration 18/1000 | Loss: 0.00001995
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001987
Iteration 24/1000 | Loss: 0.00001987
Iteration 25/1000 | Loss: 0.00001983
Iteration 26/1000 | Loss: 0.00001981
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001970
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001964
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001963
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001961
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001960
Iteration 54/1000 | Loss: 0.00001960
Iteration 55/1000 | Loss: 0.00001959
Iteration 56/1000 | Loss: 0.00001959
Iteration 57/1000 | Loss: 0.00001959
Iteration 58/1000 | Loss: 0.00001959
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001958
Iteration 61/1000 | Loss: 0.00001958
Iteration 62/1000 | Loss: 0.00001958
Iteration 63/1000 | Loss: 0.00001957
Iteration 64/1000 | Loss: 0.00001957
Iteration 65/1000 | Loss: 0.00001957
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001956
Iteration 71/1000 | Loss: 0.00001956
Iteration 72/1000 | Loss: 0.00001956
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001955
Iteration 77/1000 | Loss: 0.00001955
Iteration 78/1000 | Loss: 0.00001955
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001954
Iteration 81/1000 | Loss: 0.00001954
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001953
Iteration 84/1000 | Loss: 0.00001953
Iteration 85/1000 | Loss: 0.00001952
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001951
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001950
Iteration 96/1000 | Loss: 0.00001950
Iteration 97/1000 | Loss: 0.00001950
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001949
Iteration 101/1000 | Loss: 0.00001949
Iteration 102/1000 | Loss: 0.00001949
Iteration 103/1000 | Loss: 0.00001949
Iteration 104/1000 | Loss: 0.00001949
Iteration 105/1000 | Loss: 0.00001949
Iteration 106/1000 | Loss: 0.00001948
Iteration 107/1000 | Loss: 0.00001948
Iteration 108/1000 | Loss: 0.00001948
Iteration 109/1000 | Loss: 0.00001948
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001947
Iteration 113/1000 | Loss: 0.00001947
Iteration 114/1000 | Loss: 0.00001947
Iteration 115/1000 | Loss: 0.00001947
Iteration 116/1000 | Loss: 0.00001947
Iteration 117/1000 | Loss: 0.00001947
Iteration 118/1000 | Loss: 0.00001947
Iteration 119/1000 | Loss: 0.00001947
Iteration 120/1000 | Loss: 0.00001946
Iteration 121/1000 | Loss: 0.00001946
Iteration 122/1000 | Loss: 0.00001946
Iteration 123/1000 | Loss: 0.00001946
Iteration 124/1000 | Loss: 0.00001946
Iteration 125/1000 | Loss: 0.00001946
Iteration 126/1000 | Loss: 0.00001946
Iteration 127/1000 | Loss: 0.00001946
Iteration 128/1000 | Loss: 0.00001945
Iteration 129/1000 | Loss: 0.00001945
Iteration 130/1000 | Loss: 0.00001945
Iteration 131/1000 | Loss: 0.00001945
Iteration 132/1000 | Loss: 0.00001945
Iteration 133/1000 | Loss: 0.00001945
Iteration 134/1000 | Loss: 0.00001945
Iteration 135/1000 | Loss: 0.00001945
Iteration 136/1000 | Loss: 0.00001945
Iteration 137/1000 | Loss: 0.00001945
Iteration 138/1000 | Loss: 0.00001944
Iteration 139/1000 | Loss: 0.00001944
Iteration 140/1000 | Loss: 0.00001944
Iteration 141/1000 | Loss: 0.00001944
Iteration 142/1000 | Loss: 0.00001944
Iteration 143/1000 | Loss: 0.00001944
Iteration 144/1000 | Loss: 0.00001944
Iteration 145/1000 | Loss: 0.00001944
Iteration 146/1000 | Loss: 0.00001943
Iteration 147/1000 | Loss: 0.00001943
Iteration 148/1000 | Loss: 0.00001943
Iteration 149/1000 | Loss: 0.00001943
Iteration 150/1000 | Loss: 0.00001943
Iteration 151/1000 | Loss: 0.00001942
Iteration 152/1000 | Loss: 0.00001942
Iteration 153/1000 | Loss: 0.00001942
Iteration 154/1000 | Loss: 0.00001942
Iteration 155/1000 | Loss: 0.00001942
Iteration 156/1000 | Loss: 0.00001942
Iteration 157/1000 | Loss: 0.00001942
Iteration 158/1000 | Loss: 0.00001942
Iteration 159/1000 | Loss: 0.00001941
Iteration 160/1000 | Loss: 0.00001941
Iteration 161/1000 | Loss: 0.00001941
Iteration 162/1000 | Loss: 0.00001941
Iteration 163/1000 | Loss: 0.00001941
Iteration 164/1000 | Loss: 0.00001941
Iteration 165/1000 | Loss: 0.00001941
Iteration 166/1000 | Loss: 0.00001940
Iteration 167/1000 | Loss: 0.00001940
Iteration 168/1000 | Loss: 0.00001940
Iteration 169/1000 | Loss: 0.00001940
Iteration 170/1000 | Loss: 0.00001940
Iteration 171/1000 | Loss: 0.00001940
Iteration 172/1000 | Loss: 0.00001940
Iteration 173/1000 | Loss: 0.00001940
Iteration 174/1000 | Loss: 0.00001940
Iteration 175/1000 | Loss: 0.00001940
Iteration 176/1000 | Loss: 0.00001939
Iteration 177/1000 | Loss: 0.00001939
Iteration 178/1000 | Loss: 0.00001939
Iteration 179/1000 | Loss: 0.00001939
Iteration 180/1000 | Loss: 0.00001939
Iteration 181/1000 | Loss: 0.00001939
Iteration 182/1000 | Loss: 0.00001939
Iteration 183/1000 | Loss: 0.00001939
Iteration 184/1000 | Loss: 0.00001939
Iteration 185/1000 | Loss: 0.00001939
Iteration 186/1000 | Loss: 0.00001939
Iteration 187/1000 | Loss: 0.00001939
Iteration 188/1000 | Loss: 0.00001939
Iteration 189/1000 | Loss: 0.00001939
Iteration 190/1000 | Loss: 0.00001939
Iteration 191/1000 | Loss: 0.00001939
Iteration 192/1000 | Loss: 0.00001939
Iteration 193/1000 | Loss: 0.00001939
Iteration 194/1000 | Loss: 0.00001939
Iteration 195/1000 | Loss: 0.00001939
Iteration 196/1000 | Loss: 0.00001938
Iteration 197/1000 | Loss: 0.00001938
Iteration 198/1000 | Loss: 0.00001938
Iteration 199/1000 | Loss: 0.00001938
Iteration 200/1000 | Loss: 0.00001938
Iteration 201/1000 | Loss: 0.00001938
Iteration 202/1000 | Loss: 0.00001938
Iteration 203/1000 | Loss: 0.00001938
Iteration 204/1000 | Loss: 0.00001938
Iteration 205/1000 | Loss: 0.00001938
Iteration 206/1000 | Loss: 0.00001938
Iteration 207/1000 | Loss: 0.00001938
Iteration 208/1000 | Loss: 0.00001937
Iteration 209/1000 | Loss: 0.00001937
Iteration 210/1000 | Loss: 0.00001937
Iteration 211/1000 | Loss: 0.00001937
Iteration 212/1000 | Loss: 0.00001937
Iteration 213/1000 | Loss: 0.00001937
Iteration 214/1000 | Loss: 0.00001937
Iteration 215/1000 | Loss: 0.00001937
Iteration 216/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.937378510774579e-05, 1.937378510774579e-05, 1.937378510774579e-05, 1.937378510774579e-05, 1.937378510774579e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.937378510774579e-05

Optimization complete. Final v2v error: 3.5275697708129883 mm

Highest mean error: 11.636381149291992 mm for frame 16

Lowest mean error: 2.7925453186035156 mm for frame 190

Saving results

Total time: 97.41805648803711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025386
Iteration 2/25 | Loss: 0.00157308
Iteration 3/25 | Loss: 0.00130098
Iteration 4/25 | Loss: 0.00127503
Iteration 5/25 | Loss: 0.00126648
Iteration 6/25 | Loss: 0.00126412
Iteration 7/25 | Loss: 0.00126386
Iteration 8/25 | Loss: 0.00126377
Iteration 9/25 | Loss: 0.00126377
Iteration 10/25 | Loss: 0.00126377
Iteration 11/25 | Loss: 0.00126377
Iteration 12/25 | Loss: 0.00126377
Iteration 13/25 | Loss: 0.00126377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012637678300961852, 0.0012637678300961852, 0.0012637678300961852, 0.0012637678300961852, 0.0012637678300961852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012637678300961852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96798658
Iteration 2/25 | Loss: 0.00094644
Iteration 3/25 | Loss: 0.00094641
Iteration 4/25 | Loss: 0.00094641
Iteration 5/25 | Loss: 0.00094641
Iteration 6/25 | Loss: 0.00094641
Iteration 7/25 | Loss: 0.00094641
Iteration 8/25 | Loss: 0.00094641
Iteration 9/25 | Loss: 0.00094641
Iteration 10/25 | Loss: 0.00094641
Iteration 11/25 | Loss: 0.00094641
Iteration 12/25 | Loss: 0.00094641
Iteration 13/25 | Loss: 0.00094641
Iteration 14/25 | Loss: 0.00094641
Iteration 15/25 | Loss: 0.00094641
Iteration 16/25 | Loss: 0.00094641
Iteration 17/25 | Loss: 0.00094641
Iteration 18/25 | Loss: 0.00094641
Iteration 19/25 | Loss: 0.00094641
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009464111062698066, 0.0009464111062698066, 0.0009464111062698066, 0.0009464111062698066, 0.0009464111062698066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009464111062698066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094641
Iteration 2/1000 | Loss: 0.00008891
Iteration 3/1000 | Loss: 0.00005046
Iteration 4/1000 | Loss: 0.00003623
Iteration 5/1000 | Loss: 0.00003315
Iteration 6/1000 | Loss: 0.00003154
Iteration 7/1000 | Loss: 0.00003075
Iteration 8/1000 | Loss: 0.00003012
Iteration 9/1000 | Loss: 0.00002955
Iteration 10/1000 | Loss: 0.00002919
Iteration 11/1000 | Loss: 0.00002885
Iteration 12/1000 | Loss: 0.00002859
Iteration 13/1000 | Loss: 0.00002839
Iteration 14/1000 | Loss: 0.00002820
Iteration 15/1000 | Loss: 0.00002804
Iteration 16/1000 | Loss: 0.00002804
Iteration 17/1000 | Loss: 0.00002803
Iteration 18/1000 | Loss: 0.00002800
Iteration 19/1000 | Loss: 0.00002799
Iteration 20/1000 | Loss: 0.00002799
Iteration 21/1000 | Loss: 0.00002793
Iteration 22/1000 | Loss: 0.00002781
Iteration 23/1000 | Loss: 0.00002778
Iteration 24/1000 | Loss: 0.00002776
Iteration 25/1000 | Loss: 0.00002773
Iteration 26/1000 | Loss: 0.00002772
Iteration 27/1000 | Loss: 0.00002772
Iteration 28/1000 | Loss: 0.00002768
Iteration 29/1000 | Loss: 0.00002768
Iteration 30/1000 | Loss: 0.00002768
Iteration 31/1000 | Loss: 0.00002768
Iteration 32/1000 | Loss: 0.00002768
Iteration 33/1000 | Loss: 0.00002768
Iteration 34/1000 | Loss: 0.00002768
Iteration 35/1000 | Loss: 0.00002768
Iteration 36/1000 | Loss: 0.00002768
Iteration 37/1000 | Loss: 0.00002767
Iteration 38/1000 | Loss: 0.00002767
Iteration 39/1000 | Loss: 0.00002766
Iteration 40/1000 | Loss: 0.00002766
Iteration 41/1000 | Loss: 0.00002764
Iteration 42/1000 | Loss: 0.00002764
Iteration 43/1000 | Loss: 0.00002764
Iteration 44/1000 | Loss: 0.00002764
Iteration 45/1000 | Loss: 0.00002764
Iteration 46/1000 | Loss: 0.00002764
Iteration 47/1000 | Loss: 0.00002764
Iteration 48/1000 | Loss: 0.00002764
Iteration 49/1000 | Loss: 0.00002764
Iteration 50/1000 | Loss: 0.00002763
Iteration 51/1000 | Loss: 0.00002763
Iteration 52/1000 | Loss: 0.00002762
Iteration 53/1000 | Loss: 0.00002760
Iteration 54/1000 | Loss: 0.00002760
Iteration 55/1000 | Loss: 0.00002760
Iteration 56/1000 | Loss: 0.00002760
Iteration 57/1000 | Loss: 0.00002760
Iteration 58/1000 | Loss: 0.00002760
Iteration 59/1000 | Loss: 0.00002759
Iteration 60/1000 | Loss: 0.00002759
Iteration 61/1000 | Loss: 0.00002759
Iteration 62/1000 | Loss: 0.00002759
Iteration 63/1000 | Loss: 0.00002759
Iteration 64/1000 | Loss: 0.00002759
Iteration 65/1000 | Loss: 0.00002758
Iteration 66/1000 | Loss: 0.00002757
Iteration 67/1000 | Loss: 0.00002757
Iteration 68/1000 | Loss: 0.00002757
Iteration 69/1000 | Loss: 0.00002756
Iteration 70/1000 | Loss: 0.00002756
Iteration 71/1000 | Loss: 0.00002756
Iteration 72/1000 | Loss: 0.00002755
Iteration 73/1000 | Loss: 0.00002755
Iteration 74/1000 | Loss: 0.00002755
Iteration 75/1000 | Loss: 0.00002755
Iteration 76/1000 | Loss: 0.00002754
Iteration 77/1000 | Loss: 0.00002754
Iteration 78/1000 | Loss: 0.00002754
Iteration 79/1000 | Loss: 0.00002754
Iteration 80/1000 | Loss: 0.00002754
Iteration 81/1000 | Loss: 0.00002753
Iteration 82/1000 | Loss: 0.00002753
Iteration 83/1000 | Loss: 0.00002753
Iteration 84/1000 | Loss: 0.00002753
Iteration 85/1000 | Loss: 0.00002753
Iteration 86/1000 | Loss: 0.00002753
Iteration 87/1000 | Loss: 0.00002753
Iteration 88/1000 | Loss: 0.00002752
Iteration 89/1000 | Loss: 0.00002752
Iteration 90/1000 | Loss: 0.00002752
Iteration 91/1000 | Loss: 0.00002751
Iteration 92/1000 | Loss: 0.00002751
Iteration 93/1000 | Loss: 0.00002751
Iteration 94/1000 | Loss: 0.00002751
Iteration 95/1000 | Loss: 0.00002751
Iteration 96/1000 | Loss: 0.00002750
Iteration 97/1000 | Loss: 0.00002750
Iteration 98/1000 | Loss: 0.00002750
Iteration 99/1000 | Loss: 0.00002749
Iteration 100/1000 | Loss: 0.00002749
Iteration 101/1000 | Loss: 0.00002748
Iteration 102/1000 | Loss: 0.00002748
Iteration 103/1000 | Loss: 0.00002747
Iteration 104/1000 | Loss: 0.00002747
Iteration 105/1000 | Loss: 0.00002747
Iteration 106/1000 | Loss: 0.00002747
Iteration 107/1000 | Loss: 0.00002747
Iteration 108/1000 | Loss: 0.00002747
Iteration 109/1000 | Loss: 0.00002746
Iteration 110/1000 | Loss: 0.00002746
Iteration 111/1000 | Loss: 0.00002746
Iteration 112/1000 | Loss: 0.00002746
Iteration 113/1000 | Loss: 0.00002746
Iteration 114/1000 | Loss: 0.00002746
Iteration 115/1000 | Loss: 0.00002746
Iteration 116/1000 | Loss: 0.00002746
Iteration 117/1000 | Loss: 0.00002745
Iteration 118/1000 | Loss: 0.00002745
Iteration 119/1000 | Loss: 0.00002745
Iteration 120/1000 | Loss: 0.00002745
Iteration 121/1000 | Loss: 0.00002745
Iteration 122/1000 | Loss: 0.00002744
Iteration 123/1000 | Loss: 0.00002744
Iteration 124/1000 | Loss: 0.00002744
Iteration 125/1000 | Loss: 0.00002744
Iteration 126/1000 | Loss: 0.00002744
Iteration 127/1000 | Loss: 0.00002744
Iteration 128/1000 | Loss: 0.00002744
Iteration 129/1000 | Loss: 0.00002743
Iteration 130/1000 | Loss: 0.00002743
Iteration 131/1000 | Loss: 0.00002743
Iteration 132/1000 | Loss: 0.00002743
Iteration 133/1000 | Loss: 0.00002743
Iteration 134/1000 | Loss: 0.00002743
Iteration 135/1000 | Loss: 0.00002743
Iteration 136/1000 | Loss: 0.00002743
Iteration 137/1000 | Loss: 0.00002743
Iteration 138/1000 | Loss: 0.00002743
Iteration 139/1000 | Loss: 0.00002743
Iteration 140/1000 | Loss: 0.00002743
Iteration 141/1000 | Loss: 0.00002743
Iteration 142/1000 | Loss: 0.00002742
Iteration 143/1000 | Loss: 0.00002742
Iteration 144/1000 | Loss: 0.00002742
Iteration 145/1000 | Loss: 0.00002742
Iteration 146/1000 | Loss: 0.00002742
Iteration 147/1000 | Loss: 0.00002742
Iteration 148/1000 | Loss: 0.00002742
Iteration 149/1000 | Loss: 0.00002742
Iteration 150/1000 | Loss: 0.00002742
Iteration 151/1000 | Loss: 0.00002741
Iteration 152/1000 | Loss: 0.00002741
Iteration 153/1000 | Loss: 0.00002741
Iteration 154/1000 | Loss: 0.00002741
Iteration 155/1000 | Loss: 0.00002741
Iteration 156/1000 | Loss: 0.00002741
Iteration 157/1000 | Loss: 0.00002741
Iteration 158/1000 | Loss: 0.00002741
Iteration 159/1000 | Loss: 0.00002741
Iteration 160/1000 | Loss: 0.00002740
Iteration 161/1000 | Loss: 0.00002740
Iteration 162/1000 | Loss: 0.00002740
Iteration 163/1000 | Loss: 0.00002740
Iteration 164/1000 | Loss: 0.00002740
Iteration 165/1000 | Loss: 0.00002740
Iteration 166/1000 | Loss: 0.00002740
Iteration 167/1000 | Loss: 0.00002740
Iteration 168/1000 | Loss: 0.00002740
Iteration 169/1000 | Loss: 0.00002740
Iteration 170/1000 | Loss: 0.00002740
Iteration 171/1000 | Loss: 0.00002740
Iteration 172/1000 | Loss: 0.00002740
Iteration 173/1000 | Loss: 0.00002740
Iteration 174/1000 | Loss: 0.00002740
Iteration 175/1000 | Loss: 0.00002740
Iteration 176/1000 | Loss: 0.00002739
Iteration 177/1000 | Loss: 0.00002739
Iteration 178/1000 | Loss: 0.00002739
Iteration 179/1000 | Loss: 0.00002739
Iteration 180/1000 | Loss: 0.00002739
Iteration 181/1000 | Loss: 0.00002739
Iteration 182/1000 | Loss: 0.00002739
Iteration 183/1000 | Loss: 0.00002739
Iteration 184/1000 | Loss: 0.00002738
Iteration 185/1000 | Loss: 0.00002738
Iteration 186/1000 | Loss: 0.00002738
Iteration 187/1000 | Loss: 0.00002738
Iteration 188/1000 | Loss: 0.00002738
Iteration 189/1000 | Loss: 0.00002738
Iteration 190/1000 | Loss: 0.00002738
Iteration 191/1000 | Loss: 0.00002738
Iteration 192/1000 | Loss: 0.00002738
Iteration 193/1000 | Loss: 0.00002738
Iteration 194/1000 | Loss: 0.00002738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.738385410339106e-05, 2.738385410339106e-05, 2.738385410339106e-05, 2.738385410339106e-05, 2.738385410339106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.738385410339106e-05

Optimization complete. Final v2v error: 4.238550662994385 mm

Highest mean error: 5.174665927886963 mm for frame 52

Lowest mean error: 3.4808578491210938 mm for frame 27

Saving results

Total time: 49.641541719436646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815580
Iteration 2/25 | Loss: 0.00144243
Iteration 3/25 | Loss: 0.00121356
Iteration 4/25 | Loss: 0.00119595
Iteration 5/25 | Loss: 0.00119123
Iteration 6/25 | Loss: 0.00119002
Iteration 7/25 | Loss: 0.00119002
Iteration 8/25 | Loss: 0.00119003
Iteration 9/25 | Loss: 0.00119002
Iteration 10/25 | Loss: 0.00119002
Iteration 11/25 | Loss: 0.00119002
Iteration 12/25 | Loss: 0.00119002
Iteration 13/25 | Loss: 0.00119002
Iteration 14/25 | Loss: 0.00119002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001190024777315557, 0.001190024777315557, 0.001190024777315557, 0.001190024777315557, 0.001190024777315557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001190024777315557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29190397
Iteration 2/25 | Loss: 0.00105378
Iteration 3/25 | Loss: 0.00105378
Iteration 4/25 | Loss: 0.00105378
Iteration 5/25 | Loss: 0.00105378
Iteration 6/25 | Loss: 0.00105378
Iteration 7/25 | Loss: 0.00105378
Iteration 8/25 | Loss: 0.00105378
Iteration 9/25 | Loss: 0.00105378
Iteration 10/25 | Loss: 0.00105378
Iteration 11/25 | Loss: 0.00105378
Iteration 12/25 | Loss: 0.00105378
Iteration 13/25 | Loss: 0.00105378
Iteration 14/25 | Loss: 0.00105378
Iteration 15/25 | Loss: 0.00105378
Iteration 16/25 | Loss: 0.00105378
Iteration 17/25 | Loss: 0.00105378
Iteration 18/25 | Loss: 0.00105378
Iteration 19/25 | Loss: 0.00105378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010537776397541165, 0.0010537776397541165, 0.0010537776397541165, 0.0010537776397541165, 0.0010537776397541165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010537776397541165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105378
Iteration 2/1000 | Loss: 0.00006926
Iteration 3/1000 | Loss: 0.00004468
Iteration 4/1000 | Loss: 0.00003952
Iteration 5/1000 | Loss: 0.00003769
Iteration 6/1000 | Loss: 0.00003686
Iteration 7/1000 | Loss: 0.00003592
Iteration 8/1000 | Loss: 0.00003493
Iteration 9/1000 | Loss: 0.00003428
Iteration 10/1000 | Loss: 0.00003389
Iteration 11/1000 | Loss: 0.00003353
Iteration 12/1000 | Loss: 0.00003332
Iteration 13/1000 | Loss: 0.00003310
Iteration 14/1000 | Loss: 0.00003297
Iteration 15/1000 | Loss: 0.00003287
Iteration 16/1000 | Loss: 0.00003279
Iteration 17/1000 | Loss: 0.00003278
Iteration 18/1000 | Loss: 0.00003278
Iteration 19/1000 | Loss: 0.00003278
Iteration 20/1000 | Loss: 0.00003277
Iteration 21/1000 | Loss: 0.00003270
Iteration 22/1000 | Loss: 0.00003270
Iteration 23/1000 | Loss: 0.00003269
Iteration 24/1000 | Loss: 0.00003269
Iteration 25/1000 | Loss: 0.00003269
Iteration 26/1000 | Loss: 0.00003268
Iteration 27/1000 | Loss: 0.00003268
Iteration 28/1000 | Loss: 0.00003268
Iteration 29/1000 | Loss: 0.00003267
Iteration 30/1000 | Loss: 0.00003262
Iteration 31/1000 | Loss: 0.00003262
Iteration 32/1000 | Loss: 0.00003262
Iteration 33/1000 | Loss: 0.00003261
Iteration 34/1000 | Loss: 0.00003258
Iteration 35/1000 | Loss: 0.00003258
Iteration 36/1000 | Loss: 0.00003257
Iteration 37/1000 | Loss: 0.00003257
Iteration 38/1000 | Loss: 0.00003257
Iteration 39/1000 | Loss: 0.00003257
Iteration 40/1000 | Loss: 0.00003256
Iteration 41/1000 | Loss: 0.00003256
Iteration 42/1000 | Loss: 0.00003256
Iteration 43/1000 | Loss: 0.00003256
Iteration 44/1000 | Loss: 0.00003256
Iteration 45/1000 | Loss: 0.00003256
Iteration 46/1000 | Loss: 0.00003255
Iteration 47/1000 | Loss: 0.00003255
Iteration 48/1000 | Loss: 0.00003255
Iteration 49/1000 | Loss: 0.00003255
Iteration 50/1000 | Loss: 0.00003255
Iteration 51/1000 | Loss: 0.00003255
Iteration 52/1000 | Loss: 0.00003255
Iteration 53/1000 | Loss: 0.00003254
Iteration 54/1000 | Loss: 0.00003254
Iteration 55/1000 | Loss: 0.00003254
Iteration 56/1000 | Loss: 0.00003254
Iteration 57/1000 | Loss: 0.00003254
Iteration 58/1000 | Loss: 0.00003253
Iteration 59/1000 | Loss: 0.00003253
Iteration 60/1000 | Loss: 0.00003253
Iteration 61/1000 | Loss: 0.00003253
Iteration 62/1000 | Loss: 0.00003253
Iteration 63/1000 | Loss: 0.00003253
Iteration 64/1000 | Loss: 0.00003253
Iteration 65/1000 | Loss: 0.00003253
Iteration 66/1000 | Loss: 0.00003253
Iteration 67/1000 | Loss: 0.00003252
Iteration 68/1000 | Loss: 0.00003252
Iteration 69/1000 | Loss: 0.00003252
Iteration 70/1000 | Loss: 0.00003252
Iteration 71/1000 | Loss: 0.00003251
Iteration 72/1000 | Loss: 0.00003251
Iteration 73/1000 | Loss: 0.00003251
Iteration 74/1000 | Loss: 0.00003250
Iteration 75/1000 | Loss: 0.00003250
Iteration 76/1000 | Loss: 0.00003250
Iteration 77/1000 | Loss: 0.00003249
Iteration 78/1000 | Loss: 0.00003249
Iteration 79/1000 | Loss: 0.00003249
Iteration 80/1000 | Loss: 0.00003249
Iteration 81/1000 | Loss: 0.00003249
Iteration 82/1000 | Loss: 0.00003249
Iteration 83/1000 | Loss: 0.00003249
Iteration 84/1000 | Loss: 0.00003249
Iteration 85/1000 | Loss: 0.00003249
Iteration 86/1000 | Loss: 0.00003249
Iteration 87/1000 | Loss: 0.00003248
Iteration 88/1000 | Loss: 0.00003248
Iteration 89/1000 | Loss: 0.00003248
Iteration 90/1000 | Loss: 0.00003248
Iteration 91/1000 | Loss: 0.00003248
Iteration 92/1000 | Loss: 0.00003248
Iteration 93/1000 | Loss: 0.00003248
Iteration 94/1000 | Loss: 0.00003248
Iteration 95/1000 | Loss: 0.00003248
Iteration 96/1000 | Loss: 0.00003248
Iteration 97/1000 | Loss: 0.00003248
Iteration 98/1000 | Loss: 0.00003248
Iteration 99/1000 | Loss: 0.00003248
Iteration 100/1000 | Loss: 0.00003248
Iteration 101/1000 | Loss: 0.00003248
Iteration 102/1000 | Loss: 0.00003248
Iteration 103/1000 | Loss: 0.00003248
Iteration 104/1000 | Loss: 0.00003248
Iteration 105/1000 | Loss: 0.00003248
Iteration 106/1000 | Loss: 0.00003248
Iteration 107/1000 | Loss: 0.00003248
Iteration 108/1000 | Loss: 0.00003248
Iteration 109/1000 | Loss: 0.00003248
Iteration 110/1000 | Loss: 0.00003248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [3.247546555940062e-05, 3.247546555940062e-05, 3.247546555940062e-05, 3.247546555940062e-05, 3.247546555940062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.247546555940062e-05

Optimization complete. Final v2v error: 4.535728454589844 mm

Highest mean error: 4.610408782958984 mm for frame 32

Lowest mean error: 4.399092674255371 mm for frame 117

Saving results

Total time: 38.80968761444092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526481
Iteration 2/25 | Loss: 0.00124929
Iteration 3/25 | Loss: 0.00114053
Iteration 4/25 | Loss: 0.00113129
Iteration 5/25 | Loss: 0.00112939
Iteration 6/25 | Loss: 0.00112922
Iteration 7/25 | Loss: 0.00112922
Iteration 8/25 | Loss: 0.00112922
Iteration 9/25 | Loss: 0.00112922
Iteration 10/25 | Loss: 0.00112922
Iteration 11/25 | Loss: 0.00112922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011292177950963378, 0.0011292177950963378, 0.0011292177950963378, 0.0011292177950963378, 0.0011292177950963378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011292177950963378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.46408415
Iteration 2/25 | Loss: 0.00059671
Iteration 3/25 | Loss: 0.00059670
Iteration 4/25 | Loss: 0.00059670
Iteration 5/25 | Loss: 0.00059670
Iteration 6/25 | Loss: 0.00059670
Iteration 7/25 | Loss: 0.00059670
Iteration 8/25 | Loss: 0.00059670
Iteration 9/25 | Loss: 0.00059670
Iteration 10/25 | Loss: 0.00059670
Iteration 11/25 | Loss: 0.00059670
Iteration 12/25 | Loss: 0.00059670
Iteration 13/25 | Loss: 0.00059670
Iteration 14/25 | Loss: 0.00059670
Iteration 15/25 | Loss: 0.00059670
Iteration 16/25 | Loss: 0.00059670
Iteration 17/25 | Loss: 0.00059670
Iteration 18/25 | Loss: 0.00059670
Iteration 19/25 | Loss: 0.00059670
Iteration 20/25 | Loss: 0.00059670
Iteration 21/25 | Loss: 0.00059670
Iteration 22/25 | Loss: 0.00059670
Iteration 23/25 | Loss: 0.00059670
Iteration 24/25 | Loss: 0.00059670
Iteration 25/25 | Loss: 0.00059670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059670
Iteration 2/1000 | Loss: 0.00002667
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001430
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00001398
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001393
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001388
Iteration 32/1000 | Loss: 0.00001388
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001384
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001381
Iteration 38/1000 | Loss: 0.00001380
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001380
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001373
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001372
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001368
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001366
Iteration 83/1000 | Loss: 0.00001366
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001363
Iteration 95/1000 | Loss: 0.00001363
Iteration 96/1000 | Loss: 0.00001363
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001361
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001360
Iteration 118/1000 | Loss: 0.00001360
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001359
Iteration 124/1000 | Loss: 0.00001359
Iteration 125/1000 | Loss: 0.00001359
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001358
Iteration 131/1000 | Loss: 0.00001358
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001356
Iteration 142/1000 | Loss: 0.00001356
Iteration 143/1000 | Loss: 0.00001356
Iteration 144/1000 | Loss: 0.00001356
Iteration 145/1000 | Loss: 0.00001356
Iteration 146/1000 | Loss: 0.00001356
Iteration 147/1000 | Loss: 0.00001356
Iteration 148/1000 | Loss: 0.00001355
Iteration 149/1000 | Loss: 0.00001355
Iteration 150/1000 | Loss: 0.00001355
Iteration 151/1000 | Loss: 0.00001355
Iteration 152/1000 | Loss: 0.00001355
Iteration 153/1000 | Loss: 0.00001355
Iteration 154/1000 | Loss: 0.00001355
Iteration 155/1000 | Loss: 0.00001355
Iteration 156/1000 | Loss: 0.00001355
Iteration 157/1000 | Loss: 0.00001355
Iteration 158/1000 | Loss: 0.00001355
Iteration 159/1000 | Loss: 0.00001355
Iteration 160/1000 | Loss: 0.00001354
Iteration 161/1000 | Loss: 0.00001354
Iteration 162/1000 | Loss: 0.00001354
Iteration 163/1000 | Loss: 0.00001354
Iteration 164/1000 | Loss: 0.00001354
Iteration 165/1000 | Loss: 0.00001354
Iteration 166/1000 | Loss: 0.00001354
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001354
Iteration 182/1000 | Loss: 0.00001354
Iteration 183/1000 | Loss: 0.00001353
Iteration 184/1000 | Loss: 0.00001353
Iteration 185/1000 | Loss: 0.00001353
Iteration 186/1000 | Loss: 0.00001353
Iteration 187/1000 | Loss: 0.00001353
Iteration 188/1000 | Loss: 0.00001353
Iteration 189/1000 | Loss: 0.00001353
Iteration 190/1000 | Loss: 0.00001353
Iteration 191/1000 | Loss: 0.00001353
Iteration 192/1000 | Loss: 0.00001353
Iteration 193/1000 | Loss: 0.00001353
Iteration 194/1000 | Loss: 0.00001353
Iteration 195/1000 | Loss: 0.00001353
Iteration 196/1000 | Loss: 0.00001353
Iteration 197/1000 | Loss: 0.00001353
Iteration 198/1000 | Loss: 0.00001353
Iteration 199/1000 | Loss: 0.00001353
Iteration 200/1000 | Loss: 0.00001353
Iteration 201/1000 | Loss: 0.00001353
Iteration 202/1000 | Loss: 0.00001353
Iteration 203/1000 | Loss: 0.00001353
Iteration 204/1000 | Loss: 0.00001353
Iteration 205/1000 | Loss: 0.00001353
Iteration 206/1000 | Loss: 0.00001353
Iteration 207/1000 | Loss: 0.00001353
Iteration 208/1000 | Loss: 0.00001353
Iteration 209/1000 | Loss: 0.00001353
Iteration 210/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.3531866898119915e-05, 1.3531866898119915e-05, 1.3531866898119915e-05, 1.3531866898119915e-05, 1.3531866898119915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3531866898119915e-05

Optimization complete. Final v2v error: 3.0614781379699707 mm

Highest mean error: 3.7603790760040283 mm for frame 69

Lowest mean error: 2.658838987350464 mm for frame 12

Saving results

Total time: 38.858105182647705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382384
Iteration 2/25 | Loss: 0.00114254
Iteration 3/25 | Loss: 0.00105661
Iteration 4/25 | Loss: 0.00104238
Iteration 5/25 | Loss: 0.00103735
Iteration 6/25 | Loss: 0.00103672
Iteration 7/25 | Loss: 0.00103672
Iteration 8/25 | Loss: 0.00103672
Iteration 9/25 | Loss: 0.00103672
Iteration 10/25 | Loss: 0.00103672
Iteration 11/25 | Loss: 0.00103672
Iteration 12/25 | Loss: 0.00103672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010367167415097356, 0.0010367167415097356, 0.0010367167415097356, 0.0010367167415097356, 0.0010367167415097356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010367167415097356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48749876
Iteration 2/25 | Loss: 0.00061674
Iteration 3/25 | Loss: 0.00061674
Iteration 4/25 | Loss: 0.00061674
Iteration 5/25 | Loss: 0.00061674
Iteration 6/25 | Loss: 0.00061674
Iteration 7/25 | Loss: 0.00061674
Iteration 8/25 | Loss: 0.00061674
Iteration 9/25 | Loss: 0.00061674
Iteration 10/25 | Loss: 0.00061674
Iteration 11/25 | Loss: 0.00061674
Iteration 12/25 | Loss: 0.00061674
Iteration 13/25 | Loss: 0.00061674
Iteration 14/25 | Loss: 0.00061674
Iteration 15/25 | Loss: 0.00061674
Iteration 16/25 | Loss: 0.00061674
Iteration 17/25 | Loss: 0.00061674
Iteration 18/25 | Loss: 0.00061674
Iteration 19/25 | Loss: 0.00061674
Iteration 20/25 | Loss: 0.00061674
Iteration 21/25 | Loss: 0.00061674
Iteration 22/25 | Loss: 0.00061674
Iteration 23/25 | Loss: 0.00061674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006167410174384713, 0.0006167410174384713, 0.0006167410174384713, 0.0006167410174384713, 0.0006167410174384713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006167410174384713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061674
Iteration 2/1000 | Loss: 0.00001590
Iteration 3/1000 | Loss: 0.00001026
Iteration 4/1000 | Loss: 0.00000937
Iteration 5/1000 | Loss: 0.00000892
Iteration 6/1000 | Loss: 0.00000863
Iteration 7/1000 | Loss: 0.00000860
Iteration 8/1000 | Loss: 0.00000853
Iteration 9/1000 | Loss: 0.00000852
Iteration 10/1000 | Loss: 0.00000852
Iteration 11/1000 | Loss: 0.00000851
Iteration 12/1000 | Loss: 0.00000850
Iteration 13/1000 | Loss: 0.00000849
Iteration 14/1000 | Loss: 0.00000843
Iteration 15/1000 | Loss: 0.00000841
Iteration 16/1000 | Loss: 0.00000840
Iteration 17/1000 | Loss: 0.00000839
Iteration 18/1000 | Loss: 0.00000827
Iteration 19/1000 | Loss: 0.00000826
Iteration 20/1000 | Loss: 0.00000825
Iteration 21/1000 | Loss: 0.00000824
Iteration 22/1000 | Loss: 0.00000816
Iteration 23/1000 | Loss: 0.00000811
Iteration 24/1000 | Loss: 0.00000808
Iteration 25/1000 | Loss: 0.00000807
Iteration 26/1000 | Loss: 0.00000806
Iteration 27/1000 | Loss: 0.00000806
Iteration 28/1000 | Loss: 0.00000806
Iteration 29/1000 | Loss: 0.00000803
Iteration 30/1000 | Loss: 0.00000803
Iteration 31/1000 | Loss: 0.00000803
Iteration 32/1000 | Loss: 0.00000803
Iteration 33/1000 | Loss: 0.00000801
Iteration 34/1000 | Loss: 0.00000800
Iteration 35/1000 | Loss: 0.00000799
Iteration 36/1000 | Loss: 0.00000798
Iteration 37/1000 | Loss: 0.00000798
Iteration 38/1000 | Loss: 0.00000798
Iteration 39/1000 | Loss: 0.00000798
Iteration 40/1000 | Loss: 0.00000797
Iteration 41/1000 | Loss: 0.00000797
Iteration 42/1000 | Loss: 0.00000796
Iteration 43/1000 | Loss: 0.00000796
Iteration 44/1000 | Loss: 0.00000791
Iteration 45/1000 | Loss: 0.00000790
Iteration 46/1000 | Loss: 0.00000789
Iteration 47/1000 | Loss: 0.00000789
Iteration 48/1000 | Loss: 0.00000789
Iteration 49/1000 | Loss: 0.00000788
Iteration 50/1000 | Loss: 0.00000788
Iteration 51/1000 | Loss: 0.00000788
Iteration 52/1000 | Loss: 0.00000787
Iteration 53/1000 | Loss: 0.00000787
Iteration 54/1000 | Loss: 0.00000787
Iteration 55/1000 | Loss: 0.00000787
Iteration 56/1000 | Loss: 0.00000787
Iteration 57/1000 | Loss: 0.00000786
Iteration 58/1000 | Loss: 0.00000786
Iteration 59/1000 | Loss: 0.00000785
Iteration 60/1000 | Loss: 0.00000784
Iteration 61/1000 | Loss: 0.00000783
Iteration 62/1000 | Loss: 0.00000783
Iteration 63/1000 | Loss: 0.00000783
Iteration 64/1000 | Loss: 0.00000783
Iteration 65/1000 | Loss: 0.00000782
Iteration 66/1000 | Loss: 0.00000782
Iteration 67/1000 | Loss: 0.00000782
Iteration 68/1000 | Loss: 0.00000782
Iteration 69/1000 | Loss: 0.00000781
Iteration 70/1000 | Loss: 0.00000781
Iteration 71/1000 | Loss: 0.00000781
Iteration 72/1000 | Loss: 0.00000781
Iteration 73/1000 | Loss: 0.00000781
Iteration 74/1000 | Loss: 0.00000781
Iteration 75/1000 | Loss: 0.00000781
Iteration 76/1000 | Loss: 0.00000781
Iteration 77/1000 | Loss: 0.00000781
Iteration 78/1000 | Loss: 0.00000781
Iteration 79/1000 | Loss: 0.00000781
Iteration 80/1000 | Loss: 0.00000781
Iteration 81/1000 | Loss: 0.00000780
Iteration 82/1000 | Loss: 0.00000780
Iteration 83/1000 | Loss: 0.00000780
Iteration 84/1000 | Loss: 0.00000780
Iteration 85/1000 | Loss: 0.00000780
Iteration 86/1000 | Loss: 0.00000780
Iteration 87/1000 | Loss: 0.00000780
Iteration 88/1000 | Loss: 0.00000779
Iteration 89/1000 | Loss: 0.00000779
Iteration 90/1000 | Loss: 0.00000779
Iteration 91/1000 | Loss: 0.00000778
Iteration 92/1000 | Loss: 0.00000778
Iteration 93/1000 | Loss: 0.00000778
Iteration 94/1000 | Loss: 0.00000777
Iteration 95/1000 | Loss: 0.00000777
Iteration 96/1000 | Loss: 0.00000777
Iteration 97/1000 | Loss: 0.00000776
Iteration 98/1000 | Loss: 0.00000776
Iteration 99/1000 | Loss: 0.00000776
Iteration 100/1000 | Loss: 0.00000775
Iteration 101/1000 | Loss: 0.00000775
Iteration 102/1000 | Loss: 0.00000774
Iteration 103/1000 | Loss: 0.00000774
Iteration 104/1000 | Loss: 0.00000774
Iteration 105/1000 | Loss: 0.00000774
Iteration 106/1000 | Loss: 0.00000774
Iteration 107/1000 | Loss: 0.00000774
Iteration 108/1000 | Loss: 0.00000773
Iteration 109/1000 | Loss: 0.00000773
Iteration 110/1000 | Loss: 0.00000771
Iteration 111/1000 | Loss: 0.00000770
Iteration 112/1000 | Loss: 0.00000769
Iteration 113/1000 | Loss: 0.00000769
Iteration 114/1000 | Loss: 0.00000769
Iteration 115/1000 | Loss: 0.00000769
Iteration 116/1000 | Loss: 0.00000768
Iteration 117/1000 | Loss: 0.00000768
Iteration 118/1000 | Loss: 0.00000767
Iteration 119/1000 | Loss: 0.00000765
Iteration 120/1000 | Loss: 0.00000765
Iteration 121/1000 | Loss: 0.00000765
Iteration 122/1000 | Loss: 0.00000764
Iteration 123/1000 | Loss: 0.00000764
Iteration 124/1000 | Loss: 0.00000764
Iteration 125/1000 | Loss: 0.00000764
Iteration 126/1000 | Loss: 0.00000764
Iteration 127/1000 | Loss: 0.00000764
Iteration 128/1000 | Loss: 0.00000763
Iteration 129/1000 | Loss: 0.00000762
Iteration 130/1000 | Loss: 0.00000762
Iteration 131/1000 | Loss: 0.00000762
Iteration 132/1000 | Loss: 0.00000762
Iteration 133/1000 | Loss: 0.00000762
Iteration 134/1000 | Loss: 0.00000762
Iteration 135/1000 | Loss: 0.00000762
Iteration 136/1000 | Loss: 0.00000762
Iteration 137/1000 | Loss: 0.00000762
Iteration 138/1000 | Loss: 0.00000761
Iteration 139/1000 | Loss: 0.00000761
Iteration 140/1000 | Loss: 0.00000761
Iteration 141/1000 | Loss: 0.00000761
Iteration 142/1000 | Loss: 0.00000761
Iteration 143/1000 | Loss: 0.00000761
Iteration 144/1000 | Loss: 0.00000761
Iteration 145/1000 | Loss: 0.00000761
Iteration 146/1000 | Loss: 0.00000761
Iteration 147/1000 | Loss: 0.00000761
Iteration 148/1000 | Loss: 0.00000760
Iteration 149/1000 | Loss: 0.00000760
Iteration 150/1000 | Loss: 0.00000759
Iteration 151/1000 | Loss: 0.00000759
Iteration 152/1000 | Loss: 0.00000759
Iteration 153/1000 | Loss: 0.00000759
Iteration 154/1000 | Loss: 0.00000758
Iteration 155/1000 | Loss: 0.00000758
Iteration 156/1000 | Loss: 0.00000758
Iteration 157/1000 | Loss: 0.00000758
Iteration 158/1000 | Loss: 0.00000758
Iteration 159/1000 | Loss: 0.00000758
Iteration 160/1000 | Loss: 0.00000758
Iteration 161/1000 | Loss: 0.00000758
Iteration 162/1000 | Loss: 0.00000758
Iteration 163/1000 | Loss: 0.00000758
Iteration 164/1000 | Loss: 0.00000758
Iteration 165/1000 | Loss: 0.00000757
Iteration 166/1000 | Loss: 0.00000757
Iteration 167/1000 | Loss: 0.00000757
Iteration 168/1000 | Loss: 0.00000757
Iteration 169/1000 | Loss: 0.00000757
Iteration 170/1000 | Loss: 0.00000757
Iteration 171/1000 | Loss: 0.00000757
Iteration 172/1000 | Loss: 0.00000756
Iteration 173/1000 | Loss: 0.00000756
Iteration 174/1000 | Loss: 0.00000756
Iteration 175/1000 | Loss: 0.00000756
Iteration 176/1000 | Loss: 0.00000756
Iteration 177/1000 | Loss: 0.00000756
Iteration 178/1000 | Loss: 0.00000755
Iteration 179/1000 | Loss: 0.00000755
Iteration 180/1000 | Loss: 0.00000755
Iteration 181/1000 | Loss: 0.00000755
Iteration 182/1000 | Loss: 0.00000755
Iteration 183/1000 | Loss: 0.00000755
Iteration 184/1000 | Loss: 0.00000755
Iteration 185/1000 | Loss: 0.00000755
Iteration 186/1000 | Loss: 0.00000755
Iteration 187/1000 | Loss: 0.00000755
Iteration 188/1000 | Loss: 0.00000755
Iteration 189/1000 | Loss: 0.00000755
Iteration 190/1000 | Loss: 0.00000754
Iteration 191/1000 | Loss: 0.00000754
Iteration 192/1000 | Loss: 0.00000754
Iteration 193/1000 | Loss: 0.00000754
Iteration 194/1000 | Loss: 0.00000754
Iteration 195/1000 | Loss: 0.00000754
Iteration 196/1000 | Loss: 0.00000754
Iteration 197/1000 | Loss: 0.00000754
Iteration 198/1000 | Loss: 0.00000754
Iteration 199/1000 | Loss: 0.00000754
Iteration 200/1000 | Loss: 0.00000754
Iteration 201/1000 | Loss: 0.00000754
Iteration 202/1000 | Loss: 0.00000754
Iteration 203/1000 | Loss: 0.00000754
Iteration 204/1000 | Loss: 0.00000754
Iteration 205/1000 | Loss: 0.00000754
Iteration 206/1000 | Loss: 0.00000754
Iteration 207/1000 | Loss: 0.00000754
Iteration 208/1000 | Loss: 0.00000754
Iteration 209/1000 | Loss: 0.00000754
Iteration 210/1000 | Loss: 0.00000754
Iteration 211/1000 | Loss: 0.00000754
Iteration 212/1000 | Loss: 0.00000754
Iteration 213/1000 | Loss: 0.00000754
Iteration 214/1000 | Loss: 0.00000754
Iteration 215/1000 | Loss: 0.00000754
Iteration 216/1000 | Loss: 0.00000754
Iteration 217/1000 | Loss: 0.00000754
Iteration 218/1000 | Loss: 0.00000754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [7.535449640272418e-06, 7.535449640272418e-06, 7.535449640272418e-06, 7.535449640272418e-06, 7.535449640272418e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.535449640272418e-06

Optimization complete. Final v2v error: 2.3880343437194824 mm

Highest mean error: 2.4560182094573975 mm for frame 17

Lowest mean error: 2.340510368347168 mm for frame 86

Saving results

Total time: 38.59923529624939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795164
Iteration 2/25 | Loss: 0.00120927
Iteration 3/25 | Loss: 0.00107556
Iteration 4/25 | Loss: 0.00106109
Iteration 5/25 | Loss: 0.00105837
Iteration 6/25 | Loss: 0.00105832
Iteration 7/25 | Loss: 0.00105832
Iteration 8/25 | Loss: 0.00105832
Iteration 9/25 | Loss: 0.00105832
Iteration 10/25 | Loss: 0.00105832
Iteration 11/25 | Loss: 0.00105832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010583173716440797, 0.0010583173716440797, 0.0010583173716440797, 0.0010583173716440797, 0.0010583173716440797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010583173716440797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40258670
Iteration 2/25 | Loss: 0.00057834
Iteration 3/25 | Loss: 0.00057834
Iteration 4/25 | Loss: 0.00057834
Iteration 5/25 | Loss: 0.00057834
Iteration 6/25 | Loss: 0.00057834
Iteration 7/25 | Loss: 0.00057834
Iteration 8/25 | Loss: 0.00057834
Iteration 9/25 | Loss: 0.00057834
Iteration 10/25 | Loss: 0.00057834
Iteration 11/25 | Loss: 0.00057834
Iteration 12/25 | Loss: 0.00057834
Iteration 13/25 | Loss: 0.00057834
Iteration 14/25 | Loss: 0.00057834
Iteration 15/25 | Loss: 0.00057834
Iteration 16/25 | Loss: 0.00057834
Iteration 17/25 | Loss: 0.00057834
Iteration 18/25 | Loss: 0.00057834
Iteration 19/25 | Loss: 0.00057834
Iteration 20/25 | Loss: 0.00057834
Iteration 21/25 | Loss: 0.00057834
Iteration 22/25 | Loss: 0.00057834
Iteration 23/25 | Loss: 0.00057834
Iteration 24/25 | Loss: 0.00057834
Iteration 25/25 | Loss: 0.00057834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057834
Iteration 2/1000 | Loss: 0.00002060
Iteration 3/1000 | Loss: 0.00001341
Iteration 4/1000 | Loss: 0.00001180
Iteration 5/1000 | Loss: 0.00001111
Iteration 6/1000 | Loss: 0.00001052
Iteration 7/1000 | Loss: 0.00001011
Iteration 8/1000 | Loss: 0.00001000
Iteration 9/1000 | Loss: 0.00000999
Iteration 10/1000 | Loss: 0.00000999
Iteration 11/1000 | Loss: 0.00000990
Iteration 12/1000 | Loss: 0.00000989
Iteration 13/1000 | Loss: 0.00000964
Iteration 14/1000 | Loss: 0.00000956
Iteration 15/1000 | Loss: 0.00000953
Iteration 16/1000 | Loss: 0.00000946
Iteration 17/1000 | Loss: 0.00000945
Iteration 18/1000 | Loss: 0.00000945
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000943
Iteration 21/1000 | Loss: 0.00000943
Iteration 22/1000 | Loss: 0.00000943
Iteration 23/1000 | Loss: 0.00000943
Iteration 24/1000 | Loss: 0.00000942
Iteration 25/1000 | Loss: 0.00000942
Iteration 26/1000 | Loss: 0.00000942
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000938
Iteration 29/1000 | Loss: 0.00000937
Iteration 30/1000 | Loss: 0.00000936
Iteration 31/1000 | Loss: 0.00000936
Iteration 32/1000 | Loss: 0.00000935
Iteration 33/1000 | Loss: 0.00000934
Iteration 34/1000 | Loss: 0.00000934
Iteration 35/1000 | Loss: 0.00000933
Iteration 36/1000 | Loss: 0.00000933
Iteration 37/1000 | Loss: 0.00000933
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000932
Iteration 41/1000 | Loss: 0.00000931
Iteration 42/1000 | Loss: 0.00000931
Iteration 43/1000 | Loss: 0.00000931
Iteration 44/1000 | Loss: 0.00000931
Iteration 45/1000 | Loss: 0.00000931
Iteration 46/1000 | Loss: 0.00000931
Iteration 47/1000 | Loss: 0.00000930
Iteration 48/1000 | Loss: 0.00000929
Iteration 49/1000 | Loss: 0.00000929
Iteration 50/1000 | Loss: 0.00000929
Iteration 51/1000 | Loss: 0.00000928
Iteration 52/1000 | Loss: 0.00000928
Iteration 53/1000 | Loss: 0.00000928
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000927
Iteration 56/1000 | Loss: 0.00000926
Iteration 57/1000 | Loss: 0.00000926
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000925
Iteration 60/1000 | Loss: 0.00000924
Iteration 61/1000 | Loss: 0.00000924
Iteration 62/1000 | Loss: 0.00000924
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000924
Iteration 65/1000 | Loss: 0.00000924
Iteration 66/1000 | Loss: 0.00000923
Iteration 67/1000 | Loss: 0.00000923
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000922
Iteration 70/1000 | Loss: 0.00000922
Iteration 71/1000 | Loss: 0.00000922
Iteration 72/1000 | Loss: 0.00000922
Iteration 73/1000 | Loss: 0.00000922
Iteration 74/1000 | Loss: 0.00000921
Iteration 75/1000 | Loss: 0.00000921
Iteration 76/1000 | Loss: 0.00000921
Iteration 77/1000 | Loss: 0.00000921
Iteration 78/1000 | Loss: 0.00000921
Iteration 79/1000 | Loss: 0.00000921
Iteration 80/1000 | Loss: 0.00000921
Iteration 81/1000 | Loss: 0.00000921
Iteration 82/1000 | Loss: 0.00000921
Iteration 83/1000 | Loss: 0.00000921
Iteration 84/1000 | Loss: 0.00000921
Iteration 85/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [9.214029887516517e-06, 9.214029887516517e-06, 9.214029887516517e-06, 9.214029887516517e-06, 9.214029887516517e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.214029887516517e-06

Optimization complete. Final v2v error: 2.592207431793213 mm

Highest mean error: 2.7835497856140137 mm for frame 46

Lowest mean error: 2.4310696125030518 mm for frame 147

Saving results

Total time: 29.35617756843567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394978
Iteration 2/25 | Loss: 0.00116740
Iteration 3/25 | Loss: 0.00108942
Iteration 4/25 | Loss: 0.00107408
Iteration 5/25 | Loss: 0.00107064
Iteration 6/25 | Loss: 0.00107024
Iteration 7/25 | Loss: 0.00107024
Iteration 8/25 | Loss: 0.00107024
Iteration 9/25 | Loss: 0.00107024
Iteration 10/25 | Loss: 0.00107024
Iteration 11/25 | Loss: 0.00107024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010702372528612614, 0.0010702372528612614, 0.0010702372528612614, 0.0010702372528612614, 0.0010702372528612614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010702372528612614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54301453
Iteration 2/25 | Loss: 0.00058059
Iteration 3/25 | Loss: 0.00058059
Iteration 4/25 | Loss: 0.00058059
Iteration 5/25 | Loss: 0.00058058
Iteration 6/25 | Loss: 0.00058058
Iteration 7/25 | Loss: 0.00058058
Iteration 8/25 | Loss: 0.00058058
Iteration 9/25 | Loss: 0.00058058
Iteration 10/25 | Loss: 0.00058058
Iteration 11/25 | Loss: 0.00058058
Iteration 12/25 | Loss: 0.00058058
Iteration 13/25 | Loss: 0.00058058
Iteration 14/25 | Loss: 0.00058058
Iteration 15/25 | Loss: 0.00058058
Iteration 16/25 | Loss: 0.00058058
Iteration 17/25 | Loss: 0.00058058
Iteration 18/25 | Loss: 0.00058058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005805836990475655, 0.0005805836990475655, 0.0005805836990475655, 0.0005805836990475655, 0.0005805836990475655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005805836990475655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058058
Iteration 2/1000 | Loss: 0.00001981
Iteration 3/1000 | Loss: 0.00001479
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001297
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001237
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001215
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001204
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001200
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001192
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001191
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001186
Iteration 42/1000 | Loss: 0.00001186
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001172
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.1716098015313037e-05, 1.1716098015313037e-05, 1.1716098015313037e-05, 1.1716098015313037e-05, 1.1716098015313037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1716098015313037e-05

Optimization complete. Final v2v error: 2.944275379180908 mm

Highest mean error: 3.3114187717437744 mm for frame 187

Lowest mean error: 2.7806761264801025 mm for frame 211

Saving results

Total time: 36.81913089752197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415816
Iteration 2/25 | Loss: 0.00116547
Iteration 3/25 | Loss: 0.00110055
Iteration 4/25 | Loss: 0.00108366
Iteration 5/25 | Loss: 0.00107920
Iteration 6/25 | Loss: 0.00107846
Iteration 7/25 | Loss: 0.00107846
Iteration 8/25 | Loss: 0.00107846
Iteration 9/25 | Loss: 0.00107846
Iteration 10/25 | Loss: 0.00107846
Iteration 11/25 | Loss: 0.00107846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001078456873074174, 0.001078456873074174, 0.001078456873074174, 0.001078456873074174, 0.001078456873074174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001078456873074174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46196592
Iteration 2/25 | Loss: 0.00061084
Iteration 3/25 | Loss: 0.00061084
Iteration 4/25 | Loss: 0.00061084
Iteration 5/25 | Loss: 0.00061084
Iteration 6/25 | Loss: 0.00061084
Iteration 7/25 | Loss: 0.00061084
Iteration 8/25 | Loss: 0.00061084
Iteration 9/25 | Loss: 0.00061084
Iteration 10/25 | Loss: 0.00061084
Iteration 11/25 | Loss: 0.00061084
Iteration 12/25 | Loss: 0.00061084
Iteration 13/25 | Loss: 0.00061084
Iteration 14/25 | Loss: 0.00061084
Iteration 15/25 | Loss: 0.00061084
Iteration 16/25 | Loss: 0.00061084
Iteration 17/25 | Loss: 0.00061084
Iteration 18/25 | Loss: 0.00061084
Iteration 19/25 | Loss: 0.00061084
Iteration 20/25 | Loss: 0.00061084
Iteration 21/25 | Loss: 0.00061084
Iteration 22/25 | Loss: 0.00061084
Iteration 23/25 | Loss: 0.00061084
Iteration 24/25 | Loss: 0.00061084
Iteration 25/25 | Loss: 0.00061084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061084
Iteration 2/1000 | Loss: 0.00002626
Iteration 3/1000 | Loss: 0.00001720
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001423
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001340
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001304
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001273
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001251
Iteration 24/1000 | Loss: 0.00001251
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001239
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001230
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001227
Iteration 61/1000 | Loss: 0.00001227
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001226
Iteration 67/1000 | Loss: 0.00001226
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001226
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001225
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001224
Iteration 78/1000 | Loss: 0.00001224
Iteration 79/1000 | Loss: 0.00001224
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.2244201570865698e-05, 1.2244201570865698e-05, 1.2244201570865698e-05, 1.2244201570865698e-05, 1.2244201570865698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2244201570865698e-05

Optimization complete. Final v2v error: 3.0156586170196533 mm

Highest mean error: 3.3028616905212402 mm for frame 114

Lowest mean error: 2.915597677230835 mm for frame 140

Saving results

Total time: 31.659019470214844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832886
Iteration 2/25 | Loss: 0.00147586
Iteration 3/25 | Loss: 0.00118153
Iteration 4/25 | Loss: 0.00115350
Iteration 5/25 | Loss: 0.00114792
Iteration 6/25 | Loss: 0.00115968
Iteration 7/25 | Loss: 0.00114295
Iteration 8/25 | Loss: 0.00113537
Iteration 9/25 | Loss: 0.00113363
Iteration 10/25 | Loss: 0.00113338
Iteration 11/25 | Loss: 0.00113337
Iteration 12/25 | Loss: 0.00113337
Iteration 13/25 | Loss: 0.00113337
Iteration 14/25 | Loss: 0.00113337
Iteration 15/25 | Loss: 0.00113337
Iteration 16/25 | Loss: 0.00113337
Iteration 17/25 | Loss: 0.00113337
Iteration 18/25 | Loss: 0.00113337
Iteration 19/25 | Loss: 0.00113337
Iteration 20/25 | Loss: 0.00113337
Iteration 21/25 | Loss: 0.00113337
Iteration 22/25 | Loss: 0.00113337
Iteration 23/25 | Loss: 0.00113337
Iteration 24/25 | Loss: 0.00113337
Iteration 25/25 | Loss: 0.00113337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37827945
Iteration 2/25 | Loss: 0.00048774
Iteration 3/25 | Loss: 0.00048771
Iteration 4/25 | Loss: 0.00048771
Iteration 5/25 | Loss: 0.00048771
Iteration 6/25 | Loss: 0.00048771
Iteration 7/25 | Loss: 0.00048771
Iteration 8/25 | Loss: 0.00048771
Iteration 9/25 | Loss: 0.00048771
Iteration 10/25 | Loss: 0.00048771
Iteration 11/25 | Loss: 0.00048770
Iteration 12/25 | Loss: 0.00048770
Iteration 13/25 | Loss: 0.00048770
Iteration 14/25 | Loss: 0.00048770
Iteration 15/25 | Loss: 0.00048770
Iteration 16/25 | Loss: 0.00048770
Iteration 17/25 | Loss: 0.00048770
Iteration 18/25 | Loss: 0.00048770
Iteration 19/25 | Loss: 0.00048770
Iteration 20/25 | Loss: 0.00048770
Iteration 21/25 | Loss: 0.00048770
Iteration 22/25 | Loss: 0.00048770
Iteration 23/25 | Loss: 0.00048770
Iteration 24/25 | Loss: 0.00048770
Iteration 25/25 | Loss: 0.00048770

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048770
Iteration 2/1000 | Loss: 0.00003217
Iteration 3/1000 | Loss: 0.00002517
Iteration 4/1000 | Loss: 0.00002336
Iteration 5/1000 | Loss: 0.00002220
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001958
Iteration 12/1000 | Loss: 0.00001952
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001920
Iteration 15/1000 | Loss: 0.00001915
Iteration 16/1000 | Loss: 0.00001912
Iteration 17/1000 | Loss: 0.00001902
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001887
Iteration 22/1000 | Loss: 0.00001886
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001870
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001868
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001866
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001864
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001864
Iteration 48/1000 | Loss: 0.00001863
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001862
Iteration 51/1000 | Loss: 0.00001862
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001859
Iteration 56/1000 | Loss: 0.00001859
Iteration 57/1000 | Loss: 0.00001859
Iteration 58/1000 | Loss: 0.00001859
Iteration 59/1000 | Loss: 0.00001859
Iteration 60/1000 | Loss: 0.00001859
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001855
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001853
Iteration 80/1000 | Loss: 0.00001853
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001853
Iteration 83/1000 | Loss: 0.00001852
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001850
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001849
Iteration 96/1000 | Loss: 0.00001849
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001848
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001844
Iteration 121/1000 | Loss: 0.00001844
Iteration 122/1000 | Loss: 0.00001844
Iteration 123/1000 | Loss: 0.00001844
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001844
Iteration 128/1000 | Loss: 0.00001844
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001842
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00001842
Iteration 141/1000 | Loss: 0.00001842
Iteration 142/1000 | Loss: 0.00001842
Iteration 143/1000 | Loss: 0.00001842
Iteration 144/1000 | Loss: 0.00001842
Iteration 145/1000 | Loss: 0.00001842
Iteration 146/1000 | Loss: 0.00001842
Iteration 147/1000 | Loss: 0.00001841
Iteration 148/1000 | Loss: 0.00001841
Iteration 149/1000 | Loss: 0.00001841
Iteration 150/1000 | Loss: 0.00001841
Iteration 151/1000 | Loss: 0.00001841
Iteration 152/1000 | Loss: 0.00001841
Iteration 153/1000 | Loss: 0.00001841
Iteration 154/1000 | Loss: 0.00001841
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001841
Iteration 157/1000 | Loss: 0.00001841
Iteration 158/1000 | Loss: 0.00001841
Iteration 159/1000 | Loss: 0.00001841
Iteration 160/1000 | Loss: 0.00001841
Iteration 161/1000 | Loss: 0.00001841
Iteration 162/1000 | Loss: 0.00001841
Iteration 163/1000 | Loss: 0.00001840
Iteration 164/1000 | Loss: 0.00001840
Iteration 165/1000 | Loss: 0.00001840
Iteration 166/1000 | Loss: 0.00001840
Iteration 167/1000 | Loss: 0.00001840
Iteration 168/1000 | Loss: 0.00001840
Iteration 169/1000 | Loss: 0.00001840
Iteration 170/1000 | Loss: 0.00001840
Iteration 171/1000 | Loss: 0.00001840
Iteration 172/1000 | Loss: 0.00001839
Iteration 173/1000 | Loss: 0.00001839
Iteration 174/1000 | Loss: 0.00001839
Iteration 175/1000 | Loss: 0.00001839
Iteration 176/1000 | Loss: 0.00001839
Iteration 177/1000 | Loss: 0.00001839
Iteration 178/1000 | Loss: 0.00001839
Iteration 179/1000 | Loss: 0.00001839
Iteration 180/1000 | Loss: 0.00001839
Iteration 181/1000 | Loss: 0.00001839
Iteration 182/1000 | Loss: 0.00001839
Iteration 183/1000 | Loss: 0.00001839
Iteration 184/1000 | Loss: 0.00001839
Iteration 185/1000 | Loss: 0.00001839
Iteration 186/1000 | Loss: 0.00001839
Iteration 187/1000 | Loss: 0.00001839
Iteration 188/1000 | Loss: 0.00001839
Iteration 189/1000 | Loss: 0.00001839
Iteration 190/1000 | Loss: 0.00001839
Iteration 191/1000 | Loss: 0.00001838
Iteration 192/1000 | Loss: 0.00001838
Iteration 193/1000 | Loss: 0.00001838
Iteration 194/1000 | Loss: 0.00001838
Iteration 195/1000 | Loss: 0.00001838
Iteration 196/1000 | Loss: 0.00001838
Iteration 197/1000 | Loss: 0.00001838
Iteration 198/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.8384122085990384e-05, 1.8384122085990384e-05, 1.8384122085990384e-05, 1.8384122085990384e-05, 1.8384122085990384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8384122085990384e-05

Optimization complete. Final v2v error: 3.5316030979156494 mm

Highest mean error: 4.541324138641357 mm for frame 32

Lowest mean error: 2.896038293838501 mm for frame 72

Saving results

Total time: 58.25176286697388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049876
Iteration 2/25 | Loss: 0.00184366
Iteration 3/25 | Loss: 0.00124915
Iteration 4/25 | Loss: 0.00115891
Iteration 5/25 | Loss: 0.00115114
Iteration 6/25 | Loss: 0.00109909
Iteration 7/25 | Loss: 0.00107871
Iteration 8/25 | Loss: 0.00106190
Iteration 9/25 | Loss: 0.00106280
Iteration 10/25 | Loss: 0.00104983
Iteration 11/25 | Loss: 0.00104357
Iteration 12/25 | Loss: 0.00104407
Iteration 13/25 | Loss: 0.00104176
Iteration 14/25 | Loss: 0.00104114
Iteration 15/25 | Loss: 0.00104112
Iteration 16/25 | Loss: 0.00104112
Iteration 17/25 | Loss: 0.00104112
Iteration 18/25 | Loss: 0.00104112
Iteration 19/25 | Loss: 0.00104112
Iteration 20/25 | Loss: 0.00104112
Iteration 21/25 | Loss: 0.00104112
Iteration 22/25 | Loss: 0.00104111
Iteration 23/25 | Loss: 0.00104111
Iteration 24/25 | Loss: 0.00104111
Iteration 25/25 | Loss: 0.00104111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46507132
Iteration 2/25 | Loss: 0.00075799
Iteration 3/25 | Loss: 0.00071883
Iteration 4/25 | Loss: 0.00071883
Iteration 5/25 | Loss: 0.00071883
Iteration 6/25 | Loss: 0.00071882
Iteration 7/25 | Loss: 0.00071882
Iteration 8/25 | Loss: 0.00071882
Iteration 9/25 | Loss: 0.00071882
Iteration 10/25 | Loss: 0.00071882
Iteration 11/25 | Loss: 0.00071882
Iteration 12/25 | Loss: 0.00071882
Iteration 13/25 | Loss: 0.00071882
Iteration 14/25 | Loss: 0.00071882
Iteration 15/25 | Loss: 0.00071882
Iteration 16/25 | Loss: 0.00071882
Iteration 17/25 | Loss: 0.00071882
Iteration 18/25 | Loss: 0.00071882
Iteration 19/25 | Loss: 0.00071882
Iteration 20/25 | Loss: 0.00071882
Iteration 21/25 | Loss: 0.00071882
Iteration 22/25 | Loss: 0.00071882
Iteration 23/25 | Loss: 0.00071882
Iteration 24/25 | Loss: 0.00071882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007188226445578039, 0.0007188226445578039, 0.0007188226445578039, 0.0007188226445578039, 0.0007188226445578039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007188226445578039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071882
Iteration 2/1000 | Loss: 0.00009133
Iteration 3/1000 | Loss: 0.00004009
Iteration 4/1000 | Loss: 0.00009052
Iteration 5/1000 | Loss: 0.00004040
Iteration 6/1000 | Loss: 0.00015148
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00002423
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00001186
Iteration 11/1000 | Loss: 0.00077307
Iteration 12/1000 | Loss: 0.00011290
Iteration 13/1000 | Loss: 0.00001541
Iteration 14/1000 | Loss: 0.00001767
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00007013
Iteration 17/1000 | Loss: 0.00004280
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00001069
Iteration 21/1000 | Loss: 0.00000913
Iteration 22/1000 | Loss: 0.00000913
Iteration 23/1000 | Loss: 0.00000912
Iteration 24/1000 | Loss: 0.00000912
Iteration 25/1000 | Loss: 0.00000914
Iteration 26/1000 | Loss: 0.00000914
Iteration 27/1000 | Loss: 0.00000906
Iteration 28/1000 | Loss: 0.00000905
Iteration 29/1000 | Loss: 0.00000905
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00003878
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001373
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00003438
Iteration 37/1000 | Loss: 0.00005246
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00002995
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00002298
Iteration 43/1000 | Loss: 0.00000863
Iteration 44/1000 | Loss: 0.00000856
Iteration 45/1000 | Loss: 0.00000856
Iteration 46/1000 | Loss: 0.00000856
Iteration 47/1000 | Loss: 0.00000856
Iteration 48/1000 | Loss: 0.00000856
Iteration 49/1000 | Loss: 0.00000856
Iteration 50/1000 | Loss: 0.00000856
Iteration 51/1000 | Loss: 0.00000855
Iteration 52/1000 | Loss: 0.00000855
Iteration 53/1000 | Loss: 0.00000855
Iteration 54/1000 | Loss: 0.00000856
Iteration 55/1000 | Loss: 0.00000856
Iteration 56/1000 | Loss: 0.00000855
Iteration 57/1000 | Loss: 0.00000854
Iteration 58/1000 | Loss: 0.00000854
Iteration 59/1000 | Loss: 0.00000854
Iteration 60/1000 | Loss: 0.00000854
Iteration 61/1000 | Loss: 0.00000853
Iteration 62/1000 | Loss: 0.00000853
Iteration 63/1000 | Loss: 0.00000853
Iteration 64/1000 | Loss: 0.00000853
Iteration 65/1000 | Loss: 0.00000853
Iteration 66/1000 | Loss: 0.00000853
Iteration 67/1000 | Loss: 0.00000853
Iteration 68/1000 | Loss: 0.00000852
Iteration 69/1000 | Loss: 0.00001041
Iteration 70/1000 | Loss: 0.00000872
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00000857
Iteration 73/1000 | Loss: 0.00000850
Iteration 74/1000 | Loss: 0.00000857
Iteration 75/1000 | Loss: 0.00000851
Iteration 76/1000 | Loss: 0.00000849
Iteration 77/1000 | Loss: 0.00000849
Iteration 78/1000 | Loss: 0.00000849
Iteration 79/1000 | Loss: 0.00000849
Iteration 80/1000 | Loss: 0.00000849
Iteration 81/1000 | Loss: 0.00000849
Iteration 82/1000 | Loss: 0.00000848
Iteration 83/1000 | Loss: 0.00001029
Iteration 84/1000 | Loss: 0.00000845
Iteration 85/1000 | Loss: 0.00000845
Iteration 86/1000 | Loss: 0.00000845
Iteration 87/1000 | Loss: 0.00000844
Iteration 88/1000 | Loss: 0.00000844
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000844
Iteration 93/1000 | Loss: 0.00000844
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000843
Iteration 97/1000 | Loss: 0.00000843
Iteration 98/1000 | Loss: 0.00000843
Iteration 99/1000 | Loss: 0.00000843
Iteration 100/1000 | Loss: 0.00000843
Iteration 101/1000 | Loss: 0.00000843
Iteration 102/1000 | Loss: 0.00000843
Iteration 103/1000 | Loss: 0.00000843
Iteration 104/1000 | Loss: 0.00000843
Iteration 105/1000 | Loss: 0.00000843
Iteration 106/1000 | Loss: 0.00000843
Iteration 107/1000 | Loss: 0.00000843
Iteration 108/1000 | Loss: 0.00000843
Iteration 109/1000 | Loss: 0.00000843
Iteration 110/1000 | Loss: 0.00000843
Iteration 111/1000 | Loss: 0.00000843
Iteration 112/1000 | Loss: 0.00000843
Iteration 113/1000 | Loss: 0.00000843
Iteration 114/1000 | Loss: 0.00000843
Iteration 115/1000 | Loss: 0.00000843
Iteration 116/1000 | Loss: 0.00000843
Iteration 117/1000 | Loss: 0.00000843
Iteration 118/1000 | Loss: 0.00000843
Iteration 119/1000 | Loss: 0.00000843
Iteration 120/1000 | Loss: 0.00000843
Iteration 121/1000 | Loss: 0.00000843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [8.433831681031734e-06, 8.433831681031734e-06, 8.433831681031734e-06, 8.433831681031734e-06, 8.433831681031734e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.433831681031734e-06

Optimization complete. Final v2v error: 2.5198898315429688 mm

Highest mean error: 3.441251516342163 mm for frame 81

Lowest mean error: 2.3010830879211426 mm for frame 67

Saving results

Total time: 81.10198307037354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963132
Iteration 2/25 | Loss: 0.00963132
Iteration 3/25 | Loss: 0.00963132
Iteration 4/25 | Loss: 0.00963131
Iteration 5/25 | Loss: 0.00963131
Iteration 6/25 | Loss: 0.00963131
Iteration 7/25 | Loss: 0.00963131
Iteration 8/25 | Loss: 0.00963131
Iteration 9/25 | Loss: 0.00963131
Iteration 10/25 | Loss: 0.00963131
Iteration 11/25 | Loss: 0.00963131
Iteration 12/25 | Loss: 0.00963130
Iteration 13/25 | Loss: 0.00963130
Iteration 14/25 | Loss: 0.00963130
Iteration 15/25 | Loss: 0.00963130
Iteration 16/25 | Loss: 0.00963130
Iteration 17/25 | Loss: 0.00963130
Iteration 18/25 | Loss: 0.00963130
Iteration 19/25 | Loss: 0.00963129
Iteration 20/25 | Loss: 0.00963129
Iteration 21/25 | Loss: 0.00963129
Iteration 22/25 | Loss: 0.00963129
Iteration 23/25 | Loss: 0.00963129
Iteration 24/25 | Loss: 0.00963129
Iteration 25/25 | Loss: 0.00963129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79704487
Iteration 2/25 | Loss: 0.18482240
Iteration 3/25 | Loss: 0.18068275
Iteration 4/25 | Loss: 0.17974326
Iteration 5/25 | Loss: 0.17815378
Iteration 6/25 | Loss: 0.17814386
Iteration 7/25 | Loss: 0.17800023
Iteration 8/25 | Loss: 0.17800020
Iteration 9/25 | Loss: 0.17800020
Iteration 10/25 | Loss: 0.17800020
Iteration 11/25 | Loss: 0.17800018
Iteration 12/25 | Loss: 0.17800018
Iteration 13/25 | Loss: 0.17800015
Iteration 14/25 | Loss: 0.17800015
Iteration 15/25 | Loss: 0.17800015
Iteration 16/25 | Loss: 0.17800015
Iteration 17/25 | Loss: 0.17800015
Iteration 18/25 | Loss: 0.17800015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.17800015211105347, 0.17800015211105347, 0.17800015211105347, 0.17800015211105347, 0.17800015211105347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17800015211105347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17800015
Iteration 2/1000 | Loss: 0.02068738
Iteration 3/1000 | Loss: 0.00200853
Iteration 4/1000 | Loss: 0.00113701
Iteration 5/1000 | Loss: 0.00115675
Iteration 6/1000 | Loss: 0.00035245
Iteration 7/1000 | Loss: 0.00025579
Iteration 8/1000 | Loss: 0.00016770
Iteration 9/1000 | Loss: 0.00014452
Iteration 10/1000 | Loss: 0.00012461
Iteration 11/1000 | Loss: 0.00032619
Iteration 12/1000 | Loss: 0.00009582
Iteration 13/1000 | Loss: 0.00008090
Iteration 14/1000 | Loss: 0.00044156
Iteration 15/1000 | Loss: 0.00017851
Iteration 16/1000 | Loss: 0.00006488
Iteration 17/1000 | Loss: 0.00060939
Iteration 18/1000 | Loss: 0.00047255
Iteration 19/1000 | Loss: 0.00057138
Iteration 20/1000 | Loss: 0.00005202
Iteration 21/1000 | Loss: 0.00004526
Iteration 22/1000 | Loss: 0.00003949
Iteration 23/1000 | Loss: 0.00003568
Iteration 24/1000 | Loss: 0.00020192
Iteration 25/1000 | Loss: 0.00009163
Iteration 26/1000 | Loss: 0.00003070
Iteration 27/1000 | Loss: 0.00014102
Iteration 28/1000 | Loss: 0.00002896
Iteration 29/1000 | Loss: 0.00013855
Iteration 30/1000 | Loss: 0.00002774
Iteration 31/1000 | Loss: 0.00002703
Iteration 32/1000 | Loss: 0.00002639
Iteration 33/1000 | Loss: 0.00002586
Iteration 34/1000 | Loss: 0.00002540
Iteration 35/1000 | Loss: 0.00002503
Iteration 36/1000 | Loss: 0.00002473
Iteration 37/1000 | Loss: 0.00002450
Iteration 38/1000 | Loss: 0.00002433
Iteration 39/1000 | Loss: 0.00002428
Iteration 40/1000 | Loss: 0.00002428
Iteration 41/1000 | Loss: 0.00002427
Iteration 42/1000 | Loss: 0.00002424
Iteration 43/1000 | Loss: 0.00002417
Iteration 44/1000 | Loss: 0.00002412
Iteration 45/1000 | Loss: 0.00002412
Iteration 46/1000 | Loss: 0.00002411
Iteration 47/1000 | Loss: 0.00002411
Iteration 48/1000 | Loss: 0.00002410
Iteration 49/1000 | Loss: 0.00002410
Iteration 50/1000 | Loss: 0.00002409
Iteration 51/1000 | Loss: 0.00002409
Iteration 52/1000 | Loss: 0.00002409
Iteration 53/1000 | Loss: 0.00002408
Iteration 54/1000 | Loss: 0.00002408
Iteration 55/1000 | Loss: 0.00002405
Iteration 56/1000 | Loss: 0.00002405
Iteration 57/1000 | Loss: 0.00002401
Iteration 58/1000 | Loss: 0.00002400
Iteration 59/1000 | Loss: 0.00002400
Iteration 60/1000 | Loss: 0.00002398
Iteration 61/1000 | Loss: 0.00002398
Iteration 62/1000 | Loss: 0.00002393
Iteration 63/1000 | Loss: 0.00002390
Iteration 64/1000 | Loss: 0.00002390
Iteration 65/1000 | Loss: 0.00002390
Iteration 66/1000 | Loss: 0.00002389
Iteration 67/1000 | Loss: 0.00002389
Iteration 68/1000 | Loss: 0.00002389
Iteration 69/1000 | Loss: 0.00002388
Iteration 70/1000 | Loss: 0.00002388
Iteration 71/1000 | Loss: 0.00002388
Iteration 72/1000 | Loss: 0.00002388
Iteration 73/1000 | Loss: 0.00002387
Iteration 74/1000 | Loss: 0.00002387
Iteration 75/1000 | Loss: 0.00002387
Iteration 76/1000 | Loss: 0.00002387
Iteration 77/1000 | Loss: 0.00002386
Iteration 78/1000 | Loss: 0.00002386
Iteration 79/1000 | Loss: 0.00002385
Iteration 80/1000 | Loss: 0.00002385
Iteration 81/1000 | Loss: 0.00002385
Iteration 82/1000 | Loss: 0.00002385
Iteration 83/1000 | Loss: 0.00002385
Iteration 84/1000 | Loss: 0.00002385
Iteration 85/1000 | Loss: 0.00002384
Iteration 86/1000 | Loss: 0.00002384
Iteration 87/1000 | Loss: 0.00002384
Iteration 88/1000 | Loss: 0.00002383
Iteration 89/1000 | Loss: 0.00002383
Iteration 90/1000 | Loss: 0.00002383
Iteration 91/1000 | Loss: 0.00002382
Iteration 92/1000 | Loss: 0.00002382
Iteration 93/1000 | Loss: 0.00002382
Iteration 94/1000 | Loss: 0.00002382
Iteration 95/1000 | Loss: 0.00002382
Iteration 96/1000 | Loss: 0.00002381
Iteration 97/1000 | Loss: 0.00002381
Iteration 98/1000 | Loss: 0.00002381
Iteration 99/1000 | Loss: 0.00002381
Iteration 100/1000 | Loss: 0.00002381
Iteration 101/1000 | Loss: 0.00002381
Iteration 102/1000 | Loss: 0.00002381
Iteration 103/1000 | Loss: 0.00002380
Iteration 104/1000 | Loss: 0.00002380
Iteration 105/1000 | Loss: 0.00002380
Iteration 106/1000 | Loss: 0.00002380
Iteration 107/1000 | Loss: 0.00002379
Iteration 108/1000 | Loss: 0.00002379
Iteration 109/1000 | Loss: 0.00002379
Iteration 110/1000 | Loss: 0.00002378
Iteration 111/1000 | Loss: 0.00002378
Iteration 112/1000 | Loss: 0.00002378
Iteration 113/1000 | Loss: 0.00002377
Iteration 114/1000 | Loss: 0.00002377
Iteration 115/1000 | Loss: 0.00002377
Iteration 116/1000 | Loss: 0.00002377
Iteration 117/1000 | Loss: 0.00002377
Iteration 118/1000 | Loss: 0.00002377
Iteration 119/1000 | Loss: 0.00002377
Iteration 120/1000 | Loss: 0.00002377
Iteration 121/1000 | Loss: 0.00002377
Iteration 122/1000 | Loss: 0.00002377
Iteration 123/1000 | Loss: 0.00002376
Iteration 124/1000 | Loss: 0.00002376
Iteration 125/1000 | Loss: 0.00002376
Iteration 126/1000 | Loss: 0.00002376
Iteration 127/1000 | Loss: 0.00002376
Iteration 128/1000 | Loss: 0.00002376
Iteration 129/1000 | Loss: 0.00002375
Iteration 130/1000 | Loss: 0.00002375
Iteration 131/1000 | Loss: 0.00002375
Iteration 132/1000 | Loss: 0.00002375
Iteration 133/1000 | Loss: 0.00002375
Iteration 134/1000 | Loss: 0.00002375
Iteration 135/1000 | Loss: 0.00002375
Iteration 136/1000 | Loss: 0.00002375
Iteration 137/1000 | Loss: 0.00002375
Iteration 138/1000 | Loss: 0.00002375
Iteration 139/1000 | Loss: 0.00002375
Iteration 140/1000 | Loss: 0.00002375
Iteration 141/1000 | Loss: 0.00002375
Iteration 142/1000 | Loss: 0.00002375
Iteration 143/1000 | Loss: 0.00002375
Iteration 144/1000 | Loss: 0.00002375
Iteration 145/1000 | Loss: 0.00002375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.3748469175188802e-05, 2.3748469175188802e-05, 2.3748469175188802e-05, 2.3748469175188802e-05, 2.3748469175188802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3748469175188802e-05

Optimization complete. Final v2v error: 4.20986270904541 mm

Highest mean error: 4.508947372436523 mm for frame 129

Lowest mean error: 3.602025270462036 mm for frame 38

Saving results

Total time: 88.42861819267273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885466
Iteration 2/25 | Loss: 0.00168103
Iteration 3/25 | Loss: 0.00132392
Iteration 4/25 | Loss: 0.00131356
Iteration 5/25 | Loss: 0.00126399
Iteration 6/25 | Loss: 0.00126573
Iteration 7/25 | Loss: 0.00124781
Iteration 8/25 | Loss: 0.00121686
Iteration 9/25 | Loss: 0.00122597
Iteration 10/25 | Loss: 0.00121823
Iteration 11/25 | Loss: 0.00119625
Iteration 12/25 | Loss: 0.00119648
Iteration 13/25 | Loss: 0.00119579
Iteration 14/25 | Loss: 0.00118523
Iteration 15/25 | Loss: 0.00118956
Iteration 16/25 | Loss: 0.00119564
Iteration 17/25 | Loss: 0.00119726
Iteration 18/25 | Loss: 0.00117925
Iteration 19/25 | Loss: 0.00117736
Iteration 20/25 | Loss: 0.00118024
Iteration 21/25 | Loss: 0.00117569
Iteration 22/25 | Loss: 0.00117498
Iteration 23/25 | Loss: 0.00117480
Iteration 24/25 | Loss: 0.00117456
Iteration 25/25 | Loss: 0.00117478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.70030737
Iteration 2/25 | Loss: 0.00095547
Iteration 3/25 | Loss: 0.00095542
Iteration 4/25 | Loss: 0.00095542
Iteration 5/25 | Loss: 0.00095542
Iteration 6/25 | Loss: 0.00095542
Iteration 7/25 | Loss: 0.00095542
Iteration 8/25 | Loss: 0.00095542
Iteration 9/25 | Loss: 0.00095542
Iteration 10/25 | Loss: 0.00095542
Iteration 11/25 | Loss: 0.00095542
Iteration 12/25 | Loss: 0.00095542
Iteration 13/25 | Loss: 0.00095542
Iteration 14/25 | Loss: 0.00095542
Iteration 15/25 | Loss: 0.00095542
Iteration 16/25 | Loss: 0.00095542
Iteration 17/25 | Loss: 0.00095542
Iteration 18/25 | Loss: 0.00095542
Iteration 19/25 | Loss: 0.00095542
Iteration 20/25 | Loss: 0.00095542
Iteration 21/25 | Loss: 0.00095542
Iteration 22/25 | Loss: 0.00095542
Iteration 23/25 | Loss: 0.00095542
Iteration 24/25 | Loss: 0.00095542
Iteration 25/25 | Loss: 0.00095542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095542
Iteration 2/1000 | Loss: 0.00012878
Iteration 3/1000 | Loss: 0.00009641
Iteration 4/1000 | Loss: 0.00006018
Iteration 5/1000 | Loss: 0.00007149
Iteration 6/1000 | Loss: 0.00008495
Iteration 7/1000 | Loss: 0.00004916
Iteration 8/1000 | Loss: 0.00007789
Iteration 9/1000 | Loss: 0.00006599
Iteration 10/1000 | Loss: 0.00008821
Iteration 11/1000 | Loss: 0.00007394
Iteration 12/1000 | Loss: 0.00007038
Iteration 13/1000 | Loss: 0.00008942
Iteration 14/1000 | Loss: 0.00005744
Iteration 15/1000 | Loss: 0.00005368
Iteration 16/1000 | Loss: 0.00006122
Iteration 17/1000 | Loss: 0.00006314
Iteration 18/1000 | Loss: 0.00005429
Iteration 19/1000 | Loss: 0.00006185
Iteration 20/1000 | Loss: 0.00005127
Iteration 21/1000 | Loss: 0.00004108
Iteration 22/1000 | Loss: 0.00006131
Iteration 23/1000 | Loss: 0.00006024
Iteration 24/1000 | Loss: 0.00005916
Iteration 25/1000 | Loss: 0.00016650
Iteration 26/1000 | Loss: 0.00025585
Iteration 27/1000 | Loss: 0.00005026
Iteration 28/1000 | Loss: 0.00005544
Iteration 29/1000 | Loss: 0.00005817
Iteration 30/1000 | Loss: 0.00034835
Iteration 31/1000 | Loss: 0.00013431
Iteration 32/1000 | Loss: 0.00005906
Iteration 33/1000 | Loss: 0.00026937
Iteration 34/1000 | Loss: 0.00006005
Iteration 35/1000 | Loss: 0.00004788
Iteration 36/1000 | Loss: 0.00005383
Iteration 37/1000 | Loss: 0.00006108
Iteration 38/1000 | Loss: 0.00005194
Iteration 39/1000 | Loss: 0.00019305
Iteration 40/1000 | Loss: 0.00004121
Iteration 41/1000 | Loss: 0.00005341
Iteration 42/1000 | Loss: 0.00003616
Iteration 43/1000 | Loss: 0.00003340
Iteration 44/1000 | Loss: 0.00015314
Iteration 45/1000 | Loss: 0.00003663
Iteration 46/1000 | Loss: 0.00004385
Iteration 47/1000 | Loss: 0.00003436
Iteration 48/1000 | Loss: 0.00003270
Iteration 49/1000 | Loss: 0.00004140
Iteration 50/1000 | Loss: 0.00003258
Iteration 51/1000 | Loss: 0.00003192
Iteration 52/1000 | Loss: 0.00003372
Iteration 53/1000 | Loss: 0.00003864
Iteration 54/1000 | Loss: 0.00004555
Iteration 55/1000 | Loss: 0.00004133
Iteration 56/1000 | Loss: 0.00004304
Iteration 57/1000 | Loss: 0.00004260
Iteration 58/1000 | Loss: 0.00004451
Iteration 59/1000 | Loss: 0.00028039
Iteration 60/1000 | Loss: 0.00005787
Iteration 61/1000 | Loss: 0.00008179
Iteration 62/1000 | Loss: 0.00003582
Iteration 63/1000 | Loss: 0.00003723
Iteration 64/1000 | Loss: 0.00004107
Iteration 65/1000 | Loss: 0.00004356
Iteration 66/1000 | Loss: 0.00003633
Iteration 67/1000 | Loss: 0.00004050
Iteration 68/1000 | Loss: 0.00003437
Iteration 69/1000 | Loss: 0.00007378
Iteration 70/1000 | Loss: 0.00004471
Iteration 71/1000 | Loss: 0.00004729
Iteration 72/1000 | Loss: 0.00007617
Iteration 73/1000 | Loss: 0.00003211
Iteration 74/1000 | Loss: 0.00007415
Iteration 75/1000 | Loss: 0.00004650
Iteration 76/1000 | Loss: 0.00003939
Iteration 77/1000 | Loss: 0.00004608
Iteration 78/1000 | Loss: 0.00003522
Iteration 79/1000 | Loss: 0.00009068
Iteration 80/1000 | Loss: 0.00004508
Iteration 81/1000 | Loss: 0.00004152
Iteration 82/1000 | Loss: 0.00004418
Iteration 83/1000 | Loss: 0.00004207
Iteration 84/1000 | Loss: 0.00003929
Iteration 85/1000 | Loss: 0.00004298
Iteration 86/1000 | Loss: 0.00004398
Iteration 87/1000 | Loss: 0.00004070
Iteration 88/1000 | Loss: 0.00004066
Iteration 89/1000 | Loss: 0.00019815
Iteration 90/1000 | Loss: 0.00004056
Iteration 91/1000 | Loss: 0.00003547
Iteration 92/1000 | Loss: 0.00003281
Iteration 93/1000 | Loss: 0.00003110
Iteration 94/1000 | Loss: 0.00003057
Iteration 95/1000 | Loss: 0.00003024
Iteration 96/1000 | Loss: 0.00003023
Iteration 97/1000 | Loss: 0.00003427
Iteration 98/1000 | Loss: 0.00002996
Iteration 99/1000 | Loss: 0.00022605
Iteration 100/1000 | Loss: 0.00003807
Iteration 101/1000 | Loss: 0.00007324
Iteration 102/1000 | Loss: 0.00004646
Iteration 103/1000 | Loss: 0.00003316
Iteration 104/1000 | Loss: 0.00002957
Iteration 105/1000 | Loss: 0.00002930
Iteration 106/1000 | Loss: 0.00002915
Iteration 107/1000 | Loss: 0.00002897
Iteration 108/1000 | Loss: 0.00002880
Iteration 109/1000 | Loss: 0.00002863
Iteration 110/1000 | Loss: 0.00002856
Iteration 111/1000 | Loss: 0.00002834
Iteration 112/1000 | Loss: 0.00027629
Iteration 113/1000 | Loss: 0.00004111
Iteration 114/1000 | Loss: 0.00002837
Iteration 115/1000 | Loss: 0.00014453
Iteration 116/1000 | Loss: 0.00002844
Iteration 117/1000 | Loss: 0.00002807
Iteration 118/1000 | Loss: 0.00002795
Iteration 119/1000 | Loss: 0.00002795
Iteration 120/1000 | Loss: 0.00002793
Iteration 121/1000 | Loss: 0.00002792
Iteration 122/1000 | Loss: 0.00002792
Iteration 123/1000 | Loss: 0.00002792
Iteration 124/1000 | Loss: 0.00002792
Iteration 125/1000 | Loss: 0.00002791
Iteration 126/1000 | Loss: 0.00002791
Iteration 127/1000 | Loss: 0.00002790
Iteration 128/1000 | Loss: 0.00002790
Iteration 129/1000 | Loss: 0.00002790
Iteration 130/1000 | Loss: 0.00002790
Iteration 131/1000 | Loss: 0.00002790
Iteration 132/1000 | Loss: 0.00002790
Iteration 133/1000 | Loss: 0.00002789
Iteration 134/1000 | Loss: 0.00002789
Iteration 135/1000 | Loss: 0.00002789
Iteration 136/1000 | Loss: 0.00002789
Iteration 137/1000 | Loss: 0.00002789
Iteration 138/1000 | Loss: 0.00002789
Iteration 139/1000 | Loss: 0.00002789
Iteration 140/1000 | Loss: 0.00002789
Iteration 141/1000 | Loss: 0.00002789
Iteration 142/1000 | Loss: 0.00002789
Iteration 143/1000 | Loss: 0.00002789
Iteration 144/1000 | Loss: 0.00002789
Iteration 145/1000 | Loss: 0.00002788
Iteration 146/1000 | Loss: 0.00002788
Iteration 147/1000 | Loss: 0.00002788
Iteration 148/1000 | Loss: 0.00002788
Iteration 149/1000 | Loss: 0.00002788
Iteration 150/1000 | Loss: 0.00002788
Iteration 151/1000 | Loss: 0.00002787
Iteration 152/1000 | Loss: 0.00002787
Iteration 153/1000 | Loss: 0.00002787
Iteration 154/1000 | Loss: 0.00002787
Iteration 155/1000 | Loss: 0.00002787
Iteration 156/1000 | Loss: 0.00002787
Iteration 157/1000 | Loss: 0.00002786
Iteration 158/1000 | Loss: 0.00002785
Iteration 159/1000 | Loss: 0.00002785
Iteration 160/1000 | Loss: 0.00002785
Iteration 161/1000 | Loss: 0.00002784
Iteration 162/1000 | Loss: 0.00002783
Iteration 163/1000 | Loss: 0.00002783
Iteration 164/1000 | Loss: 0.00002783
Iteration 165/1000 | Loss: 0.00002783
Iteration 166/1000 | Loss: 0.00002782
Iteration 167/1000 | Loss: 0.00002782
Iteration 168/1000 | Loss: 0.00002782
Iteration 169/1000 | Loss: 0.00002782
Iteration 170/1000 | Loss: 0.00002781
Iteration 171/1000 | Loss: 0.00002781
Iteration 172/1000 | Loss: 0.00002781
Iteration 173/1000 | Loss: 0.00002780
Iteration 174/1000 | Loss: 0.00002780
Iteration 175/1000 | Loss: 0.00002780
Iteration 176/1000 | Loss: 0.00002780
Iteration 177/1000 | Loss: 0.00002780
Iteration 178/1000 | Loss: 0.00002780
Iteration 179/1000 | Loss: 0.00002780
Iteration 180/1000 | Loss: 0.00002779
Iteration 181/1000 | Loss: 0.00002779
Iteration 182/1000 | Loss: 0.00002779
Iteration 183/1000 | Loss: 0.00002779
Iteration 184/1000 | Loss: 0.00002779
Iteration 185/1000 | Loss: 0.00002779
Iteration 186/1000 | Loss: 0.00002779
Iteration 187/1000 | Loss: 0.00002779
Iteration 188/1000 | Loss: 0.00002779
Iteration 189/1000 | Loss: 0.00002779
Iteration 190/1000 | Loss: 0.00002779
Iteration 191/1000 | Loss: 0.00002779
Iteration 192/1000 | Loss: 0.00002779
Iteration 193/1000 | Loss: 0.00002779
Iteration 194/1000 | Loss: 0.00002779
Iteration 195/1000 | Loss: 0.00002779
Iteration 196/1000 | Loss: 0.00002779
Iteration 197/1000 | Loss: 0.00002779
Iteration 198/1000 | Loss: 0.00002779
Iteration 199/1000 | Loss: 0.00002779
Iteration 200/1000 | Loss: 0.00002779
Iteration 201/1000 | Loss: 0.00002779
Iteration 202/1000 | Loss: 0.00002779
Iteration 203/1000 | Loss: 0.00002779
Iteration 204/1000 | Loss: 0.00002779
Iteration 205/1000 | Loss: 0.00002779
Iteration 206/1000 | Loss: 0.00002779
Iteration 207/1000 | Loss: 0.00002779
Iteration 208/1000 | Loss: 0.00002779
Iteration 209/1000 | Loss: 0.00002779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.7791669708676636e-05, 2.7791669708676636e-05, 2.7791669708676636e-05, 2.7791669708676636e-05, 2.7791669708676636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7791669708676636e-05

Optimization complete. Final v2v error: 4.287600040435791 mm

Highest mean error: 6.899342060089111 mm for frame 41

Lowest mean error: 2.924398422241211 mm for frame 137

Saving results

Total time: 245.64739894866943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481067
Iteration 2/25 | Loss: 0.00118348
Iteration 3/25 | Loss: 0.00110952
Iteration 4/25 | Loss: 0.00109892
Iteration 5/25 | Loss: 0.00109610
Iteration 6/25 | Loss: 0.00109546
Iteration 7/25 | Loss: 0.00109546
Iteration 8/25 | Loss: 0.00109546
Iteration 9/25 | Loss: 0.00109546
Iteration 10/25 | Loss: 0.00109546
Iteration 11/25 | Loss: 0.00109546
Iteration 12/25 | Loss: 0.00109546
Iteration 13/25 | Loss: 0.00109546
Iteration 14/25 | Loss: 0.00109546
Iteration 15/25 | Loss: 0.00109546
Iteration 16/25 | Loss: 0.00109546
Iteration 17/25 | Loss: 0.00109546
Iteration 18/25 | Loss: 0.00109546
Iteration 19/25 | Loss: 0.00109546
Iteration 20/25 | Loss: 0.00109546
Iteration 21/25 | Loss: 0.00109546
Iteration 22/25 | Loss: 0.00109546
Iteration 23/25 | Loss: 0.00109546
Iteration 24/25 | Loss: 0.00109546
Iteration 25/25 | Loss: 0.00109546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30741572
Iteration 2/25 | Loss: 0.00062768
Iteration 3/25 | Loss: 0.00062765
Iteration 4/25 | Loss: 0.00062765
Iteration 5/25 | Loss: 0.00062765
Iteration 6/25 | Loss: 0.00062765
Iteration 7/25 | Loss: 0.00062765
Iteration 8/25 | Loss: 0.00062765
Iteration 9/25 | Loss: 0.00062765
Iteration 10/25 | Loss: 0.00062765
Iteration 11/25 | Loss: 0.00062765
Iteration 12/25 | Loss: 0.00062765
Iteration 13/25 | Loss: 0.00062765
Iteration 14/25 | Loss: 0.00062765
Iteration 15/25 | Loss: 0.00062765
Iteration 16/25 | Loss: 0.00062765
Iteration 17/25 | Loss: 0.00062765
Iteration 18/25 | Loss: 0.00062765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006276484928093851, 0.0006276484928093851, 0.0006276484928093851, 0.0006276484928093851, 0.0006276484928093851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006276484928093851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062765
Iteration 2/1000 | Loss: 0.00003041
Iteration 3/1000 | Loss: 0.00001898
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001354
Iteration 6/1000 | Loss: 0.00001278
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001205
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001181
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001163
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001162
Iteration 17/1000 | Loss: 0.00001161
Iteration 18/1000 | Loss: 0.00001155
Iteration 19/1000 | Loss: 0.00001155
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001146
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001134
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001132
Iteration 39/1000 | Loss: 0.00001130
Iteration 40/1000 | Loss: 0.00001129
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001122
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001119
Iteration 60/1000 | Loss: 0.00001118
Iteration 61/1000 | Loss: 0.00001118
Iteration 62/1000 | Loss: 0.00001118
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001111
Iteration 91/1000 | Loss: 0.00001111
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001110
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001106
Iteration 117/1000 | Loss: 0.00001106
Iteration 118/1000 | Loss: 0.00001106
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001103
Iteration 128/1000 | Loss: 0.00001103
Iteration 129/1000 | Loss: 0.00001102
Iteration 130/1000 | Loss: 0.00001102
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001102
Iteration 135/1000 | Loss: 0.00001102
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001101
Iteration 139/1000 | Loss: 0.00001101
Iteration 140/1000 | Loss: 0.00001101
Iteration 141/1000 | Loss: 0.00001101
Iteration 142/1000 | Loss: 0.00001101
Iteration 143/1000 | Loss: 0.00001101
Iteration 144/1000 | Loss: 0.00001101
Iteration 145/1000 | Loss: 0.00001101
Iteration 146/1000 | Loss: 0.00001101
Iteration 147/1000 | Loss: 0.00001101
Iteration 148/1000 | Loss: 0.00001100
Iteration 149/1000 | Loss: 0.00001099
Iteration 150/1000 | Loss: 0.00001099
Iteration 151/1000 | Loss: 0.00001099
Iteration 152/1000 | Loss: 0.00001099
Iteration 153/1000 | Loss: 0.00001099
Iteration 154/1000 | Loss: 0.00001099
Iteration 155/1000 | Loss: 0.00001099
Iteration 156/1000 | Loss: 0.00001099
Iteration 157/1000 | Loss: 0.00001099
Iteration 158/1000 | Loss: 0.00001099
Iteration 159/1000 | Loss: 0.00001098
Iteration 160/1000 | Loss: 0.00001098
Iteration 161/1000 | Loss: 0.00001098
Iteration 162/1000 | Loss: 0.00001098
Iteration 163/1000 | Loss: 0.00001098
Iteration 164/1000 | Loss: 0.00001097
Iteration 165/1000 | Loss: 0.00001097
Iteration 166/1000 | Loss: 0.00001097
Iteration 167/1000 | Loss: 0.00001097
Iteration 168/1000 | Loss: 0.00001097
Iteration 169/1000 | Loss: 0.00001096
Iteration 170/1000 | Loss: 0.00001096
Iteration 171/1000 | Loss: 0.00001096
Iteration 172/1000 | Loss: 0.00001096
Iteration 173/1000 | Loss: 0.00001096
Iteration 174/1000 | Loss: 0.00001096
Iteration 175/1000 | Loss: 0.00001096
Iteration 176/1000 | Loss: 0.00001096
Iteration 177/1000 | Loss: 0.00001096
Iteration 178/1000 | Loss: 0.00001096
Iteration 179/1000 | Loss: 0.00001096
Iteration 180/1000 | Loss: 0.00001096
Iteration 181/1000 | Loss: 0.00001096
Iteration 182/1000 | Loss: 0.00001095
Iteration 183/1000 | Loss: 0.00001095
Iteration 184/1000 | Loss: 0.00001095
Iteration 185/1000 | Loss: 0.00001095
Iteration 186/1000 | Loss: 0.00001095
Iteration 187/1000 | Loss: 0.00001095
Iteration 188/1000 | Loss: 0.00001095
Iteration 189/1000 | Loss: 0.00001095
Iteration 190/1000 | Loss: 0.00001095
Iteration 191/1000 | Loss: 0.00001095
Iteration 192/1000 | Loss: 0.00001095
Iteration 193/1000 | Loss: 0.00001095
Iteration 194/1000 | Loss: 0.00001095
Iteration 195/1000 | Loss: 0.00001095
Iteration 196/1000 | Loss: 0.00001095
Iteration 197/1000 | Loss: 0.00001095
Iteration 198/1000 | Loss: 0.00001095
Iteration 199/1000 | Loss: 0.00001094
Iteration 200/1000 | Loss: 0.00001094
Iteration 201/1000 | Loss: 0.00001094
Iteration 202/1000 | Loss: 0.00001094
Iteration 203/1000 | Loss: 0.00001094
Iteration 204/1000 | Loss: 0.00001094
Iteration 205/1000 | Loss: 0.00001093
Iteration 206/1000 | Loss: 0.00001093
Iteration 207/1000 | Loss: 0.00001093
Iteration 208/1000 | Loss: 0.00001093
Iteration 209/1000 | Loss: 0.00001093
Iteration 210/1000 | Loss: 0.00001093
Iteration 211/1000 | Loss: 0.00001093
Iteration 212/1000 | Loss: 0.00001093
Iteration 213/1000 | Loss: 0.00001093
Iteration 214/1000 | Loss: 0.00001093
Iteration 215/1000 | Loss: 0.00001093
Iteration 216/1000 | Loss: 0.00001092
Iteration 217/1000 | Loss: 0.00001092
Iteration 218/1000 | Loss: 0.00001092
Iteration 219/1000 | Loss: 0.00001092
Iteration 220/1000 | Loss: 0.00001092
Iteration 221/1000 | Loss: 0.00001092
Iteration 222/1000 | Loss: 0.00001092
Iteration 223/1000 | Loss: 0.00001092
Iteration 224/1000 | Loss: 0.00001092
Iteration 225/1000 | Loss: 0.00001092
Iteration 226/1000 | Loss: 0.00001092
Iteration 227/1000 | Loss: 0.00001092
Iteration 228/1000 | Loss: 0.00001092
Iteration 229/1000 | Loss: 0.00001092
Iteration 230/1000 | Loss: 0.00001092
Iteration 231/1000 | Loss: 0.00001092
Iteration 232/1000 | Loss: 0.00001092
Iteration 233/1000 | Loss: 0.00001092
Iteration 234/1000 | Loss: 0.00001092
Iteration 235/1000 | Loss: 0.00001092
Iteration 236/1000 | Loss: 0.00001092
Iteration 237/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.0922846740868408e-05, 1.0922846740868408e-05, 1.0922846740868408e-05, 1.0922846740868408e-05, 1.0922846740868408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0922846740868408e-05

Optimization complete. Final v2v error: 2.768249988555908 mm

Highest mean error: 3.174966335296631 mm for frame 30

Lowest mean error: 2.3907604217529297 mm for frame 129

Saving results

Total time: 42.78094959259033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081857
Iteration 2/25 | Loss: 0.01081856
Iteration 3/25 | Loss: 0.01081856
Iteration 4/25 | Loss: 0.00477573
Iteration 5/25 | Loss: 0.00309340
Iteration 6/25 | Loss: 0.00269894
Iteration 7/25 | Loss: 0.00248276
Iteration 8/25 | Loss: 0.00234436
Iteration 9/25 | Loss: 0.00223585
Iteration 10/25 | Loss: 0.00220957
Iteration 11/25 | Loss: 0.00219094
Iteration 12/25 | Loss: 0.00215251
Iteration 13/25 | Loss: 0.00213603
Iteration 14/25 | Loss: 0.00212927
Iteration 15/25 | Loss: 0.00212901
Iteration 16/25 | Loss: 0.00212091
Iteration 17/25 | Loss: 0.00211749
Iteration 18/25 | Loss: 0.00205987
Iteration 19/25 | Loss: 0.00203483
Iteration 20/25 | Loss: 0.00202780
Iteration 21/25 | Loss: 0.00202818
Iteration 22/25 | Loss: 0.00200118
Iteration 23/25 | Loss: 0.00195656
Iteration 24/25 | Loss: 0.00194724
Iteration 25/25 | Loss: 0.00193789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81444430
Iteration 2/25 | Loss: 0.00734014
Iteration 3/25 | Loss: 0.00719298
Iteration 4/25 | Loss: 0.00719298
Iteration 5/25 | Loss: 0.00719298
Iteration 6/25 | Loss: 0.00719298
Iteration 7/25 | Loss: 0.00719298
Iteration 8/25 | Loss: 0.00719297
Iteration 9/25 | Loss: 0.00719297
Iteration 10/25 | Loss: 0.00719297
Iteration 11/25 | Loss: 0.00719297
Iteration 12/25 | Loss: 0.00719297
Iteration 13/25 | Loss: 0.00719297
Iteration 14/25 | Loss: 0.00719297
Iteration 15/25 | Loss: 0.00719297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00719297444447875, 0.00719297444447875, 0.00719297444447875, 0.00719297444447875, 0.00719297444447875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00719297444447875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00719297
Iteration 2/1000 | Loss: 0.00422277
Iteration 3/1000 | Loss: 0.00098332
Iteration 4/1000 | Loss: 0.00083385
Iteration 5/1000 | Loss: 0.00055861
Iteration 6/1000 | Loss: 0.00040083
Iteration 7/1000 | Loss: 0.00061006
Iteration 8/1000 | Loss: 0.00050270
Iteration 9/1000 | Loss: 0.00071669
Iteration 10/1000 | Loss: 0.00028062
Iteration 11/1000 | Loss: 0.00025490
Iteration 12/1000 | Loss: 0.00023986
Iteration 13/1000 | Loss: 0.00068243
Iteration 14/1000 | Loss: 0.00489926
Iteration 15/1000 | Loss: 0.00398436
Iteration 16/1000 | Loss: 0.00116216
Iteration 17/1000 | Loss: 0.00065604
Iteration 18/1000 | Loss: 0.00078663
Iteration 19/1000 | Loss: 0.00024720
Iteration 20/1000 | Loss: 0.00083458
Iteration 21/1000 | Loss: 0.00029455
Iteration 22/1000 | Loss: 0.00077803
Iteration 23/1000 | Loss: 0.00035066
Iteration 24/1000 | Loss: 0.00230878
Iteration 25/1000 | Loss: 0.00160636
Iteration 26/1000 | Loss: 0.00187954
Iteration 27/1000 | Loss: 0.00305347
Iteration 28/1000 | Loss: 0.00156335
Iteration 29/1000 | Loss: 0.00100308
Iteration 30/1000 | Loss: 0.00067203
Iteration 31/1000 | Loss: 0.00028662
Iteration 32/1000 | Loss: 0.00017693
Iteration 33/1000 | Loss: 0.00036734
Iteration 34/1000 | Loss: 0.00351108
Iteration 35/1000 | Loss: 0.00086427
Iteration 36/1000 | Loss: 0.00106020
Iteration 37/1000 | Loss: 0.00064061
Iteration 38/1000 | Loss: 0.00234251
Iteration 39/1000 | Loss: 0.00058624
Iteration 40/1000 | Loss: 0.00017024
Iteration 41/1000 | Loss: 0.00059750
Iteration 42/1000 | Loss: 0.00163032
Iteration 43/1000 | Loss: 0.00266078
Iteration 44/1000 | Loss: 0.00093429
Iteration 45/1000 | Loss: 0.00156805
Iteration 46/1000 | Loss: 0.00020888
Iteration 47/1000 | Loss: 0.00262699
Iteration 48/1000 | Loss: 0.00144665
Iteration 49/1000 | Loss: 0.00034189
Iteration 50/1000 | Loss: 0.00180329
Iteration 51/1000 | Loss: 0.00154707
Iteration 52/1000 | Loss: 0.00171013
Iteration 53/1000 | Loss: 0.00165083
Iteration 54/1000 | Loss: 0.00142186
Iteration 55/1000 | Loss: 0.00215244
Iteration 56/1000 | Loss: 0.00156199
Iteration 57/1000 | Loss: 0.00050033
Iteration 58/1000 | Loss: 0.00082499
Iteration 59/1000 | Loss: 0.00017430
Iteration 60/1000 | Loss: 0.00072342
Iteration 61/1000 | Loss: 0.00037248
Iteration 62/1000 | Loss: 0.00015994
Iteration 63/1000 | Loss: 0.00014092
Iteration 64/1000 | Loss: 0.00067420
Iteration 65/1000 | Loss: 0.00030910
Iteration 66/1000 | Loss: 0.00109270
Iteration 67/1000 | Loss: 0.00044321
Iteration 68/1000 | Loss: 0.00021279
Iteration 69/1000 | Loss: 0.00013134
Iteration 70/1000 | Loss: 0.00029662
Iteration 71/1000 | Loss: 0.00023600
Iteration 72/1000 | Loss: 0.00083660
Iteration 73/1000 | Loss: 0.00023387
Iteration 74/1000 | Loss: 0.00013979
Iteration 75/1000 | Loss: 0.00012720
Iteration 76/1000 | Loss: 0.00016535
Iteration 77/1000 | Loss: 0.00060938
Iteration 78/1000 | Loss: 0.00060388
Iteration 79/1000 | Loss: 0.00017616
Iteration 80/1000 | Loss: 0.00012378
Iteration 81/1000 | Loss: 0.00020315
Iteration 82/1000 | Loss: 0.00011792
Iteration 83/1000 | Loss: 0.00011341
Iteration 84/1000 | Loss: 0.00011903
Iteration 85/1000 | Loss: 0.00059366
Iteration 86/1000 | Loss: 0.00013017
Iteration 87/1000 | Loss: 0.00056660
Iteration 88/1000 | Loss: 0.00012059
Iteration 89/1000 | Loss: 0.00011164
Iteration 90/1000 | Loss: 0.00010602
Iteration 91/1000 | Loss: 0.00010425
Iteration 92/1000 | Loss: 0.00010237
Iteration 93/1000 | Loss: 0.00010122
Iteration 94/1000 | Loss: 0.00010020
Iteration 95/1000 | Loss: 0.00009933
Iteration 96/1000 | Loss: 0.00017821
Iteration 97/1000 | Loss: 0.00111554
Iteration 98/1000 | Loss: 0.00067490
Iteration 99/1000 | Loss: 0.00021474
Iteration 100/1000 | Loss: 0.00043912
Iteration 101/1000 | Loss: 0.00011896
Iteration 102/1000 | Loss: 0.00010887
Iteration 103/1000 | Loss: 0.00010128
Iteration 104/1000 | Loss: 0.00009787
Iteration 105/1000 | Loss: 0.00009569
Iteration 106/1000 | Loss: 0.00050713
Iteration 107/1000 | Loss: 0.00010745
Iteration 108/1000 | Loss: 0.00009900
Iteration 109/1000 | Loss: 0.00071592
Iteration 110/1000 | Loss: 0.00011648
Iteration 111/1000 | Loss: 0.00009772
Iteration 112/1000 | Loss: 0.00009309
Iteration 113/1000 | Loss: 0.00009450
Iteration 114/1000 | Loss: 0.00009254
Iteration 115/1000 | Loss: 0.00009122
Iteration 116/1000 | Loss: 0.00008722
Iteration 117/1000 | Loss: 0.00008597
Iteration 118/1000 | Loss: 0.00008537
Iteration 119/1000 | Loss: 0.00008501
Iteration 120/1000 | Loss: 0.00008455
Iteration 121/1000 | Loss: 0.00081673
Iteration 122/1000 | Loss: 0.00052983
Iteration 123/1000 | Loss: 0.00009224
Iteration 124/1000 | Loss: 0.00043902
Iteration 125/1000 | Loss: 0.00176201
Iteration 126/1000 | Loss: 0.00105182
Iteration 127/1000 | Loss: 0.00017692
Iteration 128/1000 | Loss: 0.00009183
Iteration 129/1000 | Loss: 0.00024718
Iteration 130/1000 | Loss: 0.00015291
Iteration 131/1000 | Loss: 0.00013785
Iteration 132/1000 | Loss: 0.00008211
Iteration 133/1000 | Loss: 0.00008107
Iteration 134/1000 | Loss: 0.00008050
Iteration 135/1000 | Loss: 0.00007992
Iteration 136/1000 | Loss: 0.00079720
Iteration 137/1000 | Loss: 0.00043680
Iteration 138/1000 | Loss: 0.00014347
Iteration 139/1000 | Loss: 0.00091252
Iteration 140/1000 | Loss: 0.00057404
Iteration 141/1000 | Loss: 0.00022068
Iteration 142/1000 | Loss: 0.00048629
Iteration 143/1000 | Loss: 0.00008509
Iteration 144/1000 | Loss: 0.00023195
Iteration 145/1000 | Loss: 0.00008546
Iteration 146/1000 | Loss: 0.00011375
Iteration 147/1000 | Loss: 0.00008111
Iteration 148/1000 | Loss: 0.00008017
Iteration 149/1000 | Loss: 0.00007950
Iteration 150/1000 | Loss: 0.00007858
Iteration 151/1000 | Loss: 0.00007778
Iteration 152/1000 | Loss: 0.00025184
Iteration 153/1000 | Loss: 0.00079286
Iteration 154/1000 | Loss: 0.00043758
Iteration 155/1000 | Loss: 0.00008582
Iteration 156/1000 | Loss: 0.00007927
Iteration 157/1000 | Loss: 0.00007597
Iteration 158/1000 | Loss: 0.00007372
Iteration 159/1000 | Loss: 0.00007285
Iteration 160/1000 | Loss: 0.00025944
Iteration 161/1000 | Loss: 0.00007435
Iteration 162/1000 | Loss: 0.00012273
Iteration 163/1000 | Loss: 0.00007205
Iteration 164/1000 | Loss: 0.00007161
Iteration 165/1000 | Loss: 0.00007144
Iteration 166/1000 | Loss: 0.00007126
Iteration 167/1000 | Loss: 0.00007120
Iteration 168/1000 | Loss: 0.00007113
Iteration 169/1000 | Loss: 0.00007101
Iteration 170/1000 | Loss: 0.00007087
Iteration 171/1000 | Loss: 0.00007083
Iteration 172/1000 | Loss: 0.00007083
Iteration 173/1000 | Loss: 0.00007083
Iteration 174/1000 | Loss: 0.00007082
Iteration 175/1000 | Loss: 0.00007082
Iteration 176/1000 | Loss: 0.00007082
Iteration 177/1000 | Loss: 0.00007082
Iteration 178/1000 | Loss: 0.00007082
Iteration 179/1000 | Loss: 0.00007082
Iteration 180/1000 | Loss: 0.00007082
Iteration 181/1000 | Loss: 0.00007082
Iteration 182/1000 | Loss: 0.00007082
Iteration 183/1000 | Loss: 0.00007082
Iteration 184/1000 | Loss: 0.00007082
Iteration 185/1000 | Loss: 0.00007081
Iteration 186/1000 | Loss: 0.00007081
Iteration 187/1000 | Loss: 0.00007081
Iteration 188/1000 | Loss: 0.00007081
Iteration 189/1000 | Loss: 0.00007081
Iteration 190/1000 | Loss: 0.00007081
Iteration 191/1000 | Loss: 0.00007081
Iteration 192/1000 | Loss: 0.00007081
Iteration 193/1000 | Loss: 0.00007081
Iteration 194/1000 | Loss: 0.00007080
Iteration 195/1000 | Loss: 0.00007080
Iteration 196/1000 | Loss: 0.00007080
Iteration 197/1000 | Loss: 0.00007080
Iteration 198/1000 | Loss: 0.00007080
Iteration 199/1000 | Loss: 0.00007080
Iteration 200/1000 | Loss: 0.00007080
Iteration 201/1000 | Loss: 0.00007080
Iteration 202/1000 | Loss: 0.00007079
Iteration 203/1000 | Loss: 0.00007078
Iteration 204/1000 | Loss: 0.00007078
Iteration 205/1000 | Loss: 0.00007078
Iteration 206/1000 | Loss: 0.00007077
Iteration 207/1000 | Loss: 0.00007077
Iteration 208/1000 | Loss: 0.00007077
Iteration 209/1000 | Loss: 0.00007077
Iteration 210/1000 | Loss: 0.00007076
Iteration 211/1000 | Loss: 0.00007076
Iteration 212/1000 | Loss: 0.00007076
Iteration 213/1000 | Loss: 0.00007076
Iteration 214/1000 | Loss: 0.00007076
Iteration 215/1000 | Loss: 0.00007076
Iteration 216/1000 | Loss: 0.00007076
Iteration 217/1000 | Loss: 0.00007075
Iteration 218/1000 | Loss: 0.00007075
Iteration 219/1000 | Loss: 0.00007075
Iteration 220/1000 | Loss: 0.00007075
Iteration 221/1000 | Loss: 0.00007075
Iteration 222/1000 | Loss: 0.00007075
Iteration 223/1000 | Loss: 0.00007074
Iteration 224/1000 | Loss: 0.00007074
Iteration 225/1000 | Loss: 0.00007074
Iteration 226/1000 | Loss: 0.00007074
Iteration 227/1000 | Loss: 0.00007074
Iteration 228/1000 | Loss: 0.00007074
Iteration 229/1000 | Loss: 0.00007074
Iteration 230/1000 | Loss: 0.00007074
Iteration 231/1000 | Loss: 0.00007074
Iteration 232/1000 | Loss: 0.00007074
Iteration 233/1000 | Loss: 0.00007074
Iteration 234/1000 | Loss: 0.00007074
Iteration 235/1000 | Loss: 0.00007074
Iteration 236/1000 | Loss: 0.00007074
Iteration 237/1000 | Loss: 0.00007074
Iteration 238/1000 | Loss: 0.00007074
Iteration 239/1000 | Loss: 0.00007073
Iteration 240/1000 | Loss: 0.00007073
Iteration 241/1000 | Loss: 0.00007073
Iteration 242/1000 | Loss: 0.00007073
Iteration 243/1000 | Loss: 0.00007073
Iteration 244/1000 | Loss: 0.00007073
Iteration 245/1000 | Loss: 0.00007073
Iteration 246/1000 | Loss: 0.00007073
Iteration 247/1000 | Loss: 0.00007073
Iteration 248/1000 | Loss: 0.00007073
Iteration 249/1000 | Loss: 0.00007073
Iteration 250/1000 | Loss: 0.00007073
Iteration 251/1000 | Loss: 0.00007072
Iteration 252/1000 | Loss: 0.00007072
Iteration 253/1000 | Loss: 0.00007072
Iteration 254/1000 | Loss: 0.00007072
Iteration 255/1000 | Loss: 0.00007072
Iteration 256/1000 | Loss: 0.00007072
Iteration 257/1000 | Loss: 0.00007072
Iteration 258/1000 | Loss: 0.00007072
Iteration 259/1000 | Loss: 0.00007072
Iteration 260/1000 | Loss: 0.00007072
Iteration 261/1000 | Loss: 0.00007072
Iteration 262/1000 | Loss: 0.00007072
Iteration 263/1000 | Loss: 0.00007071
Iteration 264/1000 | Loss: 0.00007071
Iteration 265/1000 | Loss: 0.00007071
Iteration 266/1000 | Loss: 0.00007071
Iteration 267/1000 | Loss: 0.00007071
Iteration 268/1000 | Loss: 0.00007071
Iteration 269/1000 | Loss: 0.00007071
Iteration 270/1000 | Loss: 0.00007071
Iteration 271/1000 | Loss: 0.00007071
Iteration 272/1000 | Loss: 0.00007071
Iteration 273/1000 | Loss: 0.00007071
Iteration 274/1000 | Loss: 0.00007070
Iteration 275/1000 | Loss: 0.00007070
Iteration 276/1000 | Loss: 0.00007070
Iteration 277/1000 | Loss: 0.00020866
Iteration 278/1000 | Loss: 0.00023360
Iteration 279/1000 | Loss: 0.00008681
Iteration 280/1000 | Loss: 0.00007065
Iteration 281/1000 | Loss: 0.00006995
Iteration 282/1000 | Loss: 0.00006953
Iteration 283/1000 | Loss: 0.00006918
Iteration 284/1000 | Loss: 0.00006890
Iteration 285/1000 | Loss: 0.00006885
Iteration 286/1000 | Loss: 0.00006884
Iteration 287/1000 | Loss: 0.00006884
Iteration 288/1000 | Loss: 0.00006884
Iteration 289/1000 | Loss: 0.00006883
Iteration 290/1000 | Loss: 0.00006883
Iteration 291/1000 | Loss: 0.00006882
Iteration 292/1000 | Loss: 0.00006882
Iteration 293/1000 | Loss: 0.00006882
Iteration 294/1000 | Loss: 0.00006882
Iteration 295/1000 | Loss: 0.00006882
Iteration 296/1000 | Loss: 0.00006882
Iteration 297/1000 | Loss: 0.00006882
Iteration 298/1000 | Loss: 0.00006881
Iteration 299/1000 | Loss: 0.00006881
Iteration 300/1000 | Loss: 0.00006881
Iteration 301/1000 | Loss: 0.00006881
Iteration 302/1000 | Loss: 0.00006879
Iteration 303/1000 | Loss: 0.00006878
Iteration 304/1000 | Loss: 0.00006877
Iteration 305/1000 | Loss: 0.00006875
Iteration 306/1000 | Loss: 0.00006875
Iteration 307/1000 | Loss: 0.00006875
Iteration 308/1000 | Loss: 0.00006873
Iteration 309/1000 | Loss: 0.00006872
Iteration 310/1000 | Loss: 0.00006872
Iteration 311/1000 | Loss: 0.00006871
Iteration 312/1000 | Loss: 0.00006871
Iteration 313/1000 | Loss: 0.00006871
Iteration 314/1000 | Loss: 0.00006871
Iteration 315/1000 | Loss: 0.00006871
Iteration 316/1000 | Loss: 0.00006871
Iteration 317/1000 | Loss: 0.00006871
Iteration 318/1000 | Loss: 0.00006871
Iteration 319/1000 | Loss: 0.00006871
Iteration 320/1000 | Loss: 0.00006870
Iteration 321/1000 | Loss: 0.00006870
Iteration 322/1000 | Loss: 0.00006870
Iteration 323/1000 | Loss: 0.00006870
Iteration 324/1000 | Loss: 0.00006870
Iteration 325/1000 | Loss: 0.00006870
Iteration 326/1000 | Loss: 0.00006870
Iteration 327/1000 | Loss: 0.00006870
Iteration 328/1000 | Loss: 0.00006869
Iteration 329/1000 | Loss: 0.00006869
Iteration 330/1000 | Loss: 0.00006869
Iteration 331/1000 | Loss: 0.00006868
Iteration 332/1000 | Loss: 0.00006868
Iteration 333/1000 | Loss: 0.00006868
Iteration 334/1000 | Loss: 0.00006867
Iteration 335/1000 | Loss: 0.00006867
Iteration 336/1000 | Loss: 0.00006867
Iteration 337/1000 | Loss: 0.00006866
Iteration 338/1000 | Loss: 0.00006866
Iteration 339/1000 | Loss: 0.00006866
Iteration 340/1000 | Loss: 0.00006866
Iteration 341/1000 | Loss: 0.00006865
Iteration 342/1000 | Loss: 0.00006865
Iteration 343/1000 | Loss: 0.00006865
Iteration 344/1000 | Loss: 0.00006865
Iteration 345/1000 | Loss: 0.00006864
Iteration 346/1000 | Loss: 0.00006864
Iteration 347/1000 | Loss: 0.00006864
Iteration 348/1000 | Loss: 0.00006864
Iteration 349/1000 | Loss: 0.00006863
Iteration 350/1000 | Loss: 0.00006863
Iteration 351/1000 | Loss: 0.00006863
Iteration 352/1000 | Loss: 0.00006863
Iteration 353/1000 | Loss: 0.00006863
Iteration 354/1000 | Loss: 0.00006863
Iteration 355/1000 | Loss: 0.00006863
Iteration 356/1000 | Loss: 0.00006862
Iteration 357/1000 | Loss: 0.00006862
Iteration 358/1000 | Loss: 0.00006862
Iteration 359/1000 | Loss: 0.00006862
Iteration 360/1000 | Loss: 0.00006862
Iteration 361/1000 | Loss: 0.00006862
Iteration 362/1000 | Loss: 0.00006862
Iteration 363/1000 | Loss: 0.00006862
Iteration 364/1000 | Loss: 0.00006862
Iteration 365/1000 | Loss: 0.00006862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [6.862138980068266e-05, 6.862138980068266e-05, 6.862138980068266e-05, 6.862138980068266e-05, 6.862138980068266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.862138980068266e-05

Optimization complete. Final v2v error: 5.059262752532959 mm

Highest mean error: 12.648332595825195 mm for frame 3

Lowest mean error: 3.608685255050659 mm for frame 189

Saving results

Total time: 347.33633732795715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533599
Iteration 2/25 | Loss: 0.00144448
Iteration 3/25 | Loss: 0.00122891
Iteration 4/25 | Loss: 0.00120628
Iteration 5/25 | Loss: 0.00119880
Iteration 6/25 | Loss: 0.00119653
Iteration 7/25 | Loss: 0.00119611
Iteration 8/25 | Loss: 0.00119611
Iteration 9/25 | Loss: 0.00119611
Iteration 10/25 | Loss: 0.00119611
Iteration 11/25 | Loss: 0.00119611
Iteration 12/25 | Loss: 0.00119611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011961099226027727, 0.0011961099226027727, 0.0011961099226027727, 0.0011961099226027727, 0.0011961099226027727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011961099226027727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94135916
Iteration 2/25 | Loss: 0.00082821
Iteration 3/25 | Loss: 0.00082821
Iteration 4/25 | Loss: 0.00082821
Iteration 5/25 | Loss: 0.00082820
Iteration 6/25 | Loss: 0.00082820
Iteration 7/25 | Loss: 0.00082820
Iteration 8/25 | Loss: 0.00082820
Iteration 9/25 | Loss: 0.00082820
Iteration 10/25 | Loss: 0.00082820
Iteration 11/25 | Loss: 0.00082820
Iteration 12/25 | Loss: 0.00082820
Iteration 13/25 | Loss: 0.00082820
Iteration 14/25 | Loss: 0.00082820
Iteration 15/25 | Loss: 0.00082820
Iteration 16/25 | Loss: 0.00082820
Iteration 17/25 | Loss: 0.00082820
Iteration 18/25 | Loss: 0.00082820
Iteration 19/25 | Loss: 0.00082820
Iteration 20/25 | Loss: 0.00082820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008282032795250416, 0.0008282032795250416, 0.0008282032795250416, 0.0008282032795250416, 0.0008282032795250416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008282032795250416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082820
Iteration 2/1000 | Loss: 0.00008289
Iteration 3/1000 | Loss: 0.00004412
Iteration 4/1000 | Loss: 0.00003598
Iteration 5/1000 | Loss: 0.00003353
Iteration 6/1000 | Loss: 0.00003181
Iteration 7/1000 | Loss: 0.00003087
Iteration 8/1000 | Loss: 0.00002985
Iteration 9/1000 | Loss: 0.00002911
Iteration 10/1000 | Loss: 0.00002862
Iteration 11/1000 | Loss: 0.00002824
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002768
Iteration 14/1000 | Loss: 0.00002748
Iteration 15/1000 | Loss: 0.00002733
Iteration 16/1000 | Loss: 0.00002714
Iteration 17/1000 | Loss: 0.00002702
Iteration 18/1000 | Loss: 0.00002696
Iteration 19/1000 | Loss: 0.00002696
Iteration 20/1000 | Loss: 0.00002692
Iteration 21/1000 | Loss: 0.00002689
Iteration 22/1000 | Loss: 0.00002684
Iteration 23/1000 | Loss: 0.00002676
Iteration 24/1000 | Loss: 0.00002672
Iteration 25/1000 | Loss: 0.00002671
Iteration 26/1000 | Loss: 0.00002671
Iteration 27/1000 | Loss: 0.00002668
Iteration 28/1000 | Loss: 0.00002668
Iteration 29/1000 | Loss: 0.00002668
Iteration 30/1000 | Loss: 0.00002668
Iteration 31/1000 | Loss: 0.00002667
Iteration 32/1000 | Loss: 0.00002667
Iteration 33/1000 | Loss: 0.00002667
Iteration 34/1000 | Loss: 0.00002667
Iteration 35/1000 | Loss: 0.00002667
Iteration 36/1000 | Loss: 0.00002666
Iteration 37/1000 | Loss: 0.00002666
Iteration 38/1000 | Loss: 0.00002666
Iteration 39/1000 | Loss: 0.00002666
Iteration 40/1000 | Loss: 0.00002666
Iteration 41/1000 | Loss: 0.00002666
Iteration 42/1000 | Loss: 0.00002665
Iteration 43/1000 | Loss: 0.00002665
Iteration 44/1000 | Loss: 0.00002665
Iteration 45/1000 | Loss: 0.00002665
Iteration 46/1000 | Loss: 0.00002665
Iteration 47/1000 | Loss: 0.00002665
Iteration 48/1000 | Loss: 0.00002664
Iteration 49/1000 | Loss: 0.00002664
Iteration 50/1000 | Loss: 0.00002664
Iteration 51/1000 | Loss: 0.00002663
Iteration 52/1000 | Loss: 0.00002663
Iteration 53/1000 | Loss: 0.00002663
Iteration 54/1000 | Loss: 0.00002663
Iteration 55/1000 | Loss: 0.00002663
Iteration 56/1000 | Loss: 0.00002662
Iteration 57/1000 | Loss: 0.00002662
Iteration 58/1000 | Loss: 0.00002661
Iteration 59/1000 | Loss: 0.00002661
Iteration 60/1000 | Loss: 0.00002661
Iteration 61/1000 | Loss: 0.00002661
Iteration 62/1000 | Loss: 0.00002661
Iteration 63/1000 | Loss: 0.00002661
Iteration 64/1000 | Loss: 0.00002661
Iteration 65/1000 | Loss: 0.00002660
Iteration 66/1000 | Loss: 0.00002660
Iteration 67/1000 | Loss: 0.00002660
Iteration 68/1000 | Loss: 0.00002658
Iteration 69/1000 | Loss: 0.00002658
Iteration 70/1000 | Loss: 0.00002658
Iteration 71/1000 | Loss: 0.00002658
Iteration 72/1000 | Loss: 0.00002658
Iteration 73/1000 | Loss: 0.00002658
Iteration 74/1000 | Loss: 0.00002658
Iteration 75/1000 | Loss: 0.00002658
Iteration 76/1000 | Loss: 0.00002658
Iteration 77/1000 | Loss: 0.00002658
Iteration 78/1000 | Loss: 0.00002658
Iteration 79/1000 | Loss: 0.00002657
Iteration 80/1000 | Loss: 0.00002657
Iteration 81/1000 | Loss: 0.00002657
Iteration 82/1000 | Loss: 0.00002656
Iteration 83/1000 | Loss: 0.00002656
Iteration 84/1000 | Loss: 0.00002655
Iteration 85/1000 | Loss: 0.00002655
Iteration 86/1000 | Loss: 0.00002654
Iteration 87/1000 | Loss: 0.00002654
Iteration 88/1000 | Loss: 0.00002654
Iteration 89/1000 | Loss: 0.00002654
Iteration 90/1000 | Loss: 0.00002653
Iteration 91/1000 | Loss: 0.00002653
Iteration 92/1000 | Loss: 0.00002653
Iteration 93/1000 | Loss: 0.00002653
Iteration 94/1000 | Loss: 0.00002652
Iteration 95/1000 | Loss: 0.00002652
Iteration 96/1000 | Loss: 0.00002651
Iteration 97/1000 | Loss: 0.00002651
Iteration 98/1000 | Loss: 0.00002651
Iteration 99/1000 | Loss: 0.00002651
Iteration 100/1000 | Loss: 0.00002651
Iteration 101/1000 | Loss: 0.00002651
Iteration 102/1000 | Loss: 0.00002651
Iteration 103/1000 | Loss: 0.00002651
Iteration 104/1000 | Loss: 0.00002651
Iteration 105/1000 | Loss: 0.00002650
Iteration 106/1000 | Loss: 0.00002650
Iteration 107/1000 | Loss: 0.00002650
Iteration 108/1000 | Loss: 0.00002650
Iteration 109/1000 | Loss: 0.00002650
Iteration 110/1000 | Loss: 0.00002649
Iteration 111/1000 | Loss: 0.00002649
Iteration 112/1000 | Loss: 0.00002649
Iteration 113/1000 | Loss: 0.00002649
Iteration 114/1000 | Loss: 0.00002649
Iteration 115/1000 | Loss: 0.00002649
Iteration 116/1000 | Loss: 0.00002649
Iteration 117/1000 | Loss: 0.00002649
Iteration 118/1000 | Loss: 0.00002648
Iteration 119/1000 | Loss: 0.00002648
Iteration 120/1000 | Loss: 0.00002648
Iteration 121/1000 | Loss: 0.00002648
Iteration 122/1000 | Loss: 0.00002648
Iteration 123/1000 | Loss: 0.00002648
Iteration 124/1000 | Loss: 0.00002648
Iteration 125/1000 | Loss: 0.00002648
Iteration 126/1000 | Loss: 0.00002647
Iteration 127/1000 | Loss: 0.00002647
Iteration 128/1000 | Loss: 0.00002647
Iteration 129/1000 | Loss: 0.00002647
Iteration 130/1000 | Loss: 0.00002647
Iteration 131/1000 | Loss: 0.00002647
Iteration 132/1000 | Loss: 0.00002646
Iteration 133/1000 | Loss: 0.00002646
Iteration 134/1000 | Loss: 0.00002646
Iteration 135/1000 | Loss: 0.00002646
Iteration 136/1000 | Loss: 0.00002646
Iteration 137/1000 | Loss: 0.00002646
Iteration 138/1000 | Loss: 0.00002646
Iteration 139/1000 | Loss: 0.00002645
Iteration 140/1000 | Loss: 0.00002645
Iteration 141/1000 | Loss: 0.00002645
Iteration 142/1000 | Loss: 0.00002645
Iteration 143/1000 | Loss: 0.00002645
Iteration 144/1000 | Loss: 0.00002645
Iteration 145/1000 | Loss: 0.00002645
Iteration 146/1000 | Loss: 0.00002645
Iteration 147/1000 | Loss: 0.00002645
Iteration 148/1000 | Loss: 0.00002644
Iteration 149/1000 | Loss: 0.00002644
Iteration 150/1000 | Loss: 0.00002644
Iteration 151/1000 | Loss: 0.00002644
Iteration 152/1000 | Loss: 0.00002644
Iteration 153/1000 | Loss: 0.00002644
Iteration 154/1000 | Loss: 0.00002644
Iteration 155/1000 | Loss: 0.00002644
Iteration 156/1000 | Loss: 0.00002644
Iteration 157/1000 | Loss: 0.00002644
Iteration 158/1000 | Loss: 0.00002644
Iteration 159/1000 | Loss: 0.00002644
Iteration 160/1000 | Loss: 0.00002644
Iteration 161/1000 | Loss: 0.00002644
Iteration 162/1000 | Loss: 0.00002644
Iteration 163/1000 | Loss: 0.00002644
Iteration 164/1000 | Loss: 0.00002644
Iteration 165/1000 | Loss: 0.00002644
Iteration 166/1000 | Loss: 0.00002644
Iteration 167/1000 | Loss: 0.00002644
Iteration 168/1000 | Loss: 0.00002644
Iteration 169/1000 | Loss: 0.00002644
Iteration 170/1000 | Loss: 0.00002644
Iteration 171/1000 | Loss: 0.00002644
Iteration 172/1000 | Loss: 0.00002644
Iteration 173/1000 | Loss: 0.00002644
Iteration 174/1000 | Loss: 0.00002644
Iteration 175/1000 | Loss: 0.00002644
Iteration 176/1000 | Loss: 0.00002644
Iteration 177/1000 | Loss: 0.00002644
Iteration 178/1000 | Loss: 0.00002644
Iteration 179/1000 | Loss: 0.00002644
Iteration 180/1000 | Loss: 0.00002644
Iteration 181/1000 | Loss: 0.00002644
Iteration 182/1000 | Loss: 0.00002644
Iteration 183/1000 | Loss: 0.00002644
Iteration 184/1000 | Loss: 0.00002644
Iteration 185/1000 | Loss: 0.00002644
Iteration 186/1000 | Loss: 0.00002644
Iteration 187/1000 | Loss: 0.00002644
Iteration 188/1000 | Loss: 0.00002644
Iteration 189/1000 | Loss: 0.00002644
Iteration 190/1000 | Loss: 0.00002644
Iteration 191/1000 | Loss: 0.00002644
Iteration 192/1000 | Loss: 0.00002644
Iteration 193/1000 | Loss: 0.00002644
Iteration 194/1000 | Loss: 0.00002644
Iteration 195/1000 | Loss: 0.00002644
Iteration 196/1000 | Loss: 0.00002644
Iteration 197/1000 | Loss: 0.00002644
Iteration 198/1000 | Loss: 0.00002644
Iteration 199/1000 | Loss: 0.00002644
Iteration 200/1000 | Loss: 0.00002644
Iteration 201/1000 | Loss: 0.00002644
Iteration 202/1000 | Loss: 0.00002644
Iteration 203/1000 | Loss: 0.00002644
Iteration 204/1000 | Loss: 0.00002644
Iteration 205/1000 | Loss: 0.00002644
Iteration 206/1000 | Loss: 0.00002644
Iteration 207/1000 | Loss: 0.00002644
Iteration 208/1000 | Loss: 0.00002644
Iteration 209/1000 | Loss: 0.00002644
Iteration 210/1000 | Loss: 0.00002644
Iteration 211/1000 | Loss: 0.00002644
Iteration 212/1000 | Loss: 0.00002644
Iteration 213/1000 | Loss: 0.00002644
Iteration 214/1000 | Loss: 0.00002644
Iteration 215/1000 | Loss: 0.00002644
Iteration 216/1000 | Loss: 0.00002644
Iteration 217/1000 | Loss: 0.00002644
Iteration 218/1000 | Loss: 0.00002644
Iteration 219/1000 | Loss: 0.00002644
Iteration 220/1000 | Loss: 0.00002644
Iteration 221/1000 | Loss: 0.00002644
Iteration 222/1000 | Loss: 0.00002644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.6443924070918e-05, 2.6443924070918e-05, 2.6443924070918e-05, 2.6443924070918e-05, 2.6443924070918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6443924070918e-05

Optimization complete. Final v2v error: 4.210294246673584 mm

Highest mean error: 5.089678764343262 mm for frame 63

Lowest mean error: 3.4887566566467285 mm for frame 173

Saving results

Total time: 49.20303273200989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972196
Iteration 2/25 | Loss: 0.00274771
Iteration 3/25 | Loss: 0.00175577
Iteration 4/25 | Loss: 0.00156141
Iteration 5/25 | Loss: 0.00139356
Iteration 6/25 | Loss: 0.00131404
Iteration 7/25 | Loss: 0.00127119
Iteration 8/25 | Loss: 0.00121420
Iteration 9/25 | Loss: 0.00120779
Iteration 10/25 | Loss: 0.00119542
Iteration 11/25 | Loss: 0.00118415
Iteration 12/25 | Loss: 0.00117722
Iteration 13/25 | Loss: 0.00116219
Iteration 14/25 | Loss: 0.00115337
Iteration 15/25 | Loss: 0.00114430
Iteration 16/25 | Loss: 0.00114740
Iteration 17/25 | Loss: 0.00114030
Iteration 18/25 | Loss: 0.00113038
Iteration 19/25 | Loss: 0.00112818
Iteration 20/25 | Loss: 0.00112756
Iteration 21/25 | Loss: 0.00113051
Iteration 22/25 | Loss: 0.00112538
Iteration 23/25 | Loss: 0.00112498
Iteration 24/25 | Loss: 0.00112492
Iteration 25/25 | Loss: 0.00112491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37564588
Iteration 2/25 | Loss: 0.00046760
Iteration 3/25 | Loss: 0.00046760
Iteration 4/25 | Loss: 0.00046760
Iteration 5/25 | Loss: 0.00046760
Iteration 6/25 | Loss: 0.00046760
Iteration 7/25 | Loss: 0.00046760
Iteration 8/25 | Loss: 0.00046760
Iteration 9/25 | Loss: 0.00046760
Iteration 10/25 | Loss: 0.00046760
Iteration 11/25 | Loss: 0.00046760
Iteration 12/25 | Loss: 0.00046760
Iteration 13/25 | Loss: 0.00046760
Iteration 14/25 | Loss: 0.00046760
Iteration 15/25 | Loss: 0.00046760
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004675977979786694, 0.0004675977979786694, 0.0004675977979786694, 0.0004675977979786694, 0.0004675977979786694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004675977979786694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046760
Iteration 2/1000 | Loss: 0.00004521
Iteration 3/1000 | Loss: 0.00005182
Iteration 4/1000 | Loss: 0.00003222
Iteration 5/1000 | Loss: 0.00019450
Iteration 6/1000 | Loss: 0.00018691
Iteration 7/1000 | Loss: 0.00044007
Iteration 8/1000 | Loss: 0.00006652
Iteration 9/1000 | Loss: 0.00003717
Iteration 10/1000 | Loss: 0.00003321
Iteration 11/1000 | Loss: 0.00002661
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00002337
Iteration 14/1000 | Loss: 0.00007692
Iteration 15/1000 | Loss: 0.00003057
Iteration 16/1000 | Loss: 0.00004819
Iteration 17/1000 | Loss: 0.00002122
Iteration 18/1000 | Loss: 0.00002660
Iteration 19/1000 | Loss: 0.00002855
Iteration 20/1000 | Loss: 0.00001997
Iteration 21/1000 | Loss: 0.00004135
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001951
Iteration 24/1000 | Loss: 0.00001926
Iteration 25/1000 | Loss: 0.00002440
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00002070
Iteration 29/1000 | Loss: 0.00001899
Iteration 30/1000 | Loss: 0.00001899
Iteration 31/1000 | Loss: 0.00001897
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001889
Iteration 34/1000 | Loss: 0.00001888
Iteration 35/1000 | Loss: 0.00001888
Iteration 36/1000 | Loss: 0.00001888
Iteration 37/1000 | Loss: 0.00001886
Iteration 38/1000 | Loss: 0.00001886
Iteration 39/1000 | Loss: 0.00001884
Iteration 40/1000 | Loss: 0.00001884
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001879
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001876
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001874
Iteration 52/1000 | Loss: 0.00001874
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00002524
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00002084
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001867
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001867
Iteration 77/1000 | Loss: 0.00001867
Iteration 78/1000 | Loss: 0.00001867
Iteration 79/1000 | Loss: 0.00001867
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001866
Iteration 87/1000 | Loss: 0.00001866
Iteration 88/1000 | Loss: 0.00001866
Iteration 89/1000 | Loss: 0.00001866
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001864
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001864
Iteration 103/1000 | Loss: 0.00001863
Iteration 104/1000 | Loss: 0.00001863
Iteration 105/1000 | Loss: 0.00001863
Iteration 106/1000 | Loss: 0.00001863
Iteration 107/1000 | Loss: 0.00001863
Iteration 108/1000 | Loss: 0.00001863
Iteration 109/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.863457691797521e-05, 1.863457691797521e-05, 1.863457691797521e-05, 1.863457691797521e-05, 1.863457691797521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.863457691797521e-05

Optimization complete. Final v2v error: 3.5813348293304443 mm

Highest mean error: 4.279783725738525 mm for frame 157

Lowest mean error: 3.065594434738159 mm for frame 239

Saving results

Total time: 100.23075246810913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744075
Iteration 2/25 | Loss: 0.00192036
Iteration 3/25 | Loss: 0.00158662
Iteration 4/25 | Loss: 0.00154683
Iteration 5/25 | Loss: 0.00150153
Iteration 6/25 | Loss: 0.00148055
Iteration 7/25 | Loss: 0.00144581
Iteration 8/25 | Loss: 0.00145056
Iteration 9/25 | Loss: 0.00145460
Iteration 10/25 | Loss: 0.00144393
Iteration 11/25 | Loss: 0.00141591
Iteration 12/25 | Loss: 0.00141794
Iteration 13/25 | Loss: 0.00141538
Iteration 14/25 | Loss: 0.00141342
Iteration 15/25 | Loss: 0.00142621
Iteration 16/25 | Loss: 0.00148318
Iteration 17/25 | Loss: 0.00147100
Iteration 18/25 | Loss: 0.00140068
Iteration 19/25 | Loss: 0.00136837
Iteration 20/25 | Loss: 0.00136897
Iteration 21/25 | Loss: 0.00135799
Iteration 22/25 | Loss: 0.00135601
Iteration 23/25 | Loss: 0.00135463
Iteration 24/25 | Loss: 0.00135434
Iteration 25/25 | Loss: 0.00135422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71755898
Iteration 2/25 | Loss: 0.00148741
Iteration 3/25 | Loss: 0.00148718
Iteration 4/25 | Loss: 0.00148717
Iteration 5/25 | Loss: 0.00148717
Iteration 6/25 | Loss: 0.00148717
Iteration 7/25 | Loss: 0.00148717
Iteration 8/25 | Loss: 0.00148717
Iteration 9/25 | Loss: 0.00148717
Iteration 10/25 | Loss: 0.00148717
Iteration 11/25 | Loss: 0.00148717
Iteration 12/25 | Loss: 0.00148717
Iteration 13/25 | Loss: 0.00148717
Iteration 14/25 | Loss: 0.00148717
Iteration 15/25 | Loss: 0.00148717
Iteration 16/25 | Loss: 0.00148717
Iteration 17/25 | Loss: 0.00148717
Iteration 18/25 | Loss: 0.00148717
Iteration 19/25 | Loss: 0.00148717
Iteration 20/25 | Loss: 0.00148717
Iteration 21/25 | Loss: 0.00148717
Iteration 22/25 | Loss: 0.00148717
Iteration 23/25 | Loss: 0.00148717
Iteration 24/25 | Loss: 0.00148717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014871683670207858, 0.0014871683670207858, 0.0014871683670207858, 0.0014871683670207858, 0.0014871683670207858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014871683670207858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148717
Iteration 2/1000 | Loss: 0.00329220
Iteration 3/1000 | Loss: 0.00015495
Iteration 4/1000 | Loss: 0.00015655
Iteration 5/1000 | Loss: 0.00035725
Iteration 6/1000 | Loss: 0.00011565
Iteration 7/1000 | Loss: 0.00008001
Iteration 8/1000 | Loss: 0.00004896
Iteration 9/1000 | Loss: 0.00004427
Iteration 10/1000 | Loss: 0.00004117
Iteration 11/1000 | Loss: 0.00003950
Iteration 12/1000 | Loss: 0.00003840
Iteration 13/1000 | Loss: 0.00003735
Iteration 14/1000 | Loss: 0.00003663
Iteration 15/1000 | Loss: 0.00003585
Iteration 16/1000 | Loss: 0.00003538
Iteration 17/1000 | Loss: 0.00003497
Iteration 18/1000 | Loss: 0.00003467
Iteration 19/1000 | Loss: 0.00003442
Iteration 20/1000 | Loss: 0.00003439
Iteration 21/1000 | Loss: 0.00003416
Iteration 22/1000 | Loss: 0.00004610
Iteration 23/1000 | Loss: 0.00005089
Iteration 24/1000 | Loss: 0.00005194
Iteration 25/1000 | Loss: 0.00004361
Iteration 26/1000 | Loss: 0.00004857
Iteration 27/1000 | Loss: 0.00004282
Iteration 28/1000 | Loss: 0.00003578
Iteration 29/1000 | Loss: 0.00003487
Iteration 30/1000 | Loss: 0.00003432
Iteration 31/1000 | Loss: 0.00003530
Iteration 32/1000 | Loss: 0.00003449
Iteration 33/1000 | Loss: 0.00003391
Iteration 34/1000 | Loss: 0.00003502
Iteration 35/1000 | Loss: 0.00003501
Iteration 36/1000 | Loss: 0.00003414
Iteration 37/1000 | Loss: 0.00003382
Iteration 38/1000 | Loss: 0.00003374
Iteration 39/1000 | Loss: 0.00003374
Iteration 40/1000 | Loss: 0.00003373
Iteration 41/1000 | Loss: 0.00003373
Iteration 42/1000 | Loss: 0.00003371
Iteration 43/1000 | Loss: 0.00003368
Iteration 44/1000 | Loss: 0.00003367
Iteration 45/1000 | Loss: 0.00003367
Iteration 46/1000 | Loss: 0.00003367
Iteration 47/1000 | Loss: 0.00003366
Iteration 48/1000 | Loss: 0.00003366
Iteration 49/1000 | Loss: 0.00003365
Iteration 50/1000 | Loss: 0.00003364
Iteration 51/1000 | Loss: 0.00003363
Iteration 52/1000 | Loss: 0.00003361
Iteration 53/1000 | Loss: 0.00003361
Iteration 54/1000 | Loss: 0.00003355
Iteration 55/1000 | Loss: 0.00003355
Iteration 56/1000 | Loss: 0.00003355
Iteration 57/1000 | Loss: 0.00003355
Iteration 58/1000 | Loss: 0.00003355
Iteration 59/1000 | Loss: 0.00003355
Iteration 60/1000 | Loss: 0.00003355
Iteration 61/1000 | Loss: 0.00003355
Iteration 62/1000 | Loss: 0.00003355
Iteration 63/1000 | Loss: 0.00003354
Iteration 64/1000 | Loss: 0.00003354
Iteration 65/1000 | Loss: 0.00003353
Iteration 66/1000 | Loss: 0.00003353
Iteration 67/1000 | Loss: 0.00003352
Iteration 68/1000 | Loss: 0.00003352
Iteration 69/1000 | Loss: 0.00003352
Iteration 70/1000 | Loss: 0.00003351
Iteration 71/1000 | Loss: 0.00003351
Iteration 72/1000 | Loss: 0.00003351
Iteration 73/1000 | Loss: 0.00003350
Iteration 74/1000 | Loss: 0.00003350
Iteration 75/1000 | Loss: 0.00003350
Iteration 76/1000 | Loss: 0.00003349
Iteration 77/1000 | Loss: 0.00003349
Iteration 78/1000 | Loss: 0.00003349
Iteration 79/1000 | Loss: 0.00003349
Iteration 80/1000 | Loss: 0.00003349
Iteration 81/1000 | Loss: 0.00003349
Iteration 82/1000 | Loss: 0.00003348
Iteration 83/1000 | Loss: 0.00003348
Iteration 84/1000 | Loss: 0.00003348
Iteration 85/1000 | Loss: 0.00003347
Iteration 86/1000 | Loss: 0.00003347
Iteration 87/1000 | Loss: 0.00003347
Iteration 88/1000 | Loss: 0.00003347
Iteration 89/1000 | Loss: 0.00003346
Iteration 90/1000 | Loss: 0.00003346
Iteration 91/1000 | Loss: 0.00003345
Iteration 92/1000 | Loss: 0.00003345
Iteration 93/1000 | Loss: 0.00003345
Iteration 94/1000 | Loss: 0.00003345
Iteration 95/1000 | Loss: 0.00003345
Iteration 96/1000 | Loss: 0.00003345
Iteration 97/1000 | Loss: 0.00003344
Iteration 98/1000 | Loss: 0.00003344
Iteration 99/1000 | Loss: 0.00003344
Iteration 100/1000 | Loss: 0.00003344
Iteration 101/1000 | Loss: 0.00003343
Iteration 102/1000 | Loss: 0.00003343
Iteration 103/1000 | Loss: 0.00003343
Iteration 104/1000 | Loss: 0.00003343
Iteration 105/1000 | Loss: 0.00003342
Iteration 106/1000 | Loss: 0.00003342
Iteration 107/1000 | Loss: 0.00003342
Iteration 108/1000 | Loss: 0.00003342
Iteration 109/1000 | Loss: 0.00003342
Iteration 110/1000 | Loss: 0.00003342
Iteration 111/1000 | Loss: 0.00003342
Iteration 112/1000 | Loss: 0.00003342
Iteration 113/1000 | Loss: 0.00003342
Iteration 114/1000 | Loss: 0.00003342
Iteration 115/1000 | Loss: 0.00003342
Iteration 116/1000 | Loss: 0.00003341
Iteration 117/1000 | Loss: 0.00003341
Iteration 118/1000 | Loss: 0.00003341
Iteration 119/1000 | Loss: 0.00003341
Iteration 120/1000 | Loss: 0.00003341
Iteration 121/1000 | Loss: 0.00003341
Iteration 122/1000 | Loss: 0.00003341
Iteration 123/1000 | Loss: 0.00003341
Iteration 124/1000 | Loss: 0.00003341
Iteration 125/1000 | Loss: 0.00003341
Iteration 126/1000 | Loss: 0.00003341
Iteration 127/1000 | Loss: 0.00003341
Iteration 128/1000 | Loss: 0.00003340
Iteration 129/1000 | Loss: 0.00003340
Iteration 130/1000 | Loss: 0.00003340
Iteration 131/1000 | Loss: 0.00003340
Iteration 132/1000 | Loss: 0.00003340
Iteration 133/1000 | Loss: 0.00003340
Iteration 134/1000 | Loss: 0.00003340
Iteration 135/1000 | Loss: 0.00003340
Iteration 136/1000 | Loss: 0.00003340
Iteration 137/1000 | Loss: 0.00003340
Iteration 138/1000 | Loss: 0.00003340
Iteration 139/1000 | Loss: 0.00003340
Iteration 140/1000 | Loss: 0.00003340
Iteration 141/1000 | Loss: 0.00003340
Iteration 142/1000 | Loss: 0.00003340
Iteration 143/1000 | Loss: 0.00003340
Iteration 144/1000 | Loss: 0.00003340
Iteration 145/1000 | Loss: 0.00003339
Iteration 146/1000 | Loss: 0.00003339
Iteration 147/1000 | Loss: 0.00003339
Iteration 148/1000 | Loss: 0.00003339
Iteration 149/1000 | Loss: 0.00003339
Iteration 150/1000 | Loss: 0.00003339
Iteration 151/1000 | Loss: 0.00003339
Iteration 152/1000 | Loss: 0.00003339
Iteration 153/1000 | Loss: 0.00003339
Iteration 154/1000 | Loss: 0.00003339
Iteration 155/1000 | Loss: 0.00003339
Iteration 156/1000 | Loss: 0.00003339
Iteration 157/1000 | Loss: 0.00003339
Iteration 158/1000 | Loss: 0.00003339
Iteration 159/1000 | Loss: 0.00003339
Iteration 160/1000 | Loss: 0.00003339
Iteration 161/1000 | Loss: 0.00003339
Iteration 162/1000 | Loss: 0.00003339
Iteration 163/1000 | Loss: 0.00003339
Iteration 164/1000 | Loss: 0.00003339
Iteration 165/1000 | Loss: 0.00003339
Iteration 166/1000 | Loss: 0.00003339
Iteration 167/1000 | Loss: 0.00003339
Iteration 168/1000 | Loss: 0.00003339
Iteration 169/1000 | Loss: 0.00003339
Iteration 170/1000 | Loss: 0.00003339
Iteration 171/1000 | Loss: 0.00003339
Iteration 172/1000 | Loss: 0.00003339
Iteration 173/1000 | Loss: 0.00003339
Iteration 174/1000 | Loss: 0.00003339
Iteration 175/1000 | Loss: 0.00003339
Iteration 176/1000 | Loss: 0.00003339
Iteration 177/1000 | Loss: 0.00003339
Iteration 178/1000 | Loss: 0.00003339
Iteration 179/1000 | Loss: 0.00003339
Iteration 180/1000 | Loss: 0.00003339
Iteration 181/1000 | Loss: 0.00003339
Iteration 182/1000 | Loss: 0.00003339
Iteration 183/1000 | Loss: 0.00003339
Iteration 184/1000 | Loss: 0.00003339
Iteration 185/1000 | Loss: 0.00003339
Iteration 186/1000 | Loss: 0.00003339
Iteration 187/1000 | Loss: 0.00003339
Iteration 188/1000 | Loss: 0.00003339
Iteration 189/1000 | Loss: 0.00003339
Iteration 190/1000 | Loss: 0.00003339
Iteration 191/1000 | Loss: 0.00003339
Iteration 192/1000 | Loss: 0.00003339
Iteration 193/1000 | Loss: 0.00003339
Iteration 194/1000 | Loss: 0.00003339
Iteration 195/1000 | Loss: 0.00003339
Iteration 196/1000 | Loss: 0.00003339
Iteration 197/1000 | Loss: 0.00003339
Iteration 198/1000 | Loss: 0.00003339
Iteration 199/1000 | Loss: 0.00003339
Iteration 200/1000 | Loss: 0.00003339
Iteration 201/1000 | Loss: 0.00003339
Iteration 202/1000 | Loss: 0.00003339
Iteration 203/1000 | Loss: 0.00003339
Iteration 204/1000 | Loss: 0.00003339
Iteration 205/1000 | Loss: 0.00003339
Iteration 206/1000 | Loss: 0.00003339
Iteration 207/1000 | Loss: 0.00003339
Iteration 208/1000 | Loss: 0.00003339
Iteration 209/1000 | Loss: 0.00003339
Iteration 210/1000 | Loss: 0.00003339
Iteration 211/1000 | Loss: 0.00003339
Iteration 212/1000 | Loss: 0.00003339
Iteration 213/1000 | Loss: 0.00003339
Iteration 214/1000 | Loss: 0.00003339
Iteration 215/1000 | Loss: 0.00003339
Iteration 216/1000 | Loss: 0.00003339
Iteration 217/1000 | Loss: 0.00003339
Iteration 218/1000 | Loss: 0.00003339
Iteration 219/1000 | Loss: 0.00003339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [3.33853458869271e-05, 3.33853458869271e-05, 3.33853458869271e-05, 3.33853458869271e-05, 3.33853458869271e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.33853458869271e-05

Optimization complete. Final v2v error: 4.541794300079346 mm

Highest mean error: 7.516537189483643 mm for frame 178

Lowest mean error: 2.9338526725769043 mm for frame 239

Saving results

Total time: 120.60573601722717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834387
Iteration 2/25 | Loss: 0.00142898
Iteration 3/25 | Loss: 0.00111960
Iteration 4/25 | Loss: 0.00110018
Iteration 5/25 | Loss: 0.00109692
Iteration 6/25 | Loss: 0.00109692
Iteration 7/25 | Loss: 0.00109692
Iteration 8/25 | Loss: 0.00109692
Iteration 9/25 | Loss: 0.00109692
Iteration 10/25 | Loss: 0.00109692
Iteration 11/25 | Loss: 0.00109692
Iteration 12/25 | Loss: 0.00109692
Iteration 13/25 | Loss: 0.00109692
Iteration 14/25 | Loss: 0.00109692
Iteration 15/25 | Loss: 0.00109692
Iteration 16/25 | Loss: 0.00109692
Iteration 17/25 | Loss: 0.00109692
Iteration 18/25 | Loss: 0.00109692
Iteration 19/25 | Loss: 0.00109692
Iteration 20/25 | Loss: 0.00109692
Iteration 21/25 | Loss: 0.00109692
Iteration 22/25 | Loss: 0.00109692
Iteration 23/25 | Loss: 0.00109692
Iteration 24/25 | Loss: 0.00109692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010969190625473857, 0.0010969190625473857, 0.0010969190625473857, 0.0010969190625473857, 0.0010969190625473857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010969190625473857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39383364
Iteration 2/25 | Loss: 0.00056370
Iteration 3/25 | Loss: 0.00056368
Iteration 4/25 | Loss: 0.00056368
Iteration 5/25 | Loss: 0.00056368
Iteration 6/25 | Loss: 0.00056368
Iteration 7/25 | Loss: 0.00056368
Iteration 8/25 | Loss: 0.00056368
Iteration 9/25 | Loss: 0.00056368
Iteration 10/25 | Loss: 0.00056368
Iteration 11/25 | Loss: 0.00056368
Iteration 12/25 | Loss: 0.00056368
Iteration 13/25 | Loss: 0.00056368
Iteration 14/25 | Loss: 0.00056368
Iteration 15/25 | Loss: 0.00056368
Iteration 16/25 | Loss: 0.00056368
Iteration 17/25 | Loss: 0.00056368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005636803107336164, 0.0005636803107336164, 0.0005636803107336164, 0.0005636803107336164, 0.0005636803107336164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005636803107336164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056368
Iteration 2/1000 | Loss: 0.00002033
Iteration 3/1000 | Loss: 0.00001424
Iteration 4/1000 | Loss: 0.00001292
Iteration 5/1000 | Loss: 0.00001219
Iteration 6/1000 | Loss: 0.00001179
Iteration 7/1000 | Loss: 0.00001158
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001125
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001116
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001104
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001096
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001095
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001083
Iteration 62/1000 | Loss: 0.00001083
Iteration 63/1000 | Loss: 0.00001083
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001082
Iteration 66/1000 | Loss: 0.00001082
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001079
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001077
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001076
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001072
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001071
Iteration 103/1000 | Loss: 0.00001071
Iteration 104/1000 | Loss: 0.00001071
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001070
Iteration 110/1000 | Loss: 0.00001070
Iteration 111/1000 | Loss: 0.00001070
Iteration 112/1000 | Loss: 0.00001070
Iteration 113/1000 | Loss: 0.00001070
Iteration 114/1000 | Loss: 0.00001070
Iteration 115/1000 | Loss: 0.00001070
Iteration 116/1000 | Loss: 0.00001070
Iteration 117/1000 | Loss: 0.00001070
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001070
Iteration 120/1000 | Loss: 0.00001069
Iteration 121/1000 | Loss: 0.00001069
Iteration 122/1000 | Loss: 0.00001069
Iteration 123/1000 | Loss: 0.00001069
Iteration 124/1000 | Loss: 0.00001069
Iteration 125/1000 | Loss: 0.00001069
Iteration 126/1000 | Loss: 0.00001069
Iteration 127/1000 | Loss: 0.00001069
Iteration 128/1000 | Loss: 0.00001069
Iteration 129/1000 | Loss: 0.00001069
Iteration 130/1000 | Loss: 0.00001069
Iteration 131/1000 | Loss: 0.00001069
Iteration 132/1000 | Loss: 0.00001069
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.0688024303817656e-05, 1.0688024303817656e-05, 1.0688024303817656e-05, 1.0688024303817656e-05, 1.0688024303817656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0688024303817656e-05

Optimization complete. Final v2v error: 2.8044967651367188 mm

Highest mean error: 3.130202531814575 mm for frame 124

Lowest mean error: 2.3892436027526855 mm for frame 39

Saving results

Total time: 39.10315537452698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783178
Iteration 2/25 | Loss: 0.00145516
Iteration 3/25 | Loss: 0.00117660
Iteration 4/25 | Loss: 0.00113420
Iteration 5/25 | Loss: 0.00112657
Iteration 6/25 | Loss: 0.00112472
Iteration 7/25 | Loss: 0.00112472
Iteration 8/25 | Loss: 0.00112472
Iteration 9/25 | Loss: 0.00112472
Iteration 10/25 | Loss: 0.00112472
Iteration 11/25 | Loss: 0.00112472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011247241636738181, 0.0011247241636738181, 0.0011247241636738181, 0.0011247241636738181, 0.0011247241636738181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011247241636738181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38296306
Iteration 2/25 | Loss: 0.00050265
Iteration 3/25 | Loss: 0.00050264
Iteration 4/25 | Loss: 0.00050264
Iteration 5/25 | Loss: 0.00050264
Iteration 6/25 | Loss: 0.00050264
Iteration 7/25 | Loss: 0.00050264
Iteration 8/25 | Loss: 0.00050264
Iteration 9/25 | Loss: 0.00050264
Iteration 10/25 | Loss: 0.00050264
Iteration 11/25 | Loss: 0.00050264
Iteration 12/25 | Loss: 0.00050264
Iteration 13/25 | Loss: 0.00050264
Iteration 14/25 | Loss: 0.00050264
Iteration 15/25 | Loss: 0.00050264
Iteration 16/25 | Loss: 0.00050264
Iteration 17/25 | Loss: 0.00050264
Iteration 18/25 | Loss: 0.00050264
Iteration 19/25 | Loss: 0.00050264
Iteration 20/25 | Loss: 0.00050264
Iteration 21/25 | Loss: 0.00050264
Iteration 22/25 | Loss: 0.00050264
Iteration 23/25 | Loss: 0.00050264
Iteration 24/25 | Loss: 0.00050264
Iteration 25/25 | Loss: 0.00050264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050264
Iteration 2/1000 | Loss: 0.00003248
Iteration 3/1000 | Loss: 0.00002297
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001940
Iteration 6/1000 | Loss: 0.00001853
Iteration 7/1000 | Loss: 0.00001792
Iteration 8/1000 | Loss: 0.00001762
Iteration 9/1000 | Loss: 0.00001723
Iteration 10/1000 | Loss: 0.00001702
Iteration 11/1000 | Loss: 0.00001696
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001673
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001669
Iteration 20/1000 | Loss: 0.00001667
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001657
Iteration 24/1000 | Loss: 0.00001654
Iteration 25/1000 | Loss: 0.00001654
Iteration 26/1000 | Loss: 0.00001654
Iteration 27/1000 | Loss: 0.00001653
Iteration 28/1000 | Loss: 0.00001653
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001647
Iteration 33/1000 | Loss: 0.00001647
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001646
Iteration 42/1000 | Loss: 0.00001646
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00001645
Iteration 47/1000 | Loss: 0.00001645
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001645
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001644
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001640
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001638
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001637
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001634
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001633
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001633
Iteration 111/1000 | Loss: 0.00001633
Iteration 112/1000 | Loss: 0.00001633
Iteration 113/1000 | Loss: 0.00001633
Iteration 114/1000 | Loss: 0.00001633
Iteration 115/1000 | Loss: 0.00001632
Iteration 116/1000 | Loss: 0.00001632
Iteration 117/1000 | Loss: 0.00001632
Iteration 118/1000 | Loss: 0.00001632
Iteration 119/1000 | Loss: 0.00001632
Iteration 120/1000 | Loss: 0.00001632
Iteration 121/1000 | Loss: 0.00001632
Iteration 122/1000 | Loss: 0.00001631
Iteration 123/1000 | Loss: 0.00001631
Iteration 124/1000 | Loss: 0.00001631
Iteration 125/1000 | Loss: 0.00001631
Iteration 126/1000 | Loss: 0.00001631
Iteration 127/1000 | Loss: 0.00001630
Iteration 128/1000 | Loss: 0.00001630
Iteration 129/1000 | Loss: 0.00001630
Iteration 130/1000 | Loss: 0.00001630
Iteration 131/1000 | Loss: 0.00001630
Iteration 132/1000 | Loss: 0.00001630
Iteration 133/1000 | Loss: 0.00001630
Iteration 134/1000 | Loss: 0.00001630
Iteration 135/1000 | Loss: 0.00001630
Iteration 136/1000 | Loss: 0.00001630
Iteration 137/1000 | Loss: 0.00001629
Iteration 138/1000 | Loss: 0.00001629
Iteration 139/1000 | Loss: 0.00001629
Iteration 140/1000 | Loss: 0.00001629
Iteration 141/1000 | Loss: 0.00001628
Iteration 142/1000 | Loss: 0.00001628
Iteration 143/1000 | Loss: 0.00001628
Iteration 144/1000 | Loss: 0.00001628
Iteration 145/1000 | Loss: 0.00001628
Iteration 146/1000 | Loss: 0.00001628
Iteration 147/1000 | Loss: 0.00001628
Iteration 148/1000 | Loss: 0.00001628
Iteration 149/1000 | Loss: 0.00001628
Iteration 150/1000 | Loss: 0.00001628
Iteration 151/1000 | Loss: 0.00001627
Iteration 152/1000 | Loss: 0.00001627
Iteration 153/1000 | Loss: 0.00001627
Iteration 154/1000 | Loss: 0.00001627
Iteration 155/1000 | Loss: 0.00001627
Iteration 156/1000 | Loss: 0.00001627
Iteration 157/1000 | Loss: 0.00001627
Iteration 158/1000 | Loss: 0.00001627
Iteration 159/1000 | Loss: 0.00001627
Iteration 160/1000 | Loss: 0.00001627
Iteration 161/1000 | Loss: 0.00001627
Iteration 162/1000 | Loss: 0.00001627
Iteration 163/1000 | Loss: 0.00001627
Iteration 164/1000 | Loss: 0.00001627
Iteration 165/1000 | Loss: 0.00001626
Iteration 166/1000 | Loss: 0.00001626
Iteration 167/1000 | Loss: 0.00001626
Iteration 168/1000 | Loss: 0.00001626
Iteration 169/1000 | Loss: 0.00001626
Iteration 170/1000 | Loss: 0.00001626
Iteration 171/1000 | Loss: 0.00001626
Iteration 172/1000 | Loss: 0.00001626
Iteration 173/1000 | Loss: 0.00001626
Iteration 174/1000 | Loss: 0.00001626
Iteration 175/1000 | Loss: 0.00001626
Iteration 176/1000 | Loss: 0.00001626
Iteration 177/1000 | Loss: 0.00001626
Iteration 178/1000 | Loss: 0.00001626
Iteration 179/1000 | Loss: 0.00001625
Iteration 180/1000 | Loss: 0.00001625
Iteration 181/1000 | Loss: 0.00001625
Iteration 182/1000 | Loss: 0.00001625
Iteration 183/1000 | Loss: 0.00001625
Iteration 184/1000 | Loss: 0.00001625
Iteration 185/1000 | Loss: 0.00001625
Iteration 186/1000 | Loss: 0.00001625
Iteration 187/1000 | Loss: 0.00001625
Iteration 188/1000 | Loss: 0.00001625
Iteration 189/1000 | Loss: 0.00001625
Iteration 190/1000 | Loss: 0.00001625
Iteration 191/1000 | Loss: 0.00001625
Iteration 192/1000 | Loss: 0.00001624
Iteration 193/1000 | Loss: 0.00001624
Iteration 194/1000 | Loss: 0.00001624
Iteration 195/1000 | Loss: 0.00001624
Iteration 196/1000 | Loss: 0.00001624
Iteration 197/1000 | Loss: 0.00001624
Iteration 198/1000 | Loss: 0.00001624
Iteration 199/1000 | Loss: 0.00001624
Iteration 200/1000 | Loss: 0.00001624
Iteration 201/1000 | Loss: 0.00001624
Iteration 202/1000 | Loss: 0.00001624
Iteration 203/1000 | Loss: 0.00001624
Iteration 204/1000 | Loss: 0.00001624
Iteration 205/1000 | Loss: 0.00001624
Iteration 206/1000 | Loss: 0.00001624
Iteration 207/1000 | Loss: 0.00001624
Iteration 208/1000 | Loss: 0.00001624
Iteration 209/1000 | Loss: 0.00001624
Iteration 210/1000 | Loss: 0.00001624
Iteration 211/1000 | Loss: 0.00001624
Iteration 212/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.624226752028335e-05, 1.624226752028335e-05, 1.624226752028335e-05, 1.624226752028335e-05, 1.624226752028335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.624226752028335e-05

Optimization complete. Final v2v error: 3.393221855163574 mm

Highest mean error: 3.9006783962249756 mm for frame 184

Lowest mean error: 3.103410243988037 mm for frame 89

Saving results

Total time: 46.11927008628845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815471
Iteration 2/25 | Loss: 0.00156367
Iteration 3/25 | Loss: 0.00127421
Iteration 4/25 | Loss: 0.00125432
Iteration 5/25 | Loss: 0.00125427
Iteration 6/25 | Loss: 0.00125427
Iteration 7/25 | Loss: 0.00125427
Iteration 8/25 | Loss: 0.00125427
Iteration 9/25 | Loss: 0.00125427
Iteration 10/25 | Loss: 0.00125427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012542650802060962, 0.0012542650802060962, 0.0012542650802060962, 0.0012542650802060962, 0.0012542650802060962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012542650802060962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28770494
Iteration 2/25 | Loss: 0.00064174
Iteration 3/25 | Loss: 0.00064171
Iteration 4/25 | Loss: 0.00064171
Iteration 5/25 | Loss: 0.00064171
Iteration 6/25 | Loss: 0.00064171
Iteration 7/25 | Loss: 0.00064170
Iteration 8/25 | Loss: 0.00064170
Iteration 9/25 | Loss: 0.00064170
Iteration 10/25 | Loss: 0.00064170
Iteration 11/25 | Loss: 0.00064170
Iteration 12/25 | Loss: 0.00064170
Iteration 13/25 | Loss: 0.00064170
Iteration 14/25 | Loss: 0.00064170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006417043623514473, 0.0006417043623514473, 0.0006417043623514473, 0.0006417043623514473, 0.0006417043623514473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006417043623514473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064170
Iteration 2/1000 | Loss: 0.00003952
Iteration 3/1000 | Loss: 0.00002546
Iteration 4/1000 | Loss: 0.00002272
Iteration 5/1000 | Loss: 0.00002212
Iteration 6/1000 | Loss: 0.00002162
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002111
Iteration 9/1000 | Loss: 0.00002092
Iteration 10/1000 | Loss: 0.00002084
Iteration 11/1000 | Loss: 0.00002077
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002075
Iteration 14/1000 | Loss: 0.00002075
Iteration 15/1000 | Loss: 0.00002074
Iteration 16/1000 | Loss: 0.00002073
Iteration 17/1000 | Loss: 0.00002072
Iteration 18/1000 | Loss: 0.00002070
Iteration 19/1000 | Loss: 0.00002070
Iteration 20/1000 | Loss: 0.00002069
Iteration 21/1000 | Loss: 0.00002069
Iteration 22/1000 | Loss: 0.00002069
Iteration 23/1000 | Loss: 0.00002068
Iteration 24/1000 | Loss: 0.00002068
Iteration 25/1000 | Loss: 0.00002067
Iteration 26/1000 | Loss: 0.00002067
Iteration 27/1000 | Loss: 0.00002067
Iteration 28/1000 | Loss: 0.00002067
Iteration 29/1000 | Loss: 0.00002067
Iteration 30/1000 | Loss: 0.00002066
Iteration 31/1000 | Loss: 0.00002066
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00002063
Iteration 34/1000 | Loss: 0.00002063
Iteration 35/1000 | Loss: 0.00002063
Iteration 36/1000 | Loss: 0.00002063
Iteration 37/1000 | Loss: 0.00002063
Iteration 38/1000 | Loss: 0.00002063
Iteration 39/1000 | Loss: 0.00002063
Iteration 40/1000 | Loss: 0.00002063
Iteration 41/1000 | Loss: 0.00002062
Iteration 42/1000 | Loss: 0.00002061
Iteration 43/1000 | Loss: 0.00002061
Iteration 44/1000 | Loss: 0.00002060
Iteration 45/1000 | Loss: 0.00002059
Iteration 46/1000 | Loss: 0.00002059
Iteration 47/1000 | Loss: 0.00002058
Iteration 48/1000 | Loss: 0.00002058
Iteration 49/1000 | Loss: 0.00002058
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002057
Iteration 54/1000 | Loss: 0.00002056
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002056
Iteration 60/1000 | Loss: 0.00002056
Iteration 61/1000 | Loss: 0.00002056
Iteration 62/1000 | Loss: 0.00002055
Iteration 63/1000 | Loss: 0.00002055
Iteration 64/1000 | Loss: 0.00002055
Iteration 65/1000 | Loss: 0.00002055
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002054
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002054
Iteration 72/1000 | Loss: 0.00002054
Iteration 73/1000 | Loss: 0.00002054
Iteration 74/1000 | Loss: 0.00002054
Iteration 75/1000 | Loss: 0.00002054
Iteration 76/1000 | Loss: 0.00002054
Iteration 77/1000 | Loss: 0.00002054
Iteration 78/1000 | Loss: 0.00002053
Iteration 79/1000 | Loss: 0.00002053
Iteration 80/1000 | Loss: 0.00002053
Iteration 81/1000 | Loss: 0.00002053
Iteration 82/1000 | Loss: 0.00002053
Iteration 83/1000 | Loss: 0.00002053
Iteration 84/1000 | Loss: 0.00002052
Iteration 85/1000 | Loss: 0.00002052
Iteration 86/1000 | Loss: 0.00002052
Iteration 87/1000 | Loss: 0.00002052
Iteration 88/1000 | Loss: 0.00002052
Iteration 89/1000 | Loss: 0.00002052
Iteration 90/1000 | Loss: 0.00002051
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002051
Iteration 93/1000 | Loss: 0.00002051
Iteration 94/1000 | Loss: 0.00002051
Iteration 95/1000 | Loss: 0.00002051
Iteration 96/1000 | Loss: 0.00002051
Iteration 97/1000 | Loss: 0.00002051
Iteration 98/1000 | Loss: 0.00002050
Iteration 99/1000 | Loss: 0.00002050
Iteration 100/1000 | Loss: 0.00002050
Iteration 101/1000 | Loss: 0.00002050
Iteration 102/1000 | Loss: 0.00002050
Iteration 103/1000 | Loss: 0.00002050
Iteration 104/1000 | Loss: 0.00002050
Iteration 105/1000 | Loss: 0.00002049
Iteration 106/1000 | Loss: 0.00002049
Iteration 107/1000 | Loss: 0.00002049
Iteration 108/1000 | Loss: 0.00002049
Iteration 109/1000 | Loss: 0.00002049
Iteration 110/1000 | Loss: 0.00002049
Iteration 111/1000 | Loss: 0.00002048
Iteration 112/1000 | Loss: 0.00002048
Iteration 113/1000 | Loss: 0.00002048
Iteration 114/1000 | Loss: 0.00002047
Iteration 115/1000 | Loss: 0.00002047
Iteration 116/1000 | Loss: 0.00002047
Iteration 117/1000 | Loss: 0.00002047
Iteration 118/1000 | Loss: 0.00002047
Iteration 119/1000 | Loss: 0.00002046
Iteration 120/1000 | Loss: 0.00002046
Iteration 121/1000 | Loss: 0.00002046
Iteration 122/1000 | Loss: 0.00002045
Iteration 123/1000 | Loss: 0.00002045
Iteration 124/1000 | Loss: 0.00002045
Iteration 125/1000 | Loss: 0.00002045
Iteration 126/1000 | Loss: 0.00002045
Iteration 127/1000 | Loss: 0.00002045
Iteration 128/1000 | Loss: 0.00002045
Iteration 129/1000 | Loss: 0.00002044
Iteration 130/1000 | Loss: 0.00002044
Iteration 131/1000 | Loss: 0.00002044
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002044
Iteration 136/1000 | Loss: 0.00002044
Iteration 137/1000 | Loss: 0.00002044
Iteration 138/1000 | Loss: 0.00002044
Iteration 139/1000 | Loss: 0.00002043
Iteration 140/1000 | Loss: 0.00002043
Iteration 141/1000 | Loss: 0.00002043
Iteration 142/1000 | Loss: 0.00002043
Iteration 143/1000 | Loss: 0.00002043
Iteration 144/1000 | Loss: 0.00002043
Iteration 145/1000 | Loss: 0.00002043
Iteration 146/1000 | Loss: 0.00002043
Iteration 147/1000 | Loss: 0.00002042
Iteration 148/1000 | Loss: 0.00002041
Iteration 149/1000 | Loss: 0.00002041
Iteration 150/1000 | Loss: 0.00002041
Iteration 151/1000 | Loss: 0.00002041
Iteration 152/1000 | Loss: 0.00002040
Iteration 153/1000 | Loss: 0.00002040
Iteration 154/1000 | Loss: 0.00002040
Iteration 155/1000 | Loss: 0.00002039
Iteration 156/1000 | Loss: 0.00002039
Iteration 157/1000 | Loss: 0.00002038
Iteration 158/1000 | Loss: 0.00002038
Iteration 159/1000 | Loss: 0.00002038
Iteration 160/1000 | Loss: 0.00002038
Iteration 161/1000 | Loss: 0.00002038
Iteration 162/1000 | Loss: 0.00002037
Iteration 163/1000 | Loss: 0.00002037
Iteration 164/1000 | Loss: 0.00002037
Iteration 165/1000 | Loss: 0.00002036
Iteration 166/1000 | Loss: 0.00002036
Iteration 167/1000 | Loss: 0.00002036
Iteration 168/1000 | Loss: 0.00002036
Iteration 169/1000 | Loss: 0.00002036
Iteration 170/1000 | Loss: 0.00002036
Iteration 171/1000 | Loss: 0.00002035
Iteration 172/1000 | Loss: 0.00002035
Iteration 173/1000 | Loss: 0.00002035
Iteration 174/1000 | Loss: 0.00002035
Iteration 175/1000 | Loss: 0.00002035
Iteration 176/1000 | Loss: 0.00002035
Iteration 177/1000 | Loss: 0.00002035
Iteration 178/1000 | Loss: 0.00002035
Iteration 179/1000 | Loss: 0.00002035
Iteration 180/1000 | Loss: 0.00002035
Iteration 181/1000 | Loss: 0.00002035
Iteration 182/1000 | Loss: 0.00002035
Iteration 183/1000 | Loss: 0.00002035
Iteration 184/1000 | Loss: 0.00002035
Iteration 185/1000 | Loss: 0.00002035
Iteration 186/1000 | Loss: 0.00002035
Iteration 187/1000 | Loss: 0.00002035
Iteration 188/1000 | Loss: 0.00002035
Iteration 189/1000 | Loss: 0.00002035
Iteration 190/1000 | Loss: 0.00002035
Iteration 191/1000 | Loss: 0.00002035
Iteration 192/1000 | Loss: 0.00002035
Iteration 193/1000 | Loss: 0.00002035
Iteration 194/1000 | Loss: 0.00002035
Iteration 195/1000 | Loss: 0.00002035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.0347028112155385e-05, 2.0347028112155385e-05, 2.0347028112155385e-05, 2.0347028112155385e-05, 2.0347028112155385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0347028112155385e-05

Optimization complete. Final v2v error: 3.729999303817749 mm

Highest mean error: 3.972053050994873 mm for frame 158

Lowest mean error: 3.494473695755005 mm for frame 93

Saving results

Total time: 38.394174337387085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833663
Iteration 2/25 | Loss: 0.00173839
Iteration 3/25 | Loss: 0.00133425
Iteration 4/25 | Loss: 0.00127536
Iteration 5/25 | Loss: 0.00129365
Iteration 6/25 | Loss: 0.00125041
Iteration 7/25 | Loss: 0.00124884
Iteration 8/25 | Loss: 0.00122180
Iteration 9/25 | Loss: 0.00120556
Iteration 10/25 | Loss: 0.00120065
Iteration 11/25 | Loss: 0.00120208
Iteration 12/25 | Loss: 0.00119449
Iteration 13/25 | Loss: 0.00118948
Iteration 14/25 | Loss: 0.00118899
Iteration 15/25 | Loss: 0.00118893
Iteration 16/25 | Loss: 0.00118893
Iteration 17/25 | Loss: 0.00118893
Iteration 18/25 | Loss: 0.00118893
Iteration 19/25 | Loss: 0.00118893
Iteration 20/25 | Loss: 0.00118893
Iteration 21/25 | Loss: 0.00118893
Iteration 22/25 | Loss: 0.00118893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011889309389516711, 0.0011889309389516711, 0.0011889309389516711, 0.0011889309389516711, 0.0011889309389516711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011889309389516711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68156815
Iteration 2/25 | Loss: 0.00061658
Iteration 3/25 | Loss: 0.00057091
Iteration 4/25 | Loss: 0.00057091
Iteration 5/25 | Loss: 0.00057091
Iteration 6/25 | Loss: 0.00057091
Iteration 7/25 | Loss: 0.00057091
Iteration 8/25 | Loss: 0.00057091
Iteration 9/25 | Loss: 0.00057091
Iteration 10/25 | Loss: 0.00057091
Iteration 11/25 | Loss: 0.00057091
Iteration 12/25 | Loss: 0.00057091
Iteration 13/25 | Loss: 0.00057091
Iteration 14/25 | Loss: 0.00057091
Iteration 15/25 | Loss: 0.00057091
Iteration 16/25 | Loss: 0.00057091
Iteration 17/25 | Loss: 0.00057091
Iteration 18/25 | Loss: 0.00057091
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005709078395739198, 0.0005709078395739198, 0.0005709078395739198, 0.0005709078395739198, 0.0005709078395739198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005709078395739198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057091
Iteration 2/1000 | Loss: 0.00007160
Iteration 3/1000 | Loss: 0.00002299
Iteration 4/1000 | Loss: 0.00004479
Iteration 5/1000 | Loss: 0.00002023
Iteration 6/1000 | Loss: 0.00001946
Iteration 7/1000 | Loss: 0.00001895
Iteration 8/1000 | Loss: 0.00001849
Iteration 9/1000 | Loss: 0.00001820
Iteration 10/1000 | Loss: 0.00001803
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001788
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001784
Iteration 17/1000 | Loss: 0.00001784
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001783
Iteration 22/1000 | Loss: 0.00001782
Iteration 23/1000 | Loss: 0.00001782
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001781
Iteration 26/1000 | Loss: 0.00001781
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001778
Iteration 31/1000 | Loss: 0.00001778
Iteration 32/1000 | Loss: 0.00001778
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001776
Iteration 37/1000 | Loss: 0.00001775
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001770
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001769
Iteration 75/1000 | Loss: 0.00001769
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001768
Iteration 79/1000 | Loss: 0.00001768
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001768
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001768
Iteration 85/1000 | Loss: 0.00001768
Iteration 86/1000 | Loss: 0.00001768
Iteration 87/1000 | Loss: 0.00001768
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001766
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00001766
Iteration 99/1000 | Loss: 0.00001766
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001766
Iteration 103/1000 | Loss: 0.00001766
Iteration 104/1000 | Loss: 0.00001766
Iteration 105/1000 | Loss: 0.00001766
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001765
Iteration 109/1000 | Loss: 0.00001765
Iteration 110/1000 | Loss: 0.00001764
Iteration 111/1000 | Loss: 0.00001764
Iteration 112/1000 | Loss: 0.00001764
Iteration 113/1000 | Loss: 0.00001764
Iteration 114/1000 | Loss: 0.00001763
Iteration 115/1000 | Loss: 0.00001763
Iteration 116/1000 | Loss: 0.00001763
Iteration 117/1000 | Loss: 0.00001763
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001762
Iteration 120/1000 | Loss: 0.00001762
Iteration 121/1000 | Loss: 0.00001762
Iteration 122/1000 | Loss: 0.00001761
Iteration 123/1000 | Loss: 0.00001761
Iteration 124/1000 | Loss: 0.00001761
Iteration 125/1000 | Loss: 0.00001761
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001761
Iteration 129/1000 | Loss: 0.00001761
Iteration 130/1000 | Loss: 0.00001761
Iteration 131/1000 | Loss: 0.00001760
Iteration 132/1000 | Loss: 0.00001760
Iteration 133/1000 | Loss: 0.00001760
Iteration 134/1000 | Loss: 0.00001760
Iteration 135/1000 | Loss: 0.00001760
Iteration 136/1000 | Loss: 0.00001760
Iteration 137/1000 | Loss: 0.00001760
Iteration 138/1000 | Loss: 0.00001760
Iteration 139/1000 | Loss: 0.00001760
Iteration 140/1000 | Loss: 0.00001760
Iteration 141/1000 | Loss: 0.00001760
Iteration 142/1000 | Loss: 0.00001759
Iteration 143/1000 | Loss: 0.00001759
Iteration 144/1000 | Loss: 0.00001759
Iteration 145/1000 | Loss: 0.00001759
Iteration 146/1000 | Loss: 0.00001759
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001758
Iteration 151/1000 | Loss: 0.00001758
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001758
Iteration 154/1000 | Loss: 0.00001758
Iteration 155/1000 | Loss: 0.00001757
Iteration 156/1000 | Loss: 0.00001757
Iteration 157/1000 | Loss: 0.00001757
Iteration 158/1000 | Loss: 0.00001756
Iteration 159/1000 | Loss: 0.00001756
Iteration 160/1000 | Loss: 0.00001756
Iteration 161/1000 | Loss: 0.00001756
Iteration 162/1000 | Loss: 0.00001756
Iteration 163/1000 | Loss: 0.00001756
Iteration 164/1000 | Loss: 0.00001756
Iteration 165/1000 | Loss: 0.00001756
Iteration 166/1000 | Loss: 0.00001756
Iteration 167/1000 | Loss: 0.00001756
Iteration 168/1000 | Loss: 0.00001756
Iteration 169/1000 | Loss: 0.00001756
Iteration 170/1000 | Loss: 0.00001756
Iteration 171/1000 | Loss: 0.00001756
Iteration 172/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.7564127119840123e-05, 1.7564127119840123e-05, 1.7564127119840123e-05, 1.7564127119840123e-05, 1.7564127119840123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7564127119840123e-05

Optimization complete. Final v2v error: 3.499373435974121 mm

Highest mean error: 3.875121831893921 mm for frame 19

Lowest mean error: 3.0930168628692627 mm for frame 238

Saving results

Total time: 60.141300201416016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340893
Iteration 2/25 | Loss: 0.00137161
Iteration 3/25 | Loss: 0.00115545
Iteration 4/25 | Loss: 0.00109727
Iteration 5/25 | Loss: 0.00108611
Iteration 6/25 | Loss: 0.00108228
Iteration 7/25 | Loss: 0.00108104
Iteration 8/25 | Loss: 0.00108340
Iteration 9/25 | Loss: 0.00108063
Iteration 10/25 | Loss: 0.00108012
Iteration 11/25 | Loss: 0.00107958
Iteration 12/25 | Loss: 0.00107781
Iteration 13/25 | Loss: 0.00107744
Iteration 14/25 | Loss: 0.00107741
Iteration 15/25 | Loss: 0.00107741
Iteration 16/25 | Loss: 0.00107741
Iteration 17/25 | Loss: 0.00107740
Iteration 18/25 | Loss: 0.00107740
Iteration 19/25 | Loss: 0.00107739
Iteration 20/25 | Loss: 0.00107739
Iteration 21/25 | Loss: 0.00107738
Iteration 22/25 | Loss: 0.00107738
Iteration 23/25 | Loss: 0.00107738
Iteration 24/25 | Loss: 0.00107738
Iteration 25/25 | Loss: 0.00107737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40383160
Iteration 2/25 | Loss: 0.00068263
Iteration 3/25 | Loss: 0.00068263
Iteration 4/25 | Loss: 0.00068263
Iteration 5/25 | Loss: 0.00068263
Iteration 6/25 | Loss: 0.00068263
Iteration 7/25 | Loss: 0.00068263
Iteration 8/25 | Loss: 0.00068263
Iteration 9/25 | Loss: 0.00068263
Iteration 10/25 | Loss: 0.00068263
Iteration 11/25 | Loss: 0.00068263
Iteration 12/25 | Loss: 0.00068263
Iteration 13/25 | Loss: 0.00068263
Iteration 14/25 | Loss: 0.00068263
Iteration 15/25 | Loss: 0.00068263
Iteration 16/25 | Loss: 0.00068262
Iteration 17/25 | Loss: 0.00068262
Iteration 18/25 | Loss: 0.00068262
Iteration 19/25 | Loss: 0.00068262
Iteration 20/25 | Loss: 0.00068262
Iteration 21/25 | Loss: 0.00068262
Iteration 22/25 | Loss: 0.00068262
Iteration 23/25 | Loss: 0.00068262
Iteration 24/25 | Loss: 0.00068262
Iteration 25/25 | Loss: 0.00068262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068262
Iteration 2/1000 | Loss: 0.00004486
Iteration 3/1000 | Loss: 0.00002598
Iteration 4/1000 | Loss: 0.00001986
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001646
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001556
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001514
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001475
Iteration 14/1000 | Loss: 0.00001464
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001456
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001446
Iteration 20/1000 | Loss: 0.00001445
Iteration 21/1000 | Loss: 0.00001443
Iteration 22/1000 | Loss: 0.00001442
Iteration 23/1000 | Loss: 0.00001442
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001442
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001441
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001440
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001435
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00001434
Iteration 50/1000 | Loss: 0.00001434
Iteration 51/1000 | Loss: 0.00001434
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001433
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001433
Iteration 57/1000 | Loss: 0.00001433
Iteration 58/1000 | Loss: 0.00001433
Iteration 59/1000 | Loss: 0.00001433
Iteration 60/1000 | Loss: 0.00001433
Iteration 61/1000 | Loss: 0.00001433
Iteration 62/1000 | Loss: 0.00001432
Iteration 63/1000 | Loss: 0.00001432
Iteration 64/1000 | Loss: 0.00001432
Iteration 65/1000 | Loss: 0.00001432
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001431
Iteration 68/1000 | Loss: 0.00001431
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001430
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001428
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001425
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001425
Iteration 105/1000 | Loss: 0.00001425
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001425
Iteration 108/1000 | Loss: 0.00001425
Iteration 109/1000 | Loss: 0.00001424
Iteration 110/1000 | Loss: 0.00001424
Iteration 111/1000 | Loss: 0.00001424
Iteration 112/1000 | Loss: 0.00001424
Iteration 113/1000 | Loss: 0.00001424
Iteration 114/1000 | Loss: 0.00001424
Iteration 115/1000 | Loss: 0.00001424
Iteration 116/1000 | Loss: 0.00001424
Iteration 117/1000 | Loss: 0.00001424
Iteration 118/1000 | Loss: 0.00001424
Iteration 119/1000 | Loss: 0.00001424
Iteration 120/1000 | Loss: 0.00001424
Iteration 121/1000 | Loss: 0.00001424
Iteration 122/1000 | Loss: 0.00001424
Iteration 123/1000 | Loss: 0.00001424
Iteration 124/1000 | Loss: 0.00001424
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.4243112673284486e-05, 1.4243112673284486e-05, 1.4243112673284486e-05, 1.4243112673284486e-05, 1.4243112673284486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4243112673284486e-05

Optimization complete. Final v2v error: 3.1773598194122314 mm

Highest mean error: 3.9872429370880127 mm for frame 70

Lowest mean error: 2.5852482318878174 mm for frame 0

Saving results

Total time: 51.25323176383972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761649
Iteration 2/25 | Loss: 0.00163944
Iteration 3/25 | Loss: 0.00123315
Iteration 4/25 | Loss: 0.00119918
Iteration 5/25 | Loss: 0.00119414
Iteration 6/25 | Loss: 0.00119289
Iteration 7/25 | Loss: 0.00119289
Iteration 8/25 | Loss: 0.00119289
Iteration 9/25 | Loss: 0.00119289
Iteration 10/25 | Loss: 0.00119289
Iteration 11/25 | Loss: 0.00119289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011928853346034884, 0.0011928853346034884, 0.0011928853346034884, 0.0011928853346034884, 0.0011928853346034884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011928853346034884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27600670
Iteration 2/25 | Loss: 0.00057299
Iteration 3/25 | Loss: 0.00057299
Iteration 4/25 | Loss: 0.00057299
Iteration 5/25 | Loss: 0.00057298
Iteration 6/25 | Loss: 0.00057298
Iteration 7/25 | Loss: 0.00057298
Iteration 8/25 | Loss: 0.00057298
Iteration 9/25 | Loss: 0.00057298
Iteration 10/25 | Loss: 0.00057298
Iteration 11/25 | Loss: 0.00057298
Iteration 12/25 | Loss: 0.00057298
Iteration 13/25 | Loss: 0.00057298
Iteration 14/25 | Loss: 0.00057298
Iteration 15/25 | Loss: 0.00057298
Iteration 16/25 | Loss: 0.00057298
Iteration 17/25 | Loss: 0.00057298
Iteration 18/25 | Loss: 0.00057298
Iteration 19/25 | Loss: 0.00057298
Iteration 20/25 | Loss: 0.00057298
Iteration 21/25 | Loss: 0.00057298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005729832919314504, 0.0005729832919314504, 0.0005729832919314504, 0.0005729832919314504, 0.0005729832919314504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005729832919314504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057298
Iteration 2/1000 | Loss: 0.00004826
Iteration 3/1000 | Loss: 0.00003168
Iteration 4/1000 | Loss: 0.00002757
Iteration 5/1000 | Loss: 0.00002571
Iteration 6/1000 | Loss: 0.00002458
Iteration 7/1000 | Loss: 0.00002352
Iteration 8/1000 | Loss: 0.00002278
Iteration 9/1000 | Loss: 0.00002220
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002150
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002113
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002094
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00002081
Iteration 19/1000 | Loss: 0.00002078
Iteration 20/1000 | Loss: 0.00002072
Iteration 21/1000 | Loss: 0.00002071
Iteration 22/1000 | Loss: 0.00002071
Iteration 23/1000 | Loss: 0.00002067
Iteration 24/1000 | Loss: 0.00002067
Iteration 25/1000 | Loss: 0.00002066
Iteration 26/1000 | Loss: 0.00002064
Iteration 27/1000 | Loss: 0.00002063
Iteration 28/1000 | Loss: 0.00002062
Iteration 29/1000 | Loss: 0.00002062
Iteration 30/1000 | Loss: 0.00002062
Iteration 31/1000 | Loss: 0.00002062
Iteration 32/1000 | Loss: 0.00002062
Iteration 33/1000 | Loss: 0.00002062
Iteration 34/1000 | Loss: 0.00002061
Iteration 35/1000 | Loss: 0.00002061
Iteration 36/1000 | Loss: 0.00002060
Iteration 37/1000 | Loss: 0.00002060
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002058
Iteration 40/1000 | Loss: 0.00002058
Iteration 41/1000 | Loss: 0.00002058
Iteration 42/1000 | Loss: 0.00002058
Iteration 43/1000 | Loss: 0.00002057
Iteration 44/1000 | Loss: 0.00002057
Iteration 45/1000 | Loss: 0.00002057
Iteration 46/1000 | Loss: 0.00002057
Iteration 47/1000 | Loss: 0.00002057
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002056
Iteration 51/1000 | Loss: 0.00002056
Iteration 52/1000 | Loss: 0.00002056
Iteration 53/1000 | Loss: 0.00002055
Iteration 54/1000 | Loss: 0.00002054
Iteration 55/1000 | Loss: 0.00002054
Iteration 56/1000 | Loss: 0.00002054
Iteration 57/1000 | Loss: 0.00002054
Iteration 58/1000 | Loss: 0.00002054
Iteration 59/1000 | Loss: 0.00002054
Iteration 60/1000 | Loss: 0.00002054
Iteration 61/1000 | Loss: 0.00002054
Iteration 62/1000 | Loss: 0.00002054
Iteration 63/1000 | Loss: 0.00002054
Iteration 64/1000 | Loss: 0.00002054
Iteration 65/1000 | Loss: 0.00002053
Iteration 66/1000 | Loss: 0.00002053
Iteration 67/1000 | Loss: 0.00002053
Iteration 68/1000 | Loss: 0.00002052
Iteration 69/1000 | Loss: 0.00002052
Iteration 70/1000 | Loss: 0.00002051
Iteration 71/1000 | Loss: 0.00002051
Iteration 72/1000 | Loss: 0.00002051
Iteration 73/1000 | Loss: 0.00002051
Iteration 74/1000 | Loss: 0.00002051
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002050
Iteration 82/1000 | Loss: 0.00002050
Iteration 83/1000 | Loss: 0.00002050
Iteration 84/1000 | Loss: 0.00002049
Iteration 85/1000 | Loss: 0.00002049
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002046
Iteration 90/1000 | Loss: 0.00002046
Iteration 91/1000 | Loss: 0.00002046
Iteration 92/1000 | Loss: 0.00002046
Iteration 93/1000 | Loss: 0.00002045
Iteration 94/1000 | Loss: 0.00002045
Iteration 95/1000 | Loss: 0.00002045
Iteration 96/1000 | Loss: 0.00002045
Iteration 97/1000 | Loss: 0.00002045
Iteration 98/1000 | Loss: 0.00002045
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002044
Iteration 101/1000 | Loss: 0.00002043
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002041
Iteration 107/1000 | Loss: 0.00002041
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002040
Iteration 111/1000 | Loss: 0.00002040
Iteration 112/1000 | Loss: 0.00002040
Iteration 113/1000 | Loss: 0.00002040
Iteration 114/1000 | Loss: 0.00002040
Iteration 115/1000 | Loss: 0.00002039
Iteration 116/1000 | Loss: 0.00002039
Iteration 117/1000 | Loss: 0.00002039
Iteration 118/1000 | Loss: 0.00002039
Iteration 119/1000 | Loss: 0.00002039
Iteration 120/1000 | Loss: 0.00002038
Iteration 121/1000 | Loss: 0.00002038
Iteration 122/1000 | Loss: 0.00002038
Iteration 123/1000 | Loss: 0.00002038
Iteration 124/1000 | Loss: 0.00002037
Iteration 125/1000 | Loss: 0.00002037
Iteration 126/1000 | Loss: 0.00002037
Iteration 127/1000 | Loss: 0.00002036
Iteration 128/1000 | Loss: 0.00002036
Iteration 129/1000 | Loss: 0.00002036
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002035
Iteration 132/1000 | Loss: 0.00002035
Iteration 133/1000 | Loss: 0.00002035
Iteration 134/1000 | Loss: 0.00002035
Iteration 135/1000 | Loss: 0.00002035
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002033
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002033
Iteration 145/1000 | Loss: 0.00002033
Iteration 146/1000 | Loss: 0.00002033
Iteration 147/1000 | Loss: 0.00002032
Iteration 148/1000 | Loss: 0.00002032
Iteration 149/1000 | Loss: 0.00002032
Iteration 150/1000 | Loss: 0.00002032
Iteration 151/1000 | Loss: 0.00002032
Iteration 152/1000 | Loss: 0.00002032
Iteration 153/1000 | Loss: 0.00002032
Iteration 154/1000 | Loss: 0.00002031
Iteration 155/1000 | Loss: 0.00002031
Iteration 156/1000 | Loss: 0.00002031
Iteration 157/1000 | Loss: 0.00002031
Iteration 158/1000 | Loss: 0.00002030
Iteration 159/1000 | Loss: 0.00002030
Iteration 160/1000 | Loss: 0.00002030
Iteration 161/1000 | Loss: 0.00002030
Iteration 162/1000 | Loss: 0.00002029
Iteration 163/1000 | Loss: 0.00002029
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002028
Iteration 166/1000 | Loss: 0.00002028
Iteration 167/1000 | Loss: 0.00002028
Iteration 168/1000 | Loss: 0.00002028
Iteration 169/1000 | Loss: 0.00002028
Iteration 170/1000 | Loss: 0.00002027
Iteration 171/1000 | Loss: 0.00002027
Iteration 172/1000 | Loss: 0.00002027
Iteration 173/1000 | Loss: 0.00002027
Iteration 174/1000 | Loss: 0.00002027
Iteration 175/1000 | Loss: 0.00002026
Iteration 176/1000 | Loss: 0.00002026
Iteration 177/1000 | Loss: 0.00002026
Iteration 178/1000 | Loss: 0.00002026
Iteration 179/1000 | Loss: 0.00002026
Iteration 180/1000 | Loss: 0.00002026
Iteration 181/1000 | Loss: 0.00002026
Iteration 182/1000 | Loss: 0.00002026
Iteration 183/1000 | Loss: 0.00002026
Iteration 184/1000 | Loss: 0.00002026
Iteration 185/1000 | Loss: 0.00002026
Iteration 186/1000 | Loss: 0.00002026
Iteration 187/1000 | Loss: 0.00002026
Iteration 188/1000 | Loss: 0.00002026
Iteration 189/1000 | Loss: 0.00002026
Iteration 190/1000 | Loss: 0.00002026
Iteration 191/1000 | Loss: 0.00002026
Iteration 192/1000 | Loss: 0.00002026
Iteration 193/1000 | Loss: 0.00002026
Iteration 194/1000 | Loss: 0.00002026
Iteration 195/1000 | Loss: 0.00002026
Iteration 196/1000 | Loss: 0.00002026
Iteration 197/1000 | Loss: 0.00002026
Iteration 198/1000 | Loss: 0.00002026
Iteration 199/1000 | Loss: 0.00002026
Iteration 200/1000 | Loss: 0.00002026
Iteration 201/1000 | Loss: 0.00002026
Iteration 202/1000 | Loss: 0.00002026
Iteration 203/1000 | Loss: 0.00002026
Iteration 204/1000 | Loss: 0.00002026
Iteration 205/1000 | Loss: 0.00002026
Iteration 206/1000 | Loss: 0.00002026
Iteration 207/1000 | Loss: 0.00002026
Iteration 208/1000 | Loss: 0.00002026
Iteration 209/1000 | Loss: 0.00002026
Iteration 210/1000 | Loss: 0.00002026
Iteration 211/1000 | Loss: 0.00002026
Iteration 212/1000 | Loss: 0.00002026
Iteration 213/1000 | Loss: 0.00002026
Iteration 214/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.0262739781173877e-05, 2.0262739781173877e-05, 2.0262739781173877e-05, 2.0262739781173877e-05, 2.0262739781173877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0262739781173877e-05

Optimization complete. Final v2v error: 3.738034963607788 mm

Highest mean error: 4.524580478668213 mm for frame 161

Lowest mean error: 3.211482524871826 mm for frame 181

Saving results

Total time: 44.45537877082825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765970
Iteration 2/25 | Loss: 0.00133082
Iteration 3/25 | Loss: 0.00118877
Iteration 4/25 | Loss: 0.00117203
Iteration 5/25 | Loss: 0.00116706
Iteration 6/25 | Loss: 0.00116573
Iteration 7/25 | Loss: 0.00116573
Iteration 8/25 | Loss: 0.00116573
Iteration 9/25 | Loss: 0.00116573
Iteration 10/25 | Loss: 0.00116573
Iteration 11/25 | Loss: 0.00116573
Iteration 12/25 | Loss: 0.00116573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011657297145575285, 0.0011657297145575285, 0.0011657297145575285, 0.0011657297145575285, 0.0011657297145575285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011657297145575285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41814160
Iteration 2/25 | Loss: 0.00070569
Iteration 3/25 | Loss: 0.00070569
Iteration 4/25 | Loss: 0.00070569
Iteration 5/25 | Loss: 0.00070569
Iteration 6/25 | Loss: 0.00070569
Iteration 7/25 | Loss: 0.00070569
Iteration 8/25 | Loss: 0.00070569
Iteration 9/25 | Loss: 0.00070569
Iteration 10/25 | Loss: 0.00070569
Iteration 11/25 | Loss: 0.00070569
Iteration 12/25 | Loss: 0.00070569
Iteration 13/25 | Loss: 0.00070569
Iteration 14/25 | Loss: 0.00070569
Iteration 15/25 | Loss: 0.00070569
Iteration 16/25 | Loss: 0.00070569
Iteration 17/25 | Loss: 0.00070569
Iteration 18/25 | Loss: 0.00070569
Iteration 19/25 | Loss: 0.00070569
Iteration 20/25 | Loss: 0.00070569
Iteration 21/25 | Loss: 0.00070569
Iteration 22/25 | Loss: 0.00070569
Iteration 23/25 | Loss: 0.00070569
Iteration 24/25 | Loss: 0.00070569
Iteration 25/25 | Loss: 0.00070569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070569
Iteration 2/1000 | Loss: 0.00004384
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00002001
Iteration 5/1000 | Loss: 0.00001877
Iteration 6/1000 | Loss: 0.00001803
Iteration 7/1000 | Loss: 0.00001749
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001683
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001667
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001660
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001651
Iteration 22/1000 | Loss: 0.00001651
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001650
Iteration 25/1000 | Loss: 0.00001648
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001645
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001644
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001642
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001639
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001639
Iteration 41/1000 | Loss: 0.00001638
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001631
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001628
Iteration 57/1000 | Loss: 0.00001628
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001618
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001615
Iteration 107/1000 | Loss: 0.00001615
Iteration 108/1000 | Loss: 0.00001615
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001614
Iteration 113/1000 | Loss: 0.00001614
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001613
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001613
Iteration 118/1000 | Loss: 0.00001613
Iteration 119/1000 | Loss: 0.00001612
Iteration 120/1000 | Loss: 0.00001612
Iteration 121/1000 | Loss: 0.00001612
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001612
Iteration 124/1000 | Loss: 0.00001611
Iteration 125/1000 | Loss: 0.00001611
Iteration 126/1000 | Loss: 0.00001611
Iteration 127/1000 | Loss: 0.00001611
Iteration 128/1000 | Loss: 0.00001611
Iteration 129/1000 | Loss: 0.00001611
Iteration 130/1000 | Loss: 0.00001611
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001610
Iteration 133/1000 | Loss: 0.00001610
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001609
Iteration 138/1000 | Loss: 0.00001609
Iteration 139/1000 | Loss: 0.00001609
Iteration 140/1000 | Loss: 0.00001609
Iteration 141/1000 | Loss: 0.00001609
Iteration 142/1000 | Loss: 0.00001609
Iteration 143/1000 | Loss: 0.00001608
Iteration 144/1000 | Loss: 0.00001608
Iteration 145/1000 | Loss: 0.00001608
Iteration 146/1000 | Loss: 0.00001608
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001607
Iteration 150/1000 | Loss: 0.00001607
Iteration 151/1000 | Loss: 0.00001607
Iteration 152/1000 | Loss: 0.00001607
Iteration 153/1000 | Loss: 0.00001607
Iteration 154/1000 | Loss: 0.00001607
Iteration 155/1000 | Loss: 0.00001607
Iteration 156/1000 | Loss: 0.00001607
Iteration 157/1000 | Loss: 0.00001607
Iteration 158/1000 | Loss: 0.00001607
Iteration 159/1000 | Loss: 0.00001607
Iteration 160/1000 | Loss: 0.00001606
Iteration 161/1000 | Loss: 0.00001606
Iteration 162/1000 | Loss: 0.00001606
Iteration 163/1000 | Loss: 0.00001606
Iteration 164/1000 | Loss: 0.00001606
Iteration 165/1000 | Loss: 0.00001606
Iteration 166/1000 | Loss: 0.00001606
Iteration 167/1000 | Loss: 0.00001606
Iteration 168/1000 | Loss: 0.00001606
Iteration 169/1000 | Loss: 0.00001606
Iteration 170/1000 | Loss: 0.00001606
Iteration 171/1000 | Loss: 0.00001606
Iteration 172/1000 | Loss: 0.00001606
Iteration 173/1000 | Loss: 0.00001606
Iteration 174/1000 | Loss: 0.00001606
Iteration 175/1000 | Loss: 0.00001606
Iteration 176/1000 | Loss: 0.00001606
Iteration 177/1000 | Loss: 0.00001606
Iteration 178/1000 | Loss: 0.00001606
Iteration 179/1000 | Loss: 0.00001606
Iteration 180/1000 | Loss: 0.00001606
Iteration 181/1000 | Loss: 0.00001606
Iteration 182/1000 | Loss: 0.00001606
Iteration 183/1000 | Loss: 0.00001606
Iteration 184/1000 | Loss: 0.00001606
Iteration 185/1000 | Loss: 0.00001606
Iteration 186/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.605650868441444e-05, 1.605650868441444e-05, 1.605650868441444e-05, 1.605650868441444e-05, 1.605650868441444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.605650868441444e-05

Optimization complete. Final v2v error: 3.346057176589966 mm

Highest mean error: 4.223572731018066 mm for frame 142

Lowest mean error: 2.9347920417785645 mm for frame 114

Saving results

Total time: 44.274330139160156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582361
Iteration 2/25 | Loss: 0.00114921
Iteration 3/25 | Loss: 0.00107651
Iteration 4/25 | Loss: 0.00106498
Iteration 5/25 | Loss: 0.00106128
Iteration 6/25 | Loss: 0.00106067
Iteration 7/25 | Loss: 0.00106067
Iteration 8/25 | Loss: 0.00106067
Iteration 9/25 | Loss: 0.00106067
Iteration 10/25 | Loss: 0.00106067
Iteration 11/25 | Loss: 0.00106067
Iteration 12/25 | Loss: 0.00106067
Iteration 13/25 | Loss: 0.00106067
Iteration 14/25 | Loss: 0.00106067
Iteration 15/25 | Loss: 0.00106067
Iteration 16/25 | Loss: 0.00106067
Iteration 17/25 | Loss: 0.00106067
Iteration 18/25 | Loss: 0.00106067
Iteration 19/25 | Loss: 0.00106067
Iteration 20/25 | Loss: 0.00106067
Iteration 21/25 | Loss: 0.00106067
Iteration 22/25 | Loss: 0.00106067
Iteration 23/25 | Loss: 0.00106067
Iteration 24/25 | Loss: 0.00106067
Iteration 25/25 | Loss: 0.00106067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06769824
Iteration 2/25 | Loss: 0.00064261
Iteration 3/25 | Loss: 0.00064261
Iteration 4/25 | Loss: 0.00064261
Iteration 5/25 | Loss: 0.00064261
Iteration 6/25 | Loss: 0.00064261
Iteration 7/25 | Loss: 0.00064261
Iteration 8/25 | Loss: 0.00064261
Iteration 9/25 | Loss: 0.00064261
Iteration 10/25 | Loss: 0.00064261
Iteration 11/25 | Loss: 0.00064261
Iteration 12/25 | Loss: 0.00064261
Iteration 13/25 | Loss: 0.00064261
Iteration 14/25 | Loss: 0.00064261
Iteration 15/25 | Loss: 0.00064261
Iteration 16/25 | Loss: 0.00064261
Iteration 17/25 | Loss: 0.00064261
Iteration 18/25 | Loss: 0.00064261
Iteration 19/25 | Loss: 0.00064261
Iteration 20/25 | Loss: 0.00064261
Iteration 21/25 | Loss: 0.00064261
Iteration 22/25 | Loss: 0.00064261
Iteration 23/25 | Loss: 0.00064261
Iteration 24/25 | Loss: 0.00064261
Iteration 25/25 | Loss: 0.00064261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064261
Iteration 2/1000 | Loss: 0.00002316
Iteration 3/1000 | Loss: 0.00001525
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001190
Iteration 6/1000 | Loss: 0.00001134
Iteration 7/1000 | Loss: 0.00001091
Iteration 8/1000 | Loss: 0.00001061
Iteration 9/1000 | Loss: 0.00001059
Iteration 10/1000 | Loss: 0.00001059
Iteration 11/1000 | Loss: 0.00001057
Iteration 12/1000 | Loss: 0.00001056
Iteration 13/1000 | Loss: 0.00001053
Iteration 14/1000 | Loss: 0.00001050
Iteration 15/1000 | Loss: 0.00001050
Iteration 16/1000 | Loss: 0.00001050
Iteration 17/1000 | Loss: 0.00001049
Iteration 18/1000 | Loss: 0.00001049
Iteration 19/1000 | Loss: 0.00001047
Iteration 20/1000 | Loss: 0.00001047
Iteration 21/1000 | Loss: 0.00001047
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001040
Iteration 24/1000 | Loss: 0.00001024
Iteration 25/1000 | Loss: 0.00001023
Iteration 26/1000 | Loss: 0.00001023
Iteration 27/1000 | Loss: 0.00001021
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001010
Iteration 31/1000 | Loss: 0.00001006
Iteration 32/1000 | Loss: 0.00001005
Iteration 33/1000 | Loss: 0.00001004
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001004
Iteration 36/1000 | Loss: 0.00001001
Iteration 37/1000 | Loss: 0.00001000
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000999
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000997
Iteration 42/1000 | Loss: 0.00000995
Iteration 43/1000 | Loss: 0.00000995
Iteration 44/1000 | Loss: 0.00000994
Iteration 45/1000 | Loss: 0.00000994
Iteration 46/1000 | Loss: 0.00000993
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000992
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000990
Iteration 52/1000 | Loss: 0.00000990
Iteration 53/1000 | Loss: 0.00000990
Iteration 54/1000 | Loss: 0.00000989
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000987
Iteration 57/1000 | Loss: 0.00000987
Iteration 58/1000 | Loss: 0.00000987
Iteration 59/1000 | Loss: 0.00000986
Iteration 60/1000 | Loss: 0.00000986
Iteration 61/1000 | Loss: 0.00000986
Iteration 62/1000 | Loss: 0.00000985
Iteration 63/1000 | Loss: 0.00000983
Iteration 64/1000 | Loss: 0.00000982
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000980
Iteration 68/1000 | Loss: 0.00000980
Iteration 69/1000 | Loss: 0.00000980
Iteration 70/1000 | Loss: 0.00000980
Iteration 71/1000 | Loss: 0.00000980
Iteration 72/1000 | Loss: 0.00000980
Iteration 73/1000 | Loss: 0.00000979
Iteration 74/1000 | Loss: 0.00000979
Iteration 75/1000 | Loss: 0.00000979
Iteration 76/1000 | Loss: 0.00000979
Iteration 77/1000 | Loss: 0.00000978
Iteration 78/1000 | Loss: 0.00000978
Iteration 79/1000 | Loss: 0.00000977
Iteration 80/1000 | Loss: 0.00000977
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000976
Iteration 84/1000 | Loss: 0.00000976
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000975
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000974
Iteration 94/1000 | Loss: 0.00000974
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000973
Iteration 97/1000 | Loss: 0.00000973
Iteration 98/1000 | Loss: 0.00000973
Iteration 99/1000 | Loss: 0.00000973
Iteration 100/1000 | Loss: 0.00000973
Iteration 101/1000 | Loss: 0.00000972
Iteration 102/1000 | Loss: 0.00000972
Iteration 103/1000 | Loss: 0.00000972
Iteration 104/1000 | Loss: 0.00000972
Iteration 105/1000 | Loss: 0.00000971
Iteration 106/1000 | Loss: 0.00000971
Iteration 107/1000 | Loss: 0.00000970
Iteration 108/1000 | Loss: 0.00000970
Iteration 109/1000 | Loss: 0.00000970
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000968
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000968
Iteration 116/1000 | Loss: 0.00000968
Iteration 117/1000 | Loss: 0.00000968
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000967
Iteration 121/1000 | Loss: 0.00000966
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000965
Iteration 126/1000 | Loss: 0.00000965
Iteration 127/1000 | Loss: 0.00000965
Iteration 128/1000 | Loss: 0.00000965
Iteration 129/1000 | Loss: 0.00000965
Iteration 130/1000 | Loss: 0.00000965
Iteration 131/1000 | Loss: 0.00000965
Iteration 132/1000 | Loss: 0.00000965
Iteration 133/1000 | Loss: 0.00000965
Iteration 134/1000 | Loss: 0.00000964
Iteration 135/1000 | Loss: 0.00000964
Iteration 136/1000 | Loss: 0.00000964
Iteration 137/1000 | Loss: 0.00000964
Iteration 138/1000 | Loss: 0.00000964
Iteration 139/1000 | Loss: 0.00000964
Iteration 140/1000 | Loss: 0.00000964
Iteration 141/1000 | Loss: 0.00000963
Iteration 142/1000 | Loss: 0.00000963
Iteration 143/1000 | Loss: 0.00000963
Iteration 144/1000 | Loss: 0.00000963
Iteration 145/1000 | Loss: 0.00000963
Iteration 146/1000 | Loss: 0.00000963
Iteration 147/1000 | Loss: 0.00000963
Iteration 148/1000 | Loss: 0.00000963
Iteration 149/1000 | Loss: 0.00000963
Iteration 150/1000 | Loss: 0.00000962
Iteration 151/1000 | Loss: 0.00000962
Iteration 152/1000 | Loss: 0.00000962
Iteration 153/1000 | Loss: 0.00000962
Iteration 154/1000 | Loss: 0.00000962
Iteration 155/1000 | Loss: 0.00000962
Iteration 156/1000 | Loss: 0.00000962
Iteration 157/1000 | Loss: 0.00000962
Iteration 158/1000 | Loss: 0.00000962
Iteration 159/1000 | Loss: 0.00000962
Iteration 160/1000 | Loss: 0.00000962
Iteration 161/1000 | Loss: 0.00000962
Iteration 162/1000 | Loss: 0.00000962
Iteration 163/1000 | Loss: 0.00000962
Iteration 164/1000 | Loss: 0.00000962
Iteration 165/1000 | Loss: 0.00000962
Iteration 166/1000 | Loss: 0.00000962
Iteration 167/1000 | Loss: 0.00000962
Iteration 168/1000 | Loss: 0.00000962
Iteration 169/1000 | Loss: 0.00000961
Iteration 170/1000 | Loss: 0.00000961
Iteration 171/1000 | Loss: 0.00000961
Iteration 172/1000 | Loss: 0.00000961
Iteration 173/1000 | Loss: 0.00000961
Iteration 174/1000 | Loss: 0.00000961
Iteration 175/1000 | Loss: 0.00000961
Iteration 176/1000 | Loss: 0.00000961
Iteration 177/1000 | Loss: 0.00000961
Iteration 178/1000 | Loss: 0.00000961
Iteration 179/1000 | Loss: 0.00000961
Iteration 180/1000 | Loss: 0.00000961
Iteration 181/1000 | Loss: 0.00000961
Iteration 182/1000 | Loss: 0.00000961
Iteration 183/1000 | Loss: 0.00000960
Iteration 184/1000 | Loss: 0.00000960
Iteration 185/1000 | Loss: 0.00000960
Iteration 186/1000 | Loss: 0.00000960
Iteration 187/1000 | Loss: 0.00000960
Iteration 188/1000 | Loss: 0.00000960
Iteration 189/1000 | Loss: 0.00000960
Iteration 190/1000 | Loss: 0.00000960
Iteration 191/1000 | Loss: 0.00000960
Iteration 192/1000 | Loss: 0.00000960
Iteration 193/1000 | Loss: 0.00000960
Iteration 194/1000 | Loss: 0.00000960
Iteration 195/1000 | Loss: 0.00000960
Iteration 196/1000 | Loss: 0.00000960
Iteration 197/1000 | Loss: 0.00000960
Iteration 198/1000 | Loss: 0.00000960
Iteration 199/1000 | Loss: 0.00000960
Iteration 200/1000 | Loss: 0.00000960
Iteration 201/1000 | Loss: 0.00000960
Iteration 202/1000 | Loss: 0.00000960
Iteration 203/1000 | Loss: 0.00000960
Iteration 204/1000 | Loss: 0.00000960
Iteration 205/1000 | Loss: 0.00000960
Iteration 206/1000 | Loss: 0.00000960
Iteration 207/1000 | Loss: 0.00000960
Iteration 208/1000 | Loss: 0.00000960
Iteration 209/1000 | Loss: 0.00000960
Iteration 210/1000 | Loss: 0.00000960
Iteration 211/1000 | Loss: 0.00000960
Iteration 212/1000 | Loss: 0.00000960
Iteration 213/1000 | Loss: 0.00000960
Iteration 214/1000 | Loss: 0.00000960
Iteration 215/1000 | Loss: 0.00000960
Iteration 216/1000 | Loss: 0.00000960
Iteration 217/1000 | Loss: 0.00000960
Iteration 218/1000 | Loss: 0.00000960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [9.596453310223296e-06, 9.596453310223296e-06, 9.596453310223296e-06, 9.596453310223296e-06, 9.596453310223296e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.596453310223296e-06

Optimization complete. Final v2v error: 2.6640090942382812 mm

Highest mean error: 2.9867851734161377 mm for frame 96

Lowest mean error: 2.491905927658081 mm for frame 29

Saving results

Total time: 39.0348117351532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988986
Iteration 2/25 | Loss: 0.00174838
Iteration 3/25 | Loss: 0.00134135
Iteration 4/25 | Loss: 0.00129970
Iteration 5/25 | Loss: 0.00129305
Iteration 6/25 | Loss: 0.00129235
Iteration 7/25 | Loss: 0.00129235
Iteration 8/25 | Loss: 0.00129235
Iteration 9/25 | Loss: 0.00129235
Iteration 10/25 | Loss: 0.00129235
Iteration 11/25 | Loss: 0.00129235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012923487229272723, 0.0012923487229272723, 0.0012923487229272723, 0.0012923487229272723, 0.0012923487229272723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012923487229272723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.74255085
Iteration 2/25 | Loss: 0.00062042
Iteration 3/25 | Loss: 0.00062037
Iteration 4/25 | Loss: 0.00062037
Iteration 5/25 | Loss: 0.00062037
Iteration 6/25 | Loss: 0.00062037
Iteration 7/25 | Loss: 0.00062037
Iteration 8/25 | Loss: 0.00062037
Iteration 9/25 | Loss: 0.00062037
Iteration 10/25 | Loss: 0.00062037
Iteration 11/25 | Loss: 0.00062037
Iteration 12/25 | Loss: 0.00062037
Iteration 13/25 | Loss: 0.00062037
Iteration 14/25 | Loss: 0.00062037
Iteration 15/25 | Loss: 0.00062037
Iteration 16/25 | Loss: 0.00062037
Iteration 17/25 | Loss: 0.00062037
Iteration 18/25 | Loss: 0.00062037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006203672965057194, 0.0006203672965057194, 0.0006203672965057194, 0.0006203672965057194, 0.0006203672965057194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006203672965057194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062037
Iteration 2/1000 | Loss: 0.00007030
Iteration 3/1000 | Loss: 0.00004168
Iteration 4/1000 | Loss: 0.00003721
Iteration 5/1000 | Loss: 0.00003558
Iteration 6/1000 | Loss: 0.00003475
Iteration 7/1000 | Loss: 0.00003385
Iteration 8/1000 | Loss: 0.00003342
Iteration 9/1000 | Loss: 0.00003296
Iteration 10/1000 | Loss: 0.00003259
Iteration 11/1000 | Loss: 0.00003234
Iteration 12/1000 | Loss: 0.00003211
Iteration 13/1000 | Loss: 0.00003206
Iteration 14/1000 | Loss: 0.00003187
Iteration 15/1000 | Loss: 0.00003171
Iteration 16/1000 | Loss: 0.00003168
Iteration 17/1000 | Loss: 0.00003163
Iteration 18/1000 | Loss: 0.00003157
Iteration 19/1000 | Loss: 0.00003153
Iteration 20/1000 | Loss: 0.00003147
Iteration 21/1000 | Loss: 0.00003145
Iteration 22/1000 | Loss: 0.00003141
Iteration 23/1000 | Loss: 0.00003137
Iteration 24/1000 | Loss: 0.00003137
Iteration 25/1000 | Loss: 0.00003134
Iteration 26/1000 | Loss: 0.00003122
Iteration 27/1000 | Loss: 0.00003121
Iteration 28/1000 | Loss: 0.00003119
Iteration 29/1000 | Loss: 0.00003119
Iteration 30/1000 | Loss: 0.00003119
Iteration 31/1000 | Loss: 0.00003118
Iteration 32/1000 | Loss: 0.00003118
Iteration 33/1000 | Loss: 0.00003118
Iteration 34/1000 | Loss: 0.00003117
Iteration 35/1000 | Loss: 0.00003114
Iteration 36/1000 | Loss: 0.00003113
Iteration 37/1000 | Loss: 0.00003112
Iteration 38/1000 | Loss: 0.00003112
Iteration 39/1000 | Loss: 0.00003111
Iteration 40/1000 | Loss: 0.00003110
Iteration 41/1000 | Loss: 0.00003110
Iteration 42/1000 | Loss: 0.00003110
Iteration 43/1000 | Loss: 0.00003110
Iteration 44/1000 | Loss: 0.00003110
Iteration 45/1000 | Loss: 0.00003110
Iteration 46/1000 | Loss: 0.00003109
Iteration 47/1000 | Loss: 0.00003109
Iteration 48/1000 | Loss: 0.00003109
Iteration 49/1000 | Loss: 0.00003109
Iteration 50/1000 | Loss: 0.00003109
Iteration 51/1000 | Loss: 0.00003109
Iteration 52/1000 | Loss: 0.00003109
Iteration 53/1000 | Loss: 0.00003109
Iteration 54/1000 | Loss: 0.00003108
Iteration 55/1000 | Loss: 0.00003107
Iteration 56/1000 | Loss: 0.00003107
Iteration 57/1000 | Loss: 0.00003106
Iteration 58/1000 | Loss: 0.00003106
Iteration 59/1000 | Loss: 0.00003106
Iteration 60/1000 | Loss: 0.00003105
Iteration 61/1000 | Loss: 0.00003105
Iteration 62/1000 | Loss: 0.00003105
Iteration 63/1000 | Loss: 0.00003104
Iteration 64/1000 | Loss: 0.00003104
Iteration 65/1000 | Loss: 0.00003103
Iteration 66/1000 | Loss: 0.00003103
Iteration 67/1000 | Loss: 0.00003103
Iteration 68/1000 | Loss: 0.00003103
Iteration 69/1000 | Loss: 0.00003103
Iteration 70/1000 | Loss: 0.00003103
Iteration 71/1000 | Loss: 0.00003103
Iteration 72/1000 | Loss: 0.00003103
Iteration 73/1000 | Loss: 0.00003102
Iteration 74/1000 | Loss: 0.00003102
Iteration 75/1000 | Loss: 0.00003102
Iteration 76/1000 | Loss: 0.00003102
Iteration 77/1000 | Loss: 0.00003102
Iteration 78/1000 | Loss: 0.00003101
Iteration 79/1000 | Loss: 0.00003101
Iteration 80/1000 | Loss: 0.00003101
Iteration 81/1000 | Loss: 0.00003100
Iteration 82/1000 | Loss: 0.00003100
Iteration 83/1000 | Loss: 0.00003100
Iteration 84/1000 | Loss: 0.00003100
Iteration 85/1000 | Loss: 0.00003100
Iteration 86/1000 | Loss: 0.00003100
Iteration 87/1000 | Loss: 0.00003100
Iteration 88/1000 | Loss: 0.00003100
Iteration 89/1000 | Loss: 0.00003100
Iteration 90/1000 | Loss: 0.00003100
Iteration 91/1000 | Loss: 0.00003100
Iteration 92/1000 | Loss: 0.00003100
Iteration 93/1000 | Loss: 0.00003100
Iteration 94/1000 | Loss: 0.00003100
Iteration 95/1000 | Loss: 0.00003100
Iteration 96/1000 | Loss: 0.00003100
Iteration 97/1000 | Loss: 0.00003100
Iteration 98/1000 | Loss: 0.00003100
Iteration 99/1000 | Loss: 0.00003100
Iteration 100/1000 | Loss: 0.00003100
Iteration 101/1000 | Loss: 0.00003100
Iteration 102/1000 | Loss: 0.00003100
Iteration 103/1000 | Loss: 0.00003100
Iteration 104/1000 | Loss: 0.00003100
Iteration 105/1000 | Loss: 0.00003100
Iteration 106/1000 | Loss: 0.00003100
Iteration 107/1000 | Loss: 0.00003100
Iteration 108/1000 | Loss: 0.00003100
Iteration 109/1000 | Loss: 0.00003100
Iteration 110/1000 | Loss: 0.00003100
Iteration 111/1000 | Loss: 0.00003100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [3.099767491221428e-05, 3.099767491221428e-05, 3.099767491221428e-05, 3.099767491221428e-05, 3.099767491221428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.099767491221428e-05

Optimization complete. Final v2v error: 4.511955261230469 mm

Highest mean error: 5.125758647918701 mm for frame 81

Lowest mean error: 3.8719890117645264 mm for frame 157

Saving results

Total time: 45.28145480155945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829113
Iteration 2/25 | Loss: 0.00122237
Iteration 3/25 | Loss: 0.00108950
Iteration 4/25 | Loss: 0.00107327
Iteration 5/25 | Loss: 0.00106880
Iteration 6/25 | Loss: 0.00106743
Iteration 7/25 | Loss: 0.00106705
Iteration 8/25 | Loss: 0.00106705
Iteration 9/25 | Loss: 0.00106705
Iteration 10/25 | Loss: 0.00106705
Iteration 11/25 | Loss: 0.00106705
Iteration 12/25 | Loss: 0.00106705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001067045726813376, 0.001067045726813376, 0.001067045726813376, 0.001067045726813376, 0.001067045726813376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001067045726813376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43394470
Iteration 2/25 | Loss: 0.00064916
Iteration 3/25 | Loss: 0.00064916
Iteration 4/25 | Loss: 0.00064916
Iteration 5/25 | Loss: 0.00064916
Iteration 6/25 | Loss: 0.00064916
Iteration 7/25 | Loss: 0.00064916
Iteration 8/25 | Loss: 0.00064916
Iteration 9/25 | Loss: 0.00064916
Iteration 10/25 | Loss: 0.00064916
Iteration 11/25 | Loss: 0.00064916
Iteration 12/25 | Loss: 0.00064916
Iteration 13/25 | Loss: 0.00064916
Iteration 14/25 | Loss: 0.00064916
Iteration 15/25 | Loss: 0.00064916
Iteration 16/25 | Loss: 0.00064916
Iteration 17/25 | Loss: 0.00064916
Iteration 18/25 | Loss: 0.00064916
Iteration 19/25 | Loss: 0.00064916
Iteration 20/25 | Loss: 0.00064916
Iteration 21/25 | Loss: 0.00064916
Iteration 22/25 | Loss: 0.00064916
Iteration 23/25 | Loss: 0.00064916
Iteration 24/25 | Loss: 0.00064916
Iteration 25/25 | Loss: 0.00064916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000649155699647963, 0.000649155699647963, 0.000649155699647963, 0.000649155699647963, 0.000649155699647963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000649155699647963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064916
Iteration 2/1000 | Loss: 0.00002528
Iteration 3/1000 | Loss: 0.00001566
Iteration 4/1000 | Loss: 0.00001271
Iteration 5/1000 | Loss: 0.00001163
Iteration 6/1000 | Loss: 0.00001091
Iteration 7/1000 | Loss: 0.00001043
Iteration 8/1000 | Loss: 0.00001015
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00001001
Iteration 11/1000 | Loss: 0.00000978
Iteration 12/1000 | Loss: 0.00000967
Iteration 13/1000 | Loss: 0.00000966
Iteration 14/1000 | Loss: 0.00000964
Iteration 15/1000 | Loss: 0.00000963
Iteration 16/1000 | Loss: 0.00000959
Iteration 17/1000 | Loss: 0.00000954
Iteration 18/1000 | Loss: 0.00000954
Iteration 19/1000 | Loss: 0.00000953
Iteration 20/1000 | Loss: 0.00000953
Iteration 21/1000 | Loss: 0.00000953
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000951
Iteration 24/1000 | Loss: 0.00000950
Iteration 25/1000 | Loss: 0.00000949
Iteration 26/1000 | Loss: 0.00000949
Iteration 27/1000 | Loss: 0.00000948
Iteration 28/1000 | Loss: 0.00000948
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000948
Iteration 32/1000 | Loss: 0.00000948
Iteration 33/1000 | Loss: 0.00000947
Iteration 34/1000 | Loss: 0.00000947
Iteration 35/1000 | Loss: 0.00000947
Iteration 36/1000 | Loss: 0.00000946
Iteration 37/1000 | Loss: 0.00000946
Iteration 38/1000 | Loss: 0.00000946
Iteration 39/1000 | Loss: 0.00000946
Iteration 40/1000 | Loss: 0.00000946
Iteration 41/1000 | Loss: 0.00000945
Iteration 42/1000 | Loss: 0.00000945
Iteration 43/1000 | Loss: 0.00000945
Iteration 44/1000 | Loss: 0.00000945
Iteration 45/1000 | Loss: 0.00000944
Iteration 46/1000 | Loss: 0.00000944
Iteration 47/1000 | Loss: 0.00000943
Iteration 48/1000 | Loss: 0.00000943
Iteration 49/1000 | Loss: 0.00000943
Iteration 50/1000 | Loss: 0.00000943
Iteration 51/1000 | Loss: 0.00000942
Iteration 52/1000 | Loss: 0.00000942
Iteration 53/1000 | Loss: 0.00000942
Iteration 54/1000 | Loss: 0.00000941
Iteration 55/1000 | Loss: 0.00000941
Iteration 56/1000 | Loss: 0.00000941
Iteration 57/1000 | Loss: 0.00000941
Iteration 58/1000 | Loss: 0.00000941
Iteration 59/1000 | Loss: 0.00000941
Iteration 60/1000 | Loss: 0.00000941
Iteration 61/1000 | Loss: 0.00000940
Iteration 62/1000 | Loss: 0.00000940
Iteration 63/1000 | Loss: 0.00000940
Iteration 64/1000 | Loss: 0.00000940
Iteration 65/1000 | Loss: 0.00000940
Iteration 66/1000 | Loss: 0.00000939
Iteration 67/1000 | Loss: 0.00000939
Iteration 68/1000 | Loss: 0.00000939
Iteration 69/1000 | Loss: 0.00000939
Iteration 70/1000 | Loss: 0.00000939
Iteration 71/1000 | Loss: 0.00000938
Iteration 72/1000 | Loss: 0.00000938
Iteration 73/1000 | Loss: 0.00000938
Iteration 74/1000 | Loss: 0.00000938
Iteration 75/1000 | Loss: 0.00000938
Iteration 76/1000 | Loss: 0.00000938
Iteration 77/1000 | Loss: 0.00000938
Iteration 78/1000 | Loss: 0.00000938
Iteration 79/1000 | Loss: 0.00000938
Iteration 80/1000 | Loss: 0.00000937
Iteration 81/1000 | Loss: 0.00000937
Iteration 82/1000 | Loss: 0.00000936
Iteration 83/1000 | Loss: 0.00000936
Iteration 84/1000 | Loss: 0.00000936
Iteration 85/1000 | Loss: 0.00000936
Iteration 86/1000 | Loss: 0.00000935
Iteration 87/1000 | Loss: 0.00000935
Iteration 88/1000 | Loss: 0.00000935
Iteration 89/1000 | Loss: 0.00000935
Iteration 90/1000 | Loss: 0.00000935
Iteration 91/1000 | Loss: 0.00000934
Iteration 92/1000 | Loss: 0.00000934
Iteration 93/1000 | Loss: 0.00000934
Iteration 94/1000 | Loss: 0.00000934
Iteration 95/1000 | Loss: 0.00000934
Iteration 96/1000 | Loss: 0.00000934
Iteration 97/1000 | Loss: 0.00000934
Iteration 98/1000 | Loss: 0.00000934
Iteration 99/1000 | Loss: 0.00000934
Iteration 100/1000 | Loss: 0.00000934
Iteration 101/1000 | Loss: 0.00000933
Iteration 102/1000 | Loss: 0.00000933
Iteration 103/1000 | Loss: 0.00000933
Iteration 104/1000 | Loss: 0.00000932
Iteration 105/1000 | Loss: 0.00000932
Iteration 106/1000 | Loss: 0.00000932
Iteration 107/1000 | Loss: 0.00000932
Iteration 108/1000 | Loss: 0.00000932
Iteration 109/1000 | Loss: 0.00000931
Iteration 110/1000 | Loss: 0.00000931
Iteration 111/1000 | Loss: 0.00000931
Iteration 112/1000 | Loss: 0.00000931
Iteration 113/1000 | Loss: 0.00000931
Iteration 114/1000 | Loss: 0.00000930
Iteration 115/1000 | Loss: 0.00000930
Iteration 116/1000 | Loss: 0.00000930
Iteration 117/1000 | Loss: 0.00000930
Iteration 118/1000 | Loss: 0.00000930
Iteration 119/1000 | Loss: 0.00000930
Iteration 120/1000 | Loss: 0.00000929
Iteration 121/1000 | Loss: 0.00000929
Iteration 122/1000 | Loss: 0.00000929
Iteration 123/1000 | Loss: 0.00000929
Iteration 124/1000 | Loss: 0.00000929
Iteration 125/1000 | Loss: 0.00000929
Iteration 126/1000 | Loss: 0.00000928
Iteration 127/1000 | Loss: 0.00000928
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000927
Iteration 130/1000 | Loss: 0.00000927
Iteration 131/1000 | Loss: 0.00000927
Iteration 132/1000 | Loss: 0.00000927
Iteration 133/1000 | Loss: 0.00000927
Iteration 134/1000 | Loss: 0.00000927
Iteration 135/1000 | Loss: 0.00000927
Iteration 136/1000 | Loss: 0.00000927
Iteration 137/1000 | Loss: 0.00000927
Iteration 138/1000 | Loss: 0.00000927
Iteration 139/1000 | Loss: 0.00000927
Iteration 140/1000 | Loss: 0.00000927
Iteration 141/1000 | Loss: 0.00000927
Iteration 142/1000 | Loss: 0.00000927
Iteration 143/1000 | Loss: 0.00000927
Iteration 144/1000 | Loss: 0.00000927
Iteration 145/1000 | Loss: 0.00000927
Iteration 146/1000 | Loss: 0.00000927
Iteration 147/1000 | Loss: 0.00000927
Iteration 148/1000 | Loss: 0.00000927
Iteration 149/1000 | Loss: 0.00000927
Iteration 150/1000 | Loss: 0.00000927
Iteration 151/1000 | Loss: 0.00000927
Iteration 152/1000 | Loss: 0.00000927
Iteration 153/1000 | Loss: 0.00000927
Iteration 154/1000 | Loss: 0.00000927
Iteration 155/1000 | Loss: 0.00000927
Iteration 156/1000 | Loss: 0.00000927
Iteration 157/1000 | Loss: 0.00000927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [9.265224434784614e-06, 9.265224434784614e-06, 9.265224434784614e-06, 9.265224434784614e-06, 9.265224434784614e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.265224434784614e-06

Optimization complete. Final v2v error: 2.5761966705322266 mm

Highest mean error: 3.7384276390075684 mm for frame 89

Lowest mean error: 2.315617084503174 mm for frame 2

Saving results

Total time: 42.79214000701904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442392
Iteration 2/25 | Loss: 0.00115825
Iteration 3/25 | Loss: 0.00109214
Iteration 4/25 | Loss: 0.00108091
Iteration 5/25 | Loss: 0.00107696
Iteration 6/25 | Loss: 0.00107696
Iteration 7/25 | Loss: 0.00107696
Iteration 8/25 | Loss: 0.00107696
Iteration 9/25 | Loss: 0.00107696
Iteration 10/25 | Loss: 0.00107696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010769566288217902, 0.0010769566288217902, 0.0010769566288217902, 0.0010769566288217902, 0.0010769566288217902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010769566288217902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41097832
Iteration 2/25 | Loss: 0.00065181
Iteration 3/25 | Loss: 0.00065181
Iteration 4/25 | Loss: 0.00065181
Iteration 5/25 | Loss: 0.00065181
Iteration 6/25 | Loss: 0.00065181
Iteration 7/25 | Loss: 0.00065181
Iteration 8/25 | Loss: 0.00065181
Iteration 9/25 | Loss: 0.00065181
Iteration 10/25 | Loss: 0.00065181
Iteration 11/25 | Loss: 0.00065180
Iteration 12/25 | Loss: 0.00065180
Iteration 13/25 | Loss: 0.00065180
Iteration 14/25 | Loss: 0.00065180
Iteration 15/25 | Loss: 0.00065180
Iteration 16/25 | Loss: 0.00065180
Iteration 17/25 | Loss: 0.00065180
Iteration 18/25 | Loss: 0.00065180
Iteration 19/25 | Loss: 0.00065180
Iteration 20/25 | Loss: 0.00065180
Iteration 21/25 | Loss: 0.00065180
Iteration 22/25 | Loss: 0.00065180
Iteration 23/25 | Loss: 0.00065180
Iteration 24/25 | Loss: 0.00065180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006518044974654913, 0.0006518044974654913, 0.0006518044974654913, 0.0006518044974654913, 0.0006518044974654913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006518044974654913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065180
Iteration 2/1000 | Loss: 0.00002024
Iteration 3/1000 | Loss: 0.00001494
Iteration 4/1000 | Loss: 0.00001362
Iteration 5/1000 | Loss: 0.00001301
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001271
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001258
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001237
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001202
Iteration 23/1000 | Loss: 0.00001202
Iteration 24/1000 | Loss: 0.00001201
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001166
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001165
Iteration 105/1000 | Loss: 0.00001165
Iteration 106/1000 | Loss: 0.00001165
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001164
Iteration 112/1000 | Loss: 0.00001164
Iteration 113/1000 | Loss: 0.00001164
Iteration 114/1000 | Loss: 0.00001164
Iteration 115/1000 | Loss: 0.00001163
Iteration 116/1000 | Loss: 0.00001163
Iteration 117/1000 | Loss: 0.00001163
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001162
Iteration 120/1000 | Loss: 0.00001162
Iteration 121/1000 | Loss: 0.00001161
Iteration 122/1000 | Loss: 0.00001161
Iteration 123/1000 | Loss: 0.00001161
Iteration 124/1000 | Loss: 0.00001161
Iteration 125/1000 | Loss: 0.00001161
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001158
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1574679774639662e-05, 1.1574679774639662e-05, 1.1574679774639662e-05, 1.1574679774639662e-05, 1.1574679774639662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1574679774639662e-05

Optimization complete. Final v2v error: 2.8915035724639893 mm

Highest mean error: 2.98598575592041 mm for frame 136

Lowest mean error: 2.8004062175750732 mm for frame 36

Saving results

Total time: 35.7870192527771
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781425
Iteration 2/25 | Loss: 0.00144864
Iteration 3/25 | Loss: 0.00121229
Iteration 4/25 | Loss: 0.00118678
Iteration 5/25 | Loss: 0.00118344
Iteration 6/25 | Loss: 0.00118344
Iteration 7/25 | Loss: 0.00118344
Iteration 8/25 | Loss: 0.00118344
Iteration 9/25 | Loss: 0.00118344
Iteration 10/25 | Loss: 0.00118344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011834391625598073, 0.0011834391625598073, 0.0011834391625598073, 0.0011834391625598073, 0.0011834391625598073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011834391625598073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38023114
Iteration 2/25 | Loss: 0.00051825
Iteration 3/25 | Loss: 0.00051825
Iteration 4/25 | Loss: 0.00051825
Iteration 5/25 | Loss: 0.00051825
Iteration 6/25 | Loss: 0.00051825
Iteration 7/25 | Loss: 0.00051825
Iteration 8/25 | Loss: 0.00051825
Iteration 9/25 | Loss: 0.00051825
Iteration 10/25 | Loss: 0.00051825
Iteration 11/25 | Loss: 0.00051825
Iteration 12/25 | Loss: 0.00051825
Iteration 13/25 | Loss: 0.00051825
Iteration 14/25 | Loss: 0.00051825
Iteration 15/25 | Loss: 0.00051825
Iteration 16/25 | Loss: 0.00051825
Iteration 17/25 | Loss: 0.00051825
Iteration 18/25 | Loss: 0.00051825
Iteration 19/25 | Loss: 0.00051825
Iteration 20/25 | Loss: 0.00051825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005182478344067931, 0.0005182478344067931, 0.0005182478344067931, 0.0005182478344067931, 0.0005182478344067931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005182478344067931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051825
Iteration 2/1000 | Loss: 0.00002927
Iteration 3/1000 | Loss: 0.00002360
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00002116
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00002024
Iteration 8/1000 | Loss: 0.00001986
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001943
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001932
Iteration 15/1000 | Loss: 0.00001923
Iteration 16/1000 | Loss: 0.00001922
Iteration 17/1000 | Loss: 0.00001918
Iteration 18/1000 | Loss: 0.00001914
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001913
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001910
Iteration 24/1000 | Loss: 0.00001910
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001910
Iteration 27/1000 | Loss: 0.00001910
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001909
Iteration 32/1000 | Loss: 0.00001909
Iteration 33/1000 | Loss: 0.00001909
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001908
Iteration 40/1000 | Loss: 0.00001908
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001907
Iteration 43/1000 | Loss: 0.00001907
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001906
Iteration 46/1000 | Loss: 0.00001906
Iteration 47/1000 | Loss: 0.00001906
Iteration 48/1000 | Loss: 0.00001906
Iteration 49/1000 | Loss: 0.00001905
Iteration 50/1000 | Loss: 0.00001905
Iteration 51/1000 | Loss: 0.00001905
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00001905
Iteration 54/1000 | Loss: 0.00001905
Iteration 55/1000 | Loss: 0.00001905
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001904
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001903
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00001902
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001902
Iteration 71/1000 | Loss: 0.00001902
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001900
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001899
Iteration 90/1000 | Loss: 0.00001899
Iteration 91/1000 | Loss: 0.00001899
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001899
Iteration 97/1000 | Loss: 0.00001899
Iteration 98/1000 | Loss: 0.00001899
Iteration 99/1000 | Loss: 0.00001899
Iteration 100/1000 | Loss: 0.00001899
Iteration 101/1000 | Loss: 0.00001899
Iteration 102/1000 | Loss: 0.00001899
Iteration 103/1000 | Loss: 0.00001899
Iteration 104/1000 | Loss: 0.00001899
Iteration 105/1000 | Loss: 0.00001899
Iteration 106/1000 | Loss: 0.00001899
Iteration 107/1000 | Loss: 0.00001899
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001899
Iteration 111/1000 | Loss: 0.00001899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.898731898108963e-05, 1.898731898108963e-05, 1.898731898108963e-05, 1.898731898108963e-05, 1.898731898108963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.898731898108963e-05

Optimization complete. Final v2v error: 3.6369800567626953 mm

Highest mean error: 3.775827646255493 mm for frame 174

Lowest mean error: 3.4527859687805176 mm for frame 0

Saving results

Total time: 34.92626929283142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798609
Iteration 2/25 | Loss: 0.00209824
Iteration 3/25 | Loss: 0.00143522
Iteration 4/25 | Loss: 0.00136679
Iteration 5/25 | Loss: 0.00134676
Iteration 6/25 | Loss: 0.00134109
Iteration 7/25 | Loss: 0.00133964
Iteration 8/25 | Loss: 0.00133930
Iteration 9/25 | Loss: 0.00133921
Iteration 10/25 | Loss: 0.00133921
Iteration 11/25 | Loss: 0.00133920
Iteration 12/25 | Loss: 0.00133920
Iteration 13/25 | Loss: 0.00133920
Iteration 14/25 | Loss: 0.00133920
Iteration 15/25 | Loss: 0.00133920
Iteration 16/25 | Loss: 0.00133920
Iteration 17/25 | Loss: 0.00133920
Iteration 18/25 | Loss: 0.00133920
Iteration 19/25 | Loss: 0.00133920
Iteration 20/25 | Loss: 0.00133920
Iteration 21/25 | Loss: 0.00133920
Iteration 22/25 | Loss: 0.00133920
Iteration 23/25 | Loss: 0.00133920
Iteration 24/25 | Loss: 0.00133919
Iteration 25/25 | Loss: 0.00133919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38159168
Iteration 2/25 | Loss: 0.00074900
Iteration 3/25 | Loss: 0.00074899
Iteration 4/25 | Loss: 0.00074899
Iteration 5/25 | Loss: 0.00074899
Iteration 6/25 | Loss: 0.00074899
Iteration 7/25 | Loss: 0.00074899
Iteration 8/25 | Loss: 0.00074899
Iteration 9/25 | Loss: 0.00074899
Iteration 10/25 | Loss: 0.00074899
Iteration 11/25 | Loss: 0.00074899
Iteration 12/25 | Loss: 0.00074899
Iteration 13/25 | Loss: 0.00074899
Iteration 14/25 | Loss: 0.00074899
Iteration 15/25 | Loss: 0.00074899
Iteration 16/25 | Loss: 0.00074899
Iteration 17/25 | Loss: 0.00074899
Iteration 18/25 | Loss: 0.00074899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007489909767173231, 0.0007489909767173231, 0.0007489909767173231, 0.0007489909767173231, 0.0007489909767173231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007489909767173231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074899
Iteration 2/1000 | Loss: 0.00005930
Iteration 3/1000 | Loss: 0.00003843
Iteration 4/1000 | Loss: 0.00003411
Iteration 5/1000 | Loss: 0.00003164
Iteration 6/1000 | Loss: 0.00003037
Iteration 7/1000 | Loss: 0.00002985
Iteration 8/1000 | Loss: 0.00002937
Iteration 9/1000 | Loss: 0.00002908
Iteration 10/1000 | Loss: 0.00002896
Iteration 11/1000 | Loss: 0.00002892
Iteration 12/1000 | Loss: 0.00002877
Iteration 13/1000 | Loss: 0.00002871
Iteration 14/1000 | Loss: 0.00002867
Iteration 15/1000 | Loss: 0.00002865
Iteration 16/1000 | Loss: 0.00002865
Iteration 17/1000 | Loss: 0.00002865
Iteration 18/1000 | Loss: 0.00002865
Iteration 19/1000 | Loss: 0.00002865
Iteration 20/1000 | Loss: 0.00002865
Iteration 21/1000 | Loss: 0.00002865
Iteration 22/1000 | Loss: 0.00002864
Iteration 23/1000 | Loss: 0.00002864
Iteration 24/1000 | Loss: 0.00002864
Iteration 25/1000 | Loss: 0.00002864
Iteration 26/1000 | Loss: 0.00002864
Iteration 27/1000 | Loss: 0.00002864
Iteration 28/1000 | Loss: 0.00002864
Iteration 29/1000 | Loss: 0.00002864
Iteration 30/1000 | Loss: 0.00002864
Iteration 31/1000 | Loss: 0.00002864
Iteration 32/1000 | Loss: 0.00002863
Iteration 33/1000 | Loss: 0.00002862
Iteration 34/1000 | Loss: 0.00002862
Iteration 35/1000 | Loss: 0.00002861
Iteration 36/1000 | Loss: 0.00002861
Iteration 37/1000 | Loss: 0.00002861
Iteration 38/1000 | Loss: 0.00002861
Iteration 39/1000 | Loss: 0.00002861
Iteration 40/1000 | Loss: 0.00002861
Iteration 41/1000 | Loss: 0.00002860
Iteration 42/1000 | Loss: 0.00002859
Iteration 43/1000 | Loss: 0.00002858
Iteration 44/1000 | Loss: 0.00002858
Iteration 45/1000 | Loss: 0.00002858
Iteration 46/1000 | Loss: 0.00002858
Iteration 47/1000 | Loss: 0.00002858
Iteration 48/1000 | Loss: 0.00002858
Iteration 49/1000 | Loss: 0.00002858
Iteration 50/1000 | Loss: 0.00002858
Iteration 51/1000 | Loss: 0.00002858
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002858
Iteration 54/1000 | Loss: 0.00002858
Iteration 55/1000 | Loss: 0.00002858
Iteration 56/1000 | Loss: 0.00002858
Iteration 57/1000 | Loss: 0.00002858
Iteration 58/1000 | Loss: 0.00002858
Iteration 59/1000 | Loss: 0.00002858
Iteration 60/1000 | Loss: 0.00002858
Iteration 61/1000 | Loss: 0.00002858
Iteration 62/1000 | Loss: 0.00002858
Iteration 63/1000 | Loss: 0.00002858
Iteration 64/1000 | Loss: 0.00002858
Iteration 65/1000 | Loss: 0.00002858
Iteration 66/1000 | Loss: 0.00002858
Iteration 67/1000 | Loss: 0.00002858
Iteration 68/1000 | Loss: 0.00002858
Iteration 69/1000 | Loss: 0.00002858
Iteration 70/1000 | Loss: 0.00002858
Iteration 71/1000 | Loss: 0.00002858
Iteration 72/1000 | Loss: 0.00002858
Iteration 73/1000 | Loss: 0.00002858
Iteration 74/1000 | Loss: 0.00002858
Iteration 75/1000 | Loss: 0.00002858
Iteration 76/1000 | Loss: 0.00002858
Iteration 77/1000 | Loss: 0.00002858
Iteration 78/1000 | Loss: 0.00002858
Iteration 79/1000 | Loss: 0.00002858
Iteration 80/1000 | Loss: 0.00002858
Iteration 81/1000 | Loss: 0.00002858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.8580847356352024e-05, 2.8580847356352024e-05, 2.8580847356352024e-05, 2.8580847356352024e-05, 2.8580847356352024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8580847356352024e-05

Optimization complete. Final v2v error: 4.60941219329834 mm

Highest mean error: 4.986846923828125 mm for frame 223

Lowest mean error: 4.0808844566345215 mm for frame 43

Saving results

Total time: 40.09997248649597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029828
Iteration 2/25 | Loss: 0.00419182
Iteration 3/25 | Loss: 0.00276038
Iteration 4/25 | Loss: 0.00189392
Iteration 5/25 | Loss: 0.00173871
Iteration 6/25 | Loss: 0.00144841
Iteration 7/25 | Loss: 0.00135018
Iteration 8/25 | Loss: 0.00126170
Iteration 9/25 | Loss: 0.00121218
Iteration 10/25 | Loss: 0.00120255
Iteration 11/25 | Loss: 0.00120309
Iteration 12/25 | Loss: 0.00120247
Iteration 13/25 | Loss: 0.00119662
Iteration 14/25 | Loss: 0.00119755
Iteration 15/25 | Loss: 0.00119673
Iteration 16/25 | Loss: 0.00119585
Iteration 17/25 | Loss: 0.00118817
Iteration 18/25 | Loss: 0.00118577
Iteration 19/25 | Loss: 0.00117722
Iteration 20/25 | Loss: 0.00117951
Iteration 21/25 | Loss: 0.00117926
Iteration 22/25 | Loss: 0.00117560
Iteration 23/25 | Loss: 0.00117132
Iteration 24/25 | Loss: 0.00116943
Iteration 25/25 | Loss: 0.00116956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36508870
Iteration 2/25 | Loss: 0.00124756
Iteration 3/25 | Loss: 0.00111923
Iteration 4/25 | Loss: 0.00111923
Iteration 5/25 | Loss: 0.00111923
Iteration 6/25 | Loss: 0.00111923
Iteration 7/25 | Loss: 0.00111923
Iteration 8/25 | Loss: 0.00111923
Iteration 9/25 | Loss: 0.00111923
Iteration 10/25 | Loss: 0.00111923
Iteration 11/25 | Loss: 0.00111923
Iteration 12/25 | Loss: 0.00111923
Iteration 13/25 | Loss: 0.00111923
Iteration 14/25 | Loss: 0.00111923
Iteration 15/25 | Loss: 0.00111923
Iteration 16/25 | Loss: 0.00111923
Iteration 17/25 | Loss: 0.00111923
Iteration 18/25 | Loss: 0.00111923
Iteration 19/25 | Loss: 0.00111923
Iteration 20/25 | Loss: 0.00111923
Iteration 21/25 | Loss: 0.00111923
Iteration 22/25 | Loss: 0.00111923
Iteration 23/25 | Loss: 0.00111923
Iteration 24/25 | Loss: 0.00111923
Iteration 25/25 | Loss: 0.00111923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011192267993465066, 0.0011192267993465066, 0.0011192267993465066, 0.0011192267993465066, 0.0011192267993465066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011192267993465066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111923
Iteration 2/1000 | Loss: 0.00026888
Iteration 3/1000 | Loss: 0.00037668
Iteration 4/1000 | Loss: 0.00058380
Iteration 5/1000 | Loss: 0.00034251
Iteration 6/1000 | Loss: 0.00045871
Iteration 7/1000 | Loss: 0.00042684
Iteration 8/1000 | Loss: 0.00041669
Iteration 9/1000 | Loss: 0.00039571
Iteration 10/1000 | Loss: 0.00032227
Iteration 11/1000 | Loss: 0.00037235
Iteration 12/1000 | Loss: 0.00026093
Iteration 13/1000 | Loss: 0.00036548
Iteration 14/1000 | Loss: 0.00042878
Iteration 15/1000 | Loss: 0.00006920
Iteration 16/1000 | Loss: 0.00018874
Iteration 17/1000 | Loss: 0.00014495
Iteration 18/1000 | Loss: 0.00015442
Iteration 19/1000 | Loss: 0.00010852
Iteration 20/1000 | Loss: 0.00017845
Iteration 21/1000 | Loss: 0.00013066
Iteration 22/1000 | Loss: 0.00014813
Iteration 23/1000 | Loss: 0.00009418
Iteration 24/1000 | Loss: 0.00005053
Iteration 25/1000 | Loss: 0.00004871
Iteration 26/1000 | Loss: 0.00004773
Iteration 27/1000 | Loss: 0.00019876
Iteration 28/1000 | Loss: 0.00005238
Iteration 29/1000 | Loss: 0.00008971
Iteration 30/1000 | Loss: 0.00017473
Iteration 31/1000 | Loss: 0.00007057
Iteration 32/1000 | Loss: 0.00015259
Iteration 33/1000 | Loss: 0.00015644
Iteration 34/1000 | Loss: 0.00028289
Iteration 35/1000 | Loss: 0.00006428
Iteration 36/1000 | Loss: 0.00004940
Iteration 37/1000 | Loss: 0.00007322
Iteration 38/1000 | Loss: 0.00004890
Iteration 39/1000 | Loss: 0.00006141
Iteration 40/1000 | Loss: 0.00004557
Iteration 41/1000 | Loss: 0.00004477
Iteration 42/1000 | Loss: 0.00004413
Iteration 43/1000 | Loss: 0.00004382
Iteration 44/1000 | Loss: 0.00064641
Iteration 45/1000 | Loss: 0.00080740
Iteration 46/1000 | Loss: 0.00009383
Iteration 47/1000 | Loss: 0.00004546
Iteration 48/1000 | Loss: 0.00057244
Iteration 49/1000 | Loss: 0.00007912
Iteration 50/1000 | Loss: 0.00054706
Iteration 51/1000 | Loss: 0.00050337
Iteration 52/1000 | Loss: 0.00006918
Iteration 53/1000 | Loss: 0.00006001
Iteration 54/1000 | Loss: 0.00004723
Iteration 55/1000 | Loss: 0.00004291
Iteration 56/1000 | Loss: 0.00082552
Iteration 57/1000 | Loss: 0.00004886
Iteration 58/1000 | Loss: 0.00004271
Iteration 59/1000 | Loss: 0.00003888
Iteration 60/1000 | Loss: 0.00003751
Iteration 61/1000 | Loss: 0.00003664
Iteration 62/1000 | Loss: 0.00003603
Iteration 63/1000 | Loss: 0.00003569
Iteration 64/1000 | Loss: 0.00003540
Iteration 65/1000 | Loss: 0.00003504
Iteration 66/1000 | Loss: 0.00003473
Iteration 67/1000 | Loss: 0.00003446
Iteration 68/1000 | Loss: 0.00003430
Iteration 69/1000 | Loss: 0.00003402
Iteration 70/1000 | Loss: 0.00003372
Iteration 71/1000 | Loss: 0.00003343
Iteration 72/1000 | Loss: 0.00126351
Iteration 73/1000 | Loss: 0.00004840
Iteration 74/1000 | Loss: 0.00003724
Iteration 75/1000 | Loss: 0.00003230
Iteration 76/1000 | Loss: 0.00003077
Iteration 77/1000 | Loss: 0.00002940
Iteration 78/1000 | Loss: 0.00002873
Iteration 79/1000 | Loss: 0.00002842
Iteration 80/1000 | Loss: 0.00002819
Iteration 81/1000 | Loss: 0.00002796
Iteration 82/1000 | Loss: 0.00002775
Iteration 83/1000 | Loss: 0.00002764
Iteration 84/1000 | Loss: 0.00002749
Iteration 85/1000 | Loss: 0.00002745
Iteration 86/1000 | Loss: 0.00098021
Iteration 87/1000 | Loss: 0.00015301
Iteration 88/1000 | Loss: 0.00004366
Iteration 89/1000 | Loss: 0.00002993
Iteration 90/1000 | Loss: 0.00002755
Iteration 91/1000 | Loss: 0.00002634
Iteration 92/1000 | Loss: 0.00002538
Iteration 93/1000 | Loss: 0.00002503
Iteration 94/1000 | Loss: 0.00002486
Iteration 95/1000 | Loss: 0.00002481
Iteration 96/1000 | Loss: 0.00002475
Iteration 97/1000 | Loss: 0.00002470
Iteration 98/1000 | Loss: 0.00002465
Iteration 99/1000 | Loss: 0.00002463
Iteration 100/1000 | Loss: 0.00002463
Iteration 101/1000 | Loss: 0.00002463
Iteration 102/1000 | Loss: 0.00002462
Iteration 103/1000 | Loss: 0.00002461
Iteration 104/1000 | Loss: 0.00002459
Iteration 105/1000 | Loss: 0.00002457
Iteration 106/1000 | Loss: 0.00002456
Iteration 107/1000 | Loss: 0.00002456
Iteration 108/1000 | Loss: 0.00002456
Iteration 109/1000 | Loss: 0.00002455
Iteration 110/1000 | Loss: 0.00002454
Iteration 111/1000 | Loss: 0.00002454
Iteration 112/1000 | Loss: 0.00002453
Iteration 113/1000 | Loss: 0.00002453
Iteration 114/1000 | Loss: 0.00002452
Iteration 115/1000 | Loss: 0.00002452
Iteration 116/1000 | Loss: 0.00002452
Iteration 117/1000 | Loss: 0.00002450
Iteration 118/1000 | Loss: 0.00002450
Iteration 119/1000 | Loss: 0.00002449
Iteration 120/1000 | Loss: 0.00002449
Iteration 121/1000 | Loss: 0.00002448
Iteration 122/1000 | Loss: 0.00002448
Iteration 123/1000 | Loss: 0.00002448
Iteration 124/1000 | Loss: 0.00002447
Iteration 125/1000 | Loss: 0.00002447
Iteration 126/1000 | Loss: 0.00002447
Iteration 127/1000 | Loss: 0.00002446
Iteration 128/1000 | Loss: 0.00002446
Iteration 129/1000 | Loss: 0.00002445
Iteration 130/1000 | Loss: 0.00002445
Iteration 131/1000 | Loss: 0.00002445
Iteration 132/1000 | Loss: 0.00002445
Iteration 133/1000 | Loss: 0.00002445
Iteration 134/1000 | Loss: 0.00002445
Iteration 135/1000 | Loss: 0.00002444
Iteration 136/1000 | Loss: 0.00002443
Iteration 137/1000 | Loss: 0.00002443
Iteration 138/1000 | Loss: 0.00002443
Iteration 139/1000 | Loss: 0.00002442
Iteration 140/1000 | Loss: 0.00002442
Iteration 141/1000 | Loss: 0.00002442
Iteration 142/1000 | Loss: 0.00002442
Iteration 143/1000 | Loss: 0.00002441
Iteration 144/1000 | Loss: 0.00002441
Iteration 145/1000 | Loss: 0.00002440
Iteration 146/1000 | Loss: 0.00002440
Iteration 147/1000 | Loss: 0.00002440
Iteration 148/1000 | Loss: 0.00002440
Iteration 149/1000 | Loss: 0.00002440
Iteration 150/1000 | Loss: 0.00002440
Iteration 151/1000 | Loss: 0.00002439
Iteration 152/1000 | Loss: 0.00002439
Iteration 153/1000 | Loss: 0.00002438
Iteration 154/1000 | Loss: 0.00002438
Iteration 155/1000 | Loss: 0.00002438
Iteration 156/1000 | Loss: 0.00002438
Iteration 157/1000 | Loss: 0.00002437
Iteration 158/1000 | Loss: 0.00002437
Iteration 159/1000 | Loss: 0.00002437
Iteration 160/1000 | Loss: 0.00002437
Iteration 161/1000 | Loss: 0.00002437
Iteration 162/1000 | Loss: 0.00002437
Iteration 163/1000 | Loss: 0.00002437
Iteration 164/1000 | Loss: 0.00002436
Iteration 165/1000 | Loss: 0.00002436
Iteration 166/1000 | Loss: 0.00002436
Iteration 167/1000 | Loss: 0.00002436
Iteration 168/1000 | Loss: 0.00002436
Iteration 169/1000 | Loss: 0.00002436
Iteration 170/1000 | Loss: 0.00002436
Iteration 171/1000 | Loss: 0.00002436
Iteration 172/1000 | Loss: 0.00002436
Iteration 173/1000 | Loss: 0.00002435
Iteration 174/1000 | Loss: 0.00002435
Iteration 175/1000 | Loss: 0.00002434
Iteration 176/1000 | Loss: 0.00002434
Iteration 177/1000 | Loss: 0.00002434
Iteration 178/1000 | Loss: 0.00002434
Iteration 179/1000 | Loss: 0.00002434
Iteration 180/1000 | Loss: 0.00002434
Iteration 181/1000 | Loss: 0.00002433
Iteration 182/1000 | Loss: 0.00002433
Iteration 183/1000 | Loss: 0.00002433
Iteration 184/1000 | Loss: 0.00002433
Iteration 185/1000 | Loss: 0.00002433
Iteration 186/1000 | Loss: 0.00002433
Iteration 187/1000 | Loss: 0.00002433
Iteration 188/1000 | Loss: 0.00002433
Iteration 189/1000 | Loss: 0.00002432
Iteration 190/1000 | Loss: 0.00002432
Iteration 191/1000 | Loss: 0.00002432
Iteration 192/1000 | Loss: 0.00002432
Iteration 193/1000 | Loss: 0.00002431
Iteration 194/1000 | Loss: 0.00002430
Iteration 195/1000 | Loss: 0.00002430
Iteration 196/1000 | Loss: 0.00002430
Iteration 197/1000 | Loss: 0.00002428
Iteration 198/1000 | Loss: 0.00002426
Iteration 199/1000 | Loss: 0.00002426
Iteration 200/1000 | Loss: 0.00002425
Iteration 201/1000 | Loss: 0.00002425
Iteration 202/1000 | Loss: 0.00002425
Iteration 203/1000 | Loss: 0.00002424
Iteration 204/1000 | Loss: 0.00002424
Iteration 205/1000 | Loss: 0.00002424
Iteration 206/1000 | Loss: 0.00002423
Iteration 207/1000 | Loss: 0.00002423
Iteration 208/1000 | Loss: 0.00002423
Iteration 209/1000 | Loss: 0.00002422
Iteration 210/1000 | Loss: 0.00002422
Iteration 211/1000 | Loss: 0.00002422
Iteration 212/1000 | Loss: 0.00002422
Iteration 213/1000 | Loss: 0.00002421
Iteration 214/1000 | Loss: 0.00002421
Iteration 215/1000 | Loss: 0.00002421
Iteration 216/1000 | Loss: 0.00002420
Iteration 217/1000 | Loss: 0.00002420
Iteration 218/1000 | Loss: 0.00002419
Iteration 219/1000 | Loss: 0.00002419
Iteration 220/1000 | Loss: 0.00002418
Iteration 221/1000 | Loss: 0.00002418
Iteration 222/1000 | Loss: 0.00002418
Iteration 223/1000 | Loss: 0.00002418
Iteration 224/1000 | Loss: 0.00002418
Iteration 225/1000 | Loss: 0.00002417
Iteration 226/1000 | Loss: 0.00002417
Iteration 227/1000 | Loss: 0.00002417
Iteration 228/1000 | Loss: 0.00002417
Iteration 229/1000 | Loss: 0.00002417
Iteration 230/1000 | Loss: 0.00002417
Iteration 231/1000 | Loss: 0.00002416
Iteration 232/1000 | Loss: 0.00002416
Iteration 233/1000 | Loss: 0.00002416
Iteration 234/1000 | Loss: 0.00002416
Iteration 235/1000 | Loss: 0.00002416
Iteration 236/1000 | Loss: 0.00002416
Iteration 237/1000 | Loss: 0.00002416
Iteration 238/1000 | Loss: 0.00002415
Iteration 239/1000 | Loss: 0.00002415
Iteration 240/1000 | Loss: 0.00002415
Iteration 241/1000 | Loss: 0.00002415
Iteration 242/1000 | Loss: 0.00002414
Iteration 243/1000 | Loss: 0.00002413
Iteration 244/1000 | Loss: 0.00002413
Iteration 245/1000 | Loss: 0.00002413
Iteration 246/1000 | Loss: 0.00002412
Iteration 247/1000 | Loss: 0.00002412
Iteration 248/1000 | Loss: 0.00002412
Iteration 249/1000 | Loss: 0.00002412
Iteration 250/1000 | Loss: 0.00002411
Iteration 251/1000 | Loss: 0.00002411
Iteration 252/1000 | Loss: 0.00002411
Iteration 253/1000 | Loss: 0.00002411
Iteration 254/1000 | Loss: 0.00002411
Iteration 255/1000 | Loss: 0.00002411
Iteration 256/1000 | Loss: 0.00002411
Iteration 257/1000 | Loss: 0.00002411
Iteration 258/1000 | Loss: 0.00002411
Iteration 259/1000 | Loss: 0.00002411
Iteration 260/1000 | Loss: 0.00002411
Iteration 261/1000 | Loss: 0.00002410
Iteration 262/1000 | Loss: 0.00002410
Iteration 263/1000 | Loss: 0.00002410
Iteration 264/1000 | Loss: 0.00002410
Iteration 265/1000 | Loss: 0.00002410
Iteration 266/1000 | Loss: 0.00002409
Iteration 267/1000 | Loss: 0.00002409
Iteration 268/1000 | Loss: 0.00002409
Iteration 269/1000 | Loss: 0.00002409
Iteration 270/1000 | Loss: 0.00002409
Iteration 271/1000 | Loss: 0.00002409
Iteration 272/1000 | Loss: 0.00002408
Iteration 273/1000 | Loss: 0.00002408
Iteration 274/1000 | Loss: 0.00002408
Iteration 275/1000 | Loss: 0.00002408
Iteration 276/1000 | Loss: 0.00002408
Iteration 277/1000 | Loss: 0.00002408
Iteration 278/1000 | Loss: 0.00002408
Iteration 279/1000 | Loss: 0.00002408
Iteration 280/1000 | Loss: 0.00002408
Iteration 281/1000 | Loss: 0.00002408
Iteration 282/1000 | Loss: 0.00002408
Iteration 283/1000 | Loss: 0.00002407
Iteration 284/1000 | Loss: 0.00002407
Iteration 285/1000 | Loss: 0.00002407
Iteration 286/1000 | Loss: 0.00002407
Iteration 287/1000 | Loss: 0.00002407
Iteration 288/1000 | Loss: 0.00002407
Iteration 289/1000 | Loss: 0.00002407
Iteration 290/1000 | Loss: 0.00002407
Iteration 291/1000 | Loss: 0.00002407
Iteration 292/1000 | Loss: 0.00002407
Iteration 293/1000 | Loss: 0.00002407
Iteration 294/1000 | Loss: 0.00002406
Iteration 295/1000 | Loss: 0.00002406
Iteration 296/1000 | Loss: 0.00002406
Iteration 297/1000 | Loss: 0.00002406
Iteration 298/1000 | Loss: 0.00002406
Iteration 299/1000 | Loss: 0.00002406
Iteration 300/1000 | Loss: 0.00002406
Iteration 301/1000 | Loss: 0.00002406
Iteration 302/1000 | Loss: 0.00002406
Iteration 303/1000 | Loss: 0.00002406
Iteration 304/1000 | Loss: 0.00002406
Iteration 305/1000 | Loss: 0.00002406
Iteration 306/1000 | Loss: 0.00002406
Iteration 307/1000 | Loss: 0.00002405
Iteration 308/1000 | Loss: 0.00002405
Iteration 309/1000 | Loss: 0.00002405
Iteration 310/1000 | Loss: 0.00002405
Iteration 311/1000 | Loss: 0.00002405
Iteration 312/1000 | Loss: 0.00002405
Iteration 313/1000 | Loss: 0.00002405
Iteration 314/1000 | Loss: 0.00002405
Iteration 315/1000 | Loss: 0.00002405
Iteration 316/1000 | Loss: 0.00002405
Iteration 317/1000 | Loss: 0.00002405
Iteration 318/1000 | Loss: 0.00002405
Iteration 319/1000 | Loss: 0.00002405
Iteration 320/1000 | Loss: 0.00002405
Iteration 321/1000 | Loss: 0.00002405
Iteration 322/1000 | Loss: 0.00002405
Iteration 323/1000 | Loss: 0.00002405
Iteration 324/1000 | Loss: 0.00002404
Iteration 325/1000 | Loss: 0.00002404
Iteration 326/1000 | Loss: 0.00002404
Iteration 327/1000 | Loss: 0.00002404
Iteration 328/1000 | Loss: 0.00002404
Iteration 329/1000 | Loss: 0.00002404
Iteration 330/1000 | Loss: 0.00002404
Iteration 331/1000 | Loss: 0.00002404
Iteration 332/1000 | Loss: 0.00002404
Iteration 333/1000 | Loss: 0.00002404
Iteration 334/1000 | Loss: 0.00002404
Iteration 335/1000 | Loss: 0.00002404
Iteration 336/1000 | Loss: 0.00002404
Iteration 337/1000 | Loss: 0.00002404
Iteration 338/1000 | Loss: 0.00002404
Iteration 339/1000 | Loss: 0.00002404
Iteration 340/1000 | Loss: 0.00002404
Iteration 341/1000 | Loss: 0.00002404
Iteration 342/1000 | Loss: 0.00002404
Iteration 343/1000 | Loss: 0.00002404
Iteration 344/1000 | Loss: 0.00002404
Iteration 345/1000 | Loss: 0.00002404
Iteration 346/1000 | Loss: 0.00002404
Iteration 347/1000 | Loss: 0.00002404
Iteration 348/1000 | Loss: 0.00002404
Iteration 349/1000 | Loss: 0.00002404
Iteration 350/1000 | Loss: 0.00002404
Iteration 351/1000 | Loss: 0.00002404
Iteration 352/1000 | Loss: 0.00002404
Iteration 353/1000 | Loss: 0.00002404
Iteration 354/1000 | Loss: 0.00002404
Iteration 355/1000 | Loss: 0.00002404
Iteration 356/1000 | Loss: 0.00002404
Iteration 357/1000 | Loss: 0.00002404
Iteration 358/1000 | Loss: 0.00002404
Iteration 359/1000 | Loss: 0.00002404
Iteration 360/1000 | Loss: 0.00002404
Iteration 361/1000 | Loss: 0.00002404
Iteration 362/1000 | Loss: 0.00002404
Iteration 363/1000 | Loss: 0.00002404
Iteration 364/1000 | Loss: 0.00002404
Iteration 365/1000 | Loss: 0.00002404
Iteration 366/1000 | Loss: 0.00002404
Iteration 367/1000 | Loss: 0.00002404
Iteration 368/1000 | Loss: 0.00002404
Iteration 369/1000 | Loss: 0.00002404
Iteration 370/1000 | Loss: 0.00002404
Iteration 371/1000 | Loss: 0.00002404
Iteration 372/1000 | Loss: 0.00002404
Iteration 373/1000 | Loss: 0.00002404
Iteration 374/1000 | Loss: 0.00002404
Iteration 375/1000 | Loss: 0.00002404
Iteration 376/1000 | Loss: 0.00002404
Iteration 377/1000 | Loss: 0.00002404
Iteration 378/1000 | Loss: 0.00002404
Iteration 379/1000 | Loss: 0.00002404
Iteration 380/1000 | Loss: 0.00002404
Iteration 381/1000 | Loss: 0.00002404
Iteration 382/1000 | Loss: 0.00002404
Iteration 383/1000 | Loss: 0.00002404
Iteration 384/1000 | Loss: 0.00002404
Iteration 385/1000 | Loss: 0.00002404
Iteration 386/1000 | Loss: 0.00002404
Iteration 387/1000 | Loss: 0.00002404
Iteration 388/1000 | Loss: 0.00002404
Iteration 389/1000 | Loss: 0.00002404
Iteration 390/1000 | Loss: 0.00002404
Iteration 391/1000 | Loss: 0.00002404
Iteration 392/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 392. Stopping optimization.
Last 5 losses: [2.404032420599833e-05, 2.404032420599833e-05, 2.404032420599833e-05, 2.404032420599833e-05, 2.404032420599833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.404032420599833e-05

Optimization complete. Final v2v error: 4.146435737609863 mm

Highest mean error: 4.812540054321289 mm for frame 81

Lowest mean error: 3.1158976554870605 mm for frame 153

Saving results

Total time: 205.500346660614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829922
Iteration 2/25 | Loss: 0.00140693
Iteration 3/25 | Loss: 0.00113969
Iteration 4/25 | Loss: 0.00109419
Iteration 5/25 | Loss: 0.00108850
Iteration 6/25 | Loss: 0.00108740
Iteration 7/25 | Loss: 0.00108733
Iteration 8/25 | Loss: 0.00108733
Iteration 9/25 | Loss: 0.00108733
Iteration 10/25 | Loss: 0.00108733
Iteration 11/25 | Loss: 0.00108733
Iteration 12/25 | Loss: 0.00108733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010873264400288463, 0.0010873264400288463, 0.0010873264400288463, 0.0010873264400288463, 0.0010873264400288463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010873264400288463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99667627
Iteration 2/25 | Loss: 0.00039276
Iteration 3/25 | Loss: 0.00039275
Iteration 4/25 | Loss: 0.00039275
Iteration 5/25 | Loss: 0.00039275
Iteration 6/25 | Loss: 0.00039275
Iteration 7/25 | Loss: 0.00039275
Iteration 8/25 | Loss: 0.00039275
Iteration 9/25 | Loss: 0.00039275
Iteration 10/25 | Loss: 0.00039275
Iteration 11/25 | Loss: 0.00039275
Iteration 12/25 | Loss: 0.00039275
Iteration 13/25 | Loss: 0.00039275
Iteration 14/25 | Loss: 0.00039275
Iteration 15/25 | Loss: 0.00039275
Iteration 16/25 | Loss: 0.00039275
Iteration 17/25 | Loss: 0.00039275
Iteration 18/25 | Loss: 0.00039275
Iteration 19/25 | Loss: 0.00039275
Iteration 20/25 | Loss: 0.00039275
Iteration 21/25 | Loss: 0.00039275
Iteration 22/25 | Loss: 0.00039275
Iteration 23/25 | Loss: 0.00039275
Iteration 24/25 | Loss: 0.00039275
Iteration 25/25 | Loss: 0.00039275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039275
Iteration 2/1000 | Loss: 0.00004144
Iteration 3/1000 | Loss: 0.00003009
Iteration 4/1000 | Loss: 0.00002674
Iteration 5/1000 | Loss: 0.00002507
Iteration 6/1000 | Loss: 0.00002363
Iteration 7/1000 | Loss: 0.00002273
Iteration 8/1000 | Loss: 0.00002213
Iteration 9/1000 | Loss: 0.00002176
Iteration 10/1000 | Loss: 0.00002157
Iteration 11/1000 | Loss: 0.00002136
Iteration 12/1000 | Loss: 0.00002112
Iteration 13/1000 | Loss: 0.00002098
Iteration 14/1000 | Loss: 0.00002092
Iteration 15/1000 | Loss: 0.00002091
Iteration 16/1000 | Loss: 0.00002074
Iteration 17/1000 | Loss: 0.00002069
Iteration 18/1000 | Loss: 0.00002069
Iteration 19/1000 | Loss: 0.00002068
Iteration 20/1000 | Loss: 0.00002068
Iteration 21/1000 | Loss: 0.00002066
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002065
Iteration 24/1000 | Loss: 0.00002063
Iteration 25/1000 | Loss: 0.00002063
Iteration 26/1000 | Loss: 0.00002062
Iteration 27/1000 | Loss: 0.00002061
Iteration 28/1000 | Loss: 0.00002061
Iteration 29/1000 | Loss: 0.00002060
Iteration 30/1000 | Loss: 0.00002060
Iteration 31/1000 | Loss: 0.00002059
Iteration 32/1000 | Loss: 0.00002059
Iteration 33/1000 | Loss: 0.00002058
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002057
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002056
Iteration 41/1000 | Loss: 0.00002056
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002056
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002055
Iteration 47/1000 | Loss: 0.00002055
Iteration 48/1000 | Loss: 0.00002055
Iteration 49/1000 | Loss: 0.00002054
Iteration 50/1000 | Loss: 0.00002054
Iteration 51/1000 | Loss: 0.00002054
Iteration 52/1000 | Loss: 0.00002054
Iteration 53/1000 | Loss: 0.00002054
Iteration 54/1000 | Loss: 0.00002053
Iteration 55/1000 | Loss: 0.00002053
Iteration 56/1000 | Loss: 0.00002053
Iteration 57/1000 | Loss: 0.00002053
Iteration 58/1000 | Loss: 0.00002053
Iteration 59/1000 | Loss: 0.00002053
Iteration 60/1000 | Loss: 0.00002053
Iteration 61/1000 | Loss: 0.00002053
Iteration 62/1000 | Loss: 0.00002053
Iteration 63/1000 | Loss: 0.00002053
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002051
Iteration 70/1000 | Loss: 0.00002051
Iteration 71/1000 | Loss: 0.00002051
Iteration 72/1000 | Loss: 0.00002051
Iteration 73/1000 | Loss: 0.00002051
Iteration 74/1000 | Loss: 0.00002051
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002050
Iteration 77/1000 | Loss: 0.00002050
Iteration 78/1000 | Loss: 0.00002050
Iteration 79/1000 | Loss: 0.00002049
Iteration 80/1000 | Loss: 0.00002049
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002048
Iteration 85/1000 | Loss: 0.00002048
Iteration 86/1000 | Loss: 0.00002047
Iteration 87/1000 | Loss: 0.00002047
Iteration 88/1000 | Loss: 0.00002047
Iteration 89/1000 | Loss: 0.00002047
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00002047
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002045
Iteration 97/1000 | Loss: 0.00002045
Iteration 98/1000 | Loss: 0.00002045
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002045
Iteration 101/1000 | Loss: 0.00002044
Iteration 102/1000 | Loss: 0.00002044
Iteration 103/1000 | Loss: 0.00002044
Iteration 104/1000 | Loss: 0.00002044
Iteration 105/1000 | Loss: 0.00002044
Iteration 106/1000 | Loss: 0.00002044
Iteration 107/1000 | Loss: 0.00002044
Iteration 108/1000 | Loss: 0.00002044
Iteration 109/1000 | Loss: 0.00002044
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00002042
Iteration 113/1000 | Loss: 0.00002042
Iteration 114/1000 | Loss: 0.00002042
Iteration 115/1000 | Loss: 0.00002042
Iteration 116/1000 | Loss: 0.00002042
Iteration 117/1000 | Loss: 0.00002042
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002041
Iteration 127/1000 | Loss: 0.00002041
Iteration 128/1000 | Loss: 0.00002040
Iteration 129/1000 | Loss: 0.00002040
Iteration 130/1000 | Loss: 0.00002040
Iteration 131/1000 | Loss: 0.00002040
Iteration 132/1000 | Loss: 0.00002040
Iteration 133/1000 | Loss: 0.00002040
Iteration 134/1000 | Loss: 0.00002040
Iteration 135/1000 | Loss: 0.00002040
Iteration 136/1000 | Loss: 0.00002040
Iteration 137/1000 | Loss: 0.00002039
Iteration 138/1000 | Loss: 0.00002039
Iteration 139/1000 | Loss: 0.00002039
Iteration 140/1000 | Loss: 0.00002039
Iteration 141/1000 | Loss: 0.00002039
Iteration 142/1000 | Loss: 0.00002039
Iteration 143/1000 | Loss: 0.00002039
Iteration 144/1000 | Loss: 0.00002038
Iteration 145/1000 | Loss: 0.00002038
Iteration 146/1000 | Loss: 0.00002038
Iteration 147/1000 | Loss: 0.00002038
Iteration 148/1000 | Loss: 0.00002037
Iteration 149/1000 | Loss: 0.00002037
Iteration 150/1000 | Loss: 0.00002037
Iteration 151/1000 | Loss: 0.00002036
Iteration 152/1000 | Loss: 0.00002036
Iteration 153/1000 | Loss: 0.00002036
Iteration 154/1000 | Loss: 0.00002036
Iteration 155/1000 | Loss: 0.00002035
Iteration 156/1000 | Loss: 0.00002035
Iteration 157/1000 | Loss: 0.00002035
Iteration 158/1000 | Loss: 0.00002035
Iteration 159/1000 | Loss: 0.00002035
Iteration 160/1000 | Loss: 0.00002035
Iteration 161/1000 | Loss: 0.00002034
Iteration 162/1000 | Loss: 0.00002034
Iteration 163/1000 | Loss: 0.00002034
Iteration 164/1000 | Loss: 0.00002034
Iteration 165/1000 | Loss: 0.00002034
Iteration 166/1000 | Loss: 0.00002034
Iteration 167/1000 | Loss: 0.00002034
Iteration 168/1000 | Loss: 0.00002034
Iteration 169/1000 | Loss: 0.00002034
Iteration 170/1000 | Loss: 0.00002034
Iteration 171/1000 | Loss: 0.00002034
Iteration 172/1000 | Loss: 0.00002034
Iteration 173/1000 | Loss: 0.00002034
Iteration 174/1000 | Loss: 0.00002033
Iteration 175/1000 | Loss: 0.00002033
Iteration 176/1000 | Loss: 0.00002033
Iteration 177/1000 | Loss: 0.00002033
Iteration 178/1000 | Loss: 0.00002033
Iteration 179/1000 | Loss: 0.00002033
Iteration 180/1000 | Loss: 0.00002033
Iteration 181/1000 | Loss: 0.00002033
Iteration 182/1000 | Loss: 0.00002032
Iteration 183/1000 | Loss: 0.00002032
Iteration 184/1000 | Loss: 0.00002032
Iteration 185/1000 | Loss: 0.00002032
Iteration 186/1000 | Loss: 0.00002032
Iteration 187/1000 | Loss: 0.00002032
Iteration 188/1000 | Loss: 0.00002032
Iteration 189/1000 | Loss: 0.00002032
Iteration 190/1000 | Loss: 0.00002032
Iteration 191/1000 | Loss: 0.00002032
Iteration 192/1000 | Loss: 0.00002032
Iteration 193/1000 | Loss: 0.00002032
Iteration 194/1000 | Loss: 0.00002031
Iteration 195/1000 | Loss: 0.00002031
Iteration 196/1000 | Loss: 0.00002031
Iteration 197/1000 | Loss: 0.00002031
Iteration 198/1000 | Loss: 0.00002031
Iteration 199/1000 | Loss: 0.00002031
Iteration 200/1000 | Loss: 0.00002031
Iteration 201/1000 | Loss: 0.00002031
Iteration 202/1000 | Loss: 0.00002031
Iteration 203/1000 | Loss: 0.00002031
Iteration 204/1000 | Loss: 0.00002031
Iteration 205/1000 | Loss: 0.00002031
Iteration 206/1000 | Loss: 0.00002030
Iteration 207/1000 | Loss: 0.00002030
Iteration 208/1000 | Loss: 0.00002030
Iteration 209/1000 | Loss: 0.00002030
Iteration 210/1000 | Loss: 0.00002030
Iteration 211/1000 | Loss: 0.00002030
Iteration 212/1000 | Loss: 0.00002030
Iteration 213/1000 | Loss: 0.00002030
Iteration 214/1000 | Loss: 0.00002030
Iteration 215/1000 | Loss: 0.00002030
Iteration 216/1000 | Loss: 0.00002030
Iteration 217/1000 | Loss: 0.00002030
Iteration 218/1000 | Loss: 0.00002030
Iteration 219/1000 | Loss: 0.00002029
Iteration 220/1000 | Loss: 0.00002029
Iteration 221/1000 | Loss: 0.00002029
Iteration 222/1000 | Loss: 0.00002029
Iteration 223/1000 | Loss: 0.00002029
Iteration 224/1000 | Loss: 0.00002029
Iteration 225/1000 | Loss: 0.00002029
Iteration 226/1000 | Loss: 0.00002029
Iteration 227/1000 | Loss: 0.00002029
Iteration 228/1000 | Loss: 0.00002029
Iteration 229/1000 | Loss: 0.00002029
Iteration 230/1000 | Loss: 0.00002029
Iteration 231/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.029160714300815e-05, 2.029160714300815e-05, 2.029160714300815e-05, 2.029160714300815e-05, 2.029160714300815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029160714300815e-05

Optimization complete. Final v2v error: 3.888338565826416 mm

Highest mean error: 4.1850199699401855 mm for frame 131

Lowest mean error: 3.660982608795166 mm for frame 108

Saving results

Total time: 44.75263428688049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905505
Iteration 2/25 | Loss: 0.00125125
Iteration 3/25 | Loss: 0.00113520
Iteration 4/25 | Loss: 0.00111523
Iteration 5/25 | Loss: 0.00110780
Iteration 6/25 | Loss: 0.00110593
Iteration 7/25 | Loss: 0.00110585
Iteration 8/25 | Loss: 0.00110585
Iteration 9/25 | Loss: 0.00110585
Iteration 10/25 | Loss: 0.00110585
Iteration 11/25 | Loss: 0.00110585
Iteration 12/25 | Loss: 0.00110585
Iteration 13/25 | Loss: 0.00110585
Iteration 14/25 | Loss: 0.00110585
Iteration 15/25 | Loss: 0.00110585
Iteration 16/25 | Loss: 0.00110585
Iteration 17/25 | Loss: 0.00110585
Iteration 18/25 | Loss: 0.00110585
Iteration 19/25 | Loss: 0.00110585
Iteration 20/25 | Loss: 0.00110585
Iteration 21/25 | Loss: 0.00110585
Iteration 22/25 | Loss: 0.00110585
Iteration 23/25 | Loss: 0.00110585
Iteration 24/25 | Loss: 0.00110585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011058530071750283, 0.0011058530071750283, 0.0011058530071750283, 0.0011058530071750283, 0.0011058530071750283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011058530071750283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40265465
Iteration 2/25 | Loss: 0.00063186
Iteration 3/25 | Loss: 0.00063184
Iteration 4/25 | Loss: 0.00063184
Iteration 5/25 | Loss: 0.00063184
Iteration 6/25 | Loss: 0.00063184
Iteration 7/25 | Loss: 0.00063184
Iteration 8/25 | Loss: 0.00063184
Iteration 9/25 | Loss: 0.00063184
Iteration 10/25 | Loss: 0.00063184
Iteration 11/25 | Loss: 0.00063184
Iteration 12/25 | Loss: 0.00063184
Iteration 13/25 | Loss: 0.00063184
Iteration 14/25 | Loss: 0.00063184
Iteration 15/25 | Loss: 0.00063184
Iteration 16/25 | Loss: 0.00063184
Iteration 17/25 | Loss: 0.00063184
Iteration 18/25 | Loss: 0.00063184
Iteration 19/25 | Loss: 0.00063184
Iteration 20/25 | Loss: 0.00063184
Iteration 21/25 | Loss: 0.00063184
Iteration 22/25 | Loss: 0.00063184
Iteration 23/25 | Loss: 0.00063184
Iteration 24/25 | Loss: 0.00063184
Iteration 25/25 | Loss: 0.00063184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063184
Iteration 2/1000 | Loss: 0.00004033
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002035
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001812
Iteration 7/1000 | Loss: 0.00001761
Iteration 8/1000 | Loss: 0.00001728
Iteration 9/1000 | Loss: 0.00001680
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001634
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001632
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001622
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001606
Iteration 21/1000 | Loss: 0.00001605
Iteration 22/1000 | Loss: 0.00001605
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001601
Iteration 26/1000 | Loss: 0.00001600
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001593
Iteration 35/1000 | Loss: 0.00001592
Iteration 36/1000 | Loss: 0.00001592
Iteration 37/1000 | Loss: 0.00001590
Iteration 38/1000 | Loss: 0.00001590
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001588
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001586
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001583
Iteration 49/1000 | Loss: 0.00001583
Iteration 50/1000 | Loss: 0.00001583
Iteration 51/1000 | Loss: 0.00001582
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001581
Iteration 55/1000 | Loss: 0.00001581
Iteration 56/1000 | Loss: 0.00001581
Iteration 57/1000 | Loss: 0.00001581
Iteration 58/1000 | Loss: 0.00001581
Iteration 59/1000 | Loss: 0.00001581
Iteration 60/1000 | Loss: 0.00001581
Iteration 61/1000 | Loss: 0.00001581
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001581
Iteration 64/1000 | Loss: 0.00001580
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001580
Iteration 67/1000 | Loss: 0.00001580
Iteration 68/1000 | Loss: 0.00001580
Iteration 69/1000 | Loss: 0.00001580
Iteration 70/1000 | Loss: 0.00001579
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00001578
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001577
Iteration 75/1000 | Loss: 0.00001577
Iteration 76/1000 | Loss: 0.00001576
Iteration 77/1000 | Loss: 0.00001576
Iteration 78/1000 | Loss: 0.00001576
Iteration 79/1000 | Loss: 0.00001575
Iteration 80/1000 | Loss: 0.00001575
Iteration 81/1000 | Loss: 0.00001574
Iteration 82/1000 | Loss: 0.00001574
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001573
Iteration 87/1000 | Loss: 0.00001573
Iteration 88/1000 | Loss: 0.00001573
Iteration 89/1000 | Loss: 0.00001573
Iteration 90/1000 | Loss: 0.00001572
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001571
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001569
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001568
Iteration 102/1000 | Loss: 0.00001568
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001567
Iteration 106/1000 | Loss: 0.00001567
Iteration 107/1000 | Loss: 0.00001567
Iteration 108/1000 | Loss: 0.00001566
Iteration 109/1000 | Loss: 0.00001566
Iteration 110/1000 | Loss: 0.00001566
Iteration 111/1000 | Loss: 0.00001565
Iteration 112/1000 | Loss: 0.00001565
Iteration 113/1000 | Loss: 0.00001565
Iteration 114/1000 | Loss: 0.00001564
Iteration 115/1000 | Loss: 0.00001564
Iteration 116/1000 | Loss: 0.00001564
Iteration 117/1000 | Loss: 0.00001564
Iteration 118/1000 | Loss: 0.00001564
Iteration 119/1000 | Loss: 0.00001564
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001563
Iteration 123/1000 | Loss: 0.00001563
Iteration 124/1000 | Loss: 0.00001563
Iteration 125/1000 | Loss: 0.00001563
Iteration 126/1000 | Loss: 0.00001562
Iteration 127/1000 | Loss: 0.00001562
Iteration 128/1000 | Loss: 0.00001562
Iteration 129/1000 | Loss: 0.00001562
Iteration 130/1000 | Loss: 0.00001562
Iteration 131/1000 | Loss: 0.00001562
Iteration 132/1000 | Loss: 0.00001562
Iteration 133/1000 | Loss: 0.00001562
Iteration 134/1000 | Loss: 0.00001561
Iteration 135/1000 | Loss: 0.00001561
Iteration 136/1000 | Loss: 0.00001561
Iteration 137/1000 | Loss: 0.00001561
Iteration 138/1000 | Loss: 0.00001561
Iteration 139/1000 | Loss: 0.00001561
Iteration 140/1000 | Loss: 0.00001561
Iteration 141/1000 | Loss: 0.00001561
Iteration 142/1000 | Loss: 0.00001561
Iteration 143/1000 | Loss: 0.00001561
Iteration 144/1000 | Loss: 0.00001560
Iteration 145/1000 | Loss: 0.00001560
Iteration 146/1000 | Loss: 0.00001560
Iteration 147/1000 | Loss: 0.00001560
Iteration 148/1000 | Loss: 0.00001560
Iteration 149/1000 | Loss: 0.00001560
Iteration 150/1000 | Loss: 0.00001560
Iteration 151/1000 | Loss: 0.00001560
Iteration 152/1000 | Loss: 0.00001559
Iteration 153/1000 | Loss: 0.00001559
Iteration 154/1000 | Loss: 0.00001559
Iteration 155/1000 | Loss: 0.00001559
Iteration 156/1000 | Loss: 0.00001559
Iteration 157/1000 | Loss: 0.00001559
Iteration 158/1000 | Loss: 0.00001559
Iteration 159/1000 | Loss: 0.00001559
Iteration 160/1000 | Loss: 0.00001559
Iteration 161/1000 | Loss: 0.00001558
Iteration 162/1000 | Loss: 0.00001558
Iteration 163/1000 | Loss: 0.00001557
Iteration 164/1000 | Loss: 0.00001557
Iteration 165/1000 | Loss: 0.00001557
Iteration 166/1000 | Loss: 0.00001557
Iteration 167/1000 | Loss: 0.00001557
Iteration 168/1000 | Loss: 0.00001557
Iteration 169/1000 | Loss: 0.00001557
Iteration 170/1000 | Loss: 0.00001556
Iteration 171/1000 | Loss: 0.00001556
Iteration 172/1000 | Loss: 0.00001556
Iteration 173/1000 | Loss: 0.00001555
Iteration 174/1000 | Loss: 0.00001555
Iteration 175/1000 | Loss: 0.00001555
Iteration 176/1000 | Loss: 0.00001555
Iteration 177/1000 | Loss: 0.00001555
Iteration 178/1000 | Loss: 0.00001555
Iteration 179/1000 | Loss: 0.00001555
Iteration 180/1000 | Loss: 0.00001554
Iteration 181/1000 | Loss: 0.00001554
Iteration 182/1000 | Loss: 0.00001554
Iteration 183/1000 | Loss: 0.00001554
Iteration 184/1000 | Loss: 0.00001554
Iteration 185/1000 | Loss: 0.00001554
Iteration 186/1000 | Loss: 0.00001554
Iteration 187/1000 | Loss: 0.00001554
Iteration 188/1000 | Loss: 0.00001553
Iteration 189/1000 | Loss: 0.00001553
Iteration 190/1000 | Loss: 0.00001553
Iteration 191/1000 | Loss: 0.00001553
Iteration 192/1000 | Loss: 0.00001553
Iteration 193/1000 | Loss: 0.00001552
Iteration 194/1000 | Loss: 0.00001552
Iteration 195/1000 | Loss: 0.00001552
Iteration 196/1000 | Loss: 0.00001552
Iteration 197/1000 | Loss: 0.00001552
Iteration 198/1000 | Loss: 0.00001552
Iteration 199/1000 | Loss: 0.00001552
Iteration 200/1000 | Loss: 0.00001552
Iteration 201/1000 | Loss: 0.00001552
Iteration 202/1000 | Loss: 0.00001552
Iteration 203/1000 | Loss: 0.00001551
Iteration 204/1000 | Loss: 0.00001551
Iteration 205/1000 | Loss: 0.00001551
Iteration 206/1000 | Loss: 0.00001551
Iteration 207/1000 | Loss: 0.00001551
Iteration 208/1000 | Loss: 0.00001551
Iteration 209/1000 | Loss: 0.00001551
Iteration 210/1000 | Loss: 0.00001551
Iteration 211/1000 | Loss: 0.00001551
Iteration 212/1000 | Loss: 0.00001551
Iteration 213/1000 | Loss: 0.00001551
Iteration 214/1000 | Loss: 0.00001551
Iteration 215/1000 | Loss: 0.00001551
Iteration 216/1000 | Loss: 0.00001551
Iteration 217/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.550904562463984e-05, 1.550904562463984e-05, 1.550904562463984e-05, 1.550904562463984e-05, 1.550904562463984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.550904562463984e-05

Optimization complete. Final v2v error: 3.2973339557647705 mm

Highest mean error: 5.005669593811035 mm for frame 77

Lowest mean error: 2.7858805656433105 mm for frame 102

Saving results

Total time: 45.135093212127686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430441
Iteration 2/25 | Loss: 0.00118339
Iteration 3/25 | Loss: 0.00111033
Iteration 4/25 | Loss: 0.00110048
Iteration 5/25 | Loss: 0.00109761
Iteration 6/25 | Loss: 0.00109750
Iteration 7/25 | Loss: 0.00109750
Iteration 8/25 | Loss: 0.00109750
Iteration 9/25 | Loss: 0.00109750
Iteration 10/25 | Loss: 0.00109750
Iteration 11/25 | Loss: 0.00109750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010974961332976818, 0.0010974961332976818, 0.0010974961332976818, 0.0010974961332976818, 0.0010974961332976818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010974961332976818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44141352
Iteration 2/25 | Loss: 0.00054248
Iteration 3/25 | Loss: 0.00054247
Iteration 4/25 | Loss: 0.00054247
Iteration 5/25 | Loss: 0.00054247
Iteration 6/25 | Loss: 0.00054247
Iteration 7/25 | Loss: 0.00054247
Iteration 8/25 | Loss: 0.00054247
Iteration 9/25 | Loss: 0.00054247
Iteration 10/25 | Loss: 0.00054247
Iteration 11/25 | Loss: 0.00054247
Iteration 12/25 | Loss: 0.00054247
Iteration 13/25 | Loss: 0.00054247
Iteration 14/25 | Loss: 0.00054247
Iteration 15/25 | Loss: 0.00054247
Iteration 16/25 | Loss: 0.00054247
Iteration 17/25 | Loss: 0.00054247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005424683331511915, 0.0005424683331511915, 0.0005424683331511915, 0.0005424683331511915, 0.0005424683331511915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005424683331511915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054247
Iteration 2/1000 | Loss: 0.00002882
Iteration 3/1000 | Loss: 0.00001953
Iteration 4/1000 | Loss: 0.00001791
Iteration 5/1000 | Loss: 0.00001683
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001572
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001517
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001484
Iteration 14/1000 | Loss: 0.00001476
Iteration 15/1000 | Loss: 0.00001470
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001469
Iteration 18/1000 | Loss: 0.00001468
Iteration 19/1000 | Loss: 0.00001462
Iteration 20/1000 | Loss: 0.00001458
Iteration 21/1000 | Loss: 0.00001452
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001451
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001445
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001443
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001442
Iteration 34/1000 | Loss: 0.00001442
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001432
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001427
Iteration 44/1000 | Loss: 0.00001427
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001426
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001420
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001419
Iteration 63/1000 | Loss: 0.00001419
Iteration 64/1000 | Loss: 0.00001418
Iteration 65/1000 | Loss: 0.00001418
Iteration 66/1000 | Loss: 0.00001418
Iteration 67/1000 | Loss: 0.00001418
Iteration 68/1000 | Loss: 0.00001417
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001416
Iteration 71/1000 | Loss: 0.00001416
Iteration 72/1000 | Loss: 0.00001416
Iteration 73/1000 | Loss: 0.00001416
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001415
Iteration 77/1000 | Loss: 0.00001415
Iteration 78/1000 | Loss: 0.00001415
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001415
Iteration 85/1000 | Loss: 0.00001415
Iteration 86/1000 | Loss: 0.00001415
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001415
Iteration 93/1000 | Loss: 0.00001415
Iteration 94/1000 | Loss: 0.00001415
Iteration 95/1000 | Loss: 0.00001415
Iteration 96/1000 | Loss: 0.00001415
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.4145490240480285e-05, 1.4145490240480285e-05, 1.4145490240480285e-05, 1.4145490240480285e-05, 1.4145490240480285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4145490240480285e-05

Optimization complete. Final v2v error: 3.200233221054077 mm

Highest mean error: 3.5713024139404297 mm for frame 69

Lowest mean error: 2.8895387649536133 mm for frame 87

Saving results

Total time: 40.209603786468506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851627
Iteration 2/25 | Loss: 0.00139668
Iteration 3/25 | Loss: 0.00117450
Iteration 4/25 | Loss: 0.00114955
Iteration 5/25 | Loss: 0.00114493
Iteration 6/25 | Loss: 0.00114449
Iteration 7/25 | Loss: 0.00114449
Iteration 8/25 | Loss: 0.00114449
Iteration 9/25 | Loss: 0.00114449
Iteration 10/25 | Loss: 0.00114449
Iteration 11/25 | Loss: 0.00114449
Iteration 12/25 | Loss: 0.00114449
Iteration 13/25 | Loss: 0.00114449
Iteration 14/25 | Loss: 0.00114449
Iteration 15/25 | Loss: 0.00114449
Iteration 16/25 | Loss: 0.00114449
Iteration 17/25 | Loss: 0.00114449
Iteration 18/25 | Loss: 0.00114449
Iteration 19/25 | Loss: 0.00114449
Iteration 20/25 | Loss: 0.00114449
Iteration 21/25 | Loss: 0.00114449
Iteration 22/25 | Loss: 0.00114449
Iteration 23/25 | Loss: 0.00114449
Iteration 24/25 | Loss: 0.00114449
Iteration 25/25 | Loss: 0.00114449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95545548
Iteration 2/25 | Loss: 0.00042245
Iteration 3/25 | Loss: 0.00042244
Iteration 4/25 | Loss: 0.00042244
Iteration 5/25 | Loss: 0.00042244
Iteration 6/25 | Loss: 0.00042244
Iteration 7/25 | Loss: 0.00042244
Iteration 8/25 | Loss: 0.00042244
Iteration 9/25 | Loss: 0.00042244
Iteration 10/25 | Loss: 0.00042244
Iteration 11/25 | Loss: 0.00042244
Iteration 12/25 | Loss: 0.00042244
Iteration 13/25 | Loss: 0.00042244
Iteration 14/25 | Loss: 0.00042244
Iteration 15/25 | Loss: 0.00042244
Iteration 16/25 | Loss: 0.00042244
Iteration 17/25 | Loss: 0.00042244
Iteration 18/25 | Loss: 0.00042244
Iteration 19/25 | Loss: 0.00042244
Iteration 20/25 | Loss: 0.00042244
Iteration 21/25 | Loss: 0.00042244
Iteration 22/25 | Loss: 0.00042244
Iteration 23/25 | Loss: 0.00042244
Iteration 24/25 | Loss: 0.00042244
Iteration 25/25 | Loss: 0.00042244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042244
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00002411
Iteration 4/1000 | Loss: 0.00002247
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002085
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00002006
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001963
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001944
Iteration 17/1000 | Loss: 0.00001943
Iteration 18/1000 | Loss: 0.00001943
Iteration 19/1000 | Loss: 0.00001942
Iteration 20/1000 | Loss: 0.00001941
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001932
Iteration 24/1000 | Loss: 0.00001932
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001931
Iteration 40/1000 | Loss: 0.00001931
Iteration 41/1000 | Loss: 0.00001931
Iteration 42/1000 | Loss: 0.00001931
Iteration 43/1000 | Loss: 0.00001930
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001925
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001924
Iteration 56/1000 | Loss: 0.00001924
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001923
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001923
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001923
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001923
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001923
Iteration 78/1000 | Loss: 0.00001923
Iteration 79/1000 | Loss: 0.00001923
Iteration 80/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.9230094039812684e-05, 1.9230094039812684e-05, 1.9230094039812684e-05, 1.9230094039812684e-05, 1.9230094039812684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9230094039812684e-05

Optimization complete. Final v2v error: 3.6550183296203613 mm

Highest mean error: 3.885223865509033 mm for frame 0

Lowest mean error: 3.5426671504974365 mm for frame 157

Saving results

Total time: 33.95586538314819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423444
Iteration 2/25 | Loss: 0.00126685
Iteration 3/25 | Loss: 0.00111430
Iteration 4/25 | Loss: 0.00110327
Iteration 5/25 | Loss: 0.00110189
Iteration 6/25 | Loss: 0.00110161
Iteration 7/25 | Loss: 0.00110161
Iteration 8/25 | Loss: 0.00110160
Iteration 9/25 | Loss: 0.00110160
Iteration 10/25 | Loss: 0.00110160
Iteration 11/25 | Loss: 0.00110160
Iteration 12/25 | Loss: 0.00110160
Iteration 13/25 | Loss: 0.00110160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011015980271622539, 0.0011015980271622539, 0.0011015980271622539, 0.0011015980271622539, 0.0011015980271622539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011015980271622539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.21440077
Iteration 2/25 | Loss: 0.00051159
Iteration 3/25 | Loss: 0.00051158
Iteration 4/25 | Loss: 0.00051158
Iteration 5/25 | Loss: 0.00051158
Iteration 6/25 | Loss: 0.00051157
Iteration 7/25 | Loss: 0.00051157
Iteration 8/25 | Loss: 0.00051157
Iteration 9/25 | Loss: 0.00051157
Iteration 10/25 | Loss: 0.00051157
Iteration 11/25 | Loss: 0.00051157
Iteration 12/25 | Loss: 0.00051157
Iteration 13/25 | Loss: 0.00051157
Iteration 14/25 | Loss: 0.00051157
Iteration 15/25 | Loss: 0.00051157
Iteration 16/25 | Loss: 0.00051157
Iteration 17/25 | Loss: 0.00051157
Iteration 18/25 | Loss: 0.00051157
Iteration 19/25 | Loss: 0.00051157
Iteration 20/25 | Loss: 0.00051157
Iteration 21/25 | Loss: 0.00051157
Iteration 22/25 | Loss: 0.00051157
Iteration 23/25 | Loss: 0.00051157
Iteration 24/25 | Loss: 0.00051157
Iteration 25/25 | Loss: 0.00051157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051157
Iteration 2/1000 | Loss: 0.00002718
Iteration 3/1000 | Loss: 0.00002018
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001625
Iteration 6/1000 | Loss: 0.00001543
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001430
Iteration 10/1000 | Loss: 0.00001415
Iteration 11/1000 | Loss: 0.00001396
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001361
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001356
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001346
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001344
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001343
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001336
Iteration 69/1000 | Loss: 0.00001336
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001328
Iteration 104/1000 | Loss: 0.00001328
Iteration 105/1000 | Loss: 0.00001328
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001328
Iteration 108/1000 | Loss: 0.00001328
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001327
Iteration 149/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.3274278899189085e-05, 1.3274278899189085e-05, 1.3274278899189085e-05, 1.3274278899189085e-05, 1.3274278899189085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3274278899189085e-05

Optimization complete. Final v2v error: 3.0842301845550537 mm

Highest mean error: 3.5991103649139404 mm for frame 72

Lowest mean error: 2.7817952632904053 mm for frame 123

Saving results

Total time: 36.637696266174316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912822
Iteration 2/25 | Loss: 0.00225556
Iteration 3/25 | Loss: 0.00173966
Iteration 4/25 | Loss: 0.00165914
Iteration 5/25 | Loss: 0.00162624
Iteration 6/25 | Loss: 0.00150429
Iteration 7/25 | Loss: 0.00144887
Iteration 8/25 | Loss: 0.00142458
Iteration 9/25 | Loss: 0.00141136
Iteration 10/25 | Loss: 0.00138735
Iteration 11/25 | Loss: 0.00138389
Iteration 12/25 | Loss: 0.00136652
Iteration 13/25 | Loss: 0.00137014
Iteration 14/25 | Loss: 0.00137148
Iteration 15/25 | Loss: 0.00135004
Iteration 16/25 | Loss: 0.00134073
Iteration 17/25 | Loss: 0.00132437
Iteration 18/25 | Loss: 0.00132105
Iteration 19/25 | Loss: 0.00131908
Iteration 20/25 | Loss: 0.00131726
Iteration 21/25 | Loss: 0.00131538
Iteration 22/25 | Loss: 0.00131783
Iteration 23/25 | Loss: 0.00131709
Iteration 24/25 | Loss: 0.00131750
Iteration 25/25 | Loss: 0.00131687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86822498
Iteration 2/25 | Loss: 0.00202089
Iteration 3/25 | Loss: 0.00195970
Iteration 4/25 | Loss: 0.00195969
Iteration 5/25 | Loss: 0.00195969
Iteration 6/25 | Loss: 0.00195969
Iteration 7/25 | Loss: 0.00195969
Iteration 8/25 | Loss: 0.00195969
Iteration 9/25 | Loss: 0.00195969
Iteration 10/25 | Loss: 0.00195969
Iteration 11/25 | Loss: 0.00195969
Iteration 12/25 | Loss: 0.00195969
Iteration 13/25 | Loss: 0.00195969
Iteration 14/25 | Loss: 0.00195969
Iteration 15/25 | Loss: 0.00195969
Iteration 16/25 | Loss: 0.00195969
Iteration 17/25 | Loss: 0.00195969
Iteration 18/25 | Loss: 0.00195969
Iteration 19/25 | Loss: 0.00195969
Iteration 20/25 | Loss: 0.00195969
Iteration 21/25 | Loss: 0.00195969
Iteration 22/25 | Loss: 0.00195969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001959689427167177, 0.001959689427167177, 0.001959689427167177, 0.001959689427167177, 0.001959689427167177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001959689427167177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195969
Iteration 2/1000 | Loss: 0.00022078
Iteration 3/1000 | Loss: 0.00010585
Iteration 4/1000 | Loss: 0.00029449
Iteration 5/1000 | Loss: 0.00028777
Iteration 6/1000 | Loss: 0.00018129
Iteration 7/1000 | Loss: 0.00010194
Iteration 8/1000 | Loss: 0.00012034
Iteration 9/1000 | Loss: 0.00012517
Iteration 10/1000 | Loss: 0.00006068
Iteration 11/1000 | Loss: 0.00008481
Iteration 12/1000 | Loss: 0.00005579
Iteration 13/1000 | Loss: 0.00018090
Iteration 14/1000 | Loss: 0.00010306
Iteration 15/1000 | Loss: 0.00008060
Iteration 16/1000 | Loss: 0.00005532
Iteration 17/1000 | Loss: 0.00005418
Iteration 18/1000 | Loss: 0.00021532
Iteration 19/1000 | Loss: 0.00018313
Iteration 20/1000 | Loss: 0.00018118
Iteration 21/1000 | Loss: 0.00006022
Iteration 22/1000 | Loss: 0.00005532
Iteration 23/1000 | Loss: 0.00005427
Iteration 24/1000 | Loss: 0.00041130
Iteration 25/1000 | Loss: 0.00039773
Iteration 26/1000 | Loss: 0.00006298
Iteration 27/1000 | Loss: 0.00017322
Iteration 28/1000 | Loss: 0.00005762
Iteration 29/1000 | Loss: 0.00035182
Iteration 30/1000 | Loss: 0.00017385
Iteration 31/1000 | Loss: 0.00036055
Iteration 32/1000 | Loss: 0.00006033
Iteration 33/1000 | Loss: 0.00005460
Iteration 34/1000 | Loss: 0.00005244
Iteration 35/1000 | Loss: 0.00005165
Iteration 36/1000 | Loss: 0.00005112
Iteration 37/1000 | Loss: 0.00005079
Iteration 38/1000 | Loss: 0.00005046
Iteration 39/1000 | Loss: 0.00005026
Iteration 40/1000 | Loss: 0.00005006
Iteration 41/1000 | Loss: 0.00004998
Iteration 42/1000 | Loss: 0.00004997
Iteration 43/1000 | Loss: 0.00004995
Iteration 44/1000 | Loss: 0.00004995
Iteration 45/1000 | Loss: 0.00004994
Iteration 46/1000 | Loss: 0.00004994
Iteration 47/1000 | Loss: 0.00004992
Iteration 48/1000 | Loss: 0.00004992
Iteration 49/1000 | Loss: 0.00004990
Iteration 50/1000 | Loss: 0.00004990
Iteration 51/1000 | Loss: 0.00004990
Iteration 52/1000 | Loss: 0.00004990
Iteration 53/1000 | Loss: 0.00004990
Iteration 54/1000 | Loss: 0.00004989
Iteration 55/1000 | Loss: 0.00004989
Iteration 56/1000 | Loss: 0.00004989
Iteration 57/1000 | Loss: 0.00004989
Iteration 58/1000 | Loss: 0.00004989
Iteration 59/1000 | Loss: 0.00004989
Iteration 60/1000 | Loss: 0.00004989
Iteration 61/1000 | Loss: 0.00004988
Iteration 62/1000 | Loss: 0.00004987
Iteration 63/1000 | Loss: 0.00004987
Iteration 64/1000 | Loss: 0.00004986
Iteration 65/1000 | Loss: 0.00004984
Iteration 66/1000 | Loss: 0.00004983
Iteration 67/1000 | Loss: 0.00004983
Iteration 68/1000 | Loss: 0.00004979
Iteration 69/1000 | Loss: 0.00004974
Iteration 70/1000 | Loss: 0.00004973
Iteration 71/1000 | Loss: 0.00004971
Iteration 72/1000 | Loss: 0.00004970
Iteration 73/1000 | Loss: 0.00004970
Iteration 74/1000 | Loss: 0.00004969
Iteration 75/1000 | Loss: 0.00004967
Iteration 76/1000 | Loss: 0.00004966
Iteration 77/1000 | Loss: 0.00004966
Iteration 78/1000 | Loss: 0.00004966
Iteration 79/1000 | Loss: 0.00004966
Iteration 80/1000 | Loss: 0.00004966
Iteration 81/1000 | Loss: 0.00004966
Iteration 82/1000 | Loss: 0.00004965
Iteration 83/1000 | Loss: 0.00004965
Iteration 84/1000 | Loss: 0.00004965
Iteration 85/1000 | Loss: 0.00004965
Iteration 86/1000 | Loss: 0.00004965
Iteration 87/1000 | Loss: 0.00004964
Iteration 88/1000 | Loss: 0.00004964
Iteration 89/1000 | Loss: 0.00004964
Iteration 90/1000 | Loss: 0.00004964
Iteration 91/1000 | Loss: 0.00004964
Iteration 92/1000 | Loss: 0.00004964
Iteration 93/1000 | Loss: 0.00004963
Iteration 94/1000 | Loss: 0.00004963
Iteration 95/1000 | Loss: 0.00004963
Iteration 96/1000 | Loss: 0.00004962
Iteration 97/1000 | Loss: 0.00004962
Iteration 98/1000 | Loss: 0.00004961
Iteration 99/1000 | Loss: 0.00004960
Iteration 100/1000 | Loss: 0.00004960
Iteration 101/1000 | Loss: 0.00004959
Iteration 102/1000 | Loss: 0.00004959
Iteration 103/1000 | Loss: 0.00004958
Iteration 104/1000 | Loss: 0.00004958
Iteration 105/1000 | Loss: 0.00004957
Iteration 106/1000 | Loss: 0.00004956
Iteration 107/1000 | Loss: 0.00004956
Iteration 108/1000 | Loss: 0.00004955
Iteration 109/1000 | Loss: 0.00004955
Iteration 110/1000 | Loss: 0.00004955
Iteration 111/1000 | Loss: 0.00004954
Iteration 112/1000 | Loss: 0.00004953
Iteration 113/1000 | Loss: 0.00004952
Iteration 114/1000 | Loss: 0.00004950
Iteration 115/1000 | Loss: 0.00004949
Iteration 116/1000 | Loss: 0.00004949
Iteration 117/1000 | Loss: 0.00004945
Iteration 118/1000 | Loss: 0.00004945
Iteration 119/1000 | Loss: 0.00004944
Iteration 120/1000 | Loss: 0.00004944
Iteration 121/1000 | Loss: 0.00004943
Iteration 122/1000 | Loss: 0.00004943
Iteration 123/1000 | Loss: 0.00004942
Iteration 124/1000 | Loss: 0.00004942
Iteration 125/1000 | Loss: 0.00004942
Iteration 126/1000 | Loss: 0.00004941
Iteration 127/1000 | Loss: 0.00004941
Iteration 128/1000 | Loss: 0.00004940
Iteration 129/1000 | Loss: 0.00004940
Iteration 130/1000 | Loss: 0.00004939
Iteration 131/1000 | Loss: 0.00004939
Iteration 132/1000 | Loss: 0.00004938
Iteration 133/1000 | Loss: 0.00004938
Iteration 134/1000 | Loss: 0.00004938
Iteration 135/1000 | Loss: 0.00004938
Iteration 136/1000 | Loss: 0.00004937
Iteration 137/1000 | Loss: 0.00004937
Iteration 138/1000 | Loss: 0.00004937
Iteration 139/1000 | Loss: 0.00004936
Iteration 140/1000 | Loss: 0.00004936
Iteration 141/1000 | Loss: 0.00004936
Iteration 142/1000 | Loss: 0.00004935
Iteration 143/1000 | Loss: 0.00004934
Iteration 144/1000 | Loss: 0.00004934
Iteration 145/1000 | Loss: 0.00004933
Iteration 146/1000 | Loss: 0.00004933
Iteration 147/1000 | Loss: 0.00004932
Iteration 148/1000 | Loss: 0.00004932
Iteration 149/1000 | Loss: 0.00004932
Iteration 150/1000 | Loss: 0.00004931
Iteration 151/1000 | Loss: 0.00004930
Iteration 152/1000 | Loss: 0.00004929
Iteration 153/1000 | Loss: 0.00004929
Iteration 154/1000 | Loss: 0.00004928
Iteration 155/1000 | Loss: 0.00004928
Iteration 156/1000 | Loss: 0.00004927
Iteration 157/1000 | Loss: 0.00004927
Iteration 158/1000 | Loss: 0.00004927
Iteration 159/1000 | Loss: 0.00004925
Iteration 160/1000 | Loss: 0.00004925
Iteration 161/1000 | Loss: 0.00004925
Iteration 162/1000 | Loss: 0.00004924
Iteration 163/1000 | Loss: 0.00004924
Iteration 164/1000 | Loss: 0.00004924
Iteration 165/1000 | Loss: 0.00004924
Iteration 166/1000 | Loss: 0.00004924
Iteration 167/1000 | Loss: 0.00004923
Iteration 168/1000 | Loss: 0.00004923
Iteration 169/1000 | Loss: 0.00004922
Iteration 170/1000 | Loss: 0.00004922
Iteration 171/1000 | Loss: 0.00004919
Iteration 172/1000 | Loss: 0.00004919
Iteration 173/1000 | Loss: 0.00004906
Iteration 174/1000 | Loss: 0.00026006
Iteration 175/1000 | Loss: 0.00010505
Iteration 176/1000 | Loss: 0.00004949
Iteration 177/1000 | Loss: 0.00004906
Iteration 178/1000 | Loss: 0.00004892
Iteration 179/1000 | Loss: 0.00004891
Iteration 180/1000 | Loss: 0.00047801
Iteration 181/1000 | Loss: 0.00018597
Iteration 182/1000 | Loss: 0.00005012
Iteration 183/1000 | Loss: 0.00004927
Iteration 184/1000 | Loss: 0.00004909
Iteration 185/1000 | Loss: 0.00004897
Iteration 186/1000 | Loss: 0.00004896
Iteration 187/1000 | Loss: 0.00026635
Iteration 188/1000 | Loss: 0.00009394
Iteration 189/1000 | Loss: 0.00004913
Iteration 190/1000 | Loss: 0.00025334
Iteration 191/1000 | Loss: 0.00009233
Iteration 192/1000 | Loss: 0.00009765
Iteration 193/1000 | Loss: 0.00004945
Iteration 194/1000 | Loss: 0.00004912
Iteration 195/1000 | Loss: 0.00018135
Iteration 196/1000 | Loss: 0.00008434
Iteration 197/1000 | Loss: 0.00016950
Iteration 198/1000 | Loss: 0.00009702
Iteration 199/1000 | Loss: 0.00014647
Iteration 200/1000 | Loss: 0.00052672
Iteration 201/1000 | Loss: 0.00010765
Iteration 202/1000 | Loss: 0.00005335
Iteration 203/1000 | Loss: 0.00005038
Iteration 204/1000 | Loss: 0.00004890
Iteration 205/1000 | Loss: 0.00004820
Iteration 206/1000 | Loss: 0.00004766
Iteration 207/1000 | Loss: 0.00004714
Iteration 208/1000 | Loss: 0.00004670
Iteration 209/1000 | Loss: 0.00021036
Iteration 210/1000 | Loss: 0.00008443
Iteration 211/1000 | Loss: 0.00004993
Iteration 212/1000 | Loss: 0.00004604
Iteration 213/1000 | Loss: 0.00004521
Iteration 214/1000 | Loss: 0.00004436
Iteration 215/1000 | Loss: 0.00004403
Iteration 216/1000 | Loss: 0.00004380
Iteration 217/1000 | Loss: 0.00004375
Iteration 218/1000 | Loss: 0.00004368
Iteration 219/1000 | Loss: 0.00004364
Iteration 220/1000 | Loss: 0.00004362
Iteration 221/1000 | Loss: 0.00004361
Iteration 222/1000 | Loss: 0.00004361
Iteration 223/1000 | Loss: 0.00004361
Iteration 224/1000 | Loss: 0.00004361
Iteration 225/1000 | Loss: 0.00004361
Iteration 226/1000 | Loss: 0.00004360
Iteration 227/1000 | Loss: 0.00004360
Iteration 228/1000 | Loss: 0.00004360
Iteration 229/1000 | Loss: 0.00004360
Iteration 230/1000 | Loss: 0.00004360
Iteration 231/1000 | Loss: 0.00004360
Iteration 232/1000 | Loss: 0.00004360
Iteration 233/1000 | Loss: 0.00004360
Iteration 234/1000 | Loss: 0.00004360
Iteration 235/1000 | Loss: 0.00004360
Iteration 236/1000 | Loss: 0.00004360
Iteration 237/1000 | Loss: 0.00004359
Iteration 238/1000 | Loss: 0.00004359
Iteration 239/1000 | Loss: 0.00004359
Iteration 240/1000 | Loss: 0.00004359
Iteration 241/1000 | Loss: 0.00004358
Iteration 242/1000 | Loss: 0.00004358
Iteration 243/1000 | Loss: 0.00004357
Iteration 244/1000 | Loss: 0.00004357
Iteration 245/1000 | Loss: 0.00004357
Iteration 246/1000 | Loss: 0.00004357
Iteration 247/1000 | Loss: 0.00004356
Iteration 248/1000 | Loss: 0.00004356
Iteration 249/1000 | Loss: 0.00004356
Iteration 250/1000 | Loss: 0.00004355
Iteration 251/1000 | Loss: 0.00004355
Iteration 252/1000 | Loss: 0.00004355
Iteration 253/1000 | Loss: 0.00004355
Iteration 254/1000 | Loss: 0.00004355
Iteration 255/1000 | Loss: 0.00004355
Iteration 256/1000 | Loss: 0.00004355
Iteration 257/1000 | Loss: 0.00004355
Iteration 258/1000 | Loss: 0.00004355
Iteration 259/1000 | Loss: 0.00004355
Iteration 260/1000 | Loss: 0.00004354
Iteration 261/1000 | Loss: 0.00004354
Iteration 262/1000 | Loss: 0.00004354
Iteration 263/1000 | Loss: 0.00004354
Iteration 264/1000 | Loss: 0.00004354
Iteration 265/1000 | Loss: 0.00004354
Iteration 266/1000 | Loss: 0.00004354
Iteration 267/1000 | Loss: 0.00004354
Iteration 268/1000 | Loss: 0.00004354
Iteration 269/1000 | Loss: 0.00004354
Iteration 270/1000 | Loss: 0.00004354
Iteration 271/1000 | Loss: 0.00004354
Iteration 272/1000 | Loss: 0.00004353
Iteration 273/1000 | Loss: 0.00004353
Iteration 274/1000 | Loss: 0.00004353
Iteration 275/1000 | Loss: 0.00004353
Iteration 276/1000 | Loss: 0.00004353
Iteration 277/1000 | Loss: 0.00004353
Iteration 278/1000 | Loss: 0.00004353
Iteration 279/1000 | Loss: 0.00004353
Iteration 280/1000 | Loss: 0.00004353
Iteration 281/1000 | Loss: 0.00004353
Iteration 282/1000 | Loss: 0.00004353
Iteration 283/1000 | Loss: 0.00004352
Iteration 284/1000 | Loss: 0.00004352
Iteration 285/1000 | Loss: 0.00004352
Iteration 286/1000 | Loss: 0.00004352
Iteration 287/1000 | Loss: 0.00004351
Iteration 288/1000 | Loss: 0.00004351
Iteration 289/1000 | Loss: 0.00004351
Iteration 290/1000 | Loss: 0.00004351
Iteration 291/1000 | Loss: 0.00004351
Iteration 292/1000 | Loss: 0.00004351
Iteration 293/1000 | Loss: 0.00004350
Iteration 294/1000 | Loss: 0.00004350
Iteration 295/1000 | Loss: 0.00004350
Iteration 296/1000 | Loss: 0.00004350
Iteration 297/1000 | Loss: 0.00004350
Iteration 298/1000 | Loss: 0.00004350
Iteration 299/1000 | Loss: 0.00004350
Iteration 300/1000 | Loss: 0.00004350
Iteration 301/1000 | Loss: 0.00004350
Iteration 302/1000 | Loss: 0.00004350
Iteration 303/1000 | Loss: 0.00004350
Iteration 304/1000 | Loss: 0.00004350
Iteration 305/1000 | Loss: 0.00004350
Iteration 306/1000 | Loss: 0.00004350
Iteration 307/1000 | Loss: 0.00004350
Iteration 308/1000 | Loss: 0.00004350
Iteration 309/1000 | Loss: 0.00004350
Iteration 310/1000 | Loss: 0.00004350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [4.349975642981008e-05, 4.349975642981008e-05, 4.349975642981008e-05, 4.349975642981008e-05, 4.349975642981008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.349975642981008e-05

Optimization complete. Final v2v error: 4.46351957321167 mm

Highest mean error: 11.548284530639648 mm for frame 18

Lowest mean error: 3.365300178527832 mm for frame 87

Saving results

Total time: 214.18269801139832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990594
Iteration 2/25 | Loss: 0.00247181
Iteration 3/25 | Loss: 0.00193890
Iteration 4/25 | Loss: 0.00181853
Iteration 5/25 | Loss: 0.00155050
Iteration 6/25 | Loss: 0.00153584
Iteration 7/25 | Loss: 0.00149790
Iteration 8/25 | Loss: 0.00140869
Iteration 9/25 | Loss: 0.00130353
Iteration 10/25 | Loss: 0.00124812
Iteration 11/25 | Loss: 0.00123681
Iteration 12/25 | Loss: 0.00122426
Iteration 13/25 | Loss: 0.00122794
Iteration 14/25 | Loss: 0.00122311
Iteration 15/25 | Loss: 0.00121606
Iteration 16/25 | Loss: 0.00121628
Iteration 17/25 | Loss: 0.00121048
Iteration 18/25 | Loss: 0.00121116
Iteration 19/25 | Loss: 0.00121459
Iteration 20/25 | Loss: 0.00121207
Iteration 21/25 | Loss: 0.00120599
Iteration 22/25 | Loss: 0.00120146
Iteration 23/25 | Loss: 0.00120007
Iteration 24/25 | Loss: 0.00119931
Iteration 25/25 | Loss: 0.00119839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39032233
Iteration 2/25 | Loss: 0.00060604
Iteration 3/25 | Loss: 0.00060604
Iteration 4/25 | Loss: 0.00060604
Iteration 5/25 | Loss: 0.00060604
Iteration 6/25 | Loss: 0.00060604
Iteration 7/25 | Loss: 0.00060604
Iteration 8/25 | Loss: 0.00060604
Iteration 9/25 | Loss: 0.00060604
Iteration 10/25 | Loss: 0.00060604
Iteration 11/25 | Loss: 0.00060604
Iteration 12/25 | Loss: 0.00060604
Iteration 13/25 | Loss: 0.00060604
Iteration 14/25 | Loss: 0.00060604
Iteration 15/25 | Loss: 0.00060604
Iteration 16/25 | Loss: 0.00060604
Iteration 17/25 | Loss: 0.00060604
Iteration 18/25 | Loss: 0.00060604
Iteration 19/25 | Loss: 0.00060604
Iteration 20/25 | Loss: 0.00060604
Iteration 21/25 | Loss: 0.00060604
Iteration 22/25 | Loss: 0.00060604
Iteration 23/25 | Loss: 0.00060604
Iteration 24/25 | Loss: 0.00060604
Iteration 25/25 | Loss: 0.00060604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060604
Iteration 2/1000 | Loss: 0.00023976
Iteration 3/1000 | Loss: 0.00003618
Iteration 4/1000 | Loss: 0.00002936
Iteration 5/1000 | Loss: 0.00002559
Iteration 6/1000 | Loss: 0.00002371
Iteration 7/1000 | Loss: 0.00002267
Iteration 8/1000 | Loss: 0.00002192
Iteration 9/1000 | Loss: 0.00002150
Iteration 10/1000 | Loss: 0.00002100
Iteration 11/1000 | Loss: 0.00002064
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00002014
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001974
Iteration 16/1000 | Loss: 0.00013014
Iteration 17/1000 | Loss: 0.00002208
Iteration 18/1000 | Loss: 0.00002062
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001900
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00001788
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001776
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001774
Iteration 48/1000 | Loss: 0.00001774
Iteration 49/1000 | Loss: 0.00001773
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001773
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001769
Iteration 61/1000 | Loss: 0.00001769
Iteration 62/1000 | Loss: 0.00001769
Iteration 63/1000 | Loss: 0.00001769
Iteration 64/1000 | Loss: 0.00001769
Iteration 65/1000 | Loss: 0.00001769
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001767
Iteration 70/1000 | Loss: 0.00001767
Iteration 71/1000 | Loss: 0.00001767
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001766
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001766
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001765
Iteration 81/1000 | Loss: 0.00001765
Iteration 82/1000 | Loss: 0.00001765
Iteration 83/1000 | Loss: 0.00001765
Iteration 84/1000 | Loss: 0.00001765
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001765
Iteration 87/1000 | Loss: 0.00001765
Iteration 88/1000 | Loss: 0.00001765
Iteration 89/1000 | Loss: 0.00001765
Iteration 90/1000 | Loss: 0.00001765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.7651134839979932e-05, 1.7651134839979932e-05, 1.7651134839979932e-05, 1.7651134839979932e-05, 1.7651134839979932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7651134839979932e-05

Optimization complete. Final v2v error: 3.510331392288208 mm

Highest mean error: 4.857826232910156 mm for frame 6

Lowest mean error: 3.2985494136810303 mm for frame 24

Saving results

Total time: 86.92983031272888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00644121
Iteration 2/25 | Loss: 0.00139883
Iteration 3/25 | Loss: 0.00119081
Iteration 4/25 | Loss: 0.00114607
Iteration 5/25 | Loss: 0.00113793
Iteration 6/25 | Loss: 0.00114251
Iteration 7/25 | Loss: 0.00113337
Iteration 8/25 | Loss: 0.00112969
Iteration 9/25 | Loss: 0.00112654
Iteration 10/25 | Loss: 0.00112473
Iteration 11/25 | Loss: 0.00112404
Iteration 12/25 | Loss: 0.00112371
Iteration 13/25 | Loss: 0.00112358
Iteration 14/25 | Loss: 0.00112349
Iteration 15/25 | Loss: 0.00112349
Iteration 16/25 | Loss: 0.00112348
Iteration 17/25 | Loss: 0.00112348
Iteration 18/25 | Loss: 0.00112348
Iteration 19/25 | Loss: 0.00112347
Iteration 20/25 | Loss: 0.00112347
Iteration 21/25 | Loss: 0.00112347
Iteration 22/25 | Loss: 0.00112347
Iteration 23/25 | Loss: 0.00112347
Iteration 24/25 | Loss: 0.00112347
Iteration 25/25 | Loss: 0.00112347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75193882
Iteration 2/25 | Loss: 0.00059805
Iteration 3/25 | Loss: 0.00059804
Iteration 4/25 | Loss: 0.00059804
Iteration 5/25 | Loss: 0.00059804
Iteration 6/25 | Loss: 0.00059804
Iteration 7/25 | Loss: 0.00059804
Iteration 8/25 | Loss: 0.00059804
Iteration 9/25 | Loss: 0.00059804
Iteration 10/25 | Loss: 0.00059804
Iteration 11/25 | Loss: 0.00059804
Iteration 12/25 | Loss: 0.00059804
Iteration 13/25 | Loss: 0.00059804
Iteration 14/25 | Loss: 0.00059804
Iteration 15/25 | Loss: 0.00059804
Iteration 16/25 | Loss: 0.00059804
Iteration 17/25 | Loss: 0.00059804
Iteration 18/25 | Loss: 0.00059804
Iteration 19/25 | Loss: 0.00059804
Iteration 20/25 | Loss: 0.00059804
Iteration 21/25 | Loss: 0.00059804
Iteration 22/25 | Loss: 0.00059804
Iteration 23/25 | Loss: 0.00059804
Iteration 24/25 | Loss: 0.00059804
Iteration 25/25 | Loss: 0.00059804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059804
Iteration 2/1000 | Loss: 0.00003241
Iteration 3/1000 | Loss: 0.00001931
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001668
Iteration 6/1000 | Loss: 0.00001614
Iteration 7/1000 | Loss: 0.00007498
Iteration 8/1000 | Loss: 0.00001565
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001534
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001533
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001495
Iteration 16/1000 | Loss: 0.00011876
Iteration 17/1000 | Loss: 0.00010372
Iteration 18/1000 | Loss: 0.00001945
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00010505
Iteration 24/1000 | Loss: 0.00010505
Iteration 25/1000 | Loss: 0.00010088
Iteration 26/1000 | Loss: 0.00010361
Iteration 27/1000 | Loss: 0.00007706
Iteration 28/1000 | Loss: 0.00002465
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00004813
Iteration 31/1000 | Loss: 0.00002987
Iteration 32/1000 | Loss: 0.00004611
Iteration 33/1000 | Loss: 0.00001491
Iteration 34/1000 | Loss: 0.00002906
Iteration 35/1000 | Loss: 0.00001560
Iteration 36/1000 | Loss: 0.00008787
Iteration 37/1000 | Loss: 0.00002864
Iteration 38/1000 | Loss: 0.00008940
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002521
Iteration 41/1000 | Loss: 0.00006505
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00008411
Iteration 44/1000 | Loss: 0.00001658
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001425
Iteration 54/1000 | Loss: 0.00001424
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00005669
Iteration 58/1000 | Loss: 0.00003306
Iteration 59/1000 | Loss: 0.00001446
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00012749
Iteration 77/1000 | Loss: 0.00010271
Iteration 78/1000 | Loss: 0.00003046
Iteration 79/1000 | Loss: 0.00001574
Iteration 80/1000 | Loss: 0.00003697
Iteration 81/1000 | Loss: 0.00001597
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00002155
Iteration 84/1000 | Loss: 0.00002181
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001800
Iteration 87/1000 | Loss: 0.00008685
Iteration 88/1000 | Loss: 0.00003122
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001360
Iteration 93/1000 | Loss: 0.00001360
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00002971
Iteration 110/1000 | Loss: 0.00001357
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00003179
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00002801
Iteration 125/1000 | Loss: 0.00003983
Iteration 126/1000 | Loss: 0.00001907
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001341
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001341
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001339
Iteration 143/1000 | Loss: 0.00001339
Iteration 144/1000 | Loss: 0.00001339
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Iteration 148/1000 | Loss: 0.00002057
Iteration 149/1000 | Loss: 0.00002356
Iteration 150/1000 | Loss: 0.00001889
Iteration 151/1000 | Loss: 0.00001568
Iteration 152/1000 | Loss: 0.00003636
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001953
Iteration 155/1000 | Loss: 0.00003496
Iteration 156/1000 | Loss: 0.00001344
Iteration 157/1000 | Loss: 0.00001431
Iteration 158/1000 | Loss: 0.00001749
Iteration 159/1000 | Loss: 0.00001357
Iteration 160/1000 | Loss: 0.00001333
Iteration 161/1000 | Loss: 0.00001333
Iteration 162/1000 | Loss: 0.00001333
Iteration 163/1000 | Loss: 0.00001333
Iteration 164/1000 | Loss: 0.00001333
Iteration 165/1000 | Loss: 0.00001361
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001333
Iteration 169/1000 | Loss: 0.00001332
Iteration 170/1000 | Loss: 0.00001341
Iteration 171/1000 | Loss: 0.00001331
Iteration 172/1000 | Loss: 0.00001331
Iteration 173/1000 | Loss: 0.00001331
Iteration 174/1000 | Loss: 0.00001331
Iteration 175/1000 | Loss: 0.00001330
Iteration 176/1000 | Loss: 0.00001330
Iteration 177/1000 | Loss: 0.00001330
Iteration 178/1000 | Loss: 0.00001330
Iteration 179/1000 | Loss: 0.00001330
Iteration 180/1000 | Loss: 0.00001330
Iteration 181/1000 | Loss: 0.00001330
Iteration 182/1000 | Loss: 0.00001330
Iteration 183/1000 | Loss: 0.00001330
Iteration 184/1000 | Loss: 0.00001330
Iteration 185/1000 | Loss: 0.00001330
Iteration 186/1000 | Loss: 0.00001330
Iteration 187/1000 | Loss: 0.00001330
Iteration 188/1000 | Loss: 0.00001330
Iteration 189/1000 | Loss: 0.00001330
Iteration 190/1000 | Loss: 0.00001330
Iteration 191/1000 | Loss: 0.00001330
Iteration 192/1000 | Loss: 0.00001330
Iteration 193/1000 | Loss: 0.00001330
Iteration 194/1000 | Loss: 0.00001330
Iteration 195/1000 | Loss: 0.00001330
Iteration 196/1000 | Loss: 0.00001330
Iteration 197/1000 | Loss: 0.00001330
Iteration 198/1000 | Loss: 0.00001330
Iteration 199/1000 | Loss: 0.00001330
Iteration 200/1000 | Loss: 0.00001330
Iteration 201/1000 | Loss: 0.00001330
Iteration 202/1000 | Loss: 0.00001330
Iteration 203/1000 | Loss: 0.00001330
Iteration 204/1000 | Loss: 0.00001330
Iteration 205/1000 | Loss: 0.00001330
Iteration 206/1000 | Loss: 0.00001330
Iteration 207/1000 | Loss: 0.00001330
Iteration 208/1000 | Loss: 0.00001330
Iteration 209/1000 | Loss: 0.00001330
Iteration 210/1000 | Loss: 0.00001330
Iteration 211/1000 | Loss: 0.00001330
Iteration 212/1000 | Loss: 0.00001330
Iteration 213/1000 | Loss: 0.00001330
Iteration 214/1000 | Loss: 0.00001330
Iteration 215/1000 | Loss: 0.00001330
Iteration 216/1000 | Loss: 0.00001330
Iteration 217/1000 | Loss: 0.00001330
Iteration 218/1000 | Loss: 0.00001330
Iteration 219/1000 | Loss: 0.00001330
Iteration 220/1000 | Loss: 0.00001330
Iteration 221/1000 | Loss: 0.00001330
Iteration 222/1000 | Loss: 0.00001330
Iteration 223/1000 | Loss: 0.00001330
Iteration 224/1000 | Loss: 0.00001330
Iteration 225/1000 | Loss: 0.00001330
Iteration 226/1000 | Loss: 0.00001330
Iteration 227/1000 | Loss: 0.00001330
Iteration 228/1000 | Loss: 0.00001330
Iteration 229/1000 | Loss: 0.00001330
Iteration 230/1000 | Loss: 0.00001330
Iteration 231/1000 | Loss: 0.00001330
Iteration 232/1000 | Loss: 0.00001330
Iteration 233/1000 | Loss: 0.00001330
Iteration 234/1000 | Loss: 0.00001330
Iteration 235/1000 | Loss: 0.00001330
Iteration 236/1000 | Loss: 0.00001330
Iteration 237/1000 | Loss: 0.00001330
Iteration 238/1000 | Loss: 0.00001330
Iteration 239/1000 | Loss: 0.00001330
Iteration 240/1000 | Loss: 0.00001330
Iteration 241/1000 | Loss: 0.00001330
Iteration 242/1000 | Loss: 0.00001330
Iteration 243/1000 | Loss: 0.00001330
Iteration 244/1000 | Loss: 0.00001330
Iteration 245/1000 | Loss: 0.00001330
Iteration 246/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.330330633209087e-05, 1.330330633209087e-05, 1.330330633209087e-05, 1.330330633209087e-05, 1.330330633209087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.330330633209087e-05

Optimization complete. Final v2v error: 3.0482263565063477 mm

Highest mean error: 4.025472164154053 mm for frame 130

Lowest mean error: 2.6680350303649902 mm for frame 52

Saving results

Total time: 157.91833186149597
