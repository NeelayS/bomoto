Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=167, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9352-9407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421170
Iteration 2/25 | Loss: 0.00136145
Iteration 3/25 | Loss: 0.00126184
Iteration 4/25 | Loss: 0.00124931
Iteration 5/25 | Loss: 0.00124663
Iteration 6/25 | Loss: 0.00124663
Iteration 7/25 | Loss: 0.00124663
Iteration 8/25 | Loss: 0.00124663
Iteration 9/25 | Loss: 0.00124663
Iteration 10/25 | Loss: 0.00124663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012466255575418472, 0.0012466255575418472, 0.0012466255575418472, 0.0012466255575418472, 0.0012466255575418472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012466255575418472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22658443
Iteration 2/25 | Loss: 0.00220154
Iteration 3/25 | Loss: 0.00220154
Iteration 4/25 | Loss: 0.00220154
Iteration 5/25 | Loss: 0.00220154
Iteration 6/25 | Loss: 0.00220154
Iteration 7/25 | Loss: 0.00220153
Iteration 8/25 | Loss: 0.00220153
Iteration 9/25 | Loss: 0.00220153
Iteration 10/25 | Loss: 0.00220153
Iteration 11/25 | Loss: 0.00220153
Iteration 12/25 | Loss: 0.00220153
Iteration 13/25 | Loss: 0.00220153
Iteration 14/25 | Loss: 0.00220153
Iteration 15/25 | Loss: 0.00220153
Iteration 16/25 | Loss: 0.00220153
Iteration 17/25 | Loss: 0.00220153
Iteration 18/25 | Loss: 0.00220153
Iteration 19/25 | Loss: 0.00220153
Iteration 20/25 | Loss: 0.00220153
Iteration 21/25 | Loss: 0.00220153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022015334106981754, 0.0022015334106981754, 0.0022015334106981754, 0.0022015334106981754, 0.0022015334106981754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022015334106981754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220153
Iteration 2/1000 | Loss: 0.00002677
Iteration 3/1000 | Loss: 0.00001867
Iteration 4/1000 | Loss: 0.00001563
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001348
Iteration 7/1000 | Loss: 0.00001291
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001202
Iteration 10/1000 | Loss: 0.00001165
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001116
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001097
Iteration 15/1000 | Loss: 0.00001096
Iteration 16/1000 | Loss: 0.00001092
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001090
Iteration 19/1000 | Loss: 0.00001090
Iteration 20/1000 | Loss: 0.00001089
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001087
Iteration 23/1000 | Loss: 0.00001087
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001083
Iteration 27/1000 | Loss: 0.00001082
Iteration 28/1000 | Loss: 0.00001081
Iteration 29/1000 | Loss: 0.00001081
Iteration 30/1000 | Loss: 0.00001080
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001067
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001061
Iteration 35/1000 | Loss: 0.00001058
Iteration 36/1000 | Loss: 0.00001057
Iteration 37/1000 | Loss: 0.00001057
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001051
Iteration 41/1000 | Loss: 0.00001050
Iteration 42/1000 | Loss: 0.00001050
Iteration 43/1000 | Loss: 0.00001050
Iteration 44/1000 | Loss: 0.00001048
Iteration 45/1000 | Loss: 0.00001047
Iteration 46/1000 | Loss: 0.00001046
Iteration 47/1000 | Loss: 0.00001046
Iteration 48/1000 | Loss: 0.00001045
Iteration 49/1000 | Loss: 0.00001045
Iteration 50/1000 | Loss: 0.00001045
Iteration 51/1000 | Loss: 0.00001045
Iteration 52/1000 | Loss: 0.00001045
Iteration 53/1000 | Loss: 0.00001045
Iteration 54/1000 | Loss: 0.00001045
Iteration 55/1000 | Loss: 0.00001044
Iteration 56/1000 | Loss: 0.00001044
Iteration 57/1000 | Loss: 0.00001044
Iteration 58/1000 | Loss: 0.00001044
Iteration 59/1000 | Loss: 0.00001043
Iteration 60/1000 | Loss: 0.00001042
Iteration 61/1000 | Loss: 0.00001042
Iteration 62/1000 | Loss: 0.00001042
Iteration 63/1000 | Loss: 0.00001041
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001038
Iteration 68/1000 | Loss: 0.00001038
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001038
Iteration 75/1000 | Loss: 0.00001038
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001037
Iteration 78/1000 | Loss: 0.00001037
Iteration 79/1000 | Loss: 0.00001037
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001036
Iteration 83/1000 | Loss: 0.00001036
Iteration 84/1000 | Loss: 0.00001036
Iteration 85/1000 | Loss: 0.00001036
Iteration 86/1000 | Loss: 0.00001036
Iteration 87/1000 | Loss: 0.00001036
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001035
Iteration 90/1000 | Loss: 0.00001035
Iteration 91/1000 | Loss: 0.00001035
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001034
Iteration 94/1000 | Loss: 0.00001034
Iteration 95/1000 | Loss: 0.00001034
Iteration 96/1000 | Loss: 0.00001034
Iteration 97/1000 | Loss: 0.00001034
Iteration 98/1000 | Loss: 0.00001034
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001034
Iteration 101/1000 | Loss: 0.00001033
Iteration 102/1000 | Loss: 0.00001032
Iteration 103/1000 | Loss: 0.00001032
Iteration 104/1000 | Loss: 0.00001032
Iteration 105/1000 | Loss: 0.00001032
Iteration 106/1000 | Loss: 0.00001032
Iteration 107/1000 | Loss: 0.00001031
Iteration 108/1000 | Loss: 0.00001031
Iteration 109/1000 | Loss: 0.00001031
Iteration 110/1000 | Loss: 0.00001031
Iteration 111/1000 | Loss: 0.00001031
Iteration 112/1000 | Loss: 0.00001031
Iteration 113/1000 | Loss: 0.00001031
Iteration 114/1000 | Loss: 0.00001031
Iteration 115/1000 | Loss: 0.00001031
Iteration 116/1000 | Loss: 0.00001030
Iteration 117/1000 | Loss: 0.00001030
Iteration 118/1000 | Loss: 0.00001030
Iteration 119/1000 | Loss: 0.00001030
Iteration 120/1000 | Loss: 0.00001030
Iteration 121/1000 | Loss: 0.00001030
Iteration 122/1000 | Loss: 0.00001030
Iteration 123/1000 | Loss: 0.00001030
Iteration 124/1000 | Loss: 0.00001030
Iteration 125/1000 | Loss: 0.00001030
Iteration 126/1000 | Loss: 0.00001030
Iteration 127/1000 | Loss: 0.00001029
Iteration 128/1000 | Loss: 0.00001029
Iteration 129/1000 | Loss: 0.00001029
Iteration 130/1000 | Loss: 0.00001029
Iteration 131/1000 | Loss: 0.00001029
Iteration 132/1000 | Loss: 0.00001029
Iteration 133/1000 | Loss: 0.00001028
Iteration 134/1000 | Loss: 0.00001028
Iteration 135/1000 | Loss: 0.00001028
Iteration 136/1000 | Loss: 0.00001028
Iteration 137/1000 | Loss: 0.00001028
Iteration 138/1000 | Loss: 0.00001028
Iteration 139/1000 | Loss: 0.00001028
Iteration 140/1000 | Loss: 0.00001028
Iteration 141/1000 | Loss: 0.00001028
Iteration 142/1000 | Loss: 0.00001028
Iteration 143/1000 | Loss: 0.00001027
Iteration 144/1000 | Loss: 0.00001027
Iteration 145/1000 | Loss: 0.00001027
Iteration 146/1000 | Loss: 0.00001027
Iteration 147/1000 | Loss: 0.00001027
Iteration 148/1000 | Loss: 0.00001027
Iteration 149/1000 | Loss: 0.00001027
Iteration 150/1000 | Loss: 0.00001027
Iteration 151/1000 | Loss: 0.00001026
Iteration 152/1000 | Loss: 0.00001026
Iteration 153/1000 | Loss: 0.00001026
Iteration 154/1000 | Loss: 0.00001026
Iteration 155/1000 | Loss: 0.00001025
Iteration 156/1000 | Loss: 0.00001025
Iteration 157/1000 | Loss: 0.00001025
Iteration 158/1000 | Loss: 0.00001025
Iteration 159/1000 | Loss: 0.00001024
Iteration 160/1000 | Loss: 0.00001024
Iteration 161/1000 | Loss: 0.00001024
Iteration 162/1000 | Loss: 0.00001024
Iteration 163/1000 | Loss: 0.00001024
Iteration 164/1000 | Loss: 0.00001024
Iteration 165/1000 | Loss: 0.00001024
Iteration 166/1000 | Loss: 0.00001024
Iteration 167/1000 | Loss: 0.00001023
Iteration 168/1000 | Loss: 0.00001023
Iteration 169/1000 | Loss: 0.00001023
Iteration 170/1000 | Loss: 0.00001023
Iteration 171/1000 | Loss: 0.00001023
Iteration 172/1000 | Loss: 0.00001023
Iteration 173/1000 | Loss: 0.00001022
Iteration 174/1000 | Loss: 0.00001022
Iteration 175/1000 | Loss: 0.00001022
Iteration 176/1000 | Loss: 0.00001022
Iteration 177/1000 | Loss: 0.00001022
Iteration 178/1000 | Loss: 0.00001022
Iteration 179/1000 | Loss: 0.00001022
Iteration 180/1000 | Loss: 0.00001022
Iteration 181/1000 | Loss: 0.00001022
Iteration 182/1000 | Loss: 0.00001022
Iteration 183/1000 | Loss: 0.00001022
Iteration 184/1000 | Loss: 0.00001022
Iteration 185/1000 | Loss: 0.00001022
Iteration 186/1000 | Loss: 0.00001022
Iteration 187/1000 | Loss: 0.00001022
Iteration 188/1000 | Loss: 0.00001022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.0215392649115529e-05, 1.0215392649115529e-05, 1.0215392649115529e-05, 1.0215392649115529e-05, 1.0215392649115529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0215392649115529e-05

Optimization complete. Final v2v error: 2.7555925846099854 mm

Highest mean error: 3.829878807067871 mm for frame 64

Lowest mean error: 2.461052417755127 mm for frame 45

Saving results

Total time: 49.619579792022705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481102
Iteration 2/25 | Loss: 0.00159968
Iteration 3/25 | Loss: 0.00134369
Iteration 4/25 | Loss: 0.00130429
Iteration 5/25 | Loss: 0.00130167
Iteration 6/25 | Loss: 0.00130167
Iteration 7/25 | Loss: 0.00130167
Iteration 8/25 | Loss: 0.00130167
Iteration 9/25 | Loss: 0.00130167
Iteration 10/25 | Loss: 0.00130167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013016663724556565, 0.0013016663724556565, 0.0013016663724556565, 0.0013016663724556565, 0.0013016663724556565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013016663724556565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26523995
Iteration 2/25 | Loss: 0.00192026
Iteration 3/25 | Loss: 0.00192026
Iteration 4/25 | Loss: 0.00192026
Iteration 5/25 | Loss: 0.00192026
Iteration 6/25 | Loss: 0.00192026
Iteration 7/25 | Loss: 0.00192025
Iteration 8/25 | Loss: 0.00192025
Iteration 9/25 | Loss: 0.00192025
Iteration 10/25 | Loss: 0.00192025
Iteration 11/25 | Loss: 0.00192025
Iteration 12/25 | Loss: 0.00192025
Iteration 13/25 | Loss: 0.00192025
Iteration 14/25 | Loss: 0.00192025
Iteration 15/25 | Loss: 0.00192025
Iteration 16/25 | Loss: 0.00192025
Iteration 17/25 | Loss: 0.00192025
Iteration 18/25 | Loss: 0.00192025
Iteration 19/25 | Loss: 0.00192025
Iteration 20/25 | Loss: 0.00192025
Iteration 21/25 | Loss: 0.00192025
Iteration 22/25 | Loss: 0.00192025
Iteration 23/25 | Loss: 0.00192025
Iteration 24/25 | Loss: 0.00192025
Iteration 25/25 | Loss: 0.00192025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192025
Iteration 2/1000 | Loss: 0.00003384
Iteration 3/1000 | Loss: 0.00002476
Iteration 4/1000 | Loss: 0.00001988
Iteration 5/1000 | Loss: 0.00001852
Iteration 6/1000 | Loss: 0.00001752
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001636
Iteration 9/1000 | Loss: 0.00001592
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001476
Iteration 14/1000 | Loss: 0.00001455
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001422
Iteration 19/1000 | Loss: 0.00001418
Iteration 20/1000 | Loss: 0.00001418
Iteration 21/1000 | Loss: 0.00001417
Iteration 22/1000 | Loss: 0.00001417
Iteration 23/1000 | Loss: 0.00001416
Iteration 24/1000 | Loss: 0.00001416
Iteration 25/1000 | Loss: 0.00001415
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001414
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001412
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001400
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001381
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001380
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001375
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001374
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001372
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001371
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001368
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001367
Iteration 102/1000 | Loss: 0.00001367
Iteration 103/1000 | Loss: 0.00001367
Iteration 104/1000 | Loss: 0.00001367
Iteration 105/1000 | Loss: 0.00001367
Iteration 106/1000 | Loss: 0.00001366
Iteration 107/1000 | Loss: 0.00001366
Iteration 108/1000 | Loss: 0.00001366
Iteration 109/1000 | Loss: 0.00001366
Iteration 110/1000 | Loss: 0.00001366
Iteration 111/1000 | Loss: 0.00001366
Iteration 112/1000 | Loss: 0.00001366
Iteration 113/1000 | Loss: 0.00001366
Iteration 114/1000 | Loss: 0.00001365
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001365
Iteration 117/1000 | Loss: 0.00001365
Iteration 118/1000 | Loss: 0.00001365
Iteration 119/1000 | Loss: 0.00001365
Iteration 120/1000 | Loss: 0.00001365
Iteration 121/1000 | Loss: 0.00001365
Iteration 122/1000 | Loss: 0.00001365
Iteration 123/1000 | Loss: 0.00001365
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001364
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001364
Iteration 130/1000 | Loss: 0.00001364
Iteration 131/1000 | Loss: 0.00001364
Iteration 132/1000 | Loss: 0.00001364
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001363
Iteration 135/1000 | Loss: 0.00001363
Iteration 136/1000 | Loss: 0.00001363
Iteration 137/1000 | Loss: 0.00001363
Iteration 138/1000 | Loss: 0.00001363
Iteration 139/1000 | Loss: 0.00001363
Iteration 140/1000 | Loss: 0.00001363
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001363
Iteration 144/1000 | Loss: 0.00001363
Iteration 145/1000 | Loss: 0.00001362
Iteration 146/1000 | Loss: 0.00001362
Iteration 147/1000 | Loss: 0.00001362
Iteration 148/1000 | Loss: 0.00001362
Iteration 149/1000 | Loss: 0.00001362
Iteration 150/1000 | Loss: 0.00001362
Iteration 151/1000 | Loss: 0.00001362
Iteration 152/1000 | Loss: 0.00001362
Iteration 153/1000 | Loss: 0.00001362
Iteration 154/1000 | Loss: 0.00001362
Iteration 155/1000 | Loss: 0.00001362
Iteration 156/1000 | Loss: 0.00001362
Iteration 157/1000 | Loss: 0.00001362
Iteration 158/1000 | Loss: 0.00001362
Iteration 159/1000 | Loss: 0.00001362
Iteration 160/1000 | Loss: 0.00001361
Iteration 161/1000 | Loss: 0.00001361
Iteration 162/1000 | Loss: 0.00001361
Iteration 163/1000 | Loss: 0.00001361
Iteration 164/1000 | Loss: 0.00001361
Iteration 165/1000 | Loss: 0.00001361
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001361
Iteration 169/1000 | Loss: 0.00001361
Iteration 170/1000 | Loss: 0.00001361
Iteration 171/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.3610590940515976e-05, 1.3610590940515976e-05, 1.3610590940515976e-05, 1.3610590940515976e-05, 1.3610590940515976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3610590940515976e-05

Optimization complete. Final v2v error: 3.127197265625 mm

Highest mean error: 3.6150457859039307 mm for frame 45

Lowest mean error: 2.711397409439087 mm for frame 156

Saving results

Total time: 47.71566843986511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878147
Iteration 2/25 | Loss: 0.00376218
Iteration 3/25 | Loss: 0.00286308
Iteration 4/25 | Loss: 0.00251410
Iteration 5/25 | Loss: 0.00213588
Iteration 6/25 | Loss: 0.00204053
Iteration 7/25 | Loss: 0.00201039
Iteration 8/25 | Loss: 0.00199851
Iteration 9/25 | Loss: 0.00204272
Iteration 10/25 | Loss: 0.00199473
Iteration 11/25 | Loss: 0.00189421
Iteration 12/25 | Loss: 0.00168226
Iteration 13/25 | Loss: 0.00152964
Iteration 14/25 | Loss: 0.00142091
Iteration 15/25 | Loss: 0.00137007
Iteration 16/25 | Loss: 0.00134577
Iteration 17/25 | Loss: 0.00134620
Iteration 18/25 | Loss: 0.00134409
Iteration 19/25 | Loss: 0.00132742
Iteration 20/25 | Loss: 0.00132283
Iteration 21/25 | Loss: 0.00132241
Iteration 22/25 | Loss: 0.00132222
Iteration 23/25 | Loss: 0.00132220
Iteration 24/25 | Loss: 0.00132220
Iteration 25/25 | Loss: 0.00132220

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19942939
Iteration 2/25 | Loss: 0.00152863
Iteration 3/25 | Loss: 0.00152863
Iteration 4/25 | Loss: 0.00152863
Iteration 5/25 | Loss: 0.00152863
Iteration 6/25 | Loss: 0.00152863
Iteration 7/25 | Loss: 0.00152863
Iteration 8/25 | Loss: 0.00152863
Iteration 9/25 | Loss: 0.00152863
Iteration 10/25 | Loss: 0.00152863
Iteration 11/25 | Loss: 0.00152863
Iteration 12/25 | Loss: 0.00152863
Iteration 13/25 | Loss: 0.00152863
Iteration 14/25 | Loss: 0.00152863
Iteration 15/25 | Loss: 0.00152863
Iteration 16/25 | Loss: 0.00152863
Iteration 17/25 | Loss: 0.00152863
Iteration 18/25 | Loss: 0.00152863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015286320121958852, 0.0015286320121958852, 0.0015286320121958852, 0.0015286320121958852, 0.0015286320121958852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015286320121958852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152863
Iteration 2/1000 | Loss: 0.00002605
Iteration 3/1000 | Loss: 0.00002029
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00001739
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001589
Iteration 9/1000 | Loss: 0.00001561
Iteration 10/1000 | Loss: 0.00001532
Iteration 11/1000 | Loss: 0.00001526
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001474
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00016845
Iteration 17/1000 | Loss: 0.00002064
Iteration 18/1000 | Loss: 0.00001809
Iteration 19/1000 | Loss: 0.00001639
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001566
Iteration 22/1000 | Loss: 0.00001526
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001508
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00031748
Iteration 31/1000 | Loss: 0.00016797
Iteration 32/1000 | Loss: 0.00016643
Iteration 33/1000 | Loss: 0.00013386
Iteration 34/1000 | Loss: 0.00009746
Iteration 35/1000 | Loss: 0.00011814
Iteration 36/1000 | Loss: 0.00002629
Iteration 37/1000 | Loss: 0.00002189
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001809
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001662
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001590
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001578
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001560
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001544
Iteration 68/1000 | Loss: 0.00001543
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001541
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001540
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001535
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001520
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001486
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001483
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001471
Iteration 101/1000 | Loss: 0.00001471
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001460
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001445
Iteration 115/1000 | Loss: 0.00001445
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001443
Iteration 124/1000 | Loss: 0.00001443
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001438
Iteration 129/1000 | Loss: 0.00001436
Iteration 130/1000 | Loss: 0.00001435
Iteration 131/1000 | Loss: 0.00001435
Iteration 132/1000 | Loss: 0.00001428
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001427
Iteration 136/1000 | Loss: 0.00001427
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001427
Iteration 140/1000 | Loss: 0.00001427
Iteration 141/1000 | Loss: 0.00001427
Iteration 142/1000 | Loss: 0.00001427
Iteration 143/1000 | Loss: 0.00001426
Iteration 144/1000 | Loss: 0.00001426
Iteration 145/1000 | Loss: 0.00001426
Iteration 146/1000 | Loss: 0.00001426
Iteration 147/1000 | Loss: 0.00001425
Iteration 148/1000 | Loss: 0.00001425
Iteration 149/1000 | Loss: 0.00001425
Iteration 150/1000 | Loss: 0.00001425
Iteration 151/1000 | Loss: 0.00001425
Iteration 152/1000 | Loss: 0.00001425
Iteration 153/1000 | Loss: 0.00001425
Iteration 154/1000 | Loss: 0.00001425
Iteration 155/1000 | Loss: 0.00001424
Iteration 156/1000 | Loss: 0.00001424
Iteration 157/1000 | Loss: 0.00001424
Iteration 158/1000 | Loss: 0.00001424
Iteration 159/1000 | Loss: 0.00001424
Iteration 160/1000 | Loss: 0.00001423
Iteration 161/1000 | Loss: 0.00001423
Iteration 162/1000 | Loss: 0.00001423
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001420
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001419
Iteration 171/1000 | Loss: 0.00001419
Iteration 172/1000 | Loss: 0.00001419
Iteration 173/1000 | Loss: 0.00001419
Iteration 174/1000 | Loss: 0.00001419
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001418
Iteration 181/1000 | Loss: 0.00001418
Iteration 182/1000 | Loss: 0.00001417
Iteration 183/1000 | Loss: 0.00001417
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001414
Iteration 187/1000 | Loss: 0.00001414
Iteration 188/1000 | Loss: 0.00001414
Iteration 189/1000 | Loss: 0.00001414
Iteration 190/1000 | Loss: 0.00001414
Iteration 191/1000 | Loss: 0.00001414
Iteration 192/1000 | Loss: 0.00001414
Iteration 193/1000 | Loss: 0.00001413
Iteration 194/1000 | Loss: 0.00001413
Iteration 195/1000 | Loss: 0.00001413
Iteration 196/1000 | Loss: 0.00001413
Iteration 197/1000 | Loss: 0.00001413
Iteration 198/1000 | Loss: 0.00001413
Iteration 199/1000 | Loss: 0.00001413
Iteration 200/1000 | Loss: 0.00001412
Iteration 201/1000 | Loss: 0.00001412
Iteration 202/1000 | Loss: 0.00001412
Iteration 203/1000 | Loss: 0.00001412
Iteration 204/1000 | Loss: 0.00001412
Iteration 205/1000 | Loss: 0.00001412
Iteration 206/1000 | Loss: 0.00001411
Iteration 207/1000 | Loss: 0.00001411
Iteration 208/1000 | Loss: 0.00001410
Iteration 209/1000 | Loss: 0.00001410
Iteration 210/1000 | Loss: 0.00001410
Iteration 211/1000 | Loss: 0.00001409
Iteration 212/1000 | Loss: 0.00001409
Iteration 213/1000 | Loss: 0.00001409
Iteration 214/1000 | Loss: 0.00001408
Iteration 215/1000 | Loss: 0.00001408
Iteration 216/1000 | Loss: 0.00001408
Iteration 217/1000 | Loss: 0.00001408
Iteration 218/1000 | Loss: 0.00001407
Iteration 219/1000 | Loss: 0.00001407
Iteration 220/1000 | Loss: 0.00001407
Iteration 221/1000 | Loss: 0.00001407
Iteration 222/1000 | Loss: 0.00001407
Iteration 223/1000 | Loss: 0.00001407
Iteration 224/1000 | Loss: 0.00001407
Iteration 225/1000 | Loss: 0.00001407
Iteration 226/1000 | Loss: 0.00001407
Iteration 227/1000 | Loss: 0.00001407
Iteration 228/1000 | Loss: 0.00001407
Iteration 229/1000 | Loss: 0.00001407
Iteration 230/1000 | Loss: 0.00001407
Iteration 231/1000 | Loss: 0.00001407
Iteration 232/1000 | Loss: 0.00001406
Iteration 233/1000 | Loss: 0.00001406
Iteration 234/1000 | Loss: 0.00001406
Iteration 235/1000 | Loss: 0.00001406
Iteration 236/1000 | Loss: 0.00001406
Iteration 237/1000 | Loss: 0.00001406
Iteration 238/1000 | Loss: 0.00001406
Iteration 239/1000 | Loss: 0.00001406
Iteration 240/1000 | Loss: 0.00001406
Iteration 241/1000 | Loss: 0.00001406
Iteration 242/1000 | Loss: 0.00001406
Iteration 243/1000 | Loss: 0.00001406
Iteration 244/1000 | Loss: 0.00001406
Iteration 245/1000 | Loss: 0.00001406
Iteration 246/1000 | Loss: 0.00001406
Iteration 247/1000 | Loss: 0.00001406
Iteration 248/1000 | Loss: 0.00001406
Iteration 249/1000 | Loss: 0.00001406
Iteration 250/1000 | Loss: 0.00001406
Iteration 251/1000 | Loss: 0.00001405
Iteration 252/1000 | Loss: 0.00001405
Iteration 253/1000 | Loss: 0.00001405
Iteration 254/1000 | Loss: 0.00001405
Iteration 255/1000 | Loss: 0.00001405
Iteration 256/1000 | Loss: 0.00001405
Iteration 257/1000 | Loss: 0.00001405
Iteration 258/1000 | Loss: 0.00001405
Iteration 259/1000 | Loss: 0.00001405
Iteration 260/1000 | Loss: 0.00001405
Iteration 261/1000 | Loss: 0.00001404
Iteration 262/1000 | Loss: 0.00001404
Iteration 263/1000 | Loss: 0.00001404
Iteration 264/1000 | Loss: 0.00001404
Iteration 265/1000 | Loss: 0.00001404
Iteration 266/1000 | Loss: 0.00001403
Iteration 267/1000 | Loss: 0.00001403
Iteration 268/1000 | Loss: 0.00001403
Iteration 269/1000 | Loss: 0.00001403
Iteration 270/1000 | Loss: 0.00001403
Iteration 271/1000 | Loss: 0.00001403
Iteration 272/1000 | Loss: 0.00001403
Iteration 273/1000 | Loss: 0.00001403
Iteration 274/1000 | Loss: 0.00001402
Iteration 275/1000 | Loss: 0.00001402
Iteration 276/1000 | Loss: 0.00001402
Iteration 277/1000 | Loss: 0.00001402
Iteration 278/1000 | Loss: 0.00001402
Iteration 279/1000 | Loss: 0.00001402
Iteration 280/1000 | Loss: 0.00001402
Iteration 281/1000 | Loss: 0.00001402
Iteration 282/1000 | Loss: 0.00001402
Iteration 283/1000 | Loss: 0.00001402
Iteration 284/1000 | Loss: 0.00001402
Iteration 285/1000 | Loss: 0.00001402
Iteration 286/1000 | Loss: 0.00001402
Iteration 287/1000 | Loss: 0.00001402
Iteration 288/1000 | Loss: 0.00001402
Iteration 289/1000 | Loss: 0.00001402
Iteration 290/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.4019326954439748e-05, 1.4019326954439748e-05, 1.4019326954439748e-05, 1.4019326954439748e-05, 1.4019326954439748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4019326954439748e-05

Optimization complete. Final v2v error: 3.160463571548462 mm

Highest mean error: 5.197277545928955 mm for frame 5

Lowest mean error: 2.9957878589630127 mm for frame 139

Saving results

Total time: 129.9660348892212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371121
Iteration 2/25 | Loss: 0.00127227
Iteration 3/25 | Loss: 0.00122148
Iteration 4/25 | Loss: 0.00121491
Iteration 5/25 | Loss: 0.00121314
Iteration 6/25 | Loss: 0.00121265
Iteration 7/25 | Loss: 0.00121265
Iteration 8/25 | Loss: 0.00121265
Iteration 9/25 | Loss: 0.00121265
Iteration 10/25 | Loss: 0.00121265
Iteration 11/25 | Loss: 0.00121265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001212647301144898, 0.001212647301144898, 0.001212647301144898, 0.001212647301144898, 0.001212647301144898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001212647301144898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64993858
Iteration 2/25 | Loss: 0.00219946
Iteration 3/25 | Loss: 0.00219945
Iteration 4/25 | Loss: 0.00219945
Iteration 5/25 | Loss: 0.00219945
Iteration 6/25 | Loss: 0.00219945
Iteration 7/25 | Loss: 0.00219945
Iteration 8/25 | Loss: 0.00219945
Iteration 9/25 | Loss: 0.00219945
Iteration 10/25 | Loss: 0.00219945
Iteration 11/25 | Loss: 0.00219945
Iteration 12/25 | Loss: 0.00219945
Iteration 13/25 | Loss: 0.00219945
Iteration 14/25 | Loss: 0.00219945
Iteration 15/25 | Loss: 0.00219945
Iteration 16/25 | Loss: 0.00219945
Iteration 17/25 | Loss: 0.00219945
Iteration 18/25 | Loss: 0.00219945
Iteration 19/25 | Loss: 0.00219945
Iteration 20/25 | Loss: 0.00219945
Iteration 21/25 | Loss: 0.00219945
Iteration 22/25 | Loss: 0.00219945
Iteration 23/25 | Loss: 0.00219945
Iteration 24/25 | Loss: 0.00219945
Iteration 25/25 | Loss: 0.00219945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219945
Iteration 2/1000 | Loss: 0.00002077
Iteration 3/1000 | Loss: 0.00001399
Iteration 4/1000 | Loss: 0.00001210
Iteration 5/1000 | Loss: 0.00001091
Iteration 6/1000 | Loss: 0.00001030
Iteration 7/1000 | Loss: 0.00000976
Iteration 8/1000 | Loss: 0.00000942
Iteration 9/1000 | Loss: 0.00000919
Iteration 10/1000 | Loss: 0.00000891
Iteration 11/1000 | Loss: 0.00000875
Iteration 12/1000 | Loss: 0.00000871
Iteration 13/1000 | Loss: 0.00000867
Iteration 14/1000 | Loss: 0.00000861
Iteration 15/1000 | Loss: 0.00000856
Iteration 16/1000 | Loss: 0.00000855
Iteration 17/1000 | Loss: 0.00000854
Iteration 18/1000 | Loss: 0.00000853
Iteration 19/1000 | Loss: 0.00000850
Iteration 20/1000 | Loss: 0.00000849
Iteration 21/1000 | Loss: 0.00000849
Iteration 22/1000 | Loss: 0.00000848
Iteration 23/1000 | Loss: 0.00000848
Iteration 24/1000 | Loss: 0.00000848
Iteration 25/1000 | Loss: 0.00000847
Iteration 26/1000 | Loss: 0.00000847
Iteration 27/1000 | Loss: 0.00000846
Iteration 28/1000 | Loss: 0.00000845
Iteration 29/1000 | Loss: 0.00000844
Iteration 30/1000 | Loss: 0.00000843
Iteration 31/1000 | Loss: 0.00000842
Iteration 32/1000 | Loss: 0.00000842
Iteration 33/1000 | Loss: 0.00000840
Iteration 34/1000 | Loss: 0.00000840
Iteration 35/1000 | Loss: 0.00000840
Iteration 36/1000 | Loss: 0.00000838
Iteration 37/1000 | Loss: 0.00000829
Iteration 38/1000 | Loss: 0.00000828
Iteration 39/1000 | Loss: 0.00000826
Iteration 40/1000 | Loss: 0.00000822
Iteration 41/1000 | Loss: 0.00000820
Iteration 42/1000 | Loss: 0.00000817
Iteration 43/1000 | Loss: 0.00000817
Iteration 44/1000 | Loss: 0.00000814
Iteration 45/1000 | Loss: 0.00000814
Iteration 46/1000 | Loss: 0.00000813
Iteration 47/1000 | Loss: 0.00000813
Iteration 48/1000 | Loss: 0.00000812
Iteration 49/1000 | Loss: 0.00000812
Iteration 50/1000 | Loss: 0.00000812
Iteration 51/1000 | Loss: 0.00000812
Iteration 52/1000 | Loss: 0.00000812
Iteration 53/1000 | Loss: 0.00000812
Iteration 54/1000 | Loss: 0.00000811
Iteration 55/1000 | Loss: 0.00000811
Iteration 56/1000 | Loss: 0.00000811
Iteration 57/1000 | Loss: 0.00000811
Iteration 58/1000 | Loss: 0.00000811
Iteration 59/1000 | Loss: 0.00000811
Iteration 60/1000 | Loss: 0.00000811
Iteration 61/1000 | Loss: 0.00000810
Iteration 62/1000 | Loss: 0.00000810
Iteration 63/1000 | Loss: 0.00000809
Iteration 64/1000 | Loss: 0.00000809
Iteration 65/1000 | Loss: 0.00000809
Iteration 66/1000 | Loss: 0.00000808
Iteration 67/1000 | Loss: 0.00000808
Iteration 68/1000 | Loss: 0.00000808
Iteration 69/1000 | Loss: 0.00000808
Iteration 70/1000 | Loss: 0.00000808
Iteration 71/1000 | Loss: 0.00000808
Iteration 72/1000 | Loss: 0.00000808
Iteration 73/1000 | Loss: 0.00000807
Iteration 74/1000 | Loss: 0.00000807
Iteration 75/1000 | Loss: 0.00000807
Iteration 76/1000 | Loss: 0.00000806
Iteration 77/1000 | Loss: 0.00000806
Iteration 78/1000 | Loss: 0.00000806
Iteration 79/1000 | Loss: 0.00000806
Iteration 80/1000 | Loss: 0.00000806
Iteration 81/1000 | Loss: 0.00000806
Iteration 82/1000 | Loss: 0.00000806
Iteration 83/1000 | Loss: 0.00000805
Iteration 84/1000 | Loss: 0.00000805
Iteration 85/1000 | Loss: 0.00000805
Iteration 86/1000 | Loss: 0.00000805
Iteration 87/1000 | Loss: 0.00000805
Iteration 88/1000 | Loss: 0.00000805
Iteration 89/1000 | Loss: 0.00000805
Iteration 90/1000 | Loss: 0.00000804
Iteration 91/1000 | Loss: 0.00000804
Iteration 92/1000 | Loss: 0.00000804
Iteration 93/1000 | Loss: 0.00000804
Iteration 94/1000 | Loss: 0.00000804
Iteration 95/1000 | Loss: 0.00000804
Iteration 96/1000 | Loss: 0.00000804
Iteration 97/1000 | Loss: 0.00000804
Iteration 98/1000 | Loss: 0.00000804
Iteration 99/1000 | Loss: 0.00000804
Iteration 100/1000 | Loss: 0.00000804
Iteration 101/1000 | Loss: 0.00000804
Iteration 102/1000 | Loss: 0.00000803
Iteration 103/1000 | Loss: 0.00000803
Iteration 104/1000 | Loss: 0.00000803
Iteration 105/1000 | Loss: 0.00000803
Iteration 106/1000 | Loss: 0.00000803
Iteration 107/1000 | Loss: 0.00000803
Iteration 108/1000 | Loss: 0.00000803
Iteration 109/1000 | Loss: 0.00000803
Iteration 110/1000 | Loss: 0.00000802
Iteration 111/1000 | Loss: 0.00000802
Iteration 112/1000 | Loss: 0.00000802
Iteration 113/1000 | Loss: 0.00000802
Iteration 114/1000 | Loss: 0.00000802
Iteration 115/1000 | Loss: 0.00000802
Iteration 116/1000 | Loss: 0.00000802
Iteration 117/1000 | Loss: 0.00000802
Iteration 118/1000 | Loss: 0.00000802
Iteration 119/1000 | Loss: 0.00000802
Iteration 120/1000 | Loss: 0.00000802
Iteration 121/1000 | Loss: 0.00000802
Iteration 122/1000 | Loss: 0.00000802
Iteration 123/1000 | Loss: 0.00000801
Iteration 124/1000 | Loss: 0.00000801
Iteration 125/1000 | Loss: 0.00000801
Iteration 126/1000 | Loss: 0.00000801
Iteration 127/1000 | Loss: 0.00000801
Iteration 128/1000 | Loss: 0.00000801
Iteration 129/1000 | Loss: 0.00000801
Iteration 130/1000 | Loss: 0.00000801
Iteration 131/1000 | Loss: 0.00000800
Iteration 132/1000 | Loss: 0.00000800
Iteration 133/1000 | Loss: 0.00000800
Iteration 134/1000 | Loss: 0.00000800
Iteration 135/1000 | Loss: 0.00000800
Iteration 136/1000 | Loss: 0.00000800
Iteration 137/1000 | Loss: 0.00000800
Iteration 138/1000 | Loss: 0.00000800
Iteration 139/1000 | Loss: 0.00000799
Iteration 140/1000 | Loss: 0.00000799
Iteration 141/1000 | Loss: 0.00000799
Iteration 142/1000 | Loss: 0.00000799
Iteration 143/1000 | Loss: 0.00000799
Iteration 144/1000 | Loss: 0.00000799
Iteration 145/1000 | Loss: 0.00000798
Iteration 146/1000 | Loss: 0.00000798
Iteration 147/1000 | Loss: 0.00000798
Iteration 148/1000 | Loss: 0.00000798
Iteration 149/1000 | Loss: 0.00000798
Iteration 150/1000 | Loss: 0.00000798
Iteration 151/1000 | Loss: 0.00000798
Iteration 152/1000 | Loss: 0.00000797
Iteration 153/1000 | Loss: 0.00000797
Iteration 154/1000 | Loss: 0.00000797
Iteration 155/1000 | Loss: 0.00000797
Iteration 156/1000 | Loss: 0.00000797
Iteration 157/1000 | Loss: 0.00000797
Iteration 158/1000 | Loss: 0.00000797
Iteration 159/1000 | Loss: 0.00000797
Iteration 160/1000 | Loss: 0.00000797
Iteration 161/1000 | Loss: 0.00000797
Iteration 162/1000 | Loss: 0.00000797
Iteration 163/1000 | Loss: 0.00000797
Iteration 164/1000 | Loss: 0.00000797
Iteration 165/1000 | Loss: 0.00000797
Iteration 166/1000 | Loss: 0.00000797
Iteration 167/1000 | Loss: 0.00000797
Iteration 168/1000 | Loss: 0.00000797
Iteration 169/1000 | Loss: 0.00000797
Iteration 170/1000 | Loss: 0.00000797
Iteration 171/1000 | Loss: 0.00000796
Iteration 172/1000 | Loss: 0.00000796
Iteration 173/1000 | Loss: 0.00000796
Iteration 174/1000 | Loss: 0.00000796
Iteration 175/1000 | Loss: 0.00000796
Iteration 176/1000 | Loss: 0.00000796
Iteration 177/1000 | Loss: 0.00000796
Iteration 178/1000 | Loss: 0.00000796
Iteration 179/1000 | Loss: 0.00000796
Iteration 180/1000 | Loss: 0.00000796
Iteration 181/1000 | Loss: 0.00000796
Iteration 182/1000 | Loss: 0.00000796
Iteration 183/1000 | Loss: 0.00000796
Iteration 184/1000 | Loss: 0.00000796
Iteration 185/1000 | Loss: 0.00000796
Iteration 186/1000 | Loss: 0.00000796
Iteration 187/1000 | Loss: 0.00000796
Iteration 188/1000 | Loss: 0.00000796
Iteration 189/1000 | Loss: 0.00000796
Iteration 190/1000 | Loss: 0.00000796
Iteration 191/1000 | Loss: 0.00000796
Iteration 192/1000 | Loss: 0.00000796
Iteration 193/1000 | Loss: 0.00000796
Iteration 194/1000 | Loss: 0.00000796
Iteration 195/1000 | Loss: 0.00000796
Iteration 196/1000 | Loss: 0.00000796
Iteration 197/1000 | Loss: 0.00000796
Iteration 198/1000 | Loss: 0.00000796
Iteration 199/1000 | Loss: 0.00000796
Iteration 200/1000 | Loss: 0.00000796
Iteration 201/1000 | Loss: 0.00000796
Iteration 202/1000 | Loss: 0.00000796
Iteration 203/1000 | Loss: 0.00000796
Iteration 204/1000 | Loss: 0.00000796
Iteration 205/1000 | Loss: 0.00000796
Iteration 206/1000 | Loss: 0.00000796
Iteration 207/1000 | Loss: 0.00000796
Iteration 208/1000 | Loss: 0.00000796
Iteration 209/1000 | Loss: 0.00000796
Iteration 210/1000 | Loss: 0.00000796
Iteration 211/1000 | Loss: 0.00000796
Iteration 212/1000 | Loss: 0.00000796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [7.957801244629081e-06, 7.957801244629081e-06, 7.957801244629081e-06, 7.957801244629081e-06, 7.957801244629081e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.957801244629081e-06

Optimization complete. Final v2v error: 2.443547487258911 mm

Highest mean error: 2.8786656856536865 mm for frame 77

Lowest mean error: 2.3500826358795166 mm for frame 113

Saving results

Total time: 42.83185791969299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00282883
Iteration 2/25 | Loss: 0.00142938
Iteration 3/25 | Loss: 0.00126083
Iteration 4/25 | Loss: 0.00123873
Iteration 5/25 | Loss: 0.00123205
Iteration 6/25 | Loss: 0.00122905
Iteration 7/25 | Loss: 0.00122874
Iteration 8/25 | Loss: 0.00122874
Iteration 9/25 | Loss: 0.00122874
Iteration 10/25 | Loss: 0.00122874
Iteration 11/25 | Loss: 0.00122874
Iteration 12/25 | Loss: 0.00122874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012287413701415062, 0.0012287413701415062, 0.0012287413701415062, 0.0012287413701415062, 0.0012287413701415062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012287413701415062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17934811
Iteration 2/25 | Loss: 0.00322852
Iteration 3/25 | Loss: 0.00322852
Iteration 4/25 | Loss: 0.00322852
Iteration 5/25 | Loss: 0.00322852
Iteration 6/25 | Loss: 0.00322852
Iteration 7/25 | Loss: 0.00322852
Iteration 8/25 | Loss: 0.00322852
Iteration 9/25 | Loss: 0.00322852
Iteration 10/25 | Loss: 0.00322852
Iteration 11/25 | Loss: 0.00322852
Iteration 12/25 | Loss: 0.00322852
Iteration 13/25 | Loss: 0.00322852
Iteration 14/25 | Loss: 0.00322852
Iteration 15/25 | Loss: 0.00322852
Iteration 16/25 | Loss: 0.00322852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003228520043194294, 0.003228520043194294, 0.003228520043194294, 0.003228520043194294, 0.003228520043194294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003228520043194294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322852
Iteration 2/1000 | Loss: 0.00004060
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00001872
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001541
Iteration 7/1000 | Loss: 0.00001453
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001355
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001222
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001206
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001198
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001169
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001162
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.1591067050176207e-05, 1.1591067050176207e-05, 1.1591067050176207e-05, 1.1591067050176207e-05, 1.1591067050176207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1591067050176207e-05

Optimization complete. Final v2v error: 2.9507882595062256 mm

Highest mean error: 3.165536642074585 mm for frame 127

Lowest mean error: 2.5922672748565674 mm for frame 0

Saving results

Total time: 45.006134271621704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789717
Iteration 2/25 | Loss: 0.00133610
Iteration 3/25 | Loss: 0.00124458
Iteration 4/25 | Loss: 0.00123280
Iteration 5/25 | Loss: 0.00122941
Iteration 6/25 | Loss: 0.00122941
Iteration 7/25 | Loss: 0.00122941
Iteration 8/25 | Loss: 0.00122941
Iteration 9/25 | Loss: 0.00122941
Iteration 10/25 | Loss: 0.00122941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012294147163629532, 0.0012294147163629532, 0.0012294147163629532, 0.0012294147163629532, 0.0012294147163629532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012294147163629532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22357774
Iteration 2/25 | Loss: 0.00258068
Iteration 3/25 | Loss: 0.00258068
Iteration 4/25 | Loss: 0.00258067
Iteration 5/25 | Loss: 0.00258067
Iteration 6/25 | Loss: 0.00258067
Iteration 7/25 | Loss: 0.00258067
Iteration 8/25 | Loss: 0.00258067
Iteration 9/25 | Loss: 0.00258067
Iteration 10/25 | Loss: 0.00258067
Iteration 11/25 | Loss: 0.00258067
Iteration 12/25 | Loss: 0.00258067
Iteration 13/25 | Loss: 0.00258067
Iteration 14/25 | Loss: 0.00258067
Iteration 15/25 | Loss: 0.00258067
Iteration 16/25 | Loss: 0.00258067
Iteration 17/25 | Loss: 0.00258067
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0025806715711951256, 0.0025806715711951256, 0.0025806715711951256, 0.0025806715711951256, 0.0025806715711951256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025806715711951256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258067
Iteration 2/1000 | Loss: 0.00003582
Iteration 3/1000 | Loss: 0.00002247
Iteration 4/1000 | Loss: 0.00001708
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001229
Iteration 9/1000 | Loss: 0.00001193
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001120
Iteration 13/1000 | Loss: 0.00001114
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001105
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001101
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001097
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001096
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001085
Iteration 34/1000 | Loss: 0.00001081
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001077
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001073
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001064
Iteration 44/1000 | Loss: 0.00001063
Iteration 45/1000 | Loss: 0.00001063
Iteration 46/1000 | Loss: 0.00001063
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001062
Iteration 49/1000 | Loss: 0.00001061
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001060
Iteration 52/1000 | Loss: 0.00001060
Iteration 53/1000 | Loss: 0.00001059
Iteration 54/1000 | Loss: 0.00001059
Iteration 55/1000 | Loss: 0.00001058
Iteration 56/1000 | Loss: 0.00001057
Iteration 57/1000 | Loss: 0.00001056
Iteration 58/1000 | Loss: 0.00001052
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001050
Iteration 61/1000 | Loss: 0.00001050
Iteration 62/1000 | Loss: 0.00001049
Iteration 63/1000 | Loss: 0.00001049
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001048
Iteration 66/1000 | Loss: 0.00001048
Iteration 67/1000 | Loss: 0.00001048
Iteration 68/1000 | Loss: 0.00001047
Iteration 69/1000 | Loss: 0.00001047
Iteration 70/1000 | Loss: 0.00001047
Iteration 71/1000 | Loss: 0.00001046
Iteration 72/1000 | Loss: 0.00001046
Iteration 73/1000 | Loss: 0.00001046
Iteration 74/1000 | Loss: 0.00001044
Iteration 75/1000 | Loss: 0.00001044
Iteration 76/1000 | Loss: 0.00001044
Iteration 77/1000 | Loss: 0.00001044
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001044
Iteration 80/1000 | Loss: 0.00001043
Iteration 81/1000 | Loss: 0.00001043
Iteration 82/1000 | Loss: 0.00001043
Iteration 83/1000 | Loss: 0.00001043
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001042
Iteration 87/1000 | Loss: 0.00001042
Iteration 88/1000 | Loss: 0.00001042
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001041
Iteration 92/1000 | Loss: 0.00001041
Iteration 93/1000 | Loss: 0.00001041
Iteration 94/1000 | Loss: 0.00001041
Iteration 95/1000 | Loss: 0.00001041
Iteration 96/1000 | Loss: 0.00001041
Iteration 97/1000 | Loss: 0.00001040
Iteration 98/1000 | Loss: 0.00001040
Iteration 99/1000 | Loss: 0.00001040
Iteration 100/1000 | Loss: 0.00001040
Iteration 101/1000 | Loss: 0.00001039
Iteration 102/1000 | Loss: 0.00001039
Iteration 103/1000 | Loss: 0.00001039
Iteration 104/1000 | Loss: 0.00001038
Iteration 105/1000 | Loss: 0.00001038
Iteration 106/1000 | Loss: 0.00001038
Iteration 107/1000 | Loss: 0.00001038
Iteration 108/1000 | Loss: 0.00001038
Iteration 109/1000 | Loss: 0.00001037
Iteration 110/1000 | Loss: 0.00001037
Iteration 111/1000 | Loss: 0.00001037
Iteration 112/1000 | Loss: 0.00001037
Iteration 113/1000 | Loss: 0.00001037
Iteration 114/1000 | Loss: 0.00001037
Iteration 115/1000 | Loss: 0.00001037
Iteration 116/1000 | Loss: 0.00001037
Iteration 117/1000 | Loss: 0.00001037
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001035
Iteration 122/1000 | Loss: 0.00001035
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001035
Iteration 127/1000 | Loss: 0.00001034
Iteration 128/1000 | Loss: 0.00001034
Iteration 129/1000 | Loss: 0.00001034
Iteration 130/1000 | Loss: 0.00001034
Iteration 131/1000 | Loss: 0.00001034
Iteration 132/1000 | Loss: 0.00001034
Iteration 133/1000 | Loss: 0.00001034
Iteration 134/1000 | Loss: 0.00001034
Iteration 135/1000 | Loss: 0.00001033
Iteration 136/1000 | Loss: 0.00001033
Iteration 137/1000 | Loss: 0.00001033
Iteration 138/1000 | Loss: 0.00001033
Iteration 139/1000 | Loss: 0.00001033
Iteration 140/1000 | Loss: 0.00001033
Iteration 141/1000 | Loss: 0.00001032
Iteration 142/1000 | Loss: 0.00001032
Iteration 143/1000 | Loss: 0.00001032
Iteration 144/1000 | Loss: 0.00001032
Iteration 145/1000 | Loss: 0.00001031
Iteration 146/1000 | Loss: 0.00001031
Iteration 147/1000 | Loss: 0.00001031
Iteration 148/1000 | Loss: 0.00001031
Iteration 149/1000 | Loss: 0.00001031
Iteration 150/1000 | Loss: 0.00001030
Iteration 151/1000 | Loss: 0.00001030
Iteration 152/1000 | Loss: 0.00001030
Iteration 153/1000 | Loss: 0.00001030
Iteration 154/1000 | Loss: 0.00001029
Iteration 155/1000 | Loss: 0.00001029
Iteration 156/1000 | Loss: 0.00001029
Iteration 157/1000 | Loss: 0.00001029
Iteration 158/1000 | Loss: 0.00001029
Iteration 159/1000 | Loss: 0.00001029
Iteration 160/1000 | Loss: 0.00001028
Iteration 161/1000 | Loss: 0.00001027
Iteration 162/1000 | Loss: 0.00001027
Iteration 163/1000 | Loss: 0.00001027
Iteration 164/1000 | Loss: 0.00001027
Iteration 165/1000 | Loss: 0.00001027
Iteration 166/1000 | Loss: 0.00001027
Iteration 167/1000 | Loss: 0.00001027
Iteration 168/1000 | Loss: 0.00001027
Iteration 169/1000 | Loss: 0.00001027
Iteration 170/1000 | Loss: 0.00001026
Iteration 171/1000 | Loss: 0.00001026
Iteration 172/1000 | Loss: 0.00001026
Iteration 173/1000 | Loss: 0.00001026
Iteration 174/1000 | Loss: 0.00001026
Iteration 175/1000 | Loss: 0.00001026
Iteration 176/1000 | Loss: 0.00001026
Iteration 177/1000 | Loss: 0.00001025
Iteration 178/1000 | Loss: 0.00001025
Iteration 179/1000 | Loss: 0.00001024
Iteration 180/1000 | Loss: 0.00001024
Iteration 181/1000 | Loss: 0.00001024
Iteration 182/1000 | Loss: 0.00001024
Iteration 183/1000 | Loss: 0.00001023
Iteration 184/1000 | Loss: 0.00001023
Iteration 185/1000 | Loss: 0.00001022
Iteration 186/1000 | Loss: 0.00001022
Iteration 187/1000 | Loss: 0.00001022
Iteration 188/1000 | Loss: 0.00001022
Iteration 189/1000 | Loss: 0.00001022
Iteration 190/1000 | Loss: 0.00001022
Iteration 191/1000 | Loss: 0.00001021
Iteration 192/1000 | Loss: 0.00001021
Iteration 193/1000 | Loss: 0.00001021
Iteration 194/1000 | Loss: 0.00001021
Iteration 195/1000 | Loss: 0.00001021
Iteration 196/1000 | Loss: 0.00001020
Iteration 197/1000 | Loss: 0.00001020
Iteration 198/1000 | Loss: 0.00001020
Iteration 199/1000 | Loss: 0.00001020
Iteration 200/1000 | Loss: 0.00001020
Iteration 201/1000 | Loss: 0.00001020
Iteration 202/1000 | Loss: 0.00001020
Iteration 203/1000 | Loss: 0.00001020
Iteration 204/1000 | Loss: 0.00001020
Iteration 205/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.0199620191997383e-05, 1.0199620191997383e-05, 1.0199620191997383e-05, 1.0199620191997383e-05, 1.0199620191997383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0199620191997383e-05

Optimization complete. Final v2v error: 2.767561674118042 mm

Highest mean error: 3.3150649070739746 mm for frame 239

Lowest mean error: 2.489325761795044 mm for frame 174

Saving results

Total time: 51.59590768814087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798233
Iteration 2/25 | Loss: 0.00136404
Iteration 3/25 | Loss: 0.00128429
Iteration 4/25 | Loss: 0.00127453
Iteration 5/25 | Loss: 0.00127335
Iteration 6/25 | Loss: 0.00127335
Iteration 7/25 | Loss: 0.00127335
Iteration 8/25 | Loss: 0.00127335
Iteration 9/25 | Loss: 0.00127335
Iteration 10/25 | Loss: 0.00127335
Iteration 11/25 | Loss: 0.00127335
Iteration 12/25 | Loss: 0.00127335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012733463663607836, 0.0012733463663607836, 0.0012733463663607836, 0.0012733463663607836, 0.0012733463663607836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012733463663607836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20359683
Iteration 2/25 | Loss: 0.00197782
Iteration 3/25 | Loss: 0.00197782
Iteration 4/25 | Loss: 0.00197782
Iteration 5/25 | Loss: 0.00197782
Iteration 6/25 | Loss: 0.00197782
Iteration 7/25 | Loss: 0.00197782
Iteration 8/25 | Loss: 0.00197782
Iteration 9/25 | Loss: 0.00197782
Iteration 10/25 | Loss: 0.00197782
Iteration 11/25 | Loss: 0.00197782
Iteration 12/25 | Loss: 0.00197782
Iteration 13/25 | Loss: 0.00197782
Iteration 14/25 | Loss: 0.00197782
Iteration 15/25 | Loss: 0.00197782
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001977820647880435, 0.001977820647880435, 0.001977820647880435, 0.001977820647880435, 0.001977820647880435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001977820647880435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197782
Iteration 2/1000 | Loss: 0.00002280
Iteration 3/1000 | Loss: 0.00001755
Iteration 4/1000 | Loss: 0.00001613
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001420
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001319
Iteration 16/1000 | Loss: 0.00001303
Iteration 17/1000 | Loss: 0.00001296
Iteration 18/1000 | Loss: 0.00001296
Iteration 19/1000 | Loss: 0.00001295
Iteration 20/1000 | Loss: 0.00001294
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001225
Iteration 48/1000 | Loss: 0.00001225
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001216
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001214
Iteration 95/1000 | Loss: 0.00001214
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.2139205864514224e-05, 1.2139205864514224e-05, 1.2139205864514224e-05, 1.2139205864514224e-05, 1.2139205864514224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2139205864514224e-05

Optimization complete. Final v2v error: 2.965632677078247 mm

Highest mean error: 3.118825674057007 mm for frame 142

Lowest mean error: 2.862865447998047 mm for frame 4

Saving results

Total time: 41.19560122489929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348618
Iteration 2/25 | Loss: 0.00155738
Iteration 3/25 | Loss: 0.00129927
Iteration 4/25 | Loss: 0.00126216
Iteration 5/25 | Loss: 0.00125567
Iteration 6/25 | Loss: 0.00125359
Iteration 7/25 | Loss: 0.00125288
Iteration 8/25 | Loss: 0.00125288
Iteration 9/25 | Loss: 0.00125288
Iteration 10/25 | Loss: 0.00125288
Iteration 11/25 | Loss: 0.00125288
Iteration 12/25 | Loss: 0.00125288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012528827646747231, 0.0012528827646747231, 0.0012528827646747231, 0.0012528827646747231, 0.0012528827646747231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012528827646747231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24222064
Iteration 2/25 | Loss: 0.00265759
Iteration 3/25 | Loss: 0.00265759
Iteration 4/25 | Loss: 0.00265759
Iteration 5/25 | Loss: 0.00265759
Iteration 6/25 | Loss: 0.00265759
Iteration 7/25 | Loss: 0.00265759
Iteration 8/25 | Loss: 0.00265759
Iteration 9/25 | Loss: 0.00265759
Iteration 10/25 | Loss: 0.00265758
Iteration 11/25 | Loss: 0.00265758
Iteration 12/25 | Loss: 0.00265758
Iteration 13/25 | Loss: 0.00265758
Iteration 14/25 | Loss: 0.00265758
Iteration 15/25 | Loss: 0.00265758
Iteration 16/25 | Loss: 0.00265758
Iteration 17/25 | Loss: 0.00265758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026575843803584576, 0.0026575843803584576, 0.0026575843803584576, 0.0026575843803584576, 0.0026575843803584576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026575843803584576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265758
Iteration 2/1000 | Loss: 0.00004505
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00001679
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001293
Iteration 9/1000 | Loss: 0.00001265
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001207
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001188
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001187
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001168
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001166
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001165
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001165
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001164
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001164
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001162
Iteration 81/1000 | Loss: 0.00001162
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001158
Iteration 105/1000 | Loss: 0.00001158
Iteration 106/1000 | Loss: 0.00001158
Iteration 107/1000 | Loss: 0.00001158
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001156
Iteration 126/1000 | Loss: 0.00001156
Iteration 127/1000 | Loss: 0.00001156
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001156
Iteration 131/1000 | Loss: 0.00001156
Iteration 132/1000 | Loss: 0.00001156
Iteration 133/1000 | Loss: 0.00001156
Iteration 134/1000 | Loss: 0.00001156
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001155
Iteration 137/1000 | Loss: 0.00001155
Iteration 138/1000 | Loss: 0.00001154
Iteration 139/1000 | Loss: 0.00001154
Iteration 140/1000 | Loss: 0.00001154
Iteration 141/1000 | Loss: 0.00001154
Iteration 142/1000 | Loss: 0.00001154
Iteration 143/1000 | Loss: 0.00001154
Iteration 144/1000 | Loss: 0.00001153
Iteration 145/1000 | Loss: 0.00001153
Iteration 146/1000 | Loss: 0.00001153
Iteration 147/1000 | Loss: 0.00001153
Iteration 148/1000 | Loss: 0.00001153
Iteration 149/1000 | Loss: 0.00001153
Iteration 150/1000 | Loss: 0.00001153
Iteration 151/1000 | Loss: 0.00001152
Iteration 152/1000 | Loss: 0.00001152
Iteration 153/1000 | Loss: 0.00001152
Iteration 154/1000 | Loss: 0.00001152
Iteration 155/1000 | Loss: 0.00001152
Iteration 156/1000 | Loss: 0.00001152
Iteration 157/1000 | Loss: 0.00001152
Iteration 158/1000 | Loss: 0.00001152
Iteration 159/1000 | Loss: 0.00001152
Iteration 160/1000 | Loss: 0.00001151
Iteration 161/1000 | Loss: 0.00001151
Iteration 162/1000 | Loss: 0.00001151
Iteration 163/1000 | Loss: 0.00001151
Iteration 164/1000 | Loss: 0.00001151
Iteration 165/1000 | Loss: 0.00001151
Iteration 166/1000 | Loss: 0.00001151
Iteration 167/1000 | Loss: 0.00001151
Iteration 168/1000 | Loss: 0.00001151
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001150
Iteration 171/1000 | Loss: 0.00001150
Iteration 172/1000 | Loss: 0.00001150
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001150
Iteration 175/1000 | Loss: 0.00001150
Iteration 176/1000 | Loss: 0.00001150
Iteration 177/1000 | Loss: 0.00001150
Iteration 178/1000 | Loss: 0.00001150
Iteration 179/1000 | Loss: 0.00001150
Iteration 180/1000 | Loss: 0.00001150
Iteration 181/1000 | Loss: 0.00001149
Iteration 182/1000 | Loss: 0.00001149
Iteration 183/1000 | Loss: 0.00001149
Iteration 184/1000 | Loss: 0.00001149
Iteration 185/1000 | Loss: 0.00001149
Iteration 186/1000 | Loss: 0.00001149
Iteration 187/1000 | Loss: 0.00001149
Iteration 188/1000 | Loss: 0.00001149
Iteration 189/1000 | Loss: 0.00001149
Iteration 190/1000 | Loss: 0.00001149
Iteration 191/1000 | Loss: 0.00001149
Iteration 192/1000 | Loss: 0.00001149
Iteration 193/1000 | Loss: 0.00001148
Iteration 194/1000 | Loss: 0.00001148
Iteration 195/1000 | Loss: 0.00001148
Iteration 196/1000 | Loss: 0.00001148
Iteration 197/1000 | Loss: 0.00001148
Iteration 198/1000 | Loss: 0.00001148
Iteration 199/1000 | Loss: 0.00001148
Iteration 200/1000 | Loss: 0.00001148
Iteration 201/1000 | Loss: 0.00001148
Iteration 202/1000 | Loss: 0.00001148
Iteration 203/1000 | Loss: 0.00001148
Iteration 204/1000 | Loss: 0.00001148
Iteration 205/1000 | Loss: 0.00001148
Iteration 206/1000 | Loss: 0.00001147
Iteration 207/1000 | Loss: 0.00001147
Iteration 208/1000 | Loss: 0.00001147
Iteration 209/1000 | Loss: 0.00001147
Iteration 210/1000 | Loss: 0.00001147
Iteration 211/1000 | Loss: 0.00001147
Iteration 212/1000 | Loss: 0.00001147
Iteration 213/1000 | Loss: 0.00001147
Iteration 214/1000 | Loss: 0.00001147
Iteration 215/1000 | Loss: 0.00001147
Iteration 216/1000 | Loss: 0.00001147
Iteration 217/1000 | Loss: 0.00001147
Iteration 218/1000 | Loss: 0.00001146
Iteration 219/1000 | Loss: 0.00001146
Iteration 220/1000 | Loss: 0.00001146
Iteration 221/1000 | Loss: 0.00001146
Iteration 222/1000 | Loss: 0.00001146
Iteration 223/1000 | Loss: 0.00001146
Iteration 224/1000 | Loss: 0.00001146
Iteration 225/1000 | Loss: 0.00001146
Iteration 226/1000 | Loss: 0.00001146
Iteration 227/1000 | Loss: 0.00001146
Iteration 228/1000 | Loss: 0.00001146
Iteration 229/1000 | Loss: 0.00001145
Iteration 230/1000 | Loss: 0.00001145
Iteration 231/1000 | Loss: 0.00001145
Iteration 232/1000 | Loss: 0.00001145
Iteration 233/1000 | Loss: 0.00001145
Iteration 234/1000 | Loss: 0.00001145
Iteration 235/1000 | Loss: 0.00001145
Iteration 236/1000 | Loss: 0.00001145
Iteration 237/1000 | Loss: 0.00001145
Iteration 238/1000 | Loss: 0.00001145
Iteration 239/1000 | Loss: 0.00001145
Iteration 240/1000 | Loss: 0.00001145
Iteration 241/1000 | Loss: 0.00001145
Iteration 242/1000 | Loss: 0.00001145
Iteration 243/1000 | Loss: 0.00001145
Iteration 244/1000 | Loss: 0.00001145
Iteration 245/1000 | Loss: 0.00001145
Iteration 246/1000 | Loss: 0.00001145
Iteration 247/1000 | Loss: 0.00001145
Iteration 248/1000 | Loss: 0.00001145
Iteration 249/1000 | Loss: 0.00001145
Iteration 250/1000 | Loss: 0.00001145
Iteration 251/1000 | Loss: 0.00001145
Iteration 252/1000 | Loss: 0.00001145
Iteration 253/1000 | Loss: 0.00001145
Iteration 254/1000 | Loss: 0.00001145
Iteration 255/1000 | Loss: 0.00001145
Iteration 256/1000 | Loss: 0.00001145
Iteration 257/1000 | Loss: 0.00001145
Iteration 258/1000 | Loss: 0.00001145
Iteration 259/1000 | Loss: 0.00001145
Iteration 260/1000 | Loss: 0.00001145
Iteration 261/1000 | Loss: 0.00001145
Iteration 262/1000 | Loss: 0.00001145
Iteration 263/1000 | Loss: 0.00001145
Iteration 264/1000 | Loss: 0.00001145
Iteration 265/1000 | Loss: 0.00001145
Iteration 266/1000 | Loss: 0.00001145
Iteration 267/1000 | Loss: 0.00001145
Iteration 268/1000 | Loss: 0.00001145
Iteration 269/1000 | Loss: 0.00001145
Iteration 270/1000 | Loss: 0.00001145
Iteration 271/1000 | Loss: 0.00001145
Iteration 272/1000 | Loss: 0.00001145
Iteration 273/1000 | Loss: 0.00001145
Iteration 274/1000 | Loss: 0.00001145
Iteration 275/1000 | Loss: 0.00001145
Iteration 276/1000 | Loss: 0.00001145
Iteration 277/1000 | Loss: 0.00001145
Iteration 278/1000 | Loss: 0.00001145
Iteration 279/1000 | Loss: 0.00001145
Iteration 280/1000 | Loss: 0.00001145
Iteration 281/1000 | Loss: 0.00001145
Iteration 282/1000 | Loss: 0.00001145
Iteration 283/1000 | Loss: 0.00001145
Iteration 284/1000 | Loss: 0.00001145
Iteration 285/1000 | Loss: 0.00001145
Iteration 286/1000 | Loss: 0.00001145
Iteration 287/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [1.1451271348050795e-05, 1.1451271348050795e-05, 1.1451271348050795e-05, 1.1451271348050795e-05, 1.1451271348050795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1451271348050795e-05

Optimization complete. Final v2v error: 2.9195191860198975 mm

Highest mean error: 3.295386791229248 mm for frame 38

Lowest mean error: 2.4679152965545654 mm for frame 9

Saving results

Total time: 44.25929236412048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463995
Iteration 2/25 | Loss: 0.00136515
Iteration 3/25 | Loss: 0.00129187
Iteration 4/25 | Loss: 0.00127596
Iteration 5/25 | Loss: 0.00127221
Iteration 6/25 | Loss: 0.00127221
Iteration 7/25 | Loss: 0.00127221
Iteration 8/25 | Loss: 0.00127221
Iteration 9/25 | Loss: 0.00127221
Iteration 10/25 | Loss: 0.00127221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012722088722512126, 0.0012722088722512126, 0.0012722088722512126, 0.0012722088722512126, 0.0012722088722512126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012722088722512126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23222589
Iteration 2/25 | Loss: 0.00190411
Iteration 3/25 | Loss: 0.00190410
Iteration 4/25 | Loss: 0.00190410
Iteration 5/25 | Loss: 0.00190410
Iteration 6/25 | Loss: 0.00190410
Iteration 7/25 | Loss: 0.00190410
Iteration 8/25 | Loss: 0.00190410
Iteration 9/25 | Loss: 0.00190410
Iteration 10/25 | Loss: 0.00190410
Iteration 11/25 | Loss: 0.00190410
Iteration 12/25 | Loss: 0.00190410
Iteration 13/25 | Loss: 0.00190410
Iteration 14/25 | Loss: 0.00190410
Iteration 15/25 | Loss: 0.00190410
Iteration 16/25 | Loss: 0.00190410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019040951738134027, 0.0019040951738134027, 0.0019040951738134027, 0.0019040951738134027, 0.0019040951738134027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019040951738134027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190410
Iteration 2/1000 | Loss: 0.00002412
Iteration 3/1000 | Loss: 0.00001820
Iteration 4/1000 | Loss: 0.00001645
Iteration 5/1000 | Loss: 0.00001541
Iteration 6/1000 | Loss: 0.00001486
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00001386
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001342
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001317
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001292
Iteration 17/1000 | Loss: 0.00001290
Iteration 18/1000 | Loss: 0.00001288
Iteration 19/1000 | Loss: 0.00001287
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001275
Iteration 29/1000 | Loss: 0.00001273
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001264
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001258
Iteration 38/1000 | Loss: 0.00001258
Iteration 39/1000 | Loss: 0.00001258
Iteration 40/1000 | Loss: 0.00001257
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001256
Iteration 43/1000 | Loss: 0.00001256
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001253
Iteration 49/1000 | Loss: 0.00001252
Iteration 50/1000 | Loss: 0.00001252
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001251
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001248
Iteration 69/1000 | Loss: 0.00001248
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001247
Iteration 72/1000 | Loss: 0.00001247
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001246
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001244
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001243
Iteration 92/1000 | Loss: 0.00001243
Iteration 93/1000 | Loss: 0.00001243
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001242
Iteration 100/1000 | Loss: 0.00001242
Iteration 101/1000 | Loss: 0.00001242
Iteration 102/1000 | Loss: 0.00001242
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001240
Iteration 106/1000 | Loss: 0.00001240
Iteration 107/1000 | Loss: 0.00001240
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001239
Iteration 110/1000 | Loss: 0.00001239
Iteration 111/1000 | Loss: 0.00001239
Iteration 112/1000 | Loss: 0.00001239
Iteration 113/1000 | Loss: 0.00001239
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001239
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001236
Iteration 126/1000 | Loss: 0.00001236
Iteration 127/1000 | Loss: 0.00001236
Iteration 128/1000 | Loss: 0.00001236
Iteration 129/1000 | Loss: 0.00001236
Iteration 130/1000 | Loss: 0.00001236
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001235
Iteration 134/1000 | Loss: 0.00001235
Iteration 135/1000 | Loss: 0.00001235
Iteration 136/1000 | Loss: 0.00001235
Iteration 137/1000 | Loss: 0.00001235
Iteration 138/1000 | Loss: 0.00001235
Iteration 139/1000 | Loss: 0.00001235
Iteration 140/1000 | Loss: 0.00001235
Iteration 141/1000 | Loss: 0.00001235
Iteration 142/1000 | Loss: 0.00001234
Iteration 143/1000 | Loss: 0.00001234
Iteration 144/1000 | Loss: 0.00001234
Iteration 145/1000 | Loss: 0.00001234
Iteration 146/1000 | Loss: 0.00001234
Iteration 147/1000 | Loss: 0.00001234
Iteration 148/1000 | Loss: 0.00001234
Iteration 149/1000 | Loss: 0.00001234
Iteration 150/1000 | Loss: 0.00001233
Iteration 151/1000 | Loss: 0.00001233
Iteration 152/1000 | Loss: 0.00001233
Iteration 153/1000 | Loss: 0.00001232
Iteration 154/1000 | Loss: 0.00001232
Iteration 155/1000 | Loss: 0.00001232
Iteration 156/1000 | Loss: 0.00001232
Iteration 157/1000 | Loss: 0.00001232
Iteration 158/1000 | Loss: 0.00001232
Iteration 159/1000 | Loss: 0.00001232
Iteration 160/1000 | Loss: 0.00001232
Iteration 161/1000 | Loss: 0.00001232
Iteration 162/1000 | Loss: 0.00001232
Iteration 163/1000 | Loss: 0.00001232
Iteration 164/1000 | Loss: 0.00001232
Iteration 165/1000 | Loss: 0.00001232
Iteration 166/1000 | Loss: 0.00001232
Iteration 167/1000 | Loss: 0.00001232
Iteration 168/1000 | Loss: 0.00001231
Iteration 169/1000 | Loss: 0.00001231
Iteration 170/1000 | Loss: 0.00001231
Iteration 171/1000 | Loss: 0.00001231
Iteration 172/1000 | Loss: 0.00001231
Iteration 173/1000 | Loss: 0.00001231
Iteration 174/1000 | Loss: 0.00001231
Iteration 175/1000 | Loss: 0.00001230
Iteration 176/1000 | Loss: 0.00001230
Iteration 177/1000 | Loss: 0.00001230
Iteration 178/1000 | Loss: 0.00001230
Iteration 179/1000 | Loss: 0.00001230
Iteration 180/1000 | Loss: 0.00001230
Iteration 181/1000 | Loss: 0.00001229
Iteration 182/1000 | Loss: 0.00001229
Iteration 183/1000 | Loss: 0.00001229
Iteration 184/1000 | Loss: 0.00001229
Iteration 185/1000 | Loss: 0.00001229
Iteration 186/1000 | Loss: 0.00001229
Iteration 187/1000 | Loss: 0.00001229
Iteration 188/1000 | Loss: 0.00001229
Iteration 189/1000 | Loss: 0.00001229
Iteration 190/1000 | Loss: 0.00001229
Iteration 191/1000 | Loss: 0.00001229
Iteration 192/1000 | Loss: 0.00001228
Iteration 193/1000 | Loss: 0.00001228
Iteration 194/1000 | Loss: 0.00001228
Iteration 195/1000 | Loss: 0.00001227
Iteration 196/1000 | Loss: 0.00001227
Iteration 197/1000 | Loss: 0.00001227
Iteration 198/1000 | Loss: 0.00001227
Iteration 199/1000 | Loss: 0.00001227
Iteration 200/1000 | Loss: 0.00001227
Iteration 201/1000 | Loss: 0.00001226
Iteration 202/1000 | Loss: 0.00001226
Iteration 203/1000 | Loss: 0.00001226
Iteration 204/1000 | Loss: 0.00001226
Iteration 205/1000 | Loss: 0.00001226
Iteration 206/1000 | Loss: 0.00001226
Iteration 207/1000 | Loss: 0.00001226
Iteration 208/1000 | Loss: 0.00001226
Iteration 209/1000 | Loss: 0.00001226
Iteration 210/1000 | Loss: 0.00001226
Iteration 211/1000 | Loss: 0.00001226
Iteration 212/1000 | Loss: 0.00001226
Iteration 213/1000 | Loss: 0.00001226
Iteration 214/1000 | Loss: 0.00001225
Iteration 215/1000 | Loss: 0.00001225
Iteration 216/1000 | Loss: 0.00001225
Iteration 217/1000 | Loss: 0.00001225
Iteration 218/1000 | Loss: 0.00001225
Iteration 219/1000 | Loss: 0.00001225
Iteration 220/1000 | Loss: 0.00001225
Iteration 221/1000 | Loss: 0.00001225
Iteration 222/1000 | Loss: 0.00001225
Iteration 223/1000 | Loss: 0.00001225
Iteration 224/1000 | Loss: 0.00001225
Iteration 225/1000 | Loss: 0.00001225
Iteration 226/1000 | Loss: 0.00001224
Iteration 227/1000 | Loss: 0.00001224
Iteration 228/1000 | Loss: 0.00001224
Iteration 229/1000 | Loss: 0.00001224
Iteration 230/1000 | Loss: 0.00001224
Iteration 231/1000 | Loss: 0.00001224
Iteration 232/1000 | Loss: 0.00001224
Iteration 233/1000 | Loss: 0.00001224
Iteration 234/1000 | Loss: 0.00001224
Iteration 235/1000 | Loss: 0.00001223
Iteration 236/1000 | Loss: 0.00001223
Iteration 237/1000 | Loss: 0.00001223
Iteration 238/1000 | Loss: 0.00001223
Iteration 239/1000 | Loss: 0.00001223
Iteration 240/1000 | Loss: 0.00001223
Iteration 241/1000 | Loss: 0.00001223
Iteration 242/1000 | Loss: 0.00001223
Iteration 243/1000 | Loss: 0.00001223
Iteration 244/1000 | Loss: 0.00001223
Iteration 245/1000 | Loss: 0.00001223
Iteration 246/1000 | Loss: 0.00001223
Iteration 247/1000 | Loss: 0.00001222
Iteration 248/1000 | Loss: 0.00001222
Iteration 249/1000 | Loss: 0.00001222
Iteration 250/1000 | Loss: 0.00001222
Iteration 251/1000 | Loss: 0.00001222
Iteration 252/1000 | Loss: 0.00001222
Iteration 253/1000 | Loss: 0.00001222
Iteration 254/1000 | Loss: 0.00001222
Iteration 255/1000 | Loss: 0.00001222
Iteration 256/1000 | Loss: 0.00001222
Iteration 257/1000 | Loss: 0.00001222
Iteration 258/1000 | Loss: 0.00001221
Iteration 259/1000 | Loss: 0.00001221
Iteration 260/1000 | Loss: 0.00001221
Iteration 261/1000 | Loss: 0.00001221
Iteration 262/1000 | Loss: 0.00001221
Iteration 263/1000 | Loss: 0.00001221
Iteration 264/1000 | Loss: 0.00001221
Iteration 265/1000 | Loss: 0.00001221
Iteration 266/1000 | Loss: 0.00001221
Iteration 267/1000 | Loss: 0.00001221
Iteration 268/1000 | Loss: 0.00001221
Iteration 269/1000 | Loss: 0.00001221
Iteration 270/1000 | Loss: 0.00001221
Iteration 271/1000 | Loss: 0.00001221
Iteration 272/1000 | Loss: 0.00001221
Iteration 273/1000 | Loss: 0.00001221
Iteration 274/1000 | Loss: 0.00001221
Iteration 275/1000 | Loss: 0.00001221
Iteration 276/1000 | Loss: 0.00001220
Iteration 277/1000 | Loss: 0.00001220
Iteration 278/1000 | Loss: 0.00001220
Iteration 279/1000 | Loss: 0.00001220
Iteration 280/1000 | Loss: 0.00001220
Iteration 281/1000 | Loss: 0.00001220
Iteration 282/1000 | Loss: 0.00001220
Iteration 283/1000 | Loss: 0.00001220
Iteration 284/1000 | Loss: 0.00001220
Iteration 285/1000 | Loss: 0.00001220
Iteration 286/1000 | Loss: 0.00001220
Iteration 287/1000 | Loss: 0.00001220
Iteration 288/1000 | Loss: 0.00001220
Iteration 289/1000 | Loss: 0.00001219
Iteration 290/1000 | Loss: 0.00001219
Iteration 291/1000 | Loss: 0.00001219
Iteration 292/1000 | Loss: 0.00001219
Iteration 293/1000 | Loss: 0.00001219
Iteration 294/1000 | Loss: 0.00001219
Iteration 295/1000 | Loss: 0.00001219
Iteration 296/1000 | Loss: 0.00001219
Iteration 297/1000 | Loss: 0.00001219
Iteration 298/1000 | Loss: 0.00001219
Iteration 299/1000 | Loss: 0.00001219
Iteration 300/1000 | Loss: 0.00001219
Iteration 301/1000 | Loss: 0.00001219
Iteration 302/1000 | Loss: 0.00001219
Iteration 303/1000 | Loss: 0.00001219
Iteration 304/1000 | Loss: 0.00001219
Iteration 305/1000 | Loss: 0.00001219
Iteration 306/1000 | Loss: 0.00001218
Iteration 307/1000 | Loss: 0.00001218
Iteration 308/1000 | Loss: 0.00001218
Iteration 309/1000 | Loss: 0.00001218
Iteration 310/1000 | Loss: 0.00001218
Iteration 311/1000 | Loss: 0.00001218
Iteration 312/1000 | Loss: 0.00001218
Iteration 313/1000 | Loss: 0.00001218
Iteration 314/1000 | Loss: 0.00001218
Iteration 315/1000 | Loss: 0.00001218
Iteration 316/1000 | Loss: 0.00001218
Iteration 317/1000 | Loss: 0.00001218
Iteration 318/1000 | Loss: 0.00001218
Iteration 319/1000 | Loss: 0.00001218
Iteration 320/1000 | Loss: 0.00001218
Iteration 321/1000 | Loss: 0.00001218
Iteration 322/1000 | Loss: 0.00001218
Iteration 323/1000 | Loss: 0.00001218
Iteration 324/1000 | Loss: 0.00001218
Iteration 325/1000 | Loss: 0.00001217
Iteration 326/1000 | Loss: 0.00001217
Iteration 327/1000 | Loss: 0.00001217
Iteration 328/1000 | Loss: 0.00001217
Iteration 329/1000 | Loss: 0.00001217
Iteration 330/1000 | Loss: 0.00001217
Iteration 331/1000 | Loss: 0.00001217
Iteration 332/1000 | Loss: 0.00001217
Iteration 333/1000 | Loss: 0.00001217
Iteration 334/1000 | Loss: 0.00001217
Iteration 335/1000 | Loss: 0.00001217
Iteration 336/1000 | Loss: 0.00001217
Iteration 337/1000 | Loss: 0.00001217
Iteration 338/1000 | Loss: 0.00001217
Iteration 339/1000 | Loss: 0.00001217
Iteration 340/1000 | Loss: 0.00001217
Iteration 341/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [1.2172104106866755e-05, 1.2172104106866755e-05, 1.2172104106866755e-05, 1.2172104106866755e-05, 1.2172104106866755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2172104106866755e-05

Optimization complete. Final v2v error: 2.941279172897339 mm

Highest mean error: 3.31949520111084 mm for frame 176

Lowest mean error: 2.710049867630005 mm for frame 228

Saving results

Total time: 57.29805374145508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062245
Iteration 2/25 | Loss: 0.01062245
Iteration 3/25 | Loss: 0.01062245
Iteration 4/25 | Loss: 0.01062244
Iteration 5/25 | Loss: 0.01062244
Iteration 6/25 | Loss: 0.01062244
Iteration 7/25 | Loss: 0.00613953
Iteration 8/25 | Loss: 0.00368293
Iteration 9/25 | Loss: 0.00336856
Iteration 10/25 | Loss: 0.00271453
Iteration 11/25 | Loss: 0.00210685
Iteration 12/25 | Loss: 0.00196896
Iteration 13/25 | Loss: 0.00171630
Iteration 14/25 | Loss: 0.00155995
Iteration 15/25 | Loss: 0.00153407
Iteration 16/25 | Loss: 0.00148159
Iteration 17/25 | Loss: 0.00146214
Iteration 18/25 | Loss: 0.00142698
Iteration 19/25 | Loss: 0.00141996
Iteration 20/25 | Loss: 0.00142243
Iteration 21/25 | Loss: 0.00141812
Iteration 22/25 | Loss: 0.00141836
Iteration 23/25 | Loss: 0.00141513
Iteration 24/25 | Loss: 0.00141533
Iteration 25/25 | Loss: 0.00142005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52283949
Iteration 2/25 | Loss: 0.00178828
Iteration 3/25 | Loss: 0.00178827
Iteration 4/25 | Loss: 0.00178827
Iteration 5/25 | Loss: 0.00175947
Iteration 6/25 | Loss: 0.00175947
Iteration 7/25 | Loss: 0.00175947
Iteration 8/25 | Loss: 0.00175946
Iteration 9/25 | Loss: 0.00175946
Iteration 10/25 | Loss: 0.00175946
Iteration 11/25 | Loss: 0.00175946
Iteration 12/25 | Loss: 0.00175946
Iteration 13/25 | Loss: 0.00175946
Iteration 14/25 | Loss: 0.00175946
Iteration 15/25 | Loss: 0.00175946
Iteration 16/25 | Loss: 0.00175946
Iteration 17/25 | Loss: 0.00175946
Iteration 18/25 | Loss: 0.00175946
Iteration 19/25 | Loss: 0.00175946
Iteration 20/25 | Loss: 0.00175946
Iteration 21/25 | Loss: 0.00175946
Iteration 22/25 | Loss: 0.00175946
Iteration 23/25 | Loss: 0.00175946
Iteration 24/25 | Loss: 0.00175946
Iteration 25/25 | Loss: 0.00175946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175946
Iteration 2/1000 | Loss: 0.00011529
Iteration 3/1000 | Loss: 0.00004836
Iteration 4/1000 | Loss: 0.00004354
Iteration 5/1000 | Loss: 0.00015454
Iteration 6/1000 | Loss: 0.00005356
Iteration 7/1000 | Loss: 0.00004345
Iteration 8/1000 | Loss: 0.00004154
Iteration 9/1000 | Loss: 0.00007291
Iteration 10/1000 | Loss: 0.00004012
Iteration 11/1000 | Loss: 0.00003834
Iteration 12/1000 | Loss: 0.00003638
Iteration 13/1000 | Loss: 0.00003548
Iteration 14/1000 | Loss: 0.00003486
Iteration 15/1000 | Loss: 0.00003460
Iteration 16/1000 | Loss: 0.00003437
Iteration 17/1000 | Loss: 0.00182639
Iteration 18/1000 | Loss: 0.00229610
Iteration 19/1000 | Loss: 0.00008478
Iteration 20/1000 | Loss: 0.00003977
Iteration 21/1000 | Loss: 0.00025830
Iteration 22/1000 | Loss: 0.00003092
Iteration 23/1000 | Loss: 0.00002908
Iteration 24/1000 | Loss: 0.00002624
Iteration 25/1000 | Loss: 0.00002906
Iteration 26/1000 | Loss: 0.00008397
Iteration 27/1000 | Loss: 0.00013512
Iteration 28/1000 | Loss: 0.00008698
Iteration 29/1000 | Loss: 0.00013840
Iteration 30/1000 | Loss: 0.00009055
Iteration 31/1000 | Loss: 0.00002794
Iteration 32/1000 | Loss: 0.00003377
Iteration 33/1000 | Loss: 0.00007896
Iteration 34/1000 | Loss: 0.00002212
Iteration 35/1000 | Loss: 0.00002074
Iteration 36/1000 | Loss: 0.00015686
Iteration 37/1000 | Loss: 0.00010422
Iteration 38/1000 | Loss: 0.00002231
Iteration 39/1000 | Loss: 0.00009194
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001696
Iteration 43/1000 | Loss: 0.00001685
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001658
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00002580
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001643
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001641
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001641
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001640
Iteration 74/1000 | Loss: 0.00001640
Iteration 75/1000 | Loss: 0.00001640
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001640
Iteration 78/1000 | Loss: 0.00001640
Iteration 79/1000 | Loss: 0.00001640
Iteration 80/1000 | Loss: 0.00001640
Iteration 81/1000 | Loss: 0.00001640
Iteration 82/1000 | Loss: 0.00001640
Iteration 83/1000 | Loss: 0.00001640
Iteration 84/1000 | Loss: 0.00001640
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001639
Iteration 89/1000 | Loss: 0.00001639
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001635
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001635
Iteration 105/1000 | Loss: 0.00001635
Iteration 106/1000 | Loss: 0.00001635
Iteration 107/1000 | Loss: 0.00001635
Iteration 108/1000 | Loss: 0.00001635
Iteration 109/1000 | Loss: 0.00001635
Iteration 110/1000 | Loss: 0.00001635
Iteration 111/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6351446902262978e-05, 1.6351446902262978e-05, 1.6351446902262978e-05, 1.6351446902262978e-05, 1.6351446902262978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6351446902262978e-05

Optimization complete. Final v2v error: 3.5268871784210205 mm

Highest mean error: 3.838327646255493 mm for frame 159

Lowest mean error: 3.407966375350952 mm for frame 51

Saving results

Total time: 123.78951239585876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899963
Iteration 2/25 | Loss: 0.00248807
Iteration 3/25 | Loss: 0.00188859
Iteration 4/25 | Loss: 0.00192363
Iteration 5/25 | Loss: 0.00178612
Iteration 6/25 | Loss: 0.00164446
Iteration 7/25 | Loss: 0.00154128
Iteration 8/25 | Loss: 0.00150550
Iteration 9/25 | Loss: 0.00148685
Iteration 10/25 | Loss: 0.00147913
Iteration 11/25 | Loss: 0.00147665
Iteration 12/25 | Loss: 0.00146979
Iteration 13/25 | Loss: 0.00146584
Iteration 14/25 | Loss: 0.00144504
Iteration 15/25 | Loss: 0.00140269
Iteration 16/25 | Loss: 0.00139837
Iteration 17/25 | Loss: 0.00136889
Iteration 18/25 | Loss: 0.00136148
Iteration 19/25 | Loss: 0.00135651
Iteration 20/25 | Loss: 0.00135592
Iteration 21/25 | Loss: 0.00135563
Iteration 22/25 | Loss: 0.00135545
Iteration 23/25 | Loss: 0.00135541
Iteration 24/25 | Loss: 0.00135538
Iteration 25/25 | Loss: 0.00135538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19274747
Iteration 2/25 | Loss: 0.00200230
Iteration 3/25 | Loss: 0.00190726
Iteration 4/25 | Loss: 0.00190726
Iteration 5/25 | Loss: 0.00190726
Iteration 6/25 | Loss: 0.00190726
Iteration 7/25 | Loss: 0.00190726
Iteration 8/25 | Loss: 0.00190725
Iteration 9/25 | Loss: 0.00190725
Iteration 10/25 | Loss: 0.00190725
Iteration 11/25 | Loss: 0.00190725
Iteration 12/25 | Loss: 0.00190725
Iteration 13/25 | Loss: 0.00190725
Iteration 14/25 | Loss: 0.00190725
Iteration 15/25 | Loss: 0.00190725
Iteration 16/25 | Loss: 0.00190725
Iteration 17/25 | Loss: 0.00190725
Iteration 18/25 | Loss: 0.00190725
Iteration 19/25 | Loss: 0.00190725
Iteration 20/25 | Loss: 0.00190725
Iteration 21/25 | Loss: 0.00190725
Iteration 22/25 | Loss: 0.00190725
Iteration 23/25 | Loss: 0.00190725
Iteration 24/25 | Loss: 0.00190725
Iteration 25/25 | Loss: 0.00190725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190725
Iteration 2/1000 | Loss: 0.00023164
Iteration 3/1000 | Loss: 0.00004701
Iteration 4/1000 | Loss: 0.00003278
Iteration 5/1000 | Loss: 0.00002858
Iteration 6/1000 | Loss: 0.00002620
Iteration 7/1000 | Loss: 0.00002487
Iteration 8/1000 | Loss: 0.00002426
Iteration 9/1000 | Loss: 0.00002373
Iteration 10/1000 | Loss: 0.00002306
Iteration 11/1000 | Loss: 0.00016654
Iteration 12/1000 | Loss: 0.00057774
Iteration 13/1000 | Loss: 0.00018490
Iteration 14/1000 | Loss: 0.00002418
Iteration 15/1000 | Loss: 0.00002193
Iteration 16/1000 | Loss: 0.00002078
Iteration 17/1000 | Loss: 0.00001978
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001842
Iteration 22/1000 | Loss: 0.00001820
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00006352
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001765
Iteration 29/1000 | Loss: 0.00001765
Iteration 30/1000 | Loss: 0.00001764
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001758
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001757
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001750
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001745
Iteration 47/1000 | Loss: 0.00001744
Iteration 48/1000 | Loss: 0.00001744
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001743
Iteration 52/1000 | Loss: 0.00001743
Iteration 53/1000 | Loss: 0.00001743
Iteration 54/1000 | Loss: 0.00001743
Iteration 55/1000 | Loss: 0.00001743
Iteration 56/1000 | Loss: 0.00001743
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001742
Iteration 59/1000 | Loss: 0.00001742
Iteration 60/1000 | Loss: 0.00001742
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001741
Iteration 65/1000 | Loss: 0.00001741
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001741
Iteration 70/1000 | Loss: 0.00001741
Iteration 71/1000 | Loss: 0.00001741
Iteration 72/1000 | Loss: 0.00001740
Iteration 73/1000 | Loss: 0.00001740
Iteration 74/1000 | Loss: 0.00001740
Iteration 75/1000 | Loss: 0.00001740
Iteration 76/1000 | Loss: 0.00001740
Iteration 77/1000 | Loss: 0.00001740
Iteration 78/1000 | Loss: 0.00001740
Iteration 79/1000 | Loss: 0.00001740
Iteration 80/1000 | Loss: 0.00001739
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001739
Iteration 83/1000 | Loss: 0.00001738
Iteration 84/1000 | Loss: 0.00001738
Iteration 85/1000 | Loss: 0.00001738
Iteration 86/1000 | Loss: 0.00001738
Iteration 87/1000 | Loss: 0.00001737
Iteration 88/1000 | Loss: 0.00001737
Iteration 89/1000 | Loss: 0.00001737
Iteration 90/1000 | Loss: 0.00001737
Iteration 91/1000 | Loss: 0.00001737
Iteration 92/1000 | Loss: 0.00001737
Iteration 93/1000 | Loss: 0.00001737
Iteration 94/1000 | Loss: 0.00001737
Iteration 95/1000 | Loss: 0.00001737
Iteration 96/1000 | Loss: 0.00001737
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001736
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001736
Iteration 101/1000 | Loss: 0.00001736
Iteration 102/1000 | Loss: 0.00001736
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001734
Iteration 106/1000 | Loss: 0.00001734
Iteration 107/1000 | Loss: 0.00001734
Iteration 108/1000 | Loss: 0.00001734
Iteration 109/1000 | Loss: 0.00001733
Iteration 110/1000 | Loss: 0.00001733
Iteration 111/1000 | Loss: 0.00001733
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00001732
Iteration 114/1000 | Loss: 0.00001732
Iteration 115/1000 | Loss: 0.00001732
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001730
Iteration 118/1000 | Loss: 0.00001730
Iteration 119/1000 | Loss: 0.00001730
Iteration 120/1000 | Loss: 0.00001729
Iteration 121/1000 | Loss: 0.00001729
Iteration 122/1000 | Loss: 0.00001729
Iteration 123/1000 | Loss: 0.00001728
Iteration 124/1000 | Loss: 0.00001728
Iteration 125/1000 | Loss: 0.00001728
Iteration 126/1000 | Loss: 0.00001728
Iteration 127/1000 | Loss: 0.00001728
Iteration 128/1000 | Loss: 0.00001727
Iteration 129/1000 | Loss: 0.00001727
Iteration 130/1000 | Loss: 0.00001727
Iteration 131/1000 | Loss: 0.00001727
Iteration 132/1000 | Loss: 0.00001727
Iteration 133/1000 | Loss: 0.00001727
Iteration 134/1000 | Loss: 0.00001727
Iteration 135/1000 | Loss: 0.00001727
Iteration 136/1000 | Loss: 0.00001726
Iteration 137/1000 | Loss: 0.00001726
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001725
Iteration 140/1000 | Loss: 0.00001724
Iteration 141/1000 | Loss: 0.00001724
Iteration 142/1000 | Loss: 0.00001724
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001722
Iteration 150/1000 | Loss: 0.00001722
Iteration 151/1000 | Loss: 0.00001721
Iteration 152/1000 | Loss: 0.00001721
Iteration 153/1000 | Loss: 0.00001721
Iteration 154/1000 | Loss: 0.00001721
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001721
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001721
Iteration 160/1000 | Loss: 0.00001721
Iteration 161/1000 | Loss: 0.00001720
Iteration 162/1000 | Loss: 0.00001720
Iteration 163/1000 | Loss: 0.00001720
Iteration 164/1000 | Loss: 0.00001720
Iteration 165/1000 | Loss: 0.00001720
Iteration 166/1000 | Loss: 0.00001720
Iteration 167/1000 | Loss: 0.00001720
Iteration 168/1000 | Loss: 0.00001720
Iteration 169/1000 | Loss: 0.00001720
Iteration 170/1000 | Loss: 0.00001720
Iteration 171/1000 | Loss: 0.00001720
Iteration 172/1000 | Loss: 0.00001719
Iteration 173/1000 | Loss: 0.00001719
Iteration 174/1000 | Loss: 0.00001719
Iteration 175/1000 | Loss: 0.00001719
Iteration 176/1000 | Loss: 0.00001719
Iteration 177/1000 | Loss: 0.00001718
Iteration 178/1000 | Loss: 0.00001718
Iteration 179/1000 | Loss: 0.00001718
Iteration 180/1000 | Loss: 0.00001718
Iteration 181/1000 | Loss: 0.00001718
Iteration 182/1000 | Loss: 0.00001718
Iteration 183/1000 | Loss: 0.00001718
Iteration 184/1000 | Loss: 0.00001718
Iteration 185/1000 | Loss: 0.00001718
Iteration 186/1000 | Loss: 0.00001718
Iteration 187/1000 | Loss: 0.00001718
Iteration 188/1000 | Loss: 0.00001718
Iteration 189/1000 | Loss: 0.00001718
Iteration 190/1000 | Loss: 0.00001718
Iteration 191/1000 | Loss: 0.00001718
Iteration 192/1000 | Loss: 0.00001718
Iteration 193/1000 | Loss: 0.00001718
Iteration 194/1000 | Loss: 0.00001718
Iteration 195/1000 | Loss: 0.00001718
Iteration 196/1000 | Loss: 0.00001718
Iteration 197/1000 | Loss: 0.00001718
Iteration 198/1000 | Loss: 0.00001718
Iteration 199/1000 | Loss: 0.00001718
Iteration 200/1000 | Loss: 0.00001718
Iteration 201/1000 | Loss: 0.00001718
Iteration 202/1000 | Loss: 0.00001718
Iteration 203/1000 | Loss: 0.00001718
Iteration 204/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.71760893863393e-05, 1.71760893863393e-05, 1.71760893863393e-05, 1.71760893863393e-05, 1.71760893863393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.71760893863393e-05

Optimization complete. Final v2v error: 3.450540781021118 mm

Highest mean error: 4.093667984008789 mm for frame 147

Lowest mean error: 3.273383140563965 mm for frame 29

Saving results

Total time: 104.63369035720825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036748
Iteration 2/25 | Loss: 0.01036748
Iteration 3/25 | Loss: 0.01036748
Iteration 4/25 | Loss: 0.01036748
Iteration 5/25 | Loss: 0.01036748
Iteration 6/25 | Loss: 0.01036747
Iteration 7/25 | Loss: 0.01036747
Iteration 8/25 | Loss: 0.01036747
Iteration 9/25 | Loss: 0.01036747
Iteration 10/25 | Loss: 0.01036747
Iteration 11/25 | Loss: 0.01036747
Iteration 12/25 | Loss: 0.01036747
Iteration 13/25 | Loss: 0.01036747
Iteration 14/25 | Loss: 0.01036747
Iteration 15/25 | Loss: 0.01036746
Iteration 16/25 | Loss: 0.01036746
Iteration 17/25 | Loss: 0.01036746
Iteration 18/25 | Loss: 0.01036746
Iteration 19/25 | Loss: 0.01036746
Iteration 20/25 | Loss: 0.01036746
Iteration 21/25 | Loss: 0.01036746
Iteration 22/25 | Loss: 0.01036746
Iteration 23/25 | Loss: 0.01036746
Iteration 24/25 | Loss: 0.01036746
Iteration 25/25 | Loss: 0.01036746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35936558
Iteration 2/25 | Loss: 0.12584251
Iteration 3/25 | Loss: 0.12264805
Iteration 4/25 | Loss: 0.12226104
Iteration 5/25 | Loss: 0.12175696
Iteration 6/25 | Loss: 0.12175694
Iteration 7/25 | Loss: 0.12175692
Iteration 8/25 | Loss: 0.12175692
Iteration 9/25 | Loss: 0.12175692
Iteration 10/25 | Loss: 0.12175692
Iteration 11/25 | Loss: 0.12175690
Iteration 12/25 | Loss: 0.12175690
Iteration 13/25 | Loss: 0.12175690
Iteration 14/25 | Loss: 0.12175690
Iteration 15/25 | Loss: 0.12175690
Iteration 16/25 | Loss: 0.12175690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.1217569038271904, 0.1217569038271904, 0.1217569038271904, 0.1217569038271904, 0.1217569038271904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1217569038271904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12175690
Iteration 2/1000 | Loss: 0.00120115
Iteration 3/1000 | Loss: 0.00057105
Iteration 4/1000 | Loss: 0.00012474
Iteration 5/1000 | Loss: 0.00006138
Iteration 6/1000 | Loss: 0.00003669
Iteration 7/1000 | Loss: 0.00002684
Iteration 8/1000 | Loss: 0.00002218
Iteration 9/1000 | Loss: 0.00001862
Iteration 10/1000 | Loss: 0.00001613
Iteration 11/1000 | Loss: 0.00001458
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001048
Iteration 17/1000 | Loss: 0.00001012
Iteration 18/1000 | Loss: 0.00000985
Iteration 19/1000 | Loss: 0.00000958
Iteration 20/1000 | Loss: 0.00000944
Iteration 21/1000 | Loss: 0.00000921
Iteration 22/1000 | Loss: 0.00000914
Iteration 23/1000 | Loss: 0.00000894
Iteration 24/1000 | Loss: 0.00000888
Iteration 25/1000 | Loss: 0.00000887
Iteration 26/1000 | Loss: 0.00000871
Iteration 27/1000 | Loss: 0.00000865
Iteration 28/1000 | Loss: 0.00000864
Iteration 29/1000 | Loss: 0.00000864
Iteration 30/1000 | Loss: 0.00000861
Iteration 31/1000 | Loss: 0.00000860
Iteration 32/1000 | Loss: 0.00000859
Iteration 33/1000 | Loss: 0.00000858
Iteration 34/1000 | Loss: 0.00000853
Iteration 35/1000 | Loss: 0.00000851
Iteration 36/1000 | Loss: 0.00000849
Iteration 37/1000 | Loss: 0.00000849
Iteration 38/1000 | Loss: 0.00000849
Iteration 39/1000 | Loss: 0.00000849
Iteration 40/1000 | Loss: 0.00000849
Iteration 41/1000 | Loss: 0.00000849
Iteration 42/1000 | Loss: 0.00000844
Iteration 43/1000 | Loss: 0.00000844
Iteration 44/1000 | Loss: 0.00000844
Iteration 45/1000 | Loss: 0.00000844
Iteration 46/1000 | Loss: 0.00000844
Iteration 47/1000 | Loss: 0.00000843
Iteration 48/1000 | Loss: 0.00000841
Iteration 49/1000 | Loss: 0.00000841
Iteration 50/1000 | Loss: 0.00000838
Iteration 51/1000 | Loss: 0.00000838
Iteration 52/1000 | Loss: 0.00000837
Iteration 53/1000 | Loss: 0.00000837
Iteration 54/1000 | Loss: 0.00000836
Iteration 55/1000 | Loss: 0.00000836
Iteration 56/1000 | Loss: 0.00000835
Iteration 57/1000 | Loss: 0.00000834
Iteration 58/1000 | Loss: 0.00000834
Iteration 59/1000 | Loss: 0.00000833
Iteration 60/1000 | Loss: 0.00000831
Iteration 61/1000 | Loss: 0.00000831
Iteration 62/1000 | Loss: 0.00000830
Iteration 63/1000 | Loss: 0.00000830
Iteration 64/1000 | Loss: 0.00000829
Iteration 65/1000 | Loss: 0.00000829
Iteration 66/1000 | Loss: 0.00000828
Iteration 67/1000 | Loss: 0.00000828
Iteration 68/1000 | Loss: 0.00000828
Iteration 69/1000 | Loss: 0.00000827
Iteration 70/1000 | Loss: 0.00000827
Iteration 71/1000 | Loss: 0.00000826
Iteration 72/1000 | Loss: 0.00000826
Iteration 73/1000 | Loss: 0.00000825
Iteration 74/1000 | Loss: 0.00000825
Iteration 75/1000 | Loss: 0.00000823
Iteration 76/1000 | Loss: 0.00000822
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000821
Iteration 79/1000 | Loss: 0.00000818
Iteration 80/1000 | Loss: 0.00000817
Iteration 81/1000 | Loss: 0.00000817
Iteration 82/1000 | Loss: 0.00000815
Iteration 83/1000 | Loss: 0.00000814
Iteration 84/1000 | Loss: 0.00000813
Iteration 85/1000 | Loss: 0.00000812
Iteration 86/1000 | Loss: 0.00000812
Iteration 87/1000 | Loss: 0.00000812
Iteration 88/1000 | Loss: 0.00000811
Iteration 89/1000 | Loss: 0.00000811
Iteration 90/1000 | Loss: 0.00000810
Iteration 91/1000 | Loss: 0.00000810
Iteration 92/1000 | Loss: 0.00000809
Iteration 93/1000 | Loss: 0.00000809
Iteration 94/1000 | Loss: 0.00000809
Iteration 95/1000 | Loss: 0.00000809
Iteration 96/1000 | Loss: 0.00000809
Iteration 97/1000 | Loss: 0.00000809
Iteration 98/1000 | Loss: 0.00000808
Iteration 99/1000 | Loss: 0.00000807
Iteration 100/1000 | Loss: 0.00000807
Iteration 101/1000 | Loss: 0.00000807
Iteration 102/1000 | Loss: 0.00000807
Iteration 103/1000 | Loss: 0.00000807
Iteration 104/1000 | Loss: 0.00000807
Iteration 105/1000 | Loss: 0.00000806
Iteration 106/1000 | Loss: 0.00000806
Iteration 107/1000 | Loss: 0.00000806
Iteration 108/1000 | Loss: 0.00000806
Iteration 109/1000 | Loss: 0.00000806
Iteration 110/1000 | Loss: 0.00000805
Iteration 111/1000 | Loss: 0.00000805
Iteration 112/1000 | Loss: 0.00000805
Iteration 113/1000 | Loss: 0.00000805
Iteration 114/1000 | Loss: 0.00000805
Iteration 115/1000 | Loss: 0.00000804
Iteration 116/1000 | Loss: 0.00000804
Iteration 117/1000 | Loss: 0.00000804
Iteration 118/1000 | Loss: 0.00000804
Iteration 119/1000 | Loss: 0.00000804
Iteration 120/1000 | Loss: 0.00000804
Iteration 121/1000 | Loss: 0.00000803
Iteration 122/1000 | Loss: 0.00000803
Iteration 123/1000 | Loss: 0.00000803
Iteration 124/1000 | Loss: 0.00000803
Iteration 125/1000 | Loss: 0.00000803
Iteration 126/1000 | Loss: 0.00000802
Iteration 127/1000 | Loss: 0.00000802
Iteration 128/1000 | Loss: 0.00000802
Iteration 129/1000 | Loss: 0.00000802
Iteration 130/1000 | Loss: 0.00000802
Iteration 131/1000 | Loss: 0.00000802
Iteration 132/1000 | Loss: 0.00000802
Iteration 133/1000 | Loss: 0.00000802
Iteration 134/1000 | Loss: 0.00000801
Iteration 135/1000 | Loss: 0.00000801
Iteration 136/1000 | Loss: 0.00000801
Iteration 137/1000 | Loss: 0.00000801
Iteration 138/1000 | Loss: 0.00000800
Iteration 139/1000 | Loss: 0.00000800
Iteration 140/1000 | Loss: 0.00000800
Iteration 141/1000 | Loss: 0.00000800
Iteration 142/1000 | Loss: 0.00000800
Iteration 143/1000 | Loss: 0.00000800
Iteration 144/1000 | Loss: 0.00000800
Iteration 145/1000 | Loss: 0.00000800
Iteration 146/1000 | Loss: 0.00000800
Iteration 147/1000 | Loss: 0.00000800
Iteration 148/1000 | Loss: 0.00000800
Iteration 149/1000 | Loss: 0.00000800
Iteration 150/1000 | Loss: 0.00000799
Iteration 151/1000 | Loss: 0.00000799
Iteration 152/1000 | Loss: 0.00000799
Iteration 153/1000 | Loss: 0.00000799
Iteration 154/1000 | Loss: 0.00000799
Iteration 155/1000 | Loss: 0.00000798
Iteration 156/1000 | Loss: 0.00000798
Iteration 157/1000 | Loss: 0.00000798
Iteration 158/1000 | Loss: 0.00000798
Iteration 159/1000 | Loss: 0.00000798
Iteration 160/1000 | Loss: 0.00000798
Iteration 161/1000 | Loss: 0.00000798
Iteration 162/1000 | Loss: 0.00000798
Iteration 163/1000 | Loss: 0.00000798
Iteration 164/1000 | Loss: 0.00000798
Iteration 165/1000 | Loss: 0.00000798
Iteration 166/1000 | Loss: 0.00000797
Iteration 167/1000 | Loss: 0.00000797
Iteration 168/1000 | Loss: 0.00000797
Iteration 169/1000 | Loss: 0.00000797
Iteration 170/1000 | Loss: 0.00000797
Iteration 171/1000 | Loss: 0.00000797
Iteration 172/1000 | Loss: 0.00000797
Iteration 173/1000 | Loss: 0.00000797
Iteration 174/1000 | Loss: 0.00000797
Iteration 175/1000 | Loss: 0.00000796
Iteration 176/1000 | Loss: 0.00000796
Iteration 177/1000 | Loss: 0.00000796
Iteration 178/1000 | Loss: 0.00000795
Iteration 179/1000 | Loss: 0.00000795
Iteration 180/1000 | Loss: 0.00000795
Iteration 181/1000 | Loss: 0.00000795
Iteration 182/1000 | Loss: 0.00000795
Iteration 183/1000 | Loss: 0.00000795
Iteration 184/1000 | Loss: 0.00000795
Iteration 185/1000 | Loss: 0.00000795
Iteration 186/1000 | Loss: 0.00000795
Iteration 187/1000 | Loss: 0.00000795
Iteration 188/1000 | Loss: 0.00000795
Iteration 189/1000 | Loss: 0.00000794
Iteration 190/1000 | Loss: 0.00000794
Iteration 191/1000 | Loss: 0.00000794
Iteration 192/1000 | Loss: 0.00000794
Iteration 193/1000 | Loss: 0.00000793
Iteration 194/1000 | Loss: 0.00000793
Iteration 195/1000 | Loss: 0.00000793
Iteration 196/1000 | Loss: 0.00000793
Iteration 197/1000 | Loss: 0.00000793
Iteration 198/1000 | Loss: 0.00000793
Iteration 199/1000 | Loss: 0.00000793
Iteration 200/1000 | Loss: 0.00000793
Iteration 201/1000 | Loss: 0.00000793
Iteration 202/1000 | Loss: 0.00000793
Iteration 203/1000 | Loss: 0.00000793
Iteration 204/1000 | Loss: 0.00000793
Iteration 205/1000 | Loss: 0.00000793
Iteration 206/1000 | Loss: 0.00000793
Iteration 207/1000 | Loss: 0.00000793
Iteration 208/1000 | Loss: 0.00000793
Iteration 209/1000 | Loss: 0.00000793
Iteration 210/1000 | Loss: 0.00000793
Iteration 211/1000 | Loss: 0.00000793
Iteration 212/1000 | Loss: 0.00000793
Iteration 213/1000 | Loss: 0.00000793
Iteration 214/1000 | Loss: 0.00000793
Iteration 215/1000 | Loss: 0.00000793
Iteration 216/1000 | Loss: 0.00000793
Iteration 217/1000 | Loss: 0.00000793
Iteration 218/1000 | Loss: 0.00000793
Iteration 219/1000 | Loss: 0.00000793
Iteration 220/1000 | Loss: 0.00000793
Iteration 221/1000 | Loss: 0.00000793
Iteration 222/1000 | Loss: 0.00000793
Iteration 223/1000 | Loss: 0.00000793
Iteration 224/1000 | Loss: 0.00000793
Iteration 225/1000 | Loss: 0.00000793
Iteration 226/1000 | Loss: 0.00000793
Iteration 227/1000 | Loss: 0.00000793
Iteration 228/1000 | Loss: 0.00000793
Iteration 229/1000 | Loss: 0.00000793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [7.927240403660107e-06, 7.927240403660107e-06, 7.927240403660107e-06, 7.927240403660107e-06, 7.927240403660107e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.927240403660107e-06

Optimization complete. Final v2v error: 2.4640448093414307 mm

Highest mean error: 2.7654218673706055 mm for frame 89

Lowest mean error: 2.2852554321289062 mm for frame 238

Saving results

Total time: 67.68566060066223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578984
Iteration 2/25 | Loss: 0.00151995
Iteration 3/25 | Loss: 0.00133783
Iteration 4/25 | Loss: 0.00125592
Iteration 5/25 | Loss: 0.00124913
Iteration 6/25 | Loss: 0.00124409
Iteration 7/25 | Loss: 0.00124261
Iteration 8/25 | Loss: 0.00124081
Iteration 9/25 | Loss: 0.00124011
Iteration 10/25 | Loss: 0.00123934
Iteration 11/25 | Loss: 0.00123886
Iteration 12/25 | Loss: 0.00123865
Iteration 13/25 | Loss: 0.00123858
Iteration 14/25 | Loss: 0.00123858
Iteration 15/25 | Loss: 0.00123858
Iteration 16/25 | Loss: 0.00123858
Iteration 17/25 | Loss: 0.00123858
Iteration 18/25 | Loss: 0.00123857
Iteration 19/25 | Loss: 0.00123857
Iteration 20/25 | Loss: 0.00123857
Iteration 21/25 | Loss: 0.00123857
Iteration 22/25 | Loss: 0.00123857
Iteration 23/25 | Loss: 0.00123857
Iteration 24/25 | Loss: 0.00123857
Iteration 25/25 | Loss: 0.00123857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.40864754
Iteration 2/25 | Loss: 0.00200117
Iteration 3/25 | Loss: 0.00197955
Iteration 4/25 | Loss: 0.00197955
Iteration 5/25 | Loss: 0.00197955
Iteration 6/25 | Loss: 0.00197954
Iteration 7/25 | Loss: 0.00197954
Iteration 8/25 | Loss: 0.00197954
Iteration 9/25 | Loss: 0.00197954
Iteration 10/25 | Loss: 0.00197954
Iteration 11/25 | Loss: 0.00197954
Iteration 12/25 | Loss: 0.00197954
Iteration 13/25 | Loss: 0.00197954
Iteration 14/25 | Loss: 0.00197954
Iteration 15/25 | Loss: 0.00197954
Iteration 16/25 | Loss: 0.00197954
Iteration 17/25 | Loss: 0.00197954
Iteration 18/25 | Loss: 0.00197954
Iteration 19/25 | Loss: 0.00197954
Iteration 20/25 | Loss: 0.00197954
Iteration 21/25 | Loss: 0.00197954
Iteration 22/25 | Loss: 0.00197954
Iteration 23/25 | Loss: 0.00197954
Iteration 24/25 | Loss: 0.00197954
Iteration 25/25 | Loss: 0.00197954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197954
Iteration 2/1000 | Loss: 0.00004331
Iteration 3/1000 | Loss: 0.00001492
Iteration 4/1000 | Loss: 0.00001315
Iteration 5/1000 | Loss: 0.00004465
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00004640
Iteration 9/1000 | Loss: 0.00001241
Iteration 10/1000 | Loss: 0.00001754
Iteration 11/1000 | Loss: 0.00001084
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001030
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001028
Iteration 17/1000 | Loss: 0.00001027
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001012
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001006
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001005
Iteration 29/1000 | Loss: 0.00001005
Iteration 30/1000 | Loss: 0.00001005
Iteration 31/1000 | Loss: 0.00001005
Iteration 32/1000 | Loss: 0.00001005
Iteration 33/1000 | Loss: 0.00001004
Iteration 34/1000 | Loss: 0.00001001
Iteration 35/1000 | Loss: 0.00001001
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00001000
Iteration 38/1000 | Loss: 0.00001000
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000999
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00003967
Iteration 44/1000 | Loss: 0.00001018
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000985
Iteration 47/1000 | Loss: 0.00000985
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000985
Iteration 50/1000 | Loss: 0.00000984
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000984
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000984
Iteration 61/1000 | Loss: 0.00000984
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [9.843854058999568e-06, 9.843854058999568e-06, 9.843854058999568e-06, 9.843854058999568e-06, 9.843854058999568e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.843854058999568e-06

Optimization complete. Final v2v error: 2.6853480339050293 mm

Highest mean error: 3.28501558303833 mm for frame 205

Lowest mean error: 2.414585590362549 mm for frame 137

Saving results

Total time: 56.28103971481323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042113
Iteration 2/25 | Loss: 0.00163716
Iteration 3/25 | Loss: 0.00135981
Iteration 4/25 | Loss: 0.00133603
Iteration 5/25 | Loss: 0.00132946
Iteration 6/25 | Loss: 0.00132780
Iteration 7/25 | Loss: 0.00132780
Iteration 8/25 | Loss: 0.00132780
Iteration 9/25 | Loss: 0.00132780
Iteration 10/25 | Loss: 0.00132780
Iteration 11/25 | Loss: 0.00132780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001327798468992114, 0.001327798468992114, 0.001327798468992114, 0.001327798468992114, 0.001327798468992114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001327798468992114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05253327
Iteration 2/25 | Loss: 0.00224103
Iteration 3/25 | Loss: 0.00224102
Iteration 4/25 | Loss: 0.00224102
Iteration 5/25 | Loss: 0.00224102
Iteration 6/25 | Loss: 0.00224102
Iteration 7/25 | Loss: 0.00224102
Iteration 8/25 | Loss: 0.00224102
Iteration 9/25 | Loss: 0.00224102
Iteration 10/25 | Loss: 0.00224102
Iteration 11/25 | Loss: 0.00224102
Iteration 12/25 | Loss: 0.00224102
Iteration 13/25 | Loss: 0.00224102
Iteration 14/25 | Loss: 0.00224102
Iteration 15/25 | Loss: 0.00224102
Iteration 16/25 | Loss: 0.00224102
Iteration 17/25 | Loss: 0.00224102
Iteration 18/25 | Loss: 0.00224102
Iteration 19/25 | Loss: 0.00224102
Iteration 20/25 | Loss: 0.00224102
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022410196252167225, 0.0022410196252167225, 0.0022410196252167225, 0.0022410196252167225, 0.0022410196252167225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022410196252167225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224102
Iteration 2/1000 | Loss: 0.00005998
Iteration 3/1000 | Loss: 0.00003760
Iteration 4/1000 | Loss: 0.00002928
Iteration 5/1000 | Loss: 0.00002641
Iteration 6/1000 | Loss: 0.00002488
Iteration 7/1000 | Loss: 0.00002401
Iteration 8/1000 | Loss: 0.00002329
Iteration 9/1000 | Loss: 0.00002278
Iteration 10/1000 | Loss: 0.00002241
Iteration 11/1000 | Loss: 0.00002215
Iteration 12/1000 | Loss: 0.00002200
Iteration 13/1000 | Loss: 0.00002192
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002170
Iteration 16/1000 | Loss: 0.00002162
Iteration 17/1000 | Loss: 0.00002151
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002134
Iteration 20/1000 | Loss: 0.00002127
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002114
Iteration 23/1000 | Loss: 0.00002112
Iteration 24/1000 | Loss: 0.00002112
Iteration 25/1000 | Loss: 0.00002111
Iteration 26/1000 | Loss: 0.00002109
Iteration 27/1000 | Loss: 0.00002109
Iteration 28/1000 | Loss: 0.00002109
Iteration 29/1000 | Loss: 0.00002109
Iteration 30/1000 | Loss: 0.00002108
Iteration 31/1000 | Loss: 0.00002108
Iteration 32/1000 | Loss: 0.00002108
Iteration 33/1000 | Loss: 0.00002108
Iteration 34/1000 | Loss: 0.00002107
Iteration 35/1000 | Loss: 0.00002107
Iteration 36/1000 | Loss: 0.00002107
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002104
Iteration 39/1000 | Loss: 0.00002104
Iteration 40/1000 | Loss: 0.00002104
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002103
Iteration 43/1000 | Loss: 0.00002103
Iteration 44/1000 | Loss: 0.00002103
Iteration 45/1000 | Loss: 0.00002103
Iteration 46/1000 | Loss: 0.00002103
Iteration 47/1000 | Loss: 0.00002102
Iteration 48/1000 | Loss: 0.00002102
Iteration 49/1000 | Loss: 0.00002101
Iteration 50/1000 | Loss: 0.00002101
Iteration 51/1000 | Loss: 0.00002100
Iteration 52/1000 | Loss: 0.00002100
Iteration 53/1000 | Loss: 0.00002100
Iteration 54/1000 | Loss: 0.00002099
Iteration 55/1000 | Loss: 0.00002099
Iteration 56/1000 | Loss: 0.00002098
Iteration 57/1000 | Loss: 0.00002098
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002097
Iteration 61/1000 | Loss: 0.00002097
Iteration 62/1000 | Loss: 0.00002097
Iteration 63/1000 | Loss: 0.00002096
Iteration 64/1000 | Loss: 0.00002096
Iteration 65/1000 | Loss: 0.00002096
Iteration 66/1000 | Loss: 0.00002096
Iteration 67/1000 | Loss: 0.00002096
Iteration 68/1000 | Loss: 0.00002095
Iteration 69/1000 | Loss: 0.00002095
Iteration 70/1000 | Loss: 0.00002095
Iteration 71/1000 | Loss: 0.00002095
Iteration 72/1000 | Loss: 0.00002095
Iteration 73/1000 | Loss: 0.00002094
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002094
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002094
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002094
Iteration 80/1000 | Loss: 0.00002094
Iteration 81/1000 | Loss: 0.00002093
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002093
Iteration 84/1000 | Loss: 0.00002093
Iteration 85/1000 | Loss: 0.00002093
Iteration 86/1000 | Loss: 0.00002093
Iteration 87/1000 | Loss: 0.00002093
Iteration 88/1000 | Loss: 0.00002093
Iteration 89/1000 | Loss: 0.00002093
Iteration 90/1000 | Loss: 0.00002093
Iteration 91/1000 | Loss: 0.00002092
Iteration 92/1000 | Loss: 0.00002092
Iteration 93/1000 | Loss: 0.00002091
Iteration 94/1000 | Loss: 0.00002091
Iteration 95/1000 | Loss: 0.00002091
Iteration 96/1000 | Loss: 0.00002090
Iteration 97/1000 | Loss: 0.00002090
Iteration 98/1000 | Loss: 0.00002090
Iteration 99/1000 | Loss: 0.00002090
Iteration 100/1000 | Loss: 0.00002089
Iteration 101/1000 | Loss: 0.00002089
Iteration 102/1000 | Loss: 0.00002089
Iteration 103/1000 | Loss: 0.00002089
Iteration 104/1000 | Loss: 0.00002088
Iteration 105/1000 | Loss: 0.00002088
Iteration 106/1000 | Loss: 0.00002088
Iteration 107/1000 | Loss: 0.00002088
Iteration 108/1000 | Loss: 0.00002088
Iteration 109/1000 | Loss: 0.00002088
Iteration 110/1000 | Loss: 0.00002088
Iteration 111/1000 | Loss: 0.00002088
Iteration 112/1000 | Loss: 0.00002088
Iteration 113/1000 | Loss: 0.00002088
Iteration 114/1000 | Loss: 0.00002087
Iteration 115/1000 | Loss: 0.00002087
Iteration 116/1000 | Loss: 0.00002086
Iteration 117/1000 | Loss: 0.00002086
Iteration 118/1000 | Loss: 0.00002086
Iteration 119/1000 | Loss: 0.00002086
Iteration 120/1000 | Loss: 0.00002086
Iteration 121/1000 | Loss: 0.00002086
Iteration 122/1000 | Loss: 0.00002086
Iteration 123/1000 | Loss: 0.00002086
Iteration 124/1000 | Loss: 0.00002086
Iteration 125/1000 | Loss: 0.00002085
Iteration 126/1000 | Loss: 0.00002085
Iteration 127/1000 | Loss: 0.00002085
Iteration 128/1000 | Loss: 0.00002085
Iteration 129/1000 | Loss: 0.00002085
Iteration 130/1000 | Loss: 0.00002085
Iteration 131/1000 | Loss: 0.00002085
Iteration 132/1000 | Loss: 0.00002084
Iteration 133/1000 | Loss: 0.00002084
Iteration 134/1000 | Loss: 0.00002084
Iteration 135/1000 | Loss: 0.00002084
Iteration 136/1000 | Loss: 0.00002084
Iteration 137/1000 | Loss: 0.00002084
Iteration 138/1000 | Loss: 0.00002084
Iteration 139/1000 | Loss: 0.00002084
Iteration 140/1000 | Loss: 0.00002084
Iteration 141/1000 | Loss: 0.00002084
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002083
Iteration 144/1000 | Loss: 0.00002083
Iteration 145/1000 | Loss: 0.00002083
Iteration 146/1000 | Loss: 0.00002083
Iteration 147/1000 | Loss: 0.00002083
Iteration 148/1000 | Loss: 0.00002083
Iteration 149/1000 | Loss: 0.00002083
Iteration 150/1000 | Loss: 0.00002083
Iteration 151/1000 | Loss: 0.00002083
Iteration 152/1000 | Loss: 0.00002083
Iteration 153/1000 | Loss: 0.00002082
Iteration 154/1000 | Loss: 0.00002082
Iteration 155/1000 | Loss: 0.00002082
Iteration 156/1000 | Loss: 0.00002082
Iteration 157/1000 | Loss: 0.00002082
Iteration 158/1000 | Loss: 0.00002082
Iteration 159/1000 | Loss: 0.00002082
Iteration 160/1000 | Loss: 0.00002082
Iteration 161/1000 | Loss: 0.00002082
Iteration 162/1000 | Loss: 0.00002082
Iteration 163/1000 | Loss: 0.00002082
Iteration 164/1000 | Loss: 0.00002081
Iteration 165/1000 | Loss: 0.00002081
Iteration 166/1000 | Loss: 0.00002081
Iteration 167/1000 | Loss: 0.00002081
Iteration 168/1000 | Loss: 0.00002081
Iteration 169/1000 | Loss: 0.00002081
Iteration 170/1000 | Loss: 0.00002081
Iteration 171/1000 | Loss: 0.00002081
Iteration 172/1000 | Loss: 0.00002081
Iteration 173/1000 | Loss: 0.00002081
Iteration 174/1000 | Loss: 0.00002081
Iteration 175/1000 | Loss: 0.00002081
Iteration 176/1000 | Loss: 0.00002081
Iteration 177/1000 | Loss: 0.00002081
Iteration 178/1000 | Loss: 0.00002081
Iteration 179/1000 | Loss: 0.00002081
Iteration 180/1000 | Loss: 0.00002080
Iteration 181/1000 | Loss: 0.00002080
Iteration 182/1000 | Loss: 0.00002080
Iteration 183/1000 | Loss: 0.00002080
Iteration 184/1000 | Loss: 0.00002080
Iteration 185/1000 | Loss: 0.00002080
Iteration 186/1000 | Loss: 0.00002080
Iteration 187/1000 | Loss: 0.00002080
Iteration 188/1000 | Loss: 0.00002080
Iteration 189/1000 | Loss: 0.00002080
Iteration 190/1000 | Loss: 0.00002080
Iteration 191/1000 | Loss: 0.00002080
Iteration 192/1000 | Loss: 0.00002080
Iteration 193/1000 | Loss: 0.00002080
Iteration 194/1000 | Loss: 0.00002080
Iteration 195/1000 | Loss: 0.00002079
Iteration 196/1000 | Loss: 0.00002079
Iteration 197/1000 | Loss: 0.00002079
Iteration 198/1000 | Loss: 0.00002079
Iteration 199/1000 | Loss: 0.00002079
Iteration 200/1000 | Loss: 0.00002079
Iteration 201/1000 | Loss: 0.00002079
Iteration 202/1000 | Loss: 0.00002079
Iteration 203/1000 | Loss: 0.00002079
Iteration 204/1000 | Loss: 0.00002079
Iteration 205/1000 | Loss: 0.00002079
Iteration 206/1000 | Loss: 0.00002079
Iteration 207/1000 | Loss: 0.00002079
Iteration 208/1000 | Loss: 0.00002079
Iteration 209/1000 | Loss: 0.00002079
Iteration 210/1000 | Loss: 0.00002079
Iteration 211/1000 | Loss: 0.00002079
Iteration 212/1000 | Loss: 0.00002079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.0788065739907324e-05, 2.0788065739907324e-05, 2.0788065739907324e-05, 2.0788065739907324e-05, 2.0788065739907324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0788065739907324e-05

Optimization complete. Final v2v error: 3.7671287059783936 mm

Highest mean error: 5.199653625488281 mm for frame 47

Lowest mean error: 3.29478120803833 mm for frame 155

Saving results

Total time: 49.55009913444519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807893
Iteration 2/25 | Loss: 0.00133604
Iteration 3/25 | Loss: 0.00124300
Iteration 4/25 | Loss: 0.00123626
Iteration 5/25 | Loss: 0.00123480
Iteration 6/25 | Loss: 0.00123480
Iteration 7/25 | Loss: 0.00123480
Iteration 8/25 | Loss: 0.00123480
Iteration 9/25 | Loss: 0.00123480
Iteration 10/25 | Loss: 0.00123480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012347983429208398, 0.0012347983429208398, 0.0012347983429208398, 0.0012347983429208398, 0.0012347983429208398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012347983429208398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23299801
Iteration 2/25 | Loss: 0.00230811
Iteration 3/25 | Loss: 0.00230811
Iteration 4/25 | Loss: 0.00230811
Iteration 5/25 | Loss: 0.00230811
Iteration 6/25 | Loss: 0.00230811
Iteration 7/25 | Loss: 0.00230811
Iteration 8/25 | Loss: 0.00230811
Iteration 9/25 | Loss: 0.00230811
Iteration 10/25 | Loss: 0.00230811
Iteration 11/25 | Loss: 0.00230811
Iteration 12/25 | Loss: 0.00230811
Iteration 13/25 | Loss: 0.00230811
Iteration 14/25 | Loss: 0.00230811
Iteration 15/25 | Loss: 0.00230811
Iteration 16/25 | Loss: 0.00230811
Iteration 17/25 | Loss: 0.00230811
Iteration 18/25 | Loss: 0.00230811
Iteration 19/25 | Loss: 0.00230811
Iteration 20/25 | Loss: 0.00230811
Iteration 21/25 | Loss: 0.00230811
Iteration 22/25 | Loss: 0.00230811
Iteration 23/25 | Loss: 0.00230811
Iteration 24/25 | Loss: 0.00230811
Iteration 25/25 | Loss: 0.00230811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230811
Iteration 2/1000 | Loss: 0.00002611
Iteration 3/1000 | Loss: 0.00001861
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001193
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001107
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001031
Iteration 13/1000 | Loss: 0.00001030
Iteration 14/1000 | Loss: 0.00001018
Iteration 15/1000 | Loss: 0.00001009
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000995
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000992
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000988
Iteration 26/1000 | Loss: 0.00000984
Iteration 27/1000 | Loss: 0.00000983
Iteration 28/1000 | Loss: 0.00000978
Iteration 29/1000 | Loss: 0.00000978
Iteration 30/1000 | Loss: 0.00000978
Iteration 31/1000 | Loss: 0.00000978
Iteration 32/1000 | Loss: 0.00000977
Iteration 33/1000 | Loss: 0.00000977
Iteration 34/1000 | Loss: 0.00000977
Iteration 35/1000 | Loss: 0.00000977
Iteration 36/1000 | Loss: 0.00000977
Iteration 37/1000 | Loss: 0.00000977
Iteration 38/1000 | Loss: 0.00000977
Iteration 39/1000 | Loss: 0.00000977
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000976
Iteration 42/1000 | Loss: 0.00000972
Iteration 43/1000 | Loss: 0.00000972
Iteration 44/1000 | Loss: 0.00000972
Iteration 45/1000 | Loss: 0.00000971
Iteration 46/1000 | Loss: 0.00000971
Iteration 47/1000 | Loss: 0.00000971
Iteration 48/1000 | Loss: 0.00000970
Iteration 49/1000 | Loss: 0.00000970
Iteration 50/1000 | Loss: 0.00000969
Iteration 51/1000 | Loss: 0.00000968
Iteration 52/1000 | Loss: 0.00000968
Iteration 53/1000 | Loss: 0.00000966
Iteration 54/1000 | Loss: 0.00000966
Iteration 55/1000 | Loss: 0.00000966
Iteration 56/1000 | Loss: 0.00000966
Iteration 57/1000 | Loss: 0.00000966
Iteration 58/1000 | Loss: 0.00000966
Iteration 59/1000 | Loss: 0.00000966
Iteration 60/1000 | Loss: 0.00000965
Iteration 61/1000 | Loss: 0.00000965
Iteration 62/1000 | Loss: 0.00000965
Iteration 63/1000 | Loss: 0.00000965
Iteration 64/1000 | Loss: 0.00000965
Iteration 65/1000 | Loss: 0.00000965
Iteration 66/1000 | Loss: 0.00000965
Iteration 67/1000 | Loss: 0.00000965
Iteration 68/1000 | Loss: 0.00000964
Iteration 69/1000 | Loss: 0.00000963
Iteration 70/1000 | Loss: 0.00000963
Iteration 71/1000 | Loss: 0.00000962
Iteration 72/1000 | Loss: 0.00000961
Iteration 73/1000 | Loss: 0.00000961
Iteration 74/1000 | Loss: 0.00000961
Iteration 75/1000 | Loss: 0.00000960
Iteration 76/1000 | Loss: 0.00000960
Iteration 77/1000 | Loss: 0.00000959
Iteration 78/1000 | Loss: 0.00000958
Iteration 79/1000 | Loss: 0.00000957
Iteration 80/1000 | Loss: 0.00000957
Iteration 81/1000 | Loss: 0.00000956
Iteration 82/1000 | Loss: 0.00000956
Iteration 83/1000 | Loss: 0.00000956
Iteration 84/1000 | Loss: 0.00000956
Iteration 85/1000 | Loss: 0.00000955
Iteration 86/1000 | Loss: 0.00000955
Iteration 87/1000 | Loss: 0.00000955
Iteration 88/1000 | Loss: 0.00000955
Iteration 89/1000 | Loss: 0.00000954
Iteration 90/1000 | Loss: 0.00000954
Iteration 91/1000 | Loss: 0.00000953
Iteration 92/1000 | Loss: 0.00000953
Iteration 93/1000 | Loss: 0.00000953
Iteration 94/1000 | Loss: 0.00000952
Iteration 95/1000 | Loss: 0.00000952
Iteration 96/1000 | Loss: 0.00000952
Iteration 97/1000 | Loss: 0.00000952
Iteration 98/1000 | Loss: 0.00000951
Iteration 99/1000 | Loss: 0.00000951
Iteration 100/1000 | Loss: 0.00000951
Iteration 101/1000 | Loss: 0.00000951
Iteration 102/1000 | Loss: 0.00000951
Iteration 103/1000 | Loss: 0.00000951
Iteration 104/1000 | Loss: 0.00000951
Iteration 105/1000 | Loss: 0.00000950
Iteration 106/1000 | Loss: 0.00000950
Iteration 107/1000 | Loss: 0.00000950
Iteration 108/1000 | Loss: 0.00000949
Iteration 109/1000 | Loss: 0.00000949
Iteration 110/1000 | Loss: 0.00000949
Iteration 111/1000 | Loss: 0.00000948
Iteration 112/1000 | Loss: 0.00000948
Iteration 113/1000 | Loss: 0.00000948
Iteration 114/1000 | Loss: 0.00000948
Iteration 115/1000 | Loss: 0.00000948
Iteration 116/1000 | Loss: 0.00000947
Iteration 117/1000 | Loss: 0.00000947
Iteration 118/1000 | Loss: 0.00000947
Iteration 119/1000 | Loss: 0.00000947
Iteration 120/1000 | Loss: 0.00000947
Iteration 121/1000 | Loss: 0.00000947
Iteration 122/1000 | Loss: 0.00000947
Iteration 123/1000 | Loss: 0.00000947
Iteration 124/1000 | Loss: 0.00000947
Iteration 125/1000 | Loss: 0.00000946
Iteration 126/1000 | Loss: 0.00000946
Iteration 127/1000 | Loss: 0.00000946
Iteration 128/1000 | Loss: 0.00000946
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000945
Iteration 132/1000 | Loss: 0.00000945
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000945
Iteration 138/1000 | Loss: 0.00000945
Iteration 139/1000 | Loss: 0.00000945
Iteration 140/1000 | Loss: 0.00000945
Iteration 141/1000 | Loss: 0.00000944
Iteration 142/1000 | Loss: 0.00000944
Iteration 143/1000 | Loss: 0.00000944
Iteration 144/1000 | Loss: 0.00000944
Iteration 145/1000 | Loss: 0.00000944
Iteration 146/1000 | Loss: 0.00000944
Iteration 147/1000 | Loss: 0.00000944
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000944
Iteration 150/1000 | Loss: 0.00000944
Iteration 151/1000 | Loss: 0.00000944
Iteration 152/1000 | Loss: 0.00000944
Iteration 153/1000 | Loss: 0.00000943
Iteration 154/1000 | Loss: 0.00000943
Iteration 155/1000 | Loss: 0.00000943
Iteration 156/1000 | Loss: 0.00000943
Iteration 157/1000 | Loss: 0.00000943
Iteration 158/1000 | Loss: 0.00000943
Iteration 159/1000 | Loss: 0.00000943
Iteration 160/1000 | Loss: 0.00000943
Iteration 161/1000 | Loss: 0.00000943
Iteration 162/1000 | Loss: 0.00000943
Iteration 163/1000 | Loss: 0.00000942
Iteration 164/1000 | Loss: 0.00000942
Iteration 165/1000 | Loss: 0.00000942
Iteration 166/1000 | Loss: 0.00000942
Iteration 167/1000 | Loss: 0.00000942
Iteration 168/1000 | Loss: 0.00000942
Iteration 169/1000 | Loss: 0.00000942
Iteration 170/1000 | Loss: 0.00000942
Iteration 171/1000 | Loss: 0.00000942
Iteration 172/1000 | Loss: 0.00000941
Iteration 173/1000 | Loss: 0.00000941
Iteration 174/1000 | Loss: 0.00000941
Iteration 175/1000 | Loss: 0.00000941
Iteration 176/1000 | Loss: 0.00000941
Iteration 177/1000 | Loss: 0.00000940
Iteration 178/1000 | Loss: 0.00000940
Iteration 179/1000 | Loss: 0.00000940
Iteration 180/1000 | Loss: 0.00000940
Iteration 181/1000 | Loss: 0.00000940
Iteration 182/1000 | Loss: 0.00000940
Iteration 183/1000 | Loss: 0.00000940
Iteration 184/1000 | Loss: 0.00000940
Iteration 185/1000 | Loss: 0.00000940
Iteration 186/1000 | Loss: 0.00000940
Iteration 187/1000 | Loss: 0.00000940
Iteration 188/1000 | Loss: 0.00000940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [9.403628610016312e-06, 9.403628610016312e-06, 9.403628610016312e-06, 9.403628610016312e-06, 9.403628610016312e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.403628610016312e-06

Optimization complete. Final v2v error: 2.609973430633545 mm

Highest mean error: 3.6254611015319824 mm for frame 66

Lowest mean error: 2.2963337898254395 mm for frame 142

Saving results

Total time: 42.541232109069824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862315
Iteration 2/25 | Loss: 0.00164855
Iteration 3/25 | Loss: 0.00140075
Iteration 4/25 | Loss: 0.00137064
Iteration 5/25 | Loss: 0.00136408
Iteration 6/25 | Loss: 0.00136485
Iteration 7/25 | Loss: 0.00135435
Iteration 8/25 | Loss: 0.00134271
Iteration 9/25 | Loss: 0.00131583
Iteration 10/25 | Loss: 0.00130760
Iteration 11/25 | Loss: 0.00130591
Iteration 12/25 | Loss: 0.00130557
Iteration 13/25 | Loss: 0.00130547
Iteration 14/25 | Loss: 0.00130547
Iteration 15/25 | Loss: 0.00130546
Iteration 16/25 | Loss: 0.00130546
Iteration 17/25 | Loss: 0.00130546
Iteration 18/25 | Loss: 0.00130546
Iteration 19/25 | Loss: 0.00130546
Iteration 20/25 | Loss: 0.00130546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013054648879915476, 0.0013054648879915476, 0.0013054648879915476, 0.0013054648879915476, 0.0013054648879915476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013054648879915476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82330167
Iteration 2/25 | Loss: 0.00119131
Iteration 3/25 | Loss: 0.00119130
Iteration 4/25 | Loss: 0.00119130
Iteration 5/25 | Loss: 0.00119130
Iteration 6/25 | Loss: 0.00119130
Iteration 7/25 | Loss: 0.00119130
Iteration 8/25 | Loss: 0.00119130
Iteration 9/25 | Loss: 0.00119130
Iteration 10/25 | Loss: 0.00119130
Iteration 11/25 | Loss: 0.00119130
Iteration 12/25 | Loss: 0.00119130
Iteration 13/25 | Loss: 0.00119130
Iteration 14/25 | Loss: 0.00119130
Iteration 15/25 | Loss: 0.00119130
Iteration 16/25 | Loss: 0.00119130
Iteration 17/25 | Loss: 0.00119130
Iteration 18/25 | Loss: 0.00119130
Iteration 19/25 | Loss: 0.00119130
Iteration 20/25 | Loss: 0.00119130
Iteration 21/25 | Loss: 0.00119130
Iteration 22/25 | Loss: 0.00119130
Iteration 23/25 | Loss: 0.00119130
Iteration 24/25 | Loss: 0.00119130
Iteration 25/25 | Loss: 0.00119130

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119130
Iteration 2/1000 | Loss: 0.00004112
Iteration 3/1000 | Loss: 0.00003322
Iteration 4/1000 | Loss: 0.00003045
Iteration 5/1000 | Loss: 0.00002904
Iteration 6/1000 | Loss: 0.00002830
Iteration 7/1000 | Loss: 0.00002750
Iteration 8/1000 | Loss: 0.00002702
Iteration 9/1000 | Loss: 0.00002668
Iteration 10/1000 | Loss: 0.00002649
Iteration 11/1000 | Loss: 0.00002630
Iteration 12/1000 | Loss: 0.00002627
Iteration 13/1000 | Loss: 0.00002608
Iteration 14/1000 | Loss: 0.00002607
Iteration 15/1000 | Loss: 0.00002604
Iteration 16/1000 | Loss: 0.00002591
Iteration 17/1000 | Loss: 0.00002588
Iteration 18/1000 | Loss: 0.00002588
Iteration 19/1000 | Loss: 0.00002588
Iteration 20/1000 | Loss: 0.00002588
Iteration 21/1000 | Loss: 0.00002588
Iteration 22/1000 | Loss: 0.00002587
Iteration 23/1000 | Loss: 0.00002587
Iteration 24/1000 | Loss: 0.00002587
Iteration 25/1000 | Loss: 0.00002583
Iteration 26/1000 | Loss: 0.00002580
Iteration 27/1000 | Loss: 0.00002580
Iteration 28/1000 | Loss: 0.00002579
Iteration 29/1000 | Loss: 0.00002579
Iteration 30/1000 | Loss: 0.00002579
Iteration 31/1000 | Loss: 0.00002579
Iteration 32/1000 | Loss: 0.00002579
Iteration 33/1000 | Loss: 0.00002579
Iteration 34/1000 | Loss: 0.00002579
Iteration 35/1000 | Loss: 0.00002578
Iteration 36/1000 | Loss: 0.00002578
Iteration 37/1000 | Loss: 0.00002578
Iteration 38/1000 | Loss: 0.00002577
Iteration 39/1000 | Loss: 0.00002577
Iteration 40/1000 | Loss: 0.00002577
Iteration 41/1000 | Loss: 0.00002577
Iteration 42/1000 | Loss: 0.00002577
Iteration 43/1000 | Loss: 0.00002577
Iteration 44/1000 | Loss: 0.00002576
Iteration 45/1000 | Loss: 0.00002576
Iteration 46/1000 | Loss: 0.00002576
Iteration 47/1000 | Loss: 0.00002576
Iteration 48/1000 | Loss: 0.00002576
Iteration 49/1000 | Loss: 0.00002576
Iteration 50/1000 | Loss: 0.00002576
Iteration 51/1000 | Loss: 0.00002576
Iteration 52/1000 | Loss: 0.00002576
Iteration 53/1000 | Loss: 0.00002576
Iteration 54/1000 | Loss: 0.00002575
Iteration 55/1000 | Loss: 0.00002575
Iteration 56/1000 | Loss: 0.00002575
Iteration 57/1000 | Loss: 0.00002575
Iteration 58/1000 | Loss: 0.00002575
Iteration 59/1000 | Loss: 0.00002575
Iteration 60/1000 | Loss: 0.00002575
Iteration 61/1000 | Loss: 0.00002575
Iteration 62/1000 | Loss: 0.00002575
Iteration 63/1000 | Loss: 0.00002574
Iteration 64/1000 | Loss: 0.00002574
Iteration 65/1000 | Loss: 0.00002574
Iteration 66/1000 | Loss: 0.00002574
Iteration 67/1000 | Loss: 0.00002574
Iteration 68/1000 | Loss: 0.00002574
Iteration 69/1000 | Loss: 0.00002574
Iteration 70/1000 | Loss: 0.00002574
Iteration 71/1000 | Loss: 0.00002574
Iteration 72/1000 | Loss: 0.00002574
Iteration 73/1000 | Loss: 0.00002574
Iteration 74/1000 | Loss: 0.00002574
Iteration 75/1000 | Loss: 0.00002574
Iteration 76/1000 | Loss: 0.00002574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [2.5744664526428096e-05, 2.5744664526428096e-05, 2.5744664526428096e-05, 2.5744664526428096e-05, 2.5744664526428096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5744664526428096e-05

Optimization complete. Final v2v error: 4.3276543617248535 mm

Highest mean error: 4.601383686065674 mm for frame 134

Lowest mean error: 4.219656944274902 mm for frame 67

Saving results

Total time: 45.19217228889465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029190
Iteration 2/25 | Loss: 0.00179615
Iteration 3/25 | Loss: 0.00144605
Iteration 4/25 | Loss: 0.00140267
Iteration 5/25 | Loss: 0.00139505
Iteration 6/25 | Loss: 0.00139337
Iteration 7/25 | Loss: 0.00139337
Iteration 8/25 | Loss: 0.00139337
Iteration 9/25 | Loss: 0.00139337
Iteration 10/25 | Loss: 0.00139337
Iteration 11/25 | Loss: 0.00139337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001393373473547399, 0.001393373473547399, 0.001393373473547399, 0.001393373473547399, 0.001393373473547399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001393373473547399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10560477
Iteration 2/25 | Loss: 0.00201705
Iteration 3/25 | Loss: 0.00201700
Iteration 4/25 | Loss: 0.00201699
Iteration 5/25 | Loss: 0.00201699
Iteration 6/25 | Loss: 0.00201699
Iteration 7/25 | Loss: 0.00201699
Iteration 8/25 | Loss: 0.00201699
Iteration 9/25 | Loss: 0.00201699
Iteration 10/25 | Loss: 0.00201699
Iteration 11/25 | Loss: 0.00201699
Iteration 12/25 | Loss: 0.00201699
Iteration 13/25 | Loss: 0.00201699
Iteration 14/25 | Loss: 0.00201699
Iteration 15/25 | Loss: 0.00201699
Iteration 16/25 | Loss: 0.00201699
Iteration 17/25 | Loss: 0.00201699
Iteration 18/25 | Loss: 0.00201699
Iteration 19/25 | Loss: 0.00201699
Iteration 20/25 | Loss: 0.00201699
Iteration 21/25 | Loss: 0.00201699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002016990212723613, 0.002016990212723613, 0.002016990212723613, 0.002016990212723613, 0.002016990212723613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002016990212723613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201699
Iteration 2/1000 | Loss: 0.00006564
Iteration 3/1000 | Loss: 0.00003820
Iteration 4/1000 | Loss: 0.00003335
Iteration 5/1000 | Loss: 0.00003183
Iteration 6/1000 | Loss: 0.00003034
Iteration 7/1000 | Loss: 0.00002960
Iteration 8/1000 | Loss: 0.00002908
Iteration 9/1000 | Loss: 0.00002877
Iteration 10/1000 | Loss: 0.00002826
Iteration 11/1000 | Loss: 0.00002797
Iteration 12/1000 | Loss: 0.00002764
Iteration 13/1000 | Loss: 0.00002739
Iteration 14/1000 | Loss: 0.00002721
Iteration 15/1000 | Loss: 0.00002703
Iteration 16/1000 | Loss: 0.00002688
Iteration 17/1000 | Loss: 0.00002686
Iteration 18/1000 | Loss: 0.00002685
Iteration 19/1000 | Loss: 0.00002685
Iteration 20/1000 | Loss: 0.00002683
Iteration 21/1000 | Loss: 0.00002683
Iteration 22/1000 | Loss: 0.00002683
Iteration 23/1000 | Loss: 0.00002683
Iteration 24/1000 | Loss: 0.00002683
Iteration 25/1000 | Loss: 0.00002681
Iteration 26/1000 | Loss: 0.00002681
Iteration 27/1000 | Loss: 0.00002681
Iteration 28/1000 | Loss: 0.00002681
Iteration 29/1000 | Loss: 0.00002680
Iteration 30/1000 | Loss: 0.00002680
Iteration 31/1000 | Loss: 0.00002680
Iteration 32/1000 | Loss: 0.00002680
Iteration 33/1000 | Loss: 0.00002680
Iteration 34/1000 | Loss: 0.00002680
Iteration 35/1000 | Loss: 0.00002680
Iteration 36/1000 | Loss: 0.00002680
Iteration 37/1000 | Loss: 0.00002680
Iteration 38/1000 | Loss: 0.00002680
Iteration 39/1000 | Loss: 0.00002680
Iteration 40/1000 | Loss: 0.00002680
Iteration 41/1000 | Loss: 0.00002680
Iteration 42/1000 | Loss: 0.00002679
Iteration 43/1000 | Loss: 0.00002678
Iteration 44/1000 | Loss: 0.00002677
Iteration 45/1000 | Loss: 0.00002677
Iteration 46/1000 | Loss: 0.00002676
Iteration 47/1000 | Loss: 0.00002676
Iteration 48/1000 | Loss: 0.00002676
Iteration 49/1000 | Loss: 0.00002675
Iteration 50/1000 | Loss: 0.00002675
Iteration 51/1000 | Loss: 0.00002674
Iteration 52/1000 | Loss: 0.00002672
Iteration 53/1000 | Loss: 0.00002671
Iteration 54/1000 | Loss: 0.00002671
Iteration 55/1000 | Loss: 0.00002671
Iteration 56/1000 | Loss: 0.00002671
Iteration 57/1000 | Loss: 0.00002671
Iteration 58/1000 | Loss: 0.00002669
Iteration 59/1000 | Loss: 0.00002669
Iteration 60/1000 | Loss: 0.00002668
Iteration 61/1000 | Loss: 0.00002668
Iteration 62/1000 | Loss: 0.00002668
Iteration 63/1000 | Loss: 0.00002667
Iteration 64/1000 | Loss: 0.00002667
Iteration 65/1000 | Loss: 0.00002667
Iteration 66/1000 | Loss: 0.00002667
Iteration 67/1000 | Loss: 0.00002666
Iteration 68/1000 | Loss: 0.00002666
Iteration 69/1000 | Loss: 0.00002666
Iteration 70/1000 | Loss: 0.00002664
Iteration 71/1000 | Loss: 0.00002663
Iteration 72/1000 | Loss: 0.00002663
Iteration 73/1000 | Loss: 0.00002663
Iteration 74/1000 | Loss: 0.00002662
Iteration 75/1000 | Loss: 0.00002660
Iteration 76/1000 | Loss: 0.00002660
Iteration 77/1000 | Loss: 0.00002660
Iteration 78/1000 | Loss: 0.00002660
Iteration 79/1000 | Loss: 0.00002660
Iteration 80/1000 | Loss: 0.00002660
Iteration 81/1000 | Loss: 0.00002659
Iteration 82/1000 | Loss: 0.00002659
Iteration 83/1000 | Loss: 0.00002659
Iteration 84/1000 | Loss: 0.00002659
Iteration 85/1000 | Loss: 0.00002657
Iteration 86/1000 | Loss: 0.00002657
Iteration 87/1000 | Loss: 0.00002656
Iteration 88/1000 | Loss: 0.00002656
Iteration 89/1000 | Loss: 0.00002656
Iteration 90/1000 | Loss: 0.00002656
Iteration 91/1000 | Loss: 0.00002656
Iteration 92/1000 | Loss: 0.00002656
Iteration 93/1000 | Loss: 0.00002656
Iteration 94/1000 | Loss: 0.00002656
Iteration 95/1000 | Loss: 0.00002656
Iteration 96/1000 | Loss: 0.00002656
Iteration 97/1000 | Loss: 0.00002656
Iteration 98/1000 | Loss: 0.00002655
Iteration 99/1000 | Loss: 0.00002655
Iteration 100/1000 | Loss: 0.00002655
Iteration 101/1000 | Loss: 0.00002655
Iteration 102/1000 | Loss: 0.00002655
Iteration 103/1000 | Loss: 0.00002655
Iteration 104/1000 | Loss: 0.00002654
Iteration 105/1000 | Loss: 0.00002654
Iteration 106/1000 | Loss: 0.00002653
Iteration 107/1000 | Loss: 0.00002653
Iteration 108/1000 | Loss: 0.00002653
Iteration 109/1000 | Loss: 0.00002653
Iteration 110/1000 | Loss: 0.00002653
Iteration 111/1000 | Loss: 0.00002653
Iteration 112/1000 | Loss: 0.00002653
Iteration 113/1000 | Loss: 0.00002653
Iteration 114/1000 | Loss: 0.00002652
Iteration 115/1000 | Loss: 0.00002652
Iteration 116/1000 | Loss: 0.00002652
Iteration 117/1000 | Loss: 0.00002652
Iteration 118/1000 | Loss: 0.00002652
Iteration 119/1000 | Loss: 0.00002652
Iteration 120/1000 | Loss: 0.00002652
Iteration 121/1000 | Loss: 0.00002652
Iteration 122/1000 | Loss: 0.00002652
Iteration 123/1000 | Loss: 0.00002652
Iteration 124/1000 | Loss: 0.00002652
Iteration 125/1000 | Loss: 0.00002651
Iteration 126/1000 | Loss: 0.00002651
Iteration 127/1000 | Loss: 0.00002650
Iteration 128/1000 | Loss: 0.00002650
Iteration 129/1000 | Loss: 0.00002650
Iteration 130/1000 | Loss: 0.00002650
Iteration 131/1000 | Loss: 0.00002650
Iteration 132/1000 | Loss: 0.00002650
Iteration 133/1000 | Loss: 0.00002650
Iteration 134/1000 | Loss: 0.00002650
Iteration 135/1000 | Loss: 0.00002650
Iteration 136/1000 | Loss: 0.00002650
Iteration 137/1000 | Loss: 0.00002650
Iteration 138/1000 | Loss: 0.00002650
Iteration 139/1000 | Loss: 0.00002650
Iteration 140/1000 | Loss: 0.00002650
Iteration 141/1000 | Loss: 0.00002650
Iteration 142/1000 | Loss: 0.00002650
Iteration 143/1000 | Loss: 0.00002650
Iteration 144/1000 | Loss: 0.00002650
Iteration 145/1000 | Loss: 0.00002650
Iteration 146/1000 | Loss: 0.00002650
Iteration 147/1000 | Loss: 0.00002650
Iteration 148/1000 | Loss: 0.00002650
Iteration 149/1000 | Loss: 0.00002650
Iteration 150/1000 | Loss: 0.00002650
Iteration 151/1000 | Loss: 0.00002650
Iteration 152/1000 | Loss: 0.00002650
Iteration 153/1000 | Loss: 0.00002650
Iteration 154/1000 | Loss: 0.00002650
Iteration 155/1000 | Loss: 0.00002650
Iteration 156/1000 | Loss: 0.00002650
Iteration 157/1000 | Loss: 0.00002650
Iteration 158/1000 | Loss: 0.00002650
Iteration 159/1000 | Loss: 0.00002650
Iteration 160/1000 | Loss: 0.00002650
Iteration 161/1000 | Loss: 0.00002650
Iteration 162/1000 | Loss: 0.00002650
Iteration 163/1000 | Loss: 0.00002650
Iteration 164/1000 | Loss: 0.00002650
Iteration 165/1000 | Loss: 0.00002650
Iteration 166/1000 | Loss: 0.00002650
Iteration 167/1000 | Loss: 0.00002650
Iteration 168/1000 | Loss: 0.00002650
Iteration 169/1000 | Loss: 0.00002650
Iteration 170/1000 | Loss: 0.00002650
Iteration 171/1000 | Loss: 0.00002650
Iteration 172/1000 | Loss: 0.00002650
Iteration 173/1000 | Loss: 0.00002650
Iteration 174/1000 | Loss: 0.00002650
Iteration 175/1000 | Loss: 0.00002650
Iteration 176/1000 | Loss: 0.00002650
Iteration 177/1000 | Loss: 0.00002650
Iteration 178/1000 | Loss: 0.00002650
Iteration 179/1000 | Loss: 0.00002650
Iteration 180/1000 | Loss: 0.00002650
Iteration 181/1000 | Loss: 0.00002650
Iteration 182/1000 | Loss: 0.00002650
Iteration 183/1000 | Loss: 0.00002650
Iteration 184/1000 | Loss: 0.00002650
Iteration 185/1000 | Loss: 0.00002650
Iteration 186/1000 | Loss: 0.00002650
Iteration 187/1000 | Loss: 0.00002650
Iteration 188/1000 | Loss: 0.00002650
Iteration 189/1000 | Loss: 0.00002650
Iteration 190/1000 | Loss: 0.00002650
Iteration 191/1000 | Loss: 0.00002650
Iteration 192/1000 | Loss: 0.00002650
Iteration 193/1000 | Loss: 0.00002650
Iteration 194/1000 | Loss: 0.00002650
Iteration 195/1000 | Loss: 0.00002650
Iteration 196/1000 | Loss: 0.00002650
Iteration 197/1000 | Loss: 0.00002650
Iteration 198/1000 | Loss: 0.00002650
Iteration 199/1000 | Loss: 0.00002650
Iteration 200/1000 | Loss: 0.00002650
Iteration 201/1000 | Loss: 0.00002650
Iteration 202/1000 | Loss: 0.00002650
Iteration 203/1000 | Loss: 0.00002650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.6499734303797595e-05, 2.6499734303797595e-05, 2.6499734303797595e-05, 2.6499734303797595e-05, 2.6499734303797595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6499734303797595e-05

Optimization complete. Final v2v error: 4.315877914428711 mm

Highest mean error: 5.31567907333374 mm for frame 158

Lowest mean error: 3.714479684829712 mm for frame 232

Saving results

Total time: 47.13274693489075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797232
Iteration 2/25 | Loss: 0.00145400
Iteration 3/25 | Loss: 0.00124957
Iteration 4/25 | Loss: 0.00123470
Iteration 5/25 | Loss: 0.00123016
Iteration 6/25 | Loss: 0.00122862
Iteration 7/25 | Loss: 0.00122859
Iteration 8/25 | Loss: 0.00122859
Iteration 9/25 | Loss: 0.00122859
Iteration 10/25 | Loss: 0.00122859
Iteration 11/25 | Loss: 0.00122859
Iteration 12/25 | Loss: 0.00122859
Iteration 13/25 | Loss: 0.00122859
Iteration 14/25 | Loss: 0.00122859
Iteration 15/25 | Loss: 0.00122859
Iteration 16/25 | Loss: 0.00122859
Iteration 17/25 | Loss: 0.00122859
Iteration 18/25 | Loss: 0.00122859
Iteration 19/25 | Loss: 0.00122859
Iteration 20/25 | Loss: 0.00122859
Iteration 21/25 | Loss: 0.00122859
Iteration 22/25 | Loss: 0.00122859
Iteration 23/25 | Loss: 0.00122859
Iteration 24/25 | Loss: 0.00122859
Iteration 25/25 | Loss: 0.00122859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09749162
Iteration 2/25 | Loss: 0.00219327
Iteration 3/25 | Loss: 0.00219327
Iteration 4/25 | Loss: 0.00219327
Iteration 5/25 | Loss: 0.00219327
Iteration 6/25 | Loss: 0.00219327
Iteration 7/25 | Loss: 0.00219327
Iteration 8/25 | Loss: 0.00219327
Iteration 9/25 | Loss: 0.00219327
Iteration 10/25 | Loss: 0.00219327
Iteration 11/25 | Loss: 0.00219327
Iteration 12/25 | Loss: 0.00219327
Iteration 13/25 | Loss: 0.00219327
Iteration 14/25 | Loss: 0.00219327
Iteration 15/25 | Loss: 0.00219327
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021932690870016813, 0.0021932690870016813, 0.0021932690870016813, 0.0021932690870016813, 0.0021932690870016813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021932690870016813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219327
Iteration 2/1000 | Loss: 0.00004238
Iteration 3/1000 | Loss: 0.00002671
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00001794
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001542
Iteration 8/1000 | Loss: 0.00001473
Iteration 9/1000 | Loss: 0.00001424
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001334
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001217
Iteration 28/1000 | Loss: 0.00001216
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001207
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001203
Iteration 45/1000 | Loss: 0.00001203
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001196
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001191
Iteration 110/1000 | Loss: 0.00001191
Iteration 111/1000 | Loss: 0.00001191
Iteration 112/1000 | Loss: 0.00001191
Iteration 113/1000 | Loss: 0.00001191
Iteration 114/1000 | Loss: 0.00001190
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001190
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001187
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001187
Iteration 131/1000 | Loss: 0.00001187
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001186
Iteration 141/1000 | Loss: 0.00001186
Iteration 142/1000 | Loss: 0.00001186
Iteration 143/1000 | Loss: 0.00001186
Iteration 144/1000 | Loss: 0.00001186
Iteration 145/1000 | Loss: 0.00001186
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001185
Iteration 149/1000 | Loss: 0.00001185
Iteration 150/1000 | Loss: 0.00001185
Iteration 151/1000 | Loss: 0.00001185
Iteration 152/1000 | Loss: 0.00001185
Iteration 153/1000 | Loss: 0.00001185
Iteration 154/1000 | Loss: 0.00001185
Iteration 155/1000 | Loss: 0.00001185
Iteration 156/1000 | Loss: 0.00001185
Iteration 157/1000 | Loss: 0.00001185
Iteration 158/1000 | Loss: 0.00001185
Iteration 159/1000 | Loss: 0.00001185
Iteration 160/1000 | Loss: 0.00001184
Iteration 161/1000 | Loss: 0.00001184
Iteration 162/1000 | Loss: 0.00001184
Iteration 163/1000 | Loss: 0.00001184
Iteration 164/1000 | Loss: 0.00001184
Iteration 165/1000 | Loss: 0.00001184
Iteration 166/1000 | Loss: 0.00001184
Iteration 167/1000 | Loss: 0.00001184
Iteration 168/1000 | Loss: 0.00001184
Iteration 169/1000 | Loss: 0.00001184
Iteration 170/1000 | Loss: 0.00001183
Iteration 171/1000 | Loss: 0.00001183
Iteration 172/1000 | Loss: 0.00001183
Iteration 173/1000 | Loss: 0.00001183
Iteration 174/1000 | Loss: 0.00001183
Iteration 175/1000 | Loss: 0.00001183
Iteration 176/1000 | Loss: 0.00001183
Iteration 177/1000 | Loss: 0.00001183
Iteration 178/1000 | Loss: 0.00001183
Iteration 179/1000 | Loss: 0.00001183
Iteration 180/1000 | Loss: 0.00001182
Iteration 181/1000 | Loss: 0.00001182
Iteration 182/1000 | Loss: 0.00001182
Iteration 183/1000 | Loss: 0.00001182
Iteration 184/1000 | Loss: 0.00001182
Iteration 185/1000 | Loss: 0.00001182
Iteration 186/1000 | Loss: 0.00001182
Iteration 187/1000 | Loss: 0.00001182
Iteration 188/1000 | Loss: 0.00001182
Iteration 189/1000 | Loss: 0.00001182
Iteration 190/1000 | Loss: 0.00001182
Iteration 191/1000 | Loss: 0.00001182
Iteration 192/1000 | Loss: 0.00001182
Iteration 193/1000 | Loss: 0.00001182
Iteration 194/1000 | Loss: 0.00001182
Iteration 195/1000 | Loss: 0.00001181
Iteration 196/1000 | Loss: 0.00001181
Iteration 197/1000 | Loss: 0.00001181
Iteration 198/1000 | Loss: 0.00001181
Iteration 199/1000 | Loss: 0.00001181
Iteration 200/1000 | Loss: 0.00001181
Iteration 201/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.1814712706836872e-05, 1.1814712706836872e-05, 1.1814712706836872e-05, 1.1814712706836872e-05, 1.1814712706836872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1814712706836872e-05

Optimization complete. Final v2v error: 2.8618171215057373 mm

Highest mean error: 4.090571880340576 mm for frame 62

Lowest mean error: 2.3782899379730225 mm for frame 91

Saving results

Total time: 46.17294979095459
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394428
Iteration 2/25 | Loss: 0.00131839
Iteration 3/25 | Loss: 0.00125618
Iteration 4/25 | Loss: 0.00124810
Iteration 5/25 | Loss: 0.00124776
Iteration 6/25 | Loss: 0.00124776
Iteration 7/25 | Loss: 0.00124776
Iteration 8/25 | Loss: 0.00124776
Iteration 9/25 | Loss: 0.00124776
Iteration 10/25 | Loss: 0.00124776
Iteration 11/25 | Loss: 0.00124776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001247755135409534, 0.001247755135409534, 0.001247755135409534, 0.001247755135409534, 0.001247755135409534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001247755135409534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24303520
Iteration 2/25 | Loss: 0.00205942
Iteration 3/25 | Loss: 0.00205942
Iteration 4/25 | Loss: 0.00205942
Iteration 5/25 | Loss: 0.00205942
Iteration 6/25 | Loss: 0.00205942
Iteration 7/25 | Loss: 0.00205942
Iteration 8/25 | Loss: 0.00205942
Iteration 9/25 | Loss: 0.00205942
Iteration 10/25 | Loss: 0.00205942
Iteration 11/25 | Loss: 0.00205942
Iteration 12/25 | Loss: 0.00205942
Iteration 13/25 | Loss: 0.00205942
Iteration 14/25 | Loss: 0.00205942
Iteration 15/25 | Loss: 0.00205942
Iteration 16/25 | Loss: 0.00205942
Iteration 17/25 | Loss: 0.00205942
Iteration 18/25 | Loss: 0.00205942
Iteration 19/25 | Loss: 0.00205942
Iteration 20/25 | Loss: 0.00205942
Iteration 21/25 | Loss: 0.00205942
Iteration 22/25 | Loss: 0.00205942
Iteration 23/25 | Loss: 0.00205942
Iteration 24/25 | Loss: 0.00205942
Iteration 25/25 | Loss: 0.00205942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205942
Iteration 2/1000 | Loss: 0.00001653
Iteration 3/1000 | Loss: 0.00001362
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001200
Iteration 6/1000 | Loss: 0.00001156
Iteration 7/1000 | Loss: 0.00001138
Iteration 8/1000 | Loss: 0.00001094
Iteration 9/1000 | Loss: 0.00001071
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001051
Iteration 12/1000 | Loss: 0.00001041
Iteration 13/1000 | Loss: 0.00001040
Iteration 14/1000 | Loss: 0.00001038
Iteration 15/1000 | Loss: 0.00001027
Iteration 16/1000 | Loss: 0.00001026
Iteration 17/1000 | Loss: 0.00001015
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00000999
Iteration 20/1000 | Loss: 0.00000982
Iteration 21/1000 | Loss: 0.00000977
Iteration 22/1000 | Loss: 0.00000977
Iteration 23/1000 | Loss: 0.00000969
Iteration 24/1000 | Loss: 0.00000969
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000969
Iteration 28/1000 | Loss: 0.00000969
Iteration 29/1000 | Loss: 0.00000969
Iteration 30/1000 | Loss: 0.00000969
Iteration 31/1000 | Loss: 0.00000968
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000968
Iteration 34/1000 | Loss: 0.00000968
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000963
Iteration 41/1000 | Loss: 0.00000963
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000961
Iteration 45/1000 | Loss: 0.00000961
Iteration 46/1000 | Loss: 0.00000956
Iteration 47/1000 | Loss: 0.00000956
Iteration 48/1000 | Loss: 0.00000956
Iteration 49/1000 | Loss: 0.00000956
Iteration 50/1000 | Loss: 0.00000955
Iteration 51/1000 | Loss: 0.00000955
Iteration 52/1000 | Loss: 0.00000955
Iteration 53/1000 | Loss: 0.00000954
Iteration 54/1000 | Loss: 0.00000953
Iteration 55/1000 | Loss: 0.00000953
Iteration 56/1000 | Loss: 0.00000952
Iteration 57/1000 | Loss: 0.00000951
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000951
Iteration 60/1000 | Loss: 0.00000951
Iteration 61/1000 | Loss: 0.00000951
Iteration 62/1000 | Loss: 0.00000951
Iteration 63/1000 | Loss: 0.00000951
Iteration 64/1000 | Loss: 0.00000951
Iteration 65/1000 | Loss: 0.00000950
Iteration 66/1000 | Loss: 0.00000950
Iteration 67/1000 | Loss: 0.00000950
Iteration 68/1000 | Loss: 0.00000948
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000947
Iteration 72/1000 | Loss: 0.00000947
Iteration 73/1000 | Loss: 0.00000947
Iteration 74/1000 | Loss: 0.00000947
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000945
Iteration 85/1000 | Loss: 0.00000945
Iteration 86/1000 | Loss: 0.00000944
Iteration 87/1000 | Loss: 0.00000944
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000944
Iteration 91/1000 | Loss: 0.00000944
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000943
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000941
Iteration 99/1000 | Loss: 0.00000940
Iteration 100/1000 | Loss: 0.00000940
Iteration 101/1000 | Loss: 0.00000940
Iteration 102/1000 | Loss: 0.00000940
Iteration 103/1000 | Loss: 0.00000939
Iteration 104/1000 | Loss: 0.00000939
Iteration 105/1000 | Loss: 0.00000939
Iteration 106/1000 | Loss: 0.00000939
Iteration 107/1000 | Loss: 0.00000939
Iteration 108/1000 | Loss: 0.00000939
Iteration 109/1000 | Loss: 0.00000939
Iteration 110/1000 | Loss: 0.00000939
Iteration 111/1000 | Loss: 0.00000939
Iteration 112/1000 | Loss: 0.00000938
Iteration 113/1000 | Loss: 0.00000938
Iteration 114/1000 | Loss: 0.00000938
Iteration 115/1000 | Loss: 0.00000938
Iteration 116/1000 | Loss: 0.00000938
Iteration 117/1000 | Loss: 0.00000938
Iteration 118/1000 | Loss: 0.00000937
Iteration 119/1000 | Loss: 0.00000937
Iteration 120/1000 | Loss: 0.00000937
Iteration 121/1000 | Loss: 0.00000937
Iteration 122/1000 | Loss: 0.00000937
Iteration 123/1000 | Loss: 0.00000937
Iteration 124/1000 | Loss: 0.00000937
Iteration 125/1000 | Loss: 0.00000937
Iteration 126/1000 | Loss: 0.00000937
Iteration 127/1000 | Loss: 0.00000937
Iteration 128/1000 | Loss: 0.00000937
Iteration 129/1000 | Loss: 0.00000937
Iteration 130/1000 | Loss: 0.00000936
Iteration 131/1000 | Loss: 0.00000936
Iteration 132/1000 | Loss: 0.00000936
Iteration 133/1000 | Loss: 0.00000936
Iteration 134/1000 | Loss: 0.00000936
Iteration 135/1000 | Loss: 0.00000936
Iteration 136/1000 | Loss: 0.00000936
Iteration 137/1000 | Loss: 0.00000936
Iteration 138/1000 | Loss: 0.00000936
Iteration 139/1000 | Loss: 0.00000936
Iteration 140/1000 | Loss: 0.00000935
Iteration 141/1000 | Loss: 0.00000935
Iteration 142/1000 | Loss: 0.00000935
Iteration 143/1000 | Loss: 0.00000935
Iteration 144/1000 | Loss: 0.00000935
Iteration 145/1000 | Loss: 0.00000935
Iteration 146/1000 | Loss: 0.00000935
Iteration 147/1000 | Loss: 0.00000935
Iteration 148/1000 | Loss: 0.00000935
Iteration 149/1000 | Loss: 0.00000935
Iteration 150/1000 | Loss: 0.00000935
Iteration 151/1000 | Loss: 0.00000935
Iteration 152/1000 | Loss: 0.00000935
Iteration 153/1000 | Loss: 0.00000934
Iteration 154/1000 | Loss: 0.00000934
Iteration 155/1000 | Loss: 0.00000934
Iteration 156/1000 | Loss: 0.00000934
Iteration 157/1000 | Loss: 0.00000934
Iteration 158/1000 | Loss: 0.00000934
Iteration 159/1000 | Loss: 0.00000934
Iteration 160/1000 | Loss: 0.00000934
Iteration 161/1000 | Loss: 0.00000934
Iteration 162/1000 | Loss: 0.00000934
Iteration 163/1000 | Loss: 0.00000934
Iteration 164/1000 | Loss: 0.00000934
Iteration 165/1000 | Loss: 0.00000934
Iteration 166/1000 | Loss: 0.00000934
Iteration 167/1000 | Loss: 0.00000934
Iteration 168/1000 | Loss: 0.00000934
Iteration 169/1000 | Loss: 0.00000934
Iteration 170/1000 | Loss: 0.00000934
Iteration 171/1000 | Loss: 0.00000934
Iteration 172/1000 | Loss: 0.00000934
Iteration 173/1000 | Loss: 0.00000934
Iteration 174/1000 | Loss: 0.00000934
Iteration 175/1000 | Loss: 0.00000934
Iteration 176/1000 | Loss: 0.00000934
Iteration 177/1000 | Loss: 0.00000934
Iteration 178/1000 | Loss: 0.00000934
Iteration 179/1000 | Loss: 0.00000934
Iteration 180/1000 | Loss: 0.00000934
Iteration 181/1000 | Loss: 0.00000934
Iteration 182/1000 | Loss: 0.00000934
Iteration 183/1000 | Loss: 0.00000934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [9.335156391898636e-06, 9.335156391898636e-06, 9.335156391898636e-06, 9.335156391898636e-06, 9.335156391898636e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.335156391898636e-06

Optimization complete. Final v2v error: 2.672614336013794 mm

Highest mean error: 2.881328821182251 mm for frame 128

Lowest mean error: 2.5757899284362793 mm for frame 228

Saving results

Total time: 44.588133811950684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396240
Iteration 2/25 | Loss: 0.00131897
Iteration 3/25 | Loss: 0.00124048
Iteration 4/25 | Loss: 0.00123653
Iteration 5/25 | Loss: 0.00123653
Iteration 6/25 | Loss: 0.00123653
Iteration 7/25 | Loss: 0.00123653
Iteration 8/25 | Loss: 0.00123653
Iteration 9/25 | Loss: 0.00123653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012365335132926702, 0.0012365335132926702, 0.0012365335132926702, 0.0012365335132926702, 0.0012365335132926702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012365335132926702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41033280
Iteration 2/25 | Loss: 0.00201654
Iteration 3/25 | Loss: 0.00201654
Iteration 4/25 | Loss: 0.00201654
Iteration 5/25 | Loss: 0.00201653
Iteration 6/25 | Loss: 0.00201653
Iteration 7/25 | Loss: 0.00201653
Iteration 8/25 | Loss: 0.00201653
Iteration 9/25 | Loss: 0.00201653
Iteration 10/25 | Loss: 0.00201653
Iteration 11/25 | Loss: 0.00201653
Iteration 12/25 | Loss: 0.00201653
Iteration 13/25 | Loss: 0.00201653
Iteration 14/25 | Loss: 0.00201653
Iteration 15/25 | Loss: 0.00201653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002016532700508833, 0.002016532700508833, 0.002016532700508833, 0.002016532700508833, 0.002016532700508833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002016532700508833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201653
Iteration 2/1000 | Loss: 0.00002642
Iteration 3/1000 | Loss: 0.00001690
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00001245
Iteration 6/1000 | Loss: 0.00001167
Iteration 7/1000 | Loss: 0.00001113
Iteration 8/1000 | Loss: 0.00001075
Iteration 9/1000 | Loss: 0.00001032
Iteration 10/1000 | Loss: 0.00000996
Iteration 11/1000 | Loss: 0.00000976
Iteration 12/1000 | Loss: 0.00000961
Iteration 13/1000 | Loss: 0.00000950
Iteration 14/1000 | Loss: 0.00000947
Iteration 15/1000 | Loss: 0.00000947
Iteration 16/1000 | Loss: 0.00000945
Iteration 17/1000 | Loss: 0.00000944
Iteration 18/1000 | Loss: 0.00000944
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000942
Iteration 21/1000 | Loss: 0.00000942
Iteration 22/1000 | Loss: 0.00000942
Iteration 23/1000 | Loss: 0.00000941
Iteration 24/1000 | Loss: 0.00000939
Iteration 25/1000 | Loss: 0.00000939
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000937
Iteration 29/1000 | Loss: 0.00000937
Iteration 30/1000 | Loss: 0.00000936
Iteration 31/1000 | Loss: 0.00000930
Iteration 32/1000 | Loss: 0.00000929
Iteration 33/1000 | Loss: 0.00000927
Iteration 34/1000 | Loss: 0.00000920
Iteration 35/1000 | Loss: 0.00000918
Iteration 36/1000 | Loss: 0.00000913
Iteration 37/1000 | Loss: 0.00000909
Iteration 38/1000 | Loss: 0.00000909
Iteration 39/1000 | Loss: 0.00000909
Iteration 40/1000 | Loss: 0.00000908
Iteration 41/1000 | Loss: 0.00000906
Iteration 42/1000 | Loss: 0.00000901
Iteration 43/1000 | Loss: 0.00000901
Iteration 44/1000 | Loss: 0.00000898
Iteration 45/1000 | Loss: 0.00000897
Iteration 46/1000 | Loss: 0.00000897
Iteration 47/1000 | Loss: 0.00000897
Iteration 48/1000 | Loss: 0.00000896
Iteration 49/1000 | Loss: 0.00000896
Iteration 50/1000 | Loss: 0.00000895
Iteration 51/1000 | Loss: 0.00000894
Iteration 52/1000 | Loss: 0.00000894
Iteration 53/1000 | Loss: 0.00000894
Iteration 54/1000 | Loss: 0.00000894
Iteration 55/1000 | Loss: 0.00000894
Iteration 56/1000 | Loss: 0.00000894
Iteration 57/1000 | Loss: 0.00000894
Iteration 58/1000 | Loss: 0.00000893
Iteration 59/1000 | Loss: 0.00000893
Iteration 60/1000 | Loss: 0.00000892
Iteration 61/1000 | Loss: 0.00000892
Iteration 62/1000 | Loss: 0.00000892
Iteration 63/1000 | Loss: 0.00000892
Iteration 64/1000 | Loss: 0.00000891
Iteration 65/1000 | Loss: 0.00000891
Iteration 66/1000 | Loss: 0.00000891
Iteration 67/1000 | Loss: 0.00000890
Iteration 68/1000 | Loss: 0.00000890
Iteration 69/1000 | Loss: 0.00000889
Iteration 70/1000 | Loss: 0.00000889
Iteration 71/1000 | Loss: 0.00000889
Iteration 72/1000 | Loss: 0.00000889
Iteration 73/1000 | Loss: 0.00000889
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000888
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000887
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000887
Iteration 82/1000 | Loss: 0.00000887
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000885
Iteration 89/1000 | Loss: 0.00000885
Iteration 90/1000 | Loss: 0.00000881
Iteration 91/1000 | Loss: 0.00000881
Iteration 92/1000 | Loss: 0.00000880
Iteration 93/1000 | Loss: 0.00000879
Iteration 94/1000 | Loss: 0.00000879
Iteration 95/1000 | Loss: 0.00000877
Iteration 96/1000 | Loss: 0.00000876
Iteration 97/1000 | Loss: 0.00000876
Iteration 98/1000 | Loss: 0.00000876
Iteration 99/1000 | Loss: 0.00000876
Iteration 100/1000 | Loss: 0.00000875
Iteration 101/1000 | Loss: 0.00000875
Iteration 102/1000 | Loss: 0.00000875
Iteration 103/1000 | Loss: 0.00000875
Iteration 104/1000 | Loss: 0.00000874
Iteration 105/1000 | Loss: 0.00000874
Iteration 106/1000 | Loss: 0.00000873
Iteration 107/1000 | Loss: 0.00000872
Iteration 108/1000 | Loss: 0.00000872
Iteration 109/1000 | Loss: 0.00000872
Iteration 110/1000 | Loss: 0.00000872
Iteration 111/1000 | Loss: 0.00000872
Iteration 112/1000 | Loss: 0.00000871
Iteration 113/1000 | Loss: 0.00000871
Iteration 114/1000 | Loss: 0.00000871
Iteration 115/1000 | Loss: 0.00000871
Iteration 116/1000 | Loss: 0.00000870
Iteration 117/1000 | Loss: 0.00000870
Iteration 118/1000 | Loss: 0.00000870
Iteration 119/1000 | Loss: 0.00000869
Iteration 120/1000 | Loss: 0.00000869
Iteration 121/1000 | Loss: 0.00000869
Iteration 122/1000 | Loss: 0.00000869
Iteration 123/1000 | Loss: 0.00000869
Iteration 124/1000 | Loss: 0.00000869
Iteration 125/1000 | Loss: 0.00000869
Iteration 126/1000 | Loss: 0.00000869
Iteration 127/1000 | Loss: 0.00000869
Iteration 128/1000 | Loss: 0.00000869
Iteration 129/1000 | Loss: 0.00000869
Iteration 130/1000 | Loss: 0.00000869
Iteration 131/1000 | Loss: 0.00000869
Iteration 132/1000 | Loss: 0.00000868
Iteration 133/1000 | Loss: 0.00000868
Iteration 134/1000 | Loss: 0.00000868
Iteration 135/1000 | Loss: 0.00000868
Iteration 136/1000 | Loss: 0.00000868
Iteration 137/1000 | Loss: 0.00000868
Iteration 138/1000 | Loss: 0.00000868
Iteration 139/1000 | Loss: 0.00000868
Iteration 140/1000 | Loss: 0.00000868
Iteration 141/1000 | Loss: 0.00000868
Iteration 142/1000 | Loss: 0.00000868
Iteration 143/1000 | Loss: 0.00000868
Iteration 144/1000 | Loss: 0.00000868
Iteration 145/1000 | Loss: 0.00000868
Iteration 146/1000 | Loss: 0.00000867
Iteration 147/1000 | Loss: 0.00000867
Iteration 148/1000 | Loss: 0.00000867
Iteration 149/1000 | Loss: 0.00000867
Iteration 150/1000 | Loss: 0.00000867
Iteration 151/1000 | Loss: 0.00000867
Iteration 152/1000 | Loss: 0.00000867
Iteration 153/1000 | Loss: 0.00000867
Iteration 154/1000 | Loss: 0.00000867
Iteration 155/1000 | Loss: 0.00000867
Iteration 156/1000 | Loss: 0.00000867
Iteration 157/1000 | Loss: 0.00000866
Iteration 158/1000 | Loss: 0.00000866
Iteration 159/1000 | Loss: 0.00000866
Iteration 160/1000 | Loss: 0.00000866
Iteration 161/1000 | Loss: 0.00000866
Iteration 162/1000 | Loss: 0.00000866
Iteration 163/1000 | Loss: 0.00000866
Iteration 164/1000 | Loss: 0.00000866
Iteration 165/1000 | Loss: 0.00000866
Iteration 166/1000 | Loss: 0.00000866
Iteration 167/1000 | Loss: 0.00000866
Iteration 168/1000 | Loss: 0.00000866
Iteration 169/1000 | Loss: 0.00000866
Iteration 170/1000 | Loss: 0.00000865
Iteration 171/1000 | Loss: 0.00000865
Iteration 172/1000 | Loss: 0.00000865
Iteration 173/1000 | Loss: 0.00000865
Iteration 174/1000 | Loss: 0.00000865
Iteration 175/1000 | Loss: 0.00000865
Iteration 176/1000 | Loss: 0.00000865
Iteration 177/1000 | Loss: 0.00000865
Iteration 178/1000 | Loss: 0.00000865
Iteration 179/1000 | Loss: 0.00000865
Iteration 180/1000 | Loss: 0.00000865
Iteration 181/1000 | Loss: 0.00000865
Iteration 182/1000 | Loss: 0.00000865
Iteration 183/1000 | Loss: 0.00000865
Iteration 184/1000 | Loss: 0.00000865
Iteration 185/1000 | Loss: 0.00000865
Iteration 186/1000 | Loss: 0.00000865
Iteration 187/1000 | Loss: 0.00000865
Iteration 188/1000 | Loss: 0.00000865
Iteration 189/1000 | Loss: 0.00000865
Iteration 190/1000 | Loss: 0.00000865
Iteration 191/1000 | Loss: 0.00000865
Iteration 192/1000 | Loss: 0.00000865
Iteration 193/1000 | Loss: 0.00000865
Iteration 194/1000 | Loss: 0.00000865
Iteration 195/1000 | Loss: 0.00000865
Iteration 196/1000 | Loss: 0.00000865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [8.650533345644362e-06, 8.650533345644362e-06, 8.650533345644362e-06, 8.650533345644362e-06, 8.650533345644362e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.650533345644362e-06

Optimization complete. Final v2v error: 2.5395021438598633 mm

Highest mean error: 2.7211689949035645 mm for frame 236

Lowest mean error: 2.4051899909973145 mm for frame 203

Saving results

Total time: 49.359784841537476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808968
Iteration 2/25 | Loss: 0.00151349
Iteration 3/25 | Loss: 0.00128993
Iteration 4/25 | Loss: 0.00127209
Iteration 5/25 | Loss: 0.00126980
Iteration 6/25 | Loss: 0.00126980
Iteration 7/25 | Loss: 0.00126980
Iteration 8/25 | Loss: 0.00126980
Iteration 9/25 | Loss: 0.00126980
Iteration 10/25 | Loss: 0.00126980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012698026839643717, 0.0012698026839643717, 0.0012698026839643717, 0.0012698026839643717, 0.0012698026839643717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012698026839643717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82312828
Iteration 2/25 | Loss: 0.00123162
Iteration 3/25 | Loss: 0.00123162
Iteration 4/25 | Loss: 0.00123161
Iteration 5/25 | Loss: 0.00123161
Iteration 6/25 | Loss: 0.00123161
Iteration 7/25 | Loss: 0.00123161
Iteration 8/25 | Loss: 0.00123161
Iteration 9/25 | Loss: 0.00123161
Iteration 10/25 | Loss: 0.00123161
Iteration 11/25 | Loss: 0.00123161
Iteration 12/25 | Loss: 0.00123161
Iteration 13/25 | Loss: 0.00123161
Iteration 14/25 | Loss: 0.00123161
Iteration 15/25 | Loss: 0.00123161
Iteration 16/25 | Loss: 0.00123161
Iteration 17/25 | Loss: 0.00123161
Iteration 18/25 | Loss: 0.00123161
Iteration 19/25 | Loss: 0.00123161
Iteration 20/25 | Loss: 0.00123161
Iteration 21/25 | Loss: 0.00123161
Iteration 22/25 | Loss: 0.00123161
Iteration 23/25 | Loss: 0.00123161
Iteration 24/25 | Loss: 0.00123161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012316134525462985, 0.0012316134525462985, 0.0012316134525462985, 0.0012316134525462985, 0.0012316134525462985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012316134525462985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123161
Iteration 2/1000 | Loss: 0.00003251
Iteration 3/1000 | Loss: 0.00002500
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002171
Iteration 6/1000 | Loss: 0.00002100
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00002020
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001964
Iteration 13/1000 | Loss: 0.00001954
Iteration 14/1000 | Loss: 0.00001948
Iteration 15/1000 | Loss: 0.00001934
Iteration 16/1000 | Loss: 0.00001923
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001921
Iteration 20/1000 | Loss: 0.00001921
Iteration 21/1000 | Loss: 0.00001920
Iteration 22/1000 | Loss: 0.00001912
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001905
Iteration 25/1000 | Loss: 0.00001905
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00001904
Iteration 28/1000 | Loss: 0.00001904
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001904
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001903
Iteration 34/1000 | Loss: 0.00001901
Iteration 35/1000 | Loss: 0.00001900
Iteration 36/1000 | Loss: 0.00001900
Iteration 37/1000 | Loss: 0.00001899
Iteration 38/1000 | Loss: 0.00001899
Iteration 39/1000 | Loss: 0.00001898
Iteration 40/1000 | Loss: 0.00001898
Iteration 41/1000 | Loss: 0.00001898
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001896
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001893
Iteration 52/1000 | Loss: 0.00001893
Iteration 53/1000 | Loss: 0.00001893
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001893
Iteration 56/1000 | Loss: 0.00001892
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001892
Iteration 59/1000 | Loss: 0.00001892
Iteration 60/1000 | Loss: 0.00001892
Iteration 61/1000 | Loss: 0.00001892
Iteration 62/1000 | Loss: 0.00001892
Iteration 63/1000 | Loss: 0.00001892
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001891
Iteration 66/1000 | Loss: 0.00001891
Iteration 67/1000 | Loss: 0.00001890
Iteration 68/1000 | Loss: 0.00001890
Iteration 69/1000 | Loss: 0.00001890
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001888
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001884
Iteration 87/1000 | Loss: 0.00001883
Iteration 88/1000 | Loss: 0.00001883
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001883
Iteration 91/1000 | Loss: 0.00001882
Iteration 92/1000 | Loss: 0.00001882
Iteration 93/1000 | Loss: 0.00001882
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001882
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001880
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001879
Iteration 109/1000 | Loss: 0.00001879
Iteration 110/1000 | Loss: 0.00001879
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001878
Iteration 113/1000 | Loss: 0.00001878
Iteration 114/1000 | Loss: 0.00001878
Iteration 115/1000 | Loss: 0.00001878
Iteration 116/1000 | Loss: 0.00001878
Iteration 117/1000 | Loss: 0.00001878
Iteration 118/1000 | Loss: 0.00001878
Iteration 119/1000 | Loss: 0.00001877
Iteration 120/1000 | Loss: 0.00001877
Iteration 121/1000 | Loss: 0.00001877
Iteration 122/1000 | Loss: 0.00001877
Iteration 123/1000 | Loss: 0.00001877
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00001876
Iteration 126/1000 | Loss: 0.00001876
Iteration 127/1000 | Loss: 0.00001876
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001876
Iteration 130/1000 | Loss: 0.00001876
Iteration 131/1000 | Loss: 0.00001876
Iteration 132/1000 | Loss: 0.00001876
Iteration 133/1000 | Loss: 0.00001876
Iteration 134/1000 | Loss: 0.00001876
Iteration 135/1000 | Loss: 0.00001876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.875764792202972e-05, 1.875764792202972e-05, 1.875764792202972e-05, 1.875764792202972e-05, 1.875764792202972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.875764792202972e-05

Optimization complete. Final v2v error: 3.65185546875 mm

Highest mean error: 3.8259031772613525 mm for frame 121

Lowest mean error: 3.5297865867614746 mm for frame 136

Saving results

Total time: 36.47075843811035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408144
Iteration 2/25 | Loss: 0.00142042
Iteration 3/25 | Loss: 0.00128839
Iteration 4/25 | Loss: 0.00126798
Iteration 5/25 | Loss: 0.00126286
Iteration 6/25 | Loss: 0.00126113
Iteration 7/25 | Loss: 0.00126062
Iteration 8/25 | Loss: 0.00126056
Iteration 9/25 | Loss: 0.00126056
Iteration 10/25 | Loss: 0.00126056
Iteration 11/25 | Loss: 0.00126056
Iteration 12/25 | Loss: 0.00126056
Iteration 13/25 | Loss: 0.00126056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012605567462742329, 0.0012605567462742329, 0.0012605567462742329, 0.0012605567462742329, 0.0012605567462742329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012605567462742329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20296717
Iteration 2/25 | Loss: 0.00298529
Iteration 3/25 | Loss: 0.00298529
Iteration 4/25 | Loss: 0.00298529
Iteration 5/25 | Loss: 0.00298529
Iteration 6/25 | Loss: 0.00298529
Iteration 7/25 | Loss: 0.00298529
Iteration 8/25 | Loss: 0.00298529
Iteration 9/25 | Loss: 0.00298529
Iteration 10/25 | Loss: 0.00298529
Iteration 11/25 | Loss: 0.00298529
Iteration 12/25 | Loss: 0.00298529
Iteration 13/25 | Loss: 0.00298529
Iteration 14/25 | Loss: 0.00298529
Iteration 15/25 | Loss: 0.00298529
Iteration 16/25 | Loss: 0.00298529
Iteration 17/25 | Loss: 0.00298529
Iteration 18/25 | Loss: 0.00298529
Iteration 19/25 | Loss: 0.00298529
Iteration 20/25 | Loss: 0.00298529
Iteration 21/25 | Loss: 0.00298529
Iteration 22/25 | Loss: 0.00298529
Iteration 23/25 | Loss: 0.00298529
Iteration 24/25 | Loss: 0.00298529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002985288854688406, 0.002985288854688406, 0.002985288854688406, 0.002985288854688406, 0.002985288854688406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002985288854688406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298529
Iteration 2/1000 | Loss: 0.00004301
Iteration 3/1000 | Loss: 0.00003063
Iteration 4/1000 | Loss: 0.00002488
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002125
Iteration 7/1000 | Loss: 0.00002036
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001894
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001839
Iteration 13/1000 | Loss: 0.00001816
Iteration 14/1000 | Loss: 0.00001795
Iteration 15/1000 | Loss: 0.00001780
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001761
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001755
Iteration 27/1000 | Loss: 0.00001754
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001742
Iteration 36/1000 | Loss: 0.00001740
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001739
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001737
Iteration 42/1000 | Loss: 0.00001737
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001731
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00002083
Iteration 54/1000 | Loss: 0.00001774
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001694
Iteration 60/1000 | Loss: 0.00001693
Iteration 61/1000 | Loss: 0.00001675
Iteration 62/1000 | Loss: 0.00001660
Iteration 63/1000 | Loss: 0.00001658
Iteration 64/1000 | Loss: 0.00001657
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001645
Iteration 68/1000 | Loss: 0.00001644
Iteration 69/1000 | Loss: 0.00001643
Iteration 70/1000 | Loss: 0.00001643
Iteration 71/1000 | Loss: 0.00001643
Iteration 72/1000 | Loss: 0.00001643
Iteration 73/1000 | Loss: 0.00001643
Iteration 74/1000 | Loss: 0.00001642
Iteration 75/1000 | Loss: 0.00001642
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001641
Iteration 78/1000 | Loss: 0.00001641
Iteration 79/1000 | Loss: 0.00001641
Iteration 80/1000 | Loss: 0.00001641
Iteration 81/1000 | Loss: 0.00001641
Iteration 82/1000 | Loss: 0.00001640
Iteration 83/1000 | Loss: 0.00001640
Iteration 84/1000 | Loss: 0.00001640
Iteration 85/1000 | Loss: 0.00001640
Iteration 86/1000 | Loss: 0.00001640
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001639
Iteration 89/1000 | Loss: 0.00001639
Iteration 90/1000 | Loss: 0.00001639
Iteration 91/1000 | Loss: 0.00001639
Iteration 92/1000 | Loss: 0.00001639
Iteration 93/1000 | Loss: 0.00001638
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001637
Iteration 97/1000 | Loss: 0.00001637
Iteration 98/1000 | Loss: 0.00001637
Iteration 99/1000 | Loss: 0.00001637
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001635
Iteration 107/1000 | Loss: 0.00001635
Iteration 108/1000 | Loss: 0.00001635
Iteration 109/1000 | Loss: 0.00001634
Iteration 110/1000 | Loss: 0.00001634
Iteration 111/1000 | Loss: 0.00001634
Iteration 112/1000 | Loss: 0.00001634
Iteration 113/1000 | Loss: 0.00001633
Iteration 114/1000 | Loss: 0.00001633
Iteration 115/1000 | Loss: 0.00001633
Iteration 116/1000 | Loss: 0.00001633
Iteration 117/1000 | Loss: 0.00001633
Iteration 118/1000 | Loss: 0.00001633
Iteration 119/1000 | Loss: 0.00001633
Iteration 120/1000 | Loss: 0.00001633
Iteration 121/1000 | Loss: 0.00001633
Iteration 122/1000 | Loss: 0.00001633
Iteration 123/1000 | Loss: 0.00001632
Iteration 124/1000 | Loss: 0.00001632
Iteration 125/1000 | Loss: 0.00001632
Iteration 126/1000 | Loss: 0.00001632
Iteration 127/1000 | Loss: 0.00001632
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001631
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001631
Iteration 132/1000 | Loss: 0.00001631
Iteration 133/1000 | Loss: 0.00001631
Iteration 134/1000 | Loss: 0.00001631
Iteration 135/1000 | Loss: 0.00001630
Iteration 136/1000 | Loss: 0.00001630
Iteration 137/1000 | Loss: 0.00001630
Iteration 138/1000 | Loss: 0.00001630
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001630
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001629
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001629
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001628
Iteration 154/1000 | Loss: 0.00001628
Iteration 155/1000 | Loss: 0.00001628
Iteration 156/1000 | Loss: 0.00001628
Iteration 157/1000 | Loss: 0.00001628
Iteration 158/1000 | Loss: 0.00001627
Iteration 159/1000 | Loss: 0.00001627
Iteration 160/1000 | Loss: 0.00001627
Iteration 161/1000 | Loss: 0.00001626
Iteration 162/1000 | Loss: 0.00001626
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00001625
Iteration 166/1000 | Loss: 0.00001625
Iteration 167/1000 | Loss: 0.00001625
Iteration 168/1000 | Loss: 0.00001625
Iteration 169/1000 | Loss: 0.00001625
Iteration 170/1000 | Loss: 0.00001624
Iteration 171/1000 | Loss: 0.00001624
Iteration 172/1000 | Loss: 0.00001624
Iteration 173/1000 | Loss: 0.00001624
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001624
Iteration 176/1000 | Loss: 0.00001624
Iteration 177/1000 | Loss: 0.00001624
Iteration 178/1000 | Loss: 0.00001624
Iteration 179/1000 | Loss: 0.00001624
Iteration 180/1000 | Loss: 0.00001623
Iteration 181/1000 | Loss: 0.00001623
Iteration 182/1000 | Loss: 0.00001623
Iteration 183/1000 | Loss: 0.00001623
Iteration 184/1000 | Loss: 0.00001623
Iteration 185/1000 | Loss: 0.00001623
Iteration 186/1000 | Loss: 0.00001623
Iteration 187/1000 | Loss: 0.00001623
Iteration 188/1000 | Loss: 0.00001623
Iteration 189/1000 | Loss: 0.00001623
Iteration 190/1000 | Loss: 0.00001623
Iteration 191/1000 | Loss: 0.00001623
Iteration 192/1000 | Loss: 0.00001623
Iteration 193/1000 | Loss: 0.00001623
Iteration 194/1000 | Loss: 0.00001622
Iteration 195/1000 | Loss: 0.00001622
Iteration 196/1000 | Loss: 0.00001622
Iteration 197/1000 | Loss: 0.00001622
Iteration 198/1000 | Loss: 0.00001622
Iteration 199/1000 | Loss: 0.00001622
Iteration 200/1000 | Loss: 0.00001622
Iteration 201/1000 | Loss: 0.00001622
Iteration 202/1000 | Loss: 0.00001622
Iteration 203/1000 | Loss: 0.00001622
Iteration 204/1000 | Loss: 0.00001622
Iteration 205/1000 | Loss: 0.00001622
Iteration 206/1000 | Loss: 0.00001622
Iteration 207/1000 | Loss: 0.00001622
Iteration 208/1000 | Loss: 0.00001622
Iteration 209/1000 | Loss: 0.00001622
Iteration 210/1000 | Loss: 0.00001622
Iteration 211/1000 | Loss: 0.00001622
Iteration 212/1000 | Loss: 0.00001622
Iteration 213/1000 | Loss: 0.00001622
Iteration 214/1000 | Loss: 0.00001621
Iteration 215/1000 | Loss: 0.00001621
Iteration 216/1000 | Loss: 0.00001621
Iteration 217/1000 | Loss: 0.00001621
Iteration 218/1000 | Loss: 0.00001621
Iteration 219/1000 | Loss: 0.00001621
Iteration 220/1000 | Loss: 0.00001621
Iteration 221/1000 | Loss: 0.00001621
Iteration 222/1000 | Loss: 0.00001621
Iteration 223/1000 | Loss: 0.00001621
Iteration 224/1000 | Loss: 0.00001621
Iteration 225/1000 | Loss: 0.00001621
Iteration 226/1000 | Loss: 0.00001621
Iteration 227/1000 | Loss: 0.00001620
Iteration 228/1000 | Loss: 0.00001620
Iteration 229/1000 | Loss: 0.00001620
Iteration 230/1000 | Loss: 0.00001620
Iteration 231/1000 | Loss: 0.00001620
Iteration 232/1000 | Loss: 0.00001620
Iteration 233/1000 | Loss: 0.00001620
Iteration 234/1000 | Loss: 0.00001620
Iteration 235/1000 | Loss: 0.00001620
Iteration 236/1000 | Loss: 0.00001620
Iteration 237/1000 | Loss: 0.00001620
Iteration 238/1000 | Loss: 0.00001620
Iteration 239/1000 | Loss: 0.00001620
Iteration 240/1000 | Loss: 0.00001620
Iteration 241/1000 | Loss: 0.00001620
Iteration 242/1000 | Loss: 0.00001620
Iteration 243/1000 | Loss: 0.00001619
Iteration 244/1000 | Loss: 0.00001619
Iteration 245/1000 | Loss: 0.00001619
Iteration 246/1000 | Loss: 0.00001619
Iteration 247/1000 | Loss: 0.00001619
Iteration 248/1000 | Loss: 0.00001619
Iteration 249/1000 | Loss: 0.00001619
Iteration 250/1000 | Loss: 0.00001619
Iteration 251/1000 | Loss: 0.00001619
Iteration 252/1000 | Loss: 0.00001619
Iteration 253/1000 | Loss: 0.00001619
Iteration 254/1000 | Loss: 0.00001619
Iteration 255/1000 | Loss: 0.00001619
Iteration 256/1000 | Loss: 0.00001618
Iteration 257/1000 | Loss: 0.00001618
Iteration 258/1000 | Loss: 0.00001618
Iteration 259/1000 | Loss: 0.00001618
Iteration 260/1000 | Loss: 0.00001618
Iteration 261/1000 | Loss: 0.00001618
Iteration 262/1000 | Loss: 0.00001618
Iteration 263/1000 | Loss: 0.00001618
Iteration 264/1000 | Loss: 0.00001618
Iteration 265/1000 | Loss: 0.00001618
Iteration 266/1000 | Loss: 0.00001618
Iteration 267/1000 | Loss: 0.00001618
Iteration 268/1000 | Loss: 0.00001618
Iteration 269/1000 | Loss: 0.00001618
Iteration 270/1000 | Loss: 0.00001618
Iteration 271/1000 | Loss: 0.00001618
Iteration 272/1000 | Loss: 0.00001618
Iteration 273/1000 | Loss: 0.00001618
Iteration 274/1000 | Loss: 0.00001618
Iteration 275/1000 | Loss: 0.00001618
Iteration 276/1000 | Loss: 0.00001618
Iteration 277/1000 | Loss: 0.00001618
Iteration 278/1000 | Loss: 0.00001618
Iteration 279/1000 | Loss: 0.00001618
Iteration 280/1000 | Loss: 0.00001617
Iteration 281/1000 | Loss: 0.00001617
Iteration 282/1000 | Loss: 0.00001617
Iteration 283/1000 | Loss: 0.00001617
Iteration 284/1000 | Loss: 0.00001617
Iteration 285/1000 | Loss: 0.00001617
Iteration 286/1000 | Loss: 0.00001617
Iteration 287/1000 | Loss: 0.00001617
Iteration 288/1000 | Loss: 0.00001617
Iteration 289/1000 | Loss: 0.00001617
Iteration 290/1000 | Loss: 0.00001617
Iteration 291/1000 | Loss: 0.00001617
Iteration 292/1000 | Loss: 0.00001617
Iteration 293/1000 | Loss: 0.00001617
Iteration 294/1000 | Loss: 0.00001617
Iteration 295/1000 | Loss: 0.00001617
Iteration 296/1000 | Loss: 0.00001617
Iteration 297/1000 | Loss: 0.00001617
Iteration 298/1000 | Loss: 0.00001617
Iteration 299/1000 | Loss: 0.00001617
Iteration 300/1000 | Loss: 0.00001617
Iteration 301/1000 | Loss: 0.00001617
Iteration 302/1000 | Loss: 0.00001617
Iteration 303/1000 | Loss: 0.00001617
Iteration 304/1000 | Loss: 0.00001617
Iteration 305/1000 | Loss: 0.00001617
Iteration 306/1000 | Loss: 0.00001617
Iteration 307/1000 | Loss: 0.00001617
Iteration 308/1000 | Loss: 0.00001617
Iteration 309/1000 | Loss: 0.00001617
Iteration 310/1000 | Loss: 0.00001617
Iteration 311/1000 | Loss: 0.00001617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.6166086425073445e-05, 1.6166086425073445e-05, 1.6166086425073445e-05, 1.6166086425073445e-05, 1.6166086425073445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6166086425073445e-05

Optimization complete. Final v2v error: 3.3433139324188232 mm

Highest mean error: 4.064039707183838 mm for frame 113

Lowest mean error: 2.726945400238037 mm for frame 2

Saving results

Total time: 66.0683662891388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001542
Iteration 2/25 | Loss: 0.00333949
Iteration 3/25 | Loss: 0.00210521
Iteration 4/25 | Loss: 0.00186138
Iteration 5/25 | Loss: 0.00209505
Iteration 6/25 | Loss: 0.00165708
Iteration 7/25 | Loss: 0.00144056
Iteration 8/25 | Loss: 0.00137878
Iteration 9/25 | Loss: 0.00135871
Iteration 10/25 | Loss: 0.00133227
Iteration 11/25 | Loss: 0.00128425
Iteration 12/25 | Loss: 0.00127576
Iteration 13/25 | Loss: 0.00127706
Iteration 14/25 | Loss: 0.00128322
Iteration 15/25 | Loss: 0.00127655
Iteration 16/25 | Loss: 0.00127559
Iteration 17/25 | Loss: 0.00127551
Iteration 18/25 | Loss: 0.00127264
Iteration 19/25 | Loss: 0.00127295
Iteration 20/25 | Loss: 0.00127128
Iteration 21/25 | Loss: 0.00127151
Iteration 22/25 | Loss: 0.00127091
Iteration 23/25 | Loss: 0.00127090
Iteration 24/25 | Loss: 0.00127064
Iteration 25/25 | Loss: 0.00127047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25379586
Iteration 2/25 | Loss: 0.00249240
Iteration 3/25 | Loss: 0.00207602
Iteration 4/25 | Loss: 0.00207602
Iteration 5/25 | Loss: 0.00207602
Iteration 6/25 | Loss: 0.00207602
Iteration 7/25 | Loss: 0.00207602
Iteration 8/25 | Loss: 0.00207602
Iteration 9/25 | Loss: 0.00207602
Iteration 10/25 | Loss: 0.00207602
Iteration 11/25 | Loss: 0.00207602
Iteration 12/25 | Loss: 0.00207602
Iteration 13/25 | Loss: 0.00207602
Iteration 14/25 | Loss: 0.00207602
Iteration 15/25 | Loss: 0.00207602
Iteration 16/25 | Loss: 0.00207602
Iteration 17/25 | Loss: 0.00207602
Iteration 18/25 | Loss: 0.00207602
Iteration 19/25 | Loss: 0.00207602
Iteration 20/25 | Loss: 0.00207602
Iteration 21/25 | Loss: 0.00207602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0020760190673172474, 0.0020760190673172474, 0.0020760190673172474, 0.0020760190673172474, 0.0020760190673172474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020760190673172474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207602
Iteration 2/1000 | Loss: 0.00037124
Iteration 3/1000 | Loss: 0.00102391
Iteration 4/1000 | Loss: 0.00013090
Iteration 5/1000 | Loss: 0.00010416
Iteration 6/1000 | Loss: 0.00007404
Iteration 7/1000 | Loss: 0.00003542
Iteration 8/1000 | Loss: 0.00004582
Iteration 9/1000 | Loss: 0.00069693
Iteration 10/1000 | Loss: 0.00005622
Iteration 11/1000 | Loss: 0.00003391
Iteration 12/1000 | Loss: 0.00002856
Iteration 13/1000 | Loss: 0.00002652
Iteration 14/1000 | Loss: 0.00002561
Iteration 15/1000 | Loss: 0.00002468
Iteration 16/1000 | Loss: 0.00002376
Iteration 17/1000 | Loss: 0.00002318
Iteration 18/1000 | Loss: 0.00205724
Iteration 19/1000 | Loss: 0.00003958
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001290
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001192
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001087
Iteration 41/1000 | Loss: 0.00001086
Iteration 42/1000 | Loss: 0.00001085
Iteration 43/1000 | Loss: 0.00001084
Iteration 44/1000 | Loss: 0.00001083
Iteration 45/1000 | Loss: 0.00001083
Iteration 46/1000 | Loss: 0.00001083
Iteration 47/1000 | Loss: 0.00001082
Iteration 48/1000 | Loss: 0.00001082
Iteration 49/1000 | Loss: 0.00001081
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001076
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001074
Iteration 66/1000 | Loss: 0.00001074
Iteration 67/1000 | Loss: 0.00001074
Iteration 68/1000 | Loss: 0.00001074
Iteration 69/1000 | Loss: 0.00001074
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001073
Iteration 72/1000 | Loss: 0.00001073
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001073
Iteration 76/1000 | Loss: 0.00001072
Iteration 77/1000 | Loss: 0.00001072
Iteration 78/1000 | Loss: 0.00001069
Iteration 79/1000 | Loss: 0.00001069
Iteration 80/1000 | Loss: 0.00001069
Iteration 81/1000 | Loss: 0.00001069
Iteration 82/1000 | Loss: 0.00001069
Iteration 83/1000 | Loss: 0.00001069
Iteration 84/1000 | Loss: 0.00001069
Iteration 85/1000 | Loss: 0.00001069
Iteration 86/1000 | Loss: 0.00001069
Iteration 87/1000 | Loss: 0.00001069
Iteration 88/1000 | Loss: 0.00001069
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001068
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001067
Iteration 93/1000 | Loss: 0.00001067
Iteration 94/1000 | Loss: 0.00001067
Iteration 95/1000 | Loss: 0.00001066
Iteration 96/1000 | Loss: 0.00001066
Iteration 97/1000 | Loss: 0.00001066
Iteration 98/1000 | Loss: 0.00001066
Iteration 99/1000 | Loss: 0.00001066
Iteration 100/1000 | Loss: 0.00001066
Iteration 101/1000 | Loss: 0.00001065
Iteration 102/1000 | Loss: 0.00001065
Iteration 103/1000 | Loss: 0.00001065
Iteration 104/1000 | Loss: 0.00001065
Iteration 105/1000 | Loss: 0.00001065
Iteration 106/1000 | Loss: 0.00001065
Iteration 107/1000 | Loss: 0.00001064
Iteration 108/1000 | Loss: 0.00001064
Iteration 109/1000 | Loss: 0.00001064
Iteration 110/1000 | Loss: 0.00001064
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001064
Iteration 113/1000 | Loss: 0.00001064
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001063
Iteration 116/1000 | Loss: 0.00001063
Iteration 117/1000 | Loss: 0.00001063
Iteration 118/1000 | Loss: 0.00001063
Iteration 119/1000 | Loss: 0.00001063
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001063
Iteration 126/1000 | Loss: 0.00001063
Iteration 127/1000 | Loss: 0.00001063
Iteration 128/1000 | Loss: 0.00001063
Iteration 129/1000 | Loss: 0.00001063
Iteration 130/1000 | Loss: 0.00001063
Iteration 131/1000 | Loss: 0.00001063
Iteration 132/1000 | Loss: 0.00001063
Iteration 133/1000 | Loss: 0.00001063
Iteration 134/1000 | Loss: 0.00001063
Iteration 135/1000 | Loss: 0.00001063
Iteration 136/1000 | Loss: 0.00001063
Iteration 137/1000 | Loss: 0.00001063
Iteration 138/1000 | Loss: 0.00001063
Iteration 139/1000 | Loss: 0.00001063
Iteration 140/1000 | Loss: 0.00001063
Iteration 141/1000 | Loss: 0.00001063
Iteration 142/1000 | Loss: 0.00001063
Iteration 143/1000 | Loss: 0.00001063
Iteration 144/1000 | Loss: 0.00001063
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.0633479178068228e-05, 1.0633479178068228e-05, 1.0633479178068228e-05, 1.0633479178068228e-05, 1.0633479178068228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0633479178068228e-05

Optimization complete. Final v2v error: 2.8106627464294434 mm

Highest mean error: 3.543584108352661 mm for frame 73

Lowest mean error: 2.5058164596557617 mm for frame 25

Saving results

Total time: 97.15715408325195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770412
Iteration 2/25 | Loss: 0.00154595
Iteration 3/25 | Loss: 0.00142520
Iteration 4/25 | Loss: 0.00140873
Iteration 5/25 | Loss: 0.00140458
Iteration 6/25 | Loss: 0.00140350
Iteration 7/25 | Loss: 0.00140350
Iteration 8/25 | Loss: 0.00140350
Iteration 9/25 | Loss: 0.00140350
Iteration 10/25 | Loss: 0.00140350
Iteration 11/25 | Loss: 0.00140350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014034999767318368, 0.0014034999767318368, 0.0014034999767318368, 0.0014034999767318368, 0.0014034999767318368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014034999767318368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24820125
Iteration 2/25 | Loss: 0.00185630
Iteration 3/25 | Loss: 0.00185629
Iteration 4/25 | Loss: 0.00185629
Iteration 5/25 | Loss: 0.00185629
Iteration 6/25 | Loss: 0.00185629
Iteration 7/25 | Loss: 0.00185629
Iteration 8/25 | Loss: 0.00185629
Iteration 9/25 | Loss: 0.00185629
Iteration 10/25 | Loss: 0.00185629
Iteration 11/25 | Loss: 0.00185629
Iteration 12/25 | Loss: 0.00185629
Iteration 13/25 | Loss: 0.00185629
Iteration 14/25 | Loss: 0.00185629
Iteration 15/25 | Loss: 0.00185629
Iteration 16/25 | Loss: 0.00185629
Iteration 17/25 | Loss: 0.00185629
Iteration 18/25 | Loss: 0.00185629
Iteration 19/25 | Loss: 0.00185629
Iteration 20/25 | Loss: 0.00185629
Iteration 21/25 | Loss: 0.00185629
Iteration 22/25 | Loss: 0.00185629
Iteration 23/25 | Loss: 0.00185629
Iteration 24/25 | Loss: 0.00185629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0018562900368124247, 0.0018562900368124247, 0.0018562900368124247, 0.0018562900368124247, 0.0018562900368124247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018562900368124247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185629
Iteration 2/1000 | Loss: 0.00005588
Iteration 3/1000 | Loss: 0.00003368
Iteration 4/1000 | Loss: 0.00002921
Iteration 5/1000 | Loss: 0.00002770
Iteration 6/1000 | Loss: 0.00002730
Iteration 7/1000 | Loss: 0.00002688
Iteration 8/1000 | Loss: 0.00002648
Iteration 9/1000 | Loss: 0.00002617
Iteration 10/1000 | Loss: 0.00002598
Iteration 11/1000 | Loss: 0.00002594
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00002561
Iteration 14/1000 | Loss: 0.00002560
Iteration 15/1000 | Loss: 0.00002560
Iteration 16/1000 | Loss: 0.00002559
Iteration 17/1000 | Loss: 0.00002559
Iteration 18/1000 | Loss: 0.00002554
Iteration 19/1000 | Loss: 0.00002553
Iteration 20/1000 | Loss: 0.00002548
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002547
Iteration 23/1000 | Loss: 0.00002546
Iteration 24/1000 | Loss: 0.00002541
Iteration 25/1000 | Loss: 0.00002541
Iteration 26/1000 | Loss: 0.00002539
Iteration 27/1000 | Loss: 0.00002539
Iteration 28/1000 | Loss: 0.00002539
Iteration 29/1000 | Loss: 0.00002539
Iteration 30/1000 | Loss: 0.00002539
Iteration 31/1000 | Loss: 0.00002539
Iteration 32/1000 | Loss: 0.00002538
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002535
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002532
Iteration 40/1000 | Loss: 0.00002531
Iteration 41/1000 | Loss: 0.00002531
Iteration 42/1000 | Loss: 0.00002531
Iteration 43/1000 | Loss: 0.00002529
Iteration 44/1000 | Loss: 0.00002524
Iteration 45/1000 | Loss: 0.00002524
Iteration 46/1000 | Loss: 0.00002524
Iteration 47/1000 | Loss: 0.00002524
Iteration 48/1000 | Loss: 0.00002524
Iteration 49/1000 | Loss: 0.00002524
Iteration 50/1000 | Loss: 0.00002524
Iteration 51/1000 | Loss: 0.00002524
Iteration 52/1000 | Loss: 0.00002524
Iteration 53/1000 | Loss: 0.00002522
Iteration 54/1000 | Loss: 0.00002522
Iteration 55/1000 | Loss: 0.00002522
Iteration 56/1000 | Loss: 0.00002522
Iteration 57/1000 | Loss: 0.00002522
Iteration 58/1000 | Loss: 0.00002521
Iteration 59/1000 | Loss: 0.00002521
Iteration 60/1000 | Loss: 0.00002521
Iteration 61/1000 | Loss: 0.00002521
Iteration 62/1000 | Loss: 0.00002520
Iteration 63/1000 | Loss: 0.00002520
Iteration 64/1000 | Loss: 0.00002520
Iteration 65/1000 | Loss: 0.00002520
Iteration 66/1000 | Loss: 0.00002520
Iteration 67/1000 | Loss: 0.00002520
Iteration 68/1000 | Loss: 0.00002520
Iteration 69/1000 | Loss: 0.00002520
Iteration 70/1000 | Loss: 0.00002520
Iteration 71/1000 | Loss: 0.00002520
Iteration 72/1000 | Loss: 0.00002520
Iteration 73/1000 | Loss: 0.00002519
Iteration 74/1000 | Loss: 0.00002518
Iteration 75/1000 | Loss: 0.00002517
Iteration 76/1000 | Loss: 0.00002517
Iteration 77/1000 | Loss: 0.00002516
Iteration 78/1000 | Loss: 0.00002515
Iteration 79/1000 | Loss: 0.00002514
Iteration 80/1000 | Loss: 0.00002514
Iteration 81/1000 | Loss: 0.00002514
Iteration 82/1000 | Loss: 0.00002514
Iteration 83/1000 | Loss: 0.00002514
Iteration 84/1000 | Loss: 0.00002513
Iteration 85/1000 | Loss: 0.00002512
Iteration 86/1000 | Loss: 0.00002511
Iteration 87/1000 | Loss: 0.00002511
Iteration 88/1000 | Loss: 0.00002511
Iteration 89/1000 | Loss: 0.00002511
Iteration 90/1000 | Loss: 0.00002511
Iteration 91/1000 | Loss: 0.00002511
Iteration 92/1000 | Loss: 0.00002511
Iteration 93/1000 | Loss: 0.00002511
Iteration 94/1000 | Loss: 0.00002511
Iteration 95/1000 | Loss: 0.00002511
Iteration 96/1000 | Loss: 0.00002510
Iteration 97/1000 | Loss: 0.00002510
Iteration 98/1000 | Loss: 0.00002510
Iteration 99/1000 | Loss: 0.00002508
Iteration 100/1000 | Loss: 0.00002508
Iteration 101/1000 | Loss: 0.00002508
Iteration 102/1000 | Loss: 0.00002507
Iteration 103/1000 | Loss: 0.00002506
Iteration 104/1000 | Loss: 0.00002506
Iteration 105/1000 | Loss: 0.00002504
Iteration 106/1000 | Loss: 0.00002504
Iteration 107/1000 | Loss: 0.00002504
Iteration 108/1000 | Loss: 0.00002504
Iteration 109/1000 | Loss: 0.00002503
Iteration 110/1000 | Loss: 0.00002503
Iteration 111/1000 | Loss: 0.00002502
Iteration 112/1000 | Loss: 0.00002502
Iteration 113/1000 | Loss: 0.00002502
Iteration 114/1000 | Loss: 0.00002501
Iteration 115/1000 | Loss: 0.00002501
Iteration 116/1000 | Loss: 0.00002501
Iteration 117/1000 | Loss: 0.00002500
Iteration 118/1000 | Loss: 0.00002500
Iteration 119/1000 | Loss: 0.00002500
Iteration 120/1000 | Loss: 0.00002499
Iteration 121/1000 | Loss: 0.00002499
Iteration 122/1000 | Loss: 0.00002499
Iteration 123/1000 | Loss: 0.00002499
Iteration 124/1000 | Loss: 0.00002498
Iteration 125/1000 | Loss: 0.00002498
Iteration 126/1000 | Loss: 0.00002498
Iteration 127/1000 | Loss: 0.00002498
Iteration 128/1000 | Loss: 0.00002498
Iteration 129/1000 | Loss: 0.00002498
Iteration 130/1000 | Loss: 0.00002498
Iteration 131/1000 | Loss: 0.00002498
Iteration 132/1000 | Loss: 0.00002498
Iteration 133/1000 | Loss: 0.00002498
Iteration 134/1000 | Loss: 0.00002498
Iteration 135/1000 | Loss: 0.00002498
Iteration 136/1000 | Loss: 0.00002498
Iteration 137/1000 | Loss: 0.00002497
Iteration 138/1000 | Loss: 0.00002497
Iteration 139/1000 | Loss: 0.00002497
Iteration 140/1000 | Loss: 0.00002497
Iteration 141/1000 | Loss: 0.00002497
Iteration 142/1000 | Loss: 0.00002497
Iteration 143/1000 | Loss: 0.00002497
Iteration 144/1000 | Loss: 0.00002496
Iteration 145/1000 | Loss: 0.00002496
Iteration 146/1000 | Loss: 0.00002496
Iteration 147/1000 | Loss: 0.00002496
Iteration 148/1000 | Loss: 0.00002496
Iteration 149/1000 | Loss: 0.00002495
Iteration 150/1000 | Loss: 0.00002495
Iteration 151/1000 | Loss: 0.00002495
Iteration 152/1000 | Loss: 0.00002495
Iteration 153/1000 | Loss: 0.00002495
Iteration 154/1000 | Loss: 0.00002495
Iteration 155/1000 | Loss: 0.00002495
Iteration 156/1000 | Loss: 0.00002495
Iteration 157/1000 | Loss: 0.00002494
Iteration 158/1000 | Loss: 0.00002494
Iteration 159/1000 | Loss: 0.00002494
Iteration 160/1000 | Loss: 0.00002494
Iteration 161/1000 | Loss: 0.00002494
Iteration 162/1000 | Loss: 0.00002494
Iteration 163/1000 | Loss: 0.00002494
Iteration 164/1000 | Loss: 0.00002493
Iteration 165/1000 | Loss: 0.00002493
Iteration 166/1000 | Loss: 0.00002493
Iteration 167/1000 | Loss: 0.00002493
Iteration 168/1000 | Loss: 0.00002493
Iteration 169/1000 | Loss: 0.00002493
Iteration 170/1000 | Loss: 0.00002493
Iteration 171/1000 | Loss: 0.00002493
Iteration 172/1000 | Loss: 0.00002492
Iteration 173/1000 | Loss: 0.00002492
Iteration 174/1000 | Loss: 0.00002492
Iteration 175/1000 | Loss: 0.00002492
Iteration 176/1000 | Loss: 0.00002492
Iteration 177/1000 | Loss: 0.00002492
Iteration 178/1000 | Loss: 0.00002492
Iteration 179/1000 | Loss: 0.00002492
Iteration 180/1000 | Loss: 0.00002492
Iteration 181/1000 | Loss: 0.00002492
Iteration 182/1000 | Loss: 0.00002492
Iteration 183/1000 | Loss: 0.00002492
Iteration 184/1000 | Loss: 0.00002492
Iteration 185/1000 | Loss: 0.00002492
Iteration 186/1000 | Loss: 0.00002492
Iteration 187/1000 | Loss: 0.00002492
Iteration 188/1000 | Loss: 0.00002492
Iteration 189/1000 | Loss: 0.00002492
Iteration 190/1000 | Loss: 0.00002492
Iteration 191/1000 | Loss: 0.00002492
Iteration 192/1000 | Loss: 0.00002492
Iteration 193/1000 | Loss: 0.00002492
Iteration 194/1000 | Loss: 0.00002492
Iteration 195/1000 | Loss: 0.00002492
Iteration 196/1000 | Loss: 0.00002492
Iteration 197/1000 | Loss: 0.00002492
Iteration 198/1000 | Loss: 0.00002492
Iteration 199/1000 | Loss: 0.00002492
Iteration 200/1000 | Loss: 0.00002492
Iteration 201/1000 | Loss: 0.00002492
Iteration 202/1000 | Loss: 0.00002492
Iteration 203/1000 | Loss: 0.00002492
Iteration 204/1000 | Loss: 0.00002492
Iteration 205/1000 | Loss: 0.00002492
Iteration 206/1000 | Loss: 0.00002492
Iteration 207/1000 | Loss: 0.00002492
Iteration 208/1000 | Loss: 0.00002492
Iteration 209/1000 | Loss: 0.00002492
Iteration 210/1000 | Loss: 0.00002492
Iteration 211/1000 | Loss: 0.00002492
Iteration 212/1000 | Loss: 0.00002492
Iteration 213/1000 | Loss: 0.00002492
Iteration 214/1000 | Loss: 0.00002492
Iteration 215/1000 | Loss: 0.00002492
Iteration 216/1000 | Loss: 0.00002492
Iteration 217/1000 | Loss: 0.00002492
Iteration 218/1000 | Loss: 0.00002492
Iteration 219/1000 | Loss: 0.00002492
Iteration 220/1000 | Loss: 0.00002492
Iteration 221/1000 | Loss: 0.00002492
Iteration 222/1000 | Loss: 0.00002492
Iteration 223/1000 | Loss: 0.00002492
Iteration 224/1000 | Loss: 0.00002492
Iteration 225/1000 | Loss: 0.00002492
Iteration 226/1000 | Loss: 0.00002492
Iteration 227/1000 | Loss: 0.00002492
Iteration 228/1000 | Loss: 0.00002492
Iteration 229/1000 | Loss: 0.00002492
Iteration 230/1000 | Loss: 0.00002492
Iteration 231/1000 | Loss: 0.00002492
Iteration 232/1000 | Loss: 0.00002492
Iteration 233/1000 | Loss: 0.00002492
Iteration 234/1000 | Loss: 0.00002492
Iteration 235/1000 | Loss: 0.00002492
Iteration 236/1000 | Loss: 0.00002492
Iteration 237/1000 | Loss: 0.00002492
Iteration 238/1000 | Loss: 0.00002492
Iteration 239/1000 | Loss: 0.00002492
Iteration 240/1000 | Loss: 0.00002492
Iteration 241/1000 | Loss: 0.00002492
Iteration 242/1000 | Loss: 0.00002492
Iteration 243/1000 | Loss: 0.00002492
Iteration 244/1000 | Loss: 0.00002492
Iteration 245/1000 | Loss: 0.00002492
Iteration 246/1000 | Loss: 0.00002492
Iteration 247/1000 | Loss: 0.00002492
Iteration 248/1000 | Loss: 0.00002492
Iteration 249/1000 | Loss: 0.00002492
Iteration 250/1000 | Loss: 0.00002492
Iteration 251/1000 | Loss: 0.00002492
Iteration 252/1000 | Loss: 0.00002492
Iteration 253/1000 | Loss: 0.00002492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [2.49161876126891e-05, 2.49161876126891e-05, 2.49161876126891e-05, 2.49161876126891e-05, 2.49161876126891e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.49161876126891e-05

Optimization complete. Final v2v error: 4.027373313903809 mm

Highest mean error: 4.411726474761963 mm for frame 67

Lowest mean error: 3.076474189758301 mm for frame 0

Saving results

Total time: 43.88796305656433
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426971
Iteration 2/25 | Loss: 0.00132021
Iteration 3/25 | Loss: 0.00126066
Iteration 4/25 | Loss: 0.00124816
Iteration 5/25 | Loss: 0.00124434
Iteration 6/25 | Loss: 0.00124375
Iteration 7/25 | Loss: 0.00124375
Iteration 8/25 | Loss: 0.00124375
Iteration 9/25 | Loss: 0.00124375
Iteration 10/25 | Loss: 0.00124375
Iteration 11/25 | Loss: 0.00124375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001243754755705595, 0.001243754755705595, 0.001243754755705595, 0.001243754755705595, 0.001243754755705595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243754755705595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27442694
Iteration 2/25 | Loss: 0.00211489
Iteration 3/25 | Loss: 0.00211488
Iteration 4/25 | Loss: 0.00211488
Iteration 5/25 | Loss: 0.00211488
Iteration 6/25 | Loss: 0.00211488
Iteration 7/25 | Loss: 0.00211488
Iteration 8/25 | Loss: 0.00211488
Iteration 9/25 | Loss: 0.00211488
Iteration 10/25 | Loss: 0.00211488
Iteration 11/25 | Loss: 0.00211488
Iteration 12/25 | Loss: 0.00211488
Iteration 13/25 | Loss: 0.00211488
Iteration 14/25 | Loss: 0.00211488
Iteration 15/25 | Loss: 0.00211488
Iteration 16/25 | Loss: 0.00211488
Iteration 17/25 | Loss: 0.00211488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021148803643882275, 0.0021148803643882275, 0.0021148803643882275, 0.0021148803643882275, 0.0021148803643882275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021148803643882275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211488
Iteration 2/1000 | Loss: 0.00002418
Iteration 3/1000 | Loss: 0.00001769
Iteration 4/1000 | Loss: 0.00001613
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001390
Iteration 8/1000 | Loss: 0.00001339
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001244
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001204
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001201
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001182
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001168
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001166
Iteration 62/1000 | Loss: 0.00001166
Iteration 63/1000 | Loss: 0.00001166
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001161
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001156
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001156
Iteration 131/1000 | Loss: 0.00001156
Iteration 132/1000 | Loss: 0.00001156
Iteration 133/1000 | Loss: 0.00001156
Iteration 134/1000 | Loss: 0.00001156
Iteration 135/1000 | Loss: 0.00001156
Iteration 136/1000 | Loss: 0.00001156
Iteration 137/1000 | Loss: 0.00001156
Iteration 138/1000 | Loss: 0.00001156
Iteration 139/1000 | Loss: 0.00001156
Iteration 140/1000 | Loss: 0.00001156
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001155
Iteration 148/1000 | Loss: 0.00001155
Iteration 149/1000 | Loss: 0.00001155
Iteration 150/1000 | Loss: 0.00001155
Iteration 151/1000 | Loss: 0.00001155
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001155
Iteration 156/1000 | Loss: 0.00001155
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001155
Iteration 162/1000 | Loss: 0.00001155
Iteration 163/1000 | Loss: 0.00001155
Iteration 164/1000 | Loss: 0.00001155
Iteration 165/1000 | Loss: 0.00001155
Iteration 166/1000 | Loss: 0.00001155
Iteration 167/1000 | Loss: 0.00001155
Iteration 168/1000 | Loss: 0.00001155
Iteration 169/1000 | Loss: 0.00001155
Iteration 170/1000 | Loss: 0.00001155
Iteration 171/1000 | Loss: 0.00001155
Iteration 172/1000 | Loss: 0.00001155
Iteration 173/1000 | Loss: 0.00001155
Iteration 174/1000 | Loss: 0.00001155
Iteration 175/1000 | Loss: 0.00001155
Iteration 176/1000 | Loss: 0.00001155
Iteration 177/1000 | Loss: 0.00001155
Iteration 178/1000 | Loss: 0.00001155
Iteration 179/1000 | Loss: 0.00001155
Iteration 180/1000 | Loss: 0.00001155
Iteration 181/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.1554547199921217e-05, 1.1554547199921217e-05, 1.1554547199921217e-05, 1.1554547199921217e-05, 1.1554547199921217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1554547199921217e-05

Optimization complete. Final v2v error: 2.9750430583953857 mm

Highest mean error: 3.132715940475464 mm for frame 90

Lowest mean error: 2.678115129470825 mm for frame 129

Saving results

Total time: 39.582725286483765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593852
Iteration 2/25 | Loss: 0.00129745
Iteration 3/25 | Loss: 0.00123377
Iteration 4/25 | Loss: 0.00122664
Iteration 5/25 | Loss: 0.00122433
Iteration 6/25 | Loss: 0.00122405
Iteration 7/25 | Loss: 0.00122405
Iteration 8/25 | Loss: 0.00122405
Iteration 9/25 | Loss: 0.00122405
Iteration 10/25 | Loss: 0.00122405
Iteration 11/25 | Loss: 0.00122405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012240515789017081, 0.0012240515789017081, 0.0012240515789017081, 0.0012240515789017081, 0.0012240515789017081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012240515789017081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97947454
Iteration 2/25 | Loss: 0.00218963
Iteration 3/25 | Loss: 0.00218963
Iteration 4/25 | Loss: 0.00218962
Iteration 5/25 | Loss: 0.00218962
Iteration 6/25 | Loss: 0.00218962
Iteration 7/25 | Loss: 0.00218962
Iteration 8/25 | Loss: 0.00218962
Iteration 9/25 | Loss: 0.00218962
Iteration 10/25 | Loss: 0.00218962
Iteration 11/25 | Loss: 0.00218962
Iteration 12/25 | Loss: 0.00218962
Iteration 13/25 | Loss: 0.00218962
Iteration 14/25 | Loss: 0.00218962
Iteration 15/25 | Loss: 0.00218962
Iteration 16/25 | Loss: 0.00218962
Iteration 17/25 | Loss: 0.00218962
Iteration 18/25 | Loss: 0.00218962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002189620863646269, 0.002189620863646269, 0.002189620863646269, 0.002189620863646269, 0.002189620863646269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002189620863646269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218962
Iteration 2/1000 | Loss: 0.00002260
Iteration 3/1000 | Loss: 0.00001649
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001283
Iteration 6/1000 | Loss: 0.00001215
Iteration 7/1000 | Loss: 0.00001156
Iteration 8/1000 | Loss: 0.00001156
Iteration 9/1000 | Loss: 0.00001110
Iteration 10/1000 | Loss: 0.00001090
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001052
Iteration 13/1000 | Loss: 0.00001034
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001023
Iteration 16/1000 | Loss: 0.00001020
Iteration 17/1000 | Loss: 0.00001015
Iteration 18/1000 | Loss: 0.00001013
Iteration 19/1000 | Loss: 0.00001012
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001009
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001002
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000993
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000989
Iteration 41/1000 | Loss: 0.00000989
Iteration 42/1000 | Loss: 0.00000986
Iteration 43/1000 | Loss: 0.00000986
Iteration 44/1000 | Loss: 0.00000985
Iteration 45/1000 | Loss: 0.00000985
Iteration 46/1000 | Loss: 0.00000984
Iteration 47/1000 | Loss: 0.00000984
Iteration 48/1000 | Loss: 0.00000980
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000974
Iteration 52/1000 | Loss: 0.00000974
Iteration 53/1000 | Loss: 0.00000973
Iteration 54/1000 | Loss: 0.00000973
Iteration 55/1000 | Loss: 0.00000972
Iteration 56/1000 | Loss: 0.00000972
Iteration 57/1000 | Loss: 0.00000972
Iteration 58/1000 | Loss: 0.00000972
Iteration 59/1000 | Loss: 0.00000972
Iteration 60/1000 | Loss: 0.00000972
Iteration 61/1000 | Loss: 0.00000972
Iteration 62/1000 | Loss: 0.00000971
Iteration 63/1000 | Loss: 0.00000971
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000970
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000967
Iteration 73/1000 | Loss: 0.00000967
Iteration 74/1000 | Loss: 0.00000967
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000966
Iteration 77/1000 | Loss: 0.00000966
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000964
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000964
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000963
Iteration 94/1000 | Loss: 0.00000963
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000963
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000962
Iteration 101/1000 | Loss: 0.00000962
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000962
Iteration 109/1000 | Loss: 0.00000962
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000960
Iteration 113/1000 | Loss: 0.00000960
Iteration 114/1000 | Loss: 0.00000960
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000959
Iteration 118/1000 | Loss: 0.00000959
Iteration 119/1000 | Loss: 0.00000959
Iteration 120/1000 | Loss: 0.00000959
Iteration 121/1000 | Loss: 0.00000959
Iteration 122/1000 | Loss: 0.00000958
Iteration 123/1000 | Loss: 0.00000958
Iteration 124/1000 | Loss: 0.00000958
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000958
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000958
Iteration 129/1000 | Loss: 0.00000958
Iteration 130/1000 | Loss: 0.00000958
Iteration 131/1000 | Loss: 0.00000958
Iteration 132/1000 | Loss: 0.00000958
Iteration 133/1000 | Loss: 0.00000958
Iteration 134/1000 | Loss: 0.00000958
Iteration 135/1000 | Loss: 0.00000958
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000957
Iteration 138/1000 | Loss: 0.00000957
Iteration 139/1000 | Loss: 0.00000957
Iteration 140/1000 | Loss: 0.00000957
Iteration 141/1000 | Loss: 0.00000957
Iteration 142/1000 | Loss: 0.00000956
Iteration 143/1000 | Loss: 0.00000956
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000956
Iteration 147/1000 | Loss: 0.00000956
Iteration 148/1000 | Loss: 0.00000956
Iteration 149/1000 | Loss: 0.00000955
Iteration 150/1000 | Loss: 0.00000955
Iteration 151/1000 | Loss: 0.00000955
Iteration 152/1000 | Loss: 0.00000955
Iteration 153/1000 | Loss: 0.00000955
Iteration 154/1000 | Loss: 0.00000955
Iteration 155/1000 | Loss: 0.00000955
Iteration 156/1000 | Loss: 0.00000954
Iteration 157/1000 | Loss: 0.00000954
Iteration 158/1000 | Loss: 0.00000954
Iteration 159/1000 | Loss: 0.00000954
Iteration 160/1000 | Loss: 0.00000954
Iteration 161/1000 | Loss: 0.00000954
Iteration 162/1000 | Loss: 0.00000954
Iteration 163/1000 | Loss: 0.00000954
Iteration 164/1000 | Loss: 0.00000954
Iteration 165/1000 | Loss: 0.00000954
Iteration 166/1000 | Loss: 0.00000954
Iteration 167/1000 | Loss: 0.00000954
Iteration 168/1000 | Loss: 0.00000954
Iteration 169/1000 | Loss: 0.00000954
Iteration 170/1000 | Loss: 0.00000953
Iteration 171/1000 | Loss: 0.00000953
Iteration 172/1000 | Loss: 0.00000953
Iteration 173/1000 | Loss: 0.00000953
Iteration 174/1000 | Loss: 0.00000953
Iteration 175/1000 | Loss: 0.00000953
Iteration 176/1000 | Loss: 0.00000953
Iteration 177/1000 | Loss: 0.00000953
Iteration 178/1000 | Loss: 0.00000953
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000953
Iteration 181/1000 | Loss: 0.00000953
Iteration 182/1000 | Loss: 0.00000953
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000953
Iteration 185/1000 | Loss: 0.00000953
Iteration 186/1000 | Loss: 0.00000953
Iteration 187/1000 | Loss: 0.00000953
Iteration 188/1000 | Loss: 0.00000953
Iteration 189/1000 | Loss: 0.00000953
Iteration 190/1000 | Loss: 0.00000953
Iteration 191/1000 | Loss: 0.00000953
Iteration 192/1000 | Loss: 0.00000953
Iteration 193/1000 | Loss: 0.00000953
Iteration 194/1000 | Loss: 0.00000953
Iteration 195/1000 | Loss: 0.00000953
Iteration 196/1000 | Loss: 0.00000953
Iteration 197/1000 | Loss: 0.00000953
Iteration 198/1000 | Loss: 0.00000953
Iteration 199/1000 | Loss: 0.00000953
Iteration 200/1000 | Loss: 0.00000953
Iteration 201/1000 | Loss: 0.00000953
Iteration 202/1000 | Loss: 0.00000953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [9.525811947241891e-06, 9.525811947241891e-06, 9.525811947241891e-06, 9.525811947241891e-06, 9.525811947241891e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.525811947241891e-06

Optimization complete. Final v2v error: 2.7030351161956787 mm

Highest mean error: 2.925366163253784 mm for frame 61

Lowest mean error: 2.544431686401367 mm for frame 118

Saving results

Total time: 40.3885543346405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789455
Iteration 2/25 | Loss: 0.00143852
Iteration 3/25 | Loss: 0.00124835
Iteration 4/25 | Loss: 0.00123531
Iteration 5/25 | Loss: 0.00123114
Iteration 6/25 | Loss: 0.00122999
Iteration 7/25 | Loss: 0.00122999
Iteration 8/25 | Loss: 0.00122999
Iteration 9/25 | Loss: 0.00122999
Iteration 10/25 | Loss: 0.00122999
Iteration 11/25 | Loss: 0.00122999
Iteration 12/25 | Loss: 0.00122999
Iteration 13/25 | Loss: 0.00122999
Iteration 14/25 | Loss: 0.00122999
Iteration 15/25 | Loss: 0.00122999
Iteration 16/25 | Loss: 0.00122999
Iteration 17/25 | Loss: 0.00122999
Iteration 18/25 | Loss: 0.00122999
Iteration 19/25 | Loss: 0.00122999
Iteration 20/25 | Loss: 0.00122999
Iteration 21/25 | Loss: 0.00122999
Iteration 22/25 | Loss: 0.00122999
Iteration 23/25 | Loss: 0.00122999
Iteration 24/25 | Loss: 0.00122999
Iteration 25/25 | Loss: 0.00122999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08181012
Iteration 2/25 | Loss: 0.00216151
Iteration 3/25 | Loss: 0.00216151
Iteration 4/25 | Loss: 0.00216151
Iteration 5/25 | Loss: 0.00216150
Iteration 6/25 | Loss: 0.00216150
Iteration 7/25 | Loss: 0.00216150
Iteration 8/25 | Loss: 0.00216150
Iteration 9/25 | Loss: 0.00216150
Iteration 10/25 | Loss: 0.00216150
Iteration 11/25 | Loss: 0.00216150
Iteration 12/25 | Loss: 0.00216150
Iteration 13/25 | Loss: 0.00216150
Iteration 14/25 | Loss: 0.00216150
Iteration 15/25 | Loss: 0.00216150
Iteration 16/25 | Loss: 0.00216150
Iteration 17/25 | Loss: 0.00216150
Iteration 18/25 | Loss: 0.00216150
Iteration 19/25 | Loss: 0.00216150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021615035366266966, 0.0021615035366266966, 0.0021615035366266966, 0.0021615035366266966, 0.0021615035366266966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021615035366266966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216150
Iteration 2/1000 | Loss: 0.00004402
Iteration 3/1000 | Loss: 0.00002766
Iteration 4/1000 | Loss: 0.00002249
Iteration 5/1000 | Loss: 0.00001888
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001586
Iteration 8/1000 | Loss: 0.00001520
Iteration 9/1000 | Loss: 0.00001463
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001327
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001317
Iteration 16/1000 | Loss: 0.00001304
Iteration 17/1000 | Loss: 0.00001292
Iteration 18/1000 | Loss: 0.00001285
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001269
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001268
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001260
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001257
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001253
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001252
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001250
Iteration 58/1000 | Loss: 0.00001250
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001249
Iteration 62/1000 | Loss: 0.00001249
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001247
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001246
Iteration 81/1000 | Loss: 0.00001246
Iteration 82/1000 | Loss: 0.00001246
Iteration 83/1000 | Loss: 0.00001246
Iteration 84/1000 | Loss: 0.00001246
Iteration 85/1000 | Loss: 0.00001246
Iteration 86/1000 | Loss: 0.00001246
Iteration 87/1000 | Loss: 0.00001246
Iteration 88/1000 | Loss: 0.00001245
Iteration 89/1000 | Loss: 0.00001245
Iteration 90/1000 | Loss: 0.00001245
Iteration 91/1000 | Loss: 0.00001245
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001243
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001242
Iteration 100/1000 | Loss: 0.00001242
Iteration 101/1000 | Loss: 0.00001242
Iteration 102/1000 | Loss: 0.00001242
Iteration 103/1000 | Loss: 0.00001242
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001241
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001240
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001240
Iteration 111/1000 | Loss: 0.00001239
Iteration 112/1000 | Loss: 0.00001239
Iteration 113/1000 | Loss: 0.00001239
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001238
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001238
Iteration 124/1000 | Loss: 0.00001238
Iteration 125/1000 | Loss: 0.00001238
Iteration 126/1000 | Loss: 0.00001238
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001236
Iteration 135/1000 | Loss: 0.00001236
Iteration 136/1000 | Loss: 0.00001236
Iteration 137/1000 | Loss: 0.00001236
Iteration 138/1000 | Loss: 0.00001236
Iteration 139/1000 | Loss: 0.00001236
Iteration 140/1000 | Loss: 0.00001236
Iteration 141/1000 | Loss: 0.00001235
Iteration 142/1000 | Loss: 0.00001235
Iteration 143/1000 | Loss: 0.00001235
Iteration 144/1000 | Loss: 0.00001235
Iteration 145/1000 | Loss: 0.00001234
Iteration 146/1000 | Loss: 0.00001234
Iteration 147/1000 | Loss: 0.00001234
Iteration 148/1000 | Loss: 0.00001234
Iteration 149/1000 | Loss: 0.00001234
Iteration 150/1000 | Loss: 0.00001233
Iteration 151/1000 | Loss: 0.00001233
Iteration 152/1000 | Loss: 0.00001233
Iteration 153/1000 | Loss: 0.00001233
Iteration 154/1000 | Loss: 0.00001233
Iteration 155/1000 | Loss: 0.00001233
Iteration 156/1000 | Loss: 0.00001233
Iteration 157/1000 | Loss: 0.00001233
Iteration 158/1000 | Loss: 0.00001233
Iteration 159/1000 | Loss: 0.00001232
Iteration 160/1000 | Loss: 0.00001232
Iteration 161/1000 | Loss: 0.00001232
Iteration 162/1000 | Loss: 0.00001232
Iteration 163/1000 | Loss: 0.00001232
Iteration 164/1000 | Loss: 0.00001232
Iteration 165/1000 | Loss: 0.00001232
Iteration 166/1000 | Loss: 0.00001232
Iteration 167/1000 | Loss: 0.00001232
Iteration 168/1000 | Loss: 0.00001232
Iteration 169/1000 | Loss: 0.00001232
Iteration 170/1000 | Loss: 0.00001232
Iteration 171/1000 | Loss: 0.00001232
Iteration 172/1000 | Loss: 0.00001232
Iteration 173/1000 | Loss: 0.00001232
Iteration 174/1000 | Loss: 0.00001232
Iteration 175/1000 | Loss: 0.00001232
Iteration 176/1000 | Loss: 0.00001232
Iteration 177/1000 | Loss: 0.00001232
Iteration 178/1000 | Loss: 0.00001232
Iteration 179/1000 | Loss: 0.00001232
Iteration 180/1000 | Loss: 0.00001232
Iteration 181/1000 | Loss: 0.00001232
Iteration 182/1000 | Loss: 0.00001232
Iteration 183/1000 | Loss: 0.00001232
Iteration 184/1000 | Loss: 0.00001232
Iteration 185/1000 | Loss: 0.00001232
Iteration 186/1000 | Loss: 0.00001232
Iteration 187/1000 | Loss: 0.00001232
Iteration 188/1000 | Loss: 0.00001232
Iteration 189/1000 | Loss: 0.00001232
Iteration 190/1000 | Loss: 0.00001232
Iteration 191/1000 | Loss: 0.00001232
Iteration 192/1000 | Loss: 0.00001232
Iteration 193/1000 | Loss: 0.00001232
Iteration 194/1000 | Loss: 0.00001232
Iteration 195/1000 | Loss: 0.00001232
Iteration 196/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.232235172210494e-05, 1.232235172210494e-05, 1.232235172210494e-05, 1.232235172210494e-05, 1.232235172210494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.232235172210494e-05

Optimization complete. Final v2v error: 2.9245944023132324 mm

Highest mean error: 4.082525730133057 mm for frame 64

Lowest mean error: 2.4197123050689697 mm for frame 100

Saving results

Total time: 44.82062005996704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414793
Iteration 2/25 | Loss: 0.00131665
Iteration 3/25 | Loss: 0.00125740
Iteration 4/25 | Loss: 0.00124543
Iteration 5/25 | Loss: 0.00124192
Iteration 6/25 | Loss: 0.00124110
Iteration 7/25 | Loss: 0.00124101
Iteration 8/25 | Loss: 0.00124101
Iteration 9/25 | Loss: 0.00124101
Iteration 10/25 | Loss: 0.00124101
Iteration 11/25 | Loss: 0.00124101
Iteration 12/25 | Loss: 0.00124101
Iteration 13/25 | Loss: 0.00124101
Iteration 14/25 | Loss: 0.00124101
Iteration 15/25 | Loss: 0.00124101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001241014921106398, 0.001241014921106398, 0.001241014921106398, 0.001241014921106398, 0.001241014921106398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001241014921106398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30175221
Iteration 2/25 | Loss: 0.00211353
Iteration 3/25 | Loss: 0.00211353
Iteration 4/25 | Loss: 0.00211353
Iteration 5/25 | Loss: 0.00211353
Iteration 6/25 | Loss: 0.00211352
Iteration 7/25 | Loss: 0.00211352
Iteration 8/25 | Loss: 0.00211352
Iteration 9/25 | Loss: 0.00211352
Iteration 10/25 | Loss: 0.00211352
Iteration 11/25 | Loss: 0.00211352
Iteration 12/25 | Loss: 0.00211352
Iteration 13/25 | Loss: 0.00211352
Iteration 14/25 | Loss: 0.00211352
Iteration 15/25 | Loss: 0.00211352
Iteration 16/25 | Loss: 0.00211352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021135234273970127, 0.0021135234273970127, 0.0021135234273970127, 0.0021135234273970127, 0.0021135234273970127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021135234273970127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211352
Iteration 2/1000 | Loss: 0.00002377
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001532
Iteration 5/1000 | Loss: 0.00001430
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001189
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001176
Iteration 17/1000 | Loss: 0.00001175
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001156
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001155
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001144
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001141
Iteration 35/1000 | Loss: 0.00001140
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001136
Iteration 42/1000 | Loss: 0.00001136
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001131
Iteration 54/1000 | Loss: 0.00001130
Iteration 55/1000 | Loss: 0.00001130
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001130
Iteration 58/1000 | Loss: 0.00001129
Iteration 59/1000 | Loss: 0.00001129
Iteration 60/1000 | Loss: 0.00001129
Iteration 61/1000 | Loss: 0.00001129
Iteration 62/1000 | Loss: 0.00001129
Iteration 63/1000 | Loss: 0.00001129
Iteration 64/1000 | Loss: 0.00001128
Iteration 65/1000 | Loss: 0.00001128
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001127
Iteration 68/1000 | Loss: 0.00001127
Iteration 69/1000 | Loss: 0.00001126
Iteration 70/1000 | Loss: 0.00001126
Iteration 71/1000 | Loss: 0.00001126
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001125
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001124
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001123
Iteration 84/1000 | Loss: 0.00001123
Iteration 85/1000 | Loss: 0.00001123
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001123
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001123
Iteration 90/1000 | Loss: 0.00001123
Iteration 91/1000 | Loss: 0.00001122
Iteration 92/1000 | Loss: 0.00001122
Iteration 93/1000 | Loss: 0.00001122
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001122
Iteration 96/1000 | Loss: 0.00001121
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001121
Iteration 99/1000 | Loss: 0.00001121
Iteration 100/1000 | Loss: 0.00001121
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001121
Iteration 103/1000 | Loss: 0.00001121
Iteration 104/1000 | Loss: 0.00001121
Iteration 105/1000 | Loss: 0.00001121
Iteration 106/1000 | Loss: 0.00001121
Iteration 107/1000 | Loss: 0.00001121
Iteration 108/1000 | Loss: 0.00001121
Iteration 109/1000 | Loss: 0.00001121
Iteration 110/1000 | Loss: 0.00001121
Iteration 111/1000 | Loss: 0.00001120
Iteration 112/1000 | Loss: 0.00001120
Iteration 113/1000 | Loss: 0.00001120
Iteration 114/1000 | Loss: 0.00001120
Iteration 115/1000 | Loss: 0.00001120
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001120
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001119
Iteration 124/1000 | Loss: 0.00001119
Iteration 125/1000 | Loss: 0.00001119
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001119
Iteration 130/1000 | Loss: 0.00001119
Iteration 131/1000 | Loss: 0.00001119
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001119
Iteration 134/1000 | Loss: 0.00001119
Iteration 135/1000 | Loss: 0.00001119
Iteration 136/1000 | Loss: 0.00001118
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001118
Iteration 139/1000 | Loss: 0.00001118
Iteration 140/1000 | Loss: 0.00001118
Iteration 141/1000 | Loss: 0.00001118
Iteration 142/1000 | Loss: 0.00001118
Iteration 143/1000 | Loss: 0.00001118
Iteration 144/1000 | Loss: 0.00001118
Iteration 145/1000 | Loss: 0.00001118
Iteration 146/1000 | Loss: 0.00001118
Iteration 147/1000 | Loss: 0.00001117
Iteration 148/1000 | Loss: 0.00001117
Iteration 149/1000 | Loss: 0.00001117
Iteration 150/1000 | Loss: 0.00001117
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001116
Iteration 158/1000 | Loss: 0.00001116
Iteration 159/1000 | Loss: 0.00001116
Iteration 160/1000 | Loss: 0.00001116
Iteration 161/1000 | Loss: 0.00001116
Iteration 162/1000 | Loss: 0.00001116
Iteration 163/1000 | Loss: 0.00001116
Iteration 164/1000 | Loss: 0.00001116
Iteration 165/1000 | Loss: 0.00001116
Iteration 166/1000 | Loss: 0.00001116
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001116
Iteration 172/1000 | Loss: 0.00001116
Iteration 173/1000 | Loss: 0.00001115
Iteration 174/1000 | Loss: 0.00001115
Iteration 175/1000 | Loss: 0.00001115
Iteration 176/1000 | Loss: 0.00001115
Iteration 177/1000 | Loss: 0.00001115
Iteration 178/1000 | Loss: 0.00001115
Iteration 179/1000 | Loss: 0.00001115
Iteration 180/1000 | Loss: 0.00001115
Iteration 181/1000 | Loss: 0.00001115
Iteration 182/1000 | Loss: 0.00001115
Iteration 183/1000 | Loss: 0.00001115
Iteration 184/1000 | Loss: 0.00001114
Iteration 185/1000 | Loss: 0.00001114
Iteration 186/1000 | Loss: 0.00001114
Iteration 187/1000 | Loss: 0.00001114
Iteration 188/1000 | Loss: 0.00001114
Iteration 189/1000 | Loss: 0.00001114
Iteration 190/1000 | Loss: 0.00001114
Iteration 191/1000 | Loss: 0.00001114
Iteration 192/1000 | Loss: 0.00001114
Iteration 193/1000 | Loss: 0.00001114
Iteration 194/1000 | Loss: 0.00001114
Iteration 195/1000 | Loss: 0.00001114
Iteration 196/1000 | Loss: 0.00001114
Iteration 197/1000 | Loss: 0.00001114
Iteration 198/1000 | Loss: 0.00001114
Iteration 199/1000 | Loss: 0.00001114
Iteration 200/1000 | Loss: 0.00001114
Iteration 201/1000 | Loss: 0.00001114
Iteration 202/1000 | Loss: 0.00001114
Iteration 203/1000 | Loss: 0.00001114
Iteration 204/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.1140007700305432e-05, 1.1140007700305432e-05, 1.1140007700305432e-05, 1.1140007700305432e-05, 1.1140007700305432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1140007700305432e-05

Optimization complete. Final v2v error: 2.917304039001465 mm

Highest mean error: 3.124046564102173 mm for frame 116

Lowest mean error: 2.796889543533325 mm for frame 133

Saving results

Total time: 41.8953537940979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030953
Iteration 2/25 | Loss: 0.00206618
Iteration 3/25 | Loss: 0.00214477
Iteration 4/25 | Loss: 0.00151248
Iteration 5/25 | Loss: 0.00153239
Iteration 6/25 | Loss: 0.00141973
Iteration 7/25 | Loss: 0.00133681
Iteration 8/25 | Loss: 0.00128872
Iteration 9/25 | Loss: 0.00126924
Iteration 10/25 | Loss: 0.00125980
Iteration 11/25 | Loss: 0.00125879
Iteration 12/25 | Loss: 0.00125651
Iteration 13/25 | Loss: 0.00125670
Iteration 14/25 | Loss: 0.00125307
Iteration 15/25 | Loss: 0.00125167
Iteration 16/25 | Loss: 0.00124877
Iteration 17/25 | Loss: 0.00125145
Iteration 18/25 | Loss: 0.00124402
Iteration 19/25 | Loss: 0.00124753
Iteration 20/25 | Loss: 0.00123574
Iteration 21/25 | Loss: 0.00123324
Iteration 22/25 | Loss: 0.00124018
Iteration 23/25 | Loss: 0.00123857
Iteration 24/25 | Loss: 0.00123779
Iteration 25/25 | Loss: 0.00123720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27360475
Iteration 2/25 | Loss: 0.00268434
Iteration 3/25 | Loss: 0.00250687
Iteration 4/25 | Loss: 0.00250687
Iteration 5/25 | Loss: 0.00250686
Iteration 6/25 | Loss: 0.00250686
Iteration 7/25 | Loss: 0.00250686
Iteration 8/25 | Loss: 0.00250686
Iteration 9/25 | Loss: 0.00250686
Iteration 10/25 | Loss: 0.00250686
Iteration 11/25 | Loss: 0.00250686
Iteration 12/25 | Loss: 0.00250686
Iteration 13/25 | Loss: 0.00250686
Iteration 14/25 | Loss: 0.00250686
Iteration 15/25 | Loss: 0.00250686
Iteration 16/25 | Loss: 0.00250686
Iteration 17/25 | Loss: 0.00250686
Iteration 18/25 | Loss: 0.00250686
Iteration 19/25 | Loss: 0.00250686
Iteration 20/25 | Loss: 0.00250686
Iteration 21/25 | Loss: 0.00250686
Iteration 22/25 | Loss: 0.00250686
Iteration 23/25 | Loss: 0.00250686
Iteration 24/25 | Loss: 0.00250686
Iteration 25/25 | Loss: 0.00250686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250686
Iteration 2/1000 | Loss: 0.00010152
Iteration 3/1000 | Loss: 0.00046983
Iteration 4/1000 | Loss: 0.00025342
Iteration 5/1000 | Loss: 0.00005527
Iteration 6/1000 | Loss: 0.00036378
Iteration 7/1000 | Loss: 0.00023587
Iteration 8/1000 | Loss: 0.00015990
Iteration 9/1000 | Loss: 0.00025595
Iteration 10/1000 | Loss: 0.00020762
Iteration 11/1000 | Loss: 0.00006899
Iteration 12/1000 | Loss: 0.00040732
Iteration 13/1000 | Loss: 0.00033408
Iteration 14/1000 | Loss: 0.00037941
Iteration 15/1000 | Loss: 0.00026179
Iteration 16/1000 | Loss: 0.00007917
Iteration 17/1000 | Loss: 0.00044433
Iteration 18/1000 | Loss: 0.00028127
Iteration 19/1000 | Loss: 0.00040901
Iteration 20/1000 | Loss: 0.00022758
Iteration 21/1000 | Loss: 0.00031353
Iteration 22/1000 | Loss: 0.00022845
Iteration 23/1000 | Loss: 0.00041773
Iteration 24/1000 | Loss: 0.00031849
Iteration 25/1000 | Loss: 0.00036468
Iteration 26/1000 | Loss: 0.00352195
Iteration 27/1000 | Loss: 0.00038039
Iteration 28/1000 | Loss: 0.00033226
Iteration 29/1000 | Loss: 0.00017225
Iteration 30/1000 | Loss: 0.00031640
Iteration 31/1000 | Loss: 0.00034038
Iteration 32/1000 | Loss: 0.00008208
Iteration 33/1000 | Loss: 0.00003638
Iteration 34/1000 | Loss: 0.00004194
Iteration 35/1000 | Loss: 0.00005751
Iteration 36/1000 | Loss: 0.00005377
Iteration 37/1000 | Loss: 0.00006402
Iteration 38/1000 | Loss: 0.00005785
Iteration 39/1000 | Loss: 0.00005157
Iteration 40/1000 | Loss: 0.00005949
Iteration 41/1000 | Loss: 0.00005215
Iteration 42/1000 | Loss: 0.00005508
Iteration 43/1000 | Loss: 0.00031343
Iteration 44/1000 | Loss: 0.00025132
Iteration 45/1000 | Loss: 0.00005600
Iteration 46/1000 | Loss: 0.00003004
Iteration 47/1000 | Loss: 0.00003200
Iteration 48/1000 | Loss: 0.00002179
Iteration 49/1000 | Loss: 0.00008610
Iteration 50/1000 | Loss: 0.00025255
Iteration 51/1000 | Loss: 0.00002200
Iteration 52/1000 | Loss: 0.00003245
Iteration 53/1000 | Loss: 0.00014937
Iteration 54/1000 | Loss: 0.00012763
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00010296
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00003999
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00006948
Iteration 66/1000 | Loss: 0.00003381
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00002386
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00002198
Iteration 71/1000 | Loss: 0.00003087
Iteration 72/1000 | Loss: 0.00010492
Iteration 73/1000 | Loss: 0.00002722
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00006030
Iteration 76/1000 | Loss: 0.00001140
Iteration 77/1000 | Loss: 0.00001812
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001111
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00015313
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00000987
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000985
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001063
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000978
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000993
Iteration 96/1000 | Loss: 0.00000993
Iteration 97/1000 | Loss: 0.00000975
Iteration 98/1000 | Loss: 0.00000975
Iteration 99/1000 | Loss: 0.00000975
Iteration 100/1000 | Loss: 0.00000975
Iteration 101/1000 | Loss: 0.00000975
Iteration 102/1000 | Loss: 0.00000974
Iteration 103/1000 | Loss: 0.00000974
Iteration 104/1000 | Loss: 0.00000974
Iteration 105/1000 | Loss: 0.00000974
Iteration 106/1000 | Loss: 0.00000974
Iteration 107/1000 | Loss: 0.00000974
Iteration 108/1000 | Loss: 0.00000974
Iteration 109/1000 | Loss: 0.00000974
Iteration 110/1000 | Loss: 0.00000974
Iteration 111/1000 | Loss: 0.00000974
Iteration 112/1000 | Loss: 0.00000974
Iteration 113/1000 | Loss: 0.00000974
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000973
Iteration 117/1000 | Loss: 0.00000973
Iteration 118/1000 | Loss: 0.00000973
Iteration 119/1000 | Loss: 0.00000973
Iteration 120/1000 | Loss: 0.00000972
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001461
Iteration 123/1000 | Loss: 0.00000972
Iteration 124/1000 | Loss: 0.00000970
Iteration 125/1000 | Loss: 0.00000969
Iteration 126/1000 | Loss: 0.00000969
Iteration 127/1000 | Loss: 0.00000969
Iteration 128/1000 | Loss: 0.00000969
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000968
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000967
Iteration 137/1000 | Loss: 0.00000967
Iteration 138/1000 | Loss: 0.00000967
Iteration 139/1000 | Loss: 0.00000967
Iteration 140/1000 | Loss: 0.00000967
Iteration 141/1000 | Loss: 0.00000967
Iteration 142/1000 | Loss: 0.00000967
Iteration 143/1000 | Loss: 0.00000967
Iteration 144/1000 | Loss: 0.00000967
Iteration 145/1000 | Loss: 0.00000966
Iteration 146/1000 | Loss: 0.00000966
Iteration 147/1000 | Loss: 0.00001322
Iteration 148/1000 | Loss: 0.00000964
Iteration 149/1000 | Loss: 0.00000964
Iteration 150/1000 | Loss: 0.00000964
Iteration 151/1000 | Loss: 0.00000964
Iteration 152/1000 | Loss: 0.00000964
Iteration 153/1000 | Loss: 0.00000964
Iteration 154/1000 | Loss: 0.00000964
Iteration 155/1000 | Loss: 0.00000964
Iteration 156/1000 | Loss: 0.00000964
Iteration 157/1000 | Loss: 0.00000963
Iteration 158/1000 | Loss: 0.00000963
Iteration 159/1000 | Loss: 0.00000963
Iteration 160/1000 | Loss: 0.00000963
Iteration 161/1000 | Loss: 0.00000963
Iteration 162/1000 | Loss: 0.00000963
Iteration 163/1000 | Loss: 0.00000962
Iteration 164/1000 | Loss: 0.00000962
Iteration 165/1000 | Loss: 0.00000962
Iteration 166/1000 | Loss: 0.00000961
Iteration 167/1000 | Loss: 0.00001360
Iteration 168/1000 | Loss: 0.00000959
Iteration 169/1000 | Loss: 0.00000959
Iteration 170/1000 | Loss: 0.00000959
Iteration 171/1000 | Loss: 0.00000959
Iteration 172/1000 | Loss: 0.00000959
Iteration 173/1000 | Loss: 0.00000959
Iteration 174/1000 | Loss: 0.00000959
Iteration 175/1000 | Loss: 0.00000959
Iteration 176/1000 | Loss: 0.00000959
Iteration 177/1000 | Loss: 0.00000959
Iteration 178/1000 | Loss: 0.00000958
Iteration 179/1000 | Loss: 0.00000958
Iteration 180/1000 | Loss: 0.00000958
Iteration 181/1000 | Loss: 0.00000958
Iteration 182/1000 | Loss: 0.00000958
Iteration 183/1000 | Loss: 0.00000958
Iteration 184/1000 | Loss: 0.00000957
Iteration 185/1000 | Loss: 0.00000957
Iteration 186/1000 | Loss: 0.00000957
Iteration 187/1000 | Loss: 0.00000957
Iteration 188/1000 | Loss: 0.00000957
Iteration 189/1000 | Loss: 0.00000957
Iteration 190/1000 | Loss: 0.00002011
Iteration 191/1000 | Loss: 0.00004842
Iteration 192/1000 | Loss: 0.00000967
Iteration 193/1000 | Loss: 0.00001193
Iteration 194/1000 | Loss: 0.00000954
Iteration 195/1000 | Loss: 0.00000954
Iteration 196/1000 | Loss: 0.00000954
Iteration 197/1000 | Loss: 0.00000954
Iteration 198/1000 | Loss: 0.00000954
Iteration 199/1000 | Loss: 0.00000954
Iteration 200/1000 | Loss: 0.00000954
Iteration 201/1000 | Loss: 0.00000954
Iteration 202/1000 | Loss: 0.00000954
Iteration 203/1000 | Loss: 0.00000953
Iteration 204/1000 | Loss: 0.00000953
Iteration 205/1000 | Loss: 0.00000953
Iteration 206/1000 | Loss: 0.00001022
Iteration 207/1000 | Loss: 0.00001022
Iteration 208/1000 | Loss: 0.00000959
Iteration 209/1000 | Loss: 0.00001317
Iteration 210/1000 | Loss: 0.00001798
Iteration 211/1000 | Loss: 0.00001120
Iteration 212/1000 | Loss: 0.00001178
Iteration 213/1000 | Loss: 0.00000953
Iteration 214/1000 | Loss: 0.00000953
Iteration 215/1000 | Loss: 0.00000952
Iteration 216/1000 | Loss: 0.00000952
Iteration 217/1000 | Loss: 0.00000952
Iteration 218/1000 | Loss: 0.00000952
Iteration 219/1000 | Loss: 0.00000952
Iteration 220/1000 | Loss: 0.00000952
Iteration 221/1000 | Loss: 0.00000952
Iteration 222/1000 | Loss: 0.00000952
Iteration 223/1000 | Loss: 0.00000952
Iteration 224/1000 | Loss: 0.00000952
Iteration 225/1000 | Loss: 0.00000952
Iteration 226/1000 | Loss: 0.00000952
Iteration 227/1000 | Loss: 0.00000952
Iteration 228/1000 | Loss: 0.00000952
Iteration 229/1000 | Loss: 0.00000952
Iteration 230/1000 | Loss: 0.00000952
Iteration 231/1000 | Loss: 0.00000952
Iteration 232/1000 | Loss: 0.00000952
Iteration 233/1000 | Loss: 0.00000952
Iteration 234/1000 | Loss: 0.00000952
Iteration 235/1000 | Loss: 0.00000952
Iteration 236/1000 | Loss: 0.00000952
Iteration 237/1000 | Loss: 0.00000952
Iteration 238/1000 | Loss: 0.00000952
Iteration 239/1000 | Loss: 0.00000952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [9.524786037218291e-06, 9.524786037218291e-06, 9.524786037218291e-06, 9.524786037218291e-06, 9.524786037218291e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.524786037218291e-06

Optimization complete. Final v2v error: 2.621427536010742 mm

Highest mean error: 4.006217002868652 mm for frame 72

Lowest mean error: 2.2551088333129883 mm for frame 38

Saving results

Total time: 190.10253167152405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405423
Iteration 2/25 | Loss: 0.00133304
Iteration 3/25 | Loss: 0.00125499
Iteration 4/25 | Loss: 0.00124155
Iteration 5/25 | Loss: 0.00123733
Iteration 6/25 | Loss: 0.00123663
Iteration 7/25 | Loss: 0.00123663
Iteration 8/25 | Loss: 0.00123663
Iteration 9/25 | Loss: 0.00123663
Iteration 10/25 | Loss: 0.00123663
Iteration 11/25 | Loss: 0.00123663
Iteration 12/25 | Loss: 0.00123663
Iteration 13/25 | Loss: 0.00123663
Iteration 14/25 | Loss: 0.00123663
Iteration 15/25 | Loss: 0.00123663
Iteration 16/25 | Loss: 0.00123663
Iteration 17/25 | Loss: 0.00123663
Iteration 18/25 | Loss: 0.00123663
Iteration 19/25 | Loss: 0.00123663
Iteration 20/25 | Loss: 0.00123663
Iteration 21/25 | Loss: 0.00123663
Iteration 22/25 | Loss: 0.00123663
Iteration 23/25 | Loss: 0.00123663
Iteration 24/25 | Loss: 0.00123663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012366264127194881, 0.0012366264127194881, 0.0012366264127194881, 0.0012366264127194881, 0.0012366264127194881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012366264127194881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26393247
Iteration 2/25 | Loss: 0.00230304
Iteration 3/25 | Loss: 0.00230302
Iteration 4/25 | Loss: 0.00230302
Iteration 5/25 | Loss: 0.00230302
Iteration 6/25 | Loss: 0.00230302
Iteration 7/25 | Loss: 0.00230302
Iteration 8/25 | Loss: 0.00230302
Iteration 9/25 | Loss: 0.00230302
Iteration 10/25 | Loss: 0.00230302
Iteration 11/25 | Loss: 0.00230302
Iteration 12/25 | Loss: 0.00230302
Iteration 13/25 | Loss: 0.00230302
Iteration 14/25 | Loss: 0.00230302
Iteration 15/25 | Loss: 0.00230302
Iteration 16/25 | Loss: 0.00230302
Iteration 17/25 | Loss: 0.00230302
Iteration 18/25 | Loss: 0.00230302
Iteration 19/25 | Loss: 0.00230302
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002303019631654024, 0.002303019631654024, 0.002303019631654024, 0.002303019631654024, 0.002303019631654024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002303019631654024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230302
Iteration 2/1000 | Loss: 0.00004413
Iteration 3/1000 | Loss: 0.00002835
Iteration 4/1000 | Loss: 0.00002238
Iteration 5/1000 | Loss: 0.00001880
Iteration 6/1000 | Loss: 0.00001731
Iteration 7/1000 | Loss: 0.00001599
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001415
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001361
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001287
Iteration 39/1000 | Loss: 0.00001287
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001284
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001282
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001281
Iteration 56/1000 | Loss: 0.00001281
Iteration 57/1000 | Loss: 0.00001281
Iteration 58/1000 | Loss: 0.00001281
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001280
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001280
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001279
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001275
Iteration 91/1000 | Loss: 0.00001275
Iteration 92/1000 | Loss: 0.00001274
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001272
Iteration 99/1000 | Loss: 0.00001272
Iteration 100/1000 | Loss: 0.00001272
Iteration 101/1000 | Loss: 0.00001272
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001270
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001270
Iteration 108/1000 | Loss: 0.00001269
Iteration 109/1000 | Loss: 0.00001269
Iteration 110/1000 | Loss: 0.00001268
Iteration 111/1000 | Loss: 0.00001268
Iteration 112/1000 | Loss: 0.00001268
Iteration 113/1000 | Loss: 0.00001268
Iteration 114/1000 | Loss: 0.00001267
Iteration 115/1000 | Loss: 0.00001267
Iteration 116/1000 | Loss: 0.00001267
Iteration 117/1000 | Loss: 0.00001267
Iteration 118/1000 | Loss: 0.00001267
Iteration 119/1000 | Loss: 0.00001266
Iteration 120/1000 | Loss: 0.00001266
Iteration 121/1000 | Loss: 0.00001266
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001266
Iteration 125/1000 | Loss: 0.00001266
Iteration 126/1000 | Loss: 0.00001266
Iteration 127/1000 | Loss: 0.00001266
Iteration 128/1000 | Loss: 0.00001266
Iteration 129/1000 | Loss: 0.00001266
Iteration 130/1000 | Loss: 0.00001266
Iteration 131/1000 | Loss: 0.00001266
Iteration 132/1000 | Loss: 0.00001266
Iteration 133/1000 | Loss: 0.00001266
Iteration 134/1000 | Loss: 0.00001266
Iteration 135/1000 | Loss: 0.00001266
Iteration 136/1000 | Loss: 0.00001266
Iteration 137/1000 | Loss: 0.00001266
Iteration 138/1000 | Loss: 0.00001266
Iteration 139/1000 | Loss: 0.00001266
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001266
Iteration 143/1000 | Loss: 0.00001266
Iteration 144/1000 | Loss: 0.00001266
Iteration 145/1000 | Loss: 0.00001266
Iteration 146/1000 | Loss: 0.00001266
Iteration 147/1000 | Loss: 0.00001266
Iteration 148/1000 | Loss: 0.00001266
Iteration 149/1000 | Loss: 0.00001266
Iteration 150/1000 | Loss: 0.00001266
Iteration 151/1000 | Loss: 0.00001266
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001266
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Iteration 157/1000 | Loss: 0.00001266
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001266
Iteration 163/1000 | Loss: 0.00001266
Iteration 164/1000 | Loss: 0.00001266
Iteration 165/1000 | Loss: 0.00001266
Iteration 166/1000 | Loss: 0.00001266
Iteration 167/1000 | Loss: 0.00001266
Iteration 168/1000 | Loss: 0.00001266
Iteration 169/1000 | Loss: 0.00001266
Iteration 170/1000 | Loss: 0.00001266
Iteration 171/1000 | Loss: 0.00001266
Iteration 172/1000 | Loss: 0.00001266
Iteration 173/1000 | Loss: 0.00001266
Iteration 174/1000 | Loss: 0.00001266
Iteration 175/1000 | Loss: 0.00001266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.2657866136578377e-05, 1.2657866136578377e-05, 1.2657866136578377e-05, 1.2657866136578377e-05, 1.2657866136578377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2657866136578377e-05

Optimization complete. Final v2v error: 2.948281764984131 mm

Highest mean error: 4.877376556396484 mm for frame 83

Lowest mean error: 2.4634130001068115 mm for frame 169

Saving results

Total time: 44.99252438545227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959786
Iteration 2/25 | Loss: 0.00144134
Iteration 3/25 | Loss: 0.00127442
Iteration 4/25 | Loss: 0.00125452
Iteration 5/25 | Loss: 0.00124480
Iteration 6/25 | Loss: 0.00124372
Iteration 7/25 | Loss: 0.00124149
Iteration 8/25 | Loss: 0.00124077
Iteration 9/25 | Loss: 0.00124040
Iteration 10/25 | Loss: 0.00124020
Iteration 11/25 | Loss: 0.00124102
Iteration 12/25 | Loss: 0.00124032
Iteration 13/25 | Loss: 0.00123945
Iteration 14/25 | Loss: 0.00123915
Iteration 15/25 | Loss: 0.00123903
Iteration 16/25 | Loss: 0.00123901
Iteration 17/25 | Loss: 0.00123900
Iteration 18/25 | Loss: 0.00123900
Iteration 19/25 | Loss: 0.00123900
Iteration 20/25 | Loss: 0.00123900
Iteration 21/25 | Loss: 0.00123900
Iteration 22/25 | Loss: 0.00123900
Iteration 23/25 | Loss: 0.00123900
Iteration 24/25 | Loss: 0.00123900
Iteration 25/25 | Loss: 0.00123900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73815846
Iteration 2/25 | Loss: 0.00249244
Iteration 3/25 | Loss: 0.00249236
Iteration 4/25 | Loss: 0.00249236
Iteration 5/25 | Loss: 0.00249236
Iteration 6/25 | Loss: 0.00249236
Iteration 7/25 | Loss: 0.00249236
Iteration 8/25 | Loss: 0.00249236
Iteration 9/25 | Loss: 0.00249236
Iteration 10/25 | Loss: 0.00249236
Iteration 11/25 | Loss: 0.00249236
Iteration 12/25 | Loss: 0.00249236
Iteration 13/25 | Loss: 0.00249236
Iteration 14/25 | Loss: 0.00249236
Iteration 15/25 | Loss: 0.00249236
Iteration 16/25 | Loss: 0.00249236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024923563469201326, 0.0024923563469201326, 0.0024923563469201326, 0.0024923563469201326, 0.0024923563469201326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024923563469201326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00249236
Iteration 2/1000 | Loss: 0.00003426
Iteration 3/1000 | Loss: 0.00002529
Iteration 4/1000 | Loss: 0.00002037
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001658
Iteration 8/1000 | Loss: 0.00003105
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001517
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001470
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001467
Iteration 17/1000 | Loss: 0.00001463
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00003126
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001427
Iteration 23/1000 | Loss: 0.00001427
Iteration 24/1000 | Loss: 0.00002065
Iteration 25/1000 | Loss: 0.00001519
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001421
Iteration 28/1000 | Loss: 0.00001421
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001421
Iteration 32/1000 | Loss: 0.00001421
Iteration 33/1000 | Loss: 0.00001421
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001420
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001420
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001420
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001417
Iteration 54/1000 | Loss: 0.00001417
Iteration 55/1000 | Loss: 0.00001416
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001410
Iteration 58/1000 | Loss: 0.00001410
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001409
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001438
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001398
Iteration 77/1000 | Loss: 0.00001398
Iteration 78/1000 | Loss: 0.00001398
Iteration 79/1000 | Loss: 0.00001398
Iteration 80/1000 | Loss: 0.00001398
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001397
Iteration 84/1000 | Loss: 0.00001397
Iteration 85/1000 | Loss: 0.00001397
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001391
Iteration 103/1000 | Loss: 0.00001391
Iteration 104/1000 | Loss: 0.00001391
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001389
Iteration 108/1000 | Loss: 0.00001389
Iteration 109/1000 | Loss: 0.00001389
Iteration 110/1000 | Loss: 0.00001389
Iteration 111/1000 | Loss: 0.00001388
Iteration 112/1000 | Loss: 0.00001388
Iteration 113/1000 | Loss: 0.00001388
Iteration 114/1000 | Loss: 0.00001388
Iteration 115/1000 | Loss: 0.00001388
Iteration 116/1000 | Loss: 0.00001388
Iteration 117/1000 | Loss: 0.00001388
Iteration 118/1000 | Loss: 0.00001388
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001384
Iteration 152/1000 | Loss: 0.00001384
Iteration 153/1000 | Loss: 0.00001384
Iteration 154/1000 | Loss: 0.00001384
Iteration 155/1000 | Loss: 0.00001384
Iteration 156/1000 | Loss: 0.00001384
Iteration 157/1000 | Loss: 0.00001384
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001383
Iteration 161/1000 | Loss: 0.00001383
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001383
Iteration 168/1000 | Loss: 0.00001383
Iteration 169/1000 | Loss: 0.00001383
Iteration 170/1000 | Loss: 0.00001382
Iteration 171/1000 | Loss: 0.00001382
Iteration 172/1000 | Loss: 0.00001382
Iteration 173/1000 | Loss: 0.00001382
Iteration 174/1000 | Loss: 0.00001382
Iteration 175/1000 | Loss: 0.00001381
Iteration 176/1000 | Loss: 0.00001381
Iteration 177/1000 | Loss: 0.00001381
Iteration 178/1000 | Loss: 0.00001381
Iteration 179/1000 | Loss: 0.00001381
Iteration 180/1000 | Loss: 0.00001381
Iteration 181/1000 | Loss: 0.00001381
Iteration 182/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.3814371413900517e-05, 1.3814371413900517e-05, 1.3814371413900517e-05, 1.3814371413900517e-05, 1.3814371413900517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3814371413900517e-05

Optimization complete. Final v2v error: 3.175156593322754 mm

Highest mean error: 4.157496452331543 mm for frame 239

Lowest mean error: 2.6885359287261963 mm for frame 133

Saving results

Total time: 73.4886531829834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534123
Iteration 2/25 | Loss: 0.00142609
Iteration 3/25 | Loss: 0.00130066
Iteration 4/25 | Loss: 0.00129221
Iteration 5/25 | Loss: 0.00129023
Iteration 6/25 | Loss: 0.00128957
Iteration 7/25 | Loss: 0.00128957
Iteration 8/25 | Loss: 0.00128957
Iteration 9/25 | Loss: 0.00128957
Iteration 10/25 | Loss: 0.00128957
Iteration 11/25 | Loss: 0.00128957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012895726831629872, 0.0012895726831629872, 0.0012895726831629872, 0.0012895726831629872, 0.0012895726831629872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012895726831629872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69843507
Iteration 2/25 | Loss: 0.00168207
Iteration 3/25 | Loss: 0.00168207
Iteration 4/25 | Loss: 0.00168207
Iteration 5/25 | Loss: 0.00168206
Iteration 6/25 | Loss: 0.00168206
Iteration 7/25 | Loss: 0.00168206
Iteration 8/25 | Loss: 0.00168206
Iteration 9/25 | Loss: 0.00168206
Iteration 10/25 | Loss: 0.00168206
Iteration 11/25 | Loss: 0.00168206
Iteration 12/25 | Loss: 0.00168206
Iteration 13/25 | Loss: 0.00168206
Iteration 14/25 | Loss: 0.00168206
Iteration 15/25 | Loss: 0.00168206
Iteration 16/25 | Loss: 0.00168206
Iteration 17/25 | Loss: 0.00168206
Iteration 18/25 | Loss: 0.00168206
Iteration 19/25 | Loss: 0.00168206
Iteration 20/25 | Loss: 0.00168206
Iteration 21/25 | Loss: 0.00168206
Iteration 22/25 | Loss: 0.00168206
Iteration 23/25 | Loss: 0.00168206
Iteration 24/25 | Loss: 0.00168206
Iteration 25/25 | Loss: 0.00168206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168206
Iteration 2/1000 | Loss: 0.00003613
Iteration 3/1000 | Loss: 0.00002592
Iteration 4/1000 | Loss: 0.00002229
Iteration 5/1000 | Loss: 0.00002013
Iteration 6/1000 | Loss: 0.00001920
Iteration 7/1000 | Loss: 0.00001888
Iteration 8/1000 | Loss: 0.00001835
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001741
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001677
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001631
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001578
Iteration 23/1000 | Loss: 0.00001574
Iteration 24/1000 | Loss: 0.00001570
Iteration 25/1000 | Loss: 0.00001570
Iteration 26/1000 | Loss: 0.00001565
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001564
Iteration 29/1000 | Loss: 0.00001564
Iteration 30/1000 | Loss: 0.00001564
Iteration 31/1000 | Loss: 0.00001564
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001562
Iteration 35/1000 | Loss: 0.00001562
Iteration 36/1000 | Loss: 0.00001561
Iteration 37/1000 | Loss: 0.00001561
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001561
Iteration 45/1000 | Loss: 0.00001561
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001559
Iteration 49/1000 | Loss: 0.00001559
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001559
Iteration 52/1000 | Loss: 0.00001558
Iteration 53/1000 | Loss: 0.00001558
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001558
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00001555
Iteration 66/1000 | Loss: 0.00001555
Iteration 67/1000 | Loss: 0.00001555
Iteration 68/1000 | Loss: 0.00001555
Iteration 69/1000 | Loss: 0.00001555
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001554
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001553
Iteration 78/1000 | Loss: 0.00001553
Iteration 79/1000 | Loss: 0.00001553
Iteration 80/1000 | Loss: 0.00001553
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001552
Iteration 83/1000 | Loss: 0.00001552
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00001550
Iteration 90/1000 | Loss: 0.00001550
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001547
Iteration 110/1000 | Loss: 0.00001547
Iteration 111/1000 | Loss: 0.00001547
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001547
Iteration 116/1000 | Loss: 0.00001547
Iteration 117/1000 | Loss: 0.00001547
Iteration 118/1000 | Loss: 0.00001547
Iteration 119/1000 | Loss: 0.00001547
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001546
Iteration 123/1000 | Loss: 0.00001546
Iteration 124/1000 | Loss: 0.00001546
Iteration 125/1000 | Loss: 0.00001546
Iteration 126/1000 | Loss: 0.00001546
Iteration 127/1000 | Loss: 0.00001546
Iteration 128/1000 | Loss: 0.00001546
Iteration 129/1000 | Loss: 0.00001546
Iteration 130/1000 | Loss: 0.00001546
Iteration 131/1000 | Loss: 0.00001546
Iteration 132/1000 | Loss: 0.00001546
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001545
Iteration 140/1000 | Loss: 0.00001545
Iteration 141/1000 | Loss: 0.00001545
Iteration 142/1000 | Loss: 0.00001545
Iteration 143/1000 | Loss: 0.00001545
Iteration 144/1000 | Loss: 0.00001545
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001545
Iteration 150/1000 | Loss: 0.00001545
Iteration 151/1000 | Loss: 0.00001545
Iteration 152/1000 | Loss: 0.00001545
Iteration 153/1000 | Loss: 0.00001545
Iteration 154/1000 | Loss: 0.00001545
Iteration 155/1000 | Loss: 0.00001545
Iteration 156/1000 | Loss: 0.00001545
Iteration 157/1000 | Loss: 0.00001545
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001544
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001544
Iteration 168/1000 | Loss: 0.00001544
Iteration 169/1000 | Loss: 0.00001544
Iteration 170/1000 | Loss: 0.00001544
Iteration 171/1000 | Loss: 0.00001544
Iteration 172/1000 | Loss: 0.00001544
Iteration 173/1000 | Loss: 0.00001544
Iteration 174/1000 | Loss: 0.00001544
Iteration 175/1000 | Loss: 0.00001544
Iteration 176/1000 | Loss: 0.00001544
Iteration 177/1000 | Loss: 0.00001544
Iteration 178/1000 | Loss: 0.00001544
Iteration 179/1000 | Loss: 0.00001544
Iteration 180/1000 | Loss: 0.00001543
Iteration 181/1000 | Loss: 0.00001543
Iteration 182/1000 | Loss: 0.00001543
Iteration 183/1000 | Loss: 0.00001543
Iteration 184/1000 | Loss: 0.00001543
Iteration 185/1000 | Loss: 0.00001543
Iteration 186/1000 | Loss: 0.00001543
Iteration 187/1000 | Loss: 0.00001543
Iteration 188/1000 | Loss: 0.00001543
Iteration 189/1000 | Loss: 0.00001543
Iteration 190/1000 | Loss: 0.00001543
Iteration 191/1000 | Loss: 0.00001543
Iteration 192/1000 | Loss: 0.00001543
Iteration 193/1000 | Loss: 0.00001543
Iteration 194/1000 | Loss: 0.00001543
Iteration 195/1000 | Loss: 0.00001543
Iteration 196/1000 | Loss: 0.00001542
Iteration 197/1000 | Loss: 0.00001542
Iteration 198/1000 | Loss: 0.00001542
Iteration 199/1000 | Loss: 0.00001542
Iteration 200/1000 | Loss: 0.00001542
Iteration 201/1000 | Loss: 0.00001542
Iteration 202/1000 | Loss: 0.00001542
Iteration 203/1000 | Loss: 0.00001542
Iteration 204/1000 | Loss: 0.00001542
Iteration 205/1000 | Loss: 0.00001542
Iteration 206/1000 | Loss: 0.00001542
Iteration 207/1000 | Loss: 0.00001542
Iteration 208/1000 | Loss: 0.00001542
Iteration 209/1000 | Loss: 0.00001542
Iteration 210/1000 | Loss: 0.00001542
Iteration 211/1000 | Loss: 0.00001542
Iteration 212/1000 | Loss: 0.00001541
Iteration 213/1000 | Loss: 0.00001541
Iteration 214/1000 | Loss: 0.00001541
Iteration 215/1000 | Loss: 0.00001541
Iteration 216/1000 | Loss: 0.00001541
Iteration 217/1000 | Loss: 0.00001541
Iteration 218/1000 | Loss: 0.00001541
Iteration 219/1000 | Loss: 0.00001541
Iteration 220/1000 | Loss: 0.00001541
Iteration 221/1000 | Loss: 0.00001541
Iteration 222/1000 | Loss: 0.00001541
Iteration 223/1000 | Loss: 0.00001541
Iteration 224/1000 | Loss: 0.00001541
Iteration 225/1000 | Loss: 0.00001541
Iteration 226/1000 | Loss: 0.00001540
Iteration 227/1000 | Loss: 0.00001540
Iteration 228/1000 | Loss: 0.00001540
Iteration 229/1000 | Loss: 0.00001540
Iteration 230/1000 | Loss: 0.00001540
Iteration 231/1000 | Loss: 0.00001540
Iteration 232/1000 | Loss: 0.00001540
Iteration 233/1000 | Loss: 0.00001540
Iteration 234/1000 | Loss: 0.00001540
Iteration 235/1000 | Loss: 0.00001540
Iteration 236/1000 | Loss: 0.00001540
Iteration 237/1000 | Loss: 0.00001540
Iteration 238/1000 | Loss: 0.00001540
Iteration 239/1000 | Loss: 0.00001540
Iteration 240/1000 | Loss: 0.00001540
Iteration 241/1000 | Loss: 0.00001540
Iteration 242/1000 | Loss: 0.00001540
Iteration 243/1000 | Loss: 0.00001540
Iteration 244/1000 | Loss: 0.00001540
Iteration 245/1000 | Loss: 0.00001540
Iteration 246/1000 | Loss: 0.00001540
Iteration 247/1000 | Loss: 0.00001540
Iteration 248/1000 | Loss: 0.00001540
Iteration 249/1000 | Loss: 0.00001540
Iteration 250/1000 | Loss: 0.00001540
Iteration 251/1000 | Loss: 0.00001540
Iteration 252/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.539562435937114e-05, 1.539562435937114e-05, 1.539562435937114e-05, 1.539562435937114e-05, 1.539562435937114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.539562435937114e-05

Optimization complete. Final v2v error: 3.2703170776367188 mm

Highest mean error: 3.782505750656128 mm for frame 10

Lowest mean error: 3.015171527862549 mm for frame 46

Saving results

Total time: 51.842169761657715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816769
Iteration 2/25 | Loss: 0.00157324
Iteration 3/25 | Loss: 0.00130162
Iteration 4/25 | Loss: 0.00128101
Iteration 5/25 | Loss: 0.00127856
Iteration 6/25 | Loss: 0.00127845
Iteration 7/25 | Loss: 0.00127845
Iteration 8/25 | Loss: 0.00127845
Iteration 9/25 | Loss: 0.00127845
Iteration 10/25 | Loss: 0.00127845
Iteration 11/25 | Loss: 0.00127845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012784526916220784, 0.0012784526916220784, 0.0012784526916220784, 0.0012784526916220784, 0.0012784526916220784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012784526916220784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19282365
Iteration 2/25 | Loss: 0.00170520
Iteration 3/25 | Loss: 0.00170519
Iteration 4/25 | Loss: 0.00170519
Iteration 5/25 | Loss: 0.00170519
Iteration 6/25 | Loss: 0.00170519
Iteration 7/25 | Loss: 0.00170519
Iteration 8/25 | Loss: 0.00170519
Iteration 9/25 | Loss: 0.00170519
Iteration 10/25 | Loss: 0.00170519
Iteration 11/25 | Loss: 0.00170519
Iteration 12/25 | Loss: 0.00170519
Iteration 13/25 | Loss: 0.00170519
Iteration 14/25 | Loss: 0.00170519
Iteration 15/25 | Loss: 0.00170519
Iteration 16/25 | Loss: 0.00170519
Iteration 17/25 | Loss: 0.00170519
Iteration 18/25 | Loss: 0.00170519
Iteration 19/25 | Loss: 0.00170519
Iteration 20/25 | Loss: 0.00170519
Iteration 21/25 | Loss: 0.00170519
Iteration 22/25 | Loss: 0.00170519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017051857430487871, 0.0017051857430487871, 0.0017051857430487871, 0.0017051857430487871, 0.0017051857430487871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017051857430487871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170519
Iteration 2/1000 | Loss: 0.00002395
Iteration 3/1000 | Loss: 0.00001762
Iteration 4/1000 | Loss: 0.00001589
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001377
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001305
Iteration 10/1000 | Loss: 0.00001277
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001215
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001200
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001190
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001170
Iteration 36/1000 | Loss: 0.00001168
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001160
Iteration 41/1000 | Loss: 0.00001160
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001155
Iteration 62/1000 | Loss: 0.00001155
Iteration 63/1000 | Loss: 0.00001155
Iteration 64/1000 | Loss: 0.00001155
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001154
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001154
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001152
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001151
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001151
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001151
Iteration 135/1000 | Loss: 0.00001151
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001151
Iteration 141/1000 | Loss: 0.00001151
Iteration 142/1000 | Loss: 0.00001151
Iteration 143/1000 | Loss: 0.00001151
Iteration 144/1000 | Loss: 0.00001151
Iteration 145/1000 | Loss: 0.00001151
Iteration 146/1000 | Loss: 0.00001151
Iteration 147/1000 | Loss: 0.00001151
Iteration 148/1000 | Loss: 0.00001151
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001151
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001151
Iteration 153/1000 | Loss: 0.00001151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.150516345660435e-05, 1.150516345660435e-05, 1.150516345660435e-05, 1.150516345660435e-05, 1.150516345660435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.150516345660435e-05

Optimization complete. Final v2v error: 2.885484457015991 mm

Highest mean error: 2.9339683055877686 mm for frame 99

Lowest mean error: 2.842336893081665 mm for frame 141

Saving results

Total time: 36.96884107589722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010011
Iteration 2/25 | Loss: 0.00293525
Iteration 3/25 | Loss: 0.00252551
Iteration 4/25 | Loss: 0.00236393
Iteration 5/25 | Loss: 0.00225984
Iteration 6/25 | Loss: 0.00216023
Iteration 7/25 | Loss: 0.00205850
Iteration 8/25 | Loss: 0.00192743
Iteration 9/25 | Loss: 0.00180101
Iteration 10/25 | Loss: 0.00175807
Iteration 11/25 | Loss: 0.00169862
Iteration 12/25 | Loss: 0.00164419
Iteration 13/25 | Loss: 0.00162545
Iteration 14/25 | Loss: 0.00162808
Iteration 15/25 | Loss: 0.00161710
Iteration 16/25 | Loss: 0.00160458
Iteration 17/25 | Loss: 0.00158745
Iteration 18/25 | Loss: 0.00158154
Iteration 19/25 | Loss: 0.00158020
Iteration 20/25 | Loss: 0.00157834
Iteration 21/25 | Loss: 0.00157918
Iteration 22/25 | Loss: 0.00157776
Iteration 23/25 | Loss: 0.00157847
Iteration 24/25 | Loss: 0.00157810
Iteration 25/25 | Loss: 0.00157890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18663073
Iteration 2/25 | Loss: 0.00616907
Iteration 3/25 | Loss: 0.00616906
Iteration 4/25 | Loss: 0.00616906
Iteration 5/25 | Loss: 0.00616906
Iteration 6/25 | Loss: 0.00616906
Iteration 7/25 | Loss: 0.00616906
Iteration 8/25 | Loss: 0.00616906
Iteration 9/25 | Loss: 0.00616906
Iteration 10/25 | Loss: 0.00616906
Iteration 11/25 | Loss: 0.00616906
Iteration 12/25 | Loss: 0.00616906
Iteration 13/25 | Loss: 0.00616906
Iteration 14/25 | Loss: 0.00616906
Iteration 15/25 | Loss: 0.00616906
Iteration 16/25 | Loss: 0.00616906
Iteration 17/25 | Loss: 0.00616906
Iteration 18/25 | Loss: 0.00616906
Iteration 19/25 | Loss: 0.00616906
Iteration 20/25 | Loss: 0.00616906
Iteration 21/25 | Loss: 0.00616906
Iteration 22/25 | Loss: 0.00616906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.006169057451188564, 0.006169057451188564, 0.006169057451188564, 0.006169057451188564, 0.006169057451188564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006169057451188564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00616906
Iteration 2/1000 | Loss: 0.00540763
Iteration 3/1000 | Loss: 0.00082512
Iteration 4/1000 | Loss: 0.00031278
Iteration 5/1000 | Loss: 0.00021122
Iteration 6/1000 | Loss: 0.00014192
Iteration 7/1000 | Loss: 0.00008792
Iteration 8/1000 | Loss: 0.00006718
Iteration 9/1000 | Loss: 0.00035723
Iteration 10/1000 | Loss: 0.00111372
Iteration 11/1000 | Loss: 0.00066637
Iteration 12/1000 | Loss: 0.00012930
Iteration 13/1000 | Loss: 0.00144588
Iteration 14/1000 | Loss: 0.00058444
Iteration 15/1000 | Loss: 0.00008327
Iteration 16/1000 | Loss: 0.00006136
Iteration 17/1000 | Loss: 0.00032019
Iteration 18/1000 | Loss: 0.00006673
Iteration 19/1000 | Loss: 0.00006214
Iteration 20/1000 | Loss: 0.00004081
Iteration 21/1000 | Loss: 0.00004955
Iteration 22/1000 | Loss: 0.00003624
Iteration 23/1000 | Loss: 0.00002858
Iteration 24/1000 | Loss: 0.00004001
Iteration 25/1000 | Loss: 0.00002693
Iteration 26/1000 | Loss: 0.00003901
Iteration 27/1000 | Loss: 0.00003094
Iteration 28/1000 | Loss: 0.00002792
Iteration 29/1000 | Loss: 0.00002593
Iteration 30/1000 | Loss: 0.00003620
Iteration 31/1000 | Loss: 0.00002583
Iteration 32/1000 | Loss: 0.00003044
Iteration 33/1000 | Loss: 0.00002484
Iteration 34/1000 | Loss: 0.00002938
Iteration 35/1000 | Loss: 0.00002349
Iteration 36/1000 | Loss: 0.00003254
Iteration 37/1000 | Loss: 0.00003816
Iteration 38/1000 | Loss: 0.00003051
Iteration 39/1000 | Loss: 0.00003839
Iteration 40/1000 | Loss: 0.00002301
Iteration 41/1000 | Loss: 0.00003208
Iteration 42/1000 | Loss: 0.00003080
Iteration 43/1000 | Loss: 0.00004856
Iteration 44/1000 | Loss: 0.00002759
Iteration 45/1000 | Loss: 0.00002616
Iteration 46/1000 | Loss: 0.00001976
Iteration 47/1000 | Loss: 0.00001902
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001751
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001619
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001618
Iteration 64/1000 | Loss: 0.00003022
Iteration 65/1000 | Loss: 0.00003021
Iteration 66/1000 | Loss: 0.00002337
Iteration 67/1000 | Loss: 0.00001612
Iteration 68/1000 | Loss: 0.00001612
Iteration 69/1000 | Loss: 0.00001612
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001611
Iteration 77/1000 | Loss: 0.00001611
Iteration 78/1000 | Loss: 0.00001611
Iteration 79/1000 | Loss: 0.00001611
Iteration 80/1000 | Loss: 0.00001611
Iteration 81/1000 | Loss: 0.00001611
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001611
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001608
Iteration 92/1000 | Loss: 0.00001608
Iteration 93/1000 | Loss: 0.00001608
Iteration 94/1000 | Loss: 0.00001608
Iteration 95/1000 | Loss: 0.00001608
Iteration 96/1000 | Loss: 0.00001607
Iteration 97/1000 | Loss: 0.00001607
Iteration 98/1000 | Loss: 0.00001607
Iteration 99/1000 | Loss: 0.00001607
Iteration 100/1000 | Loss: 0.00001606
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001605
Iteration 104/1000 | Loss: 0.00001605
Iteration 105/1000 | Loss: 0.00001605
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001603
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001601
Iteration 112/1000 | Loss: 0.00001601
Iteration 113/1000 | Loss: 0.00001601
Iteration 114/1000 | Loss: 0.00001601
Iteration 115/1000 | Loss: 0.00002732
Iteration 116/1000 | Loss: 0.00002730
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001601
Iteration 120/1000 | Loss: 0.00001601
Iteration 121/1000 | Loss: 0.00001601
Iteration 122/1000 | Loss: 0.00001601
Iteration 123/1000 | Loss: 0.00001601
Iteration 124/1000 | Loss: 0.00001601
Iteration 125/1000 | Loss: 0.00001601
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001601
Iteration 128/1000 | Loss: 0.00001601
Iteration 129/1000 | Loss: 0.00001600
Iteration 130/1000 | Loss: 0.00001600
Iteration 131/1000 | Loss: 0.00001600
Iteration 132/1000 | Loss: 0.00001600
Iteration 133/1000 | Loss: 0.00001600
Iteration 134/1000 | Loss: 0.00001599
Iteration 135/1000 | Loss: 0.00001599
Iteration 136/1000 | Loss: 0.00001599
Iteration 137/1000 | Loss: 0.00001599
Iteration 138/1000 | Loss: 0.00001599
Iteration 139/1000 | Loss: 0.00001598
Iteration 140/1000 | Loss: 0.00001598
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001597
Iteration 143/1000 | Loss: 0.00001594
Iteration 144/1000 | Loss: 0.00001594
Iteration 145/1000 | Loss: 0.00001593
Iteration 146/1000 | Loss: 0.00001593
Iteration 147/1000 | Loss: 0.00001593
Iteration 148/1000 | Loss: 0.00001593
Iteration 149/1000 | Loss: 0.00001592
Iteration 150/1000 | Loss: 0.00001592
Iteration 151/1000 | Loss: 0.00001592
Iteration 152/1000 | Loss: 0.00001592
Iteration 153/1000 | Loss: 0.00001592
Iteration 154/1000 | Loss: 0.00001592
Iteration 155/1000 | Loss: 0.00001592
Iteration 156/1000 | Loss: 0.00001591
Iteration 157/1000 | Loss: 0.00001591
Iteration 158/1000 | Loss: 0.00001591
Iteration 159/1000 | Loss: 0.00001591
Iteration 160/1000 | Loss: 0.00001591
Iteration 161/1000 | Loss: 0.00001591
Iteration 162/1000 | Loss: 0.00001591
Iteration 163/1000 | Loss: 0.00001591
Iteration 164/1000 | Loss: 0.00001591
Iteration 165/1000 | Loss: 0.00001591
Iteration 166/1000 | Loss: 0.00001591
Iteration 167/1000 | Loss: 0.00001591
Iteration 168/1000 | Loss: 0.00001591
Iteration 169/1000 | Loss: 0.00001591
Iteration 170/1000 | Loss: 0.00001591
Iteration 171/1000 | Loss: 0.00001591
Iteration 172/1000 | Loss: 0.00001590
Iteration 173/1000 | Loss: 0.00001590
Iteration 174/1000 | Loss: 0.00001590
Iteration 175/1000 | Loss: 0.00001590
Iteration 176/1000 | Loss: 0.00001590
Iteration 177/1000 | Loss: 0.00001590
Iteration 178/1000 | Loss: 0.00001590
Iteration 179/1000 | Loss: 0.00001590
Iteration 180/1000 | Loss: 0.00001590
Iteration 181/1000 | Loss: 0.00001590
Iteration 182/1000 | Loss: 0.00001590
Iteration 183/1000 | Loss: 0.00001590
Iteration 184/1000 | Loss: 0.00001590
Iteration 185/1000 | Loss: 0.00001590
Iteration 186/1000 | Loss: 0.00001590
Iteration 187/1000 | Loss: 0.00001590
Iteration 188/1000 | Loss: 0.00001590
Iteration 189/1000 | Loss: 0.00001589
Iteration 190/1000 | Loss: 0.00001589
Iteration 191/1000 | Loss: 0.00001589
Iteration 192/1000 | Loss: 0.00001589
Iteration 193/1000 | Loss: 0.00001589
Iteration 194/1000 | Loss: 0.00001589
Iteration 195/1000 | Loss: 0.00001589
Iteration 196/1000 | Loss: 0.00001589
Iteration 197/1000 | Loss: 0.00001589
Iteration 198/1000 | Loss: 0.00001589
Iteration 199/1000 | Loss: 0.00001589
Iteration 200/1000 | Loss: 0.00001589
Iteration 201/1000 | Loss: 0.00001589
Iteration 202/1000 | Loss: 0.00001589
Iteration 203/1000 | Loss: 0.00001588
Iteration 204/1000 | Loss: 0.00001588
Iteration 205/1000 | Loss: 0.00001588
Iteration 206/1000 | Loss: 0.00001588
Iteration 207/1000 | Loss: 0.00001587
Iteration 208/1000 | Loss: 0.00001587
Iteration 209/1000 | Loss: 0.00001587
Iteration 210/1000 | Loss: 0.00001587
Iteration 211/1000 | Loss: 0.00001586
Iteration 212/1000 | Loss: 0.00001586
Iteration 213/1000 | Loss: 0.00001586
Iteration 214/1000 | Loss: 0.00001586
Iteration 215/1000 | Loss: 0.00001586
Iteration 216/1000 | Loss: 0.00001586
Iteration 217/1000 | Loss: 0.00001586
Iteration 218/1000 | Loss: 0.00001586
Iteration 219/1000 | Loss: 0.00001586
Iteration 220/1000 | Loss: 0.00001586
Iteration 221/1000 | Loss: 0.00001586
Iteration 222/1000 | Loss: 0.00001586
Iteration 223/1000 | Loss: 0.00001586
Iteration 224/1000 | Loss: 0.00001586
Iteration 225/1000 | Loss: 0.00001586
Iteration 226/1000 | Loss: 0.00001585
Iteration 227/1000 | Loss: 0.00001585
Iteration 228/1000 | Loss: 0.00001585
Iteration 229/1000 | Loss: 0.00001585
Iteration 230/1000 | Loss: 0.00001585
Iteration 231/1000 | Loss: 0.00001585
Iteration 232/1000 | Loss: 0.00001585
Iteration 233/1000 | Loss: 0.00001585
Iteration 234/1000 | Loss: 0.00001585
Iteration 235/1000 | Loss: 0.00001585
Iteration 236/1000 | Loss: 0.00001585
Iteration 237/1000 | Loss: 0.00002788
Iteration 238/1000 | Loss: 0.00001705
Iteration 239/1000 | Loss: 0.00001637
Iteration 240/1000 | Loss: 0.00001601
Iteration 241/1000 | Loss: 0.00001577
Iteration 242/1000 | Loss: 0.00001571
Iteration 243/1000 | Loss: 0.00001571
Iteration 244/1000 | Loss: 0.00001571
Iteration 245/1000 | Loss: 0.00001571
Iteration 246/1000 | Loss: 0.00001571
Iteration 247/1000 | Loss: 0.00001571
Iteration 248/1000 | Loss: 0.00001571
Iteration 249/1000 | Loss: 0.00001570
Iteration 250/1000 | Loss: 0.00001570
Iteration 251/1000 | Loss: 0.00001570
Iteration 252/1000 | Loss: 0.00001570
Iteration 253/1000 | Loss: 0.00001569
Iteration 254/1000 | Loss: 0.00001569
Iteration 255/1000 | Loss: 0.00001568
Iteration 256/1000 | Loss: 0.00001568
Iteration 257/1000 | Loss: 0.00001568
Iteration 258/1000 | Loss: 0.00001568
Iteration 259/1000 | Loss: 0.00001568
Iteration 260/1000 | Loss: 0.00001568
Iteration 261/1000 | Loss: 0.00001568
Iteration 262/1000 | Loss: 0.00001568
Iteration 263/1000 | Loss: 0.00001568
Iteration 264/1000 | Loss: 0.00001568
Iteration 265/1000 | Loss: 0.00001568
Iteration 266/1000 | Loss: 0.00001568
Iteration 267/1000 | Loss: 0.00001567
Iteration 268/1000 | Loss: 0.00001567
Iteration 269/1000 | Loss: 0.00001567
Iteration 270/1000 | Loss: 0.00001567
Iteration 271/1000 | Loss: 0.00001567
Iteration 272/1000 | Loss: 0.00001567
Iteration 273/1000 | Loss: 0.00001567
Iteration 274/1000 | Loss: 0.00001566
Iteration 275/1000 | Loss: 0.00001566
Iteration 276/1000 | Loss: 0.00001566
Iteration 277/1000 | Loss: 0.00001566
Iteration 278/1000 | Loss: 0.00001566
Iteration 279/1000 | Loss: 0.00001566
Iteration 280/1000 | Loss: 0.00001566
Iteration 281/1000 | Loss: 0.00001566
Iteration 282/1000 | Loss: 0.00001566
Iteration 283/1000 | Loss: 0.00001566
Iteration 284/1000 | Loss: 0.00001566
Iteration 285/1000 | Loss: 0.00001566
Iteration 286/1000 | Loss: 0.00001566
Iteration 287/1000 | Loss: 0.00001566
Iteration 288/1000 | Loss: 0.00001566
Iteration 289/1000 | Loss: 0.00001566
Iteration 290/1000 | Loss: 0.00001566
Iteration 291/1000 | Loss: 0.00001566
Iteration 292/1000 | Loss: 0.00001566
Iteration 293/1000 | Loss: 0.00001566
Iteration 294/1000 | Loss: 0.00001566
Iteration 295/1000 | Loss: 0.00001566
Iteration 296/1000 | Loss: 0.00001566
Iteration 297/1000 | Loss: 0.00001566
Iteration 298/1000 | Loss: 0.00001566
Iteration 299/1000 | Loss: 0.00001566
Iteration 300/1000 | Loss: 0.00001566
Iteration 301/1000 | Loss: 0.00001566
Iteration 302/1000 | Loss: 0.00001566
Iteration 303/1000 | Loss: 0.00001566
Iteration 304/1000 | Loss: 0.00001566
Iteration 305/1000 | Loss: 0.00001566
Iteration 306/1000 | Loss: 0.00001566
Iteration 307/1000 | Loss: 0.00001566
Iteration 308/1000 | Loss: 0.00001566
Iteration 309/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [1.5656411051168106e-05, 1.5656411051168106e-05, 1.5656411051168106e-05, 1.5656411051168106e-05, 1.5656411051168106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5656411051168106e-05

Optimization complete. Final v2v error: 3.312225103378296 mm

Highest mean error: 4.811581134796143 mm for frame 167

Lowest mean error: 2.9527416229248047 mm for frame 55

Saving results

Total time: 151.1769254207611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00732750
Iteration 2/25 | Loss: 0.00142798
Iteration 3/25 | Loss: 0.00129525
Iteration 4/25 | Loss: 0.00123131
Iteration 5/25 | Loss: 0.00122615
Iteration 6/25 | Loss: 0.00122529
Iteration 7/25 | Loss: 0.00122490
Iteration 8/25 | Loss: 0.00122479
Iteration 9/25 | Loss: 0.00122479
Iteration 10/25 | Loss: 0.00122479
Iteration 11/25 | Loss: 0.00122479
Iteration 12/25 | Loss: 0.00122479
Iteration 13/25 | Loss: 0.00122479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012247911654412746, 0.0012247911654412746, 0.0012247911654412746, 0.0012247911654412746, 0.0012247911654412746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012247911654412746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66320980
Iteration 2/25 | Loss: 0.00215580
Iteration 3/25 | Loss: 0.00215580
Iteration 4/25 | Loss: 0.00215580
Iteration 5/25 | Loss: 0.00215580
Iteration 6/25 | Loss: 0.00215580
Iteration 7/25 | Loss: 0.00215580
Iteration 8/25 | Loss: 0.00215580
Iteration 9/25 | Loss: 0.00215580
Iteration 10/25 | Loss: 0.00215580
Iteration 11/25 | Loss: 0.00215580
Iteration 12/25 | Loss: 0.00215580
Iteration 13/25 | Loss: 0.00215580
Iteration 14/25 | Loss: 0.00215580
Iteration 15/25 | Loss: 0.00215580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002155798953026533, 0.002155798953026533, 0.002155798953026533, 0.002155798953026533, 0.002155798953026533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002155798953026533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215580
Iteration 2/1000 | Loss: 0.00002019
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001313
Iteration 5/1000 | Loss: 0.00001233
Iteration 6/1000 | Loss: 0.00001163
Iteration 7/1000 | Loss: 0.00001121
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001052
Iteration 10/1000 | Loss: 0.00001042
Iteration 11/1000 | Loss: 0.00001029
Iteration 12/1000 | Loss: 0.00001024
Iteration 13/1000 | Loss: 0.00001006
Iteration 14/1000 | Loss: 0.00001004
Iteration 15/1000 | Loss: 0.00000992
Iteration 16/1000 | Loss: 0.00000990
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000986
Iteration 19/1000 | Loss: 0.00000986
Iteration 20/1000 | Loss: 0.00000986
Iteration 21/1000 | Loss: 0.00000985
Iteration 22/1000 | Loss: 0.00000985
Iteration 23/1000 | Loss: 0.00000980
Iteration 24/1000 | Loss: 0.00000978
Iteration 25/1000 | Loss: 0.00000977
Iteration 26/1000 | Loss: 0.00000974
Iteration 27/1000 | Loss: 0.00000973
Iteration 28/1000 | Loss: 0.00000972
Iteration 29/1000 | Loss: 0.00000972
Iteration 30/1000 | Loss: 0.00000971
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000964
Iteration 33/1000 | Loss: 0.00000963
Iteration 34/1000 | Loss: 0.00000963
Iteration 35/1000 | Loss: 0.00000962
Iteration 36/1000 | Loss: 0.00000962
Iteration 37/1000 | Loss: 0.00000961
Iteration 38/1000 | Loss: 0.00000960
Iteration 39/1000 | Loss: 0.00000960
Iteration 40/1000 | Loss: 0.00000959
Iteration 41/1000 | Loss: 0.00000959
Iteration 42/1000 | Loss: 0.00000958
Iteration 43/1000 | Loss: 0.00000958
Iteration 44/1000 | Loss: 0.00000958
Iteration 45/1000 | Loss: 0.00000957
Iteration 46/1000 | Loss: 0.00000957
Iteration 47/1000 | Loss: 0.00000956
Iteration 48/1000 | Loss: 0.00000956
Iteration 49/1000 | Loss: 0.00000956
Iteration 50/1000 | Loss: 0.00000955
Iteration 51/1000 | Loss: 0.00000955
Iteration 52/1000 | Loss: 0.00000954
Iteration 53/1000 | Loss: 0.00000954
Iteration 54/1000 | Loss: 0.00000953
Iteration 55/1000 | Loss: 0.00000953
Iteration 56/1000 | Loss: 0.00000952
Iteration 57/1000 | Loss: 0.00000952
Iteration 58/1000 | Loss: 0.00000952
Iteration 59/1000 | Loss: 0.00000951
Iteration 60/1000 | Loss: 0.00000951
Iteration 61/1000 | Loss: 0.00000951
Iteration 62/1000 | Loss: 0.00000950
Iteration 63/1000 | Loss: 0.00000950
Iteration 64/1000 | Loss: 0.00000950
Iteration 65/1000 | Loss: 0.00000950
Iteration 66/1000 | Loss: 0.00000950
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000948
Iteration 70/1000 | Loss: 0.00000948
Iteration 71/1000 | Loss: 0.00000948
Iteration 72/1000 | Loss: 0.00000948
Iteration 73/1000 | Loss: 0.00000948
Iteration 74/1000 | Loss: 0.00000947
Iteration 75/1000 | Loss: 0.00000947
Iteration 76/1000 | Loss: 0.00000947
Iteration 77/1000 | Loss: 0.00000947
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000946
Iteration 85/1000 | Loss: 0.00000946
Iteration 86/1000 | Loss: 0.00000945
Iteration 87/1000 | Loss: 0.00000945
Iteration 88/1000 | Loss: 0.00000945
Iteration 89/1000 | Loss: 0.00000945
Iteration 90/1000 | Loss: 0.00000945
Iteration 91/1000 | Loss: 0.00000944
Iteration 92/1000 | Loss: 0.00000944
Iteration 93/1000 | Loss: 0.00000944
Iteration 94/1000 | Loss: 0.00000944
Iteration 95/1000 | Loss: 0.00000944
Iteration 96/1000 | Loss: 0.00000943
Iteration 97/1000 | Loss: 0.00000943
Iteration 98/1000 | Loss: 0.00000943
Iteration 99/1000 | Loss: 0.00000943
Iteration 100/1000 | Loss: 0.00000943
Iteration 101/1000 | Loss: 0.00000943
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000941
Iteration 108/1000 | Loss: 0.00000941
Iteration 109/1000 | Loss: 0.00000941
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000941
Iteration 114/1000 | Loss: 0.00000940
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000940
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000939
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000939
Iteration 128/1000 | Loss: 0.00000939
Iteration 129/1000 | Loss: 0.00000939
Iteration 130/1000 | Loss: 0.00000939
Iteration 131/1000 | Loss: 0.00000938
Iteration 132/1000 | Loss: 0.00000938
Iteration 133/1000 | Loss: 0.00000938
Iteration 134/1000 | Loss: 0.00000938
Iteration 135/1000 | Loss: 0.00000937
Iteration 136/1000 | Loss: 0.00000937
Iteration 137/1000 | Loss: 0.00000937
Iteration 138/1000 | Loss: 0.00000937
Iteration 139/1000 | Loss: 0.00000937
Iteration 140/1000 | Loss: 0.00000937
Iteration 141/1000 | Loss: 0.00000936
Iteration 142/1000 | Loss: 0.00000936
Iteration 143/1000 | Loss: 0.00000936
Iteration 144/1000 | Loss: 0.00000936
Iteration 145/1000 | Loss: 0.00000936
Iteration 146/1000 | Loss: 0.00000936
Iteration 147/1000 | Loss: 0.00000936
Iteration 148/1000 | Loss: 0.00000936
Iteration 149/1000 | Loss: 0.00000935
Iteration 150/1000 | Loss: 0.00000935
Iteration 151/1000 | Loss: 0.00000935
Iteration 152/1000 | Loss: 0.00000935
Iteration 153/1000 | Loss: 0.00000935
Iteration 154/1000 | Loss: 0.00000935
Iteration 155/1000 | Loss: 0.00000935
Iteration 156/1000 | Loss: 0.00000934
Iteration 157/1000 | Loss: 0.00000934
Iteration 158/1000 | Loss: 0.00000934
Iteration 159/1000 | Loss: 0.00000934
Iteration 160/1000 | Loss: 0.00000934
Iteration 161/1000 | Loss: 0.00000934
Iteration 162/1000 | Loss: 0.00000934
Iteration 163/1000 | Loss: 0.00000934
Iteration 164/1000 | Loss: 0.00000934
Iteration 165/1000 | Loss: 0.00000934
Iteration 166/1000 | Loss: 0.00000934
Iteration 167/1000 | Loss: 0.00000934
Iteration 168/1000 | Loss: 0.00000934
Iteration 169/1000 | Loss: 0.00000934
Iteration 170/1000 | Loss: 0.00000934
Iteration 171/1000 | Loss: 0.00000933
Iteration 172/1000 | Loss: 0.00000933
Iteration 173/1000 | Loss: 0.00000933
Iteration 174/1000 | Loss: 0.00000933
Iteration 175/1000 | Loss: 0.00000933
Iteration 176/1000 | Loss: 0.00000933
Iteration 177/1000 | Loss: 0.00000933
Iteration 178/1000 | Loss: 0.00000933
Iteration 179/1000 | Loss: 0.00000933
Iteration 180/1000 | Loss: 0.00000933
Iteration 181/1000 | Loss: 0.00000933
Iteration 182/1000 | Loss: 0.00000933
Iteration 183/1000 | Loss: 0.00000932
Iteration 184/1000 | Loss: 0.00000932
Iteration 185/1000 | Loss: 0.00000932
Iteration 186/1000 | Loss: 0.00000932
Iteration 187/1000 | Loss: 0.00000932
Iteration 188/1000 | Loss: 0.00000932
Iteration 189/1000 | Loss: 0.00000932
Iteration 190/1000 | Loss: 0.00000932
Iteration 191/1000 | Loss: 0.00000932
Iteration 192/1000 | Loss: 0.00000932
Iteration 193/1000 | Loss: 0.00000932
Iteration 194/1000 | Loss: 0.00000932
Iteration 195/1000 | Loss: 0.00000932
Iteration 196/1000 | Loss: 0.00000932
Iteration 197/1000 | Loss: 0.00000932
Iteration 198/1000 | Loss: 0.00000932
Iteration 199/1000 | Loss: 0.00000932
Iteration 200/1000 | Loss: 0.00000932
Iteration 201/1000 | Loss: 0.00000932
Iteration 202/1000 | Loss: 0.00000932
Iteration 203/1000 | Loss: 0.00000932
Iteration 204/1000 | Loss: 0.00000932
Iteration 205/1000 | Loss: 0.00000932
Iteration 206/1000 | Loss: 0.00000932
Iteration 207/1000 | Loss: 0.00000932
Iteration 208/1000 | Loss: 0.00000932
Iteration 209/1000 | Loss: 0.00000932
Iteration 210/1000 | Loss: 0.00000932
Iteration 211/1000 | Loss: 0.00000932
Iteration 212/1000 | Loss: 0.00000932
Iteration 213/1000 | Loss: 0.00000932
Iteration 214/1000 | Loss: 0.00000932
Iteration 215/1000 | Loss: 0.00000932
Iteration 216/1000 | Loss: 0.00000932
Iteration 217/1000 | Loss: 0.00000932
Iteration 218/1000 | Loss: 0.00000932
Iteration 219/1000 | Loss: 0.00000932
Iteration 220/1000 | Loss: 0.00000932
Iteration 221/1000 | Loss: 0.00000932
Iteration 222/1000 | Loss: 0.00000932
Iteration 223/1000 | Loss: 0.00000932
Iteration 224/1000 | Loss: 0.00000932
Iteration 225/1000 | Loss: 0.00000932
Iteration 226/1000 | Loss: 0.00000932
Iteration 227/1000 | Loss: 0.00000932
Iteration 228/1000 | Loss: 0.00000932
Iteration 229/1000 | Loss: 0.00000932
Iteration 230/1000 | Loss: 0.00000932
Iteration 231/1000 | Loss: 0.00000932
Iteration 232/1000 | Loss: 0.00000932
Iteration 233/1000 | Loss: 0.00000932
Iteration 234/1000 | Loss: 0.00000932
Iteration 235/1000 | Loss: 0.00000932
Iteration 236/1000 | Loss: 0.00000932
Iteration 237/1000 | Loss: 0.00000932
Iteration 238/1000 | Loss: 0.00000932
Iteration 239/1000 | Loss: 0.00000932
Iteration 240/1000 | Loss: 0.00000932
Iteration 241/1000 | Loss: 0.00000932
Iteration 242/1000 | Loss: 0.00000932
Iteration 243/1000 | Loss: 0.00000932
Iteration 244/1000 | Loss: 0.00000932
Iteration 245/1000 | Loss: 0.00000932
Iteration 246/1000 | Loss: 0.00000932
Iteration 247/1000 | Loss: 0.00000932
Iteration 248/1000 | Loss: 0.00000932
Iteration 249/1000 | Loss: 0.00000932
Iteration 250/1000 | Loss: 0.00000932
Iteration 251/1000 | Loss: 0.00000932
Iteration 252/1000 | Loss: 0.00000932
Iteration 253/1000 | Loss: 0.00000932
Iteration 254/1000 | Loss: 0.00000932
Iteration 255/1000 | Loss: 0.00000932
Iteration 256/1000 | Loss: 0.00000932
Iteration 257/1000 | Loss: 0.00000932
Iteration 258/1000 | Loss: 0.00000932
Iteration 259/1000 | Loss: 0.00000932
Iteration 260/1000 | Loss: 0.00000932
Iteration 261/1000 | Loss: 0.00000932
Iteration 262/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [9.31768136069877e-06, 9.31768136069877e-06, 9.31768136069877e-06, 9.31768136069877e-06, 9.31768136069877e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.31768136069877e-06

Optimization complete. Final v2v error: 2.684955358505249 mm

Highest mean error: 2.8803749084472656 mm for frame 105

Lowest mean error: 2.5510590076446533 mm for frame 193

Saving results

Total time: 48.27775859832764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465571
Iteration 2/25 | Loss: 0.00150698
Iteration 3/25 | Loss: 0.00130780
Iteration 4/25 | Loss: 0.00128991
Iteration 5/25 | Loss: 0.00128719
Iteration 6/25 | Loss: 0.00128632
Iteration 7/25 | Loss: 0.00128632
Iteration 8/25 | Loss: 0.00128632
Iteration 9/25 | Loss: 0.00128632
Iteration 10/25 | Loss: 0.00128632
Iteration 11/25 | Loss: 0.00128632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012863189913332462, 0.0012863189913332462, 0.0012863189913332462, 0.0012863189913332462, 0.0012863189913332462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012863189913332462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28731525
Iteration 2/25 | Loss: 0.00183899
Iteration 3/25 | Loss: 0.00183897
Iteration 4/25 | Loss: 0.00183897
Iteration 5/25 | Loss: 0.00183897
Iteration 6/25 | Loss: 0.00183897
Iteration 7/25 | Loss: 0.00183897
Iteration 8/25 | Loss: 0.00183897
Iteration 9/25 | Loss: 0.00183897
Iteration 10/25 | Loss: 0.00183897
Iteration 11/25 | Loss: 0.00183897
Iteration 12/25 | Loss: 0.00183897
Iteration 13/25 | Loss: 0.00183897
Iteration 14/25 | Loss: 0.00183897
Iteration 15/25 | Loss: 0.00183897
Iteration 16/25 | Loss: 0.00183897
Iteration 17/25 | Loss: 0.00183897
Iteration 18/25 | Loss: 0.00183897
Iteration 19/25 | Loss: 0.00183897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018389702308923006, 0.0018389702308923006, 0.0018389702308923006, 0.0018389702308923006, 0.0018389702308923006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018389702308923006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183897
Iteration 2/1000 | Loss: 0.00002730
Iteration 3/1000 | Loss: 0.00001957
Iteration 4/1000 | Loss: 0.00001754
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00001610
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001523
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001463
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001441
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001397
Iteration 17/1000 | Loss: 0.00001394
Iteration 18/1000 | Loss: 0.00001392
Iteration 19/1000 | Loss: 0.00001391
Iteration 20/1000 | Loss: 0.00001390
Iteration 21/1000 | Loss: 0.00001389
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001379
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001374
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001368
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001358
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001354
Iteration 49/1000 | Loss: 0.00001354
Iteration 50/1000 | Loss: 0.00001354
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001351
Iteration 61/1000 | Loss: 0.00001351
Iteration 62/1000 | Loss: 0.00001351
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001350
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001349
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001348
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00001345
Iteration 84/1000 | Loss: 0.00001345
Iteration 85/1000 | Loss: 0.00001345
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001345
Iteration 92/1000 | Loss: 0.00001345
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001340
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001339
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001338
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001338
Iteration 144/1000 | Loss: 0.00001338
Iteration 145/1000 | Loss: 0.00001338
Iteration 146/1000 | Loss: 0.00001338
Iteration 147/1000 | Loss: 0.00001337
Iteration 148/1000 | Loss: 0.00001337
Iteration 149/1000 | Loss: 0.00001337
Iteration 150/1000 | Loss: 0.00001337
Iteration 151/1000 | Loss: 0.00001337
Iteration 152/1000 | Loss: 0.00001337
Iteration 153/1000 | Loss: 0.00001337
Iteration 154/1000 | Loss: 0.00001336
Iteration 155/1000 | Loss: 0.00001336
Iteration 156/1000 | Loss: 0.00001336
Iteration 157/1000 | Loss: 0.00001336
Iteration 158/1000 | Loss: 0.00001336
Iteration 159/1000 | Loss: 0.00001336
Iteration 160/1000 | Loss: 0.00001336
Iteration 161/1000 | Loss: 0.00001336
Iteration 162/1000 | Loss: 0.00001336
Iteration 163/1000 | Loss: 0.00001336
Iteration 164/1000 | Loss: 0.00001336
Iteration 165/1000 | Loss: 0.00001336
Iteration 166/1000 | Loss: 0.00001336
Iteration 167/1000 | Loss: 0.00001336
Iteration 168/1000 | Loss: 0.00001336
Iteration 169/1000 | Loss: 0.00001336
Iteration 170/1000 | Loss: 0.00001335
Iteration 171/1000 | Loss: 0.00001335
Iteration 172/1000 | Loss: 0.00001335
Iteration 173/1000 | Loss: 0.00001335
Iteration 174/1000 | Loss: 0.00001335
Iteration 175/1000 | Loss: 0.00001335
Iteration 176/1000 | Loss: 0.00001335
Iteration 177/1000 | Loss: 0.00001335
Iteration 178/1000 | Loss: 0.00001335
Iteration 179/1000 | Loss: 0.00001335
Iteration 180/1000 | Loss: 0.00001335
Iteration 181/1000 | Loss: 0.00001335
Iteration 182/1000 | Loss: 0.00001335
Iteration 183/1000 | Loss: 0.00001335
Iteration 184/1000 | Loss: 0.00001335
Iteration 185/1000 | Loss: 0.00001335
Iteration 186/1000 | Loss: 0.00001335
Iteration 187/1000 | Loss: 0.00001335
Iteration 188/1000 | Loss: 0.00001334
Iteration 189/1000 | Loss: 0.00001334
Iteration 190/1000 | Loss: 0.00001334
Iteration 191/1000 | Loss: 0.00001334
Iteration 192/1000 | Loss: 0.00001334
Iteration 193/1000 | Loss: 0.00001334
Iteration 194/1000 | Loss: 0.00001334
Iteration 195/1000 | Loss: 0.00001334
Iteration 196/1000 | Loss: 0.00001334
Iteration 197/1000 | Loss: 0.00001334
Iteration 198/1000 | Loss: 0.00001334
Iteration 199/1000 | Loss: 0.00001334
Iteration 200/1000 | Loss: 0.00001334
Iteration 201/1000 | Loss: 0.00001334
Iteration 202/1000 | Loss: 0.00001334
Iteration 203/1000 | Loss: 0.00001334
Iteration 204/1000 | Loss: 0.00001334
Iteration 205/1000 | Loss: 0.00001334
Iteration 206/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.3343908904062118e-05, 1.3343908904062118e-05, 1.3343908904062118e-05, 1.3343908904062118e-05, 1.3343908904062118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3343908904062118e-05

Optimization complete. Final v2v error: 3.0379910469055176 mm

Highest mean error: 3.7018420696258545 mm for frame 78

Lowest mean error: 2.495058298110962 mm for frame 147

Saving results

Total time: 43.5810763835907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893454
Iteration 2/25 | Loss: 0.00135100
Iteration 3/25 | Loss: 0.00126293
Iteration 4/25 | Loss: 0.00124765
Iteration 5/25 | Loss: 0.00124453
Iteration 6/25 | Loss: 0.00124453
Iteration 7/25 | Loss: 0.00124453
Iteration 8/25 | Loss: 0.00124453
Iteration 9/25 | Loss: 0.00124453
Iteration 10/25 | Loss: 0.00124453
Iteration 11/25 | Loss: 0.00124453
Iteration 12/25 | Loss: 0.00124453
Iteration 13/25 | Loss: 0.00124453
Iteration 14/25 | Loss: 0.00124453
Iteration 15/25 | Loss: 0.00124453
Iteration 16/25 | Loss: 0.00124453
Iteration 17/25 | Loss: 0.00124453
Iteration 18/25 | Loss: 0.00124453
Iteration 19/25 | Loss: 0.00124453
Iteration 20/25 | Loss: 0.00124453
Iteration 21/25 | Loss: 0.00124453
Iteration 22/25 | Loss: 0.00124453
Iteration 23/25 | Loss: 0.00124453
Iteration 24/25 | Loss: 0.00124453
Iteration 25/25 | Loss: 0.00124453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32220197
Iteration 2/25 | Loss: 0.00201298
Iteration 3/25 | Loss: 0.00201298
Iteration 4/25 | Loss: 0.00201298
Iteration 5/25 | Loss: 0.00201298
Iteration 6/25 | Loss: 0.00201298
Iteration 7/25 | Loss: 0.00201298
Iteration 8/25 | Loss: 0.00201298
Iteration 9/25 | Loss: 0.00201298
Iteration 10/25 | Loss: 0.00201298
Iteration 11/25 | Loss: 0.00201298
Iteration 12/25 | Loss: 0.00201298
Iteration 13/25 | Loss: 0.00201298
Iteration 14/25 | Loss: 0.00201298
Iteration 15/25 | Loss: 0.00201298
Iteration 16/25 | Loss: 0.00201298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020129780750721693, 0.0020129780750721693, 0.0020129780750721693, 0.0020129780750721693, 0.0020129780750721693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020129780750721693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201298
Iteration 2/1000 | Loss: 0.00001853
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001375
Iteration 6/1000 | Loss: 0.00001318
Iteration 7/1000 | Loss: 0.00001262
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001216
Iteration 10/1000 | Loss: 0.00001200
Iteration 11/1000 | Loss: 0.00001194
Iteration 12/1000 | Loss: 0.00001177
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001167
Iteration 15/1000 | Loss: 0.00001160
Iteration 16/1000 | Loss: 0.00001150
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001138
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001131
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001129
Iteration 23/1000 | Loss: 0.00001128
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001120
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001119
Iteration 31/1000 | Loss: 0.00001118
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001108
Iteration 35/1000 | Loss: 0.00001108
Iteration 36/1000 | Loss: 0.00001108
Iteration 37/1000 | Loss: 0.00001107
Iteration 38/1000 | Loss: 0.00001106
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001104
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001103
Iteration 44/1000 | Loss: 0.00001103
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001101
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001098
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001097
Iteration 60/1000 | Loss: 0.00001097
Iteration 61/1000 | Loss: 0.00001097
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001096
Iteration 66/1000 | Loss: 0.00001096
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001092
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001091
Iteration 96/1000 | Loss: 0.00001091
Iteration 97/1000 | Loss: 0.00001091
Iteration 98/1000 | Loss: 0.00001091
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001090
Iteration 102/1000 | Loss: 0.00001090
Iteration 103/1000 | Loss: 0.00001090
Iteration 104/1000 | Loss: 0.00001090
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001089
Iteration 108/1000 | Loss: 0.00001089
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001086
Iteration 124/1000 | Loss: 0.00001085
Iteration 125/1000 | Loss: 0.00001085
Iteration 126/1000 | Loss: 0.00001084
Iteration 127/1000 | Loss: 0.00001084
Iteration 128/1000 | Loss: 0.00001084
Iteration 129/1000 | Loss: 0.00001084
Iteration 130/1000 | Loss: 0.00001084
Iteration 131/1000 | Loss: 0.00001083
Iteration 132/1000 | Loss: 0.00001083
Iteration 133/1000 | Loss: 0.00001083
Iteration 134/1000 | Loss: 0.00001083
Iteration 135/1000 | Loss: 0.00001083
Iteration 136/1000 | Loss: 0.00001083
Iteration 137/1000 | Loss: 0.00001083
Iteration 138/1000 | Loss: 0.00001083
Iteration 139/1000 | Loss: 0.00001082
Iteration 140/1000 | Loss: 0.00001082
Iteration 141/1000 | Loss: 0.00001082
Iteration 142/1000 | Loss: 0.00001082
Iteration 143/1000 | Loss: 0.00001082
Iteration 144/1000 | Loss: 0.00001082
Iteration 145/1000 | Loss: 0.00001081
Iteration 146/1000 | Loss: 0.00001081
Iteration 147/1000 | Loss: 0.00001081
Iteration 148/1000 | Loss: 0.00001081
Iteration 149/1000 | Loss: 0.00001081
Iteration 150/1000 | Loss: 0.00001081
Iteration 151/1000 | Loss: 0.00001081
Iteration 152/1000 | Loss: 0.00001080
Iteration 153/1000 | Loss: 0.00001080
Iteration 154/1000 | Loss: 0.00001080
Iteration 155/1000 | Loss: 0.00001080
Iteration 156/1000 | Loss: 0.00001080
Iteration 157/1000 | Loss: 0.00001080
Iteration 158/1000 | Loss: 0.00001080
Iteration 159/1000 | Loss: 0.00001080
Iteration 160/1000 | Loss: 0.00001080
Iteration 161/1000 | Loss: 0.00001080
Iteration 162/1000 | Loss: 0.00001080
Iteration 163/1000 | Loss: 0.00001080
Iteration 164/1000 | Loss: 0.00001080
Iteration 165/1000 | Loss: 0.00001080
Iteration 166/1000 | Loss: 0.00001080
Iteration 167/1000 | Loss: 0.00001080
Iteration 168/1000 | Loss: 0.00001080
Iteration 169/1000 | Loss: 0.00001080
Iteration 170/1000 | Loss: 0.00001080
Iteration 171/1000 | Loss: 0.00001080
Iteration 172/1000 | Loss: 0.00001080
Iteration 173/1000 | Loss: 0.00001080
Iteration 174/1000 | Loss: 0.00001080
Iteration 175/1000 | Loss: 0.00001080
Iteration 176/1000 | Loss: 0.00001080
Iteration 177/1000 | Loss: 0.00001080
Iteration 178/1000 | Loss: 0.00001080
Iteration 179/1000 | Loss: 0.00001080
Iteration 180/1000 | Loss: 0.00001080
Iteration 181/1000 | Loss: 0.00001080
Iteration 182/1000 | Loss: 0.00001080
Iteration 183/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.0799202755151782e-05, 1.0799202755151782e-05, 1.0799202755151782e-05, 1.0799202755151782e-05, 1.0799202755151782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0799202755151782e-05

Optimization complete. Final v2v error: 2.8479115962982178 mm

Highest mean error: 3.2866482734680176 mm for frame 147

Lowest mean error: 2.771925449371338 mm for frame 206

Saving results

Total time: 46.62629723548889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064668
Iteration 2/25 | Loss: 0.00269710
Iteration 3/25 | Loss: 0.00261833
Iteration 4/25 | Loss: 0.00160396
Iteration 5/25 | Loss: 0.00142842
Iteration 6/25 | Loss: 0.00141219
Iteration 7/25 | Loss: 0.00139716
Iteration 8/25 | Loss: 0.00138866
Iteration 9/25 | Loss: 0.00138548
Iteration 10/25 | Loss: 0.00138445
Iteration 11/25 | Loss: 0.00138334
Iteration 12/25 | Loss: 0.00138151
Iteration 13/25 | Loss: 0.00138107
Iteration 14/25 | Loss: 0.00138095
Iteration 15/25 | Loss: 0.00138094
Iteration 16/25 | Loss: 0.00138094
Iteration 17/25 | Loss: 0.00138094
Iteration 18/25 | Loss: 0.00138093
Iteration 19/25 | Loss: 0.00138092
Iteration 20/25 | Loss: 0.00138092
Iteration 21/25 | Loss: 0.00138092
Iteration 22/25 | Loss: 0.00138092
Iteration 23/25 | Loss: 0.00138092
Iteration 24/25 | Loss: 0.00138091
Iteration 25/25 | Loss: 0.00138091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90596980
Iteration 2/25 | Loss: 0.00210935
Iteration 3/25 | Loss: 0.00210935
Iteration 4/25 | Loss: 0.00210935
Iteration 5/25 | Loss: 0.00210935
Iteration 6/25 | Loss: 0.00210935
Iteration 7/25 | Loss: 0.00210935
Iteration 8/25 | Loss: 0.00210935
Iteration 9/25 | Loss: 0.00210935
Iteration 10/25 | Loss: 0.00210935
Iteration 11/25 | Loss: 0.00210935
Iteration 12/25 | Loss: 0.00210935
Iteration 13/25 | Loss: 0.00210935
Iteration 14/25 | Loss: 0.00210935
Iteration 15/25 | Loss: 0.00210935
Iteration 16/25 | Loss: 0.00210935
Iteration 17/25 | Loss: 0.00210935
Iteration 18/25 | Loss: 0.00210935
Iteration 19/25 | Loss: 0.00210935
Iteration 20/25 | Loss: 0.00210935
Iteration 21/25 | Loss: 0.00210935
Iteration 22/25 | Loss: 0.00210935
Iteration 23/25 | Loss: 0.00210935
Iteration 24/25 | Loss: 0.00210935
Iteration 25/25 | Loss: 0.00210935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210935
Iteration 2/1000 | Loss: 0.00004591
Iteration 3/1000 | Loss: 0.00003530
Iteration 4/1000 | Loss: 0.00003176
Iteration 5/1000 | Loss: 0.00003021
Iteration 6/1000 | Loss: 0.00002889
Iteration 7/1000 | Loss: 0.00002816
Iteration 8/1000 | Loss: 0.00002755
Iteration 9/1000 | Loss: 0.00002709
Iteration 10/1000 | Loss: 0.00002681
Iteration 11/1000 | Loss: 0.00002647
Iteration 12/1000 | Loss: 0.00038448
Iteration 13/1000 | Loss: 0.00002779
Iteration 14/1000 | Loss: 0.00002611
Iteration 15/1000 | Loss: 0.00002493
Iteration 16/1000 | Loss: 0.00002425
Iteration 17/1000 | Loss: 0.00002391
Iteration 18/1000 | Loss: 0.00002369
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00002355
Iteration 21/1000 | Loss: 0.00002343
Iteration 22/1000 | Loss: 0.00002330
Iteration 23/1000 | Loss: 0.00002324
Iteration 24/1000 | Loss: 0.00002315
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002311
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002307
Iteration 29/1000 | Loss: 0.00002307
Iteration 30/1000 | Loss: 0.00002304
Iteration 31/1000 | Loss: 0.00002303
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00002303
Iteration 34/1000 | Loss: 0.00002301
Iteration 35/1000 | Loss: 0.00002300
Iteration 36/1000 | Loss: 0.00002300
Iteration 37/1000 | Loss: 0.00002300
Iteration 38/1000 | Loss: 0.00002299
Iteration 39/1000 | Loss: 0.00002299
Iteration 40/1000 | Loss: 0.00002299
Iteration 41/1000 | Loss: 0.00002298
Iteration 42/1000 | Loss: 0.00002298
Iteration 43/1000 | Loss: 0.00002298
Iteration 44/1000 | Loss: 0.00002297
Iteration 45/1000 | Loss: 0.00002297
Iteration 46/1000 | Loss: 0.00002297
Iteration 47/1000 | Loss: 0.00002296
Iteration 48/1000 | Loss: 0.00002296
Iteration 49/1000 | Loss: 0.00002296
Iteration 50/1000 | Loss: 0.00002296
Iteration 51/1000 | Loss: 0.00002296
Iteration 52/1000 | Loss: 0.00002296
Iteration 53/1000 | Loss: 0.00002296
Iteration 54/1000 | Loss: 0.00002296
Iteration 55/1000 | Loss: 0.00002296
Iteration 56/1000 | Loss: 0.00002296
Iteration 57/1000 | Loss: 0.00002296
Iteration 58/1000 | Loss: 0.00002295
Iteration 59/1000 | Loss: 0.00002295
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002295
Iteration 62/1000 | Loss: 0.00002295
Iteration 63/1000 | Loss: 0.00002295
Iteration 64/1000 | Loss: 0.00002295
Iteration 65/1000 | Loss: 0.00002295
Iteration 66/1000 | Loss: 0.00002295
Iteration 67/1000 | Loss: 0.00002294
Iteration 68/1000 | Loss: 0.00002294
Iteration 69/1000 | Loss: 0.00002294
Iteration 70/1000 | Loss: 0.00002294
Iteration 71/1000 | Loss: 0.00002294
Iteration 72/1000 | Loss: 0.00002294
Iteration 73/1000 | Loss: 0.00002293
Iteration 74/1000 | Loss: 0.00002293
Iteration 75/1000 | Loss: 0.00002293
Iteration 76/1000 | Loss: 0.00002293
Iteration 77/1000 | Loss: 0.00002293
Iteration 78/1000 | Loss: 0.00002293
Iteration 79/1000 | Loss: 0.00002293
Iteration 80/1000 | Loss: 0.00002293
Iteration 81/1000 | Loss: 0.00002292
Iteration 82/1000 | Loss: 0.00002292
Iteration 83/1000 | Loss: 0.00002292
Iteration 84/1000 | Loss: 0.00002292
Iteration 85/1000 | Loss: 0.00002292
Iteration 86/1000 | Loss: 0.00002292
Iteration 87/1000 | Loss: 0.00002292
Iteration 88/1000 | Loss: 0.00002292
Iteration 89/1000 | Loss: 0.00002292
Iteration 90/1000 | Loss: 0.00002292
Iteration 91/1000 | Loss: 0.00002292
Iteration 92/1000 | Loss: 0.00002292
Iteration 93/1000 | Loss: 0.00002292
Iteration 94/1000 | Loss: 0.00002292
Iteration 95/1000 | Loss: 0.00002291
Iteration 96/1000 | Loss: 0.00002291
Iteration 97/1000 | Loss: 0.00002291
Iteration 98/1000 | Loss: 0.00002291
Iteration 99/1000 | Loss: 0.00002291
Iteration 100/1000 | Loss: 0.00002291
Iteration 101/1000 | Loss: 0.00002291
Iteration 102/1000 | Loss: 0.00002291
Iteration 103/1000 | Loss: 0.00002291
Iteration 104/1000 | Loss: 0.00002291
Iteration 105/1000 | Loss: 0.00002291
Iteration 106/1000 | Loss: 0.00002291
Iteration 107/1000 | Loss: 0.00002291
Iteration 108/1000 | Loss: 0.00002291
Iteration 109/1000 | Loss: 0.00002291
Iteration 110/1000 | Loss: 0.00002291
Iteration 111/1000 | Loss: 0.00002291
Iteration 112/1000 | Loss: 0.00002291
Iteration 113/1000 | Loss: 0.00002291
Iteration 114/1000 | Loss: 0.00002291
Iteration 115/1000 | Loss: 0.00002291
Iteration 116/1000 | Loss: 0.00002291
Iteration 117/1000 | Loss: 0.00002291
Iteration 118/1000 | Loss: 0.00002291
Iteration 119/1000 | Loss: 0.00002291
Iteration 120/1000 | Loss: 0.00002291
Iteration 121/1000 | Loss: 0.00002291
Iteration 122/1000 | Loss: 0.00002291
Iteration 123/1000 | Loss: 0.00002291
Iteration 124/1000 | Loss: 0.00002291
Iteration 125/1000 | Loss: 0.00002291
Iteration 126/1000 | Loss: 0.00002291
Iteration 127/1000 | Loss: 0.00002291
Iteration 128/1000 | Loss: 0.00002290
Iteration 129/1000 | Loss: 0.00002290
Iteration 130/1000 | Loss: 0.00002290
Iteration 131/1000 | Loss: 0.00002290
Iteration 132/1000 | Loss: 0.00002290
Iteration 133/1000 | Loss: 0.00002290
Iteration 134/1000 | Loss: 0.00002290
Iteration 135/1000 | Loss: 0.00002290
Iteration 136/1000 | Loss: 0.00002290
Iteration 137/1000 | Loss: 0.00002290
Iteration 138/1000 | Loss: 0.00002290
Iteration 139/1000 | Loss: 0.00002290
Iteration 140/1000 | Loss: 0.00002290
Iteration 141/1000 | Loss: 0.00002290
Iteration 142/1000 | Loss: 0.00002290
Iteration 143/1000 | Loss: 0.00002290
Iteration 144/1000 | Loss: 0.00002290
Iteration 145/1000 | Loss: 0.00002290
Iteration 146/1000 | Loss: 0.00002290
Iteration 147/1000 | Loss: 0.00002290
Iteration 148/1000 | Loss: 0.00002290
Iteration 149/1000 | Loss: 0.00002290
Iteration 150/1000 | Loss: 0.00002290
Iteration 151/1000 | Loss: 0.00002290
Iteration 152/1000 | Loss: 0.00002290
Iteration 153/1000 | Loss: 0.00002290
Iteration 154/1000 | Loss: 0.00002290
Iteration 155/1000 | Loss: 0.00002290
Iteration 156/1000 | Loss: 0.00002290
Iteration 157/1000 | Loss: 0.00002290
Iteration 158/1000 | Loss: 0.00002290
Iteration 159/1000 | Loss: 0.00002290
Iteration 160/1000 | Loss: 0.00002290
Iteration 161/1000 | Loss: 0.00002290
Iteration 162/1000 | Loss: 0.00002290
Iteration 163/1000 | Loss: 0.00002290
Iteration 164/1000 | Loss: 0.00002290
Iteration 165/1000 | Loss: 0.00002290
Iteration 166/1000 | Loss: 0.00002290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.2904680008650757e-05, 2.2904680008650757e-05, 2.2904680008650757e-05, 2.2904680008650757e-05, 2.2904680008650757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2904680008650757e-05

Optimization complete. Final v2v error: 3.9322361946105957 mm

Highest mean error: 4.6623759269714355 mm for frame 120

Lowest mean error: 3.2022218704223633 mm for frame 27

Saving results

Total time: 63.82265758514404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593843
Iteration 2/25 | Loss: 0.00130469
Iteration 3/25 | Loss: 0.00123918
Iteration 4/25 | Loss: 0.00122823
Iteration 5/25 | Loss: 0.00122464
Iteration 6/25 | Loss: 0.00122383
Iteration 7/25 | Loss: 0.00122383
Iteration 8/25 | Loss: 0.00122383
Iteration 9/25 | Loss: 0.00122383
Iteration 10/25 | Loss: 0.00122383
Iteration 11/25 | Loss: 0.00122383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001223828294314444, 0.001223828294314444, 0.001223828294314444, 0.001223828294314444, 0.001223828294314444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001223828294314444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98005223
Iteration 2/25 | Loss: 0.00218967
Iteration 3/25 | Loss: 0.00218967
Iteration 4/25 | Loss: 0.00218967
Iteration 5/25 | Loss: 0.00218967
Iteration 6/25 | Loss: 0.00218967
Iteration 7/25 | Loss: 0.00218967
Iteration 8/25 | Loss: 0.00218967
Iteration 9/25 | Loss: 0.00218967
Iteration 10/25 | Loss: 0.00218967
Iteration 11/25 | Loss: 0.00218967
Iteration 12/25 | Loss: 0.00218967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0021896669641137123, 0.0021896669641137123, 0.0021896669641137123, 0.0021896669641137123, 0.0021896669641137123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021896669641137123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218967
Iteration 2/1000 | Loss: 0.00002259
Iteration 3/1000 | Loss: 0.00001644
Iteration 4/1000 | Loss: 0.00001377
Iteration 5/1000 | Loss: 0.00001277
Iteration 6/1000 | Loss: 0.00001218
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001098
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001058
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001042
Iteration 14/1000 | Loss: 0.00001041
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001033
Iteration 19/1000 | Loss: 0.00001033
Iteration 20/1000 | Loss: 0.00001023
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001012
Iteration 23/1000 | Loss: 0.00001011
Iteration 24/1000 | Loss: 0.00001009
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000999
Iteration 33/1000 | Loss: 0.00000997
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000996
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000987
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000977
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000974
Iteration 59/1000 | Loss: 0.00000974
Iteration 60/1000 | Loss: 0.00000973
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000971
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000970
Iteration 66/1000 | Loss: 0.00000970
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000969
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000965
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000964
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000963
Iteration 83/1000 | Loss: 0.00000963
Iteration 84/1000 | Loss: 0.00000963
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000963
Iteration 88/1000 | Loss: 0.00000963
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000962
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000962
Iteration 93/1000 | Loss: 0.00000962
Iteration 94/1000 | Loss: 0.00000962
Iteration 95/1000 | Loss: 0.00000962
Iteration 96/1000 | Loss: 0.00000962
Iteration 97/1000 | Loss: 0.00000962
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000962
Iteration 100/1000 | Loss: 0.00000962
Iteration 101/1000 | Loss: 0.00000962
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000961
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000961
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000960
Iteration 114/1000 | Loss: 0.00000960
Iteration 115/1000 | Loss: 0.00000960
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000959
Iteration 118/1000 | Loss: 0.00000959
Iteration 119/1000 | Loss: 0.00000959
Iteration 120/1000 | Loss: 0.00000959
Iteration 121/1000 | Loss: 0.00000959
Iteration 122/1000 | Loss: 0.00000959
Iteration 123/1000 | Loss: 0.00000959
Iteration 124/1000 | Loss: 0.00000959
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000958
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000957
Iteration 129/1000 | Loss: 0.00000957
Iteration 130/1000 | Loss: 0.00000957
Iteration 131/1000 | Loss: 0.00000957
Iteration 132/1000 | Loss: 0.00000957
Iteration 133/1000 | Loss: 0.00000957
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000957
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000957
Iteration 138/1000 | Loss: 0.00000957
Iteration 139/1000 | Loss: 0.00000957
Iteration 140/1000 | Loss: 0.00000957
Iteration 141/1000 | Loss: 0.00000957
Iteration 142/1000 | Loss: 0.00000957
Iteration 143/1000 | Loss: 0.00000957
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000956
Iteration 147/1000 | Loss: 0.00000956
Iteration 148/1000 | Loss: 0.00000956
Iteration 149/1000 | Loss: 0.00000956
Iteration 150/1000 | Loss: 0.00000956
Iteration 151/1000 | Loss: 0.00000956
Iteration 152/1000 | Loss: 0.00000956
Iteration 153/1000 | Loss: 0.00000956
Iteration 154/1000 | Loss: 0.00000956
Iteration 155/1000 | Loss: 0.00000955
Iteration 156/1000 | Loss: 0.00000955
Iteration 157/1000 | Loss: 0.00000955
Iteration 158/1000 | Loss: 0.00000955
Iteration 159/1000 | Loss: 0.00000955
Iteration 160/1000 | Loss: 0.00000955
Iteration 161/1000 | Loss: 0.00000955
Iteration 162/1000 | Loss: 0.00000955
Iteration 163/1000 | Loss: 0.00000955
Iteration 164/1000 | Loss: 0.00000955
Iteration 165/1000 | Loss: 0.00000955
Iteration 166/1000 | Loss: 0.00000955
Iteration 167/1000 | Loss: 0.00000954
Iteration 168/1000 | Loss: 0.00000954
Iteration 169/1000 | Loss: 0.00000954
Iteration 170/1000 | Loss: 0.00000954
Iteration 171/1000 | Loss: 0.00000954
Iteration 172/1000 | Loss: 0.00000954
Iteration 173/1000 | Loss: 0.00000954
Iteration 174/1000 | Loss: 0.00000954
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000954
Iteration 178/1000 | Loss: 0.00000953
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000953
Iteration 181/1000 | Loss: 0.00000953
Iteration 182/1000 | Loss: 0.00000953
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000953
Iteration 185/1000 | Loss: 0.00000953
Iteration 186/1000 | Loss: 0.00000953
Iteration 187/1000 | Loss: 0.00000953
Iteration 188/1000 | Loss: 0.00000953
Iteration 189/1000 | Loss: 0.00000953
Iteration 190/1000 | Loss: 0.00000953
Iteration 191/1000 | Loss: 0.00000953
Iteration 192/1000 | Loss: 0.00000952
Iteration 193/1000 | Loss: 0.00000952
Iteration 194/1000 | Loss: 0.00000952
Iteration 195/1000 | Loss: 0.00000952
Iteration 196/1000 | Loss: 0.00000952
Iteration 197/1000 | Loss: 0.00000952
Iteration 198/1000 | Loss: 0.00000952
Iteration 199/1000 | Loss: 0.00000952
Iteration 200/1000 | Loss: 0.00000952
Iteration 201/1000 | Loss: 0.00000952
Iteration 202/1000 | Loss: 0.00000952
Iteration 203/1000 | Loss: 0.00000952
Iteration 204/1000 | Loss: 0.00000952
Iteration 205/1000 | Loss: 0.00000952
Iteration 206/1000 | Loss: 0.00000952
Iteration 207/1000 | Loss: 0.00000952
Iteration 208/1000 | Loss: 0.00000952
Iteration 209/1000 | Loss: 0.00000952
Iteration 210/1000 | Loss: 0.00000952
Iteration 211/1000 | Loss: 0.00000952
Iteration 212/1000 | Loss: 0.00000952
Iteration 213/1000 | Loss: 0.00000952
Iteration 214/1000 | Loss: 0.00000952
Iteration 215/1000 | Loss: 0.00000952
Iteration 216/1000 | Loss: 0.00000952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [9.522058462607674e-06, 9.522058462607674e-06, 9.522058462607674e-06, 9.522058462607674e-06, 9.522058462607674e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.522058462607674e-06

Optimization complete. Final v2v error: 2.7023870944976807 mm

Highest mean error: 2.918696641921997 mm for frame 61

Lowest mean error: 2.5524468421936035 mm for frame 26

Saving results

Total time: 41.67709946632385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592607
Iteration 2/25 | Loss: 0.00157991
Iteration 3/25 | Loss: 0.00139681
Iteration 4/25 | Loss: 0.00138965
Iteration 5/25 | Loss: 0.00138965
Iteration 6/25 | Loss: 0.00138965
Iteration 7/25 | Loss: 0.00138965
Iteration 8/25 | Loss: 0.00138965
Iteration 9/25 | Loss: 0.00138965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0013896459713578224, 0.0013896459713578224, 0.0013896459713578224, 0.0013896459713578224, 0.0013896459713578224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013896459713578224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89070195
Iteration 2/25 | Loss: 0.00147795
Iteration 3/25 | Loss: 0.00147794
Iteration 4/25 | Loss: 0.00147794
Iteration 5/25 | Loss: 0.00147794
Iteration 6/25 | Loss: 0.00147794
Iteration 7/25 | Loss: 0.00147794
Iteration 8/25 | Loss: 0.00147794
Iteration 9/25 | Loss: 0.00147794
Iteration 10/25 | Loss: 0.00147794
Iteration 11/25 | Loss: 0.00147794
Iteration 12/25 | Loss: 0.00147794
Iteration 13/25 | Loss: 0.00147794
Iteration 14/25 | Loss: 0.00147794
Iteration 15/25 | Loss: 0.00147794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014779422199353576, 0.0014779422199353576, 0.0014779422199353576, 0.0014779422199353576, 0.0014779422199353576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014779422199353576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147794
Iteration 2/1000 | Loss: 0.00003829
Iteration 3/1000 | Loss: 0.00002933
Iteration 4/1000 | Loss: 0.00002685
Iteration 5/1000 | Loss: 0.00002551
Iteration 6/1000 | Loss: 0.00002481
Iteration 7/1000 | Loss: 0.00002422
Iteration 8/1000 | Loss: 0.00002370
Iteration 9/1000 | Loss: 0.00002327
Iteration 10/1000 | Loss: 0.00002287
Iteration 11/1000 | Loss: 0.00002255
Iteration 12/1000 | Loss: 0.00002228
Iteration 13/1000 | Loss: 0.00002207
Iteration 14/1000 | Loss: 0.00002183
Iteration 15/1000 | Loss: 0.00002177
Iteration 16/1000 | Loss: 0.00002173
Iteration 17/1000 | Loss: 0.00002172
Iteration 18/1000 | Loss: 0.00002172
Iteration 19/1000 | Loss: 0.00002172
Iteration 20/1000 | Loss: 0.00002167
Iteration 21/1000 | Loss: 0.00002167
Iteration 22/1000 | Loss: 0.00002166
Iteration 23/1000 | Loss: 0.00002166
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002158
Iteration 26/1000 | Loss: 0.00002151
Iteration 27/1000 | Loss: 0.00002151
Iteration 28/1000 | Loss: 0.00002145
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002144
Iteration 32/1000 | Loss: 0.00002144
Iteration 33/1000 | Loss: 0.00002140
Iteration 34/1000 | Loss: 0.00002140
Iteration 35/1000 | Loss: 0.00002140
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00002139
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002135
Iteration 45/1000 | Loss: 0.00002135
Iteration 46/1000 | Loss: 0.00002134
Iteration 47/1000 | Loss: 0.00002134
Iteration 48/1000 | Loss: 0.00002133
Iteration 49/1000 | Loss: 0.00002133
Iteration 50/1000 | Loss: 0.00002133
Iteration 51/1000 | Loss: 0.00002133
Iteration 52/1000 | Loss: 0.00002133
Iteration 53/1000 | Loss: 0.00002133
Iteration 54/1000 | Loss: 0.00002133
Iteration 55/1000 | Loss: 0.00002133
Iteration 56/1000 | Loss: 0.00002133
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002132
Iteration 60/1000 | Loss: 0.00002132
Iteration 61/1000 | Loss: 0.00002132
Iteration 62/1000 | Loss: 0.00002132
Iteration 63/1000 | Loss: 0.00002132
Iteration 64/1000 | Loss: 0.00002132
Iteration 65/1000 | Loss: 0.00002131
Iteration 66/1000 | Loss: 0.00002130
Iteration 67/1000 | Loss: 0.00002130
Iteration 68/1000 | Loss: 0.00002130
Iteration 69/1000 | Loss: 0.00002130
Iteration 70/1000 | Loss: 0.00002130
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00002129
Iteration 74/1000 | Loss: 0.00002129
Iteration 75/1000 | Loss: 0.00002129
Iteration 76/1000 | Loss: 0.00002129
Iteration 77/1000 | Loss: 0.00002129
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002128
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002126
Iteration 85/1000 | Loss: 0.00002126
Iteration 86/1000 | Loss: 0.00002126
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002126
Iteration 92/1000 | Loss: 0.00002126
Iteration 93/1000 | Loss: 0.00002126
Iteration 94/1000 | Loss: 0.00002126
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002125
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002124
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00002124
Iteration 113/1000 | Loss: 0.00002124
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002123
Iteration 118/1000 | Loss: 0.00002123
Iteration 119/1000 | Loss: 0.00002123
Iteration 120/1000 | Loss: 0.00002123
Iteration 121/1000 | Loss: 0.00002123
Iteration 122/1000 | Loss: 0.00002123
Iteration 123/1000 | Loss: 0.00002123
Iteration 124/1000 | Loss: 0.00002123
Iteration 125/1000 | Loss: 0.00002123
Iteration 126/1000 | Loss: 0.00002123
Iteration 127/1000 | Loss: 0.00002123
Iteration 128/1000 | Loss: 0.00002123
Iteration 129/1000 | Loss: 0.00002123
Iteration 130/1000 | Loss: 0.00002123
Iteration 131/1000 | Loss: 0.00002123
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002123
Iteration 134/1000 | Loss: 0.00002123
Iteration 135/1000 | Loss: 0.00002123
Iteration 136/1000 | Loss: 0.00002123
Iteration 137/1000 | Loss: 0.00002123
Iteration 138/1000 | Loss: 0.00002123
Iteration 139/1000 | Loss: 0.00002123
Iteration 140/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.1233376173768193e-05, 2.1233376173768193e-05, 2.1233376173768193e-05, 2.1233376173768193e-05, 2.1233376173768193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1233376173768193e-05

Optimization complete. Final v2v error: 3.9194979667663574 mm

Highest mean error: 3.982785224914551 mm for frame 47

Lowest mean error: 3.866102457046509 mm for frame 193

Saving results

Total time: 42.78198456764221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957461
Iteration 2/25 | Loss: 0.00237810
Iteration 3/25 | Loss: 0.00177029
Iteration 4/25 | Loss: 0.00169457
Iteration 5/25 | Loss: 0.00159155
Iteration 6/25 | Loss: 0.00154739
Iteration 7/25 | Loss: 0.00150244
Iteration 8/25 | Loss: 0.00147819
Iteration 9/25 | Loss: 0.00144872
Iteration 10/25 | Loss: 0.00141587
Iteration 11/25 | Loss: 0.00140469
Iteration 12/25 | Loss: 0.00140187
Iteration 13/25 | Loss: 0.00138660
Iteration 14/25 | Loss: 0.00138585
Iteration 15/25 | Loss: 0.00138443
Iteration 16/25 | Loss: 0.00137829
Iteration 17/25 | Loss: 0.00136631
Iteration 18/25 | Loss: 0.00136417
Iteration 19/25 | Loss: 0.00136349
Iteration 20/25 | Loss: 0.00136332
Iteration 21/25 | Loss: 0.00136323
Iteration 22/25 | Loss: 0.00136322
Iteration 23/25 | Loss: 0.00136322
Iteration 24/25 | Loss: 0.00136322
Iteration 25/25 | Loss: 0.00136322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17326140
Iteration 2/25 | Loss: 0.00187511
Iteration 3/25 | Loss: 0.00187511
Iteration 4/25 | Loss: 0.00187511
Iteration 5/25 | Loss: 0.00187511
Iteration 6/25 | Loss: 0.00187511
Iteration 7/25 | Loss: 0.00187511
Iteration 8/25 | Loss: 0.00187511
Iteration 9/25 | Loss: 0.00187511
Iteration 10/25 | Loss: 0.00187511
Iteration 11/25 | Loss: 0.00187511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018751061288639903, 0.0018751061288639903, 0.0018751061288639903, 0.0018751061288639903, 0.0018751061288639903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018751061288639903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187511
Iteration 2/1000 | Loss: 0.00006102
Iteration 3/1000 | Loss: 0.00004231
Iteration 4/1000 | Loss: 0.00003486
Iteration 5/1000 | Loss: 0.00003140
Iteration 6/1000 | Loss: 0.00002949
Iteration 7/1000 | Loss: 0.00002853
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002714
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002630
Iteration 12/1000 | Loss: 0.00002604
Iteration 13/1000 | Loss: 0.00002580
Iteration 14/1000 | Loss: 0.00002561
Iteration 15/1000 | Loss: 0.00002540
Iteration 16/1000 | Loss: 0.00002523
Iteration 17/1000 | Loss: 0.00002515
Iteration 18/1000 | Loss: 0.00002512
Iteration 19/1000 | Loss: 0.00002510
Iteration 20/1000 | Loss: 0.00002507
Iteration 21/1000 | Loss: 0.00002507
Iteration 22/1000 | Loss: 0.00002505
Iteration 23/1000 | Loss: 0.00002501
Iteration 24/1000 | Loss: 0.00002498
Iteration 25/1000 | Loss: 0.00002498
Iteration 26/1000 | Loss: 0.00002496
Iteration 27/1000 | Loss: 0.00002495
Iteration 28/1000 | Loss: 0.00002495
Iteration 29/1000 | Loss: 0.00002493
Iteration 30/1000 | Loss: 0.00002493
Iteration 31/1000 | Loss: 0.00002492
Iteration 32/1000 | Loss: 0.00002491
Iteration 33/1000 | Loss: 0.00002490
Iteration 34/1000 | Loss: 0.00002485
Iteration 35/1000 | Loss: 0.00002485
Iteration 36/1000 | Loss: 0.00002483
Iteration 37/1000 | Loss: 0.00002483
Iteration 38/1000 | Loss: 0.00002483
Iteration 39/1000 | Loss: 0.00002483
Iteration 40/1000 | Loss: 0.00002482
Iteration 41/1000 | Loss: 0.00002482
Iteration 42/1000 | Loss: 0.00002481
Iteration 43/1000 | Loss: 0.00002481
Iteration 44/1000 | Loss: 0.00002480
Iteration 45/1000 | Loss: 0.00002480
Iteration 46/1000 | Loss: 0.00002479
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002477
Iteration 49/1000 | Loss: 0.00002477
Iteration 50/1000 | Loss: 0.00002477
Iteration 51/1000 | Loss: 0.00002477
Iteration 52/1000 | Loss: 0.00002477
Iteration 53/1000 | Loss: 0.00002477
Iteration 54/1000 | Loss: 0.00002477
Iteration 55/1000 | Loss: 0.00002477
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002476
Iteration 58/1000 | Loss: 0.00002476
Iteration 59/1000 | Loss: 0.00002476
Iteration 60/1000 | Loss: 0.00002476
Iteration 61/1000 | Loss: 0.00002476
Iteration 62/1000 | Loss: 0.00002475
Iteration 63/1000 | Loss: 0.00002475
Iteration 64/1000 | Loss: 0.00002475
Iteration 65/1000 | Loss: 0.00002474
Iteration 66/1000 | Loss: 0.00002474
Iteration 67/1000 | Loss: 0.00002474
Iteration 68/1000 | Loss: 0.00002474
Iteration 69/1000 | Loss: 0.00002474
Iteration 70/1000 | Loss: 0.00002474
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00002474
Iteration 73/1000 | Loss: 0.00002473
Iteration 74/1000 | Loss: 0.00002473
Iteration 75/1000 | Loss: 0.00002473
Iteration 76/1000 | Loss: 0.00002473
Iteration 77/1000 | Loss: 0.00002473
Iteration 78/1000 | Loss: 0.00002473
Iteration 79/1000 | Loss: 0.00002473
Iteration 80/1000 | Loss: 0.00002473
Iteration 81/1000 | Loss: 0.00002472
Iteration 82/1000 | Loss: 0.00002472
Iteration 83/1000 | Loss: 0.00002472
Iteration 84/1000 | Loss: 0.00002471
Iteration 85/1000 | Loss: 0.00002471
Iteration 86/1000 | Loss: 0.00002471
Iteration 87/1000 | Loss: 0.00002471
Iteration 88/1000 | Loss: 0.00002471
Iteration 89/1000 | Loss: 0.00002470
Iteration 90/1000 | Loss: 0.00002470
Iteration 91/1000 | Loss: 0.00002470
Iteration 92/1000 | Loss: 0.00002469
Iteration 93/1000 | Loss: 0.00002469
Iteration 94/1000 | Loss: 0.00002469
Iteration 95/1000 | Loss: 0.00002469
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002468
Iteration 98/1000 | Loss: 0.00002468
Iteration 99/1000 | Loss: 0.00002468
Iteration 100/1000 | Loss: 0.00002468
Iteration 101/1000 | Loss: 0.00002468
Iteration 102/1000 | Loss: 0.00002468
Iteration 103/1000 | Loss: 0.00002468
Iteration 104/1000 | Loss: 0.00002468
Iteration 105/1000 | Loss: 0.00002468
Iteration 106/1000 | Loss: 0.00002468
Iteration 107/1000 | Loss: 0.00002468
Iteration 108/1000 | Loss: 0.00002468
Iteration 109/1000 | Loss: 0.00002467
Iteration 110/1000 | Loss: 0.00002467
Iteration 111/1000 | Loss: 0.00002467
Iteration 112/1000 | Loss: 0.00002467
Iteration 113/1000 | Loss: 0.00002467
Iteration 114/1000 | Loss: 0.00002467
Iteration 115/1000 | Loss: 0.00002467
Iteration 116/1000 | Loss: 0.00002467
Iteration 117/1000 | Loss: 0.00002467
Iteration 118/1000 | Loss: 0.00002467
Iteration 119/1000 | Loss: 0.00002467
Iteration 120/1000 | Loss: 0.00002467
Iteration 121/1000 | Loss: 0.00002467
Iteration 122/1000 | Loss: 0.00002467
Iteration 123/1000 | Loss: 0.00002467
Iteration 124/1000 | Loss: 0.00002467
Iteration 125/1000 | Loss: 0.00002467
Iteration 126/1000 | Loss: 0.00002467
Iteration 127/1000 | Loss: 0.00002467
Iteration 128/1000 | Loss: 0.00002467
Iteration 129/1000 | Loss: 0.00002467
Iteration 130/1000 | Loss: 0.00002467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.4669488993822597e-05, 2.4669488993822597e-05, 2.4669488993822597e-05, 2.4669488993822597e-05, 2.4669488993822597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4669488993822597e-05

Optimization complete. Final v2v error: 3.7760705947875977 mm

Highest mean error: 11.591874122619629 mm for frame 44

Lowest mean error: 3.409426212310791 mm for frame 55

Saving results

Total time: 67.07032537460327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912466
Iteration 2/25 | Loss: 0.00148810
Iteration 3/25 | Loss: 0.00130585
Iteration 4/25 | Loss: 0.00129037
Iteration 5/25 | Loss: 0.00128624
Iteration 6/25 | Loss: 0.00128531
Iteration 7/25 | Loss: 0.00128531
Iteration 8/25 | Loss: 0.00128531
Iteration 9/25 | Loss: 0.00128531
Iteration 10/25 | Loss: 0.00128531
Iteration 11/25 | Loss: 0.00128531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012853145599365234, 0.0012853145599365234, 0.0012853145599365234, 0.0012853145599365234, 0.0012853145599365234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012853145599365234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34081876
Iteration 2/25 | Loss: 0.00280246
Iteration 3/25 | Loss: 0.00280246
Iteration 4/25 | Loss: 0.00280246
Iteration 5/25 | Loss: 0.00280246
Iteration 6/25 | Loss: 0.00280246
Iteration 7/25 | Loss: 0.00280246
Iteration 8/25 | Loss: 0.00280246
Iteration 9/25 | Loss: 0.00280246
Iteration 10/25 | Loss: 0.00280246
Iteration 11/25 | Loss: 0.00280246
Iteration 12/25 | Loss: 0.00280246
Iteration 13/25 | Loss: 0.00280246
Iteration 14/25 | Loss: 0.00280246
Iteration 15/25 | Loss: 0.00280246
Iteration 16/25 | Loss: 0.00280246
Iteration 17/25 | Loss: 0.00280246
Iteration 18/25 | Loss: 0.00280246
Iteration 19/25 | Loss: 0.00280246
Iteration 20/25 | Loss: 0.00280246
Iteration 21/25 | Loss: 0.00280246
Iteration 22/25 | Loss: 0.00280246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0028024581260979176, 0.0028024581260979176, 0.0028024581260979176, 0.0028024581260979176, 0.0028024581260979176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028024581260979176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280246
Iteration 2/1000 | Loss: 0.00004769
Iteration 3/1000 | Loss: 0.00003074
Iteration 4/1000 | Loss: 0.00002366
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001860
Iteration 9/1000 | Loss: 0.00001832
Iteration 10/1000 | Loss: 0.00001804
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001727
Iteration 15/1000 | Loss: 0.00001709
Iteration 16/1000 | Loss: 0.00001705
Iteration 17/1000 | Loss: 0.00001690
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001672
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001667
Iteration 23/1000 | Loss: 0.00001665
Iteration 24/1000 | Loss: 0.00001665
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001664
Iteration 27/1000 | Loss: 0.00001664
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001663
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00001660
Iteration 33/1000 | Loss: 0.00001659
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001655
Iteration 38/1000 | Loss: 0.00001649
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001642
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001640
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001638
Iteration 46/1000 | Loss: 0.00001637
Iteration 47/1000 | Loss: 0.00001635
Iteration 48/1000 | Loss: 0.00001634
Iteration 49/1000 | Loss: 0.00001634
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001632
Iteration 56/1000 | Loss: 0.00001632
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001621
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001621
Iteration 76/1000 | Loss: 0.00001620
Iteration 77/1000 | Loss: 0.00001620
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001620
Iteration 80/1000 | Loss: 0.00001619
Iteration 81/1000 | Loss: 0.00001619
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001618
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001614
Iteration 93/1000 | Loss: 0.00001614
Iteration 94/1000 | Loss: 0.00001614
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001612
Iteration 103/1000 | Loss: 0.00001611
Iteration 104/1000 | Loss: 0.00001611
Iteration 105/1000 | Loss: 0.00001611
Iteration 106/1000 | Loss: 0.00001611
Iteration 107/1000 | Loss: 0.00001611
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001610
Iteration 110/1000 | Loss: 0.00001610
Iteration 111/1000 | Loss: 0.00001610
Iteration 112/1000 | Loss: 0.00001610
Iteration 113/1000 | Loss: 0.00001610
Iteration 114/1000 | Loss: 0.00001610
Iteration 115/1000 | Loss: 0.00001610
Iteration 116/1000 | Loss: 0.00001609
Iteration 117/1000 | Loss: 0.00001609
Iteration 118/1000 | Loss: 0.00001608
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001607
Iteration 125/1000 | Loss: 0.00001606
Iteration 126/1000 | Loss: 0.00001606
Iteration 127/1000 | Loss: 0.00001605
Iteration 128/1000 | Loss: 0.00001605
Iteration 129/1000 | Loss: 0.00001605
Iteration 130/1000 | Loss: 0.00001604
Iteration 131/1000 | Loss: 0.00001604
Iteration 132/1000 | Loss: 0.00001604
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001602
Iteration 142/1000 | Loss: 0.00001602
Iteration 143/1000 | Loss: 0.00001602
Iteration 144/1000 | Loss: 0.00001601
Iteration 145/1000 | Loss: 0.00001601
Iteration 146/1000 | Loss: 0.00001601
Iteration 147/1000 | Loss: 0.00001601
Iteration 148/1000 | Loss: 0.00001601
Iteration 149/1000 | Loss: 0.00001601
Iteration 150/1000 | Loss: 0.00001601
Iteration 151/1000 | Loss: 0.00001600
Iteration 152/1000 | Loss: 0.00001600
Iteration 153/1000 | Loss: 0.00001600
Iteration 154/1000 | Loss: 0.00001600
Iteration 155/1000 | Loss: 0.00001600
Iteration 156/1000 | Loss: 0.00001600
Iteration 157/1000 | Loss: 0.00001600
Iteration 158/1000 | Loss: 0.00001600
Iteration 159/1000 | Loss: 0.00001599
Iteration 160/1000 | Loss: 0.00001599
Iteration 161/1000 | Loss: 0.00001599
Iteration 162/1000 | Loss: 0.00001599
Iteration 163/1000 | Loss: 0.00001598
Iteration 164/1000 | Loss: 0.00001598
Iteration 165/1000 | Loss: 0.00001598
Iteration 166/1000 | Loss: 0.00001598
Iteration 167/1000 | Loss: 0.00001598
Iteration 168/1000 | Loss: 0.00001597
Iteration 169/1000 | Loss: 0.00001597
Iteration 170/1000 | Loss: 0.00001597
Iteration 171/1000 | Loss: 0.00001597
Iteration 172/1000 | Loss: 0.00001597
Iteration 173/1000 | Loss: 0.00001597
Iteration 174/1000 | Loss: 0.00001597
Iteration 175/1000 | Loss: 0.00001597
Iteration 176/1000 | Loss: 0.00001597
Iteration 177/1000 | Loss: 0.00001597
Iteration 178/1000 | Loss: 0.00001597
Iteration 179/1000 | Loss: 0.00001597
Iteration 180/1000 | Loss: 0.00001597
Iteration 181/1000 | Loss: 0.00001597
Iteration 182/1000 | Loss: 0.00001597
Iteration 183/1000 | Loss: 0.00001597
Iteration 184/1000 | Loss: 0.00001597
Iteration 185/1000 | Loss: 0.00001597
Iteration 186/1000 | Loss: 0.00001597
Iteration 187/1000 | Loss: 0.00001597
Iteration 188/1000 | Loss: 0.00001597
Iteration 189/1000 | Loss: 0.00001597
Iteration 190/1000 | Loss: 0.00001597
Iteration 191/1000 | Loss: 0.00001597
Iteration 192/1000 | Loss: 0.00001597
Iteration 193/1000 | Loss: 0.00001597
Iteration 194/1000 | Loss: 0.00001597
Iteration 195/1000 | Loss: 0.00001597
Iteration 196/1000 | Loss: 0.00001597
Iteration 197/1000 | Loss: 0.00001597
Iteration 198/1000 | Loss: 0.00001597
Iteration 199/1000 | Loss: 0.00001597
Iteration 200/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.5972638720995747e-05, 1.5972638720995747e-05, 1.5972638720995747e-05, 1.5972638720995747e-05, 1.5972638720995747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5972638720995747e-05

Optimization complete. Final v2v error: 3.4063642024993896 mm

Highest mean error: 3.73091983795166 mm for frame 166

Lowest mean error: 2.9651992321014404 mm for frame 0

Saving results

Total time: 48.48263716697693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848061
Iteration 2/25 | Loss: 0.00131462
Iteration 3/25 | Loss: 0.00123987
Iteration 4/25 | Loss: 0.00123134
Iteration 5/25 | Loss: 0.00122888
Iteration 6/25 | Loss: 0.00122825
Iteration 7/25 | Loss: 0.00122825
Iteration 8/25 | Loss: 0.00122825
Iteration 9/25 | Loss: 0.00122825
Iteration 10/25 | Loss: 0.00122825
Iteration 11/25 | Loss: 0.00122825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012282462557777762, 0.0012282462557777762, 0.0012282462557777762, 0.0012282462557777762, 0.0012282462557777762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012282462557777762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88551152
Iteration 2/25 | Loss: 0.00209143
Iteration 3/25 | Loss: 0.00209143
Iteration 4/25 | Loss: 0.00209142
Iteration 5/25 | Loss: 0.00209142
Iteration 6/25 | Loss: 0.00209142
Iteration 7/25 | Loss: 0.00209142
Iteration 8/25 | Loss: 0.00209142
Iteration 9/25 | Loss: 0.00209142
Iteration 10/25 | Loss: 0.00209142
Iteration 11/25 | Loss: 0.00209142
Iteration 12/25 | Loss: 0.00209142
Iteration 13/25 | Loss: 0.00209142
Iteration 14/25 | Loss: 0.00209142
Iteration 15/25 | Loss: 0.00209142
Iteration 16/25 | Loss: 0.00209142
Iteration 17/25 | Loss: 0.00209142
Iteration 18/25 | Loss: 0.00209142
Iteration 19/25 | Loss: 0.00209142
Iteration 20/25 | Loss: 0.00209142
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002091420814394951, 0.002091420814394951, 0.002091420814394951, 0.002091420814394951, 0.002091420814394951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002091420814394951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209142
Iteration 2/1000 | Loss: 0.00002369
Iteration 3/1000 | Loss: 0.00001655
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001278
Iteration 6/1000 | Loss: 0.00001216
Iteration 7/1000 | Loss: 0.00001158
Iteration 8/1000 | Loss: 0.00001120
Iteration 9/1000 | Loss: 0.00001100
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001052
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001034
Iteration 15/1000 | Loss: 0.00001022
Iteration 16/1000 | Loss: 0.00001018
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00001018
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001017
Iteration 22/1000 | Loss: 0.00001011
Iteration 23/1000 | Loss: 0.00001010
Iteration 24/1000 | Loss: 0.00001010
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001004
Iteration 27/1000 | Loss: 0.00001004
Iteration 28/1000 | Loss: 0.00001000
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000997
Iteration 33/1000 | Loss: 0.00000997
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000996
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000992
Iteration 40/1000 | Loss: 0.00000992
Iteration 41/1000 | Loss: 0.00000991
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000990
Iteration 44/1000 | Loss: 0.00000989
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000983
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000982
Iteration 61/1000 | Loss: 0.00000982
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000981
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000980
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000978
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000976
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000974
Iteration 89/1000 | Loss: 0.00000974
Iteration 90/1000 | Loss: 0.00000974
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000974
Iteration 94/1000 | Loss: 0.00000974
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000973
Iteration 97/1000 | Loss: 0.00000973
Iteration 98/1000 | Loss: 0.00000973
Iteration 99/1000 | Loss: 0.00000973
Iteration 100/1000 | Loss: 0.00000973
Iteration 101/1000 | Loss: 0.00000973
Iteration 102/1000 | Loss: 0.00000973
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000972
Iteration 107/1000 | Loss: 0.00000972
Iteration 108/1000 | Loss: 0.00000971
Iteration 109/1000 | Loss: 0.00000971
Iteration 110/1000 | Loss: 0.00000971
Iteration 111/1000 | Loss: 0.00000971
Iteration 112/1000 | Loss: 0.00000971
Iteration 113/1000 | Loss: 0.00000970
Iteration 114/1000 | Loss: 0.00000970
Iteration 115/1000 | Loss: 0.00000970
Iteration 116/1000 | Loss: 0.00000970
Iteration 117/1000 | Loss: 0.00000970
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000969
Iteration 121/1000 | Loss: 0.00000968
Iteration 122/1000 | Loss: 0.00000968
Iteration 123/1000 | Loss: 0.00000968
Iteration 124/1000 | Loss: 0.00000968
Iteration 125/1000 | Loss: 0.00000968
Iteration 126/1000 | Loss: 0.00000968
Iteration 127/1000 | Loss: 0.00000968
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000967
Iteration 132/1000 | Loss: 0.00000967
Iteration 133/1000 | Loss: 0.00000967
Iteration 134/1000 | Loss: 0.00000967
Iteration 135/1000 | Loss: 0.00000967
Iteration 136/1000 | Loss: 0.00000967
Iteration 137/1000 | Loss: 0.00000967
Iteration 138/1000 | Loss: 0.00000967
Iteration 139/1000 | Loss: 0.00000967
Iteration 140/1000 | Loss: 0.00000967
Iteration 141/1000 | Loss: 0.00000967
Iteration 142/1000 | Loss: 0.00000967
Iteration 143/1000 | Loss: 0.00000967
Iteration 144/1000 | Loss: 0.00000967
Iteration 145/1000 | Loss: 0.00000967
Iteration 146/1000 | Loss: 0.00000967
Iteration 147/1000 | Loss: 0.00000966
Iteration 148/1000 | Loss: 0.00000966
Iteration 149/1000 | Loss: 0.00000966
Iteration 150/1000 | Loss: 0.00000966
Iteration 151/1000 | Loss: 0.00000966
Iteration 152/1000 | Loss: 0.00000966
Iteration 153/1000 | Loss: 0.00000966
Iteration 154/1000 | Loss: 0.00000966
Iteration 155/1000 | Loss: 0.00000966
Iteration 156/1000 | Loss: 0.00000966
Iteration 157/1000 | Loss: 0.00000966
Iteration 158/1000 | Loss: 0.00000966
Iteration 159/1000 | Loss: 0.00000966
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000965
Iteration 162/1000 | Loss: 0.00000965
Iteration 163/1000 | Loss: 0.00000965
Iteration 164/1000 | Loss: 0.00000965
Iteration 165/1000 | Loss: 0.00000965
Iteration 166/1000 | Loss: 0.00000965
Iteration 167/1000 | Loss: 0.00000965
Iteration 168/1000 | Loss: 0.00000965
Iteration 169/1000 | Loss: 0.00000965
Iteration 170/1000 | Loss: 0.00000965
Iteration 171/1000 | Loss: 0.00000965
Iteration 172/1000 | Loss: 0.00000965
Iteration 173/1000 | Loss: 0.00000965
Iteration 174/1000 | Loss: 0.00000965
Iteration 175/1000 | Loss: 0.00000965
Iteration 176/1000 | Loss: 0.00000965
Iteration 177/1000 | Loss: 0.00000965
Iteration 178/1000 | Loss: 0.00000965
Iteration 179/1000 | Loss: 0.00000965
Iteration 180/1000 | Loss: 0.00000965
Iteration 181/1000 | Loss: 0.00000965
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000965
Iteration 184/1000 | Loss: 0.00000965
Iteration 185/1000 | Loss: 0.00000965
Iteration 186/1000 | Loss: 0.00000965
Iteration 187/1000 | Loss: 0.00000965
Iteration 188/1000 | Loss: 0.00000965
Iteration 189/1000 | Loss: 0.00000965
Iteration 190/1000 | Loss: 0.00000965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [9.648545528762043e-06, 9.648545528762043e-06, 9.648545528762043e-06, 9.648545528762043e-06, 9.648545528762043e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.648545528762043e-06

Optimization complete. Final v2v error: 2.655212640762329 mm

Highest mean error: 3.312772274017334 mm for frame 52

Lowest mean error: 2.4235095977783203 mm for frame 99

Saving results

Total time: 40.598578691482544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037029
Iteration 2/25 | Loss: 0.00284160
Iteration 3/25 | Loss: 0.00208249
Iteration 4/25 | Loss: 0.00200938
Iteration 5/25 | Loss: 0.00185505
Iteration 6/25 | Loss: 0.00158611
Iteration 7/25 | Loss: 0.00158316
Iteration 8/25 | Loss: 0.00158755
Iteration 9/25 | Loss: 0.00154885
Iteration 10/25 | Loss: 0.00153650
Iteration 11/25 | Loss: 0.00150373
Iteration 12/25 | Loss: 0.00147233
Iteration 13/25 | Loss: 0.00145300
Iteration 14/25 | Loss: 0.00141637
Iteration 15/25 | Loss: 0.00140627
Iteration 16/25 | Loss: 0.00138282
Iteration 17/25 | Loss: 0.00138270
Iteration 18/25 | Loss: 0.00138895
Iteration 19/25 | Loss: 0.00138139
Iteration 20/25 | Loss: 0.00136866
Iteration 21/25 | Loss: 0.00136209
Iteration 22/25 | Loss: 0.00135155
Iteration 23/25 | Loss: 0.00135673
Iteration 24/25 | Loss: 0.00134555
Iteration 25/25 | Loss: 0.00134327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26964045
Iteration 2/25 | Loss: 0.00387201
Iteration 3/25 | Loss: 0.00387200
Iteration 4/25 | Loss: 0.00387200
Iteration 5/25 | Loss: 0.00387200
Iteration 6/25 | Loss: 0.00387200
Iteration 7/25 | Loss: 0.00387200
Iteration 8/25 | Loss: 0.00387200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0038720013108104467, 0.0038720013108104467, 0.0038720013108104467, 0.0038720013108104467, 0.0038720013108104467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038720013108104467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387200
Iteration 2/1000 | Loss: 0.00120796
Iteration 3/1000 | Loss: 0.00234712
Iteration 4/1000 | Loss: 0.00244641
Iteration 5/1000 | Loss: 0.00261091
Iteration 6/1000 | Loss: 0.00267791
Iteration 7/1000 | Loss: 0.00265225
Iteration 8/1000 | Loss: 0.00246250
Iteration 9/1000 | Loss: 0.00258294
Iteration 10/1000 | Loss: 0.00288889
Iteration 11/1000 | Loss: 0.00304904
Iteration 12/1000 | Loss: 0.00301156
Iteration 13/1000 | Loss: 0.00319808
Iteration 14/1000 | Loss: 0.00322499
Iteration 15/1000 | Loss: 0.00369949
Iteration 16/1000 | Loss: 0.00328477
Iteration 17/1000 | Loss: 0.00324076
Iteration 18/1000 | Loss: 0.00300301
Iteration 19/1000 | Loss: 0.00362280
Iteration 20/1000 | Loss: 0.00307283
Iteration 21/1000 | Loss: 0.00328399
Iteration 22/1000 | Loss: 0.00295137
Iteration 23/1000 | Loss: 0.00233059
Iteration 24/1000 | Loss: 0.00298326
Iteration 25/1000 | Loss: 0.00313007
Iteration 26/1000 | Loss: 0.00321941
Iteration 27/1000 | Loss: 0.00361660
Iteration 28/1000 | Loss: 0.00349105
Iteration 29/1000 | Loss: 0.00346237
Iteration 30/1000 | Loss: 0.00320187
Iteration 31/1000 | Loss: 0.00352171
Iteration 32/1000 | Loss: 0.00314800
Iteration 33/1000 | Loss: 0.00322328
Iteration 34/1000 | Loss: 0.00308099
Iteration 35/1000 | Loss: 0.00298824
Iteration 36/1000 | Loss: 0.00390788
Iteration 37/1000 | Loss: 0.00308997
Iteration 38/1000 | Loss: 0.00237632
Iteration 39/1000 | Loss: 0.00281364
Iteration 40/1000 | Loss: 0.00254286
Iteration 41/1000 | Loss: 0.00298523
Iteration 42/1000 | Loss: 0.00362588
Iteration 43/1000 | Loss: 0.00323346
Iteration 44/1000 | Loss: 0.00321677
Iteration 45/1000 | Loss: 0.00348282
Iteration 46/1000 | Loss: 0.00336935
Iteration 47/1000 | Loss: 0.00361315
Iteration 48/1000 | Loss: 0.00422132
Iteration 49/1000 | Loss: 0.00371313
Iteration 50/1000 | Loss: 0.00426458
Iteration 51/1000 | Loss: 0.00382714
Iteration 52/1000 | Loss: 0.00378916
Iteration 53/1000 | Loss: 0.00319073
Iteration 54/1000 | Loss: 0.00434358
Iteration 55/1000 | Loss: 0.00330960
Iteration 56/1000 | Loss: 0.00402754
Iteration 57/1000 | Loss: 0.00368002
Iteration 58/1000 | Loss: 0.00436177
Iteration 59/1000 | Loss: 0.00359983
Iteration 60/1000 | Loss: 0.00358404
Iteration 61/1000 | Loss: 0.00324648
Iteration 62/1000 | Loss: 0.00308877
Iteration 63/1000 | Loss: 0.00244225
Iteration 64/1000 | Loss: 0.00273921
Iteration 65/1000 | Loss: 0.00283646
Iteration 66/1000 | Loss: 0.00290799
Iteration 67/1000 | Loss: 0.00399938
Iteration 68/1000 | Loss: 0.00398327
Iteration 69/1000 | Loss: 0.00249163
Iteration 70/1000 | Loss: 0.00250158
Iteration 71/1000 | Loss: 0.00282091
Iteration 72/1000 | Loss: 0.00407594
Iteration 73/1000 | Loss: 0.00313842
Iteration 74/1000 | Loss: 0.00267687
Iteration 75/1000 | Loss: 0.00354723
Iteration 76/1000 | Loss: 0.00329901
Iteration 77/1000 | Loss: 0.00344314
Iteration 78/1000 | Loss: 0.00265061
Iteration 79/1000 | Loss: 0.00275968
Iteration 80/1000 | Loss: 0.00254230
Iteration 81/1000 | Loss: 0.00263778
Iteration 82/1000 | Loss: 0.00451442
Iteration 83/1000 | Loss: 0.00429830
Iteration 84/1000 | Loss: 0.00425435
Iteration 85/1000 | Loss: 0.00347082
Iteration 86/1000 | Loss: 0.00544430
Iteration 87/1000 | Loss: 0.00327856
Iteration 88/1000 | Loss: 0.00378290
Iteration 89/1000 | Loss: 0.00306761
Iteration 90/1000 | Loss: 0.00335567
Iteration 91/1000 | Loss: 0.00292381
Iteration 92/1000 | Loss: 0.00335313
Iteration 93/1000 | Loss: 0.00307395
Iteration 94/1000 | Loss: 0.00330253
Iteration 95/1000 | Loss: 0.00324093
Iteration 96/1000 | Loss: 0.00388624
Iteration 97/1000 | Loss: 0.00327715
Iteration 98/1000 | Loss: 0.00391579
Iteration 99/1000 | Loss: 0.00340545
Iteration 100/1000 | Loss: 0.00409986
Iteration 101/1000 | Loss: 0.00526779
Iteration 102/1000 | Loss: 0.00251713
Iteration 103/1000 | Loss: 0.00278492
Iteration 104/1000 | Loss: 0.00275149
Iteration 105/1000 | Loss: 0.00254706
Iteration 106/1000 | Loss: 0.00175137
Iteration 107/1000 | Loss: 0.00202113
Iteration 108/1000 | Loss: 0.00228802
Iteration 109/1000 | Loss: 0.00232773
Iteration 110/1000 | Loss: 0.00217919
Iteration 111/1000 | Loss: 0.00211554
Iteration 112/1000 | Loss: 0.00325778
Iteration 113/1000 | Loss: 0.00233293
Iteration 114/1000 | Loss: 0.00234289
Iteration 115/1000 | Loss: 0.00223020
Iteration 116/1000 | Loss: 0.00232234
Iteration 117/1000 | Loss: 0.00245653
Iteration 118/1000 | Loss: 0.00245557
Iteration 119/1000 | Loss: 0.00245846
Iteration 120/1000 | Loss: 0.00264845
Iteration 121/1000 | Loss: 0.00219855
Iteration 122/1000 | Loss: 0.00245355
Iteration 123/1000 | Loss: 0.00310132
Iteration 124/1000 | Loss: 0.00172719
Iteration 125/1000 | Loss: 0.00371408
Iteration 126/1000 | Loss: 0.00279026
Iteration 127/1000 | Loss: 0.00355786
Iteration 128/1000 | Loss: 0.00246556
Iteration 129/1000 | Loss: 0.00237693
Iteration 130/1000 | Loss: 0.00213698
Iteration 131/1000 | Loss: 0.00185227
Iteration 132/1000 | Loss: 0.00202216
Iteration 133/1000 | Loss: 0.00219024
Iteration 134/1000 | Loss: 0.00283717
Iteration 135/1000 | Loss: 0.00225465
Iteration 136/1000 | Loss: 0.00242359
Iteration 137/1000 | Loss: 0.00215838
Iteration 138/1000 | Loss: 0.00240318
Iteration 139/1000 | Loss: 0.00261261
Iteration 140/1000 | Loss: 0.00244592
Iteration 141/1000 | Loss: 0.00244408
Iteration 142/1000 | Loss: 0.00221646
Iteration 143/1000 | Loss: 0.00198498
Iteration 144/1000 | Loss: 0.00388190
Iteration 145/1000 | Loss: 0.00197049
Iteration 146/1000 | Loss: 0.00190246
Iteration 147/1000 | Loss: 0.00197706
Iteration 148/1000 | Loss: 0.00241943
Iteration 149/1000 | Loss: 0.00255644
Iteration 150/1000 | Loss: 0.00252822
Iteration 151/1000 | Loss: 0.00237969
Iteration 152/1000 | Loss: 0.00249185
Iteration 153/1000 | Loss: 0.00220743
Iteration 154/1000 | Loss: 0.00202609
Iteration 155/1000 | Loss: 0.00236690
Iteration 156/1000 | Loss: 0.00233023
Iteration 157/1000 | Loss: 0.00222245
Iteration 158/1000 | Loss: 0.00226882
Iteration 159/1000 | Loss: 0.00334027
Iteration 160/1000 | Loss: 0.00265296
Iteration 161/1000 | Loss: 0.00231931
Iteration 162/1000 | Loss: 0.00225346
Iteration 163/1000 | Loss: 0.00223391
Iteration 164/1000 | Loss: 0.00227196
Iteration 165/1000 | Loss: 0.00214762
Iteration 166/1000 | Loss: 0.00240134
Iteration 167/1000 | Loss: 0.00207502
Iteration 168/1000 | Loss: 0.00234184
Iteration 169/1000 | Loss: 0.00297836
Iteration 170/1000 | Loss: 0.00284814
Iteration 171/1000 | Loss: 0.00245620
Iteration 172/1000 | Loss: 0.00242363
Iteration 173/1000 | Loss: 0.00223481
Iteration 174/1000 | Loss: 0.00247347
Iteration 175/1000 | Loss: 0.00212363
Iteration 176/1000 | Loss: 0.00253103
Iteration 177/1000 | Loss: 0.00184430
Iteration 178/1000 | Loss: 0.00187904
Iteration 179/1000 | Loss: 0.00224045
Iteration 180/1000 | Loss: 0.00240125
Iteration 181/1000 | Loss: 0.00256619
Iteration 182/1000 | Loss: 0.00253288
Iteration 183/1000 | Loss: 0.00261937
Iteration 184/1000 | Loss: 0.00240279
Iteration 185/1000 | Loss: 0.00307786
Iteration 186/1000 | Loss: 0.00247670
Iteration 187/1000 | Loss: 0.00295030
Iteration 188/1000 | Loss: 0.00299369
Iteration 189/1000 | Loss: 0.00326285
Iteration 190/1000 | Loss: 0.00138119
Iteration 191/1000 | Loss: 0.00212660
Iteration 192/1000 | Loss: 0.00184824
Iteration 193/1000 | Loss: 0.00228897
Iteration 194/1000 | Loss: 0.00209087
Iteration 195/1000 | Loss: 0.00222944
Iteration 196/1000 | Loss: 0.00245079
Iteration 197/1000 | Loss: 0.00229894
Iteration 198/1000 | Loss: 0.00273544
Iteration 199/1000 | Loss: 0.00212543
Iteration 200/1000 | Loss: 0.00238032
Iteration 201/1000 | Loss: 0.00238759
Iteration 202/1000 | Loss: 0.00318305
Iteration 203/1000 | Loss: 0.00248842
Iteration 204/1000 | Loss: 0.00286456
Iteration 205/1000 | Loss: 0.00266719
Iteration 206/1000 | Loss: 0.00224340
Iteration 207/1000 | Loss: 0.00217104
Iteration 208/1000 | Loss: 0.00213172
Iteration 209/1000 | Loss: 0.00228697
Iteration 210/1000 | Loss: 0.00249158
Iteration 211/1000 | Loss: 0.00298909
Iteration 212/1000 | Loss: 0.00240780
Iteration 213/1000 | Loss: 0.00234371
Iteration 214/1000 | Loss: 0.00195161
Iteration 215/1000 | Loss: 0.00218598
Iteration 216/1000 | Loss: 0.00216640
Iteration 217/1000 | Loss: 0.00231851
Iteration 218/1000 | Loss: 0.00188182
Iteration 219/1000 | Loss: 0.00195727
Iteration 220/1000 | Loss: 0.00191614
Iteration 221/1000 | Loss: 0.00248697
Iteration 222/1000 | Loss: 0.00289190
Iteration 223/1000 | Loss: 0.00201719
Iteration 224/1000 | Loss: 0.00206095
Iteration 225/1000 | Loss: 0.00154949
Iteration 226/1000 | Loss: 0.00169635
Iteration 227/1000 | Loss: 0.00160646
Iteration 228/1000 | Loss: 0.00161824
Iteration 229/1000 | Loss: 0.00182147
Iteration 230/1000 | Loss: 0.00138845
Iteration 231/1000 | Loss: 0.00127195
Iteration 232/1000 | Loss: 0.00136475
Iteration 233/1000 | Loss: 0.00135632
Iteration 234/1000 | Loss: 0.00291663
Iteration 235/1000 | Loss: 0.00266302
Iteration 236/1000 | Loss: 0.00227074
Iteration 237/1000 | Loss: 0.00156021
Iteration 238/1000 | Loss: 0.00188421
Iteration 239/1000 | Loss: 0.00194584
Iteration 240/1000 | Loss: 0.00169971
Iteration 241/1000 | Loss: 0.00191553
Iteration 242/1000 | Loss: 0.00184056
Iteration 243/1000 | Loss: 0.00169018
Iteration 244/1000 | Loss: 0.00218266
Iteration 245/1000 | Loss: 0.00213977
Iteration 246/1000 | Loss: 0.00202617
Iteration 247/1000 | Loss: 0.00260900
Iteration 248/1000 | Loss: 0.00211236
Iteration 249/1000 | Loss: 0.00242360
Iteration 250/1000 | Loss: 0.00265836
Iteration 251/1000 | Loss: 0.00228478
Iteration 252/1000 | Loss: 0.00159412
Iteration 253/1000 | Loss: 0.00187626
Iteration 254/1000 | Loss: 0.00233416
Iteration 255/1000 | Loss: 0.00219346
Iteration 256/1000 | Loss: 0.00218772
Iteration 257/1000 | Loss: 0.00273790
Iteration 258/1000 | Loss: 0.00251480
Iteration 259/1000 | Loss: 0.00190411
Iteration 260/1000 | Loss: 0.00148452
Iteration 261/1000 | Loss: 0.00128929
Iteration 262/1000 | Loss: 0.00149305
Iteration 263/1000 | Loss: 0.00186400
Iteration 264/1000 | Loss: 0.00141430
Iteration 265/1000 | Loss: 0.00158971
Iteration 266/1000 | Loss: 0.00149801
Iteration 267/1000 | Loss: 0.00164587
Iteration 268/1000 | Loss: 0.00185492
Iteration 269/1000 | Loss: 0.00191463
Iteration 270/1000 | Loss: 0.00210457
Iteration 271/1000 | Loss: 0.00222072
Iteration 272/1000 | Loss: 0.00199351
Iteration 273/1000 | Loss: 0.00188213
Iteration 274/1000 | Loss: 0.00148173
Iteration 275/1000 | Loss: 0.00179152
Iteration 276/1000 | Loss: 0.00178437
Iteration 277/1000 | Loss: 0.00183239
Iteration 278/1000 | Loss: 0.00228999
Iteration 279/1000 | Loss: 0.00188597
Iteration 280/1000 | Loss: 0.00185012
Iteration 281/1000 | Loss: 0.00214562
Iteration 282/1000 | Loss: 0.00197833
Iteration 283/1000 | Loss: 0.00197084
Iteration 284/1000 | Loss: 0.00177799
Iteration 285/1000 | Loss: 0.00167911
Iteration 286/1000 | Loss: 0.00183672
Iteration 287/1000 | Loss: 0.00224384
Iteration 288/1000 | Loss: 0.00222844
Iteration 289/1000 | Loss: 0.00192257
Iteration 290/1000 | Loss: 0.00228813
Iteration 291/1000 | Loss: 0.00188834
Iteration 292/1000 | Loss: 0.00189582
Iteration 293/1000 | Loss: 0.00186360
Iteration 294/1000 | Loss: 0.00191431
Iteration 295/1000 | Loss: 0.00185859
Iteration 296/1000 | Loss: 0.00143767
Iteration 297/1000 | Loss: 0.00176498
Iteration 298/1000 | Loss: 0.00213366
Iteration 299/1000 | Loss: 0.00176660
Iteration 300/1000 | Loss: 0.00207523
Iteration 301/1000 | Loss: 0.00295390
Iteration 302/1000 | Loss: 0.00269673
Iteration 303/1000 | Loss: 0.00255886
Iteration 304/1000 | Loss: 0.00196740
Iteration 305/1000 | Loss: 0.00191454
Iteration 306/1000 | Loss: 0.00236883
Iteration 307/1000 | Loss: 0.00305312
Iteration 308/1000 | Loss: 0.00199283
Iteration 309/1000 | Loss: 0.00186841
Iteration 310/1000 | Loss: 0.00207068
Iteration 311/1000 | Loss: 0.00196428
Iteration 312/1000 | Loss: 0.00246197
Iteration 313/1000 | Loss: 0.00196423
Iteration 314/1000 | Loss: 0.00217737
Iteration 315/1000 | Loss: 0.00159938
Iteration 316/1000 | Loss: 0.00218451
Iteration 317/1000 | Loss: 0.00211490
Iteration 318/1000 | Loss: 0.00186945
Iteration 319/1000 | Loss: 0.00196667
Iteration 320/1000 | Loss: 0.00177531
Iteration 321/1000 | Loss: 0.00185049
Iteration 322/1000 | Loss: 0.00205382
Iteration 323/1000 | Loss: 0.00276118
Iteration 324/1000 | Loss: 0.00196126
Iteration 325/1000 | Loss: 0.00206877
Iteration 326/1000 | Loss: 0.00195396
Iteration 327/1000 | Loss: 0.00193602
Iteration 328/1000 | Loss: 0.00203393
Iteration 329/1000 | Loss: 0.00206157
Iteration 330/1000 | Loss: 0.00173799
Iteration 331/1000 | Loss: 0.00136987
Iteration 332/1000 | Loss: 0.00211441
Iteration 333/1000 | Loss: 0.00204333
Iteration 334/1000 | Loss: 0.00202190
Iteration 335/1000 | Loss: 0.00150155
Iteration 336/1000 | Loss: 0.00181916
Iteration 337/1000 | Loss: 0.00212923
Iteration 338/1000 | Loss: 0.00192201
Iteration 339/1000 | Loss: 0.00216998
Iteration 340/1000 | Loss: 0.00203485
Iteration 341/1000 | Loss: 0.00188608
Iteration 342/1000 | Loss: 0.00190694
Iteration 343/1000 | Loss: 0.00220340
Iteration 344/1000 | Loss: 0.00187064
Iteration 345/1000 | Loss: 0.00164334
Iteration 346/1000 | Loss: 0.00185622
Iteration 347/1000 | Loss: 0.00218340
Iteration 348/1000 | Loss: 0.00249508
Iteration 349/1000 | Loss: 0.00259182
Iteration 350/1000 | Loss: 0.00213389
Iteration 351/1000 | Loss: 0.00225859
Iteration 352/1000 | Loss: 0.00195012
Iteration 353/1000 | Loss: 0.00211776
Iteration 354/1000 | Loss: 0.00207835
Iteration 355/1000 | Loss: 0.00230383
Iteration 356/1000 | Loss: 0.00183182
Iteration 357/1000 | Loss: 0.00206932
Iteration 358/1000 | Loss: 0.00171450
Iteration 359/1000 | Loss: 0.00190958
Iteration 360/1000 | Loss: 0.00167878
Iteration 361/1000 | Loss: 0.00240420
Iteration 362/1000 | Loss: 0.00196377
Iteration 363/1000 | Loss: 0.00144411
Iteration 364/1000 | Loss: 0.00172424
Iteration 365/1000 | Loss: 0.00147705
Iteration 366/1000 | Loss: 0.00213828
Iteration 367/1000 | Loss: 0.00290150
Iteration 368/1000 | Loss: 0.00222243
Iteration 369/1000 | Loss: 0.00196140
Iteration 370/1000 | Loss: 0.00141117
Iteration 371/1000 | Loss: 0.00102690
Iteration 372/1000 | Loss: 0.00108389
Iteration 373/1000 | Loss: 0.00114694
Iteration 374/1000 | Loss: 0.00135268
Iteration 375/1000 | Loss: 0.00162680
Iteration 376/1000 | Loss: 0.00130518
Iteration 377/1000 | Loss: 0.00162833
Iteration 378/1000 | Loss: 0.00202251
Iteration 379/1000 | Loss: 0.00117593
Iteration 380/1000 | Loss: 0.00116470
Iteration 381/1000 | Loss: 0.00130477
Iteration 382/1000 | Loss: 0.00127915
Iteration 383/1000 | Loss: 0.00133246
Iteration 384/1000 | Loss: 0.00159063
Iteration 385/1000 | Loss: 0.00181008
Iteration 386/1000 | Loss: 0.00205959
Iteration 387/1000 | Loss: 0.00128059
Iteration 388/1000 | Loss: 0.00130435
Iteration 389/1000 | Loss: 0.00111157
Iteration 390/1000 | Loss: 0.00140142
Iteration 391/1000 | Loss: 0.00132728
Iteration 392/1000 | Loss: 0.00137221
Iteration 393/1000 | Loss: 0.00210751
Iteration 394/1000 | Loss: 0.00183052
Iteration 395/1000 | Loss: 0.00136496
Iteration 396/1000 | Loss: 0.00106393
Iteration 397/1000 | Loss: 0.00148387
Iteration 398/1000 | Loss: 0.00131581
Iteration 399/1000 | Loss: 0.00222740
Iteration 400/1000 | Loss: 0.00224082
Iteration 401/1000 | Loss: 0.00143980
Iteration 402/1000 | Loss: 0.00114780
Iteration 403/1000 | Loss: 0.00143325
Iteration 404/1000 | Loss: 0.00131032
Iteration 405/1000 | Loss: 0.00237002
Iteration 406/1000 | Loss: 0.00171006
Iteration 407/1000 | Loss: 0.00141288
Iteration 408/1000 | Loss: 0.00174561
Iteration 409/1000 | Loss: 0.00155586
Iteration 410/1000 | Loss: 0.00191693
Iteration 411/1000 | Loss: 0.00119510
Iteration 412/1000 | Loss: 0.00128694
Iteration 413/1000 | Loss: 0.00116617
Iteration 414/1000 | Loss: 0.00126177
Iteration 415/1000 | Loss: 0.00139139
Iteration 416/1000 | Loss: 0.00127670
Iteration 417/1000 | Loss: 0.00137035
Iteration 418/1000 | Loss: 0.00159024
Iteration 419/1000 | Loss: 0.00194958
Iteration 420/1000 | Loss: 0.00147943
Iteration 421/1000 | Loss: 0.00135253
Iteration 422/1000 | Loss: 0.00133141
Iteration 423/1000 | Loss: 0.00139044
Iteration 424/1000 | Loss: 0.00210822
Iteration 425/1000 | Loss: 0.00163428
Iteration 426/1000 | Loss: 0.00134278
Iteration 427/1000 | Loss: 0.00147843
Iteration 428/1000 | Loss: 0.00148436
Iteration 429/1000 | Loss: 0.00175164
Iteration 430/1000 | Loss: 0.00142507
Iteration 431/1000 | Loss: 0.00151583
Iteration 432/1000 | Loss: 0.00152006
Iteration 433/1000 | Loss: 0.00158698
Iteration 434/1000 | Loss: 0.00147150
Iteration 435/1000 | Loss: 0.00190458
Iteration 436/1000 | Loss: 0.00250399
Iteration 437/1000 | Loss: 0.00259874
Iteration 438/1000 | Loss: 0.00180518
Iteration 439/1000 | Loss: 0.00153119
Iteration 440/1000 | Loss: 0.00162608
Iteration 441/1000 | Loss: 0.00168428
Iteration 442/1000 | Loss: 0.00162365
Iteration 443/1000 | Loss: 0.00194560
Iteration 444/1000 | Loss: 0.00161567
Iteration 445/1000 | Loss: 0.00204716
Iteration 446/1000 | Loss: 0.00145955
Iteration 447/1000 | Loss: 0.00143548
Iteration 448/1000 | Loss: 0.00162567
Iteration 449/1000 | Loss: 0.00174336
Iteration 450/1000 | Loss: 0.00161144
Iteration 451/1000 | Loss: 0.00140392
Iteration 452/1000 | Loss: 0.00117013
Iteration 453/1000 | Loss: 0.00093589
Iteration 454/1000 | Loss: 0.00133349
Iteration 455/1000 | Loss: 0.00180003
Iteration 456/1000 | Loss: 0.00145449
Iteration 457/1000 | Loss: 0.00163892
Iteration 458/1000 | Loss: 0.00144892
Iteration 459/1000 | Loss: 0.00188325
Iteration 460/1000 | Loss: 0.00184323
Iteration 461/1000 | Loss: 0.00130269
Iteration 462/1000 | Loss: 0.00149470
Iteration 463/1000 | Loss: 0.00129782
Iteration 464/1000 | Loss: 0.00085995
Iteration 465/1000 | Loss: 0.00096528
Iteration 466/1000 | Loss: 0.00128255
Iteration 467/1000 | Loss: 0.00115541
Iteration 468/1000 | Loss: 0.00058607
Iteration 469/1000 | Loss: 0.00088183
Iteration 470/1000 | Loss: 0.00101981
Iteration 471/1000 | Loss: 0.00064505
Iteration 472/1000 | Loss: 0.00146333
Iteration 473/1000 | Loss: 0.00120850
Iteration 474/1000 | Loss: 0.00124637
Iteration 475/1000 | Loss: 0.00182249
Iteration 476/1000 | Loss: 0.00163095
Iteration 477/1000 | Loss: 0.00199450
Iteration 478/1000 | Loss: 0.00156077
Iteration 479/1000 | Loss: 0.00283934
Iteration 480/1000 | Loss: 0.00167088
Iteration 481/1000 | Loss: 0.00181631
Iteration 482/1000 | Loss: 0.00230528
Iteration 483/1000 | Loss: 0.00194571
Iteration 484/1000 | Loss: 0.00188201
Iteration 485/1000 | Loss: 0.00199307
Iteration 486/1000 | Loss: 0.00211708
Iteration 487/1000 | Loss: 0.00177254
Iteration 488/1000 | Loss: 0.00176462
Iteration 489/1000 | Loss: 0.00194232
Iteration 490/1000 | Loss: 0.00167877
Iteration 491/1000 | Loss: 0.00202524
Iteration 492/1000 | Loss: 0.00131255
Iteration 493/1000 | Loss: 0.00115914
Iteration 494/1000 | Loss: 0.00120079
Iteration 495/1000 | Loss: 0.00135976
Iteration 496/1000 | Loss: 0.00095350
Iteration 497/1000 | Loss: 0.00175304
Iteration 498/1000 | Loss: 0.00143017
Iteration 499/1000 | Loss: 0.00091806
Iteration 500/1000 | Loss: 0.00100807
Iteration 501/1000 | Loss: 0.00165859
Iteration 502/1000 | Loss: 0.00190161
Iteration 503/1000 | Loss: 0.00168173
Iteration 504/1000 | Loss: 0.00154532
Iteration 505/1000 | Loss: 0.00184484
Iteration 506/1000 | Loss: 0.00126157
Iteration 507/1000 | Loss: 0.00150063
Iteration 508/1000 | Loss: 0.00147922
Iteration 509/1000 | Loss: 0.00134982
Iteration 510/1000 | Loss: 0.00122552
Iteration 511/1000 | Loss: 0.00142426
Iteration 512/1000 | Loss: 0.00133549
Iteration 513/1000 | Loss: 0.00126540
Iteration 514/1000 | Loss: 0.00112885
Iteration 515/1000 | Loss: 0.00129820
Iteration 516/1000 | Loss: 0.00182901
Iteration 517/1000 | Loss: 0.00140284
Iteration 518/1000 | Loss: 0.00112572
Iteration 519/1000 | Loss: 0.00080013
Iteration 520/1000 | Loss: 0.00089187
Iteration 521/1000 | Loss: 0.00095353
Iteration 522/1000 | Loss: 0.00161314
Iteration 523/1000 | Loss: 0.00141608
Iteration 524/1000 | Loss: 0.00099524
Iteration 525/1000 | Loss: 0.00065956
Iteration 526/1000 | Loss: 0.00076731
Iteration 527/1000 | Loss: 0.00094557
Iteration 528/1000 | Loss: 0.00149332
Iteration 529/1000 | Loss: 0.00096935
Iteration 530/1000 | Loss: 0.00091263
Iteration 531/1000 | Loss: 0.00104456
Iteration 532/1000 | Loss: 0.00116370
Iteration 533/1000 | Loss: 0.00127061
Iteration 534/1000 | Loss: 0.00094493
Iteration 535/1000 | Loss: 0.00124461
Iteration 536/1000 | Loss: 0.00120588
Iteration 537/1000 | Loss: 0.00109722
Iteration 538/1000 | Loss: 0.00093388
Iteration 539/1000 | Loss: 0.00074204
Iteration 540/1000 | Loss: 0.00079028
Iteration 541/1000 | Loss: 0.00095394
Iteration 542/1000 | Loss: 0.00113038
Iteration 543/1000 | Loss: 0.00085626
Iteration 544/1000 | Loss: 0.00071086
Iteration 545/1000 | Loss: 0.00076208
Iteration 546/1000 | Loss: 0.00047275
Iteration 547/1000 | Loss: 0.00053612
Iteration 548/1000 | Loss: 0.00087293
Iteration 549/1000 | Loss: 0.00089054
Iteration 550/1000 | Loss: 0.00086376
Iteration 551/1000 | Loss: 0.00132493
Iteration 552/1000 | Loss: 0.00095564
Iteration 553/1000 | Loss: 0.00083503
Iteration 554/1000 | Loss: 0.00108076
Iteration 555/1000 | Loss: 0.00101421
Iteration 556/1000 | Loss: 0.00107330
Iteration 557/1000 | Loss: 0.00100510
Iteration 558/1000 | Loss: 0.00070143
Iteration 559/1000 | Loss: 0.00078918
Iteration 560/1000 | Loss: 0.00095174
Iteration 561/1000 | Loss: 0.00121461
Iteration 562/1000 | Loss: 0.00079719
Iteration 563/1000 | Loss: 0.00115275
Iteration 564/1000 | Loss: 0.00069666
Iteration 565/1000 | Loss: 0.00102642
Iteration 566/1000 | Loss: 0.00092352
Iteration 567/1000 | Loss: 0.00069686
Iteration 568/1000 | Loss: 0.00065852
Iteration 569/1000 | Loss: 0.00092135
Iteration 570/1000 | Loss: 0.00075663
Iteration 571/1000 | Loss: 0.00066420
Iteration 572/1000 | Loss: 0.00064167
Iteration 573/1000 | Loss: 0.00089909
Iteration 574/1000 | Loss: 0.00101121
Iteration 575/1000 | Loss: 0.00081687
Iteration 576/1000 | Loss: 0.00080768
Iteration 577/1000 | Loss: 0.00084427
Iteration 578/1000 | Loss: 0.00085904
Iteration 579/1000 | Loss: 0.00083250
Iteration 580/1000 | Loss: 0.00161168
Iteration 581/1000 | Loss: 0.00105427
Iteration 582/1000 | Loss: 0.00116989
Iteration 583/1000 | Loss: 0.00087313
Iteration 584/1000 | Loss: 0.00080035
Iteration 585/1000 | Loss: 0.00141692
Iteration 586/1000 | Loss: 0.00075194
Iteration 587/1000 | Loss: 0.00060398
Iteration 588/1000 | Loss: 0.00093107
Iteration 589/1000 | Loss: 0.00081622
Iteration 590/1000 | Loss: 0.00088759
Iteration 591/1000 | Loss: 0.00077650
Iteration 592/1000 | Loss: 0.00080999
Iteration 593/1000 | Loss: 0.00150819
Iteration 594/1000 | Loss: 0.00100226
Iteration 595/1000 | Loss: 0.00086975
Iteration 596/1000 | Loss: 0.00069343
Iteration 597/1000 | Loss: 0.00058667
Iteration 598/1000 | Loss: 0.00041691
Iteration 599/1000 | Loss: 0.00082343
Iteration 600/1000 | Loss: 0.00068307
Iteration 601/1000 | Loss: 0.00088613
Iteration 602/1000 | Loss: 0.00079527
Iteration 603/1000 | Loss: 0.00122040
Iteration 604/1000 | Loss: 0.00075365
Iteration 605/1000 | Loss: 0.00049533
Iteration 606/1000 | Loss: 0.00065855
Iteration 607/1000 | Loss: 0.00123596
Iteration 608/1000 | Loss: 0.00054256
Iteration 609/1000 | Loss: 0.00064785
Iteration 610/1000 | Loss: 0.00070329
Iteration 611/1000 | Loss: 0.00070792
Iteration 612/1000 | Loss: 0.00051453
Iteration 613/1000 | Loss: 0.00066895
Iteration 614/1000 | Loss: 0.00062448
Iteration 615/1000 | Loss: 0.00063212
Iteration 616/1000 | Loss: 0.00064821
Iteration 617/1000 | Loss: 0.00073923
Iteration 618/1000 | Loss: 0.00065849
Iteration 619/1000 | Loss: 0.00063208
Iteration 620/1000 | Loss: 0.00063113
Iteration 621/1000 | Loss: 0.00069935
Iteration 622/1000 | Loss: 0.00080262
Iteration 623/1000 | Loss: 0.00085473
Iteration 624/1000 | Loss: 0.00063212
Iteration 625/1000 | Loss: 0.00076724
Iteration 626/1000 | Loss: 0.00073525
Iteration 627/1000 | Loss: 0.00080220
Iteration 628/1000 | Loss: 0.00071162
Iteration 629/1000 | Loss: 0.00113186
Iteration 630/1000 | Loss: 0.00077792
Iteration 631/1000 | Loss: 0.00130621
Iteration 632/1000 | Loss: 0.00131980
Iteration 633/1000 | Loss: 0.00088512
Iteration 634/1000 | Loss: 0.00087213
Iteration 635/1000 | Loss: 0.00092750
Iteration 636/1000 | Loss: 0.00071208
Iteration 637/1000 | Loss: 0.00087995
Iteration 638/1000 | Loss: 0.00117926
Iteration 639/1000 | Loss: 0.00142076
Iteration 640/1000 | Loss: 0.00124911
Iteration 641/1000 | Loss: 0.00091675
Iteration 642/1000 | Loss: 0.00061839
Iteration 643/1000 | Loss: 0.00088039
Iteration 644/1000 | Loss: 0.00042489
Iteration 645/1000 | Loss: 0.00026144
Iteration 646/1000 | Loss: 0.00047967
Iteration 647/1000 | Loss: 0.00059797
Iteration 648/1000 | Loss: 0.00054099
Iteration 649/1000 | Loss: 0.00081749
Iteration 650/1000 | Loss: 0.00057395
Iteration 651/1000 | Loss: 0.00078007
Iteration 652/1000 | Loss: 0.00082032
Iteration 653/1000 | Loss: 0.00038214
Iteration 654/1000 | Loss: 0.00035401
Iteration 655/1000 | Loss: 0.00051878
Iteration 656/1000 | Loss: 0.00077646
Iteration 657/1000 | Loss: 0.00120158
Iteration 658/1000 | Loss: 0.00041533
Iteration 659/1000 | Loss: 0.00033667
Iteration 660/1000 | Loss: 0.00046688
Iteration 661/1000 | Loss: 0.00032628
Iteration 662/1000 | Loss: 0.00028153
Iteration 663/1000 | Loss: 0.00065334
Iteration 664/1000 | Loss: 0.00044948
Iteration 665/1000 | Loss: 0.00019980
Iteration 666/1000 | Loss: 0.00016126
Iteration 667/1000 | Loss: 0.00072916
Iteration 668/1000 | Loss: 0.00031121
Iteration 669/1000 | Loss: 0.00033204
Iteration 670/1000 | Loss: 0.00047848
Iteration 671/1000 | Loss: 0.00038290
Iteration 672/1000 | Loss: 0.00049127
Iteration 673/1000 | Loss: 0.00046131
Iteration 674/1000 | Loss: 0.00070200
Iteration 675/1000 | Loss: 0.00047204
Iteration 676/1000 | Loss: 0.00023131
Iteration 677/1000 | Loss: 0.00006235
Iteration 678/1000 | Loss: 0.00015125
Iteration 679/1000 | Loss: 0.00015350
Iteration 680/1000 | Loss: 0.00017151
Iteration 681/1000 | Loss: 0.00022781
Iteration 682/1000 | Loss: 0.00016281
Iteration 683/1000 | Loss: 0.00015933
Iteration 684/1000 | Loss: 0.00006516
Iteration 685/1000 | Loss: 0.00014841
Iteration 686/1000 | Loss: 0.00004715
Iteration 687/1000 | Loss: 0.00005654
Iteration 688/1000 | Loss: 0.00006711
Iteration 689/1000 | Loss: 0.00008382
Iteration 690/1000 | Loss: 0.00007985
Iteration 691/1000 | Loss: 0.00045861
Iteration 692/1000 | Loss: 0.00028466
Iteration 693/1000 | Loss: 0.00008129
Iteration 694/1000 | Loss: 0.00038129
Iteration 695/1000 | Loss: 0.00013259
Iteration 696/1000 | Loss: 0.00021289
Iteration 697/1000 | Loss: 0.00029885
Iteration 698/1000 | Loss: 0.00042498
Iteration 699/1000 | Loss: 0.00029903
Iteration 700/1000 | Loss: 0.00009109
Iteration 701/1000 | Loss: 0.00004254
Iteration 702/1000 | Loss: 0.00019443
Iteration 703/1000 | Loss: 0.00014729
Iteration 704/1000 | Loss: 0.00005412
Iteration 705/1000 | Loss: 0.00005451
Iteration 706/1000 | Loss: 0.00005301
Iteration 707/1000 | Loss: 0.00004848
Iteration 708/1000 | Loss: 0.00004785
Iteration 709/1000 | Loss: 0.00003683
Iteration 710/1000 | Loss: 0.00003976
Iteration 711/1000 | Loss: 0.00031608
Iteration 712/1000 | Loss: 0.00013627
Iteration 713/1000 | Loss: 0.00034928
Iteration 714/1000 | Loss: 0.00012906
Iteration 715/1000 | Loss: 0.00016020
Iteration 716/1000 | Loss: 0.00002690
Iteration 717/1000 | Loss: 0.00002780
Iteration 718/1000 | Loss: 0.00007910
Iteration 719/1000 | Loss: 0.00012414
Iteration 720/1000 | Loss: 0.00010363
Iteration 721/1000 | Loss: 0.00015403
Iteration 722/1000 | Loss: 0.00011726
Iteration 723/1000 | Loss: 0.00009879
Iteration 724/1000 | Loss: 0.00010651
Iteration 725/1000 | Loss: 0.00009185
Iteration 726/1000 | Loss: 0.00011981
Iteration 727/1000 | Loss: 0.00002176
Iteration 728/1000 | Loss: 0.00014406
Iteration 729/1000 | Loss: 0.00011957
Iteration 730/1000 | Loss: 0.00011757
Iteration 731/1000 | Loss: 0.00011546
Iteration 732/1000 | Loss: 0.00010566
Iteration 733/1000 | Loss: 0.00011378
Iteration 734/1000 | Loss: 0.00011453
Iteration 735/1000 | Loss: 0.00011206
Iteration 736/1000 | Loss: 0.00011697
Iteration 737/1000 | Loss: 0.00010604
Iteration 738/1000 | Loss: 0.00010463
Iteration 739/1000 | Loss: 0.00012291
Iteration 740/1000 | Loss: 0.00004086
Iteration 741/1000 | Loss: 0.00012204
Iteration 742/1000 | Loss: 0.00007735
Iteration 743/1000 | Loss: 0.00004510
Iteration 744/1000 | Loss: 0.00013961
Iteration 745/1000 | Loss: 0.00022357
Iteration 746/1000 | Loss: 0.00012102
Iteration 747/1000 | Loss: 0.00007865
Iteration 748/1000 | Loss: 0.00012950
Iteration 749/1000 | Loss: 0.00008742
Iteration 750/1000 | Loss: 0.00009311
Iteration 751/1000 | Loss: 0.00008316
Iteration 752/1000 | Loss: 0.00011436
Iteration 753/1000 | Loss: 0.00013228
Iteration 754/1000 | Loss: 0.00013099
Iteration 755/1000 | Loss: 0.00013962
Iteration 756/1000 | Loss: 0.00012038
Iteration 757/1000 | Loss: 0.00024093
Iteration 758/1000 | Loss: 0.00003276
Iteration 759/1000 | Loss: 0.00051229
Iteration 760/1000 | Loss: 0.00003972
Iteration 761/1000 | Loss: 0.00002208
Iteration 762/1000 | Loss: 0.00005598
Iteration 763/1000 | Loss: 0.00001059
Iteration 764/1000 | Loss: 0.00000961
Iteration 765/1000 | Loss: 0.00005551
Iteration 766/1000 | Loss: 0.00001548
Iteration 767/1000 | Loss: 0.00000858
Iteration 768/1000 | Loss: 0.00001410
Iteration 769/1000 | Loss: 0.00001163
Iteration 770/1000 | Loss: 0.00000790
Iteration 771/1000 | Loss: 0.00000773
Iteration 772/1000 | Loss: 0.00000754
Iteration 773/1000 | Loss: 0.00000754
Iteration 774/1000 | Loss: 0.00000745
Iteration 775/1000 | Loss: 0.00000738
Iteration 776/1000 | Loss: 0.00000738
Iteration 777/1000 | Loss: 0.00000735
Iteration 778/1000 | Loss: 0.00000734
Iteration 779/1000 | Loss: 0.00000731
Iteration 780/1000 | Loss: 0.00000731
Iteration 781/1000 | Loss: 0.00000730
Iteration 782/1000 | Loss: 0.00000730
Iteration 783/1000 | Loss: 0.00000729
Iteration 784/1000 | Loss: 0.00000729
Iteration 785/1000 | Loss: 0.00000728
Iteration 786/1000 | Loss: 0.00000728
Iteration 787/1000 | Loss: 0.00000728
Iteration 788/1000 | Loss: 0.00000727
Iteration 789/1000 | Loss: 0.00000726
Iteration 790/1000 | Loss: 0.00000726
Iteration 791/1000 | Loss: 0.00000726
Iteration 792/1000 | Loss: 0.00000725
Iteration 793/1000 | Loss: 0.00000725
Iteration 794/1000 | Loss: 0.00000725
Iteration 795/1000 | Loss: 0.00000724
Iteration 796/1000 | Loss: 0.00000724
Iteration 797/1000 | Loss: 0.00000724
Iteration 798/1000 | Loss: 0.00000724
Iteration 799/1000 | Loss: 0.00000722
Iteration 800/1000 | Loss: 0.00000722
Iteration 801/1000 | Loss: 0.00000721
Iteration 802/1000 | Loss: 0.00000720
Iteration 803/1000 | Loss: 0.00000719
Iteration 804/1000 | Loss: 0.00000719
Iteration 805/1000 | Loss: 0.00000719
Iteration 806/1000 | Loss: 0.00000718
Iteration 807/1000 | Loss: 0.00000718
Iteration 808/1000 | Loss: 0.00000717
Iteration 809/1000 | Loss: 0.00000716
Iteration 810/1000 | Loss: 0.00000716
Iteration 811/1000 | Loss: 0.00000716
Iteration 812/1000 | Loss: 0.00000716
Iteration 813/1000 | Loss: 0.00000716
Iteration 814/1000 | Loss: 0.00000716
Iteration 815/1000 | Loss: 0.00000716
Iteration 816/1000 | Loss: 0.00000715
Iteration 817/1000 | Loss: 0.00000715
Iteration 818/1000 | Loss: 0.00000715
Iteration 819/1000 | Loss: 0.00000715
Iteration 820/1000 | Loss: 0.00000715
Iteration 821/1000 | Loss: 0.00000715
Iteration 822/1000 | Loss: 0.00000715
Iteration 823/1000 | Loss: 0.00000715
Iteration 824/1000 | Loss: 0.00000714
Iteration 825/1000 | Loss: 0.00000714
Iteration 826/1000 | Loss: 0.00000714
Iteration 827/1000 | Loss: 0.00000714
Iteration 828/1000 | Loss: 0.00000713
Iteration 829/1000 | Loss: 0.00000713
Iteration 830/1000 | Loss: 0.00000713
Iteration 831/1000 | Loss: 0.00000713
Iteration 832/1000 | Loss: 0.00000713
Iteration 833/1000 | Loss: 0.00000713
Iteration 834/1000 | Loss: 0.00000713
Iteration 835/1000 | Loss: 0.00000713
Iteration 836/1000 | Loss: 0.00000713
Iteration 837/1000 | Loss: 0.00000713
Iteration 838/1000 | Loss: 0.00000713
Iteration 839/1000 | Loss: 0.00000713
Iteration 840/1000 | Loss: 0.00000712
Iteration 841/1000 | Loss: 0.00000712
Iteration 842/1000 | Loss: 0.00000712
Iteration 843/1000 | Loss: 0.00000712
Iteration 844/1000 | Loss: 0.00000712
Iteration 845/1000 | Loss: 0.00000712
Iteration 846/1000 | Loss: 0.00000712
Iteration 847/1000 | Loss: 0.00000712
Iteration 848/1000 | Loss: 0.00000712
Iteration 849/1000 | Loss: 0.00000712
Iteration 850/1000 | Loss: 0.00000712
Iteration 851/1000 | Loss: 0.00000711
Iteration 852/1000 | Loss: 0.00000711
Iteration 853/1000 | Loss: 0.00000711
Iteration 854/1000 | Loss: 0.00000711
Iteration 855/1000 | Loss: 0.00000710
Iteration 856/1000 | Loss: 0.00000710
Iteration 857/1000 | Loss: 0.00000710
Iteration 858/1000 | Loss: 0.00000709
Iteration 859/1000 | Loss: 0.00000709
Iteration 860/1000 | Loss: 0.00000709
Iteration 861/1000 | Loss: 0.00000709
Iteration 862/1000 | Loss: 0.00000709
Iteration 863/1000 | Loss: 0.00000709
Iteration 864/1000 | Loss: 0.00000709
Iteration 865/1000 | Loss: 0.00000709
Iteration 866/1000 | Loss: 0.00000709
Iteration 867/1000 | Loss: 0.00000709
Iteration 868/1000 | Loss: 0.00000709
Iteration 869/1000 | Loss: 0.00000708
Iteration 870/1000 | Loss: 0.00000708
Iteration 871/1000 | Loss: 0.00000708
Iteration 872/1000 | Loss: 0.00000707
Iteration 873/1000 | Loss: 0.00000707
Iteration 874/1000 | Loss: 0.00000706
Iteration 875/1000 | Loss: 0.00000706
Iteration 876/1000 | Loss: 0.00000706
Iteration 877/1000 | Loss: 0.00000705
Iteration 878/1000 | Loss: 0.00000705
Iteration 879/1000 | Loss: 0.00000705
Iteration 880/1000 | Loss: 0.00000705
Iteration 881/1000 | Loss: 0.00000705
Iteration 882/1000 | Loss: 0.00000704
Iteration 883/1000 | Loss: 0.00000704
Iteration 884/1000 | Loss: 0.00000704
Iteration 885/1000 | Loss: 0.00000704
Iteration 886/1000 | Loss: 0.00000703
Iteration 887/1000 | Loss: 0.00000703
Iteration 888/1000 | Loss: 0.00000703
Iteration 889/1000 | Loss: 0.00000702
Iteration 890/1000 | Loss: 0.00000702
Iteration 891/1000 | Loss: 0.00000702
Iteration 892/1000 | Loss: 0.00000702
Iteration 893/1000 | Loss: 0.00000702
Iteration 894/1000 | Loss: 0.00000702
Iteration 895/1000 | Loss: 0.00000702
Iteration 896/1000 | Loss: 0.00000701
Iteration 897/1000 | Loss: 0.00000701
Iteration 898/1000 | Loss: 0.00000701
Iteration 899/1000 | Loss: 0.00000701
Iteration 900/1000 | Loss: 0.00000701
Iteration 901/1000 | Loss: 0.00000701
Iteration 902/1000 | Loss: 0.00000701
Iteration 903/1000 | Loss: 0.00000701
Iteration 904/1000 | Loss: 0.00000701
Iteration 905/1000 | Loss: 0.00000701
Iteration 906/1000 | Loss: 0.00000701
Iteration 907/1000 | Loss: 0.00000701
Iteration 908/1000 | Loss: 0.00000701
Iteration 909/1000 | Loss: 0.00000701
Iteration 910/1000 | Loss: 0.00000700
Iteration 911/1000 | Loss: 0.00000700
Iteration 912/1000 | Loss: 0.00000700
Iteration 913/1000 | Loss: 0.00000700
Iteration 914/1000 | Loss: 0.00000700
Iteration 915/1000 | Loss: 0.00000700
Iteration 916/1000 | Loss: 0.00000700
Iteration 917/1000 | Loss: 0.00000700
Iteration 918/1000 | Loss: 0.00000700
Iteration 919/1000 | Loss: 0.00000700
Iteration 920/1000 | Loss: 0.00000700
Iteration 921/1000 | Loss: 0.00000700
Iteration 922/1000 | Loss: 0.00000700
Iteration 923/1000 | Loss: 0.00000699
Iteration 924/1000 | Loss: 0.00000699
Iteration 925/1000 | Loss: 0.00000699
Iteration 926/1000 | Loss: 0.00000699
Iteration 927/1000 | Loss: 0.00000699
Iteration 928/1000 | Loss: 0.00000699
Iteration 929/1000 | Loss: 0.00000699
Iteration 930/1000 | Loss: 0.00000699
Iteration 931/1000 | Loss: 0.00000699
Iteration 932/1000 | Loss: 0.00000699
Iteration 933/1000 | Loss: 0.00000699
Iteration 934/1000 | Loss: 0.00000699
Iteration 935/1000 | Loss: 0.00000699
Iteration 936/1000 | Loss: 0.00000699
Iteration 937/1000 | Loss: 0.00000699
Iteration 938/1000 | Loss: 0.00000698
Iteration 939/1000 | Loss: 0.00000698
Iteration 940/1000 | Loss: 0.00000698
Iteration 941/1000 | Loss: 0.00000698
Iteration 942/1000 | Loss: 0.00000698
Iteration 943/1000 | Loss: 0.00000698
Iteration 944/1000 | Loss: 0.00000698
Iteration 945/1000 | Loss: 0.00000698
Iteration 946/1000 | Loss: 0.00000698
Iteration 947/1000 | Loss: 0.00000698
Iteration 948/1000 | Loss: 0.00000698
Iteration 949/1000 | Loss: 0.00000697
Iteration 950/1000 | Loss: 0.00000697
Iteration 951/1000 | Loss: 0.00000697
Iteration 952/1000 | Loss: 0.00000697
Iteration 953/1000 | Loss: 0.00000697
Iteration 954/1000 | Loss: 0.00000697
Iteration 955/1000 | Loss: 0.00000697
Iteration 956/1000 | Loss: 0.00000697
Iteration 957/1000 | Loss: 0.00000697
Iteration 958/1000 | Loss: 0.00000697
Iteration 959/1000 | Loss: 0.00000697
Iteration 960/1000 | Loss: 0.00000697
Iteration 961/1000 | Loss: 0.00000697
Iteration 962/1000 | Loss: 0.00000697
Iteration 963/1000 | Loss: 0.00000697
Iteration 964/1000 | Loss: 0.00000697
Iteration 965/1000 | Loss: 0.00000697
Iteration 966/1000 | Loss: 0.00000697
Iteration 967/1000 | Loss: 0.00000697
Iteration 968/1000 | Loss: 0.00000697
Iteration 969/1000 | Loss: 0.00000697
Iteration 970/1000 | Loss: 0.00000697
Iteration 971/1000 | Loss: 0.00000697
Iteration 972/1000 | Loss: 0.00000697
Iteration 973/1000 | Loss: 0.00000697
Iteration 974/1000 | Loss: 0.00000697
Iteration 975/1000 | Loss: 0.00000697
Iteration 976/1000 | Loss: 0.00000697
Iteration 977/1000 | Loss: 0.00000697
Iteration 978/1000 | Loss: 0.00000697
Iteration 979/1000 | Loss: 0.00000697
Iteration 980/1000 | Loss: 0.00000697
Iteration 981/1000 | Loss: 0.00000697
Iteration 982/1000 | Loss: 0.00000697
Iteration 983/1000 | Loss: 0.00000697
Iteration 984/1000 | Loss: 0.00000697
Iteration 985/1000 | Loss: 0.00000697
Iteration 986/1000 | Loss: 0.00000697
Iteration 987/1000 | Loss: 0.00000697
Iteration 988/1000 | Loss: 0.00000697
Iteration 989/1000 | Loss: 0.00000697
Iteration 990/1000 | Loss: 0.00000697
Iteration 991/1000 | Loss: 0.00000697
Iteration 992/1000 | Loss: 0.00000697
Iteration 993/1000 | Loss: 0.00000697
Iteration 994/1000 | Loss: 0.00000697
Iteration 995/1000 | Loss: 0.00000697
Iteration 996/1000 | Loss: 0.00000697
Iteration 997/1000 | Loss: 0.00000697
Iteration 998/1000 | Loss: 0.00000697
Iteration 999/1000 | Loss: 0.00000697
Iteration 1000/1000 | Loss: 0.00000697

Optimization complete. Final v2v error: 2.3154072761535645 mm

Highest mean error: 2.9085915088653564 mm for frame 73

Lowest mean error: 2.2251129150390625 mm for frame 9

Saving results

Total time: 1205.2600936889648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872695
Iteration 2/25 | Loss: 0.00160721
Iteration 3/25 | Loss: 0.00129747
Iteration 4/25 | Loss: 0.00124743
Iteration 5/25 | Loss: 0.00123837
Iteration 6/25 | Loss: 0.00123488
Iteration 7/25 | Loss: 0.00123235
Iteration 8/25 | Loss: 0.00123112
Iteration 9/25 | Loss: 0.00123012
Iteration 10/25 | Loss: 0.00122947
Iteration 11/25 | Loss: 0.00122884
Iteration 12/25 | Loss: 0.00122848
Iteration 13/25 | Loss: 0.00122834
Iteration 14/25 | Loss: 0.00122834
Iteration 15/25 | Loss: 0.00122834
Iteration 16/25 | Loss: 0.00122834
Iteration 17/25 | Loss: 0.00122834
Iteration 18/25 | Loss: 0.00122834
Iteration 19/25 | Loss: 0.00122834
Iteration 20/25 | Loss: 0.00122834
Iteration 21/25 | Loss: 0.00122834
Iteration 22/25 | Loss: 0.00122834
Iteration 23/25 | Loss: 0.00122834
Iteration 24/25 | Loss: 0.00122834
Iteration 25/25 | Loss: 0.00122834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94128788
Iteration 2/25 | Loss: 0.00203408
Iteration 3/25 | Loss: 0.00200940
Iteration 4/25 | Loss: 0.00200940
Iteration 5/25 | Loss: 0.00200940
Iteration 6/25 | Loss: 0.00200940
Iteration 7/25 | Loss: 0.00200940
Iteration 8/25 | Loss: 0.00200939
Iteration 9/25 | Loss: 0.00200939
Iteration 10/25 | Loss: 0.00200939
Iteration 11/25 | Loss: 0.00200939
Iteration 12/25 | Loss: 0.00200939
Iteration 13/25 | Loss: 0.00200939
Iteration 14/25 | Loss: 0.00200939
Iteration 15/25 | Loss: 0.00200939
Iteration 16/25 | Loss: 0.00200939
Iteration 17/25 | Loss: 0.00200939
Iteration 18/25 | Loss: 0.00200939
Iteration 19/25 | Loss: 0.00200939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002009393647313118, 0.002009393647313118, 0.002009393647313118, 0.002009393647313118, 0.002009393647313118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002009393647313118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200939
Iteration 2/1000 | Loss: 0.00004614
Iteration 3/1000 | Loss: 0.00001425
Iteration 4/1000 | Loss: 0.00006856
Iteration 5/1000 | Loss: 0.00023282
Iteration 6/1000 | Loss: 0.00036837
Iteration 7/1000 | Loss: 0.00003896
Iteration 8/1000 | Loss: 0.00002733
Iteration 9/1000 | Loss: 0.00001768
Iteration 10/1000 | Loss: 0.00002646
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001191
Iteration 13/1000 | Loss: 0.00001153
Iteration 14/1000 | Loss: 0.00001121
Iteration 15/1000 | Loss: 0.00001111
Iteration 16/1000 | Loss: 0.00001090
Iteration 17/1000 | Loss: 0.00001068
Iteration 18/1000 | Loss: 0.00001065
Iteration 19/1000 | Loss: 0.00001061
Iteration 20/1000 | Loss: 0.00003860
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001045
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001040
Iteration 25/1000 | Loss: 0.00001036
Iteration 26/1000 | Loss: 0.00001035
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001034
Iteration 30/1000 | Loss: 0.00001034
Iteration 31/1000 | Loss: 0.00001033
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001032
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001032
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001030
Iteration 42/1000 | Loss: 0.00001030
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001028
Iteration 46/1000 | Loss: 0.00001028
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001027
Iteration 53/1000 | Loss: 0.00001027
Iteration 54/1000 | Loss: 0.00001026
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001025
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001022
Iteration 62/1000 | Loss: 0.00001022
Iteration 63/1000 | Loss: 0.00001022
Iteration 64/1000 | Loss: 0.00001021
Iteration 65/1000 | Loss: 0.00001021
Iteration 66/1000 | Loss: 0.00001018
Iteration 67/1000 | Loss: 0.00004544
Iteration 68/1000 | Loss: 0.00002984
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00003974
Iteration 71/1000 | Loss: 0.00001170
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001017
Iteration 75/1000 | Loss: 0.00002145
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001011
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001008
Iteration 82/1000 | Loss: 0.00001007
Iteration 83/1000 | Loss: 0.00001007
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001006
Iteration 86/1000 | Loss: 0.00001006
Iteration 87/1000 | Loss: 0.00001006
Iteration 88/1000 | Loss: 0.00001006
Iteration 89/1000 | Loss: 0.00001006
Iteration 90/1000 | Loss: 0.00001005
Iteration 91/1000 | Loss: 0.00001005
Iteration 92/1000 | Loss: 0.00001005
Iteration 93/1000 | Loss: 0.00001005
Iteration 94/1000 | Loss: 0.00001005
Iteration 95/1000 | Loss: 0.00001005
Iteration 96/1000 | Loss: 0.00001004
Iteration 97/1000 | Loss: 0.00001004
Iteration 98/1000 | Loss: 0.00001004
Iteration 99/1000 | Loss: 0.00001003
Iteration 100/1000 | Loss: 0.00001003
Iteration 101/1000 | Loss: 0.00001003
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001003
Iteration 108/1000 | Loss: 0.00001003
Iteration 109/1000 | Loss: 0.00001003
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001003
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001003
Iteration 121/1000 | Loss: 0.00001003
Iteration 122/1000 | Loss: 0.00001003
Iteration 123/1000 | Loss: 0.00001003
Iteration 124/1000 | Loss: 0.00001003
Iteration 125/1000 | Loss: 0.00001003
Iteration 126/1000 | Loss: 0.00001003
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001003
Iteration 129/1000 | Loss: 0.00001003
Iteration 130/1000 | Loss: 0.00001003
Iteration 131/1000 | Loss: 0.00001003
Iteration 132/1000 | Loss: 0.00001003
Iteration 133/1000 | Loss: 0.00001003
Iteration 134/1000 | Loss: 0.00001003
Iteration 135/1000 | Loss: 0.00001003
Iteration 136/1000 | Loss: 0.00001003
Iteration 137/1000 | Loss: 0.00001003
Iteration 138/1000 | Loss: 0.00001003
Iteration 139/1000 | Loss: 0.00001003
Iteration 140/1000 | Loss: 0.00001003
Iteration 141/1000 | Loss: 0.00001003
Iteration 142/1000 | Loss: 0.00001003
Iteration 143/1000 | Loss: 0.00001003
Iteration 144/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.0025531082646921e-05, 1.0025531082646921e-05, 1.0025531082646921e-05, 1.0025531082646921e-05, 1.0025531082646921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0025531082646921e-05

Optimization complete. Final v2v error: 2.7549960613250732 mm

Highest mean error: 3.3089563846588135 mm for frame 40

Lowest mean error: 2.515249252319336 mm for frame 230

Saving results

Total time: 81.90420722961426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484667
Iteration 2/25 | Loss: 0.00140465
Iteration 3/25 | Loss: 0.00130929
Iteration 4/25 | Loss: 0.00129169
Iteration 5/25 | Loss: 0.00128766
Iteration 6/25 | Loss: 0.00128722
Iteration 7/25 | Loss: 0.00128722
Iteration 8/25 | Loss: 0.00128722
Iteration 9/25 | Loss: 0.00128722
Iteration 10/25 | Loss: 0.00128722
Iteration 11/25 | Loss: 0.00128722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012872172519564629, 0.0012872172519564629, 0.0012872172519564629, 0.0012872172519564629, 0.0012872172519564629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012872172519564629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22894263
Iteration 2/25 | Loss: 0.00215974
Iteration 3/25 | Loss: 0.00215973
Iteration 4/25 | Loss: 0.00215973
Iteration 5/25 | Loss: 0.00215973
Iteration 6/25 | Loss: 0.00215973
Iteration 7/25 | Loss: 0.00215973
Iteration 8/25 | Loss: 0.00215973
Iteration 9/25 | Loss: 0.00215972
Iteration 10/25 | Loss: 0.00215972
Iteration 11/25 | Loss: 0.00215972
Iteration 12/25 | Loss: 0.00215972
Iteration 13/25 | Loss: 0.00215972
Iteration 14/25 | Loss: 0.00215972
Iteration 15/25 | Loss: 0.00215972
Iteration 16/25 | Loss: 0.00215972
Iteration 17/25 | Loss: 0.00215972
Iteration 18/25 | Loss: 0.00215972
Iteration 19/25 | Loss: 0.00215972
Iteration 20/25 | Loss: 0.00215972
Iteration 21/25 | Loss: 0.00215972
Iteration 22/25 | Loss: 0.00215972
Iteration 23/25 | Loss: 0.00215972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002159723313525319, 0.002159723313525319, 0.002159723313525319, 0.002159723313525319, 0.002159723313525319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002159723313525319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215972
Iteration 2/1000 | Loss: 0.00003452
Iteration 3/1000 | Loss: 0.00002778
Iteration 4/1000 | Loss: 0.00002512
Iteration 5/1000 | Loss: 0.00002384
Iteration 6/1000 | Loss: 0.00002316
Iteration 7/1000 | Loss: 0.00002250
Iteration 8/1000 | Loss: 0.00002217
Iteration 9/1000 | Loss: 0.00002178
Iteration 10/1000 | Loss: 0.00002153
Iteration 11/1000 | Loss: 0.00002140
Iteration 12/1000 | Loss: 0.00002130
Iteration 13/1000 | Loss: 0.00002121
Iteration 14/1000 | Loss: 0.00002120
Iteration 15/1000 | Loss: 0.00002105
Iteration 16/1000 | Loss: 0.00002095
Iteration 17/1000 | Loss: 0.00002092
Iteration 18/1000 | Loss: 0.00002092
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002079
Iteration 21/1000 | Loss: 0.00002079
Iteration 22/1000 | Loss: 0.00002079
Iteration 23/1000 | Loss: 0.00002079
Iteration 24/1000 | Loss: 0.00002074
Iteration 25/1000 | Loss: 0.00002073
Iteration 26/1000 | Loss: 0.00002071
Iteration 27/1000 | Loss: 0.00002066
Iteration 28/1000 | Loss: 0.00002063
Iteration 29/1000 | Loss: 0.00002063
Iteration 30/1000 | Loss: 0.00002063
Iteration 31/1000 | Loss: 0.00002063
Iteration 32/1000 | Loss: 0.00002061
Iteration 33/1000 | Loss: 0.00002060
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002057
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002049
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002045
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002044
Iteration 47/1000 | Loss: 0.00002044
Iteration 48/1000 | Loss: 0.00002044
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002042
Iteration 51/1000 | Loss: 0.00002042
Iteration 52/1000 | Loss: 0.00002041
Iteration 53/1000 | Loss: 0.00002041
Iteration 54/1000 | Loss: 0.00002041
Iteration 55/1000 | Loss: 0.00002041
Iteration 56/1000 | Loss: 0.00002041
Iteration 57/1000 | Loss: 0.00002040
Iteration 58/1000 | Loss: 0.00002040
Iteration 59/1000 | Loss: 0.00002040
Iteration 60/1000 | Loss: 0.00002040
Iteration 61/1000 | Loss: 0.00002040
Iteration 62/1000 | Loss: 0.00002040
Iteration 63/1000 | Loss: 0.00002040
Iteration 64/1000 | Loss: 0.00002040
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002038
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002038
Iteration 71/1000 | Loss: 0.00002038
Iteration 72/1000 | Loss: 0.00002037
Iteration 73/1000 | Loss: 0.00002037
Iteration 74/1000 | Loss: 0.00002037
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002033
Iteration 84/1000 | Loss: 0.00002033
Iteration 85/1000 | Loss: 0.00002032
Iteration 86/1000 | Loss: 0.00002032
Iteration 87/1000 | Loss: 0.00002032
Iteration 88/1000 | Loss: 0.00002032
Iteration 89/1000 | Loss: 0.00002032
Iteration 90/1000 | Loss: 0.00002032
Iteration 91/1000 | Loss: 0.00002032
Iteration 92/1000 | Loss: 0.00002032
Iteration 93/1000 | Loss: 0.00002031
Iteration 94/1000 | Loss: 0.00002031
Iteration 95/1000 | Loss: 0.00002031
Iteration 96/1000 | Loss: 0.00002031
Iteration 97/1000 | Loss: 0.00002030
Iteration 98/1000 | Loss: 0.00002030
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002030
Iteration 101/1000 | Loss: 0.00002030
Iteration 102/1000 | Loss: 0.00002029
Iteration 103/1000 | Loss: 0.00002029
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002028
Iteration 109/1000 | Loss: 0.00002028
Iteration 110/1000 | Loss: 0.00002028
Iteration 111/1000 | Loss: 0.00002028
Iteration 112/1000 | Loss: 0.00002028
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002027
Iteration 116/1000 | Loss: 0.00002027
Iteration 117/1000 | Loss: 0.00002027
Iteration 118/1000 | Loss: 0.00002027
Iteration 119/1000 | Loss: 0.00002027
Iteration 120/1000 | Loss: 0.00002027
Iteration 121/1000 | Loss: 0.00002027
Iteration 122/1000 | Loss: 0.00002027
Iteration 123/1000 | Loss: 0.00002027
Iteration 124/1000 | Loss: 0.00002027
Iteration 125/1000 | Loss: 0.00002027
Iteration 126/1000 | Loss: 0.00002026
Iteration 127/1000 | Loss: 0.00002026
Iteration 128/1000 | Loss: 0.00002026
Iteration 129/1000 | Loss: 0.00002026
Iteration 130/1000 | Loss: 0.00002026
Iteration 131/1000 | Loss: 0.00002026
Iteration 132/1000 | Loss: 0.00002026
Iteration 133/1000 | Loss: 0.00002026
Iteration 134/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.026170477620326e-05, 2.026170477620326e-05, 2.026170477620326e-05, 2.026170477620326e-05, 2.026170477620326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.026170477620326e-05

Optimization complete. Final v2v error: 3.5170021057128906 mm

Highest mean error: 4.573798179626465 mm for frame 76

Lowest mean error: 3.0614867210388184 mm for frame 42

Saving results

Total time: 40.96106767654419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798499
Iteration 2/25 | Loss: 0.00197935
Iteration 3/25 | Loss: 0.00154643
Iteration 4/25 | Loss: 0.00148601
Iteration 5/25 | Loss: 0.00147310
Iteration 6/25 | Loss: 0.00143395
Iteration 7/25 | Loss: 0.00141845
Iteration 8/25 | Loss: 0.00140271
Iteration 9/25 | Loss: 0.00138526
Iteration 10/25 | Loss: 0.00138327
Iteration 11/25 | Loss: 0.00138280
Iteration 12/25 | Loss: 0.00138263
Iteration 13/25 | Loss: 0.00138253
Iteration 14/25 | Loss: 0.00138245
Iteration 15/25 | Loss: 0.00138244
Iteration 16/25 | Loss: 0.00138244
Iteration 17/25 | Loss: 0.00138244
Iteration 18/25 | Loss: 0.00138244
Iteration 19/25 | Loss: 0.00138244
Iteration 20/25 | Loss: 0.00138244
Iteration 21/25 | Loss: 0.00138244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001382438000291586, 0.001382438000291586, 0.001382438000291586, 0.001382438000291586, 0.001382438000291586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001382438000291586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45750403
Iteration 2/25 | Loss: 0.00171940
Iteration 3/25 | Loss: 0.00171938
Iteration 4/25 | Loss: 0.00171938
Iteration 5/25 | Loss: 0.00171938
Iteration 6/25 | Loss: 0.00171937
Iteration 7/25 | Loss: 0.00171937
Iteration 8/25 | Loss: 0.00171937
Iteration 9/25 | Loss: 0.00171937
Iteration 10/25 | Loss: 0.00171937
Iteration 11/25 | Loss: 0.00171937
Iteration 12/25 | Loss: 0.00171937
Iteration 13/25 | Loss: 0.00171937
Iteration 14/25 | Loss: 0.00171937
Iteration 15/25 | Loss: 0.00171937
Iteration 16/25 | Loss: 0.00171937
Iteration 17/25 | Loss: 0.00171937
Iteration 18/25 | Loss: 0.00171937
Iteration 19/25 | Loss: 0.00171937
Iteration 20/25 | Loss: 0.00171937
Iteration 21/25 | Loss: 0.00171937
Iteration 22/25 | Loss: 0.00171937
Iteration 23/25 | Loss: 0.00171937
Iteration 24/25 | Loss: 0.00171937
Iteration 25/25 | Loss: 0.00171937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171937
Iteration 2/1000 | Loss: 0.00004392
Iteration 3/1000 | Loss: 0.00003135
Iteration 4/1000 | Loss: 0.00002862
Iteration 5/1000 | Loss: 0.00002741
Iteration 6/1000 | Loss: 0.00002650
Iteration 7/1000 | Loss: 0.00002602
Iteration 8/1000 | Loss: 0.00002564
Iteration 9/1000 | Loss: 0.00002529
Iteration 10/1000 | Loss: 0.00002503
Iteration 11/1000 | Loss: 0.00002484
Iteration 12/1000 | Loss: 0.00002466
Iteration 13/1000 | Loss: 0.00002466
Iteration 14/1000 | Loss: 0.00002465
Iteration 15/1000 | Loss: 0.00002455
Iteration 16/1000 | Loss: 0.00002448
Iteration 17/1000 | Loss: 0.00002444
Iteration 18/1000 | Loss: 0.00002438
Iteration 19/1000 | Loss: 0.00002434
Iteration 20/1000 | Loss: 0.00002432
Iteration 21/1000 | Loss: 0.00002431
Iteration 22/1000 | Loss: 0.00002430
Iteration 23/1000 | Loss: 0.00002430
Iteration 24/1000 | Loss: 0.00002426
Iteration 25/1000 | Loss: 0.00002426
Iteration 26/1000 | Loss: 0.00002426
Iteration 27/1000 | Loss: 0.00002426
Iteration 28/1000 | Loss: 0.00002426
Iteration 29/1000 | Loss: 0.00002426
Iteration 30/1000 | Loss: 0.00002426
Iteration 31/1000 | Loss: 0.00002425
Iteration 32/1000 | Loss: 0.00002425
Iteration 33/1000 | Loss: 0.00002425
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002424
Iteration 36/1000 | Loss: 0.00002423
Iteration 37/1000 | Loss: 0.00002422
Iteration 38/1000 | Loss: 0.00002422
Iteration 39/1000 | Loss: 0.00002422
Iteration 40/1000 | Loss: 0.00002421
Iteration 41/1000 | Loss: 0.00002420
Iteration 42/1000 | Loss: 0.00002418
Iteration 43/1000 | Loss: 0.00002418
Iteration 44/1000 | Loss: 0.00002418
Iteration 45/1000 | Loss: 0.00002418
Iteration 46/1000 | Loss: 0.00002418
Iteration 47/1000 | Loss: 0.00002417
Iteration 48/1000 | Loss: 0.00002413
Iteration 49/1000 | Loss: 0.00002413
Iteration 50/1000 | Loss: 0.00002409
Iteration 51/1000 | Loss: 0.00002409
Iteration 52/1000 | Loss: 0.00002409
Iteration 53/1000 | Loss: 0.00002408
Iteration 54/1000 | Loss: 0.00002408
Iteration 55/1000 | Loss: 0.00002408
Iteration 56/1000 | Loss: 0.00002408
Iteration 57/1000 | Loss: 0.00002408
Iteration 58/1000 | Loss: 0.00002405
Iteration 59/1000 | Loss: 0.00002405
Iteration 60/1000 | Loss: 0.00002405
Iteration 61/1000 | Loss: 0.00002405
Iteration 62/1000 | Loss: 0.00002405
Iteration 63/1000 | Loss: 0.00002405
Iteration 64/1000 | Loss: 0.00002404
Iteration 65/1000 | Loss: 0.00002404
Iteration 66/1000 | Loss: 0.00002404
Iteration 67/1000 | Loss: 0.00002404
Iteration 68/1000 | Loss: 0.00002404
Iteration 69/1000 | Loss: 0.00002404
Iteration 70/1000 | Loss: 0.00002403
Iteration 71/1000 | Loss: 0.00002403
Iteration 72/1000 | Loss: 0.00002403
Iteration 73/1000 | Loss: 0.00002403
Iteration 74/1000 | Loss: 0.00002403
Iteration 75/1000 | Loss: 0.00002403
Iteration 76/1000 | Loss: 0.00002403
Iteration 77/1000 | Loss: 0.00002403
Iteration 78/1000 | Loss: 0.00002403
Iteration 79/1000 | Loss: 0.00002403
Iteration 80/1000 | Loss: 0.00002403
Iteration 81/1000 | Loss: 0.00002403
Iteration 82/1000 | Loss: 0.00002403
Iteration 83/1000 | Loss: 0.00002403
Iteration 84/1000 | Loss: 0.00002403
Iteration 85/1000 | Loss: 0.00002403
Iteration 86/1000 | Loss: 0.00002403
Iteration 87/1000 | Loss: 0.00002401
Iteration 88/1000 | Loss: 0.00002401
Iteration 89/1000 | Loss: 0.00002401
Iteration 90/1000 | Loss: 0.00002401
Iteration 91/1000 | Loss: 0.00002401
Iteration 92/1000 | Loss: 0.00002400
Iteration 93/1000 | Loss: 0.00002400
Iteration 94/1000 | Loss: 0.00002400
Iteration 95/1000 | Loss: 0.00002400
Iteration 96/1000 | Loss: 0.00002400
Iteration 97/1000 | Loss: 0.00002400
Iteration 98/1000 | Loss: 0.00002399
Iteration 99/1000 | Loss: 0.00002399
Iteration 100/1000 | Loss: 0.00002399
Iteration 101/1000 | Loss: 0.00002398
Iteration 102/1000 | Loss: 0.00002398
Iteration 103/1000 | Loss: 0.00002398
Iteration 104/1000 | Loss: 0.00002398
Iteration 105/1000 | Loss: 0.00002398
Iteration 106/1000 | Loss: 0.00002398
Iteration 107/1000 | Loss: 0.00002397
Iteration 108/1000 | Loss: 0.00002397
Iteration 109/1000 | Loss: 0.00002397
Iteration 110/1000 | Loss: 0.00002397
Iteration 111/1000 | Loss: 0.00002397
Iteration 112/1000 | Loss: 0.00002396
Iteration 113/1000 | Loss: 0.00002396
Iteration 114/1000 | Loss: 0.00002396
Iteration 115/1000 | Loss: 0.00002396
Iteration 116/1000 | Loss: 0.00002395
Iteration 117/1000 | Loss: 0.00002395
Iteration 118/1000 | Loss: 0.00002395
Iteration 119/1000 | Loss: 0.00002395
Iteration 120/1000 | Loss: 0.00002394
Iteration 121/1000 | Loss: 0.00002394
Iteration 122/1000 | Loss: 0.00002394
Iteration 123/1000 | Loss: 0.00002393
Iteration 124/1000 | Loss: 0.00002393
Iteration 125/1000 | Loss: 0.00002393
Iteration 126/1000 | Loss: 0.00002393
Iteration 127/1000 | Loss: 0.00002393
Iteration 128/1000 | Loss: 0.00002393
Iteration 129/1000 | Loss: 0.00002392
Iteration 130/1000 | Loss: 0.00002392
Iteration 131/1000 | Loss: 0.00002392
Iteration 132/1000 | Loss: 0.00002392
Iteration 133/1000 | Loss: 0.00002392
Iteration 134/1000 | Loss: 0.00002392
Iteration 135/1000 | Loss: 0.00002392
Iteration 136/1000 | Loss: 0.00002392
Iteration 137/1000 | Loss: 0.00002392
Iteration 138/1000 | Loss: 0.00002392
Iteration 139/1000 | Loss: 0.00002392
Iteration 140/1000 | Loss: 0.00002392
Iteration 141/1000 | Loss: 0.00002392
Iteration 142/1000 | Loss: 0.00002391
Iteration 143/1000 | Loss: 0.00002391
Iteration 144/1000 | Loss: 0.00002391
Iteration 145/1000 | Loss: 0.00002391
Iteration 146/1000 | Loss: 0.00002391
Iteration 147/1000 | Loss: 0.00002391
Iteration 148/1000 | Loss: 0.00002391
Iteration 149/1000 | Loss: 0.00002391
Iteration 150/1000 | Loss: 0.00002391
Iteration 151/1000 | Loss: 0.00002391
Iteration 152/1000 | Loss: 0.00002391
Iteration 153/1000 | Loss: 0.00002391
Iteration 154/1000 | Loss: 0.00002391
Iteration 155/1000 | Loss: 0.00002391
Iteration 156/1000 | Loss: 0.00002391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.391463021922391e-05, 2.391463021922391e-05, 2.391463021922391e-05, 2.391463021922391e-05, 2.391463021922391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.391463021922391e-05

Optimization complete. Final v2v error: 3.984123468399048 mm

Highest mean error: 5.935595512390137 mm for frame 188

Lowest mean error: 2.8288605213165283 mm for frame 110

Saving results

Total time: 63.95875358581543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455998
Iteration 2/25 | Loss: 0.00134357
Iteration 3/25 | Loss: 0.00125634
Iteration 4/25 | Loss: 0.00124534
Iteration 5/25 | Loss: 0.00124312
Iteration 6/25 | Loss: 0.00124306
Iteration 7/25 | Loss: 0.00124306
Iteration 8/25 | Loss: 0.00124306
Iteration 9/25 | Loss: 0.00124306
Iteration 10/25 | Loss: 0.00124306
Iteration 11/25 | Loss: 0.00124306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012430576607584953, 0.0012430576607584953, 0.0012430576607584953, 0.0012430576607584953, 0.0012430576607584953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012430576607584953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24830592
Iteration 2/25 | Loss: 0.00226557
Iteration 3/25 | Loss: 0.00226556
Iteration 4/25 | Loss: 0.00226556
Iteration 5/25 | Loss: 0.00226556
Iteration 6/25 | Loss: 0.00226556
Iteration 7/25 | Loss: 0.00226556
Iteration 8/25 | Loss: 0.00226555
Iteration 9/25 | Loss: 0.00226555
Iteration 10/25 | Loss: 0.00226555
Iteration 11/25 | Loss: 0.00226555
Iteration 12/25 | Loss: 0.00226555
Iteration 13/25 | Loss: 0.00226555
Iteration 14/25 | Loss: 0.00226555
Iteration 15/25 | Loss: 0.00226555
Iteration 16/25 | Loss: 0.00226555
Iteration 17/25 | Loss: 0.00226555
Iteration 18/25 | Loss: 0.00226555
Iteration 19/25 | Loss: 0.00226555
Iteration 20/25 | Loss: 0.00226555
Iteration 21/25 | Loss: 0.00226555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022655543871223927, 0.0022655543871223927, 0.0022655543871223927, 0.0022655543871223927, 0.0022655543871223927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022655543871223927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226555
Iteration 2/1000 | Loss: 0.00002754
Iteration 3/1000 | Loss: 0.00001570
Iteration 4/1000 | Loss: 0.00001324
Iteration 5/1000 | Loss: 0.00001210
Iteration 6/1000 | Loss: 0.00001154
Iteration 7/1000 | Loss: 0.00001100
Iteration 8/1000 | Loss: 0.00001063
Iteration 9/1000 | Loss: 0.00001049
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001010
Iteration 12/1000 | Loss: 0.00000996
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000992
Iteration 15/1000 | Loss: 0.00000991
Iteration 16/1000 | Loss: 0.00000990
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000984
Iteration 19/1000 | Loss: 0.00000983
Iteration 20/1000 | Loss: 0.00000978
Iteration 21/1000 | Loss: 0.00000978
Iteration 22/1000 | Loss: 0.00000977
Iteration 23/1000 | Loss: 0.00000975
Iteration 24/1000 | Loss: 0.00000974
Iteration 25/1000 | Loss: 0.00000974
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000968
Iteration 28/1000 | Loss: 0.00000968
Iteration 29/1000 | Loss: 0.00000968
Iteration 30/1000 | Loss: 0.00000966
Iteration 31/1000 | Loss: 0.00000966
Iteration 32/1000 | Loss: 0.00000966
Iteration 33/1000 | Loss: 0.00000966
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000966
Iteration 38/1000 | Loss: 0.00000966
Iteration 39/1000 | Loss: 0.00000965
Iteration 40/1000 | Loss: 0.00000965
Iteration 41/1000 | Loss: 0.00000965
Iteration 42/1000 | Loss: 0.00000964
Iteration 43/1000 | Loss: 0.00000963
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000962
Iteration 46/1000 | Loss: 0.00000962
Iteration 47/1000 | Loss: 0.00000961
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000956
Iteration 51/1000 | Loss: 0.00000955
Iteration 52/1000 | Loss: 0.00000954
Iteration 53/1000 | Loss: 0.00000953
Iteration 54/1000 | Loss: 0.00000953
Iteration 55/1000 | Loss: 0.00000952
Iteration 56/1000 | Loss: 0.00000951
Iteration 57/1000 | Loss: 0.00000951
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000950
Iteration 60/1000 | Loss: 0.00000950
Iteration 61/1000 | Loss: 0.00000950
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000948
Iteration 66/1000 | Loss: 0.00000948
Iteration 67/1000 | Loss: 0.00000948
Iteration 68/1000 | Loss: 0.00000948
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000947
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000946
Iteration 75/1000 | Loss: 0.00000945
Iteration 76/1000 | Loss: 0.00000945
Iteration 77/1000 | Loss: 0.00000945
Iteration 78/1000 | Loss: 0.00000944
Iteration 79/1000 | Loss: 0.00000944
Iteration 80/1000 | Loss: 0.00000944
Iteration 81/1000 | Loss: 0.00000943
Iteration 82/1000 | Loss: 0.00000943
Iteration 83/1000 | Loss: 0.00000943
Iteration 84/1000 | Loss: 0.00000942
Iteration 85/1000 | Loss: 0.00000942
Iteration 86/1000 | Loss: 0.00000942
Iteration 87/1000 | Loss: 0.00000942
Iteration 88/1000 | Loss: 0.00000942
Iteration 89/1000 | Loss: 0.00000941
Iteration 90/1000 | Loss: 0.00000941
Iteration 91/1000 | Loss: 0.00000940
Iteration 92/1000 | Loss: 0.00000939
Iteration 93/1000 | Loss: 0.00000939
Iteration 94/1000 | Loss: 0.00000939
Iteration 95/1000 | Loss: 0.00000939
Iteration 96/1000 | Loss: 0.00000938
Iteration 97/1000 | Loss: 0.00000938
Iteration 98/1000 | Loss: 0.00000938
Iteration 99/1000 | Loss: 0.00000938
Iteration 100/1000 | Loss: 0.00000937
Iteration 101/1000 | Loss: 0.00000936
Iteration 102/1000 | Loss: 0.00000935
Iteration 103/1000 | Loss: 0.00000935
Iteration 104/1000 | Loss: 0.00000935
Iteration 105/1000 | Loss: 0.00000934
Iteration 106/1000 | Loss: 0.00000934
Iteration 107/1000 | Loss: 0.00000933
Iteration 108/1000 | Loss: 0.00000933
Iteration 109/1000 | Loss: 0.00000932
Iteration 110/1000 | Loss: 0.00000932
Iteration 111/1000 | Loss: 0.00000932
Iteration 112/1000 | Loss: 0.00000932
Iteration 113/1000 | Loss: 0.00000932
Iteration 114/1000 | Loss: 0.00000932
Iteration 115/1000 | Loss: 0.00000932
Iteration 116/1000 | Loss: 0.00000931
Iteration 117/1000 | Loss: 0.00000931
Iteration 118/1000 | Loss: 0.00000931
Iteration 119/1000 | Loss: 0.00000931
Iteration 120/1000 | Loss: 0.00000931
Iteration 121/1000 | Loss: 0.00000931
Iteration 122/1000 | Loss: 0.00000931
Iteration 123/1000 | Loss: 0.00000931
Iteration 124/1000 | Loss: 0.00000931
Iteration 125/1000 | Loss: 0.00000930
Iteration 126/1000 | Loss: 0.00000930
Iteration 127/1000 | Loss: 0.00000929
Iteration 128/1000 | Loss: 0.00000929
Iteration 129/1000 | Loss: 0.00000929
Iteration 130/1000 | Loss: 0.00000929
Iteration 131/1000 | Loss: 0.00000929
Iteration 132/1000 | Loss: 0.00000929
Iteration 133/1000 | Loss: 0.00000929
Iteration 134/1000 | Loss: 0.00000928
Iteration 135/1000 | Loss: 0.00000928
Iteration 136/1000 | Loss: 0.00000928
Iteration 137/1000 | Loss: 0.00000928
Iteration 138/1000 | Loss: 0.00000927
Iteration 139/1000 | Loss: 0.00000927
Iteration 140/1000 | Loss: 0.00000927
Iteration 141/1000 | Loss: 0.00000927
Iteration 142/1000 | Loss: 0.00000927
Iteration 143/1000 | Loss: 0.00000927
Iteration 144/1000 | Loss: 0.00000927
Iteration 145/1000 | Loss: 0.00000927
Iteration 146/1000 | Loss: 0.00000927
Iteration 147/1000 | Loss: 0.00000926
Iteration 148/1000 | Loss: 0.00000926
Iteration 149/1000 | Loss: 0.00000926
Iteration 150/1000 | Loss: 0.00000926
Iteration 151/1000 | Loss: 0.00000926
Iteration 152/1000 | Loss: 0.00000926
Iteration 153/1000 | Loss: 0.00000926
Iteration 154/1000 | Loss: 0.00000926
Iteration 155/1000 | Loss: 0.00000926
Iteration 156/1000 | Loss: 0.00000926
Iteration 157/1000 | Loss: 0.00000926
Iteration 158/1000 | Loss: 0.00000925
Iteration 159/1000 | Loss: 0.00000925
Iteration 160/1000 | Loss: 0.00000925
Iteration 161/1000 | Loss: 0.00000925
Iteration 162/1000 | Loss: 0.00000924
Iteration 163/1000 | Loss: 0.00000924
Iteration 164/1000 | Loss: 0.00000924
Iteration 165/1000 | Loss: 0.00000923
Iteration 166/1000 | Loss: 0.00000923
Iteration 167/1000 | Loss: 0.00000923
Iteration 168/1000 | Loss: 0.00000923
Iteration 169/1000 | Loss: 0.00000923
Iteration 170/1000 | Loss: 0.00000923
Iteration 171/1000 | Loss: 0.00000923
Iteration 172/1000 | Loss: 0.00000923
Iteration 173/1000 | Loss: 0.00000922
Iteration 174/1000 | Loss: 0.00000922
Iteration 175/1000 | Loss: 0.00000922
Iteration 176/1000 | Loss: 0.00000922
Iteration 177/1000 | Loss: 0.00000922
Iteration 178/1000 | Loss: 0.00000922
Iteration 179/1000 | Loss: 0.00000922
Iteration 180/1000 | Loss: 0.00000922
Iteration 181/1000 | Loss: 0.00000921
Iteration 182/1000 | Loss: 0.00000921
Iteration 183/1000 | Loss: 0.00000921
Iteration 184/1000 | Loss: 0.00000920
Iteration 185/1000 | Loss: 0.00000920
Iteration 186/1000 | Loss: 0.00000920
Iteration 187/1000 | Loss: 0.00000919
Iteration 188/1000 | Loss: 0.00000919
Iteration 189/1000 | Loss: 0.00000919
Iteration 190/1000 | Loss: 0.00000919
Iteration 191/1000 | Loss: 0.00000919
Iteration 192/1000 | Loss: 0.00000919
Iteration 193/1000 | Loss: 0.00000919
Iteration 194/1000 | Loss: 0.00000918
Iteration 195/1000 | Loss: 0.00000918
Iteration 196/1000 | Loss: 0.00000918
Iteration 197/1000 | Loss: 0.00000918
Iteration 198/1000 | Loss: 0.00000918
Iteration 199/1000 | Loss: 0.00000918
Iteration 200/1000 | Loss: 0.00000918
Iteration 201/1000 | Loss: 0.00000918
Iteration 202/1000 | Loss: 0.00000918
Iteration 203/1000 | Loss: 0.00000918
Iteration 204/1000 | Loss: 0.00000918
Iteration 205/1000 | Loss: 0.00000918
Iteration 206/1000 | Loss: 0.00000918
Iteration 207/1000 | Loss: 0.00000918
Iteration 208/1000 | Loss: 0.00000918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [9.18092337087728e-06, 9.18092337087728e-06, 9.18092337087728e-06, 9.18092337087728e-06, 9.18092337087728e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.18092337087728e-06

Optimization complete. Final v2v error: 2.5284829139709473 mm

Highest mean error: 2.9426822662353516 mm for frame 58

Lowest mean error: 2.357125759124756 mm for frame 94

Saving results

Total time: 41.52567648887634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402902
Iteration 2/25 | Loss: 0.00138111
Iteration 3/25 | Loss: 0.00130510
Iteration 4/25 | Loss: 0.00128572
Iteration 5/25 | Loss: 0.00127906
Iteration 6/25 | Loss: 0.00127712
Iteration 7/25 | Loss: 0.00127712
Iteration 8/25 | Loss: 0.00127712
Iteration 9/25 | Loss: 0.00127712
Iteration 10/25 | Loss: 0.00127712
Iteration 11/25 | Loss: 0.00127712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012771205510944128, 0.0012771205510944128, 0.0012771205510944128, 0.0012771205510944128, 0.0012771205510944128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012771205510944128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.53882360
Iteration 2/25 | Loss: 0.00212455
Iteration 3/25 | Loss: 0.00212455
Iteration 4/25 | Loss: 0.00212454
Iteration 5/25 | Loss: 0.00212454
Iteration 6/25 | Loss: 0.00212454
Iteration 7/25 | Loss: 0.00212454
Iteration 8/25 | Loss: 0.00212454
Iteration 9/25 | Loss: 0.00212454
Iteration 10/25 | Loss: 0.00212454
Iteration 11/25 | Loss: 0.00212454
Iteration 12/25 | Loss: 0.00212454
Iteration 13/25 | Loss: 0.00212454
Iteration 14/25 | Loss: 0.00212454
Iteration 15/25 | Loss: 0.00212454
Iteration 16/25 | Loss: 0.00212454
Iteration 17/25 | Loss: 0.00212454
Iteration 18/25 | Loss: 0.00212454
Iteration 19/25 | Loss: 0.00212454
Iteration 20/25 | Loss: 0.00212454
Iteration 21/25 | Loss: 0.00212454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002124539576470852, 0.002124539576470852, 0.002124539576470852, 0.002124539576470852, 0.002124539576470852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002124539576470852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212454
Iteration 2/1000 | Loss: 0.00002820
Iteration 3/1000 | Loss: 0.00002399
Iteration 4/1000 | Loss: 0.00002273
Iteration 5/1000 | Loss: 0.00002193
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002107
Iteration 8/1000 | Loss: 0.00002073
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001970
Iteration 14/1000 | Loss: 0.00001960
Iteration 15/1000 | Loss: 0.00001959
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001954
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001952
Iteration 23/1000 | Loss: 0.00001951
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001944
Iteration 33/1000 | Loss: 0.00001944
Iteration 34/1000 | Loss: 0.00001944
Iteration 35/1000 | Loss: 0.00001944
Iteration 36/1000 | Loss: 0.00001943
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001941
Iteration 45/1000 | Loss: 0.00001941
Iteration 46/1000 | Loss: 0.00001941
Iteration 47/1000 | Loss: 0.00001941
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001935
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001933
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001930
Iteration 79/1000 | Loss: 0.00001930
Iteration 80/1000 | Loss: 0.00001930
Iteration 81/1000 | Loss: 0.00001930
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001928
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001926
Iteration 96/1000 | Loss: 0.00001926
Iteration 97/1000 | Loss: 0.00001925
Iteration 98/1000 | Loss: 0.00001925
Iteration 99/1000 | Loss: 0.00001925
Iteration 100/1000 | Loss: 0.00001925
Iteration 101/1000 | Loss: 0.00001925
Iteration 102/1000 | Loss: 0.00001925
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001924
Iteration 112/1000 | Loss: 0.00001924
Iteration 113/1000 | Loss: 0.00001924
Iteration 114/1000 | Loss: 0.00001924
Iteration 115/1000 | Loss: 0.00001923
Iteration 116/1000 | Loss: 0.00001923
Iteration 117/1000 | Loss: 0.00001923
Iteration 118/1000 | Loss: 0.00001923
Iteration 119/1000 | Loss: 0.00001923
Iteration 120/1000 | Loss: 0.00001923
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001922
Iteration 129/1000 | Loss: 0.00001922
Iteration 130/1000 | Loss: 0.00001922
Iteration 131/1000 | Loss: 0.00001922
Iteration 132/1000 | Loss: 0.00001922
Iteration 133/1000 | Loss: 0.00001922
Iteration 134/1000 | Loss: 0.00001922
Iteration 135/1000 | Loss: 0.00001922
Iteration 136/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.922368392115459e-05, 1.922368392115459e-05, 1.922368392115459e-05, 1.922368392115459e-05, 1.922368392115459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.922368392115459e-05

Optimization complete. Final v2v error: 3.7485034465789795 mm

Highest mean error: 4.052693843841553 mm for frame 74

Lowest mean error: 3.46598744392395 mm for frame 12

Saving results

Total time: 37.762675762176514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793274
Iteration 2/25 | Loss: 0.00128719
Iteration 3/25 | Loss: 0.00122769
Iteration 4/25 | Loss: 0.00122251
Iteration 5/25 | Loss: 0.00122251
Iteration 6/25 | Loss: 0.00122251
Iteration 7/25 | Loss: 0.00122251
Iteration 8/25 | Loss: 0.00122251
Iteration 9/25 | Loss: 0.00122251
Iteration 10/25 | Loss: 0.00122251
Iteration 11/25 | Loss: 0.00122251
Iteration 12/25 | Loss: 0.00122251
Iteration 13/25 | Loss: 0.00122251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001222507213242352, 0.001222507213242352, 0.001222507213242352, 0.001222507213242352, 0.001222507213242352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001222507213242352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23987949
Iteration 2/25 | Loss: 0.00214288
Iteration 3/25 | Loss: 0.00214288
Iteration 4/25 | Loss: 0.00214288
Iteration 5/25 | Loss: 0.00214288
Iteration 6/25 | Loss: 0.00214288
Iteration 7/25 | Loss: 0.00214288
Iteration 8/25 | Loss: 0.00214288
Iteration 9/25 | Loss: 0.00214287
Iteration 10/25 | Loss: 0.00214287
Iteration 11/25 | Loss: 0.00214287
Iteration 12/25 | Loss: 0.00214287
Iteration 13/25 | Loss: 0.00214287
Iteration 14/25 | Loss: 0.00214287
Iteration 15/25 | Loss: 0.00214287
Iteration 16/25 | Loss: 0.00214287
Iteration 17/25 | Loss: 0.00214287
Iteration 18/25 | Loss: 0.00214287
Iteration 19/25 | Loss: 0.00214287
Iteration 20/25 | Loss: 0.00214287
Iteration 21/25 | Loss: 0.00214287
Iteration 22/25 | Loss: 0.00214287
Iteration 23/25 | Loss: 0.00214287
Iteration 24/25 | Loss: 0.00214287
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002142874523997307, 0.002142874523997307, 0.002142874523997307, 0.002142874523997307, 0.002142874523997307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002142874523997307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214287
Iteration 2/1000 | Loss: 0.00002096
Iteration 3/1000 | Loss: 0.00001320
Iteration 4/1000 | Loss: 0.00001161
Iteration 5/1000 | Loss: 0.00001079
Iteration 6/1000 | Loss: 0.00001011
Iteration 7/1000 | Loss: 0.00000973
Iteration 8/1000 | Loss: 0.00000931
Iteration 9/1000 | Loss: 0.00000901
Iteration 10/1000 | Loss: 0.00000887
Iteration 11/1000 | Loss: 0.00000867
Iteration 12/1000 | Loss: 0.00000863
Iteration 13/1000 | Loss: 0.00000858
Iteration 14/1000 | Loss: 0.00000857
Iteration 15/1000 | Loss: 0.00000856
Iteration 16/1000 | Loss: 0.00000856
Iteration 17/1000 | Loss: 0.00000855
Iteration 18/1000 | Loss: 0.00000852
Iteration 19/1000 | Loss: 0.00000847
Iteration 20/1000 | Loss: 0.00000847
Iteration 21/1000 | Loss: 0.00000843
Iteration 22/1000 | Loss: 0.00000842
Iteration 23/1000 | Loss: 0.00000841
Iteration 24/1000 | Loss: 0.00000830
Iteration 25/1000 | Loss: 0.00000828
Iteration 26/1000 | Loss: 0.00000827
Iteration 27/1000 | Loss: 0.00000826
Iteration 28/1000 | Loss: 0.00000825
Iteration 29/1000 | Loss: 0.00000824
Iteration 30/1000 | Loss: 0.00000823
Iteration 31/1000 | Loss: 0.00000822
Iteration 32/1000 | Loss: 0.00000821
Iteration 33/1000 | Loss: 0.00000820
Iteration 34/1000 | Loss: 0.00000819
Iteration 35/1000 | Loss: 0.00000818
Iteration 36/1000 | Loss: 0.00000818
Iteration 37/1000 | Loss: 0.00000817
Iteration 38/1000 | Loss: 0.00000816
Iteration 39/1000 | Loss: 0.00000816
Iteration 40/1000 | Loss: 0.00000816
Iteration 41/1000 | Loss: 0.00000815
Iteration 42/1000 | Loss: 0.00000815
Iteration 43/1000 | Loss: 0.00000814
Iteration 44/1000 | Loss: 0.00000814
Iteration 45/1000 | Loss: 0.00000812
Iteration 46/1000 | Loss: 0.00000812
Iteration 47/1000 | Loss: 0.00000811
Iteration 48/1000 | Loss: 0.00000811
Iteration 49/1000 | Loss: 0.00000811
Iteration 50/1000 | Loss: 0.00000810
Iteration 51/1000 | Loss: 0.00000810
Iteration 52/1000 | Loss: 0.00000810
Iteration 53/1000 | Loss: 0.00000809
Iteration 54/1000 | Loss: 0.00000808
Iteration 55/1000 | Loss: 0.00000808
Iteration 56/1000 | Loss: 0.00000807
Iteration 57/1000 | Loss: 0.00000807
Iteration 58/1000 | Loss: 0.00000807
Iteration 59/1000 | Loss: 0.00000807
Iteration 60/1000 | Loss: 0.00000807
Iteration 61/1000 | Loss: 0.00000807
Iteration 62/1000 | Loss: 0.00000807
Iteration 63/1000 | Loss: 0.00000806
Iteration 64/1000 | Loss: 0.00000806
Iteration 65/1000 | Loss: 0.00000806
Iteration 66/1000 | Loss: 0.00000806
Iteration 67/1000 | Loss: 0.00000806
Iteration 68/1000 | Loss: 0.00000806
Iteration 69/1000 | Loss: 0.00000806
Iteration 70/1000 | Loss: 0.00000806
Iteration 71/1000 | Loss: 0.00000806
Iteration 72/1000 | Loss: 0.00000805
Iteration 73/1000 | Loss: 0.00000805
Iteration 74/1000 | Loss: 0.00000805
Iteration 75/1000 | Loss: 0.00000804
Iteration 76/1000 | Loss: 0.00000804
Iteration 77/1000 | Loss: 0.00000804
Iteration 78/1000 | Loss: 0.00000804
Iteration 79/1000 | Loss: 0.00000804
Iteration 80/1000 | Loss: 0.00000804
Iteration 81/1000 | Loss: 0.00000804
Iteration 82/1000 | Loss: 0.00000804
Iteration 83/1000 | Loss: 0.00000804
Iteration 84/1000 | Loss: 0.00000804
Iteration 85/1000 | Loss: 0.00000804
Iteration 86/1000 | Loss: 0.00000804
Iteration 87/1000 | Loss: 0.00000804
Iteration 88/1000 | Loss: 0.00000804
Iteration 89/1000 | Loss: 0.00000804
Iteration 90/1000 | Loss: 0.00000804
Iteration 91/1000 | Loss: 0.00000804
Iteration 92/1000 | Loss: 0.00000804
Iteration 93/1000 | Loss: 0.00000804
Iteration 94/1000 | Loss: 0.00000804
Iteration 95/1000 | Loss: 0.00000804
Iteration 96/1000 | Loss: 0.00000804
Iteration 97/1000 | Loss: 0.00000804
Iteration 98/1000 | Loss: 0.00000804
Iteration 99/1000 | Loss: 0.00000804
Iteration 100/1000 | Loss: 0.00000804
Iteration 101/1000 | Loss: 0.00000804
Iteration 102/1000 | Loss: 0.00000804
Iteration 103/1000 | Loss: 0.00000804
Iteration 104/1000 | Loss: 0.00000804
Iteration 105/1000 | Loss: 0.00000804
Iteration 106/1000 | Loss: 0.00000804
Iteration 107/1000 | Loss: 0.00000804
Iteration 108/1000 | Loss: 0.00000804
Iteration 109/1000 | Loss: 0.00000804
Iteration 110/1000 | Loss: 0.00000804
Iteration 111/1000 | Loss: 0.00000804
Iteration 112/1000 | Loss: 0.00000804
Iteration 113/1000 | Loss: 0.00000804
Iteration 114/1000 | Loss: 0.00000804
Iteration 115/1000 | Loss: 0.00000804
Iteration 116/1000 | Loss: 0.00000804
Iteration 117/1000 | Loss: 0.00000804
Iteration 118/1000 | Loss: 0.00000804
Iteration 119/1000 | Loss: 0.00000804
Iteration 120/1000 | Loss: 0.00000804
Iteration 121/1000 | Loss: 0.00000804
Iteration 122/1000 | Loss: 0.00000804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [8.039522072067484e-06, 8.039522072067484e-06, 8.039522072067484e-06, 8.039522072067484e-06, 8.039522072067484e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.039522072067484e-06

Optimization complete. Final v2v error: 2.453566551208496 mm

Highest mean error: 2.7258331775665283 mm for frame 104

Lowest mean error: 2.311685800552368 mm for frame 159

Saving results

Total time: 36.46737575531006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002815
Iteration 2/25 | Loss: 0.00197555
Iteration 3/25 | Loss: 0.00157482
Iteration 4/25 | Loss: 0.00146916
Iteration 5/25 | Loss: 0.00153686
Iteration 6/25 | Loss: 0.00145284
Iteration 7/25 | Loss: 0.00139957
Iteration 8/25 | Loss: 0.00139466
Iteration 9/25 | Loss: 0.00136847
Iteration 10/25 | Loss: 0.00135674
Iteration 11/25 | Loss: 0.00134345
Iteration 12/25 | Loss: 0.00134023
Iteration 13/25 | Loss: 0.00133740
Iteration 14/25 | Loss: 0.00133034
Iteration 15/25 | Loss: 0.00132621
Iteration 16/25 | Loss: 0.00132244
Iteration 17/25 | Loss: 0.00131604
Iteration 18/25 | Loss: 0.00131288
Iteration 19/25 | Loss: 0.00131346
Iteration 20/25 | Loss: 0.00130798
Iteration 21/25 | Loss: 0.00131018
Iteration 22/25 | Loss: 0.00131385
Iteration 23/25 | Loss: 0.00131343
Iteration 24/25 | Loss: 0.00130705
Iteration 25/25 | Loss: 0.00131110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26350319
Iteration 2/25 | Loss: 0.00228681
Iteration 3/25 | Loss: 0.00228681
Iteration 4/25 | Loss: 0.00228681
Iteration 5/25 | Loss: 0.00228681
Iteration 6/25 | Loss: 0.00228681
Iteration 7/25 | Loss: 0.00228681
Iteration 8/25 | Loss: 0.00228681
Iteration 9/25 | Loss: 0.00228681
Iteration 10/25 | Loss: 0.00228681
Iteration 11/25 | Loss: 0.00228681
Iteration 12/25 | Loss: 0.00228681
Iteration 13/25 | Loss: 0.00228681
Iteration 14/25 | Loss: 0.00228681
Iteration 15/25 | Loss: 0.00228681
Iteration 16/25 | Loss: 0.00228681
Iteration 17/25 | Loss: 0.00228681
Iteration 18/25 | Loss: 0.00228681
Iteration 19/25 | Loss: 0.00228681
Iteration 20/25 | Loss: 0.00228681
Iteration 21/25 | Loss: 0.00228681
Iteration 22/25 | Loss: 0.00228681
Iteration 23/25 | Loss: 0.00228681
Iteration 24/25 | Loss: 0.00228681
Iteration 25/25 | Loss: 0.00228681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228681
Iteration 2/1000 | Loss: 0.00008387
Iteration 3/1000 | Loss: 0.00004006
Iteration 4/1000 | Loss: 0.00003133
Iteration 5/1000 | Loss: 0.00004726
Iteration 6/1000 | Loss: 0.00004264
Iteration 7/1000 | Loss: 0.00002313
Iteration 8/1000 | Loss: 0.00002558
Iteration 9/1000 | Loss: 0.00003748
Iteration 10/1000 | Loss: 0.00004118
Iteration 11/1000 | Loss: 0.00003591
Iteration 12/1000 | Loss: 0.00003893
Iteration 13/1000 | Loss: 0.00005985
Iteration 14/1000 | Loss: 0.00004252
Iteration 15/1000 | Loss: 0.00002955
Iteration 16/1000 | Loss: 0.00003934
Iteration 17/1000 | Loss: 0.00006895
Iteration 18/1000 | Loss: 0.00003828
Iteration 19/1000 | Loss: 0.00005930
Iteration 20/1000 | Loss: 0.00003786
Iteration 21/1000 | Loss: 0.00005556
Iteration 22/1000 | Loss: 0.00004229
Iteration 23/1000 | Loss: 0.00004893
Iteration 24/1000 | Loss: 0.00007135
Iteration 25/1000 | Loss: 0.00005099
Iteration 26/1000 | Loss: 0.00004382
Iteration 27/1000 | Loss: 0.00005248
Iteration 28/1000 | Loss: 0.00005349
Iteration 29/1000 | Loss: 0.00003685
Iteration 30/1000 | Loss: 0.00004165
Iteration 31/1000 | Loss: 0.00003895
Iteration 32/1000 | Loss: 0.00005937
Iteration 33/1000 | Loss: 0.00004523
Iteration 34/1000 | Loss: 0.00005302
Iteration 35/1000 | Loss: 0.00006368
Iteration 36/1000 | Loss: 0.00002801
Iteration 37/1000 | Loss: 0.00002267
Iteration 38/1000 | Loss: 0.00002018
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001851
Iteration 41/1000 | Loss: 0.00001799
Iteration 42/1000 | Loss: 0.00001755
Iteration 43/1000 | Loss: 0.00001737
Iteration 44/1000 | Loss: 0.00001713
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001699
Iteration 47/1000 | Loss: 0.00001699
Iteration 48/1000 | Loss: 0.00001688
Iteration 49/1000 | Loss: 0.00001677
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001674
Iteration 55/1000 | Loss: 0.00001674
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001673
Iteration 58/1000 | Loss: 0.00001673
Iteration 59/1000 | Loss: 0.00001672
Iteration 60/1000 | Loss: 0.00001672
Iteration 61/1000 | Loss: 0.00001672
Iteration 62/1000 | Loss: 0.00001672
Iteration 63/1000 | Loss: 0.00001671
Iteration 64/1000 | Loss: 0.00001671
Iteration 65/1000 | Loss: 0.00001671
Iteration 66/1000 | Loss: 0.00001671
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001669
Iteration 69/1000 | Loss: 0.00001669
Iteration 70/1000 | Loss: 0.00001669
Iteration 71/1000 | Loss: 0.00001668
Iteration 72/1000 | Loss: 0.00001668
Iteration 73/1000 | Loss: 0.00001667
Iteration 74/1000 | Loss: 0.00001667
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001664
Iteration 82/1000 | Loss: 0.00001664
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001663
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001662
Iteration 93/1000 | Loss: 0.00001661
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001657
Iteration 100/1000 | Loss: 0.00001657
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001656
Iteration 106/1000 | Loss: 0.00001656
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001656
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001655
Iteration 113/1000 | Loss: 0.00001655
Iteration 114/1000 | Loss: 0.00001655
Iteration 115/1000 | Loss: 0.00001655
Iteration 116/1000 | Loss: 0.00001655
Iteration 117/1000 | Loss: 0.00001655
Iteration 118/1000 | Loss: 0.00001655
Iteration 119/1000 | Loss: 0.00001655
Iteration 120/1000 | Loss: 0.00001655
Iteration 121/1000 | Loss: 0.00001655
Iteration 122/1000 | Loss: 0.00001655
Iteration 123/1000 | Loss: 0.00001655
Iteration 124/1000 | Loss: 0.00001655
Iteration 125/1000 | Loss: 0.00001655
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00001654
Iteration 128/1000 | Loss: 0.00001654
Iteration 129/1000 | Loss: 0.00001654
Iteration 130/1000 | Loss: 0.00001654
Iteration 131/1000 | Loss: 0.00001654
Iteration 132/1000 | Loss: 0.00001654
Iteration 133/1000 | Loss: 0.00001654
Iteration 134/1000 | Loss: 0.00001653
Iteration 135/1000 | Loss: 0.00001653
Iteration 136/1000 | Loss: 0.00001653
Iteration 137/1000 | Loss: 0.00001653
Iteration 138/1000 | Loss: 0.00001652
Iteration 139/1000 | Loss: 0.00001652
Iteration 140/1000 | Loss: 0.00001652
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001652
Iteration 143/1000 | Loss: 0.00001652
Iteration 144/1000 | Loss: 0.00001652
Iteration 145/1000 | Loss: 0.00001652
Iteration 146/1000 | Loss: 0.00001652
Iteration 147/1000 | Loss: 0.00001651
Iteration 148/1000 | Loss: 0.00001651
Iteration 149/1000 | Loss: 0.00001651
Iteration 150/1000 | Loss: 0.00001651
Iteration 151/1000 | Loss: 0.00001651
Iteration 152/1000 | Loss: 0.00001650
Iteration 153/1000 | Loss: 0.00001650
Iteration 154/1000 | Loss: 0.00001650
Iteration 155/1000 | Loss: 0.00001650
Iteration 156/1000 | Loss: 0.00001650
Iteration 157/1000 | Loss: 0.00001650
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Iteration 162/1000 | Loss: 0.00001649
Iteration 163/1000 | Loss: 0.00001649
Iteration 164/1000 | Loss: 0.00001649
Iteration 165/1000 | Loss: 0.00001649
Iteration 166/1000 | Loss: 0.00001648
Iteration 167/1000 | Loss: 0.00001648
Iteration 168/1000 | Loss: 0.00001648
Iteration 169/1000 | Loss: 0.00001648
Iteration 170/1000 | Loss: 0.00001648
Iteration 171/1000 | Loss: 0.00001648
Iteration 172/1000 | Loss: 0.00001648
Iteration 173/1000 | Loss: 0.00001648
Iteration 174/1000 | Loss: 0.00001647
Iteration 175/1000 | Loss: 0.00001647
Iteration 176/1000 | Loss: 0.00001647
Iteration 177/1000 | Loss: 0.00001647
Iteration 178/1000 | Loss: 0.00001647
Iteration 179/1000 | Loss: 0.00001647
Iteration 180/1000 | Loss: 0.00001647
Iteration 181/1000 | Loss: 0.00001647
Iteration 182/1000 | Loss: 0.00001647
Iteration 183/1000 | Loss: 0.00001647
Iteration 184/1000 | Loss: 0.00001647
Iteration 185/1000 | Loss: 0.00001647
Iteration 186/1000 | Loss: 0.00001647
Iteration 187/1000 | Loss: 0.00001647
Iteration 188/1000 | Loss: 0.00001647
Iteration 189/1000 | Loss: 0.00001647
Iteration 190/1000 | Loss: 0.00001647
Iteration 191/1000 | Loss: 0.00001647
Iteration 192/1000 | Loss: 0.00001647
Iteration 193/1000 | Loss: 0.00001647
Iteration 194/1000 | Loss: 0.00001647
Iteration 195/1000 | Loss: 0.00001647
Iteration 196/1000 | Loss: 0.00001647
Iteration 197/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.6468582543893717e-05, 1.6468582543893717e-05, 1.6468582543893717e-05, 1.6468582543893717e-05, 1.6468582543893717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6468582543893717e-05

Optimization complete. Final v2v error: 3.4410717487335205 mm

Highest mean error: 3.8120696544647217 mm for frame 80

Lowest mean error: 2.971008062362671 mm for frame 108

Saving results

Total time: 119.0491635799408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965395
Iteration 2/25 | Loss: 0.00258049
Iteration 3/25 | Loss: 0.00182381
Iteration 4/25 | Loss: 0.00177270
Iteration 5/25 | Loss: 0.00167324
Iteration 6/25 | Loss: 0.00142234
Iteration 7/25 | Loss: 0.00136942
Iteration 8/25 | Loss: 0.00134988
Iteration 9/25 | Loss: 0.00134248
Iteration 10/25 | Loss: 0.00133987
Iteration 11/25 | Loss: 0.00134011
Iteration 12/25 | Loss: 0.00134270
Iteration 13/25 | Loss: 0.00134458
Iteration 14/25 | Loss: 0.00133645
Iteration 15/25 | Loss: 0.00133323
Iteration 16/25 | Loss: 0.00133559
Iteration 17/25 | Loss: 0.00133284
Iteration 18/25 | Loss: 0.00133338
Iteration 19/25 | Loss: 0.00133252
Iteration 20/25 | Loss: 0.00133308
Iteration 21/25 | Loss: 0.00133288
Iteration 22/25 | Loss: 0.00133192
Iteration 23/25 | Loss: 0.00132930
Iteration 24/25 | Loss: 0.00133106
Iteration 25/25 | Loss: 0.00132873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22811985
Iteration 2/25 | Loss: 0.00266938
Iteration 3/25 | Loss: 0.00245574
Iteration 4/25 | Loss: 0.00245574
Iteration 5/25 | Loss: 0.00245574
Iteration 6/25 | Loss: 0.00245574
Iteration 7/25 | Loss: 0.00245574
Iteration 8/25 | Loss: 0.00245574
Iteration 9/25 | Loss: 0.00245574
Iteration 10/25 | Loss: 0.00245574
Iteration 11/25 | Loss: 0.00245574
Iteration 12/25 | Loss: 0.00245574
Iteration 13/25 | Loss: 0.00245574
Iteration 14/25 | Loss: 0.00245574
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0024557379074394703, 0.0024557379074394703, 0.0024557379074394703, 0.0024557379074394703, 0.0024557379074394703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024557379074394703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245574
Iteration 2/1000 | Loss: 0.00014664
Iteration 3/1000 | Loss: 0.00015213
Iteration 4/1000 | Loss: 0.00004318
Iteration 5/1000 | Loss: 0.00008719
Iteration 6/1000 | Loss: 0.00005189
Iteration 7/1000 | Loss: 0.00003969
Iteration 8/1000 | Loss: 0.00005375
Iteration 9/1000 | Loss: 0.00002839
Iteration 10/1000 | Loss: 0.00004825
Iteration 11/1000 | Loss: 0.00002752
Iteration 12/1000 | Loss: 0.00005520
Iteration 13/1000 | Loss: 0.00020606
Iteration 14/1000 | Loss: 0.00111767
Iteration 15/1000 | Loss: 0.00002797
Iteration 16/1000 | Loss: 0.00003112
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001961
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001926
Iteration 23/1000 | Loss: 0.00001925
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001921
Iteration 26/1000 | Loss: 0.00004909
Iteration 27/1000 | Loss: 0.00001917
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001903
Iteration 30/1000 | Loss: 0.00001902
Iteration 31/1000 | Loss: 0.00001901
Iteration 32/1000 | Loss: 0.00001900
Iteration 33/1000 | Loss: 0.00001900
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001897
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001897
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001897
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001895
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001893
Iteration 58/1000 | Loss: 0.00001893
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00005545
Iteration 66/1000 | Loss: 0.00002060
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001882
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00001882
Iteration 77/1000 | Loss: 0.00001881
Iteration 78/1000 | Loss: 0.00001881
Iteration 79/1000 | Loss: 0.00001880
Iteration 80/1000 | Loss: 0.00001880
Iteration 81/1000 | Loss: 0.00001880
Iteration 82/1000 | Loss: 0.00001880
Iteration 83/1000 | Loss: 0.00001880
Iteration 84/1000 | Loss: 0.00001880
Iteration 85/1000 | Loss: 0.00001880
Iteration 86/1000 | Loss: 0.00001880
Iteration 87/1000 | Loss: 0.00001880
Iteration 88/1000 | Loss: 0.00001880
Iteration 89/1000 | Loss: 0.00001880
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001880
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001879
Iteration 95/1000 | Loss: 0.00001878
Iteration 96/1000 | Loss: 0.00001878
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001877
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001877
Iteration 104/1000 | Loss: 0.00001876
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001876
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001876
Iteration 116/1000 | Loss: 0.00001875
Iteration 117/1000 | Loss: 0.00001875
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001875
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001869
Iteration 126/1000 | Loss: 0.00001869
Iteration 127/1000 | Loss: 0.00001869
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001868
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001867
Iteration 132/1000 | Loss: 0.00001867
Iteration 133/1000 | Loss: 0.00001866
Iteration 134/1000 | Loss: 0.00001866
Iteration 135/1000 | Loss: 0.00001866
Iteration 136/1000 | Loss: 0.00001865
Iteration 137/1000 | Loss: 0.00001865
Iteration 138/1000 | Loss: 0.00001865
Iteration 139/1000 | Loss: 0.00001864
Iteration 140/1000 | Loss: 0.00001864
Iteration 141/1000 | Loss: 0.00001863
Iteration 142/1000 | Loss: 0.00001863
Iteration 143/1000 | Loss: 0.00001863
Iteration 144/1000 | Loss: 0.00001863
Iteration 145/1000 | Loss: 0.00001862
Iteration 146/1000 | Loss: 0.00001862
Iteration 147/1000 | Loss: 0.00001862
Iteration 148/1000 | Loss: 0.00001862
Iteration 149/1000 | Loss: 0.00001861
Iteration 150/1000 | Loss: 0.00001861
Iteration 151/1000 | Loss: 0.00001861
Iteration 152/1000 | Loss: 0.00004062
Iteration 153/1000 | Loss: 0.00016051
Iteration 154/1000 | Loss: 0.00002846
Iteration 155/1000 | Loss: 0.00004400
Iteration 156/1000 | Loss: 0.00002180
Iteration 157/1000 | Loss: 0.00003352
Iteration 158/1000 | Loss: 0.00001860
Iteration 159/1000 | Loss: 0.00001859
Iteration 160/1000 | Loss: 0.00001859
Iteration 161/1000 | Loss: 0.00001859
Iteration 162/1000 | Loss: 0.00001859
Iteration 163/1000 | Loss: 0.00001859
Iteration 164/1000 | Loss: 0.00001858
Iteration 165/1000 | Loss: 0.00001858
Iteration 166/1000 | Loss: 0.00001858
Iteration 167/1000 | Loss: 0.00001858
Iteration 168/1000 | Loss: 0.00001858
Iteration 169/1000 | Loss: 0.00001858
Iteration 170/1000 | Loss: 0.00001858
Iteration 171/1000 | Loss: 0.00001857
Iteration 172/1000 | Loss: 0.00001857
Iteration 173/1000 | Loss: 0.00001857
Iteration 174/1000 | Loss: 0.00001857
Iteration 175/1000 | Loss: 0.00001857
Iteration 176/1000 | Loss: 0.00001857
Iteration 177/1000 | Loss: 0.00001857
Iteration 178/1000 | Loss: 0.00001857
Iteration 179/1000 | Loss: 0.00001856
Iteration 180/1000 | Loss: 0.00002645
Iteration 181/1000 | Loss: 0.00002436
Iteration 182/1000 | Loss: 0.00002091
Iteration 183/1000 | Loss: 0.00002679
Iteration 184/1000 | Loss: 0.00002027
Iteration 185/1000 | Loss: 0.00002239
Iteration 186/1000 | Loss: 0.00001856
Iteration 187/1000 | Loss: 0.00001856
Iteration 188/1000 | Loss: 0.00001856
Iteration 189/1000 | Loss: 0.00001856
Iteration 190/1000 | Loss: 0.00001856
Iteration 191/1000 | Loss: 0.00001856
Iteration 192/1000 | Loss: 0.00001856
Iteration 193/1000 | Loss: 0.00001856
Iteration 194/1000 | Loss: 0.00001856
Iteration 195/1000 | Loss: 0.00001856
Iteration 196/1000 | Loss: 0.00001855
Iteration 197/1000 | Loss: 0.00001855
Iteration 198/1000 | Loss: 0.00001855
Iteration 199/1000 | Loss: 0.00001911
Iteration 200/1000 | Loss: 0.00003348
Iteration 201/1000 | Loss: 0.00003392
Iteration 202/1000 | Loss: 0.00001854
Iteration 203/1000 | Loss: 0.00001852
Iteration 204/1000 | Loss: 0.00001852
Iteration 205/1000 | Loss: 0.00001852
Iteration 206/1000 | Loss: 0.00001852
Iteration 207/1000 | Loss: 0.00001852
Iteration 208/1000 | Loss: 0.00001852
Iteration 209/1000 | Loss: 0.00001852
Iteration 210/1000 | Loss: 0.00001851
Iteration 211/1000 | Loss: 0.00001851
Iteration 212/1000 | Loss: 0.00001851
Iteration 213/1000 | Loss: 0.00001851
Iteration 214/1000 | Loss: 0.00001851
Iteration 215/1000 | Loss: 0.00001851
Iteration 216/1000 | Loss: 0.00001851
Iteration 217/1000 | Loss: 0.00001851
Iteration 218/1000 | Loss: 0.00001851
Iteration 219/1000 | Loss: 0.00001851
Iteration 220/1000 | Loss: 0.00001851
Iteration 221/1000 | Loss: 0.00001850
Iteration 222/1000 | Loss: 0.00001850
Iteration 223/1000 | Loss: 0.00001850
Iteration 224/1000 | Loss: 0.00001850
Iteration 225/1000 | Loss: 0.00001850
Iteration 226/1000 | Loss: 0.00001850
Iteration 227/1000 | Loss: 0.00001850
Iteration 228/1000 | Loss: 0.00001850
Iteration 229/1000 | Loss: 0.00001850
Iteration 230/1000 | Loss: 0.00001850
Iteration 231/1000 | Loss: 0.00001850
Iteration 232/1000 | Loss: 0.00001850
Iteration 233/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.8503898900235072e-05, 1.8503898900235072e-05, 1.8503898900235072e-05, 1.8503898900235072e-05, 1.8503898900235072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8503898900235072e-05

Optimization complete. Final v2v error: 3.641014814376831 mm

Highest mean error: 4.247698783874512 mm for frame 0

Lowest mean error: 3.223820686340332 mm for frame 128

Saving results

Total time: 112.79984307289124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414305
Iteration 2/25 | Loss: 0.00135981
Iteration 3/25 | Loss: 0.00127430
Iteration 4/25 | Loss: 0.00125906
Iteration 5/25 | Loss: 0.00125486
Iteration 6/25 | Loss: 0.00125398
Iteration 7/25 | Loss: 0.00125398
Iteration 8/25 | Loss: 0.00125398
Iteration 9/25 | Loss: 0.00125398
Iteration 10/25 | Loss: 0.00125398
Iteration 11/25 | Loss: 0.00125398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012539795134216547, 0.0012539795134216547, 0.0012539795134216547, 0.0012539795134216547, 0.0012539795134216547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012539795134216547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28707564
Iteration 2/25 | Loss: 0.00209118
Iteration 3/25 | Loss: 0.00209118
Iteration 4/25 | Loss: 0.00209118
Iteration 5/25 | Loss: 0.00209117
Iteration 6/25 | Loss: 0.00209117
Iteration 7/25 | Loss: 0.00209117
Iteration 8/25 | Loss: 0.00209117
Iteration 9/25 | Loss: 0.00209117
Iteration 10/25 | Loss: 0.00209117
Iteration 11/25 | Loss: 0.00209117
Iteration 12/25 | Loss: 0.00209117
Iteration 13/25 | Loss: 0.00209117
Iteration 14/25 | Loss: 0.00209117
Iteration 15/25 | Loss: 0.00209117
Iteration 16/25 | Loss: 0.00209117
Iteration 17/25 | Loss: 0.00209117
Iteration 18/25 | Loss: 0.00209117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020911721512675285, 0.0020911721512675285, 0.0020911721512675285, 0.0020911721512675285, 0.0020911721512675285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020911721512675285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209117
Iteration 2/1000 | Loss: 0.00002633
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00001949
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001715
Iteration 8/1000 | Loss: 0.00001666
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001547
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001521
Iteration 17/1000 | Loss: 0.00001521
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001518
Iteration 20/1000 | Loss: 0.00001518
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001516
Iteration 24/1000 | Loss: 0.00001515
Iteration 25/1000 | Loss: 0.00001514
Iteration 26/1000 | Loss: 0.00001512
Iteration 27/1000 | Loss: 0.00001511
Iteration 28/1000 | Loss: 0.00001510
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001505
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001488
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001484
Iteration 55/1000 | Loss: 0.00001484
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001483
Iteration 58/1000 | Loss: 0.00001483
Iteration 59/1000 | Loss: 0.00001483
Iteration 60/1000 | Loss: 0.00001483
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001481
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001478
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001473
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001473
Iteration 107/1000 | Loss: 0.00001473
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001472
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00001471
Iteration 123/1000 | Loss: 0.00001471
Iteration 124/1000 | Loss: 0.00001471
Iteration 125/1000 | Loss: 0.00001471
Iteration 126/1000 | Loss: 0.00001471
Iteration 127/1000 | Loss: 0.00001471
Iteration 128/1000 | Loss: 0.00001471
Iteration 129/1000 | Loss: 0.00001471
Iteration 130/1000 | Loss: 0.00001471
Iteration 131/1000 | Loss: 0.00001471
Iteration 132/1000 | Loss: 0.00001471
Iteration 133/1000 | Loss: 0.00001470
Iteration 134/1000 | Loss: 0.00001470
Iteration 135/1000 | Loss: 0.00001470
Iteration 136/1000 | Loss: 0.00001470
Iteration 137/1000 | Loss: 0.00001470
Iteration 138/1000 | Loss: 0.00001470
Iteration 139/1000 | Loss: 0.00001470
Iteration 140/1000 | Loss: 0.00001470
Iteration 141/1000 | Loss: 0.00001470
Iteration 142/1000 | Loss: 0.00001470
Iteration 143/1000 | Loss: 0.00001470
Iteration 144/1000 | Loss: 0.00001469
Iteration 145/1000 | Loss: 0.00001469
Iteration 146/1000 | Loss: 0.00001469
Iteration 147/1000 | Loss: 0.00001469
Iteration 148/1000 | Loss: 0.00001469
Iteration 149/1000 | Loss: 0.00001469
Iteration 150/1000 | Loss: 0.00001469
Iteration 151/1000 | Loss: 0.00001469
Iteration 152/1000 | Loss: 0.00001469
Iteration 153/1000 | Loss: 0.00001469
Iteration 154/1000 | Loss: 0.00001469
Iteration 155/1000 | Loss: 0.00001469
Iteration 156/1000 | Loss: 0.00001469
Iteration 157/1000 | Loss: 0.00001469
Iteration 158/1000 | Loss: 0.00001469
Iteration 159/1000 | Loss: 0.00001469
Iteration 160/1000 | Loss: 0.00001469
Iteration 161/1000 | Loss: 0.00001469
Iteration 162/1000 | Loss: 0.00001469
Iteration 163/1000 | Loss: 0.00001469
Iteration 164/1000 | Loss: 0.00001469
Iteration 165/1000 | Loss: 0.00001468
Iteration 166/1000 | Loss: 0.00001468
Iteration 167/1000 | Loss: 0.00001468
Iteration 168/1000 | Loss: 0.00001468
Iteration 169/1000 | Loss: 0.00001468
Iteration 170/1000 | Loss: 0.00001468
Iteration 171/1000 | Loss: 0.00001468
Iteration 172/1000 | Loss: 0.00001468
Iteration 173/1000 | Loss: 0.00001468
Iteration 174/1000 | Loss: 0.00001468
Iteration 175/1000 | Loss: 0.00001468
Iteration 176/1000 | Loss: 0.00001468
Iteration 177/1000 | Loss: 0.00001468
Iteration 178/1000 | Loss: 0.00001468
Iteration 179/1000 | Loss: 0.00001468
Iteration 180/1000 | Loss: 0.00001468
Iteration 181/1000 | Loss: 0.00001468
Iteration 182/1000 | Loss: 0.00001468
Iteration 183/1000 | Loss: 0.00001468
Iteration 184/1000 | Loss: 0.00001468
Iteration 185/1000 | Loss: 0.00001467
Iteration 186/1000 | Loss: 0.00001467
Iteration 187/1000 | Loss: 0.00001467
Iteration 188/1000 | Loss: 0.00001467
Iteration 189/1000 | Loss: 0.00001467
Iteration 190/1000 | Loss: 0.00001467
Iteration 191/1000 | Loss: 0.00001467
Iteration 192/1000 | Loss: 0.00001467
Iteration 193/1000 | Loss: 0.00001467
Iteration 194/1000 | Loss: 0.00001467
Iteration 195/1000 | Loss: 0.00001467
Iteration 196/1000 | Loss: 0.00001467
Iteration 197/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.4672217730549164e-05, 1.4672217730549164e-05, 1.4672217730549164e-05, 1.4672217730549164e-05, 1.4672217730549164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4672217730549164e-05

Optimization complete. Final v2v error: 3.3224456310272217 mm

Highest mean error: 3.6955795288085938 mm for frame 24

Lowest mean error: 3.0638091564178467 mm for frame 46

Saving results

Total time: 42.19360852241516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380821
Iteration 2/25 | Loss: 0.00129331
Iteration 3/25 | Loss: 0.00122289
Iteration 4/25 | Loss: 0.00121339
Iteration 5/25 | Loss: 0.00121042
Iteration 6/25 | Loss: 0.00121007
Iteration 7/25 | Loss: 0.00121007
Iteration 8/25 | Loss: 0.00121007
Iteration 9/25 | Loss: 0.00121007
Iteration 10/25 | Loss: 0.00121007
Iteration 11/25 | Loss: 0.00121007
Iteration 12/25 | Loss: 0.00121007
Iteration 13/25 | Loss: 0.00121007
Iteration 14/25 | Loss: 0.00121007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001210070215165615, 0.001210070215165615, 0.001210070215165615, 0.001210070215165615, 0.001210070215165615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001210070215165615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67350912
Iteration 2/25 | Loss: 0.00204194
Iteration 3/25 | Loss: 0.00204194
Iteration 4/25 | Loss: 0.00204194
Iteration 5/25 | Loss: 0.00204194
Iteration 6/25 | Loss: 0.00204194
Iteration 7/25 | Loss: 0.00204194
Iteration 8/25 | Loss: 0.00204194
Iteration 9/25 | Loss: 0.00204194
Iteration 10/25 | Loss: 0.00204194
Iteration 11/25 | Loss: 0.00204194
Iteration 12/25 | Loss: 0.00204194
Iteration 13/25 | Loss: 0.00204194
Iteration 14/25 | Loss: 0.00204193
Iteration 15/25 | Loss: 0.00204193
Iteration 16/25 | Loss: 0.00204193
Iteration 17/25 | Loss: 0.00204193
Iteration 18/25 | Loss: 0.00204193
Iteration 19/25 | Loss: 0.00204193
Iteration 20/25 | Loss: 0.00204193
Iteration 21/25 | Loss: 0.00204193
Iteration 22/25 | Loss: 0.00204193
Iteration 23/25 | Loss: 0.00204193
Iteration 24/25 | Loss: 0.00204193
Iteration 25/25 | Loss: 0.00204193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00204193452373147, 0.00204193452373147, 0.00204193452373147, 0.00204193452373147, 0.00204193452373147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00204193452373147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204193
Iteration 2/1000 | Loss: 0.00001904
Iteration 3/1000 | Loss: 0.00001365
Iteration 4/1000 | Loss: 0.00001215
Iteration 5/1000 | Loss: 0.00001128
Iteration 6/1000 | Loss: 0.00001049
Iteration 7/1000 | Loss: 0.00001002
Iteration 8/1000 | Loss: 0.00000974
Iteration 9/1000 | Loss: 0.00000942
Iteration 10/1000 | Loss: 0.00000923
Iteration 11/1000 | Loss: 0.00000911
Iteration 12/1000 | Loss: 0.00000899
Iteration 13/1000 | Loss: 0.00000898
Iteration 14/1000 | Loss: 0.00000895
Iteration 15/1000 | Loss: 0.00000894
Iteration 16/1000 | Loss: 0.00000891
Iteration 17/1000 | Loss: 0.00000891
Iteration 18/1000 | Loss: 0.00000890
Iteration 19/1000 | Loss: 0.00000889
Iteration 20/1000 | Loss: 0.00000888
Iteration 21/1000 | Loss: 0.00000887
Iteration 22/1000 | Loss: 0.00000886
Iteration 23/1000 | Loss: 0.00000884
Iteration 24/1000 | Loss: 0.00000879
Iteration 25/1000 | Loss: 0.00000872
Iteration 26/1000 | Loss: 0.00000867
Iteration 27/1000 | Loss: 0.00000863
Iteration 28/1000 | Loss: 0.00000860
Iteration 29/1000 | Loss: 0.00000859
Iteration 30/1000 | Loss: 0.00000858
Iteration 31/1000 | Loss: 0.00000856
Iteration 32/1000 | Loss: 0.00000856
Iteration 33/1000 | Loss: 0.00000850
Iteration 34/1000 | Loss: 0.00000846
Iteration 35/1000 | Loss: 0.00000845
Iteration 36/1000 | Loss: 0.00000845
Iteration 37/1000 | Loss: 0.00000844
Iteration 38/1000 | Loss: 0.00000844
Iteration 39/1000 | Loss: 0.00000844
Iteration 40/1000 | Loss: 0.00000843
Iteration 41/1000 | Loss: 0.00000843
Iteration 42/1000 | Loss: 0.00000843
Iteration 43/1000 | Loss: 0.00000843
Iteration 44/1000 | Loss: 0.00000843
Iteration 45/1000 | Loss: 0.00000843
Iteration 46/1000 | Loss: 0.00000842
Iteration 47/1000 | Loss: 0.00000842
Iteration 48/1000 | Loss: 0.00000842
Iteration 49/1000 | Loss: 0.00000842
Iteration 50/1000 | Loss: 0.00000840
Iteration 51/1000 | Loss: 0.00000840
Iteration 52/1000 | Loss: 0.00000840
Iteration 53/1000 | Loss: 0.00000840
Iteration 54/1000 | Loss: 0.00000839
Iteration 55/1000 | Loss: 0.00000839
Iteration 56/1000 | Loss: 0.00000839
Iteration 57/1000 | Loss: 0.00000839
Iteration 58/1000 | Loss: 0.00000839
Iteration 59/1000 | Loss: 0.00000839
Iteration 60/1000 | Loss: 0.00000839
Iteration 61/1000 | Loss: 0.00000839
Iteration 62/1000 | Loss: 0.00000839
Iteration 63/1000 | Loss: 0.00000838
Iteration 64/1000 | Loss: 0.00000838
Iteration 65/1000 | Loss: 0.00000838
Iteration 66/1000 | Loss: 0.00000838
Iteration 67/1000 | Loss: 0.00000838
Iteration 68/1000 | Loss: 0.00000838
Iteration 69/1000 | Loss: 0.00000838
Iteration 70/1000 | Loss: 0.00000837
Iteration 71/1000 | Loss: 0.00000837
Iteration 72/1000 | Loss: 0.00000837
Iteration 73/1000 | Loss: 0.00000837
Iteration 74/1000 | Loss: 0.00000836
Iteration 75/1000 | Loss: 0.00000836
Iteration 76/1000 | Loss: 0.00000836
Iteration 77/1000 | Loss: 0.00000836
Iteration 78/1000 | Loss: 0.00000836
Iteration 79/1000 | Loss: 0.00000836
Iteration 80/1000 | Loss: 0.00000835
Iteration 81/1000 | Loss: 0.00000835
Iteration 82/1000 | Loss: 0.00000835
Iteration 83/1000 | Loss: 0.00000835
Iteration 84/1000 | Loss: 0.00000835
Iteration 85/1000 | Loss: 0.00000834
Iteration 86/1000 | Loss: 0.00000834
Iteration 87/1000 | Loss: 0.00000834
Iteration 88/1000 | Loss: 0.00000834
Iteration 89/1000 | Loss: 0.00000834
Iteration 90/1000 | Loss: 0.00000834
Iteration 91/1000 | Loss: 0.00000834
Iteration 92/1000 | Loss: 0.00000834
Iteration 93/1000 | Loss: 0.00000834
Iteration 94/1000 | Loss: 0.00000834
Iteration 95/1000 | Loss: 0.00000833
Iteration 96/1000 | Loss: 0.00000833
Iteration 97/1000 | Loss: 0.00000833
Iteration 98/1000 | Loss: 0.00000832
Iteration 99/1000 | Loss: 0.00000832
Iteration 100/1000 | Loss: 0.00000832
Iteration 101/1000 | Loss: 0.00000831
Iteration 102/1000 | Loss: 0.00000831
Iteration 103/1000 | Loss: 0.00000831
Iteration 104/1000 | Loss: 0.00000831
Iteration 105/1000 | Loss: 0.00000831
Iteration 106/1000 | Loss: 0.00000831
Iteration 107/1000 | Loss: 0.00000831
Iteration 108/1000 | Loss: 0.00000831
Iteration 109/1000 | Loss: 0.00000831
Iteration 110/1000 | Loss: 0.00000831
Iteration 111/1000 | Loss: 0.00000830
Iteration 112/1000 | Loss: 0.00000830
Iteration 113/1000 | Loss: 0.00000829
Iteration 114/1000 | Loss: 0.00000829
Iteration 115/1000 | Loss: 0.00000829
Iteration 116/1000 | Loss: 0.00000829
Iteration 117/1000 | Loss: 0.00000829
Iteration 118/1000 | Loss: 0.00000829
Iteration 119/1000 | Loss: 0.00000829
Iteration 120/1000 | Loss: 0.00000829
Iteration 121/1000 | Loss: 0.00000829
Iteration 122/1000 | Loss: 0.00000829
Iteration 123/1000 | Loss: 0.00000829
Iteration 124/1000 | Loss: 0.00000829
Iteration 125/1000 | Loss: 0.00000829
Iteration 126/1000 | Loss: 0.00000829
Iteration 127/1000 | Loss: 0.00000828
Iteration 128/1000 | Loss: 0.00000828
Iteration 129/1000 | Loss: 0.00000828
Iteration 130/1000 | Loss: 0.00000828
Iteration 131/1000 | Loss: 0.00000828
Iteration 132/1000 | Loss: 0.00000828
Iteration 133/1000 | Loss: 0.00000828
Iteration 134/1000 | Loss: 0.00000828
Iteration 135/1000 | Loss: 0.00000828
Iteration 136/1000 | Loss: 0.00000828
Iteration 137/1000 | Loss: 0.00000827
Iteration 138/1000 | Loss: 0.00000827
Iteration 139/1000 | Loss: 0.00000827
Iteration 140/1000 | Loss: 0.00000827
Iteration 141/1000 | Loss: 0.00000827
Iteration 142/1000 | Loss: 0.00000827
Iteration 143/1000 | Loss: 0.00000826
Iteration 144/1000 | Loss: 0.00000826
Iteration 145/1000 | Loss: 0.00000826
Iteration 146/1000 | Loss: 0.00000826
Iteration 147/1000 | Loss: 0.00000826
Iteration 148/1000 | Loss: 0.00000826
Iteration 149/1000 | Loss: 0.00000826
Iteration 150/1000 | Loss: 0.00000826
Iteration 151/1000 | Loss: 0.00000826
Iteration 152/1000 | Loss: 0.00000826
Iteration 153/1000 | Loss: 0.00000826
Iteration 154/1000 | Loss: 0.00000826
Iteration 155/1000 | Loss: 0.00000826
Iteration 156/1000 | Loss: 0.00000826
Iteration 157/1000 | Loss: 0.00000826
Iteration 158/1000 | Loss: 0.00000826
Iteration 159/1000 | Loss: 0.00000826
Iteration 160/1000 | Loss: 0.00000826
Iteration 161/1000 | Loss: 0.00000826
Iteration 162/1000 | Loss: 0.00000826
Iteration 163/1000 | Loss: 0.00000826
Iteration 164/1000 | Loss: 0.00000826
Iteration 165/1000 | Loss: 0.00000826
Iteration 166/1000 | Loss: 0.00000826
Iteration 167/1000 | Loss: 0.00000826
Iteration 168/1000 | Loss: 0.00000826
Iteration 169/1000 | Loss: 0.00000826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [8.261033144663088e-06, 8.261033144663088e-06, 8.261033144663088e-06, 8.261033144663088e-06, 8.261033144663088e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.261033144663088e-06

Optimization complete. Final v2v error: 2.5222256183624268 mm

Highest mean error: 2.655529499053955 mm for frame 129

Lowest mean error: 2.448385000228882 mm for frame 57

Saving results

Total time: 39.32982778549194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008791
Iteration 2/25 | Loss: 0.00188420
Iteration 3/25 | Loss: 0.00164880
Iteration 4/25 | Loss: 0.00157328
Iteration 5/25 | Loss: 0.00151606
Iteration 6/25 | Loss: 0.00151503
Iteration 7/25 | Loss: 0.00142774
Iteration 8/25 | Loss: 0.00134120
Iteration 9/25 | Loss: 0.00133397
Iteration 10/25 | Loss: 0.00131881
Iteration 11/25 | Loss: 0.00131563
Iteration 12/25 | Loss: 0.00130305
Iteration 13/25 | Loss: 0.00130064
Iteration 14/25 | Loss: 0.00128556
Iteration 15/25 | Loss: 0.00128067
Iteration 16/25 | Loss: 0.00127743
Iteration 17/25 | Loss: 0.00127637
Iteration 18/25 | Loss: 0.00127321
Iteration 19/25 | Loss: 0.00127386
Iteration 20/25 | Loss: 0.00127129
Iteration 21/25 | Loss: 0.00127023
Iteration 22/25 | Loss: 0.00126992
Iteration 23/25 | Loss: 0.00126985
Iteration 24/25 | Loss: 0.00126984
Iteration 25/25 | Loss: 0.00126984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29819322
Iteration 2/25 | Loss: 0.00219497
Iteration 3/25 | Loss: 0.00219497
Iteration 4/25 | Loss: 0.00207632
Iteration 5/25 | Loss: 0.00207631
Iteration 6/25 | Loss: 0.00207631
Iteration 7/25 | Loss: 0.00207631
Iteration 8/25 | Loss: 0.00207631
Iteration 9/25 | Loss: 0.00207631
Iteration 10/25 | Loss: 0.00207631
Iteration 11/25 | Loss: 0.00207631
Iteration 12/25 | Loss: 0.00207631
Iteration 13/25 | Loss: 0.00207631
Iteration 14/25 | Loss: 0.00207631
Iteration 15/25 | Loss: 0.00207631
Iteration 16/25 | Loss: 0.00207631
Iteration 17/25 | Loss: 0.00207631
Iteration 18/25 | Loss: 0.00207631
Iteration 19/25 | Loss: 0.00207631
Iteration 20/25 | Loss: 0.00207631
Iteration 21/25 | Loss: 0.00207631
Iteration 22/25 | Loss: 0.00207631
Iteration 23/25 | Loss: 0.00207631
Iteration 24/25 | Loss: 0.00207631
Iteration 25/25 | Loss: 0.00207631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207631
Iteration 2/1000 | Loss: 0.00015343
Iteration 3/1000 | Loss: 0.00052509
Iteration 4/1000 | Loss: 0.00001702
Iteration 5/1000 | Loss: 0.00001527
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00051665
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001322
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00018239
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001233
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001156
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001131
Iteration 27/1000 | Loss: 0.00001130
Iteration 28/1000 | Loss: 0.00001129
Iteration 29/1000 | Loss: 0.00001129
Iteration 30/1000 | Loss: 0.00001127
Iteration 31/1000 | Loss: 0.00001126
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001121
Iteration 35/1000 | Loss: 0.00001121
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001119
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001118
Iteration 44/1000 | Loss: 0.00001118
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001118
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001117
Iteration 49/1000 | Loss: 0.00001117
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001117
Iteration 52/1000 | Loss: 0.00001117
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001117
Iteration 56/1000 | Loss: 0.00001117
Iteration 57/1000 | Loss: 0.00001116
Iteration 58/1000 | Loss: 0.00001116
Iteration 59/1000 | Loss: 0.00001116
Iteration 60/1000 | Loss: 0.00001116
Iteration 61/1000 | Loss: 0.00001116
Iteration 62/1000 | Loss: 0.00001116
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001116
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001108
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001106
Iteration 139/1000 | Loss: 0.00001106
Iteration 140/1000 | Loss: 0.00001106
Iteration 141/1000 | Loss: 0.00001106
Iteration 142/1000 | Loss: 0.00001106
Iteration 143/1000 | Loss: 0.00001106
Iteration 144/1000 | Loss: 0.00001106
Iteration 145/1000 | Loss: 0.00001106
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001105
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001105
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001105
Iteration 164/1000 | Loss: 0.00001105
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001105
Iteration 167/1000 | Loss: 0.00001105
Iteration 168/1000 | Loss: 0.00001105
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001104
Iteration 172/1000 | Loss: 0.00001104
Iteration 173/1000 | Loss: 0.00001104
Iteration 174/1000 | Loss: 0.00001104
Iteration 175/1000 | Loss: 0.00001104
Iteration 176/1000 | Loss: 0.00001104
Iteration 177/1000 | Loss: 0.00001104
Iteration 178/1000 | Loss: 0.00001104
Iteration 179/1000 | Loss: 0.00001104
Iteration 180/1000 | Loss: 0.00001104
Iteration 181/1000 | Loss: 0.00001104
Iteration 182/1000 | Loss: 0.00001104
Iteration 183/1000 | Loss: 0.00001104
Iteration 184/1000 | Loss: 0.00001104
Iteration 185/1000 | Loss: 0.00001104
Iteration 186/1000 | Loss: 0.00001104
Iteration 187/1000 | Loss: 0.00001103
Iteration 188/1000 | Loss: 0.00001103
Iteration 189/1000 | Loss: 0.00001103
Iteration 190/1000 | Loss: 0.00001103
Iteration 191/1000 | Loss: 0.00001103
Iteration 192/1000 | Loss: 0.00001103
Iteration 193/1000 | Loss: 0.00001103
Iteration 194/1000 | Loss: 0.00001103
Iteration 195/1000 | Loss: 0.00001103
Iteration 196/1000 | Loss: 0.00001103
Iteration 197/1000 | Loss: 0.00001103
Iteration 198/1000 | Loss: 0.00001103
Iteration 199/1000 | Loss: 0.00001103
Iteration 200/1000 | Loss: 0.00001103
Iteration 201/1000 | Loss: 0.00001103
Iteration 202/1000 | Loss: 0.00001103
Iteration 203/1000 | Loss: 0.00001102
Iteration 204/1000 | Loss: 0.00001102
Iteration 205/1000 | Loss: 0.00001102
Iteration 206/1000 | Loss: 0.00001102
Iteration 207/1000 | Loss: 0.00001102
Iteration 208/1000 | Loss: 0.00001102
Iteration 209/1000 | Loss: 0.00001102
Iteration 210/1000 | Loss: 0.00001102
Iteration 211/1000 | Loss: 0.00001102
Iteration 212/1000 | Loss: 0.00001102
Iteration 213/1000 | Loss: 0.00001102
Iteration 214/1000 | Loss: 0.00001102
Iteration 215/1000 | Loss: 0.00001101
Iteration 216/1000 | Loss: 0.00001101
Iteration 217/1000 | Loss: 0.00001101
Iteration 218/1000 | Loss: 0.00001101
Iteration 219/1000 | Loss: 0.00001101
Iteration 220/1000 | Loss: 0.00001101
Iteration 221/1000 | Loss: 0.00001101
Iteration 222/1000 | Loss: 0.00001101
Iteration 223/1000 | Loss: 0.00001101
Iteration 224/1000 | Loss: 0.00001101
Iteration 225/1000 | Loss: 0.00001101
Iteration 226/1000 | Loss: 0.00001101
Iteration 227/1000 | Loss: 0.00001101
Iteration 228/1000 | Loss: 0.00001101
Iteration 229/1000 | Loss: 0.00001100
Iteration 230/1000 | Loss: 0.00001100
Iteration 231/1000 | Loss: 0.00001100
Iteration 232/1000 | Loss: 0.00001100
Iteration 233/1000 | Loss: 0.00001100
Iteration 234/1000 | Loss: 0.00001100
Iteration 235/1000 | Loss: 0.00001100
Iteration 236/1000 | Loss: 0.00001100
Iteration 237/1000 | Loss: 0.00001100
Iteration 238/1000 | Loss: 0.00001100
Iteration 239/1000 | Loss: 0.00001100
Iteration 240/1000 | Loss: 0.00001100
Iteration 241/1000 | Loss: 0.00001100
Iteration 242/1000 | Loss: 0.00001100
Iteration 243/1000 | Loss: 0.00001100
Iteration 244/1000 | Loss: 0.00001100
Iteration 245/1000 | Loss: 0.00001100
Iteration 246/1000 | Loss: 0.00001100
Iteration 247/1000 | Loss: 0.00001100
Iteration 248/1000 | Loss: 0.00001100
Iteration 249/1000 | Loss: 0.00001100
Iteration 250/1000 | Loss: 0.00001100
Iteration 251/1000 | Loss: 0.00001100
Iteration 252/1000 | Loss: 0.00001100
Iteration 253/1000 | Loss: 0.00001100
Iteration 254/1000 | Loss: 0.00001100
Iteration 255/1000 | Loss: 0.00001100
Iteration 256/1000 | Loss: 0.00001100
Iteration 257/1000 | Loss: 0.00001100
Iteration 258/1000 | Loss: 0.00001100
Iteration 259/1000 | Loss: 0.00001100
Iteration 260/1000 | Loss: 0.00001100
Iteration 261/1000 | Loss: 0.00001100
Iteration 262/1000 | Loss: 0.00001100
Iteration 263/1000 | Loss: 0.00001100
Iteration 264/1000 | Loss: 0.00001100
Iteration 265/1000 | Loss: 0.00001100
Iteration 266/1000 | Loss: 0.00001100
Iteration 267/1000 | Loss: 0.00001100
Iteration 268/1000 | Loss: 0.00001100
Iteration 269/1000 | Loss: 0.00001100
Iteration 270/1000 | Loss: 0.00001100
Iteration 271/1000 | Loss: 0.00001100
Iteration 272/1000 | Loss: 0.00001100
Iteration 273/1000 | Loss: 0.00001100
Iteration 274/1000 | Loss: 0.00001100
Iteration 275/1000 | Loss: 0.00001100
Iteration 276/1000 | Loss: 0.00001100
Iteration 277/1000 | Loss: 0.00001100
Iteration 278/1000 | Loss: 0.00001100
Iteration 279/1000 | Loss: 0.00001100
Iteration 280/1000 | Loss: 0.00001100
Iteration 281/1000 | Loss: 0.00001100
Iteration 282/1000 | Loss: 0.00001100
Iteration 283/1000 | Loss: 0.00001100
Iteration 284/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.0997770914400462e-05, 1.0997770914400462e-05, 1.0997770914400462e-05, 1.0997770914400462e-05, 1.0997770914400462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0997770914400462e-05

Optimization complete. Final v2v error: 2.831326723098755 mm

Highest mean error: 3.649427890777588 mm for frame 32

Lowest mean error: 2.550201892852783 mm for frame 26

Saving results

Total time: 87.78674721717834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499256
Iteration 2/25 | Loss: 0.00143618
Iteration 3/25 | Loss: 0.00130200
Iteration 4/25 | Loss: 0.00127243
Iteration 5/25 | Loss: 0.00126713
Iteration 6/25 | Loss: 0.00126666
Iteration 7/25 | Loss: 0.00126666
Iteration 8/25 | Loss: 0.00126666
Iteration 9/25 | Loss: 0.00126666
Iteration 10/25 | Loss: 0.00126666
Iteration 11/25 | Loss: 0.00126666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012666645925492048, 0.0012666645925492048, 0.0012666645925492048, 0.0012666645925492048, 0.0012666645925492048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012666645925492048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16329753
Iteration 2/25 | Loss: 0.00219238
Iteration 3/25 | Loss: 0.00219237
Iteration 4/25 | Loss: 0.00219237
Iteration 5/25 | Loss: 0.00219237
Iteration 6/25 | Loss: 0.00219237
Iteration 7/25 | Loss: 0.00219236
Iteration 8/25 | Loss: 0.00219236
Iteration 9/25 | Loss: 0.00219236
Iteration 10/25 | Loss: 0.00219236
Iteration 11/25 | Loss: 0.00219236
Iteration 12/25 | Loss: 0.00219236
Iteration 13/25 | Loss: 0.00219236
Iteration 14/25 | Loss: 0.00219236
Iteration 15/25 | Loss: 0.00219236
Iteration 16/25 | Loss: 0.00219236
Iteration 17/25 | Loss: 0.00219236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021923640742897987, 0.0021923640742897987, 0.0021923640742897987, 0.0021923640742897987, 0.0021923640742897987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021923640742897987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219236
Iteration 2/1000 | Loss: 0.00003980
Iteration 3/1000 | Loss: 0.00002337
Iteration 4/1000 | Loss: 0.00002019
Iteration 5/1000 | Loss: 0.00001904
Iteration 6/1000 | Loss: 0.00001821
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001697
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001631
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001604
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001579
Iteration 18/1000 | Loss: 0.00001577
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001575
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001574
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001572
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001569
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001569
Iteration 42/1000 | Loss: 0.00001569
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001569
Iteration 46/1000 | Loss: 0.00001569
Iteration 47/1000 | Loss: 0.00001569
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001564
Iteration 64/1000 | Loss: 0.00001564
Iteration 65/1000 | Loss: 0.00001563
Iteration 66/1000 | Loss: 0.00001563
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001562
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001560
Iteration 76/1000 | Loss: 0.00001560
Iteration 77/1000 | Loss: 0.00001560
Iteration 78/1000 | Loss: 0.00001560
Iteration 79/1000 | Loss: 0.00001559
Iteration 80/1000 | Loss: 0.00001559
Iteration 81/1000 | Loss: 0.00001558
Iteration 82/1000 | Loss: 0.00001558
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001557
Iteration 85/1000 | Loss: 0.00001557
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001551
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001550
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001548
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001546
Iteration 131/1000 | Loss: 0.00001546
Iteration 132/1000 | Loss: 0.00001546
Iteration 133/1000 | Loss: 0.00001545
Iteration 134/1000 | Loss: 0.00001545
Iteration 135/1000 | Loss: 0.00001545
Iteration 136/1000 | Loss: 0.00001544
Iteration 137/1000 | Loss: 0.00001544
Iteration 138/1000 | Loss: 0.00001544
Iteration 139/1000 | Loss: 0.00001544
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001543
Iteration 143/1000 | Loss: 0.00001543
Iteration 144/1000 | Loss: 0.00001542
Iteration 145/1000 | Loss: 0.00001541
Iteration 146/1000 | Loss: 0.00001541
Iteration 147/1000 | Loss: 0.00001541
Iteration 148/1000 | Loss: 0.00001541
Iteration 149/1000 | Loss: 0.00001541
Iteration 150/1000 | Loss: 0.00001541
Iteration 151/1000 | Loss: 0.00001541
Iteration 152/1000 | Loss: 0.00001540
Iteration 153/1000 | Loss: 0.00001540
Iteration 154/1000 | Loss: 0.00001540
Iteration 155/1000 | Loss: 0.00001539
Iteration 156/1000 | Loss: 0.00001539
Iteration 157/1000 | Loss: 0.00001539
Iteration 158/1000 | Loss: 0.00001538
Iteration 159/1000 | Loss: 0.00001538
Iteration 160/1000 | Loss: 0.00001538
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001537
Iteration 164/1000 | Loss: 0.00001536
Iteration 165/1000 | Loss: 0.00001535
Iteration 166/1000 | Loss: 0.00001535
Iteration 167/1000 | Loss: 0.00001534
Iteration 168/1000 | Loss: 0.00001534
Iteration 169/1000 | Loss: 0.00001534
Iteration 170/1000 | Loss: 0.00001534
Iteration 171/1000 | Loss: 0.00001533
Iteration 172/1000 | Loss: 0.00001533
Iteration 173/1000 | Loss: 0.00001533
Iteration 174/1000 | Loss: 0.00001533
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001533
Iteration 177/1000 | Loss: 0.00001532
Iteration 178/1000 | Loss: 0.00001531
Iteration 179/1000 | Loss: 0.00001531
Iteration 180/1000 | Loss: 0.00001531
Iteration 181/1000 | Loss: 0.00001531
Iteration 182/1000 | Loss: 0.00001531
Iteration 183/1000 | Loss: 0.00001531
Iteration 184/1000 | Loss: 0.00001531
Iteration 185/1000 | Loss: 0.00001530
Iteration 186/1000 | Loss: 0.00001530
Iteration 187/1000 | Loss: 0.00001530
Iteration 188/1000 | Loss: 0.00001530
Iteration 189/1000 | Loss: 0.00001530
Iteration 190/1000 | Loss: 0.00001530
Iteration 191/1000 | Loss: 0.00001530
Iteration 192/1000 | Loss: 0.00001530
Iteration 193/1000 | Loss: 0.00001530
Iteration 194/1000 | Loss: 0.00001530
Iteration 195/1000 | Loss: 0.00001530
Iteration 196/1000 | Loss: 0.00001530
Iteration 197/1000 | Loss: 0.00001529
Iteration 198/1000 | Loss: 0.00001529
Iteration 199/1000 | Loss: 0.00001529
Iteration 200/1000 | Loss: 0.00001529
Iteration 201/1000 | Loss: 0.00001529
Iteration 202/1000 | Loss: 0.00001529
Iteration 203/1000 | Loss: 0.00001529
Iteration 204/1000 | Loss: 0.00001529
Iteration 205/1000 | Loss: 0.00001529
Iteration 206/1000 | Loss: 0.00001528
Iteration 207/1000 | Loss: 0.00001528
Iteration 208/1000 | Loss: 0.00001528
Iteration 209/1000 | Loss: 0.00001528
Iteration 210/1000 | Loss: 0.00001528
Iteration 211/1000 | Loss: 0.00001527
Iteration 212/1000 | Loss: 0.00001527
Iteration 213/1000 | Loss: 0.00001527
Iteration 214/1000 | Loss: 0.00001527
Iteration 215/1000 | Loss: 0.00001527
Iteration 216/1000 | Loss: 0.00001527
Iteration 217/1000 | Loss: 0.00001527
Iteration 218/1000 | Loss: 0.00001527
Iteration 219/1000 | Loss: 0.00001527
Iteration 220/1000 | Loss: 0.00001527
Iteration 221/1000 | Loss: 0.00001527
Iteration 222/1000 | Loss: 0.00001527
Iteration 223/1000 | Loss: 0.00001527
Iteration 224/1000 | Loss: 0.00001527
Iteration 225/1000 | Loss: 0.00001527
Iteration 226/1000 | Loss: 0.00001527
Iteration 227/1000 | Loss: 0.00001527
Iteration 228/1000 | Loss: 0.00001526
Iteration 229/1000 | Loss: 0.00001526
Iteration 230/1000 | Loss: 0.00001526
Iteration 231/1000 | Loss: 0.00001526
Iteration 232/1000 | Loss: 0.00001526
Iteration 233/1000 | Loss: 0.00001526
Iteration 234/1000 | Loss: 0.00001526
Iteration 235/1000 | Loss: 0.00001526
Iteration 236/1000 | Loss: 0.00001526
Iteration 237/1000 | Loss: 0.00001526
Iteration 238/1000 | Loss: 0.00001526
Iteration 239/1000 | Loss: 0.00001526
Iteration 240/1000 | Loss: 0.00001526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.5263736713677645e-05, 1.5263736713677645e-05, 1.5263736713677645e-05, 1.5263736713677645e-05, 1.5263736713677645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5263736713677645e-05

Optimization complete. Final v2v error: 3.38205885887146 mm

Highest mean error: 3.650202751159668 mm for frame 133

Lowest mean error: 3.255709648132324 mm for frame 64

Saving results

Total time: 41.98008155822754
