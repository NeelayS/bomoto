Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=180, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10080-10135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118684
Iteration 2/25 | Loss: 0.00139185
Iteration 3/25 | Loss: 0.00085531
Iteration 4/25 | Loss: 0.00078345
Iteration 5/25 | Loss: 0.00073440
Iteration 6/25 | Loss: 0.00072880
Iteration 7/25 | Loss: 0.00071680
Iteration 8/25 | Loss: 0.00071554
Iteration 9/25 | Loss: 0.00071895
Iteration 10/25 | Loss: 0.00071280
Iteration 11/25 | Loss: 0.00071785
Iteration 12/25 | Loss: 0.00071176
Iteration 13/25 | Loss: 0.00071680
Iteration 14/25 | Loss: 0.00071483
Iteration 15/25 | Loss: 0.00071124
Iteration 16/25 | Loss: 0.00071122
Iteration 17/25 | Loss: 0.00071122
Iteration 18/25 | Loss: 0.00071122
Iteration 19/25 | Loss: 0.00071122
Iteration 20/25 | Loss: 0.00071122
Iteration 21/25 | Loss: 0.00071121
Iteration 22/25 | Loss: 0.00071121
Iteration 23/25 | Loss: 0.00071121
Iteration 24/25 | Loss: 0.00071121
Iteration 25/25 | Loss: 0.00071121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46998405
Iteration 2/25 | Loss: 0.00024632
Iteration 3/25 | Loss: 0.00024630
Iteration 4/25 | Loss: 0.00024630
Iteration 5/25 | Loss: 0.00024630
Iteration 6/25 | Loss: 0.00024630
Iteration 7/25 | Loss: 0.00024630
Iteration 8/25 | Loss: 0.00024630
Iteration 9/25 | Loss: 0.00024630
Iteration 10/25 | Loss: 0.00024630
Iteration 11/25 | Loss: 0.00024630
Iteration 12/25 | Loss: 0.00024630
Iteration 13/25 | Loss: 0.00024630
Iteration 14/25 | Loss: 0.00024630
Iteration 15/25 | Loss: 0.00024630
Iteration 16/25 | Loss: 0.00024630
Iteration 17/25 | Loss: 0.00024630
Iteration 18/25 | Loss: 0.00024630
Iteration 19/25 | Loss: 0.00024630
Iteration 20/25 | Loss: 0.00024630
Iteration 21/25 | Loss: 0.00024630
Iteration 22/25 | Loss: 0.00024630
Iteration 23/25 | Loss: 0.00024630
Iteration 24/25 | Loss: 0.00024630
Iteration 25/25 | Loss: 0.00024630

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024630
Iteration 2/1000 | Loss: 0.00005768
Iteration 3/1000 | Loss: 0.00008598
Iteration 4/1000 | Loss: 0.00003591
Iteration 5/1000 | Loss: 0.00003406
Iteration 6/1000 | Loss: 0.00008367
Iteration 7/1000 | Loss: 0.00004638
Iteration 8/1000 | Loss: 0.00003140
Iteration 9/1000 | Loss: 0.00003061
Iteration 10/1000 | Loss: 0.00003005
Iteration 11/1000 | Loss: 0.00002975
Iteration 12/1000 | Loss: 0.00002956
Iteration 13/1000 | Loss: 0.00002938
Iteration 14/1000 | Loss: 0.00002931
Iteration 15/1000 | Loss: 0.00002925
Iteration 16/1000 | Loss: 0.00002925
Iteration 17/1000 | Loss: 0.00002924
Iteration 18/1000 | Loss: 0.00002924
Iteration 19/1000 | Loss: 0.00002922
Iteration 20/1000 | Loss: 0.00002922
Iteration 21/1000 | Loss: 0.00002921
Iteration 22/1000 | Loss: 0.00002921
Iteration 23/1000 | Loss: 0.00002921
Iteration 24/1000 | Loss: 0.00002920
Iteration 25/1000 | Loss: 0.00002920
Iteration 26/1000 | Loss: 0.00002920
Iteration 27/1000 | Loss: 0.00002919
Iteration 28/1000 | Loss: 0.00002919
Iteration 29/1000 | Loss: 0.00002919
Iteration 30/1000 | Loss: 0.00002918
Iteration 31/1000 | Loss: 0.00002918
Iteration 32/1000 | Loss: 0.00002918
Iteration 33/1000 | Loss: 0.00002918
Iteration 34/1000 | Loss: 0.00002918
Iteration 35/1000 | Loss: 0.00002917
Iteration 36/1000 | Loss: 0.00002917
Iteration 37/1000 | Loss: 0.00002916
Iteration 38/1000 | Loss: 0.00002916
Iteration 39/1000 | Loss: 0.00002916
Iteration 40/1000 | Loss: 0.00002916
Iteration 41/1000 | Loss: 0.00002916
Iteration 42/1000 | Loss: 0.00002916
Iteration 43/1000 | Loss: 0.00002916
Iteration 44/1000 | Loss: 0.00002916
Iteration 45/1000 | Loss: 0.00002916
Iteration 46/1000 | Loss: 0.00002916
Iteration 47/1000 | Loss: 0.00002915
Iteration 48/1000 | Loss: 0.00002915
Iteration 49/1000 | Loss: 0.00002915
Iteration 50/1000 | Loss: 0.00002914
Iteration 51/1000 | Loss: 0.00002914
Iteration 52/1000 | Loss: 0.00002914
Iteration 53/1000 | Loss: 0.00002914
Iteration 54/1000 | Loss: 0.00002914
Iteration 55/1000 | Loss: 0.00002914
Iteration 56/1000 | Loss: 0.00002913
Iteration 57/1000 | Loss: 0.00002913
Iteration 58/1000 | Loss: 0.00002913
Iteration 59/1000 | Loss: 0.00002913
Iteration 60/1000 | Loss: 0.00002913
Iteration 61/1000 | Loss: 0.00002913
Iteration 62/1000 | Loss: 0.00002913
Iteration 63/1000 | Loss: 0.00002913
Iteration 64/1000 | Loss: 0.00002913
Iteration 65/1000 | Loss: 0.00002913
Iteration 66/1000 | Loss: 0.00002913
Iteration 67/1000 | Loss: 0.00002913
Iteration 68/1000 | Loss: 0.00002913
Iteration 69/1000 | Loss: 0.00002913
Iteration 70/1000 | Loss: 0.00002913
Iteration 71/1000 | Loss: 0.00002913
Iteration 72/1000 | Loss: 0.00002913
Iteration 73/1000 | Loss: 0.00002913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.9126091249054298e-05, 2.9126091249054298e-05, 2.9126091249054298e-05, 2.9126091249054298e-05, 2.9126091249054298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9126091249054298e-05

Optimization complete. Final v2v error: 4.58214807510376 mm

Highest mean error: 4.949505805969238 mm for frame 18

Lowest mean error: 3.9772303104400635 mm for frame 106

Saving results

Total time: 50.46592378616333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884574
Iteration 2/25 | Loss: 0.00135668
Iteration 3/25 | Loss: 0.00088582
Iteration 4/25 | Loss: 0.00079897
Iteration 5/25 | Loss: 0.00077904
Iteration 6/25 | Loss: 0.00077236
Iteration 7/25 | Loss: 0.00076535
Iteration 8/25 | Loss: 0.00075399
Iteration 9/25 | Loss: 0.00070084
Iteration 10/25 | Loss: 0.00069300
Iteration 11/25 | Loss: 0.00069260
Iteration 12/25 | Loss: 0.00069252
Iteration 13/25 | Loss: 0.00069252
Iteration 14/25 | Loss: 0.00069252
Iteration 15/25 | Loss: 0.00069252
Iteration 16/25 | Loss: 0.00069252
Iteration 17/25 | Loss: 0.00069252
Iteration 18/25 | Loss: 0.00069252
Iteration 19/25 | Loss: 0.00069252
Iteration 20/25 | Loss: 0.00069252
Iteration 21/25 | Loss: 0.00069252
Iteration 22/25 | Loss: 0.00069252
Iteration 23/25 | Loss: 0.00069252
Iteration 24/25 | Loss: 0.00069251
Iteration 25/25 | Loss: 0.00069251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92851520
Iteration 2/25 | Loss: 0.00026829
Iteration 3/25 | Loss: 0.00026828
Iteration 4/25 | Loss: 0.00026828
Iteration 5/25 | Loss: 0.00026828
Iteration 6/25 | Loss: 0.00026828
Iteration 7/25 | Loss: 0.00026828
Iteration 8/25 | Loss: 0.00026828
Iteration 9/25 | Loss: 0.00026828
Iteration 10/25 | Loss: 0.00026828
Iteration 11/25 | Loss: 0.00026828
Iteration 12/25 | Loss: 0.00026828
Iteration 13/25 | Loss: 0.00026828
Iteration 14/25 | Loss: 0.00026828
Iteration 15/25 | Loss: 0.00026828
Iteration 16/25 | Loss: 0.00026828
Iteration 17/25 | Loss: 0.00026828
Iteration 18/25 | Loss: 0.00026828
Iteration 19/25 | Loss: 0.00026828
Iteration 20/25 | Loss: 0.00026828
Iteration 21/25 | Loss: 0.00026828
Iteration 22/25 | Loss: 0.00026828
Iteration 23/25 | Loss: 0.00026828
Iteration 24/25 | Loss: 0.00026828
Iteration 25/25 | Loss: 0.00026828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026828
Iteration 2/1000 | Loss: 0.00004352
Iteration 3/1000 | Loss: 0.00003416
Iteration 4/1000 | Loss: 0.00003047
Iteration 5/1000 | Loss: 0.00002846
Iteration 6/1000 | Loss: 0.00002730
Iteration 7/1000 | Loss: 0.00002649
Iteration 8/1000 | Loss: 0.00002589
Iteration 9/1000 | Loss: 0.00002551
Iteration 10/1000 | Loss: 0.00002524
Iteration 11/1000 | Loss: 0.00002508
Iteration 12/1000 | Loss: 0.00002504
Iteration 13/1000 | Loss: 0.00002497
Iteration 14/1000 | Loss: 0.00002493
Iteration 15/1000 | Loss: 0.00002492
Iteration 16/1000 | Loss: 0.00002492
Iteration 17/1000 | Loss: 0.00002492
Iteration 18/1000 | Loss: 0.00002491
Iteration 19/1000 | Loss: 0.00002490
Iteration 20/1000 | Loss: 0.00002489
Iteration 21/1000 | Loss: 0.00002482
Iteration 22/1000 | Loss: 0.00002482
Iteration 23/1000 | Loss: 0.00002480
Iteration 24/1000 | Loss: 0.00002479
Iteration 25/1000 | Loss: 0.00002479
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002478
Iteration 28/1000 | Loss: 0.00002478
Iteration 29/1000 | Loss: 0.00002477
Iteration 30/1000 | Loss: 0.00002475
Iteration 31/1000 | Loss: 0.00002474
Iteration 32/1000 | Loss: 0.00002474
Iteration 33/1000 | Loss: 0.00002472
Iteration 34/1000 | Loss: 0.00002472
Iteration 35/1000 | Loss: 0.00002472
Iteration 36/1000 | Loss: 0.00002472
Iteration 37/1000 | Loss: 0.00002472
Iteration 38/1000 | Loss: 0.00002472
Iteration 39/1000 | Loss: 0.00002472
Iteration 40/1000 | Loss: 0.00002472
Iteration 41/1000 | Loss: 0.00002472
Iteration 42/1000 | Loss: 0.00002472
Iteration 43/1000 | Loss: 0.00002472
Iteration 44/1000 | Loss: 0.00002472
Iteration 45/1000 | Loss: 0.00002472
Iteration 46/1000 | Loss: 0.00002472
Iteration 47/1000 | Loss: 0.00002472
Iteration 48/1000 | Loss: 0.00002472
Iteration 49/1000 | Loss: 0.00002472
Iteration 50/1000 | Loss: 0.00002472
Iteration 51/1000 | Loss: 0.00002472
Iteration 52/1000 | Loss: 0.00002472
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002472
Iteration 55/1000 | Loss: 0.00002472
Iteration 56/1000 | Loss: 0.00002472
Iteration 57/1000 | Loss: 0.00002472
Iteration 58/1000 | Loss: 0.00002472
Iteration 59/1000 | Loss: 0.00002472
Iteration 60/1000 | Loss: 0.00002472
Iteration 61/1000 | Loss: 0.00002472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [2.4717293854337186e-05, 2.4717293854337186e-05, 2.4717293854337186e-05, 2.4717293854337186e-05, 2.4717293854337186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4717293854337186e-05

Optimization complete. Final v2v error: 4.200438022613525 mm

Highest mean error: 4.479363918304443 mm for frame 81

Lowest mean error: 3.903517484664917 mm for frame 48

Saving results

Total time: 41.36719870567322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919483
Iteration 2/25 | Loss: 0.00124094
Iteration 3/25 | Loss: 0.00087890
Iteration 4/25 | Loss: 0.00083907
Iteration 5/25 | Loss: 0.00081632
Iteration 6/25 | Loss: 0.00078466
Iteration 7/25 | Loss: 0.00074698
Iteration 8/25 | Loss: 0.00073504
Iteration 9/25 | Loss: 0.00072245
Iteration 10/25 | Loss: 0.00072215
Iteration 11/25 | Loss: 0.00071778
Iteration 12/25 | Loss: 0.00071542
Iteration 13/25 | Loss: 0.00071481
Iteration 14/25 | Loss: 0.00071011
Iteration 15/25 | Loss: 0.00071523
Iteration 16/25 | Loss: 0.00071054
Iteration 17/25 | Loss: 0.00071024
Iteration 18/25 | Loss: 0.00069967
Iteration 19/25 | Loss: 0.00070471
Iteration 20/25 | Loss: 0.00070025
Iteration 21/25 | Loss: 0.00070204
Iteration 22/25 | Loss: 0.00070319
Iteration 23/25 | Loss: 0.00070142
Iteration 24/25 | Loss: 0.00070149
Iteration 25/25 | Loss: 0.00070125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77748442
Iteration 2/25 | Loss: 0.00054277
Iteration 3/25 | Loss: 0.00049566
Iteration 4/25 | Loss: 0.00049566
Iteration 5/25 | Loss: 0.00049566
Iteration 6/25 | Loss: 0.00049566
Iteration 7/25 | Loss: 0.00049565
Iteration 8/25 | Loss: 0.00049565
Iteration 9/25 | Loss: 0.00049565
Iteration 10/25 | Loss: 0.00049565
Iteration 11/25 | Loss: 0.00049565
Iteration 12/25 | Loss: 0.00049565
Iteration 13/25 | Loss: 0.00049565
Iteration 14/25 | Loss: 0.00049565
Iteration 15/25 | Loss: 0.00049565
Iteration 16/25 | Loss: 0.00049565
Iteration 17/25 | Loss: 0.00049565
Iteration 18/25 | Loss: 0.00049565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004956531920470297, 0.0004956531920470297, 0.0004956531920470297, 0.0004956531920470297, 0.0004956531920470297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004956531920470297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049565
Iteration 2/1000 | Loss: 0.00060683
Iteration 3/1000 | Loss: 0.00058382
Iteration 4/1000 | Loss: 0.00054365
Iteration 5/1000 | Loss: 0.00050430
Iteration 6/1000 | Loss: 0.00066702
Iteration 7/1000 | Loss: 0.00072799
Iteration 8/1000 | Loss: 0.00078438
Iteration 9/1000 | Loss: 0.00042150
Iteration 10/1000 | Loss: 0.00033077
Iteration 11/1000 | Loss: 0.00036533
Iteration 12/1000 | Loss: 0.00016886
Iteration 13/1000 | Loss: 0.00007036
Iteration 14/1000 | Loss: 0.00028764
Iteration 15/1000 | Loss: 0.00104848
Iteration 16/1000 | Loss: 0.00043119
Iteration 17/1000 | Loss: 0.00048378
Iteration 18/1000 | Loss: 0.00010622
Iteration 19/1000 | Loss: 0.00015799
Iteration 20/1000 | Loss: 0.00004968
Iteration 21/1000 | Loss: 0.00004090
Iteration 22/1000 | Loss: 0.00004355
Iteration 23/1000 | Loss: 0.00023342
Iteration 24/1000 | Loss: 0.00004395
Iteration 25/1000 | Loss: 0.00005110
Iteration 26/1000 | Loss: 0.00005797
Iteration 27/1000 | Loss: 0.00004216
Iteration 28/1000 | Loss: 0.00004050
Iteration 29/1000 | Loss: 0.00020375
Iteration 30/1000 | Loss: 0.00127777
Iteration 31/1000 | Loss: 0.00015504
Iteration 32/1000 | Loss: 0.00014498
Iteration 33/1000 | Loss: 0.00016178
Iteration 34/1000 | Loss: 0.00004382
Iteration 35/1000 | Loss: 0.00004040
Iteration 36/1000 | Loss: 0.00014806
Iteration 37/1000 | Loss: 0.00012019
Iteration 38/1000 | Loss: 0.00014808
Iteration 39/1000 | Loss: 0.00011679
Iteration 40/1000 | Loss: 0.00014729
Iteration 41/1000 | Loss: 0.00013828
Iteration 42/1000 | Loss: 0.00014653
Iteration 43/1000 | Loss: 0.00016486
Iteration 44/1000 | Loss: 0.00017428
Iteration 45/1000 | Loss: 0.00015809
Iteration 46/1000 | Loss: 0.00004627
Iteration 47/1000 | Loss: 0.00004108
Iteration 48/1000 | Loss: 0.00003750
Iteration 49/1000 | Loss: 0.00005740
Iteration 50/1000 | Loss: 0.00004898
Iteration 51/1000 | Loss: 0.00003249
Iteration 52/1000 | Loss: 0.00003532
Iteration 53/1000 | Loss: 0.00003617
Iteration 54/1000 | Loss: 0.00004604
Iteration 55/1000 | Loss: 0.00004503
Iteration 56/1000 | Loss: 0.00004571
Iteration 57/1000 | Loss: 0.00004491
Iteration 58/1000 | Loss: 0.00003654
Iteration 59/1000 | Loss: 0.00003883
Iteration 60/1000 | Loss: 0.00003553
Iteration 61/1000 | Loss: 0.00004245
Iteration 62/1000 | Loss: 0.00004038
Iteration 63/1000 | Loss: 0.00004010
Iteration 64/1000 | Loss: 0.00004119
Iteration 65/1000 | Loss: 0.00005156
Iteration 66/1000 | Loss: 0.00003102
Iteration 67/1000 | Loss: 0.00002909
Iteration 68/1000 | Loss: 0.00002812
Iteration 69/1000 | Loss: 0.00002751
Iteration 70/1000 | Loss: 0.00002706
Iteration 71/1000 | Loss: 0.00002692
Iteration 72/1000 | Loss: 0.00002682
Iteration 73/1000 | Loss: 0.00002679
Iteration 74/1000 | Loss: 0.00002674
Iteration 75/1000 | Loss: 0.00002673
Iteration 76/1000 | Loss: 0.00002672
Iteration 77/1000 | Loss: 0.00002665
Iteration 78/1000 | Loss: 0.00002664
Iteration 79/1000 | Loss: 0.00002664
Iteration 80/1000 | Loss: 0.00002658
Iteration 81/1000 | Loss: 0.00002658
Iteration 82/1000 | Loss: 0.00002657
Iteration 83/1000 | Loss: 0.00002655
Iteration 84/1000 | Loss: 0.00002654
Iteration 85/1000 | Loss: 0.00002652
Iteration 86/1000 | Loss: 0.00002652
Iteration 87/1000 | Loss: 0.00002651
Iteration 88/1000 | Loss: 0.00002650
Iteration 89/1000 | Loss: 0.00002650
Iteration 90/1000 | Loss: 0.00002648
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002642
Iteration 93/1000 | Loss: 0.00002631
Iteration 94/1000 | Loss: 0.00002630
Iteration 95/1000 | Loss: 0.00002621
Iteration 96/1000 | Loss: 0.00002614
Iteration 97/1000 | Loss: 0.00002614
Iteration 98/1000 | Loss: 0.00002613
Iteration 99/1000 | Loss: 0.00002613
Iteration 100/1000 | Loss: 0.00002612
Iteration 101/1000 | Loss: 0.00002612
Iteration 102/1000 | Loss: 0.00002612
Iteration 103/1000 | Loss: 0.00002611
Iteration 104/1000 | Loss: 0.00002611
Iteration 105/1000 | Loss: 0.00002611
Iteration 106/1000 | Loss: 0.00002610
Iteration 107/1000 | Loss: 0.00002610
Iteration 108/1000 | Loss: 0.00002610
Iteration 109/1000 | Loss: 0.00002610
Iteration 110/1000 | Loss: 0.00002609
Iteration 111/1000 | Loss: 0.00002609
Iteration 112/1000 | Loss: 0.00002609
Iteration 113/1000 | Loss: 0.00002609
Iteration 114/1000 | Loss: 0.00002609
Iteration 115/1000 | Loss: 0.00002609
Iteration 116/1000 | Loss: 0.00002609
Iteration 117/1000 | Loss: 0.00002609
Iteration 118/1000 | Loss: 0.00002609
Iteration 119/1000 | Loss: 0.00002609
Iteration 120/1000 | Loss: 0.00002609
Iteration 121/1000 | Loss: 0.00002609
Iteration 122/1000 | Loss: 0.00002609
Iteration 123/1000 | Loss: 0.00002608
Iteration 124/1000 | Loss: 0.00002608
Iteration 125/1000 | Loss: 0.00002608
Iteration 126/1000 | Loss: 0.00002608
Iteration 127/1000 | Loss: 0.00002608
Iteration 128/1000 | Loss: 0.00002608
Iteration 129/1000 | Loss: 0.00002608
Iteration 130/1000 | Loss: 0.00002608
Iteration 131/1000 | Loss: 0.00002608
Iteration 132/1000 | Loss: 0.00002608
Iteration 133/1000 | Loss: 0.00002608
Iteration 134/1000 | Loss: 0.00002608
Iteration 135/1000 | Loss: 0.00002608
Iteration 136/1000 | Loss: 0.00002607
Iteration 137/1000 | Loss: 0.00002607
Iteration 138/1000 | Loss: 0.00002607
Iteration 139/1000 | Loss: 0.00002607
Iteration 140/1000 | Loss: 0.00002607
Iteration 141/1000 | Loss: 0.00002607
Iteration 142/1000 | Loss: 0.00002607
Iteration 143/1000 | Loss: 0.00002607
Iteration 144/1000 | Loss: 0.00002607
Iteration 145/1000 | Loss: 0.00002607
Iteration 146/1000 | Loss: 0.00002607
Iteration 147/1000 | Loss: 0.00002607
Iteration 148/1000 | Loss: 0.00002607
Iteration 149/1000 | Loss: 0.00002607
Iteration 150/1000 | Loss: 0.00002607
Iteration 151/1000 | Loss: 0.00002606
Iteration 152/1000 | Loss: 0.00002606
Iteration 153/1000 | Loss: 0.00002606
Iteration 154/1000 | Loss: 0.00002606
Iteration 155/1000 | Loss: 0.00002606
Iteration 156/1000 | Loss: 0.00002606
Iteration 157/1000 | Loss: 0.00002606
Iteration 158/1000 | Loss: 0.00002606
Iteration 159/1000 | Loss: 0.00002606
Iteration 160/1000 | Loss: 0.00002606
Iteration 161/1000 | Loss: 0.00002606
Iteration 162/1000 | Loss: 0.00002606
Iteration 163/1000 | Loss: 0.00002605
Iteration 164/1000 | Loss: 0.00002605
Iteration 165/1000 | Loss: 0.00002605
Iteration 166/1000 | Loss: 0.00002605
Iteration 167/1000 | Loss: 0.00002605
Iteration 168/1000 | Loss: 0.00002605
Iteration 169/1000 | Loss: 0.00002605
Iteration 170/1000 | Loss: 0.00002605
Iteration 171/1000 | Loss: 0.00002605
Iteration 172/1000 | Loss: 0.00002605
Iteration 173/1000 | Loss: 0.00002605
Iteration 174/1000 | Loss: 0.00002605
Iteration 175/1000 | Loss: 0.00002605
Iteration 176/1000 | Loss: 0.00002605
Iteration 177/1000 | Loss: 0.00002604
Iteration 178/1000 | Loss: 0.00002604
Iteration 179/1000 | Loss: 0.00002604
Iteration 180/1000 | Loss: 0.00002604
Iteration 181/1000 | Loss: 0.00002604
Iteration 182/1000 | Loss: 0.00002604
Iteration 183/1000 | Loss: 0.00002604
Iteration 184/1000 | Loss: 0.00002604
Iteration 185/1000 | Loss: 0.00002604
Iteration 186/1000 | Loss: 0.00002604
Iteration 187/1000 | Loss: 0.00002604
Iteration 188/1000 | Loss: 0.00002603
Iteration 189/1000 | Loss: 0.00002603
Iteration 190/1000 | Loss: 0.00002603
Iteration 191/1000 | Loss: 0.00002603
Iteration 192/1000 | Loss: 0.00002603
Iteration 193/1000 | Loss: 0.00002603
Iteration 194/1000 | Loss: 0.00002603
Iteration 195/1000 | Loss: 0.00002603
Iteration 196/1000 | Loss: 0.00002603
Iteration 197/1000 | Loss: 0.00002603
Iteration 198/1000 | Loss: 0.00002603
Iteration 199/1000 | Loss: 0.00002603
Iteration 200/1000 | Loss: 0.00002603
Iteration 201/1000 | Loss: 0.00002603
Iteration 202/1000 | Loss: 0.00002603
Iteration 203/1000 | Loss: 0.00002603
Iteration 204/1000 | Loss: 0.00002603
Iteration 205/1000 | Loss: 0.00002603
Iteration 206/1000 | Loss: 0.00002603
Iteration 207/1000 | Loss: 0.00002603
Iteration 208/1000 | Loss: 0.00002603
Iteration 209/1000 | Loss: 0.00002603
Iteration 210/1000 | Loss: 0.00002603
Iteration 211/1000 | Loss: 0.00002603
Iteration 212/1000 | Loss: 0.00002603
Iteration 213/1000 | Loss: 0.00002603
Iteration 214/1000 | Loss: 0.00002603
Iteration 215/1000 | Loss: 0.00002603
Iteration 216/1000 | Loss: 0.00002603
Iteration 217/1000 | Loss: 0.00002603
Iteration 218/1000 | Loss: 0.00002603
Iteration 219/1000 | Loss: 0.00002603
Iteration 220/1000 | Loss: 0.00002603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.602874519652687e-05, 2.602874519652687e-05, 2.602874519652687e-05, 2.602874519652687e-05, 2.602874519652687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.602874519652687e-05

Optimization complete. Final v2v error: 4.164863109588623 mm

Highest mean error: 6.939325332641602 mm for frame 43

Lowest mean error: 3.024590492248535 mm for frame 82

Saving results

Total time: 185.08107495307922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483342
Iteration 2/25 | Loss: 0.00068455
Iteration 3/25 | Loss: 0.00056153
Iteration 4/25 | Loss: 0.00051917
Iteration 5/25 | Loss: 0.00050902
Iteration 6/25 | Loss: 0.00050788
Iteration 7/25 | Loss: 0.00050786
Iteration 8/25 | Loss: 0.00050786
Iteration 9/25 | Loss: 0.00050786
Iteration 10/25 | Loss: 0.00050786
Iteration 11/25 | Loss: 0.00050786
Iteration 12/25 | Loss: 0.00050786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005078588146716356, 0.0005078588146716356, 0.0005078588146716356, 0.0005078588146716356, 0.0005078588146716356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005078588146716356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.28166008
Iteration 2/25 | Loss: 0.00011434
Iteration 3/25 | Loss: 0.00011434
Iteration 4/25 | Loss: 0.00011434
Iteration 5/25 | Loss: 0.00011434
Iteration 6/25 | Loss: 0.00011434
Iteration 7/25 | Loss: 0.00011434
Iteration 8/25 | Loss: 0.00011434
Iteration 9/25 | Loss: 0.00011434
Iteration 10/25 | Loss: 0.00011434
Iteration 11/25 | Loss: 0.00011434
Iteration 12/25 | Loss: 0.00011434
Iteration 13/25 | Loss: 0.00011434
Iteration 14/25 | Loss: 0.00011434
Iteration 15/25 | Loss: 0.00011434
Iteration 16/25 | Loss: 0.00011434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00011434203770477325, 0.00011434203770477325, 0.00011434203770477325, 0.00011434203770477325, 0.00011434203770477325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011434203770477325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011434
Iteration 2/1000 | Loss: 0.00002280
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001628
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001364
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001332
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001321
Iteration 18/1000 | Loss: 0.00001320
Iteration 19/1000 | Loss: 0.00001320
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001316
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001312
Iteration 32/1000 | Loss: 0.00001312
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001312
Iteration 36/1000 | Loss: 0.00001311
Iteration 37/1000 | Loss: 0.00001311
Iteration 38/1000 | Loss: 0.00001311
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001306
Iteration 51/1000 | Loss: 0.00001306
Iteration 52/1000 | Loss: 0.00001305
Iteration 53/1000 | Loss: 0.00001305
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001304
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001303
Iteration 60/1000 | Loss: 0.00001303
Iteration 61/1000 | Loss: 0.00001302
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001299
Iteration 66/1000 | Loss: 0.00001299
Iteration 67/1000 | Loss: 0.00001299
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001295
Iteration 80/1000 | Loss: 0.00001295
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001295
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001295
Iteration 104/1000 | Loss: 0.00001295
Iteration 105/1000 | Loss: 0.00001295
Iteration 106/1000 | Loss: 0.00001295
Iteration 107/1000 | Loss: 0.00001295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.2949474694323726e-05, 1.2949474694323726e-05, 1.2949474694323726e-05, 1.2949474694323726e-05, 1.2949474694323726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2949474694323726e-05

Optimization complete. Final v2v error: 3.0417232513427734 mm

Highest mean error: 3.3874270915985107 mm for frame 112

Lowest mean error: 2.7723920345306396 mm for frame 47

Saving results

Total time: 31.409159898757935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845744
Iteration 2/25 | Loss: 0.00072266
Iteration 3/25 | Loss: 0.00055185
Iteration 4/25 | Loss: 0.00053018
Iteration 5/25 | Loss: 0.00052520
Iteration 6/25 | Loss: 0.00052339
Iteration 7/25 | Loss: 0.00052291
Iteration 8/25 | Loss: 0.00052291
Iteration 9/25 | Loss: 0.00052291
Iteration 10/25 | Loss: 0.00052291
Iteration 11/25 | Loss: 0.00052291
Iteration 12/25 | Loss: 0.00052291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005229148082435131, 0.0005229148082435131, 0.0005229148082435131, 0.0005229148082435131, 0.0005229148082435131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005229148082435131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35321200
Iteration 2/25 | Loss: 0.00009535
Iteration 3/25 | Loss: 0.00009535
Iteration 4/25 | Loss: 0.00009535
Iteration 5/25 | Loss: 0.00009535
Iteration 6/25 | Loss: 0.00009535
Iteration 7/25 | Loss: 0.00009535
Iteration 8/25 | Loss: 0.00009535
Iteration 9/25 | Loss: 0.00009535
Iteration 10/25 | Loss: 0.00009535
Iteration 11/25 | Loss: 0.00009535
Iteration 12/25 | Loss: 0.00009535
Iteration 13/25 | Loss: 0.00009535
Iteration 14/25 | Loss: 0.00009535
Iteration 15/25 | Loss: 0.00009535
Iteration 16/25 | Loss: 0.00009535
Iteration 17/25 | Loss: 0.00009535
Iteration 18/25 | Loss: 0.00009535
Iteration 19/25 | Loss: 0.00009535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [9.534526179777458e-05, 9.534526179777458e-05, 9.534526179777458e-05, 9.534526179777458e-05, 9.534526179777458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.534526179777458e-05

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009535
Iteration 2/1000 | Loss: 0.00002258
Iteration 3/1000 | Loss: 0.00001616
Iteration 4/1000 | Loss: 0.00001492
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001382
Iteration 7/1000 | Loss: 0.00001347
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001251
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.2463245184335392e-05, 1.2463245184335392e-05, 1.2463245184335392e-05, 1.2463245184335392e-05, 1.2463245184335392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2463245184335392e-05

Optimization complete. Final v2v error: 2.9558162689208984 mm

Highest mean error: 4.052536487579346 mm for frame 71

Lowest mean error: 2.1116318702697754 mm for frame 1

Saving results

Total time: 31.896970748901367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808827
Iteration 2/25 | Loss: 0.00086817
Iteration 3/25 | Loss: 0.00059890
Iteration 4/25 | Loss: 0.00055758
Iteration 5/25 | Loss: 0.00055229
Iteration 6/25 | Loss: 0.00055016
Iteration 7/25 | Loss: 0.00054992
Iteration 8/25 | Loss: 0.00054992
Iteration 9/25 | Loss: 0.00054992
Iteration 10/25 | Loss: 0.00054992
Iteration 11/25 | Loss: 0.00054992
Iteration 12/25 | Loss: 0.00054992
Iteration 13/25 | Loss: 0.00054992
Iteration 14/25 | Loss: 0.00054992
Iteration 15/25 | Loss: 0.00054992
Iteration 16/25 | Loss: 0.00054992
Iteration 17/25 | Loss: 0.00054992
Iteration 18/25 | Loss: 0.00054992
Iteration 19/25 | Loss: 0.00054992
Iteration 20/25 | Loss: 0.00054992
Iteration 21/25 | Loss: 0.00054992
Iteration 22/25 | Loss: 0.00054992
Iteration 23/25 | Loss: 0.00054992
Iteration 24/25 | Loss: 0.00054992
Iteration 25/25 | Loss: 0.00054992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34539604
Iteration 2/25 | Loss: 0.00007991
Iteration 3/25 | Loss: 0.00007989
Iteration 4/25 | Loss: 0.00007989
Iteration 5/25 | Loss: 0.00007989
Iteration 6/25 | Loss: 0.00007989
Iteration 7/25 | Loss: 0.00007989
Iteration 8/25 | Loss: 0.00007989
Iteration 9/25 | Loss: 0.00007989
Iteration 10/25 | Loss: 0.00007989
Iteration 11/25 | Loss: 0.00007989
Iteration 12/25 | Loss: 0.00007989
Iteration 13/25 | Loss: 0.00007989
Iteration 14/25 | Loss: 0.00007989
Iteration 15/25 | Loss: 0.00007989
Iteration 16/25 | Loss: 0.00007989
Iteration 17/25 | Loss: 0.00007989
Iteration 18/25 | Loss: 0.00007989
Iteration 19/25 | Loss: 0.00007989
Iteration 20/25 | Loss: 0.00007989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [7.988954894244671e-05, 7.988954894244671e-05, 7.988954894244671e-05, 7.988954894244671e-05, 7.988954894244671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.988954894244671e-05

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00007989
Iteration 2/1000 | Loss: 0.00002262
Iteration 3/1000 | Loss: 0.00001922
Iteration 4/1000 | Loss: 0.00001788
Iteration 5/1000 | Loss: 0.00001695
Iteration 6/1000 | Loss: 0.00001652
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001580
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001546
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001540
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001539
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001537
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001533
Iteration 27/1000 | Loss: 0.00001533
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001529
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001527
Iteration 41/1000 | Loss: 0.00001527
Iteration 42/1000 | Loss: 0.00001527
Iteration 43/1000 | Loss: 0.00001527
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00001526
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001526
Iteration 50/1000 | Loss: 0.00001526
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001518
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001517
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001517
Iteration 94/1000 | Loss: 0.00001517
Iteration 95/1000 | Loss: 0.00001517
Iteration 96/1000 | Loss: 0.00001517
Iteration 97/1000 | Loss: 0.00001517
Iteration 98/1000 | Loss: 0.00001517
Iteration 99/1000 | Loss: 0.00001517
Iteration 100/1000 | Loss: 0.00001517
Iteration 101/1000 | Loss: 0.00001517
Iteration 102/1000 | Loss: 0.00001517
Iteration 103/1000 | Loss: 0.00001517
Iteration 104/1000 | Loss: 0.00001517
Iteration 105/1000 | Loss: 0.00001517
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.5172103303484619e-05, 1.5172103303484619e-05, 1.5172103303484619e-05, 1.5172103303484619e-05, 1.5172103303484619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5172103303484619e-05

Optimization complete. Final v2v error: 3.22328519821167 mm

Highest mean error: 3.6822781562805176 mm for frame 233

Lowest mean error: 2.862215995788574 mm for frame 80

Saving results

Total time: 36.5851788520813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00333491
Iteration 2/25 | Loss: 0.00062948
Iteration 3/25 | Loss: 0.00048474
Iteration 4/25 | Loss: 0.00045932
Iteration 5/25 | Loss: 0.00045191
Iteration 6/25 | Loss: 0.00045009
Iteration 7/25 | Loss: 0.00044956
Iteration 8/25 | Loss: 0.00044956
Iteration 9/25 | Loss: 0.00044956
Iteration 10/25 | Loss: 0.00044956
Iteration 11/25 | Loss: 0.00044956
Iteration 12/25 | Loss: 0.00044956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0004495578759815544, 0.0004495578759815544, 0.0004495578759815544, 0.0004495578759815544, 0.0004495578759815544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004495578759815544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36606383
Iteration 2/25 | Loss: 0.00006861
Iteration 3/25 | Loss: 0.00006861
Iteration 4/25 | Loss: 0.00006861
Iteration 5/25 | Loss: 0.00006861
Iteration 6/25 | Loss: 0.00006861
Iteration 7/25 | Loss: 0.00006861
Iteration 8/25 | Loss: 0.00006861
Iteration 9/25 | Loss: 0.00006861
Iteration 10/25 | Loss: 0.00006860
Iteration 11/25 | Loss: 0.00006860
Iteration 12/25 | Loss: 0.00006860
Iteration 13/25 | Loss: 0.00006860
Iteration 14/25 | Loss: 0.00006860
Iteration 15/25 | Loss: 0.00006860
Iteration 16/25 | Loss: 0.00006860
Iteration 17/25 | Loss: 0.00006860
Iteration 18/25 | Loss: 0.00006860
Iteration 19/25 | Loss: 0.00006860
Iteration 20/25 | Loss: 0.00006860
Iteration 21/25 | Loss: 0.00006860
Iteration 22/25 | Loss: 0.00006860
Iteration 23/25 | Loss: 0.00006860
Iteration 24/25 | Loss: 0.00006860
Iteration 25/25 | Loss: 0.00006860

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00006860
Iteration 2/1000 | Loss: 0.00001504
Iteration 3/1000 | Loss: 0.00001019
Iteration 4/1000 | Loss: 0.00000962
Iteration 5/1000 | Loss: 0.00000918
Iteration 6/1000 | Loss: 0.00000914
Iteration 7/1000 | Loss: 0.00000899
Iteration 8/1000 | Loss: 0.00000870
Iteration 9/1000 | Loss: 0.00000860
Iteration 10/1000 | Loss: 0.00000858
Iteration 11/1000 | Loss: 0.00000855
Iteration 12/1000 | Loss: 0.00000845
Iteration 13/1000 | Loss: 0.00000841
Iteration 14/1000 | Loss: 0.00000841
Iteration 15/1000 | Loss: 0.00000841
Iteration 16/1000 | Loss: 0.00000841
Iteration 17/1000 | Loss: 0.00000841
Iteration 18/1000 | Loss: 0.00000841
Iteration 19/1000 | Loss: 0.00000840
Iteration 20/1000 | Loss: 0.00000840
Iteration 21/1000 | Loss: 0.00000840
Iteration 22/1000 | Loss: 0.00000839
Iteration 23/1000 | Loss: 0.00000837
Iteration 24/1000 | Loss: 0.00000836
Iteration 25/1000 | Loss: 0.00000836
Iteration 26/1000 | Loss: 0.00000836
Iteration 27/1000 | Loss: 0.00000836
Iteration 28/1000 | Loss: 0.00000835
Iteration 29/1000 | Loss: 0.00000832
Iteration 30/1000 | Loss: 0.00000832
Iteration 31/1000 | Loss: 0.00000832
Iteration 32/1000 | Loss: 0.00000832
Iteration 33/1000 | Loss: 0.00000832
Iteration 34/1000 | Loss: 0.00000832
Iteration 35/1000 | Loss: 0.00000832
Iteration 36/1000 | Loss: 0.00000832
Iteration 37/1000 | Loss: 0.00000831
Iteration 38/1000 | Loss: 0.00000831
Iteration 39/1000 | Loss: 0.00000831
Iteration 40/1000 | Loss: 0.00000830
Iteration 41/1000 | Loss: 0.00000830
Iteration 42/1000 | Loss: 0.00000829
Iteration 43/1000 | Loss: 0.00000829
Iteration 44/1000 | Loss: 0.00000829
Iteration 45/1000 | Loss: 0.00000829
Iteration 46/1000 | Loss: 0.00000829
Iteration 47/1000 | Loss: 0.00000829
Iteration 48/1000 | Loss: 0.00000829
Iteration 49/1000 | Loss: 0.00000829
Iteration 50/1000 | Loss: 0.00000829
Iteration 51/1000 | Loss: 0.00000828
Iteration 52/1000 | Loss: 0.00000828
Iteration 53/1000 | Loss: 0.00000828
Iteration 54/1000 | Loss: 0.00000828
Iteration 55/1000 | Loss: 0.00000828
Iteration 56/1000 | Loss: 0.00000828
Iteration 57/1000 | Loss: 0.00000828
Iteration 58/1000 | Loss: 0.00000828
Iteration 59/1000 | Loss: 0.00000828
Iteration 60/1000 | Loss: 0.00000828
Iteration 61/1000 | Loss: 0.00000827
Iteration 62/1000 | Loss: 0.00000827
Iteration 63/1000 | Loss: 0.00000827
Iteration 64/1000 | Loss: 0.00000826
Iteration 65/1000 | Loss: 0.00000826
Iteration 66/1000 | Loss: 0.00000826
Iteration 67/1000 | Loss: 0.00000826
Iteration 68/1000 | Loss: 0.00000826
Iteration 69/1000 | Loss: 0.00000826
Iteration 70/1000 | Loss: 0.00000826
Iteration 71/1000 | Loss: 0.00000826
Iteration 72/1000 | Loss: 0.00000825
Iteration 73/1000 | Loss: 0.00000825
Iteration 74/1000 | Loss: 0.00000825
Iteration 75/1000 | Loss: 0.00000825
Iteration 76/1000 | Loss: 0.00000825
Iteration 77/1000 | Loss: 0.00000825
Iteration 78/1000 | Loss: 0.00000825
Iteration 79/1000 | Loss: 0.00000825
Iteration 80/1000 | Loss: 0.00000825
Iteration 81/1000 | Loss: 0.00000825
Iteration 82/1000 | Loss: 0.00000824
Iteration 83/1000 | Loss: 0.00000824
Iteration 84/1000 | Loss: 0.00000824
Iteration 85/1000 | Loss: 0.00000824
Iteration 86/1000 | Loss: 0.00000823
Iteration 87/1000 | Loss: 0.00000823
Iteration 88/1000 | Loss: 0.00000823
Iteration 89/1000 | Loss: 0.00000823
Iteration 90/1000 | Loss: 0.00000823
Iteration 91/1000 | Loss: 0.00000822
Iteration 92/1000 | Loss: 0.00000822
Iteration 93/1000 | Loss: 0.00000822
Iteration 94/1000 | Loss: 0.00000822
Iteration 95/1000 | Loss: 0.00000822
Iteration 96/1000 | Loss: 0.00000822
Iteration 97/1000 | Loss: 0.00000822
Iteration 98/1000 | Loss: 0.00000822
Iteration 99/1000 | Loss: 0.00000822
Iteration 100/1000 | Loss: 0.00000822
Iteration 101/1000 | Loss: 0.00000821
Iteration 102/1000 | Loss: 0.00000821
Iteration 103/1000 | Loss: 0.00000821
Iteration 104/1000 | Loss: 0.00000821
Iteration 105/1000 | Loss: 0.00000821
Iteration 106/1000 | Loss: 0.00000821
Iteration 107/1000 | Loss: 0.00000821
Iteration 108/1000 | Loss: 0.00000821
Iteration 109/1000 | Loss: 0.00000821
Iteration 110/1000 | Loss: 0.00000821
Iteration 111/1000 | Loss: 0.00000820
Iteration 112/1000 | Loss: 0.00000820
Iteration 113/1000 | Loss: 0.00000820
Iteration 114/1000 | Loss: 0.00000820
Iteration 115/1000 | Loss: 0.00000820
Iteration 116/1000 | Loss: 0.00000820
Iteration 117/1000 | Loss: 0.00000820
Iteration 118/1000 | Loss: 0.00000820
Iteration 119/1000 | Loss: 0.00000820
Iteration 120/1000 | Loss: 0.00000820
Iteration 121/1000 | Loss: 0.00000820
Iteration 122/1000 | Loss: 0.00000820
Iteration 123/1000 | Loss: 0.00000820
Iteration 124/1000 | Loss: 0.00000820
Iteration 125/1000 | Loss: 0.00000820
Iteration 126/1000 | Loss: 0.00000820
Iteration 127/1000 | Loss: 0.00000820
Iteration 128/1000 | Loss: 0.00000820
Iteration 129/1000 | Loss: 0.00000820
Iteration 130/1000 | Loss: 0.00000820
Iteration 131/1000 | Loss: 0.00000820
Iteration 132/1000 | Loss: 0.00000820
Iteration 133/1000 | Loss: 0.00000820
Iteration 134/1000 | Loss: 0.00000820
Iteration 135/1000 | Loss: 0.00000820
Iteration 136/1000 | Loss: 0.00000820
Iteration 137/1000 | Loss: 0.00000820
Iteration 138/1000 | Loss: 0.00000820
Iteration 139/1000 | Loss: 0.00000820
Iteration 140/1000 | Loss: 0.00000820
Iteration 141/1000 | Loss: 0.00000820
Iteration 142/1000 | Loss: 0.00000820
Iteration 143/1000 | Loss: 0.00000820
Iteration 144/1000 | Loss: 0.00000820
Iteration 145/1000 | Loss: 0.00000820
Iteration 146/1000 | Loss: 0.00000820
Iteration 147/1000 | Loss: 0.00000820
Iteration 148/1000 | Loss: 0.00000820
Iteration 149/1000 | Loss: 0.00000820
Iteration 150/1000 | Loss: 0.00000820
Iteration 151/1000 | Loss: 0.00000820
Iteration 152/1000 | Loss: 0.00000820
Iteration 153/1000 | Loss: 0.00000820
Iteration 154/1000 | Loss: 0.00000820
Iteration 155/1000 | Loss: 0.00000820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [8.195449481718242e-06, 8.195449481718242e-06, 8.195449481718242e-06, 8.195449481718242e-06, 8.195449481718242e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.195449481718242e-06

Optimization complete. Final v2v error: 2.4309966564178467 mm

Highest mean error: 3.0264363288879395 mm for frame 157

Lowest mean error: 2.0290255546569824 mm for frame 137

Saving results

Total time: 32.36249589920044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883835
Iteration 2/25 | Loss: 0.00100677
Iteration 3/25 | Loss: 0.00068000
Iteration 4/25 | Loss: 0.00062502
Iteration 5/25 | Loss: 0.00060405
Iteration 6/25 | Loss: 0.00059536
Iteration 7/25 | Loss: 0.00059245
Iteration 8/25 | Loss: 0.00059092
Iteration 9/25 | Loss: 0.00059021
Iteration 10/25 | Loss: 0.00059004
Iteration 11/25 | Loss: 0.00059004
Iteration 12/25 | Loss: 0.00059004
Iteration 13/25 | Loss: 0.00059004
Iteration 14/25 | Loss: 0.00059004
Iteration 15/25 | Loss: 0.00059004
Iteration 16/25 | Loss: 0.00059004
Iteration 17/25 | Loss: 0.00059004
Iteration 18/25 | Loss: 0.00059004
Iteration 19/25 | Loss: 0.00059004
Iteration 20/25 | Loss: 0.00059004
Iteration 21/25 | Loss: 0.00059004
Iteration 22/25 | Loss: 0.00059004
Iteration 23/25 | Loss: 0.00059004
Iteration 24/25 | Loss: 0.00059004
Iteration 25/25 | Loss: 0.00059004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39569211
Iteration 2/25 | Loss: 0.00011318
Iteration 3/25 | Loss: 0.00011318
Iteration 4/25 | Loss: 0.00011318
Iteration 5/25 | Loss: 0.00011318
Iteration 6/25 | Loss: 0.00011318
Iteration 7/25 | Loss: 0.00011318
Iteration 8/25 | Loss: 0.00011318
Iteration 9/25 | Loss: 0.00011318
Iteration 10/25 | Loss: 0.00011318
Iteration 11/25 | Loss: 0.00011318
Iteration 12/25 | Loss: 0.00011318
Iteration 13/25 | Loss: 0.00011318
Iteration 14/25 | Loss: 0.00011318
Iteration 15/25 | Loss: 0.00011318
Iteration 16/25 | Loss: 0.00011318
Iteration 17/25 | Loss: 0.00011318
Iteration 18/25 | Loss: 0.00011318
Iteration 19/25 | Loss: 0.00011318
Iteration 20/25 | Loss: 0.00011318
Iteration 21/25 | Loss: 0.00011318
Iteration 22/25 | Loss: 0.00011318
Iteration 23/25 | Loss: 0.00011318
Iteration 24/25 | Loss: 0.00011318
Iteration 25/25 | Loss: 0.00011318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011318
Iteration 2/1000 | Loss: 0.00003216
Iteration 3/1000 | Loss: 0.00002430
Iteration 4/1000 | Loss: 0.00002189
Iteration 5/1000 | Loss: 0.00002112
Iteration 6/1000 | Loss: 0.00002058
Iteration 7/1000 | Loss: 0.00002010
Iteration 8/1000 | Loss: 0.00001985
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001905
Iteration 12/1000 | Loss: 0.00001904
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001898
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001885
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001876
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001872
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001869
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001866
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001865
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001859
Iteration 60/1000 | Loss: 0.00001859
Iteration 61/1000 | Loss: 0.00001859
Iteration 62/1000 | Loss: 0.00001859
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001857
Iteration 73/1000 | Loss: 0.00001857
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001855
Iteration 81/1000 | Loss: 0.00001855
Iteration 82/1000 | Loss: 0.00001855
Iteration 83/1000 | Loss: 0.00001855
Iteration 84/1000 | Loss: 0.00001855
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001854
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001854
Iteration 93/1000 | Loss: 0.00001853
Iteration 94/1000 | Loss: 0.00001853
Iteration 95/1000 | Loss: 0.00001853
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001853
Iteration 98/1000 | Loss: 0.00001853
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001853
Iteration 101/1000 | Loss: 0.00001853
Iteration 102/1000 | Loss: 0.00001852
Iteration 103/1000 | Loss: 0.00001852
Iteration 104/1000 | Loss: 0.00001852
Iteration 105/1000 | Loss: 0.00001852
Iteration 106/1000 | Loss: 0.00001852
Iteration 107/1000 | Loss: 0.00001852
Iteration 108/1000 | Loss: 0.00001852
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001852
Iteration 111/1000 | Loss: 0.00001852
Iteration 112/1000 | Loss: 0.00001852
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001852
Iteration 119/1000 | Loss: 0.00001852
Iteration 120/1000 | Loss: 0.00001852
Iteration 121/1000 | Loss: 0.00001851
Iteration 122/1000 | Loss: 0.00001851
Iteration 123/1000 | Loss: 0.00001851
Iteration 124/1000 | Loss: 0.00001851
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001851
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.851485467341263e-05, 1.851485467341263e-05, 1.851485467341263e-05, 1.851485467341263e-05, 1.851485467341263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.851485467341263e-05

Optimization complete. Final v2v error: 3.5424070358276367 mm

Highest mean error: 5.559950828552246 mm for frame 72

Lowest mean error: 2.527496337890625 mm for frame 129

Saving results

Total time: 40.505253314971924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495063
Iteration 2/25 | Loss: 0.00088008
Iteration 3/25 | Loss: 0.00062728
Iteration 4/25 | Loss: 0.00059316
Iteration 5/25 | Loss: 0.00058391
Iteration 6/25 | Loss: 0.00058261
Iteration 7/25 | Loss: 0.00058261
Iteration 8/25 | Loss: 0.00058261
Iteration 9/25 | Loss: 0.00058261
Iteration 10/25 | Loss: 0.00058261
Iteration 11/25 | Loss: 0.00058261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000582610082346946, 0.000582610082346946, 0.000582610082346946, 0.000582610082346946, 0.000582610082346946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000582610082346946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78826410
Iteration 2/25 | Loss: 0.00019314
Iteration 3/25 | Loss: 0.00019314
Iteration 4/25 | Loss: 0.00019313
Iteration 5/25 | Loss: 0.00019313
Iteration 6/25 | Loss: 0.00019313
Iteration 7/25 | Loss: 0.00019313
Iteration 8/25 | Loss: 0.00019313
Iteration 9/25 | Loss: 0.00019313
Iteration 10/25 | Loss: 0.00019313
Iteration 11/25 | Loss: 0.00019313
Iteration 12/25 | Loss: 0.00019313
Iteration 13/25 | Loss: 0.00019313
Iteration 14/25 | Loss: 0.00019313
Iteration 15/25 | Loss: 0.00019313
Iteration 16/25 | Loss: 0.00019313
Iteration 17/25 | Loss: 0.00019313
Iteration 18/25 | Loss: 0.00019313
Iteration 19/25 | Loss: 0.00019313
Iteration 20/25 | Loss: 0.00019313
Iteration 21/25 | Loss: 0.00019313
Iteration 22/25 | Loss: 0.00019313
Iteration 23/25 | Loss: 0.00019313
Iteration 24/25 | Loss: 0.00019313
Iteration 25/25 | Loss: 0.00019313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019313
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00002234
Iteration 4/1000 | Loss: 0.00002081
Iteration 5/1000 | Loss: 0.00001999
Iteration 6/1000 | Loss: 0.00001916
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001808
Iteration 9/1000 | Loss: 0.00001775
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001739
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001693
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001693
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001693
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001692
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001687
Iteration 30/1000 | Loss: 0.00001687
Iteration 31/1000 | Loss: 0.00001686
Iteration 32/1000 | Loss: 0.00001685
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001683
Iteration 36/1000 | Loss: 0.00001683
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001682
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001681
Iteration 45/1000 | Loss: 0.00001681
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001680
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00001680
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001678
Iteration 57/1000 | Loss: 0.00001678
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001677
Iteration 60/1000 | Loss: 0.00001677
Iteration 61/1000 | Loss: 0.00001677
Iteration 62/1000 | Loss: 0.00001677
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001673
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001672
Iteration 93/1000 | Loss: 0.00001672
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001671
Iteration 97/1000 | Loss: 0.00001671
Iteration 98/1000 | Loss: 0.00001671
Iteration 99/1000 | Loss: 0.00001671
Iteration 100/1000 | Loss: 0.00001671
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001667
Iteration 111/1000 | Loss: 0.00001667
Iteration 112/1000 | Loss: 0.00001667
Iteration 113/1000 | Loss: 0.00001667
Iteration 114/1000 | Loss: 0.00001666
Iteration 115/1000 | Loss: 0.00001666
Iteration 116/1000 | Loss: 0.00001666
Iteration 117/1000 | Loss: 0.00001666
Iteration 118/1000 | Loss: 0.00001666
Iteration 119/1000 | Loss: 0.00001666
Iteration 120/1000 | Loss: 0.00001666
Iteration 121/1000 | Loss: 0.00001666
Iteration 122/1000 | Loss: 0.00001666
Iteration 123/1000 | Loss: 0.00001665
Iteration 124/1000 | Loss: 0.00001665
Iteration 125/1000 | Loss: 0.00001665
Iteration 126/1000 | Loss: 0.00001665
Iteration 127/1000 | Loss: 0.00001665
Iteration 128/1000 | Loss: 0.00001665
Iteration 129/1000 | Loss: 0.00001664
Iteration 130/1000 | Loss: 0.00001664
Iteration 131/1000 | Loss: 0.00001664
Iteration 132/1000 | Loss: 0.00001664
Iteration 133/1000 | Loss: 0.00001664
Iteration 134/1000 | Loss: 0.00001664
Iteration 135/1000 | Loss: 0.00001663
Iteration 136/1000 | Loss: 0.00001663
Iteration 137/1000 | Loss: 0.00001663
Iteration 138/1000 | Loss: 0.00001663
Iteration 139/1000 | Loss: 0.00001663
Iteration 140/1000 | Loss: 0.00001663
Iteration 141/1000 | Loss: 0.00001663
Iteration 142/1000 | Loss: 0.00001663
Iteration 143/1000 | Loss: 0.00001663
Iteration 144/1000 | Loss: 0.00001663
Iteration 145/1000 | Loss: 0.00001663
Iteration 146/1000 | Loss: 0.00001663
Iteration 147/1000 | Loss: 0.00001663
Iteration 148/1000 | Loss: 0.00001663
Iteration 149/1000 | Loss: 0.00001663
Iteration 150/1000 | Loss: 0.00001663
Iteration 151/1000 | Loss: 0.00001663
Iteration 152/1000 | Loss: 0.00001662
Iteration 153/1000 | Loss: 0.00001662
Iteration 154/1000 | Loss: 0.00001662
Iteration 155/1000 | Loss: 0.00001662
Iteration 156/1000 | Loss: 0.00001662
Iteration 157/1000 | Loss: 0.00001662
Iteration 158/1000 | Loss: 0.00001662
Iteration 159/1000 | Loss: 0.00001662
Iteration 160/1000 | Loss: 0.00001662
Iteration 161/1000 | Loss: 0.00001662
Iteration 162/1000 | Loss: 0.00001662
Iteration 163/1000 | Loss: 0.00001662
Iteration 164/1000 | Loss: 0.00001662
Iteration 165/1000 | Loss: 0.00001661
Iteration 166/1000 | Loss: 0.00001661
Iteration 167/1000 | Loss: 0.00001661
Iteration 168/1000 | Loss: 0.00001661
Iteration 169/1000 | Loss: 0.00001661
Iteration 170/1000 | Loss: 0.00001661
Iteration 171/1000 | Loss: 0.00001661
Iteration 172/1000 | Loss: 0.00001661
Iteration 173/1000 | Loss: 0.00001661
Iteration 174/1000 | Loss: 0.00001661
Iteration 175/1000 | Loss: 0.00001661
Iteration 176/1000 | Loss: 0.00001661
Iteration 177/1000 | Loss: 0.00001661
Iteration 178/1000 | Loss: 0.00001661
Iteration 179/1000 | Loss: 0.00001661
Iteration 180/1000 | Loss: 0.00001661
Iteration 181/1000 | Loss: 0.00001661
Iteration 182/1000 | Loss: 0.00001661
Iteration 183/1000 | Loss: 0.00001661
Iteration 184/1000 | Loss: 0.00001661
Iteration 185/1000 | Loss: 0.00001661
Iteration 186/1000 | Loss: 0.00001661
Iteration 187/1000 | Loss: 0.00001661
Iteration 188/1000 | Loss: 0.00001661
Iteration 189/1000 | Loss: 0.00001661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.660618909227196e-05, 1.660618909227196e-05, 1.660618909227196e-05, 1.660618909227196e-05, 1.660618909227196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.660618909227196e-05

Optimization complete. Final v2v error: 3.4889042377471924 mm

Highest mean error: 3.7783379554748535 mm for frame 47

Lowest mean error: 2.9033424854278564 mm for frame 7

Saving results

Total time: 48.32939910888672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099340
Iteration 2/25 | Loss: 0.00227333
Iteration 3/25 | Loss: 0.00134290
Iteration 4/25 | Loss: 0.00108071
Iteration 5/25 | Loss: 0.00089534
Iteration 6/25 | Loss: 0.00076360
Iteration 7/25 | Loss: 0.00083819
Iteration 8/25 | Loss: 0.00081667
Iteration 9/25 | Loss: 0.00076099
Iteration 10/25 | Loss: 0.00072560
Iteration 11/25 | Loss: 0.00070132
Iteration 12/25 | Loss: 0.00067063
Iteration 13/25 | Loss: 0.00066159
Iteration 14/25 | Loss: 0.00065506
Iteration 15/25 | Loss: 0.00064796
Iteration 16/25 | Loss: 0.00064475
Iteration 17/25 | Loss: 0.00064426
Iteration 18/25 | Loss: 0.00064410
Iteration 19/25 | Loss: 0.00064969
Iteration 20/25 | Loss: 0.00065703
Iteration 21/25 | Loss: 0.00064203
Iteration 22/25 | Loss: 0.00064844
Iteration 23/25 | Loss: 0.00063603
Iteration 24/25 | Loss: 0.00064337
Iteration 25/25 | Loss: 0.00064209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32900286
Iteration 2/25 | Loss: 0.00037839
Iteration 3/25 | Loss: 0.00037839
Iteration 4/25 | Loss: 0.00016177
Iteration 5/25 | Loss: 0.00016177
Iteration 6/25 | Loss: 0.00016177
Iteration 7/25 | Loss: 0.00016177
Iteration 8/25 | Loss: 0.00016177
Iteration 9/25 | Loss: 0.00016177
Iteration 10/25 | Loss: 0.00016177
Iteration 11/25 | Loss: 0.00016177
Iteration 12/25 | Loss: 0.00016177
Iteration 13/25 | Loss: 0.00016177
Iteration 14/25 | Loss: 0.00016177
Iteration 15/25 | Loss: 0.00016177
Iteration 16/25 | Loss: 0.00016177
Iteration 17/25 | Loss: 0.00016177
Iteration 18/25 | Loss: 0.00016177
Iteration 19/25 | Loss: 0.00016177
Iteration 20/25 | Loss: 0.00016177
Iteration 21/25 | Loss: 0.00016177
Iteration 22/25 | Loss: 0.00016177
Iteration 23/25 | Loss: 0.00016177
Iteration 24/25 | Loss: 0.00016177
Iteration 25/25 | Loss: 0.00016177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016177
Iteration 2/1000 | Loss: 0.00003242
Iteration 3/1000 | Loss: 0.00002835
Iteration 4/1000 | Loss: 0.00002682
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002438
Iteration 7/1000 | Loss: 0.00002382
Iteration 8/1000 | Loss: 0.00002350
Iteration 9/1000 | Loss: 0.00002332
Iteration 10/1000 | Loss: 0.00002309
Iteration 11/1000 | Loss: 0.00002295
Iteration 12/1000 | Loss: 0.00002292
Iteration 13/1000 | Loss: 0.00002285
Iteration 14/1000 | Loss: 0.00002282
Iteration 15/1000 | Loss: 0.00002282
Iteration 16/1000 | Loss: 0.00002281
Iteration 17/1000 | Loss: 0.00002281
Iteration 18/1000 | Loss: 0.00002280
Iteration 19/1000 | Loss: 0.00002280
Iteration 20/1000 | Loss: 0.00002280
Iteration 21/1000 | Loss: 0.00002280
Iteration 22/1000 | Loss: 0.00002279
Iteration 23/1000 | Loss: 0.00002279
Iteration 24/1000 | Loss: 0.00002278
Iteration 25/1000 | Loss: 0.00002278
Iteration 26/1000 | Loss: 0.00002278
Iteration 27/1000 | Loss: 0.00002278
Iteration 28/1000 | Loss: 0.00002278
Iteration 29/1000 | Loss: 0.00002277
Iteration 30/1000 | Loss: 0.00002277
Iteration 31/1000 | Loss: 0.00002277
Iteration 32/1000 | Loss: 0.00002273
Iteration 33/1000 | Loss: 0.00002272
Iteration 34/1000 | Loss: 0.00002272
Iteration 35/1000 | Loss: 0.00002272
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002268
Iteration 38/1000 | Loss: 0.00002268
Iteration 39/1000 | Loss: 0.00002268
Iteration 40/1000 | Loss: 0.00002268
Iteration 41/1000 | Loss: 0.00002267
Iteration 42/1000 | Loss: 0.00002267
Iteration 43/1000 | Loss: 0.00002267
Iteration 44/1000 | Loss: 0.00002267
Iteration 45/1000 | Loss: 0.00002267
Iteration 46/1000 | Loss: 0.00002266
Iteration 47/1000 | Loss: 0.00002266
Iteration 48/1000 | Loss: 0.00002265
Iteration 49/1000 | Loss: 0.00002265
Iteration 50/1000 | Loss: 0.00002265
Iteration 51/1000 | Loss: 0.00002265
Iteration 52/1000 | Loss: 0.00002265
Iteration 53/1000 | Loss: 0.00002264
Iteration 54/1000 | Loss: 0.00002264
Iteration 55/1000 | Loss: 0.00002264
Iteration 56/1000 | Loss: 0.00002264
Iteration 57/1000 | Loss: 0.00002264
Iteration 58/1000 | Loss: 0.00002264
Iteration 59/1000 | Loss: 0.00002263
Iteration 60/1000 | Loss: 0.00002263
Iteration 61/1000 | Loss: 0.00002263
Iteration 62/1000 | Loss: 0.00002263
Iteration 63/1000 | Loss: 0.00002263
Iteration 64/1000 | Loss: 0.00002263
Iteration 65/1000 | Loss: 0.00002263
Iteration 66/1000 | Loss: 0.00002263
Iteration 67/1000 | Loss: 0.00002263
Iteration 68/1000 | Loss: 0.00002263
Iteration 69/1000 | Loss: 0.00002263
Iteration 70/1000 | Loss: 0.00002262
Iteration 71/1000 | Loss: 0.00002262
Iteration 72/1000 | Loss: 0.00002262
Iteration 73/1000 | Loss: 0.00002262
Iteration 74/1000 | Loss: 0.00002262
Iteration 75/1000 | Loss: 0.00002262
Iteration 76/1000 | Loss: 0.00002262
Iteration 77/1000 | Loss: 0.00002262
Iteration 78/1000 | Loss: 0.00002262
Iteration 79/1000 | Loss: 0.00002261
Iteration 80/1000 | Loss: 0.00002261
Iteration 81/1000 | Loss: 0.00002261
Iteration 82/1000 | Loss: 0.00002261
Iteration 83/1000 | Loss: 0.00002261
Iteration 84/1000 | Loss: 0.00002261
Iteration 85/1000 | Loss: 0.00002261
Iteration 86/1000 | Loss: 0.00002261
Iteration 87/1000 | Loss: 0.00002260
Iteration 88/1000 | Loss: 0.00002260
Iteration 89/1000 | Loss: 0.00002260
Iteration 90/1000 | Loss: 0.00002260
Iteration 91/1000 | Loss: 0.00002260
Iteration 92/1000 | Loss: 0.00002260
Iteration 93/1000 | Loss: 0.00002259
Iteration 94/1000 | Loss: 0.00002259
Iteration 95/1000 | Loss: 0.00002259
Iteration 96/1000 | Loss: 0.00002258
Iteration 97/1000 | Loss: 0.00002258
Iteration 98/1000 | Loss: 0.00002258
Iteration 99/1000 | Loss: 0.00002257
Iteration 100/1000 | Loss: 0.00002257
Iteration 101/1000 | Loss: 0.00002257
Iteration 102/1000 | Loss: 0.00002257
Iteration 103/1000 | Loss: 0.00002257
Iteration 104/1000 | Loss: 0.00002257
Iteration 105/1000 | Loss: 0.00002256
Iteration 106/1000 | Loss: 0.00002256
Iteration 107/1000 | Loss: 0.00002256
Iteration 108/1000 | Loss: 0.00002256
Iteration 109/1000 | Loss: 0.00002256
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002256
Iteration 113/1000 | Loss: 0.00002256
Iteration 114/1000 | Loss: 0.00002255
Iteration 115/1000 | Loss: 0.00002255
Iteration 116/1000 | Loss: 0.00002255
Iteration 117/1000 | Loss: 0.00024431
Iteration 118/1000 | Loss: 0.00004808
Iteration 119/1000 | Loss: 0.00046278
Iteration 120/1000 | Loss: 0.00015362
Iteration 121/1000 | Loss: 0.00005580
Iteration 122/1000 | Loss: 0.00002511
Iteration 123/1000 | Loss: 0.00002364
Iteration 124/1000 | Loss: 0.00002280
Iteration 125/1000 | Loss: 0.00002258
Iteration 126/1000 | Loss: 0.00002257
Iteration 127/1000 | Loss: 0.00002257
Iteration 128/1000 | Loss: 0.00002256
Iteration 129/1000 | Loss: 0.00002256
Iteration 130/1000 | Loss: 0.00002255
Iteration 131/1000 | Loss: 0.00002255
Iteration 132/1000 | Loss: 0.00002254
Iteration 133/1000 | Loss: 0.00002254
Iteration 134/1000 | Loss: 0.00002254
Iteration 135/1000 | Loss: 0.00002254
Iteration 136/1000 | Loss: 0.00002254
Iteration 137/1000 | Loss: 0.00002254
Iteration 138/1000 | Loss: 0.00002254
Iteration 139/1000 | Loss: 0.00002254
Iteration 140/1000 | Loss: 0.00002254
Iteration 141/1000 | Loss: 0.00002254
Iteration 142/1000 | Loss: 0.00002254
Iteration 143/1000 | Loss: 0.00002254
Iteration 144/1000 | Loss: 0.00002254
Iteration 145/1000 | Loss: 0.00002254
Iteration 146/1000 | Loss: 0.00002254
Iteration 147/1000 | Loss: 0.00002254
Iteration 148/1000 | Loss: 0.00002254
Iteration 149/1000 | Loss: 0.00002254
Iteration 150/1000 | Loss: 0.00002254
Iteration 151/1000 | Loss: 0.00002254
Iteration 152/1000 | Loss: 0.00002254
Iteration 153/1000 | Loss: 0.00002254
Iteration 154/1000 | Loss: 0.00002254
Iteration 155/1000 | Loss: 0.00002254
Iteration 156/1000 | Loss: 0.00002254
Iteration 157/1000 | Loss: 0.00002254
Iteration 158/1000 | Loss: 0.00002254
Iteration 159/1000 | Loss: 0.00002254
Iteration 160/1000 | Loss: 0.00002254
Iteration 161/1000 | Loss: 0.00002254
Iteration 162/1000 | Loss: 0.00002254
Iteration 163/1000 | Loss: 0.00002254
Iteration 164/1000 | Loss: 0.00002254
Iteration 165/1000 | Loss: 0.00002254
Iteration 166/1000 | Loss: 0.00002254
Iteration 167/1000 | Loss: 0.00002254
Iteration 168/1000 | Loss: 0.00002254
Iteration 169/1000 | Loss: 0.00002254
Iteration 170/1000 | Loss: 0.00002254
Iteration 171/1000 | Loss: 0.00002254
Iteration 172/1000 | Loss: 0.00002254
Iteration 173/1000 | Loss: 0.00002253
Iteration 174/1000 | Loss: 0.00002253
Iteration 175/1000 | Loss: 0.00002253
Iteration 176/1000 | Loss: 0.00002253
Iteration 177/1000 | Loss: 0.00002253
Iteration 178/1000 | Loss: 0.00002253
Iteration 179/1000 | Loss: 0.00002253
Iteration 180/1000 | Loss: 0.00002253
Iteration 181/1000 | Loss: 0.00002253
Iteration 182/1000 | Loss: 0.00002253
Iteration 183/1000 | Loss: 0.00002253
Iteration 184/1000 | Loss: 0.00002253
Iteration 185/1000 | Loss: 0.00002253
Iteration 186/1000 | Loss: 0.00002253
Iteration 187/1000 | Loss: 0.00002253
Iteration 188/1000 | Loss: 0.00002253
Iteration 189/1000 | Loss: 0.00002253
Iteration 190/1000 | Loss: 0.00002253
Iteration 191/1000 | Loss: 0.00002253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.2533875380759127e-05, 2.2533875380759127e-05, 2.2533875380759127e-05, 2.2533875380759127e-05, 2.2533875380759127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2533875380759127e-05

Optimization complete. Final v2v error: 3.902927875518799 mm

Highest mean error: 4.847708225250244 mm for frame 2

Lowest mean error: 3.526442289352417 mm for frame 92

Saving results

Total time: 99.19663143157959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393668
Iteration 2/25 | Loss: 0.00076185
Iteration 3/25 | Loss: 0.00056073
Iteration 4/25 | Loss: 0.00052796
Iteration 5/25 | Loss: 0.00051601
Iteration 6/25 | Loss: 0.00051229
Iteration 7/25 | Loss: 0.00051119
Iteration 8/25 | Loss: 0.00051087
Iteration 9/25 | Loss: 0.00051087
Iteration 10/25 | Loss: 0.00051087
Iteration 11/25 | Loss: 0.00051087
Iteration 12/25 | Loss: 0.00051087
Iteration 13/25 | Loss: 0.00051087
Iteration 14/25 | Loss: 0.00051087
Iteration 15/25 | Loss: 0.00051087
Iteration 16/25 | Loss: 0.00051087
Iteration 17/25 | Loss: 0.00051087
Iteration 18/25 | Loss: 0.00051087
Iteration 19/25 | Loss: 0.00051087
Iteration 20/25 | Loss: 0.00051087
Iteration 21/25 | Loss: 0.00051087
Iteration 22/25 | Loss: 0.00051087
Iteration 23/25 | Loss: 0.00051087
Iteration 24/25 | Loss: 0.00051087
Iteration 25/25 | Loss: 0.00051087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28899884
Iteration 2/25 | Loss: 0.00009300
Iteration 3/25 | Loss: 0.00009298
Iteration 4/25 | Loss: 0.00009298
Iteration 5/25 | Loss: 0.00009298
Iteration 6/25 | Loss: 0.00009298
Iteration 7/25 | Loss: 0.00009298
Iteration 8/25 | Loss: 0.00009298
Iteration 9/25 | Loss: 0.00009298
Iteration 10/25 | Loss: 0.00009298
Iteration 11/25 | Loss: 0.00009298
Iteration 12/25 | Loss: 0.00009298
Iteration 13/25 | Loss: 0.00009298
Iteration 14/25 | Loss: 0.00009298
Iteration 15/25 | Loss: 0.00009298
Iteration 16/25 | Loss: 0.00009298
Iteration 17/25 | Loss: 0.00009298
Iteration 18/25 | Loss: 0.00009298
Iteration 19/25 | Loss: 0.00009298
Iteration 20/25 | Loss: 0.00009298
Iteration 21/25 | Loss: 0.00009298
Iteration 22/25 | Loss: 0.00009298
Iteration 23/25 | Loss: 0.00009298
Iteration 24/25 | Loss: 0.00009298
Iteration 25/25 | Loss: 0.00009298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009298
Iteration 2/1000 | Loss: 0.00002060
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001243
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001227
Iteration 14/1000 | Loss: 0.00001224
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001222
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001221
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001220
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001215
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001209
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00001208
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001192
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001190
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001189
Iteration 87/1000 | Loss: 0.00001189
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001189
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001188
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001184
Iteration 109/1000 | Loss: 0.00001184
Iteration 110/1000 | Loss: 0.00001184
Iteration 111/1000 | Loss: 0.00001184
Iteration 112/1000 | Loss: 0.00001184
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001183
Iteration 116/1000 | Loss: 0.00001183
Iteration 117/1000 | Loss: 0.00001183
Iteration 118/1000 | Loss: 0.00001183
Iteration 119/1000 | Loss: 0.00001183
Iteration 120/1000 | Loss: 0.00001183
Iteration 121/1000 | Loss: 0.00001183
Iteration 122/1000 | Loss: 0.00001183
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Iteration 125/1000 | Loss: 0.00001182
Iteration 126/1000 | Loss: 0.00001182
Iteration 127/1000 | Loss: 0.00001181
Iteration 128/1000 | Loss: 0.00001181
Iteration 129/1000 | Loss: 0.00001181
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001181
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001181
Iteration 137/1000 | Loss: 0.00001181
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001180
Iteration 143/1000 | Loss: 0.00001180
Iteration 144/1000 | Loss: 0.00001180
Iteration 145/1000 | Loss: 0.00001179
Iteration 146/1000 | Loss: 0.00001179
Iteration 147/1000 | Loss: 0.00001179
Iteration 148/1000 | Loss: 0.00001178
Iteration 149/1000 | Loss: 0.00001178
Iteration 150/1000 | Loss: 0.00001178
Iteration 151/1000 | Loss: 0.00001177
Iteration 152/1000 | Loss: 0.00001177
Iteration 153/1000 | Loss: 0.00001177
Iteration 154/1000 | Loss: 0.00001177
Iteration 155/1000 | Loss: 0.00001177
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001176
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001176
Iteration 168/1000 | Loss: 0.00001176
Iteration 169/1000 | Loss: 0.00001176
Iteration 170/1000 | Loss: 0.00001176
Iteration 171/1000 | Loss: 0.00001176
Iteration 172/1000 | Loss: 0.00001176
Iteration 173/1000 | Loss: 0.00001176
Iteration 174/1000 | Loss: 0.00001176
Iteration 175/1000 | Loss: 0.00001176
Iteration 176/1000 | Loss: 0.00001176
Iteration 177/1000 | Loss: 0.00001176
Iteration 178/1000 | Loss: 0.00001176
Iteration 179/1000 | Loss: 0.00001176
Iteration 180/1000 | Loss: 0.00001176
Iteration 181/1000 | Loss: 0.00001176
Iteration 182/1000 | Loss: 0.00001176
Iteration 183/1000 | Loss: 0.00001176
Iteration 184/1000 | Loss: 0.00001176
Iteration 185/1000 | Loss: 0.00001176
Iteration 186/1000 | Loss: 0.00001176
Iteration 187/1000 | Loss: 0.00001176
Iteration 188/1000 | Loss: 0.00001176
Iteration 189/1000 | Loss: 0.00001176
Iteration 190/1000 | Loss: 0.00001176
Iteration 191/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.176010937342653e-05, 1.176010937342653e-05, 1.176010937342653e-05, 1.176010937342653e-05, 1.176010937342653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.176010937342653e-05

Optimization complete. Final v2v error: 2.765947103500366 mm

Highest mean error: 4.541031837463379 mm for frame 84

Lowest mean error: 2.1557037830352783 mm for frame 173

Saving results

Total time: 40.84735107421875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00648234
Iteration 2/25 | Loss: 0.00130204
Iteration 3/25 | Loss: 0.00072450
Iteration 4/25 | Loss: 0.00062789
Iteration 5/25 | Loss: 0.00058921
Iteration 6/25 | Loss: 0.00058870
Iteration 7/25 | Loss: 0.00057909
Iteration 8/25 | Loss: 0.00058003
Iteration 9/25 | Loss: 0.00057728
Iteration 10/25 | Loss: 0.00057447
Iteration 11/25 | Loss: 0.00057343
Iteration 12/25 | Loss: 0.00057277
Iteration 13/25 | Loss: 0.00057252
Iteration 14/25 | Loss: 0.00057235
Iteration 15/25 | Loss: 0.00057234
Iteration 16/25 | Loss: 0.00057234
Iteration 17/25 | Loss: 0.00057234
Iteration 18/25 | Loss: 0.00057234
Iteration 19/25 | Loss: 0.00057234
Iteration 20/25 | Loss: 0.00057234
Iteration 21/25 | Loss: 0.00057233
Iteration 22/25 | Loss: 0.00057233
Iteration 23/25 | Loss: 0.00057233
Iteration 24/25 | Loss: 0.00057233
Iteration 25/25 | Loss: 0.00057233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.92564440
Iteration 2/25 | Loss: 0.00014545
Iteration 3/25 | Loss: 0.00014545
Iteration 4/25 | Loss: 0.00014545
Iteration 5/25 | Loss: 0.00014545
Iteration 6/25 | Loss: 0.00014545
Iteration 7/25 | Loss: 0.00014545
Iteration 8/25 | Loss: 0.00014545
Iteration 9/25 | Loss: 0.00014545
Iteration 10/25 | Loss: 0.00014545
Iteration 11/25 | Loss: 0.00014545
Iteration 12/25 | Loss: 0.00014545
Iteration 13/25 | Loss: 0.00014545
Iteration 14/25 | Loss: 0.00014545
Iteration 15/25 | Loss: 0.00014545
Iteration 16/25 | Loss: 0.00014545
Iteration 17/25 | Loss: 0.00014545
Iteration 18/25 | Loss: 0.00014545
Iteration 19/25 | Loss: 0.00014545
Iteration 20/25 | Loss: 0.00014545
Iteration 21/25 | Loss: 0.00014545
Iteration 22/25 | Loss: 0.00014545
Iteration 23/25 | Loss: 0.00014545
Iteration 24/25 | Loss: 0.00014545
Iteration 25/25 | Loss: 0.00014545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014545
Iteration 2/1000 | Loss: 0.00002220
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001538
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00001489
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001468
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001464
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00001461
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001457
Iteration 17/1000 | Loss: 0.00001456
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001444
Iteration 25/1000 | Loss: 0.00001444
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001434
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001432
Iteration 35/1000 | Loss: 0.00001432
Iteration 36/1000 | Loss: 0.00001432
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001432
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001431
Iteration 45/1000 | Loss: 0.00001431
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001425
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001423
Iteration 69/1000 | Loss: 0.00001423
Iteration 70/1000 | Loss: 0.00001423
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001421
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Iteration 83/1000 | Loss: 0.00001420
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001418
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001417
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001417
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001416
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00001415
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001414
Iteration 103/1000 | Loss: 0.00001414
Iteration 104/1000 | Loss: 0.00001414
Iteration 105/1000 | Loss: 0.00001414
Iteration 106/1000 | Loss: 0.00001414
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001412
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00001412
Iteration 112/1000 | Loss: 0.00001412
Iteration 113/1000 | Loss: 0.00001411
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001411
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001411
Iteration 123/1000 | Loss: 0.00001411
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001409
Iteration 134/1000 | Loss: 0.00001409
Iteration 135/1000 | Loss: 0.00001409
Iteration 136/1000 | Loss: 0.00001408
Iteration 137/1000 | Loss: 0.00001408
Iteration 138/1000 | Loss: 0.00001408
Iteration 139/1000 | Loss: 0.00001408
Iteration 140/1000 | Loss: 0.00001408
Iteration 141/1000 | Loss: 0.00001408
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001406
Iteration 144/1000 | Loss: 0.00001406
Iteration 145/1000 | Loss: 0.00001406
Iteration 146/1000 | Loss: 0.00001406
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001406
Iteration 150/1000 | Loss: 0.00001406
Iteration 151/1000 | Loss: 0.00001406
Iteration 152/1000 | Loss: 0.00001406
Iteration 153/1000 | Loss: 0.00001406
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001406
Iteration 163/1000 | Loss: 0.00001406
Iteration 164/1000 | Loss: 0.00001406
Iteration 165/1000 | Loss: 0.00001406
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.405927469022572e-05, 1.405927469022572e-05, 1.405927469022572e-05, 1.405927469022572e-05, 1.405927469022572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.405927469022572e-05

Optimization complete. Final v2v error: 3.15242600440979 mm

Highest mean error: 4.247476577758789 mm for frame 90

Lowest mean error: 2.475036144256592 mm for frame 1

Saving results

Total time: 59.0978639125824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037655
Iteration 2/25 | Loss: 0.00419845
Iteration 3/25 | Loss: 0.00256465
Iteration 4/25 | Loss: 0.00233146
Iteration 5/25 | Loss: 0.00197753
Iteration 6/25 | Loss: 0.00163440
Iteration 7/25 | Loss: 0.00151374
Iteration 8/25 | Loss: 0.00142511
Iteration 9/25 | Loss: 0.00138079
Iteration 10/25 | Loss: 0.00136187
Iteration 11/25 | Loss: 0.00134052
Iteration 12/25 | Loss: 0.00131865
Iteration 13/25 | Loss: 0.00131292
Iteration 14/25 | Loss: 0.00130318
Iteration 15/25 | Loss: 0.00130789
Iteration 16/25 | Loss: 0.00129600
Iteration 17/25 | Loss: 0.00128423
Iteration 18/25 | Loss: 0.00127713
Iteration 19/25 | Loss: 0.00127779
Iteration 20/25 | Loss: 0.00128430
Iteration 21/25 | Loss: 0.00130026
Iteration 22/25 | Loss: 0.00134315
Iteration 23/25 | Loss: 0.00131056
Iteration 24/25 | Loss: 0.00120921
Iteration 25/25 | Loss: 0.00110083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34559965
Iteration 2/25 | Loss: 0.00782072
Iteration 3/25 | Loss: 0.00429594
Iteration 4/25 | Loss: 0.00429592
Iteration 5/25 | Loss: 0.00429592
Iteration 6/25 | Loss: 0.00429592
Iteration 7/25 | Loss: 0.00429592
Iteration 8/25 | Loss: 0.00429592
Iteration 9/25 | Loss: 0.00429592
Iteration 10/25 | Loss: 0.00429592
Iteration 11/25 | Loss: 0.00429592
Iteration 12/25 | Loss: 0.00429592
Iteration 13/25 | Loss: 0.00429592
Iteration 14/25 | Loss: 0.00429592
Iteration 15/25 | Loss: 0.00429592
Iteration 16/25 | Loss: 0.00429592
Iteration 17/25 | Loss: 0.00429592
Iteration 18/25 | Loss: 0.00429592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004295918624848127, 0.004295918624848127, 0.004295918624848127, 0.004295918624848127, 0.004295918624848127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004295918624848127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00429592
Iteration 2/1000 | Loss: 0.00585065
Iteration 3/1000 | Loss: 0.00823799
Iteration 4/1000 | Loss: 0.00628351
Iteration 5/1000 | Loss: 0.00348272
Iteration 6/1000 | Loss: 0.00299597
Iteration 7/1000 | Loss: 0.00172701
Iteration 8/1000 | Loss: 0.00332975
Iteration 9/1000 | Loss: 0.00190525
Iteration 10/1000 | Loss: 0.00224713
Iteration 11/1000 | Loss: 0.00247450
Iteration 12/1000 | Loss: 0.00146892
Iteration 13/1000 | Loss: 0.00060105
Iteration 14/1000 | Loss: 0.00109493
Iteration 15/1000 | Loss: 0.00097698
Iteration 16/1000 | Loss: 0.00308011
Iteration 17/1000 | Loss: 0.00292140
Iteration 18/1000 | Loss: 0.00045010
Iteration 19/1000 | Loss: 0.00141317
Iteration 20/1000 | Loss: 0.00139481
Iteration 21/1000 | Loss: 0.00228755
Iteration 22/1000 | Loss: 0.00113415
Iteration 23/1000 | Loss: 0.00093792
Iteration 24/1000 | Loss: 0.00011505
Iteration 25/1000 | Loss: 0.00011948
Iteration 26/1000 | Loss: 0.00127299
Iteration 27/1000 | Loss: 0.00231615
Iteration 28/1000 | Loss: 0.00730531
Iteration 29/1000 | Loss: 0.00254942
Iteration 30/1000 | Loss: 0.00095060
Iteration 31/1000 | Loss: 0.00083585
Iteration 32/1000 | Loss: 0.00065508
Iteration 33/1000 | Loss: 0.00052448
Iteration 34/1000 | Loss: 0.00047355
Iteration 35/1000 | Loss: 0.00035774
Iteration 36/1000 | Loss: 0.00037964
Iteration 37/1000 | Loss: 0.00022324
Iteration 38/1000 | Loss: 0.00016435
Iteration 39/1000 | Loss: 0.00016771
Iteration 40/1000 | Loss: 0.00011029
Iteration 41/1000 | Loss: 0.00004418
Iteration 42/1000 | Loss: 0.00004001
Iteration 43/1000 | Loss: 0.00090629
Iteration 44/1000 | Loss: 0.00129712
Iteration 45/1000 | Loss: 0.00008198
Iteration 46/1000 | Loss: 0.00006904
Iteration 47/1000 | Loss: 0.00036045
Iteration 48/1000 | Loss: 0.00212400
Iteration 49/1000 | Loss: 0.00058736
Iteration 50/1000 | Loss: 0.00003510
Iteration 51/1000 | Loss: 0.00012582
Iteration 52/1000 | Loss: 0.00003299
Iteration 53/1000 | Loss: 0.00003212
Iteration 54/1000 | Loss: 0.00003125
Iteration 55/1000 | Loss: 0.00003073
Iteration 56/1000 | Loss: 0.00113562
Iteration 57/1000 | Loss: 0.00034472
Iteration 58/1000 | Loss: 0.00003746
Iteration 59/1000 | Loss: 0.00006917
Iteration 60/1000 | Loss: 0.00015542
Iteration 61/1000 | Loss: 0.00006709
Iteration 62/1000 | Loss: 0.00003018
Iteration 63/1000 | Loss: 0.00082359
Iteration 64/1000 | Loss: 0.00058605
Iteration 65/1000 | Loss: 0.00063918
Iteration 66/1000 | Loss: 0.00159502
Iteration 67/1000 | Loss: 0.00016830
Iteration 68/1000 | Loss: 0.00129903
Iteration 69/1000 | Loss: 0.00028530
Iteration 70/1000 | Loss: 0.00009804
Iteration 71/1000 | Loss: 0.00010589
Iteration 72/1000 | Loss: 0.00044639
Iteration 73/1000 | Loss: 0.00004281
Iteration 74/1000 | Loss: 0.00004116
Iteration 75/1000 | Loss: 0.00017698
Iteration 76/1000 | Loss: 0.00023613
Iteration 77/1000 | Loss: 0.00003491
Iteration 78/1000 | Loss: 0.00002954
Iteration 79/1000 | Loss: 0.00004802
Iteration 80/1000 | Loss: 0.00028831
Iteration 81/1000 | Loss: 0.00098235
Iteration 82/1000 | Loss: 0.00002743
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00004101
Iteration 85/1000 | Loss: 0.00002421
Iteration 86/1000 | Loss: 0.00035385
Iteration 87/1000 | Loss: 0.00004662
Iteration 88/1000 | Loss: 0.00005230
Iteration 89/1000 | Loss: 0.00003789
Iteration 90/1000 | Loss: 0.00002336
Iteration 91/1000 | Loss: 0.00002314
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00016630
Iteration 94/1000 | Loss: 0.00002351
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00014701
Iteration 97/1000 | Loss: 0.00004235
Iteration 98/1000 | Loss: 0.00003098
Iteration 99/1000 | Loss: 0.00002292
Iteration 100/1000 | Loss: 0.00025553
Iteration 101/1000 | Loss: 0.00003012
Iteration 102/1000 | Loss: 0.00002284
Iteration 103/1000 | Loss: 0.00002274
Iteration 104/1000 | Loss: 0.00002271
Iteration 105/1000 | Loss: 0.00002269
Iteration 106/1000 | Loss: 0.00002268
Iteration 107/1000 | Loss: 0.00011831
Iteration 108/1000 | Loss: 0.00008094
Iteration 109/1000 | Loss: 0.00002293
Iteration 110/1000 | Loss: 0.00007060
Iteration 111/1000 | Loss: 0.00002280
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00002263
Iteration 114/1000 | Loss: 0.00002263
Iteration 115/1000 | Loss: 0.00002261
Iteration 116/1000 | Loss: 0.00002261
Iteration 117/1000 | Loss: 0.00002259
Iteration 118/1000 | Loss: 0.00002259
Iteration 119/1000 | Loss: 0.00002258
Iteration 120/1000 | Loss: 0.00002258
Iteration 121/1000 | Loss: 0.00002257
Iteration 122/1000 | Loss: 0.00002257
Iteration 123/1000 | Loss: 0.00002257
Iteration 124/1000 | Loss: 0.00002257
Iteration 125/1000 | Loss: 0.00002257
Iteration 126/1000 | Loss: 0.00002257
Iteration 127/1000 | Loss: 0.00002257
Iteration 128/1000 | Loss: 0.00002256
Iteration 129/1000 | Loss: 0.00002256
Iteration 130/1000 | Loss: 0.00002256
Iteration 131/1000 | Loss: 0.00002256
Iteration 132/1000 | Loss: 0.00002255
Iteration 133/1000 | Loss: 0.00002255
Iteration 134/1000 | Loss: 0.00002255
Iteration 135/1000 | Loss: 0.00002255
Iteration 136/1000 | Loss: 0.00002255
Iteration 137/1000 | Loss: 0.00002255
Iteration 138/1000 | Loss: 0.00002255
Iteration 139/1000 | Loss: 0.00002254
Iteration 140/1000 | Loss: 0.00002254
Iteration 141/1000 | Loss: 0.00002254
Iteration 142/1000 | Loss: 0.00002254
Iteration 143/1000 | Loss: 0.00002253
Iteration 144/1000 | Loss: 0.00002253
Iteration 145/1000 | Loss: 0.00002253
Iteration 146/1000 | Loss: 0.00002253
Iteration 147/1000 | Loss: 0.00013493
Iteration 148/1000 | Loss: 0.00002273
Iteration 149/1000 | Loss: 0.00002256
Iteration 150/1000 | Loss: 0.00002254
Iteration 151/1000 | Loss: 0.00002254
Iteration 152/1000 | Loss: 0.00002254
Iteration 153/1000 | Loss: 0.00002254
Iteration 154/1000 | Loss: 0.00002254
Iteration 155/1000 | Loss: 0.00002254
Iteration 156/1000 | Loss: 0.00002254
Iteration 157/1000 | Loss: 0.00002254
Iteration 158/1000 | Loss: 0.00002254
Iteration 159/1000 | Loss: 0.00002254
Iteration 160/1000 | Loss: 0.00002254
Iteration 161/1000 | Loss: 0.00002254
Iteration 162/1000 | Loss: 0.00002253
Iteration 163/1000 | Loss: 0.00002253
Iteration 164/1000 | Loss: 0.00002253
Iteration 165/1000 | Loss: 0.00002251
Iteration 166/1000 | Loss: 0.00002251
Iteration 167/1000 | Loss: 0.00002251
Iteration 168/1000 | Loss: 0.00002251
Iteration 169/1000 | Loss: 0.00002251
Iteration 170/1000 | Loss: 0.00002250
Iteration 171/1000 | Loss: 0.00002250
Iteration 172/1000 | Loss: 0.00002250
Iteration 173/1000 | Loss: 0.00002250
Iteration 174/1000 | Loss: 0.00002250
Iteration 175/1000 | Loss: 0.00002249
Iteration 176/1000 | Loss: 0.00002249
Iteration 177/1000 | Loss: 0.00002249
Iteration 178/1000 | Loss: 0.00002249
Iteration 179/1000 | Loss: 0.00002249
Iteration 180/1000 | Loss: 0.00002248
Iteration 181/1000 | Loss: 0.00002248
Iteration 182/1000 | Loss: 0.00002248
Iteration 183/1000 | Loss: 0.00002248
Iteration 184/1000 | Loss: 0.00002248
Iteration 185/1000 | Loss: 0.00002248
Iteration 186/1000 | Loss: 0.00002248
Iteration 187/1000 | Loss: 0.00002248
Iteration 188/1000 | Loss: 0.00002248
Iteration 189/1000 | Loss: 0.00002248
Iteration 190/1000 | Loss: 0.00002248
Iteration 191/1000 | Loss: 0.00002248
Iteration 192/1000 | Loss: 0.00002247
Iteration 193/1000 | Loss: 0.00002247
Iteration 194/1000 | Loss: 0.00002247
Iteration 195/1000 | Loss: 0.00002247
Iteration 196/1000 | Loss: 0.00002247
Iteration 197/1000 | Loss: 0.00002247
Iteration 198/1000 | Loss: 0.00002247
Iteration 199/1000 | Loss: 0.00002247
Iteration 200/1000 | Loss: 0.00002247
Iteration 201/1000 | Loss: 0.00002247
Iteration 202/1000 | Loss: 0.00002247
Iteration 203/1000 | Loss: 0.00002247
Iteration 204/1000 | Loss: 0.00002247
Iteration 205/1000 | Loss: 0.00002246
Iteration 206/1000 | Loss: 0.00002246
Iteration 207/1000 | Loss: 0.00002246
Iteration 208/1000 | Loss: 0.00002246
Iteration 209/1000 | Loss: 0.00002245
Iteration 210/1000 | Loss: 0.00002245
Iteration 211/1000 | Loss: 0.00002245
Iteration 212/1000 | Loss: 0.00002244
Iteration 213/1000 | Loss: 0.00002244
Iteration 214/1000 | Loss: 0.00002244
Iteration 215/1000 | Loss: 0.00002243
Iteration 216/1000 | Loss: 0.00002243
Iteration 217/1000 | Loss: 0.00002243
Iteration 218/1000 | Loss: 0.00002243
Iteration 219/1000 | Loss: 0.00002243
Iteration 220/1000 | Loss: 0.00002243
Iteration 221/1000 | Loss: 0.00002243
Iteration 222/1000 | Loss: 0.00002243
Iteration 223/1000 | Loss: 0.00002243
Iteration 224/1000 | Loss: 0.00002243
Iteration 225/1000 | Loss: 0.00002243
Iteration 226/1000 | Loss: 0.00002243
Iteration 227/1000 | Loss: 0.00002243
Iteration 228/1000 | Loss: 0.00002242
Iteration 229/1000 | Loss: 0.00002242
Iteration 230/1000 | Loss: 0.00002242
Iteration 231/1000 | Loss: 0.00002242
Iteration 232/1000 | Loss: 0.00002242
Iteration 233/1000 | Loss: 0.00002242
Iteration 234/1000 | Loss: 0.00002242
Iteration 235/1000 | Loss: 0.00002242
Iteration 236/1000 | Loss: 0.00002242
Iteration 237/1000 | Loss: 0.00002242
Iteration 238/1000 | Loss: 0.00002242
Iteration 239/1000 | Loss: 0.00002242
Iteration 240/1000 | Loss: 0.00002242
Iteration 241/1000 | Loss: 0.00002242
Iteration 242/1000 | Loss: 0.00002242
Iteration 243/1000 | Loss: 0.00002242
Iteration 244/1000 | Loss: 0.00002242
Iteration 245/1000 | Loss: 0.00002242
Iteration 246/1000 | Loss: 0.00002242
Iteration 247/1000 | Loss: 0.00002241
Iteration 248/1000 | Loss: 0.00002241
Iteration 249/1000 | Loss: 0.00002241
Iteration 250/1000 | Loss: 0.00002241
Iteration 251/1000 | Loss: 0.00002241
Iteration 252/1000 | Loss: 0.00002241
Iteration 253/1000 | Loss: 0.00002241
Iteration 254/1000 | Loss: 0.00002241
Iteration 255/1000 | Loss: 0.00002241
Iteration 256/1000 | Loss: 0.00002241
Iteration 257/1000 | Loss: 0.00002241
Iteration 258/1000 | Loss: 0.00002241
Iteration 259/1000 | Loss: 0.00002241
Iteration 260/1000 | Loss: 0.00002241
Iteration 261/1000 | Loss: 0.00002241
Iteration 262/1000 | Loss: 0.00002241
Iteration 263/1000 | Loss: 0.00002241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.2413059923565015e-05, 2.2413059923565015e-05, 2.2413059923565015e-05, 2.2413059923565015e-05, 2.2413059923565015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2413059923565015e-05

Optimization complete. Final v2v error: 3.94565749168396 mm

Highest mean error: 4.85811710357666 mm for frame 121

Lowest mean error: 3.3569910526275635 mm for frame 102

Saving results

Total time: 208.62030029296875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060400
Iteration 2/25 | Loss: 0.00189927
Iteration 3/25 | Loss: 0.00101143
Iteration 4/25 | Loss: 0.00076764
Iteration 5/25 | Loss: 0.00071968
Iteration 6/25 | Loss: 0.00069242
Iteration 7/25 | Loss: 0.00068041
Iteration 8/25 | Loss: 0.00066367
Iteration 9/25 | Loss: 0.00066074
Iteration 10/25 | Loss: 0.00066581
Iteration 11/25 | Loss: 0.00066612
Iteration 12/25 | Loss: 0.00065332
Iteration 13/25 | Loss: 0.00064891
Iteration 14/25 | Loss: 0.00064436
Iteration 15/25 | Loss: 0.00064237
Iteration 16/25 | Loss: 0.00064190
Iteration 17/25 | Loss: 0.00064148
Iteration 18/25 | Loss: 0.00064113
Iteration 19/25 | Loss: 0.00064066
Iteration 20/25 | Loss: 0.00064033
Iteration 21/25 | Loss: 0.00063964
Iteration 22/25 | Loss: 0.00063893
Iteration 23/25 | Loss: 0.00063885
Iteration 24/25 | Loss: 0.00063885
Iteration 25/25 | Loss: 0.00063884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32658422
Iteration 2/25 | Loss: 0.00009369
Iteration 3/25 | Loss: 0.00009369
Iteration 4/25 | Loss: 0.00009369
Iteration 5/25 | Loss: 0.00009369
Iteration 6/25 | Loss: 0.00009369
Iteration 7/25 | Loss: 0.00009369
Iteration 8/25 | Loss: 0.00009369
Iteration 9/25 | Loss: 0.00009369
Iteration 10/25 | Loss: 0.00009369
Iteration 11/25 | Loss: 0.00009369
Iteration 12/25 | Loss: 0.00009369
Iteration 13/25 | Loss: 0.00009369
Iteration 14/25 | Loss: 0.00009369
Iteration 15/25 | Loss: 0.00009369
Iteration 16/25 | Loss: 0.00009369
Iteration 17/25 | Loss: 0.00009369
Iteration 18/25 | Loss: 0.00009369
Iteration 19/25 | Loss: 0.00009369
Iteration 20/25 | Loss: 0.00009369
Iteration 21/25 | Loss: 0.00009369
Iteration 22/25 | Loss: 0.00009369
Iteration 23/25 | Loss: 0.00009369
Iteration 24/25 | Loss: 0.00009369
Iteration 25/25 | Loss: 0.00009369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009369
Iteration 2/1000 | Loss: 0.00007689
Iteration 3/1000 | Loss: 0.00003211
Iteration 4/1000 | Loss: 0.00002668
Iteration 5/1000 | Loss: 0.00021853
Iteration 6/1000 | Loss: 0.00010739
Iteration 7/1000 | Loss: 0.00006779
Iteration 8/1000 | Loss: 0.00016944
Iteration 9/1000 | Loss: 0.00012560
Iteration 10/1000 | Loss: 0.00017256
Iteration 11/1000 | Loss: 0.00011350
Iteration 12/1000 | Loss: 0.00015296
Iteration 13/1000 | Loss: 0.00002749
Iteration 14/1000 | Loss: 0.00002404
Iteration 15/1000 | Loss: 0.00002275
Iteration 16/1000 | Loss: 0.00002187
Iteration 17/1000 | Loss: 0.00002130
Iteration 18/1000 | Loss: 0.00002090
Iteration 19/1000 | Loss: 0.00002055
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00001994
Iteration 23/1000 | Loss: 0.00001993
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001990
Iteration 30/1000 | Loss: 0.00001990
Iteration 31/1000 | Loss: 0.00001990
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001988
Iteration 34/1000 | Loss: 0.00001987
Iteration 35/1000 | Loss: 0.00001986
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001984
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001982
Iteration 40/1000 | Loss: 0.00001981
Iteration 41/1000 | Loss: 0.00001981
Iteration 42/1000 | Loss: 0.00001981
Iteration 43/1000 | Loss: 0.00001981
Iteration 44/1000 | Loss: 0.00001981
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001981
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001980
Iteration 51/1000 | Loss: 0.00001980
Iteration 52/1000 | Loss: 0.00001980
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001980
Iteration 55/1000 | Loss: 0.00001980
Iteration 56/1000 | Loss: 0.00001980
Iteration 57/1000 | Loss: 0.00001980
Iteration 58/1000 | Loss: 0.00001980
Iteration 59/1000 | Loss: 0.00001979
Iteration 60/1000 | Loss: 0.00001979
Iteration 61/1000 | Loss: 0.00001978
Iteration 62/1000 | Loss: 0.00001978
Iteration 63/1000 | Loss: 0.00001977
Iteration 64/1000 | Loss: 0.00001977
Iteration 65/1000 | Loss: 0.00001977
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001975
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001974
Iteration 75/1000 | Loss: 0.00001973
Iteration 76/1000 | Loss: 0.00001973
Iteration 77/1000 | Loss: 0.00001972
Iteration 78/1000 | Loss: 0.00001972
Iteration 79/1000 | Loss: 0.00001972
Iteration 80/1000 | Loss: 0.00001972
Iteration 81/1000 | Loss: 0.00001972
Iteration 82/1000 | Loss: 0.00001972
Iteration 83/1000 | Loss: 0.00001972
Iteration 84/1000 | Loss: 0.00001971
Iteration 85/1000 | Loss: 0.00001971
Iteration 86/1000 | Loss: 0.00001971
Iteration 87/1000 | Loss: 0.00001971
Iteration 88/1000 | Loss: 0.00001971
Iteration 89/1000 | Loss: 0.00001971
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001970
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.9699815311469138e-05, 1.9699815311469138e-05, 1.9699815311469138e-05, 1.9699815311469138e-05, 1.9699815311469138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9699815311469138e-05

Optimization complete. Final v2v error: 3.7471487522125244 mm

Highest mean error: 4.421947002410889 mm for frame 5

Lowest mean error: 3.188849925994873 mm for frame 74

Saving results

Total time: 87.17337965965271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618082
Iteration 2/25 | Loss: 0.00065568
Iteration 3/25 | Loss: 0.00052748
Iteration 4/25 | Loss: 0.00050466
Iteration 5/25 | Loss: 0.00049582
Iteration 6/25 | Loss: 0.00049348
Iteration 7/25 | Loss: 0.00049279
Iteration 8/25 | Loss: 0.00049279
Iteration 9/25 | Loss: 0.00049279
Iteration 10/25 | Loss: 0.00049279
Iteration 11/25 | Loss: 0.00049279
Iteration 12/25 | Loss: 0.00049279
Iteration 13/25 | Loss: 0.00049279
Iteration 14/25 | Loss: 0.00049279
Iteration 15/25 | Loss: 0.00049279
Iteration 16/25 | Loss: 0.00049279
Iteration 17/25 | Loss: 0.00049279
Iteration 18/25 | Loss: 0.00049279
Iteration 19/25 | Loss: 0.00049279
Iteration 20/25 | Loss: 0.00049279
Iteration 21/25 | Loss: 0.00049279
Iteration 22/25 | Loss: 0.00049279
Iteration 23/25 | Loss: 0.00049279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004927910049445927, 0.0004927910049445927, 0.0004927910049445927, 0.0004927910049445927, 0.0004927910049445927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004927910049445927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.04440498
Iteration 2/25 | Loss: 0.00009742
Iteration 3/25 | Loss: 0.00009741
Iteration 4/25 | Loss: 0.00009741
Iteration 5/25 | Loss: 0.00009741
Iteration 6/25 | Loss: 0.00009741
Iteration 7/25 | Loss: 0.00009741
Iteration 8/25 | Loss: 0.00009741
Iteration 9/25 | Loss: 0.00009741
Iteration 10/25 | Loss: 0.00009741
Iteration 11/25 | Loss: 0.00009741
Iteration 12/25 | Loss: 0.00009741
Iteration 13/25 | Loss: 0.00009741
Iteration 14/25 | Loss: 0.00009741
Iteration 15/25 | Loss: 0.00009741
Iteration 16/25 | Loss: 0.00009741
Iteration 17/25 | Loss: 0.00009741
Iteration 18/25 | Loss: 0.00009741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [9.740734094521031e-05, 9.740734094521031e-05, 9.740734094521031e-05, 9.740734094521031e-05, 9.740734094521031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.740734094521031e-05

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009741
Iteration 2/1000 | Loss: 0.00001443
Iteration 3/1000 | Loss: 0.00001100
Iteration 4/1000 | Loss: 0.00001027
Iteration 5/1000 | Loss: 0.00001002
Iteration 6/1000 | Loss: 0.00000986
Iteration 7/1000 | Loss: 0.00000979
Iteration 8/1000 | Loss: 0.00000975
Iteration 9/1000 | Loss: 0.00000972
Iteration 10/1000 | Loss: 0.00000972
Iteration 11/1000 | Loss: 0.00000971
Iteration 12/1000 | Loss: 0.00000969
Iteration 13/1000 | Loss: 0.00000967
Iteration 14/1000 | Loss: 0.00000967
Iteration 15/1000 | Loss: 0.00000966
Iteration 16/1000 | Loss: 0.00000966
Iteration 17/1000 | Loss: 0.00000966
Iteration 18/1000 | Loss: 0.00000966
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000963
Iteration 21/1000 | Loss: 0.00000960
Iteration 22/1000 | Loss: 0.00000959
Iteration 23/1000 | Loss: 0.00000959
Iteration 24/1000 | Loss: 0.00000956
Iteration 25/1000 | Loss: 0.00000956
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000955
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000954
Iteration 30/1000 | Loss: 0.00000954
Iteration 31/1000 | Loss: 0.00000953
Iteration 32/1000 | Loss: 0.00000953
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000952
Iteration 36/1000 | Loss: 0.00000951
Iteration 37/1000 | Loss: 0.00000951
Iteration 38/1000 | Loss: 0.00000951
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000951
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000951
Iteration 45/1000 | Loss: 0.00000951
Iteration 46/1000 | Loss: 0.00000951
Iteration 47/1000 | Loss: 0.00000950
Iteration 48/1000 | Loss: 0.00000950
Iteration 49/1000 | Loss: 0.00000950
Iteration 50/1000 | Loss: 0.00000950
Iteration 51/1000 | Loss: 0.00000950
Iteration 52/1000 | Loss: 0.00000950
Iteration 53/1000 | Loss: 0.00000949
Iteration 54/1000 | Loss: 0.00000949
Iteration 55/1000 | Loss: 0.00000949
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000948
Iteration 58/1000 | Loss: 0.00000948
Iteration 59/1000 | Loss: 0.00000948
Iteration 60/1000 | Loss: 0.00000948
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000947
Iteration 63/1000 | Loss: 0.00000947
Iteration 64/1000 | Loss: 0.00000947
Iteration 65/1000 | Loss: 0.00000946
Iteration 66/1000 | Loss: 0.00000946
Iteration 67/1000 | Loss: 0.00000946
Iteration 68/1000 | Loss: 0.00000946
Iteration 69/1000 | Loss: 0.00000946
Iteration 70/1000 | Loss: 0.00000946
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000945
Iteration 75/1000 | Loss: 0.00000945
Iteration 76/1000 | Loss: 0.00000945
Iteration 77/1000 | Loss: 0.00000945
Iteration 78/1000 | Loss: 0.00000945
Iteration 79/1000 | Loss: 0.00000945
Iteration 80/1000 | Loss: 0.00000945
Iteration 81/1000 | Loss: 0.00000944
Iteration 82/1000 | Loss: 0.00000944
Iteration 83/1000 | Loss: 0.00000944
Iteration 84/1000 | Loss: 0.00000944
Iteration 85/1000 | Loss: 0.00000943
Iteration 86/1000 | Loss: 0.00000943
Iteration 87/1000 | Loss: 0.00000943
Iteration 88/1000 | Loss: 0.00000942
Iteration 89/1000 | Loss: 0.00000942
Iteration 90/1000 | Loss: 0.00000942
Iteration 91/1000 | Loss: 0.00000942
Iteration 92/1000 | Loss: 0.00000942
Iteration 93/1000 | Loss: 0.00000942
Iteration 94/1000 | Loss: 0.00000942
Iteration 95/1000 | Loss: 0.00000942
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000942
Iteration 99/1000 | Loss: 0.00000942
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000942
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000941
Iteration 105/1000 | Loss: 0.00000941
Iteration 106/1000 | Loss: 0.00000941
Iteration 107/1000 | Loss: 0.00000941
Iteration 108/1000 | Loss: 0.00000941
Iteration 109/1000 | Loss: 0.00000941
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000941
Iteration 114/1000 | Loss: 0.00000940
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000940
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000940
Iteration 125/1000 | Loss: 0.00000940
Iteration 126/1000 | Loss: 0.00000940
Iteration 127/1000 | Loss: 0.00000940
Iteration 128/1000 | Loss: 0.00000940
Iteration 129/1000 | Loss: 0.00000940
Iteration 130/1000 | Loss: 0.00000940
Iteration 131/1000 | Loss: 0.00000940
Iteration 132/1000 | Loss: 0.00000940
Iteration 133/1000 | Loss: 0.00000940
Iteration 134/1000 | Loss: 0.00000940
Iteration 135/1000 | Loss: 0.00000940
Iteration 136/1000 | Loss: 0.00000940
Iteration 137/1000 | Loss: 0.00000940
Iteration 138/1000 | Loss: 0.00000940
Iteration 139/1000 | Loss: 0.00000940
Iteration 140/1000 | Loss: 0.00000940
Iteration 141/1000 | Loss: 0.00000940
Iteration 142/1000 | Loss: 0.00000940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [9.400276212545577e-06, 9.400276212545577e-06, 9.400276212545577e-06, 9.400276212545577e-06, 9.400276212545577e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.400276212545577e-06

Optimization complete. Final v2v error: 2.556473970413208 mm

Highest mean error: 2.941486358642578 mm for frame 59

Lowest mean error: 2.181253671646118 mm for frame 42

Saving results

Total time: 31.01310157775879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812426
Iteration 2/25 | Loss: 0.00137930
Iteration 3/25 | Loss: 0.00078344
Iteration 4/25 | Loss: 0.00072114
Iteration 5/25 | Loss: 0.00067936
Iteration 6/25 | Loss: 0.00070377
Iteration 7/25 | Loss: 0.00066216
Iteration 8/25 | Loss: 0.00065530
Iteration 9/25 | Loss: 0.00061044
Iteration 10/25 | Loss: 0.00059051
Iteration 11/25 | Loss: 0.00058380
Iteration 12/25 | Loss: 0.00057665
Iteration 13/25 | Loss: 0.00058579
Iteration 14/25 | Loss: 0.00055900
Iteration 15/25 | Loss: 0.00055401
Iteration 16/25 | Loss: 0.00054929
Iteration 17/25 | Loss: 0.00054345
Iteration 18/25 | Loss: 0.00054669
Iteration 19/25 | Loss: 0.00055729
Iteration 20/25 | Loss: 0.00054258
Iteration 21/25 | Loss: 0.00053906
Iteration 22/25 | Loss: 0.00053494
Iteration 23/25 | Loss: 0.00053311
Iteration 24/25 | Loss: 0.00053284
Iteration 25/25 | Loss: 0.00053268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22279334
Iteration 2/25 | Loss: 0.00010255
Iteration 3/25 | Loss: 0.00010253
Iteration 4/25 | Loss: 0.00010253
Iteration 5/25 | Loss: 0.00010253
Iteration 6/25 | Loss: 0.00010253
Iteration 7/25 | Loss: 0.00010253
Iteration 8/25 | Loss: 0.00010253
Iteration 9/25 | Loss: 0.00010253
Iteration 10/25 | Loss: 0.00010253
Iteration 11/25 | Loss: 0.00010253
Iteration 12/25 | Loss: 0.00010253
Iteration 13/25 | Loss: 0.00010253
Iteration 14/25 | Loss: 0.00010253
Iteration 15/25 | Loss: 0.00010253
Iteration 16/25 | Loss: 0.00010253
Iteration 17/25 | Loss: 0.00010253
Iteration 18/25 | Loss: 0.00010253
Iteration 19/25 | Loss: 0.00010253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0001025298479362391, 0.0001025298479362391, 0.0001025298479362391, 0.0001025298479362391, 0.0001025298479362391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001025298479362391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010253
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001805
Iteration 4/1000 | Loss: 0.00001646
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001453
Iteration 8/1000 | Loss: 0.00001415
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001368
Iteration 12/1000 | Loss: 0.00001359
Iteration 13/1000 | Loss: 0.00001354
Iteration 14/1000 | Loss: 0.00001350
Iteration 15/1000 | Loss: 0.00001349
Iteration 16/1000 | Loss: 0.00001345
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001325
Iteration 23/1000 | Loss: 0.00001323
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001304
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001297
Iteration 39/1000 | Loss: 0.00003239
Iteration 40/1000 | Loss: 0.00003239
Iteration 41/1000 | Loss: 0.00003239
Iteration 42/1000 | Loss: 0.00009147
Iteration 43/1000 | Loss: 0.00004896
Iteration 44/1000 | Loss: 0.00003745
Iteration 45/1000 | Loss: 0.00003733
Iteration 46/1000 | Loss: 0.00002571
Iteration 47/1000 | Loss: 0.00003693
Iteration 48/1000 | Loss: 0.00002723
Iteration 49/1000 | Loss: 0.00002444
Iteration 50/1000 | Loss: 0.00002332
Iteration 51/1000 | Loss: 0.00002439
Iteration 52/1000 | Loss: 0.00002562
Iteration 53/1000 | Loss: 0.00004142
Iteration 54/1000 | Loss: 0.00002147
Iteration 55/1000 | Loss: 0.00001683
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001295
Iteration 61/1000 | Loss: 0.00001294
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001283
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001267
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001256
Iteration 81/1000 | Loss: 0.00001256
Iteration 82/1000 | Loss: 0.00001256
Iteration 83/1000 | Loss: 0.00001256
Iteration 84/1000 | Loss: 0.00001256
Iteration 85/1000 | Loss: 0.00001256
Iteration 86/1000 | Loss: 0.00001255
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001253
Iteration 89/1000 | Loss: 0.00001253
Iteration 90/1000 | Loss: 0.00001253
Iteration 91/1000 | Loss: 0.00001253
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001250
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001249
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001248
Iteration 111/1000 | Loss: 0.00001248
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001245
Iteration 127/1000 | Loss: 0.00001245
Iteration 128/1000 | Loss: 0.00001245
Iteration 129/1000 | Loss: 0.00001245
Iteration 130/1000 | Loss: 0.00001245
Iteration 131/1000 | Loss: 0.00001245
Iteration 132/1000 | Loss: 0.00001245
Iteration 133/1000 | Loss: 0.00001245
Iteration 134/1000 | Loss: 0.00001245
Iteration 135/1000 | Loss: 0.00001245
Iteration 136/1000 | Loss: 0.00001245
Iteration 137/1000 | Loss: 0.00001244
Iteration 138/1000 | Loss: 0.00001244
Iteration 139/1000 | Loss: 0.00001244
Iteration 140/1000 | Loss: 0.00001244
Iteration 141/1000 | Loss: 0.00001244
Iteration 142/1000 | Loss: 0.00001244
Iteration 143/1000 | Loss: 0.00001244
Iteration 144/1000 | Loss: 0.00001244
Iteration 145/1000 | Loss: 0.00001244
Iteration 146/1000 | Loss: 0.00001244
Iteration 147/1000 | Loss: 0.00001244
Iteration 148/1000 | Loss: 0.00001244
Iteration 149/1000 | Loss: 0.00001244
Iteration 150/1000 | Loss: 0.00001244
Iteration 151/1000 | Loss: 0.00001244
Iteration 152/1000 | Loss: 0.00001243
Iteration 153/1000 | Loss: 0.00001243
Iteration 154/1000 | Loss: 0.00001243
Iteration 155/1000 | Loss: 0.00001243
Iteration 156/1000 | Loss: 0.00001243
Iteration 157/1000 | Loss: 0.00001243
Iteration 158/1000 | Loss: 0.00001243
Iteration 159/1000 | Loss: 0.00001243
Iteration 160/1000 | Loss: 0.00001243
Iteration 161/1000 | Loss: 0.00001243
Iteration 162/1000 | Loss: 0.00001243
Iteration 163/1000 | Loss: 0.00001243
Iteration 164/1000 | Loss: 0.00001243
Iteration 165/1000 | Loss: 0.00001243
Iteration 166/1000 | Loss: 0.00001243
Iteration 167/1000 | Loss: 0.00001243
Iteration 168/1000 | Loss: 0.00001243
Iteration 169/1000 | Loss: 0.00001243
Iteration 170/1000 | Loss: 0.00001242
Iteration 171/1000 | Loss: 0.00001242
Iteration 172/1000 | Loss: 0.00001242
Iteration 173/1000 | Loss: 0.00001242
Iteration 174/1000 | Loss: 0.00001242
Iteration 175/1000 | Loss: 0.00001242
Iteration 176/1000 | Loss: 0.00001242
Iteration 177/1000 | Loss: 0.00001242
Iteration 178/1000 | Loss: 0.00001242
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001241
Iteration 181/1000 | Loss: 0.00001241
Iteration 182/1000 | Loss: 0.00001241
Iteration 183/1000 | Loss: 0.00001241
Iteration 184/1000 | Loss: 0.00001241
Iteration 185/1000 | Loss: 0.00001241
Iteration 186/1000 | Loss: 0.00001241
Iteration 187/1000 | Loss: 0.00001241
Iteration 188/1000 | Loss: 0.00001241
Iteration 189/1000 | Loss: 0.00001241
Iteration 190/1000 | Loss: 0.00001241
Iteration 191/1000 | Loss: 0.00001241
Iteration 192/1000 | Loss: 0.00001241
Iteration 193/1000 | Loss: 0.00001241
Iteration 194/1000 | Loss: 0.00001241
Iteration 195/1000 | Loss: 0.00001241
Iteration 196/1000 | Loss: 0.00001240
Iteration 197/1000 | Loss: 0.00001240
Iteration 198/1000 | Loss: 0.00001240
Iteration 199/1000 | Loss: 0.00001240
Iteration 200/1000 | Loss: 0.00001240
Iteration 201/1000 | Loss: 0.00001240
Iteration 202/1000 | Loss: 0.00001240
Iteration 203/1000 | Loss: 0.00001240
Iteration 204/1000 | Loss: 0.00001240
Iteration 205/1000 | Loss: 0.00001240
Iteration 206/1000 | Loss: 0.00001240
Iteration 207/1000 | Loss: 0.00001239
Iteration 208/1000 | Loss: 0.00001239
Iteration 209/1000 | Loss: 0.00001239
Iteration 210/1000 | Loss: 0.00001239
Iteration 211/1000 | Loss: 0.00001239
Iteration 212/1000 | Loss: 0.00001239
Iteration 213/1000 | Loss: 0.00001239
Iteration 214/1000 | Loss: 0.00001239
Iteration 215/1000 | Loss: 0.00001239
Iteration 216/1000 | Loss: 0.00001239
Iteration 217/1000 | Loss: 0.00001239
Iteration 218/1000 | Loss: 0.00001239
Iteration 219/1000 | Loss: 0.00001239
Iteration 220/1000 | Loss: 0.00001238
Iteration 221/1000 | Loss: 0.00001238
Iteration 222/1000 | Loss: 0.00001238
Iteration 223/1000 | Loss: 0.00001238
Iteration 224/1000 | Loss: 0.00001238
Iteration 225/1000 | Loss: 0.00001238
Iteration 226/1000 | Loss: 0.00001238
Iteration 227/1000 | Loss: 0.00001238
Iteration 228/1000 | Loss: 0.00001237
Iteration 229/1000 | Loss: 0.00001237
Iteration 230/1000 | Loss: 0.00001237
Iteration 231/1000 | Loss: 0.00001237
Iteration 232/1000 | Loss: 0.00001237
Iteration 233/1000 | Loss: 0.00001237
Iteration 234/1000 | Loss: 0.00001237
Iteration 235/1000 | Loss: 0.00001237
Iteration 236/1000 | Loss: 0.00001237
Iteration 237/1000 | Loss: 0.00001237
Iteration 238/1000 | Loss: 0.00001237
Iteration 239/1000 | Loss: 0.00001237
Iteration 240/1000 | Loss: 0.00001237
Iteration 241/1000 | Loss: 0.00001237
Iteration 242/1000 | Loss: 0.00001237
Iteration 243/1000 | Loss: 0.00001237
Iteration 244/1000 | Loss: 0.00001237
Iteration 245/1000 | Loss: 0.00001236
Iteration 246/1000 | Loss: 0.00001236
Iteration 247/1000 | Loss: 0.00001236
Iteration 248/1000 | Loss: 0.00001236
Iteration 249/1000 | Loss: 0.00001236
Iteration 250/1000 | Loss: 0.00001236
Iteration 251/1000 | Loss: 0.00001236
Iteration 252/1000 | Loss: 0.00001236
Iteration 253/1000 | Loss: 0.00001235
Iteration 254/1000 | Loss: 0.00001235
Iteration 255/1000 | Loss: 0.00001235
Iteration 256/1000 | Loss: 0.00001235
Iteration 257/1000 | Loss: 0.00001235
Iteration 258/1000 | Loss: 0.00001235
Iteration 259/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.2353588317637332e-05, 1.2353588317637332e-05, 1.2353588317637332e-05, 1.2353588317637332e-05, 1.2353588317637332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2353588317637332e-05

Optimization complete. Final v2v error: 2.932877779006958 mm

Highest mean error: 4.318827152252197 mm for frame 41

Lowest mean error: 2.267025947570801 mm for frame 154

Saving results

Total time: 127.14186000823975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051695
Iteration 2/25 | Loss: 0.00384789
Iteration 3/25 | Loss: 0.00176019
Iteration 4/25 | Loss: 0.00181971
Iteration 5/25 | Loss: 0.00171192
Iteration 6/25 | Loss: 0.00152571
Iteration 7/25 | Loss: 0.00170257
Iteration 8/25 | Loss: 0.00141288
Iteration 9/25 | Loss: 0.00136069
Iteration 10/25 | Loss: 0.00133545
Iteration 11/25 | Loss: 0.00132635
Iteration 12/25 | Loss: 0.00131575
Iteration 13/25 | Loss: 0.00134187
Iteration 14/25 | Loss: 0.00145921
Iteration 15/25 | Loss: 0.00134642
Iteration 16/25 | Loss: 0.00118184
Iteration 17/25 | Loss: 0.00107433
Iteration 18/25 | Loss: 0.00104450
Iteration 19/25 | Loss: 0.00102982
Iteration 20/25 | Loss: 0.00102425
Iteration 21/25 | Loss: 0.00102198
Iteration 22/25 | Loss: 0.00102088
Iteration 23/25 | Loss: 0.00102034
Iteration 24/25 | Loss: 0.00102009
Iteration 25/25 | Loss: 0.00101986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35210025
Iteration 2/25 | Loss: 0.00853927
Iteration 3/25 | Loss: 0.00170955
Iteration 4/25 | Loss: 0.00170950
Iteration 5/25 | Loss: 0.00170950
Iteration 6/25 | Loss: 0.00170950
Iteration 7/25 | Loss: 0.00170950
Iteration 8/25 | Loss: 0.00170950
Iteration 9/25 | Loss: 0.00170950
Iteration 10/25 | Loss: 0.00170950
Iteration 11/25 | Loss: 0.00170950
Iteration 12/25 | Loss: 0.00170950
Iteration 13/25 | Loss: 0.00170950
Iteration 14/25 | Loss: 0.00170950
Iteration 15/25 | Loss: 0.00170950
Iteration 16/25 | Loss: 0.00170950
Iteration 17/25 | Loss: 0.00170950
Iteration 18/25 | Loss: 0.00170950
Iteration 19/25 | Loss: 0.00170950
Iteration 20/25 | Loss: 0.00170950
Iteration 21/25 | Loss: 0.00170950
Iteration 22/25 | Loss: 0.00170950
Iteration 23/25 | Loss: 0.00170950
Iteration 24/25 | Loss: 0.00170950
Iteration 25/25 | Loss: 0.00170950

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170950
Iteration 2/1000 | Loss: 0.00803232
Iteration 3/1000 | Loss: 0.00137153
Iteration 4/1000 | Loss: 0.00448602
Iteration 5/1000 | Loss: 0.01368824
Iteration 6/1000 | Loss: 0.00163636
Iteration 7/1000 | Loss: 0.00181945
Iteration 8/1000 | Loss: 0.00431622
Iteration 9/1000 | Loss: 0.00142400
Iteration 10/1000 | Loss: 0.00253989
Iteration 11/1000 | Loss: 0.00113227
Iteration 12/1000 | Loss: 0.00112452
Iteration 13/1000 | Loss: 0.00160712
Iteration 14/1000 | Loss: 0.00148597
Iteration 15/1000 | Loss: 0.00180888
Iteration 16/1000 | Loss: 0.00090628
Iteration 17/1000 | Loss: 0.00087930
Iteration 18/1000 | Loss: 0.00100119
Iteration 19/1000 | Loss: 0.00289270
Iteration 20/1000 | Loss: 0.00307752
Iteration 21/1000 | Loss: 0.00156593
Iteration 22/1000 | Loss: 0.00067971
Iteration 23/1000 | Loss: 0.00341253
Iteration 24/1000 | Loss: 0.00048977
Iteration 25/1000 | Loss: 0.00046739
Iteration 26/1000 | Loss: 0.00100591
Iteration 27/1000 | Loss: 0.00208443
Iteration 28/1000 | Loss: 0.00045167
Iteration 29/1000 | Loss: 0.00029258
Iteration 30/1000 | Loss: 0.00024484
Iteration 31/1000 | Loss: 0.00030055
Iteration 32/1000 | Loss: 0.00009867
Iteration 33/1000 | Loss: 0.00023415
Iteration 34/1000 | Loss: 0.00021907
Iteration 35/1000 | Loss: 0.00034765
Iteration 36/1000 | Loss: 0.00014117
Iteration 37/1000 | Loss: 0.00019136
Iteration 38/1000 | Loss: 0.00007411
Iteration 39/1000 | Loss: 0.00006349
Iteration 40/1000 | Loss: 0.00072615
Iteration 41/1000 | Loss: 0.00005878
Iteration 42/1000 | Loss: 0.00018408
Iteration 43/1000 | Loss: 0.00020046
Iteration 44/1000 | Loss: 0.00011097
Iteration 45/1000 | Loss: 0.00031641
Iteration 46/1000 | Loss: 0.00006504
Iteration 47/1000 | Loss: 0.00005634
Iteration 48/1000 | Loss: 0.00052507
Iteration 49/1000 | Loss: 0.00045011
Iteration 50/1000 | Loss: 0.00058053
Iteration 51/1000 | Loss: 0.00056535
Iteration 52/1000 | Loss: 0.00047412
Iteration 53/1000 | Loss: 0.00036222
Iteration 54/1000 | Loss: 0.00071456
Iteration 55/1000 | Loss: 0.00307473
Iteration 56/1000 | Loss: 0.00004847
Iteration 57/1000 | Loss: 0.00004981
Iteration 58/1000 | Loss: 0.00015348
Iteration 59/1000 | Loss: 0.00101578
Iteration 60/1000 | Loss: 0.00022566
Iteration 61/1000 | Loss: 0.00050696
Iteration 62/1000 | Loss: 0.00014438
Iteration 63/1000 | Loss: 0.00005309
Iteration 64/1000 | Loss: 0.00004194
Iteration 65/1000 | Loss: 0.00006195
Iteration 66/1000 | Loss: 0.00004955
Iteration 67/1000 | Loss: 0.00004851
Iteration 68/1000 | Loss: 0.00003323
Iteration 69/1000 | Loss: 0.00004388
Iteration 70/1000 | Loss: 0.00004255
Iteration 71/1000 | Loss: 0.00004303
Iteration 72/1000 | Loss: 0.00003715
Iteration 73/1000 | Loss: 0.00004170
Iteration 74/1000 | Loss: 0.00004279
Iteration 75/1000 | Loss: 0.00007292
Iteration 76/1000 | Loss: 0.00005803
Iteration 77/1000 | Loss: 0.00004968
Iteration 78/1000 | Loss: 0.00004145
Iteration 79/1000 | Loss: 0.00004119
Iteration 80/1000 | Loss: 0.00004500
Iteration 81/1000 | Loss: 0.00004017
Iteration 82/1000 | Loss: 0.00024637
Iteration 83/1000 | Loss: 0.00003879
Iteration 84/1000 | Loss: 0.00002634
Iteration 85/1000 | Loss: 0.00002491
Iteration 86/1000 | Loss: 0.00002639
Iteration 87/1000 | Loss: 0.00002329
Iteration 88/1000 | Loss: 0.00023089
Iteration 89/1000 | Loss: 0.00033857
Iteration 90/1000 | Loss: 0.00004326
Iteration 91/1000 | Loss: 0.00002860
Iteration 92/1000 | Loss: 0.00008595
Iteration 93/1000 | Loss: 0.00005840
Iteration 94/1000 | Loss: 0.00016514
Iteration 95/1000 | Loss: 0.00002797
Iteration 96/1000 | Loss: 0.00003031
Iteration 97/1000 | Loss: 0.00003191
Iteration 98/1000 | Loss: 0.00002313
Iteration 99/1000 | Loss: 0.00002863
Iteration 100/1000 | Loss: 0.00002820
Iteration 101/1000 | Loss: 0.00002477
Iteration 102/1000 | Loss: 0.00002665
Iteration 103/1000 | Loss: 0.00002729
Iteration 104/1000 | Loss: 0.00002686
Iteration 105/1000 | Loss: 0.00002657
Iteration 106/1000 | Loss: 0.00050240
Iteration 107/1000 | Loss: 0.00003052
Iteration 108/1000 | Loss: 0.00002158
Iteration 109/1000 | Loss: 0.00015981
Iteration 110/1000 | Loss: 0.00011048
Iteration 111/1000 | Loss: 0.00002310
Iteration 112/1000 | Loss: 0.00002296
Iteration 113/1000 | Loss: 0.00002279
Iteration 114/1000 | Loss: 0.00002473
Iteration 115/1000 | Loss: 0.00002244
Iteration 116/1000 | Loss: 0.00002821
Iteration 117/1000 | Loss: 0.00002950
Iteration 118/1000 | Loss: 0.00002635
Iteration 119/1000 | Loss: 0.00025999
Iteration 120/1000 | Loss: 0.00020161
Iteration 121/1000 | Loss: 0.00005062
Iteration 122/1000 | Loss: 0.00002266
Iteration 123/1000 | Loss: 0.00002123
Iteration 124/1000 | Loss: 0.00002858
Iteration 125/1000 | Loss: 0.00031302
Iteration 126/1000 | Loss: 0.00005176
Iteration 127/1000 | Loss: 0.00002806
Iteration 128/1000 | Loss: 0.00002393
Iteration 129/1000 | Loss: 0.00001963
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00011554
Iteration 132/1000 | Loss: 0.00006594
Iteration 133/1000 | Loss: 0.00001879
Iteration 134/1000 | Loss: 0.00011589
Iteration 135/1000 | Loss: 0.00002339
Iteration 136/1000 | Loss: 0.00004781
Iteration 137/1000 | Loss: 0.00001838
Iteration 138/1000 | Loss: 0.00001818
Iteration 139/1000 | Loss: 0.00001797
Iteration 140/1000 | Loss: 0.00001794
Iteration 141/1000 | Loss: 0.00001790
Iteration 142/1000 | Loss: 0.00001786
Iteration 143/1000 | Loss: 0.00001785
Iteration 144/1000 | Loss: 0.00001785
Iteration 145/1000 | Loss: 0.00001785
Iteration 146/1000 | Loss: 0.00001784
Iteration 147/1000 | Loss: 0.00001784
Iteration 148/1000 | Loss: 0.00001784
Iteration 149/1000 | Loss: 0.00001783
Iteration 150/1000 | Loss: 0.00001783
Iteration 151/1000 | Loss: 0.00001783
Iteration 152/1000 | Loss: 0.00001782
Iteration 153/1000 | Loss: 0.00001782
Iteration 154/1000 | Loss: 0.00001781
Iteration 155/1000 | Loss: 0.00001781
Iteration 156/1000 | Loss: 0.00001780
Iteration 157/1000 | Loss: 0.00001780
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001779
Iteration 162/1000 | Loss: 0.00001779
Iteration 163/1000 | Loss: 0.00001779
Iteration 164/1000 | Loss: 0.00001779
Iteration 165/1000 | Loss: 0.00001779
Iteration 166/1000 | Loss: 0.00001778
Iteration 167/1000 | Loss: 0.00001778
Iteration 168/1000 | Loss: 0.00001778
Iteration 169/1000 | Loss: 0.00001778
Iteration 170/1000 | Loss: 0.00001778
Iteration 171/1000 | Loss: 0.00001778
Iteration 172/1000 | Loss: 0.00001778
Iteration 173/1000 | Loss: 0.00001778
Iteration 174/1000 | Loss: 0.00001778
Iteration 175/1000 | Loss: 0.00001778
Iteration 176/1000 | Loss: 0.00001777
Iteration 177/1000 | Loss: 0.00001777
Iteration 178/1000 | Loss: 0.00001777
Iteration 179/1000 | Loss: 0.00001777
Iteration 180/1000 | Loss: 0.00001777
Iteration 181/1000 | Loss: 0.00001777
Iteration 182/1000 | Loss: 0.00001777
Iteration 183/1000 | Loss: 0.00001777
Iteration 184/1000 | Loss: 0.00001777
Iteration 185/1000 | Loss: 0.00001776
Iteration 186/1000 | Loss: 0.00001776
Iteration 187/1000 | Loss: 0.00001776
Iteration 188/1000 | Loss: 0.00001776
Iteration 189/1000 | Loss: 0.00001776
Iteration 190/1000 | Loss: 0.00001776
Iteration 191/1000 | Loss: 0.00001776
Iteration 192/1000 | Loss: 0.00001776
Iteration 193/1000 | Loss: 0.00001776
Iteration 194/1000 | Loss: 0.00001776
Iteration 195/1000 | Loss: 0.00001776
Iteration 196/1000 | Loss: 0.00001775
Iteration 197/1000 | Loss: 0.00001775
Iteration 198/1000 | Loss: 0.00001775
Iteration 199/1000 | Loss: 0.00001775
Iteration 200/1000 | Loss: 0.00001775
Iteration 201/1000 | Loss: 0.00001775
Iteration 202/1000 | Loss: 0.00001775
Iteration 203/1000 | Loss: 0.00001775
Iteration 204/1000 | Loss: 0.00001774
Iteration 205/1000 | Loss: 0.00001774
Iteration 206/1000 | Loss: 0.00001774
Iteration 207/1000 | Loss: 0.00001774
Iteration 208/1000 | Loss: 0.00001773
Iteration 209/1000 | Loss: 0.00001773
Iteration 210/1000 | Loss: 0.00001773
Iteration 211/1000 | Loss: 0.00001772
Iteration 212/1000 | Loss: 0.00001772
Iteration 213/1000 | Loss: 0.00001772
Iteration 214/1000 | Loss: 0.00001772
Iteration 215/1000 | Loss: 0.00001772
Iteration 216/1000 | Loss: 0.00001772
Iteration 217/1000 | Loss: 0.00001771
Iteration 218/1000 | Loss: 0.00001771
Iteration 219/1000 | Loss: 0.00001771
Iteration 220/1000 | Loss: 0.00001771
Iteration 221/1000 | Loss: 0.00001771
Iteration 222/1000 | Loss: 0.00001771
Iteration 223/1000 | Loss: 0.00001771
Iteration 224/1000 | Loss: 0.00001771
Iteration 225/1000 | Loss: 0.00001770
Iteration 226/1000 | Loss: 0.00001770
Iteration 227/1000 | Loss: 0.00001770
Iteration 228/1000 | Loss: 0.00001770
Iteration 229/1000 | Loss: 0.00001770
Iteration 230/1000 | Loss: 0.00001770
Iteration 231/1000 | Loss: 0.00001770
Iteration 232/1000 | Loss: 0.00001769
Iteration 233/1000 | Loss: 0.00001769
Iteration 234/1000 | Loss: 0.00001769
Iteration 235/1000 | Loss: 0.00001769
Iteration 236/1000 | Loss: 0.00001769
Iteration 237/1000 | Loss: 0.00001769
Iteration 238/1000 | Loss: 0.00001769
Iteration 239/1000 | Loss: 0.00001769
Iteration 240/1000 | Loss: 0.00001769
Iteration 241/1000 | Loss: 0.00001769
Iteration 242/1000 | Loss: 0.00001769
Iteration 243/1000 | Loss: 0.00001769
Iteration 244/1000 | Loss: 0.00001769
Iteration 245/1000 | Loss: 0.00001769
Iteration 246/1000 | Loss: 0.00001769
Iteration 247/1000 | Loss: 0.00001768
Iteration 248/1000 | Loss: 0.00001768
Iteration 249/1000 | Loss: 0.00001768
Iteration 250/1000 | Loss: 0.00001768
Iteration 251/1000 | Loss: 0.00001768
Iteration 252/1000 | Loss: 0.00001768
Iteration 253/1000 | Loss: 0.00001768
Iteration 254/1000 | Loss: 0.00001768
Iteration 255/1000 | Loss: 0.00001768
Iteration 256/1000 | Loss: 0.00001768
Iteration 257/1000 | Loss: 0.00001768
Iteration 258/1000 | Loss: 0.00001768
Iteration 259/1000 | Loss: 0.00001768
Iteration 260/1000 | Loss: 0.00001768
Iteration 261/1000 | Loss: 0.00001768
Iteration 262/1000 | Loss: 0.00001768
Iteration 263/1000 | Loss: 0.00001768
Iteration 264/1000 | Loss: 0.00001768
Iteration 265/1000 | Loss: 0.00001768
Iteration 266/1000 | Loss: 0.00001768
Iteration 267/1000 | Loss: 0.00001768
Iteration 268/1000 | Loss: 0.00001768
Iteration 269/1000 | Loss: 0.00001768
Iteration 270/1000 | Loss: 0.00001768
Iteration 271/1000 | Loss: 0.00001768
Iteration 272/1000 | Loss: 0.00001768
Iteration 273/1000 | Loss: 0.00001767
Iteration 274/1000 | Loss: 0.00001767
Iteration 275/1000 | Loss: 0.00001767
Iteration 276/1000 | Loss: 0.00001767
Iteration 277/1000 | Loss: 0.00001767
Iteration 278/1000 | Loss: 0.00001767
Iteration 279/1000 | Loss: 0.00001767
Iteration 280/1000 | Loss: 0.00001767
Iteration 281/1000 | Loss: 0.00001767
Iteration 282/1000 | Loss: 0.00001767
Iteration 283/1000 | Loss: 0.00001767
Iteration 284/1000 | Loss: 0.00001767
Iteration 285/1000 | Loss: 0.00001767
Iteration 286/1000 | Loss: 0.00001767
Iteration 287/1000 | Loss: 0.00001767
Iteration 288/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [1.767498724802863e-05, 1.767498724802863e-05, 1.767498724802863e-05, 1.767498724802863e-05, 1.767498724802863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.767498724802863e-05

Optimization complete. Final v2v error: 3.5139920711517334 mm

Highest mean error: 4.88399076461792 mm for frame 0

Lowest mean error: 3.1332850456237793 mm for frame 110

Saving results

Total time: 285.8966953754425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947457
Iteration 2/25 | Loss: 0.00141447
Iteration 3/25 | Loss: 0.00082754
Iteration 4/25 | Loss: 0.00074159
Iteration 5/25 | Loss: 0.00071650
Iteration 6/25 | Loss: 0.00070760
Iteration 7/25 | Loss: 0.00070525
Iteration 8/25 | Loss: 0.00070491
Iteration 9/25 | Loss: 0.00070491
Iteration 10/25 | Loss: 0.00070491
Iteration 11/25 | Loss: 0.00070491
Iteration 12/25 | Loss: 0.00070491
Iteration 13/25 | Loss: 0.00070491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007049078703857958, 0.0007049078703857958, 0.0007049078703857958, 0.0007049078703857958, 0.0007049078703857958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007049078703857958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10945523
Iteration 2/25 | Loss: 0.00030111
Iteration 3/25 | Loss: 0.00030110
Iteration 4/25 | Loss: 0.00030110
Iteration 5/25 | Loss: 0.00030109
Iteration 6/25 | Loss: 0.00030109
Iteration 7/25 | Loss: 0.00030109
Iteration 8/25 | Loss: 0.00030109
Iteration 9/25 | Loss: 0.00030109
Iteration 10/25 | Loss: 0.00030109
Iteration 11/25 | Loss: 0.00030109
Iteration 12/25 | Loss: 0.00030109
Iteration 13/25 | Loss: 0.00030109
Iteration 14/25 | Loss: 0.00030109
Iteration 15/25 | Loss: 0.00030109
Iteration 16/25 | Loss: 0.00030109
Iteration 17/25 | Loss: 0.00030109
Iteration 18/25 | Loss: 0.00030109
Iteration 19/25 | Loss: 0.00030109
Iteration 20/25 | Loss: 0.00030109
Iteration 21/25 | Loss: 0.00030109
Iteration 22/25 | Loss: 0.00030109
Iteration 23/25 | Loss: 0.00030109
Iteration 24/25 | Loss: 0.00030109
Iteration 25/25 | Loss: 0.00030109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003010931541211903, 0.0003010931541211903, 0.0003010931541211903, 0.0003010931541211903, 0.0003010931541211903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003010931541211903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030109
Iteration 2/1000 | Loss: 0.00004986
Iteration 3/1000 | Loss: 0.00004111
Iteration 4/1000 | Loss: 0.00003688
Iteration 5/1000 | Loss: 0.00003479
Iteration 6/1000 | Loss: 0.00003338
Iteration 7/1000 | Loss: 0.00003230
Iteration 8/1000 | Loss: 0.00003152
Iteration 9/1000 | Loss: 0.00003104
Iteration 10/1000 | Loss: 0.00003068
Iteration 11/1000 | Loss: 0.00003045
Iteration 12/1000 | Loss: 0.00003032
Iteration 13/1000 | Loss: 0.00003021
Iteration 14/1000 | Loss: 0.00003020
Iteration 15/1000 | Loss: 0.00003014
Iteration 16/1000 | Loss: 0.00003014
Iteration 17/1000 | Loss: 0.00003013
Iteration 18/1000 | Loss: 0.00003013
Iteration 19/1000 | Loss: 0.00003012
Iteration 20/1000 | Loss: 0.00003012
Iteration 21/1000 | Loss: 0.00003011
Iteration 22/1000 | Loss: 0.00003011
Iteration 23/1000 | Loss: 0.00003010
Iteration 24/1000 | Loss: 0.00003010
Iteration 25/1000 | Loss: 0.00003010
Iteration 26/1000 | Loss: 0.00003009
Iteration 27/1000 | Loss: 0.00003009
Iteration 28/1000 | Loss: 0.00003008
Iteration 29/1000 | Loss: 0.00003007
Iteration 30/1000 | Loss: 0.00003007
Iteration 31/1000 | Loss: 0.00003007
Iteration 32/1000 | Loss: 0.00003006
Iteration 33/1000 | Loss: 0.00003006
Iteration 34/1000 | Loss: 0.00003005
Iteration 35/1000 | Loss: 0.00003005
Iteration 36/1000 | Loss: 0.00003005
Iteration 37/1000 | Loss: 0.00003005
Iteration 38/1000 | Loss: 0.00003005
Iteration 39/1000 | Loss: 0.00003005
Iteration 40/1000 | Loss: 0.00003004
Iteration 41/1000 | Loss: 0.00003004
Iteration 42/1000 | Loss: 0.00003004
Iteration 43/1000 | Loss: 0.00003004
Iteration 44/1000 | Loss: 0.00003004
Iteration 45/1000 | Loss: 0.00003003
Iteration 46/1000 | Loss: 0.00003003
Iteration 47/1000 | Loss: 0.00003003
Iteration 48/1000 | Loss: 0.00003003
Iteration 49/1000 | Loss: 0.00003002
Iteration 50/1000 | Loss: 0.00003002
Iteration 51/1000 | Loss: 0.00003002
Iteration 52/1000 | Loss: 0.00003002
Iteration 53/1000 | Loss: 0.00003002
Iteration 54/1000 | Loss: 0.00003002
Iteration 55/1000 | Loss: 0.00003002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [3.0024048101040535e-05, 3.0024048101040535e-05, 3.0024048101040535e-05, 3.0024048101040535e-05, 3.0024048101040535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0024048101040535e-05

Optimization complete. Final v2v error: 4.440370082855225 mm

Highest mean error: 5.638678073883057 mm for frame 102

Lowest mean error: 3.3367745876312256 mm for frame 122

Saving results

Total time: 34.32934212684631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414114
Iteration 2/25 | Loss: 0.00077611
Iteration 3/25 | Loss: 0.00055314
Iteration 4/25 | Loss: 0.00051267
Iteration 5/25 | Loss: 0.00050227
Iteration 6/25 | Loss: 0.00050003
Iteration 7/25 | Loss: 0.00049939
Iteration 8/25 | Loss: 0.00049928
Iteration 9/25 | Loss: 0.00049928
Iteration 10/25 | Loss: 0.00049928
Iteration 11/25 | Loss: 0.00049928
Iteration 12/25 | Loss: 0.00049928
Iteration 13/25 | Loss: 0.00049928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0004992812173441052, 0.0004992812173441052, 0.0004992812173441052, 0.0004992812173441052, 0.0004992812173441052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004992812173441052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38449597
Iteration 2/25 | Loss: 0.00009655
Iteration 3/25 | Loss: 0.00009654
Iteration 4/25 | Loss: 0.00009654
Iteration 5/25 | Loss: 0.00009654
Iteration 6/25 | Loss: 0.00009654
Iteration 7/25 | Loss: 0.00009654
Iteration 8/25 | Loss: 0.00009654
Iteration 9/25 | Loss: 0.00009654
Iteration 10/25 | Loss: 0.00009654
Iteration 11/25 | Loss: 0.00009654
Iteration 12/25 | Loss: 0.00009654
Iteration 13/25 | Loss: 0.00009654
Iteration 14/25 | Loss: 0.00009654
Iteration 15/25 | Loss: 0.00009654
Iteration 16/25 | Loss: 0.00009654
Iteration 17/25 | Loss: 0.00009654
Iteration 18/25 | Loss: 0.00009654
Iteration 19/25 | Loss: 0.00009654
Iteration 20/25 | Loss: 0.00009654
Iteration 21/25 | Loss: 0.00009654
Iteration 22/25 | Loss: 0.00009654
Iteration 23/25 | Loss: 0.00009654
Iteration 24/25 | Loss: 0.00009654
Iteration 25/25 | Loss: 0.00009654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009654
Iteration 2/1000 | Loss: 0.00001700
Iteration 3/1000 | Loss: 0.00001160
Iteration 4/1000 | Loss: 0.00001059
Iteration 5/1000 | Loss: 0.00001021
Iteration 6/1000 | Loss: 0.00000996
Iteration 7/1000 | Loss: 0.00000977
Iteration 8/1000 | Loss: 0.00000961
Iteration 9/1000 | Loss: 0.00000959
Iteration 10/1000 | Loss: 0.00000957
Iteration 11/1000 | Loss: 0.00000955
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000948
Iteration 14/1000 | Loss: 0.00000948
Iteration 15/1000 | Loss: 0.00000947
Iteration 16/1000 | Loss: 0.00000947
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000943
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000939
Iteration 21/1000 | Loss: 0.00000939
Iteration 22/1000 | Loss: 0.00000939
Iteration 23/1000 | Loss: 0.00000938
Iteration 24/1000 | Loss: 0.00000936
Iteration 25/1000 | Loss: 0.00000936
Iteration 26/1000 | Loss: 0.00000935
Iteration 27/1000 | Loss: 0.00000935
Iteration 28/1000 | Loss: 0.00000934
Iteration 29/1000 | Loss: 0.00000934
Iteration 30/1000 | Loss: 0.00000934
Iteration 31/1000 | Loss: 0.00000933
Iteration 32/1000 | Loss: 0.00000933
Iteration 33/1000 | Loss: 0.00000933
Iteration 34/1000 | Loss: 0.00000933
Iteration 35/1000 | Loss: 0.00000933
Iteration 36/1000 | Loss: 0.00000932
Iteration 37/1000 | Loss: 0.00000932
Iteration 38/1000 | Loss: 0.00000931
Iteration 39/1000 | Loss: 0.00000931
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000931
Iteration 42/1000 | Loss: 0.00000931
Iteration 43/1000 | Loss: 0.00000930
Iteration 44/1000 | Loss: 0.00000930
Iteration 45/1000 | Loss: 0.00000930
Iteration 46/1000 | Loss: 0.00000929
Iteration 47/1000 | Loss: 0.00000929
Iteration 48/1000 | Loss: 0.00000929
Iteration 49/1000 | Loss: 0.00000929
Iteration 50/1000 | Loss: 0.00000928
Iteration 51/1000 | Loss: 0.00000928
Iteration 52/1000 | Loss: 0.00000928
Iteration 53/1000 | Loss: 0.00000928
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000927
Iteration 56/1000 | Loss: 0.00000927
Iteration 57/1000 | Loss: 0.00000927
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000927
Iteration 60/1000 | Loss: 0.00000927
Iteration 61/1000 | Loss: 0.00000926
Iteration 62/1000 | Loss: 0.00000926
Iteration 63/1000 | Loss: 0.00000926
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000925
Iteration 66/1000 | Loss: 0.00000925
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000924
Iteration 75/1000 | Loss: 0.00000924
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000923
Iteration 78/1000 | Loss: 0.00000923
Iteration 79/1000 | Loss: 0.00000923
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000922
Iteration 82/1000 | Loss: 0.00000922
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000922
Iteration 89/1000 | Loss: 0.00000922
Iteration 90/1000 | Loss: 0.00000922
Iteration 91/1000 | Loss: 0.00000922
Iteration 92/1000 | Loss: 0.00000922
Iteration 93/1000 | Loss: 0.00000922
Iteration 94/1000 | Loss: 0.00000922
Iteration 95/1000 | Loss: 0.00000922
Iteration 96/1000 | Loss: 0.00000922
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000921
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000921
Iteration 102/1000 | Loss: 0.00000921
Iteration 103/1000 | Loss: 0.00000921
Iteration 104/1000 | Loss: 0.00000921
Iteration 105/1000 | Loss: 0.00000921
Iteration 106/1000 | Loss: 0.00000921
Iteration 107/1000 | Loss: 0.00000920
Iteration 108/1000 | Loss: 0.00000920
Iteration 109/1000 | Loss: 0.00000920
Iteration 110/1000 | Loss: 0.00000920
Iteration 111/1000 | Loss: 0.00000920
Iteration 112/1000 | Loss: 0.00000920
Iteration 113/1000 | Loss: 0.00000920
Iteration 114/1000 | Loss: 0.00000920
Iteration 115/1000 | Loss: 0.00000920
Iteration 116/1000 | Loss: 0.00000920
Iteration 117/1000 | Loss: 0.00000920
Iteration 118/1000 | Loss: 0.00000920
Iteration 119/1000 | Loss: 0.00000920
Iteration 120/1000 | Loss: 0.00000919
Iteration 121/1000 | Loss: 0.00000919
Iteration 122/1000 | Loss: 0.00000919
Iteration 123/1000 | Loss: 0.00000919
Iteration 124/1000 | Loss: 0.00000919
Iteration 125/1000 | Loss: 0.00000919
Iteration 126/1000 | Loss: 0.00000919
Iteration 127/1000 | Loss: 0.00000919
Iteration 128/1000 | Loss: 0.00000919
Iteration 129/1000 | Loss: 0.00000919
Iteration 130/1000 | Loss: 0.00000919
Iteration 131/1000 | Loss: 0.00000919
Iteration 132/1000 | Loss: 0.00000919
Iteration 133/1000 | Loss: 0.00000919
Iteration 134/1000 | Loss: 0.00000919
Iteration 135/1000 | Loss: 0.00000919
Iteration 136/1000 | Loss: 0.00000919
Iteration 137/1000 | Loss: 0.00000918
Iteration 138/1000 | Loss: 0.00000918
Iteration 139/1000 | Loss: 0.00000918
Iteration 140/1000 | Loss: 0.00000918
Iteration 141/1000 | Loss: 0.00000918
Iteration 142/1000 | Loss: 0.00000918
Iteration 143/1000 | Loss: 0.00000918
Iteration 144/1000 | Loss: 0.00000918
Iteration 145/1000 | Loss: 0.00000918
Iteration 146/1000 | Loss: 0.00000918
Iteration 147/1000 | Loss: 0.00000918
Iteration 148/1000 | Loss: 0.00000918
Iteration 149/1000 | Loss: 0.00000918
Iteration 150/1000 | Loss: 0.00000918
Iteration 151/1000 | Loss: 0.00000918
Iteration 152/1000 | Loss: 0.00000918
Iteration 153/1000 | Loss: 0.00000918
Iteration 154/1000 | Loss: 0.00000918
Iteration 155/1000 | Loss: 0.00000918
Iteration 156/1000 | Loss: 0.00000917
Iteration 157/1000 | Loss: 0.00000917
Iteration 158/1000 | Loss: 0.00000917
Iteration 159/1000 | Loss: 0.00000917
Iteration 160/1000 | Loss: 0.00000917
Iteration 161/1000 | Loss: 0.00000917
Iteration 162/1000 | Loss: 0.00000917
Iteration 163/1000 | Loss: 0.00000917
Iteration 164/1000 | Loss: 0.00000917
Iteration 165/1000 | Loss: 0.00000917
Iteration 166/1000 | Loss: 0.00000917
Iteration 167/1000 | Loss: 0.00000917
Iteration 168/1000 | Loss: 0.00000917
Iteration 169/1000 | Loss: 0.00000917
Iteration 170/1000 | Loss: 0.00000917
Iteration 171/1000 | Loss: 0.00000916
Iteration 172/1000 | Loss: 0.00000916
Iteration 173/1000 | Loss: 0.00000916
Iteration 174/1000 | Loss: 0.00000916
Iteration 175/1000 | Loss: 0.00000916
Iteration 176/1000 | Loss: 0.00000916
Iteration 177/1000 | Loss: 0.00000916
Iteration 178/1000 | Loss: 0.00000916
Iteration 179/1000 | Loss: 0.00000916
Iteration 180/1000 | Loss: 0.00000916
Iteration 181/1000 | Loss: 0.00000916
Iteration 182/1000 | Loss: 0.00000916
Iteration 183/1000 | Loss: 0.00000916
Iteration 184/1000 | Loss: 0.00000916
Iteration 185/1000 | Loss: 0.00000916
Iteration 186/1000 | Loss: 0.00000916
Iteration 187/1000 | Loss: 0.00000916
Iteration 188/1000 | Loss: 0.00000916
Iteration 189/1000 | Loss: 0.00000916
Iteration 190/1000 | Loss: 0.00000916
Iteration 191/1000 | Loss: 0.00000916
Iteration 192/1000 | Loss: 0.00000916
Iteration 193/1000 | Loss: 0.00000916
Iteration 194/1000 | Loss: 0.00000915
Iteration 195/1000 | Loss: 0.00000915
Iteration 196/1000 | Loss: 0.00000915
Iteration 197/1000 | Loss: 0.00000915
Iteration 198/1000 | Loss: 0.00000915
Iteration 199/1000 | Loss: 0.00000915
Iteration 200/1000 | Loss: 0.00000915
Iteration 201/1000 | Loss: 0.00000915
Iteration 202/1000 | Loss: 0.00000915
Iteration 203/1000 | Loss: 0.00000915
Iteration 204/1000 | Loss: 0.00000915
Iteration 205/1000 | Loss: 0.00000915
Iteration 206/1000 | Loss: 0.00000915
Iteration 207/1000 | Loss: 0.00000915
Iteration 208/1000 | Loss: 0.00000915
Iteration 209/1000 | Loss: 0.00000915
Iteration 210/1000 | Loss: 0.00000915
Iteration 211/1000 | Loss: 0.00000915
Iteration 212/1000 | Loss: 0.00000915
Iteration 213/1000 | Loss: 0.00000915
Iteration 214/1000 | Loss: 0.00000915
Iteration 215/1000 | Loss: 0.00000915
Iteration 216/1000 | Loss: 0.00000915
Iteration 217/1000 | Loss: 0.00000915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.14508018468041e-06, 9.14508018468041e-06, 9.14508018468041e-06, 9.14508018468041e-06, 9.14508018468041e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.14508018468041e-06

Optimization complete. Final v2v error: 2.541532278060913 mm

Highest mean error: 3.154829740524292 mm for frame 56

Lowest mean error: 2.1434199810028076 mm for frame 34

Saving results

Total time: 38.02746057510376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436089
Iteration 2/25 | Loss: 0.00076780
Iteration 3/25 | Loss: 0.00060255
Iteration 4/25 | Loss: 0.00057594
Iteration 5/25 | Loss: 0.00056628
Iteration 6/25 | Loss: 0.00056486
Iteration 7/25 | Loss: 0.00056484
Iteration 8/25 | Loss: 0.00056484
Iteration 9/25 | Loss: 0.00056484
Iteration 10/25 | Loss: 0.00056484
Iteration 11/25 | Loss: 0.00056484
Iteration 12/25 | Loss: 0.00056484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005648430087603629, 0.0005648430087603629, 0.0005648430087603629, 0.0005648430087603629, 0.0005648430087603629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005648430087603629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36172891
Iteration 2/25 | Loss: 0.00013301
Iteration 3/25 | Loss: 0.00013301
Iteration 4/25 | Loss: 0.00013301
Iteration 5/25 | Loss: 0.00013301
Iteration 6/25 | Loss: 0.00013301
Iteration 7/25 | Loss: 0.00013301
Iteration 8/25 | Loss: 0.00013301
Iteration 9/25 | Loss: 0.00013301
Iteration 10/25 | Loss: 0.00013301
Iteration 11/25 | Loss: 0.00013301
Iteration 12/25 | Loss: 0.00013301
Iteration 13/25 | Loss: 0.00013301
Iteration 14/25 | Loss: 0.00013301
Iteration 15/25 | Loss: 0.00013301
Iteration 16/25 | Loss: 0.00013301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00013301083527039737, 0.00013301083527039737, 0.00013301083527039737, 0.00013301083527039737, 0.00013301083527039737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013301083527039737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013301
Iteration 2/1000 | Loss: 0.00002501
Iteration 3/1000 | Loss: 0.00002143
Iteration 4/1000 | Loss: 0.00002056
Iteration 5/1000 | Loss: 0.00001968
Iteration 6/1000 | Loss: 0.00001907
Iteration 7/1000 | Loss: 0.00001870
Iteration 8/1000 | Loss: 0.00001854
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001830
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001819
Iteration 13/1000 | Loss: 0.00001818
Iteration 14/1000 | Loss: 0.00001818
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001806
Iteration 18/1000 | Loss: 0.00001806
Iteration 19/1000 | Loss: 0.00001805
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001803
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001800
Iteration 25/1000 | Loss: 0.00001800
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001796
Iteration 28/1000 | Loss: 0.00001795
Iteration 29/1000 | Loss: 0.00001794
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001788
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001785
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00001781
Iteration 40/1000 | Loss: 0.00001781
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001781
Iteration 43/1000 | Loss: 0.00001781
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001780
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001778
Iteration 52/1000 | Loss: 0.00001778
Iteration 53/1000 | Loss: 0.00001778
Iteration 54/1000 | Loss: 0.00001778
Iteration 55/1000 | Loss: 0.00001778
Iteration 56/1000 | Loss: 0.00001778
Iteration 57/1000 | Loss: 0.00001778
Iteration 58/1000 | Loss: 0.00001778
Iteration 59/1000 | Loss: 0.00001778
Iteration 60/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.7775788364815526e-05, 1.7775788364815526e-05, 1.7775788364815526e-05, 1.7775788364815526e-05, 1.7775788364815526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7775788364815526e-05

Optimization complete. Final v2v error: 3.580152750015259 mm

Highest mean error: 4.080699443817139 mm for frame 198

Lowest mean error: 3.2309930324554443 mm for frame 118

Saving results

Total time: 33.83014369010925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456884
Iteration 2/25 | Loss: 0.00086641
Iteration 3/25 | Loss: 0.00076056
Iteration 4/25 | Loss: 0.00072759
Iteration 5/25 | Loss: 0.00072239
Iteration 6/25 | Loss: 0.00072136
Iteration 7/25 | Loss: 0.00072136
Iteration 8/25 | Loss: 0.00072136
Iteration 9/25 | Loss: 0.00072136
Iteration 10/25 | Loss: 0.00072136
Iteration 11/25 | Loss: 0.00072136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007213627104647458, 0.0007213627104647458, 0.0007213627104647458, 0.0007213627104647458, 0.0007213627104647458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007213627104647458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20760357
Iteration 2/25 | Loss: 0.00115005
Iteration 3/25 | Loss: 0.00115004
Iteration 4/25 | Loss: 0.00115004
Iteration 5/25 | Loss: 0.00115004
Iteration 6/25 | Loss: 0.00115004
Iteration 7/25 | Loss: 0.00115004
Iteration 8/25 | Loss: 0.00115004
Iteration 9/25 | Loss: 0.00115004
Iteration 10/25 | Loss: 0.00115004
Iteration 11/25 | Loss: 0.00115004
Iteration 12/25 | Loss: 0.00115004
Iteration 13/25 | Loss: 0.00115004
Iteration 14/25 | Loss: 0.00115004
Iteration 15/25 | Loss: 0.00115004
Iteration 16/25 | Loss: 0.00115004
Iteration 17/25 | Loss: 0.00115004
Iteration 18/25 | Loss: 0.00115004
Iteration 19/25 | Loss: 0.00115004
Iteration 20/25 | Loss: 0.00115004
Iteration 21/25 | Loss: 0.00115004
Iteration 22/25 | Loss: 0.00115004
Iteration 23/25 | Loss: 0.00115004
Iteration 24/25 | Loss: 0.00115004
Iteration 25/25 | Loss: 0.00115004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115004
Iteration 2/1000 | Loss: 0.00003940
Iteration 3/1000 | Loss: 0.00002501
Iteration 4/1000 | Loss: 0.00002312
Iteration 5/1000 | Loss: 0.00002180
Iteration 6/1000 | Loss: 0.00002114
Iteration 7/1000 | Loss: 0.00002043
Iteration 8/1000 | Loss: 0.00001993
Iteration 9/1000 | Loss: 0.00001957
Iteration 10/1000 | Loss: 0.00001949
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001906
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001896
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001893
Iteration 20/1000 | Loss: 0.00001893
Iteration 21/1000 | Loss: 0.00001892
Iteration 22/1000 | Loss: 0.00001892
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001889
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001886
Iteration 29/1000 | Loss: 0.00001883
Iteration 30/1000 | Loss: 0.00001883
Iteration 31/1000 | Loss: 0.00001883
Iteration 32/1000 | Loss: 0.00001883
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00001883
Iteration 35/1000 | Loss: 0.00001883
Iteration 36/1000 | Loss: 0.00001883
Iteration 37/1000 | Loss: 0.00001883
Iteration 38/1000 | Loss: 0.00001882
Iteration 39/1000 | Loss: 0.00001882
Iteration 40/1000 | Loss: 0.00001882
Iteration 41/1000 | Loss: 0.00001882
Iteration 42/1000 | Loss: 0.00001879
Iteration 43/1000 | Loss: 0.00001878
Iteration 44/1000 | Loss: 0.00001878
Iteration 45/1000 | Loss: 0.00001878
Iteration 46/1000 | Loss: 0.00001878
Iteration 47/1000 | Loss: 0.00001878
Iteration 48/1000 | Loss: 0.00001877
Iteration 49/1000 | Loss: 0.00001877
Iteration 50/1000 | Loss: 0.00001877
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001874
Iteration 62/1000 | Loss: 0.00001874
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001874
Iteration 65/1000 | Loss: 0.00001874
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001874
Iteration 68/1000 | Loss: 0.00001874
Iteration 69/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.873987821454648e-05, 1.873987821454648e-05, 1.873987821454648e-05, 1.873987821454648e-05, 1.873987821454648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.873987821454648e-05

Optimization complete. Final v2v error: 3.6689491271972656 mm

Highest mean error: 4.221033096313477 mm for frame 29

Lowest mean error: 3.2434706687927246 mm for frame 237

Saving results

Total time: 35.22632908821106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849037
Iteration 2/25 | Loss: 0.00087797
Iteration 3/25 | Loss: 0.00068642
Iteration 4/25 | Loss: 0.00066482
Iteration 5/25 | Loss: 0.00065889
Iteration 6/25 | Loss: 0.00065733
Iteration 7/25 | Loss: 0.00065694
Iteration 8/25 | Loss: 0.00065694
Iteration 9/25 | Loss: 0.00065694
Iteration 10/25 | Loss: 0.00065694
Iteration 11/25 | Loss: 0.00065694
Iteration 12/25 | Loss: 0.00065694
Iteration 13/25 | Loss: 0.00065694
Iteration 14/25 | Loss: 0.00065694
Iteration 15/25 | Loss: 0.00065694
Iteration 16/25 | Loss: 0.00065694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006569405668415129, 0.0006569405668415129, 0.0006569405668415129, 0.0006569405668415129, 0.0006569405668415129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006569405668415129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20314610
Iteration 2/25 | Loss: 0.00123281
Iteration 3/25 | Loss: 0.00123281
Iteration 4/25 | Loss: 0.00123281
Iteration 5/25 | Loss: 0.00123281
Iteration 6/25 | Loss: 0.00123281
Iteration 7/25 | Loss: 0.00123281
Iteration 8/25 | Loss: 0.00123281
Iteration 9/25 | Loss: 0.00123281
Iteration 10/25 | Loss: 0.00123281
Iteration 11/25 | Loss: 0.00123281
Iteration 12/25 | Loss: 0.00123281
Iteration 13/25 | Loss: 0.00123281
Iteration 14/25 | Loss: 0.00123281
Iteration 15/25 | Loss: 0.00123281
Iteration 16/25 | Loss: 0.00123281
Iteration 17/25 | Loss: 0.00123281
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001232811831869185, 0.001232811831869185, 0.001232811831869185, 0.001232811831869185, 0.001232811831869185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001232811831869185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123281
Iteration 2/1000 | Loss: 0.00002685
Iteration 3/1000 | Loss: 0.00001676
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001275
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001215
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001181
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001177
Iteration 21/1000 | Loss: 0.00001177
Iteration 22/1000 | Loss: 0.00001175
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001173
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001172
Iteration 29/1000 | Loss: 0.00001172
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001171
Iteration 32/1000 | Loss: 0.00001170
Iteration 33/1000 | Loss: 0.00001169
Iteration 34/1000 | Loss: 0.00001169
Iteration 35/1000 | Loss: 0.00001169
Iteration 36/1000 | Loss: 0.00001168
Iteration 37/1000 | Loss: 0.00001168
Iteration 38/1000 | Loss: 0.00001167
Iteration 39/1000 | Loss: 0.00001167
Iteration 40/1000 | Loss: 0.00001167
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001160
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001151
Iteration 72/1000 | Loss: 0.00001151
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001150
Iteration 78/1000 | Loss: 0.00001150
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001148
Iteration 91/1000 | Loss: 0.00001148
Iteration 92/1000 | Loss: 0.00001148
Iteration 93/1000 | Loss: 0.00001148
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001148
Iteration 101/1000 | Loss: 0.00001148
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001145
Iteration 123/1000 | Loss: 0.00001145
Iteration 124/1000 | Loss: 0.00001145
Iteration 125/1000 | Loss: 0.00001145
Iteration 126/1000 | Loss: 0.00001145
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001144
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001144
Iteration 132/1000 | Loss: 0.00001144
Iteration 133/1000 | Loss: 0.00001144
Iteration 134/1000 | Loss: 0.00001144
Iteration 135/1000 | Loss: 0.00001144
Iteration 136/1000 | Loss: 0.00001144
Iteration 137/1000 | Loss: 0.00001144
Iteration 138/1000 | Loss: 0.00001144
Iteration 139/1000 | Loss: 0.00001144
Iteration 140/1000 | Loss: 0.00001144
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Iteration 155/1000 | Loss: 0.00001144
Iteration 156/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.1444054507592227e-05, 1.1444054507592227e-05, 1.1444054507592227e-05, 1.1444054507592227e-05, 1.1444054507592227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1444054507592227e-05

Optimization complete. Final v2v error: 2.8876631259918213 mm

Highest mean error: 3.240440845489502 mm for frame 69

Lowest mean error: 2.5305542945861816 mm for frame 158

Saving results

Total time: 37.03286600112915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085295
Iteration 2/25 | Loss: 0.01085295
Iteration 3/25 | Loss: 0.01085295
Iteration 4/25 | Loss: 0.01085295
Iteration 5/25 | Loss: 0.01085295
Iteration 6/25 | Loss: 0.01085295
Iteration 7/25 | Loss: 0.01085295
Iteration 8/25 | Loss: 0.01085295
Iteration 9/25 | Loss: 0.01085295
Iteration 10/25 | Loss: 0.01085295
Iteration 11/25 | Loss: 0.01085295
Iteration 12/25 | Loss: 0.01085295
Iteration 13/25 | Loss: 0.01085295
Iteration 14/25 | Loss: 0.01085295
Iteration 15/25 | Loss: 0.01085295
Iteration 16/25 | Loss: 0.01085294
Iteration 17/25 | Loss: 0.01085294
Iteration 18/25 | Loss: 0.01085294
Iteration 19/25 | Loss: 0.01085294
Iteration 20/25 | Loss: 0.01085294
Iteration 21/25 | Loss: 0.01085294
Iteration 22/25 | Loss: 0.01085294
Iteration 23/25 | Loss: 0.01085294
Iteration 24/25 | Loss: 0.01085294
Iteration 25/25 | Loss: 0.01085294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42150903
Iteration 2/25 | Loss: 0.09186257
Iteration 3/25 | Loss: 0.09081817
Iteration 4/25 | Loss: 0.09077124
Iteration 5/25 | Loss: 0.09077122
Iteration 6/25 | Loss: 0.09077121
Iteration 7/25 | Loss: 0.09077121
Iteration 8/25 | Loss: 0.09077121
Iteration 9/25 | Loss: 0.09077121
Iteration 10/25 | Loss: 0.09077121
Iteration 11/25 | Loss: 0.09077121
Iteration 12/25 | Loss: 0.09077121
Iteration 13/25 | Loss: 0.09077121
Iteration 14/25 | Loss: 0.09077121
Iteration 15/25 | Loss: 0.09077121
Iteration 16/25 | Loss: 0.09077121
Iteration 17/25 | Loss: 0.09077121
Iteration 18/25 | Loss: 0.09077121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.09077121317386627, 0.09077121317386627, 0.09077121317386627, 0.09077121317386627, 0.09077121317386627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09077121317386627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09077121
Iteration 2/1000 | Loss: 0.00192724
Iteration 3/1000 | Loss: 0.00241554
Iteration 4/1000 | Loss: 0.00083300
Iteration 5/1000 | Loss: 0.00127119
Iteration 6/1000 | Loss: 0.00026964
Iteration 7/1000 | Loss: 0.00014024
Iteration 8/1000 | Loss: 0.00079598
Iteration 9/1000 | Loss: 0.00007119
Iteration 10/1000 | Loss: 0.00069748
Iteration 11/1000 | Loss: 0.00010823
Iteration 12/1000 | Loss: 0.00009920
Iteration 13/1000 | Loss: 0.00024037
Iteration 14/1000 | Loss: 0.00004165
Iteration 15/1000 | Loss: 0.00029292
Iteration 16/1000 | Loss: 0.00003569
Iteration 17/1000 | Loss: 0.00009872
Iteration 18/1000 | Loss: 0.00060761
Iteration 19/1000 | Loss: 0.00007058
Iteration 20/1000 | Loss: 0.00002953
Iteration 21/1000 | Loss: 0.00025549
Iteration 22/1000 | Loss: 0.00036549
Iteration 23/1000 | Loss: 0.00002695
Iteration 24/1000 | Loss: 0.00002560
Iteration 25/1000 | Loss: 0.00025947
Iteration 26/1000 | Loss: 0.00002604
Iteration 27/1000 | Loss: 0.00002394
Iteration 28/1000 | Loss: 0.00002434
Iteration 29/1000 | Loss: 0.00002476
Iteration 30/1000 | Loss: 0.00002461
Iteration 31/1000 | Loss: 0.00002232
Iteration 32/1000 | Loss: 0.00002313
Iteration 33/1000 | Loss: 0.00059916
Iteration 34/1000 | Loss: 0.00002294
Iteration 35/1000 | Loss: 0.00002263
Iteration 36/1000 | Loss: 0.00002130
Iteration 37/1000 | Loss: 0.00020581
Iteration 38/1000 | Loss: 0.00002170
Iteration 39/1000 | Loss: 0.00002326
Iteration 40/1000 | Loss: 0.00002151
Iteration 41/1000 | Loss: 0.00002006
Iteration 42/1000 | Loss: 0.00002035
Iteration 43/1000 | Loss: 0.00003707
Iteration 44/1000 | Loss: 0.00003661
Iteration 45/1000 | Loss: 0.00002048
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001916
Iteration 48/1000 | Loss: 0.00001913
Iteration 49/1000 | Loss: 0.00001911
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001905
Iteration 53/1000 | Loss: 0.00003621
Iteration 54/1000 | Loss: 0.00003091
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00003618
Iteration 58/1000 | Loss: 0.00002878
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00003608
Iteration 61/1000 | Loss: 0.00002714
Iteration 62/1000 | Loss: 0.00003603
Iteration 63/1000 | Loss: 0.00046504
Iteration 64/1000 | Loss: 0.00015659
Iteration 65/1000 | Loss: 0.00002112
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00003594
Iteration 68/1000 | Loss: 0.00002464
Iteration 69/1000 | Loss: 0.00003614
Iteration 70/1000 | Loss: 0.00004355
Iteration 71/1000 | Loss: 0.00004617
Iteration 72/1000 | Loss: 0.00002959
Iteration 73/1000 | Loss: 0.00006468
Iteration 74/1000 | Loss: 0.00003414
Iteration 75/1000 | Loss: 0.00003510
Iteration 76/1000 | Loss: 0.00003635
Iteration 77/1000 | Loss: 0.00003816
Iteration 78/1000 | Loss: 0.00003302
Iteration 79/1000 | Loss: 0.00003526
Iteration 80/1000 | Loss: 0.00003286
Iteration 81/1000 | Loss: 0.00003331
Iteration 82/1000 | Loss: 0.00003209
Iteration 83/1000 | Loss: 0.00003340
Iteration 84/1000 | Loss: 0.00004311
Iteration 85/1000 | Loss: 0.00004802
Iteration 86/1000 | Loss: 0.00003342
Iteration 87/1000 | Loss: 0.00003235
Iteration 88/1000 | Loss: 0.00003994
Iteration 89/1000 | Loss: 0.00003034
Iteration 90/1000 | Loss: 0.00003962
Iteration 91/1000 | Loss: 0.00003113
Iteration 92/1000 | Loss: 0.00003809
Iteration 93/1000 | Loss: 0.00003158
Iteration 94/1000 | Loss: 0.00003925
Iteration 95/1000 | Loss: 0.00003934
Iteration 96/1000 | Loss: 0.00004193
Iteration 97/1000 | Loss: 0.00003986
Iteration 98/1000 | Loss: 0.00003844
Iteration 99/1000 | Loss: 0.00004774
Iteration 100/1000 | Loss: 0.00002324
Iteration 101/1000 | Loss: 0.00002018
Iteration 102/1000 | Loss: 0.00003155
Iteration 103/1000 | Loss: 0.00001917
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00002108
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001870
Iteration 114/1000 | Loss: 0.00001870
Iteration 115/1000 | Loss: 0.00001870
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001870
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001869
Iteration 122/1000 | Loss: 0.00001869
Iteration 123/1000 | Loss: 0.00001869
Iteration 124/1000 | Loss: 0.00001867
Iteration 125/1000 | Loss: 0.00001867
Iteration 126/1000 | Loss: 0.00001867
Iteration 127/1000 | Loss: 0.00001867
Iteration 128/1000 | Loss: 0.00001867
Iteration 129/1000 | Loss: 0.00001867
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001867
Iteration 132/1000 | Loss: 0.00001866
Iteration 133/1000 | Loss: 0.00001866
Iteration 134/1000 | Loss: 0.00001866
Iteration 135/1000 | Loss: 0.00001866
Iteration 136/1000 | Loss: 0.00001866
Iteration 137/1000 | Loss: 0.00001866
Iteration 138/1000 | Loss: 0.00001866
Iteration 139/1000 | Loss: 0.00001866
Iteration 140/1000 | Loss: 0.00001866
Iteration 141/1000 | Loss: 0.00001866
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001865
Iteration 144/1000 | Loss: 0.00001865
Iteration 145/1000 | Loss: 0.00001865
Iteration 146/1000 | Loss: 0.00001865
Iteration 147/1000 | Loss: 0.00001865
Iteration 148/1000 | Loss: 0.00001865
Iteration 149/1000 | Loss: 0.00001865
Iteration 150/1000 | Loss: 0.00001865
Iteration 151/1000 | Loss: 0.00001865
Iteration 152/1000 | Loss: 0.00001865
Iteration 153/1000 | Loss: 0.00001865
Iteration 154/1000 | Loss: 0.00001865
Iteration 155/1000 | Loss: 0.00001865
Iteration 156/1000 | Loss: 0.00001864
Iteration 157/1000 | Loss: 0.00001864
Iteration 158/1000 | Loss: 0.00001863
Iteration 159/1000 | Loss: 0.00001896
Iteration 160/1000 | Loss: 0.00001872
Iteration 161/1000 | Loss: 0.00001864
Iteration 162/1000 | Loss: 0.00001864
Iteration 163/1000 | Loss: 0.00001863
Iteration 164/1000 | Loss: 0.00001862
Iteration 165/1000 | Loss: 0.00001862
Iteration 166/1000 | Loss: 0.00001862
Iteration 167/1000 | Loss: 0.00001862
Iteration 168/1000 | Loss: 0.00001862
Iteration 169/1000 | Loss: 0.00001862
Iteration 170/1000 | Loss: 0.00001862
Iteration 171/1000 | Loss: 0.00001862
Iteration 172/1000 | Loss: 0.00001862
Iteration 173/1000 | Loss: 0.00001862
Iteration 174/1000 | Loss: 0.00001861
Iteration 175/1000 | Loss: 0.00001861
Iteration 176/1000 | Loss: 0.00001861
Iteration 177/1000 | Loss: 0.00001861
Iteration 178/1000 | Loss: 0.00001861
Iteration 179/1000 | Loss: 0.00001861
Iteration 180/1000 | Loss: 0.00001861
Iteration 181/1000 | Loss: 0.00001861
Iteration 182/1000 | Loss: 0.00001861
Iteration 183/1000 | Loss: 0.00001860
Iteration 184/1000 | Loss: 0.00001860
Iteration 185/1000 | Loss: 0.00001859
Iteration 186/1000 | Loss: 0.00001859
Iteration 187/1000 | Loss: 0.00001859
Iteration 188/1000 | Loss: 0.00001859
Iteration 189/1000 | Loss: 0.00001859
Iteration 190/1000 | Loss: 0.00001858
Iteration 191/1000 | Loss: 0.00001889
Iteration 192/1000 | Loss: 0.00001868
Iteration 193/1000 | Loss: 0.00001863
Iteration 194/1000 | Loss: 0.00001861
Iteration 195/1000 | Loss: 0.00001860
Iteration 196/1000 | Loss: 0.00001859
Iteration 197/1000 | Loss: 0.00001857
Iteration 198/1000 | Loss: 0.00001857
Iteration 199/1000 | Loss: 0.00001857
Iteration 200/1000 | Loss: 0.00001857
Iteration 201/1000 | Loss: 0.00001857
Iteration 202/1000 | Loss: 0.00001857
Iteration 203/1000 | Loss: 0.00001857
Iteration 204/1000 | Loss: 0.00001857
Iteration 205/1000 | Loss: 0.00001882
Iteration 206/1000 | Loss: 0.00001882
Iteration 207/1000 | Loss: 0.00001947
Iteration 208/1000 | Loss: 0.00001884
Iteration 209/1000 | Loss: 0.00001907
Iteration 210/1000 | Loss: 0.00001882
Iteration 211/1000 | Loss: 0.00001884
Iteration 212/1000 | Loss: 0.00001900
Iteration 213/1000 | Loss: 0.00001839
Iteration 214/1000 | Loss: 0.00001835
Iteration 215/1000 | Loss: 0.00001860
Iteration 216/1000 | Loss: 0.00001841
Iteration 217/1000 | Loss: 0.00001832
Iteration 218/1000 | Loss: 0.00001832
Iteration 219/1000 | Loss: 0.00001832
Iteration 220/1000 | Loss: 0.00001832
Iteration 221/1000 | Loss: 0.00001832
Iteration 222/1000 | Loss: 0.00001832
Iteration 223/1000 | Loss: 0.00001832
Iteration 224/1000 | Loss: 0.00001832
Iteration 225/1000 | Loss: 0.00001832
Iteration 226/1000 | Loss: 0.00001832
Iteration 227/1000 | Loss: 0.00001832
Iteration 228/1000 | Loss: 0.00001832
Iteration 229/1000 | Loss: 0.00001832
Iteration 230/1000 | Loss: 0.00001831
Iteration 231/1000 | Loss: 0.00001831
Iteration 232/1000 | Loss: 0.00001831
Iteration 233/1000 | Loss: 0.00001831
Iteration 234/1000 | Loss: 0.00001831
Iteration 235/1000 | Loss: 0.00001831
Iteration 236/1000 | Loss: 0.00001831
Iteration 237/1000 | Loss: 0.00001831
Iteration 238/1000 | Loss: 0.00001831
Iteration 239/1000 | Loss: 0.00001831
Iteration 240/1000 | Loss: 0.00001831
Iteration 241/1000 | Loss: 0.00001831
Iteration 242/1000 | Loss: 0.00001831
Iteration 243/1000 | Loss: 0.00001831
Iteration 244/1000 | Loss: 0.00001831
Iteration 245/1000 | Loss: 0.00001831
Iteration 246/1000 | Loss: 0.00001831
Iteration 247/1000 | Loss: 0.00001831
Iteration 248/1000 | Loss: 0.00001831
Iteration 249/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.8312899555894546e-05, 1.8312899555894546e-05, 1.8312899555894546e-05, 1.8312899555894546e-05, 1.8312899555894546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8312899555894546e-05

Optimization complete. Final v2v error: 3.137946367263794 mm

Highest mean error: 20.62885856628418 mm for frame 260

Lowest mean error: 2.4826385974884033 mm for frame 266

Saving results

Total time: 192.44017338752747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910922
Iteration 2/25 | Loss: 0.00205629
Iteration 3/25 | Loss: 0.00146698
Iteration 4/25 | Loss: 0.00135446
Iteration 5/25 | Loss: 0.00130809
Iteration 6/25 | Loss: 0.00134533
Iteration 7/25 | Loss: 0.00120128
Iteration 8/25 | Loss: 0.00116554
Iteration 9/25 | Loss: 0.00110640
Iteration 10/25 | Loss: 0.00103893
Iteration 11/25 | Loss: 0.00100393
Iteration 12/25 | Loss: 0.00098254
Iteration 13/25 | Loss: 0.00097162
Iteration 14/25 | Loss: 0.00096383
Iteration 15/25 | Loss: 0.00096000
Iteration 16/25 | Loss: 0.00094502
Iteration 17/25 | Loss: 0.00093507
Iteration 18/25 | Loss: 0.00092845
Iteration 19/25 | Loss: 0.00092221
Iteration 20/25 | Loss: 0.00092742
Iteration 21/25 | Loss: 0.00092438
Iteration 22/25 | Loss: 0.00092147
Iteration 23/25 | Loss: 0.00092136
Iteration 24/25 | Loss: 0.00092015
Iteration 25/25 | Loss: 0.00092021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20175028
Iteration 2/25 | Loss: 0.00146630
Iteration 3/25 | Loss: 0.00146628
Iteration 4/25 | Loss: 0.00146628
Iteration 5/25 | Loss: 0.00146628
Iteration 6/25 | Loss: 0.00146627
Iteration 7/25 | Loss: 0.00146627
Iteration 8/25 | Loss: 0.00146627
Iteration 9/25 | Loss: 0.00146627
Iteration 10/25 | Loss: 0.00146627
Iteration 11/25 | Loss: 0.00146627
Iteration 12/25 | Loss: 0.00146627
Iteration 13/25 | Loss: 0.00146627
Iteration 14/25 | Loss: 0.00146627
Iteration 15/25 | Loss: 0.00146627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001466273795813322, 0.001466273795813322, 0.001466273795813322, 0.001466273795813322, 0.001466273795813322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001466273795813322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146627
Iteration 2/1000 | Loss: 0.00016632
Iteration 3/1000 | Loss: 0.00010785
Iteration 4/1000 | Loss: 0.00012660
Iteration 5/1000 | Loss: 0.00016874
Iteration 6/1000 | Loss: 0.00018148
Iteration 7/1000 | Loss: 0.00019699
Iteration 8/1000 | Loss: 0.00022687
Iteration 9/1000 | Loss: 0.00022227
Iteration 10/1000 | Loss: 0.00022922
Iteration 11/1000 | Loss: 0.00019438
Iteration 12/1000 | Loss: 0.00017611
Iteration 13/1000 | Loss: 0.00022585
Iteration 14/1000 | Loss: 0.00013222
Iteration 15/1000 | Loss: 0.00015136
Iteration 16/1000 | Loss: 0.00017546
Iteration 17/1000 | Loss: 0.00017711
Iteration 18/1000 | Loss: 0.00016911
Iteration 19/1000 | Loss: 0.00015441
Iteration 20/1000 | Loss: 0.00016060
Iteration 21/1000 | Loss: 0.00013940
Iteration 22/1000 | Loss: 0.00012924
Iteration 23/1000 | Loss: 0.00012598
Iteration 24/1000 | Loss: 0.00013604
Iteration 25/1000 | Loss: 0.00013447
Iteration 26/1000 | Loss: 0.00024820
Iteration 27/1000 | Loss: 0.00016923
Iteration 28/1000 | Loss: 0.00007843
Iteration 29/1000 | Loss: 0.00010993
Iteration 30/1000 | Loss: 0.00013583
Iteration 31/1000 | Loss: 0.00008640
Iteration 32/1000 | Loss: 0.00015123
Iteration 33/1000 | Loss: 0.00008740
Iteration 34/1000 | Loss: 0.00009847
Iteration 35/1000 | Loss: 0.00010258
Iteration 36/1000 | Loss: 0.00011724
Iteration 37/1000 | Loss: 0.00016715
Iteration 38/1000 | Loss: 0.00012615
Iteration 39/1000 | Loss: 0.00015226
Iteration 40/1000 | Loss: 0.00014462
Iteration 41/1000 | Loss: 0.00009064
Iteration 42/1000 | Loss: 0.00011484
Iteration 43/1000 | Loss: 0.00012019
Iteration 44/1000 | Loss: 0.00012063
Iteration 45/1000 | Loss: 0.00011773
Iteration 46/1000 | Loss: 0.00006766
Iteration 47/1000 | Loss: 0.00008238
Iteration 48/1000 | Loss: 0.00007626
Iteration 49/1000 | Loss: 0.00010120
Iteration 50/1000 | Loss: 0.00013869
Iteration 51/1000 | Loss: 0.00017758
Iteration 52/1000 | Loss: 0.00014321
Iteration 53/1000 | Loss: 0.00013394
Iteration 54/1000 | Loss: 0.00013686
Iteration 55/1000 | Loss: 0.00013612
Iteration 56/1000 | Loss: 0.00017249
Iteration 57/1000 | Loss: 0.00014633
Iteration 58/1000 | Loss: 0.00012869
Iteration 59/1000 | Loss: 0.00015229
Iteration 60/1000 | Loss: 0.00015340
Iteration 61/1000 | Loss: 0.00012521
Iteration 62/1000 | Loss: 0.00013361
Iteration 63/1000 | Loss: 0.00013256
Iteration 64/1000 | Loss: 0.00013979
Iteration 65/1000 | Loss: 0.00014728
Iteration 66/1000 | Loss: 0.00005702
Iteration 67/1000 | Loss: 0.00008681
Iteration 68/1000 | Loss: 0.00005373
Iteration 69/1000 | Loss: 0.00006239
Iteration 70/1000 | Loss: 0.00005486
Iteration 71/1000 | Loss: 0.00005029
Iteration 72/1000 | Loss: 0.00005942
Iteration 73/1000 | Loss: 0.00005954
Iteration 74/1000 | Loss: 0.00006162
Iteration 75/1000 | Loss: 0.00005825
Iteration 76/1000 | Loss: 0.00006386
Iteration 77/1000 | Loss: 0.00005134
Iteration 78/1000 | Loss: 0.00004570
Iteration 79/1000 | Loss: 0.00003244
Iteration 80/1000 | Loss: 0.00004087
Iteration 81/1000 | Loss: 0.00003926
Iteration 82/1000 | Loss: 0.00003948
Iteration 83/1000 | Loss: 0.00004086
Iteration 84/1000 | Loss: 0.00004182
Iteration 85/1000 | Loss: 0.00004173
Iteration 86/1000 | Loss: 0.00003715
Iteration 87/1000 | Loss: 0.00010359
Iteration 88/1000 | Loss: 0.00005625
Iteration 89/1000 | Loss: 0.00005582
Iteration 90/1000 | Loss: 0.00003934
Iteration 91/1000 | Loss: 0.00003352
Iteration 92/1000 | Loss: 0.00003127
Iteration 93/1000 | Loss: 0.00002904
Iteration 94/1000 | Loss: 0.00002793
Iteration 95/1000 | Loss: 0.00002715
Iteration 96/1000 | Loss: 0.00002671
Iteration 97/1000 | Loss: 0.00002635
Iteration 98/1000 | Loss: 0.00002611
Iteration 99/1000 | Loss: 0.00003716
Iteration 100/1000 | Loss: 0.00003330
Iteration 101/1000 | Loss: 0.00002837
Iteration 102/1000 | Loss: 0.00002650
Iteration 103/1000 | Loss: 0.00002595
Iteration 104/1000 | Loss: 0.00002579
Iteration 105/1000 | Loss: 0.00003266
Iteration 106/1000 | Loss: 0.00003265
Iteration 107/1000 | Loss: 0.00003854
Iteration 108/1000 | Loss: 0.00004682
Iteration 109/1000 | Loss: 0.00003905
Iteration 110/1000 | Loss: 0.00003299
Iteration 111/1000 | Loss: 0.00003052
Iteration 112/1000 | Loss: 0.00002924
Iteration 113/1000 | Loss: 0.00002775
Iteration 114/1000 | Loss: 0.00002557
Iteration 115/1000 | Loss: 0.00002499
Iteration 116/1000 | Loss: 0.00002481
Iteration 117/1000 | Loss: 0.00002481
Iteration 118/1000 | Loss: 0.00002480
Iteration 119/1000 | Loss: 0.00002477
Iteration 120/1000 | Loss: 0.00002477
Iteration 121/1000 | Loss: 0.00002476
Iteration 122/1000 | Loss: 0.00002476
Iteration 123/1000 | Loss: 0.00002469
Iteration 124/1000 | Loss: 0.00002469
Iteration 125/1000 | Loss: 0.00002469
Iteration 126/1000 | Loss: 0.00002468
Iteration 127/1000 | Loss: 0.00002468
Iteration 128/1000 | Loss: 0.00002468
Iteration 129/1000 | Loss: 0.00002468
Iteration 130/1000 | Loss: 0.00002467
Iteration 131/1000 | Loss: 0.00002467
Iteration 132/1000 | Loss: 0.00002467
Iteration 133/1000 | Loss: 0.00002467
Iteration 134/1000 | Loss: 0.00002467
Iteration 135/1000 | Loss: 0.00002467
Iteration 136/1000 | Loss: 0.00002467
Iteration 137/1000 | Loss: 0.00002467
Iteration 138/1000 | Loss: 0.00002467
Iteration 139/1000 | Loss: 0.00002467
Iteration 140/1000 | Loss: 0.00002467
Iteration 141/1000 | Loss: 0.00002467
Iteration 142/1000 | Loss: 0.00002467
Iteration 143/1000 | Loss: 0.00002467
Iteration 144/1000 | Loss: 0.00002467
Iteration 145/1000 | Loss: 0.00002467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.4673114239703864e-05, 2.4673114239703864e-05, 2.4673114239703864e-05, 2.4673114239703864e-05, 2.4673114239703864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4673114239703864e-05

Optimization complete. Final v2v error: 4.429079532623291 mm

Highest mean error: 5.570101261138916 mm for frame 110

Lowest mean error: 4.1671013832092285 mm for frame 92

Saving results

Total time: 239.0936987400055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412607
Iteration 2/25 | Loss: 0.00089080
Iteration 3/25 | Loss: 0.00078179
Iteration 4/25 | Loss: 0.00075436
Iteration 5/25 | Loss: 0.00074657
Iteration 6/25 | Loss: 0.00074460
Iteration 7/25 | Loss: 0.00074459
Iteration 8/25 | Loss: 0.00074459
Iteration 9/25 | Loss: 0.00074459
Iteration 10/25 | Loss: 0.00074459
Iteration 11/25 | Loss: 0.00074459
Iteration 12/25 | Loss: 0.00074459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007445868104696274, 0.0007445868104696274, 0.0007445868104696274, 0.0007445868104696274, 0.0007445868104696274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007445868104696274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.98568654
Iteration 2/25 | Loss: 0.00124525
Iteration 3/25 | Loss: 0.00124525
Iteration 4/25 | Loss: 0.00124525
Iteration 5/25 | Loss: 0.00124525
Iteration 6/25 | Loss: 0.00124525
Iteration 7/25 | Loss: 0.00124525
Iteration 8/25 | Loss: 0.00124525
Iteration 9/25 | Loss: 0.00124525
Iteration 10/25 | Loss: 0.00124525
Iteration 11/25 | Loss: 0.00124525
Iteration 12/25 | Loss: 0.00124525
Iteration 13/25 | Loss: 0.00124525
Iteration 14/25 | Loss: 0.00124525
Iteration 15/25 | Loss: 0.00124525
Iteration 16/25 | Loss: 0.00124525
Iteration 17/25 | Loss: 0.00124525
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001245246035978198, 0.001245246035978198, 0.001245246035978198, 0.001245246035978198, 0.001245246035978198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001245246035978198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124525
Iteration 2/1000 | Loss: 0.00004040
Iteration 3/1000 | Loss: 0.00003124
Iteration 4/1000 | Loss: 0.00002906
Iteration 5/1000 | Loss: 0.00002650
Iteration 6/1000 | Loss: 0.00002534
Iteration 7/1000 | Loss: 0.00002443
Iteration 8/1000 | Loss: 0.00002394
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00002328
Iteration 11/1000 | Loss: 0.00002316
Iteration 12/1000 | Loss: 0.00002314
Iteration 13/1000 | Loss: 0.00002296
Iteration 14/1000 | Loss: 0.00002280
Iteration 15/1000 | Loss: 0.00002280
Iteration 16/1000 | Loss: 0.00002266
Iteration 17/1000 | Loss: 0.00002266
Iteration 18/1000 | Loss: 0.00002265
Iteration 19/1000 | Loss: 0.00002264
Iteration 20/1000 | Loss: 0.00002263
Iteration 21/1000 | Loss: 0.00002261
Iteration 22/1000 | Loss: 0.00002261
Iteration 23/1000 | Loss: 0.00002260
Iteration 24/1000 | Loss: 0.00002260
Iteration 25/1000 | Loss: 0.00002260
Iteration 26/1000 | Loss: 0.00002260
Iteration 27/1000 | Loss: 0.00002260
Iteration 28/1000 | Loss: 0.00002260
Iteration 29/1000 | Loss: 0.00002259
Iteration 30/1000 | Loss: 0.00002259
Iteration 31/1000 | Loss: 0.00002259
Iteration 32/1000 | Loss: 0.00002258
Iteration 33/1000 | Loss: 0.00002258
Iteration 34/1000 | Loss: 0.00002258
Iteration 35/1000 | Loss: 0.00002258
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002257
Iteration 40/1000 | Loss: 0.00002257
Iteration 41/1000 | Loss: 0.00002257
Iteration 42/1000 | Loss: 0.00002257
Iteration 43/1000 | Loss: 0.00002257
Iteration 44/1000 | Loss: 0.00002257
Iteration 45/1000 | Loss: 0.00002257
Iteration 46/1000 | Loss: 0.00002257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [2.257097548863385e-05, 2.257097548863385e-05, 2.257097548863385e-05, 2.257097548863385e-05, 2.257097548863385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.257097548863385e-05

Optimization complete. Final v2v error: 4.069762706756592 mm

Highest mean error: 4.338959693908691 mm for frame 128

Lowest mean error: 3.836090326309204 mm for frame 146

Saving results

Total time: 33.75070595741272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497831
Iteration 2/25 | Loss: 0.00110045
Iteration 3/25 | Loss: 0.00078223
Iteration 4/25 | Loss: 0.00072452
Iteration 5/25 | Loss: 0.00071349
Iteration 6/25 | Loss: 0.00070879
Iteration 7/25 | Loss: 0.00070713
Iteration 8/25 | Loss: 0.00070661
Iteration 9/25 | Loss: 0.00070657
Iteration 10/25 | Loss: 0.00070657
Iteration 11/25 | Loss: 0.00070657
Iteration 12/25 | Loss: 0.00070657
Iteration 13/25 | Loss: 0.00070657
Iteration 14/25 | Loss: 0.00070657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007065657991915941, 0.0007065657991915941, 0.0007065657991915941, 0.0007065657991915941, 0.0007065657991915941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007065657991915941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21699023
Iteration 2/25 | Loss: 0.00122606
Iteration 3/25 | Loss: 0.00122606
Iteration 4/25 | Loss: 0.00122606
Iteration 5/25 | Loss: 0.00122606
Iteration 6/25 | Loss: 0.00122606
Iteration 7/25 | Loss: 0.00122606
Iteration 8/25 | Loss: 0.00122606
Iteration 9/25 | Loss: 0.00122606
Iteration 10/25 | Loss: 0.00122606
Iteration 11/25 | Loss: 0.00122606
Iteration 12/25 | Loss: 0.00122606
Iteration 13/25 | Loss: 0.00122606
Iteration 14/25 | Loss: 0.00122606
Iteration 15/25 | Loss: 0.00122606
Iteration 16/25 | Loss: 0.00122606
Iteration 17/25 | Loss: 0.00122606
Iteration 18/25 | Loss: 0.00122606
Iteration 19/25 | Loss: 0.00122606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012260574148967862, 0.0012260574148967862, 0.0012260574148967862, 0.0012260574148967862, 0.0012260574148967862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012260574148967862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122606
Iteration 2/1000 | Loss: 0.00003528
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00002087
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001934
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001800
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001751
Iteration 12/1000 | Loss: 0.00001730
Iteration 13/1000 | Loss: 0.00001722
Iteration 14/1000 | Loss: 0.00001721
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001716
Iteration 17/1000 | Loss: 0.00001713
Iteration 18/1000 | Loss: 0.00001712
Iteration 19/1000 | Loss: 0.00001711
Iteration 20/1000 | Loss: 0.00001711
Iteration 21/1000 | Loss: 0.00001706
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001696
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001694
Iteration 48/1000 | Loss: 0.00001694
Iteration 49/1000 | Loss: 0.00001694
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001688
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001687
Iteration 69/1000 | Loss: 0.00001686
Iteration 70/1000 | Loss: 0.00001686
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001686
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001683
Iteration 88/1000 | Loss: 0.00001683
Iteration 89/1000 | Loss: 0.00001683
Iteration 90/1000 | Loss: 0.00001683
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001682
Iteration 95/1000 | Loss: 0.00001682
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001681
Iteration 103/1000 | Loss: 0.00001681
Iteration 104/1000 | Loss: 0.00001681
Iteration 105/1000 | Loss: 0.00001681
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001681
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001680
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001680
Iteration 115/1000 | Loss: 0.00001680
Iteration 116/1000 | Loss: 0.00001680
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001680
Iteration 120/1000 | Loss: 0.00001680
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001679
Iteration 123/1000 | Loss: 0.00001679
Iteration 124/1000 | Loss: 0.00001679
Iteration 125/1000 | Loss: 0.00001679
Iteration 126/1000 | Loss: 0.00001679
Iteration 127/1000 | Loss: 0.00001679
Iteration 128/1000 | Loss: 0.00001679
Iteration 129/1000 | Loss: 0.00001679
Iteration 130/1000 | Loss: 0.00001679
Iteration 131/1000 | Loss: 0.00001679
Iteration 132/1000 | Loss: 0.00001679
Iteration 133/1000 | Loss: 0.00001679
Iteration 134/1000 | Loss: 0.00001679
Iteration 135/1000 | Loss: 0.00001679
Iteration 136/1000 | Loss: 0.00001679
Iteration 137/1000 | Loss: 0.00001679
Iteration 138/1000 | Loss: 0.00001679
Iteration 139/1000 | Loss: 0.00001679
Iteration 140/1000 | Loss: 0.00001679
Iteration 141/1000 | Loss: 0.00001679
Iteration 142/1000 | Loss: 0.00001679
Iteration 143/1000 | Loss: 0.00001679
Iteration 144/1000 | Loss: 0.00001679
Iteration 145/1000 | Loss: 0.00001679
Iteration 146/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.6791816960903816e-05, 1.6791816960903816e-05, 1.6791816960903816e-05, 1.6791816960903816e-05, 1.6791816960903816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6791816960903816e-05

Optimization complete. Final v2v error: 3.31034255027771 mm

Highest mean error: 4.889700889587402 mm for frame 66

Lowest mean error: 2.7990057468414307 mm for frame 102

Saving results

Total time: 40.79532170295715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394047
Iteration 2/25 | Loss: 0.00094654
Iteration 3/25 | Loss: 0.00080947
Iteration 4/25 | Loss: 0.00078292
Iteration 5/25 | Loss: 0.00077234
Iteration 6/25 | Loss: 0.00076991
Iteration 7/25 | Loss: 0.00076962
Iteration 8/25 | Loss: 0.00076962
Iteration 9/25 | Loss: 0.00076962
Iteration 10/25 | Loss: 0.00076962
Iteration 11/25 | Loss: 0.00076962
Iteration 12/25 | Loss: 0.00076962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007696245447732508, 0.0007696245447732508, 0.0007696245447732508, 0.0007696245447732508, 0.0007696245447732508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007696245447732508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15903068
Iteration 2/25 | Loss: 0.00141335
Iteration 3/25 | Loss: 0.00141335
Iteration 4/25 | Loss: 0.00141335
Iteration 5/25 | Loss: 0.00141335
Iteration 6/25 | Loss: 0.00141335
Iteration 7/25 | Loss: 0.00141335
Iteration 8/25 | Loss: 0.00141335
Iteration 9/25 | Loss: 0.00141335
Iteration 10/25 | Loss: 0.00141335
Iteration 11/25 | Loss: 0.00141335
Iteration 12/25 | Loss: 0.00141335
Iteration 13/25 | Loss: 0.00141335
Iteration 14/25 | Loss: 0.00141335
Iteration 15/25 | Loss: 0.00141335
Iteration 16/25 | Loss: 0.00141335
Iteration 17/25 | Loss: 0.00141335
Iteration 18/25 | Loss: 0.00141335
Iteration 19/25 | Loss: 0.00141335
Iteration 20/25 | Loss: 0.00141335
Iteration 21/25 | Loss: 0.00141335
Iteration 22/25 | Loss: 0.00141335
Iteration 23/25 | Loss: 0.00141335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014133518561720848, 0.0014133518561720848, 0.0014133518561720848, 0.0014133518561720848, 0.0014133518561720848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014133518561720848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141335
Iteration 2/1000 | Loss: 0.00007782
Iteration 3/1000 | Loss: 0.00004529
Iteration 4/1000 | Loss: 0.00003953
Iteration 5/1000 | Loss: 0.00003655
Iteration 6/1000 | Loss: 0.00003436
Iteration 7/1000 | Loss: 0.00003332
Iteration 8/1000 | Loss: 0.00003225
Iteration 9/1000 | Loss: 0.00003157
Iteration 10/1000 | Loss: 0.00003112
Iteration 11/1000 | Loss: 0.00003080
Iteration 12/1000 | Loss: 0.00003046
Iteration 13/1000 | Loss: 0.00003013
Iteration 14/1000 | Loss: 0.00002995
Iteration 15/1000 | Loss: 0.00002981
Iteration 16/1000 | Loss: 0.00002979
Iteration 17/1000 | Loss: 0.00002975
Iteration 18/1000 | Loss: 0.00002961
Iteration 19/1000 | Loss: 0.00002959
Iteration 20/1000 | Loss: 0.00002952
Iteration 21/1000 | Loss: 0.00002950
Iteration 22/1000 | Loss: 0.00002948
Iteration 23/1000 | Loss: 0.00002948
Iteration 24/1000 | Loss: 0.00002948
Iteration 25/1000 | Loss: 0.00002947
Iteration 26/1000 | Loss: 0.00002946
Iteration 27/1000 | Loss: 0.00002946
Iteration 28/1000 | Loss: 0.00002945
Iteration 29/1000 | Loss: 0.00002945
Iteration 30/1000 | Loss: 0.00002945
Iteration 31/1000 | Loss: 0.00002945
Iteration 32/1000 | Loss: 0.00002944
Iteration 33/1000 | Loss: 0.00002944
Iteration 34/1000 | Loss: 0.00002943
Iteration 35/1000 | Loss: 0.00002943
Iteration 36/1000 | Loss: 0.00002942
Iteration 37/1000 | Loss: 0.00002942
Iteration 38/1000 | Loss: 0.00002941
Iteration 39/1000 | Loss: 0.00002941
Iteration 40/1000 | Loss: 0.00002941
Iteration 41/1000 | Loss: 0.00002941
Iteration 42/1000 | Loss: 0.00002940
Iteration 43/1000 | Loss: 0.00002940
Iteration 44/1000 | Loss: 0.00002940
Iteration 45/1000 | Loss: 0.00002940
Iteration 46/1000 | Loss: 0.00002940
Iteration 47/1000 | Loss: 0.00002940
Iteration 48/1000 | Loss: 0.00002940
Iteration 49/1000 | Loss: 0.00002940
Iteration 50/1000 | Loss: 0.00002940
Iteration 51/1000 | Loss: 0.00002939
Iteration 52/1000 | Loss: 0.00002939
Iteration 53/1000 | Loss: 0.00002939
Iteration 54/1000 | Loss: 0.00002938
Iteration 55/1000 | Loss: 0.00002938
Iteration 56/1000 | Loss: 0.00002938
Iteration 57/1000 | Loss: 0.00002938
Iteration 58/1000 | Loss: 0.00002938
Iteration 59/1000 | Loss: 0.00002937
Iteration 60/1000 | Loss: 0.00002937
Iteration 61/1000 | Loss: 0.00002937
Iteration 62/1000 | Loss: 0.00002937
Iteration 63/1000 | Loss: 0.00002936
Iteration 64/1000 | Loss: 0.00002936
Iteration 65/1000 | Loss: 0.00002936
Iteration 66/1000 | Loss: 0.00002936
Iteration 67/1000 | Loss: 0.00002936
Iteration 68/1000 | Loss: 0.00002936
Iteration 69/1000 | Loss: 0.00002936
Iteration 70/1000 | Loss: 0.00002936
Iteration 71/1000 | Loss: 0.00002935
Iteration 72/1000 | Loss: 0.00002935
Iteration 73/1000 | Loss: 0.00002935
Iteration 74/1000 | Loss: 0.00002935
Iteration 75/1000 | Loss: 0.00002935
Iteration 76/1000 | Loss: 0.00002935
Iteration 77/1000 | Loss: 0.00002935
Iteration 78/1000 | Loss: 0.00002935
Iteration 79/1000 | Loss: 0.00002935
Iteration 80/1000 | Loss: 0.00002935
Iteration 81/1000 | Loss: 0.00002935
Iteration 82/1000 | Loss: 0.00002935
Iteration 83/1000 | Loss: 0.00002934
Iteration 84/1000 | Loss: 0.00002934
Iteration 85/1000 | Loss: 0.00002934
Iteration 86/1000 | Loss: 0.00002934
Iteration 87/1000 | Loss: 0.00002934
Iteration 88/1000 | Loss: 0.00002934
Iteration 89/1000 | Loss: 0.00002934
Iteration 90/1000 | Loss: 0.00002934
Iteration 91/1000 | Loss: 0.00002934
Iteration 92/1000 | Loss: 0.00002934
Iteration 93/1000 | Loss: 0.00002933
Iteration 94/1000 | Loss: 0.00002933
Iteration 95/1000 | Loss: 0.00002933
Iteration 96/1000 | Loss: 0.00002933
Iteration 97/1000 | Loss: 0.00002933
Iteration 98/1000 | Loss: 0.00002933
Iteration 99/1000 | Loss: 0.00002933
Iteration 100/1000 | Loss: 0.00002933
Iteration 101/1000 | Loss: 0.00002932
Iteration 102/1000 | Loss: 0.00002932
Iteration 103/1000 | Loss: 0.00002932
Iteration 104/1000 | Loss: 0.00002932
Iteration 105/1000 | Loss: 0.00002932
Iteration 106/1000 | Loss: 0.00002932
Iteration 107/1000 | Loss: 0.00002932
Iteration 108/1000 | Loss: 0.00002932
Iteration 109/1000 | Loss: 0.00002932
Iteration 110/1000 | Loss: 0.00002932
Iteration 111/1000 | Loss: 0.00002932
Iteration 112/1000 | Loss: 0.00002932
Iteration 113/1000 | Loss: 0.00002932
Iteration 114/1000 | Loss: 0.00002932
Iteration 115/1000 | Loss: 0.00002932
Iteration 116/1000 | Loss: 0.00002932
Iteration 117/1000 | Loss: 0.00002932
Iteration 118/1000 | Loss: 0.00002932
Iteration 119/1000 | Loss: 0.00002932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.9315962819964625e-05, 2.9315962819964625e-05, 2.9315962819964625e-05, 2.9315962819964625e-05, 2.9315962819964625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9315962819964625e-05

Optimization complete. Final v2v error: 4.422941207885742 mm

Highest mean error: 4.827476978302002 mm for frame 96

Lowest mean error: 3.9146504402160645 mm for frame 236

Saving results

Total time: 46.69813513755798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413148
Iteration 2/25 | Loss: 0.00081511
Iteration 3/25 | Loss: 0.00070209
Iteration 4/25 | Loss: 0.00068664
Iteration 5/25 | Loss: 0.00068146
Iteration 6/25 | Loss: 0.00068034
Iteration 7/25 | Loss: 0.00068034
Iteration 8/25 | Loss: 0.00068034
Iteration 9/25 | Loss: 0.00068034
Iteration 10/25 | Loss: 0.00068034
Iteration 11/25 | Loss: 0.00068034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006803393480367959, 0.0006803393480367959, 0.0006803393480367959, 0.0006803393480367959, 0.0006803393480367959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006803393480367959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20934737
Iteration 2/25 | Loss: 0.00118654
Iteration 3/25 | Loss: 0.00118654
Iteration 4/25 | Loss: 0.00118654
Iteration 5/25 | Loss: 0.00118654
Iteration 6/25 | Loss: 0.00118654
Iteration 7/25 | Loss: 0.00118654
Iteration 8/25 | Loss: 0.00118654
Iteration 9/25 | Loss: 0.00118654
Iteration 10/25 | Loss: 0.00118654
Iteration 11/25 | Loss: 0.00118654
Iteration 12/25 | Loss: 0.00118654
Iteration 13/25 | Loss: 0.00118654
Iteration 14/25 | Loss: 0.00118654
Iteration 15/25 | Loss: 0.00118654
Iteration 16/25 | Loss: 0.00118654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001186535693705082, 0.001186535693705082, 0.001186535693705082, 0.001186535693705082, 0.001186535693705082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001186535693705082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118654
Iteration 2/1000 | Loss: 0.00002697
Iteration 3/1000 | Loss: 0.00001869
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001546
Iteration 6/1000 | Loss: 0.00001484
Iteration 7/1000 | Loss: 0.00001452
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001397
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001361
Iteration 13/1000 | Loss: 0.00001361
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001344
Iteration 16/1000 | Loss: 0.00001343
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001329
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001322
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001322
Iteration 29/1000 | Loss: 0.00001322
Iteration 30/1000 | Loss: 0.00001322
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001321
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001319
Iteration 36/1000 | Loss: 0.00001318
Iteration 37/1000 | Loss: 0.00001318
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001317
Iteration 42/1000 | Loss: 0.00001317
Iteration 43/1000 | Loss: 0.00001317
Iteration 44/1000 | Loss: 0.00001316
Iteration 45/1000 | Loss: 0.00001316
Iteration 46/1000 | Loss: 0.00001316
Iteration 47/1000 | Loss: 0.00001315
Iteration 48/1000 | Loss: 0.00001315
Iteration 49/1000 | Loss: 0.00001315
Iteration 50/1000 | Loss: 0.00001315
Iteration 51/1000 | Loss: 0.00001315
Iteration 52/1000 | Loss: 0.00001315
Iteration 53/1000 | Loss: 0.00001314
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001313
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001310
Iteration 80/1000 | Loss: 0.00001310
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001310
Iteration 88/1000 | Loss: 0.00001310
Iteration 89/1000 | Loss: 0.00001310
Iteration 90/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.3095666872686706e-05, 1.3095666872686706e-05, 1.3095666872686706e-05, 1.3095666872686706e-05, 1.3095666872686706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3095666872686706e-05

Optimization complete. Final v2v error: 3.1530888080596924 mm

Highest mean error: 3.616833448410034 mm for frame 239

Lowest mean error: 2.865142822265625 mm for frame 68

Saving results

Total time: 36.04327321052551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464403
Iteration 2/25 | Loss: 0.00115885
Iteration 3/25 | Loss: 0.00077653
Iteration 4/25 | Loss: 0.00069333
Iteration 5/25 | Loss: 0.00067928
Iteration 6/25 | Loss: 0.00067623
Iteration 7/25 | Loss: 0.00067549
Iteration 8/25 | Loss: 0.00067549
Iteration 9/25 | Loss: 0.00067547
Iteration 10/25 | Loss: 0.00067547
Iteration 11/25 | Loss: 0.00067547
Iteration 12/25 | Loss: 0.00067547
Iteration 13/25 | Loss: 0.00067547
Iteration 14/25 | Loss: 0.00067547
Iteration 15/25 | Loss: 0.00067547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006754724308848381, 0.0006754724308848381, 0.0006754724308848381, 0.0006754724308848381, 0.0006754724308848381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006754724308848381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19690311
Iteration 2/25 | Loss: 0.00119636
Iteration 3/25 | Loss: 0.00119635
Iteration 4/25 | Loss: 0.00119635
Iteration 5/25 | Loss: 0.00119635
Iteration 6/25 | Loss: 0.00119635
Iteration 7/25 | Loss: 0.00119635
Iteration 8/25 | Loss: 0.00119635
Iteration 9/25 | Loss: 0.00119635
Iteration 10/25 | Loss: 0.00119635
Iteration 11/25 | Loss: 0.00119635
Iteration 12/25 | Loss: 0.00119635
Iteration 13/25 | Loss: 0.00119635
Iteration 14/25 | Loss: 0.00119635
Iteration 15/25 | Loss: 0.00119635
Iteration 16/25 | Loss: 0.00119635
Iteration 17/25 | Loss: 0.00119635
Iteration 18/25 | Loss: 0.00119635
Iteration 19/25 | Loss: 0.00119635
Iteration 20/25 | Loss: 0.00119635
Iteration 21/25 | Loss: 0.00119635
Iteration 22/25 | Loss: 0.00119635
Iteration 23/25 | Loss: 0.00119635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001196350553072989, 0.001196350553072989, 0.001196350553072989, 0.001196350553072989, 0.001196350553072989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001196350553072989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119635
Iteration 2/1000 | Loss: 0.00002746
Iteration 3/1000 | Loss: 0.00001691
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001374
Iteration 7/1000 | Loss: 0.00001337
Iteration 8/1000 | Loss: 0.00001305
Iteration 9/1000 | Loss: 0.00001281
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001242
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001221
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001216
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001216
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001216
Iteration 26/1000 | Loss: 0.00001215
Iteration 27/1000 | Loss: 0.00001215
Iteration 28/1000 | Loss: 0.00001215
Iteration 29/1000 | Loss: 0.00001215
Iteration 30/1000 | Loss: 0.00001215
Iteration 31/1000 | Loss: 0.00001215
Iteration 32/1000 | Loss: 0.00001215
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001205
Iteration 60/1000 | Loss: 0.00001205
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001203
Iteration 64/1000 | Loss: 0.00001203
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001199
Iteration 70/1000 | Loss: 0.00001199
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001198
Iteration 75/1000 | Loss: 0.00001198
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001197
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001196
Iteration 84/1000 | Loss: 0.00001196
Iteration 85/1000 | Loss: 0.00001196
Iteration 86/1000 | Loss: 0.00001196
Iteration 87/1000 | Loss: 0.00001196
Iteration 88/1000 | Loss: 0.00001195
Iteration 89/1000 | Loss: 0.00001195
Iteration 90/1000 | Loss: 0.00001195
Iteration 91/1000 | Loss: 0.00001195
Iteration 92/1000 | Loss: 0.00001195
Iteration 93/1000 | Loss: 0.00001195
Iteration 94/1000 | Loss: 0.00001195
Iteration 95/1000 | Loss: 0.00001194
Iteration 96/1000 | Loss: 0.00001194
Iteration 97/1000 | Loss: 0.00001194
Iteration 98/1000 | Loss: 0.00001194
Iteration 99/1000 | Loss: 0.00001194
Iteration 100/1000 | Loss: 0.00001194
Iteration 101/1000 | Loss: 0.00001194
Iteration 102/1000 | Loss: 0.00001194
Iteration 103/1000 | Loss: 0.00001193
Iteration 104/1000 | Loss: 0.00001193
Iteration 105/1000 | Loss: 0.00001193
Iteration 106/1000 | Loss: 0.00001193
Iteration 107/1000 | Loss: 0.00001193
Iteration 108/1000 | Loss: 0.00001193
Iteration 109/1000 | Loss: 0.00001193
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001190
Iteration 134/1000 | Loss: 0.00001190
Iteration 135/1000 | Loss: 0.00001190
Iteration 136/1000 | Loss: 0.00001190
Iteration 137/1000 | Loss: 0.00001190
Iteration 138/1000 | Loss: 0.00001190
Iteration 139/1000 | Loss: 0.00001190
Iteration 140/1000 | Loss: 0.00001190
Iteration 141/1000 | Loss: 0.00001190
Iteration 142/1000 | Loss: 0.00001190
Iteration 143/1000 | Loss: 0.00001190
Iteration 144/1000 | Loss: 0.00001189
Iteration 145/1000 | Loss: 0.00001189
Iteration 146/1000 | Loss: 0.00001189
Iteration 147/1000 | Loss: 0.00001189
Iteration 148/1000 | Loss: 0.00001189
Iteration 149/1000 | Loss: 0.00001189
Iteration 150/1000 | Loss: 0.00001189
Iteration 151/1000 | Loss: 0.00001189
Iteration 152/1000 | Loss: 0.00001189
Iteration 153/1000 | Loss: 0.00001189
Iteration 154/1000 | Loss: 0.00001189
Iteration 155/1000 | Loss: 0.00001189
Iteration 156/1000 | Loss: 0.00001189
Iteration 157/1000 | Loss: 0.00001189
Iteration 158/1000 | Loss: 0.00001189
Iteration 159/1000 | Loss: 0.00001189
Iteration 160/1000 | Loss: 0.00001189
Iteration 161/1000 | Loss: 0.00001189
Iteration 162/1000 | Loss: 0.00001189
Iteration 163/1000 | Loss: 0.00001189
Iteration 164/1000 | Loss: 0.00001189
Iteration 165/1000 | Loss: 0.00001189
Iteration 166/1000 | Loss: 0.00001189
Iteration 167/1000 | Loss: 0.00001189
Iteration 168/1000 | Loss: 0.00001189
Iteration 169/1000 | Loss: 0.00001189
Iteration 170/1000 | Loss: 0.00001189
Iteration 171/1000 | Loss: 0.00001189
Iteration 172/1000 | Loss: 0.00001189
Iteration 173/1000 | Loss: 0.00001189
Iteration 174/1000 | Loss: 0.00001189
Iteration 175/1000 | Loss: 0.00001189
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Iteration 189/1000 | Loss: 0.00001189
Iteration 190/1000 | Loss: 0.00001189
Iteration 191/1000 | Loss: 0.00001189
Iteration 192/1000 | Loss: 0.00001189
Iteration 193/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.1886019819939975e-05, 1.1886019819939975e-05, 1.1886019819939975e-05, 1.1886019819939975e-05, 1.1886019819939975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1886019819939975e-05

Optimization complete. Final v2v error: 2.918198585510254 mm

Highest mean error: 3.7979769706726074 mm for frame 74

Lowest mean error: 2.519733190536499 mm for frame 129

Saving results

Total time: 40.28179740905762
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076512
Iteration 2/25 | Loss: 0.00198965
Iteration 3/25 | Loss: 0.00138980
Iteration 4/25 | Loss: 0.00121714
Iteration 5/25 | Loss: 0.00102688
Iteration 6/25 | Loss: 0.00097314
Iteration 7/25 | Loss: 0.00094027
Iteration 8/25 | Loss: 0.00091837
Iteration 9/25 | Loss: 0.00089727
Iteration 10/25 | Loss: 0.00087289
Iteration 11/25 | Loss: 0.00087198
Iteration 12/25 | Loss: 0.00083102
Iteration 13/25 | Loss: 0.00079766
Iteration 14/25 | Loss: 0.00078779
Iteration 15/25 | Loss: 0.00078122
Iteration 16/25 | Loss: 0.00078027
Iteration 17/25 | Loss: 0.00077266
Iteration 18/25 | Loss: 0.00077808
Iteration 19/25 | Loss: 0.00078084
Iteration 20/25 | Loss: 0.00076542
Iteration 21/25 | Loss: 0.00076357
Iteration 22/25 | Loss: 0.00076285
Iteration 23/25 | Loss: 0.00076255
Iteration 24/25 | Loss: 0.00076240
Iteration 25/25 | Loss: 0.00076231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40625286
Iteration 2/25 | Loss: 0.00187865
Iteration 3/25 | Loss: 0.00167624
Iteration 4/25 | Loss: 0.00167624
Iteration 5/25 | Loss: 0.00167624
Iteration 6/25 | Loss: 0.00167624
Iteration 7/25 | Loss: 0.00167624
Iteration 8/25 | Loss: 0.00167624
Iteration 9/25 | Loss: 0.00167624
Iteration 10/25 | Loss: 0.00167624
Iteration 11/25 | Loss: 0.00167624
Iteration 12/25 | Loss: 0.00167623
Iteration 13/25 | Loss: 0.00167623
Iteration 14/25 | Loss: 0.00167623
Iteration 15/25 | Loss: 0.00167623
Iteration 16/25 | Loss: 0.00167623
Iteration 17/25 | Loss: 0.00167623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016762349987402558, 0.0016762349987402558, 0.0016762349987402558, 0.0016762349987402558, 0.0016762349987402558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016762349987402558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167623
Iteration 2/1000 | Loss: 0.00029847
Iteration 3/1000 | Loss: 0.00008918
Iteration 4/1000 | Loss: 0.00006598
Iteration 5/1000 | Loss: 0.00005456
Iteration 6/1000 | Loss: 0.00004751
Iteration 7/1000 | Loss: 0.00004377
Iteration 8/1000 | Loss: 0.00004113
Iteration 9/1000 | Loss: 0.00003955
Iteration 10/1000 | Loss: 0.00003812
Iteration 11/1000 | Loss: 0.00132913
Iteration 12/1000 | Loss: 0.00016530
Iteration 13/1000 | Loss: 0.00004458
Iteration 14/1000 | Loss: 0.00003112
Iteration 15/1000 | Loss: 0.00002559
Iteration 16/1000 | Loss: 0.00002281
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00001959
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001850
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001810
Iteration 24/1000 | Loss: 0.00001806
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001787
Iteration 27/1000 | Loss: 0.00001775
Iteration 28/1000 | Loss: 0.00001774
Iteration 29/1000 | Loss: 0.00001773
Iteration 30/1000 | Loss: 0.00001773
Iteration 31/1000 | Loss: 0.00001772
Iteration 32/1000 | Loss: 0.00001772
Iteration 33/1000 | Loss: 0.00001771
Iteration 34/1000 | Loss: 0.00001771
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001769
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001768
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001767
Iteration 42/1000 | Loss: 0.00001767
Iteration 43/1000 | Loss: 0.00001767
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001766
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001766
Iteration 48/1000 | Loss: 0.00001765
Iteration 49/1000 | Loss: 0.00001765
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001764
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001757
Iteration 79/1000 | Loss: 0.00001757
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001756
Iteration 84/1000 | Loss: 0.00001756
Iteration 85/1000 | Loss: 0.00001756
Iteration 86/1000 | Loss: 0.00001756
Iteration 87/1000 | Loss: 0.00001756
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001756
Iteration 90/1000 | Loss: 0.00001756
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001755
Iteration 93/1000 | Loss: 0.00001755
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001753
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001753
Iteration 118/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.75273344211746e-05, 1.75273344211746e-05, 1.75273344211746e-05, 1.75273344211746e-05, 1.75273344211746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.75273344211746e-05

Optimization complete. Final v2v error: 3.5468995571136475 mm

Highest mean error: 9.053577423095703 mm for frame 61

Lowest mean error: 3.198920249938965 mm for frame 0

Saving results

Total time: 83.97536563873291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111529
Iteration 2/25 | Loss: 0.01111529
Iteration 3/25 | Loss: 0.01111529
Iteration 4/25 | Loss: 0.01111529
Iteration 5/25 | Loss: 0.01111529
Iteration 6/25 | Loss: 0.01111529
Iteration 7/25 | Loss: 0.01111529
Iteration 8/25 | Loss: 0.01111529
Iteration 9/25 | Loss: 0.01111529
Iteration 10/25 | Loss: 0.01111529
Iteration 11/25 | Loss: 0.01111529
Iteration 12/25 | Loss: 0.01111529
Iteration 13/25 | Loss: 0.01111529
Iteration 14/25 | Loss: 0.01111529
Iteration 15/25 | Loss: 0.01111529
Iteration 16/25 | Loss: 0.01111529
Iteration 17/25 | Loss: 0.01111529
Iteration 18/25 | Loss: 0.01111529
Iteration 19/25 | Loss: 0.01111529
Iteration 20/25 | Loss: 0.01111529
Iteration 21/25 | Loss: 0.01111529
Iteration 22/25 | Loss: 0.01111529
Iteration 23/25 | Loss: 0.01111529
Iteration 24/25 | Loss: 0.01111528
Iteration 25/25 | Loss: 0.01111528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64049387
Iteration 2/25 | Loss: 0.05661042
Iteration 3/25 | Loss: 0.05649264
Iteration 4/25 | Loss: 0.05643015
Iteration 5/25 | Loss: 0.05643014
Iteration 6/25 | Loss: 0.05643014
Iteration 7/25 | Loss: 0.05643014
Iteration 8/25 | Loss: 0.05643014
Iteration 9/25 | Loss: 0.05643013
Iteration 10/25 | Loss: 0.05643013
Iteration 11/25 | Loss: 0.05643014
Iteration 12/25 | Loss: 0.05643014
Iteration 13/25 | Loss: 0.05643014
Iteration 14/25 | Loss: 0.05643013
Iteration 15/25 | Loss: 0.05643013
Iteration 16/25 | Loss: 0.05643013
Iteration 17/25 | Loss: 0.05643013
Iteration 18/25 | Loss: 0.05643013
Iteration 19/25 | Loss: 0.05643013
Iteration 20/25 | Loss: 0.05643013
Iteration 21/25 | Loss: 0.05643013
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.05643013119697571, 0.05643013119697571, 0.05643013119697571, 0.05643013119697571, 0.05643013119697571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05643013119697571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05643013
Iteration 2/1000 | Loss: 0.00244579
Iteration 3/1000 | Loss: 0.00083520
Iteration 4/1000 | Loss: 0.00050626
Iteration 5/1000 | Loss: 0.00030991
Iteration 6/1000 | Loss: 0.00040213
Iteration 7/1000 | Loss: 0.00246651
Iteration 8/1000 | Loss: 0.00045176
Iteration 9/1000 | Loss: 0.00033788
Iteration 10/1000 | Loss: 0.00103915
Iteration 11/1000 | Loss: 0.00041252
Iteration 12/1000 | Loss: 0.00007961
Iteration 13/1000 | Loss: 0.00027965
Iteration 14/1000 | Loss: 0.00007592
Iteration 15/1000 | Loss: 0.00088458
Iteration 16/1000 | Loss: 0.00006716
Iteration 17/1000 | Loss: 0.00013391
Iteration 18/1000 | Loss: 0.00004690
Iteration 19/1000 | Loss: 0.00017890
Iteration 20/1000 | Loss: 0.00046116
Iteration 21/1000 | Loss: 0.00055101
Iteration 22/1000 | Loss: 0.00064587
Iteration 23/1000 | Loss: 0.00009012
Iteration 24/1000 | Loss: 0.00004241
Iteration 25/1000 | Loss: 0.00005142
Iteration 26/1000 | Loss: 0.00003755
Iteration 27/1000 | Loss: 0.00003562
Iteration 28/1000 | Loss: 0.00021134
Iteration 29/1000 | Loss: 0.00007820
Iteration 30/1000 | Loss: 0.00020514
Iteration 31/1000 | Loss: 0.00011962
Iteration 32/1000 | Loss: 0.00004036
Iteration 33/1000 | Loss: 0.00005364
Iteration 34/1000 | Loss: 0.00003150
Iteration 35/1000 | Loss: 0.00007136
Iteration 36/1000 | Loss: 0.00005997
Iteration 37/1000 | Loss: 0.00003335
Iteration 38/1000 | Loss: 0.00003653
Iteration 39/1000 | Loss: 0.00006003
Iteration 40/1000 | Loss: 0.00011329
Iteration 41/1000 | Loss: 0.00002921
Iteration 42/1000 | Loss: 0.00002894
Iteration 43/1000 | Loss: 0.00003044
Iteration 44/1000 | Loss: 0.00004770
Iteration 45/1000 | Loss: 0.00007025
Iteration 46/1000 | Loss: 0.00006703
Iteration 47/1000 | Loss: 0.00021805
Iteration 48/1000 | Loss: 0.00010898
Iteration 49/1000 | Loss: 0.00002887
Iteration 50/1000 | Loss: 0.00004945
Iteration 51/1000 | Loss: 0.00004565
Iteration 52/1000 | Loss: 0.00004980
Iteration 53/1000 | Loss: 0.00004574
Iteration 54/1000 | Loss: 0.00005532
Iteration 55/1000 | Loss: 0.00003990
Iteration 56/1000 | Loss: 0.00009024
Iteration 57/1000 | Loss: 0.00009492
Iteration 58/1000 | Loss: 0.00005013
Iteration 59/1000 | Loss: 0.00003336
Iteration 60/1000 | Loss: 0.00004354
Iteration 61/1000 | Loss: 0.00004109
Iteration 62/1000 | Loss: 0.00002863
Iteration 63/1000 | Loss: 0.00010281
Iteration 64/1000 | Loss: 0.00015750
Iteration 65/1000 | Loss: 0.00003982
Iteration 66/1000 | Loss: 0.00002950
Iteration 67/1000 | Loss: 0.00002772
Iteration 68/1000 | Loss: 0.00002768
Iteration 69/1000 | Loss: 0.00009730
Iteration 70/1000 | Loss: 0.00002909
Iteration 71/1000 | Loss: 0.00004505
Iteration 72/1000 | Loss: 0.00002978
Iteration 73/1000 | Loss: 0.00008656
Iteration 74/1000 | Loss: 0.00003110
Iteration 75/1000 | Loss: 0.00002873
Iteration 76/1000 | Loss: 0.00006283
Iteration 77/1000 | Loss: 0.00002759
Iteration 78/1000 | Loss: 0.00002731
Iteration 79/1000 | Loss: 0.00002729
Iteration 80/1000 | Loss: 0.00002718
Iteration 81/1000 | Loss: 0.00002710
Iteration 82/1000 | Loss: 0.00005921
Iteration 83/1000 | Loss: 0.00002701
Iteration 84/1000 | Loss: 0.00002700
Iteration 85/1000 | Loss: 0.00002699
Iteration 86/1000 | Loss: 0.00002699
Iteration 87/1000 | Loss: 0.00002698
Iteration 88/1000 | Loss: 0.00002697
Iteration 89/1000 | Loss: 0.00002697
Iteration 90/1000 | Loss: 0.00002697
Iteration 91/1000 | Loss: 0.00002697
Iteration 92/1000 | Loss: 0.00002697
Iteration 93/1000 | Loss: 0.00002697
Iteration 94/1000 | Loss: 0.00002696
Iteration 95/1000 | Loss: 0.00002696
Iteration 96/1000 | Loss: 0.00002696
Iteration 97/1000 | Loss: 0.00002695
Iteration 98/1000 | Loss: 0.00002694
Iteration 99/1000 | Loss: 0.00003200
Iteration 100/1000 | Loss: 0.00006646
Iteration 101/1000 | Loss: 0.00016483
Iteration 102/1000 | Loss: 0.00002962
Iteration 103/1000 | Loss: 0.00006067
Iteration 104/1000 | Loss: 0.00002692
Iteration 105/1000 | Loss: 0.00002685
Iteration 106/1000 | Loss: 0.00002685
Iteration 107/1000 | Loss: 0.00002684
Iteration 108/1000 | Loss: 0.00002684
Iteration 109/1000 | Loss: 0.00002684
Iteration 110/1000 | Loss: 0.00002683
Iteration 111/1000 | Loss: 0.00002683
Iteration 112/1000 | Loss: 0.00002682
Iteration 113/1000 | Loss: 0.00002682
Iteration 114/1000 | Loss: 0.00002682
Iteration 115/1000 | Loss: 0.00002682
Iteration 116/1000 | Loss: 0.00002681
Iteration 117/1000 | Loss: 0.00002681
Iteration 118/1000 | Loss: 0.00002681
Iteration 119/1000 | Loss: 0.00002681
Iteration 120/1000 | Loss: 0.00002681
Iteration 121/1000 | Loss: 0.00002681
Iteration 122/1000 | Loss: 0.00002681
Iteration 123/1000 | Loss: 0.00002681
Iteration 124/1000 | Loss: 0.00002681
Iteration 125/1000 | Loss: 0.00002680
Iteration 126/1000 | Loss: 0.00002680
Iteration 127/1000 | Loss: 0.00002680
Iteration 128/1000 | Loss: 0.00002680
Iteration 129/1000 | Loss: 0.00002680
Iteration 130/1000 | Loss: 0.00002680
Iteration 131/1000 | Loss: 0.00002680
Iteration 132/1000 | Loss: 0.00002680
Iteration 133/1000 | Loss: 0.00002680
Iteration 134/1000 | Loss: 0.00002680
Iteration 135/1000 | Loss: 0.00003177
Iteration 136/1000 | Loss: 0.00002675
Iteration 137/1000 | Loss: 0.00002742
Iteration 138/1000 | Loss: 0.00002702
Iteration 139/1000 | Loss: 0.00002702
Iteration 140/1000 | Loss: 0.00002673
Iteration 141/1000 | Loss: 0.00002673
Iteration 142/1000 | Loss: 0.00002673
Iteration 143/1000 | Loss: 0.00002673
Iteration 144/1000 | Loss: 0.00002673
Iteration 145/1000 | Loss: 0.00002673
Iteration 146/1000 | Loss: 0.00002676
Iteration 147/1000 | Loss: 0.00002671
Iteration 148/1000 | Loss: 0.00002671
Iteration 149/1000 | Loss: 0.00002671
Iteration 150/1000 | Loss: 0.00002671
Iteration 151/1000 | Loss: 0.00002671
Iteration 152/1000 | Loss: 0.00002671
Iteration 153/1000 | Loss: 0.00002671
Iteration 154/1000 | Loss: 0.00002671
Iteration 155/1000 | Loss: 0.00002671
Iteration 156/1000 | Loss: 0.00002671
Iteration 157/1000 | Loss: 0.00002671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.670793401193805e-05, 2.670793401193805e-05, 2.670793401193805e-05, 2.670793401193805e-05, 2.670793401193805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.670793401193805e-05

Optimization complete. Final v2v error: 4.399032115936279 mm

Highest mean error: 12.843276977539062 mm for frame 246

Lowest mean error: 3.8424742221832275 mm for frame 10

Saving results

Total time: 151.09302496910095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844664
Iteration 2/25 | Loss: 0.00154746
Iteration 3/25 | Loss: 0.00093923
Iteration 4/25 | Loss: 0.00084881
Iteration 5/25 | Loss: 0.00079261
Iteration 6/25 | Loss: 0.00081335
Iteration 7/25 | Loss: 0.00078483
Iteration 8/25 | Loss: 0.00076085
Iteration 9/25 | Loss: 0.00075139
Iteration 10/25 | Loss: 0.00075053
Iteration 11/25 | Loss: 0.00073956
Iteration 12/25 | Loss: 0.00074223
Iteration 13/25 | Loss: 0.00074316
Iteration 14/25 | Loss: 0.00073490
Iteration 15/25 | Loss: 0.00073995
Iteration 16/25 | Loss: 0.00073464
Iteration 17/25 | Loss: 0.00073476
Iteration 18/25 | Loss: 0.00073344
Iteration 19/25 | Loss: 0.00073012
Iteration 20/25 | Loss: 0.00072883
Iteration 21/25 | Loss: 0.00072818
Iteration 22/25 | Loss: 0.00072805
Iteration 23/25 | Loss: 0.00072802
Iteration 24/25 | Loss: 0.00072801
Iteration 25/25 | Loss: 0.00072801

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45255184
Iteration 2/25 | Loss: 0.00113562
Iteration 3/25 | Loss: 0.00113560
Iteration 4/25 | Loss: 0.00113560
Iteration 5/25 | Loss: 0.00113560
Iteration 6/25 | Loss: 0.00113560
Iteration 7/25 | Loss: 0.00113559
Iteration 8/25 | Loss: 0.00113559
Iteration 9/25 | Loss: 0.00113559
Iteration 10/25 | Loss: 0.00113559
Iteration 11/25 | Loss: 0.00113559
Iteration 12/25 | Loss: 0.00113559
Iteration 13/25 | Loss: 0.00113559
Iteration 14/25 | Loss: 0.00113559
Iteration 15/25 | Loss: 0.00113559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011355943279340863, 0.0011355943279340863, 0.0011355943279340863, 0.0011355943279340863, 0.0011355943279340863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011355943279340863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113559
Iteration 2/1000 | Loss: 0.00002992
Iteration 3/1000 | Loss: 0.00001974
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001679
Iteration 8/1000 | Loss: 0.00001653
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001625
Iteration 11/1000 | Loss: 0.00001606
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001570
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001569
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001567
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001565
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001565
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001565
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001564
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001552
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001543
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001542
Iteration 128/1000 | Loss: 0.00001542
Iteration 129/1000 | Loss: 0.00001542
Iteration 130/1000 | Loss: 0.00001542
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001541
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001541
Iteration 138/1000 | Loss: 0.00001541
Iteration 139/1000 | Loss: 0.00001541
Iteration 140/1000 | Loss: 0.00001541
Iteration 141/1000 | Loss: 0.00001541
Iteration 142/1000 | Loss: 0.00001541
Iteration 143/1000 | Loss: 0.00001541
Iteration 144/1000 | Loss: 0.00001541
Iteration 145/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.5413570508826524e-05, 1.5413570508826524e-05, 1.5413570508826524e-05, 1.5413570508826524e-05, 1.5413570508826524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5413570508826524e-05

Optimization complete. Final v2v error: 3.4145166873931885 mm

Highest mean error: 3.736307382583618 mm for frame 7

Lowest mean error: 3.0477592945098877 mm for frame 67

Saving results

Total time: 74.86349511146545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061804
Iteration 2/25 | Loss: 0.00350744
Iteration 3/25 | Loss: 0.00195488
Iteration 4/25 | Loss: 0.00164443
Iteration 5/25 | Loss: 0.00173503
Iteration 6/25 | Loss: 0.00117300
Iteration 7/25 | Loss: 0.00107274
Iteration 8/25 | Loss: 0.00086641
Iteration 9/25 | Loss: 0.00082323
Iteration 10/25 | Loss: 0.00077914
Iteration 11/25 | Loss: 0.00076437
Iteration 12/25 | Loss: 0.00076298
Iteration 13/25 | Loss: 0.00076214
Iteration 14/25 | Loss: 0.00075770
Iteration 15/25 | Loss: 0.00075210
Iteration 16/25 | Loss: 0.00074607
Iteration 17/25 | Loss: 0.00074392
Iteration 18/25 | Loss: 0.00074295
Iteration 19/25 | Loss: 0.00074233
Iteration 20/25 | Loss: 0.00074762
Iteration 21/25 | Loss: 0.00074655
Iteration 22/25 | Loss: 0.00074653
Iteration 23/25 | Loss: 0.00074716
Iteration 24/25 | Loss: 0.00074655
Iteration 25/25 | Loss: 0.00074585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21761334
Iteration 2/25 | Loss: 0.00391165
Iteration 3/25 | Loss: 0.00134121
Iteration 4/25 | Loss: 0.00134013
Iteration 5/25 | Loss: 0.00134013
Iteration 6/25 | Loss: 0.00134013
Iteration 7/25 | Loss: 0.00134013
Iteration 8/25 | Loss: 0.00134012
Iteration 9/25 | Loss: 0.00134012
Iteration 10/25 | Loss: 0.00134012
Iteration 11/25 | Loss: 0.00134012
Iteration 12/25 | Loss: 0.00134012
Iteration 13/25 | Loss: 0.00134012
Iteration 14/25 | Loss: 0.00134012
Iteration 15/25 | Loss: 0.00134012
Iteration 16/25 | Loss: 0.00134012
Iteration 17/25 | Loss: 0.00134012
Iteration 18/25 | Loss: 0.00134012
Iteration 19/25 | Loss: 0.00134012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013401239411905408, 0.0013401239411905408, 0.0013401239411905408, 0.0013401239411905408, 0.0013401239411905408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013401239411905408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134012
Iteration 2/1000 | Loss: 0.00024276
Iteration 3/1000 | Loss: 0.00017776
Iteration 4/1000 | Loss: 0.00027945
Iteration 5/1000 | Loss: 0.00022556
Iteration 6/1000 | Loss: 0.00004693
Iteration 7/1000 | Loss: 0.00004015
Iteration 8/1000 | Loss: 0.00004545
Iteration 9/1000 | Loss: 0.00028578
Iteration 10/1000 | Loss: 0.00011557
Iteration 11/1000 | Loss: 0.00002971
Iteration 12/1000 | Loss: 0.00002814
Iteration 13/1000 | Loss: 0.00030316
Iteration 14/1000 | Loss: 0.00022287
Iteration 15/1000 | Loss: 0.00077073
Iteration 16/1000 | Loss: 0.00070008
Iteration 17/1000 | Loss: 0.00004098
Iteration 18/1000 | Loss: 0.00003235
Iteration 19/1000 | Loss: 0.00002764
Iteration 20/1000 | Loss: 0.00002454
Iteration 21/1000 | Loss: 0.00002283
Iteration 22/1000 | Loss: 0.00002214
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001865
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001858
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001854
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001843
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001841
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001839
Iteration 66/1000 | Loss: 0.00001838
Iteration 67/1000 | Loss: 0.00001838
Iteration 68/1000 | Loss: 0.00001838
Iteration 69/1000 | Loss: 0.00001837
Iteration 70/1000 | Loss: 0.00001837
Iteration 71/1000 | Loss: 0.00001837
Iteration 72/1000 | Loss: 0.00001836
Iteration 73/1000 | Loss: 0.00001836
Iteration 74/1000 | Loss: 0.00001836
Iteration 75/1000 | Loss: 0.00001835
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001835
Iteration 79/1000 | Loss: 0.00001835
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001834
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001833
Iteration 85/1000 | Loss: 0.00001833
Iteration 86/1000 | Loss: 0.00001832
Iteration 87/1000 | Loss: 0.00001832
Iteration 88/1000 | Loss: 0.00001832
Iteration 89/1000 | Loss: 0.00001832
Iteration 90/1000 | Loss: 0.00001832
Iteration 91/1000 | Loss: 0.00001832
Iteration 92/1000 | Loss: 0.00001832
Iteration 93/1000 | Loss: 0.00001832
Iteration 94/1000 | Loss: 0.00001832
Iteration 95/1000 | Loss: 0.00001832
Iteration 96/1000 | Loss: 0.00001832
Iteration 97/1000 | Loss: 0.00001831
Iteration 98/1000 | Loss: 0.00001831
Iteration 99/1000 | Loss: 0.00001831
Iteration 100/1000 | Loss: 0.00001831
Iteration 101/1000 | Loss: 0.00001831
Iteration 102/1000 | Loss: 0.00001831
Iteration 103/1000 | Loss: 0.00001831
Iteration 104/1000 | Loss: 0.00001831
Iteration 105/1000 | Loss: 0.00001831
Iteration 106/1000 | Loss: 0.00001831
Iteration 107/1000 | Loss: 0.00001831
Iteration 108/1000 | Loss: 0.00001830
Iteration 109/1000 | Loss: 0.00001830
Iteration 110/1000 | Loss: 0.00001830
Iteration 111/1000 | Loss: 0.00001830
Iteration 112/1000 | Loss: 0.00001830
Iteration 113/1000 | Loss: 0.00001830
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00001830
Iteration 116/1000 | Loss: 0.00001830
Iteration 117/1000 | Loss: 0.00001830
Iteration 118/1000 | Loss: 0.00001830
Iteration 119/1000 | Loss: 0.00001830
Iteration 120/1000 | Loss: 0.00001830
Iteration 121/1000 | Loss: 0.00001830
Iteration 122/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.8299919247510843e-05, 1.8299919247510843e-05, 1.8299919247510843e-05, 1.8299919247510843e-05, 1.8299919247510843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8299919247510843e-05

Optimization complete. Final v2v error: 3.67177414894104 mm

Highest mean error: 4.1556715965271 mm for frame 75

Lowest mean error: 3.121464729309082 mm for frame 146

Saving results

Total time: 95.55572438240051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434690
Iteration 2/25 | Loss: 0.00092867
Iteration 3/25 | Loss: 0.00076852
Iteration 4/25 | Loss: 0.00073942
Iteration 5/25 | Loss: 0.00073269
Iteration 6/25 | Loss: 0.00073106
Iteration 7/25 | Loss: 0.00073106
Iteration 8/25 | Loss: 0.00073106
Iteration 9/25 | Loss: 0.00073106
Iteration 10/25 | Loss: 0.00073106
Iteration 11/25 | Loss: 0.00073106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007310605142265558, 0.0007310605142265558, 0.0007310605142265558, 0.0007310605142265558, 0.0007310605142265558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007310605142265558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20631504
Iteration 2/25 | Loss: 0.00111133
Iteration 3/25 | Loss: 0.00111133
Iteration 4/25 | Loss: 0.00111133
Iteration 5/25 | Loss: 0.00111133
Iteration 6/25 | Loss: 0.00111133
Iteration 7/25 | Loss: 0.00111133
Iteration 8/25 | Loss: 0.00111133
Iteration 9/25 | Loss: 0.00111133
Iteration 10/25 | Loss: 0.00111133
Iteration 11/25 | Loss: 0.00111133
Iteration 12/25 | Loss: 0.00111133
Iteration 13/25 | Loss: 0.00111133
Iteration 14/25 | Loss: 0.00111133
Iteration 15/25 | Loss: 0.00111133
Iteration 16/25 | Loss: 0.00111133
Iteration 17/25 | Loss: 0.00111133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011113256914541125, 0.0011113256914541125, 0.0011113256914541125, 0.0011113256914541125, 0.0011113256914541125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011113256914541125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111133
Iteration 2/1000 | Loss: 0.00003681
Iteration 3/1000 | Loss: 0.00002627
Iteration 4/1000 | Loss: 0.00002441
Iteration 5/1000 | Loss: 0.00002309
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002127
Iteration 8/1000 | Loss: 0.00002091
Iteration 9/1000 | Loss: 0.00002057
Iteration 10/1000 | Loss: 0.00002021
Iteration 11/1000 | Loss: 0.00001997
Iteration 12/1000 | Loss: 0.00001987
Iteration 13/1000 | Loss: 0.00001983
Iteration 14/1000 | Loss: 0.00001978
Iteration 15/1000 | Loss: 0.00001977
Iteration 16/1000 | Loss: 0.00001972
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001962
Iteration 21/1000 | Loss: 0.00001962
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001961
Iteration 24/1000 | Loss: 0.00001960
Iteration 25/1000 | Loss: 0.00001960
Iteration 26/1000 | Loss: 0.00001959
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001958
Iteration 29/1000 | Loss: 0.00001957
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001953
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001953
Iteration 36/1000 | Loss: 0.00001953
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001947
Iteration 43/1000 | Loss: 0.00001946
Iteration 44/1000 | Loss: 0.00001946
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001945
Iteration 48/1000 | Loss: 0.00001945
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001945
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001944
Iteration 58/1000 | Loss: 0.00001944
Iteration 59/1000 | Loss: 0.00001944
Iteration 60/1000 | Loss: 0.00001944
Iteration 61/1000 | Loss: 0.00001944
Iteration 62/1000 | Loss: 0.00001944
Iteration 63/1000 | Loss: 0.00001944
Iteration 64/1000 | Loss: 0.00001944
Iteration 65/1000 | Loss: 0.00001944
Iteration 66/1000 | Loss: 0.00001944
Iteration 67/1000 | Loss: 0.00001944
Iteration 68/1000 | Loss: 0.00001944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.9444525605649687e-05, 1.9444525605649687e-05, 1.9444525605649687e-05, 1.9444525605649687e-05, 1.9444525605649687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9444525605649687e-05

Optimization complete. Final v2v error: 3.770613193511963 mm

Highest mean error: 4.661427021026611 mm for frame 236

Lowest mean error: 3.2296276092529297 mm for frame 1

Saving results

Total time: 36.04238677024841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040195
Iteration 2/25 | Loss: 0.00207552
Iteration 3/25 | Loss: 0.00152217
Iteration 4/25 | Loss: 0.00135880
Iteration 5/25 | Loss: 0.00143353
Iteration 6/25 | Loss: 0.00141101
Iteration 7/25 | Loss: 0.00121488
Iteration 8/25 | Loss: 0.00114662
Iteration 9/25 | Loss: 0.00106735
Iteration 10/25 | Loss: 0.00102046
Iteration 11/25 | Loss: 0.00098090
Iteration 12/25 | Loss: 0.00094897
Iteration 13/25 | Loss: 0.00094193
Iteration 14/25 | Loss: 0.00098646
Iteration 15/25 | Loss: 0.00094004
Iteration 16/25 | Loss: 0.00088702
Iteration 17/25 | Loss: 0.00085487
Iteration 18/25 | Loss: 0.00085313
Iteration 19/25 | Loss: 0.00086769
Iteration 20/25 | Loss: 0.00084951
Iteration 21/25 | Loss: 0.00083302
Iteration 22/25 | Loss: 0.00082936
Iteration 23/25 | Loss: 0.00083115
Iteration 24/25 | Loss: 0.00082973
Iteration 25/25 | Loss: 0.00081297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59445155
Iteration 2/25 | Loss: 0.00284492
Iteration 3/25 | Loss: 0.00234250
Iteration 4/25 | Loss: 0.00234249
Iteration 5/25 | Loss: 0.00234249
Iteration 6/25 | Loss: 0.00234249
Iteration 7/25 | Loss: 0.00234249
Iteration 8/25 | Loss: 0.00234249
Iteration 9/25 | Loss: 0.00234249
Iteration 10/25 | Loss: 0.00234249
Iteration 11/25 | Loss: 0.00234249
Iteration 12/25 | Loss: 0.00234249
Iteration 13/25 | Loss: 0.00234249
Iteration 14/25 | Loss: 0.00234249
Iteration 15/25 | Loss: 0.00234249
Iteration 16/25 | Loss: 0.00234249
Iteration 17/25 | Loss: 0.00234249
Iteration 18/25 | Loss: 0.00234249
Iteration 19/25 | Loss: 0.00234249
Iteration 20/25 | Loss: 0.00234249
Iteration 21/25 | Loss: 0.00234249
Iteration 22/25 | Loss: 0.00234249
Iteration 23/25 | Loss: 0.00234249
Iteration 24/25 | Loss: 0.00234249
Iteration 25/25 | Loss: 0.00234249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234249
Iteration 2/1000 | Loss: 0.00078752
Iteration 3/1000 | Loss: 0.00118767
Iteration 4/1000 | Loss: 0.00234947
Iteration 5/1000 | Loss: 0.00305858
Iteration 6/1000 | Loss: 0.00143632
Iteration 7/1000 | Loss: 0.00153460
Iteration 8/1000 | Loss: 0.00115981
Iteration 9/1000 | Loss: 0.00033136
Iteration 10/1000 | Loss: 0.00022947
Iteration 11/1000 | Loss: 0.00016454
Iteration 12/1000 | Loss: 0.00031151
Iteration 13/1000 | Loss: 0.00025841
Iteration 14/1000 | Loss: 0.00037455
Iteration 15/1000 | Loss: 0.00070758
Iteration 16/1000 | Loss: 0.00031023
Iteration 17/1000 | Loss: 0.00069007
Iteration 18/1000 | Loss: 0.00007169
Iteration 19/1000 | Loss: 0.00057769
Iteration 20/1000 | Loss: 0.00035357
Iteration 21/1000 | Loss: 0.00095067
Iteration 22/1000 | Loss: 0.00056557
Iteration 23/1000 | Loss: 0.00025968
Iteration 24/1000 | Loss: 0.00049554
Iteration 25/1000 | Loss: 0.00113968
Iteration 26/1000 | Loss: 0.00025424
Iteration 27/1000 | Loss: 0.00030962
Iteration 28/1000 | Loss: 0.00060129
Iteration 29/1000 | Loss: 0.00067474
Iteration 30/1000 | Loss: 0.00063446
Iteration 31/1000 | Loss: 0.00052536
Iteration 32/1000 | Loss: 0.00057021
Iteration 33/1000 | Loss: 0.00048811
Iteration 34/1000 | Loss: 0.00007732
Iteration 35/1000 | Loss: 0.00040349
Iteration 36/1000 | Loss: 0.00028144
Iteration 37/1000 | Loss: 0.00032185
Iteration 38/1000 | Loss: 0.00006760
Iteration 39/1000 | Loss: 0.00034042
Iteration 40/1000 | Loss: 0.00031722
Iteration 41/1000 | Loss: 0.00030589
Iteration 42/1000 | Loss: 0.00013439
Iteration 43/1000 | Loss: 0.00005037
Iteration 44/1000 | Loss: 0.00004191
Iteration 45/1000 | Loss: 0.00003973
Iteration 46/1000 | Loss: 0.00087541
Iteration 47/1000 | Loss: 0.00169073
Iteration 48/1000 | Loss: 0.00039596
Iteration 49/1000 | Loss: 0.00042724
Iteration 50/1000 | Loss: 0.00025693
Iteration 51/1000 | Loss: 0.00064776
Iteration 52/1000 | Loss: 0.00027826
Iteration 53/1000 | Loss: 0.00032665
Iteration 54/1000 | Loss: 0.00034371
Iteration 55/1000 | Loss: 0.00065703
Iteration 56/1000 | Loss: 0.00023092
Iteration 57/1000 | Loss: 0.00045870
Iteration 58/1000 | Loss: 0.00088663
Iteration 59/1000 | Loss: 0.00086634
Iteration 60/1000 | Loss: 0.00126180
Iteration 61/1000 | Loss: 0.00074156
Iteration 62/1000 | Loss: 0.00041527
Iteration 63/1000 | Loss: 0.00072950
Iteration 64/1000 | Loss: 0.00119961
Iteration 65/1000 | Loss: 0.00077672
Iteration 66/1000 | Loss: 0.00098822
Iteration 67/1000 | Loss: 0.00019662
Iteration 68/1000 | Loss: 0.00005426
Iteration 69/1000 | Loss: 0.00007536
Iteration 70/1000 | Loss: 0.00010411
Iteration 71/1000 | Loss: 0.00003514
Iteration 72/1000 | Loss: 0.00002682
Iteration 73/1000 | Loss: 0.00002273
Iteration 74/1000 | Loss: 0.00002068
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001792
Iteration 77/1000 | Loss: 0.00001725
Iteration 78/1000 | Loss: 0.00003630
Iteration 79/1000 | Loss: 0.00003629
Iteration 80/1000 | Loss: 0.00007660
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00002220
Iteration 85/1000 | Loss: 0.00001568
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00007339
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001519
Iteration 94/1000 | Loss: 0.00001519
Iteration 95/1000 | Loss: 0.00001519
Iteration 96/1000 | Loss: 0.00001519
Iteration 97/1000 | Loss: 0.00001518
Iteration 98/1000 | Loss: 0.00001518
Iteration 99/1000 | Loss: 0.00001518
Iteration 100/1000 | Loss: 0.00001518
Iteration 101/1000 | Loss: 0.00001518
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001516
Iteration 113/1000 | Loss: 0.00001516
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001515
Iteration 119/1000 | Loss: 0.00001515
Iteration 120/1000 | Loss: 0.00001514
Iteration 121/1000 | Loss: 0.00001514
Iteration 122/1000 | Loss: 0.00001514
Iteration 123/1000 | Loss: 0.00001513
Iteration 124/1000 | Loss: 0.00001513
Iteration 125/1000 | Loss: 0.00001510
Iteration 126/1000 | Loss: 0.00001509
Iteration 127/1000 | Loss: 0.00001509
Iteration 128/1000 | Loss: 0.00001509
Iteration 129/1000 | Loss: 0.00001509
Iteration 130/1000 | Loss: 0.00001509
Iteration 131/1000 | Loss: 0.00001509
Iteration 132/1000 | Loss: 0.00001509
Iteration 133/1000 | Loss: 0.00001509
Iteration 134/1000 | Loss: 0.00001509
Iteration 135/1000 | Loss: 0.00001509
Iteration 136/1000 | Loss: 0.00001508
Iteration 137/1000 | Loss: 0.00001508
Iteration 138/1000 | Loss: 0.00001508
Iteration 139/1000 | Loss: 0.00001508
Iteration 140/1000 | Loss: 0.00001508
Iteration 141/1000 | Loss: 0.00001508
Iteration 142/1000 | Loss: 0.00001508
Iteration 143/1000 | Loss: 0.00001508
Iteration 144/1000 | Loss: 0.00001508
Iteration 145/1000 | Loss: 0.00001508
Iteration 146/1000 | Loss: 0.00001508
Iteration 147/1000 | Loss: 0.00001508
Iteration 148/1000 | Loss: 0.00001508
Iteration 149/1000 | Loss: 0.00001507
Iteration 150/1000 | Loss: 0.00001507
Iteration 151/1000 | Loss: 0.00001507
Iteration 152/1000 | Loss: 0.00001507
Iteration 153/1000 | Loss: 0.00001507
Iteration 154/1000 | Loss: 0.00001507
Iteration 155/1000 | Loss: 0.00001507
Iteration 156/1000 | Loss: 0.00001507
Iteration 157/1000 | Loss: 0.00001507
Iteration 158/1000 | Loss: 0.00001507
Iteration 159/1000 | Loss: 0.00001506
Iteration 160/1000 | Loss: 0.00001506
Iteration 161/1000 | Loss: 0.00001506
Iteration 162/1000 | Loss: 0.00001505
Iteration 163/1000 | Loss: 0.00001505
Iteration 164/1000 | Loss: 0.00001505
Iteration 165/1000 | Loss: 0.00001504
Iteration 166/1000 | Loss: 0.00001504
Iteration 167/1000 | Loss: 0.00001504
Iteration 168/1000 | Loss: 0.00001504
Iteration 169/1000 | Loss: 0.00001503
Iteration 170/1000 | Loss: 0.00001503
Iteration 171/1000 | Loss: 0.00001503
Iteration 172/1000 | Loss: 0.00001502
Iteration 173/1000 | Loss: 0.00001502
Iteration 174/1000 | Loss: 0.00001502
Iteration 175/1000 | Loss: 0.00001502
Iteration 176/1000 | Loss: 0.00001502
Iteration 177/1000 | Loss: 0.00001502
Iteration 178/1000 | Loss: 0.00001502
Iteration 179/1000 | Loss: 0.00001501
Iteration 180/1000 | Loss: 0.00001501
Iteration 181/1000 | Loss: 0.00001501
Iteration 182/1000 | Loss: 0.00001501
Iteration 183/1000 | Loss: 0.00001501
Iteration 184/1000 | Loss: 0.00001501
Iteration 185/1000 | Loss: 0.00001501
Iteration 186/1000 | Loss: 0.00001501
Iteration 187/1000 | Loss: 0.00001501
Iteration 188/1000 | Loss: 0.00001501
Iteration 189/1000 | Loss: 0.00001501
Iteration 190/1000 | Loss: 0.00001501
Iteration 191/1000 | Loss: 0.00001500
Iteration 192/1000 | Loss: 0.00001500
Iteration 193/1000 | Loss: 0.00001500
Iteration 194/1000 | Loss: 0.00001500
Iteration 195/1000 | Loss: 0.00001500
Iteration 196/1000 | Loss: 0.00001499
Iteration 197/1000 | Loss: 0.00001499
Iteration 198/1000 | Loss: 0.00001499
Iteration 199/1000 | Loss: 0.00001499
Iteration 200/1000 | Loss: 0.00001499
Iteration 201/1000 | Loss: 0.00001499
Iteration 202/1000 | Loss: 0.00001499
Iteration 203/1000 | Loss: 0.00001498
Iteration 204/1000 | Loss: 0.00001498
Iteration 205/1000 | Loss: 0.00001498
Iteration 206/1000 | Loss: 0.00001498
Iteration 207/1000 | Loss: 0.00001498
Iteration 208/1000 | Loss: 0.00001497
Iteration 209/1000 | Loss: 0.00001497
Iteration 210/1000 | Loss: 0.00001497
Iteration 211/1000 | Loss: 0.00001497
Iteration 212/1000 | Loss: 0.00001497
Iteration 213/1000 | Loss: 0.00001497
Iteration 214/1000 | Loss: 0.00001496
Iteration 215/1000 | Loss: 0.00001496
Iteration 216/1000 | Loss: 0.00001496
Iteration 217/1000 | Loss: 0.00001496
Iteration 218/1000 | Loss: 0.00001496
Iteration 219/1000 | Loss: 0.00001496
Iteration 220/1000 | Loss: 0.00001496
Iteration 221/1000 | Loss: 0.00001496
Iteration 222/1000 | Loss: 0.00001496
Iteration 223/1000 | Loss: 0.00001496
Iteration 224/1000 | Loss: 0.00001496
Iteration 225/1000 | Loss: 0.00001496
Iteration 226/1000 | Loss: 0.00001496
Iteration 227/1000 | Loss: 0.00001496
Iteration 228/1000 | Loss: 0.00001496
Iteration 229/1000 | Loss: 0.00001496
Iteration 230/1000 | Loss: 0.00001496
Iteration 231/1000 | Loss: 0.00001496
Iteration 232/1000 | Loss: 0.00001496
Iteration 233/1000 | Loss: 0.00001496
Iteration 234/1000 | Loss: 0.00001496
Iteration 235/1000 | Loss: 0.00001496
Iteration 236/1000 | Loss: 0.00001496
Iteration 237/1000 | Loss: 0.00001496
Iteration 238/1000 | Loss: 0.00001496
Iteration 239/1000 | Loss: 0.00001496
Iteration 240/1000 | Loss: 0.00001496
Iteration 241/1000 | Loss: 0.00001496
Iteration 242/1000 | Loss: 0.00001496
Iteration 243/1000 | Loss: 0.00001496
Iteration 244/1000 | Loss: 0.00001496
Iteration 245/1000 | Loss: 0.00001496
Iteration 246/1000 | Loss: 0.00001496
Iteration 247/1000 | Loss: 0.00001496
Iteration 248/1000 | Loss: 0.00001496
Iteration 249/1000 | Loss: 0.00001496
Iteration 250/1000 | Loss: 0.00001496
Iteration 251/1000 | Loss: 0.00001496
Iteration 252/1000 | Loss: 0.00001496
Iteration 253/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.4958386600483209e-05, 1.4958386600483209e-05, 1.4958386600483209e-05, 1.4958386600483209e-05, 1.4958386600483209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4958386600483209e-05

Optimization complete. Final v2v error: 3.3195536136627197 mm

Highest mean error: 4.440893650054932 mm for frame 44

Lowest mean error: 2.770185947418213 mm for frame 85

Saving results

Total time: 178.05386328697205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832863
Iteration 2/25 | Loss: 0.00149773
Iteration 3/25 | Loss: 0.00096830
Iteration 4/25 | Loss: 0.00087490
Iteration 5/25 | Loss: 0.00085529
Iteration 6/25 | Loss: 0.00085180
Iteration 7/25 | Loss: 0.00085335
Iteration 8/25 | Loss: 0.00085438
Iteration 9/25 | Loss: 0.00084404
Iteration 10/25 | Loss: 0.00083183
Iteration 11/25 | Loss: 0.00082268
Iteration 12/25 | Loss: 0.00082116
Iteration 13/25 | Loss: 0.00082212
Iteration 14/25 | Loss: 0.00082205
Iteration 15/25 | Loss: 0.00082060
Iteration 16/25 | Loss: 0.00081689
Iteration 17/25 | Loss: 0.00081199
Iteration 18/25 | Loss: 0.00081094
Iteration 19/25 | Loss: 0.00081412
Iteration 20/25 | Loss: 0.00081010
Iteration 21/25 | Loss: 0.00081154
Iteration 22/25 | Loss: 0.00080909
Iteration 23/25 | Loss: 0.00080666
Iteration 24/25 | Loss: 0.00080603
Iteration 25/25 | Loss: 0.00080583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32229781
Iteration 2/25 | Loss: 0.00117854
Iteration 3/25 | Loss: 0.00115466
Iteration 4/25 | Loss: 0.00115466
Iteration 5/25 | Loss: 0.00115466
Iteration 6/25 | Loss: 0.00115466
Iteration 7/25 | Loss: 0.00115466
Iteration 8/25 | Loss: 0.00115466
Iteration 9/25 | Loss: 0.00115466
Iteration 10/25 | Loss: 0.00115466
Iteration 11/25 | Loss: 0.00115466
Iteration 12/25 | Loss: 0.00115466
Iteration 13/25 | Loss: 0.00115466
Iteration 14/25 | Loss: 0.00115466
Iteration 15/25 | Loss: 0.00115466
Iteration 16/25 | Loss: 0.00115466
Iteration 17/25 | Loss: 0.00115466
Iteration 18/25 | Loss: 0.00115466
Iteration 19/25 | Loss: 0.00115466
Iteration 20/25 | Loss: 0.00115466
Iteration 21/25 | Loss: 0.00115466
Iteration 22/25 | Loss: 0.00115466
Iteration 23/25 | Loss: 0.00115466
Iteration 24/25 | Loss: 0.00115466
Iteration 25/25 | Loss: 0.00115466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011546604800969362, 0.0011546604800969362, 0.0011546604800969362, 0.0011546604800969362, 0.0011546604800969362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011546604800969362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115466
Iteration 2/1000 | Loss: 0.00006552
Iteration 3/1000 | Loss: 0.00003436
Iteration 4/1000 | Loss: 0.00002721
Iteration 5/1000 | Loss: 0.00002536
Iteration 6/1000 | Loss: 0.00009618
Iteration 7/1000 | Loss: 0.00002400
Iteration 8/1000 | Loss: 0.00002342
Iteration 9/1000 | Loss: 0.00002287
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002186
Iteration 12/1000 | Loss: 0.00002157
Iteration 13/1000 | Loss: 0.00002129
Iteration 14/1000 | Loss: 0.00002120
Iteration 15/1000 | Loss: 0.00002105
Iteration 16/1000 | Loss: 0.00002101
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00003540
Iteration 20/1000 | Loss: 0.00002362
Iteration 21/1000 | Loss: 0.00002211
Iteration 22/1000 | Loss: 0.00002091
Iteration 23/1000 | Loss: 0.00002089
Iteration 24/1000 | Loss: 0.00002088
Iteration 25/1000 | Loss: 0.00002088
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002086
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002086
Iteration 30/1000 | Loss: 0.00002085
Iteration 31/1000 | Loss: 0.00002083
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00002083
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002083
Iteration 40/1000 | Loss: 0.00002083
Iteration 41/1000 | Loss: 0.00002083
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002082
Iteration 46/1000 | Loss: 0.00002082
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002081
Iteration 49/1000 | Loss: 0.00002081
Iteration 50/1000 | Loss: 0.00002081
Iteration 51/1000 | Loss: 0.00002080
Iteration 52/1000 | Loss: 0.00002080
Iteration 53/1000 | Loss: 0.00002080
Iteration 54/1000 | Loss: 0.00002080
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002079
Iteration 57/1000 | Loss: 0.00002079
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002079
Iteration 60/1000 | Loss: 0.00002079
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00002078
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002077
Iteration 68/1000 | Loss: 0.00002077
Iteration 69/1000 | Loss: 0.00002077
Iteration 70/1000 | Loss: 0.00002077
Iteration 71/1000 | Loss: 0.00002077
Iteration 72/1000 | Loss: 0.00002077
Iteration 73/1000 | Loss: 0.00002077
Iteration 74/1000 | Loss: 0.00002077
Iteration 75/1000 | Loss: 0.00002077
Iteration 76/1000 | Loss: 0.00002077
Iteration 77/1000 | Loss: 0.00002076
Iteration 78/1000 | Loss: 0.00002076
Iteration 79/1000 | Loss: 0.00002076
Iteration 80/1000 | Loss: 0.00002076
Iteration 81/1000 | Loss: 0.00002076
Iteration 82/1000 | Loss: 0.00002076
Iteration 83/1000 | Loss: 0.00002076
Iteration 84/1000 | Loss: 0.00002076
Iteration 85/1000 | Loss: 0.00002076
Iteration 86/1000 | Loss: 0.00002076
Iteration 87/1000 | Loss: 0.00002076
Iteration 88/1000 | Loss: 0.00002076
Iteration 89/1000 | Loss: 0.00002075
Iteration 90/1000 | Loss: 0.00002075
Iteration 91/1000 | Loss: 0.00002075
Iteration 92/1000 | Loss: 0.00002075
Iteration 93/1000 | Loss: 0.00002075
Iteration 94/1000 | Loss: 0.00002075
Iteration 95/1000 | Loss: 0.00002075
Iteration 96/1000 | Loss: 0.00002075
Iteration 97/1000 | Loss: 0.00002074
Iteration 98/1000 | Loss: 0.00002074
Iteration 99/1000 | Loss: 0.00002074
Iteration 100/1000 | Loss: 0.00002074
Iteration 101/1000 | Loss: 0.00002074
Iteration 102/1000 | Loss: 0.00002074
Iteration 103/1000 | Loss: 0.00002074
Iteration 104/1000 | Loss: 0.00002074
Iteration 105/1000 | Loss: 0.00002074
Iteration 106/1000 | Loss: 0.00002074
Iteration 107/1000 | Loss: 0.00002073
Iteration 108/1000 | Loss: 0.00002073
Iteration 109/1000 | Loss: 0.00002073
Iteration 110/1000 | Loss: 0.00002073
Iteration 111/1000 | Loss: 0.00002073
Iteration 112/1000 | Loss: 0.00002073
Iteration 113/1000 | Loss: 0.00002073
Iteration 114/1000 | Loss: 0.00002073
Iteration 115/1000 | Loss: 0.00002073
Iteration 116/1000 | Loss: 0.00002073
Iteration 117/1000 | Loss: 0.00002073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.073271753033623e-05, 2.073271753033623e-05, 2.073271753033623e-05, 2.073271753033623e-05, 2.073271753033623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.073271753033623e-05

Optimization complete. Final v2v error: 3.994797468185425 mm

Highest mean error: 4.531796455383301 mm for frame 82

Lowest mean error: 3.1145761013031006 mm for frame 0

Saving results

Total time: 89.38670659065247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064682
Iteration 2/25 | Loss: 0.01064682
Iteration 3/25 | Loss: 0.01064682
Iteration 4/25 | Loss: 0.01064682
Iteration 5/25 | Loss: 0.01064682
Iteration 6/25 | Loss: 0.01064682
Iteration 7/25 | Loss: 0.01064682
Iteration 8/25 | Loss: 0.01064682
Iteration 9/25 | Loss: 0.01064682
Iteration 10/25 | Loss: 0.01064682
Iteration 11/25 | Loss: 0.01064682
Iteration 12/25 | Loss: 0.01064682
Iteration 13/25 | Loss: 0.01064682
Iteration 14/25 | Loss: 0.01064682
Iteration 15/25 | Loss: 0.01064682
Iteration 16/25 | Loss: 0.01064682
Iteration 17/25 | Loss: 0.01064681
Iteration 18/25 | Loss: 0.01064681
Iteration 19/25 | Loss: 0.01064681
Iteration 20/25 | Loss: 0.01064681
Iteration 21/25 | Loss: 0.01064681
Iteration 22/25 | Loss: 0.01064681
Iteration 23/25 | Loss: 0.01064681
Iteration 24/25 | Loss: 0.01064681
Iteration 25/25 | Loss: 0.01064681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47974098
Iteration 2/25 | Loss: 0.12994877
Iteration 3/25 | Loss: 0.10684163
Iteration 4/25 | Loss: 0.10674295
Iteration 5/25 | Loss: 0.10674292
Iteration 6/25 | Loss: 0.10674292
Iteration 7/25 | Loss: 0.10674290
Iteration 8/25 | Loss: 0.10674290
Iteration 9/25 | Loss: 0.10674290
Iteration 10/25 | Loss: 0.10674290
Iteration 11/25 | Loss: 0.10674290
Iteration 12/25 | Loss: 0.10674290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.10674290359020233, 0.10674290359020233, 0.10674290359020233, 0.10674290359020233, 0.10674290359020233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10674290359020233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10674290
Iteration 2/1000 | Loss: 0.00374358
Iteration 3/1000 | Loss: 0.02373628
Iteration 4/1000 | Loss: 0.00245255
Iteration 5/1000 | Loss: 0.00882114
Iteration 6/1000 | Loss: 0.00192512
Iteration 7/1000 | Loss: 0.00245321
Iteration 8/1000 | Loss: 0.00180776
Iteration 9/1000 | Loss: 0.00020363
Iteration 10/1000 | Loss: 0.00062265
Iteration 11/1000 | Loss: 0.00013233
Iteration 12/1000 | Loss: 0.00034663
Iteration 13/1000 | Loss: 0.00019884
Iteration 14/1000 | Loss: 0.00285860
Iteration 15/1000 | Loss: 0.00006740
Iteration 16/1000 | Loss: 0.00018169
Iteration 17/1000 | Loss: 0.00027330
Iteration 18/1000 | Loss: 0.00009652
Iteration 19/1000 | Loss: 0.00007090
Iteration 20/1000 | Loss: 0.00004563
Iteration 21/1000 | Loss: 0.00012978
Iteration 22/1000 | Loss: 0.00005882
Iteration 23/1000 | Loss: 0.00008434
Iteration 24/1000 | Loss: 0.00008335
Iteration 25/1000 | Loss: 0.00008198
Iteration 26/1000 | Loss: 0.00004517
Iteration 27/1000 | Loss: 0.00005419
Iteration 28/1000 | Loss: 0.00005262
Iteration 29/1000 | Loss: 0.00003189
Iteration 30/1000 | Loss: 0.00002843
Iteration 31/1000 | Loss: 0.00004263
Iteration 32/1000 | Loss: 0.00002722
Iteration 33/1000 | Loss: 0.00004224
Iteration 34/1000 | Loss: 0.00005262
Iteration 35/1000 | Loss: 0.00002656
Iteration 36/1000 | Loss: 0.00002631
Iteration 37/1000 | Loss: 0.00003531
Iteration 38/1000 | Loss: 0.00003927
Iteration 39/1000 | Loss: 0.00021822
Iteration 40/1000 | Loss: 0.00002984
Iteration 41/1000 | Loss: 0.00002886
Iteration 42/1000 | Loss: 0.00002617
Iteration 43/1000 | Loss: 0.00002766
Iteration 44/1000 | Loss: 0.00002568
Iteration 45/1000 | Loss: 0.00002568
Iteration 46/1000 | Loss: 0.00002568
Iteration 47/1000 | Loss: 0.00002567
Iteration 48/1000 | Loss: 0.00002567
Iteration 49/1000 | Loss: 0.00002567
Iteration 50/1000 | Loss: 0.00002567
Iteration 51/1000 | Loss: 0.00002567
Iteration 52/1000 | Loss: 0.00002567
Iteration 53/1000 | Loss: 0.00002567
Iteration 54/1000 | Loss: 0.00002567
Iteration 55/1000 | Loss: 0.00003940
Iteration 56/1000 | Loss: 0.00002551
Iteration 57/1000 | Loss: 0.00002717
Iteration 58/1000 | Loss: 0.00002548
Iteration 59/1000 | Loss: 0.00002547
Iteration 60/1000 | Loss: 0.00002547
Iteration 61/1000 | Loss: 0.00002547
Iteration 62/1000 | Loss: 0.00002547
Iteration 63/1000 | Loss: 0.00002547
Iteration 64/1000 | Loss: 0.00002547
Iteration 65/1000 | Loss: 0.00002547
Iteration 66/1000 | Loss: 0.00002547
Iteration 67/1000 | Loss: 0.00002546
Iteration 68/1000 | Loss: 0.00002546
Iteration 69/1000 | Loss: 0.00002542
Iteration 70/1000 | Loss: 0.00002542
Iteration 71/1000 | Loss: 0.00002542
Iteration 72/1000 | Loss: 0.00002542
Iteration 73/1000 | Loss: 0.00002541
Iteration 74/1000 | Loss: 0.00002540
Iteration 75/1000 | Loss: 0.00002540
Iteration 76/1000 | Loss: 0.00002540
Iteration 77/1000 | Loss: 0.00002540
Iteration 78/1000 | Loss: 0.00002540
Iteration 79/1000 | Loss: 0.00002539
Iteration 80/1000 | Loss: 0.00002539
Iteration 81/1000 | Loss: 0.00002539
Iteration 82/1000 | Loss: 0.00002539
Iteration 83/1000 | Loss: 0.00002538
Iteration 84/1000 | Loss: 0.00002538
Iteration 85/1000 | Loss: 0.00002537
Iteration 86/1000 | Loss: 0.00002537
Iteration 87/1000 | Loss: 0.00003111
Iteration 88/1000 | Loss: 0.00003266
Iteration 89/1000 | Loss: 0.00003265
Iteration 90/1000 | Loss: 0.00008708
Iteration 91/1000 | Loss: 0.00002684
Iteration 92/1000 | Loss: 0.00002665
Iteration 93/1000 | Loss: 0.00002527
Iteration 94/1000 | Loss: 0.00002527
Iteration 95/1000 | Loss: 0.00002527
Iteration 96/1000 | Loss: 0.00002527
Iteration 97/1000 | Loss: 0.00002527
Iteration 98/1000 | Loss: 0.00002527
Iteration 99/1000 | Loss: 0.00002526
Iteration 100/1000 | Loss: 0.00002825
Iteration 101/1000 | Loss: 0.00002526
Iteration 102/1000 | Loss: 0.00002526
Iteration 103/1000 | Loss: 0.00002526
Iteration 104/1000 | Loss: 0.00002526
Iteration 105/1000 | Loss: 0.00002526
Iteration 106/1000 | Loss: 0.00002526
Iteration 107/1000 | Loss: 0.00002526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.5263463612645864e-05, 2.5263463612645864e-05, 2.5263463612645864e-05, 2.5263463612645864e-05, 2.5263463612645864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5263463612645864e-05

Optimization complete. Final v2v error: 4.272314071655273 mm

Highest mean error: 4.600146770477295 mm for frame 28

Lowest mean error: 3.8195600509643555 mm for frame 239

Saving results

Total time: 87.15534377098083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709239
Iteration 2/25 | Loss: 0.00127405
Iteration 3/25 | Loss: 0.00100039
Iteration 4/25 | Loss: 0.00093310
Iteration 5/25 | Loss: 0.00091935
Iteration 6/25 | Loss: 0.00091759
Iteration 7/25 | Loss: 0.00091729
Iteration 8/25 | Loss: 0.00091729
Iteration 9/25 | Loss: 0.00091729
Iteration 10/25 | Loss: 0.00091729
Iteration 11/25 | Loss: 0.00091729
Iteration 12/25 | Loss: 0.00091729
Iteration 13/25 | Loss: 0.00091729
Iteration 14/25 | Loss: 0.00091729
Iteration 15/25 | Loss: 0.00091729
Iteration 16/25 | Loss: 0.00091729
Iteration 17/25 | Loss: 0.00091729
Iteration 18/25 | Loss: 0.00091729
Iteration 19/25 | Loss: 0.00091729
Iteration 20/25 | Loss: 0.00091729
Iteration 21/25 | Loss: 0.00091729
Iteration 22/25 | Loss: 0.00091729
Iteration 23/25 | Loss: 0.00091729
Iteration 24/25 | Loss: 0.00091729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009172888239845634, 0.0009172888239845634, 0.0009172888239845634, 0.0009172888239845634, 0.0009172888239845634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009172888239845634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65591145
Iteration 2/25 | Loss: 0.00097401
Iteration 3/25 | Loss: 0.00097400
Iteration 4/25 | Loss: 0.00097400
Iteration 5/25 | Loss: 0.00097400
Iteration 6/25 | Loss: 0.00097400
Iteration 7/25 | Loss: 0.00097400
Iteration 8/25 | Loss: 0.00097400
Iteration 9/25 | Loss: 0.00097400
Iteration 10/25 | Loss: 0.00097400
Iteration 11/25 | Loss: 0.00097400
Iteration 12/25 | Loss: 0.00097400
Iteration 13/25 | Loss: 0.00097400
Iteration 14/25 | Loss: 0.00097400
Iteration 15/25 | Loss: 0.00097400
Iteration 16/25 | Loss: 0.00097400
Iteration 17/25 | Loss: 0.00097400
Iteration 18/25 | Loss: 0.00097400
Iteration 19/25 | Loss: 0.00097400
Iteration 20/25 | Loss: 0.00097400
Iteration 21/25 | Loss: 0.00097400
Iteration 22/25 | Loss: 0.00097400
Iteration 23/25 | Loss: 0.00097400
Iteration 24/25 | Loss: 0.00097400
Iteration 25/25 | Loss: 0.00097400

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097400
Iteration 2/1000 | Loss: 0.00008557
Iteration 3/1000 | Loss: 0.00005018
Iteration 4/1000 | Loss: 0.00004453
Iteration 5/1000 | Loss: 0.00004186
Iteration 6/1000 | Loss: 0.00003999
Iteration 7/1000 | Loss: 0.00003905
Iteration 8/1000 | Loss: 0.00003841
Iteration 9/1000 | Loss: 0.00003780
Iteration 10/1000 | Loss: 0.00003732
Iteration 11/1000 | Loss: 0.00003697
Iteration 12/1000 | Loss: 0.00003672
Iteration 13/1000 | Loss: 0.00003652
Iteration 14/1000 | Loss: 0.00003636
Iteration 15/1000 | Loss: 0.00003635
Iteration 16/1000 | Loss: 0.00003622
Iteration 17/1000 | Loss: 0.00003622
Iteration 18/1000 | Loss: 0.00003619
Iteration 19/1000 | Loss: 0.00003618
Iteration 20/1000 | Loss: 0.00003618
Iteration 21/1000 | Loss: 0.00003618
Iteration 22/1000 | Loss: 0.00003617
Iteration 23/1000 | Loss: 0.00003617
Iteration 24/1000 | Loss: 0.00003616
Iteration 25/1000 | Loss: 0.00003615
Iteration 26/1000 | Loss: 0.00003614
Iteration 27/1000 | Loss: 0.00003614
Iteration 28/1000 | Loss: 0.00003613
Iteration 29/1000 | Loss: 0.00003611
Iteration 30/1000 | Loss: 0.00003610
Iteration 31/1000 | Loss: 0.00003610
Iteration 32/1000 | Loss: 0.00003610
Iteration 33/1000 | Loss: 0.00003610
Iteration 34/1000 | Loss: 0.00003609
Iteration 35/1000 | Loss: 0.00003609
Iteration 36/1000 | Loss: 0.00003609
Iteration 37/1000 | Loss: 0.00003609
Iteration 38/1000 | Loss: 0.00003608
Iteration 39/1000 | Loss: 0.00003608
Iteration 40/1000 | Loss: 0.00003608
Iteration 41/1000 | Loss: 0.00003607
Iteration 42/1000 | Loss: 0.00003607
Iteration 43/1000 | Loss: 0.00003607
Iteration 44/1000 | Loss: 0.00003607
Iteration 45/1000 | Loss: 0.00003607
Iteration 46/1000 | Loss: 0.00003607
Iteration 47/1000 | Loss: 0.00003606
Iteration 48/1000 | Loss: 0.00003606
Iteration 49/1000 | Loss: 0.00003606
Iteration 50/1000 | Loss: 0.00003606
Iteration 51/1000 | Loss: 0.00003606
Iteration 52/1000 | Loss: 0.00003606
Iteration 53/1000 | Loss: 0.00003606
Iteration 54/1000 | Loss: 0.00003606
Iteration 55/1000 | Loss: 0.00003606
Iteration 56/1000 | Loss: 0.00003606
Iteration 57/1000 | Loss: 0.00003606
Iteration 58/1000 | Loss: 0.00003606
Iteration 59/1000 | Loss: 0.00003606
Iteration 60/1000 | Loss: 0.00003605
Iteration 61/1000 | Loss: 0.00003605
Iteration 62/1000 | Loss: 0.00003604
Iteration 63/1000 | Loss: 0.00003604
Iteration 64/1000 | Loss: 0.00003604
Iteration 65/1000 | Loss: 0.00003603
Iteration 66/1000 | Loss: 0.00003603
Iteration 67/1000 | Loss: 0.00003603
Iteration 68/1000 | Loss: 0.00003603
Iteration 69/1000 | Loss: 0.00003603
Iteration 70/1000 | Loss: 0.00003603
Iteration 71/1000 | Loss: 0.00003602
Iteration 72/1000 | Loss: 0.00003602
Iteration 73/1000 | Loss: 0.00003602
Iteration 74/1000 | Loss: 0.00003601
Iteration 75/1000 | Loss: 0.00003601
Iteration 76/1000 | Loss: 0.00003601
Iteration 77/1000 | Loss: 0.00003600
Iteration 78/1000 | Loss: 0.00003600
Iteration 79/1000 | Loss: 0.00003600
Iteration 80/1000 | Loss: 0.00003599
Iteration 81/1000 | Loss: 0.00003599
Iteration 82/1000 | Loss: 0.00003599
Iteration 83/1000 | Loss: 0.00003598
Iteration 84/1000 | Loss: 0.00003598
Iteration 85/1000 | Loss: 0.00003598
Iteration 86/1000 | Loss: 0.00003598
Iteration 87/1000 | Loss: 0.00003598
Iteration 88/1000 | Loss: 0.00003598
Iteration 89/1000 | Loss: 0.00003598
Iteration 90/1000 | Loss: 0.00003598
Iteration 91/1000 | Loss: 0.00003598
Iteration 92/1000 | Loss: 0.00003597
Iteration 93/1000 | Loss: 0.00003597
Iteration 94/1000 | Loss: 0.00003597
Iteration 95/1000 | Loss: 0.00003597
Iteration 96/1000 | Loss: 0.00003597
Iteration 97/1000 | Loss: 0.00003597
Iteration 98/1000 | Loss: 0.00003596
Iteration 99/1000 | Loss: 0.00003596
Iteration 100/1000 | Loss: 0.00003596
Iteration 101/1000 | Loss: 0.00003596
Iteration 102/1000 | Loss: 0.00003596
Iteration 103/1000 | Loss: 0.00003596
Iteration 104/1000 | Loss: 0.00003595
Iteration 105/1000 | Loss: 0.00003595
Iteration 106/1000 | Loss: 0.00003595
Iteration 107/1000 | Loss: 0.00003595
Iteration 108/1000 | Loss: 0.00003594
Iteration 109/1000 | Loss: 0.00003594
Iteration 110/1000 | Loss: 0.00003594
Iteration 111/1000 | Loss: 0.00003594
Iteration 112/1000 | Loss: 0.00003594
Iteration 113/1000 | Loss: 0.00003594
Iteration 114/1000 | Loss: 0.00003594
Iteration 115/1000 | Loss: 0.00003594
Iteration 116/1000 | Loss: 0.00003594
Iteration 117/1000 | Loss: 0.00003594
Iteration 118/1000 | Loss: 0.00003594
Iteration 119/1000 | Loss: 0.00003593
Iteration 120/1000 | Loss: 0.00003593
Iteration 121/1000 | Loss: 0.00003593
Iteration 122/1000 | Loss: 0.00003593
Iteration 123/1000 | Loss: 0.00003593
Iteration 124/1000 | Loss: 0.00003593
Iteration 125/1000 | Loss: 0.00003593
Iteration 126/1000 | Loss: 0.00003593
Iteration 127/1000 | Loss: 0.00003593
Iteration 128/1000 | Loss: 0.00003593
Iteration 129/1000 | Loss: 0.00003593
Iteration 130/1000 | Loss: 0.00003593
Iteration 131/1000 | Loss: 0.00003593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.59303921868559e-05, 3.59303921868559e-05, 3.59303921868559e-05, 3.59303921868559e-05, 3.59303921868559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.59303921868559e-05

Optimization complete. Final v2v error: 4.958827972412109 mm

Highest mean error: 6.961911201477051 mm for frame 159

Lowest mean error: 3.1892123222351074 mm for frame 236

Saving results

Total time: 46.08433985710144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840199
Iteration 2/25 | Loss: 0.00120685
Iteration 3/25 | Loss: 0.00092914
Iteration 4/25 | Loss: 0.00086495
Iteration 5/25 | Loss: 0.00085137
Iteration 6/25 | Loss: 0.00084836
Iteration 7/25 | Loss: 0.00084771
Iteration 8/25 | Loss: 0.00084771
Iteration 9/25 | Loss: 0.00084771
Iteration 10/25 | Loss: 0.00084771
Iteration 11/25 | Loss: 0.00084771
Iteration 12/25 | Loss: 0.00084771
Iteration 13/25 | Loss: 0.00084771
Iteration 14/25 | Loss: 0.00084771
Iteration 15/25 | Loss: 0.00084771
Iteration 16/25 | Loss: 0.00084771
Iteration 17/25 | Loss: 0.00084771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008477051742374897, 0.0008477051742374897, 0.0008477051742374897, 0.0008477051742374897, 0.0008477051742374897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008477051742374897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02004695
Iteration 2/25 | Loss: 0.00128993
Iteration 3/25 | Loss: 0.00128990
Iteration 4/25 | Loss: 0.00128990
Iteration 5/25 | Loss: 0.00128990
Iteration 6/25 | Loss: 0.00128990
Iteration 7/25 | Loss: 0.00128990
Iteration 8/25 | Loss: 0.00128989
Iteration 9/25 | Loss: 0.00128989
Iteration 10/25 | Loss: 0.00128989
Iteration 11/25 | Loss: 0.00128989
Iteration 12/25 | Loss: 0.00128989
Iteration 13/25 | Loss: 0.00128989
Iteration 14/25 | Loss: 0.00128989
Iteration 15/25 | Loss: 0.00128989
Iteration 16/25 | Loss: 0.00128989
Iteration 17/25 | Loss: 0.00128989
Iteration 18/25 | Loss: 0.00128989
Iteration 19/25 | Loss: 0.00128989
Iteration 20/25 | Loss: 0.00128989
Iteration 21/25 | Loss: 0.00128989
Iteration 22/25 | Loss: 0.00128989
Iteration 23/25 | Loss: 0.00128989
Iteration 24/25 | Loss: 0.00128989
Iteration 25/25 | Loss: 0.00128989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128989
Iteration 2/1000 | Loss: 0.00006458
Iteration 3/1000 | Loss: 0.00004035
Iteration 4/1000 | Loss: 0.00003549
Iteration 5/1000 | Loss: 0.00003421
Iteration 6/1000 | Loss: 0.00003292
Iteration 7/1000 | Loss: 0.00003209
Iteration 8/1000 | Loss: 0.00003147
Iteration 9/1000 | Loss: 0.00003094
Iteration 10/1000 | Loss: 0.00003058
Iteration 11/1000 | Loss: 0.00003027
Iteration 12/1000 | Loss: 0.00003006
Iteration 13/1000 | Loss: 0.00002994
Iteration 14/1000 | Loss: 0.00002990
Iteration 15/1000 | Loss: 0.00002983
Iteration 16/1000 | Loss: 0.00002975
Iteration 17/1000 | Loss: 0.00002972
Iteration 18/1000 | Loss: 0.00002971
Iteration 19/1000 | Loss: 0.00002971
Iteration 20/1000 | Loss: 0.00002963
Iteration 21/1000 | Loss: 0.00002960
Iteration 22/1000 | Loss: 0.00002956
Iteration 23/1000 | Loss: 0.00002956
Iteration 24/1000 | Loss: 0.00002955
Iteration 25/1000 | Loss: 0.00002954
Iteration 26/1000 | Loss: 0.00002954
Iteration 27/1000 | Loss: 0.00002954
Iteration 28/1000 | Loss: 0.00002953
Iteration 29/1000 | Loss: 0.00002953
Iteration 30/1000 | Loss: 0.00002953
Iteration 31/1000 | Loss: 0.00002952
Iteration 32/1000 | Loss: 0.00002952
Iteration 33/1000 | Loss: 0.00002951
Iteration 34/1000 | Loss: 0.00002951
Iteration 35/1000 | Loss: 0.00002950
Iteration 36/1000 | Loss: 0.00002950
Iteration 37/1000 | Loss: 0.00002950
Iteration 38/1000 | Loss: 0.00002950
Iteration 39/1000 | Loss: 0.00002949
Iteration 40/1000 | Loss: 0.00002949
Iteration 41/1000 | Loss: 0.00002949
Iteration 42/1000 | Loss: 0.00002948
Iteration 43/1000 | Loss: 0.00002948
Iteration 44/1000 | Loss: 0.00002948
Iteration 45/1000 | Loss: 0.00002947
Iteration 46/1000 | Loss: 0.00002947
Iteration 47/1000 | Loss: 0.00002947
Iteration 48/1000 | Loss: 0.00002946
Iteration 49/1000 | Loss: 0.00002946
Iteration 50/1000 | Loss: 0.00002946
Iteration 51/1000 | Loss: 0.00002945
Iteration 52/1000 | Loss: 0.00002945
Iteration 53/1000 | Loss: 0.00002945
Iteration 54/1000 | Loss: 0.00002945
Iteration 55/1000 | Loss: 0.00002945
Iteration 56/1000 | Loss: 0.00002945
Iteration 57/1000 | Loss: 0.00002945
Iteration 58/1000 | Loss: 0.00002944
Iteration 59/1000 | Loss: 0.00002944
Iteration 60/1000 | Loss: 0.00002944
Iteration 61/1000 | Loss: 0.00002943
Iteration 62/1000 | Loss: 0.00002943
Iteration 63/1000 | Loss: 0.00002943
Iteration 64/1000 | Loss: 0.00002943
Iteration 65/1000 | Loss: 0.00002943
Iteration 66/1000 | Loss: 0.00002942
Iteration 67/1000 | Loss: 0.00002942
Iteration 68/1000 | Loss: 0.00002942
Iteration 69/1000 | Loss: 0.00002941
Iteration 70/1000 | Loss: 0.00002941
Iteration 71/1000 | Loss: 0.00002941
Iteration 72/1000 | Loss: 0.00002941
Iteration 73/1000 | Loss: 0.00002941
Iteration 74/1000 | Loss: 0.00002941
Iteration 75/1000 | Loss: 0.00002941
Iteration 76/1000 | Loss: 0.00002940
Iteration 77/1000 | Loss: 0.00002940
Iteration 78/1000 | Loss: 0.00002940
Iteration 79/1000 | Loss: 0.00002940
Iteration 80/1000 | Loss: 0.00002940
Iteration 81/1000 | Loss: 0.00002940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.940481499535963e-05, 2.940481499535963e-05, 2.940481499535963e-05, 2.940481499535963e-05, 2.940481499535963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.940481499535963e-05

Optimization complete. Final v2v error: 4.589552879333496 mm

Highest mean error: 5.72764778137207 mm for frame 7

Lowest mean error: 3.674880027770996 mm for frame 99

Saving results

Total time: 42.49553680419922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004331
Iteration 2/25 | Loss: 0.00233271
Iteration 3/25 | Loss: 0.00159000
Iteration 4/25 | Loss: 0.00138229
Iteration 5/25 | Loss: 0.00120119
Iteration 6/25 | Loss: 0.00113551
Iteration 7/25 | Loss: 0.00110029
Iteration 8/25 | Loss: 0.00109039
Iteration 9/25 | Loss: 0.00111492
Iteration 10/25 | Loss: 0.00111664
Iteration 11/25 | Loss: 0.00116362
Iteration 12/25 | Loss: 0.00108417
Iteration 13/25 | Loss: 0.00100997
Iteration 14/25 | Loss: 0.00095614
Iteration 15/25 | Loss: 0.00092273
Iteration 16/25 | Loss: 0.00090892
Iteration 17/25 | Loss: 0.00090591
Iteration 18/25 | Loss: 0.00090456
Iteration 19/25 | Loss: 0.00090399
Iteration 20/25 | Loss: 0.00090369
Iteration 21/25 | Loss: 0.00090335
Iteration 22/25 | Loss: 0.00090601
Iteration 23/25 | Loss: 0.00090347
Iteration 24/25 | Loss: 0.00090140
Iteration 25/25 | Loss: 0.00090085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25079250
Iteration 2/25 | Loss: 0.00163285
Iteration 3/25 | Loss: 0.00162662
Iteration 4/25 | Loss: 0.00162662
Iteration 5/25 | Loss: 0.00162662
Iteration 6/25 | Loss: 0.00162662
Iteration 7/25 | Loss: 0.00162662
Iteration 8/25 | Loss: 0.00162662
Iteration 9/25 | Loss: 0.00162662
Iteration 10/25 | Loss: 0.00162662
Iteration 11/25 | Loss: 0.00162662
Iteration 12/25 | Loss: 0.00162662
Iteration 13/25 | Loss: 0.00162662
Iteration 14/25 | Loss: 0.00162662
Iteration 15/25 | Loss: 0.00162662
Iteration 16/25 | Loss: 0.00162662
Iteration 17/25 | Loss: 0.00162662
Iteration 18/25 | Loss: 0.00162662
Iteration 19/25 | Loss: 0.00162662
Iteration 20/25 | Loss: 0.00162662
Iteration 21/25 | Loss: 0.00162662
Iteration 22/25 | Loss: 0.00162662
Iteration 23/25 | Loss: 0.00162662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016266191378235817, 0.0016266191378235817, 0.0016266191378235817, 0.0016266191378235817, 0.0016266191378235817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016266191378235817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162662
Iteration 2/1000 | Loss: 0.00011145
Iteration 3/1000 | Loss: 0.00012369
Iteration 4/1000 | Loss: 0.00059864
Iteration 5/1000 | Loss: 0.00006454
Iteration 6/1000 | Loss: 0.00005448
Iteration 7/1000 | Loss: 0.00004946
Iteration 8/1000 | Loss: 0.00004664
Iteration 9/1000 | Loss: 0.00004534
Iteration 10/1000 | Loss: 0.00004432
Iteration 11/1000 | Loss: 0.00008972
Iteration 12/1000 | Loss: 0.00015761
Iteration 13/1000 | Loss: 0.00007702
Iteration 14/1000 | Loss: 0.00004601
Iteration 15/1000 | Loss: 0.00004408
Iteration 16/1000 | Loss: 0.00004244
Iteration 17/1000 | Loss: 0.00004207
Iteration 18/1000 | Loss: 0.00004165
Iteration 19/1000 | Loss: 0.00004133
Iteration 20/1000 | Loss: 0.00004105
Iteration 21/1000 | Loss: 0.00004085
Iteration 22/1000 | Loss: 0.00004057
Iteration 23/1000 | Loss: 0.00004039
Iteration 24/1000 | Loss: 0.00004017
Iteration 25/1000 | Loss: 0.00075839
Iteration 26/1000 | Loss: 0.00007208
Iteration 27/1000 | Loss: 0.00005324
Iteration 28/1000 | Loss: 0.00004199
Iteration 29/1000 | Loss: 0.00003996
Iteration 30/1000 | Loss: 0.00003873
Iteration 31/1000 | Loss: 0.00004390
Iteration 32/1000 | Loss: 0.00003729
Iteration 33/1000 | Loss: 0.00003726
Iteration 34/1000 | Loss: 0.00003713
Iteration 35/1000 | Loss: 0.00003700
Iteration 36/1000 | Loss: 0.00003693
Iteration 37/1000 | Loss: 0.00003692
Iteration 38/1000 | Loss: 0.00004449
Iteration 39/1000 | Loss: 0.00003753
Iteration 40/1000 | Loss: 0.00003677
Iteration 41/1000 | Loss: 0.00003677
Iteration 42/1000 | Loss: 0.00003677
Iteration 43/1000 | Loss: 0.00003677
Iteration 44/1000 | Loss: 0.00003677
Iteration 45/1000 | Loss: 0.00003676
Iteration 46/1000 | Loss: 0.00003676
Iteration 47/1000 | Loss: 0.00003676
Iteration 48/1000 | Loss: 0.00003676
Iteration 49/1000 | Loss: 0.00003676
Iteration 50/1000 | Loss: 0.00003676
Iteration 51/1000 | Loss: 0.00003676
Iteration 52/1000 | Loss: 0.00003675
Iteration 53/1000 | Loss: 0.00003668
Iteration 54/1000 | Loss: 0.00003667
Iteration 55/1000 | Loss: 0.00003666
Iteration 56/1000 | Loss: 0.00003665
Iteration 57/1000 | Loss: 0.00003665
Iteration 58/1000 | Loss: 0.00003665
Iteration 59/1000 | Loss: 0.00003665
Iteration 60/1000 | Loss: 0.00003665
Iteration 61/1000 | Loss: 0.00003665
Iteration 62/1000 | Loss: 0.00003664
Iteration 63/1000 | Loss: 0.00003664
Iteration 64/1000 | Loss: 0.00003664
Iteration 65/1000 | Loss: 0.00003664
Iteration 66/1000 | Loss: 0.00003663
Iteration 67/1000 | Loss: 0.00003663
Iteration 68/1000 | Loss: 0.00003662
Iteration 69/1000 | Loss: 0.00003662
Iteration 70/1000 | Loss: 0.00003662
Iteration 71/1000 | Loss: 0.00003662
Iteration 72/1000 | Loss: 0.00003662
Iteration 73/1000 | Loss: 0.00003662
Iteration 74/1000 | Loss: 0.00003662
Iteration 75/1000 | Loss: 0.00003662
Iteration 76/1000 | Loss: 0.00003662
Iteration 77/1000 | Loss: 0.00003662
Iteration 78/1000 | Loss: 0.00003662
Iteration 79/1000 | Loss: 0.00003661
Iteration 80/1000 | Loss: 0.00003661
Iteration 81/1000 | Loss: 0.00003661
Iteration 82/1000 | Loss: 0.00003661
Iteration 83/1000 | Loss: 0.00003661
Iteration 84/1000 | Loss: 0.00003661
Iteration 85/1000 | Loss: 0.00003660
Iteration 86/1000 | Loss: 0.00003660
Iteration 87/1000 | Loss: 0.00003660
Iteration 88/1000 | Loss: 0.00003660
Iteration 89/1000 | Loss: 0.00003659
Iteration 90/1000 | Loss: 0.00003659
Iteration 91/1000 | Loss: 0.00003659
Iteration 92/1000 | Loss: 0.00003658
Iteration 93/1000 | Loss: 0.00003658
Iteration 94/1000 | Loss: 0.00003658
Iteration 95/1000 | Loss: 0.00003658
Iteration 96/1000 | Loss: 0.00003658
Iteration 97/1000 | Loss: 0.00003658
Iteration 98/1000 | Loss: 0.00003657
Iteration 99/1000 | Loss: 0.00003657
Iteration 100/1000 | Loss: 0.00003657
Iteration 101/1000 | Loss: 0.00003657
Iteration 102/1000 | Loss: 0.00003656
Iteration 103/1000 | Loss: 0.00003656
Iteration 104/1000 | Loss: 0.00003656
Iteration 105/1000 | Loss: 0.00003656
Iteration 106/1000 | Loss: 0.00003656
Iteration 107/1000 | Loss: 0.00003656
Iteration 108/1000 | Loss: 0.00003655
Iteration 109/1000 | Loss: 0.00003655
Iteration 110/1000 | Loss: 0.00003655
Iteration 111/1000 | Loss: 0.00003655
Iteration 112/1000 | Loss: 0.00003655
Iteration 113/1000 | Loss: 0.00003655
Iteration 114/1000 | Loss: 0.00003655
Iteration 115/1000 | Loss: 0.00003655
Iteration 116/1000 | Loss: 0.00003655
Iteration 117/1000 | Loss: 0.00003655
Iteration 118/1000 | Loss: 0.00003655
Iteration 119/1000 | Loss: 0.00003655
Iteration 120/1000 | Loss: 0.00003655
Iteration 121/1000 | Loss: 0.00003654
Iteration 122/1000 | Loss: 0.00003654
Iteration 123/1000 | Loss: 0.00003654
Iteration 124/1000 | Loss: 0.00003654
Iteration 125/1000 | Loss: 0.00003654
Iteration 126/1000 | Loss: 0.00003654
Iteration 127/1000 | Loss: 0.00003654
Iteration 128/1000 | Loss: 0.00003654
Iteration 129/1000 | Loss: 0.00003654
Iteration 130/1000 | Loss: 0.00003653
Iteration 131/1000 | Loss: 0.00003653
Iteration 132/1000 | Loss: 0.00003653
Iteration 133/1000 | Loss: 0.00003653
Iteration 134/1000 | Loss: 0.00003653
Iteration 135/1000 | Loss: 0.00003653
Iteration 136/1000 | Loss: 0.00003653
Iteration 137/1000 | Loss: 0.00003653
Iteration 138/1000 | Loss: 0.00003653
Iteration 139/1000 | Loss: 0.00003653
Iteration 140/1000 | Loss: 0.00003653
Iteration 141/1000 | Loss: 0.00003653
Iteration 142/1000 | Loss: 0.00003653
Iteration 143/1000 | Loss: 0.00003653
Iteration 144/1000 | Loss: 0.00003652
Iteration 145/1000 | Loss: 0.00003652
Iteration 146/1000 | Loss: 0.00003652
Iteration 147/1000 | Loss: 0.00003652
Iteration 148/1000 | Loss: 0.00003652
Iteration 149/1000 | Loss: 0.00003652
Iteration 150/1000 | Loss: 0.00003652
Iteration 151/1000 | Loss: 0.00003652
Iteration 152/1000 | Loss: 0.00003652
Iteration 153/1000 | Loss: 0.00003652
Iteration 154/1000 | Loss: 0.00003652
Iteration 155/1000 | Loss: 0.00003652
Iteration 156/1000 | Loss: 0.00003652
Iteration 157/1000 | Loss: 0.00003652
Iteration 158/1000 | Loss: 0.00003652
Iteration 159/1000 | Loss: 0.00003652
Iteration 160/1000 | Loss: 0.00003652
Iteration 161/1000 | Loss: 0.00003652
Iteration 162/1000 | Loss: 0.00003652
Iteration 163/1000 | Loss: 0.00003652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.6521469155559316e-05, 3.6521469155559316e-05, 3.6521469155559316e-05, 3.6521469155559316e-05, 3.6521469155559316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6521469155559316e-05

Optimization complete. Final v2v error: 4.796523094177246 mm

Highest mean error: 11.963594436645508 mm for frame 21

Lowest mean error: 3.592092514038086 mm for frame 25

Saving results

Total time: 102.99345803260803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785255
Iteration 2/25 | Loss: 0.00102870
Iteration 3/25 | Loss: 0.00082867
Iteration 4/25 | Loss: 0.00079230
Iteration 5/25 | Loss: 0.00078325
Iteration 6/25 | Loss: 0.00078196
Iteration 7/25 | Loss: 0.00078196
Iteration 8/25 | Loss: 0.00078196
Iteration 9/25 | Loss: 0.00078196
Iteration 10/25 | Loss: 0.00078196
Iteration 11/25 | Loss: 0.00078196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007819643942639232, 0.0007819643942639232, 0.0007819643942639232, 0.0007819643942639232, 0.0007819643942639232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007819643942639232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19481552
Iteration 2/25 | Loss: 0.00141183
Iteration 3/25 | Loss: 0.00141180
Iteration 4/25 | Loss: 0.00141180
Iteration 5/25 | Loss: 0.00141180
Iteration 6/25 | Loss: 0.00141180
Iteration 7/25 | Loss: 0.00141180
Iteration 8/25 | Loss: 0.00141180
Iteration 9/25 | Loss: 0.00141180
Iteration 10/25 | Loss: 0.00141180
Iteration 11/25 | Loss: 0.00141179
Iteration 12/25 | Loss: 0.00141179
Iteration 13/25 | Loss: 0.00141179
Iteration 14/25 | Loss: 0.00141179
Iteration 15/25 | Loss: 0.00141179
Iteration 16/25 | Loss: 0.00141180
Iteration 17/25 | Loss: 0.00141180
Iteration 18/25 | Loss: 0.00141180
Iteration 19/25 | Loss: 0.00141180
Iteration 20/25 | Loss: 0.00141180
Iteration 21/25 | Loss: 0.00141180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001411795150488615, 0.001411795150488615, 0.001411795150488615, 0.001411795150488615, 0.001411795150488615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001411795150488615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141180
Iteration 2/1000 | Loss: 0.00005551
Iteration 3/1000 | Loss: 0.00003885
Iteration 4/1000 | Loss: 0.00003454
Iteration 5/1000 | Loss: 0.00003249
Iteration 6/1000 | Loss: 0.00003106
Iteration 7/1000 | Loss: 0.00002999
Iteration 8/1000 | Loss: 0.00002910
Iteration 9/1000 | Loss: 0.00002834
Iteration 10/1000 | Loss: 0.00002781
Iteration 11/1000 | Loss: 0.00002739
Iteration 12/1000 | Loss: 0.00002707
Iteration 13/1000 | Loss: 0.00002681
Iteration 14/1000 | Loss: 0.00002674
Iteration 15/1000 | Loss: 0.00002667
Iteration 16/1000 | Loss: 0.00002666
Iteration 17/1000 | Loss: 0.00002665
Iteration 18/1000 | Loss: 0.00002664
Iteration 19/1000 | Loss: 0.00002661
Iteration 20/1000 | Loss: 0.00002654
Iteration 21/1000 | Loss: 0.00002653
Iteration 22/1000 | Loss: 0.00002652
Iteration 23/1000 | Loss: 0.00002652
Iteration 24/1000 | Loss: 0.00002649
Iteration 25/1000 | Loss: 0.00002648
Iteration 26/1000 | Loss: 0.00002648
Iteration 27/1000 | Loss: 0.00002647
Iteration 28/1000 | Loss: 0.00002647
Iteration 29/1000 | Loss: 0.00002647
Iteration 30/1000 | Loss: 0.00002647
Iteration 31/1000 | Loss: 0.00002646
Iteration 32/1000 | Loss: 0.00002646
Iteration 33/1000 | Loss: 0.00002646
Iteration 34/1000 | Loss: 0.00002645
Iteration 35/1000 | Loss: 0.00002645
Iteration 36/1000 | Loss: 0.00002645
Iteration 37/1000 | Loss: 0.00002645
Iteration 38/1000 | Loss: 0.00002644
Iteration 39/1000 | Loss: 0.00002644
Iteration 40/1000 | Loss: 0.00002644
Iteration 41/1000 | Loss: 0.00002644
Iteration 42/1000 | Loss: 0.00002644
Iteration 43/1000 | Loss: 0.00002644
Iteration 44/1000 | Loss: 0.00002643
Iteration 45/1000 | Loss: 0.00002643
Iteration 46/1000 | Loss: 0.00002642
Iteration 47/1000 | Loss: 0.00002642
Iteration 48/1000 | Loss: 0.00002642
Iteration 49/1000 | Loss: 0.00002642
Iteration 50/1000 | Loss: 0.00002642
Iteration 51/1000 | Loss: 0.00002642
Iteration 52/1000 | Loss: 0.00002642
Iteration 53/1000 | Loss: 0.00002641
Iteration 54/1000 | Loss: 0.00002641
Iteration 55/1000 | Loss: 0.00002640
Iteration 56/1000 | Loss: 0.00002640
Iteration 57/1000 | Loss: 0.00002640
Iteration 58/1000 | Loss: 0.00002640
Iteration 59/1000 | Loss: 0.00002639
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00002639
Iteration 62/1000 | Loss: 0.00002638
Iteration 63/1000 | Loss: 0.00002638
Iteration 64/1000 | Loss: 0.00002638
Iteration 65/1000 | Loss: 0.00002638
Iteration 66/1000 | Loss: 0.00002638
Iteration 67/1000 | Loss: 0.00002638
Iteration 68/1000 | Loss: 0.00002638
Iteration 69/1000 | Loss: 0.00002638
Iteration 70/1000 | Loss: 0.00002638
Iteration 71/1000 | Loss: 0.00002638
Iteration 72/1000 | Loss: 0.00002638
Iteration 73/1000 | Loss: 0.00002637
Iteration 74/1000 | Loss: 0.00002637
Iteration 75/1000 | Loss: 0.00002637
Iteration 76/1000 | Loss: 0.00002637
Iteration 77/1000 | Loss: 0.00002637
Iteration 78/1000 | Loss: 0.00002636
Iteration 79/1000 | Loss: 0.00002636
Iteration 80/1000 | Loss: 0.00002636
Iteration 81/1000 | Loss: 0.00002636
Iteration 82/1000 | Loss: 0.00002635
Iteration 83/1000 | Loss: 0.00002635
Iteration 84/1000 | Loss: 0.00002635
Iteration 85/1000 | Loss: 0.00002635
Iteration 86/1000 | Loss: 0.00002635
Iteration 87/1000 | Loss: 0.00002635
Iteration 88/1000 | Loss: 0.00002635
Iteration 89/1000 | Loss: 0.00002635
Iteration 90/1000 | Loss: 0.00002635
Iteration 91/1000 | Loss: 0.00002634
Iteration 92/1000 | Loss: 0.00002634
Iteration 93/1000 | Loss: 0.00002634
Iteration 94/1000 | Loss: 0.00002634
Iteration 95/1000 | Loss: 0.00002633
Iteration 96/1000 | Loss: 0.00002633
Iteration 97/1000 | Loss: 0.00002633
Iteration 98/1000 | Loss: 0.00002633
Iteration 99/1000 | Loss: 0.00002633
Iteration 100/1000 | Loss: 0.00002633
Iteration 101/1000 | Loss: 0.00002633
Iteration 102/1000 | Loss: 0.00002633
Iteration 103/1000 | Loss: 0.00002633
Iteration 104/1000 | Loss: 0.00002633
Iteration 105/1000 | Loss: 0.00002632
Iteration 106/1000 | Loss: 0.00002632
Iteration 107/1000 | Loss: 0.00002632
Iteration 108/1000 | Loss: 0.00002631
Iteration 109/1000 | Loss: 0.00002631
Iteration 110/1000 | Loss: 0.00002631
Iteration 111/1000 | Loss: 0.00002631
Iteration 112/1000 | Loss: 0.00002631
Iteration 113/1000 | Loss: 0.00002631
Iteration 114/1000 | Loss: 0.00002631
Iteration 115/1000 | Loss: 0.00002631
Iteration 116/1000 | Loss: 0.00002631
Iteration 117/1000 | Loss: 0.00002631
Iteration 118/1000 | Loss: 0.00002630
Iteration 119/1000 | Loss: 0.00002630
Iteration 120/1000 | Loss: 0.00002630
Iteration 121/1000 | Loss: 0.00002630
Iteration 122/1000 | Loss: 0.00002630
Iteration 123/1000 | Loss: 0.00002630
Iteration 124/1000 | Loss: 0.00002630
Iteration 125/1000 | Loss: 0.00002630
Iteration 126/1000 | Loss: 0.00002630
Iteration 127/1000 | Loss: 0.00002630
Iteration 128/1000 | Loss: 0.00002629
Iteration 129/1000 | Loss: 0.00002629
Iteration 130/1000 | Loss: 0.00002629
Iteration 131/1000 | Loss: 0.00002629
Iteration 132/1000 | Loss: 0.00002629
Iteration 133/1000 | Loss: 0.00002629
Iteration 134/1000 | Loss: 0.00002629
Iteration 135/1000 | Loss: 0.00002628
Iteration 136/1000 | Loss: 0.00002628
Iteration 137/1000 | Loss: 0.00002628
Iteration 138/1000 | Loss: 0.00002628
Iteration 139/1000 | Loss: 0.00002628
Iteration 140/1000 | Loss: 0.00002628
Iteration 141/1000 | Loss: 0.00002628
Iteration 142/1000 | Loss: 0.00002628
Iteration 143/1000 | Loss: 0.00002628
Iteration 144/1000 | Loss: 0.00002628
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002628
Iteration 147/1000 | Loss: 0.00002628
Iteration 148/1000 | Loss: 0.00002628
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002628
Iteration 152/1000 | Loss: 0.00002628
Iteration 153/1000 | Loss: 0.00002628
Iteration 154/1000 | Loss: 0.00002628
Iteration 155/1000 | Loss: 0.00002628
Iteration 156/1000 | Loss: 0.00002628
Iteration 157/1000 | Loss: 0.00002628
Iteration 158/1000 | Loss: 0.00002628
Iteration 159/1000 | Loss: 0.00002628
Iteration 160/1000 | Loss: 0.00002628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.6280795282218605e-05, 2.6280795282218605e-05, 2.6280795282218605e-05, 2.6280795282218605e-05, 2.6280795282218605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6280795282218605e-05

Optimization complete. Final v2v error: 4.348311901092529 mm

Highest mean error: 4.7057061195373535 mm for frame 155

Lowest mean error: 3.6527907848358154 mm for frame 237

Saving results

Total time: 45.374988317489624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393852
Iteration 2/25 | Loss: 0.00081064
Iteration 3/25 | Loss: 0.00071517
Iteration 4/25 | Loss: 0.00069242
Iteration 5/25 | Loss: 0.00068446
Iteration 6/25 | Loss: 0.00068252
Iteration 7/25 | Loss: 0.00068252
Iteration 8/25 | Loss: 0.00068252
Iteration 9/25 | Loss: 0.00068252
Iteration 10/25 | Loss: 0.00068252
Iteration 11/25 | Loss: 0.00068252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006825228920206428, 0.0006825228920206428, 0.0006825228920206428, 0.0006825228920206428, 0.0006825228920206428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006825228920206428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56998014
Iteration 2/25 | Loss: 0.00123503
Iteration 3/25 | Loss: 0.00123503
Iteration 4/25 | Loss: 0.00123503
Iteration 5/25 | Loss: 0.00123503
Iteration 6/25 | Loss: 0.00123503
Iteration 7/25 | Loss: 0.00123503
Iteration 8/25 | Loss: 0.00123503
Iteration 9/25 | Loss: 0.00123503
Iteration 10/25 | Loss: 0.00123503
Iteration 11/25 | Loss: 0.00123503
Iteration 12/25 | Loss: 0.00123503
Iteration 13/25 | Loss: 0.00123503
Iteration 14/25 | Loss: 0.00123503
Iteration 15/25 | Loss: 0.00123503
Iteration 16/25 | Loss: 0.00123503
Iteration 17/25 | Loss: 0.00123503
Iteration 18/25 | Loss: 0.00123503
Iteration 19/25 | Loss: 0.00123503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012350281467661262, 0.0012350281467661262, 0.0012350281467661262, 0.0012350281467661262, 0.0012350281467661262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012350281467661262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123503
Iteration 2/1000 | Loss: 0.00003180
Iteration 3/1000 | Loss: 0.00002163
Iteration 4/1000 | Loss: 0.00002003
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001741
Iteration 7/1000 | Loss: 0.00001670
Iteration 8/1000 | Loss: 0.00001628
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001547
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001531
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001519
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001516
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001508
Iteration 27/1000 | Loss: 0.00001508
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001502
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001501
Iteration 48/1000 | Loss: 0.00001501
Iteration 49/1000 | Loss: 0.00001501
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001500
Iteration 52/1000 | Loss: 0.00001500
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001500
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001499
Iteration 62/1000 | Loss: 0.00001499
Iteration 63/1000 | Loss: 0.00001499
Iteration 64/1000 | Loss: 0.00001499
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001497
Iteration 76/1000 | Loss: 0.00001497
Iteration 77/1000 | Loss: 0.00001496
Iteration 78/1000 | Loss: 0.00001496
Iteration 79/1000 | Loss: 0.00001496
Iteration 80/1000 | Loss: 0.00001496
Iteration 81/1000 | Loss: 0.00001495
Iteration 82/1000 | Loss: 0.00001495
Iteration 83/1000 | Loss: 0.00001495
Iteration 84/1000 | Loss: 0.00001495
Iteration 85/1000 | Loss: 0.00001495
Iteration 86/1000 | Loss: 0.00001495
Iteration 87/1000 | Loss: 0.00001495
Iteration 88/1000 | Loss: 0.00001495
Iteration 89/1000 | Loss: 0.00001495
Iteration 90/1000 | Loss: 0.00001495
Iteration 91/1000 | Loss: 0.00001495
Iteration 92/1000 | Loss: 0.00001494
Iteration 93/1000 | Loss: 0.00001494
Iteration 94/1000 | Loss: 0.00001494
Iteration 95/1000 | Loss: 0.00001494
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001493
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001493
Iteration 107/1000 | Loss: 0.00001493
Iteration 108/1000 | Loss: 0.00001493
Iteration 109/1000 | Loss: 0.00001493
Iteration 110/1000 | Loss: 0.00001493
Iteration 111/1000 | Loss: 0.00001493
Iteration 112/1000 | Loss: 0.00001493
Iteration 113/1000 | Loss: 0.00001493
Iteration 114/1000 | Loss: 0.00001493
Iteration 115/1000 | Loss: 0.00001493
Iteration 116/1000 | Loss: 0.00001492
Iteration 117/1000 | Loss: 0.00001492
Iteration 118/1000 | Loss: 0.00001492
Iteration 119/1000 | Loss: 0.00001492
Iteration 120/1000 | Loss: 0.00001492
Iteration 121/1000 | Loss: 0.00001492
Iteration 122/1000 | Loss: 0.00001492
Iteration 123/1000 | Loss: 0.00001492
Iteration 124/1000 | Loss: 0.00001492
Iteration 125/1000 | Loss: 0.00001492
Iteration 126/1000 | Loss: 0.00001492
Iteration 127/1000 | Loss: 0.00001492
Iteration 128/1000 | Loss: 0.00001491
Iteration 129/1000 | Loss: 0.00001491
Iteration 130/1000 | Loss: 0.00001491
Iteration 131/1000 | Loss: 0.00001491
Iteration 132/1000 | Loss: 0.00001491
Iteration 133/1000 | Loss: 0.00001491
Iteration 134/1000 | Loss: 0.00001491
Iteration 135/1000 | Loss: 0.00001491
Iteration 136/1000 | Loss: 0.00001491
Iteration 137/1000 | Loss: 0.00001491
Iteration 138/1000 | Loss: 0.00001491
Iteration 139/1000 | Loss: 0.00001491
Iteration 140/1000 | Loss: 0.00001491
Iteration 141/1000 | Loss: 0.00001491
Iteration 142/1000 | Loss: 0.00001491
Iteration 143/1000 | Loss: 0.00001491
Iteration 144/1000 | Loss: 0.00001490
Iteration 145/1000 | Loss: 0.00001490
Iteration 146/1000 | Loss: 0.00001490
Iteration 147/1000 | Loss: 0.00001490
Iteration 148/1000 | Loss: 0.00001490
Iteration 149/1000 | Loss: 0.00001490
Iteration 150/1000 | Loss: 0.00001490
Iteration 151/1000 | Loss: 0.00001490
Iteration 152/1000 | Loss: 0.00001490
Iteration 153/1000 | Loss: 0.00001490
Iteration 154/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.490443628426874e-05, 1.490443628426874e-05, 1.490443628426874e-05, 1.490443628426874e-05, 1.490443628426874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.490443628426874e-05

Optimization complete. Final v2v error: 3.3307337760925293 mm

Highest mean error: 3.49131441116333 mm for frame 124

Lowest mean error: 3.1253914833068848 mm for frame 6

Saving results

Total time: 37.67577004432678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846686
Iteration 2/25 | Loss: 0.00131771
Iteration 3/25 | Loss: 0.00081264
Iteration 4/25 | Loss: 0.00072542
Iteration 5/25 | Loss: 0.00070698
Iteration 6/25 | Loss: 0.00070171
Iteration 7/25 | Loss: 0.00069986
Iteration 8/25 | Loss: 0.00069934
Iteration 9/25 | Loss: 0.00069909
Iteration 10/25 | Loss: 0.00069883
Iteration 11/25 | Loss: 0.00070222
Iteration 12/25 | Loss: 0.00069578
Iteration 13/25 | Loss: 0.00069485
Iteration 14/25 | Loss: 0.00069448
Iteration 15/25 | Loss: 0.00069602
Iteration 16/25 | Loss: 0.00069518
Iteration 17/25 | Loss: 0.00069485
Iteration 18/25 | Loss: 0.00069758
Iteration 19/25 | Loss: 0.00069609
Iteration 20/25 | Loss: 0.00069661
Iteration 21/25 | Loss: 0.00069550
Iteration 22/25 | Loss: 0.00069489
Iteration 23/25 | Loss: 0.00069396
Iteration 24/25 | Loss: 0.00069310
Iteration 25/25 | Loss: 0.00069322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19715893
Iteration 2/25 | Loss: 0.00129192
Iteration 3/25 | Loss: 0.00129192
Iteration 4/25 | Loss: 0.00129192
Iteration 5/25 | Loss: 0.00129192
Iteration 6/25 | Loss: 0.00129192
Iteration 7/25 | Loss: 0.00129192
Iteration 8/25 | Loss: 0.00129192
Iteration 9/25 | Loss: 0.00129192
Iteration 10/25 | Loss: 0.00129192
Iteration 11/25 | Loss: 0.00129192
Iteration 12/25 | Loss: 0.00129192
Iteration 13/25 | Loss: 0.00129192
Iteration 14/25 | Loss: 0.00129192
Iteration 15/25 | Loss: 0.00129192
Iteration 16/25 | Loss: 0.00129192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012919181026518345, 0.0012919181026518345, 0.0012919181026518345, 0.0012919181026518345, 0.0012919181026518345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012919181026518345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129192
Iteration 2/1000 | Loss: 0.00006866
Iteration 3/1000 | Loss: 0.00002493
Iteration 4/1000 | Loss: 0.00002524
Iteration 5/1000 | Loss: 0.00003175
Iteration 6/1000 | Loss: 0.00003202
Iteration 7/1000 | Loss: 0.00004331
Iteration 8/1000 | Loss: 0.00003359
Iteration 9/1000 | Loss: 0.00002418
Iteration 10/1000 | Loss: 0.00003046
Iteration 11/1000 | Loss: 0.00004303
Iteration 12/1000 | Loss: 0.00003445
Iteration 13/1000 | Loss: 0.00002527
Iteration 14/1000 | Loss: 0.00001855
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001542
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001520
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001513
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001512
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001511
Iteration 58/1000 | Loss: 0.00001511
Iteration 59/1000 | Loss: 0.00001510
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001509
Iteration 62/1000 | Loss: 0.00001509
Iteration 63/1000 | Loss: 0.00001508
Iteration 64/1000 | Loss: 0.00001508
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001507
Iteration 67/1000 | Loss: 0.00001507
Iteration 68/1000 | Loss: 0.00001507
Iteration 69/1000 | Loss: 0.00001507
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001506
Iteration 72/1000 | Loss: 0.00001506
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001506
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001505
Iteration 80/1000 | Loss: 0.00001505
Iteration 81/1000 | Loss: 0.00001505
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001504
Iteration 87/1000 | Loss: 0.00001504
Iteration 88/1000 | Loss: 0.00001503
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001503
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001502
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001501
Iteration 98/1000 | Loss: 0.00001501
Iteration 99/1000 | Loss: 0.00001501
Iteration 100/1000 | Loss: 0.00001501
Iteration 101/1000 | Loss: 0.00001501
Iteration 102/1000 | Loss: 0.00001501
Iteration 103/1000 | Loss: 0.00001500
Iteration 104/1000 | Loss: 0.00001500
Iteration 105/1000 | Loss: 0.00001500
Iteration 106/1000 | Loss: 0.00001500
Iteration 107/1000 | Loss: 0.00001500
Iteration 108/1000 | Loss: 0.00001500
Iteration 109/1000 | Loss: 0.00001500
Iteration 110/1000 | Loss: 0.00001500
Iteration 111/1000 | Loss: 0.00001500
Iteration 112/1000 | Loss: 0.00001499
Iteration 113/1000 | Loss: 0.00001499
Iteration 114/1000 | Loss: 0.00001499
Iteration 115/1000 | Loss: 0.00001499
Iteration 116/1000 | Loss: 0.00001499
Iteration 117/1000 | Loss: 0.00001499
Iteration 118/1000 | Loss: 0.00001499
Iteration 119/1000 | Loss: 0.00001499
Iteration 120/1000 | Loss: 0.00001499
Iteration 121/1000 | Loss: 0.00001499
Iteration 122/1000 | Loss: 0.00001499
Iteration 123/1000 | Loss: 0.00001499
Iteration 124/1000 | Loss: 0.00001499
Iteration 125/1000 | Loss: 0.00001499
Iteration 126/1000 | Loss: 0.00001499
Iteration 127/1000 | Loss: 0.00001499
Iteration 128/1000 | Loss: 0.00001499
Iteration 129/1000 | Loss: 0.00001499
Iteration 130/1000 | Loss: 0.00001499
Iteration 131/1000 | Loss: 0.00001499
Iteration 132/1000 | Loss: 0.00001499
Iteration 133/1000 | Loss: 0.00001499
Iteration 134/1000 | Loss: 0.00001499
Iteration 135/1000 | Loss: 0.00001499
Iteration 136/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.4987717804615386e-05, 1.4987717804615386e-05, 1.4987717804615386e-05, 1.4987717804615386e-05, 1.4987717804615386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4987717804615386e-05

Optimization complete. Final v2v error: 3.367692708969116 mm

Highest mean error: 4.168498516082764 mm for frame 139

Lowest mean error: 2.6879191398620605 mm for frame 3

Saving results

Total time: 96.1234917640686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073514
Iteration 2/25 | Loss: 0.00192071
Iteration 3/25 | Loss: 0.00124228
Iteration 4/25 | Loss: 0.00108325
Iteration 5/25 | Loss: 0.00117249
Iteration 6/25 | Loss: 0.00108143
Iteration 7/25 | Loss: 0.00118522
Iteration 8/25 | Loss: 0.00107559
Iteration 9/25 | Loss: 0.00096544
Iteration 10/25 | Loss: 0.00093860
Iteration 11/25 | Loss: 0.00093482
Iteration 12/25 | Loss: 0.00089654
Iteration 13/25 | Loss: 0.00088976
Iteration 14/25 | Loss: 0.00088249
Iteration 15/25 | Loss: 0.00088011
Iteration 16/25 | Loss: 0.00091156
Iteration 17/25 | Loss: 0.00088647
Iteration 18/25 | Loss: 0.00085676
Iteration 19/25 | Loss: 0.00085175
Iteration 20/25 | Loss: 0.00084415
Iteration 21/25 | Loss: 0.00084470
Iteration 22/25 | Loss: 0.00084446
Iteration 23/25 | Loss: 0.00084362
Iteration 24/25 | Loss: 0.00083328
Iteration 25/25 | Loss: 0.00082888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46741652
Iteration 2/25 | Loss: 0.00342059
Iteration 3/25 | Loss: 0.00214804
Iteration 4/25 | Loss: 0.00214802
Iteration 5/25 | Loss: 0.00214802
Iteration 6/25 | Loss: 0.00214802
Iteration 7/25 | Loss: 0.00214802
Iteration 8/25 | Loss: 0.00214802
Iteration 9/25 | Loss: 0.00214802
Iteration 10/25 | Loss: 0.00214802
Iteration 11/25 | Loss: 0.00214802
Iteration 12/25 | Loss: 0.00214801
Iteration 13/25 | Loss: 0.00214801
Iteration 14/25 | Loss: 0.00214801
Iteration 15/25 | Loss: 0.00214801
Iteration 16/25 | Loss: 0.00214801
Iteration 17/25 | Loss: 0.00214801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002148014958947897, 0.002148014958947897, 0.002148014958947897, 0.002148014958947897, 0.002148014958947897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002148014958947897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214801
Iteration 2/1000 | Loss: 0.00212838
Iteration 3/1000 | Loss: 0.00078653
Iteration 4/1000 | Loss: 0.00076614
Iteration 5/1000 | Loss: 0.00072377
Iteration 6/1000 | Loss: 0.00071049
Iteration 7/1000 | Loss: 0.00080010
Iteration 8/1000 | Loss: 0.00074715
Iteration 9/1000 | Loss: 0.00054781
Iteration 10/1000 | Loss: 0.00072903
Iteration 11/1000 | Loss: 0.00358660
Iteration 12/1000 | Loss: 0.00075176
Iteration 13/1000 | Loss: 0.00078622
Iteration 14/1000 | Loss: 0.00037334
Iteration 15/1000 | Loss: 0.00054490
Iteration 16/1000 | Loss: 0.00057371
Iteration 17/1000 | Loss: 0.00049863
Iteration 18/1000 | Loss: 0.00048440
Iteration 19/1000 | Loss: 0.00062421
Iteration 20/1000 | Loss: 0.00031456
Iteration 21/1000 | Loss: 0.00067336
Iteration 22/1000 | Loss: 0.00069154
Iteration 23/1000 | Loss: 0.00100751
Iteration 24/1000 | Loss: 0.00074606
Iteration 25/1000 | Loss: 0.00040920
Iteration 26/1000 | Loss: 0.00040074
Iteration 27/1000 | Loss: 0.00032852
Iteration 28/1000 | Loss: 0.00032096
Iteration 29/1000 | Loss: 0.00070107
Iteration 30/1000 | Loss: 0.00087794
Iteration 31/1000 | Loss: 0.00046972
Iteration 32/1000 | Loss: 0.00050875
Iteration 33/1000 | Loss: 0.00039414
Iteration 34/1000 | Loss: 0.00046273
Iteration 35/1000 | Loss: 0.00065911
Iteration 36/1000 | Loss: 0.00064821
Iteration 37/1000 | Loss: 0.00047521
Iteration 38/1000 | Loss: 0.00046733
Iteration 39/1000 | Loss: 0.00037835
Iteration 40/1000 | Loss: 0.00029383
Iteration 41/1000 | Loss: 0.00068970
Iteration 42/1000 | Loss: 0.00057066
Iteration 43/1000 | Loss: 0.00057433
Iteration 44/1000 | Loss: 0.00048605
Iteration 45/1000 | Loss: 0.00086980
Iteration 46/1000 | Loss: 0.00056441
Iteration 47/1000 | Loss: 0.00035720
Iteration 48/1000 | Loss: 0.00043882
Iteration 49/1000 | Loss: 0.00056799
Iteration 50/1000 | Loss: 0.00080227
Iteration 51/1000 | Loss: 0.00131148
Iteration 52/1000 | Loss: 0.00097519
Iteration 53/1000 | Loss: 0.00125440
Iteration 54/1000 | Loss: 0.00060527
Iteration 55/1000 | Loss: 0.00056874
Iteration 56/1000 | Loss: 0.00048606
Iteration 57/1000 | Loss: 0.00061378
Iteration 58/1000 | Loss: 0.00040046
Iteration 59/1000 | Loss: 0.00056387
Iteration 60/1000 | Loss: 0.00027312
Iteration 61/1000 | Loss: 0.00050199
Iteration 62/1000 | Loss: 0.00051764
Iteration 63/1000 | Loss: 0.00054405
Iteration 64/1000 | Loss: 0.00032278
Iteration 65/1000 | Loss: 0.00043798
Iteration 66/1000 | Loss: 0.00077938
Iteration 67/1000 | Loss: 0.00035921
Iteration 68/1000 | Loss: 0.00041088
Iteration 69/1000 | Loss: 0.00033806
Iteration 70/1000 | Loss: 0.00047873
Iteration 71/1000 | Loss: 0.00069530
Iteration 72/1000 | Loss: 0.00021914
Iteration 73/1000 | Loss: 0.00065288
Iteration 74/1000 | Loss: 0.00082805
Iteration 75/1000 | Loss: 0.00026685
Iteration 76/1000 | Loss: 0.00018586
Iteration 77/1000 | Loss: 0.00019562
Iteration 78/1000 | Loss: 0.00020657
Iteration 79/1000 | Loss: 0.00035271
Iteration 80/1000 | Loss: 0.00029070
Iteration 81/1000 | Loss: 0.00031574
Iteration 82/1000 | Loss: 0.00024518
Iteration 83/1000 | Loss: 0.00030301
Iteration 84/1000 | Loss: 0.00031907
Iteration 85/1000 | Loss: 0.00039263
Iteration 86/1000 | Loss: 0.00036950
Iteration 87/1000 | Loss: 0.00078045
Iteration 88/1000 | Loss: 0.00066499
Iteration 89/1000 | Loss: 0.00029863
Iteration 90/1000 | Loss: 0.00042963
Iteration 91/1000 | Loss: 0.00030555
Iteration 92/1000 | Loss: 0.00037350
Iteration 93/1000 | Loss: 0.00060473
Iteration 94/1000 | Loss: 0.00041193
Iteration 95/1000 | Loss: 0.00006450
Iteration 96/1000 | Loss: 0.00006443
Iteration 97/1000 | Loss: 0.00005791
Iteration 98/1000 | Loss: 0.00005104
Iteration 99/1000 | Loss: 0.00004394
Iteration 100/1000 | Loss: 0.00024078
Iteration 101/1000 | Loss: 0.00053889
Iteration 102/1000 | Loss: 0.00023973
Iteration 103/1000 | Loss: 0.00020539
Iteration 104/1000 | Loss: 0.00017620
Iteration 105/1000 | Loss: 0.00011231
Iteration 106/1000 | Loss: 0.00052275
Iteration 107/1000 | Loss: 0.00020638
Iteration 108/1000 | Loss: 0.00003997
Iteration 109/1000 | Loss: 0.00043931
Iteration 110/1000 | Loss: 0.00052719
Iteration 111/1000 | Loss: 0.00104350
Iteration 112/1000 | Loss: 0.00068636
Iteration 113/1000 | Loss: 0.00022918
Iteration 114/1000 | Loss: 0.00003868
Iteration 115/1000 | Loss: 0.00003367
Iteration 116/1000 | Loss: 0.00015446
Iteration 117/1000 | Loss: 0.00009530
Iteration 118/1000 | Loss: 0.00006928
Iteration 119/1000 | Loss: 0.00003095
Iteration 120/1000 | Loss: 0.00002655
Iteration 121/1000 | Loss: 0.00002545
Iteration 122/1000 | Loss: 0.00002424
Iteration 123/1000 | Loss: 0.00020339
Iteration 124/1000 | Loss: 0.00024281
Iteration 125/1000 | Loss: 0.00019098
Iteration 126/1000 | Loss: 0.00017897
Iteration 127/1000 | Loss: 0.00014881
Iteration 128/1000 | Loss: 0.00003124
Iteration 129/1000 | Loss: 0.00007480
Iteration 130/1000 | Loss: 0.00002377
Iteration 131/1000 | Loss: 0.00002196
Iteration 132/1000 | Loss: 0.00002106
Iteration 133/1000 | Loss: 0.00002070
Iteration 134/1000 | Loss: 0.00002032
Iteration 135/1000 | Loss: 0.00001992
Iteration 136/1000 | Loss: 0.00001968
Iteration 137/1000 | Loss: 0.00001966
Iteration 138/1000 | Loss: 0.00001963
Iteration 139/1000 | Loss: 0.00001962
Iteration 140/1000 | Loss: 0.00001956
Iteration 141/1000 | Loss: 0.00001945
Iteration 142/1000 | Loss: 0.00001942
Iteration 143/1000 | Loss: 0.00001939
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00001937
Iteration 146/1000 | Loss: 0.00001935
Iteration 147/1000 | Loss: 0.00001933
Iteration 148/1000 | Loss: 0.00001932
Iteration 149/1000 | Loss: 0.00001932
Iteration 150/1000 | Loss: 0.00001932
Iteration 151/1000 | Loss: 0.00001932
Iteration 152/1000 | Loss: 0.00001931
Iteration 153/1000 | Loss: 0.00001931
Iteration 154/1000 | Loss: 0.00001931
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001931
Iteration 157/1000 | Loss: 0.00001931
Iteration 158/1000 | Loss: 0.00001930
Iteration 159/1000 | Loss: 0.00001929
Iteration 160/1000 | Loss: 0.00001929
Iteration 161/1000 | Loss: 0.00001928
Iteration 162/1000 | Loss: 0.00001928
Iteration 163/1000 | Loss: 0.00001927
Iteration 164/1000 | Loss: 0.00001927
Iteration 165/1000 | Loss: 0.00001926
Iteration 166/1000 | Loss: 0.00001926
Iteration 167/1000 | Loss: 0.00001925
Iteration 168/1000 | Loss: 0.00001925
Iteration 169/1000 | Loss: 0.00001925
Iteration 170/1000 | Loss: 0.00001924
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001922
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001921
Iteration 175/1000 | Loss: 0.00001921
Iteration 176/1000 | Loss: 0.00001921
Iteration 177/1000 | Loss: 0.00001921
Iteration 178/1000 | Loss: 0.00001921
Iteration 179/1000 | Loss: 0.00001920
Iteration 180/1000 | Loss: 0.00001920
Iteration 181/1000 | Loss: 0.00001919
Iteration 182/1000 | Loss: 0.00001919
Iteration 183/1000 | Loss: 0.00001919
Iteration 184/1000 | Loss: 0.00001919
Iteration 185/1000 | Loss: 0.00001919
Iteration 186/1000 | Loss: 0.00001919
Iteration 187/1000 | Loss: 0.00001919
Iteration 188/1000 | Loss: 0.00001919
Iteration 189/1000 | Loss: 0.00001919
Iteration 190/1000 | Loss: 0.00001918
Iteration 191/1000 | Loss: 0.00001918
Iteration 192/1000 | Loss: 0.00001918
Iteration 193/1000 | Loss: 0.00001918
Iteration 194/1000 | Loss: 0.00001917
Iteration 195/1000 | Loss: 0.00001917
Iteration 196/1000 | Loss: 0.00001917
Iteration 197/1000 | Loss: 0.00001917
Iteration 198/1000 | Loss: 0.00001916
Iteration 199/1000 | Loss: 0.00001916
Iteration 200/1000 | Loss: 0.00001915
Iteration 201/1000 | Loss: 0.00001915
Iteration 202/1000 | Loss: 0.00001915
Iteration 203/1000 | Loss: 0.00001914
Iteration 204/1000 | Loss: 0.00001914
Iteration 205/1000 | Loss: 0.00001914
Iteration 206/1000 | Loss: 0.00001914
Iteration 207/1000 | Loss: 0.00001914
Iteration 208/1000 | Loss: 0.00001914
Iteration 209/1000 | Loss: 0.00001914
Iteration 210/1000 | Loss: 0.00001914
Iteration 211/1000 | Loss: 0.00001913
Iteration 212/1000 | Loss: 0.00001913
Iteration 213/1000 | Loss: 0.00001913
Iteration 214/1000 | Loss: 0.00001912
Iteration 215/1000 | Loss: 0.00001912
Iteration 216/1000 | Loss: 0.00001911
Iteration 217/1000 | Loss: 0.00001911
Iteration 218/1000 | Loss: 0.00001911
Iteration 219/1000 | Loss: 0.00001911
Iteration 220/1000 | Loss: 0.00001911
Iteration 221/1000 | Loss: 0.00001911
Iteration 222/1000 | Loss: 0.00001911
Iteration 223/1000 | Loss: 0.00001911
Iteration 224/1000 | Loss: 0.00001911
Iteration 225/1000 | Loss: 0.00001911
Iteration 226/1000 | Loss: 0.00001911
Iteration 227/1000 | Loss: 0.00001911
Iteration 228/1000 | Loss: 0.00001910
Iteration 229/1000 | Loss: 0.00001910
Iteration 230/1000 | Loss: 0.00001910
Iteration 231/1000 | Loss: 0.00001910
Iteration 232/1000 | Loss: 0.00001910
Iteration 233/1000 | Loss: 0.00001909
Iteration 234/1000 | Loss: 0.00001909
Iteration 235/1000 | Loss: 0.00001909
Iteration 236/1000 | Loss: 0.00001909
Iteration 237/1000 | Loss: 0.00001909
Iteration 238/1000 | Loss: 0.00001908
Iteration 239/1000 | Loss: 0.00001908
Iteration 240/1000 | Loss: 0.00001908
Iteration 241/1000 | Loss: 0.00001908
Iteration 242/1000 | Loss: 0.00001908
Iteration 243/1000 | Loss: 0.00001908
Iteration 244/1000 | Loss: 0.00001908
Iteration 245/1000 | Loss: 0.00001907
Iteration 246/1000 | Loss: 0.00001907
Iteration 247/1000 | Loss: 0.00001907
Iteration 248/1000 | Loss: 0.00001906
Iteration 249/1000 | Loss: 0.00001906
Iteration 250/1000 | Loss: 0.00001906
Iteration 251/1000 | Loss: 0.00001906
Iteration 252/1000 | Loss: 0.00001906
Iteration 253/1000 | Loss: 0.00001906
Iteration 254/1000 | Loss: 0.00001906
Iteration 255/1000 | Loss: 0.00001906
Iteration 256/1000 | Loss: 0.00001905
Iteration 257/1000 | Loss: 0.00001905
Iteration 258/1000 | Loss: 0.00001905
Iteration 259/1000 | Loss: 0.00001905
Iteration 260/1000 | Loss: 0.00001905
Iteration 261/1000 | Loss: 0.00001905
Iteration 262/1000 | Loss: 0.00001905
Iteration 263/1000 | Loss: 0.00001905
Iteration 264/1000 | Loss: 0.00001905
Iteration 265/1000 | Loss: 0.00001905
Iteration 266/1000 | Loss: 0.00001905
Iteration 267/1000 | Loss: 0.00001904
Iteration 268/1000 | Loss: 0.00001904
Iteration 269/1000 | Loss: 0.00001904
Iteration 270/1000 | Loss: 0.00001904
Iteration 271/1000 | Loss: 0.00001904
Iteration 272/1000 | Loss: 0.00001904
Iteration 273/1000 | Loss: 0.00001904
Iteration 274/1000 | Loss: 0.00001904
Iteration 275/1000 | Loss: 0.00001904
Iteration 276/1000 | Loss: 0.00001904
Iteration 277/1000 | Loss: 0.00001904
Iteration 278/1000 | Loss: 0.00001904
Iteration 279/1000 | Loss: 0.00001904
Iteration 280/1000 | Loss: 0.00001904
Iteration 281/1000 | Loss: 0.00001904
Iteration 282/1000 | Loss: 0.00001903
Iteration 283/1000 | Loss: 0.00001903
Iteration 284/1000 | Loss: 0.00001903
Iteration 285/1000 | Loss: 0.00001903
Iteration 286/1000 | Loss: 0.00001903
Iteration 287/1000 | Loss: 0.00001903
Iteration 288/1000 | Loss: 0.00001903
Iteration 289/1000 | Loss: 0.00001903
Iteration 290/1000 | Loss: 0.00001903
Iteration 291/1000 | Loss: 0.00001903
Iteration 292/1000 | Loss: 0.00001903
Iteration 293/1000 | Loss: 0.00001902
Iteration 294/1000 | Loss: 0.00001902
Iteration 295/1000 | Loss: 0.00001902
Iteration 296/1000 | Loss: 0.00001902
Iteration 297/1000 | Loss: 0.00001902
Iteration 298/1000 | Loss: 0.00001902
Iteration 299/1000 | Loss: 0.00001902
Iteration 300/1000 | Loss: 0.00001902
Iteration 301/1000 | Loss: 0.00001902
Iteration 302/1000 | Loss: 0.00001902
Iteration 303/1000 | Loss: 0.00001902
Iteration 304/1000 | Loss: 0.00001902
Iteration 305/1000 | Loss: 0.00001902
Iteration 306/1000 | Loss: 0.00001902
Iteration 307/1000 | Loss: 0.00001902
Iteration 308/1000 | Loss: 0.00001902
Iteration 309/1000 | Loss: 0.00001902
Iteration 310/1000 | Loss: 0.00001901
Iteration 311/1000 | Loss: 0.00001901
Iteration 312/1000 | Loss: 0.00001901
Iteration 313/1000 | Loss: 0.00001901
Iteration 314/1000 | Loss: 0.00001901
Iteration 315/1000 | Loss: 0.00001901
Iteration 316/1000 | Loss: 0.00001901
Iteration 317/1000 | Loss: 0.00001901
Iteration 318/1000 | Loss: 0.00001901
Iteration 319/1000 | Loss: 0.00001901
Iteration 320/1000 | Loss: 0.00001901
Iteration 321/1000 | Loss: 0.00001901
Iteration 322/1000 | Loss: 0.00001901
Iteration 323/1000 | Loss: 0.00001901
Iteration 324/1000 | Loss: 0.00001901
Iteration 325/1000 | Loss: 0.00001901
Iteration 326/1000 | Loss: 0.00001901
Iteration 327/1000 | Loss: 0.00001901
Iteration 328/1000 | Loss: 0.00001901
Iteration 329/1000 | Loss: 0.00001901
Iteration 330/1000 | Loss: 0.00001901
Iteration 331/1000 | Loss: 0.00001901
Iteration 332/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [1.9011044059880078e-05, 1.9011044059880078e-05, 1.9011044059880078e-05, 1.9011044059880078e-05, 1.9011044059880078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9011044059880078e-05

Optimization complete. Final v2v error: 3.6396830081939697 mm

Highest mean error: 9.590184211730957 mm for frame 154

Lowest mean error: 3.1696245670318604 mm for frame 128

Saving results

Total time: 257.8267421722412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1587/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1587/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346808
Iteration 2/25 | Loss: 0.00090613
Iteration 3/25 | Loss: 0.00077592
Iteration 4/25 | Loss: 0.00075190
Iteration 5/25 | Loss: 0.00074349
Iteration 6/25 | Loss: 0.00074210
Iteration 7/25 | Loss: 0.00074210
Iteration 8/25 | Loss: 0.00074210
Iteration 9/25 | Loss: 0.00074210
Iteration 10/25 | Loss: 0.00074210
Iteration 11/25 | Loss: 0.00074210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000742097559850663, 0.000742097559850663, 0.000742097559850663, 0.000742097559850663, 0.000742097559850663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000742097559850663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16339827
Iteration 2/25 | Loss: 0.00162479
Iteration 3/25 | Loss: 0.00162479
Iteration 4/25 | Loss: 0.00162478
Iteration 5/25 | Loss: 0.00162478
Iteration 6/25 | Loss: 0.00162478
Iteration 7/25 | Loss: 0.00162478
Iteration 8/25 | Loss: 0.00162478
Iteration 9/25 | Loss: 0.00162478
Iteration 10/25 | Loss: 0.00162478
Iteration 11/25 | Loss: 0.00162478
Iteration 12/25 | Loss: 0.00162478
Iteration 13/25 | Loss: 0.00162478
Iteration 14/25 | Loss: 0.00162478
Iteration 15/25 | Loss: 0.00162478
Iteration 16/25 | Loss: 0.00162478
Iteration 17/25 | Loss: 0.00162478
Iteration 18/25 | Loss: 0.00162478
Iteration 19/25 | Loss: 0.00162478
Iteration 20/25 | Loss: 0.00162478
Iteration 21/25 | Loss: 0.00162478
Iteration 22/25 | Loss: 0.00162478
Iteration 23/25 | Loss: 0.00162478
Iteration 24/25 | Loss: 0.00162478
Iteration 25/25 | Loss: 0.00162478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162478
Iteration 2/1000 | Loss: 0.00005781
Iteration 3/1000 | Loss: 0.00003190
Iteration 4/1000 | Loss: 0.00002718
Iteration 5/1000 | Loss: 0.00002474
Iteration 6/1000 | Loss: 0.00002318
Iteration 7/1000 | Loss: 0.00002241
Iteration 8/1000 | Loss: 0.00002197
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002073
Iteration 12/1000 | Loss: 0.00002054
Iteration 13/1000 | Loss: 0.00002048
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002035
Iteration 16/1000 | Loss: 0.00002024
Iteration 17/1000 | Loss: 0.00002018
Iteration 18/1000 | Loss: 0.00002016
Iteration 19/1000 | Loss: 0.00002016
Iteration 20/1000 | Loss: 0.00002015
Iteration 21/1000 | Loss: 0.00002014
Iteration 22/1000 | Loss: 0.00002012
Iteration 23/1000 | Loss: 0.00002012
Iteration 24/1000 | Loss: 0.00002008
Iteration 25/1000 | Loss: 0.00002003
Iteration 26/1000 | Loss: 0.00002002
Iteration 27/1000 | Loss: 0.00002002
Iteration 28/1000 | Loss: 0.00002001
Iteration 29/1000 | Loss: 0.00002001
Iteration 30/1000 | Loss: 0.00002000
Iteration 31/1000 | Loss: 0.00002000
Iteration 32/1000 | Loss: 0.00001999
Iteration 33/1000 | Loss: 0.00001999
Iteration 34/1000 | Loss: 0.00001999
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001998
Iteration 37/1000 | Loss: 0.00001997
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001995
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001994
Iteration 44/1000 | Loss: 0.00001994
Iteration 45/1000 | Loss: 0.00001994
Iteration 46/1000 | Loss: 0.00001994
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001992
Iteration 51/1000 | Loss: 0.00001992
Iteration 52/1000 | Loss: 0.00001992
Iteration 53/1000 | Loss: 0.00001992
Iteration 54/1000 | Loss: 0.00001991
Iteration 55/1000 | Loss: 0.00001990
Iteration 56/1000 | Loss: 0.00001990
Iteration 57/1000 | Loss: 0.00001990
Iteration 58/1000 | Loss: 0.00001989
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001989
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001988
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001986
Iteration 68/1000 | Loss: 0.00001986
Iteration 69/1000 | Loss: 0.00001986
Iteration 70/1000 | Loss: 0.00001986
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001985
Iteration 73/1000 | Loss: 0.00001985
Iteration 74/1000 | Loss: 0.00001985
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001983
Iteration 79/1000 | Loss: 0.00001982
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001981
Iteration 82/1000 | Loss: 0.00001981
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00001979
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001977
Iteration 88/1000 | Loss: 0.00001977
Iteration 89/1000 | Loss: 0.00001977
Iteration 90/1000 | Loss: 0.00001976
Iteration 91/1000 | Loss: 0.00001976
Iteration 92/1000 | Loss: 0.00001976
Iteration 93/1000 | Loss: 0.00001976
Iteration 94/1000 | Loss: 0.00001975
Iteration 95/1000 | Loss: 0.00001975
Iteration 96/1000 | Loss: 0.00001975
Iteration 97/1000 | Loss: 0.00001975
Iteration 98/1000 | Loss: 0.00001975
Iteration 99/1000 | Loss: 0.00001974
Iteration 100/1000 | Loss: 0.00001974
Iteration 101/1000 | Loss: 0.00001974
Iteration 102/1000 | Loss: 0.00001973
Iteration 103/1000 | Loss: 0.00001973
Iteration 104/1000 | Loss: 0.00001972
Iteration 105/1000 | Loss: 0.00001972
Iteration 106/1000 | Loss: 0.00001972
Iteration 107/1000 | Loss: 0.00001971
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001969
Iteration 112/1000 | Loss: 0.00001969
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001969
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001969
Iteration 117/1000 | Loss: 0.00001968
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001968
Iteration 120/1000 | Loss: 0.00001968
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001967
Iteration 126/1000 | Loss: 0.00001967
Iteration 127/1000 | Loss: 0.00001967
Iteration 128/1000 | Loss: 0.00001967
Iteration 129/1000 | Loss: 0.00001966
Iteration 130/1000 | Loss: 0.00001966
Iteration 131/1000 | Loss: 0.00001966
Iteration 132/1000 | Loss: 0.00001965
Iteration 133/1000 | Loss: 0.00001965
Iteration 134/1000 | Loss: 0.00001965
Iteration 135/1000 | Loss: 0.00001965
Iteration 136/1000 | Loss: 0.00001965
Iteration 137/1000 | Loss: 0.00001965
Iteration 138/1000 | Loss: 0.00001965
Iteration 139/1000 | Loss: 0.00001965
Iteration 140/1000 | Loss: 0.00001965
Iteration 141/1000 | Loss: 0.00001965
Iteration 142/1000 | Loss: 0.00001965
Iteration 143/1000 | Loss: 0.00001965
Iteration 144/1000 | Loss: 0.00001964
Iteration 145/1000 | Loss: 0.00001964
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001964
Iteration 148/1000 | Loss: 0.00001964
Iteration 149/1000 | Loss: 0.00001963
Iteration 150/1000 | Loss: 0.00001963
Iteration 151/1000 | Loss: 0.00001963
Iteration 152/1000 | Loss: 0.00001963
Iteration 153/1000 | Loss: 0.00001963
Iteration 154/1000 | Loss: 0.00001963
Iteration 155/1000 | Loss: 0.00001963
Iteration 156/1000 | Loss: 0.00001963
Iteration 157/1000 | Loss: 0.00001963
Iteration 158/1000 | Loss: 0.00001963
Iteration 159/1000 | Loss: 0.00001963
Iteration 160/1000 | Loss: 0.00001963
Iteration 161/1000 | Loss: 0.00001963
Iteration 162/1000 | Loss: 0.00001963
Iteration 163/1000 | Loss: 0.00001963
Iteration 164/1000 | Loss: 0.00001962
Iteration 165/1000 | Loss: 0.00001962
Iteration 166/1000 | Loss: 0.00001962
Iteration 167/1000 | Loss: 0.00001962
Iteration 168/1000 | Loss: 0.00001962
Iteration 169/1000 | Loss: 0.00001962
Iteration 170/1000 | Loss: 0.00001962
Iteration 171/1000 | Loss: 0.00001962
Iteration 172/1000 | Loss: 0.00001962
Iteration 173/1000 | Loss: 0.00001962
Iteration 174/1000 | Loss: 0.00001961
Iteration 175/1000 | Loss: 0.00001961
Iteration 176/1000 | Loss: 0.00001961
Iteration 177/1000 | Loss: 0.00001961
Iteration 178/1000 | Loss: 0.00001961
Iteration 179/1000 | Loss: 0.00001961
Iteration 180/1000 | Loss: 0.00001961
Iteration 181/1000 | Loss: 0.00001961
Iteration 182/1000 | Loss: 0.00001961
Iteration 183/1000 | Loss: 0.00001961
Iteration 184/1000 | Loss: 0.00001961
Iteration 185/1000 | Loss: 0.00001961
Iteration 186/1000 | Loss: 0.00001961
Iteration 187/1000 | Loss: 0.00001961
Iteration 188/1000 | Loss: 0.00001961
Iteration 189/1000 | Loss: 0.00001960
Iteration 190/1000 | Loss: 0.00001960
Iteration 191/1000 | Loss: 0.00001960
Iteration 192/1000 | Loss: 0.00001960
Iteration 193/1000 | Loss: 0.00001960
Iteration 194/1000 | Loss: 0.00001960
Iteration 195/1000 | Loss: 0.00001960
Iteration 196/1000 | Loss: 0.00001960
Iteration 197/1000 | Loss: 0.00001960
Iteration 198/1000 | Loss: 0.00001960
Iteration 199/1000 | Loss: 0.00001960
Iteration 200/1000 | Loss: 0.00001960
Iteration 201/1000 | Loss: 0.00001960
Iteration 202/1000 | Loss: 0.00001960
Iteration 203/1000 | Loss: 0.00001960
Iteration 204/1000 | Loss: 0.00001960
Iteration 205/1000 | Loss: 0.00001959
Iteration 206/1000 | Loss: 0.00001959
Iteration 207/1000 | Loss: 0.00001959
Iteration 208/1000 | Loss: 0.00001959
Iteration 209/1000 | Loss: 0.00001959
Iteration 210/1000 | Loss: 0.00001959
Iteration 211/1000 | Loss: 0.00001959
Iteration 212/1000 | Loss: 0.00001959
Iteration 213/1000 | Loss: 0.00001959
Iteration 214/1000 | Loss: 0.00001959
Iteration 215/1000 | Loss: 0.00001959
Iteration 216/1000 | Loss: 0.00001959
Iteration 217/1000 | Loss: 0.00001959
Iteration 218/1000 | Loss: 0.00001959
Iteration 219/1000 | Loss: 0.00001959
Iteration 220/1000 | Loss: 0.00001959
Iteration 221/1000 | Loss: 0.00001959
Iteration 222/1000 | Loss: 0.00001958
Iteration 223/1000 | Loss: 0.00001958
Iteration 224/1000 | Loss: 0.00001958
Iteration 225/1000 | Loss: 0.00001958
Iteration 226/1000 | Loss: 0.00001958
Iteration 227/1000 | Loss: 0.00001958
Iteration 228/1000 | Loss: 0.00001958
Iteration 229/1000 | Loss: 0.00001958
Iteration 230/1000 | Loss: 0.00001958
Iteration 231/1000 | Loss: 0.00001958
Iteration 232/1000 | Loss: 0.00001958
Iteration 233/1000 | Loss: 0.00001958
Iteration 234/1000 | Loss: 0.00001958
Iteration 235/1000 | Loss: 0.00001958
Iteration 236/1000 | Loss: 0.00001958
Iteration 237/1000 | Loss: 0.00001958
Iteration 238/1000 | Loss: 0.00001958
Iteration 239/1000 | Loss: 0.00001958
Iteration 240/1000 | Loss: 0.00001957
Iteration 241/1000 | Loss: 0.00001957
Iteration 242/1000 | Loss: 0.00001957
Iteration 243/1000 | Loss: 0.00001957
Iteration 244/1000 | Loss: 0.00001957
Iteration 245/1000 | Loss: 0.00001957
Iteration 246/1000 | Loss: 0.00001957
Iteration 247/1000 | Loss: 0.00001957
Iteration 248/1000 | Loss: 0.00001957
Iteration 249/1000 | Loss: 0.00001957
Iteration 250/1000 | Loss: 0.00001957
Iteration 251/1000 | Loss: 0.00001957
Iteration 252/1000 | Loss: 0.00001957
Iteration 253/1000 | Loss: 0.00001957
Iteration 254/1000 | Loss: 0.00001957
Iteration 255/1000 | Loss: 0.00001957
Iteration 256/1000 | Loss: 0.00001956
Iteration 257/1000 | Loss: 0.00001956
Iteration 258/1000 | Loss: 0.00001956
Iteration 259/1000 | Loss: 0.00001956
Iteration 260/1000 | Loss: 0.00001956
Iteration 261/1000 | Loss: 0.00001956
Iteration 262/1000 | Loss: 0.00001955
Iteration 263/1000 | Loss: 0.00001955
Iteration 264/1000 | Loss: 0.00001955
Iteration 265/1000 | Loss: 0.00001955
Iteration 266/1000 | Loss: 0.00001954
Iteration 267/1000 | Loss: 0.00001954
Iteration 268/1000 | Loss: 0.00001954
Iteration 269/1000 | Loss: 0.00001954
Iteration 270/1000 | Loss: 0.00001954
Iteration 271/1000 | Loss: 0.00001954
Iteration 272/1000 | Loss: 0.00001954
Iteration 273/1000 | Loss: 0.00001954
Iteration 274/1000 | Loss: 0.00001954
Iteration 275/1000 | Loss: 0.00001954
Iteration 276/1000 | Loss: 0.00001954
Iteration 277/1000 | Loss: 0.00001953
Iteration 278/1000 | Loss: 0.00001953
Iteration 279/1000 | Loss: 0.00001953
Iteration 280/1000 | Loss: 0.00001953
Iteration 281/1000 | Loss: 0.00001953
Iteration 282/1000 | Loss: 0.00001953
Iteration 283/1000 | Loss: 0.00001953
Iteration 284/1000 | Loss: 0.00001953
Iteration 285/1000 | Loss: 0.00001953
Iteration 286/1000 | Loss: 0.00001953
Iteration 287/1000 | Loss: 0.00001953
Iteration 288/1000 | Loss: 0.00001953
Iteration 289/1000 | Loss: 0.00001953
Iteration 290/1000 | Loss: 0.00001953
Iteration 291/1000 | Loss: 0.00001953
Iteration 292/1000 | Loss: 0.00001953
Iteration 293/1000 | Loss: 0.00001953
Iteration 294/1000 | Loss: 0.00001953
Iteration 295/1000 | Loss: 0.00001953
Iteration 296/1000 | Loss: 0.00001953
Iteration 297/1000 | Loss: 0.00001953
Iteration 298/1000 | Loss: 0.00001953
Iteration 299/1000 | Loss: 0.00001953
Iteration 300/1000 | Loss: 0.00001953
Iteration 301/1000 | Loss: 0.00001953
Iteration 302/1000 | Loss: 0.00001953
Iteration 303/1000 | Loss: 0.00001953
Iteration 304/1000 | Loss: 0.00001953
Iteration 305/1000 | Loss: 0.00001953
Iteration 306/1000 | Loss: 0.00001953
Iteration 307/1000 | Loss: 0.00001953
Iteration 308/1000 | Loss: 0.00001953
Iteration 309/1000 | Loss: 0.00001953
Iteration 310/1000 | Loss: 0.00001953
Iteration 311/1000 | Loss: 0.00001953
Iteration 312/1000 | Loss: 0.00001953
Iteration 313/1000 | Loss: 0.00001953
Iteration 314/1000 | Loss: 0.00001953
Iteration 315/1000 | Loss: 0.00001953
Iteration 316/1000 | Loss: 0.00001953
Iteration 317/1000 | Loss: 0.00001953
Iteration 318/1000 | Loss: 0.00001953
Iteration 319/1000 | Loss: 0.00001953
Iteration 320/1000 | Loss: 0.00001953
Iteration 321/1000 | Loss: 0.00001953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [1.9527753465808928e-05, 1.9527753465808928e-05, 1.9527753465808928e-05, 1.9527753465808928e-05, 1.9527753465808928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9527753465808928e-05

Optimization complete. Final v2v error: 3.679138422012329 mm

Highest mean error: 4.519198894500732 mm for frame 208

Lowest mean error: 3.0150160789489746 mm for frame 91

Saving results

Total time: 59.449825286865234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810503
Iteration 2/25 | Loss: 0.00142883
Iteration 3/25 | Loss: 0.00114889
Iteration 4/25 | Loss: 0.00112169
Iteration 5/25 | Loss: 0.00111925
Iteration 6/25 | Loss: 0.00111921
Iteration 7/25 | Loss: 0.00111921
Iteration 8/25 | Loss: 0.00111921
Iteration 9/25 | Loss: 0.00111921
Iteration 10/25 | Loss: 0.00111921
Iteration 11/25 | Loss: 0.00111921
Iteration 12/25 | Loss: 0.00111921
Iteration 13/25 | Loss: 0.00111921
Iteration 14/25 | Loss: 0.00111921
Iteration 15/25 | Loss: 0.00111921
Iteration 16/25 | Loss: 0.00111921
Iteration 17/25 | Loss: 0.00111921
Iteration 18/25 | Loss: 0.00111921
Iteration 19/25 | Loss: 0.00111921
Iteration 20/25 | Loss: 0.00111921
Iteration 21/25 | Loss: 0.00111921
Iteration 22/25 | Loss: 0.00111921
Iteration 23/25 | Loss: 0.00111921
Iteration 24/25 | Loss: 0.00111921
Iteration 25/25 | Loss: 0.00111921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32444119
Iteration 2/25 | Loss: 0.00061725
Iteration 3/25 | Loss: 0.00061724
Iteration 4/25 | Loss: 0.00061724
Iteration 5/25 | Loss: 0.00061724
Iteration 6/25 | Loss: 0.00061724
Iteration 7/25 | Loss: 0.00061724
Iteration 8/25 | Loss: 0.00061724
Iteration 9/25 | Loss: 0.00061724
Iteration 10/25 | Loss: 0.00061724
Iteration 11/25 | Loss: 0.00061724
Iteration 12/25 | Loss: 0.00061724
Iteration 13/25 | Loss: 0.00061724
Iteration 14/25 | Loss: 0.00061724
Iteration 15/25 | Loss: 0.00061724
Iteration 16/25 | Loss: 0.00061724
Iteration 17/25 | Loss: 0.00061724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006172353168949485, 0.0006172353168949485, 0.0006172353168949485, 0.0006172353168949485, 0.0006172353168949485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006172353168949485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061724
Iteration 2/1000 | Loss: 0.00002501
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001707
Iteration 5/1000 | Loss: 0.00001632
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001515
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001425
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001397
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001360
Iteration 17/1000 | Loss: 0.00001357
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001345
Iteration 25/1000 | Loss: 0.00001342
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001339
Iteration 28/1000 | Loss: 0.00001338
Iteration 29/1000 | Loss: 0.00001337
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001335
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001333
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001328
Iteration 54/1000 | Loss: 0.00001328
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001324
Iteration 61/1000 | Loss: 0.00001324
Iteration 62/1000 | Loss: 0.00001324
Iteration 63/1000 | Loss: 0.00001324
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001324
Iteration 67/1000 | Loss: 0.00001324
Iteration 68/1000 | Loss: 0.00001324
Iteration 69/1000 | Loss: 0.00001323
Iteration 70/1000 | Loss: 0.00001323
Iteration 71/1000 | Loss: 0.00001323
Iteration 72/1000 | Loss: 0.00001322
Iteration 73/1000 | Loss: 0.00001322
Iteration 74/1000 | Loss: 0.00001322
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001322
Iteration 77/1000 | Loss: 0.00001321
Iteration 78/1000 | Loss: 0.00001321
Iteration 79/1000 | Loss: 0.00001321
Iteration 80/1000 | Loss: 0.00001321
Iteration 81/1000 | Loss: 0.00001321
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001321
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001320
Iteration 89/1000 | Loss: 0.00001320
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001318
Iteration 96/1000 | Loss: 0.00001318
Iteration 97/1000 | Loss: 0.00001318
Iteration 98/1000 | Loss: 0.00001318
Iteration 99/1000 | Loss: 0.00001318
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001318
Iteration 102/1000 | Loss: 0.00001318
Iteration 103/1000 | Loss: 0.00001318
Iteration 104/1000 | Loss: 0.00001318
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001318
Iteration 123/1000 | Loss: 0.00001318
Iteration 124/1000 | Loss: 0.00001318
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001318
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001318
Iteration 131/1000 | Loss: 0.00001318
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001318
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001318
Iteration 141/1000 | Loss: 0.00001318
Iteration 142/1000 | Loss: 0.00001318
Iteration 143/1000 | Loss: 0.00001318
Iteration 144/1000 | Loss: 0.00001318
Iteration 145/1000 | Loss: 0.00001318
Iteration 146/1000 | Loss: 0.00001318
Iteration 147/1000 | Loss: 0.00001318
Iteration 148/1000 | Loss: 0.00001318
Iteration 149/1000 | Loss: 0.00001318
Iteration 150/1000 | Loss: 0.00001318
Iteration 151/1000 | Loss: 0.00001318
Iteration 152/1000 | Loss: 0.00001318
Iteration 153/1000 | Loss: 0.00001318
Iteration 154/1000 | Loss: 0.00001318
Iteration 155/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3183216651668772e-05, 1.3183216651668772e-05, 1.3183216651668772e-05, 1.3183216651668772e-05, 1.3183216651668772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3183216651668772e-05

Optimization complete. Final v2v error: 3.0293209552764893 mm

Highest mean error: 3.144563913345337 mm for frame 30

Lowest mean error: 2.9534032344818115 mm for frame 112

Saving results

Total time: 35.59854054450989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818645
Iteration 2/25 | Loss: 0.00137298
Iteration 3/25 | Loss: 0.00115651
Iteration 4/25 | Loss: 0.00114922
Iteration 5/25 | Loss: 0.00114867
Iteration 6/25 | Loss: 0.00114867
Iteration 7/25 | Loss: 0.00114867
Iteration 8/25 | Loss: 0.00114867
Iteration 9/25 | Loss: 0.00114867
Iteration 10/25 | Loss: 0.00114867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011486721923574805, 0.0011486721923574805, 0.0011486721923574805, 0.0011486721923574805, 0.0011486721923574805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011486721923574805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97157300
Iteration 2/25 | Loss: 0.00043139
Iteration 3/25 | Loss: 0.00043138
Iteration 4/25 | Loss: 0.00043138
Iteration 5/25 | Loss: 0.00043138
Iteration 6/25 | Loss: 0.00043138
Iteration 7/25 | Loss: 0.00043138
Iteration 8/25 | Loss: 0.00043138
Iteration 9/25 | Loss: 0.00043138
Iteration 10/25 | Loss: 0.00043138
Iteration 11/25 | Loss: 0.00043138
Iteration 12/25 | Loss: 0.00043138
Iteration 13/25 | Loss: 0.00043138
Iteration 14/25 | Loss: 0.00043138
Iteration 15/25 | Loss: 0.00043138
Iteration 16/25 | Loss: 0.00043138
Iteration 17/25 | Loss: 0.00043138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004313759854994714, 0.0004313759854994714, 0.0004313759854994714, 0.0004313759854994714, 0.0004313759854994714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004313759854994714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043138
Iteration 2/1000 | Loss: 0.00002739
Iteration 3/1000 | Loss: 0.00002278
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001997
Iteration 7/1000 | Loss: 0.00001956
Iteration 8/1000 | Loss: 0.00001932
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001907
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001889
Iteration 13/1000 | Loss: 0.00001888
Iteration 14/1000 | Loss: 0.00001881
Iteration 15/1000 | Loss: 0.00001880
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001866
Iteration 18/1000 | Loss: 0.00001865
Iteration 19/1000 | Loss: 0.00001861
Iteration 20/1000 | Loss: 0.00001861
Iteration 21/1000 | Loss: 0.00001860
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001860
Iteration 24/1000 | Loss: 0.00001860
Iteration 25/1000 | Loss: 0.00001860
Iteration 26/1000 | Loss: 0.00001860
Iteration 27/1000 | Loss: 0.00001860
Iteration 28/1000 | Loss: 0.00001860
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001858
Iteration 32/1000 | Loss: 0.00001857
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001857
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001851
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001850
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001850
Iteration 54/1000 | Loss: 0.00001850
Iteration 55/1000 | Loss: 0.00001849
Iteration 56/1000 | Loss: 0.00001848
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001847
Iteration 59/1000 | Loss: 0.00001847
Iteration 60/1000 | Loss: 0.00001847
Iteration 61/1000 | Loss: 0.00001847
Iteration 62/1000 | Loss: 0.00001847
Iteration 63/1000 | Loss: 0.00001847
Iteration 64/1000 | Loss: 0.00001846
Iteration 65/1000 | Loss: 0.00001846
Iteration 66/1000 | Loss: 0.00001846
Iteration 67/1000 | Loss: 0.00001846
Iteration 68/1000 | Loss: 0.00001846
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001844
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001843
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001839
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00001839
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001839
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001837
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8366912627243437e-05, 1.8366912627243437e-05, 1.8366912627243437e-05, 1.8366912627243437e-05, 1.8366912627243437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8366912627243437e-05

Optimization complete. Final v2v error: 3.556614875793457 mm

Highest mean error: 3.7061171531677246 mm for frame 102

Lowest mean error: 3.423830270767212 mm for frame 239

Saving results

Total time: 37.07051205635071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807807
Iteration 2/25 | Loss: 0.00148688
Iteration 3/25 | Loss: 0.00116762
Iteration 4/25 | Loss: 0.00113678
Iteration 5/25 | Loss: 0.00113229
Iteration 6/25 | Loss: 0.00113147
Iteration 7/25 | Loss: 0.00113147
Iteration 8/25 | Loss: 0.00113147
Iteration 9/25 | Loss: 0.00113147
Iteration 10/25 | Loss: 0.00113147
Iteration 11/25 | Loss: 0.00113147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011314695002511144, 0.0011314695002511144, 0.0011314695002511144, 0.0011314695002511144, 0.0011314695002511144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011314695002511144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28118408
Iteration 2/25 | Loss: 0.00064605
Iteration 3/25 | Loss: 0.00064605
Iteration 4/25 | Loss: 0.00064605
Iteration 5/25 | Loss: 0.00064604
Iteration 6/25 | Loss: 0.00064604
Iteration 7/25 | Loss: 0.00064604
Iteration 8/25 | Loss: 0.00064604
Iteration 9/25 | Loss: 0.00064604
Iteration 10/25 | Loss: 0.00064604
Iteration 11/25 | Loss: 0.00064604
Iteration 12/25 | Loss: 0.00064604
Iteration 13/25 | Loss: 0.00064604
Iteration 14/25 | Loss: 0.00064604
Iteration 15/25 | Loss: 0.00064604
Iteration 16/25 | Loss: 0.00064604
Iteration 17/25 | Loss: 0.00064604
Iteration 18/25 | Loss: 0.00064604
Iteration 19/25 | Loss: 0.00064604
Iteration 20/25 | Loss: 0.00064604
Iteration 21/25 | Loss: 0.00064604
Iteration 22/25 | Loss: 0.00064604
Iteration 23/25 | Loss: 0.00064604
Iteration 24/25 | Loss: 0.00064604
Iteration 25/25 | Loss: 0.00064604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064604
Iteration 2/1000 | Loss: 0.00003420
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001779
Iteration 5/1000 | Loss: 0.00001668
Iteration 6/1000 | Loss: 0.00001583
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001420
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00001412
Iteration 14/1000 | Loss: 0.00001410
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001395
Iteration 17/1000 | Loss: 0.00001394
Iteration 18/1000 | Loss: 0.00001393
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001391
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001390
Iteration 24/1000 | Loss: 0.00001386
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001382
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001381
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001379
Iteration 34/1000 | Loss: 0.00001379
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001379
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001368
Iteration 62/1000 | Loss: 0.00001368
Iteration 63/1000 | Loss: 0.00001368
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001366
Iteration 72/1000 | Loss: 0.00001365
Iteration 73/1000 | Loss: 0.00001365
Iteration 74/1000 | Loss: 0.00001365
Iteration 75/1000 | Loss: 0.00001364
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001364
Iteration 78/1000 | Loss: 0.00001363
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001360
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001353
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001351
Iteration 118/1000 | Loss: 0.00001351
Iteration 119/1000 | Loss: 0.00001351
Iteration 120/1000 | Loss: 0.00001351
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001350
Iteration 123/1000 | Loss: 0.00001350
Iteration 124/1000 | Loss: 0.00001350
Iteration 125/1000 | Loss: 0.00001350
Iteration 126/1000 | Loss: 0.00001349
Iteration 127/1000 | Loss: 0.00001349
Iteration 128/1000 | Loss: 0.00001349
Iteration 129/1000 | Loss: 0.00001349
Iteration 130/1000 | Loss: 0.00001349
Iteration 131/1000 | Loss: 0.00001349
Iteration 132/1000 | Loss: 0.00001349
Iteration 133/1000 | Loss: 0.00001349
Iteration 134/1000 | Loss: 0.00001349
Iteration 135/1000 | Loss: 0.00001348
Iteration 136/1000 | Loss: 0.00001348
Iteration 137/1000 | Loss: 0.00001348
Iteration 138/1000 | Loss: 0.00001348
Iteration 139/1000 | Loss: 0.00001348
Iteration 140/1000 | Loss: 0.00001348
Iteration 141/1000 | Loss: 0.00001348
Iteration 142/1000 | Loss: 0.00001348
Iteration 143/1000 | Loss: 0.00001348
Iteration 144/1000 | Loss: 0.00001348
Iteration 145/1000 | Loss: 0.00001348
Iteration 146/1000 | Loss: 0.00001348
Iteration 147/1000 | Loss: 0.00001348
Iteration 148/1000 | Loss: 0.00001348
Iteration 149/1000 | Loss: 0.00001348
Iteration 150/1000 | Loss: 0.00001348
Iteration 151/1000 | Loss: 0.00001348
Iteration 152/1000 | Loss: 0.00001348
Iteration 153/1000 | Loss: 0.00001348
Iteration 154/1000 | Loss: 0.00001348
Iteration 155/1000 | Loss: 0.00001348
Iteration 156/1000 | Loss: 0.00001348
Iteration 157/1000 | Loss: 0.00001348
Iteration 158/1000 | Loss: 0.00001348
Iteration 159/1000 | Loss: 0.00001348
Iteration 160/1000 | Loss: 0.00001348
Iteration 161/1000 | Loss: 0.00001348
Iteration 162/1000 | Loss: 0.00001348
Iteration 163/1000 | Loss: 0.00001348
Iteration 164/1000 | Loss: 0.00001348
Iteration 165/1000 | Loss: 0.00001348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.3481185305863619e-05, 1.3481185305863619e-05, 1.3481185305863619e-05, 1.3481185305863619e-05, 1.3481185305863619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3481185305863619e-05

Optimization complete. Final v2v error: 3.110548734664917 mm

Highest mean error: 3.8444619178771973 mm for frame 89

Lowest mean error: 2.6827569007873535 mm for frame 53

Saving results

Total time: 44.980679512023926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800106
Iteration 2/25 | Loss: 0.00140344
Iteration 3/25 | Loss: 0.00120062
Iteration 4/25 | Loss: 0.00118702
Iteration 5/25 | Loss: 0.00118628
Iteration 6/25 | Loss: 0.00118628
Iteration 7/25 | Loss: 0.00118628
Iteration 8/25 | Loss: 0.00118628
Iteration 9/25 | Loss: 0.00118628
Iteration 10/25 | Loss: 0.00118628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001186277950182557, 0.001186277950182557, 0.001186277950182557, 0.001186277950182557, 0.001186277950182557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001186277950182557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32664728
Iteration 2/25 | Loss: 0.00054363
Iteration 3/25 | Loss: 0.00054362
Iteration 4/25 | Loss: 0.00054362
Iteration 5/25 | Loss: 0.00054362
Iteration 6/25 | Loss: 0.00054362
Iteration 7/25 | Loss: 0.00054362
Iteration 8/25 | Loss: 0.00054362
Iteration 9/25 | Loss: 0.00054362
Iteration 10/25 | Loss: 0.00054362
Iteration 11/25 | Loss: 0.00054362
Iteration 12/25 | Loss: 0.00054362
Iteration 13/25 | Loss: 0.00054362
Iteration 14/25 | Loss: 0.00054362
Iteration 15/25 | Loss: 0.00054362
Iteration 16/25 | Loss: 0.00054362
Iteration 17/25 | Loss: 0.00054362
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005436210194602609, 0.0005436210194602609, 0.0005436210194602609, 0.0005436210194602609, 0.0005436210194602609]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005436210194602609

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054362
Iteration 2/1000 | Loss: 0.00002452
Iteration 3/1000 | Loss: 0.00001978
Iteration 4/1000 | Loss: 0.00001860
Iteration 5/1000 | Loss: 0.00001781
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001633
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001620
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001615
Iteration 14/1000 | Loss: 0.00001612
Iteration 15/1000 | Loss: 0.00001610
Iteration 16/1000 | Loss: 0.00001610
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001609
Iteration 19/1000 | Loss: 0.00001609
Iteration 20/1000 | Loss: 0.00001609
Iteration 21/1000 | Loss: 0.00001608
Iteration 22/1000 | Loss: 0.00001607
Iteration 23/1000 | Loss: 0.00001606
Iteration 24/1000 | Loss: 0.00001603
Iteration 25/1000 | Loss: 0.00001603
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001600
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001599
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001596
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001596
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001593
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001592
Iteration 50/1000 | Loss: 0.00001592
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001589
Iteration 62/1000 | Loss: 0.00001589
Iteration 63/1000 | Loss: 0.00001589
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001587
Iteration 77/1000 | Loss: 0.00001587
Iteration 78/1000 | Loss: 0.00001587
Iteration 79/1000 | Loss: 0.00001587
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001585
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001585
Iteration 94/1000 | Loss: 0.00001585
Iteration 95/1000 | Loss: 0.00001585
Iteration 96/1000 | Loss: 0.00001585
Iteration 97/1000 | Loss: 0.00001585
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.584661367814988e-05, 1.584661367814988e-05, 1.584661367814988e-05, 1.584661367814988e-05, 1.584661367814988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.584661367814988e-05

Optimization complete. Final v2v error: 3.328226089477539 mm

Highest mean error: 3.6535398960113525 mm for frame 55

Lowest mean error: 3.201303720474243 mm for frame 141

Saving results

Total time: 30.14346170425415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061130
Iteration 2/25 | Loss: 0.00264193
Iteration 3/25 | Loss: 0.00170542
Iteration 4/25 | Loss: 0.00133764
Iteration 5/25 | Loss: 0.00122074
Iteration 6/25 | Loss: 0.00120008
Iteration 7/25 | Loss: 0.00119447
Iteration 8/25 | Loss: 0.00118972
Iteration 9/25 | Loss: 0.00118824
Iteration 10/25 | Loss: 0.00118764
Iteration 11/25 | Loss: 0.00119161
Iteration 12/25 | Loss: 0.00118631
Iteration 13/25 | Loss: 0.00118510
Iteration 14/25 | Loss: 0.00118491
Iteration 15/25 | Loss: 0.00118491
Iteration 16/25 | Loss: 0.00118490
Iteration 17/25 | Loss: 0.00118490
Iteration 18/25 | Loss: 0.00118490
Iteration 19/25 | Loss: 0.00118490
Iteration 20/25 | Loss: 0.00118490
Iteration 21/25 | Loss: 0.00118490
Iteration 22/25 | Loss: 0.00118490
Iteration 23/25 | Loss: 0.00118490
Iteration 24/25 | Loss: 0.00118490
Iteration 25/25 | Loss: 0.00118490

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33744466
Iteration 2/25 | Loss: 0.00121467
Iteration 3/25 | Loss: 0.00121466
Iteration 4/25 | Loss: 0.00121466
Iteration 5/25 | Loss: 0.00121466
Iteration 6/25 | Loss: 0.00121466
Iteration 7/25 | Loss: 0.00121466
Iteration 8/25 | Loss: 0.00121466
Iteration 9/25 | Loss: 0.00121466
Iteration 10/25 | Loss: 0.00121466
Iteration 11/25 | Loss: 0.00121466
Iteration 12/25 | Loss: 0.00121466
Iteration 13/25 | Loss: 0.00121466
Iteration 14/25 | Loss: 0.00121466
Iteration 15/25 | Loss: 0.00121466
Iteration 16/25 | Loss: 0.00121466
Iteration 17/25 | Loss: 0.00121466
Iteration 18/25 | Loss: 0.00121466
Iteration 19/25 | Loss: 0.00121466
Iteration 20/25 | Loss: 0.00121466
Iteration 21/25 | Loss: 0.00121466
Iteration 22/25 | Loss: 0.00121466
Iteration 23/25 | Loss: 0.00121466
Iteration 24/25 | Loss: 0.00121466
Iteration 25/25 | Loss: 0.00121466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121466
Iteration 2/1000 | Loss: 0.00007900
Iteration 3/1000 | Loss: 0.00006436
Iteration 4/1000 | Loss: 0.00004106
Iteration 5/1000 | Loss: 0.00003193
Iteration 6/1000 | Loss: 0.00004925
Iteration 7/1000 | Loss: 0.00002950
Iteration 8/1000 | Loss: 0.00002872
Iteration 9/1000 | Loss: 0.00002810
Iteration 10/1000 | Loss: 0.00002766
Iteration 11/1000 | Loss: 0.00002738
Iteration 12/1000 | Loss: 0.00020274
Iteration 13/1000 | Loss: 0.00030321
Iteration 14/1000 | Loss: 0.00032590
Iteration 15/1000 | Loss: 0.00026857
Iteration 16/1000 | Loss: 0.00033816
Iteration 17/1000 | Loss: 0.00024141
Iteration 18/1000 | Loss: 0.00002731
Iteration 19/1000 | Loss: 0.00002789
Iteration 20/1000 | Loss: 0.00002754
Iteration 21/1000 | Loss: 0.00002671
Iteration 22/1000 | Loss: 0.00002671
Iteration 23/1000 | Loss: 0.00002666
Iteration 24/1000 | Loss: 0.00002664
Iteration 25/1000 | Loss: 0.00003012
Iteration 26/1000 | Loss: 0.00002655
Iteration 27/1000 | Loss: 0.00002655
Iteration 28/1000 | Loss: 0.00002654
Iteration 29/1000 | Loss: 0.00002757
Iteration 30/1000 | Loss: 0.00019689
Iteration 31/1000 | Loss: 0.00002648
Iteration 32/1000 | Loss: 0.00002637
Iteration 33/1000 | Loss: 0.00002875
Iteration 34/1000 | Loss: 0.00002629
Iteration 35/1000 | Loss: 0.00002629
Iteration 36/1000 | Loss: 0.00002629
Iteration 37/1000 | Loss: 0.00002629
Iteration 38/1000 | Loss: 0.00002629
Iteration 39/1000 | Loss: 0.00002629
Iteration 40/1000 | Loss: 0.00002629
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002628
Iteration 43/1000 | Loss: 0.00002699
Iteration 44/1000 | Loss: 0.00002627
Iteration 45/1000 | Loss: 0.00002627
Iteration 46/1000 | Loss: 0.00002627
Iteration 47/1000 | Loss: 0.00002627
Iteration 48/1000 | Loss: 0.00002627
Iteration 49/1000 | Loss: 0.00002627
Iteration 50/1000 | Loss: 0.00002627
Iteration 51/1000 | Loss: 0.00002627
Iteration 52/1000 | Loss: 0.00002627
Iteration 53/1000 | Loss: 0.00002626
Iteration 54/1000 | Loss: 0.00002625
Iteration 55/1000 | Loss: 0.00002625
Iteration 56/1000 | Loss: 0.00002615
Iteration 57/1000 | Loss: 0.00002615
Iteration 58/1000 | Loss: 0.00002613
Iteration 59/1000 | Loss: 0.00002613
Iteration 60/1000 | Loss: 0.00002613
Iteration 61/1000 | Loss: 0.00002612
Iteration 62/1000 | Loss: 0.00002612
Iteration 63/1000 | Loss: 0.00002612
Iteration 64/1000 | Loss: 0.00002612
Iteration 65/1000 | Loss: 0.00002611
Iteration 66/1000 | Loss: 0.00002611
Iteration 67/1000 | Loss: 0.00002611
Iteration 68/1000 | Loss: 0.00002611
Iteration 69/1000 | Loss: 0.00002611
Iteration 70/1000 | Loss: 0.00002611
Iteration 71/1000 | Loss: 0.00002611
Iteration 72/1000 | Loss: 0.00002611
Iteration 73/1000 | Loss: 0.00002610
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002610
Iteration 76/1000 | Loss: 0.00002610
Iteration 77/1000 | Loss: 0.00002609
Iteration 78/1000 | Loss: 0.00002609
Iteration 79/1000 | Loss: 0.00002609
Iteration 80/1000 | Loss: 0.00002608
Iteration 81/1000 | Loss: 0.00002608
Iteration 82/1000 | Loss: 0.00002608
Iteration 83/1000 | Loss: 0.00002608
Iteration 84/1000 | Loss: 0.00002608
Iteration 85/1000 | Loss: 0.00002608
Iteration 86/1000 | Loss: 0.00002608
Iteration 87/1000 | Loss: 0.00002608
Iteration 88/1000 | Loss: 0.00002608
Iteration 89/1000 | Loss: 0.00021641
Iteration 90/1000 | Loss: 0.00041174
Iteration 91/1000 | Loss: 0.00016554
Iteration 92/1000 | Loss: 0.00003117
Iteration 93/1000 | Loss: 0.00002700
Iteration 94/1000 | Loss: 0.00002771
Iteration 95/1000 | Loss: 0.00002624
Iteration 96/1000 | Loss: 0.00002612
Iteration 97/1000 | Loss: 0.00002612
Iteration 98/1000 | Loss: 0.00002611
Iteration 99/1000 | Loss: 0.00002611
Iteration 100/1000 | Loss: 0.00002611
Iteration 101/1000 | Loss: 0.00002611
Iteration 102/1000 | Loss: 0.00002610
Iteration 103/1000 | Loss: 0.00002610
Iteration 104/1000 | Loss: 0.00002610
Iteration 105/1000 | Loss: 0.00002610
Iteration 106/1000 | Loss: 0.00002610
Iteration 107/1000 | Loss: 0.00002609
Iteration 108/1000 | Loss: 0.00002609
Iteration 109/1000 | Loss: 0.00002609
Iteration 110/1000 | Loss: 0.00002609
Iteration 111/1000 | Loss: 0.00002608
Iteration 112/1000 | Loss: 0.00002608
Iteration 113/1000 | Loss: 0.00002608
Iteration 114/1000 | Loss: 0.00002608
Iteration 115/1000 | Loss: 0.00002608
Iteration 116/1000 | Loss: 0.00002607
Iteration 117/1000 | Loss: 0.00002607
Iteration 118/1000 | Loss: 0.00002606
Iteration 119/1000 | Loss: 0.00002606
Iteration 120/1000 | Loss: 0.00002606
Iteration 121/1000 | Loss: 0.00002605
Iteration 122/1000 | Loss: 0.00002605
Iteration 123/1000 | Loss: 0.00002605
Iteration 124/1000 | Loss: 0.00002605
Iteration 125/1000 | Loss: 0.00002605
Iteration 126/1000 | Loss: 0.00002605
Iteration 127/1000 | Loss: 0.00002605
Iteration 128/1000 | Loss: 0.00002605
Iteration 129/1000 | Loss: 0.00002605
Iteration 130/1000 | Loss: 0.00002605
Iteration 131/1000 | Loss: 0.00002604
Iteration 132/1000 | Loss: 0.00002604
Iteration 133/1000 | Loss: 0.00002604
Iteration 134/1000 | Loss: 0.00002604
Iteration 135/1000 | Loss: 0.00002604
Iteration 136/1000 | Loss: 0.00002604
Iteration 137/1000 | Loss: 0.00002604
Iteration 138/1000 | Loss: 0.00002604
Iteration 139/1000 | Loss: 0.00002604
Iteration 140/1000 | Loss: 0.00002604
Iteration 141/1000 | Loss: 0.00002604
Iteration 142/1000 | Loss: 0.00002604
Iteration 143/1000 | Loss: 0.00002604
Iteration 144/1000 | Loss: 0.00002604
Iteration 145/1000 | Loss: 0.00002604
Iteration 146/1000 | Loss: 0.00002604
Iteration 147/1000 | Loss: 0.00002604
Iteration 148/1000 | Loss: 0.00002603
Iteration 149/1000 | Loss: 0.00002603
Iteration 150/1000 | Loss: 0.00002603
Iteration 151/1000 | Loss: 0.00002603
Iteration 152/1000 | Loss: 0.00002603
Iteration 153/1000 | Loss: 0.00002603
Iteration 154/1000 | Loss: 0.00002603
Iteration 155/1000 | Loss: 0.00002603
Iteration 156/1000 | Loss: 0.00002603
Iteration 157/1000 | Loss: 0.00002603
Iteration 158/1000 | Loss: 0.00002603
Iteration 159/1000 | Loss: 0.00002603
Iteration 160/1000 | Loss: 0.00002603
Iteration 161/1000 | Loss: 0.00002603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.603043867566157e-05, 2.603043867566157e-05, 2.603043867566157e-05, 2.603043867566157e-05, 2.603043867566157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.603043867566157e-05

Optimization complete. Final v2v error: 4.333785533905029 mm

Highest mean error: 4.640684127807617 mm for frame 8

Lowest mean error: 3.2386534214019775 mm for frame 0

Saving results

Total time: 87.44922280311584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862687
Iteration 2/25 | Loss: 0.00218178
Iteration 3/25 | Loss: 0.00180559
Iteration 4/25 | Loss: 0.00163602
Iteration 5/25 | Loss: 0.00138344
Iteration 6/25 | Loss: 0.00130896
Iteration 7/25 | Loss: 0.00129575
Iteration 8/25 | Loss: 0.00128515
Iteration 9/25 | Loss: 0.00128182
Iteration 10/25 | Loss: 0.00128016
Iteration 11/25 | Loss: 0.00127978
Iteration 12/25 | Loss: 0.00127970
Iteration 13/25 | Loss: 0.00127969
Iteration 14/25 | Loss: 0.00127968
Iteration 15/25 | Loss: 0.00127968
Iteration 16/25 | Loss: 0.00127968
Iteration 17/25 | Loss: 0.00127968
Iteration 18/25 | Loss: 0.00127968
Iteration 19/25 | Loss: 0.00127968
Iteration 20/25 | Loss: 0.00127968
Iteration 21/25 | Loss: 0.00127968
Iteration 22/25 | Loss: 0.00127968
Iteration 23/25 | Loss: 0.00127968
Iteration 24/25 | Loss: 0.00127968
Iteration 25/25 | Loss: 0.00127968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32885218
Iteration 2/25 | Loss: 0.00061158
Iteration 3/25 | Loss: 0.00061155
Iteration 4/25 | Loss: 0.00061155
Iteration 5/25 | Loss: 0.00061155
Iteration 6/25 | Loss: 0.00061155
Iteration 7/25 | Loss: 0.00061155
Iteration 8/25 | Loss: 0.00061155
Iteration 9/25 | Loss: 0.00061155
Iteration 10/25 | Loss: 0.00061155
Iteration 11/25 | Loss: 0.00061155
Iteration 12/25 | Loss: 0.00061155
Iteration 13/25 | Loss: 0.00061155
Iteration 14/25 | Loss: 0.00061155
Iteration 15/25 | Loss: 0.00061155
Iteration 16/25 | Loss: 0.00061155
Iteration 17/25 | Loss: 0.00061155
Iteration 18/25 | Loss: 0.00061155
Iteration 19/25 | Loss: 0.00061155
Iteration 20/25 | Loss: 0.00061155
Iteration 21/25 | Loss: 0.00061155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006115494179539382, 0.0006115494179539382, 0.0006115494179539382, 0.0006115494179539382, 0.0006115494179539382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006115494179539382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061155
Iteration 2/1000 | Loss: 0.00003788
Iteration 3/1000 | Loss: 0.00002535
Iteration 4/1000 | Loss: 0.00002309
Iteration 5/1000 | Loss: 0.00002194
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002066
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00002018
Iteration 10/1000 | Loss: 0.00002017
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001984
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001981
Iteration 15/1000 | Loss: 0.00001974
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001968
Iteration 21/1000 | Loss: 0.00001968
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001968
Iteration 24/1000 | Loss: 0.00001967
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001966
Iteration 28/1000 | Loss: 0.00001966
Iteration 29/1000 | Loss: 0.00001966
Iteration 30/1000 | Loss: 0.00001966
Iteration 31/1000 | Loss: 0.00001966
Iteration 32/1000 | Loss: 0.00001966
Iteration 33/1000 | Loss: 0.00001965
Iteration 34/1000 | Loss: 0.00001965
Iteration 35/1000 | Loss: 0.00001965
Iteration 36/1000 | Loss: 0.00001964
Iteration 37/1000 | Loss: 0.00001964
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001963
Iteration 41/1000 | Loss: 0.00001963
Iteration 42/1000 | Loss: 0.00001963
Iteration 43/1000 | Loss: 0.00001963
Iteration 44/1000 | Loss: 0.00001962
Iteration 45/1000 | Loss: 0.00001962
Iteration 46/1000 | Loss: 0.00001960
Iteration 47/1000 | Loss: 0.00001960
Iteration 48/1000 | Loss: 0.00001960
Iteration 49/1000 | Loss: 0.00001960
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001960
Iteration 54/1000 | Loss: 0.00001960
Iteration 55/1000 | Loss: 0.00001960
Iteration 56/1000 | Loss: 0.00001959
Iteration 57/1000 | Loss: 0.00001959
Iteration 58/1000 | Loss: 0.00001958
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001958
Iteration 61/1000 | Loss: 0.00001958
Iteration 62/1000 | Loss: 0.00001958
Iteration 63/1000 | Loss: 0.00001957
Iteration 64/1000 | Loss: 0.00001957
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001954
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001954
Iteration 78/1000 | Loss: 0.00001954
Iteration 79/1000 | Loss: 0.00001954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.954493927769363e-05, 1.954493927769363e-05, 1.954493927769363e-05, 1.954493927769363e-05, 1.954493927769363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.954493927769363e-05

Optimization complete. Final v2v error: 3.7476465702056885 mm

Highest mean error: 4.239961624145508 mm for frame 99

Lowest mean error: 3.585017204284668 mm for frame 114

Saving results

Total time: 50.16234517097473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460980
Iteration 2/25 | Loss: 0.00135185
Iteration 3/25 | Loss: 0.00113504
Iteration 4/25 | Loss: 0.00110622
Iteration 5/25 | Loss: 0.00110255
Iteration 6/25 | Loss: 0.00110140
Iteration 7/25 | Loss: 0.00110140
Iteration 8/25 | Loss: 0.00110140
Iteration 9/25 | Loss: 0.00110140
Iteration 10/25 | Loss: 0.00110140
Iteration 11/25 | Loss: 0.00110140
Iteration 12/25 | Loss: 0.00110140
Iteration 13/25 | Loss: 0.00110140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011013997718691826, 0.0011013997718691826, 0.0011013997718691826, 0.0011013997718691826, 0.0011013997718691826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011013997718691826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36093712
Iteration 2/25 | Loss: 0.00076352
Iteration 3/25 | Loss: 0.00076352
Iteration 4/25 | Loss: 0.00076352
Iteration 5/25 | Loss: 0.00076352
Iteration 6/25 | Loss: 0.00076352
Iteration 7/25 | Loss: 0.00076352
Iteration 8/25 | Loss: 0.00076352
Iteration 9/25 | Loss: 0.00076352
Iteration 10/25 | Loss: 0.00076352
Iteration 11/25 | Loss: 0.00076352
Iteration 12/25 | Loss: 0.00076352
Iteration 13/25 | Loss: 0.00076352
Iteration 14/25 | Loss: 0.00076352
Iteration 15/25 | Loss: 0.00076352
Iteration 16/25 | Loss: 0.00076352
Iteration 17/25 | Loss: 0.00076352
Iteration 18/25 | Loss: 0.00076352
Iteration 19/25 | Loss: 0.00076352
Iteration 20/25 | Loss: 0.00076352
Iteration 21/25 | Loss: 0.00076352
Iteration 22/25 | Loss: 0.00076352
Iteration 23/25 | Loss: 0.00076352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007635174551978707, 0.0007635174551978707, 0.0007635174551978707, 0.0007635174551978707, 0.0007635174551978707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007635174551978707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076352
Iteration 2/1000 | Loss: 0.00002445
Iteration 3/1000 | Loss: 0.00001476
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001145
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001086
Iteration 11/1000 | Loss: 0.00001072
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001065
Iteration 14/1000 | Loss: 0.00001063
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001062
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001059
Iteration 19/1000 | Loss: 0.00001058
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001057
Iteration 23/1000 | Loss: 0.00001055
Iteration 24/1000 | Loss: 0.00001054
Iteration 25/1000 | Loss: 0.00001053
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001044
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001044
Iteration 42/1000 | Loss: 0.00001044
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001044
Iteration 45/1000 | Loss: 0.00001044
Iteration 46/1000 | Loss: 0.00001043
Iteration 47/1000 | Loss: 0.00001042
Iteration 48/1000 | Loss: 0.00001042
Iteration 49/1000 | Loss: 0.00001042
Iteration 50/1000 | Loss: 0.00001042
Iteration 51/1000 | Loss: 0.00001042
Iteration 52/1000 | Loss: 0.00001042
Iteration 53/1000 | Loss: 0.00001042
Iteration 54/1000 | Loss: 0.00001042
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001042
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001039
Iteration 63/1000 | Loss: 0.00001039
Iteration 64/1000 | Loss: 0.00001039
Iteration 65/1000 | Loss: 0.00001038
Iteration 66/1000 | Loss: 0.00001038
Iteration 67/1000 | Loss: 0.00001038
Iteration 68/1000 | Loss: 0.00001038
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001037
Iteration 71/1000 | Loss: 0.00001037
Iteration 72/1000 | Loss: 0.00001037
Iteration 73/1000 | Loss: 0.00001036
Iteration 74/1000 | Loss: 0.00001036
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001035
Iteration 78/1000 | Loss: 0.00001035
Iteration 79/1000 | Loss: 0.00001035
Iteration 80/1000 | Loss: 0.00001034
Iteration 81/1000 | Loss: 0.00001034
Iteration 82/1000 | Loss: 0.00001034
Iteration 83/1000 | Loss: 0.00001033
Iteration 84/1000 | Loss: 0.00001033
Iteration 85/1000 | Loss: 0.00001033
Iteration 86/1000 | Loss: 0.00001033
Iteration 87/1000 | Loss: 0.00001033
Iteration 88/1000 | Loss: 0.00001033
Iteration 89/1000 | Loss: 0.00001033
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001032
Iteration 92/1000 | Loss: 0.00001032
Iteration 93/1000 | Loss: 0.00001032
Iteration 94/1000 | Loss: 0.00001032
Iteration 95/1000 | Loss: 0.00001031
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001031
Iteration 98/1000 | Loss: 0.00001031
Iteration 99/1000 | Loss: 0.00001031
Iteration 100/1000 | Loss: 0.00001031
Iteration 101/1000 | Loss: 0.00001030
Iteration 102/1000 | Loss: 0.00001030
Iteration 103/1000 | Loss: 0.00001030
Iteration 104/1000 | Loss: 0.00001030
Iteration 105/1000 | Loss: 0.00001030
Iteration 106/1000 | Loss: 0.00001030
Iteration 107/1000 | Loss: 0.00001030
Iteration 108/1000 | Loss: 0.00001029
Iteration 109/1000 | Loss: 0.00001029
Iteration 110/1000 | Loss: 0.00001029
Iteration 111/1000 | Loss: 0.00001028
Iteration 112/1000 | Loss: 0.00001028
Iteration 113/1000 | Loss: 0.00001028
Iteration 114/1000 | Loss: 0.00001028
Iteration 115/1000 | Loss: 0.00001028
Iteration 116/1000 | Loss: 0.00001028
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001027
Iteration 119/1000 | Loss: 0.00001027
Iteration 120/1000 | Loss: 0.00001027
Iteration 121/1000 | Loss: 0.00001027
Iteration 122/1000 | Loss: 0.00001027
Iteration 123/1000 | Loss: 0.00001026
Iteration 124/1000 | Loss: 0.00001026
Iteration 125/1000 | Loss: 0.00001026
Iteration 126/1000 | Loss: 0.00001026
Iteration 127/1000 | Loss: 0.00001025
Iteration 128/1000 | Loss: 0.00001025
Iteration 129/1000 | Loss: 0.00001025
Iteration 130/1000 | Loss: 0.00001025
Iteration 131/1000 | Loss: 0.00001025
Iteration 132/1000 | Loss: 0.00001024
Iteration 133/1000 | Loss: 0.00001024
Iteration 134/1000 | Loss: 0.00001024
Iteration 135/1000 | Loss: 0.00001024
Iteration 136/1000 | Loss: 0.00001023
Iteration 137/1000 | Loss: 0.00001023
Iteration 138/1000 | Loss: 0.00001023
Iteration 139/1000 | Loss: 0.00001023
Iteration 140/1000 | Loss: 0.00001023
Iteration 141/1000 | Loss: 0.00001023
Iteration 142/1000 | Loss: 0.00001022
Iteration 143/1000 | Loss: 0.00001022
Iteration 144/1000 | Loss: 0.00001022
Iteration 145/1000 | Loss: 0.00001022
Iteration 146/1000 | Loss: 0.00001021
Iteration 147/1000 | Loss: 0.00001021
Iteration 148/1000 | Loss: 0.00001021
Iteration 149/1000 | Loss: 0.00001021
Iteration 150/1000 | Loss: 0.00001021
Iteration 151/1000 | Loss: 0.00001021
Iteration 152/1000 | Loss: 0.00001021
Iteration 153/1000 | Loss: 0.00001021
Iteration 154/1000 | Loss: 0.00001020
Iteration 155/1000 | Loss: 0.00001020
Iteration 156/1000 | Loss: 0.00001020
Iteration 157/1000 | Loss: 0.00001020
Iteration 158/1000 | Loss: 0.00001020
Iteration 159/1000 | Loss: 0.00001020
Iteration 160/1000 | Loss: 0.00001020
Iteration 161/1000 | Loss: 0.00001020
Iteration 162/1000 | Loss: 0.00001020
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001019
Iteration 165/1000 | Loss: 0.00001019
Iteration 166/1000 | Loss: 0.00001019
Iteration 167/1000 | Loss: 0.00001019
Iteration 168/1000 | Loss: 0.00001019
Iteration 169/1000 | Loss: 0.00001019
Iteration 170/1000 | Loss: 0.00001019
Iteration 171/1000 | Loss: 0.00001018
Iteration 172/1000 | Loss: 0.00001018
Iteration 173/1000 | Loss: 0.00001018
Iteration 174/1000 | Loss: 0.00001018
Iteration 175/1000 | Loss: 0.00001018
Iteration 176/1000 | Loss: 0.00001018
Iteration 177/1000 | Loss: 0.00001018
Iteration 178/1000 | Loss: 0.00001018
Iteration 179/1000 | Loss: 0.00001018
Iteration 180/1000 | Loss: 0.00001018
Iteration 181/1000 | Loss: 0.00001018
Iteration 182/1000 | Loss: 0.00001018
Iteration 183/1000 | Loss: 0.00001018
Iteration 184/1000 | Loss: 0.00001018
Iteration 185/1000 | Loss: 0.00001018
Iteration 186/1000 | Loss: 0.00001018
Iteration 187/1000 | Loss: 0.00001018
Iteration 188/1000 | Loss: 0.00001018
Iteration 189/1000 | Loss: 0.00001018
Iteration 190/1000 | Loss: 0.00001018
Iteration 191/1000 | Loss: 0.00001018
Iteration 192/1000 | Loss: 0.00001018
Iteration 193/1000 | Loss: 0.00001018
Iteration 194/1000 | Loss: 0.00001018
Iteration 195/1000 | Loss: 0.00001018
Iteration 196/1000 | Loss: 0.00001018
Iteration 197/1000 | Loss: 0.00001018
Iteration 198/1000 | Loss: 0.00001018
Iteration 199/1000 | Loss: 0.00001018
Iteration 200/1000 | Loss: 0.00001018
Iteration 201/1000 | Loss: 0.00001018
Iteration 202/1000 | Loss: 0.00001018
Iteration 203/1000 | Loss: 0.00001018
Iteration 204/1000 | Loss: 0.00001018
Iteration 205/1000 | Loss: 0.00001018
Iteration 206/1000 | Loss: 0.00001018
Iteration 207/1000 | Loss: 0.00001018
Iteration 208/1000 | Loss: 0.00001018
Iteration 209/1000 | Loss: 0.00001018
Iteration 210/1000 | Loss: 0.00001018
Iteration 211/1000 | Loss: 0.00001018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.0175417628488503e-05, 1.0175417628488503e-05, 1.0175417628488503e-05, 1.0175417628488503e-05, 1.0175417628488503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0175417628488503e-05

Optimization complete. Final v2v error: 2.680056095123291 mm

Highest mean error: 3.480203628540039 mm for frame 79

Lowest mean error: 2.3716814517974854 mm for frame 158

Saving results

Total time: 38.68027210235596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929370
Iteration 2/25 | Loss: 0.00145078
Iteration 3/25 | Loss: 0.00138608
Iteration 4/25 | Loss: 0.00118775
Iteration 5/25 | Loss: 0.00117141
Iteration 6/25 | Loss: 0.00116920
Iteration 7/25 | Loss: 0.00115364
Iteration 8/25 | Loss: 0.00115231
Iteration 9/25 | Loss: 0.00114745
Iteration 10/25 | Loss: 0.00114121
Iteration 11/25 | Loss: 0.00114021
Iteration 12/25 | Loss: 0.00113976
Iteration 13/25 | Loss: 0.00113945
Iteration 14/25 | Loss: 0.00113938
Iteration 15/25 | Loss: 0.00113938
Iteration 16/25 | Loss: 0.00113938
Iteration 17/25 | Loss: 0.00113938
Iteration 18/25 | Loss: 0.00113938
Iteration 19/25 | Loss: 0.00113938
Iteration 20/25 | Loss: 0.00113938
Iteration 21/25 | Loss: 0.00113938
Iteration 22/25 | Loss: 0.00113938
Iteration 23/25 | Loss: 0.00113937
Iteration 24/25 | Loss: 0.00113937
Iteration 25/25 | Loss: 0.00113937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47524190
Iteration 2/25 | Loss: 0.00089984
Iteration 3/25 | Loss: 0.00084204
Iteration 4/25 | Loss: 0.00084204
Iteration 5/25 | Loss: 0.00084204
Iteration 6/25 | Loss: 0.00084204
Iteration 7/25 | Loss: 0.00084204
Iteration 8/25 | Loss: 0.00084204
Iteration 9/25 | Loss: 0.00084204
Iteration 10/25 | Loss: 0.00084204
Iteration 11/25 | Loss: 0.00084204
Iteration 12/25 | Loss: 0.00084204
Iteration 13/25 | Loss: 0.00084204
Iteration 14/25 | Loss: 0.00084204
Iteration 15/25 | Loss: 0.00084204
Iteration 16/25 | Loss: 0.00084204
Iteration 17/25 | Loss: 0.00084204
Iteration 18/25 | Loss: 0.00084204
Iteration 19/25 | Loss: 0.00084204
Iteration 20/25 | Loss: 0.00084204
Iteration 21/25 | Loss: 0.00084204
Iteration 22/25 | Loss: 0.00084204
Iteration 23/25 | Loss: 0.00084204
Iteration 24/25 | Loss: 0.00084204
Iteration 25/25 | Loss: 0.00084204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084204
Iteration 2/1000 | Loss: 0.00003913
Iteration 3/1000 | Loss: 0.00016202
Iteration 4/1000 | Loss: 0.00002147
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001891
Iteration 7/1000 | Loss: 0.00001831
Iteration 8/1000 | Loss: 0.00001777
Iteration 9/1000 | Loss: 0.00002914
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00002665
Iteration 12/1000 | Loss: 0.00001683
Iteration 13/1000 | Loss: 0.00001678
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00001670
Iteration 16/1000 | Loss: 0.00044911
Iteration 17/1000 | Loss: 0.00002512
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00003793
Iteration 21/1000 | Loss: 0.00002280
Iteration 22/1000 | Loss: 0.00002016
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00003118
Iteration 25/1000 | Loss: 0.00006826
Iteration 26/1000 | Loss: 0.00001526
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001492
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001490
Iteration 35/1000 | Loss: 0.00002234
Iteration 36/1000 | Loss: 0.00001749
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001473
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001463
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001455
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001450
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001449
Iteration 88/1000 | Loss: 0.00001449
Iteration 89/1000 | Loss: 0.00004672
Iteration 90/1000 | Loss: 0.00001463
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001443
Iteration 110/1000 | Loss: 0.00001443
Iteration 111/1000 | Loss: 0.00001443
Iteration 112/1000 | Loss: 0.00001443
Iteration 113/1000 | Loss: 0.00001443
Iteration 114/1000 | Loss: 0.00001443
Iteration 115/1000 | Loss: 0.00001442
Iteration 116/1000 | Loss: 0.00001442
Iteration 117/1000 | Loss: 0.00001442
Iteration 118/1000 | Loss: 0.00001442
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001441
Iteration 133/1000 | Loss: 0.00001441
Iteration 134/1000 | Loss: 0.00001441
Iteration 135/1000 | Loss: 0.00001441
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001440
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001439
Iteration 150/1000 | Loss: 0.00001439
Iteration 151/1000 | Loss: 0.00001439
Iteration 152/1000 | Loss: 0.00001439
Iteration 153/1000 | Loss: 0.00001439
Iteration 154/1000 | Loss: 0.00001439
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.439232346456265e-05, 1.439232346456265e-05, 1.439232346456265e-05, 1.439232346456265e-05, 1.439232346456265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.439232346456265e-05

Optimization complete. Final v2v error: 3.1976842880249023 mm

Highest mean error: 4.267590045928955 mm for frame 178

Lowest mean error: 2.6350626945495605 mm for frame 200

Saving results

Total time: 81.01124215126038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785118
Iteration 2/25 | Loss: 0.00143686
Iteration 3/25 | Loss: 0.00116928
Iteration 4/25 | Loss: 0.00113645
Iteration 5/25 | Loss: 0.00112925
Iteration 6/25 | Loss: 0.00112567
Iteration 7/25 | Loss: 0.00112379
Iteration 8/25 | Loss: 0.00112359
Iteration 9/25 | Loss: 0.00112346
Iteration 10/25 | Loss: 0.00112341
Iteration 11/25 | Loss: 0.00112341
Iteration 12/25 | Loss: 0.00112341
Iteration 13/25 | Loss: 0.00112341
Iteration 14/25 | Loss: 0.00112341
Iteration 15/25 | Loss: 0.00112341
Iteration 16/25 | Loss: 0.00112341
Iteration 17/25 | Loss: 0.00112340
Iteration 18/25 | Loss: 0.00112340
Iteration 19/25 | Loss: 0.00112340
Iteration 20/25 | Loss: 0.00112340
Iteration 21/25 | Loss: 0.00112340
Iteration 22/25 | Loss: 0.00112340
Iteration 23/25 | Loss: 0.00112340
Iteration 24/25 | Loss: 0.00112340
Iteration 25/25 | Loss: 0.00112340

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13589144
Iteration 2/25 | Loss: 0.00091340
Iteration 3/25 | Loss: 0.00091340
Iteration 4/25 | Loss: 0.00091339
Iteration 5/25 | Loss: 0.00091339
Iteration 6/25 | Loss: 0.00091339
Iteration 7/25 | Loss: 0.00091339
Iteration 8/25 | Loss: 0.00091339
Iteration 9/25 | Loss: 0.00091339
Iteration 10/25 | Loss: 0.00091339
Iteration 11/25 | Loss: 0.00091339
Iteration 12/25 | Loss: 0.00091339
Iteration 13/25 | Loss: 0.00091339
Iteration 14/25 | Loss: 0.00091339
Iteration 15/25 | Loss: 0.00091339
Iteration 16/25 | Loss: 0.00091339
Iteration 17/25 | Loss: 0.00091339
Iteration 18/25 | Loss: 0.00091339
Iteration 19/25 | Loss: 0.00091339
Iteration 20/25 | Loss: 0.00091339
Iteration 21/25 | Loss: 0.00091339
Iteration 22/25 | Loss: 0.00091339
Iteration 23/25 | Loss: 0.00091339
Iteration 24/25 | Loss: 0.00091339
Iteration 25/25 | Loss: 0.00091339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091339
Iteration 2/1000 | Loss: 0.00002259
Iteration 3/1000 | Loss: 0.00001793
Iteration 4/1000 | Loss: 0.00001685
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001570
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001497
Iteration 9/1000 | Loss: 0.00001913
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001420
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001402
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001400
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001400
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001398
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001397
Iteration 28/1000 | Loss: 0.00001397
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001392
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001391
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001381
Iteration 48/1000 | Loss: 0.00001381
Iteration 49/1000 | Loss: 0.00001381
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001379
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001379
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001497
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001371
Iteration 75/1000 | Loss: 0.00001371
Iteration 76/1000 | Loss: 0.00001371
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001371
Iteration 82/1000 | Loss: 0.00001371
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00001370
Iteration 85/1000 | Loss: 0.00001370
Iteration 86/1000 | Loss: 0.00001370
Iteration 87/1000 | Loss: 0.00001370
Iteration 88/1000 | Loss: 0.00001370
Iteration 89/1000 | Loss: 0.00001370
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001370
Iteration 94/1000 | Loss: 0.00001370
Iteration 95/1000 | Loss: 0.00001370
Iteration 96/1000 | Loss: 0.00001370
Iteration 97/1000 | Loss: 0.00001370
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001369
Iteration 107/1000 | Loss: 0.00001369
Iteration 108/1000 | Loss: 0.00001369
Iteration 109/1000 | Loss: 0.00001369
Iteration 110/1000 | Loss: 0.00001369
Iteration 111/1000 | Loss: 0.00001369
Iteration 112/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.3688530088984407e-05, 1.3688530088984407e-05, 1.3688530088984407e-05, 1.3688530088984407e-05, 1.3688530088984407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3688530088984407e-05

Optimization complete. Final v2v error: 3.0597617626190186 mm

Highest mean error: 5.128680229187012 mm for frame 48

Lowest mean error: 2.7009541988372803 mm for frame 151

Saving results

Total time: 49.40794849395752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021528
Iteration 2/25 | Loss: 0.00230113
Iteration 3/25 | Loss: 0.00189369
Iteration 4/25 | Loss: 0.00168979
Iteration 5/25 | Loss: 0.00153801
Iteration 6/25 | Loss: 0.00140676
Iteration 7/25 | Loss: 0.00138635
Iteration 8/25 | Loss: 0.00136318
Iteration 9/25 | Loss: 0.00129142
Iteration 10/25 | Loss: 0.00122348
Iteration 11/25 | Loss: 0.00120172
Iteration 12/25 | Loss: 0.00120163
Iteration 13/25 | Loss: 0.00120003
Iteration 14/25 | Loss: 0.00119643
Iteration 15/25 | Loss: 0.00119651
Iteration 16/25 | Loss: 0.00119604
Iteration 17/25 | Loss: 0.00119557
Iteration 18/25 | Loss: 0.00119572
Iteration 19/25 | Loss: 0.00119518
Iteration 20/25 | Loss: 0.00119509
Iteration 21/25 | Loss: 0.00119492
Iteration 22/25 | Loss: 0.00119508
Iteration 23/25 | Loss: 0.00119454
Iteration 24/25 | Loss: 0.00119393
Iteration 25/25 | Loss: 0.00119773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37690270
Iteration 2/25 | Loss: 0.00073712
Iteration 3/25 | Loss: 0.00073712
Iteration 4/25 | Loss: 0.00073712
Iteration 5/25 | Loss: 0.00073712
Iteration 6/25 | Loss: 0.00073712
Iteration 7/25 | Loss: 0.00073712
Iteration 8/25 | Loss: 0.00073712
Iteration 9/25 | Loss: 0.00073712
Iteration 10/25 | Loss: 0.00073712
Iteration 11/25 | Loss: 0.00073712
Iteration 12/25 | Loss: 0.00073712
Iteration 13/25 | Loss: 0.00073712
Iteration 14/25 | Loss: 0.00073712
Iteration 15/25 | Loss: 0.00073712
Iteration 16/25 | Loss: 0.00073712
Iteration 17/25 | Loss: 0.00073712
Iteration 18/25 | Loss: 0.00073712
Iteration 19/25 | Loss: 0.00073712
Iteration 20/25 | Loss: 0.00073712
Iteration 21/25 | Loss: 0.00073712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007371202809736133, 0.0007371202809736133, 0.0007371202809736133, 0.0007371202809736133, 0.0007371202809736133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007371202809736133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073712
Iteration 2/1000 | Loss: 0.00011827
Iteration 3/1000 | Loss: 0.00003696
Iteration 4/1000 | Loss: 0.00002956
Iteration 5/1000 | Loss: 0.00003014
Iteration 6/1000 | Loss: 0.00003389
Iteration 7/1000 | Loss: 0.00003714
Iteration 8/1000 | Loss: 0.00003599
Iteration 9/1000 | Loss: 0.00003610
Iteration 10/1000 | Loss: 0.00003517
Iteration 11/1000 | Loss: 0.00003565
Iteration 12/1000 | Loss: 0.00003875
Iteration 13/1000 | Loss: 0.00003405
Iteration 14/1000 | Loss: 0.00003172
Iteration 15/1000 | Loss: 0.00003138
Iteration 16/1000 | Loss: 0.00014369
Iteration 17/1000 | Loss: 0.00003776
Iteration 18/1000 | Loss: 0.00002852
Iteration 19/1000 | Loss: 0.00002585
Iteration 20/1000 | Loss: 0.00002447
Iteration 21/1000 | Loss: 0.00002401
Iteration 22/1000 | Loss: 0.00002364
Iteration 23/1000 | Loss: 0.00002331
Iteration 24/1000 | Loss: 0.00002316
Iteration 25/1000 | Loss: 0.00014286
Iteration 26/1000 | Loss: 0.00022712
Iteration 27/1000 | Loss: 0.00008756
Iteration 28/1000 | Loss: 0.00005337
Iteration 29/1000 | Loss: 0.00002303
Iteration 30/1000 | Loss: 0.00002292
Iteration 31/1000 | Loss: 0.00002290
Iteration 32/1000 | Loss: 0.00029020
Iteration 33/1000 | Loss: 0.00003253
Iteration 34/1000 | Loss: 0.00004936
Iteration 35/1000 | Loss: 0.00003279
Iteration 36/1000 | Loss: 0.00002300
Iteration 37/1000 | Loss: 0.00002296
Iteration 38/1000 | Loss: 0.00023236
Iteration 39/1000 | Loss: 0.00004014
Iteration 40/1000 | Loss: 0.00003573
Iteration 41/1000 | Loss: 0.00002293
Iteration 42/1000 | Loss: 0.00022807
Iteration 43/1000 | Loss: 0.00037521
Iteration 44/1000 | Loss: 0.00003095
Iteration 45/1000 | Loss: 0.00009237
Iteration 46/1000 | Loss: 0.00002396
Iteration 47/1000 | Loss: 0.00008809
Iteration 48/1000 | Loss: 0.00003001
Iteration 49/1000 | Loss: 0.00002246
Iteration 50/1000 | Loss: 0.00002245
Iteration 51/1000 | Loss: 0.00002232
Iteration 52/1000 | Loss: 0.00002226
Iteration 53/1000 | Loss: 0.00002225
Iteration 54/1000 | Loss: 0.00002224
Iteration 55/1000 | Loss: 0.00002224
Iteration 56/1000 | Loss: 0.00002223
Iteration 57/1000 | Loss: 0.00002221
Iteration 58/1000 | Loss: 0.00002221
Iteration 59/1000 | Loss: 0.00002219
Iteration 60/1000 | Loss: 0.00002219
Iteration 61/1000 | Loss: 0.00002219
Iteration 62/1000 | Loss: 0.00002219
Iteration 63/1000 | Loss: 0.00002219
Iteration 64/1000 | Loss: 0.00002219
Iteration 65/1000 | Loss: 0.00002219
Iteration 66/1000 | Loss: 0.00002218
Iteration 67/1000 | Loss: 0.00002218
Iteration 68/1000 | Loss: 0.00002218
Iteration 69/1000 | Loss: 0.00002218
Iteration 70/1000 | Loss: 0.00002218
Iteration 71/1000 | Loss: 0.00002218
Iteration 72/1000 | Loss: 0.00002217
Iteration 73/1000 | Loss: 0.00002217
Iteration 74/1000 | Loss: 0.00002217
Iteration 75/1000 | Loss: 0.00002216
Iteration 76/1000 | Loss: 0.00002216
Iteration 77/1000 | Loss: 0.00002216
Iteration 78/1000 | Loss: 0.00002216
Iteration 79/1000 | Loss: 0.00002215
Iteration 80/1000 | Loss: 0.00002214
Iteration 81/1000 | Loss: 0.00002214
Iteration 82/1000 | Loss: 0.00002214
Iteration 83/1000 | Loss: 0.00002213
Iteration 84/1000 | Loss: 0.00002213
Iteration 85/1000 | Loss: 0.00002213
Iteration 86/1000 | Loss: 0.00002213
Iteration 87/1000 | Loss: 0.00002213
Iteration 88/1000 | Loss: 0.00002213
Iteration 89/1000 | Loss: 0.00002213
Iteration 90/1000 | Loss: 0.00002213
Iteration 91/1000 | Loss: 0.00002213
Iteration 92/1000 | Loss: 0.00002213
Iteration 93/1000 | Loss: 0.00002213
Iteration 94/1000 | Loss: 0.00002212
Iteration 95/1000 | Loss: 0.00002212
Iteration 96/1000 | Loss: 0.00002212
Iteration 97/1000 | Loss: 0.00002212
Iteration 98/1000 | Loss: 0.00002212
Iteration 99/1000 | Loss: 0.00002212
Iteration 100/1000 | Loss: 0.00002212
Iteration 101/1000 | Loss: 0.00002212
Iteration 102/1000 | Loss: 0.00002211
Iteration 103/1000 | Loss: 0.00002211
Iteration 104/1000 | Loss: 0.00002211
Iteration 105/1000 | Loss: 0.00002211
Iteration 106/1000 | Loss: 0.00002211
Iteration 107/1000 | Loss: 0.00002211
Iteration 108/1000 | Loss: 0.00002211
Iteration 109/1000 | Loss: 0.00002211
Iteration 110/1000 | Loss: 0.00002210
Iteration 111/1000 | Loss: 0.00002210
Iteration 112/1000 | Loss: 0.00002210
Iteration 113/1000 | Loss: 0.00002210
Iteration 114/1000 | Loss: 0.00002210
Iteration 115/1000 | Loss: 0.00002210
Iteration 116/1000 | Loss: 0.00002210
Iteration 117/1000 | Loss: 0.00002210
Iteration 118/1000 | Loss: 0.00002209
Iteration 119/1000 | Loss: 0.00002209
Iteration 120/1000 | Loss: 0.00002209
Iteration 121/1000 | Loss: 0.00002209
Iteration 122/1000 | Loss: 0.00002208
Iteration 123/1000 | Loss: 0.00002208
Iteration 124/1000 | Loss: 0.00002208
Iteration 125/1000 | Loss: 0.00002208
Iteration 126/1000 | Loss: 0.00002207
Iteration 127/1000 | Loss: 0.00002207
Iteration 128/1000 | Loss: 0.00002207
Iteration 129/1000 | Loss: 0.00002207
Iteration 130/1000 | Loss: 0.00002207
Iteration 131/1000 | Loss: 0.00002207
Iteration 132/1000 | Loss: 0.00002207
Iteration 133/1000 | Loss: 0.00002207
Iteration 134/1000 | Loss: 0.00002207
Iteration 135/1000 | Loss: 0.00002207
Iteration 136/1000 | Loss: 0.00002206
Iteration 137/1000 | Loss: 0.00002206
Iteration 138/1000 | Loss: 0.00002206
Iteration 139/1000 | Loss: 0.00002206
Iteration 140/1000 | Loss: 0.00002206
Iteration 141/1000 | Loss: 0.00002205
Iteration 142/1000 | Loss: 0.00002205
Iteration 143/1000 | Loss: 0.00002205
Iteration 144/1000 | Loss: 0.00002205
Iteration 145/1000 | Loss: 0.00002205
Iteration 146/1000 | Loss: 0.00002205
Iteration 147/1000 | Loss: 0.00002205
Iteration 148/1000 | Loss: 0.00002205
Iteration 149/1000 | Loss: 0.00002205
Iteration 150/1000 | Loss: 0.00002205
Iteration 151/1000 | Loss: 0.00002205
Iteration 152/1000 | Loss: 0.00002205
Iteration 153/1000 | Loss: 0.00002205
Iteration 154/1000 | Loss: 0.00002205
Iteration 155/1000 | Loss: 0.00002205
Iteration 156/1000 | Loss: 0.00002205
Iteration 157/1000 | Loss: 0.00002205
Iteration 158/1000 | Loss: 0.00002205
Iteration 159/1000 | Loss: 0.00002205
Iteration 160/1000 | Loss: 0.00002205
Iteration 161/1000 | Loss: 0.00002205
Iteration 162/1000 | Loss: 0.00002205
Iteration 163/1000 | Loss: 0.00002205
Iteration 164/1000 | Loss: 0.00002204
Iteration 165/1000 | Loss: 0.00002204
Iteration 166/1000 | Loss: 0.00002204
Iteration 167/1000 | Loss: 0.00002204
Iteration 168/1000 | Loss: 0.00002204
Iteration 169/1000 | Loss: 0.00002204
Iteration 170/1000 | Loss: 0.00002204
Iteration 171/1000 | Loss: 0.00002204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.2044214347261004e-05, 2.2044214347261004e-05, 2.2044214347261004e-05, 2.2044214347261004e-05, 2.2044214347261004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2044214347261004e-05

Optimization complete. Final v2v error: 3.9404072761535645 mm

Highest mean error: 4.576413154602051 mm for frame 231

Lowest mean error: 3.5182883739471436 mm for frame 57

Saving results

Total time: 138.1457393169403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925757
Iteration 2/25 | Loss: 0.00165846
Iteration 3/25 | Loss: 0.00125072
Iteration 4/25 | Loss: 0.00122518
Iteration 5/25 | Loss: 0.00121836
Iteration 6/25 | Loss: 0.00121641
Iteration 7/25 | Loss: 0.00121621
Iteration 8/25 | Loss: 0.00121618
Iteration 9/25 | Loss: 0.00121618
Iteration 10/25 | Loss: 0.00121618
Iteration 11/25 | Loss: 0.00121618
Iteration 12/25 | Loss: 0.00121618
Iteration 13/25 | Loss: 0.00121618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012161756167188287, 0.0012161756167188287, 0.0012161756167188287, 0.0012161756167188287, 0.0012161756167188287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012161756167188287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88706672
Iteration 2/25 | Loss: 0.00080869
Iteration 3/25 | Loss: 0.00080868
Iteration 4/25 | Loss: 0.00080868
Iteration 5/25 | Loss: 0.00080868
Iteration 6/25 | Loss: 0.00080868
Iteration 7/25 | Loss: 0.00080868
Iteration 8/25 | Loss: 0.00080867
Iteration 9/25 | Loss: 0.00080867
Iteration 10/25 | Loss: 0.00080867
Iteration 11/25 | Loss: 0.00080867
Iteration 12/25 | Loss: 0.00080867
Iteration 13/25 | Loss: 0.00080867
Iteration 14/25 | Loss: 0.00080867
Iteration 15/25 | Loss: 0.00080867
Iteration 16/25 | Loss: 0.00080867
Iteration 17/25 | Loss: 0.00080867
Iteration 18/25 | Loss: 0.00080867
Iteration 19/25 | Loss: 0.00080867
Iteration 20/25 | Loss: 0.00080867
Iteration 21/25 | Loss: 0.00080867
Iteration 22/25 | Loss: 0.00080867
Iteration 23/25 | Loss: 0.00080867
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000808674085419625, 0.000808674085419625, 0.000808674085419625, 0.000808674085419625, 0.000808674085419625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000808674085419625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080867
Iteration 2/1000 | Loss: 0.00006380
Iteration 3/1000 | Loss: 0.00004230
Iteration 4/1000 | Loss: 0.00003480
Iteration 5/1000 | Loss: 0.00003260
Iteration 6/1000 | Loss: 0.00003090
Iteration 7/1000 | Loss: 0.00002987
Iteration 8/1000 | Loss: 0.00002904
Iteration 9/1000 | Loss: 0.00002839
Iteration 10/1000 | Loss: 0.00002798
Iteration 11/1000 | Loss: 0.00002766
Iteration 12/1000 | Loss: 0.00002735
Iteration 13/1000 | Loss: 0.00002711
Iteration 14/1000 | Loss: 0.00002690
Iteration 15/1000 | Loss: 0.00002665
Iteration 16/1000 | Loss: 0.00002655
Iteration 17/1000 | Loss: 0.00002643
Iteration 18/1000 | Loss: 0.00002638
Iteration 19/1000 | Loss: 0.00002624
Iteration 20/1000 | Loss: 0.00002622
Iteration 21/1000 | Loss: 0.00002609
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002603
Iteration 24/1000 | Loss: 0.00002600
Iteration 25/1000 | Loss: 0.00002599
Iteration 26/1000 | Loss: 0.00002599
Iteration 27/1000 | Loss: 0.00002598
Iteration 28/1000 | Loss: 0.00002598
Iteration 29/1000 | Loss: 0.00002597
Iteration 30/1000 | Loss: 0.00002597
Iteration 31/1000 | Loss: 0.00002596
Iteration 32/1000 | Loss: 0.00002595
Iteration 33/1000 | Loss: 0.00002592
Iteration 34/1000 | Loss: 0.00002590
Iteration 35/1000 | Loss: 0.00002590
Iteration 36/1000 | Loss: 0.00002589
Iteration 37/1000 | Loss: 0.00002589
Iteration 38/1000 | Loss: 0.00002588
Iteration 39/1000 | Loss: 0.00002585
Iteration 40/1000 | Loss: 0.00002584
Iteration 41/1000 | Loss: 0.00002583
Iteration 42/1000 | Loss: 0.00002582
Iteration 43/1000 | Loss: 0.00002581
Iteration 44/1000 | Loss: 0.00002581
Iteration 45/1000 | Loss: 0.00002580
Iteration 46/1000 | Loss: 0.00002580
Iteration 47/1000 | Loss: 0.00002580
Iteration 48/1000 | Loss: 0.00002580
Iteration 49/1000 | Loss: 0.00002580
Iteration 50/1000 | Loss: 0.00002580
Iteration 51/1000 | Loss: 0.00002580
Iteration 52/1000 | Loss: 0.00002580
Iteration 53/1000 | Loss: 0.00002579
Iteration 54/1000 | Loss: 0.00002579
Iteration 55/1000 | Loss: 0.00002579
Iteration 56/1000 | Loss: 0.00002579
Iteration 57/1000 | Loss: 0.00002579
Iteration 58/1000 | Loss: 0.00002578
Iteration 59/1000 | Loss: 0.00002578
Iteration 60/1000 | Loss: 0.00002578
Iteration 61/1000 | Loss: 0.00002578
Iteration 62/1000 | Loss: 0.00002578
Iteration 63/1000 | Loss: 0.00002578
Iteration 64/1000 | Loss: 0.00002578
Iteration 65/1000 | Loss: 0.00002578
Iteration 66/1000 | Loss: 0.00002578
Iteration 67/1000 | Loss: 0.00002578
Iteration 68/1000 | Loss: 0.00002577
Iteration 69/1000 | Loss: 0.00002577
Iteration 70/1000 | Loss: 0.00002577
Iteration 71/1000 | Loss: 0.00002577
Iteration 72/1000 | Loss: 0.00002577
Iteration 73/1000 | Loss: 0.00002577
Iteration 74/1000 | Loss: 0.00002577
Iteration 75/1000 | Loss: 0.00002576
Iteration 76/1000 | Loss: 0.00002576
Iteration 77/1000 | Loss: 0.00002576
Iteration 78/1000 | Loss: 0.00002576
Iteration 79/1000 | Loss: 0.00002576
Iteration 80/1000 | Loss: 0.00002576
Iteration 81/1000 | Loss: 0.00002576
Iteration 82/1000 | Loss: 0.00002576
Iteration 83/1000 | Loss: 0.00002576
Iteration 84/1000 | Loss: 0.00002575
Iteration 85/1000 | Loss: 0.00002575
Iteration 86/1000 | Loss: 0.00002575
Iteration 87/1000 | Loss: 0.00002575
Iteration 88/1000 | Loss: 0.00002575
Iteration 89/1000 | Loss: 0.00002575
Iteration 90/1000 | Loss: 0.00002575
Iteration 91/1000 | Loss: 0.00002575
Iteration 92/1000 | Loss: 0.00002575
Iteration 93/1000 | Loss: 0.00002574
Iteration 94/1000 | Loss: 0.00002574
Iteration 95/1000 | Loss: 0.00002574
Iteration 96/1000 | Loss: 0.00002574
Iteration 97/1000 | Loss: 0.00002574
Iteration 98/1000 | Loss: 0.00002574
Iteration 99/1000 | Loss: 0.00002574
Iteration 100/1000 | Loss: 0.00002574
Iteration 101/1000 | Loss: 0.00002574
Iteration 102/1000 | Loss: 0.00002574
Iteration 103/1000 | Loss: 0.00002573
Iteration 104/1000 | Loss: 0.00002573
Iteration 105/1000 | Loss: 0.00002573
Iteration 106/1000 | Loss: 0.00002573
Iteration 107/1000 | Loss: 0.00002573
Iteration 108/1000 | Loss: 0.00002573
Iteration 109/1000 | Loss: 0.00002573
Iteration 110/1000 | Loss: 0.00002573
Iteration 111/1000 | Loss: 0.00002573
Iteration 112/1000 | Loss: 0.00002572
Iteration 113/1000 | Loss: 0.00002572
Iteration 114/1000 | Loss: 0.00002572
Iteration 115/1000 | Loss: 0.00002572
Iteration 116/1000 | Loss: 0.00002572
Iteration 117/1000 | Loss: 0.00002572
Iteration 118/1000 | Loss: 0.00002572
Iteration 119/1000 | Loss: 0.00002572
Iteration 120/1000 | Loss: 0.00002572
Iteration 121/1000 | Loss: 0.00002572
Iteration 122/1000 | Loss: 0.00002572
Iteration 123/1000 | Loss: 0.00002572
Iteration 124/1000 | Loss: 0.00002572
Iteration 125/1000 | Loss: 0.00002571
Iteration 126/1000 | Loss: 0.00002571
Iteration 127/1000 | Loss: 0.00002571
Iteration 128/1000 | Loss: 0.00002571
Iteration 129/1000 | Loss: 0.00002571
Iteration 130/1000 | Loss: 0.00002571
Iteration 131/1000 | Loss: 0.00002571
Iteration 132/1000 | Loss: 0.00002571
Iteration 133/1000 | Loss: 0.00002571
Iteration 134/1000 | Loss: 0.00002571
Iteration 135/1000 | Loss: 0.00002571
Iteration 136/1000 | Loss: 0.00002571
Iteration 137/1000 | Loss: 0.00002570
Iteration 138/1000 | Loss: 0.00002570
Iteration 139/1000 | Loss: 0.00002570
Iteration 140/1000 | Loss: 0.00002570
Iteration 141/1000 | Loss: 0.00002570
Iteration 142/1000 | Loss: 0.00002570
Iteration 143/1000 | Loss: 0.00002570
Iteration 144/1000 | Loss: 0.00002570
Iteration 145/1000 | Loss: 0.00002570
Iteration 146/1000 | Loss: 0.00002569
Iteration 147/1000 | Loss: 0.00002569
Iteration 148/1000 | Loss: 0.00002569
Iteration 149/1000 | Loss: 0.00002569
Iteration 150/1000 | Loss: 0.00002569
Iteration 151/1000 | Loss: 0.00002569
Iteration 152/1000 | Loss: 0.00002569
Iteration 153/1000 | Loss: 0.00002569
Iteration 154/1000 | Loss: 0.00002569
Iteration 155/1000 | Loss: 0.00002569
Iteration 156/1000 | Loss: 0.00002569
Iteration 157/1000 | Loss: 0.00002569
Iteration 158/1000 | Loss: 0.00002569
Iteration 159/1000 | Loss: 0.00002569
Iteration 160/1000 | Loss: 0.00002569
Iteration 161/1000 | Loss: 0.00002568
Iteration 162/1000 | Loss: 0.00002568
Iteration 163/1000 | Loss: 0.00002568
Iteration 164/1000 | Loss: 0.00002568
Iteration 165/1000 | Loss: 0.00002568
Iteration 166/1000 | Loss: 0.00002568
Iteration 167/1000 | Loss: 0.00002568
Iteration 168/1000 | Loss: 0.00002568
Iteration 169/1000 | Loss: 0.00002568
Iteration 170/1000 | Loss: 0.00002568
Iteration 171/1000 | Loss: 0.00002568
Iteration 172/1000 | Loss: 0.00002568
Iteration 173/1000 | Loss: 0.00002568
Iteration 174/1000 | Loss: 0.00002568
Iteration 175/1000 | Loss: 0.00002568
Iteration 176/1000 | Loss: 0.00002568
Iteration 177/1000 | Loss: 0.00002568
Iteration 178/1000 | Loss: 0.00002568
Iteration 179/1000 | Loss: 0.00002568
Iteration 180/1000 | Loss: 0.00002568
Iteration 181/1000 | Loss: 0.00002568
Iteration 182/1000 | Loss: 0.00002568
Iteration 183/1000 | Loss: 0.00002568
Iteration 184/1000 | Loss: 0.00002568
Iteration 185/1000 | Loss: 0.00002568
Iteration 186/1000 | Loss: 0.00002568
Iteration 187/1000 | Loss: 0.00002568
Iteration 188/1000 | Loss: 0.00002568
Iteration 189/1000 | Loss: 0.00002568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.568245145084802e-05, 2.568245145084802e-05, 2.568245145084802e-05, 2.568245145084802e-05, 2.568245145084802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.568245145084802e-05

Optimization complete. Final v2v error: 4.218591213226318 mm

Highest mean error: 5.121572017669678 mm for frame 128

Lowest mean error: 3.355295181274414 mm for frame 25

Saving results

Total time: 47.87858033180237
