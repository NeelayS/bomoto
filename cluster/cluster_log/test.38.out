Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=38, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2128-2183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508463
Iteration 2/25 | Loss: 0.00192429
Iteration 3/25 | Loss: 0.00182903
Iteration 4/25 | Loss: 0.00181428
Iteration 5/25 | Loss: 0.00180930
Iteration 6/25 | Loss: 0.00180824
Iteration 7/25 | Loss: 0.00180824
Iteration 8/25 | Loss: 0.00180824
Iteration 9/25 | Loss: 0.00180824
Iteration 10/25 | Loss: 0.00180824
Iteration 11/25 | Loss: 0.00180824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018082363530993462, 0.0018082363530993462, 0.0018082363530993462, 0.0018082363530993462, 0.0018082363530993462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018082363530993462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.40552855
Iteration 2/25 | Loss: 0.00121930
Iteration 3/25 | Loss: 0.00121928
Iteration 4/25 | Loss: 0.00121928
Iteration 5/25 | Loss: 0.00121928
Iteration 6/25 | Loss: 0.00121928
Iteration 7/25 | Loss: 0.00121927
Iteration 8/25 | Loss: 0.00121927
Iteration 9/25 | Loss: 0.00121927
Iteration 10/25 | Loss: 0.00121927
Iteration 11/25 | Loss: 0.00121927
Iteration 12/25 | Loss: 0.00121927
Iteration 13/25 | Loss: 0.00121927
Iteration 14/25 | Loss: 0.00121927
Iteration 15/25 | Loss: 0.00121927
Iteration 16/25 | Loss: 0.00121927
Iteration 17/25 | Loss: 0.00121927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001219273661263287, 0.001219273661263287, 0.001219273661263287, 0.001219273661263287, 0.001219273661263287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001219273661263287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121927
Iteration 2/1000 | Loss: 0.00006501
Iteration 3/1000 | Loss: 0.00004469
Iteration 4/1000 | Loss: 0.00003992
Iteration 5/1000 | Loss: 0.00003875
Iteration 6/1000 | Loss: 0.00003756
Iteration 7/1000 | Loss: 0.00003646
Iteration 8/1000 | Loss: 0.00003596
Iteration 9/1000 | Loss: 0.00003549
Iteration 10/1000 | Loss: 0.00003508
Iteration 11/1000 | Loss: 0.00003472
Iteration 12/1000 | Loss: 0.00003450
Iteration 13/1000 | Loss: 0.00003439
Iteration 14/1000 | Loss: 0.00003433
Iteration 15/1000 | Loss: 0.00003427
Iteration 16/1000 | Loss: 0.00003427
Iteration 17/1000 | Loss: 0.00003425
Iteration 18/1000 | Loss: 0.00003425
Iteration 19/1000 | Loss: 0.00003424
Iteration 20/1000 | Loss: 0.00003424
Iteration 21/1000 | Loss: 0.00003423
Iteration 22/1000 | Loss: 0.00003423
Iteration 23/1000 | Loss: 0.00003422
Iteration 24/1000 | Loss: 0.00003419
Iteration 25/1000 | Loss: 0.00003419
Iteration 26/1000 | Loss: 0.00003419
Iteration 27/1000 | Loss: 0.00003419
Iteration 28/1000 | Loss: 0.00003419
Iteration 29/1000 | Loss: 0.00003419
Iteration 30/1000 | Loss: 0.00003419
Iteration 31/1000 | Loss: 0.00003419
Iteration 32/1000 | Loss: 0.00003419
Iteration 33/1000 | Loss: 0.00003419
Iteration 34/1000 | Loss: 0.00003419
Iteration 35/1000 | Loss: 0.00003417
Iteration 36/1000 | Loss: 0.00003417
Iteration 37/1000 | Loss: 0.00003417
Iteration 38/1000 | Loss: 0.00003416
Iteration 39/1000 | Loss: 0.00003416
Iteration 40/1000 | Loss: 0.00003416
Iteration 41/1000 | Loss: 0.00003416
Iteration 42/1000 | Loss: 0.00003416
Iteration 43/1000 | Loss: 0.00003415
Iteration 44/1000 | Loss: 0.00003415
Iteration 45/1000 | Loss: 0.00003414
Iteration 46/1000 | Loss: 0.00003414
Iteration 47/1000 | Loss: 0.00003413
Iteration 48/1000 | Loss: 0.00003413
Iteration 49/1000 | Loss: 0.00003413
Iteration 50/1000 | Loss: 0.00003413
Iteration 51/1000 | Loss: 0.00003413
Iteration 52/1000 | Loss: 0.00003413
Iteration 53/1000 | Loss: 0.00003413
Iteration 54/1000 | Loss: 0.00003413
Iteration 55/1000 | Loss: 0.00003412
Iteration 56/1000 | Loss: 0.00003412
Iteration 57/1000 | Loss: 0.00003412
Iteration 58/1000 | Loss: 0.00003411
Iteration 59/1000 | Loss: 0.00003411
Iteration 60/1000 | Loss: 0.00003411
Iteration 61/1000 | Loss: 0.00003410
Iteration 62/1000 | Loss: 0.00003410
Iteration 63/1000 | Loss: 0.00003410
Iteration 64/1000 | Loss: 0.00003409
Iteration 65/1000 | Loss: 0.00003409
Iteration 66/1000 | Loss: 0.00003408
Iteration 67/1000 | Loss: 0.00003408
Iteration 68/1000 | Loss: 0.00003408
Iteration 69/1000 | Loss: 0.00003408
Iteration 70/1000 | Loss: 0.00003408
Iteration 71/1000 | Loss: 0.00003408
Iteration 72/1000 | Loss: 0.00003408
Iteration 73/1000 | Loss: 0.00003408
Iteration 74/1000 | Loss: 0.00003408
Iteration 75/1000 | Loss: 0.00003408
Iteration 76/1000 | Loss: 0.00003408
Iteration 77/1000 | Loss: 0.00003408
Iteration 78/1000 | Loss: 0.00003407
Iteration 79/1000 | Loss: 0.00003407
Iteration 80/1000 | Loss: 0.00003407
Iteration 81/1000 | Loss: 0.00003407
Iteration 82/1000 | Loss: 0.00003407
Iteration 83/1000 | Loss: 0.00003407
Iteration 84/1000 | Loss: 0.00003407
Iteration 85/1000 | Loss: 0.00003407
Iteration 86/1000 | Loss: 0.00003407
Iteration 87/1000 | Loss: 0.00003406
Iteration 88/1000 | Loss: 0.00003406
Iteration 89/1000 | Loss: 0.00003406
Iteration 90/1000 | Loss: 0.00003406
Iteration 91/1000 | Loss: 0.00003406
Iteration 92/1000 | Loss: 0.00003406
Iteration 93/1000 | Loss: 0.00003406
Iteration 94/1000 | Loss: 0.00003406
Iteration 95/1000 | Loss: 0.00003405
Iteration 96/1000 | Loss: 0.00003405
Iteration 97/1000 | Loss: 0.00003405
Iteration 98/1000 | Loss: 0.00003405
Iteration 99/1000 | Loss: 0.00003405
Iteration 100/1000 | Loss: 0.00003405
Iteration 101/1000 | Loss: 0.00003405
Iteration 102/1000 | Loss: 0.00003405
Iteration 103/1000 | Loss: 0.00003405
Iteration 104/1000 | Loss: 0.00003405
Iteration 105/1000 | Loss: 0.00003405
Iteration 106/1000 | Loss: 0.00003404
Iteration 107/1000 | Loss: 0.00003404
Iteration 108/1000 | Loss: 0.00003404
Iteration 109/1000 | Loss: 0.00003404
Iteration 110/1000 | Loss: 0.00003404
Iteration 111/1000 | Loss: 0.00003404
Iteration 112/1000 | Loss: 0.00003404
Iteration 113/1000 | Loss: 0.00003404
Iteration 114/1000 | Loss: 0.00003404
Iteration 115/1000 | Loss: 0.00003403
Iteration 116/1000 | Loss: 0.00003403
Iteration 117/1000 | Loss: 0.00003403
Iteration 118/1000 | Loss: 0.00003403
Iteration 119/1000 | Loss: 0.00003403
Iteration 120/1000 | Loss: 0.00003403
Iteration 121/1000 | Loss: 0.00003403
Iteration 122/1000 | Loss: 0.00003403
Iteration 123/1000 | Loss: 0.00003403
Iteration 124/1000 | Loss: 0.00003403
Iteration 125/1000 | Loss: 0.00003403
Iteration 126/1000 | Loss: 0.00003403
Iteration 127/1000 | Loss: 0.00003403
Iteration 128/1000 | Loss: 0.00003403
Iteration 129/1000 | Loss: 0.00003403
Iteration 130/1000 | Loss: 0.00003403
Iteration 131/1000 | Loss: 0.00003403
Iteration 132/1000 | Loss: 0.00003403
Iteration 133/1000 | Loss: 0.00003403
Iteration 134/1000 | Loss: 0.00003403
Iteration 135/1000 | Loss: 0.00003403
Iteration 136/1000 | Loss: 0.00003403
Iteration 137/1000 | Loss: 0.00003403
Iteration 138/1000 | Loss: 0.00003403
Iteration 139/1000 | Loss: 0.00003403
Iteration 140/1000 | Loss: 0.00003403
Iteration 141/1000 | Loss: 0.00003403
Iteration 142/1000 | Loss: 0.00003403
Iteration 143/1000 | Loss: 0.00003403
Iteration 144/1000 | Loss: 0.00003403
Iteration 145/1000 | Loss: 0.00003403
Iteration 146/1000 | Loss: 0.00003403
Iteration 147/1000 | Loss: 0.00003403
Iteration 148/1000 | Loss: 0.00003403
Iteration 149/1000 | Loss: 0.00003403
Iteration 150/1000 | Loss: 0.00003403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [3.402915535843931e-05, 3.402915535843931e-05, 3.402915535843931e-05, 3.402915535843931e-05, 3.402915535843931e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.402915535843931e-05

Optimization complete. Final v2v error: 4.986910343170166 mm

Highest mean error: 5.409759998321533 mm for frame 86

Lowest mean error: 4.753784656524658 mm for frame 0

Saving results

Total time: 39.83076095581055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145446
Iteration 2/25 | Loss: 0.00423330
Iteration 3/25 | Loss: 0.00348060
Iteration 4/25 | Loss: 0.00298300
Iteration 5/25 | Loss: 0.00261372
Iteration 6/25 | Loss: 0.00246759
Iteration 7/25 | Loss: 0.00237557
Iteration 8/25 | Loss: 0.00231759
Iteration 9/25 | Loss: 0.00227697
Iteration 10/25 | Loss: 0.00225504
Iteration 11/25 | Loss: 0.00223475
Iteration 12/25 | Loss: 0.00222747
Iteration 13/25 | Loss: 0.00221932
Iteration 14/25 | Loss: 0.00222491
Iteration 15/25 | Loss: 0.00221165
Iteration 16/25 | Loss: 0.00221864
Iteration 17/25 | Loss: 0.00221343
Iteration 18/25 | Loss: 0.00219857
Iteration 19/25 | Loss: 0.00219322
Iteration 20/25 | Loss: 0.00219053
Iteration 21/25 | Loss: 0.00219569
Iteration 22/25 | Loss: 0.00219371
Iteration 23/25 | Loss: 0.00219236
Iteration 24/25 | Loss: 0.00219030
Iteration 25/25 | Loss: 0.00219099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37432301
Iteration 2/25 | Loss: 0.00448449
Iteration 3/25 | Loss: 0.00448449
Iteration 4/25 | Loss: 0.00448449
Iteration 5/25 | Loss: 0.00448449
Iteration 6/25 | Loss: 0.00448449
Iteration 7/25 | Loss: 0.00448449
Iteration 8/25 | Loss: 0.00448449
Iteration 9/25 | Loss: 0.00448449
Iteration 10/25 | Loss: 0.00448449
Iteration 11/25 | Loss: 0.00448448
Iteration 12/25 | Loss: 0.00448448
Iteration 13/25 | Loss: 0.00448448
Iteration 14/25 | Loss: 0.00448448
Iteration 15/25 | Loss: 0.00448448
Iteration 16/25 | Loss: 0.00448448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004484484903514385, 0.004484484903514385, 0.004484484903514385, 0.004484484903514385, 0.004484484903514385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004484484903514385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00448448
Iteration 2/1000 | Loss: 0.00077250
Iteration 3/1000 | Loss: 0.00054111
Iteration 4/1000 | Loss: 0.00046639
Iteration 5/1000 | Loss: 0.00042285
Iteration 6/1000 | Loss: 0.00152202
Iteration 7/1000 | Loss: 0.00226951
Iteration 8/1000 | Loss: 0.00125665
Iteration 9/1000 | Loss: 0.00156597
Iteration 10/1000 | Loss: 0.00165053
Iteration 11/1000 | Loss: 0.00112000
Iteration 12/1000 | Loss: 0.00037911
Iteration 13/1000 | Loss: 0.00049347
Iteration 14/1000 | Loss: 0.00035598
Iteration 15/1000 | Loss: 0.00047935
Iteration 16/1000 | Loss: 0.00034076
Iteration 17/1000 | Loss: 0.01296944
Iteration 18/1000 | Loss: 0.01391571
Iteration 19/1000 | Loss: 0.00208160
Iteration 20/1000 | Loss: 0.00070576
Iteration 21/1000 | Loss: 0.00054902
Iteration 22/1000 | Loss: 0.00288876
Iteration 23/1000 | Loss: 0.00196540
Iteration 24/1000 | Loss: 0.00038357
Iteration 25/1000 | Loss: 0.00252592
Iteration 26/1000 | Loss: 0.00019046
Iteration 27/1000 | Loss: 0.00092983
Iteration 28/1000 | Loss: 0.00157493
Iteration 29/1000 | Loss: 0.00032565
Iteration 30/1000 | Loss: 0.00023601
Iteration 31/1000 | Loss: 0.00209724
Iteration 32/1000 | Loss: 0.00114235
Iteration 33/1000 | Loss: 0.00124923
Iteration 34/1000 | Loss: 0.00080768
Iteration 35/1000 | Loss: 0.00058065
Iteration 36/1000 | Loss: 0.00071748
Iteration 37/1000 | Loss: 0.00125734
Iteration 38/1000 | Loss: 0.00075900
Iteration 39/1000 | Loss: 0.00021102
Iteration 40/1000 | Loss: 0.00009237
Iteration 41/1000 | Loss: 0.00009767
Iteration 42/1000 | Loss: 0.00012211
Iteration 43/1000 | Loss: 0.00006981
Iteration 44/1000 | Loss: 0.00019814
Iteration 45/1000 | Loss: 0.00007511
Iteration 46/1000 | Loss: 0.00006689
Iteration 47/1000 | Loss: 0.00058774
Iteration 48/1000 | Loss: 0.00087247
Iteration 49/1000 | Loss: 0.00008446
Iteration 50/1000 | Loss: 0.00012407
Iteration 51/1000 | Loss: 0.00049299
Iteration 52/1000 | Loss: 0.00068347
Iteration 53/1000 | Loss: 0.00013745
Iteration 54/1000 | Loss: 0.00015570
Iteration 55/1000 | Loss: 0.00031896
Iteration 56/1000 | Loss: 0.00019516
Iteration 57/1000 | Loss: 0.00006360
Iteration 58/1000 | Loss: 0.00004845
Iteration 59/1000 | Loss: 0.00004970
Iteration 60/1000 | Loss: 0.00109167
Iteration 61/1000 | Loss: 0.00035198
Iteration 62/1000 | Loss: 0.00013063
Iteration 63/1000 | Loss: 0.00016463
Iteration 64/1000 | Loss: 0.00012008
Iteration 65/1000 | Loss: 0.00005152
Iteration 66/1000 | Loss: 0.00004008
Iteration 67/1000 | Loss: 0.00081699
Iteration 68/1000 | Loss: 0.00027844
Iteration 69/1000 | Loss: 0.00041616
Iteration 70/1000 | Loss: 0.00004413
Iteration 71/1000 | Loss: 0.00004076
Iteration 72/1000 | Loss: 0.00004152
Iteration 73/1000 | Loss: 0.00004000
Iteration 74/1000 | Loss: 0.00004040
Iteration 75/1000 | Loss: 0.00003949
Iteration 76/1000 | Loss: 0.00079133
Iteration 77/1000 | Loss: 0.00037653
Iteration 78/1000 | Loss: 0.00004634
Iteration 79/1000 | Loss: 0.00003744
Iteration 80/1000 | Loss: 0.00003513
Iteration 81/1000 | Loss: 0.00012162
Iteration 82/1000 | Loss: 0.00003465
Iteration 83/1000 | Loss: 0.00003789
Iteration 84/1000 | Loss: 0.00003555
Iteration 85/1000 | Loss: 0.00003679
Iteration 86/1000 | Loss: 0.00003836
Iteration 87/1000 | Loss: 0.00003560
Iteration 88/1000 | Loss: 0.00003363
Iteration 89/1000 | Loss: 0.00003522
Iteration 90/1000 | Loss: 0.00003720
Iteration 91/1000 | Loss: 0.00003662
Iteration 92/1000 | Loss: 0.00003406
Iteration 93/1000 | Loss: 0.00003486
Iteration 94/1000 | Loss: 0.00003342
Iteration 95/1000 | Loss: 0.00003297
Iteration 96/1000 | Loss: 0.00003478
Iteration 97/1000 | Loss: 0.00004307
Iteration 98/1000 | Loss: 0.00003895
Iteration 99/1000 | Loss: 0.00004582
Iteration 100/1000 | Loss: 0.00004119
Iteration 101/1000 | Loss: 0.00003515
Iteration 102/1000 | Loss: 0.00003676
Iteration 103/1000 | Loss: 0.00003348
Iteration 104/1000 | Loss: 0.00003485
Iteration 105/1000 | Loss: 0.00004210
Iteration 106/1000 | Loss: 0.00004008
Iteration 107/1000 | Loss: 0.00004305
Iteration 108/1000 | Loss: 0.00004155
Iteration 109/1000 | Loss: 0.00004481
Iteration 110/1000 | Loss: 0.00004340
Iteration 111/1000 | Loss: 0.00004748
Iteration 112/1000 | Loss: 0.00004417
Iteration 113/1000 | Loss: 0.00004525
Iteration 114/1000 | Loss: 0.00004288
Iteration 115/1000 | Loss: 0.00004279
Iteration 116/1000 | Loss: 0.00004246
Iteration 117/1000 | Loss: 0.00004102
Iteration 118/1000 | Loss: 0.00003476
Iteration 119/1000 | Loss: 0.00003272
Iteration 120/1000 | Loss: 0.00003265
Iteration 121/1000 | Loss: 0.00003265
Iteration 122/1000 | Loss: 0.00003265
Iteration 123/1000 | Loss: 0.00003265
Iteration 124/1000 | Loss: 0.00003264
Iteration 125/1000 | Loss: 0.00003264
Iteration 126/1000 | Loss: 0.00003264
Iteration 127/1000 | Loss: 0.00003264
Iteration 128/1000 | Loss: 0.00003264
Iteration 129/1000 | Loss: 0.00003264
Iteration 130/1000 | Loss: 0.00003263
Iteration 131/1000 | Loss: 0.00004333
Iteration 132/1000 | Loss: 0.00004072
Iteration 133/1000 | Loss: 0.00004655
Iteration 134/1000 | Loss: 0.00004175
Iteration 135/1000 | Loss: 0.00004739
Iteration 136/1000 | Loss: 0.00004466
Iteration 137/1000 | Loss: 0.00004246
Iteration 138/1000 | Loss: 0.00004425
Iteration 139/1000 | Loss: 0.00004185
Iteration 140/1000 | Loss: 0.00004149
Iteration 141/1000 | Loss: 0.00004308
Iteration 142/1000 | Loss: 0.00004210
Iteration 143/1000 | Loss: 0.00004137
Iteration 144/1000 | Loss: 0.00004235
Iteration 145/1000 | Loss: 0.00004270
Iteration 146/1000 | Loss: 0.00004093
Iteration 147/1000 | Loss: 0.00004381
Iteration 148/1000 | Loss: 0.00004275
Iteration 149/1000 | Loss: 0.00004398
Iteration 150/1000 | Loss: 0.00004161
Iteration 151/1000 | Loss: 0.00004727
Iteration 152/1000 | Loss: 0.00003753
Iteration 153/1000 | Loss: 0.00003351
Iteration 154/1000 | Loss: 0.00003813
Iteration 155/1000 | Loss: 0.00003799
Iteration 156/1000 | Loss: 0.00003760
Iteration 157/1000 | Loss: 0.00003828
Iteration 158/1000 | Loss: 0.00003894
Iteration 159/1000 | Loss: 0.00004100
Iteration 160/1000 | Loss: 0.00003996
Iteration 161/1000 | Loss: 0.00003896
Iteration 162/1000 | Loss: 0.00004369
Iteration 163/1000 | Loss: 0.00003846
Iteration 164/1000 | Loss: 0.00004185
Iteration 165/1000 | Loss: 0.00004234
Iteration 166/1000 | Loss: 0.00003977
Iteration 167/1000 | Loss: 0.00003988
Iteration 168/1000 | Loss: 0.00004100
Iteration 169/1000 | Loss: 0.00004012
Iteration 170/1000 | Loss: 0.00004085
Iteration 171/1000 | Loss: 0.00003950
Iteration 172/1000 | Loss: 0.00004359
Iteration 173/1000 | Loss: 0.00003880
Iteration 174/1000 | Loss: 0.00004465
Iteration 175/1000 | Loss: 0.00004068
Iteration 176/1000 | Loss: 0.00004150
Iteration 177/1000 | Loss: 0.00004695
Iteration 178/1000 | Loss: 0.00004497
Iteration 179/1000 | Loss: 0.00004172
Iteration 180/1000 | Loss: 0.00003439
Iteration 181/1000 | Loss: 0.00003385
Iteration 182/1000 | Loss: 0.00003280
Iteration 183/1000 | Loss: 0.00003842
Iteration 184/1000 | Loss: 0.00004356
Iteration 185/1000 | Loss: 0.00003848
Iteration 186/1000 | Loss: 0.00004274
Iteration 187/1000 | Loss: 0.00004055
Iteration 188/1000 | Loss: 0.00004978
Iteration 189/1000 | Loss: 0.00004673
Iteration 190/1000 | Loss: 0.00004363
Iteration 191/1000 | Loss: 0.00003386
Iteration 192/1000 | Loss: 0.00003532
Iteration 193/1000 | Loss: 0.00003316
Iteration 194/1000 | Loss: 0.00003946
Iteration 195/1000 | Loss: 0.00003553
Iteration 196/1000 | Loss: 0.00003840
Iteration 197/1000 | Loss: 0.00004528
Iteration 198/1000 | Loss: 0.00004549
Iteration 199/1000 | Loss: 0.00004512
Iteration 200/1000 | Loss: 0.00004530
Iteration 201/1000 | Loss: 0.00003600
Iteration 202/1000 | Loss: 0.00004184
Iteration 203/1000 | Loss: 0.00004803
Iteration 204/1000 | Loss: 0.00004496
Iteration 205/1000 | Loss: 0.00005224
Iteration 206/1000 | Loss: 0.00004155
Iteration 207/1000 | Loss: 0.00005421
Iteration 208/1000 | Loss: 0.00004174
Iteration 209/1000 | Loss: 0.00004801
Iteration 210/1000 | Loss: 0.00004188
Iteration 211/1000 | Loss: 0.00004188
Iteration 212/1000 | Loss: 0.00004706
Iteration 213/1000 | Loss: 0.00004174
Iteration 214/1000 | Loss: 0.00004678
Iteration 215/1000 | Loss: 0.00003968
Iteration 216/1000 | Loss: 0.00004494
Iteration 217/1000 | Loss: 0.00003923
Iteration 218/1000 | Loss: 0.00004023
Iteration 219/1000 | Loss: 0.00004115
Iteration 220/1000 | Loss: 0.00003900
Iteration 221/1000 | Loss: 0.00003605
Iteration 222/1000 | Loss: 0.00004414
Iteration 223/1000 | Loss: 0.00003802
Iteration 224/1000 | Loss: 0.00004334
Iteration 225/1000 | Loss: 0.00003908
Iteration 226/1000 | Loss: 0.00004265
Iteration 227/1000 | Loss: 0.00004396
Iteration 228/1000 | Loss: 0.00004131
Iteration 229/1000 | Loss: 0.00004813
Iteration 230/1000 | Loss: 0.00004133
Iteration 231/1000 | Loss: 0.00004404
Iteration 232/1000 | Loss: 0.00003603
Iteration 233/1000 | Loss: 0.00004054
Iteration 234/1000 | Loss: 0.00004292
Iteration 235/1000 | Loss: 0.00003960
Iteration 236/1000 | Loss: 0.00004299
Iteration 237/1000 | Loss: 0.00004004
Iteration 238/1000 | Loss: 0.00004231
Iteration 239/1000 | Loss: 0.00004231
Iteration 240/1000 | Loss: 0.00003822
Iteration 241/1000 | Loss: 0.00003308
Iteration 242/1000 | Loss: 0.00003299
Iteration 243/1000 | Loss: 0.00003276
Iteration 244/1000 | Loss: 0.00003267
Iteration 245/1000 | Loss: 0.00003789
Iteration 246/1000 | Loss: 0.00003370
Iteration 247/1000 | Loss: 0.00003884
Iteration 248/1000 | Loss: 0.00003399
Iteration 249/1000 | Loss: 0.00004439
Iteration 250/1000 | Loss: 0.00003420
Iteration 251/1000 | Loss: 0.00003824
Iteration 252/1000 | Loss: 0.00003427
Iteration 253/1000 | Loss: 0.00003785
Iteration 254/1000 | Loss: 0.00003409
Iteration 255/1000 | Loss: 0.00004185
Iteration 256/1000 | Loss: 0.00003585
Iteration 257/1000 | Loss: 0.00004114
Iteration 258/1000 | Loss: 0.00003856
Iteration 259/1000 | Loss: 0.00003957
Iteration 260/1000 | Loss: 0.00003754
Iteration 261/1000 | Loss: 0.00003948
Iteration 262/1000 | Loss: 0.00003772
Iteration 263/1000 | Loss: 0.00003668
Iteration 264/1000 | Loss: 0.00003924
Iteration 265/1000 | Loss: 0.00004173
Iteration 266/1000 | Loss: 0.00004016
Iteration 267/1000 | Loss: 0.00003325
Iteration 268/1000 | Loss: 0.00003430
Iteration 269/1000 | Loss: 0.00004232
Iteration 270/1000 | Loss: 0.00004188
Iteration 271/1000 | Loss: 0.00004238
Iteration 272/1000 | Loss: 0.00004181
Iteration 273/1000 | Loss: 0.00004948
Iteration 274/1000 | Loss: 0.00004180
Iteration 275/1000 | Loss: 0.00004445
Iteration 276/1000 | Loss: 0.00004088
Iteration 277/1000 | Loss: 0.00003921
Iteration 278/1000 | Loss: 0.00005188
Iteration 279/1000 | Loss: 0.00003988
Iteration 280/1000 | Loss: 0.00003481
Iteration 281/1000 | Loss: 0.00003314
Iteration 282/1000 | Loss: 0.00004468
Iteration 283/1000 | Loss: 0.00003387
Iteration 284/1000 | Loss: 0.00004752
Iteration 285/1000 | Loss: 0.00003499
Iteration 286/1000 | Loss: 0.00003352
Iteration 287/1000 | Loss: 0.00003706
Iteration 288/1000 | Loss: 0.00003452
Iteration 289/1000 | Loss: 0.00004040
Iteration 290/1000 | Loss: 0.00003669
Iteration 291/1000 | Loss: 0.00003888
Iteration 292/1000 | Loss: 0.00003638
Iteration 293/1000 | Loss: 0.00003435
Iteration 294/1000 | Loss: 0.00003841
Iteration 295/1000 | Loss: 0.00004459
Iteration 296/1000 | Loss: 0.00003884
Iteration 297/1000 | Loss: 0.00003738
Iteration 298/1000 | Loss: 0.00004066
Iteration 299/1000 | Loss: 0.00004000
Iteration 300/1000 | Loss: 0.00003802
Iteration 301/1000 | Loss: 0.00004892
Iteration 302/1000 | Loss: 0.00004290
Iteration 303/1000 | Loss: 0.00004818
Iteration 304/1000 | Loss: 0.00004597
Iteration 305/1000 | Loss: 0.00004850
Iteration 306/1000 | Loss: 0.00004270
Iteration 307/1000 | Loss: 0.00005702
Iteration 308/1000 | Loss: 0.00005281
Iteration 309/1000 | Loss: 0.00004226
Iteration 310/1000 | Loss: 0.00005519
Iteration 311/1000 | Loss: 0.00004111
Iteration 312/1000 | Loss: 0.00005060
Iteration 313/1000 | Loss: 0.00004143
Iteration 314/1000 | Loss: 0.00004138
Iteration 315/1000 | Loss: 0.00003985
Iteration 316/1000 | Loss: 0.00004915
Iteration 317/1000 | Loss: 0.00004094
Iteration 318/1000 | Loss: 0.00004293
Iteration 319/1000 | Loss: 0.00004005
Iteration 320/1000 | Loss: 0.00004267
Iteration 321/1000 | Loss: 0.00003702
Iteration 322/1000 | Loss: 0.00004210
Iteration 323/1000 | Loss: 0.00003833
Iteration 324/1000 | Loss: 0.00004493
Iteration 325/1000 | Loss: 0.00004242
Iteration 326/1000 | Loss: 0.00004604
Iteration 327/1000 | Loss: 0.00003984
Iteration 328/1000 | Loss: 0.00004024
Iteration 329/1000 | Loss: 0.00004332
Iteration 330/1000 | Loss: 0.00004254
Iteration 331/1000 | Loss: 0.00003415
Iteration 332/1000 | Loss: 0.00003572
Iteration 333/1000 | Loss: 0.00003395
Iteration 334/1000 | Loss: 0.00003617
Iteration 335/1000 | Loss: 0.00003699
Iteration 336/1000 | Loss: 0.00003985
Iteration 337/1000 | Loss: 0.00003734
Iteration 338/1000 | Loss: 0.00004392
Iteration 339/1000 | Loss: 0.00003675
Iteration 340/1000 | Loss: 0.00004014
Iteration 341/1000 | Loss: 0.00003698
Iteration 342/1000 | Loss: 0.00004196
Iteration 343/1000 | Loss: 0.00004063
Iteration 344/1000 | Loss: 0.00004686
Iteration 345/1000 | Loss: 0.00003943
Iteration 346/1000 | Loss: 0.00003777
Iteration 347/1000 | Loss: 0.00003877
Iteration 348/1000 | Loss: 0.00003776
Iteration 349/1000 | Loss: 0.00003837
Iteration 350/1000 | Loss: 0.00003970
Iteration 351/1000 | Loss: 0.00003322
Iteration 352/1000 | Loss: 0.00003271
Iteration 353/1000 | Loss: 0.00003303
Iteration 354/1000 | Loss: 0.00003271
Iteration 355/1000 | Loss: 0.00003271
Iteration 356/1000 | Loss: 0.00003288
Iteration 357/1000 | Loss: 0.00003261
Iteration 358/1000 | Loss: 0.00003260
Iteration 359/1000 | Loss: 0.00003274
Iteration 360/1000 | Loss: 0.00003274
Iteration 361/1000 | Loss: 0.00003252
Iteration 362/1000 | Loss: 0.00003252
Iteration 363/1000 | Loss: 0.00003254
Iteration 364/1000 | Loss: 0.00003911
Iteration 365/1000 | Loss: 0.00003308
Iteration 366/1000 | Loss: 0.00003820
Iteration 367/1000 | Loss: 0.00004371
Iteration 368/1000 | Loss: 0.00003886
Iteration 369/1000 | Loss: 0.00005342
Iteration 370/1000 | Loss: 0.00003563
Iteration 371/1000 | Loss: 0.00004560
Iteration 372/1000 | Loss: 0.00003957
Iteration 373/1000 | Loss: 0.00003412
Iteration 374/1000 | Loss: 0.00003704
Iteration 375/1000 | Loss: 0.00004084
Iteration 376/1000 | Loss: 0.00003307
Iteration 377/1000 | Loss: 0.00003596
Iteration 378/1000 | Loss: 0.00003377
Iteration 379/1000 | Loss: 0.00003675
Iteration 380/1000 | Loss: 0.00004161
Iteration 381/1000 | Loss: 0.00003315
Iteration 382/1000 | Loss: 0.00004332
Iteration 383/1000 | Loss: 0.00003365
Iteration 384/1000 | Loss: 0.00003343
Iteration 385/1000 | Loss: 0.00003327
Iteration 386/1000 | Loss: 0.00004669
Iteration 387/1000 | Loss: 0.00003725
Iteration 388/1000 | Loss: 0.00004380
Iteration 389/1000 | Loss: 0.00003801
Iteration 390/1000 | Loss: 0.00003650
Iteration 391/1000 | Loss: 0.00005510
Iteration 392/1000 | Loss: 0.00003381
Iteration 393/1000 | Loss: 0.00004351
Iteration 394/1000 | Loss: 0.00003641
Iteration 395/1000 | Loss: 0.00004264
Iteration 396/1000 | Loss: 0.00003620
Iteration 397/1000 | Loss: 0.00004273
Iteration 398/1000 | Loss: 0.00003478
Iteration 399/1000 | Loss: 0.00004553
Iteration 400/1000 | Loss: 0.00003549
Iteration 401/1000 | Loss: 0.00004470
Iteration 402/1000 | Loss: 0.00003447
Iteration 403/1000 | Loss: 0.00003297
Iteration 404/1000 | Loss: 0.00003766
Iteration 405/1000 | Loss: 0.00003413
Iteration 406/1000 | Loss: 0.00003733
Iteration 407/1000 | Loss: 0.00003378
Iteration 408/1000 | Loss: 0.00003379
Iteration 409/1000 | Loss: 0.00003287
Iteration 410/1000 | Loss: 0.00004408
Iteration 411/1000 | Loss: 0.00003538
Iteration 412/1000 | Loss: 0.00003848
Iteration 413/1000 | Loss: 0.00003515
Iteration 414/1000 | Loss: 0.00003727
Iteration 415/1000 | Loss: 0.00003360
Iteration 416/1000 | Loss: 0.00004127
Iteration 417/1000 | Loss: 0.00003408
Iteration 418/1000 | Loss: 0.00004103
Iteration 419/1000 | Loss: 0.00003395
Iteration 420/1000 | Loss: 0.00004146
Iteration 421/1000 | Loss: 0.00003430
Iteration 422/1000 | Loss: 0.00004008
Iteration 423/1000 | Loss: 0.00004008
Iteration 424/1000 | Loss: 0.00003423
Iteration 425/1000 | Loss: 0.00003770
Iteration 426/1000 | Loss: 0.00003371
Iteration 427/1000 | Loss: 0.00003836
Iteration 428/1000 | Loss: 0.00003532
Iteration 429/1000 | Loss: 0.00003783
Iteration 430/1000 | Loss: 0.00003471
Iteration 431/1000 | Loss: 0.00003836
Iteration 432/1000 | Loss: 0.00003730
Iteration 433/1000 | Loss: 0.00003397
Iteration 434/1000 | Loss: 0.00003330
Iteration 435/1000 | Loss: 0.00004657
Iteration 436/1000 | Loss: 0.00003626
Iteration 437/1000 | Loss: 0.00004071
Iteration 438/1000 | Loss: 0.00003476
Iteration 439/1000 | Loss: 0.00004618
Iteration 440/1000 | Loss: 0.00003616
Iteration 441/1000 | Loss: 0.00004888
Iteration 442/1000 | Loss: 0.00003554
Iteration 443/1000 | Loss: 0.00004078
Iteration 444/1000 | Loss: 0.00003635
Iteration 445/1000 | Loss: 0.00004081
Iteration 446/1000 | Loss: 0.00003632
Iteration 447/1000 | Loss: 0.00003548
Iteration 448/1000 | Loss: 0.00003313
Iteration 449/1000 | Loss: 0.00004829
Iteration 450/1000 | Loss: 0.00003613
Iteration 451/1000 | Loss: 0.00003382
Iteration 452/1000 | Loss: 0.00003302
Iteration 453/1000 | Loss: 0.00003544
Iteration 454/1000 | Loss: 0.00003359
Iteration 455/1000 | Loss: 0.00004443
Iteration 456/1000 | Loss: 0.00003613
Iteration 457/1000 | Loss: 0.00003315
Iteration 458/1000 | Loss: 0.00004169
Iteration 459/1000 | Loss: 0.00003408
Iteration 460/1000 | Loss: 0.00004296
Iteration 461/1000 | Loss: 0.00003479
Iteration 462/1000 | Loss: 0.00004212
Iteration 463/1000 | Loss: 0.00003520
Iteration 464/1000 | Loss: 0.00003566
Iteration 465/1000 | Loss: 0.00003533
Iteration 466/1000 | Loss: 0.00003328
Iteration 467/1000 | Loss: 0.00003626
Iteration 468/1000 | Loss: 0.00003336
Iteration 469/1000 | Loss: 0.00004878
Iteration 470/1000 | Loss: 0.00003642
Iteration 471/1000 | Loss: 0.00003952
Iteration 472/1000 | Loss: 0.00003407
Iteration 473/1000 | Loss: 0.00003290
Iteration 474/1000 | Loss: 0.00003458
Iteration 475/1000 | Loss: 0.00003324
Iteration 476/1000 | Loss: 0.00004608
Iteration 477/1000 | Loss: 0.00003576
Iteration 478/1000 | Loss: 0.00003365
Iteration 479/1000 | Loss: 0.00003288
Iteration 480/1000 | Loss: 0.00003441
Iteration 481/1000 | Loss: 0.00003618
Iteration 482/1000 | Loss: 0.00003413
Iteration 483/1000 | Loss: 0.00003504
Iteration 484/1000 | Loss: 0.00003497
Iteration 485/1000 | Loss: 0.00003438
Iteration 486/1000 | Loss: 0.00003421
Iteration 487/1000 | Loss: 0.00003399
Iteration 488/1000 | Loss: 0.00003542
Iteration 489/1000 | Loss: 0.00003368
Iteration 490/1000 | Loss: 0.00003587
Iteration 491/1000 | Loss: 0.00003417
Iteration 492/1000 | Loss: 0.00003544
Iteration 493/1000 | Loss: 0.00003750
Iteration 494/1000 | Loss: 0.00003596
Iteration 495/1000 | Loss: 0.00003728
Iteration 496/1000 | Loss: 0.00003581
Iteration 497/1000 | Loss: 0.00003776
Iteration 498/1000 | Loss: 0.00003537
Iteration 499/1000 | Loss: 0.00004103
Iteration 500/1000 | Loss: 0.00003434
Iteration 501/1000 | Loss: 0.00004040
Iteration 502/1000 | Loss: 0.00003456
Iteration 503/1000 | Loss: 0.00003277
Iteration 504/1000 | Loss: 0.00004305
Iteration 505/1000 | Loss: 0.00003432
Iteration 506/1000 | Loss: 0.00004342
Iteration 507/1000 | Loss: 0.00003468
Iteration 508/1000 | Loss: 0.00004468
Iteration 509/1000 | Loss: 0.00003473
Iteration 510/1000 | Loss: 0.00004426
Iteration 511/1000 | Loss: 0.00003656
Iteration 512/1000 | Loss: 0.00004373
Iteration 513/1000 | Loss: 0.00004256
Iteration 514/1000 | Loss: 0.00004728
Iteration 515/1000 | Loss: 0.00003396
Iteration 516/1000 | Loss: 0.00004234
Iteration 517/1000 | Loss: 0.00003438
Iteration 518/1000 | Loss: 0.00004882
Iteration 519/1000 | Loss: 0.00003505
Iteration 520/1000 | Loss: 0.00004188
Iteration 521/1000 | Loss: 0.00003540
Iteration 522/1000 | Loss: 0.00004063
Iteration 523/1000 | Loss: 0.00003671
Iteration 524/1000 | Loss: 0.00004517
Iteration 525/1000 | Loss: 0.00003661
Iteration 526/1000 | Loss: 0.00004486
Iteration 527/1000 | Loss: 0.00003648
Iteration 528/1000 | Loss: 0.00003730
Iteration 529/1000 | Loss: 0.00003641
Iteration 530/1000 | Loss: 0.00004044
Iteration 531/1000 | Loss: 0.00003710
Iteration 532/1000 | Loss: 0.00004098
Iteration 533/1000 | Loss: 0.00004083
Iteration 534/1000 | Loss: 0.00004326
Iteration 535/1000 | Loss: 0.00004285
Iteration 536/1000 | Loss: 0.00004145
Iteration 537/1000 | Loss: 0.00003798
Iteration 538/1000 | Loss: 0.00003981
Iteration 539/1000 | Loss: 0.00003743
Iteration 540/1000 | Loss: 0.00003310
Iteration 541/1000 | Loss: 0.00003920
Iteration 542/1000 | Loss: 0.00003876
Iteration 543/1000 | Loss: 0.00004069
Iteration 544/1000 | Loss: 0.00003707
Iteration 545/1000 | Loss: 0.00003373
Iteration 546/1000 | Loss: 0.00003389
Iteration 547/1000 | Loss: 0.00003277
Iteration 548/1000 | Loss: 0.00003768
Iteration 549/1000 | Loss: 0.00003752
Iteration 550/1000 | Loss: 0.00003293
Iteration 551/1000 | Loss: 0.00003875
Iteration 552/1000 | Loss: 0.00003408
Iteration 553/1000 | Loss: 0.00004411
Iteration 554/1000 | Loss: 0.00003762
Iteration 555/1000 | Loss: 0.00004553
Iteration 556/1000 | Loss: 0.00003703
Iteration 557/1000 | Loss: 0.00004778
Iteration 558/1000 | Loss: 0.00003673
Iteration 559/1000 | Loss: 0.00004513
Iteration 560/1000 | Loss: 0.00003635
Iteration 561/1000 | Loss: 0.00003526
Iteration 562/1000 | Loss: 0.00003584
Iteration 563/1000 | Loss: 0.00004205
Iteration 564/1000 | Loss: 0.00003612
Iteration 565/1000 | Loss: 0.00004134
Iteration 566/1000 | Loss: 0.00003589
Iteration 567/1000 | Loss: 0.00003877
Iteration 568/1000 | Loss: 0.00003686
Iteration 569/1000 | Loss: 0.00004733
Iteration 570/1000 | Loss: 0.00003643
Iteration 571/1000 | Loss: 0.00005079
Iteration 572/1000 | Loss: 0.00003620
Iteration 573/1000 | Loss: 0.00004956
Iteration 574/1000 | Loss: 0.00003588
Iteration 575/1000 | Loss: 0.00004757
Iteration 576/1000 | Loss: 0.00003562
Iteration 577/1000 | Loss: 0.00004966
Iteration 578/1000 | Loss: 0.00003538
Iteration 579/1000 | Loss: 0.00003989
Iteration 580/1000 | Loss: 0.00003597
Iteration 581/1000 | Loss: 0.00004422
Iteration 582/1000 | Loss: 0.00003988
Iteration 583/1000 | Loss: 0.00004837
Iteration 584/1000 | Loss: 0.00004070
Iteration 585/1000 | Loss: 0.00004358
Iteration 586/1000 | Loss: 0.00004145
Iteration 587/1000 | Loss: 0.00003324
Iteration 588/1000 | Loss: 0.00003308
Iteration 589/1000 | Loss: 0.00003271
Iteration 590/1000 | Loss: 0.00004235
Iteration 591/1000 | Loss: 0.00003728
Iteration 592/1000 | Loss: 0.00004176
Iteration 593/1000 | Loss: 0.00003828
Iteration 594/1000 | Loss: 0.00003338
Iteration 595/1000 | Loss: 0.00003590
Iteration 596/1000 | Loss: 0.00003375
Iteration 597/1000 | Loss: 0.00005111
Iteration 598/1000 | Loss: 0.00003638
Iteration 599/1000 | Loss: 0.00003421
Iteration 600/1000 | Loss: 0.00003613
Iteration 601/1000 | Loss: 0.00004641
Iteration 602/1000 | Loss: 0.00003832
Iteration 603/1000 | Loss: 0.00004377
Iteration 604/1000 | Loss: 0.00003793
Iteration 605/1000 | Loss: 0.00005014
Iteration 606/1000 | Loss: 0.00003813
Iteration 607/1000 | Loss: 0.00004413
Iteration 608/1000 | Loss: 0.00003819
Iteration 609/1000 | Loss: 0.00004669
Iteration 610/1000 | Loss: 0.00003792
Iteration 611/1000 | Loss: 0.00003433
Iteration 612/1000 | Loss: 0.00003796
Iteration 613/1000 | Loss: 0.00003501
Iteration 614/1000 | Loss: 0.00003266
Iteration 615/1000 | Loss: 0.00004392
Iteration 616/1000 | Loss: 0.00003776
Iteration 617/1000 | Loss: 0.00004669
Iteration 618/1000 | Loss: 0.00004271
Iteration 619/1000 | Loss: 0.00004109
Iteration 620/1000 | Loss: 0.00004391
Iteration 621/1000 | Loss: 0.00004807
Iteration 622/1000 | Loss: 0.00004478
Iteration 623/1000 | Loss: 0.00004359
Iteration 624/1000 | Loss: 0.00004373
Iteration 625/1000 | Loss: 0.00005809
Iteration 626/1000 | Loss: 0.00004257
Iteration 627/1000 | Loss: 0.00004448
Iteration 628/1000 | Loss: 0.00003442
Iteration 629/1000 | Loss: 0.00003328
Iteration 630/1000 | Loss: 0.00003963
Iteration 631/1000 | Loss: 0.00004329
Iteration 632/1000 | Loss: 0.00004721
Iteration 633/1000 | Loss: 0.00004261
Iteration 634/1000 | Loss: 0.00004695
Iteration 635/1000 | Loss: 0.00004070
Iteration 636/1000 | Loss: 0.00004242
Iteration 637/1000 | Loss: 0.00003990
Iteration 638/1000 | Loss: 0.00004574
Iteration 639/1000 | Loss: 0.00003851
Iteration 640/1000 | Loss: 0.00004289
Iteration 641/1000 | Loss: 0.00003692
Iteration 642/1000 | Loss: 0.00004049
Iteration 643/1000 | Loss: 0.00003608
Iteration 644/1000 | Loss: 0.00004220
Iteration 645/1000 | Loss: 0.00003598
Iteration 646/1000 | Loss: 0.00003598
Iteration 647/1000 | Loss: 0.00003939
Iteration 648/1000 | Loss: 0.00003579
Iteration 649/1000 | Loss: 0.00003578
Iteration 650/1000 | Loss: 0.00003825
Iteration 651/1000 | Loss: 0.00003500
Iteration 652/1000 | Loss: 0.00003778
Iteration 653/1000 | Loss: 0.00003511
Iteration 654/1000 | Loss: 0.00003649
Iteration 655/1000 | Loss: 0.00003505
Iteration 656/1000 | Loss: 0.00003555
Iteration 657/1000 | Loss: 0.00003511
Iteration 658/1000 | Loss: 0.00003530
Iteration 659/1000 | Loss: 0.00003493
Iteration 660/1000 | Loss: 0.00003469
Iteration 661/1000 | Loss: 0.00003662
Iteration 662/1000 | Loss: 0.00003480
Iteration 663/1000 | Loss: 0.00003684
Iteration 664/1000 | Loss: 0.00003721
Iteration 665/1000 | Loss: 0.00003761
Iteration 666/1000 | Loss: 0.00003815
Iteration 667/1000 | Loss: 0.00003795
Iteration 668/1000 | Loss: 0.00003972
Iteration 669/1000 | Loss: 0.00003922
Iteration 670/1000 | Loss: 0.00003983
Iteration 671/1000 | Loss: 0.00003692
Iteration 672/1000 | Loss: 0.00003949
Iteration 673/1000 | Loss: 0.00003782
Iteration 674/1000 | Loss: 0.00004032
Iteration 675/1000 | Loss: 0.00003425
Iteration 676/1000 | Loss: 0.00003397
Iteration 677/1000 | Loss: 0.00003282
Iteration 678/1000 | Loss: 0.00003958
Iteration 679/1000 | Loss: 0.00004406
Iteration 680/1000 | Loss: 0.00004229
Iteration 681/1000 | Loss: 0.00004441
Iteration 682/1000 | Loss: 0.00004287
Iteration 683/1000 | Loss: 0.00004520
Iteration 684/1000 | Loss: 0.00004352
Iteration 685/1000 | Loss: 0.00004395
Iteration 686/1000 | Loss: 0.00004319
Iteration 687/1000 | Loss: 0.00003952
Iteration 688/1000 | Loss: 0.00004880
Iteration 689/1000 | Loss: 0.00004029
Iteration 690/1000 | Loss: 0.00004209
Iteration 691/1000 | Loss: 0.00004497
Iteration 692/1000 | Loss: 0.00004177
Iteration 693/1000 | Loss: 0.00004399
Iteration 694/1000 | Loss: 0.00003377
Iteration 695/1000 | Loss: 0.00003286
Iteration 696/1000 | Loss: 0.00004343
Iteration 697/1000 | Loss: 0.00003444
Iteration 698/1000 | Loss: 0.00003360
Iteration 699/1000 | Loss: 0.00003774
Iteration 700/1000 | Loss: 0.00003403
Iteration 701/1000 | Loss: 0.00003278
Iteration 702/1000 | Loss: 0.00004679
Iteration 703/1000 | Loss: 0.00004793
Iteration 704/1000 | Loss: 0.00003444
Iteration 705/1000 | Loss: 0.00003388
Iteration 706/1000 | Loss: 0.00003387
Iteration 707/1000 | Loss: 0.00003715
Iteration 708/1000 | Loss: 0.00003368
Iteration 709/1000 | Loss: 0.00003323
Iteration 710/1000 | Loss: 0.00005255
Iteration 711/1000 | Loss: 0.00003586
Iteration 712/1000 | Loss: 0.00003474
Iteration 713/1000 | Loss: 0.00003327
Iteration 714/1000 | Loss: 0.00003937
Iteration 715/1000 | Loss: 0.00004041
Iteration 716/1000 | Loss: 0.00004337
Iteration 717/1000 | Loss: 0.00003892
Iteration 718/1000 | Loss: 0.00004755
Iteration 719/1000 | Loss: 0.00004620
Iteration 720/1000 | Loss: 0.00004437
Iteration 721/1000 | Loss: 0.00003682
Iteration 722/1000 | Loss: 0.00003498
Iteration 723/1000 | Loss: 0.00004042
Iteration 724/1000 | Loss: 0.00005037
Iteration 725/1000 | Loss: 0.00004251
Iteration 726/1000 | Loss: 0.00003376
Iteration 727/1000 | Loss: 0.00003274
Iteration 728/1000 | Loss: 0.00004001
Iteration 729/1000 | Loss: 0.00004147
Iteration 730/1000 | Loss: 0.00004563
Iteration 731/1000 | Loss: 0.00003763
Iteration 732/1000 | Loss: 0.00004284
Iteration 733/1000 | Loss: 0.00004760
Iteration 734/1000 | Loss: 0.00003967
Iteration 735/1000 | Loss: 0.00003876
Iteration 736/1000 | Loss: 0.00004426
Iteration 737/1000 | Loss: 0.00005400
Iteration 738/1000 | Loss: 0.00004341
Iteration 739/1000 | Loss: 0.00004229
Iteration 740/1000 | Loss: 0.00004490
Iteration 741/1000 | Loss: 0.00004121
Iteration 742/1000 | Loss: 0.00004969
Iteration 743/1000 | Loss: 0.00003744
Iteration 744/1000 | Loss: 0.00003613
Iteration 745/1000 | Loss: 0.00003397
Iteration 746/1000 | Loss: 0.00004682
Iteration 747/1000 | Loss: 0.00003481
Iteration 748/1000 | Loss: 0.00003294
Iteration 749/1000 | Loss: 0.00004178
Iteration 750/1000 | Loss: 0.00004033
Iteration 751/1000 | Loss: 0.00003576
Iteration 752/1000 | Loss: 0.00004399
Iteration 753/1000 | Loss: 0.00004534
Iteration 754/1000 | Loss: 0.00005661
Iteration 755/1000 | Loss: 0.00004414
Iteration 756/1000 | Loss: 0.00004209
Iteration 757/1000 | Loss: 0.00004389
Iteration 758/1000 | Loss: 0.00004475
Iteration 759/1000 | Loss: 0.00004426
Iteration 760/1000 | Loss: 0.00003606
Iteration 761/1000 | Loss: 0.00004457
Iteration 762/1000 | Loss: 0.00003323
Iteration 763/1000 | Loss: 0.00003897
Iteration 764/1000 | Loss: 0.00004357
Iteration 765/1000 | Loss: 0.00003931
Iteration 766/1000 | Loss: 0.00003314
Iteration 767/1000 | Loss: 0.00003301
Iteration 768/1000 | Loss: 0.00003262
Iteration 769/1000 | Loss: 0.00003260
Iteration 770/1000 | Loss: 0.00003959
Iteration 771/1000 | Loss: 0.00003544
Iteration 772/1000 | Loss: 0.00003379
Iteration 773/1000 | Loss: 0.00003550
Iteration 774/1000 | Loss: 0.00003370
Iteration 775/1000 | Loss: 0.00003672
Iteration 776/1000 | Loss: 0.00003583
Iteration 777/1000 | Loss: 0.00003694
Iteration 778/1000 | Loss: 0.00003659
Iteration 779/1000 | Loss: 0.00003730
Iteration 780/1000 | Loss: 0.00003642
Iteration 781/1000 | Loss: 0.00003596
Iteration 782/1000 | Loss: 0.00003601
Iteration 783/1000 | Loss: 0.00003764
Iteration 784/1000 | Loss: 0.00003585
Iteration 785/1000 | Loss: 0.00004056
Iteration 786/1000 | Loss: 0.00003653
Iteration 787/1000 | Loss: 0.00004195
Iteration 788/1000 | Loss: 0.00004371
Iteration 789/1000 | Loss: 0.00004158
Iteration 790/1000 | Loss: 0.00004205
Iteration 791/1000 | Loss: 0.00004486
Iteration 792/1000 | Loss: 0.00003456
Iteration 793/1000 | Loss: 0.00003937
Iteration 794/1000 | Loss: 0.00003394
Iteration 795/1000 | Loss: 0.00003799
Iteration 796/1000 | Loss: 0.00004064
Iteration 797/1000 | Loss: 0.00004475
Iteration 798/1000 | Loss: 0.00003761
Iteration 799/1000 | Loss: 0.00003379
Iteration 800/1000 | Loss: 0.00003652
Iteration 801/1000 | Loss: 0.00003378
Iteration 802/1000 | Loss: 0.00003607
Iteration 803/1000 | Loss: 0.00003395
Iteration 804/1000 | Loss: 0.00003356
Iteration 805/1000 | Loss: 0.00003532
Iteration 806/1000 | Loss: 0.00003362
Iteration 807/1000 | Loss: 0.00003279
Iteration 808/1000 | Loss: 0.00004142
Iteration 809/1000 | Loss: 0.00003857
Iteration 810/1000 | Loss: 0.00003711
Iteration 811/1000 | Loss: 0.00004315
Iteration 812/1000 | Loss: 0.00003759
Iteration 813/1000 | Loss: 0.00003392
Iteration 814/1000 | Loss: 0.00004131
Iteration 815/1000 | Loss: 0.00003751
Iteration 816/1000 | Loss: 0.00003425
Iteration 817/1000 | Loss: 0.00004152
Iteration 818/1000 | Loss: 0.00003894
Iteration 819/1000 | Loss: 0.00003387
Iteration 820/1000 | Loss: 0.00003840
Iteration 821/1000 | Loss: 0.00003438
Iteration 822/1000 | Loss: 0.00003341
Iteration 823/1000 | Loss: 0.00004238
Iteration 824/1000 | Loss: 0.00004626
Iteration 825/1000 | Loss: 0.00003797
Iteration 826/1000 | Loss: 0.00003634
Iteration 827/1000 | Loss: 0.00003352
Iteration 828/1000 | Loss: 0.00004090
Iteration 829/1000 | Loss: 0.00003507
Iteration 830/1000 | Loss: 0.00003375
Iteration 831/1000 | Loss: 0.00003315
Iteration 832/1000 | Loss: 0.00003999
Iteration 833/1000 | Loss: 0.00003380
Iteration 834/1000 | Loss: 0.00003687
Iteration 835/1000 | Loss: 0.00003455
Iteration 836/1000 | Loss: 0.00004351
Iteration 837/1000 | Loss: 0.00003748
Iteration 838/1000 | Loss: 0.00003419
Iteration 839/1000 | Loss: 0.00003452
Iteration 840/1000 | Loss: 0.00004150
Iteration 841/1000 | Loss: 0.00004202
Iteration 842/1000 | Loss: 0.00003818
Iteration 843/1000 | Loss: 0.00004084
Iteration 844/1000 | Loss: 0.00004330
Iteration 845/1000 | Loss: 0.00003792
Iteration 846/1000 | Loss: 0.00003936
Iteration 847/1000 | Loss: 0.00004050
Iteration 848/1000 | Loss: 0.00004235
Iteration 849/1000 | Loss: 0.00003869
Iteration 850/1000 | Loss: 0.00004129
Iteration 851/1000 | Loss: 0.00003764
Iteration 852/1000 | Loss: 0.00004433
Iteration 853/1000 | Loss: 0.00003782
Iteration 854/1000 | Loss: 0.00004066
Iteration 855/1000 | Loss: 0.00004049
Iteration 856/1000 | Loss: 0.00003873
Iteration 857/1000 | Loss: 0.00004338
Iteration 858/1000 | Loss: 0.00004039
Iteration 859/1000 | Loss: 0.00004290
Iteration 860/1000 | Loss: 0.00004289
Iteration 861/1000 | Loss: 0.00003916
Iteration 862/1000 | Loss: 0.00004253
Iteration 863/1000 | Loss: 0.00003471
Iteration 864/1000 | Loss: 0.00003830
Iteration 865/1000 | Loss: 0.00003979
Iteration 866/1000 | Loss: 0.00004057
Iteration 867/1000 | Loss: 0.00004199
Iteration 868/1000 | Loss: 0.00004141
Iteration 869/1000 | Loss: 0.00004631
Iteration 870/1000 | Loss: 0.00004054
Iteration 871/1000 | Loss: 0.00003962
Iteration 872/1000 | Loss: 0.00003519
Iteration 873/1000 | Loss: 0.00003305
Iteration 874/1000 | Loss: 0.00003680
Iteration 875/1000 | Loss: 0.00003561
Iteration 876/1000 | Loss: 0.00003368
Iteration 877/1000 | Loss: 0.00004010
Iteration 878/1000 | Loss: 0.00003346
Iteration 879/1000 | Loss: 0.00003258
Iteration 880/1000 | Loss: 0.00004450
Iteration 881/1000 | Loss: 0.00003473
Iteration 882/1000 | Loss: 0.00003364
Iteration 883/1000 | Loss: 0.00003844
Iteration 884/1000 | Loss: 0.00003350
Iteration 885/1000 | Loss: 0.00004236
Iteration 886/1000 | Loss: 0.00004109
Iteration 887/1000 | Loss: 0.00004566
Iteration 888/1000 | Loss: 0.00004432
Iteration 889/1000 | Loss: 0.00004648
Iteration 890/1000 | Loss: 0.00003367
Iteration 891/1000 | Loss: 0.00003798
Iteration 892/1000 | Loss: 0.00003398
Iteration 893/1000 | Loss: 0.00003284
Iteration 894/1000 | Loss: 0.00003758
Iteration 895/1000 | Loss: 0.00003529
Iteration 896/1000 | Loss: 0.00003357
Iteration 897/1000 | Loss: 0.00003770
Iteration 898/1000 | Loss: 0.00003772
Iteration 899/1000 | Loss: 0.00003557
Iteration 900/1000 | Loss: 0.00003282
Iteration 901/1000 | Loss: 0.00003289
Iteration 902/1000 | Loss: 0.00003737
Iteration 903/1000 | Loss: 0.00003318
Iteration 904/1000 | Loss: 0.00003794
Iteration 905/1000 | Loss: 0.00003816
Iteration 906/1000 | Loss: 0.00004129
Iteration 907/1000 | Loss: 0.00003420
Iteration 908/1000 | Loss: 0.00004177
Iteration 909/1000 | Loss: 0.00003587
Iteration 910/1000 | Loss: 0.00003288
Iteration 911/1000 | Loss: 0.00004407
Iteration 912/1000 | Loss: 0.00003802
Iteration 913/1000 | Loss: 0.00004822
Iteration 914/1000 | Loss: 0.00003734
Iteration 915/1000 | Loss: 0.00003279
Iteration 916/1000 | Loss: 0.00004855
Iteration 917/1000 | Loss: 0.00003486
Iteration 918/1000 | Loss: 0.00003865
Iteration 919/1000 | Loss: 0.00003580
Iteration 920/1000 | Loss: 0.00003826
Iteration 921/1000 | Loss: 0.00003752
Iteration 922/1000 | Loss: 0.00003757
Iteration 923/1000 | Loss: 0.00004185
Iteration 924/1000 | Loss: 0.00004271
Iteration 925/1000 | Loss: 0.00004242
Iteration 926/1000 | Loss: 0.00004159
Iteration 927/1000 | Loss: 0.00004014
Iteration 928/1000 | Loss: 0.00004109
Iteration 929/1000 | Loss: 0.00004050
Iteration 930/1000 | Loss: 0.00004007
Iteration 931/1000 | Loss: 0.00003655
Iteration 932/1000 | Loss: 0.00003439
Iteration 933/1000 | Loss: 0.00004188
Iteration 934/1000 | Loss: 0.00003391
Iteration 935/1000 | Loss: 0.00003726
Iteration 936/1000 | Loss: 0.00003579
Iteration 937/1000 | Loss: 0.00003634
Iteration 938/1000 | Loss: 0.00003352
Iteration 939/1000 | Loss: 0.00003309
Iteration 940/1000 | Loss: 0.00003625
Iteration 941/1000 | Loss: 0.00004438
Iteration 942/1000 | Loss: 0.00003291
Iteration 943/1000 | Loss: 0.00003932
Iteration 944/1000 | Loss: 0.00004058
Iteration 945/1000 | Loss: 0.00003873
Iteration 946/1000 | Loss: 0.00004484
Iteration 947/1000 | Loss: 0.00003889
Iteration 948/1000 | Loss: 0.00004299
Iteration 949/1000 | Loss: 0.00003858
Iteration 950/1000 | Loss: 0.00004123
Iteration 951/1000 | Loss: 0.00004080
Iteration 952/1000 | Loss: 0.00003939
Iteration 953/1000 | Loss: 0.00003857
Iteration 954/1000 | Loss: 0.00004234
Iteration 955/1000 | Loss: 0.00004163
Iteration 956/1000 | Loss: 0.00004181
Iteration 957/1000 | Loss: 0.00004116
Iteration 958/1000 | Loss: 0.00004419
Iteration 959/1000 | Loss: 0.00003834
Iteration 960/1000 | Loss: 0.00004060
Iteration 961/1000 | Loss: 0.00004060
Iteration 962/1000 | Loss: 0.00004060
Iteration 963/1000 | Loss: 0.00003633
Iteration 964/1000 | Loss: 0.00004017
Iteration 965/1000 | Loss: 0.00003428
Iteration 966/1000 | Loss: 0.00003371
Iteration 967/1000 | Loss: 0.00003650
Iteration 968/1000 | Loss: 0.00003313
Iteration 969/1000 | Loss: 0.00004094
Iteration 970/1000 | Loss: 0.00004376
Iteration 971/1000 | Loss: 0.00003535
Iteration 972/1000 | Loss: 0.00003489
Iteration 973/1000 | Loss: 0.00003773
Iteration 974/1000 | Loss: 0.00003320
Iteration 975/1000 | Loss: 0.00004346
Iteration 976/1000 | Loss: 0.00003777
Iteration 977/1000 | Loss: 0.00003934
Iteration 978/1000 | Loss: 0.00004261
Iteration 979/1000 | Loss: 0.00003892
Iteration 980/1000 | Loss: 0.00004458
Iteration 981/1000 | Loss: 0.00003687
Iteration 982/1000 | Loss: 0.00004046
Iteration 983/1000 | Loss: 0.00003835
Iteration 984/1000 | Loss: 0.00003824
Iteration 985/1000 | Loss: 0.00004168
Iteration 986/1000 | Loss: 0.00004089
Iteration 987/1000 | Loss: 0.00003708
Iteration 988/1000 | Loss: 0.00003635
Iteration 989/1000 | Loss: 0.00003360
Iteration 990/1000 | Loss: 0.00003874
Iteration 991/1000 | Loss: 0.00003306
Iteration 992/1000 | Loss: 0.00004288
Iteration 993/1000 | Loss: 0.00003395
Iteration 994/1000 | Loss: 0.00003325
Iteration 995/1000 | Loss: 0.00003964
Iteration 996/1000 | Loss: 0.00003379
Iteration 997/1000 | Loss: 0.00003389
Iteration 998/1000 | Loss: 0.00003479
Iteration 999/1000 | Loss: 0.00003391
Iteration 1000/1000 | Loss: 0.00004006

Optimization complete. Final v2v error: 4.840353488922119 mm

Highest mean error: 17.995441436767578 mm for frame 72

Lowest mean error: 4.137711524963379 mm for frame 1

Saving results

Total time: 1374.7074830532074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141836
Iteration 2/25 | Loss: 0.00393678
Iteration 3/25 | Loss: 0.00292872
Iteration 4/25 | Loss: 0.00358009
Iteration 5/25 | Loss: 0.00243127
Iteration 6/25 | Loss: 0.00211243
Iteration 7/25 | Loss: 0.00198496
Iteration 8/25 | Loss: 0.00194931
Iteration 9/25 | Loss: 0.00193406
Iteration 10/25 | Loss: 0.00193168
Iteration 11/25 | Loss: 0.00193140
Iteration 12/25 | Loss: 0.00193133
Iteration 13/25 | Loss: 0.00193133
Iteration 14/25 | Loss: 0.00193133
Iteration 15/25 | Loss: 0.00193133
Iteration 16/25 | Loss: 0.00193133
Iteration 17/25 | Loss: 0.00193133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019313266966491938, 0.0019313266966491938, 0.0019313266966491938, 0.0019313266966491938, 0.0019313266966491938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019313266966491938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39987326
Iteration 2/25 | Loss: 0.00145770
Iteration 3/25 | Loss: 0.00144246
Iteration 4/25 | Loss: 0.00144246
Iteration 5/25 | Loss: 0.00144246
Iteration 6/25 | Loss: 0.00144246
Iteration 7/25 | Loss: 0.00144246
Iteration 8/25 | Loss: 0.00144246
Iteration 9/25 | Loss: 0.00144246
Iteration 10/25 | Loss: 0.00144246
Iteration 11/25 | Loss: 0.00144246
Iteration 12/25 | Loss: 0.00144246
Iteration 13/25 | Loss: 0.00144246
Iteration 14/25 | Loss: 0.00144246
Iteration 15/25 | Loss: 0.00144246
Iteration 16/25 | Loss: 0.00144246
Iteration 17/25 | Loss: 0.00144246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014424559194594622, 0.0014424559194594622, 0.0014424559194594622, 0.0014424559194594622, 0.0014424559194594622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014424559194594622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144246
Iteration 2/1000 | Loss: 0.00014312
Iteration 3/1000 | Loss: 0.00008577
Iteration 4/1000 | Loss: 0.00020546
Iteration 5/1000 | Loss: 0.00009102
Iteration 6/1000 | Loss: 0.00024245
Iteration 7/1000 | Loss: 0.00021939
Iteration 8/1000 | Loss: 0.00006441
Iteration 9/1000 | Loss: 0.00016936
Iteration 10/1000 | Loss: 0.00006532
Iteration 11/1000 | Loss: 0.00005524
Iteration 12/1000 | Loss: 0.00141689
Iteration 13/1000 | Loss: 0.00054363
Iteration 14/1000 | Loss: 0.00013072
Iteration 15/1000 | Loss: 0.00037197
Iteration 16/1000 | Loss: 0.00046529
Iteration 17/1000 | Loss: 0.00093752
Iteration 18/1000 | Loss: 0.00216721
Iteration 19/1000 | Loss: 0.00279299
Iteration 20/1000 | Loss: 0.00187456
Iteration 21/1000 | Loss: 0.00337373
Iteration 22/1000 | Loss: 0.00313648
Iteration 23/1000 | Loss: 0.00067803
Iteration 24/1000 | Loss: 0.00009340
Iteration 25/1000 | Loss: 0.00013096
Iteration 26/1000 | Loss: 0.00011346
Iteration 27/1000 | Loss: 0.00010694
Iteration 28/1000 | Loss: 0.00022974
Iteration 29/1000 | Loss: 0.00017912
Iteration 30/1000 | Loss: 0.00019604
Iteration 31/1000 | Loss: 0.00018601
Iteration 32/1000 | Loss: 0.00014239
Iteration 33/1000 | Loss: 0.00017488
Iteration 34/1000 | Loss: 0.00011520
Iteration 35/1000 | Loss: 0.00017267
Iteration 36/1000 | Loss: 0.00011958
Iteration 37/1000 | Loss: 0.00027194
Iteration 38/1000 | Loss: 0.00016054
Iteration 39/1000 | Loss: 0.00027438
Iteration 40/1000 | Loss: 0.00023169
Iteration 41/1000 | Loss: 0.00011086
Iteration 42/1000 | Loss: 0.00012802
Iteration 43/1000 | Loss: 0.00018694
Iteration 44/1000 | Loss: 0.00006755
Iteration 45/1000 | Loss: 0.00013894
Iteration 46/1000 | Loss: 0.00029225
Iteration 47/1000 | Loss: 0.00016399
Iteration 48/1000 | Loss: 0.00047194
Iteration 49/1000 | Loss: 0.00035715
Iteration 50/1000 | Loss: 0.00019211
Iteration 51/1000 | Loss: 0.00005688
Iteration 52/1000 | Loss: 0.00005749
Iteration 53/1000 | Loss: 0.00003986
Iteration 54/1000 | Loss: 0.00045458
Iteration 55/1000 | Loss: 0.00025020
Iteration 56/1000 | Loss: 0.00006937
Iteration 57/1000 | Loss: 0.00008879
Iteration 58/1000 | Loss: 0.00008534
Iteration 59/1000 | Loss: 0.00005086
Iteration 60/1000 | Loss: 0.00010395
Iteration 61/1000 | Loss: 0.00011537
Iteration 62/1000 | Loss: 0.00004279
Iteration 63/1000 | Loss: 0.00035601
Iteration 64/1000 | Loss: 0.00018528
Iteration 65/1000 | Loss: 0.00023319
Iteration 66/1000 | Loss: 0.00021841
Iteration 67/1000 | Loss: 0.00033309
Iteration 68/1000 | Loss: 0.00013938
Iteration 69/1000 | Loss: 0.00004728
Iteration 70/1000 | Loss: 0.00015171
Iteration 71/1000 | Loss: 0.00007709
Iteration 72/1000 | Loss: 0.00045493
Iteration 73/1000 | Loss: 0.00021778
Iteration 74/1000 | Loss: 0.00011635
Iteration 75/1000 | Loss: 0.00009189
Iteration 76/1000 | Loss: 0.00004732
Iteration 77/1000 | Loss: 0.00008038
Iteration 78/1000 | Loss: 0.00007295
Iteration 79/1000 | Loss: 0.00003956
Iteration 80/1000 | Loss: 0.00009340
Iteration 81/1000 | Loss: 0.00021663
Iteration 82/1000 | Loss: 0.00018237
Iteration 83/1000 | Loss: 0.00004164
Iteration 84/1000 | Loss: 0.00035434
Iteration 85/1000 | Loss: 0.00005256
Iteration 86/1000 | Loss: 0.00004707
Iteration 87/1000 | Loss: 0.00003211
Iteration 88/1000 | Loss: 0.00004379
Iteration 89/1000 | Loss: 0.00011099
Iteration 90/1000 | Loss: 0.00003453
Iteration 91/1000 | Loss: 0.00004471
Iteration 92/1000 | Loss: 0.00004072
Iteration 93/1000 | Loss: 0.00014719
Iteration 94/1000 | Loss: 0.00005509
Iteration 95/1000 | Loss: 0.00004760
Iteration 96/1000 | Loss: 0.00004859
Iteration 97/1000 | Loss: 0.00003145
Iteration 98/1000 | Loss: 0.00002975
Iteration 99/1000 | Loss: 0.00002975
Iteration 100/1000 | Loss: 0.00002973
Iteration 101/1000 | Loss: 0.00002973
Iteration 102/1000 | Loss: 0.00002972
Iteration 103/1000 | Loss: 0.00002972
Iteration 104/1000 | Loss: 0.00002971
Iteration 105/1000 | Loss: 0.00002970
Iteration 106/1000 | Loss: 0.00002970
Iteration 107/1000 | Loss: 0.00002970
Iteration 108/1000 | Loss: 0.00003829
Iteration 109/1000 | Loss: 0.00003621
Iteration 110/1000 | Loss: 0.00002967
Iteration 111/1000 | Loss: 0.00002967
Iteration 112/1000 | Loss: 0.00002966
Iteration 113/1000 | Loss: 0.00002966
Iteration 114/1000 | Loss: 0.00002966
Iteration 115/1000 | Loss: 0.00002966
Iteration 116/1000 | Loss: 0.00002966
Iteration 117/1000 | Loss: 0.00002966
Iteration 118/1000 | Loss: 0.00003948
Iteration 119/1000 | Loss: 0.00003107
Iteration 120/1000 | Loss: 0.00003207
Iteration 121/1000 | Loss: 0.00002965
Iteration 122/1000 | Loss: 0.00002965
Iteration 123/1000 | Loss: 0.00002965
Iteration 124/1000 | Loss: 0.00004278
Iteration 125/1000 | Loss: 0.00004279
Iteration 126/1000 | Loss: 0.00007247
Iteration 127/1000 | Loss: 0.00004251
Iteration 128/1000 | Loss: 0.00004797
Iteration 129/1000 | Loss: 0.00006080
Iteration 130/1000 | Loss: 0.00003850
Iteration 131/1000 | Loss: 0.00018968
Iteration 132/1000 | Loss: 0.00006095
Iteration 133/1000 | Loss: 0.00003632
Iteration 134/1000 | Loss: 0.00003187
Iteration 135/1000 | Loss: 0.00003993
Iteration 136/1000 | Loss: 0.00007463
Iteration 137/1000 | Loss: 0.00003671
Iteration 138/1000 | Loss: 0.00002961
Iteration 139/1000 | Loss: 0.00002960
Iteration 140/1000 | Loss: 0.00002959
Iteration 141/1000 | Loss: 0.00002959
Iteration 142/1000 | Loss: 0.00002958
Iteration 143/1000 | Loss: 0.00002958
Iteration 144/1000 | Loss: 0.00002957
Iteration 145/1000 | Loss: 0.00002957
Iteration 146/1000 | Loss: 0.00002957
Iteration 147/1000 | Loss: 0.00002956
Iteration 148/1000 | Loss: 0.00003626
Iteration 149/1000 | Loss: 0.00016013
Iteration 150/1000 | Loss: 0.00004672
Iteration 151/1000 | Loss: 0.00003986
Iteration 152/1000 | Loss: 0.00003405
Iteration 153/1000 | Loss: 0.00003506
Iteration 154/1000 | Loss: 0.00004553
Iteration 155/1000 | Loss: 0.00003377
Iteration 156/1000 | Loss: 0.00005236
Iteration 157/1000 | Loss: 0.00003559
Iteration 158/1000 | Loss: 0.00002961
Iteration 159/1000 | Loss: 0.00005853
Iteration 160/1000 | Loss: 0.00007402
Iteration 161/1000 | Loss: 0.00004543
Iteration 162/1000 | Loss: 0.00003463
Iteration 163/1000 | Loss: 0.00004300
Iteration 164/1000 | Loss: 0.00006156
Iteration 165/1000 | Loss: 0.00003340
Iteration 166/1000 | Loss: 0.00003983
Iteration 167/1000 | Loss: 0.00004737
Iteration 168/1000 | Loss: 0.00005144
Iteration 169/1000 | Loss: 0.00004549
Iteration 170/1000 | Loss: 0.00005343
Iteration 171/1000 | Loss: 0.00002970
Iteration 172/1000 | Loss: 0.00003665
Iteration 173/1000 | Loss: 0.00029339
Iteration 174/1000 | Loss: 0.00006790
Iteration 175/1000 | Loss: 0.00005903
Iteration 176/1000 | Loss: 0.00004912
Iteration 177/1000 | Loss: 0.00003567
Iteration 178/1000 | Loss: 0.00003776
Iteration 179/1000 | Loss: 0.00002962
Iteration 180/1000 | Loss: 0.00002958
Iteration 181/1000 | Loss: 0.00003349
Iteration 182/1000 | Loss: 0.00003223
Iteration 183/1000 | Loss: 0.00004579
Iteration 184/1000 | Loss: 0.00003038
Iteration 185/1000 | Loss: 0.00003218
Iteration 186/1000 | Loss: 0.00003365
Iteration 187/1000 | Loss: 0.00003411
Iteration 188/1000 | Loss: 0.00006961
Iteration 189/1000 | Loss: 0.00002961
Iteration 190/1000 | Loss: 0.00002956
Iteration 191/1000 | Loss: 0.00002956
Iteration 192/1000 | Loss: 0.00002955
Iteration 193/1000 | Loss: 0.00002955
Iteration 194/1000 | Loss: 0.00002955
Iteration 195/1000 | Loss: 0.00002955
Iteration 196/1000 | Loss: 0.00002955
Iteration 197/1000 | Loss: 0.00002955
Iteration 198/1000 | Loss: 0.00002955
Iteration 199/1000 | Loss: 0.00002955
Iteration 200/1000 | Loss: 0.00002955
Iteration 201/1000 | Loss: 0.00002955
Iteration 202/1000 | Loss: 0.00002955
Iteration 203/1000 | Loss: 0.00002954
Iteration 204/1000 | Loss: 0.00002954
Iteration 205/1000 | Loss: 0.00002954
Iteration 206/1000 | Loss: 0.00002954
Iteration 207/1000 | Loss: 0.00002954
Iteration 208/1000 | Loss: 0.00002954
Iteration 209/1000 | Loss: 0.00002954
Iteration 210/1000 | Loss: 0.00002954
Iteration 211/1000 | Loss: 0.00002954
Iteration 212/1000 | Loss: 0.00002954
Iteration 213/1000 | Loss: 0.00002954
Iteration 214/1000 | Loss: 0.00002954
Iteration 215/1000 | Loss: 0.00002954
Iteration 216/1000 | Loss: 0.00002953
Iteration 217/1000 | Loss: 0.00002953
Iteration 218/1000 | Loss: 0.00002953
Iteration 219/1000 | Loss: 0.00002953
Iteration 220/1000 | Loss: 0.00002953
Iteration 221/1000 | Loss: 0.00002953
Iteration 222/1000 | Loss: 0.00002953
Iteration 223/1000 | Loss: 0.00002953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.9533368433476426e-05, 2.9533368433476426e-05, 2.9533368433476426e-05, 2.9533368433476426e-05, 2.9533368433476426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9533368433476426e-05

Optimization complete. Final v2v error: 4.687408447265625 mm

Highest mean error: 5.132989883422852 mm for frame 220

Lowest mean error: 4.525893688201904 mm for frame 12

Saving results

Total time: 265.6801233291626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600761
Iteration 2/25 | Loss: 0.00219917
Iteration 3/25 | Loss: 0.00189340
Iteration 4/25 | Loss: 0.00187502
Iteration 5/25 | Loss: 0.00187251
Iteration 6/25 | Loss: 0.00187198
Iteration 7/25 | Loss: 0.00187198
Iteration 8/25 | Loss: 0.00187198
Iteration 9/25 | Loss: 0.00187198
Iteration 10/25 | Loss: 0.00187198
Iteration 11/25 | Loss: 0.00187198
Iteration 12/25 | Loss: 0.00187198
Iteration 13/25 | Loss: 0.00187198
Iteration 14/25 | Loss: 0.00187198
Iteration 15/25 | Loss: 0.00187198
Iteration 16/25 | Loss: 0.00187198
Iteration 17/25 | Loss: 0.00187198
Iteration 18/25 | Loss: 0.00187198
Iteration 19/25 | Loss: 0.00187198
Iteration 20/25 | Loss: 0.00187198
Iteration 21/25 | Loss: 0.00187198
Iteration 22/25 | Loss: 0.00187198
Iteration 23/25 | Loss: 0.00187198
Iteration 24/25 | Loss: 0.00187198
Iteration 25/25 | Loss: 0.00187198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81975019
Iteration 2/25 | Loss: 0.00159715
Iteration 3/25 | Loss: 0.00159715
Iteration 4/25 | Loss: 0.00159715
Iteration 5/25 | Loss: 0.00159715
Iteration 6/25 | Loss: 0.00159715
Iteration 7/25 | Loss: 0.00159715
Iteration 8/25 | Loss: 0.00159715
Iteration 9/25 | Loss: 0.00159715
Iteration 10/25 | Loss: 0.00159715
Iteration 11/25 | Loss: 0.00159715
Iteration 12/25 | Loss: 0.00159715
Iteration 13/25 | Loss: 0.00159715
Iteration 14/25 | Loss: 0.00159715
Iteration 15/25 | Loss: 0.00159715
Iteration 16/25 | Loss: 0.00159715
Iteration 17/25 | Loss: 0.00159715
Iteration 18/25 | Loss: 0.00159715
Iteration 19/25 | Loss: 0.00159715
Iteration 20/25 | Loss: 0.00159715
Iteration 21/25 | Loss: 0.00159715
Iteration 22/25 | Loss: 0.00159715
Iteration 23/25 | Loss: 0.00159715
Iteration 24/25 | Loss: 0.00159715
Iteration 25/25 | Loss: 0.00159715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159715
Iteration 2/1000 | Loss: 0.00011123
Iteration 3/1000 | Loss: 0.00008238
Iteration 4/1000 | Loss: 0.00007285
Iteration 5/1000 | Loss: 0.00006992
Iteration 6/1000 | Loss: 0.00006792
Iteration 7/1000 | Loss: 0.00006618
Iteration 8/1000 | Loss: 0.00006527
Iteration 9/1000 | Loss: 0.00006412
Iteration 10/1000 | Loss: 0.00006336
Iteration 11/1000 | Loss: 0.00006262
Iteration 12/1000 | Loss: 0.00006163
Iteration 13/1000 | Loss: 0.00006105
Iteration 14/1000 | Loss: 0.00006042
Iteration 15/1000 | Loss: 0.00005983
Iteration 16/1000 | Loss: 0.00005937
Iteration 17/1000 | Loss: 0.00005910
Iteration 18/1000 | Loss: 0.00005886
Iteration 19/1000 | Loss: 0.00005862
Iteration 20/1000 | Loss: 0.00005843
Iteration 21/1000 | Loss: 0.00005826
Iteration 22/1000 | Loss: 0.00005824
Iteration 23/1000 | Loss: 0.00005819
Iteration 24/1000 | Loss: 0.00005816
Iteration 25/1000 | Loss: 0.00005812
Iteration 26/1000 | Loss: 0.00005803
Iteration 27/1000 | Loss: 0.00005798
Iteration 28/1000 | Loss: 0.00005798
Iteration 29/1000 | Loss: 0.00005795
Iteration 30/1000 | Loss: 0.00005795
Iteration 31/1000 | Loss: 0.00005794
Iteration 32/1000 | Loss: 0.00005794
Iteration 33/1000 | Loss: 0.00005794
Iteration 34/1000 | Loss: 0.00005794
Iteration 35/1000 | Loss: 0.00005794
Iteration 36/1000 | Loss: 0.00005794
Iteration 37/1000 | Loss: 0.00005794
Iteration 38/1000 | Loss: 0.00005791
Iteration 39/1000 | Loss: 0.00005791
Iteration 40/1000 | Loss: 0.00005791
Iteration 41/1000 | Loss: 0.00005790
Iteration 42/1000 | Loss: 0.00005790
Iteration 43/1000 | Loss: 0.00005790
Iteration 44/1000 | Loss: 0.00005790
Iteration 45/1000 | Loss: 0.00005790
Iteration 46/1000 | Loss: 0.00005790
Iteration 47/1000 | Loss: 0.00005790
Iteration 48/1000 | Loss: 0.00005790
Iteration 49/1000 | Loss: 0.00005790
Iteration 50/1000 | Loss: 0.00005790
Iteration 51/1000 | Loss: 0.00005790
Iteration 52/1000 | Loss: 0.00005789
Iteration 53/1000 | Loss: 0.00005789
Iteration 54/1000 | Loss: 0.00005788
Iteration 55/1000 | Loss: 0.00005788
Iteration 56/1000 | Loss: 0.00005788
Iteration 57/1000 | Loss: 0.00005788
Iteration 58/1000 | Loss: 0.00005788
Iteration 59/1000 | Loss: 0.00005788
Iteration 60/1000 | Loss: 0.00005788
Iteration 61/1000 | Loss: 0.00005788
Iteration 62/1000 | Loss: 0.00005788
Iteration 63/1000 | Loss: 0.00005787
Iteration 64/1000 | Loss: 0.00005787
Iteration 65/1000 | Loss: 0.00005787
Iteration 66/1000 | Loss: 0.00005787
Iteration 67/1000 | Loss: 0.00005787
Iteration 68/1000 | Loss: 0.00005787
Iteration 69/1000 | Loss: 0.00005787
Iteration 70/1000 | Loss: 0.00005786
Iteration 71/1000 | Loss: 0.00005786
Iteration 72/1000 | Loss: 0.00005786
Iteration 73/1000 | Loss: 0.00005786
Iteration 74/1000 | Loss: 0.00005786
Iteration 75/1000 | Loss: 0.00005786
Iteration 76/1000 | Loss: 0.00005786
Iteration 77/1000 | Loss: 0.00005785
Iteration 78/1000 | Loss: 0.00005785
Iteration 79/1000 | Loss: 0.00005785
Iteration 80/1000 | Loss: 0.00005785
Iteration 81/1000 | Loss: 0.00005785
Iteration 82/1000 | Loss: 0.00005785
Iteration 83/1000 | Loss: 0.00005785
Iteration 84/1000 | Loss: 0.00005785
Iteration 85/1000 | Loss: 0.00005785
Iteration 86/1000 | Loss: 0.00005785
Iteration 87/1000 | Loss: 0.00005784
Iteration 88/1000 | Loss: 0.00005784
Iteration 89/1000 | Loss: 0.00005784
Iteration 90/1000 | Loss: 0.00005784
Iteration 91/1000 | Loss: 0.00005784
Iteration 92/1000 | Loss: 0.00005783
Iteration 93/1000 | Loss: 0.00005783
Iteration 94/1000 | Loss: 0.00005783
Iteration 95/1000 | Loss: 0.00005783
Iteration 96/1000 | Loss: 0.00005783
Iteration 97/1000 | Loss: 0.00005783
Iteration 98/1000 | Loss: 0.00005783
Iteration 99/1000 | Loss: 0.00005783
Iteration 100/1000 | Loss: 0.00005783
Iteration 101/1000 | Loss: 0.00005783
Iteration 102/1000 | Loss: 0.00005783
Iteration 103/1000 | Loss: 0.00005783
Iteration 104/1000 | Loss: 0.00005783
Iteration 105/1000 | Loss: 0.00005783
Iteration 106/1000 | Loss: 0.00005782
Iteration 107/1000 | Loss: 0.00005782
Iteration 108/1000 | Loss: 0.00005782
Iteration 109/1000 | Loss: 0.00005782
Iteration 110/1000 | Loss: 0.00005782
Iteration 111/1000 | Loss: 0.00005782
Iteration 112/1000 | Loss: 0.00005782
Iteration 113/1000 | Loss: 0.00005782
Iteration 114/1000 | Loss: 0.00005782
Iteration 115/1000 | Loss: 0.00005782
Iteration 116/1000 | Loss: 0.00005782
Iteration 117/1000 | Loss: 0.00005782
Iteration 118/1000 | Loss: 0.00005782
Iteration 119/1000 | Loss: 0.00005782
Iteration 120/1000 | Loss: 0.00005782
Iteration 121/1000 | Loss: 0.00005782
Iteration 122/1000 | Loss: 0.00005782
Iteration 123/1000 | Loss: 0.00005782
Iteration 124/1000 | Loss: 0.00005782
Iteration 125/1000 | Loss: 0.00005781
Iteration 126/1000 | Loss: 0.00005781
Iteration 127/1000 | Loss: 0.00005781
Iteration 128/1000 | Loss: 0.00005781
Iteration 129/1000 | Loss: 0.00005781
Iteration 130/1000 | Loss: 0.00005781
Iteration 131/1000 | Loss: 0.00005781
Iteration 132/1000 | Loss: 0.00005781
Iteration 133/1000 | Loss: 0.00005781
Iteration 134/1000 | Loss: 0.00005781
Iteration 135/1000 | Loss: 0.00005781
Iteration 136/1000 | Loss: 0.00005781
Iteration 137/1000 | Loss: 0.00005781
Iteration 138/1000 | Loss: 0.00005781
Iteration 139/1000 | Loss: 0.00005781
Iteration 140/1000 | Loss: 0.00005781
Iteration 141/1000 | Loss: 0.00005781
Iteration 142/1000 | Loss: 0.00005781
Iteration 143/1000 | Loss: 0.00005781
Iteration 144/1000 | Loss: 0.00005781
Iteration 145/1000 | Loss: 0.00005781
Iteration 146/1000 | Loss: 0.00005781
Iteration 147/1000 | Loss: 0.00005781
Iteration 148/1000 | Loss: 0.00005780
Iteration 149/1000 | Loss: 0.00005780
Iteration 150/1000 | Loss: 0.00005780
Iteration 151/1000 | Loss: 0.00005780
Iteration 152/1000 | Loss: 0.00005780
Iteration 153/1000 | Loss: 0.00005780
Iteration 154/1000 | Loss: 0.00005780
Iteration 155/1000 | Loss: 0.00005780
Iteration 156/1000 | Loss: 0.00005780
Iteration 157/1000 | Loss: 0.00005780
Iteration 158/1000 | Loss: 0.00005780
Iteration 159/1000 | Loss: 0.00005780
Iteration 160/1000 | Loss: 0.00005780
Iteration 161/1000 | Loss: 0.00005780
Iteration 162/1000 | Loss: 0.00005780
Iteration 163/1000 | Loss: 0.00005780
Iteration 164/1000 | Loss: 0.00005780
Iteration 165/1000 | Loss: 0.00005780
Iteration 166/1000 | Loss: 0.00005780
Iteration 167/1000 | Loss: 0.00005780
Iteration 168/1000 | Loss: 0.00005780
Iteration 169/1000 | Loss: 0.00005780
Iteration 170/1000 | Loss: 0.00005780
Iteration 171/1000 | Loss: 0.00005780
Iteration 172/1000 | Loss: 0.00005780
Iteration 173/1000 | Loss: 0.00005780
Iteration 174/1000 | Loss: 0.00005780
Iteration 175/1000 | Loss: 0.00005780
Iteration 176/1000 | Loss: 0.00005780
Iteration 177/1000 | Loss: 0.00005780
Iteration 178/1000 | Loss: 0.00005780
Iteration 179/1000 | Loss: 0.00005780
Iteration 180/1000 | Loss: 0.00005780
Iteration 181/1000 | Loss: 0.00005780
Iteration 182/1000 | Loss: 0.00005780
Iteration 183/1000 | Loss: 0.00005780
Iteration 184/1000 | Loss: 0.00005780
Iteration 185/1000 | Loss: 0.00005780
Iteration 186/1000 | Loss: 0.00005780
Iteration 187/1000 | Loss: 0.00005780
Iteration 188/1000 | Loss: 0.00005780
Iteration 189/1000 | Loss: 0.00005780
Iteration 190/1000 | Loss: 0.00005780
Iteration 191/1000 | Loss: 0.00005780
Iteration 192/1000 | Loss: 0.00005780
Iteration 193/1000 | Loss: 0.00005780
Iteration 194/1000 | Loss: 0.00005780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [5.779669663752429e-05, 5.779669663752429e-05, 5.779669663752429e-05, 5.779669663752429e-05, 5.779669663752429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.779669663752429e-05

Optimization complete. Final v2v error: 6.274233341217041 mm

Highest mean error: 6.899228096008301 mm for frame 20

Lowest mean error: 5.66289758682251 mm for frame 0

Saving results

Total time: 50.091025590896606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128672
Iteration 2/25 | Loss: 0.01128672
Iteration 3/25 | Loss: 0.01128672
Iteration 4/25 | Loss: 0.01128672
Iteration 5/25 | Loss: 0.01128672
Iteration 6/25 | Loss: 0.01128671
Iteration 7/25 | Loss: 0.01128671
Iteration 8/25 | Loss: 0.01128671
Iteration 9/25 | Loss: 0.01128671
Iteration 10/25 | Loss: 0.01128671
Iteration 11/25 | Loss: 0.01128671
Iteration 12/25 | Loss: 0.01128671
Iteration 13/25 | Loss: 0.01128671
Iteration 14/25 | Loss: 0.01128671
Iteration 15/25 | Loss: 0.01128671
Iteration 16/25 | Loss: 0.01128671
Iteration 17/25 | Loss: 0.01128671
Iteration 18/25 | Loss: 0.01128670
Iteration 19/25 | Loss: 0.01128670
Iteration 20/25 | Loss: 0.01128670
Iteration 21/25 | Loss: 0.01128670
Iteration 22/25 | Loss: 0.01128670
Iteration 23/25 | Loss: 0.01128670
Iteration 24/25 | Loss: 0.01128670
Iteration 25/25 | Loss: 0.01128670

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56876493
Iteration 2/25 | Loss: 0.09704771
Iteration 3/25 | Loss: 0.09640363
Iteration 4/25 | Loss: 0.09608469
Iteration 5/25 | Loss: 0.09593114
Iteration 6/25 | Loss: 0.09593111
Iteration 7/25 | Loss: 0.09593109
Iteration 8/25 | Loss: 0.09593109
Iteration 9/25 | Loss: 0.09593108
Iteration 10/25 | Loss: 0.09593108
Iteration 11/25 | Loss: 0.09593108
Iteration 12/25 | Loss: 0.09593108
Iteration 13/25 | Loss: 0.09593108
Iteration 14/25 | Loss: 0.09593108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.09593108296394348, 0.09593108296394348, 0.09593108296394348, 0.09593108296394348, 0.09593108296394348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09593108296394348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09593108
Iteration 2/1000 | Loss: 0.00460100
Iteration 3/1000 | Loss: 0.00421414
Iteration 4/1000 | Loss: 0.00095806
Iteration 5/1000 | Loss: 0.00041768
Iteration 6/1000 | Loss: 0.00073811
Iteration 7/1000 | Loss: 0.00016246
Iteration 8/1000 | Loss: 0.00020084
Iteration 9/1000 | Loss: 0.00010115
Iteration 10/1000 | Loss: 0.00064819
Iteration 11/1000 | Loss: 0.00020234
Iteration 12/1000 | Loss: 0.00007836
Iteration 13/1000 | Loss: 0.00016201
Iteration 14/1000 | Loss: 0.00006687
Iteration 15/1000 | Loss: 0.00045247
Iteration 16/1000 | Loss: 0.00006263
Iteration 17/1000 | Loss: 0.00005672
Iteration 18/1000 | Loss: 0.00005290
Iteration 19/1000 | Loss: 0.00006297
Iteration 20/1000 | Loss: 0.00004821
Iteration 21/1000 | Loss: 0.00034563
Iteration 22/1000 | Loss: 0.00011960
Iteration 23/1000 | Loss: 0.00012928
Iteration 24/1000 | Loss: 0.00004226
Iteration 25/1000 | Loss: 0.00004052
Iteration 26/1000 | Loss: 0.00019693
Iteration 27/1000 | Loss: 0.00004116
Iteration 28/1000 | Loss: 0.00003808
Iteration 29/1000 | Loss: 0.00003716
Iteration 30/1000 | Loss: 0.00031502
Iteration 31/1000 | Loss: 0.00003712
Iteration 32/1000 | Loss: 0.00003572
Iteration 33/1000 | Loss: 0.00003530
Iteration 34/1000 | Loss: 0.00003503
Iteration 35/1000 | Loss: 0.00005137
Iteration 36/1000 | Loss: 0.00022833
Iteration 37/1000 | Loss: 0.00041051
Iteration 38/1000 | Loss: 0.00036302
Iteration 39/1000 | Loss: 0.00006277
Iteration 40/1000 | Loss: 0.00020107
Iteration 41/1000 | Loss: 0.00004159
Iteration 42/1000 | Loss: 0.00009291
Iteration 43/1000 | Loss: 0.00010362
Iteration 44/1000 | Loss: 0.00006225
Iteration 45/1000 | Loss: 0.00004074
Iteration 46/1000 | Loss: 0.00005569
Iteration 47/1000 | Loss: 0.00006875
Iteration 48/1000 | Loss: 0.00007548
Iteration 49/1000 | Loss: 0.00004051
Iteration 50/1000 | Loss: 0.00004374
Iteration 51/1000 | Loss: 0.00003597
Iteration 52/1000 | Loss: 0.00010675
Iteration 53/1000 | Loss: 0.00018181
Iteration 54/1000 | Loss: 0.00013782
Iteration 55/1000 | Loss: 0.00004974
Iteration 56/1000 | Loss: 0.00003504
Iteration 57/1000 | Loss: 0.00010351
Iteration 58/1000 | Loss: 0.00005169
Iteration 59/1000 | Loss: 0.00009133
Iteration 60/1000 | Loss: 0.00004288
Iteration 61/1000 | Loss: 0.00007767
Iteration 62/1000 | Loss: 0.00004018
Iteration 63/1000 | Loss: 0.00013860
Iteration 64/1000 | Loss: 0.00003957
Iteration 65/1000 | Loss: 0.00008978
Iteration 66/1000 | Loss: 0.00009918
Iteration 67/1000 | Loss: 0.00009253
Iteration 68/1000 | Loss: 0.00004425
Iteration 69/1000 | Loss: 0.00009543
Iteration 70/1000 | Loss: 0.00005683
Iteration 71/1000 | Loss: 0.00012994
Iteration 72/1000 | Loss: 0.00005982
Iteration 73/1000 | Loss: 0.00009176
Iteration 74/1000 | Loss: 0.00005867
Iteration 75/1000 | Loss: 0.00009163
Iteration 76/1000 | Loss: 0.00006775
Iteration 77/1000 | Loss: 0.00005265
Iteration 78/1000 | Loss: 0.00004037
Iteration 79/1000 | Loss: 0.00004138
Iteration 80/1000 | Loss: 0.00003874
Iteration 81/1000 | Loss: 0.00004336
Iteration 82/1000 | Loss: 0.00003916
Iteration 83/1000 | Loss: 0.00005113
Iteration 84/1000 | Loss: 0.00003765
Iteration 85/1000 | Loss: 0.00003531
Iteration 86/1000 | Loss: 0.00003404
Iteration 87/1000 | Loss: 0.00003353
Iteration 88/1000 | Loss: 0.00003320
Iteration 89/1000 | Loss: 0.00003286
Iteration 90/1000 | Loss: 0.00005330
Iteration 91/1000 | Loss: 0.00004065
Iteration 92/1000 | Loss: 0.00003460
Iteration 93/1000 | Loss: 0.00003393
Iteration 94/1000 | Loss: 0.00003555
Iteration 95/1000 | Loss: 0.00003366
Iteration 96/1000 | Loss: 0.00003283
Iteration 97/1000 | Loss: 0.00003946
Iteration 98/1000 | Loss: 0.00003470
Iteration 99/1000 | Loss: 0.00003738
Iteration 100/1000 | Loss: 0.00003357
Iteration 101/1000 | Loss: 0.00005390
Iteration 102/1000 | Loss: 0.00003944
Iteration 103/1000 | Loss: 0.00003608
Iteration 104/1000 | Loss: 0.00003329
Iteration 105/1000 | Loss: 0.00003597
Iteration 106/1000 | Loss: 0.00003431
Iteration 107/1000 | Loss: 0.00005303
Iteration 108/1000 | Loss: 0.00003673
Iteration 109/1000 | Loss: 0.00003464
Iteration 110/1000 | Loss: 0.00003331
Iteration 111/1000 | Loss: 0.00004531
Iteration 112/1000 | Loss: 0.00003643
Iteration 113/1000 | Loss: 0.00004298
Iteration 114/1000 | Loss: 0.00003529
Iteration 115/1000 | Loss: 0.00004640
Iteration 116/1000 | Loss: 0.00003547
Iteration 117/1000 | Loss: 0.00004265
Iteration 118/1000 | Loss: 0.00003583
Iteration 119/1000 | Loss: 0.00004341
Iteration 120/1000 | Loss: 0.00003738
Iteration 121/1000 | Loss: 0.00004652
Iteration 122/1000 | Loss: 0.00003745
Iteration 123/1000 | Loss: 0.00003394
Iteration 124/1000 | Loss: 0.00003267
Iteration 125/1000 | Loss: 0.00003220
Iteration 126/1000 | Loss: 0.00003217
Iteration 127/1000 | Loss: 0.00003213
Iteration 128/1000 | Loss: 0.00003204
Iteration 129/1000 | Loss: 0.00003195
Iteration 130/1000 | Loss: 0.00003194
Iteration 131/1000 | Loss: 0.00003194
Iteration 132/1000 | Loss: 0.00003193
Iteration 133/1000 | Loss: 0.00003192
Iteration 134/1000 | Loss: 0.00003192
Iteration 135/1000 | Loss: 0.00003192
Iteration 136/1000 | Loss: 0.00003191
Iteration 137/1000 | Loss: 0.00003190
Iteration 138/1000 | Loss: 0.00003190
Iteration 139/1000 | Loss: 0.00003190
Iteration 140/1000 | Loss: 0.00003190
Iteration 141/1000 | Loss: 0.00003190
Iteration 142/1000 | Loss: 0.00003190
Iteration 143/1000 | Loss: 0.00003190
Iteration 144/1000 | Loss: 0.00003190
Iteration 145/1000 | Loss: 0.00003190
Iteration 146/1000 | Loss: 0.00003190
Iteration 147/1000 | Loss: 0.00003189
Iteration 148/1000 | Loss: 0.00003189
Iteration 149/1000 | Loss: 0.00003189
Iteration 150/1000 | Loss: 0.00003189
Iteration 151/1000 | Loss: 0.00003189
Iteration 152/1000 | Loss: 0.00003189
Iteration 153/1000 | Loss: 0.00003189
Iteration 154/1000 | Loss: 0.00003189
Iteration 155/1000 | Loss: 0.00003188
Iteration 156/1000 | Loss: 0.00003188
Iteration 157/1000 | Loss: 0.00003188
Iteration 158/1000 | Loss: 0.00003188
Iteration 159/1000 | Loss: 0.00003188
Iteration 160/1000 | Loss: 0.00003188
Iteration 161/1000 | Loss: 0.00003188
Iteration 162/1000 | Loss: 0.00003187
Iteration 163/1000 | Loss: 0.00003187
Iteration 164/1000 | Loss: 0.00003187
Iteration 165/1000 | Loss: 0.00003187
Iteration 166/1000 | Loss: 0.00003187
Iteration 167/1000 | Loss: 0.00003187
Iteration 168/1000 | Loss: 0.00003187
Iteration 169/1000 | Loss: 0.00003187
Iteration 170/1000 | Loss: 0.00003187
Iteration 171/1000 | Loss: 0.00003187
Iteration 172/1000 | Loss: 0.00003187
Iteration 173/1000 | Loss: 0.00003187
Iteration 174/1000 | Loss: 0.00003187
Iteration 175/1000 | Loss: 0.00003186
Iteration 176/1000 | Loss: 0.00003186
Iteration 177/1000 | Loss: 0.00003186
Iteration 178/1000 | Loss: 0.00003186
Iteration 179/1000 | Loss: 0.00003185
Iteration 180/1000 | Loss: 0.00003185
Iteration 181/1000 | Loss: 0.00003185
Iteration 182/1000 | Loss: 0.00003185
Iteration 183/1000 | Loss: 0.00003184
Iteration 184/1000 | Loss: 0.00003184
Iteration 185/1000 | Loss: 0.00003184
Iteration 186/1000 | Loss: 0.00003183
Iteration 187/1000 | Loss: 0.00003183
Iteration 188/1000 | Loss: 0.00003183
Iteration 189/1000 | Loss: 0.00003434
Iteration 190/1000 | Loss: 0.00003433
Iteration 191/1000 | Loss: 0.00003433
Iteration 192/1000 | Loss: 0.00003433
Iteration 193/1000 | Loss: 0.00003377
Iteration 194/1000 | Loss: 0.00003290
Iteration 195/1000 | Loss: 0.00003198
Iteration 196/1000 | Loss: 0.00003182
Iteration 197/1000 | Loss: 0.00003171
Iteration 198/1000 | Loss: 0.00003170
Iteration 199/1000 | Loss: 0.00003169
Iteration 200/1000 | Loss: 0.00003169
Iteration 201/1000 | Loss: 0.00003168
Iteration 202/1000 | Loss: 0.00003168
Iteration 203/1000 | Loss: 0.00003167
Iteration 204/1000 | Loss: 0.00003167
Iteration 205/1000 | Loss: 0.00003167
Iteration 206/1000 | Loss: 0.00003167
Iteration 207/1000 | Loss: 0.00003166
Iteration 208/1000 | Loss: 0.00003166
Iteration 209/1000 | Loss: 0.00003166
Iteration 210/1000 | Loss: 0.00003166
Iteration 211/1000 | Loss: 0.00003166
Iteration 212/1000 | Loss: 0.00003166
Iteration 213/1000 | Loss: 0.00003166
Iteration 214/1000 | Loss: 0.00003166
Iteration 215/1000 | Loss: 0.00003166
Iteration 216/1000 | Loss: 0.00003166
Iteration 217/1000 | Loss: 0.00003166
Iteration 218/1000 | Loss: 0.00003166
Iteration 219/1000 | Loss: 0.00003165
Iteration 220/1000 | Loss: 0.00003165
Iteration 221/1000 | Loss: 0.00003165
Iteration 222/1000 | Loss: 0.00003165
Iteration 223/1000 | Loss: 0.00003165
Iteration 224/1000 | Loss: 0.00003165
Iteration 225/1000 | Loss: 0.00003165
Iteration 226/1000 | Loss: 0.00003165
Iteration 227/1000 | Loss: 0.00003165
Iteration 228/1000 | Loss: 0.00003165
Iteration 229/1000 | Loss: 0.00003165
Iteration 230/1000 | Loss: 0.00003165
Iteration 231/1000 | Loss: 0.00003165
Iteration 232/1000 | Loss: 0.00003164
Iteration 233/1000 | Loss: 0.00003164
Iteration 234/1000 | Loss: 0.00003164
Iteration 235/1000 | Loss: 0.00003164
Iteration 236/1000 | Loss: 0.00003164
Iteration 237/1000 | Loss: 0.00003164
Iteration 238/1000 | Loss: 0.00003164
Iteration 239/1000 | Loss: 0.00003164
Iteration 240/1000 | Loss: 0.00003164
Iteration 241/1000 | Loss: 0.00003164
Iteration 242/1000 | Loss: 0.00003164
Iteration 243/1000 | Loss: 0.00003164
Iteration 244/1000 | Loss: 0.00003164
Iteration 245/1000 | Loss: 0.00003164
Iteration 246/1000 | Loss: 0.00003164
Iteration 247/1000 | Loss: 0.00003164
Iteration 248/1000 | Loss: 0.00003164
Iteration 249/1000 | Loss: 0.00003164
Iteration 250/1000 | Loss: 0.00003163
Iteration 251/1000 | Loss: 0.00003163
Iteration 252/1000 | Loss: 0.00003163
Iteration 253/1000 | Loss: 0.00003163
Iteration 254/1000 | Loss: 0.00003163
Iteration 255/1000 | Loss: 0.00003163
Iteration 256/1000 | Loss: 0.00003163
Iteration 257/1000 | Loss: 0.00003163
Iteration 258/1000 | Loss: 0.00003163
Iteration 259/1000 | Loss: 0.00003163
Iteration 260/1000 | Loss: 0.00003163
Iteration 261/1000 | Loss: 0.00003163
Iteration 262/1000 | Loss: 0.00003163
Iteration 263/1000 | Loss: 0.00003163
Iteration 264/1000 | Loss: 0.00003163
Iteration 265/1000 | Loss: 0.00003163
Iteration 266/1000 | Loss: 0.00003163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [3.163485598633997e-05, 3.163485598633997e-05, 3.163485598633997e-05, 3.163485598633997e-05, 3.163485598633997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.163485598633997e-05

Optimization complete. Final v2v error: 4.389740943908691 mm

Highest mean error: 11.858306884765625 mm for frame 195

Lowest mean error: 3.5801801681518555 mm for frame 220

Saving results

Total time: 229.87680912017822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755136
Iteration 2/25 | Loss: 0.00243444
Iteration 3/25 | Loss: 0.00202103
Iteration 4/25 | Loss: 0.00197850
Iteration 5/25 | Loss: 0.00198236
Iteration 6/25 | Loss: 0.00195481
Iteration 7/25 | Loss: 0.00194688
Iteration 8/25 | Loss: 0.00194863
Iteration 9/25 | Loss: 0.00194346
Iteration 10/25 | Loss: 0.00194147
Iteration 11/25 | Loss: 0.00194227
Iteration 12/25 | Loss: 0.00194558
Iteration 13/25 | Loss: 0.00194182
Iteration 14/25 | Loss: 0.00193977
Iteration 15/25 | Loss: 0.00193881
Iteration 16/25 | Loss: 0.00193769
Iteration 17/25 | Loss: 0.00193715
Iteration 18/25 | Loss: 0.00193700
Iteration 19/25 | Loss: 0.00193689
Iteration 20/25 | Loss: 0.00193689
Iteration 21/25 | Loss: 0.00193688
Iteration 22/25 | Loss: 0.00193688
Iteration 23/25 | Loss: 0.00193688
Iteration 24/25 | Loss: 0.00193686
Iteration 25/25 | Loss: 0.00193686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51424098
Iteration 2/25 | Loss: 0.00183309
Iteration 3/25 | Loss: 0.00183309
Iteration 4/25 | Loss: 0.00183309
Iteration 5/25 | Loss: 0.00183309
Iteration 6/25 | Loss: 0.00183309
Iteration 7/25 | Loss: 0.00183309
Iteration 8/25 | Loss: 0.00183309
Iteration 9/25 | Loss: 0.00183309
Iteration 10/25 | Loss: 0.00183309
Iteration 11/25 | Loss: 0.00183309
Iteration 12/25 | Loss: 0.00183309
Iteration 13/25 | Loss: 0.00183309
Iteration 14/25 | Loss: 0.00183309
Iteration 15/25 | Loss: 0.00183309
Iteration 16/25 | Loss: 0.00183309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018330913735553622, 0.0018330913735553622, 0.0018330913735553622, 0.0018330913735553622, 0.0018330913735553622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018330913735553622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183309
Iteration 2/1000 | Loss: 0.00011022
Iteration 3/1000 | Loss: 0.00007286
Iteration 4/1000 | Loss: 0.00006308
Iteration 5/1000 | Loss: 0.00006013
Iteration 6/1000 | Loss: 0.00005814
Iteration 7/1000 | Loss: 0.00005699
Iteration 8/1000 | Loss: 0.00005566
Iteration 9/1000 | Loss: 0.00005484
Iteration 10/1000 | Loss: 0.00005409
Iteration 11/1000 | Loss: 0.00005363
Iteration 12/1000 | Loss: 0.00005312
Iteration 13/1000 | Loss: 0.00005263
Iteration 14/1000 | Loss: 0.00005214
Iteration 15/1000 | Loss: 0.00005177
Iteration 16/1000 | Loss: 0.00005152
Iteration 17/1000 | Loss: 0.00005129
Iteration 18/1000 | Loss: 0.00005105
Iteration 19/1000 | Loss: 0.00005089
Iteration 20/1000 | Loss: 0.00005073
Iteration 21/1000 | Loss: 0.00005065
Iteration 22/1000 | Loss: 0.00005058
Iteration 23/1000 | Loss: 0.00005051
Iteration 24/1000 | Loss: 0.00005048
Iteration 25/1000 | Loss: 0.00005044
Iteration 26/1000 | Loss: 0.00005041
Iteration 27/1000 | Loss: 0.00005037
Iteration 28/1000 | Loss: 0.00005034
Iteration 29/1000 | Loss: 0.00005033
Iteration 30/1000 | Loss: 0.00005029
Iteration 31/1000 | Loss: 0.00005029
Iteration 32/1000 | Loss: 0.00005027
Iteration 33/1000 | Loss: 0.00005026
Iteration 34/1000 | Loss: 0.00005026
Iteration 35/1000 | Loss: 0.00005022
Iteration 36/1000 | Loss: 0.00005022
Iteration 37/1000 | Loss: 0.00005019
Iteration 38/1000 | Loss: 0.00005019
Iteration 39/1000 | Loss: 0.00005018
Iteration 40/1000 | Loss: 0.00005017
Iteration 41/1000 | Loss: 0.00005015
Iteration 42/1000 | Loss: 0.00005014
Iteration 43/1000 | Loss: 0.00005014
Iteration 44/1000 | Loss: 0.00005012
Iteration 45/1000 | Loss: 0.00005012
Iteration 46/1000 | Loss: 0.00005011
Iteration 47/1000 | Loss: 0.00005011
Iteration 48/1000 | Loss: 0.00005011
Iteration 49/1000 | Loss: 0.00005011
Iteration 50/1000 | Loss: 0.00005010
Iteration 51/1000 | Loss: 0.00005010
Iteration 52/1000 | Loss: 0.00005010
Iteration 53/1000 | Loss: 0.00005010
Iteration 54/1000 | Loss: 0.00005007
Iteration 55/1000 | Loss: 0.00005007
Iteration 56/1000 | Loss: 0.00005006
Iteration 57/1000 | Loss: 0.00005005
Iteration 58/1000 | Loss: 0.00005005
Iteration 59/1000 | Loss: 0.00005003
Iteration 60/1000 | Loss: 0.00005002
Iteration 61/1000 | Loss: 0.00005002
Iteration 62/1000 | Loss: 0.00005002
Iteration 63/1000 | Loss: 0.00005002
Iteration 64/1000 | Loss: 0.00005002
Iteration 65/1000 | Loss: 0.00005002
Iteration 66/1000 | Loss: 0.00005002
Iteration 67/1000 | Loss: 0.00005002
Iteration 68/1000 | Loss: 0.00005001
Iteration 69/1000 | Loss: 0.00005000
Iteration 70/1000 | Loss: 0.00005000
Iteration 71/1000 | Loss: 0.00004999
Iteration 72/1000 | Loss: 0.00004999
Iteration 73/1000 | Loss: 0.00004999
Iteration 74/1000 | Loss: 0.00004999
Iteration 75/1000 | Loss: 0.00004999
Iteration 76/1000 | Loss: 0.00004999
Iteration 77/1000 | Loss: 0.00004999
Iteration 78/1000 | Loss: 0.00004999
Iteration 79/1000 | Loss: 0.00004999
Iteration 80/1000 | Loss: 0.00004999
Iteration 81/1000 | Loss: 0.00004998
Iteration 82/1000 | Loss: 0.00004998
Iteration 83/1000 | Loss: 0.00004998
Iteration 84/1000 | Loss: 0.00004998
Iteration 85/1000 | Loss: 0.00004997
Iteration 86/1000 | Loss: 0.00004997
Iteration 87/1000 | Loss: 0.00004997
Iteration 88/1000 | Loss: 0.00004997
Iteration 89/1000 | Loss: 0.00004997
Iteration 90/1000 | Loss: 0.00004996
Iteration 91/1000 | Loss: 0.00004996
Iteration 92/1000 | Loss: 0.00004996
Iteration 93/1000 | Loss: 0.00004996
Iteration 94/1000 | Loss: 0.00004996
Iteration 95/1000 | Loss: 0.00004995
Iteration 96/1000 | Loss: 0.00004995
Iteration 97/1000 | Loss: 0.00004995
Iteration 98/1000 | Loss: 0.00004995
Iteration 99/1000 | Loss: 0.00004994
Iteration 100/1000 | Loss: 0.00004994
Iteration 101/1000 | Loss: 0.00004994
Iteration 102/1000 | Loss: 0.00004994
Iteration 103/1000 | Loss: 0.00004994
Iteration 104/1000 | Loss: 0.00004994
Iteration 105/1000 | Loss: 0.00004994
Iteration 106/1000 | Loss: 0.00004994
Iteration 107/1000 | Loss: 0.00004994
Iteration 108/1000 | Loss: 0.00004994
Iteration 109/1000 | Loss: 0.00004993
Iteration 110/1000 | Loss: 0.00004993
Iteration 111/1000 | Loss: 0.00004993
Iteration 112/1000 | Loss: 0.00004993
Iteration 113/1000 | Loss: 0.00004993
Iteration 114/1000 | Loss: 0.00004993
Iteration 115/1000 | Loss: 0.00004993
Iteration 116/1000 | Loss: 0.00004993
Iteration 117/1000 | Loss: 0.00004993
Iteration 118/1000 | Loss: 0.00004993
Iteration 119/1000 | Loss: 0.00004992
Iteration 120/1000 | Loss: 0.00004992
Iteration 121/1000 | Loss: 0.00004992
Iteration 122/1000 | Loss: 0.00004992
Iteration 123/1000 | Loss: 0.00004991
Iteration 124/1000 | Loss: 0.00004991
Iteration 125/1000 | Loss: 0.00004991
Iteration 126/1000 | Loss: 0.00004991
Iteration 127/1000 | Loss: 0.00004991
Iteration 128/1000 | Loss: 0.00004990
Iteration 129/1000 | Loss: 0.00004990
Iteration 130/1000 | Loss: 0.00004990
Iteration 131/1000 | Loss: 0.00004990
Iteration 132/1000 | Loss: 0.00004990
Iteration 133/1000 | Loss: 0.00004990
Iteration 134/1000 | Loss: 0.00004990
Iteration 135/1000 | Loss: 0.00004990
Iteration 136/1000 | Loss: 0.00004989
Iteration 137/1000 | Loss: 0.00004989
Iteration 138/1000 | Loss: 0.00004989
Iteration 139/1000 | Loss: 0.00004989
Iteration 140/1000 | Loss: 0.00004989
Iteration 141/1000 | Loss: 0.00004989
Iteration 142/1000 | Loss: 0.00004989
Iteration 143/1000 | Loss: 0.00004989
Iteration 144/1000 | Loss: 0.00004989
Iteration 145/1000 | Loss: 0.00004989
Iteration 146/1000 | Loss: 0.00004989
Iteration 147/1000 | Loss: 0.00004989
Iteration 148/1000 | Loss: 0.00004989
Iteration 149/1000 | Loss: 0.00004989
Iteration 150/1000 | Loss: 0.00004989
Iteration 151/1000 | Loss: 0.00004989
Iteration 152/1000 | Loss: 0.00004989
Iteration 153/1000 | Loss: 0.00004989
Iteration 154/1000 | Loss: 0.00004989
Iteration 155/1000 | Loss: 0.00004989
Iteration 156/1000 | Loss: 0.00004989
Iteration 157/1000 | Loss: 0.00004989
Iteration 158/1000 | Loss: 0.00004989
Iteration 159/1000 | Loss: 0.00004989
Iteration 160/1000 | Loss: 0.00004989
Iteration 161/1000 | Loss: 0.00004988
Iteration 162/1000 | Loss: 0.00004988
Iteration 163/1000 | Loss: 0.00004988
Iteration 164/1000 | Loss: 0.00004988
Iteration 165/1000 | Loss: 0.00004988
Iteration 166/1000 | Loss: 0.00004988
Iteration 167/1000 | Loss: 0.00004988
Iteration 168/1000 | Loss: 0.00004988
Iteration 169/1000 | Loss: 0.00004988
Iteration 170/1000 | Loss: 0.00004988
Iteration 171/1000 | Loss: 0.00004988
Iteration 172/1000 | Loss: 0.00004988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [4.988496220903471e-05, 4.988496220903471e-05, 4.988496220903471e-05, 4.988496220903471e-05, 4.988496220903471e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.988496220903471e-05

Optimization complete. Final v2v error: 5.840729236602783 mm

Highest mean error: 8.003721237182617 mm for frame 210

Lowest mean error: 5.01388692855835 mm for frame 230

Saving results

Total time: 89.24533581733704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582070
Iteration 2/25 | Loss: 0.00204666
Iteration 3/25 | Loss: 0.00196488
Iteration 4/25 | Loss: 0.00195123
Iteration 5/25 | Loss: 0.00194395
Iteration 6/25 | Loss: 0.00194139
Iteration 7/25 | Loss: 0.00194050
Iteration 8/25 | Loss: 0.00194050
Iteration 9/25 | Loss: 0.00194050
Iteration 10/25 | Loss: 0.00194050
Iteration 11/25 | Loss: 0.00194050
Iteration 12/25 | Loss: 0.00194050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0019405026687309146, 0.0019405026687309146, 0.0019405026687309146, 0.0019405026687309146, 0.0019405026687309146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019405026687309146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.08753157
Iteration 2/25 | Loss: 0.00147985
Iteration 3/25 | Loss: 0.00147985
Iteration 4/25 | Loss: 0.00147985
Iteration 5/25 | Loss: 0.00147985
Iteration 6/25 | Loss: 0.00147985
Iteration 7/25 | Loss: 0.00147985
Iteration 8/25 | Loss: 0.00147985
Iteration 9/25 | Loss: 0.00147985
Iteration 10/25 | Loss: 0.00147985
Iteration 11/25 | Loss: 0.00147985
Iteration 12/25 | Loss: 0.00147985
Iteration 13/25 | Loss: 0.00147985
Iteration 14/25 | Loss: 0.00147985
Iteration 15/25 | Loss: 0.00147985
Iteration 16/25 | Loss: 0.00147985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014798484044149518, 0.0014798484044149518, 0.0014798484044149518, 0.0014798484044149518, 0.0014798484044149518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014798484044149518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147985
Iteration 2/1000 | Loss: 0.00012494
Iteration 3/1000 | Loss: 0.00007472
Iteration 4/1000 | Loss: 0.00005966
Iteration 5/1000 | Loss: 0.00005336
Iteration 6/1000 | Loss: 0.00005106
Iteration 7/1000 | Loss: 0.00004968
Iteration 8/1000 | Loss: 0.00004807
Iteration 9/1000 | Loss: 0.00004704
Iteration 10/1000 | Loss: 0.00004633
Iteration 11/1000 | Loss: 0.00004570
Iteration 12/1000 | Loss: 0.00004509
Iteration 13/1000 | Loss: 0.00004463
Iteration 14/1000 | Loss: 0.00004418
Iteration 15/1000 | Loss: 0.00004396
Iteration 16/1000 | Loss: 0.00004393
Iteration 17/1000 | Loss: 0.00004380
Iteration 18/1000 | Loss: 0.00004379
Iteration 19/1000 | Loss: 0.00004364
Iteration 20/1000 | Loss: 0.00004364
Iteration 21/1000 | Loss: 0.00004363
Iteration 22/1000 | Loss: 0.00004358
Iteration 23/1000 | Loss: 0.00004357
Iteration 24/1000 | Loss: 0.00004356
Iteration 25/1000 | Loss: 0.00004355
Iteration 26/1000 | Loss: 0.00004354
Iteration 27/1000 | Loss: 0.00004350
Iteration 28/1000 | Loss: 0.00004350
Iteration 29/1000 | Loss: 0.00004349
Iteration 30/1000 | Loss: 0.00004348
Iteration 31/1000 | Loss: 0.00004348
Iteration 32/1000 | Loss: 0.00004347
Iteration 33/1000 | Loss: 0.00004346
Iteration 34/1000 | Loss: 0.00004346
Iteration 35/1000 | Loss: 0.00004346
Iteration 36/1000 | Loss: 0.00004346
Iteration 37/1000 | Loss: 0.00004345
Iteration 38/1000 | Loss: 0.00004345
Iteration 39/1000 | Loss: 0.00004344
Iteration 40/1000 | Loss: 0.00004344
Iteration 41/1000 | Loss: 0.00004344
Iteration 42/1000 | Loss: 0.00004343
Iteration 43/1000 | Loss: 0.00004343
Iteration 44/1000 | Loss: 0.00004343
Iteration 45/1000 | Loss: 0.00004343
Iteration 46/1000 | Loss: 0.00004342
Iteration 47/1000 | Loss: 0.00004341
Iteration 48/1000 | Loss: 0.00004341
Iteration 49/1000 | Loss: 0.00004341
Iteration 50/1000 | Loss: 0.00004341
Iteration 51/1000 | Loss: 0.00004341
Iteration 52/1000 | Loss: 0.00004341
Iteration 53/1000 | Loss: 0.00004340
Iteration 54/1000 | Loss: 0.00004340
Iteration 55/1000 | Loss: 0.00004339
Iteration 56/1000 | Loss: 0.00004339
Iteration 57/1000 | Loss: 0.00004339
Iteration 58/1000 | Loss: 0.00004338
Iteration 59/1000 | Loss: 0.00004338
Iteration 60/1000 | Loss: 0.00004338
Iteration 61/1000 | Loss: 0.00004337
Iteration 62/1000 | Loss: 0.00004337
Iteration 63/1000 | Loss: 0.00004337
Iteration 64/1000 | Loss: 0.00004337
Iteration 65/1000 | Loss: 0.00004337
Iteration 66/1000 | Loss: 0.00004336
Iteration 67/1000 | Loss: 0.00004336
Iteration 68/1000 | Loss: 0.00004336
Iteration 69/1000 | Loss: 0.00004336
Iteration 70/1000 | Loss: 0.00004336
Iteration 71/1000 | Loss: 0.00004335
Iteration 72/1000 | Loss: 0.00004335
Iteration 73/1000 | Loss: 0.00004335
Iteration 74/1000 | Loss: 0.00004335
Iteration 75/1000 | Loss: 0.00004334
Iteration 76/1000 | Loss: 0.00004334
Iteration 77/1000 | Loss: 0.00004334
Iteration 78/1000 | Loss: 0.00004334
Iteration 79/1000 | Loss: 0.00004334
Iteration 80/1000 | Loss: 0.00004334
Iteration 81/1000 | Loss: 0.00004334
Iteration 82/1000 | Loss: 0.00004334
Iteration 83/1000 | Loss: 0.00004334
Iteration 84/1000 | Loss: 0.00004334
Iteration 85/1000 | Loss: 0.00004334
Iteration 86/1000 | Loss: 0.00004334
Iteration 87/1000 | Loss: 0.00004334
Iteration 88/1000 | Loss: 0.00004334
Iteration 89/1000 | Loss: 0.00004334
Iteration 90/1000 | Loss: 0.00004334
Iteration 91/1000 | Loss: 0.00004334
Iteration 92/1000 | Loss: 0.00004334
Iteration 93/1000 | Loss: 0.00004334
Iteration 94/1000 | Loss: 0.00004334
Iteration 95/1000 | Loss: 0.00004334
Iteration 96/1000 | Loss: 0.00004334
Iteration 97/1000 | Loss: 0.00004334
Iteration 98/1000 | Loss: 0.00004334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [4.3341457057977095e-05, 4.3341457057977095e-05, 4.3341457057977095e-05, 4.3341457057977095e-05, 4.3341457057977095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.3341457057977095e-05

Optimization complete. Final v2v error: 5.6526594161987305 mm

Highest mean error: 6.738927364349365 mm for frame 66

Lowest mean error: 4.98759651184082 mm for frame 8

Saving results

Total time: 39.992002725601196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907227
Iteration 2/25 | Loss: 0.00204575
Iteration 3/25 | Loss: 0.00193150
Iteration 4/25 | Loss: 0.00191206
Iteration 5/25 | Loss: 0.00190738
Iteration 6/25 | Loss: 0.00190631
Iteration 7/25 | Loss: 0.00190631
Iteration 8/25 | Loss: 0.00190631
Iteration 9/25 | Loss: 0.00190631
Iteration 10/25 | Loss: 0.00190631
Iteration 11/25 | Loss: 0.00190631
Iteration 12/25 | Loss: 0.00190631
Iteration 13/25 | Loss: 0.00190631
Iteration 14/25 | Loss: 0.00190631
Iteration 15/25 | Loss: 0.00190631
Iteration 16/25 | Loss: 0.00190631
Iteration 17/25 | Loss: 0.00190631
Iteration 18/25 | Loss: 0.00190631
Iteration 19/25 | Loss: 0.00190631
Iteration 20/25 | Loss: 0.00190631
Iteration 21/25 | Loss: 0.00190631
Iteration 22/25 | Loss: 0.00190631
Iteration 23/25 | Loss: 0.00190631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019063052022829652, 0.0019063052022829652, 0.0019063052022829652, 0.0019063052022829652, 0.0019063052022829652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019063052022829652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38430488
Iteration 2/25 | Loss: 0.00154197
Iteration 3/25 | Loss: 0.00154193
Iteration 4/25 | Loss: 0.00154193
Iteration 5/25 | Loss: 0.00154192
Iteration 6/25 | Loss: 0.00154192
Iteration 7/25 | Loss: 0.00154192
Iteration 8/25 | Loss: 0.00154192
Iteration 9/25 | Loss: 0.00154192
Iteration 10/25 | Loss: 0.00154192
Iteration 11/25 | Loss: 0.00154192
Iteration 12/25 | Loss: 0.00154192
Iteration 13/25 | Loss: 0.00154192
Iteration 14/25 | Loss: 0.00154192
Iteration 15/25 | Loss: 0.00154192
Iteration 16/25 | Loss: 0.00154192
Iteration 17/25 | Loss: 0.00154192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015419231494888663, 0.0015419231494888663, 0.0015419231494888663, 0.0015419231494888663, 0.0015419231494888663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015419231494888663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154192
Iteration 2/1000 | Loss: 0.00008040
Iteration 3/1000 | Loss: 0.00004976
Iteration 4/1000 | Loss: 0.00004148
Iteration 5/1000 | Loss: 0.00003933
Iteration 6/1000 | Loss: 0.00003773
Iteration 7/1000 | Loss: 0.00003663
Iteration 8/1000 | Loss: 0.00003575
Iteration 9/1000 | Loss: 0.00003509
Iteration 10/1000 | Loss: 0.00003442
Iteration 11/1000 | Loss: 0.00003356
Iteration 12/1000 | Loss: 0.00003315
Iteration 13/1000 | Loss: 0.00003298
Iteration 14/1000 | Loss: 0.00003292
Iteration 15/1000 | Loss: 0.00003280
Iteration 16/1000 | Loss: 0.00003272
Iteration 17/1000 | Loss: 0.00003268
Iteration 18/1000 | Loss: 0.00003267
Iteration 19/1000 | Loss: 0.00003267
Iteration 20/1000 | Loss: 0.00003267
Iteration 21/1000 | Loss: 0.00003264
Iteration 22/1000 | Loss: 0.00003261
Iteration 23/1000 | Loss: 0.00003254
Iteration 24/1000 | Loss: 0.00003253
Iteration 25/1000 | Loss: 0.00003252
Iteration 26/1000 | Loss: 0.00003252
Iteration 27/1000 | Loss: 0.00003251
Iteration 28/1000 | Loss: 0.00003251
Iteration 29/1000 | Loss: 0.00003250
Iteration 30/1000 | Loss: 0.00003250
Iteration 31/1000 | Loss: 0.00003250
Iteration 32/1000 | Loss: 0.00003249
Iteration 33/1000 | Loss: 0.00003249
Iteration 34/1000 | Loss: 0.00003248
Iteration 35/1000 | Loss: 0.00003248
Iteration 36/1000 | Loss: 0.00003247
Iteration 37/1000 | Loss: 0.00003247
Iteration 38/1000 | Loss: 0.00003247
Iteration 39/1000 | Loss: 0.00003246
Iteration 40/1000 | Loss: 0.00003246
Iteration 41/1000 | Loss: 0.00003246
Iteration 42/1000 | Loss: 0.00003246
Iteration 43/1000 | Loss: 0.00003245
Iteration 44/1000 | Loss: 0.00003245
Iteration 45/1000 | Loss: 0.00003245
Iteration 46/1000 | Loss: 0.00003245
Iteration 47/1000 | Loss: 0.00003245
Iteration 48/1000 | Loss: 0.00003244
Iteration 49/1000 | Loss: 0.00003244
Iteration 50/1000 | Loss: 0.00003244
Iteration 51/1000 | Loss: 0.00003243
Iteration 52/1000 | Loss: 0.00003243
Iteration 53/1000 | Loss: 0.00003243
Iteration 54/1000 | Loss: 0.00003243
Iteration 55/1000 | Loss: 0.00003243
Iteration 56/1000 | Loss: 0.00003243
Iteration 57/1000 | Loss: 0.00003243
Iteration 58/1000 | Loss: 0.00003243
Iteration 59/1000 | Loss: 0.00003242
Iteration 60/1000 | Loss: 0.00003242
Iteration 61/1000 | Loss: 0.00003242
Iteration 62/1000 | Loss: 0.00003242
Iteration 63/1000 | Loss: 0.00003242
Iteration 64/1000 | Loss: 0.00003242
Iteration 65/1000 | Loss: 0.00003241
Iteration 66/1000 | Loss: 0.00003241
Iteration 67/1000 | Loss: 0.00003241
Iteration 68/1000 | Loss: 0.00003241
Iteration 69/1000 | Loss: 0.00003241
Iteration 70/1000 | Loss: 0.00003241
Iteration 71/1000 | Loss: 0.00003241
Iteration 72/1000 | Loss: 0.00003241
Iteration 73/1000 | Loss: 0.00003241
Iteration 74/1000 | Loss: 0.00003241
Iteration 75/1000 | Loss: 0.00003241
Iteration 76/1000 | Loss: 0.00003241
Iteration 77/1000 | Loss: 0.00003240
Iteration 78/1000 | Loss: 0.00003240
Iteration 79/1000 | Loss: 0.00003240
Iteration 80/1000 | Loss: 0.00003240
Iteration 81/1000 | Loss: 0.00003239
Iteration 82/1000 | Loss: 0.00003239
Iteration 83/1000 | Loss: 0.00003239
Iteration 84/1000 | Loss: 0.00003239
Iteration 85/1000 | Loss: 0.00003239
Iteration 86/1000 | Loss: 0.00003239
Iteration 87/1000 | Loss: 0.00003239
Iteration 88/1000 | Loss: 0.00003239
Iteration 89/1000 | Loss: 0.00003239
Iteration 90/1000 | Loss: 0.00003239
Iteration 91/1000 | Loss: 0.00003239
Iteration 92/1000 | Loss: 0.00003239
Iteration 93/1000 | Loss: 0.00003239
Iteration 94/1000 | Loss: 0.00003239
Iteration 95/1000 | Loss: 0.00003239
Iteration 96/1000 | Loss: 0.00003239
Iteration 97/1000 | Loss: 0.00003239
Iteration 98/1000 | Loss: 0.00003239
Iteration 99/1000 | Loss: 0.00003239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [3.2390533306170255e-05, 3.2390533306170255e-05, 3.2390533306170255e-05, 3.2390533306170255e-05, 3.2390533306170255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2390533306170255e-05

Optimization complete. Final v2v error: 4.869658470153809 mm

Highest mean error: 5.089023590087891 mm for frame 186

Lowest mean error: 4.603410720825195 mm for frame 130

Saving results

Total time: 41.29690194129944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01160885
Iteration 2/25 | Loss: 0.00355520
Iteration 3/25 | Loss: 0.00256841
Iteration 4/25 | Loss: 0.00266407
Iteration 5/25 | Loss: 0.00249279
Iteration 6/25 | Loss: 0.00213641
Iteration 7/25 | Loss: 0.00200011
Iteration 8/25 | Loss: 0.00198119
Iteration 9/25 | Loss: 0.00196113
Iteration 10/25 | Loss: 0.00195005
Iteration 11/25 | Loss: 0.00193731
Iteration 12/25 | Loss: 0.00193601
Iteration 13/25 | Loss: 0.00193743
Iteration 14/25 | Loss: 0.00193549
Iteration 15/25 | Loss: 0.00193389
Iteration 16/25 | Loss: 0.00193415
Iteration 17/25 | Loss: 0.00193708
Iteration 18/25 | Loss: 0.00193415
Iteration 19/25 | Loss: 0.00193300
Iteration 20/25 | Loss: 0.00193264
Iteration 21/25 | Loss: 0.00193054
Iteration 22/25 | Loss: 0.00193269
Iteration 23/25 | Loss: 0.00193240
Iteration 24/25 | Loss: 0.00193203
Iteration 25/25 | Loss: 0.00193247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38055909
Iteration 2/25 | Loss: 0.00126279
Iteration 3/25 | Loss: 0.00126279
Iteration 4/25 | Loss: 0.00126279
Iteration 5/25 | Loss: 0.00126279
Iteration 6/25 | Loss: 0.00126279
Iteration 7/25 | Loss: 0.00126279
Iteration 8/25 | Loss: 0.00126279
Iteration 9/25 | Loss: 0.00126279
Iteration 10/25 | Loss: 0.00126279
Iteration 11/25 | Loss: 0.00126279
Iteration 12/25 | Loss: 0.00126279
Iteration 13/25 | Loss: 0.00126279
Iteration 14/25 | Loss: 0.00126279
Iteration 15/25 | Loss: 0.00126279
Iteration 16/25 | Loss: 0.00126279
Iteration 17/25 | Loss: 0.00126279
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012627874966710806, 0.0012627874966710806, 0.0012627874966710806, 0.0012627874966710806, 0.0012627874966710806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012627874966710806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126279
Iteration 2/1000 | Loss: 0.00011972
Iteration 3/1000 | Loss: 0.00009096
Iteration 4/1000 | Loss: 0.00007207
Iteration 5/1000 | Loss: 0.00008980
Iteration 6/1000 | Loss: 0.00008016
Iteration 7/1000 | Loss: 0.00006485
Iteration 8/1000 | Loss: 0.00006599
Iteration 9/1000 | Loss: 0.00008977
Iteration 10/1000 | Loss: 0.00008552
Iteration 11/1000 | Loss: 0.00007772
Iteration 12/1000 | Loss: 0.00008714
Iteration 13/1000 | Loss: 0.00008955
Iteration 14/1000 | Loss: 0.00009354
Iteration 15/1000 | Loss: 0.00009693
Iteration 16/1000 | Loss: 0.00007654
Iteration 17/1000 | Loss: 0.00007435
Iteration 18/1000 | Loss: 0.00008197
Iteration 19/1000 | Loss: 0.00006693
Iteration 20/1000 | Loss: 0.00006137
Iteration 21/1000 | Loss: 0.00006098
Iteration 22/1000 | Loss: 0.00006167
Iteration 23/1000 | Loss: 0.00007076
Iteration 24/1000 | Loss: 0.00005253
Iteration 25/1000 | Loss: 0.00005581
Iteration 26/1000 | Loss: 0.00005448
Iteration 27/1000 | Loss: 0.00005757
Iteration 28/1000 | Loss: 0.00006192
Iteration 29/1000 | Loss: 0.00006359
Iteration 30/1000 | Loss: 0.00006160
Iteration 31/1000 | Loss: 0.00006335
Iteration 32/1000 | Loss: 0.00006355
Iteration 33/1000 | Loss: 0.00006939
Iteration 34/1000 | Loss: 0.00006885
Iteration 35/1000 | Loss: 0.00005591
Iteration 36/1000 | Loss: 0.00005091
Iteration 37/1000 | Loss: 0.00004629
Iteration 38/1000 | Loss: 0.00004378
Iteration 39/1000 | Loss: 0.00004195
Iteration 40/1000 | Loss: 0.00004104
Iteration 41/1000 | Loss: 0.00004029
Iteration 42/1000 | Loss: 0.00003977
Iteration 43/1000 | Loss: 0.00003905
Iteration 44/1000 | Loss: 0.00003836
Iteration 45/1000 | Loss: 0.00003763
Iteration 46/1000 | Loss: 0.00003726
Iteration 47/1000 | Loss: 0.00003717
Iteration 48/1000 | Loss: 0.00003716
Iteration 49/1000 | Loss: 0.00003716
Iteration 50/1000 | Loss: 0.00003716
Iteration 51/1000 | Loss: 0.00003716
Iteration 52/1000 | Loss: 0.00003715
Iteration 53/1000 | Loss: 0.00003715
Iteration 54/1000 | Loss: 0.00003715
Iteration 55/1000 | Loss: 0.00003715
Iteration 56/1000 | Loss: 0.00003715
Iteration 57/1000 | Loss: 0.00003715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [3.715453567565419e-05, 3.715453567565419e-05, 3.715453567565419e-05, 3.715453567565419e-05, 3.715453567565419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.715453567565419e-05

Optimization complete. Final v2v error: 5.154770374298096 mm

Highest mean error: 6.143043518066406 mm for frame 89

Lowest mean error: 4.944973945617676 mm for frame 122

Saving results

Total time: 125.36033773422241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030183
Iteration 2/25 | Loss: 0.00454411
Iteration 3/25 | Loss: 0.00327377
Iteration 4/25 | Loss: 0.00287016
Iteration 5/25 | Loss: 0.00271524
Iteration 6/25 | Loss: 0.00259824
Iteration 7/25 | Loss: 0.00255713
Iteration 8/25 | Loss: 0.00254115
Iteration 9/25 | Loss: 0.00253449
Iteration 10/25 | Loss: 0.00252624
Iteration 11/25 | Loss: 0.00252270
Iteration 12/25 | Loss: 0.00252123
Iteration 13/25 | Loss: 0.00251956
Iteration 14/25 | Loss: 0.00251995
Iteration 15/25 | Loss: 0.00251928
Iteration 16/25 | Loss: 0.00252027
Iteration 17/25 | Loss: 0.00251861
Iteration 18/25 | Loss: 0.00251833
Iteration 19/25 | Loss: 0.00251829
Iteration 20/25 | Loss: 0.00251829
Iteration 21/25 | Loss: 0.00251840
Iteration 22/25 | Loss: 0.00251828
Iteration 23/25 | Loss: 0.00251842
Iteration 24/25 | Loss: 0.00251823
Iteration 25/25 | Loss: 0.00251840

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40377462
Iteration 2/25 | Loss: 0.00965337
Iteration 3/25 | Loss: 0.00943791
Iteration 4/25 | Loss: 0.00943791
Iteration 5/25 | Loss: 0.00943791
Iteration 6/25 | Loss: 0.00943791
Iteration 7/25 | Loss: 0.00943791
Iteration 8/25 | Loss: 0.00943791
Iteration 9/25 | Loss: 0.00943791
Iteration 10/25 | Loss: 0.00943791
Iteration 11/25 | Loss: 0.00943791
Iteration 12/25 | Loss: 0.00943791
Iteration 13/25 | Loss: 0.00943791
Iteration 14/25 | Loss: 0.00943791
Iteration 15/25 | Loss: 0.00943791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.009437907487154007, 0.009437907487154007, 0.009437907487154007, 0.009437907487154007, 0.009437907487154007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009437907487154007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00943791
Iteration 2/1000 | Loss: 0.00085679
Iteration 3/1000 | Loss: 0.00071797
Iteration 4/1000 | Loss: 0.00053966
Iteration 5/1000 | Loss: 0.00049209
Iteration 6/1000 | Loss: 0.00046798
Iteration 7/1000 | Loss: 0.00044346
Iteration 8/1000 | Loss: 0.00304661
Iteration 9/1000 | Loss: 0.05668077
Iteration 10/1000 | Loss: 0.00262467
Iteration 11/1000 | Loss: 0.00117350
Iteration 12/1000 | Loss: 0.00072865
Iteration 13/1000 | Loss: 0.00120958
Iteration 14/1000 | Loss: 0.00038147
Iteration 15/1000 | Loss: 0.00031615
Iteration 16/1000 | Loss: 0.00031771
Iteration 17/1000 | Loss: 0.00016355
Iteration 18/1000 | Loss: 0.00026935
Iteration 19/1000 | Loss: 0.00026991
Iteration 20/1000 | Loss: 0.00007280
Iteration 21/1000 | Loss: 0.00006012
Iteration 22/1000 | Loss: 0.00004636
Iteration 23/1000 | Loss: 0.00004076
Iteration 24/1000 | Loss: 0.00004628
Iteration 25/1000 | Loss: 0.00003740
Iteration 26/1000 | Loss: 0.00003439
Iteration 27/1000 | Loss: 0.00003292
Iteration 28/1000 | Loss: 0.00003080
Iteration 29/1000 | Loss: 0.00003006
Iteration 30/1000 | Loss: 0.00003089
Iteration 31/1000 | Loss: 0.00002897
Iteration 32/1000 | Loss: 0.00002834
Iteration 33/1000 | Loss: 0.00002924
Iteration 34/1000 | Loss: 0.00002782
Iteration 35/1000 | Loss: 0.00002754
Iteration 36/1000 | Loss: 0.00002750
Iteration 37/1000 | Loss: 0.00002780
Iteration 38/1000 | Loss: 0.00002779
Iteration 39/1000 | Loss: 0.00002771
Iteration 40/1000 | Loss: 0.00002749
Iteration 41/1000 | Loss: 0.00002794
Iteration 42/1000 | Loss: 0.00002760
Iteration 43/1000 | Loss: 0.00002829
Iteration 44/1000 | Loss: 0.00002764
Iteration 45/1000 | Loss: 0.00002873
Iteration 46/1000 | Loss: 0.00002769
Iteration 47/1000 | Loss: 0.00002814
Iteration 48/1000 | Loss: 0.00002882
Iteration 49/1000 | Loss: 0.00002859
Iteration 50/1000 | Loss: 0.00002865
Iteration 51/1000 | Loss: 0.00002820
Iteration 52/1000 | Loss: 0.00002971
Iteration 53/1000 | Loss: 0.00002809
Iteration 54/1000 | Loss: 0.00002929
Iteration 55/1000 | Loss: 0.00002831
Iteration 56/1000 | Loss: 0.00002887
Iteration 57/1000 | Loss: 0.00002876
Iteration 58/1000 | Loss: 0.00002925
Iteration 59/1000 | Loss: 0.00002968
Iteration 60/1000 | Loss: 0.00002942
Iteration 61/1000 | Loss: 0.00002731
Iteration 62/1000 | Loss: 0.00002728
Iteration 63/1000 | Loss: 0.00002725
Iteration 64/1000 | Loss: 0.00002725
Iteration 65/1000 | Loss: 0.00002724
Iteration 66/1000 | Loss: 0.00002723
Iteration 67/1000 | Loss: 0.00002723
Iteration 68/1000 | Loss: 0.00002722
Iteration 69/1000 | Loss: 0.00002722
Iteration 70/1000 | Loss: 0.00002721
Iteration 71/1000 | Loss: 0.00002721
Iteration 72/1000 | Loss: 0.00002721
Iteration 73/1000 | Loss: 0.00002719
Iteration 74/1000 | Loss: 0.00002719
Iteration 75/1000 | Loss: 0.00002718
Iteration 76/1000 | Loss: 0.00002718
Iteration 77/1000 | Loss: 0.00002718
Iteration 78/1000 | Loss: 0.00002718
Iteration 79/1000 | Loss: 0.00002717
Iteration 80/1000 | Loss: 0.00002716
Iteration 81/1000 | Loss: 0.00002714
Iteration 82/1000 | Loss: 0.00002714
Iteration 83/1000 | Loss: 0.00002714
Iteration 84/1000 | Loss: 0.00002713
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002713
Iteration 87/1000 | Loss: 0.00002713
Iteration 88/1000 | Loss: 0.00002713
Iteration 89/1000 | Loss: 0.00002713
Iteration 90/1000 | Loss: 0.00002713
Iteration 91/1000 | Loss: 0.00002713
Iteration 92/1000 | Loss: 0.00002713
Iteration 93/1000 | Loss: 0.00002713
Iteration 94/1000 | Loss: 0.00002712
Iteration 95/1000 | Loss: 0.00002712
Iteration 96/1000 | Loss: 0.00002712
Iteration 97/1000 | Loss: 0.00002712
Iteration 98/1000 | Loss: 0.00002712
Iteration 99/1000 | Loss: 0.00002712
Iteration 100/1000 | Loss: 0.00002711
Iteration 101/1000 | Loss: 0.00002711
Iteration 102/1000 | Loss: 0.00002711
Iteration 103/1000 | Loss: 0.00002711
Iteration 104/1000 | Loss: 0.00002711
Iteration 105/1000 | Loss: 0.00002711
Iteration 106/1000 | Loss: 0.00002711
Iteration 107/1000 | Loss: 0.00003258
Iteration 108/1000 | Loss: 0.00002917
Iteration 109/1000 | Loss: 0.00003144
Iteration 110/1000 | Loss: 0.00002931
Iteration 111/1000 | Loss: 0.00002719
Iteration 112/1000 | Loss: 0.00002712
Iteration 113/1000 | Loss: 0.00002711
Iteration 114/1000 | Loss: 0.00002710
Iteration 115/1000 | Loss: 0.00002710
Iteration 116/1000 | Loss: 0.00002710
Iteration 117/1000 | Loss: 0.00002710
Iteration 118/1000 | Loss: 0.00002710
Iteration 119/1000 | Loss: 0.00002710
Iteration 120/1000 | Loss: 0.00002709
Iteration 121/1000 | Loss: 0.00002715
Iteration 122/1000 | Loss: 0.00002715
Iteration 123/1000 | Loss: 0.00002710
Iteration 124/1000 | Loss: 0.00002710
Iteration 125/1000 | Loss: 0.00002710
Iteration 126/1000 | Loss: 0.00002710
Iteration 127/1000 | Loss: 0.00002710
Iteration 128/1000 | Loss: 0.00002710
Iteration 129/1000 | Loss: 0.00002709
Iteration 130/1000 | Loss: 0.00002709
Iteration 131/1000 | Loss: 0.00002709
Iteration 132/1000 | Loss: 0.00002709
Iteration 133/1000 | Loss: 0.00002709
Iteration 134/1000 | Loss: 0.00002709
Iteration 135/1000 | Loss: 0.00002709
Iteration 136/1000 | Loss: 0.00002708
Iteration 137/1000 | Loss: 0.00002708
Iteration 138/1000 | Loss: 0.00002708
Iteration 139/1000 | Loss: 0.00002714
Iteration 140/1000 | Loss: 0.00002714
Iteration 141/1000 | Loss: 0.00002714
Iteration 142/1000 | Loss: 0.00002708
Iteration 143/1000 | Loss: 0.00002708
Iteration 144/1000 | Loss: 0.00002708
Iteration 145/1000 | Loss: 0.00002708
Iteration 146/1000 | Loss: 0.00002708
Iteration 147/1000 | Loss: 0.00002708
Iteration 148/1000 | Loss: 0.00002708
Iteration 149/1000 | Loss: 0.00002708
Iteration 150/1000 | Loss: 0.00002708
Iteration 151/1000 | Loss: 0.00002708
Iteration 152/1000 | Loss: 0.00002708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.7077359845861793e-05, 2.7077359845861793e-05, 2.7077359845861793e-05, 2.7077359845861793e-05, 2.7077359845861793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7077359845861793e-05

Optimization complete. Final v2v error: 4.465170383453369 mm

Highest mean error: 11.076122283935547 mm for frame 127

Lowest mean error: 4.202880859375 mm for frame 3

Saving results

Total time: 131.10292959213257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139622
Iteration 2/25 | Loss: 0.01139622
Iteration 3/25 | Loss: 0.01139621
Iteration 4/25 | Loss: 0.01139621
Iteration 5/25 | Loss: 0.00342579
Iteration 6/25 | Loss: 0.00266981
Iteration 7/25 | Loss: 0.00217084
Iteration 8/25 | Loss: 0.00205838
Iteration 9/25 | Loss: 0.00201802
Iteration 10/25 | Loss: 0.00191908
Iteration 11/25 | Loss: 0.00178804
Iteration 12/25 | Loss: 0.00170887
Iteration 13/25 | Loss: 0.00168134
Iteration 14/25 | Loss: 0.00165513
Iteration 15/25 | Loss: 0.00163453
Iteration 16/25 | Loss: 0.00163316
Iteration 17/25 | Loss: 0.00162709
Iteration 18/25 | Loss: 0.00162547
Iteration 19/25 | Loss: 0.00162384
Iteration 20/25 | Loss: 0.00162338
Iteration 21/25 | Loss: 0.00162326
Iteration 22/25 | Loss: 0.00162316
Iteration 23/25 | Loss: 0.00162313
Iteration 24/25 | Loss: 0.00162313
Iteration 25/25 | Loss: 0.00162312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48839319
Iteration 2/25 | Loss: 0.00131850
Iteration 3/25 | Loss: 0.00126651
Iteration 4/25 | Loss: 0.00126651
Iteration 5/25 | Loss: 0.00126651
Iteration 6/25 | Loss: 0.00126651
Iteration 7/25 | Loss: 0.00126651
Iteration 8/25 | Loss: 0.00126651
Iteration 9/25 | Loss: 0.00126651
Iteration 10/25 | Loss: 0.00126651
Iteration 11/25 | Loss: 0.00126651
Iteration 12/25 | Loss: 0.00126651
Iteration 13/25 | Loss: 0.00126651
Iteration 14/25 | Loss: 0.00126651
Iteration 15/25 | Loss: 0.00126651
Iteration 16/25 | Loss: 0.00126651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001266507781110704, 0.001266507781110704, 0.001266507781110704, 0.001266507781110704, 0.001266507781110704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001266507781110704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126651
Iteration 2/1000 | Loss: 0.00016187
Iteration 3/1000 | Loss: 0.00019343
Iteration 4/1000 | Loss: 0.00020403
Iteration 5/1000 | Loss: 0.00008994
Iteration 6/1000 | Loss: 0.00008407
Iteration 7/1000 | Loss: 0.00018305
Iteration 8/1000 | Loss: 0.00013036
Iteration 9/1000 | Loss: 0.00007638
Iteration 10/1000 | Loss: 0.00296597
Iteration 11/1000 | Loss: 0.00116093
Iteration 12/1000 | Loss: 0.00009974
Iteration 13/1000 | Loss: 0.00007270
Iteration 14/1000 | Loss: 0.00012102
Iteration 15/1000 | Loss: 0.00005436
Iteration 16/1000 | Loss: 0.00004824
Iteration 17/1000 | Loss: 0.00015730
Iteration 18/1000 | Loss: 0.00004221
Iteration 19/1000 | Loss: 0.00013354
Iteration 20/1000 | Loss: 0.00003893
Iteration 21/1000 | Loss: 0.00012738
Iteration 22/1000 | Loss: 0.00003734
Iteration 23/1000 | Loss: 0.00003611
Iteration 24/1000 | Loss: 0.00003533
Iteration 25/1000 | Loss: 0.00003458
Iteration 26/1000 | Loss: 0.00003386
Iteration 27/1000 | Loss: 0.00003342
Iteration 28/1000 | Loss: 0.00010227
Iteration 29/1000 | Loss: 0.00003316
Iteration 30/1000 | Loss: 0.00003302
Iteration 31/1000 | Loss: 0.00003300
Iteration 32/1000 | Loss: 0.00003297
Iteration 33/1000 | Loss: 0.00003296
Iteration 34/1000 | Loss: 0.00003296
Iteration 35/1000 | Loss: 0.00003295
Iteration 36/1000 | Loss: 0.00003295
Iteration 37/1000 | Loss: 0.00003294
Iteration 38/1000 | Loss: 0.00003294
Iteration 39/1000 | Loss: 0.00003290
Iteration 40/1000 | Loss: 0.00003286
Iteration 41/1000 | Loss: 0.00003286
Iteration 42/1000 | Loss: 0.00011669
Iteration 43/1000 | Loss: 0.00003308
Iteration 44/1000 | Loss: 0.00003278
Iteration 45/1000 | Loss: 0.00003277
Iteration 46/1000 | Loss: 0.00003277
Iteration 47/1000 | Loss: 0.00003277
Iteration 48/1000 | Loss: 0.00003277
Iteration 49/1000 | Loss: 0.00003276
Iteration 50/1000 | Loss: 0.00003276
Iteration 51/1000 | Loss: 0.00003276
Iteration 52/1000 | Loss: 0.00003276
Iteration 53/1000 | Loss: 0.00003276
Iteration 54/1000 | Loss: 0.00003276
Iteration 55/1000 | Loss: 0.00003276
Iteration 56/1000 | Loss: 0.00003276
Iteration 57/1000 | Loss: 0.00003275
Iteration 58/1000 | Loss: 0.00003275
Iteration 59/1000 | Loss: 0.00003274
Iteration 60/1000 | Loss: 0.00003274
Iteration 61/1000 | Loss: 0.00003273
Iteration 62/1000 | Loss: 0.00003273
Iteration 63/1000 | Loss: 0.00003273
Iteration 64/1000 | Loss: 0.00003272
Iteration 65/1000 | Loss: 0.00003272
Iteration 66/1000 | Loss: 0.00003272
Iteration 67/1000 | Loss: 0.00003272
Iteration 68/1000 | Loss: 0.00003272
Iteration 69/1000 | Loss: 0.00003271
Iteration 70/1000 | Loss: 0.00003271
Iteration 71/1000 | Loss: 0.00003271
Iteration 72/1000 | Loss: 0.00003271
Iteration 73/1000 | Loss: 0.00003271
Iteration 74/1000 | Loss: 0.00003271
Iteration 75/1000 | Loss: 0.00003270
Iteration 76/1000 | Loss: 0.00003270
Iteration 77/1000 | Loss: 0.00003270
Iteration 78/1000 | Loss: 0.00003270
Iteration 79/1000 | Loss: 0.00003269
Iteration 80/1000 | Loss: 0.00003269
Iteration 81/1000 | Loss: 0.00003269
Iteration 82/1000 | Loss: 0.00003269
Iteration 83/1000 | Loss: 0.00003269
Iteration 84/1000 | Loss: 0.00003269
Iteration 85/1000 | Loss: 0.00003268
Iteration 86/1000 | Loss: 0.00003267
Iteration 87/1000 | Loss: 0.00003267
Iteration 88/1000 | Loss: 0.00003267
Iteration 89/1000 | Loss: 0.00003267
Iteration 90/1000 | Loss: 0.00003267
Iteration 91/1000 | Loss: 0.00003267
Iteration 92/1000 | Loss: 0.00003267
Iteration 93/1000 | Loss: 0.00003267
Iteration 94/1000 | Loss: 0.00003266
Iteration 95/1000 | Loss: 0.00003266
Iteration 96/1000 | Loss: 0.00003266
Iteration 97/1000 | Loss: 0.00003266
Iteration 98/1000 | Loss: 0.00003266
Iteration 99/1000 | Loss: 0.00003266
Iteration 100/1000 | Loss: 0.00003266
Iteration 101/1000 | Loss: 0.00003266
Iteration 102/1000 | Loss: 0.00003265
Iteration 103/1000 | Loss: 0.00003265
Iteration 104/1000 | Loss: 0.00003265
Iteration 105/1000 | Loss: 0.00003265
Iteration 106/1000 | Loss: 0.00003265
Iteration 107/1000 | Loss: 0.00003265
Iteration 108/1000 | Loss: 0.00003265
Iteration 109/1000 | Loss: 0.00003265
Iteration 110/1000 | Loss: 0.00003265
Iteration 111/1000 | Loss: 0.00003265
Iteration 112/1000 | Loss: 0.00003264
Iteration 113/1000 | Loss: 0.00003264
Iteration 114/1000 | Loss: 0.00003264
Iteration 115/1000 | Loss: 0.00003264
Iteration 116/1000 | Loss: 0.00003264
Iteration 117/1000 | Loss: 0.00003264
Iteration 118/1000 | Loss: 0.00003264
Iteration 119/1000 | Loss: 0.00003264
Iteration 120/1000 | Loss: 0.00003263
Iteration 121/1000 | Loss: 0.00003263
Iteration 122/1000 | Loss: 0.00003263
Iteration 123/1000 | Loss: 0.00003262
Iteration 124/1000 | Loss: 0.00003262
Iteration 125/1000 | Loss: 0.00003262
Iteration 126/1000 | Loss: 0.00003262
Iteration 127/1000 | Loss: 0.00003262
Iteration 128/1000 | Loss: 0.00003262
Iteration 129/1000 | Loss: 0.00003262
Iteration 130/1000 | Loss: 0.00003262
Iteration 131/1000 | Loss: 0.00003261
Iteration 132/1000 | Loss: 0.00003261
Iteration 133/1000 | Loss: 0.00003261
Iteration 134/1000 | Loss: 0.00003261
Iteration 135/1000 | Loss: 0.00003261
Iteration 136/1000 | Loss: 0.00003261
Iteration 137/1000 | Loss: 0.00003261
Iteration 138/1000 | Loss: 0.00003261
Iteration 139/1000 | Loss: 0.00003261
Iteration 140/1000 | Loss: 0.00003261
Iteration 141/1000 | Loss: 0.00003261
Iteration 142/1000 | Loss: 0.00003261
Iteration 143/1000 | Loss: 0.00003261
Iteration 144/1000 | Loss: 0.00003261
Iteration 145/1000 | Loss: 0.00003261
Iteration 146/1000 | Loss: 0.00003261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.260928133386187e-05, 3.260928133386187e-05, 3.260928133386187e-05, 3.260928133386187e-05, 3.260928133386187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.260928133386187e-05

Optimization complete. Final v2v error: 4.723510265350342 mm

Highest mean error: 9.338191032409668 mm for frame 10

Lowest mean error: 4.449258327484131 mm for frame 95

Saving results

Total time: 98.84352326393127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01165176
Iteration 2/25 | Loss: 0.00322326
Iteration 3/25 | Loss: 0.00254648
Iteration 4/25 | Loss: 0.00270082
Iteration 5/25 | Loss: 0.00221315
Iteration 6/25 | Loss: 0.00206148
Iteration 7/25 | Loss: 0.00202435
Iteration 8/25 | Loss: 0.00199778
Iteration 9/25 | Loss: 0.00198075
Iteration 10/25 | Loss: 0.00196677
Iteration 11/25 | Loss: 0.00195226
Iteration 12/25 | Loss: 0.00195035
Iteration 13/25 | Loss: 0.00194451
Iteration 14/25 | Loss: 0.00193714
Iteration 15/25 | Loss: 0.00193202
Iteration 16/25 | Loss: 0.00193060
Iteration 17/25 | Loss: 0.00193413
Iteration 18/25 | Loss: 0.00192517
Iteration 19/25 | Loss: 0.00192385
Iteration 20/25 | Loss: 0.00192360
Iteration 21/25 | Loss: 0.00192347
Iteration 22/25 | Loss: 0.00192347
Iteration 23/25 | Loss: 0.00192346
Iteration 24/25 | Loss: 0.00192346
Iteration 25/25 | Loss: 0.00192345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43052888
Iteration 2/25 | Loss: 0.00158238
Iteration 3/25 | Loss: 0.00158238
Iteration 4/25 | Loss: 0.00158238
Iteration 5/25 | Loss: 0.00158238
Iteration 6/25 | Loss: 0.00158238
Iteration 7/25 | Loss: 0.00158238
Iteration 8/25 | Loss: 0.00158238
Iteration 9/25 | Loss: 0.00158238
Iteration 10/25 | Loss: 0.00158238
Iteration 11/25 | Loss: 0.00158238
Iteration 12/25 | Loss: 0.00158238
Iteration 13/25 | Loss: 0.00158238
Iteration 14/25 | Loss: 0.00158238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0015823793364688754, 0.0015823793364688754, 0.0015823793364688754, 0.0015823793364688754, 0.0015823793364688754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015823793364688754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158238
Iteration 2/1000 | Loss: 0.00092091
Iteration 3/1000 | Loss: 0.00014495
Iteration 4/1000 | Loss: 0.00062104
Iteration 5/1000 | Loss: 0.00157622
Iteration 6/1000 | Loss: 0.00207241
Iteration 7/1000 | Loss: 0.00217754
Iteration 8/1000 | Loss: 0.00566651
Iteration 9/1000 | Loss: 0.00387275
Iteration 10/1000 | Loss: 0.00426829
Iteration 11/1000 | Loss: 0.00199369
Iteration 12/1000 | Loss: 0.00139143
Iteration 13/1000 | Loss: 0.00159676
Iteration 14/1000 | Loss: 0.00023475
Iteration 15/1000 | Loss: 0.00012360
Iteration 16/1000 | Loss: 0.00009025
Iteration 17/1000 | Loss: 0.00043876
Iteration 18/1000 | Loss: 0.00021432
Iteration 19/1000 | Loss: 0.00023841
Iteration 20/1000 | Loss: 0.00007474
Iteration 21/1000 | Loss: 0.00005661
Iteration 22/1000 | Loss: 0.00004720
Iteration 23/1000 | Loss: 0.00004370
Iteration 24/1000 | Loss: 0.00004153
Iteration 25/1000 | Loss: 0.00004013
Iteration 26/1000 | Loss: 0.00003911
Iteration 27/1000 | Loss: 0.00003838
Iteration 28/1000 | Loss: 0.00003791
Iteration 29/1000 | Loss: 0.00003735
Iteration 30/1000 | Loss: 0.00003693
Iteration 31/1000 | Loss: 0.00003659
Iteration 32/1000 | Loss: 0.00003638
Iteration 33/1000 | Loss: 0.00003632
Iteration 34/1000 | Loss: 0.00003625
Iteration 35/1000 | Loss: 0.00003621
Iteration 36/1000 | Loss: 0.00003620
Iteration 37/1000 | Loss: 0.00003619
Iteration 38/1000 | Loss: 0.00003619
Iteration 39/1000 | Loss: 0.00003614
Iteration 40/1000 | Loss: 0.00003613
Iteration 41/1000 | Loss: 0.00003612
Iteration 42/1000 | Loss: 0.00003612
Iteration 43/1000 | Loss: 0.00003611
Iteration 44/1000 | Loss: 0.00003611
Iteration 45/1000 | Loss: 0.00003610
Iteration 46/1000 | Loss: 0.00003609
Iteration 47/1000 | Loss: 0.00003608
Iteration 48/1000 | Loss: 0.00003608
Iteration 49/1000 | Loss: 0.00003608
Iteration 50/1000 | Loss: 0.00003607
Iteration 51/1000 | Loss: 0.00003607
Iteration 52/1000 | Loss: 0.00003607
Iteration 53/1000 | Loss: 0.00003606
Iteration 54/1000 | Loss: 0.00003606
Iteration 55/1000 | Loss: 0.00003606
Iteration 56/1000 | Loss: 0.00003605
Iteration 57/1000 | Loss: 0.00003605
Iteration 58/1000 | Loss: 0.00003605
Iteration 59/1000 | Loss: 0.00003604
Iteration 60/1000 | Loss: 0.00003604
Iteration 61/1000 | Loss: 0.00003604
Iteration 62/1000 | Loss: 0.00003604
Iteration 63/1000 | Loss: 0.00003604
Iteration 64/1000 | Loss: 0.00003603
Iteration 65/1000 | Loss: 0.00003603
Iteration 66/1000 | Loss: 0.00003603
Iteration 67/1000 | Loss: 0.00003603
Iteration 68/1000 | Loss: 0.00003603
Iteration 69/1000 | Loss: 0.00003603
Iteration 70/1000 | Loss: 0.00003603
Iteration 71/1000 | Loss: 0.00003603
Iteration 72/1000 | Loss: 0.00003602
Iteration 73/1000 | Loss: 0.00003602
Iteration 74/1000 | Loss: 0.00003602
Iteration 75/1000 | Loss: 0.00003601
Iteration 76/1000 | Loss: 0.00003601
Iteration 77/1000 | Loss: 0.00003601
Iteration 78/1000 | Loss: 0.00003601
Iteration 79/1000 | Loss: 0.00003601
Iteration 80/1000 | Loss: 0.00003601
Iteration 81/1000 | Loss: 0.00003601
Iteration 82/1000 | Loss: 0.00003600
Iteration 83/1000 | Loss: 0.00003600
Iteration 84/1000 | Loss: 0.00003600
Iteration 85/1000 | Loss: 0.00003600
Iteration 86/1000 | Loss: 0.00003599
Iteration 87/1000 | Loss: 0.00003599
Iteration 88/1000 | Loss: 0.00003599
Iteration 89/1000 | Loss: 0.00003599
Iteration 90/1000 | Loss: 0.00003598
Iteration 91/1000 | Loss: 0.00003598
Iteration 92/1000 | Loss: 0.00003598
Iteration 93/1000 | Loss: 0.00003598
Iteration 94/1000 | Loss: 0.00003597
Iteration 95/1000 | Loss: 0.00003597
Iteration 96/1000 | Loss: 0.00003597
Iteration 97/1000 | Loss: 0.00003597
Iteration 98/1000 | Loss: 0.00003597
Iteration 99/1000 | Loss: 0.00003597
Iteration 100/1000 | Loss: 0.00003597
Iteration 101/1000 | Loss: 0.00003597
Iteration 102/1000 | Loss: 0.00003596
Iteration 103/1000 | Loss: 0.00003596
Iteration 104/1000 | Loss: 0.00003596
Iteration 105/1000 | Loss: 0.00003596
Iteration 106/1000 | Loss: 0.00003596
Iteration 107/1000 | Loss: 0.00003596
Iteration 108/1000 | Loss: 0.00003596
Iteration 109/1000 | Loss: 0.00003596
Iteration 110/1000 | Loss: 0.00003596
Iteration 111/1000 | Loss: 0.00003596
Iteration 112/1000 | Loss: 0.00003596
Iteration 113/1000 | Loss: 0.00003596
Iteration 114/1000 | Loss: 0.00003596
Iteration 115/1000 | Loss: 0.00003595
Iteration 116/1000 | Loss: 0.00003595
Iteration 117/1000 | Loss: 0.00003595
Iteration 118/1000 | Loss: 0.00003595
Iteration 119/1000 | Loss: 0.00003595
Iteration 120/1000 | Loss: 0.00003595
Iteration 121/1000 | Loss: 0.00003595
Iteration 122/1000 | Loss: 0.00003595
Iteration 123/1000 | Loss: 0.00003595
Iteration 124/1000 | Loss: 0.00003595
Iteration 125/1000 | Loss: 0.00003595
Iteration 126/1000 | Loss: 0.00003595
Iteration 127/1000 | Loss: 0.00003595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [3.5948796721640974e-05, 3.5948796721640974e-05, 3.5948796721640974e-05, 3.5948796721640974e-05, 3.5948796721640974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5948796721640974e-05

Optimization complete. Final v2v error: 5.057443141937256 mm

Highest mean error: 10.501394271850586 mm for frame 2

Lowest mean error: 4.782783508300781 mm for frame 109

Saving results

Total time: 87.46553993225098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560352
Iteration 2/25 | Loss: 0.00210871
Iteration 3/25 | Loss: 0.00195204
Iteration 4/25 | Loss: 0.00192944
Iteration 5/25 | Loss: 0.00192008
Iteration 6/25 | Loss: 0.00191872
Iteration 7/25 | Loss: 0.00191872
Iteration 8/25 | Loss: 0.00191872
Iteration 9/25 | Loss: 0.00191872
Iteration 10/25 | Loss: 0.00191872
Iteration 11/25 | Loss: 0.00191872
Iteration 12/25 | Loss: 0.00191872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0019187238067388535, 0.0019187238067388535, 0.0019187238067388535, 0.0019187238067388535, 0.0019187238067388535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019187238067388535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82376295
Iteration 2/25 | Loss: 0.00166154
Iteration 3/25 | Loss: 0.00166153
Iteration 4/25 | Loss: 0.00166153
Iteration 5/25 | Loss: 0.00166153
Iteration 6/25 | Loss: 0.00166153
Iteration 7/25 | Loss: 0.00166153
Iteration 8/25 | Loss: 0.00166153
Iteration 9/25 | Loss: 0.00166153
Iteration 10/25 | Loss: 0.00166153
Iteration 11/25 | Loss: 0.00166153
Iteration 12/25 | Loss: 0.00166153
Iteration 13/25 | Loss: 0.00166153
Iteration 14/25 | Loss: 0.00166153
Iteration 15/25 | Loss: 0.00166153
Iteration 16/25 | Loss: 0.00166153
Iteration 17/25 | Loss: 0.00166153
Iteration 18/25 | Loss: 0.00166153
Iteration 19/25 | Loss: 0.00166153
Iteration 20/25 | Loss: 0.00166153
Iteration 21/25 | Loss: 0.00166153
Iteration 22/25 | Loss: 0.00166153
Iteration 23/25 | Loss: 0.00166153
Iteration 24/25 | Loss: 0.00166153
Iteration 25/25 | Loss: 0.00166153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166153
Iteration 2/1000 | Loss: 0.00010692
Iteration 3/1000 | Loss: 0.00007832
Iteration 4/1000 | Loss: 0.00007063
Iteration 5/1000 | Loss: 0.00006796
Iteration 6/1000 | Loss: 0.00006651
Iteration 7/1000 | Loss: 0.00006490
Iteration 8/1000 | Loss: 0.00006340
Iteration 9/1000 | Loss: 0.00006240
Iteration 10/1000 | Loss: 0.00006181
Iteration 11/1000 | Loss: 0.00006119
Iteration 12/1000 | Loss: 0.00006048
Iteration 13/1000 | Loss: 0.00005995
Iteration 14/1000 | Loss: 0.00005958
Iteration 15/1000 | Loss: 0.00005932
Iteration 16/1000 | Loss: 0.00005899
Iteration 17/1000 | Loss: 0.00005862
Iteration 18/1000 | Loss: 0.00005835
Iteration 19/1000 | Loss: 0.00005797
Iteration 20/1000 | Loss: 0.00005779
Iteration 21/1000 | Loss: 0.00005769
Iteration 22/1000 | Loss: 0.00005754
Iteration 23/1000 | Loss: 0.00005737
Iteration 24/1000 | Loss: 0.00005723
Iteration 25/1000 | Loss: 0.00005721
Iteration 26/1000 | Loss: 0.00005707
Iteration 27/1000 | Loss: 0.00005707
Iteration 28/1000 | Loss: 0.00005705
Iteration 29/1000 | Loss: 0.00005705
Iteration 30/1000 | Loss: 0.00005705
Iteration 31/1000 | Loss: 0.00005705
Iteration 32/1000 | Loss: 0.00005705
Iteration 33/1000 | Loss: 0.00005704
Iteration 34/1000 | Loss: 0.00005704
Iteration 35/1000 | Loss: 0.00005702
Iteration 36/1000 | Loss: 0.00005700
Iteration 37/1000 | Loss: 0.00005698
Iteration 38/1000 | Loss: 0.00005698
Iteration 39/1000 | Loss: 0.00005698
Iteration 40/1000 | Loss: 0.00005698
Iteration 41/1000 | Loss: 0.00005697
Iteration 42/1000 | Loss: 0.00005697
Iteration 43/1000 | Loss: 0.00005697
Iteration 44/1000 | Loss: 0.00005696
Iteration 45/1000 | Loss: 0.00005696
Iteration 46/1000 | Loss: 0.00005696
Iteration 47/1000 | Loss: 0.00005695
Iteration 48/1000 | Loss: 0.00005692
Iteration 49/1000 | Loss: 0.00005692
Iteration 50/1000 | Loss: 0.00005692
Iteration 51/1000 | Loss: 0.00005692
Iteration 52/1000 | Loss: 0.00005692
Iteration 53/1000 | Loss: 0.00005691
Iteration 54/1000 | Loss: 0.00005691
Iteration 55/1000 | Loss: 0.00005691
Iteration 56/1000 | Loss: 0.00005690
Iteration 57/1000 | Loss: 0.00005688
Iteration 58/1000 | Loss: 0.00005688
Iteration 59/1000 | Loss: 0.00005688
Iteration 60/1000 | Loss: 0.00005687
Iteration 61/1000 | Loss: 0.00005687
Iteration 62/1000 | Loss: 0.00005687
Iteration 63/1000 | Loss: 0.00005687
Iteration 64/1000 | Loss: 0.00005687
Iteration 65/1000 | Loss: 0.00005686
Iteration 66/1000 | Loss: 0.00005686
Iteration 67/1000 | Loss: 0.00005686
Iteration 68/1000 | Loss: 0.00005686
Iteration 69/1000 | Loss: 0.00005686
Iteration 70/1000 | Loss: 0.00005686
Iteration 71/1000 | Loss: 0.00005686
Iteration 72/1000 | Loss: 0.00005686
Iteration 73/1000 | Loss: 0.00005686
Iteration 74/1000 | Loss: 0.00005686
Iteration 75/1000 | Loss: 0.00005686
Iteration 76/1000 | Loss: 0.00005686
Iteration 77/1000 | Loss: 0.00005686
Iteration 78/1000 | Loss: 0.00005685
Iteration 79/1000 | Loss: 0.00005685
Iteration 80/1000 | Loss: 0.00005685
Iteration 81/1000 | Loss: 0.00005685
Iteration 82/1000 | Loss: 0.00005685
Iteration 83/1000 | Loss: 0.00005685
Iteration 84/1000 | Loss: 0.00005684
Iteration 85/1000 | Loss: 0.00005684
Iteration 86/1000 | Loss: 0.00005684
Iteration 87/1000 | Loss: 0.00005684
Iteration 88/1000 | Loss: 0.00005684
Iteration 89/1000 | Loss: 0.00005684
Iteration 90/1000 | Loss: 0.00005683
Iteration 91/1000 | Loss: 0.00005683
Iteration 92/1000 | Loss: 0.00005683
Iteration 93/1000 | Loss: 0.00005683
Iteration 94/1000 | Loss: 0.00005683
Iteration 95/1000 | Loss: 0.00005683
Iteration 96/1000 | Loss: 0.00005683
Iteration 97/1000 | Loss: 0.00005683
Iteration 98/1000 | Loss: 0.00005682
Iteration 99/1000 | Loss: 0.00005682
Iteration 100/1000 | Loss: 0.00005682
Iteration 101/1000 | Loss: 0.00005681
Iteration 102/1000 | Loss: 0.00005681
Iteration 103/1000 | Loss: 0.00005681
Iteration 104/1000 | Loss: 0.00005681
Iteration 105/1000 | Loss: 0.00005681
Iteration 106/1000 | Loss: 0.00005681
Iteration 107/1000 | Loss: 0.00005681
Iteration 108/1000 | Loss: 0.00005681
Iteration 109/1000 | Loss: 0.00005681
Iteration 110/1000 | Loss: 0.00005681
Iteration 111/1000 | Loss: 0.00005681
Iteration 112/1000 | Loss: 0.00005681
Iteration 113/1000 | Loss: 0.00005681
Iteration 114/1000 | Loss: 0.00005681
Iteration 115/1000 | Loss: 0.00005681
Iteration 116/1000 | Loss: 0.00005681
Iteration 117/1000 | Loss: 0.00005681
Iteration 118/1000 | Loss: 0.00005681
Iteration 119/1000 | Loss: 0.00005681
Iteration 120/1000 | Loss: 0.00005681
Iteration 121/1000 | Loss: 0.00005681
Iteration 122/1000 | Loss: 0.00005681
Iteration 123/1000 | Loss: 0.00005681
Iteration 124/1000 | Loss: 0.00005681
Iteration 125/1000 | Loss: 0.00005681
Iteration 126/1000 | Loss: 0.00005681
Iteration 127/1000 | Loss: 0.00005681
Iteration 128/1000 | Loss: 0.00005681
Iteration 129/1000 | Loss: 0.00005681
Iteration 130/1000 | Loss: 0.00005681
Iteration 131/1000 | Loss: 0.00005681
Iteration 132/1000 | Loss: 0.00005681
Iteration 133/1000 | Loss: 0.00005681
Iteration 134/1000 | Loss: 0.00005681
Iteration 135/1000 | Loss: 0.00005681
Iteration 136/1000 | Loss: 0.00005681
Iteration 137/1000 | Loss: 0.00005681
Iteration 138/1000 | Loss: 0.00005681
Iteration 139/1000 | Loss: 0.00005681
Iteration 140/1000 | Loss: 0.00005681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [5.6806667998898774e-05, 5.6806667998898774e-05, 5.6806667998898774e-05, 5.6806667998898774e-05, 5.6806667998898774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.6806667998898774e-05

Optimization complete. Final v2v error: 6.241038799285889 mm

Highest mean error: 6.372894287109375 mm for frame 183

Lowest mean error: 6.035294055938721 mm for frame 0

Saving results

Total time: 61.14881229400635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479270
Iteration 2/25 | Loss: 0.00200291
Iteration 3/25 | Loss: 0.00185166
Iteration 4/25 | Loss: 0.00183232
Iteration 5/25 | Loss: 0.00182715
Iteration 6/25 | Loss: 0.00182647
Iteration 7/25 | Loss: 0.00182647
Iteration 8/25 | Loss: 0.00182647
Iteration 9/25 | Loss: 0.00182647
Iteration 10/25 | Loss: 0.00182647
Iteration 11/25 | Loss: 0.00182647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018264732789248228, 0.0018264732789248228, 0.0018264732789248228, 0.0018264732789248228, 0.0018264732789248228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018264732789248228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39431405
Iteration 2/25 | Loss: 0.00123989
Iteration 3/25 | Loss: 0.00123988
Iteration 4/25 | Loss: 0.00123988
Iteration 5/25 | Loss: 0.00123988
Iteration 6/25 | Loss: 0.00123988
Iteration 7/25 | Loss: 0.00123988
Iteration 8/25 | Loss: 0.00123988
Iteration 9/25 | Loss: 0.00123988
Iteration 10/25 | Loss: 0.00123988
Iteration 11/25 | Loss: 0.00123988
Iteration 12/25 | Loss: 0.00123988
Iteration 13/25 | Loss: 0.00123988
Iteration 14/25 | Loss: 0.00123988
Iteration 15/25 | Loss: 0.00123988
Iteration 16/25 | Loss: 0.00123988
Iteration 17/25 | Loss: 0.00123988
Iteration 18/25 | Loss: 0.00123988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012398791732266545, 0.0012398791732266545, 0.0012398791732266545, 0.0012398791732266545, 0.0012398791732266545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012398791732266545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123988
Iteration 2/1000 | Loss: 0.00009252
Iteration 3/1000 | Loss: 0.00005321
Iteration 4/1000 | Loss: 0.00004092
Iteration 5/1000 | Loss: 0.00003730
Iteration 6/1000 | Loss: 0.00003535
Iteration 7/1000 | Loss: 0.00003409
Iteration 8/1000 | Loss: 0.00003294
Iteration 9/1000 | Loss: 0.00003241
Iteration 10/1000 | Loss: 0.00003192
Iteration 11/1000 | Loss: 0.00003147
Iteration 12/1000 | Loss: 0.00003146
Iteration 13/1000 | Loss: 0.00003121
Iteration 14/1000 | Loss: 0.00003100
Iteration 15/1000 | Loss: 0.00003075
Iteration 16/1000 | Loss: 0.00003070
Iteration 17/1000 | Loss: 0.00003066
Iteration 18/1000 | Loss: 0.00003064
Iteration 19/1000 | Loss: 0.00003063
Iteration 20/1000 | Loss: 0.00003061
Iteration 21/1000 | Loss: 0.00003061
Iteration 22/1000 | Loss: 0.00003061
Iteration 23/1000 | Loss: 0.00003060
Iteration 24/1000 | Loss: 0.00003060
Iteration 25/1000 | Loss: 0.00003060
Iteration 26/1000 | Loss: 0.00003059
Iteration 27/1000 | Loss: 0.00003059
Iteration 28/1000 | Loss: 0.00003059
Iteration 29/1000 | Loss: 0.00003058
Iteration 30/1000 | Loss: 0.00003058
Iteration 31/1000 | Loss: 0.00003058
Iteration 32/1000 | Loss: 0.00003057
Iteration 33/1000 | Loss: 0.00003057
Iteration 34/1000 | Loss: 0.00003057
Iteration 35/1000 | Loss: 0.00003056
Iteration 36/1000 | Loss: 0.00003056
Iteration 37/1000 | Loss: 0.00003056
Iteration 38/1000 | Loss: 0.00003056
Iteration 39/1000 | Loss: 0.00003055
Iteration 40/1000 | Loss: 0.00003055
Iteration 41/1000 | Loss: 0.00003055
Iteration 42/1000 | Loss: 0.00003054
Iteration 43/1000 | Loss: 0.00003054
Iteration 44/1000 | Loss: 0.00003054
Iteration 45/1000 | Loss: 0.00003053
Iteration 46/1000 | Loss: 0.00003053
Iteration 47/1000 | Loss: 0.00003053
Iteration 48/1000 | Loss: 0.00003053
Iteration 49/1000 | Loss: 0.00003053
Iteration 50/1000 | Loss: 0.00003052
Iteration 51/1000 | Loss: 0.00003052
Iteration 52/1000 | Loss: 0.00003052
Iteration 53/1000 | Loss: 0.00003051
Iteration 54/1000 | Loss: 0.00003051
Iteration 55/1000 | Loss: 0.00003051
Iteration 56/1000 | Loss: 0.00003051
Iteration 57/1000 | Loss: 0.00003051
Iteration 58/1000 | Loss: 0.00003051
Iteration 59/1000 | Loss: 0.00003051
Iteration 60/1000 | Loss: 0.00003051
Iteration 61/1000 | Loss: 0.00003050
Iteration 62/1000 | Loss: 0.00003050
Iteration 63/1000 | Loss: 0.00003050
Iteration 64/1000 | Loss: 0.00003050
Iteration 65/1000 | Loss: 0.00003050
Iteration 66/1000 | Loss: 0.00003050
Iteration 67/1000 | Loss: 0.00003050
Iteration 68/1000 | Loss: 0.00003050
Iteration 69/1000 | Loss: 0.00003050
Iteration 70/1000 | Loss: 0.00003049
Iteration 71/1000 | Loss: 0.00003049
Iteration 72/1000 | Loss: 0.00003049
Iteration 73/1000 | Loss: 0.00003049
Iteration 74/1000 | Loss: 0.00003049
Iteration 75/1000 | Loss: 0.00003049
Iteration 76/1000 | Loss: 0.00003049
Iteration 77/1000 | Loss: 0.00003049
Iteration 78/1000 | Loss: 0.00003049
Iteration 79/1000 | Loss: 0.00003049
Iteration 80/1000 | Loss: 0.00003049
Iteration 81/1000 | Loss: 0.00003049
Iteration 82/1000 | Loss: 0.00003049
Iteration 83/1000 | Loss: 0.00003049
Iteration 84/1000 | Loss: 0.00003049
Iteration 85/1000 | Loss: 0.00003049
Iteration 86/1000 | Loss: 0.00003048
Iteration 87/1000 | Loss: 0.00003048
Iteration 88/1000 | Loss: 0.00003048
Iteration 89/1000 | Loss: 0.00003048
Iteration 90/1000 | Loss: 0.00003048
Iteration 91/1000 | Loss: 0.00003048
Iteration 92/1000 | Loss: 0.00003047
Iteration 93/1000 | Loss: 0.00003047
Iteration 94/1000 | Loss: 0.00003047
Iteration 95/1000 | Loss: 0.00003047
Iteration 96/1000 | Loss: 0.00003047
Iteration 97/1000 | Loss: 0.00003047
Iteration 98/1000 | Loss: 0.00003047
Iteration 99/1000 | Loss: 0.00003047
Iteration 100/1000 | Loss: 0.00003047
Iteration 101/1000 | Loss: 0.00003047
Iteration 102/1000 | Loss: 0.00003046
Iteration 103/1000 | Loss: 0.00003046
Iteration 104/1000 | Loss: 0.00003046
Iteration 105/1000 | Loss: 0.00003046
Iteration 106/1000 | Loss: 0.00003046
Iteration 107/1000 | Loss: 0.00003046
Iteration 108/1000 | Loss: 0.00003046
Iteration 109/1000 | Loss: 0.00003046
Iteration 110/1000 | Loss: 0.00003046
Iteration 111/1000 | Loss: 0.00003046
Iteration 112/1000 | Loss: 0.00003046
Iteration 113/1000 | Loss: 0.00003045
Iteration 114/1000 | Loss: 0.00003045
Iteration 115/1000 | Loss: 0.00003045
Iteration 116/1000 | Loss: 0.00003045
Iteration 117/1000 | Loss: 0.00003045
Iteration 118/1000 | Loss: 0.00003045
Iteration 119/1000 | Loss: 0.00003045
Iteration 120/1000 | Loss: 0.00003045
Iteration 121/1000 | Loss: 0.00003045
Iteration 122/1000 | Loss: 0.00003045
Iteration 123/1000 | Loss: 0.00003045
Iteration 124/1000 | Loss: 0.00003045
Iteration 125/1000 | Loss: 0.00003045
Iteration 126/1000 | Loss: 0.00003045
Iteration 127/1000 | Loss: 0.00003045
Iteration 128/1000 | Loss: 0.00003045
Iteration 129/1000 | Loss: 0.00003045
Iteration 130/1000 | Loss: 0.00003045
Iteration 131/1000 | Loss: 0.00003045
Iteration 132/1000 | Loss: 0.00003045
Iteration 133/1000 | Loss: 0.00003045
Iteration 134/1000 | Loss: 0.00003045
Iteration 135/1000 | Loss: 0.00003045
Iteration 136/1000 | Loss: 0.00003045
Iteration 137/1000 | Loss: 0.00003045
Iteration 138/1000 | Loss: 0.00003045
Iteration 139/1000 | Loss: 0.00003045
Iteration 140/1000 | Loss: 0.00003045
Iteration 141/1000 | Loss: 0.00003045
Iteration 142/1000 | Loss: 0.00003045
Iteration 143/1000 | Loss: 0.00003045
Iteration 144/1000 | Loss: 0.00003045
Iteration 145/1000 | Loss: 0.00003045
Iteration 146/1000 | Loss: 0.00003045
Iteration 147/1000 | Loss: 0.00003045
Iteration 148/1000 | Loss: 0.00003045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [3.0450912163360044e-05, 3.0450912163360044e-05, 3.0450912163360044e-05, 3.0450912163360044e-05, 3.0450912163360044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0450912163360044e-05

Optimization complete. Final v2v error: 4.667537212371826 mm

Highest mean error: 4.848094940185547 mm for frame 51

Lowest mean error: 4.493212699890137 mm for frame 54

Saving results

Total time: 36.28785681724548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01127716
Iteration 2/25 | Loss: 0.00237535
Iteration 3/25 | Loss: 0.00207413
Iteration 4/25 | Loss: 0.00202631
Iteration 5/25 | Loss: 0.00204210
Iteration 6/25 | Loss: 0.00199291
Iteration 7/25 | Loss: 0.00197381
Iteration 8/25 | Loss: 0.00197961
Iteration 9/25 | Loss: 0.00198048
Iteration 10/25 | Loss: 0.00197489
Iteration 11/25 | Loss: 0.00197882
Iteration 12/25 | Loss: 0.00197364
Iteration 13/25 | Loss: 0.00196472
Iteration 14/25 | Loss: 0.00196805
Iteration 15/25 | Loss: 0.00196630
Iteration 16/25 | Loss: 0.00196742
Iteration 17/25 | Loss: 0.00197087
Iteration 18/25 | Loss: 0.00196805
Iteration 19/25 | Loss: 0.00197041
Iteration 20/25 | Loss: 0.00196975
Iteration 21/25 | Loss: 0.00197215
Iteration 22/25 | Loss: 0.00196633
Iteration 23/25 | Loss: 0.00196635
Iteration 24/25 | Loss: 0.00196854
Iteration 25/25 | Loss: 0.00196925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36605799
Iteration 2/25 | Loss: 0.00170631
Iteration 3/25 | Loss: 0.00170627
Iteration 4/25 | Loss: 0.00170627
Iteration 5/25 | Loss: 0.00170627
Iteration 6/25 | Loss: 0.00170627
Iteration 7/25 | Loss: 0.00170627
Iteration 8/25 | Loss: 0.00170627
Iteration 9/25 | Loss: 0.00170626
Iteration 10/25 | Loss: 0.00170626
Iteration 11/25 | Loss: 0.00170626
Iteration 12/25 | Loss: 0.00170626
Iteration 13/25 | Loss: 0.00170626
Iteration 14/25 | Loss: 0.00170626
Iteration 15/25 | Loss: 0.00170626
Iteration 16/25 | Loss: 0.00170626
Iteration 17/25 | Loss: 0.00170626
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001706264796666801, 0.001706264796666801, 0.001706264796666801, 0.001706264796666801, 0.001706264796666801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001706264796666801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170626
Iteration 2/1000 | Loss: 0.00019235
Iteration 3/1000 | Loss: 0.00026132
Iteration 4/1000 | Loss: 0.00036618
Iteration 5/1000 | Loss: 0.00028871
Iteration 6/1000 | Loss: 0.00027387
Iteration 7/1000 | Loss: 0.00024109
Iteration 8/1000 | Loss: 0.00023792
Iteration 9/1000 | Loss: 0.00030487
Iteration 10/1000 | Loss: 0.00032110
Iteration 11/1000 | Loss: 0.00030289
Iteration 12/1000 | Loss: 0.00025905
Iteration 13/1000 | Loss: 0.00027759
Iteration 14/1000 | Loss: 0.00029983
Iteration 15/1000 | Loss: 0.00033974
Iteration 16/1000 | Loss: 0.00042720
Iteration 17/1000 | Loss: 0.00037018
Iteration 18/1000 | Loss: 0.00018090
Iteration 19/1000 | Loss: 0.00021675
Iteration 20/1000 | Loss: 0.00013912
Iteration 21/1000 | Loss: 0.00025311
Iteration 22/1000 | Loss: 0.00022016
Iteration 23/1000 | Loss: 0.00022553
Iteration 24/1000 | Loss: 0.00019003
Iteration 25/1000 | Loss: 0.00034759
Iteration 26/1000 | Loss: 0.00024952
Iteration 27/1000 | Loss: 0.00010010
Iteration 28/1000 | Loss: 0.00009467
Iteration 29/1000 | Loss: 0.00010267
Iteration 30/1000 | Loss: 0.00015097
Iteration 31/1000 | Loss: 0.00025804
Iteration 32/1000 | Loss: 0.00025915
Iteration 33/1000 | Loss: 0.00032164
Iteration 34/1000 | Loss: 0.00027334
Iteration 35/1000 | Loss: 0.00022213
Iteration 36/1000 | Loss: 0.00021504
Iteration 37/1000 | Loss: 0.00013206
Iteration 38/1000 | Loss: 0.00016956
Iteration 39/1000 | Loss: 0.00040567
Iteration 40/1000 | Loss: 0.00029011
Iteration 41/1000 | Loss: 0.00028235
Iteration 42/1000 | Loss: 0.00023217
Iteration 43/1000 | Loss: 0.00020571
Iteration 44/1000 | Loss: 0.00024955
Iteration 45/1000 | Loss: 0.00027698
Iteration 46/1000 | Loss: 0.00016911
Iteration 47/1000 | Loss: 0.00032047
Iteration 48/1000 | Loss: 0.00029054
Iteration 49/1000 | Loss: 0.00029992
Iteration 50/1000 | Loss: 0.00007349
Iteration 51/1000 | Loss: 0.00006834
Iteration 52/1000 | Loss: 0.00028781
Iteration 53/1000 | Loss: 0.00008138
Iteration 54/1000 | Loss: 0.00007273
Iteration 55/1000 | Loss: 0.00021038
Iteration 56/1000 | Loss: 0.00026604
Iteration 57/1000 | Loss: 0.00031080
Iteration 58/1000 | Loss: 0.00024431
Iteration 59/1000 | Loss: 0.00031262
Iteration 60/1000 | Loss: 0.00018478
Iteration 61/1000 | Loss: 0.00020616
Iteration 62/1000 | Loss: 0.00015521
Iteration 63/1000 | Loss: 0.00028357
Iteration 64/1000 | Loss: 0.00013106
Iteration 65/1000 | Loss: 0.00018807
Iteration 66/1000 | Loss: 0.00010266
Iteration 67/1000 | Loss: 0.00017591
Iteration 68/1000 | Loss: 0.00024457
Iteration 69/1000 | Loss: 0.00011886
Iteration 70/1000 | Loss: 0.00020331
Iteration 71/1000 | Loss: 0.00022086
Iteration 72/1000 | Loss: 0.00012332
Iteration 73/1000 | Loss: 0.00011055
Iteration 74/1000 | Loss: 0.00010432
Iteration 75/1000 | Loss: 0.00008319
Iteration 76/1000 | Loss: 0.00015622
Iteration 77/1000 | Loss: 0.00012168
Iteration 78/1000 | Loss: 0.00027013
Iteration 79/1000 | Loss: 0.00020135
Iteration 80/1000 | Loss: 0.00033000
Iteration 81/1000 | Loss: 0.00018253
Iteration 82/1000 | Loss: 0.00033175
Iteration 83/1000 | Loss: 0.00030680
Iteration 84/1000 | Loss: 0.00013452
Iteration 85/1000 | Loss: 0.00027589
Iteration 86/1000 | Loss: 0.00031968
Iteration 87/1000 | Loss: 0.00012748
Iteration 88/1000 | Loss: 0.00011756
Iteration 89/1000 | Loss: 0.00013336
Iteration 90/1000 | Loss: 0.00010787
Iteration 91/1000 | Loss: 0.00014809
Iteration 92/1000 | Loss: 0.00007449
Iteration 93/1000 | Loss: 0.00033964
Iteration 94/1000 | Loss: 0.00013604
Iteration 95/1000 | Loss: 0.00012248
Iteration 96/1000 | Loss: 0.00012495
Iteration 97/1000 | Loss: 0.00007033
Iteration 98/1000 | Loss: 0.00011228
Iteration 99/1000 | Loss: 0.00008827
Iteration 100/1000 | Loss: 0.00005926
Iteration 101/1000 | Loss: 0.00005768
Iteration 102/1000 | Loss: 0.00005686
Iteration 103/1000 | Loss: 0.00005611
Iteration 104/1000 | Loss: 0.00017551
Iteration 105/1000 | Loss: 0.00011165
Iteration 106/1000 | Loss: 0.00014294
Iteration 107/1000 | Loss: 0.00005562
Iteration 108/1000 | Loss: 0.00016736
Iteration 109/1000 | Loss: 0.00009273
Iteration 110/1000 | Loss: 0.00011982
Iteration 111/1000 | Loss: 0.00011215
Iteration 112/1000 | Loss: 0.00014434
Iteration 113/1000 | Loss: 0.00010099
Iteration 114/1000 | Loss: 0.00011030
Iteration 115/1000 | Loss: 0.00012561
Iteration 116/1000 | Loss: 0.00005749
Iteration 117/1000 | Loss: 0.00013209
Iteration 118/1000 | Loss: 0.00008826
Iteration 119/1000 | Loss: 0.00010879
Iteration 120/1000 | Loss: 0.00008687
Iteration 121/1000 | Loss: 0.00009374
Iteration 122/1000 | Loss: 0.00015565
Iteration 123/1000 | Loss: 0.00015028
Iteration 124/1000 | Loss: 0.00007203
Iteration 125/1000 | Loss: 0.00016131
Iteration 126/1000 | Loss: 0.00013792
Iteration 127/1000 | Loss: 0.00005655
Iteration 128/1000 | Loss: 0.00016656
Iteration 129/1000 | Loss: 0.00015852
Iteration 130/1000 | Loss: 0.00005598
Iteration 131/1000 | Loss: 0.00012933
Iteration 132/1000 | Loss: 0.00006530
Iteration 133/1000 | Loss: 0.00012464
Iteration 134/1000 | Loss: 0.00016019
Iteration 135/1000 | Loss: 0.00014426
Iteration 136/1000 | Loss: 0.00017415
Iteration 137/1000 | Loss: 0.00012870
Iteration 138/1000 | Loss: 0.00006313
Iteration 139/1000 | Loss: 0.00005965
Iteration 140/1000 | Loss: 0.00020773
Iteration 141/1000 | Loss: 0.00009085
Iteration 142/1000 | Loss: 0.00015655
Iteration 143/1000 | Loss: 0.00021283
Iteration 144/1000 | Loss: 0.00012508
Iteration 145/1000 | Loss: 0.00005668
Iteration 146/1000 | Loss: 0.00005554
Iteration 147/1000 | Loss: 0.00005511
Iteration 148/1000 | Loss: 0.00005473
Iteration 149/1000 | Loss: 0.00005465
Iteration 150/1000 | Loss: 0.00005450
Iteration 151/1000 | Loss: 0.00005417
Iteration 152/1000 | Loss: 0.00005388
Iteration 153/1000 | Loss: 0.00005364
Iteration 154/1000 | Loss: 0.00006318
Iteration 155/1000 | Loss: 0.00005421
Iteration 156/1000 | Loss: 0.00005355
Iteration 157/1000 | Loss: 0.00005306
Iteration 158/1000 | Loss: 0.00005285
Iteration 159/1000 | Loss: 0.00005283
Iteration 160/1000 | Loss: 0.00005278
Iteration 161/1000 | Loss: 0.00005277
Iteration 162/1000 | Loss: 0.00005276
Iteration 163/1000 | Loss: 0.00005276
Iteration 164/1000 | Loss: 0.00005274
Iteration 165/1000 | Loss: 0.00005274
Iteration 166/1000 | Loss: 0.00005274
Iteration 167/1000 | Loss: 0.00005274
Iteration 168/1000 | Loss: 0.00005273
Iteration 169/1000 | Loss: 0.00005273
Iteration 170/1000 | Loss: 0.00005273
Iteration 171/1000 | Loss: 0.00005272
Iteration 172/1000 | Loss: 0.00005272
Iteration 173/1000 | Loss: 0.00005272
Iteration 174/1000 | Loss: 0.00005271
Iteration 175/1000 | Loss: 0.00005271
Iteration 176/1000 | Loss: 0.00005271
Iteration 177/1000 | Loss: 0.00005271
Iteration 178/1000 | Loss: 0.00005271
Iteration 179/1000 | Loss: 0.00005271
Iteration 180/1000 | Loss: 0.00005271
Iteration 181/1000 | Loss: 0.00005270
Iteration 182/1000 | Loss: 0.00005270
Iteration 183/1000 | Loss: 0.00005270
Iteration 184/1000 | Loss: 0.00005270
Iteration 185/1000 | Loss: 0.00005269
Iteration 186/1000 | Loss: 0.00005269
Iteration 187/1000 | Loss: 0.00005269
Iteration 188/1000 | Loss: 0.00005269
Iteration 189/1000 | Loss: 0.00005269
Iteration 190/1000 | Loss: 0.00005268
Iteration 191/1000 | Loss: 0.00005268
Iteration 192/1000 | Loss: 0.00005268
Iteration 193/1000 | Loss: 0.00005267
Iteration 194/1000 | Loss: 0.00005267
Iteration 195/1000 | Loss: 0.00005267
Iteration 196/1000 | Loss: 0.00005266
Iteration 197/1000 | Loss: 0.00005265
Iteration 198/1000 | Loss: 0.00005265
Iteration 199/1000 | Loss: 0.00005265
Iteration 200/1000 | Loss: 0.00005264
Iteration 201/1000 | Loss: 0.00005264
Iteration 202/1000 | Loss: 0.00005264
Iteration 203/1000 | Loss: 0.00005264
Iteration 204/1000 | Loss: 0.00005264
Iteration 205/1000 | Loss: 0.00005264
Iteration 206/1000 | Loss: 0.00005263
Iteration 207/1000 | Loss: 0.00005262
Iteration 208/1000 | Loss: 0.00005262
Iteration 209/1000 | Loss: 0.00005262
Iteration 210/1000 | Loss: 0.00005261
Iteration 211/1000 | Loss: 0.00005261
Iteration 212/1000 | Loss: 0.00005261
Iteration 213/1000 | Loss: 0.00005261
Iteration 214/1000 | Loss: 0.00005261
Iteration 215/1000 | Loss: 0.00005260
Iteration 216/1000 | Loss: 0.00005260
Iteration 217/1000 | Loss: 0.00005260
Iteration 218/1000 | Loss: 0.00005260
Iteration 219/1000 | Loss: 0.00005260
Iteration 220/1000 | Loss: 0.00005260
Iteration 221/1000 | Loss: 0.00005260
Iteration 222/1000 | Loss: 0.00005260
Iteration 223/1000 | Loss: 0.00005259
Iteration 224/1000 | Loss: 0.00005259
Iteration 225/1000 | Loss: 0.00005259
Iteration 226/1000 | Loss: 0.00005259
Iteration 227/1000 | Loss: 0.00005259
Iteration 228/1000 | Loss: 0.00005259
Iteration 229/1000 | Loss: 0.00005259
Iteration 230/1000 | Loss: 0.00005259
Iteration 231/1000 | Loss: 0.00005259
Iteration 232/1000 | Loss: 0.00005258
Iteration 233/1000 | Loss: 0.00005258
Iteration 234/1000 | Loss: 0.00005258
Iteration 235/1000 | Loss: 0.00005258
Iteration 236/1000 | Loss: 0.00005258
Iteration 237/1000 | Loss: 0.00005258
Iteration 238/1000 | Loss: 0.00005258
Iteration 239/1000 | Loss: 0.00005258
Iteration 240/1000 | Loss: 0.00005258
Iteration 241/1000 | Loss: 0.00005257
Iteration 242/1000 | Loss: 0.00005257
Iteration 243/1000 | Loss: 0.00005257
Iteration 244/1000 | Loss: 0.00005257
Iteration 245/1000 | Loss: 0.00005257
Iteration 246/1000 | Loss: 0.00005257
Iteration 247/1000 | Loss: 0.00005257
Iteration 248/1000 | Loss: 0.00005257
Iteration 249/1000 | Loss: 0.00005257
Iteration 250/1000 | Loss: 0.00005257
Iteration 251/1000 | Loss: 0.00005257
Iteration 252/1000 | Loss: 0.00005257
Iteration 253/1000 | Loss: 0.00005257
Iteration 254/1000 | Loss: 0.00005257
Iteration 255/1000 | Loss: 0.00005257
Iteration 256/1000 | Loss: 0.00005257
Iteration 257/1000 | Loss: 0.00005256
Iteration 258/1000 | Loss: 0.00005256
Iteration 259/1000 | Loss: 0.00005256
Iteration 260/1000 | Loss: 0.00005256
Iteration 261/1000 | Loss: 0.00005256
Iteration 262/1000 | Loss: 0.00005256
Iteration 263/1000 | Loss: 0.00005255
Iteration 264/1000 | Loss: 0.00005255
Iteration 265/1000 | Loss: 0.00005255
Iteration 266/1000 | Loss: 0.00005255
Iteration 267/1000 | Loss: 0.00005254
Iteration 268/1000 | Loss: 0.00005254
Iteration 269/1000 | Loss: 0.00005254
Iteration 270/1000 | Loss: 0.00005254
Iteration 271/1000 | Loss: 0.00005254
Iteration 272/1000 | Loss: 0.00005253
Iteration 273/1000 | Loss: 0.00005253
Iteration 274/1000 | Loss: 0.00005253
Iteration 275/1000 | Loss: 0.00005253
Iteration 276/1000 | Loss: 0.00005253
Iteration 277/1000 | Loss: 0.00005253
Iteration 278/1000 | Loss: 0.00005252
Iteration 279/1000 | Loss: 0.00005252
Iteration 280/1000 | Loss: 0.00005252
Iteration 281/1000 | Loss: 0.00005252
Iteration 282/1000 | Loss: 0.00005252
Iteration 283/1000 | Loss: 0.00005252
Iteration 284/1000 | Loss: 0.00005252
Iteration 285/1000 | Loss: 0.00005252
Iteration 286/1000 | Loss: 0.00005252
Iteration 287/1000 | Loss: 0.00005252
Iteration 288/1000 | Loss: 0.00005252
Iteration 289/1000 | Loss: 0.00005251
Iteration 290/1000 | Loss: 0.00005251
Iteration 291/1000 | Loss: 0.00005251
Iteration 292/1000 | Loss: 0.00005251
Iteration 293/1000 | Loss: 0.00005251
Iteration 294/1000 | Loss: 0.00005251
Iteration 295/1000 | Loss: 0.00005251
Iteration 296/1000 | Loss: 0.00005251
Iteration 297/1000 | Loss: 0.00005251
Iteration 298/1000 | Loss: 0.00005251
Iteration 299/1000 | Loss: 0.00005251
Iteration 300/1000 | Loss: 0.00005251
Iteration 301/1000 | Loss: 0.00005251
Iteration 302/1000 | Loss: 0.00005251
Iteration 303/1000 | Loss: 0.00005251
Iteration 304/1000 | Loss: 0.00005251
Iteration 305/1000 | Loss: 0.00005251
Iteration 306/1000 | Loss: 0.00005251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [5.250719186733477e-05, 5.250719186733477e-05, 5.250719186733477e-05, 5.250719186733477e-05, 5.250719186733477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.250719186733477e-05

Optimization complete. Final v2v error: 6.000041961669922 mm

Highest mean error: 7.073814868927002 mm for frame 149

Lowest mean error: 5.013110637664795 mm for frame 111

Saving results

Total time: 294.5614652633667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057469
Iteration 2/25 | Loss: 0.00221393
Iteration 3/25 | Loss: 0.00199813
Iteration 4/25 | Loss: 0.00197701
Iteration 5/25 | Loss: 0.00196672
Iteration 6/25 | Loss: 0.00196488
Iteration 7/25 | Loss: 0.00196484
Iteration 8/25 | Loss: 0.00196482
Iteration 9/25 | Loss: 0.00196482
Iteration 10/25 | Loss: 0.00196482
Iteration 11/25 | Loss: 0.00196482
Iteration 12/25 | Loss: 0.00196482
Iteration 13/25 | Loss: 0.00196482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001964820548892021, 0.001964820548892021, 0.001964820548892021, 0.001964820548892021, 0.001964820548892021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001964820548892021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05119538
Iteration 2/25 | Loss: 0.00184416
Iteration 3/25 | Loss: 0.00184416
Iteration 4/25 | Loss: 0.00184415
Iteration 5/25 | Loss: 0.00184415
Iteration 6/25 | Loss: 0.00184415
Iteration 7/25 | Loss: 0.00184415
Iteration 8/25 | Loss: 0.00184415
Iteration 9/25 | Loss: 0.00184415
Iteration 10/25 | Loss: 0.00184415
Iteration 11/25 | Loss: 0.00184415
Iteration 12/25 | Loss: 0.00184415
Iteration 13/25 | Loss: 0.00184415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0018441539723426104, 0.0018441539723426104, 0.0018441539723426104, 0.0018441539723426104, 0.0018441539723426104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018441539723426104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184415
Iteration 2/1000 | Loss: 0.00017122
Iteration 3/1000 | Loss: 0.00009157
Iteration 4/1000 | Loss: 0.00007763
Iteration 5/1000 | Loss: 0.00007261
Iteration 6/1000 | Loss: 0.00006996
Iteration 7/1000 | Loss: 0.00006885
Iteration 8/1000 | Loss: 0.00006727
Iteration 9/1000 | Loss: 0.00006629
Iteration 10/1000 | Loss: 0.00006557
Iteration 11/1000 | Loss: 0.00006484
Iteration 12/1000 | Loss: 0.00006428
Iteration 13/1000 | Loss: 0.00006354
Iteration 14/1000 | Loss: 0.00006279
Iteration 15/1000 | Loss: 0.00006236
Iteration 16/1000 | Loss: 0.00006196
Iteration 17/1000 | Loss: 0.00006166
Iteration 18/1000 | Loss: 0.00006137
Iteration 19/1000 | Loss: 0.00006115
Iteration 20/1000 | Loss: 0.00006095
Iteration 21/1000 | Loss: 0.00006078
Iteration 22/1000 | Loss: 0.00006071
Iteration 23/1000 | Loss: 0.00006070
Iteration 24/1000 | Loss: 0.00006069
Iteration 25/1000 | Loss: 0.00006068
Iteration 26/1000 | Loss: 0.00006067
Iteration 27/1000 | Loss: 0.00006064
Iteration 28/1000 | Loss: 0.00006056
Iteration 29/1000 | Loss: 0.00006053
Iteration 30/1000 | Loss: 0.00006052
Iteration 31/1000 | Loss: 0.00006047
Iteration 32/1000 | Loss: 0.00006047
Iteration 33/1000 | Loss: 0.00006041
Iteration 34/1000 | Loss: 0.00006041
Iteration 35/1000 | Loss: 0.00006035
Iteration 36/1000 | Loss: 0.00006034
Iteration 37/1000 | Loss: 0.00006032
Iteration 38/1000 | Loss: 0.00006025
Iteration 39/1000 | Loss: 0.00006025
Iteration 40/1000 | Loss: 0.00006020
Iteration 41/1000 | Loss: 0.00006013
Iteration 42/1000 | Loss: 0.00006013
Iteration 43/1000 | Loss: 0.00006012
Iteration 44/1000 | Loss: 0.00006011
Iteration 45/1000 | Loss: 0.00006011
Iteration 46/1000 | Loss: 0.00006011
Iteration 47/1000 | Loss: 0.00006010
Iteration 48/1000 | Loss: 0.00006010
Iteration 49/1000 | Loss: 0.00006010
Iteration 50/1000 | Loss: 0.00006009
Iteration 51/1000 | Loss: 0.00006009
Iteration 52/1000 | Loss: 0.00006009
Iteration 53/1000 | Loss: 0.00006009
Iteration 54/1000 | Loss: 0.00006009
Iteration 55/1000 | Loss: 0.00006009
Iteration 56/1000 | Loss: 0.00006009
Iteration 57/1000 | Loss: 0.00006008
Iteration 58/1000 | Loss: 0.00006008
Iteration 59/1000 | Loss: 0.00006008
Iteration 60/1000 | Loss: 0.00006008
Iteration 61/1000 | Loss: 0.00006008
Iteration 62/1000 | Loss: 0.00006008
Iteration 63/1000 | Loss: 0.00006008
Iteration 64/1000 | Loss: 0.00006008
Iteration 65/1000 | Loss: 0.00006008
Iteration 66/1000 | Loss: 0.00006008
Iteration 67/1000 | Loss: 0.00006008
Iteration 68/1000 | Loss: 0.00006007
Iteration 69/1000 | Loss: 0.00006007
Iteration 70/1000 | Loss: 0.00006007
Iteration 71/1000 | Loss: 0.00006007
Iteration 72/1000 | Loss: 0.00006007
Iteration 73/1000 | Loss: 0.00006007
Iteration 74/1000 | Loss: 0.00006007
Iteration 75/1000 | Loss: 0.00006007
Iteration 76/1000 | Loss: 0.00006007
Iteration 77/1000 | Loss: 0.00006007
Iteration 78/1000 | Loss: 0.00006006
Iteration 79/1000 | Loss: 0.00006006
Iteration 80/1000 | Loss: 0.00006006
Iteration 81/1000 | Loss: 0.00006006
Iteration 82/1000 | Loss: 0.00006006
Iteration 83/1000 | Loss: 0.00006006
Iteration 84/1000 | Loss: 0.00006006
Iteration 85/1000 | Loss: 0.00006006
Iteration 86/1000 | Loss: 0.00006006
Iteration 87/1000 | Loss: 0.00006005
Iteration 88/1000 | Loss: 0.00006005
Iteration 89/1000 | Loss: 0.00006005
Iteration 90/1000 | Loss: 0.00006004
Iteration 91/1000 | Loss: 0.00006004
Iteration 92/1000 | Loss: 0.00006004
Iteration 93/1000 | Loss: 0.00006004
Iteration 94/1000 | Loss: 0.00006004
Iteration 95/1000 | Loss: 0.00006004
Iteration 96/1000 | Loss: 0.00006004
Iteration 97/1000 | Loss: 0.00006003
Iteration 98/1000 | Loss: 0.00006003
Iteration 99/1000 | Loss: 0.00006003
Iteration 100/1000 | Loss: 0.00006003
Iteration 101/1000 | Loss: 0.00006003
Iteration 102/1000 | Loss: 0.00006003
Iteration 103/1000 | Loss: 0.00006003
Iteration 104/1000 | Loss: 0.00006003
Iteration 105/1000 | Loss: 0.00006003
Iteration 106/1000 | Loss: 0.00006002
Iteration 107/1000 | Loss: 0.00006002
Iteration 108/1000 | Loss: 0.00006002
Iteration 109/1000 | Loss: 0.00006002
Iteration 110/1000 | Loss: 0.00006002
Iteration 111/1000 | Loss: 0.00006002
Iteration 112/1000 | Loss: 0.00006002
Iteration 113/1000 | Loss: 0.00006002
Iteration 114/1000 | Loss: 0.00006002
Iteration 115/1000 | Loss: 0.00006002
Iteration 116/1000 | Loss: 0.00006002
Iteration 117/1000 | Loss: 0.00006002
Iteration 118/1000 | Loss: 0.00006001
Iteration 119/1000 | Loss: 0.00006001
Iteration 120/1000 | Loss: 0.00006001
Iteration 121/1000 | Loss: 0.00006001
Iteration 122/1000 | Loss: 0.00006001
Iteration 123/1000 | Loss: 0.00006000
Iteration 124/1000 | Loss: 0.00006000
Iteration 125/1000 | Loss: 0.00006000
Iteration 126/1000 | Loss: 0.00006000
Iteration 127/1000 | Loss: 0.00006000
Iteration 128/1000 | Loss: 0.00006000
Iteration 129/1000 | Loss: 0.00006000
Iteration 130/1000 | Loss: 0.00006000
Iteration 131/1000 | Loss: 0.00006000
Iteration 132/1000 | Loss: 0.00006000
Iteration 133/1000 | Loss: 0.00006000
Iteration 134/1000 | Loss: 0.00005999
Iteration 135/1000 | Loss: 0.00005999
Iteration 136/1000 | Loss: 0.00005999
Iteration 137/1000 | Loss: 0.00005999
Iteration 138/1000 | Loss: 0.00005999
Iteration 139/1000 | Loss: 0.00005999
Iteration 140/1000 | Loss: 0.00005999
Iteration 141/1000 | Loss: 0.00005999
Iteration 142/1000 | Loss: 0.00005999
Iteration 143/1000 | Loss: 0.00005999
Iteration 144/1000 | Loss: 0.00005999
Iteration 145/1000 | Loss: 0.00005999
Iteration 146/1000 | Loss: 0.00005999
Iteration 147/1000 | Loss: 0.00005999
Iteration 148/1000 | Loss: 0.00005998
Iteration 149/1000 | Loss: 0.00005998
Iteration 150/1000 | Loss: 0.00005998
Iteration 151/1000 | Loss: 0.00005998
Iteration 152/1000 | Loss: 0.00005998
Iteration 153/1000 | Loss: 0.00005997
Iteration 154/1000 | Loss: 0.00005997
Iteration 155/1000 | Loss: 0.00005997
Iteration 156/1000 | Loss: 0.00005997
Iteration 157/1000 | Loss: 0.00005996
Iteration 158/1000 | Loss: 0.00005996
Iteration 159/1000 | Loss: 0.00005996
Iteration 160/1000 | Loss: 0.00005996
Iteration 161/1000 | Loss: 0.00005996
Iteration 162/1000 | Loss: 0.00005996
Iteration 163/1000 | Loss: 0.00005995
Iteration 164/1000 | Loss: 0.00005995
Iteration 165/1000 | Loss: 0.00005995
Iteration 166/1000 | Loss: 0.00005995
Iteration 167/1000 | Loss: 0.00005995
Iteration 168/1000 | Loss: 0.00005995
Iteration 169/1000 | Loss: 0.00005995
Iteration 170/1000 | Loss: 0.00005995
Iteration 171/1000 | Loss: 0.00005995
Iteration 172/1000 | Loss: 0.00005994
Iteration 173/1000 | Loss: 0.00005994
Iteration 174/1000 | Loss: 0.00005994
Iteration 175/1000 | Loss: 0.00005994
Iteration 176/1000 | Loss: 0.00005994
Iteration 177/1000 | Loss: 0.00005993
Iteration 178/1000 | Loss: 0.00005993
Iteration 179/1000 | Loss: 0.00005993
Iteration 180/1000 | Loss: 0.00005993
Iteration 181/1000 | Loss: 0.00005993
Iteration 182/1000 | Loss: 0.00005993
Iteration 183/1000 | Loss: 0.00005993
Iteration 184/1000 | Loss: 0.00005993
Iteration 185/1000 | Loss: 0.00005993
Iteration 186/1000 | Loss: 0.00005993
Iteration 187/1000 | Loss: 0.00005993
Iteration 188/1000 | Loss: 0.00005993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [5.992849401081912e-05, 5.992849401081912e-05, 5.992849401081912e-05, 5.992849401081912e-05, 5.992849401081912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.992849401081912e-05

Optimization complete. Final v2v error: 6.467326641082764 mm

Highest mean error: 7.100159645080566 mm for frame 28

Lowest mean error: 5.809958457946777 mm for frame 172

Saving results

Total time: 57.44566035270691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01162693
Iteration 2/25 | Loss: 0.00240361
Iteration 3/25 | Loss: 0.00205406
Iteration 4/25 | Loss: 0.00201743
Iteration 5/25 | Loss: 0.00201006
Iteration 6/25 | Loss: 0.00200926
Iteration 7/25 | Loss: 0.00200926
Iteration 8/25 | Loss: 0.00200926
Iteration 9/25 | Loss: 0.00200926
Iteration 10/25 | Loss: 0.00200926
Iteration 11/25 | Loss: 0.00200926
Iteration 12/25 | Loss: 0.00200926
Iteration 13/25 | Loss: 0.00200926
Iteration 14/25 | Loss: 0.00200926
Iteration 15/25 | Loss: 0.00200926
Iteration 16/25 | Loss: 0.00200926
Iteration 17/25 | Loss: 0.00200926
Iteration 18/25 | Loss: 0.00200926
Iteration 19/25 | Loss: 0.00200926
Iteration 20/25 | Loss: 0.00200926
Iteration 21/25 | Loss: 0.00200926
Iteration 22/25 | Loss: 0.00200926
Iteration 23/25 | Loss: 0.00200926
Iteration 24/25 | Loss: 0.00200926
Iteration 25/25 | Loss: 0.00200926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58134574
Iteration 2/25 | Loss: 0.00166019
Iteration 3/25 | Loss: 0.00166017
Iteration 4/25 | Loss: 0.00166017
Iteration 5/25 | Loss: 0.00166017
Iteration 6/25 | Loss: 0.00166017
Iteration 7/25 | Loss: 0.00166017
Iteration 8/25 | Loss: 0.00166017
Iteration 9/25 | Loss: 0.00166017
Iteration 10/25 | Loss: 0.00166017
Iteration 11/25 | Loss: 0.00166017
Iteration 12/25 | Loss: 0.00166017
Iteration 13/25 | Loss: 0.00166017
Iteration 14/25 | Loss: 0.00166017
Iteration 15/25 | Loss: 0.00166017
Iteration 16/25 | Loss: 0.00166017
Iteration 17/25 | Loss: 0.00166017
Iteration 18/25 | Loss: 0.00166017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016601686365902424, 0.0016601686365902424, 0.0016601686365902424, 0.0016601686365902424, 0.0016601686365902424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016601686365902424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166017
Iteration 2/1000 | Loss: 0.00012641
Iteration 3/1000 | Loss: 0.00007667
Iteration 4/1000 | Loss: 0.00006370
Iteration 5/1000 | Loss: 0.00005975
Iteration 6/1000 | Loss: 0.00005736
Iteration 7/1000 | Loss: 0.00005552
Iteration 8/1000 | Loss: 0.00005399
Iteration 9/1000 | Loss: 0.00005277
Iteration 10/1000 | Loss: 0.00005194
Iteration 11/1000 | Loss: 0.00005116
Iteration 12/1000 | Loss: 0.00005060
Iteration 13/1000 | Loss: 0.00005017
Iteration 14/1000 | Loss: 0.00004992
Iteration 15/1000 | Loss: 0.00004970
Iteration 16/1000 | Loss: 0.00004953
Iteration 17/1000 | Loss: 0.00004950
Iteration 18/1000 | Loss: 0.00004937
Iteration 19/1000 | Loss: 0.00004934
Iteration 20/1000 | Loss: 0.00004934
Iteration 21/1000 | Loss: 0.00004930
Iteration 22/1000 | Loss: 0.00004925
Iteration 23/1000 | Loss: 0.00004918
Iteration 24/1000 | Loss: 0.00004915
Iteration 25/1000 | Loss: 0.00004915
Iteration 26/1000 | Loss: 0.00004914
Iteration 27/1000 | Loss: 0.00004913
Iteration 28/1000 | Loss: 0.00004913
Iteration 29/1000 | Loss: 0.00004913
Iteration 30/1000 | Loss: 0.00004911
Iteration 31/1000 | Loss: 0.00004909
Iteration 32/1000 | Loss: 0.00004909
Iteration 33/1000 | Loss: 0.00004908
Iteration 34/1000 | Loss: 0.00004908
Iteration 35/1000 | Loss: 0.00004908
Iteration 36/1000 | Loss: 0.00004908
Iteration 37/1000 | Loss: 0.00004907
Iteration 38/1000 | Loss: 0.00004906
Iteration 39/1000 | Loss: 0.00004906
Iteration 40/1000 | Loss: 0.00004906
Iteration 41/1000 | Loss: 0.00004906
Iteration 42/1000 | Loss: 0.00004906
Iteration 43/1000 | Loss: 0.00004906
Iteration 44/1000 | Loss: 0.00004905
Iteration 45/1000 | Loss: 0.00004903
Iteration 46/1000 | Loss: 0.00004903
Iteration 47/1000 | Loss: 0.00004903
Iteration 48/1000 | Loss: 0.00004903
Iteration 49/1000 | Loss: 0.00004903
Iteration 50/1000 | Loss: 0.00004903
Iteration 51/1000 | Loss: 0.00004903
Iteration 52/1000 | Loss: 0.00004903
Iteration 53/1000 | Loss: 0.00004903
Iteration 54/1000 | Loss: 0.00004903
Iteration 55/1000 | Loss: 0.00004903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [4.9029480578610674e-05, 4.9029480578610674e-05, 4.9029480578610674e-05, 4.9029480578610674e-05, 4.9029480578610674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9029480578610674e-05

Optimization complete. Final v2v error: 5.841350078582764 mm

Highest mean error: 6.855803489685059 mm for frame 196

Lowest mean error: 5.120030879974365 mm for frame 82

Saving results

Total time: 44.086604833602905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947229
Iteration 2/25 | Loss: 0.00421038
Iteration 3/25 | Loss: 0.00358505
Iteration 4/25 | Loss: 0.00349103
Iteration 5/25 | Loss: 0.00333964
Iteration 6/25 | Loss: 0.00326717
Iteration 7/25 | Loss: 0.00323035
Iteration 8/25 | Loss: 0.00308919
Iteration 9/25 | Loss: 0.00313964
Iteration 10/25 | Loss: 0.00313221
Iteration 11/25 | Loss: 0.00301713
Iteration 12/25 | Loss: 0.00298655
Iteration 13/25 | Loss: 0.00289168
Iteration 14/25 | Loss: 0.00285053
Iteration 15/25 | Loss: 0.00271767
Iteration 16/25 | Loss: 0.00267548
Iteration 17/25 | Loss: 0.00266417
Iteration 18/25 | Loss: 0.00264215
Iteration 19/25 | Loss: 0.00263561
Iteration 20/25 | Loss: 0.00265018
Iteration 21/25 | Loss: 0.00262874
Iteration 22/25 | Loss: 0.00262293
Iteration 23/25 | Loss: 0.00261945
Iteration 24/25 | Loss: 0.00258131
Iteration 25/25 | Loss: 0.00257969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13475251
Iteration 2/25 | Loss: 0.00530240
Iteration 3/25 | Loss: 0.00530236
Iteration 4/25 | Loss: 0.00530236
Iteration 5/25 | Loss: 0.00530235
Iteration 6/25 | Loss: 0.00530235
Iteration 7/25 | Loss: 0.00530235
Iteration 8/25 | Loss: 0.00530235
Iteration 9/25 | Loss: 0.00530235
Iteration 10/25 | Loss: 0.00530235
Iteration 11/25 | Loss: 0.00530235
Iteration 12/25 | Loss: 0.00530235
Iteration 13/25 | Loss: 0.00530235
Iteration 14/25 | Loss: 0.00530235
Iteration 15/25 | Loss: 0.00530235
Iteration 16/25 | Loss: 0.00530235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005302351899445057, 0.005302351899445057, 0.005302351899445057, 0.005302351899445057, 0.005302351899445057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005302351899445057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00530235
Iteration 2/1000 | Loss: 0.00169706
Iteration 3/1000 | Loss: 0.00299064
Iteration 4/1000 | Loss: 0.00027647
Iteration 5/1000 | Loss: 0.00155901
Iteration 6/1000 | Loss: 0.00023470
Iteration 7/1000 | Loss: 0.00045456
Iteration 8/1000 | Loss: 0.00035173
Iteration 9/1000 | Loss: 0.00031603
Iteration 10/1000 | Loss: 0.00020387
Iteration 11/1000 | Loss: 0.00045764
Iteration 12/1000 | Loss: 0.00018665
Iteration 13/1000 | Loss: 0.00018107
Iteration 14/1000 | Loss: 0.00032780
Iteration 15/1000 | Loss: 0.00066973
Iteration 16/1000 | Loss: 0.00101908
Iteration 17/1000 | Loss: 0.00093049
Iteration 18/1000 | Loss: 0.00052270
Iteration 19/1000 | Loss: 0.00059366
Iteration 20/1000 | Loss: 0.00035497
Iteration 21/1000 | Loss: 0.00018516
Iteration 22/1000 | Loss: 0.00017564
Iteration 23/1000 | Loss: 0.00017049
Iteration 24/1000 | Loss: 0.00031133
Iteration 25/1000 | Loss: 0.00154364
Iteration 26/1000 | Loss: 0.00078868
Iteration 27/1000 | Loss: 0.00138767
Iteration 28/1000 | Loss: 0.00096678
Iteration 29/1000 | Loss: 0.00092429
Iteration 30/1000 | Loss: 0.00090246
Iteration 31/1000 | Loss: 0.00083583
Iteration 32/1000 | Loss: 0.00087131
Iteration 33/1000 | Loss: 0.00054741
Iteration 34/1000 | Loss: 0.00046310
Iteration 35/1000 | Loss: 0.00042862
Iteration 36/1000 | Loss: 0.00046553
Iteration 37/1000 | Loss: 0.00017965
Iteration 38/1000 | Loss: 0.00033079
Iteration 39/1000 | Loss: 0.00065162
Iteration 40/1000 | Loss: 0.00054923
Iteration 41/1000 | Loss: 0.00094357
Iteration 42/1000 | Loss: 0.00062078
Iteration 43/1000 | Loss: 0.00041141
Iteration 44/1000 | Loss: 0.00026463
Iteration 45/1000 | Loss: 0.00020978
Iteration 46/1000 | Loss: 0.00017953
Iteration 47/1000 | Loss: 0.00057386
Iteration 48/1000 | Loss: 0.00045547
Iteration 49/1000 | Loss: 0.00029031
Iteration 50/1000 | Loss: 0.00016970
Iteration 51/1000 | Loss: 0.00036556
Iteration 52/1000 | Loss: 0.00037719
Iteration 53/1000 | Loss: 0.00035327
Iteration 54/1000 | Loss: 0.00036189
Iteration 55/1000 | Loss: 0.00034457
Iteration 56/1000 | Loss: 0.00016523
Iteration 57/1000 | Loss: 0.00015946
Iteration 58/1000 | Loss: 0.00028531
Iteration 59/1000 | Loss: 0.00024145
Iteration 60/1000 | Loss: 0.00024986
Iteration 61/1000 | Loss: 0.00015666
Iteration 62/1000 | Loss: 0.00015078
Iteration 63/1000 | Loss: 0.00014720
Iteration 64/1000 | Loss: 0.00033758
Iteration 65/1000 | Loss: 0.00014729
Iteration 66/1000 | Loss: 0.00014474
Iteration 67/1000 | Loss: 0.00027142
Iteration 68/1000 | Loss: 0.00034461
Iteration 69/1000 | Loss: 0.00035320
Iteration 70/1000 | Loss: 0.00033562
Iteration 71/1000 | Loss: 0.00016671
Iteration 72/1000 | Loss: 0.00015370
Iteration 73/1000 | Loss: 0.00020277
Iteration 74/1000 | Loss: 0.00015331
Iteration 75/1000 | Loss: 0.00014730
Iteration 76/1000 | Loss: 0.00014399
Iteration 77/1000 | Loss: 0.00014136
Iteration 78/1000 | Loss: 0.00013977
Iteration 79/1000 | Loss: 0.00014981
Iteration 80/1000 | Loss: 0.00028349
Iteration 81/1000 | Loss: 0.00014634
Iteration 82/1000 | Loss: 0.00032589
Iteration 83/1000 | Loss: 0.00015371
Iteration 84/1000 | Loss: 0.00029180
Iteration 85/1000 | Loss: 0.00015302
Iteration 86/1000 | Loss: 0.00019467
Iteration 87/1000 | Loss: 0.00021596
Iteration 88/1000 | Loss: 0.00013979
Iteration 89/1000 | Loss: 0.00013687
Iteration 90/1000 | Loss: 0.00013530
Iteration 91/1000 | Loss: 0.00013389
Iteration 92/1000 | Loss: 0.00013315
Iteration 93/1000 | Loss: 0.00013235
Iteration 94/1000 | Loss: 0.00013126
Iteration 95/1000 | Loss: 0.00013009
Iteration 96/1000 | Loss: 0.00012920
Iteration 97/1000 | Loss: 0.00026905
Iteration 98/1000 | Loss: 0.00026898
Iteration 99/1000 | Loss: 0.00217072
Iteration 100/1000 | Loss: 0.00130035
Iteration 101/1000 | Loss: 0.00138099
Iteration 102/1000 | Loss: 0.00080295
Iteration 103/1000 | Loss: 0.00066955
Iteration 104/1000 | Loss: 0.00259119
Iteration 105/1000 | Loss: 0.00199320
Iteration 106/1000 | Loss: 0.00243401
Iteration 107/1000 | Loss: 0.00120962
Iteration 108/1000 | Loss: 0.00143905
Iteration 109/1000 | Loss: 0.00109019
Iteration 110/1000 | Loss: 0.00103687
Iteration 111/1000 | Loss: 0.00095462
Iteration 112/1000 | Loss: 0.00145956
Iteration 113/1000 | Loss: 0.00110410
Iteration 114/1000 | Loss: 0.00154354
Iteration 115/1000 | Loss: 0.00125863
Iteration 116/1000 | Loss: 0.00135056
Iteration 117/1000 | Loss: 0.00075818
Iteration 118/1000 | Loss: 0.00031366
Iteration 119/1000 | Loss: 0.00048930
Iteration 120/1000 | Loss: 0.00049952
Iteration 121/1000 | Loss: 0.00018870
Iteration 122/1000 | Loss: 0.00042712
Iteration 123/1000 | Loss: 0.00119372
Iteration 124/1000 | Loss: 0.00104046
Iteration 125/1000 | Loss: 0.00073484
Iteration 126/1000 | Loss: 0.00080957
Iteration 127/1000 | Loss: 0.00026102
Iteration 128/1000 | Loss: 0.00059713
Iteration 129/1000 | Loss: 0.00048425
Iteration 130/1000 | Loss: 0.00031450
Iteration 131/1000 | Loss: 0.00031223
Iteration 132/1000 | Loss: 0.00026582
Iteration 133/1000 | Loss: 0.00026088
Iteration 134/1000 | Loss: 0.00021884
Iteration 135/1000 | Loss: 0.00021361
Iteration 136/1000 | Loss: 0.00048055
Iteration 137/1000 | Loss: 0.00041402
Iteration 138/1000 | Loss: 0.00078171
Iteration 139/1000 | Loss: 0.00042619
Iteration 140/1000 | Loss: 0.00017757
Iteration 141/1000 | Loss: 0.00016407
Iteration 142/1000 | Loss: 0.00015741
Iteration 143/1000 | Loss: 0.00014215
Iteration 144/1000 | Loss: 0.00039332
Iteration 145/1000 | Loss: 0.00043725
Iteration 146/1000 | Loss: 0.00035373
Iteration 147/1000 | Loss: 0.00029299
Iteration 148/1000 | Loss: 0.00023151
Iteration 149/1000 | Loss: 0.00063836
Iteration 150/1000 | Loss: 0.00033154
Iteration 151/1000 | Loss: 0.00066020
Iteration 152/1000 | Loss: 0.00102227
Iteration 153/1000 | Loss: 0.00034770
Iteration 154/1000 | Loss: 0.00033068
Iteration 155/1000 | Loss: 0.00018931
Iteration 156/1000 | Loss: 0.00018720
Iteration 157/1000 | Loss: 0.00038942
Iteration 158/1000 | Loss: 0.00037566
Iteration 159/1000 | Loss: 0.00019426
Iteration 160/1000 | Loss: 0.00019871
Iteration 161/1000 | Loss: 0.00031344
Iteration 162/1000 | Loss: 0.00034977
Iteration 163/1000 | Loss: 0.00032677
Iteration 164/1000 | Loss: 0.00017050
Iteration 165/1000 | Loss: 0.00016049
Iteration 166/1000 | Loss: 0.00015469
Iteration 167/1000 | Loss: 0.00027639
Iteration 168/1000 | Loss: 0.00014200
Iteration 169/1000 | Loss: 0.00026575
Iteration 170/1000 | Loss: 0.00024616
Iteration 171/1000 | Loss: 0.00050302
Iteration 172/1000 | Loss: 0.00027926
Iteration 173/1000 | Loss: 0.00015547
Iteration 174/1000 | Loss: 0.00013436
Iteration 175/1000 | Loss: 0.00014404
Iteration 176/1000 | Loss: 0.00012899
Iteration 177/1000 | Loss: 0.00012617
Iteration 178/1000 | Loss: 0.00012548
Iteration 179/1000 | Loss: 0.00022711
Iteration 180/1000 | Loss: 0.00012987
Iteration 181/1000 | Loss: 0.00024188
Iteration 182/1000 | Loss: 0.00013430
Iteration 183/1000 | Loss: 0.00012800
Iteration 184/1000 | Loss: 0.00035703
Iteration 185/1000 | Loss: 0.00061439
Iteration 186/1000 | Loss: 0.00065216
Iteration 187/1000 | Loss: 0.00024669
Iteration 188/1000 | Loss: 0.00014038
Iteration 189/1000 | Loss: 0.00013131
Iteration 190/1000 | Loss: 0.00061280
Iteration 191/1000 | Loss: 0.00073281
Iteration 192/1000 | Loss: 0.00039771
Iteration 193/1000 | Loss: 0.00039084
Iteration 194/1000 | Loss: 0.00036426
Iteration 195/1000 | Loss: 0.00031710
Iteration 196/1000 | Loss: 0.00016832
Iteration 197/1000 | Loss: 0.00014569
Iteration 198/1000 | Loss: 0.00013782
Iteration 199/1000 | Loss: 0.00013138
Iteration 200/1000 | Loss: 0.00014215
Iteration 201/1000 | Loss: 0.00013543
Iteration 202/1000 | Loss: 0.00012645
Iteration 203/1000 | Loss: 0.00012130
Iteration 204/1000 | Loss: 0.00013716
Iteration 205/1000 | Loss: 0.00012292
Iteration 206/1000 | Loss: 0.00011991
Iteration 207/1000 | Loss: 0.00011872
Iteration 208/1000 | Loss: 0.00013642
Iteration 209/1000 | Loss: 0.00012160
Iteration 210/1000 | Loss: 0.00011914
Iteration 211/1000 | Loss: 0.00013187
Iteration 212/1000 | Loss: 0.00013782
Iteration 213/1000 | Loss: 0.00011860
Iteration 214/1000 | Loss: 0.00011750
Iteration 215/1000 | Loss: 0.00012614
Iteration 216/1000 | Loss: 0.00013839
Iteration 217/1000 | Loss: 0.00013149
Iteration 218/1000 | Loss: 0.00013717
Iteration 219/1000 | Loss: 0.00013201
Iteration 220/1000 | Loss: 0.00013748
Iteration 221/1000 | Loss: 0.00011511
Iteration 222/1000 | Loss: 0.00011439
Iteration 223/1000 | Loss: 0.00011368
Iteration 224/1000 | Loss: 0.00022755
Iteration 225/1000 | Loss: 0.00012207
Iteration 226/1000 | Loss: 0.00011760
Iteration 227/1000 | Loss: 0.00014653
Iteration 228/1000 | Loss: 0.00026231
Iteration 229/1000 | Loss: 0.00025420
Iteration 230/1000 | Loss: 0.00043741
Iteration 231/1000 | Loss: 0.00029808
Iteration 232/1000 | Loss: 0.00016762
Iteration 233/1000 | Loss: 0.00012491
Iteration 234/1000 | Loss: 0.00015113
Iteration 235/1000 | Loss: 0.00014071
Iteration 236/1000 | Loss: 0.00014561
Iteration 237/1000 | Loss: 0.00028520
Iteration 238/1000 | Loss: 0.00023954
Iteration 239/1000 | Loss: 0.00020864
Iteration 240/1000 | Loss: 0.00014849
Iteration 241/1000 | Loss: 0.00018076
Iteration 242/1000 | Loss: 0.00011842
Iteration 243/1000 | Loss: 0.00011533
Iteration 244/1000 | Loss: 0.00011384
Iteration 245/1000 | Loss: 0.00011283
Iteration 246/1000 | Loss: 0.00025164
Iteration 247/1000 | Loss: 0.00012344
Iteration 248/1000 | Loss: 0.00011716
Iteration 249/1000 | Loss: 0.00011494
Iteration 250/1000 | Loss: 0.00025259
Iteration 251/1000 | Loss: 0.00035500
Iteration 252/1000 | Loss: 0.00013954
Iteration 253/1000 | Loss: 0.00012280
Iteration 254/1000 | Loss: 0.00011765
Iteration 255/1000 | Loss: 0.00011647
Iteration 256/1000 | Loss: 0.00029947
Iteration 257/1000 | Loss: 0.00013378
Iteration 258/1000 | Loss: 0.00012121
Iteration 259/1000 | Loss: 0.00011762
Iteration 260/1000 | Loss: 0.00011588
Iteration 261/1000 | Loss: 0.00011482
Iteration 262/1000 | Loss: 0.00011369
Iteration 263/1000 | Loss: 0.00011272
Iteration 264/1000 | Loss: 0.00011227
Iteration 265/1000 | Loss: 0.00011179
Iteration 266/1000 | Loss: 0.00011135
Iteration 267/1000 | Loss: 0.00011085
Iteration 268/1000 | Loss: 0.00011042
Iteration 269/1000 | Loss: 0.00033869
Iteration 270/1000 | Loss: 0.00024410
Iteration 271/1000 | Loss: 0.00014136
Iteration 272/1000 | Loss: 0.00012707
Iteration 273/1000 | Loss: 0.00040447
Iteration 274/1000 | Loss: 0.00012470
Iteration 275/1000 | Loss: 0.00011810
Iteration 276/1000 | Loss: 0.00011535
Iteration 277/1000 | Loss: 0.00011386
Iteration 278/1000 | Loss: 0.00011279
Iteration 279/1000 | Loss: 0.00011205
Iteration 280/1000 | Loss: 0.00011143
Iteration 281/1000 | Loss: 0.00011105
Iteration 282/1000 | Loss: 0.00011067
Iteration 283/1000 | Loss: 0.00011035
Iteration 284/1000 | Loss: 0.00010994
Iteration 285/1000 | Loss: 0.00010953
Iteration 286/1000 | Loss: 0.00010920
Iteration 287/1000 | Loss: 0.00010885
Iteration 288/1000 | Loss: 0.00010856
Iteration 289/1000 | Loss: 0.00010833
Iteration 290/1000 | Loss: 0.00010817
Iteration 291/1000 | Loss: 0.00010815
Iteration 292/1000 | Loss: 0.00010798
Iteration 293/1000 | Loss: 0.00010789
Iteration 294/1000 | Loss: 0.00010771
Iteration 295/1000 | Loss: 0.00010756
Iteration 296/1000 | Loss: 0.00010755
Iteration 297/1000 | Loss: 0.00010753
Iteration 298/1000 | Loss: 0.00010751
Iteration 299/1000 | Loss: 0.00010748
Iteration 300/1000 | Loss: 0.00010747
Iteration 301/1000 | Loss: 0.00010745
Iteration 302/1000 | Loss: 0.00010741
Iteration 303/1000 | Loss: 0.00010741
Iteration 304/1000 | Loss: 0.00010741
Iteration 305/1000 | Loss: 0.00010741
Iteration 306/1000 | Loss: 0.00010741
Iteration 307/1000 | Loss: 0.00010741
Iteration 308/1000 | Loss: 0.00010739
Iteration 309/1000 | Loss: 0.00010739
Iteration 310/1000 | Loss: 0.00010739
Iteration 311/1000 | Loss: 0.00010739
Iteration 312/1000 | Loss: 0.00010739
Iteration 313/1000 | Loss: 0.00010739
Iteration 314/1000 | Loss: 0.00010739
Iteration 315/1000 | Loss: 0.00010739
Iteration 316/1000 | Loss: 0.00010739
Iteration 317/1000 | Loss: 0.00010739
Iteration 318/1000 | Loss: 0.00010738
Iteration 319/1000 | Loss: 0.00010738
Iteration 320/1000 | Loss: 0.00010738
Iteration 321/1000 | Loss: 0.00010738
Iteration 322/1000 | Loss: 0.00010738
Iteration 323/1000 | Loss: 0.00010738
Iteration 324/1000 | Loss: 0.00010738
Iteration 325/1000 | Loss: 0.00010737
Iteration 326/1000 | Loss: 0.00010737
Iteration 327/1000 | Loss: 0.00010737
Iteration 328/1000 | Loss: 0.00010737
Iteration 329/1000 | Loss: 0.00010737
Iteration 330/1000 | Loss: 0.00010736
Iteration 331/1000 | Loss: 0.00010736
Iteration 332/1000 | Loss: 0.00010736
Iteration 333/1000 | Loss: 0.00010735
Iteration 334/1000 | Loss: 0.00010735
Iteration 335/1000 | Loss: 0.00010735
Iteration 336/1000 | Loss: 0.00010735
Iteration 337/1000 | Loss: 0.00010735
Iteration 338/1000 | Loss: 0.00010734
Iteration 339/1000 | Loss: 0.00010734
Iteration 340/1000 | Loss: 0.00010733
Iteration 341/1000 | Loss: 0.00010733
Iteration 342/1000 | Loss: 0.00010733
Iteration 343/1000 | Loss: 0.00010733
Iteration 344/1000 | Loss: 0.00010733
Iteration 345/1000 | Loss: 0.00010733
Iteration 346/1000 | Loss: 0.00010733
Iteration 347/1000 | Loss: 0.00010733
Iteration 348/1000 | Loss: 0.00010733
Iteration 349/1000 | Loss: 0.00010732
Iteration 350/1000 | Loss: 0.00010732
Iteration 351/1000 | Loss: 0.00010732
Iteration 352/1000 | Loss: 0.00010732
Iteration 353/1000 | Loss: 0.00010732
Iteration 354/1000 | Loss: 0.00010732
Iteration 355/1000 | Loss: 0.00010732
Iteration 356/1000 | Loss: 0.00010731
Iteration 357/1000 | Loss: 0.00010731
Iteration 358/1000 | Loss: 0.00010731
Iteration 359/1000 | Loss: 0.00010731
Iteration 360/1000 | Loss: 0.00010731
Iteration 361/1000 | Loss: 0.00010731
Iteration 362/1000 | Loss: 0.00010731
Iteration 363/1000 | Loss: 0.00010731
Iteration 364/1000 | Loss: 0.00010730
Iteration 365/1000 | Loss: 0.00010730
Iteration 366/1000 | Loss: 0.00010730
Iteration 367/1000 | Loss: 0.00010730
Iteration 368/1000 | Loss: 0.00010730
Iteration 369/1000 | Loss: 0.00010729
Iteration 370/1000 | Loss: 0.00010729
Iteration 371/1000 | Loss: 0.00010729
Iteration 372/1000 | Loss: 0.00010729
Iteration 373/1000 | Loss: 0.00010729
Iteration 374/1000 | Loss: 0.00010728
Iteration 375/1000 | Loss: 0.00010728
Iteration 376/1000 | Loss: 0.00010728
Iteration 377/1000 | Loss: 0.00010727
Iteration 378/1000 | Loss: 0.00010727
Iteration 379/1000 | Loss: 0.00010726
Iteration 380/1000 | Loss: 0.00010726
Iteration 381/1000 | Loss: 0.00010725
Iteration 382/1000 | Loss: 0.00010725
Iteration 383/1000 | Loss: 0.00010725
Iteration 384/1000 | Loss: 0.00010724
Iteration 385/1000 | Loss: 0.00010724
Iteration 386/1000 | Loss: 0.00010724
Iteration 387/1000 | Loss: 0.00010724
Iteration 388/1000 | Loss: 0.00010723
Iteration 389/1000 | Loss: 0.00010723
Iteration 390/1000 | Loss: 0.00010723
Iteration 391/1000 | Loss: 0.00010723
Iteration 392/1000 | Loss: 0.00010723
Iteration 393/1000 | Loss: 0.00010723
Iteration 394/1000 | Loss: 0.00010722
Iteration 395/1000 | Loss: 0.00010722
Iteration 396/1000 | Loss: 0.00010722
Iteration 397/1000 | Loss: 0.00010722
Iteration 398/1000 | Loss: 0.00010722
Iteration 399/1000 | Loss: 0.00010722
Iteration 400/1000 | Loss: 0.00010722
Iteration 401/1000 | Loss: 0.00010722
Iteration 402/1000 | Loss: 0.00010722
Iteration 403/1000 | Loss: 0.00010722
Iteration 404/1000 | Loss: 0.00010721
Iteration 405/1000 | Loss: 0.00010721
Iteration 406/1000 | Loss: 0.00010721
Iteration 407/1000 | Loss: 0.00010721
Iteration 408/1000 | Loss: 0.00010721
Iteration 409/1000 | Loss: 0.00010721
Iteration 410/1000 | Loss: 0.00010721
Iteration 411/1000 | Loss: 0.00010721
Iteration 412/1000 | Loss: 0.00010721
Iteration 413/1000 | Loss: 0.00010721
Iteration 414/1000 | Loss: 0.00010720
Iteration 415/1000 | Loss: 0.00010720
Iteration 416/1000 | Loss: 0.00010720
Iteration 417/1000 | Loss: 0.00010720
Iteration 418/1000 | Loss: 0.00010720
Iteration 419/1000 | Loss: 0.00010720
Iteration 420/1000 | Loss: 0.00010720
Iteration 421/1000 | Loss: 0.00010720
Iteration 422/1000 | Loss: 0.00010720
Iteration 423/1000 | Loss: 0.00010720
Iteration 424/1000 | Loss: 0.00010720
Iteration 425/1000 | Loss: 0.00010720
Iteration 426/1000 | Loss: 0.00010719
Iteration 427/1000 | Loss: 0.00010719
Iteration 428/1000 | Loss: 0.00010719
Iteration 429/1000 | Loss: 0.00010719
Iteration 430/1000 | Loss: 0.00010719
Iteration 431/1000 | Loss: 0.00010719
Iteration 432/1000 | Loss: 0.00010719
Iteration 433/1000 | Loss: 0.00010719
Iteration 434/1000 | Loss: 0.00010719
Iteration 435/1000 | Loss: 0.00010719
Iteration 436/1000 | Loss: 0.00010718
Iteration 437/1000 | Loss: 0.00010718
Iteration 438/1000 | Loss: 0.00010718
Iteration 439/1000 | Loss: 0.00010718
Iteration 440/1000 | Loss: 0.00010718
Iteration 441/1000 | Loss: 0.00010718
Iteration 442/1000 | Loss: 0.00010718
Iteration 443/1000 | Loss: 0.00010718
Iteration 444/1000 | Loss: 0.00010718
Iteration 445/1000 | Loss: 0.00010718
Iteration 446/1000 | Loss: 0.00010718
Iteration 447/1000 | Loss: 0.00010717
Iteration 448/1000 | Loss: 0.00010717
Iteration 449/1000 | Loss: 0.00010717
Iteration 450/1000 | Loss: 0.00010717
Iteration 451/1000 | Loss: 0.00010717
Iteration 452/1000 | Loss: 0.00010717
Iteration 453/1000 | Loss: 0.00010717
Iteration 454/1000 | Loss: 0.00010717
Iteration 455/1000 | Loss: 0.00010717
Iteration 456/1000 | Loss: 0.00010717
Iteration 457/1000 | Loss: 0.00010717
Iteration 458/1000 | Loss: 0.00010717
Iteration 459/1000 | Loss: 0.00010717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 459. Stopping optimization.
Last 5 losses: [0.00010716893302742392, 0.00010716893302742392, 0.00010716893302742392, 0.00010716893302742392, 0.00010716893302742392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010716893302742392

Optimization complete. Final v2v error: 8.27375316619873 mm

Highest mean error: 10.979059219360352 mm for frame 147

Lowest mean error: 7.029889106750488 mm for frame 36

Saving results

Total time: 472.4271023273468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738167
Iteration 2/25 | Loss: 0.00218923
Iteration 3/25 | Loss: 0.00197200
Iteration 4/25 | Loss: 0.00194203
Iteration 5/25 | Loss: 0.00193345
Iteration 6/25 | Loss: 0.00192888
Iteration 7/25 | Loss: 0.00192638
Iteration 8/25 | Loss: 0.00192586
Iteration 9/25 | Loss: 0.00192556
Iteration 10/25 | Loss: 0.00192535
Iteration 11/25 | Loss: 0.00192534
Iteration 12/25 | Loss: 0.00192534
Iteration 13/25 | Loss: 0.00192534
Iteration 14/25 | Loss: 0.00192534
Iteration 15/25 | Loss: 0.00192534
Iteration 16/25 | Loss: 0.00192534
Iteration 17/25 | Loss: 0.00192534
Iteration 18/25 | Loss: 0.00192533
Iteration 19/25 | Loss: 0.00192533
Iteration 20/25 | Loss: 0.00192533
Iteration 21/25 | Loss: 0.00192533
Iteration 22/25 | Loss: 0.00192533
Iteration 23/25 | Loss: 0.00192533
Iteration 24/25 | Loss: 0.00192533
Iteration 25/25 | Loss: 0.00192533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18066573
Iteration 2/25 | Loss: 0.00150647
Iteration 3/25 | Loss: 0.00150628
Iteration 4/25 | Loss: 0.00150628
Iteration 5/25 | Loss: 0.00150628
Iteration 6/25 | Loss: 0.00150628
Iteration 7/25 | Loss: 0.00150628
Iteration 8/25 | Loss: 0.00150628
Iteration 9/25 | Loss: 0.00150628
Iteration 10/25 | Loss: 0.00150628
Iteration 11/25 | Loss: 0.00150628
Iteration 12/25 | Loss: 0.00150628
Iteration 13/25 | Loss: 0.00150628
Iteration 14/25 | Loss: 0.00150628
Iteration 15/25 | Loss: 0.00150628
Iteration 16/25 | Loss: 0.00150628
Iteration 17/25 | Loss: 0.00150628
Iteration 18/25 | Loss: 0.00150628
Iteration 19/25 | Loss: 0.00150628
Iteration 20/25 | Loss: 0.00150628
Iteration 21/25 | Loss: 0.00150628
Iteration 22/25 | Loss: 0.00150628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015062801539897919, 0.0015062801539897919, 0.0015062801539897919, 0.0015062801539897919, 0.0015062801539897919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015062801539897919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150628
Iteration 2/1000 | Loss: 0.00012486
Iteration 3/1000 | Loss: 0.00007802
Iteration 4/1000 | Loss: 0.00007003
Iteration 5/1000 | Loss: 0.00006583
Iteration 6/1000 | Loss: 0.00006315
Iteration 7/1000 | Loss: 0.00006112
Iteration 8/1000 | Loss: 0.00005923
Iteration 9/1000 | Loss: 0.00005707
Iteration 10/1000 | Loss: 0.00005578
Iteration 11/1000 | Loss: 0.00005462
Iteration 12/1000 | Loss: 0.00005372
Iteration 13/1000 | Loss: 0.00005300
Iteration 14/1000 | Loss: 0.00005253
Iteration 15/1000 | Loss: 0.00005207
Iteration 16/1000 | Loss: 0.00005172
Iteration 17/1000 | Loss: 0.00005141
Iteration 18/1000 | Loss: 0.00005125
Iteration 19/1000 | Loss: 0.00005112
Iteration 20/1000 | Loss: 0.00005103
Iteration 21/1000 | Loss: 0.00005095
Iteration 22/1000 | Loss: 0.00005094
Iteration 23/1000 | Loss: 0.00005080
Iteration 24/1000 | Loss: 0.00005078
Iteration 25/1000 | Loss: 0.00005077
Iteration 26/1000 | Loss: 0.00005077
Iteration 27/1000 | Loss: 0.00005076
Iteration 28/1000 | Loss: 0.00005074
Iteration 29/1000 | Loss: 0.00005074
Iteration 30/1000 | Loss: 0.00005074
Iteration 31/1000 | Loss: 0.00005073
Iteration 32/1000 | Loss: 0.00005073
Iteration 33/1000 | Loss: 0.00005072
Iteration 34/1000 | Loss: 0.00005072
Iteration 35/1000 | Loss: 0.00005072
Iteration 36/1000 | Loss: 0.00005071
Iteration 37/1000 | Loss: 0.00005071
Iteration 38/1000 | Loss: 0.00005071
Iteration 39/1000 | Loss: 0.00005071
Iteration 40/1000 | Loss: 0.00005070
Iteration 41/1000 | Loss: 0.00005070
Iteration 42/1000 | Loss: 0.00005070
Iteration 43/1000 | Loss: 0.00005069
Iteration 44/1000 | Loss: 0.00005069
Iteration 45/1000 | Loss: 0.00005069
Iteration 46/1000 | Loss: 0.00005069
Iteration 47/1000 | Loss: 0.00005068
Iteration 48/1000 | Loss: 0.00005068
Iteration 49/1000 | Loss: 0.00005067
Iteration 50/1000 | Loss: 0.00005067
Iteration 51/1000 | Loss: 0.00005067
Iteration 52/1000 | Loss: 0.00005066
Iteration 53/1000 | Loss: 0.00005066
Iteration 54/1000 | Loss: 0.00005066
Iteration 55/1000 | Loss: 0.00005065
Iteration 56/1000 | Loss: 0.00005065
Iteration 57/1000 | Loss: 0.00005065
Iteration 58/1000 | Loss: 0.00005064
Iteration 59/1000 | Loss: 0.00005064
Iteration 60/1000 | Loss: 0.00005064
Iteration 61/1000 | Loss: 0.00005063
Iteration 62/1000 | Loss: 0.00005063
Iteration 63/1000 | Loss: 0.00005062
Iteration 64/1000 | Loss: 0.00005062
Iteration 65/1000 | Loss: 0.00005062
Iteration 66/1000 | Loss: 0.00005062
Iteration 67/1000 | Loss: 0.00005061
Iteration 68/1000 | Loss: 0.00005061
Iteration 69/1000 | Loss: 0.00005061
Iteration 70/1000 | Loss: 0.00005061
Iteration 71/1000 | Loss: 0.00005060
Iteration 72/1000 | Loss: 0.00005060
Iteration 73/1000 | Loss: 0.00005060
Iteration 74/1000 | Loss: 0.00005059
Iteration 75/1000 | Loss: 0.00005059
Iteration 76/1000 | Loss: 0.00005059
Iteration 77/1000 | Loss: 0.00005058
Iteration 78/1000 | Loss: 0.00005058
Iteration 79/1000 | Loss: 0.00005058
Iteration 80/1000 | Loss: 0.00005057
Iteration 81/1000 | Loss: 0.00005057
Iteration 82/1000 | Loss: 0.00005057
Iteration 83/1000 | Loss: 0.00005057
Iteration 84/1000 | Loss: 0.00005057
Iteration 85/1000 | Loss: 0.00005056
Iteration 86/1000 | Loss: 0.00005056
Iteration 87/1000 | Loss: 0.00005056
Iteration 88/1000 | Loss: 0.00005056
Iteration 89/1000 | Loss: 0.00005056
Iteration 90/1000 | Loss: 0.00005056
Iteration 91/1000 | Loss: 0.00005056
Iteration 92/1000 | Loss: 0.00005056
Iteration 93/1000 | Loss: 0.00005056
Iteration 94/1000 | Loss: 0.00005056
Iteration 95/1000 | Loss: 0.00005055
Iteration 96/1000 | Loss: 0.00005055
Iteration 97/1000 | Loss: 0.00005055
Iteration 98/1000 | Loss: 0.00005054
Iteration 99/1000 | Loss: 0.00005054
Iteration 100/1000 | Loss: 0.00005054
Iteration 101/1000 | Loss: 0.00005054
Iteration 102/1000 | Loss: 0.00005054
Iteration 103/1000 | Loss: 0.00005054
Iteration 104/1000 | Loss: 0.00005053
Iteration 105/1000 | Loss: 0.00005053
Iteration 106/1000 | Loss: 0.00005053
Iteration 107/1000 | Loss: 0.00005053
Iteration 108/1000 | Loss: 0.00005053
Iteration 109/1000 | Loss: 0.00005053
Iteration 110/1000 | Loss: 0.00005053
Iteration 111/1000 | Loss: 0.00005053
Iteration 112/1000 | Loss: 0.00005053
Iteration 113/1000 | Loss: 0.00005053
Iteration 114/1000 | Loss: 0.00005053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [5.053338099969551e-05, 5.053338099969551e-05, 5.053338099969551e-05, 5.053338099969551e-05, 5.053338099969551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.053338099969551e-05

Optimization complete. Final v2v error: 5.8934831619262695 mm

Highest mean error: 7.884308815002441 mm for frame 118

Lowest mean error: 5.0471062660217285 mm for frame 164

Saving results

Total time: 61.11620831489563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076737
Iteration 2/25 | Loss: 0.00258678
Iteration 3/25 | Loss: 0.00184652
Iteration 4/25 | Loss: 0.00167862
Iteration 5/25 | Loss: 0.00160880
Iteration 6/25 | Loss: 0.00154716
Iteration 7/25 | Loss: 0.00156220
Iteration 8/25 | Loss: 0.00154255
Iteration 9/25 | Loss: 0.00148159
Iteration 10/25 | Loss: 0.00145600
Iteration 11/25 | Loss: 0.00143720
Iteration 12/25 | Loss: 0.00142536
Iteration 13/25 | Loss: 0.00142387
Iteration 14/25 | Loss: 0.00141982
Iteration 15/25 | Loss: 0.00141767
Iteration 16/25 | Loss: 0.00141273
Iteration 17/25 | Loss: 0.00140872
Iteration 18/25 | Loss: 0.00140841
Iteration 19/25 | Loss: 0.00140745
Iteration 20/25 | Loss: 0.00140459
Iteration 21/25 | Loss: 0.00140620
Iteration 22/25 | Loss: 0.00140526
Iteration 23/25 | Loss: 0.00140260
Iteration 24/25 | Loss: 0.00140280
Iteration 25/25 | Loss: 0.00140379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39681661
Iteration 2/25 | Loss: 0.00084186
Iteration 3/25 | Loss: 0.00084186
Iteration 4/25 | Loss: 0.00084186
Iteration 5/25 | Loss: 0.00084186
Iteration 6/25 | Loss: 0.00084186
Iteration 7/25 | Loss: 0.00084186
Iteration 8/25 | Loss: 0.00084186
Iteration 9/25 | Loss: 0.00084186
Iteration 10/25 | Loss: 0.00084186
Iteration 11/25 | Loss: 0.00084186
Iteration 12/25 | Loss: 0.00084186
Iteration 13/25 | Loss: 0.00084186
Iteration 14/25 | Loss: 0.00084186
Iteration 15/25 | Loss: 0.00084186
Iteration 16/25 | Loss: 0.00084186
Iteration 17/25 | Loss: 0.00084186
Iteration 18/25 | Loss: 0.00084186
Iteration 19/25 | Loss: 0.00084186
Iteration 20/25 | Loss: 0.00084186
Iteration 21/25 | Loss: 0.00084186
Iteration 22/25 | Loss: 0.00084186
Iteration 23/25 | Loss: 0.00084186
Iteration 24/25 | Loss: 0.00084186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008418606594204903, 0.0008418606594204903, 0.0008418606594204903, 0.0008418606594204903, 0.0008418606594204903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008418606594204903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084186
Iteration 2/1000 | Loss: 0.00011123
Iteration 3/1000 | Loss: 0.00009411
Iteration 4/1000 | Loss: 0.00010793
Iteration 5/1000 | Loss: 0.00008845
Iteration 6/1000 | Loss: 0.00010345
Iteration 7/1000 | Loss: 0.00009041
Iteration 8/1000 | Loss: 0.00010152
Iteration 9/1000 | Loss: 0.00010151
Iteration 10/1000 | Loss: 0.00009713
Iteration 11/1000 | Loss: 0.00009354
Iteration 12/1000 | Loss: 0.00011172
Iteration 13/1000 | Loss: 0.00019308
Iteration 14/1000 | Loss: 0.00013358
Iteration 15/1000 | Loss: 0.00009769
Iteration 16/1000 | Loss: 0.00010132
Iteration 17/1000 | Loss: 0.00011153
Iteration 18/1000 | Loss: 0.00009899
Iteration 19/1000 | Loss: 0.00010338
Iteration 20/1000 | Loss: 0.00009450
Iteration 21/1000 | Loss: 0.00009919
Iteration 22/1000 | Loss: 0.00011420
Iteration 23/1000 | Loss: 0.00011818
Iteration 24/1000 | Loss: 0.00012649
Iteration 25/1000 | Loss: 0.00011720
Iteration 26/1000 | Loss: 0.00013579
Iteration 27/1000 | Loss: 0.00012846
Iteration 28/1000 | Loss: 0.00017183
Iteration 29/1000 | Loss: 0.00008600
Iteration 30/1000 | Loss: 0.00010245
Iteration 31/1000 | Loss: 0.00011793
Iteration 32/1000 | Loss: 0.00011110
Iteration 33/1000 | Loss: 0.00013255
Iteration 34/1000 | Loss: 0.00016028
Iteration 35/1000 | Loss: 0.00010135
Iteration 36/1000 | Loss: 0.00009597
Iteration 37/1000 | Loss: 0.00009701
Iteration 38/1000 | Loss: 0.00023232
Iteration 39/1000 | Loss: 0.00011955
Iteration 40/1000 | Loss: 0.00011647
Iteration 41/1000 | Loss: 0.00011718
Iteration 42/1000 | Loss: 0.00010557
Iteration 43/1000 | Loss: 0.00011068
Iteration 44/1000 | Loss: 0.00010082
Iteration 45/1000 | Loss: 0.00012708
Iteration 46/1000 | Loss: 0.00017279
Iteration 47/1000 | Loss: 0.00011586
Iteration 48/1000 | Loss: 0.00008810
Iteration 49/1000 | Loss: 0.00010363
Iteration 50/1000 | Loss: 0.00010078
Iteration 51/1000 | Loss: 0.00007698
Iteration 52/1000 | Loss: 0.00006501
Iteration 53/1000 | Loss: 0.00021846
Iteration 54/1000 | Loss: 0.00019924
Iteration 55/1000 | Loss: 0.00010580
Iteration 56/1000 | Loss: 0.00009183
Iteration 57/1000 | Loss: 0.00008419
Iteration 58/1000 | Loss: 0.00007634
Iteration 59/1000 | Loss: 0.00007710
Iteration 60/1000 | Loss: 0.00008365
Iteration 61/1000 | Loss: 0.00007346
Iteration 62/1000 | Loss: 0.00007605
Iteration 63/1000 | Loss: 0.00008788
Iteration 64/1000 | Loss: 0.00007329
Iteration 65/1000 | Loss: 0.00007713
Iteration 66/1000 | Loss: 0.00009914
Iteration 67/1000 | Loss: 0.00008998
Iteration 68/1000 | Loss: 0.00008795
Iteration 69/1000 | Loss: 0.00005504
Iteration 70/1000 | Loss: 0.00005995
Iteration 71/1000 | Loss: 0.00006404
Iteration 72/1000 | Loss: 0.00007227
Iteration 73/1000 | Loss: 0.00006498
Iteration 74/1000 | Loss: 0.00008271
Iteration 75/1000 | Loss: 0.00005834
Iteration 76/1000 | Loss: 0.00007091
Iteration 77/1000 | Loss: 0.00007094
Iteration 78/1000 | Loss: 0.00006639
Iteration 79/1000 | Loss: 0.00006642
Iteration 80/1000 | Loss: 0.00007469
Iteration 81/1000 | Loss: 0.00007818
Iteration 82/1000 | Loss: 0.00018157
Iteration 83/1000 | Loss: 0.00008902
Iteration 84/1000 | Loss: 0.00018150
Iteration 85/1000 | Loss: 0.00007711
Iteration 86/1000 | Loss: 0.00005901
Iteration 87/1000 | Loss: 0.00005160
Iteration 88/1000 | Loss: 0.00004541
Iteration 89/1000 | Loss: 0.00004469
Iteration 90/1000 | Loss: 0.00006069
Iteration 91/1000 | Loss: 0.00005967
Iteration 92/1000 | Loss: 0.00005847
Iteration 93/1000 | Loss: 0.00005694
Iteration 94/1000 | Loss: 0.00005537
Iteration 95/1000 | Loss: 0.00005386
Iteration 96/1000 | Loss: 0.00005830
Iteration 97/1000 | Loss: 0.00005084
Iteration 98/1000 | Loss: 0.00004075
Iteration 99/1000 | Loss: 0.00004550
Iteration 100/1000 | Loss: 0.00004261
Iteration 101/1000 | Loss: 0.00005886
Iteration 102/1000 | Loss: 0.00005524
Iteration 103/1000 | Loss: 0.00004751
Iteration 104/1000 | Loss: 0.00004334
Iteration 105/1000 | Loss: 0.00006552
Iteration 106/1000 | Loss: 0.00006115
Iteration 107/1000 | Loss: 0.00005173
Iteration 108/1000 | Loss: 0.00004704
Iteration 109/1000 | Loss: 0.00007752
Iteration 110/1000 | Loss: 0.00006091
Iteration 111/1000 | Loss: 0.00005992
Iteration 112/1000 | Loss: 0.00006646
Iteration 113/1000 | Loss: 0.00004083
Iteration 114/1000 | Loss: 0.00006011
Iteration 115/1000 | Loss: 0.00006398
Iteration 116/1000 | Loss: 0.00004527
Iteration 117/1000 | Loss: 0.00004721
Iteration 118/1000 | Loss: 0.00004283
Iteration 119/1000 | Loss: 0.00005817
Iteration 120/1000 | Loss: 0.00006069
Iteration 121/1000 | Loss: 0.00005688
Iteration 122/1000 | Loss: 0.00006286
Iteration 123/1000 | Loss: 0.00006511
Iteration 124/1000 | Loss: 0.00005132
Iteration 125/1000 | Loss: 0.00005743
Iteration 126/1000 | Loss: 0.00003975
Iteration 127/1000 | Loss: 0.00004570
Iteration 128/1000 | Loss: 0.00004035
Iteration 129/1000 | Loss: 0.00003955
Iteration 130/1000 | Loss: 0.00004505
Iteration 131/1000 | Loss: 0.00003678
Iteration 132/1000 | Loss: 0.00005054
Iteration 133/1000 | Loss: 0.00003716
Iteration 134/1000 | Loss: 0.00004502
Iteration 135/1000 | Loss: 0.00003630
Iteration 136/1000 | Loss: 0.00004140
Iteration 137/1000 | Loss: 0.00004551
Iteration 138/1000 | Loss: 0.00004828
Iteration 139/1000 | Loss: 0.00004738
Iteration 140/1000 | Loss: 0.00004111
Iteration 141/1000 | Loss: 0.00003457
Iteration 142/1000 | Loss: 0.00003884
Iteration 143/1000 | Loss: 0.00004020
Iteration 144/1000 | Loss: 0.00011147
Iteration 145/1000 | Loss: 0.00032171
Iteration 146/1000 | Loss: 0.00004600
Iteration 147/1000 | Loss: 0.00004172
Iteration 148/1000 | Loss: 0.00003959
Iteration 149/1000 | Loss: 0.00003944
Iteration 150/1000 | Loss: 0.00003819
Iteration 151/1000 | Loss: 0.00003936
Iteration 152/1000 | Loss: 0.00003729
Iteration 153/1000 | Loss: 0.00005283
Iteration 154/1000 | Loss: 0.00004386
Iteration 155/1000 | Loss: 0.00004372
Iteration 156/1000 | Loss: 0.00004186
Iteration 157/1000 | Loss: 0.00003878
Iteration 158/1000 | Loss: 0.00004344
Iteration 159/1000 | Loss: 0.00004451
Iteration 160/1000 | Loss: 0.00004292
Iteration 161/1000 | Loss: 0.00003929
Iteration 162/1000 | Loss: 0.00005843
Iteration 163/1000 | Loss: 0.00004726
Iteration 164/1000 | Loss: 0.00003963
Iteration 165/1000 | Loss: 0.00004214
Iteration 166/1000 | Loss: 0.00003751
Iteration 167/1000 | Loss: 0.00004718
Iteration 168/1000 | Loss: 0.00004763
Iteration 169/1000 | Loss: 0.00003918
Iteration 170/1000 | Loss: 0.00004292
Iteration 171/1000 | Loss: 0.00003908
Iteration 172/1000 | Loss: 0.00004373
Iteration 173/1000 | Loss: 0.00005565
Iteration 174/1000 | Loss: 0.00003786
Iteration 175/1000 | Loss: 0.00003549
Iteration 176/1000 | Loss: 0.00003458
Iteration 177/1000 | Loss: 0.00003422
Iteration 178/1000 | Loss: 0.00003407
Iteration 179/1000 | Loss: 0.00003406
Iteration 180/1000 | Loss: 0.00003403
Iteration 181/1000 | Loss: 0.00003397
Iteration 182/1000 | Loss: 0.00003392
Iteration 183/1000 | Loss: 0.00003377
Iteration 184/1000 | Loss: 0.00003372
Iteration 185/1000 | Loss: 0.00003368
Iteration 186/1000 | Loss: 0.00003364
Iteration 187/1000 | Loss: 0.00003363
Iteration 188/1000 | Loss: 0.00003363
Iteration 189/1000 | Loss: 0.00003362
Iteration 190/1000 | Loss: 0.00003362
Iteration 191/1000 | Loss: 0.00003359
Iteration 192/1000 | Loss: 0.00003359
Iteration 193/1000 | Loss: 0.00003354
Iteration 194/1000 | Loss: 0.00003354
Iteration 195/1000 | Loss: 0.00003354
Iteration 196/1000 | Loss: 0.00003354
Iteration 197/1000 | Loss: 0.00003354
Iteration 198/1000 | Loss: 0.00003354
Iteration 199/1000 | Loss: 0.00003354
Iteration 200/1000 | Loss: 0.00003354
Iteration 201/1000 | Loss: 0.00003353
Iteration 202/1000 | Loss: 0.00003353
Iteration 203/1000 | Loss: 0.00003350
Iteration 204/1000 | Loss: 0.00003349
Iteration 205/1000 | Loss: 0.00003349
Iteration 206/1000 | Loss: 0.00003348
Iteration 207/1000 | Loss: 0.00003348
Iteration 208/1000 | Loss: 0.00003348
Iteration 209/1000 | Loss: 0.00003347
Iteration 210/1000 | Loss: 0.00003347
Iteration 211/1000 | Loss: 0.00003347
Iteration 212/1000 | Loss: 0.00003346
Iteration 213/1000 | Loss: 0.00003346
Iteration 214/1000 | Loss: 0.00003346
Iteration 215/1000 | Loss: 0.00003346
Iteration 216/1000 | Loss: 0.00003346
Iteration 217/1000 | Loss: 0.00003345
Iteration 218/1000 | Loss: 0.00003345
Iteration 219/1000 | Loss: 0.00003345
Iteration 220/1000 | Loss: 0.00003345
Iteration 221/1000 | Loss: 0.00003345
Iteration 222/1000 | Loss: 0.00003345
Iteration 223/1000 | Loss: 0.00003345
Iteration 224/1000 | Loss: 0.00003345
Iteration 225/1000 | Loss: 0.00003345
Iteration 226/1000 | Loss: 0.00003344
Iteration 227/1000 | Loss: 0.00003344
Iteration 228/1000 | Loss: 0.00003344
Iteration 229/1000 | Loss: 0.00003344
Iteration 230/1000 | Loss: 0.00003344
Iteration 231/1000 | Loss: 0.00003344
Iteration 232/1000 | Loss: 0.00003344
Iteration 233/1000 | Loss: 0.00003343
Iteration 234/1000 | Loss: 0.00003343
Iteration 235/1000 | Loss: 0.00003343
Iteration 236/1000 | Loss: 0.00003343
Iteration 237/1000 | Loss: 0.00003343
Iteration 238/1000 | Loss: 0.00003343
Iteration 239/1000 | Loss: 0.00003343
Iteration 240/1000 | Loss: 0.00003343
Iteration 241/1000 | Loss: 0.00003343
Iteration 242/1000 | Loss: 0.00003343
Iteration 243/1000 | Loss: 0.00003343
Iteration 244/1000 | Loss: 0.00003343
Iteration 245/1000 | Loss: 0.00003343
Iteration 246/1000 | Loss: 0.00003343
Iteration 247/1000 | Loss: 0.00003343
Iteration 248/1000 | Loss: 0.00003343
Iteration 249/1000 | Loss: 0.00003343
Iteration 250/1000 | Loss: 0.00003343
Iteration 251/1000 | Loss: 0.00003343
Iteration 252/1000 | Loss: 0.00003343
Iteration 253/1000 | Loss: 0.00003343
Iteration 254/1000 | Loss: 0.00003343
Iteration 255/1000 | Loss: 0.00003343
Iteration 256/1000 | Loss: 0.00003343
Iteration 257/1000 | Loss: 0.00003343
Iteration 258/1000 | Loss: 0.00003343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [3.3425629226258025e-05, 3.3425629226258025e-05, 3.3425629226258025e-05, 3.3425629226258025e-05, 3.3425629226258025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3425629226258025e-05

Optimization complete. Final v2v error: 4.6088409423828125 mm

Highest mean error: 10.480396270751953 mm for frame 51

Lowest mean error: 3.9694857597351074 mm for frame 73

Saving results

Total time: 316.42367577552795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484363
Iteration 2/25 | Loss: 0.00195850
Iteration 3/25 | Loss: 0.00185385
Iteration 4/25 | Loss: 0.00183823
Iteration 5/25 | Loss: 0.00183247
Iteration 6/25 | Loss: 0.00183063
Iteration 7/25 | Loss: 0.00183043
Iteration 8/25 | Loss: 0.00183043
Iteration 9/25 | Loss: 0.00183043
Iteration 10/25 | Loss: 0.00183043
Iteration 11/25 | Loss: 0.00183043
Iteration 12/25 | Loss: 0.00183043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018304348923265934, 0.0018304348923265934, 0.0018304348923265934, 0.0018304348923265934, 0.0018304348923265934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018304348923265934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.47770119
Iteration 2/25 | Loss: 0.00132168
Iteration 3/25 | Loss: 0.00132167
Iteration 4/25 | Loss: 0.00132167
Iteration 5/25 | Loss: 0.00132167
Iteration 6/25 | Loss: 0.00132167
Iteration 7/25 | Loss: 0.00132167
Iteration 8/25 | Loss: 0.00132167
Iteration 9/25 | Loss: 0.00132167
Iteration 10/25 | Loss: 0.00132167
Iteration 11/25 | Loss: 0.00132167
Iteration 12/25 | Loss: 0.00132167
Iteration 13/25 | Loss: 0.00132167
Iteration 14/25 | Loss: 0.00132167
Iteration 15/25 | Loss: 0.00132167
Iteration 16/25 | Loss: 0.00132167
Iteration 17/25 | Loss: 0.00132167
Iteration 18/25 | Loss: 0.00132167
Iteration 19/25 | Loss: 0.00132167
Iteration 20/25 | Loss: 0.00132167
Iteration 21/25 | Loss: 0.00132167
Iteration 22/25 | Loss: 0.00132167
Iteration 23/25 | Loss: 0.00132167
Iteration 24/25 | Loss: 0.00132167
Iteration 25/25 | Loss: 0.00132167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132167
Iteration 2/1000 | Loss: 0.00009150
Iteration 3/1000 | Loss: 0.00005508
Iteration 4/1000 | Loss: 0.00004556
Iteration 5/1000 | Loss: 0.00004222
Iteration 6/1000 | Loss: 0.00004074
Iteration 7/1000 | Loss: 0.00003953
Iteration 8/1000 | Loss: 0.00003878
Iteration 9/1000 | Loss: 0.00003816
Iteration 10/1000 | Loss: 0.00003751
Iteration 11/1000 | Loss: 0.00003719
Iteration 12/1000 | Loss: 0.00003675
Iteration 13/1000 | Loss: 0.00003651
Iteration 14/1000 | Loss: 0.00003641
Iteration 15/1000 | Loss: 0.00003640
Iteration 16/1000 | Loss: 0.00003640
Iteration 17/1000 | Loss: 0.00003639
Iteration 18/1000 | Loss: 0.00003636
Iteration 19/1000 | Loss: 0.00003636
Iteration 20/1000 | Loss: 0.00003634
Iteration 21/1000 | Loss: 0.00003633
Iteration 22/1000 | Loss: 0.00003633
Iteration 23/1000 | Loss: 0.00003633
Iteration 24/1000 | Loss: 0.00003632
Iteration 25/1000 | Loss: 0.00003628
Iteration 26/1000 | Loss: 0.00003628
Iteration 27/1000 | Loss: 0.00003628
Iteration 28/1000 | Loss: 0.00003628
Iteration 29/1000 | Loss: 0.00003628
Iteration 30/1000 | Loss: 0.00003628
Iteration 31/1000 | Loss: 0.00003628
Iteration 32/1000 | Loss: 0.00003628
Iteration 33/1000 | Loss: 0.00003628
Iteration 34/1000 | Loss: 0.00003628
Iteration 35/1000 | Loss: 0.00003626
Iteration 36/1000 | Loss: 0.00003625
Iteration 37/1000 | Loss: 0.00003625
Iteration 38/1000 | Loss: 0.00003625
Iteration 39/1000 | Loss: 0.00003625
Iteration 40/1000 | Loss: 0.00003625
Iteration 41/1000 | Loss: 0.00003625
Iteration 42/1000 | Loss: 0.00003625
Iteration 43/1000 | Loss: 0.00003625
Iteration 44/1000 | Loss: 0.00003625
Iteration 45/1000 | Loss: 0.00003625
Iteration 46/1000 | Loss: 0.00003625
Iteration 47/1000 | Loss: 0.00003625
Iteration 48/1000 | Loss: 0.00003625
Iteration 49/1000 | Loss: 0.00003625
Iteration 50/1000 | Loss: 0.00003625
Iteration 51/1000 | Loss: 0.00003625
Iteration 52/1000 | Loss: 0.00003625
Iteration 53/1000 | Loss: 0.00003625
Iteration 54/1000 | Loss: 0.00003625
Iteration 55/1000 | Loss: 0.00003625
Iteration 56/1000 | Loss: 0.00003625
Iteration 57/1000 | Loss: 0.00003625
Iteration 58/1000 | Loss: 0.00003625
Iteration 59/1000 | Loss: 0.00003625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [3.624692544690333e-05, 3.624692544690333e-05, 3.624692544690333e-05, 3.624692544690333e-05, 3.624692544690333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.624692544690333e-05

Optimization complete. Final v2v error: 5.113379955291748 mm

Highest mean error: 5.537425518035889 mm for frame 71

Lowest mean error: 4.824567794799805 mm for frame 47

Saving results

Total time: 31.285540342330933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860918
Iteration 2/25 | Loss: 0.00205102
Iteration 3/25 | Loss: 0.00195538
Iteration 4/25 | Loss: 0.00194331
Iteration 5/25 | Loss: 0.00193948
Iteration 6/25 | Loss: 0.00193832
Iteration 7/25 | Loss: 0.00193811
Iteration 8/25 | Loss: 0.00193811
Iteration 9/25 | Loss: 0.00193811
Iteration 10/25 | Loss: 0.00193811
Iteration 11/25 | Loss: 0.00193811
Iteration 12/25 | Loss: 0.00193811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0019381138263270259, 0.0019381138263270259, 0.0019381138263270259, 0.0019381138263270259, 0.0019381138263270259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019381138263270259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42185235
Iteration 2/25 | Loss: 0.00153893
Iteration 3/25 | Loss: 0.00153884
Iteration 4/25 | Loss: 0.00153884
Iteration 5/25 | Loss: 0.00153884
Iteration 6/25 | Loss: 0.00153884
Iteration 7/25 | Loss: 0.00153883
Iteration 8/25 | Loss: 0.00153883
Iteration 9/25 | Loss: 0.00153883
Iteration 10/25 | Loss: 0.00153883
Iteration 11/25 | Loss: 0.00153883
Iteration 12/25 | Loss: 0.00153883
Iteration 13/25 | Loss: 0.00153883
Iteration 14/25 | Loss: 0.00153883
Iteration 15/25 | Loss: 0.00153883
Iteration 16/25 | Loss: 0.00153883
Iteration 17/25 | Loss: 0.00153883
Iteration 18/25 | Loss: 0.00153883
Iteration 19/25 | Loss: 0.00153883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0015388340689241886, 0.0015388340689241886, 0.0015388340689241886, 0.0015388340689241886, 0.0015388340689241886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015388340689241886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153883
Iteration 2/1000 | Loss: 0.00008724
Iteration 3/1000 | Loss: 0.00005614
Iteration 4/1000 | Loss: 0.00004496
Iteration 5/1000 | Loss: 0.00004250
Iteration 6/1000 | Loss: 0.00004094
Iteration 7/1000 | Loss: 0.00003971
Iteration 8/1000 | Loss: 0.00003867
Iteration 9/1000 | Loss: 0.00003812
Iteration 10/1000 | Loss: 0.00003766
Iteration 11/1000 | Loss: 0.00003706
Iteration 12/1000 | Loss: 0.00003667
Iteration 13/1000 | Loss: 0.00003638
Iteration 14/1000 | Loss: 0.00003616
Iteration 15/1000 | Loss: 0.00003610
Iteration 16/1000 | Loss: 0.00003603
Iteration 17/1000 | Loss: 0.00003600
Iteration 18/1000 | Loss: 0.00003600
Iteration 19/1000 | Loss: 0.00003599
Iteration 20/1000 | Loss: 0.00003599
Iteration 21/1000 | Loss: 0.00003598
Iteration 22/1000 | Loss: 0.00003598
Iteration 23/1000 | Loss: 0.00003597
Iteration 24/1000 | Loss: 0.00003597
Iteration 25/1000 | Loss: 0.00003595
Iteration 26/1000 | Loss: 0.00003595
Iteration 27/1000 | Loss: 0.00003594
Iteration 28/1000 | Loss: 0.00003594
Iteration 29/1000 | Loss: 0.00003594
Iteration 30/1000 | Loss: 0.00003594
Iteration 31/1000 | Loss: 0.00003594
Iteration 32/1000 | Loss: 0.00003594
Iteration 33/1000 | Loss: 0.00003594
Iteration 34/1000 | Loss: 0.00003594
Iteration 35/1000 | Loss: 0.00003594
Iteration 36/1000 | Loss: 0.00003594
Iteration 37/1000 | Loss: 0.00003594
Iteration 38/1000 | Loss: 0.00003593
Iteration 39/1000 | Loss: 0.00003593
Iteration 40/1000 | Loss: 0.00003593
Iteration 41/1000 | Loss: 0.00003592
Iteration 42/1000 | Loss: 0.00003592
Iteration 43/1000 | Loss: 0.00003591
Iteration 44/1000 | Loss: 0.00003591
Iteration 45/1000 | Loss: 0.00003591
Iteration 46/1000 | Loss: 0.00003591
Iteration 47/1000 | Loss: 0.00003591
Iteration 48/1000 | Loss: 0.00003591
Iteration 49/1000 | Loss: 0.00003590
Iteration 50/1000 | Loss: 0.00003590
Iteration 51/1000 | Loss: 0.00003590
Iteration 52/1000 | Loss: 0.00003590
Iteration 53/1000 | Loss: 0.00003589
Iteration 54/1000 | Loss: 0.00003589
Iteration 55/1000 | Loss: 0.00003589
Iteration 56/1000 | Loss: 0.00003589
Iteration 57/1000 | Loss: 0.00003589
Iteration 58/1000 | Loss: 0.00003589
Iteration 59/1000 | Loss: 0.00003589
Iteration 60/1000 | Loss: 0.00003589
Iteration 61/1000 | Loss: 0.00003588
Iteration 62/1000 | Loss: 0.00003588
Iteration 63/1000 | Loss: 0.00003588
Iteration 64/1000 | Loss: 0.00003588
Iteration 65/1000 | Loss: 0.00003588
Iteration 66/1000 | Loss: 0.00003588
Iteration 67/1000 | Loss: 0.00003588
Iteration 68/1000 | Loss: 0.00003588
Iteration 69/1000 | Loss: 0.00003588
Iteration 70/1000 | Loss: 0.00003587
Iteration 71/1000 | Loss: 0.00003587
Iteration 72/1000 | Loss: 0.00003587
Iteration 73/1000 | Loss: 0.00003587
Iteration 74/1000 | Loss: 0.00003587
Iteration 75/1000 | Loss: 0.00003586
Iteration 76/1000 | Loss: 0.00003586
Iteration 77/1000 | Loss: 0.00003586
Iteration 78/1000 | Loss: 0.00003586
Iteration 79/1000 | Loss: 0.00003586
Iteration 80/1000 | Loss: 0.00003585
Iteration 81/1000 | Loss: 0.00003585
Iteration 82/1000 | Loss: 0.00003585
Iteration 83/1000 | Loss: 0.00003585
Iteration 84/1000 | Loss: 0.00003585
Iteration 85/1000 | Loss: 0.00003585
Iteration 86/1000 | Loss: 0.00003585
Iteration 87/1000 | Loss: 0.00003585
Iteration 88/1000 | Loss: 0.00003585
Iteration 89/1000 | Loss: 0.00003585
Iteration 90/1000 | Loss: 0.00003585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [3.585054219001904e-05, 3.585054219001904e-05, 3.585054219001904e-05, 3.585054219001904e-05, 3.585054219001904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.585054219001904e-05

Optimization complete. Final v2v error: 5.130573272705078 mm

Highest mean error: 5.5701704025268555 mm for frame 36

Lowest mean error: 4.938322067260742 mm for frame 61

Saving results

Total time: 35.400328159332275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128871
Iteration 2/25 | Loss: 0.00317301
Iteration 3/25 | Loss: 0.00133480
Iteration 4/25 | Loss: 0.00109980
Iteration 5/25 | Loss: 0.00103715
Iteration 6/25 | Loss: 0.00094780
Iteration 7/25 | Loss: 0.00088061
Iteration 8/25 | Loss: 0.00082487
Iteration 9/25 | Loss: 0.00079068
Iteration 10/25 | Loss: 0.00076775
Iteration 11/25 | Loss: 0.00075249
Iteration 12/25 | Loss: 0.00073533
Iteration 13/25 | Loss: 0.00072422
Iteration 14/25 | Loss: 0.00072346
Iteration 15/25 | Loss: 0.00071785
Iteration 16/25 | Loss: 0.00071682
Iteration 17/25 | Loss: 0.00072127
Iteration 18/25 | Loss: 0.00071876
Iteration 19/25 | Loss: 0.00071873
Iteration 20/25 | Loss: 0.00071724
Iteration 21/25 | Loss: 0.00071748
Iteration 22/25 | Loss: 0.00071697
Iteration 23/25 | Loss: 0.00071714
Iteration 24/25 | Loss: 0.00071760
Iteration 25/25 | Loss: 0.00071859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40304482
Iteration 2/25 | Loss: 0.00071343
Iteration 3/25 | Loss: 0.00071343
Iteration 4/25 | Loss: 0.00071343
Iteration 5/25 | Loss: 0.00071343
Iteration 6/25 | Loss: 0.00071343
Iteration 7/25 | Loss: 0.00071343
Iteration 8/25 | Loss: 0.00071343
Iteration 9/25 | Loss: 0.00071343
Iteration 10/25 | Loss: 0.00071343
Iteration 11/25 | Loss: 0.00071343
Iteration 12/25 | Loss: 0.00071343
Iteration 13/25 | Loss: 0.00071343
Iteration 14/25 | Loss: 0.00071343
Iteration 15/25 | Loss: 0.00071343
Iteration 16/25 | Loss: 0.00071343
Iteration 17/25 | Loss: 0.00071343
Iteration 18/25 | Loss: 0.00071343
Iteration 19/25 | Loss: 0.00071343
Iteration 20/25 | Loss: 0.00071343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007134294137358665, 0.0007134294137358665, 0.0007134294137358665, 0.0007134294137358665, 0.0007134294137358665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007134294137358665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071343
Iteration 2/1000 | Loss: 0.00012282
Iteration 3/1000 | Loss: 0.00018493
Iteration 4/1000 | Loss: 0.00024522
Iteration 5/1000 | Loss: 0.00032449
Iteration 6/1000 | Loss: 0.00046700
Iteration 7/1000 | Loss: 0.00008805
Iteration 8/1000 | Loss: 0.00018818
Iteration 9/1000 | Loss: 0.00021296
Iteration 10/1000 | Loss: 0.00033006
Iteration 11/1000 | Loss: 0.00026882
Iteration 12/1000 | Loss: 0.00044488
Iteration 13/1000 | Loss: 0.00036799
Iteration 14/1000 | Loss: 0.00046533
Iteration 15/1000 | Loss: 0.00023852
Iteration 16/1000 | Loss: 0.00057917
Iteration 17/1000 | Loss: 0.00039448
Iteration 18/1000 | Loss: 0.00030074
Iteration 19/1000 | Loss: 0.00055839
Iteration 20/1000 | Loss: 0.00055447
Iteration 21/1000 | Loss: 0.00053499
Iteration 22/1000 | Loss: 0.00057493
Iteration 23/1000 | Loss: 0.00023435
Iteration 24/1000 | Loss: 0.00017876
Iteration 25/1000 | Loss: 0.00016049
Iteration 26/1000 | Loss: 0.00005217
Iteration 27/1000 | Loss: 0.00005402
Iteration 28/1000 | Loss: 0.00044678
Iteration 29/1000 | Loss: 0.00023996
Iteration 30/1000 | Loss: 0.00014820
Iteration 31/1000 | Loss: 0.00014424
Iteration 32/1000 | Loss: 0.00011387
Iteration 33/1000 | Loss: 0.00016255
Iteration 34/1000 | Loss: 0.00004874
Iteration 35/1000 | Loss: 0.00019536
Iteration 36/1000 | Loss: 0.00012814
Iteration 37/1000 | Loss: 0.00004108
Iteration 38/1000 | Loss: 0.00060204
Iteration 39/1000 | Loss: 0.00049795
Iteration 40/1000 | Loss: 0.00062183
Iteration 41/1000 | Loss: 0.00047387
Iteration 42/1000 | Loss: 0.00016461
Iteration 43/1000 | Loss: 0.00006839
Iteration 44/1000 | Loss: 0.00005869
Iteration 45/1000 | Loss: 0.00019334
Iteration 46/1000 | Loss: 0.00006235
Iteration 47/1000 | Loss: 0.00019184
Iteration 48/1000 | Loss: 0.00017294
Iteration 49/1000 | Loss: 0.00004779
Iteration 50/1000 | Loss: 0.00019055
Iteration 51/1000 | Loss: 0.00006832
Iteration 52/1000 | Loss: 0.00005936
Iteration 53/1000 | Loss: 0.00004842
Iteration 54/1000 | Loss: 0.00005434
Iteration 55/1000 | Loss: 0.00005129
Iteration 56/1000 | Loss: 0.00004940
Iteration 57/1000 | Loss: 0.00004775
Iteration 58/1000 | Loss: 0.00005811
Iteration 59/1000 | Loss: 0.00004605
Iteration 60/1000 | Loss: 0.00004517
Iteration 61/1000 | Loss: 0.00004395
Iteration 62/1000 | Loss: 0.00004570
Iteration 63/1000 | Loss: 0.00004378
Iteration 64/1000 | Loss: 0.00004660
Iteration 65/1000 | Loss: 0.00004530
Iteration 66/1000 | Loss: 0.00004815
Iteration 67/1000 | Loss: 0.00005850
Iteration 68/1000 | Loss: 0.00004643
Iteration 69/1000 | Loss: 0.00005892
Iteration 70/1000 | Loss: 0.00004100
Iteration 71/1000 | Loss: 0.00005047
Iteration 72/1000 | Loss: 0.00004469
Iteration 73/1000 | Loss: 0.00004516
Iteration 74/1000 | Loss: 0.00004860
Iteration 75/1000 | Loss: 0.00004505
Iteration 76/1000 | Loss: 0.00004401
Iteration 77/1000 | Loss: 0.00004482
Iteration 78/1000 | Loss: 0.00004739
Iteration 79/1000 | Loss: 0.00004427
Iteration 80/1000 | Loss: 0.00004665
Iteration 81/1000 | Loss: 0.00024450
Iteration 82/1000 | Loss: 0.00017003
Iteration 83/1000 | Loss: 0.00004955
Iteration 84/1000 | Loss: 0.00003476
Iteration 85/1000 | Loss: 0.00003169
Iteration 86/1000 | Loss: 0.00003056
Iteration 87/1000 | Loss: 0.00002986
Iteration 88/1000 | Loss: 0.00002942
Iteration 89/1000 | Loss: 0.00002930
Iteration 90/1000 | Loss: 0.00002928
Iteration 91/1000 | Loss: 0.00022982
Iteration 92/1000 | Loss: 0.00003685
Iteration 93/1000 | Loss: 0.00003295
Iteration 94/1000 | Loss: 0.00003145
Iteration 95/1000 | Loss: 0.00003086
Iteration 96/1000 | Loss: 0.00003056
Iteration 97/1000 | Loss: 0.00003025
Iteration 98/1000 | Loss: 0.00002995
Iteration 99/1000 | Loss: 0.00040251
Iteration 100/1000 | Loss: 0.00032811
Iteration 101/1000 | Loss: 0.00021879
Iteration 102/1000 | Loss: 0.00044534
Iteration 103/1000 | Loss: 0.00021998
Iteration 104/1000 | Loss: 0.00027274
Iteration 105/1000 | Loss: 0.00007192
Iteration 106/1000 | Loss: 0.00003912
Iteration 107/1000 | Loss: 0.00008765
Iteration 108/1000 | Loss: 0.00005821
Iteration 109/1000 | Loss: 0.00003379
Iteration 110/1000 | Loss: 0.00013334
Iteration 111/1000 | Loss: 0.00003693
Iteration 112/1000 | Loss: 0.00003378
Iteration 113/1000 | Loss: 0.00004371
Iteration 114/1000 | Loss: 0.00003359
Iteration 115/1000 | Loss: 0.00003087
Iteration 116/1000 | Loss: 0.00003013
Iteration 117/1000 | Loss: 0.00002934
Iteration 118/1000 | Loss: 0.00002898
Iteration 119/1000 | Loss: 0.00002864
Iteration 120/1000 | Loss: 0.00002828
Iteration 121/1000 | Loss: 0.00002797
Iteration 122/1000 | Loss: 0.00002771
Iteration 123/1000 | Loss: 0.00002764
Iteration 124/1000 | Loss: 0.00002763
Iteration 125/1000 | Loss: 0.00002762
Iteration 126/1000 | Loss: 0.00002762
Iteration 127/1000 | Loss: 0.00002758
Iteration 128/1000 | Loss: 0.00002754
Iteration 129/1000 | Loss: 0.00002753
Iteration 130/1000 | Loss: 0.00002753
Iteration 131/1000 | Loss: 0.00002752
Iteration 132/1000 | Loss: 0.00002752
Iteration 133/1000 | Loss: 0.00002751
Iteration 134/1000 | Loss: 0.00002751
Iteration 135/1000 | Loss: 0.00002751
Iteration 136/1000 | Loss: 0.00002750
Iteration 137/1000 | Loss: 0.00002750
Iteration 138/1000 | Loss: 0.00002750
Iteration 139/1000 | Loss: 0.00002749
Iteration 140/1000 | Loss: 0.00002749
Iteration 141/1000 | Loss: 0.00002749
Iteration 142/1000 | Loss: 0.00002749
Iteration 143/1000 | Loss: 0.00002749
Iteration 144/1000 | Loss: 0.00002749
Iteration 145/1000 | Loss: 0.00002748
Iteration 146/1000 | Loss: 0.00002748
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00002747
Iteration 149/1000 | Loss: 0.00002747
Iteration 150/1000 | Loss: 0.00002747
Iteration 151/1000 | Loss: 0.00002747
Iteration 152/1000 | Loss: 0.00002746
Iteration 153/1000 | Loss: 0.00002746
Iteration 154/1000 | Loss: 0.00002746
Iteration 155/1000 | Loss: 0.00002746
Iteration 156/1000 | Loss: 0.00002746
Iteration 157/1000 | Loss: 0.00002746
Iteration 158/1000 | Loss: 0.00002745
Iteration 159/1000 | Loss: 0.00002745
Iteration 160/1000 | Loss: 0.00002745
Iteration 161/1000 | Loss: 0.00002745
Iteration 162/1000 | Loss: 0.00002745
Iteration 163/1000 | Loss: 0.00002745
Iteration 164/1000 | Loss: 0.00002745
Iteration 165/1000 | Loss: 0.00002744
Iteration 166/1000 | Loss: 0.00002744
Iteration 167/1000 | Loss: 0.00002744
Iteration 168/1000 | Loss: 0.00002744
Iteration 169/1000 | Loss: 0.00002743
Iteration 170/1000 | Loss: 0.00002743
Iteration 171/1000 | Loss: 0.00002743
Iteration 172/1000 | Loss: 0.00002743
Iteration 173/1000 | Loss: 0.00002743
Iteration 174/1000 | Loss: 0.00002743
Iteration 175/1000 | Loss: 0.00002743
Iteration 176/1000 | Loss: 0.00002742
Iteration 177/1000 | Loss: 0.00002742
Iteration 178/1000 | Loss: 0.00002742
Iteration 179/1000 | Loss: 0.00002742
Iteration 180/1000 | Loss: 0.00002742
Iteration 181/1000 | Loss: 0.00002742
Iteration 182/1000 | Loss: 0.00002742
Iteration 183/1000 | Loss: 0.00002742
Iteration 184/1000 | Loss: 0.00002742
Iteration 185/1000 | Loss: 0.00002742
Iteration 186/1000 | Loss: 0.00002741
Iteration 187/1000 | Loss: 0.00002741
Iteration 188/1000 | Loss: 0.00002741
Iteration 189/1000 | Loss: 0.00002741
Iteration 190/1000 | Loss: 0.00002741
Iteration 191/1000 | Loss: 0.00002741
Iteration 192/1000 | Loss: 0.00002741
Iteration 193/1000 | Loss: 0.00002741
Iteration 194/1000 | Loss: 0.00002741
Iteration 195/1000 | Loss: 0.00002741
Iteration 196/1000 | Loss: 0.00002741
Iteration 197/1000 | Loss: 0.00002741
Iteration 198/1000 | Loss: 0.00002741
Iteration 199/1000 | Loss: 0.00002741
Iteration 200/1000 | Loss: 0.00002741
Iteration 201/1000 | Loss: 0.00002741
Iteration 202/1000 | Loss: 0.00002741
Iteration 203/1000 | Loss: 0.00002741
Iteration 204/1000 | Loss: 0.00002741
Iteration 205/1000 | Loss: 0.00002741
Iteration 206/1000 | Loss: 0.00002741
Iteration 207/1000 | Loss: 0.00002741
Iteration 208/1000 | Loss: 0.00002741
Iteration 209/1000 | Loss: 0.00002741
Iteration 210/1000 | Loss: 0.00002741
Iteration 211/1000 | Loss: 0.00002741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.7408839741838165e-05, 2.7408839741838165e-05, 2.7408839741838165e-05, 2.7408839741838165e-05, 2.7408839741838165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7408839741838165e-05

Optimization complete. Final v2v error: 4.286094665527344 mm

Highest mean error: 5.219179153442383 mm for frame 88

Lowest mean error: 4.002118110656738 mm for frame 140

Saving results

Total time: 229.47261238098145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01142968
Iteration 2/25 | Loss: 0.01142968
Iteration 3/25 | Loss: 0.00560984
Iteration 4/25 | Loss: 0.00347485
Iteration 5/25 | Loss: 0.00296391
Iteration 6/25 | Loss: 0.00208278
Iteration 7/25 | Loss: 0.00153243
Iteration 8/25 | Loss: 0.00117013
Iteration 9/25 | Loss: 0.00102920
Iteration 10/25 | Loss: 0.00094987
Iteration 11/25 | Loss: 0.00091795
Iteration 12/25 | Loss: 0.00091048
Iteration 13/25 | Loss: 0.00090212
Iteration 14/25 | Loss: 0.00089930
Iteration 15/25 | Loss: 0.00089503
Iteration 16/25 | Loss: 0.00089358
Iteration 17/25 | Loss: 0.00089246
Iteration 18/25 | Loss: 0.00089038
Iteration 19/25 | Loss: 0.00088920
Iteration 20/25 | Loss: 0.00088866
Iteration 21/25 | Loss: 0.00088849
Iteration 22/25 | Loss: 0.00088841
Iteration 23/25 | Loss: 0.00088839
Iteration 24/25 | Loss: 0.00088839
Iteration 25/25 | Loss: 0.00088838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53759205
Iteration 2/25 | Loss: 0.00039833
Iteration 3/25 | Loss: 0.00039833
Iteration 4/25 | Loss: 0.00039833
Iteration 5/25 | Loss: 0.00039833
Iteration 6/25 | Loss: 0.00039832
Iteration 7/25 | Loss: 0.00039832
Iteration 8/25 | Loss: 0.00039832
Iteration 9/25 | Loss: 0.00039832
Iteration 10/25 | Loss: 0.00039832
Iteration 11/25 | Loss: 0.00039832
Iteration 12/25 | Loss: 0.00039832
Iteration 13/25 | Loss: 0.00039832
Iteration 14/25 | Loss: 0.00039832
Iteration 15/25 | Loss: 0.00039832
Iteration 16/25 | Loss: 0.00039832
Iteration 17/25 | Loss: 0.00039832
Iteration 18/25 | Loss: 0.00039832
Iteration 19/25 | Loss: 0.00039832
Iteration 20/25 | Loss: 0.00039832
Iteration 21/25 | Loss: 0.00039832
Iteration 22/25 | Loss: 0.00039832
Iteration 23/25 | Loss: 0.00039832
Iteration 24/25 | Loss: 0.00039832
Iteration 25/25 | Loss: 0.00039832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039832
Iteration 2/1000 | Loss: 0.00009468
Iteration 3/1000 | Loss: 0.00007238
Iteration 4/1000 | Loss: 0.00006436
Iteration 5/1000 | Loss: 0.00005996
Iteration 6/1000 | Loss: 0.00005808
Iteration 7/1000 | Loss: 0.00442804
Iteration 8/1000 | Loss: 0.00084320
Iteration 9/1000 | Loss: 0.00009835
Iteration 10/1000 | Loss: 0.00004867
Iteration 11/1000 | Loss: 0.00004174
Iteration 12/1000 | Loss: 0.00003768
Iteration 13/1000 | Loss: 0.00003519
Iteration 14/1000 | Loss: 0.00003408
Iteration 15/1000 | Loss: 0.00003339
Iteration 16/1000 | Loss: 0.00003293
Iteration 17/1000 | Loss: 0.00003241
Iteration 18/1000 | Loss: 0.00003215
Iteration 19/1000 | Loss: 0.00003197
Iteration 20/1000 | Loss: 0.00003189
Iteration 21/1000 | Loss: 0.00003188
Iteration 22/1000 | Loss: 0.00003188
Iteration 23/1000 | Loss: 0.00003183
Iteration 24/1000 | Loss: 0.00003183
Iteration 25/1000 | Loss: 0.00003183
Iteration 26/1000 | Loss: 0.00003183
Iteration 27/1000 | Loss: 0.00003183
Iteration 28/1000 | Loss: 0.00003182
Iteration 29/1000 | Loss: 0.00003182
Iteration 30/1000 | Loss: 0.00003182
Iteration 31/1000 | Loss: 0.00003182
Iteration 32/1000 | Loss: 0.00003182
Iteration 33/1000 | Loss: 0.00003178
Iteration 34/1000 | Loss: 0.00003176
Iteration 35/1000 | Loss: 0.00003174
Iteration 36/1000 | Loss: 0.00003174
Iteration 37/1000 | Loss: 0.00003173
Iteration 38/1000 | Loss: 0.00003171
Iteration 39/1000 | Loss: 0.00003171
Iteration 40/1000 | Loss: 0.00003171
Iteration 41/1000 | Loss: 0.00003171
Iteration 42/1000 | Loss: 0.00003171
Iteration 43/1000 | Loss: 0.00003171
Iteration 44/1000 | Loss: 0.00003171
Iteration 45/1000 | Loss: 0.00003171
Iteration 46/1000 | Loss: 0.00003171
Iteration 47/1000 | Loss: 0.00003171
Iteration 48/1000 | Loss: 0.00003170
Iteration 49/1000 | Loss: 0.00003170
Iteration 50/1000 | Loss: 0.00003170
Iteration 51/1000 | Loss: 0.00003170
Iteration 52/1000 | Loss: 0.00003170
Iteration 53/1000 | Loss: 0.00003170
Iteration 54/1000 | Loss: 0.00003170
Iteration 55/1000 | Loss: 0.00003170
Iteration 56/1000 | Loss: 0.00003169
Iteration 57/1000 | Loss: 0.00003169
Iteration 58/1000 | Loss: 0.00003169
Iteration 59/1000 | Loss: 0.00003169
Iteration 60/1000 | Loss: 0.00003168
Iteration 61/1000 | Loss: 0.00003168
Iteration 62/1000 | Loss: 0.00003168
Iteration 63/1000 | Loss: 0.00003168
Iteration 64/1000 | Loss: 0.00003168
Iteration 65/1000 | Loss: 0.00003168
Iteration 66/1000 | Loss: 0.00003168
Iteration 67/1000 | Loss: 0.00003168
Iteration 68/1000 | Loss: 0.00003168
Iteration 69/1000 | Loss: 0.00003167
Iteration 70/1000 | Loss: 0.00003167
Iteration 71/1000 | Loss: 0.00003167
Iteration 72/1000 | Loss: 0.00003167
Iteration 73/1000 | Loss: 0.00003167
Iteration 74/1000 | Loss: 0.00003167
Iteration 75/1000 | Loss: 0.00003166
Iteration 76/1000 | Loss: 0.00003166
Iteration 77/1000 | Loss: 0.00003166
Iteration 78/1000 | Loss: 0.00003166
Iteration 79/1000 | Loss: 0.00003166
Iteration 80/1000 | Loss: 0.00003166
Iteration 81/1000 | Loss: 0.00003166
Iteration 82/1000 | Loss: 0.00003166
Iteration 83/1000 | Loss: 0.00003166
Iteration 84/1000 | Loss: 0.00003164
Iteration 85/1000 | Loss: 0.00003164
Iteration 86/1000 | Loss: 0.00003164
Iteration 87/1000 | Loss: 0.00003164
Iteration 88/1000 | Loss: 0.00003164
Iteration 89/1000 | Loss: 0.00003164
Iteration 90/1000 | Loss: 0.00003164
Iteration 91/1000 | Loss: 0.00003164
Iteration 92/1000 | Loss: 0.00003164
Iteration 93/1000 | Loss: 0.00003164
Iteration 94/1000 | Loss: 0.00003164
Iteration 95/1000 | Loss: 0.00003164
Iteration 96/1000 | Loss: 0.00003164
Iteration 97/1000 | Loss: 0.00003164
Iteration 98/1000 | Loss: 0.00003164
Iteration 99/1000 | Loss: 0.00003164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [3.163629298796877e-05, 3.163629298796877e-05, 3.163629298796877e-05, 3.163629298796877e-05, 3.163629298796877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.163629298796877e-05

Optimization complete. Final v2v error: 4.773128986358643 mm

Highest mean error: 4.930168151855469 mm for frame 145

Lowest mean error: 4.53286600112915 mm for frame 15

Saving results

Total time: 81.09872770309448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871029
Iteration 2/25 | Loss: 0.00084137
Iteration 3/25 | Loss: 0.00066253
Iteration 4/25 | Loss: 0.00062097
Iteration 5/25 | Loss: 0.00061280
Iteration 6/25 | Loss: 0.00061165
Iteration 7/25 | Loss: 0.00061165
Iteration 8/25 | Loss: 0.00061165
Iteration 9/25 | Loss: 0.00061165
Iteration 10/25 | Loss: 0.00061165
Iteration 11/25 | Loss: 0.00061165
Iteration 12/25 | Loss: 0.00061165
Iteration 13/25 | Loss: 0.00061165
Iteration 14/25 | Loss: 0.00061165
Iteration 15/25 | Loss: 0.00061165
Iteration 16/25 | Loss: 0.00061165
Iteration 17/25 | Loss: 0.00061165
Iteration 18/25 | Loss: 0.00061165
Iteration 19/25 | Loss: 0.00061165
Iteration 20/25 | Loss: 0.00061165
Iteration 21/25 | Loss: 0.00061165
Iteration 22/25 | Loss: 0.00061165
Iteration 23/25 | Loss: 0.00061165
Iteration 24/25 | Loss: 0.00061165
Iteration 25/25 | Loss: 0.00061165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77229619
Iteration 2/25 | Loss: 0.00014091
Iteration 3/25 | Loss: 0.00014091
Iteration 4/25 | Loss: 0.00014091
Iteration 5/25 | Loss: 0.00014091
Iteration 6/25 | Loss: 0.00014091
Iteration 7/25 | Loss: 0.00014091
Iteration 8/25 | Loss: 0.00014091
Iteration 9/25 | Loss: 0.00014091
Iteration 10/25 | Loss: 0.00014091
Iteration 11/25 | Loss: 0.00014091
Iteration 12/25 | Loss: 0.00014091
Iteration 13/25 | Loss: 0.00014091
Iteration 14/25 | Loss: 0.00014091
Iteration 15/25 | Loss: 0.00014091
Iteration 16/25 | Loss: 0.00014091
Iteration 17/25 | Loss: 0.00014091
Iteration 18/25 | Loss: 0.00014091
Iteration 19/25 | Loss: 0.00014091
Iteration 20/25 | Loss: 0.00014091
Iteration 21/25 | Loss: 0.00014091
Iteration 22/25 | Loss: 0.00014091
Iteration 23/25 | Loss: 0.00014091
Iteration 24/25 | Loss: 0.00014091
Iteration 25/25 | Loss: 0.00014091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014091
Iteration 2/1000 | Loss: 0.00004154
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00002557
Iteration 5/1000 | Loss: 0.00002332
Iteration 6/1000 | Loss: 0.00002228
Iteration 7/1000 | Loss: 0.00002166
Iteration 8/1000 | Loss: 0.00002125
Iteration 9/1000 | Loss: 0.00002102
Iteration 10/1000 | Loss: 0.00002100
Iteration 11/1000 | Loss: 0.00002079
Iteration 12/1000 | Loss: 0.00002074
Iteration 13/1000 | Loss: 0.00002071
Iteration 14/1000 | Loss: 0.00002070
Iteration 15/1000 | Loss: 0.00002069
Iteration 16/1000 | Loss: 0.00002068
Iteration 17/1000 | Loss: 0.00002068
Iteration 18/1000 | Loss: 0.00002067
Iteration 19/1000 | Loss: 0.00002067
Iteration 20/1000 | Loss: 0.00002066
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002064
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002062
Iteration 25/1000 | Loss: 0.00002061
Iteration 26/1000 | Loss: 0.00002060
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002055
Iteration 29/1000 | Loss: 0.00002054
Iteration 30/1000 | Loss: 0.00002053
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002048
Iteration 33/1000 | Loss: 0.00002048
Iteration 34/1000 | Loss: 0.00002047
Iteration 35/1000 | Loss: 0.00002045
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002042
Iteration 38/1000 | Loss: 0.00002042
Iteration 39/1000 | Loss: 0.00002040
Iteration 40/1000 | Loss: 0.00002040
Iteration 41/1000 | Loss: 0.00002039
Iteration 42/1000 | Loss: 0.00002039
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002037
Iteration 47/1000 | Loss: 0.00002037
Iteration 48/1000 | Loss: 0.00002037
Iteration 49/1000 | Loss: 0.00002036
Iteration 50/1000 | Loss: 0.00002036
Iteration 51/1000 | Loss: 0.00002036
Iteration 52/1000 | Loss: 0.00002036
Iteration 53/1000 | Loss: 0.00002035
Iteration 54/1000 | Loss: 0.00002035
Iteration 55/1000 | Loss: 0.00002035
Iteration 56/1000 | Loss: 0.00002035
Iteration 57/1000 | Loss: 0.00002035
Iteration 58/1000 | Loss: 0.00002034
Iteration 59/1000 | Loss: 0.00002034
Iteration 60/1000 | Loss: 0.00002034
Iteration 61/1000 | Loss: 0.00002034
Iteration 62/1000 | Loss: 0.00002034
Iteration 63/1000 | Loss: 0.00002034
Iteration 64/1000 | Loss: 0.00002034
Iteration 65/1000 | Loss: 0.00002033
Iteration 66/1000 | Loss: 0.00002033
Iteration 67/1000 | Loss: 0.00002033
Iteration 68/1000 | Loss: 0.00002033
Iteration 69/1000 | Loss: 0.00002033
Iteration 70/1000 | Loss: 0.00002033
Iteration 71/1000 | Loss: 0.00002033
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002032
Iteration 75/1000 | Loss: 0.00002032
Iteration 76/1000 | Loss: 0.00002032
Iteration 77/1000 | Loss: 0.00002032
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002031
Iteration 82/1000 | Loss: 0.00002031
Iteration 83/1000 | Loss: 0.00002031
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002029
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002028
Iteration 92/1000 | Loss: 0.00002028
Iteration 93/1000 | Loss: 0.00002028
Iteration 94/1000 | Loss: 0.00002028
Iteration 95/1000 | Loss: 0.00002028
Iteration 96/1000 | Loss: 0.00002028
Iteration 97/1000 | Loss: 0.00002027
Iteration 98/1000 | Loss: 0.00002027
Iteration 99/1000 | Loss: 0.00002027
Iteration 100/1000 | Loss: 0.00002027
Iteration 101/1000 | Loss: 0.00002027
Iteration 102/1000 | Loss: 0.00002027
Iteration 103/1000 | Loss: 0.00002027
Iteration 104/1000 | Loss: 0.00002027
Iteration 105/1000 | Loss: 0.00002027
Iteration 106/1000 | Loss: 0.00002027
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002026
Iteration 113/1000 | Loss: 0.00002026
Iteration 114/1000 | Loss: 0.00002026
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002026
Iteration 121/1000 | Loss: 0.00002026
Iteration 122/1000 | Loss: 0.00002026
Iteration 123/1000 | Loss: 0.00002026
Iteration 124/1000 | Loss: 0.00002026
Iteration 125/1000 | Loss: 0.00002026
Iteration 126/1000 | Loss: 0.00002026
Iteration 127/1000 | Loss: 0.00002025
Iteration 128/1000 | Loss: 0.00002025
Iteration 129/1000 | Loss: 0.00002025
Iteration 130/1000 | Loss: 0.00002025
Iteration 131/1000 | Loss: 0.00002025
Iteration 132/1000 | Loss: 0.00002025
Iteration 133/1000 | Loss: 0.00002025
Iteration 134/1000 | Loss: 0.00002025
Iteration 135/1000 | Loss: 0.00002025
Iteration 136/1000 | Loss: 0.00002025
Iteration 137/1000 | Loss: 0.00002025
Iteration 138/1000 | Loss: 0.00002025
Iteration 139/1000 | Loss: 0.00002025
Iteration 140/1000 | Loss: 0.00002025
Iteration 141/1000 | Loss: 0.00002025
Iteration 142/1000 | Loss: 0.00002025
Iteration 143/1000 | Loss: 0.00002025
Iteration 144/1000 | Loss: 0.00002025
Iteration 145/1000 | Loss: 0.00002025
Iteration 146/1000 | Loss: 0.00002025
Iteration 147/1000 | Loss: 0.00002025
Iteration 148/1000 | Loss: 0.00002025
Iteration 149/1000 | Loss: 0.00002025
Iteration 150/1000 | Loss: 0.00002025
Iteration 151/1000 | Loss: 0.00002025
Iteration 152/1000 | Loss: 0.00002025
Iteration 153/1000 | Loss: 0.00002025
Iteration 154/1000 | Loss: 0.00002025
Iteration 155/1000 | Loss: 0.00002025
Iteration 156/1000 | Loss: 0.00002025
Iteration 157/1000 | Loss: 0.00002025
Iteration 158/1000 | Loss: 0.00002025
Iteration 159/1000 | Loss: 0.00002025
Iteration 160/1000 | Loss: 0.00002025
Iteration 161/1000 | Loss: 0.00002025
Iteration 162/1000 | Loss: 0.00002025
Iteration 163/1000 | Loss: 0.00002025
Iteration 164/1000 | Loss: 0.00002025
Iteration 165/1000 | Loss: 0.00002025
Iteration 166/1000 | Loss: 0.00002025
Iteration 167/1000 | Loss: 0.00002025
Iteration 168/1000 | Loss: 0.00002025
Iteration 169/1000 | Loss: 0.00002025
Iteration 170/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.024515015364159e-05, 2.024515015364159e-05, 2.024515015364159e-05, 2.024515015364159e-05, 2.024515015364159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.024515015364159e-05

Optimization complete. Final v2v error: 3.7421720027923584 mm

Highest mean error: 4.336286544799805 mm for frame 115

Lowest mean error: 3.438751220703125 mm for frame 74

Saving results

Total time: 38.407368898391724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565186
Iteration 2/25 | Loss: 0.00144561
Iteration 3/25 | Loss: 0.00083982
Iteration 4/25 | Loss: 0.00074261
Iteration 5/25 | Loss: 0.00072840
Iteration 6/25 | Loss: 0.00072359
Iteration 7/25 | Loss: 0.00072223
Iteration 8/25 | Loss: 0.00072198
Iteration 9/25 | Loss: 0.00072198
Iteration 10/25 | Loss: 0.00072198
Iteration 11/25 | Loss: 0.00072198
Iteration 12/25 | Loss: 0.00072198
Iteration 13/25 | Loss: 0.00072198
Iteration 14/25 | Loss: 0.00072198
Iteration 15/25 | Loss: 0.00072198
Iteration 16/25 | Loss: 0.00072198
Iteration 17/25 | Loss: 0.00072198
Iteration 18/25 | Loss: 0.00072198
Iteration 19/25 | Loss: 0.00072198
Iteration 20/25 | Loss: 0.00072198
Iteration 21/25 | Loss: 0.00072198
Iteration 22/25 | Loss: 0.00072198
Iteration 23/25 | Loss: 0.00072198
Iteration 24/25 | Loss: 0.00072198
Iteration 25/25 | Loss: 0.00072198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000721976684872061, 0.000721976684872061, 0.000721976684872061, 0.000721976684872061, 0.000721976684872061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000721976684872061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97310787
Iteration 2/25 | Loss: 0.00024340
Iteration 3/25 | Loss: 0.00024339
Iteration 4/25 | Loss: 0.00024338
Iteration 5/25 | Loss: 0.00024338
Iteration 6/25 | Loss: 0.00024338
Iteration 7/25 | Loss: 0.00024338
Iteration 8/25 | Loss: 0.00024338
Iteration 9/25 | Loss: 0.00024338
Iteration 10/25 | Loss: 0.00024338
Iteration 11/25 | Loss: 0.00024338
Iteration 12/25 | Loss: 0.00024338
Iteration 13/25 | Loss: 0.00024338
Iteration 14/25 | Loss: 0.00024338
Iteration 15/25 | Loss: 0.00024338
Iteration 16/25 | Loss: 0.00024338
Iteration 17/25 | Loss: 0.00024338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00024338244111277163, 0.00024338244111277163, 0.00024338244111277163, 0.00024338244111277163, 0.00024338244111277163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024338244111277163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024338
Iteration 2/1000 | Loss: 0.00005487
Iteration 3/1000 | Loss: 0.00003494
Iteration 4/1000 | Loss: 0.00003247
Iteration 5/1000 | Loss: 0.00003124
Iteration 6/1000 | Loss: 0.00003057
Iteration 7/1000 | Loss: 0.00002997
Iteration 8/1000 | Loss: 0.00002951
Iteration 9/1000 | Loss: 0.00002923
Iteration 10/1000 | Loss: 0.00002899
Iteration 11/1000 | Loss: 0.00002874
Iteration 12/1000 | Loss: 0.00002855
Iteration 13/1000 | Loss: 0.00002845
Iteration 14/1000 | Loss: 0.00002838
Iteration 15/1000 | Loss: 0.00002831
Iteration 16/1000 | Loss: 0.00002828
Iteration 17/1000 | Loss: 0.00002815
Iteration 18/1000 | Loss: 0.00002809
Iteration 19/1000 | Loss: 0.00002805
Iteration 20/1000 | Loss: 0.00002804
Iteration 21/1000 | Loss: 0.00002802
Iteration 22/1000 | Loss: 0.00002798
Iteration 23/1000 | Loss: 0.00002793
Iteration 24/1000 | Loss: 0.00002791
Iteration 25/1000 | Loss: 0.00002791
Iteration 26/1000 | Loss: 0.00002790
Iteration 27/1000 | Loss: 0.00002790
Iteration 28/1000 | Loss: 0.00002789
Iteration 29/1000 | Loss: 0.00002789
Iteration 30/1000 | Loss: 0.00002789
Iteration 31/1000 | Loss: 0.00002788
Iteration 32/1000 | Loss: 0.00002788
Iteration 33/1000 | Loss: 0.00002787
Iteration 34/1000 | Loss: 0.00002786
Iteration 35/1000 | Loss: 0.00002785
Iteration 36/1000 | Loss: 0.00002785
Iteration 37/1000 | Loss: 0.00002784
Iteration 38/1000 | Loss: 0.00002784
Iteration 39/1000 | Loss: 0.00002783
Iteration 40/1000 | Loss: 0.00002779
Iteration 41/1000 | Loss: 0.00002779
Iteration 42/1000 | Loss: 0.00002778
Iteration 43/1000 | Loss: 0.00002776
Iteration 44/1000 | Loss: 0.00002775
Iteration 45/1000 | Loss: 0.00002775
Iteration 46/1000 | Loss: 0.00002774
Iteration 47/1000 | Loss: 0.00002773
Iteration 48/1000 | Loss: 0.00002772
Iteration 49/1000 | Loss: 0.00002771
Iteration 50/1000 | Loss: 0.00002771
Iteration 51/1000 | Loss: 0.00002770
Iteration 52/1000 | Loss: 0.00002770
Iteration 53/1000 | Loss: 0.00002770
Iteration 54/1000 | Loss: 0.00002769
Iteration 55/1000 | Loss: 0.00002768
Iteration 56/1000 | Loss: 0.00002767
Iteration 57/1000 | Loss: 0.00002766
Iteration 58/1000 | Loss: 0.00002766
Iteration 59/1000 | Loss: 0.00002766
Iteration 60/1000 | Loss: 0.00002765
Iteration 61/1000 | Loss: 0.00002765
Iteration 62/1000 | Loss: 0.00002764
Iteration 63/1000 | Loss: 0.00002764
Iteration 64/1000 | Loss: 0.00002764
Iteration 65/1000 | Loss: 0.00002763
Iteration 66/1000 | Loss: 0.00002763
Iteration 67/1000 | Loss: 0.00002763
Iteration 68/1000 | Loss: 0.00002763
Iteration 69/1000 | Loss: 0.00002762
Iteration 70/1000 | Loss: 0.00002762
Iteration 71/1000 | Loss: 0.00002762
Iteration 72/1000 | Loss: 0.00002762
Iteration 73/1000 | Loss: 0.00002762
Iteration 74/1000 | Loss: 0.00002762
Iteration 75/1000 | Loss: 0.00002762
Iteration 76/1000 | Loss: 0.00002762
Iteration 77/1000 | Loss: 0.00002762
Iteration 78/1000 | Loss: 0.00002762
Iteration 79/1000 | Loss: 0.00002762
Iteration 80/1000 | Loss: 0.00002761
Iteration 81/1000 | Loss: 0.00002761
Iteration 82/1000 | Loss: 0.00002761
Iteration 83/1000 | Loss: 0.00002761
Iteration 84/1000 | Loss: 0.00002760
Iteration 85/1000 | Loss: 0.00002760
Iteration 86/1000 | Loss: 0.00002760
Iteration 87/1000 | Loss: 0.00002760
Iteration 88/1000 | Loss: 0.00002759
Iteration 89/1000 | Loss: 0.00002759
Iteration 90/1000 | Loss: 0.00002759
Iteration 91/1000 | Loss: 0.00002759
Iteration 92/1000 | Loss: 0.00002759
Iteration 93/1000 | Loss: 0.00002759
Iteration 94/1000 | Loss: 0.00002759
Iteration 95/1000 | Loss: 0.00002758
Iteration 96/1000 | Loss: 0.00002758
Iteration 97/1000 | Loss: 0.00002758
Iteration 98/1000 | Loss: 0.00002758
Iteration 99/1000 | Loss: 0.00002758
Iteration 100/1000 | Loss: 0.00002757
Iteration 101/1000 | Loss: 0.00002757
Iteration 102/1000 | Loss: 0.00002757
Iteration 103/1000 | Loss: 0.00002757
Iteration 104/1000 | Loss: 0.00002757
Iteration 105/1000 | Loss: 0.00002756
Iteration 106/1000 | Loss: 0.00002756
Iteration 107/1000 | Loss: 0.00002756
Iteration 108/1000 | Loss: 0.00002755
Iteration 109/1000 | Loss: 0.00002755
Iteration 110/1000 | Loss: 0.00002754
Iteration 111/1000 | Loss: 0.00002754
Iteration 112/1000 | Loss: 0.00002754
Iteration 113/1000 | Loss: 0.00002754
Iteration 114/1000 | Loss: 0.00002753
Iteration 115/1000 | Loss: 0.00002753
Iteration 116/1000 | Loss: 0.00002753
Iteration 117/1000 | Loss: 0.00002753
Iteration 118/1000 | Loss: 0.00002752
Iteration 119/1000 | Loss: 0.00002752
Iteration 120/1000 | Loss: 0.00002751
Iteration 121/1000 | Loss: 0.00002751
Iteration 122/1000 | Loss: 0.00002751
Iteration 123/1000 | Loss: 0.00002751
Iteration 124/1000 | Loss: 0.00002750
Iteration 125/1000 | Loss: 0.00002750
Iteration 126/1000 | Loss: 0.00002750
Iteration 127/1000 | Loss: 0.00002749
Iteration 128/1000 | Loss: 0.00002749
Iteration 129/1000 | Loss: 0.00002749
Iteration 130/1000 | Loss: 0.00002748
Iteration 131/1000 | Loss: 0.00002748
Iteration 132/1000 | Loss: 0.00002748
Iteration 133/1000 | Loss: 0.00002748
Iteration 134/1000 | Loss: 0.00002748
Iteration 135/1000 | Loss: 0.00002748
Iteration 136/1000 | Loss: 0.00002748
Iteration 137/1000 | Loss: 0.00002748
Iteration 138/1000 | Loss: 0.00002748
Iteration 139/1000 | Loss: 0.00002747
Iteration 140/1000 | Loss: 0.00002747
Iteration 141/1000 | Loss: 0.00002747
Iteration 142/1000 | Loss: 0.00002747
Iteration 143/1000 | Loss: 0.00002747
Iteration 144/1000 | Loss: 0.00002747
Iteration 145/1000 | Loss: 0.00002747
Iteration 146/1000 | Loss: 0.00002747
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00002747
Iteration 149/1000 | Loss: 0.00002747
Iteration 150/1000 | Loss: 0.00002747
Iteration 151/1000 | Loss: 0.00002747
Iteration 152/1000 | Loss: 0.00002747
Iteration 153/1000 | Loss: 0.00002747
Iteration 154/1000 | Loss: 0.00002747
Iteration 155/1000 | Loss: 0.00002747
Iteration 156/1000 | Loss: 0.00002747
Iteration 157/1000 | Loss: 0.00002746
Iteration 158/1000 | Loss: 0.00002746
Iteration 159/1000 | Loss: 0.00002746
Iteration 160/1000 | Loss: 0.00002745
Iteration 161/1000 | Loss: 0.00002745
Iteration 162/1000 | Loss: 0.00002745
Iteration 163/1000 | Loss: 0.00002745
Iteration 164/1000 | Loss: 0.00002745
Iteration 165/1000 | Loss: 0.00002745
Iteration 166/1000 | Loss: 0.00002744
Iteration 167/1000 | Loss: 0.00002744
Iteration 168/1000 | Loss: 0.00002744
Iteration 169/1000 | Loss: 0.00002743
Iteration 170/1000 | Loss: 0.00002743
Iteration 171/1000 | Loss: 0.00002743
Iteration 172/1000 | Loss: 0.00002743
Iteration 173/1000 | Loss: 0.00002743
Iteration 174/1000 | Loss: 0.00002743
Iteration 175/1000 | Loss: 0.00002743
Iteration 176/1000 | Loss: 0.00002742
Iteration 177/1000 | Loss: 0.00002742
Iteration 178/1000 | Loss: 0.00002742
Iteration 179/1000 | Loss: 0.00002742
Iteration 180/1000 | Loss: 0.00002742
Iteration 181/1000 | Loss: 0.00002742
Iteration 182/1000 | Loss: 0.00002741
Iteration 183/1000 | Loss: 0.00002741
Iteration 184/1000 | Loss: 0.00002741
Iteration 185/1000 | Loss: 0.00002741
Iteration 186/1000 | Loss: 0.00002741
Iteration 187/1000 | Loss: 0.00002741
Iteration 188/1000 | Loss: 0.00002741
Iteration 189/1000 | Loss: 0.00002741
Iteration 190/1000 | Loss: 0.00002741
Iteration 191/1000 | Loss: 0.00002741
Iteration 192/1000 | Loss: 0.00002740
Iteration 193/1000 | Loss: 0.00002740
Iteration 194/1000 | Loss: 0.00002740
Iteration 195/1000 | Loss: 0.00002740
Iteration 196/1000 | Loss: 0.00002740
Iteration 197/1000 | Loss: 0.00002740
Iteration 198/1000 | Loss: 0.00002740
Iteration 199/1000 | Loss: 0.00002740
Iteration 200/1000 | Loss: 0.00002740
Iteration 201/1000 | Loss: 0.00002740
Iteration 202/1000 | Loss: 0.00002739
Iteration 203/1000 | Loss: 0.00002739
Iteration 204/1000 | Loss: 0.00002739
Iteration 205/1000 | Loss: 0.00002739
Iteration 206/1000 | Loss: 0.00002739
Iteration 207/1000 | Loss: 0.00002739
Iteration 208/1000 | Loss: 0.00002739
Iteration 209/1000 | Loss: 0.00002739
Iteration 210/1000 | Loss: 0.00002739
Iteration 211/1000 | Loss: 0.00002739
Iteration 212/1000 | Loss: 0.00002739
Iteration 213/1000 | Loss: 0.00002739
Iteration 214/1000 | Loss: 0.00002739
Iteration 215/1000 | Loss: 0.00002739
Iteration 216/1000 | Loss: 0.00002739
Iteration 217/1000 | Loss: 0.00002739
Iteration 218/1000 | Loss: 0.00002739
Iteration 219/1000 | Loss: 0.00002739
Iteration 220/1000 | Loss: 0.00002739
Iteration 221/1000 | Loss: 0.00002739
Iteration 222/1000 | Loss: 0.00002739
Iteration 223/1000 | Loss: 0.00002739
Iteration 224/1000 | Loss: 0.00002739
Iteration 225/1000 | Loss: 0.00002739
Iteration 226/1000 | Loss: 0.00002739
Iteration 227/1000 | Loss: 0.00002739
Iteration 228/1000 | Loss: 0.00002739
Iteration 229/1000 | Loss: 0.00002739
Iteration 230/1000 | Loss: 0.00002739
Iteration 231/1000 | Loss: 0.00002739
Iteration 232/1000 | Loss: 0.00002739
Iteration 233/1000 | Loss: 0.00002739
Iteration 234/1000 | Loss: 0.00002739
Iteration 235/1000 | Loss: 0.00002739
Iteration 236/1000 | Loss: 0.00002739
Iteration 237/1000 | Loss: 0.00002739
Iteration 238/1000 | Loss: 0.00002739
Iteration 239/1000 | Loss: 0.00002739
Iteration 240/1000 | Loss: 0.00002739
Iteration 241/1000 | Loss: 0.00002739
Iteration 242/1000 | Loss: 0.00002739
Iteration 243/1000 | Loss: 0.00002739
Iteration 244/1000 | Loss: 0.00002739
Iteration 245/1000 | Loss: 0.00002739
Iteration 246/1000 | Loss: 0.00002739
Iteration 247/1000 | Loss: 0.00002739
Iteration 248/1000 | Loss: 0.00002739
Iteration 249/1000 | Loss: 0.00002739
Iteration 250/1000 | Loss: 0.00002739
Iteration 251/1000 | Loss: 0.00002739
Iteration 252/1000 | Loss: 0.00002739
Iteration 253/1000 | Loss: 0.00002739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [2.7387126465328038e-05, 2.7387126465328038e-05, 2.7387126465328038e-05, 2.7387126465328038e-05, 2.7387126465328038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7387126465328038e-05

Optimization complete. Final v2v error: 4.0573039054870605 mm

Highest mean error: 5.393350124359131 mm for frame 102

Lowest mean error: 2.8539304733276367 mm for frame 0

Saving results

Total time: 54.66472244262695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097613
Iteration 2/25 | Loss: 0.00287105
Iteration 3/25 | Loss: 0.00170389
Iteration 4/25 | Loss: 0.00146555
Iteration 5/25 | Loss: 0.00139160
Iteration 6/25 | Loss: 0.00119815
Iteration 7/25 | Loss: 0.00112788
Iteration 8/25 | Loss: 0.00108528
Iteration 9/25 | Loss: 0.00105570
Iteration 10/25 | Loss: 0.00103305
Iteration 11/25 | Loss: 0.00102085
Iteration 12/25 | Loss: 0.00100658
Iteration 13/25 | Loss: 0.00100115
Iteration 14/25 | Loss: 0.00099030
Iteration 15/25 | Loss: 0.00098346
Iteration 16/25 | Loss: 0.00097847
Iteration 17/25 | Loss: 0.00098153
Iteration 18/25 | Loss: 0.00098124
Iteration 19/25 | Loss: 0.00097141
Iteration 20/25 | Loss: 0.00096710
Iteration 21/25 | Loss: 0.00095347
Iteration 22/25 | Loss: 0.00094913
Iteration 23/25 | Loss: 0.00094484
Iteration 24/25 | Loss: 0.00095073
Iteration 25/25 | Loss: 0.00094780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.86283326
Iteration 2/25 | Loss: 0.00244716
Iteration 3/25 | Loss: 0.00244716
Iteration 4/25 | Loss: 0.00244716
Iteration 5/25 | Loss: 0.00244716
Iteration 6/25 | Loss: 0.00244716
Iteration 7/25 | Loss: 0.00244716
Iteration 8/25 | Loss: 0.00244716
Iteration 9/25 | Loss: 0.00244716
Iteration 10/25 | Loss: 0.00244716
Iteration 11/25 | Loss: 0.00244716
Iteration 12/25 | Loss: 0.00244716
Iteration 13/25 | Loss: 0.00244716
Iteration 14/25 | Loss: 0.00244716
Iteration 15/25 | Loss: 0.00244716
Iteration 16/25 | Loss: 0.00244716
Iteration 17/25 | Loss: 0.00244716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0024471606593579054, 0.0024471606593579054, 0.0024471606593579054, 0.0024471606593579054, 0.0024471606593579054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024471606593579054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244716
Iteration 2/1000 | Loss: 0.00042712
Iteration 3/1000 | Loss: 0.00032748
Iteration 4/1000 | Loss: 0.00132299
Iteration 5/1000 | Loss: 0.00074779
Iteration 6/1000 | Loss: 0.00026266
Iteration 7/1000 | Loss: 0.00078123
Iteration 8/1000 | Loss: 0.00042788
Iteration 9/1000 | Loss: 0.00020813
Iteration 10/1000 | Loss: 0.00018719
Iteration 11/1000 | Loss: 0.00017311
Iteration 12/1000 | Loss: 0.00089354
Iteration 13/1000 | Loss: 0.00053214
Iteration 14/1000 | Loss: 0.00145425
Iteration 15/1000 | Loss: 0.00062832
Iteration 16/1000 | Loss: 0.00084496
Iteration 17/1000 | Loss: 0.00051788
Iteration 18/1000 | Loss: 0.00051556
Iteration 19/1000 | Loss: 0.00053225
Iteration 20/1000 | Loss: 0.00053724
Iteration 21/1000 | Loss: 0.00047788
Iteration 22/1000 | Loss: 0.00052822
Iteration 23/1000 | Loss: 0.00023295
Iteration 24/1000 | Loss: 0.00015086
Iteration 25/1000 | Loss: 0.00013141
Iteration 26/1000 | Loss: 0.00012305
Iteration 27/1000 | Loss: 0.00011787
Iteration 28/1000 | Loss: 0.00011367
Iteration 29/1000 | Loss: 0.00011074
Iteration 30/1000 | Loss: 0.00010842
Iteration 31/1000 | Loss: 0.00010630
Iteration 32/1000 | Loss: 0.00010427
Iteration 33/1000 | Loss: 0.00010294
Iteration 34/1000 | Loss: 0.00010574
Iteration 35/1000 | Loss: 0.00010066
Iteration 36/1000 | Loss: 0.00009944
Iteration 37/1000 | Loss: 0.00081510
Iteration 38/1000 | Loss: 0.01359900
Iteration 39/1000 | Loss: 0.00548867
Iteration 40/1000 | Loss: 0.00749769
Iteration 41/1000 | Loss: 0.00202773
Iteration 42/1000 | Loss: 0.00212272
Iteration 43/1000 | Loss: 0.00229692
Iteration 44/1000 | Loss: 0.00160554
Iteration 45/1000 | Loss: 0.00600989
Iteration 46/1000 | Loss: 0.00541159
Iteration 47/1000 | Loss: 0.00467906
Iteration 48/1000 | Loss: 0.00188348
Iteration 49/1000 | Loss: 0.00283591
Iteration 50/1000 | Loss: 0.00264192
Iteration 51/1000 | Loss: 0.00247456
Iteration 52/1000 | Loss: 0.00337989
Iteration 53/1000 | Loss: 0.00381927
Iteration 54/1000 | Loss: 0.00338731
Iteration 55/1000 | Loss: 0.00373670
Iteration 56/1000 | Loss: 0.00451670
Iteration 57/1000 | Loss: 0.00310135
Iteration 58/1000 | Loss: 0.00490502
Iteration 59/1000 | Loss: 0.00461994
Iteration 60/1000 | Loss: 0.00378714
Iteration 61/1000 | Loss: 0.00458032
Iteration 62/1000 | Loss: 0.00454759
Iteration 63/1000 | Loss: 0.00404565
Iteration 64/1000 | Loss: 0.00442921
Iteration 65/1000 | Loss: 0.00436806
Iteration 66/1000 | Loss: 0.00396013
Iteration 67/1000 | Loss: 0.00370332
Iteration 68/1000 | Loss: 0.00786067
Iteration 69/1000 | Loss: 0.00447682
Iteration 70/1000 | Loss: 0.00495086
Iteration 71/1000 | Loss: 0.00394078
Iteration 72/1000 | Loss: 0.00267470
Iteration 73/1000 | Loss: 0.00265514
Iteration 74/1000 | Loss: 0.00558536
Iteration 75/1000 | Loss: 0.00280886
Iteration 76/1000 | Loss: 0.00336803
Iteration 77/1000 | Loss: 0.00597451
Iteration 78/1000 | Loss: 0.00474875
Iteration 79/1000 | Loss: 0.00401405
Iteration 80/1000 | Loss: 0.00287432
Iteration 81/1000 | Loss: 0.00356039
Iteration 82/1000 | Loss: 0.00314173
Iteration 83/1000 | Loss: 0.00223528
Iteration 84/1000 | Loss: 0.00222937
Iteration 85/1000 | Loss: 0.00206324
Iteration 86/1000 | Loss: 0.00256377
Iteration 87/1000 | Loss: 0.00380735
Iteration 88/1000 | Loss: 0.00374373
Iteration 89/1000 | Loss: 0.00413038
Iteration 90/1000 | Loss: 0.00355792
Iteration 91/1000 | Loss: 0.00496382
Iteration 92/1000 | Loss: 0.00244386
Iteration 93/1000 | Loss: 0.00248289
Iteration 94/1000 | Loss: 0.00149846
Iteration 95/1000 | Loss: 0.00222120
Iteration 96/1000 | Loss: 0.00196813
Iteration 97/1000 | Loss: 0.00179927
Iteration 98/1000 | Loss: 0.00265444
Iteration 99/1000 | Loss: 0.00279530
Iteration 100/1000 | Loss: 0.00507253
Iteration 101/1000 | Loss: 0.00458419
Iteration 102/1000 | Loss: 0.00250479
Iteration 103/1000 | Loss: 0.00212376
Iteration 104/1000 | Loss: 0.00254900
Iteration 105/1000 | Loss: 0.00187419
Iteration 106/1000 | Loss: 0.00158471
Iteration 107/1000 | Loss: 0.00154901
Iteration 108/1000 | Loss: 0.00153453
Iteration 109/1000 | Loss: 0.00163070
Iteration 110/1000 | Loss: 0.00065168
Iteration 111/1000 | Loss: 0.00075137
Iteration 112/1000 | Loss: 0.00135399
Iteration 113/1000 | Loss: 0.00292807
Iteration 114/1000 | Loss: 0.00107364
Iteration 115/1000 | Loss: 0.00157121
Iteration 116/1000 | Loss: 0.00510720
Iteration 117/1000 | Loss: 0.00142650
Iteration 118/1000 | Loss: 0.00126079
Iteration 119/1000 | Loss: 0.00082927
Iteration 120/1000 | Loss: 0.00095953
Iteration 121/1000 | Loss: 0.00086887
Iteration 122/1000 | Loss: 0.00127878
Iteration 123/1000 | Loss: 0.00156033
Iteration 124/1000 | Loss: 0.00148522
Iteration 125/1000 | Loss: 0.00194390
Iteration 126/1000 | Loss: 0.00073003
Iteration 127/1000 | Loss: 0.00057609
Iteration 128/1000 | Loss: 0.00061058
Iteration 129/1000 | Loss: 0.00074903
Iteration 130/1000 | Loss: 0.00043533
Iteration 131/1000 | Loss: 0.00164390
Iteration 132/1000 | Loss: 0.00101642
Iteration 133/1000 | Loss: 0.00100217
Iteration 134/1000 | Loss: 0.00097068
Iteration 135/1000 | Loss: 0.00121827
Iteration 136/1000 | Loss: 0.00167833
Iteration 137/1000 | Loss: 0.00193672
Iteration 138/1000 | Loss: 0.00126979
Iteration 139/1000 | Loss: 0.00190541
Iteration 140/1000 | Loss: 0.00089954
Iteration 141/1000 | Loss: 0.00109721
Iteration 142/1000 | Loss: 0.00118373
Iteration 143/1000 | Loss: 0.00090081
Iteration 144/1000 | Loss: 0.00169917
Iteration 145/1000 | Loss: 0.00160141
Iteration 146/1000 | Loss: 0.00100421
Iteration 147/1000 | Loss: 0.00082760
Iteration 148/1000 | Loss: 0.00086779
Iteration 149/1000 | Loss: 0.00106592
Iteration 150/1000 | Loss: 0.00157075
Iteration 151/1000 | Loss: 0.00135357
Iteration 152/1000 | Loss: 0.00125258
Iteration 153/1000 | Loss: 0.00076385
Iteration 154/1000 | Loss: 0.00111904
Iteration 155/1000 | Loss: 0.00085550
Iteration 156/1000 | Loss: 0.00085715
Iteration 157/1000 | Loss: 0.00064591
Iteration 158/1000 | Loss: 0.00074468
Iteration 159/1000 | Loss: 0.00184706
Iteration 160/1000 | Loss: 0.00094377
Iteration 161/1000 | Loss: 0.00099551
Iteration 162/1000 | Loss: 0.00088368
Iteration 163/1000 | Loss: 0.00094254
Iteration 164/1000 | Loss: 0.00094756
Iteration 165/1000 | Loss: 0.00076745
Iteration 166/1000 | Loss: 0.00033920
Iteration 167/1000 | Loss: 0.00075780
Iteration 168/1000 | Loss: 0.00034684
Iteration 169/1000 | Loss: 0.00035539
Iteration 170/1000 | Loss: 0.00065678
Iteration 171/1000 | Loss: 0.00034273
Iteration 172/1000 | Loss: 0.00059911
Iteration 173/1000 | Loss: 0.00095594
Iteration 174/1000 | Loss: 0.00101804
Iteration 175/1000 | Loss: 0.00066122
Iteration 176/1000 | Loss: 0.00080944
Iteration 177/1000 | Loss: 0.00071708
Iteration 178/1000 | Loss: 0.00048771
Iteration 179/1000 | Loss: 0.00094875
Iteration 180/1000 | Loss: 0.00066605
Iteration 181/1000 | Loss: 0.00061027
Iteration 182/1000 | Loss: 0.00061120
Iteration 183/1000 | Loss: 0.00041487
Iteration 184/1000 | Loss: 0.00071281
Iteration 185/1000 | Loss: 0.00058820
Iteration 186/1000 | Loss: 0.00060407
Iteration 187/1000 | Loss: 0.00025469
Iteration 188/1000 | Loss: 0.00064018
Iteration 189/1000 | Loss: 0.00041596
Iteration 190/1000 | Loss: 0.00059676
Iteration 191/1000 | Loss: 0.00025602
Iteration 192/1000 | Loss: 0.00051638
Iteration 193/1000 | Loss: 0.00062980
Iteration 194/1000 | Loss: 0.00078365
Iteration 195/1000 | Loss: 0.00051000
Iteration 196/1000 | Loss: 0.00050436
Iteration 197/1000 | Loss: 0.00085376
Iteration 198/1000 | Loss: 0.00160426
Iteration 199/1000 | Loss: 0.00187069
Iteration 200/1000 | Loss: 0.00134256
Iteration 201/1000 | Loss: 0.00098934
Iteration 202/1000 | Loss: 0.00090356
Iteration 203/1000 | Loss: 0.00069095
Iteration 204/1000 | Loss: 0.00095665
Iteration 205/1000 | Loss: 0.00093444
Iteration 206/1000 | Loss: 0.00070959
Iteration 207/1000 | Loss: 0.00103079
Iteration 208/1000 | Loss: 0.00031539
Iteration 209/1000 | Loss: 0.00042627
Iteration 210/1000 | Loss: 0.00046936
Iteration 211/1000 | Loss: 0.00055749
Iteration 212/1000 | Loss: 0.00216076
Iteration 213/1000 | Loss: 0.00060342
Iteration 214/1000 | Loss: 0.00076088
Iteration 215/1000 | Loss: 0.00043029
Iteration 216/1000 | Loss: 0.00008428
Iteration 217/1000 | Loss: 0.00032700
Iteration 218/1000 | Loss: 0.00042565
Iteration 219/1000 | Loss: 0.00019638
Iteration 220/1000 | Loss: 0.00034243
Iteration 221/1000 | Loss: 0.00066229
Iteration 222/1000 | Loss: 0.00093688
Iteration 223/1000 | Loss: 0.00064513
Iteration 224/1000 | Loss: 0.00047485
Iteration 225/1000 | Loss: 0.00061590
Iteration 226/1000 | Loss: 0.00070482
Iteration 227/1000 | Loss: 0.00057000
Iteration 228/1000 | Loss: 0.00043185
Iteration 229/1000 | Loss: 0.00038667
Iteration 230/1000 | Loss: 0.00048888
Iteration 231/1000 | Loss: 0.00009650
Iteration 232/1000 | Loss: 0.00027542
Iteration 233/1000 | Loss: 0.00007253
Iteration 234/1000 | Loss: 0.00006113
Iteration 235/1000 | Loss: 0.00023709
Iteration 236/1000 | Loss: 0.00006995
Iteration 237/1000 | Loss: 0.00010460
Iteration 238/1000 | Loss: 0.00005194
Iteration 239/1000 | Loss: 0.00004427
Iteration 240/1000 | Loss: 0.00004234
Iteration 241/1000 | Loss: 0.00014556
Iteration 242/1000 | Loss: 0.00029616
Iteration 243/1000 | Loss: 0.00004996
Iteration 244/1000 | Loss: 0.00017863
Iteration 245/1000 | Loss: 0.00004753
Iteration 246/1000 | Loss: 0.00004277
Iteration 247/1000 | Loss: 0.00015719
Iteration 248/1000 | Loss: 0.00049637
Iteration 249/1000 | Loss: 0.00031507
Iteration 250/1000 | Loss: 0.00042451
Iteration 251/1000 | Loss: 0.00030905
Iteration 252/1000 | Loss: 0.00024710
Iteration 253/1000 | Loss: 0.00063713
Iteration 254/1000 | Loss: 0.00016047
Iteration 255/1000 | Loss: 0.00034449
Iteration 256/1000 | Loss: 0.00004922
Iteration 257/1000 | Loss: 0.00003873
Iteration 258/1000 | Loss: 0.00003624
Iteration 259/1000 | Loss: 0.00003442
Iteration 260/1000 | Loss: 0.00003330
Iteration 261/1000 | Loss: 0.00003242
Iteration 262/1000 | Loss: 0.00003099
Iteration 263/1000 | Loss: 0.00002961
Iteration 264/1000 | Loss: 0.00002803
Iteration 265/1000 | Loss: 0.00002683
Iteration 266/1000 | Loss: 0.00002632
Iteration 267/1000 | Loss: 0.00002583
Iteration 268/1000 | Loss: 0.00002557
Iteration 269/1000 | Loss: 0.00002537
Iteration 270/1000 | Loss: 0.00002525
Iteration 271/1000 | Loss: 0.00002522
Iteration 272/1000 | Loss: 0.00002521
Iteration 273/1000 | Loss: 0.00002519
Iteration 274/1000 | Loss: 0.00002518
Iteration 275/1000 | Loss: 0.00002518
Iteration 276/1000 | Loss: 0.00002517
Iteration 277/1000 | Loss: 0.00002517
Iteration 278/1000 | Loss: 0.00002517
Iteration 279/1000 | Loss: 0.00002517
Iteration 280/1000 | Loss: 0.00002516
Iteration 281/1000 | Loss: 0.00002516
Iteration 282/1000 | Loss: 0.00002503
Iteration 283/1000 | Loss: 0.00002502
Iteration 284/1000 | Loss: 0.00002502
Iteration 285/1000 | Loss: 0.00002502
Iteration 286/1000 | Loss: 0.00002501
Iteration 287/1000 | Loss: 0.00002501
Iteration 288/1000 | Loss: 0.00002501
Iteration 289/1000 | Loss: 0.00002500
Iteration 290/1000 | Loss: 0.00002500
Iteration 291/1000 | Loss: 0.00002500
Iteration 292/1000 | Loss: 0.00002500
Iteration 293/1000 | Loss: 0.00002500
Iteration 294/1000 | Loss: 0.00002500
Iteration 295/1000 | Loss: 0.00002499
Iteration 296/1000 | Loss: 0.00002499
Iteration 297/1000 | Loss: 0.00002499
Iteration 298/1000 | Loss: 0.00002497
Iteration 299/1000 | Loss: 0.00002496
Iteration 300/1000 | Loss: 0.00002495
Iteration 301/1000 | Loss: 0.00002495
Iteration 302/1000 | Loss: 0.00002495
Iteration 303/1000 | Loss: 0.00002495
Iteration 304/1000 | Loss: 0.00002494
Iteration 305/1000 | Loss: 0.00002494
Iteration 306/1000 | Loss: 0.00002494
Iteration 307/1000 | Loss: 0.00002493
Iteration 308/1000 | Loss: 0.00002493
Iteration 309/1000 | Loss: 0.00002493
Iteration 310/1000 | Loss: 0.00002493
Iteration 311/1000 | Loss: 0.00002492
Iteration 312/1000 | Loss: 0.00002492
Iteration 313/1000 | Loss: 0.00002492
Iteration 314/1000 | Loss: 0.00002492
Iteration 315/1000 | Loss: 0.00002492
Iteration 316/1000 | Loss: 0.00002492
Iteration 317/1000 | Loss: 0.00002492
Iteration 318/1000 | Loss: 0.00002491
Iteration 319/1000 | Loss: 0.00002491
Iteration 320/1000 | Loss: 0.00002491
Iteration 321/1000 | Loss: 0.00002491
Iteration 322/1000 | Loss: 0.00002491
Iteration 323/1000 | Loss: 0.00002491
Iteration 324/1000 | Loss: 0.00002491
Iteration 325/1000 | Loss: 0.00002491
Iteration 326/1000 | Loss: 0.00002491
Iteration 327/1000 | Loss: 0.00002491
Iteration 328/1000 | Loss: 0.00002491
Iteration 329/1000 | Loss: 0.00002491
Iteration 330/1000 | Loss: 0.00002490
Iteration 331/1000 | Loss: 0.00002490
Iteration 332/1000 | Loss: 0.00002490
Iteration 333/1000 | Loss: 0.00002490
Iteration 334/1000 | Loss: 0.00002490
Iteration 335/1000 | Loss: 0.00002490
Iteration 336/1000 | Loss: 0.00002490
Iteration 337/1000 | Loss: 0.00002490
Iteration 338/1000 | Loss: 0.00002490
Iteration 339/1000 | Loss: 0.00002489
Iteration 340/1000 | Loss: 0.00002489
Iteration 341/1000 | Loss: 0.00002489
Iteration 342/1000 | Loss: 0.00002489
Iteration 343/1000 | Loss: 0.00002489
Iteration 344/1000 | Loss: 0.00002489
Iteration 345/1000 | Loss: 0.00002489
Iteration 346/1000 | Loss: 0.00002489
Iteration 347/1000 | Loss: 0.00002488
Iteration 348/1000 | Loss: 0.00002488
Iteration 349/1000 | Loss: 0.00002488
Iteration 350/1000 | Loss: 0.00002488
Iteration 351/1000 | Loss: 0.00002488
Iteration 352/1000 | Loss: 0.00002487
Iteration 353/1000 | Loss: 0.00002487
Iteration 354/1000 | Loss: 0.00002487
Iteration 355/1000 | Loss: 0.00002486
Iteration 356/1000 | Loss: 0.00002486
Iteration 357/1000 | Loss: 0.00002486
Iteration 358/1000 | Loss: 0.00002486
Iteration 359/1000 | Loss: 0.00002485
Iteration 360/1000 | Loss: 0.00002485
Iteration 361/1000 | Loss: 0.00002485
Iteration 362/1000 | Loss: 0.00002485
Iteration 363/1000 | Loss: 0.00002485
Iteration 364/1000 | Loss: 0.00002485
Iteration 365/1000 | Loss: 0.00002485
Iteration 366/1000 | Loss: 0.00002484
Iteration 367/1000 | Loss: 0.00002484
Iteration 368/1000 | Loss: 0.00002484
Iteration 369/1000 | Loss: 0.00002484
Iteration 370/1000 | Loss: 0.00002484
Iteration 371/1000 | Loss: 0.00002483
Iteration 372/1000 | Loss: 0.00002483
Iteration 373/1000 | Loss: 0.00002483
Iteration 374/1000 | Loss: 0.00002483
Iteration 375/1000 | Loss: 0.00002482
Iteration 376/1000 | Loss: 0.00002482
Iteration 377/1000 | Loss: 0.00002482
Iteration 378/1000 | Loss: 0.00002482
Iteration 379/1000 | Loss: 0.00002482
Iteration 380/1000 | Loss: 0.00002482
Iteration 381/1000 | Loss: 0.00002482
Iteration 382/1000 | Loss: 0.00002482
Iteration 383/1000 | Loss: 0.00002482
Iteration 384/1000 | Loss: 0.00002482
Iteration 385/1000 | Loss: 0.00002482
Iteration 386/1000 | Loss: 0.00002482
Iteration 387/1000 | Loss: 0.00002482
Iteration 388/1000 | Loss: 0.00002481
Iteration 389/1000 | Loss: 0.00002481
Iteration 390/1000 | Loss: 0.00002481
Iteration 391/1000 | Loss: 0.00002481
Iteration 392/1000 | Loss: 0.00002481
Iteration 393/1000 | Loss: 0.00002481
Iteration 394/1000 | Loss: 0.00002481
Iteration 395/1000 | Loss: 0.00002481
Iteration 396/1000 | Loss: 0.00002481
Iteration 397/1000 | Loss: 0.00002481
Iteration 398/1000 | Loss: 0.00002481
Iteration 399/1000 | Loss: 0.00002481
Iteration 400/1000 | Loss: 0.00002481
Iteration 401/1000 | Loss: 0.00002481
Iteration 402/1000 | Loss: 0.00002481
Iteration 403/1000 | Loss: 0.00002481
Iteration 404/1000 | Loss: 0.00002481
Iteration 405/1000 | Loss: 0.00002481
Iteration 406/1000 | Loss: 0.00002481
Iteration 407/1000 | Loss: 0.00002481
Iteration 408/1000 | Loss: 0.00002481
Iteration 409/1000 | Loss: 0.00002481
Iteration 410/1000 | Loss: 0.00002481
Iteration 411/1000 | Loss: 0.00002481
Iteration 412/1000 | Loss: 0.00002481
Iteration 413/1000 | Loss: 0.00002481
Iteration 414/1000 | Loss: 0.00002481
Iteration 415/1000 | Loss: 0.00002481
Iteration 416/1000 | Loss: 0.00002481
Iteration 417/1000 | Loss: 0.00002481
Iteration 418/1000 | Loss: 0.00002481
Iteration 419/1000 | Loss: 0.00002481
Iteration 420/1000 | Loss: 0.00002481
Iteration 421/1000 | Loss: 0.00002481
Iteration 422/1000 | Loss: 0.00002481
Iteration 423/1000 | Loss: 0.00002481
Iteration 424/1000 | Loss: 0.00002481
Iteration 425/1000 | Loss: 0.00002481
Iteration 426/1000 | Loss: 0.00002481
Iteration 427/1000 | Loss: 0.00002481
Iteration 428/1000 | Loss: 0.00002481
Iteration 429/1000 | Loss: 0.00002481
Iteration 430/1000 | Loss: 0.00002481
Iteration 431/1000 | Loss: 0.00002481
Iteration 432/1000 | Loss: 0.00002481
Iteration 433/1000 | Loss: 0.00002481
Iteration 434/1000 | Loss: 0.00002481
Iteration 435/1000 | Loss: 0.00002481
Iteration 436/1000 | Loss: 0.00002481
Iteration 437/1000 | Loss: 0.00002481
Iteration 438/1000 | Loss: 0.00002481
Iteration 439/1000 | Loss: 0.00002481
Iteration 440/1000 | Loss: 0.00002481
Iteration 441/1000 | Loss: 0.00002481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 441. Stopping optimization.
Last 5 losses: [2.48110754910158e-05, 2.48110754910158e-05, 2.48110754910158e-05, 2.48110754910158e-05, 2.48110754910158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.48110754910158e-05

Optimization complete. Final v2v error: 4.19866418838501 mm

Highest mean error: 5.214951992034912 mm for frame 38

Lowest mean error: 3.534841299057007 mm for frame 4

Saving results

Total time: 423.8359932899475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800537
Iteration 2/25 | Loss: 0.00112023
Iteration 3/25 | Loss: 0.00086022
Iteration 4/25 | Loss: 0.00078951
Iteration 5/25 | Loss: 0.00075866
Iteration 6/25 | Loss: 0.00075470
Iteration 7/25 | Loss: 0.00075310
Iteration 8/25 | Loss: 0.00075273
Iteration 9/25 | Loss: 0.00075257
Iteration 10/25 | Loss: 0.00075241
Iteration 11/25 | Loss: 0.00075223
Iteration 12/25 | Loss: 0.00075206
Iteration 13/25 | Loss: 0.00075188
Iteration 14/25 | Loss: 0.00075162
Iteration 15/25 | Loss: 0.00075514
Iteration 16/25 | Loss: 0.00075404
Iteration 17/25 | Loss: 0.00075115
Iteration 18/25 | Loss: 0.00075072
Iteration 19/25 | Loss: 0.00075325
Iteration 20/25 | Loss: 0.00075344
Iteration 21/25 | Loss: 0.00075334
Iteration 22/25 | Loss: 0.00075320
Iteration 23/25 | Loss: 0.00075383
Iteration 24/25 | Loss: 0.00075296
Iteration 25/25 | Loss: 0.00075217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94028544
Iteration 2/25 | Loss: 0.00040475
Iteration 3/25 | Loss: 0.00040467
Iteration 4/25 | Loss: 0.00040467
Iteration 5/25 | Loss: 0.00040467
Iteration 6/25 | Loss: 0.00040467
Iteration 7/25 | Loss: 0.00040467
Iteration 8/25 | Loss: 0.00040467
Iteration 9/25 | Loss: 0.00040467
Iteration 10/25 | Loss: 0.00040467
Iteration 11/25 | Loss: 0.00040467
Iteration 12/25 | Loss: 0.00040467
Iteration 13/25 | Loss: 0.00040467
Iteration 14/25 | Loss: 0.00040467
Iteration 15/25 | Loss: 0.00040467
Iteration 16/25 | Loss: 0.00040467
Iteration 17/25 | Loss: 0.00040467
Iteration 18/25 | Loss: 0.00040467
Iteration 19/25 | Loss: 0.00040467
Iteration 20/25 | Loss: 0.00040467
Iteration 21/25 | Loss: 0.00040467
Iteration 22/25 | Loss: 0.00040467
Iteration 23/25 | Loss: 0.00040467
Iteration 24/25 | Loss: 0.00040467
Iteration 25/25 | Loss: 0.00040467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040467
Iteration 2/1000 | Loss: 0.00017330
Iteration 3/1000 | Loss: 0.00013597
Iteration 4/1000 | Loss: 0.00010334
Iteration 5/1000 | Loss: 0.00015348
Iteration 6/1000 | Loss: 0.00014266
Iteration 7/1000 | Loss: 0.00015141
Iteration 8/1000 | Loss: 0.00009936
Iteration 9/1000 | Loss: 0.00016623
Iteration 10/1000 | Loss: 0.00013321
Iteration 11/1000 | Loss: 0.00015333
Iteration 12/1000 | Loss: 0.00013611
Iteration 13/1000 | Loss: 0.00015471
Iteration 14/1000 | Loss: 0.00014893
Iteration 15/1000 | Loss: 0.00007632
Iteration 16/1000 | Loss: 0.00014828
Iteration 17/1000 | Loss: 0.00011256
Iteration 18/1000 | Loss: 0.00005212
Iteration 19/1000 | Loss: 0.00013723
Iteration 20/1000 | Loss: 0.00016602
Iteration 21/1000 | Loss: 0.00010694
Iteration 22/1000 | Loss: 0.00013697
Iteration 23/1000 | Loss: 0.00011345
Iteration 24/1000 | Loss: 0.00015934
Iteration 25/1000 | Loss: 0.00014775
Iteration 26/1000 | Loss: 0.00013865
Iteration 27/1000 | Loss: 0.00013455
Iteration 28/1000 | Loss: 0.00015994
Iteration 29/1000 | Loss: 0.00013067
Iteration 30/1000 | Loss: 0.00014169
Iteration 31/1000 | Loss: 0.00005631
Iteration 32/1000 | Loss: 0.00014873
Iteration 33/1000 | Loss: 0.00012431
Iteration 34/1000 | Loss: 0.00020188
Iteration 35/1000 | Loss: 0.00019148
Iteration 36/1000 | Loss: 0.00012682
Iteration 37/1000 | Loss: 0.00028850
Iteration 38/1000 | Loss: 0.00008560
Iteration 39/1000 | Loss: 0.00021840
Iteration 40/1000 | Loss: 0.00031026
Iteration 41/1000 | Loss: 0.00069530
Iteration 42/1000 | Loss: 0.00049991
Iteration 43/1000 | Loss: 0.00066074
Iteration 44/1000 | Loss: 0.00073866
Iteration 45/1000 | Loss: 0.00008241
Iteration 46/1000 | Loss: 0.00033572
Iteration 47/1000 | Loss: 0.00004446
Iteration 48/1000 | Loss: 0.00003807
Iteration 49/1000 | Loss: 0.00003534
Iteration 50/1000 | Loss: 0.00003394
Iteration 51/1000 | Loss: 0.00003263
Iteration 52/1000 | Loss: 0.00003157
Iteration 53/1000 | Loss: 0.00003076
Iteration 54/1000 | Loss: 0.00003021
Iteration 55/1000 | Loss: 0.00003519
Iteration 56/1000 | Loss: 0.00003240
Iteration 57/1000 | Loss: 0.00003482
Iteration 58/1000 | Loss: 0.00003078
Iteration 59/1000 | Loss: 0.00002994
Iteration 60/1000 | Loss: 0.00002952
Iteration 61/1000 | Loss: 0.00002929
Iteration 62/1000 | Loss: 0.00002905
Iteration 63/1000 | Loss: 0.00002892
Iteration 64/1000 | Loss: 0.00002886
Iteration 65/1000 | Loss: 0.00002886
Iteration 66/1000 | Loss: 0.00002886
Iteration 67/1000 | Loss: 0.00002885
Iteration 68/1000 | Loss: 0.00002885
Iteration 69/1000 | Loss: 0.00002885
Iteration 70/1000 | Loss: 0.00002885
Iteration 71/1000 | Loss: 0.00002885
Iteration 72/1000 | Loss: 0.00002885
Iteration 73/1000 | Loss: 0.00002885
Iteration 74/1000 | Loss: 0.00002885
Iteration 75/1000 | Loss: 0.00002884
Iteration 76/1000 | Loss: 0.00002884
Iteration 77/1000 | Loss: 0.00002884
Iteration 78/1000 | Loss: 0.00002883
Iteration 79/1000 | Loss: 0.00002883
Iteration 80/1000 | Loss: 0.00002883
Iteration 81/1000 | Loss: 0.00002883
Iteration 82/1000 | Loss: 0.00002883
Iteration 83/1000 | Loss: 0.00002883
Iteration 84/1000 | Loss: 0.00002883
Iteration 85/1000 | Loss: 0.00002883
Iteration 86/1000 | Loss: 0.00002883
Iteration 87/1000 | Loss: 0.00002883
Iteration 88/1000 | Loss: 0.00002883
Iteration 89/1000 | Loss: 0.00002882
Iteration 90/1000 | Loss: 0.00002882
Iteration 91/1000 | Loss: 0.00002882
Iteration 92/1000 | Loss: 0.00002882
Iteration 93/1000 | Loss: 0.00002882
Iteration 94/1000 | Loss: 0.00002882
Iteration 95/1000 | Loss: 0.00002882
Iteration 96/1000 | Loss: 0.00002882
Iteration 97/1000 | Loss: 0.00002882
Iteration 98/1000 | Loss: 0.00002882
Iteration 99/1000 | Loss: 0.00002882
Iteration 100/1000 | Loss: 0.00002882
Iteration 101/1000 | Loss: 0.00002882
Iteration 102/1000 | Loss: 0.00002882
Iteration 103/1000 | Loss: 0.00002882
Iteration 104/1000 | Loss: 0.00002881
Iteration 105/1000 | Loss: 0.00002881
Iteration 106/1000 | Loss: 0.00002881
Iteration 107/1000 | Loss: 0.00002881
Iteration 108/1000 | Loss: 0.00002881
Iteration 109/1000 | Loss: 0.00002881
Iteration 110/1000 | Loss: 0.00002880
Iteration 111/1000 | Loss: 0.00002880
Iteration 112/1000 | Loss: 0.00002880
Iteration 113/1000 | Loss: 0.00002880
Iteration 114/1000 | Loss: 0.00002880
Iteration 115/1000 | Loss: 0.00002879
Iteration 116/1000 | Loss: 0.00002879
Iteration 117/1000 | Loss: 0.00002879
Iteration 118/1000 | Loss: 0.00002879
Iteration 119/1000 | Loss: 0.00002879
Iteration 120/1000 | Loss: 0.00002879
Iteration 121/1000 | Loss: 0.00002879
Iteration 122/1000 | Loss: 0.00002879
Iteration 123/1000 | Loss: 0.00002879
Iteration 124/1000 | Loss: 0.00002879
Iteration 125/1000 | Loss: 0.00002878
Iteration 126/1000 | Loss: 0.00002878
Iteration 127/1000 | Loss: 0.00002878
Iteration 128/1000 | Loss: 0.00002878
Iteration 129/1000 | Loss: 0.00002878
Iteration 130/1000 | Loss: 0.00002877
Iteration 131/1000 | Loss: 0.00002877
Iteration 132/1000 | Loss: 0.00002877
Iteration 133/1000 | Loss: 0.00002877
Iteration 134/1000 | Loss: 0.00002877
Iteration 135/1000 | Loss: 0.00002877
Iteration 136/1000 | Loss: 0.00002877
Iteration 137/1000 | Loss: 0.00002877
Iteration 138/1000 | Loss: 0.00002877
Iteration 139/1000 | Loss: 0.00002877
Iteration 140/1000 | Loss: 0.00002877
Iteration 141/1000 | Loss: 0.00002877
Iteration 142/1000 | Loss: 0.00002877
Iteration 143/1000 | Loss: 0.00002876
Iteration 144/1000 | Loss: 0.00002876
Iteration 145/1000 | Loss: 0.00002876
Iteration 146/1000 | Loss: 0.00002876
Iteration 147/1000 | Loss: 0.00002876
Iteration 148/1000 | Loss: 0.00002876
Iteration 149/1000 | Loss: 0.00002876
Iteration 150/1000 | Loss: 0.00002876
Iteration 151/1000 | Loss: 0.00002875
Iteration 152/1000 | Loss: 0.00002875
Iteration 153/1000 | Loss: 0.00002875
Iteration 154/1000 | Loss: 0.00002875
Iteration 155/1000 | Loss: 0.00002875
Iteration 156/1000 | Loss: 0.00002875
Iteration 157/1000 | Loss: 0.00002875
Iteration 158/1000 | Loss: 0.00002875
Iteration 159/1000 | Loss: 0.00002875
Iteration 160/1000 | Loss: 0.00002875
Iteration 161/1000 | Loss: 0.00002874
Iteration 162/1000 | Loss: 0.00002874
Iteration 163/1000 | Loss: 0.00002874
Iteration 164/1000 | Loss: 0.00002874
Iteration 165/1000 | Loss: 0.00002874
Iteration 166/1000 | Loss: 0.00002874
Iteration 167/1000 | Loss: 0.00002874
Iteration 168/1000 | Loss: 0.00002874
Iteration 169/1000 | Loss: 0.00002874
Iteration 170/1000 | Loss: 0.00002874
Iteration 171/1000 | Loss: 0.00002874
Iteration 172/1000 | Loss: 0.00002874
Iteration 173/1000 | Loss: 0.00002874
Iteration 174/1000 | Loss: 0.00002874
Iteration 175/1000 | Loss: 0.00002873
Iteration 176/1000 | Loss: 0.00002873
Iteration 177/1000 | Loss: 0.00002873
Iteration 178/1000 | Loss: 0.00002873
Iteration 179/1000 | Loss: 0.00002873
Iteration 180/1000 | Loss: 0.00002873
Iteration 181/1000 | Loss: 0.00002873
Iteration 182/1000 | Loss: 0.00002872
Iteration 183/1000 | Loss: 0.00002872
Iteration 184/1000 | Loss: 0.00002872
Iteration 185/1000 | Loss: 0.00002872
Iteration 186/1000 | Loss: 0.00002872
Iteration 187/1000 | Loss: 0.00002872
Iteration 188/1000 | Loss: 0.00002871
Iteration 189/1000 | Loss: 0.00002871
Iteration 190/1000 | Loss: 0.00002871
Iteration 191/1000 | Loss: 0.00002871
Iteration 192/1000 | Loss: 0.00002871
Iteration 193/1000 | Loss: 0.00002871
Iteration 194/1000 | Loss: 0.00002871
Iteration 195/1000 | Loss: 0.00002871
Iteration 196/1000 | Loss: 0.00002871
Iteration 197/1000 | Loss: 0.00002871
Iteration 198/1000 | Loss: 0.00002871
Iteration 199/1000 | Loss: 0.00002871
Iteration 200/1000 | Loss: 0.00002871
Iteration 201/1000 | Loss: 0.00002871
Iteration 202/1000 | Loss: 0.00002871
Iteration 203/1000 | Loss: 0.00002871
Iteration 204/1000 | Loss: 0.00002871
Iteration 205/1000 | Loss: 0.00002871
Iteration 206/1000 | Loss: 0.00002871
Iteration 207/1000 | Loss: 0.00002871
Iteration 208/1000 | Loss: 0.00002871
Iteration 209/1000 | Loss: 0.00002871
Iteration 210/1000 | Loss: 0.00002871
Iteration 211/1000 | Loss: 0.00002871
Iteration 212/1000 | Loss: 0.00002871
Iteration 213/1000 | Loss: 0.00002871
Iteration 214/1000 | Loss: 0.00002871
Iteration 215/1000 | Loss: 0.00002871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.870848220482003e-05, 2.870848220482003e-05, 2.870848220482003e-05, 2.870848220482003e-05, 2.870848220482003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.870848220482003e-05

Optimization complete. Final v2v error: 4.442707061767578 mm

Highest mean error: 6.41011381149292 mm for frame 106

Lowest mean error: 3.5182254314422607 mm for frame 233

Saving results

Total time: 162.38111305236816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058635
Iteration 2/25 | Loss: 0.01058635
Iteration 3/25 | Loss: 0.01058634
Iteration 4/25 | Loss: 0.00228068
Iteration 5/25 | Loss: 0.00176247
Iteration 6/25 | Loss: 0.00159584
Iteration 7/25 | Loss: 0.00149194
Iteration 8/25 | Loss: 0.00156342
Iteration 9/25 | Loss: 0.00157637
Iteration 10/25 | Loss: 0.00151778
Iteration 11/25 | Loss: 0.00143408
Iteration 12/25 | Loss: 0.00134907
Iteration 13/25 | Loss: 0.00130136
Iteration 14/25 | Loss: 0.00126308
Iteration 15/25 | Loss: 0.00124263
Iteration 16/25 | Loss: 0.00124002
Iteration 17/25 | Loss: 0.00120122
Iteration 18/25 | Loss: 0.00118235
Iteration 19/25 | Loss: 0.00117784
Iteration 20/25 | Loss: 0.00116255
Iteration 21/25 | Loss: 0.00114949
Iteration 22/25 | Loss: 0.00116720
Iteration 23/25 | Loss: 0.00115701
Iteration 24/25 | Loss: 0.00114517
Iteration 25/25 | Loss: 0.00113686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56317043
Iteration 2/25 | Loss: 0.00666970
Iteration 3/25 | Loss: 0.00680789
Iteration 4/25 | Loss: 0.00450690
Iteration 5/25 | Loss: 0.00306365
Iteration 6/25 | Loss: 0.00306360
Iteration 7/25 | Loss: 0.00306360
Iteration 8/25 | Loss: 0.00306360
Iteration 9/25 | Loss: 0.00306360
Iteration 10/25 | Loss: 0.00306360
Iteration 11/25 | Loss: 0.00306360
Iteration 12/25 | Loss: 0.00306360
Iteration 13/25 | Loss: 0.00306360
Iteration 14/25 | Loss: 0.00306360
Iteration 15/25 | Loss: 0.00306360
Iteration 16/25 | Loss: 0.00306360
Iteration 17/25 | Loss: 0.00306360
Iteration 18/25 | Loss: 0.00306360
Iteration 19/25 | Loss: 0.00306360
Iteration 20/25 | Loss: 0.00306360
Iteration 21/25 | Loss: 0.00306360
Iteration 22/25 | Loss: 0.00306360
Iteration 23/25 | Loss: 0.00306360
Iteration 24/25 | Loss: 0.00306360
Iteration 25/25 | Loss: 0.00306360

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306360
Iteration 2/1000 | Loss: 0.00226775
Iteration 3/1000 | Loss: 0.00194839
Iteration 4/1000 | Loss: 0.00311197
Iteration 5/1000 | Loss: 0.00059518
Iteration 6/1000 | Loss: 0.00062099
Iteration 7/1000 | Loss: 0.00071355
Iteration 8/1000 | Loss: 0.00481689
Iteration 9/1000 | Loss: 0.00240605
Iteration 10/1000 | Loss: 0.00202377
Iteration 11/1000 | Loss: 0.00085091
Iteration 12/1000 | Loss: 0.00050943
Iteration 13/1000 | Loss: 0.00117514
Iteration 14/1000 | Loss: 0.00374857
Iteration 15/1000 | Loss: 0.00093604
Iteration 16/1000 | Loss: 0.00045470
Iteration 17/1000 | Loss: 0.00182567
Iteration 18/1000 | Loss: 0.00228709
Iteration 19/1000 | Loss: 0.00138250
Iteration 20/1000 | Loss: 0.00179011
Iteration 21/1000 | Loss: 0.00090793
Iteration 22/1000 | Loss: 0.00089495
Iteration 23/1000 | Loss: 0.00089866
Iteration 24/1000 | Loss: 0.00125743
Iteration 25/1000 | Loss: 0.00086327
Iteration 26/1000 | Loss: 0.00095432
Iteration 27/1000 | Loss: 0.00077514
Iteration 28/1000 | Loss: 0.00159067
Iteration 29/1000 | Loss: 0.00089462
Iteration 30/1000 | Loss: 0.00195350
Iteration 31/1000 | Loss: 0.00148512
Iteration 32/1000 | Loss: 0.00271612
Iteration 33/1000 | Loss: 0.00112986
Iteration 34/1000 | Loss: 0.00098729
Iteration 35/1000 | Loss: 0.00070201
Iteration 36/1000 | Loss: 0.00049697
Iteration 37/1000 | Loss: 0.00024383
Iteration 38/1000 | Loss: 0.00019937
Iteration 39/1000 | Loss: 0.00024480
Iteration 40/1000 | Loss: 0.00044548
Iteration 41/1000 | Loss: 0.00063981
Iteration 42/1000 | Loss: 0.00055211
Iteration 43/1000 | Loss: 0.00039520
Iteration 44/1000 | Loss: 0.00028523
Iteration 45/1000 | Loss: 0.00025462
Iteration 46/1000 | Loss: 0.00027062
Iteration 47/1000 | Loss: 0.00049863
Iteration 48/1000 | Loss: 0.00018754
Iteration 49/1000 | Loss: 0.00023048
Iteration 50/1000 | Loss: 0.00021713
Iteration 51/1000 | Loss: 0.00020164
Iteration 52/1000 | Loss: 0.00019297
Iteration 53/1000 | Loss: 0.00019439
Iteration 54/1000 | Loss: 0.00038764
Iteration 55/1000 | Loss: 0.00101408
Iteration 56/1000 | Loss: 0.00065451
Iteration 57/1000 | Loss: 0.00025546
Iteration 58/1000 | Loss: 0.00030557
Iteration 59/1000 | Loss: 0.00019729
Iteration 60/1000 | Loss: 0.00024371
Iteration 61/1000 | Loss: 0.00026594
Iteration 62/1000 | Loss: 0.00018846
Iteration 63/1000 | Loss: 0.00026256
Iteration 64/1000 | Loss: 0.00021637
Iteration 65/1000 | Loss: 0.00026235
Iteration 66/1000 | Loss: 0.00021430
Iteration 67/1000 | Loss: 0.00024953
Iteration 68/1000 | Loss: 0.00021249
Iteration 69/1000 | Loss: 0.00018979
Iteration 70/1000 | Loss: 0.00022755
Iteration 71/1000 | Loss: 0.00018386
Iteration 72/1000 | Loss: 0.00024420
Iteration 73/1000 | Loss: 0.00015508
Iteration 74/1000 | Loss: 0.00025471
Iteration 75/1000 | Loss: 0.00025922
Iteration 76/1000 | Loss: 0.00025361
Iteration 77/1000 | Loss: 0.00023942
Iteration 78/1000 | Loss: 0.00032066
Iteration 79/1000 | Loss: 0.00023031
Iteration 80/1000 | Loss: 0.00028813
Iteration 81/1000 | Loss: 0.00020941
Iteration 82/1000 | Loss: 0.00015404
Iteration 83/1000 | Loss: 0.00015290
Iteration 84/1000 | Loss: 0.00013953
Iteration 85/1000 | Loss: 0.00014934
Iteration 86/1000 | Loss: 0.00014710
Iteration 87/1000 | Loss: 0.00014724
Iteration 88/1000 | Loss: 0.00014668
Iteration 89/1000 | Loss: 0.00028756
Iteration 90/1000 | Loss: 0.00099424
Iteration 91/1000 | Loss: 0.00092619
Iteration 92/1000 | Loss: 0.00043118
Iteration 93/1000 | Loss: 0.00037144
Iteration 94/1000 | Loss: 0.00024138
Iteration 95/1000 | Loss: 0.00024900
Iteration 96/1000 | Loss: 0.00027317
Iteration 97/1000 | Loss: 0.00035687
Iteration 98/1000 | Loss: 0.00037720
Iteration 99/1000 | Loss: 0.00042470
Iteration 100/1000 | Loss: 0.00065874
Iteration 101/1000 | Loss: 0.00039277
Iteration 102/1000 | Loss: 0.00033045
Iteration 103/1000 | Loss: 0.00035047
Iteration 104/1000 | Loss: 0.00032957
Iteration 105/1000 | Loss: 0.00022266
Iteration 106/1000 | Loss: 0.00014027
Iteration 107/1000 | Loss: 0.00058771
Iteration 108/1000 | Loss: 0.00022393
Iteration 109/1000 | Loss: 0.00046831
Iteration 110/1000 | Loss: 0.00012951
Iteration 111/1000 | Loss: 0.00012836
Iteration 112/1000 | Loss: 0.00053564
Iteration 113/1000 | Loss: 0.00017549
Iteration 114/1000 | Loss: 0.00013745
Iteration 115/1000 | Loss: 0.00014575
Iteration 116/1000 | Loss: 0.00014145
Iteration 117/1000 | Loss: 0.00012597
Iteration 118/1000 | Loss: 0.00013194
Iteration 119/1000 | Loss: 0.00025824
Iteration 120/1000 | Loss: 0.00017375
Iteration 121/1000 | Loss: 0.00013990
Iteration 122/1000 | Loss: 0.00011676
Iteration 123/1000 | Loss: 0.00012285
Iteration 124/1000 | Loss: 0.00013485
Iteration 125/1000 | Loss: 0.00013615
Iteration 126/1000 | Loss: 0.00013246
Iteration 127/1000 | Loss: 0.00012924
Iteration 128/1000 | Loss: 0.00012688
Iteration 129/1000 | Loss: 0.00031350
Iteration 130/1000 | Loss: 0.00031354
Iteration 131/1000 | Loss: 0.00033940
Iteration 132/1000 | Loss: 0.00059652
Iteration 133/1000 | Loss: 0.00036314
Iteration 134/1000 | Loss: 0.00063315
Iteration 135/1000 | Loss: 0.00065382
Iteration 136/1000 | Loss: 0.00047655
Iteration 137/1000 | Loss: 0.00053141
Iteration 138/1000 | Loss: 0.00039990
Iteration 139/1000 | Loss: 0.00029533
Iteration 140/1000 | Loss: 0.00037583
Iteration 141/1000 | Loss: 0.00041672
Iteration 142/1000 | Loss: 0.00046621
Iteration 143/1000 | Loss: 0.00034782
Iteration 144/1000 | Loss: 0.00023017
Iteration 145/1000 | Loss: 0.00013167
Iteration 146/1000 | Loss: 0.00011557
Iteration 147/1000 | Loss: 0.00016447
Iteration 148/1000 | Loss: 0.00021719
Iteration 149/1000 | Loss: 0.00028761
Iteration 150/1000 | Loss: 0.00034372
Iteration 151/1000 | Loss: 0.00017606
Iteration 152/1000 | Loss: 0.00023175
Iteration 153/1000 | Loss: 0.00018095
Iteration 154/1000 | Loss: 0.00013370
Iteration 155/1000 | Loss: 0.00028656
Iteration 156/1000 | Loss: 0.00040543
Iteration 157/1000 | Loss: 0.00013044
Iteration 158/1000 | Loss: 0.00013432
Iteration 159/1000 | Loss: 0.00029504
Iteration 160/1000 | Loss: 0.00014122
Iteration 161/1000 | Loss: 0.00013962
Iteration 162/1000 | Loss: 0.00013613
Iteration 163/1000 | Loss: 0.00013423
Iteration 164/1000 | Loss: 0.00023972
Iteration 165/1000 | Loss: 0.00016144
Iteration 166/1000 | Loss: 0.00013021
Iteration 167/1000 | Loss: 0.00015196
Iteration 168/1000 | Loss: 0.00011952
Iteration 169/1000 | Loss: 0.00019280
Iteration 170/1000 | Loss: 0.00044371
Iteration 171/1000 | Loss: 0.00013234
Iteration 172/1000 | Loss: 0.00011981
Iteration 173/1000 | Loss: 0.00011103
Iteration 174/1000 | Loss: 0.00011939
Iteration 175/1000 | Loss: 0.00011569
Iteration 176/1000 | Loss: 0.00011676
Iteration 177/1000 | Loss: 0.00011839
Iteration 178/1000 | Loss: 0.00011929
Iteration 179/1000 | Loss: 0.00011581
Iteration 180/1000 | Loss: 0.00011505
Iteration 181/1000 | Loss: 0.00011547
Iteration 182/1000 | Loss: 0.00011400
Iteration 183/1000 | Loss: 0.00011516
Iteration 184/1000 | Loss: 0.00011331
Iteration 185/1000 | Loss: 0.00011287
Iteration 186/1000 | Loss: 0.00012450
Iteration 187/1000 | Loss: 0.00011164
Iteration 188/1000 | Loss: 0.00011551
Iteration 189/1000 | Loss: 0.00011823
Iteration 190/1000 | Loss: 0.00011554
Iteration 191/1000 | Loss: 0.00011740
Iteration 192/1000 | Loss: 0.00010598
Iteration 193/1000 | Loss: 0.00010452
Iteration 194/1000 | Loss: 0.00010404
Iteration 195/1000 | Loss: 0.00010357
Iteration 196/1000 | Loss: 0.00010327
Iteration 197/1000 | Loss: 0.00010307
Iteration 198/1000 | Loss: 0.00010304
Iteration 199/1000 | Loss: 0.00010302
Iteration 200/1000 | Loss: 0.00010297
Iteration 201/1000 | Loss: 0.00010286
Iteration 202/1000 | Loss: 0.00010286
Iteration 203/1000 | Loss: 0.00010283
Iteration 204/1000 | Loss: 0.00010283
Iteration 205/1000 | Loss: 0.00010282
Iteration 206/1000 | Loss: 0.00010280
Iteration 207/1000 | Loss: 0.00010280
Iteration 208/1000 | Loss: 0.00010279
Iteration 209/1000 | Loss: 0.00010279
Iteration 210/1000 | Loss: 0.00010278
Iteration 211/1000 | Loss: 0.00010276
Iteration 212/1000 | Loss: 0.00010276
Iteration 213/1000 | Loss: 0.00021870
Iteration 214/1000 | Loss: 0.00025498
Iteration 215/1000 | Loss: 0.00023707
Iteration 216/1000 | Loss: 0.00017827
Iteration 217/1000 | Loss: 0.00023357
Iteration 218/1000 | Loss: 0.00025939
Iteration 219/1000 | Loss: 0.00019886
Iteration 220/1000 | Loss: 0.00024055
Iteration 221/1000 | Loss: 0.00019862
Iteration 222/1000 | Loss: 0.00022403
Iteration 223/1000 | Loss: 0.00016664
Iteration 224/1000 | Loss: 0.00017587
Iteration 225/1000 | Loss: 0.00018148
Iteration 226/1000 | Loss: 0.00019206
Iteration 227/1000 | Loss: 0.00010928
Iteration 228/1000 | Loss: 0.00024385
Iteration 229/1000 | Loss: 0.00016874
Iteration 230/1000 | Loss: 0.00010824
Iteration 231/1000 | Loss: 0.00010674
Iteration 232/1000 | Loss: 0.00023006
Iteration 233/1000 | Loss: 0.00017615
Iteration 234/1000 | Loss: 0.00023404
Iteration 235/1000 | Loss: 0.00020450
Iteration 236/1000 | Loss: 0.00018705
Iteration 237/1000 | Loss: 0.00020409
Iteration 238/1000 | Loss: 0.00015683
Iteration 239/1000 | Loss: 0.00010963
Iteration 240/1000 | Loss: 0.00046946
Iteration 241/1000 | Loss: 0.00025539
Iteration 242/1000 | Loss: 0.00041922
Iteration 243/1000 | Loss: 0.00037237
Iteration 244/1000 | Loss: 0.00021065
Iteration 245/1000 | Loss: 0.00023501
Iteration 246/1000 | Loss: 0.00011782
Iteration 247/1000 | Loss: 0.00021509
Iteration 248/1000 | Loss: 0.00018462
Iteration 249/1000 | Loss: 0.00012601
Iteration 250/1000 | Loss: 0.00017105
Iteration 251/1000 | Loss: 0.00024352
Iteration 252/1000 | Loss: 0.00020035
Iteration 253/1000 | Loss: 0.00020453
Iteration 254/1000 | Loss: 0.00010952
Iteration 255/1000 | Loss: 0.00017514
Iteration 256/1000 | Loss: 0.00016188
Iteration 257/1000 | Loss: 0.00017895
Iteration 258/1000 | Loss: 0.00019305
Iteration 259/1000 | Loss: 0.00010903
Iteration 260/1000 | Loss: 0.00023629
Iteration 261/1000 | Loss: 0.00016795
Iteration 262/1000 | Loss: 0.00023199
Iteration 263/1000 | Loss: 0.00014618
Iteration 264/1000 | Loss: 0.00036953
Iteration 265/1000 | Loss: 0.00017532
Iteration 266/1000 | Loss: 0.00017934
Iteration 267/1000 | Loss: 0.00012695
Iteration 268/1000 | Loss: 0.00010551
Iteration 269/1000 | Loss: 0.00018175
Iteration 270/1000 | Loss: 0.00023285
Iteration 271/1000 | Loss: 0.00011368
Iteration 272/1000 | Loss: 0.00011406
Iteration 273/1000 | Loss: 0.00011041
Iteration 274/1000 | Loss: 0.00011222
Iteration 275/1000 | Loss: 0.00010784
Iteration 276/1000 | Loss: 0.00010967
Iteration 277/1000 | Loss: 0.00010492
Iteration 278/1000 | Loss: 0.00010433
Iteration 279/1000 | Loss: 0.00010357
Iteration 280/1000 | Loss: 0.00010313
Iteration 281/1000 | Loss: 0.00010290
Iteration 282/1000 | Loss: 0.00010289
Iteration 283/1000 | Loss: 0.00010288
Iteration 284/1000 | Loss: 0.00010287
Iteration 285/1000 | Loss: 0.00010284
Iteration 286/1000 | Loss: 0.00010264
Iteration 287/1000 | Loss: 0.00010261
Iteration 288/1000 | Loss: 0.00010255
Iteration 289/1000 | Loss: 0.00010250
Iteration 290/1000 | Loss: 0.00010240
Iteration 291/1000 | Loss: 0.00010235
Iteration 292/1000 | Loss: 0.00010235
Iteration 293/1000 | Loss: 0.00027334
Iteration 294/1000 | Loss: 0.00017723
Iteration 295/1000 | Loss: 0.00025921
Iteration 296/1000 | Loss: 0.00010734
Iteration 297/1000 | Loss: 0.00010399
Iteration 298/1000 | Loss: 0.00010267
Iteration 299/1000 | Loss: 0.00010165
Iteration 300/1000 | Loss: 0.00010108
Iteration 301/1000 | Loss: 0.00010072
Iteration 302/1000 | Loss: 0.00010061
Iteration 303/1000 | Loss: 0.00010055
Iteration 304/1000 | Loss: 0.00010055
Iteration 305/1000 | Loss: 0.00010054
Iteration 306/1000 | Loss: 0.00010053
Iteration 307/1000 | Loss: 0.00010053
Iteration 308/1000 | Loss: 0.00010053
Iteration 309/1000 | Loss: 0.00010052
Iteration 310/1000 | Loss: 0.00010052
Iteration 311/1000 | Loss: 0.00010051
Iteration 312/1000 | Loss: 0.00010051
Iteration 313/1000 | Loss: 0.00010051
Iteration 314/1000 | Loss: 0.00010050
Iteration 315/1000 | Loss: 0.00010050
Iteration 316/1000 | Loss: 0.00010050
Iteration 317/1000 | Loss: 0.00010050
Iteration 318/1000 | Loss: 0.00010049
Iteration 319/1000 | Loss: 0.00010049
Iteration 320/1000 | Loss: 0.00010049
Iteration 321/1000 | Loss: 0.00010049
Iteration 322/1000 | Loss: 0.00010049
Iteration 323/1000 | Loss: 0.00010048
Iteration 324/1000 | Loss: 0.00010048
Iteration 325/1000 | Loss: 0.00010048
Iteration 326/1000 | Loss: 0.00010048
Iteration 327/1000 | Loss: 0.00010048
Iteration 328/1000 | Loss: 0.00010047
Iteration 329/1000 | Loss: 0.00010047
Iteration 330/1000 | Loss: 0.00010047
Iteration 331/1000 | Loss: 0.00010047
Iteration 332/1000 | Loss: 0.00010047
Iteration 333/1000 | Loss: 0.00010047
Iteration 334/1000 | Loss: 0.00010047
Iteration 335/1000 | Loss: 0.00010047
Iteration 336/1000 | Loss: 0.00010047
Iteration 337/1000 | Loss: 0.00010046
Iteration 338/1000 | Loss: 0.00010046
Iteration 339/1000 | Loss: 0.00010046
Iteration 340/1000 | Loss: 0.00010046
Iteration 341/1000 | Loss: 0.00010046
Iteration 342/1000 | Loss: 0.00010046
Iteration 343/1000 | Loss: 0.00010046
Iteration 344/1000 | Loss: 0.00010046
Iteration 345/1000 | Loss: 0.00010046
Iteration 346/1000 | Loss: 0.00010046
Iteration 347/1000 | Loss: 0.00010046
Iteration 348/1000 | Loss: 0.00010045
Iteration 349/1000 | Loss: 0.00010045
Iteration 350/1000 | Loss: 0.00010045
Iteration 351/1000 | Loss: 0.00010045
Iteration 352/1000 | Loss: 0.00010045
Iteration 353/1000 | Loss: 0.00010045
Iteration 354/1000 | Loss: 0.00010045
Iteration 355/1000 | Loss: 0.00010045
Iteration 356/1000 | Loss: 0.00010045
Iteration 357/1000 | Loss: 0.00010045
Iteration 358/1000 | Loss: 0.00010045
Iteration 359/1000 | Loss: 0.00010045
Iteration 360/1000 | Loss: 0.00010045
Iteration 361/1000 | Loss: 0.00010045
Iteration 362/1000 | Loss: 0.00010045
Iteration 363/1000 | Loss: 0.00010045
Iteration 364/1000 | Loss: 0.00010045
Iteration 365/1000 | Loss: 0.00010045
Iteration 366/1000 | Loss: 0.00010045
Iteration 367/1000 | Loss: 0.00010045
Iteration 368/1000 | Loss: 0.00010045
Iteration 369/1000 | Loss: 0.00010045
Iteration 370/1000 | Loss: 0.00010045
Iteration 371/1000 | Loss: 0.00010045
Iteration 372/1000 | Loss: 0.00010045
Iteration 373/1000 | Loss: 0.00010045
Iteration 374/1000 | Loss: 0.00010045
Iteration 375/1000 | Loss: 0.00010045
Iteration 376/1000 | Loss: 0.00010045
Iteration 377/1000 | Loss: 0.00010045
Iteration 378/1000 | Loss: 0.00010045
Iteration 379/1000 | Loss: 0.00010045
Iteration 380/1000 | Loss: 0.00010045
Iteration 381/1000 | Loss: 0.00010045
Iteration 382/1000 | Loss: 0.00010045
Iteration 383/1000 | Loss: 0.00010045
Iteration 384/1000 | Loss: 0.00010045
Iteration 385/1000 | Loss: 0.00010045
Iteration 386/1000 | Loss: 0.00010045
Iteration 387/1000 | Loss: 0.00010045
Iteration 388/1000 | Loss: 0.00010045
Iteration 389/1000 | Loss: 0.00010045
Iteration 390/1000 | Loss: 0.00010045
Iteration 391/1000 | Loss: 0.00010045
Iteration 392/1000 | Loss: 0.00010045
Iteration 393/1000 | Loss: 0.00010045
Iteration 394/1000 | Loss: 0.00010045
Iteration 395/1000 | Loss: 0.00010045
Iteration 396/1000 | Loss: 0.00010045
Iteration 397/1000 | Loss: 0.00010045
Iteration 398/1000 | Loss: 0.00010045
Iteration 399/1000 | Loss: 0.00010045
Iteration 400/1000 | Loss: 0.00010045
Iteration 401/1000 | Loss: 0.00010045
Iteration 402/1000 | Loss: 0.00010045
Iteration 403/1000 | Loss: 0.00010045
Iteration 404/1000 | Loss: 0.00010045
Iteration 405/1000 | Loss: 0.00010045
Iteration 406/1000 | Loss: 0.00010045
Iteration 407/1000 | Loss: 0.00010045
Iteration 408/1000 | Loss: 0.00010045
Iteration 409/1000 | Loss: 0.00010045
Iteration 410/1000 | Loss: 0.00010045
Iteration 411/1000 | Loss: 0.00010045
Iteration 412/1000 | Loss: 0.00010045
Iteration 413/1000 | Loss: 0.00010045
Iteration 414/1000 | Loss: 0.00010045
Iteration 415/1000 | Loss: 0.00010045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 415. Stopping optimization.
Last 5 losses: [0.0001004496225505136, 0.0001004496225505136, 0.0001004496225505136, 0.0001004496225505136, 0.0001004496225505136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001004496225505136

Optimization complete. Final v2v error: 6.108538627624512 mm

Highest mean error: 13.7079439163208 mm for frame 93

Lowest mean error: 4.149872779846191 mm for frame 221

Saving results

Total time: 510.5465977191925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859264
Iteration 2/25 | Loss: 0.00094313
Iteration 3/25 | Loss: 0.00074455
Iteration 4/25 | Loss: 0.00070627
Iteration 5/25 | Loss: 0.00069634
Iteration 6/25 | Loss: 0.00069324
Iteration 7/25 | Loss: 0.00069221
Iteration 8/25 | Loss: 0.00069592
Iteration 9/25 | Loss: 0.00069563
Iteration 10/25 | Loss: 0.00069164
Iteration 11/25 | Loss: 0.00069382
Iteration 12/25 | Loss: 0.00069341
Iteration 13/25 | Loss: 0.00069284
Iteration 14/25 | Loss: 0.00069343
Iteration 15/25 | Loss: 0.00069330
Iteration 16/25 | Loss: 0.00069212
Iteration 17/25 | Loss: 0.00069307
Iteration 18/25 | Loss: 0.00069308
Iteration 19/25 | Loss: 0.00069500
Iteration 20/25 | Loss: 0.00069185
Iteration 21/25 | Loss: 0.00068872
Iteration 22/25 | Loss: 0.00068576
Iteration 23/25 | Loss: 0.00068371
Iteration 24/25 | Loss: 0.00068292
Iteration 25/25 | Loss: 0.00068266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42412007
Iteration 2/25 | Loss: 0.00124629
Iteration 3/25 | Loss: 0.00124628
Iteration 4/25 | Loss: 0.00124628
Iteration 5/25 | Loss: 0.00124628
Iteration 6/25 | Loss: 0.00124628
Iteration 7/25 | Loss: 0.00124628
Iteration 8/25 | Loss: 0.00124628
Iteration 9/25 | Loss: 0.00124628
Iteration 10/25 | Loss: 0.00124628
Iteration 11/25 | Loss: 0.00124628
Iteration 12/25 | Loss: 0.00124628
Iteration 13/25 | Loss: 0.00124628
Iteration 14/25 | Loss: 0.00124628
Iteration 15/25 | Loss: 0.00124628
Iteration 16/25 | Loss: 0.00124628
Iteration 17/25 | Loss: 0.00124628
Iteration 18/25 | Loss: 0.00124628
Iteration 19/25 | Loss: 0.00124628
Iteration 20/25 | Loss: 0.00124628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012462784070521593, 0.0012462784070521593, 0.0012462784070521593, 0.0012462784070521593, 0.0012462784070521593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012462784070521593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124628
Iteration 2/1000 | Loss: 0.00018513
Iteration 3/1000 | Loss: 0.00012331
Iteration 4/1000 | Loss: 0.00010643
Iteration 5/1000 | Loss: 0.00009604
Iteration 6/1000 | Loss: 0.00008908
Iteration 7/1000 | Loss: 0.00027322
Iteration 8/1000 | Loss: 0.00008152
Iteration 9/1000 | Loss: 0.00018171
Iteration 10/1000 | Loss: 0.00007389
Iteration 11/1000 | Loss: 0.00007026
Iteration 12/1000 | Loss: 0.00006854
Iteration 13/1000 | Loss: 0.00006692
Iteration 14/1000 | Loss: 0.00006518
Iteration 15/1000 | Loss: 0.00006403
Iteration 16/1000 | Loss: 0.00006301
Iteration 17/1000 | Loss: 0.00006206
Iteration 18/1000 | Loss: 0.00065900
Iteration 19/1000 | Loss: 0.00007238
Iteration 20/1000 | Loss: 0.00006009
Iteration 21/1000 | Loss: 0.00005799
Iteration 22/1000 | Loss: 0.00005690
Iteration 23/1000 | Loss: 0.00166724
Iteration 24/1000 | Loss: 0.00136395
Iteration 25/1000 | Loss: 0.00008675
Iteration 26/1000 | Loss: 0.00005991
Iteration 27/1000 | Loss: 0.00005158
Iteration 28/1000 | Loss: 0.00004695
Iteration 29/1000 | Loss: 0.00066471
Iteration 30/1000 | Loss: 0.00004869
Iteration 31/1000 | Loss: 0.00004295
Iteration 32/1000 | Loss: 0.00061395
Iteration 33/1000 | Loss: 0.00005724
Iteration 34/1000 | Loss: 0.00004024
Iteration 35/1000 | Loss: 0.00003858
Iteration 36/1000 | Loss: 0.00059653
Iteration 37/1000 | Loss: 0.00003910
Iteration 38/1000 | Loss: 0.00003573
Iteration 39/1000 | Loss: 0.00003472
Iteration 40/1000 | Loss: 0.00003397
Iteration 41/1000 | Loss: 0.00003340
Iteration 42/1000 | Loss: 0.00003306
Iteration 43/1000 | Loss: 0.00003276
Iteration 44/1000 | Loss: 0.00003258
Iteration 45/1000 | Loss: 0.00003240
Iteration 46/1000 | Loss: 0.00003231
Iteration 47/1000 | Loss: 0.00003229
Iteration 48/1000 | Loss: 0.00003228
Iteration 49/1000 | Loss: 0.00003226
Iteration 50/1000 | Loss: 0.00003225
Iteration 51/1000 | Loss: 0.00003213
Iteration 52/1000 | Loss: 0.00003213
Iteration 53/1000 | Loss: 0.00003211
Iteration 54/1000 | Loss: 0.00003211
Iteration 55/1000 | Loss: 0.00003210
Iteration 56/1000 | Loss: 0.00003209
Iteration 57/1000 | Loss: 0.00003209
Iteration 58/1000 | Loss: 0.00003208
Iteration 59/1000 | Loss: 0.00003208
Iteration 60/1000 | Loss: 0.00003207
Iteration 61/1000 | Loss: 0.00003207
Iteration 62/1000 | Loss: 0.00003207
Iteration 63/1000 | Loss: 0.00003206
Iteration 64/1000 | Loss: 0.00003205
Iteration 65/1000 | Loss: 0.00003205
Iteration 66/1000 | Loss: 0.00003204
Iteration 67/1000 | Loss: 0.00003204
Iteration 68/1000 | Loss: 0.00003204
Iteration 69/1000 | Loss: 0.00003204
Iteration 70/1000 | Loss: 0.00003203
Iteration 71/1000 | Loss: 0.00003203
Iteration 72/1000 | Loss: 0.00003203
Iteration 73/1000 | Loss: 0.00003203
Iteration 74/1000 | Loss: 0.00003202
Iteration 75/1000 | Loss: 0.00003202
Iteration 76/1000 | Loss: 0.00003201
Iteration 77/1000 | Loss: 0.00003200
Iteration 78/1000 | Loss: 0.00003200
Iteration 79/1000 | Loss: 0.00003200
Iteration 80/1000 | Loss: 0.00003200
Iteration 81/1000 | Loss: 0.00003199
Iteration 82/1000 | Loss: 0.00003199
Iteration 83/1000 | Loss: 0.00003199
Iteration 84/1000 | Loss: 0.00003199
Iteration 85/1000 | Loss: 0.00003199
Iteration 86/1000 | Loss: 0.00003199
Iteration 87/1000 | Loss: 0.00003199
Iteration 88/1000 | Loss: 0.00003198
Iteration 89/1000 | Loss: 0.00003198
Iteration 90/1000 | Loss: 0.00003198
Iteration 91/1000 | Loss: 0.00003198
Iteration 92/1000 | Loss: 0.00003198
Iteration 93/1000 | Loss: 0.00003198
Iteration 94/1000 | Loss: 0.00003198
Iteration 95/1000 | Loss: 0.00003198
Iteration 96/1000 | Loss: 0.00003198
Iteration 97/1000 | Loss: 0.00003198
Iteration 98/1000 | Loss: 0.00003198
Iteration 99/1000 | Loss: 0.00003198
Iteration 100/1000 | Loss: 0.00003197
Iteration 101/1000 | Loss: 0.00003197
Iteration 102/1000 | Loss: 0.00003197
Iteration 103/1000 | Loss: 0.00003197
Iteration 104/1000 | Loss: 0.00003197
Iteration 105/1000 | Loss: 0.00003196
Iteration 106/1000 | Loss: 0.00003196
Iteration 107/1000 | Loss: 0.00003196
Iteration 108/1000 | Loss: 0.00003196
Iteration 109/1000 | Loss: 0.00003196
Iteration 110/1000 | Loss: 0.00003196
Iteration 111/1000 | Loss: 0.00003196
Iteration 112/1000 | Loss: 0.00003196
Iteration 113/1000 | Loss: 0.00003195
Iteration 114/1000 | Loss: 0.00003195
Iteration 115/1000 | Loss: 0.00003195
Iteration 116/1000 | Loss: 0.00003195
Iteration 117/1000 | Loss: 0.00003195
Iteration 118/1000 | Loss: 0.00003195
Iteration 119/1000 | Loss: 0.00003195
Iteration 120/1000 | Loss: 0.00003195
Iteration 121/1000 | Loss: 0.00003195
Iteration 122/1000 | Loss: 0.00003194
Iteration 123/1000 | Loss: 0.00056644
Iteration 124/1000 | Loss: 0.00003892
Iteration 125/1000 | Loss: 0.00003526
Iteration 126/1000 | Loss: 0.00003299
Iteration 127/1000 | Loss: 0.00003129
Iteration 128/1000 | Loss: 0.00003026
Iteration 129/1000 | Loss: 0.00002995
Iteration 130/1000 | Loss: 0.00002988
Iteration 131/1000 | Loss: 0.00002987
Iteration 132/1000 | Loss: 0.00002986
Iteration 133/1000 | Loss: 0.00002981
Iteration 134/1000 | Loss: 0.00002976
Iteration 135/1000 | Loss: 0.00002973
Iteration 136/1000 | Loss: 0.00002972
Iteration 137/1000 | Loss: 0.00002966
Iteration 138/1000 | Loss: 0.00002966
Iteration 139/1000 | Loss: 0.00002964
Iteration 140/1000 | Loss: 0.00002963
Iteration 141/1000 | Loss: 0.00002963
Iteration 142/1000 | Loss: 0.00002963
Iteration 143/1000 | Loss: 0.00002962
Iteration 144/1000 | Loss: 0.00002958
Iteration 145/1000 | Loss: 0.00002958
Iteration 146/1000 | Loss: 0.00002958
Iteration 147/1000 | Loss: 0.00002957
Iteration 148/1000 | Loss: 0.00002957
Iteration 149/1000 | Loss: 0.00002956
Iteration 150/1000 | Loss: 0.00002956
Iteration 151/1000 | Loss: 0.00002956
Iteration 152/1000 | Loss: 0.00002955
Iteration 153/1000 | Loss: 0.00002954
Iteration 154/1000 | Loss: 0.00002954
Iteration 155/1000 | Loss: 0.00002954
Iteration 156/1000 | Loss: 0.00002954
Iteration 157/1000 | Loss: 0.00002953
Iteration 158/1000 | Loss: 0.00002953
Iteration 159/1000 | Loss: 0.00002953
Iteration 160/1000 | Loss: 0.00002953
Iteration 161/1000 | Loss: 0.00002953
Iteration 162/1000 | Loss: 0.00002953
Iteration 163/1000 | Loss: 0.00002953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.9533866836572997e-05, 2.9533866836572997e-05, 2.9533866836572997e-05, 2.9533866836572997e-05, 2.9533866836572997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9533866836572997e-05

Optimization complete. Final v2v error: 3.6334457397460938 mm

Highest mean error: 11.636466979980469 mm for frame 95

Lowest mean error: 3.066254138946533 mm for frame 52

Saving results

Total time: 138.87917590141296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591355
Iteration 2/25 | Loss: 0.00106603
Iteration 3/25 | Loss: 0.00073707
Iteration 4/25 | Loss: 0.00068044
Iteration 5/25 | Loss: 0.00066505
Iteration 6/25 | Loss: 0.00065743
Iteration 7/25 | Loss: 0.00065581
Iteration 8/25 | Loss: 0.00065511
Iteration 9/25 | Loss: 0.00065470
Iteration 10/25 | Loss: 0.00065446
Iteration 11/25 | Loss: 0.00065484
Iteration 12/25 | Loss: 0.00065511
Iteration 13/25 | Loss: 0.00065221
Iteration 14/25 | Loss: 0.00064845
Iteration 15/25 | Loss: 0.00064664
Iteration 16/25 | Loss: 0.00064575
Iteration 17/25 | Loss: 0.00064541
Iteration 18/25 | Loss: 0.00064529
Iteration 19/25 | Loss: 0.00064524
Iteration 20/25 | Loss: 0.00064524
Iteration 21/25 | Loss: 0.00064524
Iteration 22/25 | Loss: 0.00064524
Iteration 23/25 | Loss: 0.00064524
Iteration 24/25 | Loss: 0.00064523
Iteration 25/25 | Loss: 0.00064523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52919066
Iteration 2/25 | Loss: 0.00015752
Iteration 3/25 | Loss: 0.00015752
Iteration 4/25 | Loss: 0.00015752
Iteration 5/25 | Loss: 0.00015752
Iteration 6/25 | Loss: 0.00015752
Iteration 7/25 | Loss: 0.00015752
Iteration 8/25 | Loss: 0.00015752
Iteration 9/25 | Loss: 0.00015752
Iteration 10/25 | Loss: 0.00015752
Iteration 11/25 | Loss: 0.00015752
Iteration 12/25 | Loss: 0.00015752
Iteration 13/25 | Loss: 0.00015752
Iteration 14/25 | Loss: 0.00015752
Iteration 15/25 | Loss: 0.00015752
Iteration 16/25 | Loss: 0.00015752
Iteration 17/25 | Loss: 0.00015752
Iteration 18/25 | Loss: 0.00015752
Iteration 19/25 | Loss: 0.00015752
Iteration 20/25 | Loss: 0.00015752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00015751816681586206, 0.00015751816681586206, 0.00015751816681586206, 0.00015751816681586206, 0.00015751816681586206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015751816681586206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015752
Iteration 2/1000 | Loss: 0.00003343
Iteration 3/1000 | Loss: 0.00002388
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002082
Iteration 6/1000 | Loss: 0.00002003
Iteration 7/1000 | Loss: 0.00001969
Iteration 8/1000 | Loss: 0.00001943
Iteration 9/1000 | Loss: 0.00001925
Iteration 10/1000 | Loss: 0.00001917
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001894
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001889
Iteration 21/1000 | Loss: 0.00001887
Iteration 22/1000 | Loss: 0.00001885
Iteration 23/1000 | Loss: 0.00001885
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001880
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00001877
Iteration 32/1000 | Loss: 0.00001876
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001875
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001875
Iteration 39/1000 | Loss: 0.00001875
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001874
Iteration 42/1000 | Loss: 0.00001874
Iteration 43/1000 | Loss: 0.00001874
Iteration 44/1000 | Loss: 0.00001874
Iteration 45/1000 | Loss: 0.00001873
Iteration 46/1000 | Loss: 0.00001873
Iteration 47/1000 | Loss: 0.00001873
Iteration 48/1000 | Loss: 0.00001872
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001872
Iteration 52/1000 | Loss: 0.00001872
Iteration 53/1000 | Loss: 0.00001872
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001872
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001872
Iteration 60/1000 | Loss: 0.00001872
Iteration 61/1000 | Loss: 0.00001872
Iteration 62/1000 | Loss: 0.00001872
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001872
Iteration 66/1000 | Loss: 0.00001872
Iteration 67/1000 | Loss: 0.00001872
Iteration 68/1000 | Loss: 0.00001872
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001872
Iteration 71/1000 | Loss: 0.00001872
Iteration 72/1000 | Loss: 0.00001872
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.8721404558164068e-05, 1.8721404558164068e-05, 1.8721404558164068e-05, 1.8721404558164068e-05, 1.8721404558164068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8721404558164068e-05

Optimization complete. Final v2v error: 3.6356358528137207 mm

Highest mean error: 4.368734836578369 mm for frame 205

Lowest mean error: 2.891190528869629 mm for frame 186

Saving results

Total time: 58.36277747154236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407852
Iteration 2/25 | Loss: 0.00078067
Iteration 3/25 | Loss: 0.00066154
Iteration 4/25 | Loss: 0.00063278
Iteration 5/25 | Loss: 0.00062244
Iteration 6/25 | Loss: 0.00062069
Iteration 7/25 | Loss: 0.00062031
Iteration 8/25 | Loss: 0.00062031
Iteration 9/25 | Loss: 0.00062031
Iteration 10/25 | Loss: 0.00062031
Iteration 11/25 | Loss: 0.00062031
Iteration 12/25 | Loss: 0.00062031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006203139200806618, 0.0006203139200806618, 0.0006203139200806618, 0.0006203139200806618, 0.0006203139200806618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006203139200806618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.37462783
Iteration 2/25 | Loss: 0.00016980
Iteration 3/25 | Loss: 0.00016979
Iteration 4/25 | Loss: 0.00016979
Iteration 5/25 | Loss: 0.00016979
Iteration 6/25 | Loss: 0.00016979
Iteration 7/25 | Loss: 0.00016979
Iteration 8/25 | Loss: 0.00016979
Iteration 9/25 | Loss: 0.00016979
Iteration 10/25 | Loss: 0.00016979
Iteration 11/25 | Loss: 0.00016979
Iteration 12/25 | Loss: 0.00016979
Iteration 13/25 | Loss: 0.00016979
Iteration 14/25 | Loss: 0.00016979
Iteration 15/25 | Loss: 0.00016979
Iteration 16/25 | Loss: 0.00016979
Iteration 17/25 | Loss: 0.00016979
Iteration 18/25 | Loss: 0.00016979
Iteration 19/25 | Loss: 0.00016979
Iteration 20/25 | Loss: 0.00016979
Iteration 21/25 | Loss: 0.00016979
Iteration 22/25 | Loss: 0.00016979
Iteration 23/25 | Loss: 0.00016979
Iteration 24/25 | Loss: 0.00016979
Iteration 25/25 | Loss: 0.00016979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016979
Iteration 2/1000 | Loss: 0.00003882
Iteration 3/1000 | Loss: 0.00002765
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002484
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002316
Iteration 9/1000 | Loss: 0.00002294
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002285
Iteration 12/1000 | Loss: 0.00002275
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002270
Iteration 15/1000 | Loss: 0.00002270
Iteration 16/1000 | Loss: 0.00002269
Iteration 17/1000 | Loss: 0.00002269
Iteration 18/1000 | Loss: 0.00002269
Iteration 19/1000 | Loss: 0.00002268
Iteration 20/1000 | Loss: 0.00002267
Iteration 21/1000 | Loss: 0.00002267
Iteration 22/1000 | Loss: 0.00002265
Iteration 23/1000 | Loss: 0.00002264
Iteration 24/1000 | Loss: 0.00002264
Iteration 25/1000 | Loss: 0.00002263
Iteration 26/1000 | Loss: 0.00002263
Iteration 27/1000 | Loss: 0.00002263
Iteration 28/1000 | Loss: 0.00002262
Iteration 29/1000 | Loss: 0.00002262
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002261
Iteration 33/1000 | Loss: 0.00002261
Iteration 34/1000 | Loss: 0.00002261
Iteration 35/1000 | Loss: 0.00002260
Iteration 36/1000 | Loss: 0.00002260
Iteration 37/1000 | Loss: 0.00002259
Iteration 38/1000 | Loss: 0.00002259
Iteration 39/1000 | Loss: 0.00002259
Iteration 40/1000 | Loss: 0.00002258
Iteration 41/1000 | Loss: 0.00002257
Iteration 42/1000 | Loss: 0.00002256
Iteration 43/1000 | Loss: 0.00002256
Iteration 44/1000 | Loss: 0.00002256
Iteration 45/1000 | Loss: 0.00002255
Iteration 46/1000 | Loss: 0.00002255
Iteration 47/1000 | Loss: 0.00002255
Iteration 48/1000 | Loss: 0.00002254
Iteration 49/1000 | Loss: 0.00002254
Iteration 50/1000 | Loss: 0.00002253
Iteration 51/1000 | Loss: 0.00002253
Iteration 52/1000 | Loss: 0.00002252
Iteration 53/1000 | Loss: 0.00002250
Iteration 54/1000 | Loss: 0.00002249
Iteration 55/1000 | Loss: 0.00002249
Iteration 56/1000 | Loss: 0.00002249
Iteration 57/1000 | Loss: 0.00002249
Iteration 58/1000 | Loss: 0.00002249
Iteration 59/1000 | Loss: 0.00002248
Iteration 60/1000 | Loss: 0.00002248
Iteration 61/1000 | Loss: 0.00002248
Iteration 62/1000 | Loss: 0.00002248
Iteration 63/1000 | Loss: 0.00002248
Iteration 64/1000 | Loss: 0.00002248
Iteration 65/1000 | Loss: 0.00002248
Iteration 66/1000 | Loss: 0.00002248
Iteration 67/1000 | Loss: 0.00002248
Iteration 68/1000 | Loss: 0.00002247
Iteration 69/1000 | Loss: 0.00002247
Iteration 70/1000 | Loss: 0.00002246
Iteration 71/1000 | Loss: 0.00002246
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002246
Iteration 74/1000 | Loss: 0.00002246
Iteration 75/1000 | Loss: 0.00002245
Iteration 76/1000 | Loss: 0.00002245
Iteration 77/1000 | Loss: 0.00002245
Iteration 78/1000 | Loss: 0.00002245
Iteration 79/1000 | Loss: 0.00002245
Iteration 80/1000 | Loss: 0.00002244
Iteration 81/1000 | Loss: 0.00002244
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002244
Iteration 84/1000 | Loss: 0.00002244
Iteration 85/1000 | Loss: 0.00002243
Iteration 86/1000 | Loss: 0.00002243
Iteration 87/1000 | Loss: 0.00002243
Iteration 88/1000 | Loss: 0.00002242
Iteration 89/1000 | Loss: 0.00002242
Iteration 90/1000 | Loss: 0.00002242
Iteration 91/1000 | Loss: 0.00002242
Iteration 92/1000 | Loss: 0.00002242
Iteration 93/1000 | Loss: 0.00002242
Iteration 94/1000 | Loss: 0.00002242
Iteration 95/1000 | Loss: 0.00002242
Iteration 96/1000 | Loss: 0.00002242
Iteration 97/1000 | Loss: 0.00002242
Iteration 98/1000 | Loss: 0.00002242
Iteration 99/1000 | Loss: 0.00002242
Iteration 100/1000 | Loss: 0.00002241
Iteration 101/1000 | Loss: 0.00002241
Iteration 102/1000 | Loss: 0.00002241
Iteration 103/1000 | Loss: 0.00002241
Iteration 104/1000 | Loss: 0.00002241
Iteration 105/1000 | Loss: 0.00002241
Iteration 106/1000 | Loss: 0.00002241
Iteration 107/1000 | Loss: 0.00002241
Iteration 108/1000 | Loss: 0.00002241
Iteration 109/1000 | Loss: 0.00002240
Iteration 110/1000 | Loss: 0.00002240
Iteration 111/1000 | Loss: 0.00002240
Iteration 112/1000 | Loss: 0.00002240
Iteration 113/1000 | Loss: 0.00002240
Iteration 114/1000 | Loss: 0.00002240
Iteration 115/1000 | Loss: 0.00002240
Iteration 116/1000 | Loss: 0.00002240
Iteration 117/1000 | Loss: 0.00002240
Iteration 118/1000 | Loss: 0.00002240
Iteration 119/1000 | Loss: 0.00002240
Iteration 120/1000 | Loss: 0.00002240
Iteration 121/1000 | Loss: 0.00002240
Iteration 122/1000 | Loss: 0.00002240
Iteration 123/1000 | Loss: 0.00002240
Iteration 124/1000 | Loss: 0.00002240
Iteration 125/1000 | Loss: 0.00002240
Iteration 126/1000 | Loss: 0.00002239
Iteration 127/1000 | Loss: 0.00002239
Iteration 128/1000 | Loss: 0.00002239
Iteration 129/1000 | Loss: 0.00002239
Iteration 130/1000 | Loss: 0.00002239
Iteration 131/1000 | Loss: 0.00002239
Iteration 132/1000 | Loss: 0.00002239
Iteration 133/1000 | Loss: 0.00002239
Iteration 134/1000 | Loss: 0.00002239
Iteration 135/1000 | Loss: 0.00002239
Iteration 136/1000 | Loss: 0.00002239
Iteration 137/1000 | Loss: 0.00002239
Iteration 138/1000 | Loss: 0.00002238
Iteration 139/1000 | Loss: 0.00002238
Iteration 140/1000 | Loss: 0.00002238
Iteration 141/1000 | Loss: 0.00002238
Iteration 142/1000 | Loss: 0.00002238
Iteration 143/1000 | Loss: 0.00002238
Iteration 144/1000 | Loss: 0.00002238
Iteration 145/1000 | Loss: 0.00002238
Iteration 146/1000 | Loss: 0.00002238
Iteration 147/1000 | Loss: 0.00002238
Iteration 148/1000 | Loss: 0.00002238
Iteration 149/1000 | Loss: 0.00002238
Iteration 150/1000 | Loss: 0.00002238
Iteration 151/1000 | Loss: 0.00002238
Iteration 152/1000 | Loss: 0.00002238
Iteration 153/1000 | Loss: 0.00002238
Iteration 154/1000 | Loss: 0.00002237
Iteration 155/1000 | Loss: 0.00002237
Iteration 156/1000 | Loss: 0.00002237
Iteration 157/1000 | Loss: 0.00002237
Iteration 158/1000 | Loss: 0.00002237
Iteration 159/1000 | Loss: 0.00002237
Iteration 160/1000 | Loss: 0.00002237
Iteration 161/1000 | Loss: 0.00002237
Iteration 162/1000 | Loss: 0.00002237
Iteration 163/1000 | Loss: 0.00002237
Iteration 164/1000 | Loss: 0.00002237
Iteration 165/1000 | Loss: 0.00002237
Iteration 166/1000 | Loss: 0.00002237
Iteration 167/1000 | Loss: 0.00002237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.2371867089532316e-05, 2.2371867089532316e-05, 2.2371867089532316e-05, 2.2371867089532316e-05, 2.2371867089532316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2371867089532316e-05

Optimization complete. Final v2v error: 3.8423357009887695 mm

Highest mean error: 4.827421188354492 mm for frame 88

Lowest mean error: 3.398709297180176 mm for frame 1

Saving results

Total time: 36.99580383300781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074938
Iteration 2/25 | Loss: 0.01074937
Iteration 3/25 | Loss: 0.01074937
Iteration 4/25 | Loss: 0.01074937
Iteration 5/25 | Loss: 0.01074937
Iteration 6/25 | Loss: 0.01074937
Iteration 7/25 | Loss: 0.01074937
Iteration 8/25 | Loss: 0.01074937
Iteration 9/25 | Loss: 0.01074937
Iteration 10/25 | Loss: 0.01074937
Iteration 11/25 | Loss: 0.01074937
Iteration 12/25 | Loss: 0.01074937
Iteration 13/25 | Loss: 0.01074937
Iteration 14/25 | Loss: 0.01074937
Iteration 15/25 | Loss: 0.01074936
Iteration 16/25 | Loss: 0.01074936
Iteration 17/25 | Loss: 0.01074936
Iteration 18/25 | Loss: 0.01074936
Iteration 19/25 | Loss: 0.01074936
Iteration 20/25 | Loss: 0.01074936
Iteration 21/25 | Loss: 0.01074936
Iteration 22/25 | Loss: 0.01074936
Iteration 23/25 | Loss: 0.01074936
Iteration 24/25 | Loss: 0.01074936
Iteration 25/25 | Loss: 0.01074936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71406162
Iteration 2/25 | Loss: 0.11412843
Iteration 3/25 | Loss: 0.10507362
Iteration 4/25 | Loss: 0.10507362
Iteration 5/25 | Loss: 0.10507362
Iteration 6/25 | Loss: 0.10507362
Iteration 7/25 | Loss: 0.10507362
Iteration 8/25 | Loss: 0.10507362
Iteration 9/25 | Loss: 0.10507362
Iteration 10/25 | Loss: 0.10507362
Iteration 11/25 | Loss: 0.10507362
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.10507361590862274, 0.10507361590862274, 0.10507361590862274, 0.10507361590862274, 0.10507361590862274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10507361590862274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10507362
Iteration 2/1000 | Loss: 0.00406506
Iteration 3/1000 | Loss: 0.00871706
Iteration 4/1000 | Loss: 0.00151639
Iteration 5/1000 | Loss: 0.00085261
Iteration 6/1000 | Loss: 0.00018418
Iteration 7/1000 | Loss: 0.00173915
Iteration 8/1000 | Loss: 0.00009409
Iteration 9/1000 | Loss: 0.00067005
Iteration 10/1000 | Loss: 0.00019738
Iteration 11/1000 | Loss: 0.00050411
Iteration 12/1000 | Loss: 0.00006989
Iteration 13/1000 | Loss: 0.00007348
Iteration 14/1000 | Loss: 0.00008621
Iteration 15/1000 | Loss: 0.00016730
Iteration 16/1000 | Loss: 0.00013925
Iteration 17/1000 | Loss: 0.00011496
Iteration 18/1000 | Loss: 0.00004239
Iteration 19/1000 | Loss: 0.00007301
Iteration 20/1000 | Loss: 0.00014047
Iteration 21/1000 | Loss: 0.00010249
Iteration 22/1000 | Loss: 0.00009202
Iteration 23/1000 | Loss: 0.00005845
Iteration 24/1000 | Loss: 0.00014358
Iteration 25/1000 | Loss: 0.00004856
Iteration 26/1000 | Loss: 0.00004630
Iteration 27/1000 | Loss: 0.00003871
Iteration 28/1000 | Loss: 0.00006802
Iteration 29/1000 | Loss: 0.00011122
Iteration 30/1000 | Loss: 0.00005308
Iteration 31/1000 | Loss: 0.00004099
Iteration 32/1000 | Loss: 0.00012173
Iteration 33/1000 | Loss: 0.00044136
Iteration 34/1000 | Loss: 0.00056914
Iteration 35/1000 | Loss: 0.00035915
Iteration 36/1000 | Loss: 0.00003729
Iteration 37/1000 | Loss: 0.00003357
Iteration 38/1000 | Loss: 0.00006246
Iteration 39/1000 | Loss: 0.00076475
Iteration 40/1000 | Loss: 0.00143954
Iteration 41/1000 | Loss: 0.00011794
Iteration 42/1000 | Loss: 0.00003656
Iteration 43/1000 | Loss: 0.00003484
Iteration 44/1000 | Loss: 0.00003484
Iteration 45/1000 | Loss: 0.00008583
Iteration 46/1000 | Loss: 0.00004220
Iteration 47/1000 | Loss: 0.00003505
Iteration 48/1000 | Loss: 0.00003638
Iteration 49/1000 | Loss: 0.00002713
Iteration 50/1000 | Loss: 0.00002937
Iteration 51/1000 | Loss: 0.00008226
Iteration 52/1000 | Loss: 0.00004989
Iteration 53/1000 | Loss: 0.00002675
Iteration 54/1000 | Loss: 0.00003757
Iteration 55/1000 | Loss: 0.00002672
Iteration 56/1000 | Loss: 0.00002670
Iteration 57/1000 | Loss: 0.00002670
Iteration 58/1000 | Loss: 0.00002670
Iteration 59/1000 | Loss: 0.00002670
Iteration 60/1000 | Loss: 0.00002670
Iteration 61/1000 | Loss: 0.00002670
Iteration 62/1000 | Loss: 0.00002670
Iteration 63/1000 | Loss: 0.00002669
Iteration 64/1000 | Loss: 0.00002669
Iteration 65/1000 | Loss: 0.00002669
Iteration 66/1000 | Loss: 0.00002669
Iteration 67/1000 | Loss: 0.00002669
Iteration 68/1000 | Loss: 0.00002668
Iteration 69/1000 | Loss: 0.00002668
Iteration 70/1000 | Loss: 0.00002668
Iteration 71/1000 | Loss: 0.00002668
Iteration 72/1000 | Loss: 0.00002667
Iteration 73/1000 | Loss: 0.00002667
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002667
Iteration 76/1000 | Loss: 0.00002667
Iteration 77/1000 | Loss: 0.00002666
Iteration 78/1000 | Loss: 0.00002666
Iteration 79/1000 | Loss: 0.00002665
Iteration 80/1000 | Loss: 0.00002665
Iteration 81/1000 | Loss: 0.00002665
Iteration 82/1000 | Loss: 0.00002665
Iteration 83/1000 | Loss: 0.00002665
Iteration 84/1000 | Loss: 0.00002664
Iteration 85/1000 | Loss: 0.00002664
Iteration 86/1000 | Loss: 0.00002663
Iteration 87/1000 | Loss: 0.00002663
Iteration 88/1000 | Loss: 0.00002663
Iteration 89/1000 | Loss: 0.00002663
Iteration 90/1000 | Loss: 0.00002662
Iteration 91/1000 | Loss: 0.00002662
Iteration 92/1000 | Loss: 0.00002662
Iteration 93/1000 | Loss: 0.00002662
Iteration 94/1000 | Loss: 0.00003255
Iteration 95/1000 | Loss: 0.00002689
Iteration 96/1000 | Loss: 0.00002870
Iteration 97/1000 | Loss: 0.00003680
Iteration 98/1000 | Loss: 0.00002659
Iteration 99/1000 | Loss: 0.00002659
Iteration 100/1000 | Loss: 0.00002659
Iteration 101/1000 | Loss: 0.00002659
Iteration 102/1000 | Loss: 0.00002658
Iteration 103/1000 | Loss: 0.00002658
Iteration 104/1000 | Loss: 0.00002658
Iteration 105/1000 | Loss: 0.00002658
Iteration 106/1000 | Loss: 0.00002657
Iteration 107/1000 | Loss: 0.00002657
Iteration 108/1000 | Loss: 0.00002657
Iteration 109/1000 | Loss: 0.00002656
Iteration 110/1000 | Loss: 0.00002656
Iteration 111/1000 | Loss: 0.00002656
Iteration 112/1000 | Loss: 0.00002656
Iteration 113/1000 | Loss: 0.00002924
Iteration 114/1000 | Loss: 0.00002923
Iteration 115/1000 | Loss: 0.00002923
Iteration 116/1000 | Loss: 0.00017285
Iteration 117/1000 | Loss: 0.00011528
Iteration 118/1000 | Loss: 0.00003956
Iteration 119/1000 | Loss: 0.00003328
Iteration 120/1000 | Loss: 0.00007417
Iteration 121/1000 | Loss: 0.00002863
Iteration 122/1000 | Loss: 0.00002771
Iteration 123/1000 | Loss: 0.00002911
Iteration 124/1000 | Loss: 0.00002911
Iteration 125/1000 | Loss: 0.00007579
Iteration 126/1000 | Loss: 0.00003013
Iteration 127/1000 | Loss: 0.00002649
Iteration 128/1000 | Loss: 0.00002987
Iteration 129/1000 | Loss: 0.00005361
Iteration 130/1000 | Loss: 0.00002645
Iteration 131/1000 | Loss: 0.00002644
Iteration 132/1000 | Loss: 0.00002644
Iteration 133/1000 | Loss: 0.00002644
Iteration 134/1000 | Loss: 0.00002644
Iteration 135/1000 | Loss: 0.00002644
Iteration 136/1000 | Loss: 0.00002644
Iteration 137/1000 | Loss: 0.00002644
Iteration 138/1000 | Loss: 0.00002644
Iteration 139/1000 | Loss: 0.00002643
Iteration 140/1000 | Loss: 0.00002643
Iteration 141/1000 | Loss: 0.00002643
Iteration 142/1000 | Loss: 0.00002643
Iteration 143/1000 | Loss: 0.00002643
Iteration 144/1000 | Loss: 0.00002643
Iteration 145/1000 | Loss: 0.00002643
Iteration 146/1000 | Loss: 0.00002643
Iteration 147/1000 | Loss: 0.00002643
Iteration 148/1000 | Loss: 0.00002643
Iteration 149/1000 | Loss: 0.00002643
Iteration 150/1000 | Loss: 0.00002643
Iteration 151/1000 | Loss: 0.00002643
Iteration 152/1000 | Loss: 0.00002642
Iteration 153/1000 | Loss: 0.00002642
Iteration 154/1000 | Loss: 0.00002642
Iteration 155/1000 | Loss: 0.00002642
Iteration 156/1000 | Loss: 0.00002642
Iteration 157/1000 | Loss: 0.00002642
Iteration 158/1000 | Loss: 0.00002642
Iteration 159/1000 | Loss: 0.00002642
Iteration 160/1000 | Loss: 0.00002642
Iteration 161/1000 | Loss: 0.00002642
Iteration 162/1000 | Loss: 0.00002642
Iteration 163/1000 | Loss: 0.00002642
Iteration 164/1000 | Loss: 0.00002642
Iteration 165/1000 | Loss: 0.00002642
Iteration 166/1000 | Loss: 0.00002642
Iteration 167/1000 | Loss: 0.00002642
Iteration 168/1000 | Loss: 0.00002642
Iteration 169/1000 | Loss: 0.00002642
Iteration 170/1000 | Loss: 0.00002642
Iteration 171/1000 | Loss: 0.00002642
Iteration 172/1000 | Loss: 0.00002642
Iteration 173/1000 | Loss: 0.00002642
Iteration 174/1000 | Loss: 0.00002642
Iteration 175/1000 | Loss: 0.00002642
Iteration 176/1000 | Loss: 0.00002642
Iteration 177/1000 | Loss: 0.00002642
Iteration 178/1000 | Loss: 0.00002642
Iteration 179/1000 | Loss: 0.00002642
Iteration 180/1000 | Loss: 0.00002642
Iteration 181/1000 | Loss: 0.00002642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.6416459149913862e-05, 2.6416459149913862e-05, 2.6416459149913862e-05, 2.6416459149913862e-05, 2.6416459149913862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6416459149913862e-05

Optimization complete. Final v2v error: 4.2753586769104 mm

Highest mean error: 4.671645641326904 mm for frame 26

Lowest mean error: 3.757356643676758 mm for frame 3

Saving results

Total time: 119.8948757648468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991053
Iteration 2/25 | Loss: 0.00133609
Iteration 3/25 | Loss: 0.00092422
Iteration 4/25 | Loss: 0.00085330
Iteration 5/25 | Loss: 0.00081502
Iteration 6/25 | Loss: 0.00081061
Iteration 7/25 | Loss: 0.00081073
Iteration 8/25 | Loss: 0.00081029
Iteration 9/25 | Loss: 0.00080966
Iteration 10/25 | Loss: 0.00080923
Iteration 11/25 | Loss: 0.00080893
Iteration 12/25 | Loss: 0.00080878
Iteration 13/25 | Loss: 0.00080877
Iteration 14/25 | Loss: 0.00080877
Iteration 15/25 | Loss: 0.00080877
Iteration 16/25 | Loss: 0.00080877
Iteration 17/25 | Loss: 0.00080877
Iteration 18/25 | Loss: 0.00080877
Iteration 19/25 | Loss: 0.00080877
Iteration 20/25 | Loss: 0.00080877
Iteration 21/25 | Loss: 0.00080877
Iteration 22/25 | Loss: 0.00080877
Iteration 23/25 | Loss: 0.00080877
Iteration 24/25 | Loss: 0.00080877
Iteration 25/25 | Loss: 0.00080877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10310388
Iteration 2/25 | Loss: 0.00041251
Iteration 3/25 | Loss: 0.00041251
Iteration 4/25 | Loss: 0.00041251
Iteration 5/25 | Loss: 0.00041251
Iteration 6/25 | Loss: 0.00041251
Iteration 7/25 | Loss: 0.00041251
Iteration 8/25 | Loss: 0.00041251
Iteration 9/25 | Loss: 0.00041250
Iteration 10/25 | Loss: 0.00041250
Iteration 11/25 | Loss: 0.00041250
Iteration 12/25 | Loss: 0.00041250
Iteration 13/25 | Loss: 0.00041250
Iteration 14/25 | Loss: 0.00041250
Iteration 15/25 | Loss: 0.00041250
Iteration 16/25 | Loss: 0.00041250
Iteration 17/25 | Loss: 0.00041250
Iteration 18/25 | Loss: 0.00041250
Iteration 19/25 | Loss: 0.00041250
Iteration 20/25 | Loss: 0.00041250
Iteration 21/25 | Loss: 0.00041250
Iteration 22/25 | Loss: 0.00041250
Iteration 23/25 | Loss: 0.00041250
Iteration 24/25 | Loss: 0.00041250
Iteration 25/25 | Loss: 0.00041250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041250
Iteration 2/1000 | Loss: 0.00007805
Iteration 3/1000 | Loss: 0.00004612
Iteration 4/1000 | Loss: 0.00004319
Iteration 5/1000 | Loss: 0.00004137
Iteration 6/1000 | Loss: 0.00004065
Iteration 7/1000 | Loss: 0.00003956
Iteration 8/1000 | Loss: 0.00003896
Iteration 9/1000 | Loss: 0.00003860
Iteration 10/1000 | Loss: 0.00003833
Iteration 11/1000 | Loss: 0.00003814
Iteration 12/1000 | Loss: 0.00003800
Iteration 13/1000 | Loss: 0.00003797
Iteration 14/1000 | Loss: 0.00003792
Iteration 15/1000 | Loss: 0.00003791
Iteration 16/1000 | Loss: 0.00003788
Iteration 17/1000 | Loss: 0.00003788
Iteration 18/1000 | Loss: 0.00003788
Iteration 19/1000 | Loss: 0.00003787
Iteration 20/1000 | Loss: 0.00003787
Iteration 21/1000 | Loss: 0.00003787
Iteration 22/1000 | Loss: 0.00003787
Iteration 23/1000 | Loss: 0.00003787
Iteration 24/1000 | Loss: 0.00003787
Iteration 25/1000 | Loss: 0.00003787
Iteration 26/1000 | Loss: 0.00003787
Iteration 27/1000 | Loss: 0.00003787
Iteration 28/1000 | Loss: 0.00003787
Iteration 29/1000 | Loss: 0.00003787
Iteration 30/1000 | Loss: 0.00003786
Iteration 31/1000 | Loss: 0.00003786
Iteration 32/1000 | Loss: 0.00003786
Iteration 33/1000 | Loss: 0.00003786
Iteration 34/1000 | Loss: 0.00003786
Iteration 35/1000 | Loss: 0.00003786
Iteration 36/1000 | Loss: 0.00003786
Iteration 37/1000 | Loss: 0.00003786
Iteration 38/1000 | Loss: 0.00003786
Iteration 39/1000 | Loss: 0.00003786
Iteration 40/1000 | Loss: 0.00003785
Iteration 41/1000 | Loss: 0.00003785
Iteration 42/1000 | Loss: 0.00003785
Iteration 43/1000 | Loss: 0.00003785
Iteration 44/1000 | Loss: 0.00003785
Iteration 45/1000 | Loss: 0.00003785
Iteration 46/1000 | Loss: 0.00003784
Iteration 47/1000 | Loss: 0.00003784
Iteration 48/1000 | Loss: 0.00003784
Iteration 49/1000 | Loss: 0.00003784
Iteration 50/1000 | Loss: 0.00003784
Iteration 51/1000 | Loss: 0.00003784
Iteration 52/1000 | Loss: 0.00003784
Iteration 53/1000 | Loss: 0.00003784
Iteration 54/1000 | Loss: 0.00003783
Iteration 55/1000 | Loss: 0.00003783
Iteration 56/1000 | Loss: 0.00003783
Iteration 57/1000 | Loss: 0.00003783
Iteration 58/1000 | Loss: 0.00003782
Iteration 59/1000 | Loss: 0.00003782
Iteration 60/1000 | Loss: 0.00003782
Iteration 61/1000 | Loss: 0.00003782
Iteration 62/1000 | Loss: 0.00003782
Iteration 63/1000 | Loss: 0.00003782
Iteration 64/1000 | Loss: 0.00003781
Iteration 65/1000 | Loss: 0.00003781
Iteration 66/1000 | Loss: 0.00003781
Iteration 67/1000 | Loss: 0.00003780
Iteration 68/1000 | Loss: 0.00003780
Iteration 69/1000 | Loss: 0.00003780
Iteration 70/1000 | Loss: 0.00003779
Iteration 71/1000 | Loss: 0.00003779
Iteration 72/1000 | Loss: 0.00003779
Iteration 73/1000 | Loss: 0.00003778
Iteration 74/1000 | Loss: 0.00003778
Iteration 75/1000 | Loss: 0.00003777
Iteration 76/1000 | Loss: 0.00003777
Iteration 77/1000 | Loss: 0.00003777
Iteration 78/1000 | Loss: 0.00003776
Iteration 79/1000 | Loss: 0.00003775
Iteration 80/1000 | Loss: 0.00003775
Iteration 81/1000 | Loss: 0.00003775
Iteration 82/1000 | Loss: 0.00003774
Iteration 83/1000 | Loss: 0.00003774
Iteration 84/1000 | Loss: 0.00003774
Iteration 85/1000 | Loss: 0.00003774
Iteration 86/1000 | Loss: 0.00003774
Iteration 87/1000 | Loss: 0.00003773
Iteration 88/1000 | Loss: 0.00003773
Iteration 89/1000 | Loss: 0.00003772
Iteration 90/1000 | Loss: 0.00003772
Iteration 91/1000 | Loss: 0.00003771
Iteration 92/1000 | Loss: 0.00003771
Iteration 93/1000 | Loss: 0.00003771
Iteration 94/1000 | Loss: 0.00003771
Iteration 95/1000 | Loss: 0.00003770
Iteration 96/1000 | Loss: 0.00003770
Iteration 97/1000 | Loss: 0.00003770
Iteration 98/1000 | Loss: 0.00003769
Iteration 99/1000 | Loss: 0.00003769
Iteration 100/1000 | Loss: 0.00003769
Iteration 101/1000 | Loss: 0.00003769
Iteration 102/1000 | Loss: 0.00003769
Iteration 103/1000 | Loss: 0.00003769
Iteration 104/1000 | Loss: 0.00003769
Iteration 105/1000 | Loss: 0.00003769
Iteration 106/1000 | Loss: 0.00003768
Iteration 107/1000 | Loss: 0.00003768
Iteration 108/1000 | Loss: 0.00003768
Iteration 109/1000 | Loss: 0.00003768
Iteration 110/1000 | Loss: 0.00003768
Iteration 111/1000 | Loss: 0.00003768
Iteration 112/1000 | Loss: 0.00003768
Iteration 113/1000 | Loss: 0.00003768
Iteration 114/1000 | Loss: 0.00003768
Iteration 115/1000 | Loss: 0.00003767
Iteration 116/1000 | Loss: 0.00003767
Iteration 117/1000 | Loss: 0.00003767
Iteration 118/1000 | Loss: 0.00003767
Iteration 119/1000 | Loss: 0.00003767
Iteration 120/1000 | Loss: 0.00003767
Iteration 121/1000 | Loss: 0.00003767
Iteration 122/1000 | Loss: 0.00003766
Iteration 123/1000 | Loss: 0.00003766
Iteration 124/1000 | Loss: 0.00003766
Iteration 125/1000 | Loss: 0.00003766
Iteration 126/1000 | Loss: 0.00003766
Iteration 127/1000 | Loss: 0.00003766
Iteration 128/1000 | Loss: 0.00003765
Iteration 129/1000 | Loss: 0.00003765
Iteration 130/1000 | Loss: 0.00003765
Iteration 131/1000 | Loss: 0.00003765
Iteration 132/1000 | Loss: 0.00003765
Iteration 133/1000 | Loss: 0.00003764
Iteration 134/1000 | Loss: 0.00003764
Iteration 135/1000 | Loss: 0.00003764
Iteration 136/1000 | Loss: 0.00003764
Iteration 137/1000 | Loss: 0.00003764
Iteration 138/1000 | Loss: 0.00003764
Iteration 139/1000 | Loss: 0.00003764
Iteration 140/1000 | Loss: 0.00003764
Iteration 141/1000 | Loss: 0.00003764
Iteration 142/1000 | Loss: 0.00003764
Iteration 143/1000 | Loss: 0.00003764
Iteration 144/1000 | Loss: 0.00003764
Iteration 145/1000 | Loss: 0.00003764
Iteration 146/1000 | Loss: 0.00003764
Iteration 147/1000 | Loss: 0.00003764
Iteration 148/1000 | Loss: 0.00003764
Iteration 149/1000 | Loss: 0.00003764
Iteration 150/1000 | Loss: 0.00003764
Iteration 151/1000 | Loss: 0.00003764
Iteration 152/1000 | Loss: 0.00003764
Iteration 153/1000 | Loss: 0.00003764
Iteration 154/1000 | Loss: 0.00003764
Iteration 155/1000 | Loss: 0.00003764
Iteration 156/1000 | Loss: 0.00003764
Iteration 157/1000 | Loss: 0.00003764
Iteration 158/1000 | Loss: 0.00003764
Iteration 159/1000 | Loss: 0.00003764
Iteration 160/1000 | Loss: 0.00003764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [3.763828135561198e-05, 3.763828135561198e-05, 3.763828135561198e-05, 3.763828135561198e-05, 3.763828135561198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.763828135561198e-05

Optimization complete. Final v2v error: 5.0144476890563965 mm

Highest mean error: 9.595850944519043 mm for frame 100

Lowest mean error: 4.443704605102539 mm for frame 0

Saving results

Total time: 54.729724407196045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876402
Iteration 2/25 | Loss: 0.00133770
Iteration 3/25 | Loss: 0.00074551
Iteration 4/25 | Loss: 0.00067144
Iteration 5/25 | Loss: 0.00065951
Iteration 6/25 | Loss: 0.00065664
Iteration 7/25 | Loss: 0.00065273
Iteration 8/25 | Loss: 0.00065003
Iteration 9/25 | Loss: 0.00064955
Iteration 10/25 | Loss: 0.00064942
Iteration 11/25 | Loss: 0.00064942
Iteration 12/25 | Loss: 0.00064942
Iteration 13/25 | Loss: 0.00064942
Iteration 14/25 | Loss: 0.00064942
Iteration 15/25 | Loss: 0.00064942
Iteration 16/25 | Loss: 0.00064941
Iteration 17/25 | Loss: 0.00064941
Iteration 18/25 | Loss: 0.00064941
Iteration 19/25 | Loss: 0.00064941
Iteration 20/25 | Loss: 0.00064941
Iteration 21/25 | Loss: 0.00064941
Iteration 22/25 | Loss: 0.00064941
Iteration 23/25 | Loss: 0.00064941
Iteration 24/25 | Loss: 0.00064941
Iteration 25/25 | Loss: 0.00064941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23021448
Iteration 2/25 | Loss: 0.00017677
Iteration 3/25 | Loss: 0.00017677
Iteration 4/25 | Loss: 0.00017677
Iteration 5/25 | Loss: 0.00017677
Iteration 6/25 | Loss: 0.00017677
Iteration 7/25 | Loss: 0.00017677
Iteration 8/25 | Loss: 0.00017677
Iteration 9/25 | Loss: 0.00017677
Iteration 10/25 | Loss: 0.00017677
Iteration 11/25 | Loss: 0.00017677
Iteration 12/25 | Loss: 0.00017677
Iteration 13/25 | Loss: 0.00017677
Iteration 14/25 | Loss: 0.00017677
Iteration 15/25 | Loss: 0.00017677
Iteration 16/25 | Loss: 0.00017677
Iteration 17/25 | Loss: 0.00017677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0001767701905919239, 0.0001767701905919239, 0.0001767701905919239, 0.0001767701905919239, 0.0001767701905919239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001767701905919239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017677
Iteration 2/1000 | Loss: 0.00003001
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001969
Iteration 7/1000 | Loss: 0.00001950
Iteration 8/1000 | Loss: 0.00001918
Iteration 9/1000 | Loss: 0.00001894
Iteration 10/1000 | Loss: 0.00001881
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001864
Iteration 14/1000 | Loss: 0.00001855
Iteration 15/1000 | Loss: 0.00001852
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001851
Iteration 18/1000 | Loss: 0.00001851
Iteration 19/1000 | Loss: 0.00001850
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001846
Iteration 26/1000 | Loss: 0.00001846
Iteration 27/1000 | Loss: 0.00001845
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001844
Iteration 31/1000 | Loss: 0.00001844
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001841
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001840
Iteration 43/1000 | Loss: 0.00001840
Iteration 44/1000 | Loss: 0.00001840
Iteration 45/1000 | Loss: 0.00001839
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001839
Iteration 49/1000 | Loss: 0.00001839
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00001838
Iteration 52/1000 | Loss: 0.00001838
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001837
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001837
Iteration 64/1000 | Loss: 0.00001836
Iteration 65/1000 | Loss: 0.00001836
Iteration 66/1000 | Loss: 0.00001835
Iteration 67/1000 | Loss: 0.00001835
Iteration 68/1000 | Loss: 0.00001835
Iteration 69/1000 | Loss: 0.00001835
Iteration 70/1000 | Loss: 0.00001835
Iteration 71/1000 | Loss: 0.00001834
Iteration 72/1000 | Loss: 0.00001834
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001833
Iteration 75/1000 | Loss: 0.00001833
Iteration 76/1000 | Loss: 0.00001833
Iteration 77/1000 | Loss: 0.00001833
Iteration 78/1000 | Loss: 0.00001832
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00001831
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001830
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001827
Iteration 115/1000 | Loss: 0.00001827
Iteration 116/1000 | Loss: 0.00001827
Iteration 117/1000 | Loss: 0.00001827
Iteration 118/1000 | Loss: 0.00001827
Iteration 119/1000 | Loss: 0.00001827
Iteration 120/1000 | Loss: 0.00001827
Iteration 121/1000 | Loss: 0.00001827
Iteration 122/1000 | Loss: 0.00001827
Iteration 123/1000 | Loss: 0.00001827
Iteration 124/1000 | Loss: 0.00001827
Iteration 125/1000 | Loss: 0.00001827
Iteration 126/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.8272974557476118e-05, 1.8272974557476118e-05, 1.8272974557476118e-05, 1.8272974557476118e-05, 1.8272974557476118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8272974557476118e-05

Optimization complete. Final v2v error: 3.6822152137756348 mm

Highest mean error: 4.21191930770874 mm for frame 109

Lowest mean error: 3.38993763923645 mm for frame 155

Saving results

Total time: 47.72363519668579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478623
Iteration 2/25 | Loss: 0.00118131
Iteration 3/25 | Loss: 0.00073452
Iteration 4/25 | Loss: 0.00061953
Iteration 5/25 | Loss: 0.00059514
Iteration 6/25 | Loss: 0.00058774
Iteration 7/25 | Loss: 0.00058505
Iteration 8/25 | Loss: 0.00058416
Iteration 9/25 | Loss: 0.00058385
Iteration 10/25 | Loss: 0.00058385
Iteration 11/25 | Loss: 0.00058385
Iteration 12/25 | Loss: 0.00058385
Iteration 13/25 | Loss: 0.00058385
Iteration 14/25 | Loss: 0.00058385
Iteration 15/25 | Loss: 0.00058385
Iteration 16/25 | Loss: 0.00058385
Iteration 17/25 | Loss: 0.00058385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005838463548570871, 0.0005838463548570871, 0.0005838463548570871, 0.0005838463548570871, 0.0005838463548570871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005838463548570871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43865907
Iteration 2/25 | Loss: 0.00018876
Iteration 3/25 | Loss: 0.00018876
Iteration 4/25 | Loss: 0.00018875
Iteration 5/25 | Loss: 0.00018875
Iteration 6/25 | Loss: 0.00018875
Iteration 7/25 | Loss: 0.00018875
Iteration 8/25 | Loss: 0.00018875
Iteration 9/25 | Loss: 0.00018875
Iteration 10/25 | Loss: 0.00018875
Iteration 11/25 | Loss: 0.00018875
Iteration 12/25 | Loss: 0.00018875
Iteration 13/25 | Loss: 0.00018875
Iteration 14/25 | Loss: 0.00018875
Iteration 15/25 | Loss: 0.00018875
Iteration 16/25 | Loss: 0.00018875
Iteration 17/25 | Loss: 0.00018875
Iteration 18/25 | Loss: 0.00018875
Iteration 19/25 | Loss: 0.00018875
Iteration 20/25 | Loss: 0.00018875
Iteration 21/25 | Loss: 0.00018875
Iteration 22/25 | Loss: 0.00018875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00018875321256928146, 0.00018875321256928146, 0.00018875321256928146, 0.00018875321256928146, 0.00018875321256928146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018875321256928146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018875
Iteration 2/1000 | Loss: 0.00004744
Iteration 3/1000 | Loss: 0.00002586
Iteration 4/1000 | Loss: 0.00002242
Iteration 5/1000 | Loss: 0.00002121
Iteration 6/1000 | Loss: 0.00002021
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001937
Iteration 9/1000 | Loss: 0.00001909
Iteration 10/1000 | Loss: 0.00001893
Iteration 11/1000 | Loss: 0.00001885
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001865
Iteration 14/1000 | Loss: 0.00001854
Iteration 15/1000 | Loss: 0.00001852
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001850
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001849
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001848
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001846
Iteration 30/1000 | Loss: 0.00001846
Iteration 31/1000 | Loss: 0.00001846
Iteration 32/1000 | Loss: 0.00001845
Iteration 33/1000 | Loss: 0.00001845
Iteration 34/1000 | Loss: 0.00001844
Iteration 35/1000 | Loss: 0.00001844
Iteration 36/1000 | Loss: 0.00001843
Iteration 37/1000 | Loss: 0.00001843
Iteration 38/1000 | Loss: 0.00001842
Iteration 39/1000 | Loss: 0.00001842
Iteration 40/1000 | Loss: 0.00001841
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001840
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001839
Iteration 45/1000 | Loss: 0.00001839
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001837
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001836
Iteration 52/1000 | Loss: 0.00001836
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001834
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001832
Iteration 62/1000 | Loss: 0.00001832
Iteration 63/1000 | Loss: 0.00001832
Iteration 64/1000 | Loss: 0.00001830
Iteration 65/1000 | Loss: 0.00001830
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001830
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001828
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001824
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001822
Iteration 87/1000 | Loss: 0.00001822
Iteration 88/1000 | Loss: 0.00001822
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001820
Iteration 96/1000 | Loss: 0.00001820
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001819
Iteration 110/1000 | Loss: 0.00001819
Iteration 111/1000 | Loss: 0.00001819
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001818
Iteration 115/1000 | Loss: 0.00001818
Iteration 116/1000 | Loss: 0.00001818
Iteration 117/1000 | Loss: 0.00001818
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001818
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001818
Iteration 126/1000 | Loss: 0.00001818
Iteration 127/1000 | Loss: 0.00001818
Iteration 128/1000 | Loss: 0.00001817
Iteration 129/1000 | Loss: 0.00001817
Iteration 130/1000 | Loss: 0.00001817
Iteration 131/1000 | Loss: 0.00001817
Iteration 132/1000 | Loss: 0.00001817
Iteration 133/1000 | Loss: 0.00001817
Iteration 134/1000 | Loss: 0.00001817
Iteration 135/1000 | Loss: 0.00001816
Iteration 136/1000 | Loss: 0.00001816
Iteration 137/1000 | Loss: 0.00001816
Iteration 138/1000 | Loss: 0.00001816
Iteration 139/1000 | Loss: 0.00001816
Iteration 140/1000 | Loss: 0.00001816
Iteration 141/1000 | Loss: 0.00001816
Iteration 142/1000 | Loss: 0.00001815
Iteration 143/1000 | Loss: 0.00001815
Iteration 144/1000 | Loss: 0.00001815
Iteration 145/1000 | Loss: 0.00001815
Iteration 146/1000 | Loss: 0.00001815
Iteration 147/1000 | Loss: 0.00001815
Iteration 148/1000 | Loss: 0.00001815
Iteration 149/1000 | Loss: 0.00001815
Iteration 150/1000 | Loss: 0.00001815
Iteration 151/1000 | Loss: 0.00001815
Iteration 152/1000 | Loss: 0.00001815
Iteration 153/1000 | Loss: 0.00001815
Iteration 154/1000 | Loss: 0.00001815
Iteration 155/1000 | Loss: 0.00001814
Iteration 156/1000 | Loss: 0.00001814
Iteration 157/1000 | Loss: 0.00001814
Iteration 158/1000 | Loss: 0.00001814
Iteration 159/1000 | Loss: 0.00001814
Iteration 160/1000 | Loss: 0.00001814
Iteration 161/1000 | Loss: 0.00001814
Iteration 162/1000 | Loss: 0.00001814
Iteration 163/1000 | Loss: 0.00001814
Iteration 164/1000 | Loss: 0.00001814
Iteration 165/1000 | Loss: 0.00001814
Iteration 166/1000 | Loss: 0.00001814
Iteration 167/1000 | Loss: 0.00001814
Iteration 168/1000 | Loss: 0.00001814
Iteration 169/1000 | Loss: 0.00001814
Iteration 170/1000 | Loss: 0.00001814
Iteration 171/1000 | Loss: 0.00001813
Iteration 172/1000 | Loss: 0.00001813
Iteration 173/1000 | Loss: 0.00001813
Iteration 174/1000 | Loss: 0.00001813
Iteration 175/1000 | Loss: 0.00001813
Iteration 176/1000 | Loss: 0.00001813
Iteration 177/1000 | Loss: 0.00001813
Iteration 178/1000 | Loss: 0.00001813
Iteration 179/1000 | Loss: 0.00001813
Iteration 180/1000 | Loss: 0.00001813
Iteration 181/1000 | Loss: 0.00001813
Iteration 182/1000 | Loss: 0.00001813
Iteration 183/1000 | Loss: 0.00001813
Iteration 184/1000 | Loss: 0.00001813
Iteration 185/1000 | Loss: 0.00001813
Iteration 186/1000 | Loss: 0.00001812
Iteration 187/1000 | Loss: 0.00001812
Iteration 188/1000 | Loss: 0.00001812
Iteration 189/1000 | Loss: 0.00001812
Iteration 190/1000 | Loss: 0.00001812
Iteration 191/1000 | Loss: 0.00001812
Iteration 192/1000 | Loss: 0.00001812
Iteration 193/1000 | Loss: 0.00001812
Iteration 194/1000 | Loss: 0.00001812
Iteration 195/1000 | Loss: 0.00001812
Iteration 196/1000 | Loss: 0.00001812
Iteration 197/1000 | Loss: 0.00001812
Iteration 198/1000 | Loss: 0.00001812
Iteration 199/1000 | Loss: 0.00001812
Iteration 200/1000 | Loss: 0.00001811
Iteration 201/1000 | Loss: 0.00001811
Iteration 202/1000 | Loss: 0.00001811
Iteration 203/1000 | Loss: 0.00001811
Iteration 204/1000 | Loss: 0.00001811
Iteration 205/1000 | Loss: 0.00001811
Iteration 206/1000 | Loss: 0.00001811
Iteration 207/1000 | Loss: 0.00001811
Iteration 208/1000 | Loss: 0.00001811
Iteration 209/1000 | Loss: 0.00001811
Iteration 210/1000 | Loss: 0.00001811
Iteration 211/1000 | Loss: 0.00001811
Iteration 212/1000 | Loss: 0.00001811
Iteration 213/1000 | Loss: 0.00001811
Iteration 214/1000 | Loss: 0.00001811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.8111226381734014e-05, 1.8111226381734014e-05, 1.8111226381734014e-05, 1.8111226381734014e-05, 1.8111226381734014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8111226381734014e-05

Optimization complete. Final v2v error: 3.382610559463501 mm

Highest mean error: 5.470933437347412 mm for frame 82

Lowest mean error: 2.581393241882324 mm for frame 158

Saving results

Total time: 46.35355806350708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835637
Iteration 2/25 | Loss: 0.00073280
Iteration 3/25 | Loss: 0.00057976
Iteration 4/25 | Loss: 0.00054882
Iteration 5/25 | Loss: 0.00054253
Iteration 6/25 | Loss: 0.00054099
Iteration 7/25 | Loss: 0.00054078
Iteration 8/25 | Loss: 0.00054078
Iteration 9/25 | Loss: 0.00054078
Iteration 10/25 | Loss: 0.00054078
Iteration 11/25 | Loss: 0.00054078
Iteration 12/25 | Loss: 0.00054078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005407820572145283, 0.0005407820572145283, 0.0005407820572145283, 0.0005407820572145283, 0.0005407820572145283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005407820572145283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43214464
Iteration 2/25 | Loss: 0.00012494
Iteration 3/25 | Loss: 0.00012494
Iteration 4/25 | Loss: 0.00012494
Iteration 5/25 | Loss: 0.00012494
Iteration 6/25 | Loss: 0.00012494
Iteration 7/25 | Loss: 0.00012494
Iteration 8/25 | Loss: 0.00012494
Iteration 9/25 | Loss: 0.00012494
Iteration 10/25 | Loss: 0.00012494
Iteration 11/25 | Loss: 0.00012494
Iteration 12/25 | Loss: 0.00012494
Iteration 13/25 | Loss: 0.00012494
Iteration 14/25 | Loss: 0.00012494
Iteration 15/25 | Loss: 0.00012494
Iteration 16/25 | Loss: 0.00012494
Iteration 17/25 | Loss: 0.00012494
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0001249407941941172, 0.0001249407941941172, 0.0001249407941941172, 0.0001249407941941172, 0.0001249407941941172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001249407941941172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012494
Iteration 2/1000 | Loss: 0.00003323
Iteration 3/1000 | Loss: 0.00001757
Iteration 4/1000 | Loss: 0.00001491
Iteration 5/1000 | Loss: 0.00001393
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001248
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001237
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001218
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001215
Iteration 26/1000 | Loss: 0.00001215
Iteration 27/1000 | Loss: 0.00001214
Iteration 28/1000 | Loss: 0.00001214
Iteration 29/1000 | Loss: 0.00001213
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001209
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001208
Iteration 38/1000 | Loss: 0.00001207
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001206
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001205
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001204
Iteration 55/1000 | Loss: 0.00001204
Iteration 56/1000 | Loss: 0.00001204
Iteration 57/1000 | Loss: 0.00001203
Iteration 58/1000 | Loss: 0.00001203
Iteration 59/1000 | Loss: 0.00001203
Iteration 60/1000 | Loss: 0.00001203
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001202
Iteration 63/1000 | Loss: 0.00001202
Iteration 64/1000 | Loss: 0.00001202
Iteration 65/1000 | Loss: 0.00001202
Iteration 66/1000 | Loss: 0.00001202
Iteration 67/1000 | Loss: 0.00001202
Iteration 68/1000 | Loss: 0.00001202
Iteration 69/1000 | Loss: 0.00001201
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001201
Iteration 74/1000 | Loss: 0.00001201
Iteration 75/1000 | Loss: 0.00001201
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001200
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001199
Iteration 88/1000 | Loss: 0.00001199
Iteration 89/1000 | Loss: 0.00001199
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001195
Iteration 125/1000 | Loss: 0.00001195
Iteration 126/1000 | Loss: 0.00001195
Iteration 127/1000 | Loss: 0.00001195
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Iteration 135/1000 | Loss: 0.00001194
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001194
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001193
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001192
Iteration 149/1000 | Loss: 0.00001192
Iteration 150/1000 | Loss: 0.00001192
Iteration 151/1000 | Loss: 0.00001192
Iteration 152/1000 | Loss: 0.00001192
Iteration 153/1000 | Loss: 0.00001192
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Iteration 170/1000 | Loss: 0.00001191
Iteration 171/1000 | Loss: 0.00001190
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001190
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001188
Iteration 193/1000 | Loss: 0.00001188
Iteration 194/1000 | Loss: 0.00001188
Iteration 195/1000 | Loss: 0.00001188
Iteration 196/1000 | Loss: 0.00001188
Iteration 197/1000 | Loss: 0.00001188
Iteration 198/1000 | Loss: 0.00001188
Iteration 199/1000 | Loss: 0.00001188
Iteration 200/1000 | Loss: 0.00001188
Iteration 201/1000 | Loss: 0.00001188
Iteration 202/1000 | Loss: 0.00001187
Iteration 203/1000 | Loss: 0.00001187
Iteration 204/1000 | Loss: 0.00001187
Iteration 205/1000 | Loss: 0.00001187
Iteration 206/1000 | Loss: 0.00001187
Iteration 207/1000 | Loss: 0.00001187
Iteration 208/1000 | Loss: 0.00001187
Iteration 209/1000 | Loss: 0.00001187
Iteration 210/1000 | Loss: 0.00001187
Iteration 211/1000 | Loss: 0.00001187
Iteration 212/1000 | Loss: 0.00001187
Iteration 213/1000 | Loss: 0.00001187
Iteration 214/1000 | Loss: 0.00001187
Iteration 215/1000 | Loss: 0.00001187
Iteration 216/1000 | Loss: 0.00001187
Iteration 217/1000 | Loss: 0.00001187
Iteration 218/1000 | Loss: 0.00001187
Iteration 219/1000 | Loss: 0.00001187
Iteration 220/1000 | Loss: 0.00001187
Iteration 221/1000 | Loss: 0.00001186
Iteration 222/1000 | Loss: 0.00001186
Iteration 223/1000 | Loss: 0.00001186
Iteration 224/1000 | Loss: 0.00001186
Iteration 225/1000 | Loss: 0.00001186
Iteration 226/1000 | Loss: 0.00001186
Iteration 227/1000 | Loss: 0.00001186
Iteration 228/1000 | Loss: 0.00001186
Iteration 229/1000 | Loss: 0.00001186
Iteration 230/1000 | Loss: 0.00001186
Iteration 231/1000 | Loss: 0.00001186
Iteration 232/1000 | Loss: 0.00001186
Iteration 233/1000 | Loss: 0.00001186
Iteration 234/1000 | Loss: 0.00001185
Iteration 235/1000 | Loss: 0.00001185
Iteration 236/1000 | Loss: 0.00001185
Iteration 237/1000 | Loss: 0.00001185
Iteration 238/1000 | Loss: 0.00001185
Iteration 239/1000 | Loss: 0.00001185
Iteration 240/1000 | Loss: 0.00001185
Iteration 241/1000 | Loss: 0.00001185
Iteration 242/1000 | Loss: 0.00001185
Iteration 243/1000 | Loss: 0.00001185
Iteration 244/1000 | Loss: 0.00001185
Iteration 245/1000 | Loss: 0.00001185
Iteration 246/1000 | Loss: 0.00001185
Iteration 247/1000 | Loss: 0.00001185
Iteration 248/1000 | Loss: 0.00001185
Iteration 249/1000 | Loss: 0.00001185
Iteration 250/1000 | Loss: 0.00001184
Iteration 251/1000 | Loss: 0.00001184
Iteration 252/1000 | Loss: 0.00001184
Iteration 253/1000 | Loss: 0.00001184
Iteration 254/1000 | Loss: 0.00001184
Iteration 255/1000 | Loss: 0.00001184
Iteration 256/1000 | Loss: 0.00001184
Iteration 257/1000 | Loss: 0.00001184
Iteration 258/1000 | Loss: 0.00001184
Iteration 259/1000 | Loss: 0.00001184
Iteration 260/1000 | Loss: 0.00001184
Iteration 261/1000 | Loss: 0.00001184
Iteration 262/1000 | Loss: 0.00001184
Iteration 263/1000 | Loss: 0.00001184
Iteration 264/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.1844330401800107e-05, 1.1844330401800107e-05, 1.1844330401800107e-05, 1.1844330401800107e-05, 1.1844330401800107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1844330401800107e-05

Optimization complete. Final v2v error: 2.965031623840332 mm

Highest mean error: 3.51204252243042 mm for frame 198

Lowest mean error: 2.658010721206665 mm for frame 121

Saving results

Total time: 47.92021989822388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879387
Iteration 2/25 | Loss: 0.00120081
Iteration 3/25 | Loss: 0.00088575
Iteration 4/25 | Loss: 0.00081651
Iteration 5/25 | Loss: 0.00080624
Iteration 6/25 | Loss: 0.00080420
Iteration 7/25 | Loss: 0.00080379
Iteration 8/25 | Loss: 0.00080379
Iteration 9/25 | Loss: 0.00080379
Iteration 10/25 | Loss: 0.00080379
Iteration 11/25 | Loss: 0.00080379
Iteration 12/25 | Loss: 0.00080379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008037924417294562, 0.0008037924417294562, 0.0008037924417294562, 0.0008037924417294562, 0.0008037924417294562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008037924417294562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99260879
Iteration 2/25 | Loss: 0.00056814
Iteration 3/25 | Loss: 0.00056814
Iteration 4/25 | Loss: 0.00056814
Iteration 5/25 | Loss: 0.00056814
Iteration 6/25 | Loss: 0.00056814
Iteration 7/25 | Loss: 0.00056814
Iteration 8/25 | Loss: 0.00056814
Iteration 9/25 | Loss: 0.00056814
Iteration 10/25 | Loss: 0.00056814
Iteration 11/25 | Loss: 0.00056814
Iteration 12/25 | Loss: 0.00056814
Iteration 13/25 | Loss: 0.00056814
Iteration 14/25 | Loss: 0.00056814
Iteration 15/25 | Loss: 0.00056814
Iteration 16/25 | Loss: 0.00056814
Iteration 17/25 | Loss: 0.00056814
Iteration 18/25 | Loss: 0.00056814
Iteration 19/25 | Loss: 0.00056814
Iteration 20/25 | Loss: 0.00056814
Iteration 21/25 | Loss: 0.00056814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005681408802047372, 0.0005681408802047372, 0.0005681408802047372, 0.0005681408802047372, 0.0005681408802047372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005681408802047372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056814
Iteration 2/1000 | Loss: 0.00011019
Iteration 3/1000 | Loss: 0.00007250
Iteration 4/1000 | Loss: 0.00005744
Iteration 5/1000 | Loss: 0.00005264
Iteration 6/1000 | Loss: 0.00004855
Iteration 7/1000 | Loss: 0.00004633
Iteration 8/1000 | Loss: 0.00004452
Iteration 9/1000 | Loss: 0.00004352
Iteration 10/1000 | Loss: 0.00004269
Iteration 11/1000 | Loss: 0.00004206
Iteration 12/1000 | Loss: 0.00004153
Iteration 13/1000 | Loss: 0.00004127
Iteration 14/1000 | Loss: 0.00004105
Iteration 15/1000 | Loss: 0.00004098
Iteration 16/1000 | Loss: 0.00004084
Iteration 17/1000 | Loss: 0.00004075
Iteration 18/1000 | Loss: 0.00004075
Iteration 19/1000 | Loss: 0.00004074
Iteration 20/1000 | Loss: 0.00004074
Iteration 21/1000 | Loss: 0.00004073
Iteration 22/1000 | Loss: 0.00004068
Iteration 23/1000 | Loss: 0.00004065
Iteration 24/1000 | Loss: 0.00004065
Iteration 25/1000 | Loss: 0.00004064
Iteration 26/1000 | Loss: 0.00004058
Iteration 27/1000 | Loss: 0.00004056
Iteration 28/1000 | Loss: 0.00004055
Iteration 29/1000 | Loss: 0.00004055
Iteration 30/1000 | Loss: 0.00004055
Iteration 31/1000 | Loss: 0.00004055
Iteration 32/1000 | Loss: 0.00004053
Iteration 33/1000 | Loss: 0.00004053
Iteration 34/1000 | Loss: 0.00004052
Iteration 35/1000 | Loss: 0.00004052
Iteration 36/1000 | Loss: 0.00004052
Iteration 37/1000 | Loss: 0.00004052
Iteration 38/1000 | Loss: 0.00004052
Iteration 39/1000 | Loss: 0.00004052
Iteration 40/1000 | Loss: 0.00004051
Iteration 41/1000 | Loss: 0.00004050
Iteration 42/1000 | Loss: 0.00004050
Iteration 43/1000 | Loss: 0.00004050
Iteration 44/1000 | Loss: 0.00004049
Iteration 45/1000 | Loss: 0.00004049
Iteration 46/1000 | Loss: 0.00004049
Iteration 47/1000 | Loss: 0.00004049
Iteration 48/1000 | Loss: 0.00004049
Iteration 49/1000 | Loss: 0.00004049
Iteration 50/1000 | Loss: 0.00004049
Iteration 51/1000 | Loss: 0.00004049
Iteration 52/1000 | Loss: 0.00004048
Iteration 53/1000 | Loss: 0.00004048
Iteration 54/1000 | Loss: 0.00004048
Iteration 55/1000 | Loss: 0.00004048
Iteration 56/1000 | Loss: 0.00004048
Iteration 57/1000 | Loss: 0.00004048
Iteration 58/1000 | Loss: 0.00004048
Iteration 59/1000 | Loss: 0.00004047
Iteration 60/1000 | Loss: 0.00004047
Iteration 61/1000 | Loss: 0.00004047
Iteration 62/1000 | Loss: 0.00004047
Iteration 63/1000 | Loss: 0.00004047
Iteration 64/1000 | Loss: 0.00004047
Iteration 65/1000 | Loss: 0.00004047
Iteration 66/1000 | Loss: 0.00004047
Iteration 67/1000 | Loss: 0.00004046
Iteration 68/1000 | Loss: 0.00004046
Iteration 69/1000 | Loss: 0.00004046
Iteration 70/1000 | Loss: 0.00004046
Iteration 71/1000 | Loss: 0.00004046
Iteration 72/1000 | Loss: 0.00004046
Iteration 73/1000 | Loss: 0.00004046
Iteration 74/1000 | Loss: 0.00004046
Iteration 75/1000 | Loss: 0.00004046
Iteration 76/1000 | Loss: 0.00004046
Iteration 77/1000 | Loss: 0.00004045
Iteration 78/1000 | Loss: 0.00004045
Iteration 79/1000 | Loss: 0.00004045
Iteration 80/1000 | Loss: 0.00004045
Iteration 81/1000 | Loss: 0.00004045
Iteration 82/1000 | Loss: 0.00004045
Iteration 83/1000 | Loss: 0.00004045
Iteration 84/1000 | Loss: 0.00004045
Iteration 85/1000 | Loss: 0.00004045
Iteration 86/1000 | Loss: 0.00004045
Iteration 87/1000 | Loss: 0.00004044
Iteration 88/1000 | Loss: 0.00004044
Iteration 89/1000 | Loss: 0.00004044
Iteration 90/1000 | Loss: 0.00004044
Iteration 91/1000 | Loss: 0.00004044
Iteration 92/1000 | Loss: 0.00004044
Iteration 93/1000 | Loss: 0.00004044
Iteration 94/1000 | Loss: 0.00004044
Iteration 95/1000 | Loss: 0.00004044
Iteration 96/1000 | Loss: 0.00004044
Iteration 97/1000 | Loss: 0.00004043
Iteration 98/1000 | Loss: 0.00004043
Iteration 99/1000 | Loss: 0.00004043
Iteration 100/1000 | Loss: 0.00004043
Iteration 101/1000 | Loss: 0.00004043
Iteration 102/1000 | Loss: 0.00004043
Iteration 103/1000 | Loss: 0.00004043
Iteration 104/1000 | Loss: 0.00004043
Iteration 105/1000 | Loss: 0.00004043
Iteration 106/1000 | Loss: 0.00004043
Iteration 107/1000 | Loss: 0.00004043
Iteration 108/1000 | Loss: 0.00004042
Iteration 109/1000 | Loss: 0.00004042
Iteration 110/1000 | Loss: 0.00004042
Iteration 111/1000 | Loss: 0.00004042
Iteration 112/1000 | Loss: 0.00004042
Iteration 113/1000 | Loss: 0.00004042
Iteration 114/1000 | Loss: 0.00004042
Iteration 115/1000 | Loss: 0.00004042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [4.042322689201683e-05, 4.042322689201683e-05, 4.042322689201683e-05, 4.042322689201683e-05, 4.042322689201683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.042322689201683e-05

Optimization complete. Final v2v error: 5.244960784912109 mm

Highest mean error: 5.787845134735107 mm for frame 126

Lowest mean error: 4.8834381103515625 mm for frame 26

Saving results

Total time: 39.975571155548096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790389
Iteration 2/25 | Loss: 0.00118854
Iteration 3/25 | Loss: 0.00076290
Iteration 4/25 | Loss: 0.00069377
Iteration 5/25 | Loss: 0.00066421
Iteration 6/25 | Loss: 0.00065740
Iteration 7/25 | Loss: 0.00066239
Iteration 8/25 | Loss: 0.00065683
Iteration 9/25 | Loss: 0.00065029
Iteration 10/25 | Loss: 0.00064723
Iteration 11/25 | Loss: 0.00064019
Iteration 12/25 | Loss: 0.00063607
Iteration 13/25 | Loss: 0.00063445
Iteration 14/25 | Loss: 0.00063483
Iteration 15/25 | Loss: 0.00063347
Iteration 16/25 | Loss: 0.00063295
Iteration 17/25 | Loss: 0.00063244
Iteration 18/25 | Loss: 0.00063226
Iteration 19/25 | Loss: 0.00063225
Iteration 20/25 | Loss: 0.00063224
Iteration 21/25 | Loss: 0.00063224
Iteration 22/25 | Loss: 0.00063224
Iteration 23/25 | Loss: 0.00063224
Iteration 24/25 | Loss: 0.00063224
Iteration 25/25 | Loss: 0.00063224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04574656
Iteration 2/25 | Loss: 0.00012445
Iteration 3/25 | Loss: 0.00012445
Iteration 4/25 | Loss: 0.00012445
Iteration 5/25 | Loss: 0.00012445
Iteration 6/25 | Loss: 0.00012445
Iteration 7/25 | Loss: 0.00012445
Iteration 8/25 | Loss: 0.00012445
Iteration 9/25 | Loss: 0.00012445
Iteration 10/25 | Loss: 0.00012445
Iteration 11/25 | Loss: 0.00012445
Iteration 12/25 | Loss: 0.00012445
Iteration 13/25 | Loss: 0.00012445
Iteration 14/25 | Loss: 0.00012445
Iteration 15/25 | Loss: 0.00012445
Iteration 16/25 | Loss: 0.00012445
Iteration 17/25 | Loss: 0.00012445
Iteration 18/25 | Loss: 0.00012445
Iteration 19/25 | Loss: 0.00012445
Iteration 20/25 | Loss: 0.00012445
Iteration 21/25 | Loss: 0.00012445
Iteration 22/25 | Loss: 0.00012445
Iteration 23/25 | Loss: 0.00012445
Iteration 24/25 | Loss: 0.00012445
Iteration 25/25 | Loss: 0.00012445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012445
Iteration 2/1000 | Loss: 0.00002791
Iteration 3/1000 | Loss: 0.00002153
Iteration 4/1000 | Loss: 0.00002017
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001857
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001797
Iteration 11/1000 | Loss: 0.00001796
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001782
Iteration 16/1000 | Loss: 0.00001765
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001756
Iteration 23/1000 | Loss: 0.00001752
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001750
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001747
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001740
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001739
Iteration 41/1000 | Loss: 0.00001739
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001738
Iteration 44/1000 | Loss: 0.00001737
Iteration 45/1000 | Loss: 0.00001737
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001736
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001730
Iteration 65/1000 | Loss: 0.00001730
Iteration 66/1000 | Loss: 0.00001730
Iteration 67/1000 | Loss: 0.00001729
Iteration 68/1000 | Loss: 0.00001729
Iteration 69/1000 | Loss: 0.00001729
Iteration 70/1000 | Loss: 0.00001729
Iteration 71/1000 | Loss: 0.00001729
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001728
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001727
Iteration 78/1000 | Loss: 0.00001727
Iteration 79/1000 | Loss: 0.00001726
Iteration 80/1000 | Loss: 0.00001726
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001725
Iteration 91/1000 | Loss: 0.00001725
Iteration 92/1000 | Loss: 0.00001725
Iteration 93/1000 | Loss: 0.00001725
Iteration 94/1000 | Loss: 0.00001725
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001724
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001724
Iteration 119/1000 | Loss: 0.00001724
Iteration 120/1000 | Loss: 0.00001724
Iteration 121/1000 | Loss: 0.00001724
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001724
Iteration 126/1000 | Loss: 0.00001724
Iteration 127/1000 | Loss: 0.00001724
Iteration 128/1000 | Loss: 0.00001724
Iteration 129/1000 | Loss: 0.00001724
Iteration 130/1000 | Loss: 0.00001724
Iteration 131/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.7237378415302373e-05, 1.7237378415302373e-05, 1.7237378415302373e-05, 1.7237378415302373e-05, 1.7237378415302373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7237378415302373e-05

Optimization complete. Final v2v error: 3.552971124649048 mm

Highest mean error: 3.8959603309631348 mm for frame 103

Lowest mean error: 3.2908127307891846 mm for frame 199

Saving results

Total time: 62.62115550041199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864020
Iteration 2/25 | Loss: 0.00075786
Iteration 3/25 | Loss: 0.00058555
Iteration 4/25 | Loss: 0.00055242
Iteration 5/25 | Loss: 0.00054576
Iteration 6/25 | Loss: 0.00054461
Iteration 7/25 | Loss: 0.00054461
Iteration 8/25 | Loss: 0.00054461
Iteration 9/25 | Loss: 0.00054461
Iteration 10/25 | Loss: 0.00054461
Iteration 11/25 | Loss: 0.00054461
Iteration 12/25 | Loss: 0.00054461
Iteration 13/25 | Loss: 0.00054461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005446118884719908, 0.0005446118884719908, 0.0005446118884719908, 0.0005446118884719908, 0.0005446118884719908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005446118884719908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41945684
Iteration 2/25 | Loss: 0.00011863
Iteration 3/25 | Loss: 0.00011863
Iteration 4/25 | Loss: 0.00011863
Iteration 5/25 | Loss: 0.00011863
Iteration 6/25 | Loss: 0.00011863
Iteration 7/25 | Loss: 0.00011863
Iteration 8/25 | Loss: 0.00011863
Iteration 9/25 | Loss: 0.00011863
Iteration 10/25 | Loss: 0.00011863
Iteration 11/25 | Loss: 0.00011863
Iteration 12/25 | Loss: 0.00011863
Iteration 13/25 | Loss: 0.00011863
Iteration 14/25 | Loss: 0.00011863
Iteration 15/25 | Loss: 0.00011863
Iteration 16/25 | Loss: 0.00011863
Iteration 17/25 | Loss: 0.00011863
Iteration 18/25 | Loss: 0.00011863
Iteration 19/25 | Loss: 0.00011863
Iteration 20/25 | Loss: 0.00011863
Iteration 21/25 | Loss: 0.00011863
Iteration 22/25 | Loss: 0.00011863
Iteration 23/25 | Loss: 0.00011863
Iteration 24/25 | Loss: 0.00011863
Iteration 25/25 | Loss: 0.00011863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011863
Iteration 2/1000 | Loss: 0.00003567
Iteration 3/1000 | Loss: 0.00002151
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001651
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001565
Iteration 9/1000 | Loss: 0.00001535
Iteration 10/1000 | Loss: 0.00001524
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001513
Iteration 13/1000 | Loss: 0.00001510
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001494
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001490
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001489
Iteration 27/1000 | Loss: 0.00001488
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001481
Iteration 36/1000 | Loss: 0.00001481
Iteration 37/1000 | Loss: 0.00001480
Iteration 38/1000 | Loss: 0.00001480
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001466
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001465
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001465
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001462
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.4570854546036571e-05, 1.4570854546036571e-05, 1.4570854546036571e-05, 1.4570854546036571e-05, 1.4570854546036571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4570854546036571e-05

Optimization complete. Final v2v error: 3.214751958847046 mm

Highest mean error: 3.6406798362731934 mm for frame 173

Lowest mean error: 2.818087339401245 mm for frame 36

Saving results

Total time: 38.44666004180908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784142
Iteration 2/25 | Loss: 0.00150023
Iteration 3/25 | Loss: 0.00088851
Iteration 4/25 | Loss: 0.00076992
Iteration 5/25 | Loss: 0.00075136
Iteration 6/25 | Loss: 0.00073762
Iteration 7/25 | Loss: 0.00072559
Iteration 8/25 | Loss: 0.00070992
Iteration 9/25 | Loss: 0.00069956
Iteration 10/25 | Loss: 0.00069812
Iteration 11/25 | Loss: 0.00069790
Iteration 12/25 | Loss: 0.00069789
Iteration 13/25 | Loss: 0.00069788
Iteration 14/25 | Loss: 0.00069788
Iteration 15/25 | Loss: 0.00069788
Iteration 16/25 | Loss: 0.00069788
Iteration 17/25 | Loss: 0.00069788
Iteration 18/25 | Loss: 0.00069788
Iteration 19/25 | Loss: 0.00069788
Iteration 20/25 | Loss: 0.00069788
Iteration 21/25 | Loss: 0.00069788
Iteration 22/25 | Loss: 0.00069787
Iteration 23/25 | Loss: 0.00069787
Iteration 24/25 | Loss: 0.00069787
Iteration 25/25 | Loss: 0.00069787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44440460
Iteration 2/25 | Loss: 0.00017946
Iteration 3/25 | Loss: 0.00017943
Iteration 4/25 | Loss: 0.00017943
Iteration 5/25 | Loss: 0.00017943
Iteration 6/25 | Loss: 0.00017943
Iteration 7/25 | Loss: 0.00017943
Iteration 8/25 | Loss: 0.00017943
Iteration 9/25 | Loss: 0.00017943
Iteration 10/25 | Loss: 0.00017943
Iteration 11/25 | Loss: 0.00017943
Iteration 12/25 | Loss: 0.00017943
Iteration 13/25 | Loss: 0.00017943
Iteration 14/25 | Loss: 0.00017943
Iteration 15/25 | Loss: 0.00017943
Iteration 16/25 | Loss: 0.00017943
Iteration 17/25 | Loss: 0.00017943
Iteration 18/25 | Loss: 0.00017943
Iteration 19/25 | Loss: 0.00017943
Iteration 20/25 | Loss: 0.00017943
Iteration 21/25 | Loss: 0.00017943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0001794251293176785, 0.0001794251293176785, 0.0001794251293176785, 0.0001794251293176785, 0.0001794251293176785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001794251293176785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017943
Iteration 2/1000 | Loss: 0.00003795
Iteration 3/1000 | Loss: 0.00002709
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002203
Iteration 6/1000 | Loss: 0.00002114
Iteration 7/1000 | Loss: 0.00002067
Iteration 8/1000 | Loss: 0.00002021
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001962
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001936
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001900
Iteration 18/1000 | Loss: 0.00001899
Iteration 19/1000 | Loss: 0.00001895
Iteration 20/1000 | Loss: 0.00001893
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001890
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001889
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001889
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001889
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001888
Iteration 37/1000 | Loss: 0.00001888
Iteration 38/1000 | Loss: 0.00001888
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001887
Iteration 41/1000 | Loss: 0.00001887
Iteration 42/1000 | Loss: 0.00001886
Iteration 43/1000 | Loss: 0.00001886
Iteration 44/1000 | Loss: 0.00001886
Iteration 45/1000 | Loss: 0.00001885
Iteration 46/1000 | Loss: 0.00001885
Iteration 47/1000 | Loss: 0.00001885
Iteration 48/1000 | Loss: 0.00001885
Iteration 49/1000 | Loss: 0.00001885
Iteration 50/1000 | Loss: 0.00001885
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001885
Iteration 53/1000 | Loss: 0.00001885
Iteration 54/1000 | Loss: 0.00001885
Iteration 55/1000 | Loss: 0.00001885
Iteration 56/1000 | Loss: 0.00001885
Iteration 57/1000 | Loss: 0.00001884
Iteration 58/1000 | Loss: 0.00001884
Iteration 59/1000 | Loss: 0.00001884
Iteration 60/1000 | Loss: 0.00001883
Iteration 61/1000 | Loss: 0.00001883
Iteration 62/1000 | Loss: 0.00001883
Iteration 63/1000 | Loss: 0.00001883
Iteration 64/1000 | Loss: 0.00001883
Iteration 65/1000 | Loss: 0.00001883
Iteration 66/1000 | Loss: 0.00001883
Iteration 67/1000 | Loss: 0.00001883
Iteration 68/1000 | Loss: 0.00001883
Iteration 69/1000 | Loss: 0.00001883
Iteration 70/1000 | Loss: 0.00001883
Iteration 71/1000 | Loss: 0.00001883
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00001883
Iteration 86/1000 | Loss: 0.00001883
Iteration 87/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.8830491171684116e-05, 1.8830491171684116e-05, 1.8830491171684116e-05, 1.8830491171684116e-05, 1.8830491171684116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8830491171684116e-05

Optimization complete. Final v2v error: 3.756983757019043 mm

Highest mean error: 4.451080322265625 mm for frame 0

Lowest mean error: 3.3083622455596924 mm for frame 170

Saving results

Total time: 48.99701118469238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836413
Iteration 2/25 | Loss: 0.00127001
Iteration 3/25 | Loss: 0.00073754
Iteration 4/25 | Loss: 0.00065075
Iteration 5/25 | Loss: 0.00063649
Iteration 6/25 | Loss: 0.00063438
Iteration 7/25 | Loss: 0.00063421
Iteration 8/25 | Loss: 0.00063421
Iteration 9/25 | Loss: 0.00063421
Iteration 10/25 | Loss: 0.00063421
Iteration 11/25 | Loss: 0.00063421
Iteration 12/25 | Loss: 0.00063421
Iteration 13/25 | Loss: 0.00063421
Iteration 14/25 | Loss: 0.00063421
Iteration 15/25 | Loss: 0.00063421
Iteration 16/25 | Loss: 0.00063421
Iteration 17/25 | Loss: 0.00063421
Iteration 18/25 | Loss: 0.00063421
Iteration 19/25 | Loss: 0.00063421
Iteration 20/25 | Loss: 0.00063421
Iteration 21/25 | Loss: 0.00063421
Iteration 22/25 | Loss: 0.00063421
Iteration 23/25 | Loss: 0.00063421
Iteration 24/25 | Loss: 0.00063421
Iteration 25/25 | Loss: 0.00063421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46083939
Iteration 2/25 | Loss: 0.00022263
Iteration 3/25 | Loss: 0.00022262
Iteration 4/25 | Loss: 0.00022262
Iteration 5/25 | Loss: 0.00022262
Iteration 6/25 | Loss: 0.00022262
Iteration 7/25 | Loss: 0.00022262
Iteration 8/25 | Loss: 0.00022262
Iteration 9/25 | Loss: 0.00022262
Iteration 10/25 | Loss: 0.00022262
Iteration 11/25 | Loss: 0.00022262
Iteration 12/25 | Loss: 0.00022262
Iteration 13/25 | Loss: 0.00022262
Iteration 14/25 | Loss: 0.00022262
Iteration 15/25 | Loss: 0.00022262
Iteration 16/25 | Loss: 0.00022262
Iteration 17/25 | Loss: 0.00022262
Iteration 18/25 | Loss: 0.00022262
Iteration 19/25 | Loss: 0.00022262
Iteration 20/25 | Loss: 0.00022262
Iteration 21/25 | Loss: 0.00022262
Iteration 22/25 | Loss: 0.00022262
Iteration 23/25 | Loss: 0.00022262
Iteration 24/25 | Loss: 0.00022262
Iteration 25/25 | Loss: 0.00022262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022262
Iteration 2/1000 | Loss: 0.00004706
Iteration 3/1000 | Loss: 0.00003406
Iteration 4/1000 | Loss: 0.00003106
Iteration 5/1000 | Loss: 0.00002990
Iteration 6/1000 | Loss: 0.00002893
Iteration 7/1000 | Loss: 0.00002805
Iteration 8/1000 | Loss: 0.00002736
Iteration 9/1000 | Loss: 0.00002693
Iteration 10/1000 | Loss: 0.00002670
Iteration 11/1000 | Loss: 0.00002650
Iteration 12/1000 | Loss: 0.00002640
Iteration 13/1000 | Loss: 0.00002637
Iteration 14/1000 | Loss: 0.00002636
Iteration 15/1000 | Loss: 0.00002636
Iteration 16/1000 | Loss: 0.00002635
Iteration 17/1000 | Loss: 0.00002633
Iteration 18/1000 | Loss: 0.00002633
Iteration 19/1000 | Loss: 0.00002631
Iteration 20/1000 | Loss: 0.00002631
Iteration 21/1000 | Loss: 0.00002631
Iteration 22/1000 | Loss: 0.00002631
Iteration 23/1000 | Loss: 0.00002631
Iteration 24/1000 | Loss: 0.00002630
Iteration 25/1000 | Loss: 0.00002630
Iteration 26/1000 | Loss: 0.00002630
Iteration 27/1000 | Loss: 0.00002624
Iteration 28/1000 | Loss: 0.00002623
Iteration 29/1000 | Loss: 0.00002623
Iteration 30/1000 | Loss: 0.00002621
Iteration 31/1000 | Loss: 0.00002621
Iteration 32/1000 | Loss: 0.00002621
Iteration 33/1000 | Loss: 0.00002621
Iteration 34/1000 | Loss: 0.00002621
Iteration 35/1000 | Loss: 0.00002621
Iteration 36/1000 | Loss: 0.00002621
Iteration 37/1000 | Loss: 0.00002620
Iteration 38/1000 | Loss: 0.00002620
Iteration 39/1000 | Loss: 0.00002620
Iteration 40/1000 | Loss: 0.00002618
Iteration 41/1000 | Loss: 0.00002618
Iteration 42/1000 | Loss: 0.00002618
Iteration 43/1000 | Loss: 0.00002618
Iteration 44/1000 | Loss: 0.00002617
Iteration 45/1000 | Loss: 0.00002617
Iteration 46/1000 | Loss: 0.00002617
Iteration 47/1000 | Loss: 0.00002617
Iteration 48/1000 | Loss: 0.00002617
Iteration 49/1000 | Loss: 0.00002616
Iteration 50/1000 | Loss: 0.00002615
Iteration 51/1000 | Loss: 0.00002614
Iteration 52/1000 | Loss: 0.00002614
Iteration 53/1000 | Loss: 0.00002614
Iteration 54/1000 | Loss: 0.00002614
Iteration 55/1000 | Loss: 0.00002613
Iteration 56/1000 | Loss: 0.00002613
Iteration 57/1000 | Loss: 0.00002613
Iteration 58/1000 | Loss: 0.00002613
Iteration 59/1000 | Loss: 0.00002613
Iteration 60/1000 | Loss: 0.00002612
Iteration 61/1000 | Loss: 0.00002612
Iteration 62/1000 | Loss: 0.00002612
Iteration 63/1000 | Loss: 0.00002612
Iteration 64/1000 | Loss: 0.00002612
Iteration 65/1000 | Loss: 0.00002612
Iteration 66/1000 | Loss: 0.00002611
Iteration 67/1000 | Loss: 0.00002611
Iteration 68/1000 | Loss: 0.00002611
Iteration 69/1000 | Loss: 0.00002611
Iteration 70/1000 | Loss: 0.00002611
Iteration 71/1000 | Loss: 0.00002611
Iteration 72/1000 | Loss: 0.00002611
Iteration 73/1000 | Loss: 0.00002611
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002610
Iteration 76/1000 | Loss: 0.00002610
Iteration 77/1000 | Loss: 0.00002610
Iteration 78/1000 | Loss: 0.00002610
Iteration 79/1000 | Loss: 0.00002610
Iteration 80/1000 | Loss: 0.00002609
Iteration 81/1000 | Loss: 0.00002609
Iteration 82/1000 | Loss: 0.00002609
Iteration 83/1000 | Loss: 0.00002609
Iteration 84/1000 | Loss: 0.00002609
Iteration 85/1000 | Loss: 0.00002609
Iteration 86/1000 | Loss: 0.00002609
Iteration 87/1000 | Loss: 0.00002609
Iteration 88/1000 | Loss: 0.00002609
Iteration 89/1000 | Loss: 0.00002609
Iteration 90/1000 | Loss: 0.00002608
Iteration 91/1000 | Loss: 0.00002608
Iteration 92/1000 | Loss: 0.00002608
Iteration 93/1000 | Loss: 0.00002608
Iteration 94/1000 | Loss: 0.00002608
Iteration 95/1000 | Loss: 0.00002608
Iteration 96/1000 | Loss: 0.00002608
Iteration 97/1000 | Loss: 0.00002608
Iteration 98/1000 | Loss: 0.00002608
Iteration 99/1000 | Loss: 0.00002608
Iteration 100/1000 | Loss: 0.00002608
Iteration 101/1000 | Loss: 0.00002608
Iteration 102/1000 | Loss: 0.00002608
Iteration 103/1000 | Loss: 0.00002608
Iteration 104/1000 | Loss: 0.00002608
Iteration 105/1000 | Loss: 0.00002608
Iteration 106/1000 | Loss: 0.00002608
Iteration 107/1000 | Loss: 0.00002608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.6082663680426776e-05, 2.6082663680426776e-05, 2.6082663680426776e-05, 2.6082663680426776e-05, 2.6082663680426776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6082663680426776e-05

Optimization complete. Final v2v error: 4.353005409240723 mm

Highest mean error: 4.583380699157715 mm for frame 28

Lowest mean error: 4.04778528213501 mm for frame 3

Saving results

Total time: 38.808181285858154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959974
Iteration 2/25 | Loss: 0.00119526
Iteration 3/25 | Loss: 0.00081924
Iteration 4/25 | Loss: 0.00076761
Iteration 5/25 | Loss: 0.00075200
Iteration 6/25 | Loss: 0.00075011
Iteration 7/25 | Loss: 0.00074993
Iteration 8/25 | Loss: 0.00074993
Iteration 9/25 | Loss: 0.00074993
Iteration 10/25 | Loss: 0.00074993
Iteration 11/25 | Loss: 0.00074993
Iteration 12/25 | Loss: 0.00074993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007499333587475121, 0.0007499333587475121, 0.0007499333587475121, 0.0007499333587475121, 0.0007499333587475121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007499333587475121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73163992
Iteration 2/25 | Loss: 0.00042451
Iteration 3/25 | Loss: 0.00042451
Iteration 4/25 | Loss: 0.00042451
Iteration 5/25 | Loss: 0.00042451
Iteration 6/25 | Loss: 0.00042451
Iteration 7/25 | Loss: 0.00042451
Iteration 8/25 | Loss: 0.00042451
Iteration 9/25 | Loss: 0.00042451
Iteration 10/25 | Loss: 0.00042451
Iteration 11/25 | Loss: 0.00042451
Iteration 12/25 | Loss: 0.00042451
Iteration 13/25 | Loss: 0.00042451
Iteration 14/25 | Loss: 0.00042451
Iteration 15/25 | Loss: 0.00042451
Iteration 16/25 | Loss: 0.00042451
Iteration 17/25 | Loss: 0.00042451
Iteration 18/25 | Loss: 0.00042451
Iteration 19/25 | Loss: 0.00042451
Iteration 20/25 | Loss: 0.00042451
Iteration 21/25 | Loss: 0.00042451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00042450561886653304, 0.00042450561886653304, 0.00042450561886653304, 0.00042450561886653304, 0.00042450561886653304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042450561886653304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042451
Iteration 2/1000 | Loss: 0.00005821
Iteration 3/1000 | Loss: 0.00004062
Iteration 4/1000 | Loss: 0.00003438
Iteration 5/1000 | Loss: 0.00003265
Iteration 6/1000 | Loss: 0.00003168
Iteration 7/1000 | Loss: 0.00003081
Iteration 8/1000 | Loss: 0.00003023
Iteration 9/1000 | Loss: 0.00002990
Iteration 10/1000 | Loss: 0.00002962
Iteration 11/1000 | Loss: 0.00002934
Iteration 12/1000 | Loss: 0.00002899
Iteration 13/1000 | Loss: 0.00002876
Iteration 14/1000 | Loss: 0.00002866
Iteration 15/1000 | Loss: 0.00002860
Iteration 16/1000 | Loss: 0.00002842
Iteration 17/1000 | Loss: 0.00002830
Iteration 18/1000 | Loss: 0.00002827
Iteration 19/1000 | Loss: 0.00002822
Iteration 20/1000 | Loss: 0.00002822
Iteration 21/1000 | Loss: 0.00002822
Iteration 22/1000 | Loss: 0.00002822
Iteration 23/1000 | Loss: 0.00002822
Iteration 24/1000 | Loss: 0.00002822
Iteration 25/1000 | Loss: 0.00002822
Iteration 26/1000 | Loss: 0.00002822
Iteration 27/1000 | Loss: 0.00002821
Iteration 28/1000 | Loss: 0.00002821
Iteration 29/1000 | Loss: 0.00002821
Iteration 30/1000 | Loss: 0.00002821
Iteration 31/1000 | Loss: 0.00002821
Iteration 32/1000 | Loss: 0.00002819
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002819
Iteration 35/1000 | Loss: 0.00002818
Iteration 36/1000 | Loss: 0.00002818
Iteration 37/1000 | Loss: 0.00002818
Iteration 38/1000 | Loss: 0.00002818
Iteration 39/1000 | Loss: 0.00002818
Iteration 40/1000 | Loss: 0.00002818
Iteration 41/1000 | Loss: 0.00002818
Iteration 42/1000 | Loss: 0.00002818
Iteration 43/1000 | Loss: 0.00002818
Iteration 44/1000 | Loss: 0.00002818
Iteration 45/1000 | Loss: 0.00002818
Iteration 46/1000 | Loss: 0.00002813
Iteration 47/1000 | Loss: 0.00002812
Iteration 48/1000 | Loss: 0.00002809
Iteration 49/1000 | Loss: 0.00002809
Iteration 50/1000 | Loss: 0.00002806
Iteration 51/1000 | Loss: 0.00002791
Iteration 52/1000 | Loss: 0.00002788
Iteration 53/1000 | Loss: 0.00002783
Iteration 54/1000 | Loss: 0.00002783
Iteration 55/1000 | Loss: 0.00002781
Iteration 56/1000 | Loss: 0.00002780
Iteration 57/1000 | Loss: 0.00002780
Iteration 58/1000 | Loss: 0.00002779
Iteration 59/1000 | Loss: 0.00002779
Iteration 60/1000 | Loss: 0.00002778
Iteration 61/1000 | Loss: 0.00002778
Iteration 62/1000 | Loss: 0.00002778
Iteration 63/1000 | Loss: 0.00002778
Iteration 64/1000 | Loss: 0.00002777
Iteration 65/1000 | Loss: 0.00002777
Iteration 66/1000 | Loss: 0.00002777
Iteration 67/1000 | Loss: 0.00002776
Iteration 68/1000 | Loss: 0.00002776
Iteration 69/1000 | Loss: 0.00002776
Iteration 70/1000 | Loss: 0.00002775
Iteration 71/1000 | Loss: 0.00002775
Iteration 72/1000 | Loss: 0.00002774
Iteration 73/1000 | Loss: 0.00002774
Iteration 74/1000 | Loss: 0.00002774
Iteration 75/1000 | Loss: 0.00002774
Iteration 76/1000 | Loss: 0.00002774
Iteration 77/1000 | Loss: 0.00002774
Iteration 78/1000 | Loss: 0.00002774
Iteration 79/1000 | Loss: 0.00002774
Iteration 80/1000 | Loss: 0.00002773
Iteration 81/1000 | Loss: 0.00002773
Iteration 82/1000 | Loss: 0.00002773
Iteration 83/1000 | Loss: 0.00002773
Iteration 84/1000 | Loss: 0.00002773
Iteration 85/1000 | Loss: 0.00002773
Iteration 86/1000 | Loss: 0.00002773
Iteration 87/1000 | Loss: 0.00002773
Iteration 88/1000 | Loss: 0.00002772
Iteration 89/1000 | Loss: 0.00002772
Iteration 90/1000 | Loss: 0.00002772
Iteration 91/1000 | Loss: 0.00002772
Iteration 92/1000 | Loss: 0.00002772
Iteration 93/1000 | Loss: 0.00002771
Iteration 94/1000 | Loss: 0.00002771
Iteration 95/1000 | Loss: 0.00002771
Iteration 96/1000 | Loss: 0.00002770
Iteration 97/1000 | Loss: 0.00002770
Iteration 98/1000 | Loss: 0.00002770
Iteration 99/1000 | Loss: 0.00002770
Iteration 100/1000 | Loss: 0.00002770
Iteration 101/1000 | Loss: 0.00002770
Iteration 102/1000 | Loss: 0.00002770
Iteration 103/1000 | Loss: 0.00002770
Iteration 104/1000 | Loss: 0.00002770
Iteration 105/1000 | Loss: 0.00002769
Iteration 106/1000 | Loss: 0.00002769
Iteration 107/1000 | Loss: 0.00002769
Iteration 108/1000 | Loss: 0.00002768
Iteration 109/1000 | Loss: 0.00002768
Iteration 110/1000 | Loss: 0.00002768
Iteration 111/1000 | Loss: 0.00002768
Iteration 112/1000 | Loss: 0.00002768
Iteration 113/1000 | Loss: 0.00002768
Iteration 114/1000 | Loss: 0.00002768
Iteration 115/1000 | Loss: 0.00002768
Iteration 116/1000 | Loss: 0.00002768
Iteration 117/1000 | Loss: 0.00002767
Iteration 118/1000 | Loss: 0.00002767
Iteration 119/1000 | Loss: 0.00002767
Iteration 120/1000 | Loss: 0.00002767
Iteration 121/1000 | Loss: 0.00002767
Iteration 122/1000 | Loss: 0.00002767
Iteration 123/1000 | Loss: 0.00002767
Iteration 124/1000 | Loss: 0.00002767
Iteration 125/1000 | Loss: 0.00002767
Iteration 126/1000 | Loss: 0.00002767
Iteration 127/1000 | Loss: 0.00002767
Iteration 128/1000 | Loss: 0.00002767
Iteration 129/1000 | Loss: 0.00002767
Iteration 130/1000 | Loss: 0.00002767
Iteration 131/1000 | Loss: 0.00002767
Iteration 132/1000 | Loss: 0.00002767
Iteration 133/1000 | Loss: 0.00002767
Iteration 134/1000 | Loss: 0.00002767
Iteration 135/1000 | Loss: 0.00002766
Iteration 136/1000 | Loss: 0.00002766
Iteration 137/1000 | Loss: 0.00002766
Iteration 138/1000 | Loss: 0.00002766
Iteration 139/1000 | Loss: 0.00002766
Iteration 140/1000 | Loss: 0.00002766
Iteration 141/1000 | Loss: 0.00002766
Iteration 142/1000 | Loss: 0.00002766
Iteration 143/1000 | Loss: 0.00002766
Iteration 144/1000 | Loss: 0.00002766
Iteration 145/1000 | Loss: 0.00002766
Iteration 146/1000 | Loss: 0.00002766
Iteration 147/1000 | Loss: 0.00002766
Iteration 148/1000 | Loss: 0.00002766
Iteration 149/1000 | Loss: 0.00002766
Iteration 150/1000 | Loss: 0.00002766
Iteration 151/1000 | Loss: 0.00002766
Iteration 152/1000 | Loss: 0.00002766
Iteration 153/1000 | Loss: 0.00002766
Iteration 154/1000 | Loss: 0.00002765
Iteration 155/1000 | Loss: 0.00002765
Iteration 156/1000 | Loss: 0.00002765
Iteration 157/1000 | Loss: 0.00002765
Iteration 158/1000 | Loss: 0.00002765
Iteration 159/1000 | Loss: 0.00002765
Iteration 160/1000 | Loss: 0.00002765
Iteration 161/1000 | Loss: 0.00002765
Iteration 162/1000 | Loss: 0.00002765
Iteration 163/1000 | Loss: 0.00002765
Iteration 164/1000 | Loss: 0.00002765
Iteration 165/1000 | Loss: 0.00002765
Iteration 166/1000 | Loss: 0.00002765
Iteration 167/1000 | Loss: 0.00002765
Iteration 168/1000 | Loss: 0.00002765
Iteration 169/1000 | Loss: 0.00002765
Iteration 170/1000 | Loss: 0.00002765
Iteration 171/1000 | Loss: 0.00002765
Iteration 172/1000 | Loss: 0.00002765
Iteration 173/1000 | Loss: 0.00002765
Iteration 174/1000 | Loss: 0.00002765
Iteration 175/1000 | Loss: 0.00002765
Iteration 176/1000 | Loss: 0.00002765
Iteration 177/1000 | Loss: 0.00002765
Iteration 178/1000 | Loss: 0.00002765
Iteration 179/1000 | Loss: 0.00002765
Iteration 180/1000 | Loss: 0.00002765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.7652904464048333e-05, 2.7652904464048333e-05, 2.7652904464048333e-05, 2.7652904464048333e-05, 2.7652904464048333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7652904464048333e-05

Optimization complete. Final v2v error: 4.336937427520752 mm

Highest mean error: 4.557517051696777 mm for frame 58

Lowest mean error: 4.093625068664551 mm for frame 162

Saving results

Total time: 47.42272663116455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061599
Iteration 2/25 | Loss: 0.00161499
Iteration 3/25 | Loss: 0.00106998
Iteration 4/25 | Loss: 0.00112950
Iteration 5/25 | Loss: 0.00088619
Iteration 6/25 | Loss: 0.00099800
Iteration 7/25 | Loss: 0.00083373
Iteration 8/25 | Loss: 0.00080009
Iteration 9/25 | Loss: 0.00078134
Iteration 10/25 | Loss: 0.00077586
Iteration 11/25 | Loss: 0.00077140
Iteration 12/25 | Loss: 0.00076946
Iteration 13/25 | Loss: 0.00076758
Iteration 14/25 | Loss: 0.00077609
Iteration 15/25 | Loss: 0.00077007
Iteration 16/25 | Loss: 0.00076460
Iteration 17/25 | Loss: 0.00076196
Iteration 18/25 | Loss: 0.00076004
Iteration 19/25 | Loss: 0.00076086
Iteration 20/25 | Loss: 0.00076018
Iteration 21/25 | Loss: 0.00076037
Iteration 22/25 | Loss: 0.00076048
Iteration 23/25 | Loss: 0.00076046
Iteration 24/25 | Loss: 0.00075997
Iteration 25/25 | Loss: 0.00076035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88645875
Iteration 2/25 | Loss: 0.00095010
Iteration 3/25 | Loss: 0.00095006
Iteration 4/25 | Loss: 0.00095006
Iteration 5/25 | Loss: 0.00095006
Iteration 6/25 | Loss: 0.00095006
Iteration 7/25 | Loss: 0.00095006
Iteration 8/25 | Loss: 0.00095006
Iteration 9/25 | Loss: 0.00095006
Iteration 10/25 | Loss: 0.00095006
Iteration 11/25 | Loss: 0.00095006
Iteration 12/25 | Loss: 0.00095006
Iteration 13/25 | Loss: 0.00095006
Iteration 14/25 | Loss: 0.00095006
Iteration 15/25 | Loss: 0.00095006
Iteration 16/25 | Loss: 0.00095006
Iteration 17/25 | Loss: 0.00095006
Iteration 18/25 | Loss: 0.00095006
Iteration 19/25 | Loss: 0.00095006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009500609012320638, 0.0009500609012320638, 0.0009500609012320638, 0.0009500609012320638, 0.0009500609012320638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009500609012320638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095006
Iteration 2/1000 | Loss: 0.00432193
Iteration 3/1000 | Loss: 0.00043322
Iteration 4/1000 | Loss: 0.00228799
Iteration 5/1000 | Loss: 0.00176760
Iteration 6/1000 | Loss: 0.00024948
Iteration 7/1000 | Loss: 0.00059800
Iteration 8/1000 | Loss: 0.00014421
Iteration 9/1000 | Loss: 0.00048397
Iteration 10/1000 | Loss: 0.00011579
Iteration 11/1000 | Loss: 0.00009972
Iteration 12/1000 | Loss: 0.00008410
Iteration 13/1000 | Loss: 0.00008001
Iteration 14/1000 | Loss: 0.00009177
Iteration 15/1000 | Loss: 0.00005188
Iteration 16/1000 | Loss: 0.00007360
Iteration 17/1000 | Loss: 0.00006868
Iteration 18/1000 | Loss: 0.00006689
Iteration 19/1000 | Loss: 0.00030677
Iteration 20/1000 | Loss: 0.00034298
Iteration 21/1000 | Loss: 0.00016903
Iteration 22/1000 | Loss: 0.00007665
Iteration 23/1000 | Loss: 0.00007579
Iteration 24/1000 | Loss: 0.00005845
Iteration 25/1000 | Loss: 0.00006436
Iteration 26/1000 | Loss: 0.00005001
Iteration 27/1000 | Loss: 0.00005383
Iteration 28/1000 | Loss: 0.00006049
Iteration 29/1000 | Loss: 0.00005955
Iteration 30/1000 | Loss: 0.00004720
Iteration 31/1000 | Loss: 0.00005730
Iteration 32/1000 | Loss: 0.00006894
Iteration 33/1000 | Loss: 0.00006005
Iteration 34/1000 | Loss: 0.00006795
Iteration 35/1000 | Loss: 0.00006103
Iteration 36/1000 | Loss: 0.00006071
Iteration 37/1000 | Loss: 0.00004308
Iteration 38/1000 | Loss: 0.00005137
Iteration 39/1000 | Loss: 0.00005919
Iteration 40/1000 | Loss: 0.00049534
Iteration 41/1000 | Loss: 0.00042578
Iteration 42/1000 | Loss: 0.00006295
Iteration 43/1000 | Loss: 0.00003969
Iteration 44/1000 | Loss: 0.00004803
Iteration 45/1000 | Loss: 0.00005048
Iteration 46/1000 | Loss: 0.00055219
Iteration 47/1000 | Loss: 0.00007967
Iteration 48/1000 | Loss: 0.00004953
Iteration 49/1000 | Loss: 0.00006198
Iteration 50/1000 | Loss: 0.00005187
Iteration 51/1000 | Loss: 0.00005912
Iteration 52/1000 | Loss: 0.00005149
Iteration 53/1000 | Loss: 0.00004268
Iteration 54/1000 | Loss: 0.00005880
Iteration 55/1000 | Loss: 0.00004968
Iteration 56/1000 | Loss: 0.00005719
Iteration 57/1000 | Loss: 0.00005058
Iteration 58/1000 | Loss: 0.00005434
Iteration 59/1000 | Loss: 0.00004224
Iteration 60/1000 | Loss: 0.00007593
Iteration 61/1000 | Loss: 0.00005947
Iteration 62/1000 | Loss: 0.00005107
Iteration 63/1000 | Loss: 0.00005779
Iteration 64/1000 | Loss: 0.00004834
Iteration 65/1000 | Loss: 0.00005576
Iteration 66/1000 | Loss: 0.00004603
Iteration 67/1000 | Loss: 0.00005330
Iteration 68/1000 | Loss: 0.00004909
Iteration 69/1000 | Loss: 0.00005854
Iteration 70/1000 | Loss: 0.00005781
Iteration 71/1000 | Loss: 0.00005508
Iteration 72/1000 | Loss: 0.00005538
Iteration 73/1000 | Loss: 0.00005533
Iteration 74/1000 | Loss: 0.00004539
Iteration 75/1000 | Loss: 0.00005800
Iteration 76/1000 | Loss: 0.00005523
Iteration 77/1000 | Loss: 0.00005600
Iteration 78/1000 | Loss: 0.00004936
Iteration 79/1000 | Loss: 0.00005690
Iteration 80/1000 | Loss: 0.00005436
Iteration 81/1000 | Loss: 0.00006037
Iteration 82/1000 | Loss: 0.00005923
Iteration 83/1000 | Loss: 0.00005669
Iteration 84/1000 | Loss: 0.00006042
Iteration 85/1000 | Loss: 0.00005822
Iteration 86/1000 | Loss: 0.00005521
Iteration 87/1000 | Loss: 0.00005446
Iteration 88/1000 | Loss: 0.00005328
Iteration 89/1000 | Loss: 0.00005592
Iteration 90/1000 | Loss: 0.00005713
Iteration 91/1000 | Loss: 0.00006182
Iteration 92/1000 | Loss: 0.00005933
Iteration 93/1000 | Loss: 0.00005196
Iteration 94/1000 | Loss: 0.00004810
Iteration 95/1000 | Loss: 0.00004819
Iteration 96/1000 | Loss: 0.00004866
Iteration 97/1000 | Loss: 0.00005191
Iteration 98/1000 | Loss: 0.00005337
Iteration 99/1000 | Loss: 0.00004759
Iteration 100/1000 | Loss: 0.00004794
Iteration 101/1000 | Loss: 0.00005609
Iteration 102/1000 | Loss: 0.00005517
Iteration 103/1000 | Loss: 0.00004921
Iteration 104/1000 | Loss: 0.00006396
Iteration 105/1000 | Loss: 0.00004899
Iteration 106/1000 | Loss: 0.00005193
Iteration 107/1000 | Loss: 0.00005163
Iteration 108/1000 | Loss: 0.00004458
Iteration 109/1000 | Loss: 0.00005101
Iteration 110/1000 | Loss: 0.00004601
Iteration 111/1000 | Loss: 0.00005152
Iteration 112/1000 | Loss: 0.00006699
Iteration 113/1000 | Loss: 0.00004992
Iteration 114/1000 | Loss: 0.00005474
Iteration 115/1000 | Loss: 0.00005337
Iteration 116/1000 | Loss: 0.00005046
Iteration 117/1000 | Loss: 0.00005154
Iteration 118/1000 | Loss: 0.00005024
Iteration 119/1000 | Loss: 0.00005071
Iteration 120/1000 | Loss: 0.00004724
Iteration 121/1000 | Loss: 0.00005611
Iteration 122/1000 | Loss: 0.00005011
Iteration 123/1000 | Loss: 0.00003911
Iteration 124/1000 | Loss: 0.00006872
Iteration 125/1000 | Loss: 0.00004935
Iteration 126/1000 | Loss: 0.00005069
Iteration 127/1000 | Loss: 0.00006126
Iteration 128/1000 | Loss: 0.00005027
Iteration 129/1000 | Loss: 0.00005688
Iteration 130/1000 | Loss: 0.00005038
Iteration 131/1000 | Loss: 0.00005983
Iteration 132/1000 | Loss: 0.00005095
Iteration 133/1000 | Loss: 0.00005575
Iteration 134/1000 | Loss: 0.00005788
Iteration 135/1000 | Loss: 0.00005411
Iteration 136/1000 | Loss: 0.00005567
Iteration 137/1000 | Loss: 0.00005570
Iteration 138/1000 | Loss: 0.00005234
Iteration 139/1000 | Loss: 0.00005765
Iteration 140/1000 | Loss: 0.00004882
Iteration 141/1000 | Loss: 0.00006310
Iteration 142/1000 | Loss: 0.00004907
Iteration 143/1000 | Loss: 0.00005632
Iteration 144/1000 | Loss: 0.00005260
Iteration 145/1000 | Loss: 0.00005948
Iteration 146/1000 | Loss: 0.00006237
Iteration 147/1000 | Loss: 0.00006529
Iteration 148/1000 | Loss: 0.00005674
Iteration 149/1000 | Loss: 0.00006851
Iteration 150/1000 | Loss: 0.00005358
Iteration 151/1000 | Loss: 0.00005748
Iteration 152/1000 | Loss: 0.00005248
Iteration 153/1000 | Loss: 0.00005725
Iteration 154/1000 | Loss: 0.00006094
Iteration 155/1000 | Loss: 0.00005948
Iteration 156/1000 | Loss: 0.00005371
Iteration 157/1000 | Loss: 0.00005786
Iteration 158/1000 | Loss: 0.00005170
Iteration 159/1000 | Loss: 0.00005740
Iteration 160/1000 | Loss: 0.00005175
Iteration 161/1000 | Loss: 0.00005858
Iteration 162/1000 | Loss: 0.00005387
Iteration 163/1000 | Loss: 0.00005314
Iteration 164/1000 | Loss: 0.00005492
Iteration 165/1000 | Loss: 0.00005519
Iteration 166/1000 | Loss: 0.00005201
Iteration 167/1000 | Loss: 0.00005694
Iteration 168/1000 | Loss: 0.00005524
Iteration 169/1000 | Loss: 0.00005942
Iteration 170/1000 | Loss: 0.00005474
Iteration 171/1000 | Loss: 0.00005824
Iteration 172/1000 | Loss: 0.00005299
Iteration 173/1000 | Loss: 0.00005759
Iteration 174/1000 | Loss: 0.00005276
Iteration 175/1000 | Loss: 0.00005318
Iteration 176/1000 | Loss: 0.00006497
Iteration 177/1000 | Loss: 0.00003980
Iteration 178/1000 | Loss: 0.00003520
Iteration 179/1000 | Loss: 0.00003304
Iteration 180/1000 | Loss: 0.00003098
Iteration 181/1000 | Loss: 0.00003028
Iteration 182/1000 | Loss: 0.00002975
Iteration 183/1000 | Loss: 0.00002943
Iteration 184/1000 | Loss: 0.00002936
Iteration 185/1000 | Loss: 0.00002934
Iteration 186/1000 | Loss: 0.00002929
Iteration 187/1000 | Loss: 0.00002927
Iteration 188/1000 | Loss: 0.00002927
Iteration 189/1000 | Loss: 0.00002926
Iteration 190/1000 | Loss: 0.00002925
Iteration 191/1000 | Loss: 0.00002924
Iteration 192/1000 | Loss: 0.00002923
Iteration 193/1000 | Loss: 0.00002917
Iteration 194/1000 | Loss: 0.00002916
Iteration 195/1000 | Loss: 0.00002916
Iteration 196/1000 | Loss: 0.00002915
Iteration 197/1000 | Loss: 0.00002914
Iteration 198/1000 | Loss: 0.00002913
Iteration 199/1000 | Loss: 0.00002912
Iteration 200/1000 | Loss: 0.00002912
Iteration 201/1000 | Loss: 0.00002911
Iteration 202/1000 | Loss: 0.00002911
Iteration 203/1000 | Loss: 0.00002910
Iteration 204/1000 | Loss: 0.00002910
Iteration 205/1000 | Loss: 0.00002909
Iteration 206/1000 | Loss: 0.00002909
Iteration 207/1000 | Loss: 0.00002909
Iteration 208/1000 | Loss: 0.00002908
Iteration 209/1000 | Loss: 0.00002908
Iteration 210/1000 | Loss: 0.00002907
Iteration 211/1000 | Loss: 0.00002907
Iteration 212/1000 | Loss: 0.00002907
Iteration 213/1000 | Loss: 0.00002907
Iteration 214/1000 | Loss: 0.00002907
Iteration 215/1000 | Loss: 0.00002907
Iteration 216/1000 | Loss: 0.00002907
Iteration 217/1000 | Loss: 0.00002907
Iteration 218/1000 | Loss: 0.00002907
Iteration 219/1000 | Loss: 0.00002907
Iteration 220/1000 | Loss: 0.00002906
Iteration 221/1000 | Loss: 0.00002906
Iteration 222/1000 | Loss: 0.00002906
Iteration 223/1000 | Loss: 0.00002905
Iteration 224/1000 | Loss: 0.00002905
Iteration 225/1000 | Loss: 0.00002905
Iteration 226/1000 | Loss: 0.00002904
Iteration 227/1000 | Loss: 0.00002903
Iteration 228/1000 | Loss: 0.00002903
Iteration 229/1000 | Loss: 0.00002903
Iteration 230/1000 | Loss: 0.00002903
Iteration 231/1000 | Loss: 0.00002903
Iteration 232/1000 | Loss: 0.00002902
Iteration 233/1000 | Loss: 0.00002902
Iteration 234/1000 | Loss: 0.00002902
Iteration 235/1000 | Loss: 0.00002901
Iteration 236/1000 | Loss: 0.00002901
Iteration 237/1000 | Loss: 0.00002901
Iteration 238/1000 | Loss: 0.00002900
Iteration 239/1000 | Loss: 0.00002900
Iteration 240/1000 | Loss: 0.00002900
Iteration 241/1000 | Loss: 0.00002899
Iteration 242/1000 | Loss: 0.00002898
Iteration 243/1000 | Loss: 0.00002898
Iteration 244/1000 | Loss: 0.00002897
Iteration 245/1000 | Loss: 0.00002897
Iteration 246/1000 | Loss: 0.00002897
Iteration 247/1000 | Loss: 0.00002896
Iteration 248/1000 | Loss: 0.00002896
Iteration 249/1000 | Loss: 0.00002896
Iteration 250/1000 | Loss: 0.00002895
Iteration 251/1000 | Loss: 0.00002895
Iteration 252/1000 | Loss: 0.00002894
Iteration 253/1000 | Loss: 0.00002894
Iteration 254/1000 | Loss: 0.00002894
Iteration 255/1000 | Loss: 0.00002893
Iteration 256/1000 | Loss: 0.00002893
Iteration 257/1000 | Loss: 0.00002893
Iteration 258/1000 | Loss: 0.00002893
Iteration 259/1000 | Loss: 0.00002892
Iteration 260/1000 | Loss: 0.00002892
Iteration 261/1000 | Loss: 0.00002891
Iteration 262/1000 | Loss: 0.00002891
Iteration 263/1000 | Loss: 0.00002891
Iteration 264/1000 | Loss: 0.00002891
Iteration 265/1000 | Loss: 0.00002890
Iteration 266/1000 | Loss: 0.00002890
Iteration 267/1000 | Loss: 0.00002890
Iteration 268/1000 | Loss: 0.00002890
Iteration 269/1000 | Loss: 0.00002890
Iteration 270/1000 | Loss: 0.00002890
Iteration 271/1000 | Loss: 0.00002889
Iteration 272/1000 | Loss: 0.00002889
Iteration 273/1000 | Loss: 0.00002889
Iteration 274/1000 | Loss: 0.00002889
Iteration 275/1000 | Loss: 0.00002889
Iteration 276/1000 | Loss: 0.00002889
Iteration 277/1000 | Loss: 0.00002889
Iteration 278/1000 | Loss: 0.00002889
Iteration 279/1000 | Loss: 0.00002889
Iteration 280/1000 | Loss: 0.00002889
Iteration 281/1000 | Loss: 0.00002889
Iteration 282/1000 | Loss: 0.00002889
Iteration 283/1000 | Loss: 0.00002889
Iteration 284/1000 | Loss: 0.00002889
Iteration 285/1000 | Loss: 0.00002889
Iteration 286/1000 | Loss: 0.00002889
Iteration 287/1000 | Loss: 0.00002889
Iteration 288/1000 | Loss: 0.00002889
Iteration 289/1000 | Loss: 0.00002889
Iteration 290/1000 | Loss: 0.00002889
Iteration 291/1000 | Loss: 0.00002889
Iteration 292/1000 | Loss: 0.00002889
Iteration 293/1000 | Loss: 0.00002889
Iteration 294/1000 | Loss: 0.00002889
Iteration 295/1000 | Loss: 0.00002889
Iteration 296/1000 | Loss: 0.00002889
Iteration 297/1000 | Loss: 0.00002889
Iteration 298/1000 | Loss: 0.00002889
Iteration 299/1000 | Loss: 0.00002889
Iteration 300/1000 | Loss: 0.00002889
Iteration 301/1000 | Loss: 0.00002889
Iteration 302/1000 | Loss: 0.00002889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [2.8887092412333004e-05, 2.8887092412333004e-05, 2.8887092412333004e-05, 2.8887092412333004e-05, 2.8887092412333004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8887092412333004e-05

Optimization complete. Final v2v error: 3.8594186305999756 mm

Highest mean error: 14.593009948730469 mm for frame 51

Lowest mean error: 2.92492938041687 mm for frame 2

Saving results

Total time: 304.14215445518494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391212
Iteration 2/25 | Loss: 0.00083116
Iteration 3/25 | Loss: 0.00066533
Iteration 4/25 | Loss: 0.00063932
Iteration 5/25 | Loss: 0.00063303
Iteration 6/25 | Loss: 0.00063115
Iteration 7/25 | Loss: 0.00063065
Iteration 8/25 | Loss: 0.00063065
Iteration 9/25 | Loss: 0.00063065
Iteration 10/25 | Loss: 0.00063065
Iteration 11/25 | Loss: 0.00063065
Iteration 12/25 | Loss: 0.00063065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006306489813141525, 0.0006306489813141525, 0.0006306489813141525, 0.0006306489813141525, 0.0006306489813141525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006306489813141525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43467343
Iteration 2/25 | Loss: 0.00014154
Iteration 3/25 | Loss: 0.00014154
Iteration 4/25 | Loss: 0.00014154
Iteration 5/25 | Loss: 0.00014153
Iteration 6/25 | Loss: 0.00014153
Iteration 7/25 | Loss: 0.00014153
Iteration 8/25 | Loss: 0.00014153
Iteration 9/25 | Loss: 0.00014153
Iteration 10/25 | Loss: 0.00014153
Iteration 11/25 | Loss: 0.00014153
Iteration 12/25 | Loss: 0.00014153
Iteration 13/25 | Loss: 0.00014153
Iteration 14/25 | Loss: 0.00014153
Iteration 15/25 | Loss: 0.00014153
Iteration 16/25 | Loss: 0.00014153
Iteration 17/25 | Loss: 0.00014153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0001415334700141102, 0.0001415334700141102, 0.0001415334700141102, 0.0001415334700141102, 0.0001415334700141102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001415334700141102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014153
Iteration 2/1000 | Loss: 0.00004057
Iteration 3/1000 | Loss: 0.00002632
Iteration 4/1000 | Loss: 0.00002455
Iteration 5/1000 | Loss: 0.00002333
Iteration 6/1000 | Loss: 0.00002276
Iteration 7/1000 | Loss: 0.00002223
Iteration 8/1000 | Loss: 0.00002184
Iteration 9/1000 | Loss: 0.00002152
Iteration 10/1000 | Loss: 0.00002142
Iteration 11/1000 | Loss: 0.00002124
Iteration 12/1000 | Loss: 0.00002104
Iteration 13/1000 | Loss: 0.00002092
Iteration 14/1000 | Loss: 0.00002088
Iteration 15/1000 | Loss: 0.00002086
Iteration 16/1000 | Loss: 0.00002080
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00002064
Iteration 19/1000 | Loss: 0.00002061
Iteration 20/1000 | Loss: 0.00002061
Iteration 21/1000 | Loss: 0.00002056
Iteration 22/1000 | Loss: 0.00002055
Iteration 23/1000 | Loss: 0.00002055
Iteration 24/1000 | Loss: 0.00002055
Iteration 25/1000 | Loss: 0.00002054
Iteration 26/1000 | Loss: 0.00002054
Iteration 27/1000 | Loss: 0.00002053
Iteration 28/1000 | Loss: 0.00002053
Iteration 29/1000 | Loss: 0.00002053
Iteration 30/1000 | Loss: 0.00002053
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002052
Iteration 34/1000 | Loss: 0.00002052
Iteration 35/1000 | Loss: 0.00002052
Iteration 36/1000 | Loss: 0.00002052
Iteration 37/1000 | Loss: 0.00002052
Iteration 38/1000 | Loss: 0.00002051
Iteration 39/1000 | Loss: 0.00002051
Iteration 40/1000 | Loss: 0.00002051
Iteration 41/1000 | Loss: 0.00002050
Iteration 42/1000 | Loss: 0.00002050
Iteration 43/1000 | Loss: 0.00002050
Iteration 44/1000 | Loss: 0.00002050
Iteration 45/1000 | Loss: 0.00002050
Iteration 46/1000 | Loss: 0.00002049
Iteration 47/1000 | Loss: 0.00002049
Iteration 48/1000 | Loss: 0.00002049
Iteration 49/1000 | Loss: 0.00002049
Iteration 50/1000 | Loss: 0.00002049
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002049
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002048
Iteration 56/1000 | Loss: 0.00002048
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002048
Iteration 59/1000 | Loss: 0.00002047
Iteration 60/1000 | Loss: 0.00002047
Iteration 61/1000 | Loss: 0.00002047
Iteration 62/1000 | Loss: 0.00002047
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00002047
Iteration 66/1000 | Loss: 0.00002046
Iteration 67/1000 | Loss: 0.00002046
Iteration 68/1000 | Loss: 0.00002046
Iteration 69/1000 | Loss: 0.00002046
Iteration 70/1000 | Loss: 0.00002046
Iteration 71/1000 | Loss: 0.00002046
Iteration 72/1000 | Loss: 0.00002046
Iteration 73/1000 | Loss: 0.00002046
Iteration 74/1000 | Loss: 0.00002046
Iteration 75/1000 | Loss: 0.00002045
Iteration 76/1000 | Loss: 0.00002045
Iteration 77/1000 | Loss: 0.00002045
Iteration 78/1000 | Loss: 0.00002045
Iteration 79/1000 | Loss: 0.00002045
Iteration 80/1000 | Loss: 0.00002045
Iteration 81/1000 | Loss: 0.00002044
Iteration 82/1000 | Loss: 0.00002044
Iteration 83/1000 | Loss: 0.00002044
Iteration 84/1000 | Loss: 0.00002044
Iteration 85/1000 | Loss: 0.00002044
Iteration 86/1000 | Loss: 0.00002044
Iteration 87/1000 | Loss: 0.00002044
Iteration 88/1000 | Loss: 0.00002044
Iteration 89/1000 | Loss: 0.00002043
Iteration 90/1000 | Loss: 0.00002043
Iteration 91/1000 | Loss: 0.00002043
Iteration 92/1000 | Loss: 0.00002043
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002043
Iteration 99/1000 | Loss: 0.00002043
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002042
Iteration 109/1000 | Loss: 0.00002042
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002041
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002040
Iteration 124/1000 | Loss: 0.00002040
Iteration 125/1000 | Loss: 0.00002040
Iteration 126/1000 | Loss: 0.00002040
Iteration 127/1000 | Loss: 0.00002040
Iteration 128/1000 | Loss: 0.00002040
Iteration 129/1000 | Loss: 0.00002040
Iteration 130/1000 | Loss: 0.00002040
Iteration 131/1000 | Loss: 0.00002040
Iteration 132/1000 | Loss: 0.00002040
Iteration 133/1000 | Loss: 0.00002040
Iteration 134/1000 | Loss: 0.00002040
Iteration 135/1000 | Loss: 0.00002040
Iteration 136/1000 | Loss: 0.00002040
Iteration 137/1000 | Loss: 0.00002039
Iteration 138/1000 | Loss: 0.00002039
Iteration 139/1000 | Loss: 0.00002039
Iteration 140/1000 | Loss: 0.00002039
Iteration 141/1000 | Loss: 0.00002039
Iteration 142/1000 | Loss: 0.00002039
Iteration 143/1000 | Loss: 0.00002039
Iteration 144/1000 | Loss: 0.00002039
Iteration 145/1000 | Loss: 0.00002039
Iteration 146/1000 | Loss: 0.00002039
Iteration 147/1000 | Loss: 0.00002039
Iteration 148/1000 | Loss: 0.00002039
Iteration 149/1000 | Loss: 0.00002039
Iteration 150/1000 | Loss: 0.00002039
Iteration 151/1000 | Loss: 0.00002039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.0392126316437498e-05, 2.0392126316437498e-05, 2.0392126316437498e-05, 2.0392126316437498e-05, 2.0392126316437498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0392126316437498e-05

Optimization complete. Final v2v error: 3.8227579593658447 mm

Highest mean error: 4.634968280792236 mm for frame 131

Lowest mean error: 3.1630477905273438 mm for frame 4

Saving results

Total time: 38.30685567855835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822146
Iteration 2/25 | Loss: 0.00139179
Iteration 3/25 | Loss: 0.00082421
Iteration 4/25 | Loss: 0.00066175
Iteration 5/25 | Loss: 0.00064015
Iteration 6/25 | Loss: 0.00063585
Iteration 7/25 | Loss: 0.00063554
Iteration 8/25 | Loss: 0.00063554
Iteration 9/25 | Loss: 0.00063554
Iteration 10/25 | Loss: 0.00063554
Iteration 11/25 | Loss: 0.00063554
Iteration 12/25 | Loss: 0.00063554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006355377845466137, 0.0006355377845466137, 0.0006355377845466137, 0.0006355377845466137, 0.0006355377845466137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006355377845466137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36817443
Iteration 2/25 | Loss: 0.00015819
Iteration 3/25 | Loss: 0.00015818
Iteration 4/25 | Loss: 0.00015818
Iteration 5/25 | Loss: 0.00015818
Iteration 6/25 | Loss: 0.00015818
Iteration 7/25 | Loss: 0.00015818
Iteration 8/25 | Loss: 0.00015818
Iteration 9/25 | Loss: 0.00015818
Iteration 10/25 | Loss: 0.00015818
Iteration 11/25 | Loss: 0.00015818
Iteration 12/25 | Loss: 0.00015818
Iteration 13/25 | Loss: 0.00015818
Iteration 14/25 | Loss: 0.00015818
Iteration 15/25 | Loss: 0.00015818
Iteration 16/25 | Loss: 0.00015818
Iteration 17/25 | Loss: 0.00015818
Iteration 18/25 | Loss: 0.00015818
Iteration 19/25 | Loss: 0.00015818
Iteration 20/25 | Loss: 0.00015818
Iteration 21/25 | Loss: 0.00015818
Iteration 22/25 | Loss: 0.00015818
Iteration 23/25 | Loss: 0.00015818
Iteration 24/25 | Loss: 0.00015818
Iteration 25/25 | Loss: 0.00015818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015818
Iteration 2/1000 | Loss: 0.00003447
Iteration 3/1000 | Loss: 0.00002332
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00002027
Iteration 6/1000 | Loss: 0.00001948
Iteration 7/1000 | Loss: 0.00001901
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001851
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001829
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001822
Iteration 14/1000 | Loss: 0.00001811
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001808
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00001803
Iteration 19/1000 | Loss: 0.00001802
Iteration 20/1000 | Loss: 0.00001802
Iteration 21/1000 | Loss: 0.00001801
Iteration 22/1000 | Loss: 0.00001799
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001795
Iteration 27/1000 | Loss: 0.00001794
Iteration 28/1000 | Loss: 0.00001794
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001793
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001792
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001792
Iteration 36/1000 | Loss: 0.00001792
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00001791
Iteration 39/1000 | Loss: 0.00001791
Iteration 40/1000 | Loss: 0.00001790
Iteration 41/1000 | Loss: 0.00001790
Iteration 42/1000 | Loss: 0.00001790
Iteration 43/1000 | Loss: 0.00001789
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001786
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001785
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001784
Iteration 57/1000 | Loss: 0.00001784
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00001784
Iteration 60/1000 | Loss: 0.00001784
Iteration 61/1000 | Loss: 0.00001784
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001783
Iteration 64/1000 | Loss: 0.00001781
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001779
Iteration 77/1000 | Loss: 0.00001779
Iteration 78/1000 | Loss: 0.00001779
Iteration 79/1000 | Loss: 0.00001779
Iteration 80/1000 | Loss: 0.00001779
Iteration 81/1000 | Loss: 0.00001779
Iteration 82/1000 | Loss: 0.00001779
Iteration 83/1000 | Loss: 0.00001779
Iteration 84/1000 | Loss: 0.00001779
Iteration 85/1000 | Loss: 0.00001779
Iteration 86/1000 | Loss: 0.00001779
Iteration 87/1000 | Loss: 0.00001779
Iteration 88/1000 | Loss: 0.00001779
Iteration 89/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.7786593161872588e-05, 1.7786593161872588e-05, 1.7786593161872588e-05, 1.7786593161872588e-05, 1.7786593161872588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7786593161872588e-05

Optimization complete. Final v2v error: 3.610098123550415 mm

Highest mean error: 4.147395133972168 mm for frame 183

Lowest mean error: 3.3161721229553223 mm for frame 95

Saving results

Total time: 36.61298584938049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1835/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1835/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00349452
Iteration 2/25 | Loss: 0.00069492
Iteration 3/25 | Loss: 0.00056654
Iteration 4/25 | Loss: 0.00054356
Iteration 5/25 | Loss: 0.00053545
Iteration 6/25 | Loss: 0.00053368
Iteration 7/25 | Loss: 0.00053343
Iteration 8/25 | Loss: 0.00053343
Iteration 9/25 | Loss: 0.00053343
Iteration 10/25 | Loss: 0.00053343
Iteration 11/25 | Loss: 0.00053343
Iteration 12/25 | Loss: 0.00053343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005334290908649564, 0.0005334290908649564, 0.0005334290908649564, 0.0005334290908649564, 0.0005334290908649564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005334290908649564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71109211
Iteration 2/25 | Loss: 0.00010478
Iteration 3/25 | Loss: 0.00010478
Iteration 4/25 | Loss: 0.00010478
Iteration 5/25 | Loss: 0.00010477
Iteration 6/25 | Loss: 0.00010477
Iteration 7/25 | Loss: 0.00010477
Iteration 8/25 | Loss: 0.00010477
Iteration 9/25 | Loss: 0.00010477
Iteration 10/25 | Loss: 0.00010477
Iteration 11/25 | Loss: 0.00010477
Iteration 12/25 | Loss: 0.00010477
Iteration 13/25 | Loss: 0.00010477
Iteration 14/25 | Loss: 0.00010477
Iteration 15/25 | Loss: 0.00010477
Iteration 16/25 | Loss: 0.00010477
Iteration 17/25 | Loss: 0.00010477
Iteration 18/25 | Loss: 0.00010477
Iteration 19/25 | Loss: 0.00010477
Iteration 20/25 | Loss: 0.00010477
Iteration 21/25 | Loss: 0.00010477
Iteration 22/25 | Loss: 0.00010477
Iteration 23/25 | Loss: 0.00010477
Iteration 24/25 | Loss: 0.00010477
Iteration 25/25 | Loss: 0.00010477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010477
Iteration 2/1000 | Loss: 0.00002720
Iteration 3/1000 | Loss: 0.00001562
Iteration 4/1000 | Loss: 0.00001423
Iteration 5/1000 | Loss: 0.00001330
Iteration 6/1000 | Loss: 0.00001291
Iteration 7/1000 | Loss: 0.00001253
Iteration 8/1000 | Loss: 0.00001239
Iteration 9/1000 | Loss: 0.00001228
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001215
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001184
Iteration 37/1000 | Loss: 0.00001184
Iteration 38/1000 | Loss: 0.00001183
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001180
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.1800520042015705e-05, 1.1800520042015705e-05, 1.1800520042015705e-05, 1.1800520042015705e-05, 1.1800520042015705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1800520042015705e-05

Optimization complete. Final v2v error: 2.924999237060547 mm

Highest mean error: 3.4898669719696045 mm for frame 72

Lowest mean error: 2.6647448539733887 mm for frame 165

Saving results

Total time: 28.28659963607788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947527
Iteration 2/25 | Loss: 0.00174645
Iteration 3/25 | Loss: 0.00160197
Iteration 4/25 | Loss: 0.00158597
Iteration 5/25 | Loss: 0.00157889
Iteration 6/25 | Loss: 0.00157731
Iteration 7/25 | Loss: 0.00157731
Iteration 8/25 | Loss: 0.00157731
Iteration 9/25 | Loss: 0.00157731
Iteration 10/25 | Loss: 0.00157731
Iteration 11/25 | Loss: 0.00157731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001577309099957347, 0.001577309099957347, 0.001577309099957347, 0.001577309099957347, 0.001577309099957347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001577309099957347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90503895
Iteration 2/25 | Loss: 0.00180030
Iteration 3/25 | Loss: 0.00180029
Iteration 4/25 | Loss: 0.00180029
Iteration 5/25 | Loss: 0.00180029
Iteration 6/25 | Loss: 0.00180029
Iteration 7/25 | Loss: 0.00180029
Iteration 8/25 | Loss: 0.00180029
Iteration 9/25 | Loss: 0.00180029
Iteration 10/25 | Loss: 0.00180029
Iteration 11/25 | Loss: 0.00180029
Iteration 12/25 | Loss: 0.00180029
Iteration 13/25 | Loss: 0.00180029
Iteration 14/25 | Loss: 0.00180029
Iteration 15/25 | Loss: 0.00180029
Iteration 16/25 | Loss: 0.00180029
Iteration 17/25 | Loss: 0.00180029
Iteration 18/25 | Loss: 0.00180029
Iteration 19/25 | Loss: 0.00180029
Iteration 20/25 | Loss: 0.00180029
Iteration 21/25 | Loss: 0.00180029
Iteration 22/25 | Loss: 0.00180029
Iteration 23/25 | Loss: 0.00180029
Iteration 24/25 | Loss: 0.00180029
Iteration 25/25 | Loss: 0.00180029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180029
Iteration 2/1000 | Loss: 0.00005461
Iteration 3/1000 | Loss: 0.00004138
Iteration 4/1000 | Loss: 0.00003779
Iteration 5/1000 | Loss: 0.00003617
Iteration 6/1000 | Loss: 0.00003495
Iteration 7/1000 | Loss: 0.00003405
Iteration 8/1000 | Loss: 0.00003341
Iteration 9/1000 | Loss: 0.00003292
Iteration 10/1000 | Loss: 0.00003258
Iteration 11/1000 | Loss: 0.00003243
Iteration 12/1000 | Loss: 0.00003241
Iteration 13/1000 | Loss: 0.00003234
Iteration 14/1000 | Loss: 0.00003233
Iteration 15/1000 | Loss: 0.00003227
Iteration 16/1000 | Loss: 0.00003226
Iteration 17/1000 | Loss: 0.00003223
Iteration 18/1000 | Loss: 0.00003222
Iteration 19/1000 | Loss: 0.00003221
Iteration 20/1000 | Loss: 0.00003220
Iteration 21/1000 | Loss: 0.00003220
Iteration 22/1000 | Loss: 0.00003216
Iteration 23/1000 | Loss: 0.00003216
Iteration 24/1000 | Loss: 0.00003216
Iteration 25/1000 | Loss: 0.00003215
Iteration 26/1000 | Loss: 0.00003215
Iteration 27/1000 | Loss: 0.00003214
Iteration 28/1000 | Loss: 0.00003211
Iteration 29/1000 | Loss: 0.00003211
Iteration 30/1000 | Loss: 0.00003210
Iteration 31/1000 | Loss: 0.00003210
Iteration 32/1000 | Loss: 0.00003210
Iteration 33/1000 | Loss: 0.00003209
Iteration 34/1000 | Loss: 0.00003206
Iteration 35/1000 | Loss: 0.00003206
Iteration 36/1000 | Loss: 0.00003206
Iteration 37/1000 | Loss: 0.00003205
Iteration 38/1000 | Loss: 0.00003204
Iteration 39/1000 | Loss: 0.00003204
Iteration 40/1000 | Loss: 0.00003204
Iteration 41/1000 | Loss: 0.00003203
Iteration 42/1000 | Loss: 0.00003203
Iteration 43/1000 | Loss: 0.00003202
Iteration 44/1000 | Loss: 0.00003202
Iteration 45/1000 | Loss: 0.00003201
Iteration 46/1000 | Loss: 0.00003201
Iteration 47/1000 | Loss: 0.00003201
Iteration 48/1000 | Loss: 0.00003200
Iteration 49/1000 | Loss: 0.00003199
Iteration 50/1000 | Loss: 0.00003199
Iteration 51/1000 | Loss: 0.00003198
Iteration 52/1000 | Loss: 0.00003198
Iteration 53/1000 | Loss: 0.00003198
Iteration 54/1000 | Loss: 0.00003198
Iteration 55/1000 | Loss: 0.00003197
Iteration 56/1000 | Loss: 0.00003197
Iteration 57/1000 | Loss: 0.00003197
Iteration 58/1000 | Loss: 0.00003196
Iteration 59/1000 | Loss: 0.00003196
Iteration 60/1000 | Loss: 0.00003195
Iteration 61/1000 | Loss: 0.00003195
Iteration 62/1000 | Loss: 0.00003195
Iteration 63/1000 | Loss: 0.00003195
Iteration 64/1000 | Loss: 0.00003195
Iteration 65/1000 | Loss: 0.00003195
Iteration 66/1000 | Loss: 0.00003195
Iteration 67/1000 | Loss: 0.00003195
Iteration 68/1000 | Loss: 0.00003195
Iteration 69/1000 | Loss: 0.00003194
Iteration 70/1000 | Loss: 0.00003194
Iteration 71/1000 | Loss: 0.00003194
Iteration 72/1000 | Loss: 0.00003194
Iteration 73/1000 | Loss: 0.00003194
Iteration 74/1000 | Loss: 0.00003194
Iteration 75/1000 | Loss: 0.00003194
Iteration 76/1000 | Loss: 0.00003194
Iteration 77/1000 | Loss: 0.00003194
Iteration 78/1000 | Loss: 0.00003194
Iteration 79/1000 | Loss: 0.00003194
Iteration 80/1000 | Loss: 0.00003194
Iteration 81/1000 | Loss: 0.00003194
Iteration 82/1000 | Loss: 0.00003193
Iteration 83/1000 | Loss: 0.00003193
Iteration 84/1000 | Loss: 0.00003193
Iteration 85/1000 | Loss: 0.00003193
Iteration 86/1000 | Loss: 0.00003193
Iteration 87/1000 | Loss: 0.00003193
Iteration 88/1000 | Loss: 0.00003193
Iteration 89/1000 | Loss: 0.00003193
Iteration 90/1000 | Loss: 0.00003192
Iteration 91/1000 | Loss: 0.00003192
Iteration 92/1000 | Loss: 0.00003192
Iteration 93/1000 | Loss: 0.00003192
Iteration 94/1000 | Loss: 0.00003192
Iteration 95/1000 | Loss: 0.00003192
Iteration 96/1000 | Loss: 0.00003192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.192294025211595e-05, 3.192294025211595e-05, 3.192294025211595e-05, 3.192294025211595e-05, 3.192294025211595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.192294025211595e-05

Optimization complete. Final v2v error: 4.717594146728516 mm

Highest mean error: 6.278505325317383 mm for frame 161

Lowest mean error: 4.177628040313721 mm for frame 110

Saving results

Total time: 38.08073806762695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484482
Iteration 2/25 | Loss: 0.00170626
Iteration 3/25 | Loss: 0.00159287
Iteration 4/25 | Loss: 0.00157093
Iteration 5/25 | Loss: 0.00156377
Iteration 6/25 | Loss: 0.00156205
Iteration 7/25 | Loss: 0.00156164
Iteration 8/25 | Loss: 0.00156164
Iteration 9/25 | Loss: 0.00156164
Iteration 10/25 | Loss: 0.00156164
Iteration 11/25 | Loss: 0.00156164
Iteration 12/25 | Loss: 0.00156164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001561639946885407, 0.001561639946885407, 0.001561639946885407, 0.001561639946885407, 0.001561639946885407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001561639946885407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55096281
Iteration 2/25 | Loss: 0.00211953
Iteration 3/25 | Loss: 0.00211953
Iteration 4/25 | Loss: 0.00211953
Iteration 5/25 | Loss: 0.00211953
Iteration 6/25 | Loss: 0.00211953
Iteration 7/25 | Loss: 0.00211953
Iteration 8/25 | Loss: 0.00211953
Iteration 9/25 | Loss: 0.00211953
Iteration 10/25 | Loss: 0.00211953
Iteration 11/25 | Loss: 0.00211953
Iteration 12/25 | Loss: 0.00211953
Iteration 13/25 | Loss: 0.00211953
Iteration 14/25 | Loss: 0.00211952
Iteration 15/25 | Loss: 0.00211952
Iteration 16/25 | Loss: 0.00211953
Iteration 17/25 | Loss: 0.00211953
Iteration 18/25 | Loss: 0.00211953
Iteration 19/25 | Loss: 0.00211953
Iteration 20/25 | Loss: 0.00211953
Iteration 21/25 | Loss: 0.00211953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021195251028984785, 0.0021195251028984785, 0.0021195251028984785, 0.0021195251028984785, 0.0021195251028984785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021195251028984785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211953
Iteration 2/1000 | Loss: 0.00006313
Iteration 3/1000 | Loss: 0.00004769
Iteration 4/1000 | Loss: 0.00004216
Iteration 5/1000 | Loss: 0.00003912
Iteration 6/1000 | Loss: 0.00003762
Iteration 7/1000 | Loss: 0.00003655
Iteration 8/1000 | Loss: 0.00003595
Iteration 9/1000 | Loss: 0.00003553
Iteration 10/1000 | Loss: 0.00003522
Iteration 11/1000 | Loss: 0.00003496
Iteration 12/1000 | Loss: 0.00003478
Iteration 13/1000 | Loss: 0.00003472
Iteration 14/1000 | Loss: 0.00003469
Iteration 15/1000 | Loss: 0.00003469
Iteration 16/1000 | Loss: 0.00003468
Iteration 17/1000 | Loss: 0.00003468
Iteration 18/1000 | Loss: 0.00003467
Iteration 19/1000 | Loss: 0.00003467
Iteration 20/1000 | Loss: 0.00003467
Iteration 21/1000 | Loss: 0.00003466
Iteration 22/1000 | Loss: 0.00003466
Iteration 23/1000 | Loss: 0.00003465
Iteration 24/1000 | Loss: 0.00003465
Iteration 25/1000 | Loss: 0.00003465
Iteration 26/1000 | Loss: 0.00003464
Iteration 27/1000 | Loss: 0.00003464
Iteration 28/1000 | Loss: 0.00003463
Iteration 29/1000 | Loss: 0.00003463
Iteration 30/1000 | Loss: 0.00003463
Iteration 31/1000 | Loss: 0.00003462
Iteration 32/1000 | Loss: 0.00003462
Iteration 33/1000 | Loss: 0.00003462
Iteration 34/1000 | Loss: 0.00003462
Iteration 35/1000 | Loss: 0.00003462
Iteration 36/1000 | Loss: 0.00003462
Iteration 37/1000 | Loss: 0.00003462
Iteration 38/1000 | Loss: 0.00003462
Iteration 39/1000 | Loss: 0.00003461
Iteration 40/1000 | Loss: 0.00003460
Iteration 41/1000 | Loss: 0.00003460
Iteration 42/1000 | Loss: 0.00003460
Iteration 43/1000 | Loss: 0.00003460
Iteration 44/1000 | Loss: 0.00003460
Iteration 45/1000 | Loss: 0.00003460
Iteration 46/1000 | Loss: 0.00003460
Iteration 47/1000 | Loss: 0.00003460
Iteration 48/1000 | Loss: 0.00003459
Iteration 49/1000 | Loss: 0.00003458
Iteration 50/1000 | Loss: 0.00003458
Iteration 51/1000 | Loss: 0.00003458
Iteration 52/1000 | Loss: 0.00003457
Iteration 53/1000 | Loss: 0.00003457
Iteration 54/1000 | Loss: 0.00003457
Iteration 55/1000 | Loss: 0.00003457
Iteration 56/1000 | Loss: 0.00003456
Iteration 57/1000 | Loss: 0.00003456
Iteration 58/1000 | Loss: 0.00003456
Iteration 59/1000 | Loss: 0.00003456
Iteration 60/1000 | Loss: 0.00003456
Iteration 61/1000 | Loss: 0.00003455
Iteration 62/1000 | Loss: 0.00003455
Iteration 63/1000 | Loss: 0.00003455
Iteration 64/1000 | Loss: 0.00003455
Iteration 65/1000 | Loss: 0.00003455
Iteration 66/1000 | Loss: 0.00003454
Iteration 67/1000 | Loss: 0.00003454
Iteration 68/1000 | Loss: 0.00003454
Iteration 69/1000 | Loss: 0.00003454
Iteration 70/1000 | Loss: 0.00003454
Iteration 71/1000 | Loss: 0.00003454
Iteration 72/1000 | Loss: 0.00003454
Iteration 73/1000 | Loss: 0.00003453
Iteration 74/1000 | Loss: 0.00003453
Iteration 75/1000 | Loss: 0.00003453
Iteration 76/1000 | Loss: 0.00003453
Iteration 77/1000 | Loss: 0.00003453
Iteration 78/1000 | Loss: 0.00003453
Iteration 79/1000 | Loss: 0.00003453
Iteration 80/1000 | Loss: 0.00003453
Iteration 81/1000 | Loss: 0.00003452
Iteration 82/1000 | Loss: 0.00003452
Iteration 83/1000 | Loss: 0.00003452
Iteration 84/1000 | Loss: 0.00003452
Iteration 85/1000 | Loss: 0.00003452
Iteration 86/1000 | Loss: 0.00003452
Iteration 87/1000 | Loss: 0.00003451
Iteration 88/1000 | Loss: 0.00003451
Iteration 89/1000 | Loss: 0.00003451
Iteration 90/1000 | Loss: 0.00003451
Iteration 91/1000 | Loss: 0.00003450
Iteration 92/1000 | Loss: 0.00003450
Iteration 93/1000 | Loss: 0.00003450
Iteration 94/1000 | Loss: 0.00003450
Iteration 95/1000 | Loss: 0.00003450
Iteration 96/1000 | Loss: 0.00003450
Iteration 97/1000 | Loss: 0.00003449
Iteration 98/1000 | Loss: 0.00003449
Iteration 99/1000 | Loss: 0.00003449
Iteration 100/1000 | Loss: 0.00003449
Iteration 101/1000 | Loss: 0.00003449
Iteration 102/1000 | Loss: 0.00003448
Iteration 103/1000 | Loss: 0.00003448
Iteration 104/1000 | Loss: 0.00003448
Iteration 105/1000 | Loss: 0.00003448
Iteration 106/1000 | Loss: 0.00003448
Iteration 107/1000 | Loss: 0.00003448
Iteration 108/1000 | Loss: 0.00003447
Iteration 109/1000 | Loss: 0.00003447
Iteration 110/1000 | Loss: 0.00003447
Iteration 111/1000 | Loss: 0.00003447
Iteration 112/1000 | Loss: 0.00003447
Iteration 113/1000 | Loss: 0.00003447
Iteration 114/1000 | Loss: 0.00003447
Iteration 115/1000 | Loss: 0.00003447
Iteration 116/1000 | Loss: 0.00003447
Iteration 117/1000 | Loss: 0.00003447
Iteration 118/1000 | Loss: 0.00003447
Iteration 119/1000 | Loss: 0.00003447
Iteration 120/1000 | Loss: 0.00003447
Iteration 121/1000 | Loss: 0.00003447
Iteration 122/1000 | Loss: 0.00003447
Iteration 123/1000 | Loss: 0.00003447
Iteration 124/1000 | Loss: 0.00003447
Iteration 125/1000 | Loss: 0.00003447
Iteration 126/1000 | Loss: 0.00003447
Iteration 127/1000 | Loss: 0.00003447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [3.4466454962966964e-05, 3.4466454962966964e-05, 3.4466454962966964e-05, 3.4466454962966964e-05, 3.4466454962966964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4466454962966964e-05

Optimization complete. Final v2v error: 5.114713191986084 mm

Highest mean error: 5.478385925292969 mm for frame 74

Lowest mean error: 4.681189060211182 mm for frame 51

Saving results

Total time: 35.27115058898926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01172043
Iteration 2/25 | Loss: 0.00339715
Iteration 3/25 | Loss: 0.00241287
Iteration 4/25 | Loss: 0.00217920
Iteration 5/25 | Loss: 0.00191635
Iteration 6/25 | Loss: 0.00185907
Iteration 7/25 | Loss: 0.00182247
Iteration 8/25 | Loss: 0.00180959
Iteration 9/25 | Loss: 0.00174345
Iteration 10/25 | Loss: 0.00174208
Iteration 11/25 | Loss: 0.00172739
Iteration 12/25 | Loss: 0.00171577
Iteration 13/25 | Loss: 0.00171241
Iteration 14/25 | Loss: 0.00171150
Iteration 15/25 | Loss: 0.00170418
Iteration 16/25 | Loss: 0.00169237
Iteration 17/25 | Loss: 0.00168326
Iteration 18/25 | Loss: 0.00168378
Iteration 19/25 | Loss: 0.00168266
Iteration 20/25 | Loss: 0.00168173
Iteration 21/25 | Loss: 0.00168141
Iteration 22/25 | Loss: 0.00168116
Iteration 23/25 | Loss: 0.00168090
Iteration 24/25 | Loss: 0.00168083
Iteration 25/25 | Loss: 0.00168083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47710121
Iteration 2/25 | Loss: 0.00388872
Iteration 3/25 | Loss: 0.00360212
Iteration 4/25 | Loss: 0.00360212
Iteration 5/25 | Loss: 0.00360212
Iteration 6/25 | Loss: 0.00360212
Iteration 7/25 | Loss: 0.00360212
Iteration 8/25 | Loss: 0.00360211
Iteration 9/25 | Loss: 0.00360211
Iteration 10/25 | Loss: 0.00360211
Iteration 11/25 | Loss: 0.00360211
Iteration 12/25 | Loss: 0.00360211
Iteration 13/25 | Loss: 0.00360211
Iteration 14/25 | Loss: 0.00360211
Iteration 15/25 | Loss: 0.00360211
Iteration 16/25 | Loss: 0.00360211
Iteration 17/25 | Loss: 0.00360211
Iteration 18/25 | Loss: 0.00360211
Iteration 19/25 | Loss: 0.00360211
Iteration 20/25 | Loss: 0.00360211
Iteration 21/25 | Loss: 0.00360211
Iteration 22/25 | Loss: 0.00360211
Iteration 23/25 | Loss: 0.00360211
Iteration 24/25 | Loss: 0.00360211
Iteration 25/25 | Loss: 0.00360211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00360211
Iteration 2/1000 | Loss: 0.00121310
Iteration 3/1000 | Loss: 0.00028852
Iteration 4/1000 | Loss: 0.00187550
Iteration 5/1000 | Loss: 0.00151402
Iteration 6/1000 | Loss: 0.00063510
Iteration 7/1000 | Loss: 0.00035797
Iteration 8/1000 | Loss: 0.00035539
Iteration 9/1000 | Loss: 0.00092930
Iteration 10/1000 | Loss: 0.00049942
Iteration 11/1000 | Loss: 0.00095744
Iteration 12/1000 | Loss: 0.00101380
Iteration 13/1000 | Loss: 0.00099867
Iteration 14/1000 | Loss: 0.00025266
Iteration 15/1000 | Loss: 0.00124279
Iteration 16/1000 | Loss: 0.00070660
Iteration 17/1000 | Loss: 0.00020968
Iteration 18/1000 | Loss: 0.00062207
Iteration 19/1000 | Loss: 0.00027317
Iteration 20/1000 | Loss: 0.00228313
Iteration 21/1000 | Loss: 0.00256066
Iteration 22/1000 | Loss: 0.00162706
Iteration 23/1000 | Loss: 0.00106482
Iteration 24/1000 | Loss: 0.00105081
Iteration 25/1000 | Loss: 0.00070348
Iteration 26/1000 | Loss: 0.00027303
Iteration 27/1000 | Loss: 0.00015447
Iteration 28/1000 | Loss: 0.00082104
Iteration 29/1000 | Loss: 0.00020937
Iteration 30/1000 | Loss: 0.00037525
Iteration 31/1000 | Loss: 0.00046892
Iteration 32/1000 | Loss: 0.00014875
Iteration 33/1000 | Loss: 0.00017676
Iteration 34/1000 | Loss: 0.00091025
Iteration 35/1000 | Loss: 0.00075027
Iteration 36/1000 | Loss: 0.00068193
Iteration 37/1000 | Loss: 0.00023022
Iteration 38/1000 | Loss: 0.00142317
Iteration 39/1000 | Loss: 0.00015199
Iteration 40/1000 | Loss: 0.00009521
Iteration 41/1000 | Loss: 0.00060069
Iteration 42/1000 | Loss: 0.00045425
Iteration 43/1000 | Loss: 0.00056112
Iteration 44/1000 | Loss: 0.00033364
Iteration 45/1000 | Loss: 0.00086559
Iteration 46/1000 | Loss: 0.00027184
Iteration 47/1000 | Loss: 0.00011323
Iteration 48/1000 | Loss: 0.00036696
Iteration 49/1000 | Loss: 0.00066446
Iteration 50/1000 | Loss: 0.00018491
Iteration 51/1000 | Loss: 0.00039906
Iteration 52/1000 | Loss: 0.00033836
Iteration 53/1000 | Loss: 0.00031568
Iteration 54/1000 | Loss: 0.00023398
Iteration 55/1000 | Loss: 0.00036444
Iteration 56/1000 | Loss: 0.00051729
Iteration 57/1000 | Loss: 0.00038481
Iteration 58/1000 | Loss: 0.00022841
Iteration 59/1000 | Loss: 0.00085048
Iteration 60/1000 | Loss: 0.00012331
Iteration 61/1000 | Loss: 0.00012569
Iteration 62/1000 | Loss: 0.00009703
Iteration 63/1000 | Loss: 0.00009401
Iteration 64/1000 | Loss: 0.00008301
Iteration 65/1000 | Loss: 0.00013269
Iteration 66/1000 | Loss: 0.00023222
Iteration 67/1000 | Loss: 0.00008515
Iteration 68/1000 | Loss: 0.00012297
Iteration 69/1000 | Loss: 0.00013046
Iteration 70/1000 | Loss: 0.00008409
Iteration 71/1000 | Loss: 0.00010177
Iteration 72/1000 | Loss: 0.00007687
Iteration 73/1000 | Loss: 0.00007616
Iteration 74/1000 | Loss: 0.00007564
Iteration 75/1000 | Loss: 0.00007517
Iteration 76/1000 | Loss: 0.00007496
Iteration 77/1000 | Loss: 0.00007469
Iteration 78/1000 | Loss: 0.00044567
Iteration 79/1000 | Loss: 0.00008406
Iteration 80/1000 | Loss: 0.00011820
Iteration 81/1000 | Loss: 0.00041755
Iteration 82/1000 | Loss: 0.00022942
Iteration 83/1000 | Loss: 0.00030151
Iteration 84/1000 | Loss: 0.00008084
Iteration 85/1000 | Loss: 0.00043169
Iteration 86/1000 | Loss: 0.00009266
Iteration 87/1000 | Loss: 0.00008238
Iteration 88/1000 | Loss: 0.00008410
Iteration 89/1000 | Loss: 0.00007591
Iteration 90/1000 | Loss: 0.00013793
Iteration 91/1000 | Loss: 0.00016014
Iteration 92/1000 | Loss: 0.00007300
Iteration 93/1000 | Loss: 0.00007687
Iteration 94/1000 | Loss: 0.00007206
Iteration 95/1000 | Loss: 0.00012401
Iteration 96/1000 | Loss: 0.00007176
Iteration 97/1000 | Loss: 0.00007149
Iteration 98/1000 | Loss: 0.00007147
Iteration 99/1000 | Loss: 0.00007145
Iteration 100/1000 | Loss: 0.00007144
Iteration 101/1000 | Loss: 0.00007143
Iteration 102/1000 | Loss: 0.00007143
Iteration 103/1000 | Loss: 0.00007142
Iteration 104/1000 | Loss: 0.00007142
Iteration 105/1000 | Loss: 0.00007142
Iteration 106/1000 | Loss: 0.00007142
Iteration 107/1000 | Loss: 0.00007142
Iteration 108/1000 | Loss: 0.00007142
Iteration 109/1000 | Loss: 0.00007140
Iteration 110/1000 | Loss: 0.00007140
Iteration 111/1000 | Loss: 0.00007138
Iteration 112/1000 | Loss: 0.00007138
Iteration 113/1000 | Loss: 0.00007138
Iteration 114/1000 | Loss: 0.00007138
Iteration 115/1000 | Loss: 0.00007137
Iteration 116/1000 | Loss: 0.00007137
Iteration 117/1000 | Loss: 0.00007136
Iteration 118/1000 | Loss: 0.00007136
Iteration 119/1000 | Loss: 0.00007136
Iteration 120/1000 | Loss: 0.00007135
Iteration 121/1000 | Loss: 0.00007135
Iteration 122/1000 | Loss: 0.00007134
Iteration 123/1000 | Loss: 0.00007134
Iteration 124/1000 | Loss: 0.00007133
Iteration 125/1000 | Loss: 0.00007133
Iteration 126/1000 | Loss: 0.00007132
Iteration 127/1000 | Loss: 0.00007132
Iteration 128/1000 | Loss: 0.00007132
Iteration 129/1000 | Loss: 0.00007132
Iteration 130/1000 | Loss: 0.00007131
Iteration 131/1000 | Loss: 0.00007131
Iteration 132/1000 | Loss: 0.00007131
Iteration 133/1000 | Loss: 0.00007131
Iteration 134/1000 | Loss: 0.00007130
Iteration 135/1000 | Loss: 0.00007130
Iteration 136/1000 | Loss: 0.00007130
Iteration 137/1000 | Loss: 0.00007129
Iteration 138/1000 | Loss: 0.00007129
Iteration 139/1000 | Loss: 0.00007129
Iteration 140/1000 | Loss: 0.00007129
Iteration 141/1000 | Loss: 0.00007129
Iteration 142/1000 | Loss: 0.00007129
Iteration 143/1000 | Loss: 0.00007129
Iteration 144/1000 | Loss: 0.00007128
Iteration 145/1000 | Loss: 0.00007128
Iteration 146/1000 | Loss: 0.00007128
Iteration 147/1000 | Loss: 0.00007128
Iteration 148/1000 | Loss: 0.00007128
Iteration 149/1000 | Loss: 0.00007128
Iteration 150/1000 | Loss: 0.00007128
Iteration 151/1000 | Loss: 0.00007128
Iteration 152/1000 | Loss: 0.00007128
Iteration 153/1000 | Loss: 0.00007128
Iteration 154/1000 | Loss: 0.00007128
Iteration 155/1000 | Loss: 0.00007128
Iteration 156/1000 | Loss: 0.00007127
Iteration 157/1000 | Loss: 0.00007127
Iteration 158/1000 | Loss: 0.00007127
Iteration 159/1000 | Loss: 0.00007127
Iteration 160/1000 | Loss: 0.00007127
Iteration 161/1000 | Loss: 0.00007127
Iteration 162/1000 | Loss: 0.00007127
Iteration 163/1000 | Loss: 0.00007127
Iteration 164/1000 | Loss: 0.00007127
Iteration 165/1000 | Loss: 0.00007127
Iteration 166/1000 | Loss: 0.00007127
Iteration 167/1000 | Loss: 0.00007127
Iteration 168/1000 | Loss: 0.00007127
Iteration 169/1000 | Loss: 0.00007127
Iteration 170/1000 | Loss: 0.00007126
Iteration 171/1000 | Loss: 0.00007126
Iteration 172/1000 | Loss: 0.00007126
Iteration 173/1000 | Loss: 0.00007126
Iteration 174/1000 | Loss: 0.00007126
Iteration 175/1000 | Loss: 0.00007126
Iteration 176/1000 | Loss: 0.00007126
Iteration 177/1000 | Loss: 0.00007126
Iteration 178/1000 | Loss: 0.00007126
Iteration 179/1000 | Loss: 0.00007126
Iteration 180/1000 | Loss: 0.00007126
Iteration 181/1000 | Loss: 0.00007126
Iteration 182/1000 | Loss: 0.00007126
Iteration 183/1000 | Loss: 0.00007126
Iteration 184/1000 | Loss: 0.00007126
Iteration 185/1000 | Loss: 0.00007126
Iteration 186/1000 | Loss: 0.00007126
Iteration 187/1000 | Loss: 0.00007126
Iteration 188/1000 | Loss: 0.00007125
Iteration 189/1000 | Loss: 0.00007125
Iteration 190/1000 | Loss: 0.00007125
Iteration 191/1000 | Loss: 0.00007125
Iteration 192/1000 | Loss: 0.00007125
Iteration 193/1000 | Loss: 0.00007125
Iteration 194/1000 | Loss: 0.00007125
Iteration 195/1000 | Loss: 0.00007125
Iteration 196/1000 | Loss: 0.00007125
Iteration 197/1000 | Loss: 0.00007125
Iteration 198/1000 | Loss: 0.00007125
Iteration 199/1000 | Loss: 0.00007125
Iteration 200/1000 | Loss: 0.00007125
Iteration 201/1000 | Loss: 0.00007125
Iteration 202/1000 | Loss: 0.00007124
Iteration 203/1000 | Loss: 0.00007124
Iteration 204/1000 | Loss: 0.00007124
Iteration 205/1000 | Loss: 0.00007124
Iteration 206/1000 | Loss: 0.00007124
Iteration 207/1000 | Loss: 0.00007124
Iteration 208/1000 | Loss: 0.00007124
Iteration 209/1000 | Loss: 0.00007124
Iteration 210/1000 | Loss: 0.00007124
Iteration 211/1000 | Loss: 0.00007124
Iteration 212/1000 | Loss: 0.00007124
Iteration 213/1000 | Loss: 0.00007124
Iteration 214/1000 | Loss: 0.00007124
Iteration 215/1000 | Loss: 0.00007124
Iteration 216/1000 | Loss: 0.00007124
Iteration 217/1000 | Loss: 0.00007124
Iteration 218/1000 | Loss: 0.00007124
Iteration 219/1000 | Loss: 0.00007124
Iteration 220/1000 | Loss: 0.00007124
Iteration 221/1000 | Loss: 0.00007124
Iteration 222/1000 | Loss: 0.00007124
Iteration 223/1000 | Loss: 0.00007124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [7.124186959117651e-05, 7.124186959117651e-05, 7.124186959117651e-05, 7.124186959117651e-05, 7.124186959117651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.124186959117651e-05

Optimization complete. Final v2v error: 5.611358642578125 mm

Highest mean error: 14.785433769226074 mm for frame 38

Lowest mean error: 4.687566757202148 mm for frame 0

Saving results

Total time: 185.8978660106659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056953
Iteration 2/25 | Loss: 0.00307054
Iteration 3/25 | Loss: 0.00222366
Iteration 4/25 | Loss: 0.00172833
Iteration 5/25 | Loss: 0.00176324
Iteration 6/25 | Loss: 0.00162335
Iteration 7/25 | Loss: 0.00140960
Iteration 8/25 | Loss: 0.00131848
Iteration 9/25 | Loss: 0.00125567
Iteration 10/25 | Loss: 0.00125377
Iteration 11/25 | Loss: 0.00123278
Iteration 12/25 | Loss: 0.00121904
Iteration 13/25 | Loss: 0.00121362
Iteration 14/25 | Loss: 0.00119658
Iteration 15/25 | Loss: 0.00118567
Iteration 16/25 | Loss: 0.00117406
Iteration 17/25 | Loss: 0.00117415
Iteration 18/25 | Loss: 0.00116190
Iteration 19/25 | Loss: 0.00116203
Iteration 20/25 | Loss: 0.00115836
Iteration 21/25 | Loss: 0.00115757
Iteration 22/25 | Loss: 0.00115727
Iteration 23/25 | Loss: 0.00115716
Iteration 24/25 | Loss: 0.00115709
Iteration 25/25 | Loss: 0.00115709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43580854
Iteration 2/25 | Loss: 0.00112220
Iteration 3/25 | Loss: 0.00112220
Iteration 4/25 | Loss: 0.00112220
Iteration 5/25 | Loss: 0.00112220
Iteration 6/25 | Loss: 0.00112220
Iteration 7/25 | Loss: 0.00112220
Iteration 8/25 | Loss: 0.00112220
Iteration 9/25 | Loss: 0.00112220
Iteration 10/25 | Loss: 0.00112220
Iteration 11/25 | Loss: 0.00112220
Iteration 12/25 | Loss: 0.00112220
Iteration 13/25 | Loss: 0.00112220
Iteration 14/25 | Loss: 0.00112220
Iteration 15/25 | Loss: 0.00112220
Iteration 16/25 | Loss: 0.00112220
Iteration 17/25 | Loss: 0.00112220
Iteration 18/25 | Loss: 0.00112220
Iteration 19/25 | Loss: 0.00112220
Iteration 20/25 | Loss: 0.00112220
Iteration 21/25 | Loss: 0.00112220
Iteration 22/25 | Loss: 0.00112220
Iteration 23/25 | Loss: 0.00112220
Iteration 24/25 | Loss: 0.00112220
Iteration 25/25 | Loss: 0.00112220

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112220
Iteration 2/1000 | Loss: 0.00016854
Iteration 3/1000 | Loss: 0.00010927
Iteration 4/1000 | Loss: 0.00008292
Iteration 5/1000 | Loss: 0.00006994
Iteration 6/1000 | Loss: 0.00006484
Iteration 7/1000 | Loss: 0.00006228
Iteration 8/1000 | Loss: 0.00006074
Iteration 9/1000 | Loss: 0.00005988
Iteration 10/1000 | Loss: 0.00005924
Iteration 11/1000 | Loss: 0.00005879
Iteration 12/1000 | Loss: 0.00030933
Iteration 13/1000 | Loss: 0.00031399
Iteration 14/1000 | Loss: 0.00039101
Iteration 15/1000 | Loss: 0.00036252
Iteration 16/1000 | Loss: 0.00019169
Iteration 17/1000 | Loss: 0.00006333
Iteration 18/1000 | Loss: 0.00009099
Iteration 19/1000 | Loss: 0.00006003
Iteration 20/1000 | Loss: 0.00005884
Iteration 21/1000 | Loss: 0.00005786
Iteration 22/1000 | Loss: 0.00005726
Iteration 23/1000 | Loss: 0.00005680
Iteration 24/1000 | Loss: 0.00005645
Iteration 25/1000 | Loss: 0.00005634
Iteration 26/1000 | Loss: 0.00005632
Iteration 27/1000 | Loss: 0.00005629
Iteration 28/1000 | Loss: 0.00005629
Iteration 29/1000 | Loss: 0.00005624
Iteration 30/1000 | Loss: 0.00005624
Iteration 31/1000 | Loss: 0.00005623
Iteration 32/1000 | Loss: 0.00005623
Iteration 33/1000 | Loss: 0.00005622
Iteration 34/1000 | Loss: 0.00005622
Iteration 35/1000 | Loss: 0.00005622
Iteration 36/1000 | Loss: 0.00005621
Iteration 37/1000 | Loss: 0.00005621
Iteration 38/1000 | Loss: 0.00005621
Iteration 39/1000 | Loss: 0.00005620
Iteration 40/1000 | Loss: 0.00005620
Iteration 41/1000 | Loss: 0.00005619
Iteration 42/1000 | Loss: 0.00005618
Iteration 43/1000 | Loss: 0.00005618
Iteration 44/1000 | Loss: 0.00005617
Iteration 45/1000 | Loss: 0.00005616
Iteration 46/1000 | Loss: 0.00005616
Iteration 47/1000 | Loss: 0.00005614
Iteration 48/1000 | Loss: 0.00005614
Iteration 49/1000 | Loss: 0.00005614
Iteration 50/1000 | Loss: 0.00005614
Iteration 51/1000 | Loss: 0.00005614
Iteration 52/1000 | Loss: 0.00005614
Iteration 53/1000 | Loss: 0.00005614
Iteration 54/1000 | Loss: 0.00005614
Iteration 55/1000 | Loss: 0.00005614
Iteration 56/1000 | Loss: 0.00005614
Iteration 57/1000 | Loss: 0.00005614
Iteration 58/1000 | Loss: 0.00005613
Iteration 59/1000 | Loss: 0.00005613
Iteration 60/1000 | Loss: 0.00005613
Iteration 61/1000 | Loss: 0.00005612
Iteration 62/1000 | Loss: 0.00005612
Iteration 63/1000 | Loss: 0.00005612
Iteration 64/1000 | Loss: 0.00005612
Iteration 65/1000 | Loss: 0.00005611
Iteration 66/1000 | Loss: 0.00005611
Iteration 67/1000 | Loss: 0.00005611
Iteration 68/1000 | Loss: 0.00005611
Iteration 69/1000 | Loss: 0.00005611
Iteration 70/1000 | Loss: 0.00005611
Iteration 71/1000 | Loss: 0.00005611
Iteration 72/1000 | Loss: 0.00005610
Iteration 73/1000 | Loss: 0.00005610
Iteration 74/1000 | Loss: 0.00005610
Iteration 75/1000 | Loss: 0.00005610
Iteration 76/1000 | Loss: 0.00005609
Iteration 77/1000 | Loss: 0.00005609
Iteration 78/1000 | Loss: 0.00005609
Iteration 79/1000 | Loss: 0.00005609
Iteration 80/1000 | Loss: 0.00005609
Iteration 81/1000 | Loss: 0.00005609
Iteration 82/1000 | Loss: 0.00005609
Iteration 83/1000 | Loss: 0.00005609
Iteration 84/1000 | Loss: 0.00005609
Iteration 85/1000 | Loss: 0.00005609
Iteration 86/1000 | Loss: 0.00005609
Iteration 87/1000 | Loss: 0.00005608
Iteration 88/1000 | Loss: 0.00005608
Iteration 89/1000 | Loss: 0.00005608
Iteration 90/1000 | Loss: 0.00005608
Iteration 91/1000 | Loss: 0.00005608
Iteration 92/1000 | Loss: 0.00005608
Iteration 93/1000 | Loss: 0.00005608
Iteration 94/1000 | Loss: 0.00005608
Iteration 95/1000 | Loss: 0.00005608
Iteration 96/1000 | Loss: 0.00005608
Iteration 97/1000 | Loss: 0.00005608
Iteration 98/1000 | Loss: 0.00005607
Iteration 99/1000 | Loss: 0.00005607
Iteration 100/1000 | Loss: 0.00005607
Iteration 101/1000 | Loss: 0.00005607
Iteration 102/1000 | Loss: 0.00005607
Iteration 103/1000 | Loss: 0.00005607
Iteration 104/1000 | Loss: 0.00005607
Iteration 105/1000 | Loss: 0.00005607
Iteration 106/1000 | Loss: 0.00005607
Iteration 107/1000 | Loss: 0.00005607
Iteration 108/1000 | Loss: 0.00005606
Iteration 109/1000 | Loss: 0.00005606
Iteration 110/1000 | Loss: 0.00005606
Iteration 111/1000 | Loss: 0.00005606
Iteration 112/1000 | Loss: 0.00005606
Iteration 113/1000 | Loss: 0.00005606
Iteration 114/1000 | Loss: 0.00005605
Iteration 115/1000 | Loss: 0.00005605
Iteration 116/1000 | Loss: 0.00005605
Iteration 117/1000 | Loss: 0.00005605
Iteration 118/1000 | Loss: 0.00005605
Iteration 119/1000 | Loss: 0.00005605
Iteration 120/1000 | Loss: 0.00005605
Iteration 121/1000 | Loss: 0.00005605
Iteration 122/1000 | Loss: 0.00005605
Iteration 123/1000 | Loss: 0.00005605
Iteration 124/1000 | Loss: 0.00005605
Iteration 125/1000 | Loss: 0.00005604
Iteration 126/1000 | Loss: 0.00005604
Iteration 127/1000 | Loss: 0.00005604
Iteration 128/1000 | Loss: 0.00005604
Iteration 129/1000 | Loss: 0.00005604
Iteration 130/1000 | Loss: 0.00005604
Iteration 131/1000 | Loss: 0.00005604
Iteration 132/1000 | Loss: 0.00005604
Iteration 133/1000 | Loss: 0.00005604
Iteration 134/1000 | Loss: 0.00005604
Iteration 135/1000 | Loss: 0.00005604
Iteration 136/1000 | Loss: 0.00005604
Iteration 137/1000 | Loss: 0.00005604
Iteration 138/1000 | Loss: 0.00005603
Iteration 139/1000 | Loss: 0.00005603
Iteration 140/1000 | Loss: 0.00005603
Iteration 141/1000 | Loss: 0.00005603
Iteration 142/1000 | Loss: 0.00005603
Iteration 143/1000 | Loss: 0.00005603
Iteration 144/1000 | Loss: 0.00005603
Iteration 145/1000 | Loss: 0.00005603
Iteration 146/1000 | Loss: 0.00005603
Iteration 147/1000 | Loss: 0.00005603
Iteration 148/1000 | Loss: 0.00005603
Iteration 149/1000 | Loss: 0.00005603
Iteration 150/1000 | Loss: 0.00005603
Iteration 151/1000 | Loss: 0.00005602
Iteration 152/1000 | Loss: 0.00005602
Iteration 153/1000 | Loss: 0.00005602
Iteration 154/1000 | Loss: 0.00005602
Iteration 155/1000 | Loss: 0.00005602
Iteration 156/1000 | Loss: 0.00005602
Iteration 157/1000 | Loss: 0.00005602
Iteration 158/1000 | Loss: 0.00005602
Iteration 159/1000 | Loss: 0.00005602
Iteration 160/1000 | Loss: 0.00005602
Iteration 161/1000 | Loss: 0.00005602
Iteration 162/1000 | Loss: 0.00005602
Iteration 163/1000 | Loss: 0.00005602
Iteration 164/1000 | Loss: 0.00005602
Iteration 165/1000 | Loss: 0.00005602
Iteration 166/1000 | Loss: 0.00005602
Iteration 167/1000 | Loss: 0.00005602
Iteration 168/1000 | Loss: 0.00005602
Iteration 169/1000 | Loss: 0.00005601
Iteration 170/1000 | Loss: 0.00005601
Iteration 171/1000 | Loss: 0.00005601
Iteration 172/1000 | Loss: 0.00005601
Iteration 173/1000 | Loss: 0.00005601
Iteration 174/1000 | Loss: 0.00005601
Iteration 175/1000 | Loss: 0.00005601
Iteration 176/1000 | Loss: 0.00005601
Iteration 177/1000 | Loss: 0.00005601
Iteration 178/1000 | Loss: 0.00005601
Iteration 179/1000 | Loss: 0.00005601
Iteration 180/1000 | Loss: 0.00005601
Iteration 181/1000 | Loss: 0.00005601
Iteration 182/1000 | Loss: 0.00005601
Iteration 183/1000 | Loss: 0.00005601
Iteration 184/1000 | Loss: 0.00005601
Iteration 185/1000 | Loss: 0.00005601
Iteration 186/1000 | Loss: 0.00005601
Iteration 187/1000 | Loss: 0.00005601
Iteration 188/1000 | Loss: 0.00005601
Iteration 189/1000 | Loss: 0.00005600
Iteration 190/1000 | Loss: 0.00005600
Iteration 191/1000 | Loss: 0.00005600
Iteration 192/1000 | Loss: 0.00005600
Iteration 193/1000 | Loss: 0.00005600
Iteration 194/1000 | Loss: 0.00005600
Iteration 195/1000 | Loss: 0.00005600
Iteration 196/1000 | Loss: 0.00005600
Iteration 197/1000 | Loss: 0.00005600
Iteration 198/1000 | Loss: 0.00005600
Iteration 199/1000 | Loss: 0.00005600
Iteration 200/1000 | Loss: 0.00005600
Iteration 201/1000 | Loss: 0.00005600
Iteration 202/1000 | Loss: 0.00005600
Iteration 203/1000 | Loss: 0.00005600
Iteration 204/1000 | Loss: 0.00005600
Iteration 205/1000 | Loss: 0.00005599
Iteration 206/1000 | Loss: 0.00005599
Iteration 207/1000 | Loss: 0.00005599
Iteration 208/1000 | Loss: 0.00005599
Iteration 209/1000 | Loss: 0.00005599
Iteration 210/1000 | Loss: 0.00005599
Iteration 211/1000 | Loss: 0.00005599
Iteration 212/1000 | Loss: 0.00005599
Iteration 213/1000 | Loss: 0.00005599
Iteration 214/1000 | Loss: 0.00005599
Iteration 215/1000 | Loss: 0.00005599
Iteration 216/1000 | Loss: 0.00005599
Iteration 217/1000 | Loss: 0.00005599
Iteration 218/1000 | Loss: 0.00005599
Iteration 219/1000 | Loss: 0.00005599
Iteration 220/1000 | Loss: 0.00005599
Iteration 221/1000 | Loss: 0.00005599
Iteration 222/1000 | Loss: 0.00005598
Iteration 223/1000 | Loss: 0.00005598
Iteration 224/1000 | Loss: 0.00005598
Iteration 225/1000 | Loss: 0.00005598
Iteration 226/1000 | Loss: 0.00005598
Iteration 227/1000 | Loss: 0.00005598
Iteration 228/1000 | Loss: 0.00005598
Iteration 229/1000 | Loss: 0.00005598
Iteration 230/1000 | Loss: 0.00005597
Iteration 231/1000 | Loss: 0.00005597
Iteration 232/1000 | Loss: 0.00005597
Iteration 233/1000 | Loss: 0.00005597
Iteration 234/1000 | Loss: 0.00005597
Iteration 235/1000 | Loss: 0.00005597
Iteration 236/1000 | Loss: 0.00005597
Iteration 237/1000 | Loss: 0.00005597
Iteration 238/1000 | Loss: 0.00005597
Iteration 239/1000 | Loss: 0.00005597
Iteration 240/1000 | Loss: 0.00005597
Iteration 241/1000 | Loss: 0.00005597
Iteration 242/1000 | Loss: 0.00005597
Iteration 243/1000 | Loss: 0.00005597
Iteration 244/1000 | Loss: 0.00005597
Iteration 245/1000 | Loss: 0.00005597
Iteration 246/1000 | Loss: 0.00005597
Iteration 247/1000 | Loss: 0.00005597
Iteration 248/1000 | Loss: 0.00005597
Iteration 249/1000 | Loss: 0.00005597
Iteration 250/1000 | Loss: 0.00005597
Iteration 251/1000 | Loss: 0.00005597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [5.596774281002581e-05, 5.596774281002581e-05, 5.596774281002581e-05, 5.596774281002581e-05, 5.596774281002581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.596774281002581e-05

Optimization complete. Final v2v error: 6.040971279144287 mm

Highest mean error: 12.54436206817627 mm for frame 191

Lowest mean error: 5.097598552703857 mm for frame 5

Saving results

Total time: 105.23386406898499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542670
Iteration 2/25 | Loss: 0.00182458
Iteration 3/25 | Loss: 0.00165765
Iteration 4/25 | Loss: 0.00161965
Iteration 5/25 | Loss: 0.00161025
Iteration 6/25 | Loss: 0.00160767
Iteration 7/25 | Loss: 0.00160706
Iteration 8/25 | Loss: 0.00160706
Iteration 9/25 | Loss: 0.00160706
Iteration 10/25 | Loss: 0.00160706
Iteration 11/25 | Loss: 0.00160706
Iteration 12/25 | Loss: 0.00160706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001607057056389749, 0.001607057056389749, 0.001607057056389749, 0.001607057056389749, 0.001607057056389749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001607057056389749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10549808
Iteration 2/25 | Loss: 0.00218727
Iteration 3/25 | Loss: 0.00218726
Iteration 4/25 | Loss: 0.00218726
Iteration 5/25 | Loss: 0.00218726
Iteration 6/25 | Loss: 0.00218726
Iteration 7/25 | Loss: 0.00218726
Iteration 8/25 | Loss: 0.00218726
Iteration 9/25 | Loss: 0.00218726
Iteration 10/25 | Loss: 0.00218726
Iteration 11/25 | Loss: 0.00218726
Iteration 12/25 | Loss: 0.00218726
Iteration 13/25 | Loss: 0.00218726
Iteration 14/25 | Loss: 0.00218726
Iteration 15/25 | Loss: 0.00218726
Iteration 16/25 | Loss: 0.00218726
Iteration 17/25 | Loss: 0.00218726
Iteration 18/25 | Loss: 0.00218726
Iteration 19/25 | Loss: 0.00218726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021872592624276876, 0.0021872592624276876, 0.0021872592624276876, 0.0021872592624276876, 0.0021872592624276876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021872592624276876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218726
Iteration 2/1000 | Loss: 0.00006840
Iteration 3/1000 | Loss: 0.00004629
Iteration 4/1000 | Loss: 0.00004025
Iteration 5/1000 | Loss: 0.00003734
Iteration 6/1000 | Loss: 0.00003532
Iteration 7/1000 | Loss: 0.00003404
Iteration 8/1000 | Loss: 0.00003319
Iteration 9/1000 | Loss: 0.00003252
Iteration 10/1000 | Loss: 0.00003201
Iteration 11/1000 | Loss: 0.00003161
Iteration 12/1000 | Loss: 0.00003135
Iteration 13/1000 | Loss: 0.00003120
Iteration 14/1000 | Loss: 0.00003116
Iteration 15/1000 | Loss: 0.00003110
Iteration 16/1000 | Loss: 0.00003105
Iteration 17/1000 | Loss: 0.00003098
Iteration 18/1000 | Loss: 0.00003094
Iteration 19/1000 | Loss: 0.00003094
Iteration 20/1000 | Loss: 0.00003093
Iteration 21/1000 | Loss: 0.00003093
Iteration 22/1000 | Loss: 0.00003089
Iteration 23/1000 | Loss: 0.00003089
Iteration 24/1000 | Loss: 0.00003087
Iteration 25/1000 | Loss: 0.00003087
Iteration 26/1000 | Loss: 0.00003087
Iteration 27/1000 | Loss: 0.00003086
Iteration 28/1000 | Loss: 0.00003086
Iteration 29/1000 | Loss: 0.00003086
Iteration 30/1000 | Loss: 0.00003086
Iteration 31/1000 | Loss: 0.00003086
Iteration 32/1000 | Loss: 0.00003085
Iteration 33/1000 | Loss: 0.00003085
Iteration 34/1000 | Loss: 0.00003085
Iteration 35/1000 | Loss: 0.00003085
Iteration 36/1000 | Loss: 0.00003084
Iteration 37/1000 | Loss: 0.00003084
Iteration 38/1000 | Loss: 0.00003082
Iteration 39/1000 | Loss: 0.00003082
Iteration 40/1000 | Loss: 0.00003082
Iteration 41/1000 | Loss: 0.00003082
Iteration 42/1000 | Loss: 0.00003082
Iteration 43/1000 | Loss: 0.00003081
Iteration 44/1000 | Loss: 0.00003081
Iteration 45/1000 | Loss: 0.00003080
Iteration 46/1000 | Loss: 0.00003080
Iteration 47/1000 | Loss: 0.00003080
Iteration 48/1000 | Loss: 0.00003080
Iteration 49/1000 | Loss: 0.00003079
Iteration 50/1000 | Loss: 0.00003079
Iteration 51/1000 | Loss: 0.00003079
Iteration 52/1000 | Loss: 0.00003079
Iteration 53/1000 | Loss: 0.00003079
Iteration 54/1000 | Loss: 0.00003079
Iteration 55/1000 | Loss: 0.00003079
Iteration 56/1000 | Loss: 0.00003078
Iteration 57/1000 | Loss: 0.00003078
Iteration 58/1000 | Loss: 0.00003078
Iteration 59/1000 | Loss: 0.00003078
Iteration 60/1000 | Loss: 0.00003078
Iteration 61/1000 | Loss: 0.00003078
Iteration 62/1000 | Loss: 0.00003078
Iteration 63/1000 | Loss: 0.00003077
Iteration 64/1000 | Loss: 0.00003077
Iteration 65/1000 | Loss: 0.00003077
Iteration 66/1000 | Loss: 0.00003077
Iteration 67/1000 | Loss: 0.00003077
Iteration 68/1000 | Loss: 0.00003077
Iteration 69/1000 | Loss: 0.00003076
Iteration 70/1000 | Loss: 0.00003076
Iteration 71/1000 | Loss: 0.00003076
Iteration 72/1000 | Loss: 0.00003076
Iteration 73/1000 | Loss: 0.00003076
Iteration 74/1000 | Loss: 0.00003075
Iteration 75/1000 | Loss: 0.00003075
Iteration 76/1000 | Loss: 0.00003075
Iteration 77/1000 | Loss: 0.00003075
Iteration 78/1000 | Loss: 0.00003074
Iteration 79/1000 | Loss: 0.00003074
Iteration 80/1000 | Loss: 0.00003074
Iteration 81/1000 | Loss: 0.00003073
Iteration 82/1000 | Loss: 0.00003073
Iteration 83/1000 | Loss: 0.00003073
Iteration 84/1000 | Loss: 0.00003072
Iteration 85/1000 | Loss: 0.00003072
Iteration 86/1000 | Loss: 0.00003072
Iteration 87/1000 | Loss: 0.00003072
Iteration 88/1000 | Loss: 0.00003072
Iteration 89/1000 | Loss: 0.00003071
Iteration 90/1000 | Loss: 0.00003071
Iteration 91/1000 | Loss: 0.00003071
Iteration 92/1000 | Loss: 0.00003071
Iteration 93/1000 | Loss: 0.00003071
Iteration 94/1000 | Loss: 0.00003070
Iteration 95/1000 | Loss: 0.00003070
Iteration 96/1000 | Loss: 0.00003070
Iteration 97/1000 | Loss: 0.00003070
Iteration 98/1000 | Loss: 0.00003070
Iteration 99/1000 | Loss: 0.00003070
Iteration 100/1000 | Loss: 0.00003070
Iteration 101/1000 | Loss: 0.00003070
Iteration 102/1000 | Loss: 0.00003070
Iteration 103/1000 | Loss: 0.00003070
Iteration 104/1000 | Loss: 0.00003070
Iteration 105/1000 | Loss: 0.00003070
Iteration 106/1000 | Loss: 0.00003070
Iteration 107/1000 | Loss: 0.00003070
Iteration 108/1000 | Loss: 0.00003070
Iteration 109/1000 | Loss: 0.00003070
Iteration 110/1000 | Loss: 0.00003070
Iteration 111/1000 | Loss: 0.00003070
Iteration 112/1000 | Loss: 0.00003070
Iteration 113/1000 | Loss: 0.00003070
Iteration 114/1000 | Loss: 0.00003070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.070132152060978e-05, 3.070132152060978e-05, 3.070132152060978e-05, 3.070132152060978e-05, 3.070132152060978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.070132152060978e-05

Optimization complete. Final v2v error: 4.85818338394165 mm

Highest mean error: 5.191754341125488 mm for frame 57

Lowest mean error: 4.3866376876831055 mm for frame 185

Saving results

Total time: 43.63038110733032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01206145
Iteration 2/25 | Loss: 0.00223810
Iteration 3/25 | Loss: 0.00187182
Iteration 4/25 | Loss: 0.00178085
Iteration 5/25 | Loss: 0.00176271
Iteration 6/25 | Loss: 0.00174248
Iteration 7/25 | Loss: 0.00172379
Iteration 8/25 | Loss: 0.00172029
Iteration 9/25 | Loss: 0.00171905
Iteration 10/25 | Loss: 0.00171861
Iteration 11/25 | Loss: 0.00171842
Iteration 12/25 | Loss: 0.00171835
Iteration 13/25 | Loss: 0.00171834
Iteration 14/25 | Loss: 0.00171833
Iteration 15/25 | Loss: 0.00171833
Iteration 16/25 | Loss: 0.00171833
Iteration 17/25 | Loss: 0.00171833
Iteration 18/25 | Loss: 0.00171833
Iteration 19/25 | Loss: 0.00171833
Iteration 20/25 | Loss: 0.00171832
Iteration 21/25 | Loss: 0.00171832
Iteration 22/25 | Loss: 0.00171832
Iteration 23/25 | Loss: 0.00171832
Iteration 24/25 | Loss: 0.00171832
Iteration 25/25 | Loss: 0.00171832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.30505848
Iteration 2/25 | Loss: 0.00277630
Iteration 3/25 | Loss: 0.00277607
Iteration 4/25 | Loss: 0.00277607
Iteration 5/25 | Loss: 0.00277607
Iteration 6/25 | Loss: 0.00277606
Iteration 7/25 | Loss: 0.00277606
Iteration 8/25 | Loss: 0.00277606
Iteration 9/25 | Loss: 0.00277606
Iteration 10/25 | Loss: 0.00277606
Iteration 11/25 | Loss: 0.00277606
Iteration 12/25 | Loss: 0.00277606
Iteration 13/25 | Loss: 0.00277606
Iteration 14/25 | Loss: 0.00277606
Iteration 15/25 | Loss: 0.00277606
Iteration 16/25 | Loss: 0.00277606
Iteration 17/25 | Loss: 0.00277606
Iteration 18/25 | Loss: 0.00277606
Iteration 19/25 | Loss: 0.00277606
Iteration 20/25 | Loss: 0.00277606
Iteration 21/25 | Loss: 0.00277606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002776064444333315, 0.002776064444333315, 0.002776064444333315, 0.002776064444333315, 0.002776064444333315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002776064444333315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277606
Iteration 2/1000 | Loss: 0.00013379
Iteration 3/1000 | Loss: 0.00009393
Iteration 4/1000 | Loss: 0.00007583
Iteration 5/1000 | Loss: 0.00006753
Iteration 6/1000 | Loss: 0.00006262
Iteration 7/1000 | Loss: 0.00005957
Iteration 8/1000 | Loss: 0.00005766
Iteration 9/1000 | Loss: 0.00065368
Iteration 10/1000 | Loss: 0.00006462
Iteration 11/1000 | Loss: 0.00005645
Iteration 12/1000 | Loss: 0.00005393
Iteration 13/1000 | Loss: 0.00005212
Iteration 14/1000 | Loss: 0.00005089
Iteration 15/1000 | Loss: 0.00004986
Iteration 16/1000 | Loss: 0.00004949
Iteration 17/1000 | Loss: 0.00004926
Iteration 18/1000 | Loss: 0.00004908
Iteration 19/1000 | Loss: 0.00004884
Iteration 20/1000 | Loss: 0.00004858
Iteration 21/1000 | Loss: 0.00004840
Iteration 22/1000 | Loss: 0.00004825
Iteration 23/1000 | Loss: 0.00004819
Iteration 24/1000 | Loss: 0.00004818
Iteration 25/1000 | Loss: 0.00004812
Iteration 26/1000 | Loss: 0.00004805
Iteration 27/1000 | Loss: 0.00004804
Iteration 28/1000 | Loss: 0.00004801
Iteration 29/1000 | Loss: 0.00004801
Iteration 30/1000 | Loss: 0.00004799
Iteration 31/1000 | Loss: 0.00004798
Iteration 32/1000 | Loss: 0.00004798
Iteration 33/1000 | Loss: 0.00004798
Iteration 34/1000 | Loss: 0.00004796
Iteration 35/1000 | Loss: 0.00004794
Iteration 36/1000 | Loss: 0.00004793
Iteration 37/1000 | Loss: 0.00004791
Iteration 38/1000 | Loss: 0.00004790
Iteration 39/1000 | Loss: 0.00004790
Iteration 40/1000 | Loss: 0.00004789
Iteration 41/1000 | Loss: 0.00004789
Iteration 42/1000 | Loss: 0.00004789
Iteration 43/1000 | Loss: 0.00004788
Iteration 44/1000 | Loss: 0.00004788
Iteration 45/1000 | Loss: 0.00004787
Iteration 46/1000 | Loss: 0.00004786
Iteration 47/1000 | Loss: 0.00004786
Iteration 48/1000 | Loss: 0.00004786
Iteration 49/1000 | Loss: 0.00004784
Iteration 50/1000 | Loss: 0.00004784
Iteration 51/1000 | Loss: 0.00004783
Iteration 52/1000 | Loss: 0.00004783
Iteration 53/1000 | Loss: 0.00004783
Iteration 54/1000 | Loss: 0.00004783
Iteration 55/1000 | Loss: 0.00004781
Iteration 56/1000 | Loss: 0.00004781
Iteration 57/1000 | Loss: 0.00004780
Iteration 58/1000 | Loss: 0.00004780
Iteration 59/1000 | Loss: 0.00004780
Iteration 60/1000 | Loss: 0.00004780
Iteration 61/1000 | Loss: 0.00004779
Iteration 62/1000 | Loss: 0.00004779
Iteration 63/1000 | Loss: 0.00004779
Iteration 64/1000 | Loss: 0.00004778
Iteration 65/1000 | Loss: 0.00004778
Iteration 66/1000 | Loss: 0.00004778
Iteration 67/1000 | Loss: 0.00004775
Iteration 68/1000 | Loss: 0.00004775
Iteration 69/1000 | Loss: 0.00004774
Iteration 70/1000 | Loss: 0.00004773
Iteration 71/1000 | Loss: 0.00004773
Iteration 72/1000 | Loss: 0.00004773
Iteration 73/1000 | Loss: 0.00004773
Iteration 74/1000 | Loss: 0.00004773
Iteration 75/1000 | Loss: 0.00004773
Iteration 76/1000 | Loss: 0.00004773
Iteration 77/1000 | Loss: 0.00004772
Iteration 78/1000 | Loss: 0.00004772
Iteration 79/1000 | Loss: 0.00004772
Iteration 80/1000 | Loss: 0.00004772
Iteration 81/1000 | Loss: 0.00004772
Iteration 82/1000 | Loss: 0.00004772
Iteration 83/1000 | Loss: 0.00004772
Iteration 84/1000 | Loss: 0.00004771
Iteration 85/1000 | Loss: 0.00004771
Iteration 86/1000 | Loss: 0.00004771
Iteration 87/1000 | Loss: 0.00004771
Iteration 88/1000 | Loss: 0.00004771
Iteration 89/1000 | Loss: 0.00004771
Iteration 90/1000 | Loss: 0.00004771
Iteration 91/1000 | Loss: 0.00004771
Iteration 92/1000 | Loss: 0.00004770
Iteration 93/1000 | Loss: 0.00004770
Iteration 94/1000 | Loss: 0.00004770
Iteration 95/1000 | Loss: 0.00004770
Iteration 96/1000 | Loss: 0.00004770
Iteration 97/1000 | Loss: 0.00004770
Iteration 98/1000 | Loss: 0.00004770
Iteration 99/1000 | Loss: 0.00004770
Iteration 100/1000 | Loss: 0.00004770
Iteration 101/1000 | Loss: 0.00004770
Iteration 102/1000 | Loss: 0.00004770
Iteration 103/1000 | Loss: 0.00004770
Iteration 104/1000 | Loss: 0.00004770
Iteration 105/1000 | Loss: 0.00004770
Iteration 106/1000 | Loss: 0.00004770
Iteration 107/1000 | Loss: 0.00004770
Iteration 108/1000 | Loss: 0.00004770
Iteration 109/1000 | Loss: 0.00004770
Iteration 110/1000 | Loss: 0.00004770
Iteration 111/1000 | Loss: 0.00004770
Iteration 112/1000 | Loss: 0.00004770
Iteration 113/1000 | Loss: 0.00004770
Iteration 114/1000 | Loss: 0.00004770
Iteration 115/1000 | Loss: 0.00004770
Iteration 116/1000 | Loss: 0.00004770
Iteration 117/1000 | Loss: 0.00004770
Iteration 118/1000 | Loss: 0.00004770
Iteration 119/1000 | Loss: 0.00004770
Iteration 120/1000 | Loss: 0.00004770
Iteration 121/1000 | Loss: 0.00004770
Iteration 122/1000 | Loss: 0.00004770
Iteration 123/1000 | Loss: 0.00004770
Iteration 124/1000 | Loss: 0.00004770
Iteration 125/1000 | Loss: 0.00004770
Iteration 126/1000 | Loss: 0.00004770
Iteration 127/1000 | Loss: 0.00004770
Iteration 128/1000 | Loss: 0.00004770
Iteration 129/1000 | Loss: 0.00004770
Iteration 130/1000 | Loss: 0.00004770
Iteration 131/1000 | Loss: 0.00004770
Iteration 132/1000 | Loss: 0.00004770
Iteration 133/1000 | Loss: 0.00004770
Iteration 134/1000 | Loss: 0.00004770
Iteration 135/1000 | Loss: 0.00004770
Iteration 136/1000 | Loss: 0.00004770
Iteration 137/1000 | Loss: 0.00004770
Iteration 138/1000 | Loss: 0.00004770
Iteration 139/1000 | Loss: 0.00004770
Iteration 140/1000 | Loss: 0.00004770
Iteration 141/1000 | Loss: 0.00004770
Iteration 142/1000 | Loss: 0.00004770
Iteration 143/1000 | Loss: 0.00004770
Iteration 144/1000 | Loss: 0.00004770
Iteration 145/1000 | Loss: 0.00004770
Iteration 146/1000 | Loss: 0.00004770
Iteration 147/1000 | Loss: 0.00004770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [4.770021769218147e-05, 4.770021769218147e-05, 4.770021769218147e-05, 4.770021769218147e-05, 4.770021769218147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.770021769218147e-05

Optimization complete. Final v2v error: 5.7743330001831055 mm

Highest mean error: 7.4641499519348145 mm for frame 14

Lowest mean error: 4.678286075592041 mm for frame 44

Saving results

Total time: 62.918381452560425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144139
Iteration 2/25 | Loss: 0.01144139
Iteration 3/25 | Loss: 0.00251424
Iteration 4/25 | Loss: 0.00191147
Iteration 5/25 | Loss: 0.00162758
Iteration 6/25 | Loss: 0.00154709
Iteration 7/25 | Loss: 0.00151936
Iteration 8/25 | Loss: 0.00150518
Iteration 9/25 | Loss: 0.00146855
Iteration 10/25 | Loss: 0.00145192
Iteration 11/25 | Loss: 0.00142602
Iteration 12/25 | Loss: 0.00142700
Iteration 13/25 | Loss: 0.00142710
Iteration 14/25 | Loss: 0.00141814
Iteration 15/25 | Loss: 0.00142196
Iteration 16/25 | Loss: 0.00143072
Iteration 17/25 | Loss: 0.00141676
Iteration 18/25 | Loss: 0.00141262
Iteration 19/25 | Loss: 0.00141163
Iteration 20/25 | Loss: 0.00141094
Iteration 21/25 | Loss: 0.00141056
Iteration 22/25 | Loss: 0.00141020
Iteration 23/25 | Loss: 0.00141070
Iteration 24/25 | Loss: 0.00141052
Iteration 25/25 | Loss: 0.00141030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50235450
Iteration 2/25 | Loss: 0.00169119
Iteration 3/25 | Loss: 0.00160608
Iteration 4/25 | Loss: 0.00160608
Iteration 5/25 | Loss: 0.00160608
Iteration 6/25 | Loss: 0.00160608
Iteration 7/25 | Loss: 0.00160608
Iteration 8/25 | Loss: 0.00160608
Iteration 9/25 | Loss: 0.00160608
Iteration 10/25 | Loss: 0.00160608
Iteration 11/25 | Loss: 0.00160608
Iteration 12/25 | Loss: 0.00160608
Iteration 13/25 | Loss: 0.00160608
Iteration 14/25 | Loss: 0.00160608
Iteration 15/25 | Loss: 0.00160608
Iteration 16/25 | Loss: 0.00160608
Iteration 17/25 | Loss: 0.00160608
Iteration 18/25 | Loss: 0.00160608
Iteration 19/25 | Loss: 0.00160608
Iteration 20/25 | Loss: 0.00160608
Iteration 21/25 | Loss: 0.00160608
Iteration 22/25 | Loss: 0.00160608
Iteration 23/25 | Loss: 0.00160608
Iteration 24/25 | Loss: 0.00160608
Iteration 25/25 | Loss: 0.00160608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160608
Iteration 2/1000 | Loss: 0.00008754
Iteration 3/1000 | Loss: 0.00023283
Iteration 4/1000 | Loss: 0.00005548
Iteration 5/1000 | Loss: 0.00014813
Iteration 6/1000 | Loss: 0.00008303
Iteration 7/1000 | Loss: 0.00006938
Iteration 8/1000 | Loss: 0.00004506
Iteration 9/1000 | Loss: 0.00004369
Iteration 10/1000 | Loss: 0.00089517
Iteration 11/1000 | Loss: 0.00047216
Iteration 12/1000 | Loss: 0.00006488
Iteration 13/1000 | Loss: 0.00014232
Iteration 14/1000 | Loss: 0.00006229
Iteration 15/1000 | Loss: 0.00004362
Iteration 16/1000 | Loss: 0.00007889
Iteration 17/1000 | Loss: 0.00021262
Iteration 18/1000 | Loss: 0.00006896
Iteration 19/1000 | Loss: 0.00004377
Iteration 20/1000 | Loss: 0.00005474
Iteration 21/1000 | Loss: 0.00004467
Iteration 22/1000 | Loss: 0.00006340
Iteration 23/1000 | Loss: 0.00004438
Iteration 24/1000 | Loss: 0.00008720
Iteration 25/1000 | Loss: 0.00005686
Iteration 26/1000 | Loss: 0.00003868
Iteration 27/1000 | Loss: 0.00005371
Iteration 28/1000 | Loss: 0.00003906
Iteration 29/1000 | Loss: 0.00006094
Iteration 30/1000 | Loss: 0.00003836
Iteration 31/1000 | Loss: 0.00003828
Iteration 32/1000 | Loss: 0.00004939
Iteration 33/1000 | Loss: 0.00004063
Iteration 34/1000 | Loss: 0.00003876
Iteration 35/1000 | Loss: 0.00004022
Iteration 36/1000 | Loss: 0.00003808
Iteration 37/1000 | Loss: 0.00003802
Iteration 38/1000 | Loss: 0.00003802
Iteration 39/1000 | Loss: 0.00003802
Iteration 40/1000 | Loss: 0.00003801
Iteration 41/1000 | Loss: 0.00004021
Iteration 42/1000 | Loss: 0.00004021
Iteration 43/1000 | Loss: 0.00007465
Iteration 44/1000 | Loss: 0.00007118
Iteration 45/1000 | Loss: 0.00004375
Iteration 46/1000 | Loss: 0.00003791
Iteration 47/1000 | Loss: 0.00003790
Iteration 48/1000 | Loss: 0.00003790
Iteration 49/1000 | Loss: 0.00003790
Iteration 50/1000 | Loss: 0.00003790
Iteration 51/1000 | Loss: 0.00003790
Iteration 52/1000 | Loss: 0.00003789
Iteration 53/1000 | Loss: 0.00006373
Iteration 54/1000 | Loss: 0.00005650
Iteration 55/1000 | Loss: 0.00003828
Iteration 56/1000 | Loss: 0.00003792
Iteration 57/1000 | Loss: 0.00003792
Iteration 58/1000 | Loss: 0.00003792
Iteration 59/1000 | Loss: 0.00003792
Iteration 60/1000 | Loss: 0.00003792
Iteration 61/1000 | Loss: 0.00003792
Iteration 62/1000 | Loss: 0.00003792
Iteration 63/1000 | Loss: 0.00003792
Iteration 64/1000 | Loss: 0.00003792
Iteration 65/1000 | Loss: 0.00003792
Iteration 66/1000 | Loss: 0.00003791
Iteration 67/1000 | Loss: 0.00003791
Iteration 68/1000 | Loss: 0.00003791
Iteration 69/1000 | Loss: 0.00003791
Iteration 70/1000 | Loss: 0.00003790
Iteration 71/1000 | Loss: 0.00003789
Iteration 72/1000 | Loss: 0.00003789
Iteration 73/1000 | Loss: 0.00003788
Iteration 74/1000 | Loss: 0.00003788
Iteration 75/1000 | Loss: 0.00003788
Iteration 76/1000 | Loss: 0.00003788
Iteration 77/1000 | Loss: 0.00003788
Iteration 78/1000 | Loss: 0.00003788
Iteration 79/1000 | Loss: 0.00003788
Iteration 80/1000 | Loss: 0.00003788
Iteration 81/1000 | Loss: 0.00004401
Iteration 82/1000 | Loss: 0.00003788
Iteration 83/1000 | Loss: 0.00003787
Iteration 84/1000 | Loss: 0.00003787
Iteration 85/1000 | Loss: 0.00003786
Iteration 86/1000 | Loss: 0.00003786
Iteration 87/1000 | Loss: 0.00003786
Iteration 88/1000 | Loss: 0.00004000
Iteration 89/1000 | Loss: 0.00003851
Iteration 90/1000 | Loss: 0.00003786
Iteration 91/1000 | Loss: 0.00003785
Iteration 92/1000 | Loss: 0.00003785
Iteration 93/1000 | Loss: 0.00003785
Iteration 94/1000 | Loss: 0.00003784
Iteration 95/1000 | Loss: 0.00003784
Iteration 96/1000 | Loss: 0.00003783
Iteration 97/1000 | Loss: 0.00003783
Iteration 98/1000 | Loss: 0.00003782
Iteration 99/1000 | Loss: 0.00003782
Iteration 100/1000 | Loss: 0.00003782
Iteration 101/1000 | Loss: 0.00003782
Iteration 102/1000 | Loss: 0.00003782
Iteration 103/1000 | Loss: 0.00003782
Iteration 104/1000 | Loss: 0.00003782
Iteration 105/1000 | Loss: 0.00003782
Iteration 106/1000 | Loss: 0.00003782
Iteration 107/1000 | Loss: 0.00003782
Iteration 108/1000 | Loss: 0.00003781
Iteration 109/1000 | Loss: 0.00003781
Iteration 110/1000 | Loss: 0.00003781
Iteration 111/1000 | Loss: 0.00003781
Iteration 112/1000 | Loss: 0.00003781
Iteration 113/1000 | Loss: 0.00003781
Iteration 114/1000 | Loss: 0.00003781
Iteration 115/1000 | Loss: 0.00003781
Iteration 116/1000 | Loss: 0.00003781
Iteration 117/1000 | Loss: 0.00007602
Iteration 118/1000 | Loss: 0.00004946
Iteration 119/1000 | Loss: 0.00004541
Iteration 120/1000 | Loss: 0.00003789
Iteration 121/1000 | Loss: 0.00006203
Iteration 122/1000 | Loss: 0.00004338
Iteration 123/1000 | Loss: 0.00004063
Iteration 124/1000 | Loss: 0.00004408
Iteration 125/1000 | Loss: 0.00004219
Iteration 126/1000 | Loss: 0.00003790
Iteration 127/1000 | Loss: 0.00003788
Iteration 128/1000 | Loss: 0.00003787
Iteration 129/1000 | Loss: 0.00003787
Iteration 130/1000 | Loss: 0.00003787
Iteration 131/1000 | Loss: 0.00003787
Iteration 132/1000 | Loss: 0.00003993
Iteration 133/1000 | Loss: 0.00003993
Iteration 134/1000 | Loss: 0.00003803
Iteration 135/1000 | Loss: 0.00003784
Iteration 136/1000 | Loss: 0.00003783
Iteration 137/1000 | Loss: 0.00003783
Iteration 138/1000 | Loss: 0.00003783
Iteration 139/1000 | Loss: 0.00003783
Iteration 140/1000 | Loss: 0.00003783
Iteration 141/1000 | Loss: 0.00003783
Iteration 142/1000 | Loss: 0.00003783
Iteration 143/1000 | Loss: 0.00003783
Iteration 144/1000 | Loss: 0.00003783
Iteration 145/1000 | Loss: 0.00003783
Iteration 146/1000 | Loss: 0.00003783
Iteration 147/1000 | Loss: 0.00003783
Iteration 148/1000 | Loss: 0.00003783
Iteration 149/1000 | Loss: 0.00003783
Iteration 150/1000 | Loss: 0.00003783
Iteration 151/1000 | Loss: 0.00003783
Iteration 152/1000 | Loss: 0.00003783
Iteration 153/1000 | Loss: 0.00003783
Iteration 154/1000 | Loss: 0.00003783
Iteration 155/1000 | Loss: 0.00003783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.782605563174002e-05, 3.782605563174002e-05, 3.782605563174002e-05, 3.782605563174002e-05, 3.782605563174002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.782605563174002e-05

Optimization complete. Final v2v error: 5.017787456512451 mm

Highest mean error: 12.653125762939453 mm for frame 235

Lowest mean error: 4.361106872558594 mm for frame 9

Saving results

Total time: 146.36364555358887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913097
Iteration 2/25 | Loss: 0.00181738
Iteration 3/25 | Loss: 0.00157482
Iteration 4/25 | Loss: 0.00156108
Iteration 5/25 | Loss: 0.00155865
Iteration 6/25 | Loss: 0.00155748
Iteration 7/25 | Loss: 0.00155659
Iteration 8/25 | Loss: 0.00155621
Iteration 9/25 | Loss: 0.00155621
Iteration 10/25 | Loss: 0.00155621
Iteration 11/25 | Loss: 0.00155621
Iteration 12/25 | Loss: 0.00155621
Iteration 13/25 | Loss: 0.00155621
Iteration 14/25 | Loss: 0.00155621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0015562083572149277, 0.0015562083572149277, 0.0015562083572149277, 0.0015562083572149277, 0.0015562083572149277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015562083572149277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37912214
Iteration 2/25 | Loss: 0.00239495
Iteration 3/25 | Loss: 0.00239495
Iteration 4/25 | Loss: 0.00239495
Iteration 5/25 | Loss: 0.00239495
Iteration 6/25 | Loss: 0.00239495
Iteration 7/25 | Loss: 0.00239495
Iteration 8/25 | Loss: 0.00239495
Iteration 9/25 | Loss: 0.00239495
Iteration 10/25 | Loss: 0.00239495
Iteration 11/25 | Loss: 0.00239495
Iteration 12/25 | Loss: 0.00239495
Iteration 13/25 | Loss: 0.00239495
Iteration 14/25 | Loss: 0.00239495
Iteration 15/25 | Loss: 0.00239495
Iteration 16/25 | Loss: 0.00239495
Iteration 17/25 | Loss: 0.00239495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002394949086010456, 0.002394949086010456, 0.002394949086010456, 0.002394949086010456, 0.002394949086010456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002394949086010456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239495
Iteration 2/1000 | Loss: 0.00008777
Iteration 3/1000 | Loss: 0.00006308
Iteration 4/1000 | Loss: 0.00005380
Iteration 5/1000 | Loss: 0.00004974
Iteration 6/1000 | Loss: 0.00004730
Iteration 7/1000 | Loss: 0.00004501
Iteration 8/1000 | Loss: 0.00004414
Iteration 9/1000 | Loss: 0.00004354
Iteration 10/1000 | Loss: 0.00004300
Iteration 11/1000 | Loss: 0.00004252
Iteration 12/1000 | Loss: 0.00004211
Iteration 13/1000 | Loss: 0.00004187
Iteration 14/1000 | Loss: 0.00004159
Iteration 15/1000 | Loss: 0.00004155
Iteration 16/1000 | Loss: 0.00004151
Iteration 17/1000 | Loss: 0.00004137
Iteration 18/1000 | Loss: 0.00004131
Iteration 19/1000 | Loss: 0.00004128
Iteration 20/1000 | Loss: 0.00004123
Iteration 21/1000 | Loss: 0.00004120
Iteration 22/1000 | Loss: 0.00004116
Iteration 23/1000 | Loss: 0.00004108
Iteration 24/1000 | Loss: 0.00004104
Iteration 25/1000 | Loss: 0.00004102
Iteration 26/1000 | Loss: 0.00004102
Iteration 27/1000 | Loss: 0.00004102
Iteration 28/1000 | Loss: 0.00004101
Iteration 29/1000 | Loss: 0.00004101
Iteration 30/1000 | Loss: 0.00004101
Iteration 31/1000 | Loss: 0.00004101
Iteration 32/1000 | Loss: 0.00004100
Iteration 33/1000 | Loss: 0.00004100
Iteration 34/1000 | Loss: 0.00004098
Iteration 35/1000 | Loss: 0.00004097
Iteration 36/1000 | Loss: 0.00004096
Iteration 37/1000 | Loss: 0.00004095
Iteration 38/1000 | Loss: 0.00004095
Iteration 39/1000 | Loss: 0.00004095
Iteration 40/1000 | Loss: 0.00004094
Iteration 41/1000 | Loss: 0.00004094
Iteration 42/1000 | Loss: 0.00004094
Iteration 43/1000 | Loss: 0.00004094
Iteration 44/1000 | Loss: 0.00004094
Iteration 45/1000 | Loss: 0.00004094
Iteration 46/1000 | Loss: 0.00004093
Iteration 47/1000 | Loss: 0.00004093
Iteration 48/1000 | Loss: 0.00004092
Iteration 49/1000 | Loss: 0.00004092
Iteration 50/1000 | Loss: 0.00004092
Iteration 51/1000 | Loss: 0.00004092
Iteration 52/1000 | Loss: 0.00004092
Iteration 53/1000 | Loss: 0.00004092
Iteration 54/1000 | Loss: 0.00004092
Iteration 55/1000 | Loss: 0.00004092
Iteration 56/1000 | Loss: 0.00004091
Iteration 57/1000 | Loss: 0.00004091
Iteration 58/1000 | Loss: 0.00004091
Iteration 59/1000 | Loss: 0.00004091
Iteration 60/1000 | Loss: 0.00004090
Iteration 61/1000 | Loss: 0.00004090
Iteration 62/1000 | Loss: 0.00004090
Iteration 63/1000 | Loss: 0.00004090
Iteration 64/1000 | Loss: 0.00004090
Iteration 65/1000 | Loss: 0.00004090
Iteration 66/1000 | Loss: 0.00004090
Iteration 67/1000 | Loss: 0.00004090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [4.090054426342249e-05, 4.090054426342249e-05, 4.090054426342249e-05, 4.090054426342249e-05, 4.090054426342249e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.090054426342249e-05

Optimization complete. Final v2v error: 5.236878871917725 mm

Highest mean error: 5.514286994934082 mm for frame 46

Lowest mean error: 4.7998480796813965 mm for frame 120

Saving results

Total time: 40.660751819610596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401148
Iteration 2/25 | Loss: 0.00172870
Iteration 3/25 | Loss: 0.00160326
Iteration 4/25 | Loss: 0.00158254
Iteration 5/25 | Loss: 0.00157764
Iteration 6/25 | Loss: 0.00157654
Iteration 7/25 | Loss: 0.00157654
Iteration 8/25 | Loss: 0.00157654
Iteration 9/25 | Loss: 0.00157654
Iteration 10/25 | Loss: 0.00157654
Iteration 11/25 | Loss: 0.00157654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001576538779772818, 0.001576538779772818, 0.001576538779772818, 0.001576538779772818, 0.001576538779772818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001576538779772818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50842321
Iteration 2/25 | Loss: 0.00226189
Iteration 3/25 | Loss: 0.00226189
Iteration 4/25 | Loss: 0.00226189
Iteration 5/25 | Loss: 0.00226189
Iteration 6/25 | Loss: 0.00226188
Iteration 7/25 | Loss: 0.00226188
Iteration 8/25 | Loss: 0.00226188
Iteration 9/25 | Loss: 0.00226188
Iteration 10/25 | Loss: 0.00226188
Iteration 11/25 | Loss: 0.00226188
Iteration 12/25 | Loss: 0.00226188
Iteration 13/25 | Loss: 0.00226188
Iteration 14/25 | Loss: 0.00226188
Iteration 15/25 | Loss: 0.00226188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022618845105171204, 0.0022618845105171204, 0.0022618845105171204, 0.0022618845105171204, 0.0022618845105171204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022618845105171204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226188
Iteration 2/1000 | Loss: 0.00006100
Iteration 3/1000 | Loss: 0.00004514
Iteration 4/1000 | Loss: 0.00003892
Iteration 5/1000 | Loss: 0.00003594
Iteration 6/1000 | Loss: 0.00003370
Iteration 7/1000 | Loss: 0.00003250
Iteration 8/1000 | Loss: 0.00003160
Iteration 9/1000 | Loss: 0.00003114
Iteration 10/1000 | Loss: 0.00003072
Iteration 11/1000 | Loss: 0.00003044
Iteration 12/1000 | Loss: 0.00003026
Iteration 13/1000 | Loss: 0.00003024
Iteration 14/1000 | Loss: 0.00003021
Iteration 15/1000 | Loss: 0.00003019
Iteration 16/1000 | Loss: 0.00003018
Iteration 17/1000 | Loss: 0.00003016
Iteration 18/1000 | Loss: 0.00003010
Iteration 19/1000 | Loss: 0.00003006
Iteration 20/1000 | Loss: 0.00003006
Iteration 21/1000 | Loss: 0.00003005
Iteration 22/1000 | Loss: 0.00003002
Iteration 23/1000 | Loss: 0.00003001
Iteration 24/1000 | Loss: 0.00003000
Iteration 25/1000 | Loss: 0.00002999
Iteration 26/1000 | Loss: 0.00002998
Iteration 27/1000 | Loss: 0.00002996
Iteration 28/1000 | Loss: 0.00002996
Iteration 29/1000 | Loss: 0.00002995
Iteration 30/1000 | Loss: 0.00002993
Iteration 31/1000 | Loss: 0.00002993
Iteration 32/1000 | Loss: 0.00002992
Iteration 33/1000 | Loss: 0.00002992
Iteration 34/1000 | Loss: 0.00002992
Iteration 35/1000 | Loss: 0.00002991
Iteration 36/1000 | Loss: 0.00002990
Iteration 37/1000 | Loss: 0.00002990
Iteration 38/1000 | Loss: 0.00002989
Iteration 39/1000 | Loss: 0.00002989
Iteration 40/1000 | Loss: 0.00002989
Iteration 41/1000 | Loss: 0.00002988
Iteration 42/1000 | Loss: 0.00002988
Iteration 43/1000 | Loss: 0.00002988
Iteration 44/1000 | Loss: 0.00002987
Iteration 45/1000 | Loss: 0.00002987
Iteration 46/1000 | Loss: 0.00002987
Iteration 47/1000 | Loss: 0.00002987
Iteration 48/1000 | Loss: 0.00002987
Iteration 49/1000 | Loss: 0.00002987
Iteration 50/1000 | Loss: 0.00002986
Iteration 51/1000 | Loss: 0.00002985
Iteration 52/1000 | Loss: 0.00002985
Iteration 53/1000 | Loss: 0.00002984
Iteration 54/1000 | Loss: 0.00002984
Iteration 55/1000 | Loss: 0.00002984
Iteration 56/1000 | Loss: 0.00002984
Iteration 57/1000 | Loss: 0.00002984
Iteration 58/1000 | Loss: 0.00002984
Iteration 59/1000 | Loss: 0.00002984
Iteration 60/1000 | Loss: 0.00002984
Iteration 61/1000 | Loss: 0.00002983
Iteration 62/1000 | Loss: 0.00002983
Iteration 63/1000 | Loss: 0.00002983
Iteration 64/1000 | Loss: 0.00002982
Iteration 65/1000 | Loss: 0.00002982
Iteration 66/1000 | Loss: 0.00002981
Iteration 67/1000 | Loss: 0.00002981
Iteration 68/1000 | Loss: 0.00002981
Iteration 69/1000 | Loss: 0.00002981
Iteration 70/1000 | Loss: 0.00002981
Iteration 71/1000 | Loss: 0.00002981
Iteration 72/1000 | Loss: 0.00002981
Iteration 73/1000 | Loss: 0.00002981
Iteration 74/1000 | Loss: 0.00002981
Iteration 75/1000 | Loss: 0.00002981
Iteration 76/1000 | Loss: 0.00002981
Iteration 77/1000 | Loss: 0.00002981
Iteration 78/1000 | Loss: 0.00002980
Iteration 79/1000 | Loss: 0.00002980
Iteration 80/1000 | Loss: 0.00002980
Iteration 81/1000 | Loss: 0.00002980
Iteration 82/1000 | Loss: 0.00002980
Iteration 83/1000 | Loss: 0.00002980
Iteration 84/1000 | Loss: 0.00002980
Iteration 85/1000 | Loss: 0.00002979
Iteration 86/1000 | Loss: 0.00002979
Iteration 87/1000 | Loss: 0.00002978
Iteration 88/1000 | Loss: 0.00002978
Iteration 89/1000 | Loss: 0.00002978
Iteration 90/1000 | Loss: 0.00002978
Iteration 91/1000 | Loss: 0.00002978
Iteration 92/1000 | Loss: 0.00002978
Iteration 93/1000 | Loss: 0.00002977
Iteration 94/1000 | Loss: 0.00002977
Iteration 95/1000 | Loss: 0.00002977
Iteration 96/1000 | Loss: 0.00002977
Iteration 97/1000 | Loss: 0.00002976
Iteration 98/1000 | Loss: 0.00002976
Iteration 99/1000 | Loss: 0.00002976
Iteration 100/1000 | Loss: 0.00002976
Iteration 101/1000 | Loss: 0.00002976
Iteration 102/1000 | Loss: 0.00002976
Iteration 103/1000 | Loss: 0.00002976
Iteration 104/1000 | Loss: 0.00002976
Iteration 105/1000 | Loss: 0.00002975
Iteration 106/1000 | Loss: 0.00002975
Iteration 107/1000 | Loss: 0.00002975
Iteration 108/1000 | Loss: 0.00002975
Iteration 109/1000 | Loss: 0.00002974
Iteration 110/1000 | Loss: 0.00002974
Iteration 111/1000 | Loss: 0.00002974
Iteration 112/1000 | Loss: 0.00002974
Iteration 113/1000 | Loss: 0.00002974
Iteration 114/1000 | Loss: 0.00002974
Iteration 115/1000 | Loss: 0.00002974
Iteration 116/1000 | Loss: 0.00002973
Iteration 117/1000 | Loss: 0.00002973
Iteration 118/1000 | Loss: 0.00002973
Iteration 119/1000 | Loss: 0.00002973
Iteration 120/1000 | Loss: 0.00002973
Iteration 121/1000 | Loss: 0.00002973
Iteration 122/1000 | Loss: 0.00002973
Iteration 123/1000 | Loss: 0.00002973
Iteration 124/1000 | Loss: 0.00002973
Iteration 125/1000 | Loss: 0.00002973
Iteration 126/1000 | Loss: 0.00002973
Iteration 127/1000 | Loss: 0.00002973
Iteration 128/1000 | Loss: 0.00002973
Iteration 129/1000 | Loss: 0.00002973
Iteration 130/1000 | Loss: 0.00002973
Iteration 131/1000 | Loss: 0.00002973
Iteration 132/1000 | Loss: 0.00002973
Iteration 133/1000 | Loss: 0.00002973
Iteration 134/1000 | Loss: 0.00002973
Iteration 135/1000 | Loss: 0.00002973
Iteration 136/1000 | Loss: 0.00002973
Iteration 137/1000 | Loss: 0.00002973
Iteration 138/1000 | Loss: 0.00002973
Iteration 139/1000 | Loss: 0.00002973
Iteration 140/1000 | Loss: 0.00002973
Iteration 141/1000 | Loss: 0.00002973
Iteration 142/1000 | Loss: 0.00002973
Iteration 143/1000 | Loss: 0.00002973
Iteration 144/1000 | Loss: 0.00002973
Iteration 145/1000 | Loss: 0.00002973
Iteration 146/1000 | Loss: 0.00002973
Iteration 147/1000 | Loss: 0.00002973
Iteration 148/1000 | Loss: 0.00002973
Iteration 149/1000 | Loss: 0.00002973
Iteration 150/1000 | Loss: 0.00002973
Iteration 151/1000 | Loss: 0.00002973
Iteration 152/1000 | Loss: 0.00002973
Iteration 153/1000 | Loss: 0.00002973
Iteration 154/1000 | Loss: 0.00002973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.9725886633968912e-05, 2.9725886633968912e-05, 2.9725886633968912e-05, 2.9725886633968912e-05, 2.9725886633968912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9725886633968912e-05

Optimization complete. Final v2v error: 4.725412845611572 mm

Highest mean error: 5.395816802978516 mm for frame 248

Lowest mean error: 4.304025173187256 mm for frame 122

Saving results

Total time: 41.663330078125
