Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=223, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12488-12543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837061
Iteration 2/25 | Loss: 0.00102477
Iteration 3/25 | Loss: 0.00087207
Iteration 4/25 | Loss: 0.00081969
Iteration 5/25 | Loss: 0.00081024
Iteration 6/25 | Loss: 0.00080838
Iteration 7/25 | Loss: 0.00080792
Iteration 8/25 | Loss: 0.00080792
Iteration 9/25 | Loss: 0.00080792
Iteration 10/25 | Loss: 0.00080792
Iteration 11/25 | Loss: 0.00080792
Iteration 12/25 | Loss: 0.00080792
Iteration 13/25 | Loss: 0.00080792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008079216349869967, 0.0008079216349869967, 0.0008079216349869967, 0.0008079216349869967, 0.0008079216349869967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008079216349869967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31215107
Iteration 2/25 | Loss: 0.00051708
Iteration 3/25 | Loss: 0.00051708
Iteration 4/25 | Loss: 0.00051708
Iteration 5/25 | Loss: 0.00051708
Iteration 6/25 | Loss: 0.00051708
Iteration 7/25 | Loss: 0.00051708
Iteration 8/25 | Loss: 0.00051708
Iteration 9/25 | Loss: 0.00051708
Iteration 10/25 | Loss: 0.00051708
Iteration 11/25 | Loss: 0.00051708
Iteration 12/25 | Loss: 0.00051708
Iteration 13/25 | Loss: 0.00051708
Iteration 14/25 | Loss: 0.00051708
Iteration 15/25 | Loss: 0.00051708
Iteration 16/25 | Loss: 0.00051708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005170769873075187, 0.0005170769873075187, 0.0005170769873075187, 0.0005170769873075187, 0.0005170769873075187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005170769873075187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051708
Iteration 2/1000 | Loss: 0.00004346
Iteration 3/1000 | Loss: 0.00003347
Iteration 4/1000 | Loss: 0.00002992
Iteration 5/1000 | Loss: 0.00002888
Iteration 6/1000 | Loss: 0.00002777
Iteration 7/1000 | Loss: 0.00002728
Iteration 8/1000 | Loss: 0.00002669
Iteration 9/1000 | Loss: 0.00002632
Iteration 10/1000 | Loss: 0.00002597
Iteration 11/1000 | Loss: 0.00002577
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00002548
Iteration 14/1000 | Loss: 0.00002527
Iteration 15/1000 | Loss: 0.00002518
Iteration 16/1000 | Loss: 0.00002511
Iteration 17/1000 | Loss: 0.00002509
Iteration 18/1000 | Loss: 0.00002509
Iteration 19/1000 | Loss: 0.00002509
Iteration 20/1000 | Loss: 0.00002509
Iteration 21/1000 | Loss: 0.00002509
Iteration 22/1000 | Loss: 0.00002509
Iteration 23/1000 | Loss: 0.00002509
Iteration 24/1000 | Loss: 0.00002509
Iteration 25/1000 | Loss: 0.00002509
Iteration 26/1000 | Loss: 0.00002509
Iteration 27/1000 | Loss: 0.00002508
Iteration 28/1000 | Loss: 0.00002508
Iteration 29/1000 | Loss: 0.00002508
Iteration 30/1000 | Loss: 0.00002508
Iteration 31/1000 | Loss: 0.00002507
Iteration 32/1000 | Loss: 0.00002505
Iteration 33/1000 | Loss: 0.00002505
Iteration 34/1000 | Loss: 0.00002505
Iteration 35/1000 | Loss: 0.00002504
Iteration 36/1000 | Loss: 0.00002504
Iteration 37/1000 | Loss: 0.00002504
Iteration 38/1000 | Loss: 0.00002504
Iteration 39/1000 | Loss: 0.00002504
Iteration 40/1000 | Loss: 0.00002504
Iteration 41/1000 | Loss: 0.00002503
Iteration 42/1000 | Loss: 0.00002501
Iteration 43/1000 | Loss: 0.00002500
Iteration 44/1000 | Loss: 0.00002500
Iteration 45/1000 | Loss: 0.00002500
Iteration 46/1000 | Loss: 0.00002499
Iteration 47/1000 | Loss: 0.00002499
Iteration 48/1000 | Loss: 0.00002499
Iteration 49/1000 | Loss: 0.00002499
Iteration 50/1000 | Loss: 0.00002499
Iteration 51/1000 | Loss: 0.00002498
Iteration 52/1000 | Loss: 0.00002498
Iteration 53/1000 | Loss: 0.00002498
Iteration 54/1000 | Loss: 0.00002498
Iteration 55/1000 | Loss: 0.00002498
Iteration 56/1000 | Loss: 0.00002498
Iteration 57/1000 | Loss: 0.00002498
Iteration 58/1000 | Loss: 0.00002498
Iteration 59/1000 | Loss: 0.00002498
Iteration 60/1000 | Loss: 0.00002498
Iteration 61/1000 | Loss: 0.00002498
Iteration 62/1000 | Loss: 0.00002497
Iteration 63/1000 | Loss: 0.00002497
Iteration 64/1000 | Loss: 0.00002496
Iteration 65/1000 | Loss: 0.00002496
Iteration 66/1000 | Loss: 0.00002496
Iteration 67/1000 | Loss: 0.00002496
Iteration 68/1000 | Loss: 0.00002496
Iteration 69/1000 | Loss: 0.00002496
Iteration 70/1000 | Loss: 0.00002496
Iteration 71/1000 | Loss: 0.00002496
Iteration 72/1000 | Loss: 0.00002495
Iteration 73/1000 | Loss: 0.00002495
Iteration 74/1000 | Loss: 0.00002495
Iteration 75/1000 | Loss: 0.00002495
Iteration 76/1000 | Loss: 0.00002495
Iteration 77/1000 | Loss: 0.00002494
Iteration 78/1000 | Loss: 0.00002494
Iteration 79/1000 | Loss: 0.00002494
Iteration 80/1000 | Loss: 0.00002493
Iteration 81/1000 | Loss: 0.00002493
Iteration 82/1000 | Loss: 0.00002493
Iteration 83/1000 | Loss: 0.00002493
Iteration 84/1000 | Loss: 0.00002492
Iteration 85/1000 | Loss: 0.00002492
Iteration 86/1000 | Loss: 0.00002492
Iteration 87/1000 | Loss: 0.00002492
Iteration 88/1000 | Loss: 0.00002492
Iteration 89/1000 | Loss: 0.00002491
Iteration 90/1000 | Loss: 0.00002491
Iteration 91/1000 | Loss: 0.00002491
Iteration 92/1000 | Loss: 0.00002491
Iteration 93/1000 | Loss: 0.00002491
Iteration 94/1000 | Loss: 0.00002490
Iteration 95/1000 | Loss: 0.00002490
Iteration 96/1000 | Loss: 0.00002490
Iteration 97/1000 | Loss: 0.00002489
Iteration 98/1000 | Loss: 0.00002489
Iteration 99/1000 | Loss: 0.00002489
Iteration 100/1000 | Loss: 0.00002489
Iteration 101/1000 | Loss: 0.00002489
Iteration 102/1000 | Loss: 0.00002488
Iteration 103/1000 | Loss: 0.00002488
Iteration 104/1000 | Loss: 0.00002488
Iteration 105/1000 | Loss: 0.00002488
Iteration 106/1000 | Loss: 0.00002488
Iteration 107/1000 | Loss: 0.00002488
Iteration 108/1000 | Loss: 0.00002488
Iteration 109/1000 | Loss: 0.00002488
Iteration 110/1000 | Loss: 0.00002488
Iteration 111/1000 | Loss: 0.00002488
Iteration 112/1000 | Loss: 0.00002488
Iteration 113/1000 | Loss: 0.00002488
Iteration 114/1000 | Loss: 0.00002488
Iteration 115/1000 | Loss: 0.00002488
Iteration 116/1000 | Loss: 0.00002487
Iteration 117/1000 | Loss: 0.00002487
Iteration 118/1000 | Loss: 0.00002487
Iteration 119/1000 | Loss: 0.00002486
Iteration 120/1000 | Loss: 0.00002486
Iteration 121/1000 | Loss: 0.00002485
Iteration 122/1000 | Loss: 0.00002485
Iteration 123/1000 | Loss: 0.00002485
Iteration 124/1000 | Loss: 0.00002484
Iteration 125/1000 | Loss: 0.00002484
Iteration 126/1000 | Loss: 0.00002484
Iteration 127/1000 | Loss: 0.00002483
Iteration 128/1000 | Loss: 0.00002483
Iteration 129/1000 | Loss: 0.00002483
Iteration 130/1000 | Loss: 0.00002483
Iteration 131/1000 | Loss: 0.00002483
Iteration 132/1000 | Loss: 0.00002483
Iteration 133/1000 | Loss: 0.00002482
Iteration 134/1000 | Loss: 0.00002482
Iteration 135/1000 | Loss: 0.00002482
Iteration 136/1000 | Loss: 0.00002482
Iteration 137/1000 | Loss: 0.00002482
Iteration 138/1000 | Loss: 0.00002482
Iteration 139/1000 | Loss: 0.00002482
Iteration 140/1000 | Loss: 0.00002482
Iteration 141/1000 | Loss: 0.00002481
Iteration 142/1000 | Loss: 0.00002481
Iteration 143/1000 | Loss: 0.00002481
Iteration 144/1000 | Loss: 0.00002481
Iteration 145/1000 | Loss: 0.00002481
Iteration 146/1000 | Loss: 0.00002481
Iteration 147/1000 | Loss: 0.00002481
Iteration 148/1000 | Loss: 0.00002481
Iteration 149/1000 | Loss: 0.00002481
Iteration 150/1000 | Loss: 0.00002481
Iteration 151/1000 | Loss: 0.00002481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.480916737113148e-05, 2.480916737113148e-05, 2.480916737113148e-05, 2.480916737113148e-05, 2.480916737113148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.480916737113148e-05

Optimization complete. Final v2v error: 4.183484077453613 mm

Highest mean error: 4.559497356414795 mm for frame 0

Lowest mean error: 4.0074262619018555 mm for frame 188

Saving results

Total time: 41.71302032470703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093945
Iteration 2/25 | Loss: 0.00122295
Iteration 3/25 | Loss: 0.00079124
Iteration 4/25 | Loss: 0.00077593
Iteration 5/25 | Loss: 0.00074299
Iteration 6/25 | Loss: 0.00074631
Iteration 7/25 | Loss: 0.00073314
Iteration 8/25 | Loss: 0.00072705
Iteration 9/25 | Loss: 0.00072683
Iteration 10/25 | Loss: 0.00072678
Iteration 11/25 | Loss: 0.00072677
Iteration 12/25 | Loss: 0.00072677
Iteration 13/25 | Loss: 0.00072677
Iteration 14/25 | Loss: 0.00072677
Iteration 15/25 | Loss: 0.00072677
Iteration 16/25 | Loss: 0.00072677
Iteration 17/25 | Loss: 0.00072677
Iteration 18/25 | Loss: 0.00072677
Iteration 19/25 | Loss: 0.00072677
Iteration 20/25 | Loss: 0.00072677
Iteration 21/25 | Loss: 0.00072676
Iteration 22/25 | Loss: 0.00072676
Iteration 23/25 | Loss: 0.00072676
Iteration 24/25 | Loss: 0.00072676
Iteration 25/25 | Loss: 0.00072676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63717103
Iteration 2/25 | Loss: 0.00047190
Iteration 3/25 | Loss: 0.00047190
Iteration 4/25 | Loss: 0.00047190
Iteration 5/25 | Loss: 0.00047190
Iteration 6/25 | Loss: 0.00047190
Iteration 7/25 | Loss: 0.00047190
Iteration 8/25 | Loss: 0.00047190
Iteration 9/25 | Loss: 0.00047190
Iteration 10/25 | Loss: 0.00047190
Iteration 11/25 | Loss: 0.00047190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0004718998970929533, 0.0004718998970929533, 0.0004718998970929533, 0.0004718998970929533, 0.0004718998970929533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004718998970929533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047190
Iteration 2/1000 | Loss: 0.00002656
Iteration 3/1000 | Loss: 0.00002105
Iteration 4/1000 | Loss: 0.00002009
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00001894
Iteration 7/1000 | Loss: 0.00001845
Iteration 8/1000 | Loss: 0.00001816
Iteration 9/1000 | Loss: 0.00001806
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001771
Iteration 12/1000 | Loss: 0.00001769
Iteration 13/1000 | Loss: 0.00001768
Iteration 14/1000 | Loss: 0.00001768
Iteration 15/1000 | Loss: 0.00001766
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001766
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001766
Iteration 21/1000 | Loss: 0.00001766
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001766
Iteration 24/1000 | Loss: 0.00001766
Iteration 25/1000 | Loss: 0.00001766
Iteration 26/1000 | Loss: 0.00001765
Iteration 27/1000 | Loss: 0.00001765
Iteration 28/1000 | Loss: 0.00001765
Iteration 29/1000 | Loss: 0.00001765
Iteration 30/1000 | Loss: 0.00001764
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001761
Iteration 36/1000 | Loss: 0.00001761
Iteration 37/1000 | Loss: 0.00001761
Iteration 38/1000 | Loss: 0.00001761
Iteration 39/1000 | Loss: 0.00001761
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001760
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001759
Iteration 48/1000 | Loss: 0.00001758
Iteration 49/1000 | Loss: 0.00001758
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001758
Iteration 52/1000 | Loss: 0.00001758
Iteration 53/1000 | Loss: 0.00001758
Iteration 54/1000 | Loss: 0.00001758
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001754
Iteration 76/1000 | Loss: 0.00001754
Iteration 77/1000 | Loss: 0.00001754
Iteration 78/1000 | Loss: 0.00001754
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001753
Iteration 82/1000 | Loss: 0.00001753
Iteration 83/1000 | Loss: 0.00001753
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001753
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001751
Iteration 110/1000 | Loss: 0.00001751
Iteration 111/1000 | Loss: 0.00001751
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001750
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00009924
Iteration 128/1000 | Loss: 0.00001761
Iteration 129/1000 | Loss: 0.00001750
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001745
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Iteration 144/1000 | Loss: 0.00001745
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001745
Iteration 147/1000 | Loss: 0.00001745
Iteration 148/1000 | Loss: 0.00001745
Iteration 149/1000 | Loss: 0.00001745
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001745
Iteration 159/1000 | Loss: 0.00001745
Iteration 160/1000 | Loss: 0.00001745
Iteration 161/1000 | Loss: 0.00001745
Iteration 162/1000 | Loss: 0.00001745
Iteration 163/1000 | Loss: 0.00001745
Iteration 164/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.744973997119814e-05, 1.744973997119814e-05, 1.744973997119814e-05, 1.744973997119814e-05, 1.744973997119814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.744973997119814e-05

Optimization complete. Final v2v error: 3.5331757068634033 mm

Highest mean error: 3.919252634048462 mm for frame 134

Lowest mean error: 3.2584152221679688 mm for frame 47

Saving results

Total time: 42.55417585372925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720705
Iteration 2/25 | Loss: 0.00099378
Iteration 3/25 | Loss: 0.00082709
Iteration 4/25 | Loss: 0.00078351
Iteration 5/25 | Loss: 0.00076099
Iteration 6/25 | Loss: 0.00075756
Iteration 7/25 | Loss: 0.00075826
Iteration 8/25 | Loss: 0.00075508
Iteration 9/25 | Loss: 0.00075479
Iteration 10/25 | Loss: 0.00075459
Iteration 11/25 | Loss: 0.00075435
Iteration 12/25 | Loss: 0.00075400
Iteration 13/25 | Loss: 0.00075663
Iteration 14/25 | Loss: 0.00075365
Iteration 15/25 | Loss: 0.00075361
Iteration 16/25 | Loss: 0.00075361
Iteration 17/25 | Loss: 0.00075361
Iteration 18/25 | Loss: 0.00075361
Iteration 19/25 | Loss: 0.00075361
Iteration 20/25 | Loss: 0.00075361
Iteration 21/25 | Loss: 0.00075361
Iteration 22/25 | Loss: 0.00075361
Iteration 23/25 | Loss: 0.00075361
Iteration 24/25 | Loss: 0.00075360
Iteration 25/25 | Loss: 0.00075360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69858098
Iteration 2/25 | Loss: 0.00045327
Iteration 3/25 | Loss: 0.00045327
Iteration 4/25 | Loss: 0.00045327
Iteration 5/25 | Loss: 0.00045327
Iteration 6/25 | Loss: 0.00045327
Iteration 7/25 | Loss: 0.00045327
Iteration 8/25 | Loss: 0.00045327
Iteration 9/25 | Loss: 0.00045327
Iteration 10/25 | Loss: 0.00045327
Iteration 11/25 | Loss: 0.00045327
Iteration 12/25 | Loss: 0.00045327
Iteration 13/25 | Loss: 0.00045327
Iteration 14/25 | Loss: 0.00045327
Iteration 15/25 | Loss: 0.00045327
Iteration 16/25 | Loss: 0.00045327
Iteration 17/25 | Loss: 0.00045327
Iteration 18/25 | Loss: 0.00045327
Iteration 19/25 | Loss: 0.00045327
Iteration 20/25 | Loss: 0.00045327
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004532662278506905, 0.0004532662278506905, 0.0004532662278506905, 0.0004532662278506905, 0.0004532662278506905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004532662278506905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045327
Iteration 2/1000 | Loss: 0.00002619
Iteration 3/1000 | Loss: 0.00002033
Iteration 4/1000 | Loss: 0.00001880
Iteration 5/1000 | Loss: 0.00040656
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001626
Iteration 9/1000 | Loss: 0.00001564
Iteration 10/1000 | Loss: 0.00001536
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001509
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001504
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001503
Iteration 21/1000 | Loss: 0.00001503
Iteration 22/1000 | Loss: 0.00001502
Iteration 23/1000 | Loss: 0.00001492
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001489
Iteration 26/1000 | Loss: 0.00001489
Iteration 27/1000 | Loss: 0.00001489
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001488
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001482
Iteration 37/1000 | Loss: 0.00001482
Iteration 38/1000 | Loss: 0.00001480
Iteration 39/1000 | Loss: 0.00001480
Iteration 40/1000 | Loss: 0.00001480
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001477
Iteration 45/1000 | Loss: 0.00001477
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001477
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00001477
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001476
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001475
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001474
Iteration 57/1000 | Loss: 0.00001474
Iteration 58/1000 | Loss: 0.00001473
Iteration 59/1000 | Loss: 0.00001473
Iteration 60/1000 | Loss: 0.00001473
Iteration 61/1000 | Loss: 0.00001473
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001472
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001472
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001471
Iteration 68/1000 | Loss: 0.00001471
Iteration 69/1000 | Loss: 0.00001471
Iteration 70/1000 | Loss: 0.00001471
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001471
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001471
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001470
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001469
Iteration 86/1000 | Loss: 0.00001469
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001469
Iteration 91/1000 | Loss: 0.00001469
Iteration 92/1000 | Loss: 0.00001469
Iteration 93/1000 | Loss: 0.00001469
Iteration 94/1000 | Loss: 0.00001469
Iteration 95/1000 | Loss: 0.00001469
Iteration 96/1000 | Loss: 0.00001469
Iteration 97/1000 | Loss: 0.00001468
Iteration 98/1000 | Loss: 0.00001468
Iteration 99/1000 | Loss: 0.00001468
Iteration 100/1000 | Loss: 0.00001468
Iteration 101/1000 | Loss: 0.00001468
Iteration 102/1000 | Loss: 0.00001468
Iteration 103/1000 | Loss: 0.00001468
Iteration 104/1000 | Loss: 0.00001468
Iteration 105/1000 | Loss: 0.00001468
Iteration 106/1000 | Loss: 0.00001468
Iteration 107/1000 | Loss: 0.00001467
Iteration 108/1000 | Loss: 0.00001467
Iteration 109/1000 | Loss: 0.00001467
Iteration 110/1000 | Loss: 0.00001467
Iteration 111/1000 | Loss: 0.00001467
Iteration 112/1000 | Loss: 0.00001467
Iteration 113/1000 | Loss: 0.00001467
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001466
Iteration 116/1000 | Loss: 0.00001466
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001465
Iteration 120/1000 | Loss: 0.00001465
Iteration 121/1000 | Loss: 0.00001465
Iteration 122/1000 | Loss: 0.00001465
Iteration 123/1000 | Loss: 0.00001465
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001464
Iteration 127/1000 | Loss: 0.00001464
Iteration 128/1000 | Loss: 0.00001464
Iteration 129/1000 | Loss: 0.00001464
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001463
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001462
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001462
Iteration 141/1000 | Loss: 0.00001462
Iteration 142/1000 | Loss: 0.00001461
Iteration 143/1000 | Loss: 0.00001461
Iteration 144/1000 | Loss: 0.00001461
Iteration 145/1000 | Loss: 0.00001461
Iteration 146/1000 | Loss: 0.00001461
Iteration 147/1000 | Loss: 0.00001461
Iteration 148/1000 | Loss: 0.00001461
Iteration 149/1000 | Loss: 0.00001461
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001461
Iteration 152/1000 | Loss: 0.00001461
Iteration 153/1000 | Loss: 0.00001461
Iteration 154/1000 | Loss: 0.00001461
Iteration 155/1000 | Loss: 0.00001460
Iteration 156/1000 | Loss: 0.00001460
Iteration 157/1000 | Loss: 0.00001460
Iteration 158/1000 | Loss: 0.00001460
Iteration 159/1000 | Loss: 0.00001460
Iteration 160/1000 | Loss: 0.00001460
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001460
Iteration 164/1000 | Loss: 0.00001460
Iteration 165/1000 | Loss: 0.00001460
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001460
Iteration 176/1000 | Loss: 0.00001460
Iteration 177/1000 | Loss: 0.00001460
Iteration 178/1000 | Loss: 0.00001460
Iteration 179/1000 | Loss: 0.00001460
Iteration 180/1000 | Loss: 0.00001460
Iteration 181/1000 | Loss: 0.00001460
Iteration 182/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.4597309927921742e-05, 1.4597309927921742e-05, 1.4597309927921742e-05, 1.4597309927921742e-05, 1.4597309927921742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4597309927921742e-05

Optimization complete. Final v2v error: 3.2440972328186035 mm

Highest mean error: 3.7858693599700928 mm for frame 140

Lowest mean error: 3.0205154418945312 mm for frame 237

Saving results

Total time: 63.69157123565674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113937
Iteration 2/25 | Loss: 0.00342979
Iteration 3/25 | Loss: 0.00193565
Iteration 4/25 | Loss: 0.00186604
Iteration 5/25 | Loss: 0.00142022
Iteration 6/25 | Loss: 0.00135680
Iteration 7/25 | Loss: 0.00113409
Iteration 8/25 | Loss: 0.00099164
Iteration 9/25 | Loss: 0.00094219
Iteration 10/25 | Loss: 0.00089057
Iteration 11/25 | Loss: 0.00084955
Iteration 12/25 | Loss: 0.00083021
Iteration 13/25 | Loss: 0.00082373
Iteration 14/25 | Loss: 0.00082023
Iteration 15/25 | Loss: 0.00081978
Iteration 16/25 | Loss: 0.00081906
Iteration 17/25 | Loss: 0.00081892
Iteration 18/25 | Loss: 0.00081883
Iteration 19/25 | Loss: 0.00081938
Iteration 20/25 | Loss: 0.00081911
Iteration 21/25 | Loss: 0.00081938
Iteration 22/25 | Loss: 0.00081938
Iteration 23/25 | Loss: 0.00081938
Iteration 24/25 | Loss: 0.00081918
Iteration 25/25 | Loss: 0.00081876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57686210
Iteration 2/25 | Loss: 0.00046662
Iteration 3/25 | Loss: 0.00046661
Iteration 4/25 | Loss: 0.00046661
Iteration 5/25 | Loss: 0.00046661
Iteration 6/25 | Loss: 0.00046661
Iteration 7/25 | Loss: 0.00046661
Iteration 8/25 | Loss: 0.00046661
Iteration 9/25 | Loss: 0.00046661
Iteration 10/25 | Loss: 0.00046661
Iteration 11/25 | Loss: 0.00046661
Iteration 12/25 | Loss: 0.00046661
Iteration 13/25 | Loss: 0.00046661
Iteration 14/25 | Loss: 0.00046661
Iteration 15/25 | Loss: 0.00046661
Iteration 16/25 | Loss: 0.00046661
Iteration 17/25 | Loss: 0.00046661
Iteration 18/25 | Loss: 0.00046661
Iteration 19/25 | Loss: 0.00046661
Iteration 20/25 | Loss: 0.00046661
Iteration 21/25 | Loss: 0.00046661
Iteration 22/25 | Loss: 0.00046661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00046660611405968666, 0.00046660611405968666, 0.00046660611405968666, 0.00046660611405968666, 0.00046660611405968666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046660611405968666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046661
Iteration 2/1000 | Loss: 0.00004687
Iteration 3/1000 | Loss: 0.00003054
Iteration 4/1000 | Loss: 0.00002716
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002484
Iteration 7/1000 | Loss: 0.00002422
Iteration 8/1000 | Loss: 0.00002366
Iteration 9/1000 | Loss: 0.00053014
Iteration 10/1000 | Loss: 0.00002352
Iteration 11/1000 | Loss: 0.00002240
Iteration 12/1000 | Loss: 0.00002150
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002080
Iteration 15/1000 | Loss: 0.00002064
Iteration 16/1000 | Loss: 0.00002057
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002056
Iteration 19/1000 | Loss: 0.00002056
Iteration 20/1000 | Loss: 0.00002055
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002054
Iteration 23/1000 | Loss: 0.00002054
Iteration 24/1000 | Loss: 0.00002054
Iteration 25/1000 | Loss: 0.00002054
Iteration 26/1000 | Loss: 0.00002050
Iteration 27/1000 | Loss: 0.00002047
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002040
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002035
Iteration 32/1000 | Loss: 0.00002032
Iteration 33/1000 | Loss: 0.00002029
Iteration 34/1000 | Loss: 0.00002028
Iteration 35/1000 | Loss: 0.00002027
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00002024
Iteration 38/1000 | Loss: 0.00002024
Iteration 39/1000 | Loss: 0.00002024
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00002021
Iteration 42/1000 | Loss: 0.00002021
Iteration 43/1000 | Loss: 0.00002021
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002020
Iteration 46/1000 | Loss: 0.00002020
Iteration 47/1000 | Loss: 0.00002020
Iteration 48/1000 | Loss: 0.00002019
Iteration 49/1000 | Loss: 0.00002019
Iteration 50/1000 | Loss: 0.00002019
Iteration 51/1000 | Loss: 0.00002018
Iteration 52/1000 | Loss: 0.00002018
Iteration 53/1000 | Loss: 0.00002018
Iteration 54/1000 | Loss: 0.00002017
Iteration 55/1000 | Loss: 0.00002017
Iteration 56/1000 | Loss: 0.00002017
Iteration 57/1000 | Loss: 0.00002016
Iteration 58/1000 | Loss: 0.00002016
Iteration 59/1000 | Loss: 0.00002016
Iteration 60/1000 | Loss: 0.00002016
Iteration 61/1000 | Loss: 0.00002016
Iteration 62/1000 | Loss: 0.00002015
Iteration 63/1000 | Loss: 0.00002015
Iteration 64/1000 | Loss: 0.00002015
Iteration 65/1000 | Loss: 0.00002014
Iteration 66/1000 | Loss: 0.00002014
Iteration 67/1000 | Loss: 0.00002014
Iteration 68/1000 | Loss: 0.00002013
Iteration 69/1000 | Loss: 0.00002013
Iteration 70/1000 | Loss: 0.00002013
Iteration 71/1000 | Loss: 0.00002013
Iteration 72/1000 | Loss: 0.00002013
Iteration 73/1000 | Loss: 0.00002013
Iteration 74/1000 | Loss: 0.00002013
Iteration 75/1000 | Loss: 0.00002013
Iteration 76/1000 | Loss: 0.00002013
Iteration 77/1000 | Loss: 0.00002013
Iteration 78/1000 | Loss: 0.00002012
Iteration 79/1000 | Loss: 0.00002012
Iteration 80/1000 | Loss: 0.00002012
Iteration 81/1000 | Loss: 0.00002012
Iteration 82/1000 | Loss: 0.00002012
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002011
Iteration 86/1000 | Loss: 0.00002011
Iteration 87/1000 | Loss: 0.00002011
Iteration 88/1000 | Loss: 0.00002010
Iteration 89/1000 | Loss: 0.00002010
Iteration 90/1000 | Loss: 0.00002009
Iteration 91/1000 | Loss: 0.00002009
Iteration 92/1000 | Loss: 0.00002009
Iteration 93/1000 | Loss: 0.00002009
Iteration 94/1000 | Loss: 0.00002009
Iteration 95/1000 | Loss: 0.00002009
Iteration 96/1000 | Loss: 0.00002008
Iteration 97/1000 | Loss: 0.00002008
Iteration 98/1000 | Loss: 0.00002008
Iteration 99/1000 | Loss: 0.00002008
Iteration 100/1000 | Loss: 0.00002008
Iteration 101/1000 | Loss: 0.00002008
Iteration 102/1000 | Loss: 0.00002008
Iteration 103/1000 | Loss: 0.00002008
Iteration 104/1000 | Loss: 0.00002007
Iteration 105/1000 | Loss: 0.00002007
Iteration 106/1000 | Loss: 0.00002007
Iteration 107/1000 | Loss: 0.00002007
Iteration 108/1000 | Loss: 0.00002007
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002005
Iteration 113/1000 | Loss: 0.00002005
Iteration 114/1000 | Loss: 0.00002005
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00002005
Iteration 118/1000 | Loss: 0.00002004
Iteration 119/1000 | Loss: 0.00002004
Iteration 120/1000 | Loss: 0.00002004
Iteration 121/1000 | Loss: 0.00002004
Iteration 122/1000 | Loss: 0.00002004
Iteration 123/1000 | Loss: 0.00002004
Iteration 124/1000 | Loss: 0.00002004
Iteration 125/1000 | Loss: 0.00002004
Iteration 126/1000 | Loss: 0.00002004
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002003
Iteration 130/1000 | Loss: 0.00002003
Iteration 131/1000 | Loss: 0.00002003
Iteration 132/1000 | Loss: 0.00002003
Iteration 133/1000 | Loss: 0.00002003
Iteration 134/1000 | Loss: 0.00002003
Iteration 135/1000 | Loss: 0.00002003
Iteration 136/1000 | Loss: 0.00002003
Iteration 137/1000 | Loss: 0.00002003
Iteration 138/1000 | Loss: 0.00002003
Iteration 139/1000 | Loss: 0.00002003
Iteration 140/1000 | Loss: 0.00002003
Iteration 141/1000 | Loss: 0.00002003
Iteration 142/1000 | Loss: 0.00002003
Iteration 143/1000 | Loss: 0.00002003
Iteration 144/1000 | Loss: 0.00002003
Iteration 145/1000 | Loss: 0.00002003
Iteration 146/1000 | Loss: 0.00002003
Iteration 147/1000 | Loss: 0.00002003
Iteration 148/1000 | Loss: 0.00002003
Iteration 149/1000 | Loss: 0.00002003
Iteration 150/1000 | Loss: 0.00002003
Iteration 151/1000 | Loss: 0.00002003
Iteration 152/1000 | Loss: 0.00002003
Iteration 153/1000 | Loss: 0.00002003
Iteration 154/1000 | Loss: 0.00002003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.0028395738336258e-05, 2.0028395738336258e-05, 2.0028395738336258e-05, 2.0028395738336258e-05, 2.0028395738336258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0028395738336258e-05

Optimization complete. Final v2v error: 3.713090658187866 mm

Highest mean error: 8.878873825073242 mm for frame 157

Lowest mean error: 3.130136728286743 mm for frame 161

Saving results

Total time: 74.94800662994385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043477
Iteration 2/25 | Loss: 0.00191604
Iteration 3/25 | Loss: 0.00133372
Iteration 4/25 | Loss: 0.00124238
Iteration 5/25 | Loss: 0.00116733
Iteration 6/25 | Loss: 0.00097791
Iteration 7/25 | Loss: 0.00088872
Iteration 8/25 | Loss: 0.00086185
Iteration 9/25 | Loss: 0.00085349
Iteration 10/25 | Loss: 0.00085108
Iteration 11/25 | Loss: 0.00085585
Iteration 12/25 | Loss: 0.00084965
Iteration 13/25 | Loss: 0.00084796
Iteration 14/25 | Loss: 0.00085287
Iteration 15/25 | Loss: 0.00084757
Iteration 16/25 | Loss: 0.00084629
Iteration 17/25 | Loss: 0.00084591
Iteration 18/25 | Loss: 0.00084578
Iteration 19/25 | Loss: 0.00084575
Iteration 20/25 | Loss: 0.00084575
Iteration 21/25 | Loss: 0.00084575
Iteration 22/25 | Loss: 0.00084575
Iteration 23/25 | Loss: 0.00084575
Iteration 24/25 | Loss: 0.00084575
Iteration 25/25 | Loss: 0.00084575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49356747
Iteration 2/25 | Loss: 0.00062504
Iteration 3/25 | Loss: 0.00060222
Iteration 4/25 | Loss: 0.00060222
Iteration 5/25 | Loss: 0.00060222
Iteration 6/25 | Loss: 0.00060221
Iteration 7/25 | Loss: 0.00060221
Iteration 8/25 | Loss: 0.00060221
Iteration 9/25 | Loss: 0.00060221
Iteration 10/25 | Loss: 0.00060221
Iteration 11/25 | Loss: 0.00060221
Iteration 12/25 | Loss: 0.00060221
Iteration 13/25 | Loss: 0.00060221
Iteration 14/25 | Loss: 0.00060221
Iteration 15/25 | Loss: 0.00060221
Iteration 16/25 | Loss: 0.00060221
Iteration 17/25 | Loss: 0.00060221
Iteration 18/25 | Loss: 0.00060221
Iteration 19/25 | Loss: 0.00060221
Iteration 20/25 | Loss: 0.00060221
Iteration 21/25 | Loss: 0.00060221
Iteration 22/25 | Loss: 0.00060221
Iteration 23/25 | Loss: 0.00060221
Iteration 24/25 | Loss: 0.00060221
Iteration 25/25 | Loss: 0.00060221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060221
Iteration 2/1000 | Loss: 0.00007143
Iteration 3/1000 | Loss: 0.00003923
Iteration 4/1000 | Loss: 0.00003380
Iteration 5/1000 | Loss: 0.00003387
Iteration 6/1000 | Loss: 0.00002954
Iteration 7/1000 | Loss: 0.00006076
Iteration 8/1000 | Loss: 0.00003428
Iteration 9/1000 | Loss: 0.00002750
Iteration 10/1000 | Loss: 0.00003253
Iteration 11/1000 | Loss: 0.00002652
Iteration 12/1000 | Loss: 0.00013799
Iteration 13/1000 | Loss: 0.00019788
Iteration 14/1000 | Loss: 0.00008549
Iteration 15/1000 | Loss: 0.00004948
Iteration 16/1000 | Loss: 0.00003218
Iteration 17/1000 | Loss: 0.00002671
Iteration 18/1000 | Loss: 0.00002458
Iteration 19/1000 | Loss: 0.00002268
Iteration 20/1000 | Loss: 0.00002098
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00001974
Iteration 23/1000 | Loss: 0.00001940
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001909
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001909
Iteration 32/1000 | Loss: 0.00001908
Iteration 33/1000 | Loss: 0.00001908
Iteration 34/1000 | Loss: 0.00001907
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001895
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001894
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001891
Iteration 47/1000 | Loss: 0.00001891
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001889
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001888
Iteration 61/1000 | Loss: 0.00001888
Iteration 62/1000 | Loss: 0.00001888
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001886
Iteration 67/1000 | Loss: 0.00001886
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001885
Iteration 78/1000 | Loss: 0.00001885
Iteration 79/1000 | Loss: 0.00001885
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001884
Iteration 82/1000 | Loss: 0.00001884
Iteration 83/1000 | Loss: 0.00001884
Iteration 84/1000 | Loss: 0.00001884
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001884
Iteration 87/1000 | Loss: 0.00001884
Iteration 88/1000 | Loss: 0.00001884
Iteration 89/1000 | Loss: 0.00001884
Iteration 90/1000 | Loss: 0.00001884
Iteration 91/1000 | Loss: 0.00001884
Iteration 92/1000 | Loss: 0.00001884
Iteration 93/1000 | Loss: 0.00001884
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001884
Iteration 102/1000 | Loss: 0.00001884
Iteration 103/1000 | Loss: 0.00001884
Iteration 104/1000 | Loss: 0.00001884
Iteration 105/1000 | Loss: 0.00001884
Iteration 106/1000 | Loss: 0.00001884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.883687036752235e-05, 1.883687036752235e-05, 1.883687036752235e-05, 1.883687036752235e-05, 1.883687036752235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.883687036752235e-05

Optimization complete. Final v2v error: 3.589038372039795 mm

Highest mean error: 3.8201704025268555 mm for frame 0

Lowest mean error: 3.319528341293335 mm for frame 2

Saving results

Total time: 74.35510897636414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00322659
Iteration 2/25 | Loss: 0.00091601
Iteration 3/25 | Loss: 0.00076740
Iteration 4/25 | Loss: 0.00073874
Iteration 5/25 | Loss: 0.00072850
Iteration 6/25 | Loss: 0.00072594
Iteration 7/25 | Loss: 0.00072512
Iteration 8/25 | Loss: 0.00072512
Iteration 9/25 | Loss: 0.00072512
Iteration 10/25 | Loss: 0.00072512
Iteration 11/25 | Loss: 0.00072512
Iteration 12/25 | Loss: 0.00072512
Iteration 13/25 | Loss: 0.00072512
Iteration 14/25 | Loss: 0.00072512
Iteration 15/25 | Loss: 0.00072512
Iteration 16/25 | Loss: 0.00072512
Iteration 17/25 | Loss: 0.00072512
Iteration 18/25 | Loss: 0.00072512
Iteration 19/25 | Loss: 0.00072512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007251197821460664, 0.0007251197821460664, 0.0007251197821460664, 0.0007251197821460664, 0.0007251197821460664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007251197821460664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50297129
Iteration 2/25 | Loss: 0.00046983
Iteration 3/25 | Loss: 0.00046983
Iteration 4/25 | Loss: 0.00046983
Iteration 5/25 | Loss: 0.00046983
Iteration 6/25 | Loss: 0.00046983
Iteration 7/25 | Loss: 0.00046983
Iteration 8/25 | Loss: 0.00046983
Iteration 9/25 | Loss: 0.00046983
Iteration 10/25 | Loss: 0.00046983
Iteration 11/25 | Loss: 0.00046983
Iteration 12/25 | Loss: 0.00046983
Iteration 13/25 | Loss: 0.00046983
Iteration 14/25 | Loss: 0.00046983
Iteration 15/25 | Loss: 0.00046983
Iteration 16/25 | Loss: 0.00046983
Iteration 17/25 | Loss: 0.00046983
Iteration 18/25 | Loss: 0.00046983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004698268312495202, 0.0004698268312495202, 0.0004698268312495202, 0.0004698268312495202, 0.0004698268312495202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004698268312495202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046983
Iteration 2/1000 | Loss: 0.00002971
Iteration 3/1000 | Loss: 0.00001978
Iteration 4/1000 | Loss: 0.00001769
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001400
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001391
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001390
Iteration 19/1000 | Loss: 0.00001389
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001386
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001376
Iteration 29/1000 | Loss: 0.00001374
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001367
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001363
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001355
Iteration 59/1000 | Loss: 0.00001355
Iteration 60/1000 | Loss: 0.00001354
Iteration 61/1000 | Loss: 0.00001354
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001349
Iteration 73/1000 | Loss: 0.00001349
Iteration 74/1000 | Loss: 0.00001348
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001348
Iteration 77/1000 | Loss: 0.00001348
Iteration 78/1000 | Loss: 0.00001348
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001346
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001344
Iteration 92/1000 | Loss: 0.00001344
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001340
Iteration 119/1000 | Loss: 0.00001340
Iteration 120/1000 | Loss: 0.00001340
Iteration 121/1000 | Loss: 0.00001340
Iteration 122/1000 | Loss: 0.00001340
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001339
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001339
Iteration 127/1000 | Loss: 0.00001339
Iteration 128/1000 | Loss: 0.00001339
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001339
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001339
Iteration 142/1000 | Loss: 0.00001339
Iteration 143/1000 | Loss: 0.00001339
Iteration 144/1000 | Loss: 0.00001339
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Iteration 148/1000 | Loss: 0.00001339
Iteration 149/1000 | Loss: 0.00001339
Iteration 150/1000 | Loss: 0.00001339
Iteration 151/1000 | Loss: 0.00001339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3386010323301889e-05, 1.3386010323301889e-05, 1.3386010323301889e-05, 1.3386010323301889e-05, 1.3386010323301889e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3386010323301889e-05

Optimization complete. Final v2v error: 3.1045851707458496 mm

Highest mean error: 3.384222984313965 mm for frame 4

Lowest mean error: 2.8920629024505615 mm for frame 32

Saving results

Total time: 46.56687688827515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351427
Iteration 2/25 | Loss: 0.00100561
Iteration 3/25 | Loss: 0.00080998
Iteration 4/25 | Loss: 0.00076103
Iteration 5/25 | Loss: 0.00075006
Iteration 6/25 | Loss: 0.00074744
Iteration 7/25 | Loss: 0.00074705
Iteration 8/25 | Loss: 0.00074705
Iteration 9/25 | Loss: 0.00074705
Iteration 10/25 | Loss: 0.00074705
Iteration 11/25 | Loss: 0.00074705
Iteration 12/25 | Loss: 0.00074705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007470492273569107, 0.0007470492273569107, 0.0007470492273569107, 0.0007470492273569107, 0.0007470492273569107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007470492273569107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54103732
Iteration 2/25 | Loss: 0.00051527
Iteration 3/25 | Loss: 0.00051527
Iteration 4/25 | Loss: 0.00051527
Iteration 5/25 | Loss: 0.00051527
Iteration 6/25 | Loss: 0.00051527
Iteration 7/25 | Loss: 0.00051527
Iteration 8/25 | Loss: 0.00051527
Iteration 9/25 | Loss: 0.00051527
Iteration 10/25 | Loss: 0.00051527
Iteration 11/25 | Loss: 0.00051527
Iteration 12/25 | Loss: 0.00051527
Iteration 13/25 | Loss: 0.00051527
Iteration 14/25 | Loss: 0.00051527
Iteration 15/25 | Loss: 0.00051527
Iteration 16/25 | Loss: 0.00051527
Iteration 17/25 | Loss: 0.00051527
Iteration 18/25 | Loss: 0.00051527
Iteration 19/25 | Loss: 0.00051527
Iteration 20/25 | Loss: 0.00051527
Iteration 21/25 | Loss: 0.00051527
Iteration 22/25 | Loss: 0.00051527
Iteration 23/25 | Loss: 0.00051527
Iteration 24/25 | Loss: 0.00051527
Iteration 25/25 | Loss: 0.00051527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051527
Iteration 2/1000 | Loss: 0.00002761
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001481
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001274
Iteration 10/1000 | Loss: 0.00001271
Iteration 11/1000 | Loss: 0.00001271
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001259
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001239
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001237
Iteration 38/1000 | Loss: 0.00001237
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001233
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001230
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001227
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001227
Iteration 80/1000 | Loss: 0.00001227
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001227
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001227
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.226618405780755e-05, 1.226618405780755e-05, 1.226618405780755e-05, 1.226618405780755e-05, 1.226618405780755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.226618405780755e-05

Optimization complete. Final v2v error: 2.913109540939331 mm

Highest mean error: 3.2159640789031982 mm for frame 12

Lowest mean error: 2.7442493438720703 mm for frame 61

Saving results

Total time: 31.784177780151367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067868
Iteration 2/25 | Loss: 0.01067868
Iteration 3/25 | Loss: 0.01067868
Iteration 4/25 | Loss: 0.01067868
Iteration 5/25 | Loss: 0.01067868
Iteration 6/25 | Loss: 0.01067868
Iteration 7/25 | Loss: 0.01067868
Iteration 8/25 | Loss: 0.01067868
Iteration 9/25 | Loss: 0.01067868
Iteration 10/25 | Loss: 0.01067868
Iteration 11/25 | Loss: 0.01067867
Iteration 12/25 | Loss: 0.01067867
Iteration 13/25 | Loss: 0.01067867
Iteration 14/25 | Loss: 0.01067867
Iteration 15/25 | Loss: 0.01067867
Iteration 16/25 | Loss: 0.01067867
Iteration 17/25 | Loss: 0.01067867
Iteration 18/25 | Loss: 0.01067867
Iteration 19/25 | Loss: 0.01067867
Iteration 20/25 | Loss: 0.01067867
Iteration 21/25 | Loss: 0.01067866
Iteration 22/25 | Loss: 0.01067866
Iteration 23/25 | Loss: 0.01067866
Iteration 24/25 | Loss: 0.01067866
Iteration 25/25 | Loss: 0.01067866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03734732
Iteration 2/25 | Loss: 0.05473297
Iteration 3/25 | Loss: 0.05468926
Iteration 4/25 | Loss: 0.05468926
Iteration 5/25 | Loss: 0.05468926
Iteration 6/25 | Loss: 0.05452312
Iteration 7/25 | Loss: 0.05452311
Iteration 8/25 | Loss: 0.05452311
Iteration 9/25 | Loss: 0.05452311
Iteration 10/25 | Loss: 0.05452310
Iteration 11/25 | Loss: 0.05452310
Iteration 12/25 | Loss: 0.05452310
Iteration 13/25 | Loss: 0.05452310
Iteration 14/25 | Loss: 0.05452310
Iteration 15/25 | Loss: 0.05452310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.054523102939128876, 0.054523102939128876, 0.054523102939128876, 0.054523102939128876, 0.054523102939128876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.054523102939128876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05452310
Iteration 2/1000 | Loss: 0.00449239
Iteration 3/1000 | Loss: 0.00138728
Iteration 4/1000 | Loss: 0.00081063
Iteration 5/1000 | Loss: 0.00090662
Iteration 6/1000 | Loss: 0.00671822
Iteration 7/1000 | Loss: 0.00501029
Iteration 8/1000 | Loss: 0.00161862
Iteration 9/1000 | Loss: 0.00118742
Iteration 10/1000 | Loss: 0.00011738
Iteration 11/1000 | Loss: 0.00072331
Iteration 12/1000 | Loss: 0.00168745
Iteration 13/1000 | Loss: 0.00010192
Iteration 14/1000 | Loss: 0.00017200
Iteration 15/1000 | Loss: 0.00034639
Iteration 16/1000 | Loss: 0.00035450
Iteration 17/1000 | Loss: 0.00055911
Iteration 18/1000 | Loss: 0.00046627
Iteration 19/1000 | Loss: 0.00006030
Iteration 20/1000 | Loss: 0.00007438
Iteration 21/1000 | Loss: 0.00007607
Iteration 22/1000 | Loss: 0.00042702
Iteration 23/1000 | Loss: 0.00033146
Iteration 24/1000 | Loss: 0.00073258
Iteration 25/1000 | Loss: 0.00039302
Iteration 26/1000 | Loss: 0.00064380
Iteration 27/1000 | Loss: 0.00017972
Iteration 28/1000 | Loss: 0.00025908
Iteration 29/1000 | Loss: 0.00005479
Iteration 30/1000 | Loss: 0.00004362
Iteration 31/1000 | Loss: 0.00004849
Iteration 32/1000 | Loss: 0.00003588
Iteration 33/1000 | Loss: 0.00025548
Iteration 34/1000 | Loss: 0.00005731
Iteration 35/1000 | Loss: 0.00039613
Iteration 36/1000 | Loss: 0.00013883
Iteration 37/1000 | Loss: 0.00037992
Iteration 38/1000 | Loss: 0.00003587
Iteration 39/1000 | Loss: 0.00011313
Iteration 40/1000 | Loss: 0.00062016
Iteration 41/1000 | Loss: 0.00003422
Iteration 42/1000 | Loss: 0.00004627
Iteration 43/1000 | Loss: 0.00003137
Iteration 44/1000 | Loss: 0.00021421
Iteration 45/1000 | Loss: 0.00003623
Iteration 46/1000 | Loss: 0.00002969
Iteration 47/1000 | Loss: 0.00046010
Iteration 48/1000 | Loss: 0.00005143
Iteration 49/1000 | Loss: 0.00004561
Iteration 50/1000 | Loss: 0.00046790
Iteration 51/1000 | Loss: 0.00017380
Iteration 52/1000 | Loss: 0.00023290
Iteration 53/1000 | Loss: 0.00014880
Iteration 54/1000 | Loss: 0.00024022
Iteration 55/1000 | Loss: 0.00011643
Iteration 56/1000 | Loss: 0.00056536
Iteration 57/1000 | Loss: 0.00022174
Iteration 58/1000 | Loss: 0.00048553
Iteration 59/1000 | Loss: 0.00019551
Iteration 60/1000 | Loss: 0.00009870
Iteration 61/1000 | Loss: 0.00013872
Iteration 62/1000 | Loss: 0.00020813
Iteration 63/1000 | Loss: 0.00002943
Iteration 64/1000 | Loss: 0.00021148
Iteration 65/1000 | Loss: 0.00010651
Iteration 66/1000 | Loss: 0.00007277
Iteration 67/1000 | Loss: 0.00034177
Iteration 68/1000 | Loss: 0.00018400
Iteration 69/1000 | Loss: 0.00010215
Iteration 70/1000 | Loss: 0.00005805
Iteration 71/1000 | Loss: 0.00007999
Iteration 72/1000 | Loss: 0.00012679
Iteration 73/1000 | Loss: 0.00014952
Iteration 74/1000 | Loss: 0.00003738
Iteration 75/1000 | Loss: 0.00013544
Iteration 76/1000 | Loss: 0.00014077
Iteration 77/1000 | Loss: 0.00013206
Iteration 78/1000 | Loss: 0.00015975
Iteration 79/1000 | Loss: 0.00010294
Iteration 80/1000 | Loss: 0.00005189
Iteration 81/1000 | Loss: 0.00003402
Iteration 82/1000 | Loss: 0.00005286
Iteration 83/1000 | Loss: 0.00008901
Iteration 84/1000 | Loss: 0.00007176
Iteration 85/1000 | Loss: 0.00005418
Iteration 86/1000 | Loss: 0.00008039
Iteration 87/1000 | Loss: 0.00003000
Iteration 88/1000 | Loss: 0.00005953
Iteration 89/1000 | Loss: 0.00002676
Iteration 90/1000 | Loss: 0.00003416
Iteration 91/1000 | Loss: 0.00003723
Iteration 92/1000 | Loss: 0.00002796
Iteration 93/1000 | Loss: 0.00002759
Iteration 94/1000 | Loss: 0.00002393
Iteration 95/1000 | Loss: 0.00006108
Iteration 96/1000 | Loss: 0.00003958
Iteration 97/1000 | Loss: 0.00002362
Iteration 98/1000 | Loss: 0.00002686
Iteration 99/1000 | Loss: 0.00002348
Iteration 100/1000 | Loss: 0.00003280
Iteration 101/1000 | Loss: 0.00002327
Iteration 102/1000 | Loss: 0.00002315
Iteration 103/1000 | Loss: 0.00002313
Iteration 104/1000 | Loss: 0.00002312
Iteration 105/1000 | Loss: 0.00002312
Iteration 106/1000 | Loss: 0.00003903
Iteration 107/1000 | Loss: 0.00004333
Iteration 108/1000 | Loss: 0.00002752
Iteration 109/1000 | Loss: 0.00002273
Iteration 110/1000 | Loss: 0.00006181
Iteration 111/1000 | Loss: 0.00002564
Iteration 112/1000 | Loss: 0.00002416
Iteration 113/1000 | Loss: 0.00002223
Iteration 114/1000 | Loss: 0.00024468
Iteration 115/1000 | Loss: 0.00002803
Iteration 116/1000 | Loss: 0.00002211
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002191
Iteration 119/1000 | Loss: 0.00002191
Iteration 120/1000 | Loss: 0.00002191
Iteration 121/1000 | Loss: 0.00002190
Iteration 122/1000 | Loss: 0.00002190
Iteration 123/1000 | Loss: 0.00002190
Iteration 124/1000 | Loss: 0.00002190
Iteration 125/1000 | Loss: 0.00002190
Iteration 126/1000 | Loss: 0.00002190
Iteration 127/1000 | Loss: 0.00002190
Iteration 128/1000 | Loss: 0.00002190
Iteration 129/1000 | Loss: 0.00002190
Iteration 130/1000 | Loss: 0.00002190
Iteration 131/1000 | Loss: 0.00002189
Iteration 132/1000 | Loss: 0.00002189
Iteration 133/1000 | Loss: 0.00002189
Iteration 134/1000 | Loss: 0.00002189
Iteration 135/1000 | Loss: 0.00002189
Iteration 136/1000 | Loss: 0.00002189
Iteration 137/1000 | Loss: 0.00002188
Iteration 138/1000 | Loss: 0.00002646
Iteration 139/1000 | Loss: 0.00002184
Iteration 140/1000 | Loss: 0.00002180
Iteration 141/1000 | Loss: 0.00002179
Iteration 142/1000 | Loss: 0.00002179
Iteration 143/1000 | Loss: 0.00002179
Iteration 144/1000 | Loss: 0.00002179
Iteration 145/1000 | Loss: 0.00002179
Iteration 146/1000 | Loss: 0.00002179
Iteration 147/1000 | Loss: 0.00002178
Iteration 148/1000 | Loss: 0.00002178
Iteration 149/1000 | Loss: 0.00002178
Iteration 150/1000 | Loss: 0.00002178
Iteration 151/1000 | Loss: 0.00002177
Iteration 152/1000 | Loss: 0.00002177
Iteration 153/1000 | Loss: 0.00002177
Iteration 154/1000 | Loss: 0.00002177
Iteration 155/1000 | Loss: 0.00002176
Iteration 156/1000 | Loss: 0.00002176
Iteration 157/1000 | Loss: 0.00002176
Iteration 158/1000 | Loss: 0.00002176
Iteration 159/1000 | Loss: 0.00002176
Iteration 160/1000 | Loss: 0.00002176
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00002176
Iteration 166/1000 | Loss: 0.00002175
Iteration 167/1000 | Loss: 0.00002175
Iteration 168/1000 | Loss: 0.00002175
Iteration 169/1000 | Loss: 0.00002175
Iteration 170/1000 | Loss: 0.00002175
Iteration 171/1000 | Loss: 0.00002175
Iteration 172/1000 | Loss: 0.00002174
Iteration 173/1000 | Loss: 0.00002174
Iteration 174/1000 | Loss: 0.00002174
Iteration 175/1000 | Loss: 0.00002174
Iteration 176/1000 | Loss: 0.00002174
Iteration 177/1000 | Loss: 0.00002173
Iteration 178/1000 | Loss: 0.00002173
Iteration 179/1000 | Loss: 0.00002173
Iteration 180/1000 | Loss: 0.00002173
Iteration 181/1000 | Loss: 0.00002172
Iteration 182/1000 | Loss: 0.00002172
Iteration 183/1000 | Loss: 0.00002172
Iteration 184/1000 | Loss: 0.00002171
Iteration 185/1000 | Loss: 0.00002171
Iteration 186/1000 | Loss: 0.00002171
Iteration 187/1000 | Loss: 0.00002171
Iteration 188/1000 | Loss: 0.00002171
Iteration 189/1000 | Loss: 0.00002171
Iteration 190/1000 | Loss: 0.00002171
Iteration 191/1000 | Loss: 0.00002171
Iteration 192/1000 | Loss: 0.00002170
Iteration 193/1000 | Loss: 0.00002170
Iteration 194/1000 | Loss: 0.00002170
Iteration 195/1000 | Loss: 0.00002170
Iteration 196/1000 | Loss: 0.00002170
Iteration 197/1000 | Loss: 0.00002170
Iteration 198/1000 | Loss: 0.00002170
Iteration 199/1000 | Loss: 0.00002170
Iteration 200/1000 | Loss: 0.00002170
Iteration 201/1000 | Loss: 0.00002170
Iteration 202/1000 | Loss: 0.00002170
Iteration 203/1000 | Loss: 0.00002170
Iteration 204/1000 | Loss: 0.00002170
Iteration 205/1000 | Loss: 0.00002169
Iteration 206/1000 | Loss: 0.00002169
Iteration 207/1000 | Loss: 0.00002169
Iteration 208/1000 | Loss: 0.00002169
Iteration 209/1000 | Loss: 0.00002169
Iteration 210/1000 | Loss: 0.00002168
Iteration 211/1000 | Loss: 0.00002168
Iteration 212/1000 | Loss: 0.00002168
Iteration 213/1000 | Loss: 0.00002168
Iteration 214/1000 | Loss: 0.00002168
Iteration 215/1000 | Loss: 0.00002168
Iteration 216/1000 | Loss: 0.00002168
Iteration 217/1000 | Loss: 0.00002167
Iteration 218/1000 | Loss: 0.00002167
Iteration 219/1000 | Loss: 0.00002167
Iteration 220/1000 | Loss: 0.00002167
Iteration 221/1000 | Loss: 0.00002167
Iteration 222/1000 | Loss: 0.00002167
Iteration 223/1000 | Loss: 0.00002167
Iteration 224/1000 | Loss: 0.00002167
Iteration 225/1000 | Loss: 0.00002166
Iteration 226/1000 | Loss: 0.00002166
Iteration 227/1000 | Loss: 0.00002166
Iteration 228/1000 | Loss: 0.00002166
Iteration 229/1000 | Loss: 0.00002166
Iteration 230/1000 | Loss: 0.00002166
Iteration 231/1000 | Loss: 0.00002166
Iteration 232/1000 | Loss: 0.00002166
Iteration 233/1000 | Loss: 0.00002166
Iteration 234/1000 | Loss: 0.00002166
Iteration 235/1000 | Loss: 0.00002166
Iteration 236/1000 | Loss: 0.00002166
Iteration 237/1000 | Loss: 0.00002166
Iteration 238/1000 | Loss: 0.00002166
Iteration 239/1000 | Loss: 0.00002166
Iteration 240/1000 | Loss: 0.00002166
Iteration 241/1000 | Loss: 0.00002166
Iteration 242/1000 | Loss: 0.00002166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [2.1659314370481297e-05, 2.1659314370481297e-05, 2.1659314370481297e-05, 2.1659314370481297e-05, 2.1659314370481297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1659314370481297e-05

Optimization complete. Final v2v error: 3.415961980819702 mm

Highest mean error: 17.412145614624023 mm for frame 80

Lowest mean error: 2.6591694355010986 mm for frame 134

Saving results

Total time: 200.01708340644836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792807
Iteration 2/25 | Loss: 0.00131194
Iteration 3/25 | Loss: 0.00095515
Iteration 4/25 | Loss: 0.00093888
Iteration 5/25 | Loss: 0.00097996
Iteration 6/25 | Loss: 0.00086800
Iteration 7/25 | Loss: 0.00082544
Iteration 8/25 | Loss: 0.00081564
Iteration 9/25 | Loss: 0.00080524
Iteration 10/25 | Loss: 0.00080717
Iteration 11/25 | Loss: 0.00080359
Iteration 12/25 | Loss: 0.00080027
Iteration 13/25 | Loss: 0.00080906
Iteration 14/25 | Loss: 0.00080057
Iteration 15/25 | Loss: 0.00079053
Iteration 16/25 | Loss: 0.00079149
Iteration 17/25 | Loss: 0.00078982
Iteration 18/25 | Loss: 0.00078770
Iteration 19/25 | Loss: 0.00078849
Iteration 20/25 | Loss: 0.00078879
Iteration 21/25 | Loss: 0.00078506
Iteration 22/25 | Loss: 0.00078254
Iteration 23/25 | Loss: 0.00078403
Iteration 24/25 | Loss: 0.00078366
Iteration 25/25 | Loss: 0.00078406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74968338
Iteration 2/25 | Loss: 0.00111934
Iteration 3/25 | Loss: 0.00111934
Iteration 4/25 | Loss: 0.00111934
Iteration 5/25 | Loss: 0.00111934
Iteration 6/25 | Loss: 0.00111934
Iteration 7/25 | Loss: 0.00111934
Iteration 8/25 | Loss: 0.00111934
Iteration 9/25 | Loss: 0.00111934
Iteration 10/25 | Loss: 0.00111934
Iteration 11/25 | Loss: 0.00111934
Iteration 12/25 | Loss: 0.00111934
Iteration 13/25 | Loss: 0.00111934
Iteration 14/25 | Loss: 0.00111934
Iteration 15/25 | Loss: 0.00111934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011193364625796676, 0.0011193364625796676, 0.0011193364625796676, 0.0011193364625796676, 0.0011193364625796676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011193364625796676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111934
Iteration 2/1000 | Loss: 0.00239630
Iteration 3/1000 | Loss: 0.00061080
Iteration 4/1000 | Loss: 0.00202757
Iteration 5/1000 | Loss: 0.00461327
Iteration 6/1000 | Loss: 0.00025542
Iteration 7/1000 | Loss: 0.00033631
Iteration 8/1000 | Loss: 0.00034790
Iteration 9/1000 | Loss: 0.00044854
Iteration 10/1000 | Loss: 0.00005177
Iteration 11/1000 | Loss: 0.00044187
Iteration 12/1000 | Loss: 0.00003965
Iteration 13/1000 | Loss: 0.00007151
Iteration 14/1000 | Loss: 0.00002939
Iteration 15/1000 | Loss: 0.00029790
Iteration 16/1000 | Loss: 0.00050233
Iteration 17/1000 | Loss: 0.00011519
Iteration 18/1000 | Loss: 0.00012766
Iteration 19/1000 | Loss: 0.00007762
Iteration 20/1000 | Loss: 0.00041204
Iteration 21/1000 | Loss: 0.00044675
Iteration 22/1000 | Loss: 0.00006678
Iteration 23/1000 | Loss: 0.00003714
Iteration 24/1000 | Loss: 0.00003529
Iteration 25/1000 | Loss: 0.00002742
Iteration 26/1000 | Loss: 0.00029512
Iteration 27/1000 | Loss: 0.00003519
Iteration 28/1000 | Loss: 0.00002640
Iteration 29/1000 | Loss: 0.00002421
Iteration 30/1000 | Loss: 0.00016226
Iteration 31/1000 | Loss: 0.00047067
Iteration 32/1000 | Loss: 0.00019298
Iteration 33/1000 | Loss: 0.00020380
Iteration 34/1000 | Loss: 0.00003002
Iteration 35/1000 | Loss: 0.00015894
Iteration 36/1000 | Loss: 0.00011062
Iteration 37/1000 | Loss: 0.00016834
Iteration 38/1000 | Loss: 0.00022210
Iteration 39/1000 | Loss: 0.00018954
Iteration 40/1000 | Loss: 0.00019176
Iteration 41/1000 | Loss: 0.00005058
Iteration 42/1000 | Loss: 0.00007448
Iteration 43/1000 | Loss: 0.00017828
Iteration 44/1000 | Loss: 0.00004060
Iteration 45/1000 | Loss: 0.00002820
Iteration 46/1000 | Loss: 0.00002455
Iteration 47/1000 | Loss: 0.00002067
Iteration 48/1000 | Loss: 0.00001952
Iteration 49/1000 | Loss: 0.00001830
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001659
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001544
Iteration 54/1000 | Loss: 0.00001509
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001467
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001450
Iteration 59/1000 | Loss: 0.00001446
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001421
Iteration 76/1000 | Loss: 0.00001421
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Iteration 83/1000 | Loss: 0.00001420
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001419
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001419
Iteration 90/1000 | Loss: 0.00001419
Iteration 91/1000 | Loss: 0.00001418
Iteration 92/1000 | Loss: 0.00001418
Iteration 93/1000 | Loss: 0.00001418
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001418
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001418
Iteration 98/1000 | Loss: 0.00001418
Iteration 99/1000 | Loss: 0.00001418
Iteration 100/1000 | Loss: 0.00001418
Iteration 101/1000 | Loss: 0.00001418
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001417
Iteration 104/1000 | Loss: 0.00001417
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001415
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001414
Iteration 126/1000 | Loss: 0.00001414
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001412
Iteration 130/1000 | Loss: 0.00001412
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001410
Iteration 140/1000 | Loss: 0.00001410
Iteration 141/1000 | Loss: 0.00001410
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001409
Iteration 144/1000 | Loss: 0.00001409
Iteration 145/1000 | Loss: 0.00001409
Iteration 146/1000 | Loss: 0.00001409
Iteration 147/1000 | Loss: 0.00001409
Iteration 148/1000 | Loss: 0.00001409
Iteration 149/1000 | Loss: 0.00001409
Iteration 150/1000 | Loss: 0.00001409
Iteration 151/1000 | Loss: 0.00001409
Iteration 152/1000 | Loss: 0.00001409
Iteration 153/1000 | Loss: 0.00001409
Iteration 154/1000 | Loss: 0.00001408
Iteration 155/1000 | Loss: 0.00001408
Iteration 156/1000 | Loss: 0.00001408
Iteration 157/1000 | Loss: 0.00001408
Iteration 158/1000 | Loss: 0.00001408
Iteration 159/1000 | Loss: 0.00001408
Iteration 160/1000 | Loss: 0.00001408
Iteration 161/1000 | Loss: 0.00001408
Iteration 162/1000 | Loss: 0.00001408
Iteration 163/1000 | Loss: 0.00001408
Iteration 164/1000 | Loss: 0.00001408
Iteration 165/1000 | Loss: 0.00001408
Iteration 166/1000 | Loss: 0.00001408
Iteration 167/1000 | Loss: 0.00001408
Iteration 168/1000 | Loss: 0.00001408
Iteration 169/1000 | Loss: 0.00001408
Iteration 170/1000 | Loss: 0.00001408
Iteration 171/1000 | Loss: 0.00001408
Iteration 172/1000 | Loss: 0.00001408
Iteration 173/1000 | Loss: 0.00001408
Iteration 174/1000 | Loss: 0.00001408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.408158459526021e-05, 1.408158459526021e-05, 1.408158459526021e-05, 1.408158459526021e-05, 1.408158459526021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.408158459526021e-05

Optimization complete. Final v2v error: 3.0986087322235107 mm

Highest mean error: 4.514391899108887 mm for frame 73

Lowest mean error: 2.4491474628448486 mm for frame 235

Saving results

Total time: 157.6100869178772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139092
Iteration 2/25 | Loss: 0.00385180
Iteration 3/25 | Loss: 0.00305315
Iteration 4/25 | Loss: 0.00207960
Iteration 5/25 | Loss: 0.00179580
Iteration 6/25 | Loss: 0.00168398
Iteration 7/25 | Loss: 0.00156667
Iteration 8/25 | Loss: 0.00152111
Iteration 9/25 | Loss: 0.00148699
Iteration 10/25 | Loss: 0.00144835
Iteration 11/25 | Loss: 0.00148196
Iteration 12/25 | Loss: 0.00143488
Iteration 13/25 | Loss: 0.00139322
Iteration 14/25 | Loss: 0.00137518
Iteration 15/25 | Loss: 0.00136790
Iteration 16/25 | Loss: 0.00136815
Iteration 17/25 | Loss: 0.00137324
Iteration 18/25 | Loss: 0.00136003
Iteration 19/25 | Loss: 0.00136026
Iteration 20/25 | Loss: 0.00136002
Iteration 21/25 | Loss: 0.00135271
Iteration 22/25 | Loss: 0.00135024
Iteration 23/25 | Loss: 0.00134959
Iteration 24/25 | Loss: 0.00134947
Iteration 25/25 | Loss: 0.00134945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51082605
Iteration 2/25 | Loss: 0.00192934
Iteration 3/25 | Loss: 0.00072819
Iteration 4/25 | Loss: 0.00072819
Iteration 5/25 | Loss: 0.00072819
Iteration 6/25 | Loss: 0.00072819
Iteration 7/25 | Loss: 0.00072819
Iteration 8/25 | Loss: 0.00072819
Iteration 9/25 | Loss: 0.00072819
Iteration 10/25 | Loss: 0.00072819
Iteration 11/25 | Loss: 0.00072819
Iteration 12/25 | Loss: 0.00072819
Iteration 13/25 | Loss: 0.00072819
Iteration 14/25 | Loss: 0.00072819
Iteration 15/25 | Loss: 0.00072819
Iteration 16/25 | Loss: 0.00072819
Iteration 17/25 | Loss: 0.00072819
Iteration 18/25 | Loss: 0.00072819
Iteration 19/25 | Loss: 0.00072819
Iteration 20/25 | Loss: 0.00072819
Iteration 21/25 | Loss: 0.00072819
Iteration 22/25 | Loss: 0.00072819
Iteration 23/25 | Loss: 0.00072819
Iteration 24/25 | Loss: 0.00072819
Iteration 25/25 | Loss: 0.00072819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072819
Iteration 2/1000 | Loss: 0.00388103
Iteration 3/1000 | Loss: 0.00416240
Iteration 4/1000 | Loss: 0.00403049
Iteration 5/1000 | Loss: 0.00096808
Iteration 6/1000 | Loss: 0.00023795
Iteration 7/1000 | Loss: 0.00015388
Iteration 8/1000 | Loss: 0.00012507
Iteration 9/1000 | Loss: 0.00010549
Iteration 10/1000 | Loss: 0.00009496
Iteration 11/1000 | Loss: 0.00089627
Iteration 12/1000 | Loss: 0.00013370
Iteration 13/1000 | Loss: 0.00009700
Iteration 14/1000 | Loss: 0.00007421
Iteration 15/1000 | Loss: 0.00006818
Iteration 16/1000 | Loss: 0.00006535
Iteration 17/1000 | Loss: 0.00006386
Iteration 18/1000 | Loss: 0.00006233
Iteration 19/1000 | Loss: 0.00006058
Iteration 20/1000 | Loss: 0.00005906
Iteration 21/1000 | Loss: 0.00005805
Iteration 22/1000 | Loss: 0.00005719
Iteration 23/1000 | Loss: 0.00005648
Iteration 24/1000 | Loss: 0.00005570
Iteration 25/1000 | Loss: 0.00005512
Iteration 26/1000 | Loss: 0.00005487
Iteration 27/1000 | Loss: 0.00005468
Iteration 28/1000 | Loss: 0.00005445
Iteration 29/1000 | Loss: 0.00005422
Iteration 30/1000 | Loss: 0.00005408
Iteration 31/1000 | Loss: 0.00005395
Iteration 32/1000 | Loss: 0.00005378
Iteration 33/1000 | Loss: 0.00005376
Iteration 34/1000 | Loss: 0.00005371
Iteration 35/1000 | Loss: 0.00005365
Iteration 36/1000 | Loss: 0.00005362
Iteration 37/1000 | Loss: 0.00005361
Iteration 38/1000 | Loss: 0.00005361
Iteration 39/1000 | Loss: 0.00005353
Iteration 40/1000 | Loss: 0.00005350
Iteration 41/1000 | Loss: 0.00005349
Iteration 42/1000 | Loss: 0.00005349
Iteration 43/1000 | Loss: 0.00005348
Iteration 44/1000 | Loss: 0.00005348
Iteration 45/1000 | Loss: 0.00005348
Iteration 46/1000 | Loss: 0.00005348
Iteration 47/1000 | Loss: 0.00005347
Iteration 48/1000 | Loss: 0.00005346
Iteration 49/1000 | Loss: 0.00005346
Iteration 50/1000 | Loss: 0.00005346
Iteration 51/1000 | Loss: 0.00005346
Iteration 52/1000 | Loss: 0.00005346
Iteration 53/1000 | Loss: 0.00005345
Iteration 54/1000 | Loss: 0.00005343
Iteration 55/1000 | Loss: 0.00005343
Iteration 56/1000 | Loss: 0.00005343
Iteration 57/1000 | Loss: 0.00005343
Iteration 58/1000 | Loss: 0.00005343
Iteration 59/1000 | Loss: 0.00005343
Iteration 60/1000 | Loss: 0.00005343
Iteration 61/1000 | Loss: 0.00005342
Iteration 62/1000 | Loss: 0.00005342
Iteration 63/1000 | Loss: 0.00005341
Iteration 64/1000 | Loss: 0.00005341
Iteration 65/1000 | Loss: 0.00005341
Iteration 66/1000 | Loss: 0.00005341
Iteration 67/1000 | Loss: 0.00005341
Iteration 68/1000 | Loss: 0.00005341
Iteration 69/1000 | Loss: 0.00005340
Iteration 70/1000 | Loss: 0.00005340
Iteration 71/1000 | Loss: 0.00005340
Iteration 72/1000 | Loss: 0.00005340
Iteration 73/1000 | Loss: 0.00005340
Iteration 74/1000 | Loss: 0.00005339
Iteration 75/1000 | Loss: 0.00005339
Iteration 76/1000 | Loss: 0.00005339
Iteration 77/1000 | Loss: 0.00005338
Iteration 78/1000 | Loss: 0.00005338
Iteration 79/1000 | Loss: 0.00005338
Iteration 80/1000 | Loss: 0.00005338
Iteration 81/1000 | Loss: 0.00005338
Iteration 82/1000 | Loss: 0.00005337
Iteration 83/1000 | Loss: 0.00005337
Iteration 84/1000 | Loss: 0.00005337
Iteration 85/1000 | Loss: 0.00005337
Iteration 86/1000 | Loss: 0.00005337
Iteration 87/1000 | Loss: 0.00005337
Iteration 88/1000 | Loss: 0.00005336
Iteration 89/1000 | Loss: 0.00005336
Iteration 90/1000 | Loss: 0.00005336
Iteration 91/1000 | Loss: 0.00005335
Iteration 92/1000 | Loss: 0.00005335
Iteration 93/1000 | Loss: 0.00005335
Iteration 94/1000 | Loss: 0.00005335
Iteration 95/1000 | Loss: 0.00005335
Iteration 96/1000 | Loss: 0.00005334
Iteration 97/1000 | Loss: 0.00005334
Iteration 98/1000 | Loss: 0.00005334
Iteration 99/1000 | Loss: 0.00005334
Iteration 100/1000 | Loss: 0.00005334
Iteration 101/1000 | Loss: 0.00005334
Iteration 102/1000 | Loss: 0.00005334
Iteration 103/1000 | Loss: 0.00005334
Iteration 104/1000 | Loss: 0.00005333
Iteration 105/1000 | Loss: 0.00005333
Iteration 106/1000 | Loss: 0.00005333
Iteration 107/1000 | Loss: 0.00005333
Iteration 108/1000 | Loss: 0.00005333
Iteration 109/1000 | Loss: 0.00005333
Iteration 110/1000 | Loss: 0.00005333
Iteration 111/1000 | Loss: 0.00005333
Iteration 112/1000 | Loss: 0.00005333
Iteration 113/1000 | Loss: 0.00005332
Iteration 114/1000 | Loss: 0.00005332
Iteration 115/1000 | Loss: 0.00005332
Iteration 116/1000 | Loss: 0.00005332
Iteration 117/1000 | Loss: 0.00005332
Iteration 118/1000 | Loss: 0.00005331
Iteration 119/1000 | Loss: 0.00005331
Iteration 120/1000 | Loss: 0.00005331
Iteration 121/1000 | Loss: 0.00005331
Iteration 122/1000 | Loss: 0.00005331
Iteration 123/1000 | Loss: 0.00005331
Iteration 124/1000 | Loss: 0.00005331
Iteration 125/1000 | Loss: 0.00005331
Iteration 126/1000 | Loss: 0.00005331
Iteration 127/1000 | Loss: 0.00005331
Iteration 128/1000 | Loss: 0.00005330
Iteration 129/1000 | Loss: 0.00005330
Iteration 130/1000 | Loss: 0.00005330
Iteration 131/1000 | Loss: 0.00005330
Iteration 132/1000 | Loss: 0.00005330
Iteration 133/1000 | Loss: 0.00005330
Iteration 134/1000 | Loss: 0.00005330
Iteration 135/1000 | Loss: 0.00005329
Iteration 136/1000 | Loss: 0.00005329
Iteration 137/1000 | Loss: 0.00005329
Iteration 138/1000 | Loss: 0.00005329
Iteration 139/1000 | Loss: 0.00005329
Iteration 140/1000 | Loss: 0.00005329
Iteration 141/1000 | Loss: 0.00005329
Iteration 142/1000 | Loss: 0.00005329
Iteration 143/1000 | Loss: 0.00005329
Iteration 144/1000 | Loss: 0.00005329
Iteration 145/1000 | Loss: 0.00005329
Iteration 146/1000 | Loss: 0.00005328
Iteration 147/1000 | Loss: 0.00005328
Iteration 148/1000 | Loss: 0.00005328
Iteration 149/1000 | Loss: 0.00005328
Iteration 150/1000 | Loss: 0.00005328
Iteration 151/1000 | Loss: 0.00005328
Iteration 152/1000 | Loss: 0.00005328
Iteration 153/1000 | Loss: 0.00005328
Iteration 154/1000 | Loss: 0.00005328
Iteration 155/1000 | Loss: 0.00005328
Iteration 156/1000 | Loss: 0.00005328
Iteration 157/1000 | Loss: 0.00005328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [5.328273255145177e-05, 5.328273255145177e-05, 5.328273255145177e-05, 5.328273255145177e-05, 5.328273255145177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.328273255145177e-05

Optimization complete. Final v2v error: 5.68373966217041 mm

Highest mean error: 7.0534515380859375 mm for frame 44

Lowest mean error: 4.508755207061768 mm for frame 121

Saving results

Total time: 99.9062237739563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844842
Iteration 2/25 | Loss: 0.00174713
Iteration 3/25 | Loss: 0.00119793
Iteration 4/25 | Loss: 0.00105940
Iteration 5/25 | Loss: 0.00107648
Iteration 6/25 | Loss: 0.00107661
Iteration 7/25 | Loss: 0.00100907
Iteration 8/25 | Loss: 0.00096467
Iteration 9/25 | Loss: 0.00094687
Iteration 10/25 | Loss: 0.00094041
Iteration 11/25 | Loss: 0.00094139
Iteration 12/25 | Loss: 0.00093883
Iteration 13/25 | Loss: 0.00093713
Iteration 14/25 | Loss: 0.00093625
Iteration 15/25 | Loss: 0.00093581
Iteration 16/25 | Loss: 0.00093573
Iteration 17/25 | Loss: 0.00093572
Iteration 18/25 | Loss: 0.00093572
Iteration 19/25 | Loss: 0.00093572
Iteration 20/25 | Loss: 0.00093572
Iteration 21/25 | Loss: 0.00093572
Iteration 22/25 | Loss: 0.00093572
Iteration 23/25 | Loss: 0.00093572
Iteration 24/25 | Loss: 0.00093572
Iteration 25/25 | Loss: 0.00093572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52207851
Iteration 2/25 | Loss: 0.00070987
Iteration 3/25 | Loss: 0.00070986
Iteration 4/25 | Loss: 0.00070985
Iteration 5/25 | Loss: 0.00070985
Iteration 6/25 | Loss: 0.00070985
Iteration 7/25 | Loss: 0.00070985
Iteration 8/25 | Loss: 0.00070985
Iteration 9/25 | Loss: 0.00070985
Iteration 10/25 | Loss: 0.00070985
Iteration 11/25 | Loss: 0.00070985
Iteration 12/25 | Loss: 0.00070985
Iteration 13/25 | Loss: 0.00070985
Iteration 14/25 | Loss: 0.00070985
Iteration 15/25 | Loss: 0.00070985
Iteration 16/25 | Loss: 0.00070985
Iteration 17/25 | Loss: 0.00070985
Iteration 18/25 | Loss: 0.00070985
Iteration 19/25 | Loss: 0.00070985
Iteration 20/25 | Loss: 0.00070985
Iteration 21/25 | Loss: 0.00070985
Iteration 22/25 | Loss: 0.00070985
Iteration 23/25 | Loss: 0.00070985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007098519708961248, 0.0007098519708961248, 0.0007098519708961248, 0.0007098519708961248, 0.0007098519708961248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007098519708961248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070985
Iteration 2/1000 | Loss: 0.00440244
Iteration 3/1000 | Loss: 0.00140704
Iteration 4/1000 | Loss: 0.00315702
Iteration 5/1000 | Loss: 0.00038891
Iteration 6/1000 | Loss: 0.00022007
Iteration 7/1000 | Loss: 0.00015025
Iteration 8/1000 | Loss: 0.00012185
Iteration 9/1000 | Loss: 0.00302523
Iteration 10/1000 | Loss: 0.00371114
Iteration 11/1000 | Loss: 0.00136327
Iteration 12/1000 | Loss: 0.00126020
Iteration 13/1000 | Loss: 0.00090503
Iteration 14/1000 | Loss: 0.00066892
Iteration 15/1000 | Loss: 0.00023182
Iteration 16/1000 | Loss: 0.00019831
Iteration 17/1000 | Loss: 0.00064860
Iteration 18/1000 | Loss: 0.00018388
Iteration 19/1000 | Loss: 0.00016522
Iteration 20/1000 | Loss: 0.00015349
Iteration 21/1000 | Loss: 0.00091668
Iteration 22/1000 | Loss: 0.00218770
Iteration 23/1000 | Loss: 0.00193413
Iteration 24/1000 | Loss: 0.00267110
Iteration 25/1000 | Loss: 0.00095203
Iteration 26/1000 | Loss: 0.00038707
Iteration 27/1000 | Loss: 0.00126860
Iteration 28/1000 | Loss: 0.00176336
Iteration 29/1000 | Loss: 0.00141582
Iteration 30/1000 | Loss: 0.00055127
Iteration 31/1000 | Loss: 0.00031715
Iteration 32/1000 | Loss: 0.00022326
Iteration 33/1000 | Loss: 0.00046767
Iteration 34/1000 | Loss: 0.00099737
Iteration 35/1000 | Loss: 0.00099759
Iteration 36/1000 | Loss: 0.00050762
Iteration 37/1000 | Loss: 0.00026908
Iteration 38/1000 | Loss: 0.00056445
Iteration 39/1000 | Loss: 0.00042652
Iteration 40/1000 | Loss: 0.00027969
Iteration 41/1000 | Loss: 0.00017690
Iteration 42/1000 | Loss: 0.00014981
Iteration 43/1000 | Loss: 0.00013316
Iteration 44/1000 | Loss: 0.00015586
Iteration 45/1000 | Loss: 0.00011956
Iteration 46/1000 | Loss: 0.00011320
Iteration 47/1000 | Loss: 0.00014558
Iteration 48/1000 | Loss: 0.00008061
Iteration 49/1000 | Loss: 0.00007832
Iteration 50/1000 | Loss: 0.00008377
Iteration 51/1000 | Loss: 0.00006292
Iteration 52/1000 | Loss: 0.00005877
Iteration 53/1000 | Loss: 0.00009197
Iteration 54/1000 | Loss: 0.00007950
Iteration 55/1000 | Loss: 0.00006680
Iteration 56/1000 | Loss: 0.00005328
Iteration 57/1000 | Loss: 0.00006162
Iteration 58/1000 | Loss: 0.00005926
Iteration 59/1000 | Loss: 0.00004958
Iteration 60/1000 | Loss: 0.00004469
Iteration 61/1000 | Loss: 0.00021084
Iteration 62/1000 | Loss: 0.00004854
Iteration 63/1000 | Loss: 0.00024079
Iteration 64/1000 | Loss: 0.00025448
Iteration 65/1000 | Loss: 0.00037516
Iteration 66/1000 | Loss: 0.00018115
Iteration 67/1000 | Loss: 0.00006422
Iteration 68/1000 | Loss: 0.00005456
Iteration 69/1000 | Loss: 0.00019620
Iteration 70/1000 | Loss: 0.00054009
Iteration 71/1000 | Loss: 0.00017853
Iteration 72/1000 | Loss: 0.00006608
Iteration 73/1000 | Loss: 0.00005215
Iteration 74/1000 | Loss: 0.00004401
Iteration 75/1000 | Loss: 0.00003709
Iteration 76/1000 | Loss: 0.00003493
Iteration 77/1000 | Loss: 0.00003347
Iteration 78/1000 | Loss: 0.00003228
Iteration 79/1000 | Loss: 0.00003130
Iteration 80/1000 | Loss: 0.00044334
Iteration 81/1000 | Loss: 0.00018474
Iteration 82/1000 | Loss: 0.00025065
Iteration 83/1000 | Loss: 0.00004354
Iteration 84/1000 | Loss: 0.00003344
Iteration 85/1000 | Loss: 0.00003198
Iteration 86/1000 | Loss: 0.00003097
Iteration 87/1000 | Loss: 0.00003004
Iteration 88/1000 | Loss: 0.00002938
Iteration 89/1000 | Loss: 0.00010772
Iteration 90/1000 | Loss: 0.00002900
Iteration 91/1000 | Loss: 0.00002778
Iteration 92/1000 | Loss: 0.00002695
Iteration 93/1000 | Loss: 0.00002614
Iteration 94/1000 | Loss: 0.00022361
Iteration 95/1000 | Loss: 0.00005353
Iteration 96/1000 | Loss: 0.00003158
Iteration 97/1000 | Loss: 0.00002867
Iteration 98/1000 | Loss: 0.00002734
Iteration 99/1000 | Loss: 0.00002692
Iteration 100/1000 | Loss: 0.00002651
Iteration 101/1000 | Loss: 0.00002617
Iteration 102/1000 | Loss: 0.00002592
Iteration 103/1000 | Loss: 0.00021768
Iteration 104/1000 | Loss: 0.00002844
Iteration 105/1000 | Loss: 0.00002597
Iteration 106/1000 | Loss: 0.00002528
Iteration 107/1000 | Loss: 0.00002500
Iteration 108/1000 | Loss: 0.00002498
Iteration 109/1000 | Loss: 0.00002481
Iteration 110/1000 | Loss: 0.00002471
Iteration 111/1000 | Loss: 0.00002466
Iteration 112/1000 | Loss: 0.00002466
Iteration 113/1000 | Loss: 0.00002466
Iteration 114/1000 | Loss: 0.00002466
Iteration 115/1000 | Loss: 0.00002466
Iteration 116/1000 | Loss: 0.00002466
Iteration 117/1000 | Loss: 0.00002466
Iteration 118/1000 | Loss: 0.00002466
Iteration 119/1000 | Loss: 0.00002466
Iteration 120/1000 | Loss: 0.00002466
Iteration 121/1000 | Loss: 0.00002466
Iteration 122/1000 | Loss: 0.00002465
Iteration 123/1000 | Loss: 0.00002465
Iteration 124/1000 | Loss: 0.00002465
Iteration 125/1000 | Loss: 0.00002456
Iteration 126/1000 | Loss: 0.00002455
Iteration 127/1000 | Loss: 0.00002455
Iteration 128/1000 | Loss: 0.00002454
Iteration 129/1000 | Loss: 0.00002454
Iteration 130/1000 | Loss: 0.00002454
Iteration 131/1000 | Loss: 0.00002453
Iteration 132/1000 | Loss: 0.00002453
Iteration 133/1000 | Loss: 0.00002451
Iteration 134/1000 | Loss: 0.00002451
Iteration 135/1000 | Loss: 0.00002451
Iteration 136/1000 | Loss: 0.00002451
Iteration 137/1000 | Loss: 0.00002451
Iteration 138/1000 | Loss: 0.00002451
Iteration 139/1000 | Loss: 0.00002451
Iteration 140/1000 | Loss: 0.00002450
Iteration 141/1000 | Loss: 0.00002450
Iteration 142/1000 | Loss: 0.00002450
Iteration 143/1000 | Loss: 0.00002446
Iteration 144/1000 | Loss: 0.00002443
Iteration 145/1000 | Loss: 0.00002441
Iteration 146/1000 | Loss: 0.00002440
Iteration 147/1000 | Loss: 0.00002440
Iteration 148/1000 | Loss: 0.00002440
Iteration 149/1000 | Loss: 0.00002439
Iteration 150/1000 | Loss: 0.00002439
Iteration 151/1000 | Loss: 0.00002438
Iteration 152/1000 | Loss: 0.00002438
Iteration 153/1000 | Loss: 0.00002437
Iteration 154/1000 | Loss: 0.00002435
Iteration 155/1000 | Loss: 0.00002435
Iteration 156/1000 | Loss: 0.00002435
Iteration 157/1000 | Loss: 0.00002434
Iteration 158/1000 | Loss: 0.00002434
Iteration 159/1000 | Loss: 0.00002434
Iteration 160/1000 | Loss: 0.00002433
Iteration 161/1000 | Loss: 0.00002432
Iteration 162/1000 | Loss: 0.00002432
Iteration 163/1000 | Loss: 0.00002432
Iteration 164/1000 | Loss: 0.00002431
Iteration 165/1000 | Loss: 0.00002430
Iteration 166/1000 | Loss: 0.00002428
Iteration 167/1000 | Loss: 0.00002427
Iteration 168/1000 | Loss: 0.00002427
Iteration 169/1000 | Loss: 0.00002427
Iteration 170/1000 | Loss: 0.00002427
Iteration 171/1000 | Loss: 0.00002426
Iteration 172/1000 | Loss: 0.00002426
Iteration 173/1000 | Loss: 0.00002425
Iteration 174/1000 | Loss: 0.00002425
Iteration 175/1000 | Loss: 0.00002425
Iteration 176/1000 | Loss: 0.00002423
Iteration 177/1000 | Loss: 0.00002423
Iteration 178/1000 | Loss: 0.00002423
Iteration 179/1000 | Loss: 0.00002423
Iteration 180/1000 | Loss: 0.00002423
Iteration 181/1000 | Loss: 0.00002422
Iteration 182/1000 | Loss: 0.00002422
Iteration 183/1000 | Loss: 0.00002422
Iteration 184/1000 | Loss: 0.00002422
Iteration 185/1000 | Loss: 0.00002422
Iteration 186/1000 | Loss: 0.00002422
Iteration 187/1000 | Loss: 0.00002422
Iteration 188/1000 | Loss: 0.00002422
Iteration 189/1000 | Loss: 0.00002422
Iteration 190/1000 | Loss: 0.00002422
Iteration 191/1000 | Loss: 0.00002422
Iteration 192/1000 | Loss: 0.00002422
Iteration 193/1000 | Loss: 0.00002422
Iteration 194/1000 | Loss: 0.00002422
Iteration 195/1000 | Loss: 0.00002422
Iteration 196/1000 | Loss: 0.00002422
Iteration 197/1000 | Loss: 0.00002422
Iteration 198/1000 | Loss: 0.00002422
Iteration 199/1000 | Loss: 0.00002422
Iteration 200/1000 | Loss: 0.00002421
Iteration 201/1000 | Loss: 0.00002421
Iteration 202/1000 | Loss: 0.00002421
Iteration 203/1000 | Loss: 0.00002421
Iteration 204/1000 | Loss: 0.00002421
Iteration 205/1000 | Loss: 0.00002421
Iteration 206/1000 | Loss: 0.00002421
Iteration 207/1000 | Loss: 0.00002421
Iteration 208/1000 | Loss: 0.00002420
Iteration 209/1000 | Loss: 0.00002420
Iteration 210/1000 | Loss: 0.00002420
Iteration 211/1000 | Loss: 0.00002420
Iteration 212/1000 | Loss: 0.00002420
Iteration 213/1000 | Loss: 0.00002419
Iteration 214/1000 | Loss: 0.00002419
Iteration 215/1000 | Loss: 0.00002419
Iteration 216/1000 | Loss: 0.00002419
Iteration 217/1000 | Loss: 0.00002419
Iteration 218/1000 | Loss: 0.00002419
Iteration 219/1000 | Loss: 0.00002419
Iteration 220/1000 | Loss: 0.00002419
Iteration 221/1000 | Loss: 0.00002419
Iteration 222/1000 | Loss: 0.00002419
Iteration 223/1000 | Loss: 0.00002419
Iteration 224/1000 | Loss: 0.00002419
Iteration 225/1000 | Loss: 0.00002419
Iteration 226/1000 | Loss: 0.00002419
Iteration 227/1000 | Loss: 0.00002419
Iteration 228/1000 | Loss: 0.00002419
Iteration 229/1000 | Loss: 0.00002419
Iteration 230/1000 | Loss: 0.00002419
Iteration 231/1000 | Loss: 0.00002418
Iteration 232/1000 | Loss: 0.00002418
Iteration 233/1000 | Loss: 0.00002418
Iteration 234/1000 | Loss: 0.00002418
Iteration 235/1000 | Loss: 0.00002418
Iteration 236/1000 | Loss: 0.00002418
Iteration 237/1000 | Loss: 0.00002418
Iteration 238/1000 | Loss: 0.00002418
Iteration 239/1000 | Loss: 0.00002417
Iteration 240/1000 | Loss: 0.00002417
Iteration 241/1000 | Loss: 0.00002417
Iteration 242/1000 | Loss: 0.00002417
Iteration 243/1000 | Loss: 0.00002417
Iteration 244/1000 | Loss: 0.00002417
Iteration 245/1000 | Loss: 0.00002417
Iteration 246/1000 | Loss: 0.00002417
Iteration 247/1000 | Loss: 0.00002417
Iteration 248/1000 | Loss: 0.00002417
Iteration 249/1000 | Loss: 0.00002417
Iteration 250/1000 | Loss: 0.00002417
Iteration 251/1000 | Loss: 0.00002417
Iteration 252/1000 | Loss: 0.00002417
Iteration 253/1000 | Loss: 0.00002417
Iteration 254/1000 | Loss: 0.00002416
Iteration 255/1000 | Loss: 0.00002416
Iteration 256/1000 | Loss: 0.00002416
Iteration 257/1000 | Loss: 0.00002416
Iteration 258/1000 | Loss: 0.00002416
Iteration 259/1000 | Loss: 0.00002416
Iteration 260/1000 | Loss: 0.00002416
Iteration 261/1000 | Loss: 0.00002416
Iteration 262/1000 | Loss: 0.00002416
Iteration 263/1000 | Loss: 0.00002416
Iteration 264/1000 | Loss: 0.00002416
Iteration 265/1000 | Loss: 0.00002416
Iteration 266/1000 | Loss: 0.00002416
Iteration 267/1000 | Loss: 0.00002416
Iteration 268/1000 | Loss: 0.00002416
Iteration 269/1000 | Loss: 0.00002416
Iteration 270/1000 | Loss: 0.00002416
Iteration 271/1000 | Loss: 0.00002416
Iteration 272/1000 | Loss: 0.00002416
Iteration 273/1000 | Loss: 0.00002416
Iteration 274/1000 | Loss: 0.00002416
Iteration 275/1000 | Loss: 0.00002416
Iteration 276/1000 | Loss: 0.00002416
Iteration 277/1000 | Loss: 0.00002416
Iteration 278/1000 | Loss: 0.00002416
Iteration 279/1000 | Loss: 0.00002416
Iteration 280/1000 | Loss: 0.00002416
Iteration 281/1000 | Loss: 0.00002416
Iteration 282/1000 | Loss: 0.00002416
Iteration 283/1000 | Loss: 0.00002416
Iteration 284/1000 | Loss: 0.00002416
Iteration 285/1000 | Loss: 0.00002416
Iteration 286/1000 | Loss: 0.00002416
Iteration 287/1000 | Loss: 0.00002416
Iteration 288/1000 | Loss: 0.00002416
Iteration 289/1000 | Loss: 0.00002416
Iteration 290/1000 | Loss: 0.00002416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [2.4161739929695614e-05, 2.4161739929695614e-05, 2.4161739929695614e-05, 2.4161739929695614e-05, 2.4161739929695614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4161739929695614e-05

Optimization complete. Final v2v error: 3.974458694458008 mm

Highest mean error: 6.046519756317139 mm for frame 149

Lowest mean error: 3.462880849838257 mm for frame 151

Saving results

Total time: 223.95271158218384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845724
Iteration 2/25 | Loss: 0.00116518
Iteration 3/25 | Loss: 0.00085485
Iteration 4/25 | Loss: 0.00083100
Iteration 5/25 | Loss: 0.00082560
Iteration 6/25 | Loss: 0.00082424
Iteration 7/25 | Loss: 0.00082414
Iteration 8/25 | Loss: 0.00082414
Iteration 9/25 | Loss: 0.00082414
Iteration 10/25 | Loss: 0.00082414
Iteration 11/25 | Loss: 0.00082414
Iteration 12/25 | Loss: 0.00082414
Iteration 13/25 | Loss: 0.00082414
Iteration 14/25 | Loss: 0.00082414
Iteration 15/25 | Loss: 0.00082414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008241405012086034, 0.0008241405012086034, 0.0008241405012086034, 0.0008241405012086034, 0.0008241405012086034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008241405012086034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07353055
Iteration 2/25 | Loss: 0.00043767
Iteration 3/25 | Loss: 0.00043767
Iteration 4/25 | Loss: 0.00043767
Iteration 5/25 | Loss: 0.00043767
Iteration 6/25 | Loss: 0.00043767
Iteration 7/25 | Loss: 0.00043766
Iteration 8/25 | Loss: 0.00043766
Iteration 9/25 | Loss: 0.00043766
Iteration 10/25 | Loss: 0.00043766
Iteration 11/25 | Loss: 0.00043766
Iteration 12/25 | Loss: 0.00043766
Iteration 13/25 | Loss: 0.00043766
Iteration 14/25 | Loss: 0.00043766
Iteration 15/25 | Loss: 0.00043766
Iteration 16/25 | Loss: 0.00043766
Iteration 17/25 | Loss: 0.00043766
Iteration 18/25 | Loss: 0.00043766
Iteration 19/25 | Loss: 0.00043766
Iteration 20/25 | Loss: 0.00043766
Iteration 21/25 | Loss: 0.00043766
Iteration 22/25 | Loss: 0.00043766
Iteration 23/25 | Loss: 0.00043766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00043766386806964874, 0.00043766386806964874, 0.00043766386806964874, 0.00043766386806964874, 0.00043766386806964874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043766386806964874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043766
Iteration 2/1000 | Loss: 0.00002791
Iteration 3/1000 | Loss: 0.00002420
Iteration 4/1000 | Loss: 0.00002293
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002076
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002038
Iteration 10/1000 | Loss: 0.00002032
Iteration 11/1000 | Loss: 0.00002031
Iteration 12/1000 | Loss: 0.00002031
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002026
Iteration 15/1000 | Loss: 0.00002026
Iteration 16/1000 | Loss: 0.00002026
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002018
Iteration 21/1000 | Loss: 0.00002016
Iteration 22/1000 | Loss: 0.00002016
Iteration 23/1000 | Loss: 0.00002016
Iteration 24/1000 | Loss: 0.00002016
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002016
Iteration 27/1000 | Loss: 0.00002016
Iteration 28/1000 | Loss: 0.00002016
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002016
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002015
Iteration 34/1000 | Loss: 0.00002014
Iteration 35/1000 | Loss: 0.00002013
Iteration 36/1000 | Loss: 0.00002013
Iteration 37/1000 | Loss: 0.00002013
Iteration 38/1000 | Loss: 0.00002013
Iteration 39/1000 | Loss: 0.00002013
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002012
Iteration 42/1000 | Loss: 0.00002012
Iteration 43/1000 | Loss: 0.00002012
Iteration 44/1000 | Loss: 0.00002012
Iteration 45/1000 | Loss: 0.00002012
Iteration 46/1000 | Loss: 0.00002012
Iteration 47/1000 | Loss: 0.00002012
Iteration 48/1000 | Loss: 0.00002012
Iteration 49/1000 | Loss: 0.00002012
Iteration 50/1000 | Loss: 0.00002012
Iteration 51/1000 | Loss: 0.00002010
Iteration 52/1000 | Loss: 0.00002010
Iteration 53/1000 | Loss: 0.00002010
Iteration 54/1000 | Loss: 0.00002010
Iteration 55/1000 | Loss: 0.00002010
Iteration 56/1000 | Loss: 0.00002010
Iteration 57/1000 | Loss: 0.00002010
Iteration 58/1000 | Loss: 0.00002010
Iteration 59/1000 | Loss: 0.00002009
Iteration 60/1000 | Loss: 0.00002009
Iteration 61/1000 | Loss: 0.00002009
Iteration 62/1000 | Loss: 0.00002009
Iteration 63/1000 | Loss: 0.00002009
Iteration 64/1000 | Loss: 0.00002009
Iteration 65/1000 | Loss: 0.00002009
Iteration 66/1000 | Loss: 0.00002009
Iteration 67/1000 | Loss: 0.00002009
Iteration 68/1000 | Loss: 0.00002009
Iteration 69/1000 | Loss: 0.00002009
Iteration 70/1000 | Loss: 0.00002009
Iteration 71/1000 | Loss: 0.00002008
Iteration 72/1000 | Loss: 0.00002008
Iteration 73/1000 | Loss: 0.00002008
Iteration 74/1000 | Loss: 0.00002008
Iteration 75/1000 | Loss: 0.00002008
Iteration 76/1000 | Loss: 0.00002008
Iteration 77/1000 | Loss: 0.00002008
Iteration 78/1000 | Loss: 0.00002008
Iteration 79/1000 | Loss: 0.00002008
Iteration 80/1000 | Loss: 0.00002008
Iteration 81/1000 | Loss: 0.00002007
Iteration 82/1000 | Loss: 0.00002007
Iteration 83/1000 | Loss: 0.00002007
Iteration 84/1000 | Loss: 0.00002007
Iteration 85/1000 | Loss: 0.00002006
Iteration 86/1000 | Loss: 0.00002006
Iteration 87/1000 | Loss: 0.00002006
Iteration 88/1000 | Loss: 0.00002006
Iteration 89/1000 | Loss: 0.00002005
Iteration 90/1000 | Loss: 0.00002005
Iteration 91/1000 | Loss: 0.00002005
Iteration 92/1000 | Loss: 0.00002003
Iteration 93/1000 | Loss: 0.00002003
Iteration 94/1000 | Loss: 0.00002003
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002002
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002001
Iteration 108/1000 | Loss: 0.00002001
Iteration 109/1000 | Loss: 0.00002001
Iteration 110/1000 | Loss: 0.00002001
Iteration 111/1000 | Loss: 0.00002000
Iteration 112/1000 | Loss: 0.00002000
Iteration 113/1000 | Loss: 0.00002000
Iteration 114/1000 | Loss: 0.00002000
Iteration 115/1000 | Loss: 0.00002000
Iteration 116/1000 | Loss: 0.00002000
Iteration 117/1000 | Loss: 0.00002000
Iteration 118/1000 | Loss: 0.00002000
Iteration 119/1000 | Loss: 0.00001999
Iteration 120/1000 | Loss: 0.00001999
Iteration 121/1000 | Loss: 0.00001999
Iteration 122/1000 | Loss: 0.00001999
Iteration 123/1000 | Loss: 0.00001999
Iteration 124/1000 | Loss: 0.00001999
Iteration 125/1000 | Loss: 0.00001999
Iteration 126/1000 | Loss: 0.00001999
Iteration 127/1000 | Loss: 0.00001998
Iteration 128/1000 | Loss: 0.00001998
Iteration 129/1000 | Loss: 0.00001998
Iteration 130/1000 | Loss: 0.00001998
Iteration 131/1000 | Loss: 0.00001998
Iteration 132/1000 | Loss: 0.00001997
Iteration 133/1000 | Loss: 0.00001997
Iteration 134/1000 | Loss: 0.00001997
Iteration 135/1000 | Loss: 0.00001997
Iteration 136/1000 | Loss: 0.00001997
Iteration 137/1000 | Loss: 0.00001997
Iteration 138/1000 | Loss: 0.00001997
Iteration 139/1000 | Loss: 0.00001997
Iteration 140/1000 | Loss: 0.00001997
Iteration 141/1000 | Loss: 0.00001997
Iteration 142/1000 | Loss: 0.00001997
Iteration 143/1000 | Loss: 0.00001997
Iteration 144/1000 | Loss: 0.00001997
Iteration 145/1000 | Loss: 0.00001997
Iteration 146/1000 | Loss: 0.00001997
Iteration 147/1000 | Loss: 0.00001997
Iteration 148/1000 | Loss: 0.00001996
Iteration 149/1000 | Loss: 0.00001996
Iteration 150/1000 | Loss: 0.00001996
Iteration 151/1000 | Loss: 0.00001996
Iteration 152/1000 | Loss: 0.00001996
Iteration 153/1000 | Loss: 0.00001996
Iteration 154/1000 | Loss: 0.00001996
Iteration 155/1000 | Loss: 0.00001996
Iteration 156/1000 | Loss: 0.00001996
Iteration 157/1000 | Loss: 0.00001996
Iteration 158/1000 | Loss: 0.00001996
Iteration 159/1000 | Loss: 0.00001996
Iteration 160/1000 | Loss: 0.00001996
Iteration 161/1000 | Loss: 0.00001995
Iteration 162/1000 | Loss: 0.00001995
Iteration 163/1000 | Loss: 0.00001995
Iteration 164/1000 | Loss: 0.00001995
Iteration 165/1000 | Loss: 0.00001995
Iteration 166/1000 | Loss: 0.00001995
Iteration 167/1000 | Loss: 0.00001995
Iteration 168/1000 | Loss: 0.00001995
Iteration 169/1000 | Loss: 0.00001995
Iteration 170/1000 | Loss: 0.00001995
Iteration 171/1000 | Loss: 0.00001995
Iteration 172/1000 | Loss: 0.00001995
Iteration 173/1000 | Loss: 0.00001995
Iteration 174/1000 | Loss: 0.00001995
Iteration 175/1000 | Loss: 0.00001995
Iteration 176/1000 | Loss: 0.00001995
Iteration 177/1000 | Loss: 0.00001995
Iteration 178/1000 | Loss: 0.00001995
Iteration 179/1000 | Loss: 0.00001995
Iteration 180/1000 | Loss: 0.00001995
Iteration 181/1000 | Loss: 0.00001995
Iteration 182/1000 | Loss: 0.00001994
Iteration 183/1000 | Loss: 0.00001994
Iteration 184/1000 | Loss: 0.00001994
Iteration 185/1000 | Loss: 0.00001994
Iteration 186/1000 | Loss: 0.00001994
Iteration 187/1000 | Loss: 0.00001994
Iteration 188/1000 | Loss: 0.00001994
Iteration 189/1000 | Loss: 0.00001994
Iteration 190/1000 | Loss: 0.00001994
Iteration 191/1000 | Loss: 0.00001994
Iteration 192/1000 | Loss: 0.00001994
Iteration 193/1000 | Loss: 0.00001994
Iteration 194/1000 | Loss: 0.00001994
Iteration 195/1000 | Loss: 0.00001994
Iteration 196/1000 | Loss: 0.00001994
Iteration 197/1000 | Loss: 0.00001994
Iteration 198/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.9940744095947593e-05, 1.9940744095947593e-05, 1.9940744095947593e-05, 1.9940744095947593e-05, 1.9940744095947593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9940744095947593e-05

Optimization complete. Final v2v error: 3.8051955699920654 mm

Highest mean error: 3.9964098930358887 mm for frame 107

Lowest mean error: 3.5059447288513184 mm for frame 44

Saving results

Total time: 34.617043018341064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01131368
Iteration 2/25 | Loss: 0.00176941
Iteration 3/25 | Loss: 0.00107081
Iteration 4/25 | Loss: 0.00098958
Iteration 5/25 | Loss: 0.00096301
Iteration 6/25 | Loss: 0.00097476
Iteration 7/25 | Loss: 0.00095713
Iteration 8/25 | Loss: 0.00096622
Iteration 9/25 | Loss: 0.00095964
Iteration 10/25 | Loss: 0.00095181
Iteration 11/25 | Loss: 0.00095323
Iteration 12/25 | Loss: 0.00095172
Iteration 13/25 | Loss: 0.00094752
Iteration 14/25 | Loss: 0.00094837
Iteration 15/25 | Loss: 0.00094361
Iteration 16/25 | Loss: 0.00094868
Iteration 17/25 | Loss: 0.00095489
Iteration 18/25 | Loss: 0.00095019
Iteration 19/25 | Loss: 0.00094411
Iteration 20/25 | Loss: 0.00095531
Iteration 21/25 | Loss: 0.00094992
Iteration 22/25 | Loss: 0.00094261
Iteration 23/25 | Loss: 0.00094231
Iteration 24/25 | Loss: 0.00093978
Iteration 25/25 | Loss: 0.00093754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06542242
Iteration 2/25 | Loss: 0.00081848
Iteration 3/25 | Loss: 0.00081848
Iteration 4/25 | Loss: 0.00081848
Iteration 5/25 | Loss: 0.00081847
Iteration 6/25 | Loss: 0.00081847
Iteration 7/25 | Loss: 0.00081847
Iteration 8/25 | Loss: 0.00081847
Iteration 9/25 | Loss: 0.00081847
Iteration 10/25 | Loss: 0.00081847
Iteration 11/25 | Loss: 0.00081847
Iteration 12/25 | Loss: 0.00081847
Iteration 13/25 | Loss: 0.00081847
Iteration 14/25 | Loss: 0.00081847
Iteration 15/25 | Loss: 0.00081847
Iteration 16/25 | Loss: 0.00081847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008184735779650509, 0.0008184735779650509, 0.0008184735779650509, 0.0008184735779650509, 0.0008184735779650509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008184735779650509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081847
Iteration 2/1000 | Loss: 0.00045917
Iteration 3/1000 | Loss: 0.00058067
Iteration 4/1000 | Loss: 0.00059173
Iteration 5/1000 | Loss: 0.00080188
Iteration 6/1000 | Loss: 0.00073491
Iteration 7/1000 | Loss: 0.00078387
Iteration 8/1000 | Loss: 0.00088736
Iteration 9/1000 | Loss: 0.00038704
Iteration 10/1000 | Loss: 0.00052028
Iteration 11/1000 | Loss: 0.00067372
Iteration 12/1000 | Loss: 0.00073945
Iteration 13/1000 | Loss: 0.00045300
Iteration 14/1000 | Loss: 0.00049928
Iteration 15/1000 | Loss: 0.00100808
Iteration 16/1000 | Loss: 0.00065897
Iteration 17/1000 | Loss: 0.00073818
Iteration 18/1000 | Loss: 0.00093879
Iteration 19/1000 | Loss: 0.00036177
Iteration 20/1000 | Loss: 0.00067294
Iteration 21/1000 | Loss: 0.00113115
Iteration 22/1000 | Loss: 0.00036989
Iteration 23/1000 | Loss: 0.00039488
Iteration 24/1000 | Loss: 0.00060047
Iteration 25/1000 | Loss: 0.00050569
Iteration 26/1000 | Loss: 0.00055851
Iteration 27/1000 | Loss: 0.00058883
Iteration 28/1000 | Loss: 0.00058882
Iteration 29/1000 | Loss: 0.00074879
Iteration 30/1000 | Loss: 0.00073229
Iteration 31/1000 | Loss: 0.00048883
Iteration 32/1000 | Loss: 0.00068194
Iteration 33/1000 | Loss: 0.00074398
Iteration 34/1000 | Loss: 0.00066546
Iteration 35/1000 | Loss: 0.00072343
Iteration 36/1000 | Loss: 0.00058022
Iteration 37/1000 | Loss: 0.00061726
Iteration 38/1000 | Loss: 0.00053889
Iteration 39/1000 | Loss: 0.00049475
Iteration 40/1000 | Loss: 0.00056040
Iteration 41/1000 | Loss: 0.00063722
Iteration 42/1000 | Loss: 0.00043836
Iteration 43/1000 | Loss: 0.00018642
Iteration 44/1000 | Loss: 0.00042466
Iteration 45/1000 | Loss: 0.00034811
Iteration 46/1000 | Loss: 0.00056893
Iteration 47/1000 | Loss: 0.00059224
Iteration 48/1000 | Loss: 0.00031724
Iteration 49/1000 | Loss: 0.00048989
Iteration 50/1000 | Loss: 0.00100264
Iteration 51/1000 | Loss: 0.00099865
Iteration 52/1000 | Loss: 0.00042324
Iteration 53/1000 | Loss: 0.00041618
Iteration 54/1000 | Loss: 0.00044787
Iteration 55/1000 | Loss: 0.00048927
Iteration 56/1000 | Loss: 0.00040867
Iteration 57/1000 | Loss: 0.00049212
Iteration 58/1000 | Loss: 0.00028139
Iteration 59/1000 | Loss: 0.00032298
Iteration 60/1000 | Loss: 0.00052971
Iteration 61/1000 | Loss: 0.00045951
Iteration 62/1000 | Loss: 0.00044686
Iteration 63/1000 | Loss: 0.00035050
Iteration 64/1000 | Loss: 0.00028559
Iteration 65/1000 | Loss: 0.00021111
Iteration 66/1000 | Loss: 0.00054425
Iteration 67/1000 | Loss: 0.00079935
Iteration 68/1000 | Loss: 0.00046286
Iteration 69/1000 | Loss: 0.00069540
Iteration 70/1000 | Loss: 0.00042715
Iteration 71/1000 | Loss: 0.00035354
Iteration 72/1000 | Loss: 0.00028551
Iteration 73/1000 | Loss: 0.00036519
Iteration 74/1000 | Loss: 0.00043751
Iteration 75/1000 | Loss: 0.00042817
Iteration 76/1000 | Loss: 0.00054664
Iteration 77/1000 | Loss: 0.00026142
Iteration 78/1000 | Loss: 0.00011169
Iteration 79/1000 | Loss: 0.00024222
Iteration 80/1000 | Loss: 0.00037256
Iteration 81/1000 | Loss: 0.00023879
Iteration 82/1000 | Loss: 0.00105105
Iteration 83/1000 | Loss: 0.00044088
Iteration 84/1000 | Loss: 0.00021561
Iteration 85/1000 | Loss: 0.00049822
Iteration 86/1000 | Loss: 0.00045451
Iteration 87/1000 | Loss: 0.00038276
Iteration 88/1000 | Loss: 0.00035543
Iteration 89/1000 | Loss: 0.00044548
Iteration 90/1000 | Loss: 0.00045681
Iteration 91/1000 | Loss: 0.00040542
Iteration 92/1000 | Loss: 0.00033930
Iteration 93/1000 | Loss: 0.00053287
Iteration 94/1000 | Loss: 0.00031980
Iteration 95/1000 | Loss: 0.00068587
Iteration 96/1000 | Loss: 0.00074160
Iteration 97/1000 | Loss: 0.00034714
Iteration 98/1000 | Loss: 0.00058253
Iteration 99/1000 | Loss: 0.00086064
Iteration 100/1000 | Loss: 0.00089014
Iteration 101/1000 | Loss: 0.00102613
Iteration 102/1000 | Loss: 0.00093658
Iteration 103/1000 | Loss: 0.00020394
Iteration 104/1000 | Loss: 0.00007392
Iteration 105/1000 | Loss: 0.00027974
Iteration 106/1000 | Loss: 0.00022006
Iteration 107/1000 | Loss: 0.00021861
Iteration 108/1000 | Loss: 0.00031677
Iteration 109/1000 | Loss: 0.00027187
Iteration 110/1000 | Loss: 0.00013541
Iteration 111/1000 | Loss: 0.00006499
Iteration 112/1000 | Loss: 0.00030249
Iteration 113/1000 | Loss: 0.00020303
Iteration 114/1000 | Loss: 0.00040301
Iteration 115/1000 | Loss: 0.00021811
Iteration 116/1000 | Loss: 0.00046687
Iteration 117/1000 | Loss: 0.00025362
Iteration 118/1000 | Loss: 0.00006026
Iteration 119/1000 | Loss: 0.00019921
Iteration 120/1000 | Loss: 0.00024172
Iteration 121/1000 | Loss: 0.00022715
Iteration 122/1000 | Loss: 0.00049190
Iteration 123/1000 | Loss: 0.00035262
Iteration 124/1000 | Loss: 0.00047805
Iteration 125/1000 | Loss: 0.00023073
Iteration 126/1000 | Loss: 0.00028440
Iteration 127/1000 | Loss: 0.00023616
Iteration 128/1000 | Loss: 0.00022919
Iteration 129/1000 | Loss: 0.00015332
Iteration 130/1000 | Loss: 0.00009294
Iteration 131/1000 | Loss: 0.00024222
Iteration 132/1000 | Loss: 0.00017457
Iteration 133/1000 | Loss: 0.00017146
Iteration 134/1000 | Loss: 0.00019613
Iteration 135/1000 | Loss: 0.00016453
Iteration 136/1000 | Loss: 0.00016879
Iteration 137/1000 | Loss: 0.00023697
Iteration 138/1000 | Loss: 0.00022932
Iteration 139/1000 | Loss: 0.00032494
Iteration 140/1000 | Loss: 0.00033099
Iteration 141/1000 | Loss: 0.00018410
Iteration 142/1000 | Loss: 0.00014873
Iteration 143/1000 | Loss: 0.00022405
Iteration 144/1000 | Loss: 0.00024522
Iteration 145/1000 | Loss: 0.00021382
Iteration 146/1000 | Loss: 0.00022083
Iteration 147/1000 | Loss: 0.00023688
Iteration 148/1000 | Loss: 0.00023297
Iteration 149/1000 | Loss: 0.00017189
Iteration 150/1000 | Loss: 0.00022377
Iteration 151/1000 | Loss: 0.00017230
Iteration 152/1000 | Loss: 0.00018760
Iteration 153/1000 | Loss: 0.00017718
Iteration 154/1000 | Loss: 0.00018620
Iteration 155/1000 | Loss: 0.00017529
Iteration 156/1000 | Loss: 0.00015104
Iteration 157/1000 | Loss: 0.00030854
Iteration 158/1000 | Loss: 0.00009888
Iteration 159/1000 | Loss: 0.00011008
Iteration 160/1000 | Loss: 0.00025289
Iteration 161/1000 | Loss: 0.00031256
Iteration 162/1000 | Loss: 0.00005841
Iteration 163/1000 | Loss: 0.00034520
Iteration 164/1000 | Loss: 0.00016870
Iteration 165/1000 | Loss: 0.00024709
Iteration 166/1000 | Loss: 0.00023900
Iteration 167/1000 | Loss: 0.00022418
Iteration 168/1000 | Loss: 0.00029372
Iteration 169/1000 | Loss: 0.00032684
Iteration 170/1000 | Loss: 0.00031063
Iteration 171/1000 | Loss: 0.00027435
Iteration 172/1000 | Loss: 0.00031882
Iteration 173/1000 | Loss: 0.00028732
Iteration 174/1000 | Loss: 0.00019002
Iteration 175/1000 | Loss: 0.00022655
Iteration 176/1000 | Loss: 0.00008582
Iteration 177/1000 | Loss: 0.00029149
Iteration 178/1000 | Loss: 0.00012760
Iteration 179/1000 | Loss: 0.00018437
Iteration 180/1000 | Loss: 0.00023345
Iteration 181/1000 | Loss: 0.00024955
Iteration 182/1000 | Loss: 0.00026755
Iteration 183/1000 | Loss: 0.00019791
Iteration 184/1000 | Loss: 0.00013606
Iteration 185/1000 | Loss: 0.00018002
Iteration 186/1000 | Loss: 0.00036575
Iteration 187/1000 | Loss: 0.00021001
Iteration 188/1000 | Loss: 0.00020598
Iteration 189/1000 | Loss: 0.00017823
Iteration 190/1000 | Loss: 0.00012952
Iteration 191/1000 | Loss: 0.00017896
Iteration 192/1000 | Loss: 0.00062195
Iteration 193/1000 | Loss: 0.00014273
Iteration 194/1000 | Loss: 0.00009971
Iteration 195/1000 | Loss: 0.00025747
Iteration 196/1000 | Loss: 0.00036181
Iteration 197/1000 | Loss: 0.00006317
Iteration 198/1000 | Loss: 0.00024082
Iteration 199/1000 | Loss: 0.00007913
Iteration 200/1000 | Loss: 0.00016100
Iteration 201/1000 | Loss: 0.00005250
Iteration 202/1000 | Loss: 0.00014848
Iteration 203/1000 | Loss: 0.00015133
Iteration 204/1000 | Loss: 0.00014756
Iteration 205/1000 | Loss: 0.00005531
Iteration 206/1000 | Loss: 0.00011353
Iteration 207/1000 | Loss: 0.00009316
Iteration 208/1000 | Loss: 0.00008336
Iteration 209/1000 | Loss: 0.00012459
Iteration 210/1000 | Loss: 0.00021458
Iteration 211/1000 | Loss: 0.00020954
Iteration 212/1000 | Loss: 0.00024517
Iteration 213/1000 | Loss: 0.00025231
Iteration 214/1000 | Loss: 0.00066087
Iteration 215/1000 | Loss: 0.00025349
Iteration 216/1000 | Loss: 0.00042647
Iteration 217/1000 | Loss: 0.00014485
Iteration 218/1000 | Loss: 0.00025255
Iteration 219/1000 | Loss: 0.00014323
Iteration 220/1000 | Loss: 0.00013757
Iteration 221/1000 | Loss: 0.00010079
Iteration 222/1000 | Loss: 0.00050296
Iteration 223/1000 | Loss: 0.00014530
Iteration 224/1000 | Loss: 0.00074257
Iteration 225/1000 | Loss: 0.00055815
Iteration 226/1000 | Loss: 0.00041544
Iteration 227/1000 | Loss: 0.00004240
Iteration 228/1000 | Loss: 0.00003264
Iteration 229/1000 | Loss: 0.00002962
Iteration 230/1000 | Loss: 0.00002862
Iteration 231/1000 | Loss: 0.00002807
Iteration 232/1000 | Loss: 0.00002745
Iteration 233/1000 | Loss: 0.00002722
Iteration 234/1000 | Loss: 0.00002701
Iteration 235/1000 | Loss: 0.00002684
Iteration 236/1000 | Loss: 0.00002672
Iteration 237/1000 | Loss: 0.00002668
Iteration 238/1000 | Loss: 0.00002666
Iteration 239/1000 | Loss: 0.00002666
Iteration 240/1000 | Loss: 0.00002665
Iteration 241/1000 | Loss: 0.00002665
Iteration 242/1000 | Loss: 0.00002661
Iteration 243/1000 | Loss: 0.00002657
Iteration 244/1000 | Loss: 0.00002655
Iteration 245/1000 | Loss: 0.00002655
Iteration 246/1000 | Loss: 0.00002654
Iteration 247/1000 | Loss: 0.00002654
Iteration 248/1000 | Loss: 0.00002654
Iteration 249/1000 | Loss: 0.00002654
Iteration 250/1000 | Loss: 0.00002651
Iteration 251/1000 | Loss: 0.00002651
Iteration 252/1000 | Loss: 0.00002650
Iteration 253/1000 | Loss: 0.00002650
Iteration 254/1000 | Loss: 0.00002647
Iteration 255/1000 | Loss: 0.00002647
Iteration 256/1000 | Loss: 0.00002644
Iteration 257/1000 | Loss: 0.00002644
Iteration 258/1000 | Loss: 0.00002643
Iteration 259/1000 | Loss: 0.00002638
Iteration 260/1000 | Loss: 0.00002638
Iteration 261/1000 | Loss: 0.00002638
Iteration 262/1000 | Loss: 0.00002637
Iteration 263/1000 | Loss: 0.00002637
Iteration 264/1000 | Loss: 0.00002637
Iteration 265/1000 | Loss: 0.00002637
Iteration 266/1000 | Loss: 0.00002637
Iteration 267/1000 | Loss: 0.00002637
Iteration 268/1000 | Loss: 0.00002637
Iteration 269/1000 | Loss: 0.00002636
Iteration 270/1000 | Loss: 0.00002636
Iteration 271/1000 | Loss: 0.00002636
Iteration 272/1000 | Loss: 0.00002636
Iteration 273/1000 | Loss: 0.00002635
Iteration 274/1000 | Loss: 0.00002635
Iteration 275/1000 | Loss: 0.00002635
Iteration 276/1000 | Loss: 0.00002635
Iteration 277/1000 | Loss: 0.00002635
Iteration 278/1000 | Loss: 0.00002635
Iteration 279/1000 | Loss: 0.00002635
Iteration 280/1000 | Loss: 0.00002635
Iteration 281/1000 | Loss: 0.00002635
Iteration 282/1000 | Loss: 0.00002635
Iteration 283/1000 | Loss: 0.00002635
Iteration 284/1000 | Loss: 0.00002635
Iteration 285/1000 | Loss: 0.00002635
Iteration 286/1000 | Loss: 0.00002635
Iteration 287/1000 | Loss: 0.00002635
Iteration 288/1000 | Loss: 0.00002635
Iteration 289/1000 | Loss: 0.00002635
Iteration 290/1000 | Loss: 0.00002635
Iteration 291/1000 | Loss: 0.00002635
Iteration 292/1000 | Loss: 0.00002635
Iteration 293/1000 | Loss: 0.00002635
Iteration 294/1000 | Loss: 0.00002635
Iteration 295/1000 | Loss: 0.00002635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [2.6345236619818024e-05, 2.6345236619818024e-05, 2.6345236619818024e-05, 2.6345236619818024e-05, 2.6345236619818024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6345236619818024e-05

Optimization complete. Final v2v error: 4.182861804962158 mm

Highest mean error: 5.586688995361328 mm for frame 153

Lowest mean error: 3.774685859680176 mm for frame 125

Saving results

Total time: 405.0500547885895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045070
Iteration 2/25 | Loss: 0.01045070
Iteration 3/25 | Loss: 0.00381374
Iteration 4/25 | Loss: 0.00198137
Iteration 5/25 | Loss: 0.00172254
Iteration 6/25 | Loss: 0.00148606
Iteration 7/25 | Loss: 0.00141789
Iteration 8/25 | Loss: 0.00130514
Iteration 9/25 | Loss: 0.00122930
Iteration 10/25 | Loss: 0.00115527
Iteration 11/25 | Loss: 0.00107286
Iteration 12/25 | Loss: 0.00102476
Iteration 13/25 | Loss: 0.00098414
Iteration 14/25 | Loss: 0.00096614
Iteration 15/25 | Loss: 0.00094838
Iteration 16/25 | Loss: 0.00094801
Iteration 17/25 | Loss: 0.00094229
Iteration 18/25 | Loss: 0.00094689
Iteration 19/25 | Loss: 0.00094803
Iteration 20/25 | Loss: 0.00094561
Iteration 21/25 | Loss: 0.00094333
Iteration 22/25 | Loss: 0.00093555
Iteration 23/25 | Loss: 0.00093464
Iteration 24/25 | Loss: 0.00093905
Iteration 25/25 | Loss: 0.00093324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50943363
Iteration 2/25 | Loss: 0.00330487
Iteration 3/25 | Loss: 0.00206476
Iteration 4/25 | Loss: 0.00206476
Iteration 5/25 | Loss: 0.00206476
Iteration 6/25 | Loss: 0.00206476
Iteration 7/25 | Loss: 0.00206476
Iteration 8/25 | Loss: 0.00206476
Iteration 9/25 | Loss: 0.00206476
Iteration 10/25 | Loss: 0.00206476
Iteration 11/25 | Loss: 0.00206476
Iteration 12/25 | Loss: 0.00206476
Iteration 13/25 | Loss: 0.00206476
Iteration 14/25 | Loss: 0.00206476
Iteration 15/25 | Loss: 0.00206476
Iteration 16/25 | Loss: 0.00206476
Iteration 17/25 | Loss: 0.00206476
Iteration 18/25 | Loss: 0.00206476
Iteration 19/25 | Loss: 0.00206476
Iteration 20/25 | Loss: 0.00206476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0020647558849304914, 0.0020647558849304914, 0.0020647558849304914, 0.0020647558849304914, 0.0020647558849304914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020647558849304914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206476
Iteration 2/1000 | Loss: 0.00223286
Iteration 3/1000 | Loss: 0.00046646
Iteration 4/1000 | Loss: 0.00040908
Iteration 5/1000 | Loss: 0.00063942
Iteration 6/1000 | Loss: 0.00032897
Iteration 7/1000 | Loss: 0.00038727
Iteration 8/1000 | Loss: 0.00012550
Iteration 9/1000 | Loss: 0.00101212
Iteration 10/1000 | Loss: 0.00013727
Iteration 11/1000 | Loss: 0.00012928
Iteration 12/1000 | Loss: 0.00058900
Iteration 13/1000 | Loss: 0.00115257
Iteration 14/1000 | Loss: 0.00030707
Iteration 15/1000 | Loss: 0.00019564
Iteration 16/1000 | Loss: 0.00013629
Iteration 17/1000 | Loss: 0.00156178
Iteration 18/1000 | Loss: 0.00073509
Iteration 19/1000 | Loss: 0.00011182
Iteration 20/1000 | Loss: 0.00087835
Iteration 21/1000 | Loss: 0.00057673
Iteration 22/1000 | Loss: 0.00168185
Iteration 23/1000 | Loss: 0.00344412
Iteration 24/1000 | Loss: 0.00036862
Iteration 25/1000 | Loss: 0.00032595
Iteration 26/1000 | Loss: 0.00010029
Iteration 27/1000 | Loss: 0.00012119
Iteration 28/1000 | Loss: 0.00067653
Iteration 29/1000 | Loss: 0.00007383
Iteration 30/1000 | Loss: 0.00008512
Iteration 31/1000 | Loss: 0.00020891
Iteration 32/1000 | Loss: 0.00007646
Iteration 33/1000 | Loss: 0.00006083
Iteration 34/1000 | Loss: 0.00123920
Iteration 35/1000 | Loss: 0.00209043
Iteration 36/1000 | Loss: 0.00094777
Iteration 37/1000 | Loss: 0.00009650
Iteration 38/1000 | Loss: 0.00007312
Iteration 39/1000 | Loss: 0.00005821
Iteration 40/1000 | Loss: 0.00062638
Iteration 41/1000 | Loss: 0.00095201
Iteration 42/1000 | Loss: 0.00074288
Iteration 43/1000 | Loss: 0.00059229
Iteration 44/1000 | Loss: 0.00053119
Iteration 45/1000 | Loss: 0.00006666
Iteration 46/1000 | Loss: 0.00006909
Iteration 47/1000 | Loss: 0.00006057
Iteration 48/1000 | Loss: 0.00033530
Iteration 49/1000 | Loss: 0.00027647
Iteration 50/1000 | Loss: 0.00005599
Iteration 51/1000 | Loss: 0.00004267
Iteration 52/1000 | Loss: 0.00008153
Iteration 53/1000 | Loss: 0.00004630
Iteration 54/1000 | Loss: 0.00032749
Iteration 55/1000 | Loss: 0.00055898
Iteration 56/1000 | Loss: 0.00006152
Iteration 57/1000 | Loss: 0.00005884
Iteration 58/1000 | Loss: 0.00007029
Iteration 59/1000 | Loss: 0.00005266
Iteration 60/1000 | Loss: 0.00028042
Iteration 61/1000 | Loss: 0.00016290
Iteration 62/1000 | Loss: 0.00005108
Iteration 63/1000 | Loss: 0.00026338
Iteration 64/1000 | Loss: 0.00022881
Iteration 65/1000 | Loss: 0.00032210
Iteration 66/1000 | Loss: 0.00022225
Iteration 67/1000 | Loss: 0.00006066
Iteration 68/1000 | Loss: 0.00005534
Iteration 69/1000 | Loss: 0.00005784
Iteration 70/1000 | Loss: 0.00005699
Iteration 71/1000 | Loss: 0.00005808
Iteration 72/1000 | Loss: 0.00004840
Iteration 73/1000 | Loss: 0.00004466
Iteration 74/1000 | Loss: 0.00004629
Iteration 75/1000 | Loss: 0.00005783
Iteration 76/1000 | Loss: 0.00005970
Iteration 77/1000 | Loss: 0.00054163
Iteration 78/1000 | Loss: 0.00049584
Iteration 79/1000 | Loss: 0.00023875
Iteration 80/1000 | Loss: 0.00004903
Iteration 81/1000 | Loss: 0.00010669
Iteration 82/1000 | Loss: 0.00004931
Iteration 83/1000 | Loss: 0.00005618
Iteration 84/1000 | Loss: 0.00004796
Iteration 85/1000 | Loss: 0.00004024
Iteration 86/1000 | Loss: 0.00004634
Iteration 87/1000 | Loss: 0.00034654
Iteration 88/1000 | Loss: 0.00017053
Iteration 89/1000 | Loss: 0.00005750
Iteration 90/1000 | Loss: 0.00025106
Iteration 91/1000 | Loss: 0.00030671
Iteration 92/1000 | Loss: 0.00045353
Iteration 93/1000 | Loss: 0.00016011
Iteration 94/1000 | Loss: 0.00024759
Iteration 95/1000 | Loss: 0.00020413
Iteration 96/1000 | Loss: 0.00019047
Iteration 97/1000 | Loss: 0.00016301
Iteration 98/1000 | Loss: 0.00016266
Iteration 99/1000 | Loss: 0.00013363
Iteration 100/1000 | Loss: 0.00014372
Iteration 101/1000 | Loss: 0.00005998
Iteration 102/1000 | Loss: 0.00006879
Iteration 103/1000 | Loss: 0.00004762
Iteration 104/1000 | Loss: 0.00004583
Iteration 105/1000 | Loss: 0.00023722
Iteration 106/1000 | Loss: 0.00007452
Iteration 107/1000 | Loss: 0.00004826
Iteration 108/1000 | Loss: 0.00005870
Iteration 109/1000 | Loss: 0.00005751
Iteration 110/1000 | Loss: 0.00005132
Iteration 111/1000 | Loss: 0.00004768
Iteration 112/1000 | Loss: 0.00005654
Iteration 113/1000 | Loss: 0.00063586
Iteration 114/1000 | Loss: 0.00018744
Iteration 115/1000 | Loss: 0.00043793
Iteration 116/1000 | Loss: 0.00016187
Iteration 117/1000 | Loss: 0.00017921
Iteration 118/1000 | Loss: 0.00017009
Iteration 119/1000 | Loss: 0.00021246
Iteration 120/1000 | Loss: 0.00011043
Iteration 121/1000 | Loss: 0.00005132
Iteration 122/1000 | Loss: 0.00005447
Iteration 123/1000 | Loss: 0.00019490
Iteration 124/1000 | Loss: 0.00018278
Iteration 125/1000 | Loss: 0.00005556
Iteration 126/1000 | Loss: 0.00005639
Iteration 127/1000 | Loss: 0.00004874
Iteration 128/1000 | Loss: 0.00004705
Iteration 129/1000 | Loss: 0.00021695
Iteration 130/1000 | Loss: 0.00015428
Iteration 131/1000 | Loss: 0.00043687
Iteration 132/1000 | Loss: 0.00010587
Iteration 133/1000 | Loss: 0.00026225
Iteration 134/1000 | Loss: 0.00006107
Iteration 135/1000 | Loss: 0.00005538
Iteration 136/1000 | Loss: 0.00005830
Iteration 137/1000 | Loss: 0.00004274
Iteration 138/1000 | Loss: 0.00060823
Iteration 139/1000 | Loss: 0.00005032
Iteration 140/1000 | Loss: 0.00025155
Iteration 141/1000 | Loss: 0.00037030
Iteration 142/1000 | Loss: 0.00022848
Iteration 143/1000 | Loss: 0.00004889
Iteration 144/1000 | Loss: 0.00004242
Iteration 145/1000 | Loss: 0.00003426
Iteration 146/1000 | Loss: 0.00006409
Iteration 147/1000 | Loss: 0.00005691
Iteration 148/1000 | Loss: 0.00005577
Iteration 149/1000 | Loss: 0.00004681
Iteration 150/1000 | Loss: 0.00004029
Iteration 151/1000 | Loss: 0.00016569
Iteration 152/1000 | Loss: 0.00021317
Iteration 153/1000 | Loss: 0.00017608
Iteration 154/1000 | Loss: 0.00044274
Iteration 155/1000 | Loss: 0.00008842
Iteration 156/1000 | Loss: 0.00005090
Iteration 157/1000 | Loss: 0.00018722
Iteration 158/1000 | Loss: 0.00004076
Iteration 159/1000 | Loss: 0.00003244
Iteration 160/1000 | Loss: 0.00030995
Iteration 161/1000 | Loss: 0.00004357
Iteration 162/1000 | Loss: 0.00005929
Iteration 163/1000 | Loss: 0.00003161
Iteration 164/1000 | Loss: 0.00004366
Iteration 165/1000 | Loss: 0.00002891
Iteration 166/1000 | Loss: 0.00004312
Iteration 167/1000 | Loss: 0.00003720
Iteration 168/1000 | Loss: 0.00002543
Iteration 169/1000 | Loss: 0.00002474
Iteration 170/1000 | Loss: 0.00006304
Iteration 171/1000 | Loss: 0.00003295
Iteration 172/1000 | Loss: 0.00003053
Iteration 173/1000 | Loss: 0.00003277
Iteration 174/1000 | Loss: 0.00012764
Iteration 175/1000 | Loss: 0.00003118
Iteration 176/1000 | Loss: 0.00002405
Iteration 177/1000 | Loss: 0.00016120
Iteration 178/1000 | Loss: 0.00014633
Iteration 179/1000 | Loss: 0.00016404
Iteration 180/1000 | Loss: 0.00004296
Iteration 181/1000 | Loss: 0.00013956
Iteration 182/1000 | Loss: 0.00058526
Iteration 183/1000 | Loss: 0.00092219
Iteration 184/1000 | Loss: 0.00083821
Iteration 185/1000 | Loss: 0.00015215
Iteration 186/1000 | Loss: 0.00050601
Iteration 187/1000 | Loss: 0.00057073
Iteration 188/1000 | Loss: 0.00023244
Iteration 189/1000 | Loss: 0.00004453
Iteration 190/1000 | Loss: 0.00021626
Iteration 191/1000 | Loss: 0.00003028
Iteration 192/1000 | Loss: 0.00008029
Iteration 193/1000 | Loss: 0.00003517
Iteration 194/1000 | Loss: 0.00004637
Iteration 195/1000 | Loss: 0.00002078
Iteration 196/1000 | Loss: 0.00012123
Iteration 197/1000 | Loss: 0.00014911
Iteration 198/1000 | Loss: 0.00010270
Iteration 199/1000 | Loss: 0.00002115
Iteration 200/1000 | Loss: 0.00001981
Iteration 201/1000 | Loss: 0.00001902
Iteration 202/1000 | Loss: 0.00022252
Iteration 203/1000 | Loss: 0.00004552
Iteration 204/1000 | Loss: 0.00006951
Iteration 205/1000 | Loss: 0.00029981
Iteration 206/1000 | Loss: 0.00002119
Iteration 207/1000 | Loss: 0.00001898
Iteration 208/1000 | Loss: 0.00001823
Iteration 209/1000 | Loss: 0.00001773
Iteration 210/1000 | Loss: 0.00001755
Iteration 211/1000 | Loss: 0.00001744
Iteration 212/1000 | Loss: 0.00001741
Iteration 213/1000 | Loss: 0.00001741
Iteration 214/1000 | Loss: 0.00002949
Iteration 215/1000 | Loss: 0.00001914
Iteration 216/1000 | Loss: 0.00001996
Iteration 217/1000 | Loss: 0.00001712
Iteration 218/1000 | Loss: 0.00001706
Iteration 219/1000 | Loss: 0.00001706
Iteration 220/1000 | Loss: 0.00001705
Iteration 221/1000 | Loss: 0.00001703
Iteration 222/1000 | Loss: 0.00001703
Iteration 223/1000 | Loss: 0.00001702
Iteration 224/1000 | Loss: 0.00001702
Iteration 225/1000 | Loss: 0.00001701
Iteration 226/1000 | Loss: 0.00001698
Iteration 227/1000 | Loss: 0.00001698
Iteration 228/1000 | Loss: 0.00001697
Iteration 229/1000 | Loss: 0.00001696
Iteration 230/1000 | Loss: 0.00001696
Iteration 231/1000 | Loss: 0.00001696
Iteration 232/1000 | Loss: 0.00001695
Iteration 233/1000 | Loss: 0.00001695
Iteration 234/1000 | Loss: 0.00001692
Iteration 235/1000 | Loss: 0.00001692
Iteration 236/1000 | Loss: 0.00001690
Iteration 237/1000 | Loss: 0.00001690
Iteration 238/1000 | Loss: 0.00001689
Iteration 239/1000 | Loss: 0.00001689
Iteration 240/1000 | Loss: 0.00001689
Iteration 241/1000 | Loss: 0.00001688
Iteration 242/1000 | Loss: 0.00001688
Iteration 243/1000 | Loss: 0.00001688
Iteration 244/1000 | Loss: 0.00001687
Iteration 245/1000 | Loss: 0.00001687
Iteration 246/1000 | Loss: 0.00001687
Iteration 247/1000 | Loss: 0.00001686
Iteration 248/1000 | Loss: 0.00004352
Iteration 249/1000 | Loss: 0.00002036
Iteration 250/1000 | Loss: 0.00002830
Iteration 251/1000 | Loss: 0.00001834
Iteration 252/1000 | Loss: 0.00002378
Iteration 253/1000 | Loss: 0.00004627
Iteration 254/1000 | Loss: 0.00001680
Iteration 255/1000 | Loss: 0.00001679
Iteration 256/1000 | Loss: 0.00001679
Iteration 257/1000 | Loss: 0.00001679
Iteration 258/1000 | Loss: 0.00001679
Iteration 259/1000 | Loss: 0.00001679
Iteration 260/1000 | Loss: 0.00001679
Iteration 261/1000 | Loss: 0.00001678
Iteration 262/1000 | Loss: 0.00001678
Iteration 263/1000 | Loss: 0.00001678
Iteration 264/1000 | Loss: 0.00001678
Iteration 265/1000 | Loss: 0.00001678
Iteration 266/1000 | Loss: 0.00001678
Iteration 267/1000 | Loss: 0.00001678
Iteration 268/1000 | Loss: 0.00001678
Iteration 269/1000 | Loss: 0.00001678
Iteration 270/1000 | Loss: 0.00001678
Iteration 271/1000 | Loss: 0.00001678
Iteration 272/1000 | Loss: 0.00001678
Iteration 273/1000 | Loss: 0.00001678
Iteration 274/1000 | Loss: 0.00001678
Iteration 275/1000 | Loss: 0.00001677
Iteration 276/1000 | Loss: 0.00001677
Iteration 277/1000 | Loss: 0.00001677
Iteration 278/1000 | Loss: 0.00001677
Iteration 279/1000 | Loss: 0.00001677
Iteration 280/1000 | Loss: 0.00001677
Iteration 281/1000 | Loss: 0.00001677
Iteration 282/1000 | Loss: 0.00001677
Iteration 283/1000 | Loss: 0.00001677
Iteration 284/1000 | Loss: 0.00001677
Iteration 285/1000 | Loss: 0.00001677
Iteration 286/1000 | Loss: 0.00001677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [1.67729303939268e-05, 1.67729303939268e-05, 1.67729303939268e-05, 1.67729303939268e-05, 1.67729303939268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.67729303939268e-05

Optimization complete. Final v2v error: 3.3597044944763184 mm

Highest mean error: 5.803632736206055 mm for frame 17

Lowest mean error: 2.980853796005249 mm for frame 221

Saving results

Total time: 414.77431631088257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073752
Iteration 2/25 | Loss: 0.01073752
Iteration 3/25 | Loss: 0.01073752
Iteration 4/25 | Loss: 0.01073752
Iteration 5/25 | Loss: 0.01073752
Iteration 6/25 | Loss: 0.01073752
Iteration 7/25 | Loss: 0.01073751
Iteration 8/25 | Loss: 0.01073751
Iteration 9/25 | Loss: 0.01073751
Iteration 10/25 | Loss: 0.01073751
Iteration 11/25 | Loss: 0.01073751
Iteration 12/25 | Loss: 0.01073751
Iteration 13/25 | Loss: 0.01073751
Iteration 14/25 | Loss: 0.01073751
Iteration 15/25 | Loss: 0.01073751
Iteration 16/25 | Loss: 0.01073751
Iteration 17/25 | Loss: 0.01073750
Iteration 18/25 | Loss: 0.01073750
Iteration 19/25 | Loss: 0.01073750
Iteration 20/25 | Loss: 0.01073750
Iteration 21/25 | Loss: 0.01073750
Iteration 22/25 | Loss: 0.01073750
Iteration 23/25 | Loss: 0.01073750
Iteration 24/25 | Loss: 0.01073750
Iteration 25/25 | Loss: 0.01073750

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96831894
Iteration 2/25 | Loss: 0.12924527
Iteration 3/25 | Loss: 0.05914538
Iteration 4/25 | Loss: 0.05873167
Iteration 5/25 | Loss: 0.05873166
Iteration 6/25 | Loss: 0.05873166
Iteration 7/25 | Loss: 0.05873166
Iteration 8/25 | Loss: 0.05873165
Iteration 9/25 | Loss: 0.05873165
Iteration 10/25 | Loss: 0.05873165
Iteration 11/25 | Loss: 0.05873165
Iteration 12/25 | Loss: 0.05873165
Iteration 13/25 | Loss: 0.05873165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.05873165279626846, 0.05873165279626846, 0.05873165279626846, 0.05873165279626846, 0.05873165279626846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05873165279626846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05873165
Iteration 2/1000 | Loss: 0.00393845
Iteration 3/1000 | Loss: 0.00062433
Iteration 4/1000 | Loss: 0.00029960
Iteration 5/1000 | Loss: 0.00016614
Iteration 6/1000 | Loss: 0.00011071
Iteration 7/1000 | Loss: 0.00008276
Iteration 8/1000 | Loss: 0.00006923
Iteration 9/1000 | Loss: 0.00005805
Iteration 10/1000 | Loss: 0.00005054
Iteration 11/1000 | Loss: 0.00004447
Iteration 12/1000 | Loss: 0.00004050
Iteration 13/1000 | Loss: 0.00003664
Iteration 14/1000 | Loss: 0.00003337
Iteration 15/1000 | Loss: 0.00003107
Iteration 16/1000 | Loss: 0.00002864
Iteration 17/1000 | Loss: 0.00002637
Iteration 18/1000 | Loss: 0.00002518
Iteration 19/1000 | Loss: 0.00002406
Iteration 20/1000 | Loss: 0.00002310
Iteration 21/1000 | Loss: 0.00002211
Iteration 22/1000 | Loss: 0.00002151
Iteration 23/1000 | Loss: 0.00002090
Iteration 24/1000 | Loss: 0.00002055
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00001989
Iteration 27/1000 | Loss: 0.00001962
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001921
Iteration 30/1000 | Loss: 0.00001917
Iteration 31/1000 | Loss: 0.00001913
Iteration 32/1000 | Loss: 0.00001909
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001900
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001888
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001884
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001877
Iteration 56/1000 | Loss: 0.00001876
Iteration 57/1000 | Loss: 0.00001875
Iteration 58/1000 | Loss: 0.00001875
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001874
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001872
Iteration 66/1000 | Loss: 0.00001871
Iteration 67/1000 | Loss: 0.00001871
Iteration 68/1000 | Loss: 0.00001871
Iteration 69/1000 | Loss: 0.00001870
Iteration 70/1000 | Loss: 0.00001870
Iteration 71/1000 | Loss: 0.00001870
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001868
Iteration 75/1000 | Loss: 0.00001868
Iteration 76/1000 | Loss: 0.00001867
Iteration 77/1000 | Loss: 0.00001867
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001864
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001863
Iteration 87/1000 | Loss: 0.00001863
Iteration 88/1000 | Loss: 0.00001863
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001862
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001862
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001860
Iteration 117/1000 | Loss: 0.00001860
Iteration 118/1000 | Loss: 0.00001860
Iteration 119/1000 | Loss: 0.00001859
Iteration 120/1000 | Loss: 0.00001859
Iteration 121/1000 | Loss: 0.00001859
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001859
Iteration 124/1000 | Loss: 0.00001859
Iteration 125/1000 | Loss: 0.00001859
Iteration 126/1000 | Loss: 0.00001859
Iteration 127/1000 | Loss: 0.00001859
Iteration 128/1000 | Loss: 0.00001859
Iteration 129/1000 | Loss: 0.00001859
Iteration 130/1000 | Loss: 0.00001859
Iteration 131/1000 | Loss: 0.00001859
Iteration 132/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.8590370018500835e-05, 1.8590370018500835e-05, 1.8590370018500835e-05, 1.8590370018500835e-05, 1.8590370018500835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8590370018500835e-05

Optimization complete. Final v2v error: 3.493218183517456 mm

Highest mean error: 5.4578046798706055 mm for frame 45

Lowest mean error: 2.8193106651306152 mm for frame 193

Saving results

Total time: 68.82051634788513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916648
Iteration 2/25 | Loss: 0.00281672
Iteration 3/25 | Loss: 0.00177659
Iteration 4/25 | Loss: 0.00137940
Iteration 5/25 | Loss: 0.00116464
Iteration 6/25 | Loss: 0.00110788
Iteration 7/25 | Loss: 0.00108768
Iteration 8/25 | Loss: 0.00107296
Iteration 9/25 | Loss: 0.00106052
Iteration 10/25 | Loss: 0.00105330
Iteration 11/25 | Loss: 0.00104737
Iteration 12/25 | Loss: 0.00104941
Iteration 13/25 | Loss: 0.00104445
Iteration 14/25 | Loss: 0.00104110
Iteration 15/25 | Loss: 0.00103941
Iteration 16/25 | Loss: 0.00104331
Iteration 17/25 | Loss: 0.00104151
Iteration 18/25 | Loss: 0.00103776
Iteration 19/25 | Loss: 0.00103698
Iteration 20/25 | Loss: 0.00103604
Iteration 21/25 | Loss: 0.00104108
Iteration 22/25 | Loss: 0.00105287
Iteration 23/25 | Loss: 0.00103045
Iteration 24/25 | Loss: 0.00102268
Iteration 25/25 | Loss: 0.00101957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.61355114
Iteration 2/25 | Loss: 0.00323017
Iteration 3/25 | Loss: 0.00226011
Iteration 4/25 | Loss: 0.00223234
Iteration 5/25 | Loss: 0.00223234
Iteration 6/25 | Loss: 0.00223234
Iteration 7/25 | Loss: 0.00223234
Iteration 8/25 | Loss: 0.00223234
Iteration 9/25 | Loss: 0.00223234
Iteration 10/25 | Loss: 0.00223234
Iteration 11/25 | Loss: 0.00223234
Iteration 12/25 | Loss: 0.00223234
Iteration 13/25 | Loss: 0.00223234
Iteration 14/25 | Loss: 0.00223234
Iteration 15/25 | Loss: 0.00223234
Iteration 16/25 | Loss: 0.00223234
Iteration 17/25 | Loss: 0.00223234
Iteration 18/25 | Loss: 0.00223234
Iteration 19/25 | Loss: 0.00223234
Iteration 20/25 | Loss: 0.00223234
Iteration 21/25 | Loss: 0.00223234
Iteration 22/25 | Loss: 0.00223234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002232340397313237, 0.002232340397313237, 0.002232340397313237, 0.002232340397313237, 0.002232340397313237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002232340397313237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223234
Iteration 2/1000 | Loss: 0.00419231
Iteration 3/1000 | Loss: 0.00105813
Iteration 4/1000 | Loss: 0.00055626
Iteration 5/1000 | Loss: 0.00071636
Iteration 6/1000 | Loss: 0.00354043
Iteration 7/1000 | Loss: 0.00585542
Iteration 8/1000 | Loss: 0.00340442
Iteration 9/1000 | Loss: 0.00565754
Iteration 10/1000 | Loss: 0.00513015
Iteration 11/1000 | Loss: 0.00348996
Iteration 12/1000 | Loss: 0.00665232
Iteration 13/1000 | Loss: 0.00674997
Iteration 14/1000 | Loss: 0.00900912
Iteration 15/1000 | Loss: 0.00465789
Iteration 16/1000 | Loss: 0.00549085
Iteration 17/1000 | Loss: 0.00285776
Iteration 18/1000 | Loss: 0.00192023
Iteration 19/1000 | Loss: 0.00250410
Iteration 20/1000 | Loss: 0.00314310
Iteration 21/1000 | Loss: 0.00139643
Iteration 22/1000 | Loss: 0.00093699
Iteration 23/1000 | Loss: 0.00145705
Iteration 24/1000 | Loss: 0.00066781
Iteration 25/1000 | Loss: 0.00066322
Iteration 26/1000 | Loss: 0.00074929
Iteration 27/1000 | Loss: 0.00119789
Iteration 28/1000 | Loss: 0.00075769
Iteration 29/1000 | Loss: 0.00064566
Iteration 30/1000 | Loss: 0.00070864
Iteration 31/1000 | Loss: 0.00049363
Iteration 32/1000 | Loss: 0.00120866
Iteration 33/1000 | Loss: 0.00090119
Iteration 34/1000 | Loss: 0.00112423
Iteration 35/1000 | Loss: 0.00079643
Iteration 36/1000 | Loss: 0.00244746
Iteration 37/1000 | Loss: 0.00184806
Iteration 38/1000 | Loss: 0.00212004
Iteration 39/1000 | Loss: 0.00180457
Iteration 40/1000 | Loss: 0.00145541
Iteration 41/1000 | Loss: 0.00110023
Iteration 42/1000 | Loss: 0.00115829
Iteration 43/1000 | Loss: 0.00073815
Iteration 44/1000 | Loss: 0.00373636
Iteration 45/1000 | Loss: 0.00405730
Iteration 46/1000 | Loss: 0.00141924
Iteration 47/1000 | Loss: 0.00039671
Iteration 48/1000 | Loss: 0.00163712
Iteration 49/1000 | Loss: 0.00087992
Iteration 50/1000 | Loss: 0.00184001
Iteration 51/1000 | Loss: 0.00078175
Iteration 52/1000 | Loss: 0.00144303
Iteration 53/1000 | Loss: 0.00129177
Iteration 54/1000 | Loss: 0.00176088
Iteration 55/1000 | Loss: 0.00172660
Iteration 56/1000 | Loss: 0.00128241
Iteration 57/1000 | Loss: 0.00101028
Iteration 58/1000 | Loss: 0.00175932
Iteration 59/1000 | Loss: 0.00030640
Iteration 60/1000 | Loss: 0.00032659
Iteration 61/1000 | Loss: 0.00008403
Iteration 62/1000 | Loss: 0.00096148
Iteration 63/1000 | Loss: 0.00061706
Iteration 64/1000 | Loss: 0.00109029
Iteration 65/1000 | Loss: 0.00048156
Iteration 66/1000 | Loss: 0.00008456
Iteration 67/1000 | Loss: 0.00006570
Iteration 68/1000 | Loss: 0.00034089
Iteration 69/1000 | Loss: 0.00076918
Iteration 70/1000 | Loss: 0.00041333
Iteration 71/1000 | Loss: 0.00110145
Iteration 72/1000 | Loss: 0.00116645
Iteration 73/1000 | Loss: 0.00045843
Iteration 74/1000 | Loss: 0.00047655
Iteration 75/1000 | Loss: 0.00146487
Iteration 76/1000 | Loss: 0.00088342
Iteration 77/1000 | Loss: 0.00121260
Iteration 78/1000 | Loss: 0.00162905
Iteration 79/1000 | Loss: 0.00063780
Iteration 80/1000 | Loss: 0.00156405
Iteration 81/1000 | Loss: 0.00182137
Iteration 82/1000 | Loss: 0.00124001
Iteration 83/1000 | Loss: 0.00092015
Iteration 84/1000 | Loss: 0.00069973
Iteration 85/1000 | Loss: 0.00067029
Iteration 86/1000 | Loss: 0.00097006
Iteration 87/1000 | Loss: 0.00059648
Iteration 88/1000 | Loss: 0.00060010
Iteration 89/1000 | Loss: 0.00093721
Iteration 90/1000 | Loss: 0.00063248
Iteration 91/1000 | Loss: 0.00105231
Iteration 92/1000 | Loss: 0.00130627
Iteration 93/1000 | Loss: 0.00105275
Iteration 94/1000 | Loss: 0.00128709
Iteration 95/1000 | Loss: 0.00119515
Iteration 96/1000 | Loss: 0.00087695
Iteration 97/1000 | Loss: 0.00095897
Iteration 98/1000 | Loss: 0.00102621
Iteration 99/1000 | Loss: 0.00101182
Iteration 100/1000 | Loss: 0.00093117
Iteration 101/1000 | Loss: 0.00097189
Iteration 102/1000 | Loss: 0.00099755
Iteration 103/1000 | Loss: 0.00072996
Iteration 104/1000 | Loss: 0.00098763
Iteration 105/1000 | Loss: 0.00207328
Iteration 106/1000 | Loss: 0.00248664
Iteration 107/1000 | Loss: 0.00091735
Iteration 108/1000 | Loss: 0.00008513
Iteration 109/1000 | Loss: 0.00023457
Iteration 110/1000 | Loss: 0.00007515
Iteration 111/1000 | Loss: 0.00006324
Iteration 112/1000 | Loss: 0.00005473
Iteration 113/1000 | Loss: 0.00071081
Iteration 114/1000 | Loss: 0.00025424
Iteration 115/1000 | Loss: 0.00051493
Iteration 116/1000 | Loss: 0.00103086
Iteration 117/1000 | Loss: 0.00059995
Iteration 118/1000 | Loss: 0.00059581
Iteration 119/1000 | Loss: 0.00048578
Iteration 120/1000 | Loss: 0.00067352
Iteration 121/1000 | Loss: 0.00073397
Iteration 122/1000 | Loss: 0.00028066
Iteration 123/1000 | Loss: 0.00051745
Iteration 124/1000 | Loss: 0.00049378
Iteration 125/1000 | Loss: 0.00113290
Iteration 126/1000 | Loss: 0.00112101
Iteration 127/1000 | Loss: 0.00016822
Iteration 128/1000 | Loss: 0.00033636
Iteration 129/1000 | Loss: 0.00046856
Iteration 130/1000 | Loss: 0.00034755
Iteration 131/1000 | Loss: 0.00012407
Iteration 132/1000 | Loss: 0.00004837
Iteration 133/1000 | Loss: 0.00024858
Iteration 134/1000 | Loss: 0.00005672
Iteration 135/1000 | Loss: 0.00018190
Iteration 136/1000 | Loss: 0.00023834
Iteration 137/1000 | Loss: 0.00032327
Iteration 138/1000 | Loss: 0.00084251
Iteration 139/1000 | Loss: 0.00053595
Iteration 140/1000 | Loss: 0.00008108
Iteration 141/1000 | Loss: 0.00138123
Iteration 142/1000 | Loss: 0.00093785
Iteration 143/1000 | Loss: 0.00089214
Iteration 144/1000 | Loss: 0.00055244
Iteration 145/1000 | Loss: 0.00005784
Iteration 146/1000 | Loss: 0.00005434
Iteration 147/1000 | Loss: 0.00003937
Iteration 148/1000 | Loss: 0.00068602
Iteration 149/1000 | Loss: 0.00006455
Iteration 150/1000 | Loss: 0.00004737
Iteration 151/1000 | Loss: 0.00047615
Iteration 152/1000 | Loss: 0.00012133
Iteration 153/1000 | Loss: 0.00004993
Iteration 154/1000 | Loss: 0.00005700
Iteration 155/1000 | Loss: 0.00005264
Iteration 156/1000 | Loss: 0.00003916
Iteration 157/1000 | Loss: 0.00004747
Iteration 158/1000 | Loss: 0.00004240
Iteration 159/1000 | Loss: 0.00003789
Iteration 160/1000 | Loss: 0.00004596
Iteration 161/1000 | Loss: 0.00003356
Iteration 162/1000 | Loss: 0.00004028
Iteration 163/1000 | Loss: 0.00003870
Iteration 164/1000 | Loss: 0.00005349
Iteration 165/1000 | Loss: 0.00084003
Iteration 166/1000 | Loss: 0.00035210
Iteration 167/1000 | Loss: 0.00007101
Iteration 168/1000 | Loss: 0.00005660
Iteration 169/1000 | Loss: 0.00004139
Iteration 170/1000 | Loss: 0.00004244
Iteration 171/1000 | Loss: 0.00004022
Iteration 172/1000 | Loss: 0.00004301
Iteration 173/1000 | Loss: 0.00003772
Iteration 174/1000 | Loss: 0.00004327
Iteration 175/1000 | Loss: 0.00005586
Iteration 176/1000 | Loss: 0.00003697
Iteration 177/1000 | Loss: 0.00004102
Iteration 178/1000 | Loss: 0.00004022
Iteration 179/1000 | Loss: 0.00004325
Iteration 180/1000 | Loss: 0.00004957
Iteration 181/1000 | Loss: 0.00003780
Iteration 182/1000 | Loss: 0.00003750
Iteration 183/1000 | Loss: 0.00004188
Iteration 184/1000 | Loss: 0.00005884
Iteration 185/1000 | Loss: 0.00076785
Iteration 186/1000 | Loss: 0.00005313
Iteration 187/1000 | Loss: 0.00005495
Iteration 188/1000 | Loss: 0.00005402
Iteration 189/1000 | Loss: 0.00004888
Iteration 190/1000 | Loss: 0.00005428
Iteration 191/1000 | Loss: 0.00003801
Iteration 192/1000 | Loss: 0.00003846
Iteration 193/1000 | Loss: 0.00003556
Iteration 194/1000 | Loss: 0.00004878
Iteration 195/1000 | Loss: 0.00005939
Iteration 196/1000 | Loss: 0.00005626
Iteration 197/1000 | Loss: 0.00005221
Iteration 198/1000 | Loss: 0.00005099
Iteration 199/1000 | Loss: 0.00005245
Iteration 200/1000 | Loss: 0.00002097
Iteration 201/1000 | Loss: 0.00002004
Iteration 202/1000 | Loss: 0.00001947
Iteration 203/1000 | Loss: 0.00001920
Iteration 204/1000 | Loss: 0.00001891
Iteration 205/1000 | Loss: 0.00001868
Iteration 206/1000 | Loss: 0.00001856
Iteration 207/1000 | Loss: 0.00001856
Iteration 208/1000 | Loss: 0.00001855
Iteration 209/1000 | Loss: 0.00001855
Iteration 210/1000 | Loss: 0.00001854
Iteration 211/1000 | Loss: 0.00001853
Iteration 212/1000 | Loss: 0.00001852
Iteration 213/1000 | Loss: 0.00001837
Iteration 214/1000 | Loss: 0.00001826
Iteration 215/1000 | Loss: 0.00001820
Iteration 216/1000 | Loss: 0.00001819
Iteration 217/1000 | Loss: 0.00001818
Iteration 218/1000 | Loss: 0.00001817
Iteration 219/1000 | Loss: 0.00001816
Iteration 220/1000 | Loss: 0.00001816
Iteration 221/1000 | Loss: 0.00001815
Iteration 222/1000 | Loss: 0.00001815
Iteration 223/1000 | Loss: 0.00001815
Iteration 224/1000 | Loss: 0.00001815
Iteration 225/1000 | Loss: 0.00001815
Iteration 226/1000 | Loss: 0.00001815
Iteration 227/1000 | Loss: 0.00001815
Iteration 228/1000 | Loss: 0.00001815
Iteration 229/1000 | Loss: 0.00001815
Iteration 230/1000 | Loss: 0.00001815
Iteration 231/1000 | Loss: 0.00001815
Iteration 232/1000 | Loss: 0.00001814
Iteration 233/1000 | Loss: 0.00001814
Iteration 234/1000 | Loss: 0.00022017
Iteration 235/1000 | Loss: 0.00058686
Iteration 236/1000 | Loss: 0.00007442
Iteration 237/1000 | Loss: 0.00002059
Iteration 238/1000 | Loss: 0.00001857
Iteration 239/1000 | Loss: 0.00001815
Iteration 240/1000 | Loss: 0.00001807
Iteration 241/1000 | Loss: 0.00001807
Iteration 242/1000 | Loss: 0.00001806
Iteration 243/1000 | Loss: 0.00001806
Iteration 244/1000 | Loss: 0.00001805
Iteration 245/1000 | Loss: 0.00001805
Iteration 246/1000 | Loss: 0.00001805
Iteration 247/1000 | Loss: 0.00001804
Iteration 248/1000 | Loss: 0.00001804
Iteration 249/1000 | Loss: 0.00001804
Iteration 250/1000 | Loss: 0.00001804
Iteration 251/1000 | Loss: 0.00001804
Iteration 252/1000 | Loss: 0.00001803
Iteration 253/1000 | Loss: 0.00001803
Iteration 254/1000 | Loss: 0.00001803
Iteration 255/1000 | Loss: 0.00001803
Iteration 256/1000 | Loss: 0.00001802
Iteration 257/1000 | Loss: 0.00001802
Iteration 258/1000 | Loss: 0.00001802
Iteration 259/1000 | Loss: 0.00001802
Iteration 260/1000 | Loss: 0.00001801
Iteration 261/1000 | Loss: 0.00001801
Iteration 262/1000 | Loss: 0.00001801
Iteration 263/1000 | Loss: 0.00001801
Iteration 264/1000 | Loss: 0.00001801
Iteration 265/1000 | Loss: 0.00001801
Iteration 266/1000 | Loss: 0.00001800
Iteration 267/1000 | Loss: 0.00001800
Iteration 268/1000 | Loss: 0.00001800
Iteration 269/1000 | Loss: 0.00001800
Iteration 270/1000 | Loss: 0.00001799
Iteration 271/1000 | Loss: 0.00001799
Iteration 272/1000 | Loss: 0.00001799
Iteration 273/1000 | Loss: 0.00001799
Iteration 274/1000 | Loss: 0.00001799
Iteration 275/1000 | Loss: 0.00001799
Iteration 276/1000 | Loss: 0.00001798
Iteration 277/1000 | Loss: 0.00001798
Iteration 278/1000 | Loss: 0.00001798
Iteration 279/1000 | Loss: 0.00001798
Iteration 280/1000 | Loss: 0.00001797
Iteration 281/1000 | Loss: 0.00001797
Iteration 282/1000 | Loss: 0.00001797
Iteration 283/1000 | Loss: 0.00001797
Iteration 284/1000 | Loss: 0.00001797
Iteration 285/1000 | Loss: 0.00001797
Iteration 286/1000 | Loss: 0.00001797
Iteration 287/1000 | Loss: 0.00001797
Iteration 288/1000 | Loss: 0.00001796
Iteration 289/1000 | Loss: 0.00001796
Iteration 290/1000 | Loss: 0.00001796
Iteration 291/1000 | Loss: 0.00001796
Iteration 292/1000 | Loss: 0.00001796
Iteration 293/1000 | Loss: 0.00001796
Iteration 294/1000 | Loss: 0.00001796
Iteration 295/1000 | Loss: 0.00001795
Iteration 296/1000 | Loss: 0.00001795
Iteration 297/1000 | Loss: 0.00001795
Iteration 298/1000 | Loss: 0.00001795
Iteration 299/1000 | Loss: 0.00001795
Iteration 300/1000 | Loss: 0.00001795
Iteration 301/1000 | Loss: 0.00001795
Iteration 302/1000 | Loss: 0.00001794
Iteration 303/1000 | Loss: 0.00001794
Iteration 304/1000 | Loss: 0.00001794
Iteration 305/1000 | Loss: 0.00001794
Iteration 306/1000 | Loss: 0.00001793
Iteration 307/1000 | Loss: 0.00001793
Iteration 308/1000 | Loss: 0.00001793
Iteration 309/1000 | Loss: 0.00001793
Iteration 310/1000 | Loss: 0.00001793
Iteration 311/1000 | Loss: 0.00001793
Iteration 312/1000 | Loss: 0.00001793
Iteration 313/1000 | Loss: 0.00001792
Iteration 314/1000 | Loss: 0.00001792
Iteration 315/1000 | Loss: 0.00001792
Iteration 316/1000 | Loss: 0.00001792
Iteration 317/1000 | Loss: 0.00001792
Iteration 318/1000 | Loss: 0.00001792
Iteration 319/1000 | Loss: 0.00001792
Iteration 320/1000 | Loss: 0.00001792
Iteration 321/1000 | Loss: 0.00001792
Iteration 322/1000 | Loss: 0.00001792
Iteration 323/1000 | Loss: 0.00001792
Iteration 324/1000 | Loss: 0.00001792
Iteration 325/1000 | Loss: 0.00001792
Iteration 326/1000 | Loss: 0.00001792
Iteration 327/1000 | Loss: 0.00001792
Iteration 328/1000 | Loss: 0.00001791
Iteration 329/1000 | Loss: 0.00001791
Iteration 330/1000 | Loss: 0.00001791
Iteration 331/1000 | Loss: 0.00001791
Iteration 332/1000 | Loss: 0.00001791
Iteration 333/1000 | Loss: 0.00001791
Iteration 334/1000 | Loss: 0.00001791
Iteration 335/1000 | Loss: 0.00001791
Iteration 336/1000 | Loss: 0.00001791
Iteration 337/1000 | Loss: 0.00001791
Iteration 338/1000 | Loss: 0.00001791
Iteration 339/1000 | Loss: 0.00001790
Iteration 340/1000 | Loss: 0.00001790
Iteration 341/1000 | Loss: 0.00001790
Iteration 342/1000 | Loss: 0.00001790
Iteration 343/1000 | Loss: 0.00001790
Iteration 344/1000 | Loss: 0.00001790
Iteration 345/1000 | Loss: 0.00001789
Iteration 346/1000 | Loss: 0.00001789
Iteration 347/1000 | Loss: 0.00001789
Iteration 348/1000 | Loss: 0.00001788
Iteration 349/1000 | Loss: 0.00001788
Iteration 350/1000 | Loss: 0.00001788
Iteration 351/1000 | Loss: 0.00001787
Iteration 352/1000 | Loss: 0.00001787
Iteration 353/1000 | Loss: 0.00001787
Iteration 354/1000 | Loss: 0.00001787
Iteration 355/1000 | Loss: 0.00001787
Iteration 356/1000 | Loss: 0.00001787
Iteration 357/1000 | Loss: 0.00001787
Iteration 358/1000 | Loss: 0.00001787
Iteration 359/1000 | Loss: 0.00001787
Iteration 360/1000 | Loss: 0.00001787
Iteration 361/1000 | Loss: 0.00001787
Iteration 362/1000 | Loss: 0.00001786
Iteration 363/1000 | Loss: 0.00001786
Iteration 364/1000 | Loss: 0.00001786
Iteration 365/1000 | Loss: 0.00001786
Iteration 366/1000 | Loss: 0.00001786
Iteration 367/1000 | Loss: 0.00001786
Iteration 368/1000 | Loss: 0.00001786
Iteration 369/1000 | Loss: 0.00001786
Iteration 370/1000 | Loss: 0.00001786
Iteration 371/1000 | Loss: 0.00001786
Iteration 372/1000 | Loss: 0.00001785
Iteration 373/1000 | Loss: 0.00001785
Iteration 374/1000 | Loss: 0.00001785
Iteration 375/1000 | Loss: 0.00001785
Iteration 376/1000 | Loss: 0.00001785
Iteration 377/1000 | Loss: 0.00001785
Iteration 378/1000 | Loss: 0.00001785
Iteration 379/1000 | Loss: 0.00001785
Iteration 380/1000 | Loss: 0.00001785
Iteration 381/1000 | Loss: 0.00001785
Iteration 382/1000 | Loss: 0.00001785
Iteration 383/1000 | Loss: 0.00001785
Iteration 384/1000 | Loss: 0.00001785
Iteration 385/1000 | Loss: 0.00001785
Iteration 386/1000 | Loss: 0.00001785
Iteration 387/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 387. Stopping optimization.
Last 5 losses: [1.785078529792372e-05, 1.785078529792372e-05, 1.785078529792372e-05, 1.785078529792372e-05, 1.785078529792372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.785078529792372e-05

Optimization complete. Final v2v error: 3.3669004440307617 mm

Highest mean error: 8.784622192382812 mm for frame 3

Lowest mean error: 2.599546194076538 mm for frame 156

Saving results

Total time: 364.5321614742279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340815
Iteration 2/25 | Loss: 0.00119389
Iteration 3/25 | Loss: 0.00080847
Iteration 4/25 | Loss: 0.00075356
Iteration 5/25 | Loss: 0.00074434
Iteration 6/25 | Loss: 0.00074253
Iteration 7/25 | Loss: 0.00074188
Iteration 8/25 | Loss: 0.00074176
Iteration 9/25 | Loss: 0.00074176
Iteration 10/25 | Loss: 0.00074176
Iteration 11/25 | Loss: 0.00074176
Iteration 12/25 | Loss: 0.00074176
Iteration 13/25 | Loss: 0.00074176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007417596643790603, 0.0007417596643790603, 0.0007417596643790603, 0.0007417596643790603, 0.0007417596643790603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007417596643790603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55536950
Iteration 2/25 | Loss: 0.00051523
Iteration 3/25 | Loss: 0.00051523
Iteration 4/25 | Loss: 0.00051523
Iteration 5/25 | Loss: 0.00051523
Iteration 6/25 | Loss: 0.00051523
Iteration 7/25 | Loss: 0.00051523
Iteration 8/25 | Loss: 0.00051522
Iteration 9/25 | Loss: 0.00051522
Iteration 10/25 | Loss: 0.00051522
Iteration 11/25 | Loss: 0.00051522
Iteration 12/25 | Loss: 0.00051522
Iteration 13/25 | Loss: 0.00051522
Iteration 14/25 | Loss: 0.00051522
Iteration 15/25 | Loss: 0.00051522
Iteration 16/25 | Loss: 0.00051522
Iteration 17/25 | Loss: 0.00051522
Iteration 18/25 | Loss: 0.00051522
Iteration 19/25 | Loss: 0.00051522
Iteration 20/25 | Loss: 0.00051522
Iteration 21/25 | Loss: 0.00051522
Iteration 22/25 | Loss: 0.00051522
Iteration 23/25 | Loss: 0.00051522
Iteration 24/25 | Loss: 0.00051522
Iteration 25/25 | Loss: 0.00051522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051522
Iteration 2/1000 | Loss: 0.00002802
Iteration 3/1000 | Loss: 0.00001945
Iteration 4/1000 | Loss: 0.00001549
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001308
Iteration 9/1000 | Loss: 0.00001274
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001188
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001184
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001178
Iteration 48/1000 | Loss: 0.00001178
Iteration 49/1000 | Loss: 0.00001178
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001175
Iteration 68/1000 | Loss: 0.00001175
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001169
Iteration 100/1000 | Loss: 0.00001169
Iteration 101/1000 | Loss: 0.00001169
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001168
Iteration 104/1000 | Loss: 0.00001168
Iteration 105/1000 | Loss: 0.00001168
Iteration 106/1000 | Loss: 0.00001168
Iteration 107/1000 | Loss: 0.00001168
Iteration 108/1000 | Loss: 0.00001168
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.1676278518280014e-05, 1.1676278518280014e-05, 1.1676278518280014e-05, 1.1676278518280014e-05, 1.1676278518280014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1676278518280014e-05

Optimization complete. Final v2v error: 2.9302306175231934 mm

Highest mean error: 3.184617280960083 mm for frame 27

Lowest mean error: 2.8051810264587402 mm for frame 61

Saving results

Total time: 36.73481631278992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451774
Iteration 2/25 | Loss: 0.00113052
Iteration 3/25 | Loss: 0.00081275
Iteration 4/25 | Loss: 0.00077817
Iteration 5/25 | Loss: 0.00076923
Iteration 6/25 | Loss: 0.00076596
Iteration 7/25 | Loss: 0.00076498
Iteration 8/25 | Loss: 0.00076479
Iteration 9/25 | Loss: 0.00076479
Iteration 10/25 | Loss: 0.00076479
Iteration 11/25 | Loss: 0.00076479
Iteration 12/25 | Loss: 0.00076479
Iteration 13/25 | Loss: 0.00076479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007647939492017031, 0.0007647939492017031, 0.0007647939492017031, 0.0007647939492017031, 0.0007647939492017031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007647939492017031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59015179
Iteration 2/25 | Loss: 0.00048346
Iteration 3/25 | Loss: 0.00048345
Iteration 4/25 | Loss: 0.00048345
Iteration 5/25 | Loss: 0.00048345
Iteration 6/25 | Loss: 0.00048345
Iteration 7/25 | Loss: 0.00048345
Iteration 8/25 | Loss: 0.00048345
Iteration 9/25 | Loss: 0.00048345
Iteration 10/25 | Loss: 0.00048345
Iteration 11/25 | Loss: 0.00048345
Iteration 12/25 | Loss: 0.00048345
Iteration 13/25 | Loss: 0.00048345
Iteration 14/25 | Loss: 0.00048345
Iteration 15/25 | Loss: 0.00048345
Iteration 16/25 | Loss: 0.00048345
Iteration 17/25 | Loss: 0.00048345
Iteration 18/25 | Loss: 0.00048345
Iteration 19/25 | Loss: 0.00048345
Iteration 20/25 | Loss: 0.00048345
Iteration 21/25 | Loss: 0.00048345
Iteration 22/25 | Loss: 0.00048345
Iteration 23/25 | Loss: 0.00048345
Iteration 24/25 | Loss: 0.00048345
Iteration 25/25 | Loss: 0.00048345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048345
Iteration 2/1000 | Loss: 0.00002557
Iteration 3/1000 | Loss: 0.00001808
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001425
Iteration 14/1000 | Loss: 0.00001424
Iteration 15/1000 | Loss: 0.00001424
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001423
Iteration 19/1000 | Loss: 0.00001418
Iteration 20/1000 | Loss: 0.00001418
Iteration 21/1000 | Loss: 0.00001418
Iteration 22/1000 | Loss: 0.00001417
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001411
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001410
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001407
Iteration 44/1000 | Loss: 0.00001407
Iteration 45/1000 | Loss: 0.00001406
Iteration 46/1000 | Loss: 0.00001406
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001405
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001403
Iteration 55/1000 | Loss: 0.00001403
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001401
Iteration 62/1000 | Loss: 0.00001401
Iteration 63/1000 | Loss: 0.00001400
Iteration 64/1000 | Loss: 0.00001400
Iteration 65/1000 | Loss: 0.00001400
Iteration 66/1000 | Loss: 0.00001400
Iteration 67/1000 | Loss: 0.00001400
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001398
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001397
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001397
Iteration 84/1000 | Loss: 0.00001397
Iteration 85/1000 | Loss: 0.00001397
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001397
Iteration 101/1000 | Loss: 0.00001397
Iteration 102/1000 | Loss: 0.00001397
Iteration 103/1000 | Loss: 0.00001397
Iteration 104/1000 | Loss: 0.00001397
Iteration 105/1000 | Loss: 0.00001397
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001397
Iteration 117/1000 | Loss: 0.00001397
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001397
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001397
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001397
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001397
Iteration 130/1000 | Loss: 0.00001397
Iteration 131/1000 | Loss: 0.00001397
Iteration 132/1000 | Loss: 0.00001397
Iteration 133/1000 | Loss: 0.00001397
Iteration 134/1000 | Loss: 0.00001397
Iteration 135/1000 | Loss: 0.00001397
Iteration 136/1000 | Loss: 0.00001397
Iteration 137/1000 | Loss: 0.00001397
Iteration 138/1000 | Loss: 0.00001397
Iteration 139/1000 | Loss: 0.00001397
Iteration 140/1000 | Loss: 0.00001397
Iteration 141/1000 | Loss: 0.00001397
Iteration 142/1000 | Loss: 0.00001397
Iteration 143/1000 | Loss: 0.00001397
Iteration 144/1000 | Loss: 0.00001397
Iteration 145/1000 | Loss: 0.00001397
Iteration 146/1000 | Loss: 0.00001397
Iteration 147/1000 | Loss: 0.00001397
Iteration 148/1000 | Loss: 0.00001397
Iteration 149/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.3967503946332727e-05, 1.3967503946332727e-05, 1.3967503946332727e-05, 1.3967503946332727e-05, 1.3967503946332727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3967503946332727e-05

Optimization complete. Final v2v error: 3.0927717685699463 mm

Highest mean error: 3.7915072441101074 mm for frame 53

Lowest mean error: 2.4432952404022217 mm for frame 122

Saving results

Total time: 34.474326848983765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718591
Iteration 2/25 | Loss: 0.00126446
Iteration 3/25 | Loss: 0.00086927
Iteration 4/25 | Loss: 0.00079418
Iteration 5/25 | Loss: 0.00077817
Iteration 6/25 | Loss: 0.00077381
Iteration 7/25 | Loss: 0.00077308
Iteration 8/25 | Loss: 0.00077308
Iteration 9/25 | Loss: 0.00077308
Iteration 10/25 | Loss: 0.00077308
Iteration 11/25 | Loss: 0.00077308
Iteration 12/25 | Loss: 0.00077308
Iteration 13/25 | Loss: 0.00077308
Iteration 14/25 | Loss: 0.00077308
Iteration 15/25 | Loss: 0.00077308
Iteration 16/25 | Loss: 0.00077308
Iteration 17/25 | Loss: 0.00077308
Iteration 18/25 | Loss: 0.00077308
Iteration 19/25 | Loss: 0.00077308
Iteration 20/25 | Loss: 0.00077308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00077308330219239, 0.00077308330219239, 0.00077308330219239, 0.00077308330219239, 0.00077308330219239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00077308330219239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52370310
Iteration 2/25 | Loss: 0.00046936
Iteration 3/25 | Loss: 0.00046935
Iteration 4/25 | Loss: 0.00046935
Iteration 5/25 | Loss: 0.00046935
Iteration 6/25 | Loss: 0.00046935
Iteration 7/25 | Loss: 0.00046935
Iteration 8/25 | Loss: 0.00046935
Iteration 9/25 | Loss: 0.00046935
Iteration 10/25 | Loss: 0.00046935
Iteration 11/25 | Loss: 0.00046935
Iteration 12/25 | Loss: 0.00046935
Iteration 13/25 | Loss: 0.00046935
Iteration 14/25 | Loss: 0.00046935
Iteration 15/25 | Loss: 0.00046935
Iteration 16/25 | Loss: 0.00046935
Iteration 17/25 | Loss: 0.00046935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00046934635611250997, 0.00046934635611250997, 0.00046934635611250997, 0.00046934635611250997, 0.00046934635611250997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046934635611250997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046935
Iteration 2/1000 | Loss: 0.00003122
Iteration 3/1000 | Loss: 0.00002219
Iteration 4/1000 | Loss: 0.00001997
Iteration 5/1000 | Loss: 0.00001889
Iteration 6/1000 | Loss: 0.00001797
Iteration 7/1000 | Loss: 0.00001753
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001670
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001648
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001611
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001599
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001594
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [1.59406044986099e-05, 1.59406044986099e-05, 1.59406044986099e-05, 1.59406044986099e-05, 1.59406044986099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.59406044986099e-05

Optimization complete. Final v2v error: 3.4345099925994873 mm

Highest mean error: 3.803804397583008 mm for frame 200

Lowest mean error: 3.1963274478912354 mm for frame 190

Saving results

Total time: 37.583125829696655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812931
Iteration 2/25 | Loss: 0.00104858
Iteration 3/25 | Loss: 0.00089652
Iteration 4/25 | Loss: 0.00085710
Iteration 5/25 | Loss: 0.00084810
Iteration 6/25 | Loss: 0.00084673
Iteration 7/25 | Loss: 0.00084666
Iteration 8/25 | Loss: 0.00084666
Iteration 9/25 | Loss: 0.00084666
Iteration 10/25 | Loss: 0.00084666
Iteration 11/25 | Loss: 0.00084666
Iteration 12/25 | Loss: 0.00084666
Iteration 13/25 | Loss: 0.00084666
Iteration 14/25 | Loss: 0.00084666
Iteration 15/25 | Loss: 0.00084666
Iteration 16/25 | Loss: 0.00084666
Iteration 17/25 | Loss: 0.00084666
Iteration 18/25 | Loss: 0.00084666
Iteration 19/25 | Loss: 0.00084666
Iteration 20/25 | Loss: 0.00084666
Iteration 21/25 | Loss: 0.00084666
Iteration 22/25 | Loss: 0.00084666
Iteration 23/25 | Loss: 0.00084666
Iteration 24/25 | Loss: 0.00084666
Iteration 25/25 | Loss: 0.00084666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.64263439
Iteration 2/25 | Loss: 0.00040531
Iteration 3/25 | Loss: 0.00040531
Iteration 4/25 | Loss: 0.00040531
Iteration 5/25 | Loss: 0.00040531
Iteration 6/25 | Loss: 0.00040531
Iteration 7/25 | Loss: 0.00040531
Iteration 8/25 | Loss: 0.00040530
Iteration 9/25 | Loss: 0.00040530
Iteration 10/25 | Loss: 0.00040530
Iteration 11/25 | Loss: 0.00040530
Iteration 12/25 | Loss: 0.00040530
Iteration 13/25 | Loss: 0.00040530
Iteration 14/25 | Loss: 0.00040530
Iteration 15/25 | Loss: 0.00040530
Iteration 16/25 | Loss: 0.00040530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00040530393016524613, 0.00040530393016524613, 0.00040530393016524613, 0.00040530393016524613, 0.00040530393016524613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040530393016524613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040530
Iteration 2/1000 | Loss: 0.00004151
Iteration 3/1000 | Loss: 0.00003059
Iteration 4/1000 | Loss: 0.00002807
Iteration 5/1000 | Loss: 0.00002659
Iteration 6/1000 | Loss: 0.00002592
Iteration 7/1000 | Loss: 0.00002534
Iteration 8/1000 | Loss: 0.00002492
Iteration 9/1000 | Loss: 0.00002457
Iteration 10/1000 | Loss: 0.00002432
Iteration 11/1000 | Loss: 0.00002416
Iteration 12/1000 | Loss: 0.00002415
Iteration 13/1000 | Loss: 0.00002411
Iteration 14/1000 | Loss: 0.00002410
Iteration 15/1000 | Loss: 0.00002410
Iteration 16/1000 | Loss: 0.00002405
Iteration 17/1000 | Loss: 0.00002403
Iteration 18/1000 | Loss: 0.00002396
Iteration 19/1000 | Loss: 0.00002396
Iteration 20/1000 | Loss: 0.00002393
Iteration 21/1000 | Loss: 0.00002391
Iteration 22/1000 | Loss: 0.00002391
Iteration 23/1000 | Loss: 0.00002391
Iteration 24/1000 | Loss: 0.00002391
Iteration 25/1000 | Loss: 0.00002390
Iteration 26/1000 | Loss: 0.00002390
Iteration 27/1000 | Loss: 0.00002390
Iteration 28/1000 | Loss: 0.00002389
Iteration 29/1000 | Loss: 0.00002389
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002389
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002388
Iteration 34/1000 | Loss: 0.00002388
Iteration 35/1000 | Loss: 0.00002388
Iteration 36/1000 | Loss: 0.00002388
Iteration 37/1000 | Loss: 0.00002388
Iteration 38/1000 | Loss: 0.00002388
Iteration 39/1000 | Loss: 0.00002388
Iteration 40/1000 | Loss: 0.00002388
Iteration 41/1000 | Loss: 0.00002388
Iteration 42/1000 | Loss: 0.00002388
Iteration 43/1000 | Loss: 0.00002388
Iteration 44/1000 | Loss: 0.00002388
Iteration 45/1000 | Loss: 0.00002388
Iteration 46/1000 | Loss: 0.00002387
Iteration 47/1000 | Loss: 0.00002387
Iteration 48/1000 | Loss: 0.00002387
Iteration 49/1000 | Loss: 0.00002387
Iteration 50/1000 | Loss: 0.00002387
Iteration 51/1000 | Loss: 0.00002387
Iteration 52/1000 | Loss: 0.00002387
Iteration 53/1000 | Loss: 0.00002387
Iteration 54/1000 | Loss: 0.00002387
Iteration 55/1000 | Loss: 0.00002387
Iteration 56/1000 | Loss: 0.00002387
Iteration 57/1000 | Loss: 0.00002386
Iteration 58/1000 | Loss: 0.00002386
Iteration 59/1000 | Loss: 0.00002386
Iteration 60/1000 | Loss: 0.00002386
Iteration 61/1000 | Loss: 0.00002386
Iteration 62/1000 | Loss: 0.00002386
Iteration 63/1000 | Loss: 0.00002386
Iteration 64/1000 | Loss: 0.00002386
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002386
Iteration 68/1000 | Loss: 0.00002386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.386111555097159e-05, 2.386111555097159e-05, 2.386111555097159e-05, 2.386111555097159e-05, 2.386111555097159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.386111555097159e-05

Optimization complete. Final v2v error: 4.13926362991333 mm

Highest mean error: 4.643651485443115 mm for frame 61

Lowest mean error: 3.671367645263672 mm for frame 175

Saving results

Total time: 31.51917552947998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872324
Iteration 2/25 | Loss: 0.00130656
Iteration 3/25 | Loss: 0.00090365
Iteration 4/25 | Loss: 0.00083406
Iteration 5/25 | Loss: 0.00081870
Iteration 6/25 | Loss: 0.00081609
Iteration 7/25 | Loss: 0.00081587
Iteration 8/25 | Loss: 0.00081587
Iteration 9/25 | Loss: 0.00081587
Iteration 10/25 | Loss: 0.00081587
Iteration 11/25 | Loss: 0.00081587
Iteration 12/25 | Loss: 0.00081587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000815872976090759, 0.000815872976090759, 0.000815872976090759, 0.000815872976090759, 0.000815872976090759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000815872976090759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.18716395
Iteration 2/25 | Loss: 0.00079034
Iteration 3/25 | Loss: 0.00079031
Iteration 4/25 | Loss: 0.00079031
Iteration 5/25 | Loss: 0.00079031
Iteration 6/25 | Loss: 0.00079031
Iteration 7/25 | Loss: 0.00079031
Iteration 8/25 | Loss: 0.00079031
Iteration 9/25 | Loss: 0.00079031
Iteration 10/25 | Loss: 0.00079031
Iteration 11/25 | Loss: 0.00079031
Iteration 12/25 | Loss: 0.00079031
Iteration 13/25 | Loss: 0.00079031
Iteration 14/25 | Loss: 0.00079031
Iteration 15/25 | Loss: 0.00079031
Iteration 16/25 | Loss: 0.00079031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007903112564235926, 0.0007903112564235926, 0.0007903112564235926, 0.0007903112564235926, 0.0007903112564235926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007903112564235926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079031
Iteration 2/1000 | Loss: 0.00004233
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001950
Iteration 6/1000 | Loss: 0.00001875
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001791
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001727
Iteration 11/1000 | Loss: 0.00001713
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001680
Iteration 20/1000 | Loss: 0.00001676
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001672
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001664
Iteration 27/1000 | Loss: 0.00001664
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001663
Iteration 33/1000 | Loss: 0.00001663
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001662
Iteration 36/1000 | Loss: 0.00001662
Iteration 37/1000 | Loss: 0.00001661
Iteration 38/1000 | Loss: 0.00001661
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001660
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00001659
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001658
Iteration 47/1000 | Loss: 0.00001658
Iteration 48/1000 | Loss: 0.00001658
Iteration 49/1000 | Loss: 0.00001657
Iteration 50/1000 | Loss: 0.00001657
Iteration 51/1000 | Loss: 0.00001656
Iteration 52/1000 | Loss: 0.00001656
Iteration 53/1000 | Loss: 0.00001656
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001655
Iteration 56/1000 | Loss: 0.00001655
Iteration 57/1000 | Loss: 0.00001655
Iteration 58/1000 | Loss: 0.00001655
Iteration 59/1000 | Loss: 0.00001655
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001655
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001655
Iteration 66/1000 | Loss: 0.00001655
Iteration 67/1000 | Loss: 0.00001655
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001655
Iteration 70/1000 | Loss: 0.00001655
Iteration 71/1000 | Loss: 0.00001655
Iteration 72/1000 | Loss: 0.00001655
Iteration 73/1000 | Loss: 0.00001655
Iteration 74/1000 | Loss: 0.00001655
Iteration 75/1000 | Loss: 0.00001655
Iteration 76/1000 | Loss: 0.00001655
Iteration 77/1000 | Loss: 0.00001655
Iteration 78/1000 | Loss: 0.00001655
Iteration 79/1000 | Loss: 0.00001655
Iteration 80/1000 | Loss: 0.00001655
Iteration 81/1000 | Loss: 0.00001655
Iteration 82/1000 | Loss: 0.00001655
Iteration 83/1000 | Loss: 0.00001655
Iteration 84/1000 | Loss: 0.00001655
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001655
Iteration 87/1000 | Loss: 0.00001655
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001655
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001655
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.654657353356015e-05, 1.654657353356015e-05, 1.654657353356015e-05, 1.654657353356015e-05, 1.654657353356015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.654657353356015e-05

Optimization complete. Final v2v error: 3.4171838760375977 mm

Highest mean error: 3.695232391357422 mm for frame 163

Lowest mean error: 3.047360420227051 mm for frame 102

Saving results

Total time: 38.51103115081787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054071
Iteration 2/25 | Loss: 0.00208435
Iteration 3/25 | Loss: 0.00118325
Iteration 4/25 | Loss: 0.00102360
Iteration 5/25 | Loss: 0.00100881
Iteration 6/25 | Loss: 0.00094462
Iteration 7/25 | Loss: 0.00096272
Iteration 8/25 | Loss: 0.00099818
Iteration 9/25 | Loss: 0.00094862
Iteration 10/25 | Loss: 0.00095785
Iteration 11/25 | Loss: 0.00088100
Iteration 12/25 | Loss: 0.00085481
Iteration 13/25 | Loss: 0.00085464
Iteration 14/25 | Loss: 0.00082182
Iteration 15/25 | Loss: 0.00082565
Iteration 16/25 | Loss: 0.00081704
Iteration 17/25 | Loss: 0.00079877
Iteration 18/25 | Loss: 0.00080984
Iteration 19/25 | Loss: 0.00080558
Iteration 20/25 | Loss: 0.00080782
Iteration 21/25 | Loss: 0.00080576
Iteration 22/25 | Loss: 0.00080894
Iteration 23/25 | Loss: 0.00080252
Iteration 24/25 | Loss: 0.00079987
Iteration 25/25 | Loss: 0.00079705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55740297
Iteration 2/25 | Loss: 0.00106293
Iteration 3/25 | Loss: 0.00106292
Iteration 4/25 | Loss: 0.00106292
Iteration 5/25 | Loss: 0.00106292
Iteration 6/25 | Loss: 0.00106292
Iteration 7/25 | Loss: 0.00106292
Iteration 8/25 | Loss: 0.00106292
Iteration 9/25 | Loss: 0.00106292
Iteration 10/25 | Loss: 0.00106292
Iteration 11/25 | Loss: 0.00106292
Iteration 12/25 | Loss: 0.00106292
Iteration 13/25 | Loss: 0.00106292
Iteration 14/25 | Loss: 0.00106292
Iteration 15/25 | Loss: 0.00106292
Iteration 16/25 | Loss: 0.00106292
Iteration 17/25 | Loss: 0.00106292
Iteration 18/25 | Loss: 0.00106292
Iteration 19/25 | Loss: 0.00106292
Iteration 20/25 | Loss: 0.00106292
Iteration 21/25 | Loss: 0.00106292
Iteration 22/25 | Loss: 0.00106292
Iteration 23/25 | Loss: 0.00106292
Iteration 24/25 | Loss: 0.00106292
Iteration 25/25 | Loss: 0.00106292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106292
Iteration 2/1000 | Loss: 0.00026534
Iteration 3/1000 | Loss: 0.00055794
Iteration 4/1000 | Loss: 0.00043650
Iteration 5/1000 | Loss: 0.00039391
Iteration 6/1000 | Loss: 0.00043797
Iteration 7/1000 | Loss: 0.00050416
Iteration 8/1000 | Loss: 0.00017720
Iteration 9/1000 | Loss: 0.00016886
Iteration 10/1000 | Loss: 0.00061227
Iteration 11/1000 | Loss: 0.00009508
Iteration 12/1000 | Loss: 0.00017414
Iteration 13/1000 | Loss: 0.00012698
Iteration 14/1000 | Loss: 0.00006861
Iteration 15/1000 | Loss: 0.00035669
Iteration 16/1000 | Loss: 0.00019462
Iteration 17/1000 | Loss: 0.00039456
Iteration 18/1000 | Loss: 0.00007642
Iteration 19/1000 | Loss: 0.00004924
Iteration 20/1000 | Loss: 0.00017651
Iteration 21/1000 | Loss: 0.00014348
Iteration 22/1000 | Loss: 0.00067005
Iteration 23/1000 | Loss: 0.00016060
Iteration 24/1000 | Loss: 0.00005625
Iteration 25/1000 | Loss: 0.00005236
Iteration 26/1000 | Loss: 0.00031173
Iteration 27/1000 | Loss: 0.00008127
Iteration 28/1000 | Loss: 0.00005593
Iteration 29/1000 | Loss: 0.00019691
Iteration 30/1000 | Loss: 0.00017372
Iteration 31/1000 | Loss: 0.00006531
Iteration 32/1000 | Loss: 0.00006418
Iteration 33/1000 | Loss: 0.00027204
Iteration 34/1000 | Loss: 0.00007074
Iteration 35/1000 | Loss: 0.00007914
Iteration 36/1000 | Loss: 0.00006617
Iteration 37/1000 | Loss: 0.00005556
Iteration 38/1000 | Loss: 0.00006291
Iteration 39/1000 | Loss: 0.00007519
Iteration 40/1000 | Loss: 0.00006068
Iteration 41/1000 | Loss: 0.00005823
Iteration 42/1000 | Loss: 0.00007918
Iteration 43/1000 | Loss: 0.00006540
Iteration 44/1000 | Loss: 0.00005994
Iteration 45/1000 | Loss: 0.00006758
Iteration 46/1000 | Loss: 0.00005569
Iteration 47/1000 | Loss: 0.00006319
Iteration 48/1000 | Loss: 0.00006779
Iteration 49/1000 | Loss: 0.00006409
Iteration 50/1000 | Loss: 0.00005483
Iteration 51/1000 | Loss: 0.00007140
Iteration 52/1000 | Loss: 0.00005389
Iteration 53/1000 | Loss: 0.00005525
Iteration 54/1000 | Loss: 0.00005141
Iteration 55/1000 | Loss: 0.00004888
Iteration 56/1000 | Loss: 0.00004806
Iteration 57/1000 | Loss: 0.00005297
Iteration 58/1000 | Loss: 0.00004206
Iteration 59/1000 | Loss: 0.00004891
Iteration 60/1000 | Loss: 0.00005214
Iteration 61/1000 | Loss: 0.00005702
Iteration 62/1000 | Loss: 0.00006188
Iteration 63/1000 | Loss: 0.00005699
Iteration 64/1000 | Loss: 0.00006499
Iteration 65/1000 | Loss: 0.00006334
Iteration 66/1000 | Loss: 0.00006430
Iteration 67/1000 | Loss: 0.00006214
Iteration 68/1000 | Loss: 0.00005566
Iteration 69/1000 | Loss: 0.00005977
Iteration 70/1000 | Loss: 0.00006001
Iteration 71/1000 | Loss: 0.00004988
Iteration 72/1000 | Loss: 0.00005631
Iteration 73/1000 | Loss: 0.00009451
Iteration 74/1000 | Loss: 0.00005329
Iteration 75/1000 | Loss: 0.00003240
Iteration 76/1000 | Loss: 0.00005933
Iteration 77/1000 | Loss: 0.00002997
Iteration 78/1000 | Loss: 0.00004347
Iteration 79/1000 | Loss: 0.00004574
Iteration 80/1000 | Loss: 0.00005536
Iteration 81/1000 | Loss: 0.00006674
Iteration 82/1000 | Loss: 0.00007042
Iteration 83/1000 | Loss: 0.00006783
Iteration 84/1000 | Loss: 0.00006667
Iteration 85/1000 | Loss: 0.00010727
Iteration 86/1000 | Loss: 0.00004561
Iteration 87/1000 | Loss: 0.00003078
Iteration 88/1000 | Loss: 0.00005245
Iteration 89/1000 | Loss: 0.00002485
Iteration 90/1000 | Loss: 0.00001878
Iteration 91/1000 | Loss: 0.00001693
Iteration 92/1000 | Loss: 0.00001626
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00010880
Iteration 99/1000 | Loss: 0.00008515
Iteration 100/1000 | Loss: 0.00009957
Iteration 101/1000 | Loss: 0.00015845
Iteration 102/1000 | Loss: 0.00009540
Iteration 103/1000 | Loss: 0.00013257
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001372
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001371
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001369
Iteration 119/1000 | Loss: 0.00001369
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001368
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001367
Iteration 127/1000 | Loss: 0.00001367
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001366
Iteration 130/1000 | Loss: 0.00001366
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001365
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001364
Iteration 135/1000 | Loss: 0.00010146
Iteration 136/1000 | Loss: 0.00009678
Iteration 137/1000 | Loss: 0.00011797
Iteration 138/1000 | Loss: 0.00009451
Iteration 139/1000 | Loss: 0.00010816
Iteration 140/1000 | Loss: 0.00008379
Iteration 141/1000 | Loss: 0.00015968
Iteration 142/1000 | Loss: 0.00009093
Iteration 143/1000 | Loss: 0.00029878
Iteration 144/1000 | Loss: 0.00019868
Iteration 145/1000 | Loss: 0.00020386
Iteration 146/1000 | Loss: 0.00015088
Iteration 147/1000 | Loss: 0.00007112
Iteration 148/1000 | Loss: 0.00002285
Iteration 149/1000 | Loss: 0.00001803
Iteration 150/1000 | Loss: 0.00001662
Iteration 151/1000 | Loss: 0.00001548
Iteration 152/1000 | Loss: 0.00001454
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001315
Iteration 156/1000 | Loss: 0.00001303
Iteration 157/1000 | Loss: 0.00001302
Iteration 158/1000 | Loss: 0.00001301
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001297
Iteration 167/1000 | Loss: 0.00001294
Iteration 168/1000 | Loss: 0.00001294
Iteration 169/1000 | Loss: 0.00001292
Iteration 170/1000 | Loss: 0.00001292
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001291
Iteration 174/1000 | Loss: 0.00001291
Iteration 175/1000 | Loss: 0.00001291
Iteration 176/1000 | Loss: 0.00001291
Iteration 177/1000 | Loss: 0.00001290
Iteration 178/1000 | Loss: 0.00001289
Iteration 179/1000 | Loss: 0.00001289
Iteration 180/1000 | Loss: 0.00001289
Iteration 181/1000 | Loss: 0.00001289
Iteration 182/1000 | Loss: 0.00001288
Iteration 183/1000 | Loss: 0.00001288
Iteration 184/1000 | Loss: 0.00001288
Iteration 185/1000 | Loss: 0.00001288
Iteration 186/1000 | Loss: 0.00001288
Iteration 187/1000 | Loss: 0.00001288
Iteration 188/1000 | Loss: 0.00001288
Iteration 189/1000 | Loss: 0.00001288
Iteration 190/1000 | Loss: 0.00001288
Iteration 191/1000 | Loss: 0.00001288
Iteration 192/1000 | Loss: 0.00001288
Iteration 193/1000 | Loss: 0.00001288
Iteration 194/1000 | Loss: 0.00001286
Iteration 195/1000 | Loss: 0.00001286
Iteration 196/1000 | Loss: 0.00001286
Iteration 197/1000 | Loss: 0.00001285
Iteration 198/1000 | Loss: 0.00001285
Iteration 199/1000 | Loss: 0.00001285
Iteration 200/1000 | Loss: 0.00001285
Iteration 201/1000 | Loss: 0.00001285
Iteration 202/1000 | Loss: 0.00001285
Iteration 203/1000 | Loss: 0.00001285
Iteration 204/1000 | Loss: 0.00001285
Iteration 205/1000 | Loss: 0.00001285
Iteration 206/1000 | Loss: 0.00001285
Iteration 207/1000 | Loss: 0.00001285
Iteration 208/1000 | Loss: 0.00001285
Iteration 209/1000 | Loss: 0.00001285
Iteration 210/1000 | Loss: 0.00001284
Iteration 211/1000 | Loss: 0.00001284
Iteration 212/1000 | Loss: 0.00001284
Iteration 213/1000 | Loss: 0.00001284
Iteration 214/1000 | Loss: 0.00001284
Iteration 215/1000 | Loss: 0.00001284
Iteration 216/1000 | Loss: 0.00001284
Iteration 217/1000 | Loss: 0.00001284
Iteration 218/1000 | Loss: 0.00001284
Iteration 219/1000 | Loss: 0.00001284
Iteration 220/1000 | Loss: 0.00001284
Iteration 221/1000 | Loss: 0.00001284
Iteration 222/1000 | Loss: 0.00001284
Iteration 223/1000 | Loss: 0.00001284
Iteration 224/1000 | Loss: 0.00001284
Iteration 225/1000 | Loss: 0.00001284
Iteration 226/1000 | Loss: 0.00001284
Iteration 227/1000 | Loss: 0.00001283
Iteration 228/1000 | Loss: 0.00001283
Iteration 229/1000 | Loss: 0.00001283
Iteration 230/1000 | Loss: 0.00001283
Iteration 231/1000 | Loss: 0.00001283
Iteration 232/1000 | Loss: 0.00001283
Iteration 233/1000 | Loss: 0.00001283
Iteration 234/1000 | Loss: 0.00001283
Iteration 235/1000 | Loss: 0.00001283
Iteration 236/1000 | Loss: 0.00001283
Iteration 237/1000 | Loss: 0.00001283
Iteration 238/1000 | Loss: 0.00001283
Iteration 239/1000 | Loss: 0.00001283
Iteration 240/1000 | Loss: 0.00001283
Iteration 241/1000 | Loss: 0.00001283
Iteration 242/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.2830501873395406e-05, 1.2830501873395406e-05, 1.2830501873395406e-05, 1.2830501873395406e-05, 1.2830501873395406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2830501873395406e-05

Optimization complete. Final v2v error: 2.9780633449554443 mm

Highest mean error: 4.216433525085449 mm for frame 63

Lowest mean error: 2.568556785583496 mm for frame 142

Saving results

Total time: 237.42295002937317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882493
Iteration 2/25 | Loss: 0.00260901
Iteration 3/25 | Loss: 0.00161414
Iteration 4/25 | Loss: 0.00165664
Iteration 5/25 | Loss: 0.00102681
Iteration 6/25 | Loss: 0.00091941
Iteration 7/25 | Loss: 0.00089712
Iteration 8/25 | Loss: 0.00088225
Iteration 9/25 | Loss: 0.00087962
Iteration 10/25 | Loss: 0.00087811
Iteration 11/25 | Loss: 0.00087740
Iteration 12/25 | Loss: 0.00087800
Iteration 13/25 | Loss: 0.00087517
Iteration 14/25 | Loss: 0.00087464
Iteration 15/25 | Loss: 0.00087428
Iteration 16/25 | Loss: 0.00087403
Iteration 17/25 | Loss: 0.00087398
Iteration 18/25 | Loss: 0.00087398
Iteration 19/25 | Loss: 0.00087398
Iteration 20/25 | Loss: 0.00087398
Iteration 21/25 | Loss: 0.00087397
Iteration 22/25 | Loss: 0.00087397
Iteration 23/25 | Loss: 0.00087397
Iteration 24/25 | Loss: 0.00087397
Iteration 25/25 | Loss: 0.00087397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45488989
Iteration 2/25 | Loss: 0.00055376
Iteration 3/25 | Loss: 0.00055368
Iteration 4/25 | Loss: 0.00055368
Iteration 5/25 | Loss: 0.00055368
Iteration 6/25 | Loss: 0.00055368
Iteration 7/25 | Loss: 0.00055368
Iteration 8/25 | Loss: 0.00055368
Iteration 9/25 | Loss: 0.00055368
Iteration 10/25 | Loss: 0.00055368
Iteration 11/25 | Loss: 0.00055368
Iteration 12/25 | Loss: 0.00055368
Iteration 13/25 | Loss: 0.00055368
Iteration 14/25 | Loss: 0.00055368
Iteration 15/25 | Loss: 0.00055368
Iteration 16/25 | Loss: 0.00055368
Iteration 17/25 | Loss: 0.00055368
Iteration 18/25 | Loss: 0.00055368
Iteration 19/25 | Loss: 0.00055368
Iteration 20/25 | Loss: 0.00055368
Iteration 21/25 | Loss: 0.00055368
Iteration 22/25 | Loss: 0.00055368
Iteration 23/25 | Loss: 0.00055368
Iteration 24/25 | Loss: 0.00055368
Iteration 25/25 | Loss: 0.00055368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055368
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00002496
Iteration 4/1000 | Loss: 0.00002311
Iteration 5/1000 | Loss: 0.00002223
Iteration 6/1000 | Loss: 0.00002171
Iteration 7/1000 | Loss: 0.00002147
Iteration 8/1000 | Loss: 0.00002122
Iteration 9/1000 | Loss: 0.00002103
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00002078
Iteration 12/1000 | Loss: 0.00002071
Iteration 13/1000 | Loss: 0.00002069
Iteration 14/1000 | Loss: 0.00002069
Iteration 15/1000 | Loss: 0.00002068
Iteration 16/1000 | Loss: 0.00002064
Iteration 17/1000 | Loss: 0.00002063
Iteration 18/1000 | Loss: 0.00002063
Iteration 19/1000 | Loss: 0.00002063
Iteration 20/1000 | Loss: 0.00002063
Iteration 21/1000 | Loss: 0.00002063
Iteration 22/1000 | Loss: 0.00002063
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002063
Iteration 25/1000 | Loss: 0.00002063
Iteration 26/1000 | Loss: 0.00002063
Iteration 27/1000 | Loss: 0.00002063
Iteration 28/1000 | Loss: 0.00002063
Iteration 29/1000 | Loss: 0.00002063
Iteration 30/1000 | Loss: 0.00002063
Iteration 31/1000 | Loss: 0.00002063
Iteration 32/1000 | Loss: 0.00002063
Iteration 33/1000 | Loss: 0.00002063
Iteration 34/1000 | Loss: 0.00002062
Iteration 35/1000 | Loss: 0.00002061
Iteration 36/1000 | Loss: 0.00002061
Iteration 37/1000 | Loss: 0.00002060
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002059
Iteration 40/1000 | Loss: 0.00002059
Iteration 41/1000 | Loss: 0.00002059
Iteration 42/1000 | Loss: 0.00002059
Iteration 43/1000 | Loss: 0.00002059
Iteration 44/1000 | Loss: 0.00002059
Iteration 45/1000 | Loss: 0.00002059
Iteration 46/1000 | Loss: 0.00002059
Iteration 47/1000 | Loss: 0.00002058
Iteration 48/1000 | Loss: 0.00002058
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002057
Iteration 54/1000 | Loss: 0.00002056
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002056
Iteration 60/1000 | Loss: 0.00002056
Iteration 61/1000 | Loss: 0.00002056
Iteration 62/1000 | Loss: 0.00002056
Iteration 63/1000 | Loss: 0.00002056
Iteration 64/1000 | Loss: 0.00002056
Iteration 65/1000 | Loss: 0.00002055
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002055
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002054
Iteration 72/1000 | Loss: 0.00002054
Iteration 73/1000 | Loss: 0.00002054
Iteration 74/1000 | Loss: 0.00002054
Iteration 75/1000 | Loss: 0.00002054
Iteration 76/1000 | Loss: 0.00002054
Iteration 77/1000 | Loss: 0.00002054
Iteration 78/1000 | Loss: 0.00002054
Iteration 79/1000 | Loss: 0.00002054
Iteration 80/1000 | Loss: 0.00002054
Iteration 81/1000 | Loss: 0.00002054
Iteration 82/1000 | Loss: 0.00002054
Iteration 83/1000 | Loss: 0.00002053
Iteration 84/1000 | Loss: 0.00002053
Iteration 85/1000 | Loss: 0.00002053
Iteration 86/1000 | Loss: 0.00002053
Iteration 87/1000 | Loss: 0.00002053
Iteration 88/1000 | Loss: 0.00002053
Iteration 89/1000 | Loss: 0.00002053
Iteration 90/1000 | Loss: 0.00002053
Iteration 91/1000 | Loss: 0.00002053
Iteration 92/1000 | Loss: 0.00002053
Iteration 93/1000 | Loss: 0.00002053
Iteration 94/1000 | Loss: 0.00002053
Iteration 95/1000 | Loss: 0.00002053
Iteration 96/1000 | Loss: 0.00002053
Iteration 97/1000 | Loss: 0.00002053
Iteration 98/1000 | Loss: 0.00002053
Iteration 99/1000 | Loss: 0.00002053
Iteration 100/1000 | Loss: 0.00002053
Iteration 101/1000 | Loss: 0.00002053
Iteration 102/1000 | Loss: 0.00002053
Iteration 103/1000 | Loss: 0.00002053
Iteration 104/1000 | Loss: 0.00002053
Iteration 105/1000 | Loss: 0.00002053
Iteration 106/1000 | Loss: 0.00002053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.0526567823253572e-05, 2.0526567823253572e-05, 2.0526567823253572e-05, 2.0526567823253572e-05, 2.0526567823253572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0526567823253572e-05

Optimization complete. Final v2v error: 3.8389477729797363 mm

Highest mean error: 4.121796607971191 mm for frame 185

Lowest mean error: 3.5927834510803223 mm for frame 4

Saving results

Total time: 59.02770113945007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997774
Iteration 2/25 | Loss: 0.00305183
Iteration 3/25 | Loss: 0.00206093
Iteration 4/25 | Loss: 0.00193831
Iteration 5/25 | Loss: 0.00181288
Iteration 6/25 | Loss: 0.00170462
Iteration 7/25 | Loss: 0.00157369
Iteration 8/25 | Loss: 0.00154706
Iteration 9/25 | Loss: 0.00171109
Iteration 10/25 | Loss: 0.00198122
Iteration 11/25 | Loss: 0.00125140
Iteration 12/25 | Loss: 0.00097895
Iteration 13/25 | Loss: 0.00092698
Iteration 14/25 | Loss: 0.00090491
Iteration 15/25 | Loss: 0.00090728
Iteration 16/25 | Loss: 0.00090210
Iteration 17/25 | Loss: 0.00089741
Iteration 18/25 | Loss: 0.00089831
Iteration 19/25 | Loss: 0.00089712
Iteration 20/25 | Loss: 0.00089460
Iteration 21/25 | Loss: 0.00089344
Iteration 22/25 | Loss: 0.00088998
Iteration 23/25 | Loss: 0.00089081
Iteration 24/25 | Loss: 0.00088890
Iteration 25/25 | Loss: 0.00088943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.20979881
Iteration 2/25 | Loss: 0.00128710
Iteration 3/25 | Loss: 0.00064348
Iteration 4/25 | Loss: 0.00064348
Iteration 5/25 | Loss: 0.00064348
Iteration 6/25 | Loss: 0.00064348
Iteration 7/25 | Loss: 0.00064348
Iteration 8/25 | Loss: 0.00064348
Iteration 9/25 | Loss: 0.00064348
Iteration 10/25 | Loss: 0.00064348
Iteration 11/25 | Loss: 0.00064348
Iteration 12/25 | Loss: 0.00064348
Iteration 13/25 | Loss: 0.00064348
Iteration 14/25 | Loss: 0.00064348
Iteration 15/25 | Loss: 0.00064348
Iteration 16/25 | Loss: 0.00064348
Iteration 17/25 | Loss: 0.00064348
Iteration 18/25 | Loss: 0.00064348
Iteration 19/25 | Loss: 0.00064348
Iteration 20/25 | Loss: 0.00064348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006434781826101243, 0.0006434781826101243, 0.0006434781826101243, 0.0006434781826101243, 0.0006434781826101243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006434781826101243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064348
Iteration 2/1000 | Loss: 0.00047657
Iteration 3/1000 | Loss: 0.00005350
Iteration 4/1000 | Loss: 0.00007265
Iteration 5/1000 | Loss: 0.00007631
Iteration 6/1000 | Loss: 0.00026172
Iteration 7/1000 | Loss: 0.00044464
Iteration 8/1000 | Loss: 0.00081182
Iteration 9/1000 | Loss: 0.00005495
Iteration 10/1000 | Loss: 0.00004207
Iteration 11/1000 | Loss: 0.00003384
Iteration 12/1000 | Loss: 0.00048271
Iteration 13/1000 | Loss: 0.00003772
Iteration 14/1000 | Loss: 0.00067963
Iteration 15/1000 | Loss: 0.00129581
Iteration 16/1000 | Loss: 0.00088084
Iteration 17/1000 | Loss: 0.00107214
Iteration 18/1000 | Loss: 0.00061733
Iteration 19/1000 | Loss: 0.00041601
Iteration 20/1000 | Loss: 0.00004352
Iteration 21/1000 | Loss: 0.00003645
Iteration 22/1000 | Loss: 0.00003434
Iteration 23/1000 | Loss: 0.00003300
Iteration 24/1000 | Loss: 0.00048114
Iteration 25/1000 | Loss: 0.00022462
Iteration 26/1000 | Loss: 0.00007059
Iteration 27/1000 | Loss: 0.00004886
Iteration 28/1000 | Loss: 0.00003578
Iteration 29/1000 | Loss: 0.00002866
Iteration 30/1000 | Loss: 0.00002727
Iteration 31/1000 | Loss: 0.00082711
Iteration 32/1000 | Loss: 0.00007475
Iteration 33/1000 | Loss: 0.00023645
Iteration 34/1000 | Loss: 0.00012611
Iteration 35/1000 | Loss: 0.00006796
Iteration 36/1000 | Loss: 0.00013056
Iteration 37/1000 | Loss: 0.00002644
Iteration 38/1000 | Loss: 0.00012704
Iteration 39/1000 | Loss: 0.00006674
Iteration 40/1000 | Loss: 0.00005381
Iteration 41/1000 | Loss: 0.00002550
Iteration 42/1000 | Loss: 0.00050061
Iteration 43/1000 | Loss: 0.00061180
Iteration 44/1000 | Loss: 0.00074225
Iteration 45/1000 | Loss: 0.00032553
Iteration 46/1000 | Loss: 0.00018372
Iteration 47/1000 | Loss: 0.00002733
Iteration 48/1000 | Loss: 0.00002565
Iteration 49/1000 | Loss: 0.00002491
Iteration 50/1000 | Loss: 0.00048619
Iteration 51/1000 | Loss: 0.00032768
Iteration 52/1000 | Loss: 0.00047829
Iteration 53/1000 | Loss: 0.00003204
Iteration 54/1000 | Loss: 0.00002963
Iteration 55/1000 | Loss: 0.00002809
Iteration 56/1000 | Loss: 0.00002743
Iteration 57/1000 | Loss: 0.00002711
Iteration 58/1000 | Loss: 0.00002682
Iteration 59/1000 | Loss: 0.00002670
Iteration 60/1000 | Loss: 0.00002651
Iteration 61/1000 | Loss: 0.00002627
Iteration 62/1000 | Loss: 0.00002605
Iteration 63/1000 | Loss: 0.00002604
Iteration 64/1000 | Loss: 0.00002600
Iteration 65/1000 | Loss: 0.00002580
Iteration 66/1000 | Loss: 0.00002558
Iteration 67/1000 | Loss: 0.00002536
Iteration 68/1000 | Loss: 0.00002491
Iteration 69/1000 | Loss: 0.00002466
Iteration 70/1000 | Loss: 0.00002444
Iteration 71/1000 | Loss: 0.00002437
Iteration 72/1000 | Loss: 0.00044872
Iteration 73/1000 | Loss: 0.00003970
Iteration 74/1000 | Loss: 0.00003037
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002725
Iteration 77/1000 | Loss: 0.00002694
Iteration 78/1000 | Loss: 0.00002667
Iteration 79/1000 | Loss: 0.00002645
Iteration 80/1000 | Loss: 0.00002612
Iteration 81/1000 | Loss: 0.00002592
Iteration 82/1000 | Loss: 0.00002591
Iteration 83/1000 | Loss: 0.00002587
Iteration 84/1000 | Loss: 0.00063962
Iteration 85/1000 | Loss: 0.00091919
Iteration 86/1000 | Loss: 0.00118715
Iteration 87/1000 | Loss: 0.00004062
Iteration 88/1000 | Loss: 0.00029489
Iteration 89/1000 | Loss: 0.00002947
Iteration 90/1000 | Loss: 0.00002719
Iteration 91/1000 | Loss: 0.00002594
Iteration 92/1000 | Loss: 0.00002534
Iteration 93/1000 | Loss: 0.00002502
Iteration 94/1000 | Loss: 0.00002460
Iteration 95/1000 | Loss: 0.00072139
Iteration 96/1000 | Loss: 0.00032992
Iteration 97/1000 | Loss: 0.00002880
Iteration 98/1000 | Loss: 0.00010394
Iteration 99/1000 | Loss: 0.00003381
Iteration 100/1000 | Loss: 0.00002461
Iteration 101/1000 | Loss: 0.00002389
Iteration 102/1000 | Loss: 0.00002348
Iteration 103/1000 | Loss: 0.00002337
Iteration 104/1000 | Loss: 0.00002330
Iteration 105/1000 | Loss: 0.00002330
Iteration 106/1000 | Loss: 0.00002329
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002326
Iteration 109/1000 | Loss: 0.00002325
Iteration 110/1000 | Loss: 0.00002321
Iteration 111/1000 | Loss: 0.00002321
Iteration 112/1000 | Loss: 0.00002321
Iteration 113/1000 | Loss: 0.00002319
Iteration 114/1000 | Loss: 0.00002308
Iteration 115/1000 | Loss: 0.00002306
Iteration 116/1000 | Loss: 0.00002303
Iteration 117/1000 | Loss: 0.00002302
Iteration 118/1000 | Loss: 0.00002302
Iteration 119/1000 | Loss: 0.00002301
Iteration 120/1000 | Loss: 0.00002300
Iteration 121/1000 | Loss: 0.00002299
Iteration 122/1000 | Loss: 0.00002299
Iteration 123/1000 | Loss: 0.00002298
Iteration 124/1000 | Loss: 0.00002296
Iteration 125/1000 | Loss: 0.00002295
Iteration 126/1000 | Loss: 0.00002294
Iteration 127/1000 | Loss: 0.00002292
Iteration 128/1000 | Loss: 0.00002290
Iteration 129/1000 | Loss: 0.00002290
Iteration 130/1000 | Loss: 0.00002289
Iteration 131/1000 | Loss: 0.00002288
Iteration 132/1000 | Loss: 0.00002288
Iteration 133/1000 | Loss: 0.00002288
Iteration 134/1000 | Loss: 0.00002287
Iteration 135/1000 | Loss: 0.00002287
Iteration 136/1000 | Loss: 0.00002287
Iteration 137/1000 | Loss: 0.00002287
Iteration 138/1000 | Loss: 0.00002287
Iteration 139/1000 | Loss: 0.00002287
Iteration 140/1000 | Loss: 0.00002287
Iteration 141/1000 | Loss: 0.00002287
Iteration 142/1000 | Loss: 0.00002287
Iteration 143/1000 | Loss: 0.00002287
Iteration 144/1000 | Loss: 0.00002286
Iteration 145/1000 | Loss: 0.00002286
Iteration 146/1000 | Loss: 0.00002286
Iteration 147/1000 | Loss: 0.00002286
Iteration 148/1000 | Loss: 0.00002286
Iteration 149/1000 | Loss: 0.00002286
Iteration 150/1000 | Loss: 0.00002285
Iteration 151/1000 | Loss: 0.00002285
Iteration 152/1000 | Loss: 0.00002285
Iteration 153/1000 | Loss: 0.00002285
Iteration 154/1000 | Loss: 0.00002285
Iteration 155/1000 | Loss: 0.00002285
Iteration 156/1000 | Loss: 0.00002285
Iteration 157/1000 | Loss: 0.00002285
Iteration 158/1000 | Loss: 0.00002285
Iteration 159/1000 | Loss: 0.00002285
Iteration 160/1000 | Loss: 0.00002285
Iteration 161/1000 | Loss: 0.00002285
Iteration 162/1000 | Loss: 0.00002285
Iteration 163/1000 | Loss: 0.00002285
Iteration 164/1000 | Loss: 0.00002285
Iteration 165/1000 | Loss: 0.00002285
Iteration 166/1000 | Loss: 0.00002285
Iteration 167/1000 | Loss: 0.00002285
Iteration 168/1000 | Loss: 0.00002285
Iteration 169/1000 | Loss: 0.00002284
Iteration 170/1000 | Loss: 0.00002284
Iteration 171/1000 | Loss: 0.00002284
Iteration 172/1000 | Loss: 0.00002284
Iteration 173/1000 | Loss: 0.00002284
Iteration 174/1000 | Loss: 0.00002284
Iteration 175/1000 | Loss: 0.00002284
Iteration 176/1000 | Loss: 0.00002284
Iteration 177/1000 | Loss: 0.00002284
Iteration 178/1000 | Loss: 0.00002284
Iteration 179/1000 | Loss: 0.00002284
Iteration 180/1000 | Loss: 0.00002284
Iteration 181/1000 | Loss: 0.00002284
Iteration 182/1000 | Loss: 0.00002284
Iteration 183/1000 | Loss: 0.00002284
Iteration 184/1000 | Loss: 0.00002284
Iteration 185/1000 | Loss: 0.00002284
Iteration 186/1000 | Loss: 0.00002284
Iteration 187/1000 | Loss: 0.00002284
Iteration 188/1000 | Loss: 0.00002284
Iteration 189/1000 | Loss: 0.00002284
Iteration 190/1000 | Loss: 0.00002284
Iteration 191/1000 | Loss: 0.00002284
Iteration 192/1000 | Loss: 0.00002284
Iteration 193/1000 | Loss: 0.00002284
Iteration 194/1000 | Loss: 0.00002284
Iteration 195/1000 | Loss: 0.00002284
Iteration 196/1000 | Loss: 0.00002284
Iteration 197/1000 | Loss: 0.00002284
Iteration 198/1000 | Loss: 0.00002284
Iteration 199/1000 | Loss: 0.00002284
Iteration 200/1000 | Loss: 0.00002284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.284231777593959e-05, 2.284231777593959e-05, 2.284231777593959e-05, 2.284231777593959e-05, 2.284231777593959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.284231777593959e-05

Optimization complete. Final v2v error: 4.010702610015869 mm

Highest mean error: 4.792009353637695 mm for frame 102

Lowest mean error: 3.615936040878296 mm for frame 122

Saving results

Total time: 195.05675339698792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770338
Iteration 2/25 | Loss: 0.00104974
Iteration 3/25 | Loss: 0.00084296
Iteration 4/25 | Loss: 0.00080235
Iteration 5/25 | Loss: 0.00079764
Iteration 6/25 | Loss: 0.00079717
Iteration 7/25 | Loss: 0.00079717
Iteration 8/25 | Loss: 0.00079717
Iteration 9/25 | Loss: 0.00079717
Iteration 10/25 | Loss: 0.00079717
Iteration 11/25 | Loss: 0.00079717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007971677696332335, 0.0007971677696332335, 0.0007971677696332335, 0.0007971677696332335, 0.0007971677696332335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007971677696332335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46720314
Iteration 2/25 | Loss: 0.00041114
Iteration 3/25 | Loss: 0.00041114
Iteration 4/25 | Loss: 0.00041114
Iteration 5/25 | Loss: 0.00041114
Iteration 6/25 | Loss: 0.00041114
Iteration 7/25 | Loss: 0.00041114
Iteration 8/25 | Loss: 0.00041114
Iteration 9/25 | Loss: 0.00041114
Iteration 10/25 | Loss: 0.00041114
Iteration 11/25 | Loss: 0.00041114
Iteration 12/25 | Loss: 0.00041114
Iteration 13/25 | Loss: 0.00041114
Iteration 14/25 | Loss: 0.00041114
Iteration 15/25 | Loss: 0.00041114
Iteration 16/25 | Loss: 0.00041114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00041114247869700193, 0.00041114247869700193, 0.00041114247869700193, 0.00041114247869700193, 0.00041114247869700193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041114247869700193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041114
Iteration 2/1000 | Loss: 0.00003952
Iteration 3/1000 | Loss: 0.00002907
Iteration 4/1000 | Loss: 0.00002478
Iteration 5/1000 | Loss: 0.00002294
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002131
Iteration 8/1000 | Loss: 0.00002089
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00002067
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002061
Iteration 13/1000 | Loss: 0.00002051
Iteration 14/1000 | Loss: 0.00002047
Iteration 15/1000 | Loss: 0.00002044
Iteration 16/1000 | Loss: 0.00002042
Iteration 17/1000 | Loss: 0.00002040
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002035
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002031
Iteration 22/1000 | Loss: 0.00002030
Iteration 23/1000 | Loss: 0.00002028
Iteration 24/1000 | Loss: 0.00002028
Iteration 25/1000 | Loss: 0.00002027
Iteration 26/1000 | Loss: 0.00002027
Iteration 27/1000 | Loss: 0.00002024
Iteration 28/1000 | Loss: 0.00002024
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002023
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002023
Iteration 34/1000 | Loss: 0.00002023
Iteration 35/1000 | Loss: 0.00002023
Iteration 36/1000 | Loss: 0.00002023
Iteration 37/1000 | Loss: 0.00002023
Iteration 38/1000 | Loss: 0.00002023
Iteration 39/1000 | Loss: 0.00002022
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00002022
Iteration 42/1000 | Loss: 0.00002022
Iteration 43/1000 | Loss: 0.00002022
Iteration 44/1000 | Loss: 0.00002022
Iteration 45/1000 | Loss: 0.00002022
Iteration 46/1000 | Loss: 0.00002021
Iteration 47/1000 | Loss: 0.00002021
Iteration 48/1000 | Loss: 0.00002021
Iteration 49/1000 | Loss: 0.00002021
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002020
Iteration 54/1000 | Loss: 0.00002020
Iteration 55/1000 | Loss: 0.00002020
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002019
Iteration 61/1000 | Loss: 0.00002019
Iteration 62/1000 | Loss: 0.00002019
Iteration 63/1000 | Loss: 0.00002019
Iteration 64/1000 | Loss: 0.00002019
Iteration 65/1000 | Loss: 0.00002019
Iteration 66/1000 | Loss: 0.00002019
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002018
Iteration 72/1000 | Loss: 0.00002018
Iteration 73/1000 | Loss: 0.00002018
Iteration 74/1000 | Loss: 0.00002018
Iteration 75/1000 | Loss: 0.00002018
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002018
Iteration 79/1000 | Loss: 0.00002018
Iteration 80/1000 | Loss: 0.00002018
Iteration 81/1000 | Loss: 0.00002018
Iteration 82/1000 | Loss: 0.00002018
Iteration 83/1000 | Loss: 0.00002018
Iteration 84/1000 | Loss: 0.00002018
Iteration 85/1000 | Loss: 0.00002018
Iteration 86/1000 | Loss: 0.00002018
Iteration 87/1000 | Loss: 0.00002018
Iteration 88/1000 | Loss: 0.00002017
Iteration 89/1000 | Loss: 0.00002017
Iteration 90/1000 | Loss: 0.00002017
Iteration 91/1000 | Loss: 0.00002017
Iteration 92/1000 | Loss: 0.00002017
Iteration 93/1000 | Loss: 0.00002017
Iteration 94/1000 | Loss: 0.00002017
Iteration 95/1000 | Loss: 0.00002017
Iteration 96/1000 | Loss: 0.00002017
Iteration 97/1000 | Loss: 0.00002017
Iteration 98/1000 | Loss: 0.00002017
Iteration 99/1000 | Loss: 0.00002017
Iteration 100/1000 | Loss: 0.00002017
Iteration 101/1000 | Loss: 0.00002017
Iteration 102/1000 | Loss: 0.00002016
Iteration 103/1000 | Loss: 0.00002016
Iteration 104/1000 | Loss: 0.00002016
Iteration 105/1000 | Loss: 0.00002016
Iteration 106/1000 | Loss: 0.00002016
Iteration 107/1000 | Loss: 0.00002016
Iteration 108/1000 | Loss: 0.00002015
Iteration 109/1000 | Loss: 0.00002015
Iteration 110/1000 | Loss: 0.00002015
Iteration 111/1000 | Loss: 0.00002015
Iteration 112/1000 | Loss: 0.00002015
Iteration 113/1000 | Loss: 0.00002015
Iteration 114/1000 | Loss: 0.00002015
Iteration 115/1000 | Loss: 0.00002015
Iteration 116/1000 | Loss: 0.00002015
Iteration 117/1000 | Loss: 0.00002015
Iteration 118/1000 | Loss: 0.00002015
Iteration 119/1000 | Loss: 0.00002014
Iteration 120/1000 | Loss: 0.00002014
Iteration 121/1000 | Loss: 0.00002014
Iteration 122/1000 | Loss: 0.00002013
Iteration 123/1000 | Loss: 0.00002013
Iteration 124/1000 | Loss: 0.00002013
Iteration 125/1000 | Loss: 0.00002013
Iteration 126/1000 | Loss: 0.00002013
Iteration 127/1000 | Loss: 0.00002013
Iteration 128/1000 | Loss: 0.00002012
Iteration 129/1000 | Loss: 0.00002012
Iteration 130/1000 | Loss: 0.00002012
Iteration 131/1000 | Loss: 0.00002012
Iteration 132/1000 | Loss: 0.00002012
Iteration 133/1000 | Loss: 0.00002012
Iteration 134/1000 | Loss: 0.00002012
Iteration 135/1000 | Loss: 0.00002012
Iteration 136/1000 | Loss: 0.00002012
Iteration 137/1000 | Loss: 0.00002012
Iteration 138/1000 | Loss: 0.00002012
Iteration 139/1000 | Loss: 0.00002012
Iteration 140/1000 | Loss: 0.00002012
Iteration 141/1000 | Loss: 0.00002012
Iteration 142/1000 | Loss: 0.00002012
Iteration 143/1000 | Loss: 0.00002012
Iteration 144/1000 | Loss: 0.00002012
Iteration 145/1000 | Loss: 0.00002012
Iteration 146/1000 | Loss: 0.00002011
Iteration 147/1000 | Loss: 0.00002011
Iteration 148/1000 | Loss: 0.00002011
Iteration 149/1000 | Loss: 0.00002011
Iteration 150/1000 | Loss: 0.00002011
Iteration 151/1000 | Loss: 0.00002011
Iteration 152/1000 | Loss: 0.00002011
Iteration 153/1000 | Loss: 0.00002011
Iteration 154/1000 | Loss: 0.00002011
Iteration 155/1000 | Loss: 0.00002011
Iteration 156/1000 | Loss: 0.00002011
Iteration 157/1000 | Loss: 0.00002011
Iteration 158/1000 | Loss: 0.00002011
Iteration 159/1000 | Loss: 0.00002011
Iteration 160/1000 | Loss: 0.00002011
Iteration 161/1000 | Loss: 0.00002011
Iteration 162/1000 | Loss: 0.00002011
Iteration 163/1000 | Loss: 0.00002011
Iteration 164/1000 | Loss: 0.00002010
Iteration 165/1000 | Loss: 0.00002010
Iteration 166/1000 | Loss: 0.00002010
Iteration 167/1000 | Loss: 0.00002010
Iteration 168/1000 | Loss: 0.00002010
Iteration 169/1000 | Loss: 0.00002010
Iteration 170/1000 | Loss: 0.00002010
Iteration 171/1000 | Loss: 0.00002010
Iteration 172/1000 | Loss: 0.00002010
Iteration 173/1000 | Loss: 0.00002010
Iteration 174/1000 | Loss: 0.00002010
Iteration 175/1000 | Loss: 0.00002010
Iteration 176/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.0102614143979736e-05, 2.0102614143979736e-05, 2.0102614143979736e-05, 2.0102614143979736e-05, 2.0102614143979736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0102614143979736e-05

Optimization complete. Final v2v error: 3.7995026111602783 mm

Highest mean error: 4.102907657623291 mm for frame 230

Lowest mean error: 3.5952956676483154 mm for frame 30

Saving results

Total time: 39.2762188911438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456274
Iteration 2/25 | Loss: 0.00090693
Iteration 3/25 | Loss: 0.00075257
Iteration 4/25 | Loss: 0.00073264
Iteration 5/25 | Loss: 0.00072972
Iteration 6/25 | Loss: 0.00072907
Iteration 7/25 | Loss: 0.00072907
Iteration 8/25 | Loss: 0.00072907
Iteration 9/25 | Loss: 0.00072907
Iteration 10/25 | Loss: 0.00072907
Iteration 11/25 | Loss: 0.00072907
Iteration 12/25 | Loss: 0.00072907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000729073362890631, 0.000729073362890631, 0.000729073362890631, 0.000729073362890631, 0.000729073362890631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000729073362890631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45809031
Iteration 2/25 | Loss: 0.00042024
Iteration 3/25 | Loss: 0.00042023
Iteration 4/25 | Loss: 0.00042023
Iteration 5/25 | Loss: 0.00042023
Iteration 6/25 | Loss: 0.00042023
Iteration 7/25 | Loss: 0.00042023
Iteration 8/25 | Loss: 0.00042023
Iteration 9/25 | Loss: 0.00042023
Iteration 10/25 | Loss: 0.00042023
Iteration 11/25 | Loss: 0.00042023
Iteration 12/25 | Loss: 0.00042023
Iteration 13/25 | Loss: 0.00042023
Iteration 14/25 | Loss: 0.00042023
Iteration 15/25 | Loss: 0.00042023
Iteration 16/25 | Loss: 0.00042023
Iteration 17/25 | Loss: 0.00042023
Iteration 18/25 | Loss: 0.00042023
Iteration 19/25 | Loss: 0.00042023
Iteration 20/25 | Loss: 0.00042023
Iteration 21/25 | Loss: 0.00042023
Iteration 22/25 | Loss: 0.00042023
Iteration 23/25 | Loss: 0.00042023
Iteration 24/25 | Loss: 0.00042023
Iteration 25/25 | Loss: 0.00042023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042023
Iteration 2/1000 | Loss: 0.00002519
Iteration 3/1000 | Loss: 0.00001561
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001336
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001280
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001240
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001220
Iteration 26/1000 | Loss: 0.00001220
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001219
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001216
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001213
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001207
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001204
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001201
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001200
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001198
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001197
Iteration 95/1000 | Loss: 0.00001197
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001196
Iteration 100/1000 | Loss: 0.00001196
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001195
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001194
Iteration 121/1000 | Loss: 0.00001194
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.1943817298742943e-05, 1.1943817298742943e-05, 1.1943817298742943e-05, 1.1943817298742943e-05, 1.1943817298742943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1943817298742943e-05

Optimization complete. Final v2v error: 2.946547031402588 mm

Highest mean error: 3.2963809967041016 mm for frame 185

Lowest mean error: 2.6073107719421387 mm for frame 167

Saving results

Total time: 37.20168995857239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840231
Iteration 2/25 | Loss: 0.00116669
Iteration 3/25 | Loss: 0.00086012
Iteration 4/25 | Loss: 0.00082345
Iteration 5/25 | Loss: 0.00081734
Iteration 6/25 | Loss: 0.00081571
Iteration 7/25 | Loss: 0.00081550
Iteration 8/25 | Loss: 0.00081550
Iteration 9/25 | Loss: 0.00081550
Iteration 10/25 | Loss: 0.00081550
Iteration 11/25 | Loss: 0.00081550
Iteration 12/25 | Loss: 0.00081550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008154959650710225, 0.0008154959650710225, 0.0008154959650710225, 0.0008154959650710225, 0.0008154959650710225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008154959650710225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00786459
Iteration 2/25 | Loss: 0.00044098
Iteration 3/25 | Loss: 0.00044097
Iteration 4/25 | Loss: 0.00044097
Iteration 5/25 | Loss: 0.00044097
Iteration 6/25 | Loss: 0.00044097
Iteration 7/25 | Loss: 0.00044097
Iteration 8/25 | Loss: 0.00044097
Iteration 9/25 | Loss: 0.00044097
Iteration 10/25 | Loss: 0.00044097
Iteration 11/25 | Loss: 0.00044097
Iteration 12/25 | Loss: 0.00044097
Iteration 13/25 | Loss: 0.00044097
Iteration 14/25 | Loss: 0.00044097
Iteration 15/25 | Loss: 0.00044097
Iteration 16/25 | Loss: 0.00044097
Iteration 17/25 | Loss: 0.00044097
Iteration 18/25 | Loss: 0.00044097
Iteration 19/25 | Loss: 0.00044097
Iteration 20/25 | Loss: 0.00044097
Iteration 21/25 | Loss: 0.00044097
Iteration 22/25 | Loss: 0.00044097
Iteration 23/25 | Loss: 0.00044097
Iteration 24/25 | Loss: 0.00044097
Iteration 25/25 | Loss: 0.00044097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004409717512317002, 0.0004409717512317002, 0.0004409717512317002, 0.0004409717512317002, 0.0004409717512317002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004409717512317002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044097
Iteration 2/1000 | Loss: 0.00002652
Iteration 3/1000 | Loss: 0.00002277
Iteration 4/1000 | Loss: 0.00002158
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00001980
Iteration 7/1000 | Loss: 0.00001940
Iteration 8/1000 | Loss: 0.00001919
Iteration 9/1000 | Loss: 0.00001916
Iteration 10/1000 | Loss: 0.00001915
Iteration 11/1000 | Loss: 0.00001915
Iteration 12/1000 | Loss: 0.00001914
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001896
Iteration 22/1000 | Loss: 0.00001896
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001888
Iteration 25/1000 | Loss: 0.00001888
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001887
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001887
Iteration 31/1000 | Loss: 0.00001887
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001887
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001887
Iteration 37/1000 | Loss: 0.00001886
Iteration 38/1000 | Loss: 0.00001886
Iteration 39/1000 | Loss: 0.00001886
Iteration 40/1000 | Loss: 0.00001886
Iteration 41/1000 | Loss: 0.00001886
Iteration 42/1000 | Loss: 0.00001886
Iteration 43/1000 | Loss: 0.00001885
Iteration 44/1000 | Loss: 0.00001885
Iteration 45/1000 | Loss: 0.00001885
Iteration 46/1000 | Loss: 0.00001885
Iteration 47/1000 | Loss: 0.00001884
Iteration 48/1000 | Loss: 0.00001884
Iteration 49/1000 | Loss: 0.00001883
Iteration 50/1000 | Loss: 0.00001883
Iteration 51/1000 | Loss: 0.00001883
Iteration 52/1000 | Loss: 0.00001882
Iteration 53/1000 | Loss: 0.00001882
Iteration 54/1000 | Loss: 0.00001882
Iteration 55/1000 | Loss: 0.00001882
Iteration 56/1000 | Loss: 0.00001882
Iteration 57/1000 | Loss: 0.00001882
Iteration 58/1000 | Loss: 0.00001882
Iteration 59/1000 | Loss: 0.00001882
Iteration 60/1000 | Loss: 0.00001882
Iteration 61/1000 | Loss: 0.00001882
Iteration 62/1000 | Loss: 0.00001881
Iteration 63/1000 | Loss: 0.00001881
Iteration 64/1000 | Loss: 0.00001881
Iteration 65/1000 | Loss: 0.00001881
Iteration 66/1000 | Loss: 0.00001881
Iteration 67/1000 | Loss: 0.00001880
Iteration 68/1000 | Loss: 0.00001880
Iteration 69/1000 | Loss: 0.00001880
Iteration 70/1000 | Loss: 0.00001880
Iteration 71/1000 | Loss: 0.00001879
Iteration 72/1000 | Loss: 0.00001879
Iteration 73/1000 | Loss: 0.00001879
Iteration 74/1000 | Loss: 0.00001879
Iteration 75/1000 | Loss: 0.00001879
Iteration 76/1000 | Loss: 0.00001879
Iteration 77/1000 | Loss: 0.00001879
Iteration 78/1000 | Loss: 0.00001878
Iteration 79/1000 | Loss: 0.00001878
Iteration 80/1000 | Loss: 0.00001878
Iteration 81/1000 | Loss: 0.00001878
Iteration 82/1000 | Loss: 0.00001878
Iteration 83/1000 | Loss: 0.00001878
Iteration 84/1000 | Loss: 0.00001878
Iteration 85/1000 | Loss: 0.00001878
Iteration 86/1000 | Loss: 0.00001878
Iteration 87/1000 | Loss: 0.00001878
Iteration 88/1000 | Loss: 0.00001878
Iteration 89/1000 | Loss: 0.00001878
Iteration 90/1000 | Loss: 0.00001878
Iteration 91/1000 | Loss: 0.00001878
Iteration 92/1000 | Loss: 0.00001877
Iteration 93/1000 | Loss: 0.00001877
Iteration 94/1000 | Loss: 0.00001877
Iteration 95/1000 | Loss: 0.00001877
Iteration 96/1000 | Loss: 0.00001877
Iteration 97/1000 | Loss: 0.00001877
Iteration 98/1000 | Loss: 0.00001877
Iteration 99/1000 | Loss: 0.00001877
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001877
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001876
Iteration 104/1000 | Loss: 0.00001876
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001876
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001876
Iteration 116/1000 | Loss: 0.00001876
Iteration 117/1000 | Loss: 0.00001876
Iteration 118/1000 | Loss: 0.00001876
Iteration 119/1000 | Loss: 0.00001876
Iteration 120/1000 | Loss: 0.00001876
Iteration 121/1000 | Loss: 0.00001876
Iteration 122/1000 | Loss: 0.00001876
Iteration 123/1000 | Loss: 0.00001876
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00001876
Iteration 126/1000 | Loss: 0.00001875
Iteration 127/1000 | Loss: 0.00001875
Iteration 128/1000 | Loss: 0.00001875
Iteration 129/1000 | Loss: 0.00001875
Iteration 130/1000 | Loss: 0.00001875
Iteration 131/1000 | Loss: 0.00001875
Iteration 132/1000 | Loss: 0.00001875
Iteration 133/1000 | Loss: 0.00001875
Iteration 134/1000 | Loss: 0.00001875
Iteration 135/1000 | Loss: 0.00001875
Iteration 136/1000 | Loss: 0.00001875
Iteration 137/1000 | Loss: 0.00001875
Iteration 138/1000 | Loss: 0.00001875
Iteration 139/1000 | Loss: 0.00001875
Iteration 140/1000 | Loss: 0.00001875
Iteration 141/1000 | Loss: 0.00001875
Iteration 142/1000 | Loss: 0.00001875
Iteration 143/1000 | Loss: 0.00001875
Iteration 144/1000 | Loss: 0.00001874
Iteration 145/1000 | Loss: 0.00001874
Iteration 146/1000 | Loss: 0.00001874
Iteration 147/1000 | Loss: 0.00001874
Iteration 148/1000 | Loss: 0.00001874
Iteration 149/1000 | Loss: 0.00001874
Iteration 150/1000 | Loss: 0.00001874
Iteration 151/1000 | Loss: 0.00001874
Iteration 152/1000 | Loss: 0.00001874
Iteration 153/1000 | Loss: 0.00001873
Iteration 154/1000 | Loss: 0.00001873
Iteration 155/1000 | Loss: 0.00001873
Iteration 156/1000 | Loss: 0.00001873
Iteration 157/1000 | Loss: 0.00001873
Iteration 158/1000 | Loss: 0.00001873
Iteration 159/1000 | Loss: 0.00001873
Iteration 160/1000 | Loss: 0.00001873
Iteration 161/1000 | Loss: 0.00001873
Iteration 162/1000 | Loss: 0.00001872
Iteration 163/1000 | Loss: 0.00001872
Iteration 164/1000 | Loss: 0.00001872
Iteration 165/1000 | Loss: 0.00001872
Iteration 166/1000 | Loss: 0.00001872
Iteration 167/1000 | Loss: 0.00001872
Iteration 168/1000 | Loss: 0.00001872
Iteration 169/1000 | Loss: 0.00001872
Iteration 170/1000 | Loss: 0.00001872
Iteration 171/1000 | Loss: 0.00001872
Iteration 172/1000 | Loss: 0.00001872
Iteration 173/1000 | Loss: 0.00001872
Iteration 174/1000 | Loss: 0.00001872
Iteration 175/1000 | Loss: 0.00001872
Iteration 176/1000 | Loss: 0.00001872
Iteration 177/1000 | Loss: 0.00001872
Iteration 178/1000 | Loss: 0.00001872
Iteration 179/1000 | Loss: 0.00001872
Iteration 180/1000 | Loss: 0.00001872
Iteration 181/1000 | Loss: 0.00001872
Iteration 182/1000 | Loss: 0.00001872
Iteration 183/1000 | Loss: 0.00001872
Iteration 184/1000 | Loss: 0.00001872
Iteration 185/1000 | Loss: 0.00001872
Iteration 186/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.8717099010245875e-05, 1.8717099010245875e-05, 1.8717099010245875e-05, 1.8717099010245875e-05, 1.8717099010245875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8717099010245875e-05

Optimization complete. Final v2v error: 3.6886863708496094 mm

Highest mean error: 3.940925359725952 mm for frame 121

Lowest mean error: 3.534499168395996 mm for frame 136

Saving results

Total time: 34.623204469680786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931434
Iteration 2/25 | Loss: 0.00160896
Iteration 3/25 | Loss: 0.00111118
Iteration 4/25 | Loss: 0.00103453
Iteration 5/25 | Loss: 0.00097991
Iteration 6/25 | Loss: 0.00099634
Iteration 7/25 | Loss: 0.00095637
Iteration 8/25 | Loss: 0.00094588
Iteration 9/25 | Loss: 0.00095084
Iteration 10/25 | Loss: 0.00095166
Iteration 11/25 | Loss: 0.00095606
Iteration 12/25 | Loss: 0.00096215
Iteration 13/25 | Loss: 0.00094813
Iteration 14/25 | Loss: 0.00094410
Iteration 15/25 | Loss: 0.00093351
Iteration 16/25 | Loss: 0.00093066
Iteration 17/25 | Loss: 0.00092737
Iteration 18/25 | Loss: 0.00093823
Iteration 19/25 | Loss: 0.00093549
Iteration 20/25 | Loss: 0.00092871
Iteration 21/25 | Loss: 0.00093013
Iteration 22/25 | Loss: 0.00093447
Iteration 23/25 | Loss: 0.00093051
Iteration 24/25 | Loss: 0.00093047
Iteration 25/25 | Loss: 0.00092756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42361903
Iteration 2/25 | Loss: 0.00094223
Iteration 3/25 | Loss: 0.00094213
Iteration 4/25 | Loss: 0.00094213
Iteration 5/25 | Loss: 0.00094213
Iteration 6/25 | Loss: 0.00094213
Iteration 7/25 | Loss: 0.00094213
Iteration 8/25 | Loss: 0.00094213
Iteration 9/25 | Loss: 0.00094213
Iteration 10/25 | Loss: 0.00094213
Iteration 11/25 | Loss: 0.00094213
Iteration 12/25 | Loss: 0.00094213
Iteration 13/25 | Loss: 0.00094213
Iteration 14/25 | Loss: 0.00094213
Iteration 15/25 | Loss: 0.00094213
Iteration 16/25 | Loss: 0.00094213
Iteration 17/25 | Loss: 0.00094213
Iteration 18/25 | Loss: 0.00094213
Iteration 19/25 | Loss: 0.00094213
Iteration 20/25 | Loss: 0.00094213
Iteration 21/25 | Loss: 0.00094213
Iteration 22/25 | Loss: 0.00094213
Iteration 23/25 | Loss: 0.00094213
Iteration 24/25 | Loss: 0.00094213
Iteration 25/25 | Loss: 0.00094213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094213
Iteration 2/1000 | Loss: 0.00171785
Iteration 3/1000 | Loss: 0.00868576
Iteration 4/1000 | Loss: 0.00223225
Iteration 5/1000 | Loss: 0.00079344
Iteration 6/1000 | Loss: 0.00077065
Iteration 7/1000 | Loss: 0.00074014
Iteration 8/1000 | Loss: 0.00364050
Iteration 9/1000 | Loss: 0.00308645
Iteration 10/1000 | Loss: 0.00359638
Iteration 11/1000 | Loss: 0.00259062
Iteration 12/1000 | Loss: 0.00252652
Iteration 13/1000 | Loss: 0.00208741
Iteration 14/1000 | Loss: 0.00229685
Iteration 15/1000 | Loss: 0.00372531
Iteration 16/1000 | Loss: 0.00072507
Iteration 17/1000 | Loss: 0.00049007
Iteration 18/1000 | Loss: 0.00278608
Iteration 19/1000 | Loss: 0.00060875
Iteration 20/1000 | Loss: 0.00491094
Iteration 21/1000 | Loss: 0.00169182
Iteration 22/1000 | Loss: 0.00154808
Iteration 23/1000 | Loss: 0.00126072
Iteration 24/1000 | Loss: 0.00047180
Iteration 25/1000 | Loss: 0.00301030
Iteration 26/1000 | Loss: 0.00282102
Iteration 27/1000 | Loss: 0.00324985
Iteration 28/1000 | Loss: 0.00229334
Iteration 29/1000 | Loss: 0.00267056
Iteration 30/1000 | Loss: 0.00312813
Iteration 31/1000 | Loss: 0.00506453
Iteration 32/1000 | Loss: 0.00117679
Iteration 33/1000 | Loss: 0.00079296
Iteration 34/1000 | Loss: 0.00158505
Iteration 35/1000 | Loss: 0.00116700
Iteration 36/1000 | Loss: 0.00061178
Iteration 37/1000 | Loss: 0.00155733
Iteration 38/1000 | Loss: 0.00098775
Iteration 39/1000 | Loss: 0.00075547
Iteration 40/1000 | Loss: 0.00073306
Iteration 41/1000 | Loss: 0.00052542
Iteration 42/1000 | Loss: 0.00215100
Iteration 43/1000 | Loss: 0.00199345
Iteration 44/1000 | Loss: 0.00113916
Iteration 45/1000 | Loss: 0.00293656
Iteration 46/1000 | Loss: 0.00632161
Iteration 47/1000 | Loss: 0.00475145
Iteration 48/1000 | Loss: 0.00250600
Iteration 49/1000 | Loss: 0.00399305
Iteration 50/1000 | Loss: 0.00476955
Iteration 51/1000 | Loss: 0.00464842
Iteration 52/1000 | Loss: 0.00459676
Iteration 53/1000 | Loss: 0.00227913
Iteration 54/1000 | Loss: 0.00126383
Iteration 55/1000 | Loss: 0.00072962
Iteration 56/1000 | Loss: 0.00083173
Iteration 57/1000 | Loss: 0.00044323
Iteration 58/1000 | Loss: 0.00036904
Iteration 59/1000 | Loss: 0.00088926
Iteration 60/1000 | Loss: 0.00095120
Iteration 61/1000 | Loss: 0.00087865
Iteration 62/1000 | Loss: 0.00094399
Iteration 63/1000 | Loss: 0.00073114
Iteration 64/1000 | Loss: 0.00056891
Iteration 65/1000 | Loss: 0.00066558
Iteration 66/1000 | Loss: 0.00171349
Iteration 67/1000 | Loss: 0.00089785
Iteration 68/1000 | Loss: 0.00090824
Iteration 69/1000 | Loss: 0.00061154
Iteration 70/1000 | Loss: 0.00045892
Iteration 71/1000 | Loss: 0.00035190
Iteration 72/1000 | Loss: 0.00046644
Iteration 73/1000 | Loss: 0.00061635
Iteration 74/1000 | Loss: 0.00051331
Iteration 75/1000 | Loss: 0.00061846
Iteration 76/1000 | Loss: 0.00062268
Iteration 77/1000 | Loss: 0.00050739
Iteration 78/1000 | Loss: 0.00059760
Iteration 79/1000 | Loss: 0.00059556
Iteration 80/1000 | Loss: 0.00030387
Iteration 81/1000 | Loss: 0.00021965
Iteration 82/1000 | Loss: 0.00053849
Iteration 83/1000 | Loss: 0.00068394
Iteration 84/1000 | Loss: 0.00031651
Iteration 85/1000 | Loss: 0.00053020
Iteration 86/1000 | Loss: 0.00057854
Iteration 87/1000 | Loss: 0.00152661
Iteration 88/1000 | Loss: 0.00040540
Iteration 89/1000 | Loss: 0.00076626
Iteration 90/1000 | Loss: 0.00049422
Iteration 91/1000 | Loss: 0.00130170
Iteration 92/1000 | Loss: 0.00047998
Iteration 93/1000 | Loss: 0.00070997
Iteration 94/1000 | Loss: 0.00086889
Iteration 95/1000 | Loss: 0.00044783
Iteration 96/1000 | Loss: 0.00066360
Iteration 97/1000 | Loss: 0.00039629
Iteration 98/1000 | Loss: 0.00075143
Iteration 99/1000 | Loss: 0.00025825
Iteration 100/1000 | Loss: 0.00046354
Iteration 101/1000 | Loss: 0.00048248
Iteration 102/1000 | Loss: 0.00043477
Iteration 103/1000 | Loss: 0.00047488
Iteration 104/1000 | Loss: 0.00049499
Iteration 105/1000 | Loss: 0.00048339
Iteration 106/1000 | Loss: 0.00060107
Iteration 107/1000 | Loss: 0.00055962
Iteration 108/1000 | Loss: 0.00051988
Iteration 109/1000 | Loss: 0.00058287
Iteration 110/1000 | Loss: 0.00056422
Iteration 111/1000 | Loss: 0.00051645
Iteration 112/1000 | Loss: 0.00054630
Iteration 113/1000 | Loss: 0.00055648
Iteration 114/1000 | Loss: 0.00034504
Iteration 115/1000 | Loss: 0.00060480
Iteration 116/1000 | Loss: 0.00042901
Iteration 117/1000 | Loss: 0.00067551
Iteration 118/1000 | Loss: 0.00044846
Iteration 119/1000 | Loss: 0.00032781
Iteration 120/1000 | Loss: 0.00044780
Iteration 121/1000 | Loss: 0.00033897
Iteration 122/1000 | Loss: 0.00015588
Iteration 123/1000 | Loss: 0.00022204
Iteration 124/1000 | Loss: 0.00045022
Iteration 125/1000 | Loss: 0.00039280
Iteration 126/1000 | Loss: 0.00039816
Iteration 127/1000 | Loss: 0.00016697
Iteration 128/1000 | Loss: 0.00005124
Iteration 129/1000 | Loss: 0.00016875
Iteration 130/1000 | Loss: 0.00040190
Iteration 131/1000 | Loss: 0.00034670
Iteration 132/1000 | Loss: 0.00033704
Iteration 133/1000 | Loss: 0.00037527
Iteration 134/1000 | Loss: 0.00065411
Iteration 135/1000 | Loss: 0.00014362
Iteration 136/1000 | Loss: 0.00012018
Iteration 137/1000 | Loss: 0.00024154
Iteration 138/1000 | Loss: 0.00004759
Iteration 139/1000 | Loss: 0.00020875
Iteration 140/1000 | Loss: 0.00023372
Iteration 141/1000 | Loss: 0.00020266
Iteration 142/1000 | Loss: 0.00028140
Iteration 143/1000 | Loss: 0.00019470
Iteration 144/1000 | Loss: 0.00024204
Iteration 145/1000 | Loss: 0.00060225
Iteration 146/1000 | Loss: 0.00065576
Iteration 147/1000 | Loss: 0.00066899
Iteration 148/1000 | Loss: 0.00028793
Iteration 149/1000 | Loss: 0.00046098
Iteration 150/1000 | Loss: 0.00032998
Iteration 151/1000 | Loss: 0.00039587
Iteration 152/1000 | Loss: 0.00036751
Iteration 153/1000 | Loss: 0.00015423
Iteration 154/1000 | Loss: 0.00038140
Iteration 155/1000 | Loss: 0.00024844
Iteration 156/1000 | Loss: 0.00026228
Iteration 157/1000 | Loss: 0.00019827
Iteration 158/1000 | Loss: 0.00047939
Iteration 159/1000 | Loss: 0.00042394
Iteration 160/1000 | Loss: 0.00022250
Iteration 161/1000 | Loss: 0.00042369
Iteration 162/1000 | Loss: 0.00029451
Iteration 163/1000 | Loss: 0.00042305
Iteration 164/1000 | Loss: 0.00024203
Iteration 165/1000 | Loss: 0.00006370
Iteration 166/1000 | Loss: 0.00025594
Iteration 167/1000 | Loss: 0.00017383
Iteration 168/1000 | Loss: 0.00015024
Iteration 169/1000 | Loss: 0.00035295
Iteration 170/1000 | Loss: 0.00023365
Iteration 171/1000 | Loss: 0.00032801
Iteration 172/1000 | Loss: 0.00038587
Iteration 173/1000 | Loss: 0.00041802
Iteration 174/1000 | Loss: 0.00011461
Iteration 175/1000 | Loss: 0.00031878
Iteration 176/1000 | Loss: 0.00020044
Iteration 177/1000 | Loss: 0.00035014
Iteration 178/1000 | Loss: 0.00026668
Iteration 179/1000 | Loss: 0.00029352
Iteration 180/1000 | Loss: 0.00024114
Iteration 181/1000 | Loss: 0.00027212
Iteration 182/1000 | Loss: 0.00006016
Iteration 183/1000 | Loss: 0.00004508
Iteration 184/1000 | Loss: 0.00003756
Iteration 185/1000 | Loss: 0.00003381
Iteration 186/1000 | Loss: 0.00003230
Iteration 187/1000 | Loss: 0.00003057
Iteration 188/1000 | Loss: 0.00002911
Iteration 189/1000 | Loss: 0.00002805
Iteration 190/1000 | Loss: 0.00002748
Iteration 191/1000 | Loss: 0.00002687
Iteration 192/1000 | Loss: 0.00002637
Iteration 193/1000 | Loss: 0.00002588
Iteration 194/1000 | Loss: 0.00002549
Iteration 195/1000 | Loss: 0.00002520
Iteration 196/1000 | Loss: 0.00002500
Iteration 197/1000 | Loss: 0.00002469
Iteration 198/1000 | Loss: 0.00002449
Iteration 199/1000 | Loss: 0.00002438
Iteration 200/1000 | Loss: 0.00002432
Iteration 201/1000 | Loss: 0.00002432
Iteration 202/1000 | Loss: 0.00002431
Iteration 203/1000 | Loss: 0.00002430
Iteration 204/1000 | Loss: 0.00002430
Iteration 205/1000 | Loss: 0.00002421
Iteration 206/1000 | Loss: 0.00002418
Iteration 207/1000 | Loss: 0.00002414
Iteration 208/1000 | Loss: 0.00002413
Iteration 209/1000 | Loss: 0.00002411
Iteration 210/1000 | Loss: 0.00002410
Iteration 211/1000 | Loss: 0.00002410
Iteration 212/1000 | Loss: 0.00002409
Iteration 213/1000 | Loss: 0.00002408
Iteration 214/1000 | Loss: 0.00002408
Iteration 215/1000 | Loss: 0.00002408
Iteration 216/1000 | Loss: 0.00002407
Iteration 217/1000 | Loss: 0.00002407
Iteration 218/1000 | Loss: 0.00002406
Iteration 219/1000 | Loss: 0.00002406
Iteration 220/1000 | Loss: 0.00002405
Iteration 221/1000 | Loss: 0.00002405
Iteration 222/1000 | Loss: 0.00002405
Iteration 223/1000 | Loss: 0.00002404
Iteration 224/1000 | Loss: 0.00002404
Iteration 225/1000 | Loss: 0.00002403
Iteration 226/1000 | Loss: 0.00002403
Iteration 227/1000 | Loss: 0.00002402
Iteration 228/1000 | Loss: 0.00002402
Iteration 229/1000 | Loss: 0.00002401
Iteration 230/1000 | Loss: 0.00002401
Iteration 231/1000 | Loss: 0.00002400
Iteration 232/1000 | Loss: 0.00002400
Iteration 233/1000 | Loss: 0.00002400
Iteration 234/1000 | Loss: 0.00002399
Iteration 235/1000 | Loss: 0.00002399
Iteration 236/1000 | Loss: 0.00002398
Iteration 237/1000 | Loss: 0.00002398
Iteration 238/1000 | Loss: 0.00002397
Iteration 239/1000 | Loss: 0.00002397
Iteration 240/1000 | Loss: 0.00002397
Iteration 241/1000 | Loss: 0.00002396
Iteration 242/1000 | Loss: 0.00002396
Iteration 243/1000 | Loss: 0.00002395
Iteration 244/1000 | Loss: 0.00002394
Iteration 245/1000 | Loss: 0.00002394
Iteration 246/1000 | Loss: 0.00002393
Iteration 247/1000 | Loss: 0.00002393
Iteration 248/1000 | Loss: 0.00002391
Iteration 249/1000 | Loss: 0.00002390
Iteration 250/1000 | Loss: 0.00002390
Iteration 251/1000 | Loss: 0.00002390
Iteration 252/1000 | Loss: 0.00002389
Iteration 253/1000 | Loss: 0.00002389
Iteration 254/1000 | Loss: 0.00002388
Iteration 255/1000 | Loss: 0.00002388
Iteration 256/1000 | Loss: 0.00002387
Iteration 257/1000 | Loss: 0.00002387
Iteration 258/1000 | Loss: 0.00002387
Iteration 259/1000 | Loss: 0.00002387
Iteration 260/1000 | Loss: 0.00002386
Iteration 261/1000 | Loss: 0.00002386
Iteration 262/1000 | Loss: 0.00002386
Iteration 263/1000 | Loss: 0.00002386
Iteration 264/1000 | Loss: 0.00002386
Iteration 265/1000 | Loss: 0.00002385
Iteration 266/1000 | Loss: 0.00002385
Iteration 267/1000 | Loss: 0.00002385
Iteration 268/1000 | Loss: 0.00002385
Iteration 269/1000 | Loss: 0.00002385
Iteration 270/1000 | Loss: 0.00002384
Iteration 271/1000 | Loss: 0.00002384
Iteration 272/1000 | Loss: 0.00002384
Iteration 273/1000 | Loss: 0.00002384
Iteration 274/1000 | Loss: 0.00002384
Iteration 275/1000 | Loss: 0.00002383
Iteration 276/1000 | Loss: 0.00002383
Iteration 277/1000 | Loss: 0.00002383
Iteration 278/1000 | Loss: 0.00002383
Iteration 279/1000 | Loss: 0.00002382
Iteration 280/1000 | Loss: 0.00002382
Iteration 281/1000 | Loss: 0.00002382
Iteration 282/1000 | Loss: 0.00002382
Iteration 283/1000 | Loss: 0.00002381
Iteration 284/1000 | Loss: 0.00002381
Iteration 285/1000 | Loss: 0.00002381
Iteration 286/1000 | Loss: 0.00002381
Iteration 287/1000 | Loss: 0.00002380
Iteration 288/1000 | Loss: 0.00002380
Iteration 289/1000 | Loss: 0.00002380
Iteration 290/1000 | Loss: 0.00002380
Iteration 291/1000 | Loss: 0.00002380
Iteration 292/1000 | Loss: 0.00002380
Iteration 293/1000 | Loss: 0.00002380
Iteration 294/1000 | Loss: 0.00002380
Iteration 295/1000 | Loss: 0.00002380
Iteration 296/1000 | Loss: 0.00002380
Iteration 297/1000 | Loss: 0.00002380
Iteration 298/1000 | Loss: 0.00002380
Iteration 299/1000 | Loss: 0.00002380
Iteration 300/1000 | Loss: 0.00002380
Iteration 301/1000 | Loss: 0.00002380
Iteration 302/1000 | Loss: 0.00002380
Iteration 303/1000 | Loss: 0.00002380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [2.380142359470483e-05, 2.380142359470483e-05, 2.380142359470483e-05, 2.380142359470483e-05, 2.380142359470483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.380142359470483e-05

Optimization complete. Final v2v error: 4.017303466796875 mm

Highest mean error: 5.621119499206543 mm for frame 99

Lowest mean error: 3.03609037399292 mm for frame 144

Saving results

Total time: 345.86146998405457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616479
Iteration 2/25 | Loss: 0.00086708
Iteration 3/25 | Loss: 0.00075620
Iteration 4/25 | Loss: 0.00073952
Iteration 5/25 | Loss: 0.00073476
Iteration 6/25 | Loss: 0.00073344
Iteration 7/25 | Loss: 0.00073330
Iteration 8/25 | Loss: 0.00073330
Iteration 9/25 | Loss: 0.00073330
Iteration 10/25 | Loss: 0.00073330
Iteration 11/25 | Loss: 0.00073330
Iteration 12/25 | Loss: 0.00073330
Iteration 13/25 | Loss: 0.00073330
Iteration 14/25 | Loss: 0.00073330
Iteration 15/25 | Loss: 0.00073330
Iteration 16/25 | Loss: 0.00073330
Iteration 17/25 | Loss: 0.00073330
Iteration 18/25 | Loss: 0.00073330
Iteration 19/25 | Loss: 0.00073330
Iteration 20/25 | Loss: 0.00073330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007333031971938908, 0.0007333031971938908, 0.0007333031971938908, 0.0007333031971938908, 0.0007333031971938908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007333031971938908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.85278988
Iteration 2/25 | Loss: 0.00053337
Iteration 3/25 | Loss: 0.00053334
Iteration 4/25 | Loss: 0.00053334
Iteration 5/25 | Loss: 0.00053334
Iteration 6/25 | Loss: 0.00053334
Iteration 7/25 | Loss: 0.00053334
Iteration 8/25 | Loss: 0.00053334
Iteration 9/25 | Loss: 0.00053334
Iteration 10/25 | Loss: 0.00053334
Iteration 11/25 | Loss: 0.00053334
Iteration 12/25 | Loss: 0.00053334
Iteration 13/25 | Loss: 0.00053334
Iteration 14/25 | Loss: 0.00053334
Iteration 15/25 | Loss: 0.00053334
Iteration 16/25 | Loss: 0.00053334
Iteration 17/25 | Loss: 0.00053334
Iteration 18/25 | Loss: 0.00053334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005333372973836958, 0.0005333372973836958, 0.0005333372973836958, 0.0005333372973836958, 0.0005333372973836958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005333372973836958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053334
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00001638
Iteration 4/1000 | Loss: 0.00001471
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001319
Iteration 7/1000 | Loss: 0.00001287
Iteration 8/1000 | Loss: 0.00001271
Iteration 9/1000 | Loss: 0.00001251
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001218
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001217
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001210
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001209
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001208
Iteration 34/1000 | Loss: 0.00001208
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001208
Iteration 37/1000 | Loss: 0.00001207
Iteration 38/1000 | Loss: 0.00001207
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001205
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001201
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001199
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001188
Iteration 94/1000 | Loss: 0.00001188
Iteration 95/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.1880981219292153e-05, 1.1880981219292153e-05, 1.1880981219292153e-05, 1.1880981219292153e-05, 1.1880981219292153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1880981219292153e-05

Optimization complete. Final v2v error: 2.936357021331787 mm

Highest mean error: 3.251899242401123 mm for frame 117

Lowest mean error: 2.722198486328125 mm for frame 22

Saving results

Total time: 37.76803803443909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910219
Iteration 2/25 | Loss: 0.00134242
Iteration 3/25 | Loss: 0.00094538
Iteration 4/25 | Loss: 0.00089342
Iteration 5/25 | Loss: 0.00088421
Iteration 6/25 | Loss: 0.00088240
Iteration 7/25 | Loss: 0.00088226
Iteration 8/25 | Loss: 0.00088226
Iteration 9/25 | Loss: 0.00088226
Iteration 10/25 | Loss: 0.00088226
Iteration 11/25 | Loss: 0.00088226
Iteration 12/25 | Loss: 0.00088226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000882263237144798, 0.000882263237144798, 0.000882263237144798, 0.000882263237144798, 0.000882263237144798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000882263237144798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03651106
Iteration 2/25 | Loss: 0.00043874
Iteration 3/25 | Loss: 0.00043874
Iteration 4/25 | Loss: 0.00043874
Iteration 5/25 | Loss: 0.00043874
Iteration 6/25 | Loss: 0.00043874
Iteration 7/25 | Loss: 0.00043874
Iteration 8/25 | Loss: 0.00043874
Iteration 9/25 | Loss: 0.00043874
Iteration 10/25 | Loss: 0.00043874
Iteration 11/25 | Loss: 0.00043874
Iteration 12/25 | Loss: 0.00043874
Iteration 13/25 | Loss: 0.00043874
Iteration 14/25 | Loss: 0.00043874
Iteration 15/25 | Loss: 0.00043874
Iteration 16/25 | Loss: 0.00043874
Iteration 17/25 | Loss: 0.00043874
Iteration 18/25 | Loss: 0.00043874
Iteration 19/25 | Loss: 0.00043874
Iteration 20/25 | Loss: 0.00043874
Iteration 21/25 | Loss: 0.00043874
Iteration 22/25 | Loss: 0.00043874
Iteration 23/25 | Loss: 0.00043874
Iteration 24/25 | Loss: 0.00043874
Iteration 25/25 | Loss: 0.00043874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043874
Iteration 2/1000 | Loss: 0.00004081
Iteration 3/1000 | Loss: 0.00003377
Iteration 4/1000 | Loss: 0.00003059
Iteration 5/1000 | Loss: 0.00002860
Iteration 6/1000 | Loss: 0.00002755
Iteration 7/1000 | Loss: 0.00002697
Iteration 8/1000 | Loss: 0.00002648
Iteration 9/1000 | Loss: 0.00002612
Iteration 10/1000 | Loss: 0.00002587
Iteration 11/1000 | Loss: 0.00002587
Iteration 12/1000 | Loss: 0.00002568
Iteration 13/1000 | Loss: 0.00002563
Iteration 14/1000 | Loss: 0.00002563
Iteration 15/1000 | Loss: 0.00002562
Iteration 16/1000 | Loss: 0.00002562
Iteration 17/1000 | Loss: 0.00002561
Iteration 18/1000 | Loss: 0.00002561
Iteration 19/1000 | Loss: 0.00002560
Iteration 20/1000 | Loss: 0.00002554
Iteration 21/1000 | Loss: 0.00002554
Iteration 22/1000 | Loss: 0.00002554
Iteration 23/1000 | Loss: 0.00002554
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00002554
Iteration 26/1000 | Loss: 0.00002554
Iteration 27/1000 | Loss: 0.00002554
Iteration 28/1000 | Loss: 0.00002553
Iteration 29/1000 | Loss: 0.00002553
Iteration 30/1000 | Loss: 0.00002553
Iteration 31/1000 | Loss: 0.00002553
Iteration 32/1000 | Loss: 0.00002553
Iteration 33/1000 | Loss: 0.00002553
Iteration 34/1000 | Loss: 0.00002553
Iteration 35/1000 | Loss: 0.00002553
Iteration 36/1000 | Loss: 0.00002552
Iteration 37/1000 | Loss: 0.00002552
Iteration 38/1000 | Loss: 0.00002552
Iteration 39/1000 | Loss: 0.00002552
Iteration 40/1000 | Loss: 0.00002550
Iteration 41/1000 | Loss: 0.00002550
Iteration 42/1000 | Loss: 0.00002550
Iteration 43/1000 | Loss: 0.00002550
Iteration 44/1000 | Loss: 0.00002550
Iteration 45/1000 | Loss: 0.00002550
Iteration 46/1000 | Loss: 0.00002549
Iteration 47/1000 | Loss: 0.00002549
Iteration 48/1000 | Loss: 0.00002549
Iteration 49/1000 | Loss: 0.00002549
Iteration 50/1000 | Loss: 0.00002547
Iteration 51/1000 | Loss: 0.00002547
Iteration 52/1000 | Loss: 0.00002547
Iteration 53/1000 | Loss: 0.00002547
Iteration 54/1000 | Loss: 0.00002547
Iteration 55/1000 | Loss: 0.00002547
Iteration 56/1000 | Loss: 0.00002547
Iteration 57/1000 | Loss: 0.00002547
Iteration 58/1000 | Loss: 0.00002547
Iteration 59/1000 | Loss: 0.00002547
Iteration 60/1000 | Loss: 0.00002546
Iteration 61/1000 | Loss: 0.00002546
Iteration 62/1000 | Loss: 0.00002546
Iteration 63/1000 | Loss: 0.00002546
Iteration 64/1000 | Loss: 0.00002546
Iteration 65/1000 | Loss: 0.00002546
Iteration 66/1000 | Loss: 0.00002545
Iteration 67/1000 | Loss: 0.00002545
Iteration 68/1000 | Loss: 0.00002545
Iteration 69/1000 | Loss: 0.00002545
Iteration 70/1000 | Loss: 0.00002545
Iteration 71/1000 | Loss: 0.00002545
Iteration 72/1000 | Loss: 0.00002545
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002545
Iteration 75/1000 | Loss: 0.00002545
Iteration 76/1000 | Loss: 0.00002545
Iteration 77/1000 | Loss: 0.00002545
Iteration 78/1000 | Loss: 0.00002545
Iteration 79/1000 | Loss: 0.00002545
Iteration 80/1000 | Loss: 0.00002545
Iteration 81/1000 | Loss: 0.00002544
Iteration 82/1000 | Loss: 0.00002544
Iteration 83/1000 | Loss: 0.00002544
Iteration 84/1000 | Loss: 0.00002544
Iteration 85/1000 | Loss: 0.00002544
Iteration 86/1000 | Loss: 0.00002544
Iteration 87/1000 | Loss: 0.00002544
Iteration 88/1000 | Loss: 0.00002544
Iteration 89/1000 | Loss: 0.00002544
Iteration 90/1000 | Loss: 0.00002544
Iteration 91/1000 | Loss: 0.00002543
Iteration 92/1000 | Loss: 0.00002543
Iteration 93/1000 | Loss: 0.00002543
Iteration 94/1000 | Loss: 0.00002543
Iteration 95/1000 | Loss: 0.00002543
Iteration 96/1000 | Loss: 0.00002543
Iteration 97/1000 | Loss: 0.00002543
Iteration 98/1000 | Loss: 0.00002543
Iteration 99/1000 | Loss: 0.00002543
Iteration 100/1000 | Loss: 0.00002543
Iteration 101/1000 | Loss: 0.00002543
Iteration 102/1000 | Loss: 0.00002543
Iteration 103/1000 | Loss: 0.00002543
Iteration 104/1000 | Loss: 0.00002543
Iteration 105/1000 | Loss: 0.00002543
Iteration 106/1000 | Loss: 0.00002543
Iteration 107/1000 | Loss: 0.00002543
Iteration 108/1000 | Loss: 0.00002542
Iteration 109/1000 | Loss: 0.00002542
Iteration 110/1000 | Loss: 0.00002542
Iteration 111/1000 | Loss: 0.00002542
Iteration 112/1000 | Loss: 0.00002542
Iteration 113/1000 | Loss: 0.00002542
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002542
Iteration 116/1000 | Loss: 0.00002542
Iteration 117/1000 | Loss: 0.00002542
Iteration 118/1000 | Loss: 0.00002542
Iteration 119/1000 | Loss: 0.00002542
Iteration 120/1000 | Loss: 0.00002542
Iteration 121/1000 | Loss: 0.00002542
Iteration 122/1000 | Loss: 0.00002542
Iteration 123/1000 | Loss: 0.00002542
Iteration 124/1000 | Loss: 0.00002542
Iteration 125/1000 | Loss: 0.00002542
Iteration 126/1000 | Loss: 0.00002542
Iteration 127/1000 | Loss: 0.00002542
Iteration 128/1000 | Loss: 0.00002542
Iteration 129/1000 | Loss: 0.00002542
Iteration 130/1000 | Loss: 0.00002542
Iteration 131/1000 | Loss: 0.00002542
Iteration 132/1000 | Loss: 0.00002542
Iteration 133/1000 | Loss: 0.00002542
Iteration 134/1000 | Loss: 0.00002541
Iteration 135/1000 | Loss: 0.00002541
Iteration 136/1000 | Loss: 0.00002541
Iteration 137/1000 | Loss: 0.00002541
Iteration 138/1000 | Loss: 0.00002541
Iteration 139/1000 | Loss: 0.00002542
Iteration 140/1000 | Loss: 0.00002542
Iteration 141/1000 | Loss: 0.00002541
Iteration 142/1000 | Loss: 0.00002542
Iteration 143/1000 | Loss: 0.00002542
Iteration 144/1000 | Loss: 0.00002541
Iteration 145/1000 | Loss: 0.00002542
Iteration 146/1000 | Loss: 0.00002541
Iteration 147/1000 | Loss: 0.00002542
Iteration 148/1000 | Loss: 0.00002542
Iteration 149/1000 | Loss: 0.00002541
Iteration 150/1000 | Loss: 0.00002542
Iteration 151/1000 | Loss: 0.00002541
Iteration 152/1000 | Loss: 0.00002541
Iteration 153/1000 | Loss: 0.00002541
Iteration 154/1000 | Loss: 0.00002541
Iteration 155/1000 | Loss: 0.00002541
Iteration 156/1000 | Loss: 0.00002541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.5414999981876463e-05, 2.5414999981876463e-05, 2.5414999981876463e-05, 2.5414999981876463e-05, 2.5414999981876463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5414999981876463e-05

Optimization complete. Final v2v error: 4.255906105041504 mm

Highest mean error: 4.751490592956543 mm for frame 28

Lowest mean error: 3.9114975929260254 mm for frame 136

Saving results

Total time: 34.44254922866821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01227044
Iteration 2/25 | Loss: 0.00323682
Iteration 3/25 | Loss: 0.00180456
Iteration 4/25 | Loss: 0.00162687
Iteration 5/25 | Loss: 0.00189496
Iteration 6/25 | Loss: 0.00171635
Iteration 7/25 | Loss: 0.00147096
Iteration 8/25 | Loss: 0.00147063
Iteration 9/25 | Loss: 0.00150075
Iteration 10/25 | Loss: 0.00143013
Iteration 11/25 | Loss: 0.00140971
Iteration 12/25 | Loss: 0.00139904
Iteration 13/25 | Loss: 0.00134111
Iteration 14/25 | Loss: 0.00136715
Iteration 15/25 | Loss: 0.00131602
Iteration 16/25 | Loss: 0.00131641
Iteration 17/25 | Loss: 0.00130854
Iteration 18/25 | Loss: 0.00130495
Iteration 19/25 | Loss: 0.00130451
Iteration 20/25 | Loss: 0.00130351
Iteration 21/25 | Loss: 0.00130317
Iteration 22/25 | Loss: 0.00130452
Iteration 23/25 | Loss: 0.00130302
Iteration 24/25 | Loss: 0.00130283
Iteration 25/25 | Loss: 0.00130260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78260761
Iteration 2/25 | Loss: 0.00268173
Iteration 3/25 | Loss: 0.00268173
Iteration 4/25 | Loss: 0.00268173
Iteration 5/25 | Loss: 0.00268173
Iteration 6/25 | Loss: 0.00268173
Iteration 7/25 | Loss: 0.00268172
Iteration 8/25 | Loss: 0.00268172
Iteration 9/25 | Loss: 0.00268172
Iteration 10/25 | Loss: 0.00268172
Iteration 11/25 | Loss: 0.00268172
Iteration 12/25 | Loss: 0.00268172
Iteration 13/25 | Loss: 0.00268172
Iteration 14/25 | Loss: 0.00268172
Iteration 15/25 | Loss: 0.00268172
Iteration 16/25 | Loss: 0.00268172
Iteration 17/25 | Loss: 0.00268172
Iteration 18/25 | Loss: 0.00268172
Iteration 19/25 | Loss: 0.00268172
Iteration 20/25 | Loss: 0.00268172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00268172356300056, 0.00268172356300056, 0.00268172356300056, 0.00268172356300056, 0.00268172356300056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00268172356300056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268172
Iteration 2/1000 | Loss: 0.00096492
Iteration 3/1000 | Loss: 0.00030548
Iteration 4/1000 | Loss: 0.00026147
Iteration 5/1000 | Loss: 0.00023276
Iteration 6/1000 | Loss: 0.00021508
Iteration 7/1000 | Loss: 0.00118923
Iteration 8/1000 | Loss: 0.00178215
Iteration 9/1000 | Loss: 0.00110816
Iteration 10/1000 | Loss: 0.00393092
Iteration 11/1000 | Loss: 0.00330710
Iteration 12/1000 | Loss: 0.00352652
Iteration 13/1000 | Loss: 0.00393922
Iteration 14/1000 | Loss: 0.00078171
Iteration 15/1000 | Loss: 0.00275268
Iteration 16/1000 | Loss: 0.00254143
Iteration 17/1000 | Loss: 0.00240996
Iteration 18/1000 | Loss: 0.00106690
Iteration 19/1000 | Loss: 0.00141135
Iteration 20/1000 | Loss: 0.00163070
Iteration 21/1000 | Loss: 0.00383691
Iteration 22/1000 | Loss: 0.00142113
Iteration 23/1000 | Loss: 0.00174071
Iteration 24/1000 | Loss: 0.00132470
Iteration 25/1000 | Loss: 0.00118116
Iteration 26/1000 | Loss: 0.00156673
Iteration 27/1000 | Loss: 0.00298319
Iteration 28/1000 | Loss: 0.00120980
Iteration 29/1000 | Loss: 0.00190207
Iteration 30/1000 | Loss: 0.00173226
Iteration 31/1000 | Loss: 0.00132518
Iteration 32/1000 | Loss: 0.00185440
Iteration 33/1000 | Loss: 0.00210226
Iteration 34/1000 | Loss: 0.00186102
Iteration 35/1000 | Loss: 0.00140281
Iteration 36/1000 | Loss: 0.00106789
Iteration 37/1000 | Loss: 0.00194788
Iteration 38/1000 | Loss: 0.00116359
Iteration 39/1000 | Loss: 0.00201037
Iteration 40/1000 | Loss: 0.00171026
Iteration 41/1000 | Loss: 0.00157928
Iteration 42/1000 | Loss: 0.00159619
Iteration 43/1000 | Loss: 0.00153674
Iteration 44/1000 | Loss: 0.00077963
Iteration 45/1000 | Loss: 0.00062714
Iteration 46/1000 | Loss: 0.00095218
Iteration 47/1000 | Loss: 0.00110700
Iteration 48/1000 | Loss: 0.00153991
Iteration 49/1000 | Loss: 0.00175065
Iteration 50/1000 | Loss: 0.00196768
Iteration 51/1000 | Loss: 0.00189046
Iteration 52/1000 | Loss: 0.00119454
Iteration 53/1000 | Loss: 0.00265649
Iteration 54/1000 | Loss: 0.00179832
Iteration 55/1000 | Loss: 0.00051193
Iteration 56/1000 | Loss: 0.00070101
Iteration 57/1000 | Loss: 0.00103835
Iteration 58/1000 | Loss: 0.00107901
Iteration 59/1000 | Loss: 0.00127618
Iteration 60/1000 | Loss: 0.00156256
Iteration 61/1000 | Loss: 0.00197667
Iteration 62/1000 | Loss: 0.00074360
Iteration 63/1000 | Loss: 0.00155469
Iteration 64/1000 | Loss: 0.00133903
Iteration 65/1000 | Loss: 0.00089823
Iteration 66/1000 | Loss: 0.00117883
Iteration 67/1000 | Loss: 0.00083506
Iteration 68/1000 | Loss: 0.00030739
Iteration 69/1000 | Loss: 0.00023735
Iteration 70/1000 | Loss: 0.00112735
Iteration 71/1000 | Loss: 0.00110818
Iteration 72/1000 | Loss: 0.00184638
Iteration 73/1000 | Loss: 0.00118812
Iteration 74/1000 | Loss: 0.00208255
Iteration 75/1000 | Loss: 0.00139999
Iteration 76/1000 | Loss: 0.00143097
Iteration 77/1000 | Loss: 0.00208241
Iteration 78/1000 | Loss: 0.00213546
Iteration 79/1000 | Loss: 0.00262065
Iteration 80/1000 | Loss: 0.00232716
Iteration 81/1000 | Loss: 0.00430429
Iteration 82/1000 | Loss: 0.00191616
Iteration 83/1000 | Loss: 0.00268104
Iteration 84/1000 | Loss: 0.00116015
Iteration 85/1000 | Loss: 0.00174243
Iteration 86/1000 | Loss: 0.00072888
Iteration 87/1000 | Loss: 0.00053608
Iteration 88/1000 | Loss: 0.00060560
Iteration 89/1000 | Loss: 0.00156856
Iteration 90/1000 | Loss: 0.00060273
Iteration 91/1000 | Loss: 0.00028831
Iteration 92/1000 | Loss: 0.00120970
Iteration 93/1000 | Loss: 0.00141604
Iteration 94/1000 | Loss: 0.00270280
Iteration 95/1000 | Loss: 0.00208262
Iteration 96/1000 | Loss: 0.00088774
Iteration 97/1000 | Loss: 0.00029972
Iteration 98/1000 | Loss: 0.00100939
Iteration 99/1000 | Loss: 0.00074055
Iteration 100/1000 | Loss: 0.00067573
Iteration 101/1000 | Loss: 0.00077210
Iteration 102/1000 | Loss: 0.00073670
Iteration 103/1000 | Loss: 0.00093981
Iteration 104/1000 | Loss: 0.00120492
Iteration 105/1000 | Loss: 0.00084940
Iteration 106/1000 | Loss: 0.00013983
Iteration 107/1000 | Loss: 0.00042301
Iteration 108/1000 | Loss: 0.00145428
Iteration 109/1000 | Loss: 0.00135404
Iteration 110/1000 | Loss: 0.00071513
Iteration 111/1000 | Loss: 0.00040165
Iteration 112/1000 | Loss: 0.00075142
Iteration 113/1000 | Loss: 0.00041919
Iteration 114/1000 | Loss: 0.00085260
Iteration 115/1000 | Loss: 0.00126870
Iteration 116/1000 | Loss: 0.00060954
Iteration 117/1000 | Loss: 0.00049948
Iteration 118/1000 | Loss: 0.00044454
Iteration 119/1000 | Loss: 0.00060243
Iteration 120/1000 | Loss: 0.00041025
Iteration 121/1000 | Loss: 0.00099490
Iteration 122/1000 | Loss: 0.00155844
Iteration 123/1000 | Loss: 0.00055996
Iteration 124/1000 | Loss: 0.00040134
Iteration 125/1000 | Loss: 0.00016405
Iteration 126/1000 | Loss: 0.00014072
Iteration 127/1000 | Loss: 0.00129626
Iteration 128/1000 | Loss: 0.00058011
Iteration 129/1000 | Loss: 0.00012428
Iteration 130/1000 | Loss: 0.00042512
Iteration 131/1000 | Loss: 0.00124637
Iteration 132/1000 | Loss: 0.00036529
Iteration 133/1000 | Loss: 0.00113019
Iteration 134/1000 | Loss: 0.00132472
Iteration 135/1000 | Loss: 0.00022354
Iteration 136/1000 | Loss: 0.00058445
Iteration 137/1000 | Loss: 0.00012939
Iteration 138/1000 | Loss: 0.00011704
Iteration 139/1000 | Loss: 0.00011446
Iteration 140/1000 | Loss: 0.00032859
Iteration 141/1000 | Loss: 0.00025406
Iteration 142/1000 | Loss: 0.00024223
Iteration 143/1000 | Loss: 0.00012347
Iteration 144/1000 | Loss: 0.00011731
Iteration 145/1000 | Loss: 0.00011221
Iteration 146/1000 | Loss: 0.00094697
Iteration 147/1000 | Loss: 0.00021179
Iteration 148/1000 | Loss: 0.00011874
Iteration 149/1000 | Loss: 0.00203930
Iteration 150/1000 | Loss: 0.00346983
Iteration 151/1000 | Loss: 0.00060357
Iteration 152/1000 | Loss: 0.00060373
Iteration 153/1000 | Loss: 0.00064299
Iteration 154/1000 | Loss: 0.00061325
Iteration 155/1000 | Loss: 0.00012524
Iteration 156/1000 | Loss: 0.00011430
Iteration 157/1000 | Loss: 0.00133232
Iteration 158/1000 | Loss: 0.00071738
Iteration 159/1000 | Loss: 0.00063992
Iteration 160/1000 | Loss: 0.00109958
Iteration 161/1000 | Loss: 0.00069814
Iteration 162/1000 | Loss: 0.00194224
Iteration 163/1000 | Loss: 0.00076023
Iteration 164/1000 | Loss: 0.00122636
Iteration 165/1000 | Loss: 0.00088263
Iteration 166/1000 | Loss: 0.00056028
Iteration 167/1000 | Loss: 0.00014647
Iteration 168/1000 | Loss: 0.00153881
Iteration 169/1000 | Loss: 0.00021979
Iteration 170/1000 | Loss: 0.00133155
Iteration 171/1000 | Loss: 0.00031968
Iteration 172/1000 | Loss: 0.00074014
Iteration 173/1000 | Loss: 0.00021364
Iteration 174/1000 | Loss: 0.00011026
Iteration 175/1000 | Loss: 0.00010238
Iteration 176/1000 | Loss: 0.00009501
Iteration 177/1000 | Loss: 0.00009129
Iteration 178/1000 | Loss: 0.00119337
Iteration 179/1000 | Loss: 0.00047441
Iteration 180/1000 | Loss: 0.00062506
Iteration 181/1000 | Loss: 0.00010443
Iteration 182/1000 | Loss: 0.00071019
Iteration 183/1000 | Loss: 0.00010573
Iteration 184/1000 | Loss: 0.00008722
Iteration 185/1000 | Loss: 0.00008336
Iteration 186/1000 | Loss: 0.00072139
Iteration 187/1000 | Loss: 0.00031396
Iteration 188/1000 | Loss: 0.00008360
Iteration 189/1000 | Loss: 0.00008069
Iteration 190/1000 | Loss: 0.00007938
Iteration 191/1000 | Loss: 0.00007839
Iteration 192/1000 | Loss: 0.00089891
Iteration 193/1000 | Loss: 0.00025154
Iteration 194/1000 | Loss: 0.00007792
Iteration 195/1000 | Loss: 0.00007652
Iteration 196/1000 | Loss: 0.00089889
Iteration 197/1000 | Loss: 0.00023047
Iteration 198/1000 | Loss: 0.00007620
Iteration 199/1000 | Loss: 0.00007512
Iteration 200/1000 | Loss: 0.00086886
Iteration 201/1000 | Loss: 0.00009464
Iteration 202/1000 | Loss: 0.00007760
Iteration 203/1000 | Loss: 0.00007455
Iteration 204/1000 | Loss: 0.00007356
Iteration 205/1000 | Loss: 0.00007301
Iteration 206/1000 | Loss: 0.00007274
Iteration 207/1000 | Loss: 0.00007255
Iteration 208/1000 | Loss: 0.00007235
Iteration 209/1000 | Loss: 0.00007221
Iteration 210/1000 | Loss: 0.00007220
Iteration 211/1000 | Loss: 0.00007200
Iteration 212/1000 | Loss: 0.00007175
Iteration 213/1000 | Loss: 0.00007125
Iteration 214/1000 | Loss: 0.00007081
Iteration 215/1000 | Loss: 0.00007049
Iteration 216/1000 | Loss: 0.00007031
Iteration 217/1000 | Loss: 0.00007020
Iteration 218/1000 | Loss: 0.00007018
Iteration 219/1000 | Loss: 0.00007016
Iteration 220/1000 | Loss: 0.00007015
Iteration 221/1000 | Loss: 0.00007014
Iteration 222/1000 | Loss: 0.00007014
Iteration 223/1000 | Loss: 0.00007013
Iteration 224/1000 | Loss: 0.00007012
Iteration 225/1000 | Loss: 0.00007012
Iteration 226/1000 | Loss: 0.00007012
Iteration 227/1000 | Loss: 0.00007012
Iteration 228/1000 | Loss: 0.00007011
Iteration 229/1000 | Loss: 0.00007011
Iteration 230/1000 | Loss: 0.00007011
Iteration 231/1000 | Loss: 0.00007011
Iteration 232/1000 | Loss: 0.00007011
Iteration 233/1000 | Loss: 0.00007011
Iteration 234/1000 | Loss: 0.00007011
Iteration 235/1000 | Loss: 0.00007011
Iteration 236/1000 | Loss: 0.00007011
Iteration 237/1000 | Loss: 0.00007011
Iteration 238/1000 | Loss: 0.00007011
Iteration 239/1000 | Loss: 0.00007010
Iteration 240/1000 | Loss: 0.00007010
Iteration 241/1000 | Loss: 0.00007010
Iteration 242/1000 | Loss: 0.00007010
Iteration 243/1000 | Loss: 0.00007010
Iteration 244/1000 | Loss: 0.00007010
Iteration 245/1000 | Loss: 0.00007010
Iteration 246/1000 | Loss: 0.00007009
Iteration 247/1000 | Loss: 0.00007009
Iteration 248/1000 | Loss: 0.00007009
Iteration 249/1000 | Loss: 0.00007008
Iteration 250/1000 | Loss: 0.00007008
Iteration 251/1000 | Loss: 0.00007008
Iteration 252/1000 | Loss: 0.00007008
Iteration 253/1000 | Loss: 0.00007007
Iteration 254/1000 | Loss: 0.00007007
Iteration 255/1000 | Loss: 0.00007007
Iteration 256/1000 | Loss: 0.00007007
Iteration 257/1000 | Loss: 0.00007007
Iteration 258/1000 | Loss: 0.00007007
Iteration 259/1000 | Loss: 0.00007007
Iteration 260/1000 | Loss: 0.00007006
Iteration 261/1000 | Loss: 0.00007006
Iteration 262/1000 | Loss: 0.00007006
Iteration 263/1000 | Loss: 0.00007006
Iteration 264/1000 | Loss: 0.00007005
Iteration 265/1000 | Loss: 0.00007005
Iteration 266/1000 | Loss: 0.00007005
Iteration 267/1000 | Loss: 0.00007004
Iteration 268/1000 | Loss: 0.00007004
Iteration 269/1000 | Loss: 0.00007004
Iteration 270/1000 | Loss: 0.00007004
Iteration 271/1000 | Loss: 0.00007004
Iteration 272/1000 | Loss: 0.00007004
Iteration 273/1000 | Loss: 0.00007004
Iteration 274/1000 | Loss: 0.00007004
Iteration 275/1000 | Loss: 0.00007004
Iteration 276/1000 | Loss: 0.00007004
Iteration 277/1000 | Loss: 0.00007004
Iteration 278/1000 | Loss: 0.00007004
Iteration 279/1000 | Loss: 0.00007003
Iteration 280/1000 | Loss: 0.00007003
Iteration 281/1000 | Loss: 0.00007003
Iteration 282/1000 | Loss: 0.00007003
Iteration 283/1000 | Loss: 0.00007003
Iteration 284/1000 | Loss: 0.00007003
Iteration 285/1000 | Loss: 0.00007003
Iteration 286/1000 | Loss: 0.00007003
Iteration 287/1000 | Loss: 0.00007002
Iteration 288/1000 | Loss: 0.00007002
Iteration 289/1000 | Loss: 0.00007002
Iteration 290/1000 | Loss: 0.00007002
Iteration 291/1000 | Loss: 0.00007002
Iteration 292/1000 | Loss: 0.00007002
Iteration 293/1000 | Loss: 0.00007002
Iteration 294/1000 | Loss: 0.00007002
Iteration 295/1000 | Loss: 0.00007002
Iteration 296/1000 | Loss: 0.00007002
Iteration 297/1000 | Loss: 0.00007002
Iteration 298/1000 | Loss: 0.00007002
Iteration 299/1000 | Loss: 0.00007002
Iteration 300/1000 | Loss: 0.00007001
Iteration 301/1000 | Loss: 0.00007001
Iteration 302/1000 | Loss: 0.00007001
Iteration 303/1000 | Loss: 0.00007001
Iteration 304/1000 | Loss: 0.00007001
Iteration 305/1000 | Loss: 0.00007001
Iteration 306/1000 | Loss: 0.00007001
Iteration 307/1000 | Loss: 0.00007001
Iteration 308/1000 | Loss: 0.00007001
Iteration 309/1000 | Loss: 0.00007001
Iteration 310/1000 | Loss: 0.00007001
Iteration 311/1000 | Loss: 0.00007001
Iteration 312/1000 | Loss: 0.00007001
Iteration 313/1000 | Loss: 0.00007001
Iteration 314/1000 | Loss: 0.00007001
Iteration 315/1000 | Loss: 0.00007001
Iteration 316/1000 | Loss: 0.00007001
Iteration 317/1000 | Loss: 0.00007001
Iteration 318/1000 | Loss: 0.00007001
Iteration 319/1000 | Loss: 0.00007001
Iteration 320/1000 | Loss: 0.00007001
Iteration 321/1000 | Loss: 0.00007001
Iteration 322/1000 | Loss: 0.00007001
Iteration 323/1000 | Loss: 0.00007001
Iteration 324/1000 | Loss: 0.00007001
Iteration 325/1000 | Loss: 0.00007001
Iteration 326/1000 | Loss: 0.00007001
Iteration 327/1000 | Loss: 0.00007001
Iteration 328/1000 | Loss: 0.00007001
Iteration 329/1000 | Loss: 0.00007001
Iteration 330/1000 | Loss: 0.00007001
Iteration 331/1000 | Loss: 0.00007001
Iteration 332/1000 | Loss: 0.00007001
Iteration 333/1000 | Loss: 0.00007001
Iteration 334/1000 | Loss: 0.00007001
Iteration 335/1000 | Loss: 0.00007001
Iteration 336/1000 | Loss: 0.00007001
Iteration 337/1000 | Loss: 0.00007001
Iteration 338/1000 | Loss: 0.00007001
Iteration 339/1000 | Loss: 0.00007001
Iteration 340/1000 | Loss: 0.00007001
Iteration 341/1000 | Loss: 0.00007001
Iteration 342/1000 | Loss: 0.00007001
Iteration 343/1000 | Loss: 0.00007001
Iteration 344/1000 | Loss: 0.00007001
Iteration 345/1000 | Loss: 0.00007001
Iteration 346/1000 | Loss: 0.00007001
Iteration 347/1000 | Loss: 0.00007001
Iteration 348/1000 | Loss: 0.00007001
Iteration 349/1000 | Loss: 0.00007001
Iteration 350/1000 | Loss: 0.00007001
Iteration 351/1000 | Loss: 0.00007001
Iteration 352/1000 | Loss: 0.00007001
Iteration 353/1000 | Loss: 0.00007001
Iteration 354/1000 | Loss: 0.00007001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 354. Stopping optimization.
Last 5 losses: [7.000512414379045e-05, 7.000512414379045e-05, 7.000512414379045e-05, 7.000512414379045e-05, 7.000512414379045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.000512414379045e-05

Optimization complete. Final v2v error: 5.628952503204346 mm

Highest mean error: 12.798787117004395 mm for frame 13

Lowest mean error: 4.650121688842773 mm for frame 134

Saving results

Total time: 362.14297008514404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093007
Iteration 2/25 | Loss: 0.00195871
Iteration 3/25 | Loss: 0.00090705
Iteration 4/25 | Loss: 0.00083485
Iteration 5/25 | Loss: 0.00080715
Iteration 6/25 | Loss: 0.00079318
Iteration 7/25 | Loss: 0.00078738
Iteration 8/25 | Loss: 0.00078031
Iteration 9/25 | Loss: 0.00077022
Iteration 10/25 | Loss: 0.00076373
Iteration 11/25 | Loss: 0.00076485
Iteration 12/25 | Loss: 0.00076415
Iteration 13/25 | Loss: 0.00076000
Iteration 14/25 | Loss: 0.00075977
Iteration 15/25 | Loss: 0.00075976
Iteration 16/25 | Loss: 0.00075976
Iteration 17/25 | Loss: 0.00075976
Iteration 18/25 | Loss: 0.00075976
Iteration 19/25 | Loss: 0.00075976
Iteration 20/25 | Loss: 0.00075976
Iteration 21/25 | Loss: 0.00075975
Iteration 22/25 | Loss: 0.00075973
Iteration 23/25 | Loss: 0.00075973
Iteration 24/25 | Loss: 0.00075973
Iteration 25/25 | Loss: 0.00075973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42606568
Iteration 2/25 | Loss: 0.00055898
Iteration 3/25 | Loss: 0.00051397
Iteration 4/25 | Loss: 0.00051397
Iteration 5/25 | Loss: 0.00051397
Iteration 6/25 | Loss: 0.00051397
Iteration 7/25 | Loss: 0.00051397
Iteration 8/25 | Loss: 0.00051397
Iteration 9/25 | Loss: 0.00051397
Iteration 10/25 | Loss: 0.00051397
Iteration 11/25 | Loss: 0.00051397
Iteration 12/25 | Loss: 0.00051397
Iteration 13/25 | Loss: 0.00051397
Iteration 14/25 | Loss: 0.00051397
Iteration 15/25 | Loss: 0.00051397
Iteration 16/25 | Loss: 0.00051397
Iteration 17/25 | Loss: 0.00051397
Iteration 18/25 | Loss: 0.00051397
Iteration 19/25 | Loss: 0.00051397
Iteration 20/25 | Loss: 0.00051397
Iteration 21/25 | Loss: 0.00051397
Iteration 22/25 | Loss: 0.00051397
Iteration 23/25 | Loss: 0.00051397
Iteration 24/25 | Loss: 0.00051397
Iteration 25/25 | Loss: 0.00051397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051397
Iteration 2/1000 | Loss: 0.00016902
Iteration 3/1000 | Loss: 0.00009453
Iteration 4/1000 | Loss: 0.00002840
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002420
Iteration 7/1000 | Loss: 0.00002283
Iteration 8/1000 | Loss: 0.00008432
Iteration 9/1000 | Loss: 0.00003420
Iteration 10/1000 | Loss: 0.00002179
Iteration 11/1000 | Loss: 0.00003184
Iteration 12/1000 | Loss: 0.00002135
Iteration 13/1000 | Loss: 0.00069076
Iteration 14/1000 | Loss: 0.00085661
Iteration 15/1000 | Loss: 0.00139262
Iteration 16/1000 | Loss: 0.00005692
Iteration 17/1000 | Loss: 0.00006101
Iteration 18/1000 | Loss: 0.00002789
Iteration 19/1000 | Loss: 0.00002364
Iteration 20/1000 | Loss: 0.00002278
Iteration 21/1000 | Loss: 0.00002220
Iteration 22/1000 | Loss: 0.00002185
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002161
Iteration 25/1000 | Loss: 0.00002145
Iteration 26/1000 | Loss: 0.00002144
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002142
Iteration 29/1000 | Loss: 0.00002141
Iteration 30/1000 | Loss: 0.00002141
Iteration 31/1000 | Loss: 0.00002140
Iteration 32/1000 | Loss: 0.00002140
Iteration 33/1000 | Loss: 0.00002140
Iteration 34/1000 | Loss: 0.00030742
Iteration 35/1000 | Loss: 0.00012926
Iteration 36/1000 | Loss: 0.00006346
Iteration 37/1000 | Loss: 0.00042919
Iteration 38/1000 | Loss: 0.00041754
Iteration 39/1000 | Loss: 0.00111253
Iteration 40/1000 | Loss: 0.00074593
Iteration 41/1000 | Loss: 0.00008656
Iteration 42/1000 | Loss: 0.00020594
Iteration 43/1000 | Loss: 0.00011374
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00003702
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00002073
Iteration 48/1000 | Loss: 0.00001566
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00003896
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001497
Iteration 53/1000 | Loss: 0.00003310
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00003456
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001459
Iteration 61/1000 | Loss: 0.00001459
Iteration 62/1000 | Loss: 0.00001459
Iteration 63/1000 | Loss: 0.00001459
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001453
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001451
Iteration 80/1000 | Loss: 0.00001450
Iteration 81/1000 | Loss: 0.00001450
Iteration 82/1000 | Loss: 0.00001450
Iteration 83/1000 | Loss: 0.00001449
Iteration 84/1000 | Loss: 0.00005211
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001446
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001445
Iteration 99/1000 | Loss: 0.00001445
Iteration 100/1000 | Loss: 0.00001445
Iteration 101/1000 | Loss: 0.00001445
Iteration 102/1000 | Loss: 0.00001445
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001444
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001444
Iteration 107/1000 | Loss: 0.00001444
Iteration 108/1000 | Loss: 0.00001444
Iteration 109/1000 | Loss: 0.00001444
Iteration 110/1000 | Loss: 0.00001444
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4441798157349695e-05, 1.4441798157349695e-05, 1.4441798157349695e-05, 1.4441798157349695e-05, 1.4441798157349695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4441798157349695e-05

Optimization complete. Final v2v error: 3.2285404205322266 mm

Highest mean error: 4.080772399902344 mm for frame 62

Lowest mean error: 3.129042148590088 mm for frame 27

Saving results

Total time: 99.26445579528809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621247
Iteration 2/25 | Loss: 0.00133650
Iteration 3/25 | Loss: 0.00094429
Iteration 4/25 | Loss: 0.00086683
Iteration 5/25 | Loss: 0.00085296
Iteration 6/25 | Loss: 0.00083913
Iteration 7/25 | Loss: 0.00083608
Iteration 8/25 | Loss: 0.00083459
Iteration 9/25 | Loss: 0.00082941
Iteration 10/25 | Loss: 0.00082790
Iteration 11/25 | Loss: 0.00082714
Iteration 12/25 | Loss: 0.00082680
Iteration 13/25 | Loss: 0.00082659
Iteration 14/25 | Loss: 0.00082650
Iteration 15/25 | Loss: 0.00082647
Iteration 16/25 | Loss: 0.00082646
Iteration 17/25 | Loss: 0.00082646
Iteration 18/25 | Loss: 0.00082646
Iteration 19/25 | Loss: 0.00082646
Iteration 20/25 | Loss: 0.00082646
Iteration 21/25 | Loss: 0.00082646
Iteration 22/25 | Loss: 0.00082646
Iteration 23/25 | Loss: 0.00082646
Iteration 24/25 | Loss: 0.00082646
Iteration 25/25 | Loss: 0.00082645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52957988
Iteration 2/25 | Loss: 0.00052404
Iteration 3/25 | Loss: 0.00052400
Iteration 4/25 | Loss: 0.00052400
Iteration 5/25 | Loss: 0.00052400
Iteration 6/25 | Loss: 0.00052400
Iteration 7/25 | Loss: 0.00052400
Iteration 8/25 | Loss: 0.00052400
Iteration 9/25 | Loss: 0.00052400
Iteration 10/25 | Loss: 0.00052400
Iteration 11/25 | Loss: 0.00052400
Iteration 12/25 | Loss: 0.00052400
Iteration 13/25 | Loss: 0.00052400
Iteration 14/25 | Loss: 0.00052400
Iteration 15/25 | Loss: 0.00052400
Iteration 16/25 | Loss: 0.00052400
Iteration 17/25 | Loss: 0.00052400
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005239982274360955, 0.0005239982274360955, 0.0005239982274360955, 0.0005239982274360955, 0.0005239982274360955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005239982274360955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052400
Iteration 2/1000 | Loss: 0.00004494
Iteration 3/1000 | Loss: 0.00002871
Iteration 4/1000 | Loss: 0.00002541
Iteration 5/1000 | Loss: 0.00002435
Iteration 6/1000 | Loss: 0.00025880
Iteration 7/1000 | Loss: 0.00003029
Iteration 8/1000 | Loss: 0.00002681
Iteration 9/1000 | Loss: 0.00002519
Iteration 10/1000 | Loss: 0.00002427
Iteration 11/1000 | Loss: 0.00002362
Iteration 12/1000 | Loss: 0.00002308
Iteration 13/1000 | Loss: 0.00002251
Iteration 14/1000 | Loss: 0.00002207
Iteration 15/1000 | Loss: 0.00053584
Iteration 16/1000 | Loss: 0.00002553
Iteration 17/1000 | Loss: 0.00002261
Iteration 18/1000 | Loss: 0.00002159
Iteration 19/1000 | Loss: 0.00002087
Iteration 20/1000 | Loss: 0.00002025
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001976
Iteration 23/1000 | Loss: 0.00001974
Iteration 24/1000 | Loss: 0.00001973
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001973
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001971
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001958
Iteration 33/1000 | Loss: 0.00001957
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00001953
Iteration 36/1000 | Loss: 0.00001950
Iteration 37/1000 | Loss: 0.00001950
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001944
Iteration 44/1000 | Loss: 0.00001942
Iteration 45/1000 | Loss: 0.00001941
Iteration 46/1000 | Loss: 0.00001941
Iteration 47/1000 | Loss: 0.00001940
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001927
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001910
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001899
Iteration 61/1000 | Loss: 0.00001896
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001895
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001894
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001893
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001890
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001889
Iteration 102/1000 | Loss: 0.00001889
Iteration 103/1000 | Loss: 0.00001889
Iteration 104/1000 | Loss: 0.00001889
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001889
Iteration 107/1000 | Loss: 0.00001889
Iteration 108/1000 | Loss: 0.00001889
Iteration 109/1000 | Loss: 0.00001889
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001888
Iteration 113/1000 | Loss: 0.00001888
Iteration 114/1000 | Loss: 0.00001888
Iteration 115/1000 | Loss: 0.00001888
Iteration 116/1000 | Loss: 0.00001888
Iteration 117/1000 | Loss: 0.00001888
Iteration 118/1000 | Loss: 0.00001888
Iteration 119/1000 | Loss: 0.00001888
Iteration 120/1000 | Loss: 0.00001888
Iteration 121/1000 | Loss: 0.00001888
Iteration 122/1000 | Loss: 0.00001888
Iteration 123/1000 | Loss: 0.00001888
Iteration 124/1000 | Loss: 0.00001888
Iteration 125/1000 | Loss: 0.00001887
Iteration 126/1000 | Loss: 0.00001887
Iteration 127/1000 | Loss: 0.00001887
Iteration 128/1000 | Loss: 0.00001887
Iteration 129/1000 | Loss: 0.00001887
Iteration 130/1000 | Loss: 0.00001887
Iteration 131/1000 | Loss: 0.00001887
Iteration 132/1000 | Loss: 0.00001887
Iteration 133/1000 | Loss: 0.00001887
Iteration 134/1000 | Loss: 0.00001887
Iteration 135/1000 | Loss: 0.00001887
Iteration 136/1000 | Loss: 0.00001887
Iteration 137/1000 | Loss: 0.00001887
Iteration 138/1000 | Loss: 0.00001886
Iteration 139/1000 | Loss: 0.00001886
Iteration 140/1000 | Loss: 0.00001886
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001885
Iteration 144/1000 | Loss: 0.00001885
Iteration 145/1000 | Loss: 0.00001885
Iteration 146/1000 | Loss: 0.00001885
Iteration 147/1000 | Loss: 0.00001885
Iteration 148/1000 | Loss: 0.00001885
Iteration 149/1000 | Loss: 0.00001884
Iteration 150/1000 | Loss: 0.00001884
Iteration 151/1000 | Loss: 0.00001884
Iteration 152/1000 | Loss: 0.00001884
Iteration 153/1000 | Loss: 0.00001883
Iteration 154/1000 | Loss: 0.00001883
Iteration 155/1000 | Loss: 0.00001883
Iteration 156/1000 | Loss: 0.00001883
Iteration 157/1000 | Loss: 0.00001882
Iteration 158/1000 | Loss: 0.00001882
Iteration 159/1000 | Loss: 0.00001882
Iteration 160/1000 | Loss: 0.00001882
Iteration 161/1000 | Loss: 0.00001882
Iteration 162/1000 | Loss: 0.00001882
Iteration 163/1000 | Loss: 0.00001882
Iteration 164/1000 | Loss: 0.00001881
Iteration 165/1000 | Loss: 0.00001881
Iteration 166/1000 | Loss: 0.00001881
Iteration 167/1000 | Loss: 0.00001881
Iteration 168/1000 | Loss: 0.00001881
Iteration 169/1000 | Loss: 0.00001880
Iteration 170/1000 | Loss: 0.00001880
Iteration 171/1000 | Loss: 0.00001880
Iteration 172/1000 | Loss: 0.00001880
Iteration 173/1000 | Loss: 0.00001880
Iteration 174/1000 | Loss: 0.00001880
Iteration 175/1000 | Loss: 0.00001880
Iteration 176/1000 | Loss: 0.00001880
Iteration 177/1000 | Loss: 0.00001880
Iteration 178/1000 | Loss: 0.00001879
Iteration 179/1000 | Loss: 0.00001879
Iteration 180/1000 | Loss: 0.00001879
Iteration 181/1000 | Loss: 0.00001879
Iteration 182/1000 | Loss: 0.00001879
Iteration 183/1000 | Loss: 0.00001879
Iteration 184/1000 | Loss: 0.00001879
Iteration 185/1000 | Loss: 0.00001879
Iteration 186/1000 | Loss: 0.00001879
Iteration 187/1000 | Loss: 0.00001879
Iteration 188/1000 | Loss: 0.00001878
Iteration 189/1000 | Loss: 0.00001878
Iteration 190/1000 | Loss: 0.00001878
Iteration 191/1000 | Loss: 0.00001878
Iteration 192/1000 | Loss: 0.00001878
Iteration 193/1000 | Loss: 0.00001878
Iteration 194/1000 | Loss: 0.00001878
Iteration 195/1000 | Loss: 0.00001877
Iteration 196/1000 | Loss: 0.00001877
Iteration 197/1000 | Loss: 0.00001877
Iteration 198/1000 | Loss: 0.00001877
Iteration 199/1000 | Loss: 0.00001877
Iteration 200/1000 | Loss: 0.00001876
Iteration 201/1000 | Loss: 0.00001876
Iteration 202/1000 | Loss: 0.00001876
Iteration 203/1000 | Loss: 0.00001876
Iteration 204/1000 | Loss: 0.00001876
Iteration 205/1000 | Loss: 0.00001876
Iteration 206/1000 | Loss: 0.00001876
Iteration 207/1000 | Loss: 0.00001876
Iteration 208/1000 | Loss: 0.00001876
Iteration 209/1000 | Loss: 0.00001876
Iteration 210/1000 | Loss: 0.00001876
Iteration 211/1000 | Loss: 0.00001875
Iteration 212/1000 | Loss: 0.00001875
Iteration 213/1000 | Loss: 0.00001875
Iteration 214/1000 | Loss: 0.00001875
Iteration 215/1000 | Loss: 0.00001875
Iteration 216/1000 | Loss: 0.00001875
Iteration 217/1000 | Loss: 0.00001875
Iteration 218/1000 | Loss: 0.00001875
Iteration 219/1000 | Loss: 0.00001875
Iteration 220/1000 | Loss: 0.00001875
Iteration 221/1000 | Loss: 0.00001875
Iteration 222/1000 | Loss: 0.00001875
Iteration 223/1000 | Loss: 0.00001875
Iteration 224/1000 | Loss: 0.00001875
Iteration 225/1000 | Loss: 0.00001874
Iteration 226/1000 | Loss: 0.00001874
Iteration 227/1000 | Loss: 0.00001874
Iteration 228/1000 | Loss: 0.00001874
Iteration 229/1000 | Loss: 0.00001874
Iteration 230/1000 | Loss: 0.00001874
Iteration 231/1000 | Loss: 0.00001874
Iteration 232/1000 | Loss: 0.00001873
Iteration 233/1000 | Loss: 0.00001873
Iteration 234/1000 | Loss: 0.00001873
Iteration 235/1000 | Loss: 0.00001873
Iteration 236/1000 | Loss: 0.00001873
Iteration 237/1000 | Loss: 0.00001873
Iteration 238/1000 | Loss: 0.00001873
Iteration 239/1000 | Loss: 0.00001873
Iteration 240/1000 | Loss: 0.00001873
Iteration 241/1000 | Loss: 0.00001873
Iteration 242/1000 | Loss: 0.00001873
Iteration 243/1000 | Loss: 0.00001873
Iteration 244/1000 | Loss: 0.00001873
Iteration 245/1000 | Loss: 0.00001873
Iteration 246/1000 | Loss: 0.00001873
Iteration 247/1000 | Loss: 0.00001873
Iteration 248/1000 | Loss: 0.00001872
Iteration 249/1000 | Loss: 0.00001872
Iteration 250/1000 | Loss: 0.00001872
Iteration 251/1000 | Loss: 0.00001872
Iteration 252/1000 | Loss: 0.00001872
Iteration 253/1000 | Loss: 0.00001872
Iteration 254/1000 | Loss: 0.00001872
Iteration 255/1000 | Loss: 0.00001872
Iteration 256/1000 | Loss: 0.00001872
Iteration 257/1000 | Loss: 0.00001872
Iteration 258/1000 | Loss: 0.00001872
Iteration 259/1000 | Loss: 0.00001872
Iteration 260/1000 | Loss: 0.00001872
Iteration 261/1000 | Loss: 0.00001872
Iteration 262/1000 | Loss: 0.00001872
Iteration 263/1000 | Loss: 0.00001872
Iteration 264/1000 | Loss: 0.00001872
Iteration 265/1000 | Loss: 0.00001871
Iteration 266/1000 | Loss: 0.00001871
Iteration 267/1000 | Loss: 0.00001871
Iteration 268/1000 | Loss: 0.00001871
Iteration 269/1000 | Loss: 0.00001871
Iteration 270/1000 | Loss: 0.00001871
Iteration 271/1000 | Loss: 0.00001871
Iteration 272/1000 | Loss: 0.00001871
Iteration 273/1000 | Loss: 0.00001871
Iteration 274/1000 | Loss: 0.00001871
Iteration 275/1000 | Loss: 0.00001871
Iteration 276/1000 | Loss: 0.00001871
Iteration 277/1000 | Loss: 0.00001871
Iteration 278/1000 | Loss: 0.00001871
Iteration 279/1000 | Loss: 0.00001871
Iteration 280/1000 | Loss: 0.00001871
Iteration 281/1000 | Loss: 0.00001871
Iteration 282/1000 | Loss: 0.00001871
Iteration 283/1000 | Loss: 0.00001871
Iteration 284/1000 | Loss: 0.00001871
Iteration 285/1000 | Loss: 0.00001870
Iteration 286/1000 | Loss: 0.00001870
Iteration 287/1000 | Loss: 0.00001870
Iteration 288/1000 | Loss: 0.00001870
Iteration 289/1000 | Loss: 0.00001870
Iteration 290/1000 | Loss: 0.00001870
Iteration 291/1000 | Loss: 0.00001870
Iteration 292/1000 | Loss: 0.00001870
Iteration 293/1000 | Loss: 0.00001870
Iteration 294/1000 | Loss: 0.00001870
Iteration 295/1000 | Loss: 0.00001870
Iteration 296/1000 | Loss: 0.00001870
Iteration 297/1000 | Loss: 0.00001870
Iteration 298/1000 | Loss: 0.00001870
Iteration 299/1000 | Loss: 0.00001870
Iteration 300/1000 | Loss: 0.00001870
Iteration 301/1000 | Loss: 0.00001870
Iteration 302/1000 | Loss: 0.00001870
Iteration 303/1000 | Loss: 0.00001870
Iteration 304/1000 | Loss: 0.00001870
Iteration 305/1000 | Loss: 0.00001870
Iteration 306/1000 | Loss: 0.00001869
Iteration 307/1000 | Loss: 0.00001869
Iteration 308/1000 | Loss: 0.00001869
Iteration 309/1000 | Loss: 0.00001869
Iteration 310/1000 | Loss: 0.00001869
Iteration 311/1000 | Loss: 0.00001869
Iteration 312/1000 | Loss: 0.00001869
Iteration 313/1000 | Loss: 0.00001869
Iteration 314/1000 | Loss: 0.00001869
Iteration 315/1000 | Loss: 0.00001869
Iteration 316/1000 | Loss: 0.00001869
Iteration 317/1000 | Loss: 0.00001869
Iteration 318/1000 | Loss: 0.00001869
Iteration 319/1000 | Loss: 0.00001869
Iteration 320/1000 | Loss: 0.00001869
Iteration 321/1000 | Loss: 0.00001869
Iteration 322/1000 | Loss: 0.00001869
Iteration 323/1000 | Loss: 0.00001869
Iteration 324/1000 | Loss: 0.00001869
Iteration 325/1000 | Loss: 0.00001869
Iteration 326/1000 | Loss: 0.00001869
Iteration 327/1000 | Loss: 0.00001869
Iteration 328/1000 | Loss: 0.00001869
Iteration 329/1000 | Loss: 0.00001869
Iteration 330/1000 | Loss: 0.00001869
Iteration 331/1000 | Loss: 0.00001869
Iteration 332/1000 | Loss: 0.00001869
Iteration 333/1000 | Loss: 0.00001869
Iteration 334/1000 | Loss: 0.00001869
Iteration 335/1000 | Loss: 0.00001869
Iteration 336/1000 | Loss: 0.00001869
Iteration 337/1000 | Loss: 0.00001869
Iteration 338/1000 | Loss: 0.00001869
Iteration 339/1000 | Loss: 0.00001869
Iteration 340/1000 | Loss: 0.00001869
Iteration 341/1000 | Loss: 0.00001869
Iteration 342/1000 | Loss: 0.00001869
Iteration 343/1000 | Loss: 0.00001869
Iteration 344/1000 | Loss: 0.00001869
Iteration 345/1000 | Loss: 0.00001869
Iteration 346/1000 | Loss: 0.00001869
Iteration 347/1000 | Loss: 0.00001869
Iteration 348/1000 | Loss: 0.00001869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 348. Stopping optimization.
Last 5 losses: [1.8691540390136652e-05, 1.8691540390136652e-05, 1.8691540390136652e-05, 1.8691540390136652e-05, 1.8691540390136652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8691540390136652e-05

Optimization complete. Final v2v error: 3.612304449081421 mm

Highest mean error: 6.3526411056518555 mm for frame 166

Lowest mean error: 2.7646334171295166 mm for frame 238

Saving results

Total time: 95.73363041877747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387161
Iteration 2/25 | Loss: 0.00081871
Iteration 3/25 | Loss: 0.00073245
Iteration 4/25 | Loss: 0.00071447
Iteration 5/25 | Loss: 0.00070957
Iteration 6/25 | Loss: 0.00070837
Iteration 7/25 | Loss: 0.00070837
Iteration 8/25 | Loss: 0.00070837
Iteration 9/25 | Loss: 0.00070837
Iteration 10/25 | Loss: 0.00070837
Iteration 11/25 | Loss: 0.00070837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007083710515871644, 0.0007083710515871644, 0.0007083710515871644, 0.0007083710515871644, 0.0007083710515871644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007083710515871644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.50083065
Iteration 2/25 | Loss: 0.00046676
Iteration 3/25 | Loss: 0.00046676
Iteration 4/25 | Loss: 0.00046676
Iteration 5/25 | Loss: 0.00046675
Iteration 6/25 | Loss: 0.00046675
Iteration 7/25 | Loss: 0.00046675
Iteration 8/25 | Loss: 0.00046675
Iteration 9/25 | Loss: 0.00046675
Iteration 10/25 | Loss: 0.00046675
Iteration 11/25 | Loss: 0.00046675
Iteration 12/25 | Loss: 0.00046675
Iteration 13/25 | Loss: 0.00046675
Iteration 14/25 | Loss: 0.00046675
Iteration 15/25 | Loss: 0.00046675
Iteration 16/25 | Loss: 0.00046675
Iteration 17/25 | Loss: 0.00046675
Iteration 18/25 | Loss: 0.00046675
Iteration 19/25 | Loss: 0.00046675
Iteration 20/25 | Loss: 0.00046675
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004667531466111541, 0.0004667531466111541, 0.0004667531466111541, 0.0004667531466111541, 0.0004667531466111541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004667531466111541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046675
Iteration 2/1000 | Loss: 0.00002012
Iteration 3/1000 | Loss: 0.00001490
Iteration 4/1000 | Loss: 0.00001399
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001297
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001253
Iteration 9/1000 | Loss: 0.00001234
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001225
Iteration 16/1000 | Loss: 0.00001224
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001194
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001190
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001176
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001173
Iteration 99/1000 | Loss: 0.00001173
Iteration 100/1000 | Loss: 0.00001173
Iteration 101/1000 | Loss: 0.00001173
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001173
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001173
Iteration 107/1000 | Loss: 0.00001173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.1730364349205047e-05, 1.1730364349205047e-05, 1.1730364349205047e-05, 1.1730364349205047e-05, 1.1730364349205047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1730364349205047e-05

Optimization complete. Final v2v error: 2.9292023181915283 mm

Highest mean error: 3.2122879028320312 mm for frame 98

Lowest mean error: 2.7813174724578857 mm for frame 121

Saving results

Total time: 36.22872805595398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479859
Iteration 2/25 | Loss: 0.00093484
Iteration 3/25 | Loss: 0.00079176
Iteration 4/25 | Loss: 0.00075654
Iteration 5/25 | Loss: 0.00074629
Iteration 6/25 | Loss: 0.00074417
Iteration 7/25 | Loss: 0.00074355
Iteration 8/25 | Loss: 0.00074355
Iteration 9/25 | Loss: 0.00074355
Iteration 10/25 | Loss: 0.00074355
Iteration 11/25 | Loss: 0.00074355
Iteration 12/25 | Loss: 0.00074355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007435478619299829, 0.0007435478619299829, 0.0007435478619299829, 0.0007435478619299829, 0.0007435478619299829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007435478619299829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53735125
Iteration 2/25 | Loss: 0.00045634
Iteration 3/25 | Loss: 0.00045633
Iteration 4/25 | Loss: 0.00045632
Iteration 5/25 | Loss: 0.00045632
Iteration 6/25 | Loss: 0.00045632
Iteration 7/25 | Loss: 0.00045632
Iteration 8/25 | Loss: 0.00045632
Iteration 9/25 | Loss: 0.00045632
Iteration 10/25 | Loss: 0.00045632
Iteration 11/25 | Loss: 0.00045632
Iteration 12/25 | Loss: 0.00045632
Iteration 13/25 | Loss: 0.00045632
Iteration 14/25 | Loss: 0.00045632
Iteration 15/25 | Loss: 0.00045632
Iteration 16/25 | Loss: 0.00045632
Iteration 17/25 | Loss: 0.00045632
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004563226830214262, 0.0004563226830214262, 0.0004563226830214262, 0.0004563226830214262, 0.0004563226830214262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004563226830214262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045632
Iteration 2/1000 | Loss: 0.00002604
Iteration 3/1000 | Loss: 0.00001873
Iteration 4/1000 | Loss: 0.00001747
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001537
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001509
Iteration 13/1000 | Loss: 0.00001507
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001496
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001488
Iteration 18/1000 | Loss: 0.00001487
Iteration 19/1000 | Loss: 0.00001486
Iteration 20/1000 | Loss: 0.00001484
Iteration 21/1000 | Loss: 0.00001484
Iteration 22/1000 | Loss: 0.00001483
Iteration 23/1000 | Loss: 0.00001483
Iteration 24/1000 | Loss: 0.00001483
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001480
Iteration 29/1000 | Loss: 0.00001480
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001477
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001477
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001475
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001474
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001474
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001473
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001472
Iteration 58/1000 | Loss: 0.00001472
Iteration 59/1000 | Loss: 0.00001471
Iteration 60/1000 | Loss: 0.00001471
Iteration 61/1000 | Loss: 0.00001471
Iteration 62/1000 | Loss: 0.00001470
Iteration 63/1000 | Loss: 0.00001470
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001467
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001467
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001465
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001465
Iteration 86/1000 | Loss: 0.00001465
Iteration 87/1000 | Loss: 0.00001464
Iteration 88/1000 | Loss: 0.00001464
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001464
Iteration 91/1000 | Loss: 0.00001464
Iteration 92/1000 | Loss: 0.00001463
Iteration 93/1000 | Loss: 0.00001463
Iteration 94/1000 | Loss: 0.00001463
Iteration 95/1000 | Loss: 0.00001463
Iteration 96/1000 | Loss: 0.00001463
Iteration 97/1000 | Loss: 0.00001463
Iteration 98/1000 | Loss: 0.00001463
Iteration 99/1000 | Loss: 0.00001462
Iteration 100/1000 | Loss: 0.00001462
Iteration 101/1000 | Loss: 0.00001462
Iteration 102/1000 | Loss: 0.00001462
Iteration 103/1000 | Loss: 0.00001462
Iteration 104/1000 | Loss: 0.00001462
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001461
Iteration 109/1000 | Loss: 0.00001461
Iteration 110/1000 | Loss: 0.00001460
Iteration 111/1000 | Loss: 0.00001460
Iteration 112/1000 | Loss: 0.00001460
Iteration 113/1000 | Loss: 0.00001460
Iteration 114/1000 | Loss: 0.00001459
Iteration 115/1000 | Loss: 0.00001459
Iteration 116/1000 | Loss: 0.00001459
Iteration 117/1000 | Loss: 0.00001458
Iteration 118/1000 | Loss: 0.00001458
Iteration 119/1000 | Loss: 0.00001458
Iteration 120/1000 | Loss: 0.00001458
Iteration 121/1000 | Loss: 0.00001457
Iteration 122/1000 | Loss: 0.00001457
Iteration 123/1000 | Loss: 0.00001457
Iteration 124/1000 | Loss: 0.00001457
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001456
Iteration 127/1000 | Loss: 0.00001456
Iteration 128/1000 | Loss: 0.00001456
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001455
Iteration 136/1000 | Loss: 0.00001455
Iteration 137/1000 | Loss: 0.00001455
Iteration 138/1000 | Loss: 0.00001455
Iteration 139/1000 | Loss: 0.00001455
Iteration 140/1000 | Loss: 0.00001455
Iteration 141/1000 | Loss: 0.00001455
Iteration 142/1000 | Loss: 0.00001455
Iteration 143/1000 | Loss: 0.00001455
Iteration 144/1000 | Loss: 0.00001455
Iteration 145/1000 | Loss: 0.00001455
Iteration 146/1000 | Loss: 0.00001455
Iteration 147/1000 | Loss: 0.00001455
Iteration 148/1000 | Loss: 0.00001455
Iteration 149/1000 | Loss: 0.00001455
Iteration 150/1000 | Loss: 0.00001455
Iteration 151/1000 | Loss: 0.00001455
Iteration 152/1000 | Loss: 0.00001455
Iteration 153/1000 | Loss: 0.00001455
Iteration 154/1000 | Loss: 0.00001455
Iteration 155/1000 | Loss: 0.00001455
Iteration 156/1000 | Loss: 0.00001455
Iteration 157/1000 | Loss: 0.00001455
Iteration 158/1000 | Loss: 0.00001455
Iteration 159/1000 | Loss: 0.00001455
Iteration 160/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4547190403391141e-05, 1.4547190403391141e-05, 1.4547190403391141e-05, 1.4547190403391141e-05, 1.4547190403391141e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4547190403391141e-05

Optimization complete. Final v2v error: 3.2292568683624268 mm

Highest mean error: 4.110966682434082 mm for frame 61

Lowest mean error: 2.7871508598327637 mm for frame 11

Saving results

Total time: 41.48726987838745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00599811
Iteration 2/25 | Loss: 0.00118410
Iteration 3/25 | Loss: 0.00088592
Iteration 4/25 | Loss: 0.00081022
Iteration 5/25 | Loss: 0.00074523
Iteration 6/25 | Loss: 0.00073921
Iteration 7/25 | Loss: 0.00073664
Iteration 8/25 | Loss: 0.00073175
Iteration 9/25 | Loss: 0.00072963
Iteration 10/25 | Loss: 0.00072808
Iteration 11/25 | Loss: 0.00072693
Iteration 12/25 | Loss: 0.00072306
Iteration 13/25 | Loss: 0.00072242
Iteration 14/25 | Loss: 0.00072177
Iteration 15/25 | Loss: 0.00072386
Iteration 16/25 | Loss: 0.00072048
Iteration 17/25 | Loss: 0.00071886
Iteration 18/25 | Loss: 0.00071810
Iteration 19/25 | Loss: 0.00071777
Iteration 20/25 | Loss: 0.00071773
Iteration 21/25 | Loss: 0.00071773
Iteration 22/25 | Loss: 0.00071773
Iteration 23/25 | Loss: 0.00071773
Iteration 24/25 | Loss: 0.00071773
Iteration 25/25 | Loss: 0.00071772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32703042
Iteration 2/25 | Loss: 0.00045368
Iteration 3/25 | Loss: 0.00045368
Iteration 4/25 | Loss: 0.00045368
Iteration 5/25 | Loss: 0.00045368
Iteration 6/25 | Loss: 0.00045368
Iteration 7/25 | Loss: 0.00045368
Iteration 8/25 | Loss: 0.00045368
Iteration 9/25 | Loss: 0.00045368
Iteration 10/25 | Loss: 0.00045368
Iteration 11/25 | Loss: 0.00045368
Iteration 12/25 | Loss: 0.00045368
Iteration 13/25 | Loss: 0.00045368
Iteration 14/25 | Loss: 0.00045368
Iteration 15/25 | Loss: 0.00045368
Iteration 16/25 | Loss: 0.00045368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00045367726124823093, 0.00045367726124823093, 0.00045367726124823093, 0.00045367726124823093, 0.00045367726124823093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045367726124823093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045368
Iteration 2/1000 | Loss: 0.00002250
Iteration 3/1000 | Loss: 0.00001460
Iteration 4/1000 | Loss: 0.00001356
Iteration 5/1000 | Loss: 0.00001270
Iteration 6/1000 | Loss: 0.00001223
Iteration 7/1000 | Loss: 0.00015119
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001162
Iteration 11/1000 | Loss: 0.00001153
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001119
Iteration 34/1000 | Loss: 0.00001119
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001118
Iteration 37/1000 | Loss: 0.00001118
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001116
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001115
Iteration 42/1000 | Loss: 0.00001114
Iteration 43/1000 | Loss: 0.00001113
Iteration 44/1000 | Loss: 0.00001113
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001112
Iteration 51/1000 | Loss: 0.00001112
Iteration 52/1000 | Loss: 0.00001112
Iteration 53/1000 | Loss: 0.00001112
Iteration 54/1000 | Loss: 0.00001112
Iteration 55/1000 | Loss: 0.00001112
Iteration 56/1000 | Loss: 0.00001112
Iteration 57/1000 | Loss: 0.00001112
Iteration 58/1000 | Loss: 0.00001112
Iteration 59/1000 | Loss: 0.00001112
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001112
Iteration 62/1000 | Loss: 0.00001112
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001112
Iteration 76/1000 | Loss: 0.00001112
Iteration 77/1000 | Loss: 0.00001112
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001112
Iteration 85/1000 | Loss: 0.00001112
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.111597339331638e-05, 1.111597339331638e-05, 1.111597339331638e-05, 1.111597339331638e-05, 1.111597339331638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.111597339331638e-05

Optimization complete. Final v2v error: 2.833972454071045 mm

Highest mean error: 3.179311752319336 mm for frame 71

Lowest mean error: 2.571321725845337 mm for frame 178

Saving results

Total time: 62.84982180595398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114229
Iteration 2/25 | Loss: 0.00322845
Iteration 3/25 | Loss: 0.00195863
Iteration 4/25 | Loss: 0.00164162
Iteration 5/25 | Loss: 0.00146305
Iteration 6/25 | Loss: 0.00129369
Iteration 7/25 | Loss: 0.00117223
Iteration 8/25 | Loss: 0.00110139
Iteration 9/25 | Loss: 0.00107515
Iteration 10/25 | Loss: 0.00105256
Iteration 11/25 | Loss: 0.00104272
Iteration 12/25 | Loss: 0.00103971
Iteration 13/25 | Loss: 0.00102878
Iteration 14/25 | Loss: 0.00102094
Iteration 15/25 | Loss: 0.00102465
Iteration 16/25 | Loss: 0.00101432
Iteration 17/25 | Loss: 0.00101201
Iteration 18/25 | Loss: 0.00102096
Iteration 19/25 | Loss: 0.00100882
Iteration 20/25 | Loss: 0.00100598
Iteration 21/25 | Loss: 0.00100529
Iteration 22/25 | Loss: 0.00100509
Iteration 23/25 | Loss: 0.00100495
Iteration 24/25 | Loss: 0.00100492
Iteration 25/25 | Loss: 0.00100492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62001675
Iteration 2/25 | Loss: 0.00747944
Iteration 3/25 | Loss: 0.00134499
Iteration 4/25 | Loss: 0.00134497
Iteration 5/25 | Loss: 0.00134497
Iteration 6/25 | Loss: 0.00134496
Iteration 7/25 | Loss: 0.00134496
Iteration 8/25 | Loss: 0.00134496
Iteration 9/25 | Loss: 0.00134496
Iteration 10/25 | Loss: 0.00134496
Iteration 11/25 | Loss: 0.00134496
Iteration 12/25 | Loss: 0.00134496
Iteration 13/25 | Loss: 0.00134496
Iteration 14/25 | Loss: 0.00134496
Iteration 15/25 | Loss: 0.00134496
Iteration 16/25 | Loss: 0.00134496
Iteration 17/25 | Loss: 0.00134496
Iteration 18/25 | Loss: 0.00134496
Iteration 19/25 | Loss: 0.00134496
Iteration 20/25 | Loss: 0.00134496
Iteration 21/25 | Loss: 0.00134496
Iteration 22/25 | Loss: 0.00134496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001344963675364852, 0.001344963675364852, 0.001344963675364852, 0.001344963675364852, 0.001344963675364852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001344963675364852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134496
Iteration 2/1000 | Loss: 0.00018264
Iteration 3/1000 | Loss: 0.00012767
Iteration 4/1000 | Loss: 0.00010958
Iteration 5/1000 | Loss: 0.00009572
Iteration 6/1000 | Loss: 0.00074648
Iteration 7/1000 | Loss: 0.00847424
Iteration 8/1000 | Loss: 0.00050587
Iteration 9/1000 | Loss: 0.00023729
Iteration 10/1000 | Loss: 0.00011666
Iteration 11/1000 | Loss: 0.00008775
Iteration 12/1000 | Loss: 0.00007910
Iteration 13/1000 | Loss: 0.00007567
Iteration 14/1000 | Loss: 0.00087327
Iteration 15/1000 | Loss: 0.00009117
Iteration 16/1000 | Loss: 0.00008093
Iteration 17/1000 | Loss: 0.00007463
Iteration 18/1000 | Loss: 0.00007300
Iteration 19/1000 | Loss: 0.00007252
Iteration 20/1000 | Loss: 0.00007195
Iteration 21/1000 | Loss: 0.00206360
Iteration 22/1000 | Loss: 0.00071297
Iteration 23/1000 | Loss: 0.00108427
Iteration 24/1000 | Loss: 0.00069391
Iteration 25/1000 | Loss: 0.00007613
Iteration 26/1000 | Loss: 0.00030668
Iteration 27/1000 | Loss: 0.00331412
Iteration 28/1000 | Loss: 0.00066412
Iteration 29/1000 | Loss: 0.00083615
Iteration 30/1000 | Loss: 0.00027969
Iteration 31/1000 | Loss: 0.00016910
Iteration 32/1000 | Loss: 0.00007465
Iteration 33/1000 | Loss: 0.00050283
Iteration 34/1000 | Loss: 0.00021324
Iteration 35/1000 | Loss: 0.00006925
Iteration 36/1000 | Loss: 0.00081060
Iteration 37/1000 | Loss: 0.00089235
Iteration 38/1000 | Loss: 0.00019680
Iteration 39/1000 | Loss: 0.00027978
Iteration 40/1000 | Loss: 0.00013829
Iteration 41/1000 | Loss: 0.00007692
Iteration 42/1000 | Loss: 0.00007053
Iteration 43/1000 | Loss: 0.00006611
Iteration 44/1000 | Loss: 0.00034391
Iteration 45/1000 | Loss: 0.00008164
Iteration 46/1000 | Loss: 0.00006509
Iteration 47/1000 | Loss: 0.00028950
Iteration 48/1000 | Loss: 0.00007935
Iteration 49/1000 | Loss: 0.00006438
Iteration 50/1000 | Loss: 0.00019879
Iteration 51/1000 | Loss: 0.00008034
Iteration 52/1000 | Loss: 0.00006420
Iteration 53/1000 | Loss: 0.00023851
Iteration 54/1000 | Loss: 0.00006430
Iteration 55/1000 | Loss: 0.00006259
Iteration 56/1000 | Loss: 0.00006155
Iteration 57/1000 | Loss: 0.00006092
Iteration 58/1000 | Loss: 0.00006029
Iteration 59/1000 | Loss: 0.00030665
Iteration 60/1000 | Loss: 0.00028803
Iteration 61/1000 | Loss: 0.00072320
Iteration 62/1000 | Loss: 0.00105654
Iteration 63/1000 | Loss: 0.00034510
Iteration 64/1000 | Loss: 0.00028942
Iteration 65/1000 | Loss: 0.00038236
Iteration 66/1000 | Loss: 0.00006224
Iteration 67/1000 | Loss: 0.00077417
Iteration 68/1000 | Loss: 0.00005779
Iteration 69/1000 | Loss: 0.00005190
Iteration 70/1000 | Loss: 0.00005004
Iteration 71/1000 | Loss: 0.00068788
Iteration 72/1000 | Loss: 0.00083535
Iteration 73/1000 | Loss: 0.00021245
Iteration 74/1000 | Loss: 0.00004872
Iteration 75/1000 | Loss: 0.00004717
Iteration 76/1000 | Loss: 0.00004639
Iteration 77/1000 | Loss: 0.00131357
Iteration 78/1000 | Loss: 0.00277818
Iteration 79/1000 | Loss: 0.00024837
Iteration 80/1000 | Loss: 0.00221926
Iteration 81/1000 | Loss: 0.00006218
Iteration 82/1000 | Loss: 0.00005009
Iteration 83/1000 | Loss: 0.00004183
Iteration 84/1000 | Loss: 0.00003622
Iteration 85/1000 | Loss: 0.00092223
Iteration 86/1000 | Loss: 0.00003467
Iteration 87/1000 | Loss: 0.00003255
Iteration 88/1000 | Loss: 0.00003083
Iteration 89/1000 | Loss: 0.00002941
Iteration 90/1000 | Loss: 0.00002867
Iteration 91/1000 | Loss: 0.00002824
Iteration 92/1000 | Loss: 0.00002781
Iteration 93/1000 | Loss: 0.00002760
Iteration 94/1000 | Loss: 0.00002751
Iteration 95/1000 | Loss: 0.00002751
Iteration 96/1000 | Loss: 0.00002749
Iteration 97/1000 | Loss: 0.00002749
Iteration 98/1000 | Loss: 0.00002747
Iteration 99/1000 | Loss: 0.00002747
Iteration 100/1000 | Loss: 0.00002747
Iteration 101/1000 | Loss: 0.00002746
Iteration 102/1000 | Loss: 0.00002746
Iteration 103/1000 | Loss: 0.00002745
Iteration 104/1000 | Loss: 0.00002745
Iteration 105/1000 | Loss: 0.00002745
Iteration 106/1000 | Loss: 0.00002745
Iteration 107/1000 | Loss: 0.00002745
Iteration 108/1000 | Loss: 0.00002744
Iteration 109/1000 | Loss: 0.00002744
Iteration 110/1000 | Loss: 0.00002743
Iteration 111/1000 | Loss: 0.00002743
Iteration 112/1000 | Loss: 0.00002743
Iteration 113/1000 | Loss: 0.00002743
Iteration 114/1000 | Loss: 0.00002743
Iteration 115/1000 | Loss: 0.00002743
Iteration 116/1000 | Loss: 0.00002743
Iteration 117/1000 | Loss: 0.00002743
Iteration 118/1000 | Loss: 0.00002742
Iteration 119/1000 | Loss: 0.00002742
Iteration 120/1000 | Loss: 0.00002742
Iteration 121/1000 | Loss: 0.00002742
Iteration 122/1000 | Loss: 0.00002741
Iteration 123/1000 | Loss: 0.00002741
Iteration 124/1000 | Loss: 0.00002741
Iteration 125/1000 | Loss: 0.00002741
Iteration 126/1000 | Loss: 0.00002741
Iteration 127/1000 | Loss: 0.00002740
Iteration 128/1000 | Loss: 0.00002740
Iteration 129/1000 | Loss: 0.00002740
Iteration 130/1000 | Loss: 0.00002740
Iteration 131/1000 | Loss: 0.00002740
Iteration 132/1000 | Loss: 0.00002740
Iteration 133/1000 | Loss: 0.00002740
Iteration 134/1000 | Loss: 0.00002740
Iteration 135/1000 | Loss: 0.00002740
Iteration 136/1000 | Loss: 0.00002740
Iteration 137/1000 | Loss: 0.00002740
Iteration 138/1000 | Loss: 0.00002740
Iteration 139/1000 | Loss: 0.00002740
Iteration 140/1000 | Loss: 0.00002740
Iteration 141/1000 | Loss: 0.00002740
Iteration 142/1000 | Loss: 0.00002740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.7400981707614847e-05, 2.7400981707614847e-05, 2.7400981707614847e-05, 2.7400981707614847e-05, 2.7400981707614847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7400981707614847e-05

Optimization complete. Final v2v error: 4.3778204917907715 mm

Highest mean error: 4.763238906860352 mm for frame 41

Lowest mean error: 4.147428512573242 mm for frame 88

Saving results

Total time: 181.55283904075623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449889
Iteration 2/25 | Loss: 0.00111630
Iteration 3/25 | Loss: 0.00081830
Iteration 4/25 | Loss: 0.00078419
Iteration 5/25 | Loss: 0.00077647
Iteration 6/25 | Loss: 0.00077515
Iteration 7/25 | Loss: 0.00077513
Iteration 8/25 | Loss: 0.00077513
Iteration 9/25 | Loss: 0.00077513
Iteration 10/25 | Loss: 0.00077513
Iteration 11/25 | Loss: 0.00077513
Iteration 12/25 | Loss: 0.00077513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007751258090138435, 0.0007751258090138435, 0.0007751258090138435, 0.0007751258090138435, 0.0007751258090138435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007751258090138435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49870324
Iteration 2/25 | Loss: 0.00053033
Iteration 3/25 | Loss: 0.00053032
Iteration 4/25 | Loss: 0.00053032
Iteration 5/25 | Loss: 0.00053032
Iteration 6/25 | Loss: 0.00053032
Iteration 7/25 | Loss: 0.00053032
Iteration 8/25 | Loss: 0.00053032
Iteration 9/25 | Loss: 0.00053032
Iteration 10/25 | Loss: 0.00053032
Iteration 11/25 | Loss: 0.00053032
Iteration 12/25 | Loss: 0.00053032
Iteration 13/25 | Loss: 0.00053032
Iteration 14/25 | Loss: 0.00053032
Iteration 15/25 | Loss: 0.00053032
Iteration 16/25 | Loss: 0.00053032
Iteration 17/25 | Loss: 0.00053032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000530317600350827, 0.000530317600350827, 0.000530317600350827, 0.000530317600350827, 0.000530317600350827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000530317600350827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053032
Iteration 2/1000 | Loss: 0.00002046
Iteration 3/1000 | Loss: 0.00001522
Iteration 4/1000 | Loss: 0.00001395
Iteration 5/1000 | Loss: 0.00001342
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001253
Iteration 8/1000 | Loss: 0.00001238
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001216
Iteration 11/1000 | Loss: 0.00001214
Iteration 12/1000 | Loss: 0.00001214
Iteration 13/1000 | Loss: 0.00001209
Iteration 14/1000 | Loss: 0.00001203
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001202
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001187
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001186
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001183
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001182
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001181
Iteration 43/1000 | Loss: 0.00001181
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001169
Iteration 100/1000 | Loss: 0.00001169
Iteration 101/1000 | Loss: 0.00001169
Iteration 102/1000 | Loss: 0.00001169
Iteration 103/1000 | Loss: 0.00001169
Iteration 104/1000 | Loss: 0.00001169
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001168
Iteration 108/1000 | Loss: 0.00001168
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001167
Iteration 116/1000 | Loss: 0.00001167
Iteration 117/1000 | Loss: 0.00001167
Iteration 118/1000 | Loss: 0.00001167
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001167
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001166
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001166
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001165
Iteration 147/1000 | Loss: 0.00001165
Iteration 148/1000 | Loss: 0.00001165
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001165
Iteration 151/1000 | Loss: 0.00001165
Iteration 152/1000 | Loss: 0.00001165
Iteration 153/1000 | Loss: 0.00001165
Iteration 154/1000 | Loss: 0.00001165
Iteration 155/1000 | Loss: 0.00001165
Iteration 156/1000 | Loss: 0.00001165
Iteration 157/1000 | Loss: 0.00001165
Iteration 158/1000 | Loss: 0.00001165
Iteration 159/1000 | Loss: 0.00001165
Iteration 160/1000 | Loss: 0.00001165
Iteration 161/1000 | Loss: 0.00001165
Iteration 162/1000 | Loss: 0.00001165
Iteration 163/1000 | Loss: 0.00001165
Iteration 164/1000 | Loss: 0.00001165
Iteration 165/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.1653613000817131e-05, 1.1653613000817131e-05, 1.1653613000817131e-05, 1.1653613000817131e-05, 1.1653613000817131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1653613000817131e-05

Optimization complete. Final v2v error: 2.829409122467041 mm

Highest mean error: 3.0518152713775635 mm for frame 137

Lowest mean error: 2.6864895820617676 mm for frame 26

Saving results

Total time: 35.253873348236084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060430
Iteration 2/25 | Loss: 0.00380435
Iteration 3/25 | Loss: 0.00287017
Iteration 4/25 | Loss: 0.00264218
Iteration 5/25 | Loss: 0.00237547
Iteration 6/25 | Loss: 0.00210951
Iteration 7/25 | Loss: 0.00196766
Iteration 8/25 | Loss: 0.00178495
Iteration 9/25 | Loss: 0.00172722
Iteration 10/25 | Loss: 0.00153084
Iteration 11/25 | Loss: 0.00146555
Iteration 12/25 | Loss: 0.00140773
Iteration 13/25 | Loss: 0.00132975
Iteration 14/25 | Loss: 0.00130668
Iteration 15/25 | Loss: 0.00128384
Iteration 16/25 | Loss: 0.00128842
Iteration 17/25 | Loss: 0.00122923
Iteration 18/25 | Loss: 0.00121988
Iteration 19/25 | Loss: 0.00120587
Iteration 20/25 | Loss: 0.00119759
Iteration 21/25 | Loss: 0.00119490
Iteration 22/25 | Loss: 0.00119541
Iteration 23/25 | Loss: 0.00118504
Iteration 24/25 | Loss: 0.00118157
Iteration 25/25 | Loss: 0.00117976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47402227
Iteration 2/25 | Loss: 0.00468954
Iteration 3/25 | Loss: 0.00449461
Iteration 4/25 | Loss: 0.00449461
Iteration 5/25 | Loss: 0.00449460
Iteration 6/25 | Loss: 0.00449460
Iteration 7/25 | Loss: 0.00449460
Iteration 8/25 | Loss: 0.00449460
Iteration 9/25 | Loss: 0.00449460
Iteration 10/25 | Loss: 0.00449460
Iteration 11/25 | Loss: 0.00449460
Iteration 12/25 | Loss: 0.00449460
Iteration 13/25 | Loss: 0.00449460
Iteration 14/25 | Loss: 0.00449460
Iteration 15/25 | Loss: 0.00449460
Iteration 16/25 | Loss: 0.00449460
Iteration 17/25 | Loss: 0.00449460
Iteration 18/25 | Loss: 0.00449460
Iteration 19/25 | Loss: 0.00449460
Iteration 20/25 | Loss: 0.00449460
Iteration 21/25 | Loss: 0.00449460
Iteration 22/25 | Loss: 0.00449460
Iteration 23/25 | Loss: 0.00449460
Iteration 24/25 | Loss: 0.00449460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004494601860642433, 0.004494601860642433, 0.004494601860642433, 0.004494601860642433, 0.004494601860642433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004494601860642433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00449460
Iteration 2/1000 | Loss: 0.00111742
Iteration 3/1000 | Loss: 0.00118867
Iteration 4/1000 | Loss: 0.00082193
Iteration 5/1000 | Loss: 0.00159129
Iteration 6/1000 | Loss: 0.00059384
Iteration 7/1000 | Loss: 0.00061861
Iteration 8/1000 | Loss: 0.00065638
Iteration 9/1000 | Loss: 0.00083399
Iteration 10/1000 | Loss: 0.00074300
Iteration 11/1000 | Loss: 0.00040321
Iteration 12/1000 | Loss: 0.00111904
Iteration 13/1000 | Loss: 0.00029036
Iteration 14/1000 | Loss: 0.00038936
Iteration 15/1000 | Loss: 0.00086295
Iteration 16/1000 | Loss: 0.00028737
Iteration 17/1000 | Loss: 0.00061439
Iteration 18/1000 | Loss: 0.00039632
Iteration 19/1000 | Loss: 0.00046041
Iteration 20/1000 | Loss: 0.00024977
Iteration 21/1000 | Loss: 0.00080127
Iteration 22/1000 | Loss: 0.00019881
Iteration 23/1000 | Loss: 0.00022950
Iteration 24/1000 | Loss: 0.00109860
Iteration 25/1000 | Loss: 0.00020526
Iteration 26/1000 | Loss: 0.00026207
Iteration 27/1000 | Loss: 0.00020932
Iteration 28/1000 | Loss: 0.00032628
Iteration 29/1000 | Loss: 0.00034288
Iteration 30/1000 | Loss: 0.00062223
Iteration 31/1000 | Loss: 0.00076068
Iteration 32/1000 | Loss: 0.00040150
Iteration 33/1000 | Loss: 0.00015960
Iteration 34/1000 | Loss: 0.00045851
Iteration 35/1000 | Loss: 0.00089142
Iteration 36/1000 | Loss: 0.00031500
Iteration 37/1000 | Loss: 0.00015590
Iteration 38/1000 | Loss: 0.00016173
Iteration 39/1000 | Loss: 0.00033958
Iteration 40/1000 | Loss: 0.00110656
Iteration 41/1000 | Loss: 0.00024489
Iteration 42/1000 | Loss: 0.00034798
Iteration 43/1000 | Loss: 0.00022239
Iteration 44/1000 | Loss: 0.00018836
Iteration 45/1000 | Loss: 0.00022218
Iteration 46/1000 | Loss: 0.00025207
Iteration 47/1000 | Loss: 0.00014802
Iteration 48/1000 | Loss: 0.00027556
Iteration 49/1000 | Loss: 0.00013714
Iteration 50/1000 | Loss: 0.00032840
Iteration 51/1000 | Loss: 0.00014332
Iteration 52/1000 | Loss: 0.00045960
Iteration 53/1000 | Loss: 0.00026662
Iteration 54/1000 | Loss: 0.00016945
Iteration 55/1000 | Loss: 0.00031819
Iteration 56/1000 | Loss: 0.00125446
Iteration 57/1000 | Loss: 0.00020791
Iteration 58/1000 | Loss: 0.00013874
Iteration 59/1000 | Loss: 0.00023080
Iteration 60/1000 | Loss: 0.00078939
Iteration 61/1000 | Loss: 0.00136538
Iteration 62/1000 | Loss: 0.00094722
Iteration 63/1000 | Loss: 0.00152514
Iteration 64/1000 | Loss: 0.00017366
Iteration 65/1000 | Loss: 0.00047112
Iteration 66/1000 | Loss: 0.00017507
Iteration 67/1000 | Loss: 0.00015657
Iteration 68/1000 | Loss: 0.00015107
Iteration 69/1000 | Loss: 0.00024588
Iteration 70/1000 | Loss: 0.00013299
Iteration 71/1000 | Loss: 0.00016944
Iteration 72/1000 | Loss: 0.00057490
Iteration 73/1000 | Loss: 0.00014468
Iteration 74/1000 | Loss: 0.00014185
Iteration 75/1000 | Loss: 0.00015711
Iteration 76/1000 | Loss: 0.00013926
Iteration 77/1000 | Loss: 0.00014516
Iteration 78/1000 | Loss: 0.00013609
Iteration 79/1000 | Loss: 0.00015752
Iteration 80/1000 | Loss: 0.00012695
Iteration 81/1000 | Loss: 0.00014077
Iteration 82/1000 | Loss: 0.00014040
Iteration 83/1000 | Loss: 0.00013739
Iteration 84/1000 | Loss: 0.00018271
Iteration 85/1000 | Loss: 0.00014941
Iteration 86/1000 | Loss: 0.00013105
Iteration 87/1000 | Loss: 0.00012541
Iteration 88/1000 | Loss: 0.00015949
Iteration 89/1000 | Loss: 0.00040811
Iteration 90/1000 | Loss: 0.00013560
Iteration 91/1000 | Loss: 0.00012446
Iteration 92/1000 | Loss: 0.00017139
Iteration 93/1000 | Loss: 0.00015957
Iteration 94/1000 | Loss: 0.00012650
Iteration 95/1000 | Loss: 0.00012655
Iteration 96/1000 | Loss: 0.00012336
Iteration 97/1000 | Loss: 0.00012268
Iteration 98/1000 | Loss: 0.00022992
Iteration 99/1000 | Loss: 0.00038384
Iteration 100/1000 | Loss: 0.00032323
Iteration 101/1000 | Loss: 0.00012305
Iteration 102/1000 | Loss: 0.00012236
Iteration 103/1000 | Loss: 0.00013334
Iteration 104/1000 | Loss: 0.00012202
Iteration 105/1000 | Loss: 0.00017009
Iteration 106/1000 | Loss: 0.00017009
Iteration 107/1000 | Loss: 0.00056450
Iteration 108/1000 | Loss: 0.00013069
Iteration 109/1000 | Loss: 0.00013506
Iteration 110/1000 | Loss: 0.00012196
Iteration 111/1000 | Loss: 0.00012191
Iteration 112/1000 | Loss: 0.00012190
Iteration 113/1000 | Loss: 0.00014140
Iteration 114/1000 | Loss: 0.00014140
Iteration 115/1000 | Loss: 0.00025410
Iteration 116/1000 | Loss: 0.00012368
Iteration 117/1000 | Loss: 0.00012719
Iteration 118/1000 | Loss: 0.00012187
Iteration 119/1000 | Loss: 0.00012183
Iteration 120/1000 | Loss: 0.00012183
Iteration 121/1000 | Loss: 0.00012183
Iteration 122/1000 | Loss: 0.00012183
Iteration 123/1000 | Loss: 0.00012183
Iteration 124/1000 | Loss: 0.00012182
Iteration 125/1000 | Loss: 0.00012182
Iteration 126/1000 | Loss: 0.00012182
Iteration 127/1000 | Loss: 0.00012182
Iteration 128/1000 | Loss: 0.00012182
Iteration 129/1000 | Loss: 0.00012182
Iteration 130/1000 | Loss: 0.00012182
Iteration 131/1000 | Loss: 0.00012182
Iteration 132/1000 | Loss: 0.00012182
Iteration 133/1000 | Loss: 0.00012182
Iteration 134/1000 | Loss: 0.00012181
Iteration 135/1000 | Loss: 0.00012181
Iteration 136/1000 | Loss: 0.00012181
Iteration 137/1000 | Loss: 0.00012181
Iteration 138/1000 | Loss: 0.00012181
Iteration 139/1000 | Loss: 0.00012180
Iteration 140/1000 | Loss: 0.00012180
Iteration 141/1000 | Loss: 0.00012180
Iteration 142/1000 | Loss: 0.00012180
Iteration 143/1000 | Loss: 0.00012180
Iteration 144/1000 | Loss: 0.00012179
Iteration 145/1000 | Loss: 0.00012179
Iteration 146/1000 | Loss: 0.00012179
Iteration 147/1000 | Loss: 0.00012179
Iteration 148/1000 | Loss: 0.00012179
Iteration 149/1000 | Loss: 0.00012178
Iteration 150/1000 | Loss: 0.00012178
Iteration 151/1000 | Loss: 0.00012177
Iteration 152/1000 | Loss: 0.00012176
Iteration 153/1000 | Loss: 0.00012176
Iteration 154/1000 | Loss: 0.00012176
Iteration 155/1000 | Loss: 0.00012176
Iteration 156/1000 | Loss: 0.00012175
Iteration 157/1000 | Loss: 0.00012175
Iteration 158/1000 | Loss: 0.00012175
Iteration 159/1000 | Loss: 0.00012175
Iteration 160/1000 | Loss: 0.00012175
Iteration 161/1000 | Loss: 0.00012175
Iteration 162/1000 | Loss: 0.00012175
Iteration 163/1000 | Loss: 0.00012175
Iteration 164/1000 | Loss: 0.00012174
Iteration 165/1000 | Loss: 0.00012172
Iteration 166/1000 | Loss: 0.00012172
Iteration 167/1000 | Loss: 0.00012172
Iteration 168/1000 | Loss: 0.00012172
Iteration 169/1000 | Loss: 0.00012172
Iteration 170/1000 | Loss: 0.00012171
Iteration 171/1000 | Loss: 0.00012171
Iteration 172/1000 | Loss: 0.00012171
Iteration 173/1000 | Loss: 0.00012171
Iteration 174/1000 | Loss: 0.00012170
Iteration 175/1000 | Loss: 0.00012170
Iteration 176/1000 | Loss: 0.00012170
Iteration 177/1000 | Loss: 0.00012170
Iteration 178/1000 | Loss: 0.00012169
Iteration 179/1000 | Loss: 0.00012169
Iteration 180/1000 | Loss: 0.00012168
Iteration 181/1000 | Loss: 0.00012167
Iteration 182/1000 | Loss: 0.00012167
Iteration 183/1000 | Loss: 0.00012167
Iteration 184/1000 | Loss: 0.00018028
Iteration 185/1000 | Loss: 0.00016026
Iteration 186/1000 | Loss: 0.00012795
Iteration 187/1000 | Loss: 0.00013995
Iteration 188/1000 | Loss: 0.00013042
Iteration 189/1000 | Loss: 0.00012878
Iteration 190/1000 | Loss: 0.00012164
Iteration 191/1000 | Loss: 0.00012157
Iteration 192/1000 | Loss: 0.00012155
Iteration 193/1000 | Loss: 0.00012155
Iteration 194/1000 | Loss: 0.00012153
Iteration 195/1000 | Loss: 0.00012153
Iteration 196/1000 | Loss: 0.00012153
Iteration 197/1000 | Loss: 0.00012153
Iteration 198/1000 | Loss: 0.00012153
Iteration 199/1000 | Loss: 0.00012153
Iteration 200/1000 | Loss: 0.00012153
Iteration 201/1000 | Loss: 0.00012153
Iteration 202/1000 | Loss: 0.00012153
Iteration 203/1000 | Loss: 0.00012152
Iteration 204/1000 | Loss: 0.00012152
Iteration 205/1000 | Loss: 0.00012152
Iteration 206/1000 | Loss: 0.00012152
Iteration 207/1000 | Loss: 0.00012152
Iteration 208/1000 | Loss: 0.00012152
Iteration 209/1000 | Loss: 0.00012152
Iteration 210/1000 | Loss: 0.00012148
Iteration 211/1000 | Loss: 0.00012148
Iteration 212/1000 | Loss: 0.00016528
Iteration 213/1000 | Loss: 0.00026237
Iteration 214/1000 | Loss: 0.00016197
Iteration 215/1000 | Loss: 0.00012216
Iteration 216/1000 | Loss: 0.00014199
Iteration 217/1000 | Loss: 0.00012148
Iteration 218/1000 | Loss: 0.00019108
Iteration 219/1000 | Loss: 0.00013132
Iteration 220/1000 | Loss: 0.00013578
Iteration 221/1000 | Loss: 0.00012143
Iteration 222/1000 | Loss: 0.00012141
Iteration 223/1000 | Loss: 0.00012141
Iteration 224/1000 | Loss: 0.00012137
Iteration 225/1000 | Loss: 0.00012137
Iteration 226/1000 | Loss: 0.00012136
Iteration 227/1000 | Loss: 0.00012135
Iteration 228/1000 | Loss: 0.00012135
Iteration 229/1000 | Loss: 0.00012134
Iteration 230/1000 | Loss: 0.00012134
Iteration 231/1000 | Loss: 0.00012133
Iteration 232/1000 | Loss: 0.00012133
Iteration 233/1000 | Loss: 0.00012133
Iteration 234/1000 | Loss: 0.00012133
Iteration 235/1000 | Loss: 0.00012133
Iteration 236/1000 | Loss: 0.00012133
Iteration 237/1000 | Loss: 0.00012133
Iteration 238/1000 | Loss: 0.00012133
Iteration 239/1000 | Loss: 0.00012133
Iteration 240/1000 | Loss: 0.00012133
Iteration 241/1000 | Loss: 0.00012132
Iteration 242/1000 | Loss: 0.00012132
Iteration 243/1000 | Loss: 0.00012131
Iteration 244/1000 | Loss: 0.00012131
Iteration 245/1000 | Loss: 0.00012131
Iteration 246/1000 | Loss: 0.00012131
Iteration 247/1000 | Loss: 0.00012131
Iteration 248/1000 | Loss: 0.00012131
Iteration 249/1000 | Loss: 0.00012131
Iteration 250/1000 | Loss: 0.00012131
Iteration 251/1000 | Loss: 0.00012130
Iteration 252/1000 | Loss: 0.00012130
Iteration 253/1000 | Loss: 0.00012130
Iteration 254/1000 | Loss: 0.00012130
Iteration 255/1000 | Loss: 0.00012130
Iteration 256/1000 | Loss: 0.00012130
Iteration 257/1000 | Loss: 0.00012130
Iteration 258/1000 | Loss: 0.00012129
Iteration 259/1000 | Loss: 0.00012129
Iteration 260/1000 | Loss: 0.00012129
Iteration 261/1000 | Loss: 0.00012128
Iteration 262/1000 | Loss: 0.00012128
Iteration 263/1000 | Loss: 0.00012128
Iteration 264/1000 | Loss: 0.00012128
Iteration 265/1000 | Loss: 0.00012128
Iteration 266/1000 | Loss: 0.00012128
Iteration 267/1000 | Loss: 0.00012128
Iteration 268/1000 | Loss: 0.00012128
Iteration 269/1000 | Loss: 0.00012128
Iteration 270/1000 | Loss: 0.00012128
Iteration 271/1000 | Loss: 0.00012128
Iteration 272/1000 | Loss: 0.00012128
Iteration 273/1000 | Loss: 0.00012128
Iteration 274/1000 | Loss: 0.00012127
Iteration 275/1000 | Loss: 0.00012127
Iteration 276/1000 | Loss: 0.00012127
Iteration 277/1000 | Loss: 0.00012127
Iteration 278/1000 | Loss: 0.00012127
Iteration 279/1000 | Loss: 0.00012127
Iteration 280/1000 | Loss: 0.00012127
Iteration 281/1000 | Loss: 0.00012127
Iteration 282/1000 | Loss: 0.00014696
Iteration 283/1000 | Loss: 0.00012997
Iteration 284/1000 | Loss: 0.00012692
Iteration 285/1000 | Loss: 0.00012134
Iteration 286/1000 | Loss: 0.00012133
Iteration 287/1000 | Loss: 0.00012132
Iteration 288/1000 | Loss: 0.00012131
Iteration 289/1000 | Loss: 0.00012131
Iteration 290/1000 | Loss: 0.00012130
Iteration 291/1000 | Loss: 0.00012129
Iteration 292/1000 | Loss: 0.00012129
Iteration 293/1000 | Loss: 0.00012129
Iteration 294/1000 | Loss: 0.00012129
Iteration 295/1000 | Loss: 0.00012129
Iteration 296/1000 | Loss: 0.00012128
Iteration 297/1000 | Loss: 0.00012128
Iteration 298/1000 | Loss: 0.00012128
Iteration 299/1000 | Loss: 0.00012128
Iteration 300/1000 | Loss: 0.00012128
Iteration 301/1000 | Loss: 0.00012128
Iteration 302/1000 | Loss: 0.00012128
Iteration 303/1000 | Loss: 0.00012127
Iteration 304/1000 | Loss: 0.00012127
Iteration 305/1000 | Loss: 0.00013813
Iteration 306/1000 | Loss: 0.00012129
Iteration 307/1000 | Loss: 0.00012129
Iteration 308/1000 | Loss: 0.00012128
Iteration 309/1000 | Loss: 0.00012128
Iteration 310/1000 | Loss: 0.00012128
Iteration 311/1000 | Loss: 0.00012128
Iteration 312/1000 | Loss: 0.00012128
Iteration 313/1000 | Loss: 0.00012128
Iteration 314/1000 | Loss: 0.00012128
Iteration 315/1000 | Loss: 0.00012128
Iteration 316/1000 | Loss: 0.00012128
Iteration 317/1000 | Loss: 0.00012128
Iteration 318/1000 | Loss: 0.00012128
Iteration 319/1000 | Loss: 0.00012128
Iteration 320/1000 | Loss: 0.00016415
Iteration 321/1000 | Loss: 0.00016415
Iteration 322/1000 | Loss: 0.00012464
Iteration 323/1000 | Loss: 0.00012128
Iteration 324/1000 | Loss: 0.00012127
Iteration 325/1000 | Loss: 0.00012127
Iteration 326/1000 | Loss: 0.00012127
Iteration 327/1000 | Loss: 0.00012127
Iteration 328/1000 | Loss: 0.00012127
Iteration 329/1000 | Loss: 0.00012127
Iteration 330/1000 | Loss: 0.00012126
Iteration 331/1000 | Loss: 0.00012126
Iteration 332/1000 | Loss: 0.00012126
Iteration 333/1000 | Loss: 0.00012126
Iteration 334/1000 | Loss: 0.00012126
Iteration 335/1000 | Loss: 0.00012126
Iteration 336/1000 | Loss: 0.00012126
Iteration 337/1000 | Loss: 0.00012126
Iteration 338/1000 | Loss: 0.00012126
Iteration 339/1000 | Loss: 0.00012126
Iteration 340/1000 | Loss: 0.00013871
Iteration 341/1000 | Loss: 0.00012584
Iteration 342/1000 | Loss: 0.00012126
Iteration 343/1000 | Loss: 0.00012126
Iteration 344/1000 | Loss: 0.00012126
Iteration 345/1000 | Loss: 0.00012126
Iteration 346/1000 | Loss: 0.00012126
Iteration 347/1000 | Loss: 0.00012126
Iteration 348/1000 | Loss: 0.00012126
Iteration 349/1000 | Loss: 0.00012126
Iteration 350/1000 | Loss: 0.00012126
Iteration 351/1000 | Loss: 0.00012126
Iteration 352/1000 | Loss: 0.00012126
Iteration 353/1000 | Loss: 0.00012126
Iteration 354/1000 | Loss: 0.00012126
Iteration 355/1000 | Loss: 0.00012126
Iteration 356/1000 | Loss: 0.00012126
Iteration 357/1000 | Loss: 0.00012126
Iteration 358/1000 | Loss: 0.00012125
Iteration 359/1000 | Loss: 0.00012125
Iteration 360/1000 | Loss: 0.00012125
Iteration 361/1000 | Loss: 0.00012125
Iteration 362/1000 | Loss: 0.00012125
Iteration 363/1000 | Loss: 0.00012125
Iteration 364/1000 | Loss: 0.00012125
Iteration 365/1000 | Loss: 0.00012125
Iteration 366/1000 | Loss: 0.00012125
Iteration 367/1000 | Loss: 0.00012125
Iteration 368/1000 | Loss: 0.00012125
Iteration 369/1000 | Loss: 0.00012125
Iteration 370/1000 | Loss: 0.00012125
Iteration 371/1000 | Loss: 0.00012125
Iteration 372/1000 | Loss: 0.00012125
Iteration 373/1000 | Loss: 0.00012125
Iteration 374/1000 | Loss: 0.00012125
Iteration 375/1000 | Loss: 0.00012125
Iteration 376/1000 | Loss: 0.00012125
Iteration 377/1000 | Loss: 0.00012125
Iteration 378/1000 | Loss: 0.00012125
Iteration 379/1000 | Loss: 0.00012125
Iteration 380/1000 | Loss: 0.00012125
Iteration 381/1000 | Loss: 0.00012125
Iteration 382/1000 | Loss: 0.00012125
Iteration 383/1000 | Loss: 0.00012125
Iteration 384/1000 | Loss: 0.00012125
Iteration 385/1000 | Loss: 0.00012125
Iteration 386/1000 | Loss: 0.00012125
Iteration 387/1000 | Loss: 0.00012125
Iteration 388/1000 | Loss: 0.00012125
Iteration 389/1000 | Loss: 0.00012125
Iteration 390/1000 | Loss: 0.00012125
Iteration 391/1000 | Loss: 0.00012125
Iteration 392/1000 | Loss: 0.00012125
Iteration 393/1000 | Loss: 0.00012125
Iteration 394/1000 | Loss: 0.00012125
Iteration 395/1000 | Loss: 0.00012125
Iteration 396/1000 | Loss: 0.00012125
Iteration 397/1000 | Loss: 0.00012125
Iteration 398/1000 | Loss: 0.00012125
Iteration 399/1000 | Loss: 0.00012125
Iteration 400/1000 | Loss: 0.00012125
Iteration 401/1000 | Loss: 0.00012125
Iteration 402/1000 | Loss: 0.00012125
Iteration 403/1000 | Loss: 0.00012125
Iteration 404/1000 | Loss: 0.00012125
Iteration 405/1000 | Loss: 0.00012125
Iteration 406/1000 | Loss: 0.00012125
Iteration 407/1000 | Loss: 0.00012125
Iteration 408/1000 | Loss: 0.00012125
Iteration 409/1000 | Loss: 0.00012125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 409. Stopping optimization.
Last 5 losses: [0.00012124799832236022, 0.00012124799832236022, 0.00012124799832236022, 0.00012124799832236022, 0.00012124799832236022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012124799832236022

Optimization complete. Final v2v error: 5.885756015777588 mm

Highest mean error: 12.069872856140137 mm for frame 137

Lowest mean error: 3.707425832748413 mm for frame 19

Saving results

Total time: 293.18469882011414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418421
Iteration 2/25 | Loss: 0.00099667
Iteration 3/25 | Loss: 0.00080040
Iteration 4/25 | Loss: 0.00077339
Iteration 5/25 | Loss: 0.00076764
Iteration 6/25 | Loss: 0.00076616
Iteration 7/25 | Loss: 0.00076562
Iteration 8/25 | Loss: 0.00076560
Iteration 9/25 | Loss: 0.00076560
Iteration 10/25 | Loss: 0.00076560
Iteration 11/25 | Loss: 0.00076560
Iteration 12/25 | Loss: 0.00076560
Iteration 13/25 | Loss: 0.00076560
Iteration 14/25 | Loss: 0.00076560
Iteration 15/25 | Loss: 0.00076560
Iteration 16/25 | Loss: 0.00076560
Iteration 17/25 | Loss: 0.00076560
Iteration 18/25 | Loss: 0.00076560
Iteration 19/25 | Loss: 0.00076560
Iteration 20/25 | Loss: 0.00076560
Iteration 21/25 | Loss: 0.00076560
Iteration 22/25 | Loss: 0.00076559
Iteration 23/25 | Loss: 0.00076559
Iteration 24/25 | Loss: 0.00076559
Iteration 25/25 | Loss: 0.00076559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56076097
Iteration 2/25 | Loss: 0.00061130
Iteration 3/25 | Loss: 0.00061129
Iteration 4/25 | Loss: 0.00061129
Iteration 5/25 | Loss: 0.00061129
Iteration 6/25 | Loss: 0.00061129
Iteration 7/25 | Loss: 0.00061129
Iteration 8/25 | Loss: 0.00061129
Iteration 9/25 | Loss: 0.00061129
Iteration 10/25 | Loss: 0.00061129
Iteration 11/25 | Loss: 0.00061129
Iteration 12/25 | Loss: 0.00061129
Iteration 13/25 | Loss: 0.00061129
Iteration 14/25 | Loss: 0.00061129
Iteration 15/25 | Loss: 0.00061129
Iteration 16/25 | Loss: 0.00061129
Iteration 17/25 | Loss: 0.00061129
Iteration 18/25 | Loss: 0.00061129
Iteration 19/25 | Loss: 0.00061129
Iteration 20/25 | Loss: 0.00061129
Iteration 21/25 | Loss: 0.00061129
Iteration 22/25 | Loss: 0.00061129
Iteration 23/25 | Loss: 0.00061129
Iteration 24/25 | Loss: 0.00061129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006112864357419312, 0.0006112864357419312, 0.0006112864357419312, 0.0006112864357419312, 0.0006112864357419312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006112864357419312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061129
Iteration 2/1000 | Loss: 0.00003799
Iteration 3/1000 | Loss: 0.00002242
Iteration 4/1000 | Loss: 0.00001715
Iteration 5/1000 | Loss: 0.00001569
Iteration 6/1000 | Loss: 0.00001487
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001407
Iteration 10/1000 | Loss: 0.00001395
Iteration 11/1000 | Loss: 0.00001392
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001389
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001385
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001384
Iteration 23/1000 | Loss: 0.00001384
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001374
Iteration 31/1000 | Loss: 0.00001374
Iteration 32/1000 | Loss: 0.00001374
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001374
Iteration 35/1000 | Loss: 0.00001374
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001373
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001371
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001370
Iteration 48/1000 | Loss: 0.00001370
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001369
Iteration 53/1000 | Loss: 0.00001369
Iteration 54/1000 | Loss: 0.00001369
Iteration 55/1000 | Loss: 0.00001369
Iteration 56/1000 | Loss: 0.00001369
Iteration 57/1000 | Loss: 0.00001368
Iteration 58/1000 | Loss: 0.00001368
Iteration 59/1000 | Loss: 0.00001368
Iteration 60/1000 | Loss: 0.00001368
Iteration 61/1000 | Loss: 0.00001368
Iteration 62/1000 | Loss: 0.00001368
Iteration 63/1000 | Loss: 0.00001368
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001367
Iteration 66/1000 | Loss: 0.00001367
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001366
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001363
Iteration 94/1000 | Loss: 0.00001363
Iteration 95/1000 | Loss: 0.00001363
Iteration 96/1000 | Loss: 0.00001363
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001362
Iteration 100/1000 | Loss: 0.00001362
Iteration 101/1000 | Loss: 0.00001362
Iteration 102/1000 | Loss: 0.00001362
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001361
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001361
Iteration 107/1000 | Loss: 0.00001361
Iteration 108/1000 | Loss: 0.00001361
Iteration 109/1000 | Loss: 0.00001361
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001360
Iteration 113/1000 | Loss: 0.00001360
Iteration 114/1000 | Loss: 0.00001360
Iteration 115/1000 | Loss: 0.00001360
Iteration 116/1000 | Loss: 0.00001360
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001359
Iteration 124/1000 | Loss: 0.00001359
Iteration 125/1000 | Loss: 0.00001359
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001359
Iteration 128/1000 | Loss: 0.00001359
Iteration 129/1000 | Loss: 0.00001359
Iteration 130/1000 | Loss: 0.00001359
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001358
Iteration 136/1000 | Loss: 0.00001358
Iteration 137/1000 | Loss: 0.00001358
Iteration 138/1000 | Loss: 0.00001358
Iteration 139/1000 | Loss: 0.00001358
Iteration 140/1000 | Loss: 0.00001358
Iteration 141/1000 | Loss: 0.00001358
Iteration 142/1000 | Loss: 0.00001358
Iteration 143/1000 | Loss: 0.00001358
Iteration 144/1000 | Loss: 0.00001358
Iteration 145/1000 | Loss: 0.00001358
Iteration 146/1000 | Loss: 0.00001358
Iteration 147/1000 | Loss: 0.00001358
Iteration 148/1000 | Loss: 0.00001358
Iteration 149/1000 | Loss: 0.00001358
Iteration 150/1000 | Loss: 0.00001358
Iteration 151/1000 | Loss: 0.00001358
Iteration 152/1000 | Loss: 0.00001358
Iteration 153/1000 | Loss: 0.00001358
Iteration 154/1000 | Loss: 0.00001358
Iteration 155/1000 | Loss: 0.00001358
Iteration 156/1000 | Loss: 0.00001358
Iteration 157/1000 | Loss: 0.00001358
Iteration 158/1000 | Loss: 0.00001358
Iteration 159/1000 | Loss: 0.00001358
Iteration 160/1000 | Loss: 0.00001358
Iteration 161/1000 | Loss: 0.00001358
Iteration 162/1000 | Loss: 0.00001358
Iteration 163/1000 | Loss: 0.00001358
Iteration 164/1000 | Loss: 0.00001358
Iteration 165/1000 | Loss: 0.00001358
Iteration 166/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.3583312465925701e-05, 1.3583312465925701e-05, 1.3583312465925701e-05, 1.3583312465925701e-05, 1.3583312465925701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3583312465925701e-05

Optimization complete. Final v2v error: 2.972949743270874 mm

Highest mean error: 4.491771221160889 mm for frame 44

Lowest mean error: 2.5396673679351807 mm for frame 5

Saving results

Total time: 36.15777039527893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372192
Iteration 2/25 | Loss: 0.00081139
Iteration 3/25 | Loss: 0.00072212
Iteration 4/25 | Loss: 0.00070853
Iteration 5/25 | Loss: 0.00070370
Iteration 6/25 | Loss: 0.00070235
Iteration 7/25 | Loss: 0.00070229
Iteration 8/25 | Loss: 0.00070229
Iteration 9/25 | Loss: 0.00070229
Iteration 10/25 | Loss: 0.00070229
Iteration 11/25 | Loss: 0.00070229
Iteration 12/25 | Loss: 0.00070229
Iteration 13/25 | Loss: 0.00070229
Iteration 14/25 | Loss: 0.00070229
Iteration 15/25 | Loss: 0.00070229
Iteration 16/25 | Loss: 0.00070229
Iteration 17/25 | Loss: 0.00070229
Iteration 18/25 | Loss: 0.00070229
Iteration 19/25 | Loss: 0.00070229
Iteration 20/25 | Loss: 0.00070229
Iteration 21/25 | Loss: 0.00070229
Iteration 22/25 | Loss: 0.00070229
Iteration 23/25 | Loss: 0.00070229
Iteration 24/25 | Loss: 0.00070229
Iteration 25/25 | Loss: 0.00070229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94250131
Iteration 2/25 | Loss: 0.00048063
Iteration 3/25 | Loss: 0.00048062
Iteration 4/25 | Loss: 0.00048062
Iteration 5/25 | Loss: 0.00048062
Iteration 6/25 | Loss: 0.00048062
Iteration 7/25 | Loss: 0.00048062
Iteration 8/25 | Loss: 0.00048062
Iteration 9/25 | Loss: 0.00048062
Iteration 10/25 | Loss: 0.00048062
Iteration 11/25 | Loss: 0.00048062
Iteration 12/25 | Loss: 0.00048062
Iteration 13/25 | Loss: 0.00048062
Iteration 14/25 | Loss: 0.00048062
Iteration 15/25 | Loss: 0.00048062
Iteration 16/25 | Loss: 0.00048062
Iteration 17/25 | Loss: 0.00048062
Iteration 18/25 | Loss: 0.00048062
Iteration 19/25 | Loss: 0.00048062
Iteration 20/25 | Loss: 0.00048062
Iteration 21/25 | Loss: 0.00048062
Iteration 22/25 | Loss: 0.00048062
Iteration 23/25 | Loss: 0.00048062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004806199576705694, 0.0004806199576705694, 0.0004806199576705694, 0.0004806199576705694, 0.0004806199576705694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004806199576705694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048062
Iteration 2/1000 | Loss: 0.00001811
Iteration 3/1000 | Loss: 0.00001279
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001136
Iteration 6/1000 | Loss: 0.00001106
Iteration 7/1000 | Loss: 0.00001104
Iteration 8/1000 | Loss: 0.00001098
Iteration 9/1000 | Loss: 0.00001095
Iteration 10/1000 | Loss: 0.00001093
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001086
Iteration 13/1000 | Loss: 0.00001085
Iteration 14/1000 | Loss: 0.00001085
Iteration 15/1000 | Loss: 0.00001070
Iteration 16/1000 | Loss: 0.00001068
Iteration 17/1000 | Loss: 0.00001068
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001067
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001057
Iteration 26/1000 | Loss: 0.00001057
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001053
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001051
Iteration 36/1000 | Loss: 0.00001051
Iteration 37/1000 | Loss: 0.00001051
Iteration 38/1000 | Loss: 0.00001050
Iteration 39/1000 | Loss: 0.00001050
Iteration 40/1000 | Loss: 0.00001049
Iteration 41/1000 | Loss: 0.00001048
Iteration 42/1000 | Loss: 0.00001047
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001044
Iteration 45/1000 | Loss: 0.00001044
Iteration 46/1000 | Loss: 0.00001044
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001044
Iteration 49/1000 | Loss: 0.00001044
Iteration 50/1000 | Loss: 0.00001044
Iteration 51/1000 | Loss: 0.00001044
Iteration 52/1000 | Loss: 0.00001044
Iteration 53/1000 | Loss: 0.00001043
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001040
Iteration 60/1000 | Loss: 0.00001039
Iteration 61/1000 | Loss: 0.00001039
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001036
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001036
Iteration 74/1000 | Loss: 0.00001036
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001034
Iteration 87/1000 | Loss: 0.00001034
Iteration 88/1000 | Loss: 0.00001033
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001031
Iteration 92/1000 | Loss: 0.00001030
Iteration 93/1000 | Loss: 0.00001030
Iteration 94/1000 | Loss: 0.00001030
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001027
Iteration 101/1000 | Loss: 0.00001025
Iteration 102/1000 | Loss: 0.00001025
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001023
Iteration 106/1000 | Loss: 0.00001023
Iteration 107/1000 | Loss: 0.00001023
Iteration 108/1000 | Loss: 0.00001023
Iteration 109/1000 | Loss: 0.00001023
Iteration 110/1000 | Loss: 0.00001023
Iteration 111/1000 | Loss: 0.00001022
Iteration 112/1000 | Loss: 0.00001022
Iteration 113/1000 | Loss: 0.00001022
Iteration 114/1000 | Loss: 0.00001022
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001021
Iteration 119/1000 | Loss: 0.00001021
Iteration 120/1000 | Loss: 0.00001021
Iteration 121/1000 | Loss: 0.00001021
Iteration 122/1000 | Loss: 0.00001021
Iteration 123/1000 | Loss: 0.00001021
Iteration 124/1000 | Loss: 0.00001021
Iteration 125/1000 | Loss: 0.00001021
Iteration 126/1000 | Loss: 0.00001021
Iteration 127/1000 | Loss: 0.00001021
Iteration 128/1000 | Loss: 0.00001021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.0209655556536745e-05, 1.0209655556536745e-05, 1.0209655556536745e-05, 1.0209655556536745e-05, 1.0209655556536745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0209655556536745e-05

Optimization complete. Final v2v error: 2.737003803253174 mm

Highest mean error: 2.891862154006958 mm for frame 130

Lowest mean error: 2.6456496715545654 mm for frame 109

Saving results

Total time: 33.70479226112366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020973
Iteration 2/25 | Loss: 0.01020972
Iteration 3/25 | Loss: 0.01020972
Iteration 4/25 | Loss: 0.01020972
Iteration 5/25 | Loss: 0.00478423
Iteration 6/25 | Loss: 0.00325748
Iteration 7/25 | Loss: 0.00263054
Iteration 8/25 | Loss: 0.00225414
Iteration 9/25 | Loss: 0.00204513
Iteration 10/25 | Loss: 0.00197139
Iteration 11/25 | Loss: 0.00188874
Iteration 12/25 | Loss: 0.00179421
Iteration 13/25 | Loss: 0.00171560
Iteration 14/25 | Loss: 0.00172964
Iteration 15/25 | Loss: 0.00163735
Iteration 16/25 | Loss: 0.00149846
Iteration 17/25 | Loss: 0.00137437
Iteration 18/25 | Loss: 0.00131276
Iteration 19/25 | Loss: 0.00125991
Iteration 20/25 | Loss: 0.00125618
Iteration 21/25 | Loss: 0.00122597
Iteration 22/25 | Loss: 0.00121748
Iteration 23/25 | Loss: 0.00120198
Iteration 24/25 | Loss: 0.00120221
Iteration 25/25 | Loss: 0.00119739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47834647
Iteration 2/25 | Loss: 0.00484214
Iteration 3/25 | Loss: 0.00429935
Iteration 4/25 | Loss: 0.00429935
Iteration 5/25 | Loss: 0.00429935
Iteration 6/25 | Loss: 0.00429935
Iteration 7/25 | Loss: 0.00429935
Iteration 8/25 | Loss: 0.00429935
Iteration 9/25 | Loss: 0.00429935
Iteration 10/25 | Loss: 0.00429935
Iteration 11/25 | Loss: 0.00429935
Iteration 12/25 | Loss: 0.00429935
Iteration 13/25 | Loss: 0.00429935
Iteration 14/25 | Loss: 0.00429935
Iteration 15/25 | Loss: 0.00429935
Iteration 16/25 | Loss: 0.00429935
Iteration 17/25 | Loss: 0.00429935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004299350082874298, 0.004299350082874298, 0.004299350082874298, 0.004299350082874298, 0.004299350082874298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004299350082874298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00429935
Iteration 2/1000 | Loss: 0.00539837
Iteration 3/1000 | Loss: 0.00131340
Iteration 4/1000 | Loss: 0.00132780
Iteration 5/1000 | Loss: 0.00054521
Iteration 6/1000 | Loss: 0.00058962
Iteration 7/1000 | Loss: 0.00041227
Iteration 8/1000 | Loss: 0.00149149
Iteration 9/1000 | Loss: 0.00057977
Iteration 10/1000 | Loss: 0.00165250
Iteration 11/1000 | Loss: 0.00093406
Iteration 12/1000 | Loss: 0.00040152
Iteration 13/1000 | Loss: 0.00031773
Iteration 14/1000 | Loss: 0.00020299
Iteration 15/1000 | Loss: 0.00058204
Iteration 16/1000 | Loss: 0.00037240
Iteration 17/1000 | Loss: 0.00036971
Iteration 18/1000 | Loss: 0.00033814
Iteration 19/1000 | Loss: 0.00014462
Iteration 20/1000 | Loss: 0.00015123
Iteration 21/1000 | Loss: 0.00031893
Iteration 22/1000 | Loss: 0.00040919
Iteration 23/1000 | Loss: 0.00033074
Iteration 24/1000 | Loss: 0.00054534
Iteration 25/1000 | Loss: 0.00036403
Iteration 26/1000 | Loss: 0.00010834
Iteration 27/1000 | Loss: 0.00020916
Iteration 28/1000 | Loss: 0.00048858
Iteration 29/1000 | Loss: 0.00020909
Iteration 30/1000 | Loss: 0.00038613
Iteration 31/1000 | Loss: 0.00011806
Iteration 32/1000 | Loss: 0.00018674
Iteration 33/1000 | Loss: 0.00033929
Iteration 34/1000 | Loss: 0.00072520
Iteration 35/1000 | Loss: 0.00146299
Iteration 36/1000 | Loss: 0.00010196
Iteration 37/1000 | Loss: 0.00010951
Iteration 38/1000 | Loss: 0.00008843
Iteration 39/1000 | Loss: 0.00021081
Iteration 40/1000 | Loss: 0.00009578
Iteration 41/1000 | Loss: 0.00027245
Iteration 42/1000 | Loss: 0.00008820
Iteration 43/1000 | Loss: 0.00014590
Iteration 44/1000 | Loss: 0.00007832
Iteration 45/1000 | Loss: 0.00028044
Iteration 46/1000 | Loss: 0.00025347
Iteration 47/1000 | Loss: 0.00100725
Iteration 48/1000 | Loss: 0.00018257
Iteration 49/1000 | Loss: 0.00044859
Iteration 50/1000 | Loss: 0.00164064
Iteration 51/1000 | Loss: 0.00062394
Iteration 52/1000 | Loss: 0.00114428
Iteration 53/1000 | Loss: 0.00081169
Iteration 54/1000 | Loss: 0.00012767
Iteration 55/1000 | Loss: 0.00013991
Iteration 56/1000 | Loss: 0.00007987
Iteration 57/1000 | Loss: 0.00013888
Iteration 58/1000 | Loss: 0.00006399
Iteration 59/1000 | Loss: 0.00023062
Iteration 60/1000 | Loss: 0.00028711
Iteration 61/1000 | Loss: 0.00071509
Iteration 62/1000 | Loss: 0.00008720
Iteration 63/1000 | Loss: 0.00004964
Iteration 64/1000 | Loss: 0.00015623
Iteration 65/1000 | Loss: 0.00041236
Iteration 66/1000 | Loss: 0.00064268
Iteration 67/1000 | Loss: 0.00068518
Iteration 68/1000 | Loss: 0.00010697
Iteration 69/1000 | Loss: 0.00004719
Iteration 70/1000 | Loss: 0.00004281
Iteration 71/1000 | Loss: 0.00008841
Iteration 72/1000 | Loss: 0.00029642
Iteration 73/1000 | Loss: 0.00008975
Iteration 74/1000 | Loss: 0.00010733
Iteration 75/1000 | Loss: 0.00004072
Iteration 76/1000 | Loss: 0.00019517
Iteration 77/1000 | Loss: 0.00007007
Iteration 78/1000 | Loss: 0.00003769
Iteration 79/1000 | Loss: 0.00045975
Iteration 80/1000 | Loss: 0.00019747
Iteration 81/1000 | Loss: 0.00013640
Iteration 82/1000 | Loss: 0.00011205
Iteration 83/1000 | Loss: 0.00007120
Iteration 84/1000 | Loss: 0.00003913
Iteration 85/1000 | Loss: 0.00027981
Iteration 86/1000 | Loss: 0.00003366
Iteration 87/1000 | Loss: 0.00023330
Iteration 88/1000 | Loss: 0.00018966
Iteration 89/1000 | Loss: 0.00011335
Iteration 90/1000 | Loss: 0.00003357
Iteration 91/1000 | Loss: 0.00012271
Iteration 92/1000 | Loss: 0.00003114
Iteration 93/1000 | Loss: 0.00003309
Iteration 94/1000 | Loss: 0.00002933
Iteration 95/1000 | Loss: 0.00024811
Iteration 96/1000 | Loss: 0.00069781
Iteration 97/1000 | Loss: 0.00011411
Iteration 98/1000 | Loss: 0.00006626
Iteration 99/1000 | Loss: 0.00024204
Iteration 100/1000 | Loss: 0.00007448
Iteration 101/1000 | Loss: 0.00002930
Iteration 102/1000 | Loss: 0.00009513
Iteration 103/1000 | Loss: 0.00019276
Iteration 104/1000 | Loss: 0.00002577
Iteration 105/1000 | Loss: 0.00002464
Iteration 106/1000 | Loss: 0.00002385
Iteration 107/1000 | Loss: 0.00024350
Iteration 108/1000 | Loss: 0.00020264
Iteration 109/1000 | Loss: 0.00054810
Iteration 110/1000 | Loss: 0.00018060
Iteration 111/1000 | Loss: 0.00005608
Iteration 112/1000 | Loss: 0.00002834
Iteration 113/1000 | Loss: 0.00002390
Iteration 114/1000 | Loss: 0.00004639
Iteration 115/1000 | Loss: 0.00002709
Iteration 116/1000 | Loss: 0.00002496
Iteration 117/1000 | Loss: 0.00006712
Iteration 118/1000 | Loss: 0.00002612
Iteration 119/1000 | Loss: 0.00002564
Iteration 120/1000 | Loss: 0.00002200
Iteration 121/1000 | Loss: 0.00002200
Iteration 122/1000 | Loss: 0.00002200
Iteration 123/1000 | Loss: 0.00002199
Iteration 124/1000 | Loss: 0.00002196
Iteration 125/1000 | Loss: 0.00002192
Iteration 126/1000 | Loss: 0.00002191
Iteration 127/1000 | Loss: 0.00002186
Iteration 128/1000 | Loss: 0.00002184
Iteration 129/1000 | Loss: 0.00002184
Iteration 130/1000 | Loss: 0.00002184
Iteration 131/1000 | Loss: 0.00002183
Iteration 132/1000 | Loss: 0.00002183
Iteration 133/1000 | Loss: 0.00002182
Iteration 134/1000 | Loss: 0.00002182
Iteration 135/1000 | Loss: 0.00002182
Iteration 136/1000 | Loss: 0.00002182
Iteration 137/1000 | Loss: 0.00002182
Iteration 138/1000 | Loss: 0.00002181
Iteration 139/1000 | Loss: 0.00002181
Iteration 140/1000 | Loss: 0.00002180
Iteration 141/1000 | Loss: 0.00002180
Iteration 142/1000 | Loss: 0.00002180
Iteration 143/1000 | Loss: 0.00002180
Iteration 144/1000 | Loss: 0.00002180
Iteration 145/1000 | Loss: 0.00002180
Iteration 146/1000 | Loss: 0.00002180
Iteration 147/1000 | Loss: 0.00002180
Iteration 148/1000 | Loss: 0.00002180
Iteration 149/1000 | Loss: 0.00002180
Iteration 150/1000 | Loss: 0.00002180
Iteration 151/1000 | Loss: 0.00002180
Iteration 152/1000 | Loss: 0.00002180
Iteration 153/1000 | Loss: 0.00002180
Iteration 154/1000 | Loss: 0.00002180
Iteration 155/1000 | Loss: 0.00002180
Iteration 156/1000 | Loss: 0.00002180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.1799702153657563e-05, 2.1799702153657563e-05, 2.1799702153657563e-05, 2.1799702153657563e-05, 2.1799702153657563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1799702153657563e-05

Optimization complete. Final v2v error: 3.537837505340576 mm

Highest mean error: 11.682204246520996 mm for frame 124

Lowest mean error: 3.1286706924438477 mm for frame 13

Saving results

Total time: 246.39003324508667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049259
Iteration 2/25 | Loss: 0.00176818
Iteration 3/25 | Loss: 0.00115119
Iteration 4/25 | Loss: 0.00107337
Iteration 5/25 | Loss: 0.00100462
Iteration 6/25 | Loss: 0.00096287
Iteration 7/25 | Loss: 0.00095729
Iteration 8/25 | Loss: 0.00094135
Iteration 9/25 | Loss: 0.00094151
Iteration 10/25 | Loss: 0.00093549
Iteration 11/25 | Loss: 0.00093038
Iteration 12/25 | Loss: 0.00094241
Iteration 13/25 | Loss: 0.00092930
Iteration 14/25 | Loss: 0.00094196
Iteration 15/25 | Loss: 0.00093731
Iteration 16/25 | Loss: 0.00093435
Iteration 17/25 | Loss: 0.00093323
Iteration 18/25 | Loss: 0.00093067
Iteration 19/25 | Loss: 0.00092599
Iteration 20/25 | Loss: 0.00093019
Iteration 21/25 | Loss: 0.00092375
Iteration 22/25 | Loss: 0.00093316
Iteration 23/25 | Loss: 0.00092462
Iteration 24/25 | Loss: 0.00092992
Iteration 25/25 | Loss: 0.00092518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.94618797
Iteration 2/25 | Loss: 0.00404457
Iteration 3/25 | Loss: 0.00225182
Iteration 4/25 | Loss: 0.00225181
Iteration 5/25 | Loss: 0.00225181
Iteration 6/25 | Loss: 0.00225181
Iteration 7/25 | Loss: 0.00225181
Iteration 8/25 | Loss: 0.00225181
Iteration 9/25 | Loss: 0.00225181
Iteration 10/25 | Loss: 0.00225181
Iteration 11/25 | Loss: 0.00225181
Iteration 12/25 | Loss: 0.00225181
Iteration 13/25 | Loss: 0.00225181
Iteration 14/25 | Loss: 0.00225181
Iteration 15/25 | Loss: 0.00225181
Iteration 16/25 | Loss: 0.00225181
Iteration 17/25 | Loss: 0.00225181
Iteration 18/25 | Loss: 0.00225181
Iteration 19/25 | Loss: 0.00225181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0022518096957355738, 0.0022518096957355738, 0.0022518096957355738, 0.0022518096957355738, 0.0022518096957355738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022518096957355738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225181
Iteration 2/1000 | Loss: 0.00150277
Iteration 3/1000 | Loss: 0.00133804
Iteration 4/1000 | Loss: 0.00056011
Iteration 5/1000 | Loss: 0.01106521
Iteration 6/1000 | Loss: 0.00345275
Iteration 7/1000 | Loss: 0.00299871
Iteration 8/1000 | Loss: 0.00137037
Iteration 9/1000 | Loss: 0.00368663
Iteration 10/1000 | Loss: 0.00125357
Iteration 11/1000 | Loss: 0.00012970
Iteration 12/1000 | Loss: 0.00059445
Iteration 13/1000 | Loss: 0.00052556
Iteration 14/1000 | Loss: 0.00067291
Iteration 15/1000 | Loss: 0.00544941
Iteration 16/1000 | Loss: 0.00019240
Iteration 17/1000 | Loss: 0.00089780
Iteration 18/1000 | Loss: 0.00023637
Iteration 19/1000 | Loss: 0.00007453
Iteration 20/1000 | Loss: 0.00005656
Iteration 21/1000 | Loss: 0.00004801
Iteration 22/1000 | Loss: 0.00413191
Iteration 23/1000 | Loss: 0.00004635
Iteration 24/1000 | Loss: 0.00003763
Iteration 25/1000 | Loss: 0.00003392
Iteration 26/1000 | Loss: 0.00003196
Iteration 27/1000 | Loss: 0.00003109
Iteration 28/1000 | Loss: 0.00003056
Iteration 29/1000 | Loss: 0.00003024
Iteration 30/1000 | Loss: 0.00002979
Iteration 31/1000 | Loss: 0.00002951
Iteration 32/1000 | Loss: 0.00002935
Iteration 33/1000 | Loss: 0.00002934
Iteration 34/1000 | Loss: 0.00002927
Iteration 35/1000 | Loss: 0.00002927
Iteration 36/1000 | Loss: 0.00002923
Iteration 37/1000 | Loss: 0.00002920
Iteration 38/1000 | Loss: 0.00002919
Iteration 39/1000 | Loss: 0.00002919
Iteration 40/1000 | Loss: 0.00002917
Iteration 41/1000 | Loss: 0.00002915
Iteration 42/1000 | Loss: 0.00002915
Iteration 43/1000 | Loss: 0.00002914
Iteration 44/1000 | Loss: 0.00002914
Iteration 45/1000 | Loss: 0.00002912
Iteration 46/1000 | Loss: 0.00002912
Iteration 47/1000 | Loss: 0.00002912
Iteration 48/1000 | Loss: 0.00002912
Iteration 49/1000 | Loss: 0.00002911
Iteration 50/1000 | Loss: 0.00002911
Iteration 51/1000 | Loss: 0.00002911
Iteration 52/1000 | Loss: 0.00002910
Iteration 53/1000 | Loss: 0.00002910
Iteration 54/1000 | Loss: 0.00002908
Iteration 55/1000 | Loss: 0.00002907
Iteration 56/1000 | Loss: 0.00002907
Iteration 57/1000 | Loss: 0.00002906
Iteration 58/1000 | Loss: 0.00002906
Iteration 59/1000 | Loss: 0.00002899
Iteration 60/1000 | Loss: 0.00002892
Iteration 61/1000 | Loss: 0.00002892
Iteration 62/1000 | Loss: 0.00002891
Iteration 63/1000 | Loss: 0.00002891
Iteration 64/1000 | Loss: 0.00002891
Iteration 65/1000 | Loss: 0.00002890
Iteration 66/1000 | Loss: 0.00002890
Iteration 67/1000 | Loss: 0.00002890
Iteration 68/1000 | Loss: 0.00002889
Iteration 69/1000 | Loss: 0.00002889
Iteration 70/1000 | Loss: 0.00002889
Iteration 71/1000 | Loss: 0.00002889
Iteration 72/1000 | Loss: 0.00002888
Iteration 73/1000 | Loss: 0.00002888
Iteration 74/1000 | Loss: 0.00002886
Iteration 75/1000 | Loss: 0.00002886
Iteration 76/1000 | Loss: 0.00002885
Iteration 77/1000 | Loss: 0.00002885
Iteration 78/1000 | Loss: 0.00002884
Iteration 79/1000 | Loss: 0.00002884
Iteration 80/1000 | Loss: 0.00002884
Iteration 81/1000 | Loss: 0.00002883
Iteration 82/1000 | Loss: 0.00002883
Iteration 83/1000 | Loss: 0.00002883
Iteration 84/1000 | Loss: 0.00002882
Iteration 85/1000 | Loss: 0.00002882
Iteration 86/1000 | Loss: 0.00002882
Iteration 87/1000 | Loss: 0.00002881
Iteration 88/1000 | Loss: 0.00002881
Iteration 89/1000 | Loss: 0.00002880
Iteration 90/1000 | Loss: 0.00002879
Iteration 91/1000 | Loss: 0.00002879
Iteration 92/1000 | Loss: 0.00002878
Iteration 93/1000 | Loss: 0.00002878
Iteration 94/1000 | Loss: 0.00002878
Iteration 95/1000 | Loss: 0.00002878
Iteration 96/1000 | Loss: 0.00002877
Iteration 97/1000 | Loss: 0.00002877
Iteration 98/1000 | Loss: 0.00002877
Iteration 99/1000 | Loss: 0.00002876
Iteration 100/1000 | Loss: 0.00002876
Iteration 101/1000 | Loss: 0.00002876
Iteration 102/1000 | Loss: 0.00002875
Iteration 103/1000 | Loss: 0.00002875
Iteration 104/1000 | Loss: 0.00002874
Iteration 105/1000 | Loss: 0.00002874
Iteration 106/1000 | Loss: 0.00002874
Iteration 107/1000 | Loss: 0.00002873
Iteration 108/1000 | Loss: 0.00002873
Iteration 109/1000 | Loss: 0.00002872
Iteration 110/1000 | Loss: 0.00002872
Iteration 111/1000 | Loss: 0.00002872
Iteration 112/1000 | Loss: 0.00002872
Iteration 113/1000 | Loss: 0.00002872
Iteration 114/1000 | Loss: 0.00002872
Iteration 115/1000 | Loss: 0.00002872
Iteration 116/1000 | Loss: 0.00002871
Iteration 117/1000 | Loss: 0.00002871
Iteration 118/1000 | Loss: 0.00002871
Iteration 119/1000 | Loss: 0.00002871
Iteration 120/1000 | Loss: 0.00002871
Iteration 121/1000 | Loss: 0.00002871
Iteration 122/1000 | Loss: 0.00002870
Iteration 123/1000 | Loss: 0.00002870
Iteration 124/1000 | Loss: 0.00002870
Iteration 125/1000 | Loss: 0.00002870
Iteration 126/1000 | Loss: 0.00002870
Iteration 127/1000 | Loss: 0.00002870
Iteration 128/1000 | Loss: 0.00002870
Iteration 129/1000 | Loss: 0.00002869
Iteration 130/1000 | Loss: 0.00002869
Iteration 131/1000 | Loss: 0.00002869
Iteration 132/1000 | Loss: 0.00002869
Iteration 133/1000 | Loss: 0.00002869
Iteration 134/1000 | Loss: 0.00002868
Iteration 135/1000 | Loss: 0.00002868
Iteration 136/1000 | Loss: 0.00002868
Iteration 137/1000 | Loss: 0.00002868
Iteration 138/1000 | Loss: 0.00002867
Iteration 139/1000 | Loss: 0.00002867
Iteration 140/1000 | Loss: 0.00002866
Iteration 141/1000 | Loss: 0.00002865
Iteration 142/1000 | Loss: 0.00002864
Iteration 143/1000 | Loss: 0.00002864
Iteration 144/1000 | Loss: 0.00002863
Iteration 145/1000 | Loss: 0.00002863
Iteration 146/1000 | Loss: 0.00002863
Iteration 147/1000 | Loss: 0.00002863
Iteration 148/1000 | Loss: 0.00002863
Iteration 149/1000 | Loss: 0.00002863
Iteration 150/1000 | Loss: 0.00002863
Iteration 151/1000 | Loss: 0.00002862
Iteration 152/1000 | Loss: 0.00002862
Iteration 153/1000 | Loss: 0.00002862
Iteration 154/1000 | Loss: 0.00002862
Iteration 155/1000 | Loss: 0.00002862
Iteration 156/1000 | Loss: 0.00002862
Iteration 157/1000 | Loss: 0.00002862
Iteration 158/1000 | Loss: 0.00002862
Iteration 159/1000 | Loss: 0.00002861
Iteration 160/1000 | Loss: 0.00002861
Iteration 161/1000 | Loss: 0.00002861
Iteration 162/1000 | Loss: 0.00002861
Iteration 163/1000 | Loss: 0.00002861
Iteration 164/1000 | Loss: 0.00002861
Iteration 165/1000 | Loss: 0.00002861
Iteration 166/1000 | Loss: 0.00002860
Iteration 167/1000 | Loss: 0.00002860
Iteration 168/1000 | Loss: 0.00002860
Iteration 169/1000 | Loss: 0.00002860
Iteration 170/1000 | Loss: 0.00002860
Iteration 171/1000 | Loss: 0.00002859
Iteration 172/1000 | Loss: 0.00002859
Iteration 173/1000 | Loss: 0.00002859
Iteration 174/1000 | Loss: 0.00002859
Iteration 175/1000 | Loss: 0.00002859
Iteration 176/1000 | Loss: 0.00002859
Iteration 177/1000 | Loss: 0.00002859
Iteration 178/1000 | Loss: 0.00002859
Iteration 179/1000 | Loss: 0.00002859
Iteration 180/1000 | Loss: 0.00002859
Iteration 181/1000 | Loss: 0.00002859
Iteration 182/1000 | Loss: 0.00002859
Iteration 183/1000 | Loss: 0.00002859
Iteration 184/1000 | Loss: 0.00002859
Iteration 185/1000 | Loss: 0.00002859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.8589473004103638e-05, 2.8589473004103638e-05, 2.8589473004103638e-05, 2.8589473004103638e-05, 2.8589473004103638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8589473004103638e-05

Optimization complete. Final v2v error: 4.3900909423828125 mm

Highest mean error: 6.276546478271484 mm for frame 124

Lowest mean error: 3.16432523727417 mm for frame 98

Saving results

Total time: 114.32477474212646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985762
Iteration 2/25 | Loss: 0.00214355
Iteration 3/25 | Loss: 0.00143099
Iteration 4/25 | Loss: 0.00130237
Iteration 5/25 | Loss: 0.00117633
Iteration 6/25 | Loss: 0.00116071
Iteration 7/25 | Loss: 0.00117690
Iteration 8/25 | Loss: 0.00110180
Iteration 9/25 | Loss: 0.00102546
Iteration 10/25 | Loss: 0.00097217
Iteration 11/25 | Loss: 0.00095759
Iteration 12/25 | Loss: 0.00097788
Iteration 13/25 | Loss: 0.00098368
Iteration 14/25 | Loss: 0.00096791
Iteration 15/25 | Loss: 0.00095218
Iteration 16/25 | Loss: 0.00094394
Iteration 17/25 | Loss: 0.00093936
Iteration 18/25 | Loss: 0.00094117
Iteration 19/25 | Loss: 0.00093504
Iteration 20/25 | Loss: 0.00093348
Iteration 21/25 | Loss: 0.00093919
Iteration 22/25 | Loss: 0.00097451
Iteration 23/25 | Loss: 0.00096825
Iteration 24/25 | Loss: 0.00096232
Iteration 25/25 | Loss: 0.00096047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56696439
Iteration 2/25 | Loss: 0.00192982
Iteration 3/25 | Loss: 0.00191962
Iteration 4/25 | Loss: 0.00191961
Iteration 5/25 | Loss: 0.00191961
Iteration 6/25 | Loss: 0.00191961
Iteration 7/25 | Loss: 0.00191961
Iteration 8/25 | Loss: 0.00191961
Iteration 9/25 | Loss: 0.00191961
Iteration 10/25 | Loss: 0.00191961
Iteration 11/25 | Loss: 0.00191961
Iteration 12/25 | Loss: 0.00191961
Iteration 13/25 | Loss: 0.00191961
Iteration 14/25 | Loss: 0.00191961
Iteration 15/25 | Loss: 0.00191961
Iteration 16/25 | Loss: 0.00191961
Iteration 17/25 | Loss: 0.00191961
Iteration 18/25 | Loss: 0.00191961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019196101929992437, 0.0019196101929992437, 0.0019196101929992437, 0.0019196101929992437, 0.0019196101929992437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019196101929992437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191961
Iteration 2/1000 | Loss: 0.00017304
Iteration 3/1000 | Loss: 0.00012370
Iteration 4/1000 | Loss: 0.00039727
Iteration 5/1000 | Loss: 0.00008620
Iteration 6/1000 | Loss: 0.00007365
Iteration 7/1000 | Loss: 0.00091854
Iteration 8/1000 | Loss: 0.00062156
Iteration 9/1000 | Loss: 0.00031229
Iteration 10/1000 | Loss: 0.00006811
Iteration 11/1000 | Loss: 0.00005658
Iteration 12/1000 | Loss: 0.00005195
Iteration 13/1000 | Loss: 0.00004771
Iteration 14/1000 | Loss: 0.00004484
Iteration 15/1000 | Loss: 0.00004317
Iteration 16/1000 | Loss: 0.00004194
Iteration 17/1000 | Loss: 0.00004063
Iteration 18/1000 | Loss: 0.00003972
Iteration 19/1000 | Loss: 0.00003899
Iteration 20/1000 | Loss: 0.00003810
Iteration 21/1000 | Loss: 0.00166777
Iteration 22/1000 | Loss: 0.00005794
Iteration 23/1000 | Loss: 0.00004238
Iteration 24/1000 | Loss: 0.00009758
Iteration 25/1000 | Loss: 0.00004289
Iteration 26/1000 | Loss: 0.00003474
Iteration 27/1000 | Loss: 0.00003259
Iteration 28/1000 | Loss: 0.00003163
Iteration 29/1000 | Loss: 0.00003120
Iteration 30/1000 | Loss: 0.00004196
Iteration 31/1000 | Loss: 0.00003062
Iteration 32/1000 | Loss: 0.00003470
Iteration 33/1000 | Loss: 0.00003022
Iteration 34/1000 | Loss: 0.00003001
Iteration 35/1000 | Loss: 0.00002999
Iteration 36/1000 | Loss: 0.00002998
Iteration 37/1000 | Loss: 0.00002998
Iteration 38/1000 | Loss: 0.00002997
Iteration 39/1000 | Loss: 0.00003284
Iteration 40/1000 | Loss: 0.00002980
Iteration 41/1000 | Loss: 0.00002980
Iteration 42/1000 | Loss: 0.00002980
Iteration 43/1000 | Loss: 0.00002980
Iteration 44/1000 | Loss: 0.00002980
Iteration 45/1000 | Loss: 0.00002980
Iteration 46/1000 | Loss: 0.00002980
Iteration 47/1000 | Loss: 0.00002980
Iteration 48/1000 | Loss: 0.00002980
Iteration 49/1000 | Loss: 0.00002978
Iteration 50/1000 | Loss: 0.00002978
Iteration 51/1000 | Loss: 0.00002976
Iteration 52/1000 | Loss: 0.00002975
Iteration 53/1000 | Loss: 0.00002974
Iteration 54/1000 | Loss: 0.00002973
Iteration 55/1000 | Loss: 0.00002972
Iteration 56/1000 | Loss: 0.00002971
Iteration 57/1000 | Loss: 0.00002971
Iteration 58/1000 | Loss: 0.00002970
Iteration 59/1000 | Loss: 0.00002970
Iteration 60/1000 | Loss: 0.00002970
Iteration 61/1000 | Loss: 0.00002970
Iteration 62/1000 | Loss: 0.00002970
Iteration 63/1000 | Loss: 0.00002969
Iteration 64/1000 | Loss: 0.00002969
Iteration 65/1000 | Loss: 0.00002969
Iteration 66/1000 | Loss: 0.00002969
Iteration 67/1000 | Loss: 0.00002969
Iteration 68/1000 | Loss: 0.00002969
Iteration 69/1000 | Loss: 0.00002968
Iteration 70/1000 | Loss: 0.00002968
Iteration 71/1000 | Loss: 0.00002968
Iteration 72/1000 | Loss: 0.00002968
Iteration 73/1000 | Loss: 0.00002968
Iteration 74/1000 | Loss: 0.00002968
Iteration 75/1000 | Loss: 0.00002968
Iteration 76/1000 | Loss: 0.00002968
Iteration 77/1000 | Loss: 0.00002967
Iteration 78/1000 | Loss: 0.00002967
Iteration 79/1000 | Loss: 0.00002967
Iteration 80/1000 | Loss: 0.00002967
Iteration 81/1000 | Loss: 0.00002966
Iteration 82/1000 | Loss: 0.00002966
Iteration 83/1000 | Loss: 0.00002966
Iteration 84/1000 | Loss: 0.00002966
Iteration 85/1000 | Loss: 0.00002966
Iteration 86/1000 | Loss: 0.00002966
Iteration 87/1000 | Loss: 0.00002965
Iteration 88/1000 | Loss: 0.00002965
Iteration 89/1000 | Loss: 0.00002965
Iteration 90/1000 | Loss: 0.00002965
Iteration 91/1000 | Loss: 0.00002964
Iteration 92/1000 | Loss: 0.00002964
Iteration 93/1000 | Loss: 0.00002964
Iteration 94/1000 | Loss: 0.00002964
Iteration 95/1000 | Loss: 0.00002964
Iteration 96/1000 | Loss: 0.00002964
Iteration 97/1000 | Loss: 0.00002964
Iteration 98/1000 | Loss: 0.00002964
Iteration 99/1000 | Loss: 0.00002963
Iteration 100/1000 | Loss: 0.00002963
Iteration 101/1000 | Loss: 0.00002963
Iteration 102/1000 | Loss: 0.00002963
Iteration 103/1000 | Loss: 0.00002963
Iteration 104/1000 | Loss: 0.00002963
Iteration 105/1000 | Loss: 0.00002963
Iteration 106/1000 | Loss: 0.00002963
Iteration 107/1000 | Loss: 0.00002963
Iteration 108/1000 | Loss: 0.00002963
Iteration 109/1000 | Loss: 0.00002963
Iteration 110/1000 | Loss: 0.00002963
Iteration 111/1000 | Loss: 0.00002963
Iteration 112/1000 | Loss: 0.00002962
Iteration 113/1000 | Loss: 0.00002962
Iteration 114/1000 | Loss: 0.00002962
Iteration 115/1000 | Loss: 0.00002962
Iteration 116/1000 | Loss: 0.00002962
Iteration 117/1000 | Loss: 0.00002962
Iteration 118/1000 | Loss: 0.00002962
Iteration 119/1000 | Loss: 0.00002962
Iteration 120/1000 | Loss: 0.00002962
Iteration 121/1000 | Loss: 0.00002962
Iteration 122/1000 | Loss: 0.00002962
Iteration 123/1000 | Loss: 0.00002961
Iteration 124/1000 | Loss: 0.00002961
Iteration 125/1000 | Loss: 0.00002961
Iteration 126/1000 | Loss: 0.00002961
Iteration 127/1000 | Loss: 0.00002961
Iteration 128/1000 | Loss: 0.00002961
Iteration 129/1000 | Loss: 0.00002961
Iteration 130/1000 | Loss: 0.00002961
Iteration 131/1000 | Loss: 0.00002961
Iteration 132/1000 | Loss: 0.00002961
Iteration 133/1000 | Loss: 0.00002961
Iteration 134/1000 | Loss: 0.00002961
Iteration 135/1000 | Loss: 0.00002960
Iteration 136/1000 | Loss: 0.00002960
Iteration 137/1000 | Loss: 0.00002960
Iteration 138/1000 | Loss: 0.00002960
Iteration 139/1000 | Loss: 0.00002959
Iteration 140/1000 | Loss: 0.00002959
Iteration 141/1000 | Loss: 0.00002958
Iteration 142/1000 | Loss: 0.00002958
Iteration 143/1000 | Loss: 0.00002958
Iteration 144/1000 | Loss: 0.00002958
Iteration 145/1000 | Loss: 0.00002958
Iteration 146/1000 | Loss: 0.00002958
Iteration 147/1000 | Loss: 0.00002957
Iteration 148/1000 | Loss: 0.00002957
Iteration 149/1000 | Loss: 0.00002957
Iteration 150/1000 | Loss: 0.00002957
Iteration 151/1000 | Loss: 0.00002956
Iteration 152/1000 | Loss: 0.00002956
Iteration 153/1000 | Loss: 0.00002956
Iteration 154/1000 | Loss: 0.00002956
Iteration 155/1000 | Loss: 0.00002956
Iteration 156/1000 | Loss: 0.00002956
Iteration 157/1000 | Loss: 0.00002956
Iteration 158/1000 | Loss: 0.00002956
Iteration 159/1000 | Loss: 0.00002956
Iteration 160/1000 | Loss: 0.00002956
Iteration 161/1000 | Loss: 0.00002956
Iteration 162/1000 | Loss: 0.00002955
Iteration 163/1000 | Loss: 0.00002955
Iteration 164/1000 | Loss: 0.00002955
Iteration 165/1000 | Loss: 0.00002955
Iteration 166/1000 | Loss: 0.00002955
Iteration 167/1000 | Loss: 0.00002955
Iteration 168/1000 | Loss: 0.00002955
Iteration 169/1000 | Loss: 0.00002955
Iteration 170/1000 | Loss: 0.00002955
Iteration 171/1000 | Loss: 0.00002955
Iteration 172/1000 | Loss: 0.00002955
Iteration 173/1000 | Loss: 0.00002955
Iteration 174/1000 | Loss: 0.00002955
Iteration 175/1000 | Loss: 0.00002955
Iteration 176/1000 | Loss: 0.00002955
Iteration 177/1000 | Loss: 0.00002955
Iteration 178/1000 | Loss: 0.00002955
Iteration 179/1000 | Loss: 0.00002955
Iteration 180/1000 | Loss: 0.00002955
Iteration 181/1000 | Loss: 0.00002955
Iteration 182/1000 | Loss: 0.00002955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.9551991246989928e-05, 2.9551991246989928e-05, 2.9551991246989928e-05, 2.9551991246989928e-05, 2.9551991246989928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9551991246989928e-05

Optimization complete. Final v2v error: 4.380037784576416 mm

Highest mean error: 5.779582977294922 mm for frame 85

Lowest mean error: 3.1389122009277344 mm for frame 10

Saving results

Total time: 108.72305369377136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736741
Iteration 2/25 | Loss: 0.00163904
Iteration 3/25 | Loss: 0.00105505
Iteration 4/25 | Loss: 0.00090567
Iteration 5/25 | Loss: 0.00089306
Iteration 6/25 | Loss: 0.00087041
Iteration 7/25 | Loss: 0.00086071
Iteration 8/25 | Loss: 0.00085906
Iteration 9/25 | Loss: 0.00085839
Iteration 10/25 | Loss: 0.00086681
Iteration 11/25 | Loss: 0.00086744
Iteration 12/25 | Loss: 0.00085483
Iteration 13/25 | Loss: 0.00085629
Iteration 14/25 | Loss: 0.00085579
Iteration 15/25 | Loss: 0.00085486
Iteration 16/25 | Loss: 0.00085506
Iteration 17/25 | Loss: 0.00085736
Iteration 18/25 | Loss: 0.00085502
Iteration 19/25 | Loss: 0.00084944
Iteration 20/25 | Loss: 0.00084897
Iteration 21/25 | Loss: 0.00084893
Iteration 22/25 | Loss: 0.00084893
Iteration 23/25 | Loss: 0.00084893
Iteration 24/25 | Loss: 0.00084893
Iteration 25/25 | Loss: 0.00084893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63430166
Iteration 2/25 | Loss: 0.00067460
Iteration 3/25 | Loss: 0.00067452
Iteration 4/25 | Loss: 0.00067452
Iteration 5/25 | Loss: 0.00067452
Iteration 6/25 | Loss: 0.00067452
Iteration 7/25 | Loss: 0.00067452
Iteration 8/25 | Loss: 0.00067452
Iteration 9/25 | Loss: 0.00067452
Iteration 10/25 | Loss: 0.00067452
Iteration 11/25 | Loss: 0.00067452
Iteration 12/25 | Loss: 0.00067452
Iteration 13/25 | Loss: 0.00067452
Iteration 14/25 | Loss: 0.00067452
Iteration 15/25 | Loss: 0.00067452
Iteration 16/25 | Loss: 0.00067452
Iteration 17/25 | Loss: 0.00067452
Iteration 18/25 | Loss: 0.00067452
Iteration 19/25 | Loss: 0.00067452
Iteration 20/25 | Loss: 0.00067452
Iteration 21/25 | Loss: 0.00067452
Iteration 22/25 | Loss: 0.00067452
Iteration 23/25 | Loss: 0.00067452
Iteration 24/25 | Loss: 0.00067452
Iteration 25/25 | Loss: 0.00067452
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006745164864696562, 0.0006745164864696562, 0.0006745164864696562, 0.0006745164864696562, 0.0006745164864696562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006745164864696562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067452
Iteration 2/1000 | Loss: 0.00003609
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002216
Iteration 5/1000 | Loss: 0.00002113
Iteration 6/1000 | Loss: 0.00002041
Iteration 7/1000 | Loss: 0.00001998
Iteration 8/1000 | Loss: 0.00001963
Iteration 9/1000 | Loss: 0.00001938
Iteration 10/1000 | Loss: 0.00001928
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001888
Iteration 15/1000 | Loss: 0.00001887
Iteration 16/1000 | Loss: 0.00001882
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001878
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001870
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001869
Iteration 49/1000 | Loss: 0.00001869
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001868
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001868
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001868
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001866
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001864
Iteration 83/1000 | Loss: 0.00001864
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001863
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001863
Iteration 93/1000 | Loss: 0.00001863
Iteration 94/1000 | Loss: 0.00001862
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001862
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001860
Iteration 104/1000 | Loss: 0.00001860
Iteration 105/1000 | Loss: 0.00001860
Iteration 106/1000 | Loss: 0.00001860
Iteration 107/1000 | Loss: 0.00001860
Iteration 108/1000 | Loss: 0.00001860
Iteration 109/1000 | Loss: 0.00001859
Iteration 110/1000 | Loss: 0.00001859
Iteration 111/1000 | Loss: 0.00001859
Iteration 112/1000 | Loss: 0.00001859
Iteration 113/1000 | Loss: 0.00001858
Iteration 114/1000 | Loss: 0.00001858
Iteration 115/1000 | Loss: 0.00001858
Iteration 116/1000 | Loss: 0.00001858
Iteration 117/1000 | Loss: 0.00001858
Iteration 118/1000 | Loss: 0.00001858
Iteration 119/1000 | Loss: 0.00001858
Iteration 120/1000 | Loss: 0.00001858
Iteration 121/1000 | Loss: 0.00001858
Iteration 122/1000 | Loss: 0.00001858
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001857
Iteration 127/1000 | Loss: 0.00001857
Iteration 128/1000 | Loss: 0.00001857
Iteration 129/1000 | Loss: 0.00001857
Iteration 130/1000 | Loss: 0.00001857
Iteration 131/1000 | Loss: 0.00001857
Iteration 132/1000 | Loss: 0.00001857
Iteration 133/1000 | Loss: 0.00001857
Iteration 134/1000 | Loss: 0.00001857
Iteration 135/1000 | Loss: 0.00001857
Iteration 136/1000 | Loss: 0.00001857
Iteration 137/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.8570426618680358e-05, 1.8570426618680358e-05, 1.8570426618680358e-05, 1.8570426618680358e-05, 1.8570426618680358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8570426618680358e-05

Optimization complete. Final v2v error: 3.575035333633423 mm

Highest mean error: 4.504856109619141 mm for frame 115

Lowest mean error: 2.905543565750122 mm for frame 235

Saving results

Total time: 73.87151002883911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990532
Iteration 2/25 | Loss: 0.00238303
Iteration 3/25 | Loss: 0.00114956
Iteration 4/25 | Loss: 0.00102384
Iteration 5/25 | Loss: 0.00090126
Iteration 6/25 | Loss: 0.00087017
Iteration 7/25 | Loss: 0.00087491
Iteration 8/25 | Loss: 0.00082060
Iteration 9/25 | Loss: 0.00081766
Iteration 10/25 | Loss: 0.00080986
Iteration 11/25 | Loss: 0.00080384
Iteration 12/25 | Loss: 0.00079749
Iteration 13/25 | Loss: 0.00079504
Iteration 14/25 | Loss: 0.00079762
Iteration 15/25 | Loss: 0.00079732
Iteration 16/25 | Loss: 0.00079223
Iteration 17/25 | Loss: 0.00079071
Iteration 18/25 | Loss: 0.00079215
Iteration 19/25 | Loss: 0.00078931
Iteration 20/25 | Loss: 0.00079161
Iteration 21/25 | Loss: 0.00078926
Iteration 22/25 | Loss: 0.00078921
Iteration 23/25 | Loss: 0.00078921
Iteration 24/25 | Loss: 0.00078921
Iteration 25/25 | Loss: 0.00078921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99330521
Iteration 2/25 | Loss: 0.00045520
Iteration 3/25 | Loss: 0.00045520
Iteration 4/25 | Loss: 0.00045520
Iteration 5/25 | Loss: 0.00045520
Iteration 6/25 | Loss: 0.00045520
Iteration 7/25 | Loss: 0.00045520
Iteration 8/25 | Loss: 0.00045520
Iteration 9/25 | Loss: 0.00045520
Iteration 10/25 | Loss: 0.00045520
Iteration 11/25 | Loss: 0.00045520
Iteration 12/25 | Loss: 0.00045520
Iteration 13/25 | Loss: 0.00045520
Iteration 14/25 | Loss: 0.00045520
Iteration 15/25 | Loss: 0.00045520
Iteration 16/25 | Loss: 0.00045520
Iteration 17/25 | Loss: 0.00045520
Iteration 18/25 | Loss: 0.00045520
Iteration 19/25 | Loss: 0.00045520
Iteration 20/25 | Loss: 0.00045520
Iteration 21/25 | Loss: 0.00045520
Iteration 22/25 | Loss: 0.00045520
Iteration 23/25 | Loss: 0.00045520
Iteration 24/25 | Loss: 0.00045520
Iteration 25/25 | Loss: 0.00045520

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045520
Iteration 2/1000 | Loss: 0.00003397
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002232
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002053
Iteration 7/1000 | Loss: 0.00001993
Iteration 8/1000 | Loss: 0.00001934
Iteration 9/1000 | Loss: 0.00006914
Iteration 10/1000 | Loss: 0.00002440
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00003851
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001835
Iteration 15/1000 | Loss: 0.00001834
Iteration 16/1000 | Loss: 0.00001821
Iteration 17/1000 | Loss: 0.00001819
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001815
Iteration 20/1000 | Loss: 0.00001815
Iteration 21/1000 | Loss: 0.00001814
Iteration 22/1000 | Loss: 0.00001814
Iteration 23/1000 | Loss: 0.00001810
Iteration 24/1000 | Loss: 0.00001806
Iteration 25/1000 | Loss: 0.00001806
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001805
Iteration 28/1000 | Loss: 0.00001805
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001803
Iteration 33/1000 | Loss: 0.00001803
Iteration 34/1000 | Loss: 0.00001802
Iteration 35/1000 | Loss: 0.00001802
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001801
Iteration 38/1000 | Loss: 0.00001801
Iteration 39/1000 | Loss: 0.00001800
Iteration 40/1000 | Loss: 0.00001800
Iteration 41/1000 | Loss: 0.00001800
Iteration 42/1000 | Loss: 0.00001800
Iteration 43/1000 | Loss: 0.00001799
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001799
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001798
Iteration 51/1000 | Loss: 0.00001798
Iteration 52/1000 | Loss: 0.00001798
Iteration 53/1000 | Loss: 0.00001798
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00007463
Iteration 58/1000 | Loss: 0.00001951
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001795
Iteration 61/1000 | Loss: 0.00001792
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001792
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001792
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001790
Iteration 78/1000 | Loss: 0.00001790
Iteration 79/1000 | Loss: 0.00001790
Iteration 80/1000 | Loss: 0.00001790
Iteration 81/1000 | Loss: 0.00001790
Iteration 82/1000 | Loss: 0.00001790
Iteration 83/1000 | Loss: 0.00001790
Iteration 84/1000 | Loss: 0.00001790
Iteration 85/1000 | Loss: 0.00001790
Iteration 86/1000 | Loss: 0.00001790
Iteration 87/1000 | Loss: 0.00001790
Iteration 88/1000 | Loss: 0.00001790
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001789
Iteration 92/1000 | Loss: 0.00001789
Iteration 93/1000 | Loss: 0.00001789
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001789
Iteration 97/1000 | Loss: 0.00001789
Iteration 98/1000 | Loss: 0.00001789
Iteration 99/1000 | Loss: 0.00007441
Iteration 100/1000 | Loss: 0.00001800
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001793
Iteration 103/1000 | Loss: 0.00001792
Iteration 104/1000 | Loss: 0.00001791
Iteration 105/1000 | Loss: 0.00001791
Iteration 106/1000 | Loss: 0.00001790
Iteration 107/1000 | Loss: 0.00001790
Iteration 108/1000 | Loss: 0.00001790
Iteration 109/1000 | Loss: 0.00001790
Iteration 110/1000 | Loss: 0.00001790
Iteration 111/1000 | Loss: 0.00001790
Iteration 112/1000 | Loss: 0.00001789
Iteration 113/1000 | Loss: 0.00001789
Iteration 114/1000 | Loss: 0.00001789
Iteration 115/1000 | Loss: 0.00001789
Iteration 116/1000 | Loss: 0.00001789
Iteration 117/1000 | Loss: 0.00001789
Iteration 118/1000 | Loss: 0.00001788
Iteration 119/1000 | Loss: 0.00001788
Iteration 120/1000 | Loss: 0.00001788
Iteration 121/1000 | Loss: 0.00001788
Iteration 122/1000 | Loss: 0.00001788
Iteration 123/1000 | Loss: 0.00001788
Iteration 124/1000 | Loss: 0.00001788
Iteration 125/1000 | Loss: 0.00001787
Iteration 126/1000 | Loss: 0.00001787
Iteration 127/1000 | Loss: 0.00001787
Iteration 128/1000 | Loss: 0.00001787
Iteration 129/1000 | Loss: 0.00001787
Iteration 130/1000 | Loss: 0.00001787
Iteration 131/1000 | Loss: 0.00001787
Iteration 132/1000 | Loss: 0.00001787
Iteration 133/1000 | Loss: 0.00001787
Iteration 134/1000 | Loss: 0.00001787
Iteration 135/1000 | Loss: 0.00001787
Iteration 136/1000 | Loss: 0.00004539
Iteration 137/1000 | Loss: 0.00001832
Iteration 138/1000 | Loss: 0.00002027
Iteration 139/1000 | Loss: 0.00002135
Iteration 140/1000 | Loss: 0.00002771
Iteration 141/1000 | Loss: 0.00002098
Iteration 142/1000 | Loss: 0.00002198
Iteration 143/1000 | Loss: 0.00001789
Iteration 144/1000 | Loss: 0.00001789
Iteration 145/1000 | Loss: 0.00001789
Iteration 146/1000 | Loss: 0.00001789
Iteration 147/1000 | Loss: 0.00001789
Iteration 148/1000 | Loss: 0.00001789
Iteration 149/1000 | Loss: 0.00001789
Iteration 150/1000 | Loss: 0.00001789
Iteration 151/1000 | Loss: 0.00001973
Iteration 152/1000 | Loss: 0.00001842
Iteration 153/1000 | Loss: 0.00001791
Iteration 154/1000 | Loss: 0.00001791
Iteration 155/1000 | Loss: 0.00001791
Iteration 156/1000 | Loss: 0.00001791
Iteration 157/1000 | Loss: 0.00001790
Iteration 158/1000 | Loss: 0.00001790
Iteration 159/1000 | Loss: 0.00001790
Iteration 160/1000 | Loss: 0.00001790
Iteration 161/1000 | Loss: 0.00001790
Iteration 162/1000 | Loss: 0.00001790
Iteration 163/1000 | Loss: 0.00001790
Iteration 164/1000 | Loss: 0.00001790
Iteration 165/1000 | Loss: 0.00001790
Iteration 166/1000 | Loss: 0.00001790
Iteration 167/1000 | Loss: 0.00001790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.790179157978855e-05, 1.790179157978855e-05, 1.790179157978855e-05, 1.790179157978855e-05, 1.790179157978855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.790179157978855e-05

Optimization complete. Final v2v error: 3.625145435333252 mm

Highest mean error: 4.82566499710083 mm for frame 152

Lowest mean error: 3.1536271572113037 mm for frame 99

Saving results

Total time: 96.82479786872864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774344
Iteration 2/25 | Loss: 0.00187222
Iteration 3/25 | Loss: 0.00100117
Iteration 4/25 | Loss: 0.00091448
Iteration 5/25 | Loss: 0.00083563
Iteration 6/25 | Loss: 0.00081266
Iteration 7/25 | Loss: 0.00078508
Iteration 8/25 | Loss: 0.00078750
Iteration 9/25 | Loss: 0.00078591
Iteration 10/25 | Loss: 0.00080244
Iteration 11/25 | Loss: 0.00079703
Iteration 12/25 | Loss: 0.00079773
Iteration 13/25 | Loss: 0.00080690
Iteration 14/25 | Loss: 0.00079342
Iteration 15/25 | Loss: 0.00078876
Iteration 16/25 | Loss: 0.00078054
Iteration 17/25 | Loss: 0.00077863
Iteration 18/25 | Loss: 0.00077520
Iteration 19/25 | Loss: 0.00076883
Iteration 20/25 | Loss: 0.00076673
Iteration 21/25 | Loss: 0.00076674
Iteration 22/25 | Loss: 0.00076514
Iteration 23/25 | Loss: 0.00076487
Iteration 24/25 | Loss: 0.00076398
Iteration 25/25 | Loss: 0.00076387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31550026
Iteration 2/25 | Loss: 0.00051023
Iteration 3/25 | Loss: 0.00047376
Iteration 4/25 | Loss: 0.00047375
Iteration 5/25 | Loss: 0.00047375
Iteration 6/25 | Loss: 0.00047375
Iteration 7/25 | Loss: 0.00047375
Iteration 8/25 | Loss: 0.00047375
Iteration 9/25 | Loss: 0.00047375
Iteration 10/25 | Loss: 0.00047375
Iteration 11/25 | Loss: 0.00047375
Iteration 12/25 | Loss: 0.00047375
Iteration 13/25 | Loss: 0.00047375
Iteration 14/25 | Loss: 0.00047375
Iteration 15/25 | Loss: 0.00047375
Iteration 16/25 | Loss: 0.00047375
Iteration 17/25 | Loss: 0.00047375
Iteration 18/25 | Loss: 0.00047375
Iteration 19/25 | Loss: 0.00047375
Iteration 20/25 | Loss: 0.00047375
Iteration 21/25 | Loss: 0.00047375
Iteration 22/25 | Loss: 0.00047375
Iteration 23/25 | Loss: 0.00047375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004737515700981021, 0.0004737515700981021, 0.0004737515700981021, 0.0004737515700981021, 0.0004737515700981021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004737515700981021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047375
Iteration 2/1000 | Loss: 0.00003144
Iteration 3/1000 | Loss: 0.00007150
Iteration 4/1000 | Loss: 0.00004348
Iteration 5/1000 | Loss: 0.00003300
Iteration 6/1000 | Loss: 0.00002987
Iteration 7/1000 | Loss: 0.00002934
Iteration 8/1000 | Loss: 0.00003050
Iteration 9/1000 | Loss: 0.00011661
Iteration 10/1000 | Loss: 0.00003531
Iteration 11/1000 | Loss: 0.00003541
Iteration 12/1000 | Loss: 0.00006357
Iteration 13/1000 | Loss: 0.00003925
Iteration 14/1000 | Loss: 0.00002147
Iteration 15/1000 | Loss: 0.00003978
Iteration 16/1000 | Loss: 0.00021292
Iteration 17/1000 | Loss: 0.00004898
Iteration 18/1000 | Loss: 0.00002597
Iteration 19/1000 | Loss: 0.00002147
Iteration 20/1000 | Loss: 0.00002015
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001830
Iteration 23/1000 | Loss: 0.00001775
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001772
Iteration 27/1000 | Loss: 0.00001908
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001990
Iteration 30/1000 | Loss: 0.00001876
Iteration 31/1000 | Loss: 0.00007169
Iteration 32/1000 | Loss: 0.00001651
Iteration 33/1000 | Loss: 0.00001629
Iteration 34/1000 | Loss: 0.00001622
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001621
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001607
Iteration 41/1000 | Loss: 0.00001597
Iteration 42/1000 | Loss: 0.00002000
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001542
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001536
Iteration 71/1000 | Loss: 0.00001536
Iteration 72/1000 | Loss: 0.00001536
Iteration 73/1000 | Loss: 0.00001536
Iteration 74/1000 | Loss: 0.00001536
Iteration 75/1000 | Loss: 0.00001536
Iteration 76/1000 | Loss: 0.00001536
Iteration 77/1000 | Loss: 0.00001536
Iteration 78/1000 | Loss: 0.00001536
Iteration 79/1000 | Loss: 0.00001536
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001536
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00001536
Iteration 87/1000 | Loss: 0.00001536
Iteration 88/1000 | Loss: 0.00001536
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.535997580504045e-05, 1.535997580504045e-05, 1.535997580504045e-05, 1.535997580504045e-05, 1.535997580504045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.535997580504045e-05

Optimization complete. Final v2v error: 3.313539981842041 mm

Highest mean error: 9.19965934753418 mm for frame 189

Lowest mean error: 2.998199701309204 mm for frame 198

Saving results

Total time: 122.6388795375824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897723
Iteration 2/25 | Loss: 0.00149420
Iteration 3/25 | Loss: 0.00110976
Iteration 4/25 | Loss: 0.00101956
Iteration 5/25 | Loss: 0.00098433
Iteration 6/25 | Loss: 0.00097476
Iteration 7/25 | Loss: 0.00098941
Iteration 8/25 | Loss: 0.00097741
Iteration 9/25 | Loss: 0.00095932
Iteration 10/25 | Loss: 0.00094348
Iteration 11/25 | Loss: 0.00096031
Iteration 12/25 | Loss: 0.00094439
Iteration 13/25 | Loss: 0.00092451
Iteration 14/25 | Loss: 0.00090260
Iteration 15/25 | Loss: 0.00090428
Iteration 16/25 | Loss: 0.00089640
Iteration 17/25 | Loss: 0.00088142
Iteration 18/25 | Loss: 0.00089084
Iteration 19/25 | Loss: 0.00088465
Iteration 20/25 | Loss: 0.00088090
Iteration 21/25 | Loss: 0.00087825
Iteration 22/25 | Loss: 0.00089127
Iteration 23/25 | Loss: 0.00089969
Iteration 24/25 | Loss: 0.00089618
Iteration 25/25 | Loss: 0.00088473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51396263
Iteration 2/25 | Loss: 0.00130492
Iteration 3/25 | Loss: 0.00130492
Iteration 4/25 | Loss: 0.00130492
Iteration 5/25 | Loss: 0.00130492
Iteration 6/25 | Loss: 0.00130492
Iteration 7/25 | Loss: 0.00130492
Iteration 8/25 | Loss: 0.00130492
Iteration 9/25 | Loss: 0.00130492
Iteration 10/25 | Loss: 0.00130492
Iteration 11/25 | Loss: 0.00130492
Iteration 12/25 | Loss: 0.00130492
Iteration 13/25 | Loss: 0.00130492
Iteration 14/25 | Loss: 0.00130492
Iteration 15/25 | Loss: 0.00130492
Iteration 16/25 | Loss: 0.00130492
Iteration 17/25 | Loss: 0.00130492
Iteration 18/25 | Loss: 0.00130492
Iteration 19/25 | Loss: 0.00130492
Iteration 20/25 | Loss: 0.00130492
Iteration 21/25 | Loss: 0.00130492
Iteration 22/25 | Loss: 0.00130492
Iteration 23/25 | Loss: 0.00130492
Iteration 24/25 | Loss: 0.00130492
Iteration 25/25 | Loss: 0.00130492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130492
Iteration 2/1000 | Loss: 0.00025158
Iteration 3/1000 | Loss: 0.00078092
Iteration 4/1000 | Loss: 0.00059071
Iteration 5/1000 | Loss: 0.00027064
Iteration 6/1000 | Loss: 0.00012841
Iteration 7/1000 | Loss: 0.00029993
Iteration 8/1000 | Loss: 0.00071008
Iteration 9/1000 | Loss: 0.00143625
Iteration 10/1000 | Loss: 0.00041477
Iteration 11/1000 | Loss: 0.00013577
Iteration 12/1000 | Loss: 0.00035350
Iteration 13/1000 | Loss: 0.00038839
Iteration 14/1000 | Loss: 0.00008227
Iteration 15/1000 | Loss: 0.00039929
Iteration 16/1000 | Loss: 0.00018930
Iteration 17/1000 | Loss: 0.00015877
Iteration 18/1000 | Loss: 0.00053395
Iteration 19/1000 | Loss: 0.00041902
Iteration 20/1000 | Loss: 0.00040688
Iteration 21/1000 | Loss: 0.00024471
Iteration 22/1000 | Loss: 0.00007625
Iteration 23/1000 | Loss: 0.00028122
Iteration 24/1000 | Loss: 0.00046784
Iteration 25/1000 | Loss: 0.00029001
Iteration 26/1000 | Loss: 0.00041918
Iteration 27/1000 | Loss: 0.00091834
Iteration 28/1000 | Loss: 0.00074394
Iteration 29/1000 | Loss: 0.00080970
Iteration 30/1000 | Loss: 0.00053734
Iteration 31/1000 | Loss: 0.00047655
Iteration 32/1000 | Loss: 0.00042268
Iteration 33/1000 | Loss: 0.00020792
Iteration 34/1000 | Loss: 0.00049098
Iteration 35/1000 | Loss: 0.00037004
Iteration 36/1000 | Loss: 0.00042401
Iteration 37/1000 | Loss: 0.00027284
Iteration 38/1000 | Loss: 0.00019404
Iteration 39/1000 | Loss: 0.00018246
Iteration 40/1000 | Loss: 0.00015993
Iteration 41/1000 | Loss: 0.00027308
Iteration 42/1000 | Loss: 0.00099412
Iteration 43/1000 | Loss: 0.00061675
Iteration 44/1000 | Loss: 0.00012675
Iteration 45/1000 | Loss: 0.00037227
Iteration 46/1000 | Loss: 0.00027610
Iteration 47/1000 | Loss: 0.00019961
Iteration 48/1000 | Loss: 0.00019654
Iteration 49/1000 | Loss: 0.00030790
Iteration 50/1000 | Loss: 0.00030375
Iteration 51/1000 | Loss: 0.00104340
Iteration 52/1000 | Loss: 0.00083647
Iteration 53/1000 | Loss: 0.00014776
Iteration 54/1000 | Loss: 0.00006844
Iteration 55/1000 | Loss: 0.00010368
Iteration 56/1000 | Loss: 0.00013018
Iteration 57/1000 | Loss: 0.00005360
Iteration 58/1000 | Loss: 0.00061537
Iteration 59/1000 | Loss: 0.00014555
Iteration 60/1000 | Loss: 0.00041398
Iteration 61/1000 | Loss: 0.00036939
Iteration 62/1000 | Loss: 0.00023775
Iteration 63/1000 | Loss: 0.00007532
Iteration 64/1000 | Loss: 0.00028474
Iteration 65/1000 | Loss: 0.00051909
Iteration 66/1000 | Loss: 0.00025579
Iteration 67/1000 | Loss: 0.00032201
Iteration 68/1000 | Loss: 0.00023680
Iteration 69/1000 | Loss: 0.00025954
Iteration 70/1000 | Loss: 0.00019153
Iteration 71/1000 | Loss: 0.00020541
Iteration 72/1000 | Loss: 0.00008867
Iteration 73/1000 | Loss: 0.00003508
Iteration 74/1000 | Loss: 0.00003165
Iteration 75/1000 | Loss: 0.00002934
Iteration 76/1000 | Loss: 0.00002823
Iteration 77/1000 | Loss: 0.00002742
Iteration 78/1000 | Loss: 0.00002671
Iteration 79/1000 | Loss: 0.00017267
Iteration 80/1000 | Loss: 0.00026116
Iteration 81/1000 | Loss: 0.00042483
Iteration 82/1000 | Loss: 0.00019027
Iteration 83/1000 | Loss: 0.00002518
Iteration 84/1000 | Loss: 0.00002480
Iteration 85/1000 | Loss: 0.00002440
Iteration 86/1000 | Loss: 0.00002403
Iteration 87/1000 | Loss: 0.00002370
Iteration 88/1000 | Loss: 0.00002338
Iteration 89/1000 | Loss: 0.00002309
Iteration 90/1000 | Loss: 0.00002289
Iteration 91/1000 | Loss: 0.00002282
Iteration 92/1000 | Loss: 0.00002275
Iteration 93/1000 | Loss: 0.00019217
Iteration 94/1000 | Loss: 0.00041195
Iteration 95/1000 | Loss: 0.00033915
Iteration 96/1000 | Loss: 0.00036030
Iteration 97/1000 | Loss: 0.00004340
Iteration 98/1000 | Loss: 0.00022231
Iteration 99/1000 | Loss: 0.00026177
Iteration 100/1000 | Loss: 0.00003175
Iteration 101/1000 | Loss: 0.00002910
Iteration 102/1000 | Loss: 0.00022960
Iteration 103/1000 | Loss: 0.00067840
Iteration 104/1000 | Loss: 0.00068094
Iteration 105/1000 | Loss: 0.00028674
Iteration 106/1000 | Loss: 0.00002764
Iteration 107/1000 | Loss: 0.00002659
Iteration 108/1000 | Loss: 0.00033465
Iteration 109/1000 | Loss: 0.00003346
Iteration 110/1000 | Loss: 0.00005611
Iteration 111/1000 | Loss: 0.00040234
Iteration 112/1000 | Loss: 0.00051604
Iteration 113/1000 | Loss: 0.00016705
Iteration 114/1000 | Loss: 0.00040167
Iteration 115/1000 | Loss: 0.00017746
Iteration 116/1000 | Loss: 0.00013019
Iteration 117/1000 | Loss: 0.00016888
Iteration 118/1000 | Loss: 0.00042672
Iteration 119/1000 | Loss: 0.00004888
Iteration 120/1000 | Loss: 0.00003977
Iteration 121/1000 | Loss: 0.00003537
Iteration 122/1000 | Loss: 0.00054560
Iteration 123/1000 | Loss: 0.00003665
Iteration 124/1000 | Loss: 0.00003186
Iteration 125/1000 | Loss: 0.00002947
Iteration 126/1000 | Loss: 0.00002802
Iteration 127/1000 | Loss: 0.00002697
Iteration 128/1000 | Loss: 0.00002621
Iteration 129/1000 | Loss: 0.00002556
Iteration 130/1000 | Loss: 0.00002515
Iteration 131/1000 | Loss: 0.00025199
Iteration 132/1000 | Loss: 0.00011704
Iteration 133/1000 | Loss: 0.00015359
Iteration 134/1000 | Loss: 0.00010964
Iteration 135/1000 | Loss: 0.00015216
Iteration 136/1000 | Loss: 0.00010443
Iteration 137/1000 | Loss: 0.00028096
Iteration 138/1000 | Loss: 0.00005537
Iteration 139/1000 | Loss: 0.00038616
Iteration 140/1000 | Loss: 0.00012321
Iteration 141/1000 | Loss: 0.00012533
Iteration 142/1000 | Loss: 0.00003584
Iteration 143/1000 | Loss: 0.00002904
Iteration 144/1000 | Loss: 0.00002723
Iteration 145/1000 | Loss: 0.00002630
Iteration 146/1000 | Loss: 0.00002584
Iteration 147/1000 | Loss: 0.00031880
Iteration 148/1000 | Loss: 0.00016215
Iteration 149/1000 | Loss: 0.00030176
Iteration 150/1000 | Loss: 0.00003545
Iteration 151/1000 | Loss: 0.00003193
Iteration 152/1000 | Loss: 0.00003048
Iteration 153/1000 | Loss: 0.00002950
Iteration 154/1000 | Loss: 0.00002831
Iteration 155/1000 | Loss: 0.00002738
Iteration 156/1000 | Loss: 0.00002672
Iteration 157/1000 | Loss: 0.00002569
Iteration 158/1000 | Loss: 0.00012275
Iteration 159/1000 | Loss: 0.00002406
Iteration 160/1000 | Loss: 0.00002314
Iteration 161/1000 | Loss: 0.00002254
Iteration 162/1000 | Loss: 0.00002164
Iteration 163/1000 | Loss: 0.00002106
Iteration 164/1000 | Loss: 0.00002079
Iteration 165/1000 | Loss: 0.00002057
Iteration 166/1000 | Loss: 0.00002051
Iteration 167/1000 | Loss: 0.00002051
Iteration 168/1000 | Loss: 0.00002050
Iteration 169/1000 | Loss: 0.00002050
Iteration 170/1000 | Loss: 0.00002050
Iteration 171/1000 | Loss: 0.00002049
Iteration 172/1000 | Loss: 0.00002048
Iteration 173/1000 | Loss: 0.00002048
Iteration 174/1000 | Loss: 0.00002048
Iteration 175/1000 | Loss: 0.00002047
Iteration 176/1000 | Loss: 0.00002045
Iteration 177/1000 | Loss: 0.00002043
Iteration 178/1000 | Loss: 0.00002042
Iteration 179/1000 | Loss: 0.00002041
Iteration 180/1000 | Loss: 0.00002041
Iteration 181/1000 | Loss: 0.00002041
Iteration 182/1000 | Loss: 0.00002041
Iteration 183/1000 | Loss: 0.00002039
Iteration 184/1000 | Loss: 0.00002039
Iteration 185/1000 | Loss: 0.00002039
Iteration 186/1000 | Loss: 0.00002039
Iteration 187/1000 | Loss: 0.00002038
Iteration 188/1000 | Loss: 0.00002038
Iteration 189/1000 | Loss: 0.00002038
Iteration 190/1000 | Loss: 0.00002038
Iteration 191/1000 | Loss: 0.00002038
Iteration 192/1000 | Loss: 0.00002038
Iteration 193/1000 | Loss: 0.00002038
Iteration 194/1000 | Loss: 0.00002037
Iteration 195/1000 | Loss: 0.00002036
Iteration 196/1000 | Loss: 0.00002036
Iteration 197/1000 | Loss: 0.00002036
Iteration 198/1000 | Loss: 0.00002036
Iteration 199/1000 | Loss: 0.00002035
Iteration 200/1000 | Loss: 0.00002035
Iteration 201/1000 | Loss: 0.00002035
Iteration 202/1000 | Loss: 0.00002035
Iteration 203/1000 | Loss: 0.00002035
Iteration 204/1000 | Loss: 0.00002035
Iteration 205/1000 | Loss: 0.00002034
Iteration 206/1000 | Loss: 0.00002034
Iteration 207/1000 | Loss: 0.00002034
Iteration 208/1000 | Loss: 0.00002034
Iteration 209/1000 | Loss: 0.00002033
Iteration 210/1000 | Loss: 0.00002033
Iteration 211/1000 | Loss: 0.00002033
Iteration 212/1000 | Loss: 0.00002032
Iteration 213/1000 | Loss: 0.00002032
Iteration 214/1000 | Loss: 0.00002032
Iteration 215/1000 | Loss: 0.00002031
Iteration 216/1000 | Loss: 0.00002031
Iteration 217/1000 | Loss: 0.00002031
Iteration 218/1000 | Loss: 0.00002030
Iteration 219/1000 | Loss: 0.00002030
Iteration 220/1000 | Loss: 0.00002029
Iteration 221/1000 | Loss: 0.00002029
Iteration 222/1000 | Loss: 0.00002028
Iteration 223/1000 | Loss: 0.00002028
Iteration 224/1000 | Loss: 0.00002028
Iteration 225/1000 | Loss: 0.00002028
Iteration 226/1000 | Loss: 0.00002028
Iteration 227/1000 | Loss: 0.00002028
Iteration 228/1000 | Loss: 0.00002027
Iteration 229/1000 | Loss: 0.00002027
Iteration 230/1000 | Loss: 0.00002027
Iteration 231/1000 | Loss: 0.00002027
Iteration 232/1000 | Loss: 0.00002026
Iteration 233/1000 | Loss: 0.00002026
Iteration 234/1000 | Loss: 0.00002026
Iteration 235/1000 | Loss: 0.00002026
Iteration 236/1000 | Loss: 0.00002025
Iteration 237/1000 | Loss: 0.00002025
Iteration 238/1000 | Loss: 0.00002025
Iteration 239/1000 | Loss: 0.00002025
Iteration 240/1000 | Loss: 0.00002025
Iteration 241/1000 | Loss: 0.00002025
Iteration 242/1000 | Loss: 0.00002025
Iteration 243/1000 | Loss: 0.00002025
Iteration 244/1000 | Loss: 0.00002024
Iteration 245/1000 | Loss: 0.00002024
Iteration 246/1000 | Loss: 0.00002024
Iteration 247/1000 | Loss: 0.00002024
Iteration 248/1000 | Loss: 0.00002023
Iteration 249/1000 | Loss: 0.00002023
Iteration 250/1000 | Loss: 0.00002023
Iteration 251/1000 | Loss: 0.00002023
Iteration 252/1000 | Loss: 0.00002023
Iteration 253/1000 | Loss: 0.00002023
Iteration 254/1000 | Loss: 0.00002023
Iteration 255/1000 | Loss: 0.00002022
Iteration 256/1000 | Loss: 0.00002022
Iteration 257/1000 | Loss: 0.00002022
Iteration 258/1000 | Loss: 0.00002022
Iteration 259/1000 | Loss: 0.00002022
Iteration 260/1000 | Loss: 0.00002022
Iteration 261/1000 | Loss: 0.00002022
Iteration 262/1000 | Loss: 0.00002021
Iteration 263/1000 | Loss: 0.00002021
Iteration 264/1000 | Loss: 0.00002021
Iteration 265/1000 | Loss: 0.00002021
Iteration 266/1000 | Loss: 0.00002021
Iteration 267/1000 | Loss: 0.00002020
Iteration 268/1000 | Loss: 0.00002020
Iteration 269/1000 | Loss: 0.00002020
Iteration 270/1000 | Loss: 0.00002020
Iteration 271/1000 | Loss: 0.00002020
Iteration 272/1000 | Loss: 0.00002020
Iteration 273/1000 | Loss: 0.00002020
Iteration 274/1000 | Loss: 0.00002020
Iteration 275/1000 | Loss: 0.00002019
Iteration 276/1000 | Loss: 0.00002019
Iteration 277/1000 | Loss: 0.00002019
Iteration 278/1000 | Loss: 0.00002019
Iteration 279/1000 | Loss: 0.00002019
Iteration 280/1000 | Loss: 0.00002019
Iteration 281/1000 | Loss: 0.00002019
Iteration 282/1000 | Loss: 0.00002019
Iteration 283/1000 | Loss: 0.00002019
Iteration 284/1000 | Loss: 0.00002019
Iteration 285/1000 | Loss: 0.00002018
Iteration 286/1000 | Loss: 0.00002018
Iteration 287/1000 | Loss: 0.00002018
Iteration 288/1000 | Loss: 0.00002018
Iteration 289/1000 | Loss: 0.00002018
Iteration 290/1000 | Loss: 0.00002018
Iteration 291/1000 | Loss: 0.00002017
Iteration 292/1000 | Loss: 0.00002017
Iteration 293/1000 | Loss: 0.00002017
Iteration 294/1000 | Loss: 0.00002017
Iteration 295/1000 | Loss: 0.00002016
Iteration 296/1000 | Loss: 0.00002016
Iteration 297/1000 | Loss: 0.00002016
Iteration 298/1000 | Loss: 0.00002016
Iteration 299/1000 | Loss: 0.00002016
Iteration 300/1000 | Loss: 0.00002016
Iteration 301/1000 | Loss: 0.00002016
Iteration 302/1000 | Loss: 0.00002016
Iteration 303/1000 | Loss: 0.00002015
Iteration 304/1000 | Loss: 0.00002015
Iteration 305/1000 | Loss: 0.00002015
Iteration 306/1000 | Loss: 0.00002015
Iteration 307/1000 | Loss: 0.00002015
Iteration 308/1000 | Loss: 0.00002015
Iteration 309/1000 | Loss: 0.00002015
Iteration 310/1000 | Loss: 0.00002014
Iteration 311/1000 | Loss: 0.00002014
Iteration 312/1000 | Loss: 0.00002014
Iteration 313/1000 | Loss: 0.00002014
Iteration 314/1000 | Loss: 0.00002014
Iteration 315/1000 | Loss: 0.00002014
Iteration 316/1000 | Loss: 0.00002014
Iteration 317/1000 | Loss: 0.00002014
Iteration 318/1000 | Loss: 0.00002014
Iteration 319/1000 | Loss: 0.00002014
Iteration 320/1000 | Loss: 0.00002014
Iteration 321/1000 | Loss: 0.00002014
Iteration 322/1000 | Loss: 0.00002014
Iteration 323/1000 | Loss: 0.00002014
Iteration 324/1000 | Loss: 0.00002014
Iteration 325/1000 | Loss: 0.00002014
Iteration 326/1000 | Loss: 0.00002014
Iteration 327/1000 | Loss: 0.00002014
Iteration 328/1000 | Loss: 0.00002014
Iteration 329/1000 | Loss: 0.00002014
Iteration 330/1000 | Loss: 0.00002013
Iteration 331/1000 | Loss: 0.00002013
Iteration 332/1000 | Loss: 0.00002013
Iteration 333/1000 | Loss: 0.00002013
Iteration 334/1000 | Loss: 0.00002013
Iteration 335/1000 | Loss: 0.00002013
Iteration 336/1000 | Loss: 0.00002013
Iteration 337/1000 | Loss: 0.00002013
Iteration 338/1000 | Loss: 0.00002013
Iteration 339/1000 | Loss: 0.00002013
Iteration 340/1000 | Loss: 0.00002013
Iteration 341/1000 | Loss: 0.00002013
Iteration 342/1000 | Loss: 0.00002013
Iteration 343/1000 | Loss: 0.00002012
Iteration 344/1000 | Loss: 0.00002012
Iteration 345/1000 | Loss: 0.00002012
Iteration 346/1000 | Loss: 0.00002012
Iteration 347/1000 | Loss: 0.00002012
Iteration 348/1000 | Loss: 0.00002012
Iteration 349/1000 | Loss: 0.00002012
Iteration 350/1000 | Loss: 0.00002012
Iteration 351/1000 | Loss: 0.00002012
Iteration 352/1000 | Loss: 0.00002012
Iteration 353/1000 | Loss: 0.00002012
Iteration 354/1000 | Loss: 0.00002012
Iteration 355/1000 | Loss: 0.00002012
Iteration 356/1000 | Loss: 0.00002012
Iteration 357/1000 | Loss: 0.00002012
Iteration 358/1000 | Loss: 0.00002012
Iteration 359/1000 | Loss: 0.00002012
Iteration 360/1000 | Loss: 0.00002012
Iteration 361/1000 | Loss: 0.00002012
Iteration 362/1000 | Loss: 0.00002012
Iteration 363/1000 | Loss: 0.00002012
Iteration 364/1000 | Loss: 0.00002012
Iteration 365/1000 | Loss: 0.00002011
Iteration 366/1000 | Loss: 0.00002011
Iteration 367/1000 | Loss: 0.00002011
Iteration 368/1000 | Loss: 0.00002011
Iteration 369/1000 | Loss: 0.00002011
Iteration 370/1000 | Loss: 0.00002011
Iteration 371/1000 | Loss: 0.00002011
Iteration 372/1000 | Loss: 0.00002011
Iteration 373/1000 | Loss: 0.00002011
Iteration 374/1000 | Loss: 0.00002011
Iteration 375/1000 | Loss: 0.00002011
Iteration 376/1000 | Loss: 0.00002011
Iteration 377/1000 | Loss: 0.00002011
Iteration 378/1000 | Loss: 0.00002011
Iteration 379/1000 | Loss: 0.00002011
Iteration 380/1000 | Loss: 0.00002011
Iteration 381/1000 | Loss: 0.00002011
Iteration 382/1000 | Loss: 0.00002011
Iteration 383/1000 | Loss: 0.00002011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 383. Stopping optimization.
Last 5 losses: [2.01105467567686e-05, 2.01105467567686e-05, 2.01105467567686e-05, 2.01105467567686e-05, 2.01105467567686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.01105467567686e-05

Optimization complete. Final v2v error: 3.700178623199463 mm

Highest mean error: 4.786846160888672 mm for frame 90

Lowest mean error: 2.9325168132781982 mm for frame 149

Saving results

Total time: 292.046893119812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449514
Iteration 2/25 | Loss: 0.00090889
Iteration 3/25 | Loss: 0.00078029
Iteration 4/25 | Loss: 0.00075537
Iteration 5/25 | Loss: 0.00074718
Iteration 6/25 | Loss: 0.00074508
Iteration 7/25 | Loss: 0.00074458
Iteration 8/25 | Loss: 0.00074458
Iteration 9/25 | Loss: 0.00074458
Iteration 10/25 | Loss: 0.00074458
Iteration 11/25 | Loss: 0.00074458
Iteration 12/25 | Loss: 0.00074458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007445770897902548, 0.0007445770897902548, 0.0007445770897902548, 0.0007445770897902548, 0.0007445770897902548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007445770897902548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.54628706
Iteration 2/25 | Loss: 0.00042538
Iteration 3/25 | Loss: 0.00042538
Iteration 4/25 | Loss: 0.00042538
Iteration 5/25 | Loss: 0.00042538
Iteration 6/25 | Loss: 0.00042538
Iteration 7/25 | Loss: 0.00042538
Iteration 8/25 | Loss: 0.00042538
Iteration 9/25 | Loss: 0.00042538
Iteration 10/25 | Loss: 0.00042538
Iteration 11/25 | Loss: 0.00042538
Iteration 12/25 | Loss: 0.00042538
Iteration 13/25 | Loss: 0.00042538
Iteration 14/25 | Loss: 0.00042538
Iteration 15/25 | Loss: 0.00042538
Iteration 16/25 | Loss: 0.00042538
Iteration 17/25 | Loss: 0.00042538
Iteration 18/25 | Loss: 0.00042538
Iteration 19/25 | Loss: 0.00042538
Iteration 20/25 | Loss: 0.00042538
Iteration 21/25 | Loss: 0.00042538
Iteration 22/25 | Loss: 0.00042538
Iteration 23/25 | Loss: 0.00042538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004253762308508158, 0.0004253762308508158, 0.0004253762308508158, 0.0004253762308508158, 0.0004253762308508158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004253762308508158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042538
Iteration 2/1000 | Loss: 0.00003215
Iteration 3/1000 | Loss: 0.00002211
Iteration 4/1000 | Loss: 0.00002036
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001793
Iteration 9/1000 | Loss: 0.00001771
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001753
Iteration 14/1000 | Loss: 0.00001752
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001720
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001720
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001718
Iteration 32/1000 | Loss: 0.00001718
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001715
Iteration 37/1000 | Loss: 0.00001715
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00001712
Iteration 40/1000 | Loss: 0.00001712
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001694
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001689
Iteration 55/1000 | Loss: 0.00001689
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001688
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001687
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001686
Iteration 62/1000 | Loss: 0.00001686
Iteration 63/1000 | Loss: 0.00001686
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001686
Iteration 70/1000 | Loss: 0.00001686
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001684
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001683
Iteration 88/1000 | Loss: 0.00001683
Iteration 89/1000 | Loss: 0.00001683
Iteration 90/1000 | Loss: 0.00001683
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001682
Iteration 95/1000 | Loss: 0.00001682
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001681
Iteration 102/1000 | Loss: 0.00001681
Iteration 103/1000 | Loss: 0.00001681
Iteration 104/1000 | Loss: 0.00001681
Iteration 105/1000 | Loss: 0.00001681
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001681
Iteration 111/1000 | Loss: 0.00001681
Iteration 112/1000 | Loss: 0.00001681
Iteration 113/1000 | Loss: 0.00001681
Iteration 114/1000 | Loss: 0.00001680
Iteration 115/1000 | Loss: 0.00001680
Iteration 116/1000 | Loss: 0.00001680
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001680
Iteration 120/1000 | Loss: 0.00001680
Iteration 121/1000 | Loss: 0.00001680
Iteration 122/1000 | Loss: 0.00001680
Iteration 123/1000 | Loss: 0.00001680
Iteration 124/1000 | Loss: 0.00001680
Iteration 125/1000 | Loss: 0.00001680
Iteration 126/1000 | Loss: 0.00001680
Iteration 127/1000 | Loss: 0.00001680
Iteration 128/1000 | Loss: 0.00001680
Iteration 129/1000 | Loss: 0.00001680
Iteration 130/1000 | Loss: 0.00001680
Iteration 131/1000 | Loss: 0.00001680
Iteration 132/1000 | Loss: 0.00001680
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Iteration 140/1000 | Loss: 0.00001680
Iteration 141/1000 | Loss: 0.00001680
Iteration 142/1000 | Loss: 0.00001680
Iteration 143/1000 | Loss: 0.00001680
Iteration 144/1000 | Loss: 0.00001680
Iteration 145/1000 | Loss: 0.00001680
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001680
Iteration 148/1000 | Loss: 0.00001680
Iteration 149/1000 | Loss: 0.00001680
Iteration 150/1000 | Loss: 0.00001680
Iteration 151/1000 | Loss: 0.00001680
Iteration 152/1000 | Loss: 0.00001680
Iteration 153/1000 | Loss: 0.00001680
Iteration 154/1000 | Loss: 0.00001680
Iteration 155/1000 | Loss: 0.00001680
Iteration 156/1000 | Loss: 0.00001680
Iteration 157/1000 | Loss: 0.00001680
Iteration 158/1000 | Loss: 0.00001680
Iteration 159/1000 | Loss: 0.00001680
Iteration 160/1000 | Loss: 0.00001680
Iteration 161/1000 | Loss: 0.00001680
Iteration 162/1000 | Loss: 0.00001680
Iteration 163/1000 | Loss: 0.00001680
Iteration 164/1000 | Loss: 0.00001680
Iteration 165/1000 | Loss: 0.00001680
Iteration 166/1000 | Loss: 0.00001680
Iteration 167/1000 | Loss: 0.00001680
Iteration 168/1000 | Loss: 0.00001680
Iteration 169/1000 | Loss: 0.00001680
Iteration 170/1000 | Loss: 0.00001680
Iteration 171/1000 | Loss: 0.00001680
Iteration 172/1000 | Loss: 0.00001680
Iteration 173/1000 | Loss: 0.00001680
Iteration 174/1000 | Loss: 0.00001680
Iteration 175/1000 | Loss: 0.00001680
Iteration 176/1000 | Loss: 0.00001680
Iteration 177/1000 | Loss: 0.00001680
Iteration 178/1000 | Loss: 0.00001680
Iteration 179/1000 | Loss: 0.00001680
Iteration 180/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.680067907727789e-05, 1.680067907727789e-05, 1.680067907727789e-05, 1.680067907727789e-05, 1.680067907727789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680067907727789e-05

Optimization complete. Final v2v error: 3.478576421737671 mm

Highest mean error: 4.1029133796691895 mm for frame 79

Lowest mean error: 3.127872943878174 mm for frame 35

Saving results

Total time: 39.77893590927124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010307
Iteration 2/25 | Loss: 0.01010307
Iteration 3/25 | Loss: 0.01010306
Iteration 4/25 | Loss: 0.01010306
Iteration 5/25 | Loss: 0.01010306
Iteration 6/25 | Loss: 0.01010305
Iteration 7/25 | Loss: 0.01010305
Iteration 8/25 | Loss: 0.01010305
Iteration 9/25 | Loss: 0.01010305
Iteration 10/25 | Loss: 0.01010305
Iteration 11/25 | Loss: 0.01010305
Iteration 12/25 | Loss: 0.01010304
Iteration 13/25 | Loss: 0.01010304
Iteration 14/25 | Loss: 0.01010304
Iteration 15/25 | Loss: 0.01010304
Iteration 16/25 | Loss: 0.01010303
Iteration 17/25 | Loss: 0.01010303
Iteration 18/25 | Loss: 0.01010303
Iteration 19/25 | Loss: 0.01010303
Iteration 20/25 | Loss: 0.01010303
Iteration 21/25 | Loss: 0.01010302
Iteration 22/25 | Loss: 0.01010302
Iteration 23/25 | Loss: 0.01010302
Iteration 24/25 | Loss: 0.01010302
Iteration 25/25 | Loss: 0.01010302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74805689
Iteration 2/25 | Loss: 0.17668609
Iteration 3/25 | Loss: 0.17666273
Iteration 4/25 | Loss: 0.17666270
Iteration 5/25 | Loss: 0.17666270
Iteration 6/25 | Loss: 0.17666265
Iteration 7/25 | Loss: 0.17666265
Iteration 8/25 | Loss: 0.17666265
Iteration 9/25 | Loss: 0.17666265
Iteration 10/25 | Loss: 0.17666265
Iteration 11/25 | Loss: 0.17666264
Iteration 12/25 | Loss: 0.17666264
Iteration 13/25 | Loss: 0.17666265
Iteration 14/25 | Loss: 0.17666265
Iteration 15/25 | Loss: 0.17666265
Iteration 16/25 | Loss: 0.17666265
Iteration 17/25 | Loss: 0.17666265
Iteration 18/25 | Loss: 0.17666264
Iteration 19/25 | Loss: 0.17666264
Iteration 20/25 | Loss: 0.17666264
Iteration 21/25 | Loss: 0.17666265
Iteration 22/25 | Loss: 0.17666265
Iteration 23/25 | Loss: 0.17666264
Iteration 24/25 | Loss: 0.17666264
Iteration 25/25 | Loss: 0.17666264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17666264
Iteration 2/1000 | Loss: 0.01427993
Iteration 3/1000 | Loss: 0.00630088
Iteration 4/1000 | Loss: 0.00244485
Iteration 5/1000 | Loss: 0.00089421
Iteration 6/1000 | Loss: 0.00072636
Iteration 7/1000 | Loss: 0.00171299
Iteration 8/1000 | Loss: 0.00041098
Iteration 9/1000 | Loss: 0.00018352
Iteration 10/1000 | Loss: 0.00168068
Iteration 11/1000 | Loss: 0.00033447
Iteration 12/1000 | Loss: 0.00024925
Iteration 13/1000 | Loss: 0.00081094
Iteration 14/1000 | Loss: 0.00060216
Iteration 15/1000 | Loss: 0.00039019
Iteration 16/1000 | Loss: 0.00031912
Iteration 17/1000 | Loss: 0.00021631
Iteration 18/1000 | Loss: 0.00064810
Iteration 19/1000 | Loss: 0.00006004
Iteration 20/1000 | Loss: 0.00039835
Iteration 21/1000 | Loss: 0.00007079
Iteration 22/1000 | Loss: 0.00005991
Iteration 23/1000 | Loss: 0.00041112
Iteration 24/1000 | Loss: 0.00168150
Iteration 25/1000 | Loss: 0.00023258
Iteration 26/1000 | Loss: 0.00002824
Iteration 27/1000 | Loss: 0.00027397
Iteration 28/1000 | Loss: 0.00002635
Iteration 29/1000 | Loss: 0.00011810
Iteration 30/1000 | Loss: 0.00009011
Iteration 31/1000 | Loss: 0.00009781
Iteration 32/1000 | Loss: 0.00002376
Iteration 33/1000 | Loss: 0.00005063
Iteration 34/1000 | Loss: 0.00002321
Iteration 35/1000 | Loss: 0.00010547
Iteration 36/1000 | Loss: 0.00008575
Iteration 37/1000 | Loss: 0.00002391
Iteration 38/1000 | Loss: 0.00004231
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002183
Iteration 41/1000 | Loss: 0.00006990
Iteration 42/1000 | Loss: 0.00006729
Iteration 43/1000 | Loss: 0.00007552
Iteration 44/1000 | Loss: 0.00002134
Iteration 45/1000 | Loss: 0.00002984
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002069
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002054
Iteration 50/1000 | Loss: 0.00002053
Iteration 51/1000 | Loss: 0.00002051
Iteration 52/1000 | Loss: 0.00002051
Iteration 53/1000 | Loss: 0.00002051
Iteration 54/1000 | Loss: 0.00002051
Iteration 55/1000 | Loss: 0.00002050
Iteration 56/1000 | Loss: 0.00002048
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002048
Iteration 59/1000 | Loss: 0.00002048
Iteration 60/1000 | Loss: 0.00002048
Iteration 61/1000 | Loss: 0.00002048
Iteration 62/1000 | Loss: 0.00002048
Iteration 63/1000 | Loss: 0.00002048
Iteration 64/1000 | Loss: 0.00002048
Iteration 65/1000 | Loss: 0.00002048
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00002048
Iteration 68/1000 | Loss: 0.00002048
Iteration 69/1000 | Loss: 0.00002048
Iteration 70/1000 | Loss: 0.00002048
Iteration 71/1000 | Loss: 0.00002047
Iteration 72/1000 | Loss: 0.00002047
Iteration 73/1000 | Loss: 0.00002047
Iteration 74/1000 | Loss: 0.00002046
Iteration 75/1000 | Loss: 0.00002046
Iteration 76/1000 | Loss: 0.00002046
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002046
Iteration 79/1000 | Loss: 0.00002046
Iteration 80/1000 | Loss: 0.00002046
Iteration 81/1000 | Loss: 0.00002046
Iteration 82/1000 | Loss: 0.00002046
Iteration 83/1000 | Loss: 0.00002045
Iteration 84/1000 | Loss: 0.00002045
Iteration 85/1000 | Loss: 0.00002045
Iteration 86/1000 | Loss: 0.00002045
Iteration 87/1000 | Loss: 0.00002045
Iteration 88/1000 | Loss: 0.00002045
Iteration 89/1000 | Loss: 0.00002045
Iteration 90/1000 | Loss: 0.00002045
Iteration 91/1000 | Loss: 0.00002045
Iteration 92/1000 | Loss: 0.00002045
Iteration 93/1000 | Loss: 0.00002045
Iteration 94/1000 | Loss: 0.00002044
Iteration 95/1000 | Loss: 0.00002044
Iteration 96/1000 | Loss: 0.00002044
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00008491
Iteration 99/1000 | Loss: 0.00008491
Iteration 100/1000 | Loss: 0.00004553
Iteration 101/1000 | Loss: 0.00007655
Iteration 102/1000 | Loss: 0.00003477
Iteration 103/1000 | Loss: 0.00002041
Iteration 104/1000 | Loss: 0.00002041
Iteration 105/1000 | Loss: 0.00002039
Iteration 106/1000 | Loss: 0.00002039
Iteration 107/1000 | Loss: 0.00002037
Iteration 108/1000 | Loss: 0.00002036
Iteration 109/1000 | Loss: 0.00002036
Iteration 110/1000 | Loss: 0.00002036
Iteration 111/1000 | Loss: 0.00002036
Iteration 112/1000 | Loss: 0.00002036
Iteration 113/1000 | Loss: 0.00002035
Iteration 114/1000 | Loss: 0.00002035
Iteration 115/1000 | Loss: 0.00002035
Iteration 116/1000 | Loss: 0.00002035
Iteration 117/1000 | Loss: 0.00002035
Iteration 118/1000 | Loss: 0.00002035
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002034
Iteration 128/1000 | Loss: 0.00002034
Iteration 129/1000 | Loss: 0.00002034
Iteration 130/1000 | Loss: 0.00002034
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002033
Iteration 137/1000 | Loss: 0.00002033
Iteration 138/1000 | Loss: 0.00002033
Iteration 139/1000 | Loss: 0.00002033
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002032
Iteration 143/1000 | Loss: 0.00002032
Iteration 144/1000 | Loss: 0.00002032
Iteration 145/1000 | Loss: 0.00002032
Iteration 146/1000 | Loss: 0.00002032
Iteration 147/1000 | Loss: 0.00002032
Iteration 148/1000 | Loss: 0.00002032
Iteration 149/1000 | Loss: 0.00002032
Iteration 150/1000 | Loss: 0.00002032
Iteration 151/1000 | Loss: 0.00002032
Iteration 152/1000 | Loss: 0.00002031
Iteration 153/1000 | Loss: 0.00002031
Iteration 154/1000 | Loss: 0.00002031
Iteration 155/1000 | Loss: 0.00002031
Iteration 156/1000 | Loss: 0.00002031
Iteration 157/1000 | Loss: 0.00002031
Iteration 158/1000 | Loss: 0.00002030
Iteration 159/1000 | Loss: 0.00002030
Iteration 160/1000 | Loss: 0.00002030
Iteration 161/1000 | Loss: 0.00002030
Iteration 162/1000 | Loss: 0.00002030
Iteration 163/1000 | Loss: 0.00002029
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002029
Iteration 166/1000 | Loss: 0.00002029
Iteration 167/1000 | Loss: 0.00002029
Iteration 168/1000 | Loss: 0.00002029
Iteration 169/1000 | Loss: 0.00002029
Iteration 170/1000 | Loss: 0.00002029
Iteration 171/1000 | Loss: 0.00002029
Iteration 172/1000 | Loss: 0.00002028
Iteration 173/1000 | Loss: 0.00002028
Iteration 174/1000 | Loss: 0.00002028
Iteration 175/1000 | Loss: 0.00002028
Iteration 176/1000 | Loss: 0.00002028
Iteration 177/1000 | Loss: 0.00002028
Iteration 178/1000 | Loss: 0.00002028
Iteration 179/1000 | Loss: 0.00002028
Iteration 180/1000 | Loss: 0.00002028
Iteration 181/1000 | Loss: 0.00002028
Iteration 182/1000 | Loss: 0.00002028
Iteration 183/1000 | Loss: 0.00002028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.028140806942247e-05, 2.028140806942247e-05, 2.028140806942247e-05, 2.028140806942247e-05, 2.028140806942247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.028140806942247e-05

Optimization complete. Final v2v error: 3.756410837173462 mm

Highest mean error: 4.059882164001465 mm for frame 219

Lowest mean error: 3.549466371536255 mm for frame 16

Saving results

Total time: 102.19141149520874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062266
Iteration 2/25 | Loss: 0.01062266
Iteration 3/25 | Loss: 0.01062266
Iteration 4/25 | Loss: 0.01062266
Iteration 5/25 | Loss: 0.01062266
Iteration 6/25 | Loss: 0.00205541
Iteration 7/25 | Loss: 0.00119596
Iteration 8/25 | Loss: 0.00108675
Iteration 9/25 | Loss: 0.00100375
Iteration 10/25 | Loss: 0.00092641
Iteration 11/25 | Loss: 0.00092301
Iteration 12/25 | Loss: 0.00089941
Iteration 13/25 | Loss: 0.00088309
Iteration 14/25 | Loss: 0.00086383
Iteration 15/25 | Loss: 0.00085212
Iteration 16/25 | Loss: 0.00085017
Iteration 17/25 | Loss: 0.00084522
Iteration 18/25 | Loss: 0.00083732
Iteration 19/25 | Loss: 0.00084113
Iteration 20/25 | Loss: 0.00083032
Iteration 21/25 | Loss: 0.00082649
Iteration 22/25 | Loss: 0.00083214
Iteration 23/25 | Loss: 0.00082257
Iteration 24/25 | Loss: 0.00082729
Iteration 25/25 | Loss: 0.00082671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50394475
Iteration 2/25 | Loss: 0.00153459
Iteration 3/25 | Loss: 0.00118541
Iteration 4/25 | Loss: 0.00118541
Iteration 5/25 | Loss: 0.00118541
Iteration 6/25 | Loss: 0.00118541
Iteration 7/25 | Loss: 0.00118541
Iteration 8/25 | Loss: 0.00118541
Iteration 9/25 | Loss: 0.00118541
Iteration 10/25 | Loss: 0.00118541
Iteration 11/25 | Loss: 0.00118541
Iteration 12/25 | Loss: 0.00118541
Iteration 13/25 | Loss: 0.00118541
Iteration 14/25 | Loss: 0.00118541
Iteration 15/25 | Loss: 0.00118541
Iteration 16/25 | Loss: 0.00118541
Iteration 17/25 | Loss: 0.00118541
Iteration 18/25 | Loss: 0.00118541
Iteration 19/25 | Loss: 0.00118541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00118540832772851, 0.00118540832772851, 0.00118540832772851, 0.00118540832772851, 0.00118540832772851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00118540832772851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118541
Iteration 2/1000 | Loss: 0.00026842
Iteration 3/1000 | Loss: 0.00091223
Iteration 4/1000 | Loss: 0.00102894
Iteration 5/1000 | Loss: 0.00020918
Iteration 6/1000 | Loss: 0.00013696
Iteration 7/1000 | Loss: 0.00034761
Iteration 8/1000 | Loss: 0.00014063
Iteration 9/1000 | Loss: 0.00031442
Iteration 10/1000 | Loss: 0.00052199
Iteration 11/1000 | Loss: 0.00037216
Iteration 12/1000 | Loss: 0.00047813
Iteration 13/1000 | Loss: 0.00023700
Iteration 14/1000 | Loss: 0.00013558
Iteration 15/1000 | Loss: 0.00057473
Iteration 16/1000 | Loss: 0.00043375
Iteration 17/1000 | Loss: 0.00043026
Iteration 18/1000 | Loss: 0.00020290
Iteration 19/1000 | Loss: 0.00006340
Iteration 20/1000 | Loss: 0.00011928
Iteration 21/1000 | Loss: 0.00011652
Iteration 22/1000 | Loss: 0.00080616
Iteration 23/1000 | Loss: 0.00077262
Iteration 24/1000 | Loss: 0.00047202
Iteration 25/1000 | Loss: 0.00052646
Iteration 26/1000 | Loss: 0.00127199
Iteration 27/1000 | Loss: 0.00072361
Iteration 28/1000 | Loss: 0.00084841
Iteration 29/1000 | Loss: 0.00066094
Iteration 30/1000 | Loss: 0.00045492
Iteration 31/1000 | Loss: 0.00019327
Iteration 32/1000 | Loss: 0.00033905
Iteration 33/1000 | Loss: 0.00017824
Iteration 34/1000 | Loss: 0.00007010
Iteration 35/1000 | Loss: 0.00006477
Iteration 36/1000 | Loss: 0.00036465
Iteration 37/1000 | Loss: 0.00030245
Iteration 38/1000 | Loss: 0.00013180
Iteration 39/1000 | Loss: 0.00046257
Iteration 40/1000 | Loss: 0.00060291
Iteration 41/1000 | Loss: 0.00012477
Iteration 42/1000 | Loss: 0.00043367
Iteration 43/1000 | Loss: 0.00023952
Iteration 44/1000 | Loss: 0.00026213
Iteration 45/1000 | Loss: 0.00005086
Iteration 46/1000 | Loss: 0.00087904
Iteration 47/1000 | Loss: 0.00051638
Iteration 48/1000 | Loss: 0.00050036
Iteration 49/1000 | Loss: 0.00039359
Iteration 50/1000 | Loss: 0.00049045
Iteration 51/1000 | Loss: 0.00008651
Iteration 52/1000 | Loss: 0.00007067
Iteration 53/1000 | Loss: 0.00006383
Iteration 54/1000 | Loss: 0.00037079
Iteration 55/1000 | Loss: 0.00010866
Iteration 56/1000 | Loss: 0.00017039
Iteration 57/1000 | Loss: 0.00004201
Iteration 58/1000 | Loss: 0.00030263
Iteration 59/1000 | Loss: 0.00004841
Iteration 60/1000 | Loss: 0.00043659
Iteration 61/1000 | Loss: 0.00035585
Iteration 62/1000 | Loss: 0.00037780
Iteration 63/1000 | Loss: 0.00040464
Iteration 64/1000 | Loss: 0.00010465
Iteration 65/1000 | Loss: 0.00004375
Iteration 66/1000 | Loss: 0.00003998
Iteration 67/1000 | Loss: 0.00003915
Iteration 68/1000 | Loss: 0.00021882
Iteration 69/1000 | Loss: 0.00064468
Iteration 70/1000 | Loss: 0.00058031
Iteration 71/1000 | Loss: 0.00209702
Iteration 72/1000 | Loss: 0.00502540
Iteration 73/1000 | Loss: 0.00200548
Iteration 74/1000 | Loss: 0.00373900
Iteration 75/1000 | Loss: 0.00030089
Iteration 76/1000 | Loss: 0.00070380
Iteration 77/1000 | Loss: 0.00163503
Iteration 78/1000 | Loss: 0.00007673
Iteration 79/1000 | Loss: 0.00006328
Iteration 80/1000 | Loss: 0.00008928
Iteration 81/1000 | Loss: 0.00013846
Iteration 82/1000 | Loss: 0.00003759
Iteration 83/1000 | Loss: 0.00086397
Iteration 84/1000 | Loss: 0.00007359
Iteration 85/1000 | Loss: 0.00011455
Iteration 86/1000 | Loss: 0.00002838
Iteration 87/1000 | Loss: 0.00020425
Iteration 88/1000 | Loss: 0.00054276
Iteration 89/1000 | Loss: 0.00020468
Iteration 90/1000 | Loss: 0.00020052
Iteration 91/1000 | Loss: 0.00018884
Iteration 92/1000 | Loss: 0.00003878
Iteration 93/1000 | Loss: 0.00004539
Iteration 94/1000 | Loss: 0.00002727
Iteration 95/1000 | Loss: 0.00007587
Iteration 96/1000 | Loss: 0.00004869
Iteration 97/1000 | Loss: 0.00002548
Iteration 98/1000 | Loss: 0.00002375
Iteration 99/1000 | Loss: 0.00002992
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002302
Iteration 102/1000 | Loss: 0.00002207
Iteration 103/1000 | Loss: 0.00017635
Iteration 104/1000 | Loss: 0.00009551
Iteration 105/1000 | Loss: 0.00003402
Iteration 106/1000 | Loss: 0.00022139
Iteration 107/1000 | Loss: 0.00024860
Iteration 108/1000 | Loss: 0.00031090
Iteration 109/1000 | Loss: 0.00014563
Iteration 110/1000 | Loss: 0.00005321
Iteration 111/1000 | Loss: 0.00003471
Iteration 112/1000 | Loss: 0.00002356
Iteration 113/1000 | Loss: 0.00005378
Iteration 114/1000 | Loss: 0.00003137
Iteration 115/1000 | Loss: 0.00003303
Iteration 116/1000 | Loss: 0.00004933
Iteration 117/1000 | Loss: 0.00003132
Iteration 118/1000 | Loss: 0.00001811
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00019537
Iteration 121/1000 | Loss: 0.00056903
Iteration 122/1000 | Loss: 0.00012306
Iteration 123/1000 | Loss: 0.00003056
Iteration 124/1000 | Loss: 0.00007926
Iteration 125/1000 | Loss: 0.00004202
Iteration 126/1000 | Loss: 0.00002047
Iteration 127/1000 | Loss: 0.00003649
Iteration 128/1000 | Loss: 0.00002608
Iteration 129/1000 | Loss: 0.00002444
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001856
Iteration 132/1000 | Loss: 0.00001512
Iteration 133/1000 | Loss: 0.00001490
Iteration 134/1000 | Loss: 0.00003412
Iteration 135/1000 | Loss: 0.00003225
Iteration 136/1000 | Loss: 0.00001738
Iteration 137/1000 | Loss: 0.00001652
Iteration 138/1000 | Loss: 0.00009011
Iteration 139/1000 | Loss: 0.00001723
Iteration 140/1000 | Loss: 0.00001848
Iteration 141/1000 | Loss: 0.00005295
Iteration 142/1000 | Loss: 0.00001636
Iteration 143/1000 | Loss: 0.00001439
Iteration 144/1000 | Loss: 0.00001439
Iteration 145/1000 | Loss: 0.00001439
Iteration 146/1000 | Loss: 0.00001439
Iteration 147/1000 | Loss: 0.00001439
Iteration 148/1000 | Loss: 0.00001439
Iteration 149/1000 | Loss: 0.00001439
Iteration 150/1000 | Loss: 0.00002744
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001434
Iteration 154/1000 | Loss: 0.00001433
Iteration 155/1000 | Loss: 0.00001433
Iteration 156/1000 | Loss: 0.00001432
Iteration 157/1000 | Loss: 0.00001432
Iteration 158/1000 | Loss: 0.00001432
Iteration 159/1000 | Loss: 0.00001432
Iteration 160/1000 | Loss: 0.00001431
Iteration 161/1000 | Loss: 0.00001431
Iteration 162/1000 | Loss: 0.00001431
Iteration 163/1000 | Loss: 0.00001431
Iteration 164/1000 | Loss: 0.00001431
Iteration 165/1000 | Loss: 0.00001431
Iteration 166/1000 | Loss: 0.00001431
Iteration 167/1000 | Loss: 0.00001431
Iteration 168/1000 | Loss: 0.00001431
Iteration 169/1000 | Loss: 0.00001431
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001429
Iteration 175/1000 | Loss: 0.00001429
Iteration 176/1000 | Loss: 0.00001428
Iteration 177/1000 | Loss: 0.00001428
Iteration 178/1000 | Loss: 0.00001428
Iteration 179/1000 | Loss: 0.00001793
Iteration 180/1000 | Loss: 0.00001428
Iteration 181/1000 | Loss: 0.00001428
Iteration 182/1000 | Loss: 0.00001428
Iteration 183/1000 | Loss: 0.00001427
Iteration 184/1000 | Loss: 0.00001427
Iteration 185/1000 | Loss: 0.00001427
Iteration 186/1000 | Loss: 0.00001427
Iteration 187/1000 | Loss: 0.00001427
Iteration 188/1000 | Loss: 0.00001427
Iteration 189/1000 | Loss: 0.00001427
Iteration 190/1000 | Loss: 0.00001427
Iteration 191/1000 | Loss: 0.00001426
Iteration 192/1000 | Loss: 0.00001426
Iteration 193/1000 | Loss: 0.00001425
Iteration 194/1000 | Loss: 0.00001425
Iteration 195/1000 | Loss: 0.00001425
Iteration 196/1000 | Loss: 0.00001425
Iteration 197/1000 | Loss: 0.00001425
Iteration 198/1000 | Loss: 0.00001425
Iteration 199/1000 | Loss: 0.00001425
Iteration 200/1000 | Loss: 0.00001425
Iteration 201/1000 | Loss: 0.00001425
Iteration 202/1000 | Loss: 0.00001425
Iteration 203/1000 | Loss: 0.00001425
Iteration 204/1000 | Loss: 0.00001424
Iteration 205/1000 | Loss: 0.00001424
Iteration 206/1000 | Loss: 0.00001424
Iteration 207/1000 | Loss: 0.00001424
Iteration 208/1000 | Loss: 0.00001424
Iteration 209/1000 | Loss: 0.00001424
Iteration 210/1000 | Loss: 0.00001718
Iteration 211/1000 | Loss: 0.00001428
Iteration 212/1000 | Loss: 0.00002428
Iteration 213/1000 | Loss: 0.00001423
Iteration 214/1000 | Loss: 0.00001423
Iteration 215/1000 | Loss: 0.00001423
Iteration 216/1000 | Loss: 0.00001423
Iteration 217/1000 | Loss: 0.00001423
Iteration 218/1000 | Loss: 0.00001423
Iteration 219/1000 | Loss: 0.00001423
Iteration 220/1000 | Loss: 0.00001423
Iteration 221/1000 | Loss: 0.00001423
Iteration 222/1000 | Loss: 0.00001422
Iteration 223/1000 | Loss: 0.00001422
Iteration 224/1000 | Loss: 0.00001422
Iteration 225/1000 | Loss: 0.00001422
Iteration 226/1000 | Loss: 0.00001422
Iteration 227/1000 | Loss: 0.00001422
Iteration 228/1000 | Loss: 0.00001422
Iteration 229/1000 | Loss: 0.00001421
Iteration 230/1000 | Loss: 0.00001421
Iteration 231/1000 | Loss: 0.00001421
Iteration 232/1000 | Loss: 0.00001421
Iteration 233/1000 | Loss: 0.00001421
Iteration 234/1000 | Loss: 0.00001421
Iteration 235/1000 | Loss: 0.00001421
Iteration 236/1000 | Loss: 0.00001421
Iteration 237/1000 | Loss: 0.00001421
Iteration 238/1000 | Loss: 0.00001421
Iteration 239/1000 | Loss: 0.00001421
Iteration 240/1000 | Loss: 0.00001421
Iteration 241/1000 | Loss: 0.00001421
Iteration 242/1000 | Loss: 0.00001600
Iteration 243/1000 | Loss: 0.00001487
Iteration 244/1000 | Loss: 0.00001421
Iteration 245/1000 | Loss: 0.00001421
Iteration 246/1000 | Loss: 0.00001421
Iteration 247/1000 | Loss: 0.00001421
Iteration 248/1000 | Loss: 0.00001421
Iteration 249/1000 | Loss: 0.00001421
Iteration 250/1000 | Loss: 0.00001421
Iteration 251/1000 | Loss: 0.00001421
Iteration 252/1000 | Loss: 0.00001421
Iteration 253/1000 | Loss: 0.00001421
Iteration 254/1000 | Loss: 0.00001421
Iteration 255/1000 | Loss: 0.00001421
Iteration 256/1000 | Loss: 0.00001421
Iteration 257/1000 | Loss: 0.00001421
Iteration 258/1000 | Loss: 0.00001421
Iteration 259/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.4208540960680693e-05, 1.4208540960680693e-05, 1.4208540960680693e-05, 1.4208540960680693e-05, 1.4208540960680693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4208540960680693e-05

Optimization complete. Final v2v error: 2.9160468578338623 mm

Highest mean error: 10.260669708251953 mm for frame 184

Lowest mean error: 2.461139678955078 mm for frame 62

Saving results

Total time: 287.90359830856323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852586
Iteration 2/25 | Loss: 0.00098793
Iteration 3/25 | Loss: 0.00077556
Iteration 4/25 | Loss: 0.00073343
Iteration 5/25 | Loss: 0.00072036
Iteration 6/25 | Loss: 0.00071597
Iteration 7/25 | Loss: 0.00071458
Iteration 8/25 | Loss: 0.00071418
Iteration 9/25 | Loss: 0.00071406
Iteration 10/25 | Loss: 0.00071406
Iteration 11/25 | Loss: 0.00071406
Iteration 12/25 | Loss: 0.00071406
Iteration 13/25 | Loss: 0.00071406
Iteration 14/25 | Loss: 0.00071406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007140613161027431, 0.0007140613161027431, 0.0007140613161027431, 0.0007140613161027431, 0.0007140613161027431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007140613161027431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51715291
Iteration 2/25 | Loss: 0.00045961
Iteration 3/25 | Loss: 0.00045961
Iteration 4/25 | Loss: 0.00045961
Iteration 5/25 | Loss: 0.00045961
Iteration 6/25 | Loss: 0.00045961
Iteration 7/25 | Loss: 0.00045961
Iteration 8/25 | Loss: 0.00045961
Iteration 9/25 | Loss: 0.00045961
Iteration 10/25 | Loss: 0.00045961
Iteration 11/25 | Loss: 0.00045961
Iteration 12/25 | Loss: 0.00045961
Iteration 13/25 | Loss: 0.00045961
Iteration 14/25 | Loss: 0.00045961
Iteration 15/25 | Loss: 0.00045961
Iteration 16/25 | Loss: 0.00045961
Iteration 17/25 | Loss: 0.00045961
Iteration 18/25 | Loss: 0.00045961
Iteration 19/25 | Loss: 0.00045961
Iteration 20/25 | Loss: 0.00045961
Iteration 21/25 | Loss: 0.00045961
Iteration 22/25 | Loss: 0.00045961
Iteration 23/25 | Loss: 0.00045961
Iteration 24/25 | Loss: 0.00045961
Iteration 25/25 | Loss: 0.00045961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045961
Iteration 2/1000 | Loss: 0.00002096
Iteration 3/1000 | Loss: 0.00001332
Iteration 4/1000 | Loss: 0.00001206
Iteration 5/1000 | Loss: 0.00001141
Iteration 6/1000 | Loss: 0.00001086
Iteration 7/1000 | Loss: 0.00001065
Iteration 8/1000 | Loss: 0.00001055
Iteration 9/1000 | Loss: 0.00001051
Iteration 10/1000 | Loss: 0.00001046
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001045
Iteration 13/1000 | Loss: 0.00001044
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001039
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001033
Iteration 21/1000 | Loss: 0.00001031
Iteration 22/1000 | Loss: 0.00001027
Iteration 23/1000 | Loss: 0.00001023
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001020
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001019
Iteration 30/1000 | Loss: 0.00001019
Iteration 31/1000 | Loss: 0.00001018
Iteration 32/1000 | Loss: 0.00001017
Iteration 33/1000 | Loss: 0.00001017
Iteration 34/1000 | Loss: 0.00001015
Iteration 35/1000 | Loss: 0.00001014
Iteration 36/1000 | Loss: 0.00001014
Iteration 37/1000 | Loss: 0.00001014
Iteration 38/1000 | Loss: 0.00001013
Iteration 39/1000 | Loss: 0.00001013
Iteration 40/1000 | Loss: 0.00001012
Iteration 41/1000 | Loss: 0.00001012
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001011
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001010
Iteration 48/1000 | Loss: 0.00001010
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001009
Iteration 51/1000 | Loss: 0.00001009
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001008
Iteration 54/1000 | Loss: 0.00001008
Iteration 55/1000 | Loss: 0.00001008
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001007
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001007
Iteration 63/1000 | Loss: 0.00001006
Iteration 64/1000 | Loss: 0.00001006
Iteration 65/1000 | Loss: 0.00001006
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001004
Iteration 74/1000 | Loss: 0.00001003
Iteration 75/1000 | Loss: 0.00001003
Iteration 76/1000 | Loss: 0.00001003
Iteration 77/1000 | Loss: 0.00001003
Iteration 78/1000 | Loss: 0.00001003
Iteration 79/1000 | Loss: 0.00001003
Iteration 80/1000 | Loss: 0.00001003
Iteration 81/1000 | Loss: 0.00001003
Iteration 82/1000 | Loss: 0.00001003
Iteration 83/1000 | Loss: 0.00001002
Iteration 84/1000 | Loss: 0.00001002
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001001
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000998
Iteration 101/1000 | Loss: 0.00000998
Iteration 102/1000 | Loss: 0.00000998
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000996
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000996
Iteration 111/1000 | Loss: 0.00000996
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000996
Iteration 115/1000 | Loss: 0.00000995
Iteration 116/1000 | Loss: 0.00000995
Iteration 117/1000 | Loss: 0.00000995
Iteration 118/1000 | Loss: 0.00000995
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000995
Iteration 124/1000 | Loss: 0.00000995
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000995
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000994
Iteration 132/1000 | Loss: 0.00000994
Iteration 133/1000 | Loss: 0.00000994
Iteration 134/1000 | Loss: 0.00000994
Iteration 135/1000 | Loss: 0.00000994
Iteration 136/1000 | Loss: 0.00000994
Iteration 137/1000 | Loss: 0.00000994
Iteration 138/1000 | Loss: 0.00000994
Iteration 139/1000 | Loss: 0.00000994
Iteration 140/1000 | Loss: 0.00000994
Iteration 141/1000 | Loss: 0.00000994
Iteration 142/1000 | Loss: 0.00000993
Iteration 143/1000 | Loss: 0.00000993
Iteration 144/1000 | Loss: 0.00000993
Iteration 145/1000 | Loss: 0.00000993
Iteration 146/1000 | Loss: 0.00000993
Iteration 147/1000 | Loss: 0.00000993
Iteration 148/1000 | Loss: 0.00000993
Iteration 149/1000 | Loss: 0.00000993
Iteration 150/1000 | Loss: 0.00000992
Iteration 151/1000 | Loss: 0.00000992
Iteration 152/1000 | Loss: 0.00000992
Iteration 153/1000 | Loss: 0.00000992
Iteration 154/1000 | Loss: 0.00000992
Iteration 155/1000 | Loss: 0.00000992
Iteration 156/1000 | Loss: 0.00000992
Iteration 157/1000 | Loss: 0.00000992
Iteration 158/1000 | Loss: 0.00000991
Iteration 159/1000 | Loss: 0.00000991
Iteration 160/1000 | Loss: 0.00000991
Iteration 161/1000 | Loss: 0.00000991
Iteration 162/1000 | Loss: 0.00000990
Iteration 163/1000 | Loss: 0.00000990
Iteration 164/1000 | Loss: 0.00000990
Iteration 165/1000 | Loss: 0.00000990
Iteration 166/1000 | Loss: 0.00000990
Iteration 167/1000 | Loss: 0.00000990
Iteration 168/1000 | Loss: 0.00000990
Iteration 169/1000 | Loss: 0.00000990
Iteration 170/1000 | Loss: 0.00000990
Iteration 171/1000 | Loss: 0.00000990
Iteration 172/1000 | Loss: 0.00000990
Iteration 173/1000 | Loss: 0.00000990
Iteration 174/1000 | Loss: 0.00000990
Iteration 175/1000 | Loss: 0.00000990
Iteration 176/1000 | Loss: 0.00000990
Iteration 177/1000 | Loss: 0.00000990
Iteration 178/1000 | Loss: 0.00000990
Iteration 179/1000 | Loss: 0.00000990
Iteration 180/1000 | Loss: 0.00000990
Iteration 181/1000 | Loss: 0.00000990
Iteration 182/1000 | Loss: 0.00000990
Iteration 183/1000 | Loss: 0.00000989
Iteration 184/1000 | Loss: 0.00000989
Iteration 185/1000 | Loss: 0.00000989
Iteration 186/1000 | Loss: 0.00000989
Iteration 187/1000 | Loss: 0.00000989
Iteration 188/1000 | Loss: 0.00000989
Iteration 189/1000 | Loss: 0.00000989
Iteration 190/1000 | Loss: 0.00000989
Iteration 191/1000 | Loss: 0.00000989
Iteration 192/1000 | Loss: 0.00000989
Iteration 193/1000 | Loss: 0.00000989
Iteration 194/1000 | Loss: 0.00000989
Iteration 195/1000 | Loss: 0.00000989
Iteration 196/1000 | Loss: 0.00000989
Iteration 197/1000 | Loss: 0.00000989
Iteration 198/1000 | Loss: 0.00000989
Iteration 199/1000 | Loss: 0.00000989
Iteration 200/1000 | Loss: 0.00000989
Iteration 201/1000 | Loss: 0.00000989
Iteration 202/1000 | Loss: 0.00000989
Iteration 203/1000 | Loss: 0.00000989
Iteration 204/1000 | Loss: 0.00000989
Iteration 205/1000 | Loss: 0.00000989
Iteration 206/1000 | Loss: 0.00000989
Iteration 207/1000 | Loss: 0.00000989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [9.89403542916989e-06, 9.89403542916989e-06, 9.89403542916989e-06, 9.89403542916989e-06, 9.89403542916989e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.89403542916989e-06

Optimization complete. Final v2v error: 2.6509711742401123 mm

Highest mean error: 3.748392105102539 mm for frame 89

Lowest mean error: 2.4326837062835693 mm for frame 16

Saving results

Total time: 46.849409341812134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900258
Iteration 2/25 | Loss: 0.00148458
Iteration 3/25 | Loss: 0.00101459
Iteration 4/25 | Loss: 0.00095136
Iteration 5/25 | Loss: 0.00094208
Iteration 6/25 | Loss: 0.00094366
Iteration 7/25 | Loss: 0.00093742
Iteration 8/25 | Loss: 0.00091720
Iteration 9/25 | Loss: 0.00090578
Iteration 10/25 | Loss: 0.00090239
Iteration 11/25 | Loss: 0.00090102
Iteration 12/25 | Loss: 0.00090044
Iteration 13/25 | Loss: 0.00090018
Iteration 14/25 | Loss: 0.00090148
Iteration 15/25 | Loss: 0.00090249
Iteration 16/25 | Loss: 0.00089637
Iteration 17/25 | Loss: 0.00089454
Iteration 18/25 | Loss: 0.00089396
Iteration 19/25 | Loss: 0.00089378
Iteration 20/25 | Loss: 0.00089378
Iteration 21/25 | Loss: 0.00089378
Iteration 22/25 | Loss: 0.00089378
Iteration 23/25 | Loss: 0.00089378
Iteration 24/25 | Loss: 0.00089378
Iteration 25/25 | Loss: 0.00089378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02657676
Iteration 2/25 | Loss: 0.00046284
Iteration 3/25 | Loss: 0.00046283
Iteration 4/25 | Loss: 0.00046283
Iteration 5/25 | Loss: 0.00046283
Iteration 6/25 | Loss: 0.00046283
Iteration 7/25 | Loss: 0.00046283
Iteration 8/25 | Loss: 0.00046283
Iteration 9/25 | Loss: 0.00046283
Iteration 10/25 | Loss: 0.00046283
Iteration 11/25 | Loss: 0.00046283
Iteration 12/25 | Loss: 0.00046283
Iteration 13/25 | Loss: 0.00046283
Iteration 14/25 | Loss: 0.00046283
Iteration 15/25 | Loss: 0.00046283
Iteration 16/25 | Loss: 0.00046283
Iteration 17/25 | Loss: 0.00046283
Iteration 18/25 | Loss: 0.00046283
Iteration 19/25 | Loss: 0.00046283
Iteration 20/25 | Loss: 0.00046283
Iteration 21/25 | Loss: 0.00046283
Iteration 22/25 | Loss: 0.00046283
Iteration 23/25 | Loss: 0.00046283
Iteration 24/25 | Loss: 0.00046283
Iteration 25/25 | Loss: 0.00046283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046283
Iteration 2/1000 | Loss: 0.00004324
Iteration 3/1000 | Loss: 0.00003444
Iteration 4/1000 | Loss: 0.00003238
Iteration 5/1000 | Loss: 0.00003064
Iteration 6/1000 | Loss: 0.00002952
Iteration 7/1000 | Loss: 0.00002858
Iteration 8/1000 | Loss: 0.00002811
Iteration 9/1000 | Loss: 0.00002788
Iteration 10/1000 | Loss: 0.00002771
Iteration 11/1000 | Loss: 0.00002767
Iteration 12/1000 | Loss: 0.00002767
Iteration 13/1000 | Loss: 0.00002767
Iteration 14/1000 | Loss: 0.00002767
Iteration 15/1000 | Loss: 0.00002767
Iteration 16/1000 | Loss: 0.00002767
Iteration 17/1000 | Loss: 0.00002767
Iteration 18/1000 | Loss: 0.00002767
Iteration 19/1000 | Loss: 0.00002767
Iteration 20/1000 | Loss: 0.00002767
Iteration 21/1000 | Loss: 0.00002766
Iteration 22/1000 | Loss: 0.00002766
Iteration 23/1000 | Loss: 0.00002765
Iteration 24/1000 | Loss: 0.00002758
Iteration 25/1000 | Loss: 0.00002757
Iteration 26/1000 | Loss: 0.00002757
Iteration 27/1000 | Loss: 0.00002757
Iteration 28/1000 | Loss: 0.00002757
Iteration 29/1000 | Loss: 0.00002757
Iteration 30/1000 | Loss: 0.00002757
Iteration 31/1000 | Loss: 0.00002757
Iteration 32/1000 | Loss: 0.00002756
Iteration 33/1000 | Loss: 0.00002756
Iteration 34/1000 | Loss: 0.00002755
Iteration 35/1000 | Loss: 0.00002753
Iteration 36/1000 | Loss: 0.00002747
Iteration 37/1000 | Loss: 0.00002747
Iteration 38/1000 | Loss: 0.00002747
Iteration 39/1000 | Loss: 0.00002746
Iteration 40/1000 | Loss: 0.00002746
Iteration 41/1000 | Loss: 0.00002746
Iteration 42/1000 | Loss: 0.00002746
Iteration 43/1000 | Loss: 0.00002746
Iteration 44/1000 | Loss: 0.00002746
Iteration 45/1000 | Loss: 0.00002746
Iteration 46/1000 | Loss: 0.00002746
Iteration 47/1000 | Loss: 0.00002746
Iteration 48/1000 | Loss: 0.00002746
Iteration 49/1000 | Loss: 0.00002746
Iteration 50/1000 | Loss: 0.00002746
Iteration 51/1000 | Loss: 0.00002746
Iteration 52/1000 | Loss: 0.00002746
Iteration 53/1000 | Loss: 0.00002746
Iteration 54/1000 | Loss: 0.00002746
Iteration 55/1000 | Loss: 0.00002746
Iteration 56/1000 | Loss: 0.00002746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [2.745597521425225e-05, 2.745597521425225e-05, 2.745597521425225e-05, 2.745597521425225e-05, 2.745597521425225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.745597521425225e-05

Optimization complete. Final v2v error: 4.410848140716553 mm

Highest mean error: 4.638192176818848 mm for frame 42

Lowest mean error: 4.271596431732178 mm for frame 144

Saving results

Total time: 53.561827659606934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870602
Iteration 2/25 | Loss: 0.00106155
Iteration 3/25 | Loss: 0.00079235
Iteration 4/25 | Loss: 0.00076082
Iteration 5/25 | Loss: 0.00075196
Iteration 6/25 | Loss: 0.00075073
Iteration 7/25 | Loss: 0.00075073
Iteration 8/25 | Loss: 0.00075073
Iteration 9/25 | Loss: 0.00075073
Iteration 10/25 | Loss: 0.00075073
Iteration 11/25 | Loss: 0.00075073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007507260888814926, 0.0007507260888814926, 0.0007507260888814926, 0.0007507260888814926, 0.0007507260888814926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007507260888814926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43157172
Iteration 2/25 | Loss: 0.00051834
Iteration 3/25 | Loss: 0.00051834
Iteration 4/25 | Loss: 0.00051834
Iteration 5/25 | Loss: 0.00051834
Iteration 6/25 | Loss: 0.00051834
Iteration 7/25 | Loss: 0.00051834
Iteration 8/25 | Loss: 0.00051834
Iteration 9/25 | Loss: 0.00051834
Iteration 10/25 | Loss: 0.00051834
Iteration 11/25 | Loss: 0.00051834
Iteration 12/25 | Loss: 0.00051834
Iteration 13/25 | Loss: 0.00051834
Iteration 14/25 | Loss: 0.00051834
Iteration 15/25 | Loss: 0.00051834
Iteration 16/25 | Loss: 0.00051834
Iteration 17/25 | Loss: 0.00051834
Iteration 18/25 | Loss: 0.00051834
Iteration 19/25 | Loss: 0.00051834
Iteration 20/25 | Loss: 0.00051834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005183377070352435, 0.0005183377070352435, 0.0005183377070352435, 0.0005183377070352435, 0.0005183377070352435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005183377070352435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051834
Iteration 2/1000 | Loss: 0.00002434
Iteration 3/1000 | Loss: 0.00001784
Iteration 4/1000 | Loss: 0.00001632
Iteration 5/1000 | Loss: 0.00001554
Iteration 6/1000 | Loss: 0.00001517
Iteration 7/1000 | Loss: 0.00001494
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001447
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001432
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001426
Iteration 17/1000 | Loss: 0.00001421
Iteration 18/1000 | Loss: 0.00001414
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001413
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001410
Iteration 28/1000 | Loss: 0.00001410
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00001409
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001408
Iteration 33/1000 | Loss: 0.00001408
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001408
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001402
Iteration 46/1000 | Loss: 0.00001401
Iteration 47/1000 | Loss: 0.00001401
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001396
Iteration 57/1000 | Loss: 0.00001396
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001395
Iteration 62/1000 | Loss: 0.00001395
Iteration 63/1000 | Loss: 0.00001395
Iteration 64/1000 | Loss: 0.00001395
Iteration 65/1000 | Loss: 0.00001395
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001393
Iteration 70/1000 | Loss: 0.00001393
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001391
Iteration 76/1000 | Loss: 0.00001391
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001390
Iteration 83/1000 | Loss: 0.00001390
Iteration 84/1000 | Loss: 0.00001390
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001387
Iteration 97/1000 | Loss: 0.00001387
Iteration 98/1000 | Loss: 0.00001387
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001386
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001385
Iteration 104/1000 | Loss: 0.00001385
Iteration 105/1000 | Loss: 0.00001385
Iteration 106/1000 | Loss: 0.00001385
Iteration 107/1000 | Loss: 0.00001385
Iteration 108/1000 | Loss: 0.00001385
Iteration 109/1000 | Loss: 0.00001384
Iteration 110/1000 | Loss: 0.00001384
Iteration 111/1000 | Loss: 0.00001384
Iteration 112/1000 | Loss: 0.00001384
Iteration 113/1000 | Loss: 0.00001384
Iteration 114/1000 | Loss: 0.00001384
Iteration 115/1000 | Loss: 0.00001384
Iteration 116/1000 | Loss: 0.00001384
Iteration 117/1000 | Loss: 0.00001384
Iteration 118/1000 | Loss: 0.00001384
Iteration 119/1000 | Loss: 0.00001383
Iteration 120/1000 | Loss: 0.00001383
Iteration 121/1000 | Loss: 0.00001383
Iteration 122/1000 | Loss: 0.00001383
Iteration 123/1000 | Loss: 0.00001383
Iteration 124/1000 | Loss: 0.00001383
Iteration 125/1000 | Loss: 0.00001383
Iteration 126/1000 | Loss: 0.00001383
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001382
Iteration 130/1000 | Loss: 0.00001382
Iteration 131/1000 | Loss: 0.00001382
Iteration 132/1000 | Loss: 0.00001382
Iteration 133/1000 | Loss: 0.00001382
Iteration 134/1000 | Loss: 0.00001382
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001381
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001379
Iteration 170/1000 | Loss: 0.00001379
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001377
Iteration 183/1000 | Loss: 0.00001377
Iteration 184/1000 | Loss: 0.00001377
Iteration 185/1000 | Loss: 0.00001377
Iteration 186/1000 | Loss: 0.00001377
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001376
Iteration 193/1000 | Loss: 0.00001376
Iteration 194/1000 | Loss: 0.00001376
Iteration 195/1000 | Loss: 0.00001376
Iteration 196/1000 | Loss: 0.00001376
Iteration 197/1000 | Loss: 0.00001376
Iteration 198/1000 | Loss: 0.00001375
Iteration 199/1000 | Loss: 0.00001375
Iteration 200/1000 | Loss: 0.00001375
Iteration 201/1000 | Loss: 0.00001375
Iteration 202/1000 | Loss: 0.00001375
Iteration 203/1000 | Loss: 0.00001375
Iteration 204/1000 | Loss: 0.00001375
Iteration 205/1000 | Loss: 0.00001375
Iteration 206/1000 | Loss: 0.00001375
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.3750398466072511e-05, 1.3750398466072511e-05, 1.3750398466072511e-05, 1.3750398466072511e-05, 1.3750398466072511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3750398466072511e-05

Optimization complete. Final v2v error: 2.9784023761749268 mm

Highest mean error: 3.9350249767303467 mm for frame 226

Lowest mean error: 2.4821982383728027 mm for frame 157

Saving results

Total time: 45.46868944168091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736368
Iteration 2/25 | Loss: 0.00094932
Iteration 3/25 | Loss: 0.00077294
Iteration 4/25 | Loss: 0.00074734
Iteration 5/25 | Loss: 0.00074091
Iteration 6/25 | Loss: 0.00073913
Iteration 7/25 | Loss: 0.00073869
Iteration 8/25 | Loss: 0.00073869
Iteration 9/25 | Loss: 0.00073869
Iteration 10/25 | Loss: 0.00073869
Iteration 11/25 | Loss: 0.00073869
Iteration 12/25 | Loss: 0.00073869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007386883953586221, 0.0007386883953586221, 0.0007386883953586221, 0.0007386883953586221, 0.0007386883953586221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007386883953586221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62012792
Iteration 2/25 | Loss: 0.00047027
Iteration 3/25 | Loss: 0.00047027
Iteration 4/25 | Loss: 0.00047027
Iteration 5/25 | Loss: 0.00047026
Iteration 6/25 | Loss: 0.00047026
Iteration 7/25 | Loss: 0.00047026
Iteration 8/25 | Loss: 0.00047026
Iteration 9/25 | Loss: 0.00047026
Iteration 10/25 | Loss: 0.00047026
Iteration 11/25 | Loss: 0.00047026
Iteration 12/25 | Loss: 0.00047026
Iteration 13/25 | Loss: 0.00047026
Iteration 14/25 | Loss: 0.00047026
Iteration 15/25 | Loss: 0.00047026
Iteration 16/25 | Loss: 0.00047026
Iteration 17/25 | Loss: 0.00047026
Iteration 18/25 | Loss: 0.00047026
Iteration 19/25 | Loss: 0.00047026
Iteration 20/25 | Loss: 0.00047026
Iteration 21/25 | Loss: 0.00047026
Iteration 22/25 | Loss: 0.00047026
Iteration 23/25 | Loss: 0.00047026
Iteration 24/25 | Loss: 0.00047026
Iteration 25/25 | Loss: 0.00047026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047026
Iteration 2/1000 | Loss: 0.00002461
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001268
Iteration 10/1000 | Loss: 0.00001262
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001243
Iteration 13/1000 | Loss: 0.00001242
Iteration 14/1000 | Loss: 0.00001242
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001237
Iteration 18/1000 | Loss: 0.00001236
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001227
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001221
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001216
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001214
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00001213
Iteration 32/1000 | Loss: 0.00001213
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001209
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001209
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001208
Iteration 49/1000 | Loss: 0.00001208
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00001207
Iteration 52/1000 | Loss: 0.00001207
Iteration 53/1000 | Loss: 0.00001207
Iteration 54/1000 | Loss: 0.00001206
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001197
Iteration 70/1000 | Loss: 0.00001197
Iteration 71/1000 | Loss: 0.00001197
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001196
Iteration 74/1000 | Loss: 0.00001196
Iteration 75/1000 | Loss: 0.00001196
Iteration 76/1000 | Loss: 0.00001196
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.1915474715351593e-05, 1.1915474715351593e-05, 1.1915474715351593e-05, 1.1915474715351593e-05, 1.1915474715351593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1915474715351593e-05

Optimization complete. Final v2v error: 2.9558725357055664 mm

Highest mean error: 3.3667283058166504 mm for frame 113

Lowest mean error: 2.6547536849975586 mm for frame 34

Saving results

Total time: 38.282506704330444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506217
Iteration 2/25 | Loss: 0.00107441
Iteration 3/25 | Loss: 0.00087164
Iteration 4/25 | Loss: 0.00082177
Iteration 5/25 | Loss: 0.00080598
Iteration 6/25 | Loss: 0.00080315
Iteration 7/25 | Loss: 0.00080205
Iteration 8/25 | Loss: 0.00080169
Iteration 9/25 | Loss: 0.00080157
Iteration 10/25 | Loss: 0.00080156
Iteration 11/25 | Loss: 0.00080156
Iteration 12/25 | Loss: 0.00080156
Iteration 13/25 | Loss: 0.00080156
Iteration 14/25 | Loss: 0.00080155
Iteration 15/25 | Loss: 0.00080155
Iteration 16/25 | Loss: 0.00080155
Iteration 17/25 | Loss: 0.00080155
Iteration 18/25 | Loss: 0.00080155
Iteration 19/25 | Loss: 0.00080155
Iteration 20/25 | Loss: 0.00080155
Iteration 21/25 | Loss: 0.00080155
Iteration 22/25 | Loss: 0.00080155
Iteration 23/25 | Loss: 0.00080155
Iteration 24/25 | Loss: 0.00080155
Iteration 25/25 | Loss: 0.00080155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56135988
Iteration 2/25 | Loss: 0.00072076
Iteration 3/25 | Loss: 0.00072075
Iteration 4/25 | Loss: 0.00072075
Iteration 5/25 | Loss: 0.00072075
Iteration 6/25 | Loss: 0.00072075
Iteration 7/25 | Loss: 0.00072075
Iteration 8/25 | Loss: 0.00072075
Iteration 9/25 | Loss: 0.00072075
Iteration 10/25 | Loss: 0.00072075
Iteration 11/25 | Loss: 0.00072075
Iteration 12/25 | Loss: 0.00072075
Iteration 13/25 | Loss: 0.00072075
Iteration 14/25 | Loss: 0.00072075
Iteration 15/25 | Loss: 0.00072075
Iteration 16/25 | Loss: 0.00072075
Iteration 17/25 | Loss: 0.00072075
Iteration 18/25 | Loss: 0.00072075
Iteration 19/25 | Loss: 0.00072075
Iteration 20/25 | Loss: 0.00072075
Iteration 21/25 | Loss: 0.00072075
Iteration 22/25 | Loss: 0.00072075
Iteration 23/25 | Loss: 0.00072075
Iteration 24/25 | Loss: 0.00072075
Iteration 25/25 | Loss: 0.00072075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072075
Iteration 2/1000 | Loss: 0.00007353
Iteration 3/1000 | Loss: 0.00007971
Iteration 4/1000 | Loss: 0.00003592
Iteration 5/1000 | Loss: 0.00006436
Iteration 6/1000 | Loss: 0.00002806
Iteration 7/1000 | Loss: 0.00006143
Iteration 8/1000 | Loss: 0.00005269
Iteration 9/1000 | Loss: 0.00006026
Iteration 10/1000 | Loss: 0.00004878
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002276
Iteration 13/1000 | Loss: 0.00003051
Iteration 14/1000 | Loss: 0.00006046
Iteration 15/1000 | Loss: 0.00004037
Iteration 16/1000 | Loss: 0.00003437
Iteration 17/1000 | Loss: 0.00003378
Iteration 18/1000 | Loss: 0.00003323
Iteration 19/1000 | Loss: 0.00005733
Iteration 20/1000 | Loss: 0.00004254
Iteration 21/1000 | Loss: 0.00004034
Iteration 22/1000 | Loss: 0.00003220
Iteration 23/1000 | Loss: 0.00003946
Iteration 24/1000 | Loss: 0.00004316
Iteration 25/1000 | Loss: 0.00004363
Iteration 26/1000 | Loss: 0.00003841
Iteration 27/1000 | Loss: 0.00004038
Iteration 28/1000 | Loss: 0.00005354
Iteration 29/1000 | Loss: 0.00003780
Iteration 30/1000 | Loss: 0.00005315
Iteration 31/1000 | Loss: 0.00004296
Iteration 32/1000 | Loss: 0.00005331
Iteration 33/1000 | Loss: 0.00004250
Iteration 34/1000 | Loss: 0.00004182
Iteration 35/1000 | Loss: 0.00004803
Iteration 36/1000 | Loss: 0.00005148
Iteration 37/1000 | Loss: 0.00002625
Iteration 38/1000 | Loss: 0.00002612
Iteration 39/1000 | Loss: 0.00005814
Iteration 40/1000 | Loss: 0.00003804
Iteration 41/1000 | Loss: 0.00004704
Iteration 42/1000 | Loss: 0.00004954
Iteration 43/1000 | Loss: 0.00004531
Iteration 44/1000 | Loss: 0.00005616
Iteration 45/1000 | Loss: 0.00004455
Iteration 46/1000 | Loss: 0.00004922
Iteration 47/1000 | Loss: 0.00004334
Iteration 48/1000 | Loss: 0.00004819
Iteration 49/1000 | Loss: 0.00004490
Iteration 50/1000 | Loss: 0.00006695
Iteration 51/1000 | Loss: 0.00003654
Iteration 52/1000 | Loss: 0.00006018
Iteration 53/1000 | Loss: 0.00004582
Iteration 54/1000 | Loss: 0.00004946
Iteration 55/1000 | Loss: 0.00004367
Iteration 56/1000 | Loss: 0.00004916
Iteration 57/1000 | Loss: 0.00004517
Iteration 58/1000 | Loss: 0.00004626
Iteration 59/1000 | Loss: 0.00004377
Iteration 60/1000 | Loss: 0.00006007
Iteration 61/1000 | Loss: 0.00004675
Iteration 62/1000 | Loss: 0.00006112
Iteration 63/1000 | Loss: 0.00004806
Iteration 64/1000 | Loss: 0.00005598
Iteration 65/1000 | Loss: 0.00004123
Iteration 66/1000 | Loss: 0.00005298
Iteration 67/1000 | Loss: 0.00003689
Iteration 68/1000 | Loss: 0.00005095
Iteration 69/1000 | Loss: 0.00003454
Iteration 70/1000 | Loss: 0.00004931
Iteration 71/1000 | Loss: 0.00002305
Iteration 72/1000 | Loss: 0.00002198
Iteration 73/1000 | Loss: 0.00002063
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001824
Iteration 78/1000 | Loss: 0.00001820
Iteration 79/1000 | Loss: 0.00001811
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001806
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001805
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001805
Iteration 96/1000 | Loss: 0.00001805
Iteration 97/1000 | Loss: 0.00001804
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001804
Iteration 101/1000 | Loss: 0.00001803
Iteration 102/1000 | Loss: 0.00001802
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001801
Iteration 107/1000 | Loss: 0.00001800
Iteration 108/1000 | Loss: 0.00001800
Iteration 109/1000 | Loss: 0.00001800
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001799
Iteration 112/1000 | Loss: 0.00001798
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001797
Iteration 116/1000 | Loss: 0.00001797
Iteration 117/1000 | Loss: 0.00001797
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001792
Iteration 120/1000 | Loss: 0.00001791
Iteration 121/1000 | Loss: 0.00001790
Iteration 122/1000 | Loss: 0.00001788
Iteration 123/1000 | Loss: 0.00001788
Iteration 124/1000 | Loss: 0.00001787
Iteration 125/1000 | Loss: 0.00001787
Iteration 126/1000 | Loss: 0.00001786
Iteration 127/1000 | Loss: 0.00001785
Iteration 128/1000 | Loss: 0.00001785
Iteration 129/1000 | Loss: 0.00001784
Iteration 130/1000 | Loss: 0.00001784
Iteration 131/1000 | Loss: 0.00001784
Iteration 132/1000 | Loss: 0.00001784
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001781
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001779
Iteration 144/1000 | Loss: 0.00001779
Iteration 145/1000 | Loss: 0.00001779
Iteration 146/1000 | Loss: 0.00001778
Iteration 147/1000 | Loss: 0.00001778
Iteration 148/1000 | Loss: 0.00001778
Iteration 149/1000 | Loss: 0.00001778
Iteration 150/1000 | Loss: 0.00001777
Iteration 151/1000 | Loss: 0.00001777
Iteration 152/1000 | Loss: 0.00001777
Iteration 153/1000 | Loss: 0.00001777
Iteration 154/1000 | Loss: 0.00001777
Iteration 155/1000 | Loss: 0.00001776
Iteration 156/1000 | Loss: 0.00001776
Iteration 157/1000 | Loss: 0.00001776
Iteration 158/1000 | Loss: 0.00001776
Iteration 159/1000 | Loss: 0.00001776
Iteration 160/1000 | Loss: 0.00001776
Iteration 161/1000 | Loss: 0.00001775
Iteration 162/1000 | Loss: 0.00001775
Iteration 163/1000 | Loss: 0.00001775
Iteration 164/1000 | Loss: 0.00001774
Iteration 165/1000 | Loss: 0.00001774
Iteration 166/1000 | Loss: 0.00001773
Iteration 167/1000 | Loss: 0.00001773
Iteration 168/1000 | Loss: 0.00001772
Iteration 169/1000 | Loss: 0.00001772
Iteration 170/1000 | Loss: 0.00001772
Iteration 171/1000 | Loss: 0.00001772
Iteration 172/1000 | Loss: 0.00001772
Iteration 173/1000 | Loss: 0.00001771
Iteration 174/1000 | Loss: 0.00001771
Iteration 175/1000 | Loss: 0.00001771
Iteration 176/1000 | Loss: 0.00001771
Iteration 177/1000 | Loss: 0.00001770
Iteration 178/1000 | Loss: 0.00001770
Iteration 179/1000 | Loss: 0.00001770
Iteration 180/1000 | Loss: 0.00001770
Iteration 181/1000 | Loss: 0.00001769
Iteration 182/1000 | Loss: 0.00001769
Iteration 183/1000 | Loss: 0.00001769
Iteration 184/1000 | Loss: 0.00001769
Iteration 185/1000 | Loss: 0.00001769
Iteration 186/1000 | Loss: 0.00001769
Iteration 187/1000 | Loss: 0.00001769
Iteration 188/1000 | Loss: 0.00001769
Iteration 189/1000 | Loss: 0.00001769
Iteration 190/1000 | Loss: 0.00001769
Iteration 191/1000 | Loss: 0.00001769
Iteration 192/1000 | Loss: 0.00001768
Iteration 193/1000 | Loss: 0.00001768
Iteration 194/1000 | Loss: 0.00001768
Iteration 195/1000 | Loss: 0.00001768
Iteration 196/1000 | Loss: 0.00001768
Iteration 197/1000 | Loss: 0.00001768
Iteration 198/1000 | Loss: 0.00001768
Iteration 199/1000 | Loss: 0.00001768
Iteration 200/1000 | Loss: 0.00001767
Iteration 201/1000 | Loss: 0.00001767
Iteration 202/1000 | Loss: 0.00001767
Iteration 203/1000 | Loss: 0.00001766
Iteration 204/1000 | Loss: 0.00001766
Iteration 205/1000 | Loss: 0.00001766
Iteration 206/1000 | Loss: 0.00001766
Iteration 207/1000 | Loss: 0.00001766
Iteration 208/1000 | Loss: 0.00001766
Iteration 209/1000 | Loss: 0.00001765
Iteration 210/1000 | Loss: 0.00001765
Iteration 211/1000 | Loss: 0.00001765
Iteration 212/1000 | Loss: 0.00001765
Iteration 213/1000 | Loss: 0.00001765
Iteration 214/1000 | Loss: 0.00001765
Iteration 215/1000 | Loss: 0.00001765
Iteration 216/1000 | Loss: 0.00001765
Iteration 217/1000 | Loss: 0.00001765
Iteration 218/1000 | Loss: 0.00001765
Iteration 219/1000 | Loss: 0.00001765
Iteration 220/1000 | Loss: 0.00001765
Iteration 221/1000 | Loss: 0.00001765
Iteration 222/1000 | Loss: 0.00001765
Iteration 223/1000 | Loss: 0.00001764
Iteration 224/1000 | Loss: 0.00001764
Iteration 225/1000 | Loss: 0.00001764
Iteration 226/1000 | Loss: 0.00001764
Iteration 227/1000 | Loss: 0.00001764
Iteration 228/1000 | Loss: 0.00001764
Iteration 229/1000 | Loss: 0.00001764
Iteration 230/1000 | Loss: 0.00001764
Iteration 231/1000 | Loss: 0.00001764
Iteration 232/1000 | Loss: 0.00001764
Iteration 233/1000 | Loss: 0.00001764
Iteration 234/1000 | Loss: 0.00001764
Iteration 235/1000 | Loss: 0.00001764
Iteration 236/1000 | Loss: 0.00001764
Iteration 237/1000 | Loss: 0.00001763
Iteration 238/1000 | Loss: 0.00001763
Iteration 239/1000 | Loss: 0.00001763
Iteration 240/1000 | Loss: 0.00001763
Iteration 241/1000 | Loss: 0.00001763
Iteration 242/1000 | Loss: 0.00001763
Iteration 243/1000 | Loss: 0.00001763
Iteration 244/1000 | Loss: 0.00001763
Iteration 245/1000 | Loss: 0.00001763
Iteration 246/1000 | Loss: 0.00001763
Iteration 247/1000 | Loss: 0.00001763
Iteration 248/1000 | Loss: 0.00001763
Iteration 249/1000 | Loss: 0.00001763
Iteration 250/1000 | Loss: 0.00001763
Iteration 251/1000 | Loss: 0.00001763
Iteration 252/1000 | Loss: 0.00001763
Iteration 253/1000 | Loss: 0.00001763
Iteration 254/1000 | Loss: 0.00001763
Iteration 255/1000 | Loss: 0.00001763
Iteration 256/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.762939791660756e-05, 1.762939791660756e-05, 1.762939791660756e-05, 1.762939791660756e-05, 1.762939791660756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.762939791660756e-05

Optimization complete. Final v2v error: 3.4190022945404053 mm

Highest mean error: 4.91373872756958 mm for frame 94

Lowest mean error: 2.7225570678710938 mm for frame 50

Saving results

Total time: 139.18769931793213
