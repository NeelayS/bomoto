Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=215, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12040-12095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109631
Iteration 2/25 | Loss: 0.00220437
Iteration 3/25 | Loss: 0.00168932
Iteration 4/25 | Loss: 0.00160648
Iteration 5/25 | Loss: 0.00160847
Iteration 6/25 | Loss: 0.00160311
Iteration 7/25 | Loss: 0.00158746
Iteration 8/25 | Loss: 0.00157024
Iteration 9/25 | Loss: 0.00156105
Iteration 10/25 | Loss: 0.00155901
Iteration 11/25 | Loss: 0.00155369
Iteration 12/25 | Loss: 0.00154526
Iteration 13/25 | Loss: 0.00154406
Iteration 14/25 | Loss: 0.00154308
Iteration 15/25 | Loss: 0.00153996
Iteration 16/25 | Loss: 0.00154082
Iteration 17/25 | Loss: 0.00153914
Iteration 18/25 | Loss: 0.00154031
Iteration 19/25 | Loss: 0.00154358
Iteration 20/25 | Loss: 0.00153944
Iteration 21/25 | Loss: 0.00153630
Iteration 22/25 | Loss: 0.00153328
Iteration 23/25 | Loss: 0.00153498
Iteration 24/25 | Loss: 0.00153207
Iteration 25/25 | Loss: 0.00153243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30504930
Iteration 2/25 | Loss: 0.00171611
Iteration 3/25 | Loss: 0.00171611
Iteration 4/25 | Loss: 0.00171611
Iteration 5/25 | Loss: 0.00171611
Iteration 6/25 | Loss: 0.00171611
Iteration 7/25 | Loss: 0.00171611
Iteration 8/25 | Loss: 0.00171611
Iteration 9/25 | Loss: 0.00171611
Iteration 10/25 | Loss: 0.00171611
Iteration 11/25 | Loss: 0.00171611
Iteration 12/25 | Loss: 0.00171611
Iteration 13/25 | Loss: 0.00171611
Iteration 14/25 | Loss: 0.00171611
Iteration 15/25 | Loss: 0.00171611
Iteration 16/25 | Loss: 0.00171611
Iteration 17/25 | Loss: 0.00171611
Iteration 18/25 | Loss: 0.00171611
Iteration 19/25 | Loss: 0.00171611
Iteration 20/25 | Loss: 0.00171611
Iteration 21/25 | Loss: 0.00171611
Iteration 22/25 | Loss: 0.00171611
Iteration 23/25 | Loss: 0.00171611
Iteration 24/25 | Loss: 0.00171611
Iteration 25/25 | Loss: 0.00171611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171611
Iteration 2/1000 | Loss: 0.00026258
Iteration 3/1000 | Loss: 0.00301085
Iteration 4/1000 | Loss: 0.00118276
Iteration 5/1000 | Loss: 0.00189708
Iteration 6/1000 | Loss: 0.00064529
Iteration 7/1000 | Loss: 0.00065810
Iteration 8/1000 | Loss: 0.00099467
Iteration 9/1000 | Loss: 0.00021723
Iteration 10/1000 | Loss: 0.00017171
Iteration 11/1000 | Loss: 0.00007634
Iteration 12/1000 | Loss: 0.00011389
Iteration 13/1000 | Loss: 0.00013551
Iteration 14/1000 | Loss: 0.00009703
Iteration 15/1000 | Loss: 0.00011461
Iteration 16/1000 | Loss: 0.00081140
Iteration 17/1000 | Loss: 0.00034551
Iteration 18/1000 | Loss: 0.00069608
Iteration 19/1000 | Loss: 0.00044880
Iteration 20/1000 | Loss: 0.00026365
Iteration 21/1000 | Loss: 0.00027510
Iteration 22/1000 | Loss: 0.00011599
Iteration 23/1000 | Loss: 0.00019559
Iteration 24/1000 | Loss: 0.00021865
Iteration 25/1000 | Loss: 0.00016544
Iteration 26/1000 | Loss: 0.00013309
Iteration 27/1000 | Loss: 0.00008963
Iteration 28/1000 | Loss: 0.00021660
Iteration 29/1000 | Loss: 0.00010902
Iteration 30/1000 | Loss: 0.00020378
Iteration 31/1000 | Loss: 0.00016443
Iteration 32/1000 | Loss: 0.00017320
Iteration 33/1000 | Loss: 0.00012739
Iteration 34/1000 | Loss: 0.00019381
Iteration 35/1000 | Loss: 0.00024209
Iteration 36/1000 | Loss: 0.00016684
Iteration 37/1000 | Loss: 0.00031035
Iteration 38/1000 | Loss: 0.00014366
Iteration 39/1000 | Loss: 0.00021242
Iteration 40/1000 | Loss: 0.00018112
Iteration 41/1000 | Loss: 0.00014779
Iteration 42/1000 | Loss: 0.00019646
Iteration 43/1000 | Loss: 0.00023113
Iteration 44/1000 | Loss: 0.00017978
Iteration 45/1000 | Loss: 0.00015088
Iteration 46/1000 | Loss: 0.00009563
Iteration 47/1000 | Loss: 0.00018781
Iteration 48/1000 | Loss: 0.00019513
Iteration 49/1000 | Loss: 0.00026617
Iteration 50/1000 | Loss: 0.00013693
Iteration 51/1000 | Loss: 0.00013448
Iteration 52/1000 | Loss: 0.00015437
Iteration 53/1000 | Loss: 0.00008618
Iteration 54/1000 | Loss: 0.00006280
Iteration 55/1000 | Loss: 0.00004977
Iteration 56/1000 | Loss: 0.00010535
Iteration 57/1000 | Loss: 0.00013407
Iteration 58/1000 | Loss: 0.00010685
Iteration 59/1000 | Loss: 0.00013096
Iteration 60/1000 | Loss: 0.00009451
Iteration 61/1000 | Loss: 0.00006126
Iteration 62/1000 | Loss: 0.00007255
Iteration 63/1000 | Loss: 0.00017148
Iteration 64/1000 | Loss: 0.00006328
Iteration 65/1000 | Loss: 0.00021476
Iteration 66/1000 | Loss: 0.00012151
Iteration 67/1000 | Loss: 0.00007148
Iteration 68/1000 | Loss: 0.00012125
Iteration 69/1000 | Loss: 0.00004448
Iteration 70/1000 | Loss: 0.00011897
Iteration 71/1000 | Loss: 0.00012482
Iteration 72/1000 | Loss: 0.00013540
Iteration 73/1000 | Loss: 0.00012778
Iteration 74/1000 | Loss: 0.00012177
Iteration 75/1000 | Loss: 0.00019114
Iteration 76/1000 | Loss: 0.00013900
Iteration 77/1000 | Loss: 0.00013435
Iteration 78/1000 | Loss: 0.00015483
Iteration 79/1000 | Loss: 0.00015339
Iteration 80/1000 | Loss: 0.00008474
Iteration 81/1000 | Loss: 0.00006614
Iteration 82/1000 | Loss: 0.00010071
Iteration 83/1000 | Loss: 0.00014797
Iteration 84/1000 | Loss: 0.00014799
Iteration 85/1000 | Loss: 0.00014973
Iteration 86/1000 | Loss: 0.00015080
Iteration 87/1000 | Loss: 0.00011517
Iteration 88/1000 | Loss: 0.00012293
Iteration 89/1000 | Loss: 0.00013940
Iteration 90/1000 | Loss: 0.00014569
Iteration 91/1000 | Loss: 0.00017015
Iteration 92/1000 | Loss: 0.00007529
Iteration 93/1000 | Loss: 0.00012610
Iteration 94/1000 | Loss: 0.00013369
Iteration 95/1000 | Loss: 0.00014851
Iteration 96/1000 | Loss: 0.00014784
Iteration 97/1000 | Loss: 0.00014818
Iteration 98/1000 | Loss: 0.00014733
Iteration 99/1000 | Loss: 0.00009888
Iteration 100/1000 | Loss: 0.00011998
Iteration 101/1000 | Loss: 0.00010918
Iteration 102/1000 | Loss: 0.00013085
Iteration 103/1000 | Loss: 0.00014443
Iteration 104/1000 | Loss: 0.00014480
Iteration 105/1000 | Loss: 0.00011194
Iteration 106/1000 | Loss: 0.00012614
Iteration 107/1000 | Loss: 0.00010653
Iteration 108/1000 | Loss: 0.00009176
Iteration 109/1000 | Loss: 0.00013772
Iteration 110/1000 | Loss: 0.00013592
Iteration 111/1000 | Loss: 0.00014824
Iteration 112/1000 | Loss: 0.00005517
Iteration 113/1000 | Loss: 0.00005190
Iteration 114/1000 | Loss: 0.00012744
Iteration 115/1000 | Loss: 0.00011272
Iteration 116/1000 | Loss: 0.00010262
Iteration 117/1000 | Loss: 0.00008530
Iteration 118/1000 | Loss: 0.00008656
Iteration 119/1000 | Loss: 0.00010376
Iteration 120/1000 | Loss: 0.00011896
Iteration 121/1000 | Loss: 0.00010371
Iteration 122/1000 | Loss: 0.00008403
Iteration 123/1000 | Loss: 0.00012015
Iteration 124/1000 | Loss: 0.00011908
Iteration 125/1000 | Loss: 0.00014624
Iteration 126/1000 | Loss: 0.00016305
Iteration 127/1000 | Loss: 0.00003758
Iteration 128/1000 | Loss: 0.00003422
Iteration 129/1000 | Loss: 0.00003292
Iteration 130/1000 | Loss: 0.00003195
Iteration 131/1000 | Loss: 0.00003125
Iteration 132/1000 | Loss: 0.00003066
Iteration 133/1000 | Loss: 0.00003023
Iteration 134/1000 | Loss: 0.00002982
Iteration 135/1000 | Loss: 0.00002955
Iteration 136/1000 | Loss: 0.00002937
Iteration 137/1000 | Loss: 0.00002926
Iteration 138/1000 | Loss: 0.00002911
Iteration 139/1000 | Loss: 0.00002910
Iteration 140/1000 | Loss: 0.00002909
Iteration 141/1000 | Loss: 0.00002903
Iteration 142/1000 | Loss: 0.00002901
Iteration 143/1000 | Loss: 0.00002900
Iteration 144/1000 | Loss: 0.00002900
Iteration 145/1000 | Loss: 0.00002900
Iteration 146/1000 | Loss: 0.00002900
Iteration 147/1000 | Loss: 0.00002897
Iteration 148/1000 | Loss: 0.00002897
Iteration 149/1000 | Loss: 0.00002897
Iteration 150/1000 | Loss: 0.00002895
Iteration 151/1000 | Loss: 0.00002895
Iteration 152/1000 | Loss: 0.00002892
Iteration 153/1000 | Loss: 0.00002892
Iteration 154/1000 | Loss: 0.00002891
Iteration 155/1000 | Loss: 0.00002891
Iteration 156/1000 | Loss: 0.00002888
Iteration 157/1000 | Loss: 0.00002888
Iteration 158/1000 | Loss: 0.00002888
Iteration 159/1000 | Loss: 0.00002888
Iteration 160/1000 | Loss: 0.00002887
Iteration 161/1000 | Loss: 0.00002887
Iteration 162/1000 | Loss: 0.00002886
Iteration 163/1000 | Loss: 0.00002886
Iteration 164/1000 | Loss: 0.00002886
Iteration 165/1000 | Loss: 0.00002886
Iteration 166/1000 | Loss: 0.00002886
Iteration 167/1000 | Loss: 0.00002886
Iteration 168/1000 | Loss: 0.00002886
Iteration 169/1000 | Loss: 0.00002886
Iteration 170/1000 | Loss: 0.00002886
Iteration 171/1000 | Loss: 0.00002886
Iteration 172/1000 | Loss: 0.00002885
Iteration 173/1000 | Loss: 0.00002885
Iteration 174/1000 | Loss: 0.00002885
Iteration 175/1000 | Loss: 0.00002885
Iteration 176/1000 | Loss: 0.00002885
Iteration 177/1000 | Loss: 0.00002885
Iteration 178/1000 | Loss: 0.00002885
Iteration 179/1000 | Loss: 0.00002885
Iteration 180/1000 | Loss: 0.00002885
Iteration 181/1000 | Loss: 0.00002885
Iteration 182/1000 | Loss: 0.00002885
Iteration 183/1000 | Loss: 0.00002885
Iteration 184/1000 | Loss: 0.00002884
Iteration 185/1000 | Loss: 0.00002884
Iteration 186/1000 | Loss: 0.00002884
Iteration 187/1000 | Loss: 0.00002884
Iteration 188/1000 | Loss: 0.00002884
Iteration 189/1000 | Loss: 0.00002884
Iteration 190/1000 | Loss: 0.00002884
Iteration 191/1000 | Loss: 0.00002884
Iteration 192/1000 | Loss: 0.00002884
Iteration 193/1000 | Loss: 0.00002883
Iteration 194/1000 | Loss: 0.00002883
Iteration 195/1000 | Loss: 0.00002883
Iteration 196/1000 | Loss: 0.00002882
Iteration 197/1000 | Loss: 0.00002881
Iteration 198/1000 | Loss: 0.00002879
Iteration 199/1000 | Loss: 0.00002877
Iteration 200/1000 | Loss: 0.00002877
Iteration 201/1000 | Loss: 0.00002877
Iteration 202/1000 | Loss: 0.00002877
Iteration 203/1000 | Loss: 0.00002877
Iteration 204/1000 | Loss: 0.00002877
Iteration 205/1000 | Loss: 0.00002876
Iteration 206/1000 | Loss: 0.00002875
Iteration 207/1000 | Loss: 0.00002874
Iteration 208/1000 | Loss: 0.00002873
Iteration 209/1000 | Loss: 0.00002873
Iteration 210/1000 | Loss: 0.00002870
Iteration 211/1000 | Loss: 0.00002870
Iteration 212/1000 | Loss: 0.00002869
Iteration 213/1000 | Loss: 0.00002869
Iteration 214/1000 | Loss: 0.00002868
Iteration 215/1000 | Loss: 0.00002868
Iteration 216/1000 | Loss: 0.00002868
Iteration 217/1000 | Loss: 0.00002868
Iteration 218/1000 | Loss: 0.00002868
Iteration 219/1000 | Loss: 0.00002868
Iteration 220/1000 | Loss: 0.00002867
Iteration 221/1000 | Loss: 0.00002867
Iteration 222/1000 | Loss: 0.00002867
Iteration 223/1000 | Loss: 0.00002867
Iteration 224/1000 | Loss: 0.00002866
Iteration 225/1000 | Loss: 0.00002865
Iteration 226/1000 | Loss: 0.00002865
Iteration 227/1000 | Loss: 0.00002865
Iteration 228/1000 | Loss: 0.00002864
Iteration 229/1000 | Loss: 0.00002864
Iteration 230/1000 | Loss: 0.00002863
Iteration 231/1000 | Loss: 0.00002863
Iteration 232/1000 | Loss: 0.00002863
Iteration 233/1000 | Loss: 0.00002863
Iteration 234/1000 | Loss: 0.00002862
Iteration 235/1000 | Loss: 0.00002862
Iteration 236/1000 | Loss: 0.00002862
Iteration 237/1000 | Loss: 0.00002862
Iteration 238/1000 | Loss: 0.00002862
Iteration 239/1000 | Loss: 0.00002861
Iteration 240/1000 | Loss: 0.00002861
Iteration 241/1000 | Loss: 0.00002861
Iteration 242/1000 | Loss: 0.00002861
Iteration 243/1000 | Loss: 0.00002861
Iteration 244/1000 | Loss: 0.00002860
Iteration 245/1000 | Loss: 0.00002860
Iteration 246/1000 | Loss: 0.00002860
Iteration 247/1000 | Loss: 0.00002860
Iteration 248/1000 | Loss: 0.00002860
Iteration 249/1000 | Loss: 0.00002860
Iteration 250/1000 | Loss: 0.00002860
Iteration 251/1000 | Loss: 0.00002860
Iteration 252/1000 | Loss: 0.00002860
Iteration 253/1000 | Loss: 0.00002860
Iteration 254/1000 | Loss: 0.00002860
Iteration 255/1000 | Loss: 0.00002859
Iteration 256/1000 | Loss: 0.00002859
Iteration 257/1000 | Loss: 0.00002859
Iteration 258/1000 | Loss: 0.00002859
Iteration 259/1000 | Loss: 0.00002859
Iteration 260/1000 | Loss: 0.00002859
Iteration 261/1000 | Loss: 0.00002858
Iteration 262/1000 | Loss: 0.00002858
Iteration 263/1000 | Loss: 0.00002858
Iteration 264/1000 | Loss: 0.00002858
Iteration 265/1000 | Loss: 0.00002858
Iteration 266/1000 | Loss: 0.00002858
Iteration 267/1000 | Loss: 0.00002858
Iteration 268/1000 | Loss: 0.00002858
Iteration 269/1000 | Loss: 0.00002858
Iteration 270/1000 | Loss: 0.00002858
Iteration 271/1000 | Loss: 0.00002858
Iteration 272/1000 | Loss: 0.00002858
Iteration 273/1000 | Loss: 0.00002858
Iteration 274/1000 | Loss: 0.00002857
Iteration 275/1000 | Loss: 0.00002857
Iteration 276/1000 | Loss: 0.00002857
Iteration 277/1000 | Loss: 0.00002856
Iteration 278/1000 | Loss: 0.00002856
Iteration 279/1000 | Loss: 0.00002856
Iteration 280/1000 | Loss: 0.00002856
Iteration 281/1000 | Loss: 0.00002856
Iteration 282/1000 | Loss: 0.00002856
Iteration 283/1000 | Loss: 0.00002855
Iteration 284/1000 | Loss: 0.00002855
Iteration 285/1000 | Loss: 0.00002855
Iteration 286/1000 | Loss: 0.00002855
Iteration 287/1000 | Loss: 0.00002855
Iteration 288/1000 | Loss: 0.00002854
Iteration 289/1000 | Loss: 0.00002854
Iteration 290/1000 | Loss: 0.00002854
Iteration 291/1000 | Loss: 0.00002854
Iteration 292/1000 | Loss: 0.00002853
Iteration 293/1000 | Loss: 0.00002853
Iteration 294/1000 | Loss: 0.00002853
Iteration 295/1000 | Loss: 0.00002853
Iteration 296/1000 | Loss: 0.00002853
Iteration 297/1000 | Loss: 0.00002853
Iteration 298/1000 | Loss: 0.00002852
Iteration 299/1000 | Loss: 0.00002852
Iteration 300/1000 | Loss: 0.00002852
Iteration 301/1000 | Loss: 0.00002852
Iteration 302/1000 | Loss: 0.00002851
Iteration 303/1000 | Loss: 0.00002851
Iteration 304/1000 | Loss: 0.00002851
Iteration 305/1000 | Loss: 0.00002851
Iteration 306/1000 | Loss: 0.00002850
Iteration 307/1000 | Loss: 0.00002850
Iteration 308/1000 | Loss: 0.00002850
Iteration 309/1000 | Loss: 0.00002850
Iteration 310/1000 | Loss: 0.00002850
Iteration 311/1000 | Loss: 0.00002850
Iteration 312/1000 | Loss: 0.00002850
Iteration 313/1000 | Loss: 0.00002850
Iteration 314/1000 | Loss: 0.00002850
Iteration 315/1000 | Loss: 0.00002850
Iteration 316/1000 | Loss: 0.00002849
Iteration 317/1000 | Loss: 0.00002849
Iteration 318/1000 | Loss: 0.00002849
Iteration 319/1000 | Loss: 0.00002849
Iteration 320/1000 | Loss: 0.00002849
Iteration 321/1000 | Loss: 0.00002849
Iteration 322/1000 | Loss: 0.00002849
Iteration 323/1000 | Loss: 0.00002849
Iteration 324/1000 | Loss: 0.00002849
Iteration 325/1000 | Loss: 0.00002849
Iteration 326/1000 | Loss: 0.00002849
Iteration 327/1000 | Loss: 0.00002849
Iteration 328/1000 | Loss: 0.00002849
Iteration 329/1000 | Loss: 0.00002849
Iteration 330/1000 | Loss: 0.00002849
Iteration 331/1000 | Loss: 0.00002849
Iteration 332/1000 | Loss: 0.00002849
Iteration 333/1000 | Loss: 0.00002849
Iteration 334/1000 | Loss: 0.00002849
Iteration 335/1000 | Loss: 0.00002849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [2.848859003279358e-05, 2.848859003279358e-05, 2.848859003279358e-05, 2.848859003279358e-05, 2.848859003279358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.848859003279358e-05

Optimization complete. Final v2v error: 4.41091251373291 mm

Highest mean error: 5.395519256591797 mm for frame 89

Lowest mean error: 3.606343984603882 mm for frame 35

Saving results

Total time: 298.11378383636475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436182
Iteration 2/25 | Loss: 0.00138657
Iteration 3/25 | Loss: 0.00130176
Iteration 4/25 | Loss: 0.00128968
Iteration 5/25 | Loss: 0.00128604
Iteration 6/25 | Loss: 0.00128542
Iteration 7/25 | Loss: 0.00128542
Iteration 8/25 | Loss: 0.00128542
Iteration 9/25 | Loss: 0.00128542
Iteration 10/25 | Loss: 0.00128542
Iteration 11/25 | Loss: 0.00128542
Iteration 12/25 | Loss: 0.00128542
Iteration 13/25 | Loss: 0.00128542
Iteration 14/25 | Loss: 0.00128542
Iteration 15/25 | Loss: 0.00128542
Iteration 16/25 | Loss: 0.00128542
Iteration 17/25 | Loss: 0.00128542
Iteration 18/25 | Loss: 0.00128542
Iteration 19/25 | Loss: 0.00128542
Iteration 20/25 | Loss: 0.00128542
Iteration 21/25 | Loss: 0.00128542
Iteration 22/25 | Loss: 0.00128542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012854168890044093, 0.0012854168890044093, 0.0012854168890044093, 0.0012854168890044093, 0.0012854168890044093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012854168890044093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40676677
Iteration 2/25 | Loss: 0.00100950
Iteration 3/25 | Loss: 0.00100949
Iteration 4/25 | Loss: 0.00100949
Iteration 5/25 | Loss: 0.00100949
Iteration 6/25 | Loss: 0.00100949
Iteration 7/25 | Loss: 0.00100949
Iteration 8/25 | Loss: 0.00100949
Iteration 9/25 | Loss: 0.00100949
Iteration 10/25 | Loss: 0.00100949
Iteration 11/25 | Loss: 0.00100949
Iteration 12/25 | Loss: 0.00100949
Iteration 13/25 | Loss: 0.00100949
Iteration 14/25 | Loss: 0.00100949
Iteration 15/25 | Loss: 0.00100949
Iteration 16/25 | Loss: 0.00100949
Iteration 17/25 | Loss: 0.00100949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010094883618876338, 0.0010094883618876338, 0.0010094883618876338, 0.0010094883618876338, 0.0010094883618876338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010094883618876338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100949
Iteration 2/1000 | Loss: 0.00003798
Iteration 3/1000 | Loss: 0.00002315
Iteration 4/1000 | Loss: 0.00001981
Iteration 5/1000 | Loss: 0.00001835
Iteration 6/1000 | Loss: 0.00001707
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001584
Iteration 9/1000 | Loss: 0.00001545
Iteration 10/1000 | Loss: 0.00001509
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001470
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001467
Iteration 17/1000 | Loss: 0.00001466
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001463
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001462
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001458
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001456
Iteration 37/1000 | Loss: 0.00001456
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001451
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001446
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001445
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001435
Iteration 88/1000 | Loss: 0.00001435
Iteration 89/1000 | Loss: 0.00001435
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001434
Iteration 92/1000 | Loss: 0.00001434
Iteration 93/1000 | Loss: 0.00001434
Iteration 94/1000 | Loss: 0.00001434
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001433
Iteration 97/1000 | Loss: 0.00001433
Iteration 98/1000 | Loss: 0.00001433
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001433
Iteration 104/1000 | Loss: 0.00001433
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001431
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001430
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001430
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001429
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001429
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001428
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001427
Iteration 136/1000 | Loss: 0.00001427
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001426
Iteration 140/1000 | Loss: 0.00001426
Iteration 141/1000 | Loss: 0.00001426
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001425
Iteration 144/1000 | Loss: 0.00001425
Iteration 145/1000 | Loss: 0.00001425
Iteration 146/1000 | Loss: 0.00001425
Iteration 147/1000 | Loss: 0.00001425
Iteration 148/1000 | Loss: 0.00001424
Iteration 149/1000 | Loss: 0.00001424
Iteration 150/1000 | Loss: 0.00001424
Iteration 151/1000 | Loss: 0.00001424
Iteration 152/1000 | Loss: 0.00001424
Iteration 153/1000 | Loss: 0.00001424
Iteration 154/1000 | Loss: 0.00001424
Iteration 155/1000 | Loss: 0.00001424
Iteration 156/1000 | Loss: 0.00001423
Iteration 157/1000 | Loss: 0.00001423
Iteration 158/1000 | Loss: 0.00001423
Iteration 159/1000 | Loss: 0.00001423
Iteration 160/1000 | Loss: 0.00001423
Iteration 161/1000 | Loss: 0.00001423
Iteration 162/1000 | Loss: 0.00001423
Iteration 163/1000 | Loss: 0.00001423
Iteration 164/1000 | Loss: 0.00001423
Iteration 165/1000 | Loss: 0.00001422
Iteration 166/1000 | Loss: 0.00001422
Iteration 167/1000 | Loss: 0.00001422
Iteration 168/1000 | Loss: 0.00001422
Iteration 169/1000 | Loss: 0.00001422
Iteration 170/1000 | Loss: 0.00001422
Iteration 171/1000 | Loss: 0.00001422
Iteration 172/1000 | Loss: 0.00001422
Iteration 173/1000 | Loss: 0.00001422
Iteration 174/1000 | Loss: 0.00001422
Iteration 175/1000 | Loss: 0.00001422
Iteration 176/1000 | Loss: 0.00001422
Iteration 177/1000 | Loss: 0.00001422
Iteration 178/1000 | Loss: 0.00001422
Iteration 179/1000 | Loss: 0.00001421
Iteration 180/1000 | Loss: 0.00001421
Iteration 181/1000 | Loss: 0.00001421
Iteration 182/1000 | Loss: 0.00001421
Iteration 183/1000 | Loss: 0.00001421
Iteration 184/1000 | Loss: 0.00001420
Iteration 185/1000 | Loss: 0.00001420
Iteration 186/1000 | Loss: 0.00001420
Iteration 187/1000 | Loss: 0.00001420
Iteration 188/1000 | Loss: 0.00001420
Iteration 189/1000 | Loss: 0.00001420
Iteration 190/1000 | Loss: 0.00001420
Iteration 191/1000 | Loss: 0.00001419
Iteration 192/1000 | Loss: 0.00001419
Iteration 193/1000 | Loss: 0.00001419
Iteration 194/1000 | Loss: 0.00001419
Iteration 195/1000 | Loss: 0.00001419
Iteration 196/1000 | Loss: 0.00001419
Iteration 197/1000 | Loss: 0.00001419
Iteration 198/1000 | Loss: 0.00001419
Iteration 199/1000 | Loss: 0.00001419
Iteration 200/1000 | Loss: 0.00001419
Iteration 201/1000 | Loss: 0.00001419
Iteration 202/1000 | Loss: 0.00001418
Iteration 203/1000 | Loss: 0.00001418
Iteration 204/1000 | Loss: 0.00001418
Iteration 205/1000 | Loss: 0.00001418
Iteration 206/1000 | Loss: 0.00001418
Iteration 207/1000 | Loss: 0.00001418
Iteration 208/1000 | Loss: 0.00001418
Iteration 209/1000 | Loss: 0.00001418
Iteration 210/1000 | Loss: 0.00001418
Iteration 211/1000 | Loss: 0.00001418
Iteration 212/1000 | Loss: 0.00001418
Iteration 213/1000 | Loss: 0.00001418
Iteration 214/1000 | Loss: 0.00001418
Iteration 215/1000 | Loss: 0.00001418
Iteration 216/1000 | Loss: 0.00001418
Iteration 217/1000 | Loss: 0.00001418
Iteration 218/1000 | Loss: 0.00001418
Iteration 219/1000 | Loss: 0.00001418
Iteration 220/1000 | Loss: 0.00001418
Iteration 221/1000 | Loss: 0.00001418
Iteration 222/1000 | Loss: 0.00001418
Iteration 223/1000 | Loss: 0.00001418
Iteration 224/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.4175260730553418e-05, 1.4175260730553418e-05, 1.4175260730553418e-05, 1.4175260730553418e-05, 1.4175260730553418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4175260730553418e-05

Optimization complete. Final v2v error: 3.195486068725586 mm

Highest mean error: 3.708667278289795 mm for frame 11

Lowest mean error: 2.8517563343048096 mm for frame 172

Saving results

Total time: 48.106306076049805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447025
Iteration 2/25 | Loss: 0.00138050
Iteration 3/25 | Loss: 0.00130909
Iteration 4/25 | Loss: 0.00129653
Iteration 5/25 | Loss: 0.00129227
Iteration 6/25 | Loss: 0.00129196
Iteration 7/25 | Loss: 0.00129196
Iteration 8/25 | Loss: 0.00129196
Iteration 9/25 | Loss: 0.00129196
Iteration 10/25 | Loss: 0.00129196
Iteration 11/25 | Loss: 0.00129196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012919623404741287, 0.0012919623404741287, 0.0012919623404741287, 0.0012919623404741287, 0.0012919623404741287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012919623404741287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41072977
Iteration 2/25 | Loss: 0.00083176
Iteration 3/25 | Loss: 0.00083176
Iteration 4/25 | Loss: 0.00083176
Iteration 5/25 | Loss: 0.00083176
Iteration 6/25 | Loss: 0.00083176
Iteration 7/25 | Loss: 0.00083176
Iteration 8/25 | Loss: 0.00083176
Iteration 9/25 | Loss: 0.00083176
Iteration 10/25 | Loss: 0.00083176
Iteration 11/25 | Loss: 0.00083176
Iteration 12/25 | Loss: 0.00083176
Iteration 13/25 | Loss: 0.00083176
Iteration 14/25 | Loss: 0.00083176
Iteration 15/25 | Loss: 0.00083176
Iteration 16/25 | Loss: 0.00083176
Iteration 17/25 | Loss: 0.00083176
Iteration 18/25 | Loss: 0.00083176
Iteration 19/25 | Loss: 0.00083176
Iteration 20/25 | Loss: 0.00083176
Iteration 21/25 | Loss: 0.00083176
Iteration 22/25 | Loss: 0.00083176
Iteration 23/25 | Loss: 0.00083176
Iteration 24/25 | Loss: 0.00083176
Iteration 25/25 | Loss: 0.00083176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083176
Iteration 2/1000 | Loss: 0.00003547
Iteration 3/1000 | Loss: 0.00002628
Iteration 4/1000 | Loss: 0.00002376
Iteration 5/1000 | Loss: 0.00002244
Iteration 6/1000 | Loss: 0.00002165
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00002058
Iteration 9/1000 | Loss: 0.00002025
Iteration 10/1000 | Loss: 0.00002007
Iteration 11/1000 | Loss: 0.00002005
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001959
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001929
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001922
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001895
Iteration 24/1000 | Loss: 0.00001894
Iteration 25/1000 | Loss: 0.00001892
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001891
Iteration 28/1000 | Loss: 0.00001891
Iteration 29/1000 | Loss: 0.00001890
Iteration 30/1000 | Loss: 0.00001890
Iteration 31/1000 | Loss: 0.00001890
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001889
Iteration 34/1000 | Loss: 0.00001888
Iteration 35/1000 | Loss: 0.00001888
Iteration 36/1000 | Loss: 0.00001886
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001885
Iteration 39/1000 | Loss: 0.00001883
Iteration 40/1000 | Loss: 0.00001882
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001880
Iteration 43/1000 | Loss: 0.00001880
Iteration 44/1000 | Loss: 0.00001879
Iteration 45/1000 | Loss: 0.00001879
Iteration 46/1000 | Loss: 0.00001879
Iteration 47/1000 | Loss: 0.00001878
Iteration 48/1000 | Loss: 0.00001877
Iteration 49/1000 | Loss: 0.00001877
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001876
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001874
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001873
Iteration 58/1000 | Loss: 0.00001873
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001873
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001872
Iteration 66/1000 | Loss: 0.00001871
Iteration 67/1000 | Loss: 0.00001871
Iteration 68/1000 | Loss: 0.00001870
Iteration 69/1000 | Loss: 0.00001870
Iteration 70/1000 | Loss: 0.00001870
Iteration 71/1000 | Loss: 0.00001870
Iteration 72/1000 | Loss: 0.00001870
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001870
Iteration 77/1000 | Loss: 0.00001870
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001867
Iteration 86/1000 | Loss: 0.00001867
Iteration 87/1000 | Loss: 0.00001867
Iteration 88/1000 | Loss: 0.00001866
Iteration 89/1000 | Loss: 0.00001866
Iteration 90/1000 | Loss: 0.00001866
Iteration 91/1000 | Loss: 0.00001866
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001865
Iteration 99/1000 | Loss: 0.00001865
Iteration 100/1000 | Loss: 0.00001865
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001864
Iteration 103/1000 | Loss: 0.00001864
Iteration 104/1000 | Loss: 0.00001864
Iteration 105/1000 | Loss: 0.00001864
Iteration 106/1000 | Loss: 0.00001864
Iteration 107/1000 | Loss: 0.00001864
Iteration 108/1000 | Loss: 0.00001864
Iteration 109/1000 | Loss: 0.00001863
Iteration 110/1000 | Loss: 0.00001863
Iteration 111/1000 | Loss: 0.00001863
Iteration 112/1000 | Loss: 0.00001862
Iteration 113/1000 | Loss: 0.00001862
Iteration 114/1000 | Loss: 0.00001862
Iteration 115/1000 | Loss: 0.00001862
Iteration 116/1000 | Loss: 0.00001862
Iteration 117/1000 | Loss: 0.00001862
Iteration 118/1000 | Loss: 0.00001862
Iteration 119/1000 | Loss: 0.00001862
Iteration 120/1000 | Loss: 0.00001862
Iteration 121/1000 | Loss: 0.00001861
Iteration 122/1000 | Loss: 0.00001861
Iteration 123/1000 | Loss: 0.00001861
Iteration 124/1000 | Loss: 0.00001861
Iteration 125/1000 | Loss: 0.00001861
Iteration 126/1000 | Loss: 0.00001861
Iteration 127/1000 | Loss: 0.00001861
Iteration 128/1000 | Loss: 0.00001861
Iteration 129/1000 | Loss: 0.00001860
Iteration 130/1000 | Loss: 0.00001860
Iteration 131/1000 | Loss: 0.00001860
Iteration 132/1000 | Loss: 0.00001860
Iteration 133/1000 | Loss: 0.00001860
Iteration 134/1000 | Loss: 0.00001860
Iteration 135/1000 | Loss: 0.00001860
Iteration 136/1000 | Loss: 0.00001859
Iteration 137/1000 | Loss: 0.00001859
Iteration 138/1000 | Loss: 0.00001859
Iteration 139/1000 | Loss: 0.00001859
Iteration 140/1000 | Loss: 0.00001859
Iteration 141/1000 | Loss: 0.00001859
Iteration 142/1000 | Loss: 0.00001859
Iteration 143/1000 | Loss: 0.00001859
Iteration 144/1000 | Loss: 0.00001859
Iteration 145/1000 | Loss: 0.00001859
Iteration 146/1000 | Loss: 0.00001859
Iteration 147/1000 | Loss: 0.00001859
Iteration 148/1000 | Loss: 0.00001859
Iteration 149/1000 | Loss: 0.00001859
Iteration 150/1000 | Loss: 0.00001859
Iteration 151/1000 | Loss: 0.00001859
Iteration 152/1000 | Loss: 0.00001859
Iteration 153/1000 | Loss: 0.00001859
Iteration 154/1000 | Loss: 0.00001859
Iteration 155/1000 | Loss: 0.00001859
Iteration 156/1000 | Loss: 0.00001859
Iteration 157/1000 | Loss: 0.00001859
Iteration 158/1000 | Loss: 0.00001859
Iteration 159/1000 | Loss: 0.00001859
Iteration 160/1000 | Loss: 0.00001859
Iteration 161/1000 | Loss: 0.00001859
Iteration 162/1000 | Loss: 0.00001859
Iteration 163/1000 | Loss: 0.00001859
Iteration 164/1000 | Loss: 0.00001859
Iteration 165/1000 | Loss: 0.00001859
Iteration 166/1000 | Loss: 0.00001859
Iteration 167/1000 | Loss: 0.00001859
Iteration 168/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.8587963495519944e-05, 1.8587963495519944e-05, 1.8587963495519944e-05, 1.8587963495519944e-05, 1.8587963495519944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8587963495519944e-05

Optimization complete. Final v2v error: 3.6668055057525635 mm

Highest mean error: 3.832972764968872 mm for frame 113

Lowest mean error: 3.4875526428222656 mm for frame 40

Saving results

Total time: 41.51935386657715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929543
Iteration 2/25 | Loss: 0.00251048
Iteration 3/25 | Loss: 0.00172790
Iteration 4/25 | Loss: 0.00154730
Iteration 5/25 | Loss: 0.00150979
Iteration 6/25 | Loss: 0.00146406
Iteration 7/25 | Loss: 0.00141695
Iteration 8/25 | Loss: 0.00137239
Iteration 9/25 | Loss: 0.00135655
Iteration 10/25 | Loss: 0.00136175
Iteration 11/25 | Loss: 0.00134977
Iteration 12/25 | Loss: 0.00134427
Iteration 13/25 | Loss: 0.00134611
Iteration 14/25 | Loss: 0.00134582
Iteration 15/25 | Loss: 0.00134615
Iteration 16/25 | Loss: 0.00134261
Iteration 17/25 | Loss: 0.00134025
Iteration 18/25 | Loss: 0.00133960
Iteration 19/25 | Loss: 0.00133851
Iteration 20/25 | Loss: 0.00133982
Iteration 21/25 | Loss: 0.00133546
Iteration 22/25 | Loss: 0.00133852
Iteration 23/25 | Loss: 0.00133795
Iteration 24/25 | Loss: 0.00133895
Iteration 25/25 | Loss: 0.00133504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48926902
Iteration 2/25 | Loss: 0.00096417
Iteration 3/25 | Loss: 0.00072129
Iteration 4/25 | Loss: 0.00072129
Iteration 5/25 | Loss: 0.00072129
Iteration 6/25 | Loss: 0.00072129
Iteration 7/25 | Loss: 0.00072129
Iteration 8/25 | Loss: 0.00072129
Iteration 9/25 | Loss: 0.00072129
Iteration 10/25 | Loss: 0.00072129
Iteration 11/25 | Loss: 0.00072129
Iteration 12/25 | Loss: 0.00072129
Iteration 13/25 | Loss: 0.00072129
Iteration 14/25 | Loss: 0.00072129
Iteration 15/25 | Loss: 0.00072129
Iteration 16/25 | Loss: 0.00072129
Iteration 17/25 | Loss: 0.00072129
Iteration 18/25 | Loss: 0.00072129
Iteration 19/25 | Loss: 0.00072129
Iteration 20/25 | Loss: 0.00072129
Iteration 21/25 | Loss: 0.00072129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007212871569208801, 0.0007212871569208801, 0.0007212871569208801, 0.0007212871569208801, 0.0007212871569208801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007212871569208801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072129
Iteration 2/1000 | Loss: 0.00029990
Iteration 3/1000 | Loss: 0.00023831
Iteration 4/1000 | Loss: 0.00003704
Iteration 5/1000 | Loss: 0.00003223
Iteration 6/1000 | Loss: 0.00002950
Iteration 7/1000 | Loss: 0.00030434
Iteration 8/1000 | Loss: 0.00003556
Iteration 9/1000 | Loss: 0.00002882
Iteration 10/1000 | Loss: 0.00002603
Iteration 11/1000 | Loss: 0.00002547
Iteration 12/1000 | Loss: 0.00063705
Iteration 13/1000 | Loss: 0.00004162
Iteration 14/1000 | Loss: 0.00002971
Iteration 15/1000 | Loss: 0.00002571
Iteration 16/1000 | Loss: 0.00002461
Iteration 17/1000 | Loss: 0.00031081
Iteration 18/1000 | Loss: 0.00002667
Iteration 19/1000 | Loss: 0.00002420
Iteration 20/1000 | Loss: 0.00002351
Iteration 21/1000 | Loss: 0.00002313
Iteration 22/1000 | Loss: 0.00002240
Iteration 23/1000 | Loss: 0.00002206
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002187
Iteration 27/1000 | Loss: 0.00002174
Iteration 28/1000 | Loss: 0.00002153
Iteration 29/1000 | Loss: 0.00002151
Iteration 30/1000 | Loss: 0.00002143
Iteration 31/1000 | Loss: 0.00002139
Iteration 32/1000 | Loss: 0.00002138
Iteration 33/1000 | Loss: 0.00002131
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002127
Iteration 37/1000 | Loss: 0.00002126
Iteration 38/1000 | Loss: 0.00002126
Iteration 39/1000 | Loss: 0.00002125
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002120
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002120
Iteration 46/1000 | Loss: 0.00002119
Iteration 47/1000 | Loss: 0.00002119
Iteration 48/1000 | Loss: 0.00002119
Iteration 49/1000 | Loss: 0.00002118
Iteration 50/1000 | Loss: 0.00002118
Iteration 51/1000 | Loss: 0.00002118
Iteration 52/1000 | Loss: 0.00002118
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002117
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002117
Iteration 59/1000 | Loss: 0.00002117
Iteration 60/1000 | Loss: 0.00002117
Iteration 61/1000 | Loss: 0.00002117
Iteration 62/1000 | Loss: 0.00002117
Iteration 63/1000 | Loss: 0.00002117
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002116
Iteration 66/1000 | Loss: 0.00002116
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002114
Iteration 69/1000 | Loss: 0.00002114
Iteration 70/1000 | Loss: 0.00002114
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002114
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002114
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002112
Iteration 81/1000 | Loss: 0.00002112
Iteration 82/1000 | Loss: 0.00002112
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002111
Iteration 87/1000 | Loss: 0.00002111
Iteration 88/1000 | Loss: 0.00002111
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002110
Iteration 98/1000 | Loss: 0.00002110
Iteration 99/1000 | Loss: 0.00002110
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002109
Iteration 105/1000 | Loss: 0.00002109
Iteration 106/1000 | Loss: 0.00002109
Iteration 107/1000 | Loss: 0.00002109
Iteration 108/1000 | Loss: 0.00002108
Iteration 109/1000 | Loss: 0.00002108
Iteration 110/1000 | Loss: 0.00002108
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002108
Iteration 113/1000 | Loss: 0.00002108
Iteration 114/1000 | Loss: 0.00002108
Iteration 115/1000 | Loss: 0.00002108
Iteration 116/1000 | Loss: 0.00002107
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002107
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002107
Iteration 123/1000 | Loss: 0.00002107
Iteration 124/1000 | Loss: 0.00002106
Iteration 125/1000 | Loss: 0.00002106
Iteration 126/1000 | Loss: 0.00002106
Iteration 127/1000 | Loss: 0.00002106
Iteration 128/1000 | Loss: 0.00002106
Iteration 129/1000 | Loss: 0.00002105
Iteration 130/1000 | Loss: 0.00002105
Iteration 131/1000 | Loss: 0.00002105
Iteration 132/1000 | Loss: 0.00002105
Iteration 133/1000 | Loss: 0.00002105
Iteration 134/1000 | Loss: 0.00002105
Iteration 135/1000 | Loss: 0.00002105
Iteration 136/1000 | Loss: 0.00002105
Iteration 137/1000 | Loss: 0.00002105
Iteration 138/1000 | Loss: 0.00002104
Iteration 139/1000 | Loss: 0.00002104
Iteration 140/1000 | Loss: 0.00002104
Iteration 141/1000 | Loss: 0.00002104
Iteration 142/1000 | Loss: 0.00002104
Iteration 143/1000 | Loss: 0.00002104
Iteration 144/1000 | Loss: 0.00002104
Iteration 145/1000 | Loss: 0.00002104
Iteration 146/1000 | Loss: 0.00002104
Iteration 147/1000 | Loss: 0.00002104
Iteration 148/1000 | Loss: 0.00002103
Iteration 149/1000 | Loss: 0.00002103
Iteration 150/1000 | Loss: 0.00002103
Iteration 151/1000 | Loss: 0.00002102
Iteration 152/1000 | Loss: 0.00002102
Iteration 153/1000 | Loss: 0.00002102
Iteration 154/1000 | Loss: 0.00002102
Iteration 155/1000 | Loss: 0.00002101
Iteration 156/1000 | Loss: 0.00002101
Iteration 157/1000 | Loss: 0.00002101
Iteration 158/1000 | Loss: 0.00002101
Iteration 159/1000 | Loss: 0.00002101
Iteration 160/1000 | Loss: 0.00002101
Iteration 161/1000 | Loss: 0.00002101
Iteration 162/1000 | Loss: 0.00002101
Iteration 163/1000 | Loss: 0.00002101
Iteration 164/1000 | Loss: 0.00002101
Iteration 165/1000 | Loss: 0.00002101
Iteration 166/1000 | Loss: 0.00002101
Iteration 167/1000 | Loss: 0.00002100
Iteration 168/1000 | Loss: 0.00002100
Iteration 169/1000 | Loss: 0.00002100
Iteration 170/1000 | Loss: 0.00002100
Iteration 171/1000 | Loss: 0.00002100
Iteration 172/1000 | Loss: 0.00002100
Iteration 173/1000 | Loss: 0.00002100
Iteration 174/1000 | Loss: 0.00002100
Iteration 175/1000 | Loss: 0.00002100
Iteration 176/1000 | Loss: 0.00002100
Iteration 177/1000 | Loss: 0.00002100
Iteration 178/1000 | Loss: 0.00002099
Iteration 179/1000 | Loss: 0.00002099
Iteration 180/1000 | Loss: 0.00002099
Iteration 181/1000 | Loss: 0.00002099
Iteration 182/1000 | Loss: 0.00002099
Iteration 183/1000 | Loss: 0.00002099
Iteration 184/1000 | Loss: 0.00002098
Iteration 185/1000 | Loss: 0.00002098
Iteration 186/1000 | Loss: 0.00002097
Iteration 187/1000 | Loss: 0.00002097
Iteration 188/1000 | Loss: 0.00002097
Iteration 189/1000 | Loss: 0.00002097
Iteration 190/1000 | Loss: 0.00002097
Iteration 191/1000 | Loss: 0.00002097
Iteration 192/1000 | Loss: 0.00002097
Iteration 193/1000 | Loss: 0.00002097
Iteration 194/1000 | Loss: 0.00002097
Iteration 195/1000 | Loss: 0.00002097
Iteration 196/1000 | Loss: 0.00002097
Iteration 197/1000 | Loss: 0.00002097
Iteration 198/1000 | Loss: 0.00002097
Iteration 199/1000 | Loss: 0.00002097
Iteration 200/1000 | Loss: 0.00002097
Iteration 201/1000 | Loss: 0.00002097
Iteration 202/1000 | Loss: 0.00002097
Iteration 203/1000 | Loss: 0.00002097
Iteration 204/1000 | Loss: 0.00002097
Iteration 205/1000 | Loss: 0.00002097
Iteration 206/1000 | Loss: 0.00002097
Iteration 207/1000 | Loss: 0.00002097
Iteration 208/1000 | Loss: 0.00002097
Iteration 209/1000 | Loss: 0.00002097
Iteration 210/1000 | Loss: 0.00002097
Iteration 211/1000 | Loss: 0.00002097
Iteration 212/1000 | Loss: 0.00002097
Iteration 213/1000 | Loss: 0.00002097
Iteration 214/1000 | Loss: 0.00002097
Iteration 215/1000 | Loss: 0.00002097
Iteration 216/1000 | Loss: 0.00002097
Iteration 217/1000 | Loss: 0.00002097
Iteration 218/1000 | Loss: 0.00002097
Iteration 219/1000 | Loss: 0.00002097
Iteration 220/1000 | Loss: 0.00002097
Iteration 221/1000 | Loss: 0.00002097
Iteration 222/1000 | Loss: 0.00002097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.0966677766409703e-05, 2.0966677766409703e-05, 2.0966677766409703e-05, 2.0966677766409703e-05, 2.0966677766409703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0966677766409703e-05

Optimization complete. Final v2v error: 3.9183404445648193 mm

Highest mean error: 4.672497749328613 mm for frame 133

Lowest mean error: 3.498406171798706 mm for frame 129

Saving results

Total time: 113.18063545227051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00681233
Iteration 2/25 | Loss: 0.00140650
Iteration 3/25 | Loss: 0.00132459
Iteration 4/25 | Loss: 0.00129037
Iteration 5/25 | Loss: 0.00128482
Iteration 6/25 | Loss: 0.00128690
Iteration 7/25 | Loss: 0.00127438
Iteration 8/25 | Loss: 0.00127312
Iteration 9/25 | Loss: 0.00127280
Iteration 10/25 | Loss: 0.00127260
Iteration 11/25 | Loss: 0.00127254
Iteration 12/25 | Loss: 0.00127254
Iteration 13/25 | Loss: 0.00127253
Iteration 14/25 | Loss: 0.00127253
Iteration 15/25 | Loss: 0.00127253
Iteration 16/25 | Loss: 0.00127253
Iteration 17/25 | Loss: 0.00127249
Iteration 18/25 | Loss: 0.00127249
Iteration 19/25 | Loss: 0.00127248
Iteration 20/25 | Loss: 0.00127248
Iteration 21/25 | Loss: 0.00127247
Iteration 22/25 | Loss: 0.00127246
Iteration 23/25 | Loss: 0.00127246
Iteration 24/25 | Loss: 0.00127246
Iteration 25/25 | Loss: 0.00127246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52981210
Iteration 2/25 | Loss: 0.00083466
Iteration 3/25 | Loss: 0.00083466
Iteration 4/25 | Loss: 0.00083466
Iteration 5/25 | Loss: 0.00083466
Iteration 6/25 | Loss: 0.00083466
Iteration 7/25 | Loss: 0.00083466
Iteration 8/25 | Loss: 0.00083466
Iteration 9/25 | Loss: 0.00083466
Iteration 10/25 | Loss: 0.00083466
Iteration 11/25 | Loss: 0.00083466
Iteration 12/25 | Loss: 0.00083466
Iteration 13/25 | Loss: 0.00083466
Iteration 14/25 | Loss: 0.00083466
Iteration 15/25 | Loss: 0.00083466
Iteration 16/25 | Loss: 0.00083466
Iteration 17/25 | Loss: 0.00083466
Iteration 18/25 | Loss: 0.00083466
Iteration 19/25 | Loss: 0.00083466
Iteration 20/25 | Loss: 0.00083466
Iteration 21/25 | Loss: 0.00083466
Iteration 22/25 | Loss: 0.00083466
Iteration 23/25 | Loss: 0.00083466
Iteration 24/25 | Loss: 0.00083466
Iteration 25/25 | Loss: 0.00083466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083466
Iteration 2/1000 | Loss: 0.00002232
Iteration 3/1000 | Loss: 0.00001869
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001571
Iteration 8/1000 | Loss: 0.00001532
Iteration 9/1000 | Loss: 0.00001518
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001493
Iteration 12/1000 | Loss: 0.00001489
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001465
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001465
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001465
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001456
Iteration 38/1000 | Loss: 0.00001454
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001453
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001452
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001448
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001447
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001445
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001443
Iteration 85/1000 | Loss: 0.00001443
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001441
Iteration 92/1000 | Loss: 0.00001441
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001441
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001438
Iteration 104/1000 | Loss: 0.00001438
Iteration 105/1000 | Loss: 0.00001437
Iteration 106/1000 | Loss: 0.00001437
Iteration 107/1000 | Loss: 0.00001436
Iteration 108/1000 | Loss: 0.00001436
Iteration 109/1000 | Loss: 0.00001436
Iteration 110/1000 | Loss: 0.00001436
Iteration 111/1000 | Loss: 0.00001436
Iteration 112/1000 | Loss: 0.00001435
Iteration 113/1000 | Loss: 0.00001435
Iteration 114/1000 | Loss: 0.00001435
Iteration 115/1000 | Loss: 0.00001435
Iteration 116/1000 | Loss: 0.00001435
Iteration 117/1000 | Loss: 0.00001435
Iteration 118/1000 | Loss: 0.00001435
Iteration 119/1000 | Loss: 0.00001435
Iteration 120/1000 | Loss: 0.00001435
Iteration 121/1000 | Loss: 0.00001435
Iteration 122/1000 | Loss: 0.00001435
Iteration 123/1000 | Loss: 0.00001435
Iteration 124/1000 | Loss: 0.00001435
Iteration 125/1000 | Loss: 0.00001435
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001435
Iteration 128/1000 | Loss: 0.00001435
Iteration 129/1000 | Loss: 0.00001435
Iteration 130/1000 | Loss: 0.00001435
Iteration 131/1000 | Loss: 0.00001435
Iteration 132/1000 | Loss: 0.00001435
Iteration 133/1000 | Loss: 0.00001435
Iteration 134/1000 | Loss: 0.00001435
Iteration 135/1000 | Loss: 0.00001435
Iteration 136/1000 | Loss: 0.00001435
Iteration 137/1000 | Loss: 0.00001435
Iteration 138/1000 | Loss: 0.00001435
Iteration 139/1000 | Loss: 0.00001435
Iteration 140/1000 | Loss: 0.00001435
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001435
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001435
Iteration 161/1000 | Loss: 0.00001435
Iteration 162/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.4350677702168468e-05, 1.4350677702168468e-05, 1.4350677702168468e-05, 1.4350677702168468e-05, 1.4350677702168468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4350677702168468e-05

Optimization complete. Final v2v error: 3.234909772872925 mm

Highest mean error: 3.536268949508667 mm for frame 65

Lowest mean error: 3.0072574615478516 mm for frame 26

Saving results

Total time: 54.67650747299194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521534
Iteration 2/25 | Loss: 0.00131247
Iteration 3/25 | Loss: 0.00123301
Iteration 4/25 | Loss: 0.00122262
Iteration 5/25 | Loss: 0.00122026
Iteration 6/25 | Loss: 0.00122026
Iteration 7/25 | Loss: 0.00122026
Iteration 8/25 | Loss: 0.00122026
Iteration 9/25 | Loss: 0.00122026
Iteration 10/25 | Loss: 0.00122026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012202603975310922, 0.0012202603975310922, 0.0012202603975310922, 0.0012202603975310922, 0.0012202603975310922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012202603975310922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03009963
Iteration 2/25 | Loss: 0.00054034
Iteration 3/25 | Loss: 0.00054032
Iteration 4/25 | Loss: 0.00054032
Iteration 5/25 | Loss: 0.00054032
Iteration 6/25 | Loss: 0.00054032
Iteration 7/25 | Loss: 0.00054032
Iteration 8/25 | Loss: 0.00054032
Iteration 9/25 | Loss: 0.00054032
Iteration 10/25 | Loss: 0.00054032
Iteration 11/25 | Loss: 0.00054032
Iteration 12/25 | Loss: 0.00054032
Iteration 13/25 | Loss: 0.00054032
Iteration 14/25 | Loss: 0.00054032
Iteration 15/25 | Loss: 0.00054032
Iteration 16/25 | Loss: 0.00054032
Iteration 17/25 | Loss: 0.00054032
Iteration 18/25 | Loss: 0.00054032
Iteration 19/25 | Loss: 0.00054032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000540315464604646, 0.000540315464604646, 0.000540315464604646, 0.000540315464604646, 0.000540315464604646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000540315464604646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054032
Iteration 2/1000 | Loss: 0.00003376
Iteration 3/1000 | Loss: 0.00002894
Iteration 4/1000 | Loss: 0.00002647
Iteration 5/1000 | Loss: 0.00002502
Iteration 6/1000 | Loss: 0.00002404
Iteration 7/1000 | Loss: 0.00002361
Iteration 8/1000 | Loss: 0.00002322
Iteration 9/1000 | Loss: 0.00002289
Iteration 10/1000 | Loss: 0.00002268
Iteration 11/1000 | Loss: 0.00002238
Iteration 12/1000 | Loss: 0.00002214
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002186
Iteration 15/1000 | Loss: 0.00002184
Iteration 16/1000 | Loss: 0.00002182
Iteration 17/1000 | Loss: 0.00002181
Iteration 18/1000 | Loss: 0.00002181
Iteration 19/1000 | Loss: 0.00002181
Iteration 20/1000 | Loss: 0.00002181
Iteration 21/1000 | Loss: 0.00002181
Iteration 22/1000 | Loss: 0.00002181
Iteration 23/1000 | Loss: 0.00002181
Iteration 24/1000 | Loss: 0.00002179
Iteration 25/1000 | Loss: 0.00002176
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00002175
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002172
Iteration 30/1000 | Loss: 0.00002172
Iteration 31/1000 | Loss: 0.00002172
Iteration 32/1000 | Loss: 0.00002172
Iteration 33/1000 | Loss: 0.00002172
Iteration 34/1000 | Loss: 0.00002172
Iteration 35/1000 | Loss: 0.00002171
Iteration 36/1000 | Loss: 0.00002171
Iteration 37/1000 | Loss: 0.00002171
Iteration 38/1000 | Loss: 0.00002170
Iteration 39/1000 | Loss: 0.00002169
Iteration 40/1000 | Loss: 0.00002169
Iteration 41/1000 | Loss: 0.00002169
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002169
Iteration 46/1000 | Loss: 0.00002169
Iteration 47/1000 | Loss: 0.00002168
Iteration 48/1000 | Loss: 0.00002168
Iteration 49/1000 | Loss: 0.00002168
Iteration 50/1000 | Loss: 0.00002168
Iteration 51/1000 | Loss: 0.00002168
Iteration 52/1000 | Loss: 0.00002168
Iteration 53/1000 | Loss: 0.00002168
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002168
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002166
Iteration 60/1000 | Loss: 0.00002166
Iteration 61/1000 | Loss: 0.00002165
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002165
Iteration 64/1000 | Loss: 0.00002165
Iteration 65/1000 | Loss: 0.00002165
Iteration 66/1000 | Loss: 0.00002165
Iteration 67/1000 | Loss: 0.00002165
Iteration 68/1000 | Loss: 0.00002164
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002163
Iteration 73/1000 | Loss: 0.00002163
Iteration 74/1000 | Loss: 0.00002163
Iteration 75/1000 | Loss: 0.00002163
Iteration 76/1000 | Loss: 0.00002163
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002163
Iteration 80/1000 | Loss: 0.00002163
Iteration 81/1000 | Loss: 0.00002163
Iteration 82/1000 | Loss: 0.00002163
Iteration 83/1000 | Loss: 0.00002163
Iteration 84/1000 | Loss: 0.00002162
Iteration 85/1000 | Loss: 0.00002162
Iteration 86/1000 | Loss: 0.00002162
Iteration 87/1000 | Loss: 0.00002162
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002162
Iteration 90/1000 | Loss: 0.00002162
Iteration 91/1000 | Loss: 0.00002162
Iteration 92/1000 | Loss: 0.00002161
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002158
Iteration 95/1000 | Loss: 0.00002158
Iteration 96/1000 | Loss: 0.00002158
Iteration 97/1000 | Loss: 0.00002158
Iteration 98/1000 | Loss: 0.00002158
Iteration 99/1000 | Loss: 0.00002158
Iteration 100/1000 | Loss: 0.00002158
Iteration 101/1000 | Loss: 0.00002158
Iteration 102/1000 | Loss: 0.00002158
Iteration 103/1000 | Loss: 0.00002158
Iteration 104/1000 | Loss: 0.00002157
Iteration 105/1000 | Loss: 0.00002157
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002157
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002156
Iteration 111/1000 | Loss: 0.00002156
Iteration 112/1000 | Loss: 0.00002156
Iteration 113/1000 | Loss: 0.00002156
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002155
Iteration 116/1000 | Loss: 0.00002155
Iteration 117/1000 | Loss: 0.00002155
Iteration 118/1000 | Loss: 0.00002154
Iteration 119/1000 | Loss: 0.00002154
Iteration 120/1000 | Loss: 0.00002154
Iteration 121/1000 | Loss: 0.00002154
Iteration 122/1000 | Loss: 0.00002154
Iteration 123/1000 | Loss: 0.00002153
Iteration 124/1000 | Loss: 0.00002153
Iteration 125/1000 | Loss: 0.00002153
Iteration 126/1000 | Loss: 0.00002153
Iteration 127/1000 | Loss: 0.00002153
Iteration 128/1000 | Loss: 0.00002153
Iteration 129/1000 | Loss: 0.00002153
Iteration 130/1000 | Loss: 0.00002153
Iteration 131/1000 | Loss: 0.00002153
Iteration 132/1000 | Loss: 0.00002153
Iteration 133/1000 | Loss: 0.00002153
Iteration 134/1000 | Loss: 0.00002153
Iteration 135/1000 | Loss: 0.00002153
Iteration 136/1000 | Loss: 0.00002152
Iteration 137/1000 | Loss: 0.00002152
Iteration 138/1000 | Loss: 0.00002152
Iteration 139/1000 | Loss: 0.00002152
Iteration 140/1000 | Loss: 0.00002152
Iteration 141/1000 | Loss: 0.00002152
Iteration 142/1000 | Loss: 0.00002152
Iteration 143/1000 | Loss: 0.00002152
Iteration 144/1000 | Loss: 0.00002152
Iteration 145/1000 | Loss: 0.00002152
Iteration 146/1000 | Loss: 0.00002152
Iteration 147/1000 | Loss: 0.00002152
Iteration 148/1000 | Loss: 0.00002152
Iteration 149/1000 | Loss: 0.00002152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.152231172658503e-05, 2.152231172658503e-05, 2.152231172658503e-05, 2.152231172658503e-05, 2.152231172658503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.152231172658503e-05

Optimization complete. Final v2v error: 4.061927318572998 mm

Highest mean error: 4.334405899047852 mm for frame 52

Lowest mean error: 3.871924877166748 mm for frame 148

Saving results

Total time: 41.803993940353394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891377
Iteration 2/25 | Loss: 0.00150609
Iteration 3/25 | Loss: 0.00136650
Iteration 4/25 | Loss: 0.00134894
Iteration 5/25 | Loss: 0.00134329
Iteration 6/25 | Loss: 0.00134220
Iteration 7/25 | Loss: 0.00134220
Iteration 8/25 | Loss: 0.00134220
Iteration 9/25 | Loss: 0.00134220
Iteration 10/25 | Loss: 0.00134220
Iteration 11/25 | Loss: 0.00134220
Iteration 12/25 | Loss: 0.00134220
Iteration 13/25 | Loss: 0.00134220
Iteration 14/25 | Loss: 0.00134220
Iteration 15/25 | Loss: 0.00134220
Iteration 16/25 | Loss: 0.00134220
Iteration 17/25 | Loss: 0.00134220
Iteration 18/25 | Loss: 0.00134220
Iteration 19/25 | Loss: 0.00134220
Iteration 20/25 | Loss: 0.00134220
Iteration 21/25 | Loss: 0.00134220
Iteration 22/25 | Loss: 0.00134220
Iteration 23/25 | Loss: 0.00134220
Iteration 24/25 | Loss: 0.00134220
Iteration 25/25 | Loss: 0.00134220

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38771367
Iteration 2/25 | Loss: 0.00113581
Iteration 3/25 | Loss: 0.00113566
Iteration 4/25 | Loss: 0.00113566
Iteration 5/25 | Loss: 0.00113566
Iteration 6/25 | Loss: 0.00113566
Iteration 7/25 | Loss: 0.00113566
Iteration 8/25 | Loss: 0.00113566
Iteration 9/25 | Loss: 0.00113566
Iteration 10/25 | Loss: 0.00113566
Iteration 11/25 | Loss: 0.00113566
Iteration 12/25 | Loss: 0.00113566
Iteration 13/25 | Loss: 0.00113566
Iteration 14/25 | Loss: 0.00113566
Iteration 15/25 | Loss: 0.00113566
Iteration 16/25 | Loss: 0.00113566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001135657075792551, 0.001135657075792551, 0.001135657075792551, 0.001135657075792551, 0.001135657075792551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001135657075792551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113566
Iteration 2/1000 | Loss: 0.00006859
Iteration 3/1000 | Loss: 0.00004513
Iteration 4/1000 | Loss: 0.00003516
Iteration 5/1000 | Loss: 0.00003126
Iteration 6/1000 | Loss: 0.00002999
Iteration 7/1000 | Loss: 0.00002890
Iteration 8/1000 | Loss: 0.00002820
Iteration 9/1000 | Loss: 0.00002764
Iteration 10/1000 | Loss: 0.00002717
Iteration 11/1000 | Loss: 0.00002697
Iteration 12/1000 | Loss: 0.00002692
Iteration 13/1000 | Loss: 0.00002675
Iteration 14/1000 | Loss: 0.00002658
Iteration 15/1000 | Loss: 0.00002645
Iteration 16/1000 | Loss: 0.00002638
Iteration 17/1000 | Loss: 0.00002636
Iteration 18/1000 | Loss: 0.00002635
Iteration 19/1000 | Loss: 0.00002634
Iteration 20/1000 | Loss: 0.00002633
Iteration 21/1000 | Loss: 0.00002633
Iteration 22/1000 | Loss: 0.00002632
Iteration 23/1000 | Loss: 0.00002632
Iteration 24/1000 | Loss: 0.00002631
Iteration 25/1000 | Loss: 0.00002631
Iteration 26/1000 | Loss: 0.00002630
Iteration 27/1000 | Loss: 0.00002630
Iteration 28/1000 | Loss: 0.00002629
Iteration 29/1000 | Loss: 0.00002626
Iteration 30/1000 | Loss: 0.00002626
Iteration 31/1000 | Loss: 0.00002624
Iteration 32/1000 | Loss: 0.00002623
Iteration 33/1000 | Loss: 0.00002622
Iteration 34/1000 | Loss: 0.00002621
Iteration 35/1000 | Loss: 0.00002621
Iteration 36/1000 | Loss: 0.00002620
Iteration 37/1000 | Loss: 0.00002620
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002618
Iteration 40/1000 | Loss: 0.00002617
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00002616
Iteration 43/1000 | Loss: 0.00002615
Iteration 44/1000 | Loss: 0.00002614
Iteration 45/1000 | Loss: 0.00002614
Iteration 46/1000 | Loss: 0.00002613
Iteration 47/1000 | Loss: 0.00002613
Iteration 48/1000 | Loss: 0.00002612
Iteration 49/1000 | Loss: 0.00002609
Iteration 50/1000 | Loss: 0.00002609
Iteration 51/1000 | Loss: 0.00002607
Iteration 52/1000 | Loss: 0.00002606
Iteration 53/1000 | Loss: 0.00002606
Iteration 54/1000 | Loss: 0.00002605
Iteration 55/1000 | Loss: 0.00002605
Iteration 56/1000 | Loss: 0.00002605
Iteration 57/1000 | Loss: 0.00002604
Iteration 58/1000 | Loss: 0.00002603
Iteration 59/1000 | Loss: 0.00002603
Iteration 60/1000 | Loss: 0.00002603
Iteration 61/1000 | Loss: 0.00002603
Iteration 62/1000 | Loss: 0.00002603
Iteration 63/1000 | Loss: 0.00002603
Iteration 64/1000 | Loss: 0.00002602
Iteration 65/1000 | Loss: 0.00002602
Iteration 66/1000 | Loss: 0.00002602
Iteration 67/1000 | Loss: 0.00002602
Iteration 68/1000 | Loss: 0.00002602
Iteration 69/1000 | Loss: 0.00002602
Iteration 70/1000 | Loss: 0.00002602
Iteration 71/1000 | Loss: 0.00002602
Iteration 72/1000 | Loss: 0.00002602
Iteration 73/1000 | Loss: 0.00002601
Iteration 74/1000 | Loss: 0.00002601
Iteration 75/1000 | Loss: 0.00002601
Iteration 76/1000 | Loss: 0.00002601
Iteration 77/1000 | Loss: 0.00002601
Iteration 78/1000 | Loss: 0.00002601
Iteration 79/1000 | Loss: 0.00002601
Iteration 80/1000 | Loss: 0.00002601
Iteration 81/1000 | Loss: 0.00002601
Iteration 82/1000 | Loss: 0.00002600
Iteration 83/1000 | Loss: 0.00002600
Iteration 84/1000 | Loss: 0.00002600
Iteration 85/1000 | Loss: 0.00002600
Iteration 86/1000 | Loss: 0.00002600
Iteration 87/1000 | Loss: 0.00002600
Iteration 88/1000 | Loss: 0.00002599
Iteration 89/1000 | Loss: 0.00002599
Iteration 90/1000 | Loss: 0.00002599
Iteration 91/1000 | Loss: 0.00002599
Iteration 92/1000 | Loss: 0.00002598
Iteration 93/1000 | Loss: 0.00002598
Iteration 94/1000 | Loss: 0.00002598
Iteration 95/1000 | Loss: 0.00002598
Iteration 96/1000 | Loss: 0.00002598
Iteration 97/1000 | Loss: 0.00002597
Iteration 98/1000 | Loss: 0.00002597
Iteration 99/1000 | Loss: 0.00002597
Iteration 100/1000 | Loss: 0.00002596
Iteration 101/1000 | Loss: 0.00002596
Iteration 102/1000 | Loss: 0.00002596
Iteration 103/1000 | Loss: 0.00002596
Iteration 104/1000 | Loss: 0.00002596
Iteration 105/1000 | Loss: 0.00002596
Iteration 106/1000 | Loss: 0.00002596
Iteration 107/1000 | Loss: 0.00002595
Iteration 108/1000 | Loss: 0.00002595
Iteration 109/1000 | Loss: 0.00002595
Iteration 110/1000 | Loss: 0.00002595
Iteration 111/1000 | Loss: 0.00002595
Iteration 112/1000 | Loss: 0.00002595
Iteration 113/1000 | Loss: 0.00002595
Iteration 114/1000 | Loss: 0.00002595
Iteration 115/1000 | Loss: 0.00002594
Iteration 116/1000 | Loss: 0.00002594
Iteration 117/1000 | Loss: 0.00002594
Iteration 118/1000 | Loss: 0.00002594
Iteration 119/1000 | Loss: 0.00002594
Iteration 120/1000 | Loss: 0.00002594
Iteration 121/1000 | Loss: 0.00002594
Iteration 122/1000 | Loss: 0.00002593
Iteration 123/1000 | Loss: 0.00002593
Iteration 124/1000 | Loss: 0.00002593
Iteration 125/1000 | Loss: 0.00002593
Iteration 126/1000 | Loss: 0.00002593
Iteration 127/1000 | Loss: 0.00002593
Iteration 128/1000 | Loss: 0.00002593
Iteration 129/1000 | Loss: 0.00002593
Iteration 130/1000 | Loss: 0.00002593
Iteration 131/1000 | Loss: 0.00002593
Iteration 132/1000 | Loss: 0.00002593
Iteration 133/1000 | Loss: 0.00002593
Iteration 134/1000 | Loss: 0.00002593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.5926658054231666e-05, 2.5926658054231666e-05, 2.5926658054231666e-05, 2.5926658054231666e-05, 2.5926658054231666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5926658054231666e-05

Optimization complete. Final v2v error: 3.864678382873535 mm

Highest mean error: 5.866354942321777 mm for frame 88

Lowest mean error: 2.815308094024658 mm for frame 48

Saving results

Total time: 40.18187355995178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660853
Iteration 2/25 | Loss: 0.00140965
Iteration 3/25 | Loss: 0.00128751
Iteration 4/25 | Loss: 0.00127608
Iteration 5/25 | Loss: 0.00127205
Iteration 6/25 | Loss: 0.00127123
Iteration 7/25 | Loss: 0.00127123
Iteration 8/25 | Loss: 0.00127123
Iteration 9/25 | Loss: 0.00127123
Iteration 10/25 | Loss: 0.00127123
Iteration 11/25 | Loss: 0.00127120
Iteration 12/25 | Loss: 0.00127120
Iteration 13/25 | Loss: 0.00127120
Iteration 14/25 | Loss: 0.00127120
Iteration 15/25 | Loss: 0.00127120
Iteration 16/25 | Loss: 0.00127120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001271202927455306, 0.001271202927455306, 0.001271202927455306, 0.001271202927455306, 0.001271202927455306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001271202927455306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49876785
Iteration 2/25 | Loss: 0.00090425
Iteration 3/25 | Loss: 0.00090425
Iteration 4/25 | Loss: 0.00090424
Iteration 5/25 | Loss: 0.00090424
Iteration 6/25 | Loss: 0.00090424
Iteration 7/25 | Loss: 0.00090424
Iteration 8/25 | Loss: 0.00090424
Iteration 9/25 | Loss: 0.00090424
Iteration 10/25 | Loss: 0.00090424
Iteration 11/25 | Loss: 0.00090424
Iteration 12/25 | Loss: 0.00090424
Iteration 13/25 | Loss: 0.00090424
Iteration 14/25 | Loss: 0.00090424
Iteration 15/25 | Loss: 0.00090424
Iteration 16/25 | Loss: 0.00090424
Iteration 17/25 | Loss: 0.00090424
Iteration 18/25 | Loss: 0.00090424
Iteration 19/25 | Loss: 0.00090424
Iteration 20/25 | Loss: 0.00090424
Iteration 21/25 | Loss: 0.00090424
Iteration 22/25 | Loss: 0.00090424
Iteration 23/25 | Loss: 0.00090424
Iteration 24/25 | Loss: 0.00090424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009042412857525051, 0.0009042412857525051, 0.0009042412857525051, 0.0009042412857525051, 0.0009042412857525051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009042412857525051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090424
Iteration 2/1000 | Loss: 0.00003420
Iteration 3/1000 | Loss: 0.00002163
Iteration 4/1000 | Loss: 0.00001849
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001607
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001482
Iteration 10/1000 | Loss: 0.00001472
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001412
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001396
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001369
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001366
Iteration 34/1000 | Loss: 0.00001366
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001365
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001362
Iteration 53/1000 | Loss: 0.00001362
Iteration 54/1000 | Loss: 0.00001361
Iteration 55/1000 | Loss: 0.00001361
Iteration 56/1000 | Loss: 0.00001361
Iteration 57/1000 | Loss: 0.00001361
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001360
Iteration 60/1000 | Loss: 0.00001360
Iteration 61/1000 | Loss: 0.00001360
Iteration 62/1000 | Loss: 0.00001360
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001358
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001355
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001353
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001352
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001347
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001346
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001345
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001345
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001344
Iteration 114/1000 | Loss: 0.00001344
Iteration 115/1000 | Loss: 0.00001344
Iteration 116/1000 | Loss: 0.00001344
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001343
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001343
Iteration 133/1000 | Loss: 0.00001343
Iteration 134/1000 | Loss: 0.00001343
Iteration 135/1000 | Loss: 0.00001343
Iteration 136/1000 | Loss: 0.00001343
Iteration 137/1000 | Loss: 0.00001343
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3425948054646142e-05, 1.3425948054646142e-05, 1.3425948054646142e-05, 1.3425948054646142e-05, 1.3425948054646142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3425948054646142e-05

Optimization complete. Final v2v error: 3.095661163330078 mm

Highest mean error: 4.07943058013916 mm for frame 72

Lowest mean error: 2.727877140045166 mm for frame 4

Saving results

Total time: 39.61348557472229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042203
Iteration 2/25 | Loss: 0.01042203
Iteration 3/25 | Loss: 0.01042203
Iteration 4/25 | Loss: 0.01042203
Iteration 5/25 | Loss: 0.01042202
Iteration 6/25 | Loss: 0.01042202
Iteration 7/25 | Loss: 0.01042202
Iteration 8/25 | Loss: 0.01042202
Iteration 9/25 | Loss: 0.01042202
Iteration 10/25 | Loss: 0.01042202
Iteration 11/25 | Loss: 0.01042202
Iteration 12/25 | Loss: 0.01042202
Iteration 13/25 | Loss: 0.01042201
Iteration 14/25 | Loss: 0.01042201
Iteration 15/25 | Loss: 0.01042201
Iteration 16/25 | Loss: 0.01042201
Iteration 17/25 | Loss: 0.01042201
Iteration 18/25 | Loss: 0.01042201
Iteration 19/25 | Loss: 0.01042201
Iteration 20/25 | Loss: 0.01042201
Iteration 21/25 | Loss: 0.01042200
Iteration 22/25 | Loss: 0.01042200
Iteration 23/25 | Loss: 0.01042200
Iteration 24/25 | Loss: 0.01042200
Iteration 25/25 | Loss: 0.01042200

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74963439
Iteration 2/25 | Loss: 0.08397119
Iteration 3/25 | Loss: 0.08396015
Iteration 4/25 | Loss: 0.08396015
Iteration 5/25 | Loss: 0.08396014
Iteration 6/25 | Loss: 0.08396013
Iteration 7/25 | Loss: 0.08396013
Iteration 8/25 | Loss: 0.08396013
Iteration 9/25 | Loss: 0.08396013
Iteration 10/25 | Loss: 0.08396013
Iteration 11/25 | Loss: 0.08396012
Iteration 12/25 | Loss: 0.08396012
Iteration 13/25 | Loss: 0.08396012
Iteration 14/25 | Loss: 0.08396012
Iteration 15/25 | Loss: 0.08396012
Iteration 16/25 | Loss: 0.08396012
Iteration 17/25 | Loss: 0.08396012
Iteration 18/25 | Loss: 0.08396012
Iteration 19/25 | Loss: 0.08396012
Iteration 20/25 | Loss: 0.08396012
Iteration 21/25 | Loss: 0.08396012
Iteration 22/25 | Loss: 0.08396012
Iteration 23/25 | Loss: 0.08396012
Iteration 24/25 | Loss: 0.08396012
Iteration 25/25 | Loss: 0.08396012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08396012
Iteration 2/1000 | Loss: 0.00185612
Iteration 3/1000 | Loss: 0.00060244
Iteration 4/1000 | Loss: 0.00051257
Iteration 5/1000 | Loss: 0.00036452
Iteration 6/1000 | Loss: 0.00012389
Iteration 7/1000 | Loss: 0.00009226
Iteration 8/1000 | Loss: 0.00012294
Iteration 9/1000 | Loss: 0.00005573
Iteration 10/1000 | Loss: 0.00007433
Iteration 11/1000 | Loss: 0.00003679
Iteration 12/1000 | Loss: 0.00003247
Iteration 13/1000 | Loss: 0.00027245
Iteration 14/1000 | Loss: 0.00027284
Iteration 15/1000 | Loss: 0.00002958
Iteration 16/1000 | Loss: 0.00002750
Iteration 17/1000 | Loss: 0.00009841
Iteration 18/1000 | Loss: 0.00029929
Iteration 19/1000 | Loss: 0.00003710
Iteration 20/1000 | Loss: 0.00003049
Iteration 21/1000 | Loss: 0.00002558
Iteration 22/1000 | Loss: 0.00002472
Iteration 23/1000 | Loss: 0.00036969
Iteration 24/1000 | Loss: 0.00005235
Iteration 25/1000 | Loss: 0.00002373
Iteration 26/1000 | Loss: 0.00025316
Iteration 27/1000 | Loss: 0.00010736
Iteration 28/1000 | Loss: 0.00006635
Iteration 29/1000 | Loss: 0.00014001
Iteration 30/1000 | Loss: 0.00003307
Iteration 31/1000 | Loss: 0.00002227
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00004550
Iteration 34/1000 | Loss: 0.00002428
Iteration 35/1000 | Loss: 0.00002093
Iteration 36/1000 | Loss: 0.00003177
Iteration 37/1000 | Loss: 0.00002031
Iteration 38/1000 | Loss: 0.00002004
Iteration 39/1000 | Loss: 0.00002004
Iteration 40/1000 | Loss: 0.00002003
Iteration 41/1000 | Loss: 0.00002002
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001978
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001950
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00004908
Iteration 50/1000 | Loss: 0.00002553
Iteration 51/1000 | Loss: 0.00002028
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001928
Iteration 56/1000 | Loss: 0.00001928
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001928
Iteration 60/1000 | Loss: 0.00001928
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001926
Iteration 66/1000 | Loss: 0.00001926
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001925
Iteration 72/1000 | Loss: 0.00001996
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001922
Iteration 77/1000 | Loss: 0.00001922
Iteration 78/1000 | Loss: 0.00001922
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001921
Iteration 81/1000 | Loss: 0.00001921
Iteration 82/1000 | Loss: 0.00001921
Iteration 83/1000 | Loss: 0.00001921
Iteration 84/1000 | Loss: 0.00001921
Iteration 85/1000 | Loss: 0.00001921
Iteration 86/1000 | Loss: 0.00001921
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001921
Iteration 94/1000 | Loss: 0.00001921
Iteration 95/1000 | Loss: 0.00001921
Iteration 96/1000 | Loss: 0.00001921
Iteration 97/1000 | Loss: 0.00001921
Iteration 98/1000 | Loss: 0.00001921
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001921
Iteration 101/1000 | Loss: 0.00001921
Iteration 102/1000 | Loss: 0.00001921
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.9210801838198677e-05, 1.9210801838198677e-05, 1.9210801838198677e-05, 1.9210801838198677e-05, 1.9210801838198677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9210801838198677e-05

Optimization complete. Final v2v error: 3.7652082443237305 mm

Highest mean error: 4.216028690338135 mm for frame 171

Lowest mean error: 3.4602763652801514 mm for frame 12

Saving results

Total time: 77.47804045677185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780767
Iteration 2/25 | Loss: 0.00136830
Iteration 3/25 | Loss: 0.00127807
Iteration 4/25 | Loss: 0.00126841
Iteration 5/25 | Loss: 0.00126476
Iteration 6/25 | Loss: 0.00126434
Iteration 7/25 | Loss: 0.00126434
Iteration 8/25 | Loss: 0.00126434
Iteration 9/25 | Loss: 0.00126434
Iteration 10/25 | Loss: 0.00126434
Iteration 11/25 | Loss: 0.00126434
Iteration 12/25 | Loss: 0.00126434
Iteration 13/25 | Loss: 0.00126434
Iteration 14/25 | Loss: 0.00126434
Iteration 15/25 | Loss: 0.00126434
Iteration 16/25 | Loss: 0.00126434
Iteration 17/25 | Loss: 0.00126434
Iteration 18/25 | Loss: 0.00126434
Iteration 19/25 | Loss: 0.00126434
Iteration 20/25 | Loss: 0.00126434
Iteration 21/25 | Loss: 0.00126434
Iteration 22/25 | Loss: 0.00126434
Iteration 23/25 | Loss: 0.00126434
Iteration 24/25 | Loss: 0.00126434
Iteration 25/25 | Loss: 0.00126434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50312352
Iteration 2/25 | Loss: 0.00085350
Iteration 3/25 | Loss: 0.00085350
Iteration 4/25 | Loss: 0.00085350
Iteration 5/25 | Loss: 0.00085350
Iteration 6/25 | Loss: 0.00085350
Iteration 7/25 | Loss: 0.00085350
Iteration 8/25 | Loss: 0.00085350
Iteration 9/25 | Loss: 0.00085350
Iteration 10/25 | Loss: 0.00085350
Iteration 11/25 | Loss: 0.00085350
Iteration 12/25 | Loss: 0.00085350
Iteration 13/25 | Loss: 0.00085350
Iteration 14/25 | Loss: 0.00085350
Iteration 15/25 | Loss: 0.00085350
Iteration 16/25 | Loss: 0.00085350
Iteration 17/25 | Loss: 0.00085350
Iteration 18/25 | Loss: 0.00085350
Iteration 19/25 | Loss: 0.00085350
Iteration 20/25 | Loss: 0.00085350
Iteration 21/25 | Loss: 0.00085350
Iteration 22/25 | Loss: 0.00085350
Iteration 23/25 | Loss: 0.00085350
Iteration 24/25 | Loss: 0.00085350
Iteration 25/25 | Loss: 0.00085350

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085350
Iteration 2/1000 | Loss: 0.00003366
Iteration 3/1000 | Loss: 0.00002009
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001425
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001384
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001357
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001351
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001340
Iteration 29/1000 | Loss: 0.00001338
Iteration 30/1000 | Loss: 0.00001338
Iteration 31/1000 | Loss: 0.00001337
Iteration 32/1000 | Loss: 0.00001337
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001337
Iteration 35/1000 | Loss: 0.00001336
Iteration 36/1000 | Loss: 0.00001336
Iteration 37/1000 | Loss: 0.00001336
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001329
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001328
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001326
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001325
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001322
Iteration 72/1000 | Loss: 0.00001322
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001319
Iteration 75/1000 | Loss: 0.00001319
Iteration 76/1000 | Loss: 0.00001319
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001315
Iteration 94/1000 | Loss: 0.00001315
Iteration 95/1000 | Loss: 0.00001315
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001314
Iteration 98/1000 | Loss: 0.00001314
Iteration 99/1000 | Loss: 0.00001313
Iteration 100/1000 | Loss: 0.00001313
Iteration 101/1000 | Loss: 0.00001313
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001309
Iteration 118/1000 | Loss: 0.00001309
Iteration 119/1000 | Loss: 0.00001309
Iteration 120/1000 | Loss: 0.00001309
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001309
Iteration 129/1000 | Loss: 0.00001309
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001308
Iteration 133/1000 | Loss: 0.00001308
Iteration 134/1000 | Loss: 0.00001308
Iteration 135/1000 | Loss: 0.00001308
Iteration 136/1000 | Loss: 0.00001308
Iteration 137/1000 | Loss: 0.00001308
Iteration 138/1000 | Loss: 0.00001308
Iteration 139/1000 | Loss: 0.00001308
Iteration 140/1000 | Loss: 0.00001308
Iteration 141/1000 | Loss: 0.00001308
Iteration 142/1000 | Loss: 0.00001308
Iteration 143/1000 | Loss: 0.00001307
Iteration 144/1000 | Loss: 0.00001307
Iteration 145/1000 | Loss: 0.00001307
Iteration 146/1000 | Loss: 0.00001307
Iteration 147/1000 | Loss: 0.00001307
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001307
Iteration 150/1000 | Loss: 0.00001307
Iteration 151/1000 | Loss: 0.00001307
Iteration 152/1000 | Loss: 0.00001307
Iteration 153/1000 | Loss: 0.00001306
Iteration 154/1000 | Loss: 0.00001306
Iteration 155/1000 | Loss: 0.00001306
Iteration 156/1000 | Loss: 0.00001306
Iteration 157/1000 | Loss: 0.00001306
Iteration 158/1000 | Loss: 0.00001306
Iteration 159/1000 | Loss: 0.00001306
Iteration 160/1000 | Loss: 0.00001306
Iteration 161/1000 | Loss: 0.00001306
Iteration 162/1000 | Loss: 0.00001306
Iteration 163/1000 | Loss: 0.00001306
Iteration 164/1000 | Loss: 0.00001306
Iteration 165/1000 | Loss: 0.00001306
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001305
Iteration 169/1000 | Loss: 0.00001305
Iteration 170/1000 | Loss: 0.00001305
Iteration 171/1000 | Loss: 0.00001305
Iteration 172/1000 | Loss: 0.00001305
Iteration 173/1000 | Loss: 0.00001305
Iteration 174/1000 | Loss: 0.00001305
Iteration 175/1000 | Loss: 0.00001305
Iteration 176/1000 | Loss: 0.00001305
Iteration 177/1000 | Loss: 0.00001305
Iteration 178/1000 | Loss: 0.00001305
Iteration 179/1000 | Loss: 0.00001305
Iteration 180/1000 | Loss: 0.00001305
Iteration 181/1000 | Loss: 0.00001305
Iteration 182/1000 | Loss: 0.00001305
Iteration 183/1000 | Loss: 0.00001305
Iteration 184/1000 | Loss: 0.00001305
Iteration 185/1000 | Loss: 0.00001305
Iteration 186/1000 | Loss: 0.00001305
Iteration 187/1000 | Loss: 0.00001305
Iteration 188/1000 | Loss: 0.00001305
Iteration 189/1000 | Loss: 0.00001305
Iteration 190/1000 | Loss: 0.00001305
Iteration 191/1000 | Loss: 0.00001305
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.3049495464656502e-05, 1.3049495464656502e-05, 1.3049495464656502e-05, 1.3049495464656502e-05, 1.3049495464656502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3049495464656502e-05

Optimization complete. Final v2v error: 3.0539300441741943 mm

Highest mean error: 3.88321590423584 mm for frame 59

Lowest mean error: 2.7744970321655273 mm for frame 34

Saving results

Total time: 40.820900678634644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445031
Iteration 2/25 | Loss: 0.00134575
Iteration 3/25 | Loss: 0.00129992
Iteration 4/25 | Loss: 0.00129324
Iteration 5/25 | Loss: 0.00129108
Iteration 6/25 | Loss: 0.00129096
Iteration 7/25 | Loss: 0.00129096
Iteration 8/25 | Loss: 0.00129096
Iteration 9/25 | Loss: 0.00129096
Iteration 10/25 | Loss: 0.00129096
Iteration 11/25 | Loss: 0.00129096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012909616343677044, 0.0012909616343677044, 0.0012909616343677044, 0.0012909616343677044, 0.0012909616343677044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012909616343677044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05762458
Iteration 2/25 | Loss: 0.00064642
Iteration 3/25 | Loss: 0.00064642
Iteration 4/25 | Loss: 0.00064642
Iteration 5/25 | Loss: 0.00064642
Iteration 6/25 | Loss: 0.00064642
Iteration 7/25 | Loss: 0.00064642
Iteration 8/25 | Loss: 0.00064642
Iteration 9/25 | Loss: 0.00064642
Iteration 10/25 | Loss: 0.00064642
Iteration 11/25 | Loss: 0.00064642
Iteration 12/25 | Loss: 0.00064642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006464204634539783, 0.0006464204634539783, 0.0006464204634539783, 0.0006464204634539783, 0.0006464204634539783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006464204634539783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064642
Iteration 2/1000 | Loss: 0.00004198
Iteration 3/1000 | Loss: 0.00002653
Iteration 4/1000 | Loss: 0.00002248
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001878
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001728
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001669
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001668
Iteration 18/1000 | Loss: 0.00001668
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001667
Iteration 23/1000 | Loss: 0.00001667
Iteration 24/1000 | Loss: 0.00001667
Iteration 25/1000 | Loss: 0.00001667
Iteration 26/1000 | Loss: 0.00001667
Iteration 27/1000 | Loss: 0.00001667
Iteration 28/1000 | Loss: 0.00001667
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001666
Iteration 31/1000 | Loss: 0.00001666
Iteration 32/1000 | Loss: 0.00001666
Iteration 33/1000 | Loss: 0.00001666
Iteration 34/1000 | Loss: 0.00001666
Iteration 35/1000 | Loss: 0.00001666
Iteration 36/1000 | Loss: 0.00001666
Iteration 37/1000 | Loss: 0.00001665
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001662
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001662
Iteration 42/1000 | Loss: 0.00001662
Iteration 43/1000 | Loss: 0.00001662
Iteration 44/1000 | Loss: 0.00001662
Iteration 45/1000 | Loss: 0.00001662
Iteration 46/1000 | Loss: 0.00001661
Iteration 47/1000 | Loss: 0.00001661
Iteration 48/1000 | Loss: 0.00001661
Iteration 49/1000 | Loss: 0.00001660
Iteration 50/1000 | Loss: 0.00001660
Iteration 51/1000 | Loss: 0.00001659
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001658
Iteration 55/1000 | Loss: 0.00001658
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001658
Iteration 59/1000 | Loss: 0.00001658
Iteration 60/1000 | Loss: 0.00001658
Iteration 61/1000 | Loss: 0.00001658
Iteration 62/1000 | Loss: 0.00001658
Iteration 63/1000 | Loss: 0.00001658
Iteration 64/1000 | Loss: 0.00001657
Iteration 65/1000 | Loss: 0.00001657
Iteration 66/1000 | Loss: 0.00001656
Iteration 67/1000 | Loss: 0.00001656
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001655
Iteration 70/1000 | Loss: 0.00001655
Iteration 71/1000 | Loss: 0.00001655
Iteration 72/1000 | Loss: 0.00001655
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001654
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001653
Iteration 81/1000 | Loss: 0.00001652
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001650
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001650
Iteration 89/1000 | Loss: 0.00001650
Iteration 90/1000 | Loss: 0.00001650
Iteration 91/1000 | Loss: 0.00001650
Iteration 92/1000 | Loss: 0.00001650
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Iteration 97/1000 | Loss: 0.00001649
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001648
Iteration 100/1000 | Loss: 0.00001648
Iteration 101/1000 | Loss: 0.00001648
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001647
Iteration 105/1000 | Loss: 0.00001647
Iteration 106/1000 | Loss: 0.00001647
Iteration 107/1000 | Loss: 0.00001647
Iteration 108/1000 | Loss: 0.00001647
Iteration 109/1000 | Loss: 0.00001647
Iteration 110/1000 | Loss: 0.00001647
Iteration 111/1000 | Loss: 0.00001647
Iteration 112/1000 | Loss: 0.00001647
Iteration 113/1000 | Loss: 0.00001647
Iteration 114/1000 | Loss: 0.00001647
Iteration 115/1000 | Loss: 0.00001647
Iteration 116/1000 | Loss: 0.00001647
Iteration 117/1000 | Loss: 0.00001647
Iteration 118/1000 | Loss: 0.00001647
Iteration 119/1000 | Loss: 0.00001646
Iteration 120/1000 | Loss: 0.00001646
Iteration 121/1000 | Loss: 0.00001646
Iteration 122/1000 | Loss: 0.00001646
Iteration 123/1000 | Loss: 0.00001646
Iteration 124/1000 | Loss: 0.00001645
Iteration 125/1000 | Loss: 0.00001645
Iteration 126/1000 | Loss: 0.00001645
Iteration 127/1000 | Loss: 0.00001645
Iteration 128/1000 | Loss: 0.00001645
Iteration 129/1000 | Loss: 0.00001645
Iteration 130/1000 | Loss: 0.00001645
Iteration 131/1000 | Loss: 0.00001644
Iteration 132/1000 | Loss: 0.00001644
Iteration 133/1000 | Loss: 0.00001644
Iteration 134/1000 | Loss: 0.00001644
Iteration 135/1000 | Loss: 0.00001644
Iteration 136/1000 | Loss: 0.00001644
Iteration 137/1000 | Loss: 0.00001644
Iteration 138/1000 | Loss: 0.00001643
Iteration 139/1000 | Loss: 0.00001643
Iteration 140/1000 | Loss: 0.00001643
Iteration 141/1000 | Loss: 0.00001643
Iteration 142/1000 | Loss: 0.00001643
Iteration 143/1000 | Loss: 0.00001643
Iteration 144/1000 | Loss: 0.00001642
Iteration 145/1000 | Loss: 0.00001642
Iteration 146/1000 | Loss: 0.00001642
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001642
Iteration 149/1000 | Loss: 0.00001642
Iteration 150/1000 | Loss: 0.00001642
Iteration 151/1000 | Loss: 0.00001642
Iteration 152/1000 | Loss: 0.00001642
Iteration 153/1000 | Loss: 0.00001642
Iteration 154/1000 | Loss: 0.00001642
Iteration 155/1000 | Loss: 0.00001642
Iteration 156/1000 | Loss: 0.00001642
Iteration 157/1000 | Loss: 0.00001641
Iteration 158/1000 | Loss: 0.00001641
Iteration 159/1000 | Loss: 0.00001641
Iteration 160/1000 | Loss: 0.00001641
Iteration 161/1000 | Loss: 0.00001641
Iteration 162/1000 | Loss: 0.00001641
Iteration 163/1000 | Loss: 0.00001641
Iteration 164/1000 | Loss: 0.00001641
Iteration 165/1000 | Loss: 0.00001641
Iteration 166/1000 | Loss: 0.00001641
Iteration 167/1000 | Loss: 0.00001641
Iteration 168/1000 | Loss: 0.00001641
Iteration 169/1000 | Loss: 0.00001641
Iteration 170/1000 | Loss: 0.00001641
Iteration 171/1000 | Loss: 0.00001641
Iteration 172/1000 | Loss: 0.00001641
Iteration 173/1000 | Loss: 0.00001641
Iteration 174/1000 | Loss: 0.00001640
Iteration 175/1000 | Loss: 0.00001640
Iteration 176/1000 | Loss: 0.00001640
Iteration 177/1000 | Loss: 0.00001640
Iteration 178/1000 | Loss: 0.00001640
Iteration 179/1000 | Loss: 0.00001640
Iteration 180/1000 | Loss: 0.00001640
Iteration 181/1000 | Loss: 0.00001640
Iteration 182/1000 | Loss: 0.00001640
Iteration 183/1000 | Loss: 0.00001639
Iteration 184/1000 | Loss: 0.00001639
Iteration 185/1000 | Loss: 0.00001639
Iteration 186/1000 | Loss: 0.00001639
Iteration 187/1000 | Loss: 0.00001639
Iteration 188/1000 | Loss: 0.00001639
Iteration 189/1000 | Loss: 0.00001639
Iteration 190/1000 | Loss: 0.00001639
Iteration 191/1000 | Loss: 0.00001639
Iteration 192/1000 | Loss: 0.00001639
Iteration 193/1000 | Loss: 0.00001639
Iteration 194/1000 | Loss: 0.00001638
Iteration 195/1000 | Loss: 0.00001638
Iteration 196/1000 | Loss: 0.00001638
Iteration 197/1000 | Loss: 0.00001638
Iteration 198/1000 | Loss: 0.00001638
Iteration 199/1000 | Loss: 0.00001638
Iteration 200/1000 | Loss: 0.00001638
Iteration 201/1000 | Loss: 0.00001638
Iteration 202/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.6384396076318808e-05, 1.6384396076318808e-05, 1.6384396076318808e-05, 1.6384396076318808e-05, 1.6384396076318808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6384396076318808e-05

Optimization complete. Final v2v error: 3.483278751373291 mm

Highest mean error: 3.509258985519409 mm for frame 55

Lowest mean error: 3.440016984939575 mm for frame 0

Saving results

Total time: 36.66083574295044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356765
Iteration 2/25 | Loss: 0.00131512
Iteration 3/25 | Loss: 0.00124248
Iteration 4/25 | Loss: 0.00123076
Iteration 5/25 | Loss: 0.00122640
Iteration 6/25 | Loss: 0.00122485
Iteration 7/25 | Loss: 0.00122453
Iteration 8/25 | Loss: 0.00122453
Iteration 9/25 | Loss: 0.00122453
Iteration 10/25 | Loss: 0.00122453
Iteration 11/25 | Loss: 0.00122453
Iteration 12/25 | Loss: 0.00122453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012245271354913712, 0.0012245271354913712, 0.0012245271354913712, 0.0012245271354913712, 0.0012245271354913712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012245271354913712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42829382
Iteration 2/25 | Loss: 0.00095272
Iteration 3/25 | Loss: 0.00095272
Iteration 4/25 | Loss: 0.00095272
Iteration 5/25 | Loss: 0.00095272
Iteration 6/25 | Loss: 0.00095272
Iteration 7/25 | Loss: 0.00095272
Iteration 8/25 | Loss: 0.00095272
Iteration 9/25 | Loss: 0.00095272
Iteration 10/25 | Loss: 0.00095272
Iteration 11/25 | Loss: 0.00095272
Iteration 12/25 | Loss: 0.00095272
Iteration 13/25 | Loss: 0.00095272
Iteration 14/25 | Loss: 0.00095272
Iteration 15/25 | Loss: 0.00095272
Iteration 16/25 | Loss: 0.00095272
Iteration 17/25 | Loss: 0.00095272
Iteration 18/25 | Loss: 0.00095272
Iteration 19/25 | Loss: 0.00095272
Iteration 20/25 | Loss: 0.00095272
Iteration 21/25 | Loss: 0.00095272
Iteration 22/25 | Loss: 0.00095272
Iteration 23/25 | Loss: 0.00095272
Iteration 24/25 | Loss: 0.00095272
Iteration 25/25 | Loss: 0.00095272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095272
Iteration 2/1000 | Loss: 0.00002984
Iteration 3/1000 | Loss: 0.00001935
Iteration 4/1000 | Loss: 0.00001698
Iteration 5/1000 | Loss: 0.00001566
Iteration 6/1000 | Loss: 0.00001513
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001395
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001385
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001382
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001328
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001318
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00001315
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001312
Iteration 133/1000 | Loss: 0.00001312
Iteration 134/1000 | Loss: 0.00001312
Iteration 135/1000 | Loss: 0.00001312
Iteration 136/1000 | Loss: 0.00001311
Iteration 137/1000 | Loss: 0.00001311
Iteration 138/1000 | Loss: 0.00001311
Iteration 139/1000 | Loss: 0.00001311
Iteration 140/1000 | Loss: 0.00001310
Iteration 141/1000 | Loss: 0.00001310
Iteration 142/1000 | Loss: 0.00001310
Iteration 143/1000 | Loss: 0.00001310
Iteration 144/1000 | Loss: 0.00001310
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001308
Iteration 149/1000 | Loss: 0.00001308
Iteration 150/1000 | Loss: 0.00001308
Iteration 151/1000 | Loss: 0.00001308
Iteration 152/1000 | Loss: 0.00001308
Iteration 153/1000 | Loss: 0.00001307
Iteration 154/1000 | Loss: 0.00001307
Iteration 155/1000 | Loss: 0.00001307
Iteration 156/1000 | Loss: 0.00001307
Iteration 157/1000 | Loss: 0.00001307
Iteration 158/1000 | Loss: 0.00001307
Iteration 159/1000 | Loss: 0.00001307
Iteration 160/1000 | Loss: 0.00001307
Iteration 161/1000 | Loss: 0.00001307
Iteration 162/1000 | Loss: 0.00001307
Iteration 163/1000 | Loss: 0.00001307
Iteration 164/1000 | Loss: 0.00001306
Iteration 165/1000 | Loss: 0.00001306
Iteration 166/1000 | Loss: 0.00001306
Iteration 167/1000 | Loss: 0.00001306
Iteration 168/1000 | Loss: 0.00001306
Iteration 169/1000 | Loss: 0.00001306
Iteration 170/1000 | Loss: 0.00001306
Iteration 171/1000 | Loss: 0.00001306
Iteration 172/1000 | Loss: 0.00001306
Iteration 173/1000 | Loss: 0.00001306
Iteration 174/1000 | Loss: 0.00001306
Iteration 175/1000 | Loss: 0.00001306
Iteration 176/1000 | Loss: 0.00001306
Iteration 177/1000 | Loss: 0.00001305
Iteration 178/1000 | Loss: 0.00001305
Iteration 179/1000 | Loss: 0.00001305
Iteration 180/1000 | Loss: 0.00001305
Iteration 181/1000 | Loss: 0.00001305
Iteration 182/1000 | Loss: 0.00001305
Iteration 183/1000 | Loss: 0.00001305
Iteration 184/1000 | Loss: 0.00001305
Iteration 185/1000 | Loss: 0.00001305
Iteration 186/1000 | Loss: 0.00001305
Iteration 187/1000 | Loss: 0.00001305
Iteration 188/1000 | Loss: 0.00001305
Iteration 189/1000 | Loss: 0.00001305
Iteration 190/1000 | Loss: 0.00001305
Iteration 191/1000 | Loss: 0.00001305
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Iteration 194/1000 | Loss: 0.00001305
Iteration 195/1000 | Loss: 0.00001305
Iteration 196/1000 | Loss: 0.00001305
Iteration 197/1000 | Loss: 0.00001305
Iteration 198/1000 | Loss: 0.00001305
Iteration 199/1000 | Loss: 0.00001305
Iteration 200/1000 | Loss: 0.00001305
Iteration 201/1000 | Loss: 0.00001305
Iteration 202/1000 | Loss: 0.00001305
Iteration 203/1000 | Loss: 0.00001305
Iteration 204/1000 | Loss: 0.00001305
Iteration 205/1000 | Loss: 0.00001305
Iteration 206/1000 | Loss: 0.00001305
Iteration 207/1000 | Loss: 0.00001305
Iteration 208/1000 | Loss: 0.00001305
Iteration 209/1000 | Loss: 0.00001305
Iteration 210/1000 | Loss: 0.00001305
Iteration 211/1000 | Loss: 0.00001305
Iteration 212/1000 | Loss: 0.00001305
Iteration 213/1000 | Loss: 0.00001305
Iteration 214/1000 | Loss: 0.00001305
Iteration 215/1000 | Loss: 0.00001305
Iteration 216/1000 | Loss: 0.00001305
Iteration 217/1000 | Loss: 0.00001305
Iteration 218/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.3050830602878705e-05, 1.3050830602878705e-05, 1.3050830602878705e-05, 1.3050830602878705e-05, 1.3050830602878705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3050830602878705e-05

Optimization complete. Final v2v error: 3.0881197452545166 mm

Highest mean error: 3.8340096473693848 mm for frame 10

Lowest mean error: 2.6838181018829346 mm for frame 133

Saving results

Total time: 41.96268343925476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400917
Iteration 2/25 | Loss: 0.00137226
Iteration 3/25 | Loss: 0.00130344
Iteration 4/25 | Loss: 0.00128463
Iteration 5/25 | Loss: 0.00127849
Iteration 6/25 | Loss: 0.00127828
Iteration 7/25 | Loss: 0.00127828
Iteration 8/25 | Loss: 0.00127828
Iteration 9/25 | Loss: 0.00127828
Iteration 10/25 | Loss: 0.00127828
Iteration 11/25 | Loss: 0.00127828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012782847043126822, 0.0012782847043126822, 0.0012782847043126822, 0.0012782847043126822, 0.0012782847043126822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012782847043126822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32213640
Iteration 2/25 | Loss: 0.00090720
Iteration 3/25 | Loss: 0.00090720
Iteration 4/25 | Loss: 0.00090720
Iteration 5/25 | Loss: 0.00090720
Iteration 6/25 | Loss: 0.00090720
Iteration 7/25 | Loss: 0.00090720
Iteration 8/25 | Loss: 0.00090720
Iteration 9/25 | Loss: 0.00090720
Iteration 10/25 | Loss: 0.00090720
Iteration 11/25 | Loss: 0.00090720
Iteration 12/25 | Loss: 0.00090720
Iteration 13/25 | Loss: 0.00090720
Iteration 14/25 | Loss: 0.00090720
Iteration 15/25 | Loss: 0.00090720
Iteration 16/25 | Loss: 0.00090720
Iteration 17/25 | Loss: 0.00090720
Iteration 18/25 | Loss: 0.00090720
Iteration 19/25 | Loss: 0.00090720
Iteration 20/25 | Loss: 0.00090720
Iteration 21/25 | Loss: 0.00090720
Iteration 22/25 | Loss: 0.00090720
Iteration 23/25 | Loss: 0.00090720
Iteration 24/25 | Loss: 0.00090720
Iteration 25/25 | Loss: 0.00090720

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090720
Iteration 2/1000 | Loss: 0.00002901
Iteration 3/1000 | Loss: 0.00002285
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002099
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00001994
Iteration 8/1000 | Loss: 0.00001961
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001916
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001888
Iteration 14/1000 | Loss: 0.00001872
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001862
Iteration 18/1000 | Loss: 0.00001861
Iteration 19/1000 | Loss: 0.00001860
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001846
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001841
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001841
Iteration 36/1000 | Loss: 0.00001841
Iteration 37/1000 | Loss: 0.00001840
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001839
Iteration 43/1000 | Loss: 0.00001838
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001838
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00001838
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001834
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001830
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001828
Iteration 69/1000 | Loss: 0.00001828
Iteration 70/1000 | Loss: 0.00001827
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001824
Iteration 73/1000 | Loss: 0.00001824
Iteration 74/1000 | Loss: 0.00001824
Iteration 75/1000 | Loss: 0.00001824
Iteration 76/1000 | Loss: 0.00001823
Iteration 77/1000 | Loss: 0.00001823
Iteration 78/1000 | Loss: 0.00001823
Iteration 79/1000 | Loss: 0.00001822
Iteration 80/1000 | Loss: 0.00001822
Iteration 81/1000 | Loss: 0.00001822
Iteration 82/1000 | Loss: 0.00001821
Iteration 83/1000 | Loss: 0.00001821
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001821
Iteration 87/1000 | Loss: 0.00001820
Iteration 88/1000 | Loss: 0.00001820
Iteration 89/1000 | Loss: 0.00001820
Iteration 90/1000 | Loss: 0.00001820
Iteration 91/1000 | Loss: 0.00001820
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001820
Iteration 96/1000 | Loss: 0.00001819
Iteration 97/1000 | Loss: 0.00001819
Iteration 98/1000 | Loss: 0.00001819
Iteration 99/1000 | Loss: 0.00001819
Iteration 100/1000 | Loss: 0.00001819
Iteration 101/1000 | Loss: 0.00001819
Iteration 102/1000 | Loss: 0.00001819
Iteration 103/1000 | Loss: 0.00001819
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001818
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001818
Iteration 114/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.8184040527557954e-05, 1.8184040527557954e-05, 1.8184040527557954e-05, 1.8184040527557954e-05, 1.8184040527557954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8184040527557954e-05

Optimization complete. Final v2v error: 3.6391024589538574 mm

Highest mean error: 3.9469330310821533 mm for frame 110

Lowest mean error: 3.4985578060150146 mm for frame 14

Saving results

Total time: 40.624345779418945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405602
Iteration 2/25 | Loss: 0.00140686
Iteration 3/25 | Loss: 0.00128939
Iteration 4/25 | Loss: 0.00128093
Iteration 5/25 | Loss: 0.00128046
Iteration 6/25 | Loss: 0.00128046
Iteration 7/25 | Loss: 0.00128046
Iteration 8/25 | Loss: 0.00128046
Iteration 9/25 | Loss: 0.00128046
Iteration 10/25 | Loss: 0.00128046
Iteration 11/25 | Loss: 0.00128046
Iteration 12/25 | Loss: 0.00128046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012804550351575017, 0.0012804550351575017, 0.0012804550351575017, 0.0012804550351575017, 0.0012804550351575017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012804550351575017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66206121
Iteration 2/25 | Loss: 0.00074253
Iteration 3/25 | Loss: 0.00074253
Iteration 4/25 | Loss: 0.00074253
Iteration 5/25 | Loss: 0.00074253
Iteration 6/25 | Loss: 0.00074253
Iteration 7/25 | Loss: 0.00074253
Iteration 8/25 | Loss: 0.00074253
Iteration 9/25 | Loss: 0.00074253
Iteration 10/25 | Loss: 0.00074253
Iteration 11/25 | Loss: 0.00074253
Iteration 12/25 | Loss: 0.00074253
Iteration 13/25 | Loss: 0.00074253
Iteration 14/25 | Loss: 0.00074253
Iteration 15/25 | Loss: 0.00074253
Iteration 16/25 | Loss: 0.00074253
Iteration 17/25 | Loss: 0.00074253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007425274816341698, 0.0007425274816341698, 0.0007425274816341698, 0.0007425274816341698, 0.0007425274816341698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007425274816341698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074253
Iteration 2/1000 | Loss: 0.00003282
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001630
Iteration 6/1000 | Loss: 0.00001541
Iteration 7/1000 | Loss: 0.00001496
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001419
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001348
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001340
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001333
Iteration 25/1000 | Loss: 0.00001330
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001322
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001321
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001320
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001309
Iteration 46/1000 | Loss: 0.00001309
Iteration 47/1000 | Loss: 0.00001309
Iteration 48/1000 | Loss: 0.00001309
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001305
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001295
Iteration 59/1000 | Loss: 0.00001295
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001291
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001279
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001277
Iteration 92/1000 | Loss: 0.00001277
Iteration 93/1000 | Loss: 0.00001277
Iteration 94/1000 | Loss: 0.00001276
Iteration 95/1000 | Loss: 0.00001276
Iteration 96/1000 | Loss: 0.00001276
Iteration 97/1000 | Loss: 0.00001276
Iteration 98/1000 | Loss: 0.00001276
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001273
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001271
Iteration 127/1000 | Loss: 0.00001271
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001270
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001269
Iteration 132/1000 | Loss: 0.00001269
Iteration 133/1000 | Loss: 0.00001269
Iteration 134/1000 | Loss: 0.00001269
Iteration 135/1000 | Loss: 0.00001269
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001269
Iteration 138/1000 | Loss: 0.00001268
Iteration 139/1000 | Loss: 0.00001268
Iteration 140/1000 | Loss: 0.00001268
Iteration 141/1000 | Loss: 0.00001268
Iteration 142/1000 | Loss: 0.00001268
Iteration 143/1000 | Loss: 0.00001267
Iteration 144/1000 | Loss: 0.00001267
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001267
Iteration 147/1000 | Loss: 0.00001267
Iteration 148/1000 | Loss: 0.00001267
Iteration 149/1000 | Loss: 0.00001267
Iteration 150/1000 | Loss: 0.00001267
Iteration 151/1000 | Loss: 0.00001267
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001266
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Iteration 157/1000 | Loss: 0.00001266
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001266
Iteration 163/1000 | Loss: 0.00001266
Iteration 164/1000 | Loss: 0.00001266
Iteration 165/1000 | Loss: 0.00001265
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Iteration 169/1000 | Loss: 0.00001265
Iteration 170/1000 | Loss: 0.00001264
Iteration 171/1000 | Loss: 0.00001264
Iteration 172/1000 | Loss: 0.00001264
Iteration 173/1000 | Loss: 0.00001264
Iteration 174/1000 | Loss: 0.00001264
Iteration 175/1000 | Loss: 0.00001264
Iteration 176/1000 | Loss: 0.00001264
Iteration 177/1000 | Loss: 0.00001264
Iteration 178/1000 | Loss: 0.00001263
Iteration 179/1000 | Loss: 0.00001263
Iteration 180/1000 | Loss: 0.00001263
Iteration 181/1000 | Loss: 0.00001263
Iteration 182/1000 | Loss: 0.00001263
Iteration 183/1000 | Loss: 0.00001263
Iteration 184/1000 | Loss: 0.00001263
Iteration 185/1000 | Loss: 0.00001263
Iteration 186/1000 | Loss: 0.00001263
Iteration 187/1000 | Loss: 0.00001263
Iteration 188/1000 | Loss: 0.00001263
Iteration 189/1000 | Loss: 0.00001263
Iteration 190/1000 | Loss: 0.00001263
Iteration 191/1000 | Loss: 0.00001263
Iteration 192/1000 | Loss: 0.00001263
Iteration 193/1000 | Loss: 0.00001263
Iteration 194/1000 | Loss: 0.00001262
Iteration 195/1000 | Loss: 0.00001262
Iteration 196/1000 | Loss: 0.00001262
Iteration 197/1000 | Loss: 0.00001262
Iteration 198/1000 | Loss: 0.00001261
Iteration 199/1000 | Loss: 0.00001261
Iteration 200/1000 | Loss: 0.00001261
Iteration 201/1000 | Loss: 0.00001261
Iteration 202/1000 | Loss: 0.00001260
Iteration 203/1000 | Loss: 0.00001260
Iteration 204/1000 | Loss: 0.00001260
Iteration 205/1000 | Loss: 0.00001260
Iteration 206/1000 | Loss: 0.00001260
Iteration 207/1000 | Loss: 0.00001260
Iteration 208/1000 | Loss: 0.00001260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.26026288853609e-05, 1.26026288853609e-05, 1.26026288853609e-05, 1.26026288853609e-05, 1.26026288853609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.26026288853609e-05

Optimization complete. Final v2v error: 3.0363030433654785 mm

Highest mean error: 3.29144549369812 mm for frame 105

Lowest mean error: 2.8919661045074463 mm for frame 195

Saving results

Total time: 48.54568409919739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548070
Iteration 2/25 | Loss: 0.00154633
Iteration 3/25 | Loss: 0.00136072
Iteration 4/25 | Loss: 0.00130519
Iteration 5/25 | Loss: 0.00129934
Iteration 6/25 | Loss: 0.00129851
Iteration 7/25 | Loss: 0.00129820
Iteration 8/25 | Loss: 0.00129808
Iteration 9/25 | Loss: 0.00129802
Iteration 10/25 | Loss: 0.00129802
Iteration 11/25 | Loss: 0.00129801
Iteration 12/25 | Loss: 0.00129801
Iteration 13/25 | Loss: 0.00129801
Iteration 14/25 | Loss: 0.00129800
Iteration 15/25 | Loss: 0.00129800
Iteration 16/25 | Loss: 0.00129800
Iteration 17/25 | Loss: 0.00129800
Iteration 18/25 | Loss: 0.00129800
Iteration 19/25 | Loss: 0.00129800
Iteration 20/25 | Loss: 0.00129800
Iteration 21/25 | Loss: 0.00129800
Iteration 22/25 | Loss: 0.00129799
Iteration 23/25 | Loss: 0.00129799
Iteration 24/25 | Loss: 0.00129799
Iteration 25/25 | Loss: 0.00129799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.28584671
Iteration 2/25 | Loss: 0.00100546
Iteration 3/25 | Loss: 0.00100545
Iteration 4/25 | Loss: 0.00100545
Iteration 5/25 | Loss: 0.00100545
Iteration 6/25 | Loss: 0.00100544
Iteration 7/25 | Loss: 0.00100544
Iteration 8/25 | Loss: 0.00100544
Iteration 9/25 | Loss: 0.00100544
Iteration 10/25 | Loss: 0.00100544
Iteration 11/25 | Loss: 0.00100544
Iteration 12/25 | Loss: 0.00100544
Iteration 13/25 | Loss: 0.00100544
Iteration 14/25 | Loss: 0.00100544
Iteration 15/25 | Loss: 0.00100544
Iteration 16/25 | Loss: 0.00100544
Iteration 17/25 | Loss: 0.00100544
Iteration 18/25 | Loss: 0.00100544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010054431622847915, 0.0010054431622847915, 0.0010054431622847915, 0.0010054431622847915, 0.0010054431622847915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010054431622847915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100544
Iteration 2/1000 | Loss: 0.00002738
Iteration 3/1000 | Loss: 0.00001891
Iteration 4/1000 | Loss: 0.00001733
Iteration 5/1000 | Loss: 0.00002474
Iteration 6/1000 | Loss: 0.00001832
Iteration 7/1000 | Loss: 0.00003129
Iteration 8/1000 | Loss: 0.00002423
Iteration 9/1000 | Loss: 0.00001703
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00002320
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001443
Iteration 19/1000 | Loss: 0.00002323
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001427
Iteration 23/1000 | Loss: 0.00001427
Iteration 24/1000 | Loss: 0.00001427
Iteration 25/1000 | Loss: 0.00001426
Iteration 26/1000 | Loss: 0.00001426
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001424
Iteration 31/1000 | Loss: 0.00001422
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001421
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001420
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001419
Iteration 38/1000 | Loss: 0.00001419
Iteration 39/1000 | Loss: 0.00001419
Iteration 40/1000 | Loss: 0.00001418
Iteration 41/1000 | Loss: 0.00001418
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001416
Iteration 48/1000 | Loss: 0.00001913
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001415
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001414
Iteration 56/1000 | Loss: 0.00001414
Iteration 57/1000 | Loss: 0.00001414
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001414
Iteration 60/1000 | Loss: 0.00001414
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001413
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001413
Iteration 78/1000 | Loss: 0.00001413
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001407
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001403
Iteration 115/1000 | Loss: 0.00001403
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001402
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001401
Iteration 134/1000 | Loss: 0.00001401
Iteration 135/1000 | Loss: 0.00001401
Iteration 136/1000 | Loss: 0.00001401
Iteration 137/1000 | Loss: 0.00001401
Iteration 138/1000 | Loss: 0.00001401
Iteration 139/1000 | Loss: 0.00001401
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001401
Iteration 142/1000 | Loss: 0.00001401
Iteration 143/1000 | Loss: 0.00001401
Iteration 144/1000 | Loss: 0.00001401
Iteration 145/1000 | Loss: 0.00001400
Iteration 146/1000 | Loss: 0.00001400
Iteration 147/1000 | Loss: 0.00001400
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001399
Iteration 159/1000 | Loss: 0.00001399
Iteration 160/1000 | Loss: 0.00001399
Iteration 161/1000 | Loss: 0.00001399
Iteration 162/1000 | Loss: 0.00001399
Iteration 163/1000 | Loss: 0.00001399
Iteration 164/1000 | Loss: 0.00001399
Iteration 165/1000 | Loss: 0.00001399
Iteration 166/1000 | Loss: 0.00001399
Iteration 167/1000 | Loss: 0.00001399
Iteration 168/1000 | Loss: 0.00001399
Iteration 169/1000 | Loss: 0.00001398
Iteration 170/1000 | Loss: 0.00001398
Iteration 171/1000 | Loss: 0.00001398
Iteration 172/1000 | Loss: 0.00001398
Iteration 173/1000 | Loss: 0.00001398
Iteration 174/1000 | Loss: 0.00001398
Iteration 175/1000 | Loss: 0.00001398
Iteration 176/1000 | Loss: 0.00001398
Iteration 177/1000 | Loss: 0.00001398
Iteration 178/1000 | Loss: 0.00001398
Iteration 179/1000 | Loss: 0.00001398
Iteration 180/1000 | Loss: 0.00001398
Iteration 181/1000 | Loss: 0.00001398
Iteration 182/1000 | Loss: 0.00001398
Iteration 183/1000 | Loss: 0.00001397
Iteration 184/1000 | Loss: 0.00001397
Iteration 185/1000 | Loss: 0.00001397
Iteration 186/1000 | Loss: 0.00001397
Iteration 187/1000 | Loss: 0.00001397
Iteration 188/1000 | Loss: 0.00001397
Iteration 189/1000 | Loss: 0.00001397
Iteration 190/1000 | Loss: 0.00001397
Iteration 191/1000 | Loss: 0.00001397
Iteration 192/1000 | Loss: 0.00001397
Iteration 193/1000 | Loss: 0.00001397
Iteration 194/1000 | Loss: 0.00001397
Iteration 195/1000 | Loss: 0.00001397
Iteration 196/1000 | Loss: 0.00001397
Iteration 197/1000 | Loss: 0.00001397
Iteration 198/1000 | Loss: 0.00001396
Iteration 199/1000 | Loss: 0.00001396
Iteration 200/1000 | Loss: 0.00001396
Iteration 201/1000 | Loss: 0.00001396
Iteration 202/1000 | Loss: 0.00001396
Iteration 203/1000 | Loss: 0.00001396
Iteration 204/1000 | Loss: 0.00001396
Iteration 205/1000 | Loss: 0.00001396
Iteration 206/1000 | Loss: 0.00001396
Iteration 207/1000 | Loss: 0.00001396
Iteration 208/1000 | Loss: 0.00001396
Iteration 209/1000 | Loss: 0.00001396
Iteration 210/1000 | Loss: 0.00001396
Iteration 211/1000 | Loss: 0.00001395
Iteration 212/1000 | Loss: 0.00001395
Iteration 213/1000 | Loss: 0.00001395
Iteration 214/1000 | Loss: 0.00001395
Iteration 215/1000 | Loss: 0.00001395
Iteration 216/1000 | Loss: 0.00001395
Iteration 217/1000 | Loss: 0.00001395
Iteration 218/1000 | Loss: 0.00001395
Iteration 219/1000 | Loss: 0.00001395
Iteration 220/1000 | Loss: 0.00001395
Iteration 221/1000 | Loss: 0.00001395
Iteration 222/1000 | Loss: 0.00001395
Iteration 223/1000 | Loss: 0.00001395
Iteration 224/1000 | Loss: 0.00001395
Iteration 225/1000 | Loss: 0.00001395
Iteration 226/1000 | Loss: 0.00001395
Iteration 227/1000 | Loss: 0.00001395
Iteration 228/1000 | Loss: 0.00001395
Iteration 229/1000 | Loss: 0.00001395
Iteration 230/1000 | Loss: 0.00001394
Iteration 231/1000 | Loss: 0.00001394
Iteration 232/1000 | Loss: 0.00001394
Iteration 233/1000 | Loss: 0.00001394
Iteration 234/1000 | Loss: 0.00001394
Iteration 235/1000 | Loss: 0.00001394
Iteration 236/1000 | Loss: 0.00001394
Iteration 237/1000 | Loss: 0.00001394
Iteration 238/1000 | Loss: 0.00001394
Iteration 239/1000 | Loss: 0.00001394
Iteration 240/1000 | Loss: 0.00001394
Iteration 241/1000 | Loss: 0.00001394
Iteration 242/1000 | Loss: 0.00001394
Iteration 243/1000 | Loss: 0.00001393
Iteration 244/1000 | Loss: 0.00001393
Iteration 245/1000 | Loss: 0.00001393
Iteration 246/1000 | Loss: 0.00001393
Iteration 247/1000 | Loss: 0.00001393
Iteration 248/1000 | Loss: 0.00001393
Iteration 249/1000 | Loss: 0.00001393
Iteration 250/1000 | Loss: 0.00001393
Iteration 251/1000 | Loss: 0.00001393
Iteration 252/1000 | Loss: 0.00001393
Iteration 253/1000 | Loss: 0.00001393
Iteration 254/1000 | Loss: 0.00001393
Iteration 255/1000 | Loss: 0.00001393
Iteration 256/1000 | Loss: 0.00001393
Iteration 257/1000 | Loss: 0.00001393
Iteration 258/1000 | Loss: 0.00001393
Iteration 259/1000 | Loss: 0.00001393
Iteration 260/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.3934330127085559e-05, 1.3934330127085559e-05, 1.3934330127085559e-05, 1.3934330127085559e-05, 1.3934330127085559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3934330127085559e-05

Optimization complete. Final v2v error: 3.1410372257232666 mm

Highest mean error: 3.8184123039245605 mm for frame 166

Lowest mean error: 2.817016124725342 mm for frame 91

Saving results

Total time: 61.60529088973999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992160
Iteration 2/25 | Loss: 0.00212428
Iteration 3/25 | Loss: 0.00172186
Iteration 4/25 | Loss: 0.00169661
Iteration 5/25 | Loss: 0.00170024
Iteration 6/25 | Loss: 0.00167325
Iteration 7/25 | Loss: 0.00160277
Iteration 8/25 | Loss: 0.00155685
Iteration 9/25 | Loss: 0.00154563
Iteration 10/25 | Loss: 0.00153759
Iteration 11/25 | Loss: 0.00151868
Iteration 12/25 | Loss: 0.00149701
Iteration 13/25 | Loss: 0.00147370
Iteration 14/25 | Loss: 0.00145981
Iteration 15/25 | Loss: 0.00145331
Iteration 16/25 | Loss: 0.00144249
Iteration 17/25 | Loss: 0.00144446
Iteration 18/25 | Loss: 0.00143359
Iteration 19/25 | Loss: 0.00142645
Iteration 20/25 | Loss: 0.00142296
Iteration 21/25 | Loss: 0.00142949
Iteration 22/25 | Loss: 0.00141847
Iteration 23/25 | Loss: 0.00141688
Iteration 24/25 | Loss: 0.00141977
Iteration 25/25 | Loss: 0.00141632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29154813
Iteration 2/25 | Loss: 0.00151556
Iteration 3/25 | Loss: 0.00151554
Iteration 4/25 | Loss: 0.00151554
Iteration 5/25 | Loss: 0.00151554
Iteration 6/25 | Loss: 0.00151554
Iteration 7/25 | Loss: 0.00151554
Iteration 8/25 | Loss: 0.00151554
Iteration 9/25 | Loss: 0.00151554
Iteration 10/25 | Loss: 0.00151554
Iteration 11/25 | Loss: 0.00151554
Iteration 12/25 | Loss: 0.00151554
Iteration 13/25 | Loss: 0.00151554
Iteration 14/25 | Loss: 0.00151554
Iteration 15/25 | Loss: 0.00151554
Iteration 16/25 | Loss: 0.00151554
Iteration 17/25 | Loss: 0.00151554
Iteration 18/25 | Loss: 0.00151554
Iteration 19/25 | Loss: 0.00151554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0015155365690588951, 0.0015155365690588951, 0.0015155365690588951, 0.0015155365690588951, 0.0015155365690588951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015155365690588951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151554
Iteration 2/1000 | Loss: 0.00053208
Iteration 3/1000 | Loss: 0.00030474
Iteration 4/1000 | Loss: 0.00021900
Iteration 5/1000 | Loss: 0.00025746
Iteration 6/1000 | Loss: 0.00049117
Iteration 7/1000 | Loss: 0.00041993
Iteration 8/1000 | Loss: 0.00031775
Iteration 9/1000 | Loss: 0.00049226
Iteration 10/1000 | Loss: 0.00055340
Iteration 11/1000 | Loss: 0.00035579
Iteration 12/1000 | Loss: 0.00048547
Iteration 13/1000 | Loss: 0.00035338
Iteration 14/1000 | Loss: 0.00017705
Iteration 15/1000 | Loss: 0.00020236
Iteration 16/1000 | Loss: 0.00022163
Iteration 17/1000 | Loss: 0.00029907
Iteration 18/1000 | Loss: 0.00026127
Iteration 19/1000 | Loss: 0.00027057
Iteration 20/1000 | Loss: 0.00019799
Iteration 21/1000 | Loss: 0.00029198
Iteration 22/1000 | Loss: 0.00026940
Iteration 23/1000 | Loss: 0.00027804
Iteration 24/1000 | Loss: 0.00020899
Iteration 25/1000 | Loss: 0.00015834
Iteration 26/1000 | Loss: 0.00015541
Iteration 27/1000 | Loss: 0.00025887
Iteration 28/1000 | Loss: 0.00027665
Iteration 29/1000 | Loss: 0.00015032
Iteration 30/1000 | Loss: 0.00030496
Iteration 31/1000 | Loss: 0.00033481
Iteration 32/1000 | Loss: 0.00033430
Iteration 33/1000 | Loss: 0.00041972
Iteration 34/1000 | Loss: 0.00038480
Iteration 35/1000 | Loss: 0.00031388
Iteration 36/1000 | Loss: 0.00028510
Iteration 37/1000 | Loss: 0.00040397
Iteration 38/1000 | Loss: 0.00020138
Iteration 39/1000 | Loss: 0.00017757
Iteration 40/1000 | Loss: 0.00039675
Iteration 41/1000 | Loss: 0.00020809
Iteration 42/1000 | Loss: 0.00017735
Iteration 43/1000 | Loss: 0.00026748
Iteration 44/1000 | Loss: 0.00024989
Iteration 45/1000 | Loss: 0.00026373
Iteration 46/1000 | Loss: 0.00018999
Iteration 47/1000 | Loss: 0.00023364
Iteration 48/1000 | Loss: 0.00054752
Iteration 49/1000 | Loss: 0.00027706
Iteration 50/1000 | Loss: 0.00021077
Iteration 51/1000 | Loss: 0.00025834
Iteration 52/1000 | Loss: 0.00021481
Iteration 53/1000 | Loss: 0.00014613
Iteration 54/1000 | Loss: 0.00023604
Iteration 55/1000 | Loss: 0.00027405
Iteration 56/1000 | Loss: 0.00014919
Iteration 57/1000 | Loss: 0.00020706
Iteration 58/1000 | Loss: 0.00022608
Iteration 59/1000 | Loss: 0.00027031
Iteration 60/1000 | Loss: 0.00034021
Iteration 61/1000 | Loss: 0.00021963
Iteration 62/1000 | Loss: 0.00021255
Iteration 63/1000 | Loss: 0.00016933
Iteration 64/1000 | Loss: 0.00017515
Iteration 65/1000 | Loss: 0.00019352
Iteration 66/1000 | Loss: 0.00018651
Iteration 67/1000 | Loss: 0.00020178
Iteration 68/1000 | Loss: 0.00025036
Iteration 69/1000 | Loss: 0.00024858
Iteration 70/1000 | Loss: 0.00016436
Iteration 71/1000 | Loss: 0.00017967
Iteration 72/1000 | Loss: 0.00017209
Iteration 73/1000 | Loss: 0.00018658
Iteration 74/1000 | Loss: 0.00020607
Iteration 75/1000 | Loss: 0.00022220
Iteration 76/1000 | Loss: 0.00027424
Iteration 77/1000 | Loss: 0.00020009
Iteration 78/1000 | Loss: 0.00016385
Iteration 79/1000 | Loss: 0.00033484
Iteration 80/1000 | Loss: 0.00019633
Iteration 81/1000 | Loss: 0.00022678
Iteration 82/1000 | Loss: 0.00021714
Iteration 83/1000 | Loss: 0.00015709
Iteration 84/1000 | Loss: 0.00014434
Iteration 85/1000 | Loss: 0.00013810
Iteration 86/1000 | Loss: 0.00015873
Iteration 87/1000 | Loss: 0.00015121
Iteration 88/1000 | Loss: 0.00021692
Iteration 89/1000 | Loss: 0.00018363
Iteration 90/1000 | Loss: 0.00012256
Iteration 91/1000 | Loss: 0.00015606
Iteration 92/1000 | Loss: 0.00011299
Iteration 93/1000 | Loss: 0.00023461
Iteration 94/1000 | Loss: 0.00018215
Iteration 95/1000 | Loss: 0.00019802
Iteration 96/1000 | Loss: 0.00020916
Iteration 97/1000 | Loss: 0.00012797
Iteration 98/1000 | Loss: 0.00012734
Iteration 99/1000 | Loss: 0.00014848
Iteration 100/1000 | Loss: 0.00015590
Iteration 101/1000 | Loss: 0.00014934
Iteration 102/1000 | Loss: 0.00013824
Iteration 103/1000 | Loss: 0.00012316
Iteration 104/1000 | Loss: 0.00014912
Iteration 105/1000 | Loss: 0.00013372
Iteration 106/1000 | Loss: 0.00011906
Iteration 107/1000 | Loss: 0.00012246
Iteration 108/1000 | Loss: 0.00014896
Iteration 109/1000 | Loss: 0.00015595
Iteration 110/1000 | Loss: 0.00013026
Iteration 111/1000 | Loss: 0.00014991
Iteration 112/1000 | Loss: 0.00014698
Iteration 113/1000 | Loss: 0.00013041
Iteration 114/1000 | Loss: 0.00009259
Iteration 115/1000 | Loss: 0.00014178
Iteration 116/1000 | Loss: 0.00015324
Iteration 117/1000 | Loss: 0.00014756
Iteration 118/1000 | Loss: 0.00014139
Iteration 119/1000 | Loss: 0.00014630
Iteration 120/1000 | Loss: 0.00014578
Iteration 121/1000 | Loss: 0.00014211
Iteration 122/1000 | Loss: 0.00014892
Iteration 123/1000 | Loss: 0.00015183
Iteration 124/1000 | Loss: 0.00015332
Iteration 125/1000 | Loss: 0.00014928
Iteration 126/1000 | Loss: 0.00016458
Iteration 127/1000 | Loss: 0.00014565
Iteration 128/1000 | Loss: 0.00014667
Iteration 129/1000 | Loss: 0.00015526
Iteration 130/1000 | Loss: 0.00015061
Iteration 131/1000 | Loss: 0.00017595
Iteration 132/1000 | Loss: 0.00014212
Iteration 133/1000 | Loss: 0.00015343
Iteration 134/1000 | Loss: 0.00014007
Iteration 135/1000 | Loss: 0.00017883
Iteration 136/1000 | Loss: 0.00016378
Iteration 137/1000 | Loss: 0.00016805
Iteration 138/1000 | Loss: 0.00014944
Iteration 139/1000 | Loss: 0.00030623
Iteration 140/1000 | Loss: 0.00015442
Iteration 141/1000 | Loss: 0.00024372
Iteration 142/1000 | Loss: 0.00020106
Iteration 143/1000 | Loss: 0.00020388
Iteration 144/1000 | Loss: 0.00085318
Iteration 145/1000 | Loss: 0.00036484
Iteration 146/1000 | Loss: 0.00029184
Iteration 147/1000 | Loss: 0.00014519
Iteration 148/1000 | Loss: 0.00011251
Iteration 149/1000 | Loss: 0.00016766
Iteration 150/1000 | Loss: 0.00017505
Iteration 151/1000 | Loss: 0.00019306
Iteration 152/1000 | Loss: 0.00015563
Iteration 153/1000 | Loss: 0.00023985
Iteration 154/1000 | Loss: 0.00040629
Iteration 155/1000 | Loss: 0.00025736
Iteration 156/1000 | Loss: 0.00027253
Iteration 157/1000 | Loss: 0.00057292
Iteration 158/1000 | Loss: 0.00050076
Iteration 159/1000 | Loss: 0.00028563
Iteration 160/1000 | Loss: 0.00016020
Iteration 161/1000 | Loss: 0.00015134
Iteration 162/1000 | Loss: 0.00015171
Iteration 163/1000 | Loss: 0.00015560
Iteration 164/1000 | Loss: 0.00032205
Iteration 165/1000 | Loss: 0.00028784
Iteration 166/1000 | Loss: 0.00016367
Iteration 167/1000 | Loss: 0.00014601
Iteration 168/1000 | Loss: 0.00016515
Iteration 169/1000 | Loss: 0.00014252
Iteration 170/1000 | Loss: 0.00015296
Iteration 171/1000 | Loss: 0.00014133
Iteration 172/1000 | Loss: 0.00015585
Iteration 173/1000 | Loss: 0.00014585
Iteration 174/1000 | Loss: 0.00014151
Iteration 175/1000 | Loss: 0.00015505
Iteration 176/1000 | Loss: 0.00014856
Iteration 177/1000 | Loss: 0.00015513
Iteration 178/1000 | Loss: 0.00014258
Iteration 179/1000 | Loss: 0.00015262
Iteration 180/1000 | Loss: 0.00010500
Iteration 181/1000 | Loss: 0.00012389
Iteration 182/1000 | Loss: 0.00009856
Iteration 183/1000 | Loss: 0.00011835
Iteration 184/1000 | Loss: 0.00015121
Iteration 185/1000 | Loss: 0.00013429
Iteration 186/1000 | Loss: 0.00012172
Iteration 187/1000 | Loss: 0.00010803
Iteration 188/1000 | Loss: 0.00010101
Iteration 189/1000 | Loss: 0.00013471
Iteration 190/1000 | Loss: 0.00014309
Iteration 191/1000 | Loss: 0.00014282
Iteration 192/1000 | Loss: 0.00014412
Iteration 193/1000 | Loss: 0.00014616
Iteration 194/1000 | Loss: 0.00014914
Iteration 195/1000 | Loss: 0.00014268
Iteration 196/1000 | Loss: 0.00015646
Iteration 197/1000 | Loss: 0.00013259
Iteration 198/1000 | Loss: 0.00014340
Iteration 199/1000 | Loss: 0.00014135
Iteration 200/1000 | Loss: 0.00014133
Iteration 201/1000 | Loss: 0.00014197
Iteration 202/1000 | Loss: 0.00014937
Iteration 203/1000 | Loss: 0.00013062
Iteration 204/1000 | Loss: 0.00013204
Iteration 205/1000 | Loss: 0.00013247
Iteration 206/1000 | Loss: 0.00012603
Iteration 207/1000 | Loss: 0.00012131
Iteration 208/1000 | Loss: 0.00013766
Iteration 209/1000 | Loss: 0.00014294
Iteration 210/1000 | Loss: 0.00015269
Iteration 211/1000 | Loss: 0.00013003
Iteration 212/1000 | Loss: 0.00014665
Iteration 213/1000 | Loss: 0.00014253
Iteration 214/1000 | Loss: 0.00014330
Iteration 215/1000 | Loss: 0.00014427
Iteration 216/1000 | Loss: 0.00014160
Iteration 217/1000 | Loss: 0.00014194
Iteration 218/1000 | Loss: 0.00014028
Iteration 219/1000 | Loss: 0.00014773
Iteration 220/1000 | Loss: 0.00010842
Iteration 221/1000 | Loss: 0.00014486
Iteration 222/1000 | Loss: 0.00014521
Iteration 223/1000 | Loss: 0.00010789
Iteration 224/1000 | Loss: 0.00008044
Iteration 225/1000 | Loss: 0.00010462
Iteration 226/1000 | Loss: 0.00015729
Iteration 227/1000 | Loss: 0.00013204
Iteration 228/1000 | Loss: 0.00015677
Iteration 229/1000 | Loss: 0.00013449
Iteration 230/1000 | Loss: 0.00014378
Iteration 231/1000 | Loss: 0.00013595
Iteration 232/1000 | Loss: 0.00014921
Iteration 233/1000 | Loss: 0.00013569
Iteration 234/1000 | Loss: 0.00014003
Iteration 235/1000 | Loss: 0.00014212
Iteration 236/1000 | Loss: 0.00013651
Iteration 237/1000 | Loss: 0.00010308
Iteration 238/1000 | Loss: 0.00010230
Iteration 239/1000 | Loss: 0.00010786
Iteration 240/1000 | Loss: 0.00012044
Iteration 241/1000 | Loss: 0.00013745
Iteration 242/1000 | Loss: 0.00014008
Iteration 243/1000 | Loss: 0.00013916
Iteration 244/1000 | Loss: 0.00013658
Iteration 245/1000 | Loss: 0.00014325
Iteration 246/1000 | Loss: 0.00014192
Iteration 247/1000 | Loss: 0.00013952
Iteration 248/1000 | Loss: 0.00013856
Iteration 249/1000 | Loss: 0.00014084
Iteration 250/1000 | Loss: 0.00014237
Iteration 251/1000 | Loss: 0.00013636
Iteration 252/1000 | Loss: 0.00013914
Iteration 253/1000 | Loss: 0.00011590
Iteration 254/1000 | Loss: 0.00008871
Iteration 255/1000 | Loss: 0.00008276
Iteration 256/1000 | Loss: 0.00009314
Iteration 257/1000 | Loss: 0.00010998
Iteration 258/1000 | Loss: 0.00013333
Iteration 259/1000 | Loss: 0.00011994
Iteration 260/1000 | Loss: 0.00012354
Iteration 261/1000 | Loss: 0.00013083
Iteration 262/1000 | Loss: 0.00014254
Iteration 263/1000 | Loss: 0.00013422
Iteration 264/1000 | Loss: 0.00014723
Iteration 265/1000 | Loss: 0.00013504
Iteration 266/1000 | Loss: 0.00010899
Iteration 267/1000 | Loss: 0.00011560
Iteration 268/1000 | Loss: 0.00014537
Iteration 269/1000 | Loss: 0.00014222
Iteration 270/1000 | Loss: 0.00013561
Iteration 271/1000 | Loss: 0.00014189
Iteration 272/1000 | Loss: 0.00014388
Iteration 273/1000 | Loss: 0.00010848
Iteration 274/1000 | Loss: 0.00016952
Iteration 275/1000 | Loss: 0.00014805
Iteration 276/1000 | Loss: 0.00013485
Iteration 277/1000 | Loss: 0.00014011
Iteration 278/1000 | Loss: 0.00013636
Iteration 279/1000 | Loss: 0.00012885
Iteration 280/1000 | Loss: 0.00014291
Iteration 281/1000 | Loss: 0.00014084
Iteration 282/1000 | Loss: 0.00015176
Iteration 283/1000 | Loss: 0.00013375
Iteration 284/1000 | Loss: 0.00009709
Iteration 285/1000 | Loss: 0.00009237
Iteration 286/1000 | Loss: 0.00013137
Iteration 287/1000 | Loss: 0.00013421
Iteration 288/1000 | Loss: 0.00013766
Iteration 289/1000 | Loss: 0.00013893
Iteration 290/1000 | Loss: 0.00014069
Iteration 291/1000 | Loss: 0.00012972
Iteration 292/1000 | Loss: 0.00013631
Iteration 293/1000 | Loss: 0.00014211
Iteration 294/1000 | Loss: 0.00013833
Iteration 295/1000 | Loss: 0.00008881
Iteration 296/1000 | Loss: 0.00010592
Iteration 297/1000 | Loss: 0.00007854
Iteration 298/1000 | Loss: 0.00011057
Iteration 299/1000 | Loss: 0.00010351
Iteration 300/1000 | Loss: 0.00009573
Iteration 301/1000 | Loss: 0.00011535
Iteration 302/1000 | Loss: 0.00014081
Iteration 303/1000 | Loss: 0.00014051
Iteration 304/1000 | Loss: 0.00013515
Iteration 305/1000 | Loss: 0.00013377
Iteration 306/1000 | Loss: 0.00013579
Iteration 307/1000 | Loss: 0.00013393
Iteration 308/1000 | Loss: 0.00014248
Iteration 309/1000 | Loss: 0.00014116
Iteration 310/1000 | Loss: 0.00014448
Iteration 311/1000 | Loss: 0.00013492
Iteration 312/1000 | Loss: 0.00013260
Iteration 313/1000 | Loss: 0.00012669
Iteration 314/1000 | Loss: 0.00013056
Iteration 315/1000 | Loss: 0.00013195
Iteration 316/1000 | Loss: 0.00013208
Iteration 317/1000 | Loss: 0.00011490
Iteration 318/1000 | Loss: 0.00016234
Iteration 319/1000 | Loss: 0.00009054
Iteration 320/1000 | Loss: 0.00008513
Iteration 321/1000 | Loss: 0.00008984
Iteration 322/1000 | Loss: 0.00006571
Iteration 323/1000 | Loss: 0.00006723
Iteration 324/1000 | Loss: 0.00007819
Iteration 325/1000 | Loss: 0.00008972
Iteration 326/1000 | Loss: 0.00009037
Iteration 327/1000 | Loss: 0.00007372
Iteration 328/1000 | Loss: 0.00007346
Iteration 329/1000 | Loss: 0.00006332
Iteration 330/1000 | Loss: 0.00006899
Iteration 331/1000 | Loss: 0.00007969
Iteration 332/1000 | Loss: 0.00008914
Iteration 333/1000 | Loss: 0.00008324
Iteration 334/1000 | Loss: 0.00008699
Iteration 335/1000 | Loss: 0.00007073
Iteration 336/1000 | Loss: 0.00007720
Iteration 337/1000 | Loss: 0.00049563
Iteration 338/1000 | Loss: 0.00007640
Iteration 339/1000 | Loss: 0.00007555
Iteration 340/1000 | Loss: 0.00006688
Iteration 341/1000 | Loss: 0.00005604
Iteration 342/1000 | Loss: 0.00006449
Iteration 343/1000 | Loss: 0.00006591
Iteration 344/1000 | Loss: 0.00006192
Iteration 345/1000 | Loss: 0.00007195
Iteration 346/1000 | Loss: 0.00007485
Iteration 347/1000 | Loss: 0.00007088
Iteration 348/1000 | Loss: 0.00007329
Iteration 349/1000 | Loss: 0.00006554
Iteration 350/1000 | Loss: 0.00006129
Iteration 351/1000 | Loss: 0.00006147
Iteration 352/1000 | Loss: 0.00006999
Iteration 353/1000 | Loss: 0.00005972
Iteration 354/1000 | Loss: 0.00005972
Iteration 355/1000 | Loss: 0.00006089
Iteration 356/1000 | Loss: 0.00005940
Iteration 357/1000 | Loss: 0.00006421
Iteration 358/1000 | Loss: 0.00006590
Iteration 359/1000 | Loss: 0.00006119
Iteration 360/1000 | Loss: 0.00007326
Iteration 361/1000 | Loss: 0.00006297
Iteration 362/1000 | Loss: 0.00007209
Iteration 363/1000 | Loss: 0.00006456
Iteration 364/1000 | Loss: 0.00007583
Iteration 365/1000 | Loss: 0.00006335
Iteration 366/1000 | Loss: 0.00006334
Iteration 367/1000 | Loss: 0.00007228
Iteration 368/1000 | Loss: 0.00006864
Iteration 369/1000 | Loss: 0.00006398
Iteration 370/1000 | Loss: 0.00005482
Iteration 371/1000 | Loss: 0.00006215
Iteration 372/1000 | Loss: 0.00006525
Iteration 373/1000 | Loss: 0.00007121
Iteration 374/1000 | Loss: 0.00006522
Iteration 375/1000 | Loss: 0.00007205
Iteration 376/1000 | Loss: 0.00006897
Iteration 377/1000 | Loss: 0.00006865
Iteration 378/1000 | Loss: 0.00007461
Iteration 379/1000 | Loss: 0.00006730
Iteration 380/1000 | Loss: 0.00006810
Iteration 381/1000 | Loss: 0.00006396
Iteration 382/1000 | Loss: 0.00006094
Iteration 383/1000 | Loss: 0.00007255
Iteration 384/1000 | Loss: 0.00006648
Iteration 385/1000 | Loss: 0.00007551
Iteration 386/1000 | Loss: 0.00005801
Iteration 387/1000 | Loss: 0.00005264
Iteration 388/1000 | Loss: 0.00005564
Iteration 389/1000 | Loss: 0.00005404
Iteration 390/1000 | Loss: 0.00005083
Iteration 391/1000 | Loss: 0.00006407
Iteration 392/1000 | Loss: 0.00007034
Iteration 393/1000 | Loss: 0.00005477
Iteration 394/1000 | Loss: 0.00005213
Iteration 395/1000 | Loss: 0.00006371
Iteration 396/1000 | Loss: 0.00006508
Iteration 397/1000 | Loss: 0.00006503
Iteration 398/1000 | Loss: 0.00005066
Iteration 399/1000 | Loss: 0.00004946
Iteration 400/1000 | Loss: 0.00004875
Iteration 401/1000 | Loss: 0.00006248
Iteration 402/1000 | Loss: 0.00004988
Iteration 403/1000 | Loss: 0.00005330
Iteration 404/1000 | Loss: 0.00005941
Iteration 405/1000 | Loss: 0.00004974
Iteration 406/1000 | Loss: 0.00004955
Iteration 407/1000 | Loss: 0.00004834
Iteration 408/1000 | Loss: 0.00004805
Iteration 409/1000 | Loss: 0.00004782
Iteration 410/1000 | Loss: 0.00004751
Iteration 411/1000 | Loss: 0.00004726
Iteration 412/1000 | Loss: 0.00004713
Iteration 413/1000 | Loss: 0.00004705
Iteration 414/1000 | Loss: 0.00004704
Iteration 415/1000 | Loss: 0.00004704
Iteration 416/1000 | Loss: 0.00004700
Iteration 417/1000 | Loss: 0.00004695
Iteration 418/1000 | Loss: 0.00004694
Iteration 419/1000 | Loss: 0.00004686
Iteration 420/1000 | Loss: 0.00004686
Iteration 421/1000 | Loss: 0.00004686
Iteration 422/1000 | Loss: 0.00004686
Iteration 423/1000 | Loss: 0.00004686
Iteration 424/1000 | Loss: 0.00004685
Iteration 425/1000 | Loss: 0.00004685
Iteration 426/1000 | Loss: 0.00004685
Iteration 427/1000 | Loss: 0.00004685
Iteration 428/1000 | Loss: 0.00004685
Iteration 429/1000 | Loss: 0.00004685
Iteration 430/1000 | Loss: 0.00004685
Iteration 431/1000 | Loss: 0.00004683
Iteration 432/1000 | Loss: 0.00004683
Iteration 433/1000 | Loss: 0.00004682
Iteration 434/1000 | Loss: 0.00004682
Iteration 435/1000 | Loss: 0.00004682
Iteration 436/1000 | Loss: 0.00004682
Iteration 437/1000 | Loss: 0.00004681
Iteration 438/1000 | Loss: 0.00004681
Iteration 439/1000 | Loss: 0.00004681
Iteration 440/1000 | Loss: 0.00004680
Iteration 441/1000 | Loss: 0.00004679
Iteration 442/1000 | Loss: 0.00004679
Iteration 443/1000 | Loss: 0.00004678
Iteration 444/1000 | Loss: 0.00004677
Iteration 445/1000 | Loss: 0.00004677
Iteration 446/1000 | Loss: 0.00004677
Iteration 447/1000 | Loss: 0.00004677
Iteration 448/1000 | Loss: 0.00004677
Iteration 449/1000 | Loss: 0.00004677
Iteration 450/1000 | Loss: 0.00004675
Iteration 451/1000 | Loss: 0.00004670
Iteration 452/1000 | Loss: 0.00004670
Iteration 453/1000 | Loss: 0.00004670
Iteration 454/1000 | Loss: 0.00004669
Iteration 455/1000 | Loss: 0.00004669
Iteration 456/1000 | Loss: 0.00004669
Iteration 457/1000 | Loss: 0.00004668
Iteration 458/1000 | Loss: 0.00004668
Iteration 459/1000 | Loss: 0.00004667
Iteration 460/1000 | Loss: 0.00004667
Iteration 461/1000 | Loss: 0.00004666
Iteration 462/1000 | Loss: 0.00004666
Iteration 463/1000 | Loss: 0.00004665
Iteration 464/1000 | Loss: 0.00004665
Iteration 465/1000 | Loss: 0.00004664
Iteration 466/1000 | Loss: 0.00004664
Iteration 467/1000 | Loss: 0.00004664
Iteration 468/1000 | Loss: 0.00004664
Iteration 469/1000 | Loss: 0.00004663
Iteration 470/1000 | Loss: 0.00004663
Iteration 471/1000 | Loss: 0.00004663
Iteration 472/1000 | Loss: 0.00004663
Iteration 473/1000 | Loss: 0.00004663
Iteration 474/1000 | Loss: 0.00004663
Iteration 475/1000 | Loss: 0.00004663
Iteration 476/1000 | Loss: 0.00004663
Iteration 477/1000 | Loss: 0.00004663
Iteration 478/1000 | Loss: 0.00004663
Iteration 479/1000 | Loss: 0.00004662
Iteration 480/1000 | Loss: 0.00004662
Iteration 481/1000 | Loss: 0.00004662
Iteration 482/1000 | Loss: 0.00004662
Iteration 483/1000 | Loss: 0.00004662
Iteration 484/1000 | Loss: 0.00004661
Iteration 485/1000 | Loss: 0.00004661
Iteration 486/1000 | Loss: 0.00004660
Iteration 487/1000 | Loss: 0.00004660
Iteration 488/1000 | Loss: 0.00004660
Iteration 489/1000 | Loss: 0.00004660
Iteration 490/1000 | Loss: 0.00004660
Iteration 491/1000 | Loss: 0.00004660
Iteration 492/1000 | Loss: 0.00004660
Iteration 493/1000 | Loss: 0.00004660
Iteration 494/1000 | Loss: 0.00004660
Iteration 495/1000 | Loss: 0.00004660
Iteration 496/1000 | Loss: 0.00004660
Iteration 497/1000 | Loss: 0.00004660
Iteration 498/1000 | Loss: 0.00004660
Iteration 499/1000 | Loss: 0.00004660
Iteration 500/1000 | Loss: 0.00004660
Iteration 501/1000 | Loss: 0.00004659
Iteration 502/1000 | Loss: 0.00004658
Iteration 503/1000 | Loss: 0.00004658
Iteration 504/1000 | Loss: 0.00004657
Iteration 505/1000 | Loss: 0.00004657
Iteration 506/1000 | Loss: 0.00004657
Iteration 507/1000 | Loss: 0.00004656
Iteration 508/1000 | Loss: 0.00004656
Iteration 509/1000 | Loss: 0.00004655
Iteration 510/1000 | Loss: 0.00004655
Iteration 511/1000 | Loss: 0.00004655
Iteration 512/1000 | Loss: 0.00004655
Iteration 513/1000 | Loss: 0.00004654
Iteration 514/1000 | Loss: 0.00004654
Iteration 515/1000 | Loss: 0.00004654
Iteration 516/1000 | Loss: 0.00004653
Iteration 517/1000 | Loss: 0.00004653
Iteration 518/1000 | Loss: 0.00004653
Iteration 519/1000 | Loss: 0.00004653
Iteration 520/1000 | Loss: 0.00004653
Iteration 521/1000 | Loss: 0.00004653
Iteration 522/1000 | Loss: 0.00004653
Iteration 523/1000 | Loss: 0.00004653
Iteration 524/1000 | Loss: 0.00004652
Iteration 525/1000 | Loss: 0.00004652
Iteration 526/1000 | Loss: 0.00004652
Iteration 527/1000 | Loss: 0.00004652
Iteration 528/1000 | Loss: 0.00004652
Iteration 529/1000 | Loss: 0.00004652
Iteration 530/1000 | Loss: 0.00004652
Iteration 531/1000 | Loss: 0.00004652
Iteration 532/1000 | Loss: 0.00004652
Iteration 533/1000 | Loss: 0.00004652
Iteration 534/1000 | Loss: 0.00004652
Iteration 535/1000 | Loss: 0.00004652
Iteration 536/1000 | Loss: 0.00004652
Iteration 537/1000 | Loss: 0.00004652
Iteration 538/1000 | Loss: 0.00004652
Iteration 539/1000 | Loss: 0.00004652
Iteration 540/1000 | Loss: 0.00004652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 540. Stopping optimization.
Last 5 losses: [4.6522160118911415e-05, 4.6522160118911415e-05, 4.6522160118911415e-05, 4.6522160118911415e-05, 4.6522160118911415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6522160118911415e-05

Optimization complete. Final v2v error: 4.354835510253906 mm

Highest mean error: 11.135701179504395 mm for frame 74

Lowest mean error: 3.6284589767456055 mm for frame 217

Saving results

Total time: 733.057944059372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610114
Iteration 2/25 | Loss: 0.00175652
Iteration 3/25 | Loss: 0.00157826
Iteration 4/25 | Loss: 0.00154052
Iteration 5/25 | Loss: 0.00154585
Iteration 6/25 | Loss: 0.00147459
Iteration 7/25 | Loss: 0.00145381
Iteration 8/25 | Loss: 0.00144561
Iteration 9/25 | Loss: 0.00143653
Iteration 10/25 | Loss: 0.00143495
Iteration 11/25 | Loss: 0.00143378
Iteration 12/25 | Loss: 0.00142931
Iteration 13/25 | Loss: 0.00142734
Iteration 14/25 | Loss: 0.00142623
Iteration 15/25 | Loss: 0.00142586
Iteration 16/25 | Loss: 0.00142571
Iteration 17/25 | Loss: 0.00142569
Iteration 18/25 | Loss: 0.00142569
Iteration 19/25 | Loss: 0.00142569
Iteration 20/25 | Loss: 0.00142569
Iteration 21/25 | Loss: 0.00142569
Iteration 22/25 | Loss: 0.00142568
Iteration 23/25 | Loss: 0.00142568
Iteration 24/25 | Loss: 0.00142568
Iteration 25/25 | Loss: 0.00142568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30954909
Iteration 2/25 | Loss: 0.00202120
Iteration 3/25 | Loss: 0.00202111
Iteration 4/25 | Loss: 0.00202111
Iteration 5/25 | Loss: 0.00202111
Iteration 6/25 | Loss: 0.00202111
Iteration 7/25 | Loss: 0.00202111
Iteration 8/25 | Loss: 0.00202111
Iteration 9/25 | Loss: 0.00202111
Iteration 10/25 | Loss: 0.00202111
Iteration 11/25 | Loss: 0.00202111
Iteration 12/25 | Loss: 0.00202111
Iteration 13/25 | Loss: 0.00202111
Iteration 14/25 | Loss: 0.00202111
Iteration 15/25 | Loss: 0.00202111
Iteration 16/25 | Loss: 0.00202111
Iteration 17/25 | Loss: 0.00202111
Iteration 18/25 | Loss: 0.00202111
Iteration 19/25 | Loss: 0.00202111
Iteration 20/25 | Loss: 0.00202111
Iteration 21/25 | Loss: 0.00202111
Iteration 22/25 | Loss: 0.00202111
Iteration 23/25 | Loss: 0.00202111
Iteration 24/25 | Loss: 0.00202111
Iteration 25/25 | Loss: 0.00202111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002021106891334057, 0.002021106891334057, 0.002021106891334057, 0.002021106891334057, 0.002021106891334057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002021106891334057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202111
Iteration 2/1000 | Loss: 0.00028596
Iteration 3/1000 | Loss: 0.00012497
Iteration 4/1000 | Loss: 0.00007878
Iteration 5/1000 | Loss: 0.00005747
Iteration 6/1000 | Loss: 0.00005012
Iteration 7/1000 | Loss: 0.00004630
Iteration 8/1000 | Loss: 0.00004327
Iteration 9/1000 | Loss: 0.00004042
Iteration 10/1000 | Loss: 0.00003819
Iteration 11/1000 | Loss: 0.00003681
Iteration 12/1000 | Loss: 0.00003586
Iteration 13/1000 | Loss: 0.00003529
Iteration 14/1000 | Loss: 0.00003472
Iteration 15/1000 | Loss: 0.00003426
Iteration 16/1000 | Loss: 0.00003394
Iteration 17/1000 | Loss: 0.00003367
Iteration 18/1000 | Loss: 0.00003344
Iteration 19/1000 | Loss: 0.00003318
Iteration 20/1000 | Loss: 0.00003298
Iteration 21/1000 | Loss: 0.00003281
Iteration 22/1000 | Loss: 0.00003279
Iteration 23/1000 | Loss: 0.00003279
Iteration 24/1000 | Loss: 0.00003278
Iteration 25/1000 | Loss: 0.00003277
Iteration 26/1000 | Loss: 0.00003273
Iteration 27/1000 | Loss: 0.00003266
Iteration 28/1000 | Loss: 0.00003263
Iteration 29/1000 | Loss: 0.00003262
Iteration 30/1000 | Loss: 0.00003258
Iteration 31/1000 | Loss: 0.00003243
Iteration 32/1000 | Loss: 0.00003243
Iteration 33/1000 | Loss: 0.00003239
Iteration 34/1000 | Loss: 0.00003237
Iteration 35/1000 | Loss: 0.00003232
Iteration 36/1000 | Loss: 0.00003231
Iteration 37/1000 | Loss: 0.00003226
Iteration 38/1000 | Loss: 0.00003226
Iteration 39/1000 | Loss: 0.00003226
Iteration 40/1000 | Loss: 0.00003225
Iteration 41/1000 | Loss: 0.00003224
Iteration 42/1000 | Loss: 0.00003224
Iteration 43/1000 | Loss: 0.00003224
Iteration 44/1000 | Loss: 0.00003223
Iteration 45/1000 | Loss: 0.00003223
Iteration 46/1000 | Loss: 0.00003222
Iteration 47/1000 | Loss: 0.00003222
Iteration 48/1000 | Loss: 0.00003222
Iteration 49/1000 | Loss: 0.00003221
Iteration 50/1000 | Loss: 0.00003221
Iteration 51/1000 | Loss: 0.00003221
Iteration 52/1000 | Loss: 0.00003220
Iteration 53/1000 | Loss: 0.00003220
Iteration 54/1000 | Loss: 0.00003220
Iteration 55/1000 | Loss: 0.00003219
Iteration 56/1000 | Loss: 0.00003219
Iteration 57/1000 | Loss: 0.00003219
Iteration 58/1000 | Loss: 0.00003219
Iteration 59/1000 | Loss: 0.00003219
Iteration 60/1000 | Loss: 0.00003218
Iteration 61/1000 | Loss: 0.00003218
Iteration 62/1000 | Loss: 0.00003218
Iteration 63/1000 | Loss: 0.00003218
Iteration 64/1000 | Loss: 0.00003217
Iteration 65/1000 | Loss: 0.00003217
Iteration 66/1000 | Loss: 0.00003217
Iteration 67/1000 | Loss: 0.00003217
Iteration 68/1000 | Loss: 0.00003216
Iteration 69/1000 | Loss: 0.00003216
Iteration 70/1000 | Loss: 0.00003216
Iteration 71/1000 | Loss: 0.00003215
Iteration 72/1000 | Loss: 0.00003215
Iteration 73/1000 | Loss: 0.00003215
Iteration 74/1000 | Loss: 0.00003214
Iteration 75/1000 | Loss: 0.00025286
Iteration 76/1000 | Loss: 0.00003819
Iteration 77/1000 | Loss: 0.00003629
Iteration 78/1000 | Loss: 0.00003554
Iteration 79/1000 | Loss: 0.00003483
Iteration 80/1000 | Loss: 0.00003390
Iteration 81/1000 | Loss: 0.00003307
Iteration 82/1000 | Loss: 0.00003265
Iteration 83/1000 | Loss: 0.00003246
Iteration 84/1000 | Loss: 0.00003246
Iteration 85/1000 | Loss: 0.00003242
Iteration 86/1000 | Loss: 0.00003242
Iteration 87/1000 | Loss: 0.00003241
Iteration 88/1000 | Loss: 0.00003240
Iteration 89/1000 | Loss: 0.00003240
Iteration 90/1000 | Loss: 0.00003235
Iteration 91/1000 | Loss: 0.00003235
Iteration 92/1000 | Loss: 0.00003234
Iteration 93/1000 | Loss: 0.00003228
Iteration 94/1000 | Loss: 0.00003226
Iteration 95/1000 | Loss: 0.00003225
Iteration 96/1000 | Loss: 0.00003225
Iteration 97/1000 | Loss: 0.00003225
Iteration 98/1000 | Loss: 0.00003225
Iteration 99/1000 | Loss: 0.00003224
Iteration 100/1000 | Loss: 0.00003224
Iteration 101/1000 | Loss: 0.00004064
Iteration 102/1000 | Loss: 0.00003550
Iteration 103/1000 | Loss: 0.00003316
Iteration 104/1000 | Loss: 0.00003221
Iteration 105/1000 | Loss: 0.00003195
Iteration 106/1000 | Loss: 0.00003192
Iteration 107/1000 | Loss: 0.00003189
Iteration 108/1000 | Loss: 0.00003188
Iteration 109/1000 | Loss: 0.00003187
Iteration 110/1000 | Loss: 0.00003187
Iteration 111/1000 | Loss: 0.00003186
Iteration 112/1000 | Loss: 0.00003184
Iteration 113/1000 | Loss: 0.00003184
Iteration 114/1000 | Loss: 0.00003183
Iteration 115/1000 | Loss: 0.00003179
Iteration 116/1000 | Loss: 0.00003179
Iteration 117/1000 | Loss: 0.00003178
Iteration 118/1000 | Loss: 0.00003178
Iteration 119/1000 | Loss: 0.00003176
Iteration 120/1000 | Loss: 0.00003176
Iteration 121/1000 | Loss: 0.00003175
Iteration 122/1000 | Loss: 0.00003175
Iteration 123/1000 | Loss: 0.00003175
Iteration 124/1000 | Loss: 0.00003174
Iteration 125/1000 | Loss: 0.00003174
Iteration 126/1000 | Loss: 0.00003173
Iteration 127/1000 | Loss: 0.00003173
Iteration 128/1000 | Loss: 0.00003173
Iteration 129/1000 | Loss: 0.00003173
Iteration 130/1000 | Loss: 0.00003172
Iteration 131/1000 | Loss: 0.00003172
Iteration 132/1000 | Loss: 0.00003170
Iteration 133/1000 | Loss: 0.00003170
Iteration 134/1000 | Loss: 0.00003169
Iteration 135/1000 | Loss: 0.00003169
Iteration 136/1000 | Loss: 0.00003169
Iteration 137/1000 | Loss: 0.00003168
Iteration 138/1000 | Loss: 0.00003168
Iteration 139/1000 | Loss: 0.00003168
Iteration 140/1000 | Loss: 0.00003168
Iteration 141/1000 | Loss: 0.00003168
Iteration 142/1000 | Loss: 0.00003168
Iteration 143/1000 | Loss: 0.00003167
Iteration 144/1000 | Loss: 0.00003167
Iteration 145/1000 | Loss: 0.00003167
Iteration 146/1000 | Loss: 0.00003166
Iteration 147/1000 | Loss: 0.00003166
Iteration 148/1000 | Loss: 0.00003166
Iteration 149/1000 | Loss: 0.00003166
Iteration 150/1000 | Loss: 0.00003165
Iteration 151/1000 | Loss: 0.00003165
Iteration 152/1000 | Loss: 0.00003164
Iteration 153/1000 | Loss: 0.00003164
Iteration 154/1000 | Loss: 0.00003164
Iteration 155/1000 | Loss: 0.00003163
Iteration 156/1000 | Loss: 0.00003163
Iteration 157/1000 | Loss: 0.00003162
Iteration 158/1000 | Loss: 0.00003161
Iteration 159/1000 | Loss: 0.00003161
Iteration 160/1000 | Loss: 0.00003161
Iteration 161/1000 | Loss: 0.00003160
Iteration 162/1000 | Loss: 0.00003160
Iteration 163/1000 | Loss: 0.00003160
Iteration 164/1000 | Loss: 0.00003159
Iteration 165/1000 | Loss: 0.00003159
Iteration 166/1000 | Loss: 0.00003159
Iteration 167/1000 | Loss: 0.00003158
Iteration 168/1000 | Loss: 0.00003158
Iteration 169/1000 | Loss: 0.00003158
Iteration 170/1000 | Loss: 0.00003157
Iteration 171/1000 | Loss: 0.00003157
Iteration 172/1000 | Loss: 0.00003157
Iteration 173/1000 | Loss: 0.00003157
Iteration 174/1000 | Loss: 0.00003157
Iteration 175/1000 | Loss: 0.00003156
Iteration 176/1000 | Loss: 0.00003156
Iteration 177/1000 | Loss: 0.00003156
Iteration 178/1000 | Loss: 0.00003156
Iteration 179/1000 | Loss: 0.00003156
Iteration 180/1000 | Loss: 0.00003156
Iteration 181/1000 | Loss: 0.00003156
Iteration 182/1000 | Loss: 0.00003156
Iteration 183/1000 | Loss: 0.00003156
Iteration 184/1000 | Loss: 0.00003156
Iteration 185/1000 | Loss: 0.00003156
Iteration 186/1000 | Loss: 0.00003156
Iteration 187/1000 | Loss: 0.00003156
Iteration 188/1000 | Loss: 0.00003155
Iteration 189/1000 | Loss: 0.00003155
Iteration 190/1000 | Loss: 0.00003155
Iteration 191/1000 | Loss: 0.00003155
Iteration 192/1000 | Loss: 0.00003155
Iteration 193/1000 | Loss: 0.00003155
Iteration 194/1000 | Loss: 0.00003155
Iteration 195/1000 | Loss: 0.00003155
Iteration 196/1000 | Loss: 0.00003155
Iteration 197/1000 | Loss: 0.00003155
Iteration 198/1000 | Loss: 0.00003155
Iteration 199/1000 | Loss: 0.00003155
Iteration 200/1000 | Loss: 0.00003155
Iteration 201/1000 | Loss: 0.00003155
Iteration 202/1000 | Loss: 0.00003155
Iteration 203/1000 | Loss: 0.00003155
Iteration 204/1000 | Loss: 0.00003155
Iteration 205/1000 | Loss: 0.00003155
Iteration 206/1000 | Loss: 0.00003155
Iteration 207/1000 | Loss: 0.00003155
Iteration 208/1000 | Loss: 0.00003155
Iteration 209/1000 | Loss: 0.00003155
Iteration 210/1000 | Loss: 0.00003155
Iteration 211/1000 | Loss: 0.00003155
Iteration 212/1000 | Loss: 0.00003155
Iteration 213/1000 | Loss: 0.00003155
Iteration 214/1000 | Loss: 0.00003155
Iteration 215/1000 | Loss: 0.00003155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [3.154715523123741e-05, 3.154715523123741e-05, 3.154715523123741e-05, 3.154715523123741e-05, 3.154715523123741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.154715523123741e-05

Optimization complete. Final v2v error: 4.1138458251953125 mm

Highest mean error: 11.17970085144043 mm for frame 137

Lowest mean error: 3.251553773880005 mm for frame 76

Saving results

Total time: 100.15550708770752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593499
Iteration 2/25 | Loss: 0.00142502
Iteration 3/25 | Loss: 0.00131786
Iteration 4/25 | Loss: 0.00130336
Iteration 5/25 | Loss: 0.00130003
Iteration 6/25 | Loss: 0.00129988
Iteration 7/25 | Loss: 0.00129988
Iteration 8/25 | Loss: 0.00129988
Iteration 9/25 | Loss: 0.00129988
Iteration 10/25 | Loss: 0.00129988
Iteration 11/25 | Loss: 0.00129988
Iteration 12/25 | Loss: 0.00129988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012998766032978892, 0.0012998766032978892, 0.0012998766032978892, 0.0012998766032978892, 0.0012998766032978892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012998766032978892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83104253
Iteration 2/25 | Loss: 0.00090638
Iteration 3/25 | Loss: 0.00090638
Iteration 4/25 | Loss: 0.00090638
Iteration 5/25 | Loss: 0.00090637
Iteration 6/25 | Loss: 0.00090637
Iteration 7/25 | Loss: 0.00090637
Iteration 8/25 | Loss: 0.00090637
Iteration 9/25 | Loss: 0.00090637
Iteration 10/25 | Loss: 0.00090637
Iteration 11/25 | Loss: 0.00090637
Iteration 12/25 | Loss: 0.00090637
Iteration 13/25 | Loss: 0.00090637
Iteration 14/25 | Loss: 0.00090637
Iteration 15/25 | Loss: 0.00090637
Iteration 16/25 | Loss: 0.00090637
Iteration 17/25 | Loss: 0.00090637
Iteration 18/25 | Loss: 0.00090637
Iteration 19/25 | Loss: 0.00090637
Iteration 20/25 | Loss: 0.00090637
Iteration 21/25 | Loss: 0.00090637
Iteration 22/25 | Loss: 0.00090637
Iteration 23/25 | Loss: 0.00090637
Iteration 24/25 | Loss: 0.00090637
Iteration 25/25 | Loss: 0.00090637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090637
Iteration 2/1000 | Loss: 0.00003784
Iteration 3/1000 | Loss: 0.00002359
Iteration 4/1000 | Loss: 0.00002070
Iteration 5/1000 | Loss: 0.00001978
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001728
Iteration 12/1000 | Loss: 0.00001719
Iteration 13/1000 | Loss: 0.00001714
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001713
Iteration 16/1000 | Loss: 0.00001711
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001701
Iteration 20/1000 | Loss: 0.00001693
Iteration 21/1000 | Loss: 0.00001690
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001677
Iteration 33/1000 | Loss: 0.00001676
Iteration 34/1000 | Loss: 0.00001676
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00001673
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001666
Iteration 54/1000 | Loss: 0.00001665
Iteration 55/1000 | Loss: 0.00001665
Iteration 56/1000 | Loss: 0.00001665
Iteration 57/1000 | Loss: 0.00001664
Iteration 58/1000 | Loss: 0.00001664
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001661
Iteration 66/1000 | Loss: 0.00001660
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001659
Iteration 70/1000 | Loss: 0.00001659
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001658
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001657
Iteration 76/1000 | Loss: 0.00001657
Iteration 77/1000 | Loss: 0.00001657
Iteration 78/1000 | Loss: 0.00001657
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001656
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001656
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001655
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001655
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001654
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001654
Iteration 99/1000 | Loss: 0.00001654
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001654
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001654
Iteration 106/1000 | Loss: 0.00001654
Iteration 107/1000 | Loss: 0.00001653
Iteration 108/1000 | Loss: 0.00001653
Iteration 109/1000 | Loss: 0.00001653
Iteration 110/1000 | Loss: 0.00001653
Iteration 111/1000 | Loss: 0.00001653
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001652
Iteration 116/1000 | Loss: 0.00001652
Iteration 117/1000 | Loss: 0.00001652
Iteration 118/1000 | Loss: 0.00001652
Iteration 119/1000 | Loss: 0.00001652
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001651
Iteration 123/1000 | Loss: 0.00001651
Iteration 124/1000 | Loss: 0.00001651
Iteration 125/1000 | Loss: 0.00001651
Iteration 126/1000 | Loss: 0.00001651
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001650
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001648
Iteration 140/1000 | Loss: 0.00001648
Iteration 141/1000 | Loss: 0.00001648
Iteration 142/1000 | Loss: 0.00001648
Iteration 143/1000 | Loss: 0.00001648
Iteration 144/1000 | Loss: 0.00001648
Iteration 145/1000 | Loss: 0.00001647
Iteration 146/1000 | Loss: 0.00001647
Iteration 147/1000 | Loss: 0.00001647
Iteration 148/1000 | Loss: 0.00001647
Iteration 149/1000 | Loss: 0.00001646
Iteration 150/1000 | Loss: 0.00001646
Iteration 151/1000 | Loss: 0.00001646
Iteration 152/1000 | Loss: 0.00001646
Iteration 153/1000 | Loss: 0.00001646
Iteration 154/1000 | Loss: 0.00001646
Iteration 155/1000 | Loss: 0.00001645
Iteration 156/1000 | Loss: 0.00001645
Iteration 157/1000 | Loss: 0.00001645
Iteration 158/1000 | Loss: 0.00001645
Iteration 159/1000 | Loss: 0.00001644
Iteration 160/1000 | Loss: 0.00001644
Iteration 161/1000 | Loss: 0.00001644
Iteration 162/1000 | Loss: 0.00001644
Iteration 163/1000 | Loss: 0.00001644
Iteration 164/1000 | Loss: 0.00001644
Iteration 165/1000 | Loss: 0.00001643
Iteration 166/1000 | Loss: 0.00001643
Iteration 167/1000 | Loss: 0.00001643
Iteration 168/1000 | Loss: 0.00001643
Iteration 169/1000 | Loss: 0.00001643
Iteration 170/1000 | Loss: 0.00001642
Iteration 171/1000 | Loss: 0.00001642
Iteration 172/1000 | Loss: 0.00001642
Iteration 173/1000 | Loss: 0.00001642
Iteration 174/1000 | Loss: 0.00001642
Iteration 175/1000 | Loss: 0.00001642
Iteration 176/1000 | Loss: 0.00001642
Iteration 177/1000 | Loss: 0.00001642
Iteration 178/1000 | Loss: 0.00001642
Iteration 179/1000 | Loss: 0.00001642
Iteration 180/1000 | Loss: 0.00001642
Iteration 181/1000 | Loss: 0.00001642
Iteration 182/1000 | Loss: 0.00001641
Iteration 183/1000 | Loss: 0.00001641
Iteration 184/1000 | Loss: 0.00001641
Iteration 185/1000 | Loss: 0.00001641
Iteration 186/1000 | Loss: 0.00001641
Iteration 187/1000 | Loss: 0.00001641
Iteration 188/1000 | Loss: 0.00001641
Iteration 189/1000 | Loss: 0.00001641
Iteration 190/1000 | Loss: 0.00001641
Iteration 191/1000 | Loss: 0.00001641
Iteration 192/1000 | Loss: 0.00001641
Iteration 193/1000 | Loss: 0.00001641
Iteration 194/1000 | Loss: 0.00001641
Iteration 195/1000 | Loss: 0.00001641
Iteration 196/1000 | Loss: 0.00001640
Iteration 197/1000 | Loss: 0.00001640
Iteration 198/1000 | Loss: 0.00001640
Iteration 199/1000 | Loss: 0.00001640
Iteration 200/1000 | Loss: 0.00001640
Iteration 201/1000 | Loss: 0.00001640
Iteration 202/1000 | Loss: 0.00001640
Iteration 203/1000 | Loss: 0.00001640
Iteration 204/1000 | Loss: 0.00001640
Iteration 205/1000 | Loss: 0.00001640
Iteration 206/1000 | Loss: 0.00001640
Iteration 207/1000 | Loss: 0.00001640
Iteration 208/1000 | Loss: 0.00001640
Iteration 209/1000 | Loss: 0.00001640
Iteration 210/1000 | Loss: 0.00001640
Iteration 211/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.6404750567744486e-05, 1.6404750567744486e-05, 1.6404750567744486e-05, 1.6404750567744486e-05, 1.6404750567744486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6404750567744486e-05

Optimization complete. Final v2v error: 3.4668800830841064 mm

Highest mean error: 4.393531799316406 mm for frame 168

Lowest mean error: 3.1567888259887695 mm for frame 46

Saving results

Total time: 46.71880507469177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00342149
Iteration 2/25 | Loss: 0.00134691
Iteration 3/25 | Loss: 0.00125330
Iteration 4/25 | Loss: 0.00124304
Iteration 5/25 | Loss: 0.00123857
Iteration 6/25 | Loss: 0.00123718
Iteration 7/25 | Loss: 0.00123718
Iteration 8/25 | Loss: 0.00123718
Iteration 9/25 | Loss: 0.00123718
Iteration 10/25 | Loss: 0.00123718
Iteration 11/25 | Loss: 0.00123718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012371819466352463, 0.0012371819466352463, 0.0012371819466352463, 0.0012371819466352463, 0.0012371819466352463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012371819466352463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39961433
Iteration 2/25 | Loss: 0.00086090
Iteration 3/25 | Loss: 0.00086090
Iteration 4/25 | Loss: 0.00086090
Iteration 5/25 | Loss: 0.00086090
Iteration 6/25 | Loss: 0.00086090
Iteration 7/25 | Loss: 0.00086090
Iteration 8/25 | Loss: 0.00086090
Iteration 9/25 | Loss: 0.00086090
Iteration 10/25 | Loss: 0.00086090
Iteration 11/25 | Loss: 0.00086090
Iteration 12/25 | Loss: 0.00086090
Iteration 13/25 | Loss: 0.00086090
Iteration 14/25 | Loss: 0.00086090
Iteration 15/25 | Loss: 0.00086090
Iteration 16/25 | Loss: 0.00086090
Iteration 17/25 | Loss: 0.00086090
Iteration 18/25 | Loss: 0.00086090
Iteration 19/25 | Loss: 0.00086090
Iteration 20/25 | Loss: 0.00086090
Iteration 21/25 | Loss: 0.00086090
Iteration 22/25 | Loss: 0.00086090
Iteration 23/25 | Loss: 0.00086090
Iteration 24/25 | Loss: 0.00086090
Iteration 25/25 | Loss: 0.00086090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086090
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00002106
Iteration 4/1000 | Loss: 0.00001872
Iteration 5/1000 | Loss: 0.00001743
Iteration 6/1000 | Loss: 0.00001648
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001518
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00001484
Iteration 12/1000 | Loss: 0.00001474
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001471
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001470
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001466
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001456
Iteration 27/1000 | Loss: 0.00001456
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001453
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001452
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001446
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001442
Iteration 49/1000 | Loss: 0.00001442
Iteration 50/1000 | Loss: 0.00001442
Iteration 51/1000 | Loss: 0.00001442
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001441
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001434
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001433
Iteration 58/1000 | Loss: 0.00001430
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001421
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001420
Iteration 104/1000 | Loss: 0.00001420
Iteration 105/1000 | Loss: 0.00001420
Iteration 106/1000 | Loss: 0.00001420
Iteration 107/1000 | Loss: 0.00001420
Iteration 108/1000 | Loss: 0.00001420
Iteration 109/1000 | Loss: 0.00001420
Iteration 110/1000 | Loss: 0.00001420
Iteration 111/1000 | Loss: 0.00001420
Iteration 112/1000 | Loss: 0.00001420
Iteration 113/1000 | Loss: 0.00001420
Iteration 114/1000 | Loss: 0.00001420
Iteration 115/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.4196454685588833e-05, 1.4196454685588833e-05, 1.4196454685588833e-05, 1.4196454685588833e-05, 1.4196454685588833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4196454685588833e-05

Optimization complete. Final v2v error: 3.2528440952301025 mm

Highest mean error: 3.5526881217956543 mm for frame 118

Lowest mean error: 3.0481159687042236 mm for frame 47

Saving results

Total time: 41.79165267944336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873176
Iteration 2/25 | Loss: 0.00150656
Iteration 3/25 | Loss: 0.00140742
Iteration 4/25 | Loss: 0.00139192
Iteration 5/25 | Loss: 0.00138762
Iteration 6/25 | Loss: 0.00138762
Iteration 7/25 | Loss: 0.00138762
Iteration 8/25 | Loss: 0.00138762
Iteration 9/25 | Loss: 0.00138762
Iteration 10/25 | Loss: 0.00138762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013876179000362754, 0.0013876179000362754, 0.0013876179000362754, 0.0013876179000362754, 0.0013876179000362754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013876179000362754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35529757
Iteration 2/25 | Loss: 0.00087279
Iteration 3/25 | Loss: 0.00087272
Iteration 4/25 | Loss: 0.00087272
Iteration 5/25 | Loss: 0.00087272
Iteration 6/25 | Loss: 0.00087272
Iteration 7/25 | Loss: 0.00087272
Iteration 8/25 | Loss: 0.00087272
Iteration 9/25 | Loss: 0.00087272
Iteration 10/25 | Loss: 0.00087272
Iteration 11/25 | Loss: 0.00087272
Iteration 12/25 | Loss: 0.00087272
Iteration 13/25 | Loss: 0.00087272
Iteration 14/25 | Loss: 0.00087272
Iteration 15/25 | Loss: 0.00087272
Iteration 16/25 | Loss: 0.00087272
Iteration 17/25 | Loss: 0.00087272
Iteration 18/25 | Loss: 0.00087272
Iteration 19/25 | Loss: 0.00087272
Iteration 20/25 | Loss: 0.00087272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008727186941541731, 0.0008727186941541731, 0.0008727186941541731, 0.0008727186941541731, 0.0008727186941541731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008727186941541731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087272
Iteration 2/1000 | Loss: 0.00005029
Iteration 3/1000 | Loss: 0.00003811
Iteration 4/1000 | Loss: 0.00003506
Iteration 5/1000 | Loss: 0.00003335
Iteration 6/1000 | Loss: 0.00003231
Iteration 7/1000 | Loss: 0.00003150
Iteration 8/1000 | Loss: 0.00003106
Iteration 9/1000 | Loss: 0.00003058
Iteration 10/1000 | Loss: 0.00003014
Iteration 11/1000 | Loss: 0.00002985
Iteration 12/1000 | Loss: 0.00002946
Iteration 13/1000 | Loss: 0.00002934
Iteration 14/1000 | Loss: 0.00002916
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002910
Iteration 17/1000 | Loss: 0.00002901
Iteration 18/1000 | Loss: 0.00002893
Iteration 19/1000 | Loss: 0.00002891
Iteration 20/1000 | Loss: 0.00002886
Iteration 21/1000 | Loss: 0.00002884
Iteration 22/1000 | Loss: 0.00002883
Iteration 23/1000 | Loss: 0.00002882
Iteration 24/1000 | Loss: 0.00002882
Iteration 25/1000 | Loss: 0.00002882
Iteration 26/1000 | Loss: 0.00002880
Iteration 27/1000 | Loss: 0.00002880
Iteration 28/1000 | Loss: 0.00002880
Iteration 29/1000 | Loss: 0.00002880
Iteration 30/1000 | Loss: 0.00002880
Iteration 31/1000 | Loss: 0.00002880
Iteration 32/1000 | Loss: 0.00002880
Iteration 33/1000 | Loss: 0.00002880
Iteration 34/1000 | Loss: 0.00002879
Iteration 35/1000 | Loss: 0.00002879
Iteration 36/1000 | Loss: 0.00002879
Iteration 37/1000 | Loss: 0.00002879
Iteration 38/1000 | Loss: 0.00002877
Iteration 39/1000 | Loss: 0.00002876
Iteration 40/1000 | Loss: 0.00002875
Iteration 41/1000 | Loss: 0.00002875
Iteration 42/1000 | Loss: 0.00002875
Iteration 43/1000 | Loss: 0.00002874
Iteration 44/1000 | Loss: 0.00002874
Iteration 45/1000 | Loss: 0.00002871
Iteration 46/1000 | Loss: 0.00002871
Iteration 47/1000 | Loss: 0.00002870
Iteration 48/1000 | Loss: 0.00002870
Iteration 49/1000 | Loss: 0.00002870
Iteration 50/1000 | Loss: 0.00002869
Iteration 51/1000 | Loss: 0.00002869
Iteration 52/1000 | Loss: 0.00002868
Iteration 53/1000 | Loss: 0.00002867
Iteration 54/1000 | Loss: 0.00002867
Iteration 55/1000 | Loss: 0.00002867
Iteration 56/1000 | Loss: 0.00002867
Iteration 57/1000 | Loss: 0.00002867
Iteration 58/1000 | Loss: 0.00002866
Iteration 59/1000 | Loss: 0.00002866
Iteration 60/1000 | Loss: 0.00002866
Iteration 61/1000 | Loss: 0.00002865
Iteration 62/1000 | Loss: 0.00002865
Iteration 63/1000 | Loss: 0.00002865
Iteration 64/1000 | Loss: 0.00002864
Iteration 65/1000 | Loss: 0.00002864
Iteration 66/1000 | Loss: 0.00002864
Iteration 67/1000 | Loss: 0.00002864
Iteration 68/1000 | Loss: 0.00002864
Iteration 69/1000 | Loss: 0.00002864
Iteration 70/1000 | Loss: 0.00002864
Iteration 71/1000 | Loss: 0.00002863
Iteration 72/1000 | Loss: 0.00002863
Iteration 73/1000 | Loss: 0.00002863
Iteration 74/1000 | Loss: 0.00002863
Iteration 75/1000 | Loss: 0.00002863
Iteration 76/1000 | Loss: 0.00002863
Iteration 77/1000 | Loss: 0.00002862
Iteration 78/1000 | Loss: 0.00002862
Iteration 79/1000 | Loss: 0.00002862
Iteration 80/1000 | Loss: 0.00002861
Iteration 81/1000 | Loss: 0.00002861
Iteration 82/1000 | Loss: 0.00002860
Iteration 83/1000 | Loss: 0.00002860
Iteration 84/1000 | Loss: 0.00002860
Iteration 85/1000 | Loss: 0.00002860
Iteration 86/1000 | Loss: 0.00002860
Iteration 87/1000 | Loss: 0.00002860
Iteration 88/1000 | Loss: 0.00002860
Iteration 89/1000 | Loss: 0.00002860
Iteration 90/1000 | Loss: 0.00002859
Iteration 91/1000 | Loss: 0.00002859
Iteration 92/1000 | Loss: 0.00002859
Iteration 93/1000 | Loss: 0.00002859
Iteration 94/1000 | Loss: 0.00002859
Iteration 95/1000 | Loss: 0.00002859
Iteration 96/1000 | Loss: 0.00002859
Iteration 97/1000 | Loss: 0.00002858
Iteration 98/1000 | Loss: 0.00002858
Iteration 99/1000 | Loss: 0.00002858
Iteration 100/1000 | Loss: 0.00002858
Iteration 101/1000 | Loss: 0.00002858
Iteration 102/1000 | Loss: 0.00002858
Iteration 103/1000 | Loss: 0.00002858
Iteration 104/1000 | Loss: 0.00002858
Iteration 105/1000 | Loss: 0.00002858
Iteration 106/1000 | Loss: 0.00002857
Iteration 107/1000 | Loss: 0.00002857
Iteration 108/1000 | Loss: 0.00002857
Iteration 109/1000 | Loss: 0.00002857
Iteration 110/1000 | Loss: 0.00002857
Iteration 111/1000 | Loss: 0.00002857
Iteration 112/1000 | Loss: 0.00002857
Iteration 113/1000 | Loss: 0.00002857
Iteration 114/1000 | Loss: 0.00002856
Iteration 115/1000 | Loss: 0.00002856
Iteration 116/1000 | Loss: 0.00002856
Iteration 117/1000 | Loss: 0.00002856
Iteration 118/1000 | Loss: 0.00002856
Iteration 119/1000 | Loss: 0.00002856
Iteration 120/1000 | Loss: 0.00002856
Iteration 121/1000 | Loss: 0.00002856
Iteration 122/1000 | Loss: 0.00002856
Iteration 123/1000 | Loss: 0.00002856
Iteration 124/1000 | Loss: 0.00002856
Iteration 125/1000 | Loss: 0.00002855
Iteration 126/1000 | Loss: 0.00002855
Iteration 127/1000 | Loss: 0.00002855
Iteration 128/1000 | Loss: 0.00002855
Iteration 129/1000 | Loss: 0.00002855
Iteration 130/1000 | Loss: 0.00002855
Iteration 131/1000 | Loss: 0.00002855
Iteration 132/1000 | Loss: 0.00002855
Iteration 133/1000 | Loss: 0.00002855
Iteration 134/1000 | Loss: 0.00002855
Iteration 135/1000 | Loss: 0.00002855
Iteration 136/1000 | Loss: 0.00002855
Iteration 137/1000 | Loss: 0.00002855
Iteration 138/1000 | Loss: 0.00002855
Iteration 139/1000 | Loss: 0.00002855
Iteration 140/1000 | Loss: 0.00002855
Iteration 141/1000 | Loss: 0.00002855
Iteration 142/1000 | Loss: 0.00002855
Iteration 143/1000 | Loss: 0.00002855
Iteration 144/1000 | Loss: 0.00002855
Iteration 145/1000 | Loss: 0.00002855
Iteration 146/1000 | Loss: 0.00002855
Iteration 147/1000 | Loss: 0.00002855
Iteration 148/1000 | Loss: 0.00002855
Iteration 149/1000 | Loss: 0.00002855
Iteration 150/1000 | Loss: 0.00002855
Iteration 151/1000 | Loss: 0.00002855
Iteration 152/1000 | Loss: 0.00002855
Iteration 153/1000 | Loss: 0.00002855
Iteration 154/1000 | Loss: 0.00002855
Iteration 155/1000 | Loss: 0.00002855
Iteration 156/1000 | Loss: 0.00002855
Iteration 157/1000 | Loss: 0.00002855
Iteration 158/1000 | Loss: 0.00002855
Iteration 159/1000 | Loss: 0.00002855
Iteration 160/1000 | Loss: 0.00002855
Iteration 161/1000 | Loss: 0.00002855
Iteration 162/1000 | Loss: 0.00002855
Iteration 163/1000 | Loss: 0.00002855
Iteration 164/1000 | Loss: 0.00002855
Iteration 165/1000 | Loss: 0.00002855
Iteration 166/1000 | Loss: 0.00002855
Iteration 167/1000 | Loss: 0.00002855
Iteration 168/1000 | Loss: 0.00002855
Iteration 169/1000 | Loss: 0.00002855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.855056300177239e-05, 2.855056300177239e-05, 2.855056300177239e-05, 2.855056300177239e-05, 2.855056300177239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.855056300177239e-05

Optimization complete. Final v2v error: 4.432337284088135 mm

Highest mean error: 4.756965637207031 mm for frame 149

Lowest mean error: 3.932159900665283 mm for frame 28

Saving results

Total time: 44.4332218170166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529838
Iteration 2/25 | Loss: 0.00153537
Iteration 3/25 | Loss: 0.00134702
Iteration 4/25 | Loss: 0.00133172
Iteration 5/25 | Loss: 0.00132811
Iteration 6/25 | Loss: 0.00132723
Iteration 7/25 | Loss: 0.00132723
Iteration 8/25 | Loss: 0.00132723
Iteration 9/25 | Loss: 0.00132723
Iteration 10/25 | Loss: 0.00132723
Iteration 11/25 | Loss: 0.00132723
Iteration 12/25 | Loss: 0.00132723
Iteration 13/25 | Loss: 0.00132723
Iteration 14/25 | Loss: 0.00132723
Iteration 15/25 | Loss: 0.00132723
Iteration 16/25 | Loss: 0.00132723
Iteration 17/25 | Loss: 0.00132723
Iteration 18/25 | Loss: 0.00132723
Iteration 19/25 | Loss: 0.00132723
Iteration 20/25 | Loss: 0.00132723
Iteration 21/25 | Loss: 0.00132723
Iteration 22/25 | Loss: 0.00132723
Iteration 23/25 | Loss: 0.00132723
Iteration 24/25 | Loss: 0.00132723
Iteration 25/25 | Loss: 0.00132723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39893949
Iteration 2/25 | Loss: 0.00072774
Iteration 3/25 | Loss: 0.00072772
Iteration 4/25 | Loss: 0.00072772
Iteration 5/25 | Loss: 0.00072772
Iteration 6/25 | Loss: 0.00072772
Iteration 7/25 | Loss: 0.00072772
Iteration 8/25 | Loss: 0.00072772
Iteration 9/25 | Loss: 0.00072772
Iteration 10/25 | Loss: 0.00072772
Iteration 11/25 | Loss: 0.00072772
Iteration 12/25 | Loss: 0.00072772
Iteration 13/25 | Loss: 0.00072772
Iteration 14/25 | Loss: 0.00072772
Iteration 15/25 | Loss: 0.00072772
Iteration 16/25 | Loss: 0.00072772
Iteration 17/25 | Loss: 0.00072772
Iteration 18/25 | Loss: 0.00072772
Iteration 19/25 | Loss: 0.00072772
Iteration 20/25 | Loss: 0.00072772
Iteration 21/25 | Loss: 0.00072772
Iteration 22/25 | Loss: 0.00072772
Iteration 23/25 | Loss: 0.00072772
Iteration 24/25 | Loss: 0.00072772
Iteration 25/25 | Loss: 0.00072772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072772
Iteration 2/1000 | Loss: 0.00003992
Iteration 3/1000 | Loss: 0.00002693
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001945
Iteration 9/1000 | Loss: 0.00001917
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001874
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001842
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001836
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00001828
Iteration 22/1000 | Loss: 0.00001828
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001826
Iteration 25/1000 | Loss: 0.00001825
Iteration 26/1000 | Loss: 0.00001825
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001823
Iteration 29/1000 | Loss: 0.00001822
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001817
Iteration 32/1000 | Loss: 0.00001817
Iteration 33/1000 | Loss: 0.00001814
Iteration 34/1000 | Loss: 0.00001814
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001814
Iteration 38/1000 | Loss: 0.00001814
Iteration 39/1000 | Loss: 0.00001814
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001814
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001813
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001811
Iteration 51/1000 | Loss: 0.00001811
Iteration 52/1000 | Loss: 0.00001811
Iteration 53/1000 | Loss: 0.00001810
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001808
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001807
Iteration 60/1000 | Loss: 0.00001807
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001803
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001803
Iteration 70/1000 | Loss: 0.00001801
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001799
Iteration 75/1000 | Loss: 0.00001799
Iteration 76/1000 | Loss: 0.00001799
Iteration 77/1000 | Loss: 0.00001798
Iteration 78/1000 | Loss: 0.00001798
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001797
Iteration 81/1000 | Loss: 0.00001797
Iteration 82/1000 | Loss: 0.00001797
Iteration 83/1000 | Loss: 0.00001797
Iteration 84/1000 | Loss: 0.00001796
Iteration 85/1000 | Loss: 0.00001796
Iteration 86/1000 | Loss: 0.00001796
Iteration 87/1000 | Loss: 0.00001795
Iteration 88/1000 | Loss: 0.00001795
Iteration 89/1000 | Loss: 0.00001795
Iteration 90/1000 | Loss: 0.00001795
Iteration 91/1000 | Loss: 0.00001795
Iteration 92/1000 | Loss: 0.00001795
Iteration 93/1000 | Loss: 0.00001794
Iteration 94/1000 | Loss: 0.00001794
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00001794
Iteration 97/1000 | Loss: 0.00001794
Iteration 98/1000 | Loss: 0.00001794
Iteration 99/1000 | Loss: 0.00001794
Iteration 100/1000 | Loss: 0.00001793
Iteration 101/1000 | Loss: 0.00001793
Iteration 102/1000 | Loss: 0.00001793
Iteration 103/1000 | Loss: 0.00001793
Iteration 104/1000 | Loss: 0.00001793
Iteration 105/1000 | Loss: 0.00001793
Iteration 106/1000 | Loss: 0.00001793
Iteration 107/1000 | Loss: 0.00001793
Iteration 108/1000 | Loss: 0.00001793
Iteration 109/1000 | Loss: 0.00001792
Iteration 110/1000 | Loss: 0.00001792
Iteration 111/1000 | Loss: 0.00001792
Iteration 112/1000 | Loss: 0.00001792
Iteration 113/1000 | Loss: 0.00001792
Iteration 114/1000 | Loss: 0.00001792
Iteration 115/1000 | Loss: 0.00001792
Iteration 116/1000 | Loss: 0.00001791
Iteration 117/1000 | Loss: 0.00001791
Iteration 118/1000 | Loss: 0.00001791
Iteration 119/1000 | Loss: 0.00001791
Iteration 120/1000 | Loss: 0.00001791
Iteration 121/1000 | Loss: 0.00001790
Iteration 122/1000 | Loss: 0.00001790
Iteration 123/1000 | Loss: 0.00001790
Iteration 124/1000 | Loss: 0.00001790
Iteration 125/1000 | Loss: 0.00001790
Iteration 126/1000 | Loss: 0.00001790
Iteration 127/1000 | Loss: 0.00001790
Iteration 128/1000 | Loss: 0.00001790
Iteration 129/1000 | Loss: 0.00001790
Iteration 130/1000 | Loss: 0.00001790
Iteration 131/1000 | Loss: 0.00001790
Iteration 132/1000 | Loss: 0.00001790
Iteration 133/1000 | Loss: 0.00001790
Iteration 134/1000 | Loss: 0.00001790
Iteration 135/1000 | Loss: 0.00001789
Iteration 136/1000 | Loss: 0.00001789
Iteration 137/1000 | Loss: 0.00001789
Iteration 138/1000 | Loss: 0.00001789
Iteration 139/1000 | Loss: 0.00001789
Iteration 140/1000 | Loss: 0.00001789
Iteration 141/1000 | Loss: 0.00001789
Iteration 142/1000 | Loss: 0.00001789
Iteration 143/1000 | Loss: 0.00001789
Iteration 144/1000 | Loss: 0.00001789
Iteration 145/1000 | Loss: 0.00001789
Iteration 146/1000 | Loss: 0.00001788
Iteration 147/1000 | Loss: 0.00001788
Iteration 148/1000 | Loss: 0.00001788
Iteration 149/1000 | Loss: 0.00001788
Iteration 150/1000 | Loss: 0.00001788
Iteration 151/1000 | Loss: 0.00001788
Iteration 152/1000 | Loss: 0.00001788
Iteration 153/1000 | Loss: 0.00001788
Iteration 154/1000 | Loss: 0.00001788
Iteration 155/1000 | Loss: 0.00001788
Iteration 156/1000 | Loss: 0.00001788
Iteration 157/1000 | Loss: 0.00001788
Iteration 158/1000 | Loss: 0.00001788
Iteration 159/1000 | Loss: 0.00001788
Iteration 160/1000 | Loss: 0.00001788
Iteration 161/1000 | Loss: 0.00001788
Iteration 162/1000 | Loss: 0.00001788
Iteration 163/1000 | Loss: 0.00001788
Iteration 164/1000 | Loss: 0.00001788
Iteration 165/1000 | Loss: 0.00001788
Iteration 166/1000 | Loss: 0.00001787
Iteration 167/1000 | Loss: 0.00001787
Iteration 168/1000 | Loss: 0.00001787
Iteration 169/1000 | Loss: 0.00001787
Iteration 170/1000 | Loss: 0.00001787
Iteration 171/1000 | Loss: 0.00001787
Iteration 172/1000 | Loss: 0.00001787
Iteration 173/1000 | Loss: 0.00001787
Iteration 174/1000 | Loss: 0.00001787
Iteration 175/1000 | Loss: 0.00001787
Iteration 176/1000 | Loss: 0.00001787
Iteration 177/1000 | Loss: 0.00001787
Iteration 178/1000 | Loss: 0.00001787
Iteration 179/1000 | Loss: 0.00001787
Iteration 180/1000 | Loss: 0.00001786
Iteration 181/1000 | Loss: 0.00001786
Iteration 182/1000 | Loss: 0.00001786
Iteration 183/1000 | Loss: 0.00001786
Iteration 184/1000 | Loss: 0.00001786
Iteration 185/1000 | Loss: 0.00001786
Iteration 186/1000 | Loss: 0.00001786
Iteration 187/1000 | Loss: 0.00001785
Iteration 188/1000 | Loss: 0.00001785
Iteration 189/1000 | Loss: 0.00001785
Iteration 190/1000 | Loss: 0.00001785
Iteration 191/1000 | Loss: 0.00001785
Iteration 192/1000 | Loss: 0.00001785
Iteration 193/1000 | Loss: 0.00001785
Iteration 194/1000 | Loss: 0.00001785
Iteration 195/1000 | Loss: 0.00001785
Iteration 196/1000 | Loss: 0.00001785
Iteration 197/1000 | Loss: 0.00001785
Iteration 198/1000 | Loss: 0.00001785
Iteration 199/1000 | Loss: 0.00001785
Iteration 200/1000 | Loss: 0.00001785
Iteration 201/1000 | Loss: 0.00001785
Iteration 202/1000 | Loss: 0.00001785
Iteration 203/1000 | Loss: 0.00001785
Iteration 204/1000 | Loss: 0.00001785
Iteration 205/1000 | Loss: 0.00001785
Iteration 206/1000 | Loss: 0.00001785
Iteration 207/1000 | Loss: 0.00001785
Iteration 208/1000 | Loss: 0.00001785
Iteration 209/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.78490299731493e-05, 1.78490299731493e-05, 1.78490299731493e-05, 1.78490299731493e-05, 1.78490299731493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.78490299731493e-05

Optimization complete. Final v2v error: 3.5070550441741943 mm

Highest mean error: 3.86051607131958 mm for frame 47

Lowest mean error: 3.1181771755218506 mm for frame 109

Saving results

Total time: 42.74686098098755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408035
Iteration 2/25 | Loss: 0.00130018
Iteration 3/25 | Loss: 0.00125175
Iteration 4/25 | Loss: 0.00124387
Iteration 5/25 | Loss: 0.00124111
Iteration 6/25 | Loss: 0.00124111
Iteration 7/25 | Loss: 0.00124111
Iteration 8/25 | Loss: 0.00124111
Iteration 9/25 | Loss: 0.00124111
Iteration 10/25 | Loss: 0.00124111
Iteration 11/25 | Loss: 0.00124111
Iteration 12/25 | Loss: 0.00124111
Iteration 13/25 | Loss: 0.00124111
Iteration 14/25 | Loss: 0.00124111
Iteration 15/25 | Loss: 0.00124111
Iteration 16/25 | Loss: 0.00124111
Iteration 17/25 | Loss: 0.00124111
Iteration 18/25 | Loss: 0.00124111
Iteration 19/25 | Loss: 0.00124111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012411061907187104, 0.0012411061907187104, 0.0012411061907187104, 0.0012411061907187104, 0.0012411061907187104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012411061907187104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84282732
Iteration 2/25 | Loss: 0.00083436
Iteration 3/25 | Loss: 0.00083436
Iteration 4/25 | Loss: 0.00083436
Iteration 5/25 | Loss: 0.00083436
Iteration 6/25 | Loss: 0.00083436
Iteration 7/25 | Loss: 0.00083436
Iteration 8/25 | Loss: 0.00083436
Iteration 9/25 | Loss: 0.00083436
Iteration 10/25 | Loss: 0.00083436
Iteration 11/25 | Loss: 0.00083436
Iteration 12/25 | Loss: 0.00083436
Iteration 13/25 | Loss: 0.00083436
Iteration 14/25 | Loss: 0.00083436
Iteration 15/25 | Loss: 0.00083436
Iteration 16/25 | Loss: 0.00083436
Iteration 17/25 | Loss: 0.00083436
Iteration 18/25 | Loss: 0.00083436
Iteration 19/25 | Loss: 0.00083436
Iteration 20/25 | Loss: 0.00083436
Iteration 21/25 | Loss: 0.00083436
Iteration 22/25 | Loss: 0.00083436
Iteration 23/25 | Loss: 0.00083436
Iteration 24/25 | Loss: 0.00083436
Iteration 25/25 | Loss: 0.00083436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083436
Iteration 2/1000 | Loss: 0.00002370
Iteration 3/1000 | Loss: 0.00001552
Iteration 4/1000 | Loss: 0.00001401
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001183
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001177
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001169
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001168
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001161
Iteration 23/1000 | Loss: 0.00001161
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00001159
Iteration 26/1000 | Loss: 0.00001158
Iteration 27/1000 | Loss: 0.00001155
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001147
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001139
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001137
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001129
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001117
Iteration 91/1000 | Loss: 0.00001117
Iteration 92/1000 | Loss: 0.00001117
Iteration 93/1000 | Loss: 0.00001117
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001116
Iteration 98/1000 | Loss: 0.00001116
Iteration 99/1000 | Loss: 0.00001115
Iteration 100/1000 | Loss: 0.00001114
Iteration 101/1000 | Loss: 0.00001114
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001112
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001108
Iteration 118/1000 | Loss: 0.00001108
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001106
Iteration 127/1000 | Loss: 0.00001106
Iteration 128/1000 | Loss: 0.00001106
Iteration 129/1000 | Loss: 0.00001106
Iteration 130/1000 | Loss: 0.00001106
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001105
Iteration 134/1000 | Loss: 0.00001105
Iteration 135/1000 | Loss: 0.00001105
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001104
Iteration 140/1000 | Loss: 0.00001104
Iteration 141/1000 | Loss: 0.00001104
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001103
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001100
Iteration 165/1000 | Loss: 0.00001100
Iteration 166/1000 | Loss: 0.00001100
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001100
Iteration 171/1000 | Loss: 0.00001100
Iteration 172/1000 | Loss: 0.00001100
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001099
Iteration 184/1000 | Loss: 0.00001099
Iteration 185/1000 | Loss: 0.00001099
Iteration 186/1000 | Loss: 0.00001099
Iteration 187/1000 | Loss: 0.00001099
Iteration 188/1000 | Loss: 0.00001099
Iteration 189/1000 | Loss: 0.00001099
Iteration 190/1000 | Loss: 0.00001099
Iteration 191/1000 | Loss: 0.00001099
Iteration 192/1000 | Loss: 0.00001099
Iteration 193/1000 | Loss: 0.00001099
Iteration 194/1000 | Loss: 0.00001099
Iteration 195/1000 | Loss: 0.00001098
Iteration 196/1000 | Loss: 0.00001098
Iteration 197/1000 | Loss: 0.00001098
Iteration 198/1000 | Loss: 0.00001098
Iteration 199/1000 | Loss: 0.00001098
Iteration 200/1000 | Loss: 0.00001098
Iteration 201/1000 | Loss: 0.00001098
Iteration 202/1000 | Loss: 0.00001098
Iteration 203/1000 | Loss: 0.00001098
Iteration 204/1000 | Loss: 0.00001098
Iteration 205/1000 | Loss: 0.00001098
Iteration 206/1000 | Loss: 0.00001098
Iteration 207/1000 | Loss: 0.00001098
Iteration 208/1000 | Loss: 0.00001098
Iteration 209/1000 | Loss: 0.00001097
Iteration 210/1000 | Loss: 0.00001097
Iteration 211/1000 | Loss: 0.00001097
Iteration 212/1000 | Loss: 0.00001097
Iteration 213/1000 | Loss: 0.00001097
Iteration 214/1000 | Loss: 0.00001097
Iteration 215/1000 | Loss: 0.00001097
Iteration 216/1000 | Loss: 0.00001097
Iteration 217/1000 | Loss: 0.00001097
Iteration 218/1000 | Loss: 0.00001097
Iteration 219/1000 | Loss: 0.00001097
Iteration 220/1000 | Loss: 0.00001097
Iteration 221/1000 | Loss: 0.00001097
Iteration 222/1000 | Loss: 0.00001097
Iteration 223/1000 | Loss: 0.00001097
Iteration 224/1000 | Loss: 0.00001097
Iteration 225/1000 | Loss: 0.00001097
Iteration 226/1000 | Loss: 0.00001097
Iteration 227/1000 | Loss: 0.00001097
Iteration 228/1000 | Loss: 0.00001096
Iteration 229/1000 | Loss: 0.00001096
Iteration 230/1000 | Loss: 0.00001096
Iteration 231/1000 | Loss: 0.00001096
Iteration 232/1000 | Loss: 0.00001096
Iteration 233/1000 | Loss: 0.00001096
Iteration 234/1000 | Loss: 0.00001096
Iteration 235/1000 | Loss: 0.00001096
Iteration 236/1000 | Loss: 0.00001096
Iteration 237/1000 | Loss: 0.00001096
Iteration 238/1000 | Loss: 0.00001096
Iteration 239/1000 | Loss: 0.00001096
Iteration 240/1000 | Loss: 0.00001096
Iteration 241/1000 | Loss: 0.00001096
Iteration 242/1000 | Loss: 0.00001096
Iteration 243/1000 | Loss: 0.00001096
Iteration 244/1000 | Loss: 0.00001096
Iteration 245/1000 | Loss: 0.00001096
Iteration 246/1000 | Loss: 0.00001096
Iteration 247/1000 | Loss: 0.00001096
Iteration 248/1000 | Loss: 0.00001096
Iteration 249/1000 | Loss: 0.00001096
Iteration 250/1000 | Loss: 0.00001096
Iteration 251/1000 | Loss: 0.00001096
Iteration 252/1000 | Loss: 0.00001096
Iteration 253/1000 | Loss: 0.00001096
Iteration 254/1000 | Loss: 0.00001096
Iteration 255/1000 | Loss: 0.00001096
Iteration 256/1000 | Loss: 0.00001096
Iteration 257/1000 | Loss: 0.00001096
Iteration 258/1000 | Loss: 0.00001096
Iteration 259/1000 | Loss: 0.00001096
Iteration 260/1000 | Loss: 0.00001096
Iteration 261/1000 | Loss: 0.00001096
Iteration 262/1000 | Loss: 0.00001096
Iteration 263/1000 | Loss: 0.00001096
Iteration 264/1000 | Loss: 0.00001096
Iteration 265/1000 | Loss: 0.00001096
Iteration 266/1000 | Loss: 0.00001096
Iteration 267/1000 | Loss: 0.00001096
Iteration 268/1000 | Loss: 0.00001096
Iteration 269/1000 | Loss: 0.00001096
Iteration 270/1000 | Loss: 0.00001096
Iteration 271/1000 | Loss: 0.00001096
Iteration 272/1000 | Loss: 0.00001096
Iteration 273/1000 | Loss: 0.00001096
Iteration 274/1000 | Loss: 0.00001096
Iteration 275/1000 | Loss: 0.00001096
Iteration 276/1000 | Loss: 0.00001096
Iteration 277/1000 | Loss: 0.00001096
Iteration 278/1000 | Loss: 0.00001096
Iteration 279/1000 | Loss: 0.00001096
Iteration 280/1000 | Loss: 0.00001096
Iteration 281/1000 | Loss: 0.00001096
Iteration 282/1000 | Loss: 0.00001096
Iteration 283/1000 | Loss: 0.00001096
Iteration 284/1000 | Loss: 0.00001096
Iteration 285/1000 | Loss: 0.00001096
Iteration 286/1000 | Loss: 0.00001096
Iteration 287/1000 | Loss: 0.00001096
Iteration 288/1000 | Loss: 0.00001096
Iteration 289/1000 | Loss: 0.00001096
Iteration 290/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.0957739505101927e-05, 1.0957739505101927e-05, 1.0957739505101927e-05, 1.0957739505101927e-05, 1.0957739505101927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0957739505101927e-05

Optimization complete. Final v2v error: 2.841912031173706 mm

Highest mean error: 3.251458168029785 mm for frame 97

Lowest mean error: 2.7477712631225586 mm for frame 207

Saving results

Total time: 48.86073040962219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509689
Iteration 2/25 | Loss: 0.00163388
Iteration 3/25 | Loss: 0.00141827
Iteration 4/25 | Loss: 0.00139240
Iteration 5/25 | Loss: 0.00138673
Iteration 6/25 | Loss: 0.00138652
Iteration 7/25 | Loss: 0.00138652
Iteration 8/25 | Loss: 0.00138652
Iteration 9/25 | Loss: 0.00138652
Iteration 10/25 | Loss: 0.00138652
Iteration 11/25 | Loss: 0.00138652
Iteration 12/25 | Loss: 0.00138652
Iteration 13/25 | Loss: 0.00138652
Iteration 14/25 | Loss: 0.00138652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013865174259990454, 0.0013865174259990454, 0.0013865174259990454, 0.0013865174259990454, 0.0013865174259990454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013865174259990454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43657231
Iteration 2/25 | Loss: 0.00097752
Iteration 3/25 | Loss: 0.00097751
Iteration 4/25 | Loss: 0.00097751
Iteration 5/25 | Loss: 0.00097751
Iteration 6/25 | Loss: 0.00097751
Iteration 7/25 | Loss: 0.00097751
Iteration 8/25 | Loss: 0.00097751
Iteration 9/25 | Loss: 0.00097751
Iteration 10/25 | Loss: 0.00097751
Iteration 11/25 | Loss: 0.00097751
Iteration 12/25 | Loss: 0.00097751
Iteration 13/25 | Loss: 0.00097751
Iteration 14/25 | Loss: 0.00097751
Iteration 15/25 | Loss: 0.00097751
Iteration 16/25 | Loss: 0.00097751
Iteration 17/25 | Loss: 0.00097751
Iteration 18/25 | Loss: 0.00097751
Iteration 19/25 | Loss: 0.00097751
Iteration 20/25 | Loss: 0.00097751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000977509655058384, 0.000977509655058384, 0.000977509655058384, 0.000977509655058384, 0.000977509655058384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000977509655058384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097751
Iteration 2/1000 | Loss: 0.00005918
Iteration 3/1000 | Loss: 0.00003756
Iteration 4/1000 | Loss: 0.00003409
Iteration 5/1000 | Loss: 0.00003254
Iteration 6/1000 | Loss: 0.00003153
Iteration 7/1000 | Loss: 0.00003083
Iteration 8/1000 | Loss: 0.00003044
Iteration 9/1000 | Loss: 0.00003007
Iteration 10/1000 | Loss: 0.00002985
Iteration 11/1000 | Loss: 0.00002965
Iteration 12/1000 | Loss: 0.00002948
Iteration 13/1000 | Loss: 0.00002936
Iteration 14/1000 | Loss: 0.00002933
Iteration 15/1000 | Loss: 0.00002926
Iteration 16/1000 | Loss: 0.00002923
Iteration 17/1000 | Loss: 0.00002923
Iteration 18/1000 | Loss: 0.00002922
Iteration 19/1000 | Loss: 0.00002919
Iteration 20/1000 | Loss: 0.00002918
Iteration 21/1000 | Loss: 0.00002917
Iteration 22/1000 | Loss: 0.00002916
Iteration 23/1000 | Loss: 0.00002915
Iteration 24/1000 | Loss: 0.00002915
Iteration 25/1000 | Loss: 0.00002915
Iteration 26/1000 | Loss: 0.00002915
Iteration 27/1000 | Loss: 0.00002915
Iteration 28/1000 | Loss: 0.00002915
Iteration 29/1000 | Loss: 0.00002915
Iteration 30/1000 | Loss: 0.00002915
Iteration 31/1000 | Loss: 0.00002915
Iteration 32/1000 | Loss: 0.00002914
Iteration 33/1000 | Loss: 0.00002914
Iteration 34/1000 | Loss: 0.00002914
Iteration 35/1000 | Loss: 0.00002914
Iteration 36/1000 | Loss: 0.00002914
Iteration 37/1000 | Loss: 0.00002914
Iteration 38/1000 | Loss: 0.00002914
Iteration 39/1000 | Loss: 0.00002914
Iteration 40/1000 | Loss: 0.00002914
Iteration 41/1000 | Loss: 0.00002914
Iteration 42/1000 | Loss: 0.00002913
Iteration 43/1000 | Loss: 0.00002913
Iteration 44/1000 | Loss: 0.00002913
Iteration 45/1000 | Loss: 0.00002913
Iteration 46/1000 | Loss: 0.00002913
Iteration 47/1000 | Loss: 0.00002913
Iteration 48/1000 | Loss: 0.00002913
Iteration 49/1000 | Loss: 0.00002913
Iteration 50/1000 | Loss: 0.00002913
Iteration 51/1000 | Loss: 0.00002913
Iteration 52/1000 | Loss: 0.00002913
Iteration 53/1000 | Loss: 0.00002913
Iteration 54/1000 | Loss: 0.00002912
Iteration 55/1000 | Loss: 0.00002912
Iteration 56/1000 | Loss: 0.00002912
Iteration 57/1000 | Loss: 0.00002912
Iteration 58/1000 | Loss: 0.00002912
Iteration 59/1000 | Loss: 0.00002911
Iteration 60/1000 | Loss: 0.00002911
Iteration 61/1000 | Loss: 0.00002911
Iteration 62/1000 | Loss: 0.00002911
Iteration 63/1000 | Loss: 0.00002911
Iteration 64/1000 | Loss: 0.00002911
Iteration 65/1000 | Loss: 0.00002911
Iteration 66/1000 | Loss: 0.00002910
Iteration 67/1000 | Loss: 0.00002910
Iteration 68/1000 | Loss: 0.00002910
Iteration 69/1000 | Loss: 0.00002910
Iteration 70/1000 | Loss: 0.00002910
Iteration 71/1000 | Loss: 0.00002910
Iteration 72/1000 | Loss: 0.00002910
Iteration 73/1000 | Loss: 0.00002910
Iteration 74/1000 | Loss: 0.00002910
Iteration 75/1000 | Loss: 0.00002910
Iteration 76/1000 | Loss: 0.00002910
Iteration 77/1000 | Loss: 0.00002910
Iteration 78/1000 | Loss: 0.00002910
Iteration 79/1000 | Loss: 0.00002909
Iteration 80/1000 | Loss: 0.00002909
Iteration 81/1000 | Loss: 0.00002909
Iteration 82/1000 | Loss: 0.00002909
Iteration 83/1000 | Loss: 0.00002909
Iteration 84/1000 | Loss: 0.00002909
Iteration 85/1000 | Loss: 0.00002909
Iteration 86/1000 | Loss: 0.00002909
Iteration 87/1000 | Loss: 0.00002909
Iteration 88/1000 | Loss: 0.00002909
Iteration 89/1000 | Loss: 0.00002909
Iteration 90/1000 | Loss: 0.00002909
Iteration 91/1000 | Loss: 0.00002909
Iteration 92/1000 | Loss: 0.00002909
Iteration 93/1000 | Loss: 0.00002909
Iteration 94/1000 | Loss: 0.00002909
Iteration 95/1000 | Loss: 0.00002909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.9090078896842897e-05, 2.9090078896842897e-05, 2.9090078896842897e-05, 2.9090078896842897e-05, 2.9090078896842897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9090078896842897e-05

Optimization complete. Final v2v error: 4.4215312004089355 mm

Highest mean error: 5.00584077835083 mm for frame 172

Lowest mean error: 3.8826370239257812 mm for frame 0

Saving results

Total time: 37.247902393341064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407540
Iteration 2/25 | Loss: 0.00131780
Iteration 3/25 | Loss: 0.00124649
Iteration 4/25 | Loss: 0.00123762
Iteration 5/25 | Loss: 0.00123658
Iteration 6/25 | Loss: 0.00123658
Iteration 7/25 | Loss: 0.00123658
Iteration 8/25 | Loss: 0.00123658
Iteration 9/25 | Loss: 0.00123658
Iteration 10/25 | Loss: 0.00123658
Iteration 11/25 | Loss: 0.00123658
Iteration 12/25 | Loss: 0.00123658
Iteration 13/25 | Loss: 0.00123658
Iteration 14/25 | Loss: 0.00123658
Iteration 15/25 | Loss: 0.00123658
Iteration 16/25 | Loss: 0.00123658
Iteration 17/25 | Loss: 0.00123658
Iteration 18/25 | Loss: 0.00123658
Iteration 19/25 | Loss: 0.00123658
Iteration 20/25 | Loss: 0.00123658
Iteration 21/25 | Loss: 0.00123658
Iteration 22/25 | Loss: 0.00123658
Iteration 23/25 | Loss: 0.00123658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012365764705464244, 0.0012365764705464244, 0.0012365764705464244, 0.0012365764705464244, 0.0012365764705464244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012365764705464244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20852089
Iteration 2/25 | Loss: 0.00078853
Iteration 3/25 | Loss: 0.00078852
Iteration 4/25 | Loss: 0.00078852
Iteration 5/25 | Loss: 0.00078852
Iteration 6/25 | Loss: 0.00078852
Iteration 7/25 | Loss: 0.00078852
Iteration 8/25 | Loss: 0.00078852
Iteration 9/25 | Loss: 0.00078852
Iteration 10/25 | Loss: 0.00078852
Iteration 11/25 | Loss: 0.00078852
Iteration 12/25 | Loss: 0.00078852
Iteration 13/25 | Loss: 0.00078852
Iteration 14/25 | Loss: 0.00078852
Iteration 15/25 | Loss: 0.00078852
Iteration 16/25 | Loss: 0.00078852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007885201484896243, 0.0007885201484896243, 0.0007885201484896243, 0.0007885201484896243, 0.0007885201484896243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007885201484896243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078852
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001643
Iteration 4/1000 | Loss: 0.00001517
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001405
Iteration 7/1000 | Loss: 0.00001375
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001308
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001303
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001284
Iteration 19/1000 | Loss: 0.00001283
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001277
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001274
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001259
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001246
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001244
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001243
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001240
Iteration 76/1000 | Loss: 0.00001240
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001239
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001235
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001224
Iteration 127/1000 | Loss: 0.00001224
Iteration 128/1000 | Loss: 0.00001223
Iteration 129/1000 | Loss: 0.00001223
Iteration 130/1000 | Loss: 0.00001223
Iteration 131/1000 | Loss: 0.00001223
Iteration 132/1000 | Loss: 0.00001222
Iteration 133/1000 | Loss: 0.00001222
Iteration 134/1000 | Loss: 0.00001221
Iteration 135/1000 | Loss: 0.00001221
Iteration 136/1000 | Loss: 0.00001221
Iteration 137/1000 | Loss: 0.00001221
Iteration 138/1000 | Loss: 0.00001221
Iteration 139/1000 | Loss: 0.00001221
Iteration 140/1000 | Loss: 0.00001220
Iteration 141/1000 | Loss: 0.00001220
Iteration 142/1000 | Loss: 0.00001220
Iteration 143/1000 | Loss: 0.00001220
Iteration 144/1000 | Loss: 0.00001219
Iteration 145/1000 | Loss: 0.00001219
Iteration 146/1000 | Loss: 0.00001219
Iteration 147/1000 | Loss: 0.00001218
Iteration 148/1000 | Loss: 0.00001218
Iteration 149/1000 | Loss: 0.00001218
Iteration 150/1000 | Loss: 0.00001218
Iteration 151/1000 | Loss: 0.00001218
Iteration 152/1000 | Loss: 0.00001218
Iteration 153/1000 | Loss: 0.00001218
Iteration 154/1000 | Loss: 0.00001218
Iteration 155/1000 | Loss: 0.00001218
Iteration 156/1000 | Loss: 0.00001218
Iteration 157/1000 | Loss: 0.00001218
Iteration 158/1000 | Loss: 0.00001218
Iteration 159/1000 | Loss: 0.00001218
Iteration 160/1000 | Loss: 0.00001218
Iteration 161/1000 | Loss: 0.00001217
Iteration 162/1000 | Loss: 0.00001217
Iteration 163/1000 | Loss: 0.00001217
Iteration 164/1000 | Loss: 0.00001217
Iteration 165/1000 | Loss: 0.00001217
Iteration 166/1000 | Loss: 0.00001217
Iteration 167/1000 | Loss: 0.00001217
Iteration 168/1000 | Loss: 0.00001217
Iteration 169/1000 | Loss: 0.00001217
Iteration 170/1000 | Loss: 0.00001217
Iteration 171/1000 | Loss: 0.00001217
Iteration 172/1000 | Loss: 0.00001217
Iteration 173/1000 | Loss: 0.00001217
Iteration 174/1000 | Loss: 0.00001217
Iteration 175/1000 | Loss: 0.00001217
Iteration 176/1000 | Loss: 0.00001217
Iteration 177/1000 | Loss: 0.00001217
Iteration 178/1000 | Loss: 0.00001217
Iteration 179/1000 | Loss: 0.00001217
Iteration 180/1000 | Loss: 0.00001217
Iteration 181/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.2167411114205606e-05, 1.2167411114205606e-05, 1.2167411114205606e-05, 1.2167411114205606e-05, 1.2167411114205606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2167411114205606e-05

Optimization complete. Final v2v error: 2.984363317489624 mm

Highest mean error: 3.4279251098632812 mm for frame 209

Lowest mean error: 2.866713285446167 mm for frame 32

Saving results

Total time: 44.41989207267761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339478
Iteration 2/25 | Loss: 0.00138664
Iteration 3/25 | Loss: 0.00126577
Iteration 4/25 | Loss: 0.00125274
Iteration 5/25 | Loss: 0.00124807
Iteration 6/25 | Loss: 0.00124732
Iteration 7/25 | Loss: 0.00124732
Iteration 8/25 | Loss: 0.00124732
Iteration 9/25 | Loss: 0.00124732
Iteration 10/25 | Loss: 0.00124732
Iteration 11/25 | Loss: 0.00124732
Iteration 12/25 | Loss: 0.00124732
Iteration 13/25 | Loss: 0.00124732
Iteration 14/25 | Loss: 0.00124732
Iteration 15/25 | Loss: 0.00124732
Iteration 16/25 | Loss: 0.00124732
Iteration 17/25 | Loss: 0.00124732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012473235838115215, 0.0012473235838115215, 0.0012473235838115215, 0.0012473235838115215, 0.0012473235838115215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012473235838115215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40831125
Iteration 2/25 | Loss: 0.00081101
Iteration 3/25 | Loss: 0.00081100
Iteration 4/25 | Loss: 0.00081100
Iteration 5/25 | Loss: 0.00081100
Iteration 6/25 | Loss: 0.00081100
Iteration 7/25 | Loss: 0.00081100
Iteration 8/25 | Loss: 0.00081100
Iteration 9/25 | Loss: 0.00081100
Iteration 10/25 | Loss: 0.00081100
Iteration 11/25 | Loss: 0.00081100
Iteration 12/25 | Loss: 0.00081100
Iteration 13/25 | Loss: 0.00081100
Iteration 14/25 | Loss: 0.00081100
Iteration 15/25 | Loss: 0.00081100
Iteration 16/25 | Loss: 0.00081100
Iteration 17/25 | Loss: 0.00081100
Iteration 18/25 | Loss: 0.00081100
Iteration 19/25 | Loss: 0.00081100
Iteration 20/25 | Loss: 0.00081100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008109952323138714, 0.0008109952323138714, 0.0008109952323138714, 0.0008109952323138714, 0.0008109952323138714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008109952323138714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081100
Iteration 2/1000 | Loss: 0.00003826
Iteration 3/1000 | Loss: 0.00002703
Iteration 4/1000 | Loss: 0.00002151
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001843
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001703
Iteration 9/1000 | Loss: 0.00001664
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001595
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001560
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001554
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001551
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001540
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001532
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001529
Iteration 46/1000 | Loss: 0.00001529
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001524
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001524
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001516
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001516
Iteration 87/1000 | Loss: 0.00001516
Iteration 88/1000 | Loss: 0.00001516
Iteration 89/1000 | Loss: 0.00001515
Iteration 90/1000 | Loss: 0.00001515
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001515
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001514
Iteration 99/1000 | Loss: 0.00001514
Iteration 100/1000 | Loss: 0.00001514
Iteration 101/1000 | Loss: 0.00001514
Iteration 102/1000 | Loss: 0.00001514
Iteration 103/1000 | Loss: 0.00001513
Iteration 104/1000 | Loss: 0.00001513
Iteration 105/1000 | Loss: 0.00001513
Iteration 106/1000 | Loss: 0.00001513
Iteration 107/1000 | Loss: 0.00001513
Iteration 108/1000 | Loss: 0.00001513
Iteration 109/1000 | Loss: 0.00001513
Iteration 110/1000 | Loss: 0.00001513
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001512
Iteration 117/1000 | Loss: 0.00001512
Iteration 118/1000 | Loss: 0.00001512
Iteration 119/1000 | Loss: 0.00001512
Iteration 120/1000 | Loss: 0.00001512
Iteration 121/1000 | Loss: 0.00001512
Iteration 122/1000 | Loss: 0.00001512
Iteration 123/1000 | Loss: 0.00001512
Iteration 124/1000 | Loss: 0.00001512
Iteration 125/1000 | Loss: 0.00001512
Iteration 126/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.5123127013794146e-05, 1.5123127013794146e-05, 1.5123127013794146e-05, 1.5123127013794146e-05, 1.5123127013794146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5123127013794146e-05

Optimization complete. Final v2v error: 3.301590919494629 mm

Highest mean error: 3.903738021850586 mm for frame 93

Lowest mean error: 2.921239137649536 mm for frame 264

Saving results

Total time: 45.13501000404358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491412
Iteration 2/25 | Loss: 0.00140902
Iteration 3/25 | Loss: 0.00132866
Iteration 4/25 | Loss: 0.00131285
Iteration 5/25 | Loss: 0.00130604
Iteration 6/25 | Loss: 0.00130604
Iteration 7/25 | Loss: 0.00130604
Iteration 8/25 | Loss: 0.00130604
Iteration 9/25 | Loss: 0.00130604
Iteration 10/25 | Loss: 0.00130604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013060352066531777, 0.0013060352066531777, 0.0013060352066531777, 0.0013060352066531777, 0.0013060352066531777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013060352066531777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83134204
Iteration 2/25 | Loss: 0.00083163
Iteration 3/25 | Loss: 0.00083163
Iteration 4/25 | Loss: 0.00083163
Iteration 5/25 | Loss: 0.00083163
Iteration 6/25 | Loss: 0.00083162
Iteration 7/25 | Loss: 0.00083162
Iteration 8/25 | Loss: 0.00083162
Iteration 9/25 | Loss: 0.00083162
Iteration 10/25 | Loss: 0.00083162
Iteration 11/25 | Loss: 0.00083162
Iteration 12/25 | Loss: 0.00083162
Iteration 13/25 | Loss: 0.00083162
Iteration 14/25 | Loss: 0.00083162
Iteration 15/25 | Loss: 0.00083162
Iteration 16/25 | Loss: 0.00083162
Iteration 17/25 | Loss: 0.00083162
Iteration 18/25 | Loss: 0.00083162
Iteration 19/25 | Loss: 0.00083162
Iteration 20/25 | Loss: 0.00083162
Iteration 21/25 | Loss: 0.00083162
Iteration 22/25 | Loss: 0.00083162
Iteration 23/25 | Loss: 0.00083162
Iteration 24/25 | Loss: 0.00083162
Iteration 25/25 | Loss: 0.00083162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083162
Iteration 2/1000 | Loss: 0.00004692
Iteration 3/1000 | Loss: 0.00003107
Iteration 4/1000 | Loss: 0.00002821
Iteration 5/1000 | Loss: 0.00002636
Iteration 6/1000 | Loss: 0.00002497
Iteration 7/1000 | Loss: 0.00002390
Iteration 8/1000 | Loss: 0.00002336
Iteration 9/1000 | Loss: 0.00002282
Iteration 10/1000 | Loss: 0.00002239
Iteration 11/1000 | Loss: 0.00002209
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002152
Iteration 14/1000 | Loss: 0.00002137
Iteration 15/1000 | Loss: 0.00002118
Iteration 16/1000 | Loss: 0.00002099
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002081
Iteration 22/1000 | Loss: 0.00002069
Iteration 23/1000 | Loss: 0.00002065
Iteration 24/1000 | Loss: 0.00002055
Iteration 25/1000 | Loss: 0.00002050
Iteration 26/1000 | Loss: 0.00002048
Iteration 27/1000 | Loss: 0.00002047
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002045
Iteration 31/1000 | Loss: 0.00002045
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002045
Iteration 35/1000 | Loss: 0.00002045
Iteration 36/1000 | Loss: 0.00002045
Iteration 37/1000 | Loss: 0.00002045
Iteration 38/1000 | Loss: 0.00002045
Iteration 39/1000 | Loss: 0.00002045
Iteration 40/1000 | Loss: 0.00002045
Iteration 41/1000 | Loss: 0.00002045
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002044
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002044
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002042
Iteration 53/1000 | Loss: 0.00002042
Iteration 54/1000 | Loss: 0.00002042
Iteration 55/1000 | Loss: 0.00002042
Iteration 56/1000 | Loss: 0.00002041
Iteration 57/1000 | Loss: 0.00002040
Iteration 58/1000 | Loss: 0.00002039
Iteration 59/1000 | Loss: 0.00002039
Iteration 60/1000 | Loss: 0.00002039
Iteration 61/1000 | Loss: 0.00002039
Iteration 62/1000 | Loss: 0.00002039
Iteration 63/1000 | Loss: 0.00002039
Iteration 64/1000 | Loss: 0.00002038
Iteration 65/1000 | Loss: 0.00002038
Iteration 66/1000 | Loss: 0.00002038
Iteration 67/1000 | Loss: 0.00002038
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002038
Iteration 71/1000 | Loss: 0.00002037
Iteration 72/1000 | Loss: 0.00002037
Iteration 73/1000 | Loss: 0.00002037
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002036
Iteration 79/1000 | Loss: 0.00002036
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002035
Iteration 83/1000 | Loss: 0.00002035
Iteration 84/1000 | Loss: 0.00002034
Iteration 85/1000 | Loss: 0.00002033
Iteration 86/1000 | Loss: 0.00002032
Iteration 87/1000 | Loss: 0.00002031
Iteration 88/1000 | Loss: 0.00002031
Iteration 89/1000 | Loss: 0.00002031
Iteration 90/1000 | Loss: 0.00002031
Iteration 91/1000 | Loss: 0.00002031
Iteration 92/1000 | Loss: 0.00002031
Iteration 93/1000 | Loss: 0.00002031
Iteration 94/1000 | Loss: 0.00002031
Iteration 95/1000 | Loss: 0.00002031
Iteration 96/1000 | Loss: 0.00002031
Iteration 97/1000 | Loss: 0.00002031
Iteration 98/1000 | Loss: 0.00002030
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002030
Iteration 101/1000 | Loss: 0.00002029
Iteration 102/1000 | Loss: 0.00002029
Iteration 103/1000 | Loss: 0.00002029
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002029
Iteration 109/1000 | Loss: 0.00002029
Iteration 110/1000 | Loss: 0.00002029
Iteration 111/1000 | Loss: 0.00002028
Iteration 112/1000 | Loss: 0.00002028
Iteration 113/1000 | Loss: 0.00002028
Iteration 114/1000 | Loss: 0.00002028
Iteration 115/1000 | Loss: 0.00002028
Iteration 116/1000 | Loss: 0.00002028
Iteration 117/1000 | Loss: 0.00002028
Iteration 118/1000 | Loss: 0.00002028
Iteration 119/1000 | Loss: 0.00002028
Iteration 120/1000 | Loss: 0.00002028
Iteration 121/1000 | Loss: 0.00002027
Iteration 122/1000 | Loss: 0.00002026
Iteration 123/1000 | Loss: 0.00002026
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002025
Iteration 127/1000 | Loss: 0.00002025
Iteration 128/1000 | Loss: 0.00002025
Iteration 129/1000 | Loss: 0.00002025
Iteration 130/1000 | Loss: 0.00002025
Iteration 131/1000 | Loss: 0.00002025
Iteration 132/1000 | Loss: 0.00002024
Iteration 133/1000 | Loss: 0.00002023
Iteration 134/1000 | Loss: 0.00002023
Iteration 135/1000 | Loss: 0.00002023
Iteration 136/1000 | Loss: 0.00002023
Iteration 137/1000 | Loss: 0.00002022
Iteration 138/1000 | Loss: 0.00002022
Iteration 139/1000 | Loss: 0.00002022
Iteration 140/1000 | Loss: 0.00002022
Iteration 141/1000 | Loss: 0.00002022
Iteration 142/1000 | Loss: 0.00002021
Iteration 143/1000 | Loss: 0.00002021
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00002021
Iteration 146/1000 | Loss: 0.00002021
Iteration 147/1000 | Loss: 0.00002021
Iteration 148/1000 | Loss: 0.00002021
Iteration 149/1000 | Loss: 0.00002021
Iteration 150/1000 | Loss: 0.00002021
Iteration 151/1000 | Loss: 0.00002020
Iteration 152/1000 | Loss: 0.00002020
Iteration 153/1000 | Loss: 0.00002020
Iteration 154/1000 | Loss: 0.00002020
Iteration 155/1000 | Loss: 0.00002020
Iteration 156/1000 | Loss: 0.00002020
Iteration 157/1000 | Loss: 0.00002020
Iteration 158/1000 | Loss: 0.00002020
Iteration 159/1000 | Loss: 0.00002019
Iteration 160/1000 | Loss: 0.00002019
Iteration 161/1000 | Loss: 0.00002019
Iteration 162/1000 | Loss: 0.00002019
Iteration 163/1000 | Loss: 0.00002019
Iteration 164/1000 | Loss: 0.00002018
Iteration 165/1000 | Loss: 0.00002018
Iteration 166/1000 | Loss: 0.00002018
Iteration 167/1000 | Loss: 0.00002018
Iteration 168/1000 | Loss: 0.00002018
Iteration 169/1000 | Loss: 0.00002018
Iteration 170/1000 | Loss: 0.00002018
Iteration 171/1000 | Loss: 0.00002017
Iteration 172/1000 | Loss: 0.00002017
Iteration 173/1000 | Loss: 0.00002017
Iteration 174/1000 | Loss: 0.00002017
Iteration 175/1000 | Loss: 0.00002017
Iteration 176/1000 | Loss: 0.00002017
Iteration 177/1000 | Loss: 0.00002016
Iteration 178/1000 | Loss: 0.00002016
Iteration 179/1000 | Loss: 0.00002016
Iteration 180/1000 | Loss: 0.00002016
Iteration 181/1000 | Loss: 0.00002016
Iteration 182/1000 | Loss: 0.00002016
Iteration 183/1000 | Loss: 0.00002016
Iteration 184/1000 | Loss: 0.00002016
Iteration 185/1000 | Loss: 0.00002016
Iteration 186/1000 | Loss: 0.00002016
Iteration 187/1000 | Loss: 0.00002016
Iteration 188/1000 | Loss: 0.00002016
Iteration 189/1000 | Loss: 0.00002016
Iteration 190/1000 | Loss: 0.00002016
Iteration 191/1000 | Loss: 0.00002016
Iteration 192/1000 | Loss: 0.00002015
Iteration 193/1000 | Loss: 0.00002015
Iteration 194/1000 | Loss: 0.00002015
Iteration 195/1000 | Loss: 0.00002015
Iteration 196/1000 | Loss: 0.00002015
Iteration 197/1000 | Loss: 0.00002015
Iteration 198/1000 | Loss: 0.00002015
Iteration 199/1000 | Loss: 0.00002015
Iteration 200/1000 | Loss: 0.00002015
Iteration 201/1000 | Loss: 0.00002015
Iteration 202/1000 | Loss: 0.00002014
Iteration 203/1000 | Loss: 0.00002014
Iteration 204/1000 | Loss: 0.00002014
Iteration 205/1000 | Loss: 0.00002014
Iteration 206/1000 | Loss: 0.00002013
Iteration 207/1000 | Loss: 0.00002013
Iteration 208/1000 | Loss: 0.00002013
Iteration 209/1000 | Loss: 0.00002012
Iteration 210/1000 | Loss: 0.00002012
Iteration 211/1000 | Loss: 0.00002012
Iteration 212/1000 | Loss: 0.00002012
Iteration 213/1000 | Loss: 0.00002012
Iteration 214/1000 | Loss: 0.00002012
Iteration 215/1000 | Loss: 0.00002012
Iteration 216/1000 | Loss: 0.00002011
Iteration 217/1000 | Loss: 0.00002011
Iteration 218/1000 | Loss: 0.00002011
Iteration 219/1000 | Loss: 0.00002011
Iteration 220/1000 | Loss: 0.00002011
Iteration 221/1000 | Loss: 0.00002011
Iteration 222/1000 | Loss: 0.00002011
Iteration 223/1000 | Loss: 0.00002011
Iteration 224/1000 | Loss: 0.00002011
Iteration 225/1000 | Loss: 0.00002011
Iteration 226/1000 | Loss: 0.00002011
Iteration 227/1000 | Loss: 0.00002011
Iteration 228/1000 | Loss: 0.00002011
Iteration 229/1000 | Loss: 0.00002011
Iteration 230/1000 | Loss: 0.00002011
Iteration 231/1000 | Loss: 0.00002011
Iteration 232/1000 | Loss: 0.00002010
Iteration 233/1000 | Loss: 0.00002010
Iteration 234/1000 | Loss: 0.00002010
Iteration 235/1000 | Loss: 0.00002010
Iteration 236/1000 | Loss: 0.00002010
Iteration 237/1000 | Loss: 0.00002010
Iteration 238/1000 | Loss: 0.00002010
Iteration 239/1000 | Loss: 0.00002010
Iteration 240/1000 | Loss: 0.00002010
Iteration 241/1000 | Loss: 0.00002010
Iteration 242/1000 | Loss: 0.00002010
Iteration 243/1000 | Loss: 0.00002010
Iteration 244/1000 | Loss: 0.00002010
Iteration 245/1000 | Loss: 0.00002010
Iteration 246/1000 | Loss: 0.00002010
Iteration 247/1000 | Loss: 0.00002009
Iteration 248/1000 | Loss: 0.00002009
Iteration 249/1000 | Loss: 0.00002009
Iteration 250/1000 | Loss: 0.00002009
Iteration 251/1000 | Loss: 0.00002009
Iteration 252/1000 | Loss: 0.00002009
Iteration 253/1000 | Loss: 0.00002009
Iteration 254/1000 | Loss: 0.00002009
Iteration 255/1000 | Loss: 0.00002009
Iteration 256/1000 | Loss: 0.00002009
Iteration 257/1000 | Loss: 0.00002008
Iteration 258/1000 | Loss: 0.00002008
Iteration 259/1000 | Loss: 0.00002008
Iteration 260/1000 | Loss: 0.00002008
Iteration 261/1000 | Loss: 0.00002008
Iteration 262/1000 | Loss: 0.00002008
Iteration 263/1000 | Loss: 0.00002008
Iteration 264/1000 | Loss: 0.00002008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.008290175581351e-05, 2.008290175581351e-05, 2.008290175581351e-05, 2.008290175581351e-05, 2.008290175581351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.008290175581351e-05

Optimization complete. Final v2v error: 3.7970564365386963 mm

Highest mean error: 4.476118087768555 mm for frame 1

Lowest mean error: 3.6046013832092285 mm for frame 51

Saving results

Total time: 60.71998429298401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790611
Iteration 2/25 | Loss: 0.00135376
Iteration 3/25 | Loss: 0.00127084
Iteration 4/25 | Loss: 0.00125865
Iteration 5/25 | Loss: 0.00125605
Iteration 6/25 | Loss: 0.00125605
Iteration 7/25 | Loss: 0.00125605
Iteration 8/25 | Loss: 0.00125605
Iteration 9/25 | Loss: 0.00125605
Iteration 10/25 | Loss: 0.00125605
Iteration 11/25 | Loss: 0.00125605
Iteration 12/25 | Loss: 0.00125605
Iteration 13/25 | Loss: 0.00125605
Iteration 14/25 | Loss: 0.00125605
Iteration 15/25 | Loss: 0.00125605
Iteration 16/25 | Loss: 0.00125605
Iteration 17/25 | Loss: 0.00125605
Iteration 18/25 | Loss: 0.00125605
Iteration 19/25 | Loss: 0.00125605
Iteration 20/25 | Loss: 0.00125605
Iteration 21/25 | Loss: 0.00125605
Iteration 22/25 | Loss: 0.00125605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012560455361381173, 0.0012560455361381173, 0.0012560455361381173, 0.0012560455361381173, 0.0012560455361381173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012560455361381173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46924472
Iteration 2/25 | Loss: 0.00079207
Iteration 3/25 | Loss: 0.00079206
Iteration 4/25 | Loss: 0.00079206
Iteration 5/25 | Loss: 0.00079206
Iteration 6/25 | Loss: 0.00079206
Iteration 7/25 | Loss: 0.00079206
Iteration 8/25 | Loss: 0.00079206
Iteration 9/25 | Loss: 0.00079206
Iteration 10/25 | Loss: 0.00079206
Iteration 11/25 | Loss: 0.00079206
Iteration 12/25 | Loss: 0.00079206
Iteration 13/25 | Loss: 0.00079206
Iteration 14/25 | Loss: 0.00079206
Iteration 15/25 | Loss: 0.00079206
Iteration 16/25 | Loss: 0.00079206
Iteration 17/25 | Loss: 0.00079206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007920599891804159, 0.0007920599891804159, 0.0007920599891804159, 0.0007920599891804159, 0.0007920599891804159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007920599891804159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079206
Iteration 2/1000 | Loss: 0.00003231
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001895
Iteration 6/1000 | Loss: 0.00001806
Iteration 7/1000 | Loss: 0.00001743
Iteration 8/1000 | Loss: 0.00001710
Iteration 9/1000 | Loss: 0.00001677
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001630
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001591
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001586
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001576
Iteration 46/1000 | Loss: 0.00001575
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001574
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001573
Iteration 53/1000 | Loss: 0.00001573
Iteration 54/1000 | Loss: 0.00001573
Iteration 55/1000 | Loss: 0.00001572
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001572
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001569
Iteration 73/1000 | Loss: 0.00001569
Iteration 74/1000 | Loss: 0.00001568
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001556
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001555
Iteration 118/1000 | Loss: 0.00001555
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001552
Iteration 136/1000 | Loss: 0.00001552
Iteration 137/1000 | Loss: 0.00001552
Iteration 138/1000 | Loss: 0.00001552
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001552
Iteration 142/1000 | Loss: 0.00001552
Iteration 143/1000 | Loss: 0.00001552
Iteration 144/1000 | Loss: 0.00001552
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001552
Iteration 147/1000 | Loss: 0.00001552
Iteration 148/1000 | Loss: 0.00001552
Iteration 149/1000 | Loss: 0.00001552
Iteration 150/1000 | Loss: 0.00001552
Iteration 151/1000 | Loss: 0.00001552
Iteration 152/1000 | Loss: 0.00001552
Iteration 153/1000 | Loss: 0.00001552
Iteration 154/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.5520094166276976e-05, 1.5520094166276976e-05, 1.5520094166276976e-05, 1.5520094166276976e-05, 1.5520094166276976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5520094166276976e-05

Optimization complete. Final v2v error: 3.3920090198516846 mm

Highest mean error: 3.6812713146209717 mm for frame 155

Lowest mean error: 3.2070248126983643 mm for frame 65

Saving results

Total time: 44.50387692451477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767243
Iteration 2/25 | Loss: 0.00148828
Iteration 3/25 | Loss: 0.00130095
Iteration 4/25 | Loss: 0.00129224
Iteration 5/25 | Loss: 0.00129012
Iteration 6/25 | Loss: 0.00129012
Iteration 7/25 | Loss: 0.00129012
Iteration 8/25 | Loss: 0.00129012
Iteration 9/25 | Loss: 0.00129012
Iteration 10/25 | Loss: 0.00129012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012901165755465627, 0.0012901165755465627, 0.0012901165755465627, 0.0012901165755465627, 0.0012901165755465627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012901165755465627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40084326
Iteration 2/25 | Loss: 0.00060362
Iteration 3/25 | Loss: 0.00060361
Iteration 4/25 | Loss: 0.00060361
Iteration 5/25 | Loss: 0.00060361
Iteration 6/25 | Loss: 0.00060361
Iteration 7/25 | Loss: 0.00060361
Iteration 8/25 | Loss: 0.00060361
Iteration 9/25 | Loss: 0.00060361
Iteration 10/25 | Loss: 0.00060361
Iteration 11/25 | Loss: 0.00060361
Iteration 12/25 | Loss: 0.00060361
Iteration 13/25 | Loss: 0.00060361
Iteration 14/25 | Loss: 0.00060361
Iteration 15/25 | Loss: 0.00060361
Iteration 16/25 | Loss: 0.00060361
Iteration 17/25 | Loss: 0.00060361
Iteration 18/25 | Loss: 0.00060361
Iteration 19/25 | Loss: 0.00060361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006036096601746976, 0.0006036096601746976, 0.0006036096601746976, 0.0006036096601746976, 0.0006036096601746976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006036096601746976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060361
Iteration 2/1000 | Loss: 0.00003093
Iteration 3/1000 | Loss: 0.00002390
Iteration 4/1000 | Loss: 0.00002193
Iteration 5/1000 | Loss: 0.00002055
Iteration 6/1000 | Loss: 0.00001959
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001724
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001694
Iteration 15/1000 | Loss: 0.00001691
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001680
Iteration 18/1000 | Loss: 0.00001679
Iteration 19/1000 | Loss: 0.00001674
Iteration 20/1000 | Loss: 0.00001674
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001672
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001670
Iteration 26/1000 | Loss: 0.00001669
Iteration 27/1000 | Loss: 0.00001666
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001655
Iteration 31/1000 | Loss: 0.00001655
Iteration 32/1000 | Loss: 0.00001655
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001655
Iteration 36/1000 | Loss: 0.00001650
Iteration 37/1000 | Loss: 0.00001650
Iteration 38/1000 | Loss: 0.00001649
Iteration 39/1000 | Loss: 0.00001649
Iteration 40/1000 | Loss: 0.00001648
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001647
Iteration 44/1000 | Loss: 0.00001647
Iteration 45/1000 | Loss: 0.00001647
Iteration 46/1000 | Loss: 0.00001647
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001647
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001647
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001646
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001646
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001639
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001635
Iteration 76/1000 | Loss: 0.00001635
Iteration 77/1000 | Loss: 0.00001635
Iteration 78/1000 | Loss: 0.00001635
Iteration 79/1000 | Loss: 0.00001634
Iteration 80/1000 | Loss: 0.00001634
Iteration 81/1000 | Loss: 0.00001634
Iteration 82/1000 | Loss: 0.00001633
Iteration 83/1000 | Loss: 0.00001633
Iteration 84/1000 | Loss: 0.00001633
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001632
Iteration 87/1000 | Loss: 0.00001632
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001631
Iteration 90/1000 | Loss: 0.00001631
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001631
Iteration 94/1000 | Loss: 0.00001630
Iteration 95/1000 | Loss: 0.00001630
Iteration 96/1000 | Loss: 0.00001630
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001629
Iteration 101/1000 | Loss: 0.00001629
Iteration 102/1000 | Loss: 0.00001628
Iteration 103/1000 | Loss: 0.00001628
Iteration 104/1000 | Loss: 0.00001628
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001628
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.628455902391579e-05, 1.628455902391579e-05, 1.628455902391579e-05, 1.628455902391579e-05, 1.628455902391579e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.628455902391579e-05

Optimization complete. Final v2v error: 3.4045517444610596 mm

Highest mean error: 3.7672414779663086 mm for frame 9

Lowest mean error: 3.28940749168396 mm for frame 122

Saving results

Total time: 42.412025451660156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772145
Iteration 2/25 | Loss: 0.00188522
Iteration 3/25 | Loss: 0.00138407
Iteration 4/25 | Loss: 0.00131545
Iteration 5/25 | Loss: 0.00131042
Iteration 6/25 | Loss: 0.00128600
Iteration 7/25 | Loss: 0.00126519
Iteration 8/25 | Loss: 0.00125397
Iteration 9/25 | Loss: 0.00124771
Iteration 10/25 | Loss: 0.00125728
Iteration 11/25 | Loss: 0.00124422
Iteration 12/25 | Loss: 0.00124248
Iteration 13/25 | Loss: 0.00124228
Iteration 14/25 | Loss: 0.00124225
Iteration 15/25 | Loss: 0.00124225
Iteration 16/25 | Loss: 0.00124225
Iteration 17/25 | Loss: 0.00124225
Iteration 18/25 | Loss: 0.00124224
Iteration 19/25 | Loss: 0.00124224
Iteration 20/25 | Loss: 0.00124224
Iteration 21/25 | Loss: 0.00124224
Iteration 22/25 | Loss: 0.00124224
Iteration 23/25 | Loss: 0.00124224
Iteration 24/25 | Loss: 0.00124224
Iteration 25/25 | Loss: 0.00124224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47866416
Iteration 2/25 | Loss: 0.00091209
Iteration 3/25 | Loss: 0.00091209
Iteration 4/25 | Loss: 0.00091209
Iteration 5/25 | Loss: 0.00091209
Iteration 6/25 | Loss: 0.00091209
Iteration 7/25 | Loss: 0.00091209
Iteration 8/25 | Loss: 0.00091209
Iteration 9/25 | Loss: 0.00091209
Iteration 10/25 | Loss: 0.00091209
Iteration 11/25 | Loss: 0.00091208
Iteration 12/25 | Loss: 0.00091208
Iteration 13/25 | Loss: 0.00091208
Iteration 14/25 | Loss: 0.00091208
Iteration 15/25 | Loss: 0.00091208
Iteration 16/25 | Loss: 0.00091208
Iteration 17/25 | Loss: 0.00091208
Iteration 18/25 | Loss: 0.00091208
Iteration 19/25 | Loss: 0.00091208
Iteration 20/25 | Loss: 0.00091208
Iteration 21/25 | Loss: 0.00091208
Iteration 22/25 | Loss: 0.00091208
Iteration 23/25 | Loss: 0.00091208
Iteration 24/25 | Loss: 0.00091208
Iteration 25/25 | Loss: 0.00091208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091208
Iteration 2/1000 | Loss: 0.00007498
Iteration 3/1000 | Loss: 0.00006647
Iteration 4/1000 | Loss: 0.00006092
Iteration 5/1000 | Loss: 0.00011416
Iteration 6/1000 | Loss: 0.00066634
Iteration 7/1000 | Loss: 0.00002085
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001395
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00011712
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00005181
Iteration 16/1000 | Loss: 0.00012251
Iteration 17/1000 | Loss: 0.00001769
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00007204
Iteration 25/1000 | Loss: 0.00006397
Iteration 26/1000 | Loss: 0.00003548
Iteration 27/1000 | Loss: 0.00031897
Iteration 28/1000 | Loss: 0.00017556
Iteration 29/1000 | Loss: 0.00006973
Iteration 30/1000 | Loss: 0.00027722
Iteration 31/1000 | Loss: 0.00002367
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001234
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001226
Iteration 46/1000 | Loss: 0.00001226
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001225
Iteration 49/1000 | Loss: 0.00001225
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001222
Iteration 53/1000 | Loss: 0.00001222
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001221
Iteration 61/1000 | Loss: 0.00001221
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001218
Iteration 71/1000 | Loss: 0.00001218
Iteration 72/1000 | Loss: 0.00001217
Iteration 73/1000 | Loss: 0.00001217
Iteration 74/1000 | Loss: 0.00001217
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001216
Iteration 81/1000 | Loss: 0.00001216
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001214
Iteration 95/1000 | Loss: 0.00001214
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001213
Iteration 101/1000 | Loss: 0.00001213
Iteration 102/1000 | Loss: 0.00001213
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00001212
Iteration 106/1000 | Loss: 0.00006580
Iteration 107/1000 | Loss: 0.00021620
Iteration 108/1000 | Loss: 0.00002275
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00004544
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001213
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00005950
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001199
Iteration 127/1000 | Loss: 0.00001199
Iteration 128/1000 | Loss: 0.00001199
Iteration 129/1000 | Loss: 0.00001199
Iteration 130/1000 | Loss: 0.00001198
Iteration 131/1000 | Loss: 0.00001198
Iteration 132/1000 | Loss: 0.00001198
Iteration 133/1000 | Loss: 0.00001198
Iteration 134/1000 | Loss: 0.00005678
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001197
Iteration 144/1000 | Loss: 0.00001197
Iteration 145/1000 | Loss: 0.00001197
Iteration 146/1000 | Loss: 0.00001197
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001197
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001196
Iteration 157/1000 | Loss: 0.00001196
Iteration 158/1000 | Loss: 0.00003315
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00002350
Iteration 161/1000 | Loss: 0.00013203
Iteration 162/1000 | Loss: 0.00003167
Iteration 163/1000 | Loss: 0.00001533
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001198
Iteration 166/1000 | Loss: 0.00001198
Iteration 167/1000 | Loss: 0.00001198
Iteration 168/1000 | Loss: 0.00001198
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001197
Iteration 173/1000 | Loss: 0.00001197
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001197
Iteration 177/1000 | Loss: 0.00001197
Iteration 178/1000 | Loss: 0.00001197
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001197
Iteration 181/1000 | Loss: 0.00001197
Iteration 182/1000 | Loss: 0.00001197
Iteration 183/1000 | Loss: 0.00001197
Iteration 184/1000 | Loss: 0.00001197
Iteration 185/1000 | Loss: 0.00001197
Iteration 186/1000 | Loss: 0.00001197
Iteration 187/1000 | Loss: 0.00001197
Iteration 188/1000 | Loss: 0.00001197
Iteration 189/1000 | Loss: 0.00001197
Iteration 190/1000 | Loss: 0.00001197
Iteration 191/1000 | Loss: 0.00001197
Iteration 192/1000 | Loss: 0.00001197
Iteration 193/1000 | Loss: 0.00001197
Iteration 194/1000 | Loss: 0.00001197
Iteration 195/1000 | Loss: 0.00001197
Iteration 196/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.1972054380748887e-05, 1.1972054380748887e-05, 1.1972054380748887e-05, 1.1972054380748887e-05, 1.1972054380748887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1972054380748887e-05

Optimization complete. Final v2v error: 2.9807870388031006 mm

Highest mean error: 3.859527349472046 mm for frame 152

Lowest mean error: 2.801192045211792 mm for frame 187

Saving results

Total time: 98.70560956001282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825679
Iteration 2/25 | Loss: 0.00148853
Iteration 3/25 | Loss: 0.00133045
Iteration 4/25 | Loss: 0.00130038
Iteration 5/25 | Loss: 0.00129296
Iteration 6/25 | Loss: 0.00129182
Iteration 7/25 | Loss: 0.00129173
Iteration 8/25 | Loss: 0.00129173
Iteration 9/25 | Loss: 0.00129173
Iteration 10/25 | Loss: 0.00129173
Iteration 11/25 | Loss: 0.00129173
Iteration 12/25 | Loss: 0.00129173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012917337007820606, 0.0012917337007820606, 0.0012917337007820606, 0.0012917337007820606, 0.0012917337007820606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012917337007820606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01804209
Iteration 2/25 | Loss: 0.00054623
Iteration 3/25 | Loss: 0.00054622
Iteration 4/25 | Loss: 0.00054622
Iteration 5/25 | Loss: 0.00054622
Iteration 6/25 | Loss: 0.00054622
Iteration 7/25 | Loss: 0.00054621
Iteration 8/25 | Loss: 0.00054621
Iteration 9/25 | Loss: 0.00054621
Iteration 10/25 | Loss: 0.00054621
Iteration 11/25 | Loss: 0.00054621
Iteration 12/25 | Loss: 0.00054621
Iteration 13/25 | Loss: 0.00054621
Iteration 14/25 | Loss: 0.00054621
Iteration 15/25 | Loss: 0.00054621
Iteration 16/25 | Loss: 0.00054621
Iteration 17/25 | Loss: 0.00054621
Iteration 18/25 | Loss: 0.00054621
Iteration 19/25 | Loss: 0.00054621
Iteration 20/25 | Loss: 0.00054621
Iteration 21/25 | Loss: 0.00054621
Iteration 22/25 | Loss: 0.00054621
Iteration 23/25 | Loss: 0.00054621
Iteration 24/25 | Loss: 0.00054621
Iteration 25/25 | Loss: 0.00054621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054621
Iteration 2/1000 | Loss: 0.00003992
Iteration 3/1000 | Loss: 0.00003106
Iteration 4/1000 | Loss: 0.00002886
Iteration 5/1000 | Loss: 0.00002769
Iteration 6/1000 | Loss: 0.00002650
Iteration 7/1000 | Loss: 0.00002563
Iteration 8/1000 | Loss: 0.00002501
Iteration 9/1000 | Loss: 0.00002462
Iteration 10/1000 | Loss: 0.00002436
Iteration 11/1000 | Loss: 0.00002407
Iteration 12/1000 | Loss: 0.00002404
Iteration 13/1000 | Loss: 0.00002394
Iteration 14/1000 | Loss: 0.00002387
Iteration 15/1000 | Loss: 0.00002383
Iteration 16/1000 | Loss: 0.00002366
Iteration 17/1000 | Loss: 0.00002364
Iteration 18/1000 | Loss: 0.00002355
Iteration 19/1000 | Loss: 0.00002354
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002353
Iteration 22/1000 | Loss: 0.00002352
Iteration 23/1000 | Loss: 0.00002352
Iteration 24/1000 | Loss: 0.00002352
Iteration 25/1000 | Loss: 0.00002352
Iteration 26/1000 | Loss: 0.00002352
Iteration 27/1000 | Loss: 0.00002352
Iteration 28/1000 | Loss: 0.00002352
Iteration 29/1000 | Loss: 0.00002351
Iteration 30/1000 | Loss: 0.00002351
Iteration 31/1000 | Loss: 0.00002351
Iteration 32/1000 | Loss: 0.00002351
Iteration 33/1000 | Loss: 0.00002351
Iteration 34/1000 | Loss: 0.00002351
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002349
Iteration 37/1000 | Loss: 0.00002349
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002349
Iteration 40/1000 | Loss: 0.00002349
Iteration 41/1000 | Loss: 0.00002349
Iteration 42/1000 | Loss: 0.00002349
Iteration 43/1000 | Loss: 0.00002349
Iteration 44/1000 | Loss: 0.00002349
Iteration 45/1000 | Loss: 0.00002349
Iteration 46/1000 | Loss: 0.00002348
Iteration 47/1000 | Loss: 0.00002348
Iteration 48/1000 | Loss: 0.00002348
Iteration 49/1000 | Loss: 0.00002348
Iteration 50/1000 | Loss: 0.00002348
Iteration 51/1000 | Loss: 0.00002347
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002346
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002346
Iteration 62/1000 | Loss: 0.00002345
Iteration 63/1000 | Loss: 0.00002345
Iteration 64/1000 | Loss: 0.00002345
Iteration 65/1000 | Loss: 0.00002345
Iteration 66/1000 | Loss: 0.00002345
Iteration 67/1000 | Loss: 0.00002345
Iteration 68/1000 | Loss: 0.00002345
Iteration 69/1000 | Loss: 0.00002345
Iteration 70/1000 | Loss: 0.00002345
Iteration 71/1000 | Loss: 0.00002345
Iteration 72/1000 | Loss: 0.00002345
Iteration 73/1000 | Loss: 0.00002344
Iteration 74/1000 | Loss: 0.00002344
Iteration 75/1000 | Loss: 0.00002344
Iteration 76/1000 | Loss: 0.00002344
Iteration 77/1000 | Loss: 0.00002344
Iteration 78/1000 | Loss: 0.00002344
Iteration 79/1000 | Loss: 0.00002344
Iteration 80/1000 | Loss: 0.00002344
Iteration 81/1000 | Loss: 0.00002344
Iteration 82/1000 | Loss: 0.00002343
Iteration 83/1000 | Loss: 0.00002343
Iteration 84/1000 | Loss: 0.00002343
Iteration 85/1000 | Loss: 0.00002343
Iteration 86/1000 | Loss: 0.00002343
Iteration 87/1000 | Loss: 0.00002343
Iteration 88/1000 | Loss: 0.00002343
Iteration 89/1000 | Loss: 0.00002343
Iteration 90/1000 | Loss: 0.00002343
Iteration 91/1000 | Loss: 0.00002343
Iteration 92/1000 | Loss: 0.00002342
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002342
Iteration 95/1000 | Loss: 0.00002342
Iteration 96/1000 | Loss: 0.00002342
Iteration 97/1000 | Loss: 0.00002341
Iteration 98/1000 | Loss: 0.00002341
Iteration 99/1000 | Loss: 0.00002341
Iteration 100/1000 | Loss: 0.00002341
Iteration 101/1000 | Loss: 0.00002341
Iteration 102/1000 | Loss: 0.00002341
Iteration 103/1000 | Loss: 0.00002341
Iteration 104/1000 | Loss: 0.00002341
Iteration 105/1000 | Loss: 0.00002341
Iteration 106/1000 | Loss: 0.00002341
Iteration 107/1000 | Loss: 0.00002341
Iteration 108/1000 | Loss: 0.00002340
Iteration 109/1000 | Loss: 0.00002340
Iteration 110/1000 | Loss: 0.00002340
Iteration 111/1000 | Loss: 0.00002340
Iteration 112/1000 | Loss: 0.00002340
Iteration 113/1000 | Loss: 0.00002340
Iteration 114/1000 | Loss: 0.00002340
Iteration 115/1000 | Loss: 0.00002340
Iteration 116/1000 | Loss: 0.00002340
Iteration 117/1000 | Loss: 0.00002339
Iteration 118/1000 | Loss: 0.00002339
Iteration 119/1000 | Loss: 0.00002339
Iteration 120/1000 | Loss: 0.00002339
Iteration 121/1000 | Loss: 0.00002339
Iteration 122/1000 | Loss: 0.00002339
Iteration 123/1000 | Loss: 0.00002339
Iteration 124/1000 | Loss: 0.00002339
Iteration 125/1000 | Loss: 0.00002339
Iteration 126/1000 | Loss: 0.00002339
Iteration 127/1000 | Loss: 0.00002339
Iteration 128/1000 | Loss: 0.00002339
Iteration 129/1000 | Loss: 0.00002339
Iteration 130/1000 | Loss: 0.00002339
Iteration 131/1000 | Loss: 0.00002339
Iteration 132/1000 | Loss: 0.00002338
Iteration 133/1000 | Loss: 0.00002338
Iteration 134/1000 | Loss: 0.00002338
Iteration 135/1000 | Loss: 0.00002338
Iteration 136/1000 | Loss: 0.00002338
Iteration 137/1000 | Loss: 0.00002338
Iteration 138/1000 | Loss: 0.00002338
Iteration 139/1000 | Loss: 0.00002338
Iteration 140/1000 | Loss: 0.00002338
Iteration 141/1000 | Loss: 0.00002338
Iteration 142/1000 | Loss: 0.00002338
Iteration 143/1000 | Loss: 0.00002338
Iteration 144/1000 | Loss: 0.00002338
Iteration 145/1000 | Loss: 0.00002338
Iteration 146/1000 | Loss: 0.00002338
Iteration 147/1000 | Loss: 0.00002337
Iteration 148/1000 | Loss: 0.00002337
Iteration 149/1000 | Loss: 0.00002337
Iteration 150/1000 | Loss: 0.00002337
Iteration 151/1000 | Loss: 0.00002336
Iteration 152/1000 | Loss: 0.00002336
Iteration 153/1000 | Loss: 0.00002336
Iteration 154/1000 | Loss: 0.00002336
Iteration 155/1000 | Loss: 0.00002335
Iteration 156/1000 | Loss: 0.00002335
Iteration 157/1000 | Loss: 0.00002335
Iteration 158/1000 | Loss: 0.00002335
Iteration 159/1000 | Loss: 0.00002335
Iteration 160/1000 | Loss: 0.00002335
Iteration 161/1000 | Loss: 0.00002334
Iteration 162/1000 | Loss: 0.00002334
Iteration 163/1000 | Loss: 0.00002334
Iteration 164/1000 | Loss: 0.00002334
Iteration 165/1000 | Loss: 0.00002334
Iteration 166/1000 | Loss: 0.00002334
Iteration 167/1000 | Loss: 0.00002334
Iteration 168/1000 | Loss: 0.00002334
Iteration 169/1000 | Loss: 0.00002334
Iteration 170/1000 | Loss: 0.00002334
Iteration 171/1000 | Loss: 0.00002333
Iteration 172/1000 | Loss: 0.00002333
Iteration 173/1000 | Loss: 0.00002333
Iteration 174/1000 | Loss: 0.00002333
Iteration 175/1000 | Loss: 0.00002333
Iteration 176/1000 | Loss: 0.00002333
Iteration 177/1000 | Loss: 0.00002333
Iteration 178/1000 | Loss: 0.00002333
Iteration 179/1000 | Loss: 0.00002333
Iteration 180/1000 | Loss: 0.00002333
Iteration 181/1000 | Loss: 0.00002333
Iteration 182/1000 | Loss: 0.00002333
Iteration 183/1000 | Loss: 0.00002333
Iteration 184/1000 | Loss: 0.00002333
Iteration 185/1000 | Loss: 0.00002333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.3330330805038102e-05, 2.3330330805038102e-05, 2.3330330805038102e-05, 2.3330330805038102e-05, 2.3330330805038102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3330330805038102e-05

Optimization complete. Final v2v error: 4.125432968139648 mm

Highest mean error: 4.202226638793945 mm for frame 42

Lowest mean error: 3.95574688911438 mm for frame 65

Saving results

Total time: 40.54754948616028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460159
Iteration 2/25 | Loss: 0.00141858
Iteration 3/25 | Loss: 0.00131643
Iteration 4/25 | Loss: 0.00130789
Iteration 5/25 | Loss: 0.00130521
Iteration 6/25 | Loss: 0.00130521
Iteration 7/25 | Loss: 0.00130521
Iteration 8/25 | Loss: 0.00130521
Iteration 9/25 | Loss: 0.00130521
Iteration 10/25 | Loss: 0.00130521
Iteration 11/25 | Loss: 0.00130521
Iteration 12/25 | Loss: 0.00130521
Iteration 13/25 | Loss: 0.00130521
Iteration 14/25 | Loss: 0.00130521
Iteration 15/25 | Loss: 0.00130521
Iteration 16/25 | Loss: 0.00130521
Iteration 17/25 | Loss: 0.00130521
Iteration 18/25 | Loss: 0.00130521
Iteration 19/25 | Loss: 0.00130521
Iteration 20/25 | Loss: 0.00130521
Iteration 21/25 | Loss: 0.00130521
Iteration 22/25 | Loss: 0.00130521
Iteration 23/25 | Loss: 0.00130521
Iteration 24/25 | Loss: 0.00130521
Iteration 25/25 | Loss: 0.00130521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97933805
Iteration 2/25 | Loss: 0.00089795
Iteration 3/25 | Loss: 0.00089794
Iteration 4/25 | Loss: 0.00089794
Iteration 5/25 | Loss: 0.00089794
Iteration 6/25 | Loss: 0.00089794
Iteration 7/25 | Loss: 0.00089794
Iteration 8/25 | Loss: 0.00089794
Iteration 9/25 | Loss: 0.00089794
Iteration 10/25 | Loss: 0.00089794
Iteration 11/25 | Loss: 0.00089794
Iteration 12/25 | Loss: 0.00089794
Iteration 13/25 | Loss: 0.00089794
Iteration 14/25 | Loss: 0.00089794
Iteration 15/25 | Loss: 0.00089794
Iteration 16/25 | Loss: 0.00089794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008979380363598466, 0.0008979380363598466, 0.0008979380363598466, 0.0008979380363598466, 0.0008979380363598466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008979380363598466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089794
Iteration 2/1000 | Loss: 0.00003453
Iteration 3/1000 | Loss: 0.00002330
Iteration 4/1000 | Loss: 0.00001932
Iteration 5/1000 | Loss: 0.00001798
Iteration 6/1000 | Loss: 0.00001710
Iteration 7/1000 | Loss: 0.00001643
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001574
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001499
Iteration 14/1000 | Loss: 0.00001497
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001474
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001468
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001464
Iteration 31/1000 | Loss: 0.00001463
Iteration 32/1000 | Loss: 0.00001463
Iteration 33/1000 | Loss: 0.00001463
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001460
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001455
Iteration 47/1000 | Loss: 0.00001454
Iteration 48/1000 | Loss: 0.00001454
Iteration 49/1000 | Loss: 0.00001454
Iteration 50/1000 | Loss: 0.00001453
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001453
Iteration 53/1000 | Loss: 0.00001453
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001452
Iteration 56/1000 | Loss: 0.00001452
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001450
Iteration 60/1000 | Loss: 0.00001450
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001450
Iteration 64/1000 | Loss: 0.00001450
Iteration 65/1000 | Loss: 0.00001449
Iteration 66/1000 | Loss: 0.00001449
Iteration 67/1000 | Loss: 0.00001449
Iteration 68/1000 | Loss: 0.00001449
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001444
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001443
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001441
Iteration 88/1000 | Loss: 0.00001440
Iteration 89/1000 | Loss: 0.00001440
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001439
Iteration 92/1000 | Loss: 0.00001439
Iteration 93/1000 | Loss: 0.00001438
Iteration 94/1000 | Loss: 0.00001438
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001437
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001436
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001433
Iteration 107/1000 | Loss: 0.00001433
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001431
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001430
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001429
Iteration 118/1000 | Loss: 0.00001429
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001427
Iteration 127/1000 | Loss: 0.00001427
Iteration 128/1000 | Loss: 0.00001427
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001426
Iteration 133/1000 | Loss: 0.00001426
Iteration 134/1000 | Loss: 0.00001426
Iteration 135/1000 | Loss: 0.00001426
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001425
Iteration 138/1000 | Loss: 0.00001425
Iteration 139/1000 | Loss: 0.00001425
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001424
Iteration 142/1000 | Loss: 0.00001424
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001421
Iteration 159/1000 | Loss: 0.00001421
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001420
Iteration 166/1000 | Loss: 0.00001420
Iteration 167/1000 | Loss: 0.00001420
Iteration 168/1000 | Loss: 0.00001420
Iteration 169/1000 | Loss: 0.00001420
Iteration 170/1000 | Loss: 0.00001420
Iteration 171/1000 | Loss: 0.00001419
Iteration 172/1000 | Loss: 0.00001419
Iteration 173/1000 | Loss: 0.00001419
Iteration 174/1000 | Loss: 0.00001419
Iteration 175/1000 | Loss: 0.00001419
Iteration 176/1000 | Loss: 0.00001419
Iteration 177/1000 | Loss: 0.00001419
Iteration 178/1000 | Loss: 0.00001419
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001418
Iteration 181/1000 | Loss: 0.00001418
Iteration 182/1000 | Loss: 0.00001418
Iteration 183/1000 | Loss: 0.00001418
Iteration 184/1000 | Loss: 0.00001418
Iteration 185/1000 | Loss: 0.00001418
Iteration 186/1000 | Loss: 0.00001418
Iteration 187/1000 | Loss: 0.00001418
Iteration 188/1000 | Loss: 0.00001418
Iteration 189/1000 | Loss: 0.00001417
Iteration 190/1000 | Loss: 0.00001417
Iteration 191/1000 | Loss: 0.00001417
Iteration 192/1000 | Loss: 0.00001417
Iteration 193/1000 | Loss: 0.00001417
Iteration 194/1000 | Loss: 0.00001417
Iteration 195/1000 | Loss: 0.00001417
Iteration 196/1000 | Loss: 0.00001416
Iteration 197/1000 | Loss: 0.00001416
Iteration 198/1000 | Loss: 0.00001416
Iteration 199/1000 | Loss: 0.00001416
Iteration 200/1000 | Loss: 0.00001416
Iteration 201/1000 | Loss: 0.00001416
Iteration 202/1000 | Loss: 0.00001416
Iteration 203/1000 | Loss: 0.00001416
Iteration 204/1000 | Loss: 0.00001416
Iteration 205/1000 | Loss: 0.00001416
Iteration 206/1000 | Loss: 0.00001416
Iteration 207/1000 | Loss: 0.00001416
Iteration 208/1000 | Loss: 0.00001416
Iteration 209/1000 | Loss: 0.00001416
Iteration 210/1000 | Loss: 0.00001416
Iteration 211/1000 | Loss: 0.00001415
Iteration 212/1000 | Loss: 0.00001415
Iteration 213/1000 | Loss: 0.00001415
Iteration 214/1000 | Loss: 0.00001415
Iteration 215/1000 | Loss: 0.00001415
Iteration 216/1000 | Loss: 0.00001415
Iteration 217/1000 | Loss: 0.00001415
Iteration 218/1000 | Loss: 0.00001415
Iteration 219/1000 | Loss: 0.00001415
Iteration 220/1000 | Loss: 0.00001415
Iteration 221/1000 | Loss: 0.00001415
Iteration 222/1000 | Loss: 0.00001415
Iteration 223/1000 | Loss: 0.00001415
Iteration 224/1000 | Loss: 0.00001415
Iteration 225/1000 | Loss: 0.00001415
Iteration 226/1000 | Loss: 0.00001415
Iteration 227/1000 | Loss: 0.00001415
Iteration 228/1000 | Loss: 0.00001415
Iteration 229/1000 | Loss: 0.00001415
Iteration 230/1000 | Loss: 0.00001415
Iteration 231/1000 | Loss: 0.00001415
Iteration 232/1000 | Loss: 0.00001415
Iteration 233/1000 | Loss: 0.00001415
Iteration 234/1000 | Loss: 0.00001415
Iteration 235/1000 | Loss: 0.00001415
Iteration 236/1000 | Loss: 0.00001415
Iteration 237/1000 | Loss: 0.00001415
Iteration 238/1000 | Loss: 0.00001415
Iteration 239/1000 | Loss: 0.00001415
Iteration 240/1000 | Loss: 0.00001415
Iteration 241/1000 | Loss: 0.00001415
Iteration 242/1000 | Loss: 0.00001415
Iteration 243/1000 | Loss: 0.00001415
Iteration 244/1000 | Loss: 0.00001415
Iteration 245/1000 | Loss: 0.00001415
Iteration 246/1000 | Loss: 0.00001415
Iteration 247/1000 | Loss: 0.00001415
Iteration 248/1000 | Loss: 0.00001415
Iteration 249/1000 | Loss: 0.00001415
Iteration 250/1000 | Loss: 0.00001415
Iteration 251/1000 | Loss: 0.00001415
Iteration 252/1000 | Loss: 0.00001415
Iteration 253/1000 | Loss: 0.00001415
Iteration 254/1000 | Loss: 0.00001415
Iteration 255/1000 | Loss: 0.00001415
Iteration 256/1000 | Loss: 0.00001415
Iteration 257/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.4148973605188075e-05, 1.4148973605188075e-05, 1.4148973605188075e-05, 1.4148973605188075e-05, 1.4148973605188075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4148973605188075e-05

Optimization complete. Final v2v error: 3.1897809505462646 mm

Highest mean error: 4.0942535400390625 mm for frame 171

Lowest mean error: 2.9312524795532227 mm for frame 193

Saving results

Total time: 51.87890315055847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973887
Iteration 2/25 | Loss: 0.00973886
Iteration 3/25 | Loss: 0.00369526
Iteration 4/25 | Loss: 0.00226175
Iteration 5/25 | Loss: 0.00206517
Iteration 6/25 | Loss: 0.00200816
Iteration 7/25 | Loss: 0.00185555
Iteration 8/25 | Loss: 0.00181861
Iteration 9/25 | Loss: 0.00178708
Iteration 10/25 | Loss: 0.00176762
Iteration 11/25 | Loss: 0.00174921
Iteration 12/25 | Loss: 0.00174530
Iteration 13/25 | Loss: 0.00175771
Iteration 14/25 | Loss: 0.00173798
Iteration 15/25 | Loss: 0.00173884
Iteration 16/25 | Loss: 0.00173538
Iteration 17/25 | Loss: 0.00173188
Iteration 18/25 | Loss: 0.00172878
Iteration 19/25 | Loss: 0.00172846
Iteration 20/25 | Loss: 0.00172291
Iteration 21/25 | Loss: 0.00171862
Iteration 22/25 | Loss: 0.00172085
Iteration 23/25 | Loss: 0.00172510
Iteration 24/25 | Loss: 0.00171659
Iteration 25/25 | Loss: 0.00172367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42873275
Iteration 2/25 | Loss: 0.00401570
Iteration 3/25 | Loss: 0.00397508
Iteration 4/25 | Loss: 0.00397532
Iteration 5/25 | Loss: 0.00395364
Iteration 6/25 | Loss: 0.00395364
Iteration 7/25 | Loss: 0.00395364
Iteration 8/25 | Loss: 0.00395364
Iteration 9/25 | Loss: 0.00395364
Iteration 10/25 | Loss: 0.00395364
Iteration 11/25 | Loss: 0.00395364
Iteration 12/25 | Loss: 0.00395364
Iteration 13/25 | Loss: 0.00395364
Iteration 14/25 | Loss: 0.00395364
Iteration 15/25 | Loss: 0.00395364
Iteration 16/25 | Loss: 0.00395364
Iteration 17/25 | Loss: 0.00395364
Iteration 18/25 | Loss: 0.00395364
Iteration 19/25 | Loss: 0.00395364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003953637555241585, 0.003953637555241585, 0.003953637555241585, 0.003953637555241585, 0.003953637555241585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003953637555241585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395364
Iteration 2/1000 | Loss: 0.00124923
Iteration 3/1000 | Loss: 0.00072027
Iteration 4/1000 | Loss: 0.00053144
Iteration 5/1000 | Loss: 0.00062412
Iteration 6/1000 | Loss: 0.00059582
Iteration 7/1000 | Loss: 0.00149837
Iteration 8/1000 | Loss: 0.00240276
Iteration 9/1000 | Loss: 0.00103547
Iteration 10/1000 | Loss: 0.00177007
Iteration 11/1000 | Loss: 0.00116611
Iteration 12/1000 | Loss: 0.00143626
Iteration 13/1000 | Loss: 0.00071469
Iteration 14/1000 | Loss: 0.00072026
Iteration 15/1000 | Loss: 0.00065710
Iteration 16/1000 | Loss: 0.00052171
Iteration 17/1000 | Loss: 0.00061636
Iteration 18/1000 | Loss: 0.00071548
Iteration 19/1000 | Loss: 0.00087978
Iteration 20/1000 | Loss: 0.00045016
Iteration 21/1000 | Loss: 0.00100207
Iteration 22/1000 | Loss: 0.00186037
Iteration 23/1000 | Loss: 0.00059036
Iteration 24/1000 | Loss: 0.00025421
Iteration 25/1000 | Loss: 0.00084570
Iteration 26/1000 | Loss: 0.00078875
Iteration 27/1000 | Loss: 0.00060898
Iteration 28/1000 | Loss: 0.00040306
Iteration 29/1000 | Loss: 0.00128660
Iteration 30/1000 | Loss: 0.00093442
Iteration 31/1000 | Loss: 0.00117160
Iteration 32/1000 | Loss: 0.00112793
Iteration 33/1000 | Loss: 0.00082939
Iteration 34/1000 | Loss: 0.00095402
Iteration 35/1000 | Loss: 0.00038643
Iteration 36/1000 | Loss: 0.00038233
Iteration 37/1000 | Loss: 0.00052686
Iteration 38/1000 | Loss: 0.00039597
Iteration 39/1000 | Loss: 0.00053489
Iteration 40/1000 | Loss: 0.00046549
Iteration 41/1000 | Loss: 0.00051004
Iteration 42/1000 | Loss: 0.00027885
Iteration 43/1000 | Loss: 0.00046790
Iteration 44/1000 | Loss: 0.00026666
Iteration 45/1000 | Loss: 0.00055247
Iteration 46/1000 | Loss: 0.00048487
Iteration 47/1000 | Loss: 0.00056419
Iteration 48/1000 | Loss: 0.00050379
Iteration 49/1000 | Loss: 0.00044869
Iteration 50/1000 | Loss: 0.00047974
Iteration 51/1000 | Loss: 0.00046139
Iteration 52/1000 | Loss: 0.00053817
Iteration 53/1000 | Loss: 0.00036502
Iteration 54/1000 | Loss: 0.00055174
Iteration 55/1000 | Loss: 0.00056016
Iteration 56/1000 | Loss: 0.00105951
Iteration 57/1000 | Loss: 0.00057986
Iteration 58/1000 | Loss: 0.00059516
Iteration 59/1000 | Loss: 0.00045029
Iteration 60/1000 | Loss: 0.00066305
Iteration 61/1000 | Loss: 0.00026739
Iteration 62/1000 | Loss: 0.00022668
Iteration 63/1000 | Loss: 0.00033674
Iteration 64/1000 | Loss: 0.00044883
Iteration 65/1000 | Loss: 0.00041713
Iteration 66/1000 | Loss: 0.00026143
Iteration 67/1000 | Loss: 0.00024587
Iteration 68/1000 | Loss: 0.00016882
Iteration 69/1000 | Loss: 0.00020179
Iteration 70/1000 | Loss: 0.00030600
Iteration 71/1000 | Loss: 0.00016368
Iteration 72/1000 | Loss: 0.00050729
Iteration 73/1000 | Loss: 0.00079085
Iteration 74/1000 | Loss: 0.00034827
Iteration 75/1000 | Loss: 0.00037853
Iteration 76/1000 | Loss: 0.00039323
Iteration 77/1000 | Loss: 0.00067478
Iteration 78/1000 | Loss: 0.00060802
Iteration 79/1000 | Loss: 0.00034500
Iteration 80/1000 | Loss: 0.00024348
Iteration 81/1000 | Loss: 0.00106379
Iteration 82/1000 | Loss: 0.00065688
Iteration 83/1000 | Loss: 0.00064313
Iteration 84/1000 | Loss: 0.00034806
Iteration 85/1000 | Loss: 0.00025153
Iteration 86/1000 | Loss: 0.00015728
Iteration 87/1000 | Loss: 0.00050039
Iteration 88/1000 | Loss: 0.00044374
Iteration 89/1000 | Loss: 0.00100131
Iteration 90/1000 | Loss: 0.00045577
Iteration 91/1000 | Loss: 0.00028602
Iteration 92/1000 | Loss: 0.00015857
Iteration 93/1000 | Loss: 0.00016069
Iteration 94/1000 | Loss: 0.00015593
Iteration 95/1000 | Loss: 0.00037675
Iteration 96/1000 | Loss: 0.00058335
Iteration 97/1000 | Loss: 0.00037369
Iteration 98/1000 | Loss: 0.00033407
Iteration 99/1000 | Loss: 0.00032176
Iteration 100/1000 | Loss: 0.00024212
Iteration 101/1000 | Loss: 0.00047313
Iteration 102/1000 | Loss: 0.00047964
Iteration 103/1000 | Loss: 0.00054486
Iteration 104/1000 | Loss: 0.00047664
Iteration 105/1000 | Loss: 0.00059352
Iteration 106/1000 | Loss: 0.00083721
Iteration 107/1000 | Loss: 0.00269944
Iteration 108/1000 | Loss: 0.00345699
Iteration 109/1000 | Loss: 0.00148438
Iteration 110/1000 | Loss: 0.00116086
Iteration 111/1000 | Loss: 0.00098985
Iteration 112/1000 | Loss: 0.00029076
Iteration 113/1000 | Loss: 0.00037472
Iteration 114/1000 | Loss: 0.00359073
Iteration 115/1000 | Loss: 0.00075419
Iteration 116/1000 | Loss: 0.00097000
Iteration 117/1000 | Loss: 0.00078263
Iteration 118/1000 | Loss: 0.00049166
Iteration 119/1000 | Loss: 0.00042963
Iteration 120/1000 | Loss: 0.00098657
Iteration 121/1000 | Loss: 0.00031135
Iteration 122/1000 | Loss: 0.00016196
Iteration 123/1000 | Loss: 0.00037092
Iteration 124/1000 | Loss: 0.00029495
Iteration 125/1000 | Loss: 0.00014161
Iteration 126/1000 | Loss: 0.00039474
Iteration 127/1000 | Loss: 0.00192531
Iteration 128/1000 | Loss: 0.00016287
Iteration 129/1000 | Loss: 0.00010374
Iteration 130/1000 | Loss: 0.00050291
Iteration 131/1000 | Loss: 0.00009410
Iteration 132/1000 | Loss: 0.00008418
Iteration 133/1000 | Loss: 0.00043145
Iteration 134/1000 | Loss: 0.00008935
Iteration 135/1000 | Loss: 0.00007952
Iteration 136/1000 | Loss: 0.00038809
Iteration 137/1000 | Loss: 0.00076854
Iteration 138/1000 | Loss: 0.00034557
Iteration 139/1000 | Loss: 0.00011131
Iteration 140/1000 | Loss: 0.00008155
Iteration 141/1000 | Loss: 0.00008363
Iteration 142/1000 | Loss: 0.00012646
Iteration 143/1000 | Loss: 0.00006481
Iteration 144/1000 | Loss: 0.00006977
Iteration 145/1000 | Loss: 0.00007350
Iteration 146/1000 | Loss: 0.00034633
Iteration 147/1000 | Loss: 0.00065759
Iteration 148/1000 | Loss: 0.00025342
Iteration 149/1000 | Loss: 0.00009255
Iteration 150/1000 | Loss: 0.00009565
Iteration 151/1000 | Loss: 0.00010217
Iteration 152/1000 | Loss: 0.00009230
Iteration 153/1000 | Loss: 0.00023576
Iteration 154/1000 | Loss: 0.00217391
Iteration 155/1000 | Loss: 0.00026168
Iteration 156/1000 | Loss: 0.00012181
Iteration 157/1000 | Loss: 0.00010994
Iteration 158/1000 | Loss: 0.00029697
Iteration 159/1000 | Loss: 0.00006528
Iteration 160/1000 | Loss: 0.00006212
Iteration 161/1000 | Loss: 0.00005651
Iteration 162/1000 | Loss: 0.00008005
Iteration 163/1000 | Loss: 0.00005178
Iteration 164/1000 | Loss: 0.00005627
Iteration 165/1000 | Loss: 0.00004585
Iteration 166/1000 | Loss: 0.00008149
Iteration 167/1000 | Loss: 0.00019121
Iteration 168/1000 | Loss: 0.00005195
Iteration 169/1000 | Loss: 0.00030704
Iteration 170/1000 | Loss: 0.00026116
Iteration 171/1000 | Loss: 0.00008888
Iteration 172/1000 | Loss: 0.00005871
Iteration 173/1000 | Loss: 0.00006751
Iteration 174/1000 | Loss: 0.00005604
Iteration 175/1000 | Loss: 0.00004423
Iteration 176/1000 | Loss: 0.00004088
Iteration 177/1000 | Loss: 0.00003825
Iteration 178/1000 | Loss: 0.00019178
Iteration 179/1000 | Loss: 0.00033340
Iteration 180/1000 | Loss: 0.00014766
Iteration 181/1000 | Loss: 0.00006352
Iteration 182/1000 | Loss: 0.00008213
Iteration 183/1000 | Loss: 0.00004384
Iteration 184/1000 | Loss: 0.00003557
Iteration 185/1000 | Loss: 0.00003450
Iteration 186/1000 | Loss: 0.00020031
Iteration 187/1000 | Loss: 0.00003848
Iteration 188/1000 | Loss: 0.00004578
Iteration 189/1000 | Loss: 0.00003525
Iteration 190/1000 | Loss: 0.00003273
Iteration 191/1000 | Loss: 0.00007230
Iteration 192/1000 | Loss: 0.00029215
Iteration 193/1000 | Loss: 0.00015097
Iteration 194/1000 | Loss: 0.00004933
Iteration 195/1000 | Loss: 0.00003842
Iteration 196/1000 | Loss: 0.00002933
Iteration 197/1000 | Loss: 0.00006423
Iteration 198/1000 | Loss: 0.00015633
Iteration 199/1000 | Loss: 0.00003889
Iteration 200/1000 | Loss: 0.00002870
Iteration 201/1000 | Loss: 0.00004278
Iteration 202/1000 | Loss: 0.00002857
Iteration 203/1000 | Loss: 0.00002854
Iteration 204/1000 | Loss: 0.00003357
Iteration 205/1000 | Loss: 0.00012929
Iteration 206/1000 | Loss: 0.00008057
Iteration 207/1000 | Loss: 0.00006594
Iteration 208/1000 | Loss: 0.00004590
Iteration 209/1000 | Loss: 0.00007732
Iteration 210/1000 | Loss: 0.00002808
Iteration 211/1000 | Loss: 0.00002808
Iteration 212/1000 | Loss: 0.00002808
Iteration 213/1000 | Loss: 0.00002808
Iteration 214/1000 | Loss: 0.00002806
Iteration 215/1000 | Loss: 0.00002944
Iteration 216/1000 | Loss: 0.00002818
Iteration 217/1000 | Loss: 0.00002803
Iteration 218/1000 | Loss: 0.00002803
Iteration 219/1000 | Loss: 0.00002803
Iteration 220/1000 | Loss: 0.00002803
Iteration 221/1000 | Loss: 0.00002803
Iteration 222/1000 | Loss: 0.00002803
Iteration 223/1000 | Loss: 0.00002803
Iteration 224/1000 | Loss: 0.00002803
Iteration 225/1000 | Loss: 0.00002803
Iteration 226/1000 | Loss: 0.00002802
Iteration 227/1000 | Loss: 0.00002801
Iteration 228/1000 | Loss: 0.00002801
Iteration 229/1000 | Loss: 0.00002800
Iteration 230/1000 | Loss: 0.00002800
Iteration 231/1000 | Loss: 0.00002799
Iteration 232/1000 | Loss: 0.00002799
Iteration 233/1000 | Loss: 0.00002798
Iteration 234/1000 | Loss: 0.00002798
Iteration 235/1000 | Loss: 0.00002796
Iteration 236/1000 | Loss: 0.00002795
Iteration 237/1000 | Loss: 0.00002820
Iteration 238/1000 | Loss: 0.00002787
Iteration 239/1000 | Loss: 0.00002787
Iteration 240/1000 | Loss: 0.00002787
Iteration 241/1000 | Loss: 0.00002787
Iteration 242/1000 | Loss: 0.00002786
Iteration 243/1000 | Loss: 0.00002786
Iteration 244/1000 | Loss: 0.00002795
Iteration 245/1000 | Loss: 0.00006671
Iteration 246/1000 | Loss: 0.00002955
Iteration 247/1000 | Loss: 0.00005615
Iteration 248/1000 | Loss: 0.00005410
Iteration 249/1000 | Loss: 0.00002801
Iteration 250/1000 | Loss: 0.00006387
Iteration 251/1000 | Loss: 0.00002756
Iteration 252/1000 | Loss: 0.00002755
Iteration 253/1000 | Loss: 0.00002755
Iteration 254/1000 | Loss: 0.00002755
Iteration 255/1000 | Loss: 0.00002755
Iteration 256/1000 | Loss: 0.00002755
Iteration 257/1000 | Loss: 0.00002754
Iteration 258/1000 | Loss: 0.00002754
Iteration 259/1000 | Loss: 0.00002754
Iteration 260/1000 | Loss: 0.00002753
Iteration 261/1000 | Loss: 0.00002752
Iteration 262/1000 | Loss: 0.00002752
Iteration 263/1000 | Loss: 0.00002752
Iteration 264/1000 | Loss: 0.00002752
Iteration 265/1000 | Loss: 0.00002752
Iteration 266/1000 | Loss: 0.00002752
Iteration 267/1000 | Loss: 0.00002752
Iteration 268/1000 | Loss: 0.00002752
Iteration 269/1000 | Loss: 0.00002751
Iteration 270/1000 | Loss: 0.00002751
Iteration 271/1000 | Loss: 0.00002749
Iteration 272/1000 | Loss: 0.00002749
Iteration 273/1000 | Loss: 0.00018306
Iteration 274/1000 | Loss: 0.00041727
Iteration 275/1000 | Loss: 0.00111999
Iteration 276/1000 | Loss: 0.00019153
Iteration 277/1000 | Loss: 0.00005017
Iteration 278/1000 | Loss: 0.00009652
Iteration 279/1000 | Loss: 0.00006219
Iteration 280/1000 | Loss: 0.00003186
Iteration 281/1000 | Loss: 0.00003291
Iteration 282/1000 | Loss: 0.00002690
Iteration 283/1000 | Loss: 0.00004155
Iteration 284/1000 | Loss: 0.00002755
Iteration 285/1000 | Loss: 0.00002635
Iteration 286/1000 | Loss: 0.00002635
Iteration 287/1000 | Loss: 0.00005716
Iteration 288/1000 | Loss: 0.00008437
Iteration 289/1000 | Loss: 0.00002827
Iteration 290/1000 | Loss: 0.00003863
Iteration 291/1000 | Loss: 0.00002584
Iteration 292/1000 | Loss: 0.00002579
Iteration 293/1000 | Loss: 0.00002824
Iteration 294/1000 | Loss: 0.00002538
Iteration 295/1000 | Loss: 0.00005369
Iteration 296/1000 | Loss: 0.00003365
Iteration 297/1000 | Loss: 0.00002468
Iteration 298/1000 | Loss: 0.00006023
Iteration 299/1000 | Loss: 0.00048381
Iteration 300/1000 | Loss: 0.00095587
Iteration 301/1000 | Loss: 0.00041780
Iteration 302/1000 | Loss: 0.00070046
Iteration 303/1000 | Loss: 0.00087742
Iteration 304/1000 | Loss: 0.00172300
Iteration 305/1000 | Loss: 0.00169938
Iteration 306/1000 | Loss: 0.00363979
Iteration 307/1000 | Loss: 0.00071279
Iteration 308/1000 | Loss: 0.00086161
Iteration 309/1000 | Loss: 0.00055368
Iteration 310/1000 | Loss: 0.00033901
Iteration 311/1000 | Loss: 0.00011730
Iteration 312/1000 | Loss: 0.00009318
Iteration 313/1000 | Loss: 0.00009358
Iteration 314/1000 | Loss: 0.00003655
Iteration 315/1000 | Loss: 0.00011156
Iteration 316/1000 | Loss: 0.00005410
Iteration 317/1000 | Loss: 0.00005454
Iteration 318/1000 | Loss: 0.00004406
Iteration 319/1000 | Loss: 0.00003809
Iteration 320/1000 | Loss: 0.00007724
Iteration 321/1000 | Loss: 0.00002432
Iteration 322/1000 | Loss: 0.00002300
Iteration 323/1000 | Loss: 0.00002171
Iteration 324/1000 | Loss: 0.00022691
Iteration 325/1000 | Loss: 0.00011191
Iteration 326/1000 | Loss: 0.00008919
Iteration 327/1000 | Loss: 0.00019380
Iteration 328/1000 | Loss: 0.00008556
Iteration 329/1000 | Loss: 0.00006279
Iteration 330/1000 | Loss: 0.00002829
Iteration 331/1000 | Loss: 0.00004295
Iteration 332/1000 | Loss: 0.00002602
Iteration 333/1000 | Loss: 0.00020708
Iteration 334/1000 | Loss: 0.00010536
Iteration 335/1000 | Loss: 0.00020687
Iteration 336/1000 | Loss: 0.00018070
Iteration 337/1000 | Loss: 0.00003097
Iteration 338/1000 | Loss: 0.00018384
Iteration 339/1000 | Loss: 0.00012008
Iteration 340/1000 | Loss: 0.00005551
Iteration 341/1000 | Loss: 0.00002348
Iteration 342/1000 | Loss: 0.00006899
Iteration 343/1000 | Loss: 0.00002959
Iteration 344/1000 | Loss: 0.00003886
Iteration 345/1000 | Loss: 0.00001884
Iteration 346/1000 | Loss: 0.00005127
Iteration 347/1000 | Loss: 0.00002989
Iteration 348/1000 | Loss: 0.00024677
Iteration 349/1000 | Loss: 0.00002277
Iteration 350/1000 | Loss: 0.00001779
Iteration 351/1000 | Loss: 0.00003579
Iteration 352/1000 | Loss: 0.00002108
Iteration 353/1000 | Loss: 0.00002047
Iteration 354/1000 | Loss: 0.00003118
Iteration 355/1000 | Loss: 0.00001784
Iteration 356/1000 | Loss: 0.00001759
Iteration 357/1000 | Loss: 0.00002976
Iteration 358/1000 | Loss: 0.00001735
Iteration 359/1000 | Loss: 0.00002434
Iteration 360/1000 | Loss: 0.00001726
Iteration 361/1000 | Loss: 0.00002407
Iteration 362/1000 | Loss: 0.00001726
Iteration 363/1000 | Loss: 0.00002449
Iteration 364/1000 | Loss: 0.00001724
Iteration 365/1000 | Loss: 0.00001723
Iteration 366/1000 | Loss: 0.00001722
Iteration 367/1000 | Loss: 0.00001722
Iteration 368/1000 | Loss: 0.00001722
Iteration 369/1000 | Loss: 0.00001722
Iteration 370/1000 | Loss: 0.00001722
Iteration 371/1000 | Loss: 0.00001722
Iteration 372/1000 | Loss: 0.00001722
Iteration 373/1000 | Loss: 0.00001722
Iteration 374/1000 | Loss: 0.00001722
Iteration 375/1000 | Loss: 0.00001722
Iteration 376/1000 | Loss: 0.00001722
Iteration 377/1000 | Loss: 0.00001721
Iteration 378/1000 | Loss: 0.00001721
Iteration 379/1000 | Loss: 0.00001721
Iteration 380/1000 | Loss: 0.00001720
Iteration 381/1000 | Loss: 0.00001720
Iteration 382/1000 | Loss: 0.00002932
Iteration 383/1000 | Loss: 0.00002011
Iteration 384/1000 | Loss: 0.00001717
Iteration 385/1000 | Loss: 0.00001716
Iteration 386/1000 | Loss: 0.00001716
Iteration 387/1000 | Loss: 0.00001716
Iteration 388/1000 | Loss: 0.00001715
Iteration 389/1000 | Loss: 0.00001715
Iteration 390/1000 | Loss: 0.00001715
Iteration 391/1000 | Loss: 0.00001715
Iteration 392/1000 | Loss: 0.00001715
Iteration 393/1000 | Loss: 0.00001715
Iteration 394/1000 | Loss: 0.00001715
Iteration 395/1000 | Loss: 0.00001715
Iteration 396/1000 | Loss: 0.00001715
Iteration 397/1000 | Loss: 0.00001844
Iteration 398/1000 | Loss: 0.00001713
Iteration 399/1000 | Loss: 0.00001713
Iteration 400/1000 | Loss: 0.00001713
Iteration 401/1000 | Loss: 0.00001713
Iteration 402/1000 | Loss: 0.00001713
Iteration 403/1000 | Loss: 0.00001713
Iteration 404/1000 | Loss: 0.00001713
Iteration 405/1000 | Loss: 0.00001713
Iteration 406/1000 | Loss: 0.00001713
Iteration 407/1000 | Loss: 0.00001713
Iteration 408/1000 | Loss: 0.00001713
Iteration 409/1000 | Loss: 0.00001712
Iteration 410/1000 | Loss: 0.00001712
Iteration 411/1000 | Loss: 0.00001712
Iteration 412/1000 | Loss: 0.00001712
Iteration 413/1000 | Loss: 0.00001712
Iteration 414/1000 | Loss: 0.00001712
Iteration 415/1000 | Loss: 0.00001712
Iteration 416/1000 | Loss: 0.00001712
Iteration 417/1000 | Loss: 0.00001712
Iteration 418/1000 | Loss: 0.00001712
Iteration 419/1000 | Loss: 0.00001712
Iteration 420/1000 | Loss: 0.00001712
Iteration 421/1000 | Loss: 0.00001712
Iteration 422/1000 | Loss: 0.00001712
Iteration 423/1000 | Loss: 0.00001712
Iteration 424/1000 | Loss: 0.00001712
Iteration 425/1000 | Loss: 0.00001712
Iteration 426/1000 | Loss: 0.00001712
Iteration 427/1000 | Loss: 0.00001711
Iteration 428/1000 | Loss: 0.00001711
Iteration 429/1000 | Loss: 0.00001711
Iteration 430/1000 | Loss: 0.00001711
Iteration 431/1000 | Loss: 0.00001711
Iteration 432/1000 | Loss: 0.00001711
Iteration 433/1000 | Loss: 0.00001711
Iteration 434/1000 | Loss: 0.00001711
Iteration 435/1000 | Loss: 0.00004598
Iteration 436/1000 | Loss: 0.00001883
Iteration 437/1000 | Loss: 0.00002799
Iteration 438/1000 | Loss: 0.00010879
Iteration 439/1000 | Loss: 0.00012361
Iteration 440/1000 | Loss: 0.00002357
Iteration 441/1000 | Loss: 0.00003776
Iteration 442/1000 | Loss: 0.00001715
Iteration 443/1000 | Loss: 0.00001707
Iteration 444/1000 | Loss: 0.00001705
Iteration 445/1000 | Loss: 0.00001705
Iteration 446/1000 | Loss: 0.00001705
Iteration 447/1000 | Loss: 0.00001703
Iteration 448/1000 | Loss: 0.00001703
Iteration 449/1000 | Loss: 0.00001703
Iteration 450/1000 | Loss: 0.00001703
Iteration 451/1000 | Loss: 0.00001703
Iteration 452/1000 | Loss: 0.00001703
Iteration 453/1000 | Loss: 0.00001703
Iteration 454/1000 | Loss: 0.00001703
Iteration 455/1000 | Loss: 0.00001702
Iteration 456/1000 | Loss: 0.00001702
Iteration 457/1000 | Loss: 0.00001702
Iteration 458/1000 | Loss: 0.00001702
Iteration 459/1000 | Loss: 0.00001702
Iteration 460/1000 | Loss: 0.00001702
Iteration 461/1000 | Loss: 0.00001702
Iteration 462/1000 | Loss: 0.00001702
Iteration 463/1000 | Loss: 0.00001702
Iteration 464/1000 | Loss: 0.00001702
Iteration 465/1000 | Loss: 0.00001702
Iteration 466/1000 | Loss: 0.00001702
Iteration 467/1000 | Loss: 0.00001702
Iteration 468/1000 | Loss: 0.00001702
Iteration 469/1000 | Loss: 0.00001702
Iteration 470/1000 | Loss: 0.00001702
Iteration 471/1000 | Loss: 0.00001702
Iteration 472/1000 | Loss: 0.00001702
Iteration 473/1000 | Loss: 0.00001702
Iteration 474/1000 | Loss: 0.00001702
Iteration 475/1000 | Loss: 0.00001702
Iteration 476/1000 | Loss: 0.00001702
Iteration 477/1000 | Loss: 0.00001702
Iteration 478/1000 | Loss: 0.00001702
Iteration 479/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 479. Stopping optimization.
Last 5 losses: [1.7017604477587156e-05, 1.7017604477587156e-05, 1.7017604477587156e-05, 1.7017604477587156e-05, 1.7017604477587156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7017604477587156e-05

Optimization complete. Final v2v error: 3.3284521102905273 mm

Highest mean error: 11.764101028442383 mm for frame 124

Lowest mean error: 2.990041732788086 mm for frame 128

Saving results

Total time: 570.354686498642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823091
Iteration 2/25 | Loss: 0.00133895
Iteration 3/25 | Loss: 0.00126884
Iteration 4/25 | Loss: 0.00126028
Iteration 5/25 | Loss: 0.00125705
Iteration 6/25 | Loss: 0.00125660
Iteration 7/25 | Loss: 0.00125660
Iteration 8/25 | Loss: 0.00125660
Iteration 9/25 | Loss: 0.00125660
Iteration 10/25 | Loss: 0.00125660
Iteration 11/25 | Loss: 0.00125660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001256602699868381, 0.001256602699868381, 0.001256602699868381, 0.001256602699868381, 0.001256602699868381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001256602699868381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64685595
Iteration 2/25 | Loss: 0.00091780
Iteration 3/25 | Loss: 0.00091779
Iteration 4/25 | Loss: 0.00091779
Iteration 5/25 | Loss: 0.00091779
Iteration 6/25 | Loss: 0.00091779
Iteration 7/25 | Loss: 0.00091779
Iteration 8/25 | Loss: 0.00091779
Iteration 9/25 | Loss: 0.00091779
Iteration 10/25 | Loss: 0.00091779
Iteration 11/25 | Loss: 0.00091779
Iteration 12/25 | Loss: 0.00091779
Iteration 13/25 | Loss: 0.00091779
Iteration 14/25 | Loss: 0.00091779
Iteration 15/25 | Loss: 0.00091779
Iteration 16/25 | Loss: 0.00091779
Iteration 17/25 | Loss: 0.00091779
Iteration 18/25 | Loss: 0.00091779
Iteration 19/25 | Loss: 0.00091779
Iteration 20/25 | Loss: 0.00091779
Iteration 21/25 | Loss: 0.00091779
Iteration 22/25 | Loss: 0.00091779
Iteration 23/25 | Loss: 0.00091779
Iteration 24/25 | Loss: 0.00091779
Iteration 25/25 | Loss: 0.00091779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091779
Iteration 2/1000 | Loss: 0.00002495
Iteration 3/1000 | Loss: 0.00001818
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001466
Iteration 6/1000 | Loss: 0.00001387
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001291
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001240
Iteration 15/1000 | Loss: 0.00001233
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001211
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001197
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001192
Iteration 48/1000 | Loss: 0.00001192
Iteration 49/1000 | Loss: 0.00001191
Iteration 50/1000 | Loss: 0.00001191
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001187
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001180
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.1797244951594621e-05, 1.1797244951594621e-05, 1.1797244951594621e-05, 1.1797244951594621e-05, 1.1797244951594621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1797244951594621e-05

Optimization complete. Final v2v error: 2.9604737758636475 mm

Highest mean error: 3.2785632610321045 mm for frame 87

Lowest mean error: 2.803757905960083 mm for frame 127

Saving results

Total time: 32.170292139053345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663874
Iteration 2/25 | Loss: 0.00185346
Iteration 3/25 | Loss: 0.00150772
Iteration 4/25 | Loss: 0.00147399
Iteration 5/25 | Loss: 0.00146722
Iteration 6/25 | Loss: 0.00144689
Iteration 7/25 | Loss: 0.00144149
Iteration 8/25 | Loss: 0.00144009
Iteration 9/25 | Loss: 0.00143846
Iteration 10/25 | Loss: 0.00143690
Iteration 11/25 | Loss: 0.00143633
Iteration 12/25 | Loss: 0.00143623
Iteration 13/25 | Loss: 0.00143614
Iteration 14/25 | Loss: 0.00143609
Iteration 15/25 | Loss: 0.00143609
Iteration 16/25 | Loss: 0.00143608
Iteration 17/25 | Loss: 0.00143608
Iteration 18/25 | Loss: 0.00143606
Iteration 19/25 | Loss: 0.00143605
Iteration 20/25 | Loss: 0.00143604
Iteration 21/25 | Loss: 0.00143604
Iteration 22/25 | Loss: 0.00143604
Iteration 23/25 | Loss: 0.00143604
Iteration 24/25 | Loss: 0.00143604
Iteration 25/25 | Loss: 0.00143604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48390746
Iteration 2/25 | Loss: 0.00084742
Iteration 3/25 | Loss: 0.00084742
Iteration 4/25 | Loss: 0.00084742
Iteration 5/25 | Loss: 0.00084742
Iteration 6/25 | Loss: 0.00084742
Iteration 7/25 | Loss: 0.00084742
Iteration 8/25 | Loss: 0.00084742
Iteration 9/25 | Loss: 0.00084742
Iteration 10/25 | Loss: 0.00084742
Iteration 11/25 | Loss: 0.00084742
Iteration 12/25 | Loss: 0.00084742
Iteration 13/25 | Loss: 0.00084742
Iteration 14/25 | Loss: 0.00084742
Iteration 15/25 | Loss: 0.00084742
Iteration 16/25 | Loss: 0.00084742
Iteration 17/25 | Loss: 0.00084742
Iteration 18/25 | Loss: 0.00084742
Iteration 19/25 | Loss: 0.00084742
Iteration 20/25 | Loss: 0.00084742
Iteration 21/25 | Loss: 0.00084742
Iteration 22/25 | Loss: 0.00084742
Iteration 23/25 | Loss: 0.00084742
Iteration 24/25 | Loss: 0.00084742
Iteration 25/25 | Loss: 0.00084742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084742
Iteration 2/1000 | Loss: 0.00006865
Iteration 3/1000 | Loss: 0.00004216
Iteration 4/1000 | Loss: 0.00003704
Iteration 5/1000 | Loss: 0.00003506
Iteration 6/1000 | Loss: 0.00003366
Iteration 7/1000 | Loss: 0.00003282
Iteration 8/1000 | Loss: 0.00003202
Iteration 9/1000 | Loss: 0.00003144
Iteration 10/1000 | Loss: 0.00003090
Iteration 11/1000 | Loss: 0.00003049
Iteration 12/1000 | Loss: 0.00003020
Iteration 13/1000 | Loss: 0.00002994
Iteration 14/1000 | Loss: 0.00002975
Iteration 15/1000 | Loss: 0.00002969
Iteration 16/1000 | Loss: 0.00002953
Iteration 17/1000 | Loss: 0.00002946
Iteration 18/1000 | Loss: 0.00002944
Iteration 19/1000 | Loss: 0.00002941
Iteration 20/1000 | Loss: 0.00002937
Iteration 21/1000 | Loss: 0.00002936
Iteration 22/1000 | Loss: 0.00002936
Iteration 23/1000 | Loss: 0.00002935
Iteration 24/1000 | Loss: 0.00002935
Iteration 25/1000 | Loss: 0.00002933
Iteration 26/1000 | Loss: 0.00002932
Iteration 27/1000 | Loss: 0.00002932
Iteration 28/1000 | Loss: 0.00002931
Iteration 29/1000 | Loss: 0.00002930
Iteration 30/1000 | Loss: 0.00002930
Iteration 31/1000 | Loss: 0.00002928
Iteration 32/1000 | Loss: 0.00002928
Iteration 33/1000 | Loss: 0.00002927
Iteration 34/1000 | Loss: 0.00002927
Iteration 35/1000 | Loss: 0.00002926
Iteration 36/1000 | Loss: 0.00002925
Iteration 37/1000 | Loss: 0.00002924
Iteration 38/1000 | Loss: 0.00002924
Iteration 39/1000 | Loss: 0.00002924
Iteration 40/1000 | Loss: 0.00002923
Iteration 41/1000 | Loss: 0.00002922
Iteration 42/1000 | Loss: 0.00002922
Iteration 43/1000 | Loss: 0.00002921
Iteration 44/1000 | Loss: 0.00002921
Iteration 45/1000 | Loss: 0.00002921
Iteration 46/1000 | Loss: 0.00002920
Iteration 47/1000 | Loss: 0.00002919
Iteration 48/1000 | Loss: 0.00002919
Iteration 49/1000 | Loss: 0.00002917
Iteration 50/1000 | Loss: 0.00002917
Iteration 51/1000 | Loss: 0.00002917
Iteration 52/1000 | Loss: 0.00002916
Iteration 53/1000 | Loss: 0.00002916
Iteration 54/1000 | Loss: 0.00002915
Iteration 55/1000 | Loss: 0.00002915
Iteration 56/1000 | Loss: 0.00002915
Iteration 57/1000 | Loss: 0.00002914
Iteration 58/1000 | Loss: 0.00002914
Iteration 59/1000 | Loss: 0.00002914
Iteration 60/1000 | Loss: 0.00002913
Iteration 61/1000 | Loss: 0.00002913
Iteration 62/1000 | Loss: 0.00002912
Iteration 63/1000 | Loss: 0.00002912
Iteration 64/1000 | Loss: 0.00002911
Iteration 65/1000 | Loss: 0.00002911
Iteration 66/1000 | Loss: 0.00002910
Iteration 67/1000 | Loss: 0.00002910
Iteration 68/1000 | Loss: 0.00002910
Iteration 69/1000 | Loss: 0.00002909
Iteration 70/1000 | Loss: 0.00002909
Iteration 71/1000 | Loss: 0.00002909
Iteration 72/1000 | Loss: 0.00002909
Iteration 73/1000 | Loss: 0.00002909
Iteration 74/1000 | Loss: 0.00002909
Iteration 75/1000 | Loss: 0.00002909
Iteration 76/1000 | Loss: 0.00002909
Iteration 77/1000 | Loss: 0.00002908
Iteration 78/1000 | Loss: 0.00002906
Iteration 79/1000 | Loss: 0.00002906
Iteration 80/1000 | Loss: 0.00002906
Iteration 81/1000 | Loss: 0.00002906
Iteration 82/1000 | Loss: 0.00002906
Iteration 83/1000 | Loss: 0.00002906
Iteration 84/1000 | Loss: 0.00002906
Iteration 85/1000 | Loss: 0.00002905
Iteration 86/1000 | Loss: 0.00002905
Iteration 87/1000 | Loss: 0.00002905
Iteration 88/1000 | Loss: 0.00002905
Iteration 89/1000 | Loss: 0.00002905
Iteration 90/1000 | Loss: 0.00002905
Iteration 91/1000 | Loss: 0.00002905
Iteration 92/1000 | Loss: 0.00002905
Iteration 93/1000 | Loss: 0.00002905
Iteration 94/1000 | Loss: 0.00002905
Iteration 95/1000 | Loss: 0.00002905
Iteration 96/1000 | Loss: 0.00002905
Iteration 97/1000 | Loss: 0.00002904
Iteration 98/1000 | Loss: 0.00002904
Iteration 99/1000 | Loss: 0.00002904
Iteration 100/1000 | Loss: 0.00002904
Iteration 101/1000 | Loss: 0.00002904
Iteration 102/1000 | Loss: 0.00002904
Iteration 103/1000 | Loss: 0.00002904
Iteration 104/1000 | Loss: 0.00002904
Iteration 105/1000 | Loss: 0.00002904
Iteration 106/1000 | Loss: 0.00002904
Iteration 107/1000 | Loss: 0.00002904
Iteration 108/1000 | Loss: 0.00002904
Iteration 109/1000 | Loss: 0.00002903
Iteration 110/1000 | Loss: 0.00002903
Iteration 111/1000 | Loss: 0.00002903
Iteration 112/1000 | Loss: 0.00002903
Iteration 113/1000 | Loss: 0.00002902
Iteration 114/1000 | Loss: 0.00002902
Iteration 115/1000 | Loss: 0.00002902
Iteration 116/1000 | Loss: 0.00002901
Iteration 117/1000 | Loss: 0.00002901
Iteration 118/1000 | Loss: 0.00002901
Iteration 119/1000 | Loss: 0.00002900
Iteration 120/1000 | Loss: 0.00002900
Iteration 121/1000 | Loss: 0.00002900
Iteration 122/1000 | Loss: 0.00002900
Iteration 123/1000 | Loss: 0.00002900
Iteration 124/1000 | Loss: 0.00002900
Iteration 125/1000 | Loss: 0.00002900
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002899
Iteration 128/1000 | Loss: 0.00002899
Iteration 129/1000 | Loss: 0.00002899
Iteration 130/1000 | Loss: 0.00002898
Iteration 131/1000 | Loss: 0.00002898
Iteration 132/1000 | Loss: 0.00002898
Iteration 133/1000 | Loss: 0.00002898
Iteration 134/1000 | Loss: 0.00002897
Iteration 135/1000 | Loss: 0.00002897
Iteration 136/1000 | Loss: 0.00002897
Iteration 137/1000 | Loss: 0.00002897
Iteration 138/1000 | Loss: 0.00002897
Iteration 139/1000 | Loss: 0.00002896
Iteration 140/1000 | Loss: 0.00002896
Iteration 141/1000 | Loss: 0.00002896
Iteration 142/1000 | Loss: 0.00002896
Iteration 143/1000 | Loss: 0.00002895
Iteration 144/1000 | Loss: 0.00002895
Iteration 145/1000 | Loss: 0.00002895
Iteration 146/1000 | Loss: 0.00002895
Iteration 147/1000 | Loss: 0.00002895
Iteration 148/1000 | Loss: 0.00002894
Iteration 149/1000 | Loss: 0.00002894
Iteration 150/1000 | Loss: 0.00002894
Iteration 151/1000 | Loss: 0.00002894
Iteration 152/1000 | Loss: 0.00002894
Iteration 153/1000 | Loss: 0.00002893
Iteration 154/1000 | Loss: 0.00002893
Iteration 155/1000 | Loss: 0.00002893
Iteration 156/1000 | Loss: 0.00002893
Iteration 157/1000 | Loss: 0.00002893
Iteration 158/1000 | Loss: 0.00002893
Iteration 159/1000 | Loss: 0.00002893
Iteration 160/1000 | Loss: 0.00002893
Iteration 161/1000 | Loss: 0.00002893
Iteration 162/1000 | Loss: 0.00002892
Iteration 163/1000 | Loss: 0.00002892
Iteration 164/1000 | Loss: 0.00002892
Iteration 165/1000 | Loss: 0.00002892
Iteration 166/1000 | Loss: 0.00002892
Iteration 167/1000 | Loss: 0.00002892
Iteration 168/1000 | Loss: 0.00002892
Iteration 169/1000 | Loss: 0.00002892
Iteration 170/1000 | Loss: 0.00002892
Iteration 171/1000 | Loss: 0.00002892
Iteration 172/1000 | Loss: 0.00002892
Iteration 173/1000 | Loss: 0.00002892
Iteration 174/1000 | Loss: 0.00002892
Iteration 175/1000 | Loss: 0.00002892
Iteration 176/1000 | Loss: 0.00002892
Iteration 177/1000 | Loss: 0.00002892
Iteration 178/1000 | Loss: 0.00002892
Iteration 179/1000 | Loss: 0.00002892
Iteration 180/1000 | Loss: 0.00002892
Iteration 181/1000 | Loss: 0.00002892
Iteration 182/1000 | Loss: 0.00002892
Iteration 183/1000 | Loss: 0.00002892
Iteration 184/1000 | Loss: 0.00002892
Iteration 185/1000 | Loss: 0.00002892
Iteration 186/1000 | Loss: 0.00002892
Iteration 187/1000 | Loss: 0.00002892
Iteration 188/1000 | Loss: 0.00002892
Iteration 189/1000 | Loss: 0.00002892
Iteration 190/1000 | Loss: 0.00002892
Iteration 191/1000 | Loss: 0.00002892
Iteration 192/1000 | Loss: 0.00002892
Iteration 193/1000 | Loss: 0.00002892
Iteration 194/1000 | Loss: 0.00002892
Iteration 195/1000 | Loss: 0.00002892
Iteration 196/1000 | Loss: 0.00002892
Iteration 197/1000 | Loss: 0.00002892
Iteration 198/1000 | Loss: 0.00002892
Iteration 199/1000 | Loss: 0.00002892
Iteration 200/1000 | Loss: 0.00002892
Iteration 201/1000 | Loss: 0.00002892
Iteration 202/1000 | Loss: 0.00002892
Iteration 203/1000 | Loss: 0.00002892
Iteration 204/1000 | Loss: 0.00002892
Iteration 205/1000 | Loss: 0.00002892
Iteration 206/1000 | Loss: 0.00002892
Iteration 207/1000 | Loss: 0.00002892
Iteration 208/1000 | Loss: 0.00002892
Iteration 209/1000 | Loss: 0.00002892
Iteration 210/1000 | Loss: 0.00002892
Iteration 211/1000 | Loss: 0.00002892
Iteration 212/1000 | Loss: 0.00002892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.89168438030174e-05, 2.89168438030174e-05, 2.89168438030174e-05, 2.89168438030174e-05, 2.89168438030174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.89168438030174e-05

Optimization complete. Final v2v error: 4.460384368896484 mm

Highest mean error: 6.66072940826416 mm for frame 138

Lowest mean error: 3.8457295894622803 mm for frame 136

Saving results

Total time: 71.57682800292969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410470
Iteration 2/25 | Loss: 0.00142708
Iteration 3/25 | Loss: 0.00130863
Iteration 4/25 | Loss: 0.00128974
Iteration 5/25 | Loss: 0.00128427
Iteration 6/25 | Loss: 0.00128260
Iteration 7/25 | Loss: 0.00128228
Iteration 8/25 | Loss: 0.00128228
Iteration 9/25 | Loss: 0.00128228
Iteration 10/25 | Loss: 0.00128228
Iteration 11/25 | Loss: 0.00128228
Iteration 12/25 | Loss: 0.00128228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012822828721255064, 0.0012822828721255064, 0.0012822828721255064, 0.0012822828721255064, 0.0012822828721255064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012822828721255064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37599277
Iteration 2/25 | Loss: 0.00094502
Iteration 3/25 | Loss: 0.00094499
Iteration 4/25 | Loss: 0.00094499
Iteration 5/25 | Loss: 0.00094499
Iteration 6/25 | Loss: 0.00094499
Iteration 7/25 | Loss: 0.00094499
Iteration 8/25 | Loss: 0.00094499
Iteration 9/25 | Loss: 0.00094499
Iteration 10/25 | Loss: 0.00094499
Iteration 11/25 | Loss: 0.00094499
Iteration 12/25 | Loss: 0.00094499
Iteration 13/25 | Loss: 0.00094499
Iteration 14/25 | Loss: 0.00094499
Iteration 15/25 | Loss: 0.00094499
Iteration 16/25 | Loss: 0.00094499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009449907811358571, 0.0009449907811358571, 0.0009449907811358571, 0.0009449907811358571, 0.0009449907811358571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009449907811358571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094499
Iteration 2/1000 | Loss: 0.00004542
Iteration 3/1000 | Loss: 0.00002945
Iteration 4/1000 | Loss: 0.00002458
Iteration 5/1000 | Loss: 0.00002267
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002022
Iteration 8/1000 | Loss: 0.00001953
Iteration 9/1000 | Loss: 0.00001897
Iteration 10/1000 | Loss: 0.00001867
Iteration 11/1000 | Loss: 0.00001842
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001803
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001800
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001799
Iteration 20/1000 | Loss: 0.00001798
Iteration 21/1000 | Loss: 0.00001797
Iteration 22/1000 | Loss: 0.00001797
Iteration 23/1000 | Loss: 0.00001796
Iteration 24/1000 | Loss: 0.00001796
Iteration 25/1000 | Loss: 0.00001796
Iteration 26/1000 | Loss: 0.00001796
Iteration 27/1000 | Loss: 0.00001796
Iteration 28/1000 | Loss: 0.00001795
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001795
Iteration 32/1000 | Loss: 0.00001795
Iteration 33/1000 | Loss: 0.00001794
Iteration 34/1000 | Loss: 0.00001794
Iteration 35/1000 | Loss: 0.00001794
Iteration 36/1000 | Loss: 0.00001794
Iteration 37/1000 | Loss: 0.00001794
Iteration 38/1000 | Loss: 0.00001793
Iteration 39/1000 | Loss: 0.00001793
Iteration 40/1000 | Loss: 0.00001793
Iteration 41/1000 | Loss: 0.00001793
Iteration 42/1000 | Loss: 0.00001793
Iteration 43/1000 | Loss: 0.00001792
Iteration 44/1000 | Loss: 0.00001792
Iteration 45/1000 | Loss: 0.00001792
Iteration 46/1000 | Loss: 0.00001791
Iteration 47/1000 | Loss: 0.00001791
Iteration 48/1000 | Loss: 0.00001791
Iteration 49/1000 | Loss: 0.00001790
Iteration 50/1000 | Loss: 0.00001790
Iteration 51/1000 | Loss: 0.00001789
Iteration 52/1000 | Loss: 0.00001789
Iteration 53/1000 | Loss: 0.00001789
Iteration 54/1000 | Loss: 0.00001788
Iteration 55/1000 | Loss: 0.00001788
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001777
Iteration 64/1000 | Loss: 0.00001777
Iteration 65/1000 | Loss: 0.00001776
Iteration 66/1000 | Loss: 0.00001775
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001772
Iteration 72/1000 | Loss: 0.00001772
Iteration 73/1000 | Loss: 0.00001772
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001768
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001767
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001766
Iteration 90/1000 | Loss: 0.00001766
Iteration 91/1000 | Loss: 0.00001766
Iteration 92/1000 | Loss: 0.00001765
Iteration 93/1000 | Loss: 0.00001765
Iteration 94/1000 | Loss: 0.00001764
Iteration 95/1000 | Loss: 0.00001764
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001762
Iteration 104/1000 | Loss: 0.00001762
Iteration 105/1000 | Loss: 0.00001761
Iteration 106/1000 | Loss: 0.00001761
Iteration 107/1000 | Loss: 0.00001761
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001759
Iteration 113/1000 | Loss: 0.00001759
Iteration 114/1000 | Loss: 0.00001759
Iteration 115/1000 | Loss: 0.00001759
Iteration 116/1000 | Loss: 0.00001758
Iteration 117/1000 | Loss: 0.00001758
Iteration 118/1000 | Loss: 0.00001758
Iteration 119/1000 | Loss: 0.00001758
Iteration 120/1000 | Loss: 0.00001758
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001757
Iteration 123/1000 | Loss: 0.00001757
Iteration 124/1000 | Loss: 0.00001757
Iteration 125/1000 | Loss: 0.00001757
Iteration 126/1000 | Loss: 0.00001757
Iteration 127/1000 | Loss: 0.00001757
Iteration 128/1000 | Loss: 0.00001757
Iteration 129/1000 | Loss: 0.00001757
Iteration 130/1000 | Loss: 0.00001757
Iteration 131/1000 | Loss: 0.00001756
Iteration 132/1000 | Loss: 0.00001756
Iteration 133/1000 | Loss: 0.00001756
Iteration 134/1000 | Loss: 0.00001755
Iteration 135/1000 | Loss: 0.00001755
Iteration 136/1000 | Loss: 0.00001755
Iteration 137/1000 | Loss: 0.00001755
Iteration 138/1000 | Loss: 0.00001755
Iteration 139/1000 | Loss: 0.00001755
Iteration 140/1000 | Loss: 0.00001754
Iteration 141/1000 | Loss: 0.00001754
Iteration 142/1000 | Loss: 0.00001754
Iteration 143/1000 | Loss: 0.00001754
Iteration 144/1000 | Loss: 0.00001754
Iteration 145/1000 | Loss: 0.00001754
Iteration 146/1000 | Loss: 0.00001754
Iteration 147/1000 | Loss: 0.00001754
Iteration 148/1000 | Loss: 0.00001754
Iteration 149/1000 | Loss: 0.00001754
Iteration 150/1000 | Loss: 0.00001754
Iteration 151/1000 | Loss: 0.00001753
Iteration 152/1000 | Loss: 0.00001753
Iteration 153/1000 | Loss: 0.00001753
Iteration 154/1000 | Loss: 0.00001753
Iteration 155/1000 | Loss: 0.00001753
Iteration 156/1000 | Loss: 0.00001753
Iteration 157/1000 | Loss: 0.00001753
Iteration 158/1000 | Loss: 0.00001753
Iteration 159/1000 | Loss: 0.00001753
Iteration 160/1000 | Loss: 0.00001753
Iteration 161/1000 | Loss: 0.00001753
Iteration 162/1000 | Loss: 0.00001753
Iteration 163/1000 | Loss: 0.00001752
Iteration 164/1000 | Loss: 0.00001752
Iteration 165/1000 | Loss: 0.00001752
Iteration 166/1000 | Loss: 0.00001752
Iteration 167/1000 | Loss: 0.00001752
Iteration 168/1000 | Loss: 0.00001752
Iteration 169/1000 | Loss: 0.00001752
Iteration 170/1000 | Loss: 0.00001752
Iteration 171/1000 | Loss: 0.00001752
Iteration 172/1000 | Loss: 0.00001752
Iteration 173/1000 | Loss: 0.00001752
Iteration 174/1000 | Loss: 0.00001752
Iteration 175/1000 | Loss: 0.00001752
Iteration 176/1000 | Loss: 0.00001752
Iteration 177/1000 | Loss: 0.00001751
Iteration 178/1000 | Loss: 0.00001751
Iteration 179/1000 | Loss: 0.00001751
Iteration 180/1000 | Loss: 0.00001751
Iteration 181/1000 | Loss: 0.00001751
Iteration 182/1000 | Loss: 0.00001751
Iteration 183/1000 | Loss: 0.00001751
Iteration 184/1000 | Loss: 0.00001751
Iteration 185/1000 | Loss: 0.00001751
Iteration 186/1000 | Loss: 0.00001751
Iteration 187/1000 | Loss: 0.00001751
Iteration 188/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.7511745681986213e-05, 1.7511745681986213e-05, 1.7511745681986213e-05, 1.7511745681986213e-05, 1.7511745681986213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7511745681986213e-05

Optimization complete. Final v2v error: 3.439178228378296 mm

Highest mean error: 5.43184232711792 mm for frame 85

Lowest mean error: 2.851339101791382 mm for frame 18

Saving results

Total time: 42.772297859191895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602874
Iteration 2/25 | Loss: 0.00174545
Iteration 3/25 | Loss: 0.00128735
Iteration 4/25 | Loss: 0.00126304
Iteration 5/25 | Loss: 0.00125925
Iteration 6/25 | Loss: 0.00125742
Iteration 7/25 | Loss: 0.00125734
Iteration 8/25 | Loss: 0.00125734
Iteration 9/25 | Loss: 0.00125734
Iteration 10/25 | Loss: 0.00125734
Iteration 11/25 | Loss: 0.00125734
Iteration 12/25 | Loss: 0.00125734
Iteration 13/25 | Loss: 0.00125734
Iteration 14/25 | Loss: 0.00125734
Iteration 15/25 | Loss: 0.00125734
Iteration 16/25 | Loss: 0.00125734
Iteration 17/25 | Loss: 0.00125734
Iteration 18/25 | Loss: 0.00125734
Iteration 19/25 | Loss: 0.00125734
Iteration 20/25 | Loss: 0.00125734
Iteration 21/25 | Loss: 0.00125734
Iteration 22/25 | Loss: 0.00125734
Iteration 23/25 | Loss: 0.00125734
Iteration 24/25 | Loss: 0.00125734
Iteration 25/25 | Loss: 0.00125734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02169752
Iteration 2/25 | Loss: 0.00092059
Iteration 3/25 | Loss: 0.00092057
Iteration 4/25 | Loss: 0.00092057
Iteration 5/25 | Loss: 0.00092057
Iteration 6/25 | Loss: 0.00092057
Iteration 7/25 | Loss: 0.00092057
Iteration 8/25 | Loss: 0.00092057
Iteration 9/25 | Loss: 0.00092057
Iteration 10/25 | Loss: 0.00092057
Iteration 11/25 | Loss: 0.00092057
Iteration 12/25 | Loss: 0.00092057
Iteration 13/25 | Loss: 0.00092057
Iteration 14/25 | Loss: 0.00092057
Iteration 15/25 | Loss: 0.00092057
Iteration 16/25 | Loss: 0.00092057
Iteration 17/25 | Loss: 0.00092057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009205707465298474, 0.0009205707465298474, 0.0009205707465298474, 0.0009205707465298474, 0.0009205707465298474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009205707465298474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092057
Iteration 2/1000 | Loss: 0.00008201
Iteration 3/1000 | Loss: 0.00005245
Iteration 4/1000 | Loss: 0.00004079
Iteration 5/1000 | Loss: 0.00003814
Iteration 6/1000 | Loss: 0.00003701
Iteration 7/1000 | Loss: 0.00003642
Iteration 8/1000 | Loss: 0.00003571
Iteration 9/1000 | Loss: 0.00003510
Iteration 10/1000 | Loss: 0.00003460
Iteration 11/1000 | Loss: 0.00003423
Iteration 12/1000 | Loss: 0.00003388
Iteration 13/1000 | Loss: 0.00003349
Iteration 14/1000 | Loss: 0.00003309
Iteration 15/1000 | Loss: 0.00003275
Iteration 16/1000 | Loss: 0.00003249
Iteration 17/1000 | Loss: 0.00003232
Iteration 18/1000 | Loss: 0.00003215
Iteration 19/1000 | Loss: 0.00003198
Iteration 20/1000 | Loss: 0.00003187
Iteration 21/1000 | Loss: 0.00003179
Iteration 22/1000 | Loss: 0.00003179
Iteration 23/1000 | Loss: 0.00003173
Iteration 24/1000 | Loss: 0.00003165
Iteration 25/1000 | Loss: 0.00003164
Iteration 26/1000 | Loss: 0.00003163
Iteration 27/1000 | Loss: 0.00003163
Iteration 28/1000 | Loss: 0.00003162
Iteration 29/1000 | Loss: 0.00003162
Iteration 30/1000 | Loss: 0.00003162
Iteration 31/1000 | Loss: 0.00003161
Iteration 32/1000 | Loss: 0.00003160
Iteration 33/1000 | Loss: 0.00003158
Iteration 34/1000 | Loss: 0.00003158
Iteration 35/1000 | Loss: 0.00003158
Iteration 36/1000 | Loss: 0.00003158
Iteration 37/1000 | Loss: 0.00003157
Iteration 38/1000 | Loss: 0.00003157
Iteration 39/1000 | Loss: 0.00003157
Iteration 40/1000 | Loss: 0.00003157
Iteration 41/1000 | Loss: 0.00003157
Iteration 42/1000 | Loss: 0.00003157
Iteration 43/1000 | Loss: 0.00003157
Iteration 44/1000 | Loss: 0.00003157
Iteration 45/1000 | Loss: 0.00003157
Iteration 46/1000 | Loss: 0.00003157
Iteration 47/1000 | Loss: 0.00003156
Iteration 48/1000 | Loss: 0.00003156
Iteration 49/1000 | Loss: 0.00003156
Iteration 50/1000 | Loss: 0.00003156
Iteration 51/1000 | Loss: 0.00003156
Iteration 52/1000 | Loss: 0.00003156
Iteration 53/1000 | Loss: 0.00003155
Iteration 54/1000 | Loss: 0.00003154
Iteration 55/1000 | Loss: 0.00003154
Iteration 56/1000 | Loss: 0.00003154
Iteration 57/1000 | Loss: 0.00003154
Iteration 58/1000 | Loss: 0.00003154
Iteration 59/1000 | Loss: 0.00003154
Iteration 60/1000 | Loss: 0.00003154
Iteration 61/1000 | Loss: 0.00003154
Iteration 62/1000 | Loss: 0.00003154
Iteration 63/1000 | Loss: 0.00003154
Iteration 64/1000 | Loss: 0.00003153
Iteration 65/1000 | Loss: 0.00003152
Iteration 66/1000 | Loss: 0.00003152
Iteration 67/1000 | Loss: 0.00003152
Iteration 68/1000 | Loss: 0.00003152
Iteration 69/1000 | Loss: 0.00003152
Iteration 70/1000 | Loss: 0.00003152
Iteration 71/1000 | Loss: 0.00003152
Iteration 72/1000 | Loss: 0.00003152
Iteration 73/1000 | Loss: 0.00003152
Iteration 74/1000 | Loss: 0.00003152
Iteration 75/1000 | Loss: 0.00003152
Iteration 76/1000 | Loss: 0.00003151
Iteration 77/1000 | Loss: 0.00003151
Iteration 78/1000 | Loss: 0.00003151
Iteration 79/1000 | Loss: 0.00003151
Iteration 80/1000 | Loss: 0.00003151
Iteration 81/1000 | Loss: 0.00003151
Iteration 82/1000 | Loss: 0.00003151
Iteration 83/1000 | Loss: 0.00003151
Iteration 84/1000 | Loss: 0.00003151
Iteration 85/1000 | Loss: 0.00003151
Iteration 86/1000 | Loss: 0.00003151
Iteration 87/1000 | Loss: 0.00003151
Iteration 88/1000 | Loss: 0.00003150
Iteration 89/1000 | Loss: 0.00003150
Iteration 90/1000 | Loss: 0.00003150
Iteration 91/1000 | Loss: 0.00003150
Iteration 92/1000 | Loss: 0.00003150
Iteration 93/1000 | Loss: 0.00003149
Iteration 94/1000 | Loss: 0.00003149
Iteration 95/1000 | Loss: 0.00003149
Iteration 96/1000 | Loss: 0.00003149
Iteration 97/1000 | Loss: 0.00003149
Iteration 98/1000 | Loss: 0.00003149
Iteration 99/1000 | Loss: 0.00003149
Iteration 100/1000 | Loss: 0.00003149
Iteration 101/1000 | Loss: 0.00003149
Iteration 102/1000 | Loss: 0.00003148
Iteration 103/1000 | Loss: 0.00003147
Iteration 104/1000 | Loss: 0.00003147
Iteration 105/1000 | Loss: 0.00003146
Iteration 106/1000 | Loss: 0.00003146
Iteration 107/1000 | Loss: 0.00003146
Iteration 108/1000 | Loss: 0.00003146
Iteration 109/1000 | Loss: 0.00003145
Iteration 110/1000 | Loss: 0.00003145
Iteration 111/1000 | Loss: 0.00003145
Iteration 112/1000 | Loss: 0.00003145
Iteration 113/1000 | Loss: 0.00003145
Iteration 114/1000 | Loss: 0.00003145
Iteration 115/1000 | Loss: 0.00003144
Iteration 116/1000 | Loss: 0.00003144
Iteration 117/1000 | Loss: 0.00003144
Iteration 118/1000 | Loss: 0.00003144
Iteration 119/1000 | Loss: 0.00003144
Iteration 120/1000 | Loss: 0.00003143
Iteration 121/1000 | Loss: 0.00003143
Iteration 122/1000 | Loss: 0.00003143
Iteration 123/1000 | Loss: 0.00003143
Iteration 124/1000 | Loss: 0.00003143
Iteration 125/1000 | Loss: 0.00003142
Iteration 126/1000 | Loss: 0.00003142
Iteration 127/1000 | Loss: 0.00003142
Iteration 128/1000 | Loss: 0.00003142
Iteration 129/1000 | Loss: 0.00003141
Iteration 130/1000 | Loss: 0.00003141
Iteration 131/1000 | Loss: 0.00003141
Iteration 132/1000 | Loss: 0.00003141
Iteration 133/1000 | Loss: 0.00003141
Iteration 134/1000 | Loss: 0.00003141
Iteration 135/1000 | Loss: 0.00003141
Iteration 136/1000 | Loss: 0.00003141
Iteration 137/1000 | Loss: 0.00003141
Iteration 138/1000 | Loss: 0.00003141
Iteration 139/1000 | Loss: 0.00003141
Iteration 140/1000 | Loss: 0.00003141
Iteration 141/1000 | Loss: 0.00003141
Iteration 142/1000 | Loss: 0.00003141
Iteration 143/1000 | Loss: 0.00003141
Iteration 144/1000 | Loss: 0.00003141
Iteration 145/1000 | Loss: 0.00003140
Iteration 146/1000 | Loss: 0.00003140
Iteration 147/1000 | Loss: 0.00003140
Iteration 148/1000 | Loss: 0.00003140
Iteration 149/1000 | Loss: 0.00003140
Iteration 150/1000 | Loss: 0.00003140
Iteration 151/1000 | Loss: 0.00003140
Iteration 152/1000 | Loss: 0.00003140
Iteration 153/1000 | Loss: 0.00003140
Iteration 154/1000 | Loss: 0.00003140
Iteration 155/1000 | Loss: 0.00003140
Iteration 156/1000 | Loss: 0.00003140
Iteration 157/1000 | Loss: 0.00003140
Iteration 158/1000 | Loss: 0.00003140
Iteration 159/1000 | Loss: 0.00003139
Iteration 160/1000 | Loss: 0.00003139
Iteration 161/1000 | Loss: 0.00003139
Iteration 162/1000 | Loss: 0.00003139
Iteration 163/1000 | Loss: 0.00003139
Iteration 164/1000 | Loss: 0.00003139
Iteration 165/1000 | Loss: 0.00003139
Iteration 166/1000 | Loss: 0.00003139
Iteration 167/1000 | Loss: 0.00003139
Iteration 168/1000 | Loss: 0.00003139
Iteration 169/1000 | Loss: 0.00003139
Iteration 170/1000 | Loss: 0.00003139
Iteration 171/1000 | Loss: 0.00003139
Iteration 172/1000 | Loss: 0.00003139
Iteration 173/1000 | Loss: 0.00003139
Iteration 174/1000 | Loss: 0.00003139
Iteration 175/1000 | Loss: 0.00003139
Iteration 176/1000 | Loss: 0.00003139
Iteration 177/1000 | Loss: 0.00003139
Iteration 178/1000 | Loss: 0.00003139
Iteration 179/1000 | Loss: 0.00003139
Iteration 180/1000 | Loss: 0.00003139
Iteration 181/1000 | Loss: 0.00003139
Iteration 182/1000 | Loss: 0.00003139
Iteration 183/1000 | Loss: 0.00003139
Iteration 184/1000 | Loss: 0.00003139
Iteration 185/1000 | Loss: 0.00003139
Iteration 186/1000 | Loss: 0.00003139
Iteration 187/1000 | Loss: 0.00003139
Iteration 188/1000 | Loss: 0.00003139
Iteration 189/1000 | Loss: 0.00003139
Iteration 190/1000 | Loss: 0.00003139
Iteration 191/1000 | Loss: 0.00003139
Iteration 192/1000 | Loss: 0.00003139
Iteration 193/1000 | Loss: 0.00003139
Iteration 194/1000 | Loss: 0.00003139
Iteration 195/1000 | Loss: 0.00003139
Iteration 196/1000 | Loss: 0.00003139
Iteration 197/1000 | Loss: 0.00003139
Iteration 198/1000 | Loss: 0.00003139
Iteration 199/1000 | Loss: 0.00003139
Iteration 200/1000 | Loss: 0.00003139
Iteration 201/1000 | Loss: 0.00003139
Iteration 202/1000 | Loss: 0.00003139
Iteration 203/1000 | Loss: 0.00003139
Iteration 204/1000 | Loss: 0.00003139
Iteration 205/1000 | Loss: 0.00003139
Iteration 206/1000 | Loss: 0.00003139
Iteration 207/1000 | Loss: 0.00003139
Iteration 208/1000 | Loss: 0.00003139
Iteration 209/1000 | Loss: 0.00003139
Iteration 210/1000 | Loss: 0.00003139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [3.138681859127246e-05, 3.138681859127246e-05, 3.138681859127246e-05, 3.138681859127246e-05, 3.138681859127246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.138681859127246e-05

Optimization complete. Final v2v error: 4.197788715362549 mm

Highest mean error: 5.3242716789245605 mm for frame 149

Lowest mean error: 3.132159948348999 mm for frame 49

Saving results

Total time: 52.69089698791504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461347
Iteration 2/25 | Loss: 0.00121114
Iteration 3/25 | Loss: 0.00111867
Iteration 4/25 | Loss: 0.00110743
Iteration 5/25 | Loss: 0.00110547
Iteration 6/25 | Loss: 0.00110547
Iteration 7/25 | Loss: 0.00110547
Iteration 8/25 | Loss: 0.00110547
Iteration 9/25 | Loss: 0.00110547
Iteration 10/25 | Loss: 0.00110547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001105474540963769, 0.001105474540963769, 0.001105474540963769, 0.001105474540963769, 0.001105474540963769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105474540963769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87089157
Iteration 2/25 | Loss: 0.00076690
Iteration 3/25 | Loss: 0.00076689
Iteration 4/25 | Loss: 0.00076689
Iteration 5/25 | Loss: 0.00076689
Iteration 6/25 | Loss: 0.00076689
Iteration 7/25 | Loss: 0.00076689
Iteration 8/25 | Loss: 0.00076689
Iteration 9/25 | Loss: 0.00076689
Iteration 10/25 | Loss: 0.00076689
Iteration 11/25 | Loss: 0.00076689
Iteration 12/25 | Loss: 0.00076689
Iteration 13/25 | Loss: 0.00076689
Iteration 14/25 | Loss: 0.00076689
Iteration 15/25 | Loss: 0.00076689
Iteration 16/25 | Loss: 0.00076689
Iteration 17/25 | Loss: 0.00076689
Iteration 18/25 | Loss: 0.00076689
Iteration 19/25 | Loss: 0.00076689
Iteration 20/25 | Loss: 0.00076689
Iteration 21/25 | Loss: 0.00076689
Iteration 22/25 | Loss: 0.00076689
Iteration 23/25 | Loss: 0.00076689
Iteration 24/25 | Loss: 0.00076689
Iteration 25/25 | Loss: 0.00076689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076689
Iteration 2/1000 | Loss: 0.00001792
Iteration 3/1000 | Loss: 0.00001497
Iteration 4/1000 | Loss: 0.00001396
Iteration 5/1000 | Loss: 0.00001325
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001194
Iteration 11/1000 | Loss: 0.00001187
Iteration 12/1000 | Loss: 0.00001186
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001170
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001149
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001136
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001136
Iteration 35/1000 | Loss: 0.00001136
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001127
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001126
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001125
Iteration 51/1000 | Loss: 0.00001125
Iteration 52/1000 | Loss: 0.00001124
Iteration 53/1000 | Loss: 0.00001124
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001123
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001122
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001118
Iteration 65/1000 | Loss: 0.00001118
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001112
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001107
Iteration 97/1000 | Loss: 0.00001107
Iteration 98/1000 | Loss: 0.00001107
Iteration 99/1000 | Loss: 0.00001107
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001106
Iteration 107/1000 | Loss: 0.00001106
Iteration 108/1000 | Loss: 0.00001106
Iteration 109/1000 | Loss: 0.00001106
Iteration 110/1000 | Loss: 0.00001106
Iteration 111/1000 | Loss: 0.00001106
Iteration 112/1000 | Loss: 0.00001106
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001106
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001104
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001104
Iteration 135/1000 | Loss: 0.00001104
Iteration 136/1000 | Loss: 0.00001103
Iteration 137/1000 | Loss: 0.00001103
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001103
Iteration 141/1000 | Loss: 0.00001103
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001103
Iteration 146/1000 | Loss: 0.00001103
Iteration 147/1000 | Loss: 0.00001103
Iteration 148/1000 | Loss: 0.00001103
Iteration 149/1000 | Loss: 0.00001103
Iteration 150/1000 | Loss: 0.00001103
Iteration 151/1000 | Loss: 0.00001103
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001103
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001103
Iteration 157/1000 | Loss: 0.00001103
Iteration 158/1000 | Loss: 0.00001103
Iteration 159/1000 | Loss: 0.00001103
Iteration 160/1000 | Loss: 0.00001103
Iteration 161/1000 | Loss: 0.00001103
Iteration 162/1000 | Loss: 0.00001103
Iteration 163/1000 | Loss: 0.00001103
Iteration 164/1000 | Loss: 0.00001103
Iteration 165/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.1025535968656186e-05, 1.1025535968656186e-05, 1.1025535968656186e-05, 1.1025535968656186e-05, 1.1025535968656186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1025535968656186e-05

Optimization complete. Final v2v error: 2.83739972114563 mm

Highest mean error: 3.0701029300689697 mm for frame 121

Lowest mean error: 2.690776824951172 mm for frame 160

Saving results

Total time: 43.43012833595276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739953
Iteration 2/25 | Loss: 0.00202104
Iteration 3/25 | Loss: 0.00161478
Iteration 4/25 | Loss: 0.00156298
Iteration 5/25 | Loss: 0.00150560
Iteration 6/25 | Loss: 0.00147300
Iteration 7/25 | Loss: 0.00146137
Iteration 8/25 | Loss: 0.00145645
Iteration 9/25 | Loss: 0.00145961
Iteration 10/25 | Loss: 0.00146952
Iteration 11/25 | Loss: 0.00142378
Iteration 12/25 | Loss: 0.00140845
Iteration 13/25 | Loss: 0.00141270
Iteration 14/25 | Loss: 0.00142950
Iteration 15/25 | Loss: 0.00146739
Iteration 16/25 | Loss: 0.00142994
Iteration 17/25 | Loss: 0.00136683
Iteration 18/25 | Loss: 0.00137686
Iteration 19/25 | Loss: 0.00136008
Iteration 20/25 | Loss: 0.00136000
Iteration 21/25 | Loss: 0.00135993
Iteration 22/25 | Loss: 0.00135995
Iteration 23/25 | Loss: 0.00135989
Iteration 24/25 | Loss: 0.00135995
Iteration 25/25 | Loss: 0.00135940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66561890
Iteration 2/25 | Loss: 0.00155864
Iteration 3/25 | Loss: 0.00155833
Iteration 4/25 | Loss: 0.00155833
Iteration 5/25 | Loss: 0.00155833
Iteration 6/25 | Loss: 0.00155833
Iteration 7/25 | Loss: 0.00155833
Iteration 8/25 | Loss: 0.00155833
Iteration 9/25 | Loss: 0.00155833
Iteration 10/25 | Loss: 0.00155833
Iteration 11/25 | Loss: 0.00155833
Iteration 12/25 | Loss: 0.00155833
Iteration 13/25 | Loss: 0.00155833
Iteration 14/25 | Loss: 0.00155833
Iteration 15/25 | Loss: 0.00155833
Iteration 16/25 | Loss: 0.00155833
Iteration 17/25 | Loss: 0.00155833
Iteration 18/25 | Loss: 0.00155833
Iteration 19/25 | Loss: 0.00155833
Iteration 20/25 | Loss: 0.00155833
Iteration 21/25 | Loss: 0.00155833
Iteration 22/25 | Loss: 0.00155833
Iteration 23/25 | Loss: 0.00155833
Iteration 24/25 | Loss: 0.00155833
Iteration 25/25 | Loss: 0.00155833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155833
Iteration 2/1000 | Loss: 0.00282176
Iteration 3/1000 | Loss: 0.00010886
Iteration 4/1000 | Loss: 0.00007274
Iteration 5/1000 | Loss: 0.00005682
Iteration 6/1000 | Loss: 0.00004875
Iteration 7/1000 | Loss: 0.00004659
Iteration 8/1000 | Loss: 0.00004213
Iteration 9/1000 | Loss: 0.00003943
Iteration 10/1000 | Loss: 0.00004146
Iteration 11/1000 | Loss: 0.00003863
Iteration 12/1000 | Loss: 0.00004013
Iteration 13/1000 | Loss: 0.00003672
Iteration 14/1000 | Loss: 0.00004006
Iteration 15/1000 | Loss: 0.00003969
Iteration 16/1000 | Loss: 0.00003847
Iteration 17/1000 | Loss: 0.00004172
Iteration 18/1000 | Loss: 0.00003674
Iteration 19/1000 | Loss: 0.00003536
Iteration 20/1000 | Loss: 0.00003583
Iteration 21/1000 | Loss: 0.00003722
Iteration 22/1000 | Loss: 0.00003954
Iteration 23/1000 | Loss: 0.00003709
Iteration 24/1000 | Loss: 0.00003891
Iteration 25/1000 | Loss: 0.00003680
Iteration 26/1000 | Loss: 0.00003960
Iteration 27/1000 | Loss: 0.00003768
Iteration 28/1000 | Loss: 0.00003752
Iteration 29/1000 | Loss: 0.00003674
Iteration 30/1000 | Loss: 0.00003691
Iteration 31/1000 | Loss: 0.00003635
Iteration 32/1000 | Loss: 0.00003777
Iteration 33/1000 | Loss: 0.00003754
Iteration 34/1000 | Loss: 0.00003534
Iteration 35/1000 | Loss: 0.00003708
Iteration 36/1000 | Loss: 0.00003694
Iteration 37/1000 | Loss: 0.00003604
Iteration 38/1000 | Loss: 0.00003690
Iteration 39/1000 | Loss: 0.00003736
Iteration 40/1000 | Loss: 0.00003707
Iteration 41/1000 | Loss: 0.00003740
Iteration 42/1000 | Loss: 0.00003740
Iteration 43/1000 | Loss: 0.00004665
Iteration 44/1000 | Loss: 0.00004195
Iteration 45/1000 | Loss: 0.00003790
Iteration 46/1000 | Loss: 0.00003681
Iteration 47/1000 | Loss: 0.00004953
Iteration 48/1000 | Loss: 0.00004364
Iteration 49/1000 | Loss: 0.00004586
Iteration 50/1000 | Loss: 0.00004122
Iteration 51/1000 | Loss: 0.00004946
Iteration 52/1000 | Loss: 0.00004399
Iteration 53/1000 | Loss: 0.00004579
Iteration 54/1000 | Loss: 0.00003523
Iteration 55/1000 | Loss: 0.00003418
Iteration 56/1000 | Loss: 0.00003372
Iteration 57/1000 | Loss: 0.00003330
Iteration 58/1000 | Loss: 0.00003310
Iteration 59/1000 | Loss: 0.00003294
Iteration 60/1000 | Loss: 0.00003293
Iteration 61/1000 | Loss: 0.00003293
Iteration 62/1000 | Loss: 0.00003292
Iteration 63/1000 | Loss: 0.00003292
Iteration 64/1000 | Loss: 0.00003291
Iteration 65/1000 | Loss: 0.00003289
Iteration 66/1000 | Loss: 0.00003289
Iteration 67/1000 | Loss: 0.00003285
Iteration 68/1000 | Loss: 0.00003280
Iteration 69/1000 | Loss: 0.00003280
Iteration 70/1000 | Loss: 0.00003278
Iteration 71/1000 | Loss: 0.00003276
Iteration 72/1000 | Loss: 0.00003276
Iteration 73/1000 | Loss: 0.00003276
Iteration 74/1000 | Loss: 0.00003275
Iteration 75/1000 | Loss: 0.00003275
Iteration 76/1000 | Loss: 0.00003272
Iteration 77/1000 | Loss: 0.00003272
Iteration 78/1000 | Loss: 0.00003272
Iteration 79/1000 | Loss: 0.00003271
Iteration 80/1000 | Loss: 0.00003271
Iteration 81/1000 | Loss: 0.00003270
Iteration 82/1000 | Loss: 0.00003269
Iteration 83/1000 | Loss: 0.00003269
Iteration 84/1000 | Loss: 0.00003267
Iteration 85/1000 | Loss: 0.00003267
Iteration 86/1000 | Loss: 0.00003266
Iteration 87/1000 | Loss: 0.00003266
Iteration 88/1000 | Loss: 0.00003265
Iteration 89/1000 | Loss: 0.00003264
Iteration 90/1000 | Loss: 0.00003264
Iteration 91/1000 | Loss: 0.00003263
Iteration 92/1000 | Loss: 0.00003263
Iteration 93/1000 | Loss: 0.00003263
Iteration 94/1000 | Loss: 0.00003263
Iteration 95/1000 | Loss: 0.00003263
Iteration 96/1000 | Loss: 0.00003263
Iteration 97/1000 | Loss: 0.00003263
Iteration 98/1000 | Loss: 0.00003263
Iteration 99/1000 | Loss: 0.00003263
Iteration 100/1000 | Loss: 0.00003263
Iteration 101/1000 | Loss: 0.00003262
Iteration 102/1000 | Loss: 0.00003262
Iteration 103/1000 | Loss: 0.00003262
Iteration 104/1000 | Loss: 0.00003261
Iteration 105/1000 | Loss: 0.00003261
Iteration 106/1000 | Loss: 0.00003261
Iteration 107/1000 | Loss: 0.00003261
Iteration 108/1000 | Loss: 0.00003261
Iteration 109/1000 | Loss: 0.00003261
Iteration 110/1000 | Loss: 0.00003261
Iteration 111/1000 | Loss: 0.00003261
Iteration 112/1000 | Loss: 0.00003261
Iteration 113/1000 | Loss: 0.00003260
Iteration 114/1000 | Loss: 0.00003260
Iteration 115/1000 | Loss: 0.00003260
Iteration 116/1000 | Loss: 0.00003260
Iteration 117/1000 | Loss: 0.00003260
Iteration 118/1000 | Loss: 0.00003259
Iteration 119/1000 | Loss: 0.00003259
Iteration 120/1000 | Loss: 0.00003259
Iteration 121/1000 | Loss: 0.00003259
Iteration 122/1000 | Loss: 0.00003259
Iteration 123/1000 | Loss: 0.00003259
Iteration 124/1000 | Loss: 0.00003259
Iteration 125/1000 | Loss: 0.00003259
Iteration 126/1000 | Loss: 0.00003258
Iteration 127/1000 | Loss: 0.00003258
Iteration 128/1000 | Loss: 0.00003258
Iteration 129/1000 | Loss: 0.00003258
Iteration 130/1000 | Loss: 0.00003258
Iteration 131/1000 | Loss: 0.00003258
Iteration 132/1000 | Loss: 0.00003258
Iteration 133/1000 | Loss: 0.00003258
Iteration 134/1000 | Loss: 0.00003258
Iteration 135/1000 | Loss: 0.00003258
Iteration 136/1000 | Loss: 0.00003258
Iteration 137/1000 | Loss: 0.00003258
Iteration 138/1000 | Loss: 0.00003258
Iteration 139/1000 | Loss: 0.00003258
Iteration 140/1000 | Loss: 0.00003258
Iteration 141/1000 | Loss: 0.00003257
Iteration 142/1000 | Loss: 0.00003257
Iteration 143/1000 | Loss: 0.00003257
Iteration 144/1000 | Loss: 0.00003257
Iteration 145/1000 | Loss: 0.00003257
Iteration 146/1000 | Loss: 0.00003257
Iteration 147/1000 | Loss: 0.00003257
Iteration 148/1000 | Loss: 0.00003257
Iteration 149/1000 | Loss: 0.00003257
Iteration 150/1000 | Loss: 0.00003257
Iteration 151/1000 | Loss: 0.00003257
Iteration 152/1000 | Loss: 0.00003257
Iteration 153/1000 | Loss: 0.00003257
Iteration 154/1000 | Loss: 0.00003257
Iteration 155/1000 | Loss: 0.00003257
Iteration 156/1000 | Loss: 0.00003256
Iteration 157/1000 | Loss: 0.00003256
Iteration 158/1000 | Loss: 0.00003256
Iteration 159/1000 | Loss: 0.00003256
Iteration 160/1000 | Loss: 0.00003256
Iteration 161/1000 | Loss: 0.00003256
Iteration 162/1000 | Loss: 0.00003255
Iteration 163/1000 | Loss: 0.00003255
Iteration 164/1000 | Loss: 0.00003255
Iteration 165/1000 | Loss: 0.00003255
Iteration 166/1000 | Loss: 0.00003255
Iteration 167/1000 | Loss: 0.00003255
Iteration 168/1000 | Loss: 0.00003255
Iteration 169/1000 | Loss: 0.00003255
Iteration 170/1000 | Loss: 0.00003255
Iteration 171/1000 | Loss: 0.00003255
Iteration 172/1000 | Loss: 0.00003255
Iteration 173/1000 | Loss: 0.00003255
Iteration 174/1000 | Loss: 0.00003255
Iteration 175/1000 | Loss: 0.00003255
Iteration 176/1000 | Loss: 0.00003255
Iteration 177/1000 | Loss: 0.00003255
Iteration 178/1000 | Loss: 0.00003254
Iteration 179/1000 | Loss: 0.00003254
Iteration 180/1000 | Loss: 0.00003254
Iteration 181/1000 | Loss: 0.00003254
Iteration 182/1000 | Loss: 0.00003254
Iteration 183/1000 | Loss: 0.00003254
Iteration 184/1000 | Loss: 0.00003254
Iteration 185/1000 | Loss: 0.00003254
Iteration 186/1000 | Loss: 0.00003254
Iteration 187/1000 | Loss: 0.00003254
Iteration 188/1000 | Loss: 0.00003254
Iteration 189/1000 | Loss: 0.00003254
Iteration 190/1000 | Loss: 0.00003254
Iteration 191/1000 | Loss: 0.00003254
Iteration 192/1000 | Loss: 0.00003254
Iteration 193/1000 | Loss: 0.00003253
Iteration 194/1000 | Loss: 0.00003253
Iteration 195/1000 | Loss: 0.00003253
Iteration 196/1000 | Loss: 0.00003253
Iteration 197/1000 | Loss: 0.00003253
Iteration 198/1000 | Loss: 0.00003253
Iteration 199/1000 | Loss: 0.00003253
Iteration 200/1000 | Loss: 0.00003253
Iteration 201/1000 | Loss: 0.00003253
Iteration 202/1000 | Loss: 0.00003253
Iteration 203/1000 | Loss: 0.00003253
Iteration 204/1000 | Loss: 0.00003253
Iteration 205/1000 | Loss: 0.00003253
Iteration 206/1000 | Loss: 0.00003253
Iteration 207/1000 | Loss: 0.00003253
Iteration 208/1000 | Loss: 0.00003253
Iteration 209/1000 | Loss: 0.00003253
Iteration 210/1000 | Loss: 0.00003253
Iteration 211/1000 | Loss: 0.00003252
Iteration 212/1000 | Loss: 0.00003252
Iteration 213/1000 | Loss: 0.00003252
Iteration 214/1000 | Loss: 0.00003252
Iteration 215/1000 | Loss: 0.00003252
Iteration 216/1000 | Loss: 0.00003252
Iteration 217/1000 | Loss: 0.00003252
Iteration 218/1000 | Loss: 0.00003252
Iteration 219/1000 | Loss: 0.00003252
Iteration 220/1000 | Loss: 0.00003252
Iteration 221/1000 | Loss: 0.00003252
Iteration 222/1000 | Loss: 0.00003252
Iteration 223/1000 | Loss: 0.00003252
Iteration 224/1000 | Loss: 0.00003252
Iteration 225/1000 | Loss: 0.00003252
Iteration 226/1000 | Loss: 0.00003252
Iteration 227/1000 | Loss: 0.00003251
Iteration 228/1000 | Loss: 0.00003251
Iteration 229/1000 | Loss: 0.00003251
Iteration 230/1000 | Loss: 0.00003251
Iteration 231/1000 | Loss: 0.00003251
Iteration 232/1000 | Loss: 0.00003251
Iteration 233/1000 | Loss: 0.00003251
Iteration 234/1000 | Loss: 0.00003251
Iteration 235/1000 | Loss: 0.00003251
Iteration 236/1000 | Loss: 0.00003251
Iteration 237/1000 | Loss: 0.00003251
Iteration 238/1000 | Loss: 0.00003251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [3.251360249123536e-05, 3.251360249123536e-05, 3.251360249123536e-05, 3.251360249123536e-05, 3.251360249123536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.251360249123536e-05

Optimization complete. Final v2v error: 4.492091655731201 mm

Highest mean error: 7.486581802368164 mm for frame 168

Lowest mean error: 2.914794683456421 mm for frame 239

Saving results

Total time: 162.79742217063904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472455
Iteration 2/25 | Loss: 0.00117740
Iteration 3/25 | Loss: 0.00109034
Iteration 4/25 | Loss: 0.00108012
Iteration 5/25 | Loss: 0.00107746
Iteration 6/25 | Loss: 0.00107741
Iteration 7/25 | Loss: 0.00107741
Iteration 8/25 | Loss: 0.00107741
Iteration 9/25 | Loss: 0.00107741
Iteration 10/25 | Loss: 0.00107741
Iteration 11/25 | Loss: 0.00107741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010774073889479041, 0.0010774073889479041, 0.0010774073889479041, 0.0010774073889479041, 0.0010774073889479041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010774073889479041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09144545
Iteration 2/25 | Loss: 0.00078739
Iteration 3/25 | Loss: 0.00078738
Iteration 4/25 | Loss: 0.00078738
Iteration 5/25 | Loss: 0.00078738
Iteration 6/25 | Loss: 0.00078738
Iteration 7/25 | Loss: 0.00078738
Iteration 8/25 | Loss: 0.00078738
Iteration 9/25 | Loss: 0.00078738
Iteration 10/25 | Loss: 0.00078738
Iteration 11/25 | Loss: 0.00078738
Iteration 12/25 | Loss: 0.00078738
Iteration 13/25 | Loss: 0.00078738
Iteration 14/25 | Loss: 0.00078738
Iteration 15/25 | Loss: 0.00078738
Iteration 16/25 | Loss: 0.00078738
Iteration 17/25 | Loss: 0.00078738
Iteration 18/25 | Loss: 0.00078738
Iteration 19/25 | Loss: 0.00078738
Iteration 20/25 | Loss: 0.00078738
Iteration 21/25 | Loss: 0.00078738
Iteration 22/25 | Loss: 0.00078738
Iteration 23/25 | Loss: 0.00078738
Iteration 24/25 | Loss: 0.00078738
Iteration 25/25 | Loss: 0.00078738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078738
Iteration 2/1000 | Loss: 0.00002288
Iteration 3/1000 | Loss: 0.00001657
Iteration 4/1000 | Loss: 0.00001379
Iteration 5/1000 | Loss: 0.00001288
Iteration 6/1000 | Loss: 0.00001214
Iteration 7/1000 | Loss: 0.00001165
Iteration 8/1000 | Loss: 0.00001120
Iteration 9/1000 | Loss: 0.00001089
Iteration 10/1000 | Loss: 0.00001064
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001047
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001031
Iteration 15/1000 | Loss: 0.00001031
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001031
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001011
Iteration 23/1000 | Loss: 0.00001010
Iteration 24/1000 | Loss: 0.00001009
Iteration 25/1000 | Loss: 0.00001009
Iteration 26/1000 | Loss: 0.00001007
Iteration 27/1000 | Loss: 0.00001006
Iteration 28/1000 | Loss: 0.00001006
Iteration 29/1000 | Loss: 0.00001005
Iteration 30/1000 | Loss: 0.00001005
Iteration 31/1000 | Loss: 0.00001005
Iteration 32/1000 | Loss: 0.00001004
Iteration 33/1000 | Loss: 0.00001004
Iteration 34/1000 | Loss: 0.00001003
Iteration 35/1000 | Loss: 0.00001003
Iteration 36/1000 | Loss: 0.00001002
Iteration 37/1000 | Loss: 0.00001001
Iteration 38/1000 | Loss: 0.00001001
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000995
Iteration 49/1000 | Loss: 0.00000994
Iteration 50/1000 | Loss: 0.00000994
Iteration 51/1000 | Loss: 0.00000993
Iteration 52/1000 | Loss: 0.00000993
Iteration 53/1000 | Loss: 0.00000993
Iteration 54/1000 | Loss: 0.00000993
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000992
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000988
Iteration 64/1000 | Loss: 0.00000988
Iteration 65/1000 | Loss: 0.00000987
Iteration 66/1000 | Loss: 0.00000987
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000984
Iteration 70/1000 | Loss: 0.00000984
Iteration 71/1000 | Loss: 0.00000982
Iteration 72/1000 | Loss: 0.00000982
Iteration 73/1000 | Loss: 0.00000982
Iteration 74/1000 | Loss: 0.00000981
Iteration 75/1000 | Loss: 0.00000981
Iteration 76/1000 | Loss: 0.00000981
Iteration 77/1000 | Loss: 0.00000980
Iteration 78/1000 | Loss: 0.00000980
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000979
Iteration 81/1000 | Loss: 0.00000979
Iteration 82/1000 | Loss: 0.00000978
Iteration 83/1000 | Loss: 0.00000978
Iteration 84/1000 | Loss: 0.00000978
Iteration 85/1000 | Loss: 0.00000978
Iteration 86/1000 | Loss: 0.00000978
Iteration 87/1000 | Loss: 0.00000977
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000975
Iteration 91/1000 | Loss: 0.00000975
Iteration 92/1000 | Loss: 0.00000975
Iteration 93/1000 | Loss: 0.00000975
Iteration 94/1000 | Loss: 0.00000975
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000974
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000972
Iteration 99/1000 | Loss: 0.00000972
Iteration 100/1000 | Loss: 0.00000972
Iteration 101/1000 | Loss: 0.00000971
Iteration 102/1000 | Loss: 0.00000971
Iteration 103/1000 | Loss: 0.00000971
Iteration 104/1000 | Loss: 0.00000971
Iteration 105/1000 | Loss: 0.00000971
Iteration 106/1000 | Loss: 0.00000971
Iteration 107/1000 | Loss: 0.00000971
Iteration 108/1000 | Loss: 0.00000970
Iteration 109/1000 | Loss: 0.00000970
Iteration 110/1000 | Loss: 0.00000970
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000969
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000968
Iteration 116/1000 | Loss: 0.00000968
Iteration 117/1000 | Loss: 0.00000968
Iteration 118/1000 | Loss: 0.00000968
Iteration 119/1000 | Loss: 0.00000968
Iteration 120/1000 | Loss: 0.00000968
Iteration 121/1000 | Loss: 0.00000968
Iteration 122/1000 | Loss: 0.00000968
Iteration 123/1000 | Loss: 0.00000968
Iteration 124/1000 | Loss: 0.00000968
Iteration 125/1000 | Loss: 0.00000968
Iteration 126/1000 | Loss: 0.00000968
Iteration 127/1000 | Loss: 0.00000968
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000968
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000968
Iteration 137/1000 | Loss: 0.00000968
Iteration 138/1000 | Loss: 0.00000968
Iteration 139/1000 | Loss: 0.00000968
Iteration 140/1000 | Loss: 0.00000968
Iteration 141/1000 | Loss: 0.00000968
Iteration 142/1000 | Loss: 0.00000968
Iteration 143/1000 | Loss: 0.00000968
Iteration 144/1000 | Loss: 0.00000968
Iteration 145/1000 | Loss: 0.00000968
Iteration 146/1000 | Loss: 0.00000968
Iteration 147/1000 | Loss: 0.00000968
Iteration 148/1000 | Loss: 0.00000968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [9.675082765170373e-06, 9.675082765170373e-06, 9.675082765170373e-06, 9.675082765170373e-06, 9.675082765170373e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.675082765170373e-06

Optimization complete. Final v2v error: 2.676284074783325 mm

Highest mean error: 3.0843448638916016 mm for frame 66

Lowest mean error: 2.4559643268585205 mm for frame 23

Saving results

Total time: 37.68129301071167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568507
Iteration 2/25 | Loss: 0.00140322
Iteration 3/25 | Loss: 0.00121235
Iteration 4/25 | Loss: 0.00119521
Iteration 5/25 | Loss: 0.00119176
Iteration 6/25 | Loss: 0.00119124
Iteration 7/25 | Loss: 0.00119124
Iteration 8/25 | Loss: 0.00119124
Iteration 9/25 | Loss: 0.00119124
Iteration 10/25 | Loss: 0.00119124
Iteration 11/25 | Loss: 0.00119124
Iteration 12/25 | Loss: 0.00119124
Iteration 13/25 | Loss: 0.00119106
Iteration 14/25 | Loss: 0.00119106
Iteration 15/25 | Loss: 0.00119106
Iteration 16/25 | Loss: 0.00119106
Iteration 17/25 | Loss: 0.00119106
Iteration 18/25 | Loss: 0.00119106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011910570319741964, 0.0011910570319741964, 0.0011910570319741964, 0.0011910570319741964, 0.0011910570319741964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011910570319741964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59031379
Iteration 2/25 | Loss: 0.00072946
Iteration 3/25 | Loss: 0.00072944
Iteration 4/25 | Loss: 0.00072944
Iteration 5/25 | Loss: 0.00072944
Iteration 6/25 | Loss: 0.00072944
Iteration 7/25 | Loss: 0.00072944
Iteration 8/25 | Loss: 0.00072944
Iteration 9/25 | Loss: 0.00072944
Iteration 10/25 | Loss: 0.00072944
Iteration 11/25 | Loss: 0.00072943
Iteration 12/25 | Loss: 0.00072943
Iteration 13/25 | Loss: 0.00072943
Iteration 14/25 | Loss: 0.00072943
Iteration 15/25 | Loss: 0.00072943
Iteration 16/25 | Loss: 0.00072943
Iteration 17/25 | Loss: 0.00072943
Iteration 18/25 | Loss: 0.00072943
Iteration 19/25 | Loss: 0.00072943
Iteration 20/25 | Loss: 0.00072943
Iteration 21/25 | Loss: 0.00072943
Iteration 22/25 | Loss: 0.00072943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007294343668036163, 0.0007294343668036163, 0.0007294343668036163, 0.0007294343668036163, 0.0007294343668036163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007294343668036163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072943
Iteration 2/1000 | Loss: 0.00003517
Iteration 3/1000 | Loss: 0.00002592
Iteration 4/1000 | Loss: 0.00002336
Iteration 5/1000 | Loss: 0.00002184
Iteration 6/1000 | Loss: 0.00002112
Iteration 7/1000 | Loss: 0.00002053
Iteration 8/1000 | Loss: 0.00002017
Iteration 9/1000 | Loss: 0.00001982
Iteration 10/1000 | Loss: 0.00001954
Iteration 11/1000 | Loss: 0.00001929
Iteration 12/1000 | Loss: 0.00001908
Iteration 13/1000 | Loss: 0.00001892
Iteration 14/1000 | Loss: 0.00001880
Iteration 15/1000 | Loss: 0.00001875
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001868
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001854
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001844
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001840
Iteration 35/1000 | Loss: 0.00001840
Iteration 36/1000 | Loss: 0.00001839
Iteration 37/1000 | Loss: 0.00001839
Iteration 38/1000 | Loss: 0.00001839
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001839
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001839
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001836
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001836
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001836
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001836
Iteration 61/1000 | Loss: 0.00001836
Iteration 62/1000 | Loss: 0.00001836
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001834
Iteration 72/1000 | Loss: 0.00001834
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001834
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001834
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001834
Iteration 85/1000 | Loss: 0.00001834
Iteration 86/1000 | Loss: 0.00001834
Iteration 87/1000 | Loss: 0.00001834
Iteration 88/1000 | Loss: 0.00001834
Iteration 89/1000 | Loss: 0.00001834
Iteration 90/1000 | Loss: 0.00001834
Iteration 91/1000 | Loss: 0.00001834
Iteration 92/1000 | Loss: 0.00001834
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001834
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001834
Iteration 98/1000 | Loss: 0.00001834
Iteration 99/1000 | Loss: 0.00001834
Iteration 100/1000 | Loss: 0.00001834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.8336286302655935e-05, 1.8336286302655935e-05, 1.8336286302655935e-05, 1.8336286302655935e-05, 1.8336286302655935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8336286302655935e-05

Optimization complete. Final v2v error: 3.544278621673584 mm

Highest mean error: 4.064209938049316 mm for frame 117

Lowest mean error: 2.8320069313049316 mm for frame 12

Saving results

Total time: 39.50910568237305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781417
Iteration 2/25 | Loss: 0.00128331
Iteration 3/25 | Loss: 0.00112334
Iteration 4/25 | Loss: 0.00110254
Iteration 5/25 | Loss: 0.00109866
Iteration 6/25 | Loss: 0.00109842
Iteration 7/25 | Loss: 0.00109842
Iteration 8/25 | Loss: 0.00109842
Iteration 9/25 | Loss: 0.00109842
Iteration 10/25 | Loss: 0.00109842
Iteration 11/25 | Loss: 0.00109842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010984215186908841, 0.0010984215186908841, 0.0010984215186908841, 0.0010984215186908841, 0.0010984215186908841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010984215186908841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34345198
Iteration 2/25 | Loss: 0.00082361
Iteration 3/25 | Loss: 0.00082361
Iteration 4/25 | Loss: 0.00082361
Iteration 5/25 | Loss: 0.00082361
Iteration 6/25 | Loss: 0.00082361
Iteration 7/25 | Loss: 0.00082361
Iteration 8/25 | Loss: 0.00082361
Iteration 9/25 | Loss: 0.00082361
Iteration 10/25 | Loss: 0.00082361
Iteration 11/25 | Loss: 0.00082360
Iteration 12/25 | Loss: 0.00082360
Iteration 13/25 | Loss: 0.00082360
Iteration 14/25 | Loss: 0.00082360
Iteration 15/25 | Loss: 0.00082360
Iteration 16/25 | Loss: 0.00082360
Iteration 17/25 | Loss: 0.00082360
Iteration 18/25 | Loss: 0.00082360
Iteration 19/25 | Loss: 0.00082360
Iteration 20/25 | Loss: 0.00082360
Iteration 21/25 | Loss: 0.00082360
Iteration 22/25 | Loss: 0.00082360
Iteration 23/25 | Loss: 0.00082360
Iteration 24/25 | Loss: 0.00082360
Iteration 25/25 | Loss: 0.00082360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008236042340286076, 0.0008236042340286076, 0.0008236042340286076, 0.0008236042340286076, 0.0008236042340286076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008236042340286076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082360
Iteration 2/1000 | Loss: 0.00002647
Iteration 3/1000 | Loss: 0.00001631
Iteration 4/1000 | Loss: 0.00001377
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001234
Iteration 7/1000 | Loss: 0.00001192
Iteration 8/1000 | Loss: 0.00001169
Iteration 9/1000 | Loss: 0.00001148
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00001119
Iteration 12/1000 | Loss: 0.00001118
Iteration 13/1000 | Loss: 0.00001113
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001104
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001095
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001092
Iteration 28/1000 | Loss: 0.00001091
Iteration 29/1000 | Loss: 0.00001090
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001085
Iteration 34/1000 | Loss: 0.00001085
Iteration 35/1000 | Loss: 0.00001084
Iteration 36/1000 | Loss: 0.00001084
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001083
Iteration 40/1000 | Loss: 0.00001083
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001080
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001075
Iteration 62/1000 | Loss: 0.00001075
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001073
Iteration 72/1000 | Loss: 0.00001073
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001072
Iteration 75/1000 | Loss: 0.00001072
Iteration 76/1000 | Loss: 0.00001072
Iteration 77/1000 | Loss: 0.00001072
Iteration 78/1000 | Loss: 0.00001072
Iteration 79/1000 | Loss: 0.00001072
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001071
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.0709398338804021e-05, 1.0709398338804021e-05, 1.0709398338804021e-05, 1.0709398338804021e-05, 1.0709398338804021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0709398338804021e-05

Optimization complete. Final v2v error: 2.782299280166626 mm

Highest mean error: 2.957970380783081 mm for frame 62

Lowest mean error: 2.5563108921051025 mm for frame 234

Saving results

Total time: 35.8232216835022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800256
Iteration 2/25 | Loss: 0.00129570
Iteration 3/25 | Loss: 0.00118739
Iteration 4/25 | Loss: 0.00117191
Iteration 5/25 | Loss: 0.00116680
Iteration 6/25 | Loss: 0.00116668
Iteration 7/25 | Loss: 0.00116668
Iteration 8/25 | Loss: 0.00116668
Iteration 9/25 | Loss: 0.00116668
Iteration 10/25 | Loss: 0.00116668
Iteration 11/25 | Loss: 0.00116668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011666790815070271, 0.0011666790815070271, 0.0011666790815070271, 0.0011666790815070271, 0.0011666790815070271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011666790815070271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25423014
Iteration 2/25 | Loss: 0.00072675
Iteration 3/25 | Loss: 0.00072669
Iteration 4/25 | Loss: 0.00072669
Iteration 5/25 | Loss: 0.00072669
Iteration 6/25 | Loss: 0.00072669
Iteration 7/25 | Loss: 0.00072669
Iteration 8/25 | Loss: 0.00072669
Iteration 9/25 | Loss: 0.00072669
Iteration 10/25 | Loss: 0.00072669
Iteration 11/25 | Loss: 0.00072669
Iteration 12/25 | Loss: 0.00072669
Iteration 13/25 | Loss: 0.00072669
Iteration 14/25 | Loss: 0.00072669
Iteration 15/25 | Loss: 0.00072669
Iteration 16/25 | Loss: 0.00072669
Iteration 17/25 | Loss: 0.00072669
Iteration 18/25 | Loss: 0.00072669
Iteration 19/25 | Loss: 0.00072669
Iteration 20/25 | Loss: 0.00072669
Iteration 21/25 | Loss: 0.00072669
Iteration 22/25 | Loss: 0.00072669
Iteration 23/25 | Loss: 0.00072669
Iteration 24/25 | Loss: 0.00072669
Iteration 25/25 | Loss: 0.00072669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072669
Iteration 2/1000 | Loss: 0.00003298
Iteration 3/1000 | Loss: 0.00002568
Iteration 4/1000 | Loss: 0.00002414
Iteration 5/1000 | Loss: 0.00002265
Iteration 6/1000 | Loss: 0.00002206
Iteration 7/1000 | Loss: 0.00002158
Iteration 8/1000 | Loss: 0.00002121
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002090
Iteration 11/1000 | Loss: 0.00002080
Iteration 12/1000 | Loss: 0.00002074
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002054
Iteration 15/1000 | Loss: 0.00002053
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002047
Iteration 19/1000 | Loss: 0.00002035
Iteration 20/1000 | Loss: 0.00002029
Iteration 21/1000 | Loss: 0.00002028
Iteration 22/1000 | Loss: 0.00002028
Iteration 23/1000 | Loss: 0.00002027
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00002026
Iteration 26/1000 | Loss: 0.00002025
Iteration 27/1000 | Loss: 0.00002025
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002022
Iteration 33/1000 | Loss: 0.00002021
Iteration 34/1000 | Loss: 0.00002020
Iteration 35/1000 | Loss: 0.00002017
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002013
Iteration 42/1000 | Loss: 0.00002011
Iteration 43/1000 | Loss: 0.00002011
Iteration 44/1000 | Loss: 0.00002011
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00002010
Iteration 47/1000 | Loss: 0.00002009
Iteration 48/1000 | Loss: 0.00002009
Iteration 49/1000 | Loss: 0.00002009
Iteration 50/1000 | Loss: 0.00002009
Iteration 51/1000 | Loss: 0.00002009
Iteration 52/1000 | Loss: 0.00002008
Iteration 53/1000 | Loss: 0.00002008
Iteration 54/1000 | Loss: 0.00002008
Iteration 55/1000 | Loss: 0.00002007
Iteration 56/1000 | Loss: 0.00002007
Iteration 57/1000 | Loss: 0.00002007
Iteration 58/1000 | Loss: 0.00002007
Iteration 59/1000 | Loss: 0.00002007
Iteration 60/1000 | Loss: 0.00002007
Iteration 61/1000 | Loss: 0.00002007
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002007
Iteration 64/1000 | Loss: 0.00002007
Iteration 65/1000 | Loss: 0.00002006
Iteration 66/1000 | Loss: 0.00002006
Iteration 67/1000 | Loss: 0.00002006
Iteration 68/1000 | Loss: 0.00002006
Iteration 69/1000 | Loss: 0.00002006
Iteration 70/1000 | Loss: 0.00002006
Iteration 71/1000 | Loss: 0.00002006
Iteration 72/1000 | Loss: 0.00002006
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002006
Iteration 79/1000 | Loss: 0.00002006
Iteration 80/1000 | Loss: 0.00002006
Iteration 81/1000 | Loss: 0.00002006
Iteration 82/1000 | Loss: 0.00002005
Iteration 83/1000 | Loss: 0.00002005
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002004
Iteration 88/1000 | Loss: 0.00002004
Iteration 89/1000 | Loss: 0.00002004
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002004
Iteration 96/1000 | Loss: 0.00002004
Iteration 97/1000 | Loss: 0.00002004
Iteration 98/1000 | Loss: 0.00002004
Iteration 99/1000 | Loss: 0.00002004
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002003
Iteration 104/1000 | Loss: 0.00002003
Iteration 105/1000 | Loss: 0.00002003
Iteration 106/1000 | Loss: 0.00002003
Iteration 107/1000 | Loss: 0.00002003
Iteration 108/1000 | Loss: 0.00002003
Iteration 109/1000 | Loss: 0.00002003
Iteration 110/1000 | Loss: 0.00002003
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00002002
Iteration 114/1000 | Loss: 0.00002002
Iteration 115/1000 | Loss: 0.00002002
Iteration 116/1000 | Loss: 0.00002002
Iteration 117/1000 | Loss: 0.00002002
Iteration 118/1000 | Loss: 0.00002002
Iteration 119/1000 | Loss: 0.00002002
Iteration 120/1000 | Loss: 0.00002002
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00002002
Iteration 123/1000 | Loss: 0.00002001
Iteration 124/1000 | Loss: 0.00002001
Iteration 125/1000 | Loss: 0.00002001
Iteration 126/1000 | Loss: 0.00002001
Iteration 127/1000 | Loss: 0.00002001
Iteration 128/1000 | Loss: 0.00002001
Iteration 129/1000 | Loss: 0.00002001
Iteration 130/1000 | Loss: 0.00002001
Iteration 131/1000 | Loss: 0.00002001
Iteration 132/1000 | Loss: 0.00002001
Iteration 133/1000 | Loss: 0.00002001
Iteration 134/1000 | Loss: 0.00002001
Iteration 135/1000 | Loss: 0.00002001
Iteration 136/1000 | Loss: 0.00002001
Iteration 137/1000 | Loss: 0.00002000
Iteration 138/1000 | Loss: 0.00002000
Iteration 139/1000 | Loss: 0.00002000
Iteration 140/1000 | Loss: 0.00002000
Iteration 141/1000 | Loss: 0.00002000
Iteration 142/1000 | Loss: 0.00002000
Iteration 143/1000 | Loss: 0.00002000
Iteration 144/1000 | Loss: 0.00002000
Iteration 145/1000 | Loss: 0.00002000
Iteration 146/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.0000368749606423e-05, 2.0000368749606423e-05, 2.0000368749606423e-05, 2.0000368749606423e-05, 2.0000368749606423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0000368749606423e-05

Optimization complete. Final v2v error: 3.757450580596924 mm

Highest mean error: 3.988326072692871 mm for frame 43

Lowest mean error: 3.388866901397705 mm for frame 210

Saving results

Total time: 41.17108774185181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977317
Iteration 2/25 | Loss: 0.00166059
Iteration 3/25 | Loss: 0.00122275
Iteration 4/25 | Loss: 0.00118532
Iteration 5/25 | Loss: 0.00118030
Iteration 6/25 | Loss: 0.00118030
Iteration 7/25 | Loss: 0.00118030
Iteration 8/25 | Loss: 0.00118030
Iteration 9/25 | Loss: 0.00118030
Iteration 10/25 | Loss: 0.00118030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011802976951003075, 0.0011802976951003075, 0.0011802976951003075, 0.0011802976951003075, 0.0011802976951003075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011802976951003075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34015703
Iteration 2/25 | Loss: 0.00067593
Iteration 3/25 | Loss: 0.00067593
Iteration 4/25 | Loss: 0.00067593
Iteration 5/25 | Loss: 0.00067593
Iteration 6/25 | Loss: 0.00067593
Iteration 7/25 | Loss: 0.00067593
Iteration 8/25 | Loss: 0.00067593
Iteration 9/25 | Loss: 0.00067593
Iteration 10/25 | Loss: 0.00067593
Iteration 11/25 | Loss: 0.00067593
Iteration 12/25 | Loss: 0.00067593
Iteration 13/25 | Loss: 0.00067593
Iteration 14/25 | Loss: 0.00067593
Iteration 15/25 | Loss: 0.00067593
Iteration 16/25 | Loss: 0.00067593
Iteration 17/25 | Loss: 0.00067593
Iteration 18/25 | Loss: 0.00067593
Iteration 19/25 | Loss: 0.00067593
Iteration 20/25 | Loss: 0.00067593
Iteration 21/25 | Loss: 0.00067593
Iteration 22/25 | Loss: 0.00067593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006759273237548769, 0.0006759273237548769, 0.0006759273237548769, 0.0006759273237548769, 0.0006759273237548769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006759273237548769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067593
Iteration 2/1000 | Loss: 0.00003012
Iteration 3/1000 | Loss: 0.00002239
Iteration 4/1000 | Loss: 0.00002123
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00002011
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001944
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001903
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001887
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001877
Iteration 16/1000 | Loss: 0.00001873
Iteration 17/1000 | Loss: 0.00001868
Iteration 18/1000 | Loss: 0.00001865
Iteration 19/1000 | Loss: 0.00001865
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001865
Iteration 22/1000 | Loss: 0.00001865
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001865
Iteration 25/1000 | Loss: 0.00001864
Iteration 26/1000 | Loss: 0.00001864
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001862
Iteration 33/1000 | Loss: 0.00001860
Iteration 34/1000 | Loss: 0.00001858
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001858
Iteration 37/1000 | Loss: 0.00001858
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001857
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001857
Iteration 43/1000 | Loss: 0.00001856
Iteration 44/1000 | Loss: 0.00001855
Iteration 45/1000 | Loss: 0.00001855
Iteration 46/1000 | Loss: 0.00001855
Iteration 47/1000 | Loss: 0.00001855
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001853
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001851
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001851
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001850
Iteration 67/1000 | Loss: 0.00001850
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001849
Iteration 74/1000 | Loss: 0.00001849
Iteration 75/1000 | Loss: 0.00001849
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001848
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001847
Iteration 86/1000 | Loss: 0.00001847
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001846
Iteration 95/1000 | Loss: 0.00001846
Iteration 96/1000 | Loss: 0.00001846
Iteration 97/1000 | Loss: 0.00001846
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001846
Iteration 101/1000 | Loss: 0.00001846
Iteration 102/1000 | Loss: 0.00001846
Iteration 103/1000 | Loss: 0.00001846
Iteration 104/1000 | Loss: 0.00001846
Iteration 105/1000 | Loss: 0.00001846
Iteration 106/1000 | Loss: 0.00001846
Iteration 107/1000 | Loss: 0.00001846
Iteration 108/1000 | Loss: 0.00001846
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001845
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Iteration 134/1000 | Loss: 0.00001845
Iteration 135/1000 | Loss: 0.00001845
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Iteration 141/1000 | Loss: 0.00001845
Iteration 142/1000 | Loss: 0.00001845
Iteration 143/1000 | Loss: 0.00001845
Iteration 144/1000 | Loss: 0.00001845
Iteration 145/1000 | Loss: 0.00001845
Iteration 146/1000 | Loss: 0.00001845
Iteration 147/1000 | Loss: 0.00001845
Iteration 148/1000 | Loss: 0.00001845
Iteration 149/1000 | Loss: 0.00001845
Iteration 150/1000 | Loss: 0.00001845
Iteration 151/1000 | Loss: 0.00001845
Iteration 152/1000 | Loss: 0.00001845
Iteration 153/1000 | Loss: 0.00001845
Iteration 154/1000 | Loss: 0.00001845
Iteration 155/1000 | Loss: 0.00001845
Iteration 156/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.8453452867106535e-05, 1.8453452867106535e-05, 1.8453452867106535e-05, 1.8453452867106535e-05, 1.8453452867106535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8453452867106535e-05

Optimization complete. Final v2v error: 3.589491605758667 mm

Highest mean error: 3.6374778747558594 mm for frame 172

Lowest mean error: 3.324097156524658 mm for frame 0

Saving results

Total time: 35.233420610427856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00631131
Iteration 2/25 | Loss: 0.00143522
Iteration 3/25 | Loss: 0.00126950
Iteration 4/25 | Loss: 0.00125611
Iteration 5/25 | Loss: 0.00125207
Iteration 6/25 | Loss: 0.00125147
Iteration 7/25 | Loss: 0.00125147
Iteration 8/25 | Loss: 0.00125147
Iteration 9/25 | Loss: 0.00125147
Iteration 10/25 | Loss: 0.00125147
Iteration 11/25 | Loss: 0.00125147
Iteration 12/25 | Loss: 0.00125147
Iteration 13/25 | Loss: 0.00125147
Iteration 14/25 | Loss: 0.00125147
Iteration 15/25 | Loss: 0.00125147
Iteration 16/25 | Loss: 0.00125147
Iteration 17/25 | Loss: 0.00125147
Iteration 18/25 | Loss: 0.00125147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012514746049419045, 0.0012514746049419045, 0.0012514746049419045, 0.0012514746049419045, 0.0012514746049419045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012514746049419045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32663655
Iteration 2/25 | Loss: 0.00109478
Iteration 3/25 | Loss: 0.00109475
Iteration 4/25 | Loss: 0.00109475
Iteration 5/25 | Loss: 0.00109475
Iteration 6/25 | Loss: 0.00109475
Iteration 7/25 | Loss: 0.00109475
Iteration 8/25 | Loss: 0.00109475
Iteration 9/25 | Loss: 0.00109475
Iteration 10/25 | Loss: 0.00109475
Iteration 11/25 | Loss: 0.00109475
Iteration 12/25 | Loss: 0.00109475
Iteration 13/25 | Loss: 0.00109475
Iteration 14/25 | Loss: 0.00109475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010947462869808078, 0.0010947462869808078, 0.0010947462869808078, 0.0010947462869808078, 0.0010947462869808078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010947462869808078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109475
Iteration 2/1000 | Loss: 0.00003894
Iteration 3/1000 | Loss: 0.00002702
Iteration 4/1000 | Loss: 0.00002525
Iteration 5/1000 | Loss: 0.00002435
Iteration 6/1000 | Loss: 0.00002371
Iteration 7/1000 | Loss: 0.00002327
Iteration 8/1000 | Loss: 0.00002290
Iteration 9/1000 | Loss: 0.00002246
Iteration 10/1000 | Loss: 0.00002222
Iteration 11/1000 | Loss: 0.00002203
Iteration 12/1000 | Loss: 0.00002196
Iteration 13/1000 | Loss: 0.00002174
Iteration 14/1000 | Loss: 0.00002156
Iteration 15/1000 | Loss: 0.00002152
Iteration 16/1000 | Loss: 0.00002149
Iteration 17/1000 | Loss: 0.00002149
Iteration 18/1000 | Loss: 0.00002135
Iteration 19/1000 | Loss: 0.00002133
Iteration 20/1000 | Loss: 0.00002133
Iteration 21/1000 | Loss: 0.00002133
Iteration 22/1000 | Loss: 0.00002132
Iteration 23/1000 | Loss: 0.00002131
Iteration 24/1000 | Loss: 0.00002120
Iteration 25/1000 | Loss: 0.00002116
Iteration 26/1000 | Loss: 0.00002115
Iteration 27/1000 | Loss: 0.00002114
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002111
Iteration 30/1000 | Loss: 0.00002111
Iteration 31/1000 | Loss: 0.00002110
Iteration 32/1000 | Loss: 0.00002109
Iteration 33/1000 | Loss: 0.00002108
Iteration 34/1000 | Loss: 0.00002108
Iteration 35/1000 | Loss: 0.00002107
Iteration 36/1000 | Loss: 0.00002107
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002106
Iteration 39/1000 | Loss: 0.00002106
Iteration 40/1000 | Loss: 0.00002105
Iteration 41/1000 | Loss: 0.00002105
Iteration 42/1000 | Loss: 0.00002101
Iteration 43/1000 | Loss: 0.00002101
Iteration 44/1000 | Loss: 0.00002100
Iteration 45/1000 | Loss: 0.00002100
Iteration 46/1000 | Loss: 0.00002100
Iteration 47/1000 | Loss: 0.00002099
Iteration 48/1000 | Loss: 0.00002098
Iteration 49/1000 | Loss: 0.00002098
Iteration 50/1000 | Loss: 0.00002097
Iteration 51/1000 | Loss: 0.00002097
Iteration 52/1000 | Loss: 0.00002097
Iteration 53/1000 | Loss: 0.00002097
Iteration 54/1000 | Loss: 0.00002097
Iteration 55/1000 | Loss: 0.00002097
Iteration 56/1000 | Loss: 0.00002097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [2.0973913706257008e-05, 2.0973913706257008e-05, 2.0973913706257008e-05, 2.0973913706257008e-05, 2.0973913706257008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0973913706257008e-05

Optimization complete. Final v2v error: 3.8061702251434326 mm

Highest mean error: 4.103318691253662 mm for frame 205

Lowest mean error: 3.3993332386016846 mm for frame 237

Saving results

Total time: 41.09435749053955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398927
Iteration 2/25 | Loss: 0.00119596
Iteration 3/25 | Loss: 0.00108002
Iteration 4/25 | Loss: 0.00106789
Iteration 5/25 | Loss: 0.00106519
Iteration 6/25 | Loss: 0.00106441
Iteration 7/25 | Loss: 0.00106436
Iteration 8/25 | Loss: 0.00106436
Iteration 9/25 | Loss: 0.00106436
Iteration 10/25 | Loss: 0.00106436
Iteration 11/25 | Loss: 0.00106437
Iteration 12/25 | Loss: 0.00106437
Iteration 13/25 | Loss: 0.00106436
Iteration 14/25 | Loss: 0.00106437
Iteration 15/25 | Loss: 0.00106437
Iteration 16/25 | Loss: 0.00106436
Iteration 17/25 | Loss: 0.00106436
Iteration 18/25 | Loss: 0.00106436
Iteration 19/25 | Loss: 0.00106437
Iteration 20/25 | Loss: 0.00106437
Iteration 21/25 | Loss: 0.00106437
Iteration 22/25 | Loss: 0.00106437
Iteration 23/25 | Loss: 0.00106437
Iteration 24/25 | Loss: 0.00106437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010643650311976671, 0.0010643650311976671, 0.0010643650311976671, 0.0010643650311976671, 0.0010643650311976671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010643650311976671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35794783
Iteration 2/25 | Loss: 0.00093845
Iteration 3/25 | Loss: 0.00093844
Iteration 4/25 | Loss: 0.00093844
Iteration 5/25 | Loss: 0.00093844
Iteration 6/25 | Loss: 0.00093844
Iteration 7/25 | Loss: 0.00093844
Iteration 8/25 | Loss: 0.00093844
Iteration 9/25 | Loss: 0.00093844
Iteration 10/25 | Loss: 0.00093844
Iteration 11/25 | Loss: 0.00093844
Iteration 12/25 | Loss: 0.00093844
Iteration 13/25 | Loss: 0.00093844
Iteration 14/25 | Loss: 0.00093844
Iteration 15/25 | Loss: 0.00093844
Iteration 16/25 | Loss: 0.00093844
Iteration 17/25 | Loss: 0.00093844
Iteration 18/25 | Loss: 0.00093844
Iteration 19/25 | Loss: 0.00093844
Iteration 20/25 | Loss: 0.00093844
Iteration 21/25 | Loss: 0.00093844
Iteration 22/25 | Loss: 0.00093844
Iteration 23/25 | Loss: 0.00093844
Iteration 24/25 | Loss: 0.00093844
Iteration 25/25 | Loss: 0.00093844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093844
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001518
Iteration 4/1000 | Loss: 0.00001155
Iteration 5/1000 | Loss: 0.00001049
Iteration 6/1000 | Loss: 0.00000985
Iteration 7/1000 | Loss: 0.00000941
Iteration 8/1000 | Loss: 0.00000911
Iteration 9/1000 | Loss: 0.00000892
Iteration 10/1000 | Loss: 0.00000890
Iteration 11/1000 | Loss: 0.00000890
Iteration 12/1000 | Loss: 0.00000887
Iteration 13/1000 | Loss: 0.00000887
Iteration 14/1000 | Loss: 0.00000885
Iteration 15/1000 | Loss: 0.00000885
Iteration 16/1000 | Loss: 0.00000884
Iteration 17/1000 | Loss: 0.00000883
Iteration 18/1000 | Loss: 0.00000883
Iteration 19/1000 | Loss: 0.00000875
Iteration 20/1000 | Loss: 0.00000873
Iteration 21/1000 | Loss: 0.00000864
Iteration 22/1000 | Loss: 0.00000863
Iteration 23/1000 | Loss: 0.00000863
Iteration 24/1000 | Loss: 0.00000862
Iteration 25/1000 | Loss: 0.00000861
Iteration 26/1000 | Loss: 0.00000861
Iteration 27/1000 | Loss: 0.00000860
Iteration 28/1000 | Loss: 0.00000859
Iteration 29/1000 | Loss: 0.00000859
Iteration 30/1000 | Loss: 0.00000858
Iteration 31/1000 | Loss: 0.00000856
Iteration 32/1000 | Loss: 0.00000856
Iteration 33/1000 | Loss: 0.00000854
Iteration 34/1000 | Loss: 0.00000851
Iteration 35/1000 | Loss: 0.00000848
Iteration 36/1000 | Loss: 0.00000844
Iteration 37/1000 | Loss: 0.00000843
Iteration 38/1000 | Loss: 0.00000843
Iteration 39/1000 | Loss: 0.00000843
Iteration 40/1000 | Loss: 0.00000842
Iteration 41/1000 | Loss: 0.00000841
Iteration 42/1000 | Loss: 0.00000840
Iteration 43/1000 | Loss: 0.00000840
Iteration 44/1000 | Loss: 0.00000839
Iteration 45/1000 | Loss: 0.00000839
Iteration 46/1000 | Loss: 0.00000839
Iteration 47/1000 | Loss: 0.00000838
Iteration 48/1000 | Loss: 0.00000838
Iteration 49/1000 | Loss: 0.00000837
Iteration 50/1000 | Loss: 0.00000837
Iteration 51/1000 | Loss: 0.00000837
Iteration 52/1000 | Loss: 0.00000837
Iteration 53/1000 | Loss: 0.00000836
Iteration 54/1000 | Loss: 0.00000836
Iteration 55/1000 | Loss: 0.00000836
Iteration 56/1000 | Loss: 0.00000836
Iteration 57/1000 | Loss: 0.00000836
Iteration 58/1000 | Loss: 0.00000836
Iteration 59/1000 | Loss: 0.00000836
Iteration 60/1000 | Loss: 0.00000835
Iteration 61/1000 | Loss: 0.00000835
Iteration 62/1000 | Loss: 0.00000834
Iteration 63/1000 | Loss: 0.00000834
Iteration 64/1000 | Loss: 0.00000834
Iteration 65/1000 | Loss: 0.00000834
Iteration 66/1000 | Loss: 0.00000833
Iteration 67/1000 | Loss: 0.00000831
Iteration 68/1000 | Loss: 0.00000830
Iteration 69/1000 | Loss: 0.00000830
Iteration 70/1000 | Loss: 0.00000830
Iteration 71/1000 | Loss: 0.00000829
Iteration 72/1000 | Loss: 0.00000829
Iteration 73/1000 | Loss: 0.00000829
Iteration 74/1000 | Loss: 0.00000828
Iteration 75/1000 | Loss: 0.00000828
Iteration 76/1000 | Loss: 0.00000828
Iteration 77/1000 | Loss: 0.00000828
Iteration 78/1000 | Loss: 0.00000828
Iteration 79/1000 | Loss: 0.00000828
Iteration 80/1000 | Loss: 0.00000828
Iteration 81/1000 | Loss: 0.00000828
Iteration 82/1000 | Loss: 0.00000828
Iteration 83/1000 | Loss: 0.00000828
Iteration 84/1000 | Loss: 0.00000827
Iteration 85/1000 | Loss: 0.00000827
Iteration 86/1000 | Loss: 0.00000827
Iteration 87/1000 | Loss: 0.00000827
Iteration 88/1000 | Loss: 0.00000827
Iteration 89/1000 | Loss: 0.00000827
Iteration 90/1000 | Loss: 0.00000827
Iteration 91/1000 | Loss: 0.00000826
Iteration 92/1000 | Loss: 0.00000826
Iteration 93/1000 | Loss: 0.00000826
Iteration 94/1000 | Loss: 0.00000826
Iteration 95/1000 | Loss: 0.00000826
Iteration 96/1000 | Loss: 0.00000826
Iteration 97/1000 | Loss: 0.00000826
Iteration 98/1000 | Loss: 0.00000826
Iteration 99/1000 | Loss: 0.00000826
Iteration 100/1000 | Loss: 0.00000826
Iteration 101/1000 | Loss: 0.00000825
Iteration 102/1000 | Loss: 0.00000825
Iteration 103/1000 | Loss: 0.00000825
Iteration 104/1000 | Loss: 0.00000825
Iteration 105/1000 | Loss: 0.00000825
Iteration 106/1000 | Loss: 0.00000825
Iteration 107/1000 | Loss: 0.00000825
Iteration 108/1000 | Loss: 0.00000825
Iteration 109/1000 | Loss: 0.00000825
Iteration 110/1000 | Loss: 0.00000825
Iteration 111/1000 | Loss: 0.00000825
Iteration 112/1000 | Loss: 0.00000825
Iteration 113/1000 | Loss: 0.00000825
Iteration 114/1000 | Loss: 0.00000825
Iteration 115/1000 | Loss: 0.00000824
Iteration 116/1000 | Loss: 0.00000824
Iteration 117/1000 | Loss: 0.00000824
Iteration 118/1000 | Loss: 0.00000824
Iteration 119/1000 | Loss: 0.00000824
Iteration 120/1000 | Loss: 0.00000824
Iteration 121/1000 | Loss: 0.00000824
Iteration 122/1000 | Loss: 0.00000824
Iteration 123/1000 | Loss: 0.00000824
Iteration 124/1000 | Loss: 0.00000824
Iteration 125/1000 | Loss: 0.00000824
Iteration 126/1000 | Loss: 0.00000824
Iteration 127/1000 | Loss: 0.00000824
Iteration 128/1000 | Loss: 0.00000824
Iteration 129/1000 | Loss: 0.00000824
Iteration 130/1000 | Loss: 0.00000824
Iteration 131/1000 | Loss: 0.00000824
Iteration 132/1000 | Loss: 0.00000824
Iteration 133/1000 | Loss: 0.00000824
Iteration 134/1000 | Loss: 0.00000824
Iteration 135/1000 | Loss: 0.00000824
Iteration 136/1000 | Loss: 0.00000824
Iteration 137/1000 | Loss: 0.00000824
Iteration 138/1000 | Loss: 0.00000824
Iteration 139/1000 | Loss: 0.00000824
Iteration 140/1000 | Loss: 0.00000824
Iteration 141/1000 | Loss: 0.00000824
Iteration 142/1000 | Loss: 0.00000824
Iteration 143/1000 | Loss: 0.00000824
Iteration 144/1000 | Loss: 0.00000824
Iteration 145/1000 | Loss: 0.00000824
Iteration 146/1000 | Loss: 0.00000824
Iteration 147/1000 | Loss: 0.00000824
Iteration 148/1000 | Loss: 0.00000824
Iteration 149/1000 | Loss: 0.00000824
Iteration 150/1000 | Loss: 0.00000824
Iteration 151/1000 | Loss: 0.00000824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [8.243550837505609e-06, 8.243550837505609e-06, 8.243550837505609e-06, 8.243550837505609e-06, 8.243550837505609e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.243550837505609e-06

Optimization complete. Final v2v error: 2.4584057331085205 mm

Highest mean error: 3.3959014415740967 mm for frame 61

Lowest mean error: 2.28344988822937 mm for frame 90

Saving results

Total time: 35.807448625564575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998914
Iteration 2/25 | Loss: 0.00172437
Iteration 3/25 | Loss: 0.00132437
Iteration 4/25 | Loss: 0.00124419
Iteration 5/25 | Loss: 0.00122877
Iteration 6/25 | Loss: 0.00120056
Iteration 7/25 | Loss: 0.00119728
Iteration 8/25 | Loss: 0.00116288
Iteration 9/25 | Loss: 0.00115019
Iteration 10/25 | Loss: 0.00113712
Iteration 11/25 | Loss: 0.00113179
Iteration 12/25 | Loss: 0.00113034
Iteration 13/25 | Loss: 0.00113063
Iteration 14/25 | Loss: 0.00111892
Iteration 15/25 | Loss: 0.00111699
Iteration 16/25 | Loss: 0.00111642
Iteration 17/25 | Loss: 0.00111632
Iteration 18/25 | Loss: 0.00111632
Iteration 19/25 | Loss: 0.00111631
Iteration 20/25 | Loss: 0.00111631
Iteration 21/25 | Loss: 0.00111631
Iteration 22/25 | Loss: 0.00111631
Iteration 23/25 | Loss: 0.00111630
Iteration 24/25 | Loss: 0.00111630
Iteration 25/25 | Loss: 0.00111630

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43888140
Iteration 2/25 | Loss: 0.00091119
Iteration 3/25 | Loss: 0.00078683
Iteration 4/25 | Loss: 0.00078673
Iteration 5/25 | Loss: 0.00078673
Iteration 6/25 | Loss: 0.00078673
Iteration 7/25 | Loss: 0.00078673
Iteration 8/25 | Loss: 0.00078673
Iteration 9/25 | Loss: 0.00078673
Iteration 10/25 | Loss: 0.00078673
Iteration 11/25 | Loss: 0.00078673
Iteration 12/25 | Loss: 0.00078673
Iteration 13/25 | Loss: 0.00078673
Iteration 14/25 | Loss: 0.00078673
Iteration 15/25 | Loss: 0.00078673
Iteration 16/25 | Loss: 0.00078673
Iteration 17/25 | Loss: 0.00078673
Iteration 18/25 | Loss: 0.00078673
Iteration 19/25 | Loss: 0.00078673
Iteration 20/25 | Loss: 0.00078673
Iteration 21/25 | Loss: 0.00078673
Iteration 22/25 | Loss: 0.00078673
Iteration 23/25 | Loss: 0.00078673
Iteration 24/25 | Loss: 0.00078673
Iteration 25/25 | Loss: 0.00078673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078673
Iteration 2/1000 | Loss: 0.00009664
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001493
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001409
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00039669
Iteration 13/1000 | Loss: 0.00001720
Iteration 14/1000 | Loss: 0.00001371
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001196
Iteration 17/1000 | Loss: 0.00001149
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001094
Iteration 20/1000 | Loss: 0.00001081
Iteration 21/1000 | Loss: 0.00001067
Iteration 22/1000 | Loss: 0.00001067
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001059
Iteration 25/1000 | Loss: 0.00001053
Iteration 26/1000 | Loss: 0.00001052
Iteration 27/1000 | Loss: 0.00001052
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001044
Iteration 35/1000 | Loss: 0.00001044
Iteration 36/1000 | Loss: 0.00001043
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001042
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001040
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001040
Iteration 60/1000 | Loss: 0.00001040
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001039
Iteration 66/1000 | Loss: 0.00001039
Iteration 67/1000 | Loss: 0.00001039
Iteration 68/1000 | Loss: 0.00001039
Iteration 69/1000 | Loss: 0.00001039
Iteration 70/1000 | Loss: 0.00001039
Iteration 71/1000 | Loss: 0.00001039
Iteration 72/1000 | Loss: 0.00001039
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001038
Iteration 75/1000 | Loss: 0.00001038
Iteration 76/1000 | Loss: 0.00001038
Iteration 77/1000 | Loss: 0.00001038
Iteration 78/1000 | Loss: 0.00001038
Iteration 79/1000 | Loss: 0.00001038
Iteration 80/1000 | Loss: 0.00001038
Iteration 81/1000 | Loss: 0.00001038
Iteration 82/1000 | Loss: 0.00001037
Iteration 83/1000 | Loss: 0.00001037
Iteration 84/1000 | Loss: 0.00001036
Iteration 85/1000 | Loss: 0.00001036
Iteration 86/1000 | Loss: 0.00001036
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001035
Iteration 90/1000 | Loss: 0.00001035
Iteration 91/1000 | Loss: 0.00001035
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001035
Iteration 94/1000 | Loss: 0.00001035
Iteration 95/1000 | Loss: 0.00001035
Iteration 96/1000 | Loss: 0.00001035
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001034
Iteration 101/1000 | Loss: 0.00001034
Iteration 102/1000 | Loss: 0.00001034
Iteration 103/1000 | Loss: 0.00001034
Iteration 104/1000 | Loss: 0.00001034
Iteration 105/1000 | Loss: 0.00001033
Iteration 106/1000 | Loss: 0.00001033
Iteration 107/1000 | Loss: 0.00001033
Iteration 108/1000 | Loss: 0.00001033
Iteration 109/1000 | Loss: 0.00001033
Iteration 110/1000 | Loss: 0.00001033
Iteration 111/1000 | Loss: 0.00001033
Iteration 112/1000 | Loss: 0.00001033
Iteration 113/1000 | Loss: 0.00001033
Iteration 114/1000 | Loss: 0.00001033
Iteration 115/1000 | Loss: 0.00001033
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001033
Iteration 118/1000 | Loss: 0.00001033
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001032
Iteration 121/1000 | Loss: 0.00001032
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001032
Iteration 124/1000 | Loss: 0.00001032
Iteration 125/1000 | Loss: 0.00001032
Iteration 126/1000 | Loss: 0.00001032
Iteration 127/1000 | Loss: 0.00001032
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001032
Iteration 130/1000 | Loss: 0.00001032
Iteration 131/1000 | Loss: 0.00001032
Iteration 132/1000 | Loss: 0.00001032
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001031
Iteration 136/1000 | Loss: 0.00001031
Iteration 137/1000 | Loss: 0.00001031
Iteration 138/1000 | Loss: 0.00001031
Iteration 139/1000 | Loss: 0.00001031
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00001031
Iteration 142/1000 | Loss: 0.00001031
Iteration 143/1000 | Loss: 0.00001031
Iteration 144/1000 | Loss: 0.00001031
Iteration 145/1000 | Loss: 0.00001031
Iteration 146/1000 | Loss: 0.00001031
Iteration 147/1000 | Loss: 0.00001031
Iteration 148/1000 | Loss: 0.00001031
Iteration 149/1000 | Loss: 0.00001031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.0311329788237344e-05, 1.0311329788237344e-05, 1.0311329788237344e-05, 1.0311329788237344e-05, 1.0311329788237344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0311329788237344e-05

Optimization complete. Final v2v error: 2.7233941555023193 mm

Highest mean error: 4.026337623596191 mm for frame 66

Lowest mean error: 2.5254878997802734 mm for frame 20

Saving results

Total time: 70.57340407371521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977334
Iteration 2/25 | Loss: 0.00195228
Iteration 3/25 | Loss: 0.00157488
Iteration 4/25 | Loss: 0.00155485
Iteration 5/25 | Loss: 0.00149981
Iteration 6/25 | Loss: 0.00146037
Iteration 7/25 | Loss: 0.00136125
Iteration 8/25 | Loss: 0.00126266
Iteration 9/25 | Loss: 0.00132920
Iteration 10/25 | Loss: 0.00133938
Iteration 11/25 | Loss: 0.00127989
Iteration 12/25 | Loss: 0.00127053
Iteration 13/25 | Loss: 0.00122941
Iteration 14/25 | Loss: 0.00125623
Iteration 15/25 | Loss: 0.00121835
Iteration 16/25 | Loss: 0.00121120
Iteration 17/25 | Loss: 0.00121424
Iteration 18/25 | Loss: 0.00121155
Iteration 19/25 | Loss: 0.00120775
Iteration 20/25 | Loss: 0.00120317
Iteration 21/25 | Loss: 0.00120328
Iteration 22/25 | Loss: 0.00120086
Iteration 23/25 | Loss: 0.00120973
Iteration 24/25 | Loss: 0.00120097
Iteration 25/25 | Loss: 0.00120710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78224945
Iteration 2/25 | Loss: 0.00139216
Iteration 3/25 | Loss: 0.00139216
Iteration 4/25 | Loss: 0.00139215
Iteration 5/25 | Loss: 0.00139215
Iteration 6/25 | Loss: 0.00139215
Iteration 7/25 | Loss: 0.00139215
Iteration 8/25 | Loss: 0.00139215
Iteration 9/25 | Loss: 0.00139215
Iteration 10/25 | Loss: 0.00139215
Iteration 11/25 | Loss: 0.00139215
Iteration 12/25 | Loss: 0.00139215
Iteration 13/25 | Loss: 0.00139215
Iteration 14/25 | Loss: 0.00139215
Iteration 15/25 | Loss: 0.00139215
Iteration 16/25 | Loss: 0.00139215
Iteration 17/25 | Loss: 0.00139215
Iteration 18/25 | Loss: 0.00139215
Iteration 19/25 | Loss: 0.00139215
Iteration 20/25 | Loss: 0.00139215
Iteration 21/25 | Loss: 0.00139215
Iteration 22/25 | Loss: 0.00139215
Iteration 23/25 | Loss: 0.00139215
Iteration 24/25 | Loss: 0.00139215
Iteration 25/25 | Loss: 0.00139215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139215
Iteration 2/1000 | Loss: 0.00010430
Iteration 3/1000 | Loss: 0.00012804
Iteration 4/1000 | Loss: 0.00045099
Iteration 5/1000 | Loss: 0.00038296
Iteration 6/1000 | Loss: 0.00035376
Iteration 7/1000 | Loss: 0.00019740
Iteration 8/1000 | Loss: 0.00025269
Iteration 9/1000 | Loss: 0.00042585
Iteration 10/1000 | Loss: 0.00040537
Iteration 11/1000 | Loss: 0.00065322
Iteration 12/1000 | Loss: 0.00059876
Iteration 13/1000 | Loss: 0.00011533
Iteration 14/1000 | Loss: 0.00015180
Iteration 15/1000 | Loss: 0.00022613
Iteration 16/1000 | Loss: 0.00018606
Iteration 17/1000 | Loss: 0.00010808
Iteration 18/1000 | Loss: 0.00012261
Iteration 19/1000 | Loss: 0.00013985
Iteration 20/1000 | Loss: 0.00022181
Iteration 21/1000 | Loss: 0.00006620
Iteration 22/1000 | Loss: 0.00010992
Iteration 23/1000 | Loss: 0.00004503
Iteration 24/1000 | Loss: 0.00039883
Iteration 25/1000 | Loss: 0.00037256
Iteration 26/1000 | Loss: 0.00015722
Iteration 27/1000 | Loss: 0.00070000
Iteration 28/1000 | Loss: 0.00019751
Iteration 29/1000 | Loss: 0.00019189
Iteration 30/1000 | Loss: 0.00006235
Iteration 31/1000 | Loss: 0.00004567
Iteration 32/1000 | Loss: 0.00018148
Iteration 33/1000 | Loss: 0.00014847
Iteration 34/1000 | Loss: 0.00013869
Iteration 35/1000 | Loss: 0.00010767
Iteration 36/1000 | Loss: 0.00012228
Iteration 37/1000 | Loss: 0.00007797
Iteration 38/1000 | Loss: 0.00005175
Iteration 39/1000 | Loss: 0.00018736
Iteration 40/1000 | Loss: 0.00004463
Iteration 41/1000 | Loss: 0.00031136
Iteration 42/1000 | Loss: 0.00014883
Iteration 43/1000 | Loss: 0.00022070
Iteration 44/1000 | Loss: 0.00032172
Iteration 45/1000 | Loss: 0.00020937
Iteration 46/1000 | Loss: 0.00030437
Iteration 47/1000 | Loss: 0.00024819
Iteration 48/1000 | Loss: 0.00021805
Iteration 49/1000 | Loss: 0.00021494
Iteration 50/1000 | Loss: 0.00002595
Iteration 51/1000 | Loss: 0.00016851
Iteration 52/1000 | Loss: 0.00020535
Iteration 53/1000 | Loss: 0.00001989
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00023904
Iteration 56/1000 | Loss: 0.00002343
Iteration 57/1000 | Loss: 0.00001743
Iteration 58/1000 | Loss: 0.00001604
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001502
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001418
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001414
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001403
Iteration 86/1000 | Loss: 0.00001403
Iteration 87/1000 | Loss: 0.00001403
Iteration 88/1000 | Loss: 0.00001403
Iteration 89/1000 | Loss: 0.00001403
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001402
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001399
Iteration 95/1000 | Loss: 0.00001398
Iteration 96/1000 | Loss: 0.00001398
Iteration 97/1000 | Loss: 0.00001398
Iteration 98/1000 | Loss: 0.00001398
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001397
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001396
Iteration 104/1000 | Loss: 0.00001396
Iteration 105/1000 | Loss: 0.00001396
Iteration 106/1000 | Loss: 0.00001396
Iteration 107/1000 | Loss: 0.00001395
Iteration 108/1000 | Loss: 0.00001395
Iteration 109/1000 | Loss: 0.00001394
Iteration 110/1000 | Loss: 0.00001394
Iteration 111/1000 | Loss: 0.00001394
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001393
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001393
Iteration 127/1000 | Loss: 0.00001393
Iteration 128/1000 | Loss: 0.00001393
Iteration 129/1000 | Loss: 0.00001393
Iteration 130/1000 | Loss: 0.00001392
Iteration 131/1000 | Loss: 0.00001392
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001392
Iteration 139/1000 | Loss: 0.00001391
Iteration 140/1000 | Loss: 0.00001391
Iteration 141/1000 | Loss: 0.00001391
Iteration 142/1000 | Loss: 0.00001391
Iteration 143/1000 | Loss: 0.00001391
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001391
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001390
Iteration 148/1000 | Loss: 0.00001390
Iteration 149/1000 | Loss: 0.00001390
Iteration 150/1000 | Loss: 0.00001390
Iteration 151/1000 | Loss: 0.00001390
Iteration 152/1000 | Loss: 0.00001390
Iteration 153/1000 | Loss: 0.00001390
Iteration 154/1000 | Loss: 0.00001390
Iteration 155/1000 | Loss: 0.00001390
Iteration 156/1000 | Loss: 0.00001389
Iteration 157/1000 | Loss: 0.00001389
Iteration 158/1000 | Loss: 0.00001389
Iteration 159/1000 | Loss: 0.00001389
Iteration 160/1000 | Loss: 0.00001389
Iteration 161/1000 | Loss: 0.00001389
Iteration 162/1000 | Loss: 0.00001389
Iteration 163/1000 | Loss: 0.00001389
Iteration 164/1000 | Loss: 0.00001389
Iteration 165/1000 | Loss: 0.00001389
Iteration 166/1000 | Loss: 0.00001389
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001389
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001389
Iteration 172/1000 | Loss: 0.00001389
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001389
Iteration 177/1000 | Loss: 0.00001389
Iteration 178/1000 | Loss: 0.00001389
Iteration 179/1000 | Loss: 0.00001389
Iteration 180/1000 | Loss: 0.00001389
Iteration 181/1000 | Loss: 0.00001389
Iteration 182/1000 | Loss: 0.00001389
Iteration 183/1000 | Loss: 0.00001389
Iteration 184/1000 | Loss: 0.00001389
Iteration 185/1000 | Loss: 0.00001389
Iteration 186/1000 | Loss: 0.00001389
Iteration 187/1000 | Loss: 0.00001389
Iteration 188/1000 | Loss: 0.00001389
Iteration 189/1000 | Loss: 0.00001389
Iteration 190/1000 | Loss: 0.00001389
Iteration 191/1000 | Loss: 0.00001389
Iteration 192/1000 | Loss: 0.00001389
Iteration 193/1000 | Loss: 0.00001389
Iteration 194/1000 | Loss: 0.00001389
Iteration 195/1000 | Loss: 0.00001389
Iteration 196/1000 | Loss: 0.00001389
Iteration 197/1000 | Loss: 0.00001389
Iteration 198/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.3886449778510723e-05, 1.3886449778510723e-05, 1.3886449778510723e-05, 1.3886449778510723e-05, 1.3886449778510723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3886449778510723e-05

Optimization complete. Final v2v error: 3.130164384841919 mm

Highest mean error: 4.202133655548096 mm for frame 64

Lowest mean error: 2.7170677185058594 mm for frame 107

Saving results

Total time: 144.16835236549377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813812
Iteration 2/25 | Loss: 0.00198974
Iteration 3/25 | Loss: 0.00135838
Iteration 4/25 | Loss: 0.00131227
Iteration 5/25 | Loss: 0.00127387
Iteration 6/25 | Loss: 0.00131736
Iteration 7/25 | Loss: 0.00126242
Iteration 8/25 | Loss: 0.00124636
Iteration 9/25 | Loss: 0.00124115
Iteration 10/25 | Loss: 0.00122300
Iteration 11/25 | Loss: 0.00122799
Iteration 12/25 | Loss: 0.00121910
Iteration 13/25 | Loss: 0.00121737
Iteration 14/25 | Loss: 0.00121715
Iteration 15/25 | Loss: 0.00121711
Iteration 16/25 | Loss: 0.00121710
Iteration 17/25 | Loss: 0.00121710
Iteration 18/25 | Loss: 0.00121710
Iteration 19/25 | Loss: 0.00121710
Iteration 20/25 | Loss: 0.00121710
Iteration 21/25 | Loss: 0.00121710
Iteration 22/25 | Loss: 0.00121710
Iteration 23/25 | Loss: 0.00121710
Iteration 24/25 | Loss: 0.00121710
Iteration 25/25 | Loss: 0.00121710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60927868
Iteration 2/25 | Loss: 0.00064456
Iteration 3/25 | Loss: 0.00064454
Iteration 4/25 | Loss: 0.00064453
Iteration 5/25 | Loss: 0.00064453
Iteration 6/25 | Loss: 0.00064453
Iteration 7/25 | Loss: 0.00064453
Iteration 8/25 | Loss: 0.00064453
Iteration 9/25 | Loss: 0.00064453
Iteration 10/25 | Loss: 0.00064453
Iteration 11/25 | Loss: 0.00064453
Iteration 12/25 | Loss: 0.00064453
Iteration 13/25 | Loss: 0.00064453
Iteration 14/25 | Loss: 0.00064453
Iteration 15/25 | Loss: 0.00064453
Iteration 16/25 | Loss: 0.00064453
Iteration 17/25 | Loss: 0.00064453
Iteration 18/25 | Loss: 0.00064453
Iteration 19/25 | Loss: 0.00064453
Iteration 20/25 | Loss: 0.00064453
Iteration 21/25 | Loss: 0.00064453
Iteration 22/25 | Loss: 0.00064453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006445312174037099, 0.0006445312174037099, 0.0006445312174037099, 0.0006445312174037099, 0.0006445312174037099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006445312174037099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064453
Iteration 2/1000 | Loss: 0.00002776
Iteration 3/1000 | Loss: 0.00002191
Iteration 4/1000 | Loss: 0.00002042
Iteration 5/1000 | Loss: 0.00001986
Iteration 6/1000 | Loss: 0.00001913
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001841
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00001785
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001782
Iteration 19/1000 | Loss: 0.00001774
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001760
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001757
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001756
Iteration 28/1000 | Loss: 0.00001756
Iteration 29/1000 | Loss: 0.00001756
Iteration 30/1000 | Loss: 0.00001756
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001753
Iteration 34/1000 | Loss: 0.00001752
Iteration 35/1000 | Loss: 0.00001752
Iteration 36/1000 | Loss: 0.00001747
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001744
Iteration 40/1000 | Loss: 0.00001744
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001744
Iteration 47/1000 | Loss: 0.00001744
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001743
Iteration 51/1000 | Loss: 0.00001743
Iteration 52/1000 | Loss: 0.00001743
Iteration 53/1000 | Loss: 0.00001743
Iteration 54/1000 | Loss: 0.00001743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [1.743427310429979e-05, 1.743427310429979e-05, 1.743427310429979e-05, 1.743427310429979e-05, 1.743427310429979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.743427310429979e-05

Optimization complete. Final v2v error: 3.50632381439209 mm

Highest mean error: 3.954876661300659 mm for frame 7

Lowest mean error: 3.18101167678833 mm for frame 117

Saving results

Total time: 51.21146035194397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810094
Iteration 2/25 | Loss: 0.00128667
Iteration 3/25 | Loss: 0.00111753
Iteration 4/25 | Loss: 0.00110290
Iteration 5/25 | Loss: 0.00109996
Iteration 6/25 | Loss: 0.00109996
Iteration 7/25 | Loss: 0.00109996
Iteration 8/25 | Loss: 0.00109996
Iteration 9/25 | Loss: 0.00109996
Iteration 10/25 | Loss: 0.00109996
Iteration 11/25 | Loss: 0.00109996
Iteration 12/25 | Loss: 0.00109996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010999604128301144, 0.0010999604128301144, 0.0010999604128301144, 0.0010999604128301144, 0.0010999604128301144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010999604128301144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35451901
Iteration 2/25 | Loss: 0.00086125
Iteration 3/25 | Loss: 0.00086125
Iteration 4/25 | Loss: 0.00086125
Iteration 5/25 | Loss: 0.00086125
Iteration 6/25 | Loss: 0.00086125
Iteration 7/25 | Loss: 0.00086125
Iteration 8/25 | Loss: 0.00086125
Iteration 9/25 | Loss: 0.00086124
Iteration 10/25 | Loss: 0.00086124
Iteration 11/25 | Loss: 0.00086124
Iteration 12/25 | Loss: 0.00086124
Iteration 13/25 | Loss: 0.00086124
Iteration 14/25 | Loss: 0.00086124
Iteration 15/25 | Loss: 0.00086124
Iteration 16/25 | Loss: 0.00086124
Iteration 17/25 | Loss: 0.00086124
Iteration 18/25 | Loss: 0.00086124
Iteration 19/25 | Loss: 0.00086124
Iteration 20/25 | Loss: 0.00086124
Iteration 21/25 | Loss: 0.00086124
Iteration 22/25 | Loss: 0.00086124
Iteration 23/25 | Loss: 0.00086124
Iteration 24/25 | Loss: 0.00086124
Iteration 25/25 | Loss: 0.00086124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008612440433353186, 0.0008612440433353186, 0.0008612440433353186, 0.0008612440433353186, 0.0008612440433353186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008612440433353186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086124
Iteration 2/1000 | Loss: 0.00002418
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001229
Iteration 5/1000 | Loss: 0.00001147
Iteration 6/1000 | Loss: 0.00001094
Iteration 7/1000 | Loss: 0.00001051
Iteration 8/1000 | Loss: 0.00001024
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000978
Iteration 11/1000 | Loss: 0.00000972
Iteration 12/1000 | Loss: 0.00000970
Iteration 13/1000 | Loss: 0.00000969
Iteration 14/1000 | Loss: 0.00000968
Iteration 15/1000 | Loss: 0.00000959
Iteration 16/1000 | Loss: 0.00000953
Iteration 17/1000 | Loss: 0.00000952
Iteration 18/1000 | Loss: 0.00000952
Iteration 19/1000 | Loss: 0.00000948
Iteration 20/1000 | Loss: 0.00000947
Iteration 21/1000 | Loss: 0.00000947
Iteration 22/1000 | Loss: 0.00000946
Iteration 23/1000 | Loss: 0.00000946
Iteration 24/1000 | Loss: 0.00000945
Iteration 25/1000 | Loss: 0.00000945
Iteration 26/1000 | Loss: 0.00000944
Iteration 27/1000 | Loss: 0.00000943
Iteration 28/1000 | Loss: 0.00000943
Iteration 29/1000 | Loss: 0.00000941
Iteration 30/1000 | Loss: 0.00000938
Iteration 31/1000 | Loss: 0.00000937
Iteration 32/1000 | Loss: 0.00000937
Iteration 33/1000 | Loss: 0.00000936
Iteration 34/1000 | Loss: 0.00000936
Iteration 35/1000 | Loss: 0.00000935
Iteration 36/1000 | Loss: 0.00000935
Iteration 37/1000 | Loss: 0.00000934
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000934
Iteration 40/1000 | Loss: 0.00000934
Iteration 41/1000 | Loss: 0.00000933
Iteration 42/1000 | Loss: 0.00000933
Iteration 43/1000 | Loss: 0.00000933
Iteration 44/1000 | Loss: 0.00000932
Iteration 45/1000 | Loss: 0.00000932
Iteration 46/1000 | Loss: 0.00000930
Iteration 47/1000 | Loss: 0.00000929
Iteration 48/1000 | Loss: 0.00000929
Iteration 49/1000 | Loss: 0.00000929
Iteration 50/1000 | Loss: 0.00000929
Iteration 51/1000 | Loss: 0.00000929
Iteration 52/1000 | Loss: 0.00000929
Iteration 53/1000 | Loss: 0.00000929
Iteration 54/1000 | Loss: 0.00000929
Iteration 55/1000 | Loss: 0.00000928
Iteration 56/1000 | Loss: 0.00000928
Iteration 57/1000 | Loss: 0.00000928
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000927
Iteration 60/1000 | Loss: 0.00000926
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000925
Iteration 63/1000 | Loss: 0.00000925
Iteration 64/1000 | Loss: 0.00000925
Iteration 65/1000 | Loss: 0.00000925
Iteration 66/1000 | Loss: 0.00000925
Iteration 67/1000 | Loss: 0.00000924
Iteration 68/1000 | Loss: 0.00000924
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000923
Iteration 73/1000 | Loss: 0.00000923
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000922
Iteration 76/1000 | Loss: 0.00000922
Iteration 77/1000 | Loss: 0.00000922
Iteration 78/1000 | Loss: 0.00000922
Iteration 79/1000 | Loss: 0.00000922
Iteration 80/1000 | Loss: 0.00000921
Iteration 81/1000 | Loss: 0.00000921
Iteration 82/1000 | Loss: 0.00000921
Iteration 83/1000 | Loss: 0.00000921
Iteration 84/1000 | Loss: 0.00000921
Iteration 85/1000 | Loss: 0.00000921
Iteration 86/1000 | Loss: 0.00000921
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000920
Iteration 90/1000 | Loss: 0.00000920
Iteration 91/1000 | Loss: 0.00000920
Iteration 92/1000 | Loss: 0.00000920
Iteration 93/1000 | Loss: 0.00000920
Iteration 94/1000 | Loss: 0.00000920
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000919
Iteration 97/1000 | Loss: 0.00000919
Iteration 98/1000 | Loss: 0.00000919
Iteration 99/1000 | Loss: 0.00000919
Iteration 100/1000 | Loss: 0.00000919
Iteration 101/1000 | Loss: 0.00000919
Iteration 102/1000 | Loss: 0.00000918
Iteration 103/1000 | Loss: 0.00000918
Iteration 104/1000 | Loss: 0.00000918
Iteration 105/1000 | Loss: 0.00000917
Iteration 106/1000 | Loss: 0.00000917
Iteration 107/1000 | Loss: 0.00000917
Iteration 108/1000 | Loss: 0.00000917
Iteration 109/1000 | Loss: 0.00000917
Iteration 110/1000 | Loss: 0.00000916
Iteration 111/1000 | Loss: 0.00000916
Iteration 112/1000 | Loss: 0.00000916
Iteration 113/1000 | Loss: 0.00000916
Iteration 114/1000 | Loss: 0.00000916
Iteration 115/1000 | Loss: 0.00000916
Iteration 116/1000 | Loss: 0.00000916
Iteration 117/1000 | Loss: 0.00000916
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [9.160835361399222e-06, 9.160835361399222e-06, 9.160835361399222e-06, 9.160835361399222e-06, 9.160835361399222e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.160835361399222e-06

Optimization complete. Final v2v error: 2.6069345474243164 mm

Highest mean error: 2.7913055419921875 mm for frame 150

Lowest mean error: 2.464956521987915 mm for frame 78

Saving results

Total time: 38.3165602684021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715194
Iteration 2/25 | Loss: 0.00165942
Iteration 3/25 | Loss: 0.00133300
Iteration 4/25 | Loss: 0.00126804
Iteration 5/25 | Loss: 0.00124555
Iteration 6/25 | Loss: 0.00123960
Iteration 7/25 | Loss: 0.00124927
Iteration 8/25 | Loss: 0.00124462
Iteration 9/25 | Loss: 0.00123915
Iteration 10/25 | Loss: 0.00123743
Iteration 11/25 | Loss: 0.00123558
Iteration 12/25 | Loss: 0.00123773
Iteration 13/25 | Loss: 0.00124080
Iteration 14/25 | Loss: 0.00124832
Iteration 15/25 | Loss: 0.00124125
Iteration 16/25 | Loss: 0.00122985
Iteration 17/25 | Loss: 0.00122875
Iteration 18/25 | Loss: 0.00120890
Iteration 19/25 | Loss: 0.00119972
Iteration 20/25 | Loss: 0.00119571
Iteration 21/25 | Loss: 0.00119436
Iteration 22/25 | Loss: 0.00118887
Iteration 23/25 | Loss: 0.00118542
Iteration 24/25 | Loss: 0.00119212
Iteration 25/25 | Loss: 0.00119131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.08503342
Iteration 2/25 | Loss: 0.00158783
Iteration 3/25 | Loss: 0.00158764
Iteration 4/25 | Loss: 0.00158764
Iteration 5/25 | Loss: 0.00158764
Iteration 6/25 | Loss: 0.00158764
Iteration 7/25 | Loss: 0.00158764
Iteration 8/25 | Loss: 0.00158764
Iteration 9/25 | Loss: 0.00158764
Iteration 10/25 | Loss: 0.00158764
Iteration 11/25 | Loss: 0.00158764
Iteration 12/25 | Loss: 0.00158764
Iteration 13/25 | Loss: 0.00158764
Iteration 14/25 | Loss: 0.00158764
Iteration 15/25 | Loss: 0.00158764
Iteration 16/25 | Loss: 0.00158764
Iteration 17/25 | Loss: 0.00158764
Iteration 18/25 | Loss: 0.00158764
Iteration 19/25 | Loss: 0.00158764
Iteration 20/25 | Loss: 0.00158764
Iteration 21/25 | Loss: 0.00158764
Iteration 22/25 | Loss: 0.00158764
Iteration 23/25 | Loss: 0.00158764
Iteration 24/25 | Loss: 0.00158764
Iteration 25/25 | Loss: 0.00158764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158764
Iteration 2/1000 | Loss: 0.00014896
Iteration 3/1000 | Loss: 0.00020302
Iteration 4/1000 | Loss: 0.00007504
Iteration 5/1000 | Loss: 0.00967710
Iteration 6/1000 | Loss: 0.00480624
Iteration 7/1000 | Loss: 0.00074617
Iteration 8/1000 | Loss: 0.00192600
Iteration 9/1000 | Loss: 0.00014179
Iteration 10/1000 | Loss: 0.00612934
Iteration 11/1000 | Loss: 0.00013117
Iteration 12/1000 | Loss: 0.00007948
Iteration 13/1000 | Loss: 0.00005242
Iteration 14/1000 | Loss: 0.00004260
Iteration 15/1000 | Loss: 0.00003586
Iteration 16/1000 | Loss: 0.00003333
Iteration 17/1000 | Loss: 0.00003066
Iteration 18/1000 | Loss: 0.00002906
Iteration 19/1000 | Loss: 0.00002792
Iteration 20/1000 | Loss: 0.00002703
Iteration 21/1000 | Loss: 0.00002627
Iteration 22/1000 | Loss: 0.00002572
Iteration 23/1000 | Loss: 0.00002536
Iteration 24/1000 | Loss: 0.00002499
Iteration 25/1000 | Loss: 0.00002442
Iteration 26/1000 | Loss: 0.00002352
Iteration 27/1000 | Loss: 0.00023925
Iteration 28/1000 | Loss: 0.00007281
Iteration 29/1000 | Loss: 0.00021133
Iteration 30/1000 | Loss: 0.00003545
Iteration 31/1000 | Loss: 0.00007472
Iteration 32/1000 | Loss: 0.00013538
Iteration 33/1000 | Loss: 0.00002614
Iteration 34/1000 | Loss: 0.00002374
Iteration 35/1000 | Loss: 0.00002241
Iteration 36/1000 | Loss: 0.00002183
Iteration 37/1000 | Loss: 0.00002155
Iteration 38/1000 | Loss: 0.00002149
Iteration 39/1000 | Loss: 0.00002134
Iteration 40/1000 | Loss: 0.00002127
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002116
Iteration 43/1000 | Loss: 0.00002116
Iteration 44/1000 | Loss: 0.00002114
Iteration 45/1000 | Loss: 0.00002113
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002112
Iteration 49/1000 | Loss: 0.00002112
Iteration 50/1000 | Loss: 0.00002111
Iteration 51/1000 | Loss: 0.00002111
Iteration 52/1000 | Loss: 0.00002111
Iteration 53/1000 | Loss: 0.00002110
Iteration 54/1000 | Loss: 0.00002110
Iteration 55/1000 | Loss: 0.00002109
Iteration 56/1000 | Loss: 0.00002097
Iteration 57/1000 | Loss: 0.00002094
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002091
Iteration 60/1000 | Loss: 0.00002088
Iteration 61/1000 | Loss: 0.00002088
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002086
Iteration 65/1000 | Loss: 0.00002086
Iteration 66/1000 | Loss: 0.00002086
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002085
Iteration 69/1000 | Loss: 0.00002085
Iteration 70/1000 | Loss: 0.00002084
Iteration 71/1000 | Loss: 0.00002084
Iteration 72/1000 | Loss: 0.00002080
Iteration 73/1000 | Loss: 0.00002080
Iteration 74/1000 | Loss: 0.00002079
Iteration 75/1000 | Loss: 0.00002078
Iteration 76/1000 | Loss: 0.00002078
Iteration 77/1000 | Loss: 0.00002077
Iteration 78/1000 | Loss: 0.00002077
Iteration 79/1000 | Loss: 0.00002075
Iteration 80/1000 | Loss: 0.00002074
Iteration 81/1000 | Loss: 0.00002072
Iteration 82/1000 | Loss: 0.00002072
Iteration 83/1000 | Loss: 0.00002072
Iteration 84/1000 | Loss: 0.00002072
Iteration 85/1000 | Loss: 0.00002072
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002072
Iteration 88/1000 | Loss: 0.00002072
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002072
Iteration 91/1000 | Loss: 0.00002072
Iteration 92/1000 | Loss: 0.00002071
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002068
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002067
Iteration 99/1000 | Loss: 0.00002067
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002065
Iteration 103/1000 | Loss: 0.00002065
Iteration 104/1000 | Loss: 0.00002064
Iteration 105/1000 | Loss: 0.00002064
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002063
Iteration 108/1000 | Loss: 0.00002063
Iteration 109/1000 | Loss: 0.00002063
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00002062
Iteration 115/1000 | Loss: 0.00002062
Iteration 116/1000 | Loss: 0.00002062
Iteration 117/1000 | Loss: 0.00002062
Iteration 118/1000 | Loss: 0.00002062
Iteration 119/1000 | Loss: 0.00002061
Iteration 120/1000 | Loss: 0.00002061
Iteration 121/1000 | Loss: 0.00002061
Iteration 122/1000 | Loss: 0.00002061
Iteration 123/1000 | Loss: 0.00002061
Iteration 124/1000 | Loss: 0.00002061
Iteration 125/1000 | Loss: 0.00002061
Iteration 126/1000 | Loss: 0.00002061
Iteration 127/1000 | Loss: 0.00002061
Iteration 128/1000 | Loss: 0.00002060
Iteration 129/1000 | Loss: 0.00002060
Iteration 130/1000 | Loss: 0.00002060
Iteration 131/1000 | Loss: 0.00002060
Iteration 132/1000 | Loss: 0.00002060
Iteration 133/1000 | Loss: 0.00002060
Iteration 134/1000 | Loss: 0.00002060
Iteration 135/1000 | Loss: 0.00002060
Iteration 136/1000 | Loss: 0.00002060
Iteration 137/1000 | Loss: 0.00002060
Iteration 138/1000 | Loss: 0.00002060
Iteration 139/1000 | Loss: 0.00002060
Iteration 140/1000 | Loss: 0.00002060
Iteration 141/1000 | Loss: 0.00002060
Iteration 142/1000 | Loss: 0.00002060
Iteration 143/1000 | Loss: 0.00002060
Iteration 144/1000 | Loss: 0.00002060
Iteration 145/1000 | Loss: 0.00002060
Iteration 146/1000 | Loss: 0.00002060
Iteration 147/1000 | Loss: 0.00002060
Iteration 148/1000 | Loss: 0.00002060
Iteration 149/1000 | Loss: 0.00002060
Iteration 150/1000 | Loss: 0.00002060
Iteration 151/1000 | Loss: 0.00002060
Iteration 152/1000 | Loss: 0.00002060
Iteration 153/1000 | Loss: 0.00002060
Iteration 154/1000 | Loss: 0.00002060
Iteration 155/1000 | Loss: 0.00002060
Iteration 156/1000 | Loss: 0.00002060
Iteration 157/1000 | Loss: 0.00002060
Iteration 158/1000 | Loss: 0.00002060
Iteration 159/1000 | Loss: 0.00002060
Iteration 160/1000 | Loss: 0.00002060
Iteration 161/1000 | Loss: 0.00002060
Iteration 162/1000 | Loss: 0.00002060
Iteration 163/1000 | Loss: 0.00002060
Iteration 164/1000 | Loss: 0.00002060
Iteration 165/1000 | Loss: 0.00002060
Iteration 166/1000 | Loss: 0.00002060
Iteration 167/1000 | Loss: 0.00002060
Iteration 168/1000 | Loss: 0.00002060
Iteration 169/1000 | Loss: 0.00002060
Iteration 170/1000 | Loss: 0.00002060
Iteration 171/1000 | Loss: 0.00002060
Iteration 172/1000 | Loss: 0.00002060
Iteration 173/1000 | Loss: 0.00002060
Iteration 174/1000 | Loss: 0.00002060
Iteration 175/1000 | Loss: 0.00002060
Iteration 176/1000 | Loss: 0.00002060
Iteration 177/1000 | Loss: 0.00002060
Iteration 178/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.0597321054083295e-05, 2.0597321054083295e-05, 2.0597321054083295e-05, 2.0597321054083295e-05, 2.0597321054083295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0597321054083295e-05

Optimization complete. Final v2v error: 3.6338346004486084 mm

Highest mean error: 5.612636089324951 mm for frame 56

Lowest mean error: 2.617563247680664 mm for frame 16

Saving results

Total time: 115.58106970787048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997387
Iteration 2/25 | Loss: 0.00997387
Iteration 3/25 | Loss: 0.00997386
Iteration 4/25 | Loss: 0.00997386
Iteration 5/25 | Loss: 0.00337362
Iteration 6/25 | Loss: 0.00212435
Iteration 7/25 | Loss: 0.00195092
Iteration 8/25 | Loss: 0.00180331
Iteration 9/25 | Loss: 0.00176930
Iteration 10/25 | Loss: 0.00171229
Iteration 11/25 | Loss: 0.00160665
Iteration 12/25 | Loss: 0.00152827
Iteration 13/25 | Loss: 0.00151354
Iteration 14/25 | Loss: 0.00153069
Iteration 15/25 | Loss: 0.00146166
Iteration 16/25 | Loss: 0.00142835
Iteration 17/25 | Loss: 0.00140948
Iteration 18/25 | Loss: 0.00140122
Iteration 19/25 | Loss: 0.00140497
Iteration 20/25 | Loss: 0.00139828
Iteration 21/25 | Loss: 0.00137499
Iteration 22/25 | Loss: 0.00136058
Iteration 23/25 | Loss: 0.00136371
Iteration 24/25 | Loss: 0.00135965
Iteration 25/25 | Loss: 0.00136354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34505713
Iteration 2/25 | Loss: 0.00398512
Iteration 3/25 | Loss: 0.00321403
Iteration 4/25 | Loss: 0.00321402
Iteration 5/25 | Loss: 0.00321402
Iteration 6/25 | Loss: 0.00321402
Iteration 7/25 | Loss: 0.00321402
Iteration 8/25 | Loss: 0.00321402
Iteration 9/25 | Loss: 0.00321402
Iteration 10/25 | Loss: 0.00321402
Iteration 11/25 | Loss: 0.00321402
Iteration 12/25 | Loss: 0.00321402
Iteration 13/25 | Loss: 0.00321402
Iteration 14/25 | Loss: 0.00321402
Iteration 15/25 | Loss: 0.00321402
Iteration 16/25 | Loss: 0.00321402
Iteration 17/25 | Loss: 0.00321402
Iteration 18/25 | Loss: 0.00321402
Iteration 19/25 | Loss: 0.00321402
Iteration 20/25 | Loss: 0.00321402
Iteration 21/25 | Loss: 0.00321402
Iteration 22/25 | Loss: 0.00321402
Iteration 23/25 | Loss: 0.00321402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0032140174880623817, 0.0032140174880623817, 0.0032140174880623817, 0.0032140174880623817, 0.0032140174880623817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032140174880623817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321402
Iteration 2/1000 | Loss: 0.00077157
Iteration 3/1000 | Loss: 0.00057561
Iteration 4/1000 | Loss: 0.00097256
Iteration 5/1000 | Loss: 0.00033475
Iteration 6/1000 | Loss: 0.00069140
Iteration 7/1000 | Loss: 0.00098165
Iteration 8/1000 | Loss: 0.00028019
Iteration 9/1000 | Loss: 0.00024277
Iteration 10/1000 | Loss: 0.00024306
Iteration 11/1000 | Loss: 0.00042573
Iteration 12/1000 | Loss: 0.00020777
Iteration 13/1000 | Loss: 0.00030851
Iteration 14/1000 | Loss: 0.00018185
Iteration 15/1000 | Loss: 0.00022016
Iteration 16/1000 | Loss: 0.00016295
Iteration 17/1000 | Loss: 0.00054521
Iteration 18/1000 | Loss: 0.00091512
Iteration 19/1000 | Loss: 0.00057148
Iteration 20/1000 | Loss: 0.00038674
Iteration 21/1000 | Loss: 0.00015527
Iteration 22/1000 | Loss: 0.00015807
Iteration 23/1000 | Loss: 0.00039992
Iteration 24/1000 | Loss: 0.00054411
Iteration 25/1000 | Loss: 0.00088983
Iteration 26/1000 | Loss: 0.00050149
Iteration 27/1000 | Loss: 0.00058835
Iteration 28/1000 | Loss: 0.00093585
Iteration 29/1000 | Loss: 0.00173727
Iteration 30/1000 | Loss: 0.00098478
Iteration 31/1000 | Loss: 0.00106009
Iteration 32/1000 | Loss: 0.00049319
Iteration 33/1000 | Loss: 0.00053211
Iteration 34/1000 | Loss: 0.00020741
Iteration 35/1000 | Loss: 0.00052775
Iteration 36/1000 | Loss: 0.00057622
Iteration 37/1000 | Loss: 0.00019284
Iteration 38/1000 | Loss: 0.00144391
Iteration 39/1000 | Loss: 0.00015377
Iteration 40/1000 | Loss: 0.00024868
Iteration 41/1000 | Loss: 0.00031594
Iteration 42/1000 | Loss: 0.00034496
Iteration 43/1000 | Loss: 0.00018788
Iteration 44/1000 | Loss: 0.00013731
Iteration 45/1000 | Loss: 0.00013622
Iteration 46/1000 | Loss: 0.00014494
Iteration 47/1000 | Loss: 0.00013015
Iteration 48/1000 | Loss: 0.00013437
Iteration 49/1000 | Loss: 0.00014218
Iteration 50/1000 | Loss: 0.00028400
Iteration 51/1000 | Loss: 0.00085450
Iteration 52/1000 | Loss: 0.00039897
Iteration 53/1000 | Loss: 0.00049035
Iteration 54/1000 | Loss: 0.00028865
Iteration 55/1000 | Loss: 0.00032272
Iteration 56/1000 | Loss: 0.00014141
Iteration 57/1000 | Loss: 0.00012655
Iteration 58/1000 | Loss: 0.00014114
Iteration 59/1000 | Loss: 0.00013374
Iteration 60/1000 | Loss: 0.00016675
Iteration 61/1000 | Loss: 0.00014577
Iteration 62/1000 | Loss: 0.00015024
Iteration 63/1000 | Loss: 0.00012244
Iteration 64/1000 | Loss: 0.00014383
Iteration 65/1000 | Loss: 0.00011740
Iteration 66/1000 | Loss: 0.00012078
Iteration 67/1000 | Loss: 0.00011687
Iteration 68/1000 | Loss: 0.00012869
Iteration 69/1000 | Loss: 0.00013237
Iteration 70/1000 | Loss: 0.00011915
Iteration 71/1000 | Loss: 0.00012406
Iteration 72/1000 | Loss: 0.00012664
Iteration 73/1000 | Loss: 0.00028338
Iteration 74/1000 | Loss: 0.00022131
Iteration 75/1000 | Loss: 0.00037125
Iteration 76/1000 | Loss: 0.00022815
Iteration 77/1000 | Loss: 0.00022973
Iteration 78/1000 | Loss: 0.00021996
Iteration 79/1000 | Loss: 0.00023165
Iteration 80/1000 | Loss: 0.00020263
Iteration 81/1000 | Loss: 0.00022537
Iteration 82/1000 | Loss: 0.00043282
Iteration 83/1000 | Loss: 0.00030685
Iteration 84/1000 | Loss: 0.00021666
Iteration 85/1000 | Loss: 0.00019427
Iteration 86/1000 | Loss: 0.00013214
Iteration 87/1000 | Loss: 0.00011058
Iteration 88/1000 | Loss: 0.00013552
Iteration 89/1000 | Loss: 0.00014158
Iteration 90/1000 | Loss: 0.00012009
Iteration 91/1000 | Loss: 0.00010783
Iteration 92/1000 | Loss: 0.00011254
Iteration 93/1000 | Loss: 0.00011197
Iteration 94/1000 | Loss: 0.00012149
Iteration 95/1000 | Loss: 0.00010466
Iteration 96/1000 | Loss: 0.00011944
Iteration 97/1000 | Loss: 0.00012301
Iteration 98/1000 | Loss: 0.00013143
Iteration 99/1000 | Loss: 0.00012985
Iteration 100/1000 | Loss: 0.00011600
Iteration 101/1000 | Loss: 0.00010540
Iteration 102/1000 | Loss: 0.00009841
Iteration 103/1000 | Loss: 0.00011601
Iteration 104/1000 | Loss: 0.00012179
Iteration 105/1000 | Loss: 0.00011230
Iteration 106/1000 | Loss: 0.00013321
Iteration 107/1000 | Loss: 0.00011653
Iteration 108/1000 | Loss: 0.00053732
Iteration 109/1000 | Loss: 0.00010673
Iteration 110/1000 | Loss: 0.00011871
Iteration 111/1000 | Loss: 0.00052134
Iteration 112/1000 | Loss: 0.00078477
Iteration 113/1000 | Loss: 0.00061349
Iteration 114/1000 | Loss: 0.00014213
Iteration 115/1000 | Loss: 0.00012176
Iteration 116/1000 | Loss: 0.00013491
Iteration 117/1000 | Loss: 0.00011487
Iteration 118/1000 | Loss: 0.00012167
Iteration 119/1000 | Loss: 0.00009352
Iteration 120/1000 | Loss: 0.00009812
Iteration 121/1000 | Loss: 0.00010366
Iteration 122/1000 | Loss: 0.00009563
Iteration 123/1000 | Loss: 0.00009740
Iteration 124/1000 | Loss: 0.00010941
Iteration 125/1000 | Loss: 0.00051273
Iteration 126/1000 | Loss: 0.00064990
Iteration 127/1000 | Loss: 0.00013900
Iteration 128/1000 | Loss: 0.00020199
Iteration 129/1000 | Loss: 0.00015205
Iteration 130/1000 | Loss: 0.00011203
Iteration 131/1000 | Loss: 0.00010591
Iteration 132/1000 | Loss: 0.00018056
Iteration 133/1000 | Loss: 0.00009044
Iteration 134/1000 | Loss: 0.00021187
Iteration 135/1000 | Loss: 0.00008957
Iteration 136/1000 | Loss: 0.00010284
Iteration 137/1000 | Loss: 0.00009135
Iteration 138/1000 | Loss: 0.00009748
Iteration 139/1000 | Loss: 0.00011401
Iteration 140/1000 | Loss: 0.00017493
Iteration 141/1000 | Loss: 0.00008774
Iteration 142/1000 | Loss: 0.00024965
Iteration 143/1000 | Loss: 0.00009881
Iteration 144/1000 | Loss: 0.00010080
Iteration 145/1000 | Loss: 0.00009008
Iteration 146/1000 | Loss: 0.00008309
Iteration 147/1000 | Loss: 0.00008623
Iteration 148/1000 | Loss: 0.00011851
Iteration 149/1000 | Loss: 0.00048859
Iteration 150/1000 | Loss: 0.00009421
Iteration 151/1000 | Loss: 0.00017145
Iteration 152/1000 | Loss: 0.00008250
Iteration 153/1000 | Loss: 0.00008153
Iteration 154/1000 | Loss: 0.00036402
Iteration 155/1000 | Loss: 0.00009116
Iteration 156/1000 | Loss: 0.00009129
Iteration 157/1000 | Loss: 0.00008871
Iteration 158/1000 | Loss: 0.00008887
Iteration 159/1000 | Loss: 0.00009010
Iteration 160/1000 | Loss: 0.00008229
Iteration 161/1000 | Loss: 0.00008610
Iteration 162/1000 | Loss: 0.00008387
Iteration 163/1000 | Loss: 0.00008374
Iteration 164/1000 | Loss: 0.00013291
Iteration 165/1000 | Loss: 0.00009045
Iteration 166/1000 | Loss: 0.00008948
Iteration 167/1000 | Loss: 0.00029152
Iteration 168/1000 | Loss: 0.00022437
Iteration 169/1000 | Loss: 0.00020567
Iteration 170/1000 | Loss: 0.00008578
Iteration 171/1000 | Loss: 0.00008850
Iteration 172/1000 | Loss: 0.00008394
Iteration 173/1000 | Loss: 0.00008621
Iteration 174/1000 | Loss: 0.00008729
Iteration 175/1000 | Loss: 0.00008763
Iteration 176/1000 | Loss: 0.00008713
Iteration 177/1000 | Loss: 0.00008971
Iteration 178/1000 | Loss: 0.00008950
Iteration 179/1000 | Loss: 0.00009008
Iteration 180/1000 | Loss: 0.00008857
Iteration 181/1000 | Loss: 0.00009077
Iteration 182/1000 | Loss: 0.00010235
Iteration 183/1000 | Loss: 0.00008078
Iteration 184/1000 | Loss: 0.00008522
Iteration 185/1000 | Loss: 0.00007835
Iteration 186/1000 | Loss: 0.00007782
Iteration 187/1000 | Loss: 0.00007741
Iteration 188/1000 | Loss: 0.00008152
Iteration 189/1000 | Loss: 0.00007864
Iteration 190/1000 | Loss: 0.00007630
Iteration 191/1000 | Loss: 0.00007605
Iteration 192/1000 | Loss: 0.00007598
Iteration 193/1000 | Loss: 0.00007591
Iteration 194/1000 | Loss: 0.00007782
Iteration 195/1000 | Loss: 0.00007570
Iteration 196/1000 | Loss: 0.00007569
Iteration 197/1000 | Loss: 0.00007569
Iteration 198/1000 | Loss: 0.00007568
Iteration 199/1000 | Loss: 0.00007568
Iteration 200/1000 | Loss: 0.00007568
Iteration 201/1000 | Loss: 0.00007568
Iteration 202/1000 | Loss: 0.00007568
Iteration 203/1000 | Loss: 0.00007568
Iteration 204/1000 | Loss: 0.00007913
Iteration 205/1000 | Loss: 0.00007560
Iteration 206/1000 | Loss: 0.00007557
Iteration 207/1000 | Loss: 0.00007557
Iteration 208/1000 | Loss: 0.00007557
Iteration 209/1000 | Loss: 0.00007557
Iteration 210/1000 | Loss: 0.00007557
Iteration 211/1000 | Loss: 0.00007557
Iteration 212/1000 | Loss: 0.00007557
Iteration 213/1000 | Loss: 0.00007557
Iteration 214/1000 | Loss: 0.00007557
Iteration 215/1000 | Loss: 0.00007556
Iteration 216/1000 | Loss: 0.00007556
Iteration 217/1000 | Loss: 0.00007553
Iteration 218/1000 | Loss: 0.00007551
Iteration 219/1000 | Loss: 0.00007551
Iteration 220/1000 | Loss: 0.00007551
Iteration 221/1000 | Loss: 0.00007551
Iteration 222/1000 | Loss: 0.00007551
Iteration 223/1000 | Loss: 0.00007551
Iteration 224/1000 | Loss: 0.00007551
Iteration 225/1000 | Loss: 0.00007551
Iteration 226/1000 | Loss: 0.00007541
Iteration 227/1000 | Loss: 0.00007538
Iteration 228/1000 | Loss: 0.00007538
Iteration 229/1000 | Loss: 0.00007538
Iteration 230/1000 | Loss: 0.00007537
Iteration 231/1000 | Loss: 0.00007532
Iteration 232/1000 | Loss: 0.00007529
Iteration 233/1000 | Loss: 0.00007528
Iteration 234/1000 | Loss: 0.00007528
Iteration 235/1000 | Loss: 0.00008554
Iteration 236/1000 | Loss: 0.00007524
Iteration 237/1000 | Loss: 0.00007517
Iteration 238/1000 | Loss: 0.00007517
Iteration 239/1000 | Loss: 0.00007516
Iteration 240/1000 | Loss: 0.00007516
Iteration 241/1000 | Loss: 0.00007516
Iteration 242/1000 | Loss: 0.00007515
Iteration 243/1000 | Loss: 0.00008005
Iteration 244/1000 | Loss: 0.00007511
Iteration 245/1000 | Loss: 0.00007511
Iteration 246/1000 | Loss: 0.00007511
Iteration 247/1000 | Loss: 0.00007509
Iteration 248/1000 | Loss: 0.00007508
Iteration 249/1000 | Loss: 0.00007507
Iteration 250/1000 | Loss: 0.00007506
Iteration 251/1000 | Loss: 0.00007505
Iteration 252/1000 | Loss: 0.00007505
Iteration 253/1000 | Loss: 0.00007505
Iteration 254/1000 | Loss: 0.00007504
Iteration 255/1000 | Loss: 0.00007503
Iteration 256/1000 | Loss: 0.00007503
Iteration 257/1000 | Loss: 0.00007499
Iteration 258/1000 | Loss: 0.00007663
Iteration 259/1000 | Loss: 0.00007483
Iteration 260/1000 | Loss: 0.00007483
Iteration 261/1000 | Loss: 0.00007483
Iteration 262/1000 | Loss: 0.00007482
Iteration 263/1000 | Loss: 0.00007482
Iteration 264/1000 | Loss: 0.00007482
Iteration 265/1000 | Loss: 0.00007481
Iteration 266/1000 | Loss: 0.00007481
Iteration 267/1000 | Loss: 0.00007472
Iteration 268/1000 | Loss: 0.00008246
Iteration 269/1000 | Loss: 0.00007465
Iteration 270/1000 | Loss: 0.00007464
Iteration 271/1000 | Loss: 0.00007464
Iteration 272/1000 | Loss: 0.00007463
Iteration 273/1000 | Loss: 0.00007461
Iteration 274/1000 | Loss: 0.00007461
Iteration 275/1000 | Loss: 0.00007461
Iteration 276/1000 | Loss: 0.00007461
Iteration 277/1000 | Loss: 0.00007461
Iteration 278/1000 | Loss: 0.00007460
Iteration 279/1000 | Loss: 0.00007460
Iteration 280/1000 | Loss: 0.00007460
Iteration 281/1000 | Loss: 0.00007459
Iteration 282/1000 | Loss: 0.00007456
Iteration 283/1000 | Loss: 0.00007456
Iteration 284/1000 | Loss: 0.00007456
Iteration 285/1000 | Loss: 0.00007456
Iteration 286/1000 | Loss: 0.00007455
Iteration 287/1000 | Loss: 0.00040626
Iteration 288/1000 | Loss: 0.00030677
Iteration 289/1000 | Loss: 0.00017812
Iteration 290/1000 | Loss: 0.00016913
Iteration 291/1000 | Loss: 0.00017556
Iteration 292/1000 | Loss: 0.00007969
Iteration 293/1000 | Loss: 0.00007736
Iteration 294/1000 | Loss: 0.00053796
Iteration 295/1000 | Loss: 0.00009059
Iteration 296/1000 | Loss: 0.00008152
Iteration 297/1000 | Loss: 0.00007650
Iteration 298/1000 | Loss: 0.00028199
Iteration 299/1000 | Loss: 0.00009712
Iteration 300/1000 | Loss: 0.00023569
Iteration 301/1000 | Loss: 0.00014715
Iteration 302/1000 | Loss: 0.00008894
Iteration 303/1000 | Loss: 0.00018121
Iteration 304/1000 | Loss: 0.00011489
Iteration 305/1000 | Loss: 0.00013709
Iteration 306/1000 | Loss: 0.00011279
Iteration 307/1000 | Loss: 0.00016946
Iteration 308/1000 | Loss: 0.00027679
Iteration 309/1000 | Loss: 0.00019948
Iteration 310/1000 | Loss: 0.00013629
Iteration 311/1000 | Loss: 0.00007590
Iteration 312/1000 | Loss: 0.00010012
Iteration 313/1000 | Loss: 0.00014909
Iteration 314/1000 | Loss: 0.00025850
Iteration 315/1000 | Loss: 0.00012556
Iteration 316/1000 | Loss: 0.00013122
Iteration 317/1000 | Loss: 0.00039803
Iteration 318/1000 | Loss: 0.00027919
Iteration 319/1000 | Loss: 0.00032529
Iteration 320/1000 | Loss: 0.00015229
Iteration 321/1000 | Loss: 0.00025944
Iteration 322/1000 | Loss: 0.00009957
Iteration 323/1000 | Loss: 0.00008342
Iteration 324/1000 | Loss: 0.00008107
Iteration 325/1000 | Loss: 0.00007679
Iteration 326/1000 | Loss: 0.00007573
Iteration 327/1000 | Loss: 0.00007559
Iteration 328/1000 | Loss: 0.00013919
Iteration 329/1000 | Loss: 0.00007387
Iteration 330/1000 | Loss: 0.00014254
Iteration 331/1000 | Loss: 0.00007317
Iteration 332/1000 | Loss: 0.00007361
Iteration 333/1000 | Loss: 0.00007286
Iteration 334/1000 | Loss: 0.00007268
Iteration 335/1000 | Loss: 0.00007266
Iteration 336/1000 | Loss: 0.00007701
Iteration 337/1000 | Loss: 0.00007314
Iteration 338/1000 | Loss: 0.00007260
Iteration 339/1000 | Loss: 0.00007259
Iteration 340/1000 | Loss: 0.00007259
Iteration 341/1000 | Loss: 0.00007258
Iteration 342/1000 | Loss: 0.00007258
Iteration 343/1000 | Loss: 0.00007256
Iteration 344/1000 | Loss: 0.00007256
Iteration 345/1000 | Loss: 0.00007334
Iteration 346/1000 | Loss: 0.00007241
Iteration 347/1000 | Loss: 0.00007240
Iteration 348/1000 | Loss: 0.00007240
Iteration 349/1000 | Loss: 0.00007240
Iteration 350/1000 | Loss: 0.00007239
Iteration 351/1000 | Loss: 0.00007237
Iteration 352/1000 | Loss: 0.00021546
Iteration 353/1000 | Loss: 0.00007306
Iteration 354/1000 | Loss: 0.00010204
Iteration 355/1000 | Loss: 0.00007153
Iteration 356/1000 | Loss: 0.00007075
Iteration 357/1000 | Loss: 0.00026082
Iteration 358/1000 | Loss: 0.00010523
Iteration 359/1000 | Loss: 0.00015441
Iteration 360/1000 | Loss: 0.00012576
Iteration 361/1000 | Loss: 0.00008187
Iteration 362/1000 | Loss: 0.00036019
Iteration 363/1000 | Loss: 0.00016330
Iteration 364/1000 | Loss: 0.00019548
Iteration 365/1000 | Loss: 0.00011350
Iteration 366/1000 | Loss: 0.00007500
Iteration 367/1000 | Loss: 0.00022700
Iteration 368/1000 | Loss: 0.00007583
Iteration 369/1000 | Loss: 0.00007330
Iteration 370/1000 | Loss: 0.00007190
Iteration 371/1000 | Loss: 0.00007092
Iteration 372/1000 | Loss: 0.00008048
Iteration 373/1000 | Loss: 0.00009405
Iteration 374/1000 | Loss: 0.00007155
Iteration 375/1000 | Loss: 0.00006933
Iteration 376/1000 | Loss: 0.00010190
Iteration 377/1000 | Loss: 0.00008059
Iteration 378/1000 | Loss: 0.00006830
Iteration 379/1000 | Loss: 0.00006805
Iteration 380/1000 | Loss: 0.00006799
Iteration 381/1000 | Loss: 0.00006794
Iteration 382/1000 | Loss: 0.00006790
Iteration 383/1000 | Loss: 0.00006790
Iteration 384/1000 | Loss: 0.00006790
Iteration 385/1000 | Loss: 0.00006790
Iteration 386/1000 | Loss: 0.00006790
Iteration 387/1000 | Loss: 0.00006790
Iteration 388/1000 | Loss: 0.00006789
Iteration 389/1000 | Loss: 0.00006788
Iteration 390/1000 | Loss: 0.00006788
Iteration 391/1000 | Loss: 0.00006788
Iteration 392/1000 | Loss: 0.00006775
Iteration 393/1000 | Loss: 0.00006771
Iteration 394/1000 | Loss: 0.00006768
Iteration 395/1000 | Loss: 0.00006762
Iteration 396/1000 | Loss: 0.00006759
Iteration 397/1000 | Loss: 0.00006759
Iteration 398/1000 | Loss: 0.00006759
Iteration 399/1000 | Loss: 0.00006758
Iteration 400/1000 | Loss: 0.00006758
Iteration 401/1000 | Loss: 0.00006758
Iteration 402/1000 | Loss: 0.00006758
Iteration 403/1000 | Loss: 0.00006758
Iteration 404/1000 | Loss: 0.00006758
Iteration 405/1000 | Loss: 0.00006757
Iteration 406/1000 | Loss: 0.00006757
Iteration 407/1000 | Loss: 0.00006756
Iteration 408/1000 | Loss: 0.00006756
Iteration 409/1000 | Loss: 0.00006756
Iteration 410/1000 | Loss: 0.00006755
Iteration 411/1000 | Loss: 0.00006755
Iteration 412/1000 | Loss: 0.00006755
Iteration 413/1000 | Loss: 0.00006755
Iteration 414/1000 | Loss: 0.00006755
Iteration 415/1000 | Loss: 0.00006755
Iteration 416/1000 | Loss: 0.00006755
Iteration 417/1000 | Loss: 0.00006755
Iteration 418/1000 | Loss: 0.00006755
Iteration 419/1000 | Loss: 0.00006755
Iteration 420/1000 | Loss: 0.00006755
Iteration 421/1000 | Loss: 0.00006754
Iteration 422/1000 | Loss: 0.00006754
Iteration 423/1000 | Loss: 0.00006754
Iteration 424/1000 | Loss: 0.00006754
Iteration 425/1000 | Loss: 0.00006754
Iteration 426/1000 | Loss: 0.00006754
Iteration 427/1000 | Loss: 0.00006754
Iteration 428/1000 | Loss: 0.00006753
Iteration 429/1000 | Loss: 0.00006753
Iteration 430/1000 | Loss: 0.00006753
Iteration 431/1000 | Loss: 0.00006753
Iteration 432/1000 | Loss: 0.00006752
Iteration 433/1000 | Loss: 0.00006752
Iteration 434/1000 | Loss: 0.00006752
Iteration 435/1000 | Loss: 0.00006752
Iteration 436/1000 | Loss: 0.00006751
Iteration 437/1000 | Loss: 0.00006751
Iteration 438/1000 | Loss: 0.00006751
Iteration 439/1000 | Loss: 0.00006751
Iteration 440/1000 | Loss: 0.00006751
Iteration 441/1000 | Loss: 0.00006750
Iteration 442/1000 | Loss: 0.00006750
Iteration 443/1000 | Loss: 0.00006750
Iteration 444/1000 | Loss: 0.00006750
Iteration 445/1000 | Loss: 0.00006750
Iteration 446/1000 | Loss: 0.00006750
Iteration 447/1000 | Loss: 0.00006750
Iteration 448/1000 | Loss: 0.00006750
Iteration 449/1000 | Loss: 0.00006750
Iteration 450/1000 | Loss: 0.00006750
Iteration 451/1000 | Loss: 0.00006750
Iteration 452/1000 | Loss: 0.00006750
Iteration 453/1000 | Loss: 0.00006750
Iteration 454/1000 | Loss: 0.00006750
Iteration 455/1000 | Loss: 0.00006750
Iteration 456/1000 | Loss: 0.00006750
Iteration 457/1000 | Loss: 0.00006750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 457. Stopping optimization.
Last 5 losses: [6.749973545083776e-05, 6.749973545083776e-05, 6.749973545083776e-05, 6.749973545083776e-05, 6.749973545083776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.749973545083776e-05

Optimization complete. Final v2v error: 4.173449516296387 mm

Highest mean error: 11.934118270874023 mm for frame 185

Lowest mean error: 2.37137770652771 mm for frame 161

Saving results

Total time: 525.353223323822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043640
Iteration 2/25 | Loss: 0.00364531
Iteration 3/25 | Loss: 0.00205447
Iteration 4/25 | Loss: 0.00189505
Iteration 5/25 | Loss: 0.00173341
Iteration 6/25 | Loss: 0.00171669
Iteration 7/25 | Loss: 0.00168482
Iteration 8/25 | Loss: 0.00163315
Iteration 9/25 | Loss: 0.00152617
Iteration 10/25 | Loss: 0.00155069
Iteration 11/25 | Loss: 0.00155155
Iteration 12/25 | Loss: 0.00150048
Iteration 13/25 | Loss: 0.00146584
Iteration 14/25 | Loss: 0.00145839
Iteration 15/25 | Loss: 0.00146266
Iteration 16/25 | Loss: 0.00145059
Iteration 17/25 | Loss: 0.00143356
Iteration 18/25 | Loss: 0.00143238
Iteration 19/25 | Loss: 0.00143398
Iteration 20/25 | Loss: 0.00141282
Iteration 21/25 | Loss: 0.00140874
Iteration 22/25 | Loss: 0.00140577
Iteration 23/25 | Loss: 0.00141492
Iteration 24/25 | Loss: 0.00141534
Iteration 25/25 | Loss: 0.00140663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31967866
Iteration 2/25 | Loss: 0.00411182
Iteration 3/25 | Loss: 0.00299465
Iteration 4/25 | Loss: 0.00299465
Iteration 5/25 | Loss: 0.00299465
Iteration 6/25 | Loss: 0.00299465
Iteration 7/25 | Loss: 0.00299465
Iteration 8/25 | Loss: 0.00299465
Iteration 9/25 | Loss: 0.00299465
Iteration 10/25 | Loss: 0.00299465
Iteration 11/25 | Loss: 0.00299465
Iteration 12/25 | Loss: 0.00299465
Iteration 13/25 | Loss: 0.00299465
Iteration 14/25 | Loss: 0.00299465
Iteration 15/25 | Loss: 0.00299465
Iteration 16/25 | Loss: 0.00299465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0029946465510874987, 0.0029946465510874987, 0.0029946465510874987, 0.0029946465510874987, 0.0029946465510874987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029946465510874987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299465
Iteration 2/1000 | Loss: 0.00196259
Iteration 3/1000 | Loss: 0.00206440
Iteration 4/1000 | Loss: 0.00168275
Iteration 5/1000 | Loss: 0.00058409
Iteration 6/1000 | Loss: 0.00525296
Iteration 7/1000 | Loss: 0.00108670
Iteration 8/1000 | Loss: 0.00481441
Iteration 9/1000 | Loss: 0.00170151
Iteration 10/1000 | Loss: 0.00466459
Iteration 11/1000 | Loss: 0.00684774
Iteration 12/1000 | Loss: 0.00367525
Iteration 13/1000 | Loss: 0.00153428
Iteration 14/1000 | Loss: 0.00024536
Iteration 15/1000 | Loss: 0.00039678
Iteration 16/1000 | Loss: 0.00166932
Iteration 17/1000 | Loss: 0.00051490
Iteration 18/1000 | Loss: 0.00093792
Iteration 19/1000 | Loss: 0.00018566
Iteration 20/1000 | Loss: 0.00030534
Iteration 21/1000 | Loss: 0.00013791
Iteration 22/1000 | Loss: 0.00102234
Iteration 23/1000 | Loss: 0.00055559
Iteration 24/1000 | Loss: 0.00608336
Iteration 25/1000 | Loss: 0.01178906
Iteration 26/1000 | Loss: 0.00233659
Iteration 27/1000 | Loss: 0.00232799
Iteration 28/1000 | Loss: 0.00224541
Iteration 29/1000 | Loss: 0.00097056
Iteration 30/1000 | Loss: 0.00036302
Iteration 31/1000 | Loss: 0.00169455
Iteration 32/1000 | Loss: 0.00096439
Iteration 33/1000 | Loss: 0.00070043
Iteration 34/1000 | Loss: 0.00142691
Iteration 35/1000 | Loss: 0.00029868
Iteration 36/1000 | Loss: 0.00110917
Iteration 37/1000 | Loss: 0.00210221
Iteration 38/1000 | Loss: 0.00127973
Iteration 39/1000 | Loss: 0.00149833
Iteration 40/1000 | Loss: 0.00173835
Iteration 41/1000 | Loss: 0.00188211
Iteration 42/1000 | Loss: 0.00328612
Iteration 43/1000 | Loss: 0.00179779
Iteration 44/1000 | Loss: 0.00126286
Iteration 45/1000 | Loss: 0.00148601
Iteration 46/1000 | Loss: 0.00083954
Iteration 47/1000 | Loss: 0.00285427
Iteration 48/1000 | Loss: 0.00134951
Iteration 49/1000 | Loss: 0.00090789
Iteration 50/1000 | Loss: 0.00160628
Iteration 51/1000 | Loss: 0.00235029
Iteration 52/1000 | Loss: 0.00206357
Iteration 53/1000 | Loss: 0.00235443
Iteration 54/1000 | Loss: 0.00193275
Iteration 55/1000 | Loss: 0.00129464
Iteration 56/1000 | Loss: 0.00129151
Iteration 57/1000 | Loss: 0.00078699
Iteration 58/1000 | Loss: 0.00094102
Iteration 59/1000 | Loss: 0.00101370
Iteration 60/1000 | Loss: 0.00084339
Iteration 61/1000 | Loss: 0.00048338
Iteration 62/1000 | Loss: 0.00036653
Iteration 63/1000 | Loss: 0.00100528
Iteration 64/1000 | Loss: 0.00077895
Iteration 65/1000 | Loss: 0.00043939
Iteration 66/1000 | Loss: 0.00062870
Iteration 67/1000 | Loss: 0.00056519
Iteration 68/1000 | Loss: 0.00039486
Iteration 69/1000 | Loss: 0.00061085
Iteration 70/1000 | Loss: 0.00116851
Iteration 71/1000 | Loss: 0.00040900
Iteration 72/1000 | Loss: 0.00041421
Iteration 73/1000 | Loss: 0.00023031
Iteration 74/1000 | Loss: 0.00054882
Iteration 75/1000 | Loss: 0.00012233
Iteration 76/1000 | Loss: 0.00029065
Iteration 77/1000 | Loss: 0.00037628
Iteration 78/1000 | Loss: 0.00044224
Iteration 79/1000 | Loss: 0.00066714
Iteration 80/1000 | Loss: 0.00033524
Iteration 81/1000 | Loss: 0.00073730
Iteration 82/1000 | Loss: 0.00014698
Iteration 83/1000 | Loss: 0.00012907
Iteration 84/1000 | Loss: 0.00081871
Iteration 85/1000 | Loss: 0.00104577
Iteration 86/1000 | Loss: 0.00032811
Iteration 87/1000 | Loss: 0.00030082
Iteration 88/1000 | Loss: 0.00017373
Iteration 89/1000 | Loss: 0.00050081
Iteration 90/1000 | Loss: 0.00003958
Iteration 91/1000 | Loss: 0.00041096
Iteration 92/1000 | Loss: 0.00079486
Iteration 93/1000 | Loss: 0.00005221
Iteration 94/1000 | Loss: 0.00060386
Iteration 95/1000 | Loss: 0.00003434
Iteration 96/1000 | Loss: 0.00008186
Iteration 97/1000 | Loss: 0.00003159
Iteration 98/1000 | Loss: 0.00008239
Iteration 99/1000 | Loss: 0.00003636
Iteration 100/1000 | Loss: 0.00003701
Iteration 101/1000 | Loss: 0.00007976
Iteration 102/1000 | Loss: 0.00003006
Iteration 103/1000 | Loss: 0.00003348
Iteration 104/1000 | Loss: 0.00003833
Iteration 105/1000 | Loss: 0.00066170
Iteration 106/1000 | Loss: 0.00029459
Iteration 107/1000 | Loss: 0.00006283
Iteration 108/1000 | Loss: 0.00018954
Iteration 109/1000 | Loss: 0.00003531
Iteration 110/1000 | Loss: 0.00002371
Iteration 111/1000 | Loss: 0.00002853
Iteration 112/1000 | Loss: 0.00005953
Iteration 113/1000 | Loss: 0.00003173
Iteration 114/1000 | Loss: 0.00002907
Iteration 115/1000 | Loss: 0.00038222
Iteration 116/1000 | Loss: 0.00040914
Iteration 117/1000 | Loss: 0.00033832
Iteration 118/1000 | Loss: 0.00015328
Iteration 119/1000 | Loss: 0.00008405
Iteration 120/1000 | Loss: 0.00023384
Iteration 121/1000 | Loss: 0.00027386
Iteration 122/1000 | Loss: 0.00024003
Iteration 123/1000 | Loss: 0.00015470
Iteration 124/1000 | Loss: 0.00074295
Iteration 125/1000 | Loss: 0.00038932
Iteration 126/1000 | Loss: 0.00044024
Iteration 127/1000 | Loss: 0.00059995
Iteration 128/1000 | Loss: 0.00035406
Iteration 129/1000 | Loss: 0.00025565
Iteration 130/1000 | Loss: 0.00043923
Iteration 131/1000 | Loss: 0.00023428
Iteration 132/1000 | Loss: 0.00014652
Iteration 133/1000 | Loss: 0.00026365
Iteration 134/1000 | Loss: 0.00046141
Iteration 135/1000 | Loss: 0.00010176
Iteration 136/1000 | Loss: 0.00048750
Iteration 137/1000 | Loss: 0.00036785
Iteration 138/1000 | Loss: 0.00027658
Iteration 139/1000 | Loss: 0.00038350
Iteration 140/1000 | Loss: 0.00071808
Iteration 141/1000 | Loss: 0.00023803
Iteration 142/1000 | Loss: 0.00023885
Iteration 143/1000 | Loss: 0.00002617
Iteration 144/1000 | Loss: 0.00028883
Iteration 145/1000 | Loss: 0.00018070
Iteration 146/1000 | Loss: 0.00033870
Iteration 147/1000 | Loss: 0.00015079
Iteration 148/1000 | Loss: 0.00028303
Iteration 149/1000 | Loss: 0.00034740
Iteration 150/1000 | Loss: 0.00029804
Iteration 151/1000 | Loss: 0.00111704
Iteration 152/1000 | Loss: 0.00003057
Iteration 153/1000 | Loss: 0.00003851
Iteration 154/1000 | Loss: 0.00003189
Iteration 155/1000 | Loss: 0.00026859
Iteration 156/1000 | Loss: 0.00053498
Iteration 157/1000 | Loss: 0.00036360
Iteration 158/1000 | Loss: 0.00011706
Iteration 159/1000 | Loss: 0.00029394
Iteration 160/1000 | Loss: 0.00037846
Iteration 161/1000 | Loss: 0.00037058
Iteration 162/1000 | Loss: 0.00027501
Iteration 163/1000 | Loss: 0.00004946
Iteration 164/1000 | Loss: 0.00004490
Iteration 165/1000 | Loss: 0.00002537
Iteration 166/1000 | Loss: 0.00003474
Iteration 167/1000 | Loss: 0.00002979
Iteration 168/1000 | Loss: 0.00032430
Iteration 169/1000 | Loss: 0.00003949
Iteration 170/1000 | Loss: 0.00006478
Iteration 171/1000 | Loss: 0.00003083
Iteration 172/1000 | Loss: 0.00001950
Iteration 173/1000 | Loss: 0.00001878
Iteration 174/1000 | Loss: 0.00002276
Iteration 175/1000 | Loss: 0.00001673
Iteration 176/1000 | Loss: 0.00042652
Iteration 177/1000 | Loss: 0.00004208
Iteration 178/1000 | Loss: 0.00016990
Iteration 179/1000 | Loss: 0.00001814
Iteration 180/1000 | Loss: 0.00001627
Iteration 181/1000 | Loss: 0.00001676
Iteration 182/1000 | Loss: 0.00001545
Iteration 183/1000 | Loss: 0.00001921
Iteration 184/1000 | Loss: 0.00007456
Iteration 185/1000 | Loss: 0.00001526
Iteration 186/1000 | Loss: 0.00001714
Iteration 187/1000 | Loss: 0.00001714
Iteration 188/1000 | Loss: 0.00034649
Iteration 189/1000 | Loss: 0.00003700
Iteration 190/1000 | Loss: 0.00006591
Iteration 191/1000 | Loss: 0.00001577
Iteration 192/1000 | Loss: 0.00023794
Iteration 193/1000 | Loss: 0.00001624
Iteration 194/1000 | Loss: 0.00001488
Iteration 195/1000 | Loss: 0.00001481
Iteration 196/1000 | Loss: 0.00001481
Iteration 197/1000 | Loss: 0.00001481
Iteration 198/1000 | Loss: 0.00001481
Iteration 199/1000 | Loss: 0.00001480
Iteration 200/1000 | Loss: 0.00001476
Iteration 201/1000 | Loss: 0.00001476
Iteration 202/1000 | Loss: 0.00001467
Iteration 203/1000 | Loss: 0.00001897
Iteration 204/1000 | Loss: 0.00001452
Iteration 205/1000 | Loss: 0.00001452
Iteration 206/1000 | Loss: 0.00001452
Iteration 207/1000 | Loss: 0.00001451
Iteration 208/1000 | Loss: 0.00001451
Iteration 209/1000 | Loss: 0.00001451
Iteration 210/1000 | Loss: 0.00001451
Iteration 211/1000 | Loss: 0.00001451
Iteration 212/1000 | Loss: 0.00001450
Iteration 213/1000 | Loss: 0.00001450
Iteration 214/1000 | Loss: 0.00001450
Iteration 215/1000 | Loss: 0.00001450
Iteration 216/1000 | Loss: 0.00001629
Iteration 217/1000 | Loss: 0.00001442
Iteration 218/1000 | Loss: 0.00001441
Iteration 219/1000 | Loss: 0.00001441
Iteration 220/1000 | Loss: 0.00001441
Iteration 221/1000 | Loss: 0.00001441
Iteration 222/1000 | Loss: 0.00001441
Iteration 223/1000 | Loss: 0.00001441
Iteration 224/1000 | Loss: 0.00001441
Iteration 225/1000 | Loss: 0.00001441
Iteration 226/1000 | Loss: 0.00001441
Iteration 227/1000 | Loss: 0.00001441
Iteration 228/1000 | Loss: 0.00001441
Iteration 229/1000 | Loss: 0.00001440
Iteration 230/1000 | Loss: 0.00001440
Iteration 231/1000 | Loss: 0.00001440
Iteration 232/1000 | Loss: 0.00001440
Iteration 233/1000 | Loss: 0.00001440
Iteration 234/1000 | Loss: 0.00001440
Iteration 235/1000 | Loss: 0.00001440
Iteration 236/1000 | Loss: 0.00001440
Iteration 237/1000 | Loss: 0.00001440
Iteration 238/1000 | Loss: 0.00001440
Iteration 239/1000 | Loss: 0.00001440
Iteration 240/1000 | Loss: 0.00001440
Iteration 241/1000 | Loss: 0.00001439
Iteration 242/1000 | Loss: 0.00001439
Iteration 243/1000 | Loss: 0.00001439
Iteration 244/1000 | Loss: 0.00001439
Iteration 245/1000 | Loss: 0.00001439
Iteration 246/1000 | Loss: 0.00001439
Iteration 247/1000 | Loss: 0.00001439
Iteration 248/1000 | Loss: 0.00001438
Iteration 249/1000 | Loss: 0.00001438
Iteration 250/1000 | Loss: 0.00001438
Iteration 251/1000 | Loss: 0.00001438
Iteration 252/1000 | Loss: 0.00001437
Iteration 253/1000 | Loss: 0.00001437
Iteration 254/1000 | Loss: 0.00001436
Iteration 255/1000 | Loss: 0.00001436
Iteration 256/1000 | Loss: 0.00001436
Iteration 257/1000 | Loss: 0.00001436
Iteration 258/1000 | Loss: 0.00001436
Iteration 259/1000 | Loss: 0.00001435
Iteration 260/1000 | Loss: 0.00001435
Iteration 261/1000 | Loss: 0.00001434
Iteration 262/1000 | Loss: 0.00001432
Iteration 263/1000 | Loss: 0.00001432
Iteration 264/1000 | Loss: 0.00001431
Iteration 265/1000 | Loss: 0.00001431
Iteration 266/1000 | Loss: 0.00001431
Iteration 267/1000 | Loss: 0.00001431
Iteration 268/1000 | Loss: 0.00001431
Iteration 269/1000 | Loss: 0.00001430
Iteration 270/1000 | Loss: 0.00001430
Iteration 271/1000 | Loss: 0.00001430
Iteration 272/1000 | Loss: 0.00001430
Iteration 273/1000 | Loss: 0.00001430
Iteration 274/1000 | Loss: 0.00001430
Iteration 275/1000 | Loss: 0.00001430
Iteration 276/1000 | Loss: 0.00001430
Iteration 277/1000 | Loss: 0.00001430
Iteration 278/1000 | Loss: 0.00001430
Iteration 279/1000 | Loss: 0.00001430
Iteration 280/1000 | Loss: 0.00001430
Iteration 281/1000 | Loss: 0.00001430
Iteration 282/1000 | Loss: 0.00001430
Iteration 283/1000 | Loss: 0.00001429
Iteration 284/1000 | Loss: 0.00001429
Iteration 285/1000 | Loss: 0.00001429
Iteration 286/1000 | Loss: 0.00001429
Iteration 287/1000 | Loss: 0.00001429
Iteration 288/1000 | Loss: 0.00001429
Iteration 289/1000 | Loss: 0.00001428
Iteration 290/1000 | Loss: 0.00001428
Iteration 291/1000 | Loss: 0.00001428
Iteration 292/1000 | Loss: 0.00001428
Iteration 293/1000 | Loss: 0.00001427
Iteration 294/1000 | Loss: 0.00001427
Iteration 295/1000 | Loss: 0.00001427
Iteration 296/1000 | Loss: 0.00001427
Iteration 297/1000 | Loss: 0.00001427
Iteration 298/1000 | Loss: 0.00001427
Iteration 299/1000 | Loss: 0.00001427
Iteration 300/1000 | Loss: 0.00001427
Iteration 301/1000 | Loss: 0.00001427
Iteration 302/1000 | Loss: 0.00001427
Iteration 303/1000 | Loss: 0.00001427
Iteration 304/1000 | Loss: 0.00001426
Iteration 305/1000 | Loss: 0.00001426
Iteration 306/1000 | Loss: 0.00001426
Iteration 307/1000 | Loss: 0.00001426
Iteration 308/1000 | Loss: 0.00001426
Iteration 309/1000 | Loss: 0.00001426
Iteration 310/1000 | Loss: 0.00001426
Iteration 311/1000 | Loss: 0.00001426
Iteration 312/1000 | Loss: 0.00001426
Iteration 313/1000 | Loss: 0.00001426
Iteration 314/1000 | Loss: 0.00001426
Iteration 315/1000 | Loss: 0.00001425
Iteration 316/1000 | Loss: 0.00001425
Iteration 317/1000 | Loss: 0.00001425
Iteration 318/1000 | Loss: 0.00001425
Iteration 319/1000 | Loss: 0.00001425
Iteration 320/1000 | Loss: 0.00001425
Iteration 321/1000 | Loss: 0.00001425
Iteration 322/1000 | Loss: 0.00001425
Iteration 323/1000 | Loss: 0.00001425
Iteration 324/1000 | Loss: 0.00001425
Iteration 325/1000 | Loss: 0.00001425
Iteration 326/1000 | Loss: 0.00001425
Iteration 327/1000 | Loss: 0.00001425
Iteration 328/1000 | Loss: 0.00001424
Iteration 329/1000 | Loss: 0.00001424
Iteration 330/1000 | Loss: 0.00001424
Iteration 331/1000 | Loss: 0.00001424
Iteration 332/1000 | Loss: 0.00001424
Iteration 333/1000 | Loss: 0.00001424
Iteration 334/1000 | Loss: 0.00001424
Iteration 335/1000 | Loss: 0.00001424
Iteration 336/1000 | Loss: 0.00001424
Iteration 337/1000 | Loss: 0.00001424
Iteration 338/1000 | Loss: 0.00001424
Iteration 339/1000 | Loss: 0.00001424
Iteration 340/1000 | Loss: 0.00001424
Iteration 341/1000 | Loss: 0.00001424
Iteration 342/1000 | Loss: 0.00001424
Iteration 343/1000 | Loss: 0.00001424
Iteration 344/1000 | Loss: 0.00001424
Iteration 345/1000 | Loss: 0.00001424
Iteration 346/1000 | Loss: 0.00001424
Iteration 347/1000 | Loss: 0.00001424
Iteration 348/1000 | Loss: 0.00001424
Iteration 349/1000 | Loss: 0.00001424
Iteration 350/1000 | Loss: 0.00001424
Iteration 351/1000 | Loss: 0.00001424
Iteration 352/1000 | Loss: 0.00001424
Iteration 353/1000 | Loss: 0.00001424
Iteration 354/1000 | Loss: 0.00001424
Iteration 355/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [1.424454057996627e-05, 1.424454057996627e-05, 1.424454057996627e-05, 1.424454057996627e-05, 1.424454057996627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.424454057996627e-05

Optimization complete. Final v2v error: 3.07436203956604 mm

Highest mean error: 5.522622108459473 mm for frame 62

Lowest mean error: 2.523829698562622 mm for frame 120

Saving results

Total time: 377.29901909828186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110336
Iteration 2/25 | Loss: 0.00215665
Iteration 3/25 | Loss: 0.00147520
Iteration 4/25 | Loss: 0.00139355
Iteration 5/25 | Loss: 0.00139255
Iteration 6/25 | Loss: 0.00138845
Iteration 7/25 | Loss: 0.00138588
Iteration 8/25 | Loss: 0.00132526
Iteration 9/25 | Loss: 0.00131415
Iteration 10/25 | Loss: 0.00130217
Iteration 11/25 | Loss: 0.00128458
Iteration 12/25 | Loss: 0.00126967
Iteration 13/25 | Loss: 0.00125913
Iteration 14/25 | Loss: 0.00125835
Iteration 15/25 | Loss: 0.00125413
Iteration 16/25 | Loss: 0.00125095
Iteration 17/25 | Loss: 0.00124331
Iteration 18/25 | Loss: 0.00124034
Iteration 19/25 | Loss: 0.00123996
Iteration 20/25 | Loss: 0.00123887
Iteration 21/25 | Loss: 0.00123813
Iteration 22/25 | Loss: 0.00123695
Iteration 23/25 | Loss: 0.00123388
Iteration 24/25 | Loss: 0.00123240
Iteration 25/25 | Loss: 0.00123347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95432299
Iteration 2/25 | Loss: 0.00139206
Iteration 3/25 | Loss: 0.00139206
Iteration 4/25 | Loss: 0.00119049
Iteration 5/25 | Loss: 0.00119049
Iteration 6/25 | Loss: 0.00119049
Iteration 7/25 | Loss: 0.00119049
Iteration 8/25 | Loss: 0.00119049
Iteration 9/25 | Loss: 0.00119049
Iteration 10/25 | Loss: 0.00119049
Iteration 11/25 | Loss: 0.00119049
Iteration 12/25 | Loss: 0.00119049
Iteration 13/25 | Loss: 0.00119049
Iteration 14/25 | Loss: 0.00119049
Iteration 15/25 | Loss: 0.00119049
Iteration 16/25 | Loss: 0.00119049
Iteration 17/25 | Loss: 0.00119049
Iteration 18/25 | Loss: 0.00119049
Iteration 19/25 | Loss: 0.00119049
Iteration 20/25 | Loss: 0.00119049
Iteration 21/25 | Loss: 0.00119049
Iteration 22/25 | Loss: 0.00119049
Iteration 23/25 | Loss: 0.00119049
Iteration 24/25 | Loss: 0.00119049
Iteration 25/25 | Loss: 0.00119049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119049
Iteration 2/1000 | Loss: 0.00090724
Iteration 3/1000 | Loss: 0.00036838
Iteration 4/1000 | Loss: 0.00025769
Iteration 5/1000 | Loss: 0.00023017
Iteration 6/1000 | Loss: 0.00020783
Iteration 7/1000 | Loss: 0.00006071
Iteration 8/1000 | Loss: 0.00014245
Iteration 9/1000 | Loss: 0.00010168
Iteration 10/1000 | Loss: 0.00008895
Iteration 11/1000 | Loss: 0.00013244
Iteration 12/1000 | Loss: 0.00009723
Iteration 13/1000 | Loss: 0.00009339
Iteration 14/1000 | Loss: 0.00009052
Iteration 15/1000 | Loss: 0.00004593
Iteration 16/1000 | Loss: 0.00003511
Iteration 17/1000 | Loss: 0.00005006
Iteration 18/1000 | Loss: 0.00004708
Iteration 19/1000 | Loss: 0.00004763
Iteration 20/1000 | Loss: 0.00015779
Iteration 21/1000 | Loss: 0.00025595
Iteration 22/1000 | Loss: 0.00024035
Iteration 23/1000 | Loss: 0.00023967
Iteration 24/1000 | Loss: 0.00019357
Iteration 25/1000 | Loss: 0.00015408
Iteration 26/1000 | Loss: 0.00028474
Iteration 27/1000 | Loss: 0.00021179
Iteration 28/1000 | Loss: 0.00019934
Iteration 29/1000 | Loss: 0.00009392
Iteration 30/1000 | Loss: 0.00013063
Iteration 31/1000 | Loss: 0.00004593
Iteration 32/1000 | Loss: 0.00012287
Iteration 33/1000 | Loss: 0.00011983
Iteration 34/1000 | Loss: 0.00010275
Iteration 35/1000 | Loss: 0.00010129
Iteration 36/1000 | Loss: 0.00012516
Iteration 37/1000 | Loss: 0.00013358
Iteration 38/1000 | Loss: 0.00008917
Iteration 39/1000 | Loss: 0.00009314
Iteration 40/1000 | Loss: 0.00013322
Iteration 41/1000 | Loss: 0.00014587
Iteration 42/1000 | Loss: 0.00012983
Iteration 43/1000 | Loss: 0.00013877
Iteration 44/1000 | Loss: 0.00012894
Iteration 45/1000 | Loss: 0.00005109
Iteration 46/1000 | Loss: 0.00011472
Iteration 47/1000 | Loss: 0.00010560
Iteration 48/1000 | Loss: 0.00004527
Iteration 49/1000 | Loss: 0.00014041
Iteration 50/1000 | Loss: 0.00013628
Iteration 51/1000 | Loss: 0.00013094
Iteration 52/1000 | Loss: 0.00010753
Iteration 53/1000 | Loss: 0.00012372
Iteration 54/1000 | Loss: 0.00009329
Iteration 55/1000 | Loss: 0.00012046
Iteration 56/1000 | Loss: 0.00013845
Iteration 57/1000 | Loss: 0.00014543
Iteration 58/1000 | Loss: 0.00003535
Iteration 59/1000 | Loss: 0.00013816
Iteration 60/1000 | Loss: 0.00007009
Iteration 61/1000 | Loss: 0.00002737
Iteration 62/1000 | Loss: 0.00002614
Iteration 63/1000 | Loss: 0.00002527
Iteration 64/1000 | Loss: 0.00002469
Iteration 65/1000 | Loss: 0.00013256
Iteration 66/1000 | Loss: 0.00002882
Iteration 67/1000 | Loss: 0.00018007
Iteration 68/1000 | Loss: 0.00013248
Iteration 69/1000 | Loss: 0.00002870
Iteration 70/1000 | Loss: 0.00010119
Iteration 71/1000 | Loss: 0.00012680
Iteration 72/1000 | Loss: 0.00015218
Iteration 73/1000 | Loss: 0.00016918
Iteration 74/1000 | Loss: 0.00011556
Iteration 75/1000 | Loss: 0.00003363
Iteration 76/1000 | Loss: 0.00002762
Iteration 77/1000 | Loss: 0.00002543
Iteration 78/1000 | Loss: 0.00002384
Iteration 79/1000 | Loss: 0.00002330
Iteration 80/1000 | Loss: 0.00002296
Iteration 81/1000 | Loss: 0.00002270
Iteration 82/1000 | Loss: 0.00002246
Iteration 83/1000 | Loss: 0.00002230
Iteration 84/1000 | Loss: 0.00002224
Iteration 85/1000 | Loss: 0.00002221
Iteration 86/1000 | Loss: 0.00002221
Iteration 87/1000 | Loss: 0.00002220
Iteration 88/1000 | Loss: 0.00002220
Iteration 89/1000 | Loss: 0.00002219
Iteration 90/1000 | Loss: 0.00002218
Iteration 91/1000 | Loss: 0.00002218
Iteration 92/1000 | Loss: 0.00002212
Iteration 93/1000 | Loss: 0.00002196
Iteration 94/1000 | Loss: 0.00002196
Iteration 95/1000 | Loss: 0.00002192
Iteration 96/1000 | Loss: 0.00002190
Iteration 97/1000 | Loss: 0.00002190
Iteration 98/1000 | Loss: 0.00002188
Iteration 99/1000 | Loss: 0.00002188
Iteration 100/1000 | Loss: 0.00002187
Iteration 101/1000 | Loss: 0.00002187
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002187
Iteration 105/1000 | Loss: 0.00002187
Iteration 106/1000 | Loss: 0.00002187
Iteration 107/1000 | Loss: 0.00002187
Iteration 108/1000 | Loss: 0.00002186
Iteration 109/1000 | Loss: 0.00002179
Iteration 110/1000 | Loss: 0.00002175
Iteration 111/1000 | Loss: 0.00002175
Iteration 112/1000 | Loss: 0.00002175
Iteration 113/1000 | Loss: 0.00002175
Iteration 114/1000 | Loss: 0.00002175
Iteration 115/1000 | Loss: 0.00002174
Iteration 116/1000 | Loss: 0.00002174
Iteration 117/1000 | Loss: 0.00002174
Iteration 118/1000 | Loss: 0.00002174
Iteration 119/1000 | Loss: 0.00002174
Iteration 120/1000 | Loss: 0.00002174
Iteration 121/1000 | Loss: 0.00002174
Iteration 122/1000 | Loss: 0.00002174
Iteration 123/1000 | Loss: 0.00002173
Iteration 124/1000 | Loss: 0.00002173
Iteration 125/1000 | Loss: 0.00002171
Iteration 126/1000 | Loss: 0.00002171
Iteration 127/1000 | Loss: 0.00002164
Iteration 128/1000 | Loss: 0.00002162
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002161
Iteration 131/1000 | Loss: 0.00002161
Iteration 132/1000 | Loss: 0.00002161
Iteration 133/1000 | Loss: 0.00002161
Iteration 134/1000 | Loss: 0.00002160
Iteration 135/1000 | Loss: 0.00002160
Iteration 136/1000 | Loss: 0.00002160
Iteration 137/1000 | Loss: 0.00002160
Iteration 138/1000 | Loss: 0.00002159
Iteration 139/1000 | Loss: 0.00002159
Iteration 140/1000 | Loss: 0.00002159
Iteration 141/1000 | Loss: 0.00002159
Iteration 142/1000 | Loss: 0.00002159
Iteration 143/1000 | Loss: 0.00002159
Iteration 144/1000 | Loss: 0.00002159
Iteration 145/1000 | Loss: 0.00002159
Iteration 146/1000 | Loss: 0.00002159
Iteration 147/1000 | Loss: 0.00002159
Iteration 148/1000 | Loss: 0.00002159
Iteration 149/1000 | Loss: 0.00002158
Iteration 150/1000 | Loss: 0.00002158
Iteration 151/1000 | Loss: 0.00002158
Iteration 152/1000 | Loss: 0.00002158
Iteration 153/1000 | Loss: 0.00002158
Iteration 154/1000 | Loss: 0.00002158
Iteration 155/1000 | Loss: 0.00002158
Iteration 156/1000 | Loss: 0.00002158
Iteration 157/1000 | Loss: 0.00002157
Iteration 158/1000 | Loss: 0.00002157
Iteration 159/1000 | Loss: 0.00002157
Iteration 160/1000 | Loss: 0.00002157
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002157
Iteration 163/1000 | Loss: 0.00002157
Iteration 164/1000 | Loss: 0.00002157
Iteration 165/1000 | Loss: 0.00002157
Iteration 166/1000 | Loss: 0.00002156
Iteration 167/1000 | Loss: 0.00002156
Iteration 168/1000 | Loss: 0.00002156
Iteration 169/1000 | Loss: 0.00002156
Iteration 170/1000 | Loss: 0.00002156
Iteration 171/1000 | Loss: 0.00002156
Iteration 172/1000 | Loss: 0.00002156
Iteration 173/1000 | Loss: 0.00002156
Iteration 174/1000 | Loss: 0.00002156
Iteration 175/1000 | Loss: 0.00002156
Iteration 176/1000 | Loss: 0.00002156
Iteration 177/1000 | Loss: 0.00002156
Iteration 178/1000 | Loss: 0.00002156
Iteration 179/1000 | Loss: 0.00002155
Iteration 180/1000 | Loss: 0.00002155
Iteration 181/1000 | Loss: 0.00002155
Iteration 182/1000 | Loss: 0.00002155
Iteration 183/1000 | Loss: 0.00002155
Iteration 184/1000 | Loss: 0.00002155
Iteration 185/1000 | Loss: 0.00002155
Iteration 186/1000 | Loss: 0.00002154
Iteration 187/1000 | Loss: 0.00002154
Iteration 188/1000 | Loss: 0.00002154
Iteration 189/1000 | Loss: 0.00002154
Iteration 190/1000 | Loss: 0.00002154
Iteration 191/1000 | Loss: 0.00002154
Iteration 192/1000 | Loss: 0.00002154
Iteration 193/1000 | Loss: 0.00002154
Iteration 194/1000 | Loss: 0.00002154
Iteration 195/1000 | Loss: 0.00002154
Iteration 196/1000 | Loss: 0.00002154
Iteration 197/1000 | Loss: 0.00002154
Iteration 198/1000 | Loss: 0.00002154
Iteration 199/1000 | Loss: 0.00002154
Iteration 200/1000 | Loss: 0.00002154
Iteration 201/1000 | Loss: 0.00002154
Iteration 202/1000 | Loss: 0.00002154
Iteration 203/1000 | Loss: 0.00002154
Iteration 204/1000 | Loss: 0.00002154
Iteration 205/1000 | Loss: 0.00002154
Iteration 206/1000 | Loss: 0.00002154
Iteration 207/1000 | Loss: 0.00002154
Iteration 208/1000 | Loss: 0.00002154
Iteration 209/1000 | Loss: 0.00002154
Iteration 210/1000 | Loss: 0.00002154
Iteration 211/1000 | Loss: 0.00002154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.153823515982367e-05, 2.153823515982367e-05, 2.153823515982367e-05, 2.153823515982367e-05, 2.153823515982367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.153823515982367e-05

Optimization complete. Final v2v error: 3.769010305404663 mm

Highest mean error: 4.762341499328613 mm for frame 141

Lowest mean error: 3.369182586669922 mm for frame 238

Saving results

Total time: 202.9795594215393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671357
Iteration 2/25 | Loss: 0.00139507
Iteration 3/25 | Loss: 0.00122704
Iteration 4/25 | Loss: 0.00119891
Iteration 5/25 | Loss: 0.00119330
Iteration 6/25 | Loss: 0.00119245
Iteration 7/25 | Loss: 0.00119245
Iteration 8/25 | Loss: 0.00119198
Iteration 9/25 | Loss: 0.00119198
Iteration 10/25 | Loss: 0.00119198
Iteration 11/25 | Loss: 0.00119198
Iteration 12/25 | Loss: 0.00119198
Iteration 13/25 | Loss: 0.00119198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011919834651052952, 0.0011919834651052952, 0.0011919834651052952, 0.0011919834651052952, 0.0011919834651052952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011919834651052952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85472488
Iteration 2/25 | Loss: 0.00074177
Iteration 3/25 | Loss: 0.00074175
Iteration 4/25 | Loss: 0.00074175
Iteration 5/25 | Loss: 0.00074175
Iteration 6/25 | Loss: 0.00074175
Iteration 7/25 | Loss: 0.00074174
Iteration 8/25 | Loss: 0.00074174
Iteration 9/25 | Loss: 0.00074174
Iteration 10/25 | Loss: 0.00074174
Iteration 11/25 | Loss: 0.00074174
Iteration 12/25 | Loss: 0.00074174
Iteration 13/25 | Loss: 0.00074174
Iteration 14/25 | Loss: 0.00074174
Iteration 15/25 | Loss: 0.00074174
Iteration 16/25 | Loss: 0.00074174
Iteration 17/25 | Loss: 0.00074174
Iteration 18/25 | Loss: 0.00074174
Iteration 19/25 | Loss: 0.00074174
Iteration 20/25 | Loss: 0.00074174
Iteration 21/25 | Loss: 0.00074174
Iteration 22/25 | Loss: 0.00074174
Iteration 23/25 | Loss: 0.00074174
Iteration 24/25 | Loss: 0.00074174
Iteration 25/25 | Loss: 0.00074174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074174
Iteration 2/1000 | Loss: 0.00003552
Iteration 3/1000 | Loss: 0.00002374
Iteration 4/1000 | Loss: 0.00002177
Iteration 5/1000 | Loss: 0.00002087
Iteration 6/1000 | Loss: 0.00002032
Iteration 7/1000 | Loss: 0.00001994
Iteration 8/1000 | Loss: 0.00001981
Iteration 9/1000 | Loss: 0.00001960
Iteration 10/1000 | Loss: 0.00001941
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001932
Iteration 13/1000 | Loss: 0.00001925
Iteration 14/1000 | Loss: 0.00001923
Iteration 15/1000 | Loss: 0.00001923
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001916
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001907
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001906
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00001904
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001898
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001897
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001895
Iteration 42/1000 | Loss: 0.00001895
Iteration 43/1000 | Loss: 0.00001895
Iteration 44/1000 | Loss: 0.00001895
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001895
Iteration 47/1000 | Loss: 0.00001895
Iteration 48/1000 | Loss: 0.00001894
Iteration 49/1000 | Loss: 0.00001894
Iteration 50/1000 | Loss: 0.00001894
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001893
Iteration 58/1000 | Loss: 0.00001893
Iteration 59/1000 | Loss: 0.00001893
Iteration 60/1000 | Loss: 0.00001893
Iteration 61/1000 | Loss: 0.00001893
Iteration 62/1000 | Loss: 0.00001893
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001892
Iteration 66/1000 | Loss: 0.00001892
Iteration 67/1000 | Loss: 0.00001891
Iteration 68/1000 | Loss: 0.00001891
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001891
Iteration 71/1000 | Loss: 0.00001891
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001890
Iteration 74/1000 | Loss: 0.00001890
Iteration 75/1000 | Loss: 0.00001890
Iteration 76/1000 | Loss: 0.00001890
Iteration 77/1000 | Loss: 0.00001890
Iteration 78/1000 | Loss: 0.00001890
Iteration 79/1000 | Loss: 0.00001890
Iteration 80/1000 | Loss: 0.00001889
Iteration 81/1000 | Loss: 0.00001889
Iteration 82/1000 | Loss: 0.00001889
Iteration 83/1000 | Loss: 0.00001889
Iteration 84/1000 | Loss: 0.00001889
Iteration 85/1000 | Loss: 0.00001889
Iteration 86/1000 | Loss: 0.00001888
Iteration 87/1000 | Loss: 0.00001888
Iteration 88/1000 | Loss: 0.00001888
Iteration 89/1000 | Loss: 0.00001888
Iteration 90/1000 | Loss: 0.00001888
Iteration 91/1000 | Loss: 0.00001887
Iteration 92/1000 | Loss: 0.00001887
Iteration 93/1000 | Loss: 0.00001887
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001887
Iteration 97/1000 | Loss: 0.00001887
Iteration 98/1000 | Loss: 0.00001887
Iteration 99/1000 | Loss: 0.00001886
Iteration 100/1000 | Loss: 0.00001886
Iteration 101/1000 | Loss: 0.00001886
Iteration 102/1000 | Loss: 0.00001886
Iteration 103/1000 | Loss: 0.00001885
Iteration 104/1000 | Loss: 0.00001885
Iteration 105/1000 | Loss: 0.00001884
Iteration 106/1000 | Loss: 0.00001884
Iteration 107/1000 | Loss: 0.00001884
Iteration 108/1000 | Loss: 0.00001884
Iteration 109/1000 | Loss: 0.00001884
Iteration 110/1000 | Loss: 0.00001884
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001883
Iteration 116/1000 | Loss: 0.00001883
Iteration 117/1000 | Loss: 0.00001883
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001883
Iteration 120/1000 | Loss: 0.00001883
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001883
Iteration 123/1000 | Loss: 0.00001883
Iteration 124/1000 | Loss: 0.00001883
Iteration 125/1000 | Loss: 0.00001883
Iteration 126/1000 | Loss: 0.00001882
Iteration 127/1000 | Loss: 0.00001882
Iteration 128/1000 | Loss: 0.00001882
Iteration 129/1000 | Loss: 0.00001882
Iteration 130/1000 | Loss: 0.00001881
Iteration 131/1000 | Loss: 0.00001881
Iteration 132/1000 | Loss: 0.00001881
Iteration 133/1000 | Loss: 0.00001881
Iteration 134/1000 | Loss: 0.00001881
Iteration 135/1000 | Loss: 0.00001881
Iteration 136/1000 | Loss: 0.00001881
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001881
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001881
Iteration 144/1000 | Loss: 0.00001881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.8813319911714643e-05, 1.8813319911714643e-05, 1.8813319911714643e-05, 1.8813319911714643e-05, 1.8813319911714643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8813319911714643e-05

Optimization complete. Final v2v error: 3.5853114128112793 mm

Highest mean error: 4.702637672424316 mm for frame 98

Lowest mean error: 2.934476613998413 mm for frame 6

Saving results

Total time: 40.81811594963074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415597
Iteration 2/25 | Loss: 0.00117821
Iteration 3/25 | Loss: 0.00111216
Iteration 4/25 | Loss: 0.00110625
Iteration 5/25 | Loss: 0.00110373
Iteration 6/25 | Loss: 0.00110373
Iteration 7/25 | Loss: 0.00110373
Iteration 8/25 | Loss: 0.00110373
Iteration 9/25 | Loss: 0.00110373
Iteration 10/25 | Loss: 0.00110373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011037335498258471, 0.0011037335498258471, 0.0011037335498258471, 0.0011037335498258471, 0.0011037335498258471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011037335498258471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44298732
Iteration 2/25 | Loss: 0.00073045
Iteration 3/25 | Loss: 0.00073045
Iteration 4/25 | Loss: 0.00073045
Iteration 5/25 | Loss: 0.00073045
Iteration 6/25 | Loss: 0.00073045
Iteration 7/25 | Loss: 0.00073045
Iteration 8/25 | Loss: 0.00073045
Iteration 9/25 | Loss: 0.00073045
Iteration 10/25 | Loss: 0.00073045
Iteration 11/25 | Loss: 0.00073045
Iteration 12/25 | Loss: 0.00073045
Iteration 13/25 | Loss: 0.00073045
Iteration 14/25 | Loss: 0.00073045
Iteration 15/25 | Loss: 0.00073045
Iteration 16/25 | Loss: 0.00073045
Iteration 17/25 | Loss: 0.00073045
Iteration 18/25 | Loss: 0.00073045
Iteration 19/25 | Loss: 0.00073045
Iteration 20/25 | Loss: 0.00073045
Iteration 21/25 | Loss: 0.00073045
Iteration 22/25 | Loss: 0.00073045
Iteration 23/25 | Loss: 0.00073045
Iteration 24/25 | Loss: 0.00073045
Iteration 25/25 | Loss: 0.00073045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073045
Iteration 2/1000 | Loss: 0.00002065
Iteration 3/1000 | Loss: 0.00001314
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001105
Iteration 6/1000 | Loss: 0.00001069
Iteration 7/1000 | Loss: 0.00001048
Iteration 8/1000 | Loss: 0.00001041
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00000983
Iteration 11/1000 | Loss: 0.00000975
Iteration 12/1000 | Loss: 0.00000967
Iteration 13/1000 | Loss: 0.00000966
Iteration 14/1000 | Loss: 0.00000962
Iteration 15/1000 | Loss: 0.00000952
Iteration 16/1000 | Loss: 0.00000949
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000943
Iteration 19/1000 | Loss: 0.00000942
Iteration 20/1000 | Loss: 0.00000941
Iteration 21/1000 | Loss: 0.00000939
Iteration 22/1000 | Loss: 0.00000933
Iteration 23/1000 | Loss: 0.00000933
Iteration 24/1000 | Loss: 0.00000932
Iteration 25/1000 | Loss: 0.00000932
Iteration 26/1000 | Loss: 0.00000931
Iteration 27/1000 | Loss: 0.00000931
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000930
Iteration 30/1000 | Loss: 0.00000930
Iteration 31/1000 | Loss: 0.00000928
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000928
Iteration 34/1000 | Loss: 0.00000928
Iteration 35/1000 | Loss: 0.00000928
Iteration 36/1000 | Loss: 0.00000927
Iteration 37/1000 | Loss: 0.00000927
Iteration 38/1000 | Loss: 0.00000926
Iteration 39/1000 | Loss: 0.00000926
Iteration 40/1000 | Loss: 0.00000925
Iteration 41/1000 | Loss: 0.00000925
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000924
Iteration 44/1000 | Loss: 0.00000923
Iteration 45/1000 | Loss: 0.00000923
Iteration 46/1000 | Loss: 0.00000923
Iteration 47/1000 | Loss: 0.00000922
Iteration 48/1000 | Loss: 0.00000922
Iteration 49/1000 | Loss: 0.00000921
Iteration 50/1000 | Loss: 0.00000921
Iteration 51/1000 | Loss: 0.00000921
Iteration 52/1000 | Loss: 0.00000921
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000920
Iteration 55/1000 | Loss: 0.00000920
Iteration 56/1000 | Loss: 0.00000920
Iteration 57/1000 | Loss: 0.00000920
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000919
Iteration 61/1000 | Loss: 0.00000919
Iteration 62/1000 | Loss: 0.00000918
Iteration 63/1000 | Loss: 0.00000918
Iteration 64/1000 | Loss: 0.00000918
Iteration 65/1000 | Loss: 0.00000918
Iteration 66/1000 | Loss: 0.00000918
Iteration 67/1000 | Loss: 0.00000918
Iteration 68/1000 | Loss: 0.00000918
Iteration 69/1000 | Loss: 0.00000918
Iteration 70/1000 | Loss: 0.00000918
Iteration 71/1000 | Loss: 0.00000918
Iteration 72/1000 | Loss: 0.00000918
Iteration 73/1000 | Loss: 0.00000918
Iteration 74/1000 | Loss: 0.00000918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [9.181679160974454e-06, 9.181679160974454e-06, 9.181679160974454e-06, 9.181679160974454e-06, 9.181679160974454e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.181679160974454e-06

Optimization complete. Final v2v error: 2.6177446842193604 mm

Highest mean error: 2.8202736377716064 mm for frame 163

Lowest mean error: 2.4805431365966797 mm for frame 194

Saving results

Total time: 33.31902456283569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814939
Iteration 2/25 | Loss: 0.00148328
Iteration 3/25 | Loss: 0.00122316
Iteration 4/25 | Loss: 0.00117705
Iteration 5/25 | Loss: 0.00116503
Iteration 6/25 | Loss: 0.00116098
Iteration 7/25 | Loss: 0.00115979
Iteration 8/25 | Loss: 0.00115941
Iteration 9/25 | Loss: 0.00115917
Iteration 10/25 | Loss: 0.00115895
Iteration 11/25 | Loss: 0.00116275
Iteration 12/25 | Loss: 0.00115923
Iteration 13/25 | Loss: 0.00116247
Iteration 14/25 | Loss: 0.00116174
Iteration 15/25 | Loss: 0.00115789
Iteration 16/25 | Loss: 0.00115608
Iteration 17/25 | Loss: 0.00115580
Iteration 18/25 | Loss: 0.00115579
Iteration 19/25 | Loss: 0.00115579
Iteration 20/25 | Loss: 0.00115579
Iteration 21/25 | Loss: 0.00115579
Iteration 22/25 | Loss: 0.00115579
Iteration 23/25 | Loss: 0.00115579
Iteration 24/25 | Loss: 0.00115579
Iteration 25/25 | Loss: 0.00115579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.61900139
Iteration 2/25 | Loss: 0.00110512
Iteration 3/25 | Loss: 0.00110506
Iteration 4/25 | Loss: 0.00110506
Iteration 5/25 | Loss: 0.00110506
Iteration 6/25 | Loss: 0.00110506
Iteration 7/25 | Loss: 0.00110506
Iteration 8/25 | Loss: 0.00110506
Iteration 9/25 | Loss: 0.00110505
Iteration 10/25 | Loss: 0.00110505
Iteration 11/25 | Loss: 0.00110505
Iteration 12/25 | Loss: 0.00110505
Iteration 13/25 | Loss: 0.00110505
Iteration 14/25 | Loss: 0.00110505
Iteration 15/25 | Loss: 0.00110505
Iteration 16/25 | Loss: 0.00110505
Iteration 17/25 | Loss: 0.00110505
Iteration 18/25 | Loss: 0.00110505
Iteration 19/25 | Loss: 0.00110505
Iteration 20/25 | Loss: 0.00110505
Iteration 21/25 | Loss: 0.00110505
Iteration 22/25 | Loss: 0.00110505
Iteration 23/25 | Loss: 0.00110505
Iteration 24/25 | Loss: 0.00110505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001105054165236652, 0.001105054165236652, 0.001105054165236652, 0.001105054165236652, 0.001105054165236652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105054165236652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110505
Iteration 2/1000 | Loss: 0.00005336
Iteration 3/1000 | Loss: 0.00003649
Iteration 4/1000 | Loss: 0.00003030
Iteration 5/1000 | Loss: 0.00002787
Iteration 6/1000 | Loss: 0.00002648
Iteration 7/1000 | Loss: 0.00002527
Iteration 8/1000 | Loss: 0.00002452
Iteration 9/1000 | Loss: 0.00002384
Iteration 10/1000 | Loss: 0.00002341
Iteration 11/1000 | Loss: 0.00002313
Iteration 12/1000 | Loss: 0.00002288
Iteration 13/1000 | Loss: 0.00002265
Iteration 14/1000 | Loss: 0.00002248
Iteration 15/1000 | Loss: 0.00002232
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002214
Iteration 19/1000 | Loss: 0.00002209
Iteration 20/1000 | Loss: 0.00002209
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002205
Iteration 23/1000 | Loss: 0.00002205
Iteration 24/1000 | Loss: 0.00002205
Iteration 25/1000 | Loss: 0.00002205
Iteration 26/1000 | Loss: 0.00002204
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002204
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002203
Iteration 32/1000 | Loss: 0.00002202
Iteration 33/1000 | Loss: 0.00002202
Iteration 34/1000 | Loss: 0.00002202
Iteration 35/1000 | Loss: 0.00002201
Iteration 36/1000 | Loss: 0.00002201
Iteration 37/1000 | Loss: 0.00002200
Iteration 38/1000 | Loss: 0.00002199
Iteration 39/1000 | Loss: 0.00002199
Iteration 40/1000 | Loss: 0.00002199
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002198
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002197
Iteration 48/1000 | Loss: 0.00002196
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002195
Iteration 53/1000 | Loss: 0.00002195
Iteration 54/1000 | Loss: 0.00002195
Iteration 55/1000 | Loss: 0.00002194
Iteration 56/1000 | Loss: 0.00002194
Iteration 57/1000 | Loss: 0.00002194
Iteration 58/1000 | Loss: 0.00002193
Iteration 59/1000 | Loss: 0.00002193
Iteration 60/1000 | Loss: 0.00002193
Iteration 61/1000 | Loss: 0.00002193
Iteration 62/1000 | Loss: 0.00002192
Iteration 63/1000 | Loss: 0.00002192
Iteration 64/1000 | Loss: 0.00002192
Iteration 65/1000 | Loss: 0.00002191
Iteration 66/1000 | Loss: 0.00002191
Iteration 67/1000 | Loss: 0.00002191
Iteration 68/1000 | Loss: 0.00002190
Iteration 69/1000 | Loss: 0.00002190
Iteration 70/1000 | Loss: 0.00002190
Iteration 71/1000 | Loss: 0.00002190
Iteration 72/1000 | Loss: 0.00002190
Iteration 73/1000 | Loss: 0.00002189
Iteration 74/1000 | Loss: 0.00002189
Iteration 75/1000 | Loss: 0.00002189
Iteration 76/1000 | Loss: 0.00002189
Iteration 77/1000 | Loss: 0.00002188
Iteration 78/1000 | Loss: 0.00002188
Iteration 79/1000 | Loss: 0.00002188
Iteration 80/1000 | Loss: 0.00002188
Iteration 81/1000 | Loss: 0.00002188
Iteration 82/1000 | Loss: 0.00002187
Iteration 83/1000 | Loss: 0.00002187
Iteration 84/1000 | Loss: 0.00002187
Iteration 85/1000 | Loss: 0.00002186
Iteration 86/1000 | Loss: 0.00002186
Iteration 87/1000 | Loss: 0.00002186
Iteration 88/1000 | Loss: 0.00002186
Iteration 89/1000 | Loss: 0.00002185
Iteration 90/1000 | Loss: 0.00002185
Iteration 91/1000 | Loss: 0.00002185
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002185
Iteration 96/1000 | Loss: 0.00002185
Iteration 97/1000 | Loss: 0.00002185
Iteration 98/1000 | Loss: 0.00002185
Iteration 99/1000 | Loss: 0.00002185
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002184
Iteration 103/1000 | Loss: 0.00002184
Iteration 104/1000 | Loss: 0.00002184
Iteration 105/1000 | Loss: 0.00002184
Iteration 106/1000 | Loss: 0.00002184
Iteration 107/1000 | Loss: 0.00002184
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00002184
Iteration 110/1000 | Loss: 0.00002183
Iteration 111/1000 | Loss: 0.00002183
Iteration 112/1000 | Loss: 0.00002183
Iteration 113/1000 | Loss: 0.00002183
Iteration 114/1000 | Loss: 0.00002183
Iteration 115/1000 | Loss: 0.00002183
Iteration 116/1000 | Loss: 0.00002183
Iteration 117/1000 | Loss: 0.00002183
Iteration 118/1000 | Loss: 0.00002183
Iteration 119/1000 | Loss: 0.00002183
Iteration 120/1000 | Loss: 0.00002183
Iteration 121/1000 | Loss: 0.00002183
Iteration 122/1000 | Loss: 0.00002183
Iteration 123/1000 | Loss: 0.00002183
Iteration 124/1000 | Loss: 0.00002183
Iteration 125/1000 | Loss: 0.00002182
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002182
Iteration 129/1000 | Loss: 0.00002182
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002181
Iteration 133/1000 | Loss: 0.00002181
Iteration 134/1000 | Loss: 0.00002181
Iteration 135/1000 | Loss: 0.00002181
Iteration 136/1000 | Loss: 0.00002180
Iteration 137/1000 | Loss: 0.00002180
Iteration 138/1000 | Loss: 0.00002180
Iteration 139/1000 | Loss: 0.00002180
Iteration 140/1000 | Loss: 0.00002180
Iteration 141/1000 | Loss: 0.00002179
Iteration 142/1000 | Loss: 0.00002179
Iteration 143/1000 | Loss: 0.00002179
Iteration 144/1000 | Loss: 0.00002179
Iteration 145/1000 | Loss: 0.00002178
Iteration 146/1000 | Loss: 0.00002178
Iteration 147/1000 | Loss: 0.00002178
Iteration 148/1000 | Loss: 0.00002178
Iteration 149/1000 | Loss: 0.00002178
Iteration 150/1000 | Loss: 0.00002178
Iteration 151/1000 | Loss: 0.00002178
Iteration 152/1000 | Loss: 0.00002178
Iteration 153/1000 | Loss: 0.00002177
Iteration 154/1000 | Loss: 0.00002177
Iteration 155/1000 | Loss: 0.00002177
Iteration 156/1000 | Loss: 0.00002177
Iteration 157/1000 | Loss: 0.00002177
Iteration 158/1000 | Loss: 0.00002177
Iteration 159/1000 | Loss: 0.00002177
Iteration 160/1000 | Loss: 0.00002177
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00002176
Iteration 166/1000 | Loss: 0.00002176
Iteration 167/1000 | Loss: 0.00002176
Iteration 168/1000 | Loss: 0.00002176
Iteration 169/1000 | Loss: 0.00002176
Iteration 170/1000 | Loss: 0.00002176
Iteration 171/1000 | Loss: 0.00002176
Iteration 172/1000 | Loss: 0.00002176
Iteration 173/1000 | Loss: 0.00002176
Iteration 174/1000 | Loss: 0.00002176
Iteration 175/1000 | Loss: 0.00002176
Iteration 176/1000 | Loss: 0.00002176
Iteration 177/1000 | Loss: 0.00002175
Iteration 178/1000 | Loss: 0.00002175
Iteration 179/1000 | Loss: 0.00002175
Iteration 180/1000 | Loss: 0.00002175
Iteration 181/1000 | Loss: 0.00002175
Iteration 182/1000 | Loss: 0.00002175
Iteration 183/1000 | Loss: 0.00002175
Iteration 184/1000 | Loss: 0.00002175
Iteration 185/1000 | Loss: 0.00002175
Iteration 186/1000 | Loss: 0.00002174
Iteration 187/1000 | Loss: 0.00002174
Iteration 188/1000 | Loss: 0.00002174
Iteration 189/1000 | Loss: 0.00002174
Iteration 190/1000 | Loss: 0.00002174
Iteration 191/1000 | Loss: 0.00002174
Iteration 192/1000 | Loss: 0.00002174
Iteration 193/1000 | Loss: 0.00002174
Iteration 194/1000 | Loss: 0.00002174
Iteration 195/1000 | Loss: 0.00002174
Iteration 196/1000 | Loss: 0.00002174
Iteration 197/1000 | Loss: 0.00002174
Iteration 198/1000 | Loss: 0.00002173
Iteration 199/1000 | Loss: 0.00002173
Iteration 200/1000 | Loss: 0.00002173
Iteration 201/1000 | Loss: 0.00002173
Iteration 202/1000 | Loss: 0.00002173
Iteration 203/1000 | Loss: 0.00002173
Iteration 204/1000 | Loss: 0.00002173
Iteration 205/1000 | Loss: 0.00002173
Iteration 206/1000 | Loss: 0.00002173
Iteration 207/1000 | Loss: 0.00002173
Iteration 208/1000 | Loss: 0.00002173
Iteration 209/1000 | Loss: 0.00002173
Iteration 210/1000 | Loss: 0.00002173
Iteration 211/1000 | Loss: 0.00002173
Iteration 212/1000 | Loss: 0.00002173
Iteration 213/1000 | Loss: 0.00002173
Iteration 214/1000 | Loss: 0.00002172
Iteration 215/1000 | Loss: 0.00002172
Iteration 216/1000 | Loss: 0.00002172
Iteration 217/1000 | Loss: 0.00002172
Iteration 218/1000 | Loss: 0.00002172
Iteration 219/1000 | Loss: 0.00002172
Iteration 220/1000 | Loss: 0.00002172
Iteration 221/1000 | Loss: 0.00002172
Iteration 222/1000 | Loss: 0.00002172
Iteration 223/1000 | Loss: 0.00002172
Iteration 224/1000 | Loss: 0.00002172
Iteration 225/1000 | Loss: 0.00002172
Iteration 226/1000 | Loss: 0.00002172
Iteration 227/1000 | Loss: 0.00002172
Iteration 228/1000 | Loss: 0.00002172
Iteration 229/1000 | Loss: 0.00002172
Iteration 230/1000 | Loss: 0.00002172
Iteration 231/1000 | Loss: 0.00002172
Iteration 232/1000 | Loss: 0.00002171
Iteration 233/1000 | Loss: 0.00002171
Iteration 234/1000 | Loss: 0.00002171
Iteration 235/1000 | Loss: 0.00002171
Iteration 236/1000 | Loss: 0.00002171
Iteration 237/1000 | Loss: 0.00002171
Iteration 238/1000 | Loss: 0.00002171
Iteration 239/1000 | Loss: 0.00002171
Iteration 240/1000 | Loss: 0.00002171
Iteration 241/1000 | Loss: 0.00002171
Iteration 242/1000 | Loss: 0.00002171
Iteration 243/1000 | Loss: 0.00002171
Iteration 244/1000 | Loss: 0.00002170
Iteration 245/1000 | Loss: 0.00002170
Iteration 246/1000 | Loss: 0.00002170
Iteration 247/1000 | Loss: 0.00002170
Iteration 248/1000 | Loss: 0.00002170
Iteration 249/1000 | Loss: 0.00002170
Iteration 250/1000 | Loss: 0.00002170
Iteration 251/1000 | Loss: 0.00002170
Iteration 252/1000 | Loss: 0.00002170
Iteration 253/1000 | Loss: 0.00002170
Iteration 254/1000 | Loss: 0.00002170
Iteration 255/1000 | Loss: 0.00002170
Iteration 256/1000 | Loss: 0.00002170
Iteration 257/1000 | Loss: 0.00002170
Iteration 258/1000 | Loss: 0.00002170
Iteration 259/1000 | Loss: 0.00002170
Iteration 260/1000 | Loss: 0.00002169
Iteration 261/1000 | Loss: 0.00002169
Iteration 262/1000 | Loss: 0.00002169
Iteration 263/1000 | Loss: 0.00002169
Iteration 264/1000 | Loss: 0.00002169
Iteration 265/1000 | Loss: 0.00002169
Iteration 266/1000 | Loss: 0.00002169
Iteration 267/1000 | Loss: 0.00002169
Iteration 268/1000 | Loss: 0.00002169
Iteration 269/1000 | Loss: 0.00002169
Iteration 270/1000 | Loss: 0.00002169
Iteration 271/1000 | Loss: 0.00002169
Iteration 272/1000 | Loss: 0.00002169
Iteration 273/1000 | Loss: 0.00002169
Iteration 274/1000 | Loss: 0.00002169
Iteration 275/1000 | Loss: 0.00002169
Iteration 276/1000 | Loss: 0.00002168
Iteration 277/1000 | Loss: 0.00002168
Iteration 278/1000 | Loss: 0.00002168
Iteration 279/1000 | Loss: 0.00002168
Iteration 280/1000 | Loss: 0.00002168
Iteration 281/1000 | Loss: 0.00002168
Iteration 282/1000 | Loss: 0.00002168
Iteration 283/1000 | Loss: 0.00002168
Iteration 284/1000 | Loss: 0.00002168
Iteration 285/1000 | Loss: 0.00002168
Iteration 286/1000 | Loss: 0.00002168
Iteration 287/1000 | Loss: 0.00002168
Iteration 288/1000 | Loss: 0.00002168
Iteration 289/1000 | Loss: 0.00002168
Iteration 290/1000 | Loss: 0.00002168
Iteration 291/1000 | Loss: 0.00002168
Iteration 292/1000 | Loss: 0.00002168
Iteration 293/1000 | Loss: 0.00002168
Iteration 294/1000 | Loss: 0.00002168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.168340506614186e-05, 2.168340506614186e-05, 2.168340506614186e-05, 2.168340506614186e-05, 2.168340506614186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.168340506614186e-05

Optimization complete. Final v2v error: 3.8870997428894043 mm

Highest mean error: 5.567314147949219 mm for frame 50

Lowest mean error: 2.837162494659424 mm for frame 27

Saving results

Total time: 70.17414259910583
