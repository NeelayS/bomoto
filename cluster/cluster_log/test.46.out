Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=46, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2576-2631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01132962
Iteration 2/25 | Loss: 0.00143459
Iteration 3/25 | Loss: 0.00103268
Iteration 4/25 | Loss: 0.00084242
Iteration 5/25 | Loss: 0.00082077
Iteration 6/25 | Loss: 0.00077888
Iteration 7/25 | Loss: 0.00078114
Iteration 8/25 | Loss: 0.00077200
Iteration 9/25 | Loss: 0.00077086
Iteration 10/25 | Loss: 0.00075991
Iteration 11/25 | Loss: 0.00076656
Iteration 12/25 | Loss: 0.00075638
Iteration 13/25 | Loss: 0.00076359
Iteration 14/25 | Loss: 0.00076766
Iteration 15/25 | Loss: 0.00076769
Iteration 16/25 | Loss: 0.00076841
Iteration 17/25 | Loss: 0.00074932
Iteration 18/25 | Loss: 0.00074535
Iteration 19/25 | Loss: 0.00074948
Iteration 20/25 | Loss: 0.00074815
Iteration 21/25 | Loss: 0.00075308
Iteration 22/25 | Loss: 0.00075162
Iteration 23/25 | Loss: 0.00075103
Iteration 24/25 | Loss: 0.00074826
Iteration 25/25 | Loss: 0.00074508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38222563
Iteration 2/25 | Loss: 0.00121763
Iteration 3/25 | Loss: 0.00121762
Iteration 4/25 | Loss: 0.00094955
Iteration 5/25 | Loss: 0.00094954
Iteration 6/25 | Loss: 0.00094954
Iteration 7/25 | Loss: 0.00094954
Iteration 8/25 | Loss: 0.00094954
Iteration 9/25 | Loss: 0.00094954
Iteration 10/25 | Loss: 0.00094954
Iteration 11/25 | Loss: 0.00094954
Iteration 12/25 | Loss: 0.00094954
Iteration 13/25 | Loss: 0.00094954
Iteration 14/25 | Loss: 0.00094954
Iteration 15/25 | Loss: 0.00094954
Iteration 16/25 | Loss: 0.00094954
Iteration 17/25 | Loss: 0.00094954
Iteration 18/25 | Loss: 0.00094954
Iteration 19/25 | Loss: 0.00094954
Iteration 20/25 | Loss: 0.00094954
Iteration 21/25 | Loss: 0.00094954
Iteration 22/25 | Loss: 0.00094954
Iteration 23/25 | Loss: 0.00094954
Iteration 24/25 | Loss: 0.00094954
Iteration 25/25 | Loss: 0.00094954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094954
Iteration 2/1000 | Loss: 0.00072380
Iteration 3/1000 | Loss: 0.00232036
Iteration 4/1000 | Loss: 0.00117703
Iteration 5/1000 | Loss: 0.00169423
Iteration 6/1000 | Loss: 0.00067672
Iteration 7/1000 | Loss: 0.00093326
Iteration 8/1000 | Loss: 0.00075069
Iteration 9/1000 | Loss: 0.00025545
Iteration 10/1000 | Loss: 0.00078867
Iteration 11/1000 | Loss: 0.00053213
Iteration 12/1000 | Loss: 0.00232156
Iteration 13/1000 | Loss: 0.00127907
Iteration 14/1000 | Loss: 0.00307761
Iteration 15/1000 | Loss: 0.00193157
Iteration 16/1000 | Loss: 0.00297212
Iteration 17/1000 | Loss: 0.00221452
Iteration 18/1000 | Loss: 0.00169381
Iteration 19/1000 | Loss: 0.00067731
Iteration 20/1000 | Loss: 0.00059944
Iteration 21/1000 | Loss: 0.00027886
Iteration 22/1000 | Loss: 0.00024617
Iteration 23/1000 | Loss: 0.00022049
Iteration 24/1000 | Loss: 0.00026230
Iteration 25/1000 | Loss: 0.00028337
Iteration 26/1000 | Loss: 0.00023568
Iteration 27/1000 | Loss: 0.00017098
Iteration 28/1000 | Loss: 0.00020837
Iteration 29/1000 | Loss: 0.00026239
Iteration 30/1000 | Loss: 0.00018798
Iteration 31/1000 | Loss: 0.00019728
Iteration 32/1000 | Loss: 0.00032876
Iteration 33/1000 | Loss: 0.00018979
Iteration 34/1000 | Loss: 0.00084420
Iteration 35/1000 | Loss: 0.00006049
Iteration 36/1000 | Loss: 0.00013425
Iteration 37/1000 | Loss: 0.00005562
Iteration 38/1000 | Loss: 0.00031432
Iteration 39/1000 | Loss: 0.00005219
Iteration 40/1000 | Loss: 0.00003074
Iteration 41/1000 | Loss: 0.00006661
Iteration 42/1000 | Loss: 0.00005311
Iteration 43/1000 | Loss: 0.00006237
Iteration 44/1000 | Loss: 0.00004789
Iteration 45/1000 | Loss: 0.00032955
Iteration 46/1000 | Loss: 0.00017149
Iteration 47/1000 | Loss: 0.00005765
Iteration 48/1000 | Loss: 0.00002697
Iteration 49/1000 | Loss: 0.00008100
Iteration 50/1000 | Loss: 0.00073042
Iteration 51/1000 | Loss: 0.00097441
Iteration 52/1000 | Loss: 0.00060500
Iteration 53/1000 | Loss: 0.00025169
Iteration 54/1000 | Loss: 0.00004457
Iteration 55/1000 | Loss: 0.00023742
Iteration 56/1000 | Loss: 0.00006014
Iteration 57/1000 | Loss: 0.00002615
Iteration 58/1000 | Loss: 0.00003623
Iteration 59/1000 | Loss: 0.00013415
Iteration 60/1000 | Loss: 0.00002315
Iteration 61/1000 | Loss: 0.00002282
Iteration 62/1000 | Loss: 0.00003829
Iteration 63/1000 | Loss: 0.00002255
Iteration 64/1000 | Loss: 0.00003886
Iteration 65/1000 | Loss: 0.00002414
Iteration 66/1000 | Loss: 0.00003284
Iteration 67/1000 | Loss: 0.00002995
Iteration 68/1000 | Loss: 0.00002223
Iteration 69/1000 | Loss: 0.00003115
Iteration 70/1000 | Loss: 0.00002214
Iteration 71/1000 | Loss: 0.00002599
Iteration 72/1000 | Loss: 0.00004580
Iteration 73/1000 | Loss: 0.00002417
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002207
Iteration 76/1000 | Loss: 0.00002207
Iteration 77/1000 | Loss: 0.00002207
Iteration 78/1000 | Loss: 0.00002207
Iteration 79/1000 | Loss: 0.00002207
Iteration 80/1000 | Loss: 0.00002207
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00002206
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002221
Iteration 85/1000 | Loss: 0.00002202
Iteration 86/1000 | Loss: 0.00002200
Iteration 87/1000 | Loss: 0.00002200
Iteration 88/1000 | Loss: 0.00002200
Iteration 89/1000 | Loss: 0.00002200
Iteration 90/1000 | Loss: 0.00002200
Iteration 91/1000 | Loss: 0.00002200
Iteration 92/1000 | Loss: 0.00002200
Iteration 93/1000 | Loss: 0.00002200
Iteration 94/1000 | Loss: 0.00002200
Iteration 95/1000 | Loss: 0.00002200
Iteration 96/1000 | Loss: 0.00002248
Iteration 97/1000 | Loss: 0.00002248
Iteration 98/1000 | Loss: 0.00002248
Iteration 99/1000 | Loss: 0.00002247
Iteration 100/1000 | Loss: 0.00004524
Iteration 101/1000 | Loss: 0.00002211
Iteration 102/1000 | Loss: 0.00002195
Iteration 103/1000 | Loss: 0.00002194
Iteration 104/1000 | Loss: 0.00002194
Iteration 105/1000 | Loss: 0.00002194
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002194
Iteration 109/1000 | Loss: 0.00002193
Iteration 110/1000 | Loss: 0.00002193
Iteration 111/1000 | Loss: 0.00002193
Iteration 112/1000 | Loss: 0.00002193
Iteration 113/1000 | Loss: 0.00002193
Iteration 114/1000 | Loss: 0.00002193
Iteration 115/1000 | Loss: 0.00002193
Iteration 116/1000 | Loss: 0.00002193
Iteration 117/1000 | Loss: 0.00002193
Iteration 118/1000 | Loss: 0.00002193
Iteration 119/1000 | Loss: 0.00002193
Iteration 120/1000 | Loss: 0.00002193
Iteration 121/1000 | Loss: 0.00002193
Iteration 122/1000 | Loss: 0.00002193
Iteration 123/1000 | Loss: 0.00002193
Iteration 124/1000 | Loss: 0.00002193
Iteration 125/1000 | Loss: 0.00002193
Iteration 126/1000 | Loss: 0.00002193
Iteration 127/1000 | Loss: 0.00002193
Iteration 128/1000 | Loss: 0.00002193
Iteration 129/1000 | Loss: 0.00002193
Iteration 130/1000 | Loss: 0.00002193
Iteration 131/1000 | Loss: 0.00002193
Iteration 132/1000 | Loss: 0.00002193
Iteration 133/1000 | Loss: 0.00002193
Iteration 134/1000 | Loss: 0.00002193
Iteration 135/1000 | Loss: 0.00002193
Iteration 136/1000 | Loss: 0.00002193
Iteration 137/1000 | Loss: 0.00002193
Iteration 138/1000 | Loss: 0.00002193
Iteration 139/1000 | Loss: 0.00002193
Iteration 140/1000 | Loss: 0.00002193
Iteration 141/1000 | Loss: 0.00002193
Iteration 142/1000 | Loss: 0.00002193
Iteration 143/1000 | Loss: 0.00002193
Iteration 144/1000 | Loss: 0.00002193
Iteration 145/1000 | Loss: 0.00002193
Iteration 146/1000 | Loss: 0.00002193
Iteration 147/1000 | Loss: 0.00002193
Iteration 148/1000 | Loss: 0.00002193
Iteration 149/1000 | Loss: 0.00002193
Iteration 150/1000 | Loss: 0.00002193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.1925336113781668e-05, 2.1925336113781668e-05, 2.1925336113781668e-05, 2.1925336113781668e-05, 2.1925336113781668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1925336113781668e-05

Optimization complete. Final v2v error: 3.93681001663208 mm

Highest mean error: 6.760592937469482 mm for frame 147

Lowest mean error: 3.472095251083374 mm for frame 136

Saving results

Total time: 157.71469140052795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544480
Iteration 2/25 | Loss: 0.00087266
Iteration 3/25 | Loss: 0.00074264
Iteration 4/25 | Loss: 0.00071542
Iteration 5/25 | Loss: 0.00070543
Iteration 6/25 | Loss: 0.00070307
Iteration 7/25 | Loss: 0.00070263
Iteration 8/25 | Loss: 0.00070263
Iteration 9/25 | Loss: 0.00070263
Iteration 10/25 | Loss: 0.00070263
Iteration 11/25 | Loss: 0.00070263
Iteration 12/25 | Loss: 0.00070263
Iteration 13/25 | Loss: 0.00070263
Iteration 14/25 | Loss: 0.00070263
Iteration 15/25 | Loss: 0.00070263
Iteration 16/25 | Loss: 0.00070263
Iteration 17/25 | Loss: 0.00070263
Iteration 18/25 | Loss: 0.00070263
Iteration 19/25 | Loss: 0.00070263
Iteration 20/25 | Loss: 0.00070263
Iteration 21/25 | Loss: 0.00070263
Iteration 22/25 | Loss: 0.00070263
Iteration 23/25 | Loss: 0.00070263
Iteration 24/25 | Loss: 0.00070263
Iteration 25/25 | Loss: 0.00070263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.78955173
Iteration 2/25 | Loss: 0.00046685
Iteration 3/25 | Loss: 0.00046683
Iteration 4/25 | Loss: 0.00046683
Iteration 5/25 | Loss: 0.00046683
Iteration 6/25 | Loss: 0.00046683
Iteration 7/25 | Loss: 0.00046683
Iteration 8/25 | Loss: 0.00046683
Iteration 9/25 | Loss: 0.00046683
Iteration 10/25 | Loss: 0.00046683
Iteration 11/25 | Loss: 0.00046683
Iteration 12/25 | Loss: 0.00046683
Iteration 13/25 | Loss: 0.00046683
Iteration 14/25 | Loss: 0.00046683
Iteration 15/25 | Loss: 0.00046683
Iteration 16/25 | Loss: 0.00046683
Iteration 17/25 | Loss: 0.00046683
Iteration 18/25 | Loss: 0.00046683
Iteration 19/25 | Loss: 0.00046683
Iteration 20/25 | Loss: 0.00046683
Iteration 21/25 | Loss: 0.00046683
Iteration 22/25 | Loss: 0.00046683
Iteration 23/25 | Loss: 0.00046683
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00046683152322657406, 0.00046683152322657406, 0.00046683152322657406, 0.00046683152322657406, 0.00046683152322657406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046683152322657406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046683
Iteration 2/1000 | Loss: 0.00005128
Iteration 3/1000 | Loss: 0.00002718
Iteration 4/1000 | Loss: 0.00002476
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002233
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002086
Iteration 9/1000 | Loss: 0.00002065
Iteration 10/1000 | Loss: 0.00002056
Iteration 11/1000 | Loss: 0.00002056
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002055
Iteration 14/1000 | Loss: 0.00002053
Iteration 15/1000 | Loss: 0.00002052
Iteration 16/1000 | Loss: 0.00002050
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002048
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002046
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00002042
Iteration 24/1000 | Loss: 0.00002042
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002041
Iteration 27/1000 | Loss: 0.00002041
Iteration 28/1000 | Loss: 0.00002041
Iteration 29/1000 | Loss: 0.00002040
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00002037
Iteration 33/1000 | Loss: 0.00002036
Iteration 34/1000 | Loss: 0.00002036
Iteration 35/1000 | Loss: 0.00002035
Iteration 36/1000 | Loss: 0.00002035
Iteration 37/1000 | Loss: 0.00002035
Iteration 38/1000 | Loss: 0.00002033
Iteration 39/1000 | Loss: 0.00002033
Iteration 40/1000 | Loss: 0.00002032
Iteration 41/1000 | Loss: 0.00002032
Iteration 42/1000 | Loss: 0.00002032
Iteration 43/1000 | Loss: 0.00002032
Iteration 44/1000 | Loss: 0.00002031
Iteration 45/1000 | Loss: 0.00002031
Iteration 46/1000 | Loss: 0.00002031
Iteration 47/1000 | Loss: 0.00002031
Iteration 48/1000 | Loss: 0.00002030
Iteration 49/1000 | Loss: 0.00002030
Iteration 50/1000 | Loss: 0.00002029
Iteration 51/1000 | Loss: 0.00002029
Iteration 52/1000 | Loss: 0.00002029
Iteration 53/1000 | Loss: 0.00002028
Iteration 54/1000 | Loss: 0.00002028
Iteration 55/1000 | Loss: 0.00002027
Iteration 56/1000 | Loss: 0.00002027
Iteration 57/1000 | Loss: 0.00002027
Iteration 58/1000 | Loss: 0.00002027
Iteration 59/1000 | Loss: 0.00002026
Iteration 60/1000 | Loss: 0.00002026
Iteration 61/1000 | Loss: 0.00002026
Iteration 62/1000 | Loss: 0.00002026
Iteration 63/1000 | Loss: 0.00002026
Iteration 64/1000 | Loss: 0.00002026
Iteration 65/1000 | Loss: 0.00002026
Iteration 66/1000 | Loss: 0.00002026
Iteration 67/1000 | Loss: 0.00002026
Iteration 68/1000 | Loss: 0.00002025
Iteration 69/1000 | Loss: 0.00002025
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002025
Iteration 72/1000 | Loss: 0.00002025
Iteration 73/1000 | Loss: 0.00002024
Iteration 74/1000 | Loss: 0.00002024
Iteration 75/1000 | Loss: 0.00002024
Iteration 76/1000 | Loss: 0.00002023
Iteration 77/1000 | Loss: 0.00002023
Iteration 78/1000 | Loss: 0.00002023
Iteration 79/1000 | Loss: 0.00002023
Iteration 80/1000 | Loss: 0.00002023
Iteration 81/1000 | Loss: 0.00002023
Iteration 82/1000 | Loss: 0.00002023
Iteration 83/1000 | Loss: 0.00002023
Iteration 84/1000 | Loss: 0.00002022
Iteration 85/1000 | Loss: 0.00002022
Iteration 86/1000 | Loss: 0.00002022
Iteration 87/1000 | Loss: 0.00002021
Iteration 88/1000 | Loss: 0.00002021
Iteration 89/1000 | Loss: 0.00002021
Iteration 90/1000 | Loss: 0.00002021
Iteration 91/1000 | Loss: 0.00002021
Iteration 92/1000 | Loss: 0.00002020
Iteration 93/1000 | Loss: 0.00002020
Iteration 94/1000 | Loss: 0.00002020
Iteration 95/1000 | Loss: 0.00002020
Iteration 96/1000 | Loss: 0.00002020
Iteration 97/1000 | Loss: 0.00002020
Iteration 98/1000 | Loss: 0.00002020
Iteration 99/1000 | Loss: 0.00002020
Iteration 100/1000 | Loss: 0.00002020
Iteration 101/1000 | Loss: 0.00002020
Iteration 102/1000 | Loss: 0.00002019
Iteration 103/1000 | Loss: 0.00002019
Iteration 104/1000 | Loss: 0.00002019
Iteration 105/1000 | Loss: 0.00002019
Iteration 106/1000 | Loss: 0.00002019
Iteration 107/1000 | Loss: 0.00002019
Iteration 108/1000 | Loss: 0.00002019
Iteration 109/1000 | Loss: 0.00002018
Iteration 110/1000 | Loss: 0.00002018
Iteration 111/1000 | Loss: 0.00002018
Iteration 112/1000 | Loss: 0.00002018
Iteration 113/1000 | Loss: 0.00002018
Iteration 114/1000 | Loss: 0.00002018
Iteration 115/1000 | Loss: 0.00002018
Iteration 116/1000 | Loss: 0.00002018
Iteration 117/1000 | Loss: 0.00002018
Iteration 118/1000 | Loss: 0.00002018
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00002018
Iteration 121/1000 | Loss: 0.00002018
Iteration 122/1000 | Loss: 0.00002018
Iteration 123/1000 | Loss: 0.00002018
Iteration 124/1000 | Loss: 0.00002018
Iteration 125/1000 | Loss: 0.00002017
Iteration 126/1000 | Loss: 0.00002017
Iteration 127/1000 | Loss: 0.00002017
Iteration 128/1000 | Loss: 0.00002017
Iteration 129/1000 | Loss: 0.00002017
Iteration 130/1000 | Loss: 0.00002017
Iteration 131/1000 | Loss: 0.00002017
Iteration 132/1000 | Loss: 0.00002017
Iteration 133/1000 | Loss: 0.00002017
Iteration 134/1000 | Loss: 0.00002017
Iteration 135/1000 | Loss: 0.00002017
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002017
Iteration 138/1000 | Loss: 0.00002017
Iteration 139/1000 | Loss: 0.00002016
Iteration 140/1000 | Loss: 0.00002016
Iteration 141/1000 | Loss: 0.00002016
Iteration 142/1000 | Loss: 0.00002016
Iteration 143/1000 | Loss: 0.00002016
Iteration 144/1000 | Loss: 0.00002016
Iteration 145/1000 | Loss: 0.00002016
Iteration 146/1000 | Loss: 0.00002016
Iteration 147/1000 | Loss: 0.00002016
Iteration 148/1000 | Loss: 0.00002016
Iteration 149/1000 | Loss: 0.00002016
Iteration 150/1000 | Loss: 0.00002016
Iteration 151/1000 | Loss: 0.00002016
Iteration 152/1000 | Loss: 0.00002016
Iteration 153/1000 | Loss: 0.00002016
Iteration 154/1000 | Loss: 0.00002016
Iteration 155/1000 | Loss: 0.00002016
Iteration 156/1000 | Loss: 0.00002016
Iteration 157/1000 | Loss: 0.00002016
Iteration 158/1000 | Loss: 0.00002016
Iteration 159/1000 | Loss: 0.00002016
Iteration 160/1000 | Loss: 0.00002016
Iteration 161/1000 | Loss: 0.00002015
Iteration 162/1000 | Loss: 0.00002015
Iteration 163/1000 | Loss: 0.00002015
Iteration 164/1000 | Loss: 0.00002015
Iteration 165/1000 | Loss: 0.00002015
Iteration 166/1000 | Loss: 0.00002015
Iteration 167/1000 | Loss: 0.00002015
Iteration 168/1000 | Loss: 0.00002015
Iteration 169/1000 | Loss: 0.00002015
Iteration 170/1000 | Loss: 0.00002015
Iteration 171/1000 | Loss: 0.00002015
Iteration 172/1000 | Loss: 0.00002015
Iteration 173/1000 | Loss: 0.00002015
Iteration 174/1000 | Loss: 0.00002015
Iteration 175/1000 | Loss: 0.00002015
Iteration 176/1000 | Loss: 0.00002015
Iteration 177/1000 | Loss: 0.00002015
Iteration 178/1000 | Loss: 0.00002015
Iteration 179/1000 | Loss: 0.00002015
Iteration 180/1000 | Loss: 0.00002015
Iteration 181/1000 | Loss: 0.00002015
Iteration 182/1000 | Loss: 0.00002015
Iteration 183/1000 | Loss: 0.00002015
Iteration 184/1000 | Loss: 0.00002014
Iteration 185/1000 | Loss: 0.00002014
Iteration 186/1000 | Loss: 0.00002014
Iteration 187/1000 | Loss: 0.00002014
Iteration 188/1000 | Loss: 0.00002014
Iteration 189/1000 | Loss: 0.00002014
Iteration 190/1000 | Loss: 0.00002014
Iteration 191/1000 | Loss: 0.00002014
Iteration 192/1000 | Loss: 0.00002014
Iteration 193/1000 | Loss: 0.00002014
Iteration 194/1000 | Loss: 0.00002014
Iteration 195/1000 | Loss: 0.00002014
Iteration 196/1000 | Loss: 0.00002014
Iteration 197/1000 | Loss: 0.00002014
Iteration 198/1000 | Loss: 0.00002014
Iteration 199/1000 | Loss: 0.00002014
Iteration 200/1000 | Loss: 0.00002014
Iteration 201/1000 | Loss: 0.00002014
Iteration 202/1000 | Loss: 0.00002014
Iteration 203/1000 | Loss: 0.00002014
Iteration 204/1000 | Loss: 0.00002014
Iteration 205/1000 | Loss: 0.00002014
Iteration 206/1000 | Loss: 0.00002014
Iteration 207/1000 | Loss: 0.00002014
Iteration 208/1000 | Loss: 0.00002014
Iteration 209/1000 | Loss: 0.00002014
Iteration 210/1000 | Loss: 0.00002014
Iteration 211/1000 | Loss: 0.00002014
Iteration 212/1000 | Loss: 0.00002014
Iteration 213/1000 | Loss: 0.00002014
Iteration 214/1000 | Loss: 0.00002014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.0140416381764226e-05, 2.0140416381764226e-05, 2.0140416381764226e-05, 2.0140416381764226e-05, 2.0140416381764226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0140416381764226e-05

Optimization complete. Final v2v error: 3.8325328826904297 mm

Highest mean error: 4.35722541809082 mm for frame 71

Lowest mean error: 3.383272171020508 mm for frame 34

Saving results

Total time: 37.327534914016724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492435
Iteration 2/25 | Loss: 0.00086119
Iteration 3/25 | Loss: 0.00074927
Iteration 4/25 | Loss: 0.00071991
Iteration 5/25 | Loss: 0.00070563
Iteration 6/25 | Loss: 0.00070260
Iteration 7/25 | Loss: 0.00070198
Iteration 8/25 | Loss: 0.00070198
Iteration 9/25 | Loss: 0.00070198
Iteration 10/25 | Loss: 0.00070198
Iteration 11/25 | Loss: 0.00070198
Iteration 12/25 | Loss: 0.00070198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007019830518402159, 0.0007019830518402159, 0.0007019830518402159, 0.0007019830518402159, 0.0007019830518402159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007019830518402159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79537463
Iteration 2/25 | Loss: 0.00052231
Iteration 3/25 | Loss: 0.00052231
Iteration 4/25 | Loss: 0.00052231
Iteration 5/25 | Loss: 0.00052231
Iteration 6/25 | Loss: 0.00052231
Iteration 7/25 | Loss: 0.00052231
Iteration 8/25 | Loss: 0.00052231
Iteration 9/25 | Loss: 0.00052231
Iteration 10/25 | Loss: 0.00052231
Iteration 11/25 | Loss: 0.00052231
Iteration 12/25 | Loss: 0.00052231
Iteration 13/25 | Loss: 0.00052231
Iteration 14/25 | Loss: 0.00052231
Iteration 15/25 | Loss: 0.00052231
Iteration 16/25 | Loss: 0.00052231
Iteration 17/25 | Loss: 0.00052231
Iteration 18/25 | Loss: 0.00052231
Iteration 19/25 | Loss: 0.00052231
Iteration 20/25 | Loss: 0.00052231
Iteration 21/25 | Loss: 0.00052231
Iteration 22/25 | Loss: 0.00052231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005223098560236394, 0.0005223098560236394, 0.0005223098560236394, 0.0005223098560236394, 0.0005223098560236394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005223098560236394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052231
Iteration 2/1000 | Loss: 0.00004202
Iteration 3/1000 | Loss: 0.00002345
Iteration 4/1000 | Loss: 0.00002116
Iteration 5/1000 | Loss: 0.00001964
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001769
Iteration 10/1000 | Loss: 0.00001769
Iteration 11/1000 | Loss: 0.00001761
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001755
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001753
Iteration 16/1000 | Loss: 0.00001751
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001749
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001747
Iteration 21/1000 | Loss: 0.00001747
Iteration 22/1000 | Loss: 0.00001746
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001745
Iteration 25/1000 | Loss: 0.00001745
Iteration 26/1000 | Loss: 0.00001744
Iteration 27/1000 | Loss: 0.00001743
Iteration 28/1000 | Loss: 0.00001743
Iteration 29/1000 | Loss: 0.00001743
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001742
Iteration 34/1000 | Loss: 0.00001742
Iteration 35/1000 | Loss: 0.00001742
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001742
Iteration 40/1000 | Loss: 0.00001742
Iteration 41/1000 | Loss: 0.00001742
Iteration 42/1000 | Loss: 0.00001742
Iteration 43/1000 | Loss: 0.00001742
Iteration 44/1000 | Loss: 0.00001742
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001741
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001740
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001732
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001730
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001730
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001723
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001723
Iteration 124/1000 | Loss: 0.00001723
Iteration 125/1000 | Loss: 0.00001723
Iteration 126/1000 | Loss: 0.00001723
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001723
Iteration 129/1000 | Loss: 0.00001722
Iteration 130/1000 | Loss: 0.00001722
Iteration 131/1000 | Loss: 0.00001721
Iteration 132/1000 | Loss: 0.00001721
Iteration 133/1000 | Loss: 0.00001721
Iteration 134/1000 | Loss: 0.00001721
Iteration 135/1000 | Loss: 0.00001721
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.7203346942551434e-05, 1.7203346942551434e-05, 1.7203346942551434e-05, 1.7203346942551434e-05, 1.7203346942551434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7203346942551434e-05

Optimization complete. Final v2v error: 3.5620594024658203 mm

Highest mean error: 3.9721498489379883 mm for frame 126

Lowest mean error: 3.207961082458496 mm for frame 68

Saving results

Total time: 33.20415472984314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867965
Iteration 2/25 | Loss: 0.00145069
Iteration 3/25 | Loss: 0.00086458
Iteration 4/25 | Loss: 0.00078845
Iteration 5/25 | Loss: 0.00077163
Iteration 6/25 | Loss: 0.00076687
Iteration 7/25 | Loss: 0.00076604
Iteration 8/25 | Loss: 0.00076601
Iteration 9/25 | Loss: 0.00076601
Iteration 10/25 | Loss: 0.00076601
Iteration 11/25 | Loss: 0.00076601
Iteration 12/25 | Loss: 0.00076601
Iteration 13/25 | Loss: 0.00076601
Iteration 14/25 | Loss: 0.00076601
Iteration 15/25 | Loss: 0.00076601
Iteration 16/25 | Loss: 0.00076601
Iteration 17/25 | Loss: 0.00076601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007660132832825184, 0.0007660132832825184, 0.0007660132832825184, 0.0007660132832825184, 0.0007660132832825184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007660132832825184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47677374
Iteration 2/25 | Loss: 0.00046665
Iteration 3/25 | Loss: 0.00046664
Iteration 4/25 | Loss: 0.00046664
Iteration 5/25 | Loss: 0.00046664
Iteration 6/25 | Loss: 0.00046664
Iteration 7/25 | Loss: 0.00046664
Iteration 8/25 | Loss: 0.00046664
Iteration 9/25 | Loss: 0.00046664
Iteration 10/25 | Loss: 0.00046664
Iteration 11/25 | Loss: 0.00046664
Iteration 12/25 | Loss: 0.00046664
Iteration 13/25 | Loss: 0.00046664
Iteration 14/25 | Loss: 0.00046664
Iteration 15/25 | Loss: 0.00046664
Iteration 16/25 | Loss: 0.00046664
Iteration 17/25 | Loss: 0.00046664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004666422901209444, 0.0004666422901209444, 0.0004666422901209444, 0.0004666422901209444, 0.0004666422901209444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004666422901209444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046664
Iteration 2/1000 | Loss: 0.00004851
Iteration 3/1000 | Loss: 0.00003112
Iteration 4/1000 | Loss: 0.00002741
Iteration 5/1000 | Loss: 0.00002593
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00002455
Iteration 8/1000 | Loss: 0.00002413
Iteration 9/1000 | Loss: 0.00002382
Iteration 10/1000 | Loss: 0.00002365
Iteration 11/1000 | Loss: 0.00002347
Iteration 12/1000 | Loss: 0.00002340
Iteration 13/1000 | Loss: 0.00002337
Iteration 14/1000 | Loss: 0.00002336
Iteration 15/1000 | Loss: 0.00002331
Iteration 16/1000 | Loss: 0.00002327
Iteration 17/1000 | Loss: 0.00002325
Iteration 18/1000 | Loss: 0.00002325
Iteration 19/1000 | Loss: 0.00002325
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00002324
Iteration 22/1000 | Loss: 0.00002322
Iteration 23/1000 | Loss: 0.00002322
Iteration 24/1000 | Loss: 0.00002319
Iteration 25/1000 | Loss: 0.00002319
Iteration 26/1000 | Loss: 0.00002317
Iteration 27/1000 | Loss: 0.00002317
Iteration 28/1000 | Loss: 0.00002317
Iteration 29/1000 | Loss: 0.00002316
Iteration 30/1000 | Loss: 0.00002316
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002314
Iteration 33/1000 | Loss: 0.00002314
Iteration 34/1000 | Loss: 0.00002314
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002314
Iteration 37/1000 | Loss: 0.00002314
Iteration 38/1000 | Loss: 0.00002314
Iteration 39/1000 | Loss: 0.00002314
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00002313
Iteration 42/1000 | Loss: 0.00002313
Iteration 43/1000 | Loss: 0.00002313
Iteration 44/1000 | Loss: 0.00002313
Iteration 45/1000 | Loss: 0.00002313
Iteration 46/1000 | Loss: 0.00002313
Iteration 47/1000 | Loss: 0.00002313
Iteration 48/1000 | Loss: 0.00002313
Iteration 49/1000 | Loss: 0.00002312
Iteration 50/1000 | Loss: 0.00002311
Iteration 51/1000 | Loss: 0.00002311
Iteration 52/1000 | Loss: 0.00002311
Iteration 53/1000 | Loss: 0.00002311
Iteration 54/1000 | Loss: 0.00002311
Iteration 55/1000 | Loss: 0.00002311
Iteration 56/1000 | Loss: 0.00002311
Iteration 57/1000 | Loss: 0.00002311
Iteration 58/1000 | Loss: 0.00002311
Iteration 59/1000 | Loss: 0.00002311
Iteration 60/1000 | Loss: 0.00002310
Iteration 61/1000 | Loss: 0.00002310
Iteration 62/1000 | Loss: 0.00002309
Iteration 63/1000 | Loss: 0.00002309
Iteration 64/1000 | Loss: 0.00002309
Iteration 65/1000 | Loss: 0.00002309
Iteration 66/1000 | Loss: 0.00002308
Iteration 67/1000 | Loss: 0.00002308
Iteration 68/1000 | Loss: 0.00002308
Iteration 69/1000 | Loss: 0.00002308
Iteration 70/1000 | Loss: 0.00002308
Iteration 71/1000 | Loss: 0.00002308
Iteration 72/1000 | Loss: 0.00002307
Iteration 73/1000 | Loss: 0.00002307
Iteration 74/1000 | Loss: 0.00002307
Iteration 75/1000 | Loss: 0.00002307
Iteration 76/1000 | Loss: 0.00002307
Iteration 77/1000 | Loss: 0.00002306
Iteration 78/1000 | Loss: 0.00002306
Iteration 79/1000 | Loss: 0.00002306
Iteration 80/1000 | Loss: 0.00002305
Iteration 81/1000 | Loss: 0.00002305
Iteration 82/1000 | Loss: 0.00002305
Iteration 83/1000 | Loss: 0.00002305
Iteration 84/1000 | Loss: 0.00002305
Iteration 85/1000 | Loss: 0.00002305
Iteration 86/1000 | Loss: 0.00002305
Iteration 87/1000 | Loss: 0.00002305
Iteration 88/1000 | Loss: 0.00002305
Iteration 89/1000 | Loss: 0.00002305
Iteration 90/1000 | Loss: 0.00002305
Iteration 91/1000 | Loss: 0.00002304
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00002304
Iteration 94/1000 | Loss: 0.00002304
Iteration 95/1000 | Loss: 0.00002304
Iteration 96/1000 | Loss: 0.00002304
Iteration 97/1000 | Loss: 0.00002303
Iteration 98/1000 | Loss: 0.00002303
Iteration 99/1000 | Loss: 0.00002303
Iteration 100/1000 | Loss: 0.00002303
Iteration 101/1000 | Loss: 0.00002303
Iteration 102/1000 | Loss: 0.00002303
Iteration 103/1000 | Loss: 0.00002303
Iteration 104/1000 | Loss: 0.00002303
Iteration 105/1000 | Loss: 0.00002302
Iteration 106/1000 | Loss: 0.00002302
Iteration 107/1000 | Loss: 0.00002302
Iteration 108/1000 | Loss: 0.00002302
Iteration 109/1000 | Loss: 0.00002302
Iteration 110/1000 | Loss: 0.00002301
Iteration 111/1000 | Loss: 0.00002301
Iteration 112/1000 | Loss: 0.00002301
Iteration 113/1000 | Loss: 0.00002301
Iteration 114/1000 | Loss: 0.00002301
Iteration 115/1000 | Loss: 0.00002301
Iteration 116/1000 | Loss: 0.00002301
Iteration 117/1000 | Loss: 0.00002301
Iteration 118/1000 | Loss: 0.00002301
Iteration 119/1000 | Loss: 0.00002301
Iteration 120/1000 | Loss: 0.00002301
Iteration 121/1000 | Loss: 0.00002301
Iteration 122/1000 | Loss: 0.00002300
Iteration 123/1000 | Loss: 0.00002300
Iteration 124/1000 | Loss: 0.00002300
Iteration 125/1000 | Loss: 0.00002300
Iteration 126/1000 | Loss: 0.00002300
Iteration 127/1000 | Loss: 0.00002299
Iteration 128/1000 | Loss: 0.00002299
Iteration 129/1000 | Loss: 0.00002299
Iteration 130/1000 | Loss: 0.00002299
Iteration 131/1000 | Loss: 0.00002299
Iteration 132/1000 | Loss: 0.00002299
Iteration 133/1000 | Loss: 0.00002299
Iteration 134/1000 | Loss: 0.00002299
Iteration 135/1000 | Loss: 0.00002299
Iteration 136/1000 | Loss: 0.00002299
Iteration 137/1000 | Loss: 0.00002299
Iteration 138/1000 | Loss: 0.00002298
Iteration 139/1000 | Loss: 0.00002298
Iteration 140/1000 | Loss: 0.00002298
Iteration 141/1000 | Loss: 0.00002298
Iteration 142/1000 | Loss: 0.00002298
Iteration 143/1000 | Loss: 0.00002298
Iteration 144/1000 | Loss: 0.00002297
Iteration 145/1000 | Loss: 0.00002297
Iteration 146/1000 | Loss: 0.00002297
Iteration 147/1000 | Loss: 0.00002297
Iteration 148/1000 | Loss: 0.00002297
Iteration 149/1000 | Loss: 0.00002297
Iteration 150/1000 | Loss: 0.00002297
Iteration 151/1000 | Loss: 0.00002297
Iteration 152/1000 | Loss: 0.00002297
Iteration 153/1000 | Loss: 0.00002296
Iteration 154/1000 | Loss: 0.00002296
Iteration 155/1000 | Loss: 0.00002296
Iteration 156/1000 | Loss: 0.00002296
Iteration 157/1000 | Loss: 0.00002296
Iteration 158/1000 | Loss: 0.00002296
Iteration 159/1000 | Loss: 0.00002295
Iteration 160/1000 | Loss: 0.00002295
Iteration 161/1000 | Loss: 0.00002295
Iteration 162/1000 | Loss: 0.00002295
Iteration 163/1000 | Loss: 0.00002295
Iteration 164/1000 | Loss: 0.00002295
Iteration 165/1000 | Loss: 0.00002294
Iteration 166/1000 | Loss: 0.00002294
Iteration 167/1000 | Loss: 0.00002294
Iteration 168/1000 | Loss: 0.00002294
Iteration 169/1000 | Loss: 0.00002294
Iteration 170/1000 | Loss: 0.00002294
Iteration 171/1000 | Loss: 0.00002294
Iteration 172/1000 | Loss: 0.00002294
Iteration 173/1000 | Loss: 0.00002294
Iteration 174/1000 | Loss: 0.00002293
Iteration 175/1000 | Loss: 0.00002293
Iteration 176/1000 | Loss: 0.00002293
Iteration 177/1000 | Loss: 0.00002293
Iteration 178/1000 | Loss: 0.00002293
Iteration 179/1000 | Loss: 0.00002293
Iteration 180/1000 | Loss: 0.00002293
Iteration 181/1000 | Loss: 0.00002293
Iteration 182/1000 | Loss: 0.00002293
Iteration 183/1000 | Loss: 0.00002293
Iteration 184/1000 | Loss: 0.00002293
Iteration 185/1000 | Loss: 0.00002293
Iteration 186/1000 | Loss: 0.00002293
Iteration 187/1000 | Loss: 0.00002293
Iteration 188/1000 | Loss: 0.00002293
Iteration 189/1000 | Loss: 0.00002293
Iteration 190/1000 | Loss: 0.00002293
Iteration 191/1000 | Loss: 0.00002293
Iteration 192/1000 | Loss: 0.00002293
Iteration 193/1000 | Loss: 0.00002292
Iteration 194/1000 | Loss: 0.00002292
Iteration 195/1000 | Loss: 0.00002292
Iteration 196/1000 | Loss: 0.00002292
Iteration 197/1000 | Loss: 0.00002292
Iteration 198/1000 | Loss: 0.00002292
Iteration 199/1000 | Loss: 0.00002292
Iteration 200/1000 | Loss: 0.00002292
Iteration 201/1000 | Loss: 0.00002292
Iteration 202/1000 | Loss: 0.00002292
Iteration 203/1000 | Loss: 0.00002291
Iteration 204/1000 | Loss: 0.00002291
Iteration 205/1000 | Loss: 0.00002291
Iteration 206/1000 | Loss: 0.00002291
Iteration 207/1000 | Loss: 0.00002291
Iteration 208/1000 | Loss: 0.00002291
Iteration 209/1000 | Loss: 0.00002291
Iteration 210/1000 | Loss: 0.00002291
Iteration 211/1000 | Loss: 0.00002291
Iteration 212/1000 | Loss: 0.00002291
Iteration 213/1000 | Loss: 0.00002290
Iteration 214/1000 | Loss: 0.00002290
Iteration 215/1000 | Loss: 0.00002290
Iteration 216/1000 | Loss: 0.00002290
Iteration 217/1000 | Loss: 0.00002290
Iteration 218/1000 | Loss: 0.00002290
Iteration 219/1000 | Loss: 0.00002290
Iteration 220/1000 | Loss: 0.00002290
Iteration 221/1000 | Loss: 0.00002290
Iteration 222/1000 | Loss: 0.00002290
Iteration 223/1000 | Loss: 0.00002290
Iteration 224/1000 | Loss: 0.00002290
Iteration 225/1000 | Loss: 0.00002290
Iteration 226/1000 | Loss: 0.00002290
Iteration 227/1000 | Loss: 0.00002290
Iteration 228/1000 | Loss: 0.00002290
Iteration 229/1000 | Loss: 0.00002290
Iteration 230/1000 | Loss: 0.00002290
Iteration 231/1000 | Loss: 0.00002290
Iteration 232/1000 | Loss: 0.00002290
Iteration 233/1000 | Loss: 0.00002290
Iteration 234/1000 | Loss: 0.00002290
Iteration 235/1000 | Loss: 0.00002290
Iteration 236/1000 | Loss: 0.00002290
Iteration 237/1000 | Loss: 0.00002290
Iteration 238/1000 | Loss: 0.00002290
Iteration 239/1000 | Loss: 0.00002290
Iteration 240/1000 | Loss: 0.00002290
Iteration 241/1000 | Loss: 0.00002290
Iteration 242/1000 | Loss: 0.00002290
Iteration 243/1000 | Loss: 0.00002290
Iteration 244/1000 | Loss: 0.00002290
Iteration 245/1000 | Loss: 0.00002290
Iteration 246/1000 | Loss: 0.00002290
Iteration 247/1000 | Loss: 0.00002290
Iteration 248/1000 | Loss: 0.00002290
Iteration 249/1000 | Loss: 0.00002290
Iteration 250/1000 | Loss: 0.00002290
Iteration 251/1000 | Loss: 0.00002290
Iteration 252/1000 | Loss: 0.00002290
Iteration 253/1000 | Loss: 0.00002290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [2.290252632519696e-05, 2.290252632519696e-05, 2.290252632519696e-05, 2.290252632519696e-05, 2.290252632519696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.290252632519696e-05

Optimization complete. Final v2v error: 3.9534335136413574 mm

Highest mean error: 5.206332206726074 mm for frame 149

Lowest mean error: 3.217360496520996 mm for frame 36

Saving results

Total time: 45.05455160140991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446033
Iteration 2/25 | Loss: 0.00084256
Iteration 3/25 | Loss: 0.00073952
Iteration 4/25 | Loss: 0.00071104
Iteration 5/25 | Loss: 0.00069749
Iteration 6/25 | Loss: 0.00069508
Iteration 7/25 | Loss: 0.00069460
Iteration 8/25 | Loss: 0.00069460
Iteration 9/25 | Loss: 0.00069460
Iteration 10/25 | Loss: 0.00069460
Iteration 11/25 | Loss: 0.00069460
Iteration 12/25 | Loss: 0.00069460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006945987697690725, 0.0006945987697690725, 0.0006945987697690725, 0.0006945987697690725, 0.0006945987697690725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006945987697690725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60879195
Iteration 2/25 | Loss: 0.00036711
Iteration 3/25 | Loss: 0.00036711
Iteration 4/25 | Loss: 0.00036711
Iteration 5/25 | Loss: 0.00036711
Iteration 6/25 | Loss: 0.00036711
Iteration 7/25 | Loss: 0.00036711
Iteration 8/25 | Loss: 0.00036711
Iteration 9/25 | Loss: 0.00036711
Iteration 10/25 | Loss: 0.00036711
Iteration 11/25 | Loss: 0.00036711
Iteration 12/25 | Loss: 0.00036711
Iteration 13/25 | Loss: 0.00036711
Iteration 14/25 | Loss: 0.00036711
Iteration 15/25 | Loss: 0.00036711
Iteration 16/25 | Loss: 0.00036711
Iteration 17/25 | Loss: 0.00036711
Iteration 18/25 | Loss: 0.00036711
Iteration 19/25 | Loss: 0.00036711
Iteration 20/25 | Loss: 0.00036711
Iteration 21/25 | Loss: 0.00036711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003671052400022745, 0.0003671052400022745, 0.0003671052400022745, 0.0003671052400022745, 0.0003671052400022745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003671052400022745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036711
Iteration 2/1000 | Loss: 0.00003559
Iteration 3/1000 | Loss: 0.00002367
Iteration 4/1000 | Loss: 0.00002225
Iteration 5/1000 | Loss: 0.00002079
Iteration 6/1000 | Loss: 0.00001997
Iteration 7/1000 | Loss: 0.00001932
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001882
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001880
Iteration 13/1000 | Loss: 0.00001880
Iteration 14/1000 | Loss: 0.00001879
Iteration 15/1000 | Loss: 0.00001879
Iteration 16/1000 | Loss: 0.00001879
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001878
Iteration 20/1000 | Loss: 0.00001878
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001877
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001869
Iteration 44/1000 | Loss: 0.00001869
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001865
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001864
Iteration 70/1000 | Loss: 0.00001864
Iteration 71/1000 | Loss: 0.00001864
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001864
Iteration 74/1000 | Loss: 0.00001864
Iteration 75/1000 | Loss: 0.00001864
Iteration 76/1000 | Loss: 0.00001864
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001863
Iteration 79/1000 | Loss: 0.00001863
Iteration 80/1000 | Loss: 0.00001863
Iteration 81/1000 | Loss: 0.00001863
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001863
Iteration 84/1000 | Loss: 0.00001863
Iteration 85/1000 | Loss: 0.00001863
Iteration 86/1000 | Loss: 0.00001863
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001862
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001860
Iteration 103/1000 | Loss: 0.00001860
Iteration 104/1000 | Loss: 0.00001860
Iteration 105/1000 | Loss: 0.00001859
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001857
Iteration 115/1000 | Loss: 0.00001857
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001857
Iteration 119/1000 | Loss: 0.00001857
Iteration 120/1000 | Loss: 0.00001857
Iteration 121/1000 | Loss: 0.00001857
Iteration 122/1000 | Loss: 0.00001857
Iteration 123/1000 | Loss: 0.00001856
Iteration 124/1000 | Loss: 0.00001856
Iteration 125/1000 | Loss: 0.00001856
Iteration 126/1000 | Loss: 0.00001856
Iteration 127/1000 | Loss: 0.00001856
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001856
Iteration 130/1000 | Loss: 0.00001856
Iteration 131/1000 | Loss: 0.00001856
Iteration 132/1000 | Loss: 0.00001856
Iteration 133/1000 | Loss: 0.00001856
Iteration 134/1000 | Loss: 0.00001856
Iteration 135/1000 | Loss: 0.00001856
Iteration 136/1000 | Loss: 0.00001856
Iteration 137/1000 | Loss: 0.00001856
Iteration 138/1000 | Loss: 0.00001856
Iteration 139/1000 | Loss: 0.00001856
Iteration 140/1000 | Loss: 0.00001856
Iteration 141/1000 | Loss: 0.00001856
Iteration 142/1000 | Loss: 0.00001856
Iteration 143/1000 | Loss: 0.00001856
Iteration 144/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.8556291252025403e-05, 1.8556291252025403e-05, 1.8556291252025403e-05, 1.8556291252025403e-05, 1.8556291252025403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8556291252025403e-05

Optimization complete. Final v2v error: 3.657808780670166 mm

Highest mean error: 4.056524276733398 mm for frame 114

Lowest mean error: 3.244150161743164 mm for frame 161

Saving results

Total time: 33.25212907791138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00264764
Iteration 2/25 | Loss: 0.00099460
Iteration 3/25 | Loss: 0.00077248
Iteration 4/25 | Loss: 0.00072936
Iteration 5/25 | Loss: 0.00071600
Iteration 6/25 | Loss: 0.00071197
Iteration 7/25 | Loss: 0.00071127
Iteration 8/25 | Loss: 0.00071090
Iteration 9/25 | Loss: 0.00071090
Iteration 10/25 | Loss: 0.00071090
Iteration 11/25 | Loss: 0.00071090
Iteration 12/25 | Loss: 0.00071090
Iteration 13/25 | Loss: 0.00071090
Iteration 14/25 | Loss: 0.00071090
Iteration 15/25 | Loss: 0.00071090
Iteration 16/25 | Loss: 0.00071090
Iteration 17/25 | Loss: 0.00071090
Iteration 18/25 | Loss: 0.00071090
Iteration 19/25 | Loss: 0.00071090
Iteration 20/25 | Loss: 0.00071090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000710895808879286, 0.000710895808879286, 0.000710895808879286, 0.000710895808879286, 0.000710895808879286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000710895808879286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46564364
Iteration 2/25 | Loss: 0.00054193
Iteration 3/25 | Loss: 0.00054193
Iteration 4/25 | Loss: 0.00054193
Iteration 5/25 | Loss: 0.00054193
Iteration 6/25 | Loss: 0.00054193
Iteration 7/25 | Loss: 0.00054193
Iteration 8/25 | Loss: 0.00054193
Iteration 9/25 | Loss: 0.00054193
Iteration 10/25 | Loss: 0.00054193
Iteration 11/25 | Loss: 0.00054193
Iteration 12/25 | Loss: 0.00054193
Iteration 13/25 | Loss: 0.00054193
Iteration 14/25 | Loss: 0.00054193
Iteration 15/25 | Loss: 0.00054193
Iteration 16/25 | Loss: 0.00054193
Iteration 17/25 | Loss: 0.00054193
Iteration 18/25 | Loss: 0.00054193
Iteration 19/25 | Loss: 0.00054193
Iteration 20/25 | Loss: 0.00054193
Iteration 21/25 | Loss: 0.00054193
Iteration 22/25 | Loss: 0.00054193
Iteration 23/25 | Loss: 0.00054193
Iteration 24/25 | Loss: 0.00054193
Iteration 25/25 | Loss: 0.00054193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054193
Iteration 2/1000 | Loss: 0.00005082
Iteration 3/1000 | Loss: 0.00002954
Iteration 4/1000 | Loss: 0.00002192
Iteration 5/1000 | Loss: 0.00002013
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001811
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001754
Iteration 11/1000 | Loss: 0.00001745
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001727
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001714
Iteration 32/1000 | Loss: 0.00001714
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001710
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001698
Iteration 76/1000 | Loss: 0.00001698
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001696
Iteration 102/1000 | Loss: 0.00001696
Iteration 103/1000 | Loss: 0.00001696
Iteration 104/1000 | Loss: 0.00001696
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001694
Iteration 125/1000 | Loss: 0.00001694
Iteration 126/1000 | Loss: 0.00001694
Iteration 127/1000 | Loss: 0.00001694
Iteration 128/1000 | Loss: 0.00001694
Iteration 129/1000 | Loss: 0.00001694
Iteration 130/1000 | Loss: 0.00001694
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001693
Iteration 135/1000 | Loss: 0.00001693
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001693
Iteration 138/1000 | Loss: 0.00001693
Iteration 139/1000 | Loss: 0.00001693
Iteration 140/1000 | Loss: 0.00001693
Iteration 141/1000 | Loss: 0.00001693
Iteration 142/1000 | Loss: 0.00001693
Iteration 143/1000 | Loss: 0.00001693
Iteration 144/1000 | Loss: 0.00001693
Iteration 145/1000 | Loss: 0.00001693
Iteration 146/1000 | Loss: 0.00001693
Iteration 147/1000 | Loss: 0.00001693
Iteration 148/1000 | Loss: 0.00001693
Iteration 149/1000 | Loss: 0.00001693
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001692
Iteration 154/1000 | Loss: 0.00001692
Iteration 155/1000 | Loss: 0.00001692
Iteration 156/1000 | Loss: 0.00001692
Iteration 157/1000 | Loss: 0.00001692
Iteration 158/1000 | Loss: 0.00001692
Iteration 159/1000 | Loss: 0.00001692
Iteration 160/1000 | Loss: 0.00001692
Iteration 161/1000 | Loss: 0.00001692
Iteration 162/1000 | Loss: 0.00001692
Iteration 163/1000 | Loss: 0.00001692
Iteration 164/1000 | Loss: 0.00001692
Iteration 165/1000 | Loss: 0.00001692
Iteration 166/1000 | Loss: 0.00001692
Iteration 167/1000 | Loss: 0.00001692
Iteration 168/1000 | Loss: 0.00001692
Iteration 169/1000 | Loss: 0.00001692
Iteration 170/1000 | Loss: 0.00001692
Iteration 171/1000 | Loss: 0.00001692
Iteration 172/1000 | Loss: 0.00001692
Iteration 173/1000 | Loss: 0.00001692
Iteration 174/1000 | Loss: 0.00001692
Iteration 175/1000 | Loss: 0.00001692
Iteration 176/1000 | Loss: 0.00001692
Iteration 177/1000 | Loss: 0.00001692
Iteration 178/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.6920395864872262e-05, 1.6920395864872262e-05, 1.6920395864872262e-05, 1.6920395864872262e-05, 1.6920395864872262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6920395864872262e-05

Optimization complete. Final v2v error: 3.532607316970825 mm

Highest mean error: 3.82515287399292 mm for frame 14

Lowest mean error: 3.3322994709014893 mm for frame 54

Saving results

Total time: 40.345569372177124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839628
Iteration 2/25 | Loss: 0.00104275
Iteration 3/25 | Loss: 0.00086430
Iteration 4/25 | Loss: 0.00082512
Iteration 5/25 | Loss: 0.00081244
Iteration 6/25 | Loss: 0.00081067
Iteration 7/25 | Loss: 0.00081063
Iteration 8/25 | Loss: 0.00081063
Iteration 9/25 | Loss: 0.00081063
Iteration 10/25 | Loss: 0.00081063
Iteration 11/25 | Loss: 0.00081063
Iteration 12/25 | Loss: 0.00081063
Iteration 13/25 | Loss: 0.00081063
Iteration 14/25 | Loss: 0.00081063
Iteration 15/25 | Loss: 0.00081063
Iteration 16/25 | Loss: 0.00081063
Iteration 17/25 | Loss: 0.00081063
Iteration 18/25 | Loss: 0.00081063
Iteration 19/25 | Loss: 0.00081063
Iteration 20/25 | Loss: 0.00081063
Iteration 21/25 | Loss: 0.00081063
Iteration 22/25 | Loss: 0.00081063
Iteration 23/25 | Loss: 0.00081063
Iteration 24/25 | Loss: 0.00081063
Iteration 25/25 | Loss: 0.00081063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62076378
Iteration 2/25 | Loss: 0.00041673
Iteration 3/25 | Loss: 0.00041673
Iteration 4/25 | Loss: 0.00041673
Iteration 5/25 | Loss: 0.00041673
Iteration 6/25 | Loss: 0.00041673
Iteration 7/25 | Loss: 0.00041673
Iteration 8/25 | Loss: 0.00041673
Iteration 9/25 | Loss: 0.00041673
Iteration 10/25 | Loss: 0.00041673
Iteration 11/25 | Loss: 0.00041673
Iteration 12/25 | Loss: 0.00041673
Iteration 13/25 | Loss: 0.00041673
Iteration 14/25 | Loss: 0.00041673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00041672895895317197, 0.00041672895895317197, 0.00041672895895317197, 0.00041672895895317197, 0.00041672895895317197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041672895895317197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041673
Iteration 2/1000 | Loss: 0.00004960
Iteration 3/1000 | Loss: 0.00003575
Iteration 4/1000 | Loss: 0.00003296
Iteration 5/1000 | Loss: 0.00003140
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00002982
Iteration 8/1000 | Loss: 0.00002902
Iteration 9/1000 | Loss: 0.00002868
Iteration 10/1000 | Loss: 0.00002846
Iteration 11/1000 | Loss: 0.00002837
Iteration 12/1000 | Loss: 0.00002825
Iteration 13/1000 | Loss: 0.00002821
Iteration 14/1000 | Loss: 0.00002821
Iteration 15/1000 | Loss: 0.00002821
Iteration 16/1000 | Loss: 0.00002821
Iteration 17/1000 | Loss: 0.00002821
Iteration 18/1000 | Loss: 0.00002821
Iteration 19/1000 | Loss: 0.00002821
Iteration 20/1000 | Loss: 0.00002821
Iteration 21/1000 | Loss: 0.00002821
Iteration 22/1000 | Loss: 0.00002820
Iteration 23/1000 | Loss: 0.00002820
Iteration 24/1000 | Loss: 0.00002820
Iteration 25/1000 | Loss: 0.00002820
Iteration 26/1000 | Loss: 0.00002820
Iteration 27/1000 | Loss: 0.00002820
Iteration 28/1000 | Loss: 0.00002820
Iteration 29/1000 | Loss: 0.00002820
Iteration 30/1000 | Loss: 0.00002819
Iteration 31/1000 | Loss: 0.00002819
Iteration 32/1000 | Loss: 0.00002818
Iteration 33/1000 | Loss: 0.00002817
Iteration 34/1000 | Loss: 0.00002817
Iteration 35/1000 | Loss: 0.00002817
Iteration 36/1000 | Loss: 0.00002817
Iteration 37/1000 | Loss: 0.00002816
Iteration 38/1000 | Loss: 0.00002816
Iteration 39/1000 | Loss: 0.00002816
Iteration 40/1000 | Loss: 0.00002816
Iteration 41/1000 | Loss: 0.00002816
Iteration 42/1000 | Loss: 0.00002816
Iteration 43/1000 | Loss: 0.00002816
Iteration 44/1000 | Loss: 0.00002816
Iteration 45/1000 | Loss: 0.00002816
Iteration 46/1000 | Loss: 0.00002815
Iteration 47/1000 | Loss: 0.00002815
Iteration 48/1000 | Loss: 0.00002815
Iteration 49/1000 | Loss: 0.00002815
Iteration 50/1000 | Loss: 0.00002815
Iteration 51/1000 | Loss: 0.00002815
Iteration 52/1000 | Loss: 0.00002815
Iteration 53/1000 | Loss: 0.00002815
Iteration 54/1000 | Loss: 0.00002815
Iteration 55/1000 | Loss: 0.00002815
Iteration 56/1000 | Loss: 0.00002815
Iteration 57/1000 | Loss: 0.00002815
Iteration 58/1000 | Loss: 0.00002815
Iteration 59/1000 | Loss: 0.00002815
Iteration 60/1000 | Loss: 0.00002815
Iteration 61/1000 | Loss: 0.00002815
Iteration 62/1000 | Loss: 0.00002815
Iteration 63/1000 | Loss: 0.00002815
Iteration 64/1000 | Loss: 0.00002815
Iteration 65/1000 | Loss: 0.00002815
Iteration 66/1000 | Loss: 0.00002815
Iteration 67/1000 | Loss: 0.00002815
Iteration 68/1000 | Loss: 0.00002815
Iteration 69/1000 | Loss: 0.00002815
Iteration 70/1000 | Loss: 0.00002815
Iteration 71/1000 | Loss: 0.00002815
Iteration 72/1000 | Loss: 0.00002815
Iteration 73/1000 | Loss: 0.00002815
Iteration 74/1000 | Loss: 0.00002815
Iteration 75/1000 | Loss: 0.00002815
Iteration 76/1000 | Loss: 0.00002815
Iteration 77/1000 | Loss: 0.00002815
Iteration 78/1000 | Loss: 0.00002815
Iteration 79/1000 | Loss: 0.00002815
Iteration 80/1000 | Loss: 0.00002815
Iteration 81/1000 | Loss: 0.00002815
Iteration 82/1000 | Loss: 0.00002815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.8145472242613323e-05, 2.8145472242613323e-05, 2.8145472242613323e-05, 2.8145472242613323e-05, 2.8145472242613323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8145472242613323e-05

Optimization complete. Final v2v error: 4.546314716339111 mm

Highest mean error: 4.9972405433654785 mm for frame 62

Lowest mean error: 3.965226888656616 mm for frame 208

Saving results

Total time: 30.562692880630493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104439
Iteration 2/25 | Loss: 0.00146316
Iteration 3/25 | Loss: 0.00092315
Iteration 4/25 | Loss: 0.00097148
Iteration 5/25 | Loss: 0.00075153
Iteration 6/25 | Loss: 0.00075862
Iteration 7/25 | Loss: 0.00074261
Iteration 8/25 | Loss: 0.00072818
Iteration 9/25 | Loss: 0.00071135
Iteration 10/25 | Loss: 0.00070729
Iteration 11/25 | Loss: 0.00070273
Iteration 12/25 | Loss: 0.00069088
Iteration 13/25 | Loss: 0.00068320
Iteration 14/25 | Loss: 0.00067900
Iteration 15/25 | Loss: 0.00067452
Iteration 16/25 | Loss: 0.00066578
Iteration 17/25 | Loss: 0.00066357
Iteration 18/25 | Loss: 0.00066268
Iteration 19/25 | Loss: 0.00066235
Iteration 20/25 | Loss: 0.00066231
Iteration 21/25 | Loss: 0.00066231
Iteration 22/25 | Loss: 0.00066231
Iteration 23/25 | Loss: 0.00066231
Iteration 24/25 | Loss: 0.00066231
Iteration 25/25 | Loss: 0.00066231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48143339
Iteration 2/25 | Loss: 0.00054374
Iteration 3/25 | Loss: 0.00054374
Iteration 4/25 | Loss: 0.00054374
Iteration 5/25 | Loss: 0.00054374
Iteration 6/25 | Loss: 0.00054374
Iteration 7/25 | Loss: 0.00054374
Iteration 8/25 | Loss: 0.00054374
Iteration 9/25 | Loss: 0.00054374
Iteration 10/25 | Loss: 0.00054374
Iteration 11/25 | Loss: 0.00054374
Iteration 12/25 | Loss: 0.00054374
Iteration 13/25 | Loss: 0.00054374
Iteration 14/25 | Loss: 0.00054374
Iteration 15/25 | Loss: 0.00054374
Iteration 16/25 | Loss: 0.00054374
Iteration 17/25 | Loss: 0.00054374
Iteration 18/25 | Loss: 0.00054374
Iteration 19/25 | Loss: 0.00054374
Iteration 20/25 | Loss: 0.00054374
Iteration 21/25 | Loss: 0.00054374
Iteration 22/25 | Loss: 0.00054374
Iteration 23/25 | Loss: 0.00054374
Iteration 24/25 | Loss: 0.00054374
Iteration 25/25 | Loss: 0.00054374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054374
Iteration 2/1000 | Loss: 0.00006420
Iteration 3/1000 | Loss: 0.00004549
Iteration 4/1000 | Loss: 0.00003553
Iteration 5/1000 | Loss: 0.00003204
Iteration 6/1000 | Loss: 0.00002911
Iteration 7/1000 | Loss: 0.00034112
Iteration 8/1000 | Loss: 0.00084911
Iteration 9/1000 | Loss: 0.00004155
Iteration 10/1000 | Loss: 0.00002497
Iteration 11/1000 | Loss: 0.00001981
Iteration 12/1000 | Loss: 0.00001661
Iteration 13/1000 | Loss: 0.00001498
Iteration 14/1000 | Loss: 0.00001361
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001218
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001201
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001185
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001184
Iteration 37/1000 | Loss: 0.00001184
Iteration 38/1000 | Loss: 0.00001184
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001183
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001179
Iteration 53/1000 | Loss: 0.00001179
Iteration 54/1000 | Loss: 0.00001179
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001176
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.174796125269495e-05, 1.174796125269495e-05, 1.174796125269495e-05, 1.174796125269495e-05, 1.174796125269495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.174796125269495e-05

Optimization complete. Final v2v error: 2.9344046115875244 mm

Highest mean error: 3.630380630493164 mm for frame 94

Lowest mean error: 2.6690139770507812 mm for frame 2

Saving results

Total time: 67.53031349182129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402468
Iteration 2/25 | Loss: 0.00086141
Iteration 3/25 | Loss: 0.00072861
Iteration 4/25 | Loss: 0.00070055
Iteration 5/25 | Loss: 0.00068888
Iteration 6/25 | Loss: 0.00068661
Iteration 7/25 | Loss: 0.00068614
Iteration 8/25 | Loss: 0.00068614
Iteration 9/25 | Loss: 0.00068614
Iteration 10/25 | Loss: 0.00068614
Iteration 11/25 | Loss: 0.00068614
Iteration 12/25 | Loss: 0.00068614
Iteration 13/25 | Loss: 0.00068614
Iteration 14/25 | Loss: 0.00068614
Iteration 15/25 | Loss: 0.00068614
Iteration 16/25 | Loss: 0.00068614
Iteration 17/25 | Loss: 0.00068614
Iteration 18/25 | Loss: 0.00068614
Iteration 19/25 | Loss: 0.00068614
Iteration 20/25 | Loss: 0.00068614
Iteration 21/25 | Loss: 0.00068614
Iteration 22/25 | Loss: 0.00068614
Iteration 23/25 | Loss: 0.00068614
Iteration 24/25 | Loss: 0.00068614
Iteration 25/25 | Loss: 0.00068614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15350533
Iteration 2/25 | Loss: 0.00042015
Iteration 3/25 | Loss: 0.00042014
Iteration 4/25 | Loss: 0.00042014
Iteration 5/25 | Loss: 0.00042014
Iteration 6/25 | Loss: 0.00042014
Iteration 7/25 | Loss: 0.00042014
Iteration 8/25 | Loss: 0.00042014
Iteration 9/25 | Loss: 0.00042014
Iteration 10/25 | Loss: 0.00042014
Iteration 11/25 | Loss: 0.00042014
Iteration 12/25 | Loss: 0.00042014
Iteration 13/25 | Loss: 0.00042014
Iteration 14/25 | Loss: 0.00042014
Iteration 15/25 | Loss: 0.00042014
Iteration 16/25 | Loss: 0.00042014
Iteration 17/25 | Loss: 0.00042014
Iteration 18/25 | Loss: 0.00042014
Iteration 19/25 | Loss: 0.00042014
Iteration 20/25 | Loss: 0.00042014
Iteration 21/25 | Loss: 0.00042014
Iteration 22/25 | Loss: 0.00042014
Iteration 23/25 | Loss: 0.00042014
Iteration 24/25 | Loss: 0.00042014
Iteration 25/25 | Loss: 0.00042014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042014
Iteration 2/1000 | Loss: 0.00004371
Iteration 3/1000 | Loss: 0.00002656
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002097
Iteration 8/1000 | Loss: 0.00002043
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001969
Iteration 13/1000 | Loss: 0.00001968
Iteration 14/1000 | Loss: 0.00001968
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001963
Iteration 17/1000 | Loss: 0.00001960
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001955
Iteration 20/1000 | Loss: 0.00001951
Iteration 21/1000 | Loss: 0.00001951
Iteration 22/1000 | Loss: 0.00001949
Iteration 23/1000 | Loss: 0.00001949
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001942
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001940
Iteration 31/1000 | Loss: 0.00001940
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001936
Iteration 35/1000 | Loss: 0.00001936
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001935
Iteration 41/1000 | Loss: 0.00001935
Iteration 42/1000 | Loss: 0.00001935
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001934
Iteration 45/1000 | Loss: 0.00001934
Iteration 46/1000 | Loss: 0.00001934
Iteration 47/1000 | Loss: 0.00001934
Iteration 48/1000 | Loss: 0.00001934
Iteration 49/1000 | Loss: 0.00001934
Iteration 50/1000 | Loss: 0.00001933
Iteration 51/1000 | Loss: 0.00001933
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001933
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001932
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001931
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001929
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001928
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00001928
Iteration 93/1000 | Loss: 0.00001928
Iteration 94/1000 | Loss: 0.00001928
Iteration 95/1000 | Loss: 0.00001928
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001927
Iteration 98/1000 | Loss: 0.00001927
Iteration 99/1000 | Loss: 0.00001927
Iteration 100/1000 | Loss: 0.00001927
Iteration 101/1000 | Loss: 0.00001927
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001926
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001925
Iteration 115/1000 | Loss: 0.00001924
Iteration 116/1000 | Loss: 0.00001924
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001924
Iteration 121/1000 | Loss: 0.00001924
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001923
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00001923
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001922
Iteration 134/1000 | Loss: 0.00001922
Iteration 135/1000 | Loss: 0.00001922
Iteration 136/1000 | Loss: 0.00001922
Iteration 137/1000 | Loss: 0.00001922
Iteration 138/1000 | Loss: 0.00001922
Iteration 139/1000 | Loss: 0.00001922
Iteration 140/1000 | Loss: 0.00001922
Iteration 141/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.922369483509101e-05, 1.922369483509101e-05, 1.922369483509101e-05, 1.922369483509101e-05, 1.922369483509101e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.922369483509101e-05

Optimization complete. Final v2v error: 3.712244749069214 mm

Highest mean error: 4.133913993835449 mm for frame 43

Lowest mean error: 3.375908374786377 mm for frame 1

Saving results

Total time: 37.82854962348938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048037
Iteration 2/25 | Loss: 0.00426178
Iteration 3/25 | Loss: 0.00308715
Iteration 4/25 | Loss: 0.00227457
Iteration 5/25 | Loss: 0.00235266
Iteration 6/25 | Loss: 0.00245632
Iteration 7/25 | Loss: 0.00209326
Iteration 8/25 | Loss: 0.00198933
Iteration 9/25 | Loss: 0.00181537
Iteration 10/25 | Loss: 0.00178300
Iteration 11/25 | Loss: 0.00178271
Iteration 12/25 | Loss: 0.00176985
Iteration 13/25 | Loss: 0.00170675
Iteration 14/25 | Loss: 0.00166589
Iteration 15/25 | Loss: 0.00169639
Iteration 16/25 | Loss: 0.00169201
Iteration 17/25 | Loss: 0.00162602
Iteration 18/25 | Loss: 0.00163911
Iteration 19/25 | Loss: 0.00166053
Iteration 20/25 | Loss: 0.00170331
Iteration 21/25 | Loss: 0.00162902
Iteration 22/25 | Loss: 0.00163744
Iteration 23/25 | Loss: 0.00168783
Iteration 24/25 | Loss: 0.00165858
Iteration 25/25 | Loss: 0.00165178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25995398
Iteration 2/25 | Loss: 0.02389534
Iteration 3/25 | Loss: 0.02001410
Iteration 4/25 | Loss: 0.01669609
Iteration 5/25 | Loss: 0.01573136
Iteration 6/25 | Loss: 0.01573132
Iteration 7/25 | Loss: 0.01573131
Iteration 8/25 | Loss: 0.01573131
Iteration 9/25 | Loss: 0.01573131
Iteration 10/25 | Loss: 0.01573131
Iteration 11/25 | Loss: 0.01573131
Iteration 12/25 | Loss: 0.01573131
Iteration 13/25 | Loss: 0.01573131
Iteration 14/25 | Loss: 0.01573131
Iteration 15/25 | Loss: 0.01573131
Iteration 16/25 | Loss: 0.01573131
Iteration 17/25 | Loss: 0.01573131
Iteration 18/25 | Loss: 0.01573131
Iteration 19/25 | Loss: 0.01573131
Iteration 20/25 | Loss: 0.01573131
Iteration 21/25 | Loss: 0.01573131
Iteration 22/25 | Loss: 0.01573131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.01573130674660206, 0.01573130674660206, 0.01573130674660206, 0.01573130674660206, 0.01573130674660206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.01573130674660206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01573131
Iteration 2/1000 | Loss: 0.01740459
Iteration 3/1000 | Loss: 0.01018487
Iteration 4/1000 | Loss: 0.01330393
Iteration 5/1000 | Loss: 0.01387480
Iteration 6/1000 | Loss: 0.02246858
Iteration 7/1000 | Loss: 0.02144376
Iteration 8/1000 | Loss: 0.01963286
Iteration 9/1000 | Loss: 0.02651452
Iteration 10/1000 | Loss: 0.01446930
Iteration 11/1000 | Loss: 0.01305333
Iteration 12/1000 | Loss: 0.02080703
Iteration 13/1000 | Loss: 0.01653825
Iteration 14/1000 | Loss: 0.02205624
Iteration 15/1000 | Loss: 0.01457219
Iteration 16/1000 | Loss: 0.01806001
Iteration 17/1000 | Loss: 0.01519681
Iteration 18/1000 | Loss: 0.02062443
Iteration 19/1000 | Loss: 0.01415579
Iteration 20/1000 | Loss: 0.01286554
Iteration 21/1000 | Loss: 0.01071665
Iteration 22/1000 | Loss: 0.01730798
Iteration 23/1000 | Loss: 0.01173582
Iteration 24/1000 | Loss: 0.01978063
Iteration 25/1000 | Loss: 0.01226834
Iteration 26/1000 | Loss: 0.02410867
Iteration 27/1000 | Loss: 0.02307863
Iteration 28/1000 | Loss: 0.02281713
Iteration 29/1000 | Loss: 0.01619127
Iteration 30/1000 | Loss: 0.01523116
Iteration 31/1000 | Loss: 0.01493185
Iteration 32/1000 | Loss: 0.01774391
Iteration 33/1000 | Loss: 0.01605031
Iteration 34/1000 | Loss: 0.01637885
Iteration 35/1000 | Loss: 0.01459630
Iteration 36/1000 | Loss: 0.02046841
Iteration 37/1000 | Loss: 0.01799458
Iteration 38/1000 | Loss: 0.01376951
Iteration 39/1000 | Loss: 0.01615772
Iteration 40/1000 | Loss: 0.01350999
Iteration 41/1000 | Loss: 0.01079069
Iteration 42/1000 | Loss: 0.01235766
Iteration 43/1000 | Loss: 0.02401338
Iteration 44/1000 | Loss: 0.01649388
Iteration 45/1000 | Loss: 0.01549222
Iteration 46/1000 | Loss: 0.01005988
Iteration 47/1000 | Loss: 0.01403478
Iteration 48/1000 | Loss: 0.01069629
Iteration 49/1000 | Loss: 0.01472298
Iteration 50/1000 | Loss: 0.01265192
Iteration 51/1000 | Loss: 0.01983776
Iteration 52/1000 | Loss: 0.01731357
Iteration 53/1000 | Loss: 0.01576939
Iteration 54/1000 | Loss: 0.01579947
Iteration 55/1000 | Loss: 0.01386207
Iteration 56/1000 | Loss: 0.00996599
Iteration 57/1000 | Loss: 0.01514524
Iteration 58/1000 | Loss: 0.01113192
Iteration 59/1000 | Loss: 0.01402022
Iteration 60/1000 | Loss: 0.01480416
Iteration 61/1000 | Loss: 0.01198445
Iteration 62/1000 | Loss: 0.01955256
Iteration 63/1000 | Loss: 0.01949906
Iteration 64/1000 | Loss: 0.02290391
Iteration 65/1000 | Loss: 0.01547454
Iteration 66/1000 | Loss: 0.02083251
Iteration 67/1000 | Loss: 0.01569882
Iteration 68/1000 | Loss: 0.01695613
Iteration 69/1000 | Loss: 0.02164184
Iteration 70/1000 | Loss: 0.01310081
Iteration 71/1000 | Loss: 0.01386283
Iteration 72/1000 | Loss: 0.01416936
Iteration 73/1000 | Loss: 0.01519539
Iteration 74/1000 | Loss: 0.01380955
Iteration 75/1000 | Loss: 0.01099684
Iteration 76/1000 | Loss: 0.01001536
Iteration 77/1000 | Loss: 0.01278153
Iteration 78/1000 | Loss: 0.01131918
Iteration 79/1000 | Loss: 0.01408966
Iteration 80/1000 | Loss: 0.01086384
Iteration 81/1000 | Loss: 0.02786857
Iteration 82/1000 | Loss: 0.01330042
Iteration 83/1000 | Loss: 0.02117448
Iteration 84/1000 | Loss: 0.01258221
Iteration 85/1000 | Loss: 0.01174386
Iteration 86/1000 | Loss: 0.01261239
Iteration 87/1000 | Loss: 0.01257693
Iteration 88/1000 | Loss: 0.01662371
Iteration 89/1000 | Loss: 0.01380118
Iteration 90/1000 | Loss: 0.01198747
Iteration 91/1000 | Loss: 0.00861796
Iteration 92/1000 | Loss: 0.01677414
Iteration 93/1000 | Loss: 0.01344923
Iteration 94/1000 | Loss: 0.01086770
Iteration 95/1000 | Loss: 0.00961446
Iteration 96/1000 | Loss: 0.01037933
Iteration 97/1000 | Loss: 0.01453843
Iteration 98/1000 | Loss: 0.01097048
Iteration 99/1000 | Loss: 0.00839868
Iteration 100/1000 | Loss: 0.00943403
Iteration 101/1000 | Loss: 0.00973504
Iteration 102/1000 | Loss: 0.02258238
Iteration 103/1000 | Loss: 0.01366904
Iteration 104/1000 | Loss: 0.01160549
Iteration 105/1000 | Loss: 0.01137332
Iteration 106/1000 | Loss: 0.01212284
Iteration 107/1000 | Loss: 0.01249362
Iteration 108/1000 | Loss: 0.01426660
Iteration 109/1000 | Loss: 0.01183670
Iteration 110/1000 | Loss: 0.00962700
Iteration 111/1000 | Loss: 0.00935979
Iteration 112/1000 | Loss: 0.00976634
Iteration 113/1000 | Loss: 0.01452623
Iteration 114/1000 | Loss: 0.01833629
Iteration 115/1000 | Loss: 0.01492825
Iteration 116/1000 | Loss: 0.00998893
Iteration 117/1000 | Loss: 0.01521609
Iteration 118/1000 | Loss: 0.01846732
Iteration 119/1000 | Loss: 0.01704884
Iteration 120/1000 | Loss: 0.02010822
Iteration 121/1000 | Loss: 0.01532772
Iteration 122/1000 | Loss: 0.01406028
Iteration 123/1000 | Loss: 0.01334054
Iteration 124/1000 | Loss: 0.01776932
Iteration 125/1000 | Loss: 0.01358476
Iteration 126/1000 | Loss: 0.01225029
Iteration 127/1000 | Loss: 0.01011208
Iteration 128/1000 | Loss: 0.01638554
Iteration 129/1000 | Loss: 0.00822571
Iteration 130/1000 | Loss: 0.01853423
Iteration 131/1000 | Loss: 0.01401116
Iteration 132/1000 | Loss: 0.00974207
Iteration 133/1000 | Loss: 0.01363074
Iteration 134/1000 | Loss: 0.01656184
Iteration 135/1000 | Loss: 0.01494743
Iteration 136/1000 | Loss: 0.01665768
Iteration 137/1000 | Loss: 0.01728672
Iteration 138/1000 | Loss: 0.01199044
Iteration 139/1000 | Loss: 0.01333528
Iteration 140/1000 | Loss: 0.01065034
Iteration 141/1000 | Loss: 0.01609412
Iteration 142/1000 | Loss: 0.01208676
Iteration 143/1000 | Loss: 0.01250454
Iteration 144/1000 | Loss: 0.00985474
Iteration 145/1000 | Loss: 0.01807665
Iteration 146/1000 | Loss: 0.01697119
Iteration 147/1000 | Loss: 0.01779912
Iteration 148/1000 | Loss: 0.01231806
Iteration 149/1000 | Loss: 0.01607290
Iteration 150/1000 | Loss: 0.01191447
Iteration 151/1000 | Loss: 0.01060426
Iteration 152/1000 | Loss: 0.00987525
Iteration 153/1000 | Loss: 0.00996495
Iteration 154/1000 | Loss: 0.01611271
Iteration 155/1000 | Loss: 0.00967806
Iteration 156/1000 | Loss: 0.01089189
Iteration 157/1000 | Loss: 0.01387008
Iteration 158/1000 | Loss: 0.01599537
Iteration 159/1000 | Loss: 0.01661346
Iteration 160/1000 | Loss: 0.01818684
Iteration 161/1000 | Loss: 0.01301391
Iteration 162/1000 | Loss: 0.01081604
Iteration 163/1000 | Loss: 0.01194939
Iteration 164/1000 | Loss: 0.00916605
Iteration 165/1000 | Loss: 0.02154225
Iteration 166/1000 | Loss: 0.01558813
Iteration 167/1000 | Loss: 0.01212856
Iteration 168/1000 | Loss: 0.01068786
Iteration 169/1000 | Loss: 0.01139779
Iteration 170/1000 | Loss: 0.00935013
Iteration 171/1000 | Loss: 0.01165388
Iteration 172/1000 | Loss: 0.00830366
Iteration 173/1000 | Loss: 0.00965174
Iteration 174/1000 | Loss: 0.00959217
Iteration 175/1000 | Loss: 0.00999709
Iteration 176/1000 | Loss: 0.00853524
Iteration 177/1000 | Loss: 0.00961937
Iteration 178/1000 | Loss: 0.01149814
Iteration 179/1000 | Loss: 0.01760902
Iteration 180/1000 | Loss: 0.01686179
Iteration 181/1000 | Loss: 0.01740692
Iteration 182/1000 | Loss: 0.01743668
Iteration 183/1000 | Loss: 0.01406515
Iteration 184/1000 | Loss: 0.01053392
Iteration 185/1000 | Loss: 0.00979612
Iteration 186/1000 | Loss: 0.00861507
Iteration 187/1000 | Loss: 0.01207875
Iteration 188/1000 | Loss: 0.00863950
Iteration 189/1000 | Loss: 0.01774535
Iteration 190/1000 | Loss: 0.01052897
Iteration 191/1000 | Loss: 0.01558786
Iteration 192/1000 | Loss: 0.01374806
Iteration 193/1000 | Loss: 0.01313034
Iteration 194/1000 | Loss: 0.01051338
Iteration 195/1000 | Loss: 0.01128720
Iteration 196/1000 | Loss: 0.01181384
Iteration 197/1000 | Loss: 0.00952230
Iteration 198/1000 | Loss: 0.01019617
Iteration 199/1000 | Loss: 0.00941277
Iteration 200/1000 | Loss: 0.00839289
Iteration 201/1000 | Loss: 0.00588232
Iteration 202/1000 | Loss: 0.01586174
Iteration 203/1000 | Loss: 0.00780195
Iteration 204/1000 | Loss: 0.00707062
Iteration 205/1000 | Loss: 0.00687795
Iteration 206/1000 | Loss: 0.00708273
Iteration 207/1000 | Loss: 0.01029768
Iteration 208/1000 | Loss: 0.00731366
Iteration 209/1000 | Loss: 0.00654895
Iteration 210/1000 | Loss: 0.00749421
Iteration 211/1000 | Loss: 0.00593966
Iteration 212/1000 | Loss: 0.01578581
Iteration 213/1000 | Loss: 0.00995459
Iteration 214/1000 | Loss: 0.00918237
Iteration 215/1000 | Loss: 0.00875535
Iteration 216/1000 | Loss: 0.01144360
Iteration 217/1000 | Loss: 0.00825627
Iteration 218/1000 | Loss: 0.00799984
Iteration 219/1000 | Loss: 0.00646144
Iteration 220/1000 | Loss: 0.01002473
Iteration 221/1000 | Loss: 0.00650065
Iteration 222/1000 | Loss: 0.00625817
Iteration 223/1000 | Loss: 0.00842889
Iteration 224/1000 | Loss: 0.01071723
Iteration 225/1000 | Loss: 0.00732044
Iteration 226/1000 | Loss: 0.00760004
Iteration 227/1000 | Loss: 0.00613248
Iteration 228/1000 | Loss: 0.00677739
Iteration 229/1000 | Loss: 0.00704463
Iteration 230/1000 | Loss: 0.01477022
Iteration 231/1000 | Loss: 0.01480962
Iteration 232/1000 | Loss: 0.00730985
Iteration 233/1000 | Loss: 0.00674586
Iteration 234/1000 | Loss: 0.00821261
Iteration 235/1000 | Loss: 0.00769754
Iteration 236/1000 | Loss: 0.01215275
Iteration 237/1000 | Loss: 0.01007680
Iteration 238/1000 | Loss: 0.00871971
Iteration 239/1000 | Loss: 0.00821582
Iteration 240/1000 | Loss: 0.01323029
Iteration 241/1000 | Loss: 0.00855308
Iteration 242/1000 | Loss: 0.01111111
Iteration 243/1000 | Loss: 0.01109596
Iteration 244/1000 | Loss: 0.01416617
Iteration 245/1000 | Loss: 0.00997728
Iteration 246/1000 | Loss: 0.01157133
Iteration 247/1000 | Loss: 0.00664044
Iteration 248/1000 | Loss: 0.00695656
Iteration 249/1000 | Loss: 0.00724312
Iteration 250/1000 | Loss: 0.01060346
Iteration 251/1000 | Loss: 0.00753817
Iteration 252/1000 | Loss: 0.00646591
Iteration 253/1000 | Loss: 0.00627987
Iteration 254/1000 | Loss: 0.01972468
Iteration 255/1000 | Loss: 0.00642655
Iteration 256/1000 | Loss: 0.01219629
Iteration 257/1000 | Loss: 0.00852290
Iteration 258/1000 | Loss: 0.01160286
Iteration 259/1000 | Loss: 0.00710288
Iteration 260/1000 | Loss: 0.00908276
Iteration 261/1000 | Loss: 0.00834247
Iteration 262/1000 | Loss: 0.01069230
Iteration 263/1000 | Loss: 0.01200548
Iteration 264/1000 | Loss: 0.00679067
Iteration 265/1000 | Loss: 0.00630318
Iteration 266/1000 | Loss: 0.00632783
Iteration 267/1000 | Loss: 0.01505840
Iteration 268/1000 | Loss: 0.00796384
Iteration 269/1000 | Loss: 0.00812047
Iteration 270/1000 | Loss: 0.00870953
Iteration 271/1000 | Loss: 0.01120387
Iteration 272/1000 | Loss: 0.00714118
Iteration 273/1000 | Loss: 0.00877377
Iteration 274/1000 | Loss: 0.00874330
Iteration 275/1000 | Loss: 0.00867114
Iteration 276/1000 | Loss: 0.00716199
Iteration 277/1000 | Loss: 0.00711466
Iteration 278/1000 | Loss: 0.00689069
Iteration 279/1000 | Loss: 0.00664611
Iteration 280/1000 | Loss: 0.00741726
Iteration 281/1000 | Loss: 0.00813095
Iteration 282/1000 | Loss: 0.01235820
Iteration 283/1000 | Loss: 0.01122827
Iteration 284/1000 | Loss: 0.00585442
Iteration 285/1000 | Loss: 0.00884355
Iteration 286/1000 | Loss: 0.00602161
Iteration 287/1000 | Loss: 0.01177535
Iteration 288/1000 | Loss: 0.01079838
Iteration 289/1000 | Loss: 0.00669056
Iteration 290/1000 | Loss: 0.00721026
Iteration 291/1000 | Loss: 0.00504775
Iteration 292/1000 | Loss: 0.00571037
Iteration 293/1000 | Loss: 0.00563562
Iteration 294/1000 | Loss: 0.00744522
Iteration 295/1000 | Loss: 0.00681646
Iteration 296/1000 | Loss: 0.00639131
Iteration 297/1000 | Loss: 0.00589912
Iteration 298/1000 | Loss: 0.00932278
Iteration 299/1000 | Loss: 0.00636936
Iteration 300/1000 | Loss: 0.00882211
Iteration 301/1000 | Loss: 0.00726394
Iteration 302/1000 | Loss: 0.00921219
Iteration 303/1000 | Loss: 0.01098590
Iteration 304/1000 | Loss: 0.00690894
Iteration 305/1000 | Loss: 0.01323729
Iteration 306/1000 | Loss: 0.00769999
Iteration 307/1000 | Loss: 0.01065504
Iteration 308/1000 | Loss: 0.00922068
Iteration 309/1000 | Loss: 0.00805380
Iteration 310/1000 | Loss: 0.01071430
Iteration 311/1000 | Loss: 0.01758635
Iteration 312/1000 | Loss: 0.00752605
Iteration 313/1000 | Loss: 0.00654388
Iteration 314/1000 | Loss: 0.00601672
Iteration 315/1000 | Loss: 0.00631588
Iteration 316/1000 | Loss: 0.00747903
Iteration 317/1000 | Loss: 0.00699974
Iteration 318/1000 | Loss: 0.00704520
Iteration 319/1000 | Loss: 0.00720312
Iteration 320/1000 | Loss: 0.00598746
Iteration 321/1000 | Loss: 0.00536305
Iteration 322/1000 | Loss: 0.01654694
Iteration 323/1000 | Loss: 0.01362414
Iteration 324/1000 | Loss: 0.00792232
Iteration 325/1000 | Loss: 0.01093525
Iteration 326/1000 | Loss: 0.00664738
Iteration 327/1000 | Loss: 0.00463612
Iteration 328/1000 | Loss: 0.00840907
Iteration 329/1000 | Loss: 0.01064016
Iteration 330/1000 | Loss: 0.00751161
Iteration 331/1000 | Loss: 0.00581292
Iteration 332/1000 | Loss: 0.00751652
Iteration 333/1000 | Loss: 0.00517689
Iteration 334/1000 | Loss: 0.00743062
Iteration 335/1000 | Loss: 0.00970066
Iteration 336/1000 | Loss: 0.00861512
Iteration 337/1000 | Loss: 0.00881817
Iteration 338/1000 | Loss: 0.00987369
Iteration 339/1000 | Loss: 0.00796172
Iteration 340/1000 | Loss: 0.00445335
Iteration 341/1000 | Loss: 0.01072115
Iteration 342/1000 | Loss: 0.00540814
Iteration 343/1000 | Loss: 0.00870454
Iteration 344/1000 | Loss: 0.01289884
Iteration 345/1000 | Loss: 0.01160404
Iteration 346/1000 | Loss: 0.00927955
Iteration 347/1000 | Loss: 0.00800238
Iteration 348/1000 | Loss: 0.00778958
Iteration 349/1000 | Loss: 0.00895432
Iteration 350/1000 | Loss: 0.01064201
Iteration 351/1000 | Loss: 0.00755606
Iteration 352/1000 | Loss: 0.00786495
Iteration 353/1000 | Loss: 0.00668936
Iteration 354/1000 | Loss: 0.00851738
Iteration 355/1000 | Loss: 0.00520024
Iteration 356/1000 | Loss: 0.00601869
Iteration 357/1000 | Loss: 0.00525688
Iteration 358/1000 | Loss: 0.00711799
Iteration 359/1000 | Loss: 0.00474661
Iteration 360/1000 | Loss: 0.00531422
Iteration 361/1000 | Loss: 0.00664015
Iteration 362/1000 | Loss: 0.00708777
Iteration 363/1000 | Loss: 0.00618253
Iteration 364/1000 | Loss: 0.01508670
Iteration 365/1000 | Loss: 0.00504140
Iteration 366/1000 | Loss: 0.00824619
Iteration 367/1000 | Loss: 0.00721718
Iteration 368/1000 | Loss: 0.00568273
Iteration 369/1000 | Loss: 0.00387260
Iteration 370/1000 | Loss: 0.00698975
Iteration 371/1000 | Loss: 0.01476486
Iteration 372/1000 | Loss: 0.00798937
Iteration 373/1000 | Loss: 0.00818277
Iteration 374/1000 | Loss: 0.00614228
Iteration 375/1000 | Loss: 0.00685601
Iteration 376/1000 | Loss: 0.00661734
Iteration 377/1000 | Loss: 0.00899799
Iteration 378/1000 | Loss: 0.00513155
Iteration 379/1000 | Loss: 0.00609053
Iteration 380/1000 | Loss: 0.00546064
Iteration 381/1000 | Loss: 0.00444550
Iteration 382/1000 | Loss: 0.00427023
Iteration 383/1000 | Loss: 0.00684959
Iteration 384/1000 | Loss: 0.00771683
Iteration 385/1000 | Loss: 0.00583166
Iteration 386/1000 | Loss: 0.01013009
Iteration 387/1000 | Loss: 0.00509786
Iteration 388/1000 | Loss: 0.00510083
Iteration 389/1000 | Loss: 0.00612319
Iteration 390/1000 | Loss: 0.00604130
Iteration 391/1000 | Loss: 0.00645634
Iteration 392/1000 | Loss: 0.00776224
Iteration 393/1000 | Loss: 0.00724240
Iteration 394/1000 | Loss: 0.00467928
Iteration 395/1000 | Loss: 0.00392830
Iteration 396/1000 | Loss: 0.00487421
Iteration 397/1000 | Loss: 0.00314305
Iteration 398/1000 | Loss: 0.00279999
Iteration 399/1000 | Loss: 0.00347906
Iteration 400/1000 | Loss: 0.00249370
Iteration 401/1000 | Loss: 0.00452929
Iteration 402/1000 | Loss: 0.00358741
Iteration 403/1000 | Loss: 0.00332865
Iteration 404/1000 | Loss: 0.00340559
Iteration 405/1000 | Loss: 0.00243596
Iteration 406/1000 | Loss: 0.00393903
Iteration 407/1000 | Loss: 0.00614737
Iteration 408/1000 | Loss: 0.00519132
Iteration 409/1000 | Loss: 0.00193750
Iteration 410/1000 | Loss: 0.00253112
Iteration 411/1000 | Loss: 0.00240447
Iteration 412/1000 | Loss: 0.00419374
Iteration 413/1000 | Loss: 0.00240970
Iteration 414/1000 | Loss: 0.00216033
Iteration 415/1000 | Loss: 0.00275272
Iteration 416/1000 | Loss: 0.00427035
Iteration 417/1000 | Loss: 0.00299972
Iteration 418/1000 | Loss: 0.00231799
Iteration 419/1000 | Loss: 0.00257357
Iteration 420/1000 | Loss: 0.00283549
Iteration 421/1000 | Loss: 0.00246295
Iteration 422/1000 | Loss: 0.00373161
Iteration 423/1000 | Loss: 0.00236195
Iteration 424/1000 | Loss: 0.00154351
Iteration 425/1000 | Loss: 0.00199211
Iteration 426/1000 | Loss: 0.00175898
Iteration 427/1000 | Loss: 0.00089915
Iteration 428/1000 | Loss: 0.00184635
Iteration 429/1000 | Loss: 0.00132992
Iteration 430/1000 | Loss: 0.00137832
Iteration 431/1000 | Loss: 0.00296450
Iteration 432/1000 | Loss: 0.00155644
Iteration 433/1000 | Loss: 0.00194468
Iteration 434/1000 | Loss: 0.00211338
Iteration 435/1000 | Loss: 0.00338988
Iteration 436/1000 | Loss: 0.00100105
Iteration 437/1000 | Loss: 0.00059385
Iteration 438/1000 | Loss: 0.00054278
Iteration 439/1000 | Loss: 0.00231428
Iteration 440/1000 | Loss: 0.00132771
Iteration 441/1000 | Loss: 0.00360113
Iteration 442/1000 | Loss: 0.00163139
Iteration 443/1000 | Loss: 0.00121747
Iteration 444/1000 | Loss: 0.00114439
Iteration 445/1000 | Loss: 0.00099271
Iteration 446/1000 | Loss: 0.00111434
Iteration 447/1000 | Loss: 0.00222869
Iteration 448/1000 | Loss: 0.00200368
Iteration 449/1000 | Loss: 0.00191334
Iteration 450/1000 | Loss: 0.00218949
Iteration 451/1000 | Loss: 0.00252148
Iteration 452/1000 | Loss: 0.00259688
Iteration 453/1000 | Loss: 0.00359638
Iteration 454/1000 | Loss: 0.00354297
Iteration 455/1000 | Loss: 0.00400713
Iteration 456/1000 | Loss: 0.00390973
Iteration 457/1000 | Loss: 0.00385281
Iteration 458/1000 | Loss: 0.00364379
Iteration 459/1000 | Loss: 0.00346343
Iteration 460/1000 | Loss: 0.00572655
Iteration 461/1000 | Loss: 0.00313949
Iteration 462/1000 | Loss: 0.00172991
Iteration 463/1000 | Loss: 0.00172833
Iteration 464/1000 | Loss: 0.00068547
Iteration 465/1000 | Loss: 0.00093972
Iteration 466/1000 | Loss: 0.00064884
Iteration 467/1000 | Loss: 0.00062986
Iteration 468/1000 | Loss: 0.00101554
Iteration 469/1000 | Loss: 0.00083777
Iteration 470/1000 | Loss: 0.00077786
Iteration 471/1000 | Loss: 0.00052170
Iteration 472/1000 | Loss: 0.00032715
Iteration 473/1000 | Loss: 0.00092919
Iteration 474/1000 | Loss: 0.00110099
Iteration 475/1000 | Loss: 0.00061959
Iteration 476/1000 | Loss: 0.00029536
Iteration 477/1000 | Loss: 0.00140414
Iteration 478/1000 | Loss: 0.00045161
Iteration 479/1000 | Loss: 0.00075213
Iteration 480/1000 | Loss: 0.00062944
Iteration 481/1000 | Loss: 0.00042649
Iteration 482/1000 | Loss: 0.00102403
Iteration 483/1000 | Loss: 0.00100468
Iteration 484/1000 | Loss: 0.00110267
Iteration 485/1000 | Loss: 0.00090056
Iteration 486/1000 | Loss: 0.00167707
Iteration 487/1000 | Loss: 0.00076122
Iteration 488/1000 | Loss: 0.00081674
Iteration 489/1000 | Loss: 0.00098751
Iteration 490/1000 | Loss: 0.00082161
Iteration 491/1000 | Loss: 0.00111065
Iteration 492/1000 | Loss: 0.00090306
Iteration 493/1000 | Loss: 0.00208294
Iteration 494/1000 | Loss: 0.00101946
Iteration 495/1000 | Loss: 0.00063026
Iteration 496/1000 | Loss: 0.00252536
Iteration 497/1000 | Loss: 0.00117364
Iteration 498/1000 | Loss: 0.00076867
Iteration 499/1000 | Loss: 0.00096560
Iteration 500/1000 | Loss: 0.00097587
Iteration 501/1000 | Loss: 0.00087835
Iteration 502/1000 | Loss: 0.00082318
Iteration 503/1000 | Loss: 0.00074009
Iteration 504/1000 | Loss: 0.00074055
Iteration 505/1000 | Loss: 0.00007987
Iteration 506/1000 | Loss: 0.00109081
Iteration 507/1000 | Loss: 0.00090113
Iteration 508/1000 | Loss: 0.00051989
Iteration 509/1000 | Loss: 0.00047853
Iteration 510/1000 | Loss: 0.00091727
Iteration 511/1000 | Loss: 0.00048925
Iteration 512/1000 | Loss: 0.00132383
Iteration 513/1000 | Loss: 0.00054253
Iteration 514/1000 | Loss: 0.00042241
Iteration 515/1000 | Loss: 0.00011394
Iteration 516/1000 | Loss: 0.00006227
Iteration 517/1000 | Loss: 0.00012476
Iteration 518/1000 | Loss: 0.00202498
Iteration 519/1000 | Loss: 0.00017069
Iteration 520/1000 | Loss: 0.00004375
Iteration 521/1000 | Loss: 0.00004000
Iteration 522/1000 | Loss: 0.00013373
Iteration 523/1000 | Loss: 0.00013013
Iteration 524/1000 | Loss: 0.00013119
Iteration 525/1000 | Loss: 0.00015017
Iteration 526/1000 | Loss: 0.00012436
Iteration 527/1000 | Loss: 0.00014775
Iteration 528/1000 | Loss: 0.00012167
Iteration 529/1000 | Loss: 0.00186188
Iteration 530/1000 | Loss: 0.00014857
Iteration 531/1000 | Loss: 0.00053352
Iteration 532/1000 | Loss: 0.00023961
Iteration 533/1000 | Loss: 0.00177247
Iteration 534/1000 | Loss: 0.00024008
Iteration 535/1000 | Loss: 0.00021556
Iteration 536/1000 | Loss: 0.00290356
Iteration 537/1000 | Loss: 0.00014308
Iteration 538/1000 | Loss: 0.00012659
Iteration 539/1000 | Loss: 0.00093939
Iteration 540/1000 | Loss: 0.00020165
Iteration 541/1000 | Loss: 0.00150790
Iteration 542/1000 | Loss: 0.00010839
Iteration 543/1000 | Loss: 0.00008632
Iteration 544/1000 | Loss: 0.00017361
Iteration 545/1000 | Loss: 0.00015319
Iteration 546/1000 | Loss: 0.00178222
Iteration 547/1000 | Loss: 0.00005218
Iteration 548/1000 | Loss: 0.00006766
Iteration 549/1000 | Loss: 0.00006279
Iteration 550/1000 | Loss: 0.00014880
Iteration 551/1000 | Loss: 0.00011749
Iteration 552/1000 | Loss: 0.00010659
Iteration 553/1000 | Loss: 0.00004347
Iteration 554/1000 | Loss: 0.00006983
Iteration 555/1000 | Loss: 0.00005196
Iteration 556/1000 | Loss: 0.00003828
Iteration 557/1000 | Loss: 0.00003712
Iteration 558/1000 | Loss: 0.00003653
Iteration 559/1000 | Loss: 0.00057322
Iteration 560/1000 | Loss: 0.00007797
Iteration 561/1000 | Loss: 0.00012759
Iteration 562/1000 | Loss: 0.00008563
Iteration 563/1000 | Loss: 0.00011572
Iteration 564/1000 | Loss: 0.00108979
Iteration 565/1000 | Loss: 0.00073204
Iteration 566/1000 | Loss: 0.00005397
Iteration 567/1000 | Loss: 0.00006769
Iteration 568/1000 | Loss: 0.00010761
Iteration 569/1000 | Loss: 0.00006093
Iteration 570/1000 | Loss: 0.00009434
Iteration 571/1000 | Loss: 0.00004554
Iteration 572/1000 | Loss: 0.00003896
Iteration 573/1000 | Loss: 0.00012137
Iteration 574/1000 | Loss: 0.00007554
Iteration 575/1000 | Loss: 0.00002920
Iteration 576/1000 | Loss: 0.00002774
Iteration 577/1000 | Loss: 0.00002687
Iteration 578/1000 | Loss: 0.00002649
Iteration 579/1000 | Loss: 0.00002630
Iteration 580/1000 | Loss: 0.00002614
Iteration 581/1000 | Loss: 0.00002594
Iteration 582/1000 | Loss: 0.00002593
Iteration 583/1000 | Loss: 0.00002592
Iteration 584/1000 | Loss: 0.00002592
Iteration 585/1000 | Loss: 0.00002591
Iteration 586/1000 | Loss: 0.00002591
Iteration 587/1000 | Loss: 0.00002585
Iteration 588/1000 | Loss: 0.00002581
Iteration 589/1000 | Loss: 0.00002581
Iteration 590/1000 | Loss: 0.00002581
Iteration 591/1000 | Loss: 0.00002580
Iteration 592/1000 | Loss: 0.00002579
Iteration 593/1000 | Loss: 0.00002578
Iteration 594/1000 | Loss: 0.00002578
Iteration 595/1000 | Loss: 0.00002578
Iteration 596/1000 | Loss: 0.00002578
Iteration 597/1000 | Loss: 0.00002577
Iteration 598/1000 | Loss: 0.00002576
Iteration 599/1000 | Loss: 0.00002575
Iteration 600/1000 | Loss: 0.00002574
Iteration 601/1000 | Loss: 0.00002574
Iteration 602/1000 | Loss: 0.00002574
Iteration 603/1000 | Loss: 0.00002574
Iteration 604/1000 | Loss: 0.00002574
Iteration 605/1000 | Loss: 0.00002574
Iteration 606/1000 | Loss: 0.00002574
Iteration 607/1000 | Loss: 0.00002574
Iteration 608/1000 | Loss: 0.00002574
Iteration 609/1000 | Loss: 0.00002574
Iteration 610/1000 | Loss: 0.00002574
Iteration 611/1000 | Loss: 0.00002574
Iteration 612/1000 | Loss: 0.00002574
Iteration 613/1000 | Loss: 0.00013624
Iteration 614/1000 | Loss: 0.00013624
Iteration 615/1000 | Loss: 0.00011650
Iteration 616/1000 | Loss: 0.00014038
Iteration 617/1000 | Loss: 0.00022228
Iteration 618/1000 | Loss: 0.00058982
Iteration 619/1000 | Loss: 0.00005142
Iteration 620/1000 | Loss: 0.00037921
Iteration 621/1000 | Loss: 0.00017351
Iteration 622/1000 | Loss: 0.00018309
Iteration 623/1000 | Loss: 0.00005161
Iteration 624/1000 | Loss: 0.00003424
Iteration 625/1000 | Loss: 0.00002680
Iteration 626/1000 | Loss: 0.00002558
Iteration 627/1000 | Loss: 0.00002473
Iteration 628/1000 | Loss: 0.00002443
Iteration 629/1000 | Loss: 0.00002422
Iteration 630/1000 | Loss: 0.00002412
Iteration 631/1000 | Loss: 0.00002411
Iteration 632/1000 | Loss: 0.00002407
Iteration 633/1000 | Loss: 0.00002405
Iteration 634/1000 | Loss: 0.00002402
Iteration 635/1000 | Loss: 0.00002398
Iteration 636/1000 | Loss: 0.00002397
Iteration 637/1000 | Loss: 0.00002397
Iteration 638/1000 | Loss: 0.00002397
Iteration 639/1000 | Loss: 0.00002396
Iteration 640/1000 | Loss: 0.00002396
Iteration 641/1000 | Loss: 0.00002395
Iteration 642/1000 | Loss: 0.00002395
Iteration 643/1000 | Loss: 0.00002395
Iteration 644/1000 | Loss: 0.00002394
Iteration 645/1000 | Loss: 0.00002394
Iteration 646/1000 | Loss: 0.00002393
Iteration 647/1000 | Loss: 0.00002392
Iteration 648/1000 | Loss: 0.00002392
Iteration 649/1000 | Loss: 0.00002391
Iteration 650/1000 | Loss: 0.00002391
Iteration 651/1000 | Loss: 0.00002391
Iteration 652/1000 | Loss: 0.00002391
Iteration 653/1000 | Loss: 0.00002391
Iteration 654/1000 | Loss: 0.00002391
Iteration 655/1000 | Loss: 0.00002391
Iteration 656/1000 | Loss: 0.00002391
Iteration 657/1000 | Loss: 0.00002390
Iteration 658/1000 | Loss: 0.00002390
Iteration 659/1000 | Loss: 0.00002390
Iteration 660/1000 | Loss: 0.00002389
Iteration 661/1000 | Loss: 0.00002389
Iteration 662/1000 | Loss: 0.00002388
Iteration 663/1000 | Loss: 0.00002388
Iteration 664/1000 | Loss: 0.00002388
Iteration 665/1000 | Loss: 0.00002387
Iteration 666/1000 | Loss: 0.00002387
Iteration 667/1000 | Loss: 0.00002386
Iteration 668/1000 | Loss: 0.00002386
Iteration 669/1000 | Loss: 0.00002386
Iteration 670/1000 | Loss: 0.00002386
Iteration 671/1000 | Loss: 0.00002385
Iteration 672/1000 | Loss: 0.00002385
Iteration 673/1000 | Loss: 0.00002385
Iteration 674/1000 | Loss: 0.00002385
Iteration 675/1000 | Loss: 0.00002385
Iteration 676/1000 | Loss: 0.00002385
Iteration 677/1000 | Loss: 0.00002385
Iteration 678/1000 | Loss: 0.00002385
Iteration 679/1000 | Loss: 0.00002385
Iteration 680/1000 | Loss: 0.00002385
Iteration 681/1000 | Loss: 0.00002385
Iteration 682/1000 | Loss: 0.00002385
Iteration 683/1000 | Loss: 0.00002384
Iteration 684/1000 | Loss: 0.00002384
Iteration 685/1000 | Loss: 0.00002384
Iteration 686/1000 | Loss: 0.00002384
Iteration 687/1000 | Loss: 0.00002384
Iteration 688/1000 | Loss: 0.00002383
Iteration 689/1000 | Loss: 0.00002383
Iteration 690/1000 | Loss: 0.00002383
Iteration 691/1000 | Loss: 0.00002383
Iteration 692/1000 | Loss: 0.00002383
Iteration 693/1000 | Loss: 0.00002383
Iteration 694/1000 | Loss: 0.00002383
Iteration 695/1000 | Loss: 0.00002383
Iteration 696/1000 | Loss: 0.00002383
Iteration 697/1000 | Loss: 0.00002383
Iteration 698/1000 | Loss: 0.00002383
Iteration 699/1000 | Loss: 0.00002383
Iteration 700/1000 | Loss: 0.00002383
Iteration 701/1000 | Loss: 0.00002383
Iteration 702/1000 | Loss: 0.00002383
Iteration 703/1000 | Loss: 0.00002383
Iteration 704/1000 | Loss: 0.00002383
Iteration 705/1000 | Loss: 0.00002383
Iteration 706/1000 | Loss: 0.00002383
Iteration 707/1000 | Loss: 0.00002383
Iteration 708/1000 | Loss: 0.00002382
Iteration 709/1000 | Loss: 0.00002382
Iteration 710/1000 | Loss: 0.00002382
Iteration 711/1000 | Loss: 0.00002382
Iteration 712/1000 | Loss: 0.00002382
Iteration 713/1000 | Loss: 0.00002382
Iteration 714/1000 | Loss: 0.00002382
Iteration 715/1000 | Loss: 0.00002382
Iteration 716/1000 | Loss: 0.00002382
Iteration 717/1000 | Loss: 0.00002382
Iteration 718/1000 | Loss: 0.00002382
Iteration 719/1000 | Loss: 0.00002382
Iteration 720/1000 | Loss: 0.00002382
Iteration 721/1000 | Loss: 0.00002382
Iteration 722/1000 | Loss: 0.00002382
Iteration 723/1000 | Loss: 0.00002382
Iteration 724/1000 | Loss: 0.00002382
Iteration 725/1000 | Loss: 0.00002382
Iteration 726/1000 | Loss: 0.00002381
Iteration 727/1000 | Loss: 0.00002381
Iteration 728/1000 | Loss: 0.00002381
Iteration 729/1000 | Loss: 0.00002381
Iteration 730/1000 | Loss: 0.00002381
Iteration 731/1000 | Loss: 0.00002381
Iteration 732/1000 | Loss: 0.00002381
Iteration 733/1000 | Loss: 0.00002381
Iteration 734/1000 | Loss: 0.00002381
Iteration 735/1000 | Loss: 0.00002381
Iteration 736/1000 | Loss: 0.00002381
Iteration 737/1000 | Loss: 0.00002381
Iteration 738/1000 | Loss: 0.00002381
Iteration 739/1000 | Loss: 0.00002381
Iteration 740/1000 | Loss: 0.00002381
Iteration 741/1000 | Loss: 0.00002381
Iteration 742/1000 | Loss: 0.00002381
Iteration 743/1000 | Loss: 0.00002381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 743. Stopping optimization.
Last 5 losses: [2.3812188373995014e-05, 2.3812188373995014e-05, 2.3812188373995014e-05, 2.3812188373995014e-05, 2.3812188373995014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3812188373995014e-05

Optimization complete. Final v2v error: 4.175116062164307 mm

Highest mean error: 6.086767196655273 mm for frame 45

Lowest mean error: 3.3847885131835938 mm for frame 97

Saving results

Total time: 902.2277338504791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068806
Iteration 2/25 | Loss: 0.00130124
Iteration 3/25 | Loss: 0.00094479
Iteration 4/25 | Loss: 0.00081936
Iteration 5/25 | Loss: 0.00083449
Iteration 6/25 | Loss: 0.00081583
Iteration 7/25 | Loss: 0.00076564
Iteration 8/25 | Loss: 0.00074953
Iteration 9/25 | Loss: 0.00073950
Iteration 10/25 | Loss: 0.00073343
Iteration 11/25 | Loss: 0.00073075
Iteration 12/25 | Loss: 0.00073017
Iteration 13/25 | Loss: 0.00073035
Iteration 14/25 | Loss: 0.00073342
Iteration 15/25 | Loss: 0.00073231
Iteration 16/25 | Loss: 0.00073343
Iteration 17/25 | Loss: 0.00073185
Iteration 18/25 | Loss: 0.00072924
Iteration 19/25 | Loss: 0.00073091
Iteration 20/25 | Loss: 0.00073280
Iteration 21/25 | Loss: 0.00072948
Iteration 22/25 | Loss: 0.00073186
Iteration 23/25 | Loss: 0.00073041
Iteration 24/25 | Loss: 0.00073280
Iteration 25/25 | Loss: 0.00073159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.24105453
Iteration 2/25 | Loss: 0.00061872
Iteration 3/25 | Loss: 0.00061871
Iteration 4/25 | Loss: 0.00061871
Iteration 5/25 | Loss: 0.00061871
Iteration 6/25 | Loss: 0.00061871
Iteration 7/25 | Loss: 0.00061871
Iteration 8/25 | Loss: 0.00061871
Iteration 9/25 | Loss: 0.00061871
Iteration 10/25 | Loss: 0.00061871
Iteration 11/25 | Loss: 0.00061871
Iteration 12/25 | Loss: 0.00061871
Iteration 13/25 | Loss: 0.00061871
Iteration 14/25 | Loss: 0.00061871
Iteration 15/25 | Loss: 0.00061871
Iteration 16/25 | Loss: 0.00061871
Iteration 17/25 | Loss: 0.00061871
Iteration 18/25 | Loss: 0.00061871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006187116960063577, 0.0006187116960063577, 0.0006187116960063577, 0.0006187116960063577, 0.0006187116960063577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006187116960063577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061871
Iteration 2/1000 | Loss: 0.00007252
Iteration 3/1000 | Loss: 0.00019208
Iteration 4/1000 | Loss: 0.00003563
Iteration 5/1000 | Loss: 0.00009894
Iteration 6/1000 | Loss: 0.00011280
Iteration 7/1000 | Loss: 0.00006732
Iteration 8/1000 | Loss: 0.00012023
Iteration 9/1000 | Loss: 0.00010190
Iteration 10/1000 | Loss: 0.00018190
Iteration 11/1000 | Loss: 0.00004075
Iteration 12/1000 | Loss: 0.00009469
Iteration 13/1000 | Loss: 0.00009949
Iteration 14/1000 | Loss: 0.00017127
Iteration 15/1000 | Loss: 0.00015409
Iteration 16/1000 | Loss: 0.00011792
Iteration 17/1000 | Loss: 0.00002843
Iteration 18/1000 | Loss: 0.00007337
Iteration 19/1000 | Loss: 0.00016908
Iteration 20/1000 | Loss: 0.00010388
Iteration 21/1000 | Loss: 0.00009831
Iteration 22/1000 | Loss: 0.00020679
Iteration 23/1000 | Loss: 0.00015705
Iteration 24/1000 | Loss: 0.00008124
Iteration 25/1000 | Loss: 0.00012902
Iteration 26/1000 | Loss: 0.00058574
Iteration 27/1000 | Loss: 0.00060840
Iteration 28/1000 | Loss: 0.00073724
Iteration 29/1000 | Loss: 0.00029575
Iteration 30/1000 | Loss: 0.00013962
Iteration 31/1000 | Loss: 0.00005556
Iteration 32/1000 | Loss: 0.00008468
Iteration 33/1000 | Loss: 0.00012827
Iteration 34/1000 | Loss: 0.00007732
Iteration 35/1000 | Loss: 0.00009294
Iteration 36/1000 | Loss: 0.00007908
Iteration 37/1000 | Loss: 0.00006133
Iteration 38/1000 | Loss: 0.00006385
Iteration 39/1000 | Loss: 0.00008252
Iteration 40/1000 | Loss: 0.00010667
Iteration 41/1000 | Loss: 0.00012698
Iteration 42/1000 | Loss: 0.00008606
Iteration 43/1000 | Loss: 0.00008231
Iteration 44/1000 | Loss: 0.00014938
Iteration 45/1000 | Loss: 0.00006887
Iteration 46/1000 | Loss: 0.00002542
Iteration 47/1000 | Loss: 0.00002428
Iteration 48/1000 | Loss: 0.00007110
Iteration 49/1000 | Loss: 0.00002470
Iteration 50/1000 | Loss: 0.00002367
Iteration 51/1000 | Loss: 0.00007565
Iteration 52/1000 | Loss: 0.00023089
Iteration 53/1000 | Loss: 0.00003772
Iteration 54/1000 | Loss: 0.00003944
Iteration 55/1000 | Loss: 0.00004451
Iteration 56/1000 | Loss: 0.00003689
Iteration 57/1000 | Loss: 0.00004043
Iteration 58/1000 | Loss: 0.00004048
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002212
Iteration 61/1000 | Loss: 0.00003088
Iteration 62/1000 | Loss: 0.00003215
Iteration 63/1000 | Loss: 0.00002860
Iteration 64/1000 | Loss: 0.00002295
Iteration 65/1000 | Loss: 0.00002215
Iteration 66/1000 | Loss: 0.00002179
Iteration 67/1000 | Loss: 0.00002161
Iteration 68/1000 | Loss: 0.00002154
Iteration 69/1000 | Loss: 0.00002153
Iteration 70/1000 | Loss: 0.00002149
Iteration 71/1000 | Loss: 0.00002148
Iteration 72/1000 | Loss: 0.00002147
Iteration 73/1000 | Loss: 0.00002146
Iteration 74/1000 | Loss: 0.00002146
Iteration 75/1000 | Loss: 0.00002146
Iteration 76/1000 | Loss: 0.00002145
Iteration 77/1000 | Loss: 0.00002145
Iteration 78/1000 | Loss: 0.00002144
Iteration 79/1000 | Loss: 0.00002144
Iteration 80/1000 | Loss: 0.00002144
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00002143
Iteration 84/1000 | Loss: 0.00002142
Iteration 85/1000 | Loss: 0.00002142
Iteration 86/1000 | Loss: 0.00002142
Iteration 87/1000 | Loss: 0.00002142
Iteration 88/1000 | Loss: 0.00002142
Iteration 89/1000 | Loss: 0.00002142
Iteration 90/1000 | Loss: 0.00002142
Iteration 91/1000 | Loss: 0.00002142
Iteration 92/1000 | Loss: 0.00002141
Iteration 93/1000 | Loss: 0.00002141
Iteration 94/1000 | Loss: 0.00002138
Iteration 95/1000 | Loss: 0.00002138
Iteration 96/1000 | Loss: 0.00002137
Iteration 97/1000 | Loss: 0.00002137
Iteration 98/1000 | Loss: 0.00002136
Iteration 99/1000 | Loss: 0.00002134
Iteration 100/1000 | Loss: 0.00002133
Iteration 101/1000 | Loss: 0.00002133
Iteration 102/1000 | Loss: 0.00002132
Iteration 103/1000 | Loss: 0.00002132
Iteration 104/1000 | Loss: 0.00002132
Iteration 105/1000 | Loss: 0.00002131
Iteration 106/1000 | Loss: 0.00002131
Iteration 107/1000 | Loss: 0.00002131
Iteration 108/1000 | Loss: 0.00002131
Iteration 109/1000 | Loss: 0.00002131
Iteration 110/1000 | Loss: 0.00002131
Iteration 111/1000 | Loss: 0.00002131
Iteration 112/1000 | Loss: 0.00002131
Iteration 113/1000 | Loss: 0.00002131
Iteration 114/1000 | Loss: 0.00002131
Iteration 115/1000 | Loss: 0.00002131
Iteration 116/1000 | Loss: 0.00002131
Iteration 117/1000 | Loss: 0.00002130
Iteration 118/1000 | Loss: 0.00002130
Iteration 119/1000 | Loss: 0.00002130
Iteration 120/1000 | Loss: 0.00002130
Iteration 121/1000 | Loss: 0.00002130
Iteration 122/1000 | Loss: 0.00002130
Iteration 123/1000 | Loss: 0.00002130
Iteration 124/1000 | Loss: 0.00002129
Iteration 125/1000 | Loss: 0.00002129
Iteration 126/1000 | Loss: 0.00002129
Iteration 127/1000 | Loss: 0.00002129
Iteration 128/1000 | Loss: 0.00002129
Iteration 129/1000 | Loss: 0.00002129
Iteration 130/1000 | Loss: 0.00002129
Iteration 131/1000 | Loss: 0.00002129
Iteration 132/1000 | Loss: 0.00002129
Iteration 133/1000 | Loss: 0.00002129
Iteration 134/1000 | Loss: 0.00002129
Iteration 135/1000 | Loss: 0.00002129
Iteration 136/1000 | Loss: 0.00002129
Iteration 137/1000 | Loss: 0.00002128
Iteration 138/1000 | Loss: 0.00002128
Iteration 139/1000 | Loss: 0.00002128
Iteration 140/1000 | Loss: 0.00002128
Iteration 141/1000 | Loss: 0.00002128
Iteration 142/1000 | Loss: 0.00002128
Iteration 143/1000 | Loss: 0.00002128
Iteration 144/1000 | Loss: 0.00002128
Iteration 145/1000 | Loss: 0.00002128
Iteration 146/1000 | Loss: 0.00002128
Iteration 147/1000 | Loss: 0.00002128
Iteration 148/1000 | Loss: 0.00002128
Iteration 149/1000 | Loss: 0.00002128
Iteration 150/1000 | Loss: 0.00002128
Iteration 151/1000 | Loss: 0.00002127
Iteration 152/1000 | Loss: 0.00002127
Iteration 153/1000 | Loss: 0.00002127
Iteration 154/1000 | Loss: 0.00002127
Iteration 155/1000 | Loss: 0.00002127
Iteration 156/1000 | Loss: 0.00002127
Iteration 157/1000 | Loss: 0.00002127
Iteration 158/1000 | Loss: 0.00002127
Iteration 159/1000 | Loss: 0.00002127
Iteration 160/1000 | Loss: 0.00002127
Iteration 161/1000 | Loss: 0.00002127
Iteration 162/1000 | Loss: 0.00002127
Iteration 163/1000 | Loss: 0.00002127
Iteration 164/1000 | Loss: 0.00002127
Iteration 165/1000 | Loss: 0.00002127
Iteration 166/1000 | Loss: 0.00002127
Iteration 167/1000 | Loss: 0.00002127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.1267142074066214e-05, 2.1267142074066214e-05, 2.1267142074066214e-05, 2.1267142074066214e-05, 2.1267142074066214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1267142074066214e-05

Optimization complete. Final v2v error: 3.8872807025909424 mm

Highest mean error: 9.583431243896484 mm for frame 138

Lowest mean error: 3.3001604080200195 mm for frame 8

Saving results

Total time: 158.7351005077362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867781
Iteration 2/25 | Loss: 0.00113812
Iteration 3/25 | Loss: 0.00080922
Iteration 4/25 | Loss: 0.00077034
Iteration 5/25 | Loss: 0.00076511
Iteration 6/25 | Loss: 0.00076387
Iteration 7/25 | Loss: 0.00076387
Iteration 8/25 | Loss: 0.00076387
Iteration 9/25 | Loss: 0.00076387
Iteration 10/25 | Loss: 0.00076387
Iteration 11/25 | Loss: 0.00076387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007638653041794896, 0.0007638653041794896, 0.0007638653041794896, 0.0007638653041794896, 0.0007638653041794896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007638653041794896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96955657
Iteration 2/25 | Loss: 0.00044922
Iteration 3/25 | Loss: 0.00044921
Iteration 4/25 | Loss: 0.00044921
Iteration 5/25 | Loss: 0.00044921
Iteration 6/25 | Loss: 0.00044921
Iteration 7/25 | Loss: 0.00044921
Iteration 8/25 | Loss: 0.00044921
Iteration 9/25 | Loss: 0.00044921
Iteration 10/25 | Loss: 0.00044921
Iteration 11/25 | Loss: 0.00044921
Iteration 12/25 | Loss: 0.00044921
Iteration 13/25 | Loss: 0.00044921
Iteration 14/25 | Loss: 0.00044921
Iteration 15/25 | Loss: 0.00044921
Iteration 16/25 | Loss: 0.00044921
Iteration 17/25 | Loss: 0.00044921
Iteration 18/25 | Loss: 0.00044921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00044921055086888373, 0.00044921055086888373, 0.00044921055086888373, 0.00044921055086888373, 0.00044921055086888373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044921055086888373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044921
Iteration 2/1000 | Loss: 0.00004450
Iteration 3/1000 | Loss: 0.00003214
Iteration 4/1000 | Loss: 0.00002988
Iteration 5/1000 | Loss: 0.00002777
Iteration 6/1000 | Loss: 0.00002690
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002547
Iteration 9/1000 | Loss: 0.00002520
Iteration 10/1000 | Loss: 0.00002503
Iteration 11/1000 | Loss: 0.00002494
Iteration 12/1000 | Loss: 0.00002492
Iteration 13/1000 | Loss: 0.00002491
Iteration 14/1000 | Loss: 0.00002491
Iteration 15/1000 | Loss: 0.00002491
Iteration 16/1000 | Loss: 0.00002490
Iteration 17/1000 | Loss: 0.00002490
Iteration 18/1000 | Loss: 0.00002490
Iteration 19/1000 | Loss: 0.00002490
Iteration 20/1000 | Loss: 0.00002490
Iteration 21/1000 | Loss: 0.00002490
Iteration 22/1000 | Loss: 0.00002490
Iteration 23/1000 | Loss: 0.00002489
Iteration 24/1000 | Loss: 0.00002489
Iteration 25/1000 | Loss: 0.00002489
Iteration 26/1000 | Loss: 0.00002489
Iteration 27/1000 | Loss: 0.00002489
Iteration 28/1000 | Loss: 0.00002488
Iteration 29/1000 | Loss: 0.00002488
Iteration 30/1000 | Loss: 0.00002488
Iteration 31/1000 | Loss: 0.00002487
Iteration 32/1000 | Loss: 0.00002483
Iteration 33/1000 | Loss: 0.00002483
Iteration 34/1000 | Loss: 0.00002481
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00002478
Iteration 37/1000 | Loss: 0.00002478
Iteration 38/1000 | Loss: 0.00002478
Iteration 39/1000 | Loss: 0.00002478
Iteration 40/1000 | Loss: 0.00002478
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002477
Iteration 44/1000 | Loss: 0.00002477
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00002477
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002476
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002474
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002474
Iteration 55/1000 | Loss: 0.00002474
Iteration 56/1000 | Loss: 0.00002474
Iteration 57/1000 | Loss: 0.00002474
Iteration 58/1000 | Loss: 0.00002474
Iteration 59/1000 | Loss: 0.00002474
Iteration 60/1000 | Loss: 0.00002473
Iteration 61/1000 | Loss: 0.00002473
Iteration 62/1000 | Loss: 0.00002473
Iteration 63/1000 | Loss: 0.00002473
Iteration 64/1000 | Loss: 0.00002473
Iteration 65/1000 | Loss: 0.00002472
Iteration 66/1000 | Loss: 0.00002472
Iteration 67/1000 | Loss: 0.00002472
Iteration 68/1000 | Loss: 0.00002472
Iteration 69/1000 | Loss: 0.00002472
Iteration 70/1000 | Loss: 0.00002472
Iteration 71/1000 | Loss: 0.00002471
Iteration 72/1000 | Loss: 0.00002471
Iteration 73/1000 | Loss: 0.00002471
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002471
Iteration 76/1000 | Loss: 0.00002471
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002470
Iteration 79/1000 | Loss: 0.00002469
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002468
Iteration 84/1000 | Loss: 0.00002468
Iteration 85/1000 | Loss: 0.00002468
Iteration 86/1000 | Loss: 0.00002468
Iteration 87/1000 | Loss: 0.00002467
Iteration 88/1000 | Loss: 0.00002467
Iteration 89/1000 | Loss: 0.00002467
Iteration 90/1000 | Loss: 0.00002467
Iteration 91/1000 | Loss: 0.00002467
Iteration 92/1000 | Loss: 0.00002467
Iteration 93/1000 | Loss: 0.00002467
Iteration 94/1000 | Loss: 0.00002467
Iteration 95/1000 | Loss: 0.00002467
Iteration 96/1000 | Loss: 0.00002466
Iteration 97/1000 | Loss: 0.00002465
Iteration 98/1000 | Loss: 0.00002465
Iteration 99/1000 | Loss: 0.00002465
Iteration 100/1000 | Loss: 0.00002465
Iteration 101/1000 | Loss: 0.00002465
Iteration 102/1000 | Loss: 0.00002465
Iteration 103/1000 | Loss: 0.00002465
Iteration 104/1000 | Loss: 0.00002465
Iteration 105/1000 | Loss: 0.00002465
Iteration 106/1000 | Loss: 0.00002465
Iteration 107/1000 | Loss: 0.00002464
Iteration 108/1000 | Loss: 0.00002464
Iteration 109/1000 | Loss: 0.00002464
Iteration 110/1000 | Loss: 0.00002463
Iteration 111/1000 | Loss: 0.00002463
Iteration 112/1000 | Loss: 0.00002462
Iteration 113/1000 | Loss: 0.00002462
Iteration 114/1000 | Loss: 0.00002462
Iteration 115/1000 | Loss: 0.00002462
Iteration 116/1000 | Loss: 0.00002462
Iteration 117/1000 | Loss: 0.00002462
Iteration 118/1000 | Loss: 0.00002462
Iteration 119/1000 | Loss: 0.00002462
Iteration 120/1000 | Loss: 0.00002462
Iteration 121/1000 | Loss: 0.00002461
Iteration 122/1000 | Loss: 0.00002461
Iteration 123/1000 | Loss: 0.00002461
Iteration 124/1000 | Loss: 0.00002461
Iteration 125/1000 | Loss: 0.00002461
Iteration 126/1000 | Loss: 0.00002460
Iteration 127/1000 | Loss: 0.00002460
Iteration 128/1000 | Loss: 0.00002460
Iteration 129/1000 | Loss: 0.00002460
Iteration 130/1000 | Loss: 0.00002460
Iteration 131/1000 | Loss: 0.00002460
Iteration 132/1000 | Loss: 0.00002460
Iteration 133/1000 | Loss: 0.00002460
Iteration 134/1000 | Loss: 0.00002460
Iteration 135/1000 | Loss: 0.00002460
Iteration 136/1000 | Loss: 0.00002460
Iteration 137/1000 | Loss: 0.00002460
Iteration 138/1000 | Loss: 0.00002460
Iteration 139/1000 | Loss: 0.00002460
Iteration 140/1000 | Loss: 0.00002460
Iteration 141/1000 | Loss: 0.00002460
Iteration 142/1000 | Loss: 0.00002460
Iteration 143/1000 | Loss: 0.00002460
Iteration 144/1000 | Loss: 0.00002460
Iteration 145/1000 | Loss: 0.00002460
Iteration 146/1000 | Loss: 0.00002460
Iteration 147/1000 | Loss: 0.00002460
Iteration 148/1000 | Loss: 0.00002460
Iteration 149/1000 | Loss: 0.00002460
Iteration 150/1000 | Loss: 0.00002460
Iteration 151/1000 | Loss: 0.00002460
Iteration 152/1000 | Loss: 0.00002460
Iteration 153/1000 | Loss: 0.00002460
Iteration 154/1000 | Loss: 0.00002460
Iteration 155/1000 | Loss: 0.00002460
Iteration 156/1000 | Loss: 0.00002460
Iteration 157/1000 | Loss: 0.00002460
Iteration 158/1000 | Loss: 0.00002460
Iteration 159/1000 | Loss: 0.00002460
Iteration 160/1000 | Loss: 0.00002460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.4597387891844846e-05, 2.4597387891844846e-05, 2.4597387891844846e-05, 2.4597387891844846e-05, 2.4597387891844846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4597387891844846e-05

Optimization complete. Final v2v error: 4.231064319610596 mm

Highest mean error: 4.4844207763671875 mm for frame 6

Lowest mean error: 4.05703592300415 mm for frame 84

Saving results

Total time: 34.18833136558533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802047
Iteration 2/25 | Loss: 0.00113265
Iteration 3/25 | Loss: 0.00077607
Iteration 4/25 | Loss: 0.00072910
Iteration 5/25 | Loss: 0.00072092
Iteration 6/25 | Loss: 0.00071928
Iteration 7/25 | Loss: 0.00071917
Iteration 8/25 | Loss: 0.00071917
Iteration 9/25 | Loss: 0.00071917
Iteration 10/25 | Loss: 0.00071917
Iteration 11/25 | Loss: 0.00071917
Iteration 12/25 | Loss: 0.00071917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000719171657692641, 0.000719171657692641, 0.000719171657692641, 0.000719171657692641, 0.000719171657692641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000719171657692641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45581496
Iteration 2/25 | Loss: 0.00046501
Iteration 3/25 | Loss: 0.00046500
Iteration 4/25 | Loss: 0.00046499
Iteration 5/25 | Loss: 0.00046499
Iteration 6/25 | Loss: 0.00046499
Iteration 7/25 | Loss: 0.00046499
Iteration 8/25 | Loss: 0.00046499
Iteration 9/25 | Loss: 0.00046499
Iteration 10/25 | Loss: 0.00046499
Iteration 11/25 | Loss: 0.00046499
Iteration 12/25 | Loss: 0.00046499
Iteration 13/25 | Loss: 0.00046499
Iteration 14/25 | Loss: 0.00046499
Iteration 15/25 | Loss: 0.00046499
Iteration 16/25 | Loss: 0.00046499
Iteration 17/25 | Loss: 0.00046499
Iteration 18/25 | Loss: 0.00046499
Iteration 19/25 | Loss: 0.00046499
Iteration 20/25 | Loss: 0.00046499
Iteration 21/25 | Loss: 0.00046499
Iteration 22/25 | Loss: 0.00046499
Iteration 23/25 | Loss: 0.00046499
Iteration 24/25 | Loss: 0.00046499
Iteration 25/25 | Loss: 0.00046499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00046499166637659073, 0.00046499166637659073, 0.00046499166637659073, 0.00046499166637659073, 0.00046499166637659073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046499166637659073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046499
Iteration 2/1000 | Loss: 0.00004229
Iteration 3/1000 | Loss: 0.00002857
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001708
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001599
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001593
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001579
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001578
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001577
Iteration 35/1000 | Loss: 0.00001576
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001576
Iteration 38/1000 | Loss: 0.00001575
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001575
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001573
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001572
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001572
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001569
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001569
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001564
Iteration 69/1000 | Loss: 0.00001564
Iteration 70/1000 | Loss: 0.00001563
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001562
Iteration 74/1000 | Loss: 0.00001562
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001557
Iteration 108/1000 | Loss: 0.00001557
Iteration 109/1000 | Loss: 0.00001557
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001557
Iteration 123/1000 | Loss: 0.00001557
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001557
Iteration 128/1000 | Loss: 0.00001557
Iteration 129/1000 | Loss: 0.00001557
Iteration 130/1000 | Loss: 0.00001557
Iteration 131/1000 | Loss: 0.00001557
Iteration 132/1000 | Loss: 0.00001557
Iteration 133/1000 | Loss: 0.00001557
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.556770621391479e-05, 1.556770621391479e-05, 1.556770621391479e-05, 1.556770621391479e-05, 1.556770621391479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.556770621391479e-05

Optimization complete. Final v2v error: 3.462657928466797 mm

Highest mean error: 3.7357890605926514 mm for frame 88

Lowest mean error: 3.101055860519409 mm for frame 37

Saving results

Total time: 36.2495493888855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821833
Iteration 2/25 | Loss: 0.00128999
Iteration 3/25 | Loss: 0.00093248
Iteration 4/25 | Loss: 0.00087837
Iteration 5/25 | Loss: 0.00086700
Iteration 6/25 | Loss: 0.00086534
Iteration 7/25 | Loss: 0.00086498
Iteration 8/25 | Loss: 0.00086497
Iteration 9/25 | Loss: 0.00086497
Iteration 10/25 | Loss: 0.00086496
Iteration 11/25 | Loss: 0.00086496
Iteration 12/25 | Loss: 0.00086496
Iteration 13/25 | Loss: 0.00086496
Iteration 14/25 | Loss: 0.00086496
Iteration 15/25 | Loss: 0.00086496
Iteration 16/25 | Loss: 0.00086496
Iteration 17/25 | Loss: 0.00086496
Iteration 18/25 | Loss: 0.00086496
Iteration 19/25 | Loss: 0.00086496
Iteration 20/25 | Loss: 0.00086496
Iteration 21/25 | Loss: 0.00086496
Iteration 22/25 | Loss: 0.00086496
Iteration 23/25 | Loss: 0.00086496
Iteration 24/25 | Loss: 0.00086496
Iteration 25/25 | Loss: 0.00086496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44331920
Iteration 2/25 | Loss: 0.00031248
Iteration 3/25 | Loss: 0.00031244
Iteration 4/25 | Loss: 0.00031244
Iteration 5/25 | Loss: 0.00031244
Iteration 6/25 | Loss: 0.00031244
Iteration 7/25 | Loss: 0.00031244
Iteration 8/25 | Loss: 0.00031244
Iteration 9/25 | Loss: 0.00031244
Iteration 10/25 | Loss: 0.00031244
Iteration 11/25 | Loss: 0.00031244
Iteration 12/25 | Loss: 0.00031244
Iteration 13/25 | Loss: 0.00031244
Iteration 14/25 | Loss: 0.00031244
Iteration 15/25 | Loss: 0.00031244
Iteration 16/25 | Loss: 0.00031244
Iteration 17/25 | Loss: 0.00031244
Iteration 18/25 | Loss: 0.00031244
Iteration 19/25 | Loss: 0.00031244
Iteration 20/25 | Loss: 0.00031244
Iteration 21/25 | Loss: 0.00031244
Iteration 22/25 | Loss: 0.00031244
Iteration 23/25 | Loss: 0.00031244
Iteration 24/25 | Loss: 0.00031244
Iteration 25/25 | Loss: 0.00031244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031244
Iteration 2/1000 | Loss: 0.00006385
Iteration 3/1000 | Loss: 0.00004225
Iteration 4/1000 | Loss: 0.00003733
Iteration 5/1000 | Loss: 0.00003495
Iteration 6/1000 | Loss: 0.00003428
Iteration 7/1000 | Loss: 0.00003364
Iteration 8/1000 | Loss: 0.00003313
Iteration 9/1000 | Loss: 0.00003274
Iteration 10/1000 | Loss: 0.00003249
Iteration 11/1000 | Loss: 0.00003237
Iteration 12/1000 | Loss: 0.00003229
Iteration 13/1000 | Loss: 0.00003228
Iteration 14/1000 | Loss: 0.00003226
Iteration 15/1000 | Loss: 0.00003225
Iteration 16/1000 | Loss: 0.00003225
Iteration 17/1000 | Loss: 0.00003224
Iteration 18/1000 | Loss: 0.00003224
Iteration 19/1000 | Loss: 0.00003224
Iteration 20/1000 | Loss: 0.00003223
Iteration 21/1000 | Loss: 0.00003223
Iteration 22/1000 | Loss: 0.00003223
Iteration 23/1000 | Loss: 0.00003223
Iteration 24/1000 | Loss: 0.00003223
Iteration 25/1000 | Loss: 0.00003223
Iteration 26/1000 | Loss: 0.00003222
Iteration 27/1000 | Loss: 0.00003222
Iteration 28/1000 | Loss: 0.00003222
Iteration 29/1000 | Loss: 0.00003222
Iteration 30/1000 | Loss: 0.00003221
Iteration 31/1000 | Loss: 0.00003221
Iteration 32/1000 | Loss: 0.00003221
Iteration 33/1000 | Loss: 0.00003221
Iteration 34/1000 | Loss: 0.00003221
Iteration 35/1000 | Loss: 0.00003221
Iteration 36/1000 | Loss: 0.00003221
Iteration 37/1000 | Loss: 0.00003221
Iteration 38/1000 | Loss: 0.00003220
Iteration 39/1000 | Loss: 0.00003220
Iteration 40/1000 | Loss: 0.00003219
Iteration 41/1000 | Loss: 0.00003219
Iteration 42/1000 | Loss: 0.00003219
Iteration 43/1000 | Loss: 0.00003218
Iteration 44/1000 | Loss: 0.00003218
Iteration 45/1000 | Loss: 0.00003218
Iteration 46/1000 | Loss: 0.00003218
Iteration 47/1000 | Loss: 0.00003218
Iteration 48/1000 | Loss: 0.00003218
Iteration 49/1000 | Loss: 0.00003218
Iteration 50/1000 | Loss: 0.00003218
Iteration 51/1000 | Loss: 0.00003218
Iteration 52/1000 | Loss: 0.00003218
Iteration 53/1000 | Loss: 0.00003217
Iteration 54/1000 | Loss: 0.00003217
Iteration 55/1000 | Loss: 0.00003217
Iteration 56/1000 | Loss: 0.00003217
Iteration 57/1000 | Loss: 0.00003217
Iteration 58/1000 | Loss: 0.00003217
Iteration 59/1000 | Loss: 0.00003217
Iteration 60/1000 | Loss: 0.00003217
Iteration 61/1000 | Loss: 0.00003217
Iteration 62/1000 | Loss: 0.00003217
Iteration 63/1000 | Loss: 0.00003216
Iteration 64/1000 | Loss: 0.00003216
Iteration 65/1000 | Loss: 0.00003216
Iteration 66/1000 | Loss: 0.00003216
Iteration 67/1000 | Loss: 0.00003216
Iteration 68/1000 | Loss: 0.00003216
Iteration 69/1000 | Loss: 0.00003216
Iteration 70/1000 | Loss: 0.00003216
Iteration 71/1000 | Loss: 0.00003216
Iteration 72/1000 | Loss: 0.00003215
Iteration 73/1000 | Loss: 0.00003215
Iteration 74/1000 | Loss: 0.00003215
Iteration 75/1000 | Loss: 0.00003215
Iteration 76/1000 | Loss: 0.00003215
Iteration 77/1000 | Loss: 0.00003215
Iteration 78/1000 | Loss: 0.00003215
Iteration 79/1000 | Loss: 0.00003215
Iteration 80/1000 | Loss: 0.00003215
Iteration 81/1000 | Loss: 0.00003215
Iteration 82/1000 | Loss: 0.00003215
Iteration 83/1000 | Loss: 0.00003215
Iteration 84/1000 | Loss: 0.00003215
Iteration 85/1000 | Loss: 0.00003215
Iteration 86/1000 | Loss: 0.00003215
Iteration 87/1000 | Loss: 0.00003215
Iteration 88/1000 | Loss: 0.00003215
Iteration 89/1000 | Loss: 0.00003215
Iteration 90/1000 | Loss: 0.00003215
Iteration 91/1000 | Loss: 0.00003215
Iteration 92/1000 | Loss: 0.00003215
Iteration 93/1000 | Loss: 0.00003215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [3.2145795557880774e-05, 3.2145795557880774e-05, 3.2145795557880774e-05, 3.2145795557880774e-05, 3.2145795557880774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2145795557880774e-05

Optimization complete. Final v2v error: 4.594717979431152 mm

Highest mean error: 4.966392517089844 mm for frame 80

Lowest mean error: 3.5843167304992676 mm for frame 2

Saving results

Total time: 33.27214813232422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381261
Iteration 2/25 | Loss: 0.00089664
Iteration 3/25 | Loss: 0.00076427
Iteration 4/25 | Loss: 0.00074035
Iteration 5/25 | Loss: 0.00073147
Iteration 6/25 | Loss: 0.00072941
Iteration 7/25 | Loss: 0.00072886
Iteration 8/25 | Loss: 0.00072886
Iteration 9/25 | Loss: 0.00072886
Iteration 10/25 | Loss: 0.00072886
Iteration 11/25 | Loss: 0.00072886
Iteration 12/25 | Loss: 0.00072886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007288573542609811, 0.0007288573542609811, 0.0007288573542609811, 0.0007288573542609811, 0.0007288573542609811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007288573542609811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47816789
Iteration 2/25 | Loss: 0.00054624
Iteration 3/25 | Loss: 0.00054624
Iteration 4/25 | Loss: 0.00054624
Iteration 5/25 | Loss: 0.00054624
Iteration 6/25 | Loss: 0.00054624
Iteration 7/25 | Loss: 0.00054624
Iteration 8/25 | Loss: 0.00054624
Iteration 9/25 | Loss: 0.00054624
Iteration 10/25 | Loss: 0.00054624
Iteration 11/25 | Loss: 0.00054624
Iteration 12/25 | Loss: 0.00054624
Iteration 13/25 | Loss: 0.00054624
Iteration 14/25 | Loss: 0.00054624
Iteration 15/25 | Loss: 0.00054624
Iteration 16/25 | Loss: 0.00054624
Iteration 17/25 | Loss: 0.00054624
Iteration 18/25 | Loss: 0.00054624
Iteration 19/25 | Loss: 0.00054624
Iteration 20/25 | Loss: 0.00054624
Iteration 21/25 | Loss: 0.00054624
Iteration 22/25 | Loss: 0.00054624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005462372209876776, 0.0005462372209876776, 0.0005462372209876776, 0.0005462372209876776, 0.0005462372209876776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005462372209876776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054624
Iteration 2/1000 | Loss: 0.00005803
Iteration 3/1000 | Loss: 0.00003526
Iteration 4/1000 | Loss: 0.00002874
Iteration 5/1000 | Loss: 0.00002673
Iteration 6/1000 | Loss: 0.00002548
Iteration 7/1000 | Loss: 0.00002467
Iteration 8/1000 | Loss: 0.00002400
Iteration 9/1000 | Loss: 0.00002354
Iteration 10/1000 | Loss: 0.00002321
Iteration 11/1000 | Loss: 0.00002317
Iteration 12/1000 | Loss: 0.00002298
Iteration 13/1000 | Loss: 0.00002296
Iteration 14/1000 | Loss: 0.00002284
Iteration 15/1000 | Loss: 0.00002284
Iteration 16/1000 | Loss: 0.00002283
Iteration 17/1000 | Loss: 0.00002281
Iteration 18/1000 | Loss: 0.00002280
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002268
Iteration 21/1000 | Loss: 0.00002268
Iteration 22/1000 | Loss: 0.00002265
Iteration 23/1000 | Loss: 0.00002265
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002265
Iteration 26/1000 | Loss: 0.00002263
Iteration 27/1000 | Loss: 0.00002263
Iteration 28/1000 | Loss: 0.00002263
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002260
Iteration 33/1000 | Loss: 0.00002258
Iteration 34/1000 | Loss: 0.00002258
Iteration 35/1000 | Loss: 0.00002258
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002253
Iteration 40/1000 | Loss: 0.00002253
Iteration 41/1000 | Loss: 0.00002253
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002252
Iteration 44/1000 | Loss: 0.00002252
Iteration 45/1000 | Loss: 0.00002252
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002250
Iteration 49/1000 | Loss: 0.00002250
Iteration 50/1000 | Loss: 0.00002250
Iteration 51/1000 | Loss: 0.00002250
Iteration 52/1000 | Loss: 0.00002249
Iteration 53/1000 | Loss: 0.00002249
Iteration 54/1000 | Loss: 0.00002249
Iteration 55/1000 | Loss: 0.00002249
Iteration 56/1000 | Loss: 0.00002249
Iteration 57/1000 | Loss: 0.00002249
Iteration 58/1000 | Loss: 0.00002248
Iteration 59/1000 | Loss: 0.00002248
Iteration 60/1000 | Loss: 0.00002248
Iteration 61/1000 | Loss: 0.00002248
Iteration 62/1000 | Loss: 0.00002248
Iteration 63/1000 | Loss: 0.00002248
Iteration 64/1000 | Loss: 0.00002248
Iteration 65/1000 | Loss: 0.00002247
Iteration 66/1000 | Loss: 0.00002247
Iteration 67/1000 | Loss: 0.00002247
Iteration 68/1000 | Loss: 0.00002247
Iteration 69/1000 | Loss: 0.00002247
Iteration 70/1000 | Loss: 0.00002247
Iteration 71/1000 | Loss: 0.00002247
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002246
Iteration 74/1000 | Loss: 0.00002246
Iteration 75/1000 | Loss: 0.00002245
Iteration 76/1000 | Loss: 0.00002245
Iteration 77/1000 | Loss: 0.00002245
Iteration 78/1000 | Loss: 0.00002245
Iteration 79/1000 | Loss: 0.00002244
Iteration 80/1000 | Loss: 0.00002244
Iteration 81/1000 | Loss: 0.00002244
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002244
Iteration 84/1000 | Loss: 0.00002243
Iteration 85/1000 | Loss: 0.00002243
Iteration 86/1000 | Loss: 0.00002243
Iteration 87/1000 | Loss: 0.00002243
Iteration 88/1000 | Loss: 0.00002243
Iteration 89/1000 | Loss: 0.00002243
Iteration 90/1000 | Loss: 0.00002243
Iteration 91/1000 | Loss: 0.00002243
Iteration 92/1000 | Loss: 0.00002242
Iteration 93/1000 | Loss: 0.00002242
Iteration 94/1000 | Loss: 0.00002242
Iteration 95/1000 | Loss: 0.00002242
Iteration 96/1000 | Loss: 0.00002242
Iteration 97/1000 | Loss: 0.00002242
Iteration 98/1000 | Loss: 0.00002242
Iteration 99/1000 | Loss: 0.00002242
Iteration 100/1000 | Loss: 0.00002242
Iteration 101/1000 | Loss: 0.00002241
Iteration 102/1000 | Loss: 0.00002241
Iteration 103/1000 | Loss: 0.00002241
Iteration 104/1000 | Loss: 0.00002241
Iteration 105/1000 | Loss: 0.00002241
Iteration 106/1000 | Loss: 0.00002241
Iteration 107/1000 | Loss: 0.00002241
Iteration 108/1000 | Loss: 0.00002241
Iteration 109/1000 | Loss: 0.00002241
Iteration 110/1000 | Loss: 0.00002241
Iteration 111/1000 | Loss: 0.00002241
Iteration 112/1000 | Loss: 0.00002241
Iteration 113/1000 | Loss: 0.00002241
Iteration 114/1000 | Loss: 0.00002241
Iteration 115/1000 | Loss: 0.00002241
Iteration 116/1000 | Loss: 0.00002241
Iteration 117/1000 | Loss: 0.00002241
Iteration 118/1000 | Loss: 0.00002241
Iteration 119/1000 | Loss: 0.00002241
Iteration 120/1000 | Loss: 0.00002241
Iteration 121/1000 | Loss: 0.00002241
Iteration 122/1000 | Loss: 0.00002241
Iteration 123/1000 | Loss: 0.00002241
Iteration 124/1000 | Loss: 0.00002241
Iteration 125/1000 | Loss: 0.00002241
Iteration 126/1000 | Loss: 0.00002241
Iteration 127/1000 | Loss: 0.00002241
Iteration 128/1000 | Loss: 0.00002241
Iteration 129/1000 | Loss: 0.00002241
Iteration 130/1000 | Loss: 0.00002241
Iteration 131/1000 | Loss: 0.00002241
Iteration 132/1000 | Loss: 0.00002241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.2414295017370023e-05, 2.2414295017370023e-05, 2.2414295017370023e-05, 2.2414295017370023e-05, 2.2414295017370023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2414295017370023e-05

Optimization complete. Final v2v error: 3.9695017337799072 mm

Highest mean error: 4.545934200286865 mm for frame 140

Lowest mean error: 3.5362207889556885 mm for frame 25

Saving results

Total time: 42.58852028846741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467172
Iteration 2/25 | Loss: 0.00110336
Iteration 3/25 | Loss: 0.00080629
Iteration 4/25 | Loss: 0.00073929
Iteration 5/25 | Loss: 0.00072032
Iteration 6/25 | Loss: 0.00071594
Iteration 7/25 | Loss: 0.00071511
Iteration 8/25 | Loss: 0.00071509
Iteration 9/25 | Loss: 0.00071509
Iteration 10/25 | Loss: 0.00071509
Iteration 11/25 | Loss: 0.00071509
Iteration 12/25 | Loss: 0.00071509
Iteration 13/25 | Loss: 0.00071509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007150874589569867, 0.0007150874589569867, 0.0007150874589569867, 0.0007150874589569867, 0.0007150874589569867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007150874589569867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39779377
Iteration 2/25 | Loss: 0.00048938
Iteration 3/25 | Loss: 0.00048937
Iteration 4/25 | Loss: 0.00048937
Iteration 5/25 | Loss: 0.00048937
Iteration 6/25 | Loss: 0.00048937
Iteration 7/25 | Loss: 0.00048937
Iteration 8/25 | Loss: 0.00048937
Iteration 9/25 | Loss: 0.00048937
Iteration 10/25 | Loss: 0.00048937
Iteration 11/25 | Loss: 0.00048937
Iteration 12/25 | Loss: 0.00048937
Iteration 13/25 | Loss: 0.00048937
Iteration 14/25 | Loss: 0.00048937
Iteration 15/25 | Loss: 0.00048937
Iteration 16/25 | Loss: 0.00048937
Iteration 17/25 | Loss: 0.00048937
Iteration 18/25 | Loss: 0.00048937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004893711884506047, 0.0004893711884506047, 0.0004893711884506047, 0.0004893711884506047, 0.0004893711884506047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004893711884506047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048937
Iteration 2/1000 | Loss: 0.00004597
Iteration 3/1000 | Loss: 0.00002679
Iteration 4/1000 | Loss: 0.00002373
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00002155
Iteration 7/1000 | Loss: 0.00002108
Iteration 8/1000 | Loss: 0.00002064
Iteration 9/1000 | Loss: 0.00002028
Iteration 10/1000 | Loss: 0.00002001
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001986
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001967
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001966
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001965
Iteration 22/1000 | Loss: 0.00001963
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001955
Iteration 34/1000 | Loss: 0.00001954
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001953
Iteration 39/1000 | Loss: 0.00001953
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001950
Iteration 44/1000 | Loss: 0.00001950
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001946
Iteration 47/1000 | Loss: 0.00001945
Iteration 48/1000 | Loss: 0.00001945
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001945
Iteration 51/1000 | Loss: 0.00001945
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001944
Iteration 58/1000 | Loss: 0.00001944
Iteration 59/1000 | Loss: 0.00001944
Iteration 60/1000 | Loss: 0.00001944
Iteration 61/1000 | Loss: 0.00001944
Iteration 62/1000 | Loss: 0.00001943
Iteration 63/1000 | Loss: 0.00001942
Iteration 64/1000 | Loss: 0.00001942
Iteration 65/1000 | Loss: 0.00001942
Iteration 66/1000 | Loss: 0.00001942
Iteration 67/1000 | Loss: 0.00001942
Iteration 68/1000 | Loss: 0.00001941
Iteration 69/1000 | Loss: 0.00001941
Iteration 70/1000 | Loss: 0.00001941
Iteration 71/1000 | Loss: 0.00001941
Iteration 72/1000 | Loss: 0.00001941
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001941
Iteration 76/1000 | Loss: 0.00001941
Iteration 77/1000 | Loss: 0.00001941
Iteration 78/1000 | Loss: 0.00001941
Iteration 79/1000 | Loss: 0.00001941
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001940
Iteration 82/1000 | Loss: 0.00001940
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001939
Iteration 85/1000 | Loss: 0.00001938
Iteration 86/1000 | Loss: 0.00001938
Iteration 87/1000 | Loss: 0.00001938
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001935
Iteration 94/1000 | Loss: 0.00001935
Iteration 95/1000 | Loss: 0.00001935
Iteration 96/1000 | Loss: 0.00001935
Iteration 97/1000 | Loss: 0.00001935
Iteration 98/1000 | Loss: 0.00001935
Iteration 99/1000 | Loss: 0.00001934
Iteration 100/1000 | Loss: 0.00001934
Iteration 101/1000 | Loss: 0.00001934
Iteration 102/1000 | Loss: 0.00001934
Iteration 103/1000 | Loss: 0.00001934
Iteration 104/1000 | Loss: 0.00001934
Iteration 105/1000 | Loss: 0.00001933
Iteration 106/1000 | Loss: 0.00001933
Iteration 107/1000 | Loss: 0.00001933
Iteration 108/1000 | Loss: 0.00001933
Iteration 109/1000 | Loss: 0.00001933
Iteration 110/1000 | Loss: 0.00001933
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001931
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001930
Iteration 121/1000 | Loss: 0.00001930
Iteration 122/1000 | Loss: 0.00001930
Iteration 123/1000 | Loss: 0.00001930
Iteration 124/1000 | Loss: 0.00001930
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001929
Iteration 133/1000 | Loss: 0.00001929
Iteration 134/1000 | Loss: 0.00001929
Iteration 135/1000 | Loss: 0.00001929
Iteration 136/1000 | Loss: 0.00001929
Iteration 137/1000 | Loss: 0.00001929
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Iteration 141/1000 | Loss: 0.00001928
Iteration 142/1000 | Loss: 0.00001928
Iteration 143/1000 | Loss: 0.00001927
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001927
Iteration 146/1000 | Loss: 0.00001927
Iteration 147/1000 | Loss: 0.00001927
Iteration 148/1000 | Loss: 0.00001927
Iteration 149/1000 | Loss: 0.00001926
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001926
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001926
Iteration 154/1000 | Loss: 0.00001926
Iteration 155/1000 | Loss: 0.00001925
Iteration 156/1000 | Loss: 0.00001925
Iteration 157/1000 | Loss: 0.00001925
Iteration 158/1000 | Loss: 0.00001925
Iteration 159/1000 | Loss: 0.00001925
Iteration 160/1000 | Loss: 0.00001925
Iteration 161/1000 | Loss: 0.00001925
Iteration 162/1000 | Loss: 0.00001925
Iteration 163/1000 | Loss: 0.00001925
Iteration 164/1000 | Loss: 0.00001925
Iteration 165/1000 | Loss: 0.00001925
Iteration 166/1000 | Loss: 0.00001925
Iteration 167/1000 | Loss: 0.00001925
Iteration 168/1000 | Loss: 0.00001925
Iteration 169/1000 | Loss: 0.00001925
Iteration 170/1000 | Loss: 0.00001925
Iteration 171/1000 | Loss: 0.00001925
Iteration 172/1000 | Loss: 0.00001924
Iteration 173/1000 | Loss: 0.00001924
Iteration 174/1000 | Loss: 0.00001924
Iteration 175/1000 | Loss: 0.00001924
Iteration 176/1000 | Loss: 0.00001924
Iteration 177/1000 | Loss: 0.00001924
Iteration 178/1000 | Loss: 0.00001924
Iteration 179/1000 | Loss: 0.00001924
Iteration 180/1000 | Loss: 0.00001924
Iteration 181/1000 | Loss: 0.00001923
Iteration 182/1000 | Loss: 0.00001923
Iteration 183/1000 | Loss: 0.00001923
Iteration 184/1000 | Loss: 0.00001923
Iteration 185/1000 | Loss: 0.00001923
Iteration 186/1000 | Loss: 0.00001923
Iteration 187/1000 | Loss: 0.00001923
Iteration 188/1000 | Loss: 0.00001923
Iteration 189/1000 | Loss: 0.00001923
Iteration 190/1000 | Loss: 0.00001923
Iteration 191/1000 | Loss: 0.00001923
Iteration 192/1000 | Loss: 0.00001923
Iteration 193/1000 | Loss: 0.00001923
Iteration 194/1000 | Loss: 0.00001923
Iteration 195/1000 | Loss: 0.00001923
Iteration 196/1000 | Loss: 0.00001922
Iteration 197/1000 | Loss: 0.00001922
Iteration 198/1000 | Loss: 0.00001922
Iteration 199/1000 | Loss: 0.00001922
Iteration 200/1000 | Loss: 0.00001922
Iteration 201/1000 | Loss: 0.00001922
Iteration 202/1000 | Loss: 0.00001922
Iteration 203/1000 | Loss: 0.00001922
Iteration 204/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.9223904018872418e-05, 1.9223904018872418e-05, 1.9223904018872418e-05, 1.9223904018872418e-05, 1.9223904018872418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9223904018872418e-05

Optimization complete. Final v2v error: 3.7712042331695557 mm

Highest mean error: 4.242073059082031 mm for frame 207

Lowest mean error: 3.072103500366211 mm for frame 2

Saving results

Total time: 48.30757403373718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00343827
Iteration 2/25 | Loss: 0.00076440
Iteration 3/25 | Loss: 0.00065976
Iteration 4/25 | Loss: 0.00063893
Iteration 5/25 | Loss: 0.00063243
Iteration 6/25 | Loss: 0.00063046
Iteration 7/25 | Loss: 0.00063014
Iteration 8/25 | Loss: 0.00063014
Iteration 9/25 | Loss: 0.00063014
Iteration 10/25 | Loss: 0.00063014
Iteration 11/25 | Loss: 0.00063014
Iteration 12/25 | Loss: 0.00063014
Iteration 13/25 | Loss: 0.00063014
Iteration 14/25 | Loss: 0.00063014
Iteration 15/25 | Loss: 0.00063014
Iteration 16/25 | Loss: 0.00063014
Iteration 17/25 | Loss: 0.00063014
Iteration 18/25 | Loss: 0.00063014
Iteration 19/25 | Loss: 0.00063014
Iteration 20/25 | Loss: 0.00063014
Iteration 21/25 | Loss: 0.00063014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006301433895714581, 0.0006301433895714581, 0.0006301433895714581, 0.0006301433895714581, 0.0006301433895714581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006301433895714581

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47185767
Iteration 2/25 | Loss: 0.00051241
Iteration 3/25 | Loss: 0.00051241
Iteration 4/25 | Loss: 0.00051241
Iteration 5/25 | Loss: 0.00051241
Iteration 6/25 | Loss: 0.00051241
Iteration 7/25 | Loss: 0.00051241
Iteration 8/25 | Loss: 0.00051241
Iteration 9/25 | Loss: 0.00051241
Iteration 10/25 | Loss: 0.00051241
Iteration 11/25 | Loss: 0.00051241
Iteration 12/25 | Loss: 0.00051241
Iteration 13/25 | Loss: 0.00051241
Iteration 14/25 | Loss: 0.00051241
Iteration 15/25 | Loss: 0.00051241
Iteration 16/25 | Loss: 0.00051241
Iteration 17/25 | Loss: 0.00051241
Iteration 18/25 | Loss: 0.00051241
Iteration 19/25 | Loss: 0.00051241
Iteration 20/25 | Loss: 0.00051241
Iteration 21/25 | Loss: 0.00051241
Iteration 22/25 | Loss: 0.00051241
Iteration 23/25 | Loss: 0.00051241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005124069284647703, 0.0005124069284647703, 0.0005124069284647703, 0.0005124069284647703, 0.0005124069284647703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005124069284647703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051241
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001438
Iteration 4/1000 | Loss: 0.00001308
Iteration 5/1000 | Loss: 0.00001248
Iteration 6/1000 | Loss: 0.00001214
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001179
Iteration 10/1000 | Loss: 0.00001177
Iteration 11/1000 | Loss: 0.00001176
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001169
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001167
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001166
Iteration 22/1000 | Loss: 0.00001165
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001163
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001161
Iteration 32/1000 | Loss: 0.00001161
Iteration 33/1000 | Loss: 0.00001161
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001160
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001157
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001156
Iteration 47/1000 | Loss: 0.00001155
Iteration 48/1000 | Loss: 0.00001155
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001151
Iteration 63/1000 | Loss: 0.00001151
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001144
Iteration 72/1000 | Loss: 0.00001144
Iteration 73/1000 | Loss: 0.00001144
Iteration 74/1000 | Loss: 0.00001144
Iteration 75/1000 | Loss: 0.00001144
Iteration 76/1000 | Loss: 0.00001144
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001143
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001142
Iteration 84/1000 | Loss: 0.00001142
Iteration 85/1000 | Loss: 0.00001142
Iteration 86/1000 | Loss: 0.00001141
Iteration 87/1000 | Loss: 0.00001141
Iteration 88/1000 | Loss: 0.00001141
Iteration 89/1000 | Loss: 0.00001141
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001140
Iteration 93/1000 | Loss: 0.00001140
Iteration 94/1000 | Loss: 0.00001139
Iteration 95/1000 | Loss: 0.00001139
Iteration 96/1000 | Loss: 0.00001139
Iteration 97/1000 | Loss: 0.00001139
Iteration 98/1000 | Loss: 0.00001139
Iteration 99/1000 | Loss: 0.00001139
Iteration 100/1000 | Loss: 0.00001139
Iteration 101/1000 | Loss: 0.00001139
Iteration 102/1000 | Loss: 0.00001139
Iteration 103/1000 | Loss: 0.00001139
Iteration 104/1000 | Loss: 0.00001139
Iteration 105/1000 | Loss: 0.00001139
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.138817606261e-05, 1.138817606261e-05, 1.138817606261e-05, 1.138817606261e-05, 1.138817606261e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.138817606261e-05

Optimization complete. Final v2v error: 2.9297420978546143 mm

Highest mean error: 3.521562099456787 mm for frame 57

Lowest mean error: 2.5047664642333984 mm for frame 131

Saving results

Total time: 29.481215476989746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815796
Iteration 2/25 | Loss: 0.00117993
Iteration 3/25 | Loss: 0.00085947
Iteration 4/25 | Loss: 0.00079070
Iteration 5/25 | Loss: 0.00076605
Iteration 6/25 | Loss: 0.00076357
Iteration 7/25 | Loss: 0.00076697
Iteration 8/25 | Loss: 0.00075753
Iteration 9/25 | Loss: 0.00074951
Iteration 10/25 | Loss: 0.00074298
Iteration 11/25 | Loss: 0.00073098
Iteration 12/25 | Loss: 0.00072478
Iteration 13/25 | Loss: 0.00072347
Iteration 14/25 | Loss: 0.00072292
Iteration 15/25 | Loss: 0.00072277
Iteration 16/25 | Loss: 0.00072275
Iteration 17/25 | Loss: 0.00072275
Iteration 18/25 | Loss: 0.00072275
Iteration 19/25 | Loss: 0.00072274
Iteration 20/25 | Loss: 0.00072274
Iteration 21/25 | Loss: 0.00072274
Iteration 22/25 | Loss: 0.00072274
Iteration 23/25 | Loss: 0.00072274
Iteration 24/25 | Loss: 0.00072274
Iteration 25/25 | Loss: 0.00072274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90356994
Iteration 2/25 | Loss: 0.00056440
Iteration 3/25 | Loss: 0.00056439
Iteration 4/25 | Loss: 0.00056439
Iteration 5/25 | Loss: 0.00056439
Iteration 6/25 | Loss: 0.00056439
Iteration 7/25 | Loss: 0.00056439
Iteration 8/25 | Loss: 0.00056439
Iteration 9/25 | Loss: 0.00056439
Iteration 10/25 | Loss: 0.00056439
Iteration 11/25 | Loss: 0.00056439
Iteration 12/25 | Loss: 0.00056439
Iteration 13/25 | Loss: 0.00056439
Iteration 14/25 | Loss: 0.00056439
Iteration 15/25 | Loss: 0.00056439
Iteration 16/25 | Loss: 0.00056439
Iteration 17/25 | Loss: 0.00056439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005643871263600886, 0.0005643871263600886, 0.0005643871263600886, 0.0005643871263600886, 0.0005643871263600886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005643871263600886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056439
Iteration 2/1000 | Loss: 0.00005112
Iteration 3/1000 | Loss: 0.00003522
Iteration 4/1000 | Loss: 0.00002841
Iteration 5/1000 | Loss: 0.00002620
Iteration 6/1000 | Loss: 0.00002487
Iteration 7/1000 | Loss: 0.00002413
Iteration 8/1000 | Loss: 0.00070031
Iteration 9/1000 | Loss: 0.00002740
Iteration 10/1000 | Loss: 0.00002323
Iteration 11/1000 | Loss: 0.00002181
Iteration 12/1000 | Loss: 0.00002097
Iteration 13/1000 | Loss: 0.00002053
Iteration 14/1000 | Loss: 0.00002047
Iteration 15/1000 | Loss: 0.00002032
Iteration 16/1000 | Loss: 0.00002030
Iteration 17/1000 | Loss: 0.00002030
Iteration 18/1000 | Loss: 0.00002022
Iteration 19/1000 | Loss: 0.00002019
Iteration 20/1000 | Loss: 0.00002018
Iteration 21/1000 | Loss: 0.00002018
Iteration 22/1000 | Loss: 0.00002017
Iteration 23/1000 | Loss: 0.00002017
Iteration 24/1000 | Loss: 0.00002016
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002013
Iteration 27/1000 | Loss: 0.00002012
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002007
Iteration 30/1000 | Loss: 0.00002007
Iteration 31/1000 | Loss: 0.00002006
Iteration 32/1000 | Loss: 0.00002006
Iteration 33/1000 | Loss: 0.00002005
Iteration 34/1000 | Loss: 0.00002004
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00002002
Iteration 40/1000 | Loss: 0.00002001
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00002001
Iteration 43/1000 | Loss: 0.00002000
Iteration 44/1000 | Loss: 0.00002000
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001998
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001997
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001997
Iteration 53/1000 | Loss: 0.00001996
Iteration 54/1000 | Loss: 0.00001994
Iteration 55/1000 | Loss: 0.00001993
Iteration 56/1000 | Loss: 0.00001993
Iteration 57/1000 | Loss: 0.00001993
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001990
Iteration 60/1000 | Loss: 0.00001990
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001990
Iteration 63/1000 | Loss: 0.00001990
Iteration 64/1000 | Loss: 0.00001990
Iteration 65/1000 | Loss: 0.00001990
Iteration 66/1000 | Loss: 0.00001990
Iteration 67/1000 | Loss: 0.00001990
Iteration 68/1000 | Loss: 0.00001989
Iteration 69/1000 | Loss: 0.00001989
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001988
Iteration 72/1000 | Loss: 0.00001988
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001988
Iteration 76/1000 | Loss: 0.00001988
Iteration 77/1000 | Loss: 0.00001987
Iteration 78/1000 | Loss: 0.00001987
Iteration 79/1000 | Loss: 0.00001987
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001987
Iteration 82/1000 | Loss: 0.00001987
Iteration 83/1000 | Loss: 0.00001987
Iteration 84/1000 | Loss: 0.00001987
Iteration 85/1000 | Loss: 0.00001987
Iteration 86/1000 | Loss: 0.00001987
Iteration 87/1000 | Loss: 0.00001987
Iteration 88/1000 | Loss: 0.00001987
Iteration 89/1000 | Loss: 0.00001987
Iteration 90/1000 | Loss: 0.00001987
Iteration 91/1000 | Loss: 0.00001987
Iteration 92/1000 | Loss: 0.00001987
Iteration 93/1000 | Loss: 0.00001987
Iteration 94/1000 | Loss: 0.00001986
Iteration 95/1000 | Loss: 0.00001986
Iteration 96/1000 | Loss: 0.00001986
Iteration 97/1000 | Loss: 0.00001986
Iteration 98/1000 | Loss: 0.00001986
Iteration 99/1000 | Loss: 0.00001986
Iteration 100/1000 | Loss: 0.00001986
Iteration 101/1000 | Loss: 0.00001986
Iteration 102/1000 | Loss: 0.00001986
Iteration 103/1000 | Loss: 0.00001986
Iteration 104/1000 | Loss: 0.00001986
Iteration 105/1000 | Loss: 0.00001986
Iteration 106/1000 | Loss: 0.00001986
Iteration 107/1000 | Loss: 0.00001986
Iteration 108/1000 | Loss: 0.00001985
Iteration 109/1000 | Loss: 0.00001985
Iteration 110/1000 | Loss: 0.00001985
Iteration 111/1000 | Loss: 0.00001985
Iteration 112/1000 | Loss: 0.00001985
Iteration 113/1000 | Loss: 0.00001985
Iteration 114/1000 | Loss: 0.00001985
Iteration 115/1000 | Loss: 0.00001985
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001985
Iteration 120/1000 | Loss: 0.00001985
Iteration 121/1000 | Loss: 0.00001985
Iteration 122/1000 | Loss: 0.00001985
Iteration 123/1000 | Loss: 0.00001985
Iteration 124/1000 | Loss: 0.00001985
Iteration 125/1000 | Loss: 0.00001985
Iteration 126/1000 | Loss: 0.00001985
Iteration 127/1000 | Loss: 0.00001985
Iteration 128/1000 | Loss: 0.00001985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.9847409930662252e-05, 1.9847409930662252e-05, 1.9847409930662252e-05, 1.9847409930662252e-05, 1.9847409930662252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9847409930662252e-05

Optimization complete. Final v2v error: 3.8212192058563232 mm

Highest mean error: 4.483214855194092 mm for frame 116

Lowest mean error: 3.348842144012451 mm for frame 74

Saving results

Total time: 56.21505403518677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068422
Iteration 2/25 | Loss: 0.01068422
Iteration 3/25 | Loss: 0.01068422
Iteration 4/25 | Loss: 0.01068422
Iteration 5/25 | Loss: 0.01068422
Iteration 6/25 | Loss: 0.01068422
Iteration 7/25 | Loss: 0.01068421
Iteration 8/25 | Loss: 0.01068421
Iteration 9/25 | Loss: 0.01068421
Iteration 10/25 | Loss: 0.01068421
Iteration 11/25 | Loss: 0.01068421
Iteration 12/25 | Loss: 0.01068421
Iteration 13/25 | Loss: 0.01068421
Iteration 14/25 | Loss: 0.01068421
Iteration 15/25 | Loss: 0.01068421
Iteration 16/25 | Loss: 0.01068420
Iteration 17/25 | Loss: 0.01068420
Iteration 18/25 | Loss: 0.01068420
Iteration 19/25 | Loss: 0.01068420
Iteration 20/25 | Loss: 0.01068420
Iteration 21/25 | Loss: 0.01068420
Iteration 22/25 | Loss: 0.01068419
Iteration 23/25 | Loss: 0.01068419
Iteration 24/25 | Loss: 0.01068419
Iteration 25/25 | Loss: 0.01068419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45066261
Iteration 2/25 | Loss: 0.16129959
Iteration 3/25 | Loss: 0.16121341
Iteration 4/25 | Loss: 0.15900756
Iteration 5/25 | Loss: 0.15900749
Iteration 6/25 | Loss: 0.15900747
Iteration 7/25 | Loss: 0.15900746
Iteration 8/25 | Loss: 0.15900746
Iteration 9/25 | Loss: 0.15900746
Iteration 10/25 | Loss: 0.15900743
Iteration 11/25 | Loss: 0.15900743
Iteration 12/25 | Loss: 0.15900743
Iteration 13/25 | Loss: 0.15900743
Iteration 14/25 | Loss: 0.15900743
Iteration 15/25 | Loss: 0.15900743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.15900743007659912, 0.15900743007659912, 0.15900743007659912, 0.15900743007659912, 0.15900743007659912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15900743007659912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15900743
Iteration 2/1000 | Loss: 0.00383381
Iteration 3/1000 | Loss: 0.00143233
Iteration 4/1000 | Loss: 0.00120450
Iteration 5/1000 | Loss: 0.00046655
Iteration 6/1000 | Loss: 0.00027857
Iteration 7/1000 | Loss: 0.00011019
Iteration 8/1000 | Loss: 0.00019867
Iteration 9/1000 | Loss: 0.00007939
Iteration 10/1000 | Loss: 0.00006924
Iteration 11/1000 | Loss: 0.00006337
Iteration 12/1000 | Loss: 0.00005809
Iteration 13/1000 | Loss: 0.00005372
Iteration 14/1000 | Loss: 0.00147760
Iteration 15/1000 | Loss: 0.00053803
Iteration 16/1000 | Loss: 0.00005630
Iteration 17/1000 | Loss: 0.00005034
Iteration 18/1000 | Loss: 0.00004795
Iteration 19/1000 | Loss: 0.00004724
Iteration 20/1000 | Loss: 0.00004503
Iteration 21/1000 | Loss: 0.00004332
Iteration 22/1000 | Loss: 0.00004271
Iteration 23/1000 | Loss: 0.00004104
Iteration 24/1000 | Loss: 0.00004366
Iteration 25/1000 | Loss: 0.00004920
Iteration 26/1000 | Loss: 0.00004216
Iteration 27/1000 | Loss: 0.00004028
Iteration 28/1000 | Loss: 0.00003930
Iteration 29/1000 | Loss: 0.00003862
Iteration 30/1000 | Loss: 0.00003806
Iteration 31/1000 | Loss: 0.00003761
Iteration 32/1000 | Loss: 0.00003885
Iteration 33/1000 | Loss: 0.00003763
Iteration 34/1000 | Loss: 0.00003813
Iteration 35/1000 | Loss: 0.00003753
Iteration 36/1000 | Loss: 0.00003787
Iteration 37/1000 | Loss: 0.00003768
Iteration 38/1000 | Loss: 0.00003795
Iteration 39/1000 | Loss: 0.00003786
Iteration 40/1000 | Loss: 0.00003781
Iteration 41/1000 | Loss: 0.00006377
Iteration 42/1000 | Loss: 0.00005469
Iteration 43/1000 | Loss: 0.00003735
Iteration 44/1000 | Loss: 0.00003728
Iteration 45/1000 | Loss: 0.00003788
Iteration 46/1000 | Loss: 0.00003765
Iteration 47/1000 | Loss: 0.00003764
Iteration 48/1000 | Loss: 0.00014056
Iteration 49/1000 | Loss: 0.00007572
Iteration 50/1000 | Loss: 0.00004333
Iteration 51/1000 | Loss: 0.00005945
Iteration 52/1000 | Loss: 0.00008002
Iteration 53/1000 | Loss: 0.00004505
Iteration 54/1000 | Loss: 0.00004793
Iteration 55/1000 | Loss: 0.00003851
Iteration 56/1000 | Loss: 0.00010666
Iteration 57/1000 | Loss: 0.00005759
Iteration 58/1000 | Loss: 0.00005877
Iteration 59/1000 | Loss: 0.00004102
Iteration 60/1000 | Loss: 0.00004468
Iteration 61/1000 | Loss: 0.00004182
Iteration 62/1000 | Loss: 0.00004294
Iteration 63/1000 | Loss: 0.00004088
Iteration 64/1000 | Loss: 0.00009607
Iteration 65/1000 | Loss: 0.00004478
Iteration 66/1000 | Loss: 0.00007439
Iteration 67/1000 | Loss: 0.00004827
Iteration 68/1000 | Loss: 0.00005066
Iteration 69/1000 | Loss: 0.00006226
Iteration 70/1000 | Loss: 0.00005162
Iteration 71/1000 | Loss: 0.00006741
Iteration 72/1000 | Loss: 0.00005084
Iteration 73/1000 | Loss: 0.00004731
Iteration 74/1000 | Loss: 0.00004739
Iteration 75/1000 | Loss: 0.00005875
Iteration 76/1000 | Loss: 0.00004781
Iteration 77/1000 | Loss: 0.00006395
Iteration 78/1000 | Loss: 0.00005068
Iteration 79/1000 | Loss: 0.00004904
Iteration 80/1000 | Loss: 0.00004046
Iteration 81/1000 | Loss: 0.00004696
Iteration 82/1000 | Loss: 0.00005466
Iteration 83/1000 | Loss: 0.00005817
Iteration 84/1000 | Loss: 0.00005029
Iteration 85/1000 | Loss: 0.00004641
Iteration 86/1000 | Loss: 0.00005151
Iteration 87/1000 | Loss: 0.00004579
Iteration 88/1000 | Loss: 0.00005211
Iteration 89/1000 | Loss: 0.00004450
Iteration 90/1000 | Loss: 0.00005664
Iteration 91/1000 | Loss: 0.00004468
Iteration 92/1000 | Loss: 0.00005356
Iteration 93/1000 | Loss: 0.00004241
Iteration 94/1000 | Loss: 0.00004966
Iteration 95/1000 | Loss: 0.00004846
Iteration 96/1000 | Loss: 0.00006826
Iteration 97/1000 | Loss: 0.00004371
Iteration 98/1000 | Loss: 0.00005405
Iteration 99/1000 | Loss: 0.00004409
Iteration 100/1000 | Loss: 0.00006016
Iteration 101/1000 | Loss: 0.00004853
Iteration 102/1000 | Loss: 0.00006698
Iteration 103/1000 | Loss: 0.00004270
Iteration 104/1000 | Loss: 0.00004838
Iteration 105/1000 | Loss: 0.00004432
Iteration 106/1000 | Loss: 0.00005045
Iteration 107/1000 | Loss: 0.00004832
Iteration 108/1000 | Loss: 0.00005077
Iteration 109/1000 | Loss: 0.00004532
Iteration 110/1000 | Loss: 0.00006078
Iteration 111/1000 | Loss: 0.00004684
Iteration 112/1000 | Loss: 0.00006123
Iteration 113/1000 | Loss: 0.00004254
Iteration 114/1000 | Loss: 0.00003765
Iteration 115/1000 | Loss: 0.00004267
Iteration 116/1000 | Loss: 0.00003698
Iteration 117/1000 | Loss: 0.00003673
Iteration 118/1000 | Loss: 0.00003655
Iteration 119/1000 | Loss: 0.00003769
Iteration 120/1000 | Loss: 0.00008627
Iteration 121/1000 | Loss: 0.00004796
Iteration 122/1000 | Loss: 0.00005749
Iteration 123/1000 | Loss: 0.00008167
Iteration 124/1000 | Loss: 0.00004116
Iteration 125/1000 | Loss: 0.00003984
Iteration 126/1000 | Loss: 0.00003787
Iteration 127/1000 | Loss: 0.00005264
Iteration 128/1000 | Loss: 0.00004038
Iteration 129/1000 | Loss: 0.00006604
Iteration 130/1000 | Loss: 0.00004132
Iteration 131/1000 | Loss: 0.00006425
Iteration 132/1000 | Loss: 0.00004214
Iteration 133/1000 | Loss: 0.00007125
Iteration 134/1000 | Loss: 0.00005031
Iteration 135/1000 | Loss: 0.00004858
Iteration 136/1000 | Loss: 0.00003979
Iteration 137/1000 | Loss: 0.00005463
Iteration 138/1000 | Loss: 0.00004225
Iteration 139/1000 | Loss: 0.00007784
Iteration 140/1000 | Loss: 0.00005358
Iteration 141/1000 | Loss: 0.00013174
Iteration 142/1000 | Loss: 0.00003809
Iteration 143/1000 | Loss: 0.00004389
Iteration 144/1000 | Loss: 0.00004320
Iteration 145/1000 | Loss: 0.00008551
Iteration 146/1000 | Loss: 0.00004240
Iteration 147/1000 | Loss: 0.00004153
Iteration 148/1000 | Loss: 0.00005485
Iteration 149/1000 | Loss: 0.00006493
Iteration 150/1000 | Loss: 0.00005963
Iteration 151/1000 | Loss: 0.00005887
Iteration 152/1000 | Loss: 0.00005693
Iteration 153/1000 | Loss: 0.00004211
Iteration 154/1000 | Loss: 0.00004252
Iteration 155/1000 | Loss: 0.00004258
Iteration 156/1000 | Loss: 0.00004290
Iteration 157/1000 | Loss: 0.00006498
Iteration 158/1000 | Loss: 0.00004725
Iteration 159/1000 | Loss: 0.00004166
Iteration 160/1000 | Loss: 0.00004770
Iteration 161/1000 | Loss: 0.00006191
Iteration 162/1000 | Loss: 0.00005300
Iteration 163/1000 | Loss: 0.00006085
Iteration 164/1000 | Loss: 0.00004696
Iteration 165/1000 | Loss: 0.00004190
Iteration 166/1000 | Loss: 0.00004901
Iteration 167/1000 | Loss: 0.00005900
Iteration 168/1000 | Loss: 0.00005026
Iteration 169/1000 | Loss: 0.00004046
Iteration 170/1000 | Loss: 0.00004254
Iteration 171/1000 | Loss: 0.00003903
Iteration 172/1000 | Loss: 0.00004244
Iteration 173/1000 | Loss: 0.00005749
Iteration 174/1000 | Loss: 0.00004366
Iteration 175/1000 | Loss: 0.00005177
Iteration 176/1000 | Loss: 0.00004512
Iteration 177/1000 | Loss: 0.00005154
Iteration 178/1000 | Loss: 0.00004666
Iteration 179/1000 | Loss: 0.00004260
Iteration 180/1000 | Loss: 0.00004331
Iteration 181/1000 | Loss: 0.00005769
Iteration 182/1000 | Loss: 0.00004525
Iteration 183/1000 | Loss: 0.00004044
Iteration 184/1000 | Loss: 0.00004119
Iteration 185/1000 | Loss: 0.00005543
Iteration 186/1000 | Loss: 0.00004210
Iteration 187/1000 | Loss: 0.00004365
Iteration 188/1000 | Loss: 0.00004540
Iteration 189/1000 | Loss: 0.00005026
Iteration 190/1000 | Loss: 0.00004108
Iteration 191/1000 | Loss: 0.00003706
Iteration 192/1000 | Loss: 0.00003684
Iteration 193/1000 | Loss: 0.00004960
Iteration 194/1000 | Loss: 0.00004101
Iteration 195/1000 | Loss: 0.00004913
Iteration 196/1000 | Loss: 0.00006131
Iteration 197/1000 | Loss: 0.00004037
Iteration 198/1000 | Loss: 0.00005808
Iteration 199/1000 | Loss: 0.00004247
Iteration 200/1000 | Loss: 0.00004252
Iteration 201/1000 | Loss: 0.00004158
Iteration 202/1000 | Loss: 0.00004270
Iteration 203/1000 | Loss: 0.00004110
Iteration 204/1000 | Loss: 0.00004110
Iteration 205/1000 | Loss: 0.00004210
Iteration 206/1000 | Loss: 0.00004111
Iteration 207/1000 | Loss: 0.00004087
Iteration 208/1000 | Loss: 0.00005961
Iteration 209/1000 | Loss: 0.00004110
Iteration 210/1000 | Loss: 0.00004772
Iteration 211/1000 | Loss: 0.00004114
Iteration 212/1000 | Loss: 0.00004516
Iteration 213/1000 | Loss: 0.00004143
Iteration 214/1000 | Loss: 0.00004190
Iteration 215/1000 | Loss: 0.00004336
Iteration 216/1000 | Loss: 0.00006562
Iteration 217/1000 | Loss: 0.00004523
Iteration 218/1000 | Loss: 0.00004583
Iteration 219/1000 | Loss: 0.00004164
Iteration 220/1000 | Loss: 0.00004885
Iteration 221/1000 | Loss: 0.00004202
Iteration 222/1000 | Loss: 0.00004337
Iteration 223/1000 | Loss: 0.00004133
Iteration 224/1000 | Loss: 0.00006553
Iteration 225/1000 | Loss: 0.00004202
Iteration 226/1000 | Loss: 0.00005240
Iteration 227/1000 | Loss: 0.00005604
Iteration 228/1000 | Loss: 0.00005188
Iteration 229/1000 | Loss: 0.00004185
Iteration 230/1000 | Loss: 0.00006211
Iteration 231/1000 | Loss: 0.00004367
Iteration 232/1000 | Loss: 0.00005272
Iteration 233/1000 | Loss: 0.00004542
Iteration 234/1000 | Loss: 0.00004193
Iteration 235/1000 | Loss: 0.00003942
Iteration 236/1000 | Loss: 0.00005041
Iteration 237/1000 | Loss: 0.00003983
Iteration 238/1000 | Loss: 0.00004139
Iteration 239/1000 | Loss: 0.00003979
Iteration 240/1000 | Loss: 0.00004351
Iteration 241/1000 | Loss: 0.00004263
Iteration 242/1000 | Loss: 0.00004791
Iteration 243/1000 | Loss: 0.00004377
Iteration 244/1000 | Loss: 0.00004397
Iteration 245/1000 | Loss: 0.00003899
Iteration 246/1000 | Loss: 0.00003862
Iteration 247/1000 | Loss: 0.00004547
Iteration 248/1000 | Loss: 0.00005231
Iteration 249/1000 | Loss: 0.00004900
Iteration 250/1000 | Loss: 0.00005197
Iteration 251/1000 | Loss: 0.00004435
Iteration 252/1000 | Loss: 0.00005825
Iteration 253/1000 | Loss: 0.00006264
Iteration 254/1000 | Loss: 0.00005720
Iteration 255/1000 | Loss: 0.00005627
Iteration 256/1000 | Loss: 0.00004384
Iteration 257/1000 | Loss: 0.00005287
Iteration 258/1000 | Loss: 0.00004692
Iteration 259/1000 | Loss: 0.00004445
Iteration 260/1000 | Loss: 0.00004793
Iteration 261/1000 | Loss: 0.00004515
Iteration 262/1000 | Loss: 0.00005760
Iteration 263/1000 | Loss: 0.00006078
Iteration 264/1000 | Loss: 0.00004651
Iteration 265/1000 | Loss: 0.00004844
Iteration 266/1000 | Loss: 0.00003996
Iteration 267/1000 | Loss: 0.00005557
Iteration 268/1000 | Loss: 0.00004255
Iteration 269/1000 | Loss: 0.00006192
Iteration 270/1000 | Loss: 0.00004783
Iteration 271/1000 | Loss: 0.00004457
Iteration 272/1000 | Loss: 0.00004293
Iteration 273/1000 | Loss: 0.00004662
Iteration 274/1000 | Loss: 0.00004227
Iteration 275/1000 | Loss: 0.00004398
Iteration 276/1000 | Loss: 0.00004100
Iteration 277/1000 | Loss: 0.00004361
Iteration 278/1000 | Loss: 0.00004116
Iteration 279/1000 | Loss: 0.00004279
Iteration 280/1000 | Loss: 0.00004121
Iteration 281/1000 | Loss: 0.00004191
Iteration 282/1000 | Loss: 0.00004259
Iteration 283/1000 | Loss: 0.00004171
Iteration 284/1000 | Loss: 0.00004472
Iteration 285/1000 | Loss: 0.00004168
Iteration 286/1000 | Loss: 0.00004321
Iteration 287/1000 | Loss: 0.00004354
Iteration 288/1000 | Loss: 0.00004255
Iteration 289/1000 | Loss: 0.00004642
Iteration 290/1000 | Loss: 0.00004202
Iteration 291/1000 | Loss: 0.00004536
Iteration 292/1000 | Loss: 0.00004265
Iteration 293/1000 | Loss: 0.00004377
Iteration 294/1000 | Loss: 0.00005317
Iteration 295/1000 | Loss: 0.00004298
Iteration 296/1000 | Loss: 0.00005420
Iteration 297/1000 | Loss: 0.00004024
Iteration 298/1000 | Loss: 0.00004515
Iteration 299/1000 | Loss: 0.00004080
Iteration 300/1000 | Loss: 0.00003926
Iteration 301/1000 | Loss: 0.00005310
Iteration 302/1000 | Loss: 0.00004206
Iteration 303/1000 | Loss: 0.00004390
Iteration 304/1000 | Loss: 0.00004210
Iteration 305/1000 | Loss: 0.00005006
Iteration 306/1000 | Loss: 0.00004739
Iteration 307/1000 | Loss: 0.00004840
Iteration 308/1000 | Loss: 0.00004380
Iteration 309/1000 | Loss: 0.00004244
Iteration 310/1000 | Loss: 0.00004767
Iteration 311/1000 | Loss: 0.00004309
Iteration 312/1000 | Loss: 0.00005801
Iteration 313/1000 | Loss: 0.00004966
Iteration 314/1000 | Loss: 0.00005369
Iteration 315/1000 | Loss: 0.00005483
Iteration 316/1000 | Loss: 0.00005656
Iteration 317/1000 | Loss: 0.00004434
Iteration 318/1000 | Loss: 0.00004261
Iteration 319/1000 | Loss: 0.00005030
Iteration 320/1000 | Loss: 0.00004212
Iteration 321/1000 | Loss: 0.00004880
Iteration 322/1000 | Loss: 0.00005273
Iteration 323/1000 | Loss: 0.00004817
Iteration 324/1000 | Loss: 0.00005858
Iteration 325/1000 | Loss: 0.00004825
Iteration 326/1000 | Loss: 0.00004067
Iteration 327/1000 | Loss: 0.00003865
Iteration 328/1000 | Loss: 0.00004752
Iteration 329/1000 | Loss: 0.00003842
Iteration 330/1000 | Loss: 0.00003794
Iteration 331/1000 | Loss: 0.00004551
Iteration 332/1000 | Loss: 0.00005314
Iteration 333/1000 | Loss: 0.00004525
Iteration 334/1000 | Loss: 0.00005251
Iteration 335/1000 | Loss: 0.00004285
Iteration 336/1000 | Loss: 0.00004747
Iteration 337/1000 | Loss: 0.00004213
Iteration 338/1000 | Loss: 0.00004608
Iteration 339/1000 | Loss: 0.00004866
Iteration 340/1000 | Loss: 0.00004596
Iteration 341/1000 | Loss: 0.00007467
Iteration 342/1000 | Loss: 0.00004896
Iteration 343/1000 | Loss: 0.00007737
Iteration 344/1000 | Loss: 0.00005348
Iteration 345/1000 | Loss: 0.00005750
Iteration 346/1000 | Loss: 0.00006530
Iteration 347/1000 | Loss: 0.00006236
Iteration 348/1000 | Loss: 0.00005919
Iteration 349/1000 | Loss: 0.00004968
Iteration 350/1000 | Loss: 0.00004091
Iteration 351/1000 | Loss: 0.00004396
Iteration 352/1000 | Loss: 0.00005445
Iteration 353/1000 | Loss: 0.00004305
Iteration 354/1000 | Loss: 0.00006105
Iteration 355/1000 | Loss: 0.00004704
Iteration 356/1000 | Loss: 0.00003831
Iteration 357/1000 | Loss: 0.00003705
Iteration 358/1000 | Loss: 0.00003672
Iteration 359/1000 | Loss: 0.00003640
Iteration 360/1000 | Loss: 0.00003746
Iteration 361/1000 | Loss: 0.00007852
Iteration 362/1000 | Loss: 0.00003853
Iteration 363/1000 | Loss: 0.00005034
Iteration 364/1000 | Loss: 0.00004371
Iteration 365/1000 | Loss: 0.00005474
Iteration 366/1000 | Loss: 0.00004673
Iteration 367/1000 | Loss: 0.00005197
Iteration 368/1000 | Loss: 0.00004842
Iteration 369/1000 | Loss: 0.00004300
Iteration 370/1000 | Loss: 0.00005109
Iteration 371/1000 | Loss: 0.00004888
Iteration 372/1000 | Loss: 0.00005708
Iteration 373/1000 | Loss: 0.00004739
Iteration 374/1000 | Loss: 0.00004273
Iteration 375/1000 | Loss: 0.00004648
Iteration 376/1000 | Loss: 0.00004197
Iteration 377/1000 | Loss: 0.00004824
Iteration 378/1000 | Loss: 0.00004378
Iteration 379/1000 | Loss: 0.00005184
Iteration 380/1000 | Loss: 0.00006857
Iteration 381/1000 | Loss: 0.00004949
Iteration 382/1000 | Loss: 0.00004729
Iteration 383/1000 | Loss: 0.00004044
Iteration 384/1000 | Loss: 0.00004281
Iteration 385/1000 | Loss: 0.00004634
Iteration 386/1000 | Loss: 0.00004625
Iteration 387/1000 | Loss: 0.00004313
Iteration 388/1000 | Loss: 0.00005757
Iteration 389/1000 | Loss: 0.00004477
Iteration 390/1000 | Loss: 0.00004795
Iteration 391/1000 | Loss: 0.00006637
Iteration 392/1000 | Loss: 0.00004587
Iteration 393/1000 | Loss: 0.00005200
Iteration 394/1000 | Loss: 0.00004505
Iteration 395/1000 | Loss: 0.00004656
Iteration 396/1000 | Loss: 0.00004241
Iteration 397/1000 | Loss: 0.00004802
Iteration 398/1000 | Loss: 0.00004969
Iteration 399/1000 | Loss: 0.00005450
Iteration 400/1000 | Loss: 0.00004419
Iteration 401/1000 | Loss: 0.00004293
Iteration 402/1000 | Loss: 0.00004483
Iteration 403/1000 | Loss: 0.00004482
Iteration 404/1000 | Loss: 0.00004659
Iteration 405/1000 | Loss: 0.00004022
Iteration 406/1000 | Loss: 0.00004000
Iteration 407/1000 | Loss: 0.00004132
Iteration 408/1000 | Loss: 0.00003922
Iteration 409/1000 | Loss: 0.00004059
Iteration 410/1000 | Loss: 0.00004113
Iteration 411/1000 | Loss: 0.00004110
Iteration 412/1000 | Loss: 0.00004311
Iteration 413/1000 | Loss: 0.00004783
Iteration 414/1000 | Loss: 0.00003894
Iteration 415/1000 | Loss: 0.00004023
Iteration 416/1000 | Loss: 0.00004100
Iteration 417/1000 | Loss: 0.00005279
Iteration 418/1000 | Loss: 0.00004202
Iteration 419/1000 | Loss: 0.00004353
Iteration 420/1000 | Loss: 0.00004164
Iteration 421/1000 | Loss: 0.00004231
Iteration 422/1000 | Loss: 0.00006201
Iteration 423/1000 | Loss: 0.00004548
Iteration 424/1000 | Loss: 0.00004659
Iteration 425/1000 | Loss: 0.00004916
Iteration 426/1000 | Loss: 0.00004680
Iteration 427/1000 | Loss: 0.00004127
Iteration 428/1000 | Loss: 0.00006066
Iteration 429/1000 | Loss: 0.00004768
Iteration 430/1000 | Loss: 0.00007014
Iteration 431/1000 | Loss: 0.00006202
Iteration 432/1000 | Loss: 0.00005155
Iteration 433/1000 | Loss: 0.00005672
Iteration 434/1000 | Loss: 0.00004261
Iteration 435/1000 | Loss: 0.00004219
Iteration 436/1000 | Loss: 0.00004444
Iteration 437/1000 | Loss: 0.00005420
Iteration 438/1000 | Loss: 0.00004769
Iteration 439/1000 | Loss: 0.00006856
Iteration 440/1000 | Loss: 0.00004685
Iteration 441/1000 | Loss: 0.00004253
Iteration 442/1000 | Loss: 0.00004695
Iteration 443/1000 | Loss: 0.00004229
Iteration 444/1000 | Loss: 0.00005247
Iteration 445/1000 | Loss: 0.00004268
Iteration 446/1000 | Loss: 0.00004554
Iteration 447/1000 | Loss: 0.00004204
Iteration 448/1000 | Loss: 0.00004006
Iteration 449/1000 | Loss: 0.00004029
Iteration 450/1000 | Loss: 0.00005036
Iteration 451/1000 | Loss: 0.00003951
Iteration 452/1000 | Loss: 0.00004318
Iteration 453/1000 | Loss: 0.00004425
Iteration 454/1000 | Loss: 0.00004863
Iteration 455/1000 | Loss: 0.00004542
Iteration 456/1000 | Loss: 0.00004583
Iteration 457/1000 | Loss: 0.00004601
Iteration 458/1000 | Loss: 0.00004510
Iteration 459/1000 | Loss: 0.00006010
Iteration 460/1000 | Loss: 0.00005702
Iteration 461/1000 | Loss: 0.00006507
Iteration 462/1000 | Loss: 0.00005367
Iteration 463/1000 | Loss: 0.00006358
Iteration 464/1000 | Loss: 0.00007278
Iteration 465/1000 | Loss: 0.00006264
Iteration 466/1000 | Loss: 0.00004170
Iteration 467/1000 | Loss: 0.00003907
Iteration 468/1000 | Loss: 0.00003815
Iteration 469/1000 | Loss: 0.00006572
Iteration 470/1000 | Loss: 0.00004930
Iteration 471/1000 | Loss: 0.00006000
Iteration 472/1000 | Loss: 0.00004492
Iteration 473/1000 | Loss: 0.00006299
Iteration 474/1000 | Loss: 0.00004565
Iteration 475/1000 | Loss: 0.00005814
Iteration 476/1000 | Loss: 0.00004968
Iteration 477/1000 | Loss: 0.00005231
Iteration 478/1000 | Loss: 0.00004666
Iteration 479/1000 | Loss: 0.00005081
Iteration 480/1000 | Loss: 0.00003960
Iteration 481/1000 | Loss: 0.00003797
Iteration 482/1000 | Loss: 0.00006539
Iteration 483/1000 | Loss: 0.00003847
Iteration 484/1000 | Loss: 0.00005043
Iteration 485/1000 | Loss: 0.00003961
Iteration 486/1000 | Loss: 0.00005545
Iteration 487/1000 | Loss: 0.00003985
Iteration 488/1000 | Loss: 0.00004534
Iteration 489/1000 | Loss: 0.00004176
Iteration 490/1000 | Loss: 0.00005613
Iteration 491/1000 | Loss: 0.00004464
Iteration 492/1000 | Loss: 0.00003992
Iteration 493/1000 | Loss: 0.00003751
Iteration 494/1000 | Loss: 0.00003702
Iteration 495/1000 | Loss: 0.00003759
Iteration 496/1000 | Loss: 0.00005079
Iteration 497/1000 | Loss: 0.00003911
Iteration 498/1000 | Loss: 0.00005061
Iteration 499/1000 | Loss: 0.00003948
Iteration 500/1000 | Loss: 0.00004620
Iteration 501/1000 | Loss: 0.00003894
Iteration 502/1000 | Loss: 0.00006955
Iteration 503/1000 | Loss: 0.00006198
Iteration 504/1000 | Loss: 0.00006622
Iteration 505/1000 | Loss: 0.00004084
Iteration 506/1000 | Loss: 0.00008999
Iteration 507/1000 | Loss: 0.00004312
Iteration 508/1000 | Loss: 0.00006899
Iteration 509/1000 | Loss: 0.00004546
Iteration 510/1000 | Loss: 0.00008013
Iteration 511/1000 | Loss: 0.00004405
Iteration 512/1000 | Loss: 0.00005304
Iteration 513/1000 | Loss: 0.00004352
Iteration 514/1000 | Loss: 0.00004430
Iteration 515/1000 | Loss: 0.00004407
Iteration 516/1000 | Loss: 0.00004461
Iteration 517/1000 | Loss: 0.00005190
Iteration 518/1000 | Loss: 0.00004678
Iteration 519/1000 | Loss: 0.00004406
Iteration 520/1000 | Loss: 0.00004649
Iteration 521/1000 | Loss: 0.00005223
Iteration 522/1000 | Loss: 0.00004480
Iteration 523/1000 | Loss: 0.00005741
Iteration 524/1000 | Loss: 0.00006311
Iteration 525/1000 | Loss: 0.00005089
Iteration 526/1000 | Loss: 0.00006660
Iteration 527/1000 | Loss: 0.00006589
Iteration 528/1000 | Loss: 0.00004850
Iteration 529/1000 | Loss: 0.00005254
Iteration 530/1000 | Loss: 0.00003929
Iteration 531/1000 | Loss: 0.00003754
Iteration 532/1000 | Loss: 0.00003682
Iteration 533/1000 | Loss: 0.00003743
Iteration 534/1000 | Loss: 0.00007033
Iteration 535/1000 | Loss: 0.00005269
Iteration 536/1000 | Loss: 0.00003972
Iteration 537/1000 | Loss: 0.00006101
Iteration 538/1000 | Loss: 0.00004778
Iteration 539/1000 | Loss: 0.00005365
Iteration 540/1000 | Loss: 0.00004543
Iteration 541/1000 | Loss: 0.00007239
Iteration 542/1000 | Loss: 0.00004799
Iteration 543/1000 | Loss: 0.00003869
Iteration 544/1000 | Loss: 0.00003969
Iteration 545/1000 | Loss: 0.00003736
Iteration 546/1000 | Loss: 0.00003694
Iteration 547/1000 | Loss: 0.00003747
Iteration 548/1000 | Loss: 0.00004615
Iteration 549/1000 | Loss: 0.00005499
Iteration 550/1000 | Loss: 0.00004595
Iteration 551/1000 | Loss: 0.00004064
Iteration 552/1000 | Loss: 0.00004000
Iteration 553/1000 | Loss: 0.00003900
Iteration 554/1000 | Loss: 0.00003947
Iteration 555/1000 | Loss: 0.00003898
Iteration 556/1000 | Loss: 0.00003888
Iteration 557/1000 | Loss: 0.00003969
Iteration 558/1000 | Loss: 0.00003957
Iteration 559/1000 | Loss: 0.00006831
Iteration 560/1000 | Loss: 0.00005714
Iteration 561/1000 | Loss: 0.00004004
Iteration 562/1000 | Loss: 0.00003859
Iteration 563/1000 | Loss: 0.00005269
Iteration 564/1000 | Loss: 0.00015209
Iteration 565/1000 | Loss: 0.00005561
Iteration 566/1000 | Loss: 0.00004616
Iteration 567/1000 | Loss: 0.00004688
Iteration 568/1000 | Loss: 0.00005164
Iteration 569/1000 | Loss: 0.00005413
Iteration 570/1000 | Loss: 0.00004447
Iteration 571/1000 | Loss: 0.00004077
Iteration 572/1000 | Loss: 0.00004265
Iteration 573/1000 | Loss: 0.00005610
Iteration 574/1000 | Loss: 0.00004732
Iteration 575/1000 | Loss: 0.00003913
Iteration 576/1000 | Loss: 0.00004267
Iteration 577/1000 | Loss: 0.00007901
Iteration 578/1000 | Loss: 0.00005137
Iteration 579/1000 | Loss: 0.00005024
Iteration 580/1000 | Loss: 0.00004485
Iteration 581/1000 | Loss: 0.00006829
Iteration 582/1000 | Loss: 0.00005245
Iteration 583/1000 | Loss: 0.00003963
Iteration 584/1000 | Loss: 0.00004986
Iteration 585/1000 | Loss: 0.00003924
Iteration 586/1000 | Loss: 0.00004014
Iteration 587/1000 | Loss: 0.00007425
Iteration 588/1000 | Loss: 0.00004420
Iteration 589/1000 | Loss: 0.00004218
Iteration 590/1000 | Loss: 0.00004130
Iteration 591/1000 | Loss: 0.00006601
Iteration 592/1000 | Loss: 0.00004830
Iteration 593/1000 | Loss: 0.00004182
Iteration 594/1000 | Loss: 0.00003935
Iteration 595/1000 | Loss: 0.00006751
Iteration 596/1000 | Loss: 0.00004368
Iteration 597/1000 | Loss: 0.00005812
Iteration 598/1000 | Loss: 0.00004316
Iteration 599/1000 | Loss: 0.00004049
Iteration 600/1000 | Loss: 0.00003926
Iteration 601/1000 | Loss: 0.00006173
Iteration 602/1000 | Loss: 0.00004191
Iteration 603/1000 | Loss: 0.00004922
Iteration 604/1000 | Loss: 0.00004758
Iteration 605/1000 | Loss: 0.00005788
Iteration 606/1000 | Loss: 0.00004918
Iteration 607/1000 | Loss: 0.00004648
Iteration 608/1000 | Loss: 0.00004146
Iteration 609/1000 | Loss: 0.00004298
Iteration 610/1000 | Loss: 0.00004466
Iteration 611/1000 | Loss: 0.00004201
Iteration 612/1000 | Loss: 0.00004721
Iteration 613/1000 | Loss: 0.00004271
Iteration 614/1000 | Loss: 0.00004691
Iteration 615/1000 | Loss: 0.00004621
Iteration 616/1000 | Loss: 0.00004879
Iteration 617/1000 | Loss: 0.00005726
Iteration 618/1000 | Loss: 0.00006973
Iteration 619/1000 | Loss: 0.00005109
Iteration 620/1000 | Loss: 0.00004621
Iteration 621/1000 | Loss: 0.00004719
Iteration 622/1000 | Loss: 0.00004359
Iteration 623/1000 | Loss: 0.00005942
Iteration 624/1000 | Loss: 0.00005487
Iteration 625/1000 | Loss: 0.00005895
Iteration 626/1000 | Loss: 0.00005376
Iteration 627/1000 | Loss: 0.00005872
Iteration 628/1000 | Loss: 0.00004217
Iteration 629/1000 | Loss: 0.00004554
Iteration 630/1000 | Loss: 0.00004046
Iteration 631/1000 | Loss: 0.00005094
Iteration 632/1000 | Loss: 0.00004116
Iteration 633/1000 | Loss: 0.00005238
Iteration 634/1000 | Loss: 0.00004299
Iteration 635/1000 | Loss: 0.00005331
Iteration 636/1000 | Loss: 0.00004401
Iteration 637/1000 | Loss: 0.00005295
Iteration 638/1000 | Loss: 0.00004740
Iteration 639/1000 | Loss: 0.00004977
Iteration 640/1000 | Loss: 0.00004721
Iteration 641/1000 | Loss: 0.00004624
Iteration 642/1000 | Loss: 0.00004299
Iteration 643/1000 | Loss: 0.00004514
Iteration 644/1000 | Loss: 0.00006172
Iteration 645/1000 | Loss: 0.00004953
Iteration 646/1000 | Loss: 0.00005041
Iteration 647/1000 | Loss: 0.00004737
Iteration 648/1000 | Loss: 0.00004534
Iteration 649/1000 | Loss: 0.00004931
Iteration 650/1000 | Loss: 0.00004815
Iteration 651/1000 | Loss: 0.00004412
Iteration 652/1000 | Loss: 0.00004096
Iteration 653/1000 | Loss: 0.00004512
Iteration 654/1000 | Loss: 0.00004284
Iteration 655/1000 | Loss: 0.00004736
Iteration 656/1000 | Loss: 0.00006069
Iteration 657/1000 | Loss: 0.00004129
Iteration 658/1000 | Loss: 0.00004411
Iteration 659/1000 | Loss: 0.00004314
Iteration 660/1000 | Loss: 0.00004139
Iteration 661/1000 | Loss: 0.00004600
Iteration 662/1000 | Loss: 0.00005679
Iteration 663/1000 | Loss: 0.00004504
Iteration 664/1000 | Loss: 0.00003966
Iteration 665/1000 | Loss: 0.00004099
Iteration 666/1000 | Loss: 0.00003927
Iteration 667/1000 | Loss: 0.00004024
Iteration 668/1000 | Loss: 0.00004016
Iteration 669/1000 | Loss: 0.00004054
Iteration 670/1000 | Loss: 0.00004187
Iteration 671/1000 | Loss: 0.00005397
Iteration 672/1000 | Loss: 0.00004384
Iteration 673/1000 | Loss: 0.00005558
Iteration 674/1000 | Loss: 0.00004500
Iteration 675/1000 | Loss: 0.00005846
Iteration 676/1000 | Loss: 0.00004488
Iteration 677/1000 | Loss: 0.00005782
Iteration 678/1000 | Loss: 0.00006605
Iteration 679/1000 | Loss: 0.00005566
Iteration 680/1000 | Loss: 0.00005180
Iteration 681/1000 | Loss: 0.00005378
Iteration 682/1000 | Loss: 0.00005568
Iteration 683/1000 | Loss: 0.00005761
Iteration 684/1000 | Loss: 0.00007251
Iteration 685/1000 | Loss: 0.00005374
Iteration 686/1000 | Loss: 0.00006039
Iteration 687/1000 | Loss: 0.00004497
Iteration 688/1000 | Loss: 0.00005816
Iteration 689/1000 | Loss: 0.00004511
Iteration 690/1000 | Loss: 0.00005135
Iteration 691/1000 | Loss: 0.00005173
Iteration 692/1000 | Loss: 0.00004792
Iteration 693/1000 | Loss: 0.00008606
Iteration 694/1000 | Loss: 0.00006063
Iteration 695/1000 | Loss: 0.00004478
Iteration 696/1000 | Loss: 0.00004362
Iteration 697/1000 | Loss: 0.00005233
Iteration 698/1000 | Loss: 0.00004773
Iteration 699/1000 | Loss: 0.00004333
Iteration 700/1000 | Loss: 0.00004138
Iteration 701/1000 | Loss: 0.00003778
Iteration 702/1000 | Loss: 0.00003727
Iteration 703/1000 | Loss: 0.00003845
Iteration 704/1000 | Loss: 0.00004196
Iteration 705/1000 | Loss: 0.00005061
Iteration 706/1000 | Loss: 0.00004532
Iteration 707/1000 | Loss: 0.00005391
Iteration 708/1000 | Loss: 0.00004737
Iteration 709/1000 | Loss: 0.00004436
Iteration 710/1000 | Loss: 0.00005388
Iteration 711/1000 | Loss: 0.00005277
Iteration 712/1000 | Loss: 0.00004838
Iteration 713/1000 | Loss: 0.00004382
Iteration 714/1000 | Loss: 0.00004959
Iteration 715/1000 | Loss: 0.00005050
Iteration 716/1000 | Loss: 0.00004375
Iteration 717/1000 | Loss: 0.00006705
Iteration 718/1000 | Loss: 0.00004394
Iteration 719/1000 | Loss: 0.00004679
Iteration 720/1000 | Loss: 0.00005473
Iteration 721/1000 | Loss: 0.00004573
Iteration 722/1000 | Loss: 0.00004988
Iteration 723/1000 | Loss: 0.00004806
Iteration 724/1000 | Loss: 0.00005203
Iteration 725/1000 | Loss: 0.00005818
Iteration 726/1000 | Loss: 0.00005509
Iteration 727/1000 | Loss: 0.00004676
Iteration 728/1000 | Loss: 0.00004152
Iteration 729/1000 | Loss: 0.00003891
Iteration 730/1000 | Loss: 0.00004489
Iteration 731/1000 | Loss: 0.00004204
Iteration 732/1000 | Loss: 0.00007088
Iteration 733/1000 | Loss: 0.00005509
Iteration 734/1000 | Loss: 0.00003985
Iteration 735/1000 | Loss: 0.00003894
Iteration 736/1000 | Loss: 0.00004067
Iteration 737/1000 | Loss: 0.00003937
Iteration 738/1000 | Loss: 0.00004126
Iteration 739/1000 | Loss: 0.00003917
Iteration 740/1000 | Loss: 0.00004330
Iteration 741/1000 | Loss: 0.00004116
Iteration 742/1000 | Loss: 0.00004632
Iteration 743/1000 | Loss: 0.00003935
Iteration 744/1000 | Loss: 0.00004239
Iteration 745/1000 | Loss: 0.00004592
Iteration 746/1000 | Loss: 0.00004695
Iteration 747/1000 | Loss: 0.00004273
Iteration 748/1000 | Loss: 0.00006807
Iteration 749/1000 | Loss: 0.00004523
Iteration 750/1000 | Loss: 0.00004394
Iteration 751/1000 | Loss: 0.00003868
Iteration 752/1000 | Loss: 0.00004461
Iteration 753/1000 | Loss: 0.00003822
Iteration 754/1000 | Loss: 0.00004211
Iteration 755/1000 | Loss: 0.00004282
Iteration 756/1000 | Loss: 0.00004312
Iteration 757/1000 | Loss: 0.00004117
Iteration 758/1000 | Loss: 0.00004238
Iteration 759/1000 | Loss: 0.00003822
Iteration 760/1000 | Loss: 0.00004146
Iteration 761/1000 | Loss: 0.00003949
Iteration 762/1000 | Loss: 0.00005457
Iteration 763/1000 | Loss: 0.00004007
Iteration 764/1000 | Loss: 0.00006716
Iteration 765/1000 | Loss: 0.00004903
Iteration 766/1000 | Loss: 0.00003935
Iteration 767/1000 | Loss: 0.00004916
Iteration 768/1000 | Loss: 0.00004148
Iteration 769/1000 | Loss: 0.00005152
Iteration 770/1000 | Loss: 0.00004181
Iteration 771/1000 | Loss: 0.00003843
Iteration 772/1000 | Loss: 0.00004619
Iteration 773/1000 | Loss: 0.00004112
Iteration 774/1000 | Loss: 0.00005937
Iteration 775/1000 | Loss: 0.00004287
Iteration 776/1000 | Loss: 0.00004320
Iteration 777/1000 | Loss: 0.00004064
Iteration 778/1000 | Loss: 0.00004196
Iteration 779/1000 | Loss: 0.00004050
Iteration 780/1000 | Loss: 0.00006204
Iteration 781/1000 | Loss: 0.00004879
Iteration 782/1000 | Loss: 0.00003977
Iteration 783/1000 | Loss: 0.00003913
Iteration 784/1000 | Loss: 0.00004395
Iteration 785/1000 | Loss: 0.00004037
Iteration 786/1000 | Loss: 0.00004451
Iteration 787/1000 | Loss: 0.00004451
Iteration 788/1000 | Loss: 0.00004164
Iteration 789/1000 | Loss: 0.00007315
Iteration 790/1000 | Loss: 0.00004510
Iteration 791/1000 | Loss: 0.00003974
Iteration 792/1000 | Loss: 0.00004308
Iteration 793/1000 | Loss: 0.00006204
Iteration 794/1000 | Loss: 0.00004441
Iteration 795/1000 | Loss: 0.00005508
Iteration 796/1000 | Loss: 0.00004802
Iteration 797/1000 | Loss: 0.00004558
Iteration 798/1000 | Loss: 0.00004586
Iteration 799/1000 | Loss: 0.00004324
Iteration 800/1000 | Loss: 0.00005055
Iteration 801/1000 | Loss: 0.00006144
Iteration 802/1000 | Loss: 0.00004395
Iteration 803/1000 | Loss: 0.00004100
Iteration 804/1000 | Loss: 0.00004016
Iteration 805/1000 | Loss: 0.00004110
Iteration 806/1000 | Loss: 0.00004068
Iteration 807/1000 | Loss: 0.00004416
Iteration 808/1000 | Loss: 0.00004089
Iteration 809/1000 | Loss: 0.00004146
Iteration 810/1000 | Loss: 0.00004691
Iteration 811/1000 | Loss: 0.00004308
Iteration 812/1000 | Loss: 0.00004805
Iteration 813/1000 | Loss: 0.00004170
Iteration 814/1000 | Loss: 0.00005736
Iteration 815/1000 | Loss: 0.00004128
Iteration 816/1000 | Loss: 0.00004724
Iteration 817/1000 | Loss: 0.00004065
Iteration 818/1000 | Loss: 0.00004726
Iteration 819/1000 | Loss: 0.00004002
Iteration 820/1000 | Loss: 0.00004219
Iteration 821/1000 | Loss: 0.00004035
Iteration 822/1000 | Loss: 0.00004277
Iteration 823/1000 | Loss: 0.00004092
Iteration 824/1000 | Loss: 0.00004295
Iteration 825/1000 | Loss: 0.00005741
Iteration 826/1000 | Loss: 0.00005512
Iteration 827/1000 | Loss: 0.00003996
Iteration 828/1000 | Loss: 0.00004066
Iteration 829/1000 | Loss: 0.00004083
Iteration 830/1000 | Loss: 0.00004097
Iteration 831/1000 | Loss: 0.00004448
Iteration 832/1000 | Loss: 0.00004297
Iteration 833/1000 | Loss: 0.00004258
Iteration 834/1000 | Loss: 0.00004511
Iteration 835/1000 | Loss: 0.00004447
Iteration 836/1000 | Loss: 0.00003984
Iteration 837/1000 | Loss: 0.00004099
Iteration 838/1000 | Loss: 0.00004398
Iteration 839/1000 | Loss: 0.00004391
Iteration 840/1000 | Loss: 0.00004964
Iteration 841/1000 | Loss: 0.00005131
Iteration 842/1000 | Loss: 0.00004079
Iteration 843/1000 | Loss: 0.00004257
Iteration 844/1000 | Loss: 0.00004171
Iteration 845/1000 | Loss: 0.00003846
Iteration 846/1000 | Loss: 0.00003797
Iteration 847/1000 | Loss: 0.00003728
Iteration 848/1000 | Loss: 0.00006717
Iteration 849/1000 | Loss: 0.00003867
Iteration 850/1000 | Loss: 0.00005431
Iteration 851/1000 | Loss: 0.00004261
Iteration 852/1000 | Loss: 0.00006148
Iteration 853/1000 | Loss: 0.00004571
Iteration 854/1000 | Loss: 0.00004330
Iteration 855/1000 | Loss: 0.00004226
Iteration 856/1000 | Loss: 0.00004225
Iteration 857/1000 | Loss: 0.00005353
Iteration 858/1000 | Loss: 0.00005223
Iteration 859/1000 | Loss: 0.00004715
Iteration 860/1000 | Loss: 0.00004150
Iteration 861/1000 | Loss: 0.00004600
Iteration 862/1000 | Loss: 0.00005623
Iteration 863/1000 | Loss: 0.00004292
Iteration 864/1000 | Loss: 0.00004519
Iteration 865/1000 | Loss: 0.00004076
Iteration 866/1000 | Loss: 0.00004180
Iteration 867/1000 | Loss: 0.00004373
Iteration 868/1000 | Loss: 0.00004018
Iteration 869/1000 | Loss: 0.00004029
Iteration 870/1000 | Loss: 0.00004022
Iteration 871/1000 | Loss: 0.00004610
Iteration 872/1000 | Loss: 0.00004856
Iteration 873/1000 | Loss: 0.00005757
Iteration 874/1000 | Loss: 0.00004535
Iteration 875/1000 | Loss: 0.00004295
Iteration 876/1000 | Loss: 0.00005363
Iteration 877/1000 | Loss: 0.00004119
Iteration 878/1000 | Loss: 0.00005475
Iteration 879/1000 | Loss: 0.00006473
Iteration 880/1000 | Loss: 0.00008728
Iteration 881/1000 | Loss: 0.00004825
Iteration 882/1000 | Loss: 0.00004795
Iteration 883/1000 | Loss: 0.00004137
Iteration 884/1000 | Loss: 0.00006815
Iteration 885/1000 | Loss: 0.00003864
Iteration 886/1000 | Loss: 0.00003768
Iteration 887/1000 | Loss: 0.00008503
Iteration 888/1000 | Loss: 0.00005154
Iteration 889/1000 | Loss: 0.00004190
Iteration 890/1000 | Loss: 0.00005114
Iteration 891/1000 | Loss: 0.00003725
Iteration 892/1000 | Loss: 0.00003803
Iteration 893/1000 | Loss: 0.00003730
Iteration 894/1000 | Loss: 0.00005554
Iteration 895/1000 | Loss: 0.00005578
Iteration 896/1000 | Loss: 0.00004832
Iteration 897/1000 | Loss: 0.00004191
Iteration 898/1000 | Loss: 0.00004642
Iteration 899/1000 | Loss: 0.00004909
Iteration 900/1000 | Loss: 0.00004772
Iteration 901/1000 | Loss: 0.00004357
Iteration 902/1000 | Loss: 0.00004045
Iteration 903/1000 | Loss: 0.00004536
Iteration 904/1000 | Loss: 0.00004198
Iteration 905/1000 | Loss: 0.00004474
Iteration 906/1000 | Loss: 0.00004193
Iteration 907/1000 | Loss: 0.00004537
Iteration 908/1000 | Loss: 0.00004202
Iteration 909/1000 | Loss: 0.00007935
Iteration 910/1000 | Loss: 0.00005162
Iteration 911/1000 | Loss: 0.00004054
Iteration 912/1000 | Loss: 0.00004352
Iteration 913/1000 | Loss: 0.00004350
Iteration 914/1000 | Loss: 0.00004413
Iteration 915/1000 | Loss: 0.00004466
Iteration 916/1000 | Loss: 0.00004669
Iteration 917/1000 | Loss: 0.00004122
Iteration 918/1000 | Loss: 0.00004257
Iteration 919/1000 | Loss: 0.00004024
Iteration 920/1000 | Loss: 0.00004091
Iteration 921/1000 | Loss: 0.00004286
Iteration 922/1000 | Loss: 0.00004069
Iteration 923/1000 | Loss: 0.00004241
Iteration 924/1000 | Loss: 0.00003990
Iteration 925/1000 | Loss: 0.00004031
Iteration 926/1000 | Loss: 0.00004275
Iteration 927/1000 | Loss: 0.00004221
Iteration 928/1000 | Loss: 0.00004263
Iteration 929/1000 | Loss: 0.00004267
Iteration 930/1000 | Loss: 0.00003911
Iteration 931/1000 | Loss: 0.00004116
Iteration 932/1000 | Loss: 0.00004376
Iteration 933/1000 | Loss: 0.00004322
Iteration 934/1000 | Loss: 0.00004178
Iteration 935/1000 | Loss: 0.00004453
Iteration 936/1000 | Loss: 0.00004389
Iteration 937/1000 | Loss: 0.00004406
Iteration 938/1000 | Loss: 0.00005048
Iteration 939/1000 | Loss: 0.00006576
Iteration 940/1000 | Loss: 0.00005804
Iteration 941/1000 | Loss: 0.00004141
Iteration 942/1000 | Loss: 0.00005998
Iteration 943/1000 | Loss: 0.00003873
Iteration 944/1000 | Loss: 0.00005224
Iteration 945/1000 | Loss: 0.00004292
Iteration 946/1000 | Loss: 0.00005102
Iteration 947/1000 | Loss: 0.00004212
Iteration 948/1000 | Loss: 0.00004762
Iteration 949/1000 | Loss: 0.00004198
Iteration 950/1000 | Loss: 0.00004166
Iteration 951/1000 | Loss: 0.00004139
Iteration 952/1000 | Loss: 0.00005407
Iteration 953/1000 | Loss: 0.00004451
Iteration 954/1000 | Loss: 0.00004699
Iteration 955/1000 | Loss: 0.00004278
Iteration 956/1000 | Loss: 0.00004611
Iteration 957/1000 | Loss: 0.00004080
Iteration 958/1000 | Loss: 0.00005708
Iteration 959/1000 | Loss: 0.00004167
Iteration 960/1000 | Loss: 0.00005358
Iteration 961/1000 | Loss: 0.00004257
Iteration 962/1000 | Loss: 0.00005188
Iteration 963/1000 | Loss: 0.00004788
Iteration 964/1000 | Loss: 0.00004947
Iteration 965/1000 | Loss: 0.00004551
Iteration 966/1000 | Loss: 0.00003901
Iteration 967/1000 | Loss: 0.00004441
Iteration 968/1000 | Loss: 0.00004689
Iteration 969/1000 | Loss: 0.00003853
Iteration 970/1000 | Loss: 0.00005588
Iteration 971/1000 | Loss: 0.00004065
Iteration 972/1000 | Loss: 0.00004995
Iteration 973/1000 | Loss: 0.00004120
Iteration 974/1000 | Loss: 0.00004152
Iteration 975/1000 | Loss: 0.00004676
Iteration 976/1000 | Loss: 0.00004573
Iteration 977/1000 | Loss: 0.00004672
Iteration 978/1000 | Loss: 0.00004481
Iteration 979/1000 | Loss: 0.00004548
Iteration 980/1000 | Loss: 0.00003780
Iteration 981/1000 | Loss: 0.00004867
Iteration 982/1000 | Loss: 0.00004667
Iteration 983/1000 | Loss: 0.00004498
Iteration 984/1000 | Loss: 0.00004997
Iteration 985/1000 | Loss: 0.00004325
Iteration 986/1000 | Loss: 0.00006084
Iteration 987/1000 | Loss: 0.00004119
Iteration 988/1000 | Loss: 0.00005111
Iteration 989/1000 | Loss: 0.00004152
Iteration 990/1000 | Loss: 0.00004568
Iteration 991/1000 | Loss: 0.00005236
Iteration 992/1000 | Loss: 0.00005064
Iteration 993/1000 | Loss: 0.00004803
Iteration 994/1000 | Loss: 0.00004042
Iteration 995/1000 | Loss: 0.00004344
Iteration 996/1000 | Loss: 0.00004156
Iteration 997/1000 | Loss: 0.00004237
Iteration 998/1000 | Loss: 0.00004138
Iteration 999/1000 | Loss: 0.00004172
Iteration 1000/1000 | Loss: 0.00004548

Optimization complete. Final v2v error: 3.850543260574341 mm

Highest mean error: 25.406875610351562 mm for frame 204

Lowest mean error: 2.8367762565612793 mm for frame 179

Saving results

Total time: 1572.2390191555023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879000
Iteration 2/25 | Loss: 0.00092084
Iteration 3/25 | Loss: 0.00075525
Iteration 4/25 | Loss: 0.00072184
Iteration 5/25 | Loss: 0.00070990
Iteration 6/25 | Loss: 0.00070767
Iteration 7/25 | Loss: 0.00070737
Iteration 8/25 | Loss: 0.00070737
Iteration 9/25 | Loss: 0.00070737
Iteration 10/25 | Loss: 0.00070737
Iteration 11/25 | Loss: 0.00070737
Iteration 12/25 | Loss: 0.00070737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000707374420017004, 0.000707374420017004, 0.000707374420017004, 0.000707374420017004, 0.000707374420017004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000707374420017004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69545043
Iteration 2/25 | Loss: 0.00051580
Iteration 3/25 | Loss: 0.00051580
Iteration 4/25 | Loss: 0.00051580
Iteration 5/25 | Loss: 0.00051580
Iteration 6/25 | Loss: 0.00051580
Iteration 7/25 | Loss: 0.00051580
Iteration 8/25 | Loss: 0.00051580
Iteration 9/25 | Loss: 0.00051580
Iteration 10/25 | Loss: 0.00051580
Iteration 11/25 | Loss: 0.00051580
Iteration 12/25 | Loss: 0.00051580
Iteration 13/25 | Loss: 0.00051580
Iteration 14/25 | Loss: 0.00051580
Iteration 15/25 | Loss: 0.00051580
Iteration 16/25 | Loss: 0.00051580
Iteration 17/25 | Loss: 0.00051580
Iteration 18/25 | Loss: 0.00051580
Iteration 19/25 | Loss: 0.00051580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005157976411283016, 0.0005157976411283016, 0.0005157976411283016, 0.0005157976411283016, 0.0005157976411283016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005157976411283016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051580
Iteration 2/1000 | Loss: 0.00003916
Iteration 3/1000 | Loss: 0.00002301
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001876
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001746
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001731
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001725
Iteration 17/1000 | Loss: 0.00001724
Iteration 18/1000 | Loss: 0.00001723
Iteration 19/1000 | Loss: 0.00001723
Iteration 20/1000 | Loss: 0.00001722
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001721
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001716
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001702
Iteration 40/1000 | Loss: 0.00001702
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001701
Iteration 44/1000 | Loss: 0.00001701
Iteration 45/1000 | Loss: 0.00001701
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001699
Iteration 49/1000 | Loss: 0.00001699
Iteration 50/1000 | Loss: 0.00001698
Iteration 51/1000 | Loss: 0.00001698
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001696
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001695
Iteration 60/1000 | Loss: 0.00001695
Iteration 61/1000 | Loss: 0.00001695
Iteration 62/1000 | Loss: 0.00001695
Iteration 63/1000 | Loss: 0.00001695
Iteration 64/1000 | Loss: 0.00001694
Iteration 65/1000 | Loss: 0.00001694
Iteration 66/1000 | Loss: 0.00001694
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001693
Iteration 74/1000 | Loss: 0.00001693
Iteration 75/1000 | Loss: 0.00001693
Iteration 76/1000 | Loss: 0.00001693
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001693
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001693
Iteration 81/1000 | Loss: 0.00001693
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.6926333046285436e-05, 1.6926333046285436e-05, 1.6926333046285436e-05, 1.6926333046285436e-05, 1.6926333046285436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6926333046285436e-05

Optimization complete. Final v2v error: 3.5106964111328125 mm

Highest mean error: 3.9881784915924072 mm for frame 70

Lowest mean error: 3.0789976119995117 mm for frame 95

Saving results

Total time: 32.1632125377655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101794
Iteration 2/25 | Loss: 0.00313893
Iteration 3/25 | Loss: 0.00216673
Iteration 4/25 | Loss: 0.00192929
Iteration 5/25 | Loss: 0.00182728
Iteration 6/25 | Loss: 0.00181285
Iteration 7/25 | Loss: 0.00171024
Iteration 8/25 | Loss: 0.00160604
Iteration 9/25 | Loss: 0.00155182
Iteration 10/25 | Loss: 0.00140940
Iteration 11/25 | Loss: 0.00134741
Iteration 12/25 | Loss: 0.00127971
Iteration 13/25 | Loss: 0.00122756
Iteration 14/25 | Loss: 0.00120627
Iteration 15/25 | Loss: 0.00116021
Iteration 16/25 | Loss: 0.00114711
Iteration 17/25 | Loss: 0.00112808
Iteration 18/25 | Loss: 0.00112185
Iteration 19/25 | Loss: 0.00112204
Iteration 20/25 | Loss: 0.00112040
Iteration 21/25 | Loss: 0.00112603
Iteration 22/25 | Loss: 0.00113722
Iteration 23/25 | Loss: 0.00112937
Iteration 24/25 | Loss: 0.00112142
Iteration 25/25 | Loss: 0.00111841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56313300
Iteration 2/25 | Loss: 0.00683509
Iteration 3/25 | Loss: 0.00309993
Iteration 4/25 | Loss: 0.00309946
Iteration 5/25 | Loss: 0.00309946
Iteration 6/25 | Loss: 0.00309946
Iteration 7/25 | Loss: 0.00309946
Iteration 8/25 | Loss: 0.00309946
Iteration 9/25 | Loss: 0.00309946
Iteration 10/25 | Loss: 0.00309946
Iteration 11/25 | Loss: 0.00309946
Iteration 12/25 | Loss: 0.00309946
Iteration 13/25 | Loss: 0.00309946
Iteration 14/25 | Loss: 0.00309946
Iteration 15/25 | Loss: 0.00309946
Iteration 16/25 | Loss: 0.00309946
Iteration 17/25 | Loss: 0.00309946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003099455963820219, 0.003099455963820219, 0.003099455963820219, 0.003099455963820219, 0.003099455963820219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003099455963820219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309946
Iteration 2/1000 | Loss: 0.00049683
Iteration 3/1000 | Loss: 0.00513724
Iteration 4/1000 | Loss: 0.00355878
Iteration 5/1000 | Loss: 0.00034038
Iteration 6/1000 | Loss: 0.00048908
Iteration 7/1000 | Loss: 0.00068315
Iteration 8/1000 | Loss: 0.00232981
Iteration 9/1000 | Loss: 0.00042143
Iteration 10/1000 | Loss: 0.00032401
Iteration 11/1000 | Loss: 0.00043569
Iteration 12/1000 | Loss: 0.00062595
Iteration 13/1000 | Loss: 0.00082784
Iteration 14/1000 | Loss: 0.00022215
Iteration 15/1000 | Loss: 0.00020665
Iteration 16/1000 | Loss: 0.00033793
Iteration 17/1000 | Loss: 0.00214350
Iteration 18/1000 | Loss: 0.00039152
Iteration 19/1000 | Loss: 0.00042849
Iteration 20/1000 | Loss: 0.00206212
Iteration 21/1000 | Loss: 0.00661248
Iteration 22/1000 | Loss: 0.00555544
Iteration 23/1000 | Loss: 0.00519540
Iteration 24/1000 | Loss: 0.00096761
Iteration 25/1000 | Loss: 0.00391808
Iteration 26/1000 | Loss: 0.00124098
Iteration 27/1000 | Loss: 0.00043639
Iteration 28/1000 | Loss: 0.00086647
Iteration 29/1000 | Loss: 0.00031804
Iteration 30/1000 | Loss: 0.00086768
Iteration 31/1000 | Loss: 0.00064198
Iteration 32/1000 | Loss: 0.00394007
Iteration 33/1000 | Loss: 0.00370180
Iteration 34/1000 | Loss: 0.00064145
Iteration 35/1000 | Loss: 0.00026305
Iteration 36/1000 | Loss: 0.00018615
Iteration 37/1000 | Loss: 0.00140206
Iteration 38/1000 | Loss: 0.00061896
Iteration 39/1000 | Loss: 0.00107799
Iteration 40/1000 | Loss: 0.00015182
Iteration 41/1000 | Loss: 0.00013771
Iteration 42/1000 | Loss: 0.00012225
Iteration 43/1000 | Loss: 0.00069377
Iteration 44/1000 | Loss: 0.00020075
Iteration 45/1000 | Loss: 0.00024304
Iteration 46/1000 | Loss: 0.00023705
Iteration 47/1000 | Loss: 0.00099644
Iteration 48/1000 | Loss: 0.00028479
Iteration 49/1000 | Loss: 0.00013466
Iteration 50/1000 | Loss: 0.00011247
Iteration 51/1000 | Loss: 0.00010605
Iteration 52/1000 | Loss: 0.00080799
Iteration 53/1000 | Loss: 0.00014158
Iteration 54/1000 | Loss: 0.00026194
Iteration 55/1000 | Loss: 0.00021452
Iteration 56/1000 | Loss: 0.00014364
Iteration 57/1000 | Loss: 0.00012564
Iteration 58/1000 | Loss: 0.00024801
Iteration 59/1000 | Loss: 0.00010928
Iteration 60/1000 | Loss: 0.00009915
Iteration 61/1000 | Loss: 0.00012019
Iteration 62/1000 | Loss: 0.00009287
Iteration 63/1000 | Loss: 0.00009083
Iteration 64/1000 | Loss: 0.00008946
Iteration 65/1000 | Loss: 0.00008853
Iteration 66/1000 | Loss: 0.00008746
Iteration 67/1000 | Loss: 0.00008664
Iteration 68/1000 | Loss: 0.00008627
Iteration 69/1000 | Loss: 0.00008599
Iteration 70/1000 | Loss: 0.00008592
Iteration 71/1000 | Loss: 0.00008588
Iteration 72/1000 | Loss: 0.00008588
Iteration 73/1000 | Loss: 0.00008586
Iteration 74/1000 | Loss: 0.00008585
Iteration 75/1000 | Loss: 0.00042775
Iteration 76/1000 | Loss: 0.00023847
Iteration 77/1000 | Loss: 0.00102424
Iteration 78/1000 | Loss: 0.00049455
Iteration 79/1000 | Loss: 0.00056689
Iteration 80/1000 | Loss: 0.00023682
Iteration 81/1000 | Loss: 0.00013668
Iteration 82/1000 | Loss: 0.00008835
Iteration 83/1000 | Loss: 0.00008660
Iteration 84/1000 | Loss: 0.00008421
Iteration 85/1000 | Loss: 0.00008275
Iteration 86/1000 | Loss: 0.00008195
Iteration 87/1000 | Loss: 0.00008159
Iteration 88/1000 | Loss: 0.00008133
Iteration 89/1000 | Loss: 0.00008128
Iteration 90/1000 | Loss: 0.00008118
Iteration 91/1000 | Loss: 0.00085031
Iteration 92/1000 | Loss: 0.00008246
Iteration 93/1000 | Loss: 0.00008124
Iteration 94/1000 | Loss: 0.00008111
Iteration 95/1000 | Loss: 0.00008109
Iteration 96/1000 | Loss: 0.00008105
Iteration 97/1000 | Loss: 0.00008104
Iteration 98/1000 | Loss: 0.00008102
Iteration 99/1000 | Loss: 0.00008102
Iteration 100/1000 | Loss: 0.00008101
Iteration 101/1000 | Loss: 0.00008101
Iteration 102/1000 | Loss: 0.00008101
Iteration 103/1000 | Loss: 0.00008101
Iteration 104/1000 | Loss: 0.00008101
Iteration 105/1000 | Loss: 0.00008100
Iteration 106/1000 | Loss: 0.00008100
Iteration 107/1000 | Loss: 0.00008100
Iteration 108/1000 | Loss: 0.00008100
Iteration 109/1000 | Loss: 0.00008100
Iteration 110/1000 | Loss: 0.00008100
Iteration 111/1000 | Loss: 0.00008100
Iteration 112/1000 | Loss: 0.00008099
Iteration 113/1000 | Loss: 0.00008099
Iteration 114/1000 | Loss: 0.00008099
Iteration 115/1000 | Loss: 0.00008099
Iteration 116/1000 | Loss: 0.00008098
Iteration 117/1000 | Loss: 0.00008098
Iteration 118/1000 | Loss: 0.00008097
Iteration 119/1000 | Loss: 0.00008096
Iteration 120/1000 | Loss: 0.00008096
Iteration 121/1000 | Loss: 0.00008095
Iteration 122/1000 | Loss: 0.00008095
Iteration 123/1000 | Loss: 0.00008095
Iteration 124/1000 | Loss: 0.00008095
Iteration 125/1000 | Loss: 0.00008095
Iteration 126/1000 | Loss: 0.00008095
Iteration 127/1000 | Loss: 0.00008095
Iteration 128/1000 | Loss: 0.00008094
Iteration 129/1000 | Loss: 0.00008094
Iteration 130/1000 | Loss: 0.00008094
Iteration 131/1000 | Loss: 0.00008094
Iteration 132/1000 | Loss: 0.00008094
Iteration 133/1000 | Loss: 0.00008094
Iteration 134/1000 | Loss: 0.00008094
Iteration 135/1000 | Loss: 0.00008094
Iteration 136/1000 | Loss: 0.00008094
Iteration 137/1000 | Loss: 0.00008094
Iteration 138/1000 | Loss: 0.00008093
Iteration 139/1000 | Loss: 0.00008093
Iteration 140/1000 | Loss: 0.00008093
Iteration 141/1000 | Loss: 0.00008093
Iteration 142/1000 | Loss: 0.00008093
Iteration 143/1000 | Loss: 0.00008093
Iteration 144/1000 | Loss: 0.00008093
Iteration 145/1000 | Loss: 0.00008093
Iteration 146/1000 | Loss: 0.00008093
Iteration 147/1000 | Loss: 0.00008093
Iteration 148/1000 | Loss: 0.00008093
Iteration 149/1000 | Loss: 0.00008093
Iteration 150/1000 | Loss: 0.00008092
Iteration 151/1000 | Loss: 0.00008092
Iteration 152/1000 | Loss: 0.00008092
Iteration 153/1000 | Loss: 0.00008092
Iteration 154/1000 | Loss: 0.00008092
Iteration 155/1000 | Loss: 0.00008091
Iteration 156/1000 | Loss: 0.00008091
Iteration 157/1000 | Loss: 0.00008091
Iteration 158/1000 | Loss: 0.00008091
Iteration 159/1000 | Loss: 0.00008091
Iteration 160/1000 | Loss: 0.00008090
Iteration 161/1000 | Loss: 0.00008090
Iteration 162/1000 | Loss: 0.00008090
Iteration 163/1000 | Loss: 0.00008090
Iteration 164/1000 | Loss: 0.00008090
Iteration 165/1000 | Loss: 0.00008089
Iteration 166/1000 | Loss: 0.00008089
Iteration 167/1000 | Loss: 0.00008089
Iteration 168/1000 | Loss: 0.00008089
Iteration 169/1000 | Loss: 0.00008089
Iteration 170/1000 | Loss: 0.00008089
Iteration 171/1000 | Loss: 0.00008089
Iteration 172/1000 | Loss: 0.00008089
Iteration 173/1000 | Loss: 0.00008089
Iteration 174/1000 | Loss: 0.00008089
Iteration 175/1000 | Loss: 0.00008089
Iteration 176/1000 | Loss: 0.00008089
Iteration 177/1000 | Loss: 0.00008089
Iteration 178/1000 | Loss: 0.00008089
Iteration 179/1000 | Loss: 0.00008089
Iteration 180/1000 | Loss: 0.00008089
Iteration 181/1000 | Loss: 0.00008089
Iteration 182/1000 | Loss: 0.00008089
Iteration 183/1000 | Loss: 0.00008089
Iteration 184/1000 | Loss: 0.00008089
Iteration 185/1000 | Loss: 0.00008089
Iteration 186/1000 | Loss: 0.00008089
Iteration 187/1000 | Loss: 0.00008089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [8.089011680567637e-05, 8.089011680567637e-05, 8.089011680567637e-05, 8.089011680567637e-05, 8.089011680567637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.089011680567637e-05

Optimization complete. Final v2v error: 5.660645008087158 mm

Highest mean error: 13.366186141967773 mm for frame 37

Lowest mean error: 4.517354488372803 mm for frame 161

Saving results

Total time: 183.2308807373047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507009
Iteration 2/25 | Loss: 0.00082498
Iteration 3/25 | Loss: 0.00069110
Iteration 4/25 | Loss: 0.00067575
Iteration 5/25 | Loss: 0.00067100
Iteration 6/25 | Loss: 0.00067006
Iteration 7/25 | Loss: 0.00067006
Iteration 8/25 | Loss: 0.00067006
Iteration 9/25 | Loss: 0.00067006
Iteration 10/25 | Loss: 0.00067006
Iteration 11/25 | Loss: 0.00067006
Iteration 12/25 | Loss: 0.00067006
Iteration 13/25 | Loss: 0.00067006
Iteration 14/25 | Loss: 0.00067006
Iteration 15/25 | Loss: 0.00067006
Iteration 16/25 | Loss: 0.00067006
Iteration 17/25 | Loss: 0.00067006
Iteration 18/25 | Loss: 0.00067006
Iteration 19/25 | Loss: 0.00067006
Iteration 20/25 | Loss: 0.00067006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006700626690872014, 0.0006700626690872014, 0.0006700626690872014, 0.0006700626690872014, 0.0006700626690872014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006700626690872014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.04472208
Iteration 2/25 | Loss: 0.00045731
Iteration 3/25 | Loss: 0.00045730
Iteration 4/25 | Loss: 0.00045730
Iteration 5/25 | Loss: 0.00045730
Iteration 6/25 | Loss: 0.00045730
Iteration 7/25 | Loss: 0.00045730
Iteration 8/25 | Loss: 0.00045730
Iteration 9/25 | Loss: 0.00045730
Iteration 10/25 | Loss: 0.00045730
Iteration 11/25 | Loss: 0.00045730
Iteration 12/25 | Loss: 0.00045730
Iteration 13/25 | Loss: 0.00045730
Iteration 14/25 | Loss: 0.00045730
Iteration 15/25 | Loss: 0.00045730
Iteration 16/25 | Loss: 0.00045730
Iteration 17/25 | Loss: 0.00045730
Iteration 18/25 | Loss: 0.00045730
Iteration 19/25 | Loss: 0.00045730
Iteration 20/25 | Loss: 0.00045730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00045730170677416027, 0.00045730170677416027, 0.00045730170677416027, 0.00045730170677416027, 0.00045730170677416027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045730170677416027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045730
Iteration 2/1000 | Loss: 0.00003327
Iteration 3/1000 | Loss: 0.00002227
Iteration 4/1000 | Loss: 0.00002033
Iteration 5/1000 | Loss: 0.00001912
Iteration 6/1000 | Loss: 0.00001856
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001746
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001735
Iteration 15/1000 | Loss: 0.00001732
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001726
Iteration 26/1000 | Loss: 0.00001726
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001711
Iteration 33/1000 | Loss: 0.00001711
Iteration 34/1000 | Loss: 0.00001711
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001710
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001710
Iteration 44/1000 | Loss: 0.00001710
Iteration 45/1000 | Loss: 0.00001708
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001708
Iteration 48/1000 | Loss: 0.00001708
Iteration 49/1000 | Loss: 0.00001708
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00001707
Iteration 53/1000 | Loss: 0.00001707
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001703
Iteration 63/1000 | Loss: 0.00001703
Iteration 64/1000 | Loss: 0.00001703
Iteration 65/1000 | Loss: 0.00001703
Iteration 66/1000 | Loss: 0.00001703
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001702
Iteration 71/1000 | Loss: 0.00001702
Iteration 72/1000 | Loss: 0.00001702
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001700
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001699
Iteration 89/1000 | Loss: 0.00001699
Iteration 90/1000 | Loss: 0.00001699
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001699
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001698
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001698
Iteration 106/1000 | Loss: 0.00001698
Iteration 107/1000 | Loss: 0.00001698
Iteration 108/1000 | Loss: 0.00001698
Iteration 109/1000 | Loss: 0.00001698
Iteration 110/1000 | Loss: 0.00001698
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001697
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001697
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001697
Iteration 117/1000 | Loss: 0.00001697
Iteration 118/1000 | Loss: 0.00001697
Iteration 119/1000 | Loss: 0.00001697
Iteration 120/1000 | Loss: 0.00001697
Iteration 121/1000 | Loss: 0.00001697
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001696
Iteration 132/1000 | Loss: 0.00001696
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001696
Iteration 135/1000 | Loss: 0.00001696
Iteration 136/1000 | Loss: 0.00001696
Iteration 137/1000 | Loss: 0.00001695
Iteration 138/1000 | Loss: 0.00001695
Iteration 139/1000 | Loss: 0.00001695
Iteration 140/1000 | Loss: 0.00001695
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001695
Iteration 143/1000 | Loss: 0.00001695
Iteration 144/1000 | Loss: 0.00001695
Iteration 145/1000 | Loss: 0.00001695
Iteration 146/1000 | Loss: 0.00001695
Iteration 147/1000 | Loss: 0.00001695
Iteration 148/1000 | Loss: 0.00001695
Iteration 149/1000 | Loss: 0.00001695
Iteration 150/1000 | Loss: 0.00001695
Iteration 151/1000 | Loss: 0.00001695
Iteration 152/1000 | Loss: 0.00001695
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001695
Iteration 156/1000 | Loss: 0.00001695
Iteration 157/1000 | Loss: 0.00001695
Iteration 158/1000 | Loss: 0.00001695
Iteration 159/1000 | Loss: 0.00001695
Iteration 160/1000 | Loss: 0.00001695
Iteration 161/1000 | Loss: 0.00001695
Iteration 162/1000 | Loss: 0.00001694
Iteration 163/1000 | Loss: 0.00001694
Iteration 164/1000 | Loss: 0.00001694
Iteration 165/1000 | Loss: 0.00001694
Iteration 166/1000 | Loss: 0.00001694
Iteration 167/1000 | Loss: 0.00001694
Iteration 168/1000 | Loss: 0.00001694
Iteration 169/1000 | Loss: 0.00001694
Iteration 170/1000 | Loss: 0.00001694
Iteration 171/1000 | Loss: 0.00001694
Iteration 172/1000 | Loss: 0.00001694
Iteration 173/1000 | Loss: 0.00001694
Iteration 174/1000 | Loss: 0.00001694
Iteration 175/1000 | Loss: 0.00001694
Iteration 176/1000 | Loss: 0.00001694
Iteration 177/1000 | Loss: 0.00001694
Iteration 178/1000 | Loss: 0.00001694
Iteration 179/1000 | Loss: 0.00001694
Iteration 180/1000 | Loss: 0.00001694
Iteration 181/1000 | Loss: 0.00001694
Iteration 182/1000 | Loss: 0.00001694
Iteration 183/1000 | Loss: 0.00001694
Iteration 184/1000 | Loss: 0.00001694
Iteration 185/1000 | Loss: 0.00001694
Iteration 186/1000 | Loss: 0.00001694
Iteration 187/1000 | Loss: 0.00001694
Iteration 188/1000 | Loss: 0.00001694
Iteration 189/1000 | Loss: 0.00001694
Iteration 190/1000 | Loss: 0.00001694
Iteration 191/1000 | Loss: 0.00001694
Iteration 192/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.693824742687866e-05, 1.693824742687866e-05, 1.693824742687866e-05, 1.693824742687866e-05, 1.693824742687866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.693824742687866e-05

Optimization complete. Final v2v error: 3.5608317852020264 mm

Highest mean error: 4.051717281341553 mm for frame 44

Lowest mean error: 2.8269052505493164 mm for frame 0

Saving results

Total time: 36.701149702072144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423409
Iteration 2/25 | Loss: 0.00088867
Iteration 3/25 | Loss: 0.00078549
Iteration 4/25 | Loss: 0.00075244
Iteration 5/25 | Loss: 0.00073933
Iteration 6/25 | Loss: 0.00073713
Iteration 7/25 | Loss: 0.00073676
Iteration 8/25 | Loss: 0.00073676
Iteration 9/25 | Loss: 0.00073676
Iteration 10/25 | Loss: 0.00073676
Iteration 11/25 | Loss: 0.00073676
Iteration 12/25 | Loss: 0.00073676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000736755842808634, 0.000736755842808634, 0.000736755842808634, 0.000736755842808634, 0.000736755842808634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000736755842808634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89817500
Iteration 2/25 | Loss: 0.00043514
Iteration 3/25 | Loss: 0.00043514
Iteration 4/25 | Loss: 0.00043514
Iteration 5/25 | Loss: 0.00043514
Iteration 6/25 | Loss: 0.00043514
Iteration 7/25 | Loss: 0.00043514
Iteration 8/25 | Loss: 0.00043513
Iteration 9/25 | Loss: 0.00043513
Iteration 10/25 | Loss: 0.00043513
Iteration 11/25 | Loss: 0.00043513
Iteration 12/25 | Loss: 0.00043513
Iteration 13/25 | Loss: 0.00043513
Iteration 14/25 | Loss: 0.00043513
Iteration 15/25 | Loss: 0.00043513
Iteration 16/25 | Loss: 0.00043513
Iteration 17/25 | Loss: 0.00043513
Iteration 18/25 | Loss: 0.00043513
Iteration 19/25 | Loss: 0.00043513
Iteration 20/25 | Loss: 0.00043513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004351344832684845, 0.0004351344832684845, 0.0004351344832684845, 0.0004351344832684845, 0.0004351344832684845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004351344832684845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043513
Iteration 2/1000 | Loss: 0.00004793
Iteration 3/1000 | Loss: 0.00003220
Iteration 4/1000 | Loss: 0.00002958
Iteration 5/1000 | Loss: 0.00002728
Iteration 6/1000 | Loss: 0.00002641
Iteration 7/1000 | Loss: 0.00002541
Iteration 8/1000 | Loss: 0.00002487
Iteration 9/1000 | Loss: 0.00002458
Iteration 10/1000 | Loss: 0.00002454
Iteration 11/1000 | Loss: 0.00002446
Iteration 12/1000 | Loss: 0.00002441
Iteration 13/1000 | Loss: 0.00002441
Iteration 14/1000 | Loss: 0.00002440
Iteration 15/1000 | Loss: 0.00002438
Iteration 16/1000 | Loss: 0.00002436
Iteration 17/1000 | Loss: 0.00002435
Iteration 18/1000 | Loss: 0.00002434
Iteration 19/1000 | Loss: 0.00002434
Iteration 20/1000 | Loss: 0.00002433
Iteration 21/1000 | Loss: 0.00002433
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002428
Iteration 24/1000 | Loss: 0.00002427
Iteration 25/1000 | Loss: 0.00002427
Iteration 26/1000 | Loss: 0.00002426
Iteration 27/1000 | Loss: 0.00002426
Iteration 28/1000 | Loss: 0.00002426
Iteration 29/1000 | Loss: 0.00002426
Iteration 30/1000 | Loss: 0.00002425
Iteration 31/1000 | Loss: 0.00002425
Iteration 32/1000 | Loss: 0.00002425
Iteration 33/1000 | Loss: 0.00002424
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002424
Iteration 36/1000 | Loss: 0.00002424
Iteration 37/1000 | Loss: 0.00002424
Iteration 38/1000 | Loss: 0.00002423
Iteration 39/1000 | Loss: 0.00002423
Iteration 40/1000 | Loss: 0.00002423
Iteration 41/1000 | Loss: 0.00002422
Iteration 42/1000 | Loss: 0.00002422
Iteration 43/1000 | Loss: 0.00002422
Iteration 44/1000 | Loss: 0.00002422
Iteration 45/1000 | Loss: 0.00002422
Iteration 46/1000 | Loss: 0.00002422
Iteration 47/1000 | Loss: 0.00002421
Iteration 48/1000 | Loss: 0.00002421
Iteration 49/1000 | Loss: 0.00002421
Iteration 50/1000 | Loss: 0.00002421
Iteration 51/1000 | Loss: 0.00002421
Iteration 52/1000 | Loss: 0.00002421
Iteration 53/1000 | Loss: 0.00002421
Iteration 54/1000 | Loss: 0.00002421
Iteration 55/1000 | Loss: 0.00002421
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002421
Iteration 58/1000 | Loss: 0.00002420
Iteration 59/1000 | Loss: 0.00002420
Iteration 60/1000 | Loss: 0.00002420
Iteration 61/1000 | Loss: 0.00002420
Iteration 62/1000 | Loss: 0.00002420
Iteration 63/1000 | Loss: 0.00002420
Iteration 64/1000 | Loss: 0.00002420
Iteration 65/1000 | Loss: 0.00002419
Iteration 66/1000 | Loss: 0.00002419
Iteration 67/1000 | Loss: 0.00002419
Iteration 68/1000 | Loss: 0.00002419
Iteration 69/1000 | Loss: 0.00002418
Iteration 70/1000 | Loss: 0.00002418
Iteration 71/1000 | Loss: 0.00002417
Iteration 72/1000 | Loss: 0.00002417
Iteration 73/1000 | Loss: 0.00002417
Iteration 74/1000 | Loss: 0.00002417
Iteration 75/1000 | Loss: 0.00002417
Iteration 76/1000 | Loss: 0.00002416
Iteration 77/1000 | Loss: 0.00002416
Iteration 78/1000 | Loss: 0.00002416
Iteration 79/1000 | Loss: 0.00002416
Iteration 80/1000 | Loss: 0.00002416
Iteration 81/1000 | Loss: 0.00002416
Iteration 82/1000 | Loss: 0.00002416
Iteration 83/1000 | Loss: 0.00002416
Iteration 84/1000 | Loss: 0.00002416
Iteration 85/1000 | Loss: 0.00002415
Iteration 86/1000 | Loss: 0.00002415
Iteration 87/1000 | Loss: 0.00002415
Iteration 88/1000 | Loss: 0.00002415
Iteration 89/1000 | Loss: 0.00002414
Iteration 90/1000 | Loss: 0.00002414
Iteration 91/1000 | Loss: 0.00002414
Iteration 92/1000 | Loss: 0.00002414
Iteration 93/1000 | Loss: 0.00002414
Iteration 94/1000 | Loss: 0.00002414
Iteration 95/1000 | Loss: 0.00002414
Iteration 96/1000 | Loss: 0.00002413
Iteration 97/1000 | Loss: 0.00002413
Iteration 98/1000 | Loss: 0.00002413
Iteration 99/1000 | Loss: 0.00002412
Iteration 100/1000 | Loss: 0.00002412
Iteration 101/1000 | Loss: 0.00002412
Iteration 102/1000 | Loss: 0.00002411
Iteration 103/1000 | Loss: 0.00002411
Iteration 104/1000 | Loss: 0.00002411
Iteration 105/1000 | Loss: 0.00002411
Iteration 106/1000 | Loss: 0.00002411
Iteration 107/1000 | Loss: 0.00002410
Iteration 108/1000 | Loss: 0.00002410
Iteration 109/1000 | Loss: 0.00002410
Iteration 110/1000 | Loss: 0.00002410
Iteration 111/1000 | Loss: 0.00002410
Iteration 112/1000 | Loss: 0.00002410
Iteration 113/1000 | Loss: 0.00002410
Iteration 114/1000 | Loss: 0.00002410
Iteration 115/1000 | Loss: 0.00002410
Iteration 116/1000 | Loss: 0.00002409
Iteration 117/1000 | Loss: 0.00002409
Iteration 118/1000 | Loss: 0.00002409
Iteration 119/1000 | Loss: 0.00002409
Iteration 120/1000 | Loss: 0.00002408
Iteration 121/1000 | Loss: 0.00002408
Iteration 122/1000 | Loss: 0.00002408
Iteration 123/1000 | Loss: 0.00002408
Iteration 124/1000 | Loss: 0.00002408
Iteration 125/1000 | Loss: 0.00002408
Iteration 126/1000 | Loss: 0.00002408
Iteration 127/1000 | Loss: 0.00002408
Iteration 128/1000 | Loss: 0.00002408
Iteration 129/1000 | Loss: 0.00002408
Iteration 130/1000 | Loss: 0.00002408
Iteration 131/1000 | Loss: 0.00002408
Iteration 132/1000 | Loss: 0.00002407
Iteration 133/1000 | Loss: 0.00002407
Iteration 134/1000 | Loss: 0.00002407
Iteration 135/1000 | Loss: 0.00002407
Iteration 136/1000 | Loss: 0.00002407
Iteration 137/1000 | Loss: 0.00002407
Iteration 138/1000 | Loss: 0.00002406
Iteration 139/1000 | Loss: 0.00002406
Iteration 140/1000 | Loss: 0.00002406
Iteration 141/1000 | Loss: 0.00002406
Iteration 142/1000 | Loss: 0.00002406
Iteration 143/1000 | Loss: 0.00002406
Iteration 144/1000 | Loss: 0.00002406
Iteration 145/1000 | Loss: 0.00002406
Iteration 146/1000 | Loss: 0.00002406
Iteration 147/1000 | Loss: 0.00002406
Iteration 148/1000 | Loss: 0.00002406
Iteration 149/1000 | Loss: 0.00002406
Iteration 150/1000 | Loss: 0.00002406
Iteration 151/1000 | Loss: 0.00002406
Iteration 152/1000 | Loss: 0.00002406
Iteration 153/1000 | Loss: 0.00002406
Iteration 154/1000 | Loss: 0.00002406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.4061244403128512e-05, 2.4061244403128512e-05, 2.4061244403128512e-05, 2.4061244403128512e-05, 2.4061244403128512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4061244403128512e-05

Optimization complete. Final v2v error: 4.149348258972168 mm

Highest mean error: 4.740171432495117 mm for frame 104

Lowest mean error: 3.6523377895355225 mm for frame 129

Saving results

Total time: 36.34052896499634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397308
Iteration 2/25 | Loss: 0.00092818
Iteration 3/25 | Loss: 0.00079976
Iteration 4/25 | Loss: 0.00077539
Iteration 5/25 | Loss: 0.00076670
Iteration 6/25 | Loss: 0.00076472
Iteration 7/25 | Loss: 0.00076472
Iteration 8/25 | Loss: 0.00076472
Iteration 9/25 | Loss: 0.00076472
Iteration 10/25 | Loss: 0.00076472
Iteration 11/25 | Loss: 0.00076472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007647150196135044, 0.0007647150196135044, 0.0007647150196135044, 0.0007647150196135044, 0.0007647150196135044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007647150196135044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41481233
Iteration 2/25 | Loss: 0.00042130
Iteration 3/25 | Loss: 0.00042130
Iteration 4/25 | Loss: 0.00042130
Iteration 5/25 | Loss: 0.00042129
Iteration 6/25 | Loss: 0.00042129
Iteration 7/25 | Loss: 0.00042129
Iteration 8/25 | Loss: 0.00042129
Iteration 9/25 | Loss: 0.00042129
Iteration 10/25 | Loss: 0.00042129
Iteration 11/25 | Loss: 0.00042129
Iteration 12/25 | Loss: 0.00042129
Iteration 13/25 | Loss: 0.00042129
Iteration 14/25 | Loss: 0.00042129
Iteration 15/25 | Loss: 0.00042129
Iteration 16/25 | Loss: 0.00042129
Iteration 17/25 | Loss: 0.00042129
Iteration 18/25 | Loss: 0.00042129
Iteration 19/25 | Loss: 0.00042129
Iteration 20/25 | Loss: 0.00042129
Iteration 21/25 | Loss: 0.00042129
Iteration 22/25 | Loss: 0.00042129
Iteration 23/25 | Loss: 0.00042129
Iteration 24/25 | Loss: 0.00042129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00042129395296797156, 0.00042129395296797156, 0.00042129395296797156, 0.00042129395296797156, 0.00042129395296797156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042129395296797156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042129
Iteration 2/1000 | Loss: 0.00008092
Iteration 3/1000 | Loss: 0.00004867
Iteration 4/1000 | Loss: 0.00004437
Iteration 5/1000 | Loss: 0.00004113
Iteration 6/1000 | Loss: 0.00003878
Iteration 7/1000 | Loss: 0.00003771
Iteration 8/1000 | Loss: 0.00003621
Iteration 9/1000 | Loss: 0.00003544
Iteration 10/1000 | Loss: 0.00003495
Iteration 11/1000 | Loss: 0.00003465
Iteration 12/1000 | Loss: 0.00003435
Iteration 13/1000 | Loss: 0.00003416
Iteration 14/1000 | Loss: 0.00003402
Iteration 15/1000 | Loss: 0.00003388
Iteration 16/1000 | Loss: 0.00003385
Iteration 17/1000 | Loss: 0.00003378
Iteration 18/1000 | Loss: 0.00003369
Iteration 19/1000 | Loss: 0.00003364
Iteration 20/1000 | Loss: 0.00003361
Iteration 21/1000 | Loss: 0.00003361
Iteration 22/1000 | Loss: 0.00003360
Iteration 23/1000 | Loss: 0.00003360
Iteration 24/1000 | Loss: 0.00003356
Iteration 25/1000 | Loss: 0.00003356
Iteration 26/1000 | Loss: 0.00003356
Iteration 27/1000 | Loss: 0.00003355
Iteration 28/1000 | Loss: 0.00003355
Iteration 29/1000 | Loss: 0.00003354
Iteration 30/1000 | Loss: 0.00003354
Iteration 31/1000 | Loss: 0.00003354
Iteration 32/1000 | Loss: 0.00003353
Iteration 33/1000 | Loss: 0.00003353
Iteration 34/1000 | Loss: 0.00003353
Iteration 35/1000 | Loss: 0.00003352
Iteration 36/1000 | Loss: 0.00003352
Iteration 37/1000 | Loss: 0.00003352
Iteration 38/1000 | Loss: 0.00003352
Iteration 39/1000 | Loss: 0.00003351
Iteration 40/1000 | Loss: 0.00003351
Iteration 41/1000 | Loss: 0.00003351
Iteration 42/1000 | Loss: 0.00003351
Iteration 43/1000 | Loss: 0.00003351
Iteration 44/1000 | Loss: 0.00003350
Iteration 45/1000 | Loss: 0.00003350
Iteration 46/1000 | Loss: 0.00003350
Iteration 47/1000 | Loss: 0.00003350
Iteration 48/1000 | Loss: 0.00003350
Iteration 49/1000 | Loss: 0.00003349
Iteration 50/1000 | Loss: 0.00003349
Iteration 51/1000 | Loss: 0.00003349
Iteration 52/1000 | Loss: 0.00003349
Iteration 53/1000 | Loss: 0.00003349
Iteration 54/1000 | Loss: 0.00003348
Iteration 55/1000 | Loss: 0.00003348
Iteration 56/1000 | Loss: 0.00003348
Iteration 57/1000 | Loss: 0.00003348
Iteration 58/1000 | Loss: 0.00003348
Iteration 59/1000 | Loss: 0.00003348
Iteration 60/1000 | Loss: 0.00003348
Iteration 61/1000 | Loss: 0.00003348
Iteration 62/1000 | Loss: 0.00003348
Iteration 63/1000 | Loss: 0.00003348
Iteration 64/1000 | Loss: 0.00003347
Iteration 65/1000 | Loss: 0.00003347
Iteration 66/1000 | Loss: 0.00003347
Iteration 67/1000 | Loss: 0.00003347
Iteration 68/1000 | Loss: 0.00003347
Iteration 69/1000 | Loss: 0.00003347
Iteration 70/1000 | Loss: 0.00003347
Iteration 71/1000 | Loss: 0.00003347
Iteration 72/1000 | Loss: 0.00003347
Iteration 73/1000 | Loss: 0.00003346
Iteration 74/1000 | Loss: 0.00003346
Iteration 75/1000 | Loss: 0.00003346
Iteration 76/1000 | Loss: 0.00003346
Iteration 77/1000 | Loss: 0.00003346
Iteration 78/1000 | Loss: 0.00003345
Iteration 79/1000 | Loss: 0.00003345
Iteration 80/1000 | Loss: 0.00003345
Iteration 81/1000 | Loss: 0.00003345
Iteration 82/1000 | Loss: 0.00003345
Iteration 83/1000 | Loss: 0.00003345
Iteration 84/1000 | Loss: 0.00003345
Iteration 85/1000 | Loss: 0.00003345
Iteration 86/1000 | Loss: 0.00003345
Iteration 87/1000 | Loss: 0.00003344
Iteration 88/1000 | Loss: 0.00003344
Iteration 89/1000 | Loss: 0.00003344
Iteration 90/1000 | Loss: 0.00003344
Iteration 91/1000 | Loss: 0.00003344
Iteration 92/1000 | Loss: 0.00003344
Iteration 93/1000 | Loss: 0.00003343
Iteration 94/1000 | Loss: 0.00003343
Iteration 95/1000 | Loss: 0.00003343
Iteration 96/1000 | Loss: 0.00003343
Iteration 97/1000 | Loss: 0.00003343
Iteration 98/1000 | Loss: 0.00003343
Iteration 99/1000 | Loss: 0.00003342
Iteration 100/1000 | Loss: 0.00003342
Iteration 101/1000 | Loss: 0.00003342
Iteration 102/1000 | Loss: 0.00003342
Iteration 103/1000 | Loss: 0.00003342
Iteration 104/1000 | Loss: 0.00003342
Iteration 105/1000 | Loss: 0.00003342
Iteration 106/1000 | Loss: 0.00003342
Iteration 107/1000 | Loss: 0.00003341
Iteration 108/1000 | Loss: 0.00003341
Iteration 109/1000 | Loss: 0.00003341
Iteration 110/1000 | Loss: 0.00003341
Iteration 111/1000 | Loss: 0.00003341
Iteration 112/1000 | Loss: 0.00003341
Iteration 113/1000 | Loss: 0.00003341
Iteration 114/1000 | Loss: 0.00003341
Iteration 115/1000 | Loss: 0.00003341
Iteration 116/1000 | Loss: 0.00003341
Iteration 117/1000 | Loss: 0.00003341
Iteration 118/1000 | Loss: 0.00003341
Iteration 119/1000 | Loss: 0.00003341
Iteration 120/1000 | Loss: 0.00003341
Iteration 121/1000 | Loss: 0.00003341
Iteration 122/1000 | Loss: 0.00003341
Iteration 123/1000 | Loss: 0.00003341
Iteration 124/1000 | Loss: 0.00003341
Iteration 125/1000 | Loss: 0.00003341
Iteration 126/1000 | Loss: 0.00003341
Iteration 127/1000 | Loss: 0.00003341
Iteration 128/1000 | Loss: 0.00003341
Iteration 129/1000 | Loss: 0.00003341
Iteration 130/1000 | Loss: 0.00003341
Iteration 131/1000 | Loss: 0.00003341
Iteration 132/1000 | Loss: 0.00003341
Iteration 133/1000 | Loss: 0.00003341
Iteration 134/1000 | Loss: 0.00003341
Iteration 135/1000 | Loss: 0.00003341
Iteration 136/1000 | Loss: 0.00003341
Iteration 137/1000 | Loss: 0.00003341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [3.340785042382777e-05, 3.340785042382777e-05, 3.340785042382777e-05, 3.340785042382777e-05, 3.340785042382777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.340785042382777e-05

Optimization complete. Final v2v error: 4.648893356323242 mm

Highest mean error: 5.040435314178467 mm for frame 85

Lowest mean error: 4.3484320640563965 mm for frame 246

Saving results

Total time: 46.493884801864624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505229
Iteration 2/25 | Loss: 0.00184267
Iteration 3/25 | Loss: 0.00158985
Iteration 4/25 | Loss: 0.00156557
Iteration 5/25 | Loss: 0.00155673
Iteration 6/25 | Loss: 0.00155412
Iteration 7/25 | Loss: 0.00155387
Iteration 8/25 | Loss: 0.00155387
Iteration 9/25 | Loss: 0.00155387
Iteration 10/25 | Loss: 0.00155387
Iteration 11/25 | Loss: 0.00155387
Iteration 12/25 | Loss: 0.00155387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015538674779236317, 0.0015538674779236317, 0.0015538674779236317, 0.0015538674779236317, 0.0015538674779236317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015538674779236317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15422428
Iteration 2/25 | Loss: 0.00226237
Iteration 3/25 | Loss: 0.00226235
Iteration 4/25 | Loss: 0.00226235
Iteration 5/25 | Loss: 0.00226235
Iteration 6/25 | Loss: 0.00226235
Iteration 7/25 | Loss: 0.00226235
Iteration 8/25 | Loss: 0.00226235
Iteration 9/25 | Loss: 0.00226235
Iteration 10/25 | Loss: 0.00226235
Iteration 11/25 | Loss: 0.00226235
Iteration 12/25 | Loss: 0.00226235
Iteration 13/25 | Loss: 0.00226235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0022623492404818535, 0.0022623492404818535, 0.0022623492404818535, 0.0022623492404818535, 0.0022623492404818535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022623492404818535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226235
Iteration 2/1000 | Loss: 0.00008293
Iteration 3/1000 | Loss: 0.00005960
Iteration 4/1000 | Loss: 0.00005038
Iteration 5/1000 | Loss: 0.00004564
Iteration 6/1000 | Loss: 0.00004186
Iteration 7/1000 | Loss: 0.00003936
Iteration 8/1000 | Loss: 0.00003838
Iteration 9/1000 | Loss: 0.00003751
Iteration 10/1000 | Loss: 0.00003703
Iteration 11/1000 | Loss: 0.00003658
Iteration 12/1000 | Loss: 0.00003619
Iteration 13/1000 | Loss: 0.00003581
Iteration 14/1000 | Loss: 0.00003555
Iteration 15/1000 | Loss: 0.00003536
Iteration 16/1000 | Loss: 0.00003533
Iteration 17/1000 | Loss: 0.00003525
Iteration 18/1000 | Loss: 0.00003517
Iteration 19/1000 | Loss: 0.00003517
Iteration 20/1000 | Loss: 0.00003517
Iteration 21/1000 | Loss: 0.00003516
Iteration 22/1000 | Loss: 0.00003515
Iteration 23/1000 | Loss: 0.00003515
Iteration 24/1000 | Loss: 0.00003515
Iteration 25/1000 | Loss: 0.00003514
Iteration 26/1000 | Loss: 0.00003513
Iteration 27/1000 | Loss: 0.00003512
Iteration 28/1000 | Loss: 0.00003512
Iteration 29/1000 | Loss: 0.00003512
Iteration 30/1000 | Loss: 0.00003511
Iteration 31/1000 | Loss: 0.00003511
Iteration 32/1000 | Loss: 0.00003511
Iteration 33/1000 | Loss: 0.00003511
Iteration 34/1000 | Loss: 0.00003511
Iteration 35/1000 | Loss: 0.00003511
Iteration 36/1000 | Loss: 0.00003511
Iteration 37/1000 | Loss: 0.00003510
Iteration 38/1000 | Loss: 0.00003510
Iteration 39/1000 | Loss: 0.00003510
Iteration 40/1000 | Loss: 0.00003509
Iteration 41/1000 | Loss: 0.00003509
Iteration 42/1000 | Loss: 0.00003509
Iteration 43/1000 | Loss: 0.00003508
Iteration 44/1000 | Loss: 0.00003508
Iteration 45/1000 | Loss: 0.00003507
Iteration 46/1000 | Loss: 0.00003507
Iteration 47/1000 | Loss: 0.00003507
Iteration 48/1000 | Loss: 0.00003506
Iteration 49/1000 | Loss: 0.00003506
Iteration 50/1000 | Loss: 0.00003506
Iteration 51/1000 | Loss: 0.00003505
Iteration 52/1000 | Loss: 0.00003505
Iteration 53/1000 | Loss: 0.00003505
Iteration 54/1000 | Loss: 0.00003505
Iteration 55/1000 | Loss: 0.00003504
Iteration 56/1000 | Loss: 0.00003504
Iteration 57/1000 | Loss: 0.00003504
Iteration 58/1000 | Loss: 0.00003504
Iteration 59/1000 | Loss: 0.00003504
Iteration 60/1000 | Loss: 0.00003504
Iteration 61/1000 | Loss: 0.00003504
Iteration 62/1000 | Loss: 0.00003504
Iteration 63/1000 | Loss: 0.00003504
Iteration 64/1000 | Loss: 0.00003504
Iteration 65/1000 | Loss: 0.00003504
Iteration 66/1000 | Loss: 0.00003503
Iteration 67/1000 | Loss: 0.00003503
Iteration 68/1000 | Loss: 0.00003503
Iteration 69/1000 | Loss: 0.00003503
Iteration 70/1000 | Loss: 0.00003503
Iteration 71/1000 | Loss: 0.00003503
Iteration 72/1000 | Loss: 0.00003503
Iteration 73/1000 | Loss: 0.00003501
Iteration 74/1000 | Loss: 0.00003500
Iteration 75/1000 | Loss: 0.00003499
Iteration 76/1000 | Loss: 0.00003498
Iteration 77/1000 | Loss: 0.00003497
Iteration 78/1000 | Loss: 0.00003497
Iteration 79/1000 | Loss: 0.00003497
Iteration 80/1000 | Loss: 0.00003497
Iteration 81/1000 | Loss: 0.00003497
Iteration 82/1000 | Loss: 0.00003497
Iteration 83/1000 | Loss: 0.00003496
Iteration 84/1000 | Loss: 0.00003496
Iteration 85/1000 | Loss: 0.00003496
Iteration 86/1000 | Loss: 0.00003494
Iteration 87/1000 | Loss: 0.00003494
Iteration 88/1000 | Loss: 0.00003494
Iteration 89/1000 | Loss: 0.00003493
Iteration 90/1000 | Loss: 0.00003493
Iteration 91/1000 | Loss: 0.00003493
Iteration 92/1000 | Loss: 0.00003493
Iteration 93/1000 | Loss: 0.00003492
Iteration 94/1000 | Loss: 0.00003492
Iteration 95/1000 | Loss: 0.00003492
Iteration 96/1000 | Loss: 0.00003492
Iteration 97/1000 | Loss: 0.00003492
Iteration 98/1000 | Loss: 0.00003491
Iteration 99/1000 | Loss: 0.00003491
Iteration 100/1000 | Loss: 0.00003491
Iteration 101/1000 | Loss: 0.00003491
Iteration 102/1000 | Loss: 0.00003491
Iteration 103/1000 | Loss: 0.00003491
Iteration 104/1000 | Loss: 0.00003491
Iteration 105/1000 | Loss: 0.00003491
Iteration 106/1000 | Loss: 0.00003491
Iteration 107/1000 | Loss: 0.00003491
Iteration 108/1000 | Loss: 0.00003491
Iteration 109/1000 | Loss: 0.00003491
Iteration 110/1000 | Loss: 0.00003490
Iteration 111/1000 | Loss: 0.00003490
Iteration 112/1000 | Loss: 0.00003490
Iteration 113/1000 | Loss: 0.00003490
Iteration 114/1000 | Loss: 0.00003490
Iteration 115/1000 | Loss: 0.00003489
Iteration 116/1000 | Loss: 0.00003489
Iteration 117/1000 | Loss: 0.00003489
Iteration 118/1000 | Loss: 0.00003489
Iteration 119/1000 | Loss: 0.00003489
Iteration 120/1000 | Loss: 0.00003489
Iteration 121/1000 | Loss: 0.00003489
Iteration 122/1000 | Loss: 0.00003488
Iteration 123/1000 | Loss: 0.00003488
Iteration 124/1000 | Loss: 0.00003487
Iteration 125/1000 | Loss: 0.00003487
Iteration 126/1000 | Loss: 0.00003487
Iteration 127/1000 | Loss: 0.00003486
Iteration 128/1000 | Loss: 0.00003486
Iteration 129/1000 | Loss: 0.00003485
Iteration 130/1000 | Loss: 0.00003485
Iteration 131/1000 | Loss: 0.00003485
Iteration 132/1000 | Loss: 0.00003485
Iteration 133/1000 | Loss: 0.00003485
Iteration 134/1000 | Loss: 0.00003485
Iteration 135/1000 | Loss: 0.00003485
Iteration 136/1000 | Loss: 0.00003484
Iteration 137/1000 | Loss: 0.00003484
Iteration 138/1000 | Loss: 0.00003484
Iteration 139/1000 | Loss: 0.00003484
Iteration 140/1000 | Loss: 0.00003484
Iteration 141/1000 | Loss: 0.00003483
Iteration 142/1000 | Loss: 0.00003483
Iteration 143/1000 | Loss: 0.00003483
Iteration 144/1000 | Loss: 0.00003482
Iteration 145/1000 | Loss: 0.00003482
Iteration 146/1000 | Loss: 0.00003482
Iteration 147/1000 | Loss: 0.00003481
Iteration 148/1000 | Loss: 0.00003481
Iteration 149/1000 | Loss: 0.00003481
Iteration 150/1000 | Loss: 0.00003481
Iteration 151/1000 | Loss: 0.00003481
Iteration 152/1000 | Loss: 0.00003481
Iteration 153/1000 | Loss: 0.00003481
Iteration 154/1000 | Loss: 0.00003481
Iteration 155/1000 | Loss: 0.00003481
Iteration 156/1000 | Loss: 0.00003481
Iteration 157/1000 | Loss: 0.00003481
Iteration 158/1000 | Loss: 0.00003481
Iteration 159/1000 | Loss: 0.00003481
Iteration 160/1000 | Loss: 0.00003481
Iteration 161/1000 | Loss: 0.00003480
Iteration 162/1000 | Loss: 0.00003480
Iteration 163/1000 | Loss: 0.00003480
Iteration 164/1000 | Loss: 0.00003480
Iteration 165/1000 | Loss: 0.00003480
Iteration 166/1000 | Loss: 0.00003480
Iteration 167/1000 | Loss: 0.00003480
Iteration 168/1000 | Loss: 0.00003479
Iteration 169/1000 | Loss: 0.00003479
Iteration 170/1000 | Loss: 0.00003479
Iteration 171/1000 | Loss: 0.00003479
Iteration 172/1000 | Loss: 0.00003478
Iteration 173/1000 | Loss: 0.00003478
Iteration 174/1000 | Loss: 0.00003478
Iteration 175/1000 | Loss: 0.00003478
Iteration 176/1000 | Loss: 0.00003478
Iteration 177/1000 | Loss: 0.00003478
Iteration 178/1000 | Loss: 0.00003478
Iteration 179/1000 | Loss: 0.00003478
Iteration 180/1000 | Loss: 0.00003478
Iteration 181/1000 | Loss: 0.00003478
Iteration 182/1000 | Loss: 0.00003478
Iteration 183/1000 | Loss: 0.00003478
Iteration 184/1000 | Loss: 0.00003478
Iteration 185/1000 | Loss: 0.00003478
Iteration 186/1000 | Loss: 0.00003478
Iteration 187/1000 | Loss: 0.00003478
Iteration 188/1000 | Loss: 0.00003478
Iteration 189/1000 | Loss: 0.00003478
Iteration 190/1000 | Loss: 0.00003478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [3.477559221209958e-05, 3.477559221209958e-05, 3.477559221209958e-05, 3.477559221209958e-05, 3.477559221209958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.477559221209958e-05

Optimization complete. Final v2v error: 4.66719388961792 mm

Highest mean error: 6.271839618682861 mm for frame 72

Lowest mean error: 3.719459295272827 mm for frame 35

Saving results

Total time: 45.03544068336487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074150
Iteration 2/25 | Loss: 0.01074149
Iteration 3/25 | Loss: 0.01074149
Iteration 4/25 | Loss: 0.01074149
Iteration 5/25 | Loss: 0.01074148
Iteration 6/25 | Loss: 0.01074148
Iteration 7/25 | Loss: 0.01074148
Iteration 8/25 | Loss: 0.01074148
Iteration 9/25 | Loss: 0.01074147
Iteration 10/25 | Loss: 0.01074147
Iteration 11/25 | Loss: 0.01074147
Iteration 12/25 | Loss: 0.01074146
Iteration 13/25 | Loss: 0.01074146
Iteration 14/25 | Loss: 0.00291952
Iteration 15/25 | Loss: 0.00235121
Iteration 16/25 | Loss: 0.00247346
Iteration 17/25 | Loss: 0.00231378
Iteration 18/25 | Loss: 0.00190092
Iteration 19/25 | Loss: 0.00180767
Iteration 20/25 | Loss: 0.00175086
Iteration 21/25 | Loss: 0.00169678
Iteration 22/25 | Loss: 0.00167521
Iteration 23/25 | Loss: 0.00164480
Iteration 24/25 | Loss: 0.00163829
Iteration 25/25 | Loss: 0.00164180

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16633391
Iteration 2/25 | Loss: 0.00322982
Iteration 3/25 | Loss: 0.00322981
Iteration 4/25 | Loss: 0.00322981
Iteration 5/25 | Loss: 0.00322981
Iteration 6/25 | Loss: 0.00322981
Iteration 7/25 | Loss: 0.00322981
Iteration 8/25 | Loss: 0.00322981
Iteration 9/25 | Loss: 0.00322981
Iteration 10/25 | Loss: 0.00322981
Iteration 11/25 | Loss: 0.00322981
Iteration 12/25 | Loss: 0.00322981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0032298092264682055, 0.0032298092264682055, 0.0032298092264682055, 0.0032298092264682055, 0.0032298092264682055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032298092264682055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322981
Iteration 2/1000 | Loss: 0.00058330
Iteration 3/1000 | Loss: 0.00088386
Iteration 4/1000 | Loss: 0.00050030
Iteration 5/1000 | Loss: 0.00061600
Iteration 6/1000 | Loss: 0.00023727
Iteration 7/1000 | Loss: 0.00043456
Iteration 8/1000 | Loss: 0.00022861
Iteration 9/1000 | Loss: 0.00043226
Iteration 10/1000 | Loss: 0.00037978
Iteration 11/1000 | Loss: 0.00038249
Iteration 12/1000 | Loss: 0.00044985
Iteration 13/1000 | Loss: 0.00035732
Iteration 14/1000 | Loss: 0.00045242
Iteration 15/1000 | Loss: 0.00027124
Iteration 16/1000 | Loss: 0.00026429
Iteration 17/1000 | Loss: 0.00032550
Iteration 18/1000 | Loss: 0.00018829
Iteration 19/1000 | Loss: 0.00007645
Iteration 20/1000 | Loss: 0.00006258
Iteration 21/1000 | Loss: 0.00004996
Iteration 22/1000 | Loss: 0.00004354
Iteration 23/1000 | Loss: 0.00004141
Iteration 24/1000 | Loss: 0.00003638
Iteration 25/1000 | Loss: 0.00003401
Iteration 26/1000 | Loss: 0.00003228
Iteration 27/1000 | Loss: 0.00003120
Iteration 28/1000 | Loss: 0.00003024
Iteration 29/1000 | Loss: 0.00003286
Iteration 30/1000 | Loss: 0.00017538
Iteration 31/1000 | Loss: 0.00003386
Iteration 32/1000 | Loss: 0.00003066
Iteration 33/1000 | Loss: 0.00002917
Iteration 34/1000 | Loss: 0.00002846
Iteration 35/1000 | Loss: 0.00002818
Iteration 36/1000 | Loss: 0.00002791
Iteration 37/1000 | Loss: 0.00002889
Iteration 38/1000 | Loss: 0.00002772
Iteration 39/1000 | Loss: 0.00002755
Iteration 40/1000 | Loss: 0.00002748
Iteration 41/1000 | Loss: 0.00002736
Iteration 42/1000 | Loss: 0.00002736
Iteration 43/1000 | Loss: 0.00002736
Iteration 44/1000 | Loss: 0.00002736
Iteration 45/1000 | Loss: 0.00002735
Iteration 46/1000 | Loss: 0.00002734
Iteration 47/1000 | Loss: 0.00002731
Iteration 48/1000 | Loss: 0.00002731
Iteration 49/1000 | Loss: 0.00002731
Iteration 50/1000 | Loss: 0.00002731
Iteration 51/1000 | Loss: 0.00002731
Iteration 52/1000 | Loss: 0.00002731
Iteration 53/1000 | Loss: 0.00002731
Iteration 54/1000 | Loss: 0.00002731
Iteration 55/1000 | Loss: 0.00002731
Iteration 56/1000 | Loss: 0.00002730
Iteration 57/1000 | Loss: 0.00007012
Iteration 58/1000 | Loss: 0.00004242
Iteration 59/1000 | Loss: 0.00003081
Iteration 60/1000 | Loss: 0.00002885
Iteration 61/1000 | Loss: 0.00002814
Iteration 62/1000 | Loss: 0.00006776
Iteration 63/1000 | Loss: 0.00004275
Iteration 64/1000 | Loss: 0.00005999
Iteration 65/1000 | Loss: 0.00006394
Iteration 66/1000 | Loss: 0.00003760
Iteration 67/1000 | Loss: 0.00003291
Iteration 68/1000 | Loss: 0.00002844
Iteration 69/1000 | Loss: 0.00002639
Iteration 70/1000 | Loss: 0.00002565
Iteration 71/1000 | Loss: 0.00002534
Iteration 72/1000 | Loss: 0.00002520
Iteration 73/1000 | Loss: 0.00002518
Iteration 74/1000 | Loss: 0.00002514
Iteration 75/1000 | Loss: 0.00002514
Iteration 76/1000 | Loss: 0.00002513
Iteration 77/1000 | Loss: 0.00002509
Iteration 78/1000 | Loss: 0.00002501
Iteration 79/1000 | Loss: 0.00002501
Iteration 80/1000 | Loss: 0.00002501
Iteration 81/1000 | Loss: 0.00002501
Iteration 82/1000 | Loss: 0.00002501
Iteration 83/1000 | Loss: 0.00002501
Iteration 84/1000 | Loss: 0.00002501
Iteration 85/1000 | Loss: 0.00002501
Iteration 86/1000 | Loss: 0.00002501
Iteration 87/1000 | Loss: 0.00002500
Iteration 88/1000 | Loss: 0.00002500
Iteration 89/1000 | Loss: 0.00002500
Iteration 90/1000 | Loss: 0.00002500
Iteration 91/1000 | Loss: 0.00002500
Iteration 92/1000 | Loss: 0.00002500
Iteration 93/1000 | Loss: 0.00002500
Iteration 94/1000 | Loss: 0.00002500
Iteration 95/1000 | Loss: 0.00002500
Iteration 96/1000 | Loss: 0.00002500
Iteration 97/1000 | Loss: 0.00002500
Iteration 98/1000 | Loss: 0.00002499
Iteration 99/1000 | Loss: 0.00002499
Iteration 100/1000 | Loss: 0.00002498
Iteration 101/1000 | Loss: 0.00002498
Iteration 102/1000 | Loss: 0.00002498
Iteration 103/1000 | Loss: 0.00002498
Iteration 104/1000 | Loss: 0.00002498
Iteration 105/1000 | Loss: 0.00002498
Iteration 106/1000 | Loss: 0.00002498
Iteration 107/1000 | Loss: 0.00002498
Iteration 108/1000 | Loss: 0.00002498
Iteration 109/1000 | Loss: 0.00002498
Iteration 110/1000 | Loss: 0.00002498
Iteration 111/1000 | Loss: 0.00002498
Iteration 112/1000 | Loss: 0.00002498
Iteration 113/1000 | Loss: 0.00002498
Iteration 114/1000 | Loss: 0.00002498
Iteration 115/1000 | Loss: 0.00002498
Iteration 116/1000 | Loss: 0.00002498
Iteration 117/1000 | Loss: 0.00002498
Iteration 118/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.4979488443932496e-05, 2.4979488443932496e-05, 2.4979488443932496e-05, 2.4979488443932496e-05, 2.4979488443932496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4979488443932496e-05

Optimization complete. Final v2v error: 3.963634729385376 mm

Highest mean error: 16.533952713012695 mm for frame 3

Lowest mean error: 3.4642128944396973 mm for frame 156

Saving results

Total time: 126.24152541160583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097206
Iteration 2/25 | Loss: 0.00250038
Iteration 3/25 | Loss: 0.00197871
Iteration 4/25 | Loss: 0.00189513
Iteration 5/25 | Loss: 0.00164751
Iteration 6/25 | Loss: 0.00161220
Iteration 7/25 | Loss: 0.00158707
Iteration 8/25 | Loss: 0.00158159
Iteration 9/25 | Loss: 0.00158026
Iteration 10/25 | Loss: 0.00157957
Iteration 11/25 | Loss: 0.00157934
Iteration 12/25 | Loss: 0.00157925
Iteration 13/25 | Loss: 0.00157917
Iteration 14/25 | Loss: 0.00157903
Iteration 15/25 | Loss: 0.00157879
Iteration 16/25 | Loss: 0.00157863
Iteration 17/25 | Loss: 0.00157852
Iteration 18/25 | Loss: 0.00157847
Iteration 19/25 | Loss: 0.00157847
Iteration 20/25 | Loss: 0.00157847
Iteration 21/25 | Loss: 0.00157846
Iteration 22/25 | Loss: 0.00157846
Iteration 23/25 | Loss: 0.00157846
Iteration 24/25 | Loss: 0.00157846
Iteration 25/25 | Loss: 0.00157845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15508926
Iteration 2/25 | Loss: 0.00234972
Iteration 3/25 | Loss: 0.00234972
Iteration 4/25 | Loss: 0.00234972
Iteration 5/25 | Loss: 0.00234972
Iteration 6/25 | Loss: 0.00234972
Iteration 7/25 | Loss: 0.00234972
Iteration 8/25 | Loss: 0.00234972
Iteration 9/25 | Loss: 0.00234972
Iteration 10/25 | Loss: 0.00234972
Iteration 11/25 | Loss: 0.00234972
Iteration 12/25 | Loss: 0.00234972
Iteration 13/25 | Loss: 0.00234972
Iteration 14/25 | Loss: 0.00234972
Iteration 15/25 | Loss: 0.00234972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002349721034988761, 0.002349721034988761, 0.002349721034988761, 0.002349721034988761, 0.002349721034988761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002349721034988761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234972
Iteration 2/1000 | Loss: 0.00040902
Iteration 3/1000 | Loss: 0.00022890
Iteration 4/1000 | Loss: 0.00048063
Iteration 5/1000 | Loss: 0.00017162
Iteration 6/1000 | Loss: 0.00016666
Iteration 7/1000 | Loss: 0.00021392
Iteration 8/1000 | Loss: 0.00011980
Iteration 9/1000 | Loss: 0.00006802
Iteration 10/1000 | Loss: 0.00025636
Iteration 11/1000 | Loss: 0.00055093
Iteration 12/1000 | Loss: 0.00033214
Iteration 13/1000 | Loss: 0.00028683
Iteration 14/1000 | Loss: 0.00005821
Iteration 15/1000 | Loss: 0.00005132
Iteration 16/1000 | Loss: 0.00004765
Iteration 17/1000 | Loss: 0.00016687
Iteration 18/1000 | Loss: 0.00035664
Iteration 19/1000 | Loss: 0.00017039
Iteration 20/1000 | Loss: 0.00019046
Iteration 21/1000 | Loss: 0.00004305
Iteration 22/1000 | Loss: 0.00004162
Iteration 23/1000 | Loss: 0.00009335
Iteration 24/1000 | Loss: 0.00030054
Iteration 25/1000 | Loss: 0.00033555
Iteration 26/1000 | Loss: 0.00074792
Iteration 27/1000 | Loss: 0.00027228
Iteration 28/1000 | Loss: 0.00004439
Iteration 29/1000 | Loss: 0.00005835
Iteration 30/1000 | Loss: 0.00003963
Iteration 31/1000 | Loss: 0.00003898
Iteration 32/1000 | Loss: 0.00003844
Iteration 33/1000 | Loss: 0.00003793
Iteration 34/1000 | Loss: 0.00003743
Iteration 35/1000 | Loss: 0.00003717
Iteration 36/1000 | Loss: 0.00003702
Iteration 37/1000 | Loss: 0.00003695
Iteration 38/1000 | Loss: 0.00003686
Iteration 39/1000 | Loss: 0.00003686
Iteration 40/1000 | Loss: 0.00003686
Iteration 41/1000 | Loss: 0.00003686
Iteration 42/1000 | Loss: 0.00012576
Iteration 43/1000 | Loss: 0.00018317
Iteration 44/1000 | Loss: 0.00037351
Iteration 45/1000 | Loss: 0.00078487
Iteration 46/1000 | Loss: 0.00019304
Iteration 47/1000 | Loss: 0.00019632
Iteration 48/1000 | Loss: 0.00036961
Iteration 49/1000 | Loss: 0.00031704
Iteration 50/1000 | Loss: 0.00062347
Iteration 51/1000 | Loss: 0.00012527
Iteration 52/1000 | Loss: 0.00005306
Iteration 53/1000 | Loss: 0.00005890
Iteration 54/1000 | Loss: 0.00007084
Iteration 55/1000 | Loss: 0.00003646
Iteration 56/1000 | Loss: 0.00003530
Iteration 57/1000 | Loss: 0.00003457
Iteration 58/1000 | Loss: 0.00003431
Iteration 59/1000 | Loss: 0.00003413
Iteration 60/1000 | Loss: 0.00003400
Iteration 61/1000 | Loss: 0.00003400
Iteration 62/1000 | Loss: 0.00003400
Iteration 63/1000 | Loss: 0.00003393
Iteration 64/1000 | Loss: 0.00003393
Iteration 65/1000 | Loss: 0.00003391
Iteration 66/1000 | Loss: 0.00003391
Iteration 67/1000 | Loss: 0.00003387
Iteration 68/1000 | Loss: 0.00003387
Iteration 69/1000 | Loss: 0.00003387
Iteration 70/1000 | Loss: 0.00003386
Iteration 71/1000 | Loss: 0.00003386
Iteration 72/1000 | Loss: 0.00003386
Iteration 73/1000 | Loss: 0.00003386
Iteration 74/1000 | Loss: 0.00003386
Iteration 75/1000 | Loss: 0.00003386
Iteration 76/1000 | Loss: 0.00003386
Iteration 77/1000 | Loss: 0.00003385
Iteration 78/1000 | Loss: 0.00003385
Iteration 79/1000 | Loss: 0.00003385
Iteration 80/1000 | Loss: 0.00003385
Iteration 81/1000 | Loss: 0.00003385
Iteration 82/1000 | Loss: 0.00003385
Iteration 83/1000 | Loss: 0.00003385
Iteration 84/1000 | Loss: 0.00003384
Iteration 85/1000 | Loss: 0.00003384
Iteration 86/1000 | Loss: 0.00003384
Iteration 87/1000 | Loss: 0.00003384
Iteration 88/1000 | Loss: 0.00003384
Iteration 89/1000 | Loss: 0.00003384
Iteration 90/1000 | Loss: 0.00003384
Iteration 91/1000 | Loss: 0.00003384
Iteration 92/1000 | Loss: 0.00003384
Iteration 93/1000 | Loss: 0.00003384
Iteration 94/1000 | Loss: 0.00003384
Iteration 95/1000 | Loss: 0.00003384
Iteration 96/1000 | Loss: 0.00003384
Iteration 97/1000 | Loss: 0.00003383
Iteration 98/1000 | Loss: 0.00003383
Iteration 99/1000 | Loss: 0.00003383
Iteration 100/1000 | Loss: 0.00003383
Iteration 101/1000 | Loss: 0.00003383
Iteration 102/1000 | Loss: 0.00003383
Iteration 103/1000 | Loss: 0.00003383
Iteration 104/1000 | Loss: 0.00003383
Iteration 105/1000 | Loss: 0.00003383
Iteration 106/1000 | Loss: 0.00003383
Iteration 107/1000 | Loss: 0.00003383
Iteration 108/1000 | Loss: 0.00003383
Iteration 109/1000 | Loss: 0.00003383
Iteration 110/1000 | Loss: 0.00003382
Iteration 111/1000 | Loss: 0.00003382
Iteration 112/1000 | Loss: 0.00003382
Iteration 113/1000 | Loss: 0.00003382
Iteration 114/1000 | Loss: 0.00003382
Iteration 115/1000 | Loss: 0.00003382
Iteration 116/1000 | Loss: 0.00003382
Iteration 117/1000 | Loss: 0.00003382
Iteration 118/1000 | Loss: 0.00003382
Iteration 119/1000 | Loss: 0.00003382
Iteration 120/1000 | Loss: 0.00003382
Iteration 121/1000 | Loss: 0.00003382
Iteration 122/1000 | Loss: 0.00003382
Iteration 123/1000 | Loss: 0.00003382
Iteration 124/1000 | Loss: 0.00003382
Iteration 125/1000 | Loss: 0.00003381
Iteration 126/1000 | Loss: 0.00003381
Iteration 127/1000 | Loss: 0.00003381
Iteration 128/1000 | Loss: 0.00003381
Iteration 129/1000 | Loss: 0.00003381
Iteration 130/1000 | Loss: 0.00003381
Iteration 131/1000 | Loss: 0.00003381
Iteration 132/1000 | Loss: 0.00003381
Iteration 133/1000 | Loss: 0.00003381
Iteration 134/1000 | Loss: 0.00003381
Iteration 135/1000 | Loss: 0.00003381
Iteration 136/1000 | Loss: 0.00003381
Iteration 137/1000 | Loss: 0.00003381
Iteration 138/1000 | Loss: 0.00003381
Iteration 139/1000 | Loss: 0.00003381
Iteration 140/1000 | Loss: 0.00003381
Iteration 141/1000 | Loss: 0.00003381
Iteration 142/1000 | Loss: 0.00003381
Iteration 143/1000 | Loss: 0.00003381
Iteration 144/1000 | Loss: 0.00003381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.380985799594782e-05, 3.380985799594782e-05, 3.380985799594782e-05, 3.380985799594782e-05, 3.380985799594782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.380985799594782e-05

Optimization complete. Final v2v error: 4.432727813720703 mm

Highest mean error: 11.224321365356445 mm for frame 133

Lowest mean error: 3.574026584625244 mm for frame 0

Saving results

Total time: 114.58643364906311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487465
Iteration 2/25 | Loss: 0.00184241
Iteration 3/25 | Loss: 0.00157256
Iteration 4/25 | Loss: 0.00153326
Iteration 5/25 | Loss: 0.00152684
Iteration 6/25 | Loss: 0.00152523
Iteration 7/25 | Loss: 0.00152509
Iteration 8/25 | Loss: 0.00152509
Iteration 9/25 | Loss: 0.00152509
Iteration 10/25 | Loss: 0.00152509
Iteration 11/25 | Loss: 0.00152509
Iteration 12/25 | Loss: 0.00152509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015250872820615768, 0.0015250872820615768, 0.0015250872820615768, 0.0015250872820615768, 0.0015250872820615768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015250872820615768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29431331
Iteration 2/25 | Loss: 0.00227868
Iteration 3/25 | Loss: 0.00227868
Iteration 4/25 | Loss: 0.00227868
Iteration 5/25 | Loss: 0.00227868
Iteration 6/25 | Loss: 0.00227868
Iteration 7/25 | Loss: 0.00227868
Iteration 8/25 | Loss: 0.00227868
Iteration 9/25 | Loss: 0.00227868
Iteration 10/25 | Loss: 0.00227868
Iteration 11/25 | Loss: 0.00227868
Iteration 12/25 | Loss: 0.00227868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0022786755580455065, 0.0022786755580455065, 0.0022786755580455065, 0.0022786755580455065, 0.0022786755580455065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022786755580455065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227868
Iteration 2/1000 | Loss: 0.00006140
Iteration 3/1000 | Loss: 0.00003838
Iteration 4/1000 | Loss: 0.00003120
Iteration 5/1000 | Loss: 0.00002698
Iteration 6/1000 | Loss: 0.00002481
Iteration 7/1000 | Loss: 0.00002374
Iteration 8/1000 | Loss: 0.00002295
Iteration 9/1000 | Loss: 0.00002242
Iteration 10/1000 | Loss: 0.00002204
Iteration 11/1000 | Loss: 0.00002168
Iteration 12/1000 | Loss: 0.00002143
Iteration 13/1000 | Loss: 0.00002130
Iteration 14/1000 | Loss: 0.00002130
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002127
Iteration 17/1000 | Loss: 0.00002126
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002125
Iteration 20/1000 | Loss: 0.00002124
Iteration 21/1000 | Loss: 0.00002124
Iteration 22/1000 | Loss: 0.00002123
Iteration 23/1000 | Loss: 0.00002123
Iteration 24/1000 | Loss: 0.00002120
Iteration 25/1000 | Loss: 0.00002119
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002118
Iteration 28/1000 | Loss: 0.00002118
Iteration 29/1000 | Loss: 0.00002118
Iteration 30/1000 | Loss: 0.00002112
Iteration 31/1000 | Loss: 0.00002112
Iteration 32/1000 | Loss: 0.00002109
Iteration 33/1000 | Loss: 0.00002109
Iteration 34/1000 | Loss: 0.00002108
Iteration 35/1000 | Loss: 0.00002108
Iteration 36/1000 | Loss: 0.00002108
Iteration 37/1000 | Loss: 0.00002107
Iteration 38/1000 | Loss: 0.00002107
Iteration 39/1000 | Loss: 0.00002105
Iteration 40/1000 | Loss: 0.00002105
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002104
Iteration 43/1000 | Loss: 0.00002104
Iteration 44/1000 | Loss: 0.00002104
Iteration 45/1000 | Loss: 0.00002103
Iteration 46/1000 | Loss: 0.00002102
Iteration 47/1000 | Loss: 0.00002102
Iteration 48/1000 | Loss: 0.00002101
Iteration 49/1000 | Loss: 0.00002101
Iteration 50/1000 | Loss: 0.00002101
Iteration 51/1000 | Loss: 0.00002100
Iteration 52/1000 | Loss: 0.00002100
Iteration 53/1000 | Loss: 0.00002099
Iteration 54/1000 | Loss: 0.00002099
Iteration 55/1000 | Loss: 0.00002098
Iteration 56/1000 | Loss: 0.00002098
Iteration 57/1000 | Loss: 0.00002097
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002097
Iteration 61/1000 | Loss: 0.00002097
Iteration 62/1000 | Loss: 0.00002097
Iteration 63/1000 | Loss: 0.00002097
Iteration 64/1000 | Loss: 0.00002097
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002097
Iteration 67/1000 | Loss: 0.00002097
Iteration 68/1000 | Loss: 0.00002097
Iteration 69/1000 | Loss: 0.00002097
Iteration 70/1000 | Loss: 0.00002097
Iteration 71/1000 | Loss: 0.00002096
Iteration 72/1000 | Loss: 0.00002096
Iteration 73/1000 | Loss: 0.00002095
Iteration 74/1000 | Loss: 0.00002095
Iteration 75/1000 | Loss: 0.00002095
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002094
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002094
Iteration 80/1000 | Loss: 0.00002094
Iteration 81/1000 | Loss: 0.00002094
Iteration 82/1000 | Loss: 0.00002094
Iteration 83/1000 | Loss: 0.00002093
Iteration 84/1000 | Loss: 0.00002093
Iteration 85/1000 | Loss: 0.00002093
Iteration 86/1000 | Loss: 0.00002093
Iteration 87/1000 | Loss: 0.00002093
Iteration 88/1000 | Loss: 0.00002093
Iteration 89/1000 | Loss: 0.00002093
Iteration 90/1000 | Loss: 0.00002093
Iteration 91/1000 | Loss: 0.00002093
Iteration 92/1000 | Loss: 0.00002093
Iteration 93/1000 | Loss: 0.00002093
Iteration 94/1000 | Loss: 0.00002093
Iteration 95/1000 | Loss: 0.00002093
Iteration 96/1000 | Loss: 0.00002093
Iteration 97/1000 | Loss: 0.00002093
Iteration 98/1000 | Loss: 0.00002093
Iteration 99/1000 | Loss: 0.00002093
Iteration 100/1000 | Loss: 0.00002093
Iteration 101/1000 | Loss: 0.00002093
Iteration 102/1000 | Loss: 0.00002093
Iteration 103/1000 | Loss: 0.00002093
Iteration 104/1000 | Loss: 0.00002093
Iteration 105/1000 | Loss: 0.00002093
Iteration 106/1000 | Loss: 0.00002093
Iteration 107/1000 | Loss: 0.00002093
Iteration 108/1000 | Loss: 0.00002093
Iteration 109/1000 | Loss: 0.00002093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.0934918211423792e-05, 2.0934918211423792e-05, 2.0934918211423792e-05, 2.0934918211423792e-05, 2.0934918211423792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0934918211423792e-05

Optimization complete. Final v2v error: 3.9193718433380127 mm

Highest mean error: 5.4528489112854 mm for frame 103

Lowest mean error: 3.3734638690948486 mm for frame 213

Saving results

Total time: 38.4054913520813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881512
Iteration 2/25 | Loss: 0.00189064
Iteration 3/25 | Loss: 0.00164488
Iteration 4/25 | Loss: 0.00161183
Iteration 5/25 | Loss: 0.00160643
Iteration 6/25 | Loss: 0.00160541
Iteration 7/25 | Loss: 0.00160541
Iteration 8/25 | Loss: 0.00160541
Iteration 9/25 | Loss: 0.00160541
Iteration 10/25 | Loss: 0.00160541
Iteration 11/25 | Loss: 0.00160541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016054065199568868, 0.0016054065199568868, 0.0016054065199568868, 0.0016054065199568868, 0.0016054065199568868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016054065199568868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19489646
Iteration 2/25 | Loss: 0.00231168
Iteration 3/25 | Loss: 0.00231165
Iteration 4/25 | Loss: 0.00231165
Iteration 5/25 | Loss: 0.00231164
Iteration 6/25 | Loss: 0.00231164
Iteration 7/25 | Loss: 0.00231164
Iteration 8/25 | Loss: 0.00231164
Iteration 9/25 | Loss: 0.00231164
Iteration 10/25 | Loss: 0.00231164
Iteration 11/25 | Loss: 0.00231164
Iteration 12/25 | Loss: 0.00231164
Iteration 13/25 | Loss: 0.00231164
Iteration 14/25 | Loss: 0.00231164
Iteration 15/25 | Loss: 0.00231164
Iteration 16/25 | Loss: 0.00231164
Iteration 17/25 | Loss: 0.00231164
Iteration 18/25 | Loss: 0.00231164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023116429802030325, 0.0023116429802030325, 0.0023116429802030325, 0.0023116429802030325, 0.0023116429802030325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023116429802030325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231164
Iteration 2/1000 | Loss: 0.00008279
Iteration 3/1000 | Loss: 0.00005331
Iteration 4/1000 | Loss: 0.00004031
Iteration 5/1000 | Loss: 0.00003463
Iteration 6/1000 | Loss: 0.00003145
Iteration 7/1000 | Loss: 0.00002928
Iteration 8/1000 | Loss: 0.00002770
Iteration 9/1000 | Loss: 0.00002667
Iteration 10/1000 | Loss: 0.00002560
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002462
Iteration 13/1000 | Loss: 0.00002434
Iteration 14/1000 | Loss: 0.00002421
Iteration 15/1000 | Loss: 0.00002419
Iteration 16/1000 | Loss: 0.00002419
Iteration 17/1000 | Loss: 0.00002419
Iteration 18/1000 | Loss: 0.00002417
Iteration 19/1000 | Loss: 0.00002411
Iteration 20/1000 | Loss: 0.00002409
Iteration 21/1000 | Loss: 0.00002407
Iteration 22/1000 | Loss: 0.00002401
Iteration 23/1000 | Loss: 0.00002399
Iteration 24/1000 | Loss: 0.00002396
Iteration 25/1000 | Loss: 0.00002395
Iteration 26/1000 | Loss: 0.00002393
Iteration 27/1000 | Loss: 0.00002392
Iteration 28/1000 | Loss: 0.00002390
Iteration 29/1000 | Loss: 0.00002389
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002388
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002388
Iteration 34/1000 | Loss: 0.00002388
Iteration 35/1000 | Loss: 0.00002387
Iteration 36/1000 | Loss: 0.00002387
Iteration 37/1000 | Loss: 0.00002387
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002386
Iteration 40/1000 | Loss: 0.00002385
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002385
Iteration 43/1000 | Loss: 0.00002385
Iteration 44/1000 | Loss: 0.00002385
Iteration 45/1000 | Loss: 0.00002384
Iteration 46/1000 | Loss: 0.00002384
Iteration 47/1000 | Loss: 0.00002384
Iteration 48/1000 | Loss: 0.00002383
Iteration 49/1000 | Loss: 0.00002382
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002381
Iteration 52/1000 | Loss: 0.00002381
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002381
Iteration 55/1000 | Loss: 0.00002381
Iteration 56/1000 | Loss: 0.00002381
Iteration 57/1000 | Loss: 0.00002381
Iteration 58/1000 | Loss: 0.00002381
Iteration 59/1000 | Loss: 0.00002380
Iteration 60/1000 | Loss: 0.00002380
Iteration 61/1000 | Loss: 0.00002379
Iteration 62/1000 | Loss: 0.00002379
Iteration 63/1000 | Loss: 0.00002378
Iteration 64/1000 | Loss: 0.00002378
Iteration 65/1000 | Loss: 0.00002378
Iteration 66/1000 | Loss: 0.00002378
Iteration 67/1000 | Loss: 0.00002377
Iteration 68/1000 | Loss: 0.00002377
Iteration 69/1000 | Loss: 0.00002377
Iteration 70/1000 | Loss: 0.00002377
Iteration 71/1000 | Loss: 0.00002377
Iteration 72/1000 | Loss: 0.00002377
Iteration 73/1000 | Loss: 0.00002377
Iteration 74/1000 | Loss: 0.00002377
Iteration 75/1000 | Loss: 0.00002377
Iteration 76/1000 | Loss: 0.00002377
Iteration 77/1000 | Loss: 0.00002377
Iteration 78/1000 | Loss: 0.00002376
Iteration 79/1000 | Loss: 0.00002376
Iteration 80/1000 | Loss: 0.00002376
Iteration 81/1000 | Loss: 0.00002376
Iteration 82/1000 | Loss: 0.00002376
Iteration 83/1000 | Loss: 0.00002375
Iteration 84/1000 | Loss: 0.00002375
Iteration 85/1000 | Loss: 0.00002375
Iteration 86/1000 | Loss: 0.00002375
Iteration 87/1000 | Loss: 0.00002375
Iteration 88/1000 | Loss: 0.00002375
Iteration 89/1000 | Loss: 0.00002375
Iteration 90/1000 | Loss: 0.00002375
Iteration 91/1000 | Loss: 0.00002375
Iteration 92/1000 | Loss: 0.00002375
Iteration 93/1000 | Loss: 0.00002375
Iteration 94/1000 | Loss: 0.00002375
Iteration 95/1000 | Loss: 0.00002375
Iteration 96/1000 | Loss: 0.00002375
Iteration 97/1000 | Loss: 0.00002375
Iteration 98/1000 | Loss: 0.00002375
Iteration 99/1000 | Loss: 0.00002375
Iteration 100/1000 | Loss: 0.00002375
Iteration 101/1000 | Loss: 0.00002375
Iteration 102/1000 | Loss: 0.00002375
Iteration 103/1000 | Loss: 0.00002375
Iteration 104/1000 | Loss: 0.00002375
Iteration 105/1000 | Loss: 0.00002375
Iteration 106/1000 | Loss: 0.00002375
Iteration 107/1000 | Loss: 0.00002375
Iteration 108/1000 | Loss: 0.00002375
Iteration 109/1000 | Loss: 0.00002375
Iteration 110/1000 | Loss: 0.00002375
Iteration 111/1000 | Loss: 0.00002375
Iteration 112/1000 | Loss: 0.00002375
Iteration 113/1000 | Loss: 0.00002375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.3747619707137346e-05, 2.3747619707137346e-05, 2.3747619707137346e-05, 2.3747619707137346e-05, 2.3747619707137346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3747619707137346e-05

Optimization complete. Final v2v error: 4.151769161224365 mm

Highest mean error: 4.546140193939209 mm for frame 35

Lowest mean error: 3.6796770095825195 mm for frame 118

Saving results

Total time: 35.96861958503723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456185
Iteration 2/25 | Loss: 0.00174942
Iteration 3/25 | Loss: 0.00159978
Iteration 4/25 | Loss: 0.00157995
Iteration 5/25 | Loss: 0.00157339
Iteration 6/25 | Loss: 0.00157191
Iteration 7/25 | Loss: 0.00157191
Iteration 8/25 | Loss: 0.00157191
Iteration 9/25 | Loss: 0.00157191
Iteration 10/25 | Loss: 0.00157191
Iteration 11/25 | Loss: 0.00157191
Iteration 12/25 | Loss: 0.00157191
Iteration 13/25 | Loss: 0.00157191
Iteration 14/25 | Loss: 0.00157191
Iteration 15/25 | Loss: 0.00157191
Iteration 16/25 | Loss: 0.00157191
Iteration 17/25 | Loss: 0.00157191
Iteration 18/25 | Loss: 0.00157191
Iteration 19/25 | Loss: 0.00157191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0015719051007181406, 0.0015719051007181406, 0.0015719051007181406, 0.0015719051007181406, 0.0015719051007181406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015719051007181406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15454054
Iteration 2/25 | Loss: 0.00226050
Iteration 3/25 | Loss: 0.00226048
Iteration 4/25 | Loss: 0.00226047
Iteration 5/25 | Loss: 0.00226047
Iteration 6/25 | Loss: 0.00226047
Iteration 7/25 | Loss: 0.00226047
Iteration 8/25 | Loss: 0.00226047
Iteration 9/25 | Loss: 0.00226047
Iteration 10/25 | Loss: 0.00226047
Iteration 11/25 | Loss: 0.00226047
Iteration 12/25 | Loss: 0.00226047
Iteration 13/25 | Loss: 0.00226047
Iteration 14/25 | Loss: 0.00226047
Iteration 15/25 | Loss: 0.00226047
Iteration 16/25 | Loss: 0.00226047
Iteration 17/25 | Loss: 0.00226047
Iteration 18/25 | Loss: 0.00226047
Iteration 19/25 | Loss: 0.00226047
Iteration 20/25 | Loss: 0.00226047
Iteration 21/25 | Loss: 0.00226047
Iteration 22/25 | Loss: 0.00226047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002260471461340785, 0.002260471461340785, 0.002260471461340785, 0.002260471461340785, 0.002260471461340785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002260471461340785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226047
Iteration 2/1000 | Loss: 0.00006411
Iteration 3/1000 | Loss: 0.00004539
Iteration 4/1000 | Loss: 0.00003834
Iteration 5/1000 | Loss: 0.00003479
Iteration 6/1000 | Loss: 0.00003334
Iteration 7/1000 | Loss: 0.00003221
Iteration 8/1000 | Loss: 0.00003150
Iteration 9/1000 | Loss: 0.00003086
Iteration 10/1000 | Loss: 0.00003039
Iteration 11/1000 | Loss: 0.00003012
Iteration 12/1000 | Loss: 0.00003005
Iteration 13/1000 | Loss: 0.00002991
Iteration 14/1000 | Loss: 0.00002983
Iteration 15/1000 | Loss: 0.00002979
Iteration 16/1000 | Loss: 0.00002979
Iteration 17/1000 | Loss: 0.00002979
Iteration 18/1000 | Loss: 0.00002978
Iteration 19/1000 | Loss: 0.00002977
Iteration 20/1000 | Loss: 0.00002977
Iteration 21/1000 | Loss: 0.00002976
Iteration 22/1000 | Loss: 0.00002975
Iteration 23/1000 | Loss: 0.00002972
Iteration 24/1000 | Loss: 0.00002970
Iteration 25/1000 | Loss: 0.00002970
Iteration 26/1000 | Loss: 0.00002969
Iteration 27/1000 | Loss: 0.00002969
Iteration 28/1000 | Loss: 0.00002969
Iteration 29/1000 | Loss: 0.00002969
Iteration 30/1000 | Loss: 0.00002968
Iteration 31/1000 | Loss: 0.00002968
Iteration 32/1000 | Loss: 0.00002968
Iteration 33/1000 | Loss: 0.00002968
Iteration 34/1000 | Loss: 0.00002967
Iteration 35/1000 | Loss: 0.00002967
Iteration 36/1000 | Loss: 0.00002967
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002967
Iteration 39/1000 | Loss: 0.00002967
Iteration 40/1000 | Loss: 0.00002967
Iteration 41/1000 | Loss: 0.00002967
Iteration 42/1000 | Loss: 0.00002967
Iteration 43/1000 | Loss: 0.00002966
Iteration 44/1000 | Loss: 0.00002966
Iteration 45/1000 | Loss: 0.00002966
Iteration 46/1000 | Loss: 0.00002966
Iteration 47/1000 | Loss: 0.00002966
Iteration 48/1000 | Loss: 0.00002966
Iteration 49/1000 | Loss: 0.00002965
Iteration 50/1000 | Loss: 0.00002965
Iteration 51/1000 | Loss: 0.00002965
Iteration 52/1000 | Loss: 0.00002965
Iteration 53/1000 | Loss: 0.00002965
Iteration 54/1000 | Loss: 0.00002964
Iteration 55/1000 | Loss: 0.00002964
Iteration 56/1000 | Loss: 0.00002964
Iteration 57/1000 | Loss: 0.00002964
Iteration 58/1000 | Loss: 0.00002964
Iteration 59/1000 | Loss: 0.00002964
Iteration 60/1000 | Loss: 0.00002964
Iteration 61/1000 | Loss: 0.00002964
Iteration 62/1000 | Loss: 0.00002964
Iteration 63/1000 | Loss: 0.00002964
Iteration 64/1000 | Loss: 0.00002963
Iteration 65/1000 | Loss: 0.00002963
Iteration 66/1000 | Loss: 0.00002963
Iteration 67/1000 | Loss: 0.00002963
Iteration 68/1000 | Loss: 0.00002963
Iteration 69/1000 | Loss: 0.00002962
Iteration 70/1000 | Loss: 0.00002962
Iteration 71/1000 | Loss: 0.00002962
Iteration 72/1000 | Loss: 0.00002962
Iteration 73/1000 | Loss: 0.00002962
Iteration 74/1000 | Loss: 0.00002962
Iteration 75/1000 | Loss: 0.00002962
Iteration 76/1000 | Loss: 0.00002962
Iteration 77/1000 | Loss: 0.00002962
Iteration 78/1000 | Loss: 0.00002962
Iteration 79/1000 | Loss: 0.00002961
Iteration 80/1000 | Loss: 0.00002961
Iteration 81/1000 | Loss: 0.00002961
Iteration 82/1000 | Loss: 0.00002961
Iteration 83/1000 | Loss: 0.00002961
Iteration 84/1000 | Loss: 0.00002961
Iteration 85/1000 | Loss: 0.00002961
Iteration 86/1000 | Loss: 0.00002961
Iteration 87/1000 | Loss: 0.00002960
Iteration 88/1000 | Loss: 0.00002960
Iteration 89/1000 | Loss: 0.00002960
Iteration 90/1000 | Loss: 0.00002959
Iteration 91/1000 | Loss: 0.00002959
Iteration 92/1000 | Loss: 0.00002959
Iteration 93/1000 | Loss: 0.00002959
Iteration 94/1000 | Loss: 0.00002959
Iteration 95/1000 | Loss: 0.00002959
Iteration 96/1000 | Loss: 0.00002959
Iteration 97/1000 | Loss: 0.00002959
Iteration 98/1000 | Loss: 0.00002958
Iteration 99/1000 | Loss: 0.00002958
Iteration 100/1000 | Loss: 0.00002958
Iteration 101/1000 | Loss: 0.00002958
Iteration 102/1000 | Loss: 0.00002958
Iteration 103/1000 | Loss: 0.00002958
Iteration 104/1000 | Loss: 0.00002958
Iteration 105/1000 | Loss: 0.00002958
Iteration 106/1000 | Loss: 0.00002958
Iteration 107/1000 | Loss: 0.00002958
Iteration 108/1000 | Loss: 0.00002958
Iteration 109/1000 | Loss: 0.00002958
Iteration 110/1000 | Loss: 0.00002958
Iteration 111/1000 | Loss: 0.00002958
Iteration 112/1000 | Loss: 0.00002958
Iteration 113/1000 | Loss: 0.00002958
Iteration 114/1000 | Loss: 0.00002958
Iteration 115/1000 | Loss: 0.00002958
Iteration 116/1000 | Loss: 0.00002958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.9579070542240515e-05, 2.9579070542240515e-05, 2.9579070542240515e-05, 2.9579070542240515e-05, 2.9579070542240515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9579070542240515e-05

Optimization complete. Final v2v error: 4.6784234046936035 mm

Highest mean error: 5.113617897033691 mm for frame 182

Lowest mean error: 4.170232772827148 mm for frame 152

Saving results

Total time: 35.86444640159607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105772
Iteration 2/25 | Loss: 0.01105772
Iteration 3/25 | Loss: 0.01105772
Iteration 4/25 | Loss: 0.01105772
Iteration 5/25 | Loss: 0.01105772
Iteration 6/25 | Loss: 0.01105772
Iteration 7/25 | Loss: 0.01105772
Iteration 8/25 | Loss: 0.01105772
Iteration 9/25 | Loss: 0.01105772
Iteration 10/25 | Loss: 0.01105772
Iteration 11/25 | Loss: 0.01105772
Iteration 12/25 | Loss: 0.01105772
Iteration 13/25 | Loss: 0.01105772
Iteration 14/25 | Loss: 0.01105771
Iteration 15/25 | Loss: 0.01105771
Iteration 16/25 | Loss: 0.01105771
Iteration 17/25 | Loss: 0.01105771
Iteration 18/25 | Loss: 0.01105771
Iteration 19/25 | Loss: 0.01105771
Iteration 20/25 | Loss: 0.01105771
Iteration 21/25 | Loss: 0.01105771
Iteration 22/25 | Loss: 0.01105771
Iteration 23/25 | Loss: 0.01105771
Iteration 24/25 | Loss: 0.01105771
Iteration 25/25 | Loss: 0.01105771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43723142
Iteration 2/25 | Loss: 0.11019982
Iteration 3/25 | Loss: 0.10793912
Iteration 4/25 | Loss: 0.10793912
Iteration 5/25 | Loss: 0.10793912
Iteration 6/25 | Loss: 0.10793909
Iteration 7/25 | Loss: 0.10793909
Iteration 8/25 | Loss: 0.10793909
Iteration 9/25 | Loss: 0.10793909
Iteration 10/25 | Loss: 0.10793909
Iteration 11/25 | Loss: 0.10793909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.10793909430503845, 0.10793909430503845, 0.10793909430503845, 0.10793909430503845, 0.10793909430503845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10793909430503845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10793909
Iteration 2/1000 | Loss: 0.00344570
Iteration 3/1000 | Loss: 0.00606975
Iteration 4/1000 | Loss: 0.00046210
Iteration 5/1000 | Loss: 0.00037529
Iteration 6/1000 | Loss: 0.00016718
Iteration 7/1000 | Loss: 0.00017763
Iteration 8/1000 | Loss: 0.00278289
Iteration 9/1000 | Loss: 0.00019416
Iteration 10/1000 | Loss: 0.00049582
Iteration 11/1000 | Loss: 0.00069615
Iteration 12/1000 | Loss: 0.00019786
Iteration 13/1000 | Loss: 0.00012745
Iteration 14/1000 | Loss: 0.00015208
Iteration 15/1000 | Loss: 0.00029733
Iteration 16/1000 | Loss: 0.00026346
Iteration 17/1000 | Loss: 0.00012343
Iteration 18/1000 | Loss: 0.00031658
Iteration 19/1000 | Loss: 0.00027662
Iteration 20/1000 | Loss: 0.00006092
Iteration 21/1000 | Loss: 0.00002989
Iteration 22/1000 | Loss: 0.00036497
Iteration 23/1000 | Loss: 0.00187876
Iteration 24/1000 | Loss: 0.00047148
Iteration 25/1000 | Loss: 0.00023041
Iteration 26/1000 | Loss: 0.00011766
Iteration 27/1000 | Loss: 0.00017829
Iteration 28/1000 | Loss: 0.00005336
Iteration 29/1000 | Loss: 0.00002709
Iteration 30/1000 | Loss: 0.00004846
Iteration 31/1000 | Loss: 0.00025331
Iteration 32/1000 | Loss: 0.00004723
Iteration 33/1000 | Loss: 0.00004610
Iteration 34/1000 | Loss: 0.00018062
Iteration 35/1000 | Loss: 0.00006047
Iteration 36/1000 | Loss: 0.00003456
Iteration 37/1000 | Loss: 0.00002965
Iteration 38/1000 | Loss: 0.00002437
Iteration 39/1000 | Loss: 0.00002402
Iteration 40/1000 | Loss: 0.00011700
Iteration 41/1000 | Loss: 0.00002551
Iteration 42/1000 | Loss: 0.00004471
Iteration 43/1000 | Loss: 0.00008980
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002716
Iteration 46/1000 | Loss: 0.00004770
Iteration 47/1000 | Loss: 0.00002713
Iteration 48/1000 | Loss: 0.00002334
Iteration 49/1000 | Loss: 0.00002325
Iteration 50/1000 | Loss: 0.00002324
Iteration 51/1000 | Loss: 0.00002324
Iteration 52/1000 | Loss: 0.00002322
Iteration 53/1000 | Loss: 0.00002322
Iteration 54/1000 | Loss: 0.00002321
Iteration 55/1000 | Loss: 0.00002319
Iteration 56/1000 | Loss: 0.00002319
Iteration 57/1000 | Loss: 0.00002317
Iteration 58/1000 | Loss: 0.00002314
Iteration 59/1000 | Loss: 0.00002313
Iteration 60/1000 | Loss: 0.00002312
Iteration 61/1000 | Loss: 0.00002312
Iteration 62/1000 | Loss: 0.00002312
Iteration 63/1000 | Loss: 0.00002311
Iteration 64/1000 | Loss: 0.00002309
Iteration 65/1000 | Loss: 0.00002309
Iteration 66/1000 | Loss: 0.00002308
Iteration 67/1000 | Loss: 0.00002308
Iteration 68/1000 | Loss: 0.00002307
Iteration 69/1000 | Loss: 0.00002305
Iteration 70/1000 | Loss: 0.00002305
Iteration 71/1000 | Loss: 0.00002305
Iteration 72/1000 | Loss: 0.00002305
Iteration 73/1000 | Loss: 0.00002305
Iteration 74/1000 | Loss: 0.00002305
Iteration 75/1000 | Loss: 0.00002305
Iteration 76/1000 | Loss: 0.00002305
Iteration 77/1000 | Loss: 0.00002305
Iteration 78/1000 | Loss: 0.00002305
Iteration 79/1000 | Loss: 0.00002304
Iteration 80/1000 | Loss: 0.00002304
Iteration 81/1000 | Loss: 0.00002304
Iteration 82/1000 | Loss: 0.00002304
Iteration 83/1000 | Loss: 0.00002304
Iteration 84/1000 | Loss: 0.00002304
Iteration 85/1000 | Loss: 0.00002304
Iteration 86/1000 | Loss: 0.00002304
Iteration 87/1000 | Loss: 0.00002304
Iteration 88/1000 | Loss: 0.00002304
Iteration 89/1000 | Loss: 0.00002304
Iteration 90/1000 | Loss: 0.00002304
Iteration 91/1000 | Loss: 0.00002304
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00002304
Iteration 94/1000 | Loss: 0.00002304
Iteration 95/1000 | Loss: 0.00002304
Iteration 96/1000 | Loss: 0.00002304
Iteration 97/1000 | Loss: 0.00002304
Iteration 98/1000 | Loss: 0.00002304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.3043077817419544e-05, 2.3043077817419544e-05, 2.3043077817419544e-05, 2.3043077817419544e-05, 2.3043077817419544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3043077817419544e-05

Optimization complete. Final v2v error: 4.171673774719238 mm

Highest mean error: 11.421564102172852 mm for frame 57

Lowest mean error: 3.8052079677581787 mm for frame 8

Saving results

Total time: 86.77941393852234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884726
Iteration 2/25 | Loss: 0.00170624
Iteration 3/25 | Loss: 0.00161021
Iteration 4/25 | Loss: 0.00158064
Iteration 5/25 | Loss: 0.00156699
Iteration 6/25 | Loss: 0.00156360
Iteration 7/25 | Loss: 0.00156223
Iteration 8/25 | Loss: 0.00156218
Iteration 9/25 | Loss: 0.00156218
Iteration 10/25 | Loss: 0.00156218
Iteration 11/25 | Loss: 0.00156218
Iteration 12/25 | Loss: 0.00156218
Iteration 13/25 | Loss: 0.00156218
Iteration 14/25 | Loss: 0.00156218
Iteration 15/25 | Loss: 0.00156218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015621829079464078, 0.0015621829079464078, 0.0015621829079464078, 0.0015621829079464078, 0.0015621829079464078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015621829079464078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29428399
Iteration 2/25 | Loss: 0.00295249
Iteration 3/25 | Loss: 0.00295248
Iteration 4/25 | Loss: 0.00295248
Iteration 5/25 | Loss: 0.00295248
Iteration 6/25 | Loss: 0.00295248
Iteration 7/25 | Loss: 0.00295248
Iteration 8/25 | Loss: 0.00295248
Iteration 9/25 | Loss: 0.00295248
Iteration 10/25 | Loss: 0.00295248
Iteration 11/25 | Loss: 0.00295248
Iteration 12/25 | Loss: 0.00295248
Iteration 13/25 | Loss: 0.00295248
Iteration 14/25 | Loss: 0.00295248
Iteration 15/25 | Loss: 0.00295248
Iteration 16/25 | Loss: 0.00295248
Iteration 17/25 | Loss: 0.00295248
Iteration 18/25 | Loss: 0.00295248
Iteration 19/25 | Loss: 0.00295248
Iteration 20/25 | Loss: 0.00295248
Iteration 21/25 | Loss: 0.00295248
Iteration 22/25 | Loss: 0.00295248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002952482085675001, 0.002952482085675001, 0.002952482085675001, 0.002952482085675001, 0.002952482085675001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002952482085675001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295248
Iteration 2/1000 | Loss: 0.00011776
Iteration 3/1000 | Loss: 0.00005972
Iteration 4/1000 | Loss: 0.00004471
Iteration 5/1000 | Loss: 0.00003830
Iteration 6/1000 | Loss: 0.00003543
Iteration 7/1000 | Loss: 0.00003279
Iteration 8/1000 | Loss: 0.00003132
Iteration 9/1000 | Loss: 0.00003053
Iteration 10/1000 | Loss: 0.00002987
Iteration 11/1000 | Loss: 0.00002937
Iteration 12/1000 | Loss: 0.00002888
Iteration 13/1000 | Loss: 0.00002850
Iteration 14/1000 | Loss: 0.00002812
Iteration 15/1000 | Loss: 0.00002778
Iteration 16/1000 | Loss: 0.00002752
Iteration 17/1000 | Loss: 0.00002733
Iteration 18/1000 | Loss: 0.00002725
Iteration 19/1000 | Loss: 0.00002714
Iteration 20/1000 | Loss: 0.00002712
Iteration 21/1000 | Loss: 0.00002705
Iteration 22/1000 | Loss: 0.00002705
Iteration 23/1000 | Loss: 0.00002705
Iteration 24/1000 | Loss: 0.00002704
Iteration 25/1000 | Loss: 0.00002704
Iteration 26/1000 | Loss: 0.00002703
Iteration 27/1000 | Loss: 0.00002703
Iteration 28/1000 | Loss: 0.00002702
Iteration 29/1000 | Loss: 0.00002702
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002701
Iteration 32/1000 | Loss: 0.00002700
Iteration 33/1000 | Loss: 0.00002700
Iteration 34/1000 | Loss: 0.00002699
Iteration 35/1000 | Loss: 0.00002699
Iteration 36/1000 | Loss: 0.00002696
Iteration 37/1000 | Loss: 0.00002696
Iteration 38/1000 | Loss: 0.00002695
Iteration 39/1000 | Loss: 0.00002694
Iteration 40/1000 | Loss: 0.00002693
Iteration 41/1000 | Loss: 0.00002693
Iteration 42/1000 | Loss: 0.00002692
Iteration 43/1000 | Loss: 0.00002692
Iteration 44/1000 | Loss: 0.00002691
Iteration 45/1000 | Loss: 0.00002690
Iteration 46/1000 | Loss: 0.00002689
Iteration 47/1000 | Loss: 0.00002689
Iteration 48/1000 | Loss: 0.00002688
Iteration 49/1000 | Loss: 0.00002688
Iteration 50/1000 | Loss: 0.00002687
Iteration 51/1000 | Loss: 0.00002687
Iteration 52/1000 | Loss: 0.00002686
Iteration 53/1000 | Loss: 0.00002686
Iteration 54/1000 | Loss: 0.00002685
Iteration 55/1000 | Loss: 0.00002685
Iteration 56/1000 | Loss: 0.00002684
Iteration 57/1000 | Loss: 0.00002684
Iteration 58/1000 | Loss: 0.00002683
Iteration 59/1000 | Loss: 0.00002683
Iteration 60/1000 | Loss: 0.00002683
Iteration 61/1000 | Loss: 0.00002682
Iteration 62/1000 | Loss: 0.00002682
Iteration 63/1000 | Loss: 0.00002682
Iteration 64/1000 | Loss: 0.00002681
Iteration 65/1000 | Loss: 0.00002681
Iteration 66/1000 | Loss: 0.00002681
Iteration 67/1000 | Loss: 0.00002680
Iteration 68/1000 | Loss: 0.00002680
Iteration 69/1000 | Loss: 0.00002680
Iteration 70/1000 | Loss: 0.00002679
Iteration 71/1000 | Loss: 0.00002679
Iteration 72/1000 | Loss: 0.00002679
Iteration 73/1000 | Loss: 0.00002679
Iteration 74/1000 | Loss: 0.00002679
Iteration 75/1000 | Loss: 0.00002678
Iteration 76/1000 | Loss: 0.00002678
Iteration 77/1000 | Loss: 0.00002678
Iteration 78/1000 | Loss: 0.00002678
Iteration 79/1000 | Loss: 0.00002678
Iteration 80/1000 | Loss: 0.00002677
Iteration 81/1000 | Loss: 0.00002677
Iteration 82/1000 | Loss: 0.00002677
Iteration 83/1000 | Loss: 0.00002677
Iteration 84/1000 | Loss: 0.00002676
Iteration 85/1000 | Loss: 0.00002676
Iteration 86/1000 | Loss: 0.00002676
Iteration 87/1000 | Loss: 0.00002676
Iteration 88/1000 | Loss: 0.00002675
Iteration 89/1000 | Loss: 0.00002675
Iteration 90/1000 | Loss: 0.00002675
Iteration 91/1000 | Loss: 0.00002675
Iteration 92/1000 | Loss: 0.00002674
Iteration 93/1000 | Loss: 0.00002674
Iteration 94/1000 | Loss: 0.00002674
Iteration 95/1000 | Loss: 0.00002674
Iteration 96/1000 | Loss: 0.00002674
Iteration 97/1000 | Loss: 0.00002674
Iteration 98/1000 | Loss: 0.00002673
Iteration 99/1000 | Loss: 0.00002673
Iteration 100/1000 | Loss: 0.00002673
Iteration 101/1000 | Loss: 0.00002673
Iteration 102/1000 | Loss: 0.00002673
Iteration 103/1000 | Loss: 0.00002673
Iteration 104/1000 | Loss: 0.00002673
Iteration 105/1000 | Loss: 0.00002673
Iteration 106/1000 | Loss: 0.00002673
Iteration 107/1000 | Loss: 0.00002672
Iteration 108/1000 | Loss: 0.00002672
Iteration 109/1000 | Loss: 0.00002672
Iteration 110/1000 | Loss: 0.00002672
Iteration 111/1000 | Loss: 0.00002672
Iteration 112/1000 | Loss: 0.00002672
Iteration 113/1000 | Loss: 0.00002672
Iteration 114/1000 | Loss: 0.00002672
Iteration 115/1000 | Loss: 0.00002672
Iteration 116/1000 | Loss: 0.00002672
Iteration 117/1000 | Loss: 0.00002672
Iteration 118/1000 | Loss: 0.00002671
Iteration 119/1000 | Loss: 0.00002671
Iteration 120/1000 | Loss: 0.00002671
Iteration 121/1000 | Loss: 0.00002671
Iteration 122/1000 | Loss: 0.00002671
Iteration 123/1000 | Loss: 0.00002671
Iteration 124/1000 | Loss: 0.00002671
Iteration 125/1000 | Loss: 0.00002671
Iteration 126/1000 | Loss: 0.00002671
Iteration 127/1000 | Loss: 0.00002671
Iteration 128/1000 | Loss: 0.00002671
Iteration 129/1000 | Loss: 0.00002671
Iteration 130/1000 | Loss: 0.00002671
Iteration 131/1000 | Loss: 0.00002671
Iteration 132/1000 | Loss: 0.00002671
Iteration 133/1000 | Loss: 0.00002671
Iteration 134/1000 | Loss: 0.00002670
Iteration 135/1000 | Loss: 0.00002670
Iteration 136/1000 | Loss: 0.00002670
Iteration 137/1000 | Loss: 0.00002670
Iteration 138/1000 | Loss: 0.00002670
Iteration 139/1000 | Loss: 0.00002670
Iteration 140/1000 | Loss: 0.00002670
Iteration 141/1000 | Loss: 0.00002670
Iteration 142/1000 | Loss: 0.00002670
Iteration 143/1000 | Loss: 0.00002670
Iteration 144/1000 | Loss: 0.00002670
Iteration 145/1000 | Loss: 0.00002670
Iteration 146/1000 | Loss: 0.00002670
Iteration 147/1000 | Loss: 0.00002670
Iteration 148/1000 | Loss: 0.00002670
Iteration 149/1000 | Loss: 0.00002670
Iteration 150/1000 | Loss: 0.00002670
Iteration 151/1000 | Loss: 0.00002670
Iteration 152/1000 | Loss: 0.00002670
Iteration 153/1000 | Loss: 0.00002670
Iteration 154/1000 | Loss: 0.00002670
Iteration 155/1000 | Loss: 0.00002670
Iteration 156/1000 | Loss: 0.00002670
Iteration 157/1000 | Loss: 0.00002669
Iteration 158/1000 | Loss: 0.00002669
Iteration 159/1000 | Loss: 0.00002669
Iteration 160/1000 | Loss: 0.00002669
Iteration 161/1000 | Loss: 0.00002669
Iteration 162/1000 | Loss: 0.00002669
Iteration 163/1000 | Loss: 0.00002669
Iteration 164/1000 | Loss: 0.00002669
Iteration 165/1000 | Loss: 0.00002669
Iteration 166/1000 | Loss: 0.00002669
Iteration 167/1000 | Loss: 0.00002669
Iteration 168/1000 | Loss: 0.00002669
Iteration 169/1000 | Loss: 0.00002669
Iteration 170/1000 | Loss: 0.00002669
Iteration 171/1000 | Loss: 0.00002669
Iteration 172/1000 | Loss: 0.00002669
Iteration 173/1000 | Loss: 0.00002669
Iteration 174/1000 | Loss: 0.00002669
Iteration 175/1000 | Loss: 0.00002669
Iteration 176/1000 | Loss: 0.00002669
Iteration 177/1000 | Loss: 0.00002669
Iteration 178/1000 | Loss: 0.00002669
Iteration 179/1000 | Loss: 0.00002669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.669223613338545e-05, 2.669223613338545e-05, 2.669223613338545e-05, 2.669223613338545e-05, 2.669223613338545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.669223613338545e-05

Optimization complete. Final v2v error: 4.376502513885498 mm

Highest mean error: 5.625345230102539 mm for frame 127

Lowest mean error: 3.7792675495147705 mm for frame 107

Saving results

Total time: 47.57703757286072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433881
Iteration 2/25 | Loss: 0.00160694
Iteration 3/25 | Loss: 0.00152995
Iteration 4/25 | Loss: 0.00152334
Iteration 5/25 | Loss: 0.00152186
Iteration 6/25 | Loss: 0.00152184
Iteration 7/25 | Loss: 0.00152184
Iteration 8/25 | Loss: 0.00152184
Iteration 9/25 | Loss: 0.00152184
Iteration 10/25 | Loss: 0.00152184
Iteration 11/25 | Loss: 0.00152184
Iteration 12/25 | Loss: 0.00152184
Iteration 13/25 | Loss: 0.00152184
Iteration 14/25 | Loss: 0.00152184
Iteration 15/25 | Loss: 0.00152184
Iteration 16/25 | Loss: 0.00152184
Iteration 17/25 | Loss: 0.00152184
Iteration 18/25 | Loss: 0.00152184
Iteration 19/25 | Loss: 0.00152184
Iteration 20/25 | Loss: 0.00152184
Iteration 21/25 | Loss: 0.00152184
Iteration 22/25 | Loss: 0.00152184
Iteration 23/25 | Loss: 0.00152184
Iteration 24/25 | Loss: 0.00152184
Iteration 25/25 | Loss: 0.00152184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21281660
Iteration 2/25 | Loss: 0.00246360
Iteration 3/25 | Loss: 0.00246360
Iteration 4/25 | Loss: 0.00246360
Iteration 5/25 | Loss: 0.00246360
Iteration 6/25 | Loss: 0.00246359
Iteration 7/25 | Loss: 0.00246359
Iteration 8/25 | Loss: 0.00246359
Iteration 9/25 | Loss: 0.00246359
Iteration 10/25 | Loss: 0.00246359
Iteration 11/25 | Loss: 0.00246359
Iteration 12/25 | Loss: 0.00246359
Iteration 13/25 | Loss: 0.00246359
Iteration 14/25 | Loss: 0.00246359
Iteration 15/25 | Loss: 0.00246359
Iteration 16/25 | Loss: 0.00246359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024635938461869955, 0.0024635938461869955, 0.0024635938461869955, 0.0024635938461869955, 0.0024635938461869955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024635938461869955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246359
Iteration 2/1000 | Loss: 0.00005712
Iteration 3/1000 | Loss: 0.00003646
Iteration 4/1000 | Loss: 0.00002907
Iteration 5/1000 | Loss: 0.00002635
Iteration 6/1000 | Loss: 0.00002410
Iteration 7/1000 | Loss: 0.00002312
Iteration 8/1000 | Loss: 0.00002230
Iteration 9/1000 | Loss: 0.00002189
Iteration 10/1000 | Loss: 0.00002156
Iteration 11/1000 | Loss: 0.00002129
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002104
Iteration 15/1000 | Loss: 0.00002104
Iteration 16/1000 | Loss: 0.00002101
Iteration 17/1000 | Loss: 0.00002101
Iteration 18/1000 | Loss: 0.00002099
Iteration 19/1000 | Loss: 0.00002099
Iteration 20/1000 | Loss: 0.00002097
Iteration 21/1000 | Loss: 0.00002096
Iteration 22/1000 | Loss: 0.00002095
Iteration 23/1000 | Loss: 0.00002095
Iteration 24/1000 | Loss: 0.00002094
Iteration 25/1000 | Loss: 0.00002094
Iteration 26/1000 | Loss: 0.00002091
Iteration 27/1000 | Loss: 0.00002089
Iteration 28/1000 | Loss: 0.00002089
Iteration 29/1000 | Loss: 0.00002088
Iteration 30/1000 | Loss: 0.00002088
Iteration 31/1000 | Loss: 0.00002085
Iteration 32/1000 | Loss: 0.00002085
Iteration 33/1000 | Loss: 0.00002084
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002082
Iteration 37/1000 | Loss: 0.00002081
Iteration 38/1000 | Loss: 0.00002080
Iteration 39/1000 | Loss: 0.00002080
Iteration 40/1000 | Loss: 0.00002080
Iteration 41/1000 | Loss: 0.00002080
Iteration 42/1000 | Loss: 0.00002079
Iteration 43/1000 | Loss: 0.00002079
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002075
Iteration 48/1000 | Loss: 0.00002074
Iteration 49/1000 | Loss: 0.00002074
Iteration 50/1000 | Loss: 0.00002073
Iteration 51/1000 | Loss: 0.00002072
Iteration 52/1000 | Loss: 0.00002072
Iteration 53/1000 | Loss: 0.00002071
Iteration 54/1000 | Loss: 0.00002070
Iteration 55/1000 | Loss: 0.00002070
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002070
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002068
Iteration 74/1000 | Loss: 0.00002068
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.067670357064344e-05, 2.067670357064344e-05, 2.067670357064344e-05, 2.067670357064344e-05, 2.067670357064344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.067670357064344e-05

Optimization complete. Final v2v error: 3.871579885482788 mm

Highest mean error: 4.857019424438477 mm for frame 72

Lowest mean error: 3.639136552810669 mm for frame 137

Saving results

Total time: 33.07100224494934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051791
Iteration 2/25 | Loss: 0.00337937
Iteration 3/25 | Loss: 0.00274550
Iteration 4/25 | Loss: 0.00246084
Iteration 5/25 | Loss: 0.00225582
Iteration 6/25 | Loss: 0.00213116
Iteration 7/25 | Loss: 0.00198347
Iteration 8/25 | Loss: 0.00192268
Iteration 9/25 | Loss: 0.00182194
Iteration 10/25 | Loss: 0.00174447
Iteration 11/25 | Loss: 0.00169821
Iteration 12/25 | Loss: 0.00168387
Iteration 13/25 | Loss: 0.00168229
Iteration 14/25 | Loss: 0.00166574
Iteration 15/25 | Loss: 0.00166434
Iteration 16/25 | Loss: 0.00165606
Iteration 17/25 | Loss: 0.00165581
Iteration 18/25 | Loss: 0.00165176
Iteration 19/25 | Loss: 0.00165176
Iteration 20/25 | Loss: 0.00164716
Iteration 21/25 | Loss: 0.00165249
Iteration 22/25 | Loss: 0.00165201
Iteration 23/25 | Loss: 0.00164738
Iteration 24/25 | Loss: 0.00164576
Iteration 25/25 | Loss: 0.00164554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18657124
Iteration 2/25 | Loss: 0.00505837
Iteration 3/25 | Loss: 0.00505835
Iteration 4/25 | Loss: 0.00505835
Iteration 5/25 | Loss: 0.00505835
Iteration 6/25 | Loss: 0.00505835
Iteration 7/25 | Loss: 0.00505835
Iteration 8/25 | Loss: 0.00505835
Iteration 9/25 | Loss: 0.00505835
Iteration 10/25 | Loss: 0.00505835
Iteration 11/25 | Loss: 0.00505835
Iteration 12/25 | Loss: 0.00505835
Iteration 13/25 | Loss: 0.00505835
Iteration 14/25 | Loss: 0.00505835
Iteration 15/25 | Loss: 0.00505835
Iteration 16/25 | Loss: 0.00505835
Iteration 17/25 | Loss: 0.00505835
Iteration 18/25 | Loss: 0.00505835
Iteration 19/25 | Loss: 0.00505835
Iteration 20/25 | Loss: 0.00505835
Iteration 21/25 | Loss: 0.00505835
Iteration 22/25 | Loss: 0.00505835
Iteration 23/25 | Loss: 0.00505835
Iteration 24/25 | Loss: 0.00505835
Iteration 25/25 | Loss: 0.00505835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00505835
Iteration 2/1000 | Loss: 0.00069667
Iteration 3/1000 | Loss: 0.00411304
Iteration 4/1000 | Loss: 0.00434084
Iteration 5/1000 | Loss: 0.00101699
Iteration 6/1000 | Loss: 0.00076542
Iteration 7/1000 | Loss: 0.00139373
Iteration 8/1000 | Loss: 0.00034306
Iteration 9/1000 | Loss: 0.00027231
Iteration 10/1000 | Loss: 0.00021221
Iteration 11/1000 | Loss: 0.00017899
Iteration 12/1000 | Loss: 0.00016288
Iteration 13/1000 | Loss: 0.00015291
Iteration 14/1000 | Loss: 0.00014401
Iteration 15/1000 | Loss: 0.00013787
Iteration 16/1000 | Loss: 0.00014562
Iteration 17/1000 | Loss: 0.00013685
Iteration 18/1000 | Loss: 0.00012997
Iteration 19/1000 | Loss: 0.00012765
Iteration 20/1000 | Loss: 0.00012513
Iteration 21/1000 | Loss: 0.00013332
Iteration 22/1000 | Loss: 0.00012143
Iteration 23/1000 | Loss: 0.00033045
Iteration 24/1000 | Loss: 0.00030113
Iteration 25/1000 | Loss: 0.00013829
Iteration 26/1000 | Loss: 0.00012101
Iteration 27/1000 | Loss: 0.00011548
Iteration 28/1000 | Loss: 0.00032738
Iteration 29/1000 | Loss: 0.00012118
Iteration 30/1000 | Loss: 0.00011283
Iteration 31/1000 | Loss: 0.00011475
Iteration 32/1000 | Loss: 0.00010709
Iteration 33/1000 | Loss: 0.00010604
Iteration 34/1000 | Loss: 0.00010490
Iteration 35/1000 | Loss: 0.00010368
Iteration 36/1000 | Loss: 0.00118535
Iteration 37/1000 | Loss: 0.00607866
Iteration 38/1000 | Loss: 0.00172573
Iteration 39/1000 | Loss: 0.00080783
Iteration 40/1000 | Loss: 0.00036104
Iteration 41/1000 | Loss: 0.00019122
Iteration 42/1000 | Loss: 0.00015289
Iteration 43/1000 | Loss: 0.00013057
Iteration 44/1000 | Loss: 0.00008525
Iteration 45/1000 | Loss: 0.00006775
Iteration 46/1000 | Loss: 0.00005514
Iteration 47/1000 | Loss: 0.00004818
Iteration 48/1000 | Loss: 0.00006267
Iteration 49/1000 | Loss: 0.00004627
Iteration 50/1000 | Loss: 0.00004298
Iteration 51/1000 | Loss: 0.00003995
Iteration 52/1000 | Loss: 0.00003927
Iteration 53/1000 | Loss: 0.00003679
Iteration 54/1000 | Loss: 0.00003665
Iteration 55/1000 | Loss: 0.00003673
Iteration 56/1000 | Loss: 0.00003432
Iteration 57/1000 | Loss: 0.00003399
Iteration 58/1000 | Loss: 0.00003388
Iteration 59/1000 | Loss: 0.00003317
Iteration 60/1000 | Loss: 0.00003294
Iteration 61/1000 | Loss: 0.00003251
Iteration 62/1000 | Loss: 0.00003216
Iteration 63/1000 | Loss: 0.00003211
Iteration 64/1000 | Loss: 0.00003213
Iteration 65/1000 | Loss: 0.00003213
Iteration 66/1000 | Loss: 0.00003191
Iteration 67/1000 | Loss: 0.00003211
Iteration 68/1000 | Loss: 0.00003189
Iteration 69/1000 | Loss: 0.00003183
Iteration 70/1000 | Loss: 0.00003183
Iteration 71/1000 | Loss: 0.00003183
Iteration 72/1000 | Loss: 0.00003182
Iteration 73/1000 | Loss: 0.00003182
Iteration 74/1000 | Loss: 0.00003182
Iteration 75/1000 | Loss: 0.00003182
Iteration 76/1000 | Loss: 0.00003182
Iteration 77/1000 | Loss: 0.00003182
Iteration 78/1000 | Loss: 0.00003182
Iteration 79/1000 | Loss: 0.00003182
Iteration 80/1000 | Loss: 0.00003182
Iteration 81/1000 | Loss: 0.00003182
Iteration 82/1000 | Loss: 0.00003181
Iteration 83/1000 | Loss: 0.00003191
Iteration 84/1000 | Loss: 0.00003191
Iteration 85/1000 | Loss: 0.00003191
Iteration 86/1000 | Loss: 0.00003191
Iteration 87/1000 | Loss: 0.00003191
Iteration 88/1000 | Loss: 0.00003608
Iteration 89/1000 | Loss: 0.00003608
Iteration 90/1000 | Loss: 0.00003935
Iteration 91/1000 | Loss: 0.00003267
Iteration 92/1000 | Loss: 0.00003209
Iteration 93/1000 | Loss: 0.00003289
Iteration 94/1000 | Loss: 0.00003207
Iteration 95/1000 | Loss: 0.00003205
Iteration 96/1000 | Loss: 0.00003204
Iteration 97/1000 | Loss: 0.00003266
Iteration 98/1000 | Loss: 0.00003265
Iteration 99/1000 | Loss: 0.00003278
Iteration 100/1000 | Loss: 0.00003250
Iteration 101/1000 | Loss: 0.00003225
Iteration 102/1000 | Loss: 0.00003204
Iteration 103/1000 | Loss: 0.00003174
Iteration 104/1000 | Loss: 0.00003174
Iteration 105/1000 | Loss: 0.00003174
Iteration 106/1000 | Loss: 0.00003174
Iteration 107/1000 | Loss: 0.00003174
Iteration 108/1000 | Loss: 0.00003174
Iteration 109/1000 | Loss: 0.00003174
Iteration 110/1000 | Loss: 0.00003174
Iteration 111/1000 | Loss: 0.00003174
Iteration 112/1000 | Loss: 0.00003174
Iteration 113/1000 | Loss: 0.00003173
Iteration 114/1000 | Loss: 0.00003173
Iteration 115/1000 | Loss: 0.00003173
Iteration 116/1000 | Loss: 0.00003173
Iteration 117/1000 | Loss: 0.00003172
Iteration 118/1000 | Loss: 0.00003172
Iteration 119/1000 | Loss: 0.00003172
Iteration 120/1000 | Loss: 0.00003172
Iteration 121/1000 | Loss: 0.00003172
Iteration 122/1000 | Loss: 0.00003172
Iteration 123/1000 | Loss: 0.00003171
Iteration 124/1000 | Loss: 0.00003171
Iteration 125/1000 | Loss: 0.00003182
Iteration 126/1000 | Loss: 0.00003228
Iteration 127/1000 | Loss: 0.00003182
Iteration 128/1000 | Loss: 0.00003181
Iteration 129/1000 | Loss: 0.00003235
Iteration 130/1000 | Loss: 0.00003190
Iteration 131/1000 | Loss: 0.00003167
Iteration 132/1000 | Loss: 0.00003172
Iteration 133/1000 | Loss: 0.00003172
Iteration 134/1000 | Loss: 0.00003171
Iteration 135/1000 | Loss: 0.00003171
Iteration 136/1000 | Loss: 0.00003171
Iteration 137/1000 | Loss: 0.00003171
Iteration 138/1000 | Loss: 0.00003171
Iteration 139/1000 | Loss: 0.00003170
Iteration 140/1000 | Loss: 0.00003170
Iteration 141/1000 | Loss: 0.00003170
Iteration 142/1000 | Loss: 0.00003170
Iteration 143/1000 | Loss: 0.00003170
Iteration 144/1000 | Loss: 0.00003162
Iteration 145/1000 | Loss: 0.00003162
Iteration 146/1000 | Loss: 0.00003162
Iteration 147/1000 | Loss: 0.00003162
Iteration 148/1000 | Loss: 0.00003162
Iteration 149/1000 | Loss: 0.00003162
Iteration 150/1000 | Loss: 0.00003162
Iteration 151/1000 | Loss: 0.00003162
Iteration 152/1000 | Loss: 0.00003162
Iteration 153/1000 | Loss: 0.00003162
Iteration 154/1000 | Loss: 0.00003162
Iteration 155/1000 | Loss: 0.00003162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.16161458613351e-05, 3.16161458613351e-05, 3.16161458613351e-05, 3.16161458613351e-05, 3.16161458613351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.16161458613351e-05

Optimization complete. Final v2v error: 4.224568843841553 mm

Highest mean error: 12.089573860168457 mm for frame 34

Lowest mean error: 3.106313943862915 mm for frame 0

Saving results

Total time: 166.60573983192444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409783
Iteration 2/25 | Loss: 0.00161538
Iteration 3/25 | Loss: 0.00153141
Iteration 4/25 | Loss: 0.00152324
Iteration 5/25 | Loss: 0.00152206
Iteration 6/25 | Loss: 0.00152206
Iteration 7/25 | Loss: 0.00152206
Iteration 8/25 | Loss: 0.00152206
Iteration 9/25 | Loss: 0.00152206
Iteration 10/25 | Loss: 0.00152206
Iteration 11/25 | Loss: 0.00152206
Iteration 12/25 | Loss: 0.00152206
Iteration 13/25 | Loss: 0.00152206
Iteration 14/25 | Loss: 0.00152206
Iteration 15/25 | Loss: 0.00152206
Iteration 16/25 | Loss: 0.00152206
Iteration 17/25 | Loss: 0.00152206
Iteration 18/25 | Loss: 0.00152206
Iteration 19/25 | Loss: 0.00152206
Iteration 20/25 | Loss: 0.00152206
Iteration 21/25 | Loss: 0.00152206
Iteration 22/25 | Loss: 0.00152206
Iteration 23/25 | Loss: 0.00152206
Iteration 24/25 | Loss: 0.00152206
Iteration 25/25 | Loss: 0.00152206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20366669
Iteration 2/25 | Loss: 0.00246054
Iteration 3/25 | Loss: 0.00246054
Iteration 4/25 | Loss: 0.00246054
Iteration 5/25 | Loss: 0.00246054
Iteration 6/25 | Loss: 0.00246054
Iteration 7/25 | Loss: 0.00246054
Iteration 8/25 | Loss: 0.00246054
Iteration 9/25 | Loss: 0.00246054
Iteration 10/25 | Loss: 0.00246054
Iteration 11/25 | Loss: 0.00246054
Iteration 12/25 | Loss: 0.00246054
Iteration 13/25 | Loss: 0.00246054
Iteration 14/25 | Loss: 0.00246054
Iteration 15/25 | Loss: 0.00246054
Iteration 16/25 | Loss: 0.00246054
Iteration 17/25 | Loss: 0.00246054
Iteration 18/25 | Loss: 0.00246054
Iteration 19/25 | Loss: 0.00246054
Iteration 20/25 | Loss: 0.00246054
Iteration 21/25 | Loss: 0.00246054
Iteration 22/25 | Loss: 0.00246054
Iteration 23/25 | Loss: 0.00246054
Iteration 24/25 | Loss: 0.00246054
Iteration 25/25 | Loss: 0.00246054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246054
Iteration 2/1000 | Loss: 0.00005040
Iteration 3/1000 | Loss: 0.00003072
Iteration 4/1000 | Loss: 0.00002511
Iteration 5/1000 | Loss: 0.00002284
Iteration 6/1000 | Loss: 0.00002203
Iteration 7/1000 | Loss: 0.00002114
Iteration 8/1000 | Loss: 0.00002065
Iteration 9/1000 | Loss: 0.00002042
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00001993
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001975
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00001974
Iteration 17/1000 | Loss: 0.00001974
Iteration 18/1000 | Loss: 0.00001973
Iteration 19/1000 | Loss: 0.00001973
Iteration 20/1000 | Loss: 0.00001972
Iteration 21/1000 | Loss: 0.00001970
Iteration 22/1000 | Loss: 0.00001970
Iteration 23/1000 | Loss: 0.00001969
Iteration 24/1000 | Loss: 0.00001969
Iteration 25/1000 | Loss: 0.00001968
Iteration 26/1000 | Loss: 0.00001968
Iteration 27/1000 | Loss: 0.00001968
Iteration 28/1000 | Loss: 0.00001967
Iteration 29/1000 | Loss: 0.00001967
Iteration 30/1000 | Loss: 0.00001967
Iteration 31/1000 | Loss: 0.00001967
Iteration 32/1000 | Loss: 0.00001966
Iteration 33/1000 | Loss: 0.00001966
Iteration 34/1000 | Loss: 0.00001965
Iteration 35/1000 | Loss: 0.00001965
Iteration 36/1000 | Loss: 0.00001965
Iteration 37/1000 | Loss: 0.00001965
Iteration 38/1000 | Loss: 0.00001965
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001964
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001963
Iteration 43/1000 | Loss: 0.00001963
Iteration 44/1000 | Loss: 0.00001962
Iteration 45/1000 | Loss: 0.00001961
Iteration 46/1000 | Loss: 0.00001961
Iteration 47/1000 | Loss: 0.00001961
Iteration 48/1000 | Loss: 0.00001960
Iteration 49/1000 | Loss: 0.00001960
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001960
Iteration 54/1000 | Loss: 0.00001960
Iteration 55/1000 | Loss: 0.00001960
Iteration 56/1000 | Loss: 0.00001960
Iteration 57/1000 | Loss: 0.00001960
Iteration 58/1000 | Loss: 0.00001960
Iteration 59/1000 | Loss: 0.00001959
Iteration 60/1000 | Loss: 0.00001959
Iteration 61/1000 | Loss: 0.00001959
Iteration 62/1000 | Loss: 0.00001959
Iteration 63/1000 | Loss: 0.00001958
Iteration 64/1000 | Loss: 0.00001958
Iteration 65/1000 | Loss: 0.00001958
Iteration 66/1000 | Loss: 0.00001958
Iteration 67/1000 | Loss: 0.00001958
Iteration 68/1000 | Loss: 0.00001958
Iteration 69/1000 | Loss: 0.00001958
Iteration 70/1000 | Loss: 0.00001958
Iteration 71/1000 | Loss: 0.00001958
Iteration 72/1000 | Loss: 0.00001958
Iteration 73/1000 | Loss: 0.00001958
Iteration 74/1000 | Loss: 0.00001957
Iteration 75/1000 | Loss: 0.00001957
Iteration 76/1000 | Loss: 0.00001957
Iteration 77/1000 | Loss: 0.00001956
Iteration 78/1000 | Loss: 0.00001956
Iteration 79/1000 | Loss: 0.00001956
Iteration 80/1000 | Loss: 0.00001955
Iteration 81/1000 | Loss: 0.00001955
Iteration 82/1000 | Loss: 0.00001955
Iteration 83/1000 | Loss: 0.00001955
Iteration 84/1000 | Loss: 0.00001955
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001954
Iteration 87/1000 | Loss: 0.00001954
Iteration 88/1000 | Loss: 0.00001954
Iteration 89/1000 | Loss: 0.00001953
Iteration 90/1000 | Loss: 0.00001953
Iteration 91/1000 | Loss: 0.00001953
Iteration 92/1000 | Loss: 0.00001953
Iteration 93/1000 | Loss: 0.00001952
Iteration 94/1000 | Loss: 0.00001952
Iteration 95/1000 | Loss: 0.00001952
Iteration 96/1000 | Loss: 0.00001952
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001952
Iteration 99/1000 | Loss: 0.00001951
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001951
Iteration 102/1000 | Loss: 0.00001951
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001950
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001950
Iteration 108/1000 | Loss: 0.00001950
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001949
Iteration 111/1000 | Loss: 0.00001949
Iteration 112/1000 | Loss: 0.00001949
Iteration 113/1000 | Loss: 0.00001949
Iteration 114/1000 | Loss: 0.00001949
Iteration 115/1000 | Loss: 0.00001949
Iteration 116/1000 | Loss: 0.00001949
Iteration 117/1000 | Loss: 0.00001949
Iteration 118/1000 | Loss: 0.00001948
Iteration 119/1000 | Loss: 0.00001948
Iteration 120/1000 | Loss: 0.00001948
Iteration 121/1000 | Loss: 0.00001948
Iteration 122/1000 | Loss: 0.00001947
Iteration 123/1000 | Loss: 0.00001947
Iteration 124/1000 | Loss: 0.00001947
Iteration 125/1000 | Loss: 0.00001947
Iteration 126/1000 | Loss: 0.00001947
Iteration 127/1000 | Loss: 0.00001947
Iteration 128/1000 | Loss: 0.00001947
Iteration 129/1000 | Loss: 0.00001947
Iteration 130/1000 | Loss: 0.00001947
Iteration 131/1000 | Loss: 0.00001947
Iteration 132/1000 | Loss: 0.00001947
Iteration 133/1000 | Loss: 0.00001947
Iteration 134/1000 | Loss: 0.00001947
Iteration 135/1000 | Loss: 0.00001947
Iteration 136/1000 | Loss: 0.00001947
Iteration 137/1000 | Loss: 0.00001947
Iteration 138/1000 | Loss: 0.00001947
Iteration 139/1000 | Loss: 0.00001946
Iteration 140/1000 | Loss: 0.00001946
Iteration 141/1000 | Loss: 0.00001946
Iteration 142/1000 | Loss: 0.00001946
Iteration 143/1000 | Loss: 0.00001946
Iteration 144/1000 | Loss: 0.00001946
Iteration 145/1000 | Loss: 0.00001946
Iteration 146/1000 | Loss: 0.00001946
Iteration 147/1000 | Loss: 0.00001946
Iteration 148/1000 | Loss: 0.00001946
Iteration 149/1000 | Loss: 0.00001946
Iteration 150/1000 | Loss: 0.00001946
Iteration 151/1000 | Loss: 0.00001946
Iteration 152/1000 | Loss: 0.00001946
Iteration 153/1000 | Loss: 0.00001946
Iteration 154/1000 | Loss: 0.00001946
Iteration 155/1000 | Loss: 0.00001946
Iteration 156/1000 | Loss: 0.00001946
Iteration 157/1000 | Loss: 0.00001946
Iteration 158/1000 | Loss: 0.00001946
Iteration 159/1000 | Loss: 0.00001946
Iteration 160/1000 | Loss: 0.00001946
Iteration 161/1000 | Loss: 0.00001946
Iteration 162/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.9459666873444803e-05, 1.9459666873444803e-05, 1.9459666873444803e-05, 1.9459666873444803e-05, 1.9459666873444803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9459666873444803e-05

Optimization complete. Final v2v error: 3.879992723464966 mm

Highest mean error: 4.122690200805664 mm for frame 121

Lowest mean error: 3.612009286880493 mm for frame 2

Saving results

Total time: 33.284396171569824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040039
Iteration 2/25 | Loss: 0.00274830
Iteration 3/25 | Loss: 0.00203124
Iteration 4/25 | Loss: 0.00178270
Iteration 5/25 | Loss: 0.00183654
Iteration 6/25 | Loss: 0.00183750
Iteration 7/25 | Loss: 0.00170314
Iteration 8/25 | Loss: 0.00158518
Iteration 9/25 | Loss: 0.00151040
Iteration 10/25 | Loss: 0.00146812
Iteration 11/25 | Loss: 0.00146450
Iteration 12/25 | Loss: 0.00146481
Iteration 13/25 | Loss: 0.00145523
Iteration 14/25 | Loss: 0.00146232
Iteration 15/25 | Loss: 0.00145268
Iteration 16/25 | Loss: 0.00144163
Iteration 17/25 | Loss: 0.00143586
Iteration 18/25 | Loss: 0.00142952
Iteration 19/25 | Loss: 0.00142645
Iteration 20/25 | Loss: 0.00142400
Iteration 21/25 | Loss: 0.00142665
Iteration 22/25 | Loss: 0.00142519
Iteration 23/25 | Loss: 0.00142541
Iteration 24/25 | Loss: 0.00141973
Iteration 25/25 | Loss: 0.00141877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22464395
Iteration 2/25 | Loss: 0.00392442
Iteration 3/25 | Loss: 0.00322734
Iteration 4/25 | Loss: 0.00322734
Iteration 5/25 | Loss: 0.00322734
Iteration 6/25 | Loss: 0.00322734
Iteration 7/25 | Loss: 0.00322734
Iteration 8/25 | Loss: 0.00322734
Iteration 9/25 | Loss: 0.00322734
Iteration 10/25 | Loss: 0.00322734
Iteration 11/25 | Loss: 0.00322734
Iteration 12/25 | Loss: 0.00322734
Iteration 13/25 | Loss: 0.00322734
Iteration 14/25 | Loss: 0.00322734
Iteration 15/25 | Loss: 0.00322734
Iteration 16/25 | Loss: 0.00322734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0032273412216454744, 0.0032273412216454744, 0.0032273412216454744, 0.0032273412216454744, 0.0032273412216454744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032273412216454744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322734
Iteration 2/1000 | Loss: 0.00064381
Iteration 3/1000 | Loss: 0.00052397
Iteration 4/1000 | Loss: 0.00032918
Iteration 5/1000 | Loss: 0.00044367
Iteration 6/1000 | Loss: 0.00090290
Iteration 7/1000 | Loss: 0.00018359
Iteration 8/1000 | Loss: 0.00024839
Iteration 9/1000 | Loss: 0.00034269
Iteration 10/1000 | Loss: 0.00018170
Iteration 11/1000 | Loss: 0.00020331
Iteration 12/1000 | Loss: 0.00086536
Iteration 13/1000 | Loss: 0.00275396
Iteration 14/1000 | Loss: 0.00089333
Iteration 15/1000 | Loss: 0.00025458
Iteration 16/1000 | Loss: 0.00019958
Iteration 17/1000 | Loss: 0.00016139
Iteration 18/1000 | Loss: 0.00008315
Iteration 19/1000 | Loss: 0.00007517
Iteration 20/1000 | Loss: 0.00021116
Iteration 21/1000 | Loss: 0.00028235
Iteration 22/1000 | Loss: 0.00015257
Iteration 23/1000 | Loss: 0.00006349
Iteration 24/1000 | Loss: 0.00024768
Iteration 25/1000 | Loss: 0.00005915
Iteration 26/1000 | Loss: 0.00005776
Iteration 27/1000 | Loss: 0.00029774
Iteration 28/1000 | Loss: 0.00005763
Iteration 29/1000 | Loss: 0.00028602
Iteration 30/1000 | Loss: 0.00011641
Iteration 31/1000 | Loss: 0.00005931
Iteration 32/1000 | Loss: 0.00005305
Iteration 33/1000 | Loss: 0.00005143
Iteration 34/1000 | Loss: 0.00005118
Iteration 35/1000 | Loss: 0.00005002
Iteration 36/1000 | Loss: 0.00004958
Iteration 37/1000 | Loss: 0.00028078
Iteration 38/1000 | Loss: 0.00017511
Iteration 39/1000 | Loss: 0.00006179
Iteration 40/1000 | Loss: 0.00004868
Iteration 41/1000 | Loss: 0.00004640
Iteration 42/1000 | Loss: 0.00004560
Iteration 43/1000 | Loss: 0.00004448
Iteration 44/1000 | Loss: 0.00004407
Iteration 45/1000 | Loss: 0.00004390
Iteration 46/1000 | Loss: 0.00004389
Iteration 47/1000 | Loss: 0.00004382
Iteration 48/1000 | Loss: 0.00004381
Iteration 49/1000 | Loss: 0.00004379
Iteration 50/1000 | Loss: 0.00004378
Iteration 51/1000 | Loss: 0.00004377
Iteration 52/1000 | Loss: 0.00004368
Iteration 53/1000 | Loss: 0.00004368
Iteration 54/1000 | Loss: 0.00004368
Iteration 55/1000 | Loss: 0.00004368
Iteration 56/1000 | Loss: 0.00004367
Iteration 57/1000 | Loss: 0.00004367
Iteration 58/1000 | Loss: 0.00004449
Iteration 59/1000 | Loss: 0.00004393
Iteration 60/1000 | Loss: 0.00004373
Iteration 61/1000 | Loss: 0.00004367
Iteration 62/1000 | Loss: 0.00004366
Iteration 63/1000 | Loss: 0.00004450
Iteration 64/1000 | Loss: 0.00004393
Iteration 65/1000 | Loss: 0.00004367
Iteration 66/1000 | Loss: 0.00004366
Iteration 67/1000 | Loss: 0.00004439
Iteration 68/1000 | Loss: 0.00004392
Iteration 69/1000 | Loss: 0.00004442
Iteration 70/1000 | Loss: 0.00004396
Iteration 71/1000 | Loss: 0.00004468
Iteration 72/1000 | Loss: 0.00004414
Iteration 73/1000 | Loss: 0.00004476
Iteration 74/1000 | Loss: 0.00004433
Iteration 75/1000 | Loss: 0.00004432
Iteration 76/1000 | Loss: 0.00004425
Iteration 77/1000 | Loss: 0.00004402
Iteration 78/1000 | Loss: 0.00004439
Iteration 79/1000 | Loss: 0.00004422
Iteration 80/1000 | Loss: 0.00004423
Iteration 81/1000 | Loss: 0.00004411
Iteration 82/1000 | Loss: 0.00004436
Iteration 83/1000 | Loss: 0.00004410
Iteration 84/1000 | Loss: 0.00004426
Iteration 85/1000 | Loss: 0.00004397
Iteration 86/1000 | Loss: 0.00004441
Iteration 87/1000 | Loss: 0.00004413
Iteration 88/1000 | Loss: 0.00004433
Iteration 89/1000 | Loss: 0.00004413
Iteration 90/1000 | Loss: 0.00004448
Iteration 91/1000 | Loss: 0.00004420
Iteration 92/1000 | Loss: 0.00004419
Iteration 93/1000 | Loss: 0.00004411
Iteration 94/1000 | Loss: 0.00004398
Iteration 95/1000 | Loss: 0.00004437
Iteration 96/1000 | Loss: 0.00004408
Iteration 97/1000 | Loss: 0.00004417
Iteration 98/1000 | Loss: 0.00004393
Iteration 99/1000 | Loss: 0.00004428
Iteration 100/1000 | Loss: 0.00004395
Iteration 101/1000 | Loss: 0.00004430
Iteration 102/1000 | Loss: 0.00004391
Iteration 103/1000 | Loss: 0.00004428
Iteration 104/1000 | Loss: 0.00004385
Iteration 105/1000 | Loss: 0.00004421
Iteration 106/1000 | Loss: 0.00004387
Iteration 107/1000 | Loss: 0.00004405
Iteration 108/1000 | Loss: 0.00004384
Iteration 109/1000 | Loss: 0.00004421
Iteration 110/1000 | Loss: 0.00004397
Iteration 111/1000 | Loss: 0.00004423
Iteration 112/1000 | Loss: 0.00004400
Iteration 113/1000 | Loss: 0.00004439
Iteration 114/1000 | Loss: 0.00004400
Iteration 115/1000 | Loss: 0.00004432
Iteration 116/1000 | Loss: 0.00004396
Iteration 117/1000 | Loss: 0.00004430
Iteration 118/1000 | Loss: 0.00004397
Iteration 119/1000 | Loss: 0.00004434
Iteration 120/1000 | Loss: 0.00004398
Iteration 121/1000 | Loss: 0.00004409
Iteration 122/1000 | Loss: 0.00004392
Iteration 123/1000 | Loss: 0.00004437
Iteration 124/1000 | Loss: 0.00004402
Iteration 125/1000 | Loss: 0.00004432
Iteration 126/1000 | Loss: 0.00004432
Iteration 127/1000 | Loss: 0.00004389
Iteration 128/1000 | Loss: 0.00004408
Iteration 129/1000 | Loss: 0.00004380
Iteration 130/1000 | Loss: 0.00004402
Iteration 131/1000 | Loss: 0.00004384
Iteration 132/1000 | Loss: 0.00004419
Iteration 133/1000 | Loss: 0.00004393
Iteration 134/1000 | Loss: 0.00004392
Iteration 135/1000 | Loss: 0.00004389
Iteration 136/1000 | Loss: 0.00004378
Iteration 137/1000 | Loss: 0.00004401
Iteration 138/1000 | Loss: 0.00004389
Iteration 139/1000 | Loss: 0.00004389
Iteration 140/1000 | Loss: 0.00004401
Iteration 141/1000 | Loss: 0.00004379
Iteration 142/1000 | Loss: 0.00004409
Iteration 143/1000 | Loss: 0.00004383
Iteration 144/1000 | Loss: 0.00004403
Iteration 145/1000 | Loss: 0.00004382
Iteration 146/1000 | Loss: 0.00004416
Iteration 147/1000 | Loss: 0.00004390
Iteration 148/1000 | Loss: 0.00004409
Iteration 149/1000 | Loss: 0.00004385
Iteration 150/1000 | Loss: 0.00004385
Iteration 151/1000 | Loss: 0.00004417
Iteration 152/1000 | Loss: 0.00004384
Iteration 153/1000 | Loss: 0.00004400
Iteration 154/1000 | Loss: 0.00004387
Iteration 155/1000 | Loss: 0.00004386
Iteration 156/1000 | Loss: 0.00004403
Iteration 157/1000 | Loss: 0.00004371
Iteration 158/1000 | Loss: 0.00004389
Iteration 159/1000 | Loss: 0.00004382
Iteration 160/1000 | Loss: 0.00004418
Iteration 161/1000 | Loss: 0.00004380
Iteration 162/1000 | Loss: 0.00004430
Iteration 163/1000 | Loss: 0.00004384
Iteration 164/1000 | Loss: 0.00004429
Iteration 165/1000 | Loss: 0.00004383
Iteration 166/1000 | Loss: 0.00004383
Iteration 167/1000 | Loss: 0.00004420
Iteration 168/1000 | Loss: 0.00004393
Iteration 169/1000 | Loss: 0.00004427
Iteration 170/1000 | Loss: 0.00004390
Iteration 171/1000 | Loss: 0.00004425
Iteration 172/1000 | Loss: 0.00004390
Iteration 173/1000 | Loss: 0.00004426
Iteration 174/1000 | Loss: 0.00004392
Iteration 175/1000 | Loss: 0.00004440
Iteration 176/1000 | Loss: 0.00004393
Iteration 177/1000 | Loss: 0.00004439
Iteration 178/1000 | Loss: 0.00004394
Iteration 179/1000 | Loss: 0.00004393
Iteration 180/1000 | Loss: 0.00004422
Iteration 181/1000 | Loss: 0.00004379
Iteration 182/1000 | Loss: 0.00004410
Iteration 183/1000 | Loss: 0.00004380
Iteration 184/1000 | Loss: 0.00004413
Iteration 185/1000 | Loss: 0.00004377
Iteration 186/1000 | Loss: 0.00004409
Iteration 187/1000 | Loss: 0.00004384
Iteration 188/1000 | Loss: 0.00004422
Iteration 189/1000 | Loss: 0.00004385
Iteration 190/1000 | Loss: 0.00004423
Iteration 191/1000 | Loss: 0.00004387
Iteration 192/1000 | Loss: 0.00004391
Iteration 193/1000 | Loss: 0.00004377
Iteration 194/1000 | Loss: 0.00004395
Iteration 195/1000 | Loss: 0.00004379
Iteration 196/1000 | Loss: 0.00004387
Iteration 197/1000 | Loss: 0.00004381
Iteration 198/1000 | Loss: 0.00004380
Iteration 199/1000 | Loss: 0.00004398
Iteration 200/1000 | Loss: 0.00004379
Iteration 201/1000 | Loss: 0.00004395
Iteration 202/1000 | Loss: 0.00004377
Iteration 203/1000 | Loss: 0.00004393
Iteration 204/1000 | Loss: 0.00004374
Iteration 205/1000 | Loss: 0.00004391
Iteration 206/1000 | Loss: 0.00004375
Iteration 207/1000 | Loss: 0.00004395
Iteration 208/1000 | Loss: 0.00004364
Iteration 209/1000 | Loss: 0.00004377
Iteration 210/1000 | Loss: 0.00004359
Iteration 211/1000 | Loss: 0.00004355
Iteration 212/1000 | Loss: 0.00004353
Iteration 213/1000 | Loss: 0.00004384
Iteration 214/1000 | Loss: 0.00004369
Iteration 215/1000 | Loss: 0.00004349
Iteration 216/1000 | Loss: 0.00004348
Iteration 217/1000 | Loss: 0.00004377
Iteration 218/1000 | Loss: 0.00004367
Iteration 219/1000 | Loss: 0.00004400
Iteration 220/1000 | Loss: 0.00004372
Iteration 221/1000 | Loss: 0.00004397
Iteration 222/1000 | Loss: 0.00004369
Iteration 223/1000 | Loss: 0.00004385
Iteration 224/1000 | Loss: 0.00004371
Iteration 225/1000 | Loss: 0.00004399
Iteration 226/1000 | Loss: 0.00004380
Iteration 227/1000 | Loss: 0.00004404
Iteration 228/1000 | Loss: 0.00004375
Iteration 229/1000 | Loss: 0.00004398
Iteration 230/1000 | Loss: 0.00004377
Iteration 231/1000 | Loss: 0.00004387
Iteration 232/1000 | Loss: 0.00004372
Iteration 233/1000 | Loss: 0.00004372
Iteration 234/1000 | Loss: 0.00004394
Iteration 235/1000 | Loss: 0.00004373
Iteration 236/1000 | Loss: 0.00004393
Iteration 237/1000 | Loss: 0.00004377
Iteration 238/1000 | Loss: 0.00004399
Iteration 239/1000 | Loss: 0.00004381
Iteration 240/1000 | Loss: 0.00004403
Iteration 241/1000 | Loss: 0.00004379
Iteration 242/1000 | Loss: 0.00004378
Iteration 243/1000 | Loss: 0.00004377
Iteration 244/1000 | Loss: 0.00004376
Iteration 245/1000 | Loss: 0.00004383
Iteration 246/1000 | Loss: 0.00004371
Iteration 247/1000 | Loss: 0.00004397
Iteration 248/1000 | Loss: 0.00004382
Iteration 249/1000 | Loss: 0.00004382
Iteration 250/1000 | Loss: 0.00004402
Iteration 251/1000 | Loss: 0.00004374
Iteration 252/1000 | Loss: 0.00004392
Iteration 253/1000 | Loss: 0.00004375
Iteration 254/1000 | Loss: 0.00004396
Iteration 255/1000 | Loss: 0.00004372
Iteration 256/1000 | Loss: 0.00004402
Iteration 257/1000 | Loss: 0.00004375
Iteration 258/1000 | Loss: 0.00004389
Iteration 259/1000 | Loss: 0.00004376
Iteration 260/1000 | Loss: 0.00004408
Iteration 261/1000 | Loss: 0.00004379
Iteration 262/1000 | Loss: 0.00004412
Iteration 263/1000 | Loss: 0.00004382
Iteration 264/1000 | Loss: 0.00004382
Iteration 265/1000 | Loss: 0.00004372
Iteration 266/1000 | Loss: 0.00004397
Iteration 267/1000 | Loss: 0.00004375
Iteration 268/1000 | Loss: 0.00004375
Iteration 269/1000 | Loss: 0.00004395
Iteration 270/1000 | Loss: 0.00004377
Iteration 271/1000 | Loss: 0.00004399
Iteration 272/1000 | Loss: 0.00004377
Iteration 273/1000 | Loss: 0.00004376
Iteration 274/1000 | Loss: 0.00004410
Iteration 275/1000 | Loss: 0.00004372
Iteration 276/1000 | Loss: 0.00004387
Iteration 277/1000 | Loss: 0.00004373
Iteration 278/1000 | Loss: 0.00004394
Iteration 279/1000 | Loss: 0.00004373
Iteration 280/1000 | Loss: 0.00004398
Iteration 281/1000 | Loss: 0.00004381
Iteration 282/1000 | Loss: 0.00004338
Iteration 283/1000 | Loss: 0.00004338
Iteration 284/1000 | Loss: 0.00004338
Iteration 285/1000 | Loss: 0.00004338
Iteration 286/1000 | Loss: 0.00004338
Iteration 287/1000 | Loss: 0.00004338
Iteration 288/1000 | Loss: 0.00004338
Iteration 289/1000 | Loss: 0.00004338
Iteration 290/1000 | Loss: 0.00004338
Iteration 291/1000 | Loss: 0.00004338
Iteration 292/1000 | Loss: 0.00004338
Iteration 293/1000 | Loss: 0.00004338
Iteration 294/1000 | Loss: 0.00004338
Iteration 295/1000 | Loss: 0.00004338
Iteration 296/1000 | Loss: 0.00004338
Iteration 297/1000 | Loss: 0.00004338
Iteration 298/1000 | Loss: 0.00004338
Iteration 299/1000 | Loss: 0.00004338
Iteration 300/1000 | Loss: 0.00004338
Iteration 301/1000 | Loss: 0.00004338
Iteration 302/1000 | Loss: 0.00004338
Iteration 303/1000 | Loss: 0.00004338
Iteration 304/1000 | Loss: 0.00004338
Iteration 305/1000 | Loss: 0.00004338
Iteration 306/1000 | Loss: 0.00004338
Iteration 307/1000 | Loss: 0.00004338
Iteration 308/1000 | Loss: 0.00004338
Iteration 309/1000 | Loss: 0.00004338
Iteration 310/1000 | Loss: 0.00004338
Iteration 311/1000 | Loss: 0.00004338
Iteration 312/1000 | Loss: 0.00004338
Iteration 313/1000 | Loss: 0.00004338
Iteration 314/1000 | Loss: 0.00004338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [4.337544669397175e-05, 4.337544669397175e-05, 4.337544669397175e-05, 4.337544669397175e-05, 4.337544669397175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.337544669397175e-05

Optimization complete. Final v2v error: 4.274302005767822 mm

Highest mean error: 22.15270233154297 mm for frame 41

Lowest mean error: 3.3013665676116943 mm for frame 109

Saving results

Total time: 338.1506314277649
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951430
Iteration 2/25 | Loss: 0.00186813
Iteration 3/25 | Loss: 0.00157126
Iteration 4/25 | Loss: 0.00154233
Iteration 5/25 | Loss: 0.00153505
Iteration 6/25 | Loss: 0.00153395
Iteration 7/25 | Loss: 0.00153395
Iteration 8/25 | Loss: 0.00153395
Iteration 9/25 | Loss: 0.00153395
Iteration 10/25 | Loss: 0.00153395
Iteration 11/25 | Loss: 0.00153395
Iteration 12/25 | Loss: 0.00153395
Iteration 13/25 | Loss: 0.00153395
Iteration 14/25 | Loss: 0.00153395
Iteration 15/25 | Loss: 0.00153395
Iteration 16/25 | Loss: 0.00153395
Iteration 17/25 | Loss: 0.00153395
Iteration 18/25 | Loss: 0.00153395
Iteration 19/25 | Loss: 0.00153395
Iteration 20/25 | Loss: 0.00153395
Iteration 21/25 | Loss: 0.00153395
Iteration 22/25 | Loss: 0.00153395
Iteration 23/25 | Loss: 0.00153395
Iteration 24/25 | Loss: 0.00153395
Iteration 25/25 | Loss: 0.00153395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77666646
Iteration 2/25 | Loss: 0.00132838
Iteration 3/25 | Loss: 0.00132838
Iteration 4/25 | Loss: 0.00132838
Iteration 5/25 | Loss: 0.00132838
Iteration 6/25 | Loss: 0.00132838
Iteration 7/25 | Loss: 0.00132838
Iteration 8/25 | Loss: 0.00132838
Iteration 9/25 | Loss: 0.00132838
Iteration 10/25 | Loss: 0.00132838
Iteration 11/25 | Loss: 0.00132838
Iteration 12/25 | Loss: 0.00132838
Iteration 13/25 | Loss: 0.00132838
Iteration 14/25 | Loss: 0.00132838
Iteration 15/25 | Loss: 0.00132838
Iteration 16/25 | Loss: 0.00132838
Iteration 17/25 | Loss: 0.00132838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013283772859722376, 0.0013283772859722376, 0.0013283772859722376, 0.0013283772859722376, 0.0013283772859722376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013283772859722376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132838
Iteration 2/1000 | Loss: 0.00005031
Iteration 3/1000 | Loss: 0.00003823
Iteration 4/1000 | Loss: 0.00003433
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00003003
Iteration 7/1000 | Loss: 0.00002931
Iteration 8/1000 | Loss: 0.00002885
Iteration 9/1000 | Loss: 0.00002859
Iteration 10/1000 | Loss: 0.00002834
Iteration 11/1000 | Loss: 0.00002823
Iteration 12/1000 | Loss: 0.00002809
Iteration 13/1000 | Loss: 0.00002805
Iteration 14/1000 | Loss: 0.00002805
Iteration 15/1000 | Loss: 0.00002804
Iteration 16/1000 | Loss: 0.00002796
Iteration 17/1000 | Loss: 0.00002791
Iteration 18/1000 | Loss: 0.00002791
Iteration 19/1000 | Loss: 0.00002791
Iteration 20/1000 | Loss: 0.00002791
Iteration 21/1000 | Loss: 0.00002791
Iteration 22/1000 | Loss: 0.00002791
Iteration 23/1000 | Loss: 0.00002791
Iteration 24/1000 | Loss: 0.00002791
Iteration 25/1000 | Loss: 0.00002790
Iteration 26/1000 | Loss: 0.00002790
Iteration 27/1000 | Loss: 0.00002790
Iteration 28/1000 | Loss: 0.00002790
Iteration 29/1000 | Loss: 0.00002789
Iteration 30/1000 | Loss: 0.00002789
Iteration 31/1000 | Loss: 0.00002789
Iteration 32/1000 | Loss: 0.00002789
Iteration 33/1000 | Loss: 0.00002787
Iteration 34/1000 | Loss: 0.00002787
Iteration 35/1000 | Loss: 0.00002787
Iteration 36/1000 | Loss: 0.00002787
Iteration 37/1000 | Loss: 0.00002786
Iteration 38/1000 | Loss: 0.00002786
Iteration 39/1000 | Loss: 0.00002785
Iteration 40/1000 | Loss: 0.00002785
Iteration 41/1000 | Loss: 0.00002784
Iteration 42/1000 | Loss: 0.00002784
Iteration 43/1000 | Loss: 0.00002784
Iteration 44/1000 | Loss: 0.00002783
Iteration 45/1000 | Loss: 0.00002783
Iteration 46/1000 | Loss: 0.00002783
Iteration 47/1000 | Loss: 0.00002783
Iteration 48/1000 | Loss: 0.00002782
Iteration 49/1000 | Loss: 0.00002782
Iteration 50/1000 | Loss: 0.00002782
Iteration 51/1000 | Loss: 0.00002782
Iteration 52/1000 | Loss: 0.00002782
Iteration 53/1000 | Loss: 0.00002782
Iteration 54/1000 | Loss: 0.00002782
Iteration 55/1000 | Loss: 0.00002782
Iteration 56/1000 | Loss: 0.00002782
Iteration 57/1000 | Loss: 0.00002781
Iteration 58/1000 | Loss: 0.00002781
Iteration 59/1000 | Loss: 0.00002781
Iteration 60/1000 | Loss: 0.00002781
Iteration 61/1000 | Loss: 0.00002781
Iteration 62/1000 | Loss: 0.00002781
Iteration 63/1000 | Loss: 0.00002781
Iteration 64/1000 | Loss: 0.00002780
Iteration 65/1000 | Loss: 0.00002780
Iteration 66/1000 | Loss: 0.00002780
Iteration 67/1000 | Loss: 0.00002780
Iteration 68/1000 | Loss: 0.00002780
Iteration 69/1000 | Loss: 0.00002780
Iteration 70/1000 | Loss: 0.00002780
Iteration 71/1000 | Loss: 0.00002780
Iteration 72/1000 | Loss: 0.00002780
Iteration 73/1000 | Loss: 0.00002780
Iteration 74/1000 | Loss: 0.00002780
Iteration 75/1000 | Loss: 0.00002780
Iteration 76/1000 | Loss: 0.00002780
Iteration 77/1000 | Loss: 0.00002778
Iteration 78/1000 | Loss: 0.00002778
Iteration 79/1000 | Loss: 0.00002778
Iteration 80/1000 | Loss: 0.00002778
Iteration 81/1000 | Loss: 0.00002778
Iteration 82/1000 | Loss: 0.00002778
Iteration 83/1000 | Loss: 0.00002778
Iteration 84/1000 | Loss: 0.00002778
Iteration 85/1000 | Loss: 0.00002777
Iteration 86/1000 | Loss: 0.00002776
Iteration 87/1000 | Loss: 0.00002776
Iteration 88/1000 | Loss: 0.00002776
Iteration 89/1000 | Loss: 0.00002776
Iteration 90/1000 | Loss: 0.00002775
Iteration 91/1000 | Loss: 0.00002775
Iteration 92/1000 | Loss: 0.00002775
Iteration 93/1000 | Loss: 0.00002775
Iteration 94/1000 | Loss: 0.00002775
Iteration 95/1000 | Loss: 0.00002775
Iteration 96/1000 | Loss: 0.00002775
Iteration 97/1000 | Loss: 0.00002775
Iteration 98/1000 | Loss: 0.00002775
Iteration 99/1000 | Loss: 0.00002774
Iteration 100/1000 | Loss: 0.00002774
Iteration 101/1000 | Loss: 0.00002774
Iteration 102/1000 | Loss: 0.00002774
Iteration 103/1000 | Loss: 0.00002774
Iteration 104/1000 | Loss: 0.00002774
Iteration 105/1000 | Loss: 0.00002774
Iteration 106/1000 | Loss: 0.00002774
Iteration 107/1000 | Loss: 0.00002774
Iteration 108/1000 | Loss: 0.00002774
Iteration 109/1000 | Loss: 0.00002774
Iteration 110/1000 | Loss: 0.00002774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.7742864403990097e-05, 2.7742864403990097e-05, 2.7742864403990097e-05, 2.7742864403990097e-05, 2.7742864403990097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7742864403990097e-05

Optimization complete. Final v2v error: 4.600401878356934 mm

Highest mean error: 4.85186767578125 mm for frame 33

Lowest mean error: 4.435338020324707 mm for frame 104

Saving results

Total time: 33.96776866912842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105653
Iteration 2/25 | Loss: 0.01105653
Iteration 3/25 | Loss: 0.01105653
Iteration 4/25 | Loss: 0.01105653
Iteration 5/25 | Loss: 0.01105652
Iteration 6/25 | Loss: 0.00275024
Iteration 7/25 | Loss: 0.00197088
Iteration 8/25 | Loss: 0.00165900
Iteration 9/25 | Loss: 0.00161994
Iteration 10/25 | Loss: 0.00154821
Iteration 11/25 | Loss: 0.00146526
Iteration 12/25 | Loss: 0.00143163
Iteration 13/25 | Loss: 0.00139303
Iteration 14/25 | Loss: 0.00138132
Iteration 15/25 | Loss: 0.00137151
Iteration 16/25 | Loss: 0.00137072
Iteration 17/25 | Loss: 0.00136875
Iteration 18/25 | Loss: 0.00136659
Iteration 19/25 | Loss: 0.00136447
Iteration 20/25 | Loss: 0.00136517
Iteration 21/25 | Loss: 0.00136141
Iteration 22/25 | Loss: 0.00136186
Iteration 23/25 | Loss: 0.00136012
Iteration 24/25 | Loss: 0.00135814
Iteration 25/25 | Loss: 0.00135665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21558976
Iteration 2/25 | Loss: 0.00299078
Iteration 3/25 | Loss: 0.00306762
Iteration 4/25 | Loss: 0.00306762
Iteration 5/25 | Loss: 0.00306762
Iteration 6/25 | Loss: 0.00307019
Iteration 7/25 | Loss: 0.00283102
Iteration 8/25 | Loss: 0.00278369
Iteration 9/25 | Loss: 0.00278273
Iteration 10/25 | Loss: 0.00278273
Iteration 11/25 | Loss: 0.00278273
Iteration 12/25 | Loss: 0.00278273
Iteration 13/25 | Loss: 0.00278273
Iteration 14/25 | Loss: 0.00278273
Iteration 15/25 | Loss: 0.00278273
Iteration 16/25 | Loss: 0.00278273
Iteration 17/25 | Loss: 0.00278273
Iteration 18/25 | Loss: 0.00278273
Iteration 19/25 | Loss: 0.00278273
Iteration 20/25 | Loss: 0.00278273
Iteration 21/25 | Loss: 0.00278273
Iteration 22/25 | Loss: 0.00278273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0027827287558466196, 0.0027827287558466196, 0.0027827287558466196, 0.0027827287558466196, 0.0027827287558466196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027827287558466196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278273
Iteration 2/1000 | Loss: 0.00049154
Iteration 3/1000 | Loss: 0.00036533
Iteration 4/1000 | Loss: 0.00021486
Iteration 5/1000 | Loss: 0.00043636
Iteration 6/1000 | Loss: 0.00071377
Iteration 7/1000 | Loss: 0.00054086
Iteration 8/1000 | Loss: 0.00013462
Iteration 9/1000 | Loss: 0.00008284
Iteration 10/1000 | Loss: 0.00019827
Iteration 11/1000 | Loss: 0.00035880
Iteration 12/1000 | Loss: 0.00044992
Iteration 13/1000 | Loss: 0.00010906
Iteration 14/1000 | Loss: 0.00020601
Iteration 15/1000 | Loss: 0.00024427
Iteration 16/1000 | Loss: 0.00016799
Iteration 17/1000 | Loss: 0.00027513
Iteration 18/1000 | Loss: 0.00011725
Iteration 19/1000 | Loss: 0.00035274
Iteration 20/1000 | Loss: 0.00013555
Iteration 21/1000 | Loss: 0.00007364
Iteration 22/1000 | Loss: 0.00006909
Iteration 23/1000 | Loss: 0.00010604
Iteration 24/1000 | Loss: 0.00012095
Iteration 25/1000 | Loss: 0.00017115
Iteration 26/1000 | Loss: 0.00017717
Iteration 27/1000 | Loss: 0.00014885
Iteration 28/1000 | Loss: 0.00016537
Iteration 29/1000 | Loss: 0.00014476
Iteration 30/1000 | Loss: 0.00022037
Iteration 31/1000 | Loss: 0.00007024
Iteration 32/1000 | Loss: 0.00015474
Iteration 33/1000 | Loss: 0.00040311
Iteration 34/1000 | Loss: 0.00008018
Iteration 35/1000 | Loss: 0.00008242
Iteration 36/1000 | Loss: 0.00005555
Iteration 37/1000 | Loss: 0.00006039
Iteration 38/1000 | Loss: 0.00005574
Iteration 39/1000 | Loss: 0.00018125
Iteration 40/1000 | Loss: 0.00005894
Iteration 41/1000 | Loss: 0.00017254
Iteration 42/1000 | Loss: 0.00015023
Iteration 43/1000 | Loss: 0.00014201
Iteration 44/1000 | Loss: 0.00005344
Iteration 45/1000 | Loss: 0.00004842
Iteration 46/1000 | Loss: 0.00004734
Iteration 47/1000 | Loss: 0.00021467
Iteration 48/1000 | Loss: 0.00005243
Iteration 49/1000 | Loss: 0.00007255
Iteration 50/1000 | Loss: 0.00011813
Iteration 51/1000 | Loss: 0.00004577
Iteration 52/1000 | Loss: 0.00025125
Iteration 53/1000 | Loss: 0.00015301
Iteration 54/1000 | Loss: 0.00005278
Iteration 55/1000 | Loss: 0.00004414
Iteration 56/1000 | Loss: 0.00006316
Iteration 57/1000 | Loss: 0.00004217
Iteration 58/1000 | Loss: 0.00004196
Iteration 59/1000 | Loss: 0.00004247
Iteration 60/1000 | Loss: 0.00004101
Iteration 61/1000 | Loss: 0.00019783
Iteration 62/1000 | Loss: 0.00024118
Iteration 63/1000 | Loss: 0.00019422
Iteration 64/1000 | Loss: 0.00012706
Iteration 65/1000 | Loss: 0.00006654
Iteration 66/1000 | Loss: 0.00006413
Iteration 67/1000 | Loss: 0.00004352
Iteration 68/1000 | Loss: 0.00004281
Iteration 69/1000 | Loss: 0.00019521
Iteration 70/1000 | Loss: 0.00004963
Iteration 71/1000 | Loss: 0.00010540
Iteration 72/1000 | Loss: 0.00010927
Iteration 73/1000 | Loss: 0.00007991
Iteration 74/1000 | Loss: 0.00003914
Iteration 75/1000 | Loss: 0.00003860
Iteration 76/1000 | Loss: 0.00010838
Iteration 77/1000 | Loss: 0.00027574
Iteration 78/1000 | Loss: 0.00015924
Iteration 79/1000 | Loss: 0.00027475
Iteration 80/1000 | Loss: 0.00006891
Iteration 81/1000 | Loss: 0.00005008
Iteration 82/1000 | Loss: 0.00003919
Iteration 83/1000 | Loss: 0.00016594
Iteration 84/1000 | Loss: 0.00034049
Iteration 85/1000 | Loss: 0.00011644
Iteration 86/1000 | Loss: 0.00003858
Iteration 87/1000 | Loss: 0.00003721
Iteration 88/1000 | Loss: 0.00025277
Iteration 89/1000 | Loss: 0.00030570
Iteration 90/1000 | Loss: 0.00003888
Iteration 91/1000 | Loss: 0.00021449
Iteration 92/1000 | Loss: 0.00031076
Iteration 93/1000 | Loss: 0.00004037
Iteration 94/1000 | Loss: 0.00021173
Iteration 95/1000 | Loss: 0.00022576
Iteration 96/1000 | Loss: 0.00015205
Iteration 97/1000 | Loss: 0.00005427
Iteration 98/1000 | Loss: 0.00018485
Iteration 99/1000 | Loss: 0.00008333
Iteration 100/1000 | Loss: 0.00008305
Iteration 101/1000 | Loss: 0.00004292
Iteration 102/1000 | Loss: 0.00008612
Iteration 103/1000 | Loss: 0.00003778
Iteration 104/1000 | Loss: 0.00007488
Iteration 105/1000 | Loss: 0.00004090
Iteration 106/1000 | Loss: 0.00004667
Iteration 107/1000 | Loss: 0.00003353
Iteration 108/1000 | Loss: 0.00009180
Iteration 109/1000 | Loss: 0.00003314
Iteration 110/1000 | Loss: 0.00003400
Iteration 111/1000 | Loss: 0.00003282
Iteration 112/1000 | Loss: 0.00003906
Iteration 113/1000 | Loss: 0.00015624
Iteration 114/1000 | Loss: 0.00004425
Iteration 115/1000 | Loss: 0.00003844
Iteration 116/1000 | Loss: 0.00005998
Iteration 117/1000 | Loss: 0.00003312
Iteration 118/1000 | Loss: 0.00003292
Iteration 119/1000 | Loss: 0.00003250
Iteration 120/1000 | Loss: 0.00003245
Iteration 121/1000 | Loss: 0.00003243
Iteration 122/1000 | Loss: 0.00003242
Iteration 123/1000 | Loss: 0.00003257
Iteration 124/1000 | Loss: 0.00003264
Iteration 125/1000 | Loss: 0.00003251
Iteration 126/1000 | Loss: 0.00003233
Iteration 127/1000 | Loss: 0.00003232
Iteration 128/1000 | Loss: 0.00003362
Iteration 129/1000 | Loss: 0.00003293
Iteration 130/1000 | Loss: 0.00003500
Iteration 131/1000 | Loss: 0.00003283
Iteration 132/1000 | Loss: 0.00003282
Iteration 133/1000 | Loss: 0.00003455
Iteration 134/1000 | Loss: 0.00003515
Iteration 135/1000 | Loss: 0.00003449
Iteration 136/1000 | Loss: 0.00003595
Iteration 137/1000 | Loss: 0.00003428
Iteration 138/1000 | Loss: 0.00003599
Iteration 139/1000 | Loss: 0.00003483
Iteration 140/1000 | Loss: 0.00003631
Iteration 141/1000 | Loss: 0.00003446
Iteration 142/1000 | Loss: 0.00003576
Iteration 143/1000 | Loss: 0.00003500
Iteration 144/1000 | Loss: 0.00003556
Iteration 145/1000 | Loss: 0.00003450
Iteration 146/1000 | Loss: 0.00003654
Iteration 147/1000 | Loss: 0.00003521
Iteration 148/1000 | Loss: 0.00013323
Iteration 149/1000 | Loss: 0.00004030
Iteration 150/1000 | Loss: 0.00003546
Iteration 151/1000 | Loss: 0.00003480
Iteration 152/1000 | Loss: 0.00003294
Iteration 153/1000 | Loss: 0.00003526
Iteration 154/1000 | Loss: 0.00003287
Iteration 155/1000 | Loss: 0.00003331
Iteration 156/1000 | Loss: 0.00003207
Iteration 157/1000 | Loss: 0.00003336
Iteration 158/1000 | Loss: 0.00003369
Iteration 159/1000 | Loss: 0.00003398
Iteration 160/1000 | Loss: 0.00003322
Iteration 161/1000 | Loss: 0.00003129
Iteration 162/1000 | Loss: 0.00003072
Iteration 163/1000 | Loss: 0.00003071
Iteration 164/1000 | Loss: 0.00003070
Iteration 165/1000 | Loss: 0.00003070
Iteration 166/1000 | Loss: 0.00003069
Iteration 167/1000 | Loss: 0.00003069
Iteration 168/1000 | Loss: 0.00003068
Iteration 169/1000 | Loss: 0.00003068
Iteration 170/1000 | Loss: 0.00003067
Iteration 171/1000 | Loss: 0.00003066
Iteration 172/1000 | Loss: 0.00003066
Iteration 173/1000 | Loss: 0.00003127
Iteration 174/1000 | Loss: 0.00003214
Iteration 175/1000 | Loss: 0.00004055
Iteration 176/1000 | Loss: 0.00003630
Iteration 177/1000 | Loss: 0.00003467
Iteration 178/1000 | Loss: 0.00003183
Iteration 179/1000 | Loss: 0.00003195
Iteration 180/1000 | Loss: 0.00012896
Iteration 181/1000 | Loss: 0.00004418
Iteration 182/1000 | Loss: 0.00003694
Iteration 183/1000 | Loss: 0.00003399
Iteration 184/1000 | Loss: 0.00003172
Iteration 185/1000 | Loss: 0.00003049
Iteration 186/1000 | Loss: 0.00003020
Iteration 187/1000 | Loss: 0.00003173
Iteration 188/1000 | Loss: 0.00003024
Iteration 189/1000 | Loss: 0.00002948
Iteration 190/1000 | Loss: 0.00002949
Iteration 191/1000 | Loss: 0.00002922
Iteration 192/1000 | Loss: 0.00002922
Iteration 193/1000 | Loss: 0.00002921
Iteration 194/1000 | Loss: 0.00002921
Iteration 195/1000 | Loss: 0.00002904
Iteration 196/1000 | Loss: 0.00003285
Iteration 197/1000 | Loss: 0.00003039
Iteration 198/1000 | Loss: 0.00002945
Iteration 199/1000 | Loss: 0.00003006
Iteration 200/1000 | Loss: 0.00003232
Iteration 201/1000 | Loss: 0.00003310
Iteration 202/1000 | Loss: 0.00002946
Iteration 203/1000 | Loss: 0.00003796
Iteration 204/1000 | Loss: 0.00003265
Iteration 205/1000 | Loss: 0.00003133
Iteration 206/1000 | Loss: 0.00003051
Iteration 207/1000 | Loss: 0.00002975
Iteration 208/1000 | Loss: 0.00002988
Iteration 209/1000 | Loss: 0.00003157
Iteration 210/1000 | Loss: 0.00003154
Iteration 211/1000 | Loss: 0.00003191
Iteration 212/1000 | Loss: 0.00003125
Iteration 213/1000 | Loss: 0.00003129
Iteration 214/1000 | Loss: 0.00003077
Iteration 215/1000 | Loss: 0.00003117
Iteration 216/1000 | Loss: 0.00002939
Iteration 217/1000 | Loss: 0.00002894
Iteration 218/1000 | Loss: 0.00003005
Iteration 219/1000 | Loss: 0.00002909
Iteration 220/1000 | Loss: 0.00002908
Iteration 221/1000 | Loss: 0.00003158
Iteration 222/1000 | Loss: 0.00003137
Iteration 223/1000 | Loss: 0.00002944
Iteration 224/1000 | Loss: 0.00002884
Iteration 225/1000 | Loss: 0.00003211
Iteration 226/1000 | Loss: 0.00003427
Iteration 227/1000 | Loss: 0.00003255
Iteration 228/1000 | Loss: 0.00003834
Iteration 229/1000 | Loss: 0.00003195
Iteration 230/1000 | Loss: 0.00003570
Iteration 231/1000 | Loss: 0.00003265
Iteration 232/1000 | Loss: 0.00003779
Iteration 233/1000 | Loss: 0.00003075
Iteration 234/1000 | Loss: 0.00003325
Iteration 235/1000 | Loss: 0.00002961
Iteration 236/1000 | Loss: 0.00002967
Iteration 237/1000 | Loss: 0.00003163
Iteration 238/1000 | Loss: 0.00003065
Iteration 239/1000 | Loss: 0.00003398
Iteration 240/1000 | Loss: 0.00003212
Iteration 241/1000 | Loss: 0.00002972
Iteration 242/1000 | Loss: 0.00002971
Iteration 243/1000 | Loss: 0.00002969
Iteration 244/1000 | Loss: 0.00002954
Iteration 245/1000 | Loss: 0.00002897
Iteration 246/1000 | Loss: 0.00002887
Iteration 247/1000 | Loss: 0.00002887
Iteration 248/1000 | Loss: 0.00002887
Iteration 249/1000 | Loss: 0.00002886
Iteration 250/1000 | Loss: 0.00002886
Iteration 251/1000 | Loss: 0.00002886
Iteration 252/1000 | Loss: 0.00002886
Iteration 253/1000 | Loss: 0.00002886
Iteration 254/1000 | Loss: 0.00002886
Iteration 255/1000 | Loss: 0.00002886
Iteration 256/1000 | Loss: 0.00003010
Iteration 257/1000 | Loss: 0.00002897
Iteration 258/1000 | Loss: 0.00002897
Iteration 259/1000 | Loss: 0.00002897
Iteration 260/1000 | Loss: 0.00002897
Iteration 261/1000 | Loss: 0.00003435
Iteration 262/1000 | Loss: 0.00003015
Iteration 263/1000 | Loss: 0.00003386
Iteration 264/1000 | Loss: 0.00003014
Iteration 265/1000 | Loss: 0.00002982
Iteration 266/1000 | Loss: 0.00002897
Iteration 267/1000 | Loss: 0.00002897
Iteration 268/1000 | Loss: 0.00002926
Iteration 269/1000 | Loss: 0.00002890
Iteration 270/1000 | Loss: 0.00002889
Iteration 271/1000 | Loss: 0.00002888
Iteration 272/1000 | Loss: 0.00002886
Iteration 273/1000 | Loss: 0.00002885
Iteration 274/1000 | Loss: 0.00002884
Iteration 275/1000 | Loss: 0.00002883
Iteration 276/1000 | Loss: 0.00002882
Iteration 277/1000 | Loss: 0.00002881
Iteration 278/1000 | Loss: 0.00002881
Iteration 279/1000 | Loss: 0.00003269
Iteration 280/1000 | Loss: 0.00003045
Iteration 281/1000 | Loss: 0.00002899
Iteration 282/1000 | Loss: 0.00003266
Iteration 283/1000 | Loss: 0.00003075
Iteration 284/1000 | Loss: 0.00003002
Iteration 285/1000 | Loss: 0.00002894
Iteration 286/1000 | Loss: 0.00002880
Iteration 287/1000 | Loss: 0.00003241
Iteration 288/1000 | Loss: 0.00003075
Iteration 289/1000 | Loss: 0.00002897
Iteration 290/1000 | Loss: 0.00003217
Iteration 291/1000 | Loss: 0.00003465
Iteration 292/1000 | Loss: 0.00002985
Iteration 293/1000 | Loss: 0.00002973
Iteration 294/1000 | Loss: 0.00003309
Iteration 295/1000 | Loss: 0.00003007
Iteration 296/1000 | Loss: 0.00003178
Iteration 297/1000 | Loss: 0.00003214
Iteration 298/1000 | Loss: 0.00002956
Iteration 299/1000 | Loss: 0.00002919
Iteration 300/1000 | Loss: 0.00003039
Iteration 301/1000 | Loss: 0.00003054
Iteration 302/1000 | Loss: 0.00002916
Iteration 303/1000 | Loss: 0.00003005
Iteration 304/1000 | Loss: 0.00002928
Iteration 305/1000 | Loss: 0.00002953
Iteration 306/1000 | Loss: 0.00003025
Iteration 307/1000 | Loss: 0.00002915
Iteration 308/1000 | Loss: 0.00002976
Iteration 309/1000 | Loss: 0.00002929
Iteration 310/1000 | Loss: 0.00002900
Iteration 311/1000 | Loss: 0.00003168
Iteration 312/1000 | Loss: 0.00002954
Iteration 313/1000 | Loss: 0.00002918
Iteration 314/1000 | Loss: 0.00003095
Iteration 315/1000 | Loss: 0.00002925
Iteration 316/1000 | Loss: 0.00002983
Iteration 317/1000 | Loss: 0.00003850
Iteration 318/1000 | Loss: 0.00003491
Iteration 319/1000 | Loss: 0.00003813
Iteration 320/1000 | Loss: 0.00003468
Iteration 321/1000 | Loss: 0.00002997
Iteration 322/1000 | Loss: 0.00003176
Iteration 323/1000 | Loss: 0.00003176
Iteration 324/1000 | Loss: 0.00003351
Iteration 325/1000 | Loss: 0.00003035
Iteration 326/1000 | Loss: 0.00002955
Iteration 327/1000 | Loss: 0.00003106
Iteration 328/1000 | Loss: 0.00003062
Iteration 329/1000 | Loss: 0.00003254
Iteration 330/1000 | Loss: 0.00002966
Iteration 331/1000 | Loss: 0.00003243
Iteration 332/1000 | Loss: 0.00002944
Iteration 333/1000 | Loss: 0.00003334
Iteration 334/1000 | Loss: 0.00002986
Iteration 335/1000 | Loss: 0.00003185
Iteration 336/1000 | Loss: 0.00003017
Iteration 337/1000 | Loss: 0.00003228
Iteration 338/1000 | Loss: 0.00003057
Iteration 339/1000 | Loss: 0.00002905
Iteration 340/1000 | Loss: 0.00003187
Iteration 341/1000 | Loss: 0.00002989
Iteration 342/1000 | Loss: 0.00003379
Iteration 343/1000 | Loss: 0.00003378
Iteration 344/1000 | Loss: 0.00003133
Iteration 345/1000 | Loss: 0.00003245
Iteration 346/1000 | Loss: 0.00002943
Iteration 347/1000 | Loss: 0.00003703
Iteration 348/1000 | Loss: 0.00003195
Iteration 349/1000 | Loss: 0.00002947
Iteration 350/1000 | Loss: 0.00003401
Iteration 351/1000 | Loss: 0.00002971
Iteration 352/1000 | Loss: 0.00003167
Iteration 353/1000 | Loss: 0.00003025
Iteration 354/1000 | Loss: 0.00003244
Iteration 355/1000 | Loss: 0.00003025
Iteration 356/1000 | Loss: 0.00003121
Iteration 357/1000 | Loss: 0.00003068
Iteration 358/1000 | Loss: 0.00003086
Iteration 359/1000 | Loss: 0.00003006
Iteration 360/1000 | Loss: 0.00003327
Iteration 361/1000 | Loss: 0.00003002
Iteration 362/1000 | Loss: 0.00003138
Iteration 363/1000 | Loss: 0.00003132
Iteration 364/1000 | Loss: 0.00002979
Iteration 365/1000 | Loss: 0.00003254
Iteration 366/1000 | Loss: 0.00003166
Iteration 367/1000 | Loss: 0.00003304
Iteration 368/1000 | Loss: 0.00003135
Iteration 369/1000 | Loss: 0.00003346
Iteration 370/1000 | Loss: 0.00003160
Iteration 371/1000 | Loss: 0.00003246
Iteration 372/1000 | Loss: 0.00003134
Iteration 373/1000 | Loss: 0.00003397
Iteration 374/1000 | Loss: 0.00003124
Iteration 375/1000 | Loss: 0.00003371
Iteration 376/1000 | Loss: 0.00003319
Iteration 377/1000 | Loss: 0.00003260
Iteration 378/1000 | Loss: 0.00003294
Iteration 379/1000 | Loss: 0.00003177
Iteration 380/1000 | Loss: 0.00003305
Iteration 381/1000 | Loss: 0.00003155
Iteration 382/1000 | Loss: 0.00003244
Iteration 383/1000 | Loss: 0.00003205
Iteration 384/1000 | Loss: 0.00003216
Iteration 385/1000 | Loss: 0.00003293
Iteration 386/1000 | Loss: 0.00003087
Iteration 387/1000 | Loss: 0.00003200
Iteration 388/1000 | Loss: 0.00003202
Iteration 389/1000 | Loss: 0.00003117
Iteration 390/1000 | Loss: 0.00003323
Iteration 391/1000 | Loss: 0.00003006
Iteration 392/1000 | Loss: 0.00003189
Iteration 393/1000 | Loss: 0.00002960
Iteration 394/1000 | Loss: 0.00003067
Iteration 395/1000 | Loss: 0.00002998
Iteration 396/1000 | Loss: 0.00002911
Iteration 397/1000 | Loss: 0.00002941
Iteration 398/1000 | Loss: 0.00003097
Iteration 399/1000 | Loss: 0.00002914
Iteration 400/1000 | Loss: 0.00003060
Iteration 401/1000 | Loss: 0.00002926
Iteration 402/1000 | Loss: 0.00002926
Iteration 403/1000 | Loss: 0.00002903
Iteration 404/1000 | Loss: 0.00002903
Iteration 405/1000 | Loss: 0.00002994
Iteration 406/1000 | Loss: 0.00002903
Iteration 407/1000 | Loss: 0.00002895
Iteration 408/1000 | Loss: 0.00003214
Iteration 409/1000 | Loss: 0.00002955
Iteration 410/1000 | Loss: 0.00002955
Iteration 411/1000 | Loss: 0.00002955
Iteration 412/1000 | Loss: 0.00002955
Iteration 413/1000 | Loss: 0.00002922
Iteration 414/1000 | Loss: 0.00003050
Iteration 415/1000 | Loss: 0.00004091
Iteration 416/1000 | Loss: 0.00003080
Iteration 417/1000 | Loss: 0.00003084
Iteration 418/1000 | Loss: 0.00003047
Iteration 419/1000 | Loss: 0.00002961
Iteration 420/1000 | Loss: 0.00003151
Iteration 421/1000 | Loss: 0.00003134
Iteration 422/1000 | Loss: 0.00003097
Iteration 423/1000 | Loss: 0.00003124
Iteration 424/1000 | Loss: 0.00003082
Iteration 425/1000 | Loss: 0.00003050
Iteration 426/1000 | Loss: 0.00003008
Iteration 427/1000 | Loss: 0.00002969
Iteration 428/1000 | Loss: 0.00002956
Iteration 429/1000 | Loss: 0.00002945
Iteration 430/1000 | Loss: 0.00002924
Iteration 431/1000 | Loss: 0.00002959
Iteration 432/1000 | Loss: 0.00002917
Iteration 433/1000 | Loss: 0.00002912
Iteration 434/1000 | Loss: 0.00002897
Iteration 435/1000 | Loss: 0.00002897
Iteration 436/1000 | Loss: 0.00002984
Iteration 437/1000 | Loss: 0.00002993
Iteration 438/1000 | Loss: 0.00002940
Iteration 439/1000 | Loss: 0.00003111
Iteration 440/1000 | Loss: 0.00002951
Iteration 441/1000 | Loss: 0.00003087
Iteration 442/1000 | Loss: 0.00002956
Iteration 443/1000 | Loss: 0.00002906
Iteration 444/1000 | Loss: 0.00003664
Iteration 445/1000 | Loss: 0.00003034
Iteration 446/1000 | Loss: 0.00003196
Iteration 447/1000 | Loss: 0.00002939
Iteration 448/1000 | Loss: 0.00003281
Iteration 449/1000 | Loss: 0.00002968
Iteration 450/1000 | Loss: 0.00003034
Iteration 451/1000 | Loss: 0.00002934
Iteration 452/1000 | Loss: 0.00003087
Iteration 453/1000 | Loss: 0.00003271
Iteration 454/1000 | Loss: 0.00002910
Iteration 455/1000 | Loss: 0.00003060
Iteration 456/1000 | Loss: 0.00003052
Iteration 457/1000 | Loss: 0.00003009
Iteration 458/1000 | Loss: 0.00002902
Iteration 459/1000 | Loss: 0.00002972
Iteration 460/1000 | Loss: 0.00002962
Iteration 461/1000 | Loss: 0.00003119
Iteration 462/1000 | Loss: 0.00002981
Iteration 463/1000 | Loss: 0.00002927
Iteration 464/1000 | Loss: 0.00002948
Iteration 465/1000 | Loss: 0.00003089
Iteration 466/1000 | Loss: 0.00003627
Iteration 467/1000 | Loss: 0.00003566
Iteration 468/1000 | Loss: 0.00003581
Iteration 469/1000 | Loss: 0.00003402
Iteration 470/1000 | Loss: 0.00003804
Iteration 471/1000 | Loss: 0.00003390
Iteration 472/1000 | Loss: 0.00003534
Iteration 473/1000 | Loss: 0.00003579
Iteration 474/1000 | Loss: 0.00003514
Iteration 475/1000 | Loss: 0.00003323
Iteration 476/1000 | Loss: 0.00003994
Iteration 477/1000 | Loss: 0.00003272
Iteration 478/1000 | Loss: 0.00003100
Iteration 479/1000 | Loss: 0.00003124
Iteration 480/1000 | Loss: 0.00003031
Iteration 481/1000 | Loss: 0.00003050
Iteration 482/1000 | Loss: 0.00002997
Iteration 483/1000 | Loss: 0.00003095
Iteration 484/1000 | Loss: 0.00002998
Iteration 485/1000 | Loss: 0.00002923
Iteration 486/1000 | Loss: 0.00002887
Iteration 487/1000 | Loss: 0.00002887
Iteration 488/1000 | Loss: 0.00002887
Iteration 489/1000 | Loss: 0.00002887
Iteration 490/1000 | Loss: 0.00002887
Iteration 491/1000 | Loss: 0.00002887
Iteration 492/1000 | Loss: 0.00002887
Iteration 493/1000 | Loss: 0.00002887
Iteration 494/1000 | Loss: 0.00002887
Iteration 495/1000 | Loss: 0.00002887
Iteration 496/1000 | Loss: 0.00002887
Iteration 497/1000 | Loss: 0.00002886
Iteration 498/1000 | Loss: 0.00002886
Iteration 499/1000 | Loss: 0.00002886
Iteration 500/1000 | Loss: 0.00002885
Iteration 501/1000 | Loss: 0.00002884
Iteration 502/1000 | Loss: 0.00002884
Iteration 503/1000 | Loss: 0.00002884
Iteration 504/1000 | Loss: 0.00002883
Iteration 505/1000 | Loss: 0.00002883
Iteration 506/1000 | Loss: 0.00002883
Iteration 507/1000 | Loss: 0.00002883
Iteration 508/1000 | Loss: 0.00002883
Iteration 509/1000 | Loss: 0.00002882
Iteration 510/1000 | Loss: 0.00002990
Iteration 511/1000 | Loss: 0.00003035
Iteration 512/1000 | Loss: 0.00003031
Iteration 513/1000 | Loss: 0.00002987
Iteration 514/1000 | Loss: 0.00003631
Iteration 515/1000 | Loss: 0.00003188
Iteration 516/1000 | Loss: 0.00003008
Iteration 517/1000 | Loss: 0.00002958
Iteration 518/1000 | Loss: 0.00002955
Iteration 519/1000 | Loss: 0.00003740
Iteration 520/1000 | Loss: 0.00003335
Iteration 521/1000 | Loss: 0.00003118
Iteration 522/1000 | Loss: 0.00003266
Iteration 523/1000 | Loss: 0.00003747
Iteration 524/1000 | Loss: 0.00003327
Iteration 525/1000 | Loss: 0.00004207
Iteration 526/1000 | Loss: 0.00003302
Iteration 527/1000 | Loss: 0.00003803
Iteration 528/1000 | Loss: 0.00003306
Iteration 529/1000 | Loss: 0.00003042
Iteration 530/1000 | Loss: 0.00003144
Iteration 531/1000 | Loss: 0.00003022
Iteration 532/1000 | Loss: 0.00002926
Iteration 533/1000 | Loss: 0.00002918
Iteration 534/1000 | Loss: 0.00003004
Iteration 535/1000 | Loss: 0.00002953
Iteration 536/1000 | Loss: 0.00003116
Iteration 537/1000 | Loss: 0.00003408
Iteration 538/1000 | Loss: 0.00003207
Iteration 539/1000 | Loss: 0.00003200
Iteration 540/1000 | Loss: 0.00003096
Iteration 541/1000 | Loss: 0.00003095
Iteration 542/1000 | Loss: 0.00003175
Iteration 543/1000 | Loss: 0.00003094
Iteration 544/1000 | Loss: 0.00003139
Iteration 545/1000 | Loss: 0.00003024
Iteration 546/1000 | Loss: 0.00003054
Iteration 547/1000 | Loss: 0.00002990
Iteration 548/1000 | Loss: 0.00002910
Iteration 549/1000 | Loss: 0.00003017
Iteration 550/1000 | Loss: 0.00004326
Iteration 551/1000 | Loss: 0.00003290
Iteration 552/1000 | Loss: 0.00003039
Iteration 553/1000 | Loss: 0.00003037
Iteration 554/1000 | Loss: 0.00003085
Iteration 555/1000 | Loss: 0.00002986
Iteration 556/1000 | Loss: 0.00002894
Iteration 557/1000 | Loss: 0.00002879
Iteration 558/1000 | Loss: 0.00002879
Iteration 559/1000 | Loss: 0.00002879
Iteration 560/1000 | Loss: 0.00002879
Iteration 561/1000 | Loss: 0.00002879
Iteration 562/1000 | Loss: 0.00002879
Iteration 563/1000 | Loss: 0.00002940
Iteration 564/1000 | Loss: 0.00002940
Iteration 565/1000 | Loss: 0.00003225
Iteration 566/1000 | Loss: 0.00003223
Iteration 567/1000 | Loss: 0.00002969
Iteration 568/1000 | Loss: 0.00002925
Iteration 569/1000 | Loss: 0.00002959
Iteration 570/1000 | Loss: 0.00003871
Iteration 571/1000 | Loss: 0.00003248
Iteration 572/1000 | Loss: 0.00003448
Iteration 573/1000 | Loss: 0.00003069
Iteration 574/1000 | Loss: 0.00002963
Iteration 575/1000 | Loss: 0.00003237
Iteration 576/1000 | Loss: 0.00003154
Iteration 577/1000 | Loss: 0.00002892
Iteration 578/1000 | Loss: 0.00002876
Iteration 579/1000 | Loss: 0.00002876
Iteration 580/1000 | Loss: 0.00002925
Iteration 581/1000 | Loss: 0.00003240
Iteration 582/1000 | Loss: 0.00003012
Iteration 583/1000 | Loss: 0.00002916
Iteration 584/1000 | Loss: 0.00002974
Iteration 585/1000 | Loss: 0.00003142
Iteration 586/1000 | Loss: 0.00002936
Iteration 587/1000 | Loss: 0.00002972
Iteration 588/1000 | Loss: 0.00003126
Iteration 589/1000 | Loss: 0.00003103
Iteration 590/1000 | Loss: 0.00002923
Iteration 591/1000 | Loss: 0.00003013
Iteration 592/1000 | Loss: 0.00002922
Iteration 593/1000 | Loss: 0.00002918
Iteration 594/1000 | Loss: 0.00002944
Iteration 595/1000 | Loss: 0.00002902
Iteration 596/1000 | Loss: 0.00002925
Iteration 597/1000 | Loss: 0.00003480
Iteration 598/1000 | Loss: 0.00003111
Iteration 599/1000 | Loss: 0.00002954
Iteration 600/1000 | Loss: 0.00003007
Iteration 601/1000 | Loss: 0.00003652
Iteration 602/1000 | Loss: 0.00003180
Iteration 603/1000 | Loss: 0.00002982
Iteration 604/1000 | Loss: 0.00004594
Iteration 605/1000 | Loss: 0.00002920
Iteration 606/1000 | Loss: 0.00003016
Iteration 607/1000 | Loss: 0.00003131
Iteration 608/1000 | Loss: 0.00002898
Iteration 609/1000 | Loss: 0.00002898
Iteration 610/1000 | Loss: 0.00002898
Iteration 611/1000 | Loss: 0.00002898
Iteration 612/1000 | Loss: 0.00002984
Iteration 613/1000 | Loss: 0.00002987
Iteration 614/1000 | Loss: 0.00003088
Iteration 615/1000 | Loss: 0.00002972
Iteration 616/1000 | Loss: 0.00003112
Iteration 617/1000 | Loss: 0.00002978
Iteration 618/1000 | Loss: 0.00002965
Iteration 619/1000 | Loss: 0.00003055
Iteration 620/1000 | Loss: 0.00002940
Iteration 621/1000 | Loss: 0.00002940
Iteration 622/1000 | Loss: 0.00003763
Iteration 623/1000 | Loss: 0.00003198
Iteration 624/1000 | Loss: 0.00002955
Iteration 625/1000 | Loss: 0.00003227
Iteration 626/1000 | Loss: 0.00003028
Iteration 627/1000 | Loss: 0.00002939
Iteration 628/1000 | Loss: 0.00003216
Iteration 629/1000 | Loss: 0.00003255
Iteration 630/1000 | Loss: 0.00003113
Iteration 631/1000 | Loss: 0.00003000
Iteration 632/1000 | Loss: 0.00003000
Iteration 633/1000 | Loss: 0.00003070
Iteration 634/1000 | Loss: 0.00002986
Iteration 635/1000 | Loss: 0.00003159
Iteration 636/1000 | Loss: 0.00003116
Iteration 637/1000 | Loss: 0.00002876
Iteration 638/1000 | Loss: 0.00002876
Iteration 639/1000 | Loss: 0.00002876
Iteration 640/1000 | Loss: 0.00002876
Iteration 641/1000 | Loss: 0.00002876
Iteration 642/1000 | Loss: 0.00002876
Iteration 643/1000 | Loss: 0.00002876
Iteration 644/1000 | Loss: 0.00002876
Iteration 645/1000 | Loss: 0.00002876
Iteration 646/1000 | Loss: 0.00002876
Iteration 647/1000 | Loss: 0.00002876
Iteration 648/1000 | Loss: 0.00002876
Iteration 649/1000 | Loss: 0.00002876
Iteration 650/1000 | Loss: 0.00002876
Iteration 651/1000 | Loss: 0.00002876
Iteration 652/1000 | Loss: 0.00002875
Iteration 653/1000 | Loss: 0.00002875
Iteration 654/1000 | Loss: 0.00002875
Iteration 655/1000 | Loss: 0.00002875
Iteration 656/1000 | Loss: 0.00002875
Iteration 657/1000 | Loss: 0.00002875
Iteration 658/1000 | Loss: 0.00002875
Iteration 659/1000 | Loss: 0.00002875
Iteration 660/1000 | Loss: 0.00002875
Iteration 661/1000 | Loss: 0.00002875
Iteration 662/1000 | Loss: 0.00002875
Iteration 663/1000 | Loss: 0.00002875
Iteration 664/1000 | Loss: 0.00002875
Iteration 665/1000 | Loss: 0.00002875
Iteration 666/1000 | Loss: 0.00002875
Iteration 667/1000 | Loss: 0.00002875
Iteration 668/1000 | Loss: 0.00002875
Iteration 669/1000 | Loss: 0.00002875
Iteration 670/1000 | Loss: 0.00002875
Iteration 671/1000 | Loss: 0.00002875
Iteration 672/1000 | Loss: 0.00002875
Iteration 673/1000 | Loss: 0.00002875
Iteration 674/1000 | Loss: 0.00002875
Iteration 675/1000 | Loss: 0.00002875
Iteration 676/1000 | Loss: 0.00002875
Iteration 677/1000 | Loss: 0.00002875
Iteration 678/1000 | Loss: 0.00002875
Iteration 679/1000 | Loss: 0.00002875
Iteration 680/1000 | Loss: 0.00002875
Iteration 681/1000 | Loss: 0.00002875
Iteration 682/1000 | Loss: 0.00002875
Iteration 683/1000 | Loss: 0.00002875
Iteration 684/1000 | Loss: 0.00002875
Iteration 685/1000 | Loss: 0.00002875
Iteration 686/1000 | Loss: 0.00002875
Iteration 687/1000 | Loss: 0.00002875
Iteration 688/1000 | Loss: 0.00002875
Iteration 689/1000 | Loss: 0.00002875
Iteration 690/1000 | Loss: 0.00002875
Iteration 691/1000 | Loss: 0.00002875
Iteration 692/1000 | Loss: 0.00002875
Iteration 693/1000 | Loss: 0.00002875
Iteration 694/1000 | Loss: 0.00002875
Iteration 695/1000 | Loss: 0.00002875
Iteration 696/1000 | Loss: 0.00002875
Iteration 697/1000 | Loss: 0.00002875
Iteration 698/1000 | Loss: 0.00002875
Iteration 699/1000 | Loss: 0.00002875
Iteration 700/1000 | Loss: 0.00002875
Iteration 701/1000 | Loss: 0.00002875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 701. Stopping optimization.
Last 5 losses: [2.875334394047968e-05, 2.875334394047968e-05, 2.875334394047968e-05, 2.875334394047968e-05, 2.875334394047968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.875334394047968e-05

Optimization complete. Final v2v error: 3.7535758018493652 mm

Highest mean error: 18.14637565612793 mm for frame 17

Lowest mean error: 3.1045942306518555 mm for frame 21

Saving results

Total time: 837.1911170482635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559846
Iteration 2/25 | Loss: 0.00179375
Iteration 3/25 | Loss: 0.00160117
Iteration 4/25 | Loss: 0.00157444
Iteration 5/25 | Loss: 0.00156695
Iteration 6/25 | Loss: 0.00156610
Iteration 7/25 | Loss: 0.00156610
Iteration 8/25 | Loss: 0.00156610
Iteration 9/25 | Loss: 0.00156610
Iteration 10/25 | Loss: 0.00156610
Iteration 11/25 | Loss: 0.00156610
Iteration 12/25 | Loss: 0.00156610
Iteration 13/25 | Loss: 0.00156610
Iteration 14/25 | Loss: 0.00156610
Iteration 15/25 | Loss: 0.00156610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015660960925742984, 0.0015660960925742984, 0.0015660960925742984, 0.0015660960925742984, 0.0015660960925742984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015660960925742984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65883631
Iteration 2/25 | Loss: 0.00190530
Iteration 3/25 | Loss: 0.00190530
Iteration 4/25 | Loss: 0.00190530
Iteration 5/25 | Loss: 0.00190530
Iteration 6/25 | Loss: 0.00190530
Iteration 7/25 | Loss: 0.00190530
Iteration 8/25 | Loss: 0.00190530
Iteration 9/25 | Loss: 0.00190530
Iteration 10/25 | Loss: 0.00190530
Iteration 11/25 | Loss: 0.00190530
Iteration 12/25 | Loss: 0.00190530
Iteration 13/25 | Loss: 0.00190530
Iteration 14/25 | Loss: 0.00190530
Iteration 15/25 | Loss: 0.00190530
Iteration 16/25 | Loss: 0.00190530
Iteration 17/25 | Loss: 0.00190530
Iteration 18/25 | Loss: 0.00190530
Iteration 19/25 | Loss: 0.00190530
Iteration 20/25 | Loss: 0.00190530
Iteration 21/25 | Loss: 0.00190530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019053022842854261, 0.0019053022842854261, 0.0019053022842854261, 0.0019053022842854261, 0.0019053022842854261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019053022842854261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190530
Iteration 2/1000 | Loss: 0.00008614
Iteration 3/1000 | Loss: 0.00005904
Iteration 4/1000 | Loss: 0.00004781
Iteration 5/1000 | Loss: 0.00004375
Iteration 6/1000 | Loss: 0.00004224
Iteration 7/1000 | Loss: 0.00004138
Iteration 8/1000 | Loss: 0.00004067
Iteration 9/1000 | Loss: 0.00004011
Iteration 10/1000 | Loss: 0.00003959
Iteration 11/1000 | Loss: 0.00003932
Iteration 12/1000 | Loss: 0.00003912
Iteration 13/1000 | Loss: 0.00003871
Iteration 14/1000 | Loss: 0.00003828
Iteration 15/1000 | Loss: 0.00003805
Iteration 16/1000 | Loss: 0.00003778
Iteration 17/1000 | Loss: 0.00003757
Iteration 18/1000 | Loss: 0.00003731
Iteration 19/1000 | Loss: 0.00003706
Iteration 20/1000 | Loss: 0.00003688
Iteration 21/1000 | Loss: 0.00003674
Iteration 22/1000 | Loss: 0.00003672
Iteration 23/1000 | Loss: 0.00003672
Iteration 24/1000 | Loss: 0.00003672
Iteration 25/1000 | Loss: 0.00003672
Iteration 26/1000 | Loss: 0.00003672
Iteration 27/1000 | Loss: 0.00003672
Iteration 28/1000 | Loss: 0.00003672
Iteration 29/1000 | Loss: 0.00003672
Iteration 30/1000 | Loss: 0.00003672
Iteration 31/1000 | Loss: 0.00003672
Iteration 32/1000 | Loss: 0.00003671
Iteration 33/1000 | Loss: 0.00003671
Iteration 34/1000 | Loss: 0.00003669
Iteration 35/1000 | Loss: 0.00003669
Iteration 36/1000 | Loss: 0.00003668
Iteration 37/1000 | Loss: 0.00003668
Iteration 38/1000 | Loss: 0.00003667
Iteration 39/1000 | Loss: 0.00003667
Iteration 40/1000 | Loss: 0.00003666
Iteration 41/1000 | Loss: 0.00003662
Iteration 42/1000 | Loss: 0.00003661
Iteration 43/1000 | Loss: 0.00003659
Iteration 44/1000 | Loss: 0.00003659
Iteration 45/1000 | Loss: 0.00003659
Iteration 46/1000 | Loss: 0.00003659
Iteration 47/1000 | Loss: 0.00003659
Iteration 48/1000 | Loss: 0.00003659
Iteration 49/1000 | Loss: 0.00003658
Iteration 50/1000 | Loss: 0.00003658
Iteration 51/1000 | Loss: 0.00003658
Iteration 52/1000 | Loss: 0.00003657
Iteration 53/1000 | Loss: 0.00003656
Iteration 54/1000 | Loss: 0.00003654
Iteration 55/1000 | Loss: 0.00003654
Iteration 56/1000 | Loss: 0.00003654
Iteration 57/1000 | Loss: 0.00003654
Iteration 58/1000 | Loss: 0.00003654
Iteration 59/1000 | Loss: 0.00003654
Iteration 60/1000 | Loss: 0.00003654
Iteration 61/1000 | Loss: 0.00003654
Iteration 62/1000 | Loss: 0.00003654
Iteration 63/1000 | Loss: 0.00003654
Iteration 64/1000 | Loss: 0.00003653
Iteration 65/1000 | Loss: 0.00003652
Iteration 66/1000 | Loss: 0.00003652
Iteration 67/1000 | Loss: 0.00003650
Iteration 68/1000 | Loss: 0.00003650
Iteration 69/1000 | Loss: 0.00003649
Iteration 70/1000 | Loss: 0.00003649
Iteration 71/1000 | Loss: 0.00003649
Iteration 72/1000 | Loss: 0.00003649
Iteration 73/1000 | Loss: 0.00003649
Iteration 74/1000 | Loss: 0.00003649
Iteration 75/1000 | Loss: 0.00003648
Iteration 76/1000 | Loss: 0.00003647
Iteration 77/1000 | Loss: 0.00003647
Iteration 78/1000 | Loss: 0.00003647
Iteration 79/1000 | Loss: 0.00003646
Iteration 80/1000 | Loss: 0.00003644
Iteration 81/1000 | Loss: 0.00003643
Iteration 82/1000 | Loss: 0.00003643
Iteration 83/1000 | Loss: 0.00003643
Iteration 84/1000 | Loss: 0.00003642
Iteration 85/1000 | Loss: 0.00003642
Iteration 86/1000 | Loss: 0.00003642
Iteration 87/1000 | Loss: 0.00003642
Iteration 88/1000 | Loss: 0.00003642
Iteration 89/1000 | Loss: 0.00003642
Iteration 90/1000 | Loss: 0.00003642
Iteration 91/1000 | Loss: 0.00003642
Iteration 92/1000 | Loss: 0.00003642
Iteration 93/1000 | Loss: 0.00003642
Iteration 94/1000 | Loss: 0.00003642
Iteration 95/1000 | Loss: 0.00003642
Iteration 96/1000 | Loss: 0.00003641
Iteration 97/1000 | Loss: 0.00003641
Iteration 98/1000 | Loss: 0.00003641
Iteration 99/1000 | Loss: 0.00003641
Iteration 100/1000 | Loss: 0.00003641
Iteration 101/1000 | Loss: 0.00003641
Iteration 102/1000 | Loss: 0.00003641
Iteration 103/1000 | Loss: 0.00003641
Iteration 104/1000 | Loss: 0.00003641
Iteration 105/1000 | Loss: 0.00003640
Iteration 106/1000 | Loss: 0.00003640
Iteration 107/1000 | Loss: 0.00003640
Iteration 108/1000 | Loss: 0.00003640
Iteration 109/1000 | Loss: 0.00003640
Iteration 110/1000 | Loss: 0.00003640
Iteration 111/1000 | Loss: 0.00003640
Iteration 112/1000 | Loss: 0.00003640
Iteration 113/1000 | Loss: 0.00003640
Iteration 114/1000 | Loss: 0.00003640
Iteration 115/1000 | Loss: 0.00003640
Iteration 116/1000 | Loss: 0.00003640
Iteration 117/1000 | Loss: 0.00003640
Iteration 118/1000 | Loss: 0.00003639
Iteration 119/1000 | Loss: 0.00003639
Iteration 120/1000 | Loss: 0.00003639
Iteration 121/1000 | Loss: 0.00003639
Iteration 122/1000 | Loss: 0.00003639
Iteration 123/1000 | Loss: 0.00003639
Iteration 124/1000 | Loss: 0.00003639
Iteration 125/1000 | Loss: 0.00003639
Iteration 126/1000 | Loss: 0.00003638
Iteration 127/1000 | Loss: 0.00003638
Iteration 128/1000 | Loss: 0.00003638
Iteration 129/1000 | Loss: 0.00003638
Iteration 130/1000 | Loss: 0.00003638
Iteration 131/1000 | Loss: 0.00003638
Iteration 132/1000 | Loss: 0.00003638
Iteration 133/1000 | Loss: 0.00003638
Iteration 134/1000 | Loss: 0.00003638
Iteration 135/1000 | Loss: 0.00003638
Iteration 136/1000 | Loss: 0.00003638
Iteration 137/1000 | Loss: 0.00003638
Iteration 138/1000 | Loss: 0.00003638
Iteration 139/1000 | Loss: 0.00003638
Iteration 140/1000 | Loss: 0.00003638
Iteration 141/1000 | Loss: 0.00003638
Iteration 142/1000 | Loss: 0.00003638
Iteration 143/1000 | Loss: 0.00003638
Iteration 144/1000 | Loss: 0.00003638
Iteration 145/1000 | Loss: 0.00003638
Iteration 146/1000 | Loss: 0.00003638
Iteration 147/1000 | Loss: 0.00003637
Iteration 148/1000 | Loss: 0.00003637
Iteration 149/1000 | Loss: 0.00003637
Iteration 150/1000 | Loss: 0.00003637
Iteration 151/1000 | Loss: 0.00003637
Iteration 152/1000 | Loss: 0.00003637
Iteration 153/1000 | Loss: 0.00003637
Iteration 154/1000 | Loss: 0.00003637
Iteration 155/1000 | Loss: 0.00003637
Iteration 156/1000 | Loss: 0.00003637
Iteration 157/1000 | Loss: 0.00003637
Iteration 158/1000 | Loss: 0.00003637
Iteration 159/1000 | Loss: 0.00003637
Iteration 160/1000 | Loss: 0.00003637
Iteration 161/1000 | Loss: 0.00003637
Iteration 162/1000 | Loss: 0.00003637
Iteration 163/1000 | Loss: 0.00003637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.63746439688839e-05, 3.63746439688839e-05, 3.63746439688839e-05, 3.63746439688839e-05, 3.63746439688839e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.63746439688839e-05

Optimization complete. Final v2v error: 5.114499092102051 mm

Highest mean error: 5.265577793121338 mm for frame 266

Lowest mean error: 4.935752868652344 mm for frame 0

Saving results

Total time: 57.09014081954956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100028
Iteration 2/25 | Loss: 0.00340075
Iteration 3/25 | Loss: 0.00259160
Iteration 4/25 | Loss: 0.00249043
Iteration 5/25 | Loss: 0.00236859
Iteration 6/25 | Loss: 0.00227261
Iteration 7/25 | Loss: 0.00226725
Iteration 8/25 | Loss: 0.00223850
Iteration 9/25 | Loss: 0.00224973
Iteration 10/25 | Loss: 0.00222936
Iteration 11/25 | Loss: 0.00220008
Iteration 12/25 | Loss: 0.00218842
Iteration 13/25 | Loss: 0.00216989
Iteration 14/25 | Loss: 0.00216561
Iteration 15/25 | Loss: 0.00217325
Iteration 16/25 | Loss: 0.00216679
Iteration 17/25 | Loss: 0.00215789
Iteration 18/25 | Loss: 0.00215692
Iteration 19/25 | Loss: 0.00215620
Iteration 20/25 | Loss: 0.00215564
Iteration 21/25 | Loss: 0.00215542
Iteration 22/25 | Loss: 0.00215513
Iteration 23/25 | Loss: 0.00215473
Iteration 24/25 | Loss: 0.00215442
Iteration 25/25 | Loss: 0.00215422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15432441
Iteration 2/25 | Loss: 0.01713964
Iteration 3/25 | Loss: 0.01125402
Iteration 4/25 | Loss: 0.01125402
Iteration 5/25 | Loss: 0.01125402
Iteration 6/25 | Loss: 0.01125402
Iteration 7/25 | Loss: 0.01125402
Iteration 8/25 | Loss: 0.01125402
Iteration 9/25 | Loss: 0.01125402
Iteration 10/25 | Loss: 0.01125402
Iteration 11/25 | Loss: 0.01125402
Iteration 12/25 | Loss: 0.01125402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.011254020035266876, 0.011254020035266876, 0.011254020035266876, 0.011254020035266876, 0.011254020035266876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.011254020035266876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01125402
Iteration 2/1000 | Loss: 0.00648652
Iteration 3/1000 | Loss: 0.01045770
Iteration 4/1000 | Loss: 0.00764417
Iteration 5/1000 | Loss: 0.00286893
Iteration 6/1000 | Loss: 0.00132317
Iteration 7/1000 | Loss: 0.00298587
Iteration 8/1000 | Loss: 0.00119196
Iteration 9/1000 | Loss: 0.00158712
Iteration 10/1000 | Loss: 0.00239378
Iteration 11/1000 | Loss: 0.00093705
Iteration 12/1000 | Loss: 0.00152657
Iteration 13/1000 | Loss: 0.00093910
Iteration 14/1000 | Loss: 0.00128092
Iteration 15/1000 | Loss: 0.00182814
Iteration 16/1000 | Loss: 0.00141958
Iteration 17/1000 | Loss: 0.00057399
Iteration 18/1000 | Loss: 0.00181944
Iteration 19/1000 | Loss: 0.00140081
Iteration 20/1000 | Loss: 0.00197071
Iteration 21/1000 | Loss: 0.00186768
Iteration 22/1000 | Loss: 0.00121274
Iteration 23/1000 | Loss: 0.00041761
Iteration 24/1000 | Loss: 0.00111987
Iteration 25/1000 | Loss: 0.00100804
Iteration 26/1000 | Loss: 0.00054505
Iteration 27/1000 | Loss: 0.00045061
Iteration 28/1000 | Loss: 0.00063702
Iteration 29/1000 | Loss: 0.00103587
Iteration 30/1000 | Loss: 0.00095340
Iteration 31/1000 | Loss: 0.00152868
Iteration 32/1000 | Loss: 0.00100542
Iteration 33/1000 | Loss: 0.00061049
Iteration 34/1000 | Loss: 0.00068000
Iteration 35/1000 | Loss: 0.00086429
Iteration 36/1000 | Loss: 0.00076736
Iteration 37/1000 | Loss: 0.00047670
Iteration 38/1000 | Loss: 0.00075326
Iteration 39/1000 | Loss: 0.00094547
Iteration 40/1000 | Loss: 0.00076662
Iteration 41/1000 | Loss: 0.00059121
Iteration 42/1000 | Loss: 0.00104396
Iteration 43/1000 | Loss: 0.00146969
Iteration 44/1000 | Loss: 0.00068320
Iteration 45/1000 | Loss: 0.00039151
Iteration 46/1000 | Loss: 0.00069209
Iteration 47/1000 | Loss: 0.00049208
Iteration 48/1000 | Loss: 0.00041164
Iteration 49/1000 | Loss: 0.00042912
Iteration 50/1000 | Loss: 0.00037151
Iteration 51/1000 | Loss: 0.00038108
Iteration 52/1000 | Loss: 0.00035860
Iteration 53/1000 | Loss: 0.00058800
Iteration 54/1000 | Loss: 0.00056448
Iteration 55/1000 | Loss: 0.00051322
Iteration 56/1000 | Loss: 0.00117147
Iteration 57/1000 | Loss: 0.00261702
Iteration 58/1000 | Loss: 0.00247024
Iteration 59/1000 | Loss: 0.00093660
Iteration 60/1000 | Loss: 0.00050528
Iteration 61/1000 | Loss: 0.00154554
Iteration 62/1000 | Loss: 0.00203428
Iteration 63/1000 | Loss: 0.00231555
Iteration 64/1000 | Loss: 0.00187186
Iteration 65/1000 | Loss: 0.00129339
Iteration 66/1000 | Loss: 0.00068520
Iteration 67/1000 | Loss: 0.00043293
Iteration 68/1000 | Loss: 0.00101593
Iteration 69/1000 | Loss: 0.00126027
Iteration 70/1000 | Loss: 0.00062239
Iteration 71/1000 | Loss: 0.00075789
Iteration 72/1000 | Loss: 0.00066158
Iteration 73/1000 | Loss: 0.00094428
Iteration 74/1000 | Loss: 0.00054445
Iteration 75/1000 | Loss: 0.00046164
Iteration 76/1000 | Loss: 0.00060020
Iteration 77/1000 | Loss: 0.00150978
Iteration 78/1000 | Loss: 0.00051936
Iteration 79/1000 | Loss: 0.00036696
Iteration 80/1000 | Loss: 0.00087055
Iteration 81/1000 | Loss: 0.00115304
Iteration 82/1000 | Loss: 0.00162669
Iteration 83/1000 | Loss: 0.00085326
Iteration 84/1000 | Loss: 0.00057276
Iteration 85/1000 | Loss: 0.00081308
Iteration 86/1000 | Loss: 0.00048626
Iteration 87/1000 | Loss: 0.00101760
Iteration 88/1000 | Loss: 0.00109765
Iteration 89/1000 | Loss: 0.00048176
Iteration 90/1000 | Loss: 0.00044119
Iteration 91/1000 | Loss: 0.00016629
Iteration 92/1000 | Loss: 0.00014934
Iteration 93/1000 | Loss: 0.00078879
Iteration 94/1000 | Loss: 0.00017923
Iteration 95/1000 | Loss: 0.00015352
Iteration 96/1000 | Loss: 0.00037269
Iteration 97/1000 | Loss: 0.00065589
Iteration 98/1000 | Loss: 0.00074093
Iteration 99/1000 | Loss: 0.00026023
Iteration 100/1000 | Loss: 0.00014067
Iteration 101/1000 | Loss: 0.00054140
Iteration 102/1000 | Loss: 0.00035452
Iteration 103/1000 | Loss: 0.00040982
Iteration 104/1000 | Loss: 0.00014715
Iteration 105/1000 | Loss: 0.00012924
Iteration 106/1000 | Loss: 0.00012330
Iteration 107/1000 | Loss: 0.00044573
Iteration 108/1000 | Loss: 0.00035854
Iteration 109/1000 | Loss: 0.00033588
Iteration 110/1000 | Loss: 0.00028702
Iteration 111/1000 | Loss: 0.00057144
Iteration 112/1000 | Loss: 0.00029690
Iteration 113/1000 | Loss: 0.00011775
Iteration 114/1000 | Loss: 0.00039440
Iteration 115/1000 | Loss: 0.00102264
Iteration 116/1000 | Loss: 0.00035258
Iteration 117/1000 | Loss: 0.00025274
Iteration 118/1000 | Loss: 0.00038961
Iteration 119/1000 | Loss: 0.00061927
Iteration 120/1000 | Loss: 0.00020142
Iteration 121/1000 | Loss: 0.00012756
Iteration 122/1000 | Loss: 0.00012213
Iteration 123/1000 | Loss: 0.00024269
Iteration 124/1000 | Loss: 0.00084275
Iteration 125/1000 | Loss: 0.00177527
Iteration 126/1000 | Loss: 0.00192926
Iteration 127/1000 | Loss: 0.00109060
Iteration 128/1000 | Loss: 0.00207790
Iteration 129/1000 | Loss: 0.00078445
Iteration 130/1000 | Loss: 0.00086078
Iteration 131/1000 | Loss: 0.00051135
Iteration 132/1000 | Loss: 0.00061715
Iteration 133/1000 | Loss: 0.00055083
Iteration 134/1000 | Loss: 0.00062760
Iteration 135/1000 | Loss: 0.00045870
Iteration 136/1000 | Loss: 0.00036747
Iteration 137/1000 | Loss: 0.00061722
Iteration 138/1000 | Loss: 0.00055637
Iteration 139/1000 | Loss: 0.00047877
Iteration 140/1000 | Loss: 0.00049110
Iteration 141/1000 | Loss: 0.00050680
Iteration 142/1000 | Loss: 0.00014859
Iteration 143/1000 | Loss: 0.00032286
Iteration 144/1000 | Loss: 0.00031603
Iteration 145/1000 | Loss: 0.00034835
Iteration 146/1000 | Loss: 0.00032057
Iteration 147/1000 | Loss: 0.00057963
Iteration 148/1000 | Loss: 0.00030348
Iteration 149/1000 | Loss: 0.00022763
Iteration 150/1000 | Loss: 0.00046290
Iteration 151/1000 | Loss: 0.00023723
Iteration 152/1000 | Loss: 0.00012074
Iteration 153/1000 | Loss: 0.00086365
Iteration 154/1000 | Loss: 0.00030671
Iteration 155/1000 | Loss: 0.00061090
Iteration 156/1000 | Loss: 0.00018219
Iteration 157/1000 | Loss: 0.00036914
Iteration 158/1000 | Loss: 0.00040258
Iteration 159/1000 | Loss: 0.00039048
Iteration 160/1000 | Loss: 0.00125433
Iteration 161/1000 | Loss: 0.00045284
Iteration 162/1000 | Loss: 0.00019115
Iteration 163/1000 | Loss: 0.00025879
Iteration 164/1000 | Loss: 0.00066321
Iteration 165/1000 | Loss: 0.00024711
Iteration 166/1000 | Loss: 0.00033419
Iteration 167/1000 | Loss: 0.00036650
Iteration 168/1000 | Loss: 0.00012807
Iteration 169/1000 | Loss: 0.00048865
Iteration 170/1000 | Loss: 0.00025844
Iteration 171/1000 | Loss: 0.00032194
Iteration 172/1000 | Loss: 0.00069553
Iteration 173/1000 | Loss: 0.00037005
Iteration 174/1000 | Loss: 0.00017209
Iteration 175/1000 | Loss: 0.00102863
Iteration 176/1000 | Loss: 0.00056430
Iteration 177/1000 | Loss: 0.00030944
Iteration 178/1000 | Loss: 0.00010525
Iteration 179/1000 | Loss: 0.00029783
Iteration 180/1000 | Loss: 0.00010234
Iteration 181/1000 | Loss: 0.00058258
Iteration 182/1000 | Loss: 0.00143463
Iteration 183/1000 | Loss: 0.00024464
Iteration 184/1000 | Loss: 0.00050729
Iteration 185/1000 | Loss: 0.00231440
Iteration 186/1000 | Loss: 0.00083123
Iteration 187/1000 | Loss: 0.00091726
Iteration 188/1000 | Loss: 0.00071464
Iteration 189/1000 | Loss: 0.00016656
Iteration 190/1000 | Loss: 0.00014444
Iteration 191/1000 | Loss: 0.00014780
Iteration 192/1000 | Loss: 0.00011293
Iteration 193/1000 | Loss: 0.00018838
Iteration 194/1000 | Loss: 0.00071907
Iteration 195/1000 | Loss: 0.00045699
Iteration 196/1000 | Loss: 0.00016690
Iteration 197/1000 | Loss: 0.00015056
Iteration 198/1000 | Loss: 0.00015093
Iteration 199/1000 | Loss: 0.00015630
Iteration 200/1000 | Loss: 0.00010969
Iteration 201/1000 | Loss: 0.00014089
Iteration 202/1000 | Loss: 0.00017933
Iteration 203/1000 | Loss: 0.00074934
Iteration 204/1000 | Loss: 0.00012415
Iteration 205/1000 | Loss: 0.00010428
Iteration 206/1000 | Loss: 0.00010005
Iteration 207/1000 | Loss: 0.00059724
Iteration 208/1000 | Loss: 0.00099001
Iteration 209/1000 | Loss: 0.00058229
Iteration 210/1000 | Loss: 0.00013196
Iteration 211/1000 | Loss: 0.00011348
Iteration 212/1000 | Loss: 0.00010247
Iteration 213/1000 | Loss: 0.00133688
Iteration 214/1000 | Loss: 0.00081319
Iteration 215/1000 | Loss: 0.00040028
Iteration 216/1000 | Loss: 0.00010256
Iteration 217/1000 | Loss: 0.00009503
Iteration 218/1000 | Loss: 0.00099382
Iteration 219/1000 | Loss: 0.00068221
Iteration 220/1000 | Loss: 0.00062759
Iteration 221/1000 | Loss: 0.00047905
Iteration 222/1000 | Loss: 0.00019904
Iteration 223/1000 | Loss: 0.00018328
Iteration 224/1000 | Loss: 0.00009725
Iteration 225/1000 | Loss: 0.00013378
Iteration 226/1000 | Loss: 0.00008622
Iteration 227/1000 | Loss: 0.00008387
Iteration 228/1000 | Loss: 0.00015114
Iteration 229/1000 | Loss: 0.00013376
Iteration 230/1000 | Loss: 0.00014682
Iteration 231/1000 | Loss: 0.00008506
Iteration 232/1000 | Loss: 0.00099337
Iteration 233/1000 | Loss: 0.00021977
Iteration 234/1000 | Loss: 0.00013669
Iteration 235/1000 | Loss: 0.00008912
Iteration 236/1000 | Loss: 0.00008484
Iteration 237/1000 | Loss: 0.00008297
Iteration 238/1000 | Loss: 0.00008211
Iteration 239/1000 | Loss: 0.00107225
Iteration 240/1000 | Loss: 0.00111736
Iteration 241/1000 | Loss: 0.00064237
Iteration 242/1000 | Loss: 0.00032207
Iteration 243/1000 | Loss: 0.00076903
Iteration 244/1000 | Loss: 0.00046736
Iteration 245/1000 | Loss: 0.00018022
Iteration 246/1000 | Loss: 0.00011916
Iteration 247/1000 | Loss: 0.00029593
Iteration 248/1000 | Loss: 0.00059514
Iteration 249/1000 | Loss: 0.00045117
Iteration 250/1000 | Loss: 0.00085770
Iteration 251/1000 | Loss: 0.00048571
Iteration 252/1000 | Loss: 0.00035924
Iteration 253/1000 | Loss: 0.00045087
Iteration 254/1000 | Loss: 0.00037834
Iteration 255/1000 | Loss: 0.00009535
Iteration 256/1000 | Loss: 0.00008875
Iteration 257/1000 | Loss: 0.00022857
Iteration 258/1000 | Loss: 0.00010119
Iteration 259/1000 | Loss: 0.00010108
Iteration 260/1000 | Loss: 0.00019725
Iteration 261/1000 | Loss: 0.00044052
Iteration 262/1000 | Loss: 0.00009138
Iteration 263/1000 | Loss: 0.00008528
Iteration 264/1000 | Loss: 0.00033307
Iteration 265/1000 | Loss: 0.00030665
Iteration 266/1000 | Loss: 0.00009324
Iteration 267/1000 | Loss: 0.00007871
Iteration 268/1000 | Loss: 0.00007751
Iteration 269/1000 | Loss: 0.00007637
Iteration 270/1000 | Loss: 0.00029400
Iteration 271/1000 | Loss: 0.00078381
Iteration 272/1000 | Loss: 0.00062975
Iteration 273/1000 | Loss: 0.00020777
Iteration 274/1000 | Loss: 0.00033786
Iteration 275/1000 | Loss: 0.00007612
Iteration 276/1000 | Loss: 0.00008228
Iteration 277/1000 | Loss: 0.00007180
Iteration 278/1000 | Loss: 0.00015152
Iteration 279/1000 | Loss: 0.00007089
Iteration 280/1000 | Loss: 0.00007026
Iteration 281/1000 | Loss: 0.00007007
Iteration 282/1000 | Loss: 0.00033474
Iteration 283/1000 | Loss: 0.00038502
Iteration 284/1000 | Loss: 0.00016501
Iteration 285/1000 | Loss: 0.00017334
Iteration 286/1000 | Loss: 0.00028148
Iteration 287/1000 | Loss: 0.00010914
Iteration 288/1000 | Loss: 0.00015263
Iteration 289/1000 | Loss: 0.00007008
Iteration 290/1000 | Loss: 0.00016866
Iteration 291/1000 | Loss: 0.00007809
Iteration 292/1000 | Loss: 0.00009330
Iteration 293/1000 | Loss: 0.00006955
Iteration 294/1000 | Loss: 0.00006936
Iteration 295/1000 | Loss: 0.00034134
Iteration 296/1000 | Loss: 0.00014789
Iteration 297/1000 | Loss: 0.00006929
Iteration 298/1000 | Loss: 0.00006925
Iteration 299/1000 | Loss: 0.00006919
Iteration 300/1000 | Loss: 0.00006906
Iteration 301/1000 | Loss: 0.00006905
Iteration 302/1000 | Loss: 0.00006905
Iteration 303/1000 | Loss: 0.00006905
Iteration 304/1000 | Loss: 0.00006904
Iteration 305/1000 | Loss: 0.00033728
Iteration 306/1000 | Loss: 0.00010984
Iteration 307/1000 | Loss: 0.00008475
Iteration 308/1000 | Loss: 0.00006918
Iteration 309/1000 | Loss: 0.00007380
Iteration 310/1000 | Loss: 0.00007229
Iteration 311/1000 | Loss: 0.00007151
Iteration 312/1000 | Loss: 0.00006928
Iteration 313/1000 | Loss: 0.00006907
Iteration 314/1000 | Loss: 0.00006900
Iteration 315/1000 | Loss: 0.00006896
Iteration 316/1000 | Loss: 0.00006894
Iteration 317/1000 | Loss: 0.00060732
Iteration 318/1000 | Loss: 0.00062333
Iteration 319/1000 | Loss: 0.00017208
Iteration 320/1000 | Loss: 0.00008379
Iteration 321/1000 | Loss: 0.00007768
Iteration 322/1000 | Loss: 0.00037278
Iteration 323/1000 | Loss: 0.00010439
Iteration 324/1000 | Loss: 0.00007374
Iteration 325/1000 | Loss: 0.00007204
Iteration 326/1000 | Loss: 0.00007071
Iteration 327/1000 | Loss: 0.00006985
Iteration 328/1000 | Loss: 0.00027104
Iteration 329/1000 | Loss: 0.00035868
Iteration 330/1000 | Loss: 0.00075298
Iteration 331/1000 | Loss: 0.00031053
Iteration 332/1000 | Loss: 0.00026624
Iteration 333/1000 | Loss: 0.00035909
Iteration 334/1000 | Loss: 0.00008911
Iteration 335/1000 | Loss: 0.00025690
Iteration 336/1000 | Loss: 0.00014690
Iteration 337/1000 | Loss: 0.00007760
Iteration 338/1000 | Loss: 0.00009467
Iteration 339/1000 | Loss: 0.00009155
Iteration 340/1000 | Loss: 0.00007146
Iteration 341/1000 | Loss: 0.00007609
Iteration 342/1000 | Loss: 0.00006527
Iteration 343/1000 | Loss: 0.00007042
Iteration 344/1000 | Loss: 0.00006459
Iteration 345/1000 | Loss: 0.00006424
Iteration 346/1000 | Loss: 0.00017201
Iteration 347/1000 | Loss: 0.00038125
Iteration 348/1000 | Loss: 0.00034532
Iteration 349/1000 | Loss: 0.00007035
Iteration 350/1000 | Loss: 0.00006518
Iteration 351/1000 | Loss: 0.00006432
Iteration 352/1000 | Loss: 0.00006401
Iteration 353/1000 | Loss: 0.00006397
Iteration 354/1000 | Loss: 0.00006396
Iteration 355/1000 | Loss: 0.00006391
Iteration 356/1000 | Loss: 0.00006391
Iteration 357/1000 | Loss: 0.00006391
Iteration 358/1000 | Loss: 0.00006391
Iteration 359/1000 | Loss: 0.00006391
Iteration 360/1000 | Loss: 0.00006391
Iteration 361/1000 | Loss: 0.00006391
Iteration 362/1000 | Loss: 0.00006391
Iteration 363/1000 | Loss: 0.00006390
Iteration 364/1000 | Loss: 0.00006390
Iteration 365/1000 | Loss: 0.00006390
Iteration 366/1000 | Loss: 0.00006389
Iteration 367/1000 | Loss: 0.00006389
Iteration 368/1000 | Loss: 0.00006389
Iteration 369/1000 | Loss: 0.00006389
Iteration 370/1000 | Loss: 0.00006388
Iteration 371/1000 | Loss: 0.00006388
Iteration 372/1000 | Loss: 0.00006388
Iteration 373/1000 | Loss: 0.00006388
Iteration 374/1000 | Loss: 0.00006388
Iteration 375/1000 | Loss: 0.00006387
Iteration 376/1000 | Loss: 0.00006387
Iteration 377/1000 | Loss: 0.00006387
Iteration 378/1000 | Loss: 0.00006387
Iteration 379/1000 | Loss: 0.00006387
Iteration 380/1000 | Loss: 0.00006387
Iteration 381/1000 | Loss: 0.00006387
Iteration 382/1000 | Loss: 0.00006387
Iteration 383/1000 | Loss: 0.00006387
Iteration 384/1000 | Loss: 0.00006387
Iteration 385/1000 | Loss: 0.00006387
Iteration 386/1000 | Loss: 0.00006387
Iteration 387/1000 | Loss: 0.00006387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 387. Stopping optimization.
Last 5 losses: [6.387245957739651e-05, 6.387245957739651e-05, 6.387245957739651e-05, 6.387245957739651e-05, 6.387245957739651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.387245957739651e-05

Optimization complete. Final v2v error: 4.715531826019287 mm

Highest mean error: 12.415029525756836 mm for frame 235

Lowest mean error: 3.526287078857422 mm for frame 18

Saving results

Total time: 610.0328764915466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905901
Iteration 2/25 | Loss: 0.00159280
Iteration 3/25 | Loss: 0.00149891
Iteration 4/25 | Loss: 0.00148443
Iteration 5/25 | Loss: 0.00148066
Iteration 6/25 | Loss: 0.00147968
Iteration 7/25 | Loss: 0.00147968
Iteration 8/25 | Loss: 0.00147968
Iteration 9/25 | Loss: 0.00147968
Iteration 10/25 | Loss: 0.00147968
Iteration 11/25 | Loss: 0.00147968
Iteration 12/25 | Loss: 0.00147968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014796759933233261, 0.0014796759933233261, 0.0014796759933233261, 0.0014796759933233261, 0.0014796759933233261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014796759933233261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21715450
Iteration 2/25 | Loss: 0.00241611
Iteration 3/25 | Loss: 0.00241611
Iteration 4/25 | Loss: 0.00241611
Iteration 5/25 | Loss: 0.00241611
Iteration 6/25 | Loss: 0.00241611
Iteration 7/25 | Loss: 0.00241611
Iteration 8/25 | Loss: 0.00241611
Iteration 9/25 | Loss: 0.00241611
Iteration 10/25 | Loss: 0.00241611
Iteration 11/25 | Loss: 0.00241611
Iteration 12/25 | Loss: 0.00241611
Iteration 13/25 | Loss: 0.00241611
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0024161054752767086, 0.0024161054752767086, 0.0024161054752767086, 0.0024161054752767086, 0.0024161054752767086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024161054752767086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241611
Iteration 2/1000 | Loss: 0.00004091
Iteration 3/1000 | Loss: 0.00002902
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00002086
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001965
Iteration 9/1000 | Loss: 0.00001921
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001827
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001810
Iteration 20/1000 | Loss: 0.00001810
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001807
Iteration 23/1000 | Loss: 0.00001807
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001806
Iteration 29/1000 | Loss: 0.00001805
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001803
Iteration 33/1000 | Loss: 0.00001803
Iteration 34/1000 | Loss: 0.00001802
Iteration 35/1000 | Loss: 0.00001802
Iteration 36/1000 | Loss: 0.00001802
Iteration 37/1000 | Loss: 0.00001802
Iteration 38/1000 | Loss: 0.00001802
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001801
Iteration 44/1000 | Loss: 0.00001801
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001799
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001796
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001796
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001793
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001792
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001792
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001792
Iteration 72/1000 | Loss: 0.00001792
Iteration 73/1000 | Loss: 0.00001792
Iteration 74/1000 | Loss: 0.00001792
Iteration 75/1000 | Loss: 0.00001792
Iteration 76/1000 | Loss: 0.00001792
Iteration 77/1000 | Loss: 0.00001792
Iteration 78/1000 | Loss: 0.00001792
Iteration 79/1000 | Loss: 0.00001792
Iteration 80/1000 | Loss: 0.00001792
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001792
Iteration 84/1000 | Loss: 0.00001792
Iteration 85/1000 | Loss: 0.00001792
Iteration 86/1000 | Loss: 0.00001792
Iteration 87/1000 | Loss: 0.00001792
Iteration 88/1000 | Loss: 0.00001792
Iteration 89/1000 | Loss: 0.00001792
Iteration 90/1000 | Loss: 0.00001792
Iteration 91/1000 | Loss: 0.00001792
Iteration 92/1000 | Loss: 0.00001792
Iteration 93/1000 | Loss: 0.00001792
Iteration 94/1000 | Loss: 0.00001792
Iteration 95/1000 | Loss: 0.00001792
Iteration 96/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.792162402125541e-05, 1.792162402125541e-05, 1.792162402125541e-05, 1.792162402125541e-05, 1.792162402125541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.792162402125541e-05

Optimization complete. Final v2v error: 3.616994857788086 mm

Highest mean error: 3.981353759765625 mm for frame 92

Lowest mean error: 3.2360875606536865 mm for frame 147

Saving results

Total time: 32.199140548706055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848744
Iteration 2/25 | Loss: 0.00186852
Iteration 3/25 | Loss: 0.00167844
Iteration 4/25 | Loss: 0.00153078
Iteration 5/25 | Loss: 0.00154812
Iteration 6/25 | Loss: 0.00153618
Iteration 7/25 | Loss: 0.00152690
Iteration 8/25 | Loss: 0.00150560
Iteration 9/25 | Loss: 0.00149888
Iteration 10/25 | Loss: 0.00149877
Iteration 11/25 | Loss: 0.00149863
Iteration 12/25 | Loss: 0.00149010
Iteration 13/25 | Loss: 0.00149011
Iteration 14/25 | Loss: 0.00148964
Iteration 15/25 | Loss: 0.00148924
Iteration 16/25 | Loss: 0.00148916
Iteration 17/25 | Loss: 0.00148916
Iteration 18/25 | Loss: 0.00148916
Iteration 19/25 | Loss: 0.00148915
Iteration 20/25 | Loss: 0.00148915
Iteration 21/25 | Loss: 0.00148915
Iteration 22/25 | Loss: 0.00148915
Iteration 23/25 | Loss: 0.00148915
Iteration 24/25 | Loss: 0.00148915
Iteration 25/25 | Loss: 0.00148915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.56838465
Iteration 2/25 | Loss: 0.00274076
Iteration 3/25 | Loss: 0.00237391
Iteration 4/25 | Loss: 0.00237391
Iteration 5/25 | Loss: 0.00237391
Iteration 6/25 | Loss: 0.00237391
Iteration 7/25 | Loss: 0.00237391
Iteration 8/25 | Loss: 0.00237391
Iteration 9/25 | Loss: 0.00237391
Iteration 10/25 | Loss: 0.00237391
Iteration 11/25 | Loss: 0.00237391
Iteration 12/25 | Loss: 0.00237391
Iteration 13/25 | Loss: 0.00237391
Iteration 14/25 | Loss: 0.00237391
Iteration 15/25 | Loss: 0.00237391
Iteration 16/25 | Loss: 0.00237391
Iteration 17/25 | Loss: 0.00237391
Iteration 18/25 | Loss: 0.00237391
Iteration 19/25 | Loss: 0.00237391
Iteration 20/25 | Loss: 0.00237391
Iteration 21/25 | Loss: 0.00237391
Iteration 22/25 | Loss: 0.00237391
Iteration 23/25 | Loss: 0.00237391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0023739051539450884, 0.0023739051539450884, 0.0023739051539450884, 0.0023739051539450884, 0.0023739051539450884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023739051539450884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237391
Iteration 2/1000 | Loss: 0.00031374
Iteration 3/1000 | Loss: 0.00016343
Iteration 4/1000 | Loss: 0.00007826
Iteration 5/1000 | Loss: 0.00005777
Iteration 6/1000 | Loss: 0.00003230
Iteration 7/1000 | Loss: 0.00003038
Iteration 8/1000 | Loss: 0.00009469
Iteration 9/1000 | Loss: 0.00002823
Iteration 10/1000 | Loss: 0.00002755
Iteration 11/1000 | Loss: 0.00116939
Iteration 12/1000 | Loss: 0.00017250
Iteration 13/1000 | Loss: 0.00003377
Iteration 14/1000 | Loss: 0.00002503
Iteration 15/1000 | Loss: 0.00003093
Iteration 16/1000 | Loss: 0.00011742
Iteration 17/1000 | Loss: 0.00002171
Iteration 18/1000 | Loss: 0.00011689
Iteration 19/1000 | Loss: 0.00002607
Iteration 20/1000 | Loss: 0.00004222
Iteration 21/1000 | Loss: 0.00002053
Iteration 22/1000 | Loss: 0.00004886
Iteration 23/1000 | Loss: 0.00002029
Iteration 24/1000 | Loss: 0.00002007
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00008134
Iteration 27/1000 | Loss: 0.00002208
Iteration 28/1000 | Loss: 0.00006934
Iteration 29/1000 | Loss: 0.00001961
Iteration 30/1000 | Loss: 0.00001958
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001946
Iteration 34/1000 | Loss: 0.00001945
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001933
Iteration 37/1000 | Loss: 0.00001928
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001917
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001915
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001914
Iteration 59/1000 | Loss: 0.00001914
Iteration 60/1000 | Loss: 0.00001914
Iteration 61/1000 | Loss: 0.00001914
Iteration 62/1000 | Loss: 0.00001914
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001913
Iteration 67/1000 | Loss: 0.00001913
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001913
Iteration 71/1000 | Loss: 0.00001913
Iteration 72/1000 | Loss: 0.00001912
Iteration 73/1000 | Loss: 0.00001912
Iteration 74/1000 | Loss: 0.00001912
Iteration 75/1000 | Loss: 0.00001912
Iteration 76/1000 | Loss: 0.00001912
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001912
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001911
Iteration 82/1000 | Loss: 0.00001911
Iteration 83/1000 | Loss: 0.00001910
Iteration 84/1000 | Loss: 0.00001910
Iteration 85/1000 | Loss: 0.00001910
Iteration 86/1000 | Loss: 0.00001910
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001910
Iteration 90/1000 | Loss: 0.00001910
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001909
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001908
Iteration 104/1000 | Loss: 0.00001908
Iteration 105/1000 | Loss: 0.00001908
Iteration 106/1000 | Loss: 0.00001908
Iteration 107/1000 | Loss: 0.00001908
Iteration 108/1000 | Loss: 0.00001908
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001908
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001908
Iteration 118/1000 | Loss: 0.00001908
Iteration 119/1000 | Loss: 0.00001908
Iteration 120/1000 | Loss: 0.00001908
Iteration 121/1000 | Loss: 0.00001908
Iteration 122/1000 | Loss: 0.00001908
Iteration 123/1000 | Loss: 0.00001908
Iteration 124/1000 | Loss: 0.00001908
Iteration 125/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.907759542518761e-05, 1.907759542518761e-05, 1.907759542518761e-05, 1.907759542518761e-05, 1.907759542518761e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.907759542518761e-05

Optimization complete. Final v2v error: 3.735121488571167 mm

Highest mean error: 10.399520874023438 mm for frame 39

Lowest mean error: 3.3287813663482666 mm for frame 27

Saving results

Total time: 88.60856127738953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534156
Iteration 2/25 | Loss: 0.00183980
Iteration 3/25 | Loss: 0.00170577
Iteration 4/25 | Loss: 0.00167981
Iteration 5/25 | Loss: 0.00167171
Iteration 6/25 | Loss: 0.00166867
Iteration 7/25 | Loss: 0.00166810
Iteration 8/25 | Loss: 0.00166712
Iteration 9/25 | Loss: 0.00166497
Iteration 10/25 | Loss: 0.00166410
Iteration 11/25 | Loss: 0.00166367
Iteration 12/25 | Loss: 0.00166358
Iteration 13/25 | Loss: 0.00166358
Iteration 14/25 | Loss: 0.00166357
Iteration 15/25 | Loss: 0.00166357
Iteration 16/25 | Loss: 0.00166357
Iteration 17/25 | Loss: 0.00166357
Iteration 18/25 | Loss: 0.00166357
Iteration 19/25 | Loss: 0.00166357
Iteration 20/25 | Loss: 0.00166357
Iteration 21/25 | Loss: 0.00166357
Iteration 22/25 | Loss: 0.00166356
Iteration 23/25 | Loss: 0.00166356
Iteration 24/25 | Loss: 0.00166356
Iteration 25/25 | Loss: 0.00166356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16218686
Iteration 2/25 | Loss: 0.00413478
Iteration 3/25 | Loss: 0.00413478
Iteration 4/25 | Loss: 0.00413478
Iteration 5/25 | Loss: 0.00413478
Iteration 6/25 | Loss: 0.00413478
Iteration 7/25 | Loss: 0.00413478
Iteration 8/25 | Loss: 0.00413478
Iteration 9/25 | Loss: 0.00413478
Iteration 10/25 | Loss: 0.00413478
Iteration 11/25 | Loss: 0.00413478
Iteration 12/25 | Loss: 0.00413478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.004134779330343008, 0.004134779330343008, 0.004134779330343008, 0.004134779330343008, 0.004134779330343008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004134779330343008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00413478
Iteration 2/1000 | Loss: 0.00030068
Iteration 3/1000 | Loss: 0.00020766
Iteration 4/1000 | Loss: 0.00017403
Iteration 5/1000 | Loss: 0.00015865
Iteration 6/1000 | Loss: 0.00014682
Iteration 7/1000 | Loss: 0.00013967
Iteration 8/1000 | Loss: 0.00077061
Iteration 9/1000 | Loss: 0.00033158
Iteration 10/1000 | Loss: 0.00054146
Iteration 11/1000 | Loss: 0.00012952
Iteration 12/1000 | Loss: 0.00094242
Iteration 13/1000 | Loss: 0.00037860
Iteration 14/1000 | Loss: 0.00037115
Iteration 15/1000 | Loss: 0.00012869
Iteration 16/1000 | Loss: 0.00039129
Iteration 17/1000 | Loss: 0.00455696
Iteration 18/1000 | Loss: 0.00024096
Iteration 19/1000 | Loss: 0.00016354
Iteration 20/1000 | Loss: 0.00010011
Iteration 21/1000 | Loss: 0.00008907
Iteration 22/1000 | Loss: 0.00052636
Iteration 23/1000 | Loss: 0.00008090
Iteration 24/1000 | Loss: 0.00007351
Iteration 25/1000 | Loss: 0.00006993
Iteration 26/1000 | Loss: 0.00059458
Iteration 27/1000 | Loss: 0.00007009
Iteration 28/1000 | Loss: 0.00006316
Iteration 29/1000 | Loss: 0.00006159
Iteration 30/1000 | Loss: 0.00005976
Iteration 31/1000 | Loss: 0.00005813
Iteration 32/1000 | Loss: 0.00005685
Iteration 33/1000 | Loss: 0.00053952
Iteration 34/1000 | Loss: 0.00063391
Iteration 35/1000 | Loss: 0.00032960
Iteration 36/1000 | Loss: 0.00022479
Iteration 37/1000 | Loss: 0.00007144
Iteration 38/1000 | Loss: 0.00021608
Iteration 39/1000 | Loss: 0.00008309
Iteration 40/1000 | Loss: 0.00072755
Iteration 41/1000 | Loss: 0.00026224
Iteration 42/1000 | Loss: 0.00007031
Iteration 43/1000 | Loss: 0.00068994
Iteration 44/1000 | Loss: 0.00008485
Iteration 45/1000 | Loss: 0.00006767
Iteration 46/1000 | Loss: 0.00005275
Iteration 47/1000 | Loss: 0.00005116
Iteration 48/1000 | Loss: 0.00004990
Iteration 49/1000 | Loss: 0.00004884
Iteration 50/1000 | Loss: 0.00004796
Iteration 51/1000 | Loss: 0.00004726
Iteration 52/1000 | Loss: 0.00004690
Iteration 53/1000 | Loss: 0.00004661
Iteration 54/1000 | Loss: 0.00004636
Iteration 55/1000 | Loss: 0.00004629
Iteration 56/1000 | Loss: 0.00004618
Iteration 57/1000 | Loss: 0.00004615
Iteration 58/1000 | Loss: 0.00004606
Iteration 59/1000 | Loss: 0.00004605
Iteration 60/1000 | Loss: 0.00004603
Iteration 61/1000 | Loss: 0.00004602
Iteration 62/1000 | Loss: 0.00004596
Iteration 63/1000 | Loss: 0.00004591
Iteration 64/1000 | Loss: 0.00004590
Iteration 65/1000 | Loss: 0.00004590
Iteration 66/1000 | Loss: 0.00004583
Iteration 67/1000 | Loss: 0.00004582
Iteration 68/1000 | Loss: 0.00004582
Iteration 69/1000 | Loss: 0.00004581
Iteration 70/1000 | Loss: 0.00004579
Iteration 71/1000 | Loss: 0.00004579
Iteration 72/1000 | Loss: 0.00004579
Iteration 73/1000 | Loss: 0.00004579
Iteration 74/1000 | Loss: 0.00004579
Iteration 75/1000 | Loss: 0.00004579
Iteration 76/1000 | Loss: 0.00004578
Iteration 77/1000 | Loss: 0.00004578
Iteration 78/1000 | Loss: 0.00004577
Iteration 79/1000 | Loss: 0.00004577
Iteration 80/1000 | Loss: 0.00004577
Iteration 81/1000 | Loss: 0.00004577
Iteration 82/1000 | Loss: 0.00004577
Iteration 83/1000 | Loss: 0.00004577
Iteration 84/1000 | Loss: 0.00004577
Iteration 85/1000 | Loss: 0.00004577
Iteration 86/1000 | Loss: 0.00004577
Iteration 87/1000 | Loss: 0.00004577
Iteration 88/1000 | Loss: 0.00004576
Iteration 89/1000 | Loss: 0.00004576
Iteration 90/1000 | Loss: 0.00004576
Iteration 91/1000 | Loss: 0.00004576
Iteration 92/1000 | Loss: 0.00004576
Iteration 93/1000 | Loss: 0.00004575
Iteration 94/1000 | Loss: 0.00004575
Iteration 95/1000 | Loss: 0.00004575
Iteration 96/1000 | Loss: 0.00004574
Iteration 97/1000 | Loss: 0.00004574
Iteration 98/1000 | Loss: 0.00004574
Iteration 99/1000 | Loss: 0.00004574
Iteration 100/1000 | Loss: 0.00004574
Iteration 101/1000 | Loss: 0.00004574
Iteration 102/1000 | Loss: 0.00004573
Iteration 103/1000 | Loss: 0.00004573
Iteration 104/1000 | Loss: 0.00004573
Iteration 105/1000 | Loss: 0.00004573
Iteration 106/1000 | Loss: 0.00004573
Iteration 107/1000 | Loss: 0.00004573
Iteration 108/1000 | Loss: 0.00004572
Iteration 109/1000 | Loss: 0.00004572
Iteration 110/1000 | Loss: 0.00004572
Iteration 111/1000 | Loss: 0.00004572
Iteration 112/1000 | Loss: 0.00004571
Iteration 113/1000 | Loss: 0.00004571
Iteration 114/1000 | Loss: 0.00004571
Iteration 115/1000 | Loss: 0.00004571
Iteration 116/1000 | Loss: 0.00004571
Iteration 117/1000 | Loss: 0.00004571
Iteration 118/1000 | Loss: 0.00004571
Iteration 119/1000 | Loss: 0.00004570
Iteration 120/1000 | Loss: 0.00004570
Iteration 121/1000 | Loss: 0.00004570
Iteration 122/1000 | Loss: 0.00004570
Iteration 123/1000 | Loss: 0.00004570
Iteration 124/1000 | Loss: 0.00004570
Iteration 125/1000 | Loss: 0.00004570
Iteration 126/1000 | Loss: 0.00004570
Iteration 127/1000 | Loss: 0.00004570
Iteration 128/1000 | Loss: 0.00004569
Iteration 129/1000 | Loss: 0.00004569
Iteration 130/1000 | Loss: 0.00004569
Iteration 131/1000 | Loss: 0.00004569
Iteration 132/1000 | Loss: 0.00004569
Iteration 133/1000 | Loss: 0.00004569
Iteration 134/1000 | Loss: 0.00004569
Iteration 135/1000 | Loss: 0.00004569
Iteration 136/1000 | Loss: 0.00004569
Iteration 137/1000 | Loss: 0.00004569
Iteration 138/1000 | Loss: 0.00004569
Iteration 139/1000 | Loss: 0.00004568
Iteration 140/1000 | Loss: 0.00004568
Iteration 141/1000 | Loss: 0.00004568
Iteration 142/1000 | Loss: 0.00004568
Iteration 143/1000 | Loss: 0.00004568
Iteration 144/1000 | Loss: 0.00004568
Iteration 145/1000 | Loss: 0.00004568
Iteration 146/1000 | Loss: 0.00004568
Iteration 147/1000 | Loss: 0.00004568
Iteration 148/1000 | Loss: 0.00004568
Iteration 149/1000 | Loss: 0.00004568
Iteration 150/1000 | Loss: 0.00004567
Iteration 151/1000 | Loss: 0.00004567
Iteration 152/1000 | Loss: 0.00004567
Iteration 153/1000 | Loss: 0.00004567
Iteration 154/1000 | Loss: 0.00004567
Iteration 155/1000 | Loss: 0.00004567
Iteration 156/1000 | Loss: 0.00004567
Iteration 157/1000 | Loss: 0.00004567
Iteration 158/1000 | Loss: 0.00004567
Iteration 159/1000 | Loss: 0.00004566
Iteration 160/1000 | Loss: 0.00004566
Iteration 161/1000 | Loss: 0.00004566
Iteration 162/1000 | Loss: 0.00004566
Iteration 163/1000 | Loss: 0.00004566
Iteration 164/1000 | Loss: 0.00004566
Iteration 165/1000 | Loss: 0.00004566
Iteration 166/1000 | Loss: 0.00004566
Iteration 167/1000 | Loss: 0.00004566
Iteration 168/1000 | Loss: 0.00004566
Iteration 169/1000 | Loss: 0.00004566
Iteration 170/1000 | Loss: 0.00004566
Iteration 171/1000 | Loss: 0.00004566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [4.566409916151315e-05, 4.566409916151315e-05, 4.566409916151315e-05, 4.566409916151315e-05, 4.566409916151315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.566409916151315e-05

Optimization complete. Final v2v error: 4.763727188110352 mm

Highest mean error: 13.379155158996582 mm for frame 57

Lowest mean error: 3.929541826248169 mm for frame 91

Saving results

Total time: 113.40718746185303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540887
Iteration 2/25 | Loss: 0.00185823
Iteration 3/25 | Loss: 0.00161696
Iteration 4/25 | Loss: 0.00159205
Iteration 5/25 | Loss: 0.00158528
Iteration 6/25 | Loss: 0.00158385
Iteration 7/25 | Loss: 0.00158385
Iteration 8/25 | Loss: 0.00158385
Iteration 9/25 | Loss: 0.00158385
Iteration 10/25 | Loss: 0.00158385
Iteration 11/25 | Loss: 0.00158385
Iteration 12/25 | Loss: 0.00158385
Iteration 13/25 | Loss: 0.00158385
Iteration 14/25 | Loss: 0.00158385
Iteration 15/25 | Loss: 0.00158385
Iteration 16/25 | Loss: 0.00158385
Iteration 17/25 | Loss: 0.00158385
Iteration 18/25 | Loss: 0.00158385
Iteration 19/25 | Loss: 0.00158385
Iteration 20/25 | Loss: 0.00158385
Iteration 21/25 | Loss: 0.00158385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015838543185964227, 0.0015838543185964227, 0.0015838543185964227, 0.0015838543185964227, 0.0015838543185964227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015838543185964227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18209267
Iteration 2/25 | Loss: 0.00222236
Iteration 3/25 | Loss: 0.00222236
Iteration 4/25 | Loss: 0.00222236
Iteration 5/25 | Loss: 0.00222236
Iteration 6/25 | Loss: 0.00222236
Iteration 7/25 | Loss: 0.00222236
Iteration 8/25 | Loss: 0.00222236
Iteration 9/25 | Loss: 0.00222236
Iteration 10/25 | Loss: 0.00222236
Iteration 11/25 | Loss: 0.00222236
Iteration 12/25 | Loss: 0.00222236
Iteration 13/25 | Loss: 0.00222236
Iteration 14/25 | Loss: 0.00222236
Iteration 15/25 | Loss: 0.00222236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002222357550635934, 0.002222357550635934, 0.002222357550635934, 0.002222357550635934, 0.002222357550635934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002222357550635934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222236
Iteration 2/1000 | Loss: 0.00009672
Iteration 3/1000 | Loss: 0.00006614
Iteration 4/1000 | Loss: 0.00005498
Iteration 5/1000 | Loss: 0.00005052
Iteration 6/1000 | Loss: 0.00004857
Iteration 7/1000 | Loss: 0.00004726
Iteration 8/1000 | Loss: 0.00004653
Iteration 9/1000 | Loss: 0.00004597
Iteration 10/1000 | Loss: 0.00004556
Iteration 11/1000 | Loss: 0.00004529
Iteration 12/1000 | Loss: 0.00004505
Iteration 13/1000 | Loss: 0.00004489
Iteration 14/1000 | Loss: 0.00004480
Iteration 15/1000 | Loss: 0.00004474
Iteration 16/1000 | Loss: 0.00004461
Iteration 17/1000 | Loss: 0.00004457
Iteration 18/1000 | Loss: 0.00004452
Iteration 19/1000 | Loss: 0.00004449
Iteration 20/1000 | Loss: 0.00004448
Iteration 21/1000 | Loss: 0.00004448
Iteration 22/1000 | Loss: 0.00004448
Iteration 23/1000 | Loss: 0.00004448
Iteration 24/1000 | Loss: 0.00004448
Iteration 25/1000 | Loss: 0.00004445
Iteration 26/1000 | Loss: 0.00004445
Iteration 27/1000 | Loss: 0.00004445
Iteration 28/1000 | Loss: 0.00004445
Iteration 29/1000 | Loss: 0.00004445
Iteration 30/1000 | Loss: 0.00004444
Iteration 31/1000 | Loss: 0.00004444
Iteration 32/1000 | Loss: 0.00004444
Iteration 33/1000 | Loss: 0.00004442
Iteration 34/1000 | Loss: 0.00004441
Iteration 35/1000 | Loss: 0.00004439
Iteration 36/1000 | Loss: 0.00004439
Iteration 37/1000 | Loss: 0.00004439
Iteration 38/1000 | Loss: 0.00004439
Iteration 39/1000 | Loss: 0.00004439
Iteration 40/1000 | Loss: 0.00004439
Iteration 41/1000 | Loss: 0.00004439
Iteration 42/1000 | Loss: 0.00004438
Iteration 43/1000 | Loss: 0.00004438
Iteration 44/1000 | Loss: 0.00004438
Iteration 45/1000 | Loss: 0.00004437
Iteration 46/1000 | Loss: 0.00004437
Iteration 47/1000 | Loss: 0.00004437
Iteration 48/1000 | Loss: 0.00004436
Iteration 49/1000 | Loss: 0.00004436
Iteration 50/1000 | Loss: 0.00004436
Iteration 51/1000 | Loss: 0.00004435
Iteration 52/1000 | Loss: 0.00004435
Iteration 53/1000 | Loss: 0.00004435
Iteration 54/1000 | Loss: 0.00004434
Iteration 55/1000 | Loss: 0.00004434
Iteration 56/1000 | Loss: 0.00004434
Iteration 57/1000 | Loss: 0.00004434
Iteration 58/1000 | Loss: 0.00004433
Iteration 59/1000 | Loss: 0.00004433
Iteration 60/1000 | Loss: 0.00004433
Iteration 61/1000 | Loss: 0.00004433
Iteration 62/1000 | Loss: 0.00004433
Iteration 63/1000 | Loss: 0.00004433
Iteration 64/1000 | Loss: 0.00004433
Iteration 65/1000 | Loss: 0.00004433
Iteration 66/1000 | Loss: 0.00004432
Iteration 67/1000 | Loss: 0.00004432
Iteration 68/1000 | Loss: 0.00004432
Iteration 69/1000 | Loss: 0.00004432
Iteration 70/1000 | Loss: 0.00004432
Iteration 71/1000 | Loss: 0.00004432
Iteration 72/1000 | Loss: 0.00004432
Iteration 73/1000 | Loss: 0.00004431
Iteration 74/1000 | Loss: 0.00004431
Iteration 75/1000 | Loss: 0.00004431
Iteration 76/1000 | Loss: 0.00004431
Iteration 77/1000 | Loss: 0.00004431
Iteration 78/1000 | Loss: 0.00004431
Iteration 79/1000 | Loss: 0.00004431
Iteration 80/1000 | Loss: 0.00004431
Iteration 81/1000 | Loss: 0.00004431
Iteration 82/1000 | Loss: 0.00004431
Iteration 83/1000 | Loss: 0.00004429
Iteration 84/1000 | Loss: 0.00004429
Iteration 85/1000 | Loss: 0.00004429
Iteration 86/1000 | Loss: 0.00004429
Iteration 87/1000 | Loss: 0.00004429
Iteration 88/1000 | Loss: 0.00004429
Iteration 89/1000 | Loss: 0.00004429
Iteration 90/1000 | Loss: 0.00004428
Iteration 91/1000 | Loss: 0.00004428
Iteration 92/1000 | Loss: 0.00004427
Iteration 93/1000 | Loss: 0.00004427
Iteration 94/1000 | Loss: 0.00004427
Iteration 95/1000 | Loss: 0.00004427
Iteration 96/1000 | Loss: 0.00004427
Iteration 97/1000 | Loss: 0.00004427
Iteration 98/1000 | Loss: 0.00004427
Iteration 99/1000 | Loss: 0.00004426
Iteration 100/1000 | Loss: 0.00004426
Iteration 101/1000 | Loss: 0.00004426
Iteration 102/1000 | Loss: 0.00004426
Iteration 103/1000 | Loss: 0.00004426
Iteration 104/1000 | Loss: 0.00004426
Iteration 105/1000 | Loss: 0.00004426
Iteration 106/1000 | Loss: 0.00004426
Iteration 107/1000 | Loss: 0.00004426
Iteration 108/1000 | Loss: 0.00004426
Iteration 109/1000 | Loss: 0.00004426
Iteration 110/1000 | Loss: 0.00004425
Iteration 111/1000 | Loss: 0.00004425
Iteration 112/1000 | Loss: 0.00004425
Iteration 113/1000 | Loss: 0.00004425
Iteration 114/1000 | Loss: 0.00004425
Iteration 115/1000 | Loss: 0.00004424
Iteration 116/1000 | Loss: 0.00004424
Iteration 117/1000 | Loss: 0.00004424
Iteration 118/1000 | Loss: 0.00004424
Iteration 119/1000 | Loss: 0.00004424
Iteration 120/1000 | Loss: 0.00004424
Iteration 121/1000 | Loss: 0.00004424
Iteration 122/1000 | Loss: 0.00004424
Iteration 123/1000 | Loss: 0.00004424
Iteration 124/1000 | Loss: 0.00004424
Iteration 125/1000 | Loss: 0.00004424
Iteration 126/1000 | Loss: 0.00004424
Iteration 127/1000 | Loss: 0.00004424
Iteration 128/1000 | Loss: 0.00004423
Iteration 129/1000 | Loss: 0.00004423
Iteration 130/1000 | Loss: 0.00004423
Iteration 131/1000 | Loss: 0.00004422
Iteration 132/1000 | Loss: 0.00004422
Iteration 133/1000 | Loss: 0.00004422
Iteration 134/1000 | Loss: 0.00004422
Iteration 135/1000 | Loss: 0.00004421
Iteration 136/1000 | Loss: 0.00004421
Iteration 137/1000 | Loss: 0.00004421
Iteration 138/1000 | Loss: 0.00004421
Iteration 139/1000 | Loss: 0.00004421
Iteration 140/1000 | Loss: 0.00004420
Iteration 141/1000 | Loss: 0.00004420
Iteration 142/1000 | Loss: 0.00004420
Iteration 143/1000 | Loss: 0.00004420
Iteration 144/1000 | Loss: 0.00004420
Iteration 145/1000 | Loss: 0.00004420
Iteration 146/1000 | Loss: 0.00004420
Iteration 147/1000 | Loss: 0.00004420
Iteration 148/1000 | Loss: 0.00004420
Iteration 149/1000 | Loss: 0.00004419
Iteration 150/1000 | Loss: 0.00004419
Iteration 151/1000 | Loss: 0.00004419
Iteration 152/1000 | Loss: 0.00004419
Iteration 153/1000 | Loss: 0.00004419
Iteration 154/1000 | Loss: 0.00004419
Iteration 155/1000 | Loss: 0.00004419
Iteration 156/1000 | Loss: 0.00004419
Iteration 157/1000 | Loss: 0.00004419
Iteration 158/1000 | Loss: 0.00004418
Iteration 159/1000 | Loss: 0.00004418
Iteration 160/1000 | Loss: 0.00004418
Iteration 161/1000 | Loss: 0.00004418
Iteration 162/1000 | Loss: 0.00004418
Iteration 163/1000 | Loss: 0.00004418
Iteration 164/1000 | Loss: 0.00004418
Iteration 165/1000 | Loss: 0.00004418
Iteration 166/1000 | Loss: 0.00004418
Iteration 167/1000 | Loss: 0.00004417
Iteration 168/1000 | Loss: 0.00004417
Iteration 169/1000 | Loss: 0.00004417
Iteration 170/1000 | Loss: 0.00004417
Iteration 171/1000 | Loss: 0.00004417
Iteration 172/1000 | Loss: 0.00004417
Iteration 173/1000 | Loss: 0.00004417
Iteration 174/1000 | Loss: 0.00004417
Iteration 175/1000 | Loss: 0.00004417
Iteration 176/1000 | Loss: 0.00004417
Iteration 177/1000 | Loss: 0.00004417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [4.4168777094455436e-05, 4.4168777094455436e-05, 4.4168777094455436e-05, 4.4168777094455436e-05, 4.4168777094455436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4168777094455436e-05

Optimization complete. Final v2v error: 5.2288594245910645 mm

Highest mean error: 6.033557891845703 mm for frame 173

Lowest mean error: 4.699281692504883 mm for frame 81

Saving results

Total time: 49.92364573478699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795599
Iteration 2/25 | Loss: 0.00181253
Iteration 3/25 | Loss: 0.00158334
Iteration 4/25 | Loss: 0.00155553
Iteration 5/25 | Loss: 0.00153831
Iteration 6/25 | Loss: 0.00153532
Iteration 7/25 | Loss: 0.00153454
Iteration 8/25 | Loss: 0.00153406
Iteration 9/25 | Loss: 0.00153372
Iteration 10/25 | Loss: 0.00153347
Iteration 11/25 | Loss: 0.00153328
Iteration 12/25 | Loss: 0.00153319
Iteration 13/25 | Loss: 0.00153319
Iteration 14/25 | Loss: 0.00153319
Iteration 15/25 | Loss: 0.00153319
Iteration 16/25 | Loss: 0.00153319
Iteration 17/25 | Loss: 0.00153319
Iteration 18/25 | Loss: 0.00153318
Iteration 19/25 | Loss: 0.00153318
Iteration 20/25 | Loss: 0.00153318
Iteration 21/25 | Loss: 0.00153318
Iteration 22/25 | Loss: 0.00153318
Iteration 23/25 | Loss: 0.00153318
Iteration 24/25 | Loss: 0.00153318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015331838512793183, 0.0015331838512793183, 0.0015331838512793183, 0.0015331838512793183, 0.0015331838512793183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015331838512793183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16555047
Iteration 2/25 | Loss: 0.00228883
Iteration 3/25 | Loss: 0.00228882
Iteration 4/25 | Loss: 0.00228882
Iteration 5/25 | Loss: 0.00228882
Iteration 6/25 | Loss: 0.00228882
Iteration 7/25 | Loss: 0.00228882
Iteration 8/25 | Loss: 0.00228882
Iteration 9/25 | Loss: 0.00228882
Iteration 10/25 | Loss: 0.00228882
Iteration 11/25 | Loss: 0.00228882
Iteration 12/25 | Loss: 0.00228882
Iteration 13/25 | Loss: 0.00228882
Iteration 14/25 | Loss: 0.00228882
Iteration 15/25 | Loss: 0.00228882
Iteration 16/25 | Loss: 0.00228882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022888181265443563, 0.0022888181265443563, 0.0022888181265443563, 0.0022888181265443563, 0.0022888181265443563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022888181265443563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228882
Iteration 2/1000 | Loss: 0.00006018
Iteration 3/1000 | Loss: 0.00003660
Iteration 4/1000 | Loss: 0.00003113
Iteration 5/1000 | Loss: 0.00002944
Iteration 6/1000 | Loss: 0.00002825
Iteration 7/1000 | Loss: 0.00002746
Iteration 8/1000 | Loss: 0.00002653
Iteration 9/1000 | Loss: 0.00002602
Iteration 10/1000 | Loss: 0.00002571
Iteration 11/1000 | Loss: 0.00002551
Iteration 12/1000 | Loss: 0.00002550
Iteration 13/1000 | Loss: 0.00002550
Iteration 14/1000 | Loss: 0.00002548
Iteration 15/1000 | Loss: 0.00002543
Iteration 16/1000 | Loss: 0.00002543
Iteration 17/1000 | Loss: 0.00002543
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002542
Iteration 20/1000 | Loss: 0.00002542
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002542
Iteration 23/1000 | Loss: 0.00002542
Iteration 24/1000 | Loss: 0.00002542
Iteration 25/1000 | Loss: 0.00002542
Iteration 26/1000 | Loss: 0.00002539
Iteration 27/1000 | Loss: 0.00002539
Iteration 28/1000 | Loss: 0.00002538
Iteration 29/1000 | Loss: 0.00002538
Iteration 30/1000 | Loss: 0.00002538
Iteration 31/1000 | Loss: 0.00002537
Iteration 32/1000 | Loss: 0.00002536
Iteration 33/1000 | Loss: 0.00002536
Iteration 34/1000 | Loss: 0.00002536
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002533
Iteration 40/1000 | Loss: 0.00002533
Iteration 41/1000 | Loss: 0.00002533
Iteration 42/1000 | Loss: 0.00002533
Iteration 43/1000 | Loss: 0.00002533
Iteration 44/1000 | Loss: 0.00002532
Iteration 45/1000 | Loss: 0.00002532
Iteration 46/1000 | Loss: 0.00002532
Iteration 47/1000 | Loss: 0.00002531
Iteration 48/1000 | Loss: 0.00002531
Iteration 49/1000 | Loss: 0.00002531
Iteration 50/1000 | Loss: 0.00002531
Iteration 51/1000 | Loss: 0.00002531
Iteration 52/1000 | Loss: 0.00002531
Iteration 53/1000 | Loss: 0.00002531
Iteration 54/1000 | Loss: 0.00002531
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002531
Iteration 57/1000 | Loss: 0.00002531
Iteration 58/1000 | Loss: 0.00002531
Iteration 59/1000 | Loss: 0.00002531
Iteration 60/1000 | Loss: 0.00002530
Iteration 61/1000 | Loss: 0.00002530
Iteration 62/1000 | Loss: 0.00002530
Iteration 63/1000 | Loss: 0.00002530
Iteration 64/1000 | Loss: 0.00002530
Iteration 65/1000 | Loss: 0.00002530
Iteration 66/1000 | Loss: 0.00002529
Iteration 67/1000 | Loss: 0.00002529
Iteration 68/1000 | Loss: 0.00002529
Iteration 69/1000 | Loss: 0.00002529
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002528
Iteration 73/1000 | Loss: 0.00002528
Iteration 74/1000 | Loss: 0.00002528
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00002527
Iteration 77/1000 | Loss: 0.00002527
Iteration 78/1000 | Loss: 0.00002527
Iteration 79/1000 | Loss: 0.00002527
Iteration 80/1000 | Loss: 0.00002527
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002527
Iteration 84/1000 | Loss: 0.00002526
Iteration 85/1000 | Loss: 0.00002526
Iteration 86/1000 | Loss: 0.00002526
Iteration 87/1000 | Loss: 0.00002526
Iteration 88/1000 | Loss: 0.00002526
Iteration 89/1000 | Loss: 0.00002526
Iteration 90/1000 | Loss: 0.00002526
Iteration 91/1000 | Loss: 0.00002525
Iteration 92/1000 | Loss: 0.00002525
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002525
Iteration 98/1000 | Loss: 0.00002524
Iteration 99/1000 | Loss: 0.00002524
Iteration 100/1000 | Loss: 0.00002524
Iteration 101/1000 | Loss: 0.00002524
Iteration 102/1000 | Loss: 0.00002524
Iteration 103/1000 | Loss: 0.00002524
Iteration 104/1000 | Loss: 0.00002524
Iteration 105/1000 | Loss: 0.00002524
Iteration 106/1000 | Loss: 0.00002523
Iteration 107/1000 | Loss: 0.00002523
Iteration 108/1000 | Loss: 0.00002523
Iteration 109/1000 | Loss: 0.00002523
Iteration 110/1000 | Loss: 0.00002523
Iteration 111/1000 | Loss: 0.00002522
Iteration 112/1000 | Loss: 0.00002522
Iteration 113/1000 | Loss: 0.00002522
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002522
Iteration 117/1000 | Loss: 0.00002522
Iteration 118/1000 | Loss: 0.00002522
Iteration 119/1000 | Loss: 0.00002522
Iteration 120/1000 | Loss: 0.00002522
Iteration 121/1000 | Loss: 0.00002522
Iteration 122/1000 | Loss: 0.00002522
Iteration 123/1000 | Loss: 0.00002522
Iteration 124/1000 | Loss: 0.00002522
Iteration 125/1000 | Loss: 0.00002522
Iteration 126/1000 | Loss: 0.00002522
Iteration 127/1000 | Loss: 0.00002522
Iteration 128/1000 | Loss: 0.00002522
Iteration 129/1000 | Loss: 0.00002522
Iteration 130/1000 | Loss: 0.00002522
Iteration 131/1000 | Loss: 0.00002522
Iteration 132/1000 | Loss: 0.00002522
Iteration 133/1000 | Loss: 0.00002522
Iteration 134/1000 | Loss: 0.00002522
Iteration 135/1000 | Loss: 0.00002522
Iteration 136/1000 | Loss: 0.00002522
Iteration 137/1000 | Loss: 0.00002522
Iteration 138/1000 | Loss: 0.00002522
Iteration 139/1000 | Loss: 0.00002522
Iteration 140/1000 | Loss: 0.00002522
Iteration 141/1000 | Loss: 0.00002522
Iteration 142/1000 | Loss: 0.00002522
Iteration 143/1000 | Loss: 0.00002522
Iteration 144/1000 | Loss: 0.00002522
Iteration 145/1000 | Loss: 0.00002522
Iteration 146/1000 | Loss: 0.00002522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.5218756491085514e-05, 2.5218756491085514e-05, 2.5218756491085514e-05, 2.5218756491085514e-05, 2.5218756491085514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5218756491085514e-05

Optimization complete. Final v2v error: 4.3001017570495605 mm

Highest mean error: 10.577826499938965 mm for frame 89

Lowest mean error: 3.918409824371338 mm for frame 145

Saving results

Total time: 50.720693588256836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458679
Iteration 2/25 | Loss: 0.00163632
Iteration 3/25 | Loss: 0.00154853
Iteration 4/25 | Loss: 0.00153327
Iteration 5/25 | Loss: 0.00152654
Iteration 6/25 | Loss: 0.00152525
Iteration 7/25 | Loss: 0.00152525
Iteration 8/25 | Loss: 0.00152525
Iteration 9/25 | Loss: 0.00152525
Iteration 10/25 | Loss: 0.00152525
Iteration 11/25 | Loss: 0.00152525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015252545708790421, 0.0015252545708790421, 0.0015252545708790421, 0.0015252545708790421, 0.0015252545708790421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015252545708790421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24351847
Iteration 2/25 | Loss: 0.00232490
Iteration 3/25 | Loss: 0.00232490
Iteration 4/25 | Loss: 0.00232490
Iteration 5/25 | Loss: 0.00232490
Iteration 6/25 | Loss: 0.00232490
Iteration 7/25 | Loss: 0.00232490
Iteration 8/25 | Loss: 0.00232490
Iteration 9/25 | Loss: 0.00232490
Iteration 10/25 | Loss: 0.00232490
Iteration 11/25 | Loss: 0.00232490
Iteration 12/25 | Loss: 0.00232490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0023248987272381783, 0.0023248987272381783, 0.0023248987272381783, 0.0023248987272381783, 0.0023248987272381783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023248987272381783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232490
Iteration 2/1000 | Loss: 0.00006140
Iteration 3/1000 | Loss: 0.00004394
Iteration 4/1000 | Loss: 0.00003519
Iteration 5/1000 | Loss: 0.00003126
Iteration 6/1000 | Loss: 0.00002872
Iteration 7/1000 | Loss: 0.00002743
Iteration 8/1000 | Loss: 0.00002668
Iteration 9/1000 | Loss: 0.00002627
Iteration 10/1000 | Loss: 0.00002590
Iteration 11/1000 | Loss: 0.00002566
Iteration 12/1000 | Loss: 0.00002566
Iteration 13/1000 | Loss: 0.00002565
Iteration 14/1000 | Loss: 0.00002564
Iteration 15/1000 | Loss: 0.00002559
Iteration 16/1000 | Loss: 0.00002556
Iteration 17/1000 | Loss: 0.00002556
Iteration 18/1000 | Loss: 0.00002548
Iteration 19/1000 | Loss: 0.00002547
Iteration 20/1000 | Loss: 0.00002542
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002542
Iteration 23/1000 | Loss: 0.00002542
Iteration 24/1000 | Loss: 0.00002541
Iteration 25/1000 | Loss: 0.00002539
Iteration 26/1000 | Loss: 0.00002538
Iteration 27/1000 | Loss: 0.00002538
Iteration 28/1000 | Loss: 0.00002538
Iteration 29/1000 | Loss: 0.00002537
Iteration 30/1000 | Loss: 0.00002537
Iteration 31/1000 | Loss: 0.00002537
Iteration 32/1000 | Loss: 0.00002536
Iteration 33/1000 | Loss: 0.00002536
Iteration 34/1000 | Loss: 0.00002536
Iteration 35/1000 | Loss: 0.00002536
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002535
Iteration 38/1000 | Loss: 0.00002535
Iteration 39/1000 | Loss: 0.00002535
Iteration 40/1000 | Loss: 0.00002535
Iteration 41/1000 | Loss: 0.00002535
Iteration 42/1000 | Loss: 0.00002535
Iteration 43/1000 | Loss: 0.00002535
Iteration 44/1000 | Loss: 0.00002535
Iteration 45/1000 | Loss: 0.00002535
Iteration 46/1000 | Loss: 0.00002535
Iteration 47/1000 | Loss: 0.00002535
Iteration 48/1000 | Loss: 0.00002535
Iteration 49/1000 | Loss: 0.00002535
Iteration 50/1000 | Loss: 0.00002535
Iteration 51/1000 | Loss: 0.00002535
Iteration 52/1000 | Loss: 0.00002535
Iteration 53/1000 | Loss: 0.00002534
Iteration 54/1000 | Loss: 0.00002534
Iteration 55/1000 | Loss: 0.00002534
Iteration 56/1000 | Loss: 0.00002534
Iteration 57/1000 | Loss: 0.00002534
Iteration 58/1000 | Loss: 0.00002534
Iteration 59/1000 | Loss: 0.00002534
Iteration 60/1000 | Loss: 0.00002534
Iteration 61/1000 | Loss: 0.00002534
Iteration 62/1000 | Loss: 0.00002534
Iteration 63/1000 | Loss: 0.00002534
Iteration 64/1000 | Loss: 0.00002534
Iteration 65/1000 | Loss: 0.00002533
Iteration 66/1000 | Loss: 0.00002533
Iteration 67/1000 | Loss: 0.00002533
Iteration 68/1000 | Loss: 0.00002533
Iteration 69/1000 | Loss: 0.00002533
Iteration 70/1000 | Loss: 0.00002533
Iteration 71/1000 | Loss: 0.00002533
Iteration 72/1000 | Loss: 0.00002533
Iteration 73/1000 | Loss: 0.00002533
Iteration 74/1000 | Loss: 0.00002533
Iteration 75/1000 | Loss: 0.00002533
Iteration 76/1000 | Loss: 0.00002532
Iteration 77/1000 | Loss: 0.00002532
Iteration 78/1000 | Loss: 0.00002532
Iteration 79/1000 | Loss: 0.00002532
Iteration 80/1000 | Loss: 0.00002532
Iteration 81/1000 | Loss: 0.00002532
Iteration 82/1000 | Loss: 0.00002532
Iteration 83/1000 | Loss: 0.00002532
Iteration 84/1000 | Loss: 0.00002532
Iteration 85/1000 | Loss: 0.00002532
Iteration 86/1000 | Loss: 0.00002532
Iteration 87/1000 | Loss: 0.00002532
Iteration 88/1000 | Loss: 0.00002532
Iteration 89/1000 | Loss: 0.00002532
Iteration 90/1000 | Loss: 0.00002532
Iteration 91/1000 | Loss: 0.00002532
Iteration 92/1000 | Loss: 0.00002532
Iteration 93/1000 | Loss: 0.00002532
Iteration 94/1000 | Loss: 0.00002532
Iteration 95/1000 | Loss: 0.00002531
Iteration 96/1000 | Loss: 0.00002531
Iteration 97/1000 | Loss: 0.00002531
Iteration 98/1000 | Loss: 0.00002531
Iteration 99/1000 | Loss: 0.00002531
Iteration 100/1000 | Loss: 0.00002531
Iteration 101/1000 | Loss: 0.00002531
Iteration 102/1000 | Loss: 0.00002531
Iteration 103/1000 | Loss: 0.00002531
Iteration 104/1000 | Loss: 0.00002531
Iteration 105/1000 | Loss: 0.00002531
Iteration 106/1000 | Loss: 0.00002531
Iteration 107/1000 | Loss: 0.00002531
Iteration 108/1000 | Loss: 0.00002531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.531384234316647e-05, 2.531384234316647e-05, 2.531384234316647e-05, 2.531384234316647e-05, 2.531384234316647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.531384234316647e-05

Optimization complete. Final v2v error: 4.31081485748291 mm

Highest mean error: 4.7146525382995605 mm for frame 25

Lowest mean error: 4.058252811431885 mm for frame 45

Saving results

Total time: 31.613872051239014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959417
Iteration 2/25 | Loss: 0.00182944
Iteration 3/25 | Loss: 0.00160791
Iteration 4/25 | Loss: 0.00158601
Iteration 5/25 | Loss: 0.00158127
Iteration 6/25 | Loss: 0.00158069
Iteration 7/25 | Loss: 0.00158069
Iteration 8/25 | Loss: 0.00158069
Iteration 9/25 | Loss: 0.00158069
Iteration 10/25 | Loss: 0.00158069
Iteration 11/25 | Loss: 0.00158069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015806896844878793, 0.0015806896844878793, 0.0015806896844878793, 0.0015806896844878793, 0.0015806896844878793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015806896844878793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77171439
Iteration 2/25 | Loss: 0.00135566
Iteration 3/25 | Loss: 0.00135566
Iteration 4/25 | Loss: 0.00135566
Iteration 5/25 | Loss: 0.00135566
Iteration 6/25 | Loss: 0.00135566
Iteration 7/25 | Loss: 0.00135566
Iteration 8/25 | Loss: 0.00135566
Iteration 9/25 | Loss: 0.00135566
Iteration 10/25 | Loss: 0.00135566
Iteration 11/25 | Loss: 0.00135566
Iteration 12/25 | Loss: 0.00135566
Iteration 13/25 | Loss: 0.00135566
Iteration 14/25 | Loss: 0.00135566
Iteration 15/25 | Loss: 0.00135566
Iteration 16/25 | Loss: 0.00135566
Iteration 17/25 | Loss: 0.00135566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001355655724182725, 0.001355655724182725, 0.001355655724182725, 0.001355655724182725, 0.001355655724182725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001355655724182725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135566
Iteration 2/1000 | Loss: 0.00006475
Iteration 3/1000 | Loss: 0.00005062
Iteration 4/1000 | Loss: 0.00004405
Iteration 5/1000 | Loss: 0.00004000
Iteration 6/1000 | Loss: 0.00003879
Iteration 7/1000 | Loss: 0.00003808
Iteration 8/1000 | Loss: 0.00003756
Iteration 9/1000 | Loss: 0.00003722
Iteration 10/1000 | Loss: 0.00003689
Iteration 11/1000 | Loss: 0.00003668
Iteration 12/1000 | Loss: 0.00003660
Iteration 13/1000 | Loss: 0.00003659
Iteration 14/1000 | Loss: 0.00003659
Iteration 15/1000 | Loss: 0.00003659
Iteration 16/1000 | Loss: 0.00003659
Iteration 17/1000 | Loss: 0.00003659
Iteration 18/1000 | Loss: 0.00003659
Iteration 19/1000 | Loss: 0.00003659
Iteration 20/1000 | Loss: 0.00003659
Iteration 21/1000 | Loss: 0.00003659
Iteration 22/1000 | Loss: 0.00003659
Iteration 23/1000 | Loss: 0.00003658
Iteration 24/1000 | Loss: 0.00003658
Iteration 25/1000 | Loss: 0.00003658
Iteration 26/1000 | Loss: 0.00003658
Iteration 27/1000 | Loss: 0.00003658
Iteration 28/1000 | Loss: 0.00003658
Iteration 29/1000 | Loss: 0.00003658
Iteration 30/1000 | Loss: 0.00003658
Iteration 31/1000 | Loss: 0.00003640
Iteration 32/1000 | Loss: 0.00003635
Iteration 33/1000 | Loss: 0.00003635
Iteration 34/1000 | Loss: 0.00003635
Iteration 35/1000 | Loss: 0.00003635
Iteration 36/1000 | Loss: 0.00003635
Iteration 37/1000 | Loss: 0.00003635
Iteration 38/1000 | Loss: 0.00003635
Iteration 39/1000 | Loss: 0.00003635
Iteration 40/1000 | Loss: 0.00003635
Iteration 41/1000 | Loss: 0.00003635
Iteration 42/1000 | Loss: 0.00003635
Iteration 43/1000 | Loss: 0.00003634
Iteration 44/1000 | Loss: 0.00003634
Iteration 45/1000 | Loss: 0.00003633
Iteration 46/1000 | Loss: 0.00003623
Iteration 47/1000 | Loss: 0.00003623
Iteration 48/1000 | Loss: 0.00003621
Iteration 49/1000 | Loss: 0.00003621
Iteration 50/1000 | Loss: 0.00003621
Iteration 51/1000 | Loss: 0.00003621
Iteration 52/1000 | Loss: 0.00003621
Iteration 53/1000 | Loss: 0.00003621
Iteration 54/1000 | Loss: 0.00003620
Iteration 55/1000 | Loss: 0.00003620
Iteration 56/1000 | Loss: 0.00003620
Iteration 57/1000 | Loss: 0.00003620
Iteration 58/1000 | Loss: 0.00003620
Iteration 59/1000 | Loss: 0.00003620
Iteration 60/1000 | Loss: 0.00003620
Iteration 61/1000 | Loss: 0.00003617
Iteration 62/1000 | Loss: 0.00003617
Iteration 63/1000 | Loss: 0.00003617
Iteration 64/1000 | Loss: 0.00003616
Iteration 65/1000 | Loss: 0.00003616
Iteration 66/1000 | Loss: 0.00003613
Iteration 67/1000 | Loss: 0.00003613
Iteration 68/1000 | Loss: 0.00003613
Iteration 69/1000 | Loss: 0.00003613
Iteration 70/1000 | Loss: 0.00003613
Iteration 71/1000 | Loss: 0.00003612
Iteration 72/1000 | Loss: 0.00003612
Iteration 73/1000 | Loss: 0.00003612
Iteration 74/1000 | Loss: 0.00003612
Iteration 75/1000 | Loss: 0.00003612
Iteration 76/1000 | Loss: 0.00003612
Iteration 77/1000 | Loss: 0.00003612
Iteration 78/1000 | Loss: 0.00003612
Iteration 79/1000 | Loss: 0.00003612
Iteration 80/1000 | Loss: 0.00003612
Iteration 81/1000 | Loss: 0.00003612
Iteration 82/1000 | Loss: 0.00003611
Iteration 83/1000 | Loss: 0.00003611
Iteration 84/1000 | Loss: 0.00003610
Iteration 85/1000 | Loss: 0.00003610
Iteration 86/1000 | Loss: 0.00003610
Iteration 87/1000 | Loss: 0.00003610
Iteration 88/1000 | Loss: 0.00003610
Iteration 89/1000 | Loss: 0.00003609
Iteration 90/1000 | Loss: 0.00003608
Iteration 91/1000 | Loss: 0.00003608
Iteration 92/1000 | Loss: 0.00003608
Iteration 93/1000 | Loss: 0.00003608
Iteration 94/1000 | Loss: 0.00003608
Iteration 95/1000 | Loss: 0.00003608
Iteration 96/1000 | Loss: 0.00003608
Iteration 97/1000 | Loss: 0.00003607
Iteration 98/1000 | Loss: 0.00003607
Iteration 99/1000 | Loss: 0.00003607
Iteration 100/1000 | Loss: 0.00003607
Iteration 101/1000 | Loss: 0.00003607
Iteration 102/1000 | Loss: 0.00003607
Iteration 103/1000 | Loss: 0.00003607
Iteration 104/1000 | Loss: 0.00003607
Iteration 105/1000 | Loss: 0.00003607
Iteration 106/1000 | Loss: 0.00003607
Iteration 107/1000 | Loss: 0.00003607
Iteration 108/1000 | Loss: 0.00003607
Iteration 109/1000 | Loss: 0.00003606
Iteration 110/1000 | Loss: 0.00003606
Iteration 111/1000 | Loss: 0.00003605
Iteration 112/1000 | Loss: 0.00003605
Iteration 113/1000 | Loss: 0.00003605
Iteration 114/1000 | Loss: 0.00003605
Iteration 115/1000 | Loss: 0.00003605
Iteration 116/1000 | Loss: 0.00003604
Iteration 117/1000 | Loss: 0.00003604
Iteration 118/1000 | Loss: 0.00003604
Iteration 119/1000 | Loss: 0.00003604
Iteration 120/1000 | Loss: 0.00003604
Iteration 121/1000 | Loss: 0.00003604
Iteration 122/1000 | Loss: 0.00003604
Iteration 123/1000 | Loss: 0.00003603
Iteration 124/1000 | Loss: 0.00003602
Iteration 125/1000 | Loss: 0.00003601
Iteration 126/1000 | Loss: 0.00003601
Iteration 127/1000 | Loss: 0.00003601
Iteration 128/1000 | Loss: 0.00003601
Iteration 129/1000 | Loss: 0.00003600
Iteration 130/1000 | Loss: 0.00003599
Iteration 131/1000 | Loss: 0.00003599
Iteration 132/1000 | Loss: 0.00003599
Iteration 133/1000 | Loss: 0.00003598
Iteration 134/1000 | Loss: 0.00003598
Iteration 135/1000 | Loss: 0.00003598
Iteration 136/1000 | Loss: 0.00003598
Iteration 137/1000 | Loss: 0.00003598
Iteration 138/1000 | Loss: 0.00003598
Iteration 139/1000 | Loss: 0.00003598
Iteration 140/1000 | Loss: 0.00003598
Iteration 141/1000 | Loss: 0.00003598
Iteration 142/1000 | Loss: 0.00003598
Iteration 143/1000 | Loss: 0.00003598
Iteration 144/1000 | Loss: 0.00003598
Iteration 145/1000 | Loss: 0.00003598
Iteration 146/1000 | Loss: 0.00003598
Iteration 147/1000 | Loss: 0.00003598
Iteration 148/1000 | Loss: 0.00003598
Iteration 149/1000 | Loss: 0.00003598
Iteration 150/1000 | Loss: 0.00003598
Iteration 151/1000 | Loss: 0.00003598
Iteration 152/1000 | Loss: 0.00003598
Iteration 153/1000 | Loss: 0.00003598
Iteration 154/1000 | Loss: 0.00003598
Iteration 155/1000 | Loss: 0.00003598
Iteration 156/1000 | Loss: 0.00003598
Iteration 157/1000 | Loss: 0.00003598
Iteration 158/1000 | Loss: 0.00003598
Iteration 159/1000 | Loss: 0.00003598
Iteration 160/1000 | Loss: 0.00003597
Iteration 161/1000 | Loss: 0.00003597
Iteration 162/1000 | Loss: 0.00003597
Iteration 163/1000 | Loss: 0.00003597
Iteration 164/1000 | Loss: 0.00003597
Iteration 165/1000 | Loss: 0.00003597
Iteration 166/1000 | Loss: 0.00003597
Iteration 167/1000 | Loss: 0.00003597
Iteration 168/1000 | Loss: 0.00003597
Iteration 169/1000 | Loss: 0.00003597
Iteration 170/1000 | Loss: 0.00003597
Iteration 171/1000 | Loss: 0.00003597
Iteration 172/1000 | Loss: 0.00003597
Iteration 173/1000 | Loss: 0.00003597
Iteration 174/1000 | Loss: 0.00003597
Iteration 175/1000 | Loss: 0.00003597
Iteration 176/1000 | Loss: 0.00003597
Iteration 177/1000 | Loss: 0.00003597
Iteration 178/1000 | Loss: 0.00003597
Iteration 179/1000 | Loss: 0.00003597
Iteration 180/1000 | Loss: 0.00003597
Iteration 181/1000 | Loss: 0.00003597
Iteration 182/1000 | Loss: 0.00003597
Iteration 183/1000 | Loss: 0.00003597
Iteration 184/1000 | Loss: 0.00003597
Iteration 185/1000 | Loss: 0.00003597
Iteration 186/1000 | Loss: 0.00003597
Iteration 187/1000 | Loss: 0.00003597
Iteration 188/1000 | Loss: 0.00003597
Iteration 189/1000 | Loss: 0.00003597
Iteration 190/1000 | Loss: 0.00003597
Iteration 191/1000 | Loss: 0.00003597
Iteration 192/1000 | Loss: 0.00003597
Iteration 193/1000 | Loss: 0.00003597
Iteration 194/1000 | Loss: 0.00003597
Iteration 195/1000 | Loss: 0.00003597
Iteration 196/1000 | Loss: 0.00003597
Iteration 197/1000 | Loss: 0.00003597
Iteration 198/1000 | Loss: 0.00003597
Iteration 199/1000 | Loss: 0.00003597
Iteration 200/1000 | Loss: 0.00003597
Iteration 201/1000 | Loss: 0.00003597
Iteration 202/1000 | Loss: 0.00003597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [3.597449540393427e-05, 3.597449540393427e-05, 3.597449540393427e-05, 3.597449540393427e-05, 3.597449540393427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.597449540393427e-05

Optimization complete. Final v2v error: 5.15176248550415 mm

Highest mean error: 5.378865718841553 mm for frame 111

Lowest mean error: 4.935327053070068 mm for frame 81

Saving results

Total time: 38.726943254470825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435616
Iteration 2/25 | Loss: 0.00158526
Iteration 3/25 | Loss: 0.00150080
Iteration 4/25 | Loss: 0.00148652
Iteration 5/25 | Loss: 0.00148114
Iteration 6/25 | Loss: 0.00148058
Iteration 7/25 | Loss: 0.00148058
Iteration 8/25 | Loss: 0.00148058
Iteration 9/25 | Loss: 0.00148058
Iteration 10/25 | Loss: 0.00148058
Iteration 11/25 | Loss: 0.00148058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014805825194343925, 0.0014805825194343925, 0.0014805825194343925, 0.0014805825194343925, 0.0014805825194343925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014805825194343925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97824895
Iteration 2/25 | Loss: 0.00248740
Iteration 3/25 | Loss: 0.00248740
Iteration 4/25 | Loss: 0.00248740
Iteration 5/25 | Loss: 0.00248740
Iteration 6/25 | Loss: 0.00248740
Iteration 7/25 | Loss: 0.00248740
Iteration 8/25 | Loss: 0.00248740
Iteration 9/25 | Loss: 0.00248739
Iteration 10/25 | Loss: 0.00248739
Iteration 11/25 | Loss: 0.00248739
Iteration 12/25 | Loss: 0.00248739
Iteration 13/25 | Loss: 0.00248739
Iteration 14/25 | Loss: 0.00248739
Iteration 15/25 | Loss: 0.00248739
Iteration 16/25 | Loss: 0.00248739
Iteration 17/25 | Loss: 0.00248739
Iteration 18/25 | Loss: 0.00248739
Iteration 19/25 | Loss: 0.00248739
Iteration 20/25 | Loss: 0.00248739
Iteration 21/25 | Loss: 0.00248739
Iteration 22/25 | Loss: 0.00248739
Iteration 23/25 | Loss: 0.00248739
Iteration 24/25 | Loss: 0.00248739
Iteration 25/25 | Loss: 0.00248739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248739
Iteration 2/1000 | Loss: 0.00003934
Iteration 3/1000 | Loss: 0.00002743
Iteration 4/1000 | Loss: 0.00002327
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00002105
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00002022
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001961
Iteration 13/1000 | Loss: 0.00001952
Iteration 14/1000 | Loss: 0.00001952
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001936
Iteration 19/1000 | Loss: 0.00001935
Iteration 20/1000 | Loss: 0.00001934
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001923
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001920
Iteration 30/1000 | Loss: 0.00001920
Iteration 31/1000 | Loss: 0.00001920
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00001919
Iteration 34/1000 | Loss: 0.00001919
Iteration 35/1000 | Loss: 0.00001919
Iteration 36/1000 | Loss: 0.00001918
Iteration 37/1000 | Loss: 0.00001918
Iteration 38/1000 | Loss: 0.00001917
Iteration 39/1000 | Loss: 0.00001917
Iteration 40/1000 | Loss: 0.00001916
Iteration 41/1000 | Loss: 0.00001916
Iteration 42/1000 | Loss: 0.00001916
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001915
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00001913
Iteration 59/1000 | Loss: 0.00001913
Iteration 60/1000 | Loss: 0.00001913
Iteration 61/1000 | Loss: 0.00001913
Iteration 62/1000 | Loss: 0.00001913
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001912
Iteration 69/1000 | Loss: 0.00001912
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001911
Iteration 75/1000 | Loss: 0.00001911
Iteration 76/1000 | Loss: 0.00001911
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001910
Iteration 84/1000 | Loss: 0.00001910
Iteration 85/1000 | Loss: 0.00001910
Iteration 86/1000 | Loss: 0.00001910
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001910
Iteration 90/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.9101764337392524e-05, 1.9101764337392524e-05, 1.9101764337392524e-05, 1.9101764337392524e-05, 1.9101764337392524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9101764337392524e-05

Optimization complete. Final v2v error: 3.6987524032592773 mm

Highest mean error: 3.988858222961426 mm for frame 209

Lowest mean error: 3.5313799381256104 mm for frame 195

Saving results

Total time: 35.596442222595215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_1208/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_1208/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043400
Iteration 2/25 | Loss: 0.00308831
Iteration 3/25 | Loss: 0.00239852
Iteration 4/25 | Loss: 0.00227095
Iteration 5/25 | Loss: 0.00214984
Iteration 6/25 | Loss: 0.00203679
Iteration 7/25 | Loss: 0.00197685
Iteration 8/25 | Loss: 0.00190383
Iteration 9/25 | Loss: 0.00190768
Iteration 10/25 | Loss: 0.00186247
Iteration 11/25 | Loss: 0.00183739
Iteration 12/25 | Loss: 0.00182563
Iteration 13/25 | Loss: 0.00182567
Iteration 14/25 | Loss: 0.00184643
Iteration 15/25 | Loss: 0.00182435
Iteration 16/25 | Loss: 0.00181587
Iteration 17/25 | Loss: 0.00180940
Iteration 18/25 | Loss: 0.00180565
Iteration 19/25 | Loss: 0.00181541
Iteration 20/25 | Loss: 0.00180935
Iteration 21/25 | Loss: 0.00180211
Iteration 22/25 | Loss: 0.00180765
Iteration 23/25 | Loss: 0.00180850
Iteration 24/25 | Loss: 0.00180544
Iteration 25/25 | Loss: 0.00180346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19000888
Iteration 2/25 | Loss: 0.00771514
Iteration 3/25 | Loss: 0.00671053
Iteration 4/25 | Loss: 0.00671044
Iteration 5/25 | Loss: 0.00671044
Iteration 6/25 | Loss: 0.00671044
Iteration 7/25 | Loss: 0.00671043
Iteration 8/25 | Loss: 0.00671043
Iteration 9/25 | Loss: 0.00671043
Iteration 10/25 | Loss: 0.00671043
Iteration 11/25 | Loss: 0.00671043
Iteration 12/25 | Loss: 0.00671043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.006710434798151255, 0.006710434798151255, 0.006710434798151255, 0.006710434798151255, 0.006710434798151255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006710434798151255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00671043
Iteration 2/1000 | Loss: 0.00128213
Iteration 3/1000 | Loss: 0.00075012
Iteration 4/1000 | Loss: 0.00063187
Iteration 5/1000 | Loss: 0.00286273
Iteration 6/1000 | Loss: 0.00212475
Iteration 7/1000 | Loss: 0.00105428
Iteration 8/1000 | Loss: 0.00071549
Iteration 9/1000 | Loss: 0.00045622
Iteration 10/1000 | Loss: 0.00072146
Iteration 11/1000 | Loss: 0.00125169
Iteration 12/1000 | Loss: 0.00058589
Iteration 13/1000 | Loss: 0.00038946
Iteration 14/1000 | Loss: 0.00041461
Iteration 15/1000 | Loss: 0.00058446
Iteration 16/1000 | Loss: 0.00037563
Iteration 17/1000 | Loss: 0.00041856
Iteration 18/1000 | Loss: 0.00043843
Iteration 19/1000 | Loss: 0.00037046
Iteration 20/1000 | Loss: 0.00085223
Iteration 21/1000 | Loss: 0.00178202
Iteration 22/1000 | Loss: 0.00098587
Iteration 23/1000 | Loss: 0.00063431
Iteration 24/1000 | Loss: 0.00134366
Iteration 25/1000 | Loss: 0.00077505
Iteration 26/1000 | Loss: 0.00057689
Iteration 27/1000 | Loss: 0.00063031
Iteration 28/1000 | Loss: 0.00101110
Iteration 29/1000 | Loss: 0.00223572
Iteration 30/1000 | Loss: 0.00206375
Iteration 31/1000 | Loss: 0.00239671
Iteration 32/1000 | Loss: 0.00095255
Iteration 33/1000 | Loss: 0.00064408
Iteration 34/1000 | Loss: 0.00099674
Iteration 35/1000 | Loss: 0.00095213
Iteration 36/1000 | Loss: 0.00145020
Iteration 37/1000 | Loss: 0.00061741
Iteration 38/1000 | Loss: 0.00107095
Iteration 39/1000 | Loss: 0.00079734
Iteration 40/1000 | Loss: 0.00075390
Iteration 41/1000 | Loss: 0.00064311
Iteration 42/1000 | Loss: 0.00046062
Iteration 43/1000 | Loss: 0.00070632
Iteration 44/1000 | Loss: 0.00197600
Iteration 45/1000 | Loss: 0.00040820
Iteration 46/1000 | Loss: 0.00071245
Iteration 47/1000 | Loss: 0.00060130
Iteration 48/1000 | Loss: 0.00024108
Iteration 49/1000 | Loss: 0.00051449
Iteration 50/1000 | Loss: 0.00023199
Iteration 51/1000 | Loss: 0.00050097
Iteration 52/1000 | Loss: 0.00046147
Iteration 53/1000 | Loss: 0.00037982
Iteration 54/1000 | Loss: 0.00037304
Iteration 55/1000 | Loss: 0.00091564
Iteration 56/1000 | Loss: 0.00039630
Iteration 57/1000 | Loss: 0.00088414
Iteration 58/1000 | Loss: 0.00034439
Iteration 59/1000 | Loss: 0.00021860
Iteration 60/1000 | Loss: 0.00105976
Iteration 61/1000 | Loss: 0.00040848
Iteration 62/1000 | Loss: 0.00061441
Iteration 63/1000 | Loss: 0.00040745
Iteration 64/1000 | Loss: 0.00019488
Iteration 65/1000 | Loss: 0.00041650
Iteration 66/1000 | Loss: 0.00027837
Iteration 67/1000 | Loss: 0.00042684
Iteration 68/1000 | Loss: 0.00075369
Iteration 69/1000 | Loss: 0.00021155
Iteration 70/1000 | Loss: 0.00023543
Iteration 71/1000 | Loss: 0.00035311
Iteration 72/1000 | Loss: 0.00021032
Iteration 73/1000 | Loss: 0.00017391
Iteration 74/1000 | Loss: 0.00022238
Iteration 75/1000 | Loss: 0.00016337
Iteration 76/1000 | Loss: 0.00020899
Iteration 77/1000 | Loss: 0.00028151
Iteration 78/1000 | Loss: 0.00019053
Iteration 79/1000 | Loss: 0.00028618
Iteration 80/1000 | Loss: 0.00030070
Iteration 81/1000 | Loss: 0.00059882
Iteration 82/1000 | Loss: 0.00044669
Iteration 83/1000 | Loss: 0.00016583
Iteration 84/1000 | Loss: 0.00023770
Iteration 85/1000 | Loss: 0.00018213
Iteration 86/1000 | Loss: 0.00019907
Iteration 87/1000 | Loss: 0.00015151
Iteration 88/1000 | Loss: 0.00020499
Iteration 89/1000 | Loss: 0.00014885
Iteration 90/1000 | Loss: 0.00014820
Iteration 91/1000 | Loss: 0.00028376
Iteration 92/1000 | Loss: 0.00028826
Iteration 93/1000 | Loss: 0.00016055
Iteration 94/1000 | Loss: 0.00022624
Iteration 95/1000 | Loss: 0.00019676
Iteration 96/1000 | Loss: 0.00017109
Iteration 97/1000 | Loss: 0.00014701
Iteration 98/1000 | Loss: 0.00027130
Iteration 99/1000 | Loss: 0.00015276
Iteration 100/1000 | Loss: 0.00015158
Iteration 101/1000 | Loss: 0.00017697
Iteration 102/1000 | Loss: 0.00014654
Iteration 103/1000 | Loss: 0.00026069
Iteration 104/1000 | Loss: 0.00014499
Iteration 105/1000 | Loss: 0.00014690
Iteration 106/1000 | Loss: 0.00014205
Iteration 107/1000 | Loss: 0.00015647
Iteration 108/1000 | Loss: 0.00015008
Iteration 109/1000 | Loss: 0.00014060
Iteration 110/1000 | Loss: 0.00014038
Iteration 111/1000 | Loss: 0.00014033
Iteration 112/1000 | Loss: 0.00014635
Iteration 113/1000 | Loss: 0.00014053
Iteration 114/1000 | Loss: 0.00014747
Iteration 115/1000 | Loss: 0.00014114
Iteration 116/1000 | Loss: 0.00014164
Iteration 117/1000 | Loss: 0.00014004
Iteration 118/1000 | Loss: 0.00014004
Iteration 119/1000 | Loss: 0.00014004
Iteration 120/1000 | Loss: 0.00014004
Iteration 121/1000 | Loss: 0.00014004
Iteration 122/1000 | Loss: 0.00014004
Iteration 123/1000 | Loss: 0.00014004
Iteration 124/1000 | Loss: 0.00014004
Iteration 125/1000 | Loss: 0.00014004
Iteration 126/1000 | Loss: 0.00014004
Iteration 127/1000 | Loss: 0.00014004
Iteration 128/1000 | Loss: 0.00014003
Iteration 129/1000 | Loss: 0.00014003
Iteration 130/1000 | Loss: 0.00014003
Iteration 131/1000 | Loss: 0.00014003
Iteration 132/1000 | Loss: 0.00014002
Iteration 133/1000 | Loss: 0.00014001
Iteration 134/1000 | Loss: 0.00014001
Iteration 135/1000 | Loss: 0.00014000
Iteration 136/1000 | Loss: 0.00014000
Iteration 137/1000 | Loss: 0.00013999
Iteration 138/1000 | Loss: 0.00013999
Iteration 139/1000 | Loss: 0.00013999
Iteration 140/1000 | Loss: 0.00013999
Iteration 141/1000 | Loss: 0.00013999
Iteration 142/1000 | Loss: 0.00013999
Iteration 143/1000 | Loss: 0.00013998
Iteration 144/1000 | Loss: 0.00013998
Iteration 145/1000 | Loss: 0.00013998
Iteration 146/1000 | Loss: 0.00013998
Iteration 147/1000 | Loss: 0.00013998
Iteration 148/1000 | Loss: 0.00013998
Iteration 149/1000 | Loss: 0.00013998
Iteration 150/1000 | Loss: 0.00013997
Iteration 151/1000 | Loss: 0.00013997
Iteration 152/1000 | Loss: 0.00013997
Iteration 153/1000 | Loss: 0.00013996
Iteration 154/1000 | Loss: 0.00013996
Iteration 155/1000 | Loss: 0.00013996
Iteration 156/1000 | Loss: 0.00013996
Iteration 157/1000 | Loss: 0.00013995
Iteration 158/1000 | Loss: 0.00013995
Iteration 159/1000 | Loss: 0.00013995
Iteration 160/1000 | Loss: 0.00013995
Iteration 161/1000 | Loss: 0.00013993
Iteration 162/1000 | Loss: 0.00013992
Iteration 163/1000 | Loss: 0.00013992
Iteration 164/1000 | Loss: 0.00013991
Iteration 165/1000 | Loss: 0.00013991
Iteration 166/1000 | Loss: 0.00013991
Iteration 167/1000 | Loss: 0.00013991
Iteration 168/1000 | Loss: 0.00013991
Iteration 169/1000 | Loss: 0.00013991
Iteration 170/1000 | Loss: 0.00013990
Iteration 171/1000 | Loss: 0.00013990
Iteration 172/1000 | Loss: 0.00013990
Iteration 173/1000 | Loss: 0.00013989
Iteration 174/1000 | Loss: 0.00013989
Iteration 175/1000 | Loss: 0.00013989
Iteration 176/1000 | Loss: 0.00013997
Iteration 177/1000 | Loss: 0.00013997
Iteration 178/1000 | Loss: 0.00013997
Iteration 179/1000 | Loss: 0.00013997
Iteration 180/1000 | Loss: 0.00013996
Iteration 181/1000 | Loss: 0.00013996
Iteration 182/1000 | Loss: 0.00013996
Iteration 183/1000 | Loss: 0.00013996
Iteration 184/1000 | Loss: 0.00013996
Iteration 185/1000 | Loss: 0.00013995
Iteration 186/1000 | Loss: 0.00013995
Iteration 187/1000 | Loss: 0.00013995
Iteration 188/1000 | Loss: 0.00013995
Iteration 189/1000 | Loss: 0.00013994
Iteration 190/1000 | Loss: 0.00013992
Iteration 191/1000 | Loss: 0.00013991
Iteration 192/1000 | Loss: 0.00013991
Iteration 193/1000 | Loss: 0.00013991
Iteration 194/1000 | Loss: 0.00013990
Iteration 195/1000 | Loss: 0.00013990
Iteration 196/1000 | Loss: 0.00013990
Iteration 197/1000 | Loss: 0.00013989
Iteration 198/1000 | Loss: 0.00013989
Iteration 199/1000 | Loss: 0.00013987
Iteration 200/1000 | Loss: 0.00013987
Iteration 201/1000 | Loss: 0.00013985
Iteration 202/1000 | Loss: 0.00013985
Iteration 203/1000 | Loss: 0.00013984
Iteration 204/1000 | Loss: 0.00013983
Iteration 205/1000 | Loss: 0.00013983
Iteration 206/1000 | Loss: 0.00013983
Iteration 207/1000 | Loss: 0.00013983
Iteration 208/1000 | Loss: 0.00013983
Iteration 209/1000 | Loss: 0.00013983
Iteration 210/1000 | Loss: 0.00013983
Iteration 211/1000 | Loss: 0.00013983
Iteration 212/1000 | Loss: 0.00013983
Iteration 213/1000 | Loss: 0.00013983
Iteration 214/1000 | Loss: 0.00013983
Iteration 215/1000 | Loss: 0.00013983
Iteration 216/1000 | Loss: 0.00013983
Iteration 217/1000 | Loss: 0.00013983
Iteration 218/1000 | Loss: 0.00013983
Iteration 219/1000 | Loss: 0.00013983
Iteration 220/1000 | Loss: 0.00013983
Iteration 221/1000 | Loss: 0.00013983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [0.00013982568634673953, 0.00013982568634673953, 0.00013982568634673953, 0.00013982568634673953, 0.00013982568634673953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013982568634673953

Optimization complete. Final v2v error: 5.659235954284668 mm

Highest mean error: 12.753569602966309 mm for frame 162

Lowest mean error: 3.2126522064208984 mm for frame 209

Saving results

Total time: 247.79317045211792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00571458
Iteration 2/25 | Loss: 0.00071817
Iteration 3/25 | Loss: 0.00061684
Iteration 4/25 | Loss: 0.00059865
Iteration 5/25 | Loss: 0.00059005
Iteration 6/25 | Loss: 0.00058806
Iteration 7/25 | Loss: 0.00058765
Iteration 8/25 | Loss: 0.00058765
Iteration 9/25 | Loss: 0.00058765
Iteration 10/25 | Loss: 0.00058765
Iteration 11/25 | Loss: 0.00058765
Iteration 12/25 | Loss: 0.00058765
Iteration 13/25 | Loss: 0.00058765
Iteration 14/25 | Loss: 0.00058765
Iteration 15/25 | Loss: 0.00058765
Iteration 16/25 | Loss: 0.00058765
Iteration 17/25 | Loss: 0.00058765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000587651738896966, 0.000587651738896966, 0.000587651738896966, 0.000587651738896966, 0.000587651738896966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000587651738896966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76041722
Iteration 2/25 | Loss: 0.00026789
Iteration 3/25 | Loss: 0.00026789
Iteration 4/25 | Loss: 0.00026789
Iteration 5/25 | Loss: 0.00026789
Iteration 6/25 | Loss: 0.00026789
Iteration 7/25 | Loss: 0.00026789
Iteration 8/25 | Loss: 0.00026789
Iteration 9/25 | Loss: 0.00026789
Iteration 10/25 | Loss: 0.00026789
Iteration 11/25 | Loss: 0.00026789
Iteration 12/25 | Loss: 0.00026789
Iteration 13/25 | Loss: 0.00026789
Iteration 14/25 | Loss: 0.00026789
Iteration 15/25 | Loss: 0.00026789
Iteration 16/25 | Loss: 0.00026789
Iteration 17/25 | Loss: 0.00026789
Iteration 18/25 | Loss: 0.00026789
Iteration 19/25 | Loss: 0.00026789
Iteration 20/25 | Loss: 0.00026789
Iteration 21/25 | Loss: 0.00026789
Iteration 22/25 | Loss: 0.00026789
Iteration 23/25 | Loss: 0.00026789
Iteration 24/25 | Loss: 0.00026789
Iteration 25/25 | Loss: 0.00026789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026789
Iteration 2/1000 | Loss: 0.00002116
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001615
Iteration 5/1000 | Loss: 0.00001544
Iteration 6/1000 | Loss: 0.00001490
Iteration 7/1000 | Loss: 0.00001459
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001430
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001420
Iteration 12/1000 | Loss: 0.00001420
Iteration 13/1000 | Loss: 0.00001419
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001419
Iteration 17/1000 | Loss: 0.00001414
Iteration 18/1000 | Loss: 0.00001412
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00001409
Iteration 21/1000 | Loss: 0.00001409
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001403
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001394
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001390
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00001390
Iteration 54/1000 | Loss: 0.00001390
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001389
Iteration 57/1000 | Loss: 0.00001389
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001389
Iteration 61/1000 | Loss: 0.00001389
Iteration 62/1000 | Loss: 0.00001389
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001386
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001386
Iteration 91/1000 | Loss: 0.00001386
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001384
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001383
Iteration 101/1000 | Loss: 0.00001383
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001382
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001380
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001379
Iteration 113/1000 | Loss: 0.00001379
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001379
Iteration 116/1000 | Loss: 0.00001379
Iteration 117/1000 | Loss: 0.00001379
Iteration 118/1000 | Loss: 0.00001379
Iteration 119/1000 | Loss: 0.00001379
Iteration 120/1000 | Loss: 0.00001379
Iteration 121/1000 | Loss: 0.00001378
Iteration 122/1000 | Loss: 0.00001378
Iteration 123/1000 | Loss: 0.00001378
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001377
Iteration 129/1000 | Loss: 0.00001377
Iteration 130/1000 | Loss: 0.00001377
Iteration 131/1000 | Loss: 0.00001377
Iteration 132/1000 | Loss: 0.00001377
Iteration 133/1000 | Loss: 0.00001377
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001377
Iteration 138/1000 | Loss: 0.00001377
Iteration 139/1000 | Loss: 0.00001377
Iteration 140/1000 | Loss: 0.00001377
Iteration 141/1000 | Loss: 0.00001377
Iteration 142/1000 | Loss: 0.00001377
Iteration 143/1000 | Loss: 0.00001377
Iteration 144/1000 | Loss: 0.00001377
Iteration 145/1000 | Loss: 0.00001377
Iteration 146/1000 | Loss: 0.00001377
Iteration 147/1000 | Loss: 0.00001377
Iteration 148/1000 | Loss: 0.00001377
Iteration 149/1000 | Loss: 0.00001377
Iteration 150/1000 | Loss: 0.00001377
Iteration 151/1000 | Loss: 0.00001377
Iteration 152/1000 | Loss: 0.00001377
Iteration 153/1000 | Loss: 0.00001377
Iteration 154/1000 | Loss: 0.00001377
Iteration 155/1000 | Loss: 0.00001377
Iteration 156/1000 | Loss: 0.00001377
Iteration 157/1000 | Loss: 0.00001377
Iteration 158/1000 | Loss: 0.00001377
Iteration 159/1000 | Loss: 0.00001377
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Iteration 163/1000 | Loss: 0.00001377
Iteration 164/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3771675185125787e-05, 1.3771675185125787e-05, 1.3771675185125787e-05, 1.3771675185125787e-05, 1.3771675185125787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3771675185125787e-05

Optimization complete. Final v2v error: 3.171856164932251 mm

Highest mean error: 3.540198802947998 mm for frame 60

Lowest mean error: 2.868208408355713 mm for frame 30

Saving results

Total time: 34.87410283088684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378783
Iteration 2/25 | Loss: 0.00095903
Iteration 3/25 | Loss: 0.00067092
Iteration 4/25 | Loss: 0.00062858
Iteration 5/25 | Loss: 0.00061561
Iteration 6/25 | Loss: 0.00061270
Iteration 7/25 | Loss: 0.00061178
Iteration 8/25 | Loss: 0.00061177
Iteration 9/25 | Loss: 0.00061177
Iteration 10/25 | Loss: 0.00061177
Iteration 11/25 | Loss: 0.00061177
Iteration 12/25 | Loss: 0.00061177
Iteration 13/25 | Loss: 0.00061177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006117660086601973, 0.0006117660086601973, 0.0006117660086601973, 0.0006117660086601973, 0.0006117660086601973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006117660086601973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50736964
Iteration 2/25 | Loss: 0.00036261
Iteration 3/25 | Loss: 0.00036261
Iteration 4/25 | Loss: 0.00036261
Iteration 5/25 | Loss: 0.00036261
Iteration 6/25 | Loss: 0.00036261
Iteration 7/25 | Loss: 0.00036261
Iteration 8/25 | Loss: 0.00036261
Iteration 9/25 | Loss: 0.00036261
Iteration 10/25 | Loss: 0.00036261
Iteration 11/25 | Loss: 0.00036261
Iteration 12/25 | Loss: 0.00036261
Iteration 13/25 | Loss: 0.00036261
Iteration 14/25 | Loss: 0.00036261
Iteration 15/25 | Loss: 0.00036261
Iteration 16/25 | Loss: 0.00036261
Iteration 17/25 | Loss: 0.00036261
Iteration 18/25 | Loss: 0.00036261
Iteration 19/25 | Loss: 0.00036261
Iteration 20/25 | Loss: 0.00036261
Iteration 21/25 | Loss: 0.00036261
Iteration 22/25 | Loss: 0.00036261
Iteration 23/25 | Loss: 0.00036261
Iteration 24/25 | Loss: 0.00036261
Iteration 25/25 | Loss: 0.00036261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036261
Iteration 2/1000 | Loss: 0.00002975
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001677
Iteration 6/1000 | Loss: 0.00001621
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001534
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001525
Iteration 12/1000 | Loss: 0.00001525
Iteration 13/1000 | Loss: 0.00001525
Iteration 14/1000 | Loss: 0.00001525
Iteration 15/1000 | Loss: 0.00001518
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001517
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001512
Iteration 23/1000 | Loss: 0.00001512
Iteration 24/1000 | Loss: 0.00001511
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001507
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001503
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001502
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001501
Iteration 48/1000 | Loss: 0.00001501
Iteration 49/1000 | Loss: 0.00001500
Iteration 50/1000 | Loss: 0.00001500
Iteration 51/1000 | Loss: 0.00001500
Iteration 52/1000 | Loss: 0.00001500
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001499
Iteration 55/1000 | Loss: 0.00001499
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001498
Iteration 58/1000 | Loss: 0.00001498
Iteration 59/1000 | Loss: 0.00001498
Iteration 60/1000 | Loss: 0.00001498
Iteration 61/1000 | Loss: 0.00001497
Iteration 62/1000 | Loss: 0.00001497
Iteration 63/1000 | Loss: 0.00001497
Iteration 64/1000 | Loss: 0.00001497
Iteration 65/1000 | Loss: 0.00001497
Iteration 66/1000 | Loss: 0.00001497
Iteration 67/1000 | Loss: 0.00001497
Iteration 68/1000 | Loss: 0.00001497
Iteration 69/1000 | Loss: 0.00001497
Iteration 70/1000 | Loss: 0.00001496
Iteration 71/1000 | Loss: 0.00001496
Iteration 72/1000 | Loss: 0.00001496
Iteration 73/1000 | Loss: 0.00001496
Iteration 74/1000 | Loss: 0.00001496
Iteration 75/1000 | Loss: 0.00001496
Iteration 76/1000 | Loss: 0.00001496
Iteration 77/1000 | Loss: 0.00001495
Iteration 78/1000 | Loss: 0.00001495
Iteration 79/1000 | Loss: 0.00001495
Iteration 80/1000 | Loss: 0.00001495
Iteration 81/1000 | Loss: 0.00001495
Iteration 82/1000 | Loss: 0.00001495
Iteration 83/1000 | Loss: 0.00001495
Iteration 84/1000 | Loss: 0.00001495
Iteration 85/1000 | Loss: 0.00001495
Iteration 86/1000 | Loss: 0.00001495
Iteration 87/1000 | Loss: 0.00001495
Iteration 88/1000 | Loss: 0.00001495
Iteration 89/1000 | Loss: 0.00001495
Iteration 90/1000 | Loss: 0.00001494
Iteration 91/1000 | Loss: 0.00001494
Iteration 92/1000 | Loss: 0.00001494
Iteration 93/1000 | Loss: 0.00001494
Iteration 94/1000 | Loss: 0.00001494
Iteration 95/1000 | Loss: 0.00001494
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001493
Iteration 98/1000 | Loss: 0.00001493
Iteration 99/1000 | Loss: 0.00001493
Iteration 100/1000 | Loss: 0.00001493
Iteration 101/1000 | Loss: 0.00001492
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001492
Iteration 104/1000 | Loss: 0.00001492
Iteration 105/1000 | Loss: 0.00001492
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001491
Iteration 108/1000 | Loss: 0.00001491
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001489
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001488
Iteration 119/1000 | Loss: 0.00001488
Iteration 120/1000 | Loss: 0.00001488
Iteration 121/1000 | Loss: 0.00001488
Iteration 122/1000 | Loss: 0.00001487
Iteration 123/1000 | Loss: 0.00001487
Iteration 124/1000 | Loss: 0.00001487
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001487
Iteration 127/1000 | Loss: 0.00001487
Iteration 128/1000 | Loss: 0.00001486
Iteration 129/1000 | Loss: 0.00001486
Iteration 130/1000 | Loss: 0.00001486
Iteration 131/1000 | Loss: 0.00001486
Iteration 132/1000 | Loss: 0.00001486
Iteration 133/1000 | Loss: 0.00001485
Iteration 134/1000 | Loss: 0.00001485
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001485
Iteration 137/1000 | Loss: 0.00001485
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001484
Iteration 141/1000 | Loss: 0.00001484
Iteration 142/1000 | Loss: 0.00001484
Iteration 143/1000 | Loss: 0.00001484
Iteration 144/1000 | Loss: 0.00001484
Iteration 145/1000 | Loss: 0.00001484
Iteration 146/1000 | Loss: 0.00001484
Iteration 147/1000 | Loss: 0.00001484
Iteration 148/1000 | Loss: 0.00001484
Iteration 149/1000 | Loss: 0.00001484
Iteration 150/1000 | Loss: 0.00001484
Iteration 151/1000 | Loss: 0.00001483
Iteration 152/1000 | Loss: 0.00001483
Iteration 153/1000 | Loss: 0.00001483
Iteration 154/1000 | Loss: 0.00001483
Iteration 155/1000 | Loss: 0.00001483
Iteration 156/1000 | Loss: 0.00001483
Iteration 157/1000 | Loss: 0.00001483
Iteration 158/1000 | Loss: 0.00001483
Iteration 159/1000 | Loss: 0.00001483
Iteration 160/1000 | Loss: 0.00001483
Iteration 161/1000 | Loss: 0.00001483
Iteration 162/1000 | Loss: 0.00001482
Iteration 163/1000 | Loss: 0.00001482
Iteration 164/1000 | Loss: 0.00001482
Iteration 165/1000 | Loss: 0.00001482
Iteration 166/1000 | Loss: 0.00001482
Iteration 167/1000 | Loss: 0.00001482
Iteration 168/1000 | Loss: 0.00001481
Iteration 169/1000 | Loss: 0.00001481
Iteration 170/1000 | Loss: 0.00001481
Iteration 171/1000 | Loss: 0.00001481
Iteration 172/1000 | Loss: 0.00001481
Iteration 173/1000 | Loss: 0.00001481
Iteration 174/1000 | Loss: 0.00001481
Iteration 175/1000 | Loss: 0.00001481
Iteration 176/1000 | Loss: 0.00001481
Iteration 177/1000 | Loss: 0.00001481
Iteration 178/1000 | Loss: 0.00001481
Iteration 179/1000 | Loss: 0.00001481
Iteration 180/1000 | Loss: 0.00001481
Iteration 181/1000 | Loss: 0.00001481
Iteration 182/1000 | Loss: 0.00001481
Iteration 183/1000 | Loss: 0.00001481
Iteration 184/1000 | Loss: 0.00001481
Iteration 185/1000 | Loss: 0.00001481
Iteration 186/1000 | Loss: 0.00001481
Iteration 187/1000 | Loss: 0.00001481
Iteration 188/1000 | Loss: 0.00001481
Iteration 189/1000 | Loss: 0.00001481
Iteration 190/1000 | Loss: 0.00001481
Iteration 191/1000 | Loss: 0.00001481
Iteration 192/1000 | Loss: 0.00001481
Iteration 193/1000 | Loss: 0.00001481
Iteration 194/1000 | Loss: 0.00001481
Iteration 195/1000 | Loss: 0.00001481
Iteration 196/1000 | Loss: 0.00001481
Iteration 197/1000 | Loss: 0.00001481
Iteration 198/1000 | Loss: 0.00001481
Iteration 199/1000 | Loss: 0.00001481
Iteration 200/1000 | Loss: 0.00001481
Iteration 201/1000 | Loss: 0.00001481
Iteration 202/1000 | Loss: 0.00001481
Iteration 203/1000 | Loss: 0.00001481
Iteration 204/1000 | Loss: 0.00001481
Iteration 205/1000 | Loss: 0.00001481
Iteration 206/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4811944311077241e-05, 1.4811944311077241e-05, 1.4811944311077241e-05, 1.4811944311077241e-05, 1.4811944311077241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4811944311077241e-05

Optimization complete. Final v2v error: 3.2523505687713623 mm

Highest mean error: 3.4620110988616943 mm for frame 26

Lowest mean error: 3.0276095867156982 mm for frame 70

Saving results

Total time: 37.82888603210449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00704732
Iteration 2/25 | Loss: 0.00096342
Iteration 3/25 | Loss: 0.00079074
Iteration 4/25 | Loss: 0.00074818
Iteration 5/25 | Loss: 0.00072922
Iteration 6/25 | Loss: 0.00072607
Iteration 7/25 | Loss: 0.00072516
Iteration 8/25 | Loss: 0.00072516
Iteration 9/25 | Loss: 0.00072516
Iteration 10/25 | Loss: 0.00072516
Iteration 11/25 | Loss: 0.00072516
Iteration 12/25 | Loss: 0.00072516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007251622155308723, 0.0007251622155308723, 0.0007251622155308723, 0.0007251622155308723, 0.0007251622155308723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007251622155308723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67922473
Iteration 2/25 | Loss: 0.00038700
Iteration 3/25 | Loss: 0.00038699
Iteration 4/25 | Loss: 0.00038699
Iteration 5/25 | Loss: 0.00038699
Iteration 6/25 | Loss: 0.00038699
Iteration 7/25 | Loss: 0.00038699
Iteration 8/25 | Loss: 0.00038699
Iteration 9/25 | Loss: 0.00038699
Iteration 10/25 | Loss: 0.00038699
Iteration 11/25 | Loss: 0.00038699
Iteration 12/25 | Loss: 0.00038699
Iteration 13/25 | Loss: 0.00038699
Iteration 14/25 | Loss: 0.00038699
Iteration 15/25 | Loss: 0.00038699
Iteration 16/25 | Loss: 0.00038699
Iteration 17/25 | Loss: 0.00038699
Iteration 18/25 | Loss: 0.00038699
Iteration 19/25 | Loss: 0.00038699
Iteration 20/25 | Loss: 0.00038699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003869864740408957, 0.0003869864740408957, 0.0003869864740408957, 0.0003869864740408957, 0.0003869864740408957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003869864740408957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038699
Iteration 2/1000 | Loss: 0.00004348
Iteration 3/1000 | Loss: 0.00003232
Iteration 4/1000 | Loss: 0.00002838
Iteration 5/1000 | Loss: 0.00002667
Iteration 6/1000 | Loss: 0.00002578
Iteration 7/1000 | Loss: 0.00002512
Iteration 8/1000 | Loss: 0.00002456
Iteration 9/1000 | Loss: 0.00002407
Iteration 10/1000 | Loss: 0.00002372
Iteration 11/1000 | Loss: 0.00002348
Iteration 12/1000 | Loss: 0.00002344
Iteration 13/1000 | Loss: 0.00002326
Iteration 14/1000 | Loss: 0.00002318
Iteration 15/1000 | Loss: 0.00002314
Iteration 16/1000 | Loss: 0.00002312
Iteration 17/1000 | Loss: 0.00002311
Iteration 18/1000 | Loss: 0.00002311
Iteration 19/1000 | Loss: 0.00002310
Iteration 20/1000 | Loss: 0.00002309
Iteration 21/1000 | Loss: 0.00002307
Iteration 22/1000 | Loss: 0.00002304
Iteration 23/1000 | Loss: 0.00002303
Iteration 24/1000 | Loss: 0.00002300
Iteration 25/1000 | Loss: 0.00002296
Iteration 26/1000 | Loss: 0.00002295
Iteration 27/1000 | Loss: 0.00002295
Iteration 28/1000 | Loss: 0.00002294
Iteration 29/1000 | Loss: 0.00002294
Iteration 30/1000 | Loss: 0.00002293
Iteration 31/1000 | Loss: 0.00002292
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002291
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002290
Iteration 38/1000 | Loss: 0.00002290
Iteration 39/1000 | Loss: 0.00002290
Iteration 40/1000 | Loss: 0.00002290
Iteration 41/1000 | Loss: 0.00002290
Iteration 42/1000 | Loss: 0.00002290
Iteration 43/1000 | Loss: 0.00002290
Iteration 44/1000 | Loss: 0.00002289
Iteration 45/1000 | Loss: 0.00002289
Iteration 46/1000 | Loss: 0.00002289
Iteration 47/1000 | Loss: 0.00002289
Iteration 48/1000 | Loss: 0.00002288
Iteration 49/1000 | Loss: 0.00002288
Iteration 50/1000 | Loss: 0.00002288
Iteration 51/1000 | Loss: 0.00002287
Iteration 52/1000 | Loss: 0.00002287
Iteration 53/1000 | Loss: 0.00002287
Iteration 54/1000 | Loss: 0.00002287
Iteration 55/1000 | Loss: 0.00002286
Iteration 56/1000 | Loss: 0.00002286
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002285
Iteration 59/1000 | Loss: 0.00002285
Iteration 60/1000 | Loss: 0.00002285
Iteration 61/1000 | Loss: 0.00002285
Iteration 62/1000 | Loss: 0.00002284
Iteration 63/1000 | Loss: 0.00002284
Iteration 64/1000 | Loss: 0.00002284
Iteration 65/1000 | Loss: 0.00002284
Iteration 66/1000 | Loss: 0.00002283
Iteration 67/1000 | Loss: 0.00002283
Iteration 68/1000 | Loss: 0.00002283
Iteration 69/1000 | Loss: 0.00002283
Iteration 70/1000 | Loss: 0.00002282
Iteration 71/1000 | Loss: 0.00002282
Iteration 72/1000 | Loss: 0.00002282
Iteration 73/1000 | Loss: 0.00002282
Iteration 74/1000 | Loss: 0.00002282
Iteration 75/1000 | Loss: 0.00002282
Iteration 76/1000 | Loss: 0.00002282
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002281
Iteration 80/1000 | Loss: 0.00002281
Iteration 81/1000 | Loss: 0.00002281
Iteration 82/1000 | Loss: 0.00002281
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002281
Iteration 87/1000 | Loss: 0.00002281
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002280
Iteration 91/1000 | Loss: 0.00002280
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002280
Iteration 94/1000 | Loss: 0.00002280
Iteration 95/1000 | Loss: 0.00002280
Iteration 96/1000 | Loss: 0.00002280
Iteration 97/1000 | Loss: 0.00002280
Iteration 98/1000 | Loss: 0.00002280
Iteration 99/1000 | Loss: 0.00002280
Iteration 100/1000 | Loss: 0.00002279
Iteration 101/1000 | Loss: 0.00002279
Iteration 102/1000 | Loss: 0.00002279
Iteration 103/1000 | Loss: 0.00002279
Iteration 104/1000 | Loss: 0.00002279
Iteration 105/1000 | Loss: 0.00002279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.279310683661606e-05, 2.279310683661606e-05, 2.279310683661606e-05, 2.279310683661606e-05, 2.279310683661606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.279310683661606e-05

Optimization complete. Final v2v error: 3.9380719661712646 mm

Highest mean error: 5.053447246551514 mm for frame 164

Lowest mean error: 3.487424850463867 mm for frame 0

Saving results

Total time: 42.90924072265625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384435
Iteration 2/25 | Loss: 0.00071105
Iteration 3/25 | Loss: 0.00058906
Iteration 4/25 | Loss: 0.00057294
Iteration 5/25 | Loss: 0.00056605
Iteration 6/25 | Loss: 0.00056463
Iteration 7/25 | Loss: 0.00056456
Iteration 8/25 | Loss: 0.00056456
Iteration 9/25 | Loss: 0.00056456
Iteration 10/25 | Loss: 0.00056456
Iteration 11/25 | Loss: 0.00056456
Iteration 12/25 | Loss: 0.00056456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005645573255605996, 0.0005645573255605996, 0.0005645573255605996, 0.0005645573255605996, 0.0005645573255605996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005645573255605996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.33656454
Iteration 2/25 | Loss: 0.00022731
Iteration 3/25 | Loss: 0.00022731
Iteration 4/25 | Loss: 0.00022731
Iteration 5/25 | Loss: 0.00022731
Iteration 6/25 | Loss: 0.00022731
Iteration 7/25 | Loss: 0.00022731
Iteration 8/25 | Loss: 0.00022731
Iteration 9/25 | Loss: 0.00022731
Iteration 10/25 | Loss: 0.00022730
Iteration 11/25 | Loss: 0.00022730
Iteration 12/25 | Loss: 0.00022730
Iteration 13/25 | Loss: 0.00022730
Iteration 14/25 | Loss: 0.00022730
Iteration 15/25 | Loss: 0.00022730
Iteration 16/25 | Loss: 0.00022730
Iteration 17/25 | Loss: 0.00022730
Iteration 18/25 | Loss: 0.00022730
Iteration 19/25 | Loss: 0.00022730
Iteration 20/25 | Loss: 0.00022730
Iteration 21/25 | Loss: 0.00022730
Iteration 22/25 | Loss: 0.00022730
Iteration 23/25 | Loss: 0.00022730
Iteration 24/25 | Loss: 0.00022730
Iteration 25/25 | Loss: 0.00022730

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022730
Iteration 2/1000 | Loss: 0.00001742
Iteration 3/1000 | Loss: 0.00001408
Iteration 4/1000 | Loss: 0.00001343
Iteration 5/1000 | Loss: 0.00001287
Iteration 6/1000 | Loss: 0.00001253
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001210
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001182
Iteration 19/1000 | Loss: 0.00001182
Iteration 20/1000 | Loss: 0.00001182
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001180
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001165
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001157
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001156
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001153
Iteration 48/1000 | Loss: 0.00001153
Iteration 49/1000 | Loss: 0.00001152
Iteration 50/1000 | Loss: 0.00001152
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001151
Iteration 60/1000 | Loss: 0.00001151
Iteration 61/1000 | Loss: 0.00001151
Iteration 62/1000 | Loss: 0.00001151
Iteration 63/1000 | Loss: 0.00001151
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001148
Iteration 69/1000 | Loss: 0.00001148
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001148
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001147
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001147
Iteration 84/1000 | Loss: 0.00001147
Iteration 85/1000 | Loss: 0.00001147
Iteration 86/1000 | Loss: 0.00001147
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.1469246601336636e-05, 1.1469246601336636e-05, 1.1469246601336636e-05, 1.1469246601336636e-05, 1.1469246601336636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1469246601336636e-05

Optimization complete. Final v2v error: 2.9205830097198486 mm

Highest mean error: 3.2447493076324463 mm for frame 137

Lowest mean error: 2.5849292278289795 mm for frame 55

Saving results

Total time: 34.31665897369385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374162
Iteration 2/25 | Loss: 0.00067377
Iteration 3/25 | Loss: 0.00054418
Iteration 4/25 | Loss: 0.00053243
Iteration 5/25 | Loss: 0.00053016
Iteration 6/25 | Loss: 0.00052948
Iteration 7/25 | Loss: 0.00052948
Iteration 8/25 | Loss: 0.00052948
Iteration 9/25 | Loss: 0.00052948
Iteration 10/25 | Loss: 0.00052948
Iteration 11/25 | Loss: 0.00052948
Iteration 12/25 | Loss: 0.00052948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005294766160659492, 0.0005294766160659492, 0.0005294766160659492, 0.0005294766160659492, 0.0005294766160659492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005294766160659492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42262197
Iteration 2/25 | Loss: 0.00022890
Iteration 3/25 | Loss: 0.00022889
Iteration 4/25 | Loss: 0.00022889
Iteration 5/25 | Loss: 0.00022889
Iteration 6/25 | Loss: 0.00022889
Iteration 7/25 | Loss: 0.00022889
Iteration 8/25 | Loss: 0.00022889
Iteration 9/25 | Loss: 0.00022889
Iteration 10/25 | Loss: 0.00022889
Iteration 11/25 | Loss: 0.00022889
Iteration 12/25 | Loss: 0.00022889
Iteration 13/25 | Loss: 0.00022889
Iteration 14/25 | Loss: 0.00022889
Iteration 15/25 | Loss: 0.00022889
Iteration 16/25 | Loss: 0.00022889
Iteration 17/25 | Loss: 0.00022889
Iteration 18/25 | Loss: 0.00022889
Iteration 19/25 | Loss: 0.00022889
Iteration 20/25 | Loss: 0.00022889
Iteration 21/25 | Loss: 0.00022889
Iteration 22/25 | Loss: 0.00022889
Iteration 23/25 | Loss: 0.00022889
Iteration 24/25 | Loss: 0.00022889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002288874820806086, 0.0002288874820806086, 0.0002288874820806086, 0.0002288874820806086, 0.0002288874820806086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002288874820806086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022889
Iteration 2/1000 | Loss: 0.00001812
Iteration 3/1000 | Loss: 0.00001262
Iteration 4/1000 | Loss: 0.00000989
Iteration 5/1000 | Loss: 0.00000917
Iteration 6/1000 | Loss: 0.00000894
Iteration 7/1000 | Loss: 0.00000862
Iteration 8/1000 | Loss: 0.00000841
Iteration 9/1000 | Loss: 0.00000831
Iteration 10/1000 | Loss: 0.00000831
Iteration 11/1000 | Loss: 0.00000828
Iteration 12/1000 | Loss: 0.00000828
Iteration 13/1000 | Loss: 0.00000822
Iteration 14/1000 | Loss: 0.00000822
Iteration 15/1000 | Loss: 0.00000819
Iteration 16/1000 | Loss: 0.00000814
Iteration 17/1000 | Loss: 0.00000814
Iteration 18/1000 | Loss: 0.00000813
Iteration 19/1000 | Loss: 0.00000813
Iteration 20/1000 | Loss: 0.00000812
Iteration 21/1000 | Loss: 0.00000812
Iteration 22/1000 | Loss: 0.00000808
Iteration 23/1000 | Loss: 0.00000808
Iteration 24/1000 | Loss: 0.00000808
Iteration 25/1000 | Loss: 0.00000808
Iteration 26/1000 | Loss: 0.00000807
Iteration 27/1000 | Loss: 0.00000807
Iteration 28/1000 | Loss: 0.00000807
Iteration 29/1000 | Loss: 0.00000807
Iteration 30/1000 | Loss: 0.00000807
Iteration 31/1000 | Loss: 0.00000807
Iteration 32/1000 | Loss: 0.00000807
Iteration 33/1000 | Loss: 0.00000807
Iteration 34/1000 | Loss: 0.00000807
Iteration 35/1000 | Loss: 0.00000806
Iteration 36/1000 | Loss: 0.00000806
Iteration 37/1000 | Loss: 0.00000806
Iteration 38/1000 | Loss: 0.00000805
Iteration 39/1000 | Loss: 0.00000805
Iteration 40/1000 | Loss: 0.00000804
Iteration 41/1000 | Loss: 0.00000804
Iteration 42/1000 | Loss: 0.00000803
Iteration 43/1000 | Loss: 0.00000802
Iteration 44/1000 | Loss: 0.00000802
Iteration 45/1000 | Loss: 0.00000801
Iteration 46/1000 | Loss: 0.00000801
Iteration 47/1000 | Loss: 0.00000801
Iteration 48/1000 | Loss: 0.00000801
Iteration 49/1000 | Loss: 0.00000801
Iteration 50/1000 | Loss: 0.00000801
Iteration 51/1000 | Loss: 0.00000800
Iteration 52/1000 | Loss: 0.00000800
Iteration 53/1000 | Loss: 0.00000797
Iteration 54/1000 | Loss: 0.00000797
Iteration 55/1000 | Loss: 0.00000796
Iteration 56/1000 | Loss: 0.00000796
Iteration 57/1000 | Loss: 0.00000796
Iteration 58/1000 | Loss: 0.00000793
Iteration 59/1000 | Loss: 0.00000792
Iteration 60/1000 | Loss: 0.00000792
Iteration 61/1000 | Loss: 0.00000792
Iteration 62/1000 | Loss: 0.00000791
Iteration 63/1000 | Loss: 0.00000791
Iteration 64/1000 | Loss: 0.00000791
Iteration 65/1000 | Loss: 0.00000790
Iteration 66/1000 | Loss: 0.00000790
Iteration 67/1000 | Loss: 0.00000790
Iteration 68/1000 | Loss: 0.00000790
Iteration 69/1000 | Loss: 0.00000790
Iteration 70/1000 | Loss: 0.00000790
Iteration 71/1000 | Loss: 0.00000789
Iteration 72/1000 | Loss: 0.00000789
Iteration 73/1000 | Loss: 0.00000789
Iteration 74/1000 | Loss: 0.00000788
Iteration 75/1000 | Loss: 0.00000788
Iteration 76/1000 | Loss: 0.00000788
Iteration 77/1000 | Loss: 0.00000788
Iteration 78/1000 | Loss: 0.00000787
Iteration 79/1000 | Loss: 0.00000787
Iteration 80/1000 | Loss: 0.00000787
Iteration 81/1000 | Loss: 0.00000786
Iteration 82/1000 | Loss: 0.00000786
Iteration 83/1000 | Loss: 0.00000786
Iteration 84/1000 | Loss: 0.00000786
Iteration 85/1000 | Loss: 0.00000786
Iteration 86/1000 | Loss: 0.00000786
Iteration 87/1000 | Loss: 0.00000786
Iteration 88/1000 | Loss: 0.00000786
Iteration 89/1000 | Loss: 0.00000786
Iteration 90/1000 | Loss: 0.00000786
Iteration 91/1000 | Loss: 0.00000786
Iteration 92/1000 | Loss: 0.00000785
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000785
Iteration 95/1000 | Loss: 0.00000785
Iteration 96/1000 | Loss: 0.00000785
Iteration 97/1000 | Loss: 0.00000785
Iteration 98/1000 | Loss: 0.00000785
Iteration 99/1000 | Loss: 0.00000785
Iteration 100/1000 | Loss: 0.00000785
Iteration 101/1000 | Loss: 0.00000785
Iteration 102/1000 | Loss: 0.00000785
Iteration 103/1000 | Loss: 0.00000785
Iteration 104/1000 | Loss: 0.00000785
Iteration 105/1000 | Loss: 0.00000785
Iteration 106/1000 | Loss: 0.00000784
Iteration 107/1000 | Loss: 0.00000784
Iteration 108/1000 | Loss: 0.00000784
Iteration 109/1000 | Loss: 0.00000784
Iteration 110/1000 | Loss: 0.00000784
Iteration 111/1000 | Loss: 0.00000784
Iteration 112/1000 | Loss: 0.00000784
Iteration 113/1000 | Loss: 0.00000784
Iteration 114/1000 | Loss: 0.00000784
Iteration 115/1000 | Loss: 0.00000784
Iteration 116/1000 | Loss: 0.00000784
Iteration 117/1000 | Loss: 0.00000784
Iteration 118/1000 | Loss: 0.00000784
Iteration 119/1000 | Loss: 0.00000784
Iteration 120/1000 | Loss: 0.00000783
Iteration 121/1000 | Loss: 0.00000783
Iteration 122/1000 | Loss: 0.00000783
Iteration 123/1000 | Loss: 0.00000783
Iteration 124/1000 | Loss: 0.00000783
Iteration 125/1000 | Loss: 0.00000783
Iteration 126/1000 | Loss: 0.00000783
Iteration 127/1000 | Loss: 0.00000783
Iteration 128/1000 | Loss: 0.00000782
Iteration 129/1000 | Loss: 0.00000782
Iteration 130/1000 | Loss: 0.00000782
Iteration 131/1000 | Loss: 0.00000782
Iteration 132/1000 | Loss: 0.00000782
Iteration 133/1000 | Loss: 0.00000782
Iteration 134/1000 | Loss: 0.00000782
Iteration 135/1000 | Loss: 0.00000782
Iteration 136/1000 | Loss: 0.00000782
Iteration 137/1000 | Loss: 0.00000782
Iteration 138/1000 | Loss: 0.00000782
Iteration 139/1000 | Loss: 0.00000782
Iteration 140/1000 | Loss: 0.00000782
Iteration 141/1000 | Loss: 0.00000782
Iteration 142/1000 | Loss: 0.00000782
Iteration 143/1000 | Loss: 0.00000782
Iteration 144/1000 | Loss: 0.00000782
Iteration 145/1000 | Loss: 0.00000782
Iteration 146/1000 | Loss: 0.00000782
Iteration 147/1000 | Loss: 0.00000781
Iteration 148/1000 | Loss: 0.00000781
Iteration 149/1000 | Loss: 0.00000781
Iteration 150/1000 | Loss: 0.00000781
Iteration 151/1000 | Loss: 0.00000781
Iteration 152/1000 | Loss: 0.00000781
Iteration 153/1000 | Loss: 0.00000781
Iteration 154/1000 | Loss: 0.00000781
Iteration 155/1000 | Loss: 0.00000781
Iteration 156/1000 | Loss: 0.00000781
Iteration 157/1000 | Loss: 0.00000781
Iteration 158/1000 | Loss: 0.00000781
Iteration 159/1000 | Loss: 0.00000781
Iteration 160/1000 | Loss: 0.00000780
Iteration 161/1000 | Loss: 0.00000780
Iteration 162/1000 | Loss: 0.00000780
Iteration 163/1000 | Loss: 0.00000780
Iteration 164/1000 | Loss: 0.00000780
Iteration 165/1000 | Loss: 0.00000780
Iteration 166/1000 | Loss: 0.00000780
Iteration 167/1000 | Loss: 0.00000780
Iteration 168/1000 | Loss: 0.00000780
Iteration 169/1000 | Loss: 0.00000780
Iteration 170/1000 | Loss: 0.00000780
Iteration 171/1000 | Loss: 0.00000780
Iteration 172/1000 | Loss: 0.00000780
Iteration 173/1000 | Loss: 0.00000780
Iteration 174/1000 | Loss: 0.00000780
Iteration 175/1000 | Loss: 0.00000780
Iteration 176/1000 | Loss: 0.00000779
Iteration 177/1000 | Loss: 0.00000779
Iteration 178/1000 | Loss: 0.00000779
Iteration 179/1000 | Loss: 0.00000779
Iteration 180/1000 | Loss: 0.00000779
Iteration 181/1000 | Loss: 0.00000779
Iteration 182/1000 | Loss: 0.00000779
Iteration 183/1000 | Loss: 0.00000779
Iteration 184/1000 | Loss: 0.00000779
Iteration 185/1000 | Loss: 0.00000779
Iteration 186/1000 | Loss: 0.00000779
Iteration 187/1000 | Loss: 0.00000779
Iteration 188/1000 | Loss: 0.00000779
Iteration 189/1000 | Loss: 0.00000778
Iteration 190/1000 | Loss: 0.00000778
Iteration 191/1000 | Loss: 0.00000778
Iteration 192/1000 | Loss: 0.00000778
Iteration 193/1000 | Loss: 0.00000778
Iteration 194/1000 | Loss: 0.00000778
Iteration 195/1000 | Loss: 0.00000778
Iteration 196/1000 | Loss: 0.00000778
Iteration 197/1000 | Loss: 0.00000778
Iteration 198/1000 | Loss: 0.00000778
Iteration 199/1000 | Loss: 0.00000778
Iteration 200/1000 | Loss: 0.00000778
Iteration 201/1000 | Loss: 0.00000778
Iteration 202/1000 | Loss: 0.00000778
Iteration 203/1000 | Loss: 0.00000778
Iteration 204/1000 | Loss: 0.00000778
Iteration 205/1000 | Loss: 0.00000778
Iteration 206/1000 | Loss: 0.00000778
Iteration 207/1000 | Loss: 0.00000778
Iteration 208/1000 | Loss: 0.00000778
Iteration 209/1000 | Loss: 0.00000778
Iteration 210/1000 | Loss: 0.00000778
Iteration 211/1000 | Loss: 0.00000778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [7.77962850406766e-06, 7.77962850406766e-06, 7.77962850406766e-06, 7.77962850406766e-06, 7.77962850406766e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.77962850406766e-06

Optimization complete. Final v2v error: 2.39673113822937 mm

Highest mean error: 2.562591791152954 mm for frame 77

Lowest mean error: 2.238292694091797 mm for frame 48

Saving results

Total time: 36.31052851676941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496017
Iteration 2/25 | Loss: 0.00090302
Iteration 3/25 | Loss: 0.00066272
Iteration 4/25 | Loss: 0.00063762
Iteration 5/25 | Loss: 0.00062934
Iteration 6/25 | Loss: 0.00062758
Iteration 7/25 | Loss: 0.00062758
Iteration 8/25 | Loss: 0.00062758
Iteration 9/25 | Loss: 0.00062758
Iteration 10/25 | Loss: 0.00062758
Iteration 11/25 | Loss: 0.00062758
Iteration 12/25 | Loss: 0.00062758
Iteration 13/25 | Loss: 0.00062758
Iteration 14/25 | Loss: 0.00062758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006275805062614381, 0.0006275805062614381, 0.0006275805062614381, 0.0006275805062614381, 0.0006275805062614381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006275805062614381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84113985
Iteration 2/25 | Loss: 0.00020959
Iteration 3/25 | Loss: 0.00020958
Iteration 4/25 | Loss: 0.00020958
Iteration 5/25 | Loss: 0.00020958
Iteration 6/25 | Loss: 0.00020958
Iteration 7/25 | Loss: 0.00020958
Iteration 8/25 | Loss: 0.00020958
Iteration 9/25 | Loss: 0.00020958
Iteration 10/25 | Loss: 0.00020958
Iteration 11/25 | Loss: 0.00020958
Iteration 12/25 | Loss: 0.00020958
Iteration 13/25 | Loss: 0.00020958
Iteration 14/25 | Loss: 0.00020958
Iteration 15/25 | Loss: 0.00020958
Iteration 16/25 | Loss: 0.00020958
Iteration 17/25 | Loss: 0.00020958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00020958091772627085, 0.00020958091772627085, 0.00020958091772627085, 0.00020958091772627085, 0.00020958091772627085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020958091772627085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020958
Iteration 2/1000 | Loss: 0.00002640
Iteration 3/1000 | Loss: 0.00002029
Iteration 4/1000 | Loss: 0.00001921
Iteration 5/1000 | Loss: 0.00001821
Iteration 6/1000 | Loss: 0.00001751
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001640
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001576
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001556
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001555
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001553
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00001551
Iteration 29/1000 | Loss: 0.00001551
Iteration 30/1000 | Loss: 0.00001551
Iteration 31/1000 | Loss: 0.00001550
Iteration 32/1000 | Loss: 0.00001550
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001544
Iteration 51/1000 | Loss: 0.00001544
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001542
Iteration 58/1000 | Loss: 0.00001542
Iteration 59/1000 | Loss: 0.00001542
Iteration 60/1000 | Loss: 0.00001542
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001542
Iteration 63/1000 | Loss: 0.00001542
Iteration 64/1000 | Loss: 0.00001542
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001541
Iteration 67/1000 | Loss: 0.00001541
Iteration 68/1000 | Loss: 0.00001540
Iteration 69/1000 | Loss: 0.00001540
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001539
Iteration 72/1000 | Loss: 0.00001539
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001539
Iteration 77/1000 | Loss: 0.00001539
Iteration 78/1000 | Loss: 0.00001539
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001533
Iteration 94/1000 | Loss: 0.00001533
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001533
Iteration 98/1000 | Loss: 0.00001533
Iteration 99/1000 | Loss: 0.00001533
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001533
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001531
Iteration 110/1000 | Loss: 0.00001531
Iteration 111/1000 | Loss: 0.00001531
Iteration 112/1000 | Loss: 0.00001531
Iteration 113/1000 | Loss: 0.00001531
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001530
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Iteration 123/1000 | Loss: 0.00001529
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001529
Iteration 127/1000 | Loss: 0.00001529
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001527
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001527
Iteration 147/1000 | Loss: 0.00001526
Iteration 148/1000 | Loss: 0.00001526
Iteration 149/1000 | Loss: 0.00001526
Iteration 150/1000 | Loss: 0.00001526
Iteration 151/1000 | Loss: 0.00001526
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Iteration 154/1000 | Loss: 0.00001525
Iteration 155/1000 | Loss: 0.00001525
Iteration 156/1000 | Loss: 0.00001525
Iteration 157/1000 | Loss: 0.00001524
Iteration 158/1000 | Loss: 0.00001524
Iteration 159/1000 | Loss: 0.00001524
Iteration 160/1000 | Loss: 0.00001524
Iteration 161/1000 | Loss: 0.00001523
Iteration 162/1000 | Loss: 0.00001523
Iteration 163/1000 | Loss: 0.00001523
Iteration 164/1000 | Loss: 0.00001523
Iteration 165/1000 | Loss: 0.00001523
Iteration 166/1000 | Loss: 0.00001523
Iteration 167/1000 | Loss: 0.00001523
Iteration 168/1000 | Loss: 0.00001523
Iteration 169/1000 | Loss: 0.00001523
Iteration 170/1000 | Loss: 0.00001523
Iteration 171/1000 | Loss: 0.00001523
Iteration 172/1000 | Loss: 0.00001523
Iteration 173/1000 | Loss: 0.00001522
Iteration 174/1000 | Loss: 0.00001522
Iteration 175/1000 | Loss: 0.00001522
Iteration 176/1000 | Loss: 0.00001522
Iteration 177/1000 | Loss: 0.00001522
Iteration 178/1000 | Loss: 0.00001522
Iteration 179/1000 | Loss: 0.00001522
Iteration 180/1000 | Loss: 0.00001522
Iteration 181/1000 | Loss: 0.00001522
Iteration 182/1000 | Loss: 0.00001522
Iteration 183/1000 | Loss: 0.00001522
Iteration 184/1000 | Loss: 0.00001522
Iteration 185/1000 | Loss: 0.00001521
Iteration 186/1000 | Loss: 0.00001521
Iteration 187/1000 | Loss: 0.00001521
Iteration 188/1000 | Loss: 0.00001521
Iteration 189/1000 | Loss: 0.00001521
Iteration 190/1000 | Loss: 0.00001521
Iteration 191/1000 | Loss: 0.00001521
Iteration 192/1000 | Loss: 0.00001521
Iteration 193/1000 | Loss: 0.00001521
Iteration 194/1000 | Loss: 0.00001521
Iteration 195/1000 | Loss: 0.00001521
Iteration 196/1000 | Loss: 0.00001520
Iteration 197/1000 | Loss: 0.00001520
Iteration 198/1000 | Loss: 0.00001520
Iteration 199/1000 | Loss: 0.00001520
Iteration 200/1000 | Loss: 0.00001519
Iteration 201/1000 | Loss: 0.00001519
Iteration 202/1000 | Loss: 0.00001519
Iteration 203/1000 | Loss: 0.00001519
Iteration 204/1000 | Loss: 0.00001519
Iteration 205/1000 | Loss: 0.00001519
Iteration 206/1000 | Loss: 0.00001519
Iteration 207/1000 | Loss: 0.00001519
Iteration 208/1000 | Loss: 0.00001519
Iteration 209/1000 | Loss: 0.00001519
Iteration 210/1000 | Loss: 0.00001519
Iteration 211/1000 | Loss: 0.00001519
Iteration 212/1000 | Loss: 0.00001519
Iteration 213/1000 | Loss: 0.00001519
Iteration 214/1000 | Loss: 0.00001519
Iteration 215/1000 | Loss: 0.00001519
Iteration 216/1000 | Loss: 0.00001518
Iteration 217/1000 | Loss: 0.00001518
Iteration 218/1000 | Loss: 0.00001518
Iteration 219/1000 | Loss: 0.00001518
Iteration 220/1000 | Loss: 0.00001518
Iteration 221/1000 | Loss: 0.00001518
Iteration 222/1000 | Loss: 0.00001518
Iteration 223/1000 | Loss: 0.00001518
Iteration 224/1000 | Loss: 0.00001518
Iteration 225/1000 | Loss: 0.00001518
Iteration 226/1000 | Loss: 0.00001518
Iteration 227/1000 | Loss: 0.00001518
Iteration 228/1000 | Loss: 0.00001518
Iteration 229/1000 | Loss: 0.00001518
Iteration 230/1000 | Loss: 0.00001518
Iteration 231/1000 | Loss: 0.00001518
Iteration 232/1000 | Loss: 0.00001518
Iteration 233/1000 | Loss: 0.00001518
Iteration 234/1000 | Loss: 0.00001518
Iteration 235/1000 | Loss: 0.00001518
Iteration 236/1000 | Loss: 0.00001518
Iteration 237/1000 | Loss: 0.00001518
Iteration 238/1000 | Loss: 0.00001518
Iteration 239/1000 | Loss: 0.00001518
Iteration 240/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.5177314708125778e-05, 1.5177314708125778e-05, 1.5177314708125778e-05, 1.5177314708125778e-05, 1.5177314708125778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5177314708125778e-05

Optimization complete. Final v2v error: 3.382780075073242 mm

Highest mean error: 3.741182565689087 mm for frame 0

Lowest mean error: 3.08323335647583 mm for frame 239

Saving results

Total time: 49.119428634643555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806354
Iteration 2/25 | Loss: 0.00120055
Iteration 3/25 | Loss: 0.00088814
Iteration 4/25 | Loss: 0.00082107
Iteration 5/25 | Loss: 0.00080919
Iteration 6/25 | Loss: 0.00081005
Iteration 7/25 | Loss: 0.00078214
Iteration 8/25 | Loss: 0.00075863
Iteration 9/25 | Loss: 0.00075350
Iteration 10/25 | Loss: 0.00075280
Iteration 11/25 | Loss: 0.00075274
Iteration 12/25 | Loss: 0.00075274
Iteration 13/25 | Loss: 0.00075274
Iteration 14/25 | Loss: 0.00075274
Iteration 15/25 | Loss: 0.00075274
Iteration 16/25 | Loss: 0.00075274
Iteration 17/25 | Loss: 0.00075274
Iteration 18/25 | Loss: 0.00075273
Iteration 19/25 | Loss: 0.00075273
Iteration 20/25 | Loss: 0.00075273
Iteration 21/25 | Loss: 0.00075273
Iteration 22/25 | Loss: 0.00075273
Iteration 23/25 | Loss: 0.00075273
Iteration 24/25 | Loss: 0.00075273
Iteration 25/25 | Loss: 0.00075273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36215293
Iteration 2/25 | Loss: 0.00017760
Iteration 3/25 | Loss: 0.00017757
Iteration 4/25 | Loss: 0.00017757
Iteration 5/25 | Loss: 0.00017757
Iteration 6/25 | Loss: 0.00017757
Iteration 7/25 | Loss: 0.00017757
Iteration 8/25 | Loss: 0.00017757
Iteration 9/25 | Loss: 0.00017757
Iteration 10/25 | Loss: 0.00017757
Iteration 11/25 | Loss: 0.00017757
Iteration 12/25 | Loss: 0.00017757
Iteration 13/25 | Loss: 0.00017757
Iteration 14/25 | Loss: 0.00017757
Iteration 15/25 | Loss: 0.00017757
Iteration 16/25 | Loss: 0.00017757
Iteration 17/25 | Loss: 0.00017757
Iteration 18/25 | Loss: 0.00017757
Iteration 19/25 | Loss: 0.00017757
Iteration 20/25 | Loss: 0.00017757
Iteration 21/25 | Loss: 0.00017757
Iteration 22/25 | Loss: 0.00017757
Iteration 23/25 | Loss: 0.00017757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00017756664601620287, 0.00017756664601620287, 0.00017756664601620287, 0.00017756664601620287, 0.00017756664601620287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017756664601620287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017757
Iteration 2/1000 | Loss: 0.00005451
Iteration 3/1000 | Loss: 0.00004362
Iteration 4/1000 | Loss: 0.00003650
Iteration 5/1000 | Loss: 0.00003444
Iteration 6/1000 | Loss: 0.00003333
Iteration 7/1000 | Loss: 0.00003229
Iteration 8/1000 | Loss: 0.00003143
Iteration 9/1000 | Loss: 0.00003078
Iteration 10/1000 | Loss: 0.00003038
Iteration 11/1000 | Loss: 0.00003008
Iteration 12/1000 | Loss: 0.00002980
Iteration 13/1000 | Loss: 0.00002957
Iteration 14/1000 | Loss: 0.00002956
Iteration 15/1000 | Loss: 0.00002939
Iteration 16/1000 | Loss: 0.00002930
Iteration 17/1000 | Loss: 0.00002918
Iteration 18/1000 | Loss: 0.00002916
Iteration 19/1000 | Loss: 0.00002910
Iteration 20/1000 | Loss: 0.00002908
Iteration 21/1000 | Loss: 0.00002907
Iteration 22/1000 | Loss: 0.00002906
Iteration 23/1000 | Loss: 0.00002906
Iteration 24/1000 | Loss: 0.00002906
Iteration 25/1000 | Loss: 0.00002905
Iteration 26/1000 | Loss: 0.00002905
Iteration 27/1000 | Loss: 0.00002904
Iteration 28/1000 | Loss: 0.00002904
Iteration 29/1000 | Loss: 0.00002903
Iteration 30/1000 | Loss: 0.00002903
Iteration 31/1000 | Loss: 0.00002903
Iteration 32/1000 | Loss: 0.00002903
Iteration 33/1000 | Loss: 0.00002903
Iteration 34/1000 | Loss: 0.00002903
Iteration 35/1000 | Loss: 0.00002902
Iteration 36/1000 | Loss: 0.00002902
Iteration 37/1000 | Loss: 0.00002902
Iteration 38/1000 | Loss: 0.00002902
Iteration 39/1000 | Loss: 0.00002902
Iteration 40/1000 | Loss: 0.00002902
Iteration 41/1000 | Loss: 0.00002901
Iteration 42/1000 | Loss: 0.00002901
Iteration 43/1000 | Loss: 0.00002900
Iteration 44/1000 | Loss: 0.00002900
Iteration 45/1000 | Loss: 0.00002900
Iteration 46/1000 | Loss: 0.00002899
Iteration 47/1000 | Loss: 0.00002899
Iteration 48/1000 | Loss: 0.00002899
Iteration 49/1000 | Loss: 0.00002899
Iteration 50/1000 | Loss: 0.00002899
Iteration 51/1000 | Loss: 0.00002898
Iteration 52/1000 | Loss: 0.00002898
Iteration 53/1000 | Loss: 0.00002898
Iteration 54/1000 | Loss: 0.00002897
Iteration 55/1000 | Loss: 0.00002897
Iteration 56/1000 | Loss: 0.00002896
Iteration 57/1000 | Loss: 0.00002896
Iteration 58/1000 | Loss: 0.00002896
Iteration 59/1000 | Loss: 0.00002896
Iteration 60/1000 | Loss: 0.00002896
Iteration 61/1000 | Loss: 0.00002895
Iteration 62/1000 | Loss: 0.00002895
Iteration 63/1000 | Loss: 0.00002895
Iteration 64/1000 | Loss: 0.00002894
Iteration 65/1000 | Loss: 0.00002894
Iteration 66/1000 | Loss: 0.00002894
Iteration 67/1000 | Loss: 0.00002894
Iteration 68/1000 | Loss: 0.00002893
Iteration 69/1000 | Loss: 0.00002893
Iteration 70/1000 | Loss: 0.00002893
Iteration 71/1000 | Loss: 0.00002893
Iteration 72/1000 | Loss: 0.00002893
Iteration 73/1000 | Loss: 0.00002893
Iteration 74/1000 | Loss: 0.00002892
Iteration 75/1000 | Loss: 0.00002892
Iteration 76/1000 | Loss: 0.00002892
Iteration 77/1000 | Loss: 0.00002892
Iteration 78/1000 | Loss: 0.00002892
Iteration 79/1000 | Loss: 0.00002891
Iteration 80/1000 | Loss: 0.00002891
Iteration 81/1000 | Loss: 0.00002891
Iteration 82/1000 | Loss: 0.00002891
Iteration 83/1000 | Loss: 0.00002891
Iteration 84/1000 | Loss: 0.00002891
Iteration 85/1000 | Loss: 0.00002891
Iteration 86/1000 | Loss: 0.00002891
Iteration 87/1000 | Loss: 0.00002891
Iteration 88/1000 | Loss: 0.00002890
Iteration 89/1000 | Loss: 0.00002890
Iteration 90/1000 | Loss: 0.00002890
Iteration 91/1000 | Loss: 0.00002889
Iteration 92/1000 | Loss: 0.00002889
Iteration 93/1000 | Loss: 0.00002889
Iteration 94/1000 | Loss: 0.00002889
Iteration 95/1000 | Loss: 0.00002888
Iteration 96/1000 | Loss: 0.00002888
Iteration 97/1000 | Loss: 0.00002888
Iteration 98/1000 | Loss: 0.00002888
Iteration 99/1000 | Loss: 0.00002887
Iteration 100/1000 | Loss: 0.00002887
Iteration 101/1000 | Loss: 0.00002887
Iteration 102/1000 | Loss: 0.00002886
Iteration 103/1000 | Loss: 0.00002886
Iteration 104/1000 | Loss: 0.00002886
Iteration 105/1000 | Loss: 0.00002886
Iteration 106/1000 | Loss: 0.00002886
Iteration 107/1000 | Loss: 0.00002885
Iteration 108/1000 | Loss: 0.00002885
Iteration 109/1000 | Loss: 0.00002885
Iteration 110/1000 | Loss: 0.00002885
Iteration 111/1000 | Loss: 0.00002885
Iteration 112/1000 | Loss: 0.00002885
Iteration 113/1000 | Loss: 0.00002885
Iteration 114/1000 | Loss: 0.00002885
Iteration 115/1000 | Loss: 0.00002884
Iteration 116/1000 | Loss: 0.00002884
Iteration 117/1000 | Loss: 0.00002884
Iteration 118/1000 | Loss: 0.00002884
Iteration 119/1000 | Loss: 0.00002884
Iteration 120/1000 | Loss: 0.00002884
Iteration 121/1000 | Loss: 0.00002884
Iteration 122/1000 | Loss: 0.00002884
Iteration 123/1000 | Loss: 0.00002884
Iteration 124/1000 | Loss: 0.00002884
Iteration 125/1000 | Loss: 0.00002884
Iteration 126/1000 | Loss: 0.00002883
Iteration 127/1000 | Loss: 0.00002883
Iteration 128/1000 | Loss: 0.00002883
Iteration 129/1000 | Loss: 0.00002883
Iteration 130/1000 | Loss: 0.00002883
Iteration 131/1000 | Loss: 0.00002882
Iteration 132/1000 | Loss: 0.00002882
Iteration 133/1000 | Loss: 0.00002882
Iteration 134/1000 | Loss: 0.00002882
Iteration 135/1000 | Loss: 0.00002882
Iteration 136/1000 | Loss: 0.00002882
Iteration 137/1000 | Loss: 0.00002882
Iteration 138/1000 | Loss: 0.00002882
Iteration 139/1000 | Loss: 0.00002882
Iteration 140/1000 | Loss: 0.00002881
Iteration 141/1000 | Loss: 0.00002881
Iteration 142/1000 | Loss: 0.00002881
Iteration 143/1000 | Loss: 0.00002881
Iteration 144/1000 | Loss: 0.00002880
Iteration 145/1000 | Loss: 0.00002880
Iteration 146/1000 | Loss: 0.00002880
Iteration 147/1000 | Loss: 0.00002880
Iteration 148/1000 | Loss: 0.00002880
Iteration 149/1000 | Loss: 0.00002880
Iteration 150/1000 | Loss: 0.00002880
Iteration 151/1000 | Loss: 0.00002880
Iteration 152/1000 | Loss: 0.00002880
Iteration 153/1000 | Loss: 0.00002880
Iteration 154/1000 | Loss: 0.00002879
Iteration 155/1000 | Loss: 0.00002879
Iteration 156/1000 | Loss: 0.00002879
Iteration 157/1000 | Loss: 0.00002879
Iteration 158/1000 | Loss: 0.00002879
Iteration 159/1000 | Loss: 0.00002879
Iteration 160/1000 | Loss: 0.00002879
Iteration 161/1000 | Loss: 0.00002879
Iteration 162/1000 | Loss: 0.00002879
Iteration 163/1000 | Loss: 0.00002879
Iteration 164/1000 | Loss: 0.00002879
Iteration 165/1000 | Loss: 0.00002879
Iteration 166/1000 | Loss: 0.00002878
Iteration 167/1000 | Loss: 0.00002878
Iteration 168/1000 | Loss: 0.00002878
Iteration 169/1000 | Loss: 0.00002878
Iteration 170/1000 | Loss: 0.00002878
Iteration 171/1000 | Loss: 0.00002878
Iteration 172/1000 | Loss: 0.00002878
Iteration 173/1000 | Loss: 0.00002878
Iteration 174/1000 | Loss: 0.00002878
Iteration 175/1000 | Loss: 0.00002878
Iteration 176/1000 | Loss: 0.00002878
Iteration 177/1000 | Loss: 0.00002878
Iteration 178/1000 | Loss: 0.00002877
Iteration 179/1000 | Loss: 0.00002877
Iteration 180/1000 | Loss: 0.00002877
Iteration 181/1000 | Loss: 0.00002877
Iteration 182/1000 | Loss: 0.00002877
Iteration 183/1000 | Loss: 0.00002877
Iteration 184/1000 | Loss: 0.00002877
Iteration 185/1000 | Loss: 0.00002877
Iteration 186/1000 | Loss: 0.00002877
Iteration 187/1000 | Loss: 0.00002877
Iteration 188/1000 | Loss: 0.00002877
Iteration 189/1000 | Loss: 0.00002877
Iteration 190/1000 | Loss: 0.00002877
Iteration 191/1000 | Loss: 0.00002876
Iteration 192/1000 | Loss: 0.00002876
Iteration 193/1000 | Loss: 0.00002876
Iteration 194/1000 | Loss: 0.00002876
Iteration 195/1000 | Loss: 0.00002876
Iteration 196/1000 | Loss: 0.00002876
Iteration 197/1000 | Loss: 0.00002876
Iteration 198/1000 | Loss: 0.00002876
Iteration 199/1000 | Loss: 0.00002876
Iteration 200/1000 | Loss: 0.00002876
Iteration 201/1000 | Loss: 0.00002876
Iteration 202/1000 | Loss: 0.00002876
Iteration 203/1000 | Loss: 0.00002876
Iteration 204/1000 | Loss: 0.00002876
Iteration 205/1000 | Loss: 0.00002876
Iteration 206/1000 | Loss: 0.00002876
Iteration 207/1000 | Loss: 0.00002876
Iteration 208/1000 | Loss: 0.00002876
Iteration 209/1000 | Loss: 0.00002876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.876132566598244e-05, 2.876132566598244e-05, 2.876132566598244e-05, 2.876132566598244e-05, 2.876132566598244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.876132566598244e-05

Optimization complete. Final v2v error: 4.392776966094971 mm

Highest mean error: 5.1289777755737305 mm for frame 126

Lowest mean error: 3.7458832263946533 mm for frame 11

Saving results

Total time: 53.570422410964966
