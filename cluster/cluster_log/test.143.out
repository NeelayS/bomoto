Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=143, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8008-8063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967117
Iteration 2/25 | Loss: 0.00152908
Iteration 3/25 | Loss: 0.00105737
Iteration 4/25 | Loss: 0.00098921
Iteration 5/25 | Loss: 0.00097794
Iteration 6/25 | Loss: 0.00097561
Iteration 7/25 | Loss: 0.00097544
Iteration 8/25 | Loss: 0.00097544
Iteration 9/25 | Loss: 0.00097544
Iteration 10/25 | Loss: 0.00097544
Iteration 11/25 | Loss: 0.00097544
Iteration 12/25 | Loss: 0.00097544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009754447382874787, 0.0009754447382874787, 0.0009754447382874787, 0.0009754447382874787, 0.0009754447382874787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009754447382874787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81085724
Iteration 2/25 | Loss: 0.00046935
Iteration 3/25 | Loss: 0.00046934
Iteration 4/25 | Loss: 0.00046934
Iteration 5/25 | Loss: 0.00046934
Iteration 6/25 | Loss: 0.00046934
Iteration 7/25 | Loss: 0.00046934
Iteration 8/25 | Loss: 0.00046934
Iteration 9/25 | Loss: 0.00046934
Iteration 10/25 | Loss: 0.00046934
Iteration 11/25 | Loss: 0.00046934
Iteration 12/25 | Loss: 0.00046934
Iteration 13/25 | Loss: 0.00046934
Iteration 14/25 | Loss: 0.00046934
Iteration 15/25 | Loss: 0.00046934
Iteration 16/25 | Loss: 0.00046934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00046934204874560237, 0.00046934204874560237, 0.00046934204874560237, 0.00046934204874560237, 0.00046934204874560237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046934204874560237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046934
Iteration 2/1000 | Loss: 0.00005857
Iteration 3/1000 | Loss: 0.00004770
Iteration 4/1000 | Loss: 0.00004397
Iteration 5/1000 | Loss: 0.00004197
Iteration 6/1000 | Loss: 0.00004061
Iteration 7/1000 | Loss: 0.00003951
Iteration 8/1000 | Loss: 0.00003910
Iteration 9/1000 | Loss: 0.00003873
Iteration 10/1000 | Loss: 0.00003852
Iteration 11/1000 | Loss: 0.00003832
Iteration 12/1000 | Loss: 0.00003816
Iteration 13/1000 | Loss: 0.00003812
Iteration 14/1000 | Loss: 0.00003812
Iteration 15/1000 | Loss: 0.00003805
Iteration 16/1000 | Loss: 0.00003796
Iteration 17/1000 | Loss: 0.00003793
Iteration 18/1000 | Loss: 0.00003793
Iteration 19/1000 | Loss: 0.00003793
Iteration 20/1000 | Loss: 0.00003793
Iteration 21/1000 | Loss: 0.00003793
Iteration 22/1000 | Loss: 0.00003793
Iteration 23/1000 | Loss: 0.00003793
Iteration 24/1000 | Loss: 0.00003792
Iteration 25/1000 | Loss: 0.00003792
Iteration 26/1000 | Loss: 0.00003792
Iteration 27/1000 | Loss: 0.00003792
Iteration 28/1000 | Loss: 0.00003792
Iteration 29/1000 | Loss: 0.00003792
Iteration 30/1000 | Loss: 0.00003792
Iteration 31/1000 | Loss: 0.00003792
Iteration 32/1000 | Loss: 0.00003792
Iteration 33/1000 | Loss: 0.00003791
Iteration 34/1000 | Loss: 0.00003791
Iteration 35/1000 | Loss: 0.00003791
Iteration 36/1000 | Loss: 0.00003791
Iteration 37/1000 | Loss: 0.00003787
Iteration 38/1000 | Loss: 0.00003787
Iteration 39/1000 | Loss: 0.00003787
Iteration 40/1000 | Loss: 0.00003787
Iteration 41/1000 | Loss: 0.00003787
Iteration 42/1000 | Loss: 0.00003787
Iteration 43/1000 | Loss: 0.00003786
Iteration 44/1000 | Loss: 0.00003786
Iteration 45/1000 | Loss: 0.00003784
Iteration 46/1000 | Loss: 0.00003784
Iteration 47/1000 | Loss: 0.00003784
Iteration 48/1000 | Loss: 0.00003784
Iteration 49/1000 | Loss: 0.00003784
Iteration 50/1000 | Loss: 0.00003784
Iteration 51/1000 | Loss: 0.00003784
Iteration 52/1000 | Loss: 0.00003784
Iteration 53/1000 | Loss: 0.00003784
Iteration 54/1000 | Loss: 0.00003784
Iteration 55/1000 | Loss: 0.00003783
Iteration 56/1000 | Loss: 0.00003783
Iteration 57/1000 | Loss: 0.00003783
Iteration 58/1000 | Loss: 0.00003783
Iteration 59/1000 | Loss: 0.00003783
Iteration 60/1000 | Loss: 0.00003783
Iteration 61/1000 | Loss: 0.00003783
Iteration 62/1000 | Loss: 0.00003782
Iteration 63/1000 | Loss: 0.00003782
Iteration 64/1000 | Loss: 0.00003782
Iteration 65/1000 | Loss: 0.00003782
Iteration 66/1000 | Loss: 0.00003782
Iteration 67/1000 | Loss: 0.00003782
Iteration 68/1000 | Loss: 0.00003782
Iteration 69/1000 | Loss: 0.00003782
Iteration 70/1000 | Loss: 0.00003782
Iteration 71/1000 | Loss: 0.00003781
Iteration 72/1000 | Loss: 0.00003781
Iteration 73/1000 | Loss: 0.00003780
Iteration 74/1000 | Loss: 0.00003780
Iteration 75/1000 | Loss: 0.00003780
Iteration 76/1000 | Loss: 0.00003780
Iteration 77/1000 | Loss: 0.00003780
Iteration 78/1000 | Loss: 0.00003779
Iteration 79/1000 | Loss: 0.00003779
Iteration 80/1000 | Loss: 0.00003779
Iteration 81/1000 | Loss: 0.00003779
Iteration 82/1000 | Loss: 0.00003779
Iteration 83/1000 | Loss: 0.00003778
Iteration 84/1000 | Loss: 0.00003778
Iteration 85/1000 | Loss: 0.00003778
Iteration 86/1000 | Loss: 0.00003778
Iteration 87/1000 | Loss: 0.00003778
Iteration 88/1000 | Loss: 0.00003778
Iteration 89/1000 | Loss: 0.00003778
Iteration 90/1000 | Loss: 0.00003778
Iteration 91/1000 | Loss: 0.00003778
Iteration 92/1000 | Loss: 0.00003777
Iteration 93/1000 | Loss: 0.00003777
Iteration 94/1000 | Loss: 0.00003777
Iteration 95/1000 | Loss: 0.00003777
Iteration 96/1000 | Loss: 0.00003777
Iteration 97/1000 | Loss: 0.00003776
Iteration 98/1000 | Loss: 0.00003776
Iteration 99/1000 | Loss: 0.00003776
Iteration 100/1000 | Loss: 0.00003776
Iteration 101/1000 | Loss: 0.00003775
Iteration 102/1000 | Loss: 0.00003773
Iteration 103/1000 | Loss: 0.00003773
Iteration 104/1000 | Loss: 0.00003773
Iteration 105/1000 | Loss: 0.00003773
Iteration 106/1000 | Loss: 0.00003773
Iteration 107/1000 | Loss: 0.00003772
Iteration 108/1000 | Loss: 0.00003772
Iteration 109/1000 | Loss: 0.00003772
Iteration 110/1000 | Loss: 0.00003772
Iteration 111/1000 | Loss: 0.00003770
Iteration 112/1000 | Loss: 0.00003770
Iteration 113/1000 | Loss: 0.00003770
Iteration 114/1000 | Loss: 0.00003770
Iteration 115/1000 | Loss: 0.00003770
Iteration 116/1000 | Loss: 0.00003770
Iteration 117/1000 | Loss: 0.00003769
Iteration 118/1000 | Loss: 0.00003769
Iteration 119/1000 | Loss: 0.00003769
Iteration 120/1000 | Loss: 0.00003769
Iteration 121/1000 | Loss: 0.00003769
Iteration 122/1000 | Loss: 0.00003769
Iteration 123/1000 | Loss: 0.00003768
Iteration 124/1000 | Loss: 0.00003768
Iteration 125/1000 | Loss: 0.00003768
Iteration 126/1000 | Loss: 0.00003768
Iteration 127/1000 | Loss: 0.00003768
Iteration 128/1000 | Loss: 0.00003768
Iteration 129/1000 | Loss: 0.00003768
Iteration 130/1000 | Loss: 0.00003768
Iteration 131/1000 | Loss: 0.00003767
Iteration 132/1000 | Loss: 0.00003767
Iteration 133/1000 | Loss: 0.00003767
Iteration 134/1000 | Loss: 0.00003767
Iteration 135/1000 | Loss: 0.00003766
Iteration 136/1000 | Loss: 0.00003765
Iteration 137/1000 | Loss: 0.00003765
Iteration 138/1000 | Loss: 0.00003765
Iteration 139/1000 | Loss: 0.00003765
Iteration 140/1000 | Loss: 0.00003765
Iteration 141/1000 | Loss: 0.00003765
Iteration 142/1000 | Loss: 0.00003765
Iteration 143/1000 | Loss: 0.00003765
Iteration 144/1000 | Loss: 0.00003765
Iteration 145/1000 | Loss: 0.00003765
Iteration 146/1000 | Loss: 0.00003765
Iteration 147/1000 | Loss: 0.00003764
Iteration 148/1000 | Loss: 0.00003764
Iteration 149/1000 | Loss: 0.00003764
Iteration 150/1000 | Loss: 0.00003764
Iteration 151/1000 | Loss: 0.00003764
Iteration 152/1000 | Loss: 0.00003764
Iteration 153/1000 | Loss: 0.00003764
Iteration 154/1000 | Loss: 0.00003764
Iteration 155/1000 | Loss: 0.00003764
Iteration 156/1000 | Loss: 0.00003764
Iteration 157/1000 | Loss: 0.00003764
Iteration 158/1000 | Loss: 0.00003764
Iteration 159/1000 | Loss: 0.00003764
Iteration 160/1000 | Loss: 0.00003764
Iteration 161/1000 | Loss: 0.00003764
Iteration 162/1000 | Loss: 0.00003764
Iteration 163/1000 | Loss: 0.00003764
Iteration 164/1000 | Loss: 0.00003763
Iteration 165/1000 | Loss: 0.00003763
Iteration 166/1000 | Loss: 0.00003763
Iteration 167/1000 | Loss: 0.00003763
Iteration 168/1000 | Loss: 0.00003763
Iteration 169/1000 | Loss: 0.00003763
Iteration 170/1000 | Loss: 0.00003763
Iteration 171/1000 | Loss: 0.00003763
Iteration 172/1000 | Loss: 0.00003763
Iteration 173/1000 | Loss: 0.00003763
Iteration 174/1000 | Loss: 0.00003763
Iteration 175/1000 | Loss: 0.00003763
Iteration 176/1000 | Loss: 0.00003763
Iteration 177/1000 | Loss: 0.00003763
Iteration 178/1000 | Loss: 0.00003763
Iteration 179/1000 | Loss: 0.00003763
Iteration 180/1000 | Loss: 0.00003763
Iteration 181/1000 | Loss: 0.00003763
Iteration 182/1000 | Loss: 0.00003763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [3.7630081351380795e-05, 3.7630081351380795e-05, 3.7630081351380795e-05, 3.7630081351380795e-05, 3.7630081351380795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7630081351380795e-05

Optimization complete. Final v2v error: 5.191126823425293 mm

Highest mean error: 5.480072021484375 mm for frame 123

Lowest mean error: 4.478576183319092 mm for frame 2

Saving results

Total time: 41.915462255477905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079690
Iteration 2/25 | Loss: 0.00272705
Iteration 3/25 | Loss: 0.00183275
Iteration 4/25 | Loss: 0.00169393
Iteration 5/25 | Loss: 0.00141096
Iteration 6/25 | Loss: 0.00147417
Iteration 7/25 | Loss: 0.00118391
Iteration 8/25 | Loss: 0.00123607
Iteration 9/25 | Loss: 0.00112615
Iteration 10/25 | Loss: 0.00095102
Iteration 11/25 | Loss: 0.00100790
Iteration 12/25 | Loss: 0.00089417
Iteration 13/25 | Loss: 0.00093815
Iteration 14/25 | Loss: 0.00087854
Iteration 15/25 | Loss: 0.00086446
Iteration 16/25 | Loss: 0.00088149
Iteration 17/25 | Loss: 0.00084799
Iteration 18/25 | Loss: 0.00084669
Iteration 19/25 | Loss: 0.00085634
Iteration 20/25 | Loss: 0.00085309
Iteration 21/25 | Loss: 0.00085733
Iteration 22/25 | Loss: 0.00082725
Iteration 23/25 | Loss: 0.00082606
Iteration 24/25 | Loss: 0.00085092
Iteration 25/25 | Loss: 0.00087419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51193678
Iteration 2/25 | Loss: 0.00411230
Iteration 3/25 | Loss: 0.00162058
Iteration 4/25 | Loss: 0.00134438
Iteration 5/25 | Loss: 0.00134100
Iteration 6/25 | Loss: 0.00134100
Iteration 7/25 | Loss: 0.00134100
Iteration 8/25 | Loss: 0.00134100
Iteration 9/25 | Loss: 0.00134100
Iteration 10/25 | Loss: 0.00134100
Iteration 11/25 | Loss: 0.00134100
Iteration 12/25 | Loss: 0.00134100
Iteration 13/25 | Loss: 0.00134100
Iteration 14/25 | Loss: 0.00134100
Iteration 15/25 | Loss: 0.00134100
Iteration 16/25 | Loss: 0.00134100
Iteration 17/25 | Loss: 0.00134100
Iteration 18/25 | Loss: 0.00134100
Iteration 19/25 | Loss: 0.00134100
Iteration 20/25 | Loss: 0.00134100
Iteration 21/25 | Loss: 0.00134100
Iteration 22/25 | Loss: 0.00134100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013409957755357027, 0.0013409957755357027, 0.0013409957755357027, 0.0013409957755357027, 0.0013409957755357027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013409957755357027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134100
Iteration 2/1000 | Loss: 0.00150038
Iteration 3/1000 | Loss: 0.00258529
Iteration 4/1000 | Loss: 0.00149111
Iteration 5/1000 | Loss: 0.00048565
Iteration 6/1000 | Loss: 0.00014072
Iteration 7/1000 | Loss: 0.00120411
Iteration 8/1000 | Loss: 0.00068379
Iteration 9/1000 | Loss: 0.00049110
Iteration 10/1000 | Loss: 0.00044931
Iteration 11/1000 | Loss: 0.00006940
Iteration 12/1000 | Loss: 0.00005283
Iteration 13/1000 | Loss: 0.00004570
Iteration 14/1000 | Loss: 0.00004303
Iteration 15/1000 | Loss: 0.00004055
Iteration 16/1000 | Loss: 0.00015775
Iteration 17/1000 | Loss: 0.00003897
Iteration 18/1000 | Loss: 0.00076341
Iteration 19/1000 | Loss: 0.00066340
Iteration 20/1000 | Loss: 0.00070543
Iteration 21/1000 | Loss: 0.00013664
Iteration 22/1000 | Loss: 0.00006493
Iteration 23/1000 | Loss: 0.00081846
Iteration 24/1000 | Loss: 0.00006563
Iteration 25/1000 | Loss: 0.00004244
Iteration 26/1000 | Loss: 0.00039354
Iteration 27/1000 | Loss: 0.00090849
Iteration 28/1000 | Loss: 0.00112073
Iteration 29/1000 | Loss: 0.00008075
Iteration 30/1000 | Loss: 0.00004891
Iteration 31/1000 | Loss: 0.00083831
Iteration 32/1000 | Loss: 0.00039398
Iteration 33/1000 | Loss: 0.00078217
Iteration 34/1000 | Loss: 0.00054124
Iteration 35/1000 | Loss: 0.00034888
Iteration 36/1000 | Loss: 0.00061266
Iteration 37/1000 | Loss: 0.00026907
Iteration 38/1000 | Loss: 0.00091729
Iteration 39/1000 | Loss: 0.00045310
Iteration 40/1000 | Loss: 0.00028100
Iteration 41/1000 | Loss: 0.00007111
Iteration 42/1000 | Loss: 0.00005817
Iteration 43/1000 | Loss: 0.00112222
Iteration 44/1000 | Loss: 0.00034731
Iteration 45/1000 | Loss: 0.00104257
Iteration 46/1000 | Loss: 0.00059800
Iteration 47/1000 | Loss: 0.00009194
Iteration 48/1000 | Loss: 0.00082209
Iteration 49/1000 | Loss: 0.00042521
Iteration 50/1000 | Loss: 0.00007215
Iteration 51/1000 | Loss: 0.00069264
Iteration 52/1000 | Loss: 0.00030229
Iteration 53/1000 | Loss: 0.00030906
Iteration 54/1000 | Loss: 0.00004560
Iteration 55/1000 | Loss: 0.00016620
Iteration 56/1000 | Loss: 0.00055338
Iteration 57/1000 | Loss: 0.00021373
Iteration 58/1000 | Loss: 0.00069720
Iteration 59/1000 | Loss: 0.00048260
Iteration 60/1000 | Loss: 0.00063225
Iteration 61/1000 | Loss: 0.00027530
Iteration 62/1000 | Loss: 0.00017609
Iteration 63/1000 | Loss: 0.00017804
Iteration 64/1000 | Loss: 0.00010817
Iteration 65/1000 | Loss: 0.00021173
Iteration 66/1000 | Loss: 0.00016354
Iteration 67/1000 | Loss: 0.00017301
Iteration 68/1000 | Loss: 0.00015575
Iteration 69/1000 | Loss: 0.00005045
Iteration 70/1000 | Loss: 0.00010898
Iteration 71/1000 | Loss: 0.00008379
Iteration 72/1000 | Loss: 0.00043195
Iteration 73/1000 | Loss: 0.00012385
Iteration 74/1000 | Loss: 0.00021494
Iteration 75/1000 | Loss: 0.00017718
Iteration 76/1000 | Loss: 0.00019314
Iteration 77/1000 | Loss: 0.00014013
Iteration 78/1000 | Loss: 0.00022647
Iteration 79/1000 | Loss: 0.00010381
Iteration 80/1000 | Loss: 0.00009558
Iteration 81/1000 | Loss: 0.00020306
Iteration 82/1000 | Loss: 0.00013861
Iteration 83/1000 | Loss: 0.00026715
Iteration 84/1000 | Loss: 0.00012761
Iteration 85/1000 | Loss: 0.00019931
Iteration 86/1000 | Loss: 0.00012134
Iteration 87/1000 | Loss: 0.00011670
Iteration 88/1000 | Loss: 0.00015038
Iteration 89/1000 | Loss: 0.00020533
Iteration 90/1000 | Loss: 0.00042009
Iteration 91/1000 | Loss: 0.00021199
Iteration 92/1000 | Loss: 0.00017337
Iteration 93/1000 | Loss: 0.00024217
Iteration 94/1000 | Loss: 0.00032072
Iteration 95/1000 | Loss: 0.00018970
Iteration 96/1000 | Loss: 0.00003987
Iteration 97/1000 | Loss: 0.00021316
Iteration 98/1000 | Loss: 0.00014052
Iteration 99/1000 | Loss: 0.00012425
Iteration 100/1000 | Loss: 0.00014499
Iteration 101/1000 | Loss: 0.00012193
Iteration 102/1000 | Loss: 0.00011270
Iteration 103/1000 | Loss: 0.00012686
Iteration 104/1000 | Loss: 0.00017092
Iteration 105/1000 | Loss: 0.00022714
Iteration 106/1000 | Loss: 0.00004225
Iteration 107/1000 | Loss: 0.00003587
Iteration 108/1000 | Loss: 0.00011635
Iteration 109/1000 | Loss: 0.00006908
Iteration 110/1000 | Loss: 0.00003408
Iteration 111/1000 | Loss: 0.00023367
Iteration 112/1000 | Loss: 0.00004675
Iteration 113/1000 | Loss: 0.00004195
Iteration 114/1000 | Loss: 0.00003334
Iteration 115/1000 | Loss: 0.00003302
Iteration 116/1000 | Loss: 0.00003274
Iteration 117/1000 | Loss: 0.00003271
Iteration 118/1000 | Loss: 0.00003258
Iteration 119/1000 | Loss: 0.00003254
Iteration 120/1000 | Loss: 0.00003254
Iteration 121/1000 | Loss: 0.00003249
Iteration 122/1000 | Loss: 0.00003241
Iteration 123/1000 | Loss: 0.00003220
Iteration 124/1000 | Loss: 0.00003201
Iteration 125/1000 | Loss: 0.00003186
Iteration 126/1000 | Loss: 0.00003182
Iteration 127/1000 | Loss: 0.00003178
Iteration 128/1000 | Loss: 0.00003178
Iteration 129/1000 | Loss: 0.00013691
Iteration 130/1000 | Loss: 0.00003189
Iteration 131/1000 | Loss: 0.00003178
Iteration 132/1000 | Loss: 0.00003176
Iteration 133/1000 | Loss: 0.00003170
Iteration 134/1000 | Loss: 0.00003170
Iteration 135/1000 | Loss: 0.00003170
Iteration 136/1000 | Loss: 0.00003170
Iteration 137/1000 | Loss: 0.00003170
Iteration 138/1000 | Loss: 0.00003170
Iteration 139/1000 | Loss: 0.00003170
Iteration 140/1000 | Loss: 0.00003170
Iteration 141/1000 | Loss: 0.00003170
Iteration 142/1000 | Loss: 0.00003170
Iteration 143/1000 | Loss: 0.00003170
Iteration 144/1000 | Loss: 0.00003170
Iteration 145/1000 | Loss: 0.00003169
Iteration 146/1000 | Loss: 0.00003169
Iteration 147/1000 | Loss: 0.00003169
Iteration 148/1000 | Loss: 0.00003169
Iteration 149/1000 | Loss: 0.00003168
Iteration 150/1000 | Loss: 0.00003168
Iteration 151/1000 | Loss: 0.00003168
Iteration 152/1000 | Loss: 0.00003167
Iteration 153/1000 | Loss: 0.00003167
Iteration 154/1000 | Loss: 0.00003167
Iteration 155/1000 | Loss: 0.00003166
Iteration 156/1000 | Loss: 0.00003166
Iteration 157/1000 | Loss: 0.00003166
Iteration 158/1000 | Loss: 0.00003166
Iteration 159/1000 | Loss: 0.00003166
Iteration 160/1000 | Loss: 0.00003165
Iteration 161/1000 | Loss: 0.00003165
Iteration 162/1000 | Loss: 0.00003165
Iteration 163/1000 | Loss: 0.00003165
Iteration 164/1000 | Loss: 0.00003165
Iteration 165/1000 | Loss: 0.00003164
Iteration 166/1000 | Loss: 0.00003164
Iteration 167/1000 | Loss: 0.00003163
Iteration 168/1000 | Loss: 0.00003163
Iteration 169/1000 | Loss: 0.00003163
Iteration 170/1000 | Loss: 0.00003163
Iteration 171/1000 | Loss: 0.00003163
Iteration 172/1000 | Loss: 0.00003163
Iteration 173/1000 | Loss: 0.00003163
Iteration 174/1000 | Loss: 0.00003163
Iteration 175/1000 | Loss: 0.00003163
Iteration 176/1000 | Loss: 0.00003163
Iteration 177/1000 | Loss: 0.00003163
Iteration 178/1000 | Loss: 0.00003163
Iteration 179/1000 | Loss: 0.00003162
Iteration 180/1000 | Loss: 0.00003162
Iteration 181/1000 | Loss: 0.00003162
Iteration 182/1000 | Loss: 0.00003162
Iteration 183/1000 | Loss: 0.00003162
Iteration 184/1000 | Loss: 0.00003162
Iteration 185/1000 | Loss: 0.00003162
Iteration 186/1000 | Loss: 0.00003162
Iteration 187/1000 | Loss: 0.00003162
Iteration 188/1000 | Loss: 0.00003162
Iteration 189/1000 | Loss: 0.00003162
Iteration 190/1000 | Loss: 0.00003162
Iteration 191/1000 | Loss: 0.00003162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [3.1621981179341674e-05, 3.1621981179341674e-05, 3.1621981179341674e-05, 3.1621981179341674e-05, 3.1621981179341674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1621981179341674e-05

Optimization complete. Final v2v error: 4.132506847381592 mm

Highest mean error: 11.517903327941895 mm for frame 25

Lowest mean error: 3.384326934814453 mm for frame 143

Saving results

Total time: 256.50316524505615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787843
Iteration 2/25 | Loss: 0.00096485
Iteration 3/25 | Loss: 0.00077889
Iteration 4/25 | Loss: 0.00073908
Iteration 5/25 | Loss: 0.00073295
Iteration 6/25 | Loss: 0.00073233
Iteration 7/25 | Loss: 0.00073233
Iteration 8/25 | Loss: 0.00073233
Iteration 9/25 | Loss: 0.00073233
Iteration 10/25 | Loss: 0.00073233
Iteration 11/25 | Loss: 0.00073233
Iteration 12/25 | Loss: 0.00073233
Iteration 13/25 | Loss: 0.00073233
Iteration 14/25 | Loss: 0.00073233
Iteration 15/25 | Loss: 0.00073233
Iteration 16/25 | Loss: 0.00073233
Iteration 17/25 | Loss: 0.00073233
Iteration 18/25 | Loss: 0.00073233
Iteration 19/25 | Loss: 0.00073233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007323261233977973, 0.0007323261233977973, 0.0007323261233977973, 0.0007323261233977973, 0.0007323261233977973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007323261233977973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44990921
Iteration 2/25 | Loss: 0.00052091
Iteration 3/25 | Loss: 0.00052088
Iteration 4/25 | Loss: 0.00052088
Iteration 5/25 | Loss: 0.00052088
Iteration 6/25 | Loss: 0.00052088
Iteration 7/25 | Loss: 0.00052088
Iteration 8/25 | Loss: 0.00052088
Iteration 9/25 | Loss: 0.00052088
Iteration 10/25 | Loss: 0.00052088
Iteration 11/25 | Loss: 0.00052088
Iteration 12/25 | Loss: 0.00052088
Iteration 13/25 | Loss: 0.00052088
Iteration 14/25 | Loss: 0.00052088
Iteration 15/25 | Loss: 0.00052088
Iteration 16/25 | Loss: 0.00052088
Iteration 17/25 | Loss: 0.00052088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005208809161558747, 0.0005208809161558747, 0.0005208809161558747, 0.0005208809161558747, 0.0005208809161558747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005208809161558747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052088
Iteration 2/1000 | Loss: 0.00005207
Iteration 3/1000 | Loss: 0.00003268
Iteration 4/1000 | Loss: 0.00002919
Iteration 5/1000 | Loss: 0.00002717
Iteration 6/1000 | Loss: 0.00002542
Iteration 7/1000 | Loss: 0.00002447
Iteration 8/1000 | Loss: 0.00002375
Iteration 9/1000 | Loss: 0.00002331
Iteration 10/1000 | Loss: 0.00002303
Iteration 11/1000 | Loss: 0.00002275
Iteration 12/1000 | Loss: 0.00002254
Iteration 13/1000 | Loss: 0.00002253
Iteration 14/1000 | Loss: 0.00002250
Iteration 15/1000 | Loss: 0.00002249
Iteration 16/1000 | Loss: 0.00002245
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002236
Iteration 19/1000 | Loss: 0.00002232
Iteration 20/1000 | Loss: 0.00002228
Iteration 21/1000 | Loss: 0.00002227
Iteration 22/1000 | Loss: 0.00002227
Iteration 23/1000 | Loss: 0.00002226
Iteration 24/1000 | Loss: 0.00002226
Iteration 25/1000 | Loss: 0.00002226
Iteration 26/1000 | Loss: 0.00002226
Iteration 27/1000 | Loss: 0.00002226
Iteration 28/1000 | Loss: 0.00002226
Iteration 29/1000 | Loss: 0.00002225
Iteration 30/1000 | Loss: 0.00002225
Iteration 31/1000 | Loss: 0.00002225
Iteration 32/1000 | Loss: 0.00002223
Iteration 33/1000 | Loss: 0.00002223
Iteration 34/1000 | Loss: 0.00002223
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002222
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002220
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002220
Iteration 41/1000 | Loss: 0.00002220
Iteration 42/1000 | Loss: 0.00002218
Iteration 43/1000 | Loss: 0.00002218
Iteration 44/1000 | Loss: 0.00002217
Iteration 45/1000 | Loss: 0.00002217
Iteration 46/1000 | Loss: 0.00002217
Iteration 47/1000 | Loss: 0.00002216
Iteration 48/1000 | Loss: 0.00002216
Iteration 49/1000 | Loss: 0.00002216
Iteration 50/1000 | Loss: 0.00002215
Iteration 51/1000 | Loss: 0.00002215
Iteration 52/1000 | Loss: 0.00002214
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00002213
Iteration 55/1000 | Loss: 0.00002213
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002213
Iteration 59/1000 | Loss: 0.00002213
Iteration 60/1000 | Loss: 0.00002213
Iteration 61/1000 | Loss: 0.00002213
Iteration 62/1000 | Loss: 0.00002213
Iteration 63/1000 | Loss: 0.00002212
Iteration 64/1000 | Loss: 0.00002212
Iteration 65/1000 | Loss: 0.00002212
Iteration 66/1000 | Loss: 0.00002212
Iteration 67/1000 | Loss: 0.00002211
Iteration 68/1000 | Loss: 0.00002211
Iteration 69/1000 | Loss: 0.00002211
Iteration 70/1000 | Loss: 0.00002210
Iteration 71/1000 | Loss: 0.00002210
Iteration 72/1000 | Loss: 0.00002209
Iteration 73/1000 | Loss: 0.00002209
Iteration 74/1000 | Loss: 0.00002209
Iteration 75/1000 | Loss: 0.00002209
Iteration 76/1000 | Loss: 0.00002209
Iteration 77/1000 | Loss: 0.00002208
Iteration 78/1000 | Loss: 0.00002208
Iteration 79/1000 | Loss: 0.00002208
Iteration 80/1000 | Loss: 0.00002208
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002207
Iteration 83/1000 | Loss: 0.00002207
Iteration 84/1000 | Loss: 0.00002207
Iteration 85/1000 | Loss: 0.00002206
Iteration 86/1000 | Loss: 0.00002206
Iteration 87/1000 | Loss: 0.00002206
Iteration 88/1000 | Loss: 0.00002206
Iteration 89/1000 | Loss: 0.00002206
Iteration 90/1000 | Loss: 0.00002206
Iteration 91/1000 | Loss: 0.00002206
Iteration 92/1000 | Loss: 0.00002206
Iteration 93/1000 | Loss: 0.00002206
Iteration 94/1000 | Loss: 0.00002206
Iteration 95/1000 | Loss: 0.00002206
Iteration 96/1000 | Loss: 0.00002206
Iteration 97/1000 | Loss: 0.00002205
Iteration 98/1000 | Loss: 0.00002205
Iteration 99/1000 | Loss: 0.00002205
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002205
Iteration 102/1000 | Loss: 0.00002205
Iteration 103/1000 | Loss: 0.00002205
Iteration 104/1000 | Loss: 0.00002205
Iteration 105/1000 | Loss: 0.00002205
Iteration 106/1000 | Loss: 0.00002205
Iteration 107/1000 | Loss: 0.00002205
Iteration 108/1000 | Loss: 0.00002205
Iteration 109/1000 | Loss: 0.00002205
Iteration 110/1000 | Loss: 0.00002205
Iteration 111/1000 | Loss: 0.00002205
Iteration 112/1000 | Loss: 0.00002204
Iteration 113/1000 | Loss: 0.00002204
Iteration 114/1000 | Loss: 0.00002204
Iteration 115/1000 | Loss: 0.00002204
Iteration 116/1000 | Loss: 0.00002203
Iteration 117/1000 | Loss: 0.00002203
Iteration 118/1000 | Loss: 0.00002203
Iteration 119/1000 | Loss: 0.00002203
Iteration 120/1000 | Loss: 0.00002203
Iteration 121/1000 | Loss: 0.00002203
Iteration 122/1000 | Loss: 0.00002203
Iteration 123/1000 | Loss: 0.00002203
Iteration 124/1000 | Loss: 0.00002203
Iteration 125/1000 | Loss: 0.00002203
Iteration 126/1000 | Loss: 0.00002203
Iteration 127/1000 | Loss: 0.00002203
Iteration 128/1000 | Loss: 0.00002203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.202663745265454e-05, 2.202663745265454e-05, 2.202663745265454e-05, 2.202663745265454e-05, 2.202663745265454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.202663745265454e-05

Optimization complete. Final v2v error: 3.9873011112213135 mm

Highest mean error: 4.734265327453613 mm for frame 212

Lowest mean error: 3.711646556854248 mm for frame 158

Saving results

Total time: 41.3155734539032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00530173
Iteration 2/25 | Loss: 0.00126308
Iteration 3/25 | Loss: 0.00082544
Iteration 4/25 | Loss: 0.00078037
Iteration 5/25 | Loss: 0.00076642
Iteration 6/25 | Loss: 0.00076331
Iteration 7/25 | Loss: 0.00076246
Iteration 8/25 | Loss: 0.00076234
Iteration 9/25 | Loss: 0.00076234
Iteration 10/25 | Loss: 0.00076234
Iteration 11/25 | Loss: 0.00076234
Iteration 12/25 | Loss: 0.00076234
Iteration 13/25 | Loss: 0.00076234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007623445126228034, 0.0007623445126228034, 0.0007623445126228034, 0.0007623445126228034, 0.0007623445126228034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007623445126228034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42894328
Iteration 2/25 | Loss: 0.00043361
Iteration 3/25 | Loss: 0.00043359
Iteration 4/25 | Loss: 0.00043359
Iteration 5/25 | Loss: 0.00043359
Iteration 6/25 | Loss: 0.00043359
Iteration 7/25 | Loss: 0.00043359
Iteration 8/25 | Loss: 0.00043359
Iteration 9/25 | Loss: 0.00043359
Iteration 10/25 | Loss: 0.00043359
Iteration 11/25 | Loss: 0.00043359
Iteration 12/25 | Loss: 0.00043359
Iteration 13/25 | Loss: 0.00043359
Iteration 14/25 | Loss: 0.00043359
Iteration 15/25 | Loss: 0.00043359
Iteration 16/25 | Loss: 0.00043359
Iteration 17/25 | Loss: 0.00043359
Iteration 18/25 | Loss: 0.00043359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004335891571827233, 0.0004335891571827233, 0.0004335891571827233, 0.0004335891571827233, 0.0004335891571827233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004335891571827233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043359
Iteration 2/1000 | Loss: 0.00005454
Iteration 3/1000 | Loss: 0.00003803
Iteration 4/1000 | Loss: 0.00003322
Iteration 5/1000 | Loss: 0.00003140
Iteration 6/1000 | Loss: 0.00002933
Iteration 7/1000 | Loss: 0.00002823
Iteration 8/1000 | Loss: 0.00002729
Iteration 9/1000 | Loss: 0.00002673
Iteration 10/1000 | Loss: 0.00002638
Iteration 11/1000 | Loss: 0.00002612
Iteration 12/1000 | Loss: 0.00002585
Iteration 13/1000 | Loss: 0.00002565
Iteration 14/1000 | Loss: 0.00002556
Iteration 15/1000 | Loss: 0.00002554
Iteration 16/1000 | Loss: 0.00002549
Iteration 17/1000 | Loss: 0.00002549
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002542
Iteration 20/1000 | Loss: 0.00002539
Iteration 21/1000 | Loss: 0.00002539
Iteration 22/1000 | Loss: 0.00002537
Iteration 23/1000 | Loss: 0.00002536
Iteration 24/1000 | Loss: 0.00002535
Iteration 25/1000 | Loss: 0.00002535
Iteration 26/1000 | Loss: 0.00002535
Iteration 27/1000 | Loss: 0.00002535
Iteration 28/1000 | Loss: 0.00002535
Iteration 29/1000 | Loss: 0.00002535
Iteration 30/1000 | Loss: 0.00002535
Iteration 31/1000 | Loss: 0.00002535
Iteration 32/1000 | Loss: 0.00002534
Iteration 33/1000 | Loss: 0.00002534
Iteration 34/1000 | Loss: 0.00002533
Iteration 35/1000 | Loss: 0.00002533
Iteration 36/1000 | Loss: 0.00002532
Iteration 37/1000 | Loss: 0.00002532
Iteration 38/1000 | Loss: 0.00002531
Iteration 39/1000 | Loss: 0.00002531
Iteration 40/1000 | Loss: 0.00002530
Iteration 41/1000 | Loss: 0.00002529
Iteration 42/1000 | Loss: 0.00002529
Iteration 43/1000 | Loss: 0.00002527
Iteration 44/1000 | Loss: 0.00002525
Iteration 45/1000 | Loss: 0.00002525
Iteration 46/1000 | Loss: 0.00002525
Iteration 47/1000 | Loss: 0.00002525
Iteration 48/1000 | Loss: 0.00002525
Iteration 49/1000 | Loss: 0.00002525
Iteration 50/1000 | Loss: 0.00002525
Iteration 51/1000 | Loss: 0.00002524
Iteration 52/1000 | Loss: 0.00002523
Iteration 53/1000 | Loss: 0.00002523
Iteration 54/1000 | Loss: 0.00002523
Iteration 55/1000 | Loss: 0.00002522
Iteration 56/1000 | Loss: 0.00002521
Iteration 57/1000 | Loss: 0.00002521
Iteration 58/1000 | Loss: 0.00002521
Iteration 59/1000 | Loss: 0.00002520
Iteration 60/1000 | Loss: 0.00002520
Iteration 61/1000 | Loss: 0.00002520
Iteration 62/1000 | Loss: 0.00002520
Iteration 63/1000 | Loss: 0.00002520
Iteration 64/1000 | Loss: 0.00002519
Iteration 65/1000 | Loss: 0.00002519
Iteration 66/1000 | Loss: 0.00002519
Iteration 67/1000 | Loss: 0.00002519
Iteration 68/1000 | Loss: 0.00002519
Iteration 69/1000 | Loss: 0.00002519
Iteration 70/1000 | Loss: 0.00002519
Iteration 71/1000 | Loss: 0.00002519
Iteration 72/1000 | Loss: 0.00002519
Iteration 73/1000 | Loss: 0.00002519
Iteration 74/1000 | Loss: 0.00002519
Iteration 75/1000 | Loss: 0.00002518
Iteration 76/1000 | Loss: 0.00002518
Iteration 77/1000 | Loss: 0.00002518
Iteration 78/1000 | Loss: 0.00002518
Iteration 79/1000 | Loss: 0.00002518
Iteration 80/1000 | Loss: 0.00002517
Iteration 81/1000 | Loss: 0.00002517
Iteration 82/1000 | Loss: 0.00002517
Iteration 83/1000 | Loss: 0.00002517
Iteration 84/1000 | Loss: 0.00002517
Iteration 85/1000 | Loss: 0.00002517
Iteration 86/1000 | Loss: 0.00002517
Iteration 87/1000 | Loss: 0.00002516
Iteration 88/1000 | Loss: 0.00002516
Iteration 89/1000 | Loss: 0.00002516
Iteration 90/1000 | Loss: 0.00002516
Iteration 91/1000 | Loss: 0.00002516
Iteration 92/1000 | Loss: 0.00002516
Iteration 93/1000 | Loss: 0.00002516
Iteration 94/1000 | Loss: 0.00002516
Iteration 95/1000 | Loss: 0.00002515
Iteration 96/1000 | Loss: 0.00002515
Iteration 97/1000 | Loss: 0.00002515
Iteration 98/1000 | Loss: 0.00002515
Iteration 99/1000 | Loss: 0.00002515
Iteration 100/1000 | Loss: 0.00002515
Iteration 101/1000 | Loss: 0.00002515
Iteration 102/1000 | Loss: 0.00002515
Iteration 103/1000 | Loss: 0.00002514
Iteration 104/1000 | Loss: 0.00002514
Iteration 105/1000 | Loss: 0.00002514
Iteration 106/1000 | Loss: 0.00002514
Iteration 107/1000 | Loss: 0.00002514
Iteration 108/1000 | Loss: 0.00002514
Iteration 109/1000 | Loss: 0.00002513
Iteration 110/1000 | Loss: 0.00002513
Iteration 111/1000 | Loss: 0.00002513
Iteration 112/1000 | Loss: 0.00002513
Iteration 113/1000 | Loss: 0.00002513
Iteration 114/1000 | Loss: 0.00002513
Iteration 115/1000 | Loss: 0.00002513
Iteration 116/1000 | Loss: 0.00002513
Iteration 117/1000 | Loss: 0.00002513
Iteration 118/1000 | Loss: 0.00002513
Iteration 119/1000 | Loss: 0.00002513
Iteration 120/1000 | Loss: 0.00002513
Iteration 121/1000 | Loss: 0.00002513
Iteration 122/1000 | Loss: 0.00002513
Iteration 123/1000 | Loss: 0.00002513
Iteration 124/1000 | Loss: 0.00002513
Iteration 125/1000 | Loss: 0.00002513
Iteration 126/1000 | Loss: 0.00002513
Iteration 127/1000 | Loss: 0.00002513
Iteration 128/1000 | Loss: 0.00002513
Iteration 129/1000 | Loss: 0.00002513
Iteration 130/1000 | Loss: 0.00002513
Iteration 131/1000 | Loss: 0.00002513
Iteration 132/1000 | Loss: 0.00002513
Iteration 133/1000 | Loss: 0.00002513
Iteration 134/1000 | Loss: 0.00002513
Iteration 135/1000 | Loss: 0.00002513
Iteration 136/1000 | Loss: 0.00002513
Iteration 137/1000 | Loss: 0.00002513
Iteration 138/1000 | Loss: 0.00002513
Iteration 139/1000 | Loss: 0.00002513
Iteration 140/1000 | Loss: 0.00002513
Iteration 141/1000 | Loss: 0.00002513
Iteration 142/1000 | Loss: 0.00002513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.512690844014287e-05, 2.512690844014287e-05, 2.512690844014287e-05, 2.512690844014287e-05, 2.512690844014287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.512690844014287e-05

Optimization complete. Final v2v error: 4.16987943649292 mm

Highest mean error: 5.829992771148682 mm for frame 58

Lowest mean error: 3.067511558532715 mm for frame 125

Saving results

Total time: 40.560906171798706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383066
Iteration 2/25 | Loss: 0.00121520
Iteration 3/25 | Loss: 0.00091782
Iteration 4/25 | Loss: 0.00084130
Iteration 5/25 | Loss: 0.00081424
Iteration 6/25 | Loss: 0.00080727
Iteration 7/25 | Loss: 0.00080574
Iteration 8/25 | Loss: 0.00080536
Iteration 9/25 | Loss: 0.00080536
Iteration 10/25 | Loss: 0.00080536
Iteration 11/25 | Loss: 0.00080536
Iteration 12/25 | Loss: 0.00080536
Iteration 13/25 | Loss: 0.00080536
Iteration 14/25 | Loss: 0.00080536
Iteration 15/25 | Loss: 0.00080536
Iteration 16/25 | Loss: 0.00080536
Iteration 17/25 | Loss: 0.00080536
Iteration 18/25 | Loss: 0.00080536
Iteration 19/25 | Loss: 0.00080536
Iteration 20/25 | Loss: 0.00080536
Iteration 21/25 | Loss: 0.00080536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008053563069552183, 0.0008053563069552183, 0.0008053563069552183, 0.0008053563069552183, 0.0008053563069552183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008053563069552183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40374601
Iteration 2/25 | Loss: 0.00063730
Iteration 3/25 | Loss: 0.00063729
Iteration 4/25 | Loss: 0.00063729
Iteration 5/25 | Loss: 0.00063729
Iteration 6/25 | Loss: 0.00063729
Iteration 7/25 | Loss: 0.00063729
Iteration 8/25 | Loss: 0.00063729
Iteration 9/25 | Loss: 0.00063729
Iteration 10/25 | Loss: 0.00063729
Iteration 11/25 | Loss: 0.00063729
Iteration 12/25 | Loss: 0.00063729
Iteration 13/25 | Loss: 0.00063729
Iteration 14/25 | Loss: 0.00063729
Iteration 15/25 | Loss: 0.00063729
Iteration 16/25 | Loss: 0.00063729
Iteration 17/25 | Loss: 0.00063729
Iteration 18/25 | Loss: 0.00063729
Iteration 19/25 | Loss: 0.00063729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006372883799485862, 0.0006372883799485862, 0.0006372883799485862, 0.0006372883799485862, 0.0006372883799485862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006372883799485862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063729
Iteration 2/1000 | Loss: 0.00006891
Iteration 3/1000 | Loss: 0.00004427
Iteration 4/1000 | Loss: 0.00003879
Iteration 5/1000 | Loss: 0.00003581
Iteration 6/1000 | Loss: 0.00003406
Iteration 7/1000 | Loss: 0.00003271
Iteration 8/1000 | Loss: 0.00003195
Iteration 9/1000 | Loss: 0.00003145
Iteration 10/1000 | Loss: 0.00003100
Iteration 11/1000 | Loss: 0.00003071
Iteration 12/1000 | Loss: 0.00003044
Iteration 13/1000 | Loss: 0.00003032
Iteration 14/1000 | Loss: 0.00003030
Iteration 15/1000 | Loss: 0.00003030
Iteration 16/1000 | Loss: 0.00003024
Iteration 17/1000 | Loss: 0.00003016
Iteration 18/1000 | Loss: 0.00003015
Iteration 19/1000 | Loss: 0.00003015
Iteration 20/1000 | Loss: 0.00003013
Iteration 21/1000 | Loss: 0.00003013
Iteration 22/1000 | Loss: 0.00003012
Iteration 23/1000 | Loss: 0.00003012
Iteration 24/1000 | Loss: 0.00003011
Iteration 25/1000 | Loss: 0.00003011
Iteration 26/1000 | Loss: 0.00003010
Iteration 27/1000 | Loss: 0.00003010
Iteration 28/1000 | Loss: 0.00003010
Iteration 29/1000 | Loss: 0.00003009
Iteration 30/1000 | Loss: 0.00003009
Iteration 31/1000 | Loss: 0.00003009
Iteration 32/1000 | Loss: 0.00003008
Iteration 33/1000 | Loss: 0.00003007
Iteration 34/1000 | Loss: 0.00003006
Iteration 35/1000 | Loss: 0.00003006
Iteration 36/1000 | Loss: 0.00003005
Iteration 37/1000 | Loss: 0.00003005
Iteration 38/1000 | Loss: 0.00003004
Iteration 39/1000 | Loss: 0.00003003
Iteration 40/1000 | Loss: 0.00003003
Iteration 41/1000 | Loss: 0.00003003
Iteration 42/1000 | Loss: 0.00003002
Iteration 43/1000 | Loss: 0.00003002
Iteration 44/1000 | Loss: 0.00003002
Iteration 45/1000 | Loss: 0.00003002
Iteration 46/1000 | Loss: 0.00003001
Iteration 47/1000 | Loss: 0.00003001
Iteration 48/1000 | Loss: 0.00003001
Iteration 49/1000 | Loss: 0.00003001
Iteration 50/1000 | Loss: 0.00003000
Iteration 51/1000 | Loss: 0.00003000
Iteration 52/1000 | Loss: 0.00003000
Iteration 53/1000 | Loss: 0.00003000
Iteration 54/1000 | Loss: 0.00003000
Iteration 55/1000 | Loss: 0.00003000
Iteration 56/1000 | Loss: 0.00002999
Iteration 57/1000 | Loss: 0.00002999
Iteration 58/1000 | Loss: 0.00002998
Iteration 59/1000 | Loss: 0.00002998
Iteration 60/1000 | Loss: 0.00002998
Iteration 61/1000 | Loss: 0.00002998
Iteration 62/1000 | Loss: 0.00002998
Iteration 63/1000 | Loss: 0.00002998
Iteration 64/1000 | Loss: 0.00002997
Iteration 65/1000 | Loss: 0.00002997
Iteration 66/1000 | Loss: 0.00002997
Iteration 67/1000 | Loss: 0.00002997
Iteration 68/1000 | Loss: 0.00002997
Iteration 69/1000 | Loss: 0.00002997
Iteration 70/1000 | Loss: 0.00002997
Iteration 71/1000 | Loss: 0.00002997
Iteration 72/1000 | Loss: 0.00002996
Iteration 73/1000 | Loss: 0.00002996
Iteration 74/1000 | Loss: 0.00002996
Iteration 75/1000 | Loss: 0.00002996
Iteration 76/1000 | Loss: 0.00002996
Iteration 77/1000 | Loss: 0.00002996
Iteration 78/1000 | Loss: 0.00002996
Iteration 79/1000 | Loss: 0.00002996
Iteration 80/1000 | Loss: 0.00002996
Iteration 81/1000 | Loss: 0.00002996
Iteration 82/1000 | Loss: 0.00002995
Iteration 83/1000 | Loss: 0.00002995
Iteration 84/1000 | Loss: 0.00002995
Iteration 85/1000 | Loss: 0.00002995
Iteration 86/1000 | Loss: 0.00002995
Iteration 87/1000 | Loss: 0.00002995
Iteration 88/1000 | Loss: 0.00002995
Iteration 89/1000 | Loss: 0.00002995
Iteration 90/1000 | Loss: 0.00002995
Iteration 91/1000 | Loss: 0.00002995
Iteration 92/1000 | Loss: 0.00002995
Iteration 93/1000 | Loss: 0.00002995
Iteration 94/1000 | Loss: 0.00002995
Iteration 95/1000 | Loss: 0.00002994
Iteration 96/1000 | Loss: 0.00002994
Iteration 97/1000 | Loss: 0.00002994
Iteration 98/1000 | Loss: 0.00002994
Iteration 99/1000 | Loss: 0.00002994
Iteration 100/1000 | Loss: 0.00002994
Iteration 101/1000 | Loss: 0.00002994
Iteration 102/1000 | Loss: 0.00002994
Iteration 103/1000 | Loss: 0.00002994
Iteration 104/1000 | Loss: 0.00002994
Iteration 105/1000 | Loss: 0.00002994
Iteration 106/1000 | Loss: 0.00002994
Iteration 107/1000 | Loss: 0.00002994
Iteration 108/1000 | Loss: 0.00002994
Iteration 109/1000 | Loss: 0.00002994
Iteration 110/1000 | Loss: 0.00002994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.994201349793002e-05, 2.994201349793002e-05, 2.994201349793002e-05, 2.994201349793002e-05, 2.994201349793002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.994201349793002e-05

Optimization complete. Final v2v error: 4.6224589347839355 mm

Highest mean error: 5.438157081604004 mm for frame 239

Lowest mean error: 3.753422737121582 mm for frame 79

Saving results

Total time: 43.49504017829895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568537
Iteration 2/25 | Loss: 0.00097018
Iteration 3/25 | Loss: 0.00078676
Iteration 4/25 | Loss: 0.00075251
Iteration 5/25 | Loss: 0.00074604
Iteration 6/25 | Loss: 0.00074604
Iteration 7/25 | Loss: 0.00074604
Iteration 8/25 | Loss: 0.00074604
Iteration 9/25 | Loss: 0.00074604
Iteration 10/25 | Loss: 0.00074604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0007460394990630448, 0.0007460394990630448, 0.0007460394990630448, 0.0007460394990630448, 0.0007460394990630448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007460394990630448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48741543
Iteration 2/25 | Loss: 0.00068103
Iteration 3/25 | Loss: 0.00068103
Iteration 4/25 | Loss: 0.00068103
Iteration 5/25 | Loss: 0.00068103
Iteration 6/25 | Loss: 0.00068103
Iteration 7/25 | Loss: 0.00068103
Iteration 8/25 | Loss: 0.00068103
Iteration 9/25 | Loss: 0.00068103
Iteration 10/25 | Loss: 0.00068103
Iteration 11/25 | Loss: 0.00068103
Iteration 12/25 | Loss: 0.00068103
Iteration 13/25 | Loss: 0.00068103
Iteration 14/25 | Loss: 0.00068103
Iteration 15/25 | Loss: 0.00068103
Iteration 16/25 | Loss: 0.00068103
Iteration 17/25 | Loss: 0.00068103
Iteration 18/25 | Loss: 0.00068103
Iteration 19/25 | Loss: 0.00068103
Iteration 20/25 | Loss: 0.00068103
Iteration 21/25 | Loss: 0.00068103
Iteration 22/25 | Loss: 0.00068103
Iteration 23/25 | Loss: 0.00068103
Iteration 24/25 | Loss: 0.00068103
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006810284103266895, 0.0006810284103266895, 0.0006810284103266895, 0.0006810284103266895, 0.0006810284103266895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006810284103266895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068103
Iteration 2/1000 | Loss: 0.00004362
Iteration 3/1000 | Loss: 0.00003030
Iteration 4/1000 | Loss: 0.00002724
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00002378
Iteration 7/1000 | Loss: 0.00002265
Iteration 8/1000 | Loss: 0.00002201
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002104
Iteration 12/1000 | Loss: 0.00002102
Iteration 13/1000 | Loss: 0.00002076
Iteration 14/1000 | Loss: 0.00002074
Iteration 15/1000 | Loss: 0.00002061
Iteration 16/1000 | Loss: 0.00002053
Iteration 17/1000 | Loss: 0.00002044
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00002023
Iteration 21/1000 | Loss: 0.00002019
Iteration 22/1000 | Loss: 0.00002018
Iteration 23/1000 | Loss: 0.00002017
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002016
Iteration 27/1000 | Loss: 0.00002016
Iteration 28/1000 | Loss: 0.00002015
Iteration 29/1000 | Loss: 0.00002011
Iteration 30/1000 | Loss: 0.00002010
Iteration 31/1000 | Loss: 0.00002010
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002008
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00002006
Iteration 37/1000 | Loss: 0.00002005
Iteration 38/1000 | Loss: 0.00002005
Iteration 39/1000 | Loss: 0.00002005
Iteration 40/1000 | Loss: 0.00002005
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002003
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001997
Iteration 53/1000 | Loss: 0.00001996
Iteration 54/1000 | Loss: 0.00001996
Iteration 55/1000 | Loss: 0.00001996
Iteration 56/1000 | Loss: 0.00001996
Iteration 57/1000 | Loss: 0.00001995
Iteration 58/1000 | Loss: 0.00001995
Iteration 59/1000 | Loss: 0.00001995
Iteration 60/1000 | Loss: 0.00001994
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001986
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001985
Iteration 73/1000 | Loss: 0.00001984
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001980
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001980
Iteration 80/1000 | Loss: 0.00001980
Iteration 81/1000 | Loss: 0.00001980
Iteration 82/1000 | Loss: 0.00001980
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001977
Iteration 86/1000 | Loss: 0.00001977
Iteration 87/1000 | Loss: 0.00001976
Iteration 88/1000 | Loss: 0.00001976
Iteration 89/1000 | Loss: 0.00001975
Iteration 90/1000 | Loss: 0.00001975
Iteration 91/1000 | Loss: 0.00001975
Iteration 92/1000 | Loss: 0.00001974
Iteration 93/1000 | Loss: 0.00001974
Iteration 94/1000 | Loss: 0.00001974
Iteration 95/1000 | Loss: 0.00001973
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001972
Iteration 98/1000 | Loss: 0.00001972
Iteration 99/1000 | Loss: 0.00001972
Iteration 100/1000 | Loss: 0.00001972
Iteration 101/1000 | Loss: 0.00001972
Iteration 102/1000 | Loss: 0.00001971
Iteration 103/1000 | Loss: 0.00001971
Iteration 104/1000 | Loss: 0.00001971
Iteration 105/1000 | Loss: 0.00001971
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001970
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001969
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001969
Iteration 117/1000 | Loss: 0.00001969
Iteration 118/1000 | Loss: 0.00001969
Iteration 119/1000 | Loss: 0.00001968
Iteration 120/1000 | Loss: 0.00001968
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001968
Iteration 126/1000 | Loss: 0.00001968
Iteration 127/1000 | Loss: 0.00001968
Iteration 128/1000 | Loss: 0.00001968
Iteration 129/1000 | Loss: 0.00001967
Iteration 130/1000 | Loss: 0.00001967
Iteration 131/1000 | Loss: 0.00001967
Iteration 132/1000 | Loss: 0.00001967
Iteration 133/1000 | Loss: 0.00001967
Iteration 134/1000 | Loss: 0.00001967
Iteration 135/1000 | Loss: 0.00001967
Iteration 136/1000 | Loss: 0.00001967
Iteration 137/1000 | Loss: 0.00001967
Iteration 138/1000 | Loss: 0.00001967
Iteration 139/1000 | Loss: 0.00001966
Iteration 140/1000 | Loss: 0.00001966
Iteration 141/1000 | Loss: 0.00001966
Iteration 142/1000 | Loss: 0.00001966
Iteration 143/1000 | Loss: 0.00001966
Iteration 144/1000 | Loss: 0.00001966
Iteration 145/1000 | Loss: 0.00001965
Iteration 146/1000 | Loss: 0.00001965
Iteration 147/1000 | Loss: 0.00001965
Iteration 148/1000 | Loss: 0.00001965
Iteration 149/1000 | Loss: 0.00001965
Iteration 150/1000 | Loss: 0.00001964
Iteration 151/1000 | Loss: 0.00001964
Iteration 152/1000 | Loss: 0.00001964
Iteration 153/1000 | Loss: 0.00001964
Iteration 154/1000 | Loss: 0.00001964
Iteration 155/1000 | Loss: 0.00001964
Iteration 156/1000 | Loss: 0.00001964
Iteration 157/1000 | Loss: 0.00001964
Iteration 158/1000 | Loss: 0.00001964
Iteration 159/1000 | Loss: 0.00001964
Iteration 160/1000 | Loss: 0.00001964
Iteration 161/1000 | Loss: 0.00001964
Iteration 162/1000 | Loss: 0.00001964
Iteration 163/1000 | Loss: 0.00001964
Iteration 164/1000 | Loss: 0.00001963
Iteration 165/1000 | Loss: 0.00001963
Iteration 166/1000 | Loss: 0.00001963
Iteration 167/1000 | Loss: 0.00001963
Iteration 168/1000 | Loss: 0.00001963
Iteration 169/1000 | Loss: 0.00001963
Iteration 170/1000 | Loss: 0.00001963
Iteration 171/1000 | Loss: 0.00001963
Iteration 172/1000 | Loss: 0.00001963
Iteration 173/1000 | Loss: 0.00001963
Iteration 174/1000 | Loss: 0.00001963
Iteration 175/1000 | Loss: 0.00001963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.963028080353979e-05, 1.963028080353979e-05, 1.963028080353979e-05, 1.963028080353979e-05, 1.963028080353979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.963028080353979e-05

Optimization complete. Final v2v error: 3.8344993591308594 mm

Highest mean error: 4.513476371765137 mm for frame 94

Lowest mean error: 3.3324971199035645 mm for frame 0

Saving results

Total time: 49.419923305511475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00336112
Iteration 2/25 | Loss: 0.00100638
Iteration 3/25 | Loss: 0.00082406
Iteration 4/25 | Loss: 0.00077399
Iteration 5/25 | Loss: 0.00075546
Iteration 6/25 | Loss: 0.00075090
Iteration 7/25 | Loss: 0.00074957
Iteration 8/25 | Loss: 0.00074916
Iteration 9/25 | Loss: 0.00074916
Iteration 10/25 | Loss: 0.00074916
Iteration 11/25 | Loss: 0.00074916
Iteration 12/25 | Loss: 0.00074916
Iteration 13/25 | Loss: 0.00074916
Iteration 14/25 | Loss: 0.00074916
Iteration 15/25 | Loss: 0.00074916
Iteration 16/25 | Loss: 0.00074916
Iteration 17/25 | Loss: 0.00074916
Iteration 18/25 | Loss: 0.00074916
Iteration 19/25 | Loss: 0.00074916
Iteration 20/25 | Loss: 0.00074916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007491576834581792, 0.0007491576834581792, 0.0007491576834581792, 0.0007491576834581792, 0.0007491576834581792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007491576834581792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42713094
Iteration 2/25 | Loss: 0.00070396
Iteration 3/25 | Loss: 0.00070396
Iteration 4/25 | Loss: 0.00070396
Iteration 5/25 | Loss: 0.00070396
Iteration 6/25 | Loss: 0.00070396
Iteration 7/25 | Loss: 0.00070396
Iteration 8/25 | Loss: 0.00070396
Iteration 9/25 | Loss: 0.00070396
Iteration 10/25 | Loss: 0.00070396
Iteration 11/25 | Loss: 0.00070396
Iteration 12/25 | Loss: 0.00070396
Iteration 13/25 | Loss: 0.00070396
Iteration 14/25 | Loss: 0.00070396
Iteration 15/25 | Loss: 0.00070396
Iteration 16/25 | Loss: 0.00070396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000703960657119751, 0.000703960657119751, 0.000703960657119751, 0.000703960657119751, 0.000703960657119751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000703960657119751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070396
Iteration 2/1000 | Loss: 0.00005780
Iteration 3/1000 | Loss: 0.00003503
Iteration 4/1000 | Loss: 0.00003103
Iteration 5/1000 | Loss: 0.00002851
Iteration 6/1000 | Loss: 0.00002707
Iteration 7/1000 | Loss: 0.00002600
Iteration 8/1000 | Loss: 0.00002532
Iteration 9/1000 | Loss: 0.00002488
Iteration 10/1000 | Loss: 0.00002446
Iteration 11/1000 | Loss: 0.00002414
Iteration 12/1000 | Loss: 0.00002392
Iteration 13/1000 | Loss: 0.00002387
Iteration 14/1000 | Loss: 0.00002385
Iteration 15/1000 | Loss: 0.00002380
Iteration 16/1000 | Loss: 0.00002375
Iteration 17/1000 | Loss: 0.00002374
Iteration 18/1000 | Loss: 0.00002369
Iteration 19/1000 | Loss: 0.00002365
Iteration 20/1000 | Loss: 0.00002364
Iteration 21/1000 | Loss: 0.00002364
Iteration 22/1000 | Loss: 0.00002363
Iteration 23/1000 | Loss: 0.00002363
Iteration 24/1000 | Loss: 0.00002363
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002361
Iteration 29/1000 | Loss: 0.00002361
Iteration 30/1000 | Loss: 0.00002361
Iteration 31/1000 | Loss: 0.00002360
Iteration 32/1000 | Loss: 0.00002360
Iteration 33/1000 | Loss: 0.00002359
Iteration 34/1000 | Loss: 0.00002359
Iteration 35/1000 | Loss: 0.00002359
Iteration 36/1000 | Loss: 0.00002359
Iteration 37/1000 | Loss: 0.00002359
Iteration 38/1000 | Loss: 0.00002359
Iteration 39/1000 | Loss: 0.00002358
Iteration 40/1000 | Loss: 0.00002358
Iteration 41/1000 | Loss: 0.00002358
Iteration 42/1000 | Loss: 0.00002357
Iteration 43/1000 | Loss: 0.00002357
Iteration 44/1000 | Loss: 0.00002357
Iteration 45/1000 | Loss: 0.00002357
Iteration 46/1000 | Loss: 0.00002356
Iteration 47/1000 | Loss: 0.00002356
Iteration 48/1000 | Loss: 0.00002356
Iteration 49/1000 | Loss: 0.00002355
Iteration 50/1000 | Loss: 0.00002355
Iteration 51/1000 | Loss: 0.00002355
Iteration 52/1000 | Loss: 0.00002354
Iteration 53/1000 | Loss: 0.00002354
Iteration 54/1000 | Loss: 0.00002354
Iteration 55/1000 | Loss: 0.00002354
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00002353
Iteration 58/1000 | Loss: 0.00002353
Iteration 59/1000 | Loss: 0.00002352
Iteration 60/1000 | Loss: 0.00002352
Iteration 61/1000 | Loss: 0.00002352
Iteration 62/1000 | Loss: 0.00002351
Iteration 63/1000 | Loss: 0.00002351
Iteration 64/1000 | Loss: 0.00002351
Iteration 65/1000 | Loss: 0.00002351
Iteration 66/1000 | Loss: 0.00002351
Iteration 67/1000 | Loss: 0.00002351
Iteration 68/1000 | Loss: 0.00002351
Iteration 69/1000 | Loss: 0.00002350
Iteration 70/1000 | Loss: 0.00002350
Iteration 71/1000 | Loss: 0.00002350
Iteration 72/1000 | Loss: 0.00002350
Iteration 73/1000 | Loss: 0.00002350
Iteration 74/1000 | Loss: 0.00002350
Iteration 75/1000 | Loss: 0.00002350
Iteration 76/1000 | Loss: 0.00002350
Iteration 77/1000 | Loss: 0.00002350
Iteration 78/1000 | Loss: 0.00002349
Iteration 79/1000 | Loss: 0.00002349
Iteration 80/1000 | Loss: 0.00002349
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002349
Iteration 84/1000 | Loss: 0.00002349
Iteration 85/1000 | Loss: 0.00002349
Iteration 86/1000 | Loss: 0.00002349
Iteration 87/1000 | Loss: 0.00002348
Iteration 88/1000 | Loss: 0.00002348
Iteration 89/1000 | Loss: 0.00002348
Iteration 90/1000 | Loss: 0.00002348
Iteration 91/1000 | Loss: 0.00002348
Iteration 92/1000 | Loss: 0.00002348
Iteration 93/1000 | Loss: 0.00002348
Iteration 94/1000 | Loss: 0.00002348
Iteration 95/1000 | Loss: 0.00002348
Iteration 96/1000 | Loss: 0.00002348
Iteration 97/1000 | Loss: 0.00002348
Iteration 98/1000 | Loss: 0.00002347
Iteration 99/1000 | Loss: 0.00002347
Iteration 100/1000 | Loss: 0.00002347
Iteration 101/1000 | Loss: 0.00002347
Iteration 102/1000 | Loss: 0.00002347
Iteration 103/1000 | Loss: 0.00002347
Iteration 104/1000 | Loss: 0.00002347
Iteration 105/1000 | Loss: 0.00002346
Iteration 106/1000 | Loss: 0.00002346
Iteration 107/1000 | Loss: 0.00002346
Iteration 108/1000 | Loss: 0.00002346
Iteration 109/1000 | Loss: 0.00002346
Iteration 110/1000 | Loss: 0.00002346
Iteration 111/1000 | Loss: 0.00002346
Iteration 112/1000 | Loss: 0.00002346
Iteration 113/1000 | Loss: 0.00002346
Iteration 114/1000 | Loss: 0.00002346
Iteration 115/1000 | Loss: 0.00002345
Iteration 116/1000 | Loss: 0.00002345
Iteration 117/1000 | Loss: 0.00002345
Iteration 118/1000 | Loss: 0.00002345
Iteration 119/1000 | Loss: 0.00002345
Iteration 120/1000 | Loss: 0.00002345
Iteration 121/1000 | Loss: 0.00002345
Iteration 122/1000 | Loss: 0.00002344
Iteration 123/1000 | Loss: 0.00002344
Iteration 124/1000 | Loss: 0.00002344
Iteration 125/1000 | Loss: 0.00002344
Iteration 126/1000 | Loss: 0.00002344
Iteration 127/1000 | Loss: 0.00002344
Iteration 128/1000 | Loss: 0.00002344
Iteration 129/1000 | Loss: 0.00002344
Iteration 130/1000 | Loss: 0.00002344
Iteration 131/1000 | Loss: 0.00002344
Iteration 132/1000 | Loss: 0.00002344
Iteration 133/1000 | Loss: 0.00002344
Iteration 134/1000 | Loss: 0.00002344
Iteration 135/1000 | Loss: 0.00002344
Iteration 136/1000 | Loss: 0.00002344
Iteration 137/1000 | Loss: 0.00002344
Iteration 138/1000 | Loss: 0.00002344
Iteration 139/1000 | Loss: 0.00002344
Iteration 140/1000 | Loss: 0.00002344
Iteration 141/1000 | Loss: 0.00002344
Iteration 142/1000 | Loss: 0.00002344
Iteration 143/1000 | Loss: 0.00002344
Iteration 144/1000 | Loss: 0.00002344
Iteration 145/1000 | Loss: 0.00002344
Iteration 146/1000 | Loss: 0.00002344
Iteration 147/1000 | Loss: 0.00002344
Iteration 148/1000 | Loss: 0.00002344
Iteration 149/1000 | Loss: 0.00002344
Iteration 150/1000 | Loss: 0.00002344
Iteration 151/1000 | Loss: 0.00002344
Iteration 152/1000 | Loss: 0.00002344
Iteration 153/1000 | Loss: 0.00002344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.3436778064933605e-05, 2.3436778064933605e-05, 2.3436778064933605e-05, 2.3436778064933605e-05, 2.3436778064933605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3436778064933605e-05

Optimization complete. Final v2v error: 4.015329837799072 mm

Highest mean error: 4.660060405731201 mm for frame 41

Lowest mean error: 3.103121757507324 mm for frame 110

Saving results

Total time: 44.61253476142883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051354
Iteration 2/25 | Loss: 0.00414141
Iteration 3/25 | Loss: 0.00331528
Iteration 4/25 | Loss: 0.00319061
Iteration 5/25 | Loss: 0.00322593
Iteration 6/25 | Loss: 0.00311911
Iteration 7/25 | Loss: 0.00274310
Iteration 8/25 | Loss: 0.00259153
Iteration 9/25 | Loss: 0.00267949
Iteration 10/25 | Loss: 0.00263498
Iteration 11/25 | Loss: 0.00246773
Iteration 12/25 | Loss: 0.00231889
Iteration 13/25 | Loss: 0.00225202
Iteration 14/25 | Loss: 0.00243853
Iteration 15/25 | Loss: 0.00207256
Iteration 16/25 | Loss: 0.00204267
Iteration 17/25 | Loss: 0.00188453
Iteration 18/25 | Loss: 0.00194107
Iteration 19/25 | Loss: 0.00184623
Iteration 20/25 | Loss: 0.00182002
Iteration 21/25 | Loss: 0.00176004
Iteration 22/25 | Loss: 0.00176921
Iteration 23/25 | Loss: 0.00174279
Iteration 24/25 | Loss: 0.00173200
Iteration 25/25 | Loss: 0.00175286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18376112
Iteration 2/25 | Loss: 0.00396852
Iteration 3/25 | Loss: 0.00374091
Iteration 4/25 | Loss: 0.00374091
Iteration 5/25 | Loss: 0.00374091
Iteration 6/25 | Loss: 0.00374091
Iteration 7/25 | Loss: 0.00374091
Iteration 8/25 | Loss: 0.00374091
Iteration 9/25 | Loss: 0.00374091
Iteration 10/25 | Loss: 0.00374091
Iteration 11/25 | Loss: 0.00374091
Iteration 12/25 | Loss: 0.00374091
Iteration 13/25 | Loss: 0.00374091
Iteration 14/25 | Loss: 0.00374091
Iteration 15/25 | Loss: 0.00374091
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003740909742191434, 0.003740909742191434, 0.003740909742191434, 0.003740909742191434, 0.003740909742191434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003740909742191434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374091
Iteration 2/1000 | Loss: 0.00702934
Iteration 3/1000 | Loss: 0.00146694
Iteration 4/1000 | Loss: 0.00083819
Iteration 5/1000 | Loss: 0.00196551
Iteration 6/1000 | Loss: 0.00078239
Iteration 7/1000 | Loss: 0.00029463
Iteration 8/1000 | Loss: 0.00026532
Iteration 9/1000 | Loss: 0.00131110
Iteration 10/1000 | Loss: 0.00027288
Iteration 11/1000 | Loss: 0.00023731
Iteration 12/1000 | Loss: 0.00021033
Iteration 13/1000 | Loss: 0.00059429
Iteration 14/1000 | Loss: 0.00066386
Iteration 15/1000 | Loss: 0.00020421
Iteration 16/1000 | Loss: 0.00019583
Iteration 17/1000 | Loss: 0.00196208
Iteration 18/1000 | Loss: 0.01259111
Iteration 19/1000 | Loss: 0.00425136
Iteration 20/1000 | Loss: 0.00209772
Iteration 21/1000 | Loss: 0.00038547
Iteration 22/1000 | Loss: 0.00024268
Iteration 23/1000 | Loss: 0.00033170
Iteration 24/1000 | Loss: 0.00042830
Iteration 25/1000 | Loss: 0.00014161
Iteration 26/1000 | Loss: 0.00013022
Iteration 27/1000 | Loss: 0.00012916
Iteration 28/1000 | Loss: 0.00011157
Iteration 29/1000 | Loss: 0.00010799
Iteration 30/1000 | Loss: 0.00010206
Iteration 31/1000 | Loss: 0.00011128
Iteration 32/1000 | Loss: 0.00008768
Iteration 33/1000 | Loss: 0.00007543
Iteration 34/1000 | Loss: 0.00009359
Iteration 35/1000 | Loss: 0.00007767
Iteration 36/1000 | Loss: 0.00032179
Iteration 37/1000 | Loss: 0.00009722
Iteration 38/1000 | Loss: 0.00027764
Iteration 39/1000 | Loss: 0.00009251
Iteration 40/1000 | Loss: 0.00023646
Iteration 41/1000 | Loss: 0.00026365
Iteration 42/1000 | Loss: 0.00032662
Iteration 43/1000 | Loss: 0.00031316
Iteration 44/1000 | Loss: 0.00030886
Iteration 45/1000 | Loss: 0.00020387
Iteration 46/1000 | Loss: 0.00070939
Iteration 47/1000 | Loss: 0.00010848
Iteration 48/1000 | Loss: 0.00009067
Iteration 49/1000 | Loss: 0.00009280
Iteration 50/1000 | Loss: 0.00007893
Iteration 51/1000 | Loss: 0.00010640
Iteration 52/1000 | Loss: 0.00009027
Iteration 53/1000 | Loss: 0.00008029
Iteration 54/1000 | Loss: 0.00008462
Iteration 55/1000 | Loss: 0.00008586
Iteration 56/1000 | Loss: 0.00010060
Iteration 57/1000 | Loss: 0.00007748
Iteration 58/1000 | Loss: 0.00010162
Iteration 59/1000 | Loss: 0.00008431
Iteration 60/1000 | Loss: 0.00007305
Iteration 61/1000 | Loss: 0.00009584
Iteration 62/1000 | Loss: 0.00024747
Iteration 63/1000 | Loss: 0.00013400
Iteration 64/1000 | Loss: 0.00007585
Iteration 65/1000 | Loss: 0.00009840
Iteration 66/1000 | Loss: 0.00007500
Iteration 67/1000 | Loss: 0.00029614
Iteration 68/1000 | Loss: 0.00007968
Iteration 69/1000 | Loss: 0.00009997
Iteration 70/1000 | Loss: 0.00022664
Iteration 71/1000 | Loss: 0.00007315
Iteration 72/1000 | Loss: 0.00006560
Iteration 73/1000 | Loss: 0.00032776
Iteration 74/1000 | Loss: 0.00006614
Iteration 75/1000 | Loss: 0.00006265
Iteration 76/1000 | Loss: 0.00006015
Iteration 77/1000 | Loss: 0.00029856
Iteration 78/1000 | Loss: 0.00053683
Iteration 79/1000 | Loss: 0.00049007
Iteration 80/1000 | Loss: 0.00030587
Iteration 81/1000 | Loss: 0.00007641
Iteration 82/1000 | Loss: 0.00019181
Iteration 83/1000 | Loss: 0.00007514
Iteration 84/1000 | Loss: 0.00006650
Iteration 85/1000 | Loss: 0.00006431
Iteration 86/1000 | Loss: 0.00006151
Iteration 87/1000 | Loss: 0.00005934
Iteration 88/1000 | Loss: 0.00026910
Iteration 89/1000 | Loss: 0.00008645
Iteration 90/1000 | Loss: 0.00012731
Iteration 91/1000 | Loss: 0.00023906
Iteration 92/1000 | Loss: 0.00006614
Iteration 93/1000 | Loss: 0.00005997
Iteration 94/1000 | Loss: 0.00005847
Iteration 95/1000 | Loss: 0.00005772
Iteration 96/1000 | Loss: 0.00008580
Iteration 97/1000 | Loss: 0.00006212
Iteration 98/1000 | Loss: 0.00005870
Iteration 99/1000 | Loss: 0.00005744
Iteration 100/1000 | Loss: 0.00005684
Iteration 101/1000 | Loss: 0.00005622
Iteration 102/1000 | Loss: 0.00005466
Iteration 103/1000 | Loss: 0.00005352
Iteration 104/1000 | Loss: 0.00005282
Iteration 105/1000 | Loss: 0.00005251
Iteration 106/1000 | Loss: 0.00005230
Iteration 107/1000 | Loss: 0.00005216
Iteration 108/1000 | Loss: 0.00005213
Iteration 109/1000 | Loss: 0.00005209
Iteration 110/1000 | Loss: 0.00005220
Iteration 111/1000 | Loss: 0.00005202
Iteration 112/1000 | Loss: 0.00008001
Iteration 113/1000 | Loss: 0.00007052
Iteration 114/1000 | Loss: 0.00007215
Iteration 115/1000 | Loss: 0.00006225
Iteration 116/1000 | Loss: 0.00005295
Iteration 117/1000 | Loss: 0.00005251
Iteration 118/1000 | Loss: 0.00007131
Iteration 119/1000 | Loss: 0.00005403
Iteration 120/1000 | Loss: 0.00005357
Iteration 121/1000 | Loss: 0.00005246
Iteration 122/1000 | Loss: 0.00005224
Iteration 123/1000 | Loss: 0.00005198
Iteration 124/1000 | Loss: 0.00005175
Iteration 125/1000 | Loss: 0.00005174
Iteration 126/1000 | Loss: 0.00005173
Iteration 127/1000 | Loss: 0.00005173
Iteration 128/1000 | Loss: 0.00005171
Iteration 129/1000 | Loss: 0.00005161
Iteration 130/1000 | Loss: 0.00005159
Iteration 131/1000 | Loss: 0.00005157
Iteration 132/1000 | Loss: 0.00005169
Iteration 133/1000 | Loss: 0.00005149
Iteration 134/1000 | Loss: 0.00005149
Iteration 135/1000 | Loss: 0.00005148
Iteration 136/1000 | Loss: 0.00005147
Iteration 137/1000 | Loss: 0.00005147
Iteration 138/1000 | Loss: 0.00005146
Iteration 139/1000 | Loss: 0.00005146
Iteration 140/1000 | Loss: 0.00005146
Iteration 141/1000 | Loss: 0.00005145
Iteration 142/1000 | Loss: 0.00005145
Iteration 143/1000 | Loss: 0.00005144
Iteration 144/1000 | Loss: 0.00005144
Iteration 145/1000 | Loss: 0.00005144
Iteration 146/1000 | Loss: 0.00005143
Iteration 147/1000 | Loss: 0.00005143
Iteration 148/1000 | Loss: 0.00005142
Iteration 149/1000 | Loss: 0.00005142
Iteration 150/1000 | Loss: 0.00005152
Iteration 151/1000 | Loss: 0.00005152
Iteration 152/1000 | Loss: 0.00005152
Iteration 153/1000 | Loss: 0.00005151
Iteration 154/1000 | Loss: 0.00005151
Iteration 155/1000 | Loss: 0.00005149
Iteration 156/1000 | Loss: 0.00005147
Iteration 157/1000 | Loss: 0.00005147
Iteration 158/1000 | Loss: 0.00005147
Iteration 159/1000 | Loss: 0.00005147
Iteration 160/1000 | Loss: 0.00005146
Iteration 161/1000 | Loss: 0.00005146
Iteration 162/1000 | Loss: 0.00005146
Iteration 163/1000 | Loss: 0.00005146
Iteration 164/1000 | Loss: 0.00005146
Iteration 165/1000 | Loss: 0.00005146
Iteration 166/1000 | Loss: 0.00005146
Iteration 167/1000 | Loss: 0.00005146
Iteration 168/1000 | Loss: 0.00005145
Iteration 169/1000 | Loss: 0.00005144
Iteration 170/1000 | Loss: 0.00005149
Iteration 171/1000 | Loss: 0.00005139
Iteration 172/1000 | Loss: 0.00005139
Iteration 173/1000 | Loss: 0.00005145
Iteration 174/1000 | Loss: 0.00005145
Iteration 175/1000 | Loss: 0.00005133
Iteration 176/1000 | Loss: 0.00005133
Iteration 177/1000 | Loss: 0.00005133
Iteration 178/1000 | Loss: 0.00005133
Iteration 179/1000 | Loss: 0.00005135
Iteration 180/1000 | Loss: 0.00005135
Iteration 181/1000 | Loss: 0.00005134
Iteration 182/1000 | Loss: 0.00005134
Iteration 183/1000 | Loss: 0.00005132
Iteration 184/1000 | Loss: 0.00005132
Iteration 185/1000 | Loss: 0.00005132
Iteration 186/1000 | Loss: 0.00005125
Iteration 187/1000 | Loss: 0.00005125
Iteration 188/1000 | Loss: 0.00005125
Iteration 189/1000 | Loss: 0.00005125
Iteration 190/1000 | Loss: 0.00005125
Iteration 191/1000 | Loss: 0.00005125
Iteration 192/1000 | Loss: 0.00005125
Iteration 193/1000 | Loss: 0.00005125
Iteration 194/1000 | Loss: 0.00005125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [5.1248327508801594e-05, 5.1248327508801594e-05, 5.1248327508801594e-05, 5.1248327508801594e-05, 5.1248327508801594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.1248327508801594e-05

Optimization complete. Final v2v error: 5.293185710906982 mm

Highest mean error: 10.791696548461914 mm for frame 94

Lowest mean error: 4.552574634552002 mm for frame 164

Saving results

Total time: 233.27945733070374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00673434
Iteration 2/25 | Loss: 0.00080807
Iteration 3/25 | Loss: 0.00068930
Iteration 4/25 | Loss: 0.00066486
Iteration 5/25 | Loss: 0.00065839
Iteration 6/25 | Loss: 0.00065698
Iteration 7/25 | Loss: 0.00065661
Iteration 8/25 | Loss: 0.00065661
Iteration 9/25 | Loss: 0.00065661
Iteration 10/25 | Loss: 0.00065661
Iteration 11/25 | Loss: 0.00065661
Iteration 12/25 | Loss: 0.00065661
Iteration 13/25 | Loss: 0.00065661
Iteration 14/25 | Loss: 0.00065661
Iteration 15/25 | Loss: 0.00065661
Iteration 16/25 | Loss: 0.00065661
Iteration 17/25 | Loss: 0.00065661
Iteration 18/25 | Loss: 0.00065661
Iteration 19/25 | Loss: 0.00065661
Iteration 20/25 | Loss: 0.00065661
Iteration 21/25 | Loss: 0.00065661
Iteration 22/25 | Loss: 0.00065661
Iteration 23/25 | Loss: 0.00065661
Iteration 24/25 | Loss: 0.00065661
Iteration 25/25 | Loss: 0.00065661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.26759005
Iteration 2/25 | Loss: 0.00040494
Iteration 3/25 | Loss: 0.00040493
Iteration 4/25 | Loss: 0.00040493
Iteration 5/25 | Loss: 0.00040493
Iteration 6/25 | Loss: 0.00040493
Iteration 7/25 | Loss: 0.00040493
Iteration 8/25 | Loss: 0.00040493
Iteration 9/25 | Loss: 0.00040493
Iteration 10/25 | Loss: 0.00040492
Iteration 11/25 | Loss: 0.00040492
Iteration 12/25 | Loss: 0.00040492
Iteration 13/25 | Loss: 0.00040492
Iteration 14/25 | Loss: 0.00040492
Iteration 15/25 | Loss: 0.00040492
Iteration 16/25 | Loss: 0.00040492
Iteration 17/25 | Loss: 0.00040492
Iteration 18/25 | Loss: 0.00040492
Iteration 19/25 | Loss: 0.00040492
Iteration 20/25 | Loss: 0.00040492
Iteration 21/25 | Loss: 0.00040492
Iteration 22/25 | Loss: 0.00040492
Iteration 23/25 | Loss: 0.00040492
Iteration 24/25 | Loss: 0.00040492
Iteration 25/25 | Loss: 0.00040492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040492
Iteration 2/1000 | Loss: 0.00003660
Iteration 3/1000 | Loss: 0.00002218
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001677
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001621
Iteration 11/1000 | Loss: 0.00001597
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001577
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001573
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001566
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001560
Iteration 24/1000 | Loss: 0.00001560
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001554
Iteration 28/1000 | Loss: 0.00001553
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001550
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001545
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001543
Iteration 39/1000 | Loss: 0.00001542
Iteration 40/1000 | Loss: 0.00001540
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001538
Iteration 46/1000 | Loss: 0.00001538
Iteration 47/1000 | Loss: 0.00001538
Iteration 48/1000 | Loss: 0.00001538
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001537
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001536
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001533
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001532
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001530
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001529
Iteration 89/1000 | Loss: 0.00001529
Iteration 90/1000 | Loss: 0.00001529
Iteration 91/1000 | Loss: 0.00001529
Iteration 92/1000 | Loss: 0.00001529
Iteration 93/1000 | Loss: 0.00001529
Iteration 94/1000 | Loss: 0.00001529
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001528
Iteration 97/1000 | Loss: 0.00001528
Iteration 98/1000 | Loss: 0.00001528
Iteration 99/1000 | Loss: 0.00001528
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001527
Iteration 109/1000 | Loss: 0.00001527
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001525
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001525
Iteration 124/1000 | Loss: 0.00001525
Iteration 125/1000 | Loss: 0.00001525
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001525
Iteration 129/1000 | Loss: 0.00001525
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.524543040432036e-05, 1.524543040432036e-05, 1.524543040432036e-05, 1.524543040432036e-05, 1.524543040432036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.524543040432036e-05

Optimization complete. Final v2v error: 3.3337762355804443 mm

Highest mean error: 3.757622241973877 mm for frame 49

Lowest mean error: 2.971173048019409 mm for frame 98

Saving results

Total time: 38.58224177360535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869781
Iteration 2/25 | Loss: 0.00118003
Iteration 3/25 | Loss: 0.00091196
Iteration 4/25 | Loss: 0.00085421
Iteration 5/25 | Loss: 0.00084455
Iteration 6/25 | Loss: 0.00084234
Iteration 7/25 | Loss: 0.00084197
Iteration 8/25 | Loss: 0.00084197
Iteration 9/25 | Loss: 0.00084197
Iteration 10/25 | Loss: 0.00084197
Iteration 11/25 | Loss: 0.00084197
Iteration 12/25 | Loss: 0.00084197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008419731166213751, 0.0008419731166213751, 0.0008419731166213751, 0.0008419731166213751, 0.0008419731166213751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008419731166213751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05953896
Iteration 2/25 | Loss: 0.00052972
Iteration 3/25 | Loss: 0.00052971
Iteration 4/25 | Loss: 0.00052971
Iteration 5/25 | Loss: 0.00052971
Iteration 6/25 | Loss: 0.00052971
Iteration 7/25 | Loss: 0.00052971
Iteration 8/25 | Loss: 0.00052971
Iteration 9/25 | Loss: 0.00052971
Iteration 10/25 | Loss: 0.00052971
Iteration 11/25 | Loss: 0.00052971
Iteration 12/25 | Loss: 0.00052971
Iteration 13/25 | Loss: 0.00052971
Iteration 14/25 | Loss: 0.00052971
Iteration 15/25 | Loss: 0.00052971
Iteration 16/25 | Loss: 0.00052971
Iteration 17/25 | Loss: 0.00052971
Iteration 18/25 | Loss: 0.00052971
Iteration 19/25 | Loss: 0.00052971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005297066527418792, 0.0005297066527418792, 0.0005297066527418792, 0.0005297066527418792, 0.0005297066527418792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005297066527418792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052971
Iteration 2/1000 | Loss: 0.00008415
Iteration 3/1000 | Loss: 0.00005550
Iteration 4/1000 | Loss: 0.00004746
Iteration 5/1000 | Loss: 0.00004341
Iteration 6/1000 | Loss: 0.00004000
Iteration 7/1000 | Loss: 0.00003773
Iteration 8/1000 | Loss: 0.00003634
Iteration 9/1000 | Loss: 0.00003574
Iteration 10/1000 | Loss: 0.00003526
Iteration 11/1000 | Loss: 0.00003499
Iteration 12/1000 | Loss: 0.00003460
Iteration 13/1000 | Loss: 0.00003439
Iteration 14/1000 | Loss: 0.00003434
Iteration 15/1000 | Loss: 0.00003430
Iteration 16/1000 | Loss: 0.00003424
Iteration 17/1000 | Loss: 0.00003417
Iteration 18/1000 | Loss: 0.00003416
Iteration 19/1000 | Loss: 0.00003416
Iteration 20/1000 | Loss: 0.00003416
Iteration 21/1000 | Loss: 0.00003415
Iteration 22/1000 | Loss: 0.00003415
Iteration 23/1000 | Loss: 0.00003414
Iteration 24/1000 | Loss: 0.00003414
Iteration 25/1000 | Loss: 0.00003414
Iteration 26/1000 | Loss: 0.00003414
Iteration 27/1000 | Loss: 0.00003414
Iteration 28/1000 | Loss: 0.00003414
Iteration 29/1000 | Loss: 0.00003414
Iteration 30/1000 | Loss: 0.00003414
Iteration 31/1000 | Loss: 0.00003414
Iteration 32/1000 | Loss: 0.00003414
Iteration 33/1000 | Loss: 0.00003413
Iteration 34/1000 | Loss: 0.00003413
Iteration 35/1000 | Loss: 0.00003413
Iteration 36/1000 | Loss: 0.00003413
Iteration 37/1000 | Loss: 0.00003410
Iteration 38/1000 | Loss: 0.00003410
Iteration 39/1000 | Loss: 0.00003410
Iteration 40/1000 | Loss: 0.00003410
Iteration 41/1000 | Loss: 0.00003409
Iteration 42/1000 | Loss: 0.00003409
Iteration 43/1000 | Loss: 0.00003409
Iteration 44/1000 | Loss: 0.00003409
Iteration 45/1000 | Loss: 0.00003408
Iteration 46/1000 | Loss: 0.00003407
Iteration 47/1000 | Loss: 0.00003407
Iteration 48/1000 | Loss: 0.00003407
Iteration 49/1000 | Loss: 0.00003407
Iteration 50/1000 | Loss: 0.00003407
Iteration 51/1000 | Loss: 0.00003406
Iteration 52/1000 | Loss: 0.00003406
Iteration 53/1000 | Loss: 0.00003406
Iteration 54/1000 | Loss: 0.00003405
Iteration 55/1000 | Loss: 0.00003405
Iteration 56/1000 | Loss: 0.00003405
Iteration 57/1000 | Loss: 0.00003405
Iteration 58/1000 | Loss: 0.00003405
Iteration 59/1000 | Loss: 0.00003405
Iteration 60/1000 | Loss: 0.00003405
Iteration 61/1000 | Loss: 0.00003405
Iteration 62/1000 | Loss: 0.00003405
Iteration 63/1000 | Loss: 0.00003405
Iteration 64/1000 | Loss: 0.00003405
Iteration 65/1000 | Loss: 0.00003404
Iteration 66/1000 | Loss: 0.00003404
Iteration 67/1000 | Loss: 0.00003404
Iteration 68/1000 | Loss: 0.00003404
Iteration 69/1000 | Loss: 0.00003404
Iteration 70/1000 | Loss: 0.00003404
Iteration 71/1000 | Loss: 0.00003404
Iteration 72/1000 | Loss: 0.00003404
Iteration 73/1000 | Loss: 0.00003403
Iteration 74/1000 | Loss: 0.00003403
Iteration 75/1000 | Loss: 0.00003403
Iteration 76/1000 | Loss: 0.00003403
Iteration 77/1000 | Loss: 0.00003402
Iteration 78/1000 | Loss: 0.00003402
Iteration 79/1000 | Loss: 0.00003402
Iteration 80/1000 | Loss: 0.00003402
Iteration 81/1000 | Loss: 0.00003401
Iteration 82/1000 | Loss: 0.00003401
Iteration 83/1000 | Loss: 0.00003401
Iteration 84/1000 | Loss: 0.00003401
Iteration 85/1000 | Loss: 0.00003401
Iteration 86/1000 | Loss: 0.00003401
Iteration 87/1000 | Loss: 0.00003401
Iteration 88/1000 | Loss: 0.00003401
Iteration 89/1000 | Loss: 0.00003401
Iteration 90/1000 | Loss: 0.00003401
Iteration 91/1000 | Loss: 0.00003400
Iteration 92/1000 | Loss: 0.00003400
Iteration 93/1000 | Loss: 0.00003400
Iteration 94/1000 | Loss: 0.00003400
Iteration 95/1000 | Loss: 0.00003400
Iteration 96/1000 | Loss: 0.00003400
Iteration 97/1000 | Loss: 0.00003400
Iteration 98/1000 | Loss: 0.00003400
Iteration 99/1000 | Loss: 0.00003400
Iteration 100/1000 | Loss: 0.00003400
Iteration 101/1000 | Loss: 0.00003400
Iteration 102/1000 | Loss: 0.00003400
Iteration 103/1000 | Loss: 0.00003400
Iteration 104/1000 | Loss: 0.00003400
Iteration 105/1000 | Loss: 0.00003400
Iteration 106/1000 | Loss: 0.00003400
Iteration 107/1000 | Loss: 0.00003400
Iteration 108/1000 | Loss: 0.00003400
Iteration 109/1000 | Loss: 0.00003400
Iteration 110/1000 | Loss: 0.00003400
Iteration 111/1000 | Loss: 0.00003400
Iteration 112/1000 | Loss: 0.00003400
Iteration 113/1000 | Loss: 0.00003400
Iteration 114/1000 | Loss: 0.00003400
Iteration 115/1000 | Loss: 0.00003400
Iteration 116/1000 | Loss: 0.00003400
Iteration 117/1000 | Loss: 0.00003400
Iteration 118/1000 | Loss: 0.00003400
Iteration 119/1000 | Loss: 0.00003400
Iteration 120/1000 | Loss: 0.00003400
Iteration 121/1000 | Loss: 0.00003400
Iteration 122/1000 | Loss: 0.00003400
Iteration 123/1000 | Loss: 0.00003400
Iteration 124/1000 | Loss: 0.00003400
Iteration 125/1000 | Loss: 0.00003400
Iteration 126/1000 | Loss: 0.00003400
Iteration 127/1000 | Loss: 0.00003400
Iteration 128/1000 | Loss: 0.00003400
Iteration 129/1000 | Loss: 0.00003400
Iteration 130/1000 | Loss: 0.00003400
Iteration 131/1000 | Loss: 0.00003400
Iteration 132/1000 | Loss: 0.00003400
Iteration 133/1000 | Loss: 0.00003400
Iteration 134/1000 | Loss: 0.00003400
Iteration 135/1000 | Loss: 0.00003400
Iteration 136/1000 | Loss: 0.00003400
Iteration 137/1000 | Loss: 0.00003400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [3.4000619052676484e-05, 3.4000619052676484e-05, 3.4000619052676484e-05, 3.4000619052676484e-05, 3.4000619052676484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4000619052676484e-05

Optimization complete. Final v2v error: 4.884425163269043 mm

Highest mean error: 5.355029106140137 mm for frame 44

Lowest mean error: 4.482089996337891 mm for frame 83

Saving results

Total time: 36.781861543655396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404095
Iteration 2/25 | Loss: 0.00083780
Iteration 3/25 | Loss: 0.00067279
Iteration 4/25 | Loss: 0.00064578
Iteration 5/25 | Loss: 0.00064055
Iteration 6/25 | Loss: 0.00063906
Iteration 7/25 | Loss: 0.00063859
Iteration 8/25 | Loss: 0.00063859
Iteration 9/25 | Loss: 0.00063859
Iteration 10/25 | Loss: 0.00063859
Iteration 11/25 | Loss: 0.00063859
Iteration 12/25 | Loss: 0.00063859
Iteration 13/25 | Loss: 0.00063859
Iteration 14/25 | Loss: 0.00063859
Iteration 15/25 | Loss: 0.00063859
Iteration 16/25 | Loss: 0.00063859
Iteration 17/25 | Loss: 0.00063859
Iteration 18/25 | Loss: 0.00063859
Iteration 19/25 | Loss: 0.00063859
Iteration 20/25 | Loss: 0.00063859
Iteration 21/25 | Loss: 0.00063859
Iteration 22/25 | Loss: 0.00063859
Iteration 23/25 | Loss: 0.00063859
Iteration 24/25 | Loss: 0.00063859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006385864107869565, 0.0006385864107869565, 0.0006385864107869565, 0.0006385864107869565, 0.0006385864107869565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006385864107869565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49627948
Iteration 2/25 | Loss: 0.00038735
Iteration 3/25 | Loss: 0.00038735
Iteration 4/25 | Loss: 0.00038735
Iteration 5/25 | Loss: 0.00038734
Iteration 6/25 | Loss: 0.00038734
Iteration 7/25 | Loss: 0.00038734
Iteration 8/25 | Loss: 0.00038734
Iteration 9/25 | Loss: 0.00038734
Iteration 10/25 | Loss: 0.00038734
Iteration 11/25 | Loss: 0.00038734
Iteration 12/25 | Loss: 0.00038734
Iteration 13/25 | Loss: 0.00038734
Iteration 14/25 | Loss: 0.00038734
Iteration 15/25 | Loss: 0.00038734
Iteration 16/25 | Loss: 0.00038734
Iteration 17/25 | Loss: 0.00038734
Iteration 18/25 | Loss: 0.00038734
Iteration 19/25 | Loss: 0.00038734
Iteration 20/25 | Loss: 0.00038734
Iteration 21/25 | Loss: 0.00038734
Iteration 22/25 | Loss: 0.00038734
Iteration 23/25 | Loss: 0.00038734
Iteration 24/25 | Loss: 0.00038734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00038734276313334703, 0.00038734276313334703, 0.00038734276313334703, 0.00038734276313334703, 0.00038734276313334703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038734276313334703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038734
Iteration 2/1000 | Loss: 0.00003149
Iteration 3/1000 | Loss: 0.00002117
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001436
Iteration 6/1000 | Loss: 0.00001298
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001229
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001215
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001179
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001173
Iteration 20/1000 | Loss: 0.00001173
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001170
Iteration 23/1000 | Loss: 0.00001169
Iteration 24/1000 | Loss: 0.00001169
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001165
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001165
Iteration 36/1000 | Loss: 0.00001163
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001161
Iteration 41/1000 | Loss: 0.00001160
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001149
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001147
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001144
Iteration 73/1000 | Loss: 0.00001144
Iteration 74/1000 | Loss: 0.00001143
Iteration 75/1000 | Loss: 0.00001143
Iteration 76/1000 | Loss: 0.00001143
Iteration 77/1000 | Loss: 0.00001143
Iteration 78/1000 | Loss: 0.00001143
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001141
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001141
Iteration 86/1000 | Loss: 0.00001141
Iteration 87/1000 | Loss: 0.00001141
Iteration 88/1000 | Loss: 0.00001141
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001140
Iteration 93/1000 | Loss: 0.00001140
Iteration 94/1000 | Loss: 0.00001140
Iteration 95/1000 | Loss: 0.00001139
Iteration 96/1000 | Loss: 0.00001139
Iteration 97/1000 | Loss: 0.00001139
Iteration 98/1000 | Loss: 0.00001139
Iteration 99/1000 | Loss: 0.00001139
Iteration 100/1000 | Loss: 0.00001139
Iteration 101/1000 | Loss: 0.00001139
Iteration 102/1000 | Loss: 0.00001139
Iteration 103/1000 | Loss: 0.00001138
Iteration 104/1000 | Loss: 0.00001138
Iteration 105/1000 | Loss: 0.00001138
Iteration 106/1000 | Loss: 0.00001138
Iteration 107/1000 | Loss: 0.00001138
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001137
Iteration 121/1000 | Loss: 0.00001137
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001136
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001136
Iteration 130/1000 | Loss: 0.00001136
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001135
Iteration 135/1000 | Loss: 0.00001135
Iteration 136/1000 | Loss: 0.00001135
Iteration 137/1000 | Loss: 0.00001135
Iteration 138/1000 | Loss: 0.00001135
Iteration 139/1000 | Loss: 0.00001135
Iteration 140/1000 | Loss: 0.00001135
Iteration 141/1000 | Loss: 0.00001135
Iteration 142/1000 | Loss: 0.00001135
Iteration 143/1000 | Loss: 0.00001135
Iteration 144/1000 | Loss: 0.00001135
Iteration 145/1000 | Loss: 0.00001135
Iteration 146/1000 | Loss: 0.00001135
Iteration 147/1000 | Loss: 0.00001135
Iteration 148/1000 | Loss: 0.00001135
Iteration 149/1000 | Loss: 0.00001135
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001135
Iteration 152/1000 | Loss: 0.00001135
Iteration 153/1000 | Loss: 0.00001135
Iteration 154/1000 | Loss: 0.00001134
Iteration 155/1000 | Loss: 0.00001134
Iteration 156/1000 | Loss: 0.00001134
Iteration 157/1000 | Loss: 0.00001134
Iteration 158/1000 | Loss: 0.00001134
Iteration 159/1000 | Loss: 0.00001134
Iteration 160/1000 | Loss: 0.00001134
Iteration 161/1000 | Loss: 0.00001134
Iteration 162/1000 | Loss: 0.00001134
Iteration 163/1000 | Loss: 0.00001134
Iteration 164/1000 | Loss: 0.00001134
Iteration 165/1000 | Loss: 0.00001134
Iteration 166/1000 | Loss: 0.00001134
Iteration 167/1000 | Loss: 0.00001134
Iteration 168/1000 | Loss: 0.00001134
Iteration 169/1000 | Loss: 0.00001134
Iteration 170/1000 | Loss: 0.00001134
Iteration 171/1000 | Loss: 0.00001134
Iteration 172/1000 | Loss: 0.00001134
Iteration 173/1000 | Loss: 0.00001134
Iteration 174/1000 | Loss: 0.00001134
Iteration 175/1000 | Loss: 0.00001134
Iteration 176/1000 | Loss: 0.00001133
Iteration 177/1000 | Loss: 0.00001133
Iteration 178/1000 | Loss: 0.00001133
Iteration 179/1000 | Loss: 0.00001133
Iteration 180/1000 | Loss: 0.00001133
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001133
Iteration 183/1000 | Loss: 0.00001133
Iteration 184/1000 | Loss: 0.00001133
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001133
Iteration 187/1000 | Loss: 0.00001133
Iteration 188/1000 | Loss: 0.00001133
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001133
Iteration 194/1000 | Loss: 0.00001133
Iteration 195/1000 | Loss: 0.00001132
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001132
Iteration 207/1000 | Loss: 0.00001132
Iteration 208/1000 | Loss: 0.00001132
Iteration 209/1000 | Loss: 0.00001132
Iteration 210/1000 | Loss: 0.00001132
Iteration 211/1000 | Loss: 0.00001132
Iteration 212/1000 | Loss: 0.00001131
Iteration 213/1000 | Loss: 0.00001131
Iteration 214/1000 | Loss: 0.00001131
Iteration 215/1000 | Loss: 0.00001131
Iteration 216/1000 | Loss: 0.00001131
Iteration 217/1000 | Loss: 0.00001131
Iteration 218/1000 | Loss: 0.00001131
Iteration 219/1000 | Loss: 0.00001131
Iteration 220/1000 | Loss: 0.00001131
Iteration 221/1000 | Loss: 0.00001131
Iteration 222/1000 | Loss: 0.00001131
Iteration 223/1000 | Loss: 0.00001131
Iteration 224/1000 | Loss: 0.00001131
Iteration 225/1000 | Loss: 0.00001130
Iteration 226/1000 | Loss: 0.00001130
Iteration 227/1000 | Loss: 0.00001130
Iteration 228/1000 | Loss: 0.00001130
Iteration 229/1000 | Loss: 0.00001130
Iteration 230/1000 | Loss: 0.00001130
Iteration 231/1000 | Loss: 0.00001130
Iteration 232/1000 | Loss: 0.00001130
Iteration 233/1000 | Loss: 0.00001130
Iteration 234/1000 | Loss: 0.00001130
Iteration 235/1000 | Loss: 0.00001130
Iteration 236/1000 | Loss: 0.00001130
Iteration 237/1000 | Loss: 0.00001130
Iteration 238/1000 | Loss: 0.00001130
Iteration 239/1000 | Loss: 0.00001130
Iteration 240/1000 | Loss: 0.00001130
Iteration 241/1000 | Loss: 0.00001130
Iteration 242/1000 | Loss: 0.00001130
Iteration 243/1000 | Loss: 0.00001130
Iteration 244/1000 | Loss: 0.00001130
Iteration 245/1000 | Loss: 0.00001130
Iteration 246/1000 | Loss: 0.00001130
Iteration 247/1000 | Loss: 0.00001130
Iteration 248/1000 | Loss: 0.00001130
Iteration 249/1000 | Loss: 0.00001130
Iteration 250/1000 | Loss: 0.00001130
Iteration 251/1000 | Loss: 0.00001130
Iteration 252/1000 | Loss: 0.00001130
Iteration 253/1000 | Loss: 0.00001130
Iteration 254/1000 | Loss: 0.00001130
Iteration 255/1000 | Loss: 0.00001130
Iteration 256/1000 | Loss: 0.00001130
Iteration 257/1000 | Loss: 0.00001130
Iteration 258/1000 | Loss: 0.00001130
Iteration 259/1000 | Loss: 0.00001130
Iteration 260/1000 | Loss: 0.00001130
Iteration 261/1000 | Loss: 0.00001130
Iteration 262/1000 | Loss: 0.00001130
Iteration 263/1000 | Loss: 0.00001130
Iteration 264/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.1296968295937404e-05, 1.1296968295937404e-05, 1.1296968295937404e-05, 1.1296968295937404e-05, 1.1296968295937404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1296968295937404e-05

Optimization complete. Final v2v error: 2.881387233734131 mm

Highest mean error: 3.5030722618103027 mm for frame 54

Lowest mean error: 2.5695128440856934 mm for frame 159

Saving results

Total time: 42.783658504486084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413266
Iteration 2/25 | Loss: 0.00080680
Iteration 3/25 | Loss: 0.00068635
Iteration 4/25 | Loss: 0.00067565
Iteration 5/25 | Loss: 0.00067208
Iteration 6/25 | Loss: 0.00067124
Iteration 7/25 | Loss: 0.00067124
Iteration 8/25 | Loss: 0.00067124
Iteration 9/25 | Loss: 0.00067124
Iteration 10/25 | Loss: 0.00067124
Iteration 11/25 | Loss: 0.00067124
Iteration 12/25 | Loss: 0.00067124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006712350877933204, 0.0006712350877933204, 0.0006712350877933204, 0.0006712350877933204, 0.0006712350877933204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006712350877933204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50880885
Iteration 2/25 | Loss: 0.00045986
Iteration 3/25 | Loss: 0.00045986
Iteration 4/25 | Loss: 0.00045986
Iteration 5/25 | Loss: 0.00045986
Iteration 6/25 | Loss: 0.00045985
Iteration 7/25 | Loss: 0.00045985
Iteration 8/25 | Loss: 0.00045985
Iteration 9/25 | Loss: 0.00045985
Iteration 10/25 | Loss: 0.00045985
Iteration 11/25 | Loss: 0.00045985
Iteration 12/25 | Loss: 0.00045985
Iteration 13/25 | Loss: 0.00045985
Iteration 14/25 | Loss: 0.00045985
Iteration 15/25 | Loss: 0.00045985
Iteration 16/25 | Loss: 0.00045985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00045985393808223307, 0.00045985393808223307, 0.00045985393808223307, 0.00045985393808223307, 0.00045985393808223307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045985393808223307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045985
Iteration 2/1000 | Loss: 0.00002615
Iteration 3/1000 | Loss: 0.00002041
Iteration 4/1000 | Loss: 0.00001829
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001576
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001499
Iteration 11/1000 | Loss: 0.00001495
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001480
Iteration 14/1000 | Loss: 0.00001480
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001479
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001479
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001473
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001463
Iteration 56/1000 | Loss: 0.00001463
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001459
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001457
Iteration 88/1000 | Loss: 0.00001457
Iteration 89/1000 | Loss: 0.00001457
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001456
Iteration 101/1000 | Loss: 0.00001456
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001455
Iteration 109/1000 | Loss: 0.00001455
Iteration 110/1000 | Loss: 0.00001455
Iteration 111/1000 | Loss: 0.00001455
Iteration 112/1000 | Loss: 0.00001455
Iteration 113/1000 | Loss: 0.00001455
Iteration 114/1000 | Loss: 0.00001454
Iteration 115/1000 | Loss: 0.00001454
Iteration 116/1000 | Loss: 0.00001454
Iteration 117/1000 | Loss: 0.00001454
Iteration 118/1000 | Loss: 0.00001454
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001454
Iteration 128/1000 | Loss: 0.00001453
Iteration 129/1000 | Loss: 0.00001453
Iteration 130/1000 | Loss: 0.00001453
Iteration 131/1000 | Loss: 0.00001453
Iteration 132/1000 | Loss: 0.00001453
Iteration 133/1000 | Loss: 0.00001453
Iteration 134/1000 | Loss: 0.00001453
Iteration 135/1000 | Loss: 0.00001453
Iteration 136/1000 | Loss: 0.00001453
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001453
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001453
Iteration 141/1000 | Loss: 0.00001453
Iteration 142/1000 | Loss: 0.00001453
Iteration 143/1000 | Loss: 0.00001453
Iteration 144/1000 | Loss: 0.00001453
Iteration 145/1000 | Loss: 0.00001453
Iteration 146/1000 | Loss: 0.00001453
Iteration 147/1000 | Loss: 0.00001453
Iteration 148/1000 | Loss: 0.00001453
Iteration 149/1000 | Loss: 0.00001453
Iteration 150/1000 | Loss: 0.00001452
Iteration 151/1000 | Loss: 0.00001452
Iteration 152/1000 | Loss: 0.00001452
Iteration 153/1000 | Loss: 0.00001452
Iteration 154/1000 | Loss: 0.00001452
Iteration 155/1000 | Loss: 0.00001452
Iteration 156/1000 | Loss: 0.00001452
Iteration 157/1000 | Loss: 0.00001452
Iteration 158/1000 | Loss: 0.00001452
Iteration 159/1000 | Loss: 0.00001452
Iteration 160/1000 | Loss: 0.00001452
Iteration 161/1000 | Loss: 0.00001452
Iteration 162/1000 | Loss: 0.00001452
Iteration 163/1000 | Loss: 0.00001452
Iteration 164/1000 | Loss: 0.00001452
Iteration 165/1000 | Loss: 0.00001452
Iteration 166/1000 | Loss: 0.00001452
Iteration 167/1000 | Loss: 0.00001452
Iteration 168/1000 | Loss: 0.00001452
Iteration 169/1000 | Loss: 0.00001452
Iteration 170/1000 | Loss: 0.00001452
Iteration 171/1000 | Loss: 0.00001452
Iteration 172/1000 | Loss: 0.00001452
Iteration 173/1000 | Loss: 0.00001452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.4524511243507732e-05, 1.4524511243507732e-05, 1.4524511243507732e-05, 1.4524511243507732e-05, 1.4524511243507732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4524511243507732e-05

Optimization complete. Final v2v error: 3.2981672286987305 mm

Highest mean error: 3.555269479751587 mm for frame 43

Lowest mean error: 2.9847042560577393 mm for frame 83

Saving results

Total time: 33.70883631706238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120985
Iteration 2/25 | Loss: 0.01120985
Iteration 3/25 | Loss: 0.01120985
Iteration 4/25 | Loss: 0.01120984
Iteration 5/25 | Loss: 0.01120984
Iteration 6/25 | Loss: 0.01120984
Iteration 7/25 | Loss: 0.01120984
Iteration 8/25 | Loss: 0.01120984
Iteration 9/25 | Loss: 0.01120983
Iteration 10/25 | Loss: 0.01120983
Iteration 11/25 | Loss: 0.01120983
Iteration 12/25 | Loss: 0.01120983
Iteration 13/25 | Loss: 0.01120983
Iteration 14/25 | Loss: 0.01120983
Iteration 15/25 | Loss: 0.01120982
Iteration 16/25 | Loss: 0.01120982
Iteration 17/25 | Loss: 0.01120982
Iteration 18/25 | Loss: 0.01120982
Iteration 19/25 | Loss: 0.01120981
Iteration 20/25 | Loss: 0.01120981
Iteration 21/25 | Loss: 0.01120981
Iteration 22/25 | Loss: 0.01120981
Iteration 23/25 | Loss: 0.01120981
Iteration 24/25 | Loss: 0.01120981
Iteration 25/25 | Loss: 0.01120981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74787652
Iteration 2/25 | Loss: 0.05556105
Iteration 3/25 | Loss: 0.05349274
Iteration 4/25 | Loss: 0.05342025
Iteration 5/25 | Loss: 0.05340545
Iteration 6/25 | Loss: 0.05340545
Iteration 7/25 | Loss: 0.05340545
Iteration 8/25 | Loss: 0.05340545
Iteration 9/25 | Loss: 0.05340545
Iteration 10/25 | Loss: 0.05340545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.05340545251965523, 0.05340545251965523, 0.05340545251965523, 0.05340545251965523, 0.05340545251965523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05340545251965523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05340545
Iteration 2/1000 | Loss: 0.01494870
Iteration 3/1000 | Loss: 0.00526636
Iteration 4/1000 | Loss: 0.00370165
Iteration 5/1000 | Loss: 0.00931157
Iteration 6/1000 | Loss: 0.00265323
Iteration 7/1000 | Loss: 0.00134250
Iteration 8/1000 | Loss: 0.00080537
Iteration 9/1000 | Loss: 0.00091440
Iteration 10/1000 | Loss: 0.00050149
Iteration 11/1000 | Loss: 0.00052043
Iteration 12/1000 | Loss: 0.00027167
Iteration 13/1000 | Loss: 0.00061638
Iteration 14/1000 | Loss: 0.00012046
Iteration 15/1000 | Loss: 0.00088031
Iteration 16/1000 | Loss: 0.00006730
Iteration 17/1000 | Loss: 0.00129418
Iteration 18/1000 | Loss: 0.00085649
Iteration 19/1000 | Loss: 0.00041182
Iteration 20/1000 | Loss: 0.00011834
Iteration 21/1000 | Loss: 0.00033336
Iteration 22/1000 | Loss: 0.00034790
Iteration 23/1000 | Loss: 0.00027323
Iteration 24/1000 | Loss: 0.00122051
Iteration 25/1000 | Loss: 0.00013727
Iteration 26/1000 | Loss: 0.00007861
Iteration 27/1000 | Loss: 0.00007032
Iteration 28/1000 | Loss: 0.00015234
Iteration 29/1000 | Loss: 0.00062032
Iteration 30/1000 | Loss: 0.00011129
Iteration 31/1000 | Loss: 0.00102788
Iteration 32/1000 | Loss: 0.00006854
Iteration 33/1000 | Loss: 0.00018809
Iteration 34/1000 | Loss: 0.00014555
Iteration 35/1000 | Loss: 0.00009160
Iteration 36/1000 | Loss: 0.00009035
Iteration 37/1000 | Loss: 0.00041168
Iteration 38/1000 | Loss: 0.00037250
Iteration 39/1000 | Loss: 0.00005843
Iteration 40/1000 | Loss: 0.00013538
Iteration 41/1000 | Loss: 0.00009072
Iteration 42/1000 | Loss: 0.00003191
Iteration 43/1000 | Loss: 0.00002664
Iteration 44/1000 | Loss: 0.00010932
Iteration 45/1000 | Loss: 0.00002744
Iteration 46/1000 | Loss: 0.00012121
Iteration 47/1000 | Loss: 0.00008414
Iteration 48/1000 | Loss: 0.00014271
Iteration 49/1000 | Loss: 0.00007360
Iteration 50/1000 | Loss: 0.00008608
Iteration 51/1000 | Loss: 0.00002447
Iteration 52/1000 | Loss: 0.00029956
Iteration 53/1000 | Loss: 0.00007899
Iteration 54/1000 | Loss: 0.00004578
Iteration 55/1000 | Loss: 0.00002648
Iteration 56/1000 | Loss: 0.00038591
Iteration 57/1000 | Loss: 0.00119166
Iteration 58/1000 | Loss: 0.00026839
Iteration 59/1000 | Loss: 0.00025645
Iteration 60/1000 | Loss: 0.00009104
Iteration 61/1000 | Loss: 0.00002815
Iteration 62/1000 | Loss: 0.00007304
Iteration 63/1000 | Loss: 0.00003398
Iteration 64/1000 | Loss: 0.00004505
Iteration 65/1000 | Loss: 0.00041588
Iteration 66/1000 | Loss: 0.00041825
Iteration 67/1000 | Loss: 0.00002679
Iteration 68/1000 | Loss: 0.00008429
Iteration 69/1000 | Loss: 0.00002855
Iteration 70/1000 | Loss: 0.00002351
Iteration 71/1000 | Loss: 0.00012516
Iteration 72/1000 | Loss: 0.00013606
Iteration 73/1000 | Loss: 0.00004299
Iteration 74/1000 | Loss: 0.00003597
Iteration 75/1000 | Loss: 0.00009743
Iteration 76/1000 | Loss: 0.00028428
Iteration 77/1000 | Loss: 0.00004396
Iteration 78/1000 | Loss: 0.00005594
Iteration 79/1000 | Loss: 0.00002872
Iteration 80/1000 | Loss: 0.00002251
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00005000
Iteration 83/1000 | Loss: 0.00002235
Iteration 84/1000 | Loss: 0.00004059
Iteration 85/1000 | Loss: 0.00004708
Iteration 86/1000 | Loss: 0.00002621
Iteration 87/1000 | Loss: 0.00002222
Iteration 88/1000 | Loss: 0.00002222
Iteration 89/1000 | Loss: 0.00002222
Iteration 90/1000 | Loss: 0.00002222
Iteration 91/1000 | Loss: 0.00002222
Iteration 92/1000 | Loss: 0.00002222
Iteration 93/1000 | Loss: 0.00002222
Iteration 94/1000 | Loss: 0.00002222
Iteration 95/1000 | Loss: 0.00002222
Iteration 96/1000 | Loss: 0.00002221
Iteration 97/1000 | Loss: 0.00002221
Iteration 98/1000 | Loss: 0.00002220
Iteration 99/1000 | Loss: 0.00002793
Iteration 100/1000 | Loss: 0.00010766
Iteration 101/1000 | Loss: 0.00003492
Iteration 102/1000 | Loss: 0.00030846
Iteration 103/1000 | Loss: 0.00002841
Iteration 104/1000 | Loss: 0.00003829
Iteration 105/1000 | Loss: 0.00002274
Iteration 106/1000 | Loss: 0.00002297
Iteration 107/1000 | Loss: 0.00006151
Iteration 108/1000 | Loss: 0.00003655
Iteration 109/1000 | Loss: 0.00002562
Iteration 110/1000 | Loss: 0.00002288
Iteration 111/1000 | Loss: 0.00002650
Iteration 112/1000 | Loss: 0.00002260
Iteration 113/1000 | Loss: 0.00002830
Iteration 114/1000 | Loss: 0.00048331
Iteration 115/1000 | Loss: 0.00003277
Iteration 116/1000 | Loss: 0.00002792
Iteration 117/1000 | Loss: 0.00002204
Iteration 118/1000 | Loss: 0.00002203
Iteration 119/1000 | Loss: 0.00003390
Iteration 120/1000 | Loss: 0.00004637
Iteration 121/1000 | Loss: 0.00010229
Iteration 122/1000 | Loss: 0.00005871
Iteration 123/1000 | Loss: 0.00003511
Iteration 124/1000 | Loss: 0.00002208
Iteration 125/1000 | Loss: 0.00002739
Iteration 126/1000 | Loss: 0.00002202
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002202
Iteration 130/1000 | Loss: 0.00002202
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002201
Iteration 138/1000 | Loss: 0.00002201
Iteration 139/1000 | Loss: 0.00002201
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002200
Iteration 143/1000 | Loss: 0.00002200
Iteration 144/1000 | Loss: 0.00002200
Iteration 145/1000 | Loss: 0.00002200
Iteration 146/1000 | Loss: 0.00002200
Iteration 147/1000 | Loss: 0.00002895
Iteration 148/1000 | Loss: 0.00002233
Iteration 149/1000 | Loss: 0.00002202
Iteration 150/1000 | Loss: 0.00002202
Iteration 151/1000 | Loss: 0.00002202
Iteration 152/1000 | Loss: 0.00002202
Iteration 153/1000 | Loss: 0.00002201
Iteration 154/1000 | Loss: 0.00002201
Iteration 155/1000 | Loss: 0.00002207
Iteration 156/1000 | Loss: 0.00002200
Iteration 157/1000 | Loss: 0.00002200
Iteration 158/1000 | Loss: 0.00002199
Iteration 159/1000 | Loss: 0.00002199
Iteration 160/1000 | Loss: 0.00002199
Iteration 161/1000 | Loss: 0.00002199
Iteration 162/1000 | Loss: 0.00002199
Iteration 163/1000 | Loss: 0.00002199
Iteration 164/1000 | Loss: 0.00002199
Iteration 165/1000 | Loss: 0.00002199
Iteration 166/1000 | Loss: 0.00002199
Iteration 167/1000 | Loss: 0.00002199
Iteration 168/1000 | Loss: 0.00002198
Iteration 169/1000 | Loss: 0.00002198
Iteration 170/1000 | Loss: 0.00004885
Iteration 171/1000 | Loss: 0.00002496
Iteration 172/1000 | Loss: 0.00002194
Iteration 173/1000 | Loss: 0.00003302
Iteration 174/1000 | Loss: 0.00003468
Iteration 175/1000 | Loss: 0.00002281
Iteration 176/1000 | Loss: 0.00002275
Iteration 177/1000 | Loss: 0.00002192
Iteration 178/1000 | Loss: 0.00002192
Iteration 179/1000 | Loss: 0.00002191
Iteration 180/1000 | Loss: 0.00002191
Iteration 181/1000 | Loss: 0.00002191
Iteration 182/1000 | Loss: 0.00002191
Iteration 183/1000 | Loss: 0.00002191
Iteration 184/1000 | Loss: 0.00002191
Iteration 185/1000 | Loss: 0.00002191
Iteration 186/1000 | Loss: 0.00002191
Iteration 187/1000 | Loss: 0.00002191
Iteration 188/1000 | Loss: 0.00002191
Iteration 189/1000 | Loss: 0.00002191
Iteration 190/1000 | Loss: 0.00002191
Iteration 191/1000 | Loss: 0.00002190
Iteration 192/1000 | Loss: 0.00002190
Iteration 193/1000 | Loss: 0.00002190
Iteration 194/1000 | Loss: 0.00002190
Iteration 195/1000 | Loss: 0.00002190
Iteration 196/1000 | Loss: 0.00002190
Iteration 197/1000 | Loss: 0.00002190
Iteration 198/1000 | Loss: 0.00002190
Iteration 199/1000 | Loss: 0.00002190
Iteration 200/1000 | Loss: 0.00002190
Iteration 201/1000 | Loss: 0.00003108
Iteration 202/1000 | Loss: 0.00002329
Iteration 203/1000 | Loss: 0.00002190
Iteration 204/1000 | Loss: 0.00002189
Iteration 205/1000 | Loss: 0.00002189
Iteration 206/1000 | Loss: 0.00002189
Iteration 207/1000 | Loss: 0.00002189
Iteration 208/1000 | Loss: 0.00002189
Iteration 209/1000 | Loss: 0.00002189
Iteration 210/1000 | Loss: 0.00002189
Iteration 211/1000 | Loss: 0.00002189
Iteration 212/1000 | Loss: 0.00002189
Iteration 213/1000 | Loss: 0.00002189
Iteration 214/1000 | Loss: 0.00002188
Iteration 215/1000 | Loss: 0.00002188
Iteration 216/1000 | Loss: 0.00002187
Iteration 217/1000 | Loss: 0.00002187
Iteration 218/1000 | Loss: 0.00002187
Iteration 219/1000 | Loss: 0.00002186
Iteration 220/1000 | Loss: 0.00002186
Iteration 221/1000 | Loss: 0.00002186
Iteration 222/1000 | Loss: 0.00002185
Iteration 223/1000 | Loss: 0.00005861
Iteration 224/1000 | Loss: 0.00011498
Iteration 225/1000 | Loss: 0.00004155
Iteration 226/1000 | Loss: 0.00002238
Iteration 227/1000 | Loss: 0.00004675
Iteration 228/1000 | Loss: 0.00002598
Iteration 229/1000 | Loss: 0.00002188
Iteration 230/1000 | Loss: 0.00002185
Iteration 231/1000 | Loss: 0.00002184
Iteration 232/1000 | Loss: 0.00002184
Iteration 233/1000 | Loss: 0.00002184
Iteration 234/1000 | Loss: 0.00002184
Iteration 235/1000 | Loss: 0.00002183
Iteration 236/1000 | Loss: 0.00002183
Iteration 237/1000 | Loss: 0.00002183
Iteration 238/1000 | Loss: 0.00002182
Iteration 239/1000 | Loss: 0.00002182
Iteration 240/1000 | Loss: 0.00002182
Iteration 241/1000 | Loss: 0.00002181
Iteration 242/1000 | Loss: 0.00002181
Iteration 243/1000 | Loss: 0.00002181
Iteration 244/1000 | Loss: 0.00002181
Iteration 245/1000 | Loss: 0.00002181
Iteration 246/1000 | Loss: 0.00002375
Iteration 247/1000 | Loss: 0.00002180
Iteration 248/1000 | Loss: 0.00002180
Iteration 249/1000 | Loss: 0.00002180
Iteration 250/1000 | Loss: 0.00002180
Iteration 251/1000 | Loss: 0.00002180
Iteration 252/1000 | Loss: 0.00002180
Iteration 253/1000 | Loss: 0.00002180
Iteration 254/1000 | Loss: 0.00002179
Iteration 255/1000 | Loss: 0.00002179
Iteration 256/1000 | Loss: 0.00002179
Iteration 257/1000 | Loss: 0.00002179
Iteration 258/1000 | Loss: 0.00002179
Iteration 259/1000 | Loss: 0.00005603
Iteration 260/1000 | Loss: 0.00015652
Iteration 261/1000 | Loss: 0.00095448
Iteration 262/1000 | Loss: 0.00189882
Iteration 263/1000 | Loss: 0.00059287
Iteration 264/1000 | Loss: 0.00224947
Iteration 265/1000 | Loss: 0.00015016
Iteration 266/1000 | Loss: 0.00003048
Iteration 267/1000 | Loss: 0.00002761
Iteration 268/1000 | Loss: 0.00002212
Iteration 269/1000 | Loss: 0.00002855
Iteration 270/1000 | Loss: 0.00002206
Iteration 271/1000 | Loss: 0.00007911
Iteration 272/1000 | Loss: 0.00004232
Iteration 273/1000 | Loss: 0.00004963
Iteration 274/1000 | Loss: 0.00036571
Iteration 275/1000 | Loss: 0.00003179
Iteration 276/1000 | Loss: 0.00007312
Iteration 277/1000 | Loss: 0.00003111
Iteration 278/1000 | Loss: 0.00002191
Iteration 279/1000 | Loss: 0.00002185
Iteration 280/1000 | Loss: 0.00002185
Iteration 281/1000 | Loss: 0.00002185
Iteration 282/1000 | Loss: 0.00002185
Iteration 283/1000 | Loss: 0.00002185
Iteration 284/1000 | Loss: 0.00002185
Iteration 285/1000 | Loss: 0.00002185
Iteration 286/1000 | Loss: 0.00002185
Iteration 287/1000 | Loss: 0.00002185
Iteration 288/1000 | Loss: 0.00002185
Iteration 289/1000 | Loss: 0.00002184
Iteration 290/1000 | Loss: 0.00002184
Iteration 291/1000 | Loss: 0.00002184
Iteration 292/1000 | Loss: 0.00002217
Iteration 293/1000 | Loss: 0.00002186
Iteration 294/1000 | Loss: 0.00002186
Iteration 295/1000 | Loss: 0.00002185
Iteration 296/1000 | Loss: 0.00002187
Iteration 297/1000 | Loss: 0.00002187
Iteration 298/1000 | Loss: 0.00002184
Iteration 299/1000 | Loss: 0.00002184
Iteration 300/1000 | Loss: 0.00002184
Iteration 301/1000 | Loss: 0.00002184
Iteration 302/1000 | Loss: 0.00002184
Iteration 303/1000 | Loss: 0.00002184
Iteration 304/1000 | Loss: 0.00002184
Iteration 305/1000 | Loss: 0.00002184
Iteration 306/1000 | Loss: 0.00002184
Iteration 307/1000 | Loss: 0.00002184
Iteration 308/1000 | Loss: 0.00002184
Iteration 309/1000 | Loss: 0.00002184
Iteration 310/1000 | Loss: 0.00002184
Iteration 311/1000 | Loss: 0.00002184
Iteration 312/1000 | Loss: 0.00002184
Iteration 313/1000 | Loss: 0.00002184
Iteration 314/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [2.1835894585819915e-05, 2.1835894585819915e-05, 2.1835894585819915e-05, 2.1835894585819915e-05, 2.1835894585819915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1835894585819915e-05

Optimization complete. Final v2v error: 3.8920886516571045 mm

Highest mean error: 9.704514503479004 mm for frame 14

Lowest mean error: 3.1802382469177246 mm for frame 234

Saving results

Total time: 246.48005509376526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846252
Iteration 2/25 | Loss: 0.00115713
Iteration 3/25 | Loss: 0.00075790
Iteration 4/25 | Loss: 0.00079975
Iteration 5/25 | Loss: 0.00080486
Iteration 6/25 | Loss: 0.00069481
Iteration 7/25 | Loss: 0.00069297
Iteration 8/25 | Loss: 0.00069234
Iteration 9/25 | Loss: 0.00069231
Iteration 10/25 | Loss: 0.00069231
Iteration 11/25 | Loss: 0.00069231
Iteration 12/25 | Loss: 0.00069231
Iteration 13/25 | Loss: 0.00069231
Iteration 14/25 | Loss: 0.00069231
Iteration 15/25 | Loss: 0.00069231
Iteration 16/25 | Loss: 0.00069231
Iteration 17/25 | Loss: 0.00069231
Iteration 18/25 | Loss: 0.00069231
Iteration 19/25 | Loss: 0.00069231
Iteration 20/25 | Loss: 0.00069231
Iteration 21/25 | Loss: 0.00069231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006923148757778108, 0.0006923148757778108, 0.0006923148757778108, 0.0006923148757778108, 0.0006923148757778108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006923148757778108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85632968
Iteration 2/25 | Loss: 0.00044971
Iteration 3/25 | Loss: 0.00044971
Iteration 4/25 | Loss: 0.00044971
Iteration 5/25 | Loss: 0.00044971
Iteration 6/25 | Loss: 0.00044971
Iteration 7/25 | Loss: 0.00044971
Iteration 8/25 | Loss: 0.00044971
Iteration 9/25 | Loss: 0.00044971
Iteration 10/25 | Loss: 0.00044971
Iteration 11/25 | Loss: 0.00044971
Iteration 12/25 | Loss: 0.00044971
Iteration 13/25 | Loss: 0.00044971
Iteration 14/25 | Loss: 0.00044971
Iteration 15/25 | Loss: 0.00044971
Iteration 16/25 | Loss: 0.00044971
Iteration 17/25 | Loss: 0.00044971
Iteration 18/25 | Loss: 0.00044971
Iteration 19/25 | Loss: 0.00044971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004497063928283751, 0.0004497063928283751, 0.0004497063928283751, 0.0004497063928283751, 0.0004497063928283751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004497063928283751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044971
Iteration 2/1000 | Loss: 0.00002798
Iteration 3/1000 | Loss: 0.00002083
Iteration 4/1000 | Loss: 0.00001922
Iteration 5/1000 | Loss: 0.00001803
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001691
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001640
Iteration 16/1000 | Loss: 0.00001640
Iteration 17/1000 | Loss: 0.00001634
Iteration 18/1000 | Loss: 0.00001631
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001624
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001615
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001602
Iteration 39/1000 | Loss: 0.00001602
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001599
Iteration 45/1000 | Loss: 0.00001599
Iteration 46/1000 | Loss: 0.00001599
Iteration 47/1000 | Loss: 0.00001599
Iteration 48/1000 | Loss: 0.00001599
Iteration 49/1000 | Loss: 0.00001599
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [1.599171264388133e-05, 1.599171264388133e-05, 1.599171264388133e-05, 1.599171264388133e-05, 1.599171264388133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.599171264388133e-05

Optimization complete. Final v2v error: 3.4074249267578125 mm

Highest mean error: 3.835869550704956 mm for frame 208

Lowest mean error: 3.098653793334961 mm for frame 82

Saving results

Total time: 36.51781463623047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019618
Iteration 2/25 | Loss: 0.00162077
Iteration 3/25 | Loss: 0.00099276
Iteration 4/25 | Loss: 0.00093848
Iteration 5/25 | Loss: 0.00092102
Iteration 6/25 | Loss: 0.00091627
Iteration 7/25 | Loss: 0.00091519
Iteration 8/25 | Loss: 0.00091511
Iteration 9/25 | Loss: 0.00091511
Iteration 10/25 | Loss: 0.00091511
Iteration 11/25 | Loss: 0.00091511
Iteration 12/25 | Loss: 0.00091511
Iteration 13/25 | Loss: 0.00091511
Iteration 14/25 | Loss: 0.00091511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009151132544502616, 0.0009151132544502616, 0.0009151132544502616, 0.0009151132544502616, 0.0009151132544502616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009151132544502616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60818982
Iteration 2/25 | Loss: 0.00029611
Iteration 3/25 | Loss: 0.00029610
Iteration 4/25 | Loss: 0.00029610
Iteration 5/25 | Loss: 0.00029610
Iteration 6/25 | Loss: 0.00029610
Iteration 7/25 | Loss: 0.00029610
Iteration 8/25 | Loss: 0.00029610
Iteration 9/25 | Loss: 0.00029610
Iteration 10/25 | Loss: 0.00029610
Iteration 11/25 | Loss: 0.00029610
Iteration 12/25 | Loss: 0.00029610
Iteration 13/25 | Loss: 0.00029610
Iteration 14/25 | Loss: 0.00029610
Iteration 15/25 | Loss: 0.00029610
Iteration 16/25 | Loss: 0.00029610
Iteration 17/25 | Loss: 0.00029610
Iteration 18/25 | Loss: 0.00029610
Iteration 19/25 | Loss: 0.00029610
Iteration 20/25 | Loss: 0.00029610
Iteration 21/25 | Loss: 0.00029610
Iteration 22/25 | Loss: 0.00029610
Iteration 23/25 | Loss: 0.00029610
Iteration 24/25 | Loss: 0.00029610
Iteration 25/25 | Loss: 0.00029610

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029610
Iteration 2/1000 | Loss: 0.00006994
Iteration 3/1000 | Loss: 0.00004947
Iteration 4/1000 | Loss: 0.00004270
Iteration 5/1000 | Loss: 0.00004020
Iteration 6/1000 | Loss: 0.00003893
Iteration 7/1000 | Loss: 0.00003770
Iteration 8/1000 | Loss: 0.00003674
Iteration 9/1000 | Loss: 0.00003615
Iteration 10/1000 | Loss: 0.00003577
Iteration 11/1000 | Loss: 0.00003542
Iteration 12/1000 | Loss: 0.00003519
Iteration 13/1000 | Loss: 0.00003495
Iteration 14/1000 | Loss: 0.00003476
Iteration 15/1000 | Loss: 0.00003462
Iteration 16/1000 | Loss: 0.00003444
Iteration 17/1000 | Loss: 0.00003444
Iteration 18/1000 | Loss: 0.00003441
Iteration 19/1000 | Loss: 0.00003426
Iteration 20/1000 | Loss: 0.00003426
Iteration 21/1000 | Loss: 0.00003425
Iteration 22/1000 | Loss: 0.00003423
Iteration 23/1000 | Loss: 0.00003422
Iteration 24/1000 | Loss: 0.00003422
Iteration 25/1000 | Loss: 0.00003417
Iteration 26/1000 | Loss: 0.00003417
Iteration 27/1000 | Loss: 0.00003412
Iteration 28/1000 | Loss: 0.00003411
Iteration 29/1000 | Loss: 0.00003408
Iteration 30/1000 | Loss: 0.00003408
Iteration 31/1000 | Loss: 0.00003406
Iteration 32/1000 | Loss: 0.00003406
Iteration 33/1000 | Loss: 0.00003406
Iteration 34/1000 | Loss: 0.00003406
Iteration 35/1000 | Loss: 0.00003406
Iteration 36/1000 | Loss: 0.00003405
Iteration 37/1000 | Loss: 0.00003405
Iteration 38/1000 | Loss: 0.00003405
Iteration 39/1000 | Loss: 0.00003404
Iteration 40/1000 | Loss: 0.00003403
Iteration 41/1000 | Loss: 0.00003403
Iteration 42/1000 | Loss: 0.00003402
Iteration 43/1000 | Loss: 0.00003402
Iteration 44/1000 | Loss: 0.00003402
Iteration 45/1000 | Loss: 0.00003402
Iteration 46/1000 | Loss: 0.00003402
Iteration 47/1000 | Loss: 0.00003402
Iteration 48/1000 | Loss: 0.00003401
Iteration 49/1000 | Loss: 0.00003401
Iteration 50/1000 | Loss: 0.00003401
Iteration 51/1000 | Loss: 0.00003401
Iteration 52/1000 | Loss: 0.00003401
Iteration 53/1000 | Loss: 0.00003401
Iteration 54/1000 | Loss: 0.00003401
Iteration 55/1000 | Loss: 0.00003401
Iteration 56/1000 | Loss: 0.00003401
Iteration 57/1000 | Loss: 0.00003401
Iteration 58/1000 | Loss: 0.00003401
Iteration 59/1000 | Loss: 0.00003400
Iteration 60/1000 | Loss: 0.00003400
Iteration 61/1000 | Loss: 0.00003400
Iteration 62/1000 | Loss: 0.00003400
Iteration 63/1000 | Loss: 0.00003400
Iteration 64/1000 | Loss: 0.00003400
Iteration 65/1000 | Loss: 0.00003400
Iteration 66/1000 | Loss: 0.00003400
Iteration 67/1000 | Loss: 0.00003400
Iteration 68/1000 | Loss: 0.00003399
Iteration 69/1000 | Loss: 0.00003399
Iteration 70/1000 | Loss: 0.00003399
Iteration 71/1000 | Loss: 0.00003399
Iteration 72/1000 | Loss: 0.00003398
Iteration 73/1000 | Loss: 0.00003398
Iteration 74/1000 | Loss: 0.00003398
Iteration 75/1000 | Loss: 0.00003398
Iteration 76/1000 | Loss: 0.00003397
Iteration 77/1000 | Loss: 0.00003397
Iteration 78/1000 | Loss: 0.00003397
Iteration 79/1000 | Loss: 0.00003397
Iteration 80/1000 | Loss: 0.00003397
Iteration 81/1000 | Loss: 0.00003397
Iteration 82/1000 | Loss: 0.00003397
Iteration 83/1000 | Loss: 0.00003397
Iteration 84/1000 | Loss: 0.00003397
Iteration 85/1000 | Loss: 0.00003397
Iteration 86/1000 | Loss: 0.00003397
Iteration 87/1000 | Loss: 0.00003397
Iteration 88/1000 | Loss: 0.00003397
Iteration 89/1000 | Loss: 0.00003397
Iteration 90/1000 | Loss: 0.00003397
Iteration 91/1000 | Loss: 0.00003397
Iteration 92/1000 | Loss: 0.00003397
Iteration 93/1000 | Loss: 0.00003397
Iteration 94/1000 | Loss: 0.00003397
Iteration 95/1000 | Loss: 0.00003397
Iteration 96/1000 | Loss: 0.00003396
Iteration 97/1000 | Loss: 0.00003396
Iteration 98/1000 | Loss: 0.00003396
Iteration 99/1000 | Loss: 0.00003396
Iteration 100/1000 | Loss: 0.00003396
Iteration 101/1000 | Loss: 0.00003396
Iteration 102/1000 | Loss: 0.00003396
Iteration 103/1000 | Loss: 0.00003396
Iteration 104/1000 | Loss: 0.00003396
Iteration 105/1000 | Loss: 0.00003396
Iteration 106/1000 | Loss: 0.00003396
Iteration 107/1000 | Loss: 0.00003396
Iteration 108/1000 | Loss: 0.00003396
Iteration 109/1000 | Loss: 0.00003396
Iteration 110/1000 | Loss: 0.00003395
Iteration 111/1000 | Loss: 0.00003395
Iteration 112/1000 | Loss: 0.00003395
Iteration 113/1000 | Loss: 0.00003395
Iteration 114/1000 | Loss: 0.00003395
Iteration 115/1000 | Loss: 0.00003395
Iteration 116/1000 | Loss: 0.00003395
Iteration 117/1000 | Loss: 0.00003395
Iteration 118/1000 | Loss: 0.00003395
Iteration 119/1000 | Loss: 0.00003395
Iteration 120/1000 | Loss: 0.00003395
Iteration 121/1000 | Loss: 0.00003395
Iteration 122/1000 | Loss: 0.00003394
Iteration 123/1000 | Loss: 0.00003394
Iteration 124/1000 | Loss: 0.00003394
Iteration 125/1000 | Loss: 0.00003394
Iteration 126/1000 | Loss: 0.00003394
Iteration 127/1000 | Loss: 0.00003394
Iteration 128/1000 | Loss: 0.00003393
Iteration 129/1000 | Loss: 0.00003393
Iteration 130/1000 | Loss: 0.00003393
Iteration 131/1000 | Loss: 0.00003393
Iteration 132/1000 | Loss: 0.00003393
Iteration 133/1000 | Loss: 0.00003393
Iteration 134/1000 | Loss: 0.00003393
Iteration 135/1000 | Loss: 0.00003393
Iteration 136/1000 | Loss: 0.00003393
Iteration 137/1000 | Loss: 0.00003393
Iteration 138/1000 | Loss: 0.00003392
Iteration 139/1000 | Loss: 0.00003392
Iteration 140/1000 | Loss: 0.00003392
Iteration 141/1000 | Loss: 0.00003392
Iteration 142/1000 | Loss: 0.00003392
Iteration 143/1000 | Loss: 0.00003392
Iteration 144/1000 | Loss: 0.00003392
Iteration 145/1000 | Loss: 0.00003392
Iteration 146/1000 | Loss: 0.00003392
Iteration 147/1000 | Loss: 0.00003392
Iteration 148/1000 | Loss: 0.00003392
Iteration 149/1000 | Loss: 0.00003392
Iteration 150/1000 | Loss: 0.00003392
Iteration 151/1000 | Loss: 0.00003392
Iteration 152/1000 | Loss: 0.00003392
Iteration 153/1000 | Loss: 0.00003391
Iteration 154/1000 | Loss: 0.00003391
Iteration 155/1000 | Loss: 0.00003391
Iteration 156/1000 | Loss: 0.00003391
Iteration 157/1000 | Loss: 0.00003391
Iteration 158/1000 | Loss: 0.00003391
Iteration 159/1000 | Loss: 0.00003391
Iteration 160/1000 | Loss: 0.00003391
Iteration 161/1000 | Loss: 0.00003391
Iteration 162/1000 | Loss: 0.00003391
Iteration 163/1000 | Loss: 0.00003391
Iteration 164/1000 | Loss: 0.00003391
Iteration 165/1000 | Loss: 0.00003390
Iteration 166/1000 | Loss: 0.00003390
Iteration 167/1000 | Loss: 0.00003390
Iteration 168/1000 | Loss: 0.00003390
Iteration 169/1000 | Loss: 0.00003390
Iteration 170/1000 | Loss: 0.00003390
Iteration 171/1000 | Loss: 0.00003390
Iteration 172/1000 | Loss: 0.00003390
Iteration 173/1000 | Loss: 0.00003389
Iteration 174/1000 | Loss: 0.00003389
Iteration 175/1000 | Loss: 0.00003389
Iteration 176/1000 | Loss: 0.00003389
Iteration 177/1000 | Loss: 0.00003389
Iteration 178/1000 | Loss: 0.00003389
Iteration 179/1000 | Loss: 0.00003389
Iteration 180/1000 | Loss: 0.00003389
Iteration 181/1000 | Loss: 0.00003389
Iteration 182/1000 | Loss: 0.00003389
Iteration 183/1000 | Loss: 0.00003389
Iteration 184/1000 | Loss: 0.00003389
Iteration 185/1000 | Loss: 0.00003389
Iteration 186/1000 | Loss: 0.00003389
Iteration 187/1000 | Loss: 0.00003389
Iteration 188/1000 | Loss: 0.00003389
Iteration 189/1000 | Loss: 0.00003389
Iteration 190/1000 | Loss: 0.00003389
Iteration 191/1000 | Loss: 0.00003389
Iteration 192/1000 | Loss: 0.00003389
Iteration 193/1000 | Loss: 0.00003389
Iteration 194/1000 | Loss: 0.00003389
Iteration 195/1000 | Loss: 0.00003389
Iteration 196/1000 | Loss: 0.00003389
Iteration 197/1000 | Loss: 0.00003389
Iteration 198/1000 | Loss: 0.00003389
Iteration 199/1000 | Loss: 0.00003389
Iteration 200/1000 | Loss: 0.00003389
Iteration 201/1000 | Loss: 0.00003389
Iteration 202/1000 | Loss: 0.00003389
Iteration 203/1000 | Loss: 0.00003389
Iteration 204/1000 | Loss: 0.00003389
Iteration 205/1000 | Loss: 0.00003389
Iteration 206/1000 | Loss: 0.00003389
Iteration 207/1000 | Loss: 0.00003389
Iteration 208/1000 | Loss: 0.00003389
Iteration 209/1000 | Loss: 0.00003389
Iteration 210/1000 | Loss: 0.00003389
Iteration 211/1000 | Loss: 0.00003389
Iteration 212/1000 | Loss: 0.00003389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [3.388987897778861e-05, 3.388987897778861e-05, 3.388987897778861e-05, 3.388987897778861e-05, 3.388987897778861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.388987897778861e-05

Optimization complete. Final v2v error: 4.853097438812256 mm

Highest mean error: 5.2358574867248535 mm for frame 81

Lowest mean error: 4.319418430328369 mm for frame 40

Saving results

Total time: 48.74285626411438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872322
Iteration 2/25 | Loss: 0.00092173
Iteration 3/25 | Loss: 0.00070674
Iteration 4/25 | Loss: 0.00068518
Iteration 5/25 | Loss: 0.00067947
Iteration 6/25 | Loss: 0.00067769
Iteration 7/25 | Loss: 0.00067722
Iteration 8/25 | Loss: 0.00067722
Iteration 9/25 | Loss: 0.00067722
Iteration 10/25 | Loss: 0.00067722
Iteration 11/25 | Loss: 0.00067722
Iteration 12/25 | Loss: 0.00067722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006772150518372655, 0.0006772150518372655, 0.0006772150518372655, 0.0006772150518372655, 0.0006772150518372655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006772150518372655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60088098
Iteration 2/25 | Loss: 0.00049056
Iteration 3/25 | Loss: 0.00049055
Iteration 4/25 | Loss: 0.00049055
Iteration 5/25 | Loss: 0.00049055
Iteration 6/25 | Loss: 0.00049055
Iteration 7/25 | Loss: 0.00049055
Iteration 8/25 | Loss: 0.00049055
Iteration 9/25 | Loss: 0.00049055
Iteration 10/25 | Loss: 0.00049055
Iteration 11/25 | Loss: 0.00049055
Iteration 12/25 | Loss: 0.00049055
Iteration 13/25 | Loss: 0.00049055
Iteration 14/25 | Loss: 0.00049055
Iteration 15/25 | Loss: 0.00049055
Iteration 16/25 | Loss: 0.00049055
Iteration 17/25 | Loss: 0.00049055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004905485548079014, 0.0004905485548079014, 0.0004905485548079014, 0.0004905485548079014, 0.0004905485548079014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004905485548079014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049055
Iteration 2/1000 | Loss: 0.00002587
Iteration 3/1000 | Loss: 0.00002043
Iteration 4/1000 | Loss: 0.00001708
Iteration 5/1000 | Loss: 0.00001586
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001357
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001324
Iteration 22/1000 | Loss: 0.00001323
Iteration 23/1000 | Loss: 0.00001322
Iteration 24/1000 | Loss: 0.00001321
Iteration 25/1000 | Loss: 0.00001321
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001320
Iteration 29/1000 | Loss: 0.00001320
Iteration 30/1000 | Loss: 0.00001320
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00001320
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001319
Iteration 42/1000 | Loss: 0.00001319
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001318
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001315
Iteration 54/1000 | Loss: 0.00001315
Iteration 55/1000 | Loss: 0.00001315
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001313
Iteration 59/1000 | Loss: 0.00001313
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001312
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001310
Iteration 68/1000 | Loss: 0.00001310
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001309
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001304
Iteration 88/1000 | Loss: 0.00001304
Iteration 89/1000 | Loss: 0.00001304
Iteration 90/1000 | Loss: 0.00001303
Iteration 91/1000 | Loss: 0.00001303
Iteration 92/1000 | Loss: 0.00001302
Iteration 93/1000 | Loss: 0.00001302
Iteration 94/1000 | Loss: 0.00001302
Iteration 95/1000 | Loss: 0.00001301
Iteration 96/1000 | Loss: 0.00001301
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001300
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001299
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00001299
Iteration 107/1000 | Loss: 0.00001299
Iteration 108/1000 | Loss: 0.00001299
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001298
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001297
Iteration 118/1000 | Loss: 0.00001296
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Iteration 122/1000 | Loss: 0.00001296
Iteration 123/1000 | Loss: 0.00001296
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001295
Iteration 128/1000 | Loss: 0.00001295
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001294
Iteration 137/1000 | Loss: 0.00001294
Iteration 138/1000 | Loss: 0.00001294
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001293
Iteration 141/1000 | Loss: 0.00001293
Iteration 142/1000 | Loss: 0.00001293
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001293
Iteration 146/1000 | Loss: 0.00001293
Iteration 147/1000 | Loss: 0.00001293
Iteration 148/1000 | Loss: 0.00001293
Iteration 149/1000 | Loss: 0.00001293
Iteration 150/1000 | Loss: 0.00001293
Iteration 151/1000 | Loss: 0.00001293
Iteration 152/1000 | Loss: 0.00001293
Iteration 153/1000 | Loss: 0.00001293
Iteration 154/1000 | Loss: 0.00001293
Iteration 155/1000 | Loss: 0.00001293
Iteration 156/1000 | Loss: 0.00001292
Iteration 157/1000 | Loss: 0.00001292
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001292
Iteration 161/1000 | Loss: 0.00001292
Iteration 162/1000 | Loss: 0.00001292
Iteration 163/1000 | Loss: 0.00001292
Iteration 164/1000 | Loss: 0.00001292
Iteration 165/1000 | Loss: 0.00001292
Iteration 166/1000 | Loss: 0.00001292
Iteration 167/1000 | Loss: 0.00001292
Iteration 168/1000 | Loss: 0.00001292
Iteration 169/1000 | Loss: 0.00001292
Iteration 170/1000 | Loss: 0.00001292
Iteration 171/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2924916518386453e-05, 1.2924916518386453e-05, 1.2924916518386453e-05, 1.2924916518386453e-05, 1.2924916518386453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2924916518386453e-05

Optimization complete. Final v2v error: 3.094026565551758 mm

Highest mean error: 3.8274831771850586 mm for frame 110

Lowest mean error: 2.673020601272583 mm for frame 0

Saving results

Total time: 38.63888359069824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081771
Iteration 2/25 | Loss: 0.00151318
Iteration 3/25 | Loss: 0.00099403
Iteration 4/25 | Loss: 0.00095808
Iteration 5/25 | Loss: 0.00096159
Iteration 6/25 | Loss: 0.00082435
Iteration 7/25 | Loss: 0.00084217
Iteration 8/25 | Loss: 0.00076140
Iteration 9/25 | Loss: 0.00074933
Iteration 10/25 | Loss: 0.00075039
Iteration 11/25 | Loss: 0.00074847
Iteration 12/25 | Loss: 0.00074499
Iteration 13/25 | Loss: 0.00073964
Iteration 14/25 | Loss: 0.00074620
Iteration 15/25 | Loss: 0.00072294
Iteration 16/25 | Loss: 0.00071603
Iteration 17/25 | Loss: 0.00072038
Iteration 18/25 | Loss: 0.00071827
Iteration 19/25 | Loss: 0.00071639
Iteration 20/25 | Loss: 0.00071386
Iteration 21/25 | Loss: 0.00071255
Iteration 22/25 | Loss: 0.00071344
Iteration 23/25 | Loss: 0.00071544
Iteration 24/25 | Loss: 0.00071323
Iteration 25/25 | Loss: 0.00071459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63570201
Iteration 2/25 | Loss: 0.00048323
Iteration 3/25 | Loss: 0.00046957
Iteration 4/25 | Loss: 0.00046957
Iteration 5/25 | Loss: 0.00046956
Iteration 6/25 | Loss: 0.00046956
Iteration 7/25 | Loss: 0.00046956
Iteration 8/25 | Loss: 0.00046956
Iteration 9/25 | Loss: 0.00046956
Iteration 10/25 | Loss: 0.00046956
Iteration 11/25 | Loss: 0.00046956
Iteration 12/25 | Loss: 0.00046956
Iteration 13/25 | Loss: 0.00046956
Iteration 14/25 | Loss: 0.00046956
Iteration 15/25 | Loss: 0.00046956
Iteration 16/25 | Loss: 0.00046956
Iteration 17/25 | Loss: 0.00046956
Iteration 18/25 | Loss: 0.00046956
Iteration 19/25 | Loss: 0.00046956
Iteration 20/25 | Loss: 0.00046956
Iteration 21/25 | Loss: 0.00046956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004695633251685649, 0.0004695633251685649, 0.0004695633251685649, 0.0004695633251685649, 0.0004695633251685649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004695633251685649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046956
Iteration 2/1000 | Loss: 0.00006231
Iteration 3/1000 | Loss: 0.00004055
Iteration 4/1000 | Loss: 0.00006087
Iteration 5/1000 | Loss: 0.00006140
Iteration 6/1000 | Loss: 0.00005004
Iteration 7/1000 | Loss: 0.00005336
Iteration 8/1000 | Loss: 0.00004847
Iteration 9/1000 | Loss: 0.00005547
Iteration 10/1000 | Loss: 0.00008253
Iteration 11/1000 | Loss: 0.00004479
Iteration 12/1000 | Loss: 0.00004429
Iteration 13/1000 | Loss: 0.00005900
Iteration 14/1000 | Loss: 0.00006685
Iteration 15/1000 | Loss: 0.00005803
Iteration 16/1000 | Loss: 0.00002424
Iteration 17/1000 | Loss: 0.00002150
Iteration 18/1000 | Loss: 0.00003304
Iteration 19/1000 | Loss: 0.00002907
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00003102
Iteration 23/1000 | Loss: 0.00001940
Iteration 24/1000 | Loss: 0.00001937
Iteration 25/1000 | Loss: 0.00001935
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001934
Iteration 28/1000 | Loss: 0.00001929
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00003969
Iteration 32/1000 | Loss: 0.00001909
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001897
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00003208
Iteration 42/1000 | Loss: 0.00002318
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001890
Iteration 50/1000 | Loss: 0.00001890
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001888
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00003016
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001885
Iteration 62/1000 | Loss: 0.00001885
Iteration 63/1000 | Loss: 0.00001885
Iteration 64/1000 | Loss: 0.00001885
Iteration 65/1000 | Loss: 0.00001885
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001884
Iteration 69/1000 | Loss: 0.00001884
Iteration 70/1000 | Loss: 0.00001884
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001882
Iteration 81/1000 | Loss: 0.00001882
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001881
Iteration 86/1000 | Loss: 0.00001881
Iteration 87/1000 | Loss: 0.00001880
Iteration 88/1000 | Loss: 0.00001880
Iteration 89/1000 | Loss: 0.00001880
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001880
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001880
Iteration 96/1000 | Loss: 0.00001880
Iteration 97/1000 | Loss: 0.00001880
Iteration 98/1000 | Loss: 0.00001879
Iteration 99/1000 | Loss: 0.00001879
Iteration 100/1000 | Loss: 0.00001879
Iteration 101/1000 | Loss: 0.00001879
Iteration 102/1000 | Loss: 0.00001879
Iteration 103/1000 | Loss: 0.00001879
Iteration 104/1000 | Loss: 0.00001879
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001878
Iteration 108/1000 | Loss: 0.00001878
Iteration 109/1000 | Loss: 0.00001878
Iteration 110/1000 | Loss: 0.00001878
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001877
Iteration 113/1000 | Loss: 0.00001877
Iteration 114/1000 | Loss: 0.00001877
Iteration 115/1000 | Loss: 0.00001877
Iteration 116/1000 | Loss: 0.00001876
Iteration 117/1000 | Loss: 0.00001876
Iteration 118/1000 | Loss: 0.00001876
Iteration 119/1000 | Loss: 0.00001876
Iteration 120/1000 | Loss: 0.00001875
Iteration 121/1000 | Loss: 0.00001875
Iteration 122/1000 | Loss: 0.00001875
Iteration 123/1000 | Loss: 0.00001875
Iteration 124/1000 | Loss: 0.00001875
Iteration 125/1000 | Loss: 0.00001874
Iteration 126/1000 | Loss: 0.00001874
Iteration 127/1000 | Loss: 0.00001874
Iteration 128/1000 | Loss: 0.00001874
Iteration 129/1000 | Loss: 0.00001874
Iteration 130/1000 | Loss: 0.00001874
Iteration 131/1000 | Loss: 0.00001874
Iteration 132/1000 | Loss: 0.00001874
Iteration 133/1000 | Loss: 0.00001874
Iteration 134/1000 | Loss: 0.00001873
Iteration 135/1000 | Loss: 0.00001873
Iteration 136/1000 | Loss: 0.00001873
Iteration 137/1000 | Loss: 0.00001873
Iteration 138/1000 | Loss: 0.00001873
Iteration 139/1000 | Loss: 0.00001873
Iteration 140/1000 | Loss: 0.00001873
Iteration 141/1000 | Loss: 0.00001873
Iteration 142/1000 | Loss: 0.00001873
Iteration 143/1000 | Loss: 0.00001873
Iteration 144/1000 | Loss: 0.00001873
Iteration 145/1000 | Loss: 0.00001873
Iteration 146/1000 | Loss: 0.00001873
Iteration 147/1000 | Loss: 0.00001873
Iteration 148/1000 | Loss: 0.00001873
Iteration 149/1000 | Loss: 0.00001873
Iteration 150/1000 | Loss: 0.00001873
Iteration 151/1000 | Loss: 0.00001873
Iteration 152/1000 | Loss: 0.00001873
Iteration 153/1000 | Loss: 0.00001873
Iteration 154/1000 | Loss: 0.00001873
Iteration 155/1000 | Loss: 0.00001873
Iteration 156/1000 | Loss: 0.00001873
Iteration 157/1000 | Loss: 0.00001873
Iteration 158/1000 | Loss: 0.00001873
Iteration 159/1000 | Loss: 0.00001873
Iteration 160/1000 | Loss: 0.00001873
Iteration 161/1000 | Loss: 0.00001873
Iteration 162/1000 | Loss: 0.00001873
Iteration 163/1000 | Loss: 0.00001873
Iteration 164/1000 | Loss: 0.00001873
Iteration 165/1000 | Loss: 0.00001873
Iteration 166/1000 | Loss: 0.00001873
Iteration 167/1000 | Loss: 0.00001873
Iteration 168/1000 | Loss: 0.00001873
Iteration 169/1000 | Loss: 0.00001873
Iteration 170/1000 | Loss: 0.00001873
Iteration 171/1000 | Loss: 0.00001873
Iteration 172/1000 | Loss: 0.00001873
Iteration 173/1000 | Loss: 0.00001873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.8727216229308397e-05, 1.8727216229308397e-05, 1.8727216229308397e-05, 1.8727216229308397e-05, 1.8727216229308397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8727216229308397e-05

Optimization complete. Final v2v error: 3.712562084197998 mm

Highest mean error: 4.075550079345703 mm for frame 61

Lowest mean error: 3.420402765274048 mm for frame 0

Saving results

Total time: 95.17439198493958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768651
Iteration 2/25 | Loss: 0.00158052
Iteration 3/25 | Loss: 0.00144293
Iteration 4/25 | Loss: 0.00141125
Iteration 5/25 | Loss: 0.00139939
Iteration 6/25 | Loss: 0.00139726
Iteration 7/25 | Loss: 0.00139705
Iteration 8/25 | Loss: 0.00139705
Iteration 9/25 | Loss: 0.00139705
Iteration 10/25 | Loss: 0.00139705
Iteration 11/25 | Loss: 0.00139705
Iteration 12/25 | Loss: 0.00139705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013970547588542104, 0.0013970547588542104, 0.0013970547588542104, 0.0013970547588542104, 0.0013970547588542104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013970547588542104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87684667
Iteration 2/25 | Loss: 0.00374716
Iteration 3/25 | Loss: 0.00374715
Iteration 4/25 | Loss: 0.00374715
Iteration 5/25 | Loss: 0.00374715
Iteration 6/25 | Loss: 0.00374715
Iteration 7/25 | Loss: 0.00374715
Iteration 8/25 | Loss: 0.00374715
Iteration 9/25 | Loss: 0.00374715
Iteration 10/25 | Loss: 0.00374715
Iteration 11/25 | Loss: 0.00374715
Iteration 12/25 | Loss: 0.00374715
Iteration 13/25 | Loss: 0.00374715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003747148672118783, 0.003747148672118783, 0.003747148672118783, 0.003747148672118783, 0.003747148672118783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003747148672118783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374715
Iteration 2/1000 | Loss: 0.00006398
Iteration 3/1000 | Loss: 0.00004348
Iteration 4/1000 | Loss: 0.00003627
Iteration 5/1000 | Loss: 0.00003329
Iteration 6/1000 | Loss: 0.00003164
Iteration 7/1000 | Loss: 0.00003044
Iteration 8/1000 | Loss: 0.00002942
Iteration 9/1000 | Loss: 0.00002873
Iteration 10/1000 | Loss: 0.00002827
Iteration 11/1000 | Loss: 0.00002797
Iteration 12/1000 | Loss: 0.00002772
Iteration 13/1000 | Loss: 0.00002748
Iteration 14/1000 | Loss: 0.00002729
Iteration 15/1000 | Loss: 0.00002727
Iteration 16/1000 | Loss: 0.00002726
Iteration 17/1000 | Loss: 0.00002721
Iteration 18/1000 | Loss: 0.00002712
Iteration 19/1000 | Loss: 0.00002704
Iteration 20/1000 | Loss: 0.00002700
Iteration 21/1000 | Loss: 0.00002699
Iteration 22/1000 | Loss: 0.00002698
Iteration 23/1000 | Loss: 0.00002698
Iteration 24/1000 | Loss: 0.00002697
Iteration 25/1000 | Loss: 0.00002696
Iteration 26/1000 | Loss: 0.00002695
Iteration 27/1000 | Loss: 0.00002694
Iteration 28/1000 | Loss: 0.00002693
Iteration 29/1000 | Loss: 0.00002693
Iteration 30/1000 | Loss: 0.00002692
Iteration 31/1000 | Loss: 0.00002690
Iteration 32/1000 | Loss: 0.00002689
Iteration 33/1000 | Loss: 0.00002688
Iteration 34/1000 | Loss: 0.00002688
Iteration 35/1000 | Loss: 0.00002687
Iteration 36/1000 | Loss: 0.00002686
Iteration 37/1000 | Loss: 0.00002686
Iteration 38/1000 | Loss: 0.00002686
Iteration 39/1000 | Loss: 0.00002685
Iteration 40/1000 | Loss: 0.00002685
Iteration 41/1000 | Loss: 0.00002683
Iteration 42/1000 | Loss: 0.00002681
Iteration 43/1000 | Loss: 0.00002680
Iteration 44/1000 | Loss: 0.00002680
Iteration 45/1000 | Loss: 0.00002680
Iteration 46/1000 | Loss: 0.00002680
Iteration 47/1000 | Loss: 0.00002679
Iteration 48/1000 | Loss: 0.00002678
Iteration 49/1000 | Loss: 0.00002677
Iteration 50/1000 | Loss: 0.00002674
Iteration 51/1000 | Loss: 0.00002673
Iteration 52/1000 | Loss: 0.00002673
Iteration 53/1000 | Loss: 0.00002672
Iteration 54/1000 | Loss: 0.00002671
Iteration 55/1000 | Loss: 0.00002671
Iteration 56/1000 | Loss: 0.00002671
Iteration 57/1000 | Loss: 0.00002671
Iteration 58/1000 | Loss: 0.00002670
Iteration 59/1000 | Loss: 0.00002670
Iteration 60/1000 | Loss: 0.00002670
Iteration 61/1000 | Loss: 0.00002670
Iteration 62/1000 | Loss: 0.00002669
Iteration 63/1000 | Loss: 0.00002669
Iteration 64/1000 | Loss: 0.00002669
Iteration 65/1000 | Loss: 0.00002669
Iteration 66/1000 | Loss: 0.00002668
Iteration 67/1000 | Loss: 0.00002668
Iteration 68/1000 | Loss: 0.00002668
Iteration 69/1000 | Loss: 0.00002668
Iteration 70/1000 | Loss: 0.00002667
Iteration 71/1000 | Loss: 0.00002667
Iteration 72/1000 | Loss: 0.00002667
Iteration 73/1000 | Loss: 0.00002666
Iteration 74/1000 | Loss: 0.00002666
Iteration 75/1000 | Loss: 0.00002666
Iteration 76/1000 | Loss: 0.00002665
Iteration 77/1000 | Loss: 0.00002665
Iteration 78/1000 | Loss: 0.00002665
Iteration 79/1000 | Loss: 0.00002665
Iteration 80/1000 | Loss: 0.00002665
Iteration 81/1000 | Loss: 0.00002665
Iteration 82/1000 | Loss: 0.00002665
Iteration 83/1000 | Loss: 0.00002665
Iteration 84/1000 | Loss: 0.00002665
Iteration 85/1000 | Loss: 0.00002665
Iteration 86/1000 | Loss: 0.00002665
Iteration 87/1000 | Loss: 0.00002664
Iteration 88/1000 | Loss: 0.00002664
Iteration 89/1000 | Loss: 0.00002664
Iteration 90/1000 | Loss: 0.00002664
Iteration 91/1000 | Loss: 0.00002663
Iteration 92/1000 | Loss: 0.00002663
Iteration 93/1000 | Loss: 0.00002663
Iteration 94/1000 | Loss: 0.00002662
Iteration 95/1000 | Loss: 0.00002662
Iteration 96/1000 | Loss: 0.00002662
Iteration 97/1000 | Loss: 0.00002662
Iteration 98/1000 | Loss: 0.00002662
Iteration 99/1000 | Loss: 0.00002662
Iteration 100/1000 | Loss: 0.00002662
Iteration 101/1000 | Loss: 0.00002661
Iteration 102/1000 | Loss: 0.00002661
Iteration 103/1000 | Loss: 0.00002661
Iteration 104/1000 | Loss: 0.00002661
Iteration 105/1000 | Loss: 0.00002661
Iteration 106/1000 | Loss: 0.00002661
Iteration 107/1000 | Loss: 0.00002661
Iteration 108/1000 | Loss: 0.00002661
Iteration 109/1000 | Loss: 0.00002660
Iteration 110/1000 | Loss: 0.00002660
Iteration 111/1000 | Loss: 0.00002660
Iteration 112/1000 | Loss: 0.00002660
Iteration 113/1000 | Loss: 0.00002659
Iteration 114/1000 | Loss: 0.00002659
Iteration 115/1000 | Loss: 0.00002659
Iteration 116/1000 | Loss: 0.00002659
Iteration 117/1000 | Loss: 0.00002659
Iteration 118/1000 | Loss: 0.00002659
Iteration 119/1000 | Loss: 0.00002659
Iteration 120/1000 | Loss: 0.00002659
Iteration 121/1000 | Loss: 0.00002659
Iteration 122/1000 | Loss: 0.00002659
Iteration 123/1000 | Loss: 0.00002659
Iteration 124/1000 | Loss: 0.00002659
Iteration 125/1000 | Loss: 0.00002659
Iteration 126/1000 | Loss: 0.00002658
Iteration 127/1000 | Loss: 0.00002658
Iteration 128/1000 | Loss: 0.00002658
Iteration 129/1000 | Loss: 0.00002658
Iteration 130/1000 | Loss: 0.00002658
Iteration 131/1000 | Loss: 0.00002658
Iteration 132/1000 | Loss: 0.00002658
Iteration 133/1000 | Loss: 0.00002658
Iteration 134/1000 | Loss: 0.00002658
Iteration 135/1000 | Loss: 0.00002657
Iteration 136/1000 | Loss: 0.00002657
Iteration 137/1000 | Loss: 0.00002657
Iteration 138/1000 | Loss: 0.00002656
Iteration 139/1000 | Loss: 0.00002656
Iteration 140/1000 | Loss: 0.00002656
Iteration 141/1000 | Loss: 0.00002656
Iteration 142/1000 | Loss: 0.00002656
Iteration 143/1000 | Loss: 0.00002656
Iteration 144/1000 | Loss: 0.00002655
Iteration 145/1000 | Loss: 0.00002655
Iteration 146/1000 | Loss: 0.00002655
Iteration 147/1000 | Loss: 0.00002655
Iteration 148/1000 | Loss: 0.00002655
Iteration 149/1000 | Loss: 0.00002655
Iteration 150/1000 | Loss: 0.00002654
Iteration 151/1000 | Loss: 0.00002654
Iteration 152/1000 | Loss: 0.00002654
Iteration 153/1000 | Loss: 0.00002654
Iteration 154/1000 | Loss: 0.00002653
Iteration 155/1000 | Loss: 0.00002653
Iteration 156/1000 | Loss: 0.00002653
Iteration 157/1000 | Loss: 0.00002653
Iteration 158/1000 | Loss: 0.00002652
Iteration 159/1000 | Loss: 0.00002652
Iteration 160/1000 | Loss: 0.00002652
Iteration 161/1000 | Loss: 0.00002652
Iteration 162/1000 | Loss: 0.00002651
Iteration 163/1000 | Loss: 0.00002651
Iteration 164/1000 | Loss: 0.00002651
Iteration 165/1000 | Loss: 0.00002651
Iteration 166/1000 | Loss: 0.00002651
Iteration 167/1000 | Loss: 0.00002651
Iteration 168/1000 | Loss: 0.00002651
Iteration 169/1000 | Loss: 0.00002651
Iteration 170/1000 | Loss: 0.00002651
Iteration 171/1000 | Loss: 0.00002651
Iteration 172/1000 | Loss: 0.00002651
Iteration 173/1000 | Loss: 0.00002651
Iteration 174/1000 | Loss: 0.00002650
Iteration 175/1000 | Loss: 0.00002650
Iteration 176/1000 | Loss: 0.00002650
Iteration 177/1000 | Loss: 0.00002650
Iteration 178/1000 | Loss: 0.00002650
Iteration 179/1000 | Loss: 0.00002650
Iteration 180/1000 | Loss: 0.00002650
Iteration 181/1000 | Loss: 0.00002650
Iteration 182/1000 | Loss: 0.00002650
Iteration 183/1000 | Loss: 0.00002650
Iteration 184/1000 | Loss: 0.00002650
Iteration 185/1000 | Loss: 0.00002649
Iteration 186/1000 | Loss: 0.00002649
Iteration 187/1000 | Loss: 0.00002649
Iteration 188/1000 | Loss: 0.00002649
Iteration 189/1000 | Loss: 0.00002649
Iteration 190/1000 | Loss: 0.00002649
Iteration 191/1000 | Loss: 0.00002649
Iteration 192/1000 | Loss: 0.00002649
Iteration 193/1000 | Loss: 0.00002649
Iteration 194/1000 | Loss: 0.00002649
Iteration 195/1000 | Loss: 0.00002649
Iteration 196/1000 | Loss: 0.00002649
Iteration 197/1000 | Loss: 0.00002649
Iteration 198/1000 | Loss: 0.00002649
Iteration 199/1000 | Loss: 0.00002649
Iteration 200/1000 | Loss: 0.00002648
Iteration 201/1000 | Loss: 0.00002648
Iteration 202/1000 | Loss: 0.00002648
Iteration 203/1000 | Loss: 0.00002648
Iteration 204/1000 | Loss: 0.00002648
Iteration 205/1000 | Loss: 0.00002648
Iteration 206/1000 | Loss: 0.00002648
Iteration 207/1000 | Loss: 0.00002648
Iteration 208/1000 | Loss: 0.00002648
Iteration 209/1000 | Loss: 0.00002648
Iteration 210/1000 | Loss: 0.00002648
Iteration 211/1000 | Loss: 0.00002648
Iteration 212/1000 | Loss: 0.00002648
Iteration 213/1000 | Loss: 0.00002647
Iteration 214/1000 | Loss: 0.00002647
Iteration 215/1000 | Loss: 0.00002647
Iteration 216/1000 | Loss: 0.00002647
Iteration 217/1000 | Loss: 0.00002647
Iteration 218/1000 | Loss: 0.00002647
Iteration 219/1000 | Loss: 0.00002646
Iteration 220/1000 | Loss: 0.00002646
Iteration 221/1000 | Loss: 0.00002646
Iteration 222/1000 | Loss: 0.00002646
Iteration 223/1000 | Loss: 0.00002646
Iteration 224/1000 | Loss: 0.00002646
Iteration 225/1000 | Loss: 0.00002646
Iteration 226/1000 | Loss: 0.00002646
Iteration 227/1000 | Loss: 0.00002646
Iteration 228/1000 | Loss: 0.00002646
Iteration 229/1000 | Loss: 0.00002646
Iteration 230/1000 | Loss: 0.00002645
Iteration 231/1000 | Loss: 0.00002645
Iteration 232/1000 | Loss: 0.00002645
Iteration 233/1000 | Loss: 0.00002645
Iteration 234/1000 | Loss: 0.00002645
Iteration 235/1000 | Loss: 0.00002645
Iteration 236/1000 | Loss: 0.00002645
Iteration 237/1000 | Loss: 0.00002645
Iteration 238/1000 | Loss: 0.00002645
Iteration 239/1000 | Loss: 0.00002645
Iteration 240/1000 | Loss: 0.00002645
Iteration 241/1000 | Loss: 0.00002645
Iteration 242/1000 | Loss: 0.00002645
Iteration 243/1000 | Loss: 0.00002645
Iteration 244/1000 | Loss: 0.00002645
Iteration 245/1000 | Loss: 0.00002645
Iteration 246/1000 | Loss: 0.00002645
Iteration 247/1000 | Loss: 0.00002645
Iteration 248/1000 | Loss: 0.00002645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.6448382413946092e-05, 2.6448382413946092e-05, 2.6448382413946092e-05, 2.6448382413946092e-05, 2.6448382413946092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6448382413946092e-05

Optimization complete. Final v2v error: 4.350982666015625 mm

Highest mean error: 4.993609428405762 mm for frame 165

Lowest mean error: 3.7579360008239746 mm for frame 70

Saving results

Total time: 56.310391426086426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135822
Iteration 2/25 | Loss: 0.00237354
Iteration 3/25 | Loss: 0.00209823
Iteration 4/25 | Loss: 0.00175072
Iteration 5/25 | Loss: 0.00193952
Iteration 6/25 | Loss: 0.00169619
Iteration 7/25 | Loss: 0.00157714
Iteration 8/25 | Loss: 0.00150782
Iteration 9/25 | Loss: 0.00142648
Iteration 10/25 | Loss: 0.00139954
Iteration 11/25 | Loss: 0.00136353
Iteration 12/25 | Loss: 0.00135847
Iteration 13/25 | Loss: 0.00142066
Iteration 14/25 | Loss: 0.00136241
Iteration 15/25 | Loss: 0.00135574
Iteration 16/25 | Loss: 0.00134200
Iteration 17/25 | Loss: 0.00133961
Iteration 18/25 | Loss: 0.00133656
Iteration 19/25 | Loss: 0.00134351
Iteration 20/25 | Loss: 0.00134125
Iteration 21/25 | Loss: 0.00133506
Iteration 22/25 | Loss: 0.00132942
Iteration 23/25 | Loss: 0.00133200
Iteration 24/25 | Loss: 0.00133140
Iteration 25/25 | Loss: 0.00132588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65657365
Iteration 2/25 | Loss: 0.00233031
Iteration 3/25 | Loss: 0.00233031
Iteration 4/25 | Loss: 0.00231110
Iteration 5/25 | Loss: 0.00231110
Iteration 6/25 | Loss: 0.00231110
Iteration 7/25 | Loss: 0.00231110
Iteration 8/25 | Loss: 0.00231110
Iteration 9/25 | Loss: 0.00231110
Iteration 10/25 | Loss: 0.00231110
Iteration 11/25 | Loss: 0.00231110
Iteration 12/25 | Loss: 0.00231110
Iteration 13/25 | Loss: 0.00231110
Iteration 14/25 | Loss: 0.00231110
Iteration 15/25 | Loss: 0.00231110
Iteration 16/25 | Loss: 0.00231110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023110955953598022, 0.0023110955953598022, 0.0023110955953598022, 0.0023110955953598022, 0.0023110955953598022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023110955953598022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231110
Iteration 2/1000 | Loss: 0.00013175
Iteration 3/1000 | Loss: 0.00009484
Iteration 4/1000 | Loss: 0.00011212
Iteration 5/1000 | Loss: 0.00009370
Iteration 6/1000 | Loss: 0.00028258
Iteration 7/1000 | Loss: 0.00025740
Iteration 8/1000 | Loss: 0.00022026
Iteration 9/1000 | Loss: 0.00019229
Iteration 10/1000 | Loss: 0.00018326
Iteration 11/1000 | Loss: 0.00020874
Iteration 12/1000 | Loss: 0.00018121
Iteration 13/1000 | Loss: 0.00021034
Iteration 14/1000 | Loss: 0.00099398
Iteration 15/1000 | Loss: 0.00021598
Iteration 16/1000 | Loss: 0.00071886
Iteration 17/1000 | Loss: 0.00008666
Iteration 18/1000 | Loss: 0.00006653
Iteration 19/1000 | Loss: 0.00076300
Iteration 20/1000 | Loss: 0.00008042
Iteration 21/1000 | Loss: 0.00030475
Iteration 22/1000 | Loss: 0.00066862
Iteration 23/1000 | Loss: 0.00109394
Iteration 24/1000 | Loss: 0.00174518
Iteration 25/1000 | Loss: 0.00039162
Iteration 26/1000 | Loss: 0.00009600
Iteration 27/1000 | Loss: 0.00007623
Iteration 28/1000 | Loss: 0.00007631
Iteration 29/1000 | Loss: 0.00078341
Iteration 30/1000 | Loss: 0.00059032
Iteration 31/1000 | Loss: 0.00039728
Iteration 32/1000 | Loss: 0.00076785
Iteration 33/1000 | Loss: 0.00032834
Iteration 34/1000 | Loss: 0.00071487
Iteration 35/1000 | Loss: 0.00030969
Iteration 36/1000 | Loss: 0.00012565
Iteration 37/1000 | Loss: 0.00008561
Iteration 38/1000 | Loss: 0.00039746
Iteration 39/1000 | Loss: 0.00299410
Iteration 40/1000 | Loss: 0.00434954
Iteration 41/1000 | Loss: 0.00268452
Iteration 42/1000 | Loss: 0.00171610
Iteration 43/1000 | Loss: 0.00116423
Iteration 44/1000 | Loss: 0.00004830
Iteration 45/1000 | Loss: 0.00003962
Iteration 46/1000 | Loss: 0.00003501
Iteration 47/1000 | Loss: 0.00003266
Iteration 48/1000 | Loss: 0.00003120
Iteration 49/1000 | Loss: 0.00037481
Iteration 50/1000 | Loss: 0.00052665
Iteration 51/1000 | Loss: 0.00032716
Iteration 52/1000 | Loss: 0.00039681
Iteration 53/1000 | Loss: 0.00039115
Iteration 54/1000 | Loss: 0.00046320
Iteration 55/1000 | Loss: 0.00041756
Iteration 56/1000 | Loss: 0.00029445
Iteration 57/1000 | Loss: 0.00034525
Iteration 58/1000 | Loss: 0.00003662
Iteration 59/1000 | Loss: 0.00042138
Iteration 60/1000 | Loss: 0.00037458
Iteration 61/1000 | Loss: 0.00055355
Iteration 62/1000 | Loss: 0.00043830
Iteration 63/1000 | Loss: 0.00028128
Iteration 64/1000 | Loss: 0.00030696
Iteration 65/1000 | Loss: 0.00052695
Iteration 66/1000 | Loss: 0.00003998
Iteration 67/1000 | Loss: 0.00003343
Iteration 68/1000 | Loss: 0.00036621
Iteration 69/1000 | Loss: 0.00033435
Iteration 70/1000 | Loss: 0.00004353
Iteration 71/1000 | Loss: 0.00003648
Iteration 72/1000 | Loss: 0.00029907
Iteration 73/1000 | Loss: 0.00022862
Iteration 74/1000 | Loss: 0.00004045
Iteration 75/1000 | Loss: 0.00003692
Iteration 76/1000 | Loss: 0.00003318
Iteration 77/1000 | Loss: 0.00033204
Iteration 78/1000 | Loss: 0.00004632
Iteration 79/1000 | Loss: 0.00029694
Iteration 80/1000 | Loss: 0.00034407
Iteration 81/1000 | Loss: 0.00003296
Iteration 82/1000 | Loss: 0.00034637
Iteration 83/1000 | Loss: 0.00025053
Iteration 84/1000 | Loss: 0.00003041
Iteration 85/1000 | Loss: 0.00027633
Iteration 86/1000 | Loss: 0.00055337
Iteration 87/1000 | Loss: 0.00014513
Iteration 88/1000 | Loss: 0.00004310
Iteration 89/1000 | Loss: 0.00003692
Iteration 90/1000 | Loss: 0.00003458
Iteration 91/1000 | Loss: 0.00003233
Iteration 92/1000 | Loss: 0.00003107
Iteration 93/1000 | Loss: 0.00003003
Iteration 94/1000 | Loss: 0.00002853
Iteration 95/1000 | Loss: 0.00002774
Iteration 96/1000 | Loss: 0.00002734
Iteration 97/1000 | Loss: 0.00002716
Iteration 98/1000 | Loss: 0.00002711
Iteration 99/1000 | Loss: 0.00002706
Iteration 100/1000 | Loss: 0.00002705
Iteration 101/1000 | Loss: 0.00002704
Iteration 102/1000 | Loss: 0.00002700
Iteration 103/1000 | Loss: 0.00002697
Iteration 104/1000 | Loss: 0.00002693
Iteration 105/1000 | Loss: 0.00002692
Iteration 106/1000 | Loss: 0.00002691
Iteration 107/1000 | Loss: 0.00002691
Iteration 108/1000 | Loss: 0.00002684
Iteration 109/1000 | Loss: 0.00002674
Iteration 110/1000 | Loss: 0.00002673
Iteration 111/1000 | Loss: 0.00002669
Iteration 112/1000 | Loss: 0.00002667
Iteration 113/1000 | Loss: 0.00002667
Iteration 114/1000 | Loss: 0.00002666
Iteration 115/1000 | Loss: 0.00002666
Iteration 116/1000 | Loss: 0.00002666
Iteration 117/1000 | Loss: 0.00002666
Iteration 118/1000 | Loss: 0.00002666
Iteration 119/1000 | Loss: 0.00002665
Iteration 120/1000 | Loss: 0.00002665
Iteration 121/1000 | Loss: 0.00002665
Iteration 122/1000 | Loss: 0.00002664
Iteration 123/1000 | Loss: 0.00002664
Iteration 124/1000 | Loss: 0.00002664
Iteration 125/1000 | Loss: 0.00002663
Iteration 126/1000 | Loss: 0.00002662
Iteration 127/1000 | Loss: 0.00002662
Iteration 128/1000 | Loss: 0.00002662
Iteration 129/1000 | Loss: 0.00002662
Iteration 130/1000 | Loss: 0.00002661
Iteration 131/1000 | Loss: 0.00002661
Iteration 132/1000 | Loss: 0.00002661
Iteration 133/1000 | Loss: 0.00002661
Iteration 134/1000 | Loss: 0.00002660
Iteration 135/1000 | Loss: 0.00002660
Iteration 136/1000 | Loss: 0.00002660
Iteration 137/1000 | Loss: 0.00002660
Iteration 138/1000 | Loss: 0.00002660
Iteration 139/1000 | Loss: 0.00002660
Iteration 140/1000 | Loss: 0.00002660
Iteration 141/1000 | Loss: 0.00002660
Iteration 142/1000 | Loss: 0.00002659
Iteration 143/1000 | Loss: 0.00002659
Iteration 144/1000 | Loss: 0.00002659
Iteration 145/1000 | Loss: 0.00002659
Iteration 146/1000 | Loss: 0.00002659
Iteration 147/1000 | Loss: 0.00002659
Iteration 148/1000 | Loss: 0.00002659
Iteration 149/1000 | Loss: 0.00002659
Iteration 150/1000 | Loss: 0.00002659
Iteration 151/1000 | Loss: 0.00002659
Iteration 152/1000 | Loss: 0.00002659
Iteration 153/1000 | Loss: 0.00002659
Iteration 154/1000 | Loss: 0.00002659
Iteration 155/1000 | Loss: 0.00002658
Iteration 156/1000 | Loss: 0.00002658
Iteration 157/1000 | Loss: 0.00002658
Iteration 158/1000 | Loss: 0.00002658
Iteration 159/1000 | Loss: 0.00002657
Iteration 160/1000 | Loss: 0.00002657
Iteration 161/1000 | Loss: 0.00002657
Iteration 162/1000 | Loss: 0.00002657
Iteration 163/1000 | Loss: 0.00002657
Iteration 164/1000 | Loss: 0.00002657
Iteration 165/1000 | Loss: 0.00002657
Iteration 166/1000 | Loss: 0.00002657
Iteration 167/1000 | Loss: 0.00002657
Iteration 168/1000 | Loss: 0.00002656
Iteration 169/1000 | Loss: 0.00002656
Iteration 170/1000 | Loss: 0.00002656
Iteration 171/1000 | Loss: 0.00002656
Iteration 172/1000 | Loss: 0.00002656
Iteration 173/1000 | Loss: 0.00002656
Iteration 174/1000 | Loss: 0.00002656
Iteration 175/1000 | Loss: 0.00002656
Iteration 176/1000 | Loss: 0.00002656
Iteration 177/1000 | Loss: 0.00002656
Iteration 178/1000 | Loss: 0.00002656
Iteration 179/1000 | Loss: 0.00002656
Iteration 180/1000 | Loss: 0.00002655
Iteration 181/1000 | Loss: 0.00002655
Iteration 182/1000 | Loss: 0.00002655
Iteration 183/1000 | Loss: 0.00002655
Iteration 184/1000 | Loss: 0.00002655
Iteration 185/1000 | Loss: 0.00002655
Iteration 186/1000 | Loss: 0.00002655
Iteration 187/1000 | Loss: 0.00002655
Iteration 188/1000 | Loss: 0.00002655
Iteration 189/1000 | Loss: 0.00002655
Iteration 190/1000 | Loss: 0.00002655
Iteration 191/1000 | Loss: 0.00002655
Iteration 192/1000 | Loss: 0.00002655
Iteration 193/1000 | Loss: 0.00002655
Iteration 194/1000 | Loss: 0.00002655
Iteration 195/1000 | Loss: 0.00002655
Iteration 196/1000 | Loss: 0.00002655
Iteration 197/1000 | Loss: 0.00002655
Iteration 198/1000 | Loss: 0.00002655
Iteration 199/1000 | Loss: 0.00002655
Iteration 200/1000 | Loss: 0.00002655
Iteration 201/1000 | Loss: 0.00002655
Iteration 202/1000 | Loss: 0.00002655
Iteration 203/1000 | Loss: 0.00002655
Iteration 204/1000 | Loss: 0.00002655
Iteration 205/1000 | Loss: 0.00002655
Iteration 206/1000 | Loss: 0.00002655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.6550207621767186e-05, 2.6550207621767186e-05, 2.6550207621767186e-05, 2.6550207621767186e-05, 2.6550207621767186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6550207621767186e-05

Optimization complete. Final v2v error: 4.418162822723389 mm

Highest mean error: 5.204717636108398 mm for frame 60

Lowest mean error: 3.864837646484375 mm for frame 0

Saving results

Total time: 187.57759761810303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918735
Iteration 2/25 | Loss: 0.00163131
Iteration 3/25 | Loss: 0.00129400
Iteration 4/25 | Loss: 0.00126021
Iteration 5/25 | Loss: 0.00125418
Iteration 6/25 | Loss: 0.00125198
Iteration 7/25 | Loss: 0.00125103
Iteration 8/25 | Loss: 0.00125068
Iteration 9/25 | Loss: 0.00125052
Iteration 10/25 | Loss: 0.00125038
Iteration 11/25 | Loss: 0.00125422
Iteration 12/25 | Loss: 0.00125151
Iteration 13/25 | Loss: 0.00124775
Iteration 14/25 | Loss: 0.00124691
Iteration 15/25 | Loss: 0.00124654
Iteration 16/25 | Loss: 0.00124653
Iteration 17/25 | Loss: 0.00124652
Iteration 18/25 | Loss: 0.00124652
Iteration 19/25 | Loss: 0.00124652
Iteration 20/25 | Loss: 0.00124652
Iteration 21/25 | Loss: 0.00124652
Iteration 22/25 | Loss: 0.00124652
Iteration 23/25 | Loss: 0.00124652
Iteration 24/25 | Loss: 0.00124652
Iteration 25/25 | Loss: 0.00124652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60349810
Iteration 2/25 | Loss: 0.00215388
Iteration 3/25 | Loss: 0.00215388
Iteration 4/25 | Loss: 0.00215387
Iteration 5/25 | Loss: 0.00215387
Iteration 6/25 | Loss: 0.00215387
Iteration 7/25 | Loss: 0.00215387
Iteration 8/25 | Loss: 0.00215387
Iteration 9/25 | Loss: 0.00215387
Iteration 10/25 | Loss: 0.00215387
Iteration 11/25 | Loss: 0.00215387
Iteration 12/25 | Loss: 0.00215387
Iteration 13/25 | Loss: 0.00215387
Iteration 14/25 | Loss: 0.00215387
Iteration 15/25 | Loss: 0.00215387
Iteration 16/25 | Loss: 0.00215387
Iteration 17/25 | Loss: 0.00215387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021538736764341593, 0.0021538736764341593, 0.0021538736764341593, 0.0021538736764341593, 0.0021538736764341593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021538736764341593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215387
Iteration 2/1000 | Loss: 0.00004149
Iteration 3/1000 | Loss: 0.00002941
Iteration 4/1000 | Loss: 0.00002636
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00002387
Iteration 7/1000 | Loss: 0.00002326
Iteration 8/1000 | Loss: 0.00002286
Iteration 9/1000 | Loss: 0.00002262
Iteration 10/1000 | Loss: 0.00002236
Iteration 11/1000 | Loss: 0.00002216
Iteration 12/1000 | Loss: 0.00002211
Iteration 13/1000 | Loss: 0.00002204
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002204
Iteration 16/1000 | Loss: 0.00002201
Iteration 17/1000 | Loss: 0.00002199
Iteration 18/1000 | Loss: 0.00002199
Iteration 19/1000 | Loss: 0.00002198
Iteration 20/1000 | Loss: 0.00002195
Iteration 21/1000 | Loss: 0.00002195
Iteration 22/1000 | Loss: 0.00002194
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002191
Iteration 27/1000 | Loss: 0.00002191
Iteration 28/1000 | Loss: 0.00002191
Iteration 29/1000 | Loss: 0.00002190
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002189
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002188
Iteration 36/1000 | Loss: 0.00002188
Iteration 37/1000 | Loss: 0.00002188
Iteration 38/1000 | Loss: 0.00002188
Iteration 39/1000 | Loss: 0.00002188
Iteration 40/1000 | Loss: 0.00002188
Iteration 41/1000 | Loss: 0.00002188
Iteration 42/1000 | Loss: 0.00002188
Iteration 43/1000 | Loss: 0.00002188
Iteration 44/1000 | Loss: 0.00002187
Iteration 45/1000 | Loss: 0.00002187
Iteration 46/1000 | Loss: 0.00002187
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002187
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002187
Iteration 51/1000 | Loss: 0.00002187
Iteration 52/1000 | Loss: 0.00002187
Iteration 53/1000 | Loss: 0.00002186
Iteration 54/1000 | Loss: 0.00002186
Iteration 55/1000 | Loss: 0.00002186
Iteration 56/1000 | Loss: 0.00002186
Iteration 57/1000 | Loss: 0.00002186
Iteration 58/1000 | Loss: 0.00002186
Iteration 59/1000 | Loss: 0.00002186
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00002185
Iteration 62/1000 | Loss: 0.00002185
Iteration 63/1000 | Loss: 0.00002185
Iteration 64/1000 | Loss: 0.00002185
Iteration 65/1000 | Loss: 0.00002184
Iteration 66/1000 | Loss: 0.00002184
Iteration 67/1000 | Loss: 0.00002184
Iteration 68/1000 | Loss: 0.00002184
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00002184
Iteration 71/1000 | Loss: 0.00002184
Iteration 72/1000 | Loss: 0.00002183
Iteration 73/1000 | Loss: 0.00002183
Iteration 74/1000 | Loss: 0.00002183
Iteration 75/1000 | Loss: 0.00002183
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002183
Iteration 78/1000 | Loss: 0.00002183
Iteration 79/1000 | Loss: 0.00002182
Iteration 80/1000 | Loss: 0.00002182
Iteration 81/1000 | Loss: 0.00002182
Iteration 82/1000 | Loss: 0.00002182
Iteration 83/1000 | Loss: 0.00002182
Iteration 84/1000 | Loss: 0.00002182
Iteration 85/1000 | Loss: 0.00002182
Iteration 86/1000 | Loss: 0.00002182
Iteration 87/1000 | Loss: 0.00002182
Iteration 88/1000 | Loss: 0.00002182
Iteration 89/1000 | Loss: 0.00002182
Iteration 90/1000 | Loss: 0.00002182
Iteration 91/1000 | Loss: 0.00002182
Iteration 92/1000 | Loss: 0.00002182
Iteration 93/1000 | Loss: 0.00002182
Iteration 94/1000 | Loss: 0.00002182
Iteration 95/1000 | Loss: 0.00002182
Iteration 96/1000 | Loss: 0.00002182
Iteration 97/1000 | Loss: 0.00002182
Iteration 98/1000 | Loss: 0.00002182
Iteration 99/1000 | Loss: 0.00002181
Iteration 100/1000 | Loss: 0.00002181
Iteration 101/1000 | Loss: 0.00002181
Iteration 102/1000 | Loss: 0.00002181
Iteration 103/1000 | Loss: 0.00002181
Iteration 104/1000 | Loss: 0.00002181
Iteration 105/1000 | Loss: 0.00002181
Iteration 106/1000 | Loss: 0.00002181
Iteration 107/1000 | Loss: 0.00002181
Iteration 108/1000 | Loss: 0.00002181
Iteration 109/1000 | Loss: 0.00002181
Iteration 110/1000 | Loss: 0.00002181
Iteration 111/1000 | Loss: 0.00002181
Iteration 112/1000 | Loss: 0.00002180
Iteration 113/1000 | Loss: 0.00002180
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002180
Iteration 118/1000 | Loss: 0.00002180
Iteration 119/1000 | Loss: 0.00002180
Iteration 120/1000 | Loss: 0.00002180
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.1804857169627212e-05, 2.1804857169627212e-05, 2.1804857169627212e-05, 2.1804857169627212e-05, 2.1804857169627212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1804857169627212e-05

Optimization complete. Final v2v error: 3.9999382495880127 mm

Highest mean error: 4.293889999389648 mm for frame 46

Lowest mean error: 3.6267411708831787 mm for frame 0

Saving results

Total time: 57.1193311214447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597624
Iteration 2/25 | Loss: 0.00148382
Iteration 3/25 | Loss: 0.00130175
Iteration 4/25 | Loss: 0.00128702
Iteration 5/25 | Loss: 0.00128370
Iteration 6/25 | Loss: 0.00128280
Iteration 7/25 | Loss: 0.00128280
Iteration 8/25 | Loss: 0.00128280
Iteration 9/25 | Loss: 0.00128280
Iteration 10/25 | Loss: 0.00128280
Iteration 11/25 | Loss: 0.00128280
Iteration 12/25 | Loss: 0.00128280
Iteration 13/25 | Loss: 0.00128280
Iteration 14/25 | Loss: 0.00128280
Iteration 15/25 | Loss: 0.00128280
Iteration 16/25 | Loss: 0.00128280
Iteration 17/25 | Loss: 0.00128280
Iteration 18/25 | Loss: 0.00128280
Iteration 19/25 | Loss: 0.00128280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012827978935092688, 0.0012827978935092688, 0.0012827978935092688, 0.0012827978935092688, 0.0012827978935092688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012827978935092688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.67178011
Iteration 2/25 | Loss: 0.00195974
Iteration 3/25 | Loss: 0.00195973
Iteration 4/25 | Loss: 0.00195973
Iteration 5/25 | Loss: 0.00195973
Iteration 6/25 | Loss: 0.00195973
Iteration 7/25 | Loss: 0.00195973
Iteration 8/25 | Loss: 0.00195973
Iteration 9/25 | Loss: 0.00195973
Iteration 10/25 | Loss: 0.00195973
Iteration 11/25 | Loss: 0.00195973
Iteration 12/25 | Loss: 0.00195973
Iteration 13/25 | Loss: 0.00195973
Iteration 14/25 | Loss: 0.00195973
Iteration 15/25 | Loss: 0.00195973
Iteration 16/25 | Loss: 0.00195973
Iteration 17/25 | Loss: 0.00195973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019597283098846674, 0.0019597283098846674, 0.0019597283098846674, 0.0019597283098846674, 0.0019597283098846674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019597283098846674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195973
Iteration 2/1000 | Loss: 0.00005745
Iteration 3/1000 | Loss: 0.00003922
Iteration 4/1000 | Loss: 0.00003449
Iteration 5/1000 | Loss: 0.00003226
Iteration 6/1000 | Loss: 0.00003088
Iteration 7/1000 | Loss: 0.00002991
Iteration 8/1000 | Loss: 0.00002934
Iteration 9/1000 | Loss: 0.00002883
Iteration 10/1000 | Loss: 0.00002852
Iteration 11/1000 | Loss: 0.00002826
Iteration 12/1000 | Loss: 0.00002819
Iteration 13/1000 | Loss: 0.00002797
Iteration 14/1000 | Loss: 0.00002792
Iteration 15/1000 | Loss: 0.00002791
Iteration 16/1000 | Loss: 0.00002791
Iteration 17/1000 | Loss: 0.00002791
Iteration 18/1000 | Loss: 0.00002791
Iteration 19/1000 | Loss: 0.00002791
Iteration 20/1000 | Loss: 0.00002791
Iteration 21/1000 | Loss: 0.00002791
Iteration 22/1000 | Loss: 0.00002791
Iteration 23/1000 | Loss: 0.00002791
Iteration 24/1000 | Loss: 0.00002791
Iteration 25/1000 | Loss: 0.00002791
Iteration 26/1000 | Loss: 0.00002787
Iteration 27/1000 | Loss: 0.00002787
Iteration 28/1000 | Loss: 0.00002787
Iteration 29/1000 | Loss: 0.00002786
Iteration 30/1000 | Loss: 0.00002786
Iteration 31/1000 | Loss: 0.00002786
Iteration 32/1000 | Loss: 0.00002786
Iteration 33/1000 | Loss: 0.00002786
Iteration 34/1000 | Loss: 0.00002786
Iteration 35/1000 | Loss: 0.00002785
Iteration 36/1000 | Loss: 0.00002782
Iteration 37/1000 | Loss: 0.00002781
Iteration 38/1000 | Loss: 0.00002781
Iteration 39/1000 | Loss: 0.00002781
Iteration 40/1000 | Loss: 0.00002781
Iteration 41/1000 | Loss: 0.00002780
Iteration 42/1000 | Loss: 0.00002778
Iteration 43/1000 | Loss: 0.00002778
Iteration 44/1000 | Loss: 0.00002777
Iteration 45/1000 | Loss: 0.00002777
Iteration 46/1000 | Loss: 0.00002776
Iteration 47/1000 | Loss: 0.00002776
Iteration 48/1000 | Loss: 0.00002776
Iteration 49/1000 | Loss: 0.00002775
Iteration 50/1000 | Loss: 0.00002774
Iteration 51/1000 | Loss: 0.00002774
Iteration 52/1000 | Loss: 0.00002773
Iteration 53/1000 | Loss: 0.00002773
Iteration 54/1000 | Loss: 0.00002773
Iteration 55/1000 | Loss: 0.00002772
Iteration 56/1000 | Loss: 0.00002772
Iteration 57/1000 | Loss: 0.00002771
Iteration 58/1000 | Loss: 0.00002771
Iteration 59/1000 | Loss: 0.00002770
Iteration 60/1000 | Loss: 0.00002770
Iteration 61/1000 | Loss: 0.00002770
Iteration 62/1000 | Loss: 0.00002770
Iteration 63/1000 | Loss: 0.00002770
Iteration 64/1000 | Loss: 0.00002770
Iteration 65/1000 | Loss: 0.00002769
Iteration 66/1000 | Loss: 0.00002769
Iteration 67/1000 | Loss: 0.00002768
Iteration 68/1000 | Loss: 0.00002768
Iteration 69/1000 | Loss: 0.00002768
Iteration 70/1000 | Loss: 0.00002768
Iteration 71/1000 | Loss: 0.00002768
Iteration 72/1000 | Loss: 0.00002768
Iteration 73/1000 | Loss: 0.00002768
Iteration 74/1000 | Loss: 0.00002767
Iteration 75/1000 | Loss: 0.00002767
Iteration 76/1000 | Loss: 0.00002767
Iteration 77/1000 | Loss: 0.00002767
Iteration 78/1000 | Loss: 0.00002767
Iteration 79/1000 | Loss: 0.00002767
Iteration 80/1000 | Loss: 0.00002766
Iteration 81/1000 | Loss: 0.00002766
Iteration 82/1000 | Loss: 0.00002766
Iteration 83/1000 | Loss: 0.00002766
Iteration 84/1000 | Loss: 0.00002766
Iteration 85/1000 | Loss: 0.00002766
Iteration 86/1000 | Loss: 0.00002766
Iteration 87/1000 | Loss: 0.00002766
Iteration 88/1000 | Loss: 0.00002766
Iteration 89/1000 | Loss: 0.00002766
Iteration 90/1000 | Loss: 0.00002765
Iteration 91/1000 | Loss: 0.00002765
Iteration 92/1000 | Loss: 0.00002765
Iteration 93/1000 | Loss: 0.00002765
Iteration 94/1000 | Loss: 0.00002765
Iteration 95/1000 | Loss: 0.00002765
Iteration 96/1000 | Loss: 0.00002764
Iteration 97/1000 | Loss: 0.00002764
Iteration 98/1000 | Loss: 0.00002764
Iteration 99/1000 | Loss: 0.00002764
Iteration 100/1000 | Loss: 0.00002764
Iteration 101/1000 | Loss: 0.00002764
Iteration 102/1000 | Loss: 0.00002764
Iteration 103/1000 | Loss: 0.00002764
Iteration 104/1000 | Loss: 0.00002764
Iteration 105/1000 | Loss: 0.00002764
Iteration 106/1000 | Loss: 0.00002764
Iteration 107/1000 | Loss: 0.00002763
Iteration 108/1000 | Loss: 0.00002763
Iteration 109/1000 | Loss: 0.00002763
Iteration 110/1000 | Loss: 0.00002763
Iteration 111/1000 | Loss: 0.00002763
Iteration 112/1000 | Loss: 0.00002763
Iteration 113/1000 | Loss: 0.00002763
Iteration 114/1000 | Loss: 0.00002763
Iteration 115/1000 | Loss: 0.00002763
Iteration 116/1000 | Loss: 0.00002763
Iteration 117/1000 | Loss: 0.00002763
Iteration 118/1000 | Loss: 0.00002763
Iteration 119/1000 | Loss: 0.00002763
Iteration 120/1000 | Loss: 0.00002762
Iteration 121/1000 | Loss: 0.00002762
Iteration 122/1000 | Loss: 0.00002762
Iteration 123/1000 | Loss: 0.00002762
Iteration 124/1000 | Loss: 0.00002762
Iteration 125/1000 | Loss: 0.00002762
Iteration 126/1000 | Loss: 0.00002762
Iteration 127/1000 | Loss: 0.00002762
Iteration 128/1000 | Loss: 0.00002762
Iteration 129/1000 | Loss: 0.00002762
Iteration 130/1000 | Loss: 0.00002762
Iteration 131/1000 | Loss: 0.00002762
Iteration 132/1000 | Loss: 0.00002762
Iteration 133/1000 | Loss: 0.00002762
Iteration 134/1000 | Loss: 0.00002762
Iteration 135/1000 | Loss: 0.00002762
Iteration 136/1000 | Loss: 0.00002761
Iteration 137/1000 | Loss: 0.00002761
Iteration 138/1000 | Loss: 0.00002761
Iteration 139/1000 | Loss: 0.00002761
Iteration 140/1000 | Loss: 0.00002761
Iteration 141/1000 | Loss: 0.00002761
Iteration 142/1000 | Loss: 0.00002761
Iteration 143/1000 | Loss: 0.00002761
Iteration 144/1000 | Loss: 0.00002761
Iteration 145/1000 | Loss: 0.00002761
Iteration 146/1000 | Loss: 0.00002761
Iteration 147/1000 | Loss: 0.00002761
Iteration 148/1000 | Loss: 0.00002761
Iteration 149/1000 | Loss: 0.00002761
Iteration 150/1000 | Loss: 0.00002761
Iteration 151/1000 | Loss: 0.00002760
Iteration 152/1000 | Loss: 0.00002760
Iteration 153/1000 | Loss: 0.00002760
Iteration 154/1000 | Loss: 0.00002760
Iteration 155/1000 | Loss: 0.00002760
Iteration 156/1000 | Loss: 0.00002760
Iteration 157/1000 | Loss: 0.00002760
Iteration 158/1000 | Loss: 0.00002760
Iteration 159/1000 | Loss: 0.00002760
Iteration 160/1000 | Loss: 0.00002760
Iteration 161/1000 | Loss: 0.00002760
Iteration 162/1000 | Loss: 0.00002760
Iteration 163/1000 | Loss: 0.00002760
Iteration 164/1000 | Loss: 0.00002760
Iteration 165/1000 | Loss: 0.00002760
Iteration 166/1000 | Loss: 0.00002760
Iteration 167/1000 | Loss: 0.00002760
Iteration 168/1000 | Loss: 0.00002760
Iteration 169/1000 | Loss: 0.00002759
Iteration 170/1000 | Loss: 0.00002759
Iteration 171/1000 | Loss: 0.00002759
Iteration 172/1000 | Loss: 0.00002759
Iteration 173/1000 | Loss: 0.00002759
Iteration 174/1000 | Loss: 0.00002759
Iteration 175/1000 | Loss: 0.00002759
Iteration 176/1000 | Loss: 0.00002759
Iteration 177/1000 | Loss: 0.00002759
Iteration 178/1000 | Loss: 0.00002759
Iteration 179/1000 | Loss: 0.00002759
Iteration 180/1000 | Loss: 0.00002759
Iteration 181/1000 | Loss: 0.00002759
Iteration 182/1000 | Loss: 0.00002759
Iteration 183/1000 | Loss: 0.00002759
Iteration 184/1000 | Loss: 0.00002759
Iteration 185/1000 | Loss: 0.00002759
Iteration 186/1000 | Loss: 0.00002759
Iteration 187/1000 | Loss: 0.00002759
Iteration 188/1000 | Loss: 0.00002759
Iteration 189/1000 | Loss: 0.00002759
Iteration 190/1000 | Loss: 0.00002759
Iteration 191/1000 | Loss: 0.00002759
Iteration 192/1000 | Loss: 0.00002759
Iteration 193/1000 | Loss: 0.00002759
Iteration 194/1000 | Loss: 0.00002759
Iteration 195/1000 | Loss: 0.00002759
Iteration 196/1000 | Loss: 0.00002759
Iteration 197/1000 | Loss: 0.00002759
Iteration 198/1000 | Loss: 0.00002759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.7587986551225185e-05, 2.7587986551225185e-05, 2.7587986551225185e-05, 2.7587986551225185e-05, 2.7587986551225185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7587986551225185e-05

Optimization complete. Final v2v error: 4.526729106903076 mm

Highest mean error: 4.827942848205566 mm for frame 133

Lowest mean error: 4.115024566650391 mm for frame 93

Saving results

Total time: 38.84842848777771
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452378
Iteration 2/25 | Loss: 0.00139055
Iteration 3/25 | Loss: 0.00125181
Iteration 4/25 | Loss: 0.00123183
Iteration 5/25 | Loss: 0.00122749
Iteration 6/25 | Loss: 0.00122634
Iteration 7/25 | Loss: 0.00122595
Iteration 8/25 | Loss: 0.00122595
Iteration 9/25 | Loss: 0.00122595
Iteration 10/25 | Loss: 0.00122593
Iteration 11/25 | Loss: 0.00122593
Iteration 12/25 | Loss: 0.00122593
Iteration 13/25 | Loss: 0.00122593
Iteration 14/25 | Loss: 0.00122593
Iteration 15/25 | Loss: 0.00122593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012259257491678, 0.0012259257491678, 0.0012259257491678, 0.0012259257491678, 0.0012259257491678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012259257491678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58919597
Iteration 2/25 | Loss: 0.00226943
Iteration 3/25 | Loss: 0.00226941
Iteration 4/25 | Loss: 0.00226940
Iteration 5/25 | Loss: 0.00226940
Iteration 6/25 | Loss: 0.00226940
Iteration 7/25 | Loss: 0.00226940
Iteration 8/25 | Loss: 0.00226940
Iteration 9/25 | Loss: 0.00226940
Iteration 10/25 | Loss: 0.00226940
Iteration 11/25 | Loss: 0.00226940
Iteration 12/25 | Loss: 0.00226940
Iteration 13/25 | Loss: 0.00226940
Iteration 14/25 | Loss: 0.00226940
Iteration 15/25 | Loss: 0.00226940
Iteration 16/25 | Loss: 0.00226940
Iteration 17/25 | Loss: 0.00226940
Iteration 18/25 | Loss: 0.00226940
Iteration 19/25 | Loss: 0.00226940
Iteration 20/25 | Loss: 0.00226940
Iteration 21/25 | Loss: 0.00226940
Iteration 22/25 | Loss: 0.00226940
Iteration 23/25 | Loss: 0.00226940
Iteration 24/25 | Loss: 0.00226940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002269401913508773, 0.002269401913508773, 0.002269401913508773, 0.002269401913508773, 0.002269401913508773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002269401913508773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226940
Iteration 2/1000 | Loss: 0.00004226
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002383
Iteration 5/1000 | Loss: 0.00002233
Iteration 6/1000 | Loss: 0.00002153
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00002028
Iteration 9/1000 | Loss: 0.00002002
Iteration 10/1000 | Loss: 0.00001990
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001951
Iteration 17/1000 | Loss: 0.00001951
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001945
Iteration 26/1000 | Loss: 0.00001945
Iteration 27/1000 | Loss: 0.00001945
Iteration 28/1000 | Loss: 0.00001942
Iteration 29/1000 | Loss: 0.00001942
Iteration 30/1000 | Loss: 0.00001942
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001942
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001940
Iteration 40/1000 | Loss: 0.00001940
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001939
Iteration 47/1000 | Loss: 0.00001939
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001938
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001937
Iteration 59/1000 | Loss: 0.00001937
Iteration 60/1000 | Loss: 0.00001937
Iteration 61/1000 | Loss: 0.00001937
Iteration 62/1000 | Loss: 0.00001937
Iteration 63/1000 | Loss: 0.00001937
Iteration 64/1000 | Loss: 0.00001937
Iteration 65/1000 | Loss: 0.00001937
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001937
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001936
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001936
Iteration 75/1000 | Loss: 0.00001936
Iteration 76/1000 | Loss: 0.00001936
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001935
Iteration 79/1000 | Loss: 0.00001935
Iteration 80/1000 | Loss: 0.00001935
Iteration 81/1000 | Loss: 0.00001935
Iteration 82/1000 | Loss: 0.00001935
Iteration 83/1000 | Loss: 0.00001935
Iteration 84/1000 | Loss: 0.00001935
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001935
Iteration 90/1000 | Loss: 0.00001935
Iteration 91/1000 | Loss: 0.00001935
Iteration 92/1000 | Loss: 0.00001935
Iteration 93/1000 | Loss: 0.00001935
Iteration 94/1000 | Loss: 0.00001935
Iteration 95/1000 | Loss: 0.00001935
Iteration 96/1000 | Loss: 0.00001935
Iteration 97/1000 | Loss: 0.00001935
Iteration 98/1000 | Loss: 0.00001935
Iteration 99/1000 | Loss: 0.00001935
Iteration 100/1000 | Loss: 0.00001935
Iteration 101/1000 | Loss: 0.00001935
Iteration 102/1000 | Loss: 0.00001935
Iteration 103/1000 | Loss: 0.00001935
Iteration 104/1000 | Loss: 0.00001935
Iteration 105/1000 | Loss: 0.00001935
Iteration 106/1000 | Loss: 0.00001935
Iteration 107/1000 | Loss: 0.00001935
Iteration 108/1000 | Loss: 0.00001935
Iteration 109/1000 | Loss: 0.00001935
Iteration 110/1000 | Loss: 0.00001935
Iteration 111/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.9345590771990828e-05, 1.9345590771990828e-05, 1.9345590771990828e-05, 1.9345590771990828e-05, 1.9345590771990828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9345590771990828e-05

Optimization complete. Final v2v error: 3.8980088233947754 mm

Highest mean error: 4.339935779571533 mm for frame 127

Lowest mean error: 3.5657355785369873 mm for frame 0

Saving results

Total time: 32.81340980529785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949486
Iteration 2/25 | Loss: 0.00168511
Iteration 3/25 | Loss: 0.00145811
Iteration 4/25 | Loss: 0.00143665
Iteration 5/25 | Loss: 0.00142993
Iteration 6/25 | Loss: 0.00142863
Iteration 7/25 | Loss: 0.00142863
Iteration 8/25 | Loss: 0.00142863
Iteration 9/25 | Loss: 0.00142863
Iteration 10/25 | Loss: 0.00142863
Iteration 11/25 | Loss: 0.00142863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014286251971498132, 0.0014286251971498132, 0.0014286251971498132, 0.0014286251971498132, 0.0014286251971498132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014286251971498132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07411361
Iteration 2/25 | Loss: 0.00216860
Iteration 3/25 | Loss: 0.00216860
Iteration 4/25 | Loss: 0.00216860
Iteration 5/25 | Loss: 0.00216860
Iteration 6/25 | Loss: 0.00216860
Iteration 7/25 | Loss: 0.00216860
Iteration 8/25 | Loss: 0.00216860
Iteration 9/25 | Loss: 0.00216860
Iteration 10/25 | Loss: 0.00216860
Iteration 11/25 | Loss: 0.00216860
Iteration 12/25 | Loss: 0.00216860
Iteration 13/25 | Loss: 0.00216860
Iteration 14/25 | Loss: 0.00216860
Iteration 15/25 | Loss: 0.00216860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021685955580323935, 0.0021685955580323935, 0.0021685955580323935, 0.0021685955580323935, 0.0021685955580323935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021685955580323935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216860
Iteration 2/1000 | Loss: 0.00005861
Iteration 3/1000 | Loss: 0.00004508
Iteration 4/1000 | Loss: 0.00004058
Iteration 5/1000 | Loss: 0.00003806
Iteration 6/1000 | Loss: 0.00003641
Iteration 7/1000 | Loss: 0.00003548
Iteration 8/1000 | Loss: 0.00003478
Iteration 9/1000 | Loss: 0.00003424
Iteration 10/1000 | Loss: 0.00003394
Iteration 11/1000 | Loss: 0.00003374
Iteration 12/1000 | Loss: 0.00003373
Iteration 13/1000 | Loss: 0.00003368
Iteration 14/1000 | Loss: 0.00003367
Iteration 15/1000 | Loss: 0.00003366
Iteration 16/1000 | Loss: 0.00003361
Iteration 17/1000 | Loss: 0.00003355
Iteration 18/1000 | Loss: 0.00003354
Iteration 19/1000 | Loss: 0.00003353
Iteration 20/1000 | Loss: 0.00003353
Iteration 21/1000 | Loss: 0.00003353
Iteration 22/1000 | Loss: 0.00003353
Iteration 23/1000 | Loss: 0.00003353
Iteration 24/1000 | Loss: 0.00003352
Iteration 25/1000 | Loss: 0.00003352
Iteration 26/1000 | Loss: 0.00003352
Iteration 27/1000 | Loss: 0.00003351
Iteration 28/1000 | Loss: 0.00003346
Iteration 29/1000 | Loss: 0.00003345
Iteration 30/1000 | Loss: 0.00003345
Iteration 31/1000 | Loss: 0.00003344
Iteration 32/1000 | Loss: 0.00003344
Iteration 33/1000 | Loss: 0.00003344
Iteration 34/1000 | Loss: 0.00003343
Iteration 35/1000 | Loss: 0.00003343
Iteration 36/1000 | Loss: 0.00003343
Iteration 37/1000 | Loss: 0.00003343
Iteration 38/1000 | Loss: 0.00003343
Iteration 39/1000 | Loss: 0.00003343
Iteration 40/1000 | Loss: 0.00003343
Iteration 41/1000 | Loss: 0.00003342
Iteration 42/1000 | Loss: 0.00003342
Iteration 43/1000 | Loss: 0.00003342
Iteration 44/1000 | Loss: 0.00003341
Iteration 45/1000 | Loss: 0.00003341
Iteration 46/1000 | Loss: 0.00003339
Iteration 47/1000 | Loss: 0.00003339
Iteration 48/1000 | Loss: 0.00003339
Iteration 49/1000 | Loss: 0.00003339
Iteration 50/1000 | Loss: 0.00003339
Iteration 51/1000 | Loss: 0.00003339
Iteration 52/1000 | Loss: 0.00003339
Iteration 53/1000 | Loss: 0.00003339
Iteration 54/1000 | Loss: 0.00003338
Iteration 55/1000 | Loss: 0.00003338
Iteration 56/1000 | Loss: 0.00003338
Iteration 57/1000 | Loss: 0.00003338
Iteration 58/1000 | Loss: 0.00003336
Iteration 59/1000 | Loss: 0.00003335
Iteration 60/1000 | Loss: 0.00003335
Iteration 61/1000 | Loss: 0.00003335
Iteration 62/1000 | Loss: 0.00003335
Iteration 63/1000 | Loss: 0.00003335
Iteration 64/1000 | Loss: 0.00003335
Iteration 65/1000 | Loss: 0.00003335
Iteration 66/1000 | Loss: 0.00003334
Iteration 67/1000 | Loss: 0.00003334
Iteration 68/1000 | Loss: 0.00003334
Iteration 69/1000 | Loss: 0.00003334
Iteration 70/1000 | Loss: 0.00003334
Iteration 71/1000 | Loss: 0.00003334
Iteration 72/1000 | Loss: 0.00003334
Iteration 73/1000 | Loss: 0.00003334
Iteration 74/1000 | Loss: 0.00003334
Iteration 75/1000 | Loss: 0.00003334
Iteration 76/1000 | Loss: 0.00003333
Iteration 77/1000 | Loss: 0.00003333
Iteration 78/1000 | Loss: 0.00003333
Iteration 79/1000 | Loss: 0.00003333
Iteration 80/1000 | Loss: 0.00003333
Iteration 81/1000 | Loss: 0.00003333
Iteration 82/1000 | Loss: 0.00003333
Iteration 83/1000 | Loss: 0.00003333
Iteration 84/1000 | Loss: 0.00003333
Iteration 85/1000 | Loss: 0.00003333
Iteration 86/1000 | Loss: 0.00003333
Iteration 87/1000 | Loss: 0.00003333
Iteration 88/1000 | Loss: 0.00003333
Iteration 89/1000 | Loss: 0.00003333
Iteration 90/1000 | Loss: 0.00003333
Iteration 91/1000 | Loss: 0.00003333
Iteration 92/1000 | Loss: 0.00003333
Iteration 93/1000 | Loss: 0.00003333
Iteration 94/1000 | Loss: 0.00003333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [3.3328644349239767e-05, 3.3328644349239767e-05, 3.3328644349239767e-05, 3.3328644349239767e-05, 3.3328644349239767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3328644349239767e-05

Optimization complete. Final v2v error: 4.781609058380127 mm

Highest mean error: 4.986011981964111 mm for frame 3

Lowest mean error: 4.661281108856201 mm for frame 96

Saving results

Total time: 33.383967876434326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033849
Iteration 2/25 | Loss: 0.00230355
Iteration 3/25 | Loss: 0.00161551
Iteration 4/25 | Loss: 0.00152725
Iteration 5/25 | Loss: 0.00152328
Iteration 6/25 | Loss: 0.00152745
Iteration 7/25 | Loss: 0.00148952
Iteration 8/25 | Loss: 0.00146023
Iteration 9/25 | Loss: 0.00145414
Iteration 10/25 | Loss: 0.00144849
Iteration 11/25 | Loss: 0.00144329
Iteration 12/25 | Loss: 0.00143899
Iteration 13/25 | Loss: 0.00143052
Iteration 14/25 | Loss: 0.00142832
Iteration 15/25 | Loss: 0.00143041
Iteration 16/25 | Loss: 0.00142774
Iteration 17/25 | Loss: 0.00143027
Iteration 18/25 | Loss: 0.00142987
Iteration 19/25 | Loss: 0.00142691
Iteration 20/25 | Loss: 0.00143146
Iteration 21/25 | Loss: 0.00142808
Iteration 22/25 | Loss: 0.00142902
Iteration 23/25 | Loss: 0.00142524
Iteration 24/25 | Loss: 0.00142653
Iteration 25/25 | Loss: 0.00142484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.54275131
Iteration 2/25 | Loss: 0.00363255
Iteration 3/25 | Loss: 0.00363255
Iteration 4/25 | Loss: 0.00363255
Iteration 5/25 | Loss: 0.00363255
Iteration 6/25 | Loss: 0.00363255
Iteration 7/25 | Loss: 0.00363255
Iteration 8/25 | Loss: 0.00363255
Iteration 9/25 | Loss: 0.00363255
Iteration 10/25 | Loss: 0.00363255
Iteration 11/25 | Loss: 0.00363255
Iteration 12/25 | Loss: 0.00363255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.003632549662142992, 0.003632549662142992, 0.003632549662142992, 0.003632549662142992, 0.003632549662142992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003632549662142992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00363255
Iteration 2/1000 | Loss: 0.00115420
Iteration 3/1000 | Loss: 0.00697576
Iteration 4/1000 | Loss: 0.00259209
Iteration 5/1000 | Loss: 0.00234704
Iteration 6/1000 | Loss: 0.00063247
Iteration 7/1000 | Loss: 0.00081089
Iteration 8/1000 | Loss: 0.00009863
Iteration 9/1000 | Loss: 0.00006795
Iteration 10/1000 | Loss: 0.00005399
Iteration 11/1000 | Loss: 0.00004508
Iteration 12/1000 | Loss: 0.00030439
Iteration 13/1000 | Loss: 0.00026833
Iteration 14/1000 | Loss: 0.00020381
Iteration 15/1000 | Loss: 0.00008797
Iteration 16/1000 | Loss: 0.00009992
Iteration 17/1000 | Loss: 0.00009058
Iteration 18/1000 | Loss: 0.00009472
Iteration 19/1000 | Loss: 0.00003933
Iteration 20/1000 | Loss: 0.00003589
Iteration 21/1000 | Loss: 0.00003360
Iteration 22/1000 | Loss: 0.00003142
Iteration 23/1000 | Loss: 0.00003045
Iteration 24/1000 | Loss: 0.00003970
Iteration 25/1000 | Loss: 0.00003473
Iteration 26/1000 | Loss: 0.00003996
Iteration 27/1000 | Loss: 0.00002984
Iteration 28/1000 | Loss: 0.00002892
Iteration 29/1000 | Loss: 0.00024642
Iteration 30/1000 | Loss: 0.00025969
Iteration 31/1000 | Loss: 0.00023889
Iteration 32/1000 | Loss: 0.00003918
Iteration 33/1000 | Loss: 0.00003386
Iteration 34/1000 | Loss: 0.00003134
Iteration 35/1000 | Loss: 0.00003011
Iteration 36/1000 | Loss: 0.00002936
Iteration 37/1000 | Loss: 0.00002883
Iteration 38/1000 | Loss: 0.00039596
Iteration 39/1000 | Loss: 0.00013849
Iteration 40/1000 | Loss: 0.00003216
Iteration 41/1000 | Loss: 0.00002818
Iteration 42/1000 | Loss: 0.00002786
Iteration 43/1000 | Loss: 0.00002754
Iteration 44/1000 | Loss: 0.00025455
Iteration 45/1000 | Loss: 0.00029408
Iteration 46/1000 | Loss: 0.00028657
Iteration 47/1000 | Loss: 0.00021508
Iteration 48/1000 | Loss: 0.00023335
Iteration 49/1000 | Loss: 0.00022492
Iteration 50/1000 | Loss: 0.00010579
Iteration 51/1000 | Loss: 0.00016418
Iteration 52/1000 | Loss: 0.00026556
Iteration 53/1000 | Loss: 0.00020226
Iteration 54/1000 | Loss: 0.00022422
Iteration 55/1000 | Loss: 0.00002746
Iteration 56/1000 | Loss: 0.00002652
Iteration 57/1000 | Loss: 0.00002642
Iteration 58/1000 | Loss: 0.00002636
Iteration 59/1000 | Loss: 0.00002630
Iteration 60/1000 | Loss: 0.00002630
Iteration 61/1000 | Loss: 0.00002629
Iteration 62/1000 | Loss: 0.00002629
Iteration 63/1000 | Loss: 0.00002628
Iteration 64/1000 | Loss: 0.00002627
Iteration 65/1000 | Loss: 0.00002626
Iteration 66/1000 | Loss: 0.00002626
Iteration 67/1000 | Loss: 0.00002626
Iteration 68/1000 | Loss: 0.00002625
Iteration 69/1000 | Loss: 0.00002625
Iteration 70/1000 | Loss: 0.00002625
Iteration 71/1000 | Loss: 0.00002625
Iteration 72/1000 | Loss: 0.00002625
Iteration 73/1000 | Loss: 0.00002625
Iteration 74/1000 | Loss: 0.00002625
Iteration 75/1000 | Loss: 0.00002625
Iteration 76/1000 | Loss: 0.00002625
Iteration 77/1000 | Loss: 0.00002625
Iteration 78/1000 | Loss: 0.00002625
Iteration 79/1000 | Loss: 0.00002622
Iteration 80/1000 | Loss: 0.00002622
Iteration 81/1000 | Loss: 0.00002622
Iteration 82/1000 | Loss: 0.00002622
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002620
Iteration 86/1000 | Loss: 0.00002620
Iteration 87/1000 | Loss: 0.00002617
Iteration 88/1000 | Loss: 0.00002617
Iteration 89/1000 | Loss: 0.00002616
Iteration 90/1000 | Loss: 0.00002616
Iteration 91/1000 | Loss: 0.00002615
Iteration 92/1000 | Loss: 0.00002615
Iteration 93/1000 | Loss: 0.00002614
Iteration 94/1000 | Loss: 0.00002612
Iteration 95/1000 | Loss: 0.00002609
Iteration 96/1000 | Loss: 0.00002609
Iteration 97/1000 | Loss: 0.00002608
Iteration 98/1000 | Loss: 0.00002608
Iteration 99/1000 | Loss: 0.00002608
Iteration 100/1000 | Loss: 0.00002607
Iteration 101/1000 | Loss: 0.00002602
Iteration 102/1000 | Loss: 0.00002601
Iteration 103/1000 | Loss: 0.00002601
Iteration 104/1000 | Loss: 0.00002601
Iteration 105/1000 | Loss: 0.00002600
Iteration 106/1000 | Loss: 0.00002600
Iteration 107/1000 | Loss: 0.00002600
Iteration 108/1000 | Loss: 0.00002600
Iteration 109/1000 | Loss: 0.00002600
Iteration 110/1000 | Loss: 0.00002599
Iteration 111/1000 | Loss: 0.00002598
Iteration 112/1000 | Loss: 0.00002597
Iteration 113/1000 | Loss: 0.00002597
Iteration 114/1000 | Loss: 0.00002597
Iteration 115/1000 | Loss: 0.00002596
Iteration 116/1000 | Loss: 0.00002596
Iteration 117/1000 | Loss: 0.00002596
Iteration 118/1000 | Loss: 0.00002595
Iteration 119/1000 | Loss: 0.00002593
Iteration 120/1000 | Loss: 0.00002591
Iteration 121/1000 | Loss: 0.00002591
Iteration 122/1000 | Loss: 0.00002591
Iteration 123/1000 | Loss: 0.00002590
Iteration 124/1000 | Loss: 0.00002590
Iteration 125/1000 | Loss: 0.00002589
Iteration 126/1000 | Loss: 0.00002589
Iteration 127/1000 | Loss: 0.00002588
Iteration 128/1000 | Loss: 0.00002588
Iteration 129/1000 | Loss: 0.00002588
Iteration 130/1000 | Loss: 0.00002587
Iteration 131/1000 | Loss: 0.00002587
Iteration 132/1000 | Loss: 0.00002587
Iteration 133/1000 | Loss: 0.00002587
Iteration 134/1000 | Loss: 0.00002587
Iteration 135/1000 | Loss: 0.00002587
Iteration 136/1000 | Loss: 0.00002585
Iteration 137/1000 | Loss: 0.00002585
Iteration 138/1000 | Loss: 0.00002584
Iteration 139/1000 | Loss: 0.00002583
Iteration 140/1000 | Loss: 0.00002583
Iteration 141/1000 | Loss: 0.00002583
Iteration 142/1000 | Loss: 0.00002583
Iteration 143/1000 | Loss: 0.00002583
Iteration 144/1000 | Loss: 0.00002583
Iteration 145/1000 | Loss: 0.00002583
Iteration 146/1000 | Loss: 0.00002583
Iteration 147/1000 | Loss: 0.00002583
Iteration 148/1000 | Loss: 0.00002583
Iteration 149/1000 | Loss: 0.00002583
Iteration 150/1000 | Loss: 0.00002583
Iteration 151/1000 | Loss: 0.00002583
Iteration 152/1000 | Loss: 0.00002582
Iteration 153/1000 | Loss: 0.00002582
Iteration 154/1000 | Loss: 0.00002581
Iteration 155/1000 | Loss: 0.00002581
Iteration 156/1000 | Loss: 0.00002581
Iteration 157/1000 | Loss: 0.00002581
Iteration 158/1000 | Loss: 0.00002581
Iteration 159/1000 | Loss: 0.00002581
Iteration 160/1000 | Loss: 0.00002580
Iteration 161/1000 | Loss: 0.00002580
Iteration 162/1000 | Loss: 0.00002580
Iteration 163/1000 | Loss: 0.00002580
Iteration 164/1000 | Loss: 0.00002580
Iteration 165/1000 | Loss: 0.00002580
Iteration 166/1000 | Loss: 0.00002580
Iteration 167/1000 | Loss: 0.00002580
Iteration 168/1000 | Loss: 0.00002580
Iteration 169/1000 | Loss: 0.00002579
Iteration 170/1000 | Loss: 0.00002578
Iteration 171/1000 | Loss: 0.00002578
Iteration 172/1000 | Loss: 0.00002578
Iteration 173/1000 | Loss: 0.00002578
Iteration 174/1000 | Loss: 0.00002577
Iteration 175/1000 | Loss: 0.00002577
Iteration 176/1000 | Loss: 0.00002577
Iteration 177/1000 | Loss: 0.00002576
Iteration 178/1000 | Loss: 0.00002576
Iteration 179/1000 | Loss: 0.00002576
Iteration 180/1000 | Loss: 0.00002576
Iteration 181/1000 | Loss: 0.00002575
Iteration 182/1000 | Loss: 0.00002575
Iteration 183/1000 | Loss: 0.00002575
Iteration 184/1000 | Loss: 0.00002575
Iteration 185/1000 | Loss: 0.00002575
Iteration 186/1000 | Loss: 0.00002574
Iteration 187/1000 | Loss: 0.00002574
Iteration 188/1000 | Loss: 0.00002574
Iteration 189/1000 | Loss: 0.00002574
Iteration 190/1000 | Loss: 0.00002574
Iteration 191/1000 | Loss: 0.00002573
Iteration 192/1000 | Loss: 0.00002573
Iteration 193/1000 | Loss: 0.00002573
Iteration 194/1000 | Loss: 0.00002573
Iteration 195/1000 | Loss: 0.00002573
Iteration 196/1000 | Loss: 0.00002573
Iteration 197/1000 | Loss: 0.00002572
Iteration 198/1000 | Loss: 0.00002572
Iteration 199/1000 | Loss: 0.00002572
Iteration 200/1000 | Loss: 0.00002572
Iteration 201/1000 | Loss: 0.00002572
Iteration 202/1000 | Loss: 0.00002572
Iteration 203/1000 | Loss: 0.00002572
Iteration 204/1000 | Loss: 0.00002572
Iteration 205/1000 | Loss: 0.00002572
Iteration 206/1000 | Loss: 0.00002571
Iteration 207/1000 | Loss: 0.00002571
Iteration 208/1000 | Loss: 0.00002571
Iteration 209/1000 | Loss: 0.00002571
Iteration 210/1000 | Loss: 0.00002571
Iteration 211/1000 | Loss: 0.00002571
Iteration 212/1000 | Loss: 0.00002571
Iteration 213/1000 | Loss: 0.00002571
Iteration 214/1000 | Loss: 0.00002571
Iteration 215/1000 | Loss: 0.00002571
Iteration 216/1000 | Loss: 0.00002571
Iteration 217/1000 | Loss: 0.00002571
Iteration 218/1000 | Loss: 0.00002571
Iteration 219/1000 | Loss: 0.00002571
Iteration 220/1000 | Loss: 0.00002571
Iteration 221/1000 | Loss: 0.00002571
Iteration 222/1000 | Loss: 0.00002571
Iteration 223/1000 | Loss: 0.00002571
Iteration 224/1000 | Loss: 0.00002571
Iteration 225/1000 | Loss: 0.00002571
Iteration 226/1000 | Loss: 0.00002571
Iteration 227/1000 | Loss: 0.00002571
Iteration 228/1000 | Loss: 0.00002571
Iteration 229/1000 | Loss: 0.00002571
Iteration 230/1000 | Loss: 0.00002571
Iteration 231/1000 | Loss: 0.00002571
Iteration 232/1000 | Loss: 0.00002571
Iteration 233/1000 | Loss: 0.00002571
Iteration 234/1000 | Loss: 0.00002571
Iteration 235/1000 | Loss: 0.00002571
Iteration 236/1000 | Loss: 0.00002571
Iteration 237/1000 | Loss: 0.00002571
Iteration 238/1000 | Loss: 0.00002571
Iteration 239/1000 | Loss: 0.00002571
Iteration 240/1000 | Loss: 0.00002571
Iteration 241/1000 | Loss: 0.00002571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.570613105490338e-05, 2.570613105490338e-05, 2.570613105490338e-05, 2.570613105490338e-05, 2.570613105490338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.570613105490338e-05

Optimization complete. Final v2v error: 4.203020095825195 mm

Highest mean error: 9.676973342895508 mm for frame 44

Lowest mean error: 3.5015814304351807 mm for frame 121

Saving results

Total time: 143.31275725364685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003599
Iteration 2/25 | Loss: 0.00206375
Iteration 3/25 | Loss: 0.00157135
Iteration 4/25 | Loss: 0.00162547
Iteration 5/25 | Loss: 0.00161310
Iteration 6/25 | Loss: 0.00143141
Iteration 7/25 | Loss: 0.00138907
Iteration 8/25 | Loss: 0.00139499
Iteration 9/25 | Loss: 0.00139110
Iteration 10/25 | Loss: 0.00138683
Iteration 11/25 | Loss: 0.00137016
Iteration 12/25 | Loss: 0.00135013
Iteration 13/25 | Loss: 0.00132850
Iteration 14/25 | Loss: 0.00131527
Iteration 15/25 | Loss: 0.00131155
Iteration 16/25 | Loss: 0.00131056
Iteration 17/25 | Loss: 0.00131031
Iteration 18/25 | Loss: 0.00131021
Iteration 19/25 | Loss: 0.00131012
Iteration 20/25 | Loss: 0.00131011
Iteration 21/25 | Loss: 0.00131011
Iteration 22/25 | Loss: 0.00131010
Iteration 23/25 | Loss: 0.00131010
Iteration 24/25 | Loss: 0.00131010
Iteration 25/25 | Loss: 0.00131010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62033772
Iteration 2/25 | Loss: 0.00224846
Iteration 3/25 | Loss: 0.00216195
Iteration 4/25 | Loss: 0.00216194
Iteration 5/25 | Loss: 0.00216194
Iteration 6/25 | Loss: 0.00216194
Iteration 7/25 | Loss: 0.00216193
Iteration 8/25 | Loss: 0.00216193
Iteration 9/25 | Loss: 0.00216193
Iteration 10/25 | Loss: 0.00216193
Iteration 11/25 | Loss: 0.00216193
Iteration 12/25 | Loss: 0.00216193
Iteration 13/25 | Loss: 0.00216193
Iteration 14/25 | Loss: 0.00216193
Iteration 15/25 | Loss: 0.00216193
Iteration 16/25 | Loss: 0.00216193
Iteration 17/25 | Loss: 0.00216193
Iteration 18/25 | Loss: 0.00216193
Iteration 19/25 | Loss: 0.00216193
Iteration 20/25 | Loss: 0.00216193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0021619342733174562, 0.0021619342733174562, 0.0021619342733174562, 0.0021619342733174562, 0.0021619342733174562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021619342733174562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216193
Iteration 2/1000 | Loss: 0.00008411
Iteration 3/1000 | Loss: 0.00016448
Iteration 4/1000 | Loss: 0.00009931
Iteration 5/1000 | Loss: 0.00004530
Iteration 6/1000 | Loss: 0.00004242
Iteration 7/1000 | Loss: 0.00004099
Iteration 8/1000 | Loss: 0.00003950
Iteration 9/1000 | Loss: 0.00164784
Iteration 10/1000 | Loss: 0.00056365
Iteration 11/1000 | Loss: 0.00003920
Iteration 12/1000 | Loss: 0.00004600
Iteration 13/1000 | Loss: 0.00003025
Iteration 14/1000 | Loss: 0.00002885
Iteration 15/1000 | Loss: 0.00011566
Iteration 16/1000 | Loss: 0.00002755
Iteration 17/1000 | Loss: 0.00002715
Iteration 18/1000 | Loss: 0.00012169
Iteration 19/1000 | Loss: 0.00002673
Iteration 20/1000 | Loss: 0.00002651
Iteration 21/1000 | Loss: 0.00009806
Iteration 22/1000 | Loss: 0.00002644
Iteration 23/1000 | Loss: 0.00002629
Iteration 24/1000 | Loss: 0.00002629
Iteration 25/1000 | Loss: 0.00002627
Iteration 26/1000 | Loss: 0.00002625
Iteration 27/1000 | Loss: 0.00002625
Iteration 28/1000 | Loss: 0.00002624
Iteration 29/1000 | Loss: 0.00002623
Iteration 30/1000 | Loss: 0.00002622
Iteration 31/1000 | Loss: 0.00002622
Iteration 32/1000 | Loss: 0.00002621
Iteration 33/1000 | Loss: 0.00002620
Iteration 34/1000 | Loss: 0.00002620
Iteration 35/1000 | Loss: 0.00002620
Iteration 36/1000 | Loss: 0.00002619
Iteration 37/1000 | Loss: 0.00002619
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002619
Iteration 40/1000 | Loss: 0.00002619
Iteration 41/1000 | Loss: 0.00002619
Iteration 42/1000 | Loss: 0.00002619
Iteration 43/1000 | Loss: 0.00002619
Iteration 44/1000 | Loss: 0.00002619
Iteration 45/1000 | Loss: 0.00002616
Iteration 46/1000 | Loss: 0.00002613
Iteration 47/1000 | Loss: 0.00002612
Iteration 48/1000 | Loss: 0.00002612
Iteration 49/1000 | Loss: 0.00002611
Iteration 50/1000 | Loss: 0.00002611
Iteration 51/1000 | Loss: 0.00002611
Iteration 52/1000 | Loss: 0.00002610
Iteration 53/1000 | Loss: 0.00002609
Iteration 54/1000 | Loss: 0.00002609
Iteration 55/1000 | Loss: 0.00002608
Iteration 56/1000 | Loss: 0.00002608
Iteration 57/1000 | Loss: 0.00002608
Iteration 58/1000 | Loss: 0.00002608
Iteration 59/1000 | Loss: 0.00002607
Iteration 60/1000 | Loss: 0.00002607
Iteration 61/1000 | Loss: 0.00002607
Iteration 62/1000 | Loss: 0.00002607
Iteration 63/1000 | Loss: 0.00002607
Iteration 64/1000 | Loss: 0.00002606
Iteration 65/1000 | Loss: 0.00002606
Iteration 66/1000 | Loss: 0.00002606
Iteration 67/1000 | Loss: 0.00002605
Iteration 68/1000 | Loss: 0.00002605
Iteration 69/1000 | Loss: 0.00002605
Iteration 70/1000 | Loss: 0.00002605
Iteration 71/1000 | Loss: 0.00002605
Iteration 72/1000 | Loss: 0.00002605
Iteration 73/1000 | Loss: 0.00002605
Iteration 74/1000 | Loss: 0.00002605
Iteration 75/1000 | Loss: 0.00002605
Iteration 76/1000 | Loss: 0.00002605
Iteration 77/1000 | Loss: 0.00002604
Iteration 78/1000 | Loss: 0.00002604
Iteration 79/1000 | Loss: 0.00002604
Iteration 80/1000 | Loss: 0.00002604
Iteration 81/1000 | Loss: 0.00002604
Iteration 82/1000 | Loss: 0.00002604
Iteration 83/1000 | Loss: 0.00002604
Iteration 84/1000 | Loss: 0.00002604
Iteration 85/1000 | Loss: 0.00002603
Iteration 86/1000 | Loss: 0.00002603
Iteration 87/1000 | Loss: 0.00002603
Iteration 88/1000 | Loss: 0.00002603
Iteration 89/1000 | Loss: 0.00002603
Iteration 90/1000 | Loss: 0.00002603
Iteration 91/1000 | Loss: 0.00002603
Iteration 92/1000 | Loss: 0.00002603
Iteration 93/1000 | Loss: 0.00002603
Iteration 94/1000 | Loss: 0.00002603
Iteration 95/1000 | Loss: 0.00002603
Iteration 96/1000 | Loss: 0.00002603
Iteration 97/1000 | Loss: 0.00002603
Iteration 98/1000 | Loss: 0.00002603
Iteration 99/1000 | Loss: 0.00002603
Iteration 100/1000 | Loss: 0.00002603
Iteration 101/1000 | Loss: 0.00002603
Iteration 102/1000 | Loss: 0.00002603
Iteration 103/1000 | Loss: 0.00002603
Iteration 104/1000 | Loss: 0.00002603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.6027506464743055e-05, 2.6027506464743055e-05, 2.6027506464743055e-05, 2.6027506464743055e-05, 2.6027506464743055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6027506464743055e-05

Optimization complete. Final v2v error: 4.408313751220703 mm

Highest mean error: 4.965169906616211 mm for frame 76

Lowest mean error: 3.853527307510376 mm for frame 0

Saving results

Total time: 71.35915184020996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422867
Iteration 2/25 | Loss: 0.00137042
Iteration 3/25 | Loss: 0.00130447
Iteration 4/25 | Loss: 0.00129649
Iteration 5/25 | Loss: 0.00129122
Iteration 6/25 | Loss: 0.00129056
Iteration 7/25 | Loss: 0.00129056
Iteration 8/25 | Loss: 0.00129056
Iteration 9/25 | Loss: 0.00129056
Iteration 10/25 | Loss: 0.00129056
Iteration 11/25 | Loss: 0.00129056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012905646581202745, 0.0012905646581202745, 0.0012905646581202745, 0.0012905646581202745, 0.0012905646581202745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012905646581202745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78346860
Iteration 2/25 | Loss: 0.00251232
Iteration 3/25 | Loss: 0.00251232
Iteration 4/25 | Loss: 0.00251232
Iteration 5/25 | Loss: 0.00251232
Iteration 6/25 | Loss: 0.00251232
Iteration 7/25 | Loss: 0.00251232
Iteration 8/25 | Loss: 0.00251232
Iteration 9/25 | Loss: 0.00251232
Iteration 10/25 | Loss: 0.00251232
Iteration 11/25 | Loss: 0.00251232
Iteration 12/25 | Loss: 0.00251232
Iteration 13/25 | Loss: 0.00251232
Iteration 14/25 | Loss: 0.00251232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0025123211089521646, 0.0025123211089521646, 0.0025123211089521646, 0.0025123211089521646, 0.0025123211089521646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025123211089521646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251232
Iteration 2/1000 | Loss: 0.00004864
Iteration 3/1000 | Loss: 0.00002953
Iteration 4/1000 | Loss: 0.00002637
Iteration 5/1000 | Loss: 0.00002486
Iteration 6/1000 | Loss: 0.00002376
Iteration 7/1000 | Loss: 0.00002310
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002226
Iteration 10/1000 | Loss: 0.00002199
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002156
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002138
Iteration 15/1000 | Loss: 0.00002133
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002129
Iteration 19/1000 | Loss: 0.00002128
Iteration 20/1000 | Loss: 0.00002127
Iteration 21/1000 | Loss: 0.00002127
Iteration 22/1000 | Loss: 0.00002126
Iteration 23/1000 | Loss: 0.00002126
Iteration 24/1000 | Loss: 0.00002125
Iteration 25/1000 | Loss: 0.00002125
Iteration 26/1000 | Loss: 0.00002124
Iteration 27/1000 | Loss: 0.00002124
Iteration 28/1000 | Loss: 0.00002124
Iteration 29/1000 | Loss: 0.00002124
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002123
Iteration 32/1000 | Loss: 0.00002123
Iteration 33/1000 | Loss: 0.00002120
Iteration 34/1000 | Loss: 0.00002120
Iteration 35/1000 | Loss: 0.00002120
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002119
Iteration 38/1000 | Loss: 0.00002119
Iteration 39/1000 | Loss: 0.00002119
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002119
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002118
Iteration 46/1000 | Loss: 0.00002118
Iteration 47/1000 | Loss: 0.00002118
Iteration 48/1000 | Loss: 0.00002118
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00002117
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002117
Iteration 53/1000 | Loss: 0.00002117
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002116
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002115
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002114
Iteration 62/1000 | Loss: 0.00002114
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002114
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00002113
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002113
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002112
Iteration 72/1000 | Loss: 0.00002112
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002112
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002111
Iteration 81/1000 | Loss: 0.00002111
Iteration 82/1000 | Loss: 0.00002111
Iteration 83/1000 | Loss: 0.00002111
Iteration 84/1000 | Loss: 0.00002110
Iteration 85/1000 | Loss: 0.00002110
Iteration 86/1000 | Loss: 0.00002110
Iteration 87/1000 | Loss: 0.00002110
Iteration 88/1000 | Loss: 0.00002110
Iteration 89/1000 | Loss: 0.00002110
Iteration 90/1000 | Loss: 0.00002110
Iteration 91/1000 | Loss: 0.00002110
Iteration 92/1000 | Loss: 0.00002109
Iteration 93/1000 | Loss: 0.00002109
Iteration 94/1000 | Loss: 0.00002109
Iteration 95/1000 | Loss: 0.00002109
Iteration 96/1000 | Loss: 0.00002108
Iteration 97/1000 | Loss: 0.00002108
Iteration 98/1000 | Loss: 0.00002108
Iteration 99/1000 | Loss: 0.00002108
Iteration 100/1000 | Loss: 0.00002108
Iteration 101/1000 | Loss: 0.00002108
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002107
Iteration 105/1000 | Loss: 0.00002107
Iteration 106/1000 | Loss: 0.00002107
Iteration 107/1000 | Loss: 0.00002107
Iteration 108/1000 | Loss: 0.00002107
Iteration 109/1000 | Loss: 0.00002107
Iteration 110/1000 | Loss: 0.00002107
Iteration 111/1000 | Loss: 0.00002107
Iteration 112/1000 | Loss: 0.00002107
Iteration 113/1000 | Loss: 0.00002107
Iteration 114/1000 | Loss: 0.00002107
Iteration 115/1000 | Loss: 0.00002107
Iteration 116/1000 | Loss: 0.00002107
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002107
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002107
Iteration 123/1000 | Loss: 0.00002107
Iteration 124/1000 | Loss: 0.00002107
Iteration 125/1000 | Loss: 0.00002107
Iteration 126/1000 | Loss: 0.00002107
Iteration 127/1000 | Loss: 0.00002107
Iteration 128/1000 | Loss: 0.00002107
Iteration 129/1000 | Loss: 0.00002107
Iteration 130/1000 | Loss: 0.00002107
Iteration 131/1000 | Loss: 0.00002107
Iteration 132/1000 | Loss: 0.00002107
Iteration 133/1000 | Loss: 0.00002107
Iteration 134/1000 | Loss: 0.00002107
Iteration 135/1000 | Loss: 0.00002107
Iteration 136/1000 | Loss: 0.00002107
Iteration 137/1000 | Loss: 0.00002107
Iteration 138/1000 | Loss: 0.00002107
Iteration 139/1000 | Loss: 0.00002107
Iteration 140/1000 | Loss: 0.00002107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.107251202687621e-05, 2.107251202687621e-05, 2.107251202687621e-05, 2.107251202687621e-05, 2.107251202687621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.107251202687621e-05

Optimization complete. Final v2v error: 4.054849624633789 mm

Highest mean error: 4.333292007446289 mm for frame 196

Lowest mean error: 3.728595733642578 mm for frame 31

Saving results

Total time: 40.75771450996399
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444963
Iteration 2/25 | Loss: 0.00140162
Iteration 3/25 | Loss: 0.00127372
Iteration 4/25 | Loss: 0.00125920
Iteration 5/25 | Loss: 0.00125608
Iteration 6/25 | Loss: 0.00125519
Iteration 7/25 | Loss: 0.00125517
Iteration 8/25 | Loss: 0.00125517
Iteration 9/25 | Loss: 0.00125517
Iteration 10/25 | Loss: 0.00125517
Iteration 11/25 | Loss: 0.00125517
Iteration 12/25 | Loss: 0.00125517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012551664840430021, 0.0012551664840430021, 0.0012551664840430021, 0.0012551664840430021, 0.0012551664840430021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012551664840430021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60754550
Iteration 2/25 | Loss: 0.00209408
Iteration 3/25 | Loss: 0.00209408
Iteration 4/25 | Loss: 0.00209408
Iteration 5/25 | Loss: 0.00209408
Iteration 6/25 | Loss: 0.00209408
Iteration 7/25 | Loss: 0.00209408
Iteration 8/25 | Loss: 0.00209408
Iteration 9/25 | Loss: 0.00209408
Iteration 10/25 | Loss: 0.00209408
Iteration 11/25 | Loss: 0.00209408
Iteration 12/25 | Loss: 0.00209408
Iteration 13/25 | Loss: 0.00209408
Iteration 14/25 | Loss: 0.00209408
Iteration 15/25 | Loss: 0.00209408
Iteration 16/25 | Loss: 0.00209408
Iteration 17/25 | Loss: 0.00209408
Iteration 18/25 | Loss: 0.00209408
Iteration 19/25 | Loss: 0.00209408
Iteration 20/25 | Loss: 0.00209408
Iteration 21/25 | Loss: 0.00209408
Iteration 22/25 | Loss: 0.00209408
Iteration 23/25 | Loss: 0.00209408
Iteration 24/25 | Loss: 0.00209408
Iteration 25/25 | Loss: 0.00209408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209408
Iteration 2/1000 | Loss: 0.00004915
Iteration 3/1000 | Loss: 0.00003314
Iteration 4/1000 | Loss: 0.00002744
Iteration 5/1000 | Loss: 0.00002571
Iteration 6/1000 | Loss: 0.00002459
Iteration 7/1000 | Loss: 0.00002401
Iteration 8/1000 | Loss: 0.00002351
Iteration 9/1000 | Loss: 0.00002325
Iteration 10/1000 | Loss: 0.00002304
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002277
Iteration 13/1000 | Loss: 0.00002276
Iteration 14/1000 | Loss: 0.00002275
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002272
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002267
Iteration 19/1000 | Loss: 0.00002266
Iteration 20/1000 | Loss: 0.00002266
Iteration 21/1000 | Loss: 0.00002266
Iteration 22/1000 | Loss: 0.00002263
Iteration 23/1000 | Loss: 0.00002262
Iteration 24/1000 | Loss: 0.00002262
Iteration 25/1000 | Loss: 0.00002262
Iteration 26/1000 | Loss: 0.00002261
Iteration 27/1000 | Loss: 0.00002261
Iteration 28/1000 | Loss: 0.00002261
Iteration 29/1000 | Loss: 0.00002260
Iteration 30/1000 | Loss: 0.00002260
Iteration 31/1000 | Loss: 0.00002259
Iteration 32/1000 | Loss: 0.00002259
Iteration 33/1000 | Loss: 0.00002258
Iteration 34/1000 | Loss: 0.00002258
Iteration 35/1000 | Loss: 0.00002257
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002256
Iteration 42/1000 | Loss: 0.00002255
Iteration 43/1000 | Loss: 0.00002255
Iteration 44/1000 | Loss: 0.00002255
Iteration 45/1000 | Loss: 0.00002255
Iteration 46/1000 | Loss: 0.00002254
Iteration 47/1000 | Loss: 0.00002254
Iteration 48/1000 | Loss: 0.00002254
Iteration 49/1000 | Loss: 0.00002254
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00002253
Iteration 52/1000 | Loss: 0.00002253
Iteration 53/1000 | Loss: 0.00002253
Iteration 54/1000 | Loss: 0.00002253
Iteration 55/1000 | Loss: 0.00002253
Iteration 56/1000 | Loss: 0.00002253
Iteration 57/1000 | Loss: 0.00002252
Iteration 58/1000 | Loss: 0.00002252
Iteration 59/1000 | Loss: 0.00002252
Iteration 60/1000 | Loss: 0.00002252
Iteration 61/1000 | Loss: 0.00002252
Iteration 62/1000 | Loss: 0.00002251
Iteration 63/1000 | Loss: 0.00002251
Iteration 64/1000 | Loss: 0.00002251
Iteration 65/1000 | Loss: 0.00002251
Iteration 66/1000 | Loss: 0.00002251
Iteration 67/1000 | Loss: 0.00002251
Iteration 68/1000 | Loss: 0.00002251
Iteration 69/1000 | Loss: 0.00002251
Iteration 70/1000 | Loss: 0.00002251
Iteration 71/1000 | Loss: 0.00002251
Iteration 72/1000 | Loss: 0.00002251
Iteration 73/1000 | Loss: 0.00002251
Iteration 74/1000 | Loss: 0.00002251
Iteration 75/1000 | Loss: 0.00002251
Iteration 76/1000 | Loss: 0.00002251
Iteration 77/1000 | Loss: 0.00002250
Iteration 78/1000 | Loss: 0.00002250
Iteration 79/1000 | Loss: 0.00002250
Iteration 80/1000 | Loss: 0.00002250
Iteration 81/1000 | Loss: 0.00002250
Iteration 82/1000 | Loss: 0.00002250
Iteration 83/1000 | Loss: 0.00002250
Iteration 84/1000 | Loss: 0.00002250
Iteration 85/1000 | Loss: 0.00002250
Iteration 86/1000 | Loss: 0.00002250
Iteration 87/1000 | Loss: 0.00002250
Iteration 88/1000 | Loss: 0.00002250
Iteration 89/1000 | Loss: 0.00002250
Iteration 90/1000 | Loss: 0.00002249
Iteration 91/1000 | Loss: 0.00002249
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002249
Iteration 98/1000 | Loss: 0.00002249
Iteration 99/1000 | Loss: 0.00002249
Iteration 100/1000 | Loss: 0.00002249
Iteration 101/1000 | Loss: 0.00002249
Iteration 102/1000 | Loss: 0.00002249
Iteration 103/1000 | Loss: 0.00002249
Iteration 104/1000 | Loss: 0.00002249
Iteration 105/1000 | Loss: 0.00002249
Iteration 106/1000 | Loss: 0.00002249
Iteration 107/1000 | Loss: 0.00002249
Iteration 108/1000 | Loss: 0.00002249
Iteration 109/1000 | Loss: 0.00002248
Iteration 110/1000 | Loss: 0.00002248
Iteration 111/1000 | Loss: 0.00002248
Iteration 112/1000 | Loss: 0.00002248
Iteration 113/1000 | Loss: 0.00002248
Iteration 114/1000 | Loss: 0.00002248
Iteration 115/1000 | Loss: 0.00002248
Iteration 116/1000 | Loss: 0.00002248
Iteration 117/1000 | Loss: 0.00002248
Iteration 118/1000 | Loss: 0.00002248
Iteration 119/1000 | Loss: 0.00002248
Iteration 120/1000 | Loss: 0.00002248
Iteration 121/1000 | Loss: 0.00002248
Iteration 122/1000 | Loss: 0.00002248
Iteration 123/1000 | Loss: 0.00002248
Iteration 124/1000 | Loss: 0.00002248
Iteration 125/1000 | Loss: 0.00002248
Iteration 126/1000 | Loss: 0.00002248
Iteration 127/1000 | Loss: 0.00002248
Iteration 128/1000 | Loss: 0.00002248
Iteration 129/1000 | Loss: 0.00002248
Iteration 130/1000 | Loss: 0.00002248
Iteration 131/1000 | Loss: 0.00002248
Iteration 132/1000 | Loss: 0.00002248
Iteration 133/1000 | Loss: 0.00002248
Iteration 134/1000 | Loss: 0.00002248
Iteration 135/1000 | Loss: 0.00002248
Iteration 136/1000 | Loss: 0.00002248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.247897646157071e-05, 2.247897646157071e-05, 2.247897646157071e-05, 2.247897646157071e-05, 2.247897646157071e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.247897646157071e-05

Optimization complete. Final v2v error: 4.0670013427734375 mm

Highest mean error: 4.654502868652344 mm for frame 20

Lowest mean error: 3.570089817047119 mm for frame 2

Saving results

Total time: 32.80778098106384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436933
Iteration 2/25 | Loss: 0.00132200
Iteration 3/25 | Loss: 0.00122582
Iteration 4/25 | Loss: 0.00121398
Iteration 5/25 | Loss: 0.00121106
Iteration 6/25 | Loss: 0.00121003
Iteration 7/25 | Loss: 0.00120980
Iteration 8/25 | Loss: 0.00120980
Iteration 9/25 | Loss: 0.00120980
Iteration 10/25 | Loss: 0.00120980
Iteration 11/25 | Loss: 0.00120980
Iteration 12/25 | Loss: 0.00120980
Iteration 13/25 | Loss: 0.00120980
Iteration 14/25 | Loss: 0.00120980
Iteration 15/25 | Loss: 0.00120980
Iteration 16/25 | Loss: 0.00120980
Iteration 17/25 | Loss: 0.00120980
Iteration 18/25 | Loss: 0.00120980
Iteration 19/25 | Loss: 0.00120980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012097954750061035, 0.0012097954750061035, 0.0012097954750061035, 0.0012097954750061035, 0.0012097954750061035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012097954750061035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59589279
Iteration 2/25 | Loss: 0.00193879
Iteration 3/25 | Loss: 0.00193878
Iteration 4/25 | Loss: 0.00193878
Iteration 5/25 | Loss: 0.00193878
Iteration 6/25 | Loss: 0.00193878
Iteration 7/25 | Loss: 0.00193878
Iteration 8/25 | Loss: 0.00193878
Iteration 9/25 | Loss: 0.00193878
Iteration 10/25 | Loss: 0.00193878
Iteration 11/25 | Loss: 0.00193878
Iteration 12/25 | Loss: 0.00193878
Iteration 13/25 | Loss: 0.00193878
Iteration 14/25 | Loss: 0.00193878
Iteration 15/25 | Loss: 0.00193878
Iteration 16/25 | Loss: 0.00193878
Iteration 17/25 | Loss: 0.00193878
Iteration 18/25 | Loss: 0.00193878
Iteration 19/25 | Loss: 0.00193878
Iteration 20/25 | Loss: 0.00193878
Iteration 21/25 | Loss: 0.00193878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019387819338589907, 0.0019387819338589907, 0.0019387819338589907, 0.0019387819338589907, 0.0019387819338589907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019387819338589907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193878
Iteration 2/1000 | Loss: 0.00003230
Iteration 3/1000 | Loss: 0.00002274
Iteration 4/1000 | Loss: 0.00002027
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001856
Iteration 7/1000 | Loss: 0.00001820
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001802
Iteration 10/1000 | Loss: 0.00001790
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001767
Iteration 14/1000 | Loss: 0.00001766
Iteration 15/1000 | Loss: 0.00001766
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001766
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001760
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001756
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001755
Iteration 27/1000 | Loss: 0.00001754
Iteration 28/1000 | Loss: 0.00001754
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001752
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001751
Iteration 34/1000 | Loss: 0.00001751
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001748
Iteration 41/1000 | Loss: 0.00001748
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001747
Iteration 47/1000 | Loss: 0.00001747
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001747
Iteration 51/1000 | Loss: 0.00001747
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001745
Iteration 63/1000 | Loss: 0.00001745
Iteration 64/1000 | Loss: 0.00001745
Iteration 65/1000 | Loss: 0.00001745
Iteration 66/1000 | Loss: 0.00001745
Iteration 67/1000 | Loss: 0.00001745
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.7445821868022904e-05, 1.7445821868022904e-05, 1.7445821868022904e-05, 1.7445821868022904e-05, 1.7445821868022904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7445821868022904e-05

Optimization complete. Final v2v error: 3.6186296939849854 mm

Highest mean error: 3.7807247638702393 mm for frame 81

Lowest mean error: 3.432114601135254 mm for frame 106

Saving results

Total time: 28.034964084625244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485948
Iteration 2/25 | Loss: 0.00139302
Iteration 3/25 | Loss: 0.00128294
Iteration 4/25 | Loss: 0.00126687
Iteration 5/25 | Loss: 0.00126078
Iteration 6/25 | Loss: 0.00125937
Iteration 7/25 | Loss: 0.00125934
Iteration 8/25 | Loss: 0.00125934
Iteration 9/25 | Loss: 0.00125934
Iteration 10/25 | Loss: 0.00125934
Iteration 11/25 | Loss: 0.00125934
Iteration 12/25 | Loss: 0.00125934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012593403225764632, 0.0012593403225764632, 0.0012593403225764632, 0.0012593403225764632, 0.0012593403225764632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012593403225764632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.59477711
Iteration 2/25 | Loss: 0.00189074
Iteration 3/25 | Loss: 0.00189072
Iteration 4/25 | Loss: 0.00189072
Iteration 5/25 | Loss: 0.00189072
Iteration 6/25 | Loss: 0.00189072
Iteration 7/25 | Loss: 0.00189072
Iteration 8/25 | Loss: 0.00189072
Iteration 9/25 | Loss: 0.00189072
Iteration 10/25 | Loss: 0.00189072
Iteration 11/25 | Loss: 0.00189072
Iteration 12/25 | Loss: 0.00189072
Iteration 13/25 | Loss: 0.00189072
Iteration 14/25 | Loss: 0.00189072
Iteration 15/25 | Loss: 0.00189072
Iteration 16/25 | Loss: 0.00189072
Iteration 17/25 | Loss: 0.00189072
Iteration 18/25 | Loss: 0.00189072
Iteration 19/25 | Loss: 0.00189072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018907217308878899, 0.0018907217308878899, 0.0018907217308878899, 0.0018907217308878899, 0.0018907217308878899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018907217308878899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189072
Iteration 2/1000 | Loss: 0.00004449
Iteration 3/1000 | Loss: 0.00003634
Iteration 4/1000 | Loss: 0.00003364
Iteration 5/1000 | Loss: 0.00003236
Iteration 6/1000 | Loss: 0.00003155
Iteration 7/1000 | Loss: 0.00003103
Iteration 8/1000 | Loss: 0.00003058
Iteration 9/1000 | Loss: 0.00003024
Iteration 10/1000 | Loss: 0.00003003
Iteration 11/1000 | Loss: 0.00002995
Iteration 12/1000 | Loss: 0.00002993
Iteration 13/1000 | Loss: 0.00002987
Iteration 14/1000 | Loss: 0.00002982
Iteration 15/1000 | Loss: 0.00002980
Iteration 16/1000 | Loss: 0.00002979
Iteration 17/1000 | Loss: 0.00002978
Iteration 18/1000 | Loss: 0.00002978
Iteration 19/1000 | Loss: 0.00002976
Iteration 20/1000 | Loss: 0.00002976
Iteration 21/1000 | Loss: 0.00002975
Iteration 22/1000 | Loss: 0.00002975
Iteration 23/1000 | Loss: 0.00002975
Iteration 24/1000 | Loss: 0.00002974
Iteration 25/1000 | Loss: 0.00002972
Iteration 26/1000 | Loss: 0.00002972
Iteration 27/1000 | Loss: 0.00002972
Iteration 28/1000 | Loss: 0.00002971
Iteration 29/1000 | Loss: 0.00002971
Iteration 30/1000 | Loss: 0.00002971
Iteration 31/1000 | Loss: 0.00002970
Iteration 32/1000 | Loss: 0.00002970
Iteration 33/1000 | Loss: 0.00002970
Iteration 34/1000 | Loss: 0.00002969
Iteration 35/1000 | Loss: 0.00002969
Iteration 36/1000 | Loss: 0.00002968
Iteration 37/1000 | Loss: 0.00002968
Iteration 38/1000 | Loss: 0.00002968
Iteration 39/1000 | Loss: 0.00002967
Iteration 40/1000 | Loss: 0.00002967
Iteration 41/1000 | Loss: 0.00002967
Iteration 42/1000 | Loss: 0.00002966
Iteration 43/1000 | Loss: 0.00002964
Iteration 44/1000 | Loss: 0.00002964
Iteration 45/1000 | Loss: 0.00002962
Iteration 46/1000 | Loss: 0.00002962
Iteration 47/1000 | Loss: 0.00002962
Iteration 48/1000 | Loss: 0.00002961
Iteration 49/1000 | Loss: 0.00002961
Iteration 50/1000 | Loss: 0.00002961
Iteration 51/1000 | Loss: 0.00002960
Iteration 52/1000 | Loss: 0.00002960
Iteration 53/1000 | Loss: 0.00002960
Iteration 54/1000 | Loss: 0.00002960
Iteration 55/1000 | Loss: 0.00002960
Iteration 56/1000 | Loss: 0.00002959
Iteration 57/1000 | Loss: 0.00002959
Iteration 58/1000 | Loss: 0.00002959
Iteration 59/1000 | Loss: 0.00002958
Iteration 60/1000 | Loss: 0.00002958
Iteration 61/1000 | Loss: 0.00002958
Iteration 62/1000 | Loss: 0.00002958
Iteration 63/1000 | Loss: 0.00002958
Iteration 64/1000 | Loss: 0.00002958
Iteration 65/1000 | Loss: 0.00002957
Iteration 66/1000 | Loss: 0.00002957
Iteration 67/1000 | Loss: 0.00002957
Iteration 68/1000 | Loss: 0.00002957
Iteration 69/1000 | Loss: 0.00002957
Iteration 70/1000 | Loss: 0.00002957
Iteration 71/1000 | Loss: 0.00002957
Iteration 72/1000 | Loss: 0.00002957
Iteration 73/1000 | Loss: 0.00002956
Iteration 74/1000 | Loss: 0.00002956
Iteration 75/1000 | Loss: 0.00002956
Iteration 76/1000 | Loss: 0.00002956
Iteration 77/1000 | Loss: 0.00002956
Iteration 78/1000 | Loss: 0.00002956
Iteration 79/1000 | Loss: 0.00002956
Iteration 80/1000 | Loss: 0.00002956
Iteration 81/1000 | Loss: 0.00002956
Iteration 82/1000 | Loss: 0.00002956
Iteration 83/1000 | Loss: 0.00002956
Iteration 84/1000 | Loss: 0.00002956
Iteration 85/1000 | Loss: 0.00002956
Iteration 86/1000 | Loss: 0.00002956
Iteration 87/1000 | Loss: 0.00002956
Iteration 88/1000 | Loss: 0.00002956
Iteration 89/1000 | Loss: 0.00002956
Iteration 90/1000 | Loss: 0.00002956
Iteration 91/1000 | Loss: 0.00002956
Iteration 92/1000 | Loss: 0.00002956
Iteration 93/1000 | Loss: 0.00002956
Iteration 94/1000 | Loss: 0.00002956
Iteration 95/1000 | Loss: 0.00002956
Iteration 96/1000 | Loss: 0.00002956
Iteration 97/1000 | Loss: 0.00002956
Iteration 98/1000 | Loss: 0.00002956
Iteration 99/1000 | Loss: 0.00002956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.955808668048121e-05, 2.955808668048121e-05, 2.955808668048121e-05, 2.955808668048121e-05, 2.955808668048121e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.955808668048121e-05

Optimization complete. Final v2v error: 4.704113960266113 mm

Highest mean error: 5.170876502990723 mm for frame 149

Lowest mean error: 4.400012493133545 mm for frame 35

Saving results

Total time: 33.62218403816223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902514
Iteration 2/25 | Loss: 0.00160455
Iteration 3/25 | Loss: 0.00133221
Iteration 4/25 | Loss: 0.00131355
Iteration 5/25 | Loss: 0.00130611
Iteration 6/25 | Loss: 0.00130348
Iteration 7/25 | Loss: 0.00130327
Iteration 8/25 | Loss: 0.00130327
Iteration 9/25 | Loss: 0.00130325
Iteration 10/25 | Loss: 0.00130325
Iteration 11/25 | Loss: 0.00130325
Iteration 12/25 | Loss: 0.00130325
Iteration 13/25 | Loss: 0.00130325
Iteration 14/25 | Loss: 0.00130325
Iteration 15/25 | Loss: 0.00130325
Iteration 16/25 | Loss: 0.00130325
Iteration 17/25 | Loss: 0.00130325
Iteration 18/25 | Loss: 0.00130325
Iteration 19/25 | Loss: 0.00130325
Iteration 20/25 | Loss: 0.00130325
Iteration 21/25 | Loss: 0.00130325
Iteration 22/25 | Loss: 0.00130325
Iteration 23/25 | Loss: 0.00130325
Iteration 24/25 | Loss: 0.00130325
Iteration 25/25 | Loss: 0.00130325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35720348
Iteration 2/25 | Loss: 0.00219397
Iteration 3/25 | Loss: 0.00219397
Iteration 4/25 | Loss: 0.00219396
Iteration 5/25 | Loss: 0.00219396
Iteration 6/25 | Loss: 0.00219396
Iteration 7/25 | Loss: 0.00219396
Iteration 8/25 | Loss: 0.00219396
Iteration 9/25 | Loss: 0.00219396
Iteration 10/25 | Loss: 0.00219396
Iteration 11/25 | Loss: 0.00219396
Iteration 12/25 | Loss: 0.00219396
Iteration 13/25 | Loss: 0.00219396
Iteration 14/25 | Loss: 0.00219396
Iteration 15/25 | Loss: 0.00219396
Iteration 16/25 | Loss: 0.00219396
Iteration 17/25 | Loss: 0.00219396
Iteration 18/25 | Loss: 0.00219396
Iteration 19/25 | Loss: 0.00219396
Iteration 20/25 | Loss: 0.00219396
Iteration 21/25 | Loss: 0.00219396
Iteration 22/25 | Loss: 0.00219396
Iteration 23/25 | Loss: 0.00219396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002193960826843977, 0.002193960826843977, 0.002193960826843977, 0.002193960826843977, 0.002193960826843977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002193960826843977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219396
Iteration 2/1000 | Loss: 0.00005967
Iteration 3/1000 | Loss: 0.00004271
Iteration 4/1000 | Loss: 0.00003409
Iteration 5/1000 | Loss: 0.00003169
Iteration 6/1000 | Loss: 0.00003036
Iteration 7/1000 | Loss: 0.00002940
Iteration 8/1000 | Loss: 0.00002873
Iteration 9/1000 | Loss: 0.00002809
Iteration 10/1000 | Loss: 0.00002762
Iteration 11/1000 | Loss: 0.00002729
Iteration 12/1000 | Loss: 0.00002703
Iteration 13/1000 | Loss: 0.00002685
Iteration 14/1000 | Loss: 0.00002674
Iteration 15/1000 | Loss: 0.00002670
Iteration 16/1000 | Loss: 0.00002666
Iteration 17/1000 | Loss: 0.00002666
Iteration 18/1000 | Loss: 0.00002666
Iteration 19/1000 | Loss: 0.00002665
Iteration 20/1000 | Loss: 0.00002665
Iteration 21/1000 | Loss: 0.00002665
Iteration 22/1000 | Loss: 0.00002665
Iteration 23/1000 | Loss: 0.00002665
Iteration 24/1000 | Loss: 0.00002665
Iteration 25/1000 | Loss: 0.00002665
Iteration 26/1000 | Loss: 0.00002665
Iteration 27/1000 | Loss: 0.00002665
Iteration 28/1000 | Loss: 0.00002665
Iteration 29/1000 | Loss: 0.00002664
Iteration 30/1000 | Loss: 0.00002664
Iteration 31/1000 | Loss: 0.00002661
Iteration 32/1000 | Loss: 0.00002660
Iteration 33/1000 | Loss: 0.00002660
Iteration 34/1000 | Loss: 0.00002660
Iteration 35/1000 | Loss: 0.00002660
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002659
Iteration 38/1000 | Loss: 0.00002659
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00002656
Iteration 41/1000 | Loss: 0.00002655
Iteration 42/1000 | Loss: 0.00002655
Iteration 43/1000 | Loss: 0.00002655
Iteration 44/1000 | Loss: 0.00002655
Iteration 45/1000 | Loss: 0.00002655
Iteration 46/1000 | Loss: 0.00002654
Iteration 47/1000 | Loss: 0.00002654
Iteration 48/1000 | Loss: 0.00002654
Iteration 49/1000 | Loss: 0.00002653
Iteration 50/1000 | Loss: 0.00002653
Iteration 51/1000 | Loss: 0.00002653
Iteration 52/1000 | Loss: 0.00002652
Iteration 53/1000 | Loss: 0.00002651
Iteration 54/1000 | Loss: 0.00002651
Iteration 55/1000 | Loss: 0.00002651
Iteration 56/1000 | Loss: 0.00002650
Iteration 57/1000 | Loss: 0.00002650
Iteration 58/1000 | Loss: 0.00002650
Iteration 59/1000 | Loss: 0.00002650
Iteration 60/1000 | Loss: 0.00002649
Iteration 61/1000 | Loss: 0.00002649
Iteration 62/1000 | Loss: 0.00002649
Iteration 63/1000 | Loss: 0.00002648
Iteration 64/1000 | Loss: 0.00002648
Iteration 65/1000 | Loss: 0.00002648
Iteration 66/1000 | Loss: 0.00002648
Iteration 67/1000 | Loss: 0.00002647
Iteration 68/1000 | Loss: 0.00002647
Iteration 69/1000 | Loss: 0.00002647
Iteration 70/1000 | Loss: 0.00002647
Iteration 71/1000 | Loss: 0.00002646
Iteration 72/1000 | Loss: 0.00002646
Iteration 73/1000 | Loss: 0.00002646
Iteration 74/1000 | Loss: 0.00002646
Iteration 75/1000 | Loss: 0.00002645
Iteration 76/1000 | Loss: 0.00002645
Iteration 77/1000 | Loss: 0.00002645
Iteration 78/1000 | Loss: 0.00002644
Iteration 79/1000 | Loss: 0.00002644
Iteration 80/1000 | Loss: 0.00002644
Iteration 81/1000 | Loss: 0.00002644
Iteration 82/1000 | Loss: 0.00002644
Iteration 83/1000 | Loss: 0.00002644
Iteration 84/1000 | Loss: 0.00002644
Iteration 85/1000 | Loss: 0.00002643
Iteration 86/1000 | Loss: 0.00002643
Iteration 87/1000 | Loss: 0.00002643
Iteration 88/1000 | Loss: 0.00002643
Iteration 89/1000 | Loss: 0.00002642
Iteration 90/1000 | Loss: 0.00002642
Iteration 91/1000 | Loss: 0.00002642
Iteration 92/1000 | Loss: 0.00002642
Iteration 93/1000 | Loss: 0.00002642
Iteration 94/1000 | Loss: 0.00002641
Iteration 95/1000 | Loss: 0.00002641
Iteration 96/1000 | Loss: 0.00002641
Iteration 97/1000 | Loss: 0.00002640
Iteration 98/1000 | Loss: 0.00002640
Iteration 99/1000 | Loss: 0.00002640
Iteration 100/1000 | Loss: 0.00002640
Iteration 101/1000 | Loss: 0.00002639
Iteration 102/1000 | Loss: 0.00002639
Iteration 103/1000 | Loss: 0.00002639
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002637
Iteration 108/1000 | Loss: 0.00002637
Iteration 109/1000 | Loss: 0.00002637
Iteration 110/1000 | Loss: 0.00002637
Iteration 111/1000 | Loss: 0.00002637
Iteration 112/1000 | Loss: 0.00002636
Iteration 113/1000 | Loss: 0.00002636
Iteration 114/1000 | Loss: 0.00002636
Iteration 115/1000 | Loss: 0.00002636
Iteration 116/1000 | Loss: 0.00002636
Iteration 117/1000 | Loss: 0.00002636
Iteration 118/1000 | Loss: 0.00002636
Iteration 119/1000 | Loss: 0.00002636
Iteration 120/1000 | Loss: 0.00002636
Iteration 121/1000 | Loss: 0.00002636
Iteration 122/1000 | Loss: 0.00002636
Iteration 123/1000 | Loss: 0.00002636
Iteration 124/1000 | Loss: 0.00002636
Iteration 125/1000 | Loss: 0.00002636
Iteration 126/1000 | Loss: 0.00002636
Iteration 127/1000 | Loss: 0.00002636
Iteration 128/1000 | Loss: 0.00002636
Iteration 129/1000 | Loss: 0.00002636
Iteration 130/1000 | Loss: 0.00002635
Iteration 131/1000 | Loss: 0.00002635
Iteration 132/1000 | Loss: 0.00002635
Iteration 133/1000 | Loss: 0.00002635
Iteration 134/1000 | Loss: 0.00002635
Iteration 135/1000 | Loss: 0.00002635
Iteration 136/1000 | Loss: 0.00002635
Iteration 137/1000 | Loss: 0.00002635
Iteration 138/1000 | Loss: 0.00002634
Iteration 139/1000 | Loss: 0.00002634
Iteration 140/1000 | Loss: 0.00002634
Iteration 141/1000 | Loss: 0.00002634
Iteration 142/1000 | Loss: 0.00002634
Iteration 143/1000 | Loss: 0.00002634
Iteration 144/1000 | Loss: 0.00002634
Iteration 145/1000 | Loss: 0.00002634
Iteration 146/1000 | Loss: 0.00002634
Iteration 147/1000 | Loss: 0.00002634
Iteration 148/1000 | Loss: 0.00002634
Iteration 149/1000 | Loss: 0.00002634
Iteration 150/1000 | Loss: 0.00002634
Iteration 151/1000 | Loss: 0.00002634
Iteration 152/1000 | Loss: 0.00002634
Iteration 153/1000 | Loss: 0.00002634
Iteration 154/1000 | Loss: 0.00002634
Iteration 155/1000 | Loss: 0.00002634
Iteration 156/1000 | Loss: 0.00002634
Iteration 157/1000 | Loss: 0.00002634
Iteration 158/1000 | Loss: 0.00002634
Iteration 159/1000 | Loss: 0.00002634
Iteration 160/1000 | Loss: 0.00002634
Iteration 161/1000 | Loss: 0.00002634
Iteration 162/1000 | Loss: 0.00002634
Iteration 163/1000 | Loss: 0.00002634
Iteration 164/1000 | Loss: 0.00002634
Iteration 165/1000 | Loss: 0.00002634
Iteration 166/1000 | Loss: 0.00002634
Iteration 167/1000 | Loss: 0.00002634
Iteration 168/1000 | Loss: 0.00002634
Iteration 169/1000 | Loss: 0.00002634
Iteration 170/1000 | Loss: 0.00002634
Iteration 171/1000 | Loss: 0.00002634
Iteration 172/1000 | Loss: 0.00002634
Iteration 173/1000 | Loss: 0.00002634
Iteration 174/1000 | Loss: 0.00002634
Iteration 175/1000 | Loss: 0.00002634
Iteration 176/1000 | Loss: 0.00002634
Iteration 177/1000 | Loss: 0.00002634
Iteration 178/1000 | Loss: 0.00002634
Iteration 179/1000 | Loss: 0.00002634
Iteration 180/1000 | Loss: 0.00002634
Iteration 181/1000 | Loss: 0.00002634
Iteration 182/1000 | Loss: 0.00002634
Iteration 183/1000 | Loss: 0.00002634
Iteration 184/1000 | Loss: 0.00002634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.633627264003735e-05, 2.633627264003735e-05, 2.633627264003735e-05, 2.633627264003735e-05, 2.633627264003735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.633627264003735e-05

Optimization complete. Final v2v error: 4.403941631317139 mm

Highest mean error: 5.6902289390563965 mm for frame 64

Lowest mean error: 3.7203381061553955 mm for frame 94

Saving results

Total time: 41.295352935791016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469743
Iteration 2/25 | Loss: 0.00139577
Iteration 3/25 | Loss: 0.00128963
Iteration 4/25 | Loss: 0.00127969
Iteration 5/25 | Loss: 0.00127614
Iteration 6/25 | Loss: 0.00127520
Iteration 7/25 | Loss: 0.00127510
Iteration 8/25 | Loss: 0.00127510
Iteration 9/25 | Loss: 0.00127510
Iteration 10/25 | Loss: 0.00127510
Iteration 11/25 | Loss: 0.00127510
Iteration 12/25 | Loss: 0.00127510
Iteration 13/25 | Loss: 0.00127510
Iteration 14/25 | Loss: 0.00127510
Iteration 15/25 | Loss: 0.00127510
Iteration 16/25 | Loss: 0.00127510
Iteration 17/25 | Loss: 0.00127510
Iteration 18/25 | Loss: 0.00127510
Iteration 19/25 | Loss: 0.00127510
Iteration 20/25 | Loss: 0.00127510
Iteration 21/25 | Loss: 0.00127510
Iteration 22/25 | Loss: 0.00127510
Iteration 23/25 | Loss: 0.00127510
Iteration 24/25 | Loss: 0.00127510
Iteration 25/25 | Loss: 0.00127510

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.83131313
Iteration 2/25 | Loss: 0.00200897
Iteration 3/25 | Loss: 0.00200895
Iteration 4/25 | Loss: 0.00200895
Iteration 5/25 | Loss: 0.00200895
Iteration 6/25 | Loss: 0.00200895
Iteration 7/25 | Loss: 0.00200895
Iteration 8/25 | Loss: 0.00200895
Iteration 9/25 | Loss: 0.00200895
Iteration 10/25 | Loss: 0.00200895
Iteration 11/25 | Loss: 0.00200895
Iteration 12/25 | Loss: 0.00200895
Iteration 13/25 | Loss: 0.00200895
Iteration 14/25 | Loss: 0.00200895
Iteration 15/25 | Loss: 0.00200895
Iteration 16/25 | Loss: 0.00200895
Iteration 17/25 | Loss: 0.00200895
Iteration 18/25 | Loss: 0.00200895
Iteration 19/25 | Loss: 0.00200895
Iteration 20/25 | Loss: 0.00200895
Iteration 21/25 | Loss: 0.00200895
Iteration 22/25 | Loss: 0.00200895
Iteration 23/25 | Loss: 0.00200895
Iteration 24/25 | Loss: 0.00200895
Iteration 25/25 | Loss: 0.00200895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200895
Iteration 2/1000 | Loss: 0.00004220
Iteration 3/1000 | Loss: 0.00003230
Iteration 4/1000 | Loss: 0.00002984
Iteration 5/1000 | Loss: 0.00002860
Iteration 6/1000 | Loss: 0.00002755
Iteration 7/1000 | Loss: 0.00002690
Iteration 8/1000 | Loss: 0.00002641
Iteration 9/1000 | Loss: 0.00002611
Iteration 10/1000 | Loss: 0.00002588
Iteration 11/1000 | Loss: 0.00002578
Iteration 12/1000 | Loss: 0.00002571
Iteration 13/1000 | Loss: 0.00002567
Iteration 14/1000 | Loss: 0.00002562
Iteration 15/1000 | Loss: 0.00002561
Iteration 16/1000 | Loss: 0.00002559
Iteration 17/1000 | Loss: 0.00002556
Iteration 18/1000 | Loss: 0.00002556
Iteration 19/1000 | Loss: 0.00002555
Iteration 20/1000 | Loss: 0.00002555
Iteration 21/1000 | Loss: 0.00002553
Iteration 22/1000 | Loss: 0.00002553
Iteration 23/1000 | Loss: 0.00002553
Iteration 24/1000 | Loss: 0.00002552
Iteration 25/1000 | Loss: 0.00002552
Iteration 26/1000 | Loss: 0.00002552
Iteration 27/1000 | Loss: 0.00002552
Iteration 28/1000 | Loss: 0.00002552
Iteration 29/1000 | Loss: 0.00002552
Iteration 30/1000 | Loss: 0.00002552
Iteration 31/1000 | Loss: 0.00002552
Iteration 32/1000 | Loss: 0.00002552
Iteration 33/1000 | Loss: 0.00002552
Iteration 34/1000 | Loss: 0.00002551
Iteration 35/1000 | Loss: 0.00002551
Iteration 36/1000 | Loss: 0.00002550
Iteration 37/1000 | Loss: 0.00002550
Iteration 38/1000 | Loss: 0.00002550
Iteration 39/1000 | Loss: 0.00002550
Iteration 40/1000 | Loss: 0.00002549
Iteration 41/1000 | Loss: 0.00002549
Iteration 42/1000 | Loss: 0.00002549
Iteration 43/1000 | Loss: 0.00002549
Iteration 44/1000 | Loss: 0.00002549
Iteration 45/1000 | Loss: 0.00002548
Iteration 46/1000 | Loss: 0.00002548
Iteration 47/1000 | Loss: 0.00002548
Iteration 48/1000 | Loss: 0.00002547
Iteration 49/1000 | Loss: 0.00002547
Iteration 50/1000 | Loss: 0.00002547
Iteration 51/1000 | Loss: 0.00002547
Iteration 52/1000 | Loss: 0.00002547
Iteration 53/1000 | Loss: 0.00002546
Iteration 54/1000 | Loss: 0.00002546
Iteration 55/1000 | Loss: 0.00002546
Iteration 56/1000 | Loss: 0.00002546
Iteration 57/1000 | Loss: 0.00002546
Iteration 58/1000 | Loss: 0.00002545
Iteration 59/1000 | Loss: 0.00002545
Iteration 60/1000 | Loss: 0.00002545
Iteration 61/1000 | Loss: 0.00002545
Iteration 62/1000 | Loss: 0.00002544
Iteration 63/1000 | Loss: 0.00002544
Iteration 64/1000 | Loss: 0.00002544
Iteration 65/1000 | Loss: 0.00002544
Iteration 66/1000 | Loss: 0.00002543
Iteration 67/1000 | Loss: 0.00002543
Iteration 68/1000 | Loss: 0.00002543
Iteration 69/1000 | Loss: 0.00002543
Iteration 70/1000 | Loss: 0.00002543
Iteration 71/1000 | Loss: 0.00002543
Iteration 72/1000 | Loss: 0.00002542
Iteration 73/1000 | Loss: 0.00002542
Iteration 74/1000 | Loss: 0.00002542
Iteration 75/1000 | Loss: 0.00002542
Iteration 76/1000 | Loss: 0.00002542
Iteration 77/1000 | Loss: 0.00002542
Iteration 78/1000 | Loss: 0.00002542
Iteration 79/1000 | Loss: 0.00002541
Iteration 80/1000 | Loss: 0.00002541
Iteration 81/1000 | Loss: 0.00002541
Iteration 82/1000 | Loss: 0.00002541
Iteration 83/1000 | Loss: 0.00002541
Iteration 84/1000 | Loss: 0.00002541
Iteration 85/1000 | Loss: 0.00002541
Iteration 86/1000 | Loss: 0.00002541
Iteration 87/1000 | Loss: 0.00002541
Iteration 88/1000 | Loss: 0.00002541
Iteration 89/1000 | Loss: 0.00002541
Iteration 90/1000 | Loss: 0.00002541
Iteration 91/1000 | Loss: 0.00002541
Iteration 92/1000 | Loss: 0.00002540
Iteration 93/1000 | Loss: 0.00002540
Iteration 94/1000 | Loss: 0.00002540
Iteration 95/1000 | Loss: 0.00002540
Iteration 96/1000 | Loss: 0.00002539
Iteration 97/1000 | Loss: 0.00002539
Iteration 98/1000 | Loss: 0.00002539
Iteration 99/1000 | Loss: 0.00002539
Iteration 100/1000 | Loss: 0.00002539
Iteration 101/1000 | Loss: 0.00002539
Iteration 102/1000 | Loss: 0.00002539
Iteration 103/1000 | Loss: 0.00002539
Iteration 104/1000 | Loss: 0.00002539
Iteration 105/1000 | Loss: 0.00002539
Iteration 106/1000 | Loss: 0.00002539
Iteration 107/1000 | Loss: 0.00002539
Iteration 108/1000 | Loss: 0.00002538
Iteration 109/1000 | Loss: 0.00002538
Iteration 110/1000 | Loss: 0.00002538
Iteration 111/1000 | Loss: 0.00002538
Iteration 112/1000 | Loss: 0.00002538
Iteration 113/1000 | Loss: 0.00002538
Iteration 114/1000 | Loss: 0.00002538
Iteration 115/1000 | Loss: 0.00002538
Iteration 116/1000 | Loss: 0.00002537
Iteration 117/1000 | Loss: 0.00002537
Iteration 118/1000 | Loss: 0.00002537
Iteration 119/1000 | Loss: 0.00002537
Iteration 120/1000 | Loss: 0.00002537
Iteration 121/1000 | Loss: 0.00002537
Iteration 122/1000 | Loss: 0.00002537
Iteration 123/1000 | Loss: 0.00002537
Iteration 124/1000 | Loss: 0.00002537
Iteration 125/1000 | Loss: 0.00002537
Iteration 126/1000 | Loss: 0.00002537
Iteration 127/1000 | Loss: 0.00002537
Iteration 128/1000 | Loss: 0.00002537
Iteration 129/1000 | Loss: 0.00002537
Iteration 130/1000 | Loss: 0.00002537
Iteration 131/1000 | Loss: 0.00002536
Iteration 132/1000 | Loss: 0.00002536
Iteration 133/1000 | Loss: 0.00002536
Iteration 134/1000 | Loss: 0.00002536
Iteration 135/1000 | Loss: 0.00002536
Iteration 136/1000 | Loss: 0.00002536
Iteration 137/1000 | Loss: 0.00002536
Iteration 138/1000 | Loss: 0.00002536
Iteration 139/1000 | Loss: 0.00002536
Iteration 140/1000 | Loss: 0.00002536
Iteration 141/1000 | Loss: 0.00002536
Iteration 142/1000 | Loss: 0.00002536
Iteration 143/1000 | Loss: 0.00002536
Iteration 144/1000 | Loss: 0.00002536
Iteration 145/1000 | Loss: 0.00002536
Iteration 146/1000 | Loss: 0.00002536
Iteration 147/1000 | Loss: 0.00002536
Iteration 148/1000 | Loss: 0.00002536
Iteration 149/1000 | Loss: 0.00002536
Iteration 150/1000 | Loss: 0.00002536
Iteration 151/1000 | Loss: 0.00002536
Iteration 152/1000 | Loss: 0.00002536
Iteration 153/1000 | Loss: 0.00002536
Iteration 154/1000 | Loss: 0.00002536
Iteration 155/1000 | Loss: 0.00002536
Iteration 156/1000 | Loss: 0.00002536
Iteration 157/1000 | Loss: 0.00002536
Iteration 158/1000 | Loss: 0.00002536
Iteration 159/1000 | Loss: 0.00002536
Iteration 160/1000 | Loss: 0.00002536
Iteration 161/1000 | Loss: 0.00002536
Iteration 162/1000 | Loss: 0.00002536
Iteration 163/1000 | Loss: 0.00002536
Iteration 164/1000 | Loss: 0.00002536
Iteration 165/1000 | Loss: 0.00002536
Iteration 166/1000 | Loss: 0.00002536
Iteration 167/1000 | Loss: 0.00002536
Iteration 168/1000 | Loss: 0.00002536
Iteration 169/1000 | Loss: 0.00002536
Iteration 170/1000 | Loss: 0.00002536
Iteration 171/1000 | Loss: 0.00002536
Iteration 172/1000 | Loss: 0.00002536
Iteration 173/1000 | Loss: 0.00002536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.5363317035953514e-05, 2.5363317035953514e-05, 2.5363317035953514e-05, 2.5363317035953514e-05, 2.5363317035953514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5363317035953514e-05

Optimization complete. Final v2v error: 4.354561805725098 mm

Highest mean error: 4.737307071685791 mm for frame 71

Lowest mean error: 4.09637975692749 mm for frame 18

Saving results

Total time: 35.41441488265991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851985
Iteration 2/25 | Loss: 0.00143227
Iteration 3/25 | Loss: 0.00132483
Iteration 4/25 | Loss: 0.00130527
Iteration 5/25 | Loss: 0.00129943
Iteration 6/25 | Loss: 0.00129881
Iteration 7/25 | Loss: 0.00129881
Iteration 8/25 | Loss: 0.00129881
Iteration 9/25 | Loss: 0.00129881
Iteration 10/25 | Loss: 0.00129881
Iteration 11/25 | Loss: 0.00129881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012988133821636438, 0.0012988133821636438, 0.0012988133821636438, 0.0012988133821636438, 0.0012988133821636438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012988133821636438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68682659
Iteration 2/25 | Loss: 0.00207298
Iteration 3/25 | Loss: 0.00207298
Iteration 4/25 | Loss: 0.00207298
Iteration 5/25 | Loss: 0.00207298
Iteration 6/25 | Loss: 0.00207298
Iteration 7/25 | Loss: 0.00207298
Iteration 8/25 | Loss: 0.00207298
Iteration 9/25 | Loss: 0.00207298
Iteration 10/25 | Loss: 0.00207298
Iteration 11/25 | Loss: 0.00207298
Iteration 12/25 | Loss: 0.00207298
Iteration 13/25 | Loss: 0.00207298
Iteration 14/25 | Loss: 0.00207298
Iteration 15/25 | Loss: 0.00207298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020729796960949898, 0.0020729796960949898, 0.0020729796960949898, 0.0020729796960949898, 0.0020729796960949898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020729796960949898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207298
Iteration 2/1000 | Loss: 0.00003959
Iteration 3/1000 | Loss: 0.00003387
Iteration 4/1000 | Loss: 0.00003148
Iteration 5/1000 | Loss: 0.00003014
Iteration 6/1000 | Loss: 0.00002948
Iteration 7/1000 | Loss: 0.00002901
Iteration 8/1000 | Loss: 0.00002870
Iteration 9/1000 | Loss: 0.00002844
Iteration 10/1000 | Loss: 0.00002829
Iteration 11/1000 | Loss: 0.00002828
Iteration 12/1000 | Loss: 0.00002823
Iteration 13/1000 | Loss: 0.00002823
Iteration 14/1000 | Loss: 0.00002823
Iteration 15/1000 | Loss: 0.00002823
Iteration 16/1000 | Loss: 0.00002822
Iteration 17/1000 | Loss: 0.00002822
Iteration 18/1000 | Loss: 0.00002822
Iteration 19/1000 | Loss: 0.00002822
Iteration 20/1000 | Loss: 0.00002821
Iteration 21/1000 | Loss: 0.00002821
Iteration 22/1000 | Loss: 0.00002814
Iteration 23/1000 | Loss: 0.00002811
Iteration 24/1000 | Loss: 0.00002810
Iteration 25/1000 | Loss: 0.00002810
Iteration 26/1000 | Loss: 0.00002810
Iteration 27/1000 | Loss: 0.00002809
Iteration 28/1000 | Loss: 0.00002807
Iteration 29/1000 | Loss: 0.00002805
Iteration 30/1000 | Loss: 0.00002804
Iteration 31/1000 | Loss: 0.00002804
Iteration 32/1000 | Loss: 0.00002803
Iteration 33/1000 | Loss: 0.00002803
Iteration 34/1000 | Loss: 0.00002803
Iteration 35/1000 | Loss: 0.00002803
Iteration 36/1000 | Loss: 0.00002803
Iteration 37/1000 | Loss: 0.00002802
Iteration 38/1000 | Loss: 0.00002802
Iteration 39/1000 | Loss: 0.00002802
Iteration 40/1000 | Loss: 0.00002801
Iteration 41/1000 | Loss: 0.00002796
Iteration 42/1000 | Loss: 0.00002796
Iteration 43/1000 | Loss: 0.00002795
Iteration 44/1000 | Loss: 0.00002795
Iteration 45/1000 | Loss: 0.00002794
Iteration 46/1000 | Loss: 0.00002793
Iteration 47/1000 | Loss: 0.00002793
Iteration 48/1000 | Loss: 0.00002792
Iteration 49/1000 | Loss: 0.00002792
Iteration 50/1000 | Loss: 0.00002791
Iteration 51/1000 | Loss: 0.00002791
Iteration 52/1000 | Loss: 0.00002791
Iteration 53/1000 | Loss: 0.00002790
Iteration 54/1000 | Loss: 0.00002790
Iteration 55/1000 | Loss: 0.00002789
Iteration 56/1000 | Loss: 0.00002789
Iteration 57/1000 | Loss: 0.00002789
Iteration 58/1000 | Loss: 0.00002789
Iteration 59/1000 | Loss: 0.00002789
Iteration 60/1000 | Loss: 0.00002789
Iteration 61/1000 | Loss: 0.00002788
Iteration 62/1000 | Loss: 0.00002788
Iteration 63/1000 | Loss: 0.00002788
Iteration 64/1000 | Loss: 0.00002787
Iteration 65/1000 | Loss: 0.00002787
Iteration 66/1000 | Loss: 0.00002786
Iteration 67/1000 | Loss: 0.00002786
Iteration 68/1000 | Loss: 0.00002786
Iteration 69/1000 | Loss: 0.00002786
Iteration 70/1000 | Loss: 0.00002786
Iteration 71/1000 | Loss: 0.00002786
Iteration 72/1000 | Loss: 0.00002786
Iteration 73/1000 | Loss: 0.00002785
Iteration 74/1000 | Loss: 0.00002785
Iteration 75/1000 | Loss: 0.00002785
Iteration 76/1000 | Loss: 0.00002785
Iteration 77/1000 | Loss: 0.00002785
Iteration 78/1000 | Loss: 0.00002784
Iteration 79/1000 | Loss: 0.00002784
Iteration 80/1000 | Loss: 0.00002784
Iteration 81/1000 | Loss: 0.00002784
Iteration 82/1000 | Loss: 0.00002784
Iteration 83/1000 | Loss: 0.00002784
Iteration 84/1000 | Loss: 0.00002784
Iteration 85/1000 | Loss: 0.00002784
Iteration 86/1000 | Loss: 0.00002784
Iteration 87/1000 | Loss: 0.00002784
Iteration 88/1000 | Loss: 0.00002784
Iteration 89/1000 | Loss: 0.00002784
Iteration 90/1000 | Loss: 0.00002783
Iteration 91/1000 | Loss: 0.00002783
Iteration 92/1000 | Loss: 0.00002783
Iteration 93/1000 | Loss: 0.00002783
Iteration 94/1000 | Loss: 0.00002783
Iteration 95/1000 | Loss: 0.00002783
Iteration 96/1000 | Loss: 0.00002783
Iteration 97/1000 | Loss: 0.00002783
Iteration 98/1000 | Loss: 0.00002783
Iteration 99/1000 | Loss: 0.00002782
Iteration 100/1000 | Loss: 0.00002782
Iteration 101/1000 | Loss: 0.00002782
Iteration 102/1000 | Loss: 0.00002782
Iteration 103/1000 | Loss: 0.00002782
Iteration 104/1000 | Loss: 0.00002782
Iteration 105/1000 | Loss: 0.00002782
Iteration 106/1000 | Loss: 0.00002782
Iteration 107/1000 | Loss: 0.00002782
Iteration 108/1000 | Loss: 0.00002782
Iteration 109/1000 | Loss: 0.00002782
Iteration 110/1000 | Loss: 0.00002782
Iteration 111/1000 | Loss: 0.00002782
Iteration 112/1000 | Loss: 0.00002782
Iteration 113/1000 | Loss: 0.00002782
Iteration 114/1000 | Loss: 0.00002782
Iteration 115/1000 | Loss: 0.00002782
Iteration 116/1000 | Loss: 0.00002782
Iteration 117/1000 | Loss: 0.00002782
Iteration 118/1000 | Loss: 0.00002782
Iteration 119/1000 | Loss: 0.00002782
Iteration 120/1000 | Loss: 0.00002782
Iteration 121/1000 | Loss: 0.00002782
Iteration 122/1000 | Loss: 0.00002782
Iteration 123/1000 | Loss: 0.00002782
Iteration 124/1000 | Loss: 0.00002782
Iteration 125/1000 | Loss: 0.00002782
Iteration 126/1000 | Loss: 0.00002782
Iteration 127/1000 | Loss: 0.00002782
Iteration 128/1000 | Loss: 0.00002782
Iteration 129/1000 | Loss: 0.00002782
Iteration 130/1000 | Loss: 0.00002782
Iteration 131/1000 | Loss: 0.00002782
Iteration 132/1000 | Loss: 0.00002782
Iteration 133/1000 | Loss: 0.00002782
Iteration 134/1000 | Loss: 0.00002782
Iteration 135/1000 | Loss: 0.00002782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.781522744044196e-05, 2.781522744044196e-05, 2.781522744044196e-05, 2.781522744044196e-05, 2.781522744044196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.781522744044196e-05

Optimization complete. Final v2v error: 4.503210544586182 mm

Highest mean error: 5.003679275512695 mm for frame 58

Lowest mean error: 3.7261710166931152 mm for frame 13

Saving results

Total time: 37.15620231628418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864378
Iteration 2/25 | Loss: 0.00182954
Iteration 3/25 | Loss: 0.00144797
Iteration 4/25 | Loss: 0.00141016
Iteration 5/25 | Loss: 0.00140087
Iteration 6/25 | Loss: 0.00139885
Iteration 7/25 | Loss: 0.00139855
Iteration 8/25 | Loss: 0.00139855
Iteration 9/25 | Loss: 0.00139855
Iteration 10/25 | Loss: 0.00139855
Iteration 11/25 | Loss: 0.00139855
Iteration 12/25 | Loss: 0.00139855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013985487166792154, 0.0013985487166792154, 0.0013985487166792154, 0.0013985487166792154, 0.0013985487166792154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013985487166792154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55195594
Iteration 2/25 | Loss: 0.00145798
Iteration 3/25 | Loss: 0.00145794
Iteration 4/25 | Loss: 0.00145794
Iteration 5/25 | Loss: 0.00145794
Iteration 6/25 | Loss: 0.00145794
Iteration 7/25 | Loss: 0.00145794
Iteration 8/25 | Loss: 0.00145794
Iteration 9/25 | Loss: 0.00145794
Iteration 10/25 | Loss: 0.00145794
Iteration 11/25 | Loss: 0.00145794
Iteration 12/25 | Loss: 0.00145794
Iteration 13/25 | Loss: 0.00145794
Iteration 14/25 | Loss: 0.00145794
Iteration 15/25 | Loss: 0.00145794
Iteration 16/25 | Loss: 0.00145794
Iteration 17/25 | Loss: 0.00145794
Iteration 18/25 | Loss: 0.00145794
Iteration 19/25 | Loss: 0.00145794
Iteration 20/25 | Loss: 0.00145794
Iteration 21/25 | Loss: 0.00145794
Iteration 22/25 | Loss: 0.00145794
Iteration 23/25 | Loss: 0.00145794
Iteration 24/25 | Loss: 0.00145794
Iteration 25/25 | Loss: 0.00145794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145794
Iteration 2/1000 | Loss: 0.00006717
Iteration 3/1000 | Loss: 0.00005485
Iteration 4/1000 | Loss: 0.00005012
Iteration 5/1000 | Loss: 0.00004741
Iteration 6/1000 | Loss: 0.00004577
Iteration 7/1000 | Loss: 0.00004491
Iteration 8/1000 | Loss: 0.00004435
Iteration 9/1000 | Loss: 0.00004377
Iteration 10/1000 | Loss: 0.00004342
Iteration 11/1000 | Loss: 0.00004323
Iteration 12/1000 | Loss: 0.00004308
Iteration 13/1000 | Loss: 0.00004307
Iteration 14/1000 | Loss: 0.00004306
Iteration 15/1000 | Loss: 0.00004302
Iteration 16/1000 | Loss: 0.00004299
Iteration 17/1000 | Loss: 0.00004298
Iteration 18/1000 | Loss: 0.00004296
Iteration 19/1000 | Loss: 0.00004296
Iteration 20/1000 | Loss: 0.00004291
Iteration 21/1000 | Loss: 0.00004291
Iteration 22/1000 | Loss: 0.00004291
Iteration 23/1000 | Loss: 0.00004289
Iteration 24/1000 | Loss: 0.00004288
Iteration 25/1000 | Loss: 0.00004284
Iteration 26/1000 | Loss: 0.00004283
Iteration 27/1000 | Loss: 0.00004282
Iteration 28/1000 | Loss: 0.00004282
Iteration 29/1000 | Loss: 0.00004282
Iteration 30/1000 | Loss: 0.00004281
Iteration 31/1000 | Loss: 0.00004280
Iteration 32/1000 | Loss: 0.00004280
Iteration 33/1000 | Loss: 0.00004280
Iteration 34/1000 | Loss: 0.00004280
Iteration 35/1000 | Loss: 0.00004279
Iteration 36/1000 | Loss: 0.00004279
Iteration 37/1000 | Loss: 0.00004279
Iteration 38/1000 | Loss: 0.00004279
Iteration 39/1000 | Loss: 0.00004279
Iteration 40/1000 | Loss: 0.00004279
Iteration 41/1000 | Loss: 0.00004279
Iteration 42/1000 | Loss: 0.00004279
Iteration 43/1000 | Loss: 0.00004278
Iteration 44/1000 | Loss: 0.00004278
Iteration 45/1000 | Loss: 0.00004278
Iteration 46/1000 | Loss: 0.00004278
Iteration 47/1000 | Loss: 0.00004278
Iteration 48/1000 | Loss: 0.00004278
Iteration 49/1000 | Loss: 0.00004277
Iteration 50/1000 | Loss: 0.00004277
Iteration 51/1000 | Loss: 0.00004277
Iteration 52/1000 | Loss: 0.00004277
Iteration 53/1000 | Loss: 0.00004276
Iteration 54/1000 | Loss: 0.00004276
Iteration 55/1000 | Loss: 0.00004275
Iteration 56/1000 | Loss: 0.00004275
Iteration 57/1000 | Loss: 0.00004274
Iteration 58/1000 | Loss: 0.00004274
Iteration 59/1000 | Loss: 0.00004274
Iteration 60/1000 | Loss: 0.00004274
Iteration 61/1000 | Loss: 0.00004274
Iteration 62/1000 | Loss: 0.00004274
Iteration 63/1000 | Loss: 0.00004274
Iteration 64/1000 | Loss: 0.00004274
Iteration 65/1000 | Loss: 0.00004274
Iteration 66/1000 | Loss: 0.00004273
Iteration 67/1000 | Loss: 0.00004273
Iteration 68/1000 | Loss: 0.00004272
Iteration 69/1000 | Loss: 0.00004272
Iteration 70/1000 | Loss: 0.00004272
Iteration 71/1000 | Loss: 0.00004272
Iteration 72/1000 | Loss: 0.00004272
Iteration 73/1000 | Loss: 0.00004272
Iteration 74/1000 | Loss: 0.00004272
Iteration 75/1000 | Loss: 0.00004271
Iteration 76/1000 | Loss: 0.00004271
Iteration 77/1000 | Loss: 0.00004271
Iteration 78/1000 | Loss: 0.00004271
Iteration 79/1000 | Loss: 0.00004271
Iteration 80/1000 | Loss: 0.00004270
Iteration 81/1000 | Loss: 0.00004270
Iteration 82/1000 | Loss: 0.00004270
Iteration 83/1000 | Loss: 0.00004270
Iteration 84/1000 | Loss: 0.00004269
Iteration 85/1000 | Loss: 0.00004269
Iteration 86/1000 | Loss: 0.00004268
Iteration 87/1000 | Loss: 0.00004268
Iteration 88/1000 | Loss: 0.00004268
Iteration 89/1000 | Loss: 0.00004267
Iteration 90/1000 | Loss: 0.00004267
Iteration 91/1000 | Loss: 0.00004266
Iteration 92/1000 | Loss: 0.00004266
Iteration 93/1000 | Loss: 0.00004266
Iteration 94/1000 | Loss: 0.00004266
Iteration 95/1000 | Loss: 0.00004266
Iteration 96/1000 | Loss: 0.00004265
Iteration 97/1000 | Loss: 0.00004265
Iteration 98/1000 | Loss: 0.00004265
Iteration 99/1000 | Loss: 0.00004265
Iteration 100/1000 | Loss: 0.00004265
Iteration 101/1000 | Loss: 0.00004265
Iteration 102/1000 | Loss: 0.00004265
Iteration 103/1000 | Loss: 0.00004264
Iteration 104/1000 | Loss: 0.00004264
Iteration 105/1000 | Loss: 0.00004264
Iteration 106/1000 | Loss: 0.00004264
Iteration 107/1000 | Loss: 0.00004264
Iteration 108/1000 | Loss: 0.00004264
Iteration 109/1000 | Loss: 0.00004264
Iteration 110/1000 | Loss: 0.00004263
Iteration 111/1000 | Loss: 0.00004263
Iteration 112/1000 | Loss: 0.00004263
Iteration 113/1000 | Loss: 0.00004263
Iteration 114/1000 | Loss: 0.00004263
Iteration 115/1000 | Loss: 0.00004263
Iteration 116/1000 | Loss: 0.00004263
Iteration 117/1000 | Loss: 0.00004263
Iteration 118/1000 | Loss: 0.00004262
Iteration 119/1000 | Loss: 0.00004262
Iteration 120/1000 | Loss: 0.00004262
Iteration 121/1000 | Loss: 0.00004262
Iteration 122/1000 | Loss: 0.00004262
Iteration 123/1000 | Loss: 0.00004261
Iteration 124/1000 | Loss: 0.00004261
Iteration 125/1000 | Loss: 0.00004261
Iteration 126/1000 | Loss: 0.00004261
Iteration 127/1000 | Loss: 0.00004261
Iteration 128/1000 | Loss: 0.00004261
Iteration 129/1000 | Loss: 0.00004261
Iteration 130/1000 | Loss: 0.00004260
Iteration 131/1000 | Loss: 0.00004260
Iteration 132/1000 | Loss: 0.00004260
Iteration 133/1000 | Loss: 0.00004260
Iteration 134/1000 | Loss: 0.00004260
Iteration 135/1000 | Loss: 0.00004260
Iteration 136/1000 | Loss: 0.00004260
Iteration 137/1000 | Loss: 0.00004259
Iteration 138/1000 | Loss: 0.00004259
Iteration 139/1000 | Loss: 0.00004259
Iteration 140/1000 | Loss: 0.00004259
Iteration 141/1000 | Loss: 0.00004259
Iteration 142/1000 | Loss: 0.00004259
Iteration 143/1000 | Loss: 0.00004259
Iteration 144/1000 | Loss: 0.00004259
Iteration 145/1000 | Loss: 0.00004259
Iteration 146/1000 | Loss: 0.00004258
Iteration 147/1000 | Loss: 0.00004258
Iteration 148/1000 | Loss: 0.00004258
Iteration 149/1000 | Loss: 0.00004258
Iteration 150/1000 | Loss: 0.00004258
Iteration 151/1000 | Loss: 0.00004258
Iteration 152/1000 | Loss: 0.00004258
Iteration 153/1000 | Loss: 0.00004258
Iteration 154/1000 | Loss: 0.00004257
Iteration 155/1000 | Loss: 0.00004257
Iteration 156/1000 | Loss: 0.00004257
Iteration 157/1000 | Loss: 0.00004257
Iteration 158/1000 | Loss: 0.00004257
Iteration 159/1000 | Loss: 0.00004257
Iteration 160/1000 | Loss: 0.00004257
Iteration 161/1000 | Loss: 0.00004257
Iteration 162/1000 | Loss: 0.00004257
Iteration 163/1000 | Loss: 0.00004257
Iteration 164/1000 | Loss: 0.00004257
Iteration 165/1000 | Loss: 0.00004257
Iteration 166/1000 | Loss: 0.00004257
Iteration 167/1000 | Loss: 0.00004257
Iteration 168/1000 | Loss: 0.00004257
Iteration 169/1000 | Loss: 0.00004256
Iteration 170/1000 | Loss: 0.00004256
Iteration 171/1000 | Loss: 0.00004256
Iteration 172/1000 | Loss: 0.00004256
Iteration 173/1000 | Loss: 0.00004256
Iteration 174/1000 | Loss: 0.00004256
Iteration 175/1000 | Loss: 0.00004256
Iteration 176/1000 | Loss: 0.00004256
Iteration 177/1000 | Loss: 0.00004256
Iteration 178/1000 | Loss: 0.00004256
Iteration 179/1000 | Loss: 0.00004256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [4.256449028616771e-05, 4.256449028616771e-05, 4.256449028616771e-05, 4.256449028616771e-05, 4.256449028616771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.256449028616771e-05

Optimization complete. Final v2v error: 5.363213062286377 mm

Highest mean error: 5.771090030670166 mm for frame 82

Lowest mean error: 4.143617630004883 mm for frame 5

Saving results

Total time: 41.04058027267456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145572
Iteration 2/25 | Loss: 0.01145572
Iteration 3/25 | Loss: 0.01145572
Iteration 4/25 | Loss: 0.00270818
Iteration 5/25 | Loss: 0.00228957
Iteration 6/25 | Loss: 0.00224024
Iteration 7/25 | Loss: 0.00217290
Iteration 8/25 | Loss: 0.00207782
Iteration 9/25 | Loss: 0.00183891
Iteration 10/25 | Loss: 0.00168475
Iteration 11/25 | Loss: 0.00159594
Iteration 12/25 | Loss: 0.00154909
Iteration 13/25 | Loss: 0.00152711
Iteration 14/25 | Loss: 0.00151095
Iteration 15/25 | Loss: 0.00150129
Iteration 16/25 | Loss: 0.00150410
Iteration 17/25 | Loss: 0.00149522
Iteration 18/25 | Loss: 0.00149161
Iteration 19/25 | Loss: 0.00149595
Iteration 20/25 | Loss: 0.00148832
Iteration 21/25 | Loss: 0.00148541
Iteration 22/25 | Loss: 0.00148448
Iteration 23/25 | Loss: 0.00148423
Iteration 24/25 | Loss: 0.00148399
Iteration 25/25 | Loss: 0.00148382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55416024
Iteration 2/25 | Loss: 0.00438770
Iteration 3/25 | Loss: 0.00438770
Iteration 4/25 | Loss: 0.00438770
Iteration 5/25 | Loss: 0.00438770
Iteration 6/25 | Loss: 0.00438770
Iteration 7/25 | Loss: 0.00438770
Iteration 8/25 | Loss: 0.00438770
Iteration 9/25 | Loss: 0.00438770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0043876999989151955, 0.0043876999989151955, 0.0043876999989151955, 0.0043876999989151955, 0.0043876999989151955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0043876999989151955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00438770
Iteration 2/1000 | Loss: 0.00031627
Iteration 3/1000 | Loss: 0.00020414
Iteration 4/1000 | Loss: 0.00016497
Iteration 5/1000 | Loss: 0.00014472
Iteration 6/1000 | Loss: 0.00012847
Iteration 7/1000 | Loss: 0.00068596
Iteration 8/1000 | Loss: 0.00013053
Iteration 9/1000 | Loss: 0.00011576
Iteration 10/1000 | Loss: 0.00010463
Iteration 11/1000 | Loss: 0.00010061
Iteration 12/1000 | Loss: 0.01411628
Iteration 13/1000 | Loss: 0.00482390
Iteration 14/1000 | Loss: 0.00041321
Iteration 15/1000 | Loss: 0.00009793
Iteration 16/1000 | Loss: 0.00006985
Iteration 17/1000 | Loss: 0.00004988
Iteration 18/1000 | Loss: 0.00004119
Iteration 19/1000 | Loss: 0.00003706
Iteration 20/1000 | Loss: 0.00003397
Iteration 21/1000 | Loss: 0.00003225
Iteration 22/1000 | Loss: 0.00003127
Iteration 23/1000 | Loss: 0.00003075
Iteration 24/1000 | Loss: 0.00003039
Iteration 25/1000 | Loss: 0.00003015
Iteration 26/1000 | Loss: 0.00002990
Iteration 27/1000 | Loss: 0.00002967
Iteration 28/1000 | Loss: 0.00002941
Iteration 29/1000 | Loss: 0.00002955
Iteration 30/1000 | Loss: 0.00002905
Iteration 31/1000 | Loss: 0.00002898
Iteration 32/1000 | Loss: 0.00002890
Iteration 33/1000 | Loss: 0.00002886
Iteration 34/1000 | Loss: 0.00002884
Iteration 35/1000 | Loss: 0.00002884
Iteration 36/1000 | Loss: 0.00002882
Iteration 37/1000 | Loss: 0.00002881
Iteration 38/1000 | Loss: 0.00002877
Iteration 39/1000 | Loss: 0.00002877
Iteration 40/1000 | Loss: 0.00002876
Iteration 41/1000 | Loss: 0.00002876
Iteration 42/1000 | Loss: 0.00002875
Iteration 43/1000 | Loss: 0.00002875
Iteration 44/1000 | Loss: 0.00002875
Iteration 45/1000 | Loss: 0.00002875
Iteration 46/1000 | Loss: 0.00002875
Iteration 47/1000 | Loss: 0.00002875
Iteration 48/1000 | Loss: 0.00002874
Iteration 49/1000 | Loss: 0.00002874
Iteration 50/1000 | Loss: 0.00002874
Iteration 51/1000 | Loss: 0.00002873
Iteration 52/1000 | Loss: 0.00002873
Iteration 53/1000 | Loss: 0.00002873
Iteration 54/1000 | Loss: 0.00002873
Iteration 55/1000 | Loss: 0.00002873
Iteration 56/1000 | Loss: 0.00002873
Iteration 57/1000 | Loss: 0.00002873
Iteration 58/1000 | Loss: 0.00002873
Iteration 59/1000 | Loss: 0.00002873
Iteration 60/1000 | Loss: 0.00002873
Iteration 61/1000 | Loss: 0.00002873
Iteration 62/1000 | Loss: 0.00002873
Iteration 63/1000 | Loss: 0.00002873
Iteration 64/1000 | Loss: 0.00002873
Iteration 65/1000 | Loss: 0.00002873
Iteration 66/1000 | Loss: 0.00002873
Iteration 67/1000 | Loss: 0.00002873
Iteration 68/1000 | Loss: 0.00002873
Iteration 69/1000 | Loss: 0.00002873
Iteration 70/1000 | Loss: 0.00002873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.8729824407491833e-05, 2.8729824407491833e-05, 2.8729824407491833e-05, 2.8729824407491833e-05, 2.8729824407491833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8729824407491833e-05

Optimization complete. Final v2v error: 4.556816101074219 mm

Highest mean error: 5.757215976715088 mm for frame 166

Lowest mean error: 4.23053503036499 mm for frame 229

Saving results

Total time: 100.06798505783081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901599
Iteration 2/25 | Loss: 0.00145729
Iteration 3/25 | Loss: 0.00131594
Iteration 4/25 | Loss: 0.00130416
Iteration 5/25 | Loss: 0.00129893
Iteration 6/25 | Loss: 0.00129803
Iteration 7/25 | Loss: 0.00129803
Iteration 8/25 | Loss: 0.00129803
Iteration 9/25 | Loss: 0.00129803
Iteration 10/25 | Loss: 0.00129803
Iteration 11/25 | Loss: 0.00129803
Iteration 12/25 | Loss: 0.00129803
Iteration 13/25 | Loss: 0.00129803
Iteration 14/25 | Loss: 0.00129803
Iteration 15/25 | Loss: 0.00129803
Iteration 16/25 | Loss: 0.00129803
Iteration 17/25 | Loss: 0.00129803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012980286264792085, 0.0012980286264792085, 0.0012980286264792085, 0.0012980286264792085, 0.0012980286264792085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012980286264792085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60757804
Iteration 2/25 | Loss: 0.00223079
Iteration 3/25 | Loss: 0.00223077
Iteration 4/25 | Loss: 0.00223077
Iteration 5/25 | Loss: 0.00223077
Iteration 6/25 | Loss: 0.00223077
Iteration 7/25 | Loss: 0.00223077
Iteration 8/25 | Loss: 0.00223077
Iteration 9/25 | Loss: 0.00223077
Iteration 10/25 | Loss: 0.00223077
Iteration 11/25 | Loss: 0.00223077
Iteration 12/25 | Loss: 0.00223077
Iteration 13/25 | Loss: 0.00223077
Iteration 14/25 | Loss: 0.00223077
Iteration 15/25 | Loss: 0.00223077
Iteration 16/25 | Loss: 0.00223077
Iteration 17/25 | Loss: 0.00223077
Iteration 18/25 | Loss: 0.00223077
Iteration 19/25 | Loss: 0.00223077
Iteration 20/25 | Loss: 0.00223077
Iteration 21/25 | Loss: 0.00223077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022307694889605045, 0.0022307694889605045, 0.0022307694889605045, 0.0022307694889605045, 0.0022307694889605045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022307694889605045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223077
Iteration 2/1000 | Loss: 0.00005536
Iteration 3/1000 | Loss: 0.00003771
Iteration 4/1000 | Loss: 0.00003307
Iteration 5/1000 | Loss: 0.00003042
Iteration 6/1000 | Loss: 0.00002901
Iteration 7/1000 | Loss: 0.00002828
Iteration 8/1000 | Loss: 0.00002769
Iteration 9/1000 | Loss: 0.00002733
Iteration 10/1000 | Loss: 0.00002703
Iteration 11/1000 | Loss: 0.00002669
Iteration 12/1000 | Loss: 0.00002651
Iteration 13/1000 | Loss: 0.00002648
Iteration 14/1000 | Loss: 0.00002642
Iteration 15/1000 | Loss: 0.00002638
Iteration 16/1000 | Loss: 0.00002637
Iteration 17/1000 | Loss: 0.00002636
Iteration 18/1000 | Loss: 0.00002635
Iteration 19/1000 | Loss: 0.00002635
Iteration 20/1000 | Loss: 0.00002631
Iteration 21/1000 | Loss: 0.00002631
Iteration 22/1000 | Loss: 0.00002630
Iteration 23/1000 | Loss: 0.00002629
Iteration 24/1000 | Loss: 0.00002629
Iteration 25/1000 | Loss: 0.00002629
Iteration 26/1000 | Loss: 0.00002629
Iteration 27/1000 | Loss: 0.00002627
Iteration 28/1000 | Loss: 0.00002627
Iteration 29/1000 | Loss: 0.00002627
Iteration 30/1000 | Loss: 0.00002627
Iteration 31/1000 | Loss: 0.00002627
Iteration 32/1000 | Loss: 0.00002627
Iteration 33/1000 | Loss: 0.00002627
Iteration 34/1000 | Loss: 0.00002627
Iteration 35/1000 | Loss: 0.00002627
Iteration 36/1000 | Loss: 0.00002627
Iteration 37/1000 | Loss: 0.00002626
Iteration 38/1000 | Loss: 0.00002626
Iteration 39/1000 | Loss: 0.00002626
Iteration 40/1000 | Loss: 0.00002626
Iteration 41/1000 | Loss: 0.00002626
Iteration 42/1000 | Loss: 0.00002625
Iteration 43/1000 | Loss: 0.00002625
Iteration 44/1000 | Loss: 0.00002625
Iteration 45/1000 | Loss: 0.00002624
Iteration 46/1000 | Loss: 0.00002624
Iteration 47/1000 | Loss: 0.00002624
Iteration 48/1000 | Loss: 0.00002624
Iteration 49/1000 | Loss: 0.00002624
Iteration 50/1000 | Loss: 0.00002624
Iteration 51/1000 | Loss: 0.00002624
Iteration 52/1000 | Loss: 0.00002624
Iteration 53/1000 | Loss: 0.00002624
Iteration 54/1000 | Loss: 0.00002624
Iteration 55/1000 | Loss: 0.00002624
Iteration 56/1000 | Loss: 0.00002624
Iteration 57/1000 | Loss: 0.00002624
Iteration 58/1000 | Loss: 0.00002623
Iteration 59/1000 | Loss: 0.00002623
Iteration 60/1000 | Loss: 0.00002623
Iteration 61/1000 | Loss: 0.00002622
Iteration 62/1000 | Loss: 0.00002622
Iteration 63/1000 | Loss: 0.00002622
Iteration 64/1000 | Loss: 0.00002622
Iteration 65/1000 | Loss: 0.00002622
Iteration 66/1000 | Loss: 0.00002622
Iteration 67/1000 | Loss: 0.00002622
Iteration 68/1000 | Loss: 0.00002622
Iteration 69/1000 | Loss: 0.00002622
Iteration 70/1000 | Loss: 0.00002622
Iteration 71/1000 | Loss: 0.00002622
Iteration 72/1000 | Loss: 0.00002621
Iteration 73/1000 | Loss: 0.00002621
Iteration 74/1000 | Loss: 0.00002621
Iteration 75/1000 | Loss: 0.00002621
Iteration 76/1000 | Loss: 0.00002621
Iteration 77/1000 | Loss: 0.00002621
Iteration 78/1000 | Loss: 0.00002621
Iteration 79/1000 | Loss: 0.00002621
Iteration 80/1000 | Loss: 0.00002621
Iteration 81/1000 | Loss: 0.00002621
Iteration 82/1000 | Loss: 0.00002621
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002621
Iteration 86/1000 | Loss: 0.00002620
Iteration 87/1000 | Loss: 0.00002620
Iteration 88/1000 | Loss: 0.00002620
Iteration 89/1000 | Loss: 0.00002620
Iteration 90/1000 | Loss: 0.00002620
Iteration 91/1000 | Loss: 0.00002620
Iteration 92/1000 | Loss: 0.00002619
Iteration 93/1000 | Loss: 0.00002619
Iteration 94/1000 | Loss: 0.00002619
Iteration 95/1000 | Loss: 0.00002619
Iteration 96/1000 | Loss: 0.00002619
Iteration 97/1000 | Loss: 0.00002619
Iteration 98/1000 | Loss: 0.00002619
Iteration 99/1000 | Loss: 0.00002619
Iteration 100/1000 | Loss: 0.00002619
Iteration 101/1000 | Loss: 0.00002619
Iteration 102/1000 | Loss: 0.00002618
Iteration 103/1000 | Loss: 0.00002618
Iteration 104/1000 | Loss: 0.00002618
Iteration 105/1000 | Loss: 0.00002618
Iteration 106/1000 | Loss: 0.00002618
Iteration 107/1000 | Loss: 0.00002618
Iteration 108/1000 | Loss: 0.00002618
Iteration 109/1000 | Loss: 0.00002618
Iteration 110/1000 | Loss: 0.00002617
Iteration 111/1000 | Loss: 0.00002617
Iteration 112/1000 | Loss: 0.00002617
Iteration 113/1000 | Loss: 0.00002617
Iteration 114/1000 | Loss: 0.00002617
Iteration 115/1000 | Loss: 0.00002617
Iteration 116/1000 | Loss: 0.00002617
Iteration 117/1000 | Loss: 0.00002617
Iteration 118/1000 | Loss: 0.00002617
Iteration 119/1000 | Loss: 0.00002617
Iteration 120/1000 | Loss: 0.00002617
Iteration 121/1000 | Loss: 0.00002617
Iteration 122/1000 | Loss: 0.00002617
Iteration 123/1000 | Loss: 0.00002617
Iteration 124/1000 | Loss: 0.00002617
Iteration 125/1000 | Loss: 0.00002617
Iteration 126/1000 | Loss: 0.00002617
Iteration 127/1000 | Loss: 0.00002617
Iteration 128/1000 | Loss: 0.00002617
Iteration 129/1000 | Loss: 0.00002616
Iteration 130/1000 | Loss: 0.00002616
Iteration 131/1000 | Loss: 0.00002616
Iteration 132/1000 | Loss: 0.00002616
Iteration 133/1000 | Loss: 0.00002616
Iteration 134/1000 | Loss: 0.00002616
Iteration 135/1000 | Loss: 0.00002616
Iteration 136/1000 | Loss: 0.00002616
Iteration 137/1000 | Loss: 0.00002616
Iteration 138/1000 | Loss: 0.00002616
Iteration 139/1000 | Loss: 0.00002616
Iteration 140/1000 | Loss: 0.00002616
Iteration 141/1000 | Loss: 0.00002616
Iteration 142/1000 | Loss: 0.00002616
Iteration 143/1000 | Loss: 0.00002616
Iteration 144/1000 | Loss: 0.00002615
Iteration 145/1000 | Loss: 0.00002615
Iteration 146/1000 | Loss: 0.00002615
Iteration 147/1000 | Loss: 0.00002615
Iteration 148/1000 | Loss: 0.00002615
Iteration 149/1000 | Loss: 0.00002615
Iteration 150/1000 | Loss: 0.00002615
Iteration 151/1000 | Loss: 0.00002615
Iteration 152/1000 | Loss: 0.00002615
Iteration 153/1000 | Loss: 0.00002614
Iteration 154/1000 | Loss: 0.00002614
Iteration 155/1000 | Loss: 0.00002614
Iteration 156/1000 | Loss: 0.00002614
Iteration 157/1000 | Loss: 0.00002614
Iteration 158/1000 | Loss: 0.00002614
Iteration 159/1000 | Loss: 0.00002614
Iteration 160/1000 | Loss: 0.00002614
Iteration 161/1000 | Loss: 0.00002614
Iteration 162/1000 | Loss: 0.00002614
Iteration 163/1000 | Loss: 0.00002614
Iteration 164/1000 | Loss: 0.00002614
Iteration 165/1000 | Loss: 0.00002614
Iteration 166/1000 | Loss: 0.00002614
Iteration 167/1000 | Loss: 0.00002614
Iteration 168/1000 | Loss: 0.00002614
Iteration 169/1000 | Loss: 0.00002614
Iteration 170/1000 | Loss: 0.00002614
Iteration 171/1000 | Loss: 0.00002614
Iteration 172/1000 | Loss: 0.00002614
Iteration 173/1000 | Loss: 0.00002614
Iteration 174/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.614079312479589e-05, 2.614079312479589e-05, 2.614079312479589e-05, 2.614079312479589e-05, 2.614079312479589e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.614079312479589e-05

Optimization complete. Final v2v error: 4.3924736976623535 mm

Highest mean error: 4.8562235832214355 mm for frame 75

Lowest mean error: 4.146756172180176 mm for frame 134

Saving results

Total time: 37.97256398200989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421696
Iteration 2/25 | Loss: 0.00133835
Iteration 3/25 | Loss: 0.00125182
Iteration 4/25 | Loss: 0.00123832
Iteration 5/25 | Loss: 0.00123269
Iteration 6/25 | Loss: 0.00123129
Iteration 7/25 | Loss: 0.00123100
Iteration 8/25 | Loss: 0.00123100
Iteration 9/25 | Loss: 0.00123100
Iteration 10/25 | Loss: 0.00123100
Iteration 11/25 | Loss: 0.00123100
Iteration 12/25 | Loss: 0.00123100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012309992453083396, 0.0012309992453083396, 0.0012309992453083396, 0.0012309992453083396, 0.0012309992453083396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012309992453083396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08386970
Iteration 2/25 | Loss: 0.00193898
Iteration 3/25 | Loss: 0.00193898
Iteration 4/25 | Loss: 0.00193898
Iteration 5/25 | Loss: 0.00193898
Iteration 6/25 | Loss: 0.00193898
Iteration 7/25 | Loss: 0.00193898
Iteration 8/25 | Loss: 0.00193898
Iteration 9/25 | Loss: 0.00193898
Iteration 10/25 | Loss: 0.00193898
Iteration 11/25 | Loss: 0.00193898
Iteration 12/25 | Loss: 0.00193898
Iteration 13/25 | Loss: 0.00193898
Iteration 14/25 | Loss: 0.00193898
Iteration 15/25 | Loss: 0.00193898
Iteration 16/25 | Loss: 0.00193898
Iteration 17/25 | Loss: 0.00193898
Iteration 18/25 | Loss: 0.00193898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019389752997085452, 0.0019389752997085452, 0.0019389752997085452, 0.0019389752997085452, 0.0019389752997085452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019389752997085452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193898
Iteration 2/1000 | Loss: 0.00003388
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002556
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002363
Iteration 7/1000 | Loss: 0.00002315
Iteration 8/1000 | Loss: 0.00002275
Iteration 9/1000 | Loss: 0.00002245
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00002225
Iteration 12/1000 | Loss: 0.00002220
Iteration 13/1000 | Loss: 0.00002220
Iteration 14/1000 | Loss: 0.00002218
Iteration 15/1000 | Loss: 0.00002216
Iteration 16/1000 | Loss: 0.00002215
Iteration 17/1000 | Loss: 0.00002214
Iteration 18/1000 | Loss: 0.00002214
Iteration 19/1000 | Loss: 0.00002213
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002212
Iteration 22/1000 | Loss: 0.00002211
Iteration 23/1000 | Loss: 0.00002210
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002209
Iteration 26/1000 | Loss: 0.00002208
Iteration 27/1000 | Loss: 0.00002208
Iteration 28/1000 | Loss: 0.00002208
Iteration 29/1000 | Loss: 0.00002207
Iteration 30/1000 | Loss: 0.00002207
Iteration 31/1000 | Loss: 0.00002207
Iteration 32/1000 | Loss: 0.00002206
Iteration 33/1000 | Loss: 0.00002205
Iteration 34/1000 | Loss: 0.00002205
Iteration 35/1000 | Loss: 0.00002205
Iteration 36/1000 | Loss: 0.00002204
Iteration 37/1000 | Loss: 0.00002204
Iteration 38/1000 | Loss: 0.00002204
Iteration 39/1000 | Loss: 0.00002204
Iteration 40/1000 | Loss: 0.00002203
Iteration 41/1000 | Loss: 0.00002203
Iteration 42/1000 | Loss: 0.00002203
Iteration 43/1000 | Loss: 0.00002202
Iteration 44/1000 | Loss: 0.00002201
Iteration 45/1000 | Loss: 0.00002201
Iteration 46/1000 | Loss: 0.00002201
Iteration 47/1000 | Loss: 0.00002201
Iteration 48/1000 | Loss: 0.00002201
Iteration 49/1000 | Loss: 0.00002200
Iteration 50/1000 | Loss: 0.00002200
Iteration 51/1000 | Loss: 0.00002200
Iteration 52/1000 | Loss: 0.00002199
Iteration 53/1000 | Loss: 0.00002199
Iteration 54/1000 | Loss: 0.00002199
Iteration 55/1000 | Loss: 0.00002199
Iteration 56/1000 | Loss: 0.00002199
Iteration 57/1000 | Loss: 0.00002198
Iteration 58/1000 | Loss: 0.00002198
Iteration 59/1000 | Loss: 0.00002198
Iteration 60/1000 | Loss: 0.00002198
Iteration 61/1000 | Loss: 0.00002197
Iteration 62/1000 | Loss: 0.00002197
Iteration 63/1000 | Loss: 0.00002197
Iteration 64/1000 | Loss: 0.00002197
Iteration 65/1000 | Loss: 0.00002197
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00002197
Iteration 68/1000 | Loss: 0.00002197
Iteration 69/1000 | Loss: 0.00002197
Iteration 70/1000 | Loss: 0.00002197
Iteration 71/1000 | Loss: 0.00002197
Iteration 72/1000 | Loss: 0.00002197
Iteration 73/1000 | Loss: 0.00002197
Iteration 74/1000 | Loss: 0.00002197
Iteration 75/1000 | Loss: 0.00002196
Iteration 76/1000 | Loss: 0.00002196
Iteration 77/1000 | Loss: 0.00002196
Iteration 78/1000 | Loss: 0.00002196
Iteration 79/1000 | Loss: 0.00002196
Iteration 80/1000 | Loss: 0.00002196
Iteration 81/1000 | Loss: 0.00002196
Iteration 82/1000 | Loss: 0.00002196
Iteration 83/1000 | Loss: 0.00002196
Iteration 84/1000 | Loss: 0.00002196
Iteration 85/1000 | Loss: 0.00002196
Iteration 86/1000 | Loss: 0.00002196
Iteration 87/1000 | Loss: 0.00002196
Iteration 88/1000 | Loss: 0.00002196
Iteration 89/1000 | Loss: 0.00002196
Iteration 90/1000 | Loss: 0.00002196
Iteration 91/1000 | Loss: 0.00002196
Iteration 92/1000 | Loss: 0.00002196
Iteration 93/1000 | Loss: 0.00002196
Iteration 94/1000 | Loss: 0.00002196
Iteration 95/1000 | Loss: 0.00002196
Iteration 96/1000 | Loss: 0.00002196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.1956158889224753e-05, 2.1956158889224753e-05, 2.1956158889224753e-05, 2.1956158889224753e-05, 2.1956158889224753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1956158889224753e-05

Optimization complete. Final v2v error: 4.0233049392700195 mm

Highest mean error: 4.506642818450928 mm for frame 97

Lowest mean error: 3.628920793533325 mm for frame 4

Saving results

Total time: 30.22994589805603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102847
Iteration 2/25 | Loss: 0.00381281
Iteration 3/25 | Loss: 0.00260367
Iteration 4/25 | Loss: 0.00248749
Iteration 5/25 | Loss: 0.00229547
Iteration 6/25 | Loss: 0.00228517
Iteration 7/25 | Loss: 0.00215078
Iteration 8/25 | Loss: 0.00212884
Iteration 9/25 | Loss: 0.00203662
Iteration 10/25 | Loss: 0.00204237
Iteration 11/25 | Loss: 0.00200581
Iteration 12/25 | Loss: 0.00199279
Iteration 13/25 | Loss: 0.00198983
Iteration 14/25 | Loss: 0.00200071
Iteration 15/25 | Loss: 0.00198761
Iteration 16/25 | Loss: 0.00198151
Iteration 17/25 | Loss: 0.00198006
Iteration 18/25 | Loss: 0.00197925
Iteration 19/25 | Loss: 0.00197884
Iteration 20/25 | Loss: 0.00197868
Iteration 21/25 | Loss: 0.00197863
Iteration 22/25 | Loss: 0.00197863
Iteration 23/25 | Loss: 0.00197862
Iteration 24/25 | Loss: 0.00197862
Iteration 25/25 | Loss: 0.00197862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57478380
Iteration 2/25 | Loss: 0.00704400
Iteration 3/25 | Loss: 0.00704400
Iteration 4/25 | Loss: 0.00704400
Iteration 5/25 | Loss: 0.00704400
Iteration 6/25 | Loss: 0.00704400
Iteration 7/25 | Loss: 0.00704400
Iteration 8/25 | Loss: 0.00704400
Iteration 9/25 | Loss: 0.00704400
Iteration 10/25 | Loss: 0.00704400
Iteration 11/25 | Loss: 0.00704400
Iteration 12/25 | Loss: 0.00704400
Iteration 13/25 | Loss: 0.00704400
Iteration 14/25 | Loss: 0.00704400
Iteration 15/25 | Loss: 0.00704400
Iteration 16/25 | Loss: 0.00704400
Iteration 17/25 | Loss: 0.00704400
Iteration 18/25 | Loss: 0.00704400
Iteration 19/25 | Loss: 0.00704400
Iteration 20/25 | Loss: 0.00704400
Iteration 21/25 | Loss: 0.00704400
Iteration 22/25 | Loss: 0.00704400
Iteration 23/25 | Loss: 0.00704400
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.007043997757136822, 0.007043997757136822, 0.007043997757136822, 0.007043997757136822, 0.007043997757136822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007043997757136822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00704400
Iteration 2/1000 | Loss: 0.00128380
Iteration 3/1000 | Loss: 0.00184437
Iteration 4/1000 | Loss: 0.00124509
Iteration 5/1000 | Loss: 0.00103244
Iteration 6/1000 | Loss: 0.00108735
Iteration 7/1000 | Loss: 0.00093543
Iteration 8/1000 | Loss: 0.00076871
Iteration 9/1000 | Loss: 0.00109634
Iteration 10/1000 | Loss: 0.00076718
Iteration 11/1000 | Loss: 0.00178616
Iteration 12/1000 | Loss: 0.00227081
Iteration 13/1000 | Loss: 0.00381656
Iteration 14/1000 | Loss: 0.01465082
Iteration 15/1000 | Loss: 0.00731594
Iteration 16/1000 | Loss: 0.00119564
Iteration 17/1000 | Loss: 0.00088620
Iteration 18/1000 | Loss: 0.00054951
Iteration 19/1000 | Loss: 0.00045628
Iteration 20/1000 | Loss: 0.00056141
Iteration 21/1000 | Loss: 0.00051649
Iteration 22/1000 | Loss: 0.00032026
Iteration 23/1000 | Loss: 0.00046642
Iteration 24/1000 | Loss: 0.00016227
Iteration 25/1000 | Loss: 0.00020675
Iteration 26/1000 | Loss: 0.00022603
Iteration 27/1000 | Loss: 0.00022738
Iteration 28/1000 | Loss: 0.00007872
Iteration 29/1000 | Loss: 0.00007812
Iteration 30/1000 | Loss: 0.00005693
Iteration 31/1000 | Loss: 0.00005789
Iteration 32/1000 | Loss: 0.00005263
Iteration 33/1000 | Loss: 0.00006716
Iteration 34/1000 | Loss: 0.00004597
Iteration 35/1000 | Loss: 0.00005024
Iteration 36/1000 | Loss: 0.00003789
Iteration 37/1000 | Loss: 0.00004930
Iteration 38/1000 | Loss: 0.00004991
Iteration 39/1000 | Loss: 0.00004762
Iteration 40/1000 | Loss: 0.00004590
Iteration 41/1000 | Loss: 0.00005430
Iteration 42/1000 | Loss: 0.00020289
Iteration 43/1000 | Loss: 0.00004630
Iteration 44/1000 | Loss: 0.00003552
Iteration 45/1000 | Loss: 0.00003202
Iteration 46/1000 | Loss: 0.00003087
Iteration 47/1000 | Loss: 0.00003004
Iteration 48/1000 | Loss: 0.00002957
Iteration 49/1000 | Loss: 0.00002922
Iteration 50/1000 | Loss: 0.00002894
Iteration 51/1000 | Loss: 0.00002877
Iteration 52/1000 | Loss: 0.00002862
Iteration 53/1000 | Loss: 0.00002858
Iteration 54/1000 | Loss: 0.00002852
Iteration 55/1000 | Loss: 0.00002851
Iteration 56/1000 | Loss: 0.00002851
Iteration 57/1000 | Loss: 0.00002851
Iteration 58/1000 | Loss: 0.00002851
Iteration 59/1000 | Loss: 0.00002851
Iteration 60/1000 | Loss: 0.00002849
Iteration 61/1000 | Loss: 0.00002848
Iteration 62/1000 | Loss: 0.00002848
Iteration 63/1000 | Loss: 0.00002847
Iteration 64/1000 | Loss: 0.00002847
Iteration 65/1000 | Loss: 0.00002847
Iteration 66/1000 | Loss: 0.00002847
Iteration 67/1000 | Loss: 0.00002846
Iteration 68/1000 | Loss: 0.00002846
Iteration 69/1000 | Loss: 0.00002845
Iteration 70/1000 | Loss: 0.00002845
Iteration 71/1000 | Loss: 0.00002845
Iteration 72/1000 | Loss: 0.00002845
Iteration 73/1000 | Loss: 0.00002845
Iteration 74/1000 | Loss: 0.00002845
Iteration 75/1000 | Loss: 0.00002845
Iteration 76/1000 | Loss: 0.00002845
Iteration 77/1000 | Loss: 0.00002844
Iteration 78/1000 | Loss: 0.00002844
Iteration 79/1000 | Loss: 0.00002844
Iteration 80/1000 | Loss: 0.00002844
Iteration 81/1000 | Loss: 0.00002844
Iteration 82/1000 | Loss: 0.00002844
Iteration 83/1000 | Loss: 0.00002844
Iteration 84/1000 | Loss: 0.00002844
Iteration 85/1000 | Loss: 0.00002844
Iteration 86/1000 | Loss: 0.00002843
Iteration 87/1000 | Loss: 0.00002843
Iteration 88/1000 | Loss: 0.00002843
Iteration 89/1000 | Loss: 0.00002843
Iteration 90/1000 | Loss: 0.00002843
Iteration 91/1000 | Loss: 0.00002843
Iteration 92/1000 | Loss: 0.00002843
Iteration 93/1000 | Loss: 0.00002843
Iteration 94/1000 | Loss: 0.00002843
Iteration 95/1000 | Loss: 0.00002843
Iteration 96/1000 | Loss: 0.00002843
Iteration 97/1000 | Loss: 0.00002843
Iteration 98/1000 | Loss: 0.00002843
Iteration 99/1000 | Loss: 0.00002843
Iteration 100/1000 | Loss: 0.00002843
Iteration 101/1000 | Loss: 0.00002843
Iteration 102/1000 | Loss: 0.00002843
Iteration 103/1000 | Loss: 0.00002843
Iteration 104/1000 | Loss: 0.00002843
Iteration 105/1000 | Loss: 0.00002843
Iteration 106/1000 | Loss: 0.00002843
Iteration 107/1000 | Loss: 0.00002843
Iteration 108/1000 | Loss: 0.00002843
Iteration 109/1000 | Loss: 0.00002843
Iteration 110/1000 | Loss: 0.00002843
Iteration 111/1000 | Loss: 0.00002843
Iteration 112/1000 | Loss: 0.00002843
Iteration 113/1000 | Loss: 0.00002843
Iteration 114/1000 | Loss: 0.00002843
Iteration 115/1000 | Loss: 0.00002843
Iteration 116/1000 | Loss: 0.00002843
Iteration 117/1000 | Loss: 0.00002843
Iteration 118/1000 | Loss: 0.00002843
Iteration 119/1000 | Loss: 0.00002843
Iteration 120/1000 | Loss: 0.00002843
Iteration 121/1000 | Loss: 0.00002843
Iteration 122/1000 | Loss: 0.00002843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.8427171855582856e-05, 2.8427171855582856e-05, 2.8427171855582856e-05, 2.8427171855582856e-05, 2.8427171855582856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8427171855582856e-05

Optimization complete. Final v2v error: 4.447174072265625 mm

Highest mean error: 10.874652862548828 mm for frame 43

Lowest mean error: 4.134137153625488 mm for frame 1

Saving results

Total time: 130.47380423545837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01180180
Iteration 2/25 | Loss: 0.00224596
Iteration 3/25 | Loss: 0.00145586
Iteration 4/25 | Loss: 0.00135162
Iteration 5/25 | Loss: 0.00132377
Iteration 6/25 | Loss: 0.00131322
Iteration 7/25 | Loss: 0.00127716
Iteration 8/25 | Loss: 0.00125167
Iteration 9/25 | Loss: 0.00122763
Iteration 10/25 | Loss: 0.00121962
Iteration 11/25 | Loss: 0.00121418
Iteration 12/25 | Loss: 0.00121671
Iteration 13/25 | Loss: 0.00120921
Iteration 14/25 | Loss: 0.00120947
Iteration 15/25 | Loss: 0.00120803
Iteration 16/25 | Loss: 0.00120942
Iteration 17/25 | Loss: 0.00120902
Iteration 18/25 | Loss: 0.00120684
Iteration 19/25 | Loss: 0.00120681
Iteration 20/25 | Loss: 0.00120678
Iteration 21/25 | Loss: 0.00120678
Iteration 22/25 | Loss: 0.00120678
Iteration 23/25 | Loss: 0.00120677
Iteration 24/25 | Loss: 0.00120677
Iteration 25/25 | Loss: 0.00120677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65558028
Iteration 2/25 | Loss: 0.00252104
Iteration 3/25 | Loss: 0.00238095
Iteration 4/25 | Loss: 0.00238095
Iteration 5/25 | Loss: 0.00238095
Iteration 6/25 | Loss: 0.00238095
Iteration 7/25 | Loss: 0.00238095
Iteration 8/25 | Loss: 0.00238095
Iteration 9/25 | Loss: 0.00238095
Iteration 10/25 | Loss: 0.00238095
Iteration 11/25 | Loss: 0.00238095
Iteration 12/25 | Loss: 0.00238095
Iteration 13/25 | Loss: 0.00238095
Iteration 14/25 | Loss: 0.00238095
Iteration 15/25 | Loss: 0.00238095
Iteration 16/25 | Loss: 0.00238095
Iteration 17/25 | Loss: 0.00238095
Iteration 18/25 | Loss: 0.00238095
Iteration 19/25 | Loss: 0.00238095
Iteration 20/25 | Loss: 0.00238095
Iteration 21/25 | Loss: 0.00238095
Iteration 22/25 | Loss: 0.00238095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002380950842052698, 0.002380950842052698, 0.002380950842052698, 0.002380950842052698, 0.002380950842052698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002380950842052698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238095
Iteration 2/1000 | Loss: 0.00015498
Iteration 3/1000 | Loss: 0.00012993
Iteration 4/1000 | Loss: 0.00004398
Iteration 5/1000 | Loss: 0.00002671
Iteration 6/1000 | Loss: 0.00002515
Iteration 7/1000 | Loss: 0.00192062
Iteration 8/1000 | Loss: 0.00004211
Iteration 9/1000 | Loss: 0.00004717
Iteration 10/1000 | Loss: 0.00003556
Iteration 11/1000 | Loss: 0.00002104
Iteration 12/1000 | Loss: 0.00005122
Iteration 13/1000 | Loss: 0.00002364
Iteration 14/1000 | Loss: 0.00001847
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001718
Iteration 28/1000 | Loss: 0.00001718
Iteration 29/1000 | Loss: 0.00003876
Iteration 30/1000 | Loss: 0.00002310
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00003028
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001704
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00003643
Iteration 49/1000 | Loss: 0.00002252
Iteration 50/1000 | Loss: 0.00001700
Iteration 51/1000 | Loss: 0.00001700
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00002418
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001699
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001698
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001695
Iteration 82/1000 | Loss: 0.00001695
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001694
Iteration 101/1000 | Loss: 0.00001694
Iteration 102/1000 | Loss: 0.00001694
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001694
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.6942722140811384e-05, 1.6942722140811384e-05, 1.6942722140811384e-05, 1.6942722140811384e-05, 1.6942722140811384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6942722140811384e-05

Optimization complete. Final v2v error: 3.5259904861450195 mm

Highest mean error: 11.015040397644043 mm for frame 125

Lowest mean error: 3.373687267303467 mm for frame 104

Saving results

Total time: 75.74848985671997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811605
Iteration 2/25 | Loss: 0.00174279
Iteration 3/25 | Loss: 0.00148351
Iteration 4/25 | Loss: 0.00144399
Iteration 5/25 | Loss: 0.00143005
Iteration 6/25 | Loss: 0.00142694
Iteration 7/25 | Loss: 0.00142815
Iteration 8/25 | Loss: 0.00142458
Iteration 9/25 | Loss: 0.00142443
Iteration 10/25 | Loss: 0.00142257
Iteration 11/25 | Loss: 0.00142264
Iteration 12/25 | Loss: 0.00142287
Iteration 13/25 | Loss: 0.00142297
Iteration 14/25 | Loss: 0.00142204
Iteration 15/25 | Loss: 0.00142270
Iteration 16/25 | Loss: 0.00142139
Iteration 17/25 | Loss: 0.00142392
Iteration 18/25 | Loss: 0.00142340
Iteration 19/25 | Loss: 0.00142298
Iteration 20/25 | Loss: 0.00142166
Iteration 21/25 | Loss: 0.00142260
Iteration 22/25 | Loss: 0.00142322
Iteration 23/25 | Loss: 0.00142197
Iteration 24/25 | Loss: 0.00142291
Iteration 25/25 | Loss: 0.00142266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47958529
Iteration 2/25 | Loss: 0.00277929
Iteration 3/25 | Loss: 0.00277925
Iteration 4/25 | Loss: 0.00277925
Iteration 5/25 | Loss: 0.00277925
Iteration 6/25 | Loss: 0.00277925
Iteration 7/25 | Loss: 0.00277925
Iteration 8/25 | Loss: 0.00277925
Iteration 9/25 | Loss: 0.00277925
Iteration 10/25 | Loss: 0.00277925
Iteration 11/25 | Loss: 0.00277925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002779252827167511, 0.002779252827167511, 0.002779252827167511, 0.002779252827167511, 0.002779252827167511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002779252827167511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277925
Iteration 2/1000 | Loss: 0.00030473
Iteration 3/1000 | Loss: 0.00023533
Iteration 4/1000 | Loss: 0.00024914
Iteration 5/1000 | Loss: 0.00015319
Iteration 6/1000 | Loss: 0.00023470
Iteration 7/1000 | Loss: 0.00014156
Iteration 8/1000 | Loss: 0.00027784
Iteration 9/1000 | Loss: 0.00023874
Iteration 10/1000 | Loss: 0.00025406
Iteration 11/1000 | Loss: 0.00028550
Iteration 12/1000 | Loss: 0.00027555
Iteration 13/1000 | Loss: 0.00023628
Iteration 14/1000 | Loss: 0.00029014
Iteration 15/1000 | Loss: 0.00008678
Iteration 16/1000 | Loss: 0.00103229
Iteration 17/1000 | Loss: 0.00032811
Iteration 18/1000 | Loss: 0.00028440
Iteration 19/1000 | Loss: 0.00020551
Iteration 20/1000 | Loss: 0.00017445
Iteration 21/1000 | Loss: 0.00009021
Iteration 22/1000 | Loss: 0.00014745
Iteration 23/1000 | Loss: 0.00017488
Iteration 24/1000 | Loss: 0.00182754
Iteration 25/1000 | Loss: 0.00192572
Iteration 26/1000 | Loss: 0.00055496
Iteration 27/1000 | Loss: 0.00080235
Iteration 28/1000 | Loss: 0.00026970
Iteration 29/1000 | Loss: 0.00015834
Iteration 30/1000 | Loss: 0.00008254
Iteration 31/1000 | Loss: 0.00045144
Iteration 32/1000 | Loss: 0.00029136
Iteration 33/1000 | Loss: 0.00019117
Iteration 34/1000 | Loss: 0.00019631
Iteration 35/1000 | Loss: 0.00013503
Iteration 36/1000 | Loss: 0.00021811
Iteration 37/1000 | Loss: 0.00069348
Iteration 38/1000 | Loss: 0.00063547
Iteration 39/1000 | Loss: 0.00023860
Iteration 40/1000 | Loss: 0.00005718
Iteration 41/1000 | Loss: 0.00014263
Iteration 42/1000 | Loss: 0.00011226
Iteration 43/1000 | Loss: 0.00012002
Iteration 44/1000 | Loss: 0.00016555
Iteration 45/1000 | Loss: 0.00012015
Iteration 46/1000 | Loss: 0.00006992
Iteration 47/1000 | Loss: 0.00007179
Iteration 48/1000 | Loss: 0.00013058
Iteration 49/1000 | Loss: 0.00014060
Iteration 50/1000 | Loss: 0.00008802
Iteration 51/1000 | Loss: 0.00018069
Iteration 52/1000 | Loss: 0.00006964
Iteration 53/1000 | Loss: 0.00010161
Iteration 54/1000 | Loss: 0.00011652
Iteration 55/1000 | Loss: 0.00016697
Iteration 56/1000 | Loss: 0.00013301
Iteration 57/1000 | Loss: 0.00014486
Iteration 58/1000 | Loss: 0.00019711
Iteration 59/1000 | Loss: 0.00019185
Iteration 60/1000 | Loss: 0.00016692
Iteration 61/1000 | Loss: 0.00017508
Iteration 62/1000 | Loss: 0.00014347
Iteration 63/1000 | Loss: 0.00012060
Iteration 64/1000 | Loss: 0.00006072
Iteration 65/1000 | Loss: 0.00011187
Iteration 66/1000 | Loss: 0.00012510
Iteration 67/1000 | Loss: 0.00011013
Iteration 68/1000 | Loss: 0.00008172
Iteration 69/1000 | Loss: 0.00008402
Iteration 70/1000 | Loss: 0.00014988
Iteration 71/1000 | Loss: 0.00008950
Iteration 72/1000 | Loss: 0.00013568
Iteration 73/1000 | Loss: 0.00016113
Iteration 74/1000 | Loss: 0.00005083
Iteration 75/1000 | Loss: 0.00005280
Iteration 76/1000 | Loss: 0.00014804
Iteration 77/1000 | Loss: 0.00013269
Iteration 78/1000 | Loss: 0.00009613
Iteration 79/1000 | Loss: 0.00017269
Iteration 80/1000 | Loss: 0.00013238
Iteration 81/1000 | Loss: 0.00016318
Iteration 82/1000 | Loss: 0.00012692
Iteration 83/1000 | Loss: 0.00013538
Iteration 84/1000 | Loss: 0.00013970
Iteration 85/1000 | Loss: 0.00015527
Iteration 86/1000 | Loss: 0.00014179
Iteration 87/1000 | Loss: 0.00014288
Iteration 88/1000 | Loss: 0.00013511
Iteration 89/1000 | Loss: 0.00014055
Iteration 90/1000 | Loss: 0.00012549
Iteration 91/1000 | Loss: 0.00013915
Iteration 92/1000 | Loss: 0.00014900
Iteration 93/1000 | Loss: 0.00012906
Iteration 94/1000 | Loss: 0.00012623
Iteration 95/1000 | Loss: 0.00012477
Iteration 96/1000 | Loss: 0.00024806
Iteration 97/1000 | Loss: 0.00014431
Iteration 98/1000 | Loss: 0.00021162
Iteration 99/1000 | Loss: 0.00022526
Iteration 100/1000 | Loss: 0.00017861
Iteration 101/1000 | Loss: 0.00012138
Iteration 102/1000 | Loss: 0.00011266
Iteration 103/1000 | Loss: 0.00009261
Iteration 104/1000 | Loss: 0.00005856
Iteration 105/1000 | Loss: 0.00006025
Iteration 106/1000 | Loss: 0.00004747
Iteration 107/1000 | Loss: 0.00006598
Iteration 108/1000 | Loss: 0.00003841
Iteration 109/1000 | Loss: 0.00003809
Iteration 110/1000 | Loss: 0.00012234
Iteration 111/1000 | Loss: 0.00003841
Iteration 112/1000 | Loss: 0.00003749
Iteration 113/1000 | Loss: 0.00003713
Iteration 114/1000 | Loss: 0.00003689
Iteration 115/1000 | Loss: 0.00003679
Iteration 116/1000 | Loss: 0.00003650
Iteration 117/1000 | Loss: 0.00003632
Iteration 118/1000 | Loss: 0.00003624
Iteration 119/1000 | Loss: 0.00003620
Iteration 120/1000 | Loss: 0.00003612
Iteration 121/1000 | Loss: 0.00003609
Iteration 122/1000 | Loss: 0.00003609
Iteration 123/1000 | Loss: 0.00003608
Iteration 124/1000 | Loss: 0.00003607
Iteration 125/1000 | Loss: 0.00003607
Iteration 126/1000 | Loss: 0.00003606
Iteration 127/1000 | Loss: 0.00003606
Iteration 128/1000 | Loss: 0.00003606
Iteration 129/1000 | Loss: 0.00003605
Iteration 130/1000 | Loss: 0.00003605
Iteration 131/1000 | Loss: 0.00003605
Iteration 132/1000 | Loss: 0.00003604
Iteration 133/1000 | Loss: 0.00003604
Iteration 134/1000 | Loss: 0.00003604
Iteration 135/1000 | Loss: 0.00003604
Iteration 136/1000 | Loss: 0.00003603
Iteration 137/1000 | Loss: 0.00003603
Iteration 138/1000 | Loss: 0.00003602
Iteration 139/1000 | Loss: 0.00003602
Iteration 140/1000 | Loss: 0.00003602
Iteration 141/1000 | Loss: 0.00003602
Iteration 142/1000 | Loss: 0.00003602
Iteration 143/1000 | Loss: 0.00003602
Iteration 144/1000 | Loss: 0.00003602
Iteration 145/1000 | Loss: 0.00003602
Iteration 146/1000 | Loss: 0.00003602
Iteration 147/1000 | Loss: 0.00003602
Iteration 148/1000 | Loss: 0.00003602
Iteration 149/1000 | Loss: 0.00003601
Iteration 150/1000 | Loss: 0.00003601
Iteration 151/1000 | Loss: 0.00003601
Iteration 152/1000 | Loss: 0.00003601
Iteration 153/1000 | Loss: 0.00003601
Iteration 154/1000 | Loss: 0.00003601
Iteration 155/1000 | Loss: 0.00003601
Iteration 156/1000 | Loss: 0.00003601
Iteration 157/1000 | Loss: 0.00003600
Iteration 158/1000 | Loss: 0.00003600
Iteration 159/1000 | Loss: 0.00003600
Iteration 160/1000 | Loss: 0.00003600
Iteration 161/1000 | Loss: 0.00003600
Iteration 162/1000 | Loss: 0.00003600
Iteration 163/1000 | Loss: 0.00003600
Iteration 164/1000 | Loss: 0.00003600
Iteration 165/1000 | Loss: 0.00003600
Iteration 166/1000 | Loss: 0.00003599
Iteration 167/1000 | Loss: 0.00003599
Iteration 168/1000 | Loss: 0.00003599
Iteration 169/1000 | Loss: 0.00003599
Iteration 170/1000 | Loss: 0.00003599
Iteration 171/1000 | Loss: 0.00003598
Iteration 172/1000 | Loss: 0.00003598
Iteration 173/1000 | Loss: 0.00003598
Iteration 174/1000 | Loss: 0.00003598
Iteration 175/1000 | Loss: 0.00003598
Iteration 176/1000 | Loss: 0.00003598
Iteration 177/1000 | Loss: 0.00003597
Iteration 178/1000 | Loss: 0.00003597
Iteration 179/1000 | Loss: 0.00003597
Iteration 180/1000 | Loss: 0.00003596
Iteration 181/1000 | Loss: 0.00003596
Iteration 182/1000 | Loss: 0.00003596
Iteration 183/1000 | Loss: 0.00003596
Iteration 184/1000 | Loss: 0.00003595
Iteration 185/1000 | Loss: 0.00003595
Iteration 186/1000 | Loss: 0.00003595
Iteration 187/1000 | Loss: 0.00003595
Iteration 188/1000 | Loss: 0.00003594
Iteration 189/1000 | Loss: 0.00003594
Iteration 190/1000 | Loss: 0.00003594
Iteration 191/1000 | Loss: 0.00003594
Iteration 192/1000 | Loss: 0.00003594
Iteration 193/1000 | Loss: 0.00003594
Iteration 194/1000 | Loss: 0.00003594
Iteration 195/1000 | Loss: 0.00003594
Iteration 196/1000 | Loss: 0.00003594
Iteration 197/1000 | Loss: 0.00003594
Iteration 198/1000 | Loss: 0.00003594
Iteration 199/1000 | Loss: 0.00003594
Iteration 200/1000 | Loss: 0.00003594
Iteration 201/1000 | Loss: 0.00003593
Iteration 202/1000 | Loss: 0.00003593
Iteration 203/1000 | Loss: 0.00003593
Iteration 204/1000 | Loss: 0.00003593
Iteration 205/1000 | Loss: 0.00003593
Iteration 206/1000 | Loss: 0.00003593
Iteration 207/1000 | Loss: 0.00003593
Iteration 208/1000 | Loss: 0.00003593
Iteration 209/1000 | Loss: 0.00003593
Iteration 210/1000 | Loss: 0.00003593
Iteration 211/1000 | Loss: 0.00003592
Iteration 212/1000 | Loss: 0.00003592
Iteration 213/1000 | Loss: 0.00003592
Iteration 214/1000 | Loss: 0.00003592
Iteration 215/1000 | Loss: 0.00003592
Iteration 216/1000 | Loss: 0.00003592
Iteration 217/1000 | Loss: 0.00003592
Iteration 218/1000 | Loss: 0.00003592
Iteration 219/1000 | Loss: 0.00003591
Iteration 220/1000 | Loss: 0.00003591
Iteration 221/1000 | Loss: 0.00003591
Iteration 222/1000 | Loss: 0.00003591
Iteration 223/1000 | Loss: 0.00003591
Iteration 224/1000 | Loss: 0.00003591
Iteration 225/1000 | Loss: 0.00003591
Iteration 226/1000 | Loss: 0.00003591
Iteration 227/1000 | Loss: 0.00003591
Iteration 228/1000 | Loss: 0.00003591
Iteration 229/1000 | Loss: 0.00003591
Iteration 230/1000 | Loss: 0.00003591
Iteration 231/1000 | Loss: 0.00003591
Iteration 232/1000 | Loss: 0.00003591
Iteration 233/1000 | Loss: 0.00003591
Iteration 234/1000 | Loss: 0.00003591
Iteration 235/1000 | Loss: 0.00003591
Iteration 236/1000 | Loss: 0.00003591
Iteration 237/1000 | Loss: 0.00003591
Iteration 238/1000 | Loss: 0.00003591
Iteration 239/1000 | Loss: 0.00003591
Iteration 240/1000 | Loss: 0.00003591
Iteration 241/1000 | Loss: 0.00003591
Iteration 242/1000 | Loss: 0.00003591
Iteration 243/1000 | Loss: 0.00003591
Iteration 244/1000 | Loss: 0.00003591
Iteration 245/1000 | Loss: 0.00003591
Iteration 246/1000 | Loss: 0.00003591
Iteration 247/1000 | Loss: 0.00003591
Iteration 248/1000 | Loss: 0.00003591
Iteration 249/1000 | Loss: 0.00003591
Iteration 250/1000 | Loss: 0.00003591
Iteration 251/1000 | Loss: 0.00003591
Iteration 252/1000 | Loss: 0.00003591
Iteration 253/1000 | Loss: 0.00003591
Iteration 254/1000 | Loss: 0.00003591
Iteration 255/1000 | Loss: 0.00003591
Iteration 256/1000 | Loss: 0.00003591
Iteration 257/1000 | Loss: 0.00003591
Iteration 258/1000 | Loss: 0.00003591
Iteration 259/1000 | Loss: 0.00003591
Iteration 260/1000 | Loss: 0.00003591
Iteration 261/1000 | Loss: 0.00003591
Iteration 262/1000 | Loss: 0.00003591
Iteration 263/1000 | Loss: 0.00003591
Iteration 264/1000 | Loss: 0.00003591
Iteration 265/1000 | Loss: 0.00003591
Iteration 266/1000 | Loss: 0.00003591
Iteration 267/1000 | Loss: 0.00003591
Iteration 268/1000 | Loss: 0.00003591
Iteration 269/1000 | Loss: 0.00003591
Iteration 270/1000 | Loss: 0.00003591
Iteration 271/1000 | Loss: 0.00003591
Iteration 272/1000 | Loss: 0.00003591
Iteration 273/1000 | Loss: 0.00003591
Iteration 274/1000 | Loss: 0.00003591
Iteration 275/1000 | Loss: 0.00003591
Iteration 276/1000 | Loss: 0.00003591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [3.5907003621105105e-05, 3.5907003621105105e-05, 3.5907003621105105e-05, 3.5907003621105105e-05, 3.5907003621105105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5907003621105105e-05

Optimization complete. Final v2v error: 4.82634973526001 mm

Highest mean error: 13.462409973144531 mm for frame 91

Lowest mean error: 4.20518684387207 mm for frame 235

Saving results

Total time: 250.63541626930237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00658758
Iteration 2/25 | Loss: 0.00172055
Iteration 3/25 | Loss: 0.00159547
Iteration 4/25 | Loss: 0.00154195
Iteration 5/25 | Loss: 0.00153003
Iteration 6/25 | Loss: 0.00152640
Iteration 7/25 | Loss: 0.00152532
Iteration 8/25 | Loss: 0.00152529
Iteration 9/25 | Loss: 0.00152529
Iteration 10/25 | Loss: 0.00152529
Iteration 11/25 | Loss: 0.00152529
Iteration 12/25 | Loss: 0.00152529
Iteration 13/25 | Loss: 0.00152529
Iteration 14/25 | Loss: 0.00152529
Iteration 15/25 | Loss: 0.00152529
Iteration 16/25 | Loss: 0.00152529
Iteration 17/25 | Loss: 0.00152529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001525286235846579, 0.001525286235846579, 0.001525286235846579, 0.001525286235846579, 0.001525286235846579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001525286235846579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60421097
Iteration 2/25 | Loss: 0.00297958
Iteration 3/25 | Loss: 0.00297953
Iteration 4/25 | Loss: 0.00297953
Iteration 5/25 | Loss: 0.00297953
Iteration 6/25 | Loss: 0.00297953
Iteration 7/25 | Loss: 0.00297953
Iteration 8/25 | Loss: 0.00297952
Iteration 9/25 | Loss: 0.00297952
Iteration 10/25 | Loss: 0.00297952
Iteration 11/25 | Loss: 0.00297952
Iteration 12/25 | Loss: 0.00297952
Iteration 13/25 | Loss: 0.00297952
Iteration 14/25 | Loss: 0.00297952
Iteration 15/25 | Loss: 0.00297952
Iteration 16/25 | Loss: 0.00297952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0029795244336128235, 0.0029795244336128235, 0.0029795244336128235, 0.0029795244336128235, 0.0029795244336128235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029795244336128235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00297952
Iteration 2/1000 | Loss: 0.00010644
Iteration 3/1000 | Loss: 0.00007972
Iteration 4/1000 | Loss: 0.00006806
Iteration 5/1000 | Loss: 0.00006136
Iteration 6/1000 | Loss: 0.00005916
Iteration 7/1000 | Loss: 0.00005701
Iteration 8/1000 | Loss: 0.00005526
Iteration 9/1000 | Loss: 0.00005396
Iteration 10/1000 | Loss: 0.00005297
Iteration 11/1000 | Loss: 0.00005230
Iteration 12/1000 | Loss: 0.00005188
Iteration 13/1000 | Loss: 0.00005152
Iteration 14/1000 | Loss: 0.00005129
Iteration 15/1000 | Loss: 0.00005123
Iteration 16/1000 | Loss: 0.00005121
Iteration 17/1000 | Loss: 0.00005120
Iteration 18/1000 | Loss: 0.00005120
Iteration 19/1000 | Loss: 0.00005116
Iteration 20/1000 | Loss: 0.00005115
Iteration 21/1000 | Loss: 0.00005114
Iteration 22/1000 | Loss: 0.00005110
Iteration 23/1000 | Loss: 0.00005106
Iteration 24/1000 | Loss: 0.00005105
Iteration 25/1000 | Loss: 0.00005104
Iteration 26/1000 | Loss: 0.00005104
Iteration 27/1000 | Loss: 0.00005103
Iteration 28/1000 | Loss: 0.00005103
Iteration 29/1000 | Loss: 0.00005102
Iteration 30/1000 | Loss: 0.00005102
Iteration 31/1000 | Loss: 0.00005101
Iteration 32/1000 | Loss: 0.00005101
Iteration 33/1000 | Loss: 0.00005101
Iteration 34/1000 | Loss: 0.00005100
Iteration 35/1000 | Loss: 0.00005100
Iteration 36/1000 | Loss: 0.00005099
Iteration 37/1000 | Loss: 0.00005099
Iteration 38/1000 | Loss: 0.00005099
Iteration 39/1000 | Loss: 0.00005098
Iteration 40/1000 | Loss: 0.00005098
Iteration 41/1000 | Loss: 0.00005098
Iteration 42/1000 | Loss: 0.00005098
Iteration 43/1000 | Loss: 0.00005098
Iteration 44/1000 | Loss: 0.00005097
Iteration 45/1000 | Loss: 0.00005097
Iteration 46/1000 | Loss: 0.00005096
Iteration 47/1000 | Loss: 0.00005096
Iteration 48/1000 | Loss: 0.00005096
Iteration 49/1000 | Loss: 0.00005096
Iteration 50/1000 | Loss: 0.00005095
Iteration 51/1000 | Loss: 0.00005095
Iteration 52/1000 | Loss: 0.00005095
Iteration 53/1000 | Loss: 0.00005094
Iteration 54/1000 | Loss: 0.00005094
Iteration 55/1000 | Loss: 0.00005094
Iteration 56/1000 | Loss: 0.00005093
Iteration 57/1000 | Loss: 0.00005093
Iteration 58/1000 | Loss: 0.00005092
Iteration 59/1000 | Loss: 0.00005092
Iteration 60/1000 | Loss: 0.00005092
Iteration 61/1000 | Loss: 0.00005091
Iteration 62/1000 | Loss: 0.00005091
Iteration 63/1000 | Loss: 0.00005091
Iteration 64/1000 | Loss: 0.00005090
Iteration 65/1000 | Loss: 0.00005090
Iteration 66/1000 | Loss: 0.00005090
Iteration 67/1000 | Loss: 0.00005089
Iteration 68/1000 | Loss: 0.00005089
Iteration 69/1000 | Loss: 0.00005089
Iteration 70/1000 | Loss: 0.00005089
Iteration 71/1000 | Loss: 0.00005088
Iteration 72/1000 | Loss: 0.00005088
Iteration 73/1000 | Loss: 0.00005088
Iteration 74/1000 | Loss: 0.00005088
Iteration 75/1000 | Loss: 0.00005088
Iteration 76/1000 | Loss: 0.00005088
Iteration 77/1000 | Loss: 0.00005088
Iteration 78/1000 | Loss: 0.00005088
Iteration 79/1000 | Loss: 0.00005088
Iteration 80/1000 | Loss: 0.00005087
Iteration 81/1000 | Loss: 0.00005087
Iteration 82/1000 | Loss: 0.00005087
Iteration 83/1000 | Loss: 0.00005087
Iteration 84/1000 | Loss: 0.00005087
Iteration 85/1000 | Loss: 0.00005086
Iteration 86/1000 | Loss: 0.00005086
Iteration 87/1000 | Loss: 0.00005086
Iteration 88/1000 | Loss: 0.00005086
Iteration 89/1000 | Loss: 0.00005086
Iteration 90/1000 | Loss: 0.00005086
Iteration 91/1000 | Loss: 0.00005085
Iteration 92/1000 | Loss: 0.00005085
Iteration 93/1000 | Loss: 0.00005085
Iteration 94/1000 | Loss: 0.00005085
Iteration 95/1000 | Loss: 0.00005085
Iteration 96/1000 | Loss: 0.00005085
Iteration 97/1000 | Loss: 0.00005085
Iteration 98/1000 | Loss: 0.00005085
Iteration 99/1000 | Loss: 0.00005085
Iteration 100/1000 | Loss: 0.00005085
Iteration 101/1000 | Loss: 0.00005084
Iteration 102/1000 | Loss: 0.00005084
Iteration 103/1000 | Loss: 0.00005084
Iteration 104/1000 | Loss: 0.00005084
Iteration 105/1000 | Loss: 0.00005084
Iteration 106/1000 | Loss: 0.00005084
Iteration 107/1000 | Loss: 0.00005084
Iteration 108/1000 | Loss: 0.00005084
Iteration 109/1000 | Loss: 0.00005084
Iteration 110/1000 | Loss: 0.00005084
Iteration 111/1000 | Loss: 0.00005084
Iteration 112/1000 | Loss: 0.00005083
Iteration 113/1000 | Loss: 0.00005083
Iteration 114/1000 | Loss: 0.00005083
Iteration 115/1000 | Loss: 0.00005082
Iteration 116/1000 | Loss: 0.00005082
Iteration 117/1000 | Loss: 0.00005082
Iteration 118/1000 | Loss: 0.00005082
Iteration 119/1000 | Loss: 0.00005082
Iteration 120/1000 | Loss: 0.00005082
Iteration 121/1000 | Loss: 0.00005082
Iteration 122/1000 | Loss: 0.00005081
Iteration 123/1000 | Loss: 0.00005081
Iteration 124/1000 | Loss: 0.00005081
Iteration 125/1000 | Loss: 0.00005081
Iteration 126/1000 | Loss: 0.00005081
Iteration 127/1000 | Loss: 0.00005081
Iteration 128/1000 | Loss: 0.00005081
Iteration 129/1000 | Loss: 0.00005081
Iteration 130/1000 | Loss: 0.00005080
Iteration 131/1000 | Loss: 0.00005080
Iteration 132/1000 | Loss: 0.00005080
Iteration 133/1000 | Loss: 0.00005080
Iteration 134/1000 | Loss: 0.00005080
Iteration 135/1000 | Loss: 0.00005080
Iteration 136/1000 | Loss: 0.00005080
Iteration 137/1000 | Loss: 0.00005080
Iteration 138/1000 | Loss: 0.00005080
Iteration 139/1000 | Loss: 0.00005079
Iteration 140/1000 | Loss: 0.00005079
Iteration 141/1000 | Loss: 0.00005079
Iteration 142/1000 | Loss: 0.00005079
Iteration 143/1000 | Loss: 0.00005079
Iteration 144/1000 | Loss: 0.00005079
Iteration 145/1000 | Loss: 0.00005079
Iteration 146/1000 | Loss: 0.00005079
Iteration 147/1000 | Loss: 0.00005079
Iteration 148/1000 | Loss: 0.00005079
Iteration 149/1000 | Loss: 0.00005078
Iteration 150/1000 | Loss: 0.00005078
Iteration 151/1000 | Loss: 0.00005078
Iteration 152/1000 | Loss: 0.00005078
Iteration 153/1000 | Loss: 0.00005078
Iteration 154/1000 | Loss: 0.00005078
Iteration 155/1000 | Loss: 0.00005078
Iteration 156/1000 | Loss: 0.00005078
Iteration 157/1000 | Loss: 0.00005078
Iteration 158/1000 | Loss: 0.00005078
Iteration 159/1000 | Loss: 0.00005078
Iteration 160/1000 | Loss: 0.00005078
Iteration 161/1000 | Loss: 0.00005078
Iteration 162/1000 | Loss: 0.00005078
Iteration 163/1000 | Loss: 0.00005078
Iteration 164/1000 | Loss: 0.00005078
Iteration 165/1000 | Loss: 0.00005078
Iteration 166/1000 | Loss: 0.00005078
Iteration 167/1000 | Loss: 0.00005078
Iteration 168/1000 | Loss: 0.00005078
Iteration 169/1000 | Loss: 0.00005078
Iteration 170/1000 | Loss: 0.00005078
Iteration 171/1000 | Loss: 0.00005078
Iteration 172/1000 | Loss: 0.00005078
Iteration 173/1000 | Loss: 0.00005077
Iteration 174/1000 | Loss: 0.00005077
Iteration 175/1000 | Loss: 0.00005077
Iteration 176/1000 | Loss: 0.00005077
Iteration 177/1000 | Loss: 0.00005077
Iteration 178/1000 | Loss: 0.00005077
Iteration 179/1000 | Loss: 0.00005077
Iteration 180/1000 | Loss: 0.00005077
Iteration 181/1000 | Loss: 0.00005077
Iteration 182/1000 | Loss: 0.00005077
Iteration 183/1000 | Loss: 0.00005077
Iteration 184/1000 | Loss: 0.00005077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [5.077154855825938e-05, 5.077154855825938e-05, 5.077154855825938e-05, 5.077154855825938e-05, 5.077154855825938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.077154855825938e-05

Optimization complete. Final v2v error: 5.869773864746094 mm

Highest mean error: 6.426432132720947 mm for frame 122

Lowest mean error: 5.026836395263672 mm for frame 15

Saving results

Total time: 44.182788133621216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01282490
Iteration 2/25 | Loss: 0.00324502
Iteration 3/25 | Loss: 0.00182665
Iteration 4/25 | Loss: 0.00164942
Iteration 5/25 | Loss: 0.00184347
Iteration 6/25 | Loss: 0.00160946
Iteration 7/25 | Loss: 0.00156974
Iteration 8/25 | Loss: 0.00149632
Iteration 9/25 | Loss: 0.00148331
Iteration 10/25 | Loss: 0.00149529
Iteration 11/25 | Loss: 0.00146904
Iteration 12/25 | Loss: 0.00144957
Iteration 13/25 | Loss: 0.00140448
Iteration 14/25 | Loss: 0.00139693
Iteration 15/25 | Loss: 0.00139534
Iteration 16/25 | Loss: 0.00141583
Iteration 17/25 | Loss: 0.00138587
Iteration 18/25 | Loss: 0.00138435
Iteration 19/25 | Loss: 0.00138413
Iteration 20/25 | Loss: 0.00138411
Iteration 21/25 | Loss: 0.00138410
Iteration 22/25 | Loss: 0.00138410
Iteration 23/25 | Loss: 0.00138410
Iteration 24/25 | Loss: 0.00138409
Iteration 25/25 | Loss: 0.00138408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84670937
Iteration 2/25 | Loss: 0.00239047
Iteration 3/25 | Loss: 0.00239046
Iteration 4/25 | Loss: 0.00239046
Iteration 5/25 | Loss: 0.00239045
Iteration 6/25 | Loss: 0.00239045
Iteration 7/25 | Loss: 0.00239045
Iteration 8/25 | Loss: 0.00239045
Iteration 9/25 | Loss: 0.00239045
Iteration 10/25 | Loss: 0.00239045
Iteration 11/25 | Loss: 0.00239045
Iteration 12/25 | Loss: 0.00239045
Iteration 13/25 | Loss: 0.00239045
Iteration 14/25 | Loss: 0.00239045
Iteration 15/25 | Loss: 0.00239045
Iteration 16/25 | Loss: 0.00239045
Iteration 17/25 | Loss: 0.00239045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023904531262815, 0.0023904531262815, 0.0023904531262815, 0.0023904531262815, 0.0023904531262815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023904531262815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239045
Iteration 2/1000 | Loss: 0.00042508
Iteration 3/1000 | Loss: 0.00029645
Iteration 4/1000 | Loss: 0.00027460
Iteration 5/1000 | Loss: 0.00205683
Iteration 6/1000 | Loss: 0.00023520
Iteration 7/1000 | Loss: 0.00093351
Iteration 8/1000 | Loss: 0.00019152
Iteration 9/1000 | Loss: 0.00016043
Iteration 10/1000 | Loss: 0.00014624
Iteration 11/1000 | Loss: 0.00198019
Iteration 12/1000 | Loss: 0.00106155
Iteration 13/1000 | Loss: 0.00237639
Iteration 14/1000 | Loss: 0.00081982
Iteration 15/1000 | Loss: 0.00048513
Iteration 16/1000 | Loss: 0.00195890
Iteration 17/1000 | Loss: 0.00153345
Iteration 18/1000 | Loss: 0.00028201
Iteration 19/1000 | Loss: 0.00328712
Iteration 20/1000 | Loss: 0.00141273
Iteration 21/1000 | Loss: 0.00148892
Iteration 22/1000 | Loss: 0.00277845
Iteration 23/1000 | Loss: 0.00090277
Iteration 24/1000 | Loss: 0.00087685
Iteration 25/1000 | Loss: 0.00263465
Iteration 26/1000 | Loss: 0.00201149
Iteration 27/1000 | Loss: 0.00268808
Iteration 28/1000 | Loss: 0.00110298
Iteration 29/1000 | Loss: 0.00269817
Iteration 30/1000 | Loss: 0.00241024
Iteration 31/1000 | Loss: 0.00190696
Iteration 32/1000 | Loss: 0.00331629
Iteration 33/1000 | Loss: 0.00287290
Iteration 34/1000 | Loss: 0.00420428
Iteration 35/1000 | Loss: 0.00253030
Iteration 36/1000 | Loss: 0.00121903
Iteration 37/1000 | Loss: 0.00231636
Iteration 38/1000 | Loss: 0.00152866
Iteration 39/1000 | Loss: 0.00152856
Iteration 40/1000 | Loss: 0.00141041
Iteration 41/1000 | Loss: 0.00118906
Iteration 42/1000 | Loss: 0.00224321
Iteration 43/1000 | Loss: 0.00247520
Iteration 44/1000 | Loss: 0.00265562
Iteration 45/1000 | Loss: 0.00214165
Iteration 46/1000 | Loss: 0.00228009
Iteration 47/1000 | Loss: 0.00255432
Iteration 48/1000 | Loss: 0.00120596
Iteration 49/1000 | Loss: 0.00131353
Iteration 50/1000 | Loss: 0.00509510
Iteration 51/1000 | Loss: 0.00272661
Iteration 52/1000 | Loss: 0.00188873
Iteration 53/1000 | Loss: 0.00145344
Iteration 54/1000 | Loss: 0.00119659
Iteration 55/1000 | Loss: 0.00250444
Iteration 56/1000 | Loss: 0.00173148
Iteration 57/1000 | Loss: 0.00116758
Iteration 58/1000 | Loss: 0.00263389
Iteration 59/1000 | Loss: 0.00175996
Iteration 60/1000 | Loss: 0.00167928
Iteration 61/1000 | Loss: 0.00184119
Iteration 62/1000 | Loss: 0.00188149
Iteration 63/1000 | Loss: 0.00294144
Iteration 64/1000 | Loss: 0.00236751
Iteration 65/1000 | Loss: 0.00369869
Iteration 66/1000 | Loss: 0.00273126
Iteration 67/1000 | Loss: 0.00417448
Iteration 68/1000 | Loss: 0.00348855
Iteration 69/1000 | Loss: 0.00219212
Iteration 70/1000 | Loss: 0.00179180
Iteration 71/1000 | Loss: 0.00324326
Iteration 72/1000 | Loss: 0.00219745
Iteration 73/1000 | Loss: 0.00104740
Iteration 74/1000 | Loss: 0.00112133
Iteration 75/1000 | Loss: 0.00219671
Iteration 76/1000 | Loss: 0.00166075
Iteration 77/1000 | Loss: 0.00198293
Iteration 78/1000 | Loss: 0.00079431
Iteration 79/1000 | Loss: 0.00178329
Iteration 80/1000 | Loss: 0.00030619
Iteration 81/1000 | Loss: 0.00071183
Iteration 82/1000 | Loss: 0.00023343
Iteration 83/1000 | Loss: 0.00034746
Iteration 84/1000 | Loss: 0.00077273
Iteration 85/1000 | Loss: 0.00052545
Iteration 86/1000 | Loss: 0.00049207
Iteration 87/1000 | Loss: 0.00025184
Iteration 88/1000 | Loss: 0.00144108
Iteration 89/1000 | Loss: 0.00070951
Iteration 90/1000 | Loss: 0.00128934
Iteration 91/1000 | Loss: 0.00137178
Iteration 92/1000 | Loss: 0.00059287
Iteration 93/1000 | Loss: 0.00075428
Iteration 94/1000 | Loss: 0.00146670
Iteration 95/1000 | Loss: 0.00088866
Iteration 96/1000 | Loss: 0.00083026
Iteration 97/1000 | Loss: 0.00169616
Iteration 98/1000 | Loss: 0.00016113
Iteration 99/1000 | Loss: 0.00013065
Iteration 100/1000 | Loss: 0.00086764
Iteration 101/1000 | Loss: 0.00170129
Iteration 102/1000 | Loss: 0.00080601
Iteration 103/1000 | Loss: 0.00164750
Iteration 104/1000 | Loss: 0.00124629
Iteration 105/1000 | Loss: 0.00064092
Iteration 106/1000 | Loss: 0.00076692
Iteration 107/1000 | Loss: 0.00100223
Iteration 108/1000 | Loss: 0.00036211
Iteration 109/1000 | Loss: 0.00084550
Iteration 110/1000 | Loss: 0.00042923
Iteration 111/1000 | Loss: 0.00064057
Iteration 112/1000 | Loss: 0.00059797
Iteration 113/1000 | Loss: 0.00066209
Iteration 114/1000 | Loss: 0.00033692
Iteration 115/1000 | Loss: 0.00059309
Iteration 116/1000 | Loss: 0.00029421
Iteration 117/1000 | Loss: 0.00050047
Iteration 118/1000 | Loss: 0.00065808
Iteration 119/1000 | Loss: 0.00210610
Iteration 120/1000 | Loss: 0.00101242
Iteration 121/1000 | Loss: 0.00179224
Iteration 122/1000 | Loss: 0.00334122
Iteration 123/1000 | Loss: 0.00287744
Iteration 124/1000 | Loss: 0.00132608
Iteration 125/1000 | Loss: 0.00219012
Iteration 126/1000 | Loss: 0.00182662
Iteration 127/1000 | Loss: 0.00091501
Iteration 128/1000 | Loss: 0.00040893
Iteration 129/1000 | Loss: 0.00063945
Iteration 130/1000 | Loss: 0.00083913
Iteration 131/1000 | Loss: 0.00009694
Iteration 132/1000 | Loss: 0.00011160
Iteration 133/1000 | Loss: 0.00011611
Iteration 134/1000 | Loss: 0.00045027
Iteration 135/1000 | Loss: 0.00056647
Iteration 136/1000 | Loss: 0.00064150
Iteration 137/1000 | Loss: 0.00030573
Iteration 138/1000 | Loss: 0.00018273
Iteration 139/1000 | Loss: 0.00030559
Iteration 140/1000 | Loss: 0.00058714
Iteration 141/1000 | Loss: 0.00097423
Iteration 142/1000 | Loss: 0.00293076
Iteration 143/1000 | Loss: 0.00081329
Iteration 144/1000 | Loss: 0.00130809
Iteration 145/1000 | Loss: 0.00305701
Iteration 146/1000 | Loss: 0.00241820
Iteration 147/1000 | Loss: 0.00159220
Iteration 148/1000 | Loss: 0.00058308
Iteration 149/1000 | Loss: 0.00011371
Iteration 150/1000 | Loss: 0.00088751
Iteration 151/1000 | Loss: 0.00300476
Iteration 152/1000 | Loss: 0.00220460
Iteration 153/1000 | Loss: 0.00107945
Iteration 154/1000 | Loss: 0.00136958
Iteration 155/1000 | Loss: 0.00022019
Iteration 156/1000 | Loss: 0.00076725
Iteration 157/1000 | Loss: 0.00182996
Iteration 158/1000 | Loss: 0.00078201
Iteration 159/1000 | Loss: 0.00114849
Iteration 160/1000 | Loss: 0.00148232
Iteration 161/1000 | Loss: 0.00097106
Iteration 162/1000 | Loss: 0.00137873
Iteration 163/1000 | Loss: 0.00062467
Iteration 164/1000 | Loss: 0.00159471
Iteration 165/1000 | Loss: 0.00141535
Iteration 166/1000 | Loss: 0.00134306
Iteration 167/1000 | Loss: 0.00097110
Iteration 168/1000 | Loss: 0.00065415
Iteration 169/1000 | Loss: 0.00073282
Iteration 170/1000 | Loss: 0.00060081
Iteration 171/1000 | Loss: 0.00079791
Iteration 172/1000 | Loss: 0.00106289
Iteration 173/1000 | Loss: 0.00079999
Iteration 174/1000 | Loss: 0.00144857
Iteration 175/1000 | Loss: 0.00157640
Iteration 176/1000 | Loss: 0.00278935
Iteration 177/1000 | Loss: 0.00113624
Iteration 178/1000 | Loss: 0.00090385
Iteration 179/1000 | Loss: 0.00013967
Iteration 180/1000 | Loss: 0.00010853
Iteration 181/1000 | Loss: 0.00103656
Iteration 182/1000 | Loss: 0.00075556
Iteration 183/1000 | Loss: 0.00084802
Iteration 184/1000 | Loss: 0.00009993
Iteration 185/1000 | Loss: 0.00008693
Iteration 186/1000 | Loss: 0.00007616
Iteration 187/1000 | Loss: 0.00006967
Iteration 188/1000 | Loss: 0.00006621
Iteration 189/1000 | Loss: 0.00006332
Iteration 190/1000 | Loss: 0.00006161
Iteration 191/1000 | Loss: 0.00006082
Iteration 192/1000 | Loss: 0.00006031
Iteration 193/1000 | Loss: 0.00005979
Iteration 194/1000 | Loss: 0.00005944
Iteration 195/1000 | Loss: 0.00005921
Iteration 196/1000 | Loss: 0.00005905
Iteration 197/1000 | Loss: 0.00005903
Iteration 198/1000 | Loss: 0.00005887
Iteration 199/1000 | Loss: 0.00005880
Iteration 200/1000 | Loss: 0.00005875
Iteration 201/1000 | Loss: 0.00005875
Iteration 202/1000 | Loss: 0.00005865
Iteration 203/1000 | Loss: 0.00005865
Iteration 204/1000 | Loss: 0.00005865
Iteration 205/1000 | Loss: 0.00005865
Iteration 206/1000 | Loss: 0.00005865
Iteration 207/1000 | Loss: 0.00005865
Iteration 208/1000 | Loss: 0.00005865
Iteration 209/1000 | Loss: 0.00005865
Iteration 210/1000 | Loss: 0.00005865
Iteration 211/1000 | Loss: 0.00005865
Iteration 212/1000 | Loss: 0.00005865
Iteration 213/1000 | Loss: 0.00005864
Iteration 214/1000 | Loss: 0.00005864
Iteration 215/1000 | Loss: 0.00005864
Iteration 216/1000 | Loss: 0.00005864
Iteration 217/1000 | Loss: 0.00005863
Iteration 218/1000 | Loss: 0.00005863
Iteration 219/1000 | Loss: 0.00005861
Iteration 220/1000 | Loss: 0.00005861
Iteration 221/1000 | Loss: 0.00005861
Iteration 222/1000 | Loss: 0.00005861
Iteration 223/1000 | Loss: 0.00005861
Iteration 224/1000 | Loss: 0.00005861
Iteration 225/1000 | Loss: 0.00005861
Iteration 226/1000 | Loss: 0.00005859
Iteration 227/1000 | Loss: 0.00005859
Iteration 228/1000 | Loss: 0.00005859
Iteration 229/1000 | Loss: 0.00005859
Iteration 230/1000 | Loss: 0.00005859
Iteration 231/1000 | Loss: 0.00005859
Iteration 232/1000 | Loss: 0.00005859
Iteration 233/1000 | Loss: 0.00005859
Iteration 234/1000 | Loss: 0.00005856
Iteration 235/1000 | Loss: 0.00005856
Iteration 236/1000 | Loss: 0.00005856
Iteration 237/1000 | Loss: 0.00005856
Iteration 238/1000 | Loss: 0.00005856
Iteration 239/1000 | Loss: 0.00005856
Iteration 240/1000 | Loss: 0.00005856
Iteration 241/1000 | Loss: 0.00005856
Iteration 242/1000 | Loss: 0.00005855
Iteration 243/1000 | Loss: 0.00005855
Iteration 244/1000 | Loss: 0.00005855
Iteration 245/1000 | Loss: 0.00005854
Iteration 246/1000 | Loss: 0.00005854
Iteration 247/1000 | Loss: 0.00005854
Iteration 248/1000 | Loss: 0.00005854
Iteration 249/1000 | Loss: 0.00005854
Iteration 250/1000 | Loss: 0.00005854
Iteration 251/1000 | Loss: 0.00005854
Iteration 252/1000 | Loss: 0.00005854
Iteration 253/1000 | Loss: 0.00005853
Iteration 254/1000 | Loss: 0.00005853
Iteration 255/1000 | Loss: 0.00005853
Iteration 256/1000 | Loss: 0.00005853
Iteration 257/1000 | Loss: 0.00005853
Iteration 258/1000 | Loss: 0.00005853
Iteration 259/1000 | Loss: 0.00005853
Iteration 260/1000 | Loss: 0.00005853
Iteration 261/1000 | Loss: 0.00005852
Iteration 262/1000 | Loss: 0.00005852
Iteration 263/1000 | Loss: 0.00005852
Iteration 264/1000 | Loss: 0.00005852
Iteration 265/1000 | Loss: 0.00005852
Iteration 266/1000 | Loss: 0.00005852
Iteration 267/1000 | Loss: 0.00005851
Iteration 268/1000 | Loss: 0.00005851
Iteration 269/1000 | Loss: 0.00005851
Iteration 270/1000 | Loss: 0.00005851
Iteration 271/1000 | Loss: 0.00005851
Iteration 272/1000 | Loss: 0.00005851
Iteration 273/1000 | Loss: 0.00005851
Iteration 274/1000 | Loss: 0.00005851
Iteration 275/1000 | Loss: 0.00005851
Iteration 276/1000 | Loss: 0.00005851
Iteration 277/1000 | Loss: 0.00005850
Iteration 278/1000 | Loss: 0.00005850
Iteration 279/1000 | Loss: 0.00005850
Iteration 280/1000 | Loss: 0.00005850
Iteration 281/1000 | Loss: 0.00005850
Iteration 282/1000 | Loss: 0.00005850
Iteration 283/1000 | Loss: 0.00005850
Iteration 284/1000 | Loss: 0.00005850
Iteration 285/1000 | Loss: 0.00005849
Iteration 286/1000 | Loss: 0.00005849
Iteration 287/1000 | Loss: 0.00005849
Iteration 288/1000 | Loss: 0.00005849
Iteration 289/1000 | Loss: 0.00005849
Iteration 290/1000 | Loss: 0.00005849
Iteration 291/1000 | Loss: 0.00005849
Iteration 292/1000 | Loss: 0.00005849
Iteration 293/1000 | Loss: 0.00005848
Iteration 294/1000 | Loss: 0.00005848
Iteration 295/1000 | Loss: 0.00005848
Iteration 296/1000 | Loss: 0.00005847
Iteration 297/1000 | Loss: 0.00005847
Iteration 298/1000 | Loss: 0.00005847
Iteration 299/1000 | Loss: 0.00005847
Iteration 300/1000 | Loss: 0.00005847
Iteration 301/1000 | Loss: 0.00005847
Iteration 302/1000 | Loss: 0.00005847
Iteration 303/1000 | Loss: 0.00005847
Iteration 304/1000 | Loss: 0.00005847
Iteration 305/1000 | Loss: 0.00005847
Iteration 306/1000 | Loss: 0.00005847
Iteration 307/1000 | Loss: 0.00005847
Iteration 308/1000 | Loss: 0.00005847
Iteration 309/1000 | Loss: 0.00005847
Iteration 310/1000 | Loss: 0.00005846
Iteration 311/1000 | Loss: 0.00005846
Iteration 312/1000 | Loss: 0.00005846
Iteration 313/1000 | Loss: 0.00005846
Iteration 314/1000 | Loss: 0.00005846
Iteration 315/1000 | Loss: 0.00005846
Iteration 316/1000 | Loss: 0.00005846
Iteration 317/1000 | Loss: 0.00005846
Iteration 318/1000 | Loss: 0.00005846
Iteration 319/1000 | Loss: 0.00005846
Iteration 320/1000 | Loss: 0.00005846
Iteration 321/1000 | Loss: 0.00005846
Iteration 322/1000 | Loss: 0.00005846
Iteration 323/1000 | Loss: 0.00005846
Iteration 324/1000 | Loss: 0.00005846
Iteration 325/1000 | Loss: 0.00005846
Iteration 326/1000 | Loss: 0.00005845
Iteration 327/1000 | Loss: 0.00005845
Iteration 328/1000 | Loss: 0.00005845
Iteration 329/1000 | Loss: 0.00005845
Iteration 330/1000 | Loss: 0.00005845
Iteration 331/1000 | Loss: 0.00005845
Iteration 332/1000 | Loss: 0.00005845
Iteration 333/1000 | Loss: 0.00005845
Iteration 334/1000 | Loss: 0.00005844
Iteration 335/1000 | Loss: 0.00005844
Iteration 336/1000 | Loss: 0.00005844
Iteration 337/1000 | Loss: 0.00005844
Iteration 338/1000 | Loss: 0.00005844
Iteration 339/1000 | Loss: 0.00005844
Iteration 340/1000 | Loss: 0.00005844
Iteration 341/1000 | Loss: 0.00005844
Iteration 342/1000 | Loss: 0.00005844
Iteration 343/1000 | Loss: 0.00005844
Iteration 344/1000 | Loss: 0.00005844
Iteration 345/1000 | Loss: 0.00005844
Iteration 346/1000 | Loss: 0.00005844
Iteration 347/1000 | Loss: 0.00005844
Iteration 348/1000 | Loss: 0.00005844
Iteration 349/1000 | Loss: 0.00005844
Iteration 350/1000 | Loss: 0.00005843
Iteration 351/1000 | Loss: 0.00005843
Iteration 352/1000 | Loss: 0.00005843
Iteration 353/1000 | Loss: 0.00005843
Iteration 354/1000 | Loss: 0.00005843
Iteration 355/1000 | Loss: 0.00005843
Iteration 356/1000 | Loss: 0.00005843
Iteration 357/1000 | Loss: 0.00005843
Iteration 358/1000 | Loss: 0.00005843
Iteration 359/1000 | Loss: 0.00005843
Iteration 360/1000 | Loss: 0.00005843
Iteration 361/1000 | Loss: 0.00005843
Iteration 362/1000 | Loss: 0.00005843
Iteration 363/1000 | Loss: 0.00005843
Iteration 364/1000 | Loss: 0.00005843
Iteration 365/1000 | Loss: 0.00005843
Iteration 366/1000 | Loss: 0.00005843
Iteration 367/1000 | Loss: 0.00005843
Iteration 368/1000 | Loss: 0.00005843
Iteration 369/1000 | Loss: 0.00005843
Iteration 370/1000 | Loss: 0.00005843
Iteration 371/1000 | Loss: 0.00005843
Iteration 372/1000 | Loss: 0.00005843
Iteration 373/1000 | Loss: 0.00005843
Iteration 374/1000 | Loss: 0.00005843
Iteration 375/1000 | Loss: 0.00005843
Iteration 376/1000 | Loss: 0.00005843
Iteration 377/1000 | Loss: 0.00005843
Iteration 378/1000 | Loss: 0.00005843
Iteration 379/1000 | Loss: 0.00005843
Iteration 380/1000 | Loss: 0.00005843
Iteration 381/1000 | Loss: 0.00005843
Iteration 382/1000 | Loss: 0.00005843
Iteration 383/1000 | Loss: 0.00005843
Iteration 384/1000 | Loss: 0.00005843
Iteration 385/1000 | Loss: 0.00005843
Iteration 386/1000 | Loss: 0.00005843
Iteration 387/1000 | Loss: 0.00005843
Iteration 388/1000 | Loss: 0.00005843
Iteration 389/1000 | Loss: 0.00005843
Iteration 390/1000 | Loss: 0.00005843
Iteration 391/1000 | Loss: 0.00005843
Iteration 392/1000 | Loss: 0.00005843
Iteration 393/1000 | Loss: 0.00005843
Iteration 394/1000 | Loss: 0.00005843
Iteration 395/1000 | Loss: 0.00005843
Iteration 396/1000 | Loss: 0.00005843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 396. Stopping optimization.
Last 5 losses: [5.842810060130432e-05, 5.842810060130432e-05, 5.842810060130432e-05, 5.842810060130432e-05, 5.842810060130432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.842810060130432e-05

Optimization complete. Final v2v error: 6.249176025390625 mm

Highest mean error: 11.850208282470703 mm for frame 20

Lowest mean error: 5.449685096740723 mm for frame 25

Saving results

Total time: 322.9458541870117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_5462/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_5462/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425150
Iteration 2/25 | Loss: 0.00131968
Iteration 3/25 | Loss: 0.00122600
Iteration 4/25 | Loss: 0.00121651
Iteration 5/25 | Loss: 0.00121358
Iteration 6/25 | Loss: 0.00121309
Iteration 7/25 | Loss: 0.00121309
Iteration 8/25 | Loss: 0.00121309
Iteration 9/25 | Loss: 0.00121309
Iteration 10/25 | Loss: 0.00121309
Iteration 11/25 | Loss: 0.00121309
Iteration 12/25 | Loss: 0.00121309
Iteration 13/25 | Loss: 0.00121309
Iteration 14/25 | Loss: 0.00121309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012130915420129895, 0.0012130915420129895, 0.0012130915420129895, 0.0012130915420129895, 0.0012130915420129895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012130915420129895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62393141
Iteration 2/25 | Loss: 0.00200770
Iteration 3/25 | Loss: 0.00200770
Iteration 4/25 | Loss: 0.00200770
Iteration 5/25 | Loss: 0.00200770
Iteration 6/25 | Loss: 0.00200770
Iteration 7/25 | Loss: 0.00200770
Iteration 8/25 | Loss: 0.00200770
Iteration 9/25 | Loss: 0.00200770
Iteration 10/25 | Loss: 0.00200770
Iteration 11/25 | Loss: 0.00200770
Iteration 12/25 | Loss: 0.00200770
Iteration 13/25 | Loss: 0.00200770
Iteration 14/25 | Loss: 0.00200770
Iteration 15/25 | Loss: 0.00200770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002007698640227318, 0.002007698640227318, 0.002007698640227318, 0.002007698640227318, 0.002007698640227318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002007698640227318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200770
Iteration 2/1000 | Loss: 0.00003733
Iteration 3/1000 | Loss: 0.00002595
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002163
Iteration 7/1000 | Loss: 0.00002125
Iteration 8/1000 | Loss: 0.00002096
Iteration 9/1000 | Loss: 0.00002075
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002062
Iteration 12/1000 | Loss: 0.00002058
Iteration 13/1000 | Loss: 0.00002057
Iteration 14/1000 | Loss: 0.00002056
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00002052
Iteration 18/1000 | Loss: 0.00002047
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002047
Iteration 21/1000 | Loss: 0.00002046
Iteration 22/1000 | Loss: 0.00002046
Iteration 23/1000 | Loss: 0.00002045
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00002044
Iteration 26/1000 | Loss: 0.00002044
Iteration 27/1000 | Loss: 0.00002044
Iteration 28/1000 | Loss: 0.00002044
Iteration 29/1000 | Loss: 0.00002044
Iteration 30/1000 | Loss: 0.00002044
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002043
Iteration 34/1000 | Loss: 0.00002042
Iteration 35/1000 | Loss: 0.00002042
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002041
Iteration 38/1000 | Loss: 0.00002041
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002040
Iteration 42/1000 | Loss: 0.00002040
Iteration 43/1000 | Loss: 0.00002040
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002040
Iteration 46/1000 | Loss: 0.00002040
Iteration 47/1000 | Loss: 0.00002040
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002040
Iteration 50/1000 | Loss: 0.00002040
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002039
Iteration 55/1000 | Loss: 0.00002039
Iteration 56/1000 | Loss: 0.00002039
Iteration 57/1000 | Loss: 0.00002039
Iteration 58/1000 | Loss: 0.00002039
Iteration 59/1000 | Loss: 0.00002039
Iteration 60/1000 | Loss: 0.00002039
Iteration 61/1000 | Loss: 0.00002039
Iteration 62/1000 | Loss: 0.00002039
Iteration 63/1000 | Loss: 0.00002039
Iteration 64/1000 | Loss: 0.00002038
Iteration 65/1000 | Loss: 0.00002038
Iteration 66/1000 | Loss: 0.00002038
Iteration 67/1000 | Loss: 0.00002038
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002038
Iteration 71/1000 | Loss: 0.00002038
Iteration 72/1000 | Loss: 0.00002038
Iteration 73/1000 | Loss: 0.00002038
Iteration 74/1000 | Loss: 0.00002038
Iteration 75/1000 | Loss: 0.00002037
Iteration 76/1000 | Loss: 0.00002037
Iteration 77/1000 | Loss: 0.00002037
Iteration 78/1000 | Loss: 0.00002037
Iteration 79/1000 | Loss: 0.00002037
Iteration 80/1000 | Loss: 0.00002037
Iteration 81/1000 | Loss: 0.00002037
Iteration 82/1000 | Loss: 0.00002037
Iteration 83/1000 | Loss: 0.00002037
Iteration 84/1000 | Loss: 0.00002037
Iteration 85/1000 | Loss: 0.00002037
Iteration 86/1000 | Loss: 0.00002037
Iteration 87/1000 | Loss: 0.00002037
Iteration 88/1000 | Loss: 0.00002037
Iteration 89/1000 | Loss: 0.00002037
Iteration 90/1000 | Loss: 0.00002037
Iteration 91/1000 | Loss: 0.00002037
Iteration 92/1000 | Loss: 0.00002037
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00002036
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002036
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002036
Iteration 101/1000 | Loss: 0.00002036
Iteration 102/1000 | Loss: 0.00002036
Iteration 103/1000 | Loss: 0.00002036
Iteration 104/1000 | Loss: 0.00002036
Iteration 105/1000 | Loss: 0.00002036
Iteration 106/1000 | Loss: 0.00002036
Iteration 107/1000 | Loss: 0.00002036
Iteration 108/1000 | Loss: 0.00002035
Iteration 109/1000 | Loss: 0.00002035
Iteration 110/1000 | Loss: 0.00002035
Iteration 111/1000 | Loss: 0.00002035
Iteration 112/1000 | Loss: 0.00002035
Iteration 113/1000 | Loss: 0.00002035
Iteration 114/1000 | Loss: 0.00002035
Iteration 115/1000 | Loss: 0.00002035
Iteration 116/1000 | Loss: 0.00002035
Iteration 117/1000 | Loss: 0.00002035
Iteration 118/1000 | Loss: 0.00002035
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002034
Iteration 127/1000 | Loss: 0.00002034
Iteration 128/1000 | Loss: 0.00002034
Iteration 129/1000 | Loss: 0.00002034
Iteration 130/1000 | Loss: 0.00002034
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002033
Iteration 139/1000 | Loss: 0.00002033
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002033
Iteration 145/1000 | Loss: 0.00002033
Iteration 146/1000 | Loss: 0.00002033
Iteration 147/1000 | Loss: 0.00002033
Iteration 148/1000 | Loss: 0.00002033
Iteration 149/1000 | Loss: 0.00002033
Iteration 150/1000 | Loss: 0.00002033
Iteration 151/1000 | Loss: 0.00002033
Iteration 152/1000 | Loss: 0.00002033
Iteration 153/1000 | Loss: 0.00002033
Iteration 154/1000 | Loss: 0.00002033
Iteration 155/1000 | Loss: 0.00002033
Iteration 156/1000 | Loss: 0.00002033
Iteration 157/1000 | Loss: 0.00002033
Iteration 158/1000 | Loss: 0.00002033
Iteration 159/1000 | Loss: 0.00002033
Iteration 160/1000 | Loss: 0.00002033
Iteration 161/1000 | Loss: 0.00002033
Iteration 162/1000 | Loss: 0.00002033
Iteration 163/1000 | Loss: 0.00002033
Iteration 164/1000 | Loss: 0.00002033
Iteration 165/1000 | Loss: 0.00002033
Iteration 166/1000 | Loss: 0.00002033
Iteration 167/1000 | Loss: 0.00002033
Iteration 168/1000 | Loss: 0.00002033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.0329343897174112e-05, 2.0329343897174112e-05, 2.0329343897174112e-05, 2.0329343897174112e-05, 2.0329343897174112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0329343897174112e-05

Optimization complete. Final v2v error: 3.8978474140167236 mm

Highest mean error: 4.170070171356201 mm for frame 10

Lowest mean error: 3.5782394409179688 mm for frame 152

Saving results

Total time: 31.324238538742065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061183
Iteration 2/25 | Loss: 0.01061183
Iteration 3/25 | Loss: 0.01061183
Iteration 4/25 | Loss: 0.01061183
Iteration 5/25 | Loss: 0.01061183
Iteration 6/25 | Loss: 0.01061183
Iteration 7/25 | Loss: 0.01061183
Iteration 8/25 | Loss: 0.01061183
Iteration 9/25 | Loss: 0.01061182
Iteration 10/25 | Loss: 0.01061182
Iteration 11/25 | Loss: 0.01061182
Iteration 12/25 | Loss: 0.01061182
Iteration 13/25 | Loss: 0.01061182
Iteration 14/25 | Loss: 0.01061182
Iteration 15/25 | Loss: 0.01061182
Iteration 16/25 | Loss: 0.01061182
Iteration 17/25 | Loss: 0.01061182
Iteration 18/25 | Loss: 0.01061182
Iteration 19/25 | Loss: 0.01061181
Iteration 20/25 | Loss: 0.01061181
Iteration 21/25 | Loss: 0.01061181
Iteration 22/25 | Loss: 0.01061181
Iteration 23/25 | Loss: 0.01061181
Iteration 24/25 | Loss: 0.01061181
Iteration 25/25 | Loss: 0.01061181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73977602
Iteration 2/25 | Loss: 0.05807780
Iteration 3/25 | Loss: 0.05807691
Iteration 4/25 | Loss: 0.05805035
Iteration 5/25 | Loss: 0.05805034
Iteration 6/25 | Loss: 0.05805033
Iteration 7/25 | Loss: 0.05805033
Iteration 8/25 | Loss: 0.05805033
Iteration 9/25 | Loss: 0.05805033
Iteration 10/25 | Loss: 0.05805033
Iteration 11/25 | Loss: 0.05805033
Iteration 12/25 | Loss: 0.05805033
Iteration 13/25 | Loss: 0.05805033
Iteration 14/25 | Loss: 0.05805033
Iteration 15/25 | Loss: 0.05805033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.058050330728292465, 0.058050330728292465, 0.058050330728292465, 0.058050330728292465, 0.058050330728292465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.058050330728292465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05805033
Iteration 2/1000 | Loss: 0.00361415
Iteration 3/1000 | Loss: 0.00160405
Iteration 4/1000 | Loss: 0.00050557
Iteration 5/1000 | Loss: 0.00031193
Iteration 6/1000 | Loss: 0.00040236
Iteration 7/1000 | Loss: 0.00018551
Iteration 8/1000 | Loss: 0.00007213
Iteration 9/1000 | Loss: 0.00040261
Iteration 10/1000 | Loss: 0.00039964
Iteration 11/1000 | Loss: 0.00002860
Iteration 12/1000 | Loss: 0.00020554
Iteration 13/1000 | Loss: 0.00062398
Iteration 14/1000 | Loss: 0.00203093
Iteration 15/1000 | Loss: 0.00016956
Iteration 16/1000 | Loss: 0.00056155
Iteration 17/1000 | Loss: 0.00026253
Iteration 18/1000 | Loss: 0.00004795
Iteration 19/1000 | Loss: 0.00002720
Iteration 20/1000 | Loss: 0.00018976
Iteration 21/1000 | Loss: 0.00002036
Iteration 22/1000 | Loss: 0.00006642
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00011052
Iteration 25/1000 | Loss: 0.00003458
Iteration 26/1000 | Loss: 0.00003820
Iteration 27/1000 | Loss: 0.00006094
Iteration 28/1000 | Loss: 0.00017289
Iteration 29/1000 | Loss: 0.00002069
Iteration 30/1000 | Loss: 0.00002998
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00004096
Iteration 33/1000 | Loss: 0.00036484
Iteration 34/1000 | Loss: 0.00007320
Iteration 35/1000 | Loss: 0.00006489
Iteration 36/1000 | Loss: 0.00001621
Iteration 37/1000 | Loss: 0.00001240
Iteration 38/1000 | Loss: 0.00003296
Iteration 39/1000 | Loss: 0.00003317
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00016991
Iteration 43/1000 | Loss: 0.00006912
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001083
Iteration 47/1000 | Loss: 0.00001069
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001062
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001059
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001055
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001054
Iteration 57/1000 | Loss: 0.00001051
Iteration 58/1000 | Loss: 0.00001051
Iteration 59/1000 | Loss: 0.00001051
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00022203
Iteration 62/1000 | Loss: 0.00007971
Iteration 63/1000 | Loss: 0.00006398
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00004182
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001027
Iteration 73/1000 | Loss: 0.00001027
Iteration 74/1000 | Loss: 0.00001027
Iteration 75/1000 | Loss: 0.00001027
Iteration 76/1000 | Loss: 0.00001027
Iteration 77/1000 | Loss: 0.00001026
Iteration 78/1000 | Loss: 0.00001026
Iteration 79/1000 | Loss: 0.00001026
Iteration 80/1000 | Loss: 0.00001026
Iteration 81/1000 | Loss: 0.00001026
Iteration 82/1000 | Loss: 0.00001026
Iteration 83/1000 | Loss: 0.00001026
Iteration 84/1000 | Loss: 0.00001026
Iteration 85/1000 | Loss: 0.00001026
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001025
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001024
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001023
Iteration 96/1000 | Loss: 0.00001023
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001023
Iteration 99/1000 | Loss: 0.00001022
Iteration 100/1000 | Loss: 0.00001022
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001020
Iteration 103/1000 | Loss: 0.00001020
Iteration 104/1000 | Loss: 0.00001020
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001019
Iteration 108/1000 | Loss: 0.00001019
Iteration 109/1000 | Loss: 0.00001019
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001018
Iteration 112/1000 | Loss: 0.00001018
Iteration 113/1000 | Loss: 0.00001018
Iteration 114/1000 | Loss: 0.00001018
Iteration 115/1000 | Loss: 0.00001018
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001018
Iteration 120/1000 | Loss: 0.00001018
Iteration 121/1000 | Loss: 0.00001018
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001017
Iteration 124/1000 | Loss: 0.00001017
Iteration 125/1000 | Loss: 0.00001017
Iteration 126/1000 | Loss: 0.00001017
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001016
Iteration 132/1000 | Loss: 0.00001016
Iteration 133/1000 | Loss: 0.00001016
Iteration 134/1000 | Loss: 0.00001016
Iteration 135/1000 | Loss: 0.00001016
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001016
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001016
Iteration 140/1000 | Loss: 0.00001016
Iteration 141/1000 | Loss: 0.00001016
Iteration 142/1000 | Loss: 0.00001016
Iteration 143/1000 | Loss: 0.00001016
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001015
Iteration 147/1000 | Loss: 0.00003247
Iteration 148/1000 | Loss: 0.00011081
Iteration 149/1000 | Loss: 0.00002016
Iteration 150/1000 | Loss: 0.00001042
Iteration 151/1000 | Loss: 0.00006866
Iteration 152/1000 | Loss: 0.00003617
Iteration 153/1000 | Loss: 0.00001824
Iteration 154/1000 | Loss: 0.00001031
Iteration 155/1000 | Loss: 0.00002533
Iteration 156/1000 | Loss: 0.00002658
Iteration 157/1000 | Loss: 0.00004433
Iteration 158/1000 | Loss: 0.00004032
Iteration 159/1000 | Loss: 0.00001018
Iteration 160/1000 | Loss: 0.00001015
Iteration 161/1000 | Loss: 0.00001014
Iteration 162/1000 | Loss: 0.00001014
Iteration 163/1000 | Loss: 0.00001013
Iteration 164/1000 | Loss: 0.00001013
Iteration 165/1000 | Loss: 0.00001013
Iteration 166/1000 | Loss: 0.00001013
Iteration 167/1000 | Loss: 0.00001013
Iteration 168/1000 | Loss: 0.00001013
Iteration 169/1000 | Loss: 0.00001013
Iteration 170/1000 | Loss: 0.00001013
Iteration 171/1000 | Loss: 0.00001013
Iteration 172/1000 | Loss: 0.00001013
Iteration 173/1000 | Loss: 0.00001012
Iteration 174/1000 | Loss: 0.00001012
Iteration 175/1000 | Loss: 0.00001012
Iteration 176/1000 | Loss: 0.00001012
Iteration 177/1000 | Loss: 0.00001012
Iteration 178/1000 | Loss: 0.00001012
Iteration 179/1000 | Loss: 0.00001012
Iteration 180/1000 | Loss: 0.00001012
Iteration 181/1000 | Loss: 0.00001012
Iteration 182/1000 | Loss: 0.00001012
Iteration 183/1000 | Loss: 0.00001012
Iteration 184/1000 | Loss: 0.00001011
Iteration 185/1000 | Loss: 0.00001011
Iteration 186/1000 | Loss: 0.00001011
Iteration 187/1000 | Loss: 0.00001011
Iteration 188/1000 | Loss: 0.00001010
Iteration 189/1000 | Loss: 0.00001010
Iteration 190/1000 | Loss: 0.00001010
Iteration 191/1000 | Loss: 0.00001010
Iteration 192/1000 | Loss: 0.00001274
Iteration 193/1000 | Loss: 0.00001017
Iteration 194/1000 | Loss: 0.00001013
Iteration 195/1000 | Loss: 0.00001013
Iteration 196/1000 | Loss: 0.00001013
Iteration 197/1000 | Loss: 0.00001013
Iteration 198/1000 | Loss: 0.00001013
Iteration 199/1000 | Loss: 0.00001013
Iteration 200/1000 | Loss: 0.00001013
Iteration 201/1000 | Loss: 0.00001012
Iteration 202/1000 | Loss: 0.00001012
Iteration 203/1000 | Loss: 0.00001012
Iteration 204/1000 | Loss: 0.00001012
Iteration 205/1000 | Loss: 0.00001012
Iteration 206/1000 | Loss: 0.00001011
Iteration 207/1000 | Loss: 0.00001011
Iteration 208/1000 | Loss: 0.00004082
Iteration 209/1000 | Loss: 0.00004082
Iteration 210/1000 | Loss: 0.00022451
Iteration 211/1000 | Loss: 0.00007682
Iteration 212/1000 | Loss: 0.00009356
Iteration 213/1000 | Loss: 0.00002355
Iteration 214/1000 | Loss: 0.00002768
Iteration 215/1000 | Loss: 0.00001047
Iteration 216/1000 | Loss: 0.00001121
Iteration 217/1000 | Loss: 0.00001497
Iteration 218/1000 | Loss: 0.00001010
Iteration 219/1000 | Loss: 0.00001010
Iteration 220/1000 | Loss: 0.00001010
Iteration 221/1000 | Loss: 0.00001009
Iteration 222/1000 | Loss: 0.00001009
Iteration 223/1000 | Loss: 0.00001009
Iteration 224/1000 | Loss: 0.00001009
Iteration 225/1000 | Loss: 0.00001009
Iteration 226/1000 | Loss: 0.00001008
Iteration 227/1000 | Loss: 0.00001008
Iteration 228/1000 | Loss: 0.00002747
Iteration 229/1000 | Loss: 0.00001300
Iteration 230/1000 | Loss: 0.00001015
Iteration 231/1000 | Loss: 0.00001010
Iteration 232/1000 | Loss: 0.00001010
Iteration 233/1000 | Loss: 0.00001010
Iteration 234/1000 | Loss: 0.00001009
Iteration 235/1000 | Loss: 0.00001009
Iteration 236/1000 | Loss: 0.00001009
Iteration 237/1000 | Loss: 0.00001009
Iteration 238/1000 | Loss: 0.00001009
Iteration 239/1000 | Loss: 0.00001009
Iteration 240/1000 | Loss: 0.00001009
Iteration 241/1000 | Loss: 0.00001009
Iteration 242/1000 | Loss: 0.00001009
Iteration 243/1000 | Loss: 0.00001009
Iteration 244/1000 | Loss: 0.00001009
Iteration 245/1000 | Loss: 0.00001009
Iteration 246/1000 | Loss: 0.00001009
Iteration 247/1000 | Loss: 0.00001009
Iteration 248/1000 | Loss: 0.00001009
Iteration 249/1000 | Loss: 0.00001009
Iteration 250/1000 | Loss: 0.00001009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.0091325748362578e-05, 1.0091325748362578e-05, 1.0091325748362578e-05, 1.0091325748362578e-05, 1.0091325748362578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0091325748362578e-05

Optimization complete. Final v2v error: 2.72833514213562 mm

Highest mean error: 3.8416786193847656 mm for frame 198

Lowest mean error: 2.401024580001831 mm for frame 222

Saving results

Total time: 143.77809286117554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016686
Iteration 2/25 | Loss: 0.00428131
Iteration 3/25 | Loss: 0.00250734
Iteration 4/25 | Loss: 0.00201269
Iteration 5/25 | Loss: 0.00165019
Iteration 6/25 | Loss: 0.00153559
Iteration 7/25 | Loss: 0.00152985
Iteration 8/25 | Loss: 0.00144811
Iteration 9/25 | Loss: 0.00136582
Iteration 10/25 | Loss: 0.00138672
Iteration 11/25 | Loss: 0.00124181
Iteration 12/25 | Loss: 0.00126601
Iteration 13/25 | Loss: 0.00122354
Iteration 14/25 | Loss: 0.00120150
Iteration 15/25 | Loss: 0.00118825
Iteration 16/25 | Loss: 0.00118268
Iteration 17/25 | Loss: 0.00118084
Iteration 18/25 | Loss: 0.00118074
Iteration 19/25 | Loss: 0.00117395
Iteration 20/25 | Loss: 0.00117545
Iteration 21/25 | Loss: 0.00116710
Iteration 22/25 | Loss: 0.00116930
Iteration 23/25 | Loss: 0.00116403
Iteration 24/25 | Loss: 0.00116448
Iteration 25/25 | Loss: 0.00115955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44983697
Iteration 2/25 | Loss: 0.00128947
Iteration 3/25 | Loss: 0.00128947
Iteration 4/25 | Loss: 0.00128947
Iteration 5/25 | Loss: 0.00112651
Iteration 6/25 | Loss: 0.00109878
Iteration 7/25 | Loss: 0.00107380
Iteration 8/25 | Loss: 0.00104247
Iteration 9/25 | Loss: 0.00104229
Iteration 10/25 | Loss: 0.00104239
Iteration 11/25 | Loss: 0.00104226
Iteration 12/25 | Loss: 0.00104225
Iteration 13/25 | Loss: 0.00103747
Iteration 14/25 | Loss: 0.00103747
Iteration 15/25 | Loss: 0.00103747
Iteration 16/25 | Loss: 0.00103747
Iteration 17/25 | Loss: 0.00103747
Iteration 18/25 | Loss: 0.00103747
Iteration 19/25 | Loss: 0.00103747
Iteration 20/25 | Loss: 0.00103747
Iteration 21/25 | Loss: 0.00103747
Iteration 22/25 | Loss: 0.00103747
Iteration 23/25 | Loss: 0.00103747
Iteration 24/25 | Loss: 0.00103747
Iteration 25/25 | Loss: 0.00103747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103747
Iteration 2/1000 | Loss: 0.00060424
Iteration 3/1000 | Loss: 0.00091715
Iteration 4/1000 | Loss: 0.00050375
Iteration 5/1000 | Loss: 0.00023552
Iteration 6/1000 | Loss: 0.00134787
Iteration 7/1000 | Loss: 0.00012811
Iteration 8/1000 | Loss: 0.00014402
Iteration 9/1000 | Loss: 0.00052490
Iteration 10/1000 | Loss: 0.00162530
Iteration 11/1000 | Loss: 0.00108962
Iteration 12/1000 | Loss: 0.00031512
Iteration 13/1000 | Loss: 0.00029527
Iteration 14/1000 | Loss: 0.00022072
Iteration 15/1000 | Loss: 0.00010625
Iteration 16/1000 | Loss: 0.00040522
Iteration 17/1000 | Loss: 0.00006240
Iteration 18/1000 | Loss: 0.00042491
Iteration 19/1000 | Loss: 0.00026682
Iteration 20/1000 | Loss: 0.00040638
Iteration 21/1000 | Loss: 0.00005543
Iteration 22/1000 | Loss: 0.00022771
Iteration 23/1000 | Loss: 0.00048654
Iteration 24/1000 | Loss: 0.00122971
Iteration 25/1000 | Loss: 0.00028351
Iteration 26/1000 | Loss: 0.00015958
Iteration 27/1000 | Loss: 0.00011158
Iteration 28/1000 | Loss: 0.00005064
Iteration 29/1000 | Loss: 0.00007089
Iteration 30/1000 | Loss: 0.00004723
Iteration 31/1000 | Loss: 0.00004560
Iteration 32/1000 | Loss: 0.00019038
Iteration 33/1000 | Loss: 0.00099809
Iteration 34/1000 | Loss: 0.00129993
Iteration 35/1000 | Loss: 0.00127030
Iteration 36/1000 | Loss: 0.00093222
Iteration 37/1000 | Loss: 0.00052784
Iteration 38/1000 | Loss: 0.00053181
Iteration 39/1000 | Loss: 0.00040674
Iteration 40/1000 | Loss: 0.00004955
Iteration 41/1000 | Loss: 0.00009552
Iteration 42/1000 | Loss: 0.00005753
Iteration 43/1000 | Loss: 0.00004207
Iteration 44/1000 | Loss: 0.00029576
Iteration 45/1000 | Loss: 0.00044019
Iteration 46/1000 | Loss: 0.00032870
Iteration 47/1000 | Loss: 0.00009234
Iteration 48/1000 | Loss: 0.00004455
Iteration 49/1000 | Loss: 0.00019464
Iteration 50/1000 | Loss: 0.00029576
Iteration 51/1000 | Loss: 0.00004638
Iteration 52/1000 | Loss: 0.00014487
Iteration 53/1000 | Loss: 0.00003723
Iteration 54/1000 | Loss: 0.00019564
Iteration 55/1000 | Loss: 0.00046812
Iteration 56/1000 | Loss: 0.00026520
Iteration 57/1000 | Loss: 0.00004168
Iteration 58/1000 | Loss: 0.00004087
Iteration 59/1000 | Loss: 0.00019846
Iteration 60/1000 | Loss: 0.00004351
Iteration 61/1000 | Loss: 0.00022167
Iteration 62/1000 | Loss: 0.00007796
Iteration 63/1000 | Loss: 0.00041411
Iteration 64/1000 | Loss: 0.00054124
Iteration 65/1000 | Loss: 0.00008263
Iteration 66/1000 | Loss: 0.00017643
Iteration 67/1000 | Loss: 0.00004926
Iteration 68/1000 | Loss: 0.00003393
Iteration 69/1000 | Loss: 0.00003054
Iteration 70/1000 | Loss: 0.00016206
Iteration 71/1000 | Loss: 0.00011230
Iteration 72/1000 | Loss: 0.00003123
Iteration 73/1000 | Loss: 0.00021245
Iteration 74/1000 | Loss: 0.00034980
Iteration 75/1000 | Loss: 0.00033578
Iteration 76/1000 | Loss: 0.00008223
Iteration 77/1000 | Loss: 0.00023732
Iteration 78/1000 | Loss: 0.00008136
Iteration 79/1000 | Loss: 0.00019817
Iteration 80/1000 | Loss: 0.00003288
Iteration 81/1000 | Loss: 0.00003125
Iteration 82/1000 | Loss: 0.00024122
Iteration 83/1000 | Loss: 0.00003837
Iteration 84/1000 | Loss: 0.00006009
Iteration 85/1000 | Loss: 0.00002821
Iteration 86/1000 | Loss: 0.00017737
Iteration 87/1000 | Loss: 0.00003099
Iteration 88/1000 | Loss: 0.00002805
Iteration 89/1000 | Loss: 0.00002738
Iteration 90/1000 | Loss: 0.00002732
Iteration 91/1000 | Loss: 0.00002731
Iteration 92/1000 | Loss: 0.00009003
Iteration 93/1000 | Loss: 0.00002887
Iteration 94/1000 | Loss: 0.00007493
Iteration 95/1000 | Loss: 0.00002711
Iteration 96/1000 | Loss: 0.00002693
Iteration 97/1000 | Loss: 0.00008665
Iteration 98/1000 | Loss: 0.00002837
Iteration 99/1000 | Loss: 0.00002694
Iteration 100/1000 | Loss: 0.00002694
Iteration 101/1000 | Loss: 0.00002693
Iteration 102/1000 | Loss: 0.00002692
Iteration 103/1000 | Loss: 0.00005642
Iteration 104/1000 | Loss: 0.00003025
Iteration 105/1000 | Loss: 0.00002788
Iteration 106/1000 | Loss: 0.00016533
Iteration 107/1000 | Loss: 0.00006801
Iteration 108/1000 | Loss: 0.00004116
Iteration 109/1000 | Loss: 0.00013353
Iteration 110/1000 | Loss: 0.00002695
Iteration 111/1000 | Loss: 0.00005623
Iteration 112/1000 | Loss: 0.00013582
Iteration 113/1000 | Loss: 0.00006453
Iteration 114/1000 | Loss: 0.00012142
Iteration 115/1000 | Loss: 0.00093001
Iteration 116/1000 | Loss: 0.00264878
Iteration 117/1000 | Loss: 0.00012196
Iteration 118/1000 | Loss: 0.00066396
Iteration 119/1000 | Loss: 0.00005675
Iteration 120/1000 | Loss: 0.00003597
Iteration 121/1000 | Loss: 0.00002895
Iteration 122/1000 | Loss: 0.00002681
Iteration 123/1000 | Loss: 0.00013280
Iteration 124/1000 | Loss: 0.00010921
Iteration 125/1000 | Loss: 0.00002501
Iteration 126/1000 | Loss: 0.00002447
Iteration 127/1000 | Loss: 0.00002400
Iteration 128/1000 | Loss: 0.00002368
Iteration 129/1000 | Loss: 0.00002365
Iteration 130/1000 | Loss: 0.00037580
Iteration 131/1000 | Loss: 0.00005070
Iteration 132/1000 | Loss: 0.00004673
Iteration 133/1000 | Loss: 0.00005577
Iteration 134/1000 | Loss: 0.00002353
Iteration 135/1000 | Loss: 0.00002334
Iteration 136/1000 | Loss: 0.00002327
Iteration 137/1000 | Loss: 0.00002319
Iteration 138/1000 | Loss: 0.00002316
Iteration 139/1000 | Loss: 0.00002315
Iteration 140/1000 | Loss: 0.00002315
Iteration 141/1000 | Loss: 0.00002315
Iteration 142/1000 | Loss: 0.00002315
Iteration 143/1000 | Loss: 0.00002315
Iteration 144/1000 | Loss: 0.00002314
Iteration 145/1000 | Loss: 0.00002313
Iteration 146/1000 | Loss: 0.00002312
Iteration 147/1000 | Loss: 0.00002312
Iteration 148/1000 | Loss: 0.00002312
Iteration 149/1000 | Loss: 0.00002312
Iteration 150/1000 | Loss: 0.00002312
Iteration 151/1000 | Loss: 0.00002311
Iteration 152/1000 | Loss: 0.00002311
Iteration 153/1000 | Loss: 0.00002311
Iteration 154/1000 | Loss: 0.00002311
Iteration 155/1000 | Loss: 0.00002310
Iteration 156/1000 | Loss: 0.00002310
Iteration 157/1000 | Loss: 0.00002309
Iteration 158/1000 | Loss: 0.00002309
Iteration 159/1000 | Loss: 0.00002308
Iteration 160/1000 | Loss: 0.00002308
Iteration 161/1000 | Loss: 0.00002308
Iteration 162/1000 | Loss: 0.00002308
Iteration 163/1000 | Loss: 0.00002308
Iteration 164/1000 | Loss: 0.00002308
Iteration 165/1000 | Loss: 0.00002308
Iteration 166/1000 | Loss: 0.00002308
Iteration 167/1000 | Loss: 0.00002308
Iteration 168/1000 | Loss: 0.00002308
Iteration 169/1000 | Loss: 0.00002308
Iteration 170/1000 | Loss: 0.00002308
Iteration 171/1000 | Loss: 0.00002307
Iteration 172/1000 | Loss: 0.00002307
Iteration 173/1000 | Loss: 0.00002307
Iteration 174/1000 | Loss: 0.00002307
Iteration 175/1000 | Loss: 0.00002307
Iteration 176/1000 | Loss: 0.00002307
Iteration 177/1000 | Loss: 0.00002307
Iteration 178/1000 | Loss: 0.00002307
Iteration 179/1000 | Loss: 0.00002307
Iteration 180/1000 | Loss: 0.00002307
Iteration 181/1000 | Loss: 0.00002307
Iteration 182/1000 | Loss: 0.00002307
Iteration 183/1000 | Loss: 0.00002307
Iteration 184/1000 | Loss: 0.00002307
Iteration 185/1000 | Loss: 0.00002307
Iteration 186/1000 | Loss: 0.00002307
Iteration 187/1000 | Loss: 0.00002307
Iteration 188/1000 | Loss: 0.00002307
Iteration 189/1000 | Loss: 0.00002307
Iteration 190/1000 | Loss: 0.00002307
Iteration 191/1000 | Loss: 0.00002307
Iteration 192/1000 | Loss: 0.00002307
Iteration 193/1000 | Loss: 0.00002307
Iteration 194/1000 | Loss: 0.00002307
Iteration 195/1000 | Loss: 0.00002307
Iteration 196/1000 | Loss: 0.00002307
Iteration 197/1000 | Loss: 0.00002307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.3066249923431315e-05, 2.3066249923431315e-05, 2.3066249923431315e-05, 2.3066249923431315e-05, 2.3066249923431315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3066249923431315e-05

Optimization complete. Final v2v error: 3.552971124649048 mm

Highest mean error: 11.628418922424316 mm for frame 196

Lowest mean error: 3.0335445404052734 mm for frame 136

Saving results

Total time: 268.9323432445526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388949
Iteration 2/25 | Loss: 0.00105041
Iteration 3/25 | Loss: 0.00098374
Iteration 4/25 | Loss: 0.00097357
Iteration 5/25 | Loss: 0.00097061
Iteration 6/25 | Loss: 0.00096990
Iteration 7/25 | Loss: 0.00096979
Iteration 8/25 | Loss: 0.00096979
Iteration 9/25 | Loss: 0.00096979
Iteration 10/25 | Loss: 0.00096979
Iteration 11/25 | Loss: 0.00096979
Iteration 12/25 | Loss: 0.00096979
Iteration 13/25 | Loss: 0.00096979
Iteration 14/25 | Loss: 0.00096979
Iteration 15/25 | Loss: 0.00096979
Iteration 16/25 | Loss: 0.00096979
Iteration 17/25 | Loss: 0.00096979
Iteration 18/25 | Loss: 0.00096979
Iteration 19/25 | Loss: 0.00096979
Iteration 20/25 | Loss: 0.00096979
Iteration 21/25 | Loss: 0.00096979
Iteration 22/25 | Loss: 0.00096979
Iteration 23/25 | Loss: 0.00096979
Iteration 24/25 | Loss: 0.00096979
Iteration 25/25 | Loss: 0.00096979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88568187
Iteration 2/25 | Loss: 0.00069344
Iteration 3/25 | Loss: 0.00069343
Iteration 4/25 | Loss: 0.00069343
Iteration 5/25 | Loss: 0.00069343
Iteration 6/25 | Loss: 0.00069343
Iteration 7/25 | Loss: 0.00069343
Iteration 8/25 | Loss: 0.00069343
Iteration 9/25 | Loss: 0.00069343
Iteration 10/25 | Loss: 0.00069343
Iteration 11/25 | Loss: 0.00069343
Iteration 12/25 | Loss: 0.00069343
Iteration 13/25 | Loss: 0.00069343
Iteration 14/25 | Loss: 0.00069343
Iteration 15/25 | Loss: 0.00069343
Iteration 16/25 | Loss: 0.00069343
Iteration 17/25 | Loss: 0.00069343
Iteration 18/25 | Loss: 0.00069343
Iteration 19/25 | Loss: 0.00069343
Iteration 20/25 | Loss: 0.00069343
Iteration 21/25 | Loss: 0.00069343
Iteration 22/25 | Loss: 0.00069343
Iteration 23/25 | Loss: 0.00069343
Iteration 24/25 | Loss: 0.00069343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006934314151294529, 0.0006934314151294529, 0.0006934314151294529, 0.0006934314151294529, 0.0006934314151294529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006934314151294529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069343
Iteration 2/1000 | Loss: 0.00001999
Iteration 3/1000 | Loss: 0.00001487
Iteration 4/1000 | Loss: 0.00001263
Iteration 5/1000 | Loss: 0.00001185
Iteration 6/1000 | Loss: 0.00001133
Iteration 7/1000 | Loss: 0.00001098
Iteration 8/1000 | Loss: 0.00001074
Iteration 9/1000 | Loss: 0.00001065
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001049
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001034
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001026
Iteration 17/1000 | Loss: 0.00001025
Iteration 18/1000 | Loss: 0.00001024
Iteration 19/1000 | Loss: 0.00001023
Iteration 20/1000 | Loss: 0.00001022
Iteration 21/1000 | Loss: 0.00001021
Iteration 22/1000 | Loss: 0.00001020
Iteration 23/1000 | Loss: 0.00001020
Iteration 24/1000 | Loss: 0.00001019
Iteration 25/1000 | Loss: 0.00001019
Iteration 26/1000 | Loss: 0.00001018
Iteration 27/1000 | Loss: 0.00001018
Iteration 28/1000 | Loss: 0.00001017
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001010
Iteration 34/1000 | Loss: 0.00001010
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001007
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001006
Iteration 39/1000 | Loss: 0.00001006
Iteration 40/1000 | Loss: 0.00001003
Iteration 41/1000 | Loss: 0.00001003
Iteration 42/1000 | Loss: 0.00001003
Iteration 43/1000 | Loss: 0.00001002
Iteration 44/1000 | Loss: 0.00001002
Iteration 45/1000 | Loss: 0.00001002
Iteration 46/1000 | Loss: 0.00001002
Iteration 47/1000 | Loss: 0.00001002
Iteration 48/1000 | Loss: 0.00001002
Iteration 49/1000 | Loss: 0.00001002
Iteration 50/1000 | Loss: 0.00001002
Iteration 51/1000 | Loss: 0.00001001
Iteration 52/1000 | Loss: 0.00001000
Iteration 53/1000 | Loss: 0.00001000
Iteration 54/1000 | Loss: 0.00001000
Iteration 55/1000 | Loss: 0.00001000
Iteration 56/1000 | Loss: 0.00001000
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00000999
Iteration 59/1000 | Loss: 0.00000998
Iteration 60/1000 | Loss: 0.00000998
Iteration 61/1000 | Loss: 0.00000997
Iteration 62/1000 | Loss: 0.00000996
Iteration 63/1000 | Loss: 0.00000996
Iteration 64/1000 | Loss: 0.00000995
Iteration 65/1000 | Loss: 0.00000995
Iteration 66/1000 | Loss: 0.00000995
Iteration 67/1000 | Loss: 0.00000994
Iteration 68/1000 | Loss: 0.00000994
Iteration 69/1000 | Loss: 0.00000993
Iteration 70/1000 | Loss: 0.00000993
Iteration 71/1000 | Loss: 0.00000992
Iteration 72/1000 | Loss: 0.00000992
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000991
Iteration 75/1000 | Loss: 0.00000991
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000990
Iteration 79/1000 | Loss: 0.00000990
Iteration 80/1000 | Loss: 0.00000990
Iteration 81/1000 | Loss: 0.00000989
Iteration 82/1000 | Loss: 0.00000989
Iteration 83/1000 | Loss: 0.00000989
Iteration 84/1000 | Loss: 0.00000988
Iteration 85/1000 | Loss: 0.00000988
Iteration 86/1000 | Loss: 0.00000988
Iteration 87/1000 | Loss: 0.00000988
Iteration 88/1000 | Loss: 0.00000988
Iteration 89/1000 | Loss: 0.00000987
Iteration 90/1000 | Loss: 0.00000987
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000987
Iteration 93/1000 | Loss: 0.00000987
Iteration 94/1000 | Loss: 0.00000986
Iteration 95/1000 | Loss: 0.00000986
Iteration 96/1000 | Loss: 0.00000986
Iteration 97/1000 | Loss: 0.00000986
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000985
Iteration 100/1000 | Loss: 0.00000985
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000985
Iteration 103/1000 | Loss: 0.00000985
Iteration 104/1000 | Loss: 0.00000985
Iteration 105/1000 | Loss: 0.00000985
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000984
Iteration 110/1000 | Loss: 0.00000984
Iteration 111/1000 | Loss: 0.00000983
Iteration 112/1000 | Loss: 0.00000983
Iteration 113/1000 | Loss: 0.00000983
Iteration 114/1000 | Loss: 0.00000982
Iteration 115/1000 | Loss: 0.00000982
Iteration 116/1000 | Loss: 0.00000982
Iteration 117/1000 | Loss: 0.00000982
Iteration 118/1000 | Loss: 0.00000981
Iteration 119/1000 | Loss: 0.00000981
Iteration 120/1000 | Loss: 0.00000981
Iteration 121/1000 | Loss: 0.00000980
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000979
Iteration 126/1000 | Loss: 0.00000978
Iteration 127/1000 | Loss: 0.00000978
Iteration 128/1000 | Loss: 0.00000978
Iteration 129/1000 | Loss: 0.00000978
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000977
Iteration 134/1000 | Loss: 0.00000977
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000976
Iteration 138/1000 | Loss: 0.00000976
Iteration 139/1000 | Loss: 0.00000975
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000975
Iteration 142/1000 | Loss: 0.00000975
Iteration 143/1000 | Loss: 0.00000975
Iteration 144/1000 | Loss: 0.00000975
Iteration 145/1000 | Loss: 0.00000974
Iteration 146/1000 | Loss: 0.00000974
Iteration 147/1000 | Loss: 0.00000974
Iteration 148/1000 | Loss: 0.00000974
Iteration 149/1000 | Loss: 0.00000974
Iteration 150/1000 | Loss: 0.00000973
Iteration 151/1000 | Loss: 0.00000973
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000973
Iteration 154/1000 | Loss: 0.00000973
Iteration 155/1000 | Loss: 0.00000973
Iteration 156/1000 | Loss: 0.00000972
Iteration 157/1000 | Loss: 0.00000972
Iteration 158/1000 | Loss: 0.00000972
Iteration 159/1000 | Loss: 0.00000972
Iteration 160/1000 | Loss: 0.00000972
Iteration 161/1000 | Loss: 0.00000972
Iteration 162/1000 | Loss: 0.00000972
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000972
Iteration 165/1000 | Loss: 0.00000972
Iteration 166/1000 | Loss: 0.00000972
Iteration 167/1000 | Loss: 0.00000971
Iteration 168/1000 | Loss: 0.00000971
Iteration 169/1000 | Loss: 0.00000971
Iteration 170/1000 | Loss: 0.00000971
Iteration 171/1000 | Loss: 0.00000971
Iteration 172/1000 | Loss: 0.00000971
Iteration 173/1000 | Loss: 0.00000971
Iteration 174/1000 | Loss: 0.00000970
Iteration 175/1000 | Loss: 0.00000970
Iteration 176/1000 | Loss: 0.00000970
Iteration 177/1000 | Loss: 0.00000970
Iteration 178/1000 | Loss: 0.00000970
Iteration 179/1000 | Loss: 0.00000970
Iteration 180/1000 | Loss: 0.00000970
Iteration 181/1000 | Loss: 0.00000970
Iteration 182/1000 | Loss: 0.00000970
Iteration 183/1000 | Loss: 0.00000970
Iteration 184/1000 | Loss: 0.00000970
Iteration 185/1000 | Loss: 0.00000970
Iteration 186/1000 | Loss: 0.00000970
Iteration 187/1000 | Loss: 0.00000969
Iteration 188/1000 | Loss: 0.00000969
Iteration 189/1000 | Loss: 0.00000969
Iteration 190/1000 | Loss: 0.00000969
Iteration 191/1000 | Loss: 0.00000969
Iteration 192/1000 | Loss: 0.00000969
Iteration 193/1000 | Loss: 0.00000969
Iteration 194/1000 | Loss: 0.00000968
Iteration 195/1000 | Loss: 0.00000968
Iteration 196/1000 | Loss: 0.00000968
Iteration 197/1000 | Loss: 0.00000968
Iteration 198/1000 | Loss: 0.00000968
Iteration 199/1000 | Loss: 0.00000968
Iteration 200/1000 | Loss: 0.00000968
Iteration 201/1000 | Loss: 0.00000968
Iteration 202/1000 | Loss: 0.00000968
Iteration 203/1000 | Loss: 0.00000968
Iteration 204/1000 | Loss: 0.00000967
Iteration 205/1000 | Loss: 0.00000967
Iteration 206/1000 | Loss: 0.00000967
Iteration 207/1000 | Loss: 0.00000967
Iteration 208/1000 | Loss: 0.00000967
Iteration 209/1000 | Loss: 0.00000967
Iteration 210/1000 | Loss: 0.00000967
Iteration 211/1000 | Loss: 0.00000967
Iteration 212/1000 | Loss: 0.00000967
Iteration 213/1000 | Loss: 0.00000967
Iteration 214/1000 | Loss: 0.00000967
Iteration 215/1000 | Loss: 0.00000967
Iteration 216/1000 | Loss: 0.00000967
Iteration 217/1000 | Loss: 0.00000967
Iteration 218/1000 | Loss: 0.00000967
Iteration 219/1000 | Loss: 0.00000967
Iteration 220/1000 | Loss: 0.00000967
Iteration 221/1000 | Loss: 0.00000967
Iteration 222/1000 | Loss: 0.00000966
Iteration 223/1000 | Loss: 0.00000966
Iteration 224/1000 | Loss: 0.00000966
Iteration 225/1000 | Loss: 0.00000966
Iteration 226/1000 | Loss: 0.00000966
Iteration 227/1000 | Loss: 0.00000966
Iteration 228/1000 | Loss: 0.00000966
Iteration 229/1000 | Loss: 0.00000966
Iteration 230/1000 | Loss: 0.00000966
Iteration 231/1000 | Loss: 0.00000966
Iteration 232/1000 | Loss: 0.00000966
Iteration 233/1000 | Loss: 0.00000966
Iteration 234/1000 | Loss: 0.00000966
Iteration 235/1000 | Loss: 0.00000966
Iteration 236/1000 | Loss: 0.00000966
Iteration 237/1000 | Loss: 0.00000966
Iteration 238/1000 | Loss: 0.00000966
Iteration 239/1000 | Loss: 0.00000966
Iteration 240/1000 | Loss: 0.00000966
Iteration 241/1000 | Loss: 0.00000966
Iteration 242/1000 | Loss: 0.00000966
Iteration 243/1000 | Loss: 0.00000966
Iteration 244/1000 | Loss: 0.00000966
Iteration 245/1000 | Loss: 0.00000966
Iteration 246/1000 | Loss: 0.00000966
Iteration 247/1000 | Loss: 0.00000966
Iteration 248/1000 | Loss: 0.00000966
Iteration 249/1000 | Loss: 0.00000966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [9.663679520599544e-06, 9.663679520599544e-06, 9.663679520599544e-06, 9.663679520599544e-06, 9.663679520599544e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.663679520599544e-06

Optimization complete. Final v2v error: 2.659883975982666 mm

Highest mean error: 3.2104427814483643 mm for frame 74

Lowest mean error: 2.371011257171631 mm for frame 95

Saving results

Total time: 40.58732199668884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420812
Iteration 2/25 | Loss: 0.00111359
Iteration 3/25 | Loss: 0.00103432
Iteration 4/25 | Loss: 0.00101352
Iteration 5/25 | Loss: 0.00100548
Iteration 6/25 | Loss: 0.00100404
Iteration 7/25 | Loss: 0.00100404
Iteration 8/25 | Loss: 0.00100404
Iteration 9/25 | Loss: 0.00100404
Iteration 10/25 | Loss: 0.00100404
Iteration 11/25 | Loss: 0.00100404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010040361667051911, 0.0010040361667051911, 0.0010040361667051911, 0.0010040361667051911, 0.0010040361667051911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010040361667051911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35632527
Iteration 2/25 | Loss: 0.00097270
Iteration 3/25 | Loss: 0.00097270
Iteration 4/25 | Loss: 0.00097270
Iteration 5/25 | Loss: 0.00097270
Iteration 6/25 | Loss: 0.00097270
Iteration 7/25 | Loss: 0.00097270
Iteration 8/25 | Loss: 0.00097270
Iteration 9/25 | Loss: 0.00097270
Iteration 10/25 | Loss: 0.00097270
Iteration 11/25 | Loss: 0.00097270
Iteration 12/25 | Loss: 0.00097270
Iteration 13/25 | Loss: 0.00097270
Iteration 14/25 | Loss: 0.00097270
Iteration 15/25 | Loss: 0.00097270
Iteration 16/25 | Loss: 0.00097270
Iteration 17/25 | Loss: 0.00097270
Iteration 18/25 | Loss: 0.00097270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009726996067911386, 0.0009726996067911386, 0.0009726996067911386, 0.0009726996067911386, 0.0009726996067911386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009726996067911386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097270
Iteration 2/1000 | Loss: 0.00004636
Iteration 3/1000 | Loss: 0.00003158
Iteration 4/1000 | Loss: 0.00002474
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002160
Iteration 7/1000 | Loss: 0.00002053
Iteration 8/1000 | Loss: 0.00001993
Iteration 9/1000 | Loss: 0.00001956
Iteration 10/1000 | Loss: 0.00001924
Iteration 11/1000 | Loss: 0.00001897
Iteration 12/1000 | Loss: 0.00001878
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001858
Iteration 15/1000 | Loss: 0.00001858
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001850
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001844
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001842
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001840
Iteration 33/1000 | Loss: 0.00001840
Iteration 34/1000 | Loss: 0.00001839
Iteration 35/1000 | Loss: 0.00001839
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001838
Iteration 38/1000 | Loss: 0.00001838
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001833
Iteration 49/1000 | Loss: 0.00001833
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001832
Iteration 52/1000 | Loss: 0.00001832
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001829
Iteration 57/1000 | Loss: 0.00001829
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001824
Iteration 60/1000 | Loss: 0.00001824
Iteration 61/1000 | Loss: 0.00001824
Iteration 62/1000 | Loss: 0.00001823
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001821
Iteration 65/1000 | Loss: 0.00001821
Iteration 66/1000 | Loss: 0.00001821
Iteration 67/1000 | Loss: 0.00001820
Iteration 68/1000 | Loss: 0.00001820
Iteration 69/1000 | Loss: 0.00001820
Iteration 70/1000 | Loss: 0.00001820
Iteration 71/1000 | Loss: 0.00001819
Iteration 72/1000 | Loss: 0.00001819
Iteration 73/1000 | Loss: 0.00001819
Iteration 74/1000 | Loss: 0.00001819
Iteration 75/1000 | Loss: 0.00001819
Iteration 76/1000 | Loss: 0.00001818
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001818
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001815
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001814
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001814
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001813
Iteration 94/1000 | Loss: 0.00001813
Iteration 95/1000 | Loss: 0.00001813
Iteration 96/1000 | Loss: 0.00001812
Iteration 97/1000 | Loss: 0.00001812
Iteration 98/1000 | Loss: 0.00001812
Iteration 99/1000 | Loss: 0.00001812
Iteration 100/1000 | Loss: 0.00001811
Iteration 101/1000 | Loss: 0.00001811
Iteration 102/1000 | Loss: 0.00001810
Iteration 103/1000 | Loss: 0.00001810
Iteration 104/1000 | Loss: 0.00001810
Iteration 105/1000 | Loss: 0.00001810
Iteration 106/1000 | Loss: 0.00001809
Iteration 107/1000 | Loss: 0.00001809
Iteration 108/1000 | Loss: 0.00001809
Iteration 109/1000 | Loss: 0.00001809
Iteration 110/1000 | Loss: 0.00001809
Iteration 111/1000 | Loss: 0.00001809
Iteration 112/1000 | Loss: 0.00001808
Iteration 113/1000 | Loss: 0.00001808
Iteration 114/1000 | Loss: 0.00001808
Iteration 115/1000 | Loss: 0.00001808
Iteration 116/1000 | Loss: 0.00001808
Iteration 117/1000 | Loss: 0.00001808
Iteration 118/1000 | Loss: 0.00001808
Iteration 119/1000 | Loss: 0.00001808
Iteration 120/1000 | Loss: 0.00001808
Iteration 121/1000 | Loss: 0.00001808
Iteration 122/1000 | Loss: 0.00001808
Iteration 123/1000 | Loss: 0.00001808
Iteration 124/1000 | Loss: 0.00001808
Iteration 125/1000 | Loss: 0.00001808
Iteration 126/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.808344131859485e-05, 1.808344131859485e-05, 1.808344131859485e-05, 1.808344131859485e-05, 1.808344131859485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.808344131859485e-05

Optimization complete. Final v2v error: 3.375720739364624 mm

Highest mean error: 3.973905086517334 mm for frame 199

Lowest mean error: 2.6136887073516846 mm for frame 133

Saving results

Total time: 41.91146469116211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713208
Iteration 2/25 | Loss: 0.00227650
Iteration 3/25 | Loss: 0.00149105
Iteration 4/25 | Loss: 0.00123919
Iteration 5/25 | Loss: 0.00117149
Iteration 6/25 | Loss: 0.00116035
Iteration 7/25 | Loss: 0.00108047
Iteration 8/25 | Loss: 0.00102724
Iteration 9/25 | Loss: 0.00099969
Iteration 10/25 | Loss: 0.00097760
Iteration 11/25 | Loss: 0.00096914
Iteration 12/25 | Loss: 0.00096729
Iteration 13/25 | Loss: 0.00096686
Iteration 14/25 | Loss: 0.00096666
Iteration 15/25 | Loss: 0.00096661
Iteration 16/25 | Loss: 0.00096661
Iteration 17/25 | Loss: 0.00096661
Iteration 18/25 | Loss: 0.00096660
Iteration 19/25 | Loss: 0.00096660
Iteration 20/25 | Loss: 0.00096660
Iteration 21/25 | Loss: 0.00096660
Iteration 22/25 | Loss: 0.00096660
Iteration 23/25 | Loss: 0.00096660
Iteration 24/25 | Loss: 0.00096660
Iteration 25/25 | Loss: 0.00096660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70291197
Iteration 2/25 | Loss: 0.00075305
Iteration 3/25 | Loss: 0.00075305
Iteration 4/25 | Loss: 0.00075305
Iteration 5/25 | Loss: 0.00075305
Iteration 6/25 | Loss: 0.00075305
Iteration 7/25 | Loss: 0.00075305
Iteration 8/25 | Loss: 0.00075305
Iteration 9/25 | Loss: 0.00075304
Iteration 10/25 | Loss: 0.00075304
Iteration 11/25 | Loss: 0.00075304
Iteration 12/25 | Loss: 0.00075304
Iteration 13/25 | Loss: 0.00075304
Iteration 14/25 | Loss: 0.00075304
Iteration 15/25 | Loss: 0.00075304
Iteration 16/25 | Loss: 0.00075304
Iteration 17/25 | Loss: 0.00075304
Iteration 18/25 | Loss: 0.00075304
Iteration 19/25 | Loss: 0.00075304
Iteration 20/25 | Loss: 0.00075304
Iteration 21/25 | Loss: 0.00075304
Iteration 22/25 | Loss: 0.00075304
Iteration 23/25 | Loss: 0.00075304
Iteration 24/25 | Loss: 0.00075304
Iteration 25/25 | Loss: 0.00075304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075304
Iteration 2/1000 | Loss: 0.00001791
Iteration 3/1000 | Loss: 0.00003734
Iteration 4/1000 | Loss: 0.00004545
Iteration 5/1000 | Loss: 0.00001210
Iteration 6/1000 | Loss: 0.00001158
Iteration 7/1000 | Loss: 0.00001112
Iteration 8/1000 | Loss: 0.00001088
Iteration 9/1000 | Loss: 0.00002622
Iteration 10/1000 | Loss: 0.00001057
Iteration 11/1000 | Loss: 0.00001052
Iteration 12/1000 | Loss: 0.00001052
Iteration 13/1000 | Loss: 0.00001042
Iteration 14/1000 | Loss: 0.00046642
Iteration 15/1000 | Loss: 0.00001493
Iteration 16/1000 | Loss: 0.00002910
Iteration 17/1000 | Loss: 0.00001019
Iteration 18/1000 | Loss: 0.00000965
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00000894
Iteration 21/1000 | Loss: 0.00000875
Iteration 22/1000 | Loss: 0.00000873
Iteration 23/1000 | Loss: 0.00000873
Iteration 24/1000 | Loss: 0.00000873
Iteration 25/1000 | Loss: 0.00000872
Iteration 26/1000 | Loss: 0.00000872
Iteration 27/1000 | Loss: 0.00000871
Iteration 28/1000 | Loss: 0.00000870
Iteration 29/1000 | Loss: 0.00000870
Iteration 30/1000 | Loss: 0.00000869
Iteration 31/1000 | Loss: 0.00000867
Iteration 32/1000 | Loss: 0.00000866
Iteration 33/1000 | Loss: 0.00000866
Iteration 34/1000 | Loss: 0.00000866
Iteration 35/1000 | Loss: 0.00000865
Iteration 36/1000 | Loss: 0.00000865
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000863
Iteration 39/1000 | Loss: 0.00000863
Iteration 40/1000 | Loss: 0.00000862
Iteration 41/1000 | Loss: 0.00000862
Iteration 42/1000 | Loss: 0.00000862
Iteration 43/1000 | Loss: 0.00000861
Iteration 44/1000 | Loss: 0.00000861
Iteration 45/1000 | Loss: 0.00000860
Iteration 46/1000 | Loss: 0.00000860
Iteration 47/1000 | Loss: 0.00000860
Iteration 48/1000 | Loss: 0.00000859
Iteration 49/1000 | Loss: 0.00000859
Iteration 50/1000 | Loss: 0.00000856
Iteration 51/1000 | Loss: 0.00000856
Iteration 52/1000 | Loss: 0.00000856
Iteration 53/1000 | Loss: 0.00000856
Iteration 54/1000 | Loss: 0.00000856
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000852
Iteration 57/1000 | Loss: 0.00000851
Iteration 58/1000 | Loss: 0.00000851
Iteration 59/1000 | Loss: 0.00000850
Iteration 60/1000 | Loss: 0.00000850
Iteration 61/1000 | Loss: 0.00000849
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002141
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00000840
Iteration 66/1000 | Loss: 0.00000834
Iteration 67/1000 | Loss: 0.00000834
Iteration 68/1000 | Loss: 0.00000834
Iteration 69/1000 | Loss: 0.00000834
Iteration 70/1000 | Loss: 0.00000834
Iteration 71/1000 | Loss: 0.00000834
Iteration 72/1000 | Loss: 0.00000833
Iteration 73/1000 | Loss: 0.00000833
Iteration 74/1000 | Loss: 0.00000833
Iteration 75/1000 | Loss: 0.00000833
Iteration 76/1000 | Loss: 0.00000833
Iteration 77/1000 | Loss: 0.00000833
Iteration 78/1000 | Loss: 0.00000833
Iteration 79/1000 | Loss: 0.00000833
Iteration 80/1000 | Loss: 0.00000833
Iteration 81/1000 | Loss: 0.00000833
Iteration 82/1000 | Loss: 0.00000833
Iteration 83/1000 | Loss: 0.00000832
Iteration 84/1000 | Loss: 0.00000832
Iteration 85/1000 | Loss: 0.00000832
Iteration 86/1000 | Loss: 0.00000832
Iteration 87/1000 | Loss: 0.00000832
Iteration 88/1000 | Loss: 0.00000832
Iteration 89/1000 | Loss: 0.00000832
Iteration 90/1000 | Loss: 0.00000832
Iteration 91/1000 | Loss: 0.00000832
Iteration 92/1000 | Loss: 0.00000831
Iteration 93/1000 | Loss: 0.00000831
Iteration 94/1000 | Loss: 0.00000831
Iteration 95/1000 | Loss: 0.00000831
Iteration 96/1000 | Loss: 0.00000831
Iteration 97/1000 | Loss: 0.00000831
Iteration 98/1000 | Loss: 0.00000831
Iteration 99/1000 | Loss: 0.00000831
Iteration 100/1000 | Loss: 0.00000831
Iteration 101/1000 | Loss: 0.00000831
Iteration 102/1000 | Loss: 0.00000831
Iteration 103/1000 | Loss: 0.00000831
Iteration 104/1000 | Loss: 0.00000831
Iteration 105/1000 | Loss: 0.00000831
Iteration 106/1000 | Loss: 0.00000830
Iteration 107/1000 | Loss: 0.00000830
Iteration 108/1000 | Loss: 0.00000830
Iteration 109/1000 | Loss: 0.00000830
Iteration 110/1000 | Loss: 0.00000830
Iteration 111/1000 | Loss: 0.00000830
Iteration 112/1000 | Loss: 0.00000830
Iteration 113/1000 | Loss: 0.00000830
Iteration 114/1000 | Loss: 0.00000830
Iteration 115/1000 | Loss: 0.00000830
Iteration 116/1000 | Loss: 0.00000830
Iteration 117/1000 | Loss: 0.00000830
Iteration 118/1000 | Loss: 0.00000830
Iteration 119/1000 | Loss: 0.00000830
Iteration 120/1000 | Loss: 0.00000830
Iteration 121/1000 | Loss: 0.00000829
Iteration 122/1000 | Loss: 0.00000829
Iteration 123/1000 | Loss: 0.00000829
Iteration 124/1000 | Loss: 0.00000829
Iteration 125/1000 | Loss: 0.00000829
Iteration 126/1000 | Loss: 0.00000829
Iteration 127/1000 | Loss: 0.00000828
Iteration 128/1000 | Loss: 0.00000828
Iteration 129/1000 | Loss: 0.00000828
Iteration 130/1000 | Loss: 0.00000828
Iteration 131/1000 | Loss: 0.00000828
Iteration 132/1000 | Loss: 0.00000828
Iteration 133/1000 | Loss: 0.00000828
Iteration 134/1000 | Loss: 0.00000828
Iteration 135/1000 | Loss: 0.00000828
Iteration 136/1000 | Loss: 0.00000828
Iteration 137/1000 | Loss: 0.00000827
Iteration 138/1000 | Loss: 0.00000827
Iteration 139/1000 | Loss: 0.00000827
Iteration 140/1000 | Loss: 0.00000827
Iteration 141/1000 | Loss: 0.00000827
Iteration 142/1000 | Loss: 0.00000827
Iteration 143/1000 | Loss: 0.00000827
Iteration 144/1000 | Loss: 0.00000827
Iteration 145/1000 | Loss: 0.00000827
Iteration 146/1000 | Loss: 0.00000827
Iteration 147/1000 | Loss: 0.00000826
Iteration 148/1000 | Loss: 0.00000826
Iteration 149/1000 | Loss: 0.00000826
Iteration 150/1000 | Loss: 0.00000826
Iteration 151/1000 | Loss: 0.00000826
Iteration 152/1000 | Loss: 0.00000826
Iteration 153/1000 | Loss: 0.00000826
Iteration 154/1000 | Loss: 0.00000826
Iteration 155/1000 | Loss: 0.00000826
Iteration 156/1000 | Loss: 0.00000826
Iteration 157/1000 | Loss: 0.00000826
Iteration 158/1000 | Loss: 0.00000826
Iteration 159/1000 | Loss: 0.00000826
Iteration 160/1000 | Loss: 0.00000826
Iteration 161/1000 | Loss: 0.00000826
Iteration 162/1000 | Loss: 0.00000826
Iteration 163/1000 | Loss: 0.00000826
Iteration 164/1000 | Loss: 0.00000826
Iteration 165/1000 | Loss: 0.00000826
Iteration 166/1000 | Loss: 0.00000826
Iteration 167/1000 | Loss: 0.00000825
Iteration 168/1000 | Loss: 0.00000825
Iteration 169/1000 | Loss: 0.00000825
Iteration 170/1000 | Loss: 0.00000825
Iteration 171/1000 | Loss: 0.00000825
Iteration 172/1000 | Loss: 0.00000825
Iteration 173/1000 | Loss: 0.00000825
Iteration 174/1000 | Loss: 0.00000825
Iteration 175/1000 | Loss: 0.00000825
Iteration 176/1000 | Loss: 0.00000825
Iteration 177/1000 | Loss: 0.00000825
Iteration 178/1000 | Loss: 0.00000825
Iteration 179/1000 | Loss: 0.00000825
Iteration 180/1000 | Loss: 0.00000825
Iteration 181/1000 | Loss: 0.00000825
Iteration 182/1000 | Loss: 0.00000825
Iteration 183/1000 | Loss: 0.00000825
Iteration 184/1000 | Loss: 0.00000825
Iteration 185/1000 | Loss: 0.00000825
Iteration 186/1000 | Loss: 0.00000825
Iteration 187/1000 | Loss: 0.00000825
Iteration 188/1000 | Loss: 0.00000825
Iteration 189/1000 | Loss: 0.00000825
Iteration 190/1000 | Loss: 0.00000825
Iteration 191/1000 | Loss: 0.00000825
Iteration 192/1000 | Loss: 0.00000825
Iteration 193/1000 | Loss: 0.00000824
Iteration 194/1000 | Loss: 0.00000824
Iteration 195/1000 | Loss: 0.00000824
Iteration 196/1000 | Loss: 0.00000824
Iteration 197/1000 | Loss: 0.00000824
Iteration 198/1000 | Loss: 0.00000824
Iteration 199/1000 | Loss: 0.00000824
Iteration 200/1000 | Loss: 0.00000824
Iteration 201/1000 | Loss: 0.00000824
Iteration 202/1000 | Loss: 0.00000824
Iteration 203/1000 | Loss: 0.00000824
Iteration 204/1000 | Loss: 0.00000824
Iteration 205/1000 | Loss: 0.00000824
Iteration 206/1000 | Loss: 0.00000824
Iteration 207/1000 | Loss: 0.00000824
Iteration 208/1000 | Loss: 0.00000824
Iteration 209/1000 | Loss: 0.00000824
Iteration 210/1000 | Loss: 0.00000824
Iteration 211/1000 | Loss: 0.00000824
Iteration 212/1000 | Loss: 0.00000824
Iteration 213/1000 | Loss: 0.00000824
Iteration 214/1000 | Loss: 0.00000824
Iteration 215/1000 | Loss: 0.00000824
Iteration 216/1000 | Loss: 0.00000824
Iteration 217/1000 | Loss: 0.00000824
Iteration 218/1000 | Loss: 0.00000824
Iteration 219/1000 | Loss: 0.00000824
Iteration 220/1000 | Loss: 0.00000824
Iteration 221/1000 | Loss: 0.00000824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [8.239116141339764e-06, 8.239116141339764e-06, 8.239116141339764e-06, 8.239116141339764e-06, 8.239116141339764e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.239116141339764e-06

Optimization complete. Final v2v error: 2.4870128631591797 mm

Highest mean error: 3.4421539306640625 mm for frame 162

Lowest mean error: 2.2909040451049805 mm for frame 182

Saving results

Total time: 73.79466271400452
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821269
Iteration 2/25 | Loss: 0.00107437
Iteration 3/25 | Loss: 0.00097495
Iteration 4/25 | Loss: 0.00096500
Iteration 5/25 | Loss: 0.00096287
Iteration 6/25 | Loss: 0.00096287
Iteration 7/25 | Loss: 0.00096287
Iteration 8/25 | Loss: 0.00096287
Iteration 9/25 | Loss: 0.00096287
Iteration 10/25 | Loss: 0.00096287
Iteration 11/25 | Loss: 0.00096287
Iteration 12/25 | Loss: 0.00096287
Iteration 13/25 | Loss: 0.00096287
Iteration 14/25 | Loss: 0.00096287
Iteration 15/25 | Loss: 0.00096287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009628709522075951, 0.0009628709522075951, 0.0009628709522075951, 0.0009628709522075951, 0.0009628709522075951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009628709522075951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38005221
Iteration 2/25 | Loss: 0.00068478
Iteration 3/25 | Loss: 0.00068478
Iteration 4/25 | Loss: 0.00068478
Iteration 5/25 | Loss: 0.00068478
Iteration 6/25 | Loss: 0.00068478
Iteration 7/25 | Loss: 0.00068478
Iteration 8/25 | Loss: 0.00068478
Iteration 9/25 | Loss: 0.00068478
Iteration 10/25 | Loss: 0.00068478
Iteration 11/25 | Loss: 0.00068478
Iteration 12/25 | Loss: 0.00068478
Iteration 13/25 | Loss: 0.00068478
Iteration 14/25 | Loss: 0.00068478
Iteration 15/25 | Loss: 0.00068478
Iteration 16/25 | Loss: 0.00068478
Iteration 17/25 | Loss: 0.00068478
Iteration 18/25 | Loss: 0.00068477
Iteration 19/25 | Loss: 0.00068477
Iteration 20/25 | Loss: 0.00068477
Iteration 21/25 | Loss: 0.00068477
Iteration 22/25 | Loss: 0.00068477
Iteration 23/25 | Loss: 0.00068477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006847749464213848, 0.0006847749464213848, 0.0006847749464213848, 0.0006847749464213848, 0.0006847749464213848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006847749464213848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068477
Iteration 2/1000 | Loss: 0.00001678
Iteration 3/1000 | Loss: 0.00001146
Iteration 4/1000 | Loss: 0.00001029
Iteration 5/1000 | Loss: 0.00000943
Iteration 6/1000 | Loss: 0.00000899
Iteration 7/1000 | Loss: 0.00000862
Iteration 8/1000 | Loss: 0.00000839
Iteration 9/1000 | Loss: 0.00000823
Iteration 10/1000 | Loss: 0.00000810
Iteration 11/1000 | Loss: 0.00000806
Iteration 12/1000 | Loss: 0.00000806
Iteration 13/1000 | Loss: 0.00000801
Iteration 14/1000 | Loss: 0.00000796
Iteration 15/1000 | Loss: 0.00000796
Iteration 16/1000 | Loss: 0.00000795
Iteration 17/1000 | Loss: 0.00000794
Iteration 18/1000 | Loss: 0.00000792
Iteration 19/1000 | Loss: 0.00000791
Iteration 20/1000 | Loss: 0.00000786
Iteration 21/1000 | Loss: 0.00000784
Iteration 22/1000 | Loss: 0.00000784
Iteration 23/1000 | Loss: 0.00000784
Iteration 24/1000 | Loss: 0.00000784
Iteration 25/1000 | Loss: 0.00000784
Iteration 26/1000 | Loss: 0.00000784
Iteration 27/1000 | Loss: 0.00000784
Iteration 28/1000 | Loss: 0.00000783
Iteration 29/1000 | Loss: 0.00000783
Iteration 30/1000 | Loss: 0.00000783
Iteration 31/1000 | Loss: 0.00000783
Iteration 32/1000 | Loss: 0.00000783
Iteration 33/1000 | Loss: 0.00000783
Iteration 34/1000 | Loss: 0.00000782
Iteration 35/1000 | Loss: 0.00000782
Iteration 36/1000 | Loss: 0.00000780
Iteration 37/1000 | Loss: 0.00000780
Iteration 38/1000 | Loss: 0.00000780
Iteration 39/1000 | Loss: 0.00000780
Iteration 40/1000 | Loss: 0.00000779
Iteration 41/1000 | Loss: 0.00000779
Iteration 42/1000 | Loss: 0.00000779
Iteration 43/1000 | Loss: 0.00000779
Iteration 44/1000 | Loss: 0.00000779
Iteration 45/1000 | Loss: 0.00000778
Iteration 46/1000 | Loss: 0.00000778
Iteration 47/1000 | Loss: 0.00000778
Iteration 48/1000 | Loss: 0.00000777
Iteration 49/1000 | Loss: 0.00000777
Iteration 50/1000 | Loss: 0.00000776
Iteration 51/1000 | Loss: 0.00000776
Iteration 52/1000 | Loss: 0.00000775
Iteration 53/1000 | Loss: 0.00000775
Iteration 54/1000 | Loss: 0.00000775
Iteration 55/1000 | Loss: 0.00000775
Iteration 56/1000 | Loss: 0.00000775
Iteration 57/1000 | Loss: 0.00000775
Iteration 58/1000 | Loss: 0.00000775
Iteration 59/1000 | Loss: 0.00000774
Iteration 60/1000 | Loss: 0.00000774
Iteration 61/1000 | Loss: 0.00000774
Iteration 62/1000 | Loss: 0.00000774
Iteration 63/1000 | Loss: 0.00000773
Iteration 64/1000 | Loss: 0.00000773
Iteration 65/1000 | Loss: 0.00000773
Iteration 66/1000 | Loss: 0.00000773
Iteration 67/1000 | Loss: 0.00000773
Iteration 68/1000 | Loss: 0.00000772
Iteration 69/1000 | Loss: 0.00000772
Iteration 70/1000 | Loss: 0.00000772
Iteration 71/1000 | Loss: 0.00000772
Iteration 72/1000 | Loss: 0.00000771
Iteration 73/1000 | Loss: 0.00000771
Iteration 74/1000 | Loss: 0.00000771
Iteration 75/1000 | Loss: 0.00000771
Iteration 76/1000 | Loss: 0.00000771
Iteration 77/1000 | Loss: 0.00000770
Iteration 78/1000 | Loss: 0.00000770
Iteration 79/1000 | Loss: 0.00000769
Iteration 80/1000 | Loss: 0.00000769
Iteration 81/1000 | Loss: 0.00000769
Iteration 82/1000 | Loss: 0.00000769
Iteration 83/1000 | Loss: 0.00000769
Iteration 84/1000 | Loss: 0.00000769
Iteration 85/1000 | Loss: 0.00000768
Iteration 86/1000 | Loss: 0.00000768
Iteration 87/1000 | Loss: 0.00000768
Iteration 88/1000 | Loss: 0.00000767
Iteration 89/1000 | Loss: 0.00000767
Iteration 90/1000 | Loss: 0.00000766
Iteration 91/1000 | Loss: 0.00000766
Iteration 92/1000 | Loss: 0.00000765
Iteration 93/1000 | Loss: 0.00000765
Iteration 94/1000 | Loss: 0.00000765
Iteration 95/1000 | Loss: 0.00000765
Iteration 96/1000 | Loss: 0.00000765
Iteration 97/1000 | Loss: 0.00000764
Iteration 98/1000 | Loss: 0.00000764
Iteration 99/1000 | Loss: 0.00000763
Iteration 100/1000 | Loss: 0.00000762
Iteration 101/1000 | Loss: 0.00000762
Iteration 102/1000 | Loss: 0.00000762
Iteration 103/1000 | Loss: 0.00000761
Iteration 104/1000 | Loss: 0.00000761
Iteration 105/1000 | Loss: 0.00000761
Iteration 106/1000 | Loss: 0.00000760
Iteration 107/1000 | Loss: 0.00000760
Iteration 108/1000 | Loss: 0.00000760
Iteration 109/1000 | Loss: 0.00000760
Iteration 110/1000 | Loss: 0.00000759
Iteration 111/1000 | Loss: 0.00000759
Iteration 112/1000 | Loss: 0.00000758
Iteration 113/1000 | Loss: 0.00000758
Iteration 114/1000 | Loss: 0.00000758
Iteration 115/1000 | Loss: 0.00000758
Iteration 116/1000 | Loss: 0.00000758
Iteration 117/1000 | Loss: 0.00000758
Iteration 118/1000 | Loss: 0.00000758
Iteration 119/1000 | Loss: 0.00000757
Iteration 120/1000 | Loss: 0.00000757
Iteration 121/1000 | Loss: 0.00000757
Iteration 122/1000 | Loss: 0.00000757
Iteration 123/1000 | Loss: 0.00000757
Iteration 124/1000 | Loss: 0.00000757
Iteration 125/1000 | Loss: 0.00000757
Iteration 126/1000 | Loss: 0.00000757
Iteration 127/1000 | Loss: 0.00000756
Iteration 128/1000 | Loss: 0.00000756
Iteration 129/1000 | Loss: 0.00000756
Iteration 130/1000 | Loss: 0.00000756
Iteration 131/1000 | Loss: 0.00000756
Iteration 132/1000 | Loss: 0.00000755
Iteration 133/1000 | Loss: 0.00000755
Iteration 134/1000 | Loss: 0.00000755
Iteration 135/1000 | Loss: 0.00000755
Iteration 136/1000 | Loss: 0.00000755
Iteration 137/1000 | Loss: 0.00000754
Iteration 138/1000 | Loss: 0.00000754
Iteration 139/1000 | Loss: 0.00000754
Iteration 140/1000 | Loss: 0.00000754
Iteration 141/1000 | Loss: 0.00000754
Iteration 142/1000 | Loss: 0.00000754
Iteration 143/1000 | Loss: 0.00000754
Iteration 144/1000 | Loss: 0.00000754
Iteration 145/1000 | Loss: 0.00000754
Iteration 146/1000 | Loss: 0.00000754
Iteration 147/1000 | Loss: 0.00000753
Iteration 148/1000 | Loss: 0.00000753
Iteration 149/1000 | Loss: 0.00000753
Iteration 150/1000 | Loss: 0.00000753
Iteration 151/1000 | Loss: 0.00000753
Iteration 152/1000 | Loss: 0.00000753
Iteration 153/1000 | Loss: 0.00000753
Iteration 154/1000 | Loss: 0.00000753
Iteration 155/1000 | Loss: 0.00000753
Iteration 156/1000 | Loss: 0.00000752
Iteration 157/1000 | Loss: 0.00000752
Iteration 158/1000 | Loss: 0.00000752
Iteration 159/1000 | Loss: 0.00000752
Iteration 160/1000 | Loss: 0.00000751
Iteration 161/1000 | Loss: 0.00000751
Iteration 162/1000 | Loss: 0.00000751
Iteration 163/1000 | Loss: 0.00000750
Iteration 164/1000 | Loss: 0.00000750
Iteration 165/1000 | Loss: 0.00000750
Iteration 166/1000 | Loss: 0.00000750
Iteration 167/1000 | Loss: 0.00000750
Iteration 168/1000 | Loss: 0.00000750
Iteration 169/1000 | Loss: 0.00000750
Iteration 170/1000 | Loss: 0.00000750
Iteration 171/1000 | Loss: 0.00000750
Iteration 172/1000 | Loss: 0.00000749
Iteration 173/1000 | Loss: 0.00000749
Iteration 174/1000 | Loss: 0.00000749
Iteration 175/1000 | Loss: 0.00000749
Iteration 176/1000 | Loss: 0.00000749
Iteration 177/1000 | Loss: 0.00000749
Iteration 178/1000 | Loss: 0.00000749
Iteration 179/1000 | Loss: 0.00000749
Iteration 180/1000 | Loss: 0.00000749
Iteration 181/1000 | Loss: 0.00000749
Iteration 182/1000 | Loss: 0.00000749
Iteration 183/1000 | Loss: 0.00000748
Iteration 184/1000 | Loss: 0.00000748
Iteration 185/1000 | Loss: 0.00000748
Iteration 186/1000 | Loss: 0.00000748
Iteration 187/1000 | Loss: 0.00000748
Iteration 188/1000 | Loss: 0.00000748
Iteration 189/1000 | Loss: 0.00000748
Iteration 190/1000 | Loss: 0.00000748
Iteration 191/1000 | Loss: 0.00000748
Iteration 192/1000 | Loss: 0.00000748
Iteration 193/1000 | Loss: 0.00000748
Iteration 194/1000 | Loss: 0.00000748
Iteration 195/1000 | Loss: 0.00000748
Iteration 196/1000 | Loss: 0.00000747
Iteration 197/1000 | Loss: 0.00000747
Iteration 198/1000 | Loss: 0.00000747
Iteration 199/1000 | Loss: 0.00000747
Iteration 200/1000 | Loss: 0.00000747
Iteration 201/1000 | Loss: 0.00000747
Iteration 202/1000 | Loss: 0.00000747
Iteration 203/1000 | Loss: 0.00000747
Iteration 204/1000 | Loss: 0.00000747
Iteration 205/1000 | Loss: 0.00000747
Iteration 206/1000 | Loss: 0.00000747
Iteration 207/1000 | Loss: 0.00000747
Iteration 208/1000 | Loss: 0.00000746
Iteration 209/1000 | Loss: 0.00000746
Iteration 210/1000 | Loss: 0.00000746
Iteration 211/1000 | Loss: 0.00000746
Iteration 212/1000 | Loss: 0.00000746
Iteration 213/1000 | Loss: 0.00000746
Iteration 214/1000 | Loss: 0.00000746
Iteration 215/1000 | Loss: 0.00000746
Iteration 216/1000 | Loss: 0.00000746
Iteration 217/1000 | Loss: 0.00000745
Iteration 218/1000 | Loss: 0.00000745
Iteration 219/1000 | Loss: 0.00000745
Iteration 220/1000 | Loss: 0.00000745
Iteration 221/1000 | Loss: 0.00000745
Iteration 222/1000 | Loss: 0.00000745
Iteration 223/1000 | Loss: 0.00000745
Iteration 224/1000 | Loss: 0.00000745
Iteration 225/1000 | Loss: 0.00000744
Iteration 226/1000 | Loss: 0.00000744
Iteration 227/1000 | Loss: 0.00000744
Iteration 228/1000 | Loss: 0.00000744
Iteration 229/1000 | Loss: 0.00000744
Iteration 230/1000 | Loss: 0.00000744
Iteration 231/1000 | Loss: 0.00000744
Iteration 232/1000 | Loss: 0.00000744
Iteration 233/1000 | Loss: 0.00000744
Iteration 234/1000 | Loss: 0.00000744
Iteration 235/1000 | Loss: 0.00000744
Iteration 236/1000 | Loss: 0.00000744
Iteration 237/1000 | Loss: 0.00000743
Iteration 238/1000 | Loss: 0.00000743
Iteration 239/1000 | Loss: 0.00000743
Iteration 240/1000 | Loss: 0.00000743
Iteration 241/1000 | Loss: 0.00000743
Iteration 242/1000 | Loss: 0.00000743
Iteration 243/1000 | Loss: 0.00000743
Iteration 244/1000 | Loss: 0.00000743
Iteration 245/1000 | Loss: 0.00000743
Iteration 246/1000 | Loss: 0.00000743
Iteration 247/1000 | Loss: 0.00000743
Iteration 248/1000 | Loss: 0.00000743
Iteration 249/1000 | Loss: 0.00000743
Iteration 250/1000 | Loss: 0.00000743
Iteration 251/1000 | Loss: 0.00000743
Iteration 252/1000 | Loss: 0.00000743
Iteration 253/1000 | Loss: 0.00000743
Iteration 254/1000 | Loss: 0.00000743
Iteration 255/1000 | Loss: 0.00000743
Iteration 256/1000 | Loss: 0.00000743
Iteration 257/1000 | Loss: 0.00000743
Iteration 258/1000 | Loss: 0.00000743
Iteration 259/1000 | Loss: 0.00000743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [7.428076060023159e-06, 7.428076060023159e-06, 7.428076060023159e-06, 7.428076060023159e-06, 7.428076060023159e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.428076060023159e-06

Optimization complete. Final v2v error: 2.3251142501831055 mm

Highest mean error: 2.5051183700561523 mm for frame 109

Lowest mean error: 2.186931610107422 mm for frame 197

Saving results

Total time: 42.66368269920349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425532
Iteration 2/25 | Loss: 0.00106215
Iteration 3/25 | Loss: 0.00100064
Iteration 4/25 | Loss: 0.00098732
Iteration 5/25 | Loss: 0.00098318
Iteration 6/25 | Loss: 0.00098224
Iteration 7/25 | Loss: 0.00098224
Iteration 8/25 | Loss: 0.00098224
Iteration 9/25 | Loss: 0.00098224
Iteration 10/25 | Loss: 0.00098224
Iteration 11/25 | Loss: 0.00098224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009822382126003504, 0.0009822382126003504, 0.0009822382126003504, 0.0009822382126003504, 0.0009822382126003504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009822382126003504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.26497841
Iteration 2/25 | Loss: 0.00070648
Iteration 3/25 | Loss: 0.00070648
Iteration 4/25 | Loss: 0.00070648
Iteration 5/25 | Loss: 0.00070648
Iteration 6/25 | Loss: 0.00070648
Iteration 7/25 | Loss: 0.00070648
Iteration 8/25 | Loss: 0.00070648
Iteration 9/25 | Loss: 0.00070648
Iteration 10/25 | Loss: 0.00070648
Iteration 11/25 | Loss: 0.00070648
Iteration 12/25 | Loss: 0.00070648
Iteration 13/25 | Loss: 0.00070648
Iteration 14/25 | Loss: 0.00070648
Iteration 15/25 | Loss: 0.00070648
Iteration 16/25 | Loss: 0.00070648
Iteration 17/25 | Loss: 0.00070648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007064797100611031, 0.0007064797100611031, 0.0007064797100611031, 0.0007064797100611031, 0.0007064797100611031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007064797100611031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070648
Iteration 2/1000 | Loss: 0.00001769
Iteration 3/1000 | Loss: 0.00001423
Iteration 4/1000 | Loss: 0.00001326
Iteration 5/1000 | Loss: 0.00001272
Iteration 6/1000 | Loss: 0.00001232
Iteration 7/1000 | Loss: 0.00001199
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001146
Iteration 12/1000 | Loss: 0.00001142
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001139
Iteration 17/1000 | Loss: 0.00001139
Iteration 18/1000 | Loss: 0.00001138
Iteration 19/1000 | Loss: 0.00001138
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001134
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001133
Iteration 30/1000 | Loss: 0.00001133
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001132
Iteration 33/1000 | Loss: 0.00001132
Iteration 34/1000 | Loss: 0.00001130
Iteration 35/1000 | Loss: 0.00001129
Iteration 36/1000 | Loss: 0.00001129
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001126
Iteration 43/1000 | Loss: 0.00001126
Iteration 44/1000 | Loss: 0.00001125
Iteration 45/1000 | Loss: 0.00001124
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001122
Iteration 55/1000 | Loss: 0.00001122
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001117
Iteration 74/1000 | Loss: 0.00001117
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001116
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001116
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001106
Iteration 118/1000 | Loss: 0.00001106
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001104
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001103
Iteration 129/1000 | Loss: 0.00001103
Iteration 130/1000 | Loss: 0.00001102
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001101
Iteration 135/1000 | Loss: 0.00001101
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001100
Iteration 141/1000 | Loss: 0.00001100
Iteration 142/1000 | Loss: 0.00001100
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Iteration 146/1000 | Loss: 0.00001099
Iteration 147/1000 | Loss: 0.00001099
Iteration 148/1000 | Loss: 0.00001099
Iteration 149/1000 | Loss: 0.00001099
Iteration 150/1000 | Loss: 0.00001099
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001097
Iteration 154/1000 | Loss: 0.00001097
Iteration 155/1000 | Loss: 0.00001097
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001096
Iteration 159/1000 | Loss: 0.00001096
Iteration 160/1000 | Loss: 0.00001096
Iteration 161/1000 | Loss: 0.00001096
Iteration 162/1000 | Loss: 0.00001096
Iteration 163/1000 | Loss: 0.00001096
Iteration 164/1000 | Loss: 0.00001096
Iteration 165/1000 | Loss: 0.00001096
Iteration 166/1000 | Loss: 0.00001096
Iteration 167/1000 | Loss: 0.00001096
Iteration 168/1000 | Loss: 0.00001096
Iteration 169/1000 | Loss: 0.00001096
Iteration 170/1000 | Loss: 0.00001096
Iteration 171/1000 | Loss: 0.00001096
Iteration 172/1000 | Loss: 0.00001096
Iteration 173/1000 | Loss: 0.00001096
Iteration 174/1000 | Loss: 0.00001096
Iteration 175/1000 | Loss: 0.00001096
Iteration 176/1000 | Loss: 0.00001096
Iteration 177/1000 | Loss: 0.00001096
Iteration 178/1000 | Loss: 0.00001096
Iteration 179/1000 | Loss: 0.00001096
Iteration 180/1000 | Loss: 0.00001096
Iteration 181/1000 | Loss: 0.00001096
Iteration 182/1000 | Loss: 0.00001096
Iteration 183/1000 | Loss: 0.00001096
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001096
Iteration 186/1000 | Loss: 0.00001096
Iteration 187/1000 | Loss: 0.00001096
Iteration 188/1000 | Loss: 0.00001096
Iteration 189/1000 | Loss: 0.00001096
Iteration 190/1000 | Loss: 0.00001096
Iteration 191/1000 | Loss: 0.00001096
Iteration 192/1000 | Loss: 0.00001096
Iteration 193/1000 | Loss: 0.00001096
Iteration 194/1000 | Loss: 0.00001096
Iteration 195/1000 | Loss: 0.00001096
Iteration 196/1000 | Loss: 0.00001096
Iteration 197/1000 | Loss: 0.00001096
Iteration 198/1000 | Loss: 0.00001096
Iteration 199/1000 | Loss: 0.00001096
Iteration 200/1000 | Loss: 0.00001096
Iteration 201/1000 | Loss: 0.00001096
Iteration 202/1000 | Loss: 0.00001096
Iteration 203/1000 | Loss: 0.00001096
Iteration 204/1000 | Loss: 0.00001096
Iteration 205/1000 | Loss: 0.00001096
Iteration 206/1000 | Loss: 0.00001096
Iteration 207/1000 | Loss: 0.00001096
Iteration 208/1000 | Loss: 0.00001096
Iteration 209/1000 | Loss: 0.00001096
Iteration 210/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.0957575796055607e-05, 1.0957575796055607e-05, 1.0957575796055607e-05, 1.0957575796055607e-05, 1.0957575796055607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0957575796055607e-05

Optimization complete. Final v2v error: 2.8327057361602783 mm

Highest mean error: 3.2655582427978516 mm for frame 177

Lowest mean error: 2.5323634147644043 mm for frame 210

Saving results

Total time: 41.35375165939331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386610
Iteration 2/25 | Loss: 0.00107905
Iteration 3/25 | Loss: 0.00099582
Iteration 4/25 | Loss: 0.00098834
Iteration 5/25 | Loss: 0.00098545
Iteration 6/25 | Loss: 0.00098496
Iteration 7/25 | Loss: 0.00098496
Iteration 8/25 | Loss: 0.00098496
Iteration 9/25 | Loss: 0.00098496
Iteration 10/25 | Loss: 0.00098496
Iteration 11/25 | Loss: 0.00098496
Iteration 12/25 | Loss: 0.00098496
Iteration 13/25 | Loss: 0.00098496
Iteration 14/25 | Loss: 0.00098496
Iteration 15/25 | Loss: 0.00098496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009849611669778824, 0.0009849611669778824, 0.0009849611669778824, 0.0009849611669778824, 0.0009849611669778824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009849611669778824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38446319
Iteration 2/25 | Loss: 0.00075545
Iteration 3/25 | Loss: 0.00075544
Iteration 4/25 | Loss: 0.00075544
Iteration 5/25 | Loss: 0.00075544
Iteration 6/25 | Loss: 0.00075544
Iteration 7/25 | Loss: 0.00075544
Iteration 8/25 | Loss: 0.00075544
Iteration 9/25 | Loss: 0.00075544
Iteration 10/25 | Loss: 0.00075544
Iteration 11/25 | Loss: 0.00075544
Iteration 12/25 | Loss: 0.00075544
Iteration 13/25 | Loss: 0.00075544
Iteration 14/25 | Loss: 0.00075544
Iteration 15/25 | Loss: 0.00075544
Iteration 16/25 | Loss: 0.00075544
Iteration 17/25 | Loss: 0.00075544
Iteration 18/25 | Loss: 0.00075544
Iteration 19/25 | Loss: 0.00075544
Iteration 20/25 | Loss: 0.00075544
Iteration 21/25 | Loss: 0.00075544
Iteration 22/25 | Loss: 0.00075544
Iteration 23/25 | Loss: 0.00075544
Iteration 24/25 | Loss: 0.00075544
Iteration 25/25 | Loss: 0.00075544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075544
Iteration 2/1000 | Loss: 0.00001756
Iteration 3/1000 | Loss: 0.00001122
Iteration 4/1000 | Loss: 0.00001027
Iteration 5/1000 | Loss: 0.00000988
Iteration 6/1000 | Loss: 0.00000950
Iteration 7/1000 | Loss: 0.00000920
Iteration 8/1000 | Loss: 0.00000915
Iteration 9/1000 | Loss: 0.00000910
Iteration 10/1000 | Loss: 0.00000909
Iteration 11/1000 | Loss: 0.00000909
Iteration 12/1000 | Loss: 0.00000908
Iteration 13/1000 | Loss: 0.00000906
Iteration 14/1000 | Loss: 0.00000886
Iteration 15/1000 | Loss: 0.00000886
Iteration 16/1000 | Loss: 0.00000885
Iteration 17/1000 | Loss: 0.00000884
Iteration 18/1000 | Loss: 0.00000883
Iteration 19/1000 | Loss: 0.00000882
Iteration 20/1000 | Loss: 0.00000882
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000878
Iteration 23/1000 | Loss: 0.00000877
Iteration 24/1000 | Loss: 0.00000877
Iteration 25/1000 | Loss: 0.00000876
Iteration 26/1000 | Loss: 0.00000876
Iteration 27/1000 | Loss: 0.00000870
Iteration 28/1000 | Loss: 0.00000868
Iteration 29/1000 | Loss: 0.00000868
Iteration 30/1000 | Loss: 0.00000868
Iteration 31/1000 | Loss: 0.00000868
Iteration 32/1000 | Loss: 0.00000867
Iteration 33/1000 | Loss: 0.00000867
Iteration 34/1000 | Loss: 0.00000866
Iteration 35/1000 | Loss: 0.00000865
Iteration 36/1000 | Loss: 0.00000865
Iteration 37/1000 | Loss: 0.00000864
Iteration 38/1000 | Loss: 0.00000864
Iteration 39/1000 | Loss: 0.00000863
Iteration 40/1000 | Loss: 0.00000863
Iteration 41/1000 | Loss: 0.00000862
Iteration 42/1000 | Loss: 0.00000860
Iteration 43/1000 | Loss: 0.00000859
Iteration 44/1000 | Loss: 0.00000859
Iteration 45/1000 | Loss: 0.00000858
Iteration 46/1000 | Loss: 0.00000858
Iteration 47/1000 | Loss: 0.00000857
Iteration 48/1000 | Loss: 0.00000857
Iteration 49/1000 | Loss: 0.00000856
Iteration 50/1000 | Loss: 0.00000856
Iteration 51/1000 | Loss: 0.00000855
Iteration 52/1000 | Loss: 0.00000855
Iteration 53/1000 | Loss: 0.00000853
Iteration 54/1000 | Loss: 0.00000853
Iteration 55/1000 | Loss: 0.00000852
Iteration 56/1000 | Loss: 0.00000852
Iteration 57/1000 | Loss: 0.00000852
Iteration 58/1000 | Loss: 0.00000852
Iteration 59/1000 | Loss: 0.00000852
Iteration 60/1000 | Loss: 0.00000852
Iteration 61/1000 | Loss: 0.00000852
Iteration 62/1000 | Loss: 0.00000852
Iteration 63/1000 | Loss: 0.00000852
Iteration 64/1000 | Loss: 0.00000851
Iteration 65/1000 | Loss: 0.00000851
Iteration 66/1000 | Loss: 0.00000851
Iteration 67/1000 | Loss: 0.00000851
Iteration 68/1000 | Loss: 0.00000851
Iteration 69/1000 | Loss: 0.00000850
Iteration 70/1000 | Loss: 0.00000850
Iteration 71/1000 | Loss: 0.00000849
Iteration 72/1000 | Loss: 0.00000849
Iteration 73/1000 | Loss: 0.00000849
Iteration 74/1000 | Loss: 0.00000849
Iteration 75/1000 | Loss: 0.00000848
Iteration 76/1000 | Loss: 0.00000848
Iteration 77/1000 | Loss: 0.00000848
Iteration 78/1000 | Loss: 0.00000848
Iteration 79/1000 | Loss: 0.00000848
Iteration 80/1000 | Loss: 0.00000848
Iteration 81/1000 | Loss: 0.00000848
Iteration 82/1000 | Loss: 0.00000848
Iteration 83/1000 | Loss: 0.00000848
Iteration 84/1000 | Loss: 0.00000847
Iteration 85/1000 | Loss: 0.00000847
Iteration 86/1000 | Loss: 0.00000846
Iteration 87/1000 | Loss: 0.00000846
Iteration 88/1000 | Loss: 0.00000845
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000843
Iteration 93/1000 | Loss: 0.00000843
Iteration 94/1000 | Loss: 0.00000842
Iteration 95/1000 | Loss: 0.00000842
Iteration 96/1000 | Loss: 0.00000842
Iteration 97/1000 | Loss: 0.00000842
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000841
Iteration 100/1000 | Loss: 0.00000841
Iteration 101/1000 | Loss: 0.00000841
Iteration 102/1000 | Loss: 0.00000841
Iteration 103/1000 | Loss: 0.00000841
Iteration 104/1000 | Loss: 0.00000840
Iteration 105/1000 | Loss: 0.00000840
Iteration 106/1000 | Loss: 0.00000839
Iteration 107/1000 | Loss: 0.00000838
Iteration 108/1000 | Loss: 0.00000838
Iteration 109/1000 | Loss: 0.00000838
Iteration 110/1000 | Loss: 0.00000838
Iteration 111/1000 | Loss: 0.00000838
Iteration 112/1000 | Loss: 0.00000838
Iteration 113/1000 | Loss: 0.00000837
Iteration 114/1000 | Loss: 0.00000837
Iteration 115/1000 | Loss: 0.00000837
Iteration 116/1000 | Loss: 0.00000837
Iteration 117/1000 | Loss: 0.00000837
Iteration 118/1000 | Loss: 0.00000836
Iteration 119/1000 | Loss: 0.00000836
Iteration 120/1000 | Loss: 0.00000836
Iteration 121/1000 | Loss: 0.00000836
Iteration 122/1000 | Loss: 0.00000836
Iteration 123/1000 | Loss: 0.00000835
Iteration 124/1000 | Loss: 0.00000835
Iteration 125/1000 | Loss: 0.00000834
Iteration 126/1000 | Loss: 0.00000833
Iteration 127/1000 | Loss: 0.00000832
Iteration 128/1000 | Loss: 0.00000832
Iteration 129/1000 | Loss: 0.00000832
Iteration 130/1000 | Loss: 0.00000832
Iteration 131/1000 | Loss: 0.00000832
Iteration 132/1000 | Loss: 0.00000832
Iteration 133/1000 | Loss: 0.00000831
Iteration 134/1000 | Loss: 0.00000831
Iteration 135/1000 | Loss: 0.00000831
Iteration 136/1000 | Loss: 0.00000831
Iteration 137/1000 | Loss: 0.00000831
Iteration 138/1000 | Loss: 0.00000830
Iteration 139/1000 | Loss: 0.00000830
Iteration 140/1000 | Loss: 0.00000830
Iteration 141/1000 | Loss: 0.00000830
Iteration 142/1000 | Loss: 0.00000830
Iteration 143/1000 | Loss: 0.00000830
Iteration 144/1000 | Loss: 0.00000830
Iteration 145/1000 | Loss: 0.00000830
Iteration 146/1000 | Loss: 0.00000830
Iteration 147/1000 | Loss: 0.00000830
Iteration 148/1000 | Loss: 0.00000830
Iteration 149/1000 | Loss: 0.00000830
Iteration 150/1000 | Loss: 0.00000830
Iteration 151/1000 | Loss: 0.00000830
Iteration 152/1000 | Loss: 0.00000829
Iteration 153/1000 | Loss: 0.00000829
Iteration 154/1000 | Loss: 0.00000829
Iteration 155/1000 | Loss: 0.00000829
Iteration 156/1000 | Loss: 0.00000829
Iteration 157/1000 | Loss: 0.00000829
Iteration 158/1000 | Loss: 0.00000829
Iteration 159/1000 | Loss: 0.00000829
Iteration 160/1000 | Loss: 0.00000829
Iteration 161/1000 | Loss: 0.00000828
Iteration 162/1000 | Loss: 0.00000828
Iteration 163/1000 | Loss: 0.00000828
Iteration 164/1000 | Loss: 0.00000828
Iteration 165/1000 | Loss: 0.00000828
Iteration 166/1000 | Loss: 0.00000828
Iteration 167/1000 | Loss: 0.00000828
Iteration 168/1000 | Loss: 0.00000827
Iteration 169/1000 | Loss: 0.00000827
Iteration 170/1000 | Loss: 0.00000827
Iteration 171/1000 | Loss: 0.00000827
Iteration 172/1000 | Loss: 0.00000826
Iteration 173/1000 | Loss: 0.00000826
Iteration 174/1000 | Loss: 0.00000826
Iteration 175/1000 | Loss: 0.00000826
Iteration 176/1000 | Loss: 0.00000826
Iteration 177/1000 | Loss: 0.00000826
Iteration 178/1000 | Loss: 0.00000825
Iteration 179/1000 | Loss: 0.00000825
Iteration 180/1000 | Loss: 0.00000825
Iteration 181/1000 | Loss: 0.00000825
Iteration 182/1000 | Loss: 0.00000825
Iteration 183/1000 | Loss: 0.00000825
Iteration 184/1000 | Loss: 0.00000825
Iteration 185/1000 | Loss: 0.00000825
Iteration 186/1000 | Loss: 0.00000825
Iteration 187/1000 | Loss: 0.00000825
Iteration 188/1000 | Loss: 0.00000825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [8.252937732322607e-06, 8.252937732322607e-06, 8.252937732322607e-06, 8.252937732322607e-06, 8.252937732322607e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.252937732322607e-06

Optimization complete. Final v2v error: 2.5061142444610596 mm

Highest mean error: 2.5959155559539795 mm for frame 59

Lowest mean error: 2.452401876449585 mm for frame 1

Saving results

Total time: 35.75465106964111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615469
Iteration 2/25 | Loss: 0.00129313
Iteration 3/25 | Loss: 0.00113779
Iteration 4/25 | Loss: 0.00112844
Iteration 5/25 | Loss: 0.00112623
Iteration 6/25 | Loss: 0.00112623
Iteration 7/25 | Loss: 0.00112623
Iteration 8/25 | Loss: 0.00112623
Iteration 9/25 | Loss: 0.00112623
Iteration 10/25 | Loss: 0.00112623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011262263869866729, 0.0011262263869866729, 0.0011262263869866729, 0.0011262263869866729, 0.0011262263869866729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011262263869866729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.60532522
Iteration 2/25 | Loss: 0.00072373
Iteration 3/25 | Loss: 0.00072357
Iteration 4/25 | Loss: 0.00072357
Iteration 5/25 | Loss: 0.00072357
Iteration 6/25 | Loss: 0.00072357
Iteration 7/25 | Loss: 0.00072357
Iteration 8/25 | Loss: 0.00072357
Iteration 9/25 | Loss: 0.00072357
Iteration 10/25 | Loss: 0.00072357
Iteration 11/25 | Loss: 0.00072356
Iteration 12/25 | Loss: 0.00072356
Iteration 13/25 | Loss: 0.00072356
Iteration 14/25 | Loss: 0.00072356
Iteration 15/25 | Loss: 0.00072356
Iteration 16/25 | Loss: 0.00072356
Iteration 17/25 | Loss: 0.00072356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007235647062771022, 0.0007235647062771022, 0.0007235647062771022, 0.0007235647062771022, 0.0007235647062771022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007235647062771022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072356
Iteration 2/1000 | Loss: 0.00004383
Iteration 3/1000 | Loss: 0.00002020
Iteration 4/1000 | Loss: 0.00001726
Iteration 5/1000 | Loss: 0.00001623
Iteration 6/1000 | Loss: 0.00001567
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001504
Iteration 9/1000 | Loss: 0.00001488
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001461
Iteration 12/1000 | Loss: 0.00001454
Iteration 13/1000 | Loss: 0.00001454
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001453
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001450
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001445
Iteration 30/1000 | Loss: 0.00001445
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001441
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001440
Iteration 44/1000 | Loss: 0.00001440
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001433
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.4316489796328824e-05, 1.4316489796328824e-05, 1.4316489796328824e-05, 1.4316489796328824e-05, 1.4316489796328824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4316489796328824e-05

Optimization complete. Final v2v error: 3.124086380004883 mm

Highest mean error: 3.548140525817871 mm for frame 170

Lowest mean error: 2.594886541366577 mm for frame 209

Saving results

Total time: 34.32294178009033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064402
Iteration 2/25 | Loss: 0.01064402
Iteration 3/25 | Loss: 0.01064402
Iteration 4/25 | Loss: 0.00343873
Iteration 5/25 | Loss: 0.00189161
Iteration 6/25 | Loss: 0.00180682
Iteration 7/25 | Loss: 0.00175758
Iteration 8/25 | Loss: 0.00174648
Iteration 9/25 | Loss: 0.00175079
Iteration 10/25 | Loss: 0.00186050
Iteration 11/25 | Loss: 0.00158042
Iteration 12/25 | Loss: 0.00144361
Iteration 13/25 | Loss: 0.00137886
Iteration 14/25 | Loss: 0.00137106
Iteration 15/25 | Loss: 0.00137007
Iteration 16/25 | Loss: 0.00136938
Iteration 17/25 | Loss: 0.00136862
Iteration 18/25 | Loss: 0.00136846
Iteration 19/25 | Loss: 0.00136881
Iteration 20/25 | Loss: 0.00136862
Iteration 21/25 | Loss: 0.00136836
Iteration 22/25 | Loss: 0.00136836
Iteration 23/25 | Loss: 0.00136836
Iteration 24/25 | Loss: 0.00136836
Iteration 25/25 | Loss: 0.00136836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55650669
Iteration 2/25 | Loss: 0.00095878
Iteration 3/25 | Loss: 0.00095878
Iteration 4/25 | Loss: 0.00095878
Iteration 5/25 | Loss: 0.00095878
Iteration 6/25 | Loss: 0.00095878
Iteration 7/25 | Loss: 0.00095878
Iteration 8/25 | Loss: 0.00095878
Iteration 9/25 | Loss: 0.00095878
Iteration 10/25 | Loss: 0.00095877
Iteration 11/25 | Loss: 0.00095877
Iteration 12/25 | Loss: 0.00095877
Iteration 13/25 | Loss: 0.00095877
Iteration 14/25 | Loss: 0.00095877
Iteration 15/25 | Loss: 0.00095877
Iteration 16/25 | Loss: 0.00095877
Iteration 17/25 | Loss: 0.00095877
Iteration 18/25 | Loss: 0.00095877
Iteration 19/25 | Loss: 0.00095877
Iteration 20/25 | Loss: 0.00095877
Iteration 21/25 | Loss: 0.00095877
Iteration 22/25 | Loss: 0.00095877
Iteration 23/25 | Loss: 0.00095877
Iteration 24/25 | Loss: 0.00095877
Iteration 25/25 | Loss: 0.00095877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095877
Iteration 2/1000 | Loss: 0.00042497
Iteration 3/1000 | Loss: 0.00021083
Iteration 4/1000 | Loss: 0.00036152
Iteration 5/1000 | Loss: 0.00042789
Iteration 6/1000 | Loss: 0.00013423
Iteration 7/1000 | Loss: 0.00027911
Iteration 8/1000 | Loss: 0.00012178
Iteration 9/1000 | Loss: 0.00011397
Iteration 10/1000 | Loss: 0.00006313
Iteration 11/1000 | Loss: 0.00009851
Iteration 12/1000 | Loss: 0.00006147
Iteration 13/1000 | Loss: 0.00006453
Iteration 14/1000 | Loss: 0.00006434
Iteration 15/1000 | Loss: 0.00007187
Iteration 16/1000 | Loss: 0.00006583
Iteration 17/1000 | Loss: 0.00006452
Iteration 18/1000 | Loss: 0.00010627
Iteration 19/1000 | Loss: 0.00009814
Iteration 20/1000 | Loss: 0.00010159
Iteration 21/1000 | Loss: 0.00006357
Iteration 22/1000 | Loss: 0.00006554
Iteration 23/1000 | Loss: 0.00006002
Iteration 24/1000 | Loss: 0.00005879
Iteration 25/1000 | Loss: 0.00006443
Iteration 26/1000 | Loss: 0.00005854
Iteration 27/1000 | Loss: 0.00006005
Iteration 28/1000 | Loss: 0.00006015
Iteration 29/1000 | Loss: 0.00006466
Iteration 30/1000 | Loss: 0.00006921
Iteration 31/1000 | Loss: 0.00005931
Iteration 32/1000 | Loss: 0.00006105
Iteration 33/1000 | Loss: 0.00006046
Iteration 34/1000 | Loss: 0.00007988
Iteration 35/1000 | Loss: 0.00025123
Iteration 36/1000 | Loss: 0.00012419
Iteration 37/1000 | Loss: 0.00007070
Iteration 38/1000 | Loss: 0.00009321
Iteration 39/1000 | Loss: 0.00011927
Iteration 40/1000 | Loss: 0.00006190
Iteration 41/1000 | Loss: 0.00005955
Iteration 42/1000 | Loss: 0.00005992
Iteration 43/1000 | Loss: 0.00006313
Iteration 44/1000 | Loss: 0.00005987
Iteration 45/1000 | Loss: 0.00005795
Iteration 46/1000 | Loss: 0.00005857
Iteration 47/1000 | Loss: 0.00005966
Iteration 48/1000 | Loss: 0.00005939
Iteration 49/1000 | Loss: 0.00006354
Iteration 50/1000 | Loss: 0.00007249
Iteration 51/1000 | Loss: 0.00005834
Iteration 52/1000 | Loss: 0.00005783
Iteration 53/1000 | Loss: 0.00005783
Iteration 54/1000 | Loss: 0.00005946
Iteration 55/1000 | Loss: 0.00005971
Iteration 56/1000 | Loss: 0.00005770
Iteration 57/1000 | Loss: 0.00005811
Iteration 58/1000 | Loss: 0.00007532
Iteration 59/1000 | Loss: 0.00005838
Iteration 60/1000 | Loss: 0.00005742
Iteration 61/1000 | Loss: 0.00005742
Iteration 62/1000 | Loss: 0.00005741
Iteration 63/1000 | Loss: 0.00005822
Iteration 64/1000 | Loss: 0.00005746
Iteration 65/1000 | Loss: 0.00005768
Iteration 66/1000 | Loss: 0.00005740
Iteration 67/1000 | Loss: 0.00005744
Iteration 68/1000 | Loss: 0.00005739
Iteration 69/1000 | Loss: 0.00005738
Iteration 70/1000 | Loss: 0.00005738
Iteration 71/1000 | Loss: 0.00005738
Iteration 72/1000 | Loss: 0.00005738
Iteration 73/1000 | Loss: 0.00005738
Iteration 74/1000 | Loss: 0.00005738
Iteration 75/1000 | Loss: 0.00005738
Iteration 76/1000 | Loss: 0.00005738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [5.738047548220493e-05, 5.738047548220493e-05, 5.738047548220493e-05, 5.738047548220493e-05, 5.738047548220493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.738047548220493e-05

Optimization complete. Final v2v error: 4.350299835205078 mm

Highest mean error: 20.289831161499023 mm for frame 237

Lowest mean error: 3.517639398574829 mm for frame 27

Saving results

Total time: 122.59379458427429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820003
Iteration 2/25 | Loss: 0.00164784
Iteration 3/25 | Loss: 0.00113973
Iteration 4/25 | Loss: 0.00109493
Iteration 5/25 | Loss: 0.00108964
Iteration 6/25 | Loss: 0.00108853
Iteration 7/25 | Loss: 0.00108853
Iteration 8/25 | Loss: 0.00108853
Iteration 9/25 | Loss: 0.00108853
Iteration 10/25 | Loss: 0.00108853
Iteration 11/25 | Loss: 0.00108853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001088533317670226, 0.001088533317670226, 0.001088533317670226, 0.001088533317670226, 0.001088533317670226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001088533317670226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36423647
Iteration 2/25 | Loss: 0.00045373
Iteration 3/25 | Loss: 0.00045373
Iteration 4/25 | Loss: 0.00045373
Iteration 5/25 | Loss: 0.00045373
Iteration 6/25 | Loss: 0.00045373
Iteration 7/25 | Loss: 0.00045373
Iteration 8/25 | Loss: 0.00045373
Iteration 9/25 | Loss: 0.00045373
Iteration 10/25 | Loss: 0.00045373
Iteration 11/25 | Loss: 0.00045373
Iteration 12/25 | Loss: 0.00045373
Iteration 13/25 | Loss: 0.00045373
Iteration 14/25 | Loss: 0.00045373
Iteration 15/25 | Loss: 0.00045373
Iteration 16/25 | Loss: 0.00045373
Iteration 17/25 | Loss: 0.00045373
Iteration 18/25 | Loss: 0.00045373
Iteration 19/25 | Loss: 0.00045373
Iteration 20/25 | Loss: 0.00045373
Iteration 21/25 | Loss: 0.00045373
Iteration 22/25 | Loss: 0.00045373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00045372743625193834, 0.00045372743625193834, 0.00045372743625193834, 0.00045372743625193834, 0.00045372743625193834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045372743625193834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045373
Iteration 2/1000 | Loss: 0.00003802
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001845
Iteration 6/1000 | Loss: 0.00001772
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001649
Iteration 10/1000 | Loss: 0.00001639
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001612
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001594
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001581
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001577
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001576
Iteration 53/1000 | Loss: 0.00001576
Iteration 54/1000 | Loss: 0.00001576
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001575
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001575
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001573
Iteration 82/1000 | Loss: 0.00001573
Iteration 83/1000 | Loss: 0.00001573
Iteration 84/1000 | Loss: 0.00001573
Iteration 85/1000 | Loss: 0.00001573
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001572
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001570
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001569
Iteration 98/1000 | Loss: 0.00001569
Iteration 99/1000 | Loss: 0.00001569
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001568
Iteration 102/1000 | Loss: 0.00001568
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001568
Iteration 106/1000 | Loss: 0.00001567
Iteration 107/1000 | Loss: 0.00001567
Iteration 108/1000 | Loss: 0.00001567
Iteration 109/1000 | Loss: 0.00001567
Iteration 110/1000 | Loss: 0.00001567
Iteration 111/1000 | Loss: 0.00001567
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001566
Iteration 117/1000 | Loss: 0.00001566
Iteration 118/1000 | Loss: 0.00001566
Iteration 119/1000 | Loss: 0.00001566
Iteration 120/1000 | Loss: 0.00001566
Iteration 121/1000 | Loss: 0.00001566
Iteration 122/1000 | Loss: 0.00001566
Iteration 123/1000 | Loss: 0.00001566
Iteration 124/1000 | Loss: 0.00001565
Iteration 125/1000 | Loss: 0.00001565
Iteration 126/1000 | Loss: 0.00001565
Iteration 127/1000 | Loss: 0.00001565
Iteration 128/1000 | Loss: 0.00001565
Iteration 129/1000 | Loss: 0.00001565
Iteration 130/1000 | Loss: 0.00001565
Iteration 131/1000 | Loss: 0.00001565
Iteration 132/1000 | Loss: 0.00001565
Iteration 133/1000 | Loss: 0.00001564
Iteration 134/1000 | Loss: 0.00001564
Iteration 135/1000 | Loss: 0.00001564
Iteration 136/1000 | Loss: 0.00001564
Iteration 137/1000 | Loss: 0.00001564
Iteration 138/1000 | Loss: 0.00001564
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001563
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001563
Iteration 144/1000 | Loss: 0.00001563
Iteration 145/1000 | Loss: 0.00001563
Iteration 146/1000 | Loss: 0.00001563
Iteration 147/1000 | Loss: 0.00001562
Iteration 148/1000 | Loss: 0.00001562
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001562
Iteration 151/1000 | Loss: 0.00001562
Iteration 152/1000 | Loss: 0.00001562
Iteration 153/1000 | Loss: 0.00001562
Iteration 154/1000 | Loss: 0.00001562
Iteration 155/1000 | Loss: 0.00001561
Iteration 156/1000 | Loss: 0.00001561
Iteration 157/1000 | Loss: 0.00001561
Iteration 158/1000 | Loss: 0.00001561
Iteration 159/1000 | Loss: 0.00001561
Iteration 160/1000 | Loss: 0.00001561
Iteration 161/1000 | Loss: 0.00001561
Iteration 162/1000 | Loss: 0.00001561
Iteration 163/1000 | Loss: 0.00001561
Iteration 164/1000 | Loss: 0.00001561
Iteration 165/1000 | Loss: 0.00001560
Iteration 166/1000 | Loss: 0.00001560
Iteration 167/1000 | Loss: 0.00001560
Iteration 168/1000 | Loss: 0.00001560
Iteration 169/1000 | Loss: 0.00001560
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001560
Iteration 173/1000 | Loss: 0.00001560
Iteration 174/1000 | Loss: 0.00001560
Iteration 175/1000 | Loss: 0.00001559
Iteration 176/1000 | Loss: 0.00001559
Iteration 177/1000 | Loss: 0.00001559
Iteration 178/1000 | Loss: 0.00001559
Iteration 179/1000 | Loss: 0.00001559
Iteration 180/1000 | Loss: 0.00001559
Iteration 181/1000 | Loss: 0.00001559
Iteration 182/1000 | Loss: 0.00001559
Iteration 183/1000 | Loss: 0.00001559
Iteration 184/1000 | Loss: 0.00001559
Iteration 185/1000 | Loss: 0.00001559
Iteration 186/1000 | Loss: 0.00001559
Iteration 187/1000 | Loss: 0.00001559
Iteration 188/1000 | Loss: 0.00001559
Iteration 189/1000 | Loss: 0.00001559
Iteration 190/1000 | Loss: 0.00001559
Iteration 191/1000 | Loss: 0.00001559
Iteration 192/1000 | Loss: 0.00001558
Iteration 193/1000 | Loss: 0.00001558
Iteration 194/1000 | Loss: 0.00001558
Iteration 195/1000 | Loss: 0.00001558
Iteration 196/1000 | Loss: 0.00001558
Iteration 197/1000 | Loss: 0.00001558
Iteration 198/1000 | Loss: 0.00001558
Iteration 199/1000 | Loss: 0.00001558
Iteration 200/1000 | Loss: 0.00001558
Iteration 201/1000 | Loss: 0.00001558
Iteration 202/1000 | Loss: 0.00001558
Iteration 203/1000 | Loss: 0.00001557
Iteration 204/1000 | Loss: 0.00001557
Iteration 205/1000 | Loss: 0.00001557
Iteration 206/1000 | Loss: 0.00001557
Iteration 207/1000 | Loss: 0.00001557
Iteration 208/1000 | Loss: 0.00001557
Iteration 209/1000 | Loss: 0.00001557
Iteration 210/1000 | Loss: 0.00001557
Iteration 211/1000 | Loss: 0.00001557
Iteration 212/1000 | Loss: 0.00001557
Iteration 213/1000 | Loss: 0.00001557
Iteration 214/1000 | Loss: 0.00001557
Iteration 215/1000 | Loss: 0.00001557
Iteration 216/1000 | Loss: 0.00001557
Iteration 217/1000 | Loss: 0.00001557
Iteration 218/1000 | Loss: 0.00001557
Iteration 219/1000 | Loss: 0.00001557
Iteration 220/1000 | Loss: 0.00001557
Iteration 221/1000 | Loss: 0.00001557
Iteration 222/1000 | Loss: 0.00001557
Iteration 223/1000 | Loss: 0.00001557
Iteration 224/1000 | Loss: 0.00001557
Iteration 225/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.556610550323967e-05, 1.556610550323967e-05, 1.556610550323967e-05, 1.556610550323967e-05, 1.556610550323967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.556610550323967e-05

Optimization complete. Final v2v error: 3.3597464561462402 mm

Highest mean error: 3.8440492153167725 mm for frame 224

Lowest mean error: 2.8809423446655273 mm for frame 62

Saving results

Total time: 46.61429190635681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490231
Iteration 2/25 | Loss: 0.00126146
Iteration 3/25 | Loss: 0.00106540
Iteration 4/25 | Loss: 0.00104515
Iteration 5/25 | Loss: 0.00103704
Iteration 6/25 | Loss: 0.00103523
Iteration 7/25 | Loss: 0.00103523
Iteration 8/25 | Loss: 0.00103523
Iteration 9/25 | Loss: 0.00103523
Iteration 10/25 | Loss: 0.00103523
Iteration 11/25 | Loss: 0.00103523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001035233843140304, 0.001035233843140304, 0.001035233843140304, 0.001035233843140304, 0.001035233843140304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001035233843140304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77500415
Iteration 2/25 | Loss: 0.00068568
Iteration 3/25 | Loss: 0.00068568
Iteration 4/25 | Loss: 0.00068568
Iteration 5/25 | Loss: 0.00068568
Iteration 6/25 | Loss: 0.00068568
Iteration 7/25 | Loss: 0.00068568
Iteration 8/25 | Loss: 0.00068568
Iteration 9/25 | Loss: 0.00068568
Iteration 10/25 | Loss: 0.00068568
Iteration 11/25 | Loss: 0.00068568
Iteration 12/25 | Loss: 0.00068568
Iteration 13/25 | Loss: 0.00068568
Iteration 14/25 | Loss: 0.00068568
Iteration 15/25 | Loss: 0.00068568
Iteration 16/25 | Loss: 0.00068568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006856807740405202, 0.0006856807740405202, 0.0006856807740405202, 0.0006856807740405202, 0.0006856807740405202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006856807740405202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068568
Iteration 2/1000 | Loss: 0.00003689
Iteration 3/1000 | Loss: 0.00002670
Iteration 4/1000 | Loss: 0.00002471
Iteration 5/1000 | Loss: 0.00002364
Iteration 6/1000 | Loss: 0.00002288
Iteration 7/1000 | Loss: 0.00002217
Iteration 8/1000 | Loss: 0.00002176
Iteration 9/1000 | Loss: 0.00002132
Iteration 10/1000 | Loss: 0.00002099
Iteration 11/1000 | Loss: 0.00002076
Iteration 12/1000 | Loss: 0.00002056
Iteration 13/1000 | Loss: 0.00002035
Iteration 14/1000 | Loss: 0.00002023
Iteration 15/1000 | Loss: 0.00002012
Iteration 16/1000 | Loss: 0.00002000
Iteration 17/1000 | Loss: 0.00002000
Iteration 18/1000 | Loss: 0.00001998
Iteration 19/1000 | Loss: 0.00001994
Iteration 20/1000 | Loss: 0.00001994
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001974
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001974
Iteration 33/1000 | Loss: 0.00001974
Iteration 34/1000 | Loss: 0.00001973
Iteration 35/1000 | Loss: 0.00001970
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001965
Iteration 38/1000 | Loss: 0.00001965
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001964
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001964
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001964
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001963
Iteration 48/1000 | Loss: 0.00001963
Iteration 49/1000 | Loss: 0.00001963
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001959
Iteration 52/1000 | Loss: 0.00001959
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001959
Iteration 55/1000 | Loss: 0.00001959
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001958
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001957
Iteration 60/1000 | Loss: 0.00001957
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001956
Iteration 71/1000 | Loss: 0.00001956
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001953
Iteration 77/1000 | Loss: 0.00001953
Iteration 78/1000 | Loss: 0.00001953
Iteration 79/1000 | Loss: 0.00001953
Iteration 80/1000 | Loss: 0.00001953
Iteration 81/1000 | Loss: 0.00001952
Iteration 82/1000 | Loss: 0.00001952
Iteration 83/1000 | Loss: 0.00001952
Iteration 84/1000 | Loss: 0.00001952
Iteration 85/1000 | Loss: 0.00001952
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001951
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001950
Iteration 94/1000 | Loss: 0.00001950
Iteration 95/1000 | Loss: 0.00001950
Iteration 96/1000 | Loss: 0.00001949
Iteration 97/1000 | Loss: 0.00001948
Iteration 98/1000 | Loss: 0.00001948
Iteration 99/1000 | Loss: 0.00001947
Iteration 100/1000 | Loss: 0.00001947
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001944
Iteration 104/1000 | Loss: 0.00001942
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001940
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001939
Iteration 109/1000 | Loss: 0.00001938
Iteration 110/1000 | Loss: 0.00001937
Iteration 111/1000 | Loss: 0.00001934
Iteration 112/1000 | Loss: 0.00001934
Iteration 113/1000 | Loss: 0.00001934
Iteration 114/1000 | Loss: 0.00001933
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001931
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001930
Iteration 122/1000 | Loss: 0.00001930
Iteration 123/1000 | Loss: 0.00001930
Iteration 124/1000 | Loss: 0.00001930
Iteration 125/1000 | Loss: 0.00001930
Iteration 126/1000 | Loss: 0.00001930
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001927
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Iteration 143/1000 | Loss: 0.00001927
Iteration 144/1000 | Loss: 0.00001926
Iteration 145/1000 | Loss: 0.00001926
Iteration 146/1000 | Loss: 0.00001926
Iteration 147/1000 | Loss: 0.00001926
Iteration 148/1000 | Loss: 0.00001926
Iteration 149/1000 | Loss: 0.00001926
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001926
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001925
Iteration 154/1000 | Loss: 0.00001925
Iteration 155/1000 | Loss: 0.00001925
Iteration 156/1000 | Loss: 0.00001925
Iteration 157/1000 | Loss: 0.00001925
Iteration 158/1000 | Loss: 0.00001925
Iteration 159/1000 | Loss: 0.00001925
Iteration 160/1000 | Loss: 0.00001924
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001924
Iteration 163/1000 | Loss: 0.00001924
Iteration 164/1000 | Loss: 0.00001924
Iteration 165/1000 | Loss: 0.00001923
Iteration 166/1000 | Loss: 0.00001923
Iteration 167/1000 | Loss: 0.00001923
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001922
Iteration 171/1000 | Loss: 0.00001922
Iteration 172/1000 | Loss: 0.00001922
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001922
Iteration 175/1000 | Loss: 0.00001921
Iteration 176/1000 | Loss: 0.00001921
Iteration 177/1000 | Loss: 0.00001921
Iteration 178/1000 | Loss: 0.00001921
Iteration 179/1000 | Loss: 0.00001921
Iteration 180/1000 | Loss: 0.00001921
Iteration 181/1000 | Loss: 0.00001921
Iteration 182/1000 | Loss: 0.00001921
Iteration 183/1000 | Loss: 0.00001921
Iteration 184/1000 | Loss: 0.00001921
Iteration 185/1000 | Loss: 0.00001921
Iteration 186/1000 | Loss: 0.00001921
Iteration 187/1000 | Loss: 0.00001921
Iteration 188/1000 | Loss: 0.00001921
Iteration 189/1000 | Loss: 0.00001921
Iteration 190/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.9205315766157582e-05, 1.9205315766157582e-05, 1.9205315766157582e-05, 1.9205315766157582e-05, 1.9205315766157582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9205315766157582e-05

Optimization complete. Final v2v error: 3.7518320083618164 mm

Highest mean error: 4.257287502288818 mm for frame 245

Lowest mean error: 3.6204605102539062 mm for frame 134

Saving results

Total time: 56.20877385139465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423046
Iteration 2/25 | Loss: 0.00126017
Iteration 3/25 | Loss: 0.00104318
Iteration 4/25 | Loss: 0.00102935
Iteration 5/25 | Loss: 0.00102736
Iteration 6/25 | Loss: 0.00102684
Iteration 7/25 | Loss: 0.00102684
Iteration 8/25 | Loss: 0.00102684
Iteration 9/25 | Loss: 0.00102684
Iteration 10/25 | Loss: 0.00102684
Iteration 11/25 | Loss: 0.00102684
Iteration 12/25 | Loss: 0.00102684
Iteration 13/25 | Loss: 0.00102684
Iteration 14/25 | Loss: 0.00102684
Iteration 15/25 | Loss: 0.00102684
Iteration 16/25 | Loss: 0.00102684
Iteration 17/25 | Loss: 0.00102684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010268447222188115, 0.0010268447222188115, 0.0010268447222188115, 0.0010268447222188115, 0.0010268447222188115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010268447222188115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.98897600
Iteration 2/25 | Loss: 0.00064618
Iteration 3/25 | Loss: 0.00064618
Iteration 4/25 | Loss: 0.00064618
Iteration 5/25 | Loss: 0.00064618
Iteration 6/25 | Loss: 0.00064618
Iteration 7/25 | Loss: 0.00064618
Iteration 8/25 | Loss: 0.00064618
Iteration 9/25 | Loss: 0.00064618
Iteration 10/25 | Loss: 0.00064618
Iteration 11/25 | Loss: 0.00064618
Iteration 12/25 | Loss: 0.00064618
Iteration 13/25 | Loss: 0.00064618
Iteration 14/25 | Loss: 0.00064618
Iteration 15/25 | Loss: 0.00064618
Iteration 16/25 | Loss: 0.00064618
Iteration 17/25 | Loss: 0.00064618
Iteration 18/25 | Loss: 0.00064618
Iteration 19/25 | Loss: 0.00064618
Iteration 20/25 | Loss: 0.00064618
Iteration 21/25 | Loss: 0.00064618
Iteration 22/25 | Loss: 0.00064618
Iteration 23/25 | Loss: 0.00064618
Iteration 24/25 | Loss: 0.00064617
Iteration 25/25 | Loss: 0.00064617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064617
Iteration 2/1000 | Loss: 0.00002388
Iteration 3/1000 | Loss: 0.00001608
Iteration 4/1000 | Loss: 0.00001404
Iteration 5/1000 | Loss: 0.00001304
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001208
Iteration 8/1000 | Loss: 0.00001177
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001157
Iteration 11/1000 | Loss: 0.00001146
Iteration 12/1000 | Loss: 0.00001143
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001130
Iteration 21/1000 | Loss: 0.00001130
Iteration 22/1000 | Loss: 0.00001128
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001126
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001121
Iteration 30/1000 | Loss: 0.00001119
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001105
Iteration 67/1000 | Loss: 0.00001105
Iteration 68/1000 | Loss: 0.00001105
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001104
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001100
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001099
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001098
Iteration 90/1000 | Loss: 0.00001098
Iteration 91/1000 | Loss: 0.00001098
Iteration 92/1000 | Loss: 0.00001098
Iteration 93/1000 | Loss: 0.00001098
Iteration 94/1000 | Loss: 0.00001097
Iteration 95/1000 | Loss: 0.00001097
Iteration 96/1000 | Loss: 0.00001097
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001096
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001095
Iteration 107/1000 | Loss: 0.00001095
Iteration 108/1000 | Loss: 0.00001095
Iteration 109/1000 | Loss: 0.00001095
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001095
Iteration 112/1000 | Loss: 0.00001094
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001094
Iteration 115/1000 | Loss: 0.00001094
Iteration 116/1000 | Loss: 0.00001094
Iteration 117/1000 | Loss: 0.00001094
Iteration 118/1000 | Loss: 0.00001094
Iteration 119/1000 | Loss: 0.00001094
Iteration 120/1000 | Loss: 0.00001094
Iteration 121/1000 | Loss: 0.00001093
Iteration 122/1000 | Loss: 0.00001093
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001093
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001092
Iteration 133/1000 | Loss: 0.00001092
Iteration 134/1000 | Loss: 0.00001092
Iteration 135/1000 | Loss: 0.00001092
Iteration 136/1000 | Loss: 0.00001092
Iteration 137/1000 | Loss: 0.00001092
Iteration 138/1000 | Loss: 0.00001092
Iteration 139/1000 | Loss: 0.00001092
Iteration 140/1000 | Loss: 0.00001092
Iteration 141/1000 | Loss: 0.00001092
Iteration 142/1000 | Loss: 0.00001092
Iteration 143/1000 | Loss: 0.00001092
Iteration 144/1000 | Loss: 0.00001091
Iteration 145/1000 | Loss: 0.00001091
Iteration 146/1000 | Loss: 0.00001091
Iteration 147/1000 | Loss: 0.00001091
Iteration 148/1000 | Loss: 0.00001091
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001091
Iteration 155/1000 | Loss: 0.00001091
Iteration 156/1000 | Loss: 0.00001090
Iteration 157/1000 | Loss: 0.00001090
Iteration 158/1000 | Loss: 0.00001090
Iteration 159/1000 | Loss: 0.00001090
Iteration 160/1000 | Loss: 0.00001090
Iteration 161/1000 | Loss: 0.00001089
Iteration 162/1000 | Loss: 0.00001089
Iteration 163/1000 | Loss: 0.00001089
Iteration 164/1000 | Loss: 0.00001089
Iteration 165/1000 | Loss: 0.00001089
Iteration 166/1000 | Loss: 0.00001089
Iteration 167/1000 | Loss: 0.00001088
Iteration 168/1000 | Loss: 0.00001088
Iteration 169/1000 | Loss: 0.00001088
Iteration 170/1000 | Loss: 0.00001088
Iteration 171/1000 | Loss: 0.00001088
Iteration 172/1000 | Loss: 0.00001088
Iteration 173/1000 | Loss: 0.00001087
Iteration 174/1000 | Loss: 0.00001087
Iteration 175/1000 | Loss: 0.00001087
Iteration 176/1000 | Loss: 0.00001087
Iteration 177/1000 | Loss: 0.00001087
Iteration 178/1000 | Loss: 0.00001087
Iteration 179/1000 | Loss: 0.00001087
Iteration 180/1000 | Loss: 0.00001087
Iteration 181/1000 | Loss: 0.00001087
Iteration 182/1000 | Loss: 0.00001087
Iteration 183/1000 | Loss: 0.00001087
Iteration 184/1000 | Loss: 0.00001087
Iteration 185/1000 | Loss: 0.00001086
Iteration 186/1000 | Loss: 0.00001086
Iteration 187/1000 | Loss: 0.00001086
Iteration 188/1000 | Loss: 0.00001086
Iteration 189/1000 | Loss: 0.00001086
Iteration 190/1000 | Loss: 0.00001086
Iteration 191/1000 | Loss: 0.00001085
Iteration 192/1000 | Loss: 0.00001085
Iteration 193/1000 | Loss: 0.00001085
Iteration 194/1000 | Loss: 0.00001085
Iteration 195/1000 | Loss: 0.00001085
Iteration 196/1000 | Loss: 0.00001085
Iteration 197/1000 | Loss: 0.00001085
Iteration 198/1000 | Loss: 0.00001084
Iteration 199/1000 | Loss: 0.00001084
Iteration 200/1000 | Loss: 0.00001084
Iteration 201/1000 | Loss: 0.00001084
Iteration 202/1000 | Loss: 0.00001084
Iteration 203/1000 | Loss: 0.00001084
Iteration 204/1000 | Loss: 0.00001084
Iteration 205/1000 | Loss: 0.00001084
Iteration 206/1000 | Loss: 0.00001084
Iteration 207/1000 | Loss: 0.00001084
Iteration 208/1000 | Loss: 0.00001084
Iteration 209/1000 | Loss: 0.00001084
Iteration 210/1000 | Loss: 0.00001084
Iteration 211/1000 | Loss: 0.00001084
Iteration 212/1000 | Loss: 0.00001084
Iteration 213/1000 | Loss: 0.00001084
Iteration 214/1000 | Loss: 0.00001084
Iteration 215/1000 | Loss: 0.00001084
Iteration 216/1000 | Loss: 0.00001084
Iteration 217/1000 | Loss: 0.00001083
Iteration 218/1000 | Loss: 0.00001083
Iteration 219/1000 | Loss: 0.00001083
Iteration 220/1000 | Loss: 0.00001083
Iteration 221/1000 | Loss: 0.00001083
Iteration 222/1000 | Loss: 0.00001083
Iteration 223/1000 | Loss: 0.00001083
Iteration 224/1000 | Loss: 0.00001083
Iteration 225/1000 | Loss: 0.00001083
Iteration 226/1000 | Loss: 0.00001083
Iteration 227/1000 | Loss: 0.00001083
Iteration 228/1000 | Loss: 0.00001083
Iteration 229/1000 | Loss: 0.00001083
Iteration 230/1000 | Loss: 0.00001083
Iteration 231/1000 | Loss: 0.00001083
Iteration 232/1000 | Loss: 0.00001083
Iteration 233/1000 | Loss: 0.00001083
Iteration 234/1000 | Loss: 0.00001083
Iteration 235/1000 | Loss: 0.00001083
Iteration 236/1000 | Loss: 0.00001083
Iteration 237/1000 | Loss: 0.00001083
Iteration 238/1000 | Loss: 0.00001083
Iteration 239/1000 | Loss: 0.00001083
Iteration 240/1000 | Loss: 0.00001083
Iteration 241/1000 | Loss: 0.00001083
Iteration 242/1000 | Loss: 0.00001083
Iteration 243/1000 | Loss: 0.00001083
Iteration 244/1000 | Loss: 0.00001083
Iteration 245/1000 | Loss: 0.00001083
Iteration 246/1000 | Loss: 0.00001083
Iteration 247/1000 | Loss: 0.00001083
Iteration 248/1000 | Loss: 0.00001083
Iteration 249/1000 | Loss: 0.00001083
Iteration 250/1000 | Loss: 0.00001083
Iteration 251/1000 | Loss: 0.00001083
Iteration 252/1000 | Loss: 0.00001083
Iteration 253/1000 | Loss: 0.00001083
Iteration 254/1000 | Loss: 0.00001083
Iteration 255/1000 | Loss: 0.00001083
Iteration 256/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.0832305633812211e-05, 1.0832305633812211e-05, 1.0832305633812211e-05, 1.0832305633812211e-05, 1.0832305633812211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0832305633812211e-05

Optimization complete. Final v2v error: 2.7867720127105713 mm

Highest mean error: 3.518052577972412 mm for frame 75

Lowest mean error: 2.4465646743774414 mm for frame 109

Saving results

Total time: 39.4699969291687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986197
Iteration 2/25 | Loss: 0.00718955
Iteration 3/25 | Loss: 0.00229195
Iteration 4/25 | Loss: 0.00172277
Iteration 5/25 | Loss: 0.00152648
Iteration 6/25 | Loss: 0.00160826
Iteration 7/25 | Loss: 0.00162526
Iteration 8/25 | Loss: 0.00132447
Iteration 9/25 | Loss: 0.00120245
Iteration 10/25 | Loss: 0.00116033
Iteration 11/25 | Loss: 0.00114508
Iteration 12/25 | Loss: 0.00112293
Iteration 13/25 | Loss: 0.00108161
Iteration 14/25 | Loss: 0.00107115
Iteration 15/25 | Loss: 0.00106059
Iteration 16/25 | Loss: 0.00104892
Iteration 17/25 | Loss: 0.00103923
Iteration 18/25 | Loss: 0.00103599
Iteration 19/25 | Loss: 0.00103499
Iteration 20/25 | Loss: 0.00103461
Iteration 21/25 | Loss: 0.00103440
Iteration 22/25 | Loss: 0.00103742
Iteration 23/25 | Loss: 0.00103326
Iteration 24/25 | Loss: 0.00103050
Iteration 25/25 | Loss: 0.00102905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41163611
Iteration 2/25 | Loss: 0.00092750
Iteration 3/25 | Loss: 0.00040077
Iteration 4/25 | Loss: 0.00040077
Iteration 5/25 | Loss: 0.00040077
Iteration 6/25 | Loss: 0.00040077
Iteration 7/25 | Loss: 0.00040077
Iteration 8/25 | Loss: 0.00040076
Iteration 9/25 | Loss: 0.00040076
Iteration 10/25 | Loss: 0.00040076
Iteration 11/25 | Loss: 0.00040076
Iteration 12/25 | Loss: 0.00040076
Iteration 13/25 | Loss: 0.00040076
Iteration 14/25 | Loss: 0.00040076
Iteration 15/25 | Loss: 0.00040076
Iteration 16/25 | Loss: 0.00040076
Iteration 17/25 | Loss: 0.00040076
Iteration 18/25 | Loss: 0.00040076
Iteration 19/25 | Loss: 0.00040076
Iteration 20/25 | Loss: 0.00040076
Iteration 21/25 | Loss: 0.00040076
Iteration 22/25 | Loss: 0.00040076
Iteration 23/25 | Loss: 0.00040076
Iteration 24/25 | Loss: 0.00040076
Iteration 25/25 | Loss: 0.00040076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040076
Iteration 2/1000 | Loss: 0.00014778
Iteration 3/1000 | Loss: 0.00046477
Iteration 4/1000 | Loss: 0.00003224
Iteration 5/1000 | Loss: 0.00037028
Iteration 6/1000 | Loss: 0.00002643
Iteration 7/1000 | Loss: 0.00036407
Iteration 8/1000 | Loss: 0.00002401
Iteration 9/1000 | Loss: 0.00002327
Iteration 10/1000 | Loss: 0.00019446
Iteration 11/1000 | Loss: 0.00002257
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00022401
Iteration 14/1000 | Loss: 0.00012333
Iteration 15/1000 | Loss: 0.00002199
Iteration 16/1000 | Loss: 0.00005558
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002123
Iteration 20/1000 | Loss: 0.00020974
Iteration 21/1000 | Loss: 0.00002734
Iteration 22/1000 | Loss: 0.00005999
Iteration 23/1000 | Loss: 0.00002110
Iteration 24/1000 | Loss: 0.00002089
Iteration 25/1000 | Loss: 0.00002086
Iteration 26/1000 | Loss: 0.00002084
Iteration 27/1000 | Loss: 0.00002079
Iteration 28/1000 | Loss: 0.00002079
Iteration 29/1000 | Loss: 0.00002077
Iteration 30/1000 | Loss: 0.00002076
Iteration 31/1000 | Loss: 0.00002075
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002074
Iteration 34/1000 | Loss: 0.00002074
Iteration 35/1000 | Loss: 0.00002073
Iteration 36/1000 | Loss: 0.00002072
Iteration 37/1000 | Loss: 0.00002072
Iteration 38/1000 | Loss: 0.00002071
Iteration 39/1000 | Loss: 0.00002070
Iteration 40/1000 | Loss: 0.00002067
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00002064
Iteration 43/1000 | Loss: 0.00002063
Iteration 44/1000 | Loss: 0.00002062
Iteration 45/1000 | Loss: 0.00002062
Iteration 46/1000 | Loss: 0.00002062
Iteration 47/1000 | Loss: 0.00002059
Iteration 48/1000 | Loss: 0.00002059
Iteration 49/1000 | Loss: 0.00002058
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002057
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002056
Iteration 60/1000 | Loss: 0.00002056
Iteration 61/1000 | Loss: 0.00002056
Iteration 62/1000 | Loss: 0.00002056
Iteration 63/1000 | Loss: 0.00002056
Iteration 64/1000 | Loss: 0.00002055
Iteration 65/1000 | Loss: 0.00002055
Iteration 66/1000 | Loss: 0.00002054
Iteration 67/1000 | Loss: 0.00002054
Iteration 68/1000 | Loss: 0.00002054
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002054
Iteration 72/1000 | Loss: 0.00002054
Iteration 73/1000 | Loss: 0.00002053
Iteration 74/1000 | Loss: 0.00002053
Iteration 75/1000 | Loss: 0.00002053
Iteration 76/1000 | Loss: 0.00002053
Iteration 77/1000 | Loss: 0.00002053
Iteration 78/1000 | Loss: 0.00002053
Iteration 79/1000 | Loss: 0.00002052
Iteration 80/1000 | Loss: 0.00002052
Iteration 81/1000 | Loss: 0.00002052
Iteration 82/1000 | Loss: 0.00002052
Iteration 83/1000 | Loss: 0.00002052
Iteration 84/1000 | Loss: 0.00002052
Iteration 85/1000 | Loss: 0.00002052
Iteration 86/1000 | Loss: 0.00002052
Iteration 87/1000 | Loss: 0.00002052
Iteration 88/1000 | Loss: 0.00002052
Iteration 89/1000 | Loss: 0.00002052
Iteration 90/1000 | Loss: 0.00002052
Iteration 91/1000 | Loss: 0.00002052
Iteration 92/1000 | Loss: 0.00002052
Iteration 93/1000 | Loss: 0.00002052
Iteration 94/1000 | Loss: 0.00002052
Iteration 95/1000 | Loss: 0.00002052
Iteration 96/1000 | Loss: 0.00002052
Iteration 97/1000 | Loss: 0.00002052
Iteration 98/1000 | Loss: 0.00002052
Iteration 99/1000 | Loss: 0.00002052
Iteration 100/1000 | Loss: 0.00002052
Iteration 101/1000 | Loss: 0.00002052
Iteration 102/1000 | Loss: 0.00002052
Iteration 103/1000 | Loss: 0.00002052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.0515810319921002e-05, 2.0515810319921002e-05, 2.0515810319921002e-05, 2.0515810319921002e-05, 2.0515810319921002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0515810319921002e-05

Optimization complete. Final v2v error: 3.8812508583068848 mm

Highest mean error: 4.52116584777832 mm for frame 45

Lowest mean error: 3.339637279510498 mm for frame 97

Saving results

Total time: 83.26044130325317
