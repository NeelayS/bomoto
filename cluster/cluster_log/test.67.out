Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=67, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3752-3807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876078
Iteration 2/25 | Loss: 0.00164366
Iteration 3/25 | Loss: 0.00141516
Iteration 4/25 | Loss: 0.00136928
Iteration 5/25 | Loss: 0.00137214
Iteration 6/25 | Loss: 0.00136949
Iteration 7/25 | Loss: 0.00135092
Iteration 8/25 | Loss: 0.00133279
Iteration 9/25 | Loss: 0.00131778
Iteration 10/25 | Loss: 0.00131289
Iteration 11/25 | Loss: 0.00130963
Iteration 12/25 | Loss: 0.00130605
Iteration 13/25 | Loss: 0.00131532
Iteration 14/25 | Loss: 0.00131821
Iteration 15/25 | Loss: 0.00131764
Iteration 16/25 | Loss: 0.00130769
Iteration 17/25 | Loss: 0.00130062
Iteration 18/25 | Loss: 0.00129895
Iteration 19/25 | Loss: 0.00129866
Iteration 20/25 | Loss: 0.00129859
Iteration 21/25 | Loss: 0.00129858
Iteration 22/25 | Loss: 0.00129857
Iteration 23/25 | Loss: 0.00129857
Iteration 24/25 | Loss: 0.00129857
Iteration 25/25 | Loss: 0.00129857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.23510504
Iteration 2/25 | Loss: 0.00132903
Iteration 3/25 | Loss: 0.00132902
Iteration 4/25 | Loss: 0.00132902
Iteration 5/25 | Loss: 0.00132902
Iteration 6/25 | Loss: 0.00132902
Iteration 7/25 | Loss: 0.00132902
Iteration 8/25 | Loss: 0.00132902
Iteration 9/25 | Loss: 0.00132902
Iteration 10/25 | Loss: 0.00132902
Iteration 11/25 | Loss: 0.00132902
Iteration 12/25 | Loss: 0.00132902
Iteration 13/25 | Loss: 0.00132902
Iteration 14/25 | Loss: 0.00132902
Iteration 15/25 | Loss: 0.00132902
Iteration 16/25 | Loss: 0.00132902
Iteration 17/25 | Loss: 0.00132902
Iteration 18/25 | Loss: 0.00132902
Iteration 19/25 | Loss: 0.00132902
Iteration 20/25 | Loss: 0.00132902
Iteration 21/25 | Loss: 0.00132902
Iteration 22/25 | Loss: 0.00132902
Iteration 23/25 | Loss: 0.00132902
Iteration 24/25 | Loss: 0.00132902
Iteration 25/25 | Loss: 0.00132902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132902
Iteration 2/1000 | Loss: 0.00014082
Iteration 3/1000 | Loss: 0.00030530
Iteration 4/1000 | Loss: 0.00004109
Iteration 5/1000 | Loss: 0.00003025
Iteration 6/1000 | Loss: 0.00002716
Iteration 7/1000 | Loss: 0.00002608
Iteration 8/1000 | Loss: 0.00002534
Iteration 9/1000 | Loss: 0.00002432
Iteration 10/1000 | Loss: 0.00002366
Iteration 11/1000 | Loss: 0.00002317
Iteration 12/1000 | Loss: 0.00002286
Iteration 13/1000 | Loss: 0.00002255
Iteration 14/1000 | Loss: 0.00002230
Iteration 15/1000 | Loss: 0.00002211
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002175
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002171
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002165
Iteration 22/1000 | Loss: 0.00002161
Iteration 23/1000 | Loss: 0.00002161
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002157
Iteration 26/1000 | Loss: 0.00002156
Iteration 27/1000 | Loss: 0.00002154
Iteration 28/1000 | Loss: 0.00002154
Iteration 29/1000 | Loss: 0.00002153
Iteration 30/1000 | Loss: 0.00002152
Iteration 31/1000 | Loss: 0.00002145
Iteration 32/1000 | Loss: 0.00002145
Iteration 33/1000 | Loss: 0.00002144
Iteration 34/1000 | Loss: 0.00002144
Iteration 35/1000 | Loss: 0.00002144
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002140
Iteration 39/1000 | Loss: 0.00002139
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002138
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002136
Iteration 48/1000 | Loss: 0.00002136
Iteration 49/1000 | Loss: 0.00002135
Iteration 50/1000 | Loss: 0.00002135
Iteration 51/1000 | Loss: 0.00002135
Iteration 52/1000 | Loss: 0.00002135
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002133
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002133
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002132
Iteration 66/1000 | Loss: 0.00002132
Iteration 67/1000 | Loss: 0.00002132
Iteration 68/1000 | Loss: 0.00002132
Iteration 69/1000 | Loss: 0.00002132
Iteration 70/1000 | Loss: 0.00002131
Iteration 71/1000 | Loss: 0.00002131
Iteration 72/1000 | Loss: 0.00002131
Iteration 73/1000 | Loss: 0.00002130
Iteration 74/1000 | Loss: 0.00002130
Iteration 75/1000 | Loss: 0.00002129
Iteration 76/1000 | Loss: 0.00002129
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002127
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002126
Iteration 85/1000 | Loss: 0.00002126
Iteration 86/1000 | Loss: 0.00002126
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002125
Iteration 91/1000 | Loss: 0.00002125
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002124
Iteration 98/1000 | Loss: 0.00002124
Iteration 99/1000 | Loss: 0.00002124
Iteration 100/1000 | Loss: 0.00002124
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002124
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002123
Iteration 109/1000 | Loss: 0.00002123
Iteration 110/1000 | Loss: 0.00002123
Iteration 111/1000 | Loss: 0.00002123
Iteration 112/1000 | Loss: 0.00002123
Iteration 113/1000 | Loss: 0.00002123
Iteration 114/1000 | Loss: 0.00002123
Iteration 115/1000 | Loss: 0.00002122
Iteration 116/1000 | Loss: 0.00002122
Iteration 117/1000 | Loss: 0.00002122
Iteration 118/1000 | Loss: 0.00002122
Iteration 119/1000 | Loss: 0.00002121
Iteration 120/1000 | Loss: 0.00002121
Iteration 121/1000 | Loss: 0.00002121
Iteration 122/1000 | Loss: 0.00002120
Iteration 123/1000 | Loss: 0.00002120
Iteration 124/1000 | Loss: 0.00002120
Iteration 125/1000 | Loss: 0.00002120
Iteration 126/1000 | Loss: 0.00002120
Iteration 127/1000 | Loss: 0.00002119
Iteration 128/1000 | Loss: 0.00002119
Iteration 129/1000 | Loss: 0.00002119
Iteration 130/1000 | Loss: 0.00002119
Iteration 131/1000 | Loss: 0.00002119
Iteration 132/1000 | Loss: 0.00002119
Iteration 133/1000 | Loss: 0.00002119
Iteration 134/1000 | Loss: 0.00002119
Iteration 135/1000 | Loss: 0.00002118
Iteration 136/1000 | Loss: 0.00002118
Iteration 137/1000 | Loss: 0.00002118
Iteration 138/1000 | Loss: 0.00002118
Iteration 139/1000 | Loss: 0.00002118
Iteration 140/1000 | Loss: 0.00002118
Iteration 141/1000 | Loss: 0.00002118
Iteration 142/1000 | Loss: 0.00002118
Iteration 143/1000 | Loss: 0.00002117
Iteration 144/1000 | Loss: 0.00002117
Iteration 145/1000 | Loss: 0.00002117
Iteration 146/1000 | Loss: 0.00002117
Iteration 147/1000 | Loss: 0.00002117
Iteration 148/1000 | Loss: 0.00002117
Iteration 149/1000 | Loss: 0.00002117
Iteration 150/1000 | Loss: 0.00002116
Iteration 151/1000 | Loss: 0.00002116
Iteration 152/1000 | Loss: 0.00002116
Iteration 153/1000 | Loss: 0.00002116
Iteration 154/1000 | Loss: 0.00002116
Iteration 155/1000 | Loss: 0.00002116
Iteration 156/1000 | Loss: 0.00002115
Iteration 157/1000 | Loss: 0.00002115
Iteration 158/1000 | Loss: 0.00002115
Iteration 159/1000 | Loss: 0.00002115
Iteration 160/1000 | Loss: 0.00002115
Iteration 161/1000 | Loss: 0.00002115
Iteration 162/1000 | Loss: 0.00002115
Iteration 163/1000 | Loss: 0.00002115
Iteration 164/1000 | Loss: 0.00002115
Iteration 165/1000 | Loss: 0.00002115
Iteration 166/1000 | Loss: 0.00002115
Iteration 167/1000 | Loss: 0.00002114
Iteration 168/1000 | Loss: 0.00002114
Iteration 169/1000 | Loss: 0.00002114
Iteration 170/1000 | Loss: 0.00002114
Iteration 171/1000 | Loss: 0.00002114
Iteration 172/1000 | Loss: 0.00002114
Iteration 173/1000 | Loss: 0.00002114
Iteration 174/1000 | Loss: 0.00002114
Iteration 175/1000 | Loss: 0.00002114
Iteration 176/1000 | Loss: 0.00002114
Iteration 177/1000 | Loss: 0.00002114
Iteration 178/1000 | Loss: 0.00002114
Iteration 179/1000 | Loss: 0.00002114
Iteration 180/1000 | Loss: 0.00002114
Iteration 181/1000 | Loss: 0.00002114
Iteration 182/1000 | Loss: 0.00002114
Iteration 183/1000 | Loss: 0.00002114
Iteration 184/1000 | Loss: 0.00002114
Iteration 185/1000 | Loss: 0.00002114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.113623486366123e-05, 2.113623486366123e-05, 2.113623486366123e-05, 2.113623486366123e-05, 2.113623486366123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.113623486366123e-05

Optimization complete. Final v2v error: 3.8300600051879883 mm

Highest mean error: 5.8213629722595215 mm for frame 106

Lowest mean error: 3.252586603164673 mm for frame 70

Saving results

Total time: 78.05834913253784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987056
Iteration 2/25 | Loss: 0.00262102
Iteration 3/25 | Loss: 0.00198292
Iteration 4/25 | Loss: 0.00200798
Iteration 5/25 | Loss: 0.00185364
Iteration 6/25 | Loss: 0.00160227
Iteration 7/25 | Loss: 0.00144077
Iteration 8/25 | Loss: 0.00140965
Iteration 9/25 | Loss: 0.00139422
Iteration 10/25 | Loss: 0.00138677
Iteration 11/25 | Loss: 0.00137921
Iteration 12/25 | Loss: 0.00137495
Iteration 13/25 | Loss: 0.00137244
Iteration 14/25 | Loss: 0.00137591
Iteration 15/25 | Loss: 0.00136445
Iteration 16/25 | Loss: 0.00135972
Iteration 17/25 | Loss: 0.00136093
Iteration 18/25 | Loss: 0.00136245
Iteration 19/25 | Loss: 0.00135988
Iteration 20/25 | Loss: 0.00136176
Iteration 21/25 | Loss: 0.00136256
Iteration 22/25 | Loss: 0.00136153
Iteration 23/25 | Loss: 0.00136042
Iteration 24/25 | Loss: 0.00135916
Iteration 25/25 | Loss: 0.00135975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33740759
Iteration 2/25 | Loss: 0.00135833
Iteration 3/25 | Loss: 0.00131236
Iteration 4/25 | Loss: 0.00131236
Iteration 5/25 | Loss: 0.00131236
Iteration 6/25 | Loss: 0.00131236
Iteration 7/25 | Loss: 0.00131236
Iteration 8/25 | Loss: 0.00131236
Iteration 9/25 | Loss: 0.00131236
Iteration 10/25 | Loss: 0.00131236
Iteration 11/25 | Loss: 0.00131236
Iteration 12/25 | Loss: 0.00131236
Iteration 13/25 | Loss: 0.00131236
Iteration 14/25 | Loss: 0.00131236
Iteration 15/25 | Loss: 0.00131236
Iteration 16/25 | Loss: 0.00131236
Iteration 17/25 | Loss: 0.00131236
Iteration 18/25 | Loss: 0.00131236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013123604003340006, 0.0013123604003340006, 0.0013123604003340006, 0.0013123604003340006, 0.0013123604003340006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013123604003340006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131236
Iteration 2/1000 | Loss: 0.00090897
Iteration 3/1000 | Loss: 0.00036056
Iteration 4/1000 | Loss: 0.00050029
Iteration 5/1000 | Loss: 0.00019071
Iteration 6/1000 | Loss: 0.00052356
Iteration 7/1000 | Loss: 0.00035326
Iteration 8/1000 | Loss: 0.00040138
Iteration 9/1000 | Loss: 0.00010448
Iteration 10/1000 | Loss: 0.00044009
Iteration 11/1000 | Loss: 0.00006488
Iteration 12/1000 | Loss: 0.00035883
Iteration 13/1000 | Loss: 0.00057713
Iteration 14/1000 | Loss: 0.00049894
Iteration 15/1000 | Loss: 0.00047055
Iteration 16/1000 | Loss: 0.00035754
Iteration 17/1000 | Loss: 0.00031003
Iteration 18/1000 | Loss: 0.00013155
Iteration 19/1000 | Loss: 0.00008809
Iteration 20/1000 | Loss: 0.00017663
Iteration 21/1000 | Loss: 0.00010962
Iteration 22/1000 | Loss: 0.00065778
Iteration 23/1000 | Loss: 0.00036826
Iteration 24/1000 | Loss: 0.00081073
Iteration 25/1000 | Loss: 0.00025724
Iteration 26/1000 | Loss: 0.00015305
Iteration 27/1000 | Loss: 0.00038614
Iteration 28/1000 | Loss: 0.00036087
Iteration 29/1000 | Loss: 0.00033613
Iteration 30/1000 | Loss: 0.00051577
Iteration 31/1000 | Loss: 0.00156550
Iteration 32/1000 | Loss: 0.00049266
Iteration 33/1000 | Loss: 0.00055767
Iteration 34/1000 | Loss: 0.00075024
Iteration 35/1000 | Loss: 0.00056893
Iteration 36/1000 | Loss: 0.00081288
Iteration 37/1000 | Loss: 0.00057556
Iteration 38/1000 | Loss: 0.00032465
Iteration 39/1000 | Loss: 0.00045257
Iteration 40/1000 | Loss: 0.00046534
Iteration 41/1000 | Loss: 0.00081737
Iteration 42/1000 | Loss: 0.00049036
Iteration 43/1000 | Loss: 0.00050896
Iteration 44/1000 | Loss: 0.00042978
Iteration 45/1000 | Loss: 0.00075485
Iteration 46/1000 | Loss: 0.00035857
Iteration 47/1000 | Loss: 0.00078183
Iteration 48/1000 | Loss: 0.00015938
Iteration 49/1000 | Loss: 0.00059454
Iteration 50/1000 | Loss: 0.00005991
Iteration 51/1000 | Loss: 0.00005279
Iteration 52/1000 | Loss: 0.00007502
Iteration 53/1000 | Loss: 0.00005392
Iteration 54/1000 | Loss: 0.00036704
Iteration 55/1000 | Loss: 0.00019568
Iteration 56/1000 | Loss: 0.00005947
Iteration 57/1000 | Loss: 0.00048814
Iteration 58/1000 | Loss: 0.00021481
Iteration 59/1000 | Loss: 0.00006631
Iteration 60/1000 | Loss: 0.00053224
Iteration 61/1000 | Loss: 0.00006179
Iteration 62/1000 | Loss: 0.00008268
Iteration 63/1000 | Loss: 0.00012314
Iteration 64/1000 | Loss: 0.00048585
Iteration 65/1000 | Loss: 0.00012036
Iteration 66/1000 | Loss: 0.00005435
Iteration 67/1000 | Loss: 0.00012919
Iteration 68/1000 | Loss: 0.00008018
Iteration 69/1000 | Loss: 0.00005065
Iteration 70/1000 | Loss: 0.00005881
Iteration 71/1000 | Loss: 0.00004541
Iteration 72/1000 | Loss: 0.00005558
Iteration 73/1000 | Loss: 0.00005087
Iteration 74/1000 | Loss: 0.00003906
Iteration 75/1000 | Loss: 0.00004810
Iteration 76/1000 | Loss: 0.00015953
Iteration 77/1000 | Loss: 0.00013866
Iteration 78/1000 | Loss: 0.00004797
Iteration 79/1000 | Loss: 0.00004391
Iteration 80/1000 | Loss: 0.00013775
Iteration 81/1000 | Loss: 0.00005211
Iteration 82/1000 | Loss: 0.00003567
Iteration 83/1000 | Loss: 0.00004624
Iteration 84/1000 | Loss: 0.00003452
Iteration 85/1000 | Loss: 0.00004246
Iteration 86/1000 | Loss: 0.00004026
Iteration 87/1000 | Loss: 0.00019167
Iteration 88/1000 | Loss: 0.00013879
Iteration 89/1000 | Loss: 0.00021021
Iteration 90/1000 | Loss: 0.00018061
Iteration 91/1000 | Loss: 0.00086081
Iteration 92/1000 | Loss: 0.00009322
Iteration 93/1000 | Loss: 0.00003509
Iteration 94/1000 | Loss: 0.00019313
Iteration 95/1000 | Loss: 0.00003617
Iteration 96/1000 | Loss: 0.00003532
Iteration 97/1000 | Loss: 0.00004709
Iteration 98/1000 | Loss: 0.00004621
Iteration 99/1000 | Loss: 0.00004237
Iteration 100/1000 | Loss: 0.00003751
Iteration 101/1000 | Loss: 0.00003801
Iteration 102/1000 | Loss: 0.00004191
Iteration 103/1000 | Loss: 0.00003802
Iteration 104/1000 | Loss: 0.00004118
Iteration 105/1000 | Loss: 0.00003228
Iteration 106/1000 | Loss: 0.00004074
Iteration 107/1000 | Loss: 0.00003045
Iteration 108/1000 | Loss: 0.00003638
Iteration 109/1000 | Loss: 0.00006412
Iteration 110/1000 | Loss: 0.00005447
Iteration 111/1000 | Loss: 0.00004529
Iteration 112/1000 | Loss: 0.00004581
Iteration 113/1000 | Loss: 0.00004310
Iteration 114/1000 | Loss: 0.00003959
Iteration 115/1000 | Loss: 0.00004203
Iteration 116/1000 | Loss: 0.00004182
Iteration 117/1000 | Loss: 0.00005377
Iteration 118/1000 | Loss: 0.00004087
Iteration 119/1000 | Loss: 0.00005270
Iteration 120/1000 | Loss: 0.00005909
Iteration 121/1000 | Loss: 0.00005450
Iteration 122/1000 | Loss: 0.00003705
Iteration 123/1000 | Loss: 0.00005059
Iteration 124/1000 | Loss: 0.00003786
Iteration 125/1000 | Loss: 0.00003751
Iteration 126/1000 | Loss: 0.00004574
Iteration 127/1000 | Loss: 0.00003668
Iteration 128/1000 | Loss: 0.00003646
Iteration 129/1000 | Loss: 0.00005346
Iteration 130/1000 | Loss: 0.00004921
Iteration 131/1000 | Loss: 0.00005130
Iteration 132/1000 | Loss: 0.00005228
Iteration 133/1000 | Loss: 0.00005203
Iteration 134/1000 | Loss: 0.00005295
Iteration 135/1000 | Loss: 0.00005010
Iteration 136/1000 | Loss: 0.00003694
Iteration 137/1000 | Loss: 0.00004596
Iteration 138/1000 | Loss: 0.00004659
Iteration 139/1000 | Loss: 0.00005271
Iteration 140/1000 | Loss: 0.00004026
Iteration 141/1000 | Loss: 0.00003827
Iteration 142/1000 | Loss: 0.00003588
Iteration 143/1000 | Loss: 0.00003230
Iteration 144/1000 | Loss: 0.00003879
Iteration 145/1000 | Loss: 0.00003254
Iteration 146/1000 | Loss: 0.00003393
Iteration 147/1000 | Loss: 0.00004005
Iteration 148/1000 | Loss: 0.00003883
Iteration 149/1000 | Loss: 0.00009640
Iteration 150/1000 | Loss: 0.00005055
Iteration 151/1000 | Loss: 0.00004423
Iteration 152/1000 | Loss: 0.00004918
Iteration 153/1000 | Loss: 0.00003974
Iteration 154/1000 | Loss: 0.00003996
Iteration 155/1000 | Loss: 0.00004201
Iteration 156/1000 | Loss: 0.00003743
Iteration 157/1000 | Loss: 0.00003476
Iteration 158/1000 | Loss: 0.00003708
Iteration 159/1000 | Loss: 0.00003596
Iteration 160/1000 | Loss: 0.00003849
Iteration 161/1000 | Loss: 0.00003450
Iteration 162/1000 | Loss: 0.00003945
Iteration 163/1000 | Loss: 0.00003594
Iteration 164/1000 | Loss: 0.00009987
Iteration 165/1000 | Loss: 0.00018292
Iteration 166/1000 | Loss: 0.00034020
Iteration 167/1000 | Loss: 0.00005998
Iteration 168/1000 | Loss: 0.00002995
Iteration 169/1000 | Loss: 0.00003492
Iteration 170/1000 | Loss: 0.00003424
Iteration 171/1000 | Loss: 0.00003198
Iteration 172/1000 | Loss: 0.00004001
Iteration 173/1000 | Loss: 0.00003384
Iteration 174/1000 | Loss: 0.00010505
Iteration 175/1000 | Loss: 0.00003768
Iteration 176/1000 | Loss: 0.00004187
Iteration 177/1000 | Loss: 0.00005169
Iteration 178/1000 | Loss: 0.00004043
Iteration 179/1000 | Loss: 0.00003931
Iteration 180/1000 | Loss: 0.00004126
Iteration 181/1000 | Loss: 0.00004154
Iteration 182/1000 | Loss: 0.00010758
Iteration 183/1000 | Loss: 0.00004448
Iteration 184/1000 | Loss: 0.00006098
Iteration 185/1000 | Loss: 0.00005169
Iteration 186/1000 | Loss: 0.00003582
Iteration 187/1000 | Loss: 0.00020161
Iteration 188/1000 | Loss: 0.00006403
Iteration 189/1000 | Loss: 0.00004842
Iteration 190/1000 | Loss: 0.00012356
Iteration 191/1000 | Loss: 0.00003079
Iteration 192/1000 | Loss: 0.00014115
Iteration 193/1000 | Loss: 0.00004999
Iteration 194/1000 | Loss: 0.00003596
Iteration 195/1000 | Loss: 0.00002788
Iteration 196/1000 | Loss: 0.00015452
Iteration 197/1000 | Loss: 0.00003543
Iteration 198/1000 | Loss: 0.00008903
Iteration 199/1000 | Loss: 0.00003051
Iteration 200/1000 | Loss: 0.00002966
Iteration 201/1000 | Loss: 0.00002904
Iteration 202/1000 | Loss: 0.00014461
Iteration 203/1000 | Loss: 0.00004039
Iteration 204/1000 | Loss: 0.00002944
Iteration 205/1000 | Loss: 0.00002676
Iteration 206/1000 | Loss: 0.00002638
Iteration 207/1000 | Loss: 0.00003768
Iteration 208/1000 | Loss: 0.00017149
Iteration 209/1000 | Loss: 0.00005827
Iteration 210/1000 | Loss: 0.00002553
Iteration 211/1000 | Loss: 0.00002522
Iteration 212/1000 | Loss: 0.00002509
Iteration 213/1000 | Loss: 0.00002493
Iteration 214/1000 | Loss: 0.00002484
Iteration 215/1000 | Loss: 0.00002483
Iteration 216/1000 | Loss: 0.00002483
Iteration 217/1000 | Loss: 0.00002482
Iteration 218/1000 | Loss: 0.00002482
Iteration 219/1000 | Loss: 0.00002482
Iteration 220/1000 | Loss: 0.00002482
Iteration 221/1000 | Loss: 0.00002481
Iteration 222/1000 | Loss: 0.00002481
Iteration 223/1000 | Loss: 0.00002481
Iteration 224/1000 | Loss: 0.00002481
Iteration 225/1000 | Loss: 0.00002481
Iteration 226/1000 | Loss: 0.00002481
Iteration 227/1000 | Loss: 0.00002480
Iteration 228/1000 | Loss: 0.00002480
Iteration 229/1000 | Loss: 0.00002479
Iteration 230/1000 | Loss: 0.00002478
Iteration 231/1000 | Loss: 0.00002478
Iteration 232/1000 | Loss: 0.00002478
Iteration 233/1000 | Loss: 0.00002478
Iteration 234/1000 | Loss: 0.00002478
Iteration 235/1000 | Loss: 0.00002477
Iteration 236/1000 | Loss: 0.00002477
Iteration 237/1000 | Loss: 0.00002477
Iteration 238/1000 | Loss: 0.00002477
Iteration 239/1000 | Loss: 0.00002477
Iteration 240/1000 | Loss: 0.00002476
Iteration 241/1000 | Loss: 0.00002476
Iteration 242/1000 | Loss: 0.00002475
Iteration 243/1000 | Loss: 0.00002475
Iteration 244/1000 | Loss: 0.00002475
Iteration 245/1000 | Loss: 0.00002475
Iteration 246/1000 | Loss: 0.00002474
Iteration 247/1000 | Loss: 0.00002474
Iteration 248/1000 | Loss: 0.00002474
Iteration 249/1000 | Loss: 0.00002473
Iteration 250/1000 | Loss: 0.00002473
Iteration 251/1000 | Loss: 0.00002473
Iteration 252/1000 | Loss: 0.00002472
Iteration 253/1000 | Loss: 0.00002472
Iteration 254/1000 | Loss: 0.00002472
Iteration 255/1000 | Loss: 0.00002472
Iteration 256/1000 | Loss: 0.00002472
Iteration 257/1000 | Loss: 0.00002471
Iteration 258/1000 | Loss: 0.00002470
Iteration 259/1000 | Loss: 0.00002470
Iteration 260/1000 | Loss: 0.00002469
Iteration 261/1000 | Loss: 0.00002469
Iteration 262/1000 | Loss: 0.00002468
Iteration 263/1000 | Loss: 0.00002468
Iteration 264/1000 | Loss: 0.00002467
Iteration 265/1000 | Loss: 0.00002467
Iteration 266/1000 | Loss: 0.00002467
Iteration 267/1000 | Loss: 0.00002467
Iteration 268/1000 | Loss: 0.00002467
Iteration 269/1000 | Loss: 0.00002467
Iteration 270/1000 | Loss: 0.00002467
Iteration 271/1000 | Loss: 0.00002467
Iteration 272/1000 | Loss: 0.00002467
Iteration 273/1000 | Loss: 0.00002466
Iteration 274/1000 | Loss: 0.00002466
Iteration 275/1000 | Loss: 0.00002466
Iteration 276/1000 | Loss: 0.00002466
Iteration 277/1000 | Loss: 0.00002465
Iteration 278/1000 | Loss: 0.00002465
Iteration 279/1000 | Loss: 0.00002465
Iteration 280/1000 | Loss: 0.00002465
Iteration 281/1000 | Loss: 0.00002465
Iteration 282/1000 | Loss: 0.00002465
Iteration 283/1000 | Loss: 0.00002465
Iteration 284/1000 | Loss: 0.00002465
Iteration 285/1000 | Loss: 0.00002465
Iteration 286/1000 | Loss: 0.00002465
Iteration 287/1000 | Loss: 0.00002465
Iteration 288/1000 | Loss: 0.00002465
Iteration 289/1000 | Loss: 0.00002465
Iteration 290/1000 | Loss: 0.00002464
Iteration 291/1000 | Loss: 0.00002464
Iteration 292/1000 | Loss: 0.00002464
Iteration 293/1000 | Loss: 0.00002464
Iteration 294/1000 | Loss: 0.00002464
Iteration 295/1000 | Loss: 0.00002463
Iteration 296/1000 | Loss: 0.00002463
Iteration 297/1000 | Loss: 0.00002463
Iteration 298/1000 | Loss: 0.00002463
Iteration 299/1000 | Loss: 0.00002463
Iteration 300/1000 | Loss: 0.00002463
Iteration 301/1000 | Loss: 0.00002463
Iteration 302/1000 | Loss: 0.00002463
Iteration 303/1000 | Loss: 0.00002463
Iteration 304/1000 | Loss: 0.00002463
Iteration 305/1000 | Loss: 0.00002463
Iteration 306/1000 | Loss: 0.00002463
Iteration 307/1000 | Loss: 0.00002463
Iteration 308/1000 | Loss: 0.00002463
Iteration 309/1000 | Loss: 0.00002463
Iteration 310/1000 | Loss: 0.00002463
Iteration 311/1000 | Loss: 0.00002463
Iteration 312/1000 | Loss: 0.00002463
Iteration 313/1000 | Loss: 0.00002463
Iteration 314/1000 | Loss: 0.00002463
Iteration 315/1000 | Loss: 0.00002463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [2.4631224732729606e-05, 2.4631224732729606e-05, 2.4631224732729606e-05, 2.4631224732729606e-05, 2.4631224732729606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4631224732729606e-05

Optimization complete. Final v2v error: 3.9521782398223877 mm

Highest mean error: 11.326016426086426 mm for frame 239

Lowest mean error: 3.3410539627075195 mm for frame 60

Saving results

Total time: 394.9817245006561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034840
Iteration 2/25 | Loss: 0.00313220
Iteration 3/25 | Loss: 0.00171708
Iteration 4/25 | Loss: 0.00144133
Iteration 5/25 | Loss: 0.00137548
Iteration 6/25 | Loss: 0.00136042
Iteration 7/25 | Loss: 0.00138438
Iteration 8/25 | Loss: 0.00135160
Iteration 9/25 | Loss: 0.00131706
Iteration 10/25 | Loss: 0.00130829
Iteration 11/25 | Loss: 0.00131191
Iteration 12/25 | Loss: 0.00131361
Iteration 13/25 | Loss: 0.00131433
Iteration 14/25 | Loss: 0.00130602
Iteration 15/25 | Loss: 0.00130340
Iteration 16/25 | Loss: 0.00128495
Iteration 17/25 | Loss: 0.00127155
Iteration 18/25 | Loss: 0.00127114
Iteration 19/25 | Loss: 0.00126980
Iteration 20/25 | Loss: 0.00126323
Iteration 21/25 | Loss: 0.00126299
Iteration 22/25 | Loss: 0.00125937
Iteration 23/25 | Loss: 0.00126233
Iteration 24/25 | Loss: 0.00125747
Iteration 25/25 | Loss: 0.00126316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77122521
Iteration 2/25 | Loss: 0.00162613
Iteration 3/25 | Loss: 0.00162612
Iteration 4/25 | Loss: 0.00155216
Iteration 5/25 | Loss: 0.00155216
Iteration 6/25 | Loss: 0.00155216
Iteration 7/25 | Loss: 0.00155216
Iteration 8/25 | Loss: 0.00155216
Iteration 9/25 | Loss: 0.00155216
Iteration 10/25 | Loss: 0.00155215
Iteration 11/25 | Loss: 0.00155215
Iteration 12/25 | Loss: 0.00155215
Iteration 13/25 | Loss: 0.00155215
Iteration 14/25 | Loss: 0.00155215
Iteration 15/25 | Loss: 0.00155215
Iteration 16/25 | Loss: 0.00155215
Iteration 17/25 | Loss: 0.00155215
Iteration 18/25 | Loss: 0.00155215
Iteration 19/25 | Loss: 0.00155215
Iteration 20/25 | Loss: 0.00155215
Iteration 21/25 | Loss: 0.00155215
Iteration 22/25 | Loss: 0.00155215
Iteration 23/25 | Loss: 0.00155215
Iteration 24/25 | Loss: 0.00155215
Iteration 25/25 | Loss: 0.00155215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155215
Iteration 2/1000 | Loss: 0.00033278
Iteration 3/1000 | Loss: 0.00028891
Iteration 4/1000 | Loss: 0.00019711
Iteration 5/1000 | Loss: 0.00009440
Iteration 6/1000 | Loss: 0.00016173
Iteration 7/1000 | Loss: 0.00017622
Iteration 8/1000 | Loss: 0.00015759
Iteration 9/1000 | Loss: 0.00031539
Iteration 10/1000 | Loss: 0.00027089
Iteration 11/1000 | Loss: 0.00010205
Iteration 12/1000 | Loss: 0.00005781
Iteration 13/1000 | Loss: 0.00003761
Iteration 14/1000 | Loss: 0.00004424
Iteration 15/1000 | Loss: 0.00030407
Iteration 16/1000 | Loss: 0.00006685
Iteration 17/1000 | Loss: 0.00034103
Iteration 18/1000 | Loss: 0.00075125
Iteration 19/1000 | Loss: 0.00060494
Iteration 20/1000 | Loss: 0.00020861
Iteration 21/1000 | Loss: 0.00006155
Iteration 22/1000 | Loss: 0.00002941
Iteration 23/1000 | Loss: 0.00076284
Iteration 24/1000 | Loss: 0.00043495
Iteration 25/1000 | Loss: 0.00074951
Iteration 26/1000 | Loss: 0.00003981
Iteration 27/1000 | Loss: 0.00008613
Iteration 28/1000 | Loss: 0.00002652
Iteration 29/1000 | Loss: 0.00021765
Iteration 30/1000 | Loss: 0.00003626
Iteration 31/1000 | Loss: 0.00002530
Iteration 32/1000 | Loss: 0.00014368
Iteration 33/1000 | Loss: 0.00017798
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002050
Iteration 36/1000 | Loss: 0.00019259
Iteration 37/1000 | Loss: 0.00002371
Iteration 38/1000 | Loss: 0.00025989
Iteration 39/1000 | Loss: 0.00037910
Iteration 40/1000 | Loss: 0.00030384
Iteration 41/1000 | Loss: 0.00034388
Iteration 42/1000 | Loss: 0.00017064
Iteration 43/1000 | Loss: 0.00013461
Iteration 44/1000 | Loss: 0.00015724
Iteration 45/1000 | Loss: 0.00002519
Iteration 46/1000 | Loss: 0.00021429
Iteration 47/1000 | Loss: 0.00021247
Iteration 48/1000 | Loss: 0.00013626
Iteration 49/1000 | Loss: 0.00003407
Iteration 50/1000 | Loss: 0.00019476
Iteration 51/1000 | Loss: 0.00010760
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00049830
Iteration 54/1000 | Loss: 0.00071592
Iteration 55/1000 | Loss: 0.00065492
Iteration 56/1000 | Loss: 0.00041818
Iteration 57/1000 | Loss: 0.00023802
Iteration 58/1000 | Loss: 0.00022345
Iteration 59/1000 | Loss: 0.00002884
Iteration 60/1000 | Loss: 0.00003087
Iteration 61/1000 | Loss: 0.00071249
Iteration 62/1000 | Loss: 0.00023492
Iteration 63/1000 | Loss: 0.00054061
Iteration 64/1000 | Loss: 0.00033407
Iteration 65/1000 | Loss: 0.00055870
Iteration 66/1000 | Loss: 0.00099959
Iteration 67/1000 | Loss: 0.00043877
Iteration 68/1000 | Loss: 0.00024530
Iteration 69/1000 | Loss: 0.00034678
Iteration 70/1000 | Loss: 0.00004235
Iteration 71/1000 | Loss: 0.00040116
Iteration 72/1000 | Loss: 0.00044619
Iteration 73/1000 | Loss: 0.00031107
Iteration 74/1000 | Loss: 0.00031713
Iteration 75/1000 | Loss: 0.00044306
Iteration 76/1000 | Loss: 0.00029036
Iteration 77/1000 | Loss: 0.00022980
Iteration 78/1000 | Loss: 0.00038667
Iteration 79/1000 | Loss: 0.00034499
Iteration 80/1000 | Loss: 0.00013026
Iteration 81/1000 | Loss: 0.00015446
Iteration 82/1000 | Loss: 0.00008953
Iteration 83/1000 | Loss: 0.00068263
Iteration 84/1000 | Loss: 0.00097209
Iteration 85/1000 | Loss: 0.00025859
Iteration 86/1000 | Loss: 0.00010251
Iteration 87/1000 | Loss: 0.00017801
Iteration 88/1000 | Loss: 0.00002693
Iteration 89/1000 | Loss: 0.00005807
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00002042
Iteration 92/1000 | Loss: 0.00002419
Iteration 93/1000 | Loss: 0.00001853
Iteration 94/1000 | Loss: 0.00003051
Iteration 95/1000 | Loss: 0.00001716
Iteration 96/1000 | Loss: 0.00001640
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001558
Iteration 99/1000 | Loss: 0.00001532
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001492
Iteration 105/1000 | Loss: 0.00001486
Iteration 106/1000 | Loss: 0.00001485
Iteration 107/1000 | Loss: 0.00001485
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001480
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001478
Iteration 113/1000 | Loss: 0.00001478
Iteration 114/1000 | Loss: 0.00001477
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001470
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001469
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001475
Iteration 136/1000 | Loss: 0.00026399
Iteration 137/1000 | Loss: 0.00013509
Iteration 138/1000 | Loss: 0.00002345
Iteration 139/1000 | Loss: 0.00003308
Iteration 140/1000 | Loss: 0.00001707
Iteration 141/1000 | Loss: 0.00019640
Iteration 142/1000 | Loss: 0.00001992
Iteration 143/1000 | Loss: 0.00008493
Iteration 144/1000 | Loss: 0.00002164
Iteration 145/1000 | Loss: 0.00001795
Iteration 146/1000 | Loss: 0.00001633
Iteration 147/1000 | Loss: 0.00001792
Iteration 148/1000 | Loss: 0.00001558
Iteration 149/1000 | Loss: 0.00023721
Iteration 150/1000 | Loss: 0.00012997
Iteration 151/1000 | Loss: 0.00001710
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001535
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001513
Iteration 157/1000 | Loss: 0.00023688
Iteration 158/1000 | Loss: 0.00010776
Iteration 159/1000 | Loss: 0.00005465
Iteration 160/1000 | Loss: 0.00001881
Iteration 161/1000 | Loss: 0.00007746
Iteration 162/1000 | Loss: 0.00026364
Iteration 163/1000 | Loss: 0.00006744
Iteration 164/1000 | Loss: 0.00002084
Iteration 165/1000 | Loss: 0.00002444
Iteration 166/1000 | Loss: 0.00001539
Iteration 167/1000 | Loss: 0.00001609
Iteration 168/1000 | Loss: 0.00001432
Iteration 169/1000 | Loss: 0.00001410
Iteration 170/1000 | Loss: 0.00001399
Iteration 171/1000 | Loss: 0.00001399
Iteration 172/1000 | Loss: 0.00001398
Iteration 173/1000 | Loss: 0.00001398
Iteration 174/1000 | Loss: 0.00001397
Iteration 175/1000 | Loss: 0.00001394
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001392
Iteration 179/1000 | Loss: 0.00001392
Iteration 180/1000 | Loss: 0.00001392
Iteration 181/1000 | Loss: 0.00001392
Iteration 182/1000 | Loss: 0.00001392
Iteration 183/1000 | Loss: 0.00001392
Iteration 184/1000 | Loss: 0.00001391
Iteration 185/1000 | Loss: 0.00001384
Iteration 186/1000 | Loss: 0.00001381
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001436
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001373
Iteration 192/1000 | Loss: 0.00001373
Iteration 193/1000 | Loss: 0.00001373
Iteration 194/1000 | Loss: 0.00001373
Iteration 195/1000 | Loss: 0.00001373
Iteration 196/1000 | Loss: 0.00001373
Iteration 197/1000 | Loss: 0.00001373
Iteration 198/1000 | Loss: 0.00001373
Iteration 199/1000 | Loss: 0.00001373
Iteration 200/1000 | Loss: 0.00001373
Iteration 201/1000 | Loss: 0.00001373
Iteration 202/1000 | Loss: 0.00001373
Iteration 203/1000 | Loss: 0.00001373
Iteration 204/1000 | Loss: 0.00001373
Iteration 205/1000 | Loss: 0.00001373
Iteration 206/1000 | Loss: 0.00001373
Iteration 207/1000 | Loss: 0.00001373
Iteration 208/1000 | Loss: 0.00001373
Iteration 209/1000 | Loss: 0.00001373
Iteration 210/1000 | Loss: 0.00001373
Iteration 211/1000 | Loss: 0.00001373
Iteration 212/1000 | Loss: 0.00001373
Iteration 213/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.372760107187787e-05, 1.372760107187787e-05, 1.372760107187787e-05, 1.372760107187787e-05, 1.372760107187787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.372760107187787e-05

Optimization complete. Final v2v error: 3.148904800415039 mm

Highest mean error: 4.595214366912842 mm for frame 94

Lowest mean error: 2.945023775100708 mm for frame 148

Saving results

Total time: 242.05039763450623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018345
Iteration 2/25 | Loss: 0.00258677
Iteration 3/25 | Loss: 0.00194119
Iteration 4/25 | Loss: 0.00203035
Iteration 5/25 | Loss: 0.00179954
Iteration 6/25 | Loss: 0.00163717
Iteration 7/25 | Loss: 0.00152622
Iteration 8/25 | Loss: 0.00147926
Iteration 9/25 | Loss: 0.00137783
Iteration 10/25 | Loss: 0.00140073
Iteration 11/25 | Loss: 0.00137040
Iteration 12/25 | Loss: 0.00135114
Iteration 13/25 | Loss: 0.00131021
Iteration 14/25 | Loss: 0.00130100
Iteration 15/25 | Loss: 0.00129776
Iteration 16/25 | Loss: 0.00129834
Iteration 17/25 | Loss: 0.00129527
Iteration 18/25 | Loss: 0.00129298
Iteration 19/25 | Loss: 0.00129140
Iteration 20/25 | Loss: 0.00129153
Iteration 21/25 | Loss: 0.00129321
Iteration 22/25 | Loss: 0.00129134
Iteration 23/25 | Loss: 0.00129202
Iteration 24/25 | Loss: 0.00128891
Iteration 25/25 | Loss: 0.00129088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38642466
Iteration 2/25 | Loss: 0.00168994
Iteration 3/25 | Loss: 0.00168849
Iteration 4/25 | Loss: 0.00168849
Iteration 5/25 | Loss: 0.00168849
Iteration 6/25 | Loss: 0.00168849
Iteration 7/25 | Loss: 0.00168849
Iteration 8/25 | Loss: 0.00168849
Iteration 9/25 | Loss: 0.00168849
Iteration 10/25 | Loss: 0.00168849
Iteration 11/25 | Loss: 0.00168849
Iteration 12/25 | Loss: 0.00168849
Iteration 13/25 | Loss: 0.00168849
Iteration 14/25 | Loss: 0.00168849
Iteration 15/25 | Loss: 0.00168849
Iteration 16/25 | Loss: 0.00168849
Iteration 17/25 | Loss: 0.00168849
Iteration 18/25 | Loss: 0.00168849
Iteration 19/25 | Loss: 0.00168849
Iteration 20/25 | Loss: 0.00168849
Iteration 21/25 | Loss: 0.00168849
Iteration 22/25 | Loss: 0.00168849
Iteration 23/25 | Loss: 0.00168849
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001688485499471426, 0.001688485499471426, 0.001688485499471426, 0.001688485499471426, 0.001688485499471426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001688485499471426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168849
Iteration 2/1000 | Loss: 0.00111200
Iteration 3/1000 | Loss: 0.00012087
Iteration 4/1000 | Loss: 0.00032000
Iteration 5/1000 | Loss: 0.00009269
Iteration 6/1000 | Loss: 0.00006878
Iteration 7/1000 | Loss: 0.00007379
Iteration 8/1000 | Loss: 0.00028809
Iteration 9/1000 | Loss: 0.00005815
Iteration 10/1000 | Loss: 0.00065908
Iteration 11/1000 | Loss: 0.00006449
Iteration 12/1000 | Loss: 0.00005984
Iteration 13/1000 | Loss: 0.00005283
Iteration 14/1000 | Loss: 0.00005405
Iteration 15/1000 | Loss: 0.00006066
Iteration 16/1000 | Loss: 0.00006016
Iteration 17/1000 | Loss: 0.00004657
Iteration 18/1000 | Loss: 0.00012657
Iteration 19/1000 | Loss: 0.00004125
Iteration 20/1000 | Loss: 0.00008868
Iteration 21/1000 | Loss: 0.00012323
Iteration 22/1000 | Loss: 0.00005462
Iteration 23/1000 | Loss: 0.00041707
Iteration 24/1000 | Loss: 0.00005983
Iteration 25/1000 | Loss: 0.00059581
Iteration 26/1000 | Loss: 0.00034469
Iteration 27/1000 | Loss: 0.00043351
Iteration 28/1000 | Loss: 0.00053692
Iteration 29/1000 | Loss: 0.00005866
Iteration 30/1000 | Loss: 0.00005126
Iteration 31/1000 | Loss: 0.00004222
Iteration 32/1000 | Loss: 0.00003878
Iteration 33/1000 | Loss: 0.00004684
Iteration 34/1000 | Loss: 0.00032717
Iteration 35/1000 | Loss: 0.00007153
Iteration 36/1000 | Loss: 0.00031295
Iteration 37/1000 | Loss: 0.00006344
Iteration 38/1000 | Loss: 0.00004090
Iteration 39/1000 | Loss: 0.00003480
Iteration 40/1000 | Loss: 0.00003105
Iteration 41/1000 | Loss: 0.00002862
Iteration 42/1000 | Loss: 0.00002697
Iteration 43/1000 | Loss: 0.00022175
Iteration 44/1000 | Loss: 0.00003034
Iteration 45/1000 | Loss: 0.00002553
Iteration 46/1000 | Loss: 0.00002280
Iteration 47/1000 | Loss: 0.00002064
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001771
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001524
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001474
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001469
Iteration 78/1000 | Loss: 0.00001468
Iteration 79/1000 | Loss: 0.00001467
Iteration 80/1000 | Loss: 0.00001464
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001456
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001452
Iteration 93/1000 | Loss: 0.00001452
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001450
Iteration 97/1000 | Loss: 0.00001450
Iteration 98/1000 | Loss: 0.00001450
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001450
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001449
Iteration 105/1000 | Loss: 0.00001449
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001449
Iteration 108/1000 | Loss: 0.00001449
Iteration 109/1000 | Loss: 0.00001449
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001449
Iteration 117/1000 | Loss: 0.00001449
Iteration 118/1000 | Loss: 0.00001448
Iteration 119/1000 | Loss: 0.00001448
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001447
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001446
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00001446
Iteration 138/1000 | Loss: 0.00001446
Iteration 139/1000 | Loss: 0.00001446
Iteration 140/1000 | Loss: 0.00001446
Iteration 141/1000 | Loss: 0.00001446
Iteration 142/1000 | Loss: 0.00001446
Iteration 143/1000 | Loss: 0.00001446
Iteration 144/1000 | Loss: 0.00001446
Iteration 145/1000 | Loss: 0.00001446
Iteration 146/1000 | Loss: 0.00001446
Iteration 147/1000 | Loss: 0.00001446
Iteration 148/1000 | Loss: 0.00001446
Iteration 149/1000 | Loss: 0.00001445
Iteration 150/1000 | Loss: 0.00001445
Iteration 151/1000 | Loss: 0.00001445
Iteration 152/1000 | Loss: 0.00001445
Iteration 153/1000 | Loss: 0.00001445
Iteration 154/1000 | Loss: 0.00001445
Iteration 155/1000 | Loss: 0.00001445
Iteration 156/1000 | Loss: 0.00001445
Iteration 157/1000 | Loss: 0.00001445
Iteration 158/1000 | Loss: 0.00001445
Iteration 159/1000 | Loss: 0.00001445
Iteration 160/1000 | Loss: 0.00001445
Iteration 161/1000 | Loss: 0.00001445
Iteration 162/1000 | Loss: 0.00001445
Iteration 163/1000 | Loss: 0.00001445
Iteration 164/1000 | Loss: 0.00001445
Iteration 165/1000 | Loss: 0.00001445
Iteration 166/1000 | Loss: 0.00001445
Iteration 167/1000 | Loss: 0.00001444
Iteration 168/1000 | Loss: 0.00001444
Iteration 169/1000 | Loss: 0.00001444
Iteration 170/1000 | Loss: 0.00001444
Iteration 171/1000 | Loss: 0.00001444
Iteration 172/1000 | Loss: 0.00001444
Iteration 173/1000 | Loss: 0.00001444
Iteration 174/1000 | Loss: 0.00001444
Iteration 175/1000 | Loss: 0.00001444
Iteration 176/1000 | Loss: 0.00001444
Iteration 177/1000 | Loss: 0.00001444
Iteration 178/1000 | Loss: 0.00001444
Iteration 179/1000 | Loss: 0.00001444
Iteration 180/1000 | Loss: 0.00001444
Iteration 181/1000 | Loss: 0.00001444
Iteration 182/1000 | Loss: 0.00001444
Iteration 183/1000 | Loss: 0.00001444
Iteration 184/1000 | Loss: 0.00001444
Iteration 185/1000 | Loss: 0.00001444
Iteration 186/1000 | Loss: 0.00001444
Iteration 187/1000 | Loss: 0.00001444
Iteration 188/1000 | Loss: 0.00001444
Iteration 189/1000 | Loss: 0.00001444
Iteration 190/1000 | Loss: 0.00001444
Iteration 191/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.4444851331063546e-05, 1.4444851331063546e-05, 1.4444851331063546e-05, 1.4444851331063546e-05, 1.4444851331063546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4444851331063546e-05

Optimization complete. Final v2v error: 3.2140424251556396 mm

Highest mean error: 4.816401481628418 mm for frame 77

Lowest mean error: 2.7173948287963867 mm for frame 113

Saving results

Total time: 134.52778601646423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958358
Iteration 2/25 | Loss: 0.00958358
Iteration 3/25 | Loss: 0.00958357
Iteration 4/25 | Loss: 0.00958357
Iteration 5/25 | Loss: 0.00958357
Iteration 6/25 | Loss: 0.00958357
Iteration 7/25 | Loss: 0.00958357
Iteration 8/25 | Loss: 0.00958357
Iteration 9/25 | Loss: 0.00958357
Iteration 10/25 | Loss: 0.00958357
Iteration 11/25 | Loss: 0.00958357
Iteration 12/25 | Loss: 0.00958357
Iteration 13/25 | Loss: 0.00958357
Iteration 14/25 | Loss: 0.00958356
Iteration 15/25 | Loss: 0.00958356
Iteration 16/25 | Loss: 0.00958356
Iteration 17/25 | Loss: 0.00958356
Iteration 18/25 | Loss: 0.00958356
Iteration 19/25 | Loss: 0.00958356
Iteration 20/25 | Loss: 0.00958356
Iteration 21/25 | Loss: 0.00958356
Iteration 22/25 | Loss: 0.00958356
Iteration 23/25 | Loss: 0.00958356
Iteration 24/25 | Loss: 0.00958356
Iteration 25/25 | Loss: 0.00958356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65302908
Iteration 2/25 | Loss: 0.15812768
Iteration 3/25 | Loss: 0.14950812
Iteration 4/25 | Loss: 0.14716160
Iteration 5/25 | Loss: 0.14710824
Iteration 6/25 | Loss: 0.14710824
Iteration 7/25 | Loss: 0.14710821
Iteration 8/25 | Loss: 0.14710820
Iteration 9/25 | Loss: 0.14710820
Iteration 10/25 | Loss: 0.14710820
Iteration 11/25 | Loss: 0.14710820
Iteration 12/25 | Loss: 0.14710820
Iteration 13/25 | Loss: 0.14710820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.14710819721221924, 0.14710819721221924, 0.14710819721221924, 0.14710819721221924, 0.14710819721221924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14710819721221924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14710820
Iteration 2/1000 | Loss: 0.00748893
Iteration 3/1000 | Loss: 0.00159223
Iteration 4/1000 | Loss: 0.00093760
Iteration 5/1000 | Loss: 0.00125992
Iteration 6/1000 | Loss: 0.00025061
Iteration 7/1000 | Loss: 0.00035819
Iteration 8/1000 | Loss: 0.00044679
Iteration 9/1000 | Loss: 0.00039066
Iteration 10/1000 | Loss: 0.00023278
Iteration 11/1000 | Loss: 0.00010037
Iteration 12/1000 | Loss: 0.00050346
Iteration 13/1000 | Loss: 0.00005879
Iteration 14/1000 | Loss: 0.00005394
Iteration 15/1000 | Loss: 0.00027215
Iteration 16/1000 | Loss: 0.00004244
Iteration 17/1000 | Loss: 0.00049296
Iteration 18/1000 | Loss: 0.00003579
Iteration 19/1000 | Loss: 0.00081674
Iteration 20/1000 | Loss: 0.00008231
Iteration 21/1000 | Loss: 0.00028139
Iteration 22/1000 | Loss: 0.00002951
Iteration 23/1000 | Loss: 0.00002741
Iteration 24/1000 | Loss: 0.00013324
Iteration 25/1000 | Loss: 0.00028451
Iteration 26/1000 | Loss: 0.00022460
Iteration 27/1000 | Loss: 0.00083814
Iteration 28/1000 | Loss: 0.00010904
Iteration 29/1000 | Loss: 0.00002499
Iteration 30/1000 | Loss: 0.00023048
Iteration 31/1000 | Loss: 0.00002331
Iteration 32/1000 | Loss: 0.00034054
Iteration 33/1000 | Loss: 0.00003328
Iteration 34/1000 | Loss: 0.00024798
Iteration 35/1000 | Loss: 0.00005671
Iteration 36/1000 | Loss: 0.00013122
Iteration 37/1000 | Loss: 0.00015735
Iteration 38/1000 | Loss: 0.00003314
Iteration 39/1000 | Loss: 0.00016179
Iteration 40/1000 | Loss: 0.00256710
Iteration 41/1000 | Loss: 0.00031389
Iteration 42/1000 | Loss: 0.00009683
Iteration 43/1000 | Loss: 0.00009332
Iteration 44/1000 | Loss: 0.00006450
Iteration 45/1000 | Loss: 0.00015785
Iteration 46/1000 | Loss: 0.00011507
Iteration 47/1000 | Loss: 0.00002434
Iteration 48/1000 | Loss: 0.00009062
Iteration 49/1000 | Loss: 0.00002104
Iteration 50/1000 | Loss: 0.00006775
Iteration 51/1000 | Loss: 0.00005012
Iteration 52/1000 | Loss: 0.00002313
Iteration 53/1000 | Loss: 0.00002030
Iteration 54/1000 | Loss: 0.00002025
Iteration 55/1000 | Loss: 0.00002189
Iteration 56/1000 | Loss: 0.00002021
Iteration 57/1000 | Loss: 0.00002021
Iteration 58/1000 | Loss: 0.00002019
Iteration 59/1000 | Loss: 0.00002019
Iteration 60/1000 | Loss: 0.00002018
Iteration 61/1000 | Loss: 0.00016434
Iteration 62/1000 | Loss: 0.00002308
Iteration 63/1000 | Loss: 0.00056470
Iteration 64/1000 | Loss: 0.00020413
Iteration 65/1000 | Loss: 0.00012199
Iteration 66/1000 | Loss: 0.00014754
Iteration 67/1000 | Loss: 0.00004405
Iteration 68/1000 | Loss: 0.00004562
Iteration 69/1000 | Loss: 0.00015721
Iteration 70/1000 | Loss: 0.00002268
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00004475
Iteration 73/1000 | Loss: 0.00001968
Iteration 74/1000 | Loss: 0.00001961
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00004081
Iteration 77/1000 | Loss: 0.00002040
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001957
Iteration 82/1000 | Loss: 0.00001957
Iteration 83/1000 | Loss: 0.00001956
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001951
Iteration 88/1000 | Loss: 0.00001950
Iteration 89/1000 | Loss: 0.00008445
Iteration 90/1000 | Loss: 0.00081220
Iteration 91/1000 | Loss: 0.00006892
Iteration 92/1000 | Loss: 0.00003924
Iteration 93/1000 | Loss: 0.00005539
Iteration 94/1000 | Loss: 0.00001977
Iteration 95/1000 | Loss: 0.00013305
Iteration 96/1000 | Loss: 0.00002039
Iteration 97/1000 | Loss: 0.00013700
Iteration 98/1000 | Loss: 0.00003144
Iteration 99/1000 | Loss: 0.00003572
Iteration 100/1000 | Loss: 0.00004121
Iteration 101/1000 | Loss: 0.00001965
Iteration 102/1000 | Loss: 0.00003187
Iteration 103/1000 | Loss: 0.00004316
Iteration 104/1000 | Loss: 0.00013868
Iteration 105/1000 | Loss: 0.00010368
Iteration 106/1000 | Loss: 0.00007383
Iteration 107/1000 | Loss: 0.00002580
Iteration 108/1000 | Loss: 0.00002394
Iteration 109/1000 | Loss: 0.00002090
Iteration 110/1000 | Loss: 0.00001944
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001939
Iteration 113/1000 | Loss: 0.00001939
Iteration 114/1000 | Loss: 0.00001938
Iteration 115/1000 | Loss: 0.00001938
Iteration 116/1000 | Loss: 0.00001938
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001936
Iteration 120/1000 | Loss: 0.00001936
Iteration 121/1000 | Loss: 0.00001936
Iteration 122/1000 | Loss: 0.00001935
Iteration 123/1000 | Loss: 0.00001935
Iteration 124/1000 | Loss: 0.00001935
Iteration 125/1000 | Loss: 0.00001935
Iteration 126/1000 | Loss: 0.00001934
Iteration 127/1000 | Loss: 0.00006730
Iteration 128/1000 | Loss: 0.00003544
Iteration 129/1000 | Loss: 0.00002631
Iteration 130/1000 | Loss: 0.00001945
Iteration 131/1000 | Loss: 0.00001936
Iteration 132/1000 | Loss: 0.00001935
Iteration 133/1000 | Loss: 0.00001934
Iteration 134/1000 | Loss: 0.00001934
Iteration 135/1000 | Loss: 0.00001933
Iteration 136/1000 | Loss: 0.00001933
Iteration 137/1000 | Loss: 0.00001932
Iteration 138/1000 | Loss: 0.00001932
Iteration 139/1000 | Loss: 0.00001932
Iteration 140/1000 | Loss: 0.00001931
Iteration 141/1000 | Loss: 0.00001931
Iteration 142/1000 | Loss: 0.00001930
Iteration 143/1000 | Loss: 0.00001929
Iteration 144/1000 | Loss: 0.00001929
Iteration 145/1000 | Loss: 0.00001929
Iteration 146/1000 | Loss: 0.00001928
Iteration 147/1000 | Loss: 0.00001928
Iteration 148/1000 | Loss: 0.00001928
Iteration 149/1000 | Loss: 0.00001927
Iteration 150/1000 | Loss: 0.00001927
Iteration 151/1000 | Loss: 0.00010020
Iteration 152/1000 | Loss: 0.00012215
Iteration 153/1000 | Loss: 0.00002006
Iteration 154/1000 | Loss: 0.00011661
Iteration 155/1000 | Loss: 0.00005897
Iteration 156/1000 | Loss: 0.00002624
Iteration 157/1000 | Loss: 0.00006426
Iteration 158/1000 | Loss: 0.00002353
Iteration 159/1000 | Loss: 0.00003226
Iteration 160/1000 | Loss: 0.00002153
Iteration 161/1000 | Loss: 0.00002143
Iteration 162/1000 | Loss: 0.00001940
Iteration 163/1000 | Loss: 0.00002484
Iteration 164/1000 | Loss: 0.00001938
Iteration 165/1000 | Loss: 0.00003049
Iteration 166/1000 | Loss: 0.00032255
Iteration 167/1000 | Loss: 0.00053880
Iteration 168/1000 | Loss: 0.00002228
Iteration 169/1000 | Loss: 0.00001935
Iteration 170/1000 | Loss: 0.00002647
Iteration 171/1000 | Loss: 0.00002647
Iteration 172/1000 | Loss: 0.00001976
Iteration 173/1000 | Loss: 0.00011050
Iteration 174/1000 | Loss: 0.00007550
Iteration 175/1000 | Loss: 0.00005117
Iteration 176/1000 | Loss: 0.00002062
Iteration 177/1000 | Loss: 0.00001946
Iteration 178/1000 | Loss: 0.00001930
Iteration 179/1000 | Loss: 0.00001930
Iteration 180/1000 | Loss: 0.00001929
Iteration 181/1000 | Loss: 0.00001928
Iteration 182/1000 | Loss: 0.00004630
Iteration 183/1000 | Loss: 0.00025168
Iteration 184/1000 | Loss: 0.00004707
Iteration 185/1000 | Loss: 0.00003096
Iteration 186/1000 | Loss: 0.00002205
Iteration 187/1000 | Loss: 0.00001928
Iteration 188/1000 | Loss: 0.00001927
Iteration 189/1000 | Loss: 0.00003495
Iteration 190/1000 | Loss: 0.00001935
Iteration 191/1000 | Loss: 0.00002536
Iteration 192/1000 | Loss: 0.00002650
Iteration 193/1000 | Loss: 0.00001973
Iteration 194/1000 | Loss: 0.00001934
Iteration 195/1000 | Loss: 0.00001930
Iteration 196/1000 | Loss: 0.00001930
Iteration 197/1000 | Loss: 0.00001929
Iteration 198/1000 | Loss: 0.00003755
Iteration 199/1000 | Loss: 0.00001962
Iteration 200/1000 | Loss: 0.00002052
Iteration 201/1000 | Loss: 0.00001929
Iteration 202/1000 | Loss: 0.00001929
Iteration 203/1000 | Loss: 0.00001929
Iteration 204/1000 | Loss: 0.00001928
Iteration 205/1000 | Loss: 0.00001928
Iteration 206/1000 | Loss: 0.00001927
Iteration 207/1000 | Loss: 0.00001927
Iteration 208/1000 | Loss: 0.00001924
Iteration 209/1000 | Loss: 0.00001923
Iteration 210/1000 | Loss: 0.00001923
Iteration 211/1000 | Loss: 0.00001923
Iteration 212/1000 | Loss: 0.00001922
Iteration 213/1000 | Loss: 0.00001922
Iteration 214/1000 | Loss: 0.00001921
Iteration 215/1000 | Loss: 0.00001921
Iteration 216/1000 | Loss: 0.00001921
Iteration 217/1000 | Loss: 0.00001920
Iteration 218/1000 | Loss: 0.00001920
Iteration 219/1000 | Loss: 0.00001920
Iteration 220/1000 | Loss: 0.00001920
Iteration 221/1000 | Loss: 0.00001920
Iteration 222/1000 | Loss: 0.00001920
Iteration 223/1000 | Loss: 0.00001919
Iteration 224/1000 | Loss: 0.00001919
Iteration 225/1000 | Loss: 0.00001919
Iteration 226/1000 | Loss: 0.00001919
Iteration 227/1000 | Loss: 0.00001919
Iteration 228/1000 | Loss: 0.00001919
Iteration 229/1000 | Loss: 0.00001919
Iteration 230/1000 | Loss: 0.00001919
Iteration 231/1000 | Loss: 0.00001919
Iteration 232/1000 | Loss: 0.00001919
Iteration 233/1000 | Loss: 0.00001919
Iteration 234/1000 | Loss: 0.00001919
Iteration 235/1000 | Loss: 0.00001919
Iteration 236/1000 | Loss: 0.00001919
Iteration 237/1000 | Loss: 0.00001919
Iteration 238/1000 | Loss: 0.00001919
Iteration 239/1000 | Loss: 0.00001919
Iteration 240/1000 | Loss: 0.00001919
Iteration 241/1000 | Loss: 0.00001918
Iteration 242/1000 | Loss: 0.00001918
Iteration 243/1000 | Loss: 0.00001918
Iteration 244/1000 | Loss: 0.00001918
Iteration 245/1000 | Loss: 0.00001918
Iteration 246/1000 | Loss: 0.00001918
Iteration 247/1000 | Loss: 0.00001918
Iteration 248/1000 | Loss: 0.00001918
Iteration 249/1000 | Loss: 0.00001918
Iteration 250/1000 | Loss: 0.00001918
Iteration 251/1000 | Loss: 0.00001918
Iteration 252/1000 | Loss: 0.00001917
Iteration 253/1000 | Loss: 0.00001917
Iteration 254/1000 | Loss: 0.00001917
Iteration 255/1000 | Loss: 0.00001917
Iteration 256/1000 | Loss: 0.00001917
Iteration 257/1000 | Loss: 0.00001917
Iteration 258/1000 | Loss: 0.00001917
Iteration 259/1000 | Loss: 0.00001917
Iteration 260/1000 | Loss: 0.00001917
Iteration 261/1000 | Loss: 0.00001917
Iteration 262/1000 | Loss: 0.00001917
Iteration 263/1000 | Loss: 0.00001917
Iteration 264/1000 | Loss: 0.00001917
Iteration 265/1000 | Loss: 0.00001917
Iteration 266/1000 | Loss: 0.00001917
Iteration 267/1000 | Loss: 0.00001917
Iteration 268/1000 | Loss: 0.00001917
Iteration 269/1000 | Loss: 0.00001917
Iteration 270/1000 | Loss: 0.00001917
Iteration 271/1000 | Loss: 0.00001917
Iteration 272/1000 | Loss: 0.00001917
Iteration 273/1000 | Loss: 0.00001917
Iteration 274/1000 | Loss: 0.00001917
Iteration 275/1000 | Loss: 0.00001917
Iteration 276/1000 | Loss: 0.00001917
Iteration 277/1000 | Loss: 0.00001917
Iteration 278/1000 | Loss: 0.00001917
Iteration 279/1000 | Loss: 0.00001917
Iteration 280/1000 | Loss: 0.00001917
Iteration 281/1000 | Loss: 0.00001917
Iteration 282/1000 | Loss: 0.00001917
Iteration 283/1000 | Loss: 0.00001917
Iteration 284/1000 | Loss: 0.00001917
Iteration 285/1000 | Loss: 0.00001917
Iteration 286/1000 | Loss: 0.00001917
Iteration 287/1000 | Loss: 0.00001917
Iteration 288/1000 | Loss: 0.00001917
Iteration 289/1000 | Loss: 0.00001917
Iteration 290/1000 | Loss: 0.00001917
Iteration 291/1000 | Loss: 0.00001917
Iteration 292/1000 | Loss: 0.00001917
Iteration 293/1000 | Loss: 0.00001917
Iteration 294/1000 | Loss: 0.00001917
Iteration 295/1000 | Loss: 0.00001917
Iteration 296/1000 | Loss: 0.00001917
Iteration 297/1000 | Loss: 0.00001917
Iteration 298/1000 | Loss: 0.00001917
Iteration 299/1000 | Loss: 0.00001917
Iteration 300/1000 | Loss: 0.00001917
Iteration 301/1000 | Loss: 0.00001917
Iteration 302/1000 | Loss: 0.00001917
Iteration 303/1000 | Loss: 0.00001917
Iteration 304/1000 | Loss: 0.00001917
Iteration 305/1000 | Loss: 0.00001917
Iteration 306/1000 | Loss: 0.00001917
Iteration 307/1000 | Loss: 0.00001917
Iteration 308/1000 | Loss: 0.00001917
Iteration 309/1000 | Loss: 0.00001917
Iteration 310/1000 | Loss: 0.00001917
Iteration 311/1000 | Loss: 0.00001917
Iteration 312/1000 | Loss: 0.00001917
Iteration 313/1000 | Loss: 0.00001917
Iteration 314/1000 | Loss: 0.00001917
Iteration 315/1000 | Loss: 0.00001917
Iteration 316/1000 | Loss: 0.00001917
Iteration 317/1000 | Loss: 0.00001917
Iteration 318/1000 | Loss: 0.00001917
Iteration 319/1000 | Loss: 0.00001917
Iteration 320/1000 | Loss: 0.00001917
Iteration 321/1000 | Loss: 0.00001917
Iteration 322/1000 | Loss: 0.00001917
Iteration 323/1000 | Loss: 0.00001917
Iteration 324/1000 | Loss: 0.00001917
Iteration 325/1000 | Loss: 0.00001917
Iteration 326/1000 | Loss: 0.00001917
Iteration 327/1000 | Loss: 0.00001917
Iteration 328/1000 | Loss: 0.00001917
Iteration 329/1000 | Loss: 0.00001917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [1.9173254258930683e-05, 1.9173254258930683e-05, 1.9173254258930683e-05, 1.9173254258930683e-05, 1.9173254258930683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9173254258930683e-05

Optimization complete. Final v2v error: 3.7502927780151367 mm

Highest mean error: 4.205812454223633 mm for frame 15

Lowest mean error: 3.425220251083374 mm for frame 86

Saving results

Total time: 225.85516738891602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387395
Iteration 2/25 | Loss: 0.00132776
Iteration 3/25 | Loss: 0.00122899
Iteration 4/25 | Loss: 0.00122227
Iteration 5/25 | Loss: 0.00122156
Iteration 6/25 | Loss: 0.00122156
Iteration 7/25 | Loss: 0.00122156
Iteration 8/25 | Loss: 0.00122156
Iteration 9/25 | Loss: 0.00122156
Iteration 10/25 | Loss: 0.00122156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012215590104460716, 0.0012215590104460716, 0.0012215590104460716, 0.0012215590104460716, 0.0012215590104460716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012215590104460716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50723660
Iteration 2/25 | Loss: 0.00090427
Iteration 3/25 | Loss: 0.00090427
Iteration 4/25 | Loss: 0.00090427
Iteration 5/25 | Loss: 0.00090427
Iteration 6/25 | Loss: 0.00090427
Iteration 7/25 | Loss: 0.00090427
Iteration 8/25 | Loss: 0.00090427
Iteration 9/25 | Loss: 0.00090427
Iteration 10/25 | Loss: 0.00090427
Iteration 11/25 | Loss: 0.00090427
Iteration 12/25 | Loss: 0.00090427
Iteration 13/25 | Loss: 0.00090427
Iteration 14/25 | Loss: 0.00090427
Iteration 15/25 | Loss: 0.00090427
Iteration 16/25 | Loss: 0.00090427
Iteration 17/25 | Loss: 0.00090427
Iteration 18/25 | Loss: 0.00090427
Iteration 19/25 | Loss: 0.00090427
Iteration 20/25 | Loss: 0.00090427
Iteration 21/25 | Loss: 0.00090427
Iteration 22/25 | Loss: 0.00090427
Iteration 23/25 | Loss: 0.00090427
Iteration 24/25 | Loss: 0.00090427
Iteration 25/25 | Loss: 0.00090427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090427
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00001706
Iteration 4/1000 | Loss: 0.00001495
Iteration 5/1000 | Loss: 0.00001384
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001269
Iteration 8/1000 | Loss: 0.00001238
Iteration 9/1000 | Loss: 0.00001201
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001138
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001118
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001094
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001089
Iteration 48/1000 | Loss: 0.00001088
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001086
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001085
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001084
Iteration 57/1000 | Loss: 0.00001083
Iteration 58/1000 | Loss: 0.00001083
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001079
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001077
Iteration 68/1000 | Loss: 0.00001077
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001074
Iteration 74/1000 | Loss: 0.00001074
Iteration 75/1000 | Loss: 0.00001074
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001073
Iteration 79/1000 | Loss: 0.00001073
Iteration 80/1000 | Loss: 0.00001073
Iteration 81/1000 | Loss: 0.00001072
Iteration 82/1000 | Loss: 0.00001072
Iteration 83/1000 | Loss: 0.00001072
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001070
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001069
Iteration 92/1000 | Loss: 0.00001069
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001068
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001066
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001065
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001065
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001064
Iteration 113/1000 | Loss: 0.00001064
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001063
Iteration 116/1000 | Loss: 0.00001063
Iteration 117/1000 | Loss: 0.00001063
Iteration 118/1000 | Loss: 0.00001063
Iteration 119/1000 | Loss: 0.00001063
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001062
Iteration 123/1000 | Loss: 0.00001062
Iteration 124/1000 | Loss: 0.00001062
Iteration 125/1000 | Loss: 0.00001062
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001061
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001060
Iteration 140/1000 | Loss: 0.00001060
Iteration 141/1000 | Loss: 0.00001060
Iteration 142/1000 | Loss: 0.00001060
Iteration 143/1000 | Loss: 0.00001060
Iteration 144/1000 | Loss: 0.00001060
Iteration 145/1000 | Loss: 0.00001060
Iteration 146/1000 | Loss: 0.00001060
Iteration 147/1000 | Loss: 0.00001059
Iteration 148/1000 | Loss: 0.00001059
Iteration 149/1000 | Loss: 0.00001059
Iteration 150/1000 | Loss: 0.00001059
Iteration 151/1000 | Loss: 0.00001059
Iteration 152/1000 | Loss: 0.00001059
Iteration 153/1000 | Loss: 0.00001059
Iteration 154/1000 | Loss: 0.00001059
Iteration 155/1000 | Loss: 0.00001059
Iteration 156/1000 | Loss: 0.00001059
Iteration 157/1000 | Loss: 0.00001058
Iteration 158/1000 | Loss: 0.00001058
Iteration 159/1000 | Loss: 0.00001058
Iteration 160/1000 | Loss: 0.00001058
Iteration 161/1000 | Loss: 0.00001058
Iteration 162/1000 | Loss: 0.00001058
Iteration 163/1000 | Loss: 0.00001058
Iteration 164/1000 | Loss: 0.00001057
Iteration 165/1000 | Loss: 0.00001057
Iteration 166/1000 | Loss: 0.00001057
Iteration 167/1000 | Loss: 0.00001057
Iteration 168/1000 | Loss: 0.00001057
Iteration 169/1000 | Loss: 0.00001057
Iteration 170/1000 | Loss: 0.00001057
Iteration 171/1000 | Loss: 0.00001057
Iteration 172/1000 | Loss: 0.00001057
Iteration 173/1000 | Loss: 0.00001057
Iteration 174/1000 | Loss: 0.00001057
Iteration 175/1000 | Loss: 0.00001057
Iteration 176/1000 | Loss: 0.00001056
Iteration 177/1000 | Loss: 0.00001056
Iteration 178/1000 | Loss: 0.00001056
Iteration 179/1000 | Loss: 0.00001056
Iteration 180/1000 | Loss: 0.00001056
Iteration 181/1000 | Loss: 0.00001056
Iteration 182/1000 | Loss: 0.00001056
Iteration 183/1000 | Loss: 0.00001056
Iteration 184/1000 | Loss: 0.00001056
Iteration 185/1000 | Loss: 0.00001056
Iteration 186/1000 | Loss: 0.00001056
Iteration 187/1000 | Loss: 0.00001056
Iteration 188/1000 | Loss: 0.00001056
Iteration 189/1000 | Loss: 0.00001055
Iteration 190/1000 | Loss: 0.00001055
Iteration 191/1000 | Loss: 0.00001055
Iteration 192/1000 | Loss: 0.00001055
Iteration 193/1000 | Loss: 0.00001055
Iteration 194/1000 | Loss: 0.00001055
Iteration 195/1000 | Loss: 0.00001055
Iteration 196/1000 | Loss: 0.00001055
Iteration 197/1000 | Loss: 0.00001055
Iteration 198/1000 | Loss: 0.00001055
Iteration 199/1000 | Loss: 0.00001055
Iteration 200/1000 | Loss: 0.00001055
Iteration 201/1000 | Loss: 0.00001055
Iteration 202/1000 | Loss: 0.00001055
Iteration 203/1000 | Loss: 0.00001055
Iteration 204/1000 | Loss: 0.00001054
Iteration 205/1000 | Loss: 0.00001054
Iteration 206/1000 | Loss: 0.00001054
Iteration 207/1000 | Loss: 0.00001054
Iteration 208/1000 | Loss: 0.00001054
Iteration 209/1000 | Loss: 0.00001054
Iteration 210/1000 | Loss: 0.00001054
Iteration 211/1000 | Loss: 0.00001054
Iteration 212/1000 | Loss: 0.00001054
Iteration 213/1000 | Loss: 0.00001053
Iteration 214/1000 | Loss: 0.00001053
Iteration 215/1000 | Loss: 0.00001053
Iteration 216/1000 | Loss: 0.00001053
Iteration 217/1000 | Loss: 0.00001053
Iteration 218/1000 | Loss: 0.00001053
Iteration 219/1000 | Loss: 0.00001053
Iteration 220/1000 | Loss: 0.00001053
Iteration 221/1000 | Loss: 0.00001053
Iteration 222/1000 | Loss: 0.00001053
Iteration 223/1000 | Loss: 0.00001053
Iteration 224/1000 | Loss: 0.00001053
Iteration 225/1000 | Loss: 0.00001053
Iteration 226/1000 | Loss: 0.00001053
Iteration 227/1000 | Loss: 0.00001053
Iteration 228/1000 | Loss: 0.00001053
Iteration 229/1000 | Loss: 0.00001053
Iteration 230/1000 | Loss: 0.00001053
Iteration 231/1000 | Loss: 0.00001053
Iteration 232/1000 | Loss: 0.00001053
Iteration 233/1000 | Loss: 0.00001053
Iteration 234/1000 | Loss: 0.00001053
Iteration 235/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.0525909601710737e-05, 1.0525909601710737e-05, 1.0525909601710737e-05, 1.0525909601710737e-05, 1.0525909601710737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0525909601710737e-05

Optimization complete. Final v2v error: 2.7677206993103027 mm

Highest mean error: 2.9201247692108154 mm for frame 215

Lowest mean error: 2.631720542907715 mm for frame 67

Saving results

Total time: 50.3508403301239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426532
Iteration 2/25 | Loss: 0.00140403
Iteration 3/25 | Loss: 0.00124373
Iteration 4/25 | Loss: 0.00123073
Iteration 5/25 | Loss: 0.00122950
Iteration 6/25 | Loss: 0.00122950
Iteration 7/25 | Loss: 0.00122950
Iteration 8/25 | Loss: 0.00122950
Iteration 9/25 | Loss: 0.00122950
Iteration 10/25 | Loss: 0.00122950
Iteration 11/25 | Loss: 0.00122950
Iteration 12/25 | Loss: 0.00122950
Iteration 13/25 | Loss: 0.00122950
Iteration 14/25 | Loss: 0.00122950
Iteration 15/25 | Loss: 0.00122950
Iteration 16/25 | Loss: 0.00122950
Iteration 17/25 | Loss: 0.00122950
Iteration 18/25 | Loss: 0.00122950
Iteration 19/25 | Loss: 0.00122950
Iteration 20/25 | Loss: 0.00122950
Iteration 21/25 | Loss: 0.00122950
Iteration 22/25 | Loss: 0.00122950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001229495625011623, 0.001229495625011623, 0.001229495625011623, 0.001229495625011623, 0.001229495625011623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001229495625011623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15684032
Iteration 2/25 | Loss: 0.00065515
Iteration 3/25 | Loss: 0.00065514
Iteration 4/25 | Loss: 0.00065514
Iteration 5/25 | Loss: 0.00065514
Iteration 6/25 | Loss: 0.00065514
Iteration 7/25 | Loss: 0.00065514
Iteration 8/25 | Loss: 0.00065514
Iteration 9/25 | Loss: 0.00065514
Iteration 10/25 | Loss: 0.00065514
Iteration 11/25 | Loss: 0.00065514
Iteration 12/25 | Loss: 0.00065514
Iteration 13/25 | Loss: 0.00065514
Iteration 14/25 | Loss: 0.00065514
Iteration 15/25 | Loss: 0.00065514
Iteration 16/25 | Loss: 0.00065514
Iteration 17/25 | Loss: 0.00065514
Iteration 18/25 | Loss: 0.00065514
Iteration 19/25 | Loss: 0.00065514
Iteration 20/25 | Loss: 0.00065514
Iteration 21/25 | Loss: 0.00065514
Iteration 22/25 | Loss: 0.00065514
Iteration 23/25 | Loss: 0.00065514
Iteration 24/25 | Loss: 0.00065514
Iteration 25/25 | Loss: 0.00065514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065514
Iteration 2/1000 | Loss: 0.00003283
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001398
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001275
Iteration 11/1000 | Loss: 0.00001261
Iteration 12/1000 | Loss: 0.00001248
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001211
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001210
Iteration 20/1000 | Loss: 0.00001210
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001208
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001206
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001200
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001199
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001198
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001197
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001196
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001195
Iteration 88/1000 | Loss: 0.00001195
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001191
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001187
Iteration 118/1000 | Loss: 0.00001187
Iteration 119/1000 | Loss: 0.00001187
Iteration 120/1000 | Loss: 0.00001187
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001187
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001186
Iteration 130/1000 | Loss: 0.00001186
Iteration 131/1000 | Loss: 0.00001186
Iteration 132/1000 | Loss: 0.00001186
Iteration 133/1000 | Loss: 0.00001186
Iteration 134/1000 | Loss: 0.00001185
Iteration 135/1000 | Loss: 0.00001185
Iteration 136/1000 | Loss: 0.00001185
Iteration 137/1000 | Loss: 0.00001185
Iteration 138/1000 | Loss: 0.00001185
Iteration 139/1000 | Loss: 0.00001185
Iteration 140/1000 | Loss: 0.00001185
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001185
Iteration 145/1000 | Loss: 0.00001185
Iteration 146/1000 | Loss: 0.00001185
Iteration 147/1000 | Loss: 0.00001185
Iteration 148/1000 | Loss: 0.00001185
Iteration 149/1000 | Loss: 0.00001185
Iteration 150/1000 | Loss: 0.00001185
Iteration 151/1000 | Loss: 0.00001185
Iteration 152/1000 | Loss: 0.00001185
Iteration 153/1000 | Loss: 0.00001185
Iteration 154/1000 | Loss: 0.00001185
Iteration 155/1000 | Loss: 0.00001184
Iteration 156/1000 | Loss: 0.00001184
Iteration 157/1000 | Loss: 0.00001184
Iteration 158/1000 | Loss: 0.00001184
Iteration 159/1000 | Loss: 0.00001184
Iteration 160/1000 | Loss: 0.00001184
Iteration 161/1000 | Loss: 0.00001184
Iteration 162/1000 | Loss: 0.00001184
Iteration 163/1000 | Loss: 0.00001184
Iteration 164/1000 | Loss: 0.00001184
Iteration 165/1000 | Loss: 0.00001184
Iteration 166/1000 | Loss: 0.00001184
Iteration 167/1000 | Loss: 0.00001184
Iteration 168/1000 | Loss: 0.00001184
Iteration 169/1000 | Loss: 0.00001184
Iteration 170/1000 | Loss: 0.00001184
Iteration 171/1000 | Loss: 0.00001184
Iteration 172/1000 | Loss: 0.00001184
Iteration 173/1000 | Loss: 0.00001184
Iteration 174/1000 | Loss: 0.00001184
Iteration 175/1000 | Loss: 0.00001184
Iteration 176/1000 | Loss: 0.00001184
Iteration 177/1000 | Loss: 0.00001184
Iteration 178/1000 | Loss: 0.00001184
Iteration 179/1000 | Loss: 0.00001184
Iteration 180/1000 | Loss: 0.00001184
Iteration 181/1000 | Loss: 0.00001184
Iteration 182/1000 | Loss: 0.00001184
Iteration 183/1000 | Loss: 0.00001184
Iteration 184/1000 | Loss: 0.00001184
Iteration 185/1000 | Loss: 0.00001184
Iteration 186/1000 | Loss: 0.00001184
Iteration 187/1000 | Loss: 0.00001184
Iteration 188/1000 | Loss: 0.00001184
Iteration 189/1000 | Loss: 0.00001184
Iteration 190/1000 | Loss: 0.00001184
Iteration 191/1000 | Loss: 0.00001184
Iteration 192/1000 | Loss: 0.00001184
Iteration 193/1000 | Loss: 0.00001184
Iteration 194/1000 | Loss: 0.00001184
Iteration 195/1000 | Loss: 0.00001184
Iteration 196/1000 | Loss: 0.00001184
Iteration 197/1000 | Loss: 0.00001184
Iteration 198/1000 | Loss: 0.00001184
Iteration 199/1000 | Loss: 0.00001184
Iteration 200/1000 | Loss: 0.00001184
Iteration 201/1000 | Loss: 0.00001184
Iteration 202/1000 | Loss: 0.00001184
Iteration 203/1000 | Loss: 0.00001184
Iteration 204/1000 | Loss: 0.00001184
Iteration 205/1000 | Loss: 0.00001184
Iteration 206/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.1837001693493221e-05, 1.1837001693493221e-05, 1.1837001693493221e-05, 1.1837001693493221e-05, 1.1837001693493221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1837001693493221e-05

Optimization complete. Final v2v error: 2.9744691848754883 mm

Highest mean error: 3.0085437297821045 mm for frame 102

Lowest mean error: 2.9480555057525635 mm for frame 31

Saving results

Total time: 34.91136431694031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786534
Iteration 2/25 | Loss: 0.00164764
Iteration 3/25 | Loss: 0.00137199
Iteration 4/25 | Loss: 0.00133953
Iteration 5/25 | Loss: 0.00133145
Iteration 6/25 | Loss: 0.00132864
Iteration 7/25 | Loss: 0.00132838
Iteration 8/25 | Loss: 0.00132838
Iteration 9/25 | Loss: 0.00132838
Iteration 10/25 | Loss: 0.00132838
Iteration 11/25 | Loss: 0.00132838
Iteration 12/25 | Loss: 0.00132838
Iteration 13/25 | Loss: 0.00132838
Iteration 14/25 | Loss: 0.00132838
Iteration 15/25 | Loss: 0.00132838
Iteration 16/25 | Loss: 0.00132838
Iteration 17/25 | Loss: 0.00132838
Iteration 18/25 | Loss: 0.00132838
Iteration 19/25 | Loss: 0.00132838
Iteration 20/25 | Loss: 0.00132838
Iteration 21/25 | Loss: 0.00132838
Iteration 22/25 | Loss: 0.00132838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001328381011262536, 0.001328381011262536, 0.001328381011262536, 0.001328381011262536, 0.001328381011262536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001328381011262536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12998879
Iteration 2/25 | Loss: 0.00124174
Iteration 3/25 | Loss: 0.00124174
Iteration 4/25 | Loss: 0.00124174
Iteration 5/25 | Loss: 0.00124174
Iteration 6/25 | Loss: 0.00124174
Iteration 7/25 | Loss: 0.00124174
Iteration 8/25 | Loss: 0.00124174
Iteration 9/25 | Loss: 0.00124174
Iteration 10/25 | Loss: 0.00124174
Iteration 11/25 | Loss: 0.00124174
Iteration 12/25 | Loss: 0.00124174
Iteration 13/25 | Loss: 0.00124174
Iteration 14/25 | Loss: 0.00124174
Iteration 15/25 | Loss: 0.00124174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012417399557307363, 0.0012417399557307363, 0.0012417399557307363, 0.0012417399557307363, 0.0012417399557307363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012417399557307363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124174
Iteration 2/1000 | Loss: 0.00009689
Iteration 3/1000 | Loss: 0.00005982
Iteration 4/1000 | Loss: 0.00004571
Iteration 5/1000 | Loss: 0.00004171
Iteration 6/1000 | Loss: 0.00003987
Iteration 7/1000 | Loss: 0.00003819
Iteration 8/1000 | Loss: 0.00003713
Iteration 9/1000 | Loss: 0.00003644
Iteration 10/1000 | Loss: 0.00003602
Iteration 11/1000 | Loss: 0.00003563
Iteration 12/1000 | Loss: 0.00003520
Iteration 13/1000 | Loss: 0.00003485
Iteration 14/1000 | Loss: 0.00003467
Iteration 15/1000 | Loss: 0.00003446
Iteration 16/1000 | Loss: 0.00003421
Iteration 17/1000 | Loss: 0.00003405
Iteration 18/1000 | Loss: 0.00003392
Iteration 19/1000 | Loss: 0.00003380
Iteration 20/1000 | Loss: 0.00003372
Iteration 21/1000 | Loss: 0.00003371
Iteration 22/1000 | Loss: 0.00003371
Iteration 23/1000 | Loss: 0.00003368
Iteration 24/1000 | Loss: 0.00003367
Iteration 25/1000 | Loss: 0.00003367
Iteration 26/1000 | Loss: 0.00003364
Iteration 27/1000 | Loss: 0.00003364
Iteration 28/1000 | Loss: 0.00003361
Iteration 29/1000 | Loss: 0.00003361
Iteration 30/1000 | Loss: 0.00003361
Iteration 31/1000 | Loss: 0.00003361
Iteration 32/1000 | Loss: 0.00003361
Iteration 33/1000 | Loss: 0.00003361
Iteration 34/1000 | Loss: 0.00003361
Iteration 35/1000 | Loss: 0.00003361
Iteration 36/1000 | Loss: 0.00003361
Iteration 37/1000 | Loss: 0.00003361
Iteration 38/1000 | Loss: 0.00003361
Iteration 39/1000 | Loss: 0.00003360
Iteration 40/1000 | Loss: 0.00003360
Iteration 41/1000 | Loss: 0.00003360
Iteration 42/1000 | Loss: 0.00003360
Iteration 43/1000 | Loss: 0.00003360
Iteration 44/1000 | Loss: 0.00003360
Iteration 45/1000 | Loss: 0.00003360
Iteration 46/1000 | Loss: 0.00003359
Iteration 47/1000 | Loss: 0.00003359
Iteration 48/1000 | Loss: 0.00003359
Iteration 49/1000 | Loss: 0.00003359
Iteration 50/1000 | Loss: 0.00003357
Iteration 51/1000 | Loss: 0.00003357
Iteration 52/1000 | Loss: 0.00003357
Iteration 53/1000 | Loss: 0.00003357
Iteration 54/1000 | Loss: 0.00003357
Iteration 55/1000 | Loss: 0.00003357
Iteration 56/1000 | Loss: 0.00003356
Iteration 57/1000 | Loss: 0.00003356
Iteration 58/1000 | Loss: 0.00003355
Iteration 59/1000 | Loss: 0.00003355
Iteration 60/1000 | Loss: 0.00003354
Iteration 61/1000 | Loss: 0.00003354
Iteration 62/1000 | Loss: 0.00003354
Iteration 63/1000 | Loss: 0.00003353
Iteration 64/1000 | Loss: 0.00003352
Iteration 65/1000 | Loss: 0.00003352
Iteration 66/1000 | Loss: 0.00003352
Iteration 67/1000 | Loss: 0.00003352
Iteration 68/1000 | Loss: 0.00003352
Iteration 69/1000 | Loss: 0.00003351
Iteration 70/1000 | Loss: 0.00003351
Iteration 71/1000 | Loss: 0.00003351
Iteration 72/1000 | Loss: 0.00003351
Iteration 73/1000 | Loss: 0.00003351
Iteration 74/1000 | Loss: 0.00003351
Iteration 75/1000 | Loss: 0.00003351
Iteration 76/1000 | Loss: 0.00003351
Iteration 77/1000 | Loss: 0.00003351
Iteration 78/1000 | Loss: 0.00003351
Iteration 79/1000 | Loss: 0.00003351
Iteration 80/1000 | Loss: 0.00003350
Iteration 81/1000 | Loss: 0.00003350
Iteration 82/1000 | Loss: 0.00003350
Iteration 83/1000 | Loss: 0.00003350
Iteration 84/1000 | Loss: 0.00003349
Iteration 85/1000 | Loss: 0.00003348
Iteration 86/1000 | Loss: 0.00003348
Iteration 87/1000 | Loss: 0.00003347
Iteration 88/1000 | Loss: 0.00003347
Iteration 89/1000 | Loss: 0.00003347
Iteration 90/1000 | Loss: 0.00003347
Iteration 91/1000 | Loss: 0.00003347
Iteration 92/1000 | Loss: 0.00003347
Iteration 93/1000 | Loss: 0.00003346
Iteration 94/1000 | Loss: 0.00003346
Iteration 95/1000 | Loss: 0.00003346
Iteration 96/1000 | Loss: 0.00003345
Iteration 97/1000 | Loss: 0.00003345
Iteration 98/1000 | Loss: 0.00003345
Iteration 99/1000 | Loss: 0.00003344
Iteration 100/1000 | Loss: 0.00003344
Iteration 101/1000 | Loss: 0.00003344
Iteration 102/1000 | Loss: 0.00003344
Iteration 103/1000 | Loss: 0.00003344
Iteration 104/1000 | Loss: 0.00003343
Iteration 105/1000 | Loss: 0.00003343
Iteration 106/1000 | Loss: 0.00003343
Iteration 107/1000 | Loss: 0.00003343
Iteration 108/1000 | Loss: 0.00003343
Iteration 109/1000 | Loss: 0.00003343
Iteration 110/1000 | Loss: 0.00003343
Iteration 111/1000 | Loss: 0.00003343
Iteration 112/1000 | Loss: 0.00003342
Iteration 113/1000 | Loss: 0.00003342
Iteration 114/1000 | Loss: 0.00003342
Iteration 115/1000 | Loss: 0.00003342
Iteration 116/1000 | Loss: 0.00003342
Iteration 117/1000 | Loss: 0.00003342
Iteration 118/1000 | Loss: 0.00003342
Iteration 119/1000 | Loss: 0.00003342
Iteration 120/1000 | Loss: 0.00003342
Iteration 121/1000 | Loss: 0.00003341
Iteration 122/1000 | Loss: 0.00003341
Iteration 123/1000 | Loss: 0.00003341
Iteration 124/1000 | Loss: 0.00003341
Iteration 125/1000 | Loss: 0.00003341
Iteration 126/1000 | Loss: 0.00003341
Iteration 127/1000 | Loss: 0.00003341
Iteration 128/1000 | Loss: 0.00003341
Iteration 129/1000 | Loss: 0.00003340
Iteration 130/1000 | Loss: 0.00003340
Iteration 131/1000 | Loss: 0.00003340
Iteration 132/1000 | Loss: 0.00003340
Iteration 133/1000 | Loss: 0.00003340
Iteration 134/1000 | Loss: 0.00003340
Iteration 135/1000 | Loss: 0.00003340
Iteration 136/1000 | Loss: 0.00003340
Iteration 137/1000 | Loss: 0.00003340
Iteration 138/1000 | Loss: 0.00003340
Iteration 139/1000 | Loss: 0.00003340
Iteration 140/1000 | Loss: 0.00003340
Iteration 141/1000 | Loss: 0.00003340
Iteration 142/1000 | Loss: 0.00003340
Iteration 143/1000 | Loss: 0.00003340
Iteration 144/1000 | Loss: 0.00003340
Iteration 145/1000 | Loss: 0.00003340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.339711474836804e-05, 3.339711474836804e-05, 3.339711474836804e-05, 3.339711474836804e-05, 3.339711474836804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.339711474836804e-05

Optimization complete. Final v2v error: 4.663084983825684 mm

Highest mean error: 5.54453182220459 mm for frame 81

Lowest mean error: 3.906254291534424 mm for frame 13

Saving results

Total time: 46.395050287246704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969197
Iteration 2/25 | Loss: 0.00177102
Iteration 3/25 | Loss: 0.00151545
Iteration 4/25 | Loss: 0.00155387
Iteration 5/25 | Loss: 0.00148349
Iteration 6/25 | Loss: 0.00145534
Iteration 7/25 | Loss: 0.00143932
Iteration 8/25 | Loss: 0.00142258
Iteration 9/25 | Loss: 0.00140446
Iteration 10/25 | Loss: 0.00139922
Iteration 11/25 | Loss: 0.00139848
Iteration 12/25 | Loss: 0.00140573
Iteration 13/25 | Loss: 0.00138854
Iteration 14/25 | Loss: 0.00137879
Iteration 15/25 | Loss: 0.00137253
Iteration 16/25 | Loss: 0.00137092
Iteration 17/25 | Loss: 0.00136777
Iteration 18/25 | Loss: 0.00136919
Iteration 19/25 | Loss: 0.00136781
Iteration 20/25 | Loss: 0.00136866
Iteration 21/25 | Loss: 0.00136784
Iteration 22/25 | Loss: 0.00136878
Iteration 23/25 | Loss: 0.00136630
Iteration 24/25 | Loss: 0.00136864
Iteration 25/25 | Loss: 0.00136898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26630259
Iteration 2/25 | Loss: 0.00161440
Iteration 3/25 | Loss: 0.00161436
Iteration 4/25 | Loss: 0.00161436
Iteration 5/25 | Loss: 0.00161436
Iteration 6/25 | Loss: 0.00161436
Iteration 7/25 | Loss: 0.00161436
Iteration 8/25 | Loss: 0.00161436
Iteration 9/25 | Loss: 0.00161436
Iteration 10/25 | Loss: 0.00161436
Iteration 11/25 | Loss: 0.00161436
Iteration 12/25 | Loss: 0.00161436
Iteration 13/25 | Loss: 0.00161436
Iteration 14/25 | Loss: 0.00161436
Iteration 15/25 | Loss: 0.00161436
Iteration 16/25 | Loss: 0.00161436
Iteration 17/25 | Loss: 0.00161436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016143603716045618, 0.0016143603716045618, 0.0016143603716045618, 0.0016143603716045618, 0.0016143603716045618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016143603716045618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161436
Iteration 2/1000 | Loss: 0.00164288
Iteration 3/1000 | Loss: 0.00010973
Iteration 4/1000 | Loss: 0.00005244
Iteration 5/1000 | Loss: 0.00121555
Iteration 6/1000 | Loss: 0.00126752
Iteration 7/1000 | Loss: 0.00163738
Iteration 8/1000 | Loss: 0.00006407
Iteration 9/1000 | Loss: 0.00136595
Iteration 10/1000 | Loss: 0.00183211
Iteration 11/1000 | Loss: 0.00144880
Iteration 12/1000 | Loss: 0.00160864
Iteration 13/1000 | Loss: 0.00004123
Iteration 14/1000 | Loss: 0.00090541
Iteration 15/1000 | Loss: 0.00125740
Iteration 16/1000 | Loss: 0.00128731
Iteration 17/1000 | Loss: 0.00138003
Iteration 18/1000 | Loss: 0.00098813
Iteration 19/1000 | Loss: 0.00111299
Iteration 20/1000 | Loss: 0.00217886
Iteration 21/1000 | Loss: 0.00102616
Iteration 22/1000 | Loss: 0.00078150
Iteration 23/1000 | Loss: 0.00074840
Iteration 24/1000 | Loss: 0.00118659
Iteration 25/1000 | Loss: 0.00089107
Iteration 26/1000 | Loss: 0.00133583
Iteration 27/1000 | Loss: 0.00123899
Iteration 28/1000 | Loss: 0.00150329
Iteration 29/1000 | Loss: 0.00100220
Iteration 30/1000 | Loss: 0.00005571
Iteration 31/1000 | Loss: 0.00116041
Iteration 32/1000 | Loss: 0.00135427
Iteration 33/1000 | Loss: 0.00010982
Iteration 34/1000 | Loss: 0.00004838
Iteration 35/1000 | Loss: 0.00060853
Iteration 36/1000 | Loss: 0.00075325
Iteration 37/1000 | Loss: 0.00067144
Iteration 38/1000 | Loss: 0.00071892
Iteration 39/1000 | Loss: 0.00005523
Iteration 40/1000 | Loss: 0.00005431
Iteration 41/1000 | Loss: 0.00153658
Iteration 42/1000 | Loss: 0.00094491
Iteration 43/1000 | Loss: 0.00004226
Iteration 44/1000 | Loss: 0.00004294
Iteration 45/1000 | Loss: 0.00005064
Iteration 46/1000 | Loss: 0.00004664
Iteration 47/1000 | Loss: 0.00003510
Iteration 48/1000 | Loss: 0.00007777
Iteration 49/1000 | Loss: 0.00010090
Iteration 50/1000 | Loss: 0.00004696
Iteration 51/1000 | Loss: 0.00017766
Iteration 52/1000 | Loss: 0.00006242
Iteration 53/1000 | Loss: 0.00004884
Iteration 54/1000 | Loss: 0.00004197
Iteration 55/1000 | Loss: 0.00007196
Iteration 56/1000 | Loss: 0.00006246
Iteration 57/1000 | Loss: 0.00004860
Iteration 58/1000 | Loss: 0.00004450
Iteration 59/1000 | Loss: 0.00187808
Iteration 60/1000 | Loss: 0.00100196
Iteration 61/1000 | Loss: 0.00177119
Iteration 62/1000 | Loss: 0.00074142
Iteration 63/1000 | Loss: 0.00128640
Iteration 64/1000 | Loss: 0.00004254
Iteration 65/1000 | Loss: 0.00006612
Iteration 66/1000 | Loss: 0.00005274
Iteration 67/1000 | Loss: 0.00005793
Iteration 68/1000 | Loss: 0.00006013
Iteration 69/1000 | Loss: 0.00044002
Iteration 70/1000 | Loss: 0.00048310
Iteration 71/1000 | Loss: 0.00045117
Iteration 72/1000 | Loss: 0.00032909
Iteration 73/1000 | Loss: 0.00013743
Iteration 74/1000 | Loss: 0.00009088
Iteration 75/1000 | Loss: 0.00020712
Iteration 76/1000 | Loss: 0.00046953
Iteration 77/1000 | Loss: 0.00009975
Iteration 78/1000 | Loss: 0.00017677
Iteration 79/1000 | Loss: 0.00028312
Iteration 80/1000 | Loss: 0.00024633
Iteration 81/1000 | Loss: 0.00013189
Iteration 82/1000 | Loss: 0.00012922
Iteration 83/1000 | Loss: 0.00216055
Iteration 84/1000 | Loss: 0.00013641
Iteration 85/1000 | Loss: 0.00031754
Iteration 86/1000 | Loss: 0.00039156
Iteration 87/1000 | Loss: 0.00010870
Iteration 88/1000 | Loss: 0.00025981
Iteration 89/1000 | Loss: 0.00038847
Iteration 90/1000 | Loss: 0.00019774
Iteration 91/1000 | Loss: 0.00034621
Iteration 92/1000 | Loss: 0.00030349
Iteration 93/1000 | Loss: 0.00050445
Iteration 94/1000 | Loss: 0.00018437
Iteration 95/1000 | Loss: 0.00039845
Iteration 96/1000 | Loss: 0.00005416
Iteration 97/1000 | Loss: 0.00003911
Iteration 98/1000 | Loss: 0.00022143
Iteration 99/1000 | Loss: 0.00003505
Iteration 100/1000 | Loss: 0.00003111
Iteration 101/1000 | Loss: 0.00002984
Iteration 102/1000 | Loss: 0.00002928
Iteration 103/1000 | Loss: 0.00002894
Iteration 104/1000 | Loss: 0.00002861
Iteration 105/1000 | Loss: 0.00002833
Iteration 106/1000 | Loss: 0.00002809
Iteration 107/1000 | Loss: 0.00002772
Iteration 108/1000 | Loss: 0.00171590
Iteration 109/1000 | Loss: 0.00006205
Iteration 110/1000 | Loss: 0.00003084
Iteration 111/1000 | Loss: 0.00002554
Iteration 112/1000 | Loss: 0.00002398
Iteration 113/1000 | Loss: 0.00002297
Iteration 114/1000 | Loss: 0.00002228
Iteration 115/1000 | Loss: 0.00002199
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002172
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002169
Iteration 120/1000 | Loss: 0.00002169
Iteration 121/1000 | Loss: 0.00002168
Iteration 122/1000 | Loss: 0.00002168
Iteration 123/1000 | Loss: 0.00002167
Iteration 124/1000 | Loss: 0.00002166
Iteration 125/1000 | Loss: 0.00002166
Iteration 126/1000 | Loss: 0.00002155
Iteration 127/1000 | Loss: 0.00002154
Iteration 128/1000 | Loss: 0.00002153
Iteration 129/1000 | Loss: 0.00002153
Iteration 130/1000 | Loss: 0.00002153
Iteration 131/1000 | Loss: 0.00002152
Iteration 132/1000 | Loss: 0.00002152
Iteration 133/1000 | Loss: 0.00002151
Iteration 134/1000 | Loss: 0.00002151
Iteration 135/1000 | Loss: 0.00002151
Iteration 136/1000 | Loss: 0.00002150
Iteration 137/1000 | Loss: 0.00002150
Iteration 138/1000 | Loss: 0.00002150
Iteration 139/1000 | Loss: 0.00002150
Iteration 140/1000 | Loss: 0.00002150
Iteration 141/1000 | Loss: 0.00002150
Iteration 142/1000 | Loss: 0.00002149
Iteration 143/1000 | Loss: 0.00002149
Iteration 144/1000 | Loss: 0.00002149
Iteration 145/1000 | Loss: 0.00002148
Iteration 146/1000 | Loss: 0.00002148
Iteration 147/1000 | Loss: 0.00002148
Iteration 148/1000 | Loss: 0.00002148
Iteration 149/1000 | Loss: 0.00002147
Iteration 150/1000 | Loss: 0.00002147
Iteration 151/1000 | Loss: 0.00002147
Iteration 152/1000 | Loss: 0.00002147
Iteration 153/1000 | Loss: 0.00002146
Iteration 154/1000 | Loss: 0.00002146
Iteration 155/1000 | Loss: 0.00002146
Iteration 156/1000 | Loss: 0.00002145
Iteration 157/1000 | Loss: 0.00002145
Iteration 158/1000 | Loss: 0.00002145
Iteration 159/1000 | Loss: 0.00002145
Iteration 160/1000 | Loss: 0.00002144
Iteration 161/1000 | Loss: 0.00002144
Iteration 162/1000 | Loss: 0.00002143
Iteration 163/1000 | Loss: 0.00002143
Iteration 164/1000 | Loss: 0.00002143
Iteration 165/1000 | Loss: 0.00002143
Iteration 166/1000 | Loss: 0.00002143
Iteration 167/1000 | Loss: 0.00002143
Iteration 168/1000 | Loss: 0.00002143
Iteration 169/1000 | Loss: 0.00002143
Iteration 170/1000 | Loss: 0.00002143
Iteration 171/1000 | Loss: 0.00002142
Iteration 172/1000 | Loss: 0.00002142
Iteration 173/1000 | Loss: 0.00002142
Iteration 174/1000 | Loss: 0.00002142
Iteration 175/1000 | Loss: 0.00002142
Iteration 176/1000 | Loss: 0.00002142
Iteration 177/1000 | Loss: 0.00002142
Iteration 178/1000 | Loss: 0.00002142
Iteration 179/1000 | Loss: 0.00002142
Iteration 180/1000 | Loss: 0.00002142
Iteration 181/1000 | Loss: 0.00002142
Iteration 182/1000 | Loss: 0.00002142
Iteration 183/1000 | Loss: 0.00002142
Iteration 184/1000 | Loss: 0.00002142
Iteration 185/1000 | Loss: 0.00002142
Iteration 186/1000 | Loss: 0.00002142
Iteration 187/1000 | Loss: 0.00002142
Iteration 188/1000 | Loss: 0.00002142
Iteration 189/1000 | Loss: 0.00002141
Iteration 190/1000 | Loss: 0.00002141
Iteration 191/1000 | Loss: 0.00002141
Iteration 192/1000 | Loss: 0.00002141
Iteration 193/1000 | Loss: 0.00002141
Iteration 194/1000 | Loss: 0.00002141
Iteration 195/1000 | Loss: 0.00002141
Iteration 196/1000 | Loss: 0.00002141
Iteration 197/1000 | Loss: 0.00002141
Iteration 198/1000 | Loss: 0.00002141
Iteration 199/1000 | Loss: 0.00002141
Iteration 200/1000 | Loss: 0.00002141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.1413196009234525e-05, 2.1413196009234525e-05, 2.1413196009234525e-05, 2.1413196009234525e-05, 2.1413196009234525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1413196009234525e-05

Optimization complete. Final v2v error: 3.792760133743286 mm

Highest mean error: 4.759580135345459 mm for frame 78

Lowest mean error: 2.8813300132751465 mm for frame 168

Saving results

Total time: 230.77550339698792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605345
Iteration 2/25 | Loss: 0.00149163
Iteration 3/25 | Loss: 0.00133155
Iteration 4/25 | Loss: 0.00130234
Iteration 5/25 | Loss: 0.00130077
Iteration 6/25 | Loss: 0.00129924
Iteration 7/25 | Loss: 0.00129253
Iteration 8/25 | Loss: 0.00128952
Iteration 9/25 | Loss: 0.00128844
Iteration 10/25 | Loss: 0.00128804
Iteration 11/25 | Loss: 0.00128797
Iteration 12/25 | Loss: 0.00128797
Iteration 13/25 | Loss: 0.00128796
Iteration 14/25 | Loss: 0.00128796
Iteration 15/25 | Loss: 0.00128796
Iteration 16/25 | Loss: 0.00128795
Iteration 17/25 | Loss: 0.00128795
Iteration 18/25 | Loss: 0.00128795
Iteration 19/25 | Loss: 0.00128794
Iteration 20/25 | Loss: 0.00128794
Iteration 21/25 | Loss: 0.00128794
Iteration 22/25 | Loss: 0.00128794
Iteration 23/25 | Loss: 0.00128794
Iteration 24/25 | Loss: 0.00128794
Iteration 25/25 | Loss: 0.00128794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36541533
Iteration 2/25 | Loss: 0.00117808
Iteration 3/25 | Loss: 0.00117807
Iteration 4/25 | Loss: 0.00117807
Iteration 5/25 | Loss: 0.00117807
Iteration 6/25 | Loss: 0.00117807
Iteration 7/25 | Loss: 0.00117807
Iteration 8/25 | Loss: 0.00117806
Iteration 9/25 | Loss: 0.00117806
Iteration 10/25 | Loss: 0.00117806
Iteration 11/25 | Loss: 0.00117806
Iteration 12/25 | Loss: 0.00117806
Iteration 13/25 | Loss: 0.00117806
Iteration 14/25 | Loss: 0.00117806
Iteration 15/25 | Loss: 0.00117806
Iteration 16/25 | Loss: 0.00117806
Iteration 17/25 | Loss: 0.00117806
Iteration 18/25 | Loss: 0.00117806
Iteration 19/25 | Loss: 0.00117806
Iteration 20/25 | Loss: 0.00117806
Iteration 21/25 | Loss: 0.00117806
Iteration 22/25 | Loss: 0.00117806
Iteration 23/25 | Loss: 0.00117806
Iteration 24/25 | Loss: 0.00117806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011780636850744486, 0.0011780636850744486, 0.0011780636850744486, 0.0011780636850744486, 0.0011780636850744486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011780636850744486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117806
Iteration 2/1000 | Loss: 0.00004861
Iteration 3/1000 | Loss: 0.00002981
Iteration 4/1000 | Loss: 0.00002607
Iteration 5/1000 | Loss: 0.00002444
Iteration 6/1000 | Loss: 0.00002294
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002133
Iteration 9/1000 | Loss: 0.00002087
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002012
Iteration 12/1000 | Loss: 0.00001989
Iteration 13/1000 | Loss: 0.00001976
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00001962
Iteration 16/1000 | Loss: 0.00001955
Iteration 17/1000 | Loss: 0.00001951
Iteration 18/1000 | Loss: 0.00001950
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001949
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001931
Iteration 25/1000 | Loss: 0.00001924
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001918
Iteration 28/1000 | Loss: 0.00001916
Iteration 29/1000 | Loss: 0.00001916
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001915
Iteration 33/1000 | Loss: 0.00001915
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001914
Iteration 38/1000 | Loss: 0.00001913
Iteration 39/1000 | Loss: 0.00001913
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001912
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001911
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001910
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001907
Iteration 55/1000 | Loss: 0.00001907
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001904
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001903
Iteration 74/1000 | Loss: 0.00001903
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001903
Iteration 77/1000 | Loss: 0.00001902
Iteration 78/1000 | Loss: 0.00001902
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001899
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001899
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001898
Iteration 90/1000 | Loss: 0.00001898
Iteration 91/1000 | Loss: 0.00001897
Iteration 92/1000 | Loss: 0.00001897
Iteration 93/1000 | Loss: 0.00001897
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001896
Iteration 96/1000 | Loss: 0.00001896
Iteration 97/1000 | Loss: 0.00001896
Iteration 98/1000 | Loss: 0.00001896
Iteration 99/1000 | Loss: 0.00001896
Iteration 100/1000 | Loss: 0.00001895
Iteration 101/1000 | Loss: 0.00001895
Iteration 102/1000 | Loss: 0.00001895
Iteration 103/1000 | Loss: 0.00001895
Iteration 104/1000 | Loss: 0.00001894
Iteration 105/1000 | Loss: 0.00001894
Iteration 106/1000 | Loss: 0.00001894
Iteration 107/1000 | Loss: 0.00001894
Iteration 108/1000 | Loss: 0.00001894
Iteration 109/1000 | Loss: 0.00001894
Iteration 110/1000 | Loss: 0.00001893
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001892
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001891
Iteration 117/1000 | Loss: 0.00001891
Iteration 118/1000 | Loss: 0.00001891
Iteration 119/1000 | Loss: 0.00001891
Iteration 120/1000 | Loss: 0.00001890
Iteration 121/1000 | Loss: 0.00001890
Iteration 122/1000 | Loss: 0.00001890
Iteration 123/1000 | Loss: 0.00001889
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001889
Iteration 126/1000 | Loss: 0.00001888
Iteration 127/1000 | Loss: 0.00001888
Iteration 128/1000 | Loss: 0.00001888
Iteration 129/1000 | Loss: 0.00001887
Iteration 130/1000 | Loss: 0.00001887
Iteration 131/1000 | Loss: 0.00001887
Iteration 132/1000 | Loss: 0.00001887
Iteration 133/1000 | Loss: 0.00001887
Iteration 134/1000 | Loss: 0.00001887
Iteration 135/1000 | Loss: 0.00001887
Iteration 136/1000 | Loss: 0.00001886
Iteration 137/1000 | Loss: 0.00001886
Iteration 138/1000 | Loss: 0.00001886
Iteration 139/1000 | Loss: 0.00001886
Iteration 140/1000 | Loss: 0.00001886
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001885
Iteration 144/1000 | Loss: 0.00001885
Iteration 145/1000 | Loss: 0.00001885
Iteration 146/1000 | Loss: 0.00001884
Iteration 147/1000 | Loss: 0.00001884
Iteration 148/1000 | Loss: 0.00001884
Iteration 149/1000 | Loss: 0.00001884
Iteration 150/1000 | Loss: 0.00001884
Iteration 151/1000 | Loss: 0.00001883
Iteration 152/1000 | Loss: 0.00001883
Iteration 153/1000 | Loss: 0.00001883
Iteration 154/1000 | Loss: 0.00001883
Iteration 155/1000 | Loss: 0.00001883
Iteration 156/1000 | Loss: 0.00001882
Iteration 157/1000 | Loss: 0.00001882
Iteration 158/1000 | Loss: 0.00001882
Iteration 159/1000 | Loss: 0.00001882
Iteration 160/1000 | Loss: 0.00001882
Iteration 161/1000 | Loss: 0.00001882
Iteration 162/1000 | Loss: 0.00001882
Iteration 163/1000 | Loss: 0.00001882
Iteration 164/1000 | Loss: 0.00001881
Iteration 165/1000 | Loss: 0.00001881
Iteration 166/1000 | Loss: 0.00001881
Iteration 167/1000 | Loss: 0.00001881
Iteration 168/1000 | Loss: 0.00001881
Iteration 169/1000 | Loss: 0.00001881
Iteration 170/1000 | Loss: 0.00001881
Iteration 171/1000 | Loss: 0.00001881
Iteration 172/1000 | Loss: 0.00001881
Iteration 173/1000 | Loss: 0.00001881
Iteration 174/1000 | Loss: 0.00001881
Iteration 175/1000 | Loss: 0.00001881
Iteration 176/1000 | Loss: 0.00001880
Iteration 177/1000 | Loss: 0.00001880
Iteration 178/1000 | Loss: 0.00001880
Iteration 179/1000 | Loss: 0.00001880
Iteration 180/1000 | Loss: 0.00001880
Iteration 181/1000 | Loss: 0.00001880
Iteration 182/1000 | Loss: 0.00001880
Iteration 183/1000 | Loss: 0.00001880
Iteration 184/1000 | Loss: 0.00001879
Iteration 185/1000 | Loss: 0.00001879
Iteration 186/1000 | Loss: 0.00001879
Iteration 187/1000 | Loss: 0.00001879
Iteration 188/1000 | Loss: 0.00001879
Iteration 189/1000 | Loss: 0.00001879
Iteration 190/1000 | Loss: 0.00001879
Iteration 191/1000 | Loss: 0.00001878
Iteration 192/1000 | Loss: 0.00001878
Iteration 193/1000 | Loss: 0.00001878
Iteration 194/1000 | Loss: 0.00001878
Iteration 195/1000 | Loss: 0.00001878
Iteration 196/1000 | Loss: 0.00001878
Iteration 197/1000 | Loss: 0.00001878
Iteration 198/1000 | Loss: 0.00001878
Iteration 199/1000 | Loss: 0.00001878
Iteration 200/1000 | Loss: 0.00001878
Iteration 201/1000 | Loss: 0.00001878
Iteration 202/1000 | Loss: 0.00001878
Iteration 203/1000 | Loss: 0.00001878
Iteration 204/1000 | Loss: 0.00001878
Iteration 205/1000 | Loss: 0.00001878
Iteration 206/1000 | Loss: 0.00001878
Iteration 207/1000 | Loss: 0.00001878
Iteration 208/1000 | Loss: 0.00001878
Iteration 209/1000 | Loss: 0.00001878
Iteration 210/1000 | Loss: 0.00001878
Iteration 211/1000 | Loss: 0.00001877
Iteration 212/1000 | Loss: 0.00001877
Iteration 213/1000 | Loss: 0.00001877
Iteration 214/1000 | Loss: 0.00001877
Iteration 215/1000 | Loss: 0.00001877
Iteration 216/1000 | Loss: 0.00001877
Iteration 217/1000 | Loss: 0.00001877
Iteration 218/1000 | Loss: 0.00001877
Iteration 219/1000 | Loss: 0.00001877
Iteration 220/1000 | Loss: 0.00001877
Iteration 221/1000 | Loss: 0.00001877
Iteration 222/1000 | Loss: 0.00001877
Iteration 223/1000 | Loss: 0.00001877
Iteration 224/1000 | Loss: 0.00001877
Iteration 225/1000 | Loss: 0.00001877
Iteration 226/1000 | Loss: 0.00001877
Iteration 227/1000 | Loss: 0.00001877
Iteration 228/1000 | Loss: 0.00001877
Iteration 229/1000 | Loss: 0.00001877
Iteration 230/1000 | Loss: 0.00001877
Iteration 231/1000 | Loss: 0.00001877
Iteration 232/1000 | Loss: 0.00001877
Iteration 233/1000 | Loss: 0.00001877
Iteration 234/1000 | Loss: 0.00001877
Iteration 235/1000 | Loss: 0.00001877
Iteration 236/1000 | Loss: 0.00001877
Iteration 237/1000 | Loss: 0.00001877
Iteration 238/1000 | Loss: 0.00001877
Iteration 239/1000 | Loss: 0.00001877
Iteration 240/1000 | Loss: 0.00001877
Iteration 241/1000 | Loss: 0.00001877
Iteration 242/1000 | Loss: 0.00001877
Iteration 243/1000 | Loss: 0.00001877
Iteration 244/1000 | Loss: 0.00001877
Iteration 245/1000 | Loss: 0.00001877
Iteration 246/1000 | Loss: 0.00001877
Iteration 247/1000 | Loss: 0.00001877
Iteration 248/1000 | Loss: 0.00001877
Iteration 249/1000 | Loss: 0.00001877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.876842179626692e-05, 1.876842179626692e-05, 1.876842179626692e-05, 1.876842179626692e-05, 1.876842179626692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.876842179626692e-05

Optimization complete. Final v2v error: 3.6342854499816895 mm

Highest mean error: 6.031085968017578 mm for frame 166

Lowest mean error: 2.8888378143310547 mm for frame 191

Saving results

Total time: 67.50804424285889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492174
Iteration 2/25 | Loss: 0.00138634
Iteration 3/25 | Loss: 0.00129654
Iteration 4/25 | Loss: 0.00127873
Iteration 5/25 | Loss: 0.00127309
Iteration 6/25 | Loss: 0.00127284
Iteration 7/25 | Loss: 0.00127284
Iteration 8/25 | Loss: 0.00127284
Iteration 9/25 | Loss: 0.00127284
Iteration 10/25 | Loss: 0.00127284
Iteration 11/25 | Loss: 0.00127284
Iteration 12/25 | Loss: 0.00127284
Iteration 13/25 | Loss: 0.00127284
Iteration 14/25 | Loss: 0.00127284
Iteration 15/25 | Loss: 0.00127284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012728357687592506, 0.0012728357687592506, 0.0012728357687592506, 0.0012728357687592506, 0.0012728357687592506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012728357687592506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33607042
Iteration 2/25 | Loss: 0.00117194
Iteration 3/25 | Loss: 0.00117194
Iteration 4/25 | Loss: 0.00117194
Iteration 5/25 | Loss: 0.00117194
Iteration 6/25 | Loss: 0.00117194
Iteration 7/25 | Loss: 0.00117193
Iteration 8/25 | Loss: 0.00117193
Iteration 9/25 | Loss: 0.00117193
Iteration 10/25 | Loss: 0.00117193
Iteration 11/25 | Loss: 0.00117193
Iteration 12/25 | Loss: 0.00117193
Iteration 13/25 | Loss: 0.00117193
Iteration 14/25 | Loss: 0.00117193
Iteration 15/25 | Loss: 0.00117193
Iteration 16/25 | Loss: 0.00117193
Iteration 17/25 | Loss: 0.00117193
Iteration 18/25 | Loss: 0.00117193
Iteration 19/25 | Loss: 0.00117193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011719337198883295, 0.0011719337198883295, 0.0011719337198883295, 0.0011719337198883295, 0.0011719337198883295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011719337198883295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117193
Iteration 2/1000 | Loss: 0.00004842
Iteration 3/1000 | Loss: 0.00003314
Iteration 4/1000 | Loss: 0.00002881
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002562
Iteration 7/1000 | Loss: 0.00002495
Iteration 8/1000 | Loss: 0.00002445
Iteration 9/1000 | Loss: 0.00002412
Iteration 10/1000 | Loss: 0.00002378
Iteration 11/1000 | Loss: 0.00002364
Iteration 12/1000 | Loss: 0.00002342
Iteration 13/1000 | Loss: 0.00002341
Iteration 14/1000 | Loss: 0.00002340
Iteration 15/1000 | Loss: 0.00002331
Iteration 16/1000 | Loss: 0.00002327
Iteration 17/1000 | Loss: 0.00002327
Iteration 18/1000 | Loss: 0.00002326
Iteration 19/1000 | Loss: 0.00002318
Iteration 20/1000 | Loss: 0.00002317
Iteration 21/1000 | Loss: 0.00002316
Iteration 22/1000 | Loss: 0.00002309
Iteration 23/1000 | Loss: 0.00002303
Iteration 24/1000 | Loss: 0.00002303
Iteration 25/1000 | Loss: 0.00002294
Iteration 26/1000 | Loss: 0.00002292
Iteration 27/1000 | Loss: 0.00002291
Iteration 28/1000 | Loss: 0.00002291
Iteration 29/1000 | Loss: 0.00002290
Iteration 30/1000 | Loss: 0.00002290
Iteration 31/1000 | Loss: 0.00002283
Iteration 32/1000 | Loss: 0.00002279
Iteration 33/1000 | Loss: 0.00002279
Iteration 34/1000 | Loss: 0.00002270
Iteration 35/1000 | Loss: 0.00002268
Iteration 36/1000 | Loss: 0.00002266
Iteration 37/1000 | Loss: 0.00002266
Iteration 38/1000 | Loss: 0.00002266
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002265
Iteration 41/1000 | Loss: 0.00002264
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002263
Iteration 44/1000 | Loss: 0.00002263
Iteration 45/1000 | Loss: 0.00002263
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002262
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002262
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002260
Iteration 57/1000 | Loss: 0.00002260
Iteration 58/1000 | Loss: 0.00002260
Iteration 59/1000 | Loss: 0.00002260
Iteration 60/1000 | Loss: 0.00002260
Iteration 61/1000 | Loss: 0.00002259
Iteration 62/1000 | Loss: 0.00002259
Iteration 63/1000 | Loss: 0.00002259
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002258
Iteration 66/1000 | Loss: 0.00002258
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002257
Iteration 69/1000 | Loss: 0.00002257
Iteration 70/1000 | Loss: 0.00002256
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002256
Iteration 73/1000 | Loss: 0.00002255
Iteration 74/1000 | Loss: 0.00002255
Iteration 75/1000 | Loss: 0.00002255
Iteration 76/1000 | Loss: 0.00002255
Iteration 77/1000 | Loss: 0.00002255
Iteration 78/1000 | Loss: 0.00002255
Iteration 79/1000 | Loss: 0.00002254
Iteration 80/1000 | Loss: 0.00002254
Iteration 81/1000 | Loss: 0.00002254
Iteration 82/1000 | Loss: 0.00002253
Iteration 83/1000 | Loss: 0.00002253
Iteration 84/1000 | Loss: 0.00002253
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002252
Iteration 87/1000 | Loss: 0.00002252
Iteration 88/1000 | Loss: 0.00002251
Iteration 89/1000 | Loss: 0.00002251
Iteration 90/1000 | Loss: 0.00002251
Iteration 91/1000 | Loss: 0.00002251
Iteration 92/1000 | Loss: 0.00002250
Iteration 93/1000 | Loss: 0.00002250
Iteration 94/1000 | Loss: 0.00002250
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002249
Iteration 98/1000 | Loss: 0.00002249
Iteration 99/1000 | Loss: 0.00002248
Iteration 100/1000 | Loss: 0.00002248
Iteration 101/1000 | Loss: 0.00002247
Iteration 102/1000 | Loss: 0.00002247
Iteration 103/1000 | Loss: 0.00002247
Iteration 104/1000 | Loss: 0.00002247
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002246
Iteration 110/1000 | Loss: 0.00002246
Iteration 111/1000 | Loss: 0.00002246
Iteration 112/1000 | Loss: 0.00002246
Iteration 113/1000 | Loss: 0.00002246
Iteration 114/1000 | Loss: 0.00002246
Iteration 115/1000 | Loss: 0.00002246
Iteration 116/1000 | Loss: 0.00002245
Iteration 117/1000 | Loss: 0.00002245
Iteration 118/1000 | Loss: 0.00002245
Iteration 119/1000 | Loss: 0.00002244
Iteration 120/1000 | Loss: 0.00002244
Iteration 121/1000 | Loss: 0.00002244
Iteration 122/1000 | Loss: 0.00002244
Iteration 123/1000 | Loss: 0.00002244
Iteration 124/1000 | Loss: 0.00002244
Iteration 125/1000 | Loss: 0.00002244
Iteration 126/1000 | Loss: 0.00002244
Iteration 127/1000 | Loss: 0.00002244
Iteration 128/1000 | Loss: 0.00002243
Iteration 129/1000 | Loss: 0.00002243
Iteration 130/1000 | Loss: 0.00002243
Iteration 131/1000 | Loss: 0.00002243
Iteration 132/1000 | Loss: 0.00002243
Iteration 133/1000 | Loss: 0.00002243
Iteration 134/1000 | Loss: 0.00002243
Iteration 135/1000 | Loss: 0.00002243
Iteration 136/1000 | Loss: 0.00002243
Iteration 137/1000 | Loss: 0.00002243
Iteration 138/1000 | Loss: 0.00002243
Iteration 139/1000 | Loss: 0.00002243
Iteration 140/1000 | Loss: 0.00002243
Iteration 141/1000 | Loss: 0.00002243
Iteration 142/1000 | Loss: 0.00002243
Iteration 143/1000 | Loss: 0.00002242
Iteration 144/1000 | Loss: 0.00002242
Iteration 145/1000 | Loss: 0.00002242
Iteration 146/1000 | Loss: 0.00002242
Iteration 147/1000 | Loss: 0.00002242
Iteration 148/1000 | Loss: 0.00002242
Iteration 149/1000 | Loss: 0.00002242
Iteration 150/1000 | Loss: 0.00002242
Iteration 151/1000 | Loss: 0.00002242
Iteration 152/1000 | Loss: 0.00002242
Iteration 153/1000 | Loss: 0.00002242
Iteration 154/1000 | Loss: 0.00002242
Iteration 155/1000 | Loss: 0.00002242
Iteration 156/1000 | Loss: 0.00002242
Iteration 157/1000 | Loss: 0.00002242
Iteration 158/1000 | Loss: 0.00002242
Iteration 159/1000 | Loss: 0.00002242
Iteration 160/1000 | Loss: 0.00002242
Iteration 161/1000 | Loss: 0.00002242
Iteration 162/1000 | Loss: 0.00002242
Iteration 163/1000 | Loss: 0.00002242
Iteration 164/1000 | Loss: 0.00002242
Iteration 165/1000 | Loss: 0.00002242
Iteration 166/1000 | Loss: 0.00002242
Iteration 167/1000 | Loss: 0.00002242
Iteration 168/1000 | Loss: 0.00002242
Iteration 169/1000 | Loss: 0.00002242
Iteration 170/1000 | Loss: 0.00002242
Iteration 171/1000 | Loss: 0.00002242
Iteration 172/1000 | Loss: 0.00002242
Iteration 173/1000 | Loss: 0.00002242
Iteration 174/1000 | Loss: 0.00002242
Iteration 175/1000 | Loss: 0.00002242
Iteration 176/1000 | Loss: 0.00002242
Iteration 177/1000 | Loss: 0.00002242
Iteration 178/1000 | Loss: 0.00002242
Iteration 179/1000 | Loss: 0.00002242
Iteration 180/1000 | Loss: 0.00002242
Iteration 181/1000 | Loss: 0.00002242
Iteration 182/1000 | Loss: 0.00002242
Iteration 183/1000 | Loss: 0.00002242
Iteration 184/1000 | Loss: 0.00002242
Iteration 185/1000 | Loss: 0.00002242
Iteration 186/1000 | Loss: 0.00002242
Iteration 187/1000 | Loss: 0.00002242
Iteration 188/1000 | Loss: 0.00002242
Iteration 189/1000 | Loss: 0.00002242
Iteration 190/1000 | Loss: 0.00002242
Iteration 191/1000 | Loss: 0.00002242
Iteration 192/1000 | Loss: 0.00002242
Iteration 193/1000 | Loss: 0.00002242
Iteration 194/1000 | Loss: 0.00002242
Iteration 195/1000 | Loss: 0.00002242
Iteration 196/1000 | Loss: 0.00002242
Iteration 197/1000 | Loss: 0.00002242
Iteration 198/1000 | Loss: 0.00002242
Iteration 199/1000 | Loss: 0.00002242
Iteration 200/1000 | Loss: 0.00002242
Iteration 201/1000 | Loss: 0.00002242
Iteration 202/1000 | Loss: 0.00002242
Iteration 203/1000 | Loss: 0.00002242
Iteration 204/1000 | Loss: 0.00002242
Iteration 205/1000 | Loss: 0.00002242
Iteration 206/1000 | Loss: 0.00002242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.2418615117203444e-05, 2.2418615117203444e-05, 2.2418615117203444e-05, 2.2418615117203444e-05, 2.2418615117203444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2418615117203444e-05

Optimization complete. Final v2v error: 3.644679307937622 mm

Highest mean error: 4.669939994812012 mm for frame 93

Lowest mean error: 3.150832176208496 mm for frame 135

Saving results

Total time: 45.828293561935425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00668738
Iteration 2/25 | Loss: 0.00160034
Iteration 3/25 | Loss: 0.00138472
Iteration 4/25 | Loss: 0.00128636
Iteration 5/25 | Loss: 0.00127100
Iteration 6/25 | Loss: 0.00126628
Iteration 7/25 | Loss: 0.00126124
Iteration 8/25 | Loss: 0.00125917
Iteration 9/25 | Loss: 0.00125810
Iteration 10/25 | Loss: 0.00125765
Iteration 11/25 | Loss: 0.00125752
Iteration 12/25 | Loss: 0.00125743
Iteration 13/25 | Loss: 0.00125742
Iteration 14/25 | Loss: 0.00125742
Iteration 15/25 | Loss: 0.00125741
Iteration 16/25 | Loss: 0.00125741
Iteration 17/25 | Loss: 0.00125741
Iteration 18/25 | Loss: 0.00125741
Iteration 19/25 | Loss: 0.00125741
Iteration 20/25 | Loss: 0.00125741
Iteration 21/25 | Loss: 0.00125741
Iteration 22/25 | Loss: 0.00125741
Iteration 23/25 | Loss: 0.00125741
Iteration 24/25 | Loss: 0.00125741
Iteration 25/25 | Loss: 0.00125741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.55917120
Iteration 2/25 | Loss: 0.00096869
Iteration 3/25 | Loss: 0.00096860
Iteration 4/25 | Loss: 0.00096860
Iteration 5/25 | Loss: 0.00096860
Iteration 6/25 | Loss: 0.00096860
Iteration 7/25 | Loss: 0.00096860
Iteration 8/25 | Loss: 0.00096860
Iteration 9/25 | Loss: 0.00096860
Iteration 10/25 | Loss: 0.00096859
Iteration 11/25 | Loss: 0.00096859
Iteration 12/25 | Loss: 0.00096859
Iteration 13/25 | Loss: 0.00096859
Iteration 14/25 | Loss: 0.00096859
Iteration 15/25 | Loss: 0.00096859
Iteration 16/25 | Loss: 0.00096859
Iteration 17/25 | Loss: 0.00096859
Iteration 18/25 | Loss: 0.00096859
Iteration 19/25 | Loss: 0.00096859
Iteration 20/25 | Loss: 0.00096859
Iteration 21/25 | Loss: 0.00096859
Iteration 22/25 | Loss: 0.00096859
Iteration 23/25 | Loss: 0.00096859
Iteration 24/25 | Loss: 0.00096859
Iteration 25/25 | Loss: 0.00096859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009685939294286072, 0.0009685939294286072, 0.0009685939294286072, 0.0009685939294286072, 0.0009685939294286072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009685939294286072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096859
Iteration 2/1000 | Loss: 0.00003172
Iteration 3/1000 | Loss: 0.00007717
Iteration 4/1000 | Loss: 0.00010151
Iteration 5/1000 | Loss: 0.00003256
Iteration 6/1000 | Loss: 0.00002012
Iteration 7/1000 | Loss: 0.00004347
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00009221
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00006098
Iteration 12/1000 | Loss: 0.00001745
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001685
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00009625
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001609
Iteration 20/1000 | Loss: 0.00001609
Iteration 21/1000 | Loss: 0.00001608
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00006746
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001593
Iteration 26/1000 | Loss: 0.00001593
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001588
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001587
Iteration 35/1000 | Loss: 0.00001587
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001586
Iteration 40/1000 | Loss: 0.00001585
Iteration 41/1000 | Loss: 0.00001585
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001584
Iteration 45/1000 | Loss: 0.00001584
Iteration 46/1000 | Loss: 0.00001584
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001584
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001584
Iteration 51/1000 | Loss: 0.00001583
Iteration 52/1000 | Loss: 0.00001580
Iteration 53/1000 | Loss: 0.00001579
Iteration 54/1000 | Loss: 0.00001578
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001575
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001572
Iteration 72/1000 | Loss: 0.00001572
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001571
Iteration 75/1000 | Loss: 0.00001571
Iteration 76/1000 | Loss: 0.00001571
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001570
Iteration 79/1000 | Loss: 0.00001570
Iteration 80/1000 | Loss: 0.00001569
Iteration 81/1000 | Loss: 0.00001568
Iteration 82/1000 | Loss: 0.00001568
Iteration 83/1000 | Loss: 0.00001568
Iteration 84/1000 | Loss: 0.00001568
Iteration 85/1000 | Loss: 0.00001568
Iteration 86/1000 | Loss: 0.00001568
Iteration 87/1000 | Loss: 0.00001567
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001567
Iteration 91/1000 | Loss: 0.00001567
Iteration 92/1000 | Loss: 0.00001567
Iteration 93/1000 | Loss: 0.00001567
Iteration 94/1000 | Loss: 0.00001567
Iteration 95/1000 | Loss: 0.00001567
Iteration 96/1000 | Loss: 0.00001567
Iteration 97/1000 | Loss: 0.00001567
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001566
Iteration 101/1000 | Loss: 0.00001566
Iteration 102/1000 | Loss: 0.00001565
Iteration 103/1000 | Loss: 0.00001565
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001564
Iteration 107/1000 | Loss: 0.00001564
Iteration 108/1000 | Loss: 0.00001564
Iteration 109/1000 | Loss: 0.00001564
Iteration 110/1000 | Loss: 0.00001564
Iteration 111/1000 | Loss: 0.00001563
Iteration 112/1000 | Loss: 0.00001563
Iteration 113/1000 | Loss: 0.00001563
Iteration 114/1000 | Loss: 0.00001563
Iteration 115/1000 | Loss: 0.00001563
Iteration 116/1000 | Loss: 0.00001563
Iteration 117/1000 | Loss: 0.00001563
Iteration 118/1000 | Loss: 0.00001563
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001563
Iteration 123/1000 | Loss: 0.00001562
Iteration 124/1000 | Loss: 0.00001562
Iteration 125/1000 | Loss: 0.00001562
Iteration 126/1000 | Loss: 0.00001561
Iteration 127/1000 | Loss: 0.00001561
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001559
Iteration 134/1000 | Loss: 0.00001559
Iteration 135/1000 | Loss: 0.00001559
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001558
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001558
Iteration 146/1000 | Loss: 0.00001558
Iteration 147/1000 | Loss: 0.00001558
Iteration 148/1000 | Loss: 0.00001558
Iteration 149/1000 | Loss: 0.00001558
Iteration 150/1000 | Loss: 0.00001558
Iteration 151/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.558150688651949e-05, 1.558150688651949e-05, 1.558150688651949e-05, 1.558150688651949e-05, 1.558150688651949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.558150688651949e-05

Optimization complete. Final v2v error: 3.3503451347351074 mm

Highest mean error: 4.0404815673828125 mm for frame 7

Lowest mean error: 2.997293710708618 mm for frame 57

Saving results

Total time: 70.44728660583496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899809
Iteration 2/25 | Loss: 0.00190514
Iteration 3/25 | Loss: 0.00155970
Iteration 4/25 | Loss: 0.00152563
Iteration 5/25 | Loss: 0.00148287
Iteration 6/25 | Loss: 0.00142504
Iteration 7/25 | Loss: 0.00139682
Iteration 8/25 | Loss: 0.00138222
Iteration 9/25 | Loss: 0.00137495
Iteration 10/25 | Loss: 0.00137522
Iteration 11/25 | Loss: 0.00136748
Iteration 12/25 | Loss: 0.00136264
Iteration 13/25 | Loss: 0.00136279
Iteration 14/25 | Loss: 0.00136087
Iteration 15/25 | Loss: 0.00136352
Iteration 16/25 | Loss: 0.00135324
Iteration 17/25 | Loss: 0.00134951
Iteration 18/25 | Loss: 0.00134881
Iteration 19/25 | Loss: 0.00134795
Iteration 20/25 | Loss: 0.00134864
Iteration 21/25 | Loss: 0.00134766
Iteration 22/25 | Loss: 0.00134717
Iteration 23/25 | Loss: 0.00134705
Iteration 24/25 | Loss: 0.00134696
Iteration 25/25 | Loss: 0.00134686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34626079
Iteration 2/25 | Loss: 0.00116253
Iteration 3/25 | Loss: 0.00116251
Iteration 4/25 | Loss: 0.00116251
Iteration 5/25 | Loss: 0.00116251
Iteration 6/25 | Loss: 0.00116251
Iteration 7/25 | Loss: 0.00116251
Iteration 8/25 | Loss: 0.00116251
Iteration 9/25 | Loss: 0.00116251
Iteration 10/25 | Loss: 0.00116251
Iteration 11/25 | Loss: 0.00116251
Iteration 12/25 | Loss: 0.00116251
Iteration 13/25 | Loss: 0.00116251
Iteration 14/25 | Loss: 0.00116251
Iteration 15/25 | Loss: 0.00116251
Iteration 16/25 | Loss: 0.00116251
Iteration 17/25 | Loss: 0.00116251
Iteration 18/25 | Loss: 0.00116251
Iteration 19/25 | Loss: 0.00116251
Iteration 20/25 | Loss: 0.00116251
Iteration 21/25 | Loss: 0.00116251
Iteration 22/25 | Loss: 0.00116251
Iteration 23/25 | Loss: 0.00116251
Iteration 24/25 | Loss: 0.00116251
Iteration 25/25 | Loss: 0.00116251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116251
Iteration 2/1000 | Loss: 0.00034847
Iteration 3/1000 | Loss: 0.00035367
Iteration 4/1000 | Loss: 0.00025814
Iteration 5/1000 | Loss: 0.00019618
Iteration 6/1000 | Loss: 0.00013304
Iteration 7/1000 | Loss: 0.00009955
Iteration 8/1000 | Loss: 0.00008646
Iteration 9/1000 | Loss: 0.00007872
Iteration 10/1000 | Loss: 0.00021374
Iteration 11/1000 | Loss: 0.00015924
Iteration 12/1000 | Loss: 0.00023646
Iteration 13/1000 | Loss: 0.00012176
Iteration 14/1000 | Loss: 0.00009844
Iteration 15/1000 | Loss: 0.00015426
Iteration 16/1000 | Loss: 0.00011528
Iteration 17/1000 | Loss: 0.00011439
Iteration 18/1000 | Loss: 0.00007585
Iteration 19/1000 | Loss: 0.00006731
Iteration 20/1000 | Loss: 0.00005645
Iteration 21/1000 | Loss: 0.00005135
Iteration 22/1000 | Loss: 0.00004642
Iteration 23/1000 | Loss: 0.00004268
Iteration 24/1000 | Loss: 0.00004002
Iteration 25/1000 | Loss: 0.00003768
Iteration 26/1000 | Loss: 0.00008339
Iteration 27/1000 | Loss: 0.00005975
Iteration 28/1000 | Loss: 0.00004416
Iteration 29/1000 | Loss: 0.00003842
Iteration 30/1000 | Loss: 0.00003473
Iteration 31/1000 | Loss: 0.00003326
Iteration 32/1000 | Loss: 0.00003180
Iteration 33/1000 | Loss: 0.00003075
Iteration 34/1000 | Loss: 0.00002961
Iteration 35/1000 | Loss: 0.00002855
Iteration 36/1000 | Loss: 0.00002790
Iteration 37/1000 | Loss: 0.00002746
Iteration 38/1000 | Loss: 0.00002715
Iteration 39/1000 | Loss: 0.00002677
Iteration 40/1000 | Loss: 0.00002640
Iteration 41/1000 | Loss: 0.00002604
Iteration 42/1000 | Loss: 0.00005399
Iteration 43/1000 | Loss: 0.00006329
Iteration 44/1000 | Loss: 0.00007013
Iteration 45/1000 | Loss: 0.00005382
Iteration 46/1000 | Loss: 0.00009637
Iteration 47/1000 | Loss: 0.00008711
Iteration 48/1000 | Loss: 0.00005007
Iteration 49/1000 | Loss: 0.00003126
Iteration 50/1000 | Loss: 0.00005052
Iteration 51/1000 | Loss: 0.00003514
Iteration 52/1000 | Loss: 0.00003113
Iteration 53/1000 | Loss: 0.00002936
Iteration 54/1000 | Loss: 0.00002722
Iteration 55/1000 | Loss: 0.00002656
Iteration 56/1000 | Loss: 0.00002618
Iteration 57/1000 | Loss: 0.00002561
Iteration 58/1000 | Loss: 0.00008301
Iteration 59/1000 | Loss: 0.00005818
Iteration 60/1000 | Loss: 0.00002590
Iteration 61/1000 | Loss: 0.00002498
Iteration 62/1000 | Loss: 0.00002480
Iteration 63/1000 | Loss: 0.00002475
Iteration 64/1000 | Loss: 0.00005184
Iteration 65/1000 | Loss: 0.00007359
Iteration 66/1000 | Loss: 0.00005729
Iteration 67/1000 | Loss: 0.00004183
Iteration 68/1000 | Loss: 0.00007468
Iteration 69/1000 | Loss: 0.00007891
Iteration 70/1000 | Loss: 0.00004987
Iteration 71/1000 | Loss: 0.00002589
Iteration 72/1000 | Loss: 0.00002488
Iteration 73/1000 | Loss: 0.00002476
Iteration 74/1000 | Loss: 0.00002475
Iteration 75/1000 | Loss: 0.00002475
Iteration 76/1000 | Loss: 0.00002473
Iteration 77/1000 | Loss: 0.00002473
Iteration 78/1000 | Loss: 0.00004970
Iteration 79/1000 | Loss: 0.00003398
Iteration 80/1000 | Loss: 0.00002471
Iteration 81/1000 | Loss: 0.00002470
Iteration 82/1000 | Loss: 0.00002470
Iteration 83/1000 | Loss: 0.00002470
Iteration 84/1000 | Loss: 0.00002470
Iteration 85/1000 | Loss: 0.00002470
Iteration 86/1000 | Loss: 0.00002470
Iteration 87/1000 | Loss: 0.00002470
Iteration 88/1000 | Loss: 0.00002470
Iteration 89/1000 | Loss: 0.00002470
Iteration 90/1000 | Loss: 0.00002470
Iteration 91/1000 | Loss: 0.00002470
Iteration 92/1000 | Loss: 0.00004917
Iteration 93/1000 | Loss: 0.00003405
Iteration 94/1000 | Loss: 0.00002473
Iteration 95/1000 | Loss: 0.00004934
Iteration 96/1000 | Loss: 0.00003570
Iteration 97/1000 | Loss: 0.00002474
Iteration 98/1000 | Loss: 0.00004915
Iteration 99/1000 | Loss: 0.00006393
Iteration 100/1000 | Loss: 0.00007254
Iteration 101/1000 | Loss: 0.00002472
Iteration 102/1000 | Loss: 0.00002470
Iteration 103/1000 | Loss: 0.00002469
Iteration 104/1000 | Loss: 0.00002469
Iteration 105/1000 | Loss: 0.00002469
Iteration 106/1000 | Loss: 0.00002469
Iteration 107/1000 | Loss: 0.00002468
Iteration 108/1000 | Loss: 0.00002468
Iteration 109/1000 | Loss: 0.00002468
Iteration 110/1000 | Loss: 0.00002468
Iteration 111/1000 | Loss: 0.00002468
Iteration 112/1000 | Loss: 0.00002468
Iteration 113/1000 | Loss: 0.00002468
Iteration 114/1000 | Loss: 0.00002467
Iteration 115/1000 | Loss: 0.00002467
Iteration 116/1000 | Loss: 0.00002467
Iteration 117/1000 | Loss: 0.00002467
Iteration 118/1000 | Loss: 0.00002467
Iteration 119/1000 | Loss: 0.00002467
Iteration 120/1000 | Loss: 0.00002467
Iteration 121/1000 | Loss: 0.00002467
Iteration 122/1000 | Loss: 0.00002467
Iteration 123/1000 | Loss: 0.00002467
Iteration 124/1000 | Loss: 0.00002467
Iteration 125/1000 | Loss: 0.00002467
Iteration 126/1000 | Loss: 0.00002467
Iteration 127/1000 | Loss: 0.00002467
Iteration 128/1000 | Loss: 0.00002467
Iteration 129/1000 | Loss: 0.00002467
Iteration 130/1000 | Loss: 0.00002467
Iteration 131/1000 | Loss: 0.00002467
Iteration 132/1000 | Loss: 0.00002467
Iteration 133/1000 | Loss: 0.00002467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.467204467393458e-05, 2.467204467393458e-05, 2.467204467393458e-05, 2.467204467393458e-05, 2.467204467393458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.467204467393458e-05

Optimization complete. Final v2v error: 4.022994518280029 mm

Highest mean error: 6.3440141677856445 mm for frame 104

Lowest mean error: 3.0971102714538574 mm for frame 74

Saving results

Total time: 169.2215597629547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814933
Iteration 2/25 | Loss: 0.00153929
Iteration 3/25 | Loss: 0.00126978
Iteration 4/25 | Loss: 0.00125329
Iteration 5/25 | Loss: 0.00125011
Iteration 6/25 | Loss: 0.00124994
Iteration 7/25 | Loss: 0.00124994
Iteration 8/25 | Loss: 0.00124994
Iteration 9/25 | Loss: 0.00124994
Iteration 10/25 | Loss: 0.00124994
Iteration 11/25 | Loss: 0.00124994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012499445583671331, 0.0012499445583671331, 0.0012499445583671331, 0.0012499445583671331, 0.0012499445583671331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012499445583671331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32365561
Iteration 2/25 | Loss: 0.00098031
Iteration 3/25 | Loss: 0.00098031
Iteration 4/25 | Loss: 0.00098030
Iteration 5/25 | Loss: 0.00098030
Iteration 6/25 | Loss: 0.00098030
Iteration 7/25 | Loss: 0.00098030
Iteration 8/25 | Loss: 0.00098030
Iteration 9/25 | Loss: 0.00098030
Iteration 10/25 | Loss: 0.00098030
Iteration 11/25 | Loss: 0.00098030
Iteration 12/25 | Loss: 0.00098030
Iteration 13/25 | Loss: 0.00098030
Iteration 14/25 | Loss: 0.00098030
Iteration 15/25 | Loss: 0.00098030
Iteration 16/25 | Loss: 0.00098030
Iteration 17/25 | Loss: 0.00098030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000980302575044334, 0.000980302575044334, 0.000980302575044334, 0.000980302575044334, 0.000980302575044334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000980302575044334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098030
Iteration 2/1000 | Loss: 0.00002882
Iteration 3/1000 | Loss: 0.00002077
Iteration 4/1000 | Loss: 0.00001857
Iteration 5/1000 | Loss: 0.00001723
Iteration 6/1000 | Loss: 0.00001639
Iteration 7/1000 | Loss: 0.00001590
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001503
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001483
Iteration 15/1000 | Loss: 0.00001466
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001446
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001441
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001436
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001436
Iteration 27/1000 | Loss: 0.00001436
Iteration 28/1000 | Loss: 0.00001436
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001435
Iteration 31/1000 | Loss: 0.00001435
Iteration 32/1000 | Loss: 0.00001434
Iteration 33/1000 | Loss: 0.00001433
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001431
Iteration 36/1000 | Loss: 0.00001430
Iteration 37/1000 | Loss: 0.00001430
Iteration 38/1000 | Loss: 0.00001428
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001425
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001421
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001420
Iteration 47/1000 | Loss: 0.00001420
Iteration 48/1000 | Loss: 0.00001419
Iteration 49/1000 | Loss: 0.00001419
Iteration 50/1000 | Loss: 0.00001418
Iteration 51/1000 | Loss: 0.00001418
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001417
Iteration 54/1000 | Loss: 0.00001417
Iteration 55/1000 | Loss: 0.00001417
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001417
Iteration 58/1000 | Loss: 0.00001416
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001411
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001410
Iteration 80/1000 | Loss: 0.00001410
Iteration 81/1000 | Loss: 0.00001409
Iteration 82/1000 | Loss: 0.00001409
Iteration 83/1000 | Loss: 0.00001409
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001408
Iteration 86/1000 | Loss: 0.00001408
Iteration 87/1000 | Loss: 0.00001408
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001408
Iteration 94/1000 | Loss: 0.00001408
Iteration 95/1000 | Loss: 0.00001408
Iteration 96/1000 | Loss: 0.00001408
Iteration 97/1000 | Loss: 0.00001408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.4075349099584855e-05, 1.4075349099584855e-05, 1.4075349099584855e-05, 1.4075349099584855e-05, 1.4075349099584855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4075349099584855e-05

Optimization complete. Final v2v error: 3.1688268184661865 mm

Highest mean error: 3.4362971782684326 mm for frame 55

Lowest mean error: 2.9658379554748535 mm for frame 110

Saving results

Total time: 34.62467837333679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763440
Iteration 2/25 | Loss: 0.00163834
Iteration 3/25 | Loss: 0.00140951
Iteration 4/25 | Loss: 0.00138698
Iteration 5/25 | Loss: 0.00137722
Iteration 6/25 | Loss: 0.00137672
Iteration 7/25 | Loss: 0.00137127
Iteration 8/25 | Loss: 0.00137251
Iteration 9/25 | Loss: 0.00136979
Iteration 10/25 | Loss: 0.00136890
Iteration 11/25 | Loss: 0.00136756
Iteration 12/25 | Loss: 0.00136587
Iteration 13/25 | Loss: 0.00136546
Iteration 14/25 | Loss: 0.00136538
Iteration 15/25 | Loss: 0.00136538
Iteration 16/25 | Loss: 0.00136538
Iteration 17/25 | Loss: 0.00136538
Iteration 18/25 | Loss: 0.00136538
Iteration 19/25 | Loss: 0.00136538
Iteration 20/25 | Loss: 0.00136538
Iteration 21/25 | Loss: 0.00136538
Iteration 22/25 | Loss: 0.00136538
Iteration 23/25 | Loss: 0.00136538
Iteration 24/25 | Loss: 0.00136538
Iteration 25/25 | Loss: 0.00136538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35252595
Iteration 2/25 | Loss: 0.00095722
Iteration 3/25 | Loss: 0.00095719
Iteration 4/25 | Loss: 0.00095719
Iteration 5/25 | Loss: 0.00095719
Iteration 6/25 | Loss: 0.00095719
Iteration 7/25 | Loss: 0.00095719
Iteration 8/25 | Loss: 0.00095718
Iteration 9/25 | Loss: 0.00095718
Iteration 10/25 | Loss: 0.00095718
Iteration 11/25 | Loss: 0.00095718
Iteration 12/25 | Loss: 0.00095718
Iteration 13/25 | Loss: 0.00095718
Iteration 14/25 | Loss: 0.00095718
Iteration 15/25 | Loss: 0.00095718
Iteration 16/25 | Loss: 0.00095718
Iteration 17/25 | Loss: 0.00095718
Iteration 18/25 | Loss: 0.00095718
Iteration 19/25 | Loss: 0.00095718
Iteration 20/25 | Loss: 0.00095718
Iteration 21/25 | Loss: 0.00095718
Iteration 22/25 | Loss: 0.00095718
Iteration 23/25 | Loss: 0.00095718
Iteration 24/25 | Loss: 0.00095718
Iteration 25/25 | Loss: 0.00095718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095718
Iteration 2/1000 | Loss: 0.00004080
Iteration 3/1000 | Loss: 0.00002808
Iteration 4/1000 | Loss: 0.00002581
Iteration 5/1000 | Loss: 0.00002443
Iteration 6/1000 | Loss: 0.00002370
Iteration 7/1000 | Loss: 0.00002309
Iteration 8/1000 | Loss: 0.00002264
Iteration 9/1000 | Loss: 0.00002231
Iteration 10/1000 | Loss: 0.00002210
Iteration 11/1000 | Loss: 0.00002204
Iteration 12/1000 | Loss: 0.00002198
Iteration 13/1000 | Loss: 0.00002195
Iteration 14/1000 | Loss: 0.00002193
Iteration 15/1000 | Loss: 0.00002176
Iteration 16/1000 | Loss: 0.00002176
Iteration 17/1000 | Loss: 0.00002175
Iteration 18/1000 | Loss: 0.00002168
Iteration 19/1000 | Loss: 0.00002166
Iteration 20/1000 | Loss: 0.00002163
Iteration 21/1000 | Loss: 0.00002162
Iteration 22/1000 | Loss: 0.00002153
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002151
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002149
Iteration 27/1000 | Loss: 0.00002148
Iteration 28/1000 | Loss: 0.00002148
Iteration 29/1000 | Loss: 0.00002148
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002147
Iteration 32/1000 | Loss: 0.00002146
Iteration 33/1000 | Loss: 0.00002145
Iteration 34/1000 | Loss: 0.00002144
Iteration 35/1000 | Loss: 0.00002143
Iteration 36/1000 | Loss: 0.00002143
Iteration 37/1000 | Loss: 0.00002142
Iteration 38/1000 | Loss: 0.00002142
Iteration 39/1000 | Loss: 0.00002139
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002139
Iteration 43/1000 | Loss: 0.00002139
Iteration 44/1000 | Loss: 0.00002139
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002139
Iteration 47/1000 | Loss: 0.00002139
Iteration 48/1000 | Loss: 0.00002138
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00002135
Iteration 60/1000 | Loss: 0.00002135
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002132
Iteration 68/1000 | Loss: 0.00002132
Iteration 69/1000 | Loss: 0.00002132
Iteration 70/1000 | Loss: 0.00002132
Iteration 71/1000 | Loss: 0.00002132
Iteration 72/1000 | Loss: 0.00002131
Iteration 73/1000 | Loss: 0.00002131
Iteration 74/1000 | Loss: 0.00002131
Iteration 75/1000 | Loss: 0.00002130
Iteration 76/1000 | Loss: 0.00002130
Iteration 77/1000 | Loss: 0.00002130
Iteration 78/1000 | Loss: 0.00002130
Iteration 79/1000 | Loss: 0.00002129
Iteration 80/1000 | Loss: 0.00002129
Iteration 81/1000 | Loss: 0.00002129
Iteration 82/1000 | Loss: 0.00002128
Iteration 83/1000 | Loss: 0.00002128
Iteration 84/1000 | Loss: 0.00002128
Iteration 85/1000 | Loss: 0.00002128
Iteration 86/1000 | Loss: 0.00002128
Iteration 87/1000 | Loss: 0.00002128
Iteration 88/1000 | Loss: 0.00002128
Iteration 89/1000 | Loss: 0.00002128
Iteration 90/1000 | Loss: 0.00002127
Iteration 91/1000 | Loss: 0.00002127
Iteration 92/1000 | Loss: 0.00002127
Iteration 93/1000 | Loss: 0.00002127
Iteration 94/1000 | Loss: 0.00002127
Iteration 95/1000 | Loss: 0.00002127
Iteration 96/1000 | Loss: 0.00002127
Iteration 97/1000 | Loss: 0.00002127
Iteration 98/1000 | Loss: 0.00002127
Iteration 99/1000 | Loss: 0.00002126
Iteration 100/1000 | Loss: 0.00002126
Iteration 101/1000 | Loss: 0.00002126
Iteration 102/1000 | Loss: 0.00002126
Iteration 103/1000 | Loss: 0.00002126
Iteration 104/1000 | Loss: 0.00002126
Iteration 105/1000 | Loss: 0.00002126
Iteration 106/1000 | Loss: 0.00002125
Iteration 107/1000 | Loss: 0.00002125
Iteration 108/1000 | Loss: 0.00002125
Iteration 109/1000 | Loss: 0.00002125
Iteration 110/1000 | Loss: 0.00002125
Iteration 111/1000 | Loss: 0.00002125
Iteration 112/1000 | Loss: 0.00002125
Iteration 113/1000 | Loss: 0.00002125
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002124
Iteration 127/1000 | Loss: 0.00002124
Iteration 128/1000 | Loss: 0.00002124
Iteration 129/1000 | Loss: 0.00002124
Iteration 130/1000 | Loss: 0.00002123
Iteration 131/1000 | Loss: 0.00002123
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002123
Iteration 134/1000 | Loss: 0.00002123
Iteration 135/1000 | Loss: 0.00002123
Iteration 136/1000 | Loss: 0.00002123
Iteration 137/1000 | Loss: 0.00002123
Iteration 138/1000 | Loss: 0.00002123
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00002122
Iteration 141/1000 | Loss: 0.00002122
Iteration 142/1000 | Loss: 0.00002122
Iteration 143/1000 | Loss: 0.00002122
Iteration 144/1000 | Loss: 0.00002122
Iteration 145/1000 | Loss: 0.00002122
Iteration 146/1000 | Loss: 0.00002122
Iteration 147/1000 | Loss: 0.00002122
Iteration 148/1000 | Loss: 0.00002122
Iteration 149/1000 | Loss: 0.00002121
Iteration 150/1000 | Loss: 0.00002121
Iteration 151/1000 | Loss: 0.00002121
Iteration 152/1000 | Loss: 0.00002121
Iteration 153/1000 | Loss: 0.00002121
Iteration 154/1000 | Loss: 0.00002121
Iteration 155/1000 | Loss: 0.00002121
Iteration 156/1000 | Loss: 0.00002121
Iteration 157/1000 | Loss: 0.00002121
Iteration 158/1000 | Loss: 0.00002121
Iteration 159/1000 | Loss: 0.00002121
Iteration 160/1000 | Loss: 0.00002121
Iteration 161/1000 | Loss: 0.00002121
Iteration 162/1000 | Loss: 0.00002121
Iteration 163/1000 | Loss: 0.00002121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.1208008547546342e-05, 2.1208008547546342e-05, 2.1208008547546342e-05, 2.1208008547546342e-05, 2.1208008547546342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1208008547546342e-05

Optimization complete. Final v2v error: 3.7678115367889404 mm

Highest mean error: 5.704185485839844 mm for frame 65

Lowest mean error: 3.3305037021636963 mm for frame 193

Saving results

Total time: 62.38555192947388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975696
Iteration 2/25 | Loss: 0.00338555
Iteration 3/25 | Loss: 0.00236411
Iteration 4/25 | Loss: 0.00211440
Iteration 5/25 | Loss: 0.00190641
Iteration 6/25 | Loss: 0.00194251
Iteration 7/25 | Loss: 0.00184401
Iteration 8/25 | Loss: 0.00174257
Iteration 9/25 | Loss: 0.00166176
Iteration 10/25 | Loss: 0.00159808
Iteration 11/25 | Loss: 0.00160739
Iteration 12/25 | Loss: 0.00158717
Iteration 13/25 | Loss: 0.00161036
Iteration 14/25 | Loss: 0.00156320
Iteration 15/25 | Loss: 0.00153455
Iteration 16/25 | Loss: 0.00154175
Iteration 17/25 | Loss: 0.00153249
Iteration 18/25 | Loss: 0.00151564
Iteration 19/25 | Loss: 0.00150866
Iteration 20/25 | Loss: 0.00150390
Iteration 21/25 | Loss: 0.00150270
Iteration 22/25 | Loss: 0.00150142
Iteration 23/25 | Loss: 0.00149984
Iteration 24/25 | Loss: 0.00149979
Iteration 25/25 | Loss: 0.00150010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37442482
Iteration 2/25 | Loss: 0.00321722
Iteration 3/25 | Loss: 0.00271497
Iteration 4/25 | Loss: 0.00263788
Iteration 5/25 | Loss: 0.00263788
Iteration 6/25 | Loss: 0.00263788
Iteration 7/25 | Loss: 0.00263788
Iteration 8/25 | Loss: 0.00263788
Iteration 9/25 | Loss: 0.00263788
Iteration 10/25 | Loss: 0.00263788
Iteration 11/25 | Loss: 0.00263787
Iteration 12/25 | Loss: 0.00263787
Iteration 13/25 | Loss: 0.00263787
Iteration 14/25 | Loss: 0.00263787
Iteration 15/25 | Loss: 0.00263787
Iteration 16/25 | Loss: 0.00263787
Iteration 17/25 | Loss: 0.00263787
Iteration 18/25 | Loss: 0.00263787
Iteration 19/25 | Loss: 0.00263787
Iteration 20/25 | Loss: 0.00263787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002637874335050583, 0.002637874335050583, 0.002637874335050583, 0.002637874335050583, 0.002637874335050583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002637874335050583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263787
Iteration 2/1000 | Loss: 0.00116530
Iteration 3/1000 | Loss: 0.00081329
Iteration 4/1000 | Loss: 0.00117999
Iteration 5/1000 | Loss: 0.00186114
Iteration 6/1000 | Loss: 0.00037071
Iteration 7/1000 | Loss: 0.00066220
Iteration 8/1000 | Loss: 0.00078038
Iteration 9/1000 | Loss: 0.00314702
Iteration 10/1000 | Loss: 0.00756013
Iteration 11/1000 | Loss: 0.00297098
Iteration 12/1000 | Loss: 0.00144107
Iteration 13/1000 | Loss: 0.00029010
Iteration 14/1000 | Loss: 0.00022219
Iteration 15/1000 | Loss: 0.00025008
Iteration 16/1000 | Loss: 0.00113873
Iteration 17/1000 | Loss: 0.00046587
Iteration 18/1000 | Loss: 0.00017631
Iteration 19/1000 | Loss: 0.00028451
Iteration 20/1000 | Loss: 0.00048780
Iteration 21/1000 | Loss: 0.00016597
Iteration 22/1000 | Loss: 0.00019233
Iteration 23/1000 | Loss: 0.00024347
Iteration 24/1000 | Loss: 0.00056180
Iteration 25/1000 | Loss: 0.00036131
Iteration 26/1000 | Loss: 0.00106364
Iteration 27/1000 | Loss: 0.00062217
Iteration 28/1000 | Loss: 0.00078195
Iteration 29/1000 | Loss: 0.00017816
Iteration 30/1000 | Loss: 0.00067968
Iteration 31/1000 | Loss: 0.00074657
Iteration 32/1000 | Loss: 0.00052444
Iteration 33/1000 | Loss: 0.00059392
Iteration 34/1000 | Loss: 0.00083400
Iteration 35/1000 | Loss: 0.00130757
Iteration 36/1000 | Loss: 0.00019400
Iteration 37/1000 | Loss: 0.00033360
Iteration 38/1000 | Loss: 0.00034127
Iteration 39/1000 | Loss: 0.00071625
Iteration 40/1000 | Loss: 0.00257632
Iteration 41/1000 | Loss: 0.00153880
Iteration 42/1000 | Loss: 0.00074536
Iteration 43/1000 | Loss: 0.00049325
Iteration 44/1000 | Loss: 0.00076955
Iteration 45/1000 | Loss: 0.00068241
Iteration 46/1000 | Loss: 0.00038044
Iteration 47/1000 | Loss: 0.00018281
Iteration 48/1000 | Loss: 0.00031485
Iteration 49/1000 | Loss: 0.00017663
Iteration 50/1000 | Loss: 0.00015246
Iteration 51/1000 | Loss: 0.00043004
Iteration 52/1000 | Loss: 0.00023763
Iteration 53/1000 | Loss: 0.00096658
Iteration 54/1000 | Loss: 0.00094507
Iteration 55/1000 | Loss: 0.00011101
Iteration 56/1000 | Loss: 0.00017135
Iteration 57/1000 | Loss: 0.00015392
Iteration 58/1000 | Loss: 0.00041929
Iteration 59/1000 | Loss: 0.00008894
Iteration 60/1000 | Loss: 0.00013224
Iteration 61/1000 | Loss: 0.00007370
Iteration 62/1000 | Loss: 0.00037700
Iteration 63/1000 | Loss: 0.00011764
Iteration 64/1000 | Loss: 0.00007449
Iteration 65/1000 | Loss: 0.00014429
Iteration 66/1000 | Loss: 0.00015865
Iteration 67/1000 | Loss: 0.00043904
Iteration 68/1000 | Loss: 0.00158484
Iteration 69/1000 | Loss: 0.00019262
Iteration 70/1000 | Loss: 0.00011155
Iteration 71/1000 | Loss: 0.00052375
Iteration 72/1000 | Loss: 0.00014084
Iteration 73/1000 | Loss: 0.00008354
Iteration 74/1000 | Loss: 0.00005943
Iteration 75/1000 | Loss: 0.00006033
Iteration 76/1000 | Loss: 0.00029280
Iteration 77/1000 | Loss: 0.00037966
Iteration 78/1000 | Loss: 0.00184379
Iteration 79/1000 | Loss: 0.00079812
Iteration 80/1000 | Loss: 0.00160193
Iteration 81/1000 | Loss: 0.00006535
Iteration 82/1000 | Loss: 0.00023143
Iteration 83/1000 | Loss: 0.00005783
Iteration 84/1000 | Loss: 0.00061337
Iteration 85/1000 | Loss: 0.00014616
Iteration 86/1000 | Loss: 0.00032395
Iteration 87/1000 | Loss: 0.00041772
Iteration 88/1000 | Loss: 0.00072092
Iteration 89/1000 | Loss: 0.00116937
Iteration 90/1000 | Loss: 0.00066105
Iteration 91/1000 | Loss: 0.00127534
Iteration 92/1000 | Loss: 0.00010991
Iteration 93/1000 | Loss: 0.00011113
Iteration 94/1000 | Loss: 0.00019488
Iteration 95/1000 | Loss: 0.00038064
Iteration 96/1000 | Loss: 0.00013929
Iteration 97/1000 | Loss: 0.00005149
Iteration 98/1000 | Loss: 0.00020838
Iteration 99/1000 | Loss: 0.00026891
Iteration 100/1000 | Loss: 0.00005399
Iteration 101/1000 | Loss: 0.00004388
Iteration 102/1000 | Loss: 0.00003596
Iteration 103/1000 | Loss: 0.00008311
Iteration 104/1000 | Loss: 0.00005515
Iteration 105/1000 | Loss: 0.00003144
Iteration 106/1000 | Loss: 0.00005424
Iteration 107/1000 | Loss: 0.00015616
Iteration 108/1000 | Loss: 0.00016576
Iteration 109/1000 | Loss: 0.00003589
Iteration 110/1000 | Loss: 0.00003029
Iteration 111/1000 | Loss: 0.00005566
Iteration 112/1000 | Loss: 0.00008469
Iteration 113/1000 | Loss: 0.00002763
Iteration 114/1000 | Loss: 0.00007091
Iteration 115/1000 | Loss: 0.00017290
Iteration 116/1000 | Loss: 0.00004871
Iteration 117/1000 | Loss: 0.00005113
Iteration 118/1000 | Loss: 0.00002734
Iteration 119/1000 | Loss: 0.00002616
Iteration 120/1000 | Loss: 0.00002541
Iteration 121/1000 | Loss: 0.00006385
Iteration 122/1000 | Loss: 0.00005071
Iteration 123/1000 | Loss: 0.00003435
Iteration 124/1000 | Loss: 0.00006399
Iteration 125/1000 | Loss: 0.00006139
Iteration 126/1000 | Loss: 0.00003026
Iteration 127/1000 | Loss: 0.00010613
Iteration 128/1000 | Loss: 0.00016770
Iteration 129/1000 | Loss: 0.00003518
Iteration 130/1000 | Loss: 0.00006929
Iteration 131/1000 | Loss: 0.00002729
Iteration 132/1000 | Loss: 0.00002815
Iteration 133/1000 | Loss: 0.00009403
Iteration 134/1000 | Loss: 0.00003408
Iteration 135/1000 | Loss: 0.00003401
Iteration 136/1000 | Loss: 0.00003139
Iteration 137/1000 | Loss: 0.00049715
Iteration 138/1000 | Loss: 0.00019368
Iteration 139/1000 | Loss: 0.00015186
Iteration 140/1000 | Loss: 0.00011521
Iteration 141/1000 | Loss: 0.00036787
Iteration 142/1000 | Loss: 0.00003339
Iteration 143/1000 | Loss: 0.00004467
Iteration 144/1000 | Loss: 0.00003917
Iteration 145/1000 | Loss: 0.00002274
Iteration 146/1000 | Loss: 0.00006973
Iteration 147/1000 | Loss: 0.00006541
Iteration 148/1000 | Loss: 0.00002676
Iteration 149/1000 | Loss: 0.00002993
Iteration 150/1000 | Loss: 0.00002016
Iteration 151/1000 | Loss: 0.00003400
Iteration 152/1000 | Loss: 0.00011894
Iteration 153/1000 | Loss: 0.00002248
Iteration 154/1000 | Loss: 0.00002745
Iteration 155/1000 | Loss: 0.00001960
Iteration 156/1000 | Loss: 0.00001939
Iteration 157/1000 | Loss: 0.00002311
Iteration 158/1000 | Loss: 0.00003056
Iteration 159/1000 | Loss: 0.00004761
Iteration 160/1000 | Loss: 0.00003804
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001908
Iteration 163/1000 | Loss: 0.00001907
Iteration 164/1000 | Loss: 0.00001903
Iteration 165/1000 | Loss: 0.00002890
Iteration 166/1000 | Loss: 0.00023959
Iteration 167/1000 | Loss: 0.00016036
Iteration 168/1000 | Loss: 0.00002595
Iteration 169/1000 | Loss: 0.00002218
Iteration 170/1000 | Loss: 0.00021990
Iteration 171/1000 | Loss: 0.00003032
Iteration 172/1000 | Loss: 0.00002892
Iteration 173/1000 | Loss: 0.00002765
Iteration 174/1000 | Loss: 0.00002076
Iteration 175/1000 | Loss: 0.00003244
Iteration 176/1000 | Loss: 0.00003340
Iteration 177/1000 | Loss: 0.00002540
Iteration 178/1000 | Loss: 0.00003129
Iteration 179/1000 | Loss: 0.00002196
Iteration 180/1000 | Loss: 0.00002160
Iteration 181/1000 | Loss: 0.00002002
Iteration 182/1000 | Loss: 0.00002001
Iteration 183/1000 | Loss: 0.00002000
Iteration 184/1000 | Loss: 0.00002000
Iteration 185/1000 | Loss: 0.00002000
Iteration 186/1000 | Loss: 0.00002000
Iteration 187/1000 | Loss: 0.00002000
Iteration 188/1000 | Loss: 0.00001999
Iteration 189/1000 | Loss: 0.00001999
Iteration 190/1000 | Loss: 0.00001999
Iteration 191/1000 | Loss: 0.00001999
Iteration 192/1000 | Loss: 0.00001999
Iteration 193/1000 | Loss: 0.00001999
Iteration 194/1000 | Loss: 0.00001996
Iteration 195/1000 | Loss: 0.00002050
Iteration 196/1000 | Loss: 0.00001993
Iteration 197/1000 | Loss: 0.00003829
Iteration 198/1000 | Loss: 0.00002530
Iteration 199/1000 | Loss: 0.00001988
Iteration 200/1000 | Loss: 0.00002705
Iteration 201/1000 | Loss: 0.00003501
Iteration 202/1000 | Loss: 0.00004604
Iteration 203/1000 | Loss: 0.00016366
Iteration 204/1000 | Loss: 0.00002797
Iteration 205/1000 | Loss: 0.00003187
Iteration 206/1000 | Loss: 0.00002097
Iteration 207/1000 | Loss: 0.00003998
Iteration 208/1000 | Loss: 0.00006449
Iteration 209/1000 | Loss: 0.00003490
Iteration 210/1000 | Loss: 0.00003623
Iteration 211/1000 | Loss: 0.00001910
Iteration 212/1000 | Loss: 0.00007900
Iteration 213/1000 | Loss: 0.00005628
Iteration 214/1000 | Loss: 0.00002575
Iteration 215/1000 | Loss: 0.00002431
Iteration 216/1000 | Loss: 0.00001868
Iteration 217/1000 | Loss: 0.00001864
Iteration 218/1000 | Loss: 0.00001863
Iteration 219/1000 | Loss: 0.00001862
Iteration 220/1000 | Loss: 0.00001862
Iteration 221/1000 | Loss: 0.00001861
Iteration 222/1000 | Loss: 0.00001861
Iteration 223/1000 | Loss: 0.00001861
Iteration 224/1000 | Loss: 0.00001860
Iteration 225/1000 | Loss: 0.00001860
Iteration 226/1000 | Loss: 0.00001860
Iteration 227/1000 | Loss: 0.00001859
Iteration 228/1000 | Loss: 0.00001859
Iteration 229/1000 | Loss: 0.00001858
Iteration 230/1000 | Loss: 0.00002614
Iteration 231/1000 | Loss: 0.00006342
Iteration 232/1000 | Loss: 0.00001850
Iteration 233/1000 | Loss: 0.00001850
Iteration 234/1000 | Loss: 0.00001849
Iteration 235/1000 | Loss: 0.00001848
Iteration 236/1000 | Loss: 0.00001847
Iteration 237/1000 | Loss: 0.00001847
Iteration 238/1000 | Loss: 0.00001846
Iteration 239/1000 | Loss: 0.00001846
Iteration 240/1000 | Loss: 0.00001846
Iteration 241/1000 | Loss: 0.00001845
Iteration 242/1000 | Loss: 0.00001845
Iteration 243/1000 | Loss: 0.00001845
Iteration 244/1000 | Loss: 0.00001844
Iteration 245/1000 | Loss: 0.00001844
Iteration 246/1000 | Loss: 0.00001844
Iteration 247/1000 | Loss: 0.00001844
Iteration 248/1000 | Loss: 0.00001842
Iteration 249/1000 | Loss: 0.00001842
Iteration 250/1000 | Loss: 0.00001955
Iteration 251/1000 | Loss: 0.00001845
Iteration 252/1000 | Loss: 0.00001840
Iteration 253/1000 | Loss: 0.00001839
Iteration 254/1000 | Loss: 0.00001839
Iteration 255/1000 | Loss: 0.00001839
Iteration 256/1000 | Loss: 0.00001838
Iteration 257/1000 | Loss: 0.00001838
Iteration 258/1000 | Loss: 0.00001838
Iteration 259/1000 | Loss: 0.00001838
Iteration 260/1000 | Loss: 0.00001838
Iteration 261/1000 | Loss: 0.00001838
Iteration 262/1000 | Loss: 0.00001838
Iteration 263/1000 | Loss: 0.00001838
Iteration 264/1000 | Loss: 0.00001837
Iteration 265/1000 | Loss: 0.00001837
Iteration 266/1000 | Loss: 0.00001836
Iteration 267/1000 | Loss: 0.00001836
Iteration 268/1000 | Loss: 0.00001836
Iteration 269/1000 | Loss: 0.00001836
Iteration 270/1000 | Loss: 0.00001836
Iteration 271/1000 | Loss: 0.00001836
Iteration 272/1000 | Loss: 0.00001835
Iteration 273/1000 | Loss: 0.00001835
Iteration 274/1000 | Loss: 0.00001835
Iteration 275/1000 | Loss: 0.00001835
Iteration 276/1000 | Loss: 0.00001835
Iteration 277/1000 | Loss: 0.00001835
Iteration 278/1000 | Loss: 0.00001834
Iteration 279/1000 | Loss: 0.00001834
Iteration 280/1000 | Loss: 0.00001834
Iteration 281/1000 | Loss: 0.00001834
Iteration 282/1000 | Loss: 0.00001833
Iteration 283/1000 | Loss: 0.00001833
Iteration 284/1000 | Loss: 0.00001833
Iteration 285/1000 | Loss: 0.00001833
Iteration 286/1000 | Loss: 0.00001833
Iteration 287/1000 | Loss: 0.00001833
Iteration 288/1000 | Loss: 0.00001833
Iteration 289/1000 | Loss: 0.00001833
Iteration 290/1000 | Loss: 0.00001833
Iteration 291/1000 | Loss: 0.00001832
Iteration 292/1000 | Loss: 0.00001832
Iteration 293/1000 | Loss: 0.00001832
Iteration 294/1000 | Loss: 0.00001831
Iteration 295/1000 | Loss: 0.00001831
Iteration 296/1000 | Loss: 0.00001831
Iteration 297/1000 | Loss: 0.00001831
Iteration 298/1000 | Loss: 0.00001831
Iteration 299/1000 | Loss: 0.00001858
Iteration 300/1000 | Loss: 0.00001830
Iteration 301/1000 | Loss: 0.00001830
Iteration 302/1000 | Loss: 0.00001830
Iteration 303/1000 | Loss: 0.00001830
Iteration 304/1000 | Loss: 0.00001830
Iteration 305/1000 | Loss: 0.00001830
Iteration 306/1000 | Loss: 0.00001830
Iteration 307/1000 | Loss: 0.00001830
Iteration 308/1000 | Loss: 0.00001830
Iteration 309/1000 | Loss: 0.00001830
Iteration 310/1000 | Loss: 0.00001830
Iteration 311/1000 | Loss: 0.00001830
Iteration 312/1000 | Loss: 0.00001830
Iteration 313/1000 | Loss: 0.00001830
Iteration 314/1000 | Loss: 0.00001830
Iteration 315/1000 | Loss: 0.00001830
Iteration 316/1000 | Loss: 0.00001829
Iteration 317/1000 | Loss: 0.00001829
Iteration 318/1000 | Loss: 0.00001829
Iteration 319/1000 | Loss: 0.00001829
Iteration 320/1000 | Loss: 0.00001828
Iteration 321/1000 | Loss: 0.00001828
Iteration 322/1000 | Loss: 0.00001828
Iteration 323/1000 | Loss: 0.00001828
Iteration 324/1000 | Loss: 0.00001828
Iteration 325/1000 | Loss: 0.00001827
Iteration 326/1000 | Loss: 0.00001827
Iteration 327/1000 | Loss: 0.00001827
Iteration 328/1000 | Loss: 0.00001827
Iteration 329/1000 | Loss: 0.00001827
Iteration 330/1000 | Loss: 0.00001826
Iteration 331/1000 | Loss: 0.00001826
Iteration 332/1000 | Loss: 0.00001826
Iteration 333/1000 | Loss: 0.00001826
Iteration 334/1000 | Loss: 0.00001826
Iteration 335/1000 | Loss: 0.00001826
Iteration 336/1000 | Loss: 0.00001826
Iteration 337/1000 | Loss: 0.00001826
Iteration 338/1000 | Loss: 0.00001826
Iteration 339/1000 | Loss: 0.00001826
Iteration 340/1000 | Loss: 0.00001826
Iteration 341/1000 | Loss: 0.00001826
Iteration 342/1000 | Loss: 0.00001825
Iteration 343/1000 | Loss: 0.00001825
Iteration 344/1000 | Loss: 0.00001825
Iteration 345/1000 | Loss: 0.00001825
Iteration 346/1000 | Loss: 0.00001825
Iteration 347/1000 | Loss: 0.00001825
Iteration 348/1000 | Loss: 0.00001825
Iteration 349/1000 | Loss: 0.00001825
Iteration 350/1000 | Loss: 0.00001825
Iteration 351/1000 | Loss: 0.00001825
Iteration 352/1000 | Loss: 0.00001825
Iteration 353/1000 | Loss: 0.00001825
Iteration 354/1000 | Loss: 0.00001825
Iteration 355/1000 | Loss: 0.00002081
Iteration 356/1000 | Loss: 0.00004333
Iteration 357/1000 | Loss: 0.00001836
Iteration 358/1000 | Loss: 0.00001823
Iteration 359/1000 | Loss: 0.00001823
Iteration 360/1000 | Loss: 0.00001823
Iteration 361/1000 | Loss: 0.00001822
Iteration 362/1000 | Loss: 0.00001822
Iteration 363/1000 | Loss: 0.00001822
Iteration 364/1000 | Loss: 0.00001822
Iteration 365/1000 | Loss: 0.00001822
Iteration 366/1000 | Loss: 0.00001821
Iteration 367/1000 | Loss: 0.00001821
Iteration 368/1000 | Loss: 0.00001821
Iteration 369/1000 | Loss: 0.00001821
Iteration 370/1000 | Loss: 0.00001821
Iteration 371/1000 | Loss: 0.00001821
Iteration 372/1000 | Loss: 0.00001821
Iteration 373/1000 | Loss: 0.00001821
Iteration 374/1000 | Loss: 0.00001821
Iteration 375/1000 | Loss: 0.00001821
Iteration 376/1000 | Loss: 0.00001821
Iteration 377/1000 | Loss: 0.00001821
Iteration 378/1000 | Loss: 0.00001821
Iteration 379/1000 | Loss: 0.00001821
Iteration 380/1000 | Loss: 0.00001821
Iteration 381/1000 | Loss: 0.00001821
Iteration 382/1000 | Loss: 0.00001821
Iteration 383/1000 | Loss: 0.00001821
Iteration 384/1000 | Loss: 0.00001821
Iteration 385/1000 | Loss: 0.00001820
Iteration 386/1000 | Loss: 0.00001820
Iteration 387/1000 | Loss: 0.00001820
Iteration 388/1000 | Loss: 0.00001820
Iteration 389/1000 | Loss: 0.00001820
Iteration 390/1000 | Loss: 0.00001820
Iteration 391/1000 | Loss: 0.00001820
Iteration 392/1000 | Loss: 0.00001820
Iteration 393/1000 | Loss: 0.00001820
Iteration 394/1000 | Loss: 0.00001820
Iteration 395/1000 | Loss: 0.00001820
Iteration 396/1000 | Loss: 0.00001820
Iteration 397/1000 | Loss: 0.00001820
Iteration 398/1000 | Loss: 0.00001820
Iteration 399/1000 | Loss: 0.00001820
Iteration 400/1000 | Loss: 0.00001820
Iteration 401/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 401. Stopping optimization.
Last 5 losses: [1.8202083083451726e-05, 1.8202083083451726e-05, 1.8202083083451726e-05, 1.8202083083451726e-05, 1.8202083083451726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8202083083451726e-05

Optimization complete. Final v2v error: 3.2198569774627686 mm

Highest mean error: 10.664633750915527 mm for frame 172

Lowest mean error: 2.824185848236084 mm for frame 154

Saving results

Total time: 390.54537868499756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465715
Iteration 2/25 | Loss: 0.00134468
Iteration 3/25 | Loss: 0.00124784
Iteration 4/25 | Loss: 0.00123491
Iteration 5/25 | Loss: 0.00123177
Iteration 6/25 | Loss: 0.00123177
Iteration 7/25 | Loss: 0.00123177
Iteration 8/25 | Loss: 0.00123177
Iteration 9/25 | Loss: 0.00123177
Iteration 10/25 | Loss: 0.00123177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012317696819081903, 0.0012317696819081903, 0.0012317696819081903, 0.0012317696819081903, 0.0012317696819081903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012317696819081903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.63715458
Iteration 2/25 | Loss: 0.00102810
Iteration 3/25 | Loss: 0.00102809
Iteration 4/25 | Loss: 0.00102809
Iteration 5/25 | Loss: 0.00102809
Iteration 6/25 | Loss: 0.00102809
Iteration 7/25 | Loss: 0.00102809
Iteration 8/25 | Loss: 0.00102809
Iteration 9/25 | Loss: 0.00102809
Iteration 10/25 | Loss: 0.00102809
Iteration 11/25 | Loss: 0.00102809
Iteration 12/25 | Loss: 0.00102809
Iteration 13/25 | Loss: 0.00102809
Iteration 14/25 | Loss: 0.00102809
Iteration 15/25 | Loss: 0.00102809
Iteration 16/25 | Loss: 0.00102809
Iteration 17/25 | Loss: 0.00102809
Iteration 18/25 | Loss: 0.00102809
Iteration 19/25 | Loss: 0.00102809
Iteration 20/25 | Loss: 0.00102809
Iteration 21/25 | Loss: 0.00102809
Iteration 22/25 | Loss: 0.00102809
Iteration 23/25 | Loss: 0.00102809
Iteration 24/25 | Loss: 0.00102809
Iteration 25/25 | Loss: 0.00102809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102809
Iteration 2/1000 | Loss: 0.00002139
Iteration 3/1000 | Loss: 0.00001733
Iteration 4/1000 | Loss: 0.00001572
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001411
Iteration 8/1000 | Loss: 0.00001378
Iteration 9/1000 | Loss: 0.00001342
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001308
Iteration 15/1000 | Loss: 0.00001304
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001297
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001277
Iteration 24/1000 | Loss: 0.00001274
Iteration 25/1000 | Loss: 0.00001274
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001268
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001259
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001245
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001238
Iteration 71/1000 | Loss: 0.00001238
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001224
Iteration 122/1000 | Loss: 0.00001224
Iteration 123/1000 | Loss: 0.00001224
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001223
Iteration 127/1000 | Loss: 0.00001223
Iteration 128/1000 | Loss: 0.00001223
Iteration 129/1000 | Loss: 0.00001223
Iteration 130/1000 | Loss: 0.00001223
Iteration 131/1000 | Loss: 0.00001223
Iteration 132/1000 | Loss: 0.00001223
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001223
Iteration 135/1000 | Loss: 0.00001223
Iteration 136/1000 | Loss: 0.00001223
Iteration 137/1000 | Loss: 0.00001222
Iteration 138/1000 | Loss: 0.00001222
Iteration 139/1000 | Loss: 0.00001222
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001222
Iteration 142/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.2224827514728531e-05, 1.2224827514728531e-05, 1.2224827514728531e-05, 1.2224827514728531e-05, 1.2224827514728531e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2224827514728531e-05

Optimization complete. Final v2v error: 2.9945144653320312 mm

Highest mean error: 3.2732481956481934 mm for frame 90

Lowest mean error: 2.735133171081543 mm for frame 39

Saving results

Total time: 43.27011966705322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403927
Iteration 2/25 | Loss: 0.00129569
Iteration 3/25 | Loss: 0.00121060
Iteration 4/25 | Loss: 0.00119850
Iteration 5/25 | Loss: 0.00119494
Iteration 6/25 | Loss: 0.00119405
Iteration 7/25 | Loss: 0.00119405
Iteration 8/25 | Loss: 0.00119405
Iteration 9/25 | Loss: 0.00119405
Iteration 10/25 | Loss: 0.00119405
Iteration 11/25 | Loss: 0.00119405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011940529802814126, 0.0011940529802814126, 0.0011940529802814126, 0.0011940529802814126, 0.0011940529802814126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011940529802814126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94355416
Iteration 2/25 | Loss: 0.00101447
Iteration 3/25 | Loss: 0.00101447
Iteration 4/25 | Loss: 0.00101446
Iteration 5/25 | Loss: 0.00101446
Iteration 6/25 | Loss: 0.00101446
Iteration 7/25 | Loss: 0.00101446
Iteration 8/25 | Loss: 0.00101446
Iteration 9/25 | Loss: 0.00101446
Iteration 10/25 | Loss: 0.00101446
Iteration 11/25 | Loss: 0.00101446
Iteration 12/25 | Loss: 0.00101446
Iteration 13/25 | Loss: 0.00101446
Iteration 14/25 | Loss: 0.00101446
Iteration 15/25 | Loss: 0.00101446
Iteration 16/25 | Loss: 0.00101446
Iteration 17/25 | Loss: 0.00101446
Iteration 18/25 | Loss: 0.00101446
Iteration 19/25 | Loss: 0.00101446
Iteration 20/25 | Loss: 0.00101446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010144611587747931, 0.0010144611587747931, 0.0010144611587747931, 0.0010144611587747931, 0.0010144611587747931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010144611587747931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101446
Iteration 2/1000 | Loss: 0.00003487
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00001901
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001487
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001382
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001354
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001338
Iteration 40/1000 | Loss: 0.00001337
Iteration 41/1000 | Loss: 0.00001336
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001324
Iteration 48/1000 | Loss: 0.00001324
Iteration 49/1000 | Loss: 0.00001323
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001321
Iteration 53/1000 | Loss: 0.00001321
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001316
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001313
Iteration 73/1000 | Loss: 0.00001313
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001312
Iteration 78/1000 | Loss: 0.00001312
Iteration 79/1000 | Loss: 0.00001312
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001309
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001306
Iteration 96/1000 | Loss: 0.00001306
Iteration 97/1000 | Loss: 0.00001306
Iteration 98/1000 | Loss: 0.00001306
Iteration 99/1000 | Loss: 0.00001306
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001305
Iteration 103/1000 | Loss: 0.00001305
Iteration 104/1000 | Loss: 0.00001305
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001304
Iteration 115/1000 | Loss: 0.00001304
Iteration 116/1000 | Loss: 0.00001304
Iteration 117/1000 | Loss: 0.00001304
Iteration 118/1000 | Loss: 0.00001304
Iteration 119/1000 | Loss: 0.00001304
Iteration 120/1000 | Loss: 0.00001304
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001303
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001303
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001303
Iteration 137/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3032866263529286e-05, 1.3032866263529286e-05, 1.3032866263529286e-05, 1.3032866263529286e-05, 1.3032866263529286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3032866263529286e-05

Optimization complete. Final v2v error: 3.0986788272857666 mm

Highest mean error: 4.061856746673584 mm for frame 57

Lowest mean error: 2.83524227142334 mm for frame 91

Saving results

Total time: 43.42549538612366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872128
Iteration 2/25 | Loss: 0.00135216
Iteration 3/25 | Loss: 0.00126561
Iteration 4/25 | Loss: 0.00125008
Iteration 5/25 | Loss: 0.00124731
Iteration 6/25 | Loss: 0.00124651
Iteration 7/25 | Loss: 0.00124651
Iteration 8/25 | Loss: 0.00124651
Iteration 9/25 | Loss: 0.00124649
Iteration 10/25 | Loss: 0.00124649
Iteration 11/25 | Loss: 0.00124649
Iteration 12/25 | Loss: 0.00124649
Iteration 13/25 | Loss: 0.00124649
Iteration 14/25 | Loss: 0.00124649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012464914470911026, 0.0012464914470911026, 0.0012464914470911026, 0.0012464914470911026, 0.0012464914470911026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012464914470911026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36851406
Iteration 2/25 | Loss: 0.00090415
Iteration 3/25 | Loss: 0.00090415
Iteration 4/25 | Loss: 0.00090415
Iteration 5/25 | Loss: 0.00090415
Iteration 6/25 | Loss: 0.00090415
Iteration 7/25 | Loss: 0.00090415
Iteration 8/25 | Loss: 0.00090415
Iteration 9/25 | Loss: 0.00090415
Iteration 10/25 | Loss: 0.00090415
Iteration 11/25 | Loss: 0.00090415
Iteration 12/25 | Loss: 0.00090415
Iteration 13/25 | Loss: 0.00090415
Iteration 14/25 | Loss: 0.00090414
Iteration 15/25 | Loss: 0.00090414
Iteration 16/25 | Loss: 0.00090414
Iteration 17/25 | Loss: 0.00090414
Iteration 18/25 | Loss: 0.00090414
Iteration 19/25 | Loss: 0.00090414
Iteration 20/25 | Loss: 0.00090414
Iteration 21/25 | Loss: 0.00090414
Iteration 22/25 | Loss: 0.00090414
Iteration 23/25 | Loss: 0.00090414
Iteration 24/25 | Loss: 0.00090414
Iteration 25/25 | Loss: 0.00090414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090414
Iteration 2/1000 | Loss: 0.00002979
Iteration 3/1000 | Loss: 0.00002349
Iteration 4/1000 | Loss: 0.00002081
Iteration 5/1000 | Loss: 0.00001999
Iteration 6/1000 | Loss: 0.00001937
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001859
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001844
Iteration 11/1000 | Loss: 0.00001832
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001774
Iteration 15/1000 | Loss: 0.00001774
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001754
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001743
Iteration 21/1000 | Loss: 0.00001741
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001740
Iteration 24/1000 | Loss: 0.00001739
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001735
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00001735
Iteration 42/1000 | Loss: 0.00001735
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001727
Iteration 46/1000 | Loss: 0.00001727
Iteration 47/1000 | Loss: 0.00001725
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001723
Iteration 58/1000 | Loss: 0.00001723
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001722
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001722
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001722
Iteration 69/1000 | Loss: 0.00001722
Iteration 70/1000 | Loss: 0.00001722
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001721
Iteration 74/1000 | Loss: 0.00001721
Iteration 75/1000 | Loss: 0.00001721
Iteration 76/1000 | Loss: 0.00001721
Iteration 77/1000 | Loss: 0.00001721
Iteration 78/1000 | Loss: 0.00001721
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001720
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001719
Iteration 98/1000 | Loss: 0.00001719
Iteration 99/1000 | Loss: 0.00001719
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.718392741167918e-05, 1.718392741167918e-05, 1.718392741167918e-05, 1.718392741167918e-05, 1.718392741167918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.718392741167918e-05

Optimization complete. Final v2v error: 3.5386011600494385 mm

Highest mean error: 3.848538875579834 mm for frame 5

Lowest mean error: 3.225053071975708 mm for frame 33

Saving results

Total time: 36.66179943084717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677808
Iteration 2/25 | Loss: 0.00184955
Iteration 3/25 | Loss: 0.00144384
Iteration 4/25 | Loss: 0.00138770
Iteration 5/25 | Loss: 0.00138402
Iteration 6/25 | Loss: 0.00137204
Iteration 7/25 | Loss: 0.00135415
Iteration 8/25 | Loss: 0.00133167
Iteration 9/25 | Loss: 0.00132062
Iteration 10/25 | Loss: 0.00130980
Iteration 11/25 | Loss: 0.00129650
Iteration 12/25 | Loss: 0.00130130
Iteration 13/25 | Loss: 0.00129285
Iteration 14/25 | Loss: 0.00129071
Iteration 15/25 | Loss: 0.00128860
Iteration 16/25 | Loss: 0.00128766
Iteration 17/25 | Loss: 0.00128820
Iteration 18/25 | Loss: 0.00128853
Iteration 19/25 | Loss: 0.00128818
Iteration 20/25 | Loss: 0.00128807
Iteration 21/25 | Loss: 0.00128807
Iteration 22/25 | Loss: 0.00128841
Iteration 23/25 | Loss: 0.00128754
Iteration 24/25 | Loss: 0.00128782
Iteration 25/25 | Loss: 0.00128814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25563741
Iteration 2/25 | Loss: 0.00112826
Iteration 3/25 | Loss: 0.00112823
Iteration 4/25 | Loss: 0.00112823
Iteration 5/25 | Loss: 0.00112823
Iteration 6/25 | Loss: 0.00112823
Iteration 7/25 | Loss: 0.00112823
Iteration 8/25 | Loss: 0.00112823
Iteration 9/25 | Loss: 0.00112823
Iteration 10/25 | Loss: 0.00112823
Iteration 11/25 | Loss: 0.00112823
Iteration 12/25 | Loss: 0.00112823
Iteration 13/25 | Loss: 0.00112823
Iteration 14/25 | Loss: 0.00112823
Iteration 15/25 | Loss: 0.00112823
Iteration 16/25 | Loss: 0.00112823
Iteration 17/25 | Loss: 0.00112823
Iteration 18/25 | Loss: 0.00112823
Iteration 19/25 | Loss: 0.00112823
Iteration 20/25 | Loss: 0.00112823
Iteration 21/25 | Loss: 0.00112823
Iteration 22/25 | Loss: 0.00112823
Iteration 23/25 | Loss: 0.00112823
Iteration 24/25 | Loss: 0.00112823
Iteration 25/25 | Loss: 0.00112823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112823
Iteration 2/1000 | Loss: 0.00027608
Iteration 3/1000 | Loss: 0.00004044
Iteration 4/1000 | Loss: 0.00003491
Iteration 5/1000 | Loss: 0.00004043
Iteration 6/1000 | Loss: 0.00003701
Iteration 7/1000 | Loss: 0.00003159
Iteration 8/1000 | Loss: 0.00003085
Iteration 9/1000 | Loss: 0.00002971
Iteration 10/1000 | Loss: 0.00003996
Iteration 11/1000 | Loss: 0.00003574
Iteration 12/1000 | Loss: 0.00003772
Iteration 13/1000 | Loss: 0.00004268
Iteration 14/1000 | Loss: 0.00003153
Iteration 15/1000 | Loss: 0.00002589
Iteration 16/1000 | Loss: 0.00003381
Iteration 17/1000 | Loss: 0.00003675
Iteration 18/1000 | Loss: 0.00003713
Iteration 19/1000 | Loss: 0.00004239
Iteration 20/1000 | Loss: 0.00003600
Iteration 21/1000 | Loss: 0.00004079
Iteration 22/1000 | Loss: 0.00003028
Iteration 23/1000 | Loss: 0.00003505
Iteration 24/1000 | Loss: 0.00002715
Iteration 25/1000 | Loss: 0.00002343
Iteration 26/1000 | Loss: 0.00002252
Iteration 27/1000 | Loss: 0.00002220
Iteration 28/1000 | Loss: 0.00002196
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002179
Iteration 31/1000 | Loss: 0.00002171
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002156
Iteration 34/1000 | Loss: 0.00002151
Iteration 35/1000 | Loss: 0.00002138
Iteration 36/1000 | Loss: 0.00002134
Iteration 37/1000 | Loss: 0.00002130
Iteration 38/1000 | Loss: 0.00002126
Iteration 39/1000 | Loss: 0.00002125
Iteration 40/1000 | Loss: 0.00002123
Iteration 41/1000 | Loss: 0.00002117
Iteration 42/1000 | Loss: 0.00002117
Iteration 43/1000 | Loss: 0.00002116
Iteration 44/1000 | Loss: 0.00002110
Iteration 45/1000 | Loss: 0.00002110
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002108
Iteration 54/1000 | Loss: 0.00002108
Iteration 55/1000 | Loss: 0.00002107
Iteration 56/1000 | Loss: 0.00002107
Iteration 57/1000 | Loss: 0.00002106
Iteration 58/1000 | Loss: 0.00002106
Iteration 59/1000 | Loss: 0.00002105
Iteration 60/1000 | Loss: 0.00002105
Iteration 61/1000 | Loss: 0.00002104
Iteration 62/1000 | Loss: 0.00002103
Iteration 63/1000 | Loss: 0.00002103
Iteration 64/1000 | Loss: 0.00002103
Iteration 65/1000 | Loss: 0.00002103
Iteration 66/1000 | Loss: 0.00002102
Iteration 67/1000 | Loss: 0.00002102
Iteration 68/1000 | Loss: 0.00002101
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00002101
Iteration 71/1000 | Loss: 0.00002101
Iteration 72/1000 | Loss: 0.00002100
Iteration 73/1000 | Loss: 0.00002100
Iteration 74/1000 | Loss: 0.00002100
Iteration 75/1000 | Loss: 0.00002100
Iteration 76/1000 | Loss: 0.00002100
Iteration 77/1000 | Loss: 0.00002100
Iteration 78/1000 | Loss: 0.00002099
Iteration 79/1000 | Loss: 0.00002097
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002095
Iteration 83/1000 | Loss: 0.00002094
Iteration 84/1000 | Loss: 0.00002094
Iteration 85/1000 | Loss: 0.00002093
Iteration 86/1000 | Loss: 0.00002093
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002090
Iteration 90/1000 | Loss: 0.00002090
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002088
Iteration 94/1000 | Loss: 0.00002088
Iteration 95/1000 | Loss: 0.00002088
Iteration 96/1000 | Loss: 0.00002087
Iteration 97/1000 | Loss: 0.00002087
Iteration 98/1000 | Loss: 0.00002087
Iteration 99/1000 | Loss: 0.00002087
Iteration 100/1000 | Loss: 0.00002087
Iteration 101/1000 | Loss: 0.00002087
Iteration 102/1000 | Loss: 0.00002086
Iteration 103/1000 | Loss: 0.00002086
Iteration 104/1000 | Loss: 0.00002086
Iteration 105/1000 | Loss: 0.00002086
Iteration 106/1000 | Loss: 0.00002086
Iteration 107/1000 | Loss: 0.00002086
Iteration 108/1000 | Loss: 0.00002086
Iteration 109/1000 | Loss: 0.00002086
Iteration 110/1000 | Loss: 0.00002086
Iteration 111/1000 | Loss: 0.00002086
Iteration 112/1000 | Loss: 0.00002086
Iteration 113/1000 | Loss: 0.00002085
Iteration 114/1000 | Loss: 0.00002085
Iteration 115/1000 | Loss: 0.00002085
Iteration 116/1000 | Loss: 0.00002085
Iteration 117/1000 | Loss: 0.00002085
Iteration 118/1000 | Loss: 0.00002085
Iteration 119/1000 | Loss: 0.00002085
Iteration 120/1000 | Loss: 0.00002084
Iteration 121/1000 | Loss: 0.00002084
Iteration 122/1000 | Loss: 0.00002084
Iteration 123/1000 | Loss: 0.00002084
Iteration 124/1000 | Loss: 0.00002084
Iteration 125/1000 | Loss: 0.00002084
Iteration 126/1000 | Loss: 0.00002084
Iteration 127/1000 | Loss: 0.00002084
Iteration 128/1000 | Loss: 0.00002084
Iteration 129/1000 | Loss: 0.00002083
Iteration 130/1000 | Loss: 0.00002083
Iteration 131/1000 | Loss: 0.00002082
Iteration 132/1000 | Loss: 0.00002082
Iteration 133/1000 | Loss: 0.00002082
Iteration 134/1000 | Loss: 0.00002082
Iteration 135/1000 | Loss: 0.00002081
Iteration 136/1000 | Loss: 0.00002081
Iteration 137/1000 | Loss: 0.00002081
Iteration 138/1000 | Loss: 0.00002080
Iteration 139/1000 | Loss: 0.00002080
Iteration 140/1000 | Loss: 0.00002080
Iteration 141/1000 | Loss: 0.00002079
Iteration 142/1000 | Loss: 0.00002079
Iteration 143/1000 | Loss: 0.00002079
Iteration 144/1000 | Loss: 0.00002078
Iteration 145/1000 | Loss: 0.00002078
Iteration 146/1000 | Loss: 0.00002078
Iteration 147/1000 | Loss: 0.00002077
Iteration 148/1000 | Loss: 0.00002077
Iteration 149/1000 | Loss: 0.00002077
Iteration 150/1000 | Loss: 0.00002076
Iteration 151/1000 | Loss: 0.00002076
Iteration 152/1000 | Loss: 0.00002076
Iteration 153/1000 | Loss: 0.00002075
Iteration 154/1000 | Loss: 0.00002075
Iteration 155/1000 | Loss: 0.00002075
Iteration 156/1000 | Loss: 0.00002074
Iteration 157/1000 | Loss: 0.00002074
Iteration 158/1000 | Loss: 0.00002074
Iteration 159/1000 | Loss: 0.00002073
Iteration 160/1000 | Loss: 0.00002073
Iteration 161/1000 | Loss: 0.00002072
Iteration 162/1000 | Loss: 0.00002072
Iteration 163/1000 | Loss: 0.00002072
Iteration 164/1000 | Loss: 0.00002071
Iteration 165/1000 | Loss: 0.00002071
Iteration 166/1000 | Loss: 0.00002071
Iteration 167/1000 | Loss: 0.00002070
Iteration 168/1000 | Loss: 0.00002070
Iteration 169/1000 | Loss: 0.00002070
Iteration 170/1000 | Loss: 0.00002069
Iteration 171/1000 | Loss: 0.00002069
Iteration 172/1000 | Loss: 0.00002069
Iteration 173/1000 | Loss: 0.00002069
Iteration 174/1000 | Loss: 0.00002069
Iteration 175/1000 | Loss: 0.00002069
Iteration 176/1000 | Loss: 0.00002068
Iteration 177/1000 | Loss: 0.00002068
Iteration 178/1000 | Loss: 0.00002068
Iteration 179/1000 | Loss: 0.00002067
Iteration 180/1000 | Loss: 0.00002067
Iteration 181/1000 | Loss: 0.00002067
Iteration 182/1000 | Loss: 0.00002067
Iteration 183/1000 | Loss: 0.00002067
Iteration 184/1000 | Loss: 0.00002067
Iteration 185/1000 | Loss: 0.00002066
Iteration 186/1000 | Loss: 0.00002066
Iteration 187/1000 | Loss: 0.00002066
Iteration 188/1000 | Loss: 0.00002066
Iteration 189/1000 | Loss: 0.00002066
Iteration 190/1000 | Loss: 0.00002066
Iteration 191/1000 | Loss: 0.00002065
Iteration 192/1000 | Loss: 0.00002065
Iteration 193/1000 | Loss: 0.00002065
Iteration 194/1000 | Loss: 0.00002065
Iteration 195/1000 | Loss: 0.00002065
Iteration 196/1000 | Loss: 0.00002065
Iteration 197/1000 | Loss: 0.00002065
Iteration 198/1000 | Loss: 0.00002065
Iteration 199/1000 | Loss: 0.00002065
Iteration 200/1000 | Loss: 0.00002065
Iteration 201/1000 | Loss: 0.00002065
Iteration 202/1000 | Loss: 0.00002065
Iteration 203/1000 | Loss: 0.00002065
Iteration 204/1000 | Loss: 0.00002064
Iteration 205/1000 | Loss: 0.00002064
Iteration 206/1000 | Loss: 0.00002064
Iteration 207/1000 | Loss: 0.00002064
Iteration 208/1000 | Loss: 0.00002063
Iteration 209/1000 | Loss: 0.00002063
Iteration 210/1000 | Loss: 0.00002063
Iteration 211/1000 | Loss: 0.00002063
Iteration 212/1000 | Loss: 0.00002063
Iteration 213/1000 | Loss: 0.00002062
Iteration 214/1000 | Loss: 0.00002062
Iteration 215/1000 | Loss: 0.00002062
Iteration 216/1000 | Loss: 0.00002062
Iteration 217/1000 | Loss: 0.00002062
Iteration 218/1000 | Loss: 0.00002062
Iteration 219/1000 | Loss: 0.00002062
Iteration 220/1000 | Loss: 0.00002062
Iteration 221/1000 | Loss: 0.00002062
Iteration 222/1000 | Loss: 0.00002061
Iteration 223/1000 | Loss: 0.00002061
Iteration 224/1000 | Loss: 0.00002061
Iteration 225/1000 | Loss: 0.00002061
Iteration 226/1000 | Loss: 0.00002061
Iteration 227/1000 | Loss: 0.00002061
Iteration 228/1000 | Loss: 0.00002061
Iteration 229/1000 | Loss: 0.00002061
Iteration 230/1000 | Loss: 0.00002061
Iteration 231/1000 | Loss: 0.00002060
Iteration 232/1000 | Loss: 0.00002060
Iteration 233/1000 | Loss: 0.00002060
Iteration 234/1000 | Loss: 0.00002060
Iteration 235/1000 | Loss: 0.00002060
Iteration 236/1000 | Loss: 0.00002060
Iteration 237/1000 | Loss: 0.00002060
Iteration 238/1000 | Loss: 0.00002059
Iteration 239/1000 | Loss: 0.00002059
Iteration 240/1000 | Loss: 0.00002059
Iteration 241/1000 | Loss: 0.00002059
Iteration 242/1000 | Loss: 0.00002059
Iteration 243/1000 | Loss: 0.00002059
Iteration 244/1000 | Loss: 0.00002059
Iteration 245/1000 | Loss: 0.00002059
Iteration 246/1000 | Loss: 0.00002059
Iteration 247/1000 | Loss: 0.00002059
Iteration 248/1000 | Loss: 0.00002059
Iteration 249/1000 | Loss: 0.00002059
Iteration 250/1000 | Loss: 0.00002059
Iteration 251/1000 | Loss: 0.00002059
Iteration 252/1000 | Loss: 0.00002058
Iteration 253/1000 | Loss: 0.00002058
Iteration 254/1000 | Loss: 0.00002058
Iteration 255/1000 | Loss: 0.00002058
Iteration 256/1000 | Loss: 0.00002058
Iteration 257/1000 | Loss: 0.00002058
Iteration 258/1000 | Loss: 0.00002058
Iteration 259/1000 | Loss: 0.00002058
Iteration 260/1000 | Loss: 0.00002058
Iteration 261/1000 | Loss: 0.00002058
Iteration 262/1000 | Loss: 0.00002058
Iteration 263/1000 | Loss: 0.00002058
Iteration 264/1000 | Loss: 0.00002058
Iteration 265/1000 | Loss: 0.00002058
Iteration 266/1000 | Loss: 0.00002058
Iteration 267/1000 | Loss: 0.00002058
Iteration 268/1000 | Loss: 0.00002058
Iteration 269/1000 | Loss: 0.00002058
Iteration 270/1000 | Loss: 0.00002058
Iteration 271/1000 | Loss: 0.00002058
Iteration 272/1000 | Loss: 0.00002058
Iteration 273/1000 | Loss: 0.00002058
Iteration 274/1000 | Loss: 0.00002058
Iteration 275/1000 | Loss: 0.00002058
Iteration 276/1000 | Loss: 0.00002057
Iteration 277/1000 | Loss: 0.00002057
Iteration 278/1000 | Loss: 0.00002057
Iteration 279/1000 | Loss: 0.00002057
Iteration 280/1000 | Loss: 0.00002057
Iteration 281/1000 | Loss: 0.00002057
Iteration 282/1000 | Loss: 0.00002057
Iteration 283/1000 | Loss: 0.00002057
Iteration 284/1000 | Loss: 0.00002057
Iteration 285/1000 | Loss: 0.00002057
Iteration 286/1000 | Loss: 0.00002057
Iteration 287/1000 | Loss: 0.00002057
Iteration 288/1000 | Loss: 0.00002057
Iteration 289/1000 | Loss: 0.00002057
Iteration 290/1000 | Loss: 0.00002057
Iteration 291/1000 | Loss: 0.00002057
Iteration 292/1000 | Loss: 0.00002057
Iteration 293/1000 | Loss: 0.00002057
Iteration 294/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.0569361367961392e-05, 2.0569361367961392e-05, 2.0569361367961392e-05, 2.0569361367961392e-05, 2.0569361367961392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0569361367961392e-05

Optimization complete. Final v2v error: 3.6511871814727783 mm

Highest mean error: 11.489272117614746 mm for frame 16

Lowest mean error: 2.922715902328491 mm for frame 190

Saving results

Total time: 128.58042097091675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903250
Iteration 2/25 | Loss: 0.00169899
Iteration 3/25 | Loss: 0.00146058
Iteration 4/25 | Loss: 0.00143555
Iteration 5/25 | Loss: 0.00143094
Iteration 6/25 | Loss: 0.00142960
Iteration 7/25 | Loss: 0.00142940
Iteration 8/25 | Loss: 0.00142940
Iteration 9/25 | Loss: 0.00142940
Iteration 10/25 | Loss: 0.00142940
Iteration 11/25 | Loss: 0.00142940
Iteration 12/25 | Loss: 0.00142940
Iteration 13/25 | Loss: 0.00142940
Iteration 14/25 | Loss: 0.00142940
Iteration 15/25 | Loss: 0.00142940
Iteration 16/25 | Loss: 0.00142940
Iteration 17/25 | Loss: 0.00142940
Iteration 18/25 | Loss: 0.00142940
Iteration 19/25 | Loss: 0.00142940
Iteration 20/25 | Loss: 0.00142940
Iteration 21/25 | Loss: 0.00142940
Iteration 22/25 | Loss: 0.00142940
Iteration 23/25 | Loss: 0.00142940
Iteration 24/25 | Loss: 0.00142940
Iteration 25/25 | Loss: 0.00142940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54966289
Iteration 2/25 | Loss: 0.00121500
Iteration 3/25 | Loss: 0.00121500
Iteration 4/25 | Loss: 0.00121500
Iteration 5/25 | Loss: 0.00121500
Iteration 6/25 | Loss: 0.00121500
Iteration 7/25 | Loss: 0.00121500
Iteration 8/25 | Loss: 0.00121500
Iteration 9/25 | Loss: 0.00121500
Iteration 10/25 | Loss: 0.00121500
Iteration 11/25 | Loss: 0.00121500
Iteration 12/25 | Loss: 0.00121500
Iteration 13/25 | Loss: 0.00121500
Iteration 14/25 | Loss: 0.00121500
Iteration 15/25 | Loss: 0.00121500
Iteration 16/25 | Loss: 0.00121500
Iteration 17/25 | Loss: 0.00121500
Iteration 18/25 | Loss: 0.00121500
Iteration 19/25 | Loss: 0.00121500
Iteration 20/25 | Loss: 0.00121500
Iteration 21/25 | Loss: 0.00121500
Iteration 22/25 | Loss: 0.00121500
Iteration 23/25 | Loss: 0.00121500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012149963295087218, 0.0012149963295087218, 0.0012149963295087218, 0.0012149963295087218, 0.0012149963295087218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012149963295087218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121500
Iteration 2/1000 | Loss: 0.00007194
Iteration 3/1000 | Loss: 0.00003896
Iteration 4/1000 | Loss: 0.00003156
Iteration 5/1000 | Loss: 0.00002917
Iteration 6/1000 | Loss: 0.00002795
Iteration 7/1000 | Loss: 0.00002719
Iteration 8/1000 | Loss: 0.00002655
Iteration 9/1000 | Loss: 0.00002606
Iteration 10/1000 | Loss: 0.00002581
Iteration 11/1000 | Loss: 0.00002561
Iteration 12/1000 | Loss: 0.00002544
Iteration 13/1000 | Loss: 0.00002533
Iteration 14/1000 | Loss: 0.00002528
Iteration 15/1000 | Loss: 0.00002525
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002509
Iteration 18/1000 | Loss: 0.00002501
Iteration 19/1000 | Loss: 0.00002498
Iteration 20/1000 | Loss: 0.00002498
Iteration 21/1000 | Loss: 0.00002497
Iteration 22/1000 | Loss: 0.00002494
Iteration 23/1000 | Loss: 0.00002493
Iteration 24/1000 | Loss: 0.00002493
Iteration 25/1000 | Loss: 0.00002493
Iteration 26/1000 | Loss: 0.00002492
Iteration 27/1000 | Loss: 0.00002491
Iteration 28/1000 | Loss: 0.00002491
Iteration 29/1000 | Loss: 0.00002490
Iteration 30/1000 | Loss: 0.00002490
Iteration 31/1000 | Loss: 0.00002490
Iteration 32/1000 | Loss: 0.00002490
Iteration 33/1000 | Loss: 0.00002489
Iteration 34/1000 | Loss: 0.00002489
Iteration 35/1000 | Loss: 0.00002489
Iteration 36/1000 | Loss: 0.00002489
Iteration 37/1000 | Loss: 0.00002488
Iteration 38/1000 | Loss: 0.00002488
Iteration 39/1000 | Loss: 0.00002488
Iteration 40/1000 | Loss: 0.00002488
Iteration 41/1000 | Loss: 0.00002487
Iteration 42/1000 | Loss: 0.00002487
Iteration 43/1000 | Loss: 0.00002487
Iteration 44/1000 | Loss: 0.00002487
Iteration 45/1000 | Loss: 0.00002487
Iteration 46/1000 | Loss: 0.00002486
Iteration 47/1000 | Loss: 0.00002486
Iteration 48/1000 | Loss: 0.00002486
Iteration 49/1000 | Loss: 0.00002486
Iteration 50/1000 | Loss: 0.00002485
Iteration 51/1000 | Loss: 0.00002485
Iteration 52/1000 | Loss: 0.00002485
Iteration 53/1000 | Loss: 0.00002485
Iteration 54/1000 | Loss: 0.00002484
Iteration 55/1000 | Loss: 0.00002484
Iteration 56/1000 | Loss: 0.00002484
Iteration 57/1000 | Loss: 0.00002484
Iteration 58/1000 | Loss: 0.00002483
Iteration 59/1000 | Loss: 0.00002483
Iteration 60/1000 | Loss: 0.00002483
Iteration 61/1000 | Loss: 0.00002483
Iteration 62/1000 | Loss: 0.00002483
Iteration 63/1000 | Loss: 0.00002483
Iteration 64/1000 | Loss: 0.00002483
Iteration 65/1000 | Loss: 0.00002483
Iteration 66/1000 | Loss: 0.00002483
Iteration 67/1000 | Loss: 0.00002483
Iteration 68/1000 | Loss: 0.00002483
Iteration 69/1000 | Loss: 0.00002483
Iteration 70/1000 | Loss: 0.00002483
Iteration 71/1000 | Loss: 0.00002483
Iteration 72/1000 | Loss: 0.00002483
Iteration 73/1000 | Loss: 0.00002483
Iteration 74/1000 | Loss: 0.00002483
Iteration 75/1000 | Loss: 0.00002483
Iteration 76/1000 | Loss: 0.00002483
Iteration 77/1000 | Loss: 0.00002483
Iteration 78/1000 | Loss: 0.00002483
Iteration 79/1000 | Loss: 0.00002483
Iteration 80/1000 | Loss: 0.00002483
Iteration 81/1000 | Loss: 0.00002483
Iteration 82/1000 | Loss: 0.00002483
Iteration 83/1000 | Loss: 0.00002483
Iteration 84/1000 | Loss: 0.00002483
Iteration 85/1000 | Loss: 0.00002483
Iteration 86/1000 | Loss: 0.00002483
Iteration 87/1000 | Loss: 0.00002483
Iteration 88/1000 | Loss: 0.00002483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.483052412571851e-05, 2.483052412571851e-05, 2.483052412571851e-05, 2.483052412571851e-05, 2.483052412571851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.483052412571851e-05

Optimization complete. Final v2v error: 4.207546234130859 mm

Highest mean error: 4.660565376281738 mm for frame 20

Lowest mean error: 3.801492214202881 mm for frame 42

Saving results

Total time: 36.13205909729004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382076
Iteration 2/25 | Loss: 0.00125730
Iteration 3/25 | Loss: 0.00120510
Iteration 4/25 | Loss: 0.00119705
Iteration 5/25 | Loss: 0.00119439
Iteration 6/25 | Loss: 0.00119401
Iteration 7/25 | Loss: 0.00119401
Iteration 8/25 | Loss: 0.00119401
Iteration 9/25 | Loss: 0.00119401
Iteration 10/25 | Loss: 0.00119401
Iteration 11/25 | Loss: 0.00119401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011940058320760727, 0.0011940058320760727, 0.0011940058320760727, 0.0011940058320760727, 0.0011940058320760727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011940058320760727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.91516447
Iteration 2/25 | Loss: 0.00107526
Iteration 3/25 | Loss: 0.00107526
Iteration 4/25 | Loss: 0.00107526
Iteration 5/25 | Loss: 0.00107526
Iteration 6/25 | Loss: 0.00107526
Iteration 7/25 | Loss: 0.00107526
Iteration 8/25 | Loss: 0.00107526
Iteration 9/25 | Loss: 0.00107526
Iteration 10/25 | Loss: 0.00107526
Iteration 11/25 | Loss: 0.00107526
Iteration 12/25 | Loss: 0.00107526
Iteration 13/25 | Loss: 0.00107526
Iteration 14/25 | Loss: 0.00107526
Iteration 15/25 | Loss: 0.00107526
Iteration 16/25 | Loss: 0.00107526
Iteration 17/25 | Loss: 0.00107526
Iteration 18/25 | Loss: 0.00107526
Iteration 19/25 | Loss: 0.00107526
Iteration 20/25 | Loss: 0.00107526
Iteration 21/25 | Loss: 0.00107526
Iteration 22/25 | Loss: 0.00107526
Iteration 23/25 | Loss: 0.00107526
Iteration 24/25 | Loss: 0.00107526
Iteration 25/25 | Loss: 0.00107526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107526
Iteration 2/1000 | Loss: 0.00002235
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001243
Iteration 6/1000 | Loss: 0.00001183
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001116
Iteration 10/1000 | Loss: 0.00001096
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001057
Iteration 13/1000 | Loss: 0.00001053
Iteration 14/1000 | Loss: 0.00001052
Iteration 15/1000 | Loss: 0.00001052
Iteration 16/1000 | Loss: 0.00001045
Iteration 17/1000 | Loss: 0.00001042
Iteration 18/1000 | Loss: 0.00001035
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001024
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001019
Iteration 24/1000 | Loss: 0.00001019
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001014
Iteration 29/1000 | Loss: 0.00001013
Iteration 30/1000 | Loss: 0.00001013
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001013
Iteration 33/1000 | Loss: 0.00001010
Iteration 34/1000 | Loss: 0.00001009
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001002
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000998
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000998
Iteration 46/1000 | Loss: 0.00000998
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000997
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000996
Iteration 51/1000 | Loss: 0.00000995
Iteration 52/1000 | Loss: 0.00000995
Iteration 53/1000 | Loss: 0.00000995
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000994
Iteration 56/1000 | Loss: 0.00000994
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000993
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000993
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000992
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000991
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000990
Iteration 69/1000 | Loss: 0.00000990
Iteration 70/1000 | Loss: 0.00000989
Iteration 71/1000 | Loss: 0.00000989
Iteration 72/1000 | Loss: 0.00000989
Iteration 73/1000 | Loss: 0.00000989
Iteration 74/1000 | Loss: 0.00000989
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000987
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000986
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000985
Iteration 85/1000 | Loss: 0.00000985
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000982
Iteration 95/1000 | Loss: 0.00000982
Iteration 96/1000 | Loss: 0.00000982
Iteration 97/1000 | Loss: 0.00000982
Iteration 98/1000 | Loss: 0.00000982
Iteration 99/1000 | Loss: 0.00000982
Iteration 100/1000 | Loss: 0.00000981
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000981
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000980
Iteration 106/1000 | Loss: 0.00000980
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000979
Iteration 112/1000 | Loss: 0.00000979
Iteration 113/1000 | Loss: 0.00000979
Iteration 114/1000 | Loss: 0.00000979
Iteration 115/1000 | Loss: 0.00000979
Iteration 116/1000 | Loss: 0.00000978
Iteration 117/1000 | Loss: 0.00000978
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000978
Iteration 120/1000 | Loss: 0.00000978
Iteration 121/1000 | Loss: 0.00000978
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000978
Iteration 124/1000 | Loss: 0.00000978
Iteration 125/1000 | Loss: 0.00000978
Iteration 126/1000 | Loss: 0.00000978
Iteration 127/1000 | Loss: 0.00000978
Iteration 128/1000 | Loss: 0.00000977
Iteration 129/1000 | Loss: 0.00000977
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000977
Iteration 134/1000 | Loss: 0.00000977
Iteration 135/1000 | Loss: 0.00000977
Iteration 136/1000 | Loss: 0.00000977
Iteration 137/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [9.77124091150472e-06, 9.77124091150472e-06, 9.77124091150472e-06, 9.77124091150472e-06, 9.77124091150472e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.77124091150472e-06

Optimization complete. Final v2v error: 2.7083210945129395 mm

Highest mean error: 3.038876533508301 mm for frame 94

Lowest mean error: 2.561755657196045 mm for frame 29

Saving results

Total time: 39.86559057235718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428979
Iteration 2/25 | Loss: 0.00134773
Iteration 3/25 | Loss: 0.00126469
Iteration 4/25 | Loss: 0.00125156
Iteration 5/25 | Loss: 0.00124855
Iteration 6/25 | Loss: 0.00124793
Iteration 7/25 | Loss: 0.00124793
Iteration 8/25 | Loss: 0.00124793
Iteration 9/25 | Loss: 0.00124793
Iteration 10/25 | Loss: 0.00124793
Iteration 11/25 | Loss: 0.00124793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012479289434850216, 0.0012479289434850216, 0.0012479289434850216, 0.0012479289434850216, 0.0012479289434850216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012479289434850216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36677372
Iteration 2/25 | Loss: 0.00116611
Iteration 3/25 | Loss: 0.00116611
Iteration 4/25 | Loss: 0.00116611
Iteration 5/25 | Loss: 0.00116611
Iteration 6/25 | Loss: 0.00116611
Iteration 7/25 | Loss: 0.00116611
Iteration 8/25 | Loss: 0.00116611
Iteration 9/25 | Loss: 0.00116611
Iteration 10/25 | Loss: 0.00116610
Iteration 11/25 | Loss: 0.00116610
Iteration 12/25 | Loss: 0.00116610
Iteration 13/25 | Loss: 0.00116610
Iteration 14/25 | Loss: 0.00116610
Iteration 15/25 | Loss: 0.00116610
Iteration 16/25 | Loss: 0.00116610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011661045718938112, 0.0011661045718938112, 0.0011661045718938112, 0.0011661045718938112, 0.0011661045718938112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011661045718938112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116610
Iteration 2/1000 | Loss: 0.00002969
Iteration 3/1000 | Loss: 0.00001940
Iteration 4/1000 | Loss: 0.00001748
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001592
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001389
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001388
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001387
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001386
Iteration 52/1000 | Loss: 0.00001386
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001383
Iteration 62/1000 | Loss: 0.00001383
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001380
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001380
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001379
Iteration 81/1000 | Loss: 0.00001379
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001378
Iteration 87/1000 | Loss: 0.00001378
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001374
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001372
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001369
Iteration 134/1000 | Loss: 0.00001369
Iteration 135/1000 | Loss: 0.00001369
Iteration 136/1000 | Loss: 0.00001369
Iteration 137/1000 | Loss: 0.00001369
Iteration 138/1000 | Loss: 0.00001369
Iteration 139/1000 | Loss: 0.00001369
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001369
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001369
Iteration 151/1000 | Loss: 0.00001369
Iteration 152/1000 | Loss: 0.00001369
Iteration 153/1000 | Loss: 0.00001369
Iteration 154/1000 | Loss: 0.00001369
Iteration 155/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3690893865714315e-05, 1.3690893865714315e-05, 1.3690893865714315e-05, 1.3690893865714315e-05, 1.3690893865714315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3690893865714315e-05

Optimization complete. Final v2v error: 3.1135451793670654 mm

Highest mean error: 3.4571845531463623 mm for frame 19

Lowest mean error: 2.7254345417022705 mm for frame 79

Saving results

Total time: 39.04262089729309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795141
Iteration 2/25 | Loss: 0.00135183
Iteration 3/25 | Loss: 0.00126585
Iteration 4/25 | Loss: 0.00125751
Iteration 5/25 | Loss: 0.00125534
Iteration 6/25 | Loss: 0.00125534
Iteration 7/25 | Loss: 0.00125534
Iteration 8/25 | Loss: 0.00125534
Iteration 9/25 | Loss: 0.00125534
Iteration 10/25 | Loss: 0.00125534
Iteration 11/25 | Loss: 0.00125534
Iteration 12/25 | Loss: 0.00125534
Iteration 13/25 | Loss: 0.00125534
Iteration 14/25 | Loss: 0.00125534
Iteration 15/25 | Loss: 0.00125534
Iteration 16/25 | Loss: 0.00125534
Iteration 17/25 | Loss: 0.00125534
Iteration 18/25 | Loss: 0.00125534
Iteration 19/25 | Loss: 0.00125534
Iteration 20/25 | Loss: 0.00125534
Iteration 21/25 | Loss: 0.00125534
Iteration 22/25 | Loss: 0.00125534
Iteration 23/25 | Loss: 0.00125534
Iteration 24/25 | Loss: 0.00125534
Iteration 25/25 | Loss: 0.00125534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32194996
Iteration 2/25 | Loss: 0.00088498
Iteration 3/25 | Loss: 0.00088498
Iteration 4/25 | Loss: 0.00088498
Iteration 5/25 | Loss: 0.00088497
Iteration 6/25 | Loss: 0.00088497
Iteration 7/25 | Loss: 0.00088497
Iteration 8/25 | Loss: 0.00088497
Iteration 9/25 | Loss: 0.00088497
Iteration 10/25 | Loss: 0.00088497
Iteration 11/25 | Loss: 0.00088497
Iteration 12/25 | Loss: 0.00088497
Iteration 13/25 | Loss: 0.00088497
Iteration 14/25 | Loss: 0.00088497
Iteration 15/25 | Loss: 0.00088497
Iteration 16/25 | Loss: 0.00088497
Iteration 17/25 | Loss: 0.00088497
Iteration 18/25 | Loss: 0.00088497
Iteration 19/25 | Loss: 0.00088497
Iteration 20/25 | Loss: 0.00088497
Iteration 21/25 | Loss: 0.00088497
Iteration 22/25 | Loss: 0.00088497
Iteration 23/25 | Loss: 0.00088497
Iteration 24/25 | Loss: 0.00088497
Iteration 25/25 | Loss: 0.00088497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088497
Iteration 2/1000 | Loss: 0.00002666
Iteration 3/1000 | Loss: 0.00002029
Iteration 4/1000 | Loss: 0.00001866
Iteration 5/1000 | Loss: 0.00001777
Iteration 6/1000 | Loss: 0.00001731
Iteration 7/1000 | Loss: 0.00001694
Iteration 8/1000 | Loss: 0.00001659
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001585
Iteration 13/1000 | Loss: 0.00001582
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001562
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001558
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001550
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001542
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001531
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001528
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001527
Iteration 36/1000 | Loss: 0.00001527
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001527
Iteration 41/1000 | Loss: 0.00001527
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001520
Iteration 52/1000 | Loss: 0.00001520
Iteration 53/1000 | Loss: 0.00001520
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001519
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001517
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001516
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001505
Iteration 68/1000 | Loss: 0.00001505
Iteration 69/1000 | Loss: 0.00001505
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001504
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001501
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001500
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001497
Iteration 115/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.4974209079809953e-05, 1.4974209079809953e-05, 1.4974209079809953e-05, 1.4974209079809953e-05, 1.4974209079809953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4974209079809953e-05

Optimization complete. Final v2v error: 3.2380893230438232 mm

Highest mean error: 3.39748215675354 mm for frame 141

Lowest mean error: 3.1433491706848145 mm for frame 42

Saving results

Total time: 42.39248752593994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00294354
Iteration 2/25 | Loss: 0.00128255
Iteration 3/25 | Loss: 0.00122175
Iteration 4/25 | Loss: 0.00120291
Iteration 5/25 | Loss: 0.00119491
Iteration 6/25 | Loss: 0.00119308
Iteration 7/25 | Loss: 0.00119243
Iteration 8/25 | Loss: 0.00119243
Iteration 9/25 | Loss: 0.00119243
Iteration 10/25 | Loss: 0.00119243
Iteration 11/25 | Loss: 0.00119243
Iteration 12/25 | Loss: 0.00119243
Iteration 13/25 | Loss: 0.00119243
Iteration 14/25 | Loss: 0.00119243
Iteration 15/25 | Loss: 0.00119243
Iteration 16/25 | Loss: 0.00119243
Iteration 17/25 | Loss: 0.00119243
Iteration 18/25 | Loss: 0.00119243
Iteration 19/25 | Loss: 0.00119243
Iteration 20/25 | Loss: 0.00119243
Iteration 21/25 | Loss: 0.00119243
Iteration 22/25 | Loss: 0.00119243
Iteration 23/25 | Loss: 0.00119243
Iteration 24/25 | Loss: 0.00119243
Iteration 25/25 | Loss: 0.00119243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21284676
Iteration 2/25 | Loss: 0.00139858
Iteration 3/25 | Loss: 0.00139858
Iteration 4/25 | Loss: 0.00139858
Iteration 5/25 | Loss: 0.00139858
Iteration 6/25 | Loss: 0.00139857
Iteration 7/25 | Loss: 0.00139857
Iteration 8/25 | Loss: 0.00139857
Iteration 9/25 | Loss: 0.00139857
Iteration 10/25 | Loss: 0.00139857
Iteration 11/25 | Loss: 0.00139857
Iteration 12/25 | Loss: 0.00139857
Iteration 13/25 | Loss: 0.00139857
Iteration 14/25 | Loss: 0.00139857
Iteration 15/25 | Loss: 0.00139857
Iteration 16/25 | Loss: 0.00139857
Iteration 17/25 | Loss: 0.00139857
Iteration 18/25 | Loss: 0.00139857
Iteration 19/25 | Loss: 0.00139857
Iteration 20/25 | Loss: 0.00139857
Iteration 21/25 | Loss: 0.00139857
Iteration 22/25 | Loss: 0.00139857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013985737459734082, 0.0013985737459734082, 0.0013985737459734082, 0.0013985737459734082, 0.0013985737459734082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013985737459734082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139857
Iteration 2/1000 | Loss: 0.00004402
Iteration 3/1000 | Loss: 0.00002876
Iteration 4/1000 | Loss: 0.00002179
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001916
Iteration 7/1000 | Loss: 0.00001852
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001760
Iteration 11/1000 | Loss: 0.00001733
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001709
Iteration 14/1000 | Loss: 0.00001693
Iteration 15/1000 | Loss: 0.00001688
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001679
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001659
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001656
Iteration 28/1000 | Loss: 0.00001656
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001655
Iteration 31/1000 | Loss: 0.00001654
Iteration 32/1000 | Loss: 0.00001654
Iteration 33/1000 | Loss: 0.00001654
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001649
Iteration 46/1000 | Loss: 0.00001649
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001648
Iteration 49/1000 | Loss: 0.00001648
Iteration 50/1000 | Loss: 0.00001648
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001647
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001645
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001644
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00001642
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001641
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001638
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001637
Iteration 92/1000 | Loss: 0.00001637
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001636
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001635
Iteration 105/1000 | Loss: 0.00001635
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001633
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001633
Iteration 111/1000 | Loss: 0.00001633
Iteration 112/1000 | Loss: 0.00001633
Iteration 113/1000 | Loss: 0.00001633
Iteration 114/1000 | Loss: 0.00001632
Iteration 115/1000 | Loss: 0.00001632
Iteration 116/1000 | Loss: 0.00001632
Iteration 117/1000 | Loss: 0.00001632
Iteration 118/1000 | Loss: 0.00001632
Iteration 119/1000 | Loss: 0.00001632
Iteration 120/1000 | Loss: 0.00001632
Iteration 121/1000 | Loss: 0.00001632
Iteration 122/1000 | Loss: 0.00001632
Iteration 123/1000 | Loss: 0.00001631
Iteration 124/1000 | Loss: 0.00001631
Iteration 125/1000 | Loss: 0.00001631
Iteration 126/1000 | Loss: 0.00001631
Iteration 127/1000 | Loss: 0.00001631
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001631
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001631
Iteration 132/1000 | Loss: 0.00001631
Iteration 133/1000 | Loss: 0.00001631
Iteration 134/1000 | Loss: 0.00001631
Iteration 135/1000 | Loss: 0.00001631
Iteration 136/1000 | Loss: 0.00001631
Iteration 137/1000 | Loss: 0.00001631
Iteration 138/1000 | Loss: 0.00001631
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001630
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001630
Iteration 144/1000 | Loss: 0.00001630
Iteration 145/1000 | Loss: 0.00001630
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001629
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001629
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001629
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001628
Iteration 159/1000 | Loss: 0.00001628
Iteration 160/1000 | Loss: 0.00001628
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001628
Iteration 163/1000 | Loss: 0.00001628
Iteration 164/1000 | Loss: 0.00001628
Iteration 165/1000 | Loss: 0.00001628
Iteration 166/1000 | Loss: 0.00001628
Iteration 167/1000 | Loss: 0.00001628
Iteration 168/1000 | Loss: 0.00001627
Iteration 169/1000 | Loss: 0.00001627
Iteration 170/1000 | Loss: 0.00001627
Iteration 171/1000 | Loss: 0.00001627
Iteration 172/1000 | Loss: 0.00001627
Iteration 173/1000 | Loss: 0.00001627
Iteration 174/1000 | Loss: 0.00001627
Iteration 175/1000 | Loss: 0.00001627
Iteration 176/1000 | Loss: 0.00001627
Iteration 177/1000 | Loss: 0.00001627
Iteration 178/1000 | Loss: 0.00001627
Iteration 179/1000 | Loss: 0.00001627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.6268864783341996e-05, 1.6268864783341996e-05, 1.6268864783341996e-05, 1.6268864783341996e-05, 1.6268864783341996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6268864783341996e-05

Optimization complete. Final v2v error: 3.427933931350708 mm

Highest mean error: 3.8865153789520264 mm for frame 136

Lowest mean error: 3.134035110473633 mm for frame 27

Saving results

Total time: 44.34042263031006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419040
Iteration 2/25 | Loss: 0.00131113
Iteration 3/25 | Loss: 0.00124148
Iteration 4/25 | Loss: 0.00122581
Iteration 5/25 | Loss: 0.00122144
Iteration 6/25 | Loss: 0.00122078
Iteration 7/25 | Loss: 0.00122078
Iteration 8/25 | Loss: 0.00122078
Iteration 9/25 | Loss: 0.00122078
Iteration 10/25 | Loss: 0.00122078
Iteration 11/25 | Loss: 0.00122078
Iteration 12/25 | Loss: 0.00122078
Iteration 13/25 | Loss: 0.00122078
Iteration 14/25 | Loss: 0.00122078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001220779144205153, 0.001220779144205153, 0.001220779144205153, 0.001220779144205153, 0.001220779144205153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001220779144205153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41029584
Iteration 2/25 | Loss: 0.00101366
Iteration 3/25 | Loss: 0.00101366
Iteration 4/25 | Loss: 0.00101366
Iteration 5/25 | Loss: 0.00101366
Iteration 6/25 | Loss: 0.00101366
Iteration 7/25 | Loss: 0.00101366
Iteration 8/25 | Loss: 0.00101366
Iteration 9/25 | Loss: 0.00101366
Iteration 10/25 | Loss: 0.00101366
Iteration 11/25 | Loss: 0.00101366
Iteration 12/25 | Loss: 0.00101366
Iteration 13/25 | Loss: 0.00101366
Iteration 14/25 | Loss: 0.00101366
Iteration 15/25 | Loss: 0.00101366
Iteration 16/25 | Loss: 0.00101366
Iteration 17/25 | Loss: 0.00101366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010136611526831985, 0.0010136611526831985, 0.0010136611526831985, 0.0010136611526831985, 0.0010136611526831985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010136611526831985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101366
Iteration 2/1000 | Loss: 0.00002684
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001655
Iteration 5/1000 | Loss: 0.00001579
Iteration 6/1000 | Loss: 0.00001522
Iteration 7/1000 | Loss: 0.00001478
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001417
Iteration 10/1000 | Loss: 0.00001390
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001372
Iteration 13/1000 | Loss: 0.00001358
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001351
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001339
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001329
Iteration 27/1000 | Loss: 0.00001328
Iteration 28/1000 | Loss: 0.00001328
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001322
Iteration 35/1000 | Loss: 0.00001322
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001314
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001313
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001309
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001309
Iteration 54/1000 | Loss: 0.00001309
Iteration 55/1000 | Loss: 0.00001308
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001308
Iteration 58/1000 | Loss: 0.00001308
Iteration 59/1000 | Loss: 0.00001307
Iteration 60/1000 | Loss: 0.00001307
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001305
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001305
Iteration 69/1000 | Loss: 0.00001305
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001302
Iteration 79/1000 | Loss: 0.00001302
Iteration 80/1000 | Loss: 0.00001302
Iteration 81/1000 | Loss: 0.00001302
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001300
Iteration 86/1000 | Loss: 0.00001300
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001295
Iteration 109/1000 | Loss: 0.00001295
Iteration 110/1000 | Loss: 0.00001295
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001294
Iteration 114/1000 | Loss: 0.00001294
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001293
Iteration 131/1000 | Loss: 0.00001293
Iteration 132/1000 | Loss: 0.00001293
Iteration 133/1000 | Loss: 0.00001293
Iteration 134/1000 | Loss: 0.00001293
Iteration 135/1000 | Loss: 0.00001293
Iteration 136/1000 | Loss: 0.00001293
Iteration 137/1000 | Loss: 0.00001293
Iteration 138/1000 | Loss: 0.00001293
Iteration 139/1000 | Loss: 0.00001293
Iteration 140/1000 | Loss: 0.00001293
Iteration 141/1000 | Loss: 0.00001293
Iteration 142/1000 | Loss: 0.00001293
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.2934577171108685e-05, 1.2934577171108685e-05, 1.2934577171108685e-05, 1.2934577171108685e-05, 1.2934577171108685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2934577171108685e-05

Optimization complete. Final v2v error: 3.100181818008423 mm

Highest mean error: 3.385460615158081 mm for frame 114

Lowest mean error: 3.001275062561035 mm for frame 134

Saving results

Total time: 38.81266760826111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993657
Iteration 2/25 | Loss: 0.00993657
Iteration 3/25 | Loss: 0.00993657
Iteration 4/25 | Loss: 0.00993657
Iteration 5/25 | Loss: 0.00993657
Iteration 6/25 | Loss: 0.00993657
Iteration 7/25 | Loss: 0.00993656
Iteration 8/25 | Loss: 0.00993656
Iteration 9/25 | Loss: 0.00993656
Iteration 10/25 | Loss: 0.00993656
Iteration 11/25 | Loss: 0.00993656
Iteration 12/25 | Loss: 0.00993655
Iteration 13/25 | Loss: 0.00993655
Iteration 14/25 | Loss: 0.00993655
Iteration 15/25 | Loss: 0.00993655
Iteration 16/25 | Loss: 0.00993654
Iteration 17/25 | Loss: 0.00993654
Iteration 18/25 | Loss: 0.00993654
Iteration 19/25 | Loss: 0.00993654
Iteration 20/25 | Loss: 0.00993654
Iteration 21/25 | Loss: 0.00993654
Iteration 22/25 | Loss: 0.00993653
Iteration 23/25 | Loss: 0.00993653
Iteration 24/25 | Loss: 0.00993653
Iteration 25/25 | Loss: 0.00993653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46954930
Iteration 2/25 | Loss: 0.13582399
Iteration 3/25 | Loss: 0.13577068
Iteration 4/25 | Loss: 0.13580664
Iteration 5/25 | Loss: 0.13577068
Iteration 6/25 | Loss: 0.13577068
Iteration 7/25 | Loss: 0.13577066
Iteration 8/25 | Loss: 0.13577066
Iteration 9/25 | Loss: 0.13577066
Iteration 10/25 | Loss: 0.13577065
Iteration 11/25 | Loss: 0.13577065
Iteration 12/25 | Loss: 0.13577065
Iteration 13/25 | Loss: 0.13577065
Iteration 14/25 | Loss: 0.13577065
Iteration 15/25 | Loss: 0.13577065
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.13577064871788025, 0.13577064871788025, 0.13577064871788025, 0.13577064871788025, 0.13577064871788025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.13577064871788025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13577065
Iteration 2/1000 | Loss: 0.00269414
Iteration 3/1000 | Loss: 0.00077326
Iteration 4/1000 | Loss: 0.00154202
Iteration 5/1000 | Loss: 0.00018687
Iteration 6/1000 | Loss: 0.00014715
Iteration 7/1000 | Loss: 0.00009195
Iteration 8/1000 | Loss: 0.00007495
Iteration 9/1000 | Loss: 0.00009173
Iteration 10/1000 | Loss: 0.00004720
Iteration 11/1000 | Loss: 0.00003864
Iteration 12/1000 | Loss: 0.00005357
Iteration 13/1000 | Loss: 0.00011368
Iteration 14/1000 | Loss: 0.00003676
Iteration 15/1000 | Loss: 0.00005066
Iteration 16/1000 | Loss: 0.00002631
Iteration 17/1000 | Loss: 0.00009509
Iteration 18/1000 | Loss: 0.00003306
Iteration 19/1000 | Loss: 0.00004600
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00003568
Iteration 22/1000 | Loss: 0.00002764
Iteration 23/1000 | Loss: 0.00001726
Iteration 24/1000 | Loss: 0.00005049
Iteration 25/1000 | Loss: 0.00001651
Iteration 26/1000 | Loss: 0.00003882
Iteration 27/1000 | Loss: 0.00002533
Iteration 28/1000 | Loss: 0.00001960
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001553
Iteration 31/1000 | Loss: 0.00002409
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00004227
Iteration 35/1000 | Loss: 0.00006788
Iteration 36/1000 | Loss: 0.00002526
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001440
Iteration 39/1000 | Loss: 0.00001439
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001438
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001438
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00001435
Iteration 50/1000 | Loss: 0.00001434
Iteration 51/1000 | Loss: 0.00001433
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001424
Iteration 57/1000 | Loss: 0.00001424
Iteration 58/1000 | Loss: 0.00001423
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001422
Iteration 61/1000 | Loss: 0.00001653
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001414
Iteration 75/1000 | Loss: 0.00001414
Iteration 76/1000 | Loss: 0.00001414
Iteration 77/1000 | Loss: 0.00001414
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001414
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001413
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001413
Iteration 85/1000 | Loss: 0.00001413
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001407
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001406
Iteration 110/1000 | Loss: 0.00001405
Iteration 111/1000 | Loss: 0.00001405
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001405
Iteration 114/1000 | Loss: 0.00001405
Iteration 115/1000 | Loss: 0.00001405
Iteration 116/1000 | Loss: 0.00001405
Iteration 117/1000 | Loss: 0.00001405
Iteration 118/1000 | Loss: 0.00001405
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001405
Iteration 122/1000 | Loss: 0.00001405
Iteration 123/1000 | Loss: 0.00001404
Iteration 124/1000 | Loss: 0.00001404
Iteration 125/1000 | Loss: 0.00001404
Iteration 126/1000 | Loss: 0.00001403
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001401
Iteration 134/1000 | Loss: 0.00001401
Iteration 135/1000 | Loss: 0.00001401
Iteration 136/1000 | Loss: 0.00001401
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001888
Iteration 139/1000 | Loss: 0.00001400
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001398
Iteration 147/1000 | Loss: 0.00001398
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Iteration 156/1000 | Loss: 0.00001398
Iteration 157/1000 | Loss: 0.00001398
Iteration 158/1000 | Loss: 0.00001398
Iteration 159/1000 | Loss: 0.00001397
Iteration 160/1000 | Loss: 0.00001397
Iteration 161/1000 | Loss: 0.00001397
Iteration 162/1000 | Loss: 0.00001397
Iteration 163/1000 | Loss: 0.00001397
Iteration 164/1000 | Loss: 0.00001397
Iteration 165/1000 | Loss: 0.00001397
Iteration 166/1000 | Loss: 0.00001397
Iteration 167/1000 | Loss: 0.00001397
Iteration 168/1000 | Loss: 0.00001397
Iteration 169/1000 | Loss: 0.00001397
Iteration 170/1000 | Loss: 0.00001397
Iteration 171/1000 | Loss: 0.00001397
Iteration 172/1000 | Loss: 0.00001397
Iteration 173/1000 | Loss: 0.00001397
Iteration 174/1000 | Loss: 0.00001397
Iteration 175/1000 | Loss: 0.00001397
Iteration 176/1000 | Loss: 0.00001397
Iteration 177/1000 | Loss: 0.00001397
Iteration 178/1000 | Loss: 0.00001397
Iteration 179/1000 | Loss: 0.00001397
Iteration 180/1000 | Loss: 0.00001397
Iteration 181/1000 | Loss: 0.00001397
Iteration 182/1000 | Loss: 0.00001397
Iteration 183/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.3969206520414446e-05, 1.3969206520414446e-05, 1.3969206520414446e-05, 1.3969206520414446e-05, 1.3969206520414446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3969206520414446e-05

Optimization complete. Final v2v error: 3.148010015487671 mm

Highest mean error: 3.3510091304779053 mm for frame 79

Lowest mean error: 2.941420555114746 mm for frame 8

Saving results

Total time: 83.73447227478027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867876
Iteration 2/25 | Loss: 0.00145024
Iteration 3/25 | Loss: 0.00129028
Iteration 4/25 | Loss: 0.00124466
Iteration 5/25 | Loss: 0.00123325
Iteration 6/25 | Loss: 0.00123147
Iteration 7/25 | Loss: 0.00123013
Iteration 8/25 | Loss: 0.00122937
Iteration 9/25 | Loss: 0.00122921
Iteration 10/25 | Loss: 0.00122918
Iteration 11/25 | Loss: 0.00122918
Iteration 12/25 | Loss: 0.00122918
Iteration 13/25 | Loss: 0.00122918
Iteration 14/25 | Loss: 0.00122918
Iteration 15/25 | Loss: 0.00122918
Iteration 16/25 | Loss: 0.00122918
Iteration 17/25 | Loss: 0.00122918
Iteration 18/25 | Loss: 0.00122917
Iteration 19/25 | Loss: 0.00122917
Iteration 20/25 | Loss: 0.00122917
Iteration 21/25 | Loss: 0.00122916
Iteration 22/25 | Loss: 0.00122916
Iteration 23/25 | Loss: 0.00122916
Iteration 24/25 | Loss: 0.00122916
Iteration 25/25 | Loss: 0.00122916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.65924168
Iteration 2/25 | Loss: 0.00118445
Iteration 3/25 | Loss: 0.00118441
Iteration 4/25 | Loss: 0.00118441
Iteration 5/25 | Loss: 0.00118441
Iteration 6/25 | Loss: 0.00118441
Iteration 7/25 | Loss: 0.00118441
Iteration 8/25 | Loss: 0.00118441
Iteration 9/25 | Loss: 0.00118441
Iteration 10/25 | Loss: 0.00118441
Iteration 11/25 | Loss: 0.00118441
Iteration 12/25 | Loss: 0.00118441
Iteration 13/25 | Loss: 0.00118441
Iteration 14/25 | Loss: 0.00118441
Iteration 15/25 | Loss: 0.00118441
Iteration 16/25 | Loss: 0.00118441
Iteration 17/25 | Loss: 0.00118441
Iteration 18/25 | Loss: 0.00118441
Iteration 19/25 | Loss: 0.00118441
Iteration 20/25 | Loss: 0.00118441
Iteration 21/25 | Loss: 0.00118441
Iteration 22/25 | Loss: 0.00118441
Iteration 23/25 | Loss: 0.00118441
Iteration 24/25 | Loss: 0.00118441
Iteration 25/25 | Loss: 0.00118441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118441
Iteration 2/1000 | Loss: 0.00002384
Iteration 3/1000 | Loss: 0.00001710
Iteration 4/1000 | Loss: 0.00001519
Iteration 5/1000 | Loss: 0.00001423
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00003785
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001205
Iteration 20/1000 | Loss: 0.00001204
Iteration 21/1000 | Loss: 0.00004577
Iteration 22/1000 | Loss: 0.00001561
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001760
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001186
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001183
Iteration 48/1000 | Loss: 0.00001183
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001174
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001171
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001165
Iteration 91/1000 | Loss: 0.00001165
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001164
Iteration 95/1000 | Loss: 0.00001164
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001162
Iteration 104/1000 | Loss: 0.00001162
Iteration 105/1000 | Loss: 0.00001162
Iteration 106/1000 | Loss: 0.00001162
Iteration 107/1000 | Loss: 0.00001162
Iteration 108/1000 | Loss: 0.00001162
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001161
Iteration 111/1000 | Loss: 0.00001161
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001161
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001158
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001157
Iteration 143/1000 | Loss: 0.00001157
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001155
Iteration 151/1000 | Loss: 0.00001155
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001155
Iteration 156/1000 | Loss: 0.00001155
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001155
Iteration 162/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.1549923328857403e-05, 1.1549923328857403e-05, 1.1549923328857403e-05, 1.1549923328857403e-05, 1.1549923328857403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1549923328857403e-05

Optimization complete. Final v2v error: 2.8608827590942383 mm

Highest mean error: 3.5050485134124756 mm for frame 84

Lowest mean error: 2.5670065879821777 mm for frame 176

Saving results

Total time: 60.63372182846069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516442
Iteration 2/25 | Loss: 0.00136464
Iteration 3/25 | Loss: 0.00127456
Iteration 4/25 | Loss: 0.00125410
Iteration 5/25 | Loss: 0.00125064
Iteration 6/25 | Loss: 0.00125064
Iteration 7/25 | Loss: 0.00125064
Iteration 8/25 | Loss: 0.00125064
Iteration 9/25 | Loss: 0.00125064
Iteration 10/25 | Loss: 0.00125064
Iteration 11/25 | Loss: 0.00125064
Iteration 12/25 | Loss: 0.00125064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012506439816206694, 0.0012506439816206694, 0.0012506439816206694, 0.0012506439816206694, 0.0012506439816206694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012506439816206694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93005180
Iteration 2/25 | Loss: 0.00106793
Iteration 3/25 | Loss: 0.00106792
Iteration 4/25 | Loss: 0.00106792
Iteration 5/25 | Loss: 0.00106792
Iteration 6/25 | Loss: 0.00106792
Iteration 7/25 | Loss: 0.00106792
Iteration 8/25 | Loss: 0.00106792
Iteration 9/25 | Loss: 0.00106792
Iteration 10/25 | Loss: 0.00106792
Iteration 11/25 | Loss: 0.00106792
Iteration 12/25 | Loss: 0.00106792
Iteration 13/25 | Loss: 0.00106792
Iteration 14/25 | Loss: 0.00106792
Iteration 15/25 | Loss: 0.00106792
Iteration 16/25 | Loss: 0.00106792
Iteration 17/25 | Loss: 0.00106792
Iteration 18/25 | Loss: 0.00106792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010679209372028708, 0.0010679209372028708, 0.0010679209372028708, 0.0010679209372028708, 0.0010679209372028708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010679209372028708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106792
Iteration 2/1000 | Loss: 0.00002856
Iteration 3/1000 | Loss: 0.00002105
Iteration 4/1000 | Loss: 0.00001976
Iteration 5/1000 | Loss: 0.00001825
Iteration 6/1000 | Loss: 0.00001768
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001597
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001566
Iteration 14/1000 | Loss: 0.00001557
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001534
Iteration 20/1000 | Loss: 0.00001532
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001532
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001529
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001528
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001518
Iteration 65/1000 | Loss: 0.00001518
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001517
Iteration 68/1000 | Loss: 0.00001517
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001516
Iteration 71/1000 | Loss: 0.00001516
Iteration 72/1000 | Loss: 0.00001515
Iteration 73/1000 | Loss: 0.00001515
Iteration 74/1000 | Loss: 0.00001515
Iteration 75/1000 | Loss: 0.00001515
Iteration 76/1000 | Loss: 0.00001515
Iteration 77/1000 | Loss: 0.00001515
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001514
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001513
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001509
Iteration 99/1000 | Loss: 0.00001509
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001508
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001506
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001504
Iteration 125/1000 | Loss: 0.00001504
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001504
Iteration 132/1000 | Loss: 0.00001504
Iteration 133/1000 | Loss: 0.00001503
Iteration 134/1000 | Loss: 0.00001503
Iteration 135/1000 | Loss: 0.00001503
Iteration 136/1000 | Loss: 0.00001503
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001501
Iteration 152/1000 | Loss: 0.00001501
Iteration 153/1000 | Loss: 0.00001501
Iteration 154/1000 | Loss: 0.00001501
Iteration 155/1000 | Loss: 0.00001501
Iteration 156/1000 | Loss: 0.00001501
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.5011646610219032e-05, 1.5011646610219032e-05, 1.5011646610219032e-05, 1.5011646610219032e-05, 1.5011646610219032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5011646610219032e-05

Optimization complete. Final v2v error: 3.2922821044921875 mm

Highest mean error: 3.8063430786132812 mm for frame 74

Lowest mean error: 2.9806604385375977 mm for frame 217

Saving results

Total time: 44.2087025642395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796146
Iteration 2/25 | Loss: 0.00128939
Iteration 3/25 | Loss: 0.00123123
Iteration 4/25 | Loss: 0.00122067
Iteration 5/25 | Loss: 0.00121787
Iteration 6/25 | Loss: 0.00121775
Iteration 7/25 | Loss: 0.00121775
Iteration 8/25 | Loss: 0.00121775
Iteration 9/25 | Loss: 0.00121775
Iteration 10/25 | Loss: 0.00121775
Iteration 11/25 | Loss: 0.00121775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001217754208482802, 0.001217754208482802, 0.001217754208482802, 0.001217754208482802, 0.001217754208482802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001217754208482802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.46575451
Iteration 2/25 | Loss: 0.00108518
Iteration 3/25 | Loss: 0.00108516
Iteration 4/25 | Loss: 0.00108516
Iteration 5/25 | Loss: 0.00108516
Iteration 6/25 | Loss: 0.00108516
Iteration 7/25 | Loss: 0.00108516
Iteration 8/25 | Loss: 0.00108516
Iteration 9/25 | Loss: 0.00108516
Iteration 10/25 | Loss: 0.00108516
Iteration 11/25 | Loss: 0.00108516
Iteration 12/25 | Loss: 0.00108516
Iteration 13/25 | Loss: 0.00108516
Iteration 14/25 | Loss: 0.00108516
Iteration 15/25 | Loss: 0.00108516
Iteration 16/25 | Loss: 0.00108516
Iteration 17/25 | Loss: 0.00108516
Iteration 18/25 | Loss: 0.00108516
Iteration 19/25 | Loss: 0.00108516
Iteration 20/25 | Loss: 0.00108516
Iteration 21/25 | Loss: 0.00108516
Iteration 22/25 | Loss: 0.00108516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010851605329662561, 0.0010851605329662561, 0.0010851605329662561, 0.0010851605329662561, 0.0010851605329662561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010851605329662561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108516
Iteration 2/1000 | Loss: 0.00002685
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001913
Iteration 5/1000 | Loss: 0.00001810
Iteration 6/1000 | Loss: 0.00001722
Iteration 7/1000 | Loss: 0.00001662
Iteration 8/1000 | Loss: 0.00001625
Iteration 9/1000 | Loss: 0.00001587
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001542
Iteration 12/1000 | Loss: 0.00001541
Iteration 13/1000 | Loss: 0.00001523
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001519
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001518
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001512
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001510
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001501
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001498
Iteration 46/1000 | Loss: 0.00001497
Iteration 47/1000 | Loss: 0.00001497
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001492
Iteration 57/1000 | Loss: 0.00001492
Iteration 58/1000 | Loss: 0.00001492
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001491
Iteration 62/1000 | Loss: 0.00001491
Iteration 63/1000 | Loss: 0.00001490
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001490
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001488
Iteration 79/1000 | Loss: 0.00001488
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001487
Iteration 82/1000 | Loss: 0.00001487
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001484
Iteration 95/1000 | Loss: 0.00001484
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001483
Iteration 99/1000 | Loss: 0.00001483
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001480
Iteration 109/1000 | Loss: 0.00001480
Iteration 110/1000 | Loss: 0.00001480
Iteration 111/1000 | Loss: 0.00001480
Iteration 112/1000 | Loss: 0.00001480
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001478
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001477
Iteration 121/1000 | Loss: 0.00001477
Iteration 122/1000 | Loss: 0.00001477
Iteration 123/1000 | Loss: 0.00001477
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001477
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001476
Iteration 130/1000 | Loss: 0.00001476
Iteration 131/1000 | Loss: 0.00001476
Iteration 132/1000 | Loss: 0.00001476
Iteration 133/1000 | Loss: 0.00001476
Iteration 134/1000 | Loss: 0.00001475
Iteration 135/1000 | Loss: 0.00001475
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001474
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001473
Iteration 157/1000 | Loss: 0.00001473
Iteration 158/1000 | Loss: 0.00001473
Iteration 159/1000 | Loss: 0.00001473
Iteration 160/1000 | Loss: 0.00001473
Iteration 161/1000 | Loss: 0.00001473
Iteration 162/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.4725835171702784e-05, 1.4725835171702784e-05, 1.4725835171702784e-05, 1.4725835171702784e-05, 1.4725835171702784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4725835171702784e-05

Optimization complete. Final v2v error: 3.241962194442749 mm

Highest mean error: 3.9380202293395996 mm for frame 146

Lowest mean error: 2.9293618202209473 mm for frame 18

Saving results

Total time: 38.00762939453125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031528
Iteration 2/25 | Loss: 0.01031528
Iteration 3/25 | Loss: 0.01031528
Iteration 4/25 | Loss: 0.01031527
Iteration 5/25 | Loss: 0.01031527
Iteration 6/25 | Loss: 0.00375481
Iteration 7/25 | Loss: 0.00263448
Iteration 8/25 | Loss: 0.00220312
Iteration 9/25 | Loss: 0.00199797
Iteration 10/25 | Loss: 0.00185796
Iteration 11/25 | Loss: 0.00175640
Iteration 12/25 | Loss: 0.00170554
Iteration 13/25 | Loss: 0.00163567
Iteration 14/25 | Loss: 0.00163055
Iteration 15/25 | Loss: 0.00162256
Iteration 16/25 | Loss: 0.00160411
Iteration 17/25 | Loss: 0.00159323
Iteration 18/25 | Loss: 0.00159610
Iteration 19/25 | Loss: 0.00158730
Iteration 20/25 | Loss: 0.00157341
Iteration 21/25 | Loss: 0.00157278
Iteration 22/25 | Loss: 0.00157212
Iteration 23/25 | Loss: 0.00157366
Iteration 24/25 | Loss: 0.00157026
Iteration 25/25 | Loss: 0.00157259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23887932
Iteration 2/25 | Loss: 0.00429273
Iteration 3/25 | Loss: 0.00348231
Iteration 4/25 | Loss: 0.00348231
Iteration 5/25 | Loss: 0.00348231
Iteration 6/25 | Loss: 0.00348231
Iteration 7/25 | Loss: 0.00348231
Iteration 8/25 | Loss: 0.00348231
Iteration 9/25 | Loss: 0.00348231
Iteration 10/25 | Loss: 0.00348231
Iteration 11/25 | Loss: 0.00348231
Iteration 12/25 | Loss: 0.00348231
Iteration 13/25 | Loss: 0.00348231
Iteration 14/25 | Loss: 0.00348231
Iteration 15/25 | Loss: 0.00348231
Iteration 16/25 | Loss: 0.00348231
Iteration 17/25 | Loss: 0.00348231
Iteration 18/25 | Loss: 0.00348231
Iteration 19/25 | Loss: 0.00348231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0034823070745915174, 0.0034823070745915174, 0.0034823070745915174, 0.0034823070745915174, 0.0034823070745915174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034823070745915174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348231
Iteration 2/1000 | Loss: 0.00658663
Iteration 3/1000 | Loss: 0.01104342
Iteration 4/1000 | Loss: 0.00182631
Iteration 5/1000 | Loss: 0.00159401
Iteration 6/1000 | Loss: 0.00083127
Iteration 7/1000 | Loss: 0.00072583
Iteration 8/1000 | Loss: 0.00109569
Iteration 9/1000 | Loss: 0.00060167
Iteration 10/1000 | Loss: 0.00030889
Iteration 11/1000 | Loss: 0.00042181
Iteration 12/1000 | Loss: 0.00032904
Iteration 13/1000 | Loss: 0.00023237
Iteration 14/1000 | Loss: 0.00022015
Iteration 15/1000 | Loss: 0.00018053
Iteration 16/1000 | Loss: 0.00049149
Iteration 17/1000 | Loss: 0.00155453
Iteration 18/1000 | Loss: 0.00140932
Iteration 19/1000 | Loss: 0.00059433
Iteration 20/1000 | Loss: 0.00033907
Iteration 21/1000 | Loss: 0.00019613
Iteration 22/1000 | Loss: 0.00021664
Iteration 23/1000 | Loss: 0.00015590
Iteration 24/1000 | Loss: 0.00020307
Iteration 25/1000 | Loss: 0.00067775
Iteration 26/1000 | Loss: 0.00428861
Iteration 27/1000 | Loss: 0.01044863
Iteration 28/1000 | Loss: 0.00562331
Iteration 29/1000 | Loss: 0.00321013
Iteration 30/1000 | Loss: 0.00395464
Iteration 31/1000 | Loss: 0.00289559
Iteration 32/1000 | Loss: 0.00234289
Iteration 33/1000 | Loss: 0.00041010
Iteration 34/1000 | Loss: 0.00107863
Iteration 35/1000 | Loss: 0.00013343
Iteration 36/1000 | Loss: 0.00033399
Iteration 37/1000 | Loss: 0.00010900
Iteration 38/1000 | Loss: 0.00007052
Iteration 39/1000 | Loss: 0.00011011
Iteration 40/1000 | Loss: 0.00007482
Iteration 41/1000 | Loss: 0.00014653
Iteration 42/1000 | Loss: 0.00005116
Iteration 43/1000 | Loss: 0.00006406
Iteration 44/1000 | Loss: 0.00003736
Iteration 45/1000 | Loss: 0.00017980
Iteration 46/1000 | Loss: 0.00028141
Iteration 47/1000 | Loss: 0.00002918
Iteration 48/1000 | Loss: 0.00006937
Iteration 49/1000 | Loss: 0.00002761
Iteration 50/1000 | Loss: 0.00011069
Iteration 51/1000 | Loss: 0.00013708
Iteration 52/1000 | Loss: 0.00003106
Iteration 53/1000 | Loss: 0.00005528
Iteration 54/1000 | Loss: 0.00014003
Iteration 55/1000 | Loss: 0.00002212
Iteration 56/1000 | Loss: 0.00007474
Iteration 57/1000 | Loss: 0.00004955
Iteration 58/1000 | Loss: 0.00007700
Iteration 59/1000 | Loss: 0.00001995
Iteration 60/1000 | Loss: 0.00004488
Iteration 61/1000 | Loss: 0.00010216
Iteration 62/1000 | Loss: 0.00010063
Iteration 63/1000 | Loss: 0.00002257
Iteration 64/1000 | Loss: 0.00002734
Iteration 65/1000 | Loss: 0.00001884
Iteration 66/1000 | Loss: 0.00010919
Iteration 67/1000 | Loss: 0.00001883
Iteration 68/1000 | Loss: 0.00004116
Iteration 69/1000 | Loss: 0.00001823
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00002559
Iteration 72/1000 | Loss: 0.00001787
Iteration 73/1000 | Loss: 0.00001783
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00001782
Iteration 77/1000 | Loss: 0.00001782
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00002452
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001762
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001760
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001759
Iteration 103/1000 | Loss: 0.00001759
Iteration 104/1000 | Loss: 0.00001758
Iteration 105/1000 | Loss: 0.00001758
Iteration 106/1000 | Loss: 0.00001758
Iteration 107/1000 | Loss: 0.00001758
Iteration 108/1000 | Loss: 0.00001758
Iteration 109/1000 | Loss: 0.00001758
Iteration 110/1000 | Loss: 0.00001758
Iteration 111/1000 | Loss: 0.00001757
Iteration 112/1000 | Loss: 0.00001756
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00002449
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00003974
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00002256
Iteration 127/1000 | Loss: 0.00001777
Iteration 128/1000 | Loss: 0.00001777
Iteration 129/1000 | Loss: 0.00001795
Iteration 130/1000 | Loss: 0.00001753
Iteration 131/1000 | Loss: 0.00001739
Iteration 132/1000 | Loss: 0.00001739
Iteration 133/1000 | Loss: 0.00001739
Iteration 134/1000 | Loss: 0.00001739
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001739
Iteration 137/1000 | Loss: 0.00001739
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001738
Iteration 141/1000 | Loss: 0.00001738
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00001738
Iteration 145/1000 | Loss: 0.00001738
Iteration 146/1000 | Loss: 0.00001738
Iteration 147/1000 | Loss: 0.00001738
Iteration 148/1000 | Loss: 0.00001738
Iteration 149/1000 | Loss: 0.00001738
Iteration 150/1000 | Loss: 0.00001738
Iteration 151/1000 | Loss: 0.00001738
Iteration 152/1000 | Loss: 0.00001742
Iteration 153/1000 | Loss: 0.00001740
Iteration 154/1000 | Loss: 0.00001739
Iteration 155/1000 | Loss: 0.00001739
Iteration 156/1000 | Loss: 0.00001739
Iteration 157/1000 | Loss: 0.00001738
Iteration 158/1000 | Loss: 0.00001738
Iteration 159/1000 | Loss: 0.00001738
Iteration 160/1000 | Loss: 0.00001742
Iteration 161/1000 | Loss: 0.00001738
Iteration 162/1000 | Loss: 0.00001738
Iteration 163/1000 | Loss: 0.00001738
Iteration 164/1000 | Loss: 0.00001738
Iteration 165/1000 | Loss: 0.00001738
Iteration 166/1000 | Loss: 0.00001738
Iteration 167/1000 | Loss: 0.00001737
Iteration 168/1000 | Loss: 0.00001737
Iteration 169/1000 | Loss: 0.00001737
Iteration 170/1000 | Loss: 0.00001737
Iteration 171/1000 | Loss: 0.00001737
Iteration 172/1000 | Loss: 0.00001737
Iteration 173/1000 | Loss: 0.00001737
Iteration 174/1000 | Loss: 0.00001737
Iteration 175/1000 | Loss: 0.00001737
Iteration 176/1000 | Loss: 0.00001737
Iteration 177/1000 | Loss: 0.00001737
Iteration 178/1000 | Loss: 0.00001737
Iteration 179/1000 | Loss: 0.00001737
Iteration 180/1000 | Loss: 0.00001737
Iteration 181/1000 | Loss: 0.00001737
Iteration 182/1000 | Loss: 0.00001737
Iteration 183/1000 | Loss: 0.00001737
Iteration 184/1000 | Loss: 0.00001737
Iteration 185/1000 | Loss: 0.00001737
Iteration 186/1000 | Loss: 0.00001737
Iteration 187/1000 | Loss: 0.00001737
Iteration 188/1000 | Loss: 0.00001737
Iteration 189/1000 | Loss: 0.00001737
Iteration 190/1000 | Loss: 0.00001737
Iteration 191/1000 | Loss: 0.00001737
Iteration 192/1000 | Loss: 0.00001737
Iteration 193/1000 | Loss: 0.00001737
Iteration 194/1000 | Loss: 0.00001737
Iteration 195/1000 | Loss: 0.00001737
Iteration 196/1000 | Loss: 0.00001737
Iteration 197/1000 | Loss: 0.00001737
Iteration 198/1000 | Loss: 0.00001737
Iteration 199/1000 | Loss: 0.00001737
Iteration 200/1000 | Loss: 0.00001737
Iteration 201/1000 | Loss: 0.00001737
Iteration 202/1000 | Loss: 0.00001737
Iteration 203/1000 | Loss: 0.00001737
Iteration 204/1000 | Loss: 0.00001737
Iteration 205/1000 | Loss: 0.00001737
Iteration 206/1000 | Loss: 0.00001737
Iteration 207/1000 | Loss: 0.00001737
Iteration 208/1000 | Loss: 0.00001737
Iteration 209/1000 | Loss: 0.00001737
Iteration 210/1000 | Loss: 0.00001737
Iteration 211/1000 | Loss: 0.00001737
Iteration 212/1000 | Loss: 0.00001737
Iteration 213/1000 | Loss: 0.00001737
Iteration 214/1000 | Loss: 0.00001737
Iteration 215/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.7374346498399973e-05, 1.7374346498399973e-05, 1.7374346498399973e-05, 1.7374346498399973e-05, 1.7374346498399973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7374346498399973e-05

Optimization complete. Final v2v error: 3.3758420944213867 mm

Highest mean error: 6.147881031036377 mm for frame 87

Lowest mean error: 2.791546106338501 mm for frame 102

Saving results

Total time: 169.0736916065216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783682
Iteration 2/25 | Loss: 0.00135599
Iteration 3/25 | Loss: 0.00122808
Iteration 4/25 | Loss: 0.00121856
Iteration 5/25 | Loss: 0.00121738
Iteration 6/25 | Loss: 0.00121738
Iteration 7/25 | Loss: 0.00121738
Iteration 8/25 | Loss: 0.00121738
Iteration 9/25 | Loss: 0.00121738
Iteration 10/25 | Loss: 0.00121738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012173786526545882, 0.0012173786526545882, 0.0012173786526545882, 0.0012173786526545882, 0.0012173786526545882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012173786526545882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33271861
Iteration 2/25 | Loss: 0.00085168
Iteration 3/25 | Loss: 0.00085166
Iteration 4/25 | Loss: 0.00085166
Iteration 5/25 | Loss: 0.00085166
Iteration 6/25 | Loss: 0.00085166
Iteration 7/25 | Loss: 0.00085166
Iteration 8/25 | Loss: 0.00085166
Iteration 9/25 | Loss: 0.00085166
Iteration 10/25 | Loss: 0.00085166
Iteration 11/25 | Loss: 0.00085166
Iteration 12/25 | Loss: 0.00085166
Iteration 13/25 | Loss: 0.00085166
Iteration 14/25 | Loss: 0.00085166
Iteration 15/25 | Loss: 0.00085166
Iteration 16/25 | Loss: 0.00085166
Iteration 17/25 | Loss: 0.00085166
Iteration 18/25 | Loss: 0.00085166
Iteration 19/25 | Loss: 0.00085166
Iteration 20/25 | Loss: 0.00085166
Iteration 21/25 | Loss: 0.00085166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008516550878994167, 0.0008516550878994167, 0.0008516550878994167, 0.0008516550878994167, 0.0008516550878994167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008516550878994167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085166
Iteration 2/1000 | Loss: 0.00002477
Iteration 3/1000 | Loss: 0.00001980
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001604
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001355
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001308
Iteration 15/1000 | Loss: 0.00001288
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001277
Iteration 21/1000 | Loss: 0.00001276
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001261
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001249
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001223
Iteration 49/1000 | Loss: 0.00001223
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001219
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001218
Iteration 64/1000 | Loss: 0.00001218
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001217
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001216
Iteration 81/1000 | Loss: 0.00001216
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.2151036571594886e-05, 1.2151036571594886e-05, 1.2151036571594886e-05, 1.2151036571594886e-05, 1.2151036571594886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2151036571594886e-05

Optimization complete. Final v2v error: 2.9395477771759033 mm

Highest mean error: 3.2754034996032715 mm for frame 62

Lowest mean error: 2.6104564666748047 mm for frame 28

Saving results

Total time: 38.30436372756958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552672
Iteration 2/25 | Loss: 0.00134929
Iteration 3/25 | Loss: 0.00127424
Iteration 4/25 | Loss: 0.00126664
Iteration 5/25 | Loss: 0.00126423
Iteration 6/25 | Loss: 0.00126421
Iteration 7/25 | Loss: 0.00126421
Iteration 8/25 | Loss: 0.00126421
Iteration 9/25 | Loss: 0.00126421
Iteration 10/25 | Loss: 0.00126421
Iteration 11/25 | Loss: 0.00126421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012642148649320006, 0.0012642148649320006, 0.0012642148649320006, 0.0012642148649320006, 0.0012642148649320006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012642148649320006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31359565
Iteration 2/25 | Loss: 0.00104948
Iteration 3/25 | Loss: 0.00104943
Iteration 4/25 | Loss: 0.00104943
Iteration 5/25 | Loss: 0.00104943
Iteration 6/25 | Loss: 0.00104943
Iteration 7/25 | Loss: 0.00104943
Iteration 8/25 | Loss: 0.00104943
Iteration 9/25 | Loss: 0.00104943
Iteration 10/25 | Loss: 0.00104943
Iteration 11/25 | Loss: 0.00104943
Iteration 12/25 | Loss: 0.00104943
Iteration 13/25 | Loss: 0.00104943
Iteration 14/25 | Loss: 0.00104943
Iteration 15/25 | Loss: 0.00104943
Iteration 16/25 | Loss: 0.00104943
Iteration 17/25 | Loss: 0.00104943
Iteration 18/25 | Loss: 0.00104943
Iteration 19/25 | Loss: 0.00104943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010494306916370988, 0.0010494306916370988, 0.0010494306916370988, 0.0010494306916370988, 0.0010494306916370988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010494306916370988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104943
Iteration 2/1000 | Loss: 0.00002470
Iteration 3/1000 | Loss: 0.00001765
Iteration 4/1000 | Loss: 0.00001594
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001337
Iteration 9/1000 | Loss: 0.00001318
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001298
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001291
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001253
Iteration 44/1000 | Loss: 0.00001253
Iteration 45/1000 | Loss: 0.00001253
Iteration 46/1000 | Loss: 0.00001253
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001252
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001251
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001250
Iteration 57/1000 | Loss: 0.00001250
Iteration 58/1000 | Loss: 0.00001250
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001249
Iteration 71/1000 | Loss: 0.00001249
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001248
Iteration 76/1000 | Loss: 0.00001248
Iteration 77/1000 | Loss: 0.00001248
Iteration 78/1000 | Loss: 0.00001248
Iteration 79/1000 | Loss: 0.00001248
Iteration 80/1000 | Loss: 0.00001248
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001247
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001246
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001245
Iteration 106/1000 | Loss: 0.00001245
Iteration 107/1000 | Loss: 0.00001245
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001245
Iteration 114/1000 | Loss: 0.00001245
Iteration 115/1000 | Loss: 0.00001245
Iteration 116/1000 | Loss: 0.00001245
Iteration 117/1000 | Loss: 0.00001245
Iteration 118/1000 | Loss: 0.00001245
Iteration 119/1000 | Loss: 0.00001245
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001244
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001244
Iteration 132/1000 | Loss: 0.00001244
Iteration 133/1000 | Loss: 0.00001244
Iteration 134/1000 | Loss: 0.00001244
Iteration 135/1000 | Loss: 0.00001244
Iteration 136/1000 | Loss: 0.00001243
Iteration 137/1000 | Loss: 0.00001243
Iteration 138/1000 | Loss: 0.00001243
Iteration 139/1000 | Loss: 0.00001243
Iteration 140/1000 | Loss: 0.00001243
Iteration 141/1000 | Loss: 0.00001243
Iteration 142/1000 | Loss: 0.00001243
Iteration 143/1000 | Loss: 0.00001243
Iteration 144/1000 | Loss: 0.00001243
Iteration 145/1000 | Loss: 0.00001243
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001243
Iteration 148/1000 | Loss: 0.00001243
Iteration 149/1000 | Loss: 0.00001243
Iteration 150/1000 | Loss: 0.00001243
Iteration 151/1000 | Loss: 0.00001243
Iteration 152/1000 | Loss: 0.00001243
Iteration 153/1000 | Loss: 0.00001243
Iteration 154/1000 | Loss: 0.00001243
Iteration 155/1000 | Loss: 0.00001243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.2434738891897723e-05, 1.2434738891897723e-05, 1.2434738891897723e-05, 1.2434738891897723e-05, 1.2434738891897723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2434738891897723e-05

Optimization complete. Final v2v error: 3.0091843605041504 mm

Highest mean error: 3.154217481613159 mm for frame 149

Lowest mean error: 2.9040701389312744 mm for frame 55

Saving results

Total time: 34.18949365615845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019228
Iteration 2/25 | Loss: 0.00166331
Iteration 3/25 | Loss: 0.00140022
Iteration 4/25 | Loss: 0.00137798
Iteration 5/25 | Loss: 0.00139478
Iteration 6/25 | Loss: 0.00136253
Iteration 7/25 | Loss: 0.00132619
Iteration 8/25 | Loss: 0.00130295
Iteration 9/25 | Loss: 0.00129627
Iteration 10/25 | Loss: 0.00129372
Iteration 11/25 | Loss: 0.00129312
Iteration 12/25 | Loss: 0.00129297
Iteration 13/25 | Loss: 0.00129296
Iteration 14/25 | Loss: 0.00129296
Iteration 15/25 | Loss: 0.00129295
Iteration 16/25 | Loss: 0.00129295
Iteration 17/25 | Loss: 0.00129295
Iteration 18/25 | Loss: 0.00129295
Iteration 19/25 | Loss: 0.00129295
Iteration 20/25 | Loss: 0.00129295
Iteration 21/25 | Loss: 0.00129295
Iteration 22/25 | Loss: 0.00129295
Iteration 23/25 | Loss: 0.00129295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012929494259878993, 0.0012929494259878993, 0.0012929494259878993, 0.0012929494259878993, 0.0012929494259878993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012929494259878993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54785931
Iteration 2/25 | Loss: 0.00118599
Iteration 3/25 | Loss: 0.00118597
Iteration 4/25 | Loss: 0.00118597
Iteration 5/25 | Loss: 0.00118597
Iteration 6/25 | Loss: 0.00118597
Iteration 7/25 | Loss: 0.00118597
Iteration 8/25 | Loss: 0.00118597
Iteration 9/25 | Loss: 0.00118597
Iteration 10/25 | Loss: 0.00118597
Iteration 11/25 | Loss: 0.00118597
Iteration 12/25 | Loss: 0.00118596
Iteration 13/25 | Loss: 0.00118597
Iteration 14/25 | Loss: 0.00118596
Iteration 15/25 | Loss: 0.00118597
Iteration 16/25 | Loss: 0.00118596
Iteration 17/25 | Loss: 0.00118596
Iteration 18/25 | Loss: 0.00118596
Iteration 19/25 | Loss: 0.00118597
Iteration 20/25 | Loss: 0.00118596
Iteration 21/25 | Loss: 0.00118596
Iteration 22/25 | Loss: 0.00118596
Iteration 23/25 | Loss: 0.00118596
Iteration 24/25 | Loss: 0.00118596
Iteration 25/25 | Loss: 0.00118596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011859649093821645, 0.0011859649093821645, 0.0011859649093821645, 0.0011859649093821645, 0.0011859649093821645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011859649093821645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118596
Iteration 2/1000 | Loss: 0.00004445
Iteration 3/1000 | Loss: 0.00002842
Iteration 4/1000 | Loss: 0.00002117
Iteration 5/1000 | Loss: 0.00001922
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001667
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001648
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001643
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001632
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001621
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001621
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001620
Iteration 34/1000 | Loss: 0.00001620
Iteration 35/1000 | Loss: 0.00001620
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001620
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001619
Iteration 41/1000 | Loss: 0.00001619
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001618
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001617
Iteration 46/1000 | Loss: 0.00001617
Iteration 47/1000 | Loss: 0.00001616
Iteration 48/1000 | Loss: 0.00001616
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001615
Iteration 51/1000 | Loss: 0.00001615
Iteration 52/1000 | Loss: 0.00001615
Iteration 53/1000 | Loss: 0.00001615
Iteration 54/1000 | Loss: 0.00001615
Iteration 55/1000 | Loss: 0.00001615
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001615
Iteration 58/1000 | Loss: 0.00001615
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001615
Iteration 61/1000 | Loss: 0.00001614
Iteration 62/1000 | Loss: 0.00001614
Iteration 63/1000 | Loss: 0.00001614
Iteration 64/1000 | Loss: 0.00001614
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001613
Iteration 73/1000 | Loss: 0.00001613
Iteration 74/1000 | Loss: 0.00001613
Iteration 75/1000 | Loss: 0.00001613
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001612
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001611
Iteration 92/1000 | Loss: 0.00001611
Iteration 93/1000 | Loss: 0.00001611
Iteration 94/1000 | Loss: 0.00001611
Iteration 95/1000 | Loss: 0.00001611
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001611
Iteration 100/1000 | Loss: 0.00001611
Iteration 101/1000 | Loss: 0.00001611
Iteration 102/1000 | Loss: 0.00001611
Iteration 103/1000 | Loss: 0.00001611
Iteration 104/1000 | Loss: 0.00001610
Iteration 105/1000 | Loss: 0.00001610
Iteration 106/1000 | Loss: 0.00001610
Iteration 107/1000 | Loss: 0.00001610
Iteration 108/1000 | Loss: 0.00001610
Iteration 109/1000 | Loss: 0.00001610
Iteration 110/1000 | Loss: 0.00001610
Iteration 111/1000 | Loss: 0.00001610
Iteration 112/1000 | Loss: 0.00001610
Iteration 113/1000 | Loss: 0.00001610
Iteration 114/1000 | Loss: 0.00001610
Iteration 115/1000 | Loss: 0.00001610
Iteration 116/1000 | Loss: 0.00001610
Iteration 117/1000 | Loss: 0.00001610
Iteration 118/1000 | Loss: 0.00001610
Iteration 119/1000 | Loss: 0.00001610
Iteration 120/1000 | Loss: 0.00001610
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001609
Iteration 124/1000 | Loss: 0.00001609
Iteration 125/1000 | Loss: 0.00001609
Iteration 126/1000 | Loss: 0.00001609
Iteration 127/1000 | Loss: 0.00001609
Iteration 128/1000 | Loss: 0.00001609
Iteration 129/1000 | Loss: 0.00001609
Iteration 130/1000 | Loss: 0.00001609
Iteration 131/1000 | Loss: 0.00001609
Iteration 132/1000 | Loss: 0.00001609
Iteration 133/1000 | Loss: 0.00001609
Iteration 134/1000 | Loss: 0.00001609
Iteration 135/1000 | Loss: 0.00001609
Iteration 136/1000 | Loss: 0.00001609
Iteration 137/1000 | Loss: 0.00001608
Iteration 138/1000 | Loss: 0.00001608
Iteration 139/1000 | Loss: 0.00001608
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001608
Iteration 142/1000 | Loss: 0.00001608
Iteration 143/1000 | Loss: 0.00001608
Iteration 144/1000 | Loss: 0.00001608
Iteration 145/1000 | Loss: 0.00001608
Iteration 146/1000 | Loss: 0.00001608
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001608
Iteration 150/1000 | Loss: 0.00001608
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001608
Iteration 153/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.607938793313224e-05, 1.607938793313224e-05, 1.607938793313224e-05, 1.607938793313224e-05, 1.607938793313224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.607938793313224e-05

Optimization complete. Final v2v error: 3.3221864700317383 mm

Highest mean error: 4.984488010406494 mm for frame 0

Lowest mean error: 2.883657932281494 mm for frame 57

Saving results

Total time: 51.551326274871826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460811
Iteration 2/25 | Loss: 0.00133335
Iteration 3/25 | Loss: 0.00123868
Iteration 4/25 | Loss: 0.00123006
Iteration 5/25 | Loss: 0.00122835
Iteration 6/25 | Loss: 0.00122835
Iteration 7/25 | Loss: 0.00122835
Iteration 8/25 | Loss: 0.00122835
Iteration 9/25 | Loss: 0.00122835
Iteration 10/25 | Loss: 0.00122835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012283538235351443, 0.0012283538235351443, 0.0012283538235351443, 0.0012283538235351443, 0.0012283538235351443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012283538235351443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.29822063
Iteration 2/25 | Loss: 0.00102540
Iteration 3/25 | Loss: 0.00102539
Iteration 4/25 | Loss: 0.00102539
Iteration 5/25 | Loss: 0.00102539
Iteration 6/25 | Loss: 0.00102539
Iteration 7/25 | Loss: 0.00102539
Iteration 8/25 | Loss: 0.00102539
Iteration 9/25 | Loss: 0.00102539
Iteration 10/25 | Loss: 0.00102539
Iteration 11/25 | Loss: 0.00102539
Iteration 12/25 | Loss: 0.00102539
Iteration 13/25 | Loss: 0.00102539
Iteration 14/25 | Loss: 0.00102539
Iteration 15/25 | Loss: 0.00102539
Iteration 16/25 | Loss: 0.00102539
Iteration 17/25 | Loss: 0.00102539
Iteration 18/25 | Loss: 0.00102539
Iteration 19/25 | Loss: 0.00102539
Iteration 20/25 | Loss: 0.00102539
Iteration 21/25 | Loss: 0.00102539
Iteration 22/25 | Loss: 0.00102539
Iteration 23/25 | Loss: 0.00102539
Iteration 24/25 | Loss: 0.00102539
Iteration 25/25 | Loss: 0.00102539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102539
Iteration 2/1000 | Loss: 0.00002102
Iteration 3/1000 | Loss: 0.00001633
Iteration 4/1000 | Loss: 0.00001515
Iteration 5/1000 | Loss: 0.00001429
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001264
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001247
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001233
Iteration 21/1000 | Loss: 0.00001233
Iteration 22/1000 | Loss: 0.00001233
Iteration 23/1000 | Loss: 0.00001228
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001213
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001197
Iteration 42/1000 | Loss: 0.00001197
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001194
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001193
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001192
Iteration 50/1000 | Loss: 0.00001191
Iteration 51/1000 | Loss: 0.00001191
Iteration 52/1000 | Loss: 0.00001191
Iteration 53/1000 | Loss: 0.00001190
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001190
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001184
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001179
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001177
Iteration 103/1000 | Loss: 0.00001177
Iteration 104/1000 | Loss: 0.00001177
Iteration 105/1000 | Loss: 0.00001177
Iteration 106/1000 | Loss: 0.00001177
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001176
Iteration 110/1000 | Loss: 0.00001176
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001175
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Iteration 150/1000 | Loss: 0.00001170
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001170
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001169
Iteration 165/1000 | Loss: 0.00001169
Iteration 166/1000 | Loss: 0.00001169
Iteration 167/1000 | Loss: 0.00001169
Iteration 168/1000 | Loss: 0.00001169
Iteration 169/1000 | Loss: 0.00001168
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001168
Iteration 172/1000 | Loss: 0.00001168
Iteration 173/1000 | Loss: 0.00001168
Iteration 174/1000 | Loss: 0.00001168
Iteration 175/1000 | Loss: 0.00001168
Iteration 176/1000 | Loss: 0.00001168
Iteration 177/1000 | Loss: 0.00001168
Iteration 178/1000 | Loss: 0.00001168
Iteration 179/1000 | Loss: 0.00001168
Iteration 180/1000 | Loss: 0.00001168
Iteration 181/1000 | Loss: 0.00001168
Iteration 182/1000 | Loss: 0.00001168
Iteration 183/1000 | Loss: 0.00001168
Iteration 184/1000 | Loss: 0.00001168
Iteration 185/1000 | Loss: 0.00001168
Iteration 186/1000 | Loss: 0.00001168
Iteration 187/1000 | Loss: 0.00001168
Iteration 188/1000 | Loss: 0.00001168
Iteration 189/1000 | Loss: 0.00001168
Iteration 190/1000 | Loss: 0.00001168
Iteration 191/1000 | Loss: 0.00001168
Iteration 192/1000 | Loss: 0.00001168
Iteration 193/1000 | Loss: 0.00001168
Iteration 194/1000 | Loss: 0.00001168
Iteration 195/1000 | Loss: 0.00001168
Iteration 196/1000 | Loss: 0.00001168
Iteration 197/1000 | Loss: 0.00001168
Iteration 198/1000 | Loss: 0.00001168
Iteration 199/1000 | Loss: 0.00001168
Iteration 200/1000 | Loss: 0.00001168
Iteration 201/1000 | Loss: 0.00001168
Iteration 202/1000 | Loss: 0.00001168
Iteration 203/1000 | Loss: 0.00001168
Iteration 204/1000 | Loss: 0.00001168
Iteration 205/1000 | Loss: 0.00001168
Iteration 206/1000 | Loss: 0.00001168
Iteration 207/1000 | Loss: 0.00001168
Iteration 208/1000 | Loss: 0.00001168
Iteration 209/1000 | Loss: 0.00001168
Iteration 210/1000 | Loss: 0.00001168
Iteration 211/1000 | Loss: 0.00001168
Iteration 212/1000 | Loss: 0.00001168
Iteration 213/1000 | Loss: 0.00001168
Iteration 214/1000 | Loss: 0.00001168
Iteration 215/1000 | Loss: 0.00001168
Iteration 216/1000 | Loss: 0.00001168
Iteration 217/1000 | Loss: 0.00001168
Iteration 218/1000 | Loss: 0.00001168
Iteration 219/1000 | Loss: 0.00001168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.1681727301038336e-05, 1.1681727301038336e-05, 1.1681727301038336e-05, 1.1681727301038336e-05, 1.1681727301038336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1681727301038336e-05

Optimization complete. Final v2v error: 2.93390154838562 mm

Highest mean error: 3.2752833366394043 mm for frame 184

Lowest mean error: 2.687615156173706 mm for frame 165

Saving results

Total time: 44.976693868637085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820151
Iteration 2/25 | Loss: 0.00135427
Iteration 3/25 | Loss: 0.00125346
Iteration 4/25 | Loss: 0.00124033
Iteration 5/25 | Loss: 0.00123648
Iteration 6/25 | Loss: 0.00123624
Iteration 7/25 | Loss: 0.00123624
Iteration 8/25 | Loss: 0.00123624
Iteration 9/25 | Loss: 0.00123624
Iteration 10/25 | Loss: 0.00123624
Iteration 11/25 | Loss: 0.00123624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012362414272502065, 0.0012362414272502065, 0.0012362414272502065, 0.0012362414272502065, 0.0012362414272502065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012362414272502065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72367692
Iteration 2/25 | Loss: 0.00109007
Iteration 3/25 | Loss: 0.00109006
Iteration 4/25 | Loss: 0.00109006
Iteration 5/25 | Loss: 0.00109006
Iteration 6/25 | Loss: 0.00109006
Iteration 7/25 | Loss: 0.00109006
Iteration 8/25 | Loss: 0.00109006
Iteration 9/25 | Loss: 0.00109006
Iteration 10/25 | Loss: 0.00109006
Iteration 11/25 | Loss: 0.00109006
Iteration 12/25 | Loss: 0.00109006
Iteration 13/25 | Loss: 0.00109006
Iteration 14/25 | Loss: 0.00109006
Iteration 15/25 | Loss: 0.00109006
Iteration 16/25 | Loss: 0.00109006
Iteration 17/25 | Loss: 0.00109006
Iteration 18/25 | Loss: 0.00109006
Iteration 19/25 | Loss: 0.00109006
Iteration 20/25 | Loss: 0.00109006
Iteration 21/25 | Loss: 0.00109006
Iteration 22/25 | Loss: 0.00109006
Iteration 23/25 | Loss: 0.00109006
Iteration 24/25 | Loss: 0.00109006
Iteration 25/25 | Loss: 0.00109006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109006
Iteration 2/1000 | Loss: 0.00002195
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001444
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00001309
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001284
Iteration 13/1000 | Loss: 0.00001283
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001237
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001228
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001226
Iteration 59/1000 | Loss: 0.00001226
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001218
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001211
Iteration 137/1000 | Loss: 0.00001211
Iteration 138/1000 | Loss: 0.00001210
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001210
Iteration 142/1000 | Loss: 0.00001210
Iteration 143/1000 | Loss: 0.00001210
Iteration 144/1000 | Loss: 0.00001210
Iteration 145/1000 | Loss: 0.00001210
Iteration 146/1000 | Loss: 0.00001210
Iteration 147/1000 | Loss: 0.00001210
Iteration 148/1000 | Loss: 0.00001209
Iteration 149/1000 | Loss: 0.00001209
Iteration 150/1000 | Loss: 0.00001209
Iteration 151/1000 | Loss: 0.00001209
Iteration 152/1000 | Loss: 0.00001209
Iteration 153/1000 | Loss: 0.00001209
Iteration 154/1000 | Loss: 0.00001209
Iteration 155/1000 | Loss: 0.00001209
Iteration 156/1000 | Loss: 0.00001209
Iteration 157/1000 | Loss: 0.00001209
Iteration 158/1000 | Loss: 0.00001209
Iteration 159/1000 | Loss: 0.00001209
Iteration 160/1000 | Loss: 0.00001209
Iteration 161/1000 | Loss: 0.00001209
Iteration 162/1000 | Loss: 0.00001209
Iteration 163/1000 | Loss: 0.00001209
Iteration 164/1000 | Loss: 0.00001209
Iteration 165/1000 | Loss: 0.00001208
Iteration 166/1000 | Loss: 0.00001208
Iteration 167/1000 | Loss: 0.00001208
Iteration 168/1000 | Loss: 0.00001208
Iteration 169/1000 | Loss: 0.00001208
Iteration 170/1000 | Loss: 0.00001208
Iteration 171/1000 | Loss: 0.00001208
Iteration 172/1000 | Loss: 0.00001208
Iteration 173/1000 | Loss: 0.00001208
Iteration 174/1000 | Loss: 0.00001208
Iteration 175/1000 | Loss: 0.00001208
Iteration 176/1000 | Loss: 0.00001208
Iteration 177/1000 | Loss: 0.00001208
Iteration 178/1000 | Loss: 0.00001208
Iteration 179/1000 | Loss: 0.00001208
Iteration 180/1000 | Loss: 0.00001208
Iteration 181/1000 | Loss: 0.00001208
Iteration 182/1000 | Loss: 0.00001208
Iteration 183/1000 | Loss: 0.00001208
Iteration 184/1000 | Loss: 0.00001208
Iteration 185/1000 | Loss: 0.00001208
Iteration 186/1000 | Loss: 0.00001208
Iteration 187/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.207682907988783e-05, 1.207682907988783e-05, 1.207682907988783e-05, 1.207682907988783e-05, 1.207682907988783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.207682907988783e-05

Optimization complete. Final v2v error: 2.9800684452056885 mm

Highest mean error: 3.6714558601379395 mm for frame 115

Lowest mean error: 2.7416131496429443 mm for frame 70

Saving results

Total time: 41.413288593292236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984141
Iteration 2/25 | Loss: 0.00255983
Iteration 3/25 | Loss: 0.00208202
Iteration 4/25 | Loss: 0.00191863
Iteration 5/25 | Loss: 0.00172481
Iteration 6/25 | Loss: 0.00154234
Iteration 7/25 | Loss: 0.00147398
Iteration 8/25 | Loss: 0.00143356
Iteration 9/25 | Loss: 0.00138029
Iteration 10/25 | Loss: 0.00136509
Iteration 11/25 | Loss: 0.00136527
Iteration 12/25 | Loss: 0.00135405
Iteration 13/25 | Loss: 0.00135468
Iteration 14/25 | Loss: 0.00136068
Iteration 15/25 | Loss: 0.00134741
Iteration 16/25 | Loss: 0.00134465
Iteration 17/25 | Loss: 0.00133100
Iteration 18/25 | Loss: 0.00133001
Iteration 19/25 | Loss: 0.00132974
Iteration 20/25 | Loss: 0.00132962
Iteration 21/25 | Loss: 0.00132951
Iteration 22/25 | Loss: 0.00132943
Iteration 23/25 | Loss: 0.00132942
Iteration 24/25 | Loss: 0.00132942
Iteration 25/25 | Loss: 0.00132942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24670923
Iteration 2/25 | Loss: 0.00086233
Iteration 3/25 | Loss: 0.00086231
Iteration 4/25 | Loss: 0.00086231
Iteration 5/25 | Loss: 0.00086231
Iteration 6/25 | Loss: 0.00086231
Iteration 7/25 | Loss: 0.00086231
Iteration 8/25 | Loss: 0.00086231
Iteration 9/25 | Loss: 0.00086231
Iteration 10/25 | Loss: 0.00086231
Iteration 11/25 | Loss: 0.00086231
Iteration 12/25 | Loss: 0.00086231
Iteration 13/25 | Loss: 0.00086231
Iteration 14/25 | Loss: 0.00086231
Iteration 15/25 | Loss: 0.00086231
Iteration 16/25 | Loss: 0.00086231
Iteration 17/25 | Loss: 0.00086231
Iteration 18/25 | Loss: 0.00086231
Iteration 19/25 | Loss: 0.00086231
Iteration 20/25 | Loss: 0.00086231
Iteration 21/25 | Loss: 0.00086231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000862312619574368, 0.000862312619574368, 0.000862312619574368, 0.000862312619574368, 0.000862312619574368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000862312619574368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086231
Iteration 2/1000 | Loss: 0.00007772
Iteration 3/1000 | Loss: 0.00013327
Iteration 4/1000 | Loss: 0.00007291
Iteration 5/1000 | Loss: 0.00004223
Iteration 6/1000 | Loss: 0.00003258
Iteration 7/1000 | Loss: 0.00003050
Iteration 8/1000 | Loss: 0.00015856
Iteration 9/1000 | Loss: 0.00037335
Iteration 10/1000 | Loss: 0.00010884
Iteration 11/1000 | Loss: 0.00015554
Iteration 12/1000 | Loss: 0.00048584
Iteration 13/1000 | Loss: 0.00015320
Iteration 14/1000 | Loss: 0.00019730
Iteration 15/1000 | Loss: 0.00012995
Iteration 16/1000 | Loss: 0.00040925
Iteration 17/1000 | Loss: 0.00021446
Iteration 18/1000 | Loss: 0.00067381
Iteration 19/1000 | Loss: 0.00003958
Iteration 20/1000 | Loss: 0.00002818
Iteration 21/1000 | Loss: 0.00002520
Iteration 22/1000 | Loss: 0.00002415
Iteration 23/1000 | Loss: 0.00030928
Iteration 24/1000 | Loss: 0.00003993
Iteration 25/1000 | Loss: 0.00002570
Iteration 26/1000 | Loss: 0.00002043
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001875
Iteration 30/1000 | Loss: 0.00001853
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001839
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001802
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001793
Iteration 41/1000 | Loss: 0.00001789
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001778
Iteration 88/1000 | Loss: 0.00001778
Iteration 89/1000 | Loss: 0.00001778
Iteration 90/1000 | Loss: 0.00001778
Iteration 91/1000 | Loss: 0.00001778
Iteration 92/1000 | Loss: 0.00001778
Iteration 93/1000 | Loss: 0.00001778
Iteration 94/1000 | Loss: 0.00001778
Iteration 95/1000 | Loss: 0.00001778
Iteration 96/1000 | Loss: 0.00001778
Iteration 97/1000 | Loss: 0.00001778
Iteration 98/1000 | Loss: 0.00001778
Iteration 99/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.7779962945496663e-05, 1.7779962945496663e-05, 1.7779962945496663e-05, 1.7779962945496663e-05, 1.7779962945496663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7779962945496663e-05

Optimization complete. Final v2v error: 3.5646111965179443 mm

Highest mean error: 4.328271865844727 mm for frame 10

Lowest mean error: 3.2102012634277344 mm for frame 107

Saving results

Total time: 87.73485732078552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044021
Iteration 2/25 | Loss: 0.00286560
Iteration 3/25 | Loss: 0.00194345
Iteration 4/25 | Loss: 0.00179813
Iteration 5/25 | Loss: 0.00180947
Iteration 6/25 | Loss: 0.00176579
Iteration 7/25 | Loss: 0.00170899
Iteration 8/25 | Loss: 0.00169537
Iteration 9/25 | Loss: 0.00165889
Iteration 10/25 | Loss: 0.00165664
Iteration 11/25 | Loss: 0.00165386
Iteration 12/25 | Loss: 0.00164828
Iteration 13/25 | Loss: 0.00164484
Iteration 14/25 | Loss: 0.00164084
Iteration 15/25 | Loss: 0.00163894
Iteration 16/25 | Loss: 0.00164150
Iteration 17/25 | Loss: 0.00163993
Iteration 18/25 | Loss: 0.00163750
Iteration 19/25 | Loss: 0.00163614
Iteration 20/25 | Loss: 0.00163590
Iteration 21/25 | Loss: 0.00163563
Iteration 22/25 | Loss: 0.00163542
Iteration 23/25 | Loss: 0.00163526
Iteration 24/25 | Loss: 0.00163512
Iteration 25/25 | Loss: 0.00163490

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15523124
Iteration 2/25 | Loss: 0.00429120
Iteration 3/25 | Loss: 0.00429120
Iteration 4/25 | Loss: 0.00429120
Iteration 5/25 | Loss: 0.00429120
Iteration 6/25 | Loss: 0.00429120
Iteration 7/25 | Loss: 0.00429120
Iteration 8/25 | Loss: 0.00429120
Iteration 9/25 | Loss: 0.00429120
Iteration 10/25 | Loss: 0.00429120
Iteration 11/25 | Loss: 0.00429120
Iteration 12/25 | Loss: 0.00429120
Iteration 13/25 | Loss: 0.00429120
Iteration 14/25 | Loss: 0.00429120
Iteration 15/25 | Loss: 0.00429120
Iteration 16/25 | Loss: 0.00429120
Iteration 17/25 | Loss: 0.00429120
Iteration 18/25 | Loss: 0.00429120
Iteration 19/25 | Loss: 0.00429120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0042911977507174015, 0.0042911977507174015, 0.0042911977507174015, 0.0042911977507174015, 0.0042911977507174015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0042911977507174015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00429120
Iteration 2/1000 | Loss: 0.00289351
Iteration 3/1000 | Loss: 0.00177897
Iteration 4/1000 | Loss: 0.00289362
Iteration 5/1000 | Loss: 0.00180377
Iteration 6/1000 | Loss: 0.00028987
Iteration 7/1000 | Loss: 0.00242206
Iteration 8/1000 | Loss: 0.00130504
Iteration 9/1000 | Loss: 0.00165103
Iteration 10/1000 | Loss: 0.00031802
Iteration 11/1000 | Loss: 0.00065076
Iteration 12/1000 | Loss: 0.00075858
Iteration 13/1000 | Loss: 0.00015940
Iteration 14/1000 | Loss: 0.00024333
Iteration 15/1000 | Loss: 0.00013078
Iteration 16/1000 | Loss: 0.00021647
Iteration 17/1000 | Loss: 0.00011757
Iteration 18/1000 | Loss: 0.00015991
Iteration 19/1000 | Loss: 0.00010645
Iteration 20/1000 | Loss: 0.00014099
Iteration 21/1000 | Loss: 0.00010958
Iteration 22/1000 | Loss: 0.00036129
Iteration 23/1000 | Loss: 0.00475789
Iteration 24/1000 | Loss: 0.00125086
Iteration 25/1000 | Loss: 0.00032947
Iteration 26/1000 | Loss: 0.00013080
Iteration 27/1000 | Loss: 0.00009954
Iteration 28/1000 | Loss: 0.00018571
Iteration 29/1000 | Loss: 0.00006542
Iteration 30/1000 | Loss: 0.00005529
Iteration 31/1000 | Loss: 0.00013243
Iteration 32/1000 | Loss: 0.00004487
Iteration 33/1000 | Loss: 0.00008916
Iteration 34/1000 | Loss: 0.00004048
Iteration 35/1000 | Loss: 0.00003888
Iteration 36/1000 | Loss: 0.00003739
Iteration 37/1000 | Loss: 0.00003659
Iteration 38/1000 | Loss: 0.00003592
Iteration 39/1000 | Loss: 0.00003514
Iteration 40/1000 | Loss: 0.00003462
Iteration 41/1000 | Loss: 0.00003429
Iteration 42/1000 | Loss: 0.00003416
Iteration 43/1000 | Loss: 0.00003392
Iteration 44/1000 | Loss: 0.00003376
Iteration 45/1000 | Loss: 0.00003362
Iteration 46/1000 | Loss: 0.00003358
Iteration 47/1000 | Loss: 0.00013445
Iteration 48/1000 | Loss: 0.00003550
Iteration 49/1000 | Loss: 0.00003370
Iteration 50/1000 | Loss: 0.00003345
Iteration 51/1000 | Loss: 0.00003340
Iteration 52/1000 | Loss: 0.00003339
Iteration 53/1000 | Loss: 0.00003338
Iteration 54/1000 | Loss: 0.00003337
Iteration 55/1000 | Loss: 0.00003337
Iteration 56/1000 | Loss: 0.00003337
Iteration 57/1000 | Loss: 0.00003336
Iteration 58/1000 | Loss: 0.00003336
Iteration 59/1000 | Loss: 0.00003336
Iteration 60/1000 | Loss: 0.00003336
Iteration 61/1000 | Loss: 0.00003336
Iteration 62/1000 | Loss: 0.00003335
Iteration 63/1000 | Loss: 0.00003335
Iteration 64/1000 | Loss: 0.00003335
Iteration 65/1000 | Loss: 0.00003334
Iteration 66/1000 | Loss: 0.00003334
Iteration 67/1000 | Loss: 0.00003334
Iteration 68/1000 | Loss: 0.00003333
Iteration 69/1000 | Loss: 0.00003333
Iteration 70/1000 | Loss: 0.00003333
Iteration 71/1000 | Loss: 0.00003333
Iteration 72/1000 | Loss: 0.00003333
Iteration 73/1000 | Loss: 0.00003333
Iteration 74/1000 | Loss: 0.00003333
Iteration 75/1000 | Loss: 0.00003332
Iteration 76/1000 | Loss: 0.00003332
Iteration 77/1000 | Loss: 0.00003332
Iteration 78/1000 | Loss: 0.00003332
Iteration 79/1000 | Loss: 0.00003332
Iteration 80/1000 | Loss: 0.00003332
Iteration 81/1000 | Loss: 0.00003332
Iteration 82/1000 | Loss: 0.00003332
Iteration 83/1000 | Loss: 0.00003330
Iteration 84/1000 | Loss: 0.00003330
Iteration 85/1000 | Loss: 0.00003330
Iteration 86/1000 | Loss: 0.00003330
Iteration 87/1000 | Loss: 0.00003330
Iteration 88/1000 | Loss: 0.00003330
Iteration 89/1000 | Loss: 0.00003330
Iteration 90/1000 | Loss: 0.00003330
Iteration 91/1000 | Loss: 0.00003330
Iteration 92/1000 | Loss: 0.00003330
Iteration 93/1000 | Loss: 0.00003330
Iteration 94/1000 | Loss: 0.00003330
Iteration 95/1000 | Loss: 0.00003329
Iteration 96/1000 | Loss: 0.00003329
Iteration 97/1000 | Loss: 0.00003329
Iteration 98/1000 | Loss: 0.00003328
Iteration 99/1000 | Loss: 0.00003328
Iteration 100/1000 | Loss: 0.00003328
Iteration 101/1000 | Loss: 0.00003327
Iteration 102/1000 | Loss: 0.00003327
Iteration 103/1000 | Loss: 0.00003325
Iteration 104/1000 | Loss: 0.00003325
Iteration 105/1000 | Loss: 0.00003325
Iteration 106/1000 | Loss: 0.00003325
Iteration 107/1000 | Loss: 0.00003325
Iteration 108/1000 | Loss: 0.00003325
Iteration 109/1000 | Loss: 0.00003325
Iteration 110/1000 | Loss: 0.00003324
Iteration 111/1000 | Loss: 0.00003324
Iteration 112/1000 | Loss: 0.00003324
Iteration 113/1000 | Loss: 0.00003324
Iteration 114/1000 | Loss: 0.00003323
Iteration 115/1000 | Loss: 0.00003323
Iteration 116/1000 | Loss: 0.00003323
Iteration 117/1000 | Loss: 0.00003323
Iteration 118/1000 | Loss: 0.00003323
Iteration 119/1000 | Loss: 0.00003323
Iteration 120/1000 | Loss: 0.00003322
Iteration 121/1000 | Loss: 0.00003322
Iteration 122/1000 | Loss: 0.00003322
Iteration 123/1000 | Loss: 0.00003322
Iteration 124/1000 | Loss: 0.00003321
Iteration 125/1000 | Loss: 0.00003321
Iteration 126/1000 | Loss: 0.00003321
Iteration 127/1000 | Loss: 0.00003321
Iteration 128/1000 | Loss: 0.00003321
Iteration 129/1000 | Loss: 0.00003321
Iteration 130/1000 | Loss: 0.00003321
Iteration 131/1000 | Loss: 0.00003320
Iteration 132/1000 | Loss: 0.00003320
Iteration 133/1000 | Loss: 0.00003320
Iteration 134/1000 | Loss: 0.00003320
Iteration 135/1000 | Loss: 0.00003320
Iteration 136/1000 | Loss: 0.00003320
Iteration 137/1000 | Loss: 0.00003320
Iteration 138/1000 | Loss: 0.00003320
Iteration 139/1000 | Loss: 0.00003320
Iteration 140/1000 | Loss: 0.00003320
Iteration 141/1000 | Loss: 0.00003320
Iteration 142/1000 | Loss: 0.00003319
Iteration 143/1000 | Loss: 0.00003319
Iteration 144/1000 | Loss: 0.00003319
Iteration 145/1000 | Loss: 0.00003319
Iteration 146/1000 | Loss: 0.00003319
Iteration 147/1000 | Loss: 0.00003319
Iteration 148/1000 | Loss: 0.00003319
Iteration 149/1000 | Loss: 0.00003319
Iteration 150/1000 | Loss: 0.00003319
Iteration 151/1000 | Loss: 0.00003318
Iteration 152/1000 | Loss: 0.00003318
Iteration 153/1000 | Loss: 0.00003318
Iteration 154/1000 | Loss: 0.00003317
Iteration 155/1000 | Loss: 0.00003316
Iteration 156/1000 | Loss: 0.00003315
Iteration 157/1000 | Loss: 0.00003315
Iteration 158/1000 | Loss: 0.00003315
Iteration 159/1000 | Loss: 0.00003314
Iteration 160/1000 | Loss: 0.00003314
Iteration 161/1000 | Loss: 0.00003314
Iteration 162/1000 | Loss: 0.00003314
Iteration 163/1000 | Loss: 0.00003314
Iteration 164/1000 | Loss: 0.00003313
Iteration 165/1000 | Loss: 0.00003313
Iteration 166/1000 | Loss: 0.00003313
Iteration 167/1000 | Loss: 0.00003312
Iteration 168/1000 | Loss: 0.00003312
Iteration 169/1000 | Loss: 0.00003310
Iteration 170/1000 | Loss: 0.00003310
Iteration 171/1000 | Loss: 0.00003310
Iteration 172/1000 | Loss: 0.00003310
Iteration 173/1000 | Loss: 0.00003310
Iteration 174/1000 | Loss: 0.00003310
Iteration 175/1000 | Loss: 0.00003310
Iteration 176/1000 | Loss: 0.00003309
Iteration 177/1000 | Loss: 0.00003309
Iteration 178/1000 | Loss: 0.00003309
Iteration 179/1000 | Loss: 0.00003309
Iteration 180/1000 | Loss: 0.00003309
Iteration 181/1000 | Loss: 0.00003309
Iteration 182/1000 | Loss: 0.00003309
Iteration 183/1000 | Loss: 0.00003309
Iteration 184/1000 | Loss: 0.00003308
Iteration 185/1000 | Loss: 0.00003308
Iteration 186/1000 | Loss: 0.00003308
Iteration 187/1000 | Loss: 0.00003308
Iteration 188/1000 | Loss: 0.00003308
Iteration 189/1000 | Loss: 0.00003308
Iteration 190/1000 | Loss: 0.00003308
Iteration 191/1000 | Loss: 0.00003308
Iteration 192/1000 | Loss: 0.00003308
Iteration 193/1000 | Loss: 0.00003308
Iteration 194/1000 | Loss: 0.00003308
Iteration 195/1000 | Loss: 0.00003308
Iteration 196/1000 | Loss: 0.00003308
Iteration 197/1000 | Loss: 0.00003308
Iteration 198/1000 | Loss: 0.00003308
Iteration 199/1000 | Loss: 0.00003308
Iteration 200/1000 | Loss: 0.00003308
Iteration 201/1000 | Loss: 0.00003308
Iteration 202/1000 | Loss: 0.00003308
Iteration 203/1000 | Loss: 0.00003308
Iteration 204/1000 | Loss: 0.00003308
Iteration 205/1000 | Loss: 0.00003308
Iteration 206/1000 | Loss: 0.00003308
Iteration 207/1000 | Loss: 0.00003308
Iteration 208/1000 | Loss: 0.00003308
Iteration 209/1000 | Loss: 0.00003308
Iteration 210/1000 | Loss: 0.00003308
Iteration 211/1000 | Loss: 0.00003308
Iteration 212/1000 | Loss: 0.00003308
Iteration 213/1000 | Loss: 0.00003308
Iteration 214/1000 | Loss: 0.00003308
Iteration 215/1000 | Loss: 0.00003308
Iteration 216/1000 | Loss: 0.00003308
Iteration 217/1000 | Loss: 0.00003308
Iteration 218/1000 | Loss: 0.00003308
Iteration 219/1000 | Loss: 0.00003308
Iteration 220/1000 | Loss: 0.00003308
Iteration 221/1000 | Loss: 0.00003308
Iteration 222/1000 | Loss: 0.00003308
Iteration 223/1000 | Loss: 0.00003308
Iteration 224/1000 | Loss: 0.00003308
Iteration 225/1000 | Loss: 0.00003308
Iteration 226/1000 | Loss: 0.00003308
Iteration 227/1000 | Loss: 0.00003308
Iteration 228/1000 | Loss: 0.00003308
Iteration 229/1000 | Loss: 0.00003308
Iteration 230/1000 | Loss: 0.00003308
Iteration 231/1000 | Loss: 0.00003308
Iteration 232/1000 | Loss: 0.00003308
Iteration 233/1000 | Loss: 0.00003308
Iteration 234/1000 | Loss: 0.00003308
Iteration 235/1000 | Loss: 0.00003308
Iteration 236/1000 | Loss: 0.00003308
Iteration 237/1000 | Loss: 0.00003308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [3.307944643893279e-05, 3.307944643893279e-05, 3.307944643893279e-05, 3.307944643893279e-05, 3.307944643893279e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.307944643893279e-05

Optimization complete. Final v2v error: 3.9933338165283203 mm

Highest mean error: 12.027019500732422 mm for frame 66

Lowest mean error: 2.907594919204712 mm for frame 139

Saving results

Total time: 121.36845874786377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384339
Iteration 2/25 | Loss: 0.00127873
Iteration 3/25 | Loss: 0.00121277
Iteration 4/25 | Loss: 0.00120418
Iteration 5/25 | Loss: 0.00120183
Iteration 6/25 | Loss: 0.00120183
Iteration 7/25 | Loss: 0.00120183
Iteration 8/25 | Loss: 0.00120183
Iteration 9/25 | Loss: 0.00120183
Iteration 10/25 | Loss: 0.00120183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012018285924568772, 0.0012018285924568772, 0.0012018285924568772, 0.0012018285924568772, 0.0012018285924568772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012018285924568772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39976251
Iteration 2/25 | Loss: 0.00104877
Iteration 3/25 | Loss: 0.00104877
Iteration 4/25 | Loss: 0.00104877
Iteration 5/25 | Loss: 0.00104877
Iteration 6/25 | Loss: 0.00104877
Iteration 7/25 | Loss: 0.00104876
Iteration 8/25 | Loss: 0.00104876
Iteration 9/25 | Loss: 0.00104876
Iteration 10/25 | Loss: 0.00104876
Iteration 11/25 | Loss: 0.00104876
Iteration 12/25 | Loss: 0.00104876
Iteration 13/25 | Loss: 0.00104876
Iteration 14/25 | Loss: 0.00104876
Iteration 15/25 | Loss: 0.00104876
Iteration 16/25 | Loss: 0.00104876
Iteration 17/25 | Loss: 0.00104876
Iteration 18/25 | Loss: 0.00104876
Iteration 19/25 | Loss: 0.00104876
Iteration 20/25 | Loss: 0.00104876
Iteration 21/25 | Loss: 0.00104876
Iteration 22/25 | Loss: 0.00104876
Iteration 23/25 | Loss: 0.00104876
Iteration 24/25 | Loss: 0.00104876
Iteration 25/25 | Loss: 0.00104876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104876
Iteration 2/1000 | Loss: 0.00002696
Iteration 3/1000 | Loss: 0.00001685
Iteration 4/1000 | Loss: 0.00001356
Iteration 5/1000 | Loss: 0.00001252
Iteration 6/1000 | Loss: 0.00001170
Iteration 7/1000 | Loss: 0.00001121
Iteration 8/1000 | Loss: 0.00001096
Iteration 9/1000 | Loss: 0.00001070
Iteration 10/1000 | Loss: 0.00001056
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001046
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001037
Iteration 15/1000 | Loss: 0.00001036
Iteration 16/1000 | Loss: 0.00001036
Iteration 17/1000 | Loss: 0.00001034
Iteration 18/1000 | Loss: 0.00001032
Iteration 19/1000 | Loss: 0.00001032
Iteration 20/1000 | Loss: 0.00001031
Iteration 21/1000 | Loss: 0.00001031
Iteration 22/1000 | Loss: 0.00001030
Iteration 23/1000 | Loss: 0.00001029
Iteration 24/1000 | Loss: 0.00001027
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001018
Iteration 28/1000 | Loss: 0.00001017
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001016
Iteration 31/1000 | Loss: 0.00001012
Iteration 32/1000 | Loss: 0.00001010
Iteration 33/1000 | Loss: 0.00001005
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001001
Iteration 36/1000 | Loss: 0.00001001
Iteration 37/1000 | Loss: 0.00001000
Iteration 38/1000 | Loss: 0.00001000
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00001000
Iteration 41/1000 | Loss: 0.00001000
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000998
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000995
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000991
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000986
Iteration 54/1000 | Loss: 0.00000985
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000984
Iteration 59/1000 | Loss: 0.00000983
Iteration 60/1000 | Loss: 0.00000983
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000983
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000982
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000982
Iteration 67/1000 | Loss: 0.00000982
Iteration 68/1000 | Loss: 0.00000981
Iteration 69/1000 | Loss: 0.00000981
Iteration 70/1000 | Loss: 0.00000981
Iteration 71/1000 | Loss: 0.00000980
Iteration 72/1000 | Loss: 0.00000980
Iteration 73/1000 | Loss: 0.00000980
Iteration 74/1000 | Loss: 0.00000980
Iteration 75/1000 | Loss: 0.00000980
Iteration 76/1000 | Loss: 0.00000979
Iteration 77/1000 | Loss: 0.00000979
Iteration 78/1000 | Loss: 0.00000979
Iteration 79/1000 | Loss: 0.00000979
Iteration 80/1000 | Loss: 0.00000979
Iteration 81/1000 | Loss: 0.00000979
Iteration 82/1000 | Loss: 0.00000979
Iteration 83/1000 | Loss: 0.00000979
Iteration 84/1000 | Loss: 0.00000979
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00000978
Iteration 87/1000 | Loss: 0.00000977
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000976
Iteration 90/1000 | Loss: 0.00000976
Iteration 91/1000 | Loss: 0.00000976
Iteration 92/1000 | Loss: 0.00000976
Iteration 93/1000 | Loss: 0.00000976
Iteration 94/1000 | Loss: 0.00000976
Iteration 95/1000 | Loss: 0.00000975
Iteration 96/1000 | Loss: 0.00000975
Iteration 97/1000 | Loss: 0.00000975
Iteration 98/1000 | Loss: 0.00000975
Iteration 99/1000 | Loss: 0.00000974
Iteration 100/1000 | Loss: 0.00000974
Iteration 101/1000 | Loss: 0.00000974
Iteration 102/1000 | Loss: 0.00000974
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000973
Iteration 107/1000 | Loss: 0.00000972
Iteration 108/1000 | Loss: 0.00000972
Iteration 109/1000 | Loss: 0.00000972
Iteration 110/1000 | Loss: 0.00000971
Iteration 111/1000 | Loss: 0.00000971
Iteration 112/1000 | Loss: 0.00000971
Iteration 113/1000 | Loss: 0.00000971
Iteration 114/1000 | Loss: 0.00000971
Iteration 115/1000 | Loss: 0.00000971
Iteration 116/1000 | Loss: 0.00000971
Iteration 117/1000 | Loss: 0.00000971
Iteration 118/1000 | Loss: 0.00000970
Iteration 119/1000 | Loss: 0.00000970
Iteration 120/1000 | Loss: 0.00000970
Iteration 121/1000 | Loss: 0.00000970
Iteration 122/1000 | Loss: 0.00000969
Iteration 123/1000 | Loss: 0.00000969
Iteration 124/1000 | Loss: 0.00000969
Iteration 125/1000 | Loss: 0.00000969
Iteration 126/1000 | Loss: 0.00000969
Iteration 127/1000 | Loss: 0.00000969
Iteration 128/1000 | Loss: 0.00000969
Iteration 129/1000 | Loss: 0.00000969
Iteration 130/1000 | Loss: 0.00000969
Iteration 131/1000 | Loss: 0.00000969
Iteration 132/1000 | Loss: 0.00000968
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000968
Iteration 137/1000 | Loss: 0.00000968
Iteration 138/1000 | Loss: 0.00000968
Iteration 139/1000 | Loss: 0.00000968
Iteration 140/1000 | Loss: 0.00000967
Iteration 141/1000 | Loss: 0.00000967
Iteration 142/1000 | Loss: 0.00000967
Iteration 143/1000 | Loss: 0.00000967
Iteration 144/1000 | Loss: 0.00000967
Iteration 145/1000 | Loss: 0.00000966
Iteration 146/1000 | Loss: 0.00000966
Iteration 147/1000 | Loss: 0.00000966
Iteration 148/1000 | Loss: 0.00000966
Iteration 149/1000 | Loss: 0.00000966
Iteration 150/1000 | Loss: 0.00000966
Iteration 151/1000 | Loss: 0.00000966
Iteration 152/1000 | Loss: 0.00000966
Iteration 153/1000 | Loss: 0.00000966
Iteration 154/1000 | Loss: 0.00000966
Iteration 155/1000 | Loss: 0.00000966
Iteration 156/1000 | Loss: 0.00000966
Iteration 157/1000 | Loss: 0.00000966
Iteration 158/1000 | Loss: 0.00000965
Iteration 159/1000 | Loss: 0.00000965
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000965
Iteration 162/1000 | Loss: 0.00000964
Iteration 163/1000 | Loss: 0.00000964
Iteration 164/1000 | Loss: 0.00000964
Iteration 165/1000 | Loss: 0.00000964
Iteration 166/1000 | Loss: 0.00000964
Iteration 167/1000 | Loss: 0.00000964
Iteration 168/1000 | Loss: 0.00000964
Iteration 169/1000 | Loss: 0.00000964
Iteration 170/1000 | Loss: 0.00000964
Iteration 171/1000 | Loss: 0.00000964
Iteration 172/1000 | Loss: 0.00000964
Iteration 173/1000 | Loss: 0.00000963
Iteration 174/1000 | Loss: 0.00000963
Iteration 175/1000 | Loss: 0.00000963
Iteration 176/1000 | Loss: 0.00000963
Iteration 177/1000 | Loss: 0.00000963
Iteration 178/1000 | Loss: 0.00000963
Iteration 179/1000 | Loss: 0.00000963
Iteration 180/1000 | Loss: 0.00000963
Iteration 181/1000 | Loss: 0.00000963
Iteration 182/1000 | Loss: 0.00000963
Iteration 183/1000 | Loss: 0.00000963
Iteration 184/1000 | Loss: 0.00000962
Iteration 185/1000 | Loss: 0.00000962
Iteration 186/1000 | Loss: 0.00000962
Iteration 187/1000 | Loss: 0.00000962
Iteration 188/1000 | Loss: 0.00000962
Iteration 189/1000 | Loss: 0.00000961
Iteration 190/1000 | Loss: 0.00000961
Iteration 191/1000 | Loss: 0.00000961
Iteration 192/1000 | Loss: 0.00000961
Iteration 193/1000 | Loss: 0.00000961
Iteration 194/1000 | Loss: 0.00000961
Iteration 195/1000 | Loss: 0.00000961
Iteration 196/1000 | Loss: 0.00000960
Iteration 197/1000 | Loss: 0.00000960
Iteration 198/1000 | Loss: 0.00000960
Iteration 199/1000 | Loss: 0.00000960
Iteration 200/1000 | Loss: 0.00000960
Iteration 201/1000 | Loss: 0.00000960
Iteration 202/1000 | Loss: 0.00000959
Iteration 203/1000 | Loss: 0.00000959
Iteration 204/1000 | Loss: 0.00000959
Iteration 205/1000 | Loss: 0.00000959
Iteration 206/1000 | Loss: 0.00000959
Iteration 207/1000 | Loss: 0.00000959
Iteration 208/1000 | Loss: 0.00000959
Iteration 209/1000 | Loss: 0.00000959
Iteration 210/1000 | Loss: 0.00000959
Iteration 211/1000 | Loss: 0.00000959
Iteration 212/1000 | Loss: 0.00000958
Iteration 213/1000 | Loss: 0.00000958
Iteration 214/1000 | Loss: 0.00000958
Iteration 215/1000 | Loss: 0.00000958
Iteration 216/1000 | Loss: 0.00000958
Iteration 217/1000 | Loss: 0.00000958
Iteration 218/1000 | Loss: 0.00000958
Iteration 219/1000 | Loss: 0.00000958
Iteration 220/1000 | Loss: 0.00000958
Iteration 221/1000 | Loss: 0.00000958
Iteration 222/1000 | Loss: 0.00000958
Iteration 223/1000 | Loss: 0.00000958
Iteration 224/1000 | Loss: 0.00000958
Iteration 225/1000 | Loss: 0.00000958
Iteration 226/1000 | Loss: 0.00000958
Iteration 227/1000 | Loss: 0.00000958
Iteration 228/1000 | Loss: 0.00000957
Iteration 229/1000 | Loss: 0.00000957
Iteration 230/1000 | Loss: 0.00000957
Iteration 231/1000 | Loss: 0.00000957
Iteration 232/1000 | Loss: 0.00000957
Iteration 233/1000 | Loss: 0.00000957
Iteration 234/1000 | Loss: 0.00000957
Iteration 235/1000 | Loss: 0.00000957
Iteration 236/1000 | Loss: 0.00000957
Iteration 237/1000 | Loss: 0.00000957
Iteration 238/1000 | Loss: 0.00000957
Iteration 239/1000 | Loss: 0.00000957
Iteration 240/1000 | Loss: 0.00000957
Iteration 241/1000 | Loss: 0.00000956
Iteration 242/1000 | Loss: 0.00000956
Iteration 243/1000 | Loss: 0.00000956
Iteration 244/1000 | Loss: 0.00000956
Iteration 245/1000 | Loss: 0.00000956
Iteration 246/1000 | Loss: 0.00000956
Iteration 247/1000 | Loss: 0.00000956
Iteration 248/1000 | Loss: 0.00000956
Iteration 249/1000 | Loss: 0.00000956
Iteration 250/1000 | Loss: 0.00000956
Iteration 251/1000 | Loss: 0.00000956
Iteration 252/1000 | Loss: 0.00000956
Iteration 253/1000 | Loss: 0.00000955
Iteration 254/1000 | Loss: 0.00000955
Iteration 255/1000 | Loss: 0.00000955
Iteration 256/1000 | Loss: 0.00000955
Iteration 257/1000 | Loss: 0.00000955
Iteration 258/1000 | Loss: 0.00000955
Iteration 259/1000 | Loss: 0.00000954
Iteration 260/1000 | Loss: 0.00000954
Iteration 261/1000 | Loss: 0.00000954
Iteration 262/1000 | Loss: 0.00000954
Iteration 263/1000 | Loss: 0.00000954
Iteration 264/1000 | Loss: 0.00000954
Iteration 265/1000 | Loss: 0.00000953
Iteration 266/1000 | Loss: 0.00000953
Iteration 267/1000 | Loss: 0.00000953
Iteration 268/1000 | Loss: 0.00000952
Iteration 269/1000 | Loss: 0.00000952
Iteration 270/1000 | Loss: 0.00000952
Iteration 271/1000 | Loss: 0.00000952
Iteration 272/1000 | Loss: 0.00000952
Iteration 273/1000 | Loss: 0.00000952
Iteration 274/1000 | Loss: 0.00000952
Iteration 275/1000 | Loss: 0.00000952
Iteration 276/1000 | Loss: 0.00000952
Iteration 277/1000 | Loss: 0.00000952
Iteration 278/1000 | Loss: 0.00000952
Iteration 279/1000 | Loss: 0.00000952
Iteration 280/1000 | Loss: 0.00000952
Iteration 281/1000 | Loss: 0.00000952
Iteration 282/1000 | Loss: 0.00000952
Iteration 283/1000 | Loss: 0.00000952
Iteration 284/1000 | Loss: 0.00000952
Iteration 285/1000 | Loss: 0.00000951
Iteration 286/1000 | Loss: 0.00000951
Iteration 287/1000 | Loss: 0.00000951
Iteration 288/1000 | Loss: 0.00000951
Iteration 289/1000 | Loss: 0.00000951
Iteration 290/1000 | Loss: 0.00000951
Iteration 291/1000 | Loss: 0.00000951
Iteration 292/1000 | Loss: 0.00000951
Iteration 293/1000 | Loss: 0.00000951
Iteration 294/1000 | Loss: 0.00000951
Iteration 295/1000 | Loss: 0.00000950
Iteration 296/1000 | Loss: 0.00000950
Iteration 297/1000 | Loss: 0.00000950
Iteration 298/1000 | Loss: 0.00000950
Iteration 299/1000 | Loss: 0.00000950
Iteration 300/1000 | Loss: 0.00000950
Iteration 301/1000 | Loss: 0.00000950
Iteration 302/1000 | Loss: 0.00000950
Iteration 303/1000 | Loss: 0.00000950
Iteration 304/1000 | Loss: 0.00000950
Iteration 305/1000 | Loss: 0.00000950
Iteration 306/1000 | Loss: 0.00000950
Iteration 307/1000 | Loss: 0.00000950
Iteration 308/1000 | Loss: 0.00000950
Iteration 309/1000 | Loss: 0.00000950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [9.502421562501695e-06, 9.502421562501695e-06, 9.502421562501695e-06, 9.502421562501695e-06, 9.502421562501695e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.502421562501695e-06

Optimization complete. Final v2v error: 2.6455934047698975 mm

Highest mean error: 3.1491189002990723 mm for frame 80

Lowest mean error: 2.519395351409912 mm for frame 9

Saving results

Total time: 49.283421754837036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011171
Iteration 2/25 | Loss: 0.00259968
Iteration 3/25 | Loss: 0.00219298
Iteration 4/25 | Loss: 0.00210388
Iteration 5/25 | Loss: 0.00189563
Iteration 6/25 | Loss: 0.00157675
Iteration 7/25 | Loss: 0.00149676
Iteration 8/25 | Loss: 0.00142538
Iteration 9/25 | Loss: 0.00140951
Iteration 10/25 | Loss: 0.00139187
Iteration 11/25 | Loss: 0.00138265
Iteration 12/25 | Loss: 0.00138170
Iteration 13/25 | Loss: 0.00137999
Iteration 14/25 | Loss: 0.00138233
Iteration 15/25 | Loss: 0.00138234
Iteration 16/25 | Loss: 0.00137709
Iteration 17/25 | Loss: 0.00136892
Iteration 18/25 | Loss: 0.00136632
Iteration 19/25 | Loss: 0.00136522
Iteration 20/25 | Loss: 0.00136701
Iteration 21/25 | Loss: 0.00136580
Iteration 22/25 | Loss: 0.00136604
Iteration 23/25 | Loss: 0.00136652
Iteration 24/25 | Loss: 0.00136311
Iteration 25/25 | Loss: 0.00136262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38518131
Iteration 2/25 | Loss: 0.00089711
Iteration 3/25 | Loss: 0.00089711
Iteration 4/25 | Loss: 0.00089711
Iteration 5/25 | Loss: 0.00089711
Iteration 6/25 | Loss: 0.00089711
Iteration 7/25 | Loss: 0.00089711
Iteration 8/25 | Loss: 0.00089711
Iteration 9/25 | Loss: 0.00089711
Iteration 10/25 | Loss: 0.00089711
Iteration 11/25 | Loss: 0.00089711
Iteration 12/25 | Loss: 0.00089711
Iteration 13/25 | Loss: 0.00089711
Iteration 14/25 | Loss: 0.00089711
Iteration 15/25 | Loss: 0.00089711
Iteration 16/25 | Loss: 0.00089711
Iteration 17/25 | Loss: 0.00089711
Iteration 18/25 | Loss: 0.00089711
Iteration 19/25 | Loss: 0.00089711
Iteration 20/25 | Loss: 0.00089711
Iteration 21/25 | Loss: 0.00089711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008971101487986743, 0.0008971101487986743, 0.0008971101487986743, 0.0008971101487986743, 0.0008971101487986743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008971101487986743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089711
Iteration 2/1000 | Loss: 0.00004830
Iteration 3/1000 | Loss: 0.00003595
Iteration 4/1000 | Loss: 0.00003263
Iteration 5/1000 | Loss: 0.00003118
Iteration 6/1000 | Loss: 0.00003032
Iteration 7/1000 | Loss: 0.00002965
Iteration 8/1000 | Loss: 0.00002922
Iteration 9/1000 | Loss: 0.00002884
Iteration 10/1000 | Loss: 0.00002854
Iteration 11/1000 | Loss: 0.00014707
Iteration 12/1000 | Loss: 0.00002995
Iteration 13/1000 | Loss: 0.00002850
Iteration 14/1000 | Loss: 0.00002760
Iteration 15/1000 | Loss: 0.00002689
Iteration 16/1000 | Loss: 0.00002653
Iteration 17/1000 | Loss: 0.00002638
Iteration 18/1000 | Loss: 0.00002630
Iteration 19/1000 | Loss: 0.00002619
Iteration 20/1000 | Loss: 0.00002616
Iteration 21/1000 | Loss: 0.00002614
Iteration 22/1000 | Loss: 0.00002612
Iteration 23/1000 | Loss: 0.00002612
Iteration 24/1000 | Loss: 0.00002610
Iteration 25/1000 | Loss: 0.00002604
Iteration 26/1000 | Loss: 0.00002595
Iteration 27/1000 | Loss: 0.00002594
Iteration 28/1000 | Loss: 0.00002594
Iteration 29/1000 | Loss: 0.00002593
Iteration 30/1000 | Loss: 0.00002593
Iteration 31/1000 | Loss: 0.00002593
Iteration 32/1000 | Loss: 0.00002589
Iteration 33/1000 | Loss: 0.00002589
Iteration 34/1000 | Loss: 0.00002589
Iteration 35/1000 | Loss: 0.00002588
Iteration 36/1000 | Loss: 0.00002588
Iteration 37/1000 | Loss: 0.00002588
Iteration 38/1000 | Loss: 0.00002587
Iteration 39/1000 | Loss: 0.00002587
Iteration 40/1000 | Loss: 0.00002587
Iteration 41/1000 | Loss: 0.00002587
Iteration 42/1000 | Loss: 0.00002587
Iteration 43/1000 | Loss: 0.00002587
Iteration 44/1000 | Loss: 0.00002587
Iteration 45/1000 | Loss: 0.00002587
Iteration 46/1000 | Loss: 0.00002587
Iteration 47/1000 | Loss: 0.00002587
Iteration 48/1000 | Loss: 0.00002586
Iteration 49/1000 | Loss: 0.00002586
Iteration 50/1000 | Loss: 0.00012706
Iteration 51/1000 | Loss: 0.00002942
Iteration 52/1000 | Loss: 0.00002739
Iteration 53/1000 | Loss: 0.00002671
Iteration 54/1000 | Loss: 0.00002638
Iteration 55/1000 | Loss: 0.00002624
Iteration 56/1000 | Loss: 0.00002621
Iteration 57/1000 | Loss: 0.00002616
Iteration 58/1000 | Loss: 0.00002615
Iteration 59/1000 | Loss: 0.00002609
Iteration 60/1000 | Loss: 0.00002609
Iteration 61/1000 | Loss: 0.00002608
Iteration 62/1000 | Loss: 0.00002608
Iteration 63/1000 | Loss: 0.00002608
Iteration 64/1000 | Loss: 0.00002607
Iteration 65/1000 | Loss: 0.00002607
Iteration 66/1000 | Loss: 0.00002607
Iteration 67/1000 | Loss: 0.00002607
Iteration 68/1000 | Loss: 0.00002607
Iteration 69/1000 | Loss: 0.00002607
Iteration 70/1000 | Loss: 0.00002607
Iteration 71/1000 | Loss: 0.00018272
Iteration 72/1000 | Loss: 0.00008161
Iteration 73/1000 | Loss: 0.00002634
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002608
Iteration 76/1000 | Loss: 0.00002606
Iteration 77/1000 | Loss: 0.00002606
Iteration 78/1000 | Loss: 0.00002606
Iteration 79/1000 | Loss: 0.00002606
Iteration 80/1000 | Loss: 0.00002605
Iteration 81/1000 | Loss: 0.00002605
Iteration 82/1000 | Loss: 0.00002605
Iteration 83/1000 | Loss: 0.00002605
Iteration 84/1000 | Loss: 0.00002605
Iteration 85/1000 | Loss: 0.00002605
Iteration 86/1000 | Loss: 0.00002605
Iteration 87/1000 | Loss: 0.00002605
Iteration 88/1000 | Loss: 0.00002605
Iteration 89/1000 | Loss: 0.00002605
Iteration 90/1000 | Loss: 0.00002605
Iteration 91/1000 | Loss: 0.00002605
Iteration 92/1000 | Loss: 0.00002604
Iteration 93/1000 | Loss: 0.00002604
Iteration 94/1000 | Loss: 0.00002604
Iteration 95/1000 | Loss: 0.00002604
Iteration 96/1000 | Loss: 0.00002604
Iteration 97/1000 | Loss: 0.00002604
Iteration 98/1000 | Loss: 0.00002604
Iteration 99/1000 | Loss: 0.00018490
Iteration 100/1000 | Loss: 0.00024129
Iteration 101/1000 | Loss: 0.00014845
Iteration 102/1000 | Loss: 0.00002646
Iteration 103/1000 | Loss: 0.00002602
Iteration 104/1000 | Loss: 0.00002602
Iteration 105/1000 | Loss: 0.00002602
Iteration 106/1000 | Loss: 0.00002602
Iteration 107/1000 | Loss: 0.00002602
Iteration 108/1000 | Loss: 0.00002602
Iteration 109/1000 | Loss: 0.00002602
Iteration 110/1000 | Loss: 0.00002602
Iteration 111/1000 | Loss: 0.00002602
Iteration 112/1000 | Loss: 0.00002602
Iteration 113/1000 | Loss: 0.00002602
Iteration 114/1000 | Loss: 0.00002602
Iteration 115/1000 | Loss: 0.00002602
Iteration 116/1000 | Loss: 0.00002602
Iteration 117/1000 | Loss: 0.00002602
Iteration 118/1000 | Loss: 0.00002602
Iteration 119/1000 | Loss: 0.00002602
Iteration 120/1000 | Loss: 0.00002602
Iteration 121/1000 | Loss: 0.00002602
Iteration 122/1000 | Loss: 0.00002602
Iteration 123/1000 | Loss: 0.00002602
Iteration 124/1000 | Loss: 0.00002602
Iteration 125/1000 | Loss: 0.00002602
Iteration 126/1000 | Loss: 0.00002602
Iteration 127/1000 | Loss: 0.00002602
Iteration 128/1000 | Loss: 0.00002602
Iteration 129/1000 | Loss: 0.00002602
Iteration 130/1000 | Loss: 0.00002602
Iteration 131/1000 | Loss: 0.00002602
Iteration 132/1000 | Loss: 0.00002602
Iteration 133/1000 | Loss: 0.00002602
Iteration 134/1000 | Loss: 0.00002602
Iteration 135/1000 | Loss: 0.00002602
Iteration 136/1000 | Loss: 0.00002602
Iteration 137/1000 | Loss: 0.00002602
Iteration 138/1000 | Loss: 0.00002602
Iteration 139/1000 | Loss: 0.00002602
Iteration 140/1000 | Loss: 0.00002602
Iteration 141/1000 | Loss: 0.00002602
Iteration 142/1000 | Loss: 0.00002602
Iteration 143/1000 | Loss: 0.00002602
Iteration 144/1000 | Loss: 0.00002602
Iteration 145/1000 | Loss: 0.00002602
Iteration 146/1000 | Loss: 0.00002602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.6015864932560362e-05, 2.6015864932560362e-05, 2.6015864932560362e-05, 2.6015864932560362e-05, 2.6015864932560362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6015864932560362e-05

Optimization complete. Final v2v error: 4.337103843688965 mm

Highest mean error: 7.469180583953857 mm for frame 3

Lowest mean error: 3.7938013076782227 mm for frame 6

Saving results

Total time: 99.30746746063232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501400
Iteration 2/25 | Loss: 0.00135145
Iteration 3/25 | Loss: 0.00127694
Iteration 4/25 | Loss: 0.00126870
Iteration 5/25 | Loss: 0.00126795
Iteration 6/25 | Loss: 0.00126795
Iteration 7/25 | Loss: 0.00126795
Iteration 8/25 | Loss: 0.00126795
Iteration 9/25 | Loss: 0.00126795
Iteration 10/25 | Loss: 0.00126795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012679535429924726, 0.0012679535429924726, 0.0012679535429924726, 0.0012679535429924726, 0.0012679535429924726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012679535429924726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34355319
Iteration 2/25 | Loss: 0.00085872
Iteration 3/25 | Loss: 0.00085871
Iteration 4/25 | Loss: 0.00085870
Iteration 5/25 | Loss: 0.00085870
Iteration 6/25 | Loss: 0.00085870
Iteration 7/25 | Loss: 0.00085870
Iteration 8/25 | Loss: 0.00085870
Iteration 9/25 | Loss: 0.00085870
Iteration 10/25 | Loss: 0.00085870
Iteration 11/25 | Loss: 0.00085870
Iteration 12/25 | Loss: 0.00085870
Iteration 13/25 | Loss: 0.00085870
Iteration 14/25 | Loss: 0.00085870
Iteration 15/25 | Loss: 0.00085870
Iteration 16/25 | Loss: 0.00085870
Iteration 17/25 | Loss: 0.00085870
Iteration 18/25 | Loss: 0.00085870
Iteration 19/25 | Loss: 0.00085870
Iteration 20/25 | Loss: 0.00085870
Iteration 21/25 | Loss: 0.00085870
Iteration 22/25 | Loss: 0.00085870
Iteration 23/25 | Loss: 0.00085870
Iteration 24/25 | Loss: 0.00085870
Iteration 25/25 | Loss: 0.00085870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008587019983679056, 0.0008587019983679056, 0.0008587019983679056, 0.0008587019983679056, 0.0008587019983679056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008587019983679056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085870
Iteration 2/1000 | Loss: 0.00002417
Iteration 3/1000 | Loss: 0.00001806
Iteration 4/1000 | Loss: 0.00001667
Iteration 5/1000 | Loss: 0.00001586
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001465
Iteration 9/1000 | Loss: 0.00001440
Iteration 10/1000 | Loss: 0.00001423
Iteration 11/1000 | Loss: 0.00001419
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001416
Iteration 14/1000 | Loss: 0.00001409
Iteration 15/1000 | Loss: 0.00001398
Iteration 16/1000 | Loss: 0.00001397
Iteration 17/1000 | Loss: 0.00001396
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001370
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001363
Iteration 32/1000 | Loss: 0.00001363
Iteration 33/1000 | Loss: 0.00001362
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001361
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001357
Iteration 40/1000 | Loss: 0.00001356
Iteration 41/1000 | Loss: 0.00001356
Iteration 42/1000 | Loss: 0.00001355
Iteration 43/1000 | Loss: 0.00001355
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001345
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001328
Iteration 103/1000 | Loss: 0.00001328
Iteration 104/1000 | Loss: 0.00001328
Iteration 105/1000 | Loss: 0.00001328
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001325
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001324
Iteration 126/1000 | Loss: 0.00001324
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001323
Iteration 142/1000 | Loss: 0.00001323
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001323
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3227863746578805e-05, 1.3227863746578805e-05, 1.3227863746578805e-05, 1.3227863746578805e-05, 1.3227863746578805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3227863746578805e-05

Optimization complete. Final v2v error: 3.0576186180114746 mm

Highest mean error: 3.2664549350738525 mm for frame 173

Lowest mean error: 2.7706096172332764 mm for frame 224

Saving results

Total time: 42.76861834526062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164351
Iteration 2/25 | Loss: 0.00385579
Iteration 3/25 | Loss: 0.00253967
Iteration 4/25 | Loss: 0.00232699
Iteration 5/25 | Loss: 0.00207145
Iteration 6/25 | Loss: 0.00181881
Iteration 7/25 | Loss: 0.00172393
Iteration 8/25 | Loss: 0.00172837
Iteration 9/25 | Loss: 0.00167963
Iteration 10/25 | Loss: 0.00162624
Iteration 11/25 | Loss: 0.00158633
Iteration 12/25 | Loss: 0.00156637
Iteration 13/25 | Loss: 0.00156247
Iteration 14/25 | Loss: 0.00156134
Iteration 15/25 | Loss: 0.00155843
Iteration 16/25 | Loss: 0.00155760
Iteration 17/25 | Loss: 0.00155705
Iteration 18/25 | Loss: 0.00156094
Iteration 19/25 | Loss: 0.00154786
Iteration 20/25 | Loss: 0.00154642
Iteration 21/25 | Loss: 0.00154601
Iteration 22/25 | Loss: 0.00154586
Iteration 23/25 | Loss: 0.00154584
Iteration 24/25 | Loss: 0.00154583
Iteration 25/25 | Loss: 0.00154583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64512074
Iteration 2/25 | Loss: 0.00172662
Iteration 3/25 | Loss: 0.00172662
Iteration 4/25 | Loss: 0.00172661
Iteration 5/25 | Loss: 0.00172661
Iteration 6/25 | Loss: 0.00172661
Iteration 7/25 | Loss: 0.00172661
Iteration 8/25 | Loss: 0.00172661
Iteration 9/25 | Loss: 0.00172661
Iteration 10/25 | Loss: 0.00172661
Iteration 11/25 | Loss: 0.00172661
Iteration 12/25 | Loss: 0.00172661
Iteration 13/25 | Loss: 0.00172661
Iteration 14/25 | Loss: 0.00172661
Iteration 15/25 | Loss: 0.00172661
Iteration 16/25 | Loss: 0.00172661
Iteration 17/25 | Loss: 0.00172661
Iteration 18/25 | Loss: 0.00172661
Iteration 19/25 | Loss: 0.00172661
Iteration 20/25 | Loss: 0.00172661
Iteration 21/25 | Loss: 0.00172661
Iteration 22/25 | Loss: 0.00172661
Iteration 23/25 | Loss: 0.00172661
Iteration 24/25 | Loss: 0.00172661
Iteration 25/25 | Loss: 0.00172661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172661
Iteration 2/1000 | Loss: 0.00016301
Iteration 3/1000 | Loss: 0.00009242
Iteration 4/1000 | Loss: 0.00007624
Iteration 5/1000 | Loss: 0.00007118
Iteration 6/1000 | Loss: 0.00006816
Iteration 7/1000 | Loss: 0.00006635
Iteration 8/1000 | Loss: 0.00006476
Iteration 9/1000 | Loss: 0.00006337
Iteration 10/1000 | Loss: 0.00006225
Iteration 11/1000 | Loss: 0.00006068
Iteration 12/1000 | Loss: 0.00005759
Iteration 13/1000 | Loss: 0.00005538
Iteration 14/1000 | Loss: 0.00065874
Iteration 15/1000 | Loss: 0.00007972
Iteration 16/1000 | Loss: 0.00005393
Iteration 17/1000 | Loss: 0.00004623
Iteration 18/1000 | Loss: 0.00004374
Iteration 19/1000 | Loss: 0.00004261
Iteration 20/1000 | Loss: 0.00004217
Iteration 21/1000 | Loss: 0.00004182
Iteration 22/1000 | Loss: 0.00004152
Iteration 23/1000 | Loss: 0.00004133
Iteration 24/1000 | Loss: 0.00004131
Iteration 25/1000 | Loss: 0.00004126
Iteration 26/1000 | Loss: 0.00004123
Iteration 27/1000 | Loss: 0.00004122
Iteration 28/1000 | Loss: 0.00004121
Iteration 29/1000 | Loss: 0.00004121
Iteration 30/1000 | Loss: 0.00004120
Iteration 31/1000 | Loss: 0.00004120
Iteration 32/1000 | Loss: 0.00004120
Iteration 33/1000 | Loss: 0.00004120
Iteration 34/1000 | Loss: 0.00004120
Iteration 35/1000 | Loss: 0.00004120
Iteration 36/1000 | Loss: 0.00004120
Iteration 37/1000 | Loss: 0.00004120
Iteration 38/1000 | Loss: 0.00004120
Iteration 39/1000 | Loss: 0.00004120
Iteration 40/1000 | Loss: 0.00004120
Iteration 41/1000 | Loss: 0.00004120
Iteration 42/1000 | Loss: 0.00004119
Iteration 43/1000 | Loss: 0.00004119
Iteration 44/1000 | Loss: 0.00004119
Iteration 45/1000 | Loss: 0.00004119
Iteration 46/1000 | Loss: 0.00004118
Iteration 47/1000 | Loss: 0.00004118
Iteration 48/1000 | Loss: 0.00004118
Iteration 49/1000 | Loss: 0.00004118
Iteration 50/1000 | Loss: 0.00004118
Iteration 51/1000 | Loss: 0.00004117
Iteration 52/1000 | Loss: 0.00004117
Iteration 53/1000 | Loss: 0.00004117
Iteration 54/1000 | Loss: 0.00004117
Iteration 55/1000 | Loss: 0.00004117
Iteration 56/1000 | Loss: 0.00004116
Iteration 57/1000 | Loss: 0.00004116
Iteration 58/1000 | Loss: 0.00004115
Iteration 59/1000 | Loss: 0.00004115
Iteration 60/1000 | Loss: 0.00004115
Iteration 61/1000 | Loss: 0.00004114
Iteration 62/1000 | Loss: 0.00004114
Iteration 63/1000 | Loss: 0.00004114
Iteration 64/1000 | Loss: 0.00004114
Iteration 65/1000 | Loss: 0.00004114
Iteration 66/1000 | Loss: 0.00004114
Iteration 67/1000 | Loss: 0.00004114
Iteration 68/1000 | Loss: 0.00004113
Iteration 69/1000 | Loss: 0.00004113
Iteration 70/1000 | Loss: 0.00004113
Iteration 71/1000 | Loss: 0.00004113
Iteration 72/1000 | Loss: 0.00004113
Iteration 73/1000 | Loss: 0.00004112
Iteration 74/1000 | Loss: 0.00004112
Iteration 75/1000 | Loss: 0.00004112
Iteration 76/1000 | Loss: 0.00004112
Iteration 77/1000 | Loss: 0.00004112
Iteration 78/1000 | Loss: 0.00004112
Iteration 79/1000 | Loss: 0.00004111
Iteration 80/1000 | Loss: 0.00004111
Iteration 81/1000 | Loss: 0.00004111
Iteration 82/1000 | Loss: 0.00004111
Iteration 83/1000 | Loss: 0.00004111
Iteration 84/1000 | Loss: 0.00004111
Iteration 85/1000 | Loss: 0.00004110
Iteration 86/1000 | Loss: 0.00004110
Iteration 87/1000 | Loss: 0.00004110
Iteration 88/1000 | Loss: 0.00004110
Iteration 89/1000 | Loss: 0.00004110
Iteration 90/1000 | Loss: 0.00004110
Iteration 91/1000 | Loss: 0.00004110
Iteration 92/1000 | Loss: 0.00004110
Iteration 93/1000 | Loss: 0.00004110
Iteration 94/1000 | Loss: 0.00004110
Iteration 95/1000 | Loss: 0.00004110
Iteration 96/1000 | Loss: 0.00004110
Iteration 97/1000 | Loss: 0.00004110
Iteration 98/1000 | Loss: 0.00004110
Iteration 99/1000 | Loss: 0.00004110
Iteration 100/1000 | Loss: 0.00004110
Iteration 101/1000 | Loss: 0.00004110
Iteration 102/1000 | Loss: 0.00004110
Iteration 103/1000 | Loss: 0.00004110
Iteration 104/1000 | Loss: 0.00004110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [4.110186637262814e-05, 4.110186637262814e-05, 4.110186637262814e-05, 4.110186637262814e-05, 4.110186637262814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.110186637262814e-05

Optimization complete. Final v2v error: 5.394705772399902 mm

Highest mean error: 5.877277851104736 mm for frame 4

Lowest mean error: 4.706592082977295 mm for frame 20

Saving results

Total time: 75.02242612838745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426231
Iteration 2/25 | Loss: 0.00135022
Iteration 3/25 | Loss: 0.00124748
Iteration 4/25 | Loss: 0.00123432
Iteration 5/25 | Loss: 0.00123082
Iteration 6/25 | Loss: 0.00123000
Iteration 7/25 | Loss: 0.00123000
Iteration 8/25 | Loss: 0.00123000
Iteration 9/25 | Loss: 0.00123000
Iteration 10/25 | Loss: 0.00123000
Iteration 11/25 | Loss: 0.00123000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012300036614760756, 0.0012300036614760756, 0.0012300036614760756, 0.0012300036614760756, 0.0012300036614760756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012300036614760756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32551193
Iteration 2/25 | Loss: 0.00111898
Iteration 3/25 | Loss: 0.00111897
Iteration 4/25 | Loss: 0.00111897
Iteration 5/25 | Loss: 0.00111897
Iteration 6/25 | Loss: 0.00111897
Iteration 7/25 | Loss: 0.00111897
Iteration 8/25 | Loss: 0.00111897
Iteration 9/25 | Loss: 0.00111897
Iteration 10/25 | Loss: 0.00111897
Iteration 11/25 | Loss: 0.00111897
Iteration 12/25 | Loss: 0.00111897
Iteration 13/25 | Loss: 0.00111897
Iteration 14/25 | Loss: 0.00111897
Iteration 15/25 | Loss: 0.00111897
Iteration 16/25 | Loss: 0.00111897
Iteration 17/25 | Loss: 0.00111897
Iteration 18/25 | Loss: 0.00111897
Iteration 19/25 | Loss: 0.00111897
Iteration 20/25 | Loss: 0.00111897
Iteration 21/25 | Loss: 0.00111897
Iteration 22/25 | Loss: 0.00111897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001118966145440936, 0.001118966145440936, 0.001118966145440936, 0.001118966145440936, 0.001118966145440936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118966145440936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111897
Iteration 2/1000 | Loss: 0.00003068
Iteration 3/1000 | Loss: 0.00002004
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001520
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001298
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001244
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001225
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001213
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00001212
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001211
Iteration 34/1000 | Loss: 0.00001210
Iteration 35/1000 | Loss: 0.00001210
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001209
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001209
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001208
Iteration 44/1000 | Loss: 0.00001208
Iteration 45/1000 | Loss: 0.00001208
Iteration 46/1000 | Loss: 0.00001207
Iteration 47/1000 | Loss: 0.00001207
Iteration 48/1000 | Loss: 0.00001207
Iteration 49/1000 | Loss: 0.00001205
Iteration 50/1000 | Loss: 0.00001205
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001203
Iteration 59/1000 | Loss: 0.00001203
Iteration 60/1000 | Loss: 0.00001203
Iteration 61/1000 | Loss: 0.00001202
Iteration 62/1000 | Loss: 0.00001202
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001189
Iteration 105/1000 | Loss: 0.00001189
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001188
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001187
Iteration 114/1000 | Loss: 0.00001187
Iteration 115/1000 | Loss: 0.00001187
Iteration 116/1000 | Loss: 0.00001187
Iteration 117/1000 | Loss: 0.00001187
Iteration 118/1000 | Loss: 0.00001187
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001184
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001184
Iteration 134/1000 | Loss: 0.00001184
Iteration 135/1000 | Loss: 0.00001184
Iteration 136/1000 | Loss: 0.00001184
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001183
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001182
Iteration 146/1000 | Loss: 0.00001182
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001182
Iteration 149/1000 | Loss: 0.00001181
Iteration 150/1000 | Loss: 0.00001181
Iteration 151/1000 | Loss: 0.00001181
Iteration 152/1000 | Loss: 0.00001181
Iteration 153/1000 | Loss: 0.00001181
Iteration 154/1000 | Loss: 0.00001181
Iteration 155/1000 | Loss: 0.00001181
Iteration 156/1000 | Loss: 0.00001180
Iteration 157/1000 | Loss: 0.00001180
Iteration 158/1000 | Loss: 0.00001180
Iteration 159/1000 | Loss: 0.00001180
Iteration 160/1000 | Loss: 0.00001180
Iteration 161/1000 | Loss: 0.00001179
Iteration 162/1000 | Loss: 0.00001179
Iteration 163/1000 | Loss: 0.00001179
Iteration 164/1000 | Loss: 0.00001179
Iteration 165/1000 | Loss: 0.00001179
Iteration 166/1000 | Loss: 0.00001179
Iteration 167/1000 | Loss: 0.00001179
Iteration 168/1000 | Loss: 0.00001179
Iteration 169/1000 | Loss: 0.00001179
Iteration 170/1000 | Loss: 0.00001179
Iteration 171/1000 | Loss: 0.00001179
Iteration 172/1000 | Loss: 0.00001179
Iteration 173/1000 | Loss: 0.00001179
Iteration 174/1000 | Loss: 0.00001179
Iteration 175/1000 | Loss: 0.00001179
Iteration 176/1000 | Loss: 0.00001179
Iteration 177/1000 | Loss: 0.00001179
Iteration 178/1000 | Loss: 0.00001179
Iteration 179/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1794645615736954e-05, 1.1794645615736954e-05, 1.1794645615736954e-05, 1.1794645615736954e-05, 1.1794645615736954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1794645615736954e-05

Optimization complete. Final v2v error: 2.9407684803009033 mm

Highest mean error: 3.917093276977539 mm for frame 56

Lowest mean error: 2.6603572368621826 mm for frame 35

Saving results

Total time: 44.42852544784546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951420
Iteration 2/25 | Loss: 0.00951419
Iteration 3/25 | Loss: 0.00951419
Iteration 4/25 | Loss: 0.00951419
Iteration 5/25 | Loss: 0.00951419
Iteration 6/25 | Loss: 0.00951419
Iteration 7/25 | Loss: 0.00951419
Iteration 8/25 | Loss: 0.00951419
Iteration 9/25 | Loss: 0.00951419
Iteration 10/25 | Loss: 0.00951419
Iteration 11/25 | Loss: 0.00951418
Iteration 12/25 | Loss: 0.00951418
Iteration 13/25 | Loss: 0.00951418
Iteration 14/25 | Loss: 0.00951418
Iteration 15/25 | Loss: 0.00951418
Iteration 16/25 | Loss: 0.00951418
Iteration 17/25 | Loss: 0.00951418
Iteration 18/25 | Loss: 0.00951418
Iteration 19/25 | Loss: 0.00951417
Iteration 20/25 | Loss: 0.00951417
Iteration 21/25 | Loss: 0.00951417
Iteration 22/25 | Loss: 0.00951417
Iteration 23/25 | Loss: 0.00951417
Iteration 24/25 | Loss: 0.00951417
Iteration 25/25 | Loss: 0.00951417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72386611
Iteration 2/25 | Loss: 0.18171780
Iteration 3/25 | Loss: 0.17907539
Iteration 4/25 | Loss: 0.17850693
Iteration 5/25 | Loss: 0.17807503
Iteration 6/25 | Loss: 0.17803599
Iteration 7/25 | Loss: 0.17799924
Iteration 8/25 | Loss: 0.17793076
Iteration 9/25 | Loss: 0.17793073
Iteration 10/25 | Loss: 0.17793073
Iteration 11/25 | Loss: 0.17793068
Iteration 12/25 | Loss: 0.17793068
Iteration 13/25 | Loss: 0.17793068
Iteration 14/25 | Loss: 0.17793068
Iteration 15/25 | Loss: 0.17793068
Iteration 16/25 | Loss: 0.17793068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.17793068289756775, 0.17793068289756775, 0.17793068289756775, 0.17793068289756775, 0.17793068289756775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17793068289756775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17793068
Iteration 2/1000 | Loss: 0.01837295
Iteration 3/1000 | Loss: 0.00202813
Iteration 4/1000 | Loss: 0.00093299
Iteration 5/1000 | Loss: 0.00064420
Iteration 6/1000 | Loss: 0.00074311
Iteration 7/1000 | Loss: 0.00027311
Iteration 8/1000 | Loss: 0.00017594
Iteration 9/1000 | Loss: 0.00014358
Iteration 10/1000 | Loss: 0.00046919
Iteration 11/1000 | Loss: 0.00012792
Iteration 12/1000 | Loss: 0.00010849
Iteration 13/1000 | Loss: 0.00008967
Iteration 14/1000 | Loss: 0.00007988
Iteration 15/1000 | Loss: 0.00006918
Iteration 16/1000 | Loss: 0.00006121
Iteration 17/1000 | Loss: 0.00005286
Iteration 18/1000 | Loss: 0.00004523
Iteration 19/1000 | Loss: 0.00006467
Iteration 20/1000 | Loss: 0.00003750
Iteration 21/1000 | Loss: 0.00007945
Iteration 22/1000 | Loss: 0.00003354
Iteration 23/1000 | Loss: 0.00003152
Iteration 24/1000 | Loss: 0.00003003
Iteration 25/1000 | Loss: 0.00002911
Iteration 26/1000 | Loss: 0.00002838
Iteration 27/1000 | Loss: 0.00002780
Iteration 28/1000 | Loss: 0.00002733
Iteration 29/1000 | Loss: 0.00002679
Iteration 30/1000 | Loss: 0.00002636
Iteration 31/1000 | Loss: 0.00002599
Iteration 32/1000 | Loss: 0.00002571
Iteration 33/1000 | Loss: 0.00002551
Iteration 34/1000 | Loss: 0.00002533
Iteration 35/1000 | Loss: 0.00002525
Iteration 36/1000 | Loss: 0.00002519
Iteration 37/1000 | Loss: 0.00002513
Iteration 38/1000 | Loss: 0.00002513
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002509
Iteration 41/1000 | Loss: 0.00002504
Iteration 42/1000 | Loss: 0.00002502
Iteration 43/1000 | Loss: 0.00002501
Iteration 44/1000 | Loss: 0.00002497
Iteration 45/1000 | Loss: 0.00002497
Iteration 46/1000 | Loss: 0.00002492
Iteration 47/1000 | Loss: 0.00002492
Iteration 48/1000 | Loss: 0.00002491
Iteration 49/1000 | Loss: 0.00002491
Iteration 50/1000 | Loss: 0.00002486
Iteration 51/1000 | Loss: 0.00002486
Iteration 52/1000 | Loss: 0.00002483
Iteration 53/1000 | Loss: 0.00002483
Iteration 54/1000 | Loss: 0.00002482
Iteration 55/1000 | Loss: 0.00002482
Iteration 56/1000 | Loss: 0.00002482
Iteration 57/1000 | Loss: 0.00002482
Iteration 58/1000 | Loss: 0.00002482
Iteration 59/1000 | Loss: 0.00002482
Iteration 60/1000 | Loss: 0.00002482
Iteration 61/1000 | Loss: 0.00002481
Iteration 62/1000 | Loss: 0.00002481
Iteration 63/1000 | Loss: 0.00002481
Iteration 64/1000 | Loss: 0.00002480
Iteration 65/1000 | Loss: 0.00002480
Iteration 66/1000 | Loss: 0.00002480
Iteration 67/1000 | Loss: 0.00002480
Iteration 68/1000 | Loss: 0.00002480
Iteration 69/1000 | Loss: 0.00002480
Iteration 70/1000 | Loss: 0.00002479
Iteration 71/1000 | Loss: 0.00002479
Iteration 72/1000 | Loss: 0.00002478
Iteration 73/1000 | Loss: 0.00002477
Iteration 74/1000 | Loss: 0.00002477
Iteration 75/1000 | Loss: 0.00002477
Iteration 76/1000 | Loss: 0.00002476
Iteration 77/1000 | Loss: 0.00002476
Iteration 78/1000 | Loss: 0.00002476
Iteration 79/1000 | Loss: 0.00002475
Iteration 80/1000 | Loss: 0.00002475
Iteration 81/1000 | Loss: 0.00002475
Iteration 82/1000 | Loss: 0.00002475
Iteration 83/1000 | Loss: 0.00002475
Iteration 84/1000 | Loss: 0.00002475
Iteration 85/1000 | Loss: 0.00002475
Iteration 86/1000 | Loss: 0.00002474
Iteration 87/1000 | Loss: 0.00002474
Iteration 88/1000 | Loss: 0.00002474
Iteration 89/1000 | Loss: 0.00002474
Iteration 90/1000 | Loss: 0.00002474
Iteration 91/1000 | Loss: 0.00002473
Iteration 92/1000 | Loss: 0.00002473
Iteration 93/1000 | Loss: 0.00002473
Iteration 94/1000 | Loss: 0.00002473
Iteration 95/1000 | Loss: 0.00002473
Iteration 96/1000 | Loss: 0.00002473
Iteration 97/1000 | Loss: 0.00002472
Iteration 98/1000 | Loss: 0.00002472
Iteration 99/1000 | Loss: 0.00002472
Iteration 100/1000 | Loss: 0.00002471
Iteration 101/1000 | Loss: 0.00002471
Iteration 102/1000 | Loss: 0.00002470
Iteration 103/1000 | Loss: 0.00002470
Iteration 104/1000 | Loss: 0.00002470
Iteration 105/1000 | Loss: 0.00002470
Iteration 106/1000 | Loss: 0.00002469
Iteration 107/1000 | Loss: 0.00002469
Iteration 108/1000 | Loss: 0.00002469
Iteration 109/1000 | Loss: 0.00002469
Iteration 110/1000 | Loss: 0.00002469
Iteration 111/1000 | Loss: 0.00002469
Iteration 112/1000 | Loss: 0.00002469
Iteration 113/1000 | Loss: 0.00002469
Iteration 114/1000 | Loss: 0.00002469
Iteration 115/1000 | Loss: 0.00002469
Iteration 116/1000 | Loss: 0.00002468
Iteration 117/1000 | Loss: 0.00002468
Iteration 118/1000 | Loss: 0.00002468
Iteration 119/1000 | Loss: 0.00002468
Iteration 120/1000 | Loss: 0.00002468
Iteration 121/1000 | Loss: 0.00002468
Iteration 122/1000 | Loss: 0.00002468
Iteration 123/1000 | Loss: 0.00002468
Iteration 124/1000 | Loss: 0.00002468
Iteration 125/1000 | Loss: 0.00002468
Iteration 126/1000 | Loss: 0.00002468
Iteration 127/1000 | Loss: 0.00002467
Iteration 128/1000 | Loss: 0.00002467
Iteration 129/1000 | Loss: 0.00002467
Iteration 130/1000 | Loss: 0.00002467
Iteration 131/1000 | Loss: 0.00002467
Iteration 132/1000 | Loss: 0.00002467
Iteration 133/1000 | Loss: 0.00002467
Iteration 134/1000 | Loss: 0.00002466
Iteration 135/1000 | Loss: 0.00002466
Iteration 136/1000 | Loss: 0.00002465
Iteration 137/1000 | Loss: 0.00002465
Iteration 138/1000 | Loss: 0.00002465
Iteration 139/1000 | Loss: 0.00002465
Iteration 140/1000 | Loss: 0.00002465
Iteration 141/1000 | Loss: 0.00002465
Iteration 142/1000 | Loss: 0.00002465
Iteration 143/1000 | Loss: 0.00002465
Iteration 144/1000 | Loss: 0.00002465
Iteration 145/1000 | Loss: 0.00002465
Iteration 146/1000 | Loss: 0.00002465
Iteration 147/1000 | Loss: 0.00002465
Iteration 148/1000 | Loss: 0.00002465
Iteration 149/1000 | Loss: 0.00002465
Iteration 150/1000 | Loss: 0.00002465
Iteration 151/1000 | Loss: 0.00002465
Iteration 152/1000 | Loss: 0.00002465
Iteration 153/1000 | Loss: 0.00002465
Iteration 154/1000 | Loss: 0.00002465
Iteration 155/1000 | Loss: 0.00002465
Iteration 156/1000 | Loss: 0.00002465
Iteration 157/1000 | Loss: 0.00002465
Iteration 158/1000 | Loss: 0.00002465
Iteration 159/1000 | Loss: 0.00002465
Iteration 160/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.4647653845022433e-05, 2.4647653845022433e-05, 2.4647653845022433e-05, 2.4647653845022433e-05, 2.4647653845022433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4647653845022433e-05

Optimization complete. Final v2v error: 4.29941463470459 mm

Highest mean error: 4.585963249206543 mm for frame 126

Lowest mean error: 3.681882619857788 mm for frame 37

Saving results

Total time: 83.45872211456299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997754
Iteration 2/25 | Loss: 0.00218398
Iteration 3/25 | Loss: 0.00182282
Iteration 4/25 | Loss: 0.00154602
Iteration 5/25 | Loss: 0.00151265
Iteration 6/25 | Loss: 0.00149990
Iteration 7/25 | Loss: 0.00146717
Iteration 8/25 | Loss: 0.00143554
Iteration 9/25 | Loss: 0.00142562
Iteration 10/25 | Loss: 0.00142525
Iteration 11/25 | Loss: 0.00142960
Iteration 12/25 | Loss: 0.00141385
Iteration 13/25 | Loss: 0.00140153
Iteration 14/25 | Loss: 0.00140450
Iteration 15/25 | Loss: 0.00137924
Iteration 16/25 | Loss: 0.00137747
Iteration 17/25 | Loss: 0.00136903
Iteration 18/25 | Loss: 0.00136860
Iteration 19/25 | Loss: 0.00138291
Iteration 20/25 | Loss: 0.00138739
Iteration 21/25 | Loss: 0.00137215
Iteration 22/25 | Loss: 0.00136710
Iteration 23/25 | Loss: 0.00136817
Iteration 24/25 | Loss: 0.00136804
Iteration 25/25 | Loss: 0.00137541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34243929
Iteration 2/25 | Loss: 0.00209620
Iteration 3/25 | Loss: 0.00198851
Iteration 4/25 | Loss: 0.00198849
Iteration 5/25 | Loss: 0.00198849
Iteration 6/25 | Loss: 0.00198848
Iteration 7/25 | Loss: 0.00198848
Iteration 8/25 | Loss: 0.00198848
Iteration 9/25 | Loss: 0.00198848
Iteration 10/25 | Loss: 0.00198848
Iteration 11/25 | Loss: 0.00198848
Iteration 12/25 | Loss: 0.00198848
Iteration 13/25 | Loss: 0.00198848
Iteration 14/25 | Loss: 0.00198848
Iteration 15/25 | Loss: 0.00198848
Iteration 16/25 | Loss: 0.00198848
Iteration 17/25 | Loss: 0.00198848
Iteration 18/25 | Loss: 0.00198848
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019884821958839893, 0.0019884821958839893, 0.0019884821958839893, 0.0019884821958839893, 0.0019884821958839893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019884821958839893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198848
Iteration 2/1000 | Loss: 0.00057440
Iteration 3/1000 | Loss: 0.00050650
Iteration 4/1000 | Loss: 0.00065323
Iteration 5/1000 | Loss: 0.00024228
Iteration 6/1000 | Loss: 0.00029185
Iteration 7/1000 | Loss: 0.00075749
Iteration 8/1000 | Loss: 0.00058318
Iteration 9/1000 | Loss: 0.00019845
Iteration 10/1000 | Loss: 0.00012512
Iteration 11/1000 | Loss: 0.00064277
Iteration 12/1000 | Loss: 0.00070102
Iteration 13/1000 | Loss: 0.00024583
Iteration 14/1000 | Loss: 0.00049220
Iteration 15/1000 | Loss: 0.00029131
Iteration 16/1000 | Loss: 0.00009929
Iteration 17/1000 | Loss: 0.00036314
Iteration 18/1000 | Loss: 0.00061461
Iteration 19/1000 | Loss: 0.00034801
Iteration 20/1000 | Loss: 0.00106882
Iteration 21/1000 | Loss: 0.00050214
Iteration 22/1000 | Loss: 0.00038652
Iteration 23/1000 | Loss: 0.00041783
Iteration 24/1000 | Loss: 0.00039498
Iteration 25/1000 | Loss: 0.00011887
Iteration 26/1000 | Loss: 0.00010415
Iteration 27/1000 | Loss: 0.00009411
Iteration 28/1000 | Loss: 0.00045208
Iteration 29/1000 | Loss: 0.00018719
Iteration 30/1000 | Loss: 0.00029804
Iteration 31/1000 | Loss: 0.00035562
Iteration 32/1000 | Loss: 0.00008922
Iteration 33/1000 | Loss: 0.00074697
Iteration 34/1000 | Loss: 0.00043903
Iteration 35/1000 | Loss: 0.00101085
Iteration 36/1000 | Loss: 0.00051547
Iteration 37/1000 | Loss: 0.00094713
Iteration 38/1000 | Loss: 0.00065682
Iteration 39/1000 | Loss: 0.00055185
Iteration 40/1000 | Loss: 0.00009783
Iteration 41/1000 | Loss: 0.00032351
Iteration 42/1000 | Loss: 0.00036893
Iteration 43/1000 | Loss: 0.00008844
Iteration 44/1000 | Loss: 0.00008409
Iteration 45/1000 | Loss: 0.00009530
Iteration 46/1000 | Loss: 0.00046441
Iteration 47/1000 | Loss: 0.00046779
Iteration 48/1000 | Loss: 0.00075435
Iteration 49/1000 | Loss: 0.00078782
Iteration 50/1000 | Loss: 0.00035178
Iteration 51/1000 | Loss: 0.00077098
Iteration 52/1000 | Loss: 0.00034487
Iteration 53/1000 | Loss: 0.00101801
Iteration 54/1000 | Loss: 0.00133413
Iteration 55/1000 | Loss: 0.00017803
Iteration 56/1000 | Loss: 0.00025732
Iteration 57/1000 | Loss: 0.00088411
Iteration 58/1000 | Loss: 0.00028377
Iteration 59/1000 | Loss: 0.00061249
Iteration 60/1000 | Loss: 0.00082429
Iteration 61/1000 | Loss: 0.00240073
Iteration 62/1000 | Loss: 0.00357173
Iteration 63/1000 | Loss: 0.00270828
Iteration 64/1000 | Loss: 0.00013040
Iteration 65/1000 | Loss: 0.00134157
Iteration 66/1000 | Loss: 0.00107607
Iteration 67/1000 | Loss: 0.00007700
Iteration 68/1000 | Loss: 0.00086215
Iteration 69/1000 | Loss: 0.00031082
Iteration 70/1000 | Loss: 0.00004168
Iteration 71/1000 | Loss: 0.00127641
Iteration 72/1000 | Loss: 0.00025773
Iteration 73/1000 | Loss: 0.00019098
Iteration 74/1000 | Loss: 0.00007798
Iteration 75/1000 | Loss: 0.00002860
Iteration 76/1000 | Loss: 0.00048368
Iteration 77/1000 | Loss: 0.00002953
Iteration 78/1000 | Loss: 0.00002411
Iteration 79/1000 | Loss: 0.00002912
Iteration 80/1000 | Loss: 0.00002101
Iteration 81/1000 | Loss: 0.00053530
Iteration 82/1000 | Loss: 0.00003411
Iteration 83/1000 | Loss: 0.00051447
Iteration 84/1000 | Loss: 0.00028725
Iteration 85/1000 | Loss: 0.00067402
Iteration 86/1000 | Loss: 0.00022899
Iteration 87/1000 | Loss: 0.00020432
Iteration 88/1000 | Loss: 0.00018253
Iteration 89/1000 | Loss: 0.00019578
Iteration 90/1000 | Loss: 0.00029178
Iteration 91/1000 | Loss: 0.00022713
Iteration 92/1000 | Loss: 0.00016582
Iteration 93/1000 | Loss: 0.00019811
Iteration 94/1000 | Loss: 0.00014035
Iteration 95/1000 | Loss: 0.00018300
Iteration 96/1000 | Loss: 0.00006088
Iteration 97/1000 | Loss: 0.00011434
Iteration 98/1000 | Loss: 0.00015367
Iteration 99/1000 | Loss: 0.00014927
Iteration 100/1000 | Loss: 0.00038297
Iteration 101/1000 | Loss: 0.00020705
Iteration 102/1000 | Loss: 0.00048232
Iteration 103/1000 | Loss: 0.00023471
Iteration 104/1000 | Loss: 0.00005328
Iteration 105/1000 | Loss: 0.00003438
Iteration 106/1000 | Loss: 0.00006341
Iteration 107/1000 | Loss: 0.00100668
Iteration 108/1000 | Loss: 0.00017482
Iteration 109/1000 | Loss: 0.00023917
Iteration 110/1000 | Loss: 0.00025801
Iteration 111/1000 | Loss: 0.00002908
Iteration 112/1000 | Loss: 0.00002490
Iteration 113/1000 | Loss: 0.00006078
Iteration 114/1000 | Loss: 0.00006383
Iteration 115/1000 | Loss: 0.00005724
Iteration 116/1000 | Loss: 0.00003039
Iteration 117/1000 | Loss: 0.00006557
Iteration 118/1000 | Loss: 0.00008653
Iteration 119/1000 | Loss: 0.00008146
Iteration 120/1000 | Loss: 0.00002825
Iteration 121/1000 | Loss: 0.00002311
Iteration 122/1000 | Loss: 0.00002146
Iteration 123/1000 | Loss: 0.00002109
Iteration 124/1000 | Loss: 0.00002068
Iteration 125/1000 | Loss: 0.00002038
Iteration 126/1000 | Loss: 0.00002075
Iteration 127/1000 | Loss: 0.00043771
Iteration 128/1000 | Loss: 0.00060666
Iteration 129/1000 | Loss: 0.00025609
Iteration 130/1000 | Loss: 0.00025541
Iteration 131/1000 | Loss: 0.00092986
Iteration 132/1000 | Loss: 0.00084816
Iteration 133/1000 | Loss: 0.00077313
Iteration 134/1000 | Loss: 0.00048783
Iteration 135/1000 | Loss: 0.00002674
Iteration 136/1000 | Loss: 0.00004436
Iteration 137/1000 | Loss: 0.00002246
Iteration 138/1000 | Loss: 0.00001988
Iteration 139/1000 | Loss: 0.00001849
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001522
Iteration 142/1000 | Loss: 0.00001447
Iteration 143/1000 | Loss: 0.00001371
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001275
Iteration 148/1000 | Loss: 0.00001272
Iteration 149/1000 | Loss: 0.00001272
Iteration 150/1000 | Loss: 0.00001271
Iteration 151/1000 | Loss: 0.00001268
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001260
Iteration 154/1000 | Loss: 0.00001251
Iteration 155/1000 | Loss: 0.00001250
Iteration 156/1000 | Loss: 0.00001247
Iteration 157/1000 | Loss: 0.00001244
Iteration 158/1000 | Loss: 0.00001239
Iteration 159/1000 | Loss: 0.00001239
Iteration 160/1000 | Loss: 0.00001238
Iteration 161/1000 | Loss: 0.00001238
Iteration 162/1000 | Loss: 0.00001237
Iteration 163/1000 | Loss: 0.00001237
Iteration 164/1000 | Loss: 0.00001236
Iteration 165/1000 | Loss: 0.00001236
Iteration 166/1000 | Loss: 0.00001235
Iteration 167/1000 | Loss: 0.00001235
Iteration 168/1000 | Loss: 0.00001235
Iteration 169/1000 | Loss: 0.00001235
Iteration 170/1000 | Loss: 0.00001235
Iteration 171/1000 | Loss: 0.00001235
Iteration 172/1000 | Loss: 0.00001235
Iteration 173/1000 | Loss: 0.00001235
Iteration 174/1000 | Loss: 0.00001235
Iteration 175/1000 | Loss: 0.00001235
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Iteration 180/1000 | Loss: 0.00001234
Iteration 181/1000 | Loss: 0.00001234
Iteration 182/1000 | Loss: 0.00001234
Iteration 183/1000 | Loss: 0.00001234
Iteration 184/1000 | Loss: 0.00001234
Iteration 185/1000 | Loss: 0.00001234
Iteration 186/1000 | Loss: 0.00001234
Iteration 187/1000 | Loss: 0.00001234
Iteration 188/1000 | Loss: 0.00001233
Iteration 189/1000 | Loss: 0.00001233
Iteration 190/1000 | Loss: 0.00001233
Iteration 191/1000 | Loss: 0.00001233
Iteration 192/1000 | Loss: 0.00001232
Iteration 193/1000 | Loss: 0.00001232
Iteration 194/1000 | Loss: 0.00001232
Iteration 195/1000 | Loss: 0.00001232
Iteration 196/1000 | Loss: 0.00001232
Iteration 197/1000 | Loss: 0.00001231
Iteration 198/1000 | Loss: 0.00001231
Iteration 199/1000 | Loss: 0.00001231
Iteration 200/1000 | Loss: 0.00001231
Iteration 201/1000 | Loss: 0.00001231
Iteration 202/1000 | Loss: 0.00001231
Iteration 203/1000 | Loss: 0.00001230
Iteration 204/1000 | Loss: 0.00001230
Iteration 205/1000 | Loss: 0.00001230
Iteration 206/1000 | Loss: 0.00001230
Iteration 207/1000 | Loss: 0.00001230
Iteration 208/1000 | Loss: 0.00001230
Iteration 209/1000 | Loss: 0.00001230
Iteration 210/1000 | Loss: 0.00001230
Iteration 211/1000 | Loss: 0.00001230
Iteration 212/1000 | Loss: 0.00001230
Iteration 213/1000 | Loss: 0.00001230
Iteration 214/1000 | Loss: 0.00001230
Iteration 215/1000 | Loss: 0.00001230
Iteration 216/1000 | Loss: 0.00001229
Iteration 217/1000 | Loss: 0.00001229
Iteration 218/1000 | Loss: 0.00001229
Iteration 219/1000 | Loss: 0.00001229
Iteration 220/1000 | Loss: 0.00001229
Iteration 221/1000 | Loss: 0.00001229
Iteration 222/1000 | Loss: 0.00001229
Iteration 223/1000 | Loss: 0.00001229
Iteration 224/1000 | Loss: 0.00001229
Iteration 225/1000 | Loss: 0.00001229
Iteration 226/1000 | Loss: 0.00001229
Iteration 227/1000 | Loss: 0.00001228
Iteration 228/1000 | Loss: 0.00001228
Iteration 229/1000 | Loss: 0.00001228
Iteration 230/1000 | Loss: 0.00001228
Iteration 231/1000 | Loss: 0.00001228
Iteration 232/1000 | Loss: 0.00001228
Iteration 233/1000 | Loss: 0.00001228
Iteration 234/1000 | Loss: 0.00001228
Iteration 235/1000 | Loss: 0.00001228
Iteration 236/1000 | Loss: 0.00001228
Iteration 237/1000 | Loss: 0.00001228
Iteration 238/1000 | Loss: 0.00001228
Iteration 239/1000 | Loss: 0.00001228
Iteration 240/1000 | Loss: 0.00001228
Iteration 241/1000 | Loss: 0.00001228
Iteration 242/1000 | Loss: 0.00001228
Iteration 243/1000 | Loss: 0.00001227
Iteration 244/1000 | Loss: 0.00001227
Iteration 245/1000 | Loss: 0.00001227
Iteration 246/1000 | Loss: 0.00001227
Iteration 247/1000 | Loss: 0.00001227
Iteration 248/1000 | Loss: 0.00001227
Iteration 249/1000 | Loss: 0.00001227
Iteration 250/1000 | Loss: 0.00001227
Iteration 251/1000 | Loss: 0.00001227
Iteration 252/1000 | Loss: 0.00001227
Iteration 253/1000 | Loss: 0.00001227
Iteration 254/1000 | Loss: 0.00001227
Iteration 255/1000 | Loss: 0.00001227
Iteration 256/1000 | Loss: 0.00001227
Iteration 257/1000 | Loss: 0.00001227
Iteration 258/1000 | Loss: 0.00001227
Iteration 259/1000 | Loss: 0.00001227
Iteration 260/1000 | Loss: 0.00001227
Iteration 261/1000 | Loss: 0.00001227
Iteration 262/1000 | Loss: 0.00001227
Iteration 263/1000 | Loss: 0.00001227
Iteration 264/1000 | Loss: 0.00001227
Iteration 265/1000 | Loss: 0.00001227
Iteration 266/1000 | Loss: 0.00001227
Iteration 267/1000 | Loss: 0.00001227
Iteration 268/1000 | Loss: 0.00001227
Iteration 269/1000 | Loss: 0.00001227
Iteration 270/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.226848780788714e-05, 1.226848780788714e-05, 1.226848780788714e-05, 1.226848780788714e-05, 1.226848780788714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.226848780788714e-05

Optimization complete. Final v2v error: 2.906564474105835 mm

Highest mean error: 5.101443767547607 mm for frame 55

Lowest mean error: 2.5653328895568848 mm for frame 31

Saving results

Total time: 253.91174054145813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483064
Iteration 2/25 | Loss: 0.00158865
Iteration 3/25 | Loss: 0.00130284
Iteration 4/25 | Loss: 0.00127664
Iteration 5/25 | Loss: 0.00127409
Iteration 6/25 | Loss: 0.00127409
Iteration 7/25 | Loss: 0.00127409
Iteration 8/25 | Loss: 0.00127409
Iteration 9/25 | Loss: 0.00127409
Iteration 10/25 | Loss: 0.00127409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012740908423438668, 0.0012740908423438668, 0.0012740908423438668, 0.0012740908423438668, 0.0012740908423438668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012740908423438668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39085770
Iteration 2/25 | Loss: 0.00081622
Iteration 3/25 | Loss: 0.00081622
Iteration 4/25 | Loss: 0.00081622
Iteration 5/25 | Loss: 0.00081622
Iteration 6/25 | Loss: 0.00081622
Iteration 7/25 | Loss: 0.00081622
Iteration 8/25 | Loss: 0.00081622
Iteration 9/25 | Loss: 0.00081622
Iteration 10/25 | Loss: 0.00081622
Iteration 11/25 | Loss: 0.00081622
Iteration 12/25 | Loss: 0.00081622
Iteration 13/25 | Loss: 0.00081622
Iteration 14/25 | Loss: 0.00081622
Iteration 15/25 | Loss: 0.00081622
Iteration 16/25 | Loss: 0.00081622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008162164012901485, 0.0008162164012901485, 0.0008162164012901485, 0.0008162164012901485, 0.0008162164012901485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008162164012901485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081622
Iteration 2/1000 | Loss: 0.00003210
Iteration 3/1000 | Loss: 0.00002300
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00001999
Iteration 6/1000 | Loss: 0.00001911
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001814
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001715
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001657
Iteration 15/1000 | Loss: 0.00001653
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001645
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001641
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001638
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001637
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001629
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001624
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001621
Iteration 83/1000 | Loss: 0.00001621
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001615
Iteration 101/1000 | Loss: 0.00001615
Iteration 102/1000 | Loss: 0.00001615
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00001614
Iteration 106/1000 | Loss: 0.00001614
Iteration 107/1000 | Loss: 0.00001614
Iteration 108/1000 | Loss: 0.00001614
Iteration 109/1000 | Loss: 0.00001614
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001613
Iteration 114/1000 | Loss: 0.00001613
Iteration 115/1000 | Loss: 0.00001613
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001613
Iteration 118/1000 | Loss: 0.00001612
Iteration 119/1000 | Loss: 0.00001612
Iteration 120/1000 | Loss: 0.00001612
Iteration 121/1000 | Loss: 0.00001612
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001611
Iteration 124/1000 | Loss: 0.00001611
Iteration 125/1000 | Loss: 0.00001611
Iteration 126/1000 | Loss: 0.00001611
Iteration 127/1000 | Loss: 0.00001611
Iteration 128/1000 | Loss: 0.00001611
Iteration 129/1000 | Loss: 0.00001611
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001610
Iteration 133/1000 | Loss: 0.00001610
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001609
Iteration 138/1000 | Loss: 0.00001609
Iteration 139/1000 | Loss: 0.00001609
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001608
Iteration 142/1000 | Loss: 0.00001608
Iteration 143/1000 | Loss: 0.00001607
Iteration 144/1000 | Loss: 0.00001607
Iteration 145/1000 | Loss: 0.00001607
Iteration 146/1000 | Loss: 0.00001607
Iteration 147/1000 | Loss: 0.00001607
Iteration 148/1000 | Loss: 0.00001607
Iteration 149/1000 | Loss: 0.00001607
Iteration 150/1000 | Loss: 0.00001606
Iteration 151/1000 | Loss: 0.00001606
Iteration 152/1000 | Loss: 0.00001606
Iteration 153/1000 | Loss: 0.00001606
Iteration 154/1000 | Loss: 0.00001605
Iteration 155/1000 | Loss: 0.00001605
Iteration 156/1000 | Loss: 0.00001605
Iteration 157/1000 | Loss: 0.00001605
Iteration 158/1000 | Loss: 0.00001605
Iteration 159/1000 | Loss: 0.00001605
Iteration 160/1000 | Loss: 0.00001605
Iteration 161/1000 | Loss: 0.00001605
Iteration 162/1000 | Loss: 0.00001605
Iteration 163/1000 | Loss: 0.00001605
Iteration 164/1000 | Loss: 0.00001604
Iteration 165/1000 | Loss: 0.00001604
Iteration 166/1000 | Loss: 0.00001604
Iteration 167/1000 | Loss: 0.00001604
Iteration 168/1000 | Loss: 0.00001604
Iteration 169/1000 | Loss: 0.00001604
Iteration 170/1000 | Loss: 0.00001604
Iteration 171/1000 | Loss: 0.00001604
Iteration 172/1000 | Loss: 0.00001604
Iteration 173/1000 | Loss: 0.00001604
Iteration 174/1000 | Loss: 0.00001604
Iteration 175/1000 | Loss: 0.00001603
Iteration 176/1000 | Loss: 0.00001603
Iteration 177/1000 | Loss: 0.00001603
Iteration 178/1000 | Loss: 0.00001603
Iteration 179/1000 | Loss: 0.00001603
Iteration 180/1000 | Loss: 0.00001603
Iteration 181/1000 | Loss: 0.00001603
Iteration 182/1000 | Loss: 0.00001603
Iteration 183/1000 | Loss: 0.00001603
Iteration 184/1000 | Loss: 0.00001603
Iteration 185/1000 | Loss: 0.00001603
Iteration 186/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.6027352103265002e-05, 1.6027352103265002e-05, 1.6027352103265002e-05, 1.6027352103265002e-05, 1.6027352103265002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6027352103265002e-05

Optimization complete. Final v2v error: 3.3722705841064453 mm

Highest mean error: 3.8721883296966553 mm for frame 133

Lowest mean error: 3.005401611328125 mm for frame 4

Saving results

Total time: 47.947542905807495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426625
Iteration 2/25 | Loss: 0.00142155
Iteration 3/25 | Loss: 0.00127972
Iteration 4/25 | Loss: 0.00126147
Iteration 5/25 | Loss: 0.00125777
Iteration 6/25 | Loss: 0.00125679
Iteration 7/25 | Loss: 0.00125668
Iteration 8/25 | Loss: 0.00125668
Iteration 9/25 | Loss: 0.00125668
Iteration 10/25 | Loss: 0.00125668
Iteration 11/25 | Loss: 0.00125668
Iteration 12/25 | Loss: 0.00125668
Iteration 13/25 | Loss: 0.00125668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012566811637952924, 0.0012566811637952924, 0.0012566811637952924, 0.0012566811637952924, 0.0012566811637952924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012566811637952924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41600275
Iteration 2/25 | Loss: 0.00114350
Iteration 3/25 | Loss: 0.00114350
Iteration 4/25 | Loss: 0.00114350
Iteration 5/25 | Loss: 0.00114350
Iteration 6/25 | Loss: 0.00114350
Iteration 7/25 | Loss: 0.00114350
Iteration 8/25 | Loss: 0.00114350
Iteration 9/25 | Loss: 0.00114349
Iteration 10/25 | Loss: 0.00114349
Iteration 11/25 | Loss: 0.00114349
Iteration 12/25 | Loss: 0.00114349
Iteration 13/25 | Loss: 0.00114349
Iteration 14/25 | Loss: 0.00114349
Iteration 15/25 | Loss: 0.00114349
Iteration 16/25 | Loss: 0.00114349
Iteration 17/25 | Loss: 0.00114349
Iteration 18/25 | Loss: 0.00114349
Iteration 19/25 | Loss: 0.00114349
Iteration 20/25 | Loss: 0.00114349
Iteration 21/25 | Loss: 0.00114349
Iteration 22/25 | Loss: 0.00114349
Iteration 23/25 | Loss: 0.00114349
Iteration 24/25 | Loss: 0.00114349
Iteration 25/25 | Loss: 0.00114349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114349
Iteration 2/1000 | Loss: 0.00003603
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00001782
Iteration 5/1000 | Loss: 0.00001657
Iteration 6/1000 | Loss: 0.00001562
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001464
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001394
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001361
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001350
Iteration 20/1000 | Loss: 0.00001350
Iteration 21/1000 | Loss: 0.00001349
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001348
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001338
Iteration 33/1000 | Loss: 0.00001338
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001333
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001331
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001329
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001328
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001326
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001325
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001324
Iteration 67/1000 | Loss: 0.00001324
Iteration 68/1000 | Loss: 0.00001323
Iteration 69/1000 | Loss: 0.00001323
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001322
Iteration 72/1000 | Loss: 0.00001322
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001318
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001317
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001313
Iteration 108/1000 | Loss: 0.00001313
Iteration 109/1000 | Loss: 0.00001313
Iteration 110/1000 | Loss: 0.00001313
Iteration 111/1000 | Loss: 0.00001313
Iteration 112/1000 | Loss: 0.00001313
Iteration 113/1000 | Loss: 0.00001313
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001311
Iteration 121/1000 | Loss: 0.00001311
Iteration 122/1000 | Loss: 0.00001311
Iteration 123/1000 | Loss: 0.00001311
Iteration 124/1000 | Loss: 0.00001311
Iteration 125/1000 | Loss: 0.00001310
Iteration 126/1000 | Loss: 0.00001310
Iteration 127/1000 | Loss: 0.00001310
Iteration 128/1000 | Loss: 0.00001310
Iteration 129/1000 | Loss: 0.00001309
Iteration 130/1000 | Loss: 0.00001309
Iteration 131/1000 | Loss: 0.00001309
Iteration 132/1000 | Loss: 0.00001309
Iteration 133/1000 | Loss: 0.00001309
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00001309
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001308
Iteration 138/1000 | Loss: 0.00001308
Iteration 139/1000 | Loss: 0.00001308
Iteration 140/1000 | Loss: 0.00001308
Iteration 141/1000 | Loss: 0.00001308
Iteration 142/1000 | Loss: 0.00001308
Iteration 143/1000 | Loss: 0.00001308
Iteration 144/1000 | Loss: 0.00001308
Iteration 145/1000 | Loss: 0.00001308
Iteration 146/1000 | Loss: 0.00001308
Iteration 147/1000 | Loss: 0.00001308
Iteration 148/1000 | Loss: 0.00001308
Iteration 149/1000 | Loss: 0.00001308
Iteration 150/1000 | Loss: 0.00001308
Iteration 151/1000 | Loss: 0.00001308
Iteration 152/1000 | Loss: 0.00001308
Iteration 153/1000 | Loss: 0.00001308
Iteration 154/1000 | Loss: 0.00001307
Iteration 155/1000 | Loss: 0.00001307
Iteration 156/1000 | Loss: 0.00001307
Iteration 157/1000 | Loss: 0.00001307
Iteration 158/1000 | Loss: 0.00001307
Iteration 159/1000 | Loss: 0.00001307
Iteration 160/1000 | Loss: 0.00001307
Iteration 161/1000 | Loss: 0.00001307
Iteration 162/1000 | Loss: 0.00001307
Iteration 163/1000 | Loss: 0.00001307
Iteration 164/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3074099115328863e-05, 1.3074099115328863e-05, 1.3074099115328863e-05, 1.3074099115328863e-05, 1.3074099115328863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3074099115328863e-05

Optimization complete. Final v2v error: 3.067007064819336 mm

Highest mean error: 4.599704742431641 mm for frame 55

Lowest mean error: 2.78053617477417 mm for frame 33

Saving results

Total time: 40.643192768096924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780316
Iteration 2/25 | Loss: 0.00218740
Iteration 3/25 | Loss: 0.00179491
Iteration 4/25 | Loss: 0.00169664
Iteration 5/25 | Loss: 0.00160616
Iteration 6/25 | Loss: 0.00157467
Iteration 7/25 | Loss: 0.00154305
Iteration 8/25 | Loss: 0.00148654
Iteration 9/25 | Loss: 0.00148750
Iteration 10/25 | Loss: 0.00148132
Iteration 11/25 | Loss: 0.00148585
Iteration 12/25 | Loss: 0.00146425
Iteration 13/25 | Loss: 0.00145363
Iteration 14/25 | Loss: 0.00141983
Iteration 15/25 | Loss: 0.00140554
Iteration 16/25 | Loss: 0.00141413
Iteration 17/25 | Loss: 0.00141203
Iteration 18/25 | Loss: 0.00140363
Iteration 19/25 | Loss: 0.00140078
Iteration 20/25 | Loss: 0.00140001
Iteration 21/25 | Loss: 0.00139980
Iteration 22/25 | Loss: 0.00139970
Iteration 23/25 | Loss: 0.00139969
Iteration 24/25 | Loss: 0.00139969
Iteration 25/25 | Loss: 0.00139969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34117043
Iteration 2/25 | Loss: 0.00082322
Iteration 3/25 | Loss: 0.00082321
Iteration 4/25 | Loss: 0.00082320
Iteration 5/25 | Loss: 0.00082320
Iteration 6/25 | Loss: 0.00082320
Iteration 7/25 | Loss: 0.00082320
Iteration 8/25 | Loss: 0.00082320
Iteration 9/25 | Loss: 0.00082320
Iteration 10/25 | Loss: 0.00082320
Iteration 11/25 | Loss: 0.00082320
Iteration 12/25 | Loss: 0.00082320
Iteration 13/25 | Loss: 0.00082320
Iteration 14/25 | Loss: 0.00082320
Iteration 15/25 | Loss: 0.00082320
Iteration 16/25 | Loss: 0.00082320
Iteration 17/25 | Loss: 0.00082320
Iteration 18/25 | Loss: 0.00082320
Iteration 19/25 | Loss: 0.00082320
Iteration 20/25 | Loss: 0.00082320
Iteration 21/25 | Loss: 0.00082320
Iteration 22/25 | Loss: 0.00082320
Iteration 23/25 | Loss: 0.00082320
Iteration 24/25 | Loss: 0.00082320
Iteration 25/25 | Loss: 0.00082320

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082320
Iteration 2/1000 | Loss: 0.00004406
Iteration 3/1000 | Loss: 0.00003281
Iteration 4/1000 | Loss: 0.00003014
Iteration 5/1000 | Loss: 0.00002940
Iteration 6/1000 | Loss: 0.00002869
Iteration 7/1000 | Loss: 0.00002830
Iteration 8/1000 | Loss: 0.00002799
Iteration 9/1000 | Loss: 0.00002768
Iteration 10/1000 | Loss: 0.00002743
Iteration 11/1000 | Loss: 0.00002732
Iteration 12/1000 | Loss: 0.00002718
Iteration 13/1000 | Loss: 0.00002714
Iteration 14/1000 | Loss: 0.00002710
Iteration 15/1000 | Loss: 0.00002709
Iteration 16/1000 | Loss: 0.00002707
Iteration 17/1000 | Loss: 0.00002706
Iteration 18/1000 | Loss: 0.00002706
Iteration 19/1000 | Loss: 0.00002689
Iteration 20/1000 | Loss: 0.00002689
Iteration 21/1000 | Loss: 0.00002688
Iteration 22/1000 | Loss: 0.00002687
Iteration 23/1000 | Loss: 0.00002686
Iteration 24/1000 | Loss: 0.00002685
Iteration 25/1000 | Loss: 0.00002685
Iteration 26/1000 | Loss: 0.00002684
Iteration 27/1000 | Loss: 0.00002684
Iteration 28/1000 | Loss: 0.00002684
Iteration 29/1000 | Loss: 0.00002684
Iteration 30/1000 | Loss: 0.00002684
Iteration 31/1000 | Loss: 0.00002683
Iteration 32/1000 | Loss: 0.00002683
Iteration 33/1000 | Loss: 0.00002681
Iteration 34/1000 | Loss: 0.00002681
Iteration 35/1000 | Loss: 0.00002680
Iteration 36/1000 | Loss: 0.00002680
Iteration 37/1000 | Loss: 0.00002680
Iteration 38/1000 | Loss: 0.00002680
Iteration 39/1000 | Loss: 0.00002679
Iteration 40/1000 | Loss: 0.00002678
Iteration 41/1000 | Loss: 0.00002678
Iteration 42/1000 | Loss: 0.00002677
Iteration 43/1000 | Loss: 0.00002677
Iteration 44/1000 | Loss: 0.00002677
Iteration 45/1000 | Loss: 0.00002677
Iteration 46/1000 | Loss: 0.00002677
Iteration 47/1000 | Loss: 0.00002677
Iteration 48/1000 | Loss: 0.00002676
Iteration 49/1000 | Loss: 0.00002676
Iteration 50/1000 | Loss: 0.00002675
Iteration 51/1000 | Loss: 0.00002675
Iteration 52/1000 | Loss: 0.00002675
Iteration 53/1000 | Loss: 0.00002675
Iteration 54/1000 | Loss: 0.00002675
Iteration 55/1000 | Loss: 0.00002675
Iteration 56/1000 | Loss: 0.00002675
Iteration 57/1000 | Loss: 0.00002674
Iteration 58/1000 | Loss: 0.00002674
Iteration 59/1000 | Loss: 0.00002674
Iteration 60/1000 | Loss: 0.00002674
Iteration 61/1000 | Loss: 0.00002674
Iteration 62/1000 | Loss: 0.00002674
Iteration 63/1000 | Loss: 0.00002674
Iteration 64/1000 | Loss: 0.00002674
Iteration 65/1000 | Loss: 0.00002674
Iteration 66/1000 | Loss: 0.00002673
Iteration 67/1000 | Loss: 0.00002673
Iteration 68/1000 | Loss: 0.00002673
Iteration 69/1000 | Loss: 0.00002673
Iteration 70/1000 | Loss: 0.00002673
Iteration 71/1000 | Loss: 0.00002673
Iteration 72/1000 | Loss: 0.00002673
Iteration 73/1000 | Loss: 0.00002672
Iteration 74/1000 | Loss: 0.00002672
Iteration 75/1000 | Loss: 0.00002672
Iteration 76/1000 | Loss: 0.00002672
Iteration 77/1000 | Loss: 0.00002672
Iteration 78/1000 | Loss: 0.00002672
Iteration 79/1000 | Loss: 0.00002672
Iteration 80/1000 | Loss: 0.00002672
Iteration 81/1000 | Loss: 0.00002672
Iteration 82/1000 | Loss: 0.00002672
Iteration 83/1000 | Loss: 0.00002672
Iteration 84/1000 | Loss: 0.00002672
Iteration 85/1000 | Loss: 0.00002672
Iteration 86/1000 | Loss: 0.00002672
Iteration 87/1000 | Loss: 0.00002672
Iteration 88/1000 | Loss: 0.00002672
Iteration 89/1000 | Loss: 0.00002672
Iteration 90/1000 | Loss: 0.00002672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.6718933440861292e-05, 2.6718933440861292e-05, 2.6718933440861292e-05, 2.6718933440861292e-05, 2.6718933440861292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6718933440861292e-05

Optimization complete. Final v2v error: 4.316488265991211 mm

Highest mean error: 4.5629658699035645 mm for frame 6

Lowest mean error: 4.01217794418335 mm for frame 154

Saving results

Total time: 63.64542031288147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787957
Iteration 2/25 | Loss: 0.00149073
Iteration 3/25 | Loss: 0.00125137
Iteration 4/25 | Loss: 0.00123618
Iteration 5/25 | Loss: 0.00123082
Iteration 6/25 | Loss: 0.00122947
Iteration 7/25 | Loss: 0.00122947
Iteration 8/25 | Loss: 0.00122947
Iteration 9/25 | Loss: 0.00122947
Iteration 10/25 | Loss: 0.00122947
Iteration 11/25 | Loss: 0.00122947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012294681509956717, 0.0012294681509956717, 0.0012294681509956717, 0.0012294681509956717, 0.0012294681509956717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012294681509956717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10400534
Iteration 2/25 | Loss: 0.00108643
Iteration 3/25 | Loss: 0.00108643
Iteration 4/25 | Loss: 0.00108643
Iteration 5/25 | Loss: 0.00108643
Iteration 6/25 | Loss: 0.00108643
Iteration 7/25 | Loss: 0.00108643
Iteration 8/25 | Loss: 0.00108643
Iteration 9/25 | Loss: 0.00108643
Iteration 10/25 | Loss: 0.00108643
Iteration 11/25 | Loss: 0.00108643
Iteration 12/25 | Loss: 0.00108643
Iteration 13/25 | Loss: 0.00108643
Iteration 14/25 | Loss: 0.00108643
Iteration 15/25 | Loss: 0.00108643
Iteration 16/25 | Loss: 0.00108643
Iteration 17/25 | Loss: 0.00108643
Iteration 18/25 | Loss: 0.00108643
Iteration 19/25 | Loss: 0.00108643
Iteration 20/25 | Loss: 0.00108643
Iteration 21/25 | Loss: 0.00108643
Iteration 22/25 | Loss: 0.00108643
Iteration 23/25 | Loss: 0.00108643
Iteration 24/25 | Loss: 0.00108643
Iteration 25/25 | Loss: 0.00108643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108643
Iteration 2/1000 | Loss: 0.00004827
Iteration 3/1000 | Loss: 0.00003130
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001924
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001687
Iteration 12/1000 | Loss: 0.00001652
Iteration 13/1000 | Loss: 0.00001630
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001579
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001555
Iteration 23/1000 | Loss: 0.00001555
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001554
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001534
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001528
Iteration 47/1000 | Loss: 0.00001527
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001526
Iteration 50/1000 | Loss: 0.00001526
Iteration 51/1000 | Loss: 0.00001526
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001522
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001515
Iteration 89/1000 | Loss: 0.00001515
Iteration 90/1000 | Loss: 0.00001515
Iteration 91/1000 | Loss: 0.00001514
Iteration 92/1000 | Loss: 0.00001514
Iteration 93/1000 | Loss: 0.00001514
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001513
Iteration 98/1000 | Loss: 0.00001513
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001513
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001512
Iteration 104/1000 | Loss: 0.00001512
Iteration 105/1000 | Loss: 0.00001511
Iteration 106/1000 | Loss: 0.00001511
Iteration 107/1000 | Loss: 0.00001511
Iteration 108/1000 | Loss: 0.00001511
Iteration 109/1000 | Loss: 0.00001511
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001510
Iteration 113/1000 | Loss: 0.00001510
Iteration 114/1000 | Loss: 0.00001510
Iteration 115/1000 | Loss: 0.00001510
Iteration 116/1000 | Loss: 0.00001510
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001509
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001509
Iteration 121/1000 | Loss: 0.00001509
Iteration 122/1000 | Loss: 0.00001509
Iteration 123/1000 | Loss: 0.00001509
Iteration 124/1000 | Loss: 0.00001509
Iteration 125/1000 | Loss: 0.00001509
Iteration 126/1000 | Loss: 0.00001509
Iteration 127/1000 | Loss: 0.00001509
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00001508
Iteration 130/1000 | Loss: 0.00001508
Iteration 131/1000 | Loss: 0.00001508
Iteration 132/1000 | Loss: 0.00001508
Iteration 133/1000 | Loss: 0.00001508
Iteration 134/1000 | Loss: 0.00001508
Iteration 135/1000 | Loss: 0.00001508
Iteration 136/1000 | Loss: 0.00001508
Iteration 137/1000 | Loss: 0.00001508
Iteration 138/1000 | Loss: 0.00001508
Iteration 139/1000 | Loss: 0.00001508
Iteration 140/1000 | Loss: 0.00001508
Iteration 141/1000 | Loss: 0.00001508
Iteration 142/1000 | Loss: 0.00001508
Iteration 143/1000 | Loss: 0.00001508
Iteration 144/1000 | Loss: 0.00001508
Iteration 145/1000 | Loss: 0.00001508
Iteration 146/1000 | Loss: 0.00001508
Iteration 147/1000 | Loss: 0.00001508
Iteration 148/1000 | Loss: 0.00001508
Iteration 149/1000 | Loss: 0.00001508
Iteration 150/1000 | Loss: 0.00001508
Iteration 151/1000 | Loss: 0.00001508
Iteration 152/1000 | Loss: 0.00001508
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001508
Iteration 155/1000 | Loss: 0.00001508
Iteration 156/1000 | Loss: 0.00001508
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.5078813703439664e-05, 1.5078813703439664e-05, 1.5078813703439664e-05, 1.5078813703439664e-05, 1.5078813703439664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5078813703439664e-05

Optimization complete. Final v2v error: 3.2095930576324463 mm

Highest mean error: 4.3114705085754395 mm for frame 81

Lowest mean error: 2.5548932552337646 mm for frame 196

Saving results

Total time: 51.5825891494751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987084
Iteration 2/25 | Loss: 0.00231317
Iteration 3/25 | Loss: 0.00181605
Iteration 4/25 | Loss: 0.00175179
Iteration 5/25 | Loss: 0.00172819
Iteration 6/25 | Loss: 0.00168878
Iteration 7/25 | Loss: 0.00167635
Iteration 8/25 | Loss: 0.00166328
Iteration 9/25 | Loss: 0.00162799
Iteration 10/25 | Loss: 0.00162422
Iteration 11/25 | Loss: 0.00159316
Iteration 12/25 | Loss: 0.00158772
Iteration 13/25 | Loss: 0.00158124
Iteration 14/25 | Loss: 0.00157674
Iteration 15/25 | Loss: 0.00157752
Iteration 16/25 | Loss: 0.00157351
Iteration 17/25 | Loss: 0.00156853
Iteration 18/25 | Loss: 0.00156753
Iteration 19/25 | Loss: 0.00156729
Iteration 20/25 | Loss: 0.00156715
Iteration 21/25 | Loss: 0.00156691
Iteration 22/25 | Loss: 0.00156655
Iteration 23/25 | Loss: 0.00156626
Iteration 24/25 | Loss: 0.00156913
Iteration 25/25 | Loss: 0.00156482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30734241
Iteration 2/25 | Loss: 0.00278396
Iteration 3/25 | Loss: 0.00270542
Iteration 4/25 | Loss: 0.00270542
Iteration 5/25 | Loss: 0.00270542
Iteration 6/25 | Loss: 0.00270542
Iteration 7/25 | Loss: 0.00270542
Iteration 8/25 | Loss: 0.00270542
Iteration 9/25 | Loss: 0.00270542
Iteration 10/25 | Loss: 0.00270542
Iteration 11/25 | Loss: 0.00270542
Iteration 12/25 | Loss: 0.00270542
Iteration 13/25 | Loss: 0.00270542
Iteration 14/25 | Loss: 0.00270542
Iteration 15/25 | Loss: 0.00270542
Iteration 16/25 | Loss: 0.00270542
Iteration 17/25 | Loss: 0.00270542
Iteration 18/25 | Loss: 0.00270542
Iteration 19/25 | Loss: 0.00270542
Iteration 20/25 | Loss: 0.00270542
Iteration 21/25 | Loss: 0.00270542
Iteration 22/25 | Loss: 0.00270542
Iteration 23/25 | Loss: 0.00270542
Iteration 24/25 | Loss: 0.00270542
Iteration 25/25 | Loss: 0.00270542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270542
Iteration 2/1000 | Loss: 0.00038869
Iteration 3/1000 | Loss: 0.00025645
Iteration 4/1000 | Loss: 0.00022049
Iteration 5/1000 | Loss: 0.00020285
Iteration 6/1000 | Loss: 0.00021228
Iteration 7/1000 | Loss: 0.00025788
Iteration 8/1000 | Loss: 0.00023991
Iteration 9/1000 | Loss: 0.00020202
Iteration 10/1000 | Loss: 0.00021678
Iteration 11/1000 | Loss: 0.00016068
Iteration 12/1000 | Loss: 0.00015629
Iteration 13/1000 | Loss: 0.00015300
Iteration 14/1000 | Loss: 0.00031394
Iteration 15/1000 | Loss: 0.00053835
Iteration 16/1000 | Loss: 0.00147999
Iteration 17/1000 | Loss: 0.00901294
Iteration 18/1000 | Loss: 0.00100734
Iteration 19/1000 | Loss: 0.00028790
Iteration 20/1000 | Loss: 0.00020877
Iteration 21/1000 | Loss: 0.00015168
Iteration 22/1000 | Loss: 0.00010023
Iteration 23/1000 | Loss: 0.00006557
Iteration 24/1000 | Loss: 0.00004770
Iteration 25/1000 | Loss: 0.00003832
Iteration 26/1000 | Loss: 0.00003236
Iteration 27/1000 | Loss: 0.00002769
Iteration 28/1000 | Loss: 0.00002450
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00001944
Iteration 31/1000 | Loss: 0.00001741
Iteration 32/1000 | Loss: 0.00001629
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001420
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001279
Iteration 39/1000 | Loss: 0.00001269
Iteration 40/1000 | Loss: 0.00001259
Iteration 41/1000 | Loss: 0.00001258
Iteration 42/1000 | Loss: 0.00001257
Iteration 43/1000 | Loss: 0.00001257
Iteration 44/1000 | Loss: 0.00001257
Iteration 45/1000 | Loss: 0.00001256
Iteration 46/1000 | Loss: 0.00001256
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001242
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001241
Iteration 68/1000 | Loss: 0.00001241
Iteration 69/1000 | Loss: 0.00001241
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001240
Iteration 75/1000 | Loss: 0.00001240
Iteration 76/1000 | Loss: 0.00001240
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.240161873283796e-05, 1.240161873283796e-05, 1.240161873283796e-05, 1.240161873283796e-05, 1.240161873283796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.240161873283796e-05

Optimization complete. Final v2v error: 3.050278425216675 mm

Highest mean error: 3.187969923019409 mm for frame 121

Lowest mean error: 2.9155735969543457 mm for frame 79

Saving results

Total time: 119.81258225440979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764922
Iteration 2/25 | Loss: 0.00206851
Iteration 3/25 | Loss: 0.00149371
Iteration 4/25 | Loss: 0.00141953
Iteration 5/25 | Loss: 0.00132398
Iteration 6/25 | Loss: 0.00135068
Iteration 7/25 | Loss: 0.00126860
Iteration 8/25 | Loss: 0.00128484
Iteration 9/25 | Loss: 0.00122870
Iteration 10/25 | Loss: 0.00121931
Iteration 11/25 | Loss: 0.00121198
Iteration 12/25 | Loss: 0.00120364
Iteration 13/25 | Loss: 0.00120344
Iteration 14/25 | Loss: 0.00120255
Iteration 15/25 | Loss: 0.00120251
Iteration 16/25 | Loss: 0.00120251
Iteration 17/25 | Loss: 0.00120251
Iteration 18/25 | Loss: 0.00120251
Iteration 19/25 | Loss: 0.00120251
Iteration 20/25 | Loss: 0.00120251
Iteration 21/25 | Loss: 0.00120251
Iteration 22/25 | Loss: 0.00120250
Iteration 23/25 | Loss: 0.00120250
Iteration 24/25 | Loss: 0.00120250
Iteration 25/25 | Loss: 0.00120250

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02722406
Iteration 2/25 | Loss: 0.00112914
Iteration 3/25 | Loss: 0.00107435
Iteration 4/25 | Loss: 0.00107435
Iteration 5/25 | Loss: 0.00107434
Iteration 6/25 | Loss: 0.00107434
Iteration 7/25 | Loss: 0.00107434
Iteration 8/25 | Loss: 0.00107434
Iteration 9/25 | Loss: 0.00107434
Iteration 10/25 | Loss: 0.00107434
Iteration 11/25 | Loss: 0.00107434
Iteration 12/25 | Loss: 0.00107434
Iteration 13/25 | Loss: 0.00107434
Iteration 14/25 | Loss: 0.00107434
Iteration 15/25 | Loss: 0.00107434
Iteration 16/25 | Loss: 0.00107434
Iteration 17/25 | Loss: 0.00107434
Iteration 18/25 | Loss: 0.00107434
Iteration 19/25 | Loss: 0.00107434
Iteration 20/25 | Loss: 0.00107434
Iteration 21/25 | Loss: 0.00107434
Iteration 22/25 | Loss: 0.00107434
Iteration 23/25 | Loss: 0.00107434
Iteration 24/25 | Loss: 0.00107434
Iteration 25/25 | Loss: 0.00107434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107434
Iteration 2/1000 | Loss: 0.00008513
Iteration 3/1000 | Loss: 0.00008066
Iteration 4/1000 | Loss: 0.00010507
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00015064
Iteration 7/1000 | Loss: 0.00028081
Iteration 8/1000 | Loss: 0.00002864
Iteration 9/1000 | Loss: 0.00004108
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001792
Iteration 15/1000 | Loss: 0.00004902
Iteration 16/1000 | Loss: 0.00008747
Iteration 17/1000 | Loss: 0.00009491
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00005681
Iteration 20/1000 | Loss: 0.00002404
Iteration 21/1000 | Loss: 0.00003165
Iteration 22/1000 | Loss: 0.00004444
Iteration 23/1000 | Loss: 0.00002941
Iteration 24/1000 | Loss: 0.00001133
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00004743
Iteration 27/1000 | Loss: 0.00041413
Iteration 28/1000 | Loss: 0.00468446
Iteration 29/1000 | Loss: 0.00005052
Iteration 30/1000 | Loss: 0.00059185
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00003364
Iteration 33/1000 | Loss: 0.00004925
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00003025
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001080
Iteration 38/1000 | Loss: 0.00001124
Iteration 39/1000 | Loss: 0.00001124
Iteration 40/1000 | Loss: 0.00003650
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001080
Iteration 43/1000 | Loss: 0.00002270
Iteration 44/1000 | Loss: 0.00008220
Iteration 45/1000 | Loss: 0.00002790
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001847
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001075
Iteration 54/1000 | Loss: 0.00001075
Iteration 55/1000 | Loss: 0.00001075
Iteration 56/1000 | Loss: 0.00001075
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001074
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001072
Iteration 72/1000 | Loss: 0.00001072
Iteration 73/1000 | Loss: 0.00001072
Iteration 74/1000 | Loss: 0.00001072
Iteration 75/1000 | Loss: 0.00001072
Iteration 76/1000 | Loss: 0.00001072
Iteration 77/1000 | Loss: 0.00001076
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001069
Iteration 81/1000 | Loss: 0.00001069
Iteration 82/1000 | Loss: 0.00001069
Iteration 83/1000 | Loss: 0.00001068
Iteration 84/1000 | Loss: 0.00001069
Iteration 85/1000 | Loss: 0.00001068
Iteration 86/1000 | Loss: 0.00001068
Iteration 87/1000 | Loss: 0.00001068
Iteration 88/1000 | Loss: 0.00001068
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001068
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001068
Iteration 97/1000 | Loss: 0.00001068
Iteration 98/1000 | Loss: 0.00001068
Iteration 99/1000 | Loss: 0.00001068
Iteration 100/1000 | Loss: 0.00001068
Iteration 101/1000 | Loss: 0.00001068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.067961875378387e-05, 1.067961875378387e-05, 1.067961875378387e-05, 1.067961875378387e-05, 1.067961875378387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.067961875378387e-05

Optimization complete. Final v2v error: 2.8409676551818848 mm

Highest mean error: 3.140892744064331 mm for frame 61

Lowest mean error: 2.610877752304077 mm for frame 114

Saving results

Total time: 95.84563374519348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813046
Iteration 2/25 | Loss: 0.00141083
Iteration 3/25 | Loss: 0.00129403
Iteration 4/25 | Loss: 0.00128057
Iteration 5/25 | Loss: 0.00127692
Iteration 6/25 | Loss: 0.00127642
Iteration 7/25 | Loss: 0.00127642
Iteration 8/25 | Loss: 0.00127642
Iteration 9/25 | Loss: 0.00127642
Iteration 10/25 | Loss: 0.00127642
Iteration 11/25 | Loss: 0.00127642
Iteration 12/25 | Loss: 0.00127642
Iteration 13/25 | Loss: 0.00127642
Iteration 14/25 | Loss: 0.00127642
Iteration 15/25 | Loss: 0.00127642
Iteration 16/25 | Loss: 0.00127642
Iteration 17/25 | Loss: 0.00127642
Iteration 18/25 | Loss: 0.00127642
Iteration 19/25 | Loss: 0.00127642
Iteration 20/25 | Loss: 0.00127642
Iteration 21/25 | Loss: 0.00127642
Iteration 22/25 | Loss: 0.00127642
Iteration 23/25 | Loss: 0.00127642
Iteration 24/25 | Loss: 0.00127642
Iteration 25/25 | Loss: 0.00127642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51549506
Iteration 2/25 | Loss: 0.00104055
Iteration 3/25 | Loss: 0.00104054
Iteration 4/25 | Loss: 0.00104054
Iteration 5/25 | Loss: 0.00104054
Iteration 6/25 | Loss: 0.00104054
Iteration 7/25 | Loss: 0.00104054
Iteration 8/25 | Loss: 0.00104054
Iteration 9/25 | Loss: 0.00104054
Iteration 10/25 | Loss: 0.00104054
Iteration 11/25 | Loss: 0.00104054
Iteration 12/25 | Loss: 0.00104054
Iteration 13/25 | Loss: 0.00104054
Iteration 14/25 | Loss: 0.00104054
Iteration 15/25 | Loss: 0.00104054
Iteration 16/25 | Loss: 0.00104054
Iteration 17/25 | Loss: 0.00104054
Iteration 18/25 | Loss: 0.00104054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001040539238601923, 0.001040539238601923, 0.001040539238601923, 0.001040539238601923, 0.001040539238601923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001040539238601923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104054
Iteration 2/1000 | Loss: 0.00003519
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002225
Iteration 5/1000 | Loss: 0.00002109
Iteration 6/1000 | Loss: 0.00002004
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001731
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001715
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001709
Iteration 27/1000 | Loss: 0.00001707
Iteration 28/1000 | Loss: 0.00001706
Iteration 29/1000 | Loss: 0.00001705
Iteration 30/1000 | Loss: 0.00001705
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00001703
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001701
Iteration 35/1000 | Loss: 0.00001700
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001690
Iteration 40/1000 | Loss: 0.00001690
Iteration 41/1000 | Loss: 0.00001690
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001689
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001685
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001685
Iteration 52/1000 | Loss: 0.00001685
Iteration 53/1000 | Loss: 0.00001684
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001683
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001682
Iteration 59/1000 | Loss: 0.00001682
Iteration 60/1000 | Loss: 0.00001681
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001680
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001679
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001673
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001672
Iteration 80/1000 | Loss: 0.00001672
Iteration 81/1000 | Loss: 0.00001671
Iteration 82/1000 | Loss: 0.00001671
Iteration 83/1000 | Loss: 0.00001671
Iteration 84/1000 | Loss: 0.00001671
Iteration 85/1000 | Loss: 0.00001671
Iteration 86/1000 | Loss: 0.00001671
Iteration 87/1000 | Loss: 0.00001670
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001670
Iteration 90/1000 | Loss: 0.00001670
Iteration 91/1000 | Loss: 0.00001669
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001669
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001669
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001668
Iteration 99/1000 | Loss: 0.00001668
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001667
Iteration 104/1000 | Loss: 0.00001667
Iteration 105/1000 | Loss: 0.00001666
Iteration 106/1000 | Loss: 0.00001666
Iteration 107/1000 | Loss: 0.00001665
Iteration 108/1000 | Loss: 0.00001665
Iteration 109/1000 | Loss: 0.00001665
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001664
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001663
Iteration 116/1000 | Loss: 0.00001662
Iteration 117/1000 | Loss: 0.00001662
Iteration 118/1000 | Loss: 0.00001662
Iteration 119/1000 | Loss: 0.00001662
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001661
Iteration 126/1000 | Loss: 0.00001661
Iteration 127/1000 | Loss: 0.00001661
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001659
Iteration 134/1000 | Loss: 0.00001659
Iteration 135/1000 | Loss: 0.00001659
Iteration 136/1000 | Loss: 0.00001659
Iteration 137/1000 | Loss: 0.00001659
Iteration 138/1000 | Loss: 0.00001659
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001658
Iteration 143/1000 | Loss: 0.00001658
Iteration 144/1000 | Loss: 0.00001658
Iteration 145/1000 | Loss: 0.00001658
Iteration 146/1000 | Loss: 0.00001658
Iteration 147/1000 | Loss: 0.00001658
Iteration 148/1000 | Loss: 0.00001658
Iteration 149/1000 | Loss: 0.00001658
Iteration 150/1000 | Loss: 0.00001657
Iteration 151/1000 | Loss: 0.00001657
Iteration 152/1000 | Loss: 0.00001657
Iteration 153/1000 | Loss: 0.00001657
Iteration 154/1000 | Loss: 0.00001657
Iteration 155/1000 | Loss: 0.00001657
Iteration 156/1000 | Loss: 0.00001657
Iteration 157/1000 | Loss: 0.00001657
Iteration 158/1000 | Loss: 0.00001657
Iteration 159/1000 | Loss: 0.00001657
Iteration 160/1000 | Loss: 0.00001657
Iteration 161/1000 | Loss: 0.00001656
Iteration 162/1000 | Loss: 0.00001656
Iteration 163/1000 | Loss: 0.00001656
Iteration 164/1000 | Loss: 0.00001656
Iteration 165/1000 | Loss: 0.00001656
Iteration 166/1000 | Loss: 0.00001656
Iteration 167/1000 | Loss: 0.00001656
Iteration 168/1000 | Loss: 0.00001656
Iteration 169/1000 | Loss: 0.00001656
Iteration 170/1000 | Loss: 0.00001656
Iteration 171/1000 | Loss: 0.00001656
Iteration 172/1000 | Loss: 0.00001656
Iteration 173/1000 | Loss: 0.00001656
Iteration 174/1000 | Loss: 0.00001656
Iteration 175/1000 | Loss: 0.00001656
Iteration 176/1000 | Loss: 0.00001656
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001655
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Iteration 185/1000 | Loss: 0.00001654
Iteration 186/1000 | Loss: 0.00001654
Iteration 187/1000 | Loss: 0.00001654
Iteration 188/1000 | Loss: 0.00001654
Iteration 189/1000 | Loss: 0.00001654
Iteration 190/1000 | Loss: 0.00001654
Iteration 191/1000 | Loss: 0.00001654
Iteration 192/1000 | Loss: 0.00001654
Iteration 193/1000 | Loss: 0.00001654
Iteration 194/1000 | Loss: 0.00001654
Iteration 195/1000 | Loss: 0.00001654
Iteration 196/1000 | Loss: 0.00001654
Iteration 197/1000 | Loss: 0.00001654
Iteration 198/1000 | Loss: 0.00001654
Iteration 199/1000 | Loss: 0.00001654
Iteration 200/1000 | Loss: 0.00001654
Iteration 201/1000 | Loss: 0.00001654
Iteration 202/1000 | Loss: 0.00001654
Iteration 203/1000 | Loss: 0.00001654
Iteration 204/1000 | Loss: 0.00001654
Iteration 205/1000 | Loss: 0.00001654
Iteration 206/1000 | Loss: 0.00001654
Iteration 207/1000 | Loss: 0.00001654
Iteration 208/1000 | Loss: 0.00001654
Iteration 209/1000 | Loss: 0.00001654
Iteration 210/1000 | Loss: 0.00001654
Iteration 211/1000 | Loss: 0.00001654
Iteration 212/1000 | Loss: 0.00001654
Iteration 213/1000 | Loss: 0.00001654
Iteration 214/1000 | Loss: 0.00001654
Iteration 215/1000 | Loss: 0.00001654
Iteration 216/1000 | Loss: 0.00001654
Iteration 217/1000 | Loss: 0.00001654
Iteration 218/1000 | Loss: 0.00001654
Iteration 219/1000 | Loss: 0.00001654
Iteration 220/1000 | Loss: 0.00001654
Iteration 221/1000 | Loss: 0.00001654
Iteration 222/1000 | Loss: 0.00001654
Iteration 223/1000 | Loss: 0.00001654
Iteration 224/1000 | Loss: 0.00001654
Iteration 225/1000 | Loss: 0.00001654
Iteration 226/1000 | Loss: 0.00001654
Iteration 227/1000 | Loss: 0.00001654
Iteration 228/1000 | Loss: 0.00001654
Iteration 229/1000 | Loss: 0.00001654
Iteration 230/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.6542651792406105e-05, 1.6542651792406105e-05, 1.6542651792406105e-05, 1.6542651792406105e-05, 1.6542651792406105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6542651792406105e-05

Optimization complete. Final v2v error: 3.428692579269409 mm

Highest mean error: 4.335550785064697 mm for frame 86

Lowest mean error: 2.9046638011932373 mm for frame 60

Saving results

Total time: 46.16207242012024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042814
Iteration 2/25 | Loss: 0.01042814
Iteration 3/25 | Loss: 0.01042814
Iteration 4/25 | Loss: 0.01042813
Iteration 5/25 | Loss: 0.01042813
Iteration 6/25 | Loss: 0.01042813
Iteration 7/25 | Loss: 0.01042813
Iteration 8/25 | Loss: 0.01042813
Iteration 9/25 | Loss: 0.01042812
Iteration 10/25 | Loss: 0.01042812
Iteration 11/25 | Loss: 0.01042812
Iteration 12/25 | Loss: 0.01042812
Iteration 13/25 | Loss: 0.01042812
Iteration 14/25 | Loss: 0.01042812
Iteration 15/25 | Loss: 0.01042811
Iteration 16/25 | Loss: 0.01042811
Iteration 17/25 | Loss: 0.01042811
Iteration 18/25 | Loss: 0.01042811
Iteration 19/25 | Loss: 0.01042811
Iteration 20/25 | Loss: 0.01042811
Iteration 21/25 | Loss: 0.01042810
Iteration 22/25 | Loss: 0.01042810
Iteration 23/25 | Loss: 0.01042810
Iteration 24/25 | Loss: 0.01042809
Iteration 25/25 | Loss: 0.01042809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99054396
Iteration 2/25 | Loss: 0.09161606
Iteration 3/25 | Loss: 0.09066291
Iteration 4/25 | Loss: 0.08941916
Iteration 5/25 | Loss: 0.08941913
Iteration 6/25 | Loss: 0.08941912
Iteration 7/25 | Loss: 0.08941912
Iteration 8/25 | Loss: 0.08941912
Iteration 9/25 | Loss: 0.08941912
Iteration 10/25 | Loss: 0.08941912
Iteration 11/25 | Loss: 0.08941912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.08941911906003952, 0.08941911906003952, 0.08941911906003952, 0.08941911906003952, 0.08941911906003952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08941911906003952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08941912
Iteration 2/1000 | Loss: 0.00163606
Iteration 3/1000 | Loss: 0.00201715
Iteration 4/1000 | Loss: 0.00243125
Iteration 5/1000 | Loss: 0.00246130
Iteration 6/1000 | Loss: 0.00184715
Iteration 7/1000 | Loss: 0.00198324
Iteration 8/1000 | Loss: 0.00227499
Iteration 9/1000 | Loss: 0.00111876
Iteration 10/1000 | Loss: 0.00154919
Iteration 11/1000 | Loss: 0.00025548
Iteration 12/1000 | Loss: 0.00009303
Iteration 13/1000 | Loss: 0.00009235
Iteration 14/1000 | Loss: 0.00052979
Iteration 15/1000 | Loss: 0.00032739
Iteration 16/1000 | Loss: 0.00007749
Iteration 17/1000 | Loss: 0.00129102
Iteration 18/1000 | Loss: 0.00096940
Iteration 19/1000 | Loss: 0.00074213
Iteration 20/1000 | Loss: 0.00008115
Iteration 21/1000 | Loss: 0.00095667
Iteration 22/1000 | Loss: 0.00307273
Iteration 23/1000 | Loss: 0.00251030
Iteration 24/1000 | Loss: 0.00024538
Iteration 25/1000 | Loss: 0.00004275
Iteration 26/1000 | Loss: 0.00008735
Iteration 27/1000 | Loss: 0.00004620
Iteration 28/1000 | Loss: 0.00004854
Iteration 29/1000 | Loss: 0.00003436
Iteration 30/1000 | Loss: 0.00057100
Iteration 31/1000 | Loss: 0.00020495
Iteration 32/1000 | Loss: 0.00003564
Iteration 33/1000 | Loss: 0.00004001
Iteration 34/1000 | Loss: 0.00009008
Iteration 35/1000 | Loss: 0.00004739
Iteration 36/1000 | Loss: 0.00003192
Iteration 37/1000 | Loss: 0.00012486
Iteration 38/1000 | Loss: 0.00002927
Iteration 39/1000 | Loss: 0.00017065
Iteration 40/1000 | Loss: 0.00022474
Iteration 41/1000 | Loss: 0.00032882
Iteration 42/1000 | Loss: 0.00004179
Iteration 43/1000 | Loss: 0.00005609
Iteration 44/1000 | Loss: 0.00003814
Iteration 45/1000 | Loss: 0.00002627
Iteration 46/1000 | Loss: 0.00014931
Iteration 47/1000 | Loss: 0.00009553
Iteration 48/1000 | Loss: 0.00005969
Iteration 49/1000 | Loss: 0.00030322
Iteration 50/1000 | Loss: 0.00168906
Iteration 51/1000 | Loss: 0.00003014
Iteration 52/1000 | Loss: 0.00002985
Iteration 53/1000 | Loss: 0.00003126
Iteration 54/1000 | Loss: 0.00019070
Iteration 55/1000 | Loss: 0.00003604
Iteration 56/1000 | Loss: 0.00002344
Iteration 57/1000 | Loss: 0.00004386
Iteration 58/1000 | Loss: 0.00002288
Iteration 59/1000 | Loss: 0.00002241
Iteration 60/1000 | Loss: 0.00014024
Iteration 61/1000 | Loss: 0.00047408
Iteration 62/1000 | Loss: 0.00008091
Iteration 63/1000 | Loss: 0.00030199
Iteration 64/1000 | Loss: 0.00033468
Iteration 65/1000 | Loss: 0.00044624
Iteration 66/1000 | Loss: 0.00046085
Iteration 67/1000 | Loss: 0.00008073
Iteration 68/1000 | Loss: 0.00036400
Iteration 69/1000 | Loss: 0.00003075
Iteration 70/1000 | Loss: 0.00010713
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00003904
Iteration 73/1000 | Loss: 0.00003634
Iteration 74/1000 | Loss: 0.00002728
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00003219
Iteration 77/1000 | Loss: 0.00002110
Iteration 78/1000 | Loss: 0.00002108
Iteration 79/1000 | Loss: 0.00005905
Iteration 80/1000 | Loss: 0.00002243
Iteration 81/1000 | Loss: 0.00003777
Iteration 82/1000 | Loss: 0.00004724
Iteration 83/1000 | Loss: 0.00002094
Iteration 84/1000 | Loss: 0.00006701
Iteration 85/1000 | Loss: 0.00061806
Iteration 86/1000 | Loss: 0.00002775
Iteration 87/1000 | Loss: 0.00004842
Iteration 88/1000 | Loss: 0.00002099
Iteration 89/1000 | Loss: 0.00006011
Iteration 90/1000 | Loss: 0.00004378
Iteration 91/1000 | Loss: 0.00004073
Iteration 92/1000 | Loss: 0.00004306
Iteration 93/1000 | Loss: 0.00003283
Iteration 94/1000 | Loss: 0.00006847
Iteration 95/1000 | Loss: 0.00004919
Iteration 96/1000 | Loss: 0.00002271
Iteration 97/1000 | Loss: 0.00005081
Iteration 98/1000 | Loss: 0.00003695
Iteration 99/1000 | Loss: 0.00002757
Iteration 100/1000 | Loss: 0.00004773
Iteration 101/1000 | Loss: 0.00007433
Iteration 102/1000 | Loss: 0.00003546
Iteration 103/1000 | Loss: 0.00036737
Iteration 104/1000 | Loss: 0.00007153
Iteration 105/1000 | Loss: 0.00002142
Iteration 106/1000 | Loss: 0.00002057
Iteration 107/1000 | Loss: 0.00003484
Iteration 108/1000 | Loss: 0.00002049
Iteration 109/1000 | Loss: 0.00002049
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002326
Iteration 112/1000 | Loss: 0.00003852
Iteration 113/1000 | Loss: 0.00002889
Iteration 114/1000 | Loss: 0.00003369
Iteration 115/1000 | Loss: 0.00005054
Iteration 116/1000 | Loss: 0.00002168
Iteration 117/1000 | Loss: 0.00002030
Iteration 118/1000 | Loss: 0.00002030
Iteration 119/1000 | Loss: 0.00002029
Iteration 120/1000 | Loss: 0.00002029
Iteration 121/1000 | Loss: 0.00002029
Iteration 122/1000 | Loss: 0.00002029
Iteration 123/1000 | Loss: 0.00002029
Iteration 124/1000 | Loss: 0.00002029
Iteration 125/1000 | Loss: 0.00002029
Iteration 126/1000 | Loss: 0.00002029
Iteration 127/1000 | Loss: 0.00002029
Iteration 128/1000 | Loss: 0.00002029
Iteration 129/1000 | Loss: 0.00002029
Iteration 130/1000 | Loss: 0.00002028
Iteration 131/1000 | Loss: 0.00002028
Iteration 132/1000 | Loss: 0.00002028
Iteration 133/1000 | Loss: 0.00002028
Iteration 134/1000 | Loss: 0.00002028
Iteration 135/1000 | Loss: 0.00002028
Iteration 136/1000 | Loss: 0.00002028
Iteration 137/1000 | Loss: 0.00002167
Iteration 138/1000 | Loss: 0.00007393
Iteration 139/1000 | Loss: 0.00002158
Iteration 140/1000 | Loss: 0.00003287
Iteration 141/1000 | Loss: 0.00002024
Iteration 142/1000 | Loss: 0.00002022
Iteration 143/1000 | Loss: 0.00002020
Iteration 144/1000 | Loss: 0.00002020
Iteration 145/1000 | Loss: 0.00002540
Iteration 146/1000 | Loss: 0.00002044
Iteration 147/1000 | Loss: 0.00002585
Iteration 148/1000 | Loss: 0.00006817
Iteration 149/1000 | Loss: 0.00002011
Iteration 150/1000 | Loss: 0.00002007
Iteration 151/1000 | Loss: 0.00002006
Iteration 152/1000 | Loss: 0.00002006
Iteration 153/1000 | Loss: 0.00002005
Iteration 154/1000 | Loss: 0.00002005
Iteration 155/1000 | Loss: 0.00002004
Iteration 156/1000 | Loss: 0.00002004
Iteration 157/1000 | Loss: 0.00002003
Iteration 158/1000 | Loss: 0.00002003
Iteration 159/1000 | Loss: 0.00002003
Iteration 160/1000 | Loss: 0.00002002
Iteration 161/1000 | Loss: 0.00002002
Iteration 162/1000 | Loss: 0.00002002
Iteration 163/1000 | Loss: 0.00002002
Iteration 164/1000 | Loss: 0.00002002
Iteration 165/1000 | Loss: 0.00002002
Iteration 166/1000 | Loss: 0.00002002
Iteration 167/1000 | Loss: 0.00002001
Iteration 168/1000 | Loss: 0.00002001
Iteration 169/1000 | Loss: 0.00002001
Iteration 170/1000 | Loss: 0.00002001
Iteration 171/1000 | Loss: 0.00002001
Iteration 172/1000 | Loss: 0.00002001
Iteration 173/1000 | Loss: 0.00002000
Iteration 174/1000 | Loss: 0.00002000
Iteration 175/1000 | Loss: 0.00002000
Iteration 176/1000 | Loss: 0.00004566
Iteration 177/1000 | Loss: 0.00002004
Iteration 178/1000 | Loss: 0.00001997
Iteration 179/1000 | Loss: 0.00001997
Iteration 180/1000 | Loss: 0.00001996
Iteration 181/1000 | Loss: 0.00001996
Iteration 182/1000 | Loss: 0.00001996
Iteration 183/1000 | Loss: 0.00001996
Iteration 184/1000 | Loss: 0.00001996
Iteration 185/1000 | Loss: 0.00001996
Iteration 186/1000 | Loss: 0.00001996
Iteration 187/1000 | Loss: 0.00001996
Iteration 188/1000 | Loss: 0.00001996
Iteration 189/1000 | Loss: 0.00001996
Iteration 190/1000 | Loss: 0.00001996
Iteration 191/1000 | Loss: 0.00008839
Iteration 192/1000 | Loss: 0.00005114
Iteration 193/1000 | Loss: 0.00012073
Iteration 194/1000 | Loss: 0.00002814
Iteration 195/1000 | Loss: 0.00006335
Iteration 196/1000 | Loss: 0.00002334
Iteration 197/1000 | Loss: 0.00002377
Iteration 198/1000 | Loss: 0.00002002
Iteration 199/1000 | Loss: 0.00002526
Iteration 200/1000 | Loss: 0.00009586
Iteration 201/1000 | Loss: 0.00003160
Iteration 202/1000 | Loss: 0.00002001
Iteration 203/1000 | Loss: 0.00001987
Iteration 204/1000 | Loss: 0.00001987
Iteration 205/1000 | Loss: 0.00001987
Iteration 206/1000 | Loss: 0.00001987
Iteration 207/1000 | Loss: 0.00001987
Iteration 208/1000 | Loss: 0.00001987
Iteration 209/1000 | Loss: 0.00001986
Iteration 210/1000 | Loss: 0.00001986
Iteration 211/1000 | Loss: 0.00002119
Iteration 212/1000 | Loss: 0.00001983
Iteration 213/1000 | Loss: 0.00001983
Iteration 214/1000 | Loss: 0.00001983
Iteration 215/1000 | Loss: 0.00001983
Iteration 216/1000 | Loss: 0.00001982
Iteration 217/1000 | Loss: 0.00001982
Iteration 218/1000 | Loss: 0.00001982
Iteration 219/1000 | Loss: 0.00001982
Iteration 220/1000 | Loss: 0.00001982
Iteration 221/1000 | Loss: 0.00001982
Iteration 222/1000 | Loss: 0.00001982
Iteration 223/1000 | Loss: 0.00001982
Iteration 224/1000 | Loss: 0.00001982
Iteration 225/1000 | Loss: 0.00001982
Iteration 226/1000 | Loss: 0.00001981
Iteration 227/1000 | Loss: 0.00001981
Iteration 228/1000 | Loss: 0.00001981
Iteration 229/1000 | Loss: 0.00001981
Iteration 230/1000 | Loss: 0.00001981
Iteration 231/1000 | Loss: 0.00001981
Iteration 232/1000 | Loss: 0.00001981
Iteration 233/1000 | Loss: 0.00001981
Iteration 234/1000 | Loss: 0.00001981
Iteration 235/1000 | Loss: 0.00001981
Iteration 236/1000 | Loss: 0.00001981
Iteration 237/1000 | Loss: 0.00001981
Iteration 238/1000 | Loss: 0.00001981
Iteration 239/1000 | Loss: 0.00001981
Iteration 240/1000 | Loss: 0.00001981
Iteration 241/1000 | Loss: 0.00001981
Iteration 242/1000 | Loss: 0.00001981
Iteration 243/1000 | Loss: 0.00001981
Iteration 244/1000 | Loss: 0.00001981
Iteration 245/1000 | Loss: 0.00001980
Iteration 246/1000 | Loss: 0.00001980
Iteration 247/1000 | Loss: 0.00001980
Iteration 248/1000 | Loss: 0.00001980
Iteration 249/1000 | Loss: 0.00001980
Iteration 250/1000 | Loss: 0.00001980
Iteration 251/1000 | Loss: 0.00001980
Iteration 252/1000 | Loss: 0.00001980
Iteration 253/1000 | Loss: 0.00001980
Iteration 254/1000 | Loss: 0.00001979
Iteration 255/1000 | Loss: 0.00001979
Iteration 256/1000 | Loss: 0.00001979
Iteration 257/1000 | Loss: 0.00001978
Iteration 258/1000 | Loss: 0.00001978
Iteration 259/1000 | Loss: 0.00001978
Iteration 260/1000 | Loss: 0.00001978
Iteration 261/1000 | Loss: 0.00001978
Iteration 262/1000 | Loss: 0.00001977
Iteration 263/1000 | Loss: 0.00001977
Iteration 264/1000 | Loss: 0.00001977
Iteration 265/1000 | Loss: 0.00001977
Iteration 266/1000 | Loss: 0.00001977
Iteration 267/1000 | Loss: 0.00001977
Iteration 268/1000 | Loss: 0.00001976
Iteration 269/1000 | Loss: 0.00001976
Iteration 270/1000 | Loss: 0.00001976
Iteration 271/1000 | Loss: 0.00001976
Iteration 272/1000 | Loss: 0.00002198
Iteration 273/1000 | Loss: 0.00001999
Iteration 274/1000 | Loss: 0.00001975
Iteration 275/1000 | Loss: 0.00001975
Iteration 276/1000 | Loss: 0.00001975
Iteration 277/1000 | Loss: 0.00001975
Iteration 278/1000 | Loss: 0.00001975
Iteration 279/1000 | Loss: 0.00001975
Iteration 280/1000 | Loss: 0.00001975
Iteration 281/1000 | Loss: 0.00001975
Iteration 282/1000 | Loss: 0.00001975
Iteration 283/1000 | Loss: 0.00001975
Iteration 284/1000 | Loss: 0.00001975
Iteration 285/1000 | Loss: 0.00001975
Iteration 286/1000 | Loss: 0.00001975
Iteration 287/1000 | Loss: 0.00001975
Iteration 288/1000 | Loss: 0.00001975
Iteration 289/1000 | Loss: 0.00001975
Iteration 290/1000 | Loss: 0.00001975
Iteration 291/1000 | Loss: 0.00001975
Iteration 292/1000 | Loss: 0.00001974
Iteration 293/1000 | Loss: 0.00001974
Iteration 294/1000 | Loss: 0.00001974
Iteration 295/1000 | Loss: 0.00001974
Iteration 296/1000 | Loss: 0.00001974
Iteration 297/1000 | Loss: 0.00001974
Iteration 298/1000 | Loss: 0.00001974
Iteration 299/1000 | Loss: 0.00001974
Iteration 300/1000 | Loss: 0.00001974
Iteration 301/1000 | Loss: 0.00001974
Iteration 302/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [1.974433871509973e-05, 1.974433871509973e-05, 1.974433871509973e-05, 1.974433871509973e-05, 1.974433871509973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.974433871509973e-05

Optimization complete. Final v2v error: 3.777756690979004 mm

Highest mean error: 5.133009433746338 mm for frame 53

Lowest mean error: 2.9850335121154785 mm for frame 134

Saving results

Total time: 229.99550557136536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776351
Iteration 2/25 | Loss: 0.00145381
Iteration 3/25 | Loss: 0.00132740
Iteration 4/25 | Loss: 0.00130191
Iteration 5/25 | Loss: 0.00129697
Iteration 6/25 | Loss: 0.00129430
Iteration 7/25 | Loss: 0.00128275
Iteration 8/25 | Loss: 0.00127824
Iteration 9/25 | Loss: 0.00127207
Iteration 10/25 | Loss: 0.00127167
Iteration 11/25 | Loss: 0.00126973
Iteration 12/25 | Loss: 0.00126927
Iteration 13/25 | Loss: 0.00126816
Iteration 14/25 | Loss: 0.00126793
Iteration 15/25 | Loss: 0.00126781
Iteration 16/25 | Loss: 0.00126777
Iteration 17/25 | Loss: 0.00126777
Iteration 18/25 | Loss: 0.00126777
Iteration 19/25 | Loss: 0.00126777
Iteration 20/25 | Loss: 0.00126777
Iteration 21/25 | Loss: 0.00126777
Iteration 22/25 | Loss: 0.00126777
Iteration 23/25 | Loss: 0.00126777
Iteration 24/25 | Loss: 0.00126777
Iteration 25/25 | Loss: 0.00126777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63305187
Iteration 2/25 | Loss: 0.00093365
Iteration 3/25 | Loss: 0.00093365
Iteration 4/25 | Loss: 0.00093364
Iteration 5/25 | Loss: 0.00093364
Iteration 6/25 | Loss: 0.00093364
Iteration 7/25 | Loss: 0.00093364
Iteration 8/25 | Loss: 0.00093364
Iteration 9/25 | Loss: 0.00093364
Iteration 10/25 | Loss: 0.00093364
Iteration 11/25 | Loss: 0.00093364
Iteration 12/25 | Loss: 0.00093364
Iteration 13/25 | Loss: 0.00093364
Iteration 14/25 | Loss: 0.00093364
Iteration 15/25 | Loss: 0.00093364
Iteration 16/25 | Loss: 0.00093364
Iteration 17/25 | Loss: 0.00093364
Iteration 18/25 | Loss: 0.00093364
Iteration 19/25 | Loss: 0.00093364
Iteration 20/25 | Loss: 0.00093364
Iteration 21/25 | Loss: 0.00093364
Iteration 22/25 | Loss: 0.00093364
Iteration 23/25 | Loss: 0.00093364
Iteration 24/25 | Loss: 0.00093364
Iteration 25/25 | Loss: 0.00093364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093364
Iteration 2/1000 | Loss: 0.00002717
Iteration 3/1000 | Loss: 0.00002083
Iteration 4/1000 | Loss: 0.00001951
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001831
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001772
Iteration 9/1000 | Loss: 0.00001752
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001718
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001708
Iteration 21/1000 | Loss: 0.00001707
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001705
Iteration 25/1000 | Loss: 0.00001704
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001701
Iteration 30/1000 | Loss: 0.00001701
Iteration 31/1000 | Loss: 0.00001701
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001701
Iteration 34/1000 | Loss: 0.00001701
Iteration 35/1000 | Loss: 0.00001700
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001697
Iteration 39/1000 | Loss: 0.00001697
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001696
Iteration 43/1000 | Loss: 0.00001696
Iteration 44/1000 | Loss: 0.00001696
Iteration 45/1000 | Loss: 0.00001696
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001694
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001693
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001690
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001683
Iteration 70/1000 | Loss: 0.00001683
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001683
Iteration 73/1000 | Loss: 0.00001683
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001682
Iteration 76/1000 | Loss: 0.00001681
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001679
Iteration 81/1000 | Loss: 0.00001679
Iteration 82/1000 | Loss: 0.00001679
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001677
Iteration 86/1000 | Loss: 0.00001677
Iteration 87/1000 | Loss: 0.00001677
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001676
Iteration 91/1000 | Loss: 0.00001676
Iteration 92/1000 | Loss: 0.00001676
Iteration 93/1000 | Loss: 0.00001676
Iteration 94/1000 | Loss: 0.00001675
Iteration 95/1000 | Loss: 0.00001675
Iteration 96/1000 | Loss: 0.00001675
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001674
Iteration 99/1000 | Loss: 0.00001674
Iteration 100/1000 | Loss: 0.00001674
Iteration 101/1000 | Loss: 0.00001674
Iteration 102/1000 | Loss: 0.00001674
Iteration 103/1000 | Loss: 0.00001673
Iteration 104/1000 | Loss: 0.00001673
Iteration 105/1000 | Loss: 0.00001673
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001671
Iteration 114/1000 | Loss: 0.00001671
Iteration 115/1000 | Loss: 0.00001671
Iteration 116/1000 | Loss: 0.00001671
Iteration 117/1000 | Loss: 0.00001671
Iteration 118/1000 | Loss: 0.00001671
Iteration 119/1000 | Loss: 0.00001671
Iteration 120/1000 | Loss: 0.00001671
Iteration 121/1000 | Loss: 0.00001671
Iteration 122/1000 | Loss: 0.00001671
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001670
Iteration 133/1000 | Loss: 0.00001670
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001670
Iteration 139/1000 | Loss: 0.00001670
Iteration 140/1000 | Loss: 0.00001670
Iteration 141/1000 | Loss: 0.00001670
Iteration 142/1000 | Loss: 0.00001670
Iteration 143/1000 | Loss: 0.00001670
Iteration 144/1000 | Loss: 0.00001670
Iteration 145/1000 | Loss: 0.00001670
Iteration 146/1000 | Loss: 0.00001670
Iteration 147/1000 | Loss: 0.00001670
Iteration 148/1000 | Loss: 0.00001670
Iteration 149/1000 | Loss: 0.00001670
Iteration 150/1000 | Loss: 0.00001669
Iteration 151/1000 | Loss: 0.00001669
Iteration 152/1000 | Loss: 0.00001669
Iteration 153/1000 | Loss: 0.00001669
Iteration 154/1000 | Loss: 0.00001669
Iteration 155/1000 | Loss: 0.00001669
Iteration 156/1000 | Loss: 0.00001669
Iteration 157/1000 | Loss: 0.00001669
Iteration 158/1000 | Loss: 0.00001669
Iteration 159/1000 | Loss: 0.00001669
Iteration 160/1000 | Loss: 0.00001669
Iteration 161/1000 | Loss: 0.00001669
Iteration 162/1000 | Loss: 0.00001669
Iteration 163/1000 | Loss: 0.00001669
Iteration 164/1000 | Loss: 0.00001669
Iteration 165/1000 | Loss: 0.00001669
Iteration 166/1000 | Loss: 0.00001669
Iteration 167/1000 | Loss: 0.00001669
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001669
Iteration 170/1000 | Loss: 0.00001669
Iteration 171/1000 | Loss: 0.00001669
Iteration 172/1000 | Loss: 0.00001668
Iteration 173/1000 | Loss: 0.00001668
Iteration 174/1000 | Loss: 0.00001668
Iteration 175/1000 | Loss: 0.00001668
Iteration 176/1000 | Loss: 0.00001668
Iteration 177/1000 | Loss: 0.00001668
Iteration 178/1000 | Loss: 0.00001668
Iteration 179/1000 | Loss: 0.00001668
Iteration 180/1000 | Loss: 0.00001668
Iteration 181/1000 | Loss: 0.00001668
Iteration 182/1000 | Loss: 0.00001668
Iteration 183/1000 | Loss: 0.00001668
Iteration 184/1000 | Loss: 0.00001668
Iteration 185/1000 | Loss: 0.00001668
Iteration 186/1000 | Loss: 0.00001668
Iteration 187/1000 | Loss: 0.00001668
Iteration 188/1000 | Loss: 0.00001668
Iteration 189/1000 | Loss: 0.00001668
Iteration 190/1000 | Loss: 0.00001668
Iteration 191/1000 | Loss: 0.00001668
Iteration 192/1000 | Loss: 0.00001668
Iteration 193/1000 | Loss: 0.00001668
Iteration 194/1000 | Loss: 0.00001668
Iteration 195/1000 | Loss: 0.00001668
Iteration 196/1000 | Loss: 0.00001668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.668486038397532e-05, 1.668486038397532e-05, 1.668486038397532e-05, 1.668486038397532e-05, 1.668486038397532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.668486038397532e-05

Optimization complete. Final v2v error: 3.3472189903259277 mm

Highest mean error: 4.315643310546875 mm for frame 118

Lowest mean error: 2.931638717651367 mm for frame 157

Saving results

Total time: 56.05971813201904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720859
Iteration 2/25 | Loss: 0.00150482
Iteration 3/25 | Loss: 0.00134412
Iteration 4/25 | Loss: 0.00131392
Iteration 5/25 | Loss: 0.00130446
Iteration 6/25 | Loss: 0.00130196
Iteration 7/25 | Loss: 0.00130196
Iteration 8/25 | Loss: 0.00130196
Iteration 9/25 | Loss: 0.00130196
Iteration 10/25 | Loss: 0.00130196
Iteration 11/25 | Loss: 0.00130196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013019636971876025, 0.0013019636971876025, 0.0013019636971876025, 0.0013019636971876025, 0.0013019636971876025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013019636971876025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55803001
Iteration 2/25 | Loss: 0.00143209
Iteration 3/25 | Loss: 0.00143209
Iteration 4/25 | Loss: 0.00143208
Iteration 5/25 | Loss: 0.00143208
Iteration 6/25 | Loss: 0.00143208
Iteration 7/25 | Loss: 0.00143208
Iteration 8/25 | Loss: 0.00143208
Iteration 9/25 | Loss: 0.00143208
Iteration 10/25 | Loss: 0.00143208
Iteration 11/25 | Loss: 0.00143208
Iteration 12/25 | Loss: 0.00143208
Iteration 13/25 | Loss: 0.00143208
Iteration 14/25 | Loss: 0.00143208
Iteration 15/25 | Loss: 0.00143208
Iteration 16/25 | Loss: 0.00143208
Iteration 17/25 | Loss: 0.00143208
Iteration 18/25 | Loss: 0.00143208
Iteration 19/25 | Loss: 0.00143208
Iteration 20/25 | Loss: 0.00143208
Iteration 21/25 | Loss: 0.00143208
Iteration 22/25 | Loss: 0.00143208
Iteration 23/25 | Loss: 0.00143208
Iteration 24/25 | Loss: 0.00143208
Iteration 25/25 | Loss: 0.00143208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143208
Iteration 2/1000 | Loss: 0.00006711
Iteration 3/1000 | Loss: 0.00003957
Iteration 4/1000 | Loss: 0.00003078
Iteration 5/1000 | Loss: 0.00002760
Iteration 6/1000 | Loss: 0.00002577
Iteration 7/1000 | Loss: 0.00002464
Iteration 8/1000 | Loss: 0.00002373
Iteration 9/1000 | Loss: 0.00002313
Iteration 10/1000 | Loss: 0.00002271
Iteration 11/1000 | Loss: 0.00002230
Iteration 12/1000 | Loss: 0.00002198
Iteration 13/1000 | Loss: 0.00002178
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002151
Iteration 16/1000 | Loss: 0.00002144
Iteration 17/1000 | Loss: 0.00002138
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00002128
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00002127
Iteration 23/1000 | Loss: 0.00002127
Iteration 24/1000 | Loss: 0.00002126
Iteration 25/1000 | Loss: 0.00002124
Iteration 26/1000 | Loss: 0.00002124
Iteration 27/1000 | Loss: 0.00002122
Iteration 28/1000 | Loss: 0.00002122
Iteration 29/1000 | Loss: 0.00002121
Iteration 30/1000 | Loss: 0.00002121
Iteration 31/1000 | Loss: 0.00002120
Iteration 32/1000 | Loss: 0.00002120
Iteration 33/1000 | Loss: 0.00002119
Iteration 34/1000 | Loss: 0.00002119
Iteration 35/1000 | Loss: 0.00002119
Iteration 36/1000 | Loss: 0.00002118
Iteration 37/1000 | Loss: 0.00002118
Iteration 38/1000 | Loss: 0.00002118
Iteration 39/1000 | Loss: 0.00002118
Iteration 40/1000 | Loss: 0.00002117
Iteration 41/1000 | Loss: 0.00002117
Iteration 42/1000 | Loss: 0.00002117
Iteration 43/1000 | Loss: 0.00002117
Iteration 44/1000 | Loss: 0.00002116
Iteration 45/1000 | Loss: 0.00002116
Iteration 46/1000 | Loss: 0.00002115
Iteration 47/1000 | Loss: 0.00002114
Iteration 48/1000 | Loss: 0.00002114
Iteration 49/1000 | Loss: 0.00002114
Iteration 50/1000 | Loss: 0.00002113
Iteration 51/1000 | Loss: 0.00002113
Iteration 52/1000 | Loss: 0.00002113
Iteration 53/1000 | Loss: 0.00002112
Iteration 54/1000 | Loss: 0.00002112
Iteration 55/1000 | Loss: 0.00002111
Iteration 56/1000 | Loss: 0.00002110
Iteration 57/1000 | Loss: 0.00002110
Iteration 58/1000 | Loss: 0.00002110
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002110
Iteration 61/1000 | Loss: 0.00002109
Iteration 62/1000 | Loss: 0.00002109
Iteration 63/1000 | Loss: 0.00002109
Iteration 64/1000 | Loss: 0.00002109
Iteration 65/1000 | Loss: 0.00002109
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002108
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002108
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002107
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002106
Iteration 79/1000 | Loss: 0.00002106
Iteration 80/1000 | Loss: 0.00002106
Iteration 81/1000 | Loss: 0.00002105
Iteration 82/1000 | Loss: 0.00002105
Iteration 83/1000 | Loss: 0.00002105
Iteration 84/1000 | Loss: 0.00002105
Iteration 85/1000 | Loss: 0.00002104
Iteration 86/1000 | Loss: 0.00002104
Iteration 87/1000 | Loss: 0.00002104
Iteration 88/1000 | Loss: 0.00002104
Iteration 89/1000 | Loss: 0.00002104
Iteration 90/1000 | Loss: 0.00002104
Iteration 91/1000 | Loss: 0.00002104
Iteration 92/1000 | Loss: 0.00002103
Iteration 93/1000 | Loss: 0.00002103
Iteration 94/1000 | Loss: 0.00002103
Iteration 95/1000 | Loss: 0.00002103
Iteration 96/1000 | Loss: 0.00002103
Iteration 97/1000 | Loss: 0.00002103
Iteration 98/1000 | Loss: 0.00002103
Iteration 99/1000 | Loss: 0.00002103
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002102
Iteration 102/1000 | Loss: 0.00002102
Iteration 103/1000 | Loss: 0.00002102
Iteration 104/1000 | Loss: 0.00002102
Iteration 105/1000 | Loss: 0.00002102
Iteration 106/1000 | Loss: 0.00002101
Iteration 107/1000 | Loss: 0.00002101
Iteration 108/1000 | Loss: 0.00002101
Iteration 109/1000 | Loss: 0.00002101
Iteration 110/1000 | Loss: 0.00002101
Iteration 111/1000 | Loss: 0.00002100
Iteration 112/1000 | Loss: 0.00002100
Iteration 113/1000 | Loss: 0.00002100
Iteration 114/1000 | Loss: 0.00002099
Iteration 115/1000 | Loss: 0.00002099
Iteration 116/1000 | Loss: 0.00002099
Iteration 117/1000 | Loss: 0.00002098
Iteration 118/1000 | Loss: 0.00002098
Iteration 119/1000 | Loss: 0.00002098
Iteration 120/1000 | Loss: 0.00002098
Iteration 121/1000 | Loss: 0.00002098
Iteration 122/1000 | Loss: 0.00002098
Iteration 123/1000 | Loss: 0.00002097
Iteration 124/1000 | Loss: 0.00002097
Iteration 125/1000 | Loss: 0.00002097
Iteration 126/1000 | Loss: 0.00002096
Iteration 127/1000 | Loss: 0.00002096
Iteration 128/1000 | Loss: 0.00002096
Iteration 129/1000 | Loss: 0.00002096
Iteration 130/1000 | Loss: 0.00002096
Iteration 131/1000 | Loss: 0.00002095
Iteration 132/1000 | Loss: 0.00002095
Iteration 133/1000 | Loss: 0.00002095
Iteration 134/1000 | Loss: 0.00002095
Iteration 135/1000 | Loss: 0.00002095
Iteration 136/1000 | Loss: 0.00002095
Iteration 137/1000 | Loss: 0.00002095
Iteration 138/1000 | Loss: 0.00002095
Iteration 139/1000 | Loss: 0.00002095
Iteration 140/1000 | Loss: 0.00002094
Iteration 141/1000 | Loss: 0.00002094
Iteration 142/1000 | Loss: 0.00002094
Iteration 143/1000 | Loss: 0.00002094
Iteration 144/1000 | Loss: 0.00002094
Iteration 145/1000 | Loss: 0.00002094
Iteration 146/1000 | Loss: 0.00002094
Iteration 147/1000 | Loss: 0.00002094
Iteration 148/1000 | Loss: 0.00002094
Iteration 149/1000 | Loss: 0.00002094
Iteration 150/1000 | Loss: 0.00002094
Iteration 151/1000 | Loss: 0.00002093
Iteration 152/1000 | Loss: 0.00002093
Iteration 153/1000 | Loss: 0.00002093
Iteration 154/1000 | Loss: 0.00002093
Iteration 155/1000 | Loss: 0.00002093
Iteration 156/1000 | Loss: 0.00002093
Iteration 157/1000 | Loss: 0.00002093
Iteration 158/1000 | Loss: 0.00002093
Iteration 159/1000 | Loss: 0.00002093
Iteration 160/1000 | Loss: 0.00002093
Iteration 161/1000 | Loss: 0.00002093
Iteration 162/1000 | Loss: 0.00002093
Iteration 163/1000 | Loss: 0.00002093
Iteration 164/1000 | Loss: 0.00002093
Iteration 165/1000 | Loss: 0.00002092
Iteration 166/1000 | Loss: 0.00002092
Iteration 167/1000 | Loss: 0.00002092
Iteration 168/1000 | Loss: 0.00002092
Iteration 169/1000 | Loss: 0.00002091
Iteration 170/1000 | Loss: 0.00002091
Iteration 171/1000 | Loss: 0.00002091
Iteration 172/1000 | Loss: 0.00002091
Iteration 173/1000 | Loss: 0.00002091
Iteration 174/1000 | Loss: 0.00002091
Iteration 175/1000 | Loss: 0.00002091
Iteration 176/1000 | Loss: 0.00002091
Iteration 177/1000 | Loss: 0.00002090
Iteration 178/1000 | Loss: 0.00002090
Iteration 179/1000 | Loss: 0.00002090
Iteration 180/1000 | Loss: 0.00002090
Iteration 181/1000 | Loss: 0.00002090
Iteration 182/1000 | Loss: 0.00002090
Iteration 183/1000 | Loss: 0.00002089
Iteration 184/1000 | Loss: 0.00002089
Iteration 185/1000 | Loss: 0.00002089
Iteration 186/1000 | Loss: 0.00002089
Iteration 187/1000 | Loss: 0.00002089
Iteration 188/1000 | Loss: 0.00002089
Iteration 189/1000 | Loss: 0.00002089
Iteration 190/1000 | Loss: 0.00002089
Iteration 191/1000 | Loss: 0.00002089
Iteration 192/1000 | Loss: 0.00002089
Iteration 193/1000 | Loss: 0.00002089
Iteration 194/1000 | Loss: 0.00002089
Iteration 195/1000 | Loss: 0.00002089
Iteration 196/1000 | Loss: 0.00002089
Iteration 197/1000 | Loss: 0.00002089
Iteration 198/1000 | Loss: 0.00002089
Iteration 199/1000 | Loss: 0.00002089
Iteration 200/1000 | Loss: 0.00002089
Iteration 201/1000 | Loss: 0.00002089
Iteration 202/1000 | Loss: 0.00002089
Iteration 203/1000 | Loss: 0.00002089
Iteration 204/1000 | Loss: 0.00002089
Iteration 205/1000 | Loss: 0.00002089
Iteration 206/1000 | Loss: 0.00002089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.0887026039417833e-05, 2.0887026039417833e-05, 2.0887026039417833e-05, 2.0887026039417833e-05, 2.0887026039417833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0887026039417833e-05

Optimization complete. Final v2v error: 3.877908706665039 mm

Highest mean error: 5.256755828857422 mm for frame 65

Lowest mean error: 3.063464879989624 mm for frame 49

Saving results

Total time: 52.08203411102295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00687231
Iteration 2/25 | Loss: 0.00154839
Iteration 3/25 | Loss: 0.00134158
Iteration 4/25 | Loss: 0.00128773
Iteration 5/25 | Loss: 0.00127136
Iteration 6/25 | Loss: 0.00126625
Iteration 7/25 | Loss: 0.00126404
Iteration 8/25 | Loss: 0.00126113
Iteration 9/25 | Loss: 0.00125942
Iteration 10/25 | Loss: 0.00126383
Iteration 11/25 | Loss: 0.00125827
Iteration 12/25 | Loss: 0.00124635
Iteration 13/25 | Loss: 0.00124213
Iteration 14/25 | Loss: 0.00124119
Iteration 15/25 | Loss: 0.00124226
Iteration 16/25 | Loss: 0.00123946
Iteration 17/25 | Loss: 0.00123776
Iteration 18/25 | Loss: 0.00123742
Iteration 19/25 | Loss: 0.00123725
Iteration 20/25 | Loss: 0.00124237
Iteration 21/25 | Loss: 0.00123693
Iteration 22/25 | Loss: 0.00123455
Iteration 23/25 | Loss: 0.00123450
Iteration 24/25 | Loss: 0.00123386
Iteration 25/25 | Loss: 0.00123279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33364332
Iteration 2/25 | Loss: 0.00169603
Iteration 3/25 | Loss: 0.00169603
Iteration 4/25 | Loss: 0.00169603
Iteration 5/25 | Loss: 0.00169603
Iteration 6/25 | Loss: 0.00169603
Iteration 7/25 | Loss: 0.00169603
Iteration 8/25 | Loss: 0.00169603
Iteration 9/25 | Loss: 0.00169603
Iteration 10/25 | Loss: 0.00169603
Iteration 11/25 | Loss: 0.00169603
Iteration 12/25 | Loss: 0.00169603
Iteration 13/25 | Loss: 0.00169603
Iteration 14/25 | Loss: 0.00169603
Iteration 15/25 | Loss: 0.00169603
Iteration 16/25 | Loss: 0.00169603
Iteration 17/25 | Loss: 0.00169603
Iteration 18/25 | Loss: 0.00169603
Iteration 19/25 | Loss: 0.00169603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016960252542048693, 0.0016960252542048693, 0.0016960252542048693, 0.0016960252542048693, 0.0016960252542048693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016960252542048693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169603
Iteration 2/1000 | Loss: 0.00131151
Iteration 3/1000 | Loss: 0.00072446
Iteration 4/1000 | Loss: 0.00068621
Iteration 5/1000 | Loss: 0.00058159
Iteration 6/1000 | Loss: 0.00070037
Iteration 7/1000 | Loss: 0.00045153
Iteration 8/1000 | Loss: 0.00044680
Iteration 9/1000 | Loss: 0.00008333
Iteration 10/1000 | Loss: 0.00032016
Iteration 11/1000 | Loss: 0.00062766
Iteration 12/1000 | Loss: 0.00041039
Iteration 13/1000 | Loss: 0.00055299
Iteration 14/1000 | Loss: 0.00028362
Iteration 15/1000 | Loss: 0.00033965
Iteration 16/1000 | Loss: 0.00021295
Iteration 17/1000 | Loss: 0.00079119
Iteration 18/1000 | Loss: 0.00004723
Iteration 19/1000 | Loss: 0.00003398
Iteration 20/1000 | Loss: 0.00010871
Iteration 21/1000 | Loss: 0.00002943
Iteration 22/1000 | Loss: 0.00002716
Iteration 23/1000 | Loss: 0.00006366
Iteration 24/1000 | Loss: 0.00003271
Iteration 25/1000 | Loss: 0.00007387
Iteration 26/1000 | Loss: 0.00002600
Iteration 27/1000 | Loss: 0.00009269
Iteration 28/1000 | Loss: 0.00002975
Iteration 29/1000 | Loss: 0.00002463
Iteration 30/1000 | Loss: 0.00002286
Iteration 31/1000 | Loss: 0.00002191
Iteration 32/1000 | Loss: 0.00002122
Iteration 33/1000 | Loss: 0.00002071
Iteration 34/1000 | Loss: 0.00002043
Iteration 35/1000 | Loss: 0.00002012
Iteration 36/1000 | Loss: 0.00001986
Iteration 37/1000 | Loss: 0.00001981
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001916
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001915
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001905
Iteration 58/1000 | Loss: 0.00001904
Iteration 59/1000 | Loss: 0.00001904
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001899
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001897
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001896
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001895
Iteration 77/1000 | Loss: 0.00001895
Iteration 78/1000 | Loss: 0.00001895
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001895
Iteration 81/1000 | Loss: 0.00001895
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001895
Iteration 85/1000 | Loss: 0.00001895
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001894
Iteration 88/1000 | Loss: 0.00001894
Iteration 89/1000 | Loss: 0.00001894
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001890
Iteration 103/1000 | Loss: 0.00001890
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001889
Iteration 107/1000 | Loss: 0.00001889
Iteration 108/1000 | Loss: 0.00001889
Iteration 109/1000 | Loss: 0.00001889
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001888
Iteration 114/1000 | Loss: 0.00001888
Iteration 115/1000 | Loss: 0.00001888
Iteration 116/1000 | Loss: 0.00001888
Iteration 117/1000 | Loss: 0.00001887
Iteration 118/1000 | Loss: 0.00001887
Iteration 119/1000 | Loss: 0.00001887
Iteration 120/1000 | Loss: 0.00001887
Iteration 121/1000 | Loss: 0.00001887
Iteration 122/1000 | Loss: 0.00001887
Iteration 123/1000 | Loss: 0.00001887
Iteration 124/1000 | Loss: 0.00001887
Iteration 125/1000 | Loss: 0.00001886
Iteration 126/1000 | Loss: 0.00001886
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00001886
Iteration 129/1000 | Loss: 0.00001886
Iteration 130/1000 | Loss: 0.00001886
Iteration 131/1000 | Loss: 0.00001886
Iteration 132/1000 | Loss: 0.00001886
Iteration 133/1000 | Loss: 0.00001886
Iteration 134/1000 | Loss: 0.00001886
Iteration 135/1000 | Loss: 0.00001886
Iteration 136/1000 | Loss: 0.00001886
Iteration 137/1000 | Loss: 0.00001886
Iteration 138/1000 | Loss: 0.00001886
Iteration 139/1000 | Loss: 0.00001886
Iteration 140/1000 | Loss: 0.00001886
Iteration 141/1000 | Loss: 0.00001886
Iteration 142/1000 | Loss: 0.00001886
Iteration 143/1000 | Loss: 0.00001886
Iteration 144/1000 | Loss: 0.00001886
Iteration 145/1000 | Loss: 0.00001886
Iteration 146/1000 | Loss: 0.00001886
Iteration 147/1000 | Loss: 0.00001886
Iteration 148/1000 | Loss: 0.00001886
Iteration 149/1000 | Loss: 0.00001886
Iteration 150/1000 | Loss: 0.00001886
Iteration 151/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.8860991986002773e-05, 1.8860991986002773e-05, 1.8860991986002773e-05, 1.8860991986002773e-05, 1.8860991986002773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8860991986002773e-05

Optimization complete. Final v2v error: 3.6585612297058105 mm

Highest mean error: 5.115518093109131 mm for frame 139

Lowest mean error: 3.1005849838256836 mm for frame 197

Saving results

Total time: 120.33686017990112
