Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=228, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12768-12823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824480
Iteration 2/25 | Loss: 0.00140049
Iteration 3/25 | Loss: 0.00124716
Iteration 4/25 | Loss: 0.00123040
Iteration 5/25 | Loss: 0.00122756
Iteration 6/25 | Loss: 0.00122752
Iteration 7/25 | Loss: 0.00122752
Iteration 8/25 | Loss: 0.00122752
Iteration 9/25 | Loss: 0.00122752
Iteration 10/25 | Loss: 0.00122752
Iteration 11/25 | Loss: 0.00122752
Iteration 12/25 | Loss: 0.00122752
Iteration 13/25 | Loss: 0.00122752
Iteration 14/25 | Loss: 0.00122752
Iteration 15/25 | Loss: 0.00122752
Iteration 16/25 | Loss: 0.00122752
Iteration 17/25 | Loss: 0.00122752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012275249464437366, 0.0012275249464437366, 0.0012275249464437366, 0.0012275249464437366, 0.0012275249464437366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012275249464437366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11319339
Iteration 2/25 | Loss: 0.00361202
Iteration 3/25 | Loss: 0.00361202
Iteration 4/25 | Loss: 0.00361202
Iteration 5/25 | Loss: 0.00361202
Iteration 6/25 | Loss: 0.00361202
Iteration 7/25 | Loss: 0.00361202
Iteration 8/25 | Loss: 0.00361202
Iteration 9/25 | Loss: 0.00361202
Iteration 10/25 | Loss: 0.00361202
Iteration 11/25 | Loss: 0.00361202
Iteration 12/25 | Loss: 0.00361202
Iteration 13/25 | Loss: 0.00361202
Iteration 14/25 | Loss: 0.00361202
Iteration 15/25 | Loss: 0.00361202
Iteration 16/25 | Loss: 0.00361202
Iteration 17/25 | Loss: 0.00361202
Iteration 18/25 | Loss: 0.00361202
Iteration 19/25 | Loss: 0.00361202
Iteration 20/25 | Loss: 0.00361202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003612016560509801, 0.003612016560509801, 0.003612016560509801, 0.003612016560509801, 0.003612016560509801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003612016560509801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361202
Iteration 2/1000 | Loss: 0.00004833
Iteration 3/1000 | Loss: 0.00002530
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002019
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001545
Iteration 10/1000 | Loss: 0.00001472
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001394
Iteration 17/1000 | Loss: 0.00001392
Iteration 18/1000 | Loss: 0.00001392
Iteration 19/1000 | Loss: 0.00001390
Iteration 20/1000 | Loss: 0.00001388
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001383
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001381
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001376
Iteration 39/1000 | Loss: 0.00001373
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001366
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001364
Iteration 73/1000 | Loss: 0.00001364
Iteration 74/1000 | Loss: 0.00001364
Iteration 75/1000 | Loss: 0.00001364
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001364
Iteration 78/1000 | Loss: 0.00001364
Iteration 79/1000 | Loss: 0.00001364
Iteration 80/1000 | Loss: 0.00001364
Iteration 81/1000 | Loss: 0.00001364
Iteration 82/1000 | Loss: 0.00001364
Iteration 83/1000 | Loss: 0.00001364
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001364
Iteration 86/1000 | Loss: 0.00001364
Iteration 87/1000 | Loss: 0.00001364
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001364
Iteration 99/1000 | Loss: 0.00001364
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001364
Iteration 106/1000 | Loss: 0.00001364
Iteration 107/1000 | Loss: 0.00001364
Iteration 108/1000 | Loss: 0.00001364
Iteration 109/1000 | Loss: 0.00001364
Iteration 110/1000 | Loss: 0.00001364
Iteration 111/1000 | Loss: 0.00001364
Iteration 112/1000 | Loss: 0.00001364
Iteration 113/1000 | Loss: 0.00001364
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001364
Iteration 119/1000 | Loss: 0.00001364
Iteration 120/1000 | Loss: 0.00001364
Iteration 121/1000 | Loss: 0.00001364
Iteration 122/1000 | Loss: 0.00001364
Iteration 123/1000 | Loss: 0.00001364
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001364
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.3637918527820148e-05, 1.3637918527820148e-05, 1.3637918527820148e-05, 1.3637918527820148e-05, 1.3637918527820148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3637918527820148e-05

Optimization complete. Final v2v error: 3.269648313522339 mm

Highest mean error: 3.759875535964966 mm for frame 126

Lowest mean error: 2.9744603633880615 mm for frame 0

Saving results

Total time: 38.6494026184082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103979
Iteration 2/25 | Loss: 0.00285255
Iteration 3/25 | Loss: 0.00250008
Iteration 4/25 | Loss: 0.00211649
Iteration 5/25 | Loss: 0.00197824
Iteration 6/25 | Loss: 0.00192589
Iteration 7/25 | Loss: 0.00168370
Iteration 8/25 | Loss: 0.00152523
Iteration 9/25 | Loss: 0.00149745
Iteration 10/25 | Loss: 0.00144730
Iteration 11/25 | Loss: 0.00143858
Iteration 12/25 | Loss: 0.00143645
Iteration 13/25 | Loss: 0.00143507
Iteration 14/25 | Loss: 0.00142982
Iteration 15/25 | Loss: 0.00142743
Iteration 16/25 | Loss: 0.00142708
Iteration 17/25 | Loss: 0.00142698
Iteration 18/25 | Loss: 0.00142695
Iteration 19/25 | Loss: 0.00142695
Iteration 20/25 | Loss: 0.00142695
Iteration 21/25 | Loss: 0.00142695
Iteration 22/25 | Loss: 0.00142694
Iteration 23/25 | Loss: 0.00142694
Iteration 24/25 | Loss: 0.00142694
Iteration 25/25 | Loss: 0.00142694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08545280
Iteration 2/25 | Loss: 0.00565643
Iteration 3/25 | Loss: 0.00458186
Iteration 4/25 | Loss: 0.00458186
Iteration 5/25 | Loss: 0.00458186
Iteration 6/25 | Loss: 0.00458186
Iteration 7/25 | Loss: 0.00458186
Iteration 8/25 | Loss: 0.00458186
Iteration 9/25 | Loss: 0.00458186
Iteration 10/25 | Loss: 0.00458186
Iteration 11/25 | Loss: 0.00458186
Iteration 12/25 | Loss: 0.00458186
Iteration 13/25 | Loss: 0.00458186
Iteration 14/25 | Loss: 0.00458186
Iteration 15/25 | Loss: 0.00458186
Iteration 16/25 | Loss: 0.00458185
Iteration 17/25 | Loss: 0.00458185
Iteration 18/25 | Loss: 0.00458185
Iteration 19/25 | Loss: 0.00458185
Iteration 20/25 | Loss: 0.00458185
Iteration 21/25 | Loss: 0.00458185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0045818546786904335, 0.0045818546786904335, 0.0045818546786904335, 0.0045818546786904335, 0.0045818546786904335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045818546786904335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00458185
Iteration 2/1000 | Loss: 0.00045992
Iteration 3/1000 | Loss: 0.00151753
Iteration 4/1000 | Loss: 0.00251247
Iteration 5/1000 | Loss: 0.00023151
Iteration 6/1000 | Loss: 0.00020374
Iteration 7/1000 | Loss: 0.00027375
Iteration 8/1000 | Loss: 0.00020345
Iteration 9/1000 | Loss: 0.00016923
Iteration 10/1000 | Loss: 0.00014276
Iteration 11/1000 | Loss: 0.00067474
Iteration 12/1000 | Loss: 0.00032939
Iteration 13/1000 | Loss: 0.00048439
Iteration 14/1000 | Loss: 0.00013099
Iteration 15/1000 | Loss: 0.00019578
Iteration 16/1000 | Loss: 0.00016982
Iteration 17/1000 | Loss: 0.00055662
Iteration 18/1000 | Loss: 0.00174932
Iteration 19/1000 | Loss: 0.00149713
Iteration 20/1000 | Loss: 0.00139901
Iteration 21/1000 | Loss: 0.00026591
Iteration 22/1000 | Loss: 0.00037366
Iteration 23/1000 | Loss: 0.00011690
Iteration 24/1000 | Loss: 0.00025120
Iteration 25/1000 | Loss: 0.00023853
Iteration 26/1000 | Loss: 0.00013394
Iteration 27/1000 | Loss: 0.00012704
Iteration 28/1000 | Loss: 0.00010937
Iteration 29/1000 | Loss: 0.00027911
Iteration 30/1000 | Loss: 0.00010579
Iteration 31/1000 | Loss: 0.00031808
Iteration 32/1000 | Loss: 0.00015267
Iteration 33/1000 | Loss: 0.00010413
Iteration 34/1000 | Loss: 0.00028796
Iteration 35/1000 | Loss: 0.00163072
Iteration 36/1000 | Loss: 0.00029946
Iteration 37/1000 | Loss: 0.00012979
Iteration 38/1000 | Loss: 0.00021305
Iteration 39/1000 | Loss: 0.00011512
Iteration 40/1000 | Loss: 0.00010270
Iteration 41/1000 | Loss: 0.00010678
Iteration 42/1000 | Loss: 0.00010891
Iteration 43/1000 | Loss: 0.00009672
Iteration 44/1000 | Loss: 0.00009604
Iteration 45/1000 | Loss: 0.00009565
Iteration 46/1000 | Loss: 0.00014638
Iteration 47/1000 | Loss: 0.00009552
Iteration 48/1000 | Loss: 0.00016030
Iteration 49/1000 | Loss: 0.00029484
Iteration 50/1000 | Loss: 0.00012868
Iteration 51/1000 | Loss: 0.00010135
Iteration 52/1000 | Loss: 0.00009469
Iteration 53/1000 | Loss: 0.00025173
Iteration 54/1000 | Loss: 0.00013597
Iteration 55/1000 | Loss: 0.00009474
Iteration 56/1000 | Loss: 0.00009424
Iteration 57/1000 | Loss: 0.00009420
Iteration 58/1000 | Loss: 0.00009414
Iteration 59/1000 | Loss: 0.00009413
Iteration 60/1000 | Loss: 0.00009413
Iteration 61/1000 | Loss: 0.00016547
Iteration 62/1000 | Loss: 0.00010153
Iteration 63/1000 | Loss: 0.00009943
Iteration 64/1000 | Loss: 0.00009403
Iteration 65/1000 | Loss: 0.00009390
Iteration 66/1000 | Loss: 0.00009390
Iteration 67/1000 | Loss: 0.00009390
Iteration 68/1000 | Loss: 0.00009390
Iteration 69/1000 | Loss: 0.00009389
Iteration 70/1000 | Loss: 0.00009389
Iteration 71/1000 | Loss: 0.00009389
Iteration 72/1000 | Loss: 0.00009388
Iteration 73/1000 | Loss: 0.00009388
Iteration 74/1000 | Loss: 0.00009387
Iteration 75/1000 | Loss: 0.00009382
Iteration 76/1000 | Loss: 0.00009382
Iteration 77/1000 | Loss: 0.00009381
Iteration 78/1000 | Loss: 0.00009381
Iteration 79/1000 | Loss: 0.00009380
Iteration 80/1000 | Loss: 0.00009380
Iteration 81/1000 | Loss: 0.00009380
Iteration 82/1000 | Loss: 0.00009379
Iteration 83/1000 | Loss: 0.00009379
Iteration 84/1000 | Loss: 0.00009378
Iteration 85/1000 | Loss: 0.00009378
Iteration 86/1000 | Loss: 0.00009378
Iteration 87/1000 | Loss: 0.00009377
Iteration 88/1000 | Loss: 0.00009377
Iteration 89/1000 | Loss: 0.00009377
Iteration 90/1000 | Loss: 0.00009376
Iteration 91/1000 | Loss: 0.00009376
Iteration 92/1000 | Loss: 0.00009375
Iteration 93/1000 | Loss: 0.00009375
Iteration 94/1000 | Loss: 0.00009375
Iteration 95/1000 | Loss: 0.00009375
Iteration 96/1000 | Loss: 0.00009375
Iteration 97/1000 | Loss: 0.00009375
Iteration 98/1000 | Loss: 0.00009374
Iteration 99/1000 | Loss: 0.00009374
Iteration 100/1000 | Loss: 0.00009374
Iteration 101/1000 | Loss: 0.00009373
Iteration 102/1000 | Loss: 0.00009373
Iteration 103/1000 | Loss: 0.00009373
Iteration 104/1000 | Loss: 0.00009372
Iteration 105/1000 | Loss: 0.00009372
Iteration 106/1000 | Loss: 0.00009372
Iteration 107/1000 | Loss: 0.00009372
Iteration 108/1000 | Loss: 0.00009371
Iteration 109/1000 | Loss: 0.00009371
Iteration 110/1000 | Loss: 0.00009371
Iteration 111/1000 | Loss: 0.00009371
Iteration 112/1000 | Loss: 0.00009370
Iteration 113/1000 | Loss: 0.00009370
Iteration 114/1000 | Loss: 0.00009370
Iteration 115/1000 | Loss: 0.00009370
Iteration 116/1000 | Loss: 0.00009370
Iteration 117/1000 | Loss: 0.00009370
Iteration 118/1000 | Loss: 0.00009370
Iteration 119/1000 | Loss: 0.00009369
Iteration 120/1000 | Loss: 0.00009369
Iteration 121/1000 | Loss: 0.00009369
Iteration 122/1000 | Loss: 0.00009369
Iteration 123/1000 | Loss: 0.00009369
Iteration 124/1000 | Loss: 0.00009368
Iteration 125/1000 | Loss: 0.00009368
Iteration 126/1000 | Loss: 0.00009368
Iteration 127/1000 | Loss: 0.00009368
Iteration 128/1000 | Loss: 0.00009368
Iteration 129/1000 | Loss: 0.00009368
Iteration 130/1000 | Loss: 0.00009368
Iteration 131/1000 | Loss: 0.00009368
Iteration 132/1000 | Loss: 0.00009368
Iteration 133/1000 | Loss: 0.00009368
Iteration 134/1000 | Loss: 0.00009368
Iteration 135/1000 | Loss: 0.00009367
Iteration 136/1000 | Loss: 0.00009367
Iteration 137/1000 | Loss: 0.00009367
Iteration 138/1000 | Loss: 0.00009367
Iteration 139/1000 | Loss: 0.00009367
Iteration 140/1000 | Loss: 0.00009367
Iteration 141/1000 | Loss: 0.00009367
Iteration 142/1000 | Loss: 0.00009367
Iteration 143/1000 | Loss: 0.00009367
Iteration 144/1000 | Loss: 0.00009367
Iteration 145/1000 | Loss: 0.00009367
Iteration 146/1000 | Loss: 0.00009367
Iteration 147/1000 | Loss: 0.00009367
Iteration 148/1000 | Loss: 0.00009367
Iteration 149/1000 | Loss: 0.00009367
Iteration 150/1000 | Loss: 0.00009367
Iteration 151/1000 | Loss: 0.00009367
Iteration 152/1000 | Loss: 0.00009367
Iteration 153/1000 | Loss: 0.00009367
Iteration 154/1000 | Loss: 0.00009367
Iteration 155/1000 | Loss: 0.00009367
Iteration 156/1000 | Loss: 0.00009367
Iteration 157/1000 | Loss: 0.00009367
Iteration 158/1000 | Loss: 0.00009367
Iteration 159/1000 | Loss: 0.00009367
Iteration 160/1000 | Loss: 0.00009367
Iteration 161/1000 | Loss: 0.00009367
Iteration 162/1000 | Loss: 0.00009367
Iteration 163/1000 | Loss: 0.00009367
Iteration 164/1000 | Loss: 0.00009367
Iteration 165/1000 | Loss: 0.00009367
Iteration 166/1000 | Loss: 0.00009367
Iteration 167/1000 | Loss: 0.00009367
Iteration 168/1000 | Loss: 0.00009367
Iteration 169/1000 | Loss: 0.00009367
Iteration 170/1000 | Loss: 0.00009367
Iteration 171/1000 | Loss: 0.00009367
Iteration 172/1000 | Loss: 0.00009367
Iteration 173/1000 | Loss: 0.00009367
Iteration 174/1000 | Loss: 0.00009367
Iteration 175/1000 | Loss: 0.00009367
Iteration 176/1000 | Loss: 0.00009367
Iteration 177/1000 | Loss: 0.00009367
Iteration 178/1000 | Loss: 0.00009367
Iteration 179/1000 | Loss: 0.00009367
Iteration 180/1000 | Loss: 0.00009367
Iteration 181/1000 | Loss: 0.00009367
Iteration 182/1000 | Loss: 0.00009367
Iteration 183/1000 | Loss: 0.00009367
Iteration 184/1000 | Loss: 0.00009367
Iteration 185/1000 | Loss: 0.00009367
Iteration 186/1000 | Loss: 0.00009367
Iteration 187/1000 | Loss: 0.00009367
Iteration 188/1000 | Loss: 0.00009367
Iteration 189/1000 | Loss: 0.00009367
Iteration 190/1000 | Loss: 0.00009367
Iteration 191/1000 | Loss: 0.00009367
Iteration 192/1000 | Loss: 0.00009367
Iteration 193/1000 | Loss: 0.00009367
Iteration 194/1000 | Loss: 0.00009367
Iteration 195/1000 | Loss: 0.00009367
Iteration 196/1000 | Loss: 0.00009367
Iteration 197/1000 | Loss: 0.00009367
Iteration 198/1000 | Loss: 0.00009367
Iteration 199/1000 | Loss: 0.00009367
Iteration 200/1000 | Loss: 0.00009367
Iteration 201/1000 | Loss: 0.00009367
Iteration 202/1000 | Loss: 0.00009367
Iteration 203/1000 | Loss: 0.00009367
Iteration 204/1000 | Loss: 0.00009367
Iteration 205/1000 | Loss: 0.00009367
Iteration 206/1000 | Loss: 0.00009367
Iteration 207/1000 | Loss: 0.00009367
Iteration 208/1000 | Loss: 0.00009367
Iteration 209/1000 | Loss: 0.00009367
Iteration 210/1000 | Loss: 0.00009367
Iteration 211/1000 | Loss: 0.00009367
Iteration 212/1000 | Loss: 0.00009367
Iteration 213/1000 | Loss: 0.00009367
Iteration 214/1000 | Loss: 0.00009367
Iteration 215/1000 | Loss: 0.00009367
Iteration 216/1000 | Loss: 0.00009367
Iteration 217/1000 | Loss: 0.00009367
Iteration 218/1000 | Loss: 0.00009367
Iteration 219/1000 | Loss: 0.00009367
Iteration 220/1000 | Loss: 0.00009367
Iteration 221/1000 | Loss: 0.00009367
Iteration 222/1000 | Loss: 0.00009367
Iteration 223/1000 | Loss: 0.00009367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [9.366529411636293e-05, 9.366529411636293e-05, 9.366529411636293e-05, 9.366529411636293e-05, 9.366529411636293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.366529411636293e-05

Optimization complete. Final v2v error: 5.273889064788818 mm

Highest mean error: 10.888313293457031 mm for frame 29

Lowest mean error: 3.484304428100586 mm for frame 1

Saving results

Total time: 130.7165186405182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448302
Iteration 2/25 | Loss: 0.00133659
Iteration 3/25 | Loss: 0.00123610
Iteration 4/25 | Loss: 0.00122118
Iteration 5/25 | Loss: 0.00121656
Iteration 6/25 | Loss: 0.00121529
Iteration 7/25 | Loss: 0.00121529
Iteration 8/25 | Loss: 0.00121529
Iteration 9/25 | Loss: 0.00121529
Iteration 10/25 | Loss: 0.00121529
Iteration 11/25 | Loss: 0.00121529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012152945855632424, 0.0012152945855632424, 0.0012152945855632424, 0.0012152945855632424, 0.0012152945855632424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012152945855632424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13757992
Iteration 2/25 | Loss: 0.00328580
Iteration 3/25 | Loss: 0.00328580
Iteration 4/25 | Loss: 0.00328580
Iteration 5/25 | Loss: 0.00328580
Iteration 6/25 | Loss: 0.00328580
Iteration 7/25 | Loss: 0.00328580
Iteration 8/25 | Loss: 0.00328580
Iteration 9/25 | Loss: 0.00328580
Iteration 10/25 | Loss: 0.00328580
Iteration 11/25 | Loss: 0.00328580
Iteration 12/25 | Loss: 0.00328580
Iteration 13/25 | Loss: 0.00328580
Iteration 14/25 | Loss: 0.00328580
Iteration 15/25 | Loss: 0.00328580
Iteration 16/25 | Loss: 0.00328580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0032857991755008698, 0.0032857991755008698, 0.0032857991755008698, 0.0032857991755008698, 0.0032857991755008698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032857991755008698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00328580
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00002252
Iteration 4/1000 | Loss: 0.00002014
Iteration 5/1000 | Loss: 0.00001877
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001517
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001501
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001493
Iteration 20/1000 | Loss: 0.00001487
Iteration 21/1000 | Loss: 0.00001486
Iteration 22/1000 | Loss: 0.00001485
Iteration 23/1000 | Loss: 0.00001483
Iteration 24/1000 | Loss: 0.00001482
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00001481
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001480
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001477
Iteration 32/1000 | Loss: 0.00001476
Iteration 33/1000 | Loss: 0.00001476
Iteration 34/1000 | Loss: 0.00001475
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001472
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001471
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001464
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001462
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001459
Iteration 92/1000 | Loss: 0.00001459
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001452
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001451
Iteration 130/1000 | Loss: 0.00001451
Iteration 131/1000 | Loss: 0.00001451
Iteration 132/1000 | Loss: 0.00001451
Iteration 133/1000 | Loss: 0.00001451
Iteration 134/1000 | Loss: 0.00001451
Iteration 135/1000 | Loss: 0.00001451
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001450
Iteration 138/1000 | Loss: 0.00001450
Iteration 139/1000 | Loss: 0.00001450
Iteration 140/1000 | Loss: 0.00001450
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00001450
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001448
Iteration 146/1000 | Loss: 0.00001448
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001447
Iteration 149/1000 | Loss: 0.00001447
Iteration 150/1000 | Loss: 0.00001447
Iteration 151/1000 | Loss: 0.00001447
Iteration 152/1000 | Loss: 0.00001447
Iteration 153/1000 | Loss: 0.00001447
Iteration 154/1000 | Loss: 0.00001447
Iteration 155/1000 | Loss: 0.00001447
Iteration 156/1000 | Loss: 0.00001446
Iteration 157/1000 | Loss: 0.00001446
Iteration 158/1000 | Loss: 0.00001445
Iteration 159/1000 | Loss: 0.00001445
Iteration 160/1000 | Loss: 0.00001445
Iteration 161/1000 | Loss: 0.00001444
Iteration 162/1000 | Loss: 0.00001444
Iteration 163/1000 | Loss: 0.00001444
Iteration 164/1000 | Loss: 0.00001444
Iteration 165/1000 | Loss: 0.00001444
Iteration 166/1000 | Loss: 0.00001444
Iteration 167/1000 | Loss: 0.00001443
Iteration 168/1000 | Loss: 0.00001443
Iteration 169/1000 | Loss: 0.00001443
Iteration 170/1000 | Loss: 0.00001442
Iteration 171/1000 | Loss: 0.00001442
Iteration 172/1000 | Loss: 0.00001442
Iteration 173/1000 | Loss: 0.00001442
Iteration 174/1000 | Loss: 0.00001442
Iteration 175/1000 | Loss: 0.00001442
Iteration 176/1000 | Loss: 0.00001441
Iteration 177/1000 | Loss: 0.00001441
Iteration 178/1000 | Loss: 0.00001441
Iteration 179/1000 | Loss: 0.00001441
Iteration 180/1000 | Loss: 0.00001441
Iteration 181/1000 | Loss: 0.00001441
Iteration 182/1000 | Loss: 0.00001441
Iteration 183/1000 | Loss: 0.00001441
Iteration 184/1000 | Loss: 0.00001441
Iteration 185/1000 | Loss: 0.00001441
Iteration 186/1000 | Loss: 0.00001441
Iteration 187/1000 | Loss: 0.00001440
Iteration 188/1000 | Loss: 0.00001440
Iteration 189/1000 | Loss: 0.00001440
Iteration 190/1000 | Loss: 0.00001440
Iteration 191/1000 | Loss: 0.00001439
Iteration 192/1000 | Loss: 0.00001439
Iteration 193/1000 | Loss: 0.00001439
Iteration 194/1000 | Loss: 0.00001439
Iteration 195/1000 | Loss: 0.00001439
Iteration 196/1000 | Loss: 0.00001439
Iteration 197/1000 | Loss: 0.00001439
Iteration 198/1000 | Loss: 0.00001439
Iteration 199/1000 | Loss: 0.00001438
Iteration 200/1000 | Loss: 0.00001438
Iteration 201/1000 | Loss: 0.00001438
Iteration 202/1000 | Loss: 0.00001438
Iteration 203/1000 | Loss: 0.00001438
Iteration 204/1000 | Loss: 0.00001438
Iteration 205/1000 | Loss: 0.00001438
Iteration 206/1000 | Loss: 0.00001438
Iteration 207/1000 | Loss: 0.00001438
Iteration 208/1000 | Loss: 0.00001438
Iteration 209/1000 | Loss: 0.00001437
Iteration 210/1000 | Loss: 0.00001437
Iteration 211/1000 | Loss: 0.00001437
Iteration 212/1000 | Loss: 0.00001437
Iteration 213/1000 | Loss: 0.00001436
Iteration 214/1000 | Loss: 0.00001436
Iteration 215/1000 | Loss: 0.00001436
Iteration 216/1000 | Loss: 0.00001436
Iteration 217/1000 | Loss: 0.00001436
Iteration 218/1000 | Loss: 0.00001436
Iteration 219/1000 | Loss: 0.00001436
Iteration 220/1000 | Loss: 0.00001436
Iteration 221/1000 | Loss: 0.00001436
Iteration 222/1000 | Loss: 0.00001436
Iteration 223/1000 | Loss: 0.00001436
Iteration 224/1000 | Loss: 0.00001436
Iteration 225/1000 | Loss: 0.00001436
Iteration 226/1000 | Loss: 0.00001436
Iteration 227/1000 | Loss: 0.00001435
Iteration 228/1000 | Loss: 0.00001435
Iteration 229/1000 | Loss: 0.00001435
Iteration 230/1000 | Loss: 0.00001435
Iteration 231/1000 | Loss: 0.00001435
Iteration 232/1000 | Loss: 0.00001435
Iteration 233/1000 | Loss: 0.00001435
Iteration 234/1000 | Loss: 0.00001435
Iteration 235/1000 | Loss: 0.00001435
Iteration 236/1000 | Loss: 0.00001435
Iteration 237/1000 | Loss: 0.00001435
Iteration 238/1000 | Loss: 0.00001435
Iteration 239/1000 | Loss: 0.00001435
Iteration 240/1000 | Loss: 0.00001435
Iteration 241/1000 | Loss: 0.00001435
Iteration 242/1000 | Loss: 0.00001435
Iteration 243/1000 | Loss: 0.00001435
Iteration 244/1000 | Loss: 0.00001435
Iteration 245/1000 | Loss: 0.00001435
Iteration 246/1000 | Loss: 0.00001435
Iteration 247/1000 | Loss: 0.00001435
Iteration 248/1000 | Loss: 0.00001435
Iteration 249/1000 | Loss: 0.00001435
Iteration 250/1000 | Loss: 0.00001435
Iteration 251/1000 | Loss: 0.00001435
Iteration 252/1000 | Loss: 0.00001435
Iteration 253/1000 | Loss: 0.00001435
Iteration 254/1000 | Loss: 0.00001435
Iteration 255/1000 | Loss: 0.00001435
Iteration 256/1000 | Loss: 0.00001435
Iteration 257/1000 | Loss: 0.00001435
Iteration 258/1000 | Loss: 0.00001435
Iteration 259/1000 | Loss: 0.00001435
Iteration 260/1000 | Loss: 0.00001435
Iteration 261/1000 | Loss: 0.00001435
Iteration 262/1000 | Loss: 0.00001435
Iteration 263/1000 | Loss: 0.00001435
Iteration 264/1000 | Loss: 0.00001435
Iteration 265/1000 | Loss: 0.00001435
Iteration 266/1000 | Loss: 0.00001435
Iteration 267/1000 | Loss: 0.00001435
Iteration 268/1000 | Loss: 0.00001435
Iteration 269/1000 | Loss: 0.00001435
Iteration 270/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.4354021914186887e-05, 1.4354021914186887e-05, 1.4354021914186887e-05, 1.4354021914186887e-05, 1.4354021914186887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4354021914186887e-05

Optimization complete. Final v2v error: 3.341057300567627 mm

Highest mean error: 3.998765230178833 mm for frame 93

Lowest mean error: 2.7428038120269775 mm for frame 1

Saving results

Total time: 50.32240271568298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876604
Iteration 2/25 | Loss: 0.00136660
Iteration 3/25 | Loss: 0.00122840
Iteration 4/25 | Loss: 0.00121500
Iteration 5/25 | Loss: 0.00121214
Iteration 6/25 | Loss: 0.00121180
Iteration 7/25 | Loss: 0.00121180
Iteration 8/25 | Loss: 0.00121180
Iteration 9/25 | Loss: 0.00121180
Iteration 10/25 | Loss: 0.00121180
Iteration 11/25 | Loss: 0.00121180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001211802358739078, 0.001211802358739078, 0.001211802358739078, 0.001211802358739078, 0.001211802358739078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001211802358739078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18722248
Iteration 2/25 | Loss: 0.00341799
Iteration 3/25 | Loss: 0.00341799
Iteration 4/25 | Loss: 0.00341799
Iteration 5/25 | Loss: 0.00341799
Iteration 6/25 | Loss: 0.00341799
Iteration 7/25 | Loss: 0.00341799
Iteration 8/25 | Loss: 0.00341799
Iteration 9/25 | Loss: 0.00341799
Iteration 10/25 | Loss: 0.00341799
Iteration 11/25 | Loss: 0.00341799
Iteration 12/25 | Loss: 0.00341799
Iteration 13/25 | Loss: 0.00341799
Iteration 14/25 | Loss: 0.00341799
Iteration 15/25 | Loss: 0.00341799
Iteration 16/25 | Loss: 0.00341799
Iteration 17/25 | Loss: 0.00341799
Iteration 18/25 | Loss: 0.00341799
Iteration 19/25 | Loss: 0.00341799
Iteration 20/25 | Loss: 0.00341799
Iteration 21/25 | Loss: 0.00341799
Iteration 22/25 | Loss: 0.00341799
Iteration 23/25 | Loss: 0.00341799
Iteration 24/25 | Loss: 0.00341799
Iteration 25/25 | Loss: 0.00341799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00341799
Iteration 2/1000 | Loss: 0.00005678
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00001865
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001630
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001389
Iteration 17/1000 | Loss: 0.00001385
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001382
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001380
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001364
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001361
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001360
Iteration 34/1000 | Loss: 0.00001360
Iteration 35/1000 | Loss: 0.00001356
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001351
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001350
Iteration 49/1000 | Loss: 0.00001350
Iteration 50/1000 | Loss: 0.00001350
Iteration 51/1000 | Loss: 0.00001350
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001349
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001347
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001346
Iteration 67/1000 | Loss: 0.00001346
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001345
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001345
Iteration 72/1000 | Loss: 0.00001345
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001344
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001343
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001342
Iteration 81/1000 | Loss: 0.00001342
Iteration 82/1000 | Loss: 0.00001342
Iteration 83/1000 | Loss: 0.00001342
Iteration 84/1000 | Loss: 0.00001342
Iteration 85/1000 | Loss: 0.00001341
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001340
Iteration 88/1000 | Loss: 0.00001340
Iteration 89/1000 | Loss: 0.00001340
Iteration 90/1000 | Loss: 0.00001340
Iteration 91/1000 | Loss: 0.00001340
Iteration 92/1000 | Loss: 0.00001340
Iteration 93/1000 | Loss: 0.00001340
Iteration 94/1000 | Loss: 0.00001340
Iteration 95/1000 | Loss: 0.00001340
Iteration 96/1000 | Loss: 0.00001339
Iteration 97/1000 | Loss: 0.00001339
Iteration 98/1000 | Loss: 0.00001339
Iteration 99/1000 | Loss: 0.00001339
Iteration 100/1000 | Loss: 0.00001339
Iteration 101/1000 | Loss: 0.00001339
Iteration 102/1000 | Loss: 0.00001339
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001339
Iteration 105/1000 | Loss: 0.00001339
Iteration 106/1000 | Loss: 0.00001339
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001338
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001337
Iteration 115/1000 | Loss: 0.00001337
Iteration 116/1000 | Loss: 0.00001337
Iteration 117/1000 | Loss: 0.00001337
Iteration 118/1000 | Loss: 0.00001337
Iteration 119/1000 | Loss: 0.00001337
Iteration 120/1000 | Loss: 0.00001337
Iteration 121/1000 | Loss: 0.00001336
Iteration 122/1000 | Loss: 0.00001336
Iteration 123/1000 | Loss: 0.00001336
Iteration 124/1000 | Loss: 0.00001336
Iteration 125/1000 | Loss: 0.00001336
Iteration 126/1000 | Loss: 0.00001336
Iteration 127/1000 | Loss: 0.00001336
Iteration 128/1000 | Loss: 0.00001335
Iteration 129/1000 | Loss: 0.00001335
Iteration 130/1000 | Loss: 0.00001334
Iteration 131/1000 | Loss: 0.00001334
Iteration 132/1000 | Loss: 0.00001334
Iteration 133/1000 | Loss: 0.00001334
Iteration 134/1000 | Loss: 0.00001333
Iteration 135/1000 | Loss: 0.00001333
Iteration 136/1000 | Loss: 0.00001333
Iteration 137/1000 | Loss: 0.00001333
Iteration 138/1000 | Loss: 0.00001333
Iteration 139/1000 | Loss: 0.00001332
Iteration 140/1000 | Loss: 0.00001332
Iteration 141/1000 | Loss: 0.00001332
Iteration 142/1000 | Loss: 0.00001331
Iteration 143/1000 | Loss: 0.00001331
Iteration 144/1000 | Loss: 0.00001330
Iteration 145/1000 | Loss: 0.00001330
Iteration 146/1000 | Loss: 0.00001330
Iteration 147/1000 | Loss: 0.00001330
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001329
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001328
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001327
Iteration 159/1000 | Loss: 0.00001327
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.3269780538394116e-05, 1.3269780538394116e-05, 1.3269780538394116e-05, 1.3269780538394116e-05, 1.3269780538394116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3269780538394116e-05

Optimization complete. Final v2v error: 3.098065137863159 mm

Highest mean error: 4.038105010986328 mm for frame 111

Lowest mean error: 2.664731502532959 mm for frame 0

Saving results

Total time: 45.01883625984192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042962
Iteration 2/25 | Loss: 0.01042962
Iteration 3/25 | Loss: 0.01042961
Iteration 4/25 | Loss: 0.01042961
Iteration 5/25 | Loss: 0.01042961
Iteration 6/25 | Loss: 0.01042961
Iteration 7/25 | Loss: 0.01042960
Iteration 8/25 | Loss: 0.01042960
Iteration 9/25 | Loss: 0.01042960
Iteration 10/25 | Loss: 0.01042960
Iteration 11/25 | Loss: 0.01042959
Iteration 12/25 | Loss: 0.01042959
Iteration 13/25 | Loss: 0.01042959
Iteration 14/25 | Loss: 0.01042958
Iteration 15/25 | Loss: 0.01042958
Iteration 16/25 | Loss: 0.01042958
Iteration 17/25 | Loss: 0.01042958
Iteration 18/25 | Loss: 0.01042957
Iteration 19/25 | Loss: 0.01042957
Iteration 20/25 | Loss: 0.01042957
Iteration 21/25 | Loss: 0.01042957
Iteration 22/25 | Loss: 0.01042956
Iteration 23/25 | Loss: 0.01042956
Iteration 24/25 | Loss: 0.01042956
Iteration 25/25 | Loss: 0.01042956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76479208
Iteration 2/25 | Loss: 0.19625115
Iteration 3/25 | Loss: 0.19624095
Iteration 4/25 | Loss: 0.19624081
Iteration 5/25 | Loss: 0.19624081
Iteration 6/25 | Loss: 0.19624081
Iteration 7/25 | Loss: 0.19624081
Iteration 8/25 | Loss: 0.19624081
Iteration 9/25 | Loss: 0.19624081
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.19624081254005432, 0.19624081254005432, 0.19624081254005432, 0.19624081254005432, 0.19624081254005432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.19624081254005432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.19624081
Iteration 2/1000 | Loss: 0.00523275
Iteration 3/1000 | Loss: 0.00550559
Iteration 4/1000 | Loss: 0.00049556
Iteration 5/1000 | Loss: 0.00029015
Iteration 6/1000 | Loss: 0.00021203
Iteration 7/1000 | Loss: 0.00016108
Iteration 8/1000 | Loss: 0.00013188
Iteration 9/1000 | Loss: 0.00015235
Iteration 10/1000 | Loss: 0.00011100
Iteration 11/1000 | Loss: 0.00010239
Iteration 12/1000 | Loss: 0.00009659
Iteration 13/1000 | Loss: 0.00008911
Iteration 14/1000 | Loss: 0.00018321
Iteration 15/1000 | Loss: 0.00020101
Iteration 16/1000 | Loss: 0.00008564
Iteration 17/1000 | Loss: 0.00007700
Iteration 18/1000 | Loss: 0.00007239
Iteration 19/1000 | Loss: 0.00006744
Iteration 20/1000 | Loss: 0.00006175
Iteration 21/1000 | Loss: 0.00005938
Iteration 22/1000 | Loss: 0.00005797
Iteration 23/1000 | Loss: 0.00027965
Iteration 24/1000 | Loss: 0.00006526
Iteration 25/1000 | Loss: 0.00006058
Iteration 26/1000 | Loss: 0.00005789
Iteration 27/1000 | Loss: 0.00005555
Iteration 28/1000 | Loss: 0.00005301
Iteration 29/1000 | Loss: 0.00005203
Iteration 30/1000 | Loss: 0.00005147
Iteration 31/1000 | Loss: 0.00005098
Iteration 32/1000 | Loss: 0.00005059
Iteration 33/1000 | Loss: 0.00005023
Iteration 34/1000 | Loss: 0.00004979
Iteration 35/1000 | Loss: 0.00004929
Iteration 36/1000 | Loss: 0.00004879
Iteration 37/1000 | Loss: 0.00004842
Iteration 38/1000 | Loss: 0.00004808
Iteration 39/1000 | Loss: 0.00004785
Iteration 40/1000 | Loss: 0.00004765
Iteration 41/1000 | Loss: 0.00004740
Iteration 42/1000 | Loss: 0.00004737
Iteration 43/1000 | Loss: 0.00004714
Iteration 44/1000 | Loss: 0.00004711
Iteration 45/1000 | Loss: 0.00004691
Iteration 46/1000 | Loss: 0.00004679
Iteration 47/1000 | Loss: 0.00004677
Iteration 48/1000 | Loss: 0.00004677
Iteration 49/1000 | Loss: 0.00004677
Iteration 50/1000 | Loss: 0.00004662
Iteration 51/1000 | Loss: 0.00004659
Iteration 52/1000 | Loss: 0.00004659
Iteration 53/1000 | Loss: 0.00004658
Iteration 54/1000 | Loss: 0.00004658
Iteration 55/1000 | Loss: 0.00004657
Iteration 56/1000 | Loss: 0.00004656
Iteration 57/1000 | Loss: 0.00004650
Iteration 58/1000 | Loss: 0.00004646
Iteration 59/1000 | Loss: 0.00004646
Iteration 60/1000 | Loss: 0.00004644
Iteration 61/1000 | Loss: 0.00004644
Iteration 62/1000 | Loss: 0.00004642
Iteration 63/1000 | Loss: 0.00004642
Iteration 64/1000 | Loss: 0.00004641
Iteration 65/1000 | Loss: 0.00004641
Iteration 66/1000 | Loss: 0.00004641
Iteration 67/1000 | Loss: 0.00004641
Iteration 68/1000 | Loss: 0.00004641
Iteration 69/1000 | Loss: 0.00004641
Iteration 70/1000 | Loss: 0.00004641
Iteration 71/1000 | Loss: 0.00004641
Iteration 72/1000 | Loss: 0.00004641
Iteration 73/1000 | Loss: 0.00004641
Iteration 74/1000 | Loss: 0.00004640
Iteration 75/1000 | Loss: 0.00004639
Iteration 76/1000 | Loss: 0.00004639
Iteration 77/1000 | Loss: 0.00004639
Iteration 78/1000 | Loss: 0.00004639
Iteration 79/1000 | Loss: 0.00004639
Iteration 80/1000 | Loss: 0.00004639
Iteration 81/1000 | Loss: 0.00004639
Iteration 82/1000 | Loss: 0.00004639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [4.638609243556857e-05, 4.638609243556857e-05, 4.638609243556857e-05, 4.638609243556857e-05, 4.638609243556857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.638609243556857e-05

Optimization complete. Final v2v error: 4.612425327301025 mm

Highest mean error: 21.253875732421875 mm for frame 167

Lowest mean error: 3.720839023590088 mm for frame 28

Saving results

Total time: 86.72753739356995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104217
Iteration 2/25 | Loss: 0.00387694
Iteration 3/25 | Loss: 0.00294463
Iteration 4/25 | Loss: 0.00285980
Iteration 5/25 | Loss: 0.00246734
Iteration 6/25 | Loss: 0.00226148
Iteration 7/25 | Loss: 0.00210447
Iteration 8/25 | Loss: 0.00202956
Iteration 9/25 | Loss: 0.00192473
Iteration 10/25 | Loss: 0.00185097
Iteration 11/25 | Loss: 0.00181833
Iteration 12/25 | Loss: 0.00177647
Iteration 13/25 | Loss: 0.00178255
Iteration 14/25 | Loss: 0.00177870
Iteration 15/25 | Loss: 0.00176163
Iteration 16/25 | Loss: 0.00173935
Iteration 17/25 | Loss: 0.00172945
Iteration 18/25 | Loss: 0.00171235
Iteration 19/25 | Loss: 0.00169623
Iteration 20/25 | Loss: 0.00168129
Iteration 21/25 | Loss: 0.00167311
Iteration 22/25 | Loss: 0.00167211
Iteration 23/25 | Loss: 0.00168022
Iteration 24/25 | Loss: 0.00167210
Iteration 25/25 | Loss: 0.00166487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98342347
Iteration 2/25 | Loss: 0.00699872
Iteration 3/25 | Loss: 0.00687809
Iteration 4/25 | Loss: 0.00687809
Iteration 5/25 | Loss: 0.00687809
Iteration 6/25 | Loss: 0.00687809
Iteration 7/25 | Loss: 0.00687809
Iteration 8/25 | Loss: 0.00687809
Iteration 9/25 | Loss: 0.00687809
Iteration 10/25 | Loss: 0.00687809
Iteration 11/25 | Loss: 0.00687809
Iteration 12/25 | Loss: 0.00687808
Iteration 13/25 | Loss: 0.00687808
Iteration 14/25 | Loss: 0.00687808
Iteration 15/25 | Loss: 0.00687808
Iteration 16/25 | Loss: 0.00687809
Iteration 17/25 | Loss: 0.00687808
Iteration 18/25 | Loss: 0.00687808
Iteration 19/25 | Loss: 0.00687808
Iteration 20/25 | Loss: 0.00687808
Iteration 21/25 | Loss: 0.00687808
Iteration 22/25 | Loss: 0.00687808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.006878084968775511, 0.006878084968775511, 0.006878084968775511, 0.006878084968775511, 0.006878084968775511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006878084968775511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00687808
Iteration 2/1000 | Loss: 0.00082922
Iteration 3/1000 | Loss: 0.00136045
Iteration 4/1000 | Loss: 0.00073802
Iteration 5/1000 | Loss: 0.00065083
Iteration 6/1000 | Loss: 0.00072711
Iteration 7/1000 | Loss: 0.00083199
Iteration 8/1000 | Loss: 0.00101367
Iteration 9/1000 | Loss: 0.00065290
Iteration 10/1000 | Loss: 0.00055244
Iteration 11/1000 | Loss: 0.00069633
Iteration 12/1000 | Loss: 0.00078720
Iteration 13/1000 | Loss: 0.00073066
Iteration 14/1000 | Loss: 0.00091235
Iteration 15/1000 | Loss: 0.00088839
Iteration 16/1000 | Loss: 0.00069306
Iteration 17/1000 | Loss: 0.00063532
Iteration 18/1000 | Loss: 0.00064807
Iteration 19/1000 | Loss: 0.00090445
Iteration 20/1000 | Loss: 0.00077525
Iteration 21/1000 | Loss: 0.00073812
Iteration 22/1000 | Loss: 0.00094219
Iteration 23/1000 | Loss: 0.00059765
Iteration 24/1000 | Loss: 0.00064440
Iteration 25/1000 | Loss: 0.00058199
Iteration 26/1000 | Loss: 0.00038316
Iteration 27/1000 | Loss: 0.00092019
Iteration 28/1000 | Loss: 0.00039350
Iteration 29/1000 | Loss: 0.00061806
Iteration 30/1000 | Loss: 0.00075494
Iteration 31/1000 | Loss: 0.00060414
Iteration 32/1000 | Loss: 0.00058951
Iteration 33/1000 | Loss: 0.00050515
Iteration 34/1000 | Loss: 0.00031793
Iteration 35/1000 | Loss: 0.00044153
Iteration 36/1000 | Loss: 0.00034599
Iteration 37/1000 | Loss: 0.00031283
Iteration 38/1000 | Loss: 0.00025089
Iteration 39/1000 | Loss: 0.00038831
Iteration 40/1000 | Loss: 0.00075321
Iteration 41/1000 | Loss: 0.00037091
Iteration 42/1000 | Loss: 0.00058739
Iteration 43/1000 | Loss: 0.00035639
Iteration 44/1000 | Loss: 0.00036927
Iteration 45/1000 | Loss: 0.00034614
Iteration 46/1000 | Loss: 0.00026202
Iteration 47/1000 | Loss: 0.00024553
Iteration 48/1000 | Loss: 0.00035022
Iteration 49/1000 | Loss: 0.00033251
Iteration 50/1000 | Loss: 0.00031063
Iteration 51/1000 | Loss: 0.00026511
Iteration 52/1000 | Loss: 0.00026992
Iteration 53/1000 | Loss: 0.00028450
Iteration 54/1000 | Loss: 0.00022758
Iteration 55/1000 | Loss: 0.00037138
Iteration 56/1000 | Loss: 0.00032910
Iteration 57/1000 | Loss: 0.00034933
Iteration 58/1000 | Loss: 0.00032093
Iteration 59/1000 | Loss: 0.00025972
Iteration 60/1000 | Loss: 0.00030529
Iteration 61/1000 | Loss: 0.00020983
Iteration 62/1000 | Loss: 0.00021394
Iteration 63/1000 | Loss: 0.00036531
Iteration 64/1000 | Loss: 0.00033357
Iteration 65/1000 | Loss: 0.00060335
Iteration 66/1000 | Loss: 0.00095629
Iteration 67/1000 | Loss: 0.00115328
Iteration 68/1000 | Loss: 0.00332282
Iteration 69/1000 | Loss: 0.00294966
Iteration 70/1000 | Loss: 0.00132674
Iteration 71/1000 | Loss: 0.00065477
Iteration 72/1000 | Loss: 0.00045656
Iteration 73/1000 | Loss: 0.00028599
Iteration 74/1000 | Loss: 0.00024505
Iteration 75/1000 | Loss: 0.00022292
Iteration 76/1000 | Loss: 0.00016424
Iteration 77/1000 | Loss: 0.00033716
Iteration 78/1000 | Loss: 0.00052378
Iteration 79/1000 | Loss: 0.00023165
Iteration 80/1000 | Loss: 0.00015216
Iteration 81/1000 | Loss: 0.00018191
Iteration 82/1000 | Loss: 0.00014311
Iteration 83/1000 | Loss: 0.00015655
Iteration 84/1000 | Loss: 0.00014300
Iteration 85/1000 | Loss: 0.00013176
Iteration 86/1000 | Loss: 0.00024029
Iteration 87/1000 | Loss: 0.00070137
Iteration 88/1000 | Loss: 0.00072569
Iteration 89/1000 | Loss: 0.00024724
Iteration 90/1000 | Loss: 0.00021544
Iteration 91/1000 | Loss: 0.00013226
Iteration 92/1000 | Loss: 0.00024325
Iteration 93/1000 | Loss: 0.00058865
Iteration 94/1000 | Loss: 0.00034253
Iteration 95/1000 | Loss: 0.00030699
Iteration 96/1000 | Loss: 0.00013089
Iteration 97/1000 | Loss: 0.00013820
Iteration 98/1000 | Loss: 0.00019544
Iteration 99/1000 | Loss: 0.00024895
Iteration 100/1000 | Loss: 0.00017540
Iteration 101/1000 | Loss: 0.00023788
Iteration 102/1000 | Loss: 0.00014191
Iteration 103/1000 | Loss: 0.00013633
Iteration 104/1000 | Loss: 0.00015437
Iteration 105/1000 | Loss: 0.00021878
Iteration 106/1000 | Loss: 0.00040759
Iteration 107/1000 | Loss: 0.00014029
Iteration 108/1000 | Loss: 0.00013953
Iteration 109/1000 | Loss: 0.00013074
Iteration 110/1000 | Loss: 0.00028133
Iteration 111/1000 | Loss: 0.00013047
Iteration 112/1000 | Loss: 0.00013384
Iteration 113/1000 | Loss: 0.00016915
Iteration 114/1000 | Loss: 0.00018083
Iteration 115/1000 | Loss: 0.00018158
Iteration 116/1000 | Loss: 0.00023449
Iteration 117/1000 | Loss: 0.00034312
Iteration 118/1000 | Loss: 0.00036594
Iteration 119/1000 | Loss: 0.00028711
Iteration 120/1000 | Loss: 0.00022749
Iteration 121/1000 | Loss: 0.00013304
Iteration 122/1000 | Loss: 0.00012131
Iteration 123/1000 | Loss: 0.00012823
Iteration 124/1000 | Loss: 0.00013029
Iteration 125/1000 | Loss: 0.00012481
Iteration 126/1000 | Loss: 0.00012844
Iteration 127/1000 | Loss: 0.00011939
Iteration 128/1000 | Loss: 0.00013943
Iteration 129/1000 | Loss: 0.00013019
Iteration 130/1000 | Loss: 0.00012665
Iteration 131/1000 | Loss: 0.00013036
Iteration 132/1000 | Loss: 0.00013959
Iteration 133/1000 | Loss: 0.00032094
Iteration 134/1000 | Loss: 0.00018172
Iteration 135/1000 | Loss: 0.00020142
Iteration 136/1000 | Loss: 0.00012629
Iteration 137/1000 | Loss: 0.00013416
Iteration 138/1000 | Loss: 0.00011552
Iteration 139/1000 | Loss: 0.00013361
Iteration 140/1000 | Loss: 0.00013108
Iteration 141/1000 | Loss: 0.00013472
Iteration 142/1000 | Loss: 0.00013318
Iteration 143/1000 | Loss: 0.00013647
Iteration 144/1000 | Loss: 0.00012455
Iteration 145/1000 | Loss: 0.00015602
Iteration 146/1000 | Loss: 0.00013491
Iteration 147/1000 | Loss: 0.00012845
Iteration 148/1000 | Loss: 0.00013571
Iteration 149/1000 | Loss: 0.00013921
Iteration 150/1000 | Loss: 0.00012673
Iteration 151/1000 | Loss: 0.00014061
Iteration 152/1000 | Loss: 0.00012915
Iteration 153/1000 | Loss: 0.00013075
Iteration 154/1000 | Loss: 0.00013611
Iteration 155/1000 | Loss: 0.00013243
Iteration 156/1000 | Loss: 0.00013477
Iteration 157/1000 | Loss: 0.00012395
Iteration 158/1000 | Loss: 0.00012322
Iteration 159/1000 | Loss: 0.00012218
Iteration 160/1000 | Loss: 0.00013597
Iteration 161/1000 | Loss: 0.00011614
Iteration 162/1000 | Loss: 0.00012952
Iteration 163/1000 | Loss: 0.00011540
Iteration 164/1000 | Loss: 0.00016021
Iteration 165/1000 | Loss: 0.00012680
Iteration 166/1000 | Loss: 0.00013538
Iteration 167/1000 | Loss: 0.00013213
Iteration 168/1000 | Loss: 0.00013294
Iteration 169/1000 | Loss: 0.00013007
Iteration 170/1000 | Loss: 0.00013366
Iteration 171/1000 | Loss: 0.00013681
Iteration 172/1000 | Loss: 0.00010584
Iteration 173/1000 | Loss: 0.00010449
Iteration 174/1000 | Loss: 0.00010930
Iteration 175/1000 | Loss: 0.00010483
Iteration 176/1000 | Loss: 0.00009819
Iteration 177/1000 | Loss: 0.00010246
Iteration 178/1000 | Loss: 0.00010620
Iteration 179/1000 | Loss: 0.00010362
Iteration 180/1000 | Loss: 0.00010957
Iteration 181/1000 | Loss: 0.00010886
Iteration 182/1000 | Loss: 0.00010857
Iteration 183/1000 | Loss: 0.00010949
Iteration 184/1000 | Loss: 0.00011528
Iteration 185/1000 | Loss: 0.00011568
Iteration 186/1000 | Loss: 0.00010882
Iteration 187/1000 | Loss: 0.00011190
Iteration 188/1000 | Loss: 0.00011275
Iteration 189/1000 | Loss: 0.00011394
Iteration 190/1000 | Loss: 0.00011348
Iteration 191/1000 | Loss: 0.00010614
Iteration 192/1000 | Loss: 0.00011535
Iteration 193/1000 | Loss: 0.00010962
Iteration 194/1000 | Loss: 0.00013783
Iteration 195/1000 | Loss: 0.00012081
Iteration 196/1000 | Loss: 0.00014926
Iteration 197/1000 | Loss: 0.00013050
Iteration 198/1000 | Loss: 0.00009862
Iteration 199/1000 | Loss: 0.00011159
Iteration 200/1000 | Loss: 0.00010971
Iteration 201/1000 | Loss: 0.00011067
Iteration 202/1000 | Loss: 0.00012097
Iteration 203/1000 | Loss: 0.00011969
Iteration 204/1000 | Loss: 0.00012689
Iteration 205/1000 | Loss: 0.00030933
Iteration 206/1000 | Loss: 0.00021329
Iteration 207/1000 | Loss: 0.00020501
Iteration 208/1000 | Loss: 0.00010298
Iteration 209/1000 | Loss: 0.00009861
Iteration 210/1000 | Loss: 0.00011127
Iteration 211/1000 | Loss: 0.00010861
Iteration 212/1000 | Loss: 0.00011228
Iteration 213/1000 | Loss: 0.00012457
Iteration 214/1000 | Loss: 0.00012528
Iteration 215/1000 | Loss: 0.00012105
Iteration 216/1000 | Loss: 0.00010064
Iteration 217/1000 | Loss: 0.00010585
Iteration 218/1000 | Loss: 0.00010735
Iteration 219/1000 | Loss: 0.00010377
Iteration 220/1000 | Loss: 0.00013767
Iteration 221/1000 | Loss: 0.00016630
Iteration 222/1000 | Loss: 0.00012057
Iteration 223/1000 | Loss: 0.00012068
Iteration 224/1000 | Loss: 0.00011937
Iteration 225/1000 | Loss: 0.00012690
Iteration 226/1000 | Loss: 0.00011898
Iteration 227/1000 | Loss: 0.00012864
Iteration 228/1000 | Loss: 0.00012909
Iteration 229/1000 | Loss: 0.00011877
Iteration 230/1000 | Loss: 0.00011941
Iteration 231/1000 | Loss: 0.00012060
Iteration 232/1000 | Loss: 0.00012126
Iteration 233/1000 | Loss: 0.00012034
Iteration 234/1000 | Loss: 0.00013912
Iteration 235/1000 | Loss: 0.00011279
Iteration 236/1000 | Loss: 0.00013843
Iteration 237/1000 | Loss: 0.00011207
Iteration 238/1000 | Loss: 0.00011603
Iteration 239/1000 | Loss: 0.00011142
Iteration 240/1000 | Loss: 0.00012247
Iteration 241/1000 | Loss: 0.00010759
Iteration 242/1000 | Loss: 0.00011637
Iteration 243/1000 | Loss: 0.00010508
Iteration 244/1000 | Loss: 0.00010105
Iteration 245/1000 | Loss: 0.00010458
Iteration 246/1000 | Loss: 0.00009783
Iteration 247/1000 | Loss: 0.00009284
Iteration 248/1000 | Loss: 0.00010386
Iteration 249/1000 | Loss: 0.00010187
Iteration 250/1000 | Loss: 0.00010644
Iteration 251/1000 | Loss: 0.00011078
Iteration 252/1000 | Loss: 0.00011213
Iteration 253/1000 | Loss: 0.00010801
Iteration 254/1000 | Loss: 0.00010350
Iteration 255/1000 | Loss: 0.00010123
Iteration 256/1000 | Loss: 0.00011568
Iteration 257/1000 | Loss: 0.00011568
Iteration 258/1000 | Loss: 0.00012177
Iteration 259/1000 | Loss: 0.00012032
Iteration 260/1000 | Loss: 0.00009646
Iteration 261/1000 | Loss: 0.00013255
Iteration 262/1000 | Loss: 0.00011375
Iteration 263/1000 | Loss: 0.00011174
Iteration 264/1000 | Loss: 0.00009948
Iteration 265/1000 | Loss: 0.00012071
Iteration 266/1000 | Loss: 0.00011242
Iteration 267/1000 | Loss: 0.00011942
Iteration 268/1000 | Loss: 0.00010582
Iteration 269/1000 | Loss: 0.00011716
Iteration 270/1000 | Loss: 0.00010759
Iteration 271/1000 | Loss: 0.00011656
Iteration 272/1000 | Loss: 0.00011179
Iteration 273/1000 | Loss: 0.00011625
Iteration 274/1000 | Loss: 0.00010712
Iteration 275/1000 | Loss: 0.00011750
Iteration 276/1000 | Loss: 0.00011133
Iteration 277/1000 | Loss: 0.00011161
Iteration 278/1000 | Loss: 0.00011663
Iteration 279/1000 | Loss: 0.00011123
Iteration 280/1000 | Loss: 0.00010707
Iteration 281/1000 | Loss: 0.00011045
Iteration 282/1000 | Loss: 0.00011500
Iteration 283/1000 | Loss: 0.00010471
Iteration 284/1000 | Loss: 0.00011018
Iteration 285/1000 | Loss: 0.00010897
Iteration 286/1000 | Loss: 0.00010629
Iteration 287/1000 | Loss: 0.00010866
Iteration 288/1000 | Loss: 0.00010721
Iteration 289/1000 | Loss: 0.00010962
Iteration 290/1000 | Loss: 0.00011688
Iteration 291/1000 | Loss: 0.00011109
Iteration 292/1000 | Loss: 0.00010797
Iteration 293/1000 | Loss: 0.00011347
Iteration 294/1000 | Loss: 0.00010723
Iteration 295/1000 | Loss: 0.00010895
Iteration 296/1000 | Loss: 0.00010704
Iteration 297/1000 | Loss: 0.00011036
Iteration 298/1000 | Loss: 0.00011103
Iteration 299/1000 | Loss: 0.00011757
Iteration 300/1000 | Loss: 0.00011299
Iteration 301/1000 | Loss: 0.00012591
Iteration 302/1000 | Loss: 0.00010646
Iteration 303/1000 | Loss: 0.00010554
Iteration 304/1000 | Loss: 0.00010912
Iteration 305/1000 | Loss: 0.00010952
Iteration 306/1000 | Loss: 0.00011520
Iteration 307/1000 | Loss: 0.00011261
Iteration 308/1000 | Loss: 0.00010878
Iteration 309/1000 | Loss: 0.00011068
Iteration 310/1000 | Loss: 0.00010873
Iteration 311/1000 | Loss: 0.00012267
Iteration 312/1000 | Loss: 0.00009754
Iteration 313/1000 | Loss: 0.00010055
Iteration 314/1000 | Loss: 0.00010976
Iteration 315/1000 | Loss: 0.00011083
Iteration 316/1000 | Loss: 0.00012292
Iteration 317/1000 | Loss: 0.00010856
Iteration 318/1000 | Loss: 0.00011032
Iteration 319/1000 | Loss: 0.00011154
Iteration 320/1000 | Loss: 0.00011006
Iteration 321/1000 | Loss: 0.00010064
Iteration 322/1000 | Loss: 0.00011016
Iteration 323/1000 | Loss: 0.00010763
Iteration 324/1000 | Loss: 0.00010665
Iteration 325/1000 | Loss: 0.00011000
Iteration 326/1000 | Loss: 0.00011037
Iteration 327/1000 | Loss: 0.00012625
Iteration 328/1000 | Loss: 0.00009779
Iteration 329/1000 | Loss: 0.00009580
Iteration 330/1000 | Loss: 0.00009537
Iteration 331/1000 | Loss: 0.00009358
Iteration 332/1000 | Loss: 0.00009263
Iteration 333/1000 | Loss: 0.00009233
Iteration 334/1000 | Loss: 0.00009200
Iteration 335/1000 | Loss: 0.00009173
Iteration 336/1000 | Loss: 0.00009164
Iteration 337/1000 | Loss: 0.00009151
Iteration 338/1000 | Loss: 0.00009151
Iteration 339/1000 | Loss: 0.00009150
Iteration 340/1000 | Loss: 0.00009150
Iteration 341/1000 | Loss: 0.00009149
Iteration 342/1000 | Loss: 0.00009148
Iteration 343/1000 | Loss: 0.00009148
Iteration 344/1000 | Loss: 0.00009148
Iteration 345/1000 | Loss: 0.00009148
Iteration 346/1000 | Loss: 0.00009148
Iteration 347/1000 | Loss: 0.00009146
Iteration 348/1000 | Loss: 0.00009146
Iteration 349/1000 | Loss: 0.00009145
Iteration 350/1000 | Loss: 0.00009145
Iteration 351/1000 | Loss: 0.00009145
Iteration 352/1000 | Loss: 0.00009145
Iteration 353/1000 | Loss: 0.00009145
Iteration 354/1000 | Loss: 0.00009144
Iteration 355/1000 | Loss: 0.00009144
Iteration 356/1000 | Loss: 0.00009144
Iteration 357/1000 | Loss: 0.00009144
Iteration 358/1000 | Loss: 0.00009144
Iteration 359/1000 | Loss: 0.00009144
Iteration 360/1000 | Loss: 0.00009144
Iteration 361/1000 | Loss: 0.00009144
Iteration 362/1000 | Loss: 0.00009143
Iteration 363/1000 | Loss: 0.00009143
Iteration 364/1000 | Loss: 0.00009143
Iteration 365/1000 | Loss: 0.00009143
Iteration 366/1000 | Loss: 0.00009143
Iteration 367/1000 | Loss: 0.00009143
Iteration 368/1000 | Loss: 0.00009143
Iteration 369/1000 | Loss: 0.00009143
Iteration 370/1000 | Loss: 0.00009143
Iteration 371/1000 | Loss: 0.00009143
Iteration 372/1000 | Loss: 0.00009143
Iteration 373/1000 | Loss: 0.00009142
Iteration 374/1000 | Loss: 0.00009142
Iteration 375/1000 | Loss: 0.00009142
Iteration 376/1000 | Loss: 0.00009142
Iteration 377/1000 | Loss: 0.00009142
Iteration 378/1000 | Loss: 0.00009142
Iteration 379/1000 | Loss: 0.00009142
Iteration 380/1000 | Loss: 0.00009142
Iteration 381/1000 | Loss: 0.00009141
Iteration 382/1000 | Loss: 0.00009141
Iteration 383/1000 | Loss: 0.00009141
Iteration 384/1000 | Loss: 0.00009141
Iteration 385/1000 | Loss: 0.00009141
Iteration 386/1000 | Loss: 0.00009141
Iteration 387/1000 | Loss: 0.00009141
Iteration 388/1000 | Loss: 0.00009141
Iteration 389/1000 | Loss: 0.00009141
Iteration 390/1000 | Loss: 0.00009140
Iteration 391/1000 | Loss: 0.00009140
Iteration 392/1000 | Loss: 0.00009140
Iteration 393/1000 | Loss: 0.00009140
Iteration 394/1000 | Loss: 0.00009140
Iteration 395/1000 | Loss: 0.00009140
Iteration 396/1000 | Loss: 0.00009140
Iteration 397/1000 | Loss: 0.00009140
Iteration 398/1000 | Loss: 0.00009140
Iteration 399/1000 | Loss: 0.00009140
Iteration 400/1000 | Loss: 0.00009141
Iteration 401/1000 | Loss: 0.00009141
Iteration 402/1000 | Loss: 0.00009141
Iteration 403/1000 | Loss: 0.00009141
Iteration 404/1000 | Loss: 0.00009141
Iteration 405/1000 | Loss: 0.00009141
Iteration 406/1000 | Loss: 0.00009140
Iteration 407/1000 | Loss: 0.00009140
Iteration 408/1000 | Loss: 0.00009139
Iteration 409/1000 | Loss: 0.00009138
Iteration 410/1000 | Loss: 0.00009137
Iteration 411/1000 | Loss: 0.00009136
Iteration 412/1000 | Loss: 0.00009135
Iteration 413/1000 | Loss: 0.00009135
Iteration 414/1000 | Loss: 0.00009135
Iteration 415/1000 | Loss: 0.00009134
Iteration 416/1000 | Loss: 0.00009134
Iteration 417/1000 | Loss: 0.00009134
Iteration 418/1000 | Loss: 0.00009133
Iteration 419/1000 | Loss: 0.00009132
Iteration 420/1000 | Loss: 0.00009132
Iteration 421/1000 | Loss: 0.00009132
Iteration 422/1000 | Loss: 0.00009132
Iteration 423/1000 | Loss: 0.00009131
Iteration 424/1000 | Loss: 0.00009131
Iteration 425/1000 | Loss: 0.00009131
Iteration 426/1000 | Loss: 0.00009131
Iteration 427/1000 | Loss: 0.00009131
Iteration 428/1000 | Loss: 0.00009131
Iteration 429/1000 | Loss: 0.00009131
Iteration 430/1000 | Loss: 0.00009130
Iteration 431/1000 | Loss: 0.00009130
Iteration 432/1000 | Loss: 0.00009130
Iteration 433/1000 | Loss: 0.00009130
Iteration 434/1000 | Loss: 0.00009130
Iteration 435/1000 | Loss: 0.00009130
Iteration 436/1000 | Loss: 0.00009130
Iteration 437/1000 | Loss: 0.00009130
Iteration 438/1000 | Loss: 0.00009130
Iteration 439/1000 | Loss: 0.00009130
Iteration 440/1000 | Loss: 0.00009130
Iteration 441/1000 | Loss: 0.00009130
Iteration 442/1000 | Loss: 0.00009130
Iteration 443/1000 | Loss: 0.00009130
Iteration 444/1000 | Loss: 0.00009130
Iteration 445/1000 | Loss: 0.00009129
Iteration 446/1000 | Loss: 0.00009129
Iteration 447/1000 | Loss: 0.00009129
Iteration 448/1000 | Loss: 0.00009129
Iteration 449/1000 | Loss: 0.00009128
Iteration 450/1000 | Loss: 0.00009132
Iteration 451/1000 | Loss: 0.00009132
Iteration 452/1000 | Loss: 0.00009129
Iteration 453/1000 | Loss: 0.00009129
Iteration 454/1000 | Loss: 0.00009129
Iteration 455/1000 | Loss: 0.00009129
Iteration 456/1000 | Loss: 0.00009129
Iteration 457/1000 | Loss: 0.00009129
Iteration 458/1000 | Loss: 0.00009128
Iteration 459/1000 | Loss: 0.00009128
Iteration 460/1000 | Loss: 0.00009128
Iteration 461/1000 | Loss: 0.00009128
Iteration 462/1000 | Loss: 0.00009128
Iteration 463/1000 | Loss: 0.00009128
Iteration 464/1000 | Loss: 0.00009128
Iteration 465/1000 | Loss: 0.00009128
Iteration 466/1000 | Loss: 0.00009128
Iteration 467/1000 | Loss: 0.00009128
Iteration 468/1000 | Loss: 0.00009128
Iteration 469/1000 | Loss: 0.00009127
Iteration 470/1000 | Loss: 0.00009127
Iteration 471/1000 | Loss: 0.00009127
Iteration 472/1000 | Loss: 0.00009127
Iteration 473/1000 | Loss: 0.00009127
Iteration 474/1000 | Loss: 0.00009127
Iteration 475/1000 | Loss: 0.00009127
Iteration 476/1000 | Loss: 0.00009126
Iteration 477/1000 | Loss: 0.00009126
Iteration 478/1000 | Loss: 0.00027636
Iteration 479/1000 | Loss: 0.00009394
Iteration 480/1000 | Loss: 0.00009223
Iteration 481/1000 | Loss: 0.00027426
Iteration 482/1000 | Loss: 0.00028514
Iteration 483/1000 | Loss: 0.00021075
Iteration 484/1000 | Loss: 0.00009306
Iteration 485/1000 | Loss: 0.00009084
Iteration 486/1000 | Loss: 0.00008967
Iteration 487/1000 | Loss: 0.00008892
Iteration 488/1000 | Loss: 0.00008854
Iteration 489/1000 | Loss: 0.00008831
Iteration 490/1000 | Loss: 0.00008815
Iteration 491/1000 | Loss: 0.00008815
Iteration 492/1000 | Loss: 0.00008815
Iteration 493/1000 | Loss: 0.00008810
Iteration 494/1000 | Loss: 0.00008808
Iteration 495/1000 | Loss: 0.00008807
Iteration 496/1000 | Loss: 0.00008804
Iteration 497/1000 | Loss: 0.00008804
Iteration 498/1000 | Loss: 0.00008803
Iteration 499/1000 | Loss: 0.00008802
Iteration 500/1000 | Loss: 0.00008802
Iteration 501/1000 | Loss: 0.00008802
Iteration 502/1000 | Loss: 0.00008801
Iteration 503/1000 | Loss: 0.00008801
Iteration 504/1000 | Loss: 0.00008800
Iteration 505/1000 | Loss: 0.00008800
Iteration 506/1000 | Loss: 0.00008800
Iteration 507/1000 | Loss: 0.00008800
Iteration 508/1000 | Loss: 0.00008800
Iteration 509/1000 | Loss: 0.00008800
Iteration 510/1000 | Loss: 0.00008800
Iteration 511/1000 | Loss: 0.00008800
Iteration 512/1000 | Loss: 0.00008800
Iteration 513/1000 | Loss: 0.00008800
Iteration 514/1000 | Loss: 0.00008800
Iteration 515/1000 | Loss: 0.00008799
Iteration 516/1000 | Loss: 0.00008799
Iteration 517/1000 | Loss: 0.00008799
Iteration 518/1000 | Loss: 0.00008799
Iteration 519/1000 | Loss: 0.00008799
Iteration 520/1000 | Loss: 0.00008799
Iteration 521/1000 | Loss: 0.00008798
Iteration 522/1000 | Loss: 0.00008798
Iteration 523/1000 | Loss: 0.00008798
Iteration 524/1000 | Loss: 0.00008798
Iteration 525/1000 | Loss: 0.00008798
Iteration 526/1000 | Loss: 0.00008798
Iteration 527/1000 | Loss: 0.00008798
Iteration 528/1000 | Loss: 0.00008798
Iteration 529/1000 | Loss: 0.00008797
Iteration 530/1000 | Loss: 0.00008797
Iteration 531/1000 | Loss: 0.00008797
Iteration 532/1000 | Loss: 0.00008797
Iteration 533/1000 | Loss: 0.00008797
Iteration 534/1000 | Loss: 0.00008797
Iteration 535/1000 | Loss: 0.00008797
Iteration 536/1000 | Loss: 0.00008797
Iteration 537/1000 | Loss: 0.00008797
Iteration 538/1000 | Loss: 0.00008797
Iteration 539/1000 | Loss: 0.00008797
Iteration 540/1000 | Loss: 0.00008797
Iteration 541/1000 | Loss: 0.00008797
Iteration 542/1000 | Loss: 0.00008797
Iteration 543/1000 | Loss: 0.00008797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 543. Stopping optimization.
Last 5 losses: [8.79685248946771e-05, 8.79685248946771e-05, 8.79685248946771e-05, 8.79685248946771e-05, 8.79685248946771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.79685248946771e-05

Optimization complete. Final v2v error: 5.554051399230957 mm

Highest mean error: 21.26316261291504 mm for frame 195

Lowest mean error: 3.9197299480438232 mm for frame 21

Saving results

Total time: 634.2178995609283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864809
Iteration 2/25 | Loss: 0.00161450
Iteration 3/25 | Loss: 0.00131559
Iteration 4/25 | Loss: 0.00128028
Iteration 5/25 | Loss: 0.00127260
Iteration 6/25 | Loss: 0.00127061
Iteration 7/25 | Loss: 0.00126993
Iteration 8/25 | Loss: 0.00126985
Iteration 9/25 | Loss: 0.00126985
Iteration 10/25 | Loss: 0.00126985
Iteration 11/25 | Loss: 0.00126985
Iteration 12/25 | Loss: 0.00126985
Iteration 13/25 | Loss: 0.00126985
Iteration 14/25 | Loss: 0.00126985
Iteration 15/25 | Loss: 0.00126985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012698525097221136, 0.0012698525097221136, 0.0012698525097221136, 0.0012698525097221136, 0.0012698525097221136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012698525097221136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11994958
Iteration 2/25 | Loss: 0.00351601
Iteration 3/25 | Loss: 0.00351601
Iteration 4/25 | Loss: 0.00351601
Iteration 5/25 | Loss: 0.00351601
Iteration 6/25 | Loss: 0.00351601
Iteration 7/25 | Loss: 0.00351601
Iteration 8/25 | Loss: 0.00351601
Iteration 9/25 | Loss: 0.00351600
Iteration 10/25 | Loss: 0.00351600
Iteration 11/25 | Loss: 0.00351600
Iteration 12/25 | Loss: 0.00351600
Iteration 13/25 | Loss: 0.00351600
Iteration 14/25 | Loss: 0.00351600
Iteration 15/25 | Loss: 0.00351600
Iteration 16/25 | Loss: 0.00351600
Iteration 17/25 | Loss: 0.00351600
Iteration 18/25 | Loss: 0.00351600
Iteration 19/25 | Loss: 0.00351600
Iteration 20/25 | Loss: 0.00351600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0035160041879862547, 0.0035160041879862547, 0.0035160041879862547, 0.0035160041879862547, 0.0035160041879862547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035160041879862547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00351600
Iteration 2/1000 | Loss: 0.00004751
Iteration 3/1000 | Loss: 0.00002932
Iteration 4/1000 | Loss: 0.00002542
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00003336
Iteration 8/1000 | Loss: 0.00002211
Iteration 9/1000 | Loss: 0.00006153
Iteration 10/1000 | Loss: 0.00007202
Iteration 11/1000 | Loss: 0.00005195
Iteration 12/1000 | Loss: 0.00004310
Iteration 13/1000 | Loss: 0.00002300
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001838
Iteration 16/1000 | Loss: 0.00001741
Iteration 17/1000 | Loss: 0.00001695
Iteration 18/1000 | Loss: 0.00001658
Iteration 19/1000 | Loss: 0.00001628
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001495
Iteration 26/1000 | Loss: 0.00001493
Iteration 27/1000 | Loss: 0.00001492
Iteration 28/1000 | Loss: 0.00001491
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001490
Iteration 31/1000 | Loss: 0.00001490
Iteration 32/1000 | Loss: 0.00001489
Iteration 33/1000 | Loss: 0.00001489
Iteration 34/1000 | Loss: 0.00001489
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001481
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001477
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001477
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001474
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001474
Iteration 83/1000 | Loss: 0.00001474
Iteration 84/1000 | Loss: 0.00001474
Iteration 85/1000 | Loss: 0.00001474
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001473
Iteration 89/1000 | Loss: 0.00001473
Iteration 90/1000 | Loss: 0.00001473
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001472
Iteration 97/1000 | Loss: 0.00001472
Iteration 98/1000 | Loss: 0.00001472
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001471
Iteration 101/1000 | Loss: 0.00001471
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001470
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001470
Iteration 112/1000 | Loss: 0.00001469
Iteration 113/1000 | Loss: 0.00001469
Iteration 114/1000 | Loss: 0.00001469
Iteration 115/1000 | Loss: 0.00001469
Iteration 116/1000 | Loss: 0.00001469
Iteration 117/1000 | Loss: 0.00001469
Iteration 118/1000 | Loss: 0.00001469
Iteration 119/1000 | Loss: 0.00001469
Iteration 120/1000 | Loss: 0.00001469
Iteration 121/1000 | Loss: 0.00001468
Iteration 122/1000 | Loss: 0.00001468
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001467
Iteration 126/1000 | Loss: 0.00001467
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001466
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001466
Iteration 135/1000 | Loss: 0.00001466
Iteration 136/1000 | Loss: 0.00001466
Iteration 137/1000 | Loss: 0.00001466
Iteration 138/1000 | Loss: 0.00001466
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001465
Iteration 141/1000 | Loss: 0.00001465
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001465
Iteration 144/1000 | Loss: 0.00001465
Iteration 145/1000 | Loss: 0.00001465
Iteration 146/1000 | Loss: 0.00001465
Iteration 147/1000 | Loss: 0.00001464
Iteration 148/1000 | Loss: 0.00001464
Iteration 149/1000 | Loss: 0.00001464
Iteration 150/1000 | Loss: 0.00001464
Iteration 151/1000 | Loss: 0.00001464
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001464
Iteration 154/1000 | Loss: 0.00001464
Iteration 155/1000 | Loss: 0.00001464
Iteration 156/1000 | Loss: 0.00001463
Iteration 157/1000 | Loss: 0.00001463
Iteration 158/1000 | Loss: 0.00001463
Iteration 159/1000 | Loss: 0.00001463
Iteration 160/1000 | Loss: 0.00001463
Iteration 161/1000 | Loss: 0.00001463
Iteration 162/1000 | Loss: 0.00001463
Iteration 163/1000 | Loss: 0.00001463
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001462
Iteration 169/1000 | Loss: 0.00001462
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001462
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001462
Iteration 174/1000 | Loss: 0.00001462
Iteration 175/1000 | Loss: 0.00001462
Iteration 176/1000 | Loss: 0.00001462
Iteration 177/1000 | Loss: 0.00001462
Iteration 178/1000 | Loss: 0.00001462
Iteration 179/1000 | Loss: 0.00001462
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001461
Iteration 188/1000 | Loss: 0.00001461
Iteration 189/1000 | Loss: 0.00001461
Iteration 190/1000 | Loss: 0.00001461
Iteration 191/1000 | Loss: 0.00001460
Iteration 192/1000 | Loss: 0.00001460
Iteration 193/1000 | Loss: 0.00001460
Iteration 194/1000 | Loss: 0.00001460
Iteration 195/1000 | Loss: 0.00001460
Iteration 196/1000 | Loss: 0.00001460
Iteration 197/1000 | Loss: 0.00001460
Iteration 198/1000 | Loss: 0.00001460
Iteration 199/1000 | Loss: 0.00001460
Iteration 200/1000 | Loss: 0.00001460
Iteration 201/1000 | Loss: 0.00001460
Iteration 202/1000 | Loss: 0.00001460
Iteration 203/1000 | Loss: 0.00001459
Iteration 204/1000 | Loss: 0.00001459
Iteration 205/1000 | Loss: 0.00001459
Iteration 206/1000 | Loss: 0.00001459
Iteration 207/1000 | Loss: 0.00001459
Iteration 208/1000 | Loss: 0.00001459
Iteration 209/1000 | Loss: 0.00001459
Iteration 210/1000 | Loss: 0.00001459
Iteration 211/1000 | Loss: 0.00001459
Iteration 212/1000 | Loss: 0.00001459
Iteration 213/1000 | Loss: 0.00001459
Iteration 214/1000 | Loss: 0.00001459
Iteration 215/1000 | Loss: 0.00001458
Iteration 216/1000 | Loss: 0.00001458
Iteration 217/1000 | Loss: 0.00001458
Iteration 218/1000 | Loss: 0.00001458
Iteration 219/1000 | Loss: 0.00001458
Iteration 220/1000 | Loss: 0.00001458
Iteration 221/1000 | Loss: 0.00001458
Iteration 222/1000 | Loss: 0.00001458
Iteration 223/1000 | Loss: 0.00001458
Iteration 224/1000 | Loss: 0.00001458
Iteration 225/1000 | Loss: 0.00001458
Iteration 226/1000 | Loss: 0.00001457
Iteration 227/1000 | Loss: 0.00001457
Iteration 228/1000 | Loss: 0.00001457
Iteration 229/1000 | Loss: 0.00001457
Iteration 230/1000 | Loss: 0.00001457
Iteration 231/1000 | Loss: 0.00001457
Iteration 232/1000 | Loss: 0.00001456
Iteration 233/1000 | Loss: 0.00001456
Iteration 234/1000 | Loss: 0.00001456
Iteration 235/1000 | Loss: 0.00001456
Iteration 236/1000 | Loss: 0.00001456
Iteration 237/1000 | Loss: 0.00001456
Iteration 238/1000 | Loss: 0.00001455
Iteration 239/1000 | Loss: 0.00001455
Iteration 240/1000 | Loss: 0.00001455
Iteration 241/1000 | Loss: 0.00001454
Iteration 242/1000 | Loss: 0.00001454
Iteration 243/1000 | Loss: 0.00001454
Iteration 244/1000 | Loss: 0.00001453
Iteration 245/1000 | Loss: 0.00001453
Iteration 246/1000 | Loss: 0.00001453
Iteration 247/1000 | Loss: 0.00001453
Iteration 248/1000 | Loss: 0.00001452
Iteration 249/1000 | Loss: 0.00001452
Iteration 250/1000 | Loss: 0.00001452
Iteration 251/1000 | Loss: 0.00001452
Iteration 252/1000 | Loss: 0.00001452
Iteration 253/1000 | Loss: 0.00001452
Iteration 254/1000 | Loss: 0.00001452
Iteration 255/1000 | Loss: 0.00001452
Iteration 256/1000 | Loss: 0.00001452
Iteration 257/1000 | Loss: 0.00001452
Iteration 258/1000 | Loss: 0.00001451
Iteration 259/1000 | Loss: 0.00001451
Iteration 260/1000 | Loss: 0.00001451
Iteration 261/1000 | Loss: 0.00001451
Iteration 262/1000 | Loss: 0.00001451
Iteration 263/1000 | Loss: 0.00001451
Iteration 264/1000 | Loss: 0.00001451
Iteration 265/1000 | Loss: 0.00001451
Iteration 266/1000 | Loss: 0.00001451
Iteration 267/1000 | Loss: 0.00001450
Iteration 268/1000 | Loss: 0.00001450
Iteration 269/1000 | Loss: 0.00001450
Iteration 270/1000 | Loss: 0.00001450
Iteration 271/1000 | Loss: 0.00001450
Iteration 272/1000 | Loss: 0.00001450
Iteration 273/1000 | Loss: 0.00001450
Iteration 274/1000 | Loss: 0.00001450
Iteration 275/1000 | Loss: 0.00001450
Iteration 276/1000 | Loss: 0.00001450
Iteration 277/1000 | Loss: 0.00001450
Iteration 278/1000 | Loss: 0.00001450
Iteration 279/1000 | Loss: 0.00001450
Iteration 280/1000 | Loss: 0.00001450
Iteration 281/1000 | Loss: 0.00001450
Iteration 282/1000 | Loss: 0.00001450
Iteration 283/1000 | Loss: 0.00001450
Iteration 284/1000 | Loss: 0.00001450
Iteration 285/1000 | Loss: 0.00001450
Iteration 286/1000 | Loss: 0.00001450
Iteration 287/1000 | Loss: 0.00001450
Iteration 288/1000 | Loss: 0.00001450
Iteration 289/1000 | Loss: 0.00001450
Iteration 290/1000 | Loss: 0.00001450
Iteration 291/1000 | Loss: 0.00001450
Iteration 292/1000 | Loss: 0.00001450
Iteration 293/1000 | Loss: 0.00001450
Iteration 294/1000 | Loss: 0.00001450
Iteration 295/1000 | Loss: 0.00001450
Iteration 296/1000 | Loss: 0.00001450
Iteration 297/1000 | Loss: 0.00001450
Iteration 298/1000 | Loss: 0.00001450
Iteration 299/1000 | Loss: 0.00001450
Iteration 300/1000 | Loss: 0.00001450
Iteration 301/1000 | Loss: 0.00001450
Iteration 302/1000 | Loss: 0.00001450
Iteration 303/1000 | Loss: 0.00001450
Iteration 304/1000 | Loss: 0.00001450
Iteration 305/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [1.449672163289506e-05, 1.449672163289506e-05, 1.449672163289506e-05, 1.449672163289506e-05, 1.449672163289506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.449672163289506e-05

Optimization complete. Final v2v error: 3.3919899463653564 mm

Highest mean error: 4.611626625061035 mm for frame 61

Lowest mean error: 3.0452499389648438 mm for frame 15

Saving results

Total time: 75.03331136703491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867125
Iteration 2/25 | Loss: 0.00163098
Iteration 3/25 | Loss: 0.00118122
Iteration 4/25 | Loss: 0.00115342
Iteration 5/25 | Loss: 0.00114481
Iteration 6/25 | Loss: 0.00114375
Iteration 7/25 | Loss: 0.00114375
Iteration 8/25 | Loss: 0.00114375
Iteration 9/25 | Loss: 0.00114375
Iteration 10/25 | Loss: 0.00114375
Iteration 11/25 | Loss: 0.00114375
Iteration 12/25 | Loss: 0.00114375
Iteration 13/25 | Loss: 0.00114375
Iteration 14/25 | Loss: 0.00114375
Iteration 15/25 | Loss: 0.00114375
Iteration 16/25 | Loss: 0.00114375
Iteration 17/25 | Loss: 0.00114375
Iteration 18/25 | Loss: 0.00114375
Iteration 19/25 | Loss: 0.00114375
Iteration 20/25 | Loss: 0.00114375
Iteration 21/25 | Loss: 0.00114375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011437488719820976, 0.0011437488719820976, 0.0011437488719820976, 0.0011437488719820976, 0.0011437488719820976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011437488719820976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.40732589
Iteration 2/25 | Loss: 0.00077723
Iteration 3/25 | Loss: 0.00077723
Iteration 4/25 | Loss: 0.00077723
Iteration 5/25 | Loss: 0.00077723
Iteration 6/25 | Loss: 0.00077723
Iteration 7/25 | Loss: 0.00077723
Iteration 8/25 | Loss: 0.00077723
Iteration 9/25 | Loss: 0.00077723
Iteration 10/25 | Loss: 0.00077723
Iteration 11/25 | Loss: 0.00077722
Iteration 12/25 | Loss: 0.00077722
Iteration 13/25 | Loss: 0.00077722
Iteration 14/25 | Loss: 0.00077722
Iteration 15/25 | Loss: 0.00077722
Iteration 16/25 | Loss: 0.00077722
Iteration 17/25 | Loss: 0.00077722
Iteration 18/25 | Loss: 0.00077722
Iteration 19/25 | Loss: 0.00077722
Iteration 20/25 | Loss: 0.00077722
Iteration 21/25 | Loss: 0.00077722
Iteration 22/25 | Loss: 0.00077722
Iteration 23/25 | Loss: 0.00077722
Iteration 24/25 | Loss: 0.00077722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000777224893681705, 0.000777224893681705, 0.000777224893681705, 0.000777224893681705, 0.000777224893681705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000777224893681705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077722
Iteration 2/1000 | Loss: 0.00007301
Iteration 3/1000 | Loss: 0.00005997
Iteration 4/1000 | Loss: 0.00005184
Iteration 5/1000 | Loss: 0.00004859
Iteration 6/1000 | Loss: 0.00004663
Iteration 7/1000 | Loss: 0.00004563
Iteration 8/1000 | Loss: 0.00004475
Iteration 9/1000 | Loss: 0.00004415
Iteration 10/1000 | Loss: 0.00004376
Iteration 11/1000 | Loss: 0.00004307
Iteration 12/1000 | Loss: 0.00004247
Iteration 13/1000 | Loss: 0.00004206
Iteration 14/1000 | Loss: 0.00004179
Iteration 15/1000 | Loss: 0.00004157
Iteration 16/1000 | Loss: 0.00004140
Iteration 17/1000 | Loss: 0.00004139
Iteration 18/1000 | Loss: 0.00004136
Iteration 19/1000 | Loss: 0.00004136
Iteration 20/1000 | Loss: 0.00004119
Iteration 21/1000 | Loss: 0.00004116
Iteration 22/1000 | Loss: 0.00004113
Iteration 23/1000 | Loss: 0.00004111
Iteration 24/1000 | Loss: 0.00004110
Iteration 25/1000 | Loss: 0.00004109
Iteration 26/1000 | Loss: 0.00004103
Iteration 27/1000 | Loss: 0.00004103
Iteration 28/1000 | Loss: 0.00004103
Iteration 29/1000 | Loss: 0.00004102
Iteration 30/1000 | Loss: 0.00004102
Iteration 31/1000 | Loss: 0.00004100
Iteration 32/1000 | Loss: 0.00004100
Iteration 33/1000 | Loss: 0.00004100
Iteration 34/1000 | Loss: 0.00004100
Iteration 35/1000 | Loss: 0.00004100
Iteration 36/1000 | Loss: 0.00004100
Iteration 37/1000 | Loss: 0.00004100
Iteration 38/1000 | Loss: 0.00004100
Iteration 39/1000 | Loss: 0.00004099
Iteration 40/1000 | Loss: 0.00004099
Iteration 41/1000 | Loss: 0.00004099
Iteration 42/1000 | Loss: 0.00004098
Iteration 43/1000 | Loss: 0.00004098
Iteration 44/1000 | Loss: 0.00004097
Iteration 45/1000 | Loss: 0.00004097
Iteration 46/1000 | Loss: 0.00004097
Iteration 47/1000 | Loss: 0.00004097
Iteration 48/1000 | Loss: 0.00004097
Iteration 49/1000 | Loss: 0.00004097
Iteration 50/1000 | Loss: 0.00004096
Iteration 51/1000 | Loss: 0.00004096
Iteration 52/1000 | Loss: 0.00004096
Iteration 53/1000 | Loss: 0.00004095
Iteration 54/1000 | Loss: 0.00004095
Iteration 55/1000 | Loss: 0.00004095
Iteration 56/1000 | Loss: 0.00004095
Iteration 57/1000 | Loss: 0.00004095
Iteration 58/1000 | Loss: 0.00004095
Iteration 59/1000 | Loss: 0.00004095
Iteration 60/1000 | Loss: 0.00004095
Iteration 61/1000 | Loss: 0.00004095
Iteration 62/1000 | Loss: 0.00004095
Iteration 63/1000 | Loss: 0.00004095
Iteration 64/1000 | Loss: 0.00004095
Iteration 65/1000 | Loss: 0.00004095
Iteration 66/1000 | Loss: 0.00004094
Iteration 67/1000 | Loss: 0.00004094
Iteration 68/1000 | Loss: 0.00004094
Iteration 69/1000 | Loss: 0.00004094
Iteration 70/1000 | Loss: 0.00004094
Iteration 71/1000 | Loss: 0.00004094
Iteration 72/1000 | Loss: 0.00004094
Iteration 73/1000 | Loss: 0.00004094
Iteration 74/1000 | Loss: 0.00004094
Iteration 75/1000 | Loss: 0.00004094
Iteration 76/1000 | Loss: 0.00004094
Iteration 77/1000 | Loss: 0.00004093
Iteration 78/1000 | Loss: 0.00004093
Iteration 79/1000 | Loss: 0.00004093
Iteration 80/1000 | Loss: 0.00004093
Iteration 81/1000 | Loss: 0.00004093
Iteration 82/1000 | Loss: 0.00004092
Iteration 83/1000 | Loss: 0.00004092
Iteration 84/1000 | Loss: 0.00004092
Iteration 85/1000 | Loss: 0.00004091
Iteration 86/1000 | Loss: 0.00004091
Iteration 87/1000 | Loss: 0.00004091
Iteration 88/1000 | Loss: 0.00004090
Iteration 89/1000 | Loss: 0.00004090
Iteration 90/1000 | Loss: 0.00004090
Iteration 91/1000 | Loss: 0.00004090
Iteration 92/1000 | Loss: 0.00004090
Iteration 93/1000 | Loss: 0.00004090
Iteration 94/1000 | Loss: 0.00004089
Iteration 95/1000 | Loss: 0.00004089
Iteration 96/1000 | Loss: 0.00004089
Iteration 97/1000 | Loss: 0.00004089
Iteration 98/1000 | Loss: 0.00004089
Iteration 99/1000 | Loss: 0.00004089
Iteration 100/1000 | Loss: 0.00004089
Iteration 101/1000 | Loss: 0.00004088
Iteration 102/1000 | Loss: 0.00004088
Iteration 103/1000 | Loss: 0.00004088
Iteration 104/1000 | Loss: 0.00004088
Iteration 105/1000 | Loss: 0.00004088
Iteration 106/1000 | Loss: 0.00004088
Iteration 107/1000 | Loss: 0.00004088
Iteration 108/1000 | Loss: 0.00004087
Iteration 109/1000 | Loss: 0.00004087
Iteration 110/1000 | Loss: 0.00004087
Iteration 111/1000 | Loss: 0.00004087
Iteration 112/1000 | Loss: 0.00004087
Iteration 113/1000 | Loss: 0.00004087
Iteration 114/1000 | Loss: 0.00004087
Iteration 115/1000 | Loss: 0.00004087
Iteration 116/1000 | Loss: 0.00004087
Iteration 117/1000 | Loss: 0.00004087
Iteration 118/1000 | Loss: 0.00004086
Iteration 119/1000 | Loss: 0.00004086
Iteration 120/1000 | Loss: 0.00004086
Iteration 121/1000 | Loss: 0.00004086
Iteration 122/1000 | Loss: 0.00004086
Iteration 123/1000 | Loss: 0.00004086
Iteration 124/1000 | Loss: 0.00004086
Iteration 125/1000 | Loss: 0.00004085
Iteration 126/1000 | Loss: 0.00004085
Iteration 127/1000 | Loss: 0.00004085
Iteration 128/1000 | Loss: 0.00004085
Iteration 129/1000 | Loss: 0.00004085
Iteration 130/1000 | Loss: 0.00004085
Iteration 131/1000 | Loss: 0.00004085
Iteration 132/1000 | Loss: 0.00004085
Iteration 133/1000 | Loss: 0.00004085
Iteration 134/1000 | Loss: 0.00004085
Iteration 135/1000 | Loss: 0.00004085
Iteration 136/1000 | Loss: 0.00004084
Iteration 137/1000 | Loss: 0.00004084
Iteration 138/1000 | Loss: 0.00004084
Iteration 139/1000 | Loss: 0.00004084
Iteration 140/1000 | Loss: 0.00004083
Iteration 141/1000 | Loss: 0.00004083
Iteration 142/1000 | Loss: 0.00004083
Iteration 143/1000 | Loss: 0.00004083
Iteration 144/1000 | Loss: 0.00004083
Iteration 145/1000 | Loss: 0.00004083
Iteration 146/1000 | Loss: 0.00004083
Iteration 147/1000 | Loss: 0.00004082
Iteration 148/1000 | Loss: 0.00004082
Iteration 149/1000 | Loss: 0.00004082
Iteration 150/1000 | Loss: 0.00004082
Iteration 151/1000 | Loss: 0.00004082
Iteration 152/1000 | Loss: 0.00004081
Iteration 153/1000 | Loss: 0.00004081
Iteration 154/1000 | Loss: 0.00004081
Iteration 155/1000 | Loss: 0.00004081
Iteration 156/1000 | Loss: 0.00004080
Iteration 157/1000 | Loss: 0.00004080
Iteration 158/1000 | Loss: 0.00004080
Iteration 159/1000 | Loss: 0.00004080
Iteration 160/1000 | Loss: 0.00004080
Iteration 161/1000 | Loss: 0.00004080
Iteration 162/1000 | Loss: 0.00004080
Iteration 163/1000 | Loss: 0.00004080
Iteration 164/1000 | Loss: 0.00004080
Iteration 165/1000 | Loss: 0.00004080
Iteration 166/1000 | Loss: 0.00004080
Iteration 167/1000 | Loss: 0.00004079
Iteration 168/1000 | Loss: 0.00004079
Iteration 169/1000 | Loss: 0.00004079
Iteration 170/1000 | Loss: 0.00004079
Iteration 171/1000 | Loss: 0.00004079
Iteration 172/1000 | Loss: 0.00004079
Iteration 173/1000 | Loss: 0.00004079
Iteration 174/1000 | Loss: 0.00004079
Iteration 175/1000 | Loss: 0.00004079
Iteration 176/1000 | Loss: 0.00004079
Iteration 177/1000 | Loss: 0.00004079
Iteration 178/1000 | Loss: 0.00004079
Iteration 179/1000 | Loss: 0.00004079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [4.079216523678042e-05, 4.079216523678042e-05, 4.079216523678042e-05, 4.079216523678042e-05, 4.079216523678042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.079216523678042e-05

Optimization complete. Final v2v error: 4.9642014503479 mm

Highest mean error: 5.165652275085449 mm for frame 115

Lowest mean error: 4.7525105476379395 mm for frame 131

Saving results

Total time: 46.04949426651001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501853
Iteration 2/25 | Loss: 0.00099819
Iteration 3/25 | Loss: 0.00086170
Iteration 4/25 | Loss: 0.00082554
Iteration 5/25 | Loss: 0.00081614
Iteration 6/25 | Loss: 0.00081470
Iteration 7/25 | Loss: 0.00081451
Iteration 8/25 | Loss: 0.00081450
Iteration 9/25 | Loss: 0.00081450
Iteration 10/25 | Loss: 0.00081450
Iteration 11/25 | Loss: 0.00081450
Iteration 12/25 | Loss: 0.00081450
Iteration 13/25 | Loss: 0.00081450
Iteration 14/25 | Loss: 0.00081450
Iteration 15/25 | Loss: 0.00081450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008144972962327302, 0.0008144972962327302, 0.0008144972962327302, 0.0008144972962327302, 0.0008144972962327302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008144972962327302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54275155
Iteration 2/25 | Loss: 0.00110110
Iteration 3/25 | Loss: 0.00110107
Iteration 4/25 | Loss: 0.00110107
Iteration 5/25 | Loss: 0.00110106
Iteration 6/25 | Loss: 0.00110106
Iteration 7/25 | Loss: 0.00110106
Iteration 8/25 | Loss: 0.00110106
Iteration 9/25 | Loss: 0.00110106
Iteration 10/25 | Loss: 0.00110106
Iteration 11/25 | Loss: 0.00110106
Iteration 12/25 | Loss: 0.00110106
Iteration 13/25 | Loss: 0.00110106
Iteration 14/25 | Loss: 0.00110106
Iteration 15/25 | Loss: 0.00110106
Iteration 16/25 | Loss: 0.00110106
Iteration 17/25 | Loss: 0.00110106
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011010629823431373, 0.0011010629823431373, 0.0011010629823431373, 0.0011010629823431373, 0.0011010629823431373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011010629823431373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110106
Iteration 2/1000 | Loss: 0.00005399
Iteration 3/1000 | Loss: 0.00003521
Iteration 4/1000 | Loss: 0.00002950
Iteration 5/1000 | Loss: 0.00002764
Iteration 6/1000 | Loss: 0.00002642
Iteration 7/1000 | Loss: 0.00002513
Iteration 8/1000 | Loss: 0.00002444
Iteration 9/1000 | Loss: 0.00002391
Iteration 10/1000 | Loss: 0.00002353
Iteration 11/1000 | Loss: 0.00002325
Iteration 12/1000 | Loss: 0.00002304
Iteration 13/1000 | Loss: 0.00002294
Iteration 14/1000 | Loss: 0.00002273
Iteration 15/1000 | Loss: 0.00002270
Iteration 16/1000 | Loss: 0.00002263
Iteration 17/1000 | Loss: 0.00002261
Iteration 18/1000 | Loss: 0.00002260
Iteration 19/1000 | Loss: 0.00002257
Iteration 20/1000 | Loss: 0.00002256
Iteration 21/1000 | Loss: 0.00002256
Iteration 22/1000 | Loss: 0.00002255
Iteration 23/1000 | Loss: 0.00002252
Iteration 24/1000 | Loss: 0.00002251
Iteration 25/1000 | Loss: 0.00002251
Iteration 26/1000 | Loss: 0.00002251
Iteration 27/1000 | Loss: 0.00002250
Iteration 28/1000 | Loss: 0.00002250
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002247
Iteration 31/1000 | Loss: 0.00002246
Iteration 32/1000 | Loss: 0.00002245
Iteration 33/1000 | Loss: 0.00002245
Iteration 34/1000 | Loss: 0.00002244
Iteration 35/1000 | Loss: 0.00002241
Iteration 36/1000 | Loss: 0.00002240
Iteration 37/1000 | Loss: 0.00002240
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002238
Iteration 41/1000 | Loss: 0.00002237
Iteration 42/1000 | Loss: 0.00002237
Iteration 43/1000 | Loss: 0.00002237
Iteration 44/1000 | Loss: 0.00002236
Iteration 45/1000 | Loss: 0.00002236
Iteration 46/1000 | Loss: 0.00002236
Iteration 47/1000 | Loss: 0.00002236
Iteration 48/1000 | Loss: 0.00002236
Iteration 49/1000 | Loss: 0.00002236
Iteration 50/1000 | Loss: 0.00002236
Iteration 51/1000 | Loss: 0.00002236
Iteration 52/1000 | Loss: 0.00002236
Iteration 53/1000 | Loss: 0.00002236
Iteration 54/1000 | Loss: 0.00002236
Iteration 55/1000 | Loss: 0.00002236
Iteration 56/1000 | Loss: 0.00002236
Iteration 57/1000 | Loss: 0.00002236
Iteration 58/1000 | Loss: 0.00002236
Iteration 59/1000 | Loss: 0.00002236
Iteration 60/1000 | Loss: 0.00002236
Iteration 61/1000 | Loss: 0.00002236
Iteration 62/1000 | Loss: 0.00002236
Iteration 63/1000 | Loss: 0.00002236
Iteration 64/1000 | Loss: 0.00002236
Iteration 65/1000 | Loss: 0.00002236
Iteration 66/1000 | Loss: 0.00002236
Iteration 67/1000 | Loss: 0.00002236
Iteration 68/1000 | Loss: 0.00002236
Iteration 69/1000 | Loss: 0.00002236
Iteration 70/1000 | Loss: 0.00002236
Iteration 71/1000 | Loss: 0.00002236
Iteration 72/1000 | Loss: 0.00002236
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002236
Iteration 76/1000 | Loss: 0.00002236
Iteration 77/1000 | Loss: 0.00002236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.2359343347488903e-05, 2.2359343347488903e-05, 2.2359343347488903e-05, 2.2359343347488903e-05, 2.2359343347488903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2359343347488903e-05

Optimization complete. Final v2v error: 3.9108469486236572 mm

Highest mean error: 4.354996681213379 mm for frame 100

Lowest mean error: 3.3041536808013916 mm for frame 18

Saving results

Total time: 35.77465748786926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854400
Iteration 2/25 | Loss: 0.00122721
Iteration 3/25 | Loss: 0.00090144
Iteration 4/25 | Loss: 0.00084646
Iteration 5/25 | Loss: 0.00083674
Iteration 6/25 | Loss: 0.00083547
Iteration 7/25 | Loss: 0.00083547
Iteration 8/25 | Loss: 0.00083547
Iteration 9/25 | Loss: 0.00083547
Iteration 10/25 | Loss: 0.00083547
Iteration 11/25 | Loss: 0.00083547
Iteration 12/25 | Loss: 0.00083547
Iteration 13/25 | Loss: 0.00083547
Iteration 14/25 | Loss: 0.00083547
Iteration 15/25 | Loss: 0.00083547
Iteration 16/25 | Loss: 0.00083547
Iteration 17/25 | Loss: 0.00083547
Iteration 18/25 | Loss: 0.00083547
Iteration 19/25 | Loss: 0.00083547
Iteration 20/25 | Loss: 0.00083547
Iteration 21/25 | Loss: 0.00083547
Iteration 22/25 | Loss: 0.00083547
Iteration 23/25 | Loss: 0.00083547
Iteration 24/25 | Loss: 0.00083547
Iteration 25/25 | Loss: 0.00083547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60129440
Iteration 2/25 | Loss: 0.00128910
Iteration 3/25 | Loss: 0.00128909
Iteration 4/25 | Loss: 0.00128909
Iteration 5/25 | Loss: 0.00128909
Iteration 6/25 | Loss: 0.00128909
Iteration 7/25 | Loss: 0.00128909
Iteration 8/25 | Loss: 0.00128909
Iteration 9/25 | Loss: 0.00128909
Iteration 10/25 | Loss: 0.00128909
Iteration 11/25 | Loss: 0.00128909
Iteration 12/25 | Loss: 0.00128909
Iteration 13/25 | Loss: 0.00128909
Iteration 14/25 | Loss: 0.00128909
Iteration 15/25 | Loss: 0.00128909
Iteration 16/25 | Loss: 0.00128909
Iteration 17/25 | Loss: 0.00128909
Iteration 18/25 | Loss: 0.00128909
Iteration 19/25 | Loss: 0.00128909
Iteration 20/25 | Loss: 0.00128909
Iteration 21/25 | Loss: 0.00128909
Iteration 22/25 | Loss: 0.00128909
Iteration 23/25 | Loss: 0.00128909
Iteration 24/25 | Loss: 0.00128909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012890861835330725, 0.0012890861835330725, 0.0012890861835330725, 0.0012890861835330725, 0.0012890861835330725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012890861835330725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128909
Iteration 2/1000 | Loss: 0.00003633
Iteration 3/1000 | Loss: 0.00002794
Iteration 4/1000 | Loss: 0.00002600
Iteration 5/1000 | Loss: 0.00002443
Iteration 6/1000 | Loss: 0.00002361
Iteration 7/1000 | Loss: 0.00002312
Iteration 8/1000 | Loss: 0.00002284
Iteration 9/1000 | Loss: 0.00002266
Iteration 10/1000 | Loss: 0.00002242
Iteration 11/1000 | Loss: 0.00002216
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002203
Iteration 14/1000 | Loss: 0.00002197
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002193
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002192
Iteration 20/1000 | Loss: 0.00002192
Iteration 21/1000 | Loss: 0.00002192
Iteration 22/1000 | Loss: 0.00002192
Iteration 23/1000 | Loss: 0.00002192
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002192
Iteration 28/1000 | Loss: 0.00002192
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002191
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002190
Iteration 35/1000 | Loss: 0.00002190
Iteration 36/1000 | Loss: 0.00002189
Iteration 37/1000 | Loss: 0.00002189
Iteration 38/1000 | Loss: 0.00002188
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002186
Iteration 41/1000 | Loss: 0.00002186
Iteration 42/1000 | Loss: 0.00002185
Iteration 43/1000 | Loss: 0.00002184
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002183
Iteration 46/1000 | Loss: 0.00002182
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002180
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002178
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002176
Iteration 65/1000 | Loss: 0.00002176
Iteration 66/1000 | Loss: 0.00002176
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002175
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002173
Iteration 79/1000 | Loss: 0.00002172
Iteration 80/1000 | Loss: 0.00002172
Iteration 81/1000 | Loss: 0.00002172
Iteration 82/1000 | Loss: 0.00002172
Iteration 83/1000 | Loss: 0.00002172
Iteration 84/1000 | Loss: 0.00002172
Iteration 85/1000 | Loss: 0.00002171
Iteration 86/1000 | Loss: 0.00002171
Iteration 87/1000 | Loss: 0.00002171
Iteration 88/1000 | Loss: 0.00002171
Iteration 89/1000 | Loss: 0.00002171
Iteration 90/1000 | Loss: 0.00002171
Iteration 91/1000 | Loss: 0.00002171
Iteration 92/1000 | Loss: 0.00002171
Iteration 93/1000 | Loss: 0.00002171
Iteration 94/1000 | Loss: 0.00002171
Iteration 95/1000 | Loss: 0.00002171
Iteration 96/1000 | Loss: 0.00002171
Iteration 97/1000 | Loss: 0.00002171
Iteration 98/1000 | Loss: 0.00002171
Iteration 99/1000 | Loss: 0.00002171
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002171
Iteration 104/1000 | Loss: 0.00002171
Iteration 105/1000 | Loss: 0.00002171
Iteration 106/1000 | Loss: 0.00002171
Iteration 107/1000 | Loss: 0.00002171
Iteration 108/1000 | Loss: 0.00002171
Iteration 109/1000 | Loss: 0.00002171
Iteration 110/1000 | Loss: 0.00002171
Iteration 111/1000 | Loss: 0.00002171
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00002171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.1707168343709782e-05, 2.1707168343709782e-05, 2.1707168343709782e-05, 2.1707168343709782e-05, 2.1707168343709782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1707168343709782e-05

Optimization complete. Final v2v error: 3.880729913711548 mm

Highest mean error: 3.966531753540039 mm for frame 132

Lowest mean error: 3.818898916244507 mm for frame 56

Saving results

Total time: 34.547181844711304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060458
Iteration 2/25 | Loss: 0.00208820
Iteration 3/25 | Loss: 0.00178462
Iteration 4/25 | Loss: 0.00170859
Iteration 5/25 | Loss: 0.00163637
Iteration 6/25 | Loss: 0.00123861
Iteration 7/25 | Loss: 0.00107942
Iteration 8/25 | Loss: 0.00096863
Iteration 9/25 | Loss: 0.00094297
Iteration 10/25 | Loss: 0.00093980
Iteration 11/25 | Loss: 0.00093885
Iteration 12/25 | Loss: 0.00093871
Iteration 13/25 | Loss: 0.00093871
Iteration 14/25 | Loss: 0.00093871
Iteration 15/25 | Loss: 0.00093871
Iteration 16/25 | Loss: 0.00093871
Iteration 17/25 | Loss: 0.00093871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00093870545970276, 0.00093870545970276, 0.00093870545970276, 0.00093870545970276, 0.00093870545970276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00093870545970276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55731392
Iteration 2/25 | Loss: 0.00127851
Iteration 3/25 | Loss: 0.00127850
Iteration 4/25 | Loss: 0.00127850
Iteration 5/25 | Loss: 0.00127850
Iteration 6/25 | Loss: 0.00127850
Iteration 7/25 | Loss: 0.00127850
Iteration 8/25 | Loss: 0.00127850
Iteration 9/25 | Loss: 0.00127850
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012785030994564295, 0.0012785030994564295, 0.0012785030994564295, 0.0012785030994564295, 0.0012785030994564295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012785030994564295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127850
Iteration 2/1000 | Loss: 0.00003698
Iteration 3/1000 | Loss: 0.00002965
Iteration 4/1000 | Loss: 0.00002697
Iteration 5/1000 | Loss: 0.00002558
Iteration 6/1000 | Loss: 0.00002496
Iteration 7/1000 | Loss: 0.00002450
Iteration 8/1000 | Loss: 0.00002425
Iteration 9/1000 | Loss: 0.00002406
Iteration 10/1000 | Loss: 0.00002405
Iteration 11/1000 | Loss: 0.00002401
Iteration 12/1000 | Loss: 0.00002401
Iteration 13/1000 | Loss: 0.00002398
Iteration 14/1000 | Loss: 0.00002397
Iteration 15/1000 | Loss: 0.00002397
Iteration 16/1000 | Loss: 0.00002397
Iteration 17/1000 | Loss: 0.00002395
Iteration 18/1000 | Loss: 0.00002393
Iteration 19/1000 | Loss: 0.00002393
Iteration 20/1000 | Loss: 0.00002393
Iteration 21/1000 | Loss: 0.00002393
Iteration 22/1000 | Loss: 0.00002393
Iteration 23/1000 | Loss: 0.00002393
Iteration 24/1000 | Loss: 0.00002393
Iteration 25/1000 | Loss: 0.00002393
Iteration 26/1000 | Loss: 0.00002392
Iteration 27/1000 | Loss: 0.00002392
Iteration 28/1000 | Loss: 0.00002391
Iteration 29/1000 | Loss: 0.00002390
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002389
Iteration 32/1000 | Loss: 0.00002386
Iteration 33/1000 | Loss: 0.00002386
Iteration 34/1000 | Loss: 0.00002386
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00002386
Iteration 37/1000 | Loss: 0.00002386
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002386
Iteration 40/1000 | Loss: 0.00002386
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002385
Iteration 43/1000 | Loss: 0.00002385
Iteration 44/1000 | Loss: 0.00002384
Iteration 45/1000 | Loss: 0.00002384
Iteration 46/1000 | Loss: 0.00002384
Iteration 47/1000 | Loss: 0.00002384
Iteration 48/1000 | Loss: 0.00002384
Iteration 49/1000 | Loss: 0.00002383
Iteration 50/1000 | Loss: 0.00002383
Iteration 51/1000 | Loss: 0.00002383
Iteration 52/1000 | Loss: 0.00002383
Iteration 53/1000 | Loss: 0.00002383
Iteration 54/1000 | Loss: 0.00002382
Iteration 55/1000 | Loss: 0.00002382
Iteration 56/1000 | Loss: 0.00002382
Iteration 57/1000 | Loss: 0.00002382
Iteration 58/1000 | Loss: 0.00002381
Iteration 59/1000 | Loss: 0.00002381
Iteration 60/1000 | Loss: 0.00002381
Iteration 61/1000 | Loss: 0.00002381
Iteration 62/1000 | Loss: 0.00002380
Iteration 63/1000 | Loss: 0.00002380
Iteration 64/1000 | Loss: 0.00002380
Iteration 65/1000 | Loss: 0.00002379
Iteration 66/1000 | Loss: 0.00002379
Iteration 67/1000 | Loss: 0.00002379
Iteration 68/1000 | Loss: 0.00002379
Iteration 69/1000 | Loss: 0.00002379
Iteration 70/1000 | Loss: 0.00002378
Iteration 71/1000 | Loss: 0.00002378
Iteration 72/1000 | Loss: 0.00002378
Iteration 73/1000 | Loss: 0.00002377
Iteration 74/1000 | Loss: 0.00002377
Iteration 75/1000 | Loss: 0.00002377
Iteration 76/1000 | Loss: 0.00002377
Iteration 77/1000 | Loss: 0.00002377
Iteration 78/1000 | Loss: 0.00002376
Iteration 79/1000 | Loss: 0.00002376
Iteration 80/1000 | Loss: 0.00002376
Iteration 81/1000 | Loss: 0.00002376
Iteration 82/1000 | Loss: 0.00002376
Iteration 83/1000 | Loss: 0.00002376
Iteration 84/1000 | Loss: 0.00002376
Iteration 85/1000 | Loss: 0.00002376
Iteration 86/1000 | Loss: 0.00002376
Iteration 87/1000 | Loss: 0.00002376
Iteration 88/1000 | Loss: 0.00002375
Iteration 89/1000 | Loss: 0.00002375
Iteration 90/1000 | Loss: 0.00002375
Iteration 91/1000 | Loss: 0.00002374
Iteration 92/1000 | Loss: 0.00002374
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002374
Iteration 95/1000 | Loss: 0.00002373
Iteration 96/1000 | Loss: 0.00002373
Iteration 97/1000 | Loss: 0.00002373
Iteration 98/1000 | Loss: 0.00002373
Iteration 99/1000 | Loss: 0.00002373
Iteration 100/1000 | Loss: 0.00002373
Iteration 101/1000 | Loss: 0.00002373
Iteration 102/1000 | Loss: 0.00002373
Iteration 103/1000 | Loss: 0.00002373
Iteration 104/1000 | Loss: 0.00002372
Iteration 105/1000 | Loss: 0.00002372
Iteration 106/1000 | Loss: 0.00002372
Iteration 107/1000 | Loss: 0.00002372
Iteration 108/1000 | Loss: 0.00002372
Iteration 109/1000 | Loss: 0.00002372
Iteration 110/1000 | Loss: 0.00002372
Iteration 111/1000 | Loss: 0.00002372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.3721728211967275e-05, 2.3721728211967275e-05, 2.3721728211967275e-05, 2.3721728211967275e-05, 2.3721728211967275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3721728211967275e-05

Optimization complete. Final v2v error: 4.091200351715088 mm

Highest mean error: 4.370781421661377 mm for frame 181

Lowest mean error: 3.9330272674560547 mm for frame 44

Saving results

Total time: 45.99638605117798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781325
Iteration 2/25 | Loss: 0.00134677
Iteration 3/25 | Loss: 0.00100219
Iteration 4/25 | Loss: 0.00086314
Iteration 5/25 | Loss: 0.00081833
Iteration 6/25 | Loss: 0.00081186
Iteration 7/25 | Loss: 0.00079909
Iteration 8/25 | Loss: 0.00078828
Iteration 9/25 | Loss: 0.00077947
Iteration 10/25 | Loss: 0.00077197
Iteration 11/25 | Loss: 0.00076802
Iteration 12/25 | Loss: 0.00076603
Iteration 13/25 | Loss: 0.00076535
Iteration 14/25 | Loss: 0.00076508
Iteration 15/25 | Loss: 0.00076495
Iteration 16/25 | Loss: 0.00076493
Iteration 17/25 | Loss: 0.00076493
Iteration 18/25 | Loss: 0.00076493
Iteration 19/25 | Loss: 0.00076493
Iteration 20/25 | Loss: 0.00076493
Iteration 21/25 | Loss: 0.00076493
Iteration 22/25 | Loss: 0.00076493
Iteration 23/25 | Loss: 0.00076493
Iteration 24/25 | Loss: 0.00076493
Iteration 25/25 | Loss: 0.00076492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.91754818
Iteration 2/25 | Loss: 0.00126469
Iteration 3/25 | Loss: 0.00126466
Iteration 4/25 | Loss: 0.00126466
Iteration 5/25 | Loss: 0.00126466
Iteration 6/25 | Loss: 0.00126465
Iteration 7/25 | Loss: 0.00126465
Iteration 8/25 | Loss: 0.00126465
Iteration 9/25 | Loss: 0.00126465
Iteration 10/25 | Loss: 0.00126465
Iteration 11/25 | Loss: 0.00126465
Iteration 12/25 | Loss: 0.00126465
Iteration 13/25 | Loss: 0.00126465
Iteration 14/25 | Loss: 0.00126465
Iteration 15/25 | Loss: 0.00126465
Iteration 16/25 | Loss: 0.00126465
Iteration 17/25 | Loss: 0.00126465
Iteration 18/25 | Loss: 0.00126465
Iteration 19/25 | Loss: 0.00126465
Iteration 20/25 | Loss: 0.00126465
Iteration 21/25 | Loss: 0.00126465
Iteration 22/25 | Loss: 0.00126465
Iteration 23/25 | Loss: 0.00126465
Iteration 24/25 | Loss: 0.00126465
Iteration 25/25 | Loss: 0.00126465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126465
Iteration 2/1000 | Loss: 0.00003336
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00006896
Iteration 5/1000 | Loss: 0.00043861
Iteration 6/1000 | Loss: 0.00002771
Iteration 7/1000 | Loss: 0.00002179
Iteration 8/1000 | Loss: 0.00002001
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001806
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001763
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001724
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001711
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001711
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001710
Iteration 44/1000 | Loss: 0.00001710
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00001709
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001706
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001706
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001704
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001703
Iteration 82/1000 | Loss: 0.00001703
Iteration 83/1000 | Loss: 0.00001703
Iteration 84/1000 | Loss: 0.00001702
Iteration 85/1000 | Loss: 0.00001702
Iteration 86/1000 | Loss: 0.00001702
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001702
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Iteration 106/1000 | Loss: 0.00001700
Iteration 107/1000 | Loss: 0.00001700
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001699
Iteration 120/1000 | Loss: 0.00001699
Iteration 121/1000 | Loss: 0.00001699
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001698
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001698
Iteration 126/1000 | Loss: 0.00001698
Iteration 127/1000 | Loss: 0.00001698
Iteration 128/1000 | Loss: 0.00001698
Iteration 129/1000 | Loss: 0.00001698
Iteration 130/1000 | Loss: 0.00001698
Iteration 131/1000 | Loss: 0.00001698
Iteration 132/1000 | Loss: 0.00001698
Iteration 133/1000 | Loss: 0.00001698
Iteration 134/1000 | Loss: 0.00001698
Iteration 135/1000 | Loss: 0.00001698
Iteration 136/1000 | Loss: 0.00001698
Iteration 137/1000 | Loss: 0.00001698
Iteration 138/1000 | Loss: 0.00001698
Iteration 139/1000 | Loss: 0.00001698
Iteration 140/1000 | Loss: 0.00001698
Iteration 141/1000 | Loss: 0.00001698
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001698
Iteration 144/1000 | Loss: 0.00001698
Iteration 145/1000 | Loss: 0.00001698
Iteration 146/1000 | Loss: 0.00001698
Iteration 147/1000 | Loss: 0.00001698
Iteration 148/1000 | Loss: 0.00001698
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001698
Iteration 152/1000 | Loss: 0.00001698
Iteration 153/1000 | Loss: 0.00001698
Iteration 154/1000 | Loss: 0.00001698
Iteration 155/1000 | Loss: 0.00001698
Iteration 156/1000 | Loss: 0.00001698
Iteration 157/1000 | Loss: 0.00001698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.698354935797397e-05, 1.698354935797397e-05, 1.698354935797397e-05, 1.698354935797397e-05, 1.698354935797397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.698354935797397e-05

Optimization complete. Final v2v error: 3.5035340785980225 mm

Highest mean error: 4.088983058929443 mm for frame 20

Lowest mean error: 3.1352522373199463 mm for frame 44

Saving results

Total time: 67.32737493515015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391669
Iteration 2/25 | Loss: 0.00098201
Iteration 3/25 | Loss: 0.00082182
Iteration 4/25 | Loss: 0.00078625
Iteration 5/25 | Loss: 0.00077625
Iteration 6/25 | Loss: 0.00077387
Iteration 7/25 | Loss: 0.00077311
Iteration 8/25 | Loss: 0.00077294
Iteration 9/25 | Loss: 0.00077294
Iteration 10/25 | Loss: 0.00077294
Iteration 11/25 | Loss: 0.00077294
Iteration 12/25 | Loss: 0.00077294
Iteration 13/25 | Loss: 0.00077294
Iteration 14/25 | Loss: 0.00077294
Iteration 15/25 | Loss: 0.00077294
Iteration 16/25 | Loss: 0.00077294
Iteration 17/25 | Loss: 0.00077294
Iteration 18/25 | Loss: 0.00077294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000772937317378819, 0.000772937317378819, 0.000772937317378819, 0.000772937317378819, 0.000772937317378819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000772937317378819

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.04293633
Iteration 2/25 | Loss: 0.00172227
Iteration 3/25 | Loss: 0.00172222
Iteration 4/25 | Loss: 0.00172221
Iteration 5/25 | Loss: 0.00172221
Iteration 6/25 | Loss: 0.00172221
Iteration 7/25 | Loss: 0.00172221
Iteration 8/25 | Loss: 0.00172221
Iteration 9/25 | Loss: 0.00172221
Iteration 10/25 | Loss: 0.00172221
Iteration 11/25 | Loss: 0.00172221
Iteration 12/25 | Loss: 0.00172221
Iteration 13/25 | Loss: 0.00172221
Iteration 14/25 | Loss: 0.00172221
Iteration 15/25 | Loss: 0.00172221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017222125316038728, 0.0017222125316038728, 0.0017222125316038728, 0.0017222125316038728, 0.0017222125316038728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017222125316038728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172221
Iteration 2/1000 | Loss: 0.00003834
Iteration 3/1000 | Loss: 0.00002717
Iteration 4/1000 | Loss: 0.00001983
Iteration 5/1000 | Loss: 0.00001739
Iteration 6/1000 | Loss: 0.00001640
Iteration 7/1000 | Loss: 0.00001550
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001433
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001384
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001370
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001363
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001344
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001328
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001326
Iteration 100/1000 | Loss: 0.00001326
Iteration 101/1000 | Loss: 0.00001326
Iteration 102/1000 | Loss: 0.00001326
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Iteration 137/1000 | Loss: 0.00001322
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001320
Iteration 156/1000 | Loss: 0.00001320
Iteration 157/1000 | Loss: 0.00001320
Iteration 158/1000 | Loss: 0.00001320
Iteration 159/1000 | Loss: 0.00001320
Iteration 160/1000 | Loss: 0.00001320
Iteration 161/1000 | Loss: 0.00001320
Iteration 162/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.319608691119356e-05, 1.319608691119356e-05, 1.319608691119356e-05, 1.319608691119356e-05, 1.319608691119356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.319608691119356e-05

Optimization complete. Final v2v error: 3.0825095176696777 mm

Highest mean error: 3.6297476291656494 mm for frame 8

Lowest mean error: 2.581617593765259 mm for frame 102

Saving results

Total time: 46.14163374900818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832720
Iteration 2/25 | Loss: 0.00115449
Iteration 3/25 | Loss: 0.00081176
Iteration 4/25 | Loss: 0.00078224
Iteration 5/25 | Loss: 0.00077423
Iteration 6/25 | Loss: 0.00077248
Iteration 7/25 | Loss: 0.00077222
Iteration 8/25 | Loss: 0.00077222
Iteration 9/25 | Loss: 0.00077222
Iteration 10/25 | Loss: 0.00077222
Iteration 11/25 | Loss: 0.00077222
Iteration 12/25 | Loss: 0.00077222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007722174632363021, 0.0007722174632363021, 0.0007722174632363021, 0.0007722174632363021, 0.0007722174632363021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007722174632363021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36227047
Iteration 2/25 | Loss: 0.00117800
Iteration 3/25 | Loss: 0.00117800
Iteration 4/25 | Loss: 0.00117800
Iteration 5/25 | Loss: 0.00117799
Iteration 6/25 | Loss: 0.00117799
Iteration 7/25 | Loss: 0.00117799
Iteration 8/25 | Loss: 0.00117799
Iteration 9/25 | Loss: 0.00117799
Iteration 10/25 | Loss: 0.00117799
Iteration 11/25 | Loss: 0.00117799
Iteration 12/25 | Loss: 0.00117799
Iteration 13/25 | Loss: 0.00117799
Iteration 14/25 | Loss: 0.00117799
Iteration 15/25 | Loss: 0.00117799
Iteration 16/25 | Loss: 0.00117799
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011779933702200651, 0.0011779933702200651, 0.0011779933702200651, 0.0011779933702200651, 0.0011779933702200651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011779933702200651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117799
Iteration 2/1000 | Loss: 0.00003887
Iteration 3/1000 | Loss: 0.00002836
Iteration 4/1000 | Loss: 0.00002418
Iteration 5/1000 | Loss: 0.00002265
Iteration 6/1000 | Loss: 0.00002144
Iteration 7/1000 | Loss: 0.00002072
Iteration 8/1000 | Loss: 0.00002016
Iteration 9/1000 | Loss: 0.00001971
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001925
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00001908
Iteration 14/1000 | Loss: 0.00001901
Iteration 15/1000 | Loss: 0.00001884
Iteration 16/1000 | Loss: 0.00001878
Iteration 17/1000 | Loss: 0.00001873
Iteration 18/1000 | Loss: 0.00001868
Iteration 19/1000 | Loss: 0.00001864
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00001861
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001860
Iteration 24/1000 | Loss: 0.00001859
Iteration 25/1000 | Loss: 0.00001859
Iteration 26/1000 | Loss: 0.00001858
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001857
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001856
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001854
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001841
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001839
Iteration 65/1000 | Loss: 0.00001839
Iteration 66/1000 | Loss: 0.00001838
Iteration 67/1000 | Loss: 0.00001837
Iteration 68/1000 | Loss: 0.00001837
Iteration 69/1000 | Loss: 0.00001837
Iteration 70/1000 | Loss: 0.00001836
Iteration 71/1000 | Loss: 0.00001836
Iteration 72/1000 | Loss: 0.00001834
Iteration 73/1000 | Loss: 0.00001833
Iteration 74/1000 | Loss: 0.00001833
Iteration 75/1000 | Loss: 0.00001833
Iteration 76/1000 | Loss: 0.00001833
Iteration 77/1000 | Loss: 0.00001832
Iteration 78/1000 | Loss: 0.00001832
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001830
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001828
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001827
Iteration 108/1000 | Loss: 0.00001827
Iteration 109/1000 | Loss: 0.00001827
Iteration 110/1000 | Loss: 0.00001827
Iteration 111/1000 | Loss: 0.00001827
Iteration 112/1000 | Loss: 0.00001827
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001827
Iteration 115/1000 | Loss: 0.00001827
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001826
Iteration 119/1000 | Loss: 0.00001826
Iteration 120/1000 | Loss: 0.00001826
Iteration 121/1000 | Loss: 0.00001826
Iteration 122/1000 | Loss: 0.00001825
Iteration 123/1000 | Loss: 0.00001825
Iteration 124/1000 | Loss: 0.00001825
Iteration 125/1000 | Loss: 0.00001825
Iteration 126/1000 | Loss: 0.00001825
Iteration 127/1000 | Loss: 0.00001824
Iteration 128/1000 | Loss: 0.00001824
Iteration 129/1000 | Loss: 0.00001824
Iteration 130/1000 | Loss: 0.00001824
Iteration 131/1000 | Loss: 0.00001824
Iteration 132/1000 | Loss: 0.00001824
Iteration 133/1000 | Loss: 0.00001824
Iteration 134/1000 | Loss: 0.00001824
Iteration 135/1000 | Loss: 0.00001824
Iteration 136/1000 | Loss: 0.00001824
Iteration 137/1000 | Loss: 0.00001824
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001823
Iteration 143/1000 | Loss: 0.00001823
Iteration 144/1000 | Loss: 0.00001823
Iteration 145/1000 | Loss: 0.00001823
Iteration 146/1000 | Loss: 0.00001822
Iteration 147/1000 | Loss: 0.00001822
Iteration 148/1000 | Loss: 0.00001822
Iteration 149/1000 | Loss: 0.00001822
Iteration 150/1000 | Loss: 0.00001822
Iteration 151/1000 | Loss: 0.00001822
Iteration 152/1000 | Loss: 0.00001822
Iteration 153/1000 | Loss: 0.00001821
Iteration 154/1000 | Loss: 0.00001821
Iteration 155/1000 | Loss: 0.00001821
Iteration 156/1000 | Loss: 0.00001821
Iteration 157/1000 | Loss: 0.00001821
Iteration 158/1000 | Loss: 0.00001821
Iteration 159/1000 | Loss: 0.00001821
Iteration 160/1000 | Loss: 0.00001821
Iteration 161/1000 | Loss: 0.00001821
Iteration 162/1000 | Loss: 0.00001821
Iteration 163/1000 | Loss: 0.00001821
Iteration 164/1000 | Loss: 0.00001821
Iteration 165/1000 | Loss: 0.00001821
Iteration 166/1000 | Loss: 0.00001821
Iteration 167/1000 | Loss: 0.00001821
Iteration 168/1000 | Loss: 0.00001821
Iteration 169/1000 | Loss: 0.00001821
Iteration 170/1000 | Loss: 0.00001821
Iteration 171/1000 | Loss: 0.00001821
Iteration 172/1000 | Loss: 0.00001821
Iteration 173/1000 | Loss: 0.00001821
Iteration 174/1000 | Loss: 0.00001821
Iteration 175/1000 | Loss: 0.00001821
Iteration 176/1000 | Loss: 0.00001821
Iteration 177/1000 | Loss: 0.00001821
Iteration 178/1000 | Loss: 0.00001821
Iteration 179/1000 | Loss: 0.00001821
Iteration 180/1000 | Loss: 0.00001821
Iteration 181/1000 | Loss: 0.00001821
Iteration 182/1000 | Loss: 0.00001821
Iteration 183/1000 | Loss: 0.00001821
Iteration 184/1000 | Loss: 0.00001821
Iteration 185/1000 | Loss: 0.00001821
Iteration 186/1000 | Loss: 0.00001821
Iteration 187/1000 | Loss: 0.00001821
Iteration 188/1000 | Loss: 0.00001821
Iteration 189/1000 | Loss: 0.00001821
Iteration 190/1000 | Loss: 0.00001821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.821087789721787e-05, 1.821087789721787e-05, 1.821087789721787e-05, 1.821087789721787e-05, 1.821087789721787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.821087789721787e-05

Optimization complete. Final v2v error: 3.5383992195129395 mm

Highest mean error: 5.063458442687988 mm for frame 64

Lowest mean error: 2.8839638233184814 mm for frame 95

Saving results

Total time: 45.35539174079895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869040
Iteration 2/25 | Loss: 0.00140583
Iteration 3/25 | Loss: 0.00099821
Iteration 4/25 | Loss: 0.00088267
Iteration 5/25 | Loss: 0.00086631
Iteration 6/25 | Loss: 0.00086453
Iteration 7/25 | Loss: 0.00086453
Iteration 8/25 | Loss: 0.00086453
Iteration 9/25 | Loss: 0.00086453
Iteration 10/25 | Loss: 0.00086453
Iteration 11/25 | Loss: 0.00086453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008645277703180909, 0.0008645277703180909, 0.0008645277703180909, 0.0008645277703180909, 0.0008645277703180909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008645277703180909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60667896
Iteration 2/25 | Loss: 0.00133900
Iteration 3/25 | Loss: 0.00133900
Iteration 4/25 | Loss: 0.00133900
Iteration 5/25 | Loss: 0.00133900
Iteration 6/25 | Loss: 0.00133900
Iteration 7/25 | Loss: 0.00133900
Iteration 8/25 | Loss: 0.00133899
Iteration 9/25 | Loss: 0.00133899
Iteration 10/25 | Loss: 0.00133899
Iteration 11/25 | Loss: 0.00133899
Iteration 12/25 | Loss: 0.00133899
Iteration 13/25 | Loss: 0.00133899
Iteration 14/25 | Loss: 0.00133899
Iteration 15/25 | Loss: 0.00133899
Iteration 16/25 | Loss: 0.00133899
Iteration 17/25 | Loss: 0.00133899
Iteration 18/25 | Loss: 0.00133899
Iteration 19/25 | Loss: 0.00133899
Iteration 20/25 | Loss: 0.00133899
Iteration 21/25 | Loss: 0.00133899
Iteration 22/25 | Loss: 0.00133899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013389940140768886, 0.0013389940140768886, 0.0013389940140768886, 0.0013389940140768886, 0.0013389940140768886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013389940140768886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133899
Iteration 2/1000 | Loss: 0.00003399
Iteration 3/1000 | Loss: 0.00002851
Iteration 4/1000 | Loss: 0.00002617
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002443
Iteration 7/1000 | Loss: 0.00002392
Iteration 8/1000 | Loss: 0.00002364
Iteration 9/1000 | Loss: 0.00002327
Iteration 10/1000 | Loss: 0.00002308
Iteration 11/1000 | Loss: 0.00002284
Iteration 12/1000 | Loss: 0.00002272
Iteration 13/1000 | Loss: 0.00002267
Iteration 14/1000 | Loss: 0.00002266
Iteration 15/1000 | Loss: 0.00002265
Iteration 16/1000 | Loss: 0.00002262
Iteration 17/1000 | Loss: 0.00002262
Iteration 18/1000 | Loss: 0.00002262
Iteration 19/1000 | Loss: 0.00002262
Iteration 20/1000 | Loss: 0.00002262
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002261
Iteration 23/1000 | Loss: 0.00002261
Iteration 24/1000 | Loss: 0.00002261
Iteration 25/1000 | Loss: 0.00002260
Iteration 26/1000 | Loss: 0.00002260
Iteration 27/1000 | Loss: 0.00002259
Iteration 28/1000 | Loss: 0.00002259
Iteration 29/1000 | Loss: 0.00002259
Iteration 30/1000 | Loss: 0.00002258
Iteration 31/1000 | Loss: 0.00002257
Iteration 32/1000 | Loss: 0.00002255
Iteration 33/1000 | Loss: 0.00002255
Iteration 34/1000 | Loss: 0.00002255
Iteration 35/1000 | Loss: 0.00002252
Iteration 36/1000 | Loss: 0.00002252
Iteration 37/1000 | Loss: 0.00002251
Iteration 38/1000 | Loss: 0.00002251
Iteration 39/1000 | Loss: 0.00002251
Iteration 40/1000 | Loss: 0.00002250
Iteration 41/1000 | Loss: 0.00002250
Iteration 42/1000 | Loss: 0.00002250
Iteration 43/1000 | Loss: 0.00002249
Iteration 44/1000 | Loss: 0.00002249
Iteration 45/1000 | Loss: 0.00002248
Iteration 46/1000 | Loss: 0.00002248
Iteration 47/1000 | Loss: 0.00002248
Iteration 48/1000 | Loss: 0.00002247
Iteration 49/1000 | Loss: 0.00002247
Iteration 50/1000 | Loss: 0.00002246
Iteration 51/1000 | Loss: 0.00002246
Iteration 52/1000 | Loss: 0.00002246
Iteration 53/1000 | Loss: 0.00002245
Iteration 54/1000 | Loss: 0.00002245
Iteration 55/1000 | Loss: 0.00002245
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00002245
Iteration 58/1000 | Loss: 0.00002245
Iteration 59/1000 | Loss: 0.00002245
Iteration 60/1000 | Loss: 0.00002245
Iteration 61/1000 | Loss: 0.00002244
Iteration 62/1000 | Loss: 0.00002244
Iteration 63/1000 | Loss: 0.00002244
Iteration 64/1000 | Loss: 0.00002244
Iteration 65/1000 | Loss: 0.00002244
Iteration 66/1000 | Loss: 0.00002244
Iteration 67/1000 | Loss: 0.00002244
Iteration 68/1000 | Loss: 0.00002244
Iteration 69/1000 | Loss: 0.00002244
Iteration 70/1000 | Loss: 0.00002244
Iteration 71/1000 | Loss: 0.00002244
Iteration 72/1000 | Loss: 0.00002244
Iteration 73/1000 | Loss: 0.00002243
Iteration 74/1000 | Loss: 0.00002243
Iteration 75/1000 | Loss: 0.00002241
Iteration 76/1000 | Loss: 0.00002241
Iteration 77/1000 | Loss: 0.00002241
Iteration 78/1000 | Loss: 0.00002241
Iteration 79/1000 | Loss: 0.00002241
Iteration 80/1000 | Loss: 0.00002241
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00002241
Iteration 83/1000 | Loss: 0.00002241
Iteration 84/1000 | Loss: 0.00002240
Iteration 85/1000 | Loss: 0.00002240
Iteration 86/1000 | Loss: 0.00002240
Iteration 87/1000 | Loss: 0.00002240
Iteration 88/1000 | Loss: 0.00002239
Iteration 89/1000 | Loss: 0.00002239
Iteration 90/1000 | Loss: 0.00002239
Iteration 91/1000 | Loss: 0.00002239
Iteration 92/1000 | Loss: 0.00002239
Iteration 93/1000 | Loss: 0.00002239
Iteration 94/1000 | Loss: 0.00002239
Iteration 95/1000 | Loss: 0.00002239
Iteration 96/1000 | Loss: 0.00002239
Iteration 97/1000 | Loss: 0.00002239
Iteration 98/1000 | Loss: 0.00002238
Iteration 99/1000 | Loss: 0.00002238
Iteration 100/1000 | Loss: 0.00002238
Iteration 101/1000 | Loss: 0.00002238
Iteration 102/1000 | Loss: 0.00002238
Iteration 103/1000 | Loss: 0.00002238
Iteration 104/1000 | Loss: 0.00002238
Iteration 105/1000 | Loss: 0.00002237
Iteration 106/1000 | Loss: 0.00002237
Iteration 107/1000 | Loss: 0.00002237
Iteration 108/1000 | Loss: 0.00002237
Iteration 109/1000 | Loss: 0.00002237
Iteration 110/1000 | Loss: 0.00002237
Iteration 111/1000 | Loss: 0.00002237
Iteration 112/1000 | Loss: 0.00002237
Iteration 113/1000 | Loss: 0.00002237
Iteration 114/1000 | Loss: 0.00002237
Iteration 115/1000 | Loss: 0.00002237
Iteration 116/1000 | Loss: 0.00002236
Iteration 117/1000 | Loss: 0.00002236
Iteration 118/1000 | Loss: 0.00002236
Iteration 119/1000 | Loss: 0.00002236
Iteration 120/1000 | Loss: 0.00002236
Iteration 121/1000 | Loss: 0.00002236
Iteration 122/1000 | Loss: 0.00002236
Iteration 123/1000 | Loss: 0.00002236
Iteration 124/1000 | Loss: 0.00002236
Iteration 125/1000 | Loss: 0.00002236
Iteration 126/1000 | Loss: 0.00002236
Iteration 127/1000 | Loss: 0.00002236
Iteration 128/1000 | Loss: 0.00002236
Iteration 129/1000 | Loss: 0.00002236
Iteration 130/1000 | Loss: 0.00002235
Iteration 131/1000 | Loss: 0.00002235
Iteration 132/1000 | Loss: 0.00002235
Iteration 133/1000 | Loss: 0.00002235
Iteration 134/1000 | Loss: 0.00002235
Iteration 135/1000 | Loss: 0.00002235
Iteration 136/1000 | Loss: 0.00002235
Iteration 137/1000 | Loss: 0.00002235
Iteration 138/1000 | Loss: 0.00002235
Iteration 139/1000 | Loss: 0.00002235
Iteration 140/1000 | Loss: 0.00002235
Iteration 141/1000 | Loss: 0.00002235
Iteration 142/1000 | Loss: 0.00002235
Iteration 143/1000 | Loss: 0.00002235
Iteration 144/1000 | Loss: 0.00002235
Iteration 145/1000 | Loss: 0.00002235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.2352563973981887e-05, 2.2352563973981887e-05, 2.2352563973981887e-05, 2.2352563973981887e-05, 2.2352563973981887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2352563973981887e-05

Optimization complete. Final v2v error: 3.9574034214019775 mm

Highest mean error: 4.136415958404541 mm for frame 2

Lowest mean error: 3.7147715091705322 mm for frame 169

Saving results

Total time: 37.27021288871765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422635
Iteration 2/25 | Loss: 0.00090432
Iteration 3/25 | Loss: 0.00080727
Iteration 4/25 | Loss: 0.00077831
Iteration 5/25 | Loss: 0.00077203
Iteration 6/25 | Loss: 0.00077036
Iteration 7/25 | Loss: 0.00076984
Iteration 8/25 | Loss: 0.00076978
Iteration 9/25 | Loss: 0.00076978
Iteration 10/25 | Loss: 0.00076978
Iteration 11/25 | Loss: 0.00076978
Iteration 12/25 | Loss: 0.00076977
Iteration 13/25 | Loss: 0.00076977
Iteration 14/25 | Loss: 0.00076977
Iteration 15/25 | Loss: 0.00076977
Iteration 16/25 | Loss: 0.00076977
Iteration 17/25 | Loss: 0.00076977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007697730325162411, 0.0007697730325162411, 0.0007697730325162411, 0.0007697730325162411, 0.0007697730325162411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007697730325162411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60194659
Iteration 2/25 | Loss: 0.00122718
Iteration 3/25 | Loss: 0.00122717
Iteration 4/25 | Loss: 0.00122717
Iteration 5/25 | Loss: 0.00122717
Iteration 6/25 | Loss: 0.00122717
Iteration 7/25 | Loss: 0.00122717
Iteration 8/25 | Loss: 0.00122717
Iteration 9/25 | Loss: 0.00122717
Iteration 10/25 | Loss: 0.00122717
Iteration 11/25 | Loss: 0.00122717
Iteration 12/25 | Loss: 0.00122717
Iteration 13/25 | Loss: 0.00122717
Iteration 14/25 | Loss: 0.00122717
Iteration 15/25 | Loss: 0.00122717
Iteration 16/25 | Loss: 0.00122717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012271706946194172, 0.0012271706946194172, 0.0012271706946194172, 0.0012271706946194172, 0.0012271706946194172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012271706946194172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122717
Iteration 2/1000 | Loss: 0.00003645
Iteration 3/1000 | Loss: 0.00002303
Iteration 4/1000 | Loss: 0.00001980
Iteration 5/1000 | Loss: 0.00001861
Iteration 6/1000 | Loss: 0.00001795
Iteration 7/1000 | Loss: 0.00001742
Iteration 8/1000 | Loss: 0.00001733
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001681
Iteration 14/1000 | Loss: 0.00001680
Iteration 15/1000 | Loss: 0.00001678
Iteration 16/1000 | Loss: 0.00001678
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001673
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001670
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001668
Iteration 25/1000 | Loss: 0.00001667
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001665
Iteration 29/1000 | Loss: 0.00001665
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001665
Iteration 32/1000 | Loss: 0.00001665
Iteration 33/1000 | Loss: 0.00001665
Iteration 34/1000 | Loss: 0.00001665
Iteration 35/1000 | Loss: 0.00001665
Iteration 36/1000 | Loss: 0.00001664
Iteration 37/1000 | Loss: 0.00001664
Iteration 38/1000 | Loss: 0.00001664
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001662
Iteration 42/1000 | Loss: 0.00001662
Iteration 43/1000 | Loss: 0.00001662
Iteration 44/1000 | Loss: 0.00001661
Iteration 45/1000 | Loss: 0.00001661
Iteration 46/1000 | Loss: 0.00001661
Iteration 47/1000 | Loss: 0.00001661
Iteration 48/1000 | Loss: 0.00001661
Iteration 49/1000 | Loss: 0.00001661
Iteration 50/1000 | Loss: 0.00001661
Iteration 51/1000 | Loss: 0.00001660
Iteration 52/1000 | Loss: 0.00001660
Iteration 53/1000 | Loss: 0.00001660
Iteration 54/1000 | Loss: 0.00001659
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001659
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001659
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001658
Iteration 64/1000 | Loss: 0.00001658
Iteration 65/1000 | Loss: 0.00001658
Iteration 66/1000 | Loss: 0.00001658
Iteration 67/1000 | Loss: 0.00001658
Iteration 68/1000 | Loss: 0.00001658
Iteration 69/1000 | Loss: 0.00001658
Iteration 70/1000 | Loss: 0.00001658
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001658
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001657
Iteration 76/1000 | Loss: 0.00001657
Iteration 77/1000 | Loss: 0.00001656
Iteration 78/1000 | Loss: 0.00001656
Iteration 79/1000 | Loss: 0.00001656
Iteration 80/1000 | Loss: 0.00001656
Iteration 81/1000 | Loss: 0.00001656
Iteration 82/1000 | Loss: 0.00001655
Iteration 83/1000 | Loss: 0.00001655
Iteration 84/1000 | Loss: 0.00001655
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001654
Iteration 87/1000 | Loss: 0.00001654
Iteration 88/1000 | Loss: 0.00001654
Iteration 89/1000 | Loss: 0.00001653
Iteration 90/1000 | Loss: 0.00001653
Iteration 91/1000 | Loss: 0.00001652
Iteration 92/1000 | Loss: 0.00001652
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001652
Iteration 95/1000 | Loss: 0.00001651
Iteration 96/1000 | Loss: 0.00001648
Iteration 97/1000 | Loss: 0.00001648
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001648
Iteration 100/1000 | Loss: 0.00001647
Iteration 101/1000 | Loss: 0.00001647
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001645
Iteration 104/1000 | Loss: 0.00001645
Iteration 105/1000 | Loss: 0.00001645
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001644
Iteration 109/1000 | Loss: 0.00001644
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001642
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001641
Iteration 118/1000 | Loss: 0.00001641
Iteration 119/1000 | Loss: 0.00001641
Iteration 120/1000 | Loss: 0.00001641
Iteration 121/1000 | Loss: 0.00001641
Iteration 122/1000 | Loss: 0.00001641
Iteration 123/1000 | Loss: 0.00001641
Iteration 124/1000 | Loss: 0.00001641
Iteration 125/1000 | Loss: 0.00001641
Iteration 126/1000 | Loss: 0.00001641
Iteration 127/1000 | Loss: 0.00001641
Iteration 128/1000 | Loss: 0.00001641
Iteration 129/1000 | Loss: 0.00001640
Iteration 130/1000 | Loss: 0.00001640
Iteration 131/1000 | Loss: 0.00001640
Iteration 132/1000 | Loss: 0.00001640
Iteration 133/1000 | Loss: 0.00001640
Iteration 134/1000 | Loss: 0.00001640
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001640
Iteration 144/1000 | Loss: 0.00001640
Iteration 145/1000 | Loss: 0.00001640
Iteration 146/1000 | Loss: 0.00001640
Iteration 147/1000 | Loss: 0.00001640
Iteration 148/1000 | Loss: 0.00001640
Iteration 149/1000 | Loss: 0.00001640
Iteration 150/1000 | Loss: 0.00001640
Iteration 151/1000 | Loss: 0.00001640
Iteration 152/1000 | Loss: 0.00001640
Iteration 153/1000 | Loss: 0.00001640
Iteration 154/1000 | Loss: 0.00001640
Iteration 155/1000 | Loss: 0.00001640
Iteration 156/1000 | Loss: 0.00001640
Iteration 157/1000 | Loss: 0.00001640
Iteration 158/1000 | Loss: 0.00001640
Iteration 159/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.639990477997344e-05, 1.639990477997344e-05, 1.639990477997344e-05, 1.639990477997344e-05, 1.639990477997344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.639990477997344e-05

Optimization complete. Final v2v error: 3.4282100200653076 mm

Highest mean error: 3.6976983547210693 mm for frame 89

Lowest mean error: 3.2090911865234375 mm for frame 116

Saving results

Total time: 37.422736406326294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783233
Iteration 2/25 | Loss: 0.00107015
Iteration 3/25 | Loss: 0.00087397
Iteration 4/25 | Loss: 0.00084458
Iteration 5/25 | Loss: 0.00083752
Iteration 6/25 | Loss: 0.00083682
Iteration 7/25 | Loss: 0.00083682
Iteration 8/25 | Loss: 0.00083682
Iteration 9/25 | Loss: 0.00083682
Iteration 10/25 | Loss: 0.00083682
Iteration 11/25 | Loss: 0.00083682
Iteration 12/25 | Loss: 0.00083682
Iteration 13/25 | Loss: 0.00083682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008368197595700622, 0.0008368197595700622, 0.0008368197595700622, 0.0008368197595700622, 0.0008368197595700622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008368197595700622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59739161
Iteration 2/25 | Loss: 0.00148571
Iteration 3/25 | Loss: 0.00148568
Iteration 4/25 | Loss: 0.00148567
Iteration 5/25 | Loss: 0.00148567
Iteration 6/25 | Loss: 0.00148567
Iteration 7/25 | Loss: 0.00148567
Iteration 8/25 | Loss: 0.00148567
Iteration 9/25 | Loss: 0.00148567
Iteration 10/25 | Loss: 0.00148567
Iteration 11/25 | Loss: 0.00148567
Iteration 12/25 | Loss: 0.00148567
Iteration 13/25 | Loss: 0.00148567
Iteration 14/25 | Loss: 0.00148567
Iteration 15/25 | Loss: 0.00148567
Iteration 16/25 | Loss: 0.00148567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001485672197304666, 0.001485672197304666, 0.001485672197304666, 0.001485672197304666, 0.001485672197304666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001485672197304666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148567
Iteration 2/1000 | Loss: 0.00005577
Iteration 3/1000 | Loss: 0.00003877
Iteration 4/1000 | Loss: 0.00003229
Iteration 5/1000 | Loss: 0.00002975
Iteration 6/1000 | Loss: 0.00002769
Iteration 7/1000 | Loss: 0.00002666
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002505
Iteration 10/1000 | Loss: 0.00002467
Iteration 11/1000 | Loss: 0.00002436
Iteration 12/1000 | Loss: 0.00002405
Iteration 13/1000 | Loss: 0.00002383
Iteration 14/1000 | Loss: 0.00002380
Iteration 15/1000 | Loss: 0.00002366
Iteration 16/1000 | Loss: 0.00002364
Iteration 17/1000 | Loss: 0.00002355
Iteration 18/1000 | Loss: 0.00002354
Iteration 19/1000 | Loss: 0.00002353
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002353
Iteration 22/1000 | Loss: 0.00002353
Iteration 23/1000 | Loss: 0.00002353
Iteration 24/1000 | Loss: 0.00002353
Iteration 25/1000 | Loss: 0.00002352
Iteration 26/1000 | Loss: 0.00002352
Iteration 27/1000 | Loss: 0.00002352
Iteration 28/1000 | Loss: 0.00002351
Iteration 29/1000 | Loss: 0.00002351
Iteration 30/1000 | Loss: 0.00002351
Iteration 31/1000 | Loss: 0.00002351
Iteration 32/1000 | Loss: 0.00002350
Iteration 33/1000 | Loss: 0.00002350
Iteration 34/1000 | Loss: 0.00002350
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002350
Iteration 38/1000 | Loss: 0.00002350
Iteration 39/1000 | Loss: 0.00002350
Iteration 40/1000 | Loss: 0.00002350
Iteration 41/1000 | Loss: 0.00002349
Iteration 42/1000 | Loss: 0.00002349
Iteration 43/1000 | Loss: 0.00002349
Iteration 44/1000 | Loss: 0.00002349
Iteration 45/1000 | Loss: 0.00002349
Iteration 46/1000 | Loss: 0.00002349
Iteration 47/1000 | Loss: 0.00002349
Iteration 48/1000 | Loss: 0.00002348
Iteration 49/1000 | Loss: 0.00002348
Iteration 50/1000 | Loss: 0.00002348
Iteration 51/1000 | Loss: 0.00002348
Iteration 52/1000 | Loss: 0.00002348
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002347
Iteration 56/1000 | Loss: 0.00002347
Iteration 57/1000 | Loss: 0.00002347
Iteration 58/1000 | Loss: 0.00002347
Iteration 59/1000 | Loss: 0.00002347
Iteration 60/1000 | Loss: 0.00002347
Iteration 61/1000 | Loss: 0.00002347
Iteration 62/1000 | Loss: 0.00002346
Iteration 63/1000 | Loss: 0.00002346
Iteration 64/1000 | Loss: 0.00002346
Iteration 65/1000 | Loss: 0.00002346
Iteration 66/1000 | Loss: 0.00002346
Iteration 67/1000 | Loss: 0.00002346
Iteration 68/1000 | Loss: 0.00002346
Iteration 69/1000 | Loss: 0.00002346
Iteration 70/1000 | Loss: 0.00002345
Iteration 71/1000 | Loss: 0.00002345
Iteration 72/1000 | Loss: 0.00002345
Iteration 73/1000 | Loss: 0.00002345
Iteration 74/1000 | Loss: 0.00002345
Iteration 75/1000 | Loss: 0.00002345
Iteration 76/1000 | Loss: 0.00002345
Iteration 77/1000 | Loss: 0.00002345
Iteration 78/1000 | Loss: 0.00002345
Iteration 79/1000 | Loss: 0.00002345
Iteration 80/1000 | Loss: 0.00002345
Iteration 81/1000 | Loss: 0.00002345
Iteration 82/1000 | Loss: 0.00002345
Iteration 83/1000 | Loss: 0.00002344
Iteration 84/1000 | Loss: 0.00002344
Iteration 85/1000 | Loss: 0.00002344
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002343
Iteration 91/1000 | Loss: 0.00002343
Iteration 92/1000 | Loss: 0.00002343
Iteration 93/1000 | Loss: 0.00002343
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002343
Iteration 99/1000 | Loss: 0.00002343
Iteration 100/1000 | Loss: 0.00002343
Iteration 101/1000 | Loss: 0.00002343
Iteration 102/1000 | Loss: 0.00002342
Iteration 103/1000 | Loss: 0.00002342
Iteration 104/1000 | Loss: 0.00002342
Iteration 105/1000 | Loss: 0.00002342
Iteration 106/1000 | Loss: 0.00002342
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002342
Iteration 112/1000 | Loss: 0.00002342
Iteration 113/1000 | Loss: 0.00002342
Iteration 114/1000 | Loss: 0.00002342
Iteration 115/1000 | Loss: 0.00002342
Iteration 116/1000 | Loss: 0.00002342
Iteration 117/1000 | Loss: 0.00002342
Iteration 118/1000 | Loss: 0.00002342
Iteration 119/1000 | Loss: 0.00002342
Iteration 120/1000 | Loss: 0.00002342
Iteration 121/1000 | Loss: 0.00002342
Iteration 122/1000 | Loss: 0.00002342
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002342
Iteration 125/1000 | Loss: 0.00002341
Iteration 126/1000 | Loss: 0.00002341
Iteration 127/1000 | Loss: 0.00002341
Iteration 128/1000 | Loss: 0.00002341
Iteration 129/1000 | Loss: 0.00002341
Iteration 130/1000 | Loss: 0.00002341
Iteration 131/1000 | Loss: 0.00002341
Iteration 132/1000 | Loss: 0.00002341
Iteration 133/1000 | Loss: 0.00002341
Iteration 134/1000 | Loss: 0.00002341
Iteration 135/1000 | Loss: 0.00002340
Iteration 136/1000 | Loss: 0.00002340
Iteration 137/1000 | Loss: 0.00002340
Iteration 138/1000 | Loss: 0.00002340
Iteration 139/1000 | Loss: 0.00002340
Iteration 140/1000 | Loss: 0.00002340
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00002340
Iteration 143/1000 | Loss: 0.00002340
Iteration 144/1000 | Loss: 0.00002340
Iteration 145/1000 | Loss: 0.00002340
Iteration 146/1000 | Loss: 0.00002340
Iteration 147/1000 | Loss: 0.00002340
Iteration 148/1000 | Loss: 0.00002340
Iteration 149/1000 | Loss: 0.00002340
Iteration 150/1000 | Loss: 0.00002340
Iteration 151/1000 | Loss: 0.00002340
Iteration 152/1000 | Loss: 0.00002340
Iteration 153/1000 | Loss: 0.00002340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.3399979909299873e-05, 2.3399979909299873e-05, 2.3399979909299873e-05, 2.3399979909299873e-05, 2.3399979909299873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3399979909299873e-05

Optimization complete. Final v2v error: 4.006581783294678 mm

Highest mean error: 4.317660808563232 mm for frame 156

Lowest mean error: 3.69378662109375 mm for frame 239

Saving results

Total time: 43.58398652076721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837652
Iteration 2/25 | Loss: 0.00163267
Iteration 3/25 | Loss: 0.00094405
Iteration 4/25 | Loss: 0.00083142
Iteration 5/25 | Loss: 0.00081755
Iteration 6/25 | Loss: 0.00081610
Iteration 7/25 | Loss: 0.00081610
Iteration 8/25 | Loss: 0.00081610
Iteration 9/25 | Loss: 0.00081610
Iteration 10/25 | Loss: 0.00081610
Iteration 11/25 | Loss: 0.00081610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008160974248312414, 0.0008160974248312414, 0.0008160974248312414, 0.0008160974248312414, 0.0008160974248312414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008160974248312414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55143285
Iteration 2/25 | Loss: 0.00137142
Iteration 3/25 | Loss: 0.00137142
Iteration 4/25 | Loss: 0.00137142
Iteration 5/25 | Loss: 0.00137142
Iteration 6/25 | Loss: 0.00137142
Iteration 7/25 | Loss: 0.00137142
Iteration 8/25 | Loss: 0.00137142
Iteration 9/25 | Loss: 0.00137142
Iteration 10/25 | Loss: 0.00137142
Iteration 11/25 | Loss: 0.00137142
Iteration 12/25 | Loss: 0.00137142
Iteration 13/25 | Loss: 0.00137142
Iteration 14/25 | Loss: 0.00137142
Iteration 15/25 | Loss: 0.00137142
Iteration 16/25 | Loss: 0.00137142
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001371416263282299, 0.001371416263282299, 0.001371416263282299, 0.001371416263282299, 0.001371416263282299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001371416263282299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137142
Iteration 2/1000 | Loss: 0.00003528
Iteration 3/1000 | Loss: 0.00002649
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002147
Iteration 6/1000 | Loss: 0.00002060
Iteration 7/1000 | Loss: 0.00002004
Iteration 8/1000 | Loss: 0.00001952
Iteration 9/1000 | Loss: 0.00001920
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001870
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001823
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001820
Iteration 21/1000 | Loss: 0.00001820
Iteration 22/1000 | Loss: 0.00001820
Iteration 23/1000 | Loss: 0.00001819
Iteration 24/1000 | Loss: 0.00001819
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001817
Iteration 28/1000 | Loss: 0.00001817
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001816
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001815
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001813
Iteration 38/1000 | Loss: 0.00001813
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001812
Iteration 42/1000 | Loss: 0.00001812
Iteration 43/1000 | Loss: 0.00001811
Iteration 44/1000 | Loss: 0.00001811
Iteration 45/1000 | Loss: 0.00001811
Iteration 46/1000 | Loss: 0.00001810
Iteration 47/1000 | Loss: 0.00001810
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001809
Iteration 50/1000 | Loss: 0.00001809
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001807
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001806
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001803
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001802
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001802
Iteration 87/1000 | Loss: 0.00001802
Iteration 88/1000 | Loss: 0.00001802
Iteration 89/1000 | Loss: 0.00001802
Iteration 90/1000 | Loss: 0.00001802
Iteration 91/1000 | Loss: 0.00001801
Iteration 92/1000 | Loss: 0.00001801
Iteration 93/1000 | Loss: 0.00001801
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001801
Iteration 96/1000 | Loss: 0.00001801
Iteration 97/1000 | Loss: 0.00001801
Iteration 98/1000 | Loss: 0.00001800
Iteration 99/1000 | Loss: 0.00001800
Iteration 100/1000 | Loss: 0.00001800
Iteration 101/1000 | Loss: 0.00001800
Iteration 102/1000 | Loss: 0.00001800
Iteration 103/1000 | Loss: 0.00001800
Iteration 104/1000 | Loss: 0.00001799
Iteration 105/1000 | Loss: 0.00001799
Iteration 106/1000 | Loss: 0.00001799
Iteration 107/1000 | Loss: 0.00001799
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001799
Iteration 110/1000 | Loss: 0.00001799
Iteration 111/1000 | Loss: 0.00001799
Iteration 112/1000 | Loss: 0.00001799
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001798
Iteration 117/1000 | Loss: 0.00001798
Iteration 118/1000 | Loss: 0.00001798
Iteration 119/1000 | Loss: 0.00001798
Iteration 120/1000 | Loss: 0.00001798
Iteration 121/1000 | Loss: 0.00001798
Iteration 122/1000 | Loss: 0.00001798
Iteration 123/1000 | Loss: 0.00001798
Iteration 124/1000 | Loss: 0.00001798
Iteration 125/1000 | Loss: 0.00001798
Iteration 126/1000 | Loss: 0.00001797
Iteration 127/1000 | Loss: 0.00001797
Iteration 128/1000 | Loss: 0.00001797
Iteration 129/1000 | Loss: 0.00001797
Iteration 130/1000 | Loss: 0.00001796
Iteration 131/1000 | Loss: 0.00001796
Iteration 132/1000 | Loss: 0.00001796
Iteration 133/1000 | Loss: 0.00001796
Iteration 134/1000 | Loss: 0.00001796
Iteration 135/1000 | Loss: 0.00001795
Iteration 136/1000 | Loss: 0.00001795
Iteration 137/1000 | Loss: 0.00001795
Iteration 138/1000 | Loss: 0.00001795
Iteration 139/1000 | Loss: 0.00001795
Iteration 140/1000 | Loss: 0.00001795
Iteration 141/1000 | Loss: 0.00001795
Iteration 142/1000 | Loss: 0.00001795
Iteration 143/1000 | Loss: 0.00001795
Iteration 144/1000 | Loss: 0.00001795
Iteration 145/1000 | Loss: 0.00001795
Iteration 146/1000 | Loss: 0.00001795
Iteration 147/1000 | Loss: 0.00001795
Iteration 148/1000 | Loss: 0.00001795
Iteration 149/1000 | Loss: 0.00001795
Iteration 150/1000 | Loss: 0.00001795
Iteration 151/1000 | Loss: 0.00001795
Iteration 152/1000 | Loss: 0.00001795
Iteration 153/1000 | Loss: 0.00001795
Iteration 154/1000 | Loss: 0.00001795
Iteration 155/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7945551007869653e-05, 1.7945551007869653e-05, 1.7945551007869653e-05, 1.7945551007869653e-05, 1.7945551007869653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7945551007869653e-05

Optimization complete. Final v2v error: 3.4498744010925293 mm

Highest mean error: 4.674386501312256 mm for frame 118

Lowest mean error: 2.9553956985473633 mm for frame 98

Saving results

Total time: 44.215548038482666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839489
Iteration 2/25 | Loss: 0.00182099
Iteration 3/25 | Loss: 0.00114546
Iteration 4/25 | Loss: 0.00102810
Iteration 5/25 | Loss: 0.00098890
Iteration 6/25 | Loss: 0.00097039
Iteration 7/25 | Loss: 0.00094377
Iteration 8/25 | Loss: 0.00092764
Iteration 9/25 | Loss: 0.00090356
Iteration 10/25 | Loss: 0.00089518
Iteration 11/25 | Loss: 0.00088542
Iteration 12/25 | Loss: 0.00088404
Iteration 13/25 | Loss: 0.00088367
Iteration 14/25 | Loss: 0.00088360
Iteration 15/25 | Loss: 0.00088360
Iteration 16/25 | Loss: 0.00088360
Iteration 17/25 | Loss: 0.00088360
Iteration 18/25 | Loss: 0.00088360
Iteration 19/25 | Loss: 0.00088360
Iteration 20/25 | Loss: 0.00088360
Iteration 21/25 | Loss: 0.00088360
Iteration 22/25 | Loss: 0.00088360
Iteration 23/25 | Loss: 0.00088360
Iteration 24/25 | Loss: 0.00088359
Iteration 25/25 | Loss: 0.00088359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79252064
Iteration 2/25 | Loss: 0.00134626
Iteration 3/25 | Loss: 0.00134624
Iteration 4/25 | Loss: 0.00134623
Iteration 5/25 | Loss: 0.00134623
Iteration 6/25 | Loss: 0.00134623
Iteration 7/25 | Loss: 0.00134623
Iteration 8/25 | Loss: 0.00134623
Iteration 9/25 | Loss: 0.00134623
Iteration 10/25 | Loss: 0.00134623
Iteration 11/25 | Loss: 0.00134623
Iteration 12/25 | Loss: 0.00134623
Iteration 13/25 | Loss: 0.00134623
Iteration 14/25 | Loss: 0.00134623
Iteration 15/25 | Loss: 0.00134623
Iteration 16/25 | Loss: 0.00134623
Iteration 17/25 | Loss: 0.00134623
Iteration 18/25 | Loss: 0.00134623
Iteration 19/25 | Loss: 0.00134623
Iteration 20/25 | Loss: 0.00134623
Iteration 21/25 | Loss: 0.00134623
Iteration 22/25 | Loss: 0.00134623
Iteration 23/25 | Loss: 0.00134623
Iteration 24/25 | Loss: 0.00134623
Iteration 25/25 | Loss: 0.00134623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134623
Iteration 2/1000 | Loss: 0.00004178
Iteration 3/1000 | Loss: 0.00003086
Iteration 4/1000 | Loss: 0.00002845
Iteration 5/1000 | Loss: 0.00002687
Iteration 6/1000 | Loss: 0.00002601
Iteration 7/1000 | Loss: 0.00002528
Iteration 8/1000 | Loss: 0.00002482
Iteration 9/1000 | Loss: 0.00002449
Iteration 10/1000 | Loss: 0.00002427
Iteration 11/1000 | Loss: 0.00002405
Iteration 12/1000 | Loss: 0.00002398
Iteration 13/1000 | Loss: 0.00002389
Iteration 14/1000 | Loss: 0.00002377
Iteration 15/1000 | Loss: 0.00002369
Iteration 16/1000 | Loss: 0.00002369
Iteration 17/1000 | Loss: 0.00002365
Iteration 18/1000 | Loss: 0.00002361
Iteration 19/1000 | Loss: 0.00002359
Iteration 20/1000 | Loss: 0.00002358
Iteration 21/1000 | Loss: 0.00002358
Iteration 22/1000 | Loss: 0.00002358
Iteration 23/1000 | Loss: 0.00002357
Iteration 24/1000 | Loss: 0.00002356
Iteration 25/1000 | Loss: 0.00002356
Iteration 26/1000 | Loss: 0.00002356
Iteration 27/1000 | Loss: 0.00002356
Iteration 28/1000 | Loss: 0.00002355
Iteration 29/1000 | Loss: 0.00002354
Iteration 30/1000 | Loss: 0.00002354
Iteration 31/1000 | Loss: 0.00002354
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002352
Iteration 34/1000 | Loss: 0.00002352
Iteration 35/1000 | Loss: 0.00002351
Iteration 36/1000 | Loss: 0.00002351
Iteration 37/1000 | Loss: 0.00002350
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002349
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002347
Iteration 45/1000 | Loss: 0.00002347
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002345
Iteration 48/1000 | Loss: 0.00002345
Iteration 49/1000 | Loss: 0.00002345
Iteration 50/1000 | Loss: 0.00002344
Iteration 51/1000 | Loss: 0.00002344
Iteration 52/1000 | Loss: 0.00002344
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002343
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002342
Iteration 57/1000 | Loss: 0.00002342
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002341
Iteration 60/1000 | Loss: 0.00002341
Iteration 61/1000 | Loss: 0.00002341
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00002340
Iteration 64/1000 | Loss: 0.00002340
Iteration 65/1000 | Loss: 0.00002339
Iteration 66/1000 | Loss: 0.00002339
Iteration 67/1000 | Loss: 0.00002339
Iteration 68/1000 | Loss: 0.00002339
Iteration 69/1000 | Loss: 0.00002338
Iteration 70/1000 | Loss: 0.00002338
Iteration 71/1000 | Loss: 0.00002338
Iteration 72/1000 | Loss: 0.00002338
Iteration 73/1000 | Loss: 0.00002337
Iteration 74/1000 | Loss: 0.00002337
Iteration 75/1000 | Loss: 0.00002337
Iteration 76/1000 | Loss: 0.00002337
Iteration 77/1000 | Loss: 0.00002336
Iteration 78/1000 | Loss: 0.00002336
Iteration 79/1000 | Loss: 0.00002336
Iteration 80/1000 | Loss: 0.00002336
Iteration 81/1000 | Loss: 0.00002336
Iteration 82/1000 | Loss: 0.00002336
Iteration 83/1000 | Loss: 0.00002335
Iteration 84/1000 | Loss: 0.00002335
Iteration 85/1000 | Loss: 0.00002335
Iteration 86/1000 | Loss: 0.00002335
Iteration 87/1000 | Loss: 0.00002335
Iteration 88/1000 | Loss: 0.00002335
Iteration 89/1000 | Loss: 0.00002335
Iteration 90/1000 | Loss: 0.00002335
Iteration 91/1000 | Loss: 0.00002335
Iteration 92/1000 | Loss: 0.00002334
Iteration 93/1000 | Loss: 0.00002334
Iteration 94/1000 | Loss: 0.00002334
Iteration 95/1000 | Loss: 0.00002334
Iteration 96/1000 | Loss: 0.00002334
Iteration 97/1000 | Loss: 0.00002334
Iteration 98/1000 | Loss: 0.00002334
Iteration 99/1000 | Loss: 0.00002334
Iteration 100/1000 | Loss: 0.00002334
Iteration 101/1000 | Loss: 0.00002334
Iteration 102/1000 | Loss: 0.00002334
Iteration 103/1000 | Loss: 0.00002334
Iteration 104/1000 | Loss: 0.00002334
Iteration 105/1000 | Loss: 0.00002334
Iteration 106/1000 | Loss: 0.00002334
Iteration 107/1000 | Loss: 0.00002334
Iteration 108/1000 | Loss: 0.00002334
Iteration 109/1000 | Loss: 0.00002334
Iteration 110/1000 | Loss: 0.00002334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.334181044716388e-05, 2.334181044716388e-05, 2.334181044716388e-05, 2.334181044716388e-05, 2.334181044716388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.334181044716388e-05

Optimization complete. Final v2v error: 3.8951237201690674 mm

Highest mean error: 6.1038103103637695 mm for frame 188

Lowest mean error: 3.028705596923828 mm for frame 109

Saving results

Total time: 58.974323749542236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00745374
Iteration 2/25 | Loss: 0.00109582
Iteration 3/25 | Loss: 0.00086751
Iteration 4/25 | Loss: 0.00083009
Iteration 5/25 | Loss: 0.00082338
Iteration 6/25 | Loss: 0.00082163
Iteration 7/25 | Loss: 0.00082131
Iteration 8/25 | Loss: 0.00082131
Iteration 9/25 | Loss: 0.00082131
Iteration 10/25 | Loss: 0.00082131
Iteration 11/25 | Loss: 0.00082131
Iteration 12/25 | Loss: 0.00082131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008213068940676749, 0.0008213068940676749, 0.0008213068940676749, 0.0008213068940676749, 0.0008213068940676749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008213068940676749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52840209
Iteration 2/25 | Loss: 0.00143518
Iteration 3/25 | Loss: 0.00143516
Iteration 4/25 | Loss: 0.00143516
Iteration 5/25 | Loss: 0.00143516
Iteration 6/25 | Loss: 0.00143516
Iteration 7/25 | Loss: 0.00143516
Iteration 8/25 | Loss: 0.00143516
Iteration 9/25 | Loss: 0.00143516
Iteration 10/25 | Loss: 0.00143516
Iteration 11/25 | Loss: 0.00143516
Iteration 12/25 | Loss: 0.00143516
Iteration 13/25 | Loss: 0.00143516
Iteration 14/25 | Loss: 0.00143516
Iteration 15/25 | Loss: 0.00143516
Iteration 16/25 | Loss: 0.00143516
Iteration 17/25 | Loss: 0.00143516
Iteration 18/25 | Loss: 0.00143516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014351597055792809, 0.0014351597055792809, 0.0014351597055792809, 0.0014351597055792809, 0.0014351597055792809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014351597055792809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143516
Iteration 2/1000 | Loss: 0.00003375
Iteration 3/1000 | Loss: 0.00002585
Iteration 4/1000 | Loss: 0.00002328
Iteration 5/1000 | Loss: 0.00002177
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001914
Iteration 11/1000 | Loss: 0.00001914
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001892
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001891
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001884
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001880
Iteration 25/1000 | Loss: 0.00001879
Iteration 26/1000 | Loss: 0.00001879
Iteration 27/1000 | Loss: 0.00001879
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001877
Iteration 32/1000 | Loss: 0.00001876
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001875
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001874
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001873
Iteration 40/1000 | Loss: 0.00001873
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001873
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001870
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001869
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001867
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001867
Iteration 77/1000 | Loss: 0.00001867
Iteration 78/1000 | Loss: 0.00001867
Iteration 79/1000 | Loss: 0.00001867
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001866
Iteration 84/1000 | Loss: 0.00001866
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001864
Iteration 94/1000 | Loss: 0.00001864
Iteration 95/1000 | Loss: 0.00001864
Iteration 96/1000 | Loss: 0.00001864
Iteration 97/1000 | Loss: 0.00001864
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001864
Iteration 103/1000 | Loss: 0.00001864
Iteration 104/1000 | Loss: 0.00001864
Iteration 105/1000 | Loss: 0.00001864
Iteration 106/1000 | Loss: 0.00001864
Iteration 107/1000 | Loss: 0.00001864
Iteration 108/1000 | Loss: 0.00001864
Iteration 109/1000 | Loss: 0.00001863
Iteration 110/1000 | Loss: 0.00001863
Iteration 111/1000 | Loss: 0.00001863
Iteration 112/1000 | Loss: 0.00001863
Iteration 113/1000 | Loss: 0.00001863
Iteration 114/1000 | Loss: 0.00001863
Iteration 115/1000 | Loss: 0.00001862
Iteration 116/1000 | Loss: 0.00001862
Iteration 117/1000 | Loss: 0.00001862
Iteration 118/1000 | Loss: 0.00001862
Iteration 119/1000 | Loss: 0.00001862
Iteration 120/1000 | Loss: 0.00001862
Iteration 121/1000 | Loss: 0.00001862
Iteration 122/1000 | Loss: 0.00001862
Iteration 123/1000 | Loss: 0.00001862
Iteration 124/1000 | Loss: 0.00001862
Iteration 125/1000 | Loss: 0.00001861
Iteration 126/1000 | Loss: 0.00001861
Iteration 127/1000 | Loss: 0.00001861
Iteration 128/1000 | Loss: 0.00001861
Iteration 129/1000 | Loss: 0.00001861
Iteration 130/1000 | Loss: 0.00001861
Iteration 131/1000 | Loss: 0.00001861
Iteration 132/1000 | Loss: 0.00001861
Iteration 133/1000 | Loss: 0.00001860
Iteration 134/1000 | Loss: 0.00001860
Iteration 135/1000 | Loss: 0.00001860
Iteration 136/1000 | Loss: 0.00001860
Iteration 137/1000 | Loss: 0.00001860
Iteration 138/1000 | Loss: 0.00001860
Iteration 139/1000 | Loss: 0.00001860
Iteration 140/1000 | Loss: 0.00001860
Iteration 141/1000 | Loss: 0.00001859
Iteration 142/1000 | Loss: 0.00001859
Iteration 143/1000 | Loss: 0.00001859
Iteration 144/1000 | Loss: 0.00001859
Iteration 145/1000 | Loss: 0.00001859
Iteration 146/1000 | Loss: 0.00001859
Iteration 147/1000 | Loss: 0.00001859
Iteration 148/1000 | Loss: 0.00001858
Iteration 149/1000 | Loss: 0.00001858
Iteration 150/1000 | Loss: 0.00001858
Iteration 151/1000 | Loss: 0.00001858
Iteration 152/1000 | Loss: 0.00001858
Iteration 153/1000 | Loss: 0.00001858
Iteration 154/1000 | Loss: 0.00001858
Iteration 155/1000 | Loss: 0.00001858
Iteration 156/1000 | Loss: 0.00001858
Iteration 157/1000 | Loss: 0.00001858
Iteration 158/1000 | Loss: 0.00001858
Iteration 159/1000 | Loss: 0.00001857
Iteration 160/1000 | Loss: 0.00001857
Iteration 161/1000 | Loss: 0.00001857
Iteration 162/1000 | Loss: 0.00001857
Iteration 163/1000 | Loss: 0.00001857
Iteration 164/1000 | Loss: 0.00001857
Iteration 165/1000 | Loss: 0.00001857
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001856
Iteration 168/1000 | Loss: 0.00001856
Iteration 169/1000 | Loss: 0.00001856
Iteration 170/1000 | Loss: 0.00001856
Iteration 171/1000 | Loss: 0.00001856
Iteration 172/1000 | Loss: 0.00001856
Iteration 173/1000 | Loss: 0.00001856
Iteration 174/1000 | Loss: 0.00001856
Iteration 175/1000 | Loss: 0.00001856
Iteration 176/1000 | Loss: 0.00001856
Iteration 177/1000 | Loss: 0.00001856
Iteration 178/1000 | Loss: 0.00001856
Iteration 179/1000 | Loss: 0.00001856
Iteration 180/1000 | Loss: 0.00001856
Iteration 181/1000 | Loss: 0.00001856
Iteration 182/1000 | Loss: 0.00001856
Iteration 183/1000 | Loss: 0.00001856
Iteration 184/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.8557311705080792e-05, 1.8557311705080792e-05, 1.8557311705080792e-05, 1.8557311705080792e-05, 1.8557311705080792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8557311705080792e-05

Optimization complete. Final v2v error: 3.6310184001922607 mm

Highest mean error: 4.03148889541626 mm for frame 26

Lowest mean error: 3.3261172771453857 mm for frame 73

Saving results

Total time: 38.438326835632324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029657
Iteration 2/25 | Loss: 0.00262794
Iteration 3/25 | Loss: 0.00172456
Iteration 4/25 | Loss: 0.00146698
Iteration 5/25 | Loss: 0.00146127
Iteration 6/25 | Loss: 0.00146540
Iteration 7/25 | Loss: 0.00148659
Iteration 8/25 | Loss: 0.00136021
Iteration 9/25 | Loss: 0.00140823
Iteration 10/25 | Loss: 0.00135028
Iteration 11/25 | Loss: 0.00127958
Iteration 12/25 | Loss: 0.00118879
Iteration 13/25 | Loss: 0.00112198
Iteration 14/25 | Loss: 0.00108044
Iteration 15/25 | Loss: 0.00105751
Iteration 16/25 | Loss: 0.00101985
Iteration 17/25 | Loss: 0.00101083
Iteration 18/25 | Loss: 0.00100170
Iteration 19/25 | Loss: 0.00100015
Iteration 20/25 | Loss: 0.00100383
Iteration 21/25 | Loss: 0.00099526
Iteration 22/25 | Loss: 0.00099285
Iteration 23/25 | Loss: 0.00098679
Iteration 24/25 | Loss: 0.00098555
Iteration 25/25 | Loss: 0.00099049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67830396
Iteration 2/25 | Loss: 0.00623440
Iteration 3/25 | Loss: 0.00385378
Iteration 4/25 | Loss: 0.00385378
Iteration 5/25 | Loss: 0.00385378
Iteration 6/25 | Loss: 0.00385378
Iteration 7/25 | Loss: 0.00385378
Iteration 8/25 | Loss: 0.00385378
Iteration 9/25 | Loss: 0.00385378
Iteration 10/25 | Loss: 0.00385378
Iteration 11/25 | Loss: 0.00385378
Iteration 12/25 | Loss: 0.00385378
Iteration 13/25 | Loss: 0.00385378
Iteration 14/25 | Loss: 0.00385378
Iteration 15/25 | Loss: 0.00385378
Iteration 16/25 | Loss: 0.00385378
Iteration 17/25 | Loss: 0.00385378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003853779984638095, 0.003853779984638095, 0.003853779984638095, 0.003853779984638095, 0.003853779984638095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003853779984638095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00385378
Iteration 2/1000 | Loss: 0.00135460
Iteration 3/1000 | Loss: 0.00267143
Iteration 4/1000 | Loss: 0.00165971
Iteration 5/1000 | Loss: 0.00230315
Iteration 6/1000 | Loss: 0.00215766
Iteration 7/1000 | Loss: 0.00196131
Iteration 8/1000 | Loss: 0.00161289
Iteration 9/1000 | Loss: 0.00338961
Iteration 10/1000 | Loss: 0.00785492
Iteration 11/1000 | Loss: 0.00215393
Iteration 12/1000 | Loss: 0.00663973
Iteration 13/1000 | Loss: 0.00223984
Iteration 14/1000 | Loss: 0.00396458
Iteration 15/1000 | Loss: 0.00238799
Iteration 16/1000 | Loss: 0.00386523
Iteration 17/1000 | Loss: 0.00332235
Iteration 18/1000 | Loss: 0.00204767
Iteration 19/1000 | Loss: 0.00399773
Iteration 20/1000 | Loss: 0.00494408
Iteration 21/1000 | Loss: 0.00737198
Iteration 22/1000 | Loss: 0.01072094
Iteration 23/1000 | Loss: 0.00469517
Iteration 24/1000 | Loss: 0.00788227
Iteration 25/1000 | Loss: 0.00199736
Iteration 26/1000 | Loss: 0.00436858
Iteration 27/1000 | Loss: 0.00193362
Iteration 28/1000 | Loss: 0.00287487
Iteration 29/1000 | Loss: 0.00158435
Iteration 30/1000 | Loss: 0.00209047
Iteration 31/1000 | Loss: 0.00143062
Iteration 32/1000 | Loss: 0.00295869
Iteration 33/1000 | Loss: 0.00253751
Iteration 34/1000 | Loss: 0.00237384
Iteration 35/1000 | Loss: 0.00180373
Iteration 36/1000 | Loss: 0.00134472
Iteration 37/1000 | Loss: 0.00133938
Iteration 38/1000 | Loss: 0.00181079
Iteration 39/1000 | Loss: 0.00074661
Iteration 40/1000 | Loss: 0.00153266
Iteration 41/1000 | Loss: 0.00158543
Iteration 42/1000 | Loss: 0.00058879
Iteration 43/1000 | Loss: 0.00084643
Iteration 44/1000 | Loss: 0.00066949
Iteration 45/1000 | Loss: 0.00038044
Iteration 46/1000 | Loss: 0.00048942
Iteration 47/1000 | Loss: 0.00067382
Iteration 48/1000 | Loss: 0.00197330
Iteration 49/1000 | Loss: 0.00261858
Iteration 50/1000 | Loss: 0.00159719
Iteration 51/1000 | Loss: 0.00203175
Iteration 52/1000 | Loss: 0.00445838
Iteration 53/1000 | Loss: 0.00518308
Iteration 54/1000 | Loss: 0.00182640
Iteration 55/1000 | Loss: 0.00098030
Iteration 56/1000 | Loss: 0.00062186
Iteration 57/1000 | Loss: 0.00074072
Iteration 58/1000 | Loss: 0.00090325
Iteration 59/1000 | Loss: 0.00108084
Iteration 60/1000 | Loss: 0.00195634
Iteration 61/1000 | Loss: 0.00091272
Iteration 62/1000 | Loss: 0.00064253
Iteration 63/1000 | Loss: 0.00076748
Iteration 64/1000 | Loss: 0.00067222
Iteration 65/1000 | Loss: 0.00091774
Iteration 66/1000 | Loss: 0.00076163
Iteration 67/1000 | Loss: 0.00176957
Iteration 68/1000 | Loss: 0.00104794
Iteration 69/1000 | Loss: 0.00116675
Iteration 70/1000 | Loss: 0.00080197
Iteration 71/1000 | Loss: 0.00088517
Iteration 72/1000 | Loss: 0.00141011
Iteration 73/1000 | Loss: 0.00113991
Iteration 74/1000 | Loss: 0.00114090
Iteration 75/1000 | Loss: 0.00019630
Iteration 76/1000 | Loss: 0.00080007
Iteration 77/1000 | Loss: 0.00017236
Iteration 78/1000 | Loss: 0.00111747
Iteration 79/1000 | Loss: 0.00088814
Iteration 80/1000 | Loss: 0.00118383
Iteration 81/1000 | Loss: 0.00082525
Iteration 82/1000 | Loss: 0.00104355
Iteration 83/1000 | Loss: 0.00016215
Iteration 84/1000 | Loss: 0.00090450
Iteration 85/1000 | Loss: 0.00137822
Iteration 86/1000 | Loss: 0.00051786
Iteration 87/1000 | Loss: 0.00118745
Iteration 88/1000 | Loss: 0.00015539
Iteration 89/1000 | Loss: 0.00017074
Iteration 90/1000 | Loss: 0.00013634
Iteration 91/1000 | Loss: 0.00011970
Iteration 92/1000 | Loss: 0.00018630
Iteration 93/1000 | Loss: 0.00009295
Iteration 94/1000 | Loss: 0.00014844
Iteration 95/1000 | Loss: 0.00038442
Iteration 96/1000 | Loss: 0.00045572
Iteration 97/1000 | Loss: 0.00081785
Iteration 98/1000 | Loss: 0.00036920
Iteration 99/1000 | Loss: 0.00037833
Iteration 100/1000 | Loss: 0.00144536
Iteration 101/1000 | Loss: 0.00128130
Iteration 102/1000 | Loss: 0.00047806
Iteration 103/1000 | Loss: 0.00076625
Iteration 104/1000 | Loss: 0.00013896
Iteration 105/1000 | Loss: 0.00012226
Iteration 106/1000 | Loss: 0.00016597
Iteration 107/1000 | Loss: 0.00013555
Iteration 108/1000 | Loss: 0.00012362
Iteration 109/1000 | Loss: 0.00012795
Iteration 110/1000 | Loss: 0.00051355
Iteration 111/1000 | Loss: 0.00013411
Iteration 112/1000 | Loss: 0.00013442
Iteration 113/1000 | Loss: 0.00012525
Iteration 114/1000 | Loss: 0.00046141
Iteration 115/1000 | Loss: 0.00044989
Iteration 116/1000 | Loss: 0.00130145
Iteration 117/1000 | Loss: 0.00066834
Iteration 118/1000 | Loss: 0.00062039
Iteration 119/1000 | Loss: 0.00056566
Iteration 120/1000 | Loss: 0.00050670
Iteration 121/1000 | Loss: 0.00128595
Iteration 122/1000 | Loss: 0.00038871
Iteration 123/1000 | Loss: 0.00040119
Iteration 124/1000 | Loss: 0.00015854
Iteration 125/1000 | Loss: 0.00012967
Iteration 126/1000 | Loss: 0.00022541
Iteration 127/1000 | Loss: 0.00030880
Iteration 128/1000 | Loss: 0.00125090
Iteration 129/1000 | Loss: 0.00023347
Iteration 130/1000 | Loss: 0.00008813
Iteration 131/1000 | Loss: 0.00016104
Iteration 132/1000 | Loss: 0.00018781
Iteration 133/1000 | Loss: 0.00013321
Iteration 134/1000 | Loss: 0.00029692
Iteration 135/1000 | Loss: 0.00033078
Iteration 136/1000 | Loss: 0.00025033
Iteration 137/1000 | Loss: 0.00024438
Iteration 138/1000 | Loss: 0.00011650
Iteration 139/1000 | Loss: 0.00068503
Iteration 140/1000 | Loss: 0.00054571
Iteration 141/1000 | Loss: 0.00031043
Iteration 142/1000 | Loss: 0.00054087
Iteration 143/1000 | Loss: 0.00045735
Iteration 144/1000 | Loss: 0.00067453
Iteration 145/1000 | Loss: 0.00030681
Iteration 146/1000 | Loss: 0.00012951
Iteration 147/1000 | Loss: 0.00037187
Iteration 148/1000 | Loss: 0.00013116
Iteration 149/1000 | Loss: 0.00013064
Iteration 150/1000 | Loss: 0.00012142
Iteration 151/1000 | Loss: 0.00014966
Iteration 152/1000 | Loss: 0.00043187
Iteration 153/1000 | Loss: 0.00017698
Iteration 154/1000 | Loss: 0.00014310
Iteration 155/1000 | Loss: 0.00014288
Iteration 156/1000 | Loss: 0.00027280
Iteration 157/1000 | Loss: 0.00054757
Iteration 158/1000 | Loss: 0.00089062
Iteration 159/1000 | Loss: 0.00111468
Iteration 160/1000 | Loss: 0.00092227
Iteration 161/1000 | Loss: 0.00109446
Iteration 162/1000 | Loss: 0.00137024
Iteration 163/1000 | Loss: 0.00102749
Iteration 164/1000 | Loss: 0.00295056
Iteration 165/1000 | Loss: 0.00017776
Iteration 166/1000 | Loss: 0.00176504
Iteration 167/1000 | Loss: 0.00098218
Iteration 168/1000 | Loss: 0.00066607
Iteration 169/1000 | Loss: 0.00029954
Iteration 170/1000 | Loss: 0.00035216
Iteration 171/1000 | Loss: 0.00066829
Iteration 172/1000 | Loss: 0.00153832
Iteration 173/1000 | Loss: 0.00094511
Iteration 174/1000 | Loss: 0.00082783
Iteration 175/1000 | Loss: 0.00044693
Iteration 176/1000 | Loss: 0.00093255
Iteration 177/1000 | Loss: 0.00055366
Iteration 178/1000 | Loss: 0.00089163
Iteration 179/1000 | Loss: 0.00174755
Iteration 180/1000 | Loss: 0.00092577
Iteration 181/1000 | Loss: 0.00091465
Iteration 182/1000 | Loss: 0.00114555
Iteration 183/1000 | Loss: 0.00092552
Iteration 184/1000 | Loss: 0.00109310
Iteration 185/1000 | Loss: 0.00060571
Iteration 186/1000 | Loss: 0.00073503
Iteration 187/1000 | Loss: 0.00016494
Iteration 188/1000 | Loss: 0.00167954
Iteration 189/1000 | Loss: 0.00129317
Iteration 190/1000 | Loss: 0.00071677
Iteration 191/1000 | Loss: 0.00038390
Iteration 192/1000 | Loss: 0.00043498
Iteration 193/1000 | Loss: 0.00027466
Iteration 194/1000 | Loss: 0.00030309
Iteration 195/1000 | Loss: 0.00040768
Iteration 196/1000 | Loss: 0.00079427
Iteration 197/1000 | Loss: 0.00030512
Iteration 198/1000 | Loss: 0.00008907
Iteration 199/1000 | Loss: 0.00010080
Iteration 200/1000 | Loss: 0.00011701
Iteration 201/1000 | Loss: 0.00012584
Iteration 202/1000 | Loss: 0.00011990
Iteration 203/1000 | Loss: 0.00011642
Iteration 204/1000 | Loss: 0.00010504
Iteration 205/1000 | Loss: 0.00010617
Iteration 206/1000 | Loss: 0.00006925
Iteration 207/1000 | Loss: 0.00013022
Iteration 208/1000 | Loss: 0.00011663
Iteration 209/1000 | Loss: 0.00011099
Iteration 210/1000 | Loss: 0.00010942
Iteration 211/1000 | Loss: 0.00034966
Iteration 212/1000 | Loss: 0.00016172
Iteration 213/1000 | Loss: 0.00009858
Iteration 214/1000 | Loss: 0.00104598
Iteration 215/1000 | Loss: 0.00017059
Iteration 216/1000 | Loss: 0.00010890
Iteration 217/1000 | Loss: 0.00049336
Iteration 218/1000 | Loss: 0.00004379
Iteration 219/1000 | Loss: 0.00010953
Iteration 220/1000 | Loss: 0.00004296
Iteration 221/1000 | Loss: 0.00025153
Iteration 222/1000 | Loss: 0.00007780
Iteration 223/1000 | Loss: 0.00003239
Iteration 224/1000 | Loss: 0.00004715
Iteration 225/1000 | Loss: 0.00003332
Iteration 226/1000 | Loss: 0.00002939
Iteration 227/1000 | Loss: 0.00003829
Iteration 228/1000 | Loss: 0.00004730
Iteration 229/1000 | Loss: 0.00003513
Iteration 230/1000 | Loss: 0.00026903
Iteration 231/1000 | Loss: 0.00007359
Iteration 232/1000 | Loss: 0.00006563
Iteration 233/1000 | Loss: 0.00007260
Iteration 234/1000 | Loss: 0.00018993
Iteration 235/1000 | Loss: 0.00010873
Iteration 236/1000 | Loss: 0.00006331
Iteration 237/1000 | Loss: 0.00006636
Iteration 238/1000 | Loss: 0.00006977
Iteration 239/1000 | Loss: 0.00006711
Iteration 240/1000 | Loss: 0.00009321
Iteration 241/1000 | Loss: 0.00006078
Iteration 242/1000 | Loss: 0.00006963
Iteration 243/1000 | Loss: 0.00007233
Iteration 244/1000 | Loss: 0.00005286
Iteration 245/1000 | Loss: 0.00005068
Iteration 246/1000 | Loss: 0.00005465
Iteration 247/1000 | Loss: 0.00005637
Iteration 248/1000 | Loss: 0.00005623
Iteration 249/1000 | Loss: 0.00005081
Iteration 250/1000 | Loss: 0.00005437
Iteration 251/1000 | Loss: 0.00004069
Iteration 252/1000 | Loss: 0.00007053
Iteration 253/1000 | Loss: 0.00002363
Iteration 254/1000 | Loss: 0.00008463
Iteration 255/1000 | Loss: 0.00001917
Iteration 256/1000 | Loss: 0.00004111
Iteration 257/1000 | Loss: 0.00001711
Iteration 258/1000 | Loss: 0.00001622
Iteration 259/1000 | Loss: 0.00001579
Iteration 260/1000 | Loss: 0.00001563
Iteration 261/1000 | Loss: 0.00001556
Iteration 262/1000 | Loss: 0.00001553
Iteration 263/1000 | Loss: 0.00001530
Iteration 264/1000 | Loss: 0.00001524
Iteration 265/1000 | Loss: 0.00001522
Iteration 266/1000 | Loss: 0.00001520
Iteration 267/1000 | Loss: 0.00001511
Iteration 268/1000 | Loss: 0.00001505
Iteration 269/1000 | Loss: 0.00001500
Iteration 270/1000 | Loss: 0.00001499
Iteration 271/1000 | Loss: 0.00001498
Iteration 272/1000 | Loss: 0.00001493
Iteration 273/1000 | Loss: 0.00001489
Iteration 274/1000 | Loss: 0.00001487
Iteration 275/1000 | Loss: 0.00001486
Iteration 276/1000 | Loss: 0.00001486
Iteration 277/1000 | Loss: 0.00001486
Iteration 278/1000 | Loss: 0.00001486
Iteration 279/1000 | Loss: 0.00001485
Iteration 280/1000 | Loss: 0.00001485
Iteration 281/1000 | Loss: 0.00001485
Iteration 282/1000 | Loss: 0.00001484
Iteration 283/1000 | Loss: 0.00001484
Iteration 284/1000 | Loss: 0.00001484
Iteration 285/1000 | Loss: 0.00001483
Iteration 286/1000 | Loss: 0.00001483
Iteration 287/1000 | Loss: 0.00001483
Iteration 288/1000 | Loss: 0.00001482
Iteration 289/1000 | Loss: 0.00001482
Iteration 290/1000 | Loss: 0.00001481
Iteration 291/1000 | Loss: 0.00001481
Iteration 292/1000 | Loss: 0.00001479
Iteration 293/1000 | Loss: 0.00001479
Iteration 294/1000 | Loss: 0.00001479
Iteration 295/1000 | Loss: 0.00001479
Iteration 296/1000 | Loss: 0.00001479
Iteration 297/1000 | Loss: 0.00001479
Iteration 298/1000 | Loss: 0.00001479
Iteration 299/1000 | Loss: 0.00001478
Iteration 300/1000 | Loss: 0.00001478
Iteration 301/1000 | Loss: 0.00001478
Iteration 302/1000 | Loss: 0.00001478
Iteration 303/1000 | Loss: 0.00001477
Iteration 304/1000 | Loss: 0.00001477
Iteration 305/1000 | Loss: 0.00001477
Iteration 306/1000 | Loss: 0.00001475
Iteration 307/1000 | Loss: 0.00001475
Iteration 308/1000 | Loss: 0.00001475
Iteration 309/1000 | Loss: 0.00001475
Iteration 310/1000 | Loss: 0.00001475
Iteration 311/1000 | Loss: 0.00001475
Iteration 312/1000 | Loss: 0.00001475
Iteration 313/1000 | Loss: 0.00001474
Iteration 314/1000 | Loss: 0.00001474
Iteration 315/1000 | Loss: 0.00001474
Iteration 316/1000 | Loss: 0.00001473
Iteration 317/1000 | Loss: 0.00001472
Iteration 318/1000 | Loss: 0.00001472
Iteration 319/1000 | Loss: 0.00001471
Iteration 320/1000 | Loss: 0.00001471
Iteration 321/1000 | Loss: 0.00001471
Iteration 322/1000 | Loss: 0.00001471
Iteration 323/1000 | Loss: 0.00001470
Iteration 324/1000 | Loss: 0.00001470
Iteration 325/1000 | Loss: 0.00001470
Iteration 326/1000 | Loss: 0.00001470
Iteration 327/1000 | Loss: 0.00001470
Iteration 328/1000 | Loss: 0.00001469
Iteration 329/1000 | Loss: 0.00001469
Iteration 330/1000 | Loss: 0.00001469
Iteration 331/1000 | Loss: 0.00001468
Iteration 332/1000 | Loss: 0.00001468
Iteration 333/1000 | Loss: 0.00001468
Iteration 334/1000 | Loss: 0.00001468
Iteration 335/1000 | Loss: 0.00001467
Iteration 336/1000 | Loss: 0.00001467
Iteration 337/1000 | Loss: 0.00001467
Iteration 338/1000 | Loss: 0.00001467
Iteration 339/1000 | Loss: 0.00001467
Iteration 340/1000 | Loss: 0.00001467
Iteration 341/1000 | Loss: 0.00001467
Iteration 342/1000 | Loss: 0.00001465
Iteration 343/1000 | Loss: 0.00001465
Iteration 344/1000 | Loss: 0.00001465
Iteration 345/1000 | Loss: 0.00001464
Iteration 346/1000 | Loss: 0.00001464
Iteration 347/1000 | Loss: 0.00001464
Iteration 348/1000 | Loss: 0.00001464
Iteration 349/1000 | Loss: 0.00001464
Iteration 350/1000 | Loss: 0.00001464
Iteration 351/1000 | Loss: 0.00001464
Iteration 352/1000 | Loss: 0.00001464
Iteration 353/1000 | Loss: 0.00001464
Iteration 354/1000 | Loss: 0.00001464
Iteration 355/1000 | Loss: 0.00001464
Iteration 356/1000 | Loss: 0.00001463
Iteration 357/1000 | Loss: 0.00001463
Iteration 358/1000 | Loss: 0.00001463
Iteration 359/1000 | Loss: 0.00001463
Iteration 360/1000 | Loss: 0.00001463
Iteration 361/1000 | Loss: 0.00001462
Iteration 362/1000 | Loss: 0.00001462
Iteration 363/1000 | Loss: 0.00001462
Iteration 364/1000 | Loss: 0.00001461
Iteration 365/1000 | Loss: 0.00001461
Iteration 366/1000 | Loss: 0.00001461
Iteration 367/1000 | Loss: 0.00001461
Iteration 368/1000 | Loss: 0.00001461
Iteration 369/1000 | Loss: 0.00001461
Iteration 370/1000 | Loss: 0.00001461
Iteration 371/1000 | Loss: 0.00001460
Iteration 372/1000 | Loss: 0.00001460
Iteration 373/1000 | Loss: 0.00001460
Iteration 374/1000 | Loss: 0.00001460
Iteration 375/1000 | Loss: 0.00001460
Iteration 376/1000 | Loss: 0.00001460
Iteration 377/1000 | Loss: 0.00001460
Iteration 378/1000 | Loss: 0.00001460
Iteration 379/1000 | Loss: 0.00001460
Iteration 380/1000 | Loss: 0.00001460
Iteration 381/1000 | Loss: 0.00001459
Iteration 382/1000 | Loss: 0.00001459
Iteration 383/1000 | Loss: 0.00001458
Iteration 384/1000 | Loss: 0.00001458
Iteration 385/1000 | Loss: 0.00001458
Iteration 386/1000 | Loss: 0.00001458
Iteration 387/1000 | Loss: 0.00001458
Iteration 388/1000 | Loss: 0.00001458
Iteration 389/1000 | Loss: 0.00001458
Iteration 390/1000 | Loss: 0.00001458
Iteration 391/1000 | Loss: 0.00001458
Iteration 392/1000 | Loss: 0.00001457
Iteration 393/1000 | Loss: 0.00001457
Iteration 394/1000 | Loss: 0.00001457
Iteration 395/1000 | Loss: 0.00001457
Iteration 396/1000 | Loss: 0.00001456
Iteration 397/1000 | Loss: 0.00001456
Iteration 398/1000 | Loss: 0.00001456
Iteration 399/1000 | Loss: 0.00001456
Iteration 400/1000 | Loss: 0.00001456
Iteration 401/1000 | Loss: 0.00001456
Iteration 402/1000 | Loss: 0.00001456
Iteration 403/1000 | Loss: 0.00001456
Iteration 404/1000 | Loss: 0.00001456
Iteration 405/1000 | Loss: 0.00001456
Iteration 406/1000 | Loss: 0.00001456
Iteration 407/1000 | Loss: 0.00001456
Iteration 408/1000 | Loss: 0.00001456
Iteration 409/1000 | Loss: 0.00001456
Iteration 410/1000 | Loss: 0.00001456
Iteration 411/1000 | Loss: 0.00001456
Iteration 412/1000 | Loss: 0.00001455
Iteration 413/1000 | Loss: 0.00001455
Iteration 414/1000 | Loss: 0.00001455
Iteration 415/1000 | Loss: 0.00001455
Iteration 416/1000 | Loss: 0.00001455
Iteration 417/1000 | Loss: 0.00001455
Iteration 418/1000 | Loss: 0.00001455
Iteration 419/1000 | Loss: 0.00001455
Iteration 420/1000 | Loss: 0.00001455
Iteration 421/1000 | Loss: 0.00001455
Iteration 422/1000 | Loss: 0.00001455
Iteration 423/1000 | Loss: 0.00001454
Iteration 424/1000 | Loss: 0.00001454
Iteration 425/1000 | Loss: 0.00001454
Iteration 426/1000 | Loss: 0.00001454
Iteration 427/1000 | Loss: 0.00001454
Iteration 428/1000 | Loss: 0.00001454
Iteration 429/1000 | Loss: 0.00001454
Iteration 430/1000 | Loss: 0.00001454
Iteration 431/1000 | Loss: 0.00001454
Iteration 432/1000 | Loss: 0.00001454
Iteration 433/1000 | Loss: 0.00001454
Iteration 434/1000 | Loss: 0.00001454
Iteration 435/1000 | Loss: 0.00001454
Iteration 436/1000 | Loss: 0.00001454
Iteration 437/1000 | Loss: 0.00001454
Iteration 438/1000 | Loss: 0.00001454
Iteration 439/1000 | Loss: 0.00001454
Iteration 440/1000 | Loss: 0.00001453
Iteration 441/1000 | Loss: 0.00001453
Iteration 442/1000 | Loss: 0.00001453
Iteration 443/1000 | Loss: 0.00001453
Iteration 444/1000 | Loss: 0.00001453
Iteration 445/1000 | Loss: 0.00001453
Iteration 446/1000 | Loss: 0.00001453
Iteration 447/1000 | Loss: 0.00001453
Iteration 448/1000 | Loss: 0.00001453
Iteration 449/1000 | Loss: 0.00001453
Iteration 450/1000 | Loss: 0.00001453
Iteration 451/1000 | Loss: 0.00001453
Iteration 452/1000 | Loss: 0.00001453
Iteration 453/1000 | Loss: 0.00001453
Iteration 454/1000 | Loss: 0.00001453
Iteration 455/1000 | Loss: 0.00001453
Iteration 456/1000 | Loss: 0.00001453
Iteration 457/1000 | Loss: 0.00001453
Iteration 458/1000 | Loss: 0.00001453
Iteration 459/1000 | Loss: 0.00001453
Iteration 460/1000 | Loss: 0.00001453
Iteration 461/1000 | Loss: 0.00001453
Iteration 462/1000 | Loss: 0.00001453
Iteration 463/1000 | Loss: 0.00001453
Iteration 464/1000 | Loss: 0.00001453
Iteration 465/1000 | Loss: 0.00001453
Iteration 466/1000 | Loss: 0.00001453
Iteration 467/1000 | Loss: 0.00001453
Iteration 468/1000 | Loss: 0.00001453
Iteration 469/1000 | Loss: 0.00001453
Iteration 470/1000 | Loss: 0.00001453
Iteration 471/1000 | Loss: 0.00001453
Iteration 472/1000 | Loss: 0.00001453
Iteration 473/1000 | Loss: 0.00001453
Iteration 474/1000 | Loss: 0.00001453
Iteration 475/1000 | Loss: 0.00001453
Iteration 476/1000 | Loss: 0.00001453
Iteration 477/1000 | Loss: 0.00001453
Iteration 478/1000 | Loss: 0.00001453
Iteration 479/1000 | Loss: 0.00001453
Iteration 480/1000 | Loss: 0.00001453
Iteration 481/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 481. Stopping optimization.
Last 5 losses: [1.4528313840855844e-05, 1.4528313840855844e-05, 1.4528313840855844e-05, 1.4528313840855844e-05, 1.4528313840855844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4528313840855844e-05

Optimization complete. Final v2v error: 3.2268662452697754 mm

Highest mean error: 4.450235366821289 mm for frame 51

Lowest mean error: 2.9096920490264893 mm for frame 12

Saving results

Total time: 439.6926963329315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539261
Iteration 2/25 | Loss: 0.00118391
Iteration 3/25 | Loss: 0.00092678
Iteration 4/25 | Loss: 0.00090183
Iteration 5/25 | Loss: 0.00089654
Iteration 6/25 | Loss: 0.00089559
Iteration 7/25 | Loss: 0.00089559
Iteration 8/25 | Loss: 0.00089559
Iteration 9/25 | Loss: 0.00089559
Iteration 10/25 | Loss: 0.00089559
Iteration 11/25 | Loss: 0.00089559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008955881930887699, 0.0008955881930887699, 0.0008955881930887699, 0.0008955881930887699, 0.0008955881930887699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008955881930887699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86316246
Iteration 2/25 | Loss: 0.00098649
Iteration 3/25 | Loss: 0.00098649
Iteration 4/25 | Loss: 0.00098649
Iteration 5/25 | Loss: 0.00098649
Iteration 6/25 | Loss: 0.00098649
Iteration 7/25 | Loss: 0.00098649
Iteration 8/25 | Loss: 0.00098649
Iteration 9/25 | Loss: 0.00098649
Iteration 10/25 | Loss: 0.00098649
Iteration 11/25 | Loss: 0.00098648
Iteration 12/25 | Loss: 0.00098648
Iteration 13/25 | Loss: 0.00098648
Iteration 14/25 | Loss: 0.00098648
Iteration 15/25 | Loss: 0.00098648
Iteration 16/25 | Loss: 0.00098648
Iteration 17/25 | Loss: 0.00098648
Iteration 18/25 | Loss: 0.00098648
Iteration 19/25 | Loss: 0.00098648
Iteration 20/25 | Loss: 0.00098648
Iteration 21/25 | Loss: 0.00098648
Iteration 22/25 | Loss: 0.00098648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009864848107099533, 0.0009864848107099533, 0.0009864848107099533, 0.0009864848107099533, 0.0009864848107099533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009864848107099533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098648
Iteration 2/1000 | Loss: 0.00005002
Iteration 3/1000 | Loss: 0.00003968
Iteration 4/1000 | Loss: 0.00003700
Iteration 5/1000 | Loss: 0.00003587
Iteration 6/1000 | Loss: 0.00003495
Iteration 7/1000 | Loss: 0.00003446
Iteration 8/1000 | Loss: 0.00003403
Iteration 9/1000 | Loss: 0.00003376
Iteration 10/1000 | Loss: 0.00003349
Iteration 11/1000 | Loss: 0.00003328
Iteration 12/1000 | Loss: 0.00003317
Iteration 13/1000 | Loss: 0.00003290
Iteration 14/1000 | Loss: 0.00003273
Iteration 15/1000 | Loss: 0.00003270
Iteration 16/1000 | Loss: 0.00003261
Iteration 17/1000 | Loss: 0.00003258
Iteration 18/1000 | Loss: 0.00003258
Iteration 19/1000 | Loss: 0.00003257
Iteration 20/1000 | Loss: 0.00003257
Iteration 21/1000 | Loss: 0.00003256
Iteration 22/1000 | Loss: 0.00003256
Iteration 23/1000 | Loss: 0.00003255
Iteration 24/1000 | Loss: 0.00003255
Iteration 25/1000 | Loss: 0.00003255
Iteration 26/1000 | Loss: 0.00003255
Iteration 27/1000 | Loss: 0.00003255
Iteration 28/1000 | Loss: 0.00003255
Iteration 29/1000 | Loss: 0.00003255
Iteration 30/1000 | Loss: 0.00003255
Iteration 31/1000 | Loss: 0.00003255
Iteration 32/1000 | Loss: 0.00003255
Iteration 33/1000 | Loss: 0.00003255
Iteration 34/1000 | Loss: 0.00003255
Iteration 35/1000 | Loss: 0.00003255
Iteration 36/1000 | Loss: 0.00003254
Iteration 37/1000 | Loss: 0.00003254
Iteration 38/1000 | Loss: 0.00003254
Iteration 39/1000 | Loss: 0.00003253
Iteration 40/1000 | Loss: 0.00003253
Iteration 41/1000 | Loss: 0.00003253
Iteration 42/1000 | Loss: 0.00003253
Iteration 43/1000 | Loss: 0.00003253
Iteration 44/1000 | Loss: 0.00003253
Iteration 45/1000 | Loss: 0.00003253
Iteration 46/1000 | Loss: 0.00003253
Iteration 47/1000 | Loss: 0.00003253
Iteration 48/1000 | Loss: 0.00003253
Iteration 49/1000 | Loss: 0.00003252
Iteration 50/1000 | Loss: 0.00003250
Iteration 51/1000 | Loss: 0.00003250
Iteration 52/1000 | Loss: 0.00003250
Iteration 53/1000 | Loss: 0.00003249
Iteration 54/1000 | Loss: 0.00003249
Iteration 55/1000 | Loss: 0.00003248
Iteration 56/1000 | Loss: 0.00003247
Iteration 57/1000 | Loss: 0.00003246
Iteration 58/1000 | Loss: 0.00003246
Iteration 59/1000 | Loss: 0.00003246
Iteration 60/1000 | Loss: 0.00003246
Iteration 61/1000 | Loss: 0.00003245
Iteration 62/1000 | Loss: 0.00003245
Iteration 63/1000 | Loss: 0.00003245
Iteration 64/1000 | Loss: 0.00003245
Iteration 65/1000 | Loss: 0.00003244
Iteration 66/1000 | Loss: 0.00003244
Iteration 67/1000 | Loss: 0.00003244
Iteration 68/1000 | Loss: 0.00003244
Iteration 69/1000 | Loss: 0.00003244
Iteration 70/1000 | Loss: 0.00003244
Iteration 71/1000 | Loss: 0.00003244
Iteration 72/1000 | Loss: 0.00003244
Iteration 73/1000 | Loss: 0.00003243
Iteration 74/1000 | Loss: 0.00003243
Iteration 75/1000 | Loss: 0.00003243
Iteration 76/1000 | Loss: 0.00003243
Iteration 77/1000 | Loss: 0.00003243
Iteration 78/1000 | Loss: 0.00003243
Iteration 79/1000 | Loss: 0.00003243
Iteration 80/1000 | Loss: 0.00003242
Iteration 81/1000 | Loss: 0.00003242
Iteration 82/1000 | Loss: 0.00003242
Iteration 83/1000 | Loss: 0.00003242
Iteration 84/1000 | Loss: 0.00003242
Iteration 85/1000 | Loss: 0.00003242
Iteration 86/1000 | Loss: 0.00003242
Iteration 87/1000 | Loss: 0.00003242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [3.242298407712951e-05, 3.242298407712951e-05, 3.242298407712951e-05, 3.242298407712951e-05, 3.242298407712951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.242298407712951e-05

Optimization complete. Final v2v error: 4.638702869415283 mm

Highest mean error: 4.6976542472839355 mm for frame 0

Lowest mean error: 4.558343410491943 mm for frame 96

Saving results

Total time: 40.837557315826416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040312
Iteration 2/25 | Loss: 0.00254837
Iteration 3/25 | Loss: 0.00156446
Iteration 4/25 | Loss: 0.00135161
Iteration 5/25 | Loss: 0.00131698
Iteration 6/25 | Loss: 0.00131732
Iteration 7/25 | Loss: 0.00125611
Iteration 8/25 | Loss: 0.00116155
Iteration 9/25 | Loss: 0.00107350
Iteration 10/25 | Loss: 0.00103233
Iteration 11/25 | Loss: 0.00100773
Iteration 12/25 | Loss: 0.00099677
Iteration 13/25 | Loss: 0.00098423
Iteration 14/25 | Loss: 0.00099080
Iteration 15/25 | Loss: 0.00099227
Iteration 16/25 | Loss: 0.00099335
Iteration 17/25 | Loss: 0.00095531
Iteration 18/25 | Loss: 0.00093163
Iteration 19/25 | Loss: 0.00093595
Iteration 20/25 | Loss: 0.00091456
Iteration 21/25 | Loss: 0.00089966
Iteration 22/25 | Loss: 0.00089558
Iteration 23/25 | Loss: 0.00089378
Iteration 24/25 | Loss: 0.00089330
Iteration 25/25 | Loss: 0.00089325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98187399
Iteration 2/25 | Loss: 0.00227581
Iteration 3/25 | Loss: 0.00227581
Iteration 4/25 | Loss: 0.00227581
Iteration 5/25 | Loss: 0.00227581
Iteration 6/25 | Loss: 0.00227581
Iteration 7/25 | Loss: 0.00227581
Iteration 8/25 | Loss: 0.00227581
Iteration 9/25 | Loss: 0.00227581
Iteration 10/25 | Loss: 0.00227581
Iteration 11/25 | Loss: 0.00227581
Iteration 12/25 | Loss: 0.00227581
Iteration 13/25 | Loss: 0.00227581
Iteration 14/25 | Loss: 0.00227581
Iteration 15/25 | Loss: 0.00227581
Iteration 16/25 | Loss: 0.00227581
Iteration 17/25 | Loss: 0.00227581
Iteration 18/25 | Loss: 0.00227581
Iteration 19/25 | Loss: 0.00227581
Iteration 20/25 | Loss: 0.00227581
Iteration 21/25 | Loss: 0.00227581
Iteration 22/25 | Loss: 0.00227581
Iteration 23/25 | Loss: 0.00227581
Iteration 24/25 | Loss: 0.00227581
Iteration 25/25 | Loss: 0.00227581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227581
Iteration 2/1000 | Loss: 0.00111721
Iteration 3/1000 | Loss: 0.00198902
Iteration 4/1000 | Loss: 0.00020353
Iteration 5/1000 | Loss: 0.00142216
Iteration 6/1000 | Loss: 0.00079235
Iteration 7/1000 | Loss: 0.00175638
Iteration 8/1000 | Loss: 0.00229567
Iteration 9/1000 | Loss: 0.00359987
Iteration 10/1000 | Loss: 0.00045290
Iteration 11/1000 | Loss: 0.00157784
Iteration 12/1000 | Loss: 0.00107776
Iteration 13/1000 | Loss: 0.00096728
Iteration 14/1000 | Loss: 0.00080487
Iteration 15/1000 | Loss: 0.00121051
Iteration 16/1000 | Loss: 0.00058740
Iteration 17/1000 | Loss: 0.00011358
Iteration 18/1000 | Loss: 0.00021112
Iteration 19/1000 | Loss: 0.00009935
Iteration 20/1000 | Loss: 0.00009119
Iteration 21/1000 | Loss: 0.00035884
Iteration 22/1000 | Loss: 0.00018984
Iteration 23/1000 | Loss: 0.00205901
Iteration 24/1000 | Loss: 0.00078938
Iteration 25/1000 | Loss: 0.00127869
Iteration 26/1000 | Loss: 0.00140055
Iteration 27/1000 | Loss: 0.00190411
Iteration 28/1000 | Loss: 0.00255482
Iteration 29/1000 | Loss: 0.00171000
Iteration 30/1000 | Loss: 0.00045291
Iteration 31/1000 | Loss: 0.00059945
Iteration 32/1000 | Loss: 0.00034054
Iteration 33/1000 | Loss: 0.00044117
Iteration 34/1000 | Loss: 0.00016773
Iteration 35/1000 | Loss: 0.00030047
Iteration 36/1000 | Loss: 0.00190308
Iteration 37/1000 | Loss: 0.00415253
Iteration 38/1000 | Loss: 0.00490918
Iteration 39/1000 | Loss: 0.00596340
Iteration 40/1000 | Loss: 0.00426157
Iteration 41/1000 | Loss: 0.00288389
Iteration 42/1000 | Loss: 0.00032502
Iteration 43/1000 | Loss: 0.00063647
Iteration 44/1000 | Loss: 0.00018618
Iteration 45/1000 | Loss: 0.00052415
Iteration 46/1000 | Loss: 0.00050357
Iteration 47/1000 | Loss: 0.00031629
Iteration 48/1000 | Loss: 0.00133542
Iteration 49/1000 | Loss: 0.00097005
Iteration 50/1000 | Loss: 0.00005310
Iteration 51/1000 | Loss: 0.00004238
Iteration 52/1000 | Loss: 0.00003509
Iteration 53/1000 | Loss: 0.00027551
Iteration 54/1000 | Loss: 0.00020745
Iteration 55/1000 | Loss: 0.00003028
Iteration 56/1000 | Loss: 0.00002808
Iteration 57/1000 | Loss: 0.00041007
Iteration 58/1000 | Loss: 0.00025353
Iteration 59/1000 | Loss: 0.00052995
Iteration 60/1000 | Loss: 0.00003149
Iteration 61/1000 | Loss: 0.00002668
Iteration 62/1000 | Loss: 0.00002518
Iteration 63/1000 | Loss: 0.00002412
Iteration 64/1000 | Loss: 0.00002330
Iteration 65/1000 | Loss: 0.00035497
Iteration 66/1000 | Loss: 0.00004664
Iteration 67/1000 | Loss: 0.00002863
Iteration 68/1000 | Loss: 0.00027083
Iteration 69/1000 | Loss: 0.00019624
Iteration 70/1000 | Loss: 0.00013709
Iteration 71/1000 | Loss: 0.00002921
Iteration 72/1000 | Loss: 0.00044424
Iteration 73/1000 | Loss: 0.00130672
Iteration 74/1000 | Loss: 0.00085885
Iteration 75/1000 | Loss: 0.00040782
Iteration 76/1000 | Loss: 0.00004421
Iteration 77/1000 | Loss: 0.00003314
Iteration 78/1000 | Loss: 0.00002835
Iteration 79/1000 | Loss: 0.00049516
Iteration 80/1000 | Loss: 0.00006080
Iteration 81/1000 | Loss: 0.00016181
Iteration 82/1000 | Loss: 0.00002058
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001594
Iteration 88/1000 | Loss: 0.00001587
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001574
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001568
Iteration 94/1000 | Loss: 0.00001568
Iteration 95/1000 | Loss: 0.00001567
Iteration 96/1000 | Loss: 0.00001566
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001542
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001535
Iteration 113/1000 | Loss: 0.00001534
Iteration 114/1000 | Loss: 0.00001534
Iteration 115/1000 | Loss: 0.00001534
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001532
Iteration 119/1000 | Loss: 0.00001531
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001531
Iteration 122/1000 | Loss: 0.00001531
Iteration 123/1000 | Loss: 0.00001531
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00001529
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001529
Iteration 131/1000 | Loss: 0.00001529
Iteration 132/1000 | Loss: 0.00001529
Iteration 133/1000 | Loss: 0.00001529
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001526
Iteration 143/1000 | Loss: 0.00001526
Iteration 144/1000 | Loss: 0.00001526
Iteration 145/1000 | Loss: 0.00001526
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001526
Iteration 148/1000 | Loss: 0.00001526
Iteration 149/1000 | Loss: 0.00001526
Iteration 150/1000 | Loss: 0.00001526
Iteration 151/1000 | Loss: 0.00001526
Iteration 152/1000 | Loss: 0.00001526
Iteration 153/1000 | Loss: 0.00001526
Iteration 154/1000 | Loss: 0.00001526
Iteration 155/1000 | Loss: 0.00001526
Iteration 156/1000 | Loss: 0.00001526
Iteration 157/1000 | Loss: 0.00001526
Iteration 158/1000 | Loss: 0.00001526
Iteration 159/1000 | Loss: 0.00001526
Iteration 160/1000 | Loss: 0.00001526
Iteration 161/1000 | Loss: 0.00001525
Iteration 162/1000 | Loss: 0.00001525
Iteration 163/1000 | Loss: 0.00001525
Iteration 164/1000 | Loss: 0.00001525
Iteration 165/1000 | Loss: 0.00001525
Iteration 166/1000 | Loss: 0.00001525
Iteration 167/1000 | Loss: 0.00001525
Iteration 168/1000 | Loss: 0.00001525
Iteration 169/1000 | Loss: 0.00001525
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001525
Iteration 174/1000 | Loss: 0.00001525
Iteration 175/1000 | Loss: 0.00001525
Iteration 176/1000 | Loss: 0.00001525
Iteration 177/1000 | Loss: 0.00001525
Iteration 178/1000 | Loss: 0.00001525
Iteration 179/1000 | Loss: 0.00001525
Iteration 180/1000 | Loss: 0.00001525
Iteration 181/1000 | Loss: 0.00001525
Iteration 182/1000 | Loss: 0.00001524
Iteration 183/1000 | Loss: 0.00001524
Iteration 184/1000 | Loss: 0.00001524
Iteration 185/1000 | Loss: 0.00001524
Iteration 186/1000 | Loss: 0.00001524
Iteration 187/1000 | Loss: 0.00001524
Iteration 188/1000 | Loss: 0.00001524
Iteration 189/1000 | Loss: 0.00001524
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001523
Iteration 193/1000 | Loss: 0.00001523
Iteration 194/1000 | Loss: 0.00001523
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001523
Iteration 197/1000 | Loss: 0.00001523
Iteration 198/1000 | Loss: 0.00001523
Iteration 199/1000 | Loss: 0.00001523
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001522
Iteration 204/1000 | Loss: 0.00001522
Iteration 205/1000 | Loss: 0.00001522
Iteration 206/1000 | Loss: 0.00001522
Iteration 207/1000 | Loss: 0.00001522
Iteration 208/1000 | Loss: 0.00001522
Iteration 209/1000 | Loss: 0.00001522
Iteration 210/1000 | Loss: 0.00001521
Iteration 211/1000 | Loss: 0.00001521
Iteration 212/1000 | Loss: 0.00001521
Iteration 213/1000 | Loss: 0.00001521
Iteration 214/1000 | Loss: 0.00001521
Iteration 215/1000 | Loss: 0.00001521
Iteration 216/1000 | Loss: 0.00001521
Iteration 217/1000 | Loss: 0.00001521
Iteration 218/1000 | Loss: 0.00001521
Iteration 219/1000 | Loss: 0.00001521
Iteration 220/1000 | Loss: 0.00001521
Iteration 221/1000 | Loss: 0.00001520
Iteration 222/1000 | Loss: 0.00001520
Iteration 223/1000 | Loss: 0.00001520
Iteration 224/1000 | Loss: 0.00001520
Iteration 225/1000 | Loss: 0.00001520
Iteration 226/1000 | Loss: 0.00001520
Iteration 227/1000 | Loss: 0.00001520
Iteration 228/1000 | Loss: 0.00001520
Iteration 229/1000 | Loss: 0.00001520
Iteration 230/1000 | Loss: 0.00001520
Iteration 231/1000 | Loss: 0.00001520
Iteration 232/1000 | Loss: 0.00001520
Iteration 233/1000 | Loss: 0.00001520
Iteration 234/1000 | Loss: 0.00001520
Iteration 235/1000 | Loss: 0.00001520
Iteration 236/1000 | Loss: 0.00001520
Iteration 237/1000 | Loss: 0.00001519
Iteration 238/1000 | Loss: 0.00001519
Iteration 239/1000 | Loss: 0.00001519
Iteration 240/1000 | Loss: 0.00001519
Iteration 241/1000 | Loss: 0.00001519
Iteration 242/1000 | Loss: 0.00001519
Iteration 243/1000 | Loss: 0.00001519
Iteration 244/1000 | Loss: 0.00001519
Iteration 245/1000 | Loss: 0.00001519
Iteration 246/1000 | Loss: 0.00001519
Iteration 247/1000 | Loss: 0.00001519
Iteration 248/1000 | Loss: 0.00001519
Iteration 249/1000 | Loss: 0.00001519
Iteration 250/1000 | Loss: 0.00001519
Iteration 251/1000 | Loss: 0.00001519
Iteration 252/1000 | Loss: 0.00001519
Iteration 253/1000 | Loss: 0.00001519
Iteration 254/1000 | Loss: 0.00001519
Iteration 255/1000 | Loss: 0.00001519
Iteration 256/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.5192163118626922e-05, 1.5192163118626922e-05, 1.5192163118626922e-05, 1.5192163118626922e-05, 1.5192163118626922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5192163118626922e-05

Optimization complete. Final v2v error: 3.299682855606079 mm

Highest mean error: 4.274580478668213 mm for frame 45

Lowest mean error: 2.7560226917266846 mm for frame 98

Saving results

Total time: 178.42395973205566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042256
Iteration 2/25 | Loss: 0.00189542
Iteration 3/25 | Loss: 0.00132093
Iteration 4/25 | Loss: 0.00121499
Iteration 5/25 | Loss: 0.00125923
Iteration 6/25 | Loss: 0.00136424
Iteration 7/25 | Loss: 0.00115788
Iteration 8/25 | Loss: 0.00106723
Iteration 9/25 | Loss: 0.00103582
Iteration 10/25 | Loss: 0.00100403
Iteration 11/25 | Loss: 0.00099842
Iteration 12/25 | Loss: 0.00098658
Iteration 13/25 | Loss: 0.00097710
Iteration 14/25 | Loss: 0.00097494
Iteration 15/25 | Loss: 0.00097200
Iteration 16/25 | Loss: 0.00096668
Iteration 17/25 | Loss: 0.00096302
Iteration 18/25 | Loss: 0.00096153
Iteration 19/25 | Loss: 0.00096741
Iteration 20/25 | Loss: 0.00096491
Iteration 21/25 | Loss: 0.00096406
Iteration 22/25 | Loss: 0.00096441
Iteration 23/25 | Loss: 0.00096293
Iteration 24/25 | Loss: 0.00096451
Iteration 25/25 | Loss: 0.00096330

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09088337
Iteration 2/25 | Loss: 0.00173379
Iteration 3/25 | Loss: 0.00173379
Iteration 4/25 | Loss: 0.00173379
Iteration 5/25 | Loss: 0.00173379
Iteration 6/25 | Loss: 0.00173379
Iteration 7/25 | Loss: 0.00173379
Iteration 8/25 | Loss: 0.00173379
Iteration 9/25 | Loss: 0.00173379
Iteration 10/25 | Loss: 0.00173379
Iteration 11/25 | Loss: 0.00173379
Iteration 12/25 | Loss: 0.00173379
Iteration 13/25 | Loss: 0.00173379
Iteration 14/25 | Loss: 0.00173379
Iteration 15/25 | Loss: 0.00173379
Iteration 16/25 | Loss: 0.00173379
Iteration 17/25 | Loss: 0.00173379
Iteration 18/25 | Loss: 0.00173379
Iteration 19/25 | Loss: 0.00173379
Iteration 20/25 | Loss: 0.00173379
Iteration 21/25 | Loss: 0.00173379
Iteration 22/25 | Loss: 0.00173379
Iteration 23/25 | Loss: 0.00173379
Iteration 24/25 | Loss: 0.00173379
Iteration 25/25 | Loss: 0.00173379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017337899189442396, 0.0017337899189442396, 0.0017337899189442396, 0.0017337899189442396, 0.0017337899189442396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017337899189442396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173379
Iteration 2/1000 | Loss: 0.00016422
Iteration 3/1000 | Loss: 0.00013735
Iteration 4/1000 | Loss: 0.00013558
Iteration 5/1000 | Loss: 0.00012354
Iteration 6/1000 | Loss: 0.00013562
Iteration 7/1000 | Loss: 0.00009467
Iteration 8/1000 | Loss: 0.00008563
Iteration 9/1000 | Loss: 0.00009728
Iteration 10/1000 | Loss: 0.00009840
Iteration 11/1000 | Loss: 0.00010514
Iteration 12/1000 | Loss: 0.00013791
Iteration 13/1000 | Loss: 0.00013033
Iteration 14/1000 | Loss: 0.00011949
Iteration 15/1000 | Loss: 0.00009676
Iteration 16/1000 | Loss: 0.00012555
Iteration 17/1000 | Loss: 0.00009811
Iteration 18/1000 | Loss: 0.00010040
Iteration 19/1000 | Loss: 0.00007517
Iteration 20/1000 | Loss: 0.00007765
Iteration 21/1000 | Loss: 0.00009361
Iteration 22/1000 | Loss: 0.00008350
Iteration 23/1000 | Loss: 0.00008794
Iteration 24/1000 | Loss: 0.00010147
Iteration 25/1000 | Loss: 0.00010684
Iteration 26/1000 | Loss: 0.00010456
Iteration 27/1000 | Loss: 0.00008690
Iteration 28/1000 | Loss: 0.00009262
Iteration 29/1000 | Loss: 0.00010686
Iteration 30/1000 | Loss: 0.00010317
Iteration 31/1000 | Loss: 0.00009295
Iteration 32/1000 | Loss: 0.00010381
Iteration 33/1000 | Loss: 0.00010720
Iteration 34/1000 | Loss: 0.00010346
Iteration 35/1000 | Loss: 0.00011331
Iteration 36/1000 | Loss: 0.00017768
Iteration 37/1000 | Loss: 0.00011745
Iteration 38/1000 | Loss: 0.00013906
Iteration 39/1000 | Loss: 0.00010983
Iteration 40/1000 | Loss: 0.00012792
Iteration 41/1000 | Loss: 0.00012124
Iteration 42/1000 | Loss: 0.00011014
Iteration 43/1000 | Loss: 0.00011121
Iteration 44/1000 | Loss: 0.00009469
Iteration 45/1000 | Loss: 0.00009285
Iteration 46/1000 | Loss: 0.00009141
Iteration 47/1000 | Loss: 0.00006296
Iteration 48/1000 | Loss: 0.00007353
Iteration 49/1000 | Loss: 0.00007984
Iteration 50/1000 | Loss: 0.00008616
Iteration 51/1000 | Loss: 0.00005520
Iteration 52/1000 | Loss: 0.00006153
Iteration 53/1000 | Loss: 0.00004789
Iteration 54/1000 | Loss: 0.00006358
Iteration 55/1000 | Loss: 0.00006202
Iteration 56/1000 | Loss: 0.00006114
Iteration 57/1000 | Loss: 0.00006233
Iteration 58/1000 | Loss: 0.00006603
Iteration 59/1000 | Loss: 0.00006346
Iteration 60/1000 | Loss: 0.00006454
Iteration 61/1000 | Loss: 0.00006391
Iteration 62/1000 | Loss: 0.00005398
Iteration 63/1000 | Loss: 0.00009017
Iteration 64/1000 | Loss: 0.00006170
Iteration 65/1000 | Loss: 0.00005395
Iteration 66/1000 | Loss: 0.00003138
Iteration 67/1000 | Loss: 0.00006936
Iteration 68/1000 | Loss: 0.00006042
Iteration 69/1000 | Loss: 0.00004810
Iteration 70/1000 | Loss: 0.00004603
Iteration 71/1000 | Loss: 0.00004099
Iteration 72/1000 | Loss: 0.00003995
Iteration 73/1000 | Loss: 0.00004518
Iteration 74/1000 | Loss: 0.00004480
Iteration 75/1000 | Loss: 0.00005751
Iteration 76/1000 | Loss: 0.00004413
Iteration 77/1000 | Loss: 0.00004643
Iteration 78/1000 | Loss: 0.00004689
Iteration 79/1000 | Loss: 0.00006752
Iteration 80/1000 | Loss: 0.00006016
Iteration 81/1000 | Loss: 0.00004116
Iteration 82/1000 | Loss: 0.00004375
Iteration 83/1000 | Loss: 0.00004090
Iteration 84/1000 | Loss: 0.00004749
Iteration 85/1000 | Loss: 0.00006515
Iteration 86/1000 | Loss: 0.00005706
Iteration 87/1000 | Loss: 0.00004197
Iteration 88/1000 | Loss: 0.00004410
Iteration 89/1000 | Loss: 0.00006435
Iteration 90/1000 | Loss: 0.00005536
Iteration 91/1000 | Loss: 0.00004673
Iteration 92/1000 | Loss: 0.00004398
Iteration 93/1000 | Loss: 0.00003639
Iteration 94/1000 | Loss: 0.00004487
Iteration 95/1000 | Loss: 0.00003138
Iteration 96/1000 | Loss: 0.00004973
Iteration 97/1000 | Loss: 0.00003403
Iteration 98/1000 | Loss: 0.00002937
Iteration 99/1000 | Loss: 0.00002783
Iteration 100/1000 | Loss: 0.00002657
Iteration 101/1000 | Loss: 0.00015358
Iteration 102/1000 | Loss: 0.00003142
Iteration 103/1000 | Loss: 0.00002797
Iteration 104/1000 | Loss: 0.00002687
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00004513
Iteration 107/1000 | Loss: 0.00002933
Iteration 108/1000 | Loss: 0.00002834
Iteration 109/1000 | Loss: 0.00003383
Iteration 110/1000 | Loss: 0.00003937
Iteration 111/1000 | Loss: 0.00003901
Iteration 112/1000 | Loss: 0.00015096
Iteration 113/1000 | Loss: 0.00013336
Iteration 114/1000 | Loss: 0.00009635
Iteration 115/1000 | Loss: 0.00003851
Iteration 116/1000 | Loss: 0.00002808
Iteration 117/1000 | Loss: 0.00002670
Iteration 118/1000 | Loss: 0.00002614
Iteration 119/1000 | Loss: 0.00002568
Iteration 120/1000 | Loss: 0.00002525
Iteration 121/1000 | Loss: 0.00002487
Iteration 122/1000 | Loss: 0.00002467
Iteration 123/1000 | Loss: 0.00002456
Iteration 124/1000 | Loss: 0.00002447
Iteration 125/1000 | Loss: 0.00002442
Iteration 126/1000 | Loss: 0.00002435
Iteration 127/1000 | Loss: 0.00002430
Iteration 128/1000 | Loss: 0.00002426
Iteration 129/1000 | Loss: 0.00002424
Iteration 130/1000 | Loss: 0.00002423
Iteration 131/1000 | Loss: 0.00002417
Iteration 132/1000 | Loss: 0.00002416
Iteration 133/1000 | Loss: 0.00002415
Iteration 134/1000 | Loss: 0.00002414
Iteration 135/1000 | Loss: 0.00002397
Iteration 136/1000 | Loss: 0.00002392
Iteration 137/1000 | Loss: 0.00002380
Iteration 138/1000 | Loss: 0.00002380
Iteration 139/1000 | Loss: 0.00002379
Iteration 140/1000 | Loss: 0.00002376
Iteration 141/1000 | Loss: 0.00002376
Iteration 142/1000 | Loss: 0.00002371
Iteration 143/1000 | Loss: 0.00002370
Iteration 144/1000 | Loss: 0.00002365
Iteration 145/1000 | Loss: 0.00002362
Iteration 146/1000 | Loss: 0.00002361
Iteration 147/1000 | Loss: 0.00002361
Iteration 148/1000 | Loss: 0.00002361
Iteration 149/1000 | Loss: 0.00002360
Iteration 150/1000 | Loss: 0.00002360
Iteration 151/1000 | Loss: 0.00002360
Iteration 152/1000 | Loss: 0.00002360
Iteration 153/1000 | Loss: 0.00002360
Iteration 154/1000 | Loss: 0.00002360
Iteration 155/1000 | Loss: 0.00002360
Iteration 156/1000 | Loss: 0.00002360
Iteration 157/1000 | Loss: 0.00002360
Iteration 158/1000 | Loss: 0.00002360
Iteration 159/1000 | Loss: 0.00002360
Iteration 160/1000 | Loss: 0.00002360
Iteration 161/1000 | Loss: 0.00002359
Iteration 162/1000 | Loss: 0.00002358
Iteration 163/1000 | Loss: 0.00002358
Iteration 164/1000 | Loss: 0.00002358
Iteration 165/1000 | Loss: 0.00002358
Iteration 166/1000 | Loss: 0.00002358
Iteration 167/1000 | Loss: 0.00002358
Iteration 168/1000 | Loss: 0.00002358
Iteration 169/1000 | Loss: 0.00002358
Iteration 170/1000 | Loss: 0.00002358
Iteration 171/1000 | Loss: 0.00002357
Iteration 172/1000 | Loss: 0.00002357
Iteration 173/1000 | Loss: 0.00002357
Iteration 174/1000 | Loss: 0.00002357
Iteration 175/1000 | Loss: 0.00002357
Iteration 176/1000 | Loss: 0.00002357
Iteration 177/1000 | Loss: 0.00002357
Iteration 178/1000 | Loss: 0.00002356
Iteration 179/1000 | Loss: 0.00002356
Iteration 180/1000 | Loss: 0.00002356
Iteration 181/1000 | Loss: 0.00002356
Iteration 182/1000 | Loss: 0.00002356
Iteration 183/1000 | Loss: 0.00002356
Iteration 184/1000 | Loss: 0.00002356
Iteration 185/1000 | Loss: 0.00002356
Iteration 186/1000 | Loss: 0.00002356
Iteration 187/1000 | Loss: 0.00002356
Iteration 188/1000 | Loss: 0.00002356
Iteration 189/1000 | Loss: 0.00002356
Iteration 190/1000 | Loss: 0.00002355
Iteration 191/1000 | Loss: 0.00002355
Iteration 192/1000 | Loss: 0.00002355
Iteration 193/1000 | Loss: 0.00002355
Iteration 194/1000 | Loss: 0.00002354
Iteration 195/1000 | Loss: 0.00002354
Iteration 196/1000 | Loss: 0.00002354
Iteration 197/1000 | Loss: 0.00002354
Iteration 198/1000 | Loss: 0.00002353
Iteration 199/1000 | Loss: 0.00002353
Iteration 200/1000 | Loss: 0.00002353
Iteration 201/1000 | Loss: 0.00002353
Iteration 202/1000 | Loss: 0.00002353
Iteration 203/1000 | Loss: 0.00002353
Iteration 204/1000 | Loss: 0.00002352
Iteration 205/1000 | Loss: 0.00002352
Iteration 206/1000 | Loss: 0.00002352
Iteration 207/1000 | Loss: 0.00002352
Iteration 208/1000 | Loss: 0.00002352
Iteration 209/1000 | Loss: 0.00002352
Iteration 210/1000 | Loss: 0.00002351
Iteration 211/1000 | Loss: 0.00002351
Iteration 212/1000 | Loss: 0.00002351
Iteration 213/1000 | Loss: 0.00002350
Iteration 214/1000 | Loss: 0.00002347
Iteration 215/1000 | Loss: 0.00002347
Iteration 216/1000 | Loss: 0.00002347
Iteration 217/1000 | Loss: 0.00002347
Iteration 218/1000 | Loss: 0.00002347
Iteration 219/1000 | Loss: 0.00002347
Iteration 220/1000 | Loss: 0.00002347
Iteration 221/1000 | Loss: 0.00002347
Iteration 222/1000 | Loss: 0.00002346
Iteration 223/1000 | Loss: 0.00002346
Iteration 224/1000 | Loss: 0.00002346
Iteration 225/1000 | Loss: 0.00002346
Iteration 226/1000 | Loss: 0.00014456
Iteration 227/1000 | Loss: 0.00004688
Iteration 228/1000 | Loss: 0.00002640
Iteration 229/1000 | Loss: 0.00002425
Iteration 230/1000 | Loss: 0.00002376
Iteration 231/1000 | Loss: 0.00002343
Iteration 232/1000 | Loss: 0.00002320
Iteration 233/1000 | Loss: 0.00002308
Iteration 234/1000 | Loss: 0.00002305
Iteration 235/1000 | Loss: 0.00002302
Iteration 236/1000 | Loss: 0.00002302
Iteration 237/1000 | Loss: 0.00002302
Iteration 238/1000 | Loss: 0.00002301
Iteration 239/1000 | Loss: 0.00002301
Iteration 240/1000 | Loss: 0.00002301
Iteration 241/1000 | Loss: 0.00002301
Iteration 242/1000 | Loss: 0.00002301
Iteration 243/1000 | Loss: 0.00002301
Iteration 244/1000 | Loss: 0.00002300
Iteration 245/1000 | Loss: 0.00002300
Iteration 246/1000 | Loss: 0.00002300
Iteration 247/1000 | Loss: 0.00002300
Iteration 248/1000 | Loss: 0.00002300
Iteration 249/1000 | Loss: 0.00002300
Iteration 250/1000 | Loss: 0.00002300
Iteration 251/1000 | Loss: 0.00002299
Iteration 252/1000 | Loss: 0.00002299
Iteration 253/1000 | Loss: 0.00002299
Iteration 254/1000 | Loss: 0.00002299
Iteration 255/1000 | Loss: 0.00002299
Iteration 256/1000 | Loss: 0.00002299
Iteration 257/1000 | Loss: 0.00002298
Iteration 258/1000 | Loss: 0.00002298
Iteration 259/1000 | Loss: 0.00002298
Iteration 260/1000 | Loss: 0.00002298
Iteration 261/1000 | Loss: 0.00002298
Iteration 262/1000 | Loss: 0.00002298
Iteration 263/1000 | Loss: 0.00002297
Iteration 264/1000 | Loss: 0.00002297
Iteration 265/1000 | Loss: 0.00002297
Iteration 266/1000 | Loss: 0.00002296
Iteration 267/1000 | Loss: 0.00002296
Iteration 268/1000 | Loss: 0.00002295
Iteration 269/1000 | Loss: 0.00002295
Iteration 270/1000 | Loss: 0.00002295
Iteration 271/1000 | Loss: 0.00002294
Iteration 272/1000 | Loss: 0.00002294
Iteration 273/1000 | Loss: 0.00002294
Iteration 274/1000 | Loss: 0.00002294
Iteration 275/1000 | Loss: 0.00002294
Iteration 276/1000 | Loss: 0.00002293
Iteration 277/1000 | Loss: 0.00002293
Iteration 278/1000 | Loss: 0.00002293
Iteration 279/1000 | Loss: 0.00002293
Iteration 280/1000 | Loss: 0.00002293
Iteration 281/1000 | Loss: 0.00002293
Iteration 282/1000 | Loss: 0.00002293
Iteration 283/1000 | Loss: 0.00002293
Iteration 284/1000 | Loss: 0.00002293
Iteration 285/1000 | Loss: 0.00002293
Iteration 286/1000 | Loss: 0.00002292
Iteration 287/1000 | Loss: 0.00002292
Iteration 288/1000 | Loss: 0.00002292
Iteration 289/1000 | Loss: 0.00002292
Iteration 290/1000 | Loss: 0.00002292
Iteration 291/1000 | Loss: 0.00002292
Iteration 292/1000 | Loss: 0.00002292
Iteration 293/1000 | Loss: 0.00002292
Iteration 294/1000 | Loss: 0.00002292
Iteration 295/1000 | Loss: 0.00002292
Iteration 296/1000 | Loss: 0.00002292
Iteration 297/1000 | Loss: 0.00002291
Iteration 298/1000 | Loss: 0.00002291
Iteration 299/1000 | Loss: 0.00002291
Iteration 300/1000 | Loss: 0.00002291
Iteration 301/1000 | Loss: 0.00002291
Iteration 302/1000 | Loss: 0.00002291
Iteration 303/1000 | Loss: 0.00002291
Iteration 304/1000 | Loss: 0.00002291
Iteration 305/1000 | Loss: 0.00002291
Iteration 306/1000 | Loss: 0.00002291
Iteration 307/1000 | Loss: 0.00002291
Iteration 308/1000 | Loss: 0.00002291
Iteration 309/1000 | Loss: 0.00002291
Iteration 310/1000 | Loss: 0.00002291
Iteration 311/1000 | Loss: 0.00002291
Iteration 312/1000 | Loss: 0.00002291
Iteration 313/1000 | Loss: 0.00002291
Iteration 314/1000 | Loss: 0.00002290
Iteration 315/1000 | Loss: 0.00002290
Iteration 316/1000 | Loss: 0.00002290
Iteration 317/1000 | Loss: 0.00002290
Iteration 318/1000 | Loss: 0.00002290
Iteration 319/1000 | Loss: 0.00002290
Iteration 320/1000 | Loss: 0.00002290
Iteration 321/1000 | Loss: 0.00002290
Iteration 322/1000 | Loss: 0.00002290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [2.2903454009792767e-05, 2.2903454009792767e-05, 2.2903454009792767e-05, 2.2903454009792767e-05, 2.2903454009792767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2903454009792767e-05

Optimization complete. Final v2v error: 3.8649914264678955 mm

Highest mean error: 5.105598449707031 mm for frame 113

Lowest mean error: 3.5224897861480713 mm for frame 2

Saving results

Total time: 291.07960987091064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913388
Iteration 2/25 | Loss: 0.00136287
Iteration 3/25 | Loss: 0.00091075
Iteration 4/25 | Loss: 0.00084464
Iteration 5/25 | Loss: 0.00080293
Iteration 6/25 | Loss: 0.00079654
Iteration 7/25 | Loss: 0.00079119
Iteration 8/25 | Loss: 0.00078956
Iteration 9/25 | Loss: 0.00078444
Iteration 10/25 | Loss: 0.00078317
Iteration 11/25 | Loss: 0.00078260
Iteration 12/25 | Loss: 0.00078103
Iteration 13/25 | Loss: 0.00078057
Iteration 14/25 | Loss: 0.00078042
Iteration 15/25 | Loss: 0.00078041
Iteration 16/25 | Loss: 0.00078040
Iteration 17/25 | Loss: 0.00078040
Iteration 18/25 | Loss: 0.00078039
Iteration 19/25 | Loss: 0.00078039
Iteration 20/25 | Loss: 0.00078039
Iteration 21/25 | Loss: 0.00078039
Iteration 22/25 | Loss: 0.00078039
Iteration 23/25 | Loss: 0.00078039
Iteration 24/25 | Loss: 0.00078039
Iteration 25/25 | Loss: 0.00078039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40277267
Iteration 2/25 | Loss: 0.00137280
Iteration 3/25 | Loss: 0.00137280
Iteration 4/25 | Loss: 0.00137280
Iteration 5/25 | Loss: 0.00137280
Iteration 6/25 | Loss: 0.00137280
Iteration 7/25 | Loss: 0.00137280
Iteration 8/25 | Loss: 0.00137280
Iteration 9/25 | Loss: 0.00137280
Iteration 10/25 | Loss: 0.00137280
Iteration 11/25 | Loss: 0.00137280
Iteration 12/25 | Loss: 0.00137280
Iteration 13/25 | Loss: 0.00137280
Iteration 14/25 | Loss: 0.00137280
Iteration 15/25 | Loss: 0.00137280
Iteration 16/25 | Loss: 0.00137280
Iteration 17/25 | Loss: 0.00137280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013727981131523848, 0.0013727981131523848, 0.0013727981131523848, 0.0013727981131523848, 0.0013727981131523848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013727981131523848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137280
Iteration 2/1000 | Loss: 0.00005788
Iteration 3/1000 | Loss: 0.00004909
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00004694
Iteration 6/1000 | Loss: 0.00005269
Iteration 7/1000 | Loss: 0.00002292
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001631
Iteration 10/1000 | Loss: 0.00001594
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001556
Iteration 13/1000 | Loss: 0.00001555
Iteration 14/1000 | Loss: 0.00001545
Iteration 15/1000 | Loss: 0.00001542
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00001539
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00005601
Iteration 21/1000 | Loss: 0.00001541
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001534
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001529
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001529
Iteration 33/1000 | Loss: 0.00001529
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001527
Iteration 36/1000 | Loss: 0.00001527
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001526
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001525
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001522
Iteration 56/1000 | Loss: 0.00001522
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001519
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00004754
Iteration 78/1000 | Loss: 0.00005225
Iteration 79/1000 | Loss: 0.00001514
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001514
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00003596
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001507
Iteration 108/1000 | Loss: 0.00001507
Iteration 109/1000 | Loss: 0.00001507
Iteration 110/1000 | Loss: 0.00001507
Iteration 111/1000 | Loss: 0.00001507
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001506
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001504
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001504
Iteration 132/1000 | Loss: 0.00001504
Iteration 133/1000 | Loss: 0.00001504
Iteration 134/1000 | Loss: 0.00001504
Iteration 135/1000 | Loss: 0.00001504
Iteration 136/1000 | Loss: 0.00001503
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.5022378647699952e-05, 1.5022378647699952e-05, 1.5022378647699952e-05, 1.5022378647699952e-05, 1.5022378647699952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5022378647699952e-05

Optimization complete. Final v2v error: 3.2687783241271973 mm

Highest mean error: 3.52536940574646 mm for frame 50

Lowest mean error: 2.982228994369507 mm for frame 3

Saving results

Total time: 69.19845414161682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082868
Iteration 2/25 | Loss: 0.00154590
Iteration 3/25 | Loss: 0.00116119
Iteration 4/25 | Loss: 0.00115442
Iteration 5/25 | Loss: 0.00111966
Iteration 6/25 | Loss: 0.00109003
Iteration 7/25 | Loss: 0.00105513
Iteration 8/25 | Loss: 0.00114969
Iteration 9/25 | Loss: 0.00107758
Iteration 10/25 | Loss: 0.00102447
Iteration 11/25 | Loss: 0.00100608
Iteration 12/25 | Loss: 0.00096426
Iteration 13/25 | Loss: 0.00095098
Iteration 14/25 | Loss: 0.00095281
Iteration 15/25 | Loss: 0.00094974
Iteration 16/25 | Loss: 0.00094373
Iteration 17/25 | Loss: 0.00094240
Iteration 18/25 | Loss: 0.00094155
Iteration 19/25 | Loss: 0.00094088
Iteration 20/25 | Loss: 0.00094037
Iteration 21/25 | Loss: 0.00100052
Iteration 22/25 | Loss: 0.00092946
Iteration 23/25 | Loss: 0.00092009
Iteration 24/25 | Loss: 0.00091747
Iteration 25/25 | Loss: 0.00091642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.81260109
Iteration 2/25 | Loss: 0.00264826
Iteration 3/25 | Loss: 0.00264826
Iteration 4/25 | Loss: 0.00264826
Iteration 5/25 | Loss: 0.00264826
Iteration 6/25 | Loss: 0.00264826
Iteration 7/25 | Loss: 0.00264826
Iteration 8/25 | Loss: 0.00264826
Iteration 9/25 | Loss: 0.00264825
Iteration 10/25 | Loss: 0.00264825
Iteration 11/25 | Loss: 0.00264825
Iteration 12/25 | Loss: 0.00264825
Iteration 13/25 | Loss: 0.00264825
Iteration 14/25 | Loss: 0.00264825
Iteration 15/25 | Loss: 0.00264825
Iteration 16/25 | Loss: 0.00264825
Iteration 17/25 | Loss: 0.00264825
Iteration 18/25 | Loss: 0.00264825
Iteration 19/25 | Loss: 0.00264825
Iteration 20/25 | Loss: 0.00264825
Iteration 21/25 | Loss: 0.00264825
Iteration 22/25 | Loss: 0.00264825
Iteration 23/25 | Loss: 0.00264825
Iteration 24/25 | Loss: 0.00264825
Iteration 25/25 | Loss: 0.00264825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264825
Iteration 2/1000 | Loss: 0.01279922
Iteration 3/1000 | Loss: 0.00749360
Iteration 4/1000 | Loss: 0.00688044
Iteration 5/1000 | Loss: 0.00477641
Iteration 6/1000 | Loss: 0.00191671
Iteration 7/1000 | Loss: 0.00290853
Iteration 8/1000 | Loss: 0.00215975
Iteration 9/1000 | Loss: 0.00173982
Iteration 10/1000 | Loss: 0.00194280
Iteration 11/1000 | Loss: 0.00078034
Iteration 12/1000 | Loss: 0.00228073
Iteration 13/1000 | Loss: 0.00218123
Iteration 14/1000 | Loss: 0.00183603
Iteration 15/1000 | Loss: 0.00136474
Iteration 16/1000 | Loss: 0.00219594
Iteration 17/1000 | Loss: 0.00338021
Iteration 18/1000 | Loss: 0.00278734
Iteration 19/1000 | Loss: 0.00317774
Iteration 20/1000 | Loss: 0.00210697
Iteration 21/1000 | Loss: 0.00203681
Iteration 22/1000 | Loss: 0.00218439
Iteration 23/1000 | Loss: 0.00366269
Iteration 24/1000 | Loss: 0.00593368
Iteration 25/1000 | Loss: 0.00485169
Iteration 26/1000 | Loss: 0.00251297
Iteration 27/1000 | Loss: 0.00114085
Iteration 28/1000 | Loss: 0.00075427
Iteration 29/1000 | Loss: 0.00237287
Iteration 30/1000 | Loss: 0.00106641
Iteration 31/1000 | Loss: 0.00040430
Iteration 32/1000 | Loss: 0.00167617
Iteration 33/1000 | Loss: 0.00073580
Iteration 34/1000 | Loss: 0.00005086
Iteration 35/1000 | Loss: 0.00003633
Iteration 36/1000 | Loss: 0.00070106
Iteration 37/1000 | Loss: 0.00126751
Iteration 38/1000 | Loss: 0.00065125
Iteration 39/1000 | Loss: 0.00080880
Iteration 40/1000 | Loss: 0.00130975
Iteration 41/1000 | Loss: 0.00009236
Iteration 42/1000 | Loss: 0.00004304
Iteration 43/1000 | Loss: 0.00098172
Iteration 44/1000 | Loss: 0.00131611
Iteration 45/1000 | Loss: 0.00003086
Iteration 46/1000 | Loss: 0.00010550
Iteration 47/1000 | Loss: 0.00057199
Iteration 48/1000 | Loss: 0.00025599
Iteration 49/1000 | Loss: 0.00036281
Iteration 50/1000 | Loss: 0.00099585
Iteration 51/1000 | Loss: 0.00139262
Iteration 52/1000 | Loss: 0.00132600
Iteration 53/1000 | Loss: 0.00078247
Iteration 54/1000 | Loss: 0.00080163
Iteration 55/1000 | Loss: 0.00051949
Iteration 56/1000 | Loss: 0.00177956
Iteration 57/1000 | Loss: 0.00204043
Iteration 58/1000 | Loss: 0.00051157
Iteration 59/1000 | Loss: 0.00036051
Iteration 60/1000 | Loss: 0.00097860
Iteration 61/1000 | Loss: 0.00061831
Iteration 62/1000 | Loss: 0.00003055
Iteration 63/1000 | Loss: 0.00132694
Iteration 64/1000 | Loss: 0.00008835
Iteration 65/1000 | Loss: 0.00003935
Iteration 66/1000 | Loss: 0.00124572
Iteration 67/1000 | Loss: 0.00003060
Iteration 68/1000 | Loss: 0.00002110
Iteration 69/1000 | Loss: 0.00001836
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001490
Iteration 73/1000 | Loss: 0.00001428
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00040064
Iteration 78/1000 | Loss: 0.00002281
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001353
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001255
Iteration 88/1000 | Loss: 0.00001255
Iteration 89/1000 | Loss: 0.00001254
Iteration 90/1000 | Loss: 0.00001253
Iteration 91/1000 | Loss: 0.00001253
Iteration 92/1000 | Loss: 0.00001253
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001252
Iteration 95/1000 | Loss: 0.00001252
Iteration 96/1000 | Loss: 0.00001252
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001244
Iteration 111/1000 | Loss: 0.00001244
Iteration 112/1000 | Loss: 0.00001244
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001243
Iteration 120/1000 | Loss: 0.00001243
Iteration 121/1000 | Loss: 0.00001243
Iteration 122/1000 | Loss: 0.00001243
Iteration 123/1000 | Loss: 0.00001243
Iteration 124/1000 | Loss: 0.00001243
Iteration 125/1000 | Loss: 0.00001243
Iteration 126/1000 | Loss: 0.00001243
Iteration 127/1000 | Loss: 0.00001242
Iteration 128/1000 | Loss: 0.00001242
Iteration 129/1000 | Loss: 0.00001242
Iteration 130/1000 | Loss: 0.00001242
Iteration 131/1000 | Loss: 0.00001242
Iteration 132/1000 | Loss: 0.00001242
Iteration 133/1000 | Loss: 0.00001241
Iteration 134/1000 | Loss: 0.00001241
Iteration 135/1000 | Loss: 0.00001241
Iteration 136/1000 | Loss: 0.00001241
Iteration 137/1000 | Loss: 0.00001241
Iteration 138/1000 | Loss: 0.00001241
Iteration 139/1000 | Loss: 0.00001241
Iteration 140/1000 | Loss: 0.00001241
Iteration 141/1000 | Loss: 0.00001241
Iteration 142/1000 | Loss: 0.00001240
Iteration 143/1000 | Loss: 0.00001240
Iteration 144/1000 | Loss: 0.00001240
Iteration 145/1000 | Loss: 0.00001240
Iteration 146/1000 | Loss: 0.00001240
Iteration 147/1000 | Loss: 0.00001240
Iteration 148/1000 | Loss: 0.00001240
Iteration 149/1000 | Loss: 0.00001240
Iteration 150/1000 | Loss: 0.00001240
Iteration 151/1000 | Loss: 0.00001239
Iteration 152/1000 | Loss: 0.00001239
Iteration 153/1000 | Loss: 0.00001239
Iteration 154/1000 | Loss: 0.00001239
Iteration 155/1000 | Loss: 0.00001239
Iteration 156/1000 | Loss: 0.00001239
Iteration 157/1000 | Loss: 0.00001239
Iteration 158/1000 | Loss: 0.00001238
Iteration 159/1000 | Loss: 0.00001238
Iteration 160/1000 | Loss: 0.00001238
Iteration 161/1000 | Loss: 0.00001238
Iteration 162/1000 | Loss: 0.00001238
Iteration 163/1000 | Loss: 0.00001237
Iteration 164/1000 | Loss: 0.00001237
Iteration 165/1000 | Loss: 0.00001237
Iteration 166/1000 | Loss: 0.00001237
Iteration 167/1000 | Loss: 0.00001237
Iteration 168/1000 | Loss: 0.00001237
Iteration 169/1000 | Loss: 0.00001236
Iteration 170/1000 | Loss: 0.00001236
Iteration 171/1000 | Loss: 0.00001236
Iteration 172/1000 | Loss: 0.00001236
Iteration 173/1000 | Loss: 0.00001236
Iteration 174/1000 | Loss: 0.00001236
Iteration 175/1000 | Loss: 0.00001236
Iteration 176/1000 | Loss: 0.00001236
Iteration 177/1000 | Loss: 0.00001236
Iteration 178/1000 | Loss: 0.00001236
Iteration 179/1000 | Loss: 0.00001236
Iteration 180/1000 | Loss: 0.00001236
Iteration 181/1000 | Loss: 0.00001236
Iteration 182/1000 | Loss: 0.00001235
Iteration 183/1000 | Loss: 0.00001235
Iteration 184/1000 | Loss: 0.00001235
Iteration 185/1000 | Loss: 0.00001235
Iteration 186/1000 | Loss: 0.00001235
Iteration 187/1000 | Loss: 0.00001235
Iteration 188/1000 | Loss: 0.00001235
Iteration 189/1000 | Loss: 0.00001235
Iteration 190/1000 | Loss: 0.00001235
Iteration 191/1000 | Loss: 0.00001235
Iteration 192/1000 | Loss: 0.00001235
Iteration 193/1000 | Loss: 0.00001235
Iteration 194/1000 | Loss: 0.00001235
Iteration 195/1000 | Loss: 0.00001235
Iteration 196/1000 | Loss: 0.00001235
Iteration 197/1000 | Loss: 0.00001235
Iteration 198/1000 | Loss: 0.00001235
Iteration 199/1000 | Loss: 0.00001235
Iteration 200/1000 | Loss: 0.00001235
Iteration 201/1000 | Loss: 0.00001235
Iteration 202/1000 | Loss: 0.00001235
Iteration 203/1000 | Loss: 0.00001235
Iteration 204/1000 | Loss: 0.00001235
Iteration 205/1000 | Loss: 0.00001235
Iteration 206/1000 | Loss: 0.00001235
Iteration 207/1000 | Loss: 0.00001235
Iteration 208/1000 | Loss: 0.00001235
Iteration 209/1000 | Loss: 0.00001235
Iteration 210/1000 | Loss: 0.00001235
Iteration 211/1000 | Loss: 0.00001235
Iteration 212/1000 | Loss: 0.00001235
Iteration 213/1000 | Loss: 0.00001235
Iteration 214/1000 | Loss: 0.00001235
Iteration 215/1000 | Loss: 0.00001235
Iteration 216/1000 | Loss: 0.00001235
Iteration 217/1000 | Loss: 0.00001235
Iteration 218/1000 | Loss: 0.00001235
Iteration 219/1000 | Loss: 0.00001235
Iteration 220/1000 | Loss: 0.00001235
Iteration 221/1000 | Loss: 0.00001235
Iteration 222/1000 | Loss: 0.00001235
Iteration 223/1000 | Loss: 0.00001235
Iteration 224/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.2349235475994647e-05, 1.2349235475994647e-05, 1.2349235475994647e-05, 1.2349235475994647e-05, 1.2349235475994647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2349235475994647e-05

Optimization complete. Final v2v error: 2.94752836227417 mm

Highest mean error: 4.164559841156006 mm for frame 21

Lowest mean error: 2.5542335510253906 mm for frame 99

Saving results

Total time: 176.4972722530365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833176
Iteration 2/25 | Loss: 0.00104568
Iteration 3/25 | Loss: 0.00087578
Iteration 4/25 | Loss: 0.00083518
Iteration 5/25 | Loss: 0.00082899
Iteration 6/25 | Loss: 0.00082864
Iteration 7/25 | Loss: 0.00082864
Iteration 8/25 | Loss: 0.00082864
Iteration 9/25 | Loss: 0.00082864
Iteration 10/25 | Loss: 0.00082864
Iteration 11/25 | Loss: 0.00082864
Iteration 12/25 | Loss: 0.00082864
Iteration 13/25 | Loss: 0.00082864
Iteration 14/25 | Loss: 0.00082864
Iteration 15/25 | Loss: 0.00082864
Iteration 16/25 | Loss: 0.00082864
Iteration 17/25 | Loss: 0.00082864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000828636169899255, 0.000828636169899255, 0.000828636169899255, 0.000828636169899255, 0.000828636169899255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000828636169899255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57182777
Iteration 2/25 | Loss: 0.00134199
Iteration 3/25 | Loss: 0.00134196
Iteration 4/25 | Loss: 0.00134196
Iteration 5/25 | Loss: 0.00134196
Iteration 6/25 | Loss: 0.00134196
Iteration 7/25 | Loss: 0.00134196
Iteration 8/25 | Loss: 0.00134196
Iteration 9/25 | Loss: 0.00134196
Iteration 10/25 | Loss: 0.00134196
Iteration 11/25 | Loss: 0.00134196
Iteration 12/25 | Loss: 0.00134196
Iteration 13/25 | Loss: 0.00134196
Iteration 14/25 | Loss: 0.00134196
Iteration 15/25 | Loss: 0.00134196
Iteration 16/25 | Loss: 0.00134196
Iteration 17/25 | Loss: 0.00134196
Iteration 18/25 | Loss: 0.00134196
Iteration 19/25 | Loss: 0.00134196
Iteration 20/25 | Loss: 0.00134196
Iteration 21/25 | Loss: 0.00134196
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001341962139122188, 0.001341962139122188, 0.001341962139122188, 0.001341962139122188, 0.001341962139122188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001341962139122188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134196
Iteration 2/1000 | Loss: 0.00003848
Iteration 3/1000 | Loss: 0.00002862
Iteration 4/1000 | Loss: 0.00002679
Iteration 5/1000 | Loss: 0.00002543
Iteration 6/1000 | Loss: 0.00002392
Iteration 7/1000 | Loss: 0.00002308
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002222
Iteration 10/1000 | Loss: 0.00002204
Iteration 11/1000 | Loss: 0.00002200
Iteration 12/1000 | Loss: 0.00002188
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00002186
Iteration 15/1000 | Loss: 0.00002186
Iteration 16/1000 | Loss: 0.00002186
Iteration 17/1000 | Loss: 0.00002174
Iteration 18/1000 | Loss: 0.00002170
Iteration 19/1000 | Loss: 0.00002168
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002167
Iteration 22/1000 | Loss: 0.00002167
Iteration 23/1000 | Loss: 0.00002166
Iteration 24/1000 | Loss: 0.00002161
Iteration 25/1000 | Loss: 0.00002160
Iteration 26/1000 | Loss: 0.00002160
Iteration 27/1000 | Loss: 0.00002157
Iteration 28/1000 | Loss: 0.00002157
Iteration 29/1000 | Loss: 0.00002156
Iteration 30/1000 | Loss: 0.00002156
Iteration 31/1000 | Loss: 0.00002156
Iteration 32/1000 | Loss: 0.00002156
Iteration 33/1000 | Loss: 0.00002156
Iteration 34/1000 | Loss: 0.00002156
Iteration 35/1000 | Loss: 0.00002156
Iteration 36/1000 | Loss: 0.00002155
Iteration 37/1000 | Loss: 0.00002155
Iteration 38/1000 | Loss: 0.00002155
Iteration 39/1000 | Loss: 0.00002155
Iteration 40/1000 | Loss: 0.00002155
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00002155
Iteration 43/1000 | Loss: 0.00002154
Iteration 44/1000 | Loss: 0.00002153
Iteration 45/1000 | Loss: 0.00002153
Iteration 46/1000 | Loss: 0.00002153
Iteration 47/1000 | Loss: 0.00002152
Iteration 48/1000 | Loss: 0.00002151
Iteration 49/1000 | Loss: 0.00002151
Iteration 50/1000 | Loss: 0.00002150
Iteration 51/1000 | Loss: 0.00002149
Iteration 52/1000 | Loss: 0.00002148
Iteration 53/1000 | Loss: 0.00002148
Iteration 54/1000 | Loss: 0.00002148
Iteration 55/1000 | Loss: 0.00002147
Iteration 56/1000 | Loss: 0.00002145
Iteration 57/1000 | Loss: 0.00002145
Iteration 58/1000 | Loss: 0.00002145
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002144
Iteration 61/1000 | Loss: 0.00002143
Iteration 62/1000 | Loss: 0.00002143
Iteration 63/1000 | Loss: 0.00002143
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002142
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002141
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002141
Iteration 71/1000 | Loss: 0.00002141
Iteration 72/1000 | Loss: 0.00002141
Iteration 73/1000 | Loss: 0.00002141
Iteration 74/1000 | Loss: 0.00002140
Iteration 75/1000 | Loss: 0.00002140
Iteration 76/1000 | Loss: 0.00002139
Iteration 77/1000 | Loss: 0.00002139
Iteration 78/1000 | Loss: 0.00002139
Iteration 79/1000 | Loss: 0.00002138
Iteration 80/1000 | Loss: 0.00002138
Iteration 81/1000 | Loss: 0.00002138
Iteration 82/1000 | Loss: 0.00002138
Iteration 83/1000 | Loss: 0.00002138
Iteration 84/1000 | Loss: 0.00002138
Iteration 85/1000 | Loss: 0.00002137
Iteration 86/1000 | Loss: 0.00002137
Iteration 87/1000 | Loss: 0.00002137
Iteration 88/1000 | Loss: 0.00002136
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002134
Iteration 91/1000 | Loss: 0.00002134
Iteration 92/1000 | Loss: 0.00002133
Iteration 93/1000 | Loss: 0.00002132
Iteration 94/1000 | Loss: 0.00002132
Iteration 95/1000 | Loss: 0.00002132
Iteration 96/1000 | Loss: 0.00002132
Iteration 97/1000 | Loss: 0.00002132
Iteration 98/1000 | Loss: 0.00002132
Iteration 99/1000 | Loss: 0.00002132
Iteration 100/1000 | Loss: 0.00002132
Iteration 101/1000 | Loss: 0.00002132
Iteration 102/1000 | Loss: 0.00002132
Iteration 103/1000 | Loss: 0.00002132
Iteration 104/1000 | Loss: 0.00002132
Iteration 105/1000 | Loss: 0.00002132
Iteration 106/1000 | Loss: 0.00002132
Iteration 107/1000 | Loss: 0.00002132
Iteration 108/1000 | Loss: 0.00002132
Iteration 109/1000 | Loss: 0.00002132
Iteration 110/1000 | Loss: 0.00002132
Iteration 111/1000 | Loss: 0.00002132
Iteration 112/1000 | Loss: 0.00002132
Iteration 113/1000 | Loss: 0.00002132
Iteration 114/1000 | Loss: 0.00002132
Iteration 115/1000 | Loss: 0.00002132
Iteration 116/1000 | Loss: 0.00002132
Iteration 117/1000 | Loss: 0.00002132
Iteration 118/1000 | Loss: 0.00002132
Iteration 119/1000 | Loss: 0.00002132
Iteration 120/1000 | Loss: 0.00002132
Iteration 121/1000 | Loss: 0.00002132
Iteration 122/1000 | Loss: 0.00002132
Iteration 123/1000 | Loss: 0.00002132
Iteration 124/1000 | Loss: 0.00002132
Iteration 125/1000 | Loss: 0.00002132
Iteration 126/1000 | Loss: 0.00002132
Iteration 127/1000 | Loss: 0.00002132
Iteration 128/1000 | Loss: 0.00002132
Iteration 129/1000 | Loss: 0.00002132
Iteration 130/1000 | Loss: 0.00002132
Iteration 131/1000 | Loss: 0.00002132
Iteration 132/1000 | Loss: 0.00002132
Iteration 133/1000 | Loss: 0.00002132
Iteration 134/1000 | Loss: 0.00002132
Iteration 135/1000 | Loss: 0.00002132
Iteration 136/1000 | Loss: 0.00002132
Iteration 137/1000 | Loss: 0.00002132
Iteration 138/1000 | Loss: 0.00002132
Iteration 139/1000 | Loss: 0.00002132
Iteration 140/1000 | Loss: 0.00002132
Iteration 141/1000 | Loss: 0.00002132
Iteration 142/1000 | Loss: 0.00002132
Iteration 143/1000 | Loss: 0.00002132
Iteration 144/1000 | Loss: 0.00002132
Iteration 145/1000 | Loss: 0.00002132
Iteration 146/1000 | Loss: 0.00002132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.1318553990568034e-05, 2.1318553990568034e-05, 2.1318553990568034e-05, 2.1318553990568034e-05, 2.1318553990568034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1318553990568034e-05

Optimization complete. Final v2v error: 3.886547088623047 mm

Highest mean error: 4.167341709136963 mm for frame 65

Lowest mean error: 3.5284388065338135 mm for frame 33

Saving results

Total time: 36.925830364227295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01172981
Iteration 2/25 | Loss: 0.00196464
Iteration 3/25 | Loss: 0.00126208
Iteration 4/25 | Loss: 0.00114425
Iteration 5/25 | Loss: 0.00112598
Iteration 6/25 | Loss: 0.00109734
Iteration 7/25 | Loss: 0.00108763
Iteration 8/25 | Loss: 0.00108500
Iteration 9/25 | Loss: 0.00107362
Iteration 10/25 | Loss: 0.00103508
Iteration 11/25 | Loss: 0.00102875
Iteration 12/25 | Loss: 0.00102458
Iteration 13/25 | Loss: 0.00101611
Iteration 14/25 | Loss: 0.00100819
Iteration 15/25 | Loss: 0.00100943
Iteration 16/25 | Loss: 0.00100100
Iteration 17/25 | Loss: 0.00099882
Iteration 18/25 | Loss: 0.00099230
Iteration 19/25 | Loss: 0.00099501
Iteration 20/25 | Loss: 0.00098771
Iteration 21/25 | Loss: 0.00098761
Iteration 22/25 | Loss: 0.00098727
Iteration 23/25 | Loss: 0.00098399
Iteration 24/25 | Loss: 0.00098356
Iteration 25/25 | Loss: 0.00098308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07721186
Iteration 2/25 | Loss: 0.00183935
Iteration 3/25 | Loss: 0.00166489
Iteration 4/25 | Loss: 0.00166489
Iteration 5/25 | Loss: 0.00166489
Iteration 6/25 | Loss: 0.00166489
Iteration 7/25 | Loss: 0.00166489
Iteration 8/25 | Loss: 0.00166489
Iteration 9/25 | Loss: 0.00166489
Iteration 10/25 | Loss: 0.00166489
Iteration 11/25 | Loss: 0.00166489
Iteration 12/25 | Loss: 0.00166489
Iteration 13/25 | Loss: 0.00166489
Iteration 14/25 | Loss: 0.00166489
Iteration 15/25 | Loss: 0.00166489
Iteration 16/25 | Loss: 0.00166489
Iteration 17/25 | Loss: 0.00166489
Iteration 18/25 | Loss: 0.00166489
Iteration 19/25 | Loss: 0.00166489
Iteration 20/25 | Loss: 0.00166489
Iteration 21/25 | Loss: 0.00166489
Iteration 22/25 | Loss: 0.00166489
Iteration 23/25 | Loss: 0.00166489
Iteration 24/25 | Loss: 0.00166489
Iteration 25/25 | Loss: 0.00166489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166489
Iteration 2/1000 | Loss: 0.00047914
Iteration 3/1000 | Loss: 0.00039302
Iteration 4/1000 | Loss: 0.00173057
Iteration 5/1000 | Loss: 0.00074312
Iteration 6/1000 | Loss: 0.00123834
Iteration 7/1000 | Loss: 0.00099708
Iteration 8/1000 | Loss: 0.00080419
Iteration 9/1000 | Loss: 0.00152035
Iteration 10/1000 | Loss: 0.00063297
Iteration 11/1000 | Loss: 0.00176099
Iteration 12/1000 | Loss: 0.00127790
Iteration 13/1000 | Loss: 0.00067287
Iteration 14/1000 | Loss: 0.00088015
Iteration 15/1000 | Loss: 0.00061624
Iteration 16/1000 | Loss: 0.00052782
Iteration 17/1000 | Loss: 0.00038459
Iteration 18/1000 | Loss: 0.00029676
Iteration 19/1000 | Loss: 0.00065283
Iteration 20/1000 | Loss: 0.00025390
Iteration 21/1000 | Loss: 0.00046859
Iteration 22/1000 | Loss: 0.00056016
Iteration 23/1000 | Loss: 0.00042606
Iteration 24/1000 | Loss: 0.00113213
Iteration 25/1000 | Loss: 0.00064241
Iteration 26/1000 | Loss: 0.00059803
Iteration 27/1000 | Loss: 0.00107021
Iteration 28/1000 | Loss: 0.00039880
Iteration 29/1000 | Loss: 0.00071460
Iteration 30/1000 | Loss: 0.00020919
Iteration 31/1000 | Loss: 0.00007631
Iteration 32/1000 | Loss: 0.00012172
Iteration 33/1000 | Loss: 0.00005388
Iteration 34/1000 | Loss: 0.00004938
Iteration 35/1000 | Loss: 0.00004588
Iteration 36/1000 | Loss: 0.00020367
Iteration 37/1000 | Loss: 0.00115789
Iteration 38/1000 | Loss: 0.00087580
Iteration 39/1000 | Loss: 0.00100422
Iteration 40/1000 | Loss: 0.00064111
Iteration 41/1000 | Loss: 0.00094710
Iteration 42/1000 | Loss: 0.00056811
Iteration 43/1000 | Loss: 0.00079288
Iteration 44/1000 | Loss: 0.00062443
Iteration 45/1000 | Loss: 0.00007100
Iteration 46/1000 | Loss: 0.00020542
Iteration 47/1000 | Loss: 0.00026062
Iteration 48/1000 | Loss: 0.00027642
Iteration 49/1000 | Loss: 0.00019482
Iteration 50/1000 | Loss: 0.00015825
Iteration 51/1000 | Loss: 0.00011961
Iteration 52/1000 | Loss: 0.00021626
Iteration 53/1000 | Loss: 0.00029963
Iteration 54/1000 | Loss: 0.00041301
Iteration 55/1000 | Loss: 0.00025188
Iteration 56/1000 | Loss: 0.00005844
Iteration 57/1000 | Loss: 0.00020494
Iteration 58/1000 | Loss: 0.00013045
Iteration 59/1000 | Loss: 0.00019703
Iteration 60/1000 | Loss: 0.00012540
Iteration 61/1000 | Loss: 0.00034285
Iteration 62/1000 | Loss: 0.00019917
Iteration 63/1000 | Loss: 0.00029704
Iteration 64/1000 | Loss: 0.00017341
Iteration 65/1000 | Loss: 0.00027291
Iteration 66/1000 | Loss: 0.00026085
Iteration 67/1000 | Loss: 0.00025008
Iteration 68/1000 | Loss: 0.00004264
Iteration 69/1000 | Loss: 0.00004049
Iteration 70/1000 | Loss: 0.00024984
Iteration 71/1000 | Loss: 0.00021142
Iteration 72/1000 | Loss: 0.00023107
Iteration 73/1000 | Loss: 0.00017634
Iteration 74/1000 | Loss: 0.00036163
Iteration 75/1000 | Loss: 0.00008001
Iteration 76/1000 | Loss: 0.00007019
Iteration 77/1000 | Loss: 0.00003824
Iteration 78/1000 | Loss: 0.00003594
Iteration 79/1000 | Loss: 0.00003468
Iteration 80/1000 | Loss: 0.00003360
Iteration 81/1000 | Loss: 0.00003281
Iteration 82/1000 | Loss: 0.00017452
Iteration 83/1000 | Loss: 0.00012062
Iteration 84/1000 | Loss: 0.00003260
Iteration 85/1000 | Loss: 0.00015996
Iteration 86/1000 | Loss: 0.00012627
Iteration 87/1000 | Loss: 0.00003308
Iteration 88/1000 | Loss: 0.00003206
Iteration 89/1000 | Loss: 0.00014939
Iteration 90/1000 | Loss: 0.00014346
Iteration 91/1000 | Loss: 0.00017337
Iteration 92/1000 | Loss: 0.00014183
Iteration 93/1000 | Loss: 0.00003302
Iteration 94/1000 | Loss: 0.00003200
Iteration 95/1000 | Loss: 0.00003153
Iteration 96/1000 | Loss: 0.00003118
Iteration 97/1000 | Loss: 0.00003093
Iteration 98/1000 | Loss: 0.00017085
Iteration 99/1000 | Loss: 0.00010002
Iteration 100/1000 | Loss: 0.00012452
Iteration 101/1000 | Loss: 0.00003307
Iteration 102/1000 | Loss: 0.00003225
Iteration 103/1000 | Loss: 0.00011948
Iteration 104/1000 | Loss: 0.00014498
Iteration 105/1000 | Loss: 0.00020354
Iteration 106/1000 | Loss: 0.00015258
Iteration 107/1000 | Loss: 0.00005052
Iteration 108/1000 | Loss: 0.00003108
Iteration 109/1000 | Loss: 0.00003065
Iteration 110/1000 | Loss: 0.00003057
Iteration 111/1000 | Loss: 0.00003057
Iteration 112/1000 | Loss: 0.00003056
Iteration 113/1000 | Loss: 0.00003055
Iteration 114/1000 | Loss: 0.00016022
Iteration 115/1000 | Loss: 0.00014632
Iteration 116/1000 | Loss: 0.00017233
Iteration 117/1000 | Loss: 0.00014064
Iteration 118/1000 | Loss: 0.00016407
Iteration 119/1000 | Loss: 0.00018167
Iteration 120/1000 | Loss: 0.00017448
Iteration 121/1000 | Loss: 0.00011311
Iteration 122/1000 | Loss: 0.00022715
Iteration 123/1000 | Loss: 0.00028464
Iteration 124/1000 | Loss: 0.00004440
Iteration 125/1000 | Loss: 0.00005939
Iteration 126/1000 | Loss: 0.00006802
Iteration 127/1000 | Loss: 0.00003435
Iteration 128/1000 | Loss: 0.00019998
Iteration 129/1000 | Loss: 0.00006238
Iteration 130/1000 | Loss: 0.00017910
Iteration 131/1000 | Loss: 0.00018507
Iteration 132/1000 | Loss: 0.00003530
Iteration 133/1000 | Loss: 0.00003406
Iteration 134/1000 | Loss: 0.00003236
Iteration 135/1000 | Loss: 0.00003120
Iteration 136/1000 | Loss: 0.00003086
Iteration 137/1000 | Loss: 0.00003058
Iteration 138/1000 | Loss: 0.00003035
Iteration 139/1000 | Loss: 0.00003015
Iteration 140/1000 | Loss: 0.00003001
Iteration 141/1000 | Loss: 0.00002986
Iteration 142/1000 | Loss: 0.00002985
Iteration 143/1000 | Loss: 0.00002983
Iteration 144/1000 | Loss: 0.00002982
Iteration 145/1000 | Loss: 0.00002982
Iteration 146/1000 | Loss: 0.00002982
Iteration 147/1000 | Loss: 0.00002982
Iteration 148/1000 | Loss: 0.00002981
Iteration 149/1000 | Loss: 0.00002981
Iteration 150/1000 | Loss: 0.00002981
Iteration 151/1000 | Loss: 0.00002981
Iteration 152/1000 | Loss: 0.00002981
Iteration 153/1000 | Loss: 0.00002980
Iteration 154/1000 | Loss: 0.00002979
Iteration 155/1000 | Loss: 0.00002979
Iteration 156/1000 | Loss: 0.00002978
Iteration 157/1000 | Loss: 0.00002978
Iteration 158/1000 | Loss: 0.00002978
Iteration 159/1000 | Loss: 0.00002977
Iteration 160/1000 | Loss: 0.00002977
Iteration 161/1000 | Loss: 0.00002975
Iteration 162/1000 | Loss: 0.00002975
Iteration 163/1000 | Loss: 0.00002975
Iteration 164/1000 | Loss: 0.00002974
Iteration 165/1000 | Loss: 0.00002974
Iteration 166/1000 | Loss: 0.00002971
Iteration 167/1000 | Loss: 0.00002971
Iteration 168/1000 | Loss: 0.00002971
Iteration 169/1000 | Loss: 0.00002971
Iteration 170/1000 | Loss: 0.00002971
Iteration 171/1000 | Loss: 0.00002971
Iteration 172/1000 | Loss: 0.00002971
Iteration 173/1000 | Loss: 0.00002970
Iteration 174/1000 | Loss: 0.00002970
Iteration 175/1000 | Loss: 0.00002970
Iteration 176/1000 | Loss: 0.00002970
Iteration 177/1000 | Loss: 0.00002970
Iteration 178/1000 | Loss: 0.00002970
Iteration 179/1000 | Loss: 0.00002970
Iteration 180/1000 | Loss: 0.00002970
Iteration 181/1000 | Loss: 0.00002969
Iteration 182/1000 | Loss: 0.00002969
Iteration 183/1000 | Loss: 0.00002969
Iteration 184/1000 | Loss: 0.00002969
Iteration 185/1000 | Loss: 0.00002969
Iteration 186/1000 | Loss: 0.00002969
Iteration 187/1000 | Loss: 0.00002969
Iteration 188/1000 | Loss: 0.00002969
Iteration 189/1000 | Loss: 0.00002969
Iteration 190/1000 | Loss: 0.00002969
Iteration 191/1000 | Loss: 0.00002969
Iteration 192/1000 | Loss: 0.00002969
Iteration 193/1000 | Loss: 0.00002969
Iteration 194/1000 | Loss: 0.00002969
Iteration 195/1000 | Loss: 0.00002969
Iteration 196/1000 | Loss: 0.00002969
Iteration 197/1000 | Loss: 0.00002968
Iteration 198/1000 | Loss: 0.00002968
Iteration 199/1000 | Loss: 0.00002968
Iteration 200/1000 | Loss: 0.00002968
Iteration 201/1000 | Loss: 0.00002968
Iteration 202/1000 | Loss: 0.00002967
Iteration 203/1000 | Loss: 0.00002967
Iteration 204/1000 | Loss: 0.00002967
Iteration 205/1000 | Loss: 0.00002967
Iteration 206/1000 | Loss: 0.00002967
Iteration 207/1000 | Loss: 0.00002967
Iteration 208/1000 | Loss: 0.00002967
Iteration 209/1000 | Loss: 0.00002967
Iteration 210/1000 | Loss: 0.00002967
Iteration 211/1000 | Loss: 0.00002967
Iteration 212/1000 | Loss: 0.00002966
Iteration 213/1000 | Loss: 0.00002966
Iteration 214/1000 | Loss: 0.00002966
Iteration 215/1000 | Loss: 0.00002965
Iteration 216/1000 | Loss: 0.00002965
Iteration 217/1000 | Loss: 0.00002965
Iteration 218/1000 | Loss: 0.00002965
Iteration 219/1000 | Loss: 0.00002965
Iteration 220/1000 | Loss: 0.00002965
Iteration 221/1000 | Loss: 0.00002964
Iteration 222/1000 | Loss: 0.00002964
Iteration 223/1000 | Loss: 0.00002964
Iteration 224/1000 | Loss: 0.00002964
Iteration 225/1000 | Loss: 0.00002964
Iteration 226/1000 | Loss: 0.00002964
Iteration 227/1000 | Loss: 0.00002964
Iteration 228/1000 | Loss: 0.00002964
Iteration 229/1000 | Loss: 0.00002964
Iteration 230/1000 | Loss: 0.00002964
Iteration 231/1000 | Loss: 0.00002964
Iteration 232/1000 | Loss: 0.00002964
Iteration 233/1000 | Loss: 0.00002964
Iteration 234/1000 | Loss: 0.00002964
Iteration 235/1000 | Loss: 0.00002964
Iteration 236/1000 | Loss: 0.00002964
Iteration 237/1000 | Loss: 0.00002964
Iteration 238/1000 | Loss: 0.00002964
Iteration 239/1000 | Loss: 0.00002964
Iteration 240/1000 | Loss: 0.00002964
Iteration 241/1000 | Loss: 0.00002964
Iteration 242/1000 | Loss: 0.00002964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [2.9637240004376508e-05, 2.9637240004376508e-05, 2.9637240004376508e-05, 2.9637240004376508e-05, 2.9637240004376508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9637240004376508e-05

Optimization complete. Final v2v error: 4.491246700286865 mm

Highest mean error: 5.861791610717773 mm for frame 36

Lowest mean error: 4.0567708015441895 mm for frame 238

Saving results

Total time: 285.1180167198181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838673
Iteration 2/25 | Loss: 0.00102729
Iteration 3/25 | Loss: 0.00086517
Iteration 4/25 | Loss: 0.00081230
Iteration 5/25 | Loss: 0.00080041
Iteration 6/25 | Loss: 0.00079734
Iteration 7/25 | Loss: 0.00079624
Iteration 8/25 | Loss: 0.00079624
Iteration 9/25 | Loss: 0.00079624
Iteration 10/25 | Loss: 0.00079624
Iteration 11/25 | Loss: 0.00079624
Iteration 12/25 | Loss: 0.00079624
Iteration 13/25 | Loss: 0.00079624
Iteration 14/25 | Loss: 0.00079624
Iteration 15/25 | Loss: 0.00079624
Iteration 16/25 | Loss: 0.00079624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000796243199147284, 0.000796243199147284, 0.000796243199147284, 0.000796243199147284, 0.000796243199147284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000796243199147284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78758538
Iteration 2/25 | Loss: 0.00137380
Iteration 3/25 | Loss: 0.00137369
Iteration 4/25 | Loss: 0.00137369
Iteration 5/25 | Loss: 0.00137369
Iteration 6/25 | Loss: 0.00137369
Iteration 7/25 | Loss: 0.00137369
Iteration 8/25 | Loss: 0.00137369
Iteration 9/25 | Loss: 0.00137369
Iteration 10/25 | Loss: 0.00137369
Iteration 11/25 | Loss: 0.00137369
Iteration 12/25 | Loss: 0.00137369
Iteration 13/25 | Loss: 0.00137369
Iteration 14/25 | Loss: 0.00137369
Iteration 15/25 | Loss: 0.00137369
Iteration 16/25 | Loss: 0.00137369
Iteration 17/25 | Loss: 0.00137369
Iteration 18/25 | Loss: 0.00137369
Iteration 19/25 | Loss: 0.00137369
Iteration 20/25 | Loss: 0.00137369
Iteration 21/25 | Loss: 0.00137369
Iteration 22/25 | Loss: 0.00137369
Iteration 23/25 | Loss: 0.00137369
Iteration 24/25 | Loss: 0.00137369
Iteration 25/25 | Loss: 0.00137369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137369
Iteration 2/1000 | Loss: 0.00004592
Iteration 3/1000 | Loss: 0.00003224
Iteration 4/1000 | Loss: 0.00002701
Iteration 5/1000 | Loss: 0.00002397
Iteration 6/1000 | Loss: 0.00002228
Iteration 7/1000 | Loss: 0.00002133
Iteration 8/1000 | Loss: 0.00002046
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001956
Iteration 11/1000 | Loss: 0.00001927
Iteration 12/1000 | Loss: 0.00001898
Iteration 13/1000 | Loss: 0.00001875
Iteration 14/1000 | Loss: 0.00001854
Iteration 15/1000 | Loss: 0.00001852
Iteration 16/1000 | Loss: 0.00001838
Iteration 17/1000 | Loss: 0.00001826
Iteration 18/1000 | Loss: 0.00001817
Iteration 19/1000 | Loss: 0.00001807
Iteration 20/1000 | Loss: 0.00001801
Iteration 21/1000 | Loss: 0.00001796
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00001794
Iteration 26/1000 | Loss: 0.00001794
Iteration 27/1000 | Loss: 0.00001793
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001788
Iteration 34/1000 | Loss: 0.00001788
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001783
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001782
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001781
Iteration 48/1000 | Loss: 0.00001781
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001773
Iteration 88/1000 | Loss: 0.00001773
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001772
Iteration 92/1000 | Loss: 0.00001772
Iteration 93/1000 | Loss: 0.00001772
Iteration 94/1000 | Loss: 0.00001772
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001771
Iteration 113/1000 | Loss: 0.00001771
Iteration 114/1000 | Loss: 0.00001771
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001770
Iteration 118/1000 | Loss: 0.00001770
Iteration 119/1000 | Loss: 0.00001770
Iteration 120/1000 | Loss: 0.00001770
Iteration 121/1000 | Loss: 0.00001770
Iteration 122/1000 | Loss: 0.00001770
Iteration 123/1000 | Loss: 0.00001770
Iteration 124/1000 | Loss: 0.00001770
Iteration 125/1000 | Loss: 0.00001770
Iteration 126/1000 | Loss: 0.00001770
Iteration 127/1000 | Loss: 0.00001769
Iteration 128/1000 | Loss: 0.00001769
Iteration 129/1000 | Loss: 0.00001769
Iteration 130/1000 | Loss: 0.00001769
Iteration 131/1000 | Loss: 0.00001769
Iteration 132/1000 | Loss: 0.00001769
Iteration 133/1000 | Loss: 0.00001769
Iteration 134/1000 | Loss: 0.00001769
Iteration 135/1000 | Loss: 0.00001769
Iteration 136/1000 | Loss: 0.00001769
Iteration 137/1000 | Loss: 0.00001769
Iteration 138/1000 | Loss: 0.00001769
Iteration 139/1000 | Loss: 0.00001769
Iteration 140/1000 | Loss: 0.00001769
Iteration 141/1000 | Loss: 0.00001769
Iteration 142/1000 | Loss: 0.00001769
Iteration 143/1000 | Loss: 0.00001769
Iteration 144/1000 | Loss: 0.00001769
Iteration 145/1000 | Loss: 0.00001769
Iteration 146/1000 | Loss: 0.00001769
Iteration 147/1000 | Loss: 0.00001769
Iteration 148/1000 | Loss: 0.00001769
Iteration 149/1000 | Loss: 0.00001769
Iteration 150/1000 | Loss: 0.00001769
Iteration 151/1000 | Loss: 0.00001769
Iteration 152/1000 | Loss: 0.00001769
Iteration 153/1000 | Loss: 0.00001769
Iteration 154/1000 | Loss: 0.00001769
Iteration 155/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7692957044346258e-05, 1.7692957044346258e-05, 1.7692957044346258e-05, 1.7692957044346258e-05, 1.7692957044346258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7692957044346258e-05

Optimization complete. Final v2v error: 3.555957317352295 mm

Highest mean error: 3.865156412124634 mm for frame 124

Lowest mean error: 3.135403633117676 mm for frame 183

Saving results

Total time: 47.25710368156433
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110192
Iteration 2/25 | Loss: 0.00272553
Iteration 3/25 | Loss: 0.00252901
Iteration 4/25 | Loss: 0.00202901
Iteration 5/25 | Loss: 0.00164332
Iteration 6/25 | Loss: 0.00139229
Iteration 7/25 | Loss: 0.00125414
Iteration 8/25 | Loss: 0.00111680
Iteration 9/25 | Loss: 0.00105978
Iteration 10/25 | Loss: 0.00101562
Iteration 11/25 | Loss: 0.00100175
Iteration 12/25 | Loss: 0.00095550
Iteration 13/25 | Loss: 0.00094029
Iteration 14/25 | Loss: 0.00091669
Iteration 15/25 | Loss: 0.00091066
Iteration 16/25 | Loss: 0.00090840
Iteration 17/25 | Loss: 0.00090997
Iteration 18/25 | Loss: 0.00090971
Iteration 19/25 | Loss: 0.00090825
Iteration 20/25 | Loss: 0.00091489
Iteration 21/25 | Loss: 0.00090540
Iteration 22/25 | Loss: 0.00090270
Iteration 23/25 | Loss: 0.00088985
Iteration 24/25 | Loss: 0.00087302
Iteration 25/25 | Loss: 0.00086656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60873914
Iteration 2/25 | Loss: 0.00144182
Iteration 3/25 | Loss: 0.00135466
Iteration 4/25 | Loss: 0.00135466
Iteration 5/25 | Loss: 0.00135466
Iteration 6/25 | Loss: 0.00135466
Iteration 7/25 | Loss: 0.00135466
Iteration 8/25 | Loss: 0.00135466
Iteration 9/25 | Loss: 0.00135466
Iteration 10/25 | Loss: 0.00135466
Iteration 11/25 | Loss: 0.00135466
Iteration 12/25 | Loss: 0.00135466
Iteration 13/25 | Loss: 0.00135466
Iteration 14/25 | Loss: 0.00135466
Iteration 15/25 | Loss: 0.00135466
Iteration 16/25 | Loss: 0.00135466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013546599075198174, 0.0013546599075198174, 0.0013546599075198174, 0.0013546599075198174, 0.0013546599075198174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013546599075198174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135466
Iteration 2/1000 | Loss: 0.00021897
Iteration 3/1000 | Loss: 0.00048466
Iteration 4/1000 | Loss: 0.00004223
Iteration 5/1000 | Loss: 0.00049265
Iteration 6/1000 | Loss: 0.00069159
Iteration 7/1000 | Loss: 0.00003311
Iteration 8/1000 | Loss: 0.00003003
Iteration 9/1000 | Loss: 0.00004531
Iteration 10/1000 | Loss: 0.00004464
Iteration 11/1000 | Loss: 0.00004383
Iteration 12/1000 | Loss: 0.00004085
Iteration 13/1000 | Loss: 0.00004251
Iteration 14/1000 | Loss: 0.00075383
Iteration 15/1000 | Loss: 0.00081715
Iteration 16/1000 | Loss: 0.00052771
Iteration 17/1000 | Loss: 0.00005988
Iteration 18/1000 | Loss: 0.00003781
Iteration 19/1000 | Loss: 0.00004893
Iteration 20/1000 | Loss: 0.00012650
Iteration 21/1000 | Loss: 0.00083840
Iteration 22/1000 | Loss: 0.00042583
Iteration 23/1000 | Loss: 0.00014059
Iteration 24/1000 | Loss: 0.00008040
Iteration 25/1000 | Loss: 0.00035729
Iteration 26/1000 | Loss: 0.00003228
Iteration 27/1000 | Loss: 0.00002697
Iteration 28/1000 | Loss: 0.00002417
Iteration 29/1000 | Loss: 0.00099172
Iteration 30/1000 | Loss: 0.00026354
Iteration 31/1000 | Loss: 0.00026989
Iteration 32/1000 | Loss: 0.00005743
Iteration 33/1000 | Loss: 0.00006812
Iteration 34/1000 | Loss: 0.00005466
Iteration 35/1000 | Loss: 0.00006430
Iteration 36/1000 | Loss: 0.00002648
Iteration 37/1000 | Loss: 0.00002202
Iteration 38/1000 | Loss: 0.00002005
Iteration 39/1000 | Loss: 0.00001871
Iteration 40/1000 | Loss: 0.00001807
Iteration 41/1000 | Loss: 0.00001772
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001732
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001726
Iteration 47/1000 | Loss: 0.00001726
Iteration 48/1000 | Loss: 0.00001725
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001722
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001718
Iteration 59/1000 | Loss: 0.00001718
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001709
Iteration 73/1000 | Loss: 0.00001709
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001707
Iteration 80/1000 | Loss: 0.00001707
Iteration 81/1000 | Loss: 0.00001707
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001706
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001706
Iteration 87/1000 | Loss: 0.00001706
Iteration 88/1000 | Loss: 0.00001706
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001704
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001703
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001699
Iteration 101/1000 | Loss: 0.00001698
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001695
Iteration 117/1000 | Loss: 0.00001695
Iteration 118/1000 | Loss: 0.00001695
Iteration 119/1000 | Loss: 0.00001695
Iteration 120/1000 | Loss: 0.00001695
Iteration 121/1000 | Loss: 0.00001695
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001694
Iteration 125/1000 | Loss: 0.00001694
Iteration 126/1000 | Loss: 0.00001694
Iteration 127/1000 | Loss: 0.00001694
Iteration 128/1000 | Loss: 0.00001694
Iteration 129/1000 | Loss: 0.00001694
Iteration 130/1000 | Loss: 0.00001694
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001694
Iteration 134/1000 | Loss: 0.00001694
Iteration 135/1000 | Loss: 0.00001694
Iteration 136/1000 | Loss: 0.00001694
Iteration 137/1000 | Loss: 0.00001694
Iteration 138/1000 | Loss: 0.00001694
Iteration 139/1000 | Loss: 0.00001694
Iteration 140/1000 | Loss: 0.00001694
Iteration 141/1000 | Loss: 0.00001694
Iteration 142/1000 | Loss: 0.00001694
Iteration 143/1000 | Loss: 0.00001694
Iteration 144/1000 | Loss: 0.00001694
Iteration 145/1000 | Loss: 0.00001694
Iteration 146/1000 | Loss: 0.00001694
Iteration 147/1000 | Loss: 0.00001694
Iteration 148/1000 | Loss: 0.00001694
Iteration 149/1000 | Loss: 0.00001694
Iteration 150/1000 | Loss: 0.00001694
Iteration 151/1000 | Loss: 0.00001694
Iteration 152/1000 | Loss: 0.00001694
Iteration 153/1000 | Loss: 0.00001694
Iteration 154/1000 | Loss: 0.00001694
Iteration 155/1000 | Loss: 0.00001694
Iteration 156/1000 | Loss: 0.00001694
Iteration 157/1000 | Loss: 0.00001694
Iteration 158/1000 | Loss: 0.00001694
Iteration 159/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.694313505140599e-05, 1.694313505140599e-05, 1.694313505140599e-05, 1.694313505140599e-05, 1.694313505140599e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.694313505140599e-05

Optimization complete. Final v2v error: 3.4285714626312256 mm

Highest mean error: 4.430979251861572 mm for frame 112

Lowest mean error: 2.853726387023926 mm for frame 126

Saving results

Total time: 116.19107413291931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708913
Iteration 2/25 | Loss: 0.00155749
Iteration 3/25 | Loss: 0.00109434
Iteration 4/25 | Loss: 0.00101201
Iteration 5/25 | Loss: 0.00093466
Iteration 6/25 | Loss: 0.00088770
Iteration 7/25 | Loss: 0.00088039
Iteration 8/25 | Loss: 0.00087682
Iteration 9/25 | Loss: 0.00088312
Iteration 10/25 | Loss: 0.00087610
Iteration 11/25 | Loss: 0.00087258
Iteration 12/25 | Loss: 0.00086608
Iteration 13/25 | Loss: 0.00086940
Iteration 14/25 | Loss: 0.00086715
Iteration 15/25 | Loss: 0.00085772
Iteration 16/25 | Loss: 0.00085536
Iteration 17/25 | Loss: 0.00085745
Iteration 18/25 | Loss: 0.00085600
Iteration 19/25 | Loss: 0.00085451
Iteration 20/25 | Loss: 0.00085234
Iteration 21/25 | Loss: 0.00085349
Iteration 22/25 | Loss: 0.00085098
Iteration 23/25 | Loss: 0.00085017
Iteration 24/25 | Loss: 0.00084902
Iteration 25/25 | Loss: 0.00084878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47489405
Iteration 2/25 | Loss: 0.00130028
Iteration 3/25 | Loss: 0.00130023
Iteration 4/25 | Loss: 0.00130023
Iteration 5/25 | Loss: 0.00130023
Iteration 6/25 | Loss: 0.00130023
Iteration 7/25 | Loss: 0.00130023
Iteration 8/25 | Loss: 0.00130023
Iteration 9/25 | Loss: 0.00130023
Iteration 10/25 | Loss: 0.00130023
Iteration 11/25 | Loss: 0.00130023
Iteration 12/25 | Loss: 0.00130023
Iteration 13/25 | Loss: 0.00130023
Iteration 14/25 | Loss: 0.00130023
Iteration 15/25 | Loss: 0.00130023
Iteration 16/25 | Loss: 0.00130023
Iteration 17/25 | Loss: 0.00130023
Iteration 18/25 | Loss: 0.00130023
Iteration 19/25 | Loss: 0.00130023
Iteration 20/25 | Loss: 0.00130023
Iteration 21/25 | Loss: 0.00130023
Iteration 22/25 | Loss: 0.00130023
Iteration 23/25 | Loss: 0.00130023
Iteration 24/25 | Loss: 0.00130023
Iteration 25/25 | Loss: 0.00130023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130023
Iteration 2/1000 | Loss: 0.00003829
Iteration 3/1000 | Loss: 0.00002808
Iteration 4/1000 | Loss: 0.00002428
Iteration 5/1000 | Loss: 0.00002244
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00015586
Iteration 10/1000 | Loss: 0.00002142
Iteration 11/1000 | Loss: 0.00001960
Iteration 12/1000 | Loss: 0.00001854
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001710
Iteration 18/1000 | Loss: 0.00001709
Iteration 19/1000 | Loss: 0.00001708
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001698
Iteration 22/1000 | Loss: 0.00001698
Iteration 23/1000 | Loss: 0.00001696
Iteration 24/1000 | Loss: 0.00001695
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001690
Iteration 30/1000 | Loss: 0.00001689
Iteration 31/1000 | Loss: 0.00001689
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001688
Iteration 36/1000 | Loss: 0.00001688
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001687
Iteration 39/1000 | Loss: 0.00001686
Iteration 40/1000 | Loss: 0.00001686
Iteration 41/1000 | Loss: 0.00001686
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001683
Iteration 45/1000 | Loss: 0.00001681
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001680
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001678
Iteration 51/1000 | Loss: 0.00001678
Iteration 52/1000 | Loss: 0.00001678
Iteration 53/1000 | Loss: 0.00001678
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001677
Iteration 58/1000 | Loss: 0.00001677
Iteration 59/1000 | Loss: 0.00001677
Iteration 60/1000 | Loss: 0.00001677
Iteration 61/1000 | Loss: 0.00001677
Iteration 62/1000 | Loss: 0.00001677
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001674
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001672
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001671
Iteration 87/1000 | Loss: 0.00001671
Iteration 88/1000 | Loss: 0.00001671
Iteration 89/1000 | Loss: 0.00001671
Iteration 90/1000 | Loss: 0.00001671
Iteration 91/1000 | Loss: 0.00001671
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001670
Iteration 95/1000 | Loss: 0.00001670
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001670
Iteration 98/1000 | Loss: 0.00001670
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001669
Iteration 103/1000 | Loss: 0.00001669
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001669
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001669
Iteration 111/1000 | Loss: 0.00001669
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001668
Iteration 114/1000 | Loss: 0.00001668
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001668
Iteration 117/1000 | Loss: 0.00001668
Iteration 118/1000 | Loss: 0.00001667
Iteration 119/1000 | Loss: 0.00001667
Iteration 120/1000 | Loss: 0.00001667
Iteration 121/1000 | Loss: 0.00001667
Iteration 122/1000 | Loss: 0.00001667
Iteration 123/1000 | Loss: 0.00001667
Iteration 124/1000 | Loss: 0.00001667
Iteration 125/1000 | Loss: 0.00001667
Iteration 126/1000 | Loss: 0.00001667
Iteration 127/1000 | Loss: 0.00001667
Iteration 128/1000 | Loss: 0.00001667
Iteration 129/1000 | Loss: 0.00001667
Iteration 130/1000 | Loss: 0.00001667
Iteration 131/1000 | Loss: 0.00001667
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.6670375771354884e-05, 1.6670375771354884e-05, 1.6670375771354884e-05, 1.6670375771354884e-05, 1.6670375771354884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6670375771354884e-05

Optimization complete. Final v2v error: 3.4685134887695312 mm

Highest mean error: 4.0688934326171875 mm for frame 18

Lowest mean error: 2.970362424850464 mm for frame 132

Saving results

Total time: 80.0464379787445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091598
Iteration 2/25 | Loss: 0.00200214
Iteration 3/25 | Loss: 0.00136823
Iteration 4/25 | Loss: 0.00178926
Iteration 5/25 | Loss: 0.00098311
Iteration 6/25 | Loss: 0.00090978
Iteration 7/25 | Loss: 0.00096999
Iteration 8/25 | Loss: 0.00083005
Iteration 9/25 | Loss: 0.00079825
Iteration 10/25 | Loss: 0.00077924
Iteration 11/25 | Loss: 0.00077092
Iteration 12/25 | Loss: 0.00076945
Iteration 13/25 | Loss: 0.00076389
Iteration 14/25 | Loss: 0.00076485
Iteration 15/25 | Loss: 0.00076515
Iteration 16/25 | Loss: 0.00075992
Iteration 17/25 | Loss: 0.00075686
Iteration 18/25 | Loss: 0.00075661
Iteration 19/25 | Loss: 0.00075513
Iteration 20/25 | Loss: 0.00075610
Iteration 21/25 | Loss: 0.00075387
Iteration 22/25 | Loss: 0.00075239
Iteration 23/25 | Loss: 0.00075490
Iteration 24/25 | Loss: 0.00074895
Iteration 25/25 | Loss: 0.00074477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60308850
Iteration 2/25 | Loss: 0.00138761
Iteration 3/25 | Loss: 0.00130475
Iteration 4/25 | Loss: 0.00130475
Iteration 5/25 | Loss: 0.00130475
Iteration 6/25 | Loss: 0.00130475
Iteration 7/25 | Loss: 0.00130475
Iteration 8/25 | Loss: 0.00130475
Iteration 9/25 | Loss: 0.00130475
Iteration 10/25 | Loss: 0.00130475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013047470711171627, 0.0013047470711171627, 0.0013047470711171627, 0.0013047470711171627, 0.0013047470711171627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013047470711171627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130475
Iteration 2/1000 | Loss: 0.00007941
Iteration 3/1000 | Loss: 0.00006830
Iteration 4/1000 | Loss: 0.00004922
Iteration 5/1000 | Loss: 0.00007492
Iteration 6/1000 | Loss: 0.00048372
Iteration 7/1000 | Loss: 0.00033470
Iteration 8/1000 | Loss: 0.00030562
Iteration 9/1000 | Loss: 0.00017673
Iteration 10/1000 | Loss: 0.00016198
Iteration 11/1000 | Loss: 0.00021449
Iteration 12/1000 | Loss: 0.00006823
Iteration 13/1000 | Loss: 0.00003175
Iteration 14/1000 | Loss: 0.00002766
Iteration 15/1000 | Loss: 0.00003721
Iteration 16/1000 | Loss: 0.00003816
Iteration 17/1000 | Loss: 0.00002513
Iteration 18/1000 | Loss: 0.00028216
Iteration 19/1000 | Loss: 0.00042254
Iteration 20/1000 | Loss: 0.00039300
Iteration 21/1000 | Loss: 0.00039972
Iteration 22/1000 | Loss: 0.00038862
Iteration 23/1000 | Loss: 0.00037505
Iteration 24/1000 | Loss: 0.00033599
Iteration 25/1000 | Loss: 0.00021832
Iteration 26/1000 | Loss: 0.00019595
Iteration 27/1000 | Loss: 0.00012068
Iteration 28/1000 | Loss: 0.00003642
Iteration 29/1000 | Loss: 0.00003733
Iteration 30/1000 | Loss: 0.00002394
Iteration 31/1000 | Loss: 0.00004726
Iteration 32/1000 | Loss: 0.00001747
Iteration 33/1000 | Loss: 0.00003146
Iteration 34/1000 | Loss: 0.00002211
Iteration 35/1000 | Loss: 0.00026870
Iteration 36/1000 | Loss: 0.00001982
Iteration 37/1000 | Loss: 0.00023529
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002013
Iteration 40/1000 | Loss: 0.00001359
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001270
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00002054
Iteration 47/1000 | Loss: 0.00001542
Iteration 48/1000 | Loss: 0.00001324
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00002884
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001140
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00002264
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001132
Iteration 67/1000 | Loss: 0.00001132
Iteration 68/1000 | Loss: 0.00001132
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001129
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001126
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001125
Iteration 94/1000 | Loss: 0.00001125
Iteration 95/1000 | Loss: 0.00001125
Iteration 96/1000 | Loss: 0.00001124
Iteration 97/1000 | Loss: 0.00001124
Iteration 98/1000 | Loss: 0.00001123
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001122
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001122
Iteration 104/1000 | Loss: 0.00001122
Iteration 105/1000 | Loss: 0.00001122
Iteration 106/1000 | Loss: 0.00001122
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001121
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001121
Iteration 121/1000 | Loss: 0.00001121
Iteration 122/1000 | Loss: 0.00001121
Iteration 123/1000 | Loss: 0.00001121
Iteration 124/1000 | Loss: 0.00001121
Iteration 125/1000 | Loss: 0.00001121
Iteration 126/1000 | Loss: 0.00001121
Iteration 127/1000 | Loss: 0.00001121
Iteration 128/1000 | Loss: 0.00001121
Iteration 129/1000 | Loss: 0.00001121
Iteration 130/1000 | Loss: 0.00001121
Iteration 131/1000 | Loss: 0.00001121
Iteration 132/1000 | Loss: 0.00001121
Iteration 133/1000 | Loss: 0.00001121
Iteration 134/1000 | Loss: 0.00001121
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001121
Iteration 138/1000 | Loss: 0.00001121
Iteration 139/1000 | Loss: 0.00001121
Iteration 140/1000 | Loss: 0.00001121
Iteration 141/1000 | Loss: 0.00001121
Iteration 142/1000 | Loss: 0.00001121
Iteration 143/1000 | Loss: 0.00001121
Iteration 144/1000 | Loss: 0.00001121
Iteration 145/1000 | Loss: 0.00001121
Iteration 146/1000 | Loss: 0.00001121
Iteration 147/1000 | Loss: 0.00001121
Iteration 148/1000 | Loss: 0.00001121
Iteration 149/1000 | Loss: 0.00001121
Iteration 150/1000 | Loss: 0.00001121
Iteration 151/1000 | Loss: 0.00001121
Iteration 152/1000 | Loss: 0.00001121
Iteration 153/1000 | Loss: 0.00001121
Iteration 154/1000 | Loss: 0.00001121
Iteration 155/1000 | Loss: 0.00001121
Iteration 156/1000 | Loss: 0.00001121
Iteration 157/1000 | Loss: 0.00001121
Iteration 158/1000 | Loss: 0.00001121
Iteration 159/1000 | Loss: 0.00001121
Iteration 160/1000 | Loss: 0.00001121
Iteration 161/1000 | Loss: 0.00001121
Iteration 162/1000 | Loss: 0.00001121
Iteration 163/1000 | Loss: 0.00001121
Iteration 164/1000 | Loss: 0.00001121
Iteration 165/1000 | Loss: 0.00001121
Iteration 166/1000 | Loss: 0.00001121
Iteration 167/1000 | Loss: 0.00001121
Iteration 168/1000 | Loss: 0.00001121
Iteration 169/1000 | Loss: 0.00001121
Iteration 170/1000 | Loss: 0.00001121
Iteration 171/1000 | Loss: 0.00001121
Iteration 172/1000 | Loss: 0.00001121
Iteration 173/1000 | Loss: 0.00001121
Iteration 174/1000 | Loss: 0.00001121
Iteration 175/1000 | Loss: 0.00001121
Iteration 176/1000 | Loss: 0.00001121
Iteration 177/1000 | Loss: 0.00001121
Iteration 178/1000 | Loss: 0.00001121
Iteration 179/1000 | Loss: 0.00001121
Iteration 180/1000 | Loss: 0.00001121
Iteration 181/1000 | Loss: 0.00001121
Iteration 182/1000 | Loss: 0.00001121
Iteration 183/1000 | Loss: 0.00001121
Iteration 184/1000 | Loss: 0.00001121
Iteration 185/1000 | Loss: 0.00001121
Iteration 186/1000 | Loss: 0.00001121
Iteration 187/1000 | Loss: 0.00001121
Iteration 188/1000 | Loss: 0.00001121
Iteration 189/1000 | Loss: 0.00001121
Iteration 190/1000 | Loss: 0.00001121
Iteration 191/1000 | Loss: 0.00001121
Iteration 192/1000 | Loss: 0.00001121
Iteration 193/1000 | Loss: 0.00001121
Iteration 194/1000 | Loss: 0.00001121
Iteration 195/1000 | Loss: 0.00001121
Iteration 196/1000 | Loss: 0.00001121
Iteration 197/1000 | Loss: 0.00001121
Iteration 198/1000 | Loss: 0.00001121
Iteration 199/1000 | Loss: 0.00001121
Iteration 200/1000 | Loss: 0.00001121
Iteration 201/1000 | Loss: 0.00001121
Iteration 202/1000 | Loss: 0.00001121
Iteration 203/1000 | Loss: 0.00001121
Iteration 204/1000 | Loss: 0.00001121
Iteration 205/1000 | Loss: 0.00001121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1207179340999573e-05, 1.1207179340999573e-05, 1.1207179340999573e-05, 1.1207179340999573e-05, 1.1207179340999573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1207179340999573e-05

Optimization complete. Final v2v error: 2.8110313415527344 mm

Highest mean error: 5.0453104972839355 mm for frame 72

Lowest mean error: 2.376953363418579 mm for frame 137

Saving results

Total time: 127.22763681411743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026228
Iteration 2/25 | Loss: 0.00516861
Iteration 3/25 | Loss: 0.00285385
Iteration 4/25 | Loss: 0.00257148
Iteration 5/25 | Loss: 0.00199520
Iteration 6/25 | Loss: 0.00196593
Iteration 7/25 | Loss: 0.00183217
Iteration 8/25 | Loss: 0.00174510
Iteration 9/25 | Loss: 0.00172170
Iteration 10/25 | Loss: 0.00165266
Iteration 11/25 | Loss: 0.00181238
Iteration 12/25 | Loss: 0.00182056
Iteration 13/25 | Loss: 0.00143935
Iteration 14/25 | Loss: 0.00135853
Iteration 15/25 | Loss: 0.00134011
Iteration 16/25 | Loss: 0.00131915
Iteration 17/25 | Loss: 0.00130346
Iteration 18/25 | Loss: 0.00127488
Iteration 19/25 | Loss: 0.00127858
Iteration 20/25 | Loss: 0.00126682
Iteration 21/25 | Loss: 0.00125973
Iteration 22/25 | Loss: 0.00126696
Iteration 23/25 | Loss: 0.00125852
Iteration 24/25 | Loss: 0.00125948
Iteration 25/25 | Loss: 0.00125539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58806932
Iteration 2/25 | Loss: 0.00689493
Iteration 3/25 | Loss: 0.00315006
Iteration 4/25 | Loss: 0.00315006
Iteration 5/25 | Loss: 0.00315005
Iteration 6/25 | Loss: 0.00315005
Iteration 7/25 | Loss: 0.00315005
Iteration 8/25 | Loss: 0.00315005
Iteration 9/25 | Loss: 0.00315005
Iteration 10/25 | Loss: 0.00315005
Iteration 11/25 | Loss: 0.00315005
Iteration 12/25 | Loss: 0.00315005
Iteration 13/25 | Loss: 0.00315005
Iteration 14/25 | Loss: 0.00315005
Iteration 15/25 | Loss: 0.00315005
Iteration 16/25 | Loss: 0.00315005
Iteration 17/25 | Loss: 0.00315005
Iteration 18/25 | Loss: 0.00315005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003150052623823285, 0.003150052623823285, 0.003150052623823285, 0.003150052623823285, 0.003150052623823285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003150052623823285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315005
Iteration 2/1000 | Loss: 0.00605961
Iteration 3/1000 | Loss: 0.00039558
Iteration 4/1000 | Loss: 0.00036380
Iteration 5/1000 | Loss: 0.00187849
Iteration 6/1000 | Loss: 0.00135169
Iteration 7/1000 | Loss: 0.00075177
Iteration 8/1000 | Loss: 0.00024180
Iteration 9/1000 | Loss: 0.00022902
Iteration 10/1000 | Loss: 0.00581921
Iteration 11/1000 | Loss: 0.00406027
Iteration 12/1000 | Loss: 0.00678525
Iteration 13/1000 | Loss: 0.00424131
Iteration 14/1000 | Loss: 0.00515104
Iteration 15/1000 | Loss: 0.00434533
Iteration 16/1000 | Loss: 0.00072945
Iteration 17/1000 | Loss: 0.00022003
Iteration 18/1000 | Loss: 0.00017263
Iteration 19/1000 | Loss: 0.00028209
Iteration 20/1000 | Loss: 0.00079558
Iteration 21/1000 | Loss: 0.00025235
Iteration 22/1000 | Loss: 0.00006739
Iteration 23/1000 | Loss: 0.00017672
Iteration 24/1000 | Loss: 0.00003090
Iteration 25/1000 | Loss: 0.00002565
Iteration 26/1000 | Loss: 0.00002173
Iteration 27/1000 | Loss: 0.00001876
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00018105
Iteration 30/1000 | Loss: 0.00008608
Iteration 31/1000 | Loss: 0.00003137
Iteration 32/1000 | Loss: 0.00001397
Iteration 33/1000 | Loss: 0.00012246
Iteration 34/1000 | Loss: 0.00006194
Iteration 35/1000 | Loss: 0.00001283
Iteration 36/1000 | Loss: 0.00005352
Iteration 37/1000 | Loss: 0.00001258
Iteration 38/1000 | Loss: 0.00003416
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00012757
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00009135
Iteration 43/1000 | Loss: 0.00001191
Iteration 44/1000 | Loss: 0.00012799
Iteration 45/1000 | Loss: 0.00001188
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001163
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001162
Iteration 88/1000 | Loss: 0.00001162
Iteration 89/1000 | Loss: 0.00001162
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001161
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001160
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001160
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.159182284027338e-05, 1.159182284027338e-05, 1.159182284027338e-05, 1.159182284027338e-05, 1.159182284027338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.159182284027338e-05

Optimization complete. Final v2v error: 2.880521774291992 mm

Highest mean error: 3.024951696395874 mm for frame 94

Lowest mean error: 2.7743449211120605 mm for frame 108

Saving results

Total time: 113.3373076915741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032352
Iteration 2/25 | Loss: 0.00263303
Iteration 3/25 | Loss: 0.00208794
Iteration 4/25 | Loss: 0.00217289
Iteration 5/25 | Loss: 0.00213916
Iteration 6/25 | Loss: 0.00197678
Iteration 7/25 | Loss: 0.00173722
Iteration 8/25 | Loss: 0.00154971
Iteration 9/25 | Loss: 0.00150457
Iteration 10/25 | Loss: 0.00153006
Iteration 11/25 | Loss: 0.00141095
Iteration 12/25 | Loss: 0.00137551
Iteration 13/25 | Loss: 0.00138861
Iteration 14/25 | Loss: 0.00137703
Iteration 15/25 | Loss: 0.00130020
Iteration 16/25 | Loss: 0.00126774
Iteration 17/25 | Loss: 0.00122729
Iteration 18/25 | Loss: 0.00120328
Iteration 19/25 | Loss: 0.00119572
Iteration 20/25 | Loss: 0.00118549
Iteration 21/25 | Loss: 0.00120483
Iteration 22/25 | Loss: 0.00120421
Iteration 23/25 | Loss: 0.00119314
Iteration 24/25 | Loss: 0.00116945
Iteration 25/25 | Loss: 0.00116889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55994511
Iteration 2/25 | Loss: 0.00389879
Iteration 3/25 | Loss: 0.00389879
Iteration 4/25 | Loss: 0.00389879
Iteration 5/25 | Loss: 0.00389879
Iteration 6/25 | Loss: 0.00389879
Iteration 7/25 | Loss: 0.00389879
Iteration 8/25 | Loss: 0.00389879
Iteration 9/25 | Loss: 0.00389879
Iteration 10/25 | Loss: 0.00389879
Iteration 11/25 | Loss: 0.00389879
Iteration 12/25 | Loss: 0.00389879
Iteration 13/25 | Loss: 0.00389879
Iteration 14/25 | Loss: 0.00389879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003898786613717675, 0.003898786613717675, 0.003898786613717675, 0.003898786613717675, 0.003898786613717675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003898786613717675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00389879
Iteration 2/1000 | Loss: 0.00402361
Iteration 3/1000 | Loss: 0.00294787
Iteration 4/1000 | Loss: 0.00215859
Iteration 5/1000 | Loss: 0.00079511
Iteration 6/1000 | Loss: 0.00067965
Iteration 7/1000 | Loss: 0.00093074
Iteration 8/1000 | Loss: 0.00229611
Iteration 9/1000 | Loss: 0.00119059
Iteration 10/1000 | Loss: 0.00121985
Iteration 11/1000 | Loss: 0.00058130
Iteration 12/1000 | Loss: 0.00114823
Iteration 13/1000 | Loss: 0.00021173
Iteration 14/1000 | Loss: 0.00015790
Iteration 15/1000 | Loss: 0.00011616
Iteration 16/1000 | Loss: 0.00009594
Iteration 17/1000 | Loss: 0.00008443
Iteration 18/1000 | Loss: 0.00014407
Iteration 19/1000 | Loss: 0.00006849
Iteration 20/1000 | Loss: 0.00006341
Iteration 21/1000 | Loss: 0.00018197
Iteration 22/1000 | Loss: 0.00017382
Iteration 23/1000 | Loss: 0.00016863
Iteration 24/1000 | Loss: 0.00032233
Iteration 25/1000 | Loss: 0.00024093
Iteration 26/1000 | Loss: 0.00018923
Iteration 27/1000 | Loss: 0.00006592
Iteration 28/1000 | Loss: 0.00020127
Iteration 29/1000 | Loss: 0.00010664
Iteration 30/1000 | Loss: 0.00013076
Iteration 31/1000 | Loss: 0.00009737
Iteration 32/1000 | Loss: 0.00010388
Iteration 33/1000 | Loss: 0.00017578
Iteration 34/1000 | Loss: 0.00005053
Iteration 35/1000 | Loss: 0.00004908
Iteration 36/1000 | Loss: 0.00004807
Iteration 37/1000 | Loss: 0.00004729
Iteration 38/1000 | Loss: 0.00004670
Iteration 39/1000 | Loss: 0.00004616
Iteration 40/1000 | Loss: 0.00019139
Iteration 41/1000 | Loss: 0.00019383
Iteration 42/1000 | Loss: 0.00005890
Iteration 43/1000 | Loss: 0.00005061
Iteration 44/1000 | Loss: 0.00004748
Iteration 45/1000 | Loss: 0.00004489
Iteration 46/1000 | Loss: 0.00004332
Iteration 47/1000 | Loss: 0.00004247
Iteration 48/1000 | Loss: 0.00004199
Iteration 49/1000 | Loss: 0.00004161
Iteration 50/1000 | Loss: 0.00022328
Iteration 51/1000 | Loss: 0.00029738
Iteration 52/1000 | Loss: 0.00027874
Iteration 53/1000 | Loss: 0.00004279
Iteration 54/1000 | Loss: 0.00031067
Iteration 55/1000 | Loss: 0.00029933
Iteration 56/1000 | Loss: 0.00021600
Iteration 57/1000 | Loss: 0.00024823
Iteration 58/1000 | Loss: 0.00006441
Iteration 59/1000 | Loss: 0.00009150
Iteration 60/1000 | Loss: 0.00004647
Iteration 61/1000 | Loss: 0.00004182
Iteration 62/1000 | Loss: 0.00004053
Iteration 63/1000 | Loss: 0.00003951
Iteration 64/1000 | Loss: 0.00003889
Iteration 65/1000 | Loss: 0.00003865
Iteration 66/1000 | Loss: 0.00003847
Iteration 67/1000 | Loss: 0.00003838
Iteration 68/1000 | Loss: 0.00003838
Iteration 69/1000 | Loss: 0.00003837
Iteration 70/1000 | Loss: 0.00003837
Iteration 71/1000 | Loss: 0.00003836
Iteration 72/1000 | Loss: 0.00003836
Iteration 73/1000 | Loss: 0.00003835
Iteration 74/1000 | Loss: 0.00003835
Iteration 75/1000 | Loss: 0.00003835
Iteration 76/1000 | Loss: 0.00003834
Iteration 77/1000 | Loss: 0.00003834
Iteration 78/1000 | Loss: 0.00003834
Iteration 79/1000 | Loss: 0.00003833
Iteration 80/1000 | Loss: 0.00003833
Iteration 81/1000 | Loss: 0.00003833
Iteration 82/1000 | Loss: 0.00003833
Iteration 83/1000 | Loss: 0.00003833
Iteration 84/1000 | Loss: 0.00003832
Iteration 85/1000 | Loss: 0.00003832
Iteration 86/1000 | Loss: 0.00003832
Iteration 87/1000 | Loss: 0.00003832
Iteration 88/1000 | Loss: 0.00003832
Iteration 89/1000 | Loss: 0.00003832
Iteration 90/1000 | Loss: 0.00003832
Iteration 91/1000 | Loss: 0.00003832
Iteration 92/1000 | Loss: 0.00003832
Iteration 93/1000 | Loss: 0.00003832
Iteration 94/1000 | Loss: 0.00003832
Iteration 95/1000 | Loss: 0.00003832
Iteration 96/1000 | Loss: 0.00003832
Iteration 97/1000 | Loss: 0.00003831
Iteration 98/1000 | Loss: 0.00003831
Iteration 99/1000 | Loss: 0.00003831
Iteration 100/1000 | Loss: 0.00003831
Iteration 101/1000 | Loss: 0.00003831
Iteration 102/1000 | Loss: 0.00003831
Iteration 103/1000 | Loss: 0.00003831
Iteration 104/1000 | Loss: 0.00003831
Iteration 105/1000 | Loss: 0.00003831
Iteration 106/1000 | Loss: 0.00003831
Iteration 107/1000 | Loss: 0.00003830
Iteration 108/1000 | Loss: 0.00003830
Iteration 109/1000 | Loss: 0.00003830
Iteration 110/1000 | Loss: 0.00003830
Iteration 111/1000 | Loss: 0.00003830
Iteration 112/1000 | Loss: 0.00003830
Iteration 113/1000 | Loss: 0.00003830
Iteration 114/1000 | Loss: 0.00003830
Iteration 115/1000 | Loss: 0.00003830
Iteration 116/1000 | Loss: 0.00003830
Iteration 117/1000 | Loss: 0.00003830
Iteration 118/1000 | Loss: 0.00003830
Iteration 119/1000 | Loss: 0.00003830
Iteration 120/1000 | Loss: 0.00003830
Iteration 121/1000 | Loss: 0.00003830
Iteration 122/1000 | Loss: 0.00003830
Iteration 123/1000 | Loss: 0.00003830
Iteration 124/1000 | Loss: 0.00003830
Iteration 125/1000 | Loss: 0.00003830
Iteration 126/1000 | Loss: 0.00003829
Iteration 127/1000 | Loss: 0.00003829
Iteration 128/1000 | Loss: 0.00003829
Iteration 129/1000 | Loss: 0.00003829
Iteration 130/1000 | Loss: 0.00003829
Iteration 131/1000 | Loss: 0.00003829
Iteration 132/1000 | Loss: 0.00003829
Iteration 133/1000 | Loss: 0.00003829
Iteration 134/1000 | Loss: 0.00003829
Iteration 135/1000 | Loss: 0.00003829
Iteration 136/1000 | Loss: 0.00003829
Iteration 137/1000 | Loss: 0.00003829
Iteration 138/1000 | Loss: 0.00003828
Iteration 139/1000 | Loss: 0.00003828
Iteration 140/1000 | Loss: 0.00003828
Iteration 141/1000 | Loss: 0.00003828
Iteration 142/1000 | Loss: 0.00003828
Iteration 143/1000 | Loss: 0.00003828
Iteration 144/1000 | Loss: 0.00003828
Iteration 145/1000 | Loss: 0.00003828
Iteration 146/1000 | Loss: 0.00003828
Iteration 147/1000 | Loss: 0.00003828
Iteration 148/1000 | Loss: 0.00003828
Iteration 149/1000 | Loss: 0.00003828
Iteration 150/1000 | Loss: 0.00003828
Iteration 151/1000 | Loss: 0.00003828
Iteration 152/1000 | Loss: 0.00003828
Iteration 153/1000 | Loss: 0.00003828
Iteration 154/1000 | Loss: 0.00003827
Iteration 155/1000 | Loss: 0.00003827
Iteration 156/1000 | Loss: 0.00003827
Iteration 157/1000 | Loss: 0.00003827
Iteration 158/1000 | Loss: 0.00003827
Iteration 159/1000 | Loss: 0.00003827
Iteration 160/1000 | Loss: 0.00003827
Iteration 161/1000 | Loss: 0.00003827
Iteration 162/1000 | Loss: 0.00003827
Iteration 163/1000 | Loss: 0.00003827
Iteration 164/1000 | Loss: 0.00003827
Iteration 165/1000 | Loss: 0.00003827
Iteration 166/1000 | Loss: 0.00003827
Iteration 167/1000 | Loss: 0.00003827
Iteration 168/1000 | Loss: 0.00003827
Iteration 169/1000 | Loss: 0.00003827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.8270216464297846e-05, 3.8270216464297846e-05, 3.8270216464297846e-05, 3.8270216464297846e-05, 3.8270216464297846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8270216464297846e-05

Optimization complete. Final v2v error: 4.022116184234619 mm

Highest mean error: 11.57249927520752 mm for frame 157

Lowest mean error: 3.185800552368164 mm for frame 29

Saving results

Total time: 169.46575665473938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950580
Iteration 2/25 | Loss: 0.00099240
Iteration 3/25 | Loss: 0.00082817
Iteration 4/25 | Loss: 0.00081068
Iteration 5/25 | Loss: 0.00080683
Iteration 6/25 | Loss: 0.00080526
Iteration 7/25 | Loss: 0.00080488
Iteration 8/25 | Loss: 0.00080488
Iteration 9/25 | Loss: 0.00080488
Iteration 10/25 | Loss: 0.00080488
Iteration 11/25 | Loss: 0.00080488
Iteration 12/25 | Loss: 0.00080488
Iteration 13/25 | Loss: 0.00080488
Iteration 14/25 | Loss: 0.00080488
Iteration 15/25 | Loss: 0.00080488
Iteration 16/25 | Loss: 0.00080488
Iteration 17/25 | Loss: 0.00080488
Iteration 18/25 | Loss: 0.00080488
Iteration 19/25 | Loss: 0.00080488
Iteration 20/25 | Loss: 0.00080488
Iteration 21/25 | Loss: 0.00080488
Iteration 22/25 | Loss: 0.00080488
Iteration 23/25 | Loss: 0.00080488
Iteration 24/25 | Loss: 0.00080488
Iteration 25/25 | Loss: 0.00080488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39042234
Iteration 2/25 | Loss: 0.00131544
Iteration 3/25 | Loss: 0.00131543
Iteration 4/25 | Loss: 0.00131543
Iteration 5/25 | Loss: 0.00131543
Iteration 6/25 | Loss: 0.00131543
Iteration 7/25 | Loss: 0.00131543
Iteration 8/25 | Loss: 0.00131543
Iteration 9/25 | Loss: 0.00131543
Iteration 10/25 | Loss: 0.00131543
Iteration 11/25 | Loss: 0.00131543
Iteration 12/25 | Loss: 0.00131543
Iteration 13/25 | Loss: 0.00131542
Iteration 14/25 | Loss: 0.00131542
Iteration 15/25 | Loss: 0.00131542
Iteration 16/25 | Loss: 0.00131542
Iteration 17/25 | Loss: 0.00131542
Iteration 18/25 | Loss: 0.00131542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001315424800850451, 0.001315424800850451, 0.001315424800850451, 0.001315424800850451, 0.001315424800850451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001315424800850451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131542
Iteration 2/1000 | Loss: 0.00003286
Iteration 3/1000 | Loss: 0.00001903
Iteration 4/1000 | Loss: 0.00001677
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001456
Iteration 11/1000 | Loss: 0.00001451
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001434
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001431
Iteration 22/1000 | Loss: 0.00001430
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001427
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001424
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001404
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001400
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001397
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001397
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001393
Iteration 85/1000 | Loss: 0.00001393
Iteration 86/1000 | Loss: 0.00001393
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001392
Iteration 89/1000 | Loss: 0.00001392
Iteration 90/1000 | Loss: 0.00001392
Iteration 91/1000 | Loss: 0.00001391
Iteration 92/1000 | Loss: 0.00001391
Iteration 93/1000 | Loss: 0.00001391
Iteration 94/1000 | Loss: 0.00001391
Iteration 95/1000 | Loss: 0.00001391
Iteration 96/1000 | Loss: 0.00001391
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001390
Iteration 99/1000 | Loss: 0.00001390
Iteration 100/1000 | Loss: 0.00001390
Iteration 101/1000 | Loss: 0.00001390
Iteration 102/1000 | Loss: 0.00001390
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001390
Iteration 108/1000 | Loss: 0.00001390
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001389
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001389
Iteration 120/1000 | Loss: 0.00001389
Iteration 121/1000 | Loss: 0.00001389
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001387
Iteration 131/1000 | Loss: 0.00001387
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001386
Iteration 134/1000 | Loss: 0.00001386
Iteration 135/1000 | Loss: 0.00001386
Iteration 136/1000 | Loss: 0.00001386
Iteration 137/1000 | Loss: 0.00001386
Iteration 138/1000 | Loss: 0.00001386
Iteration 139/1000 | Loss: 0.00001386
Iteration 140/1000 | Loss: 0.00001386
Iteration 141/1000 | Loss: 0.00001386
Iteration 142/1000 | Loss: 0.00001386
Iteration 143/1000 | Loss: 0.00001386
Iteration 144/1000 | Loss: 0.00001386
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001385
Iteration 147/1000 | Loss: 0.00001385
Iteration 148/1000 | Loss: 0.00001385
Iteration 149/1000 | Loss: 0.00001385
Iteration 150/1000 | Loss: 0.00001385
Iteration 151/1000 | Loss: 0.00001385
Iteration 152/1000 | Loss: 0.00001385
Iteration 153/1000 | Loss: 0.00001385
Iteration 154/1000 | Loss: 0.00001385
Iteration 155/1000 | Loss: 0.00001385
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001385
Iteration 160/1000 | Loss: 0.00001385
Iteration 161/1000 | Loss: 0.00001385
Iteration 162/1000 | Loss: 0.00001385
Iteration 163/1000 | Loss: 0.00001385
Iteration 164/1000 | Loss: 0.00001385
Iteration 165/1000 | Loss: 0.00001385
Iteration 166/1000 | Loss: 0.00001385
Iteration 167/1000 | Loss: 0.00001385
Iteration 168/1000 | Loss: 0.00001385
Iteration 169/1000 | Loss: 0.00001385
Iteration 170/1000 | Loss: 0.00001385
Iteration 171/1000 | Loss: 0.00001385
Iteration 172/1000 | Loss: 0.00001385
Iteration 173/1000 | Loss: 0.00001385
Iteration 174/1000 | Loss: 0.00001385
Iteration 175/1000 | Loss: 0.00001385
Iteration 176/1000 | Loss: 0.00001385
Iteration 177/1000 | Loss: 0.00001385
Iteration 178/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3852254596713465e-05, 1.3852254596713465e-05, 1.3852254596713465e-05, 1.3852254596713465e-05, 1.3852254596713465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3852254596713465e-05

Optimization complete. Final v2v error: 3.1575136184692383 mm

Highest mean error: 3.311453104019165 mm for frame 5

Lowest mean error: 3.012044668197632 mm for frame 102

Saving results

Total time: 38.40369629859924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726034
Iteration 2/25 | Loss: 0.00121658
Iteration 3/25 | Loss: 0.00095763
Iteration 4/25 | Loss: 0.00089252
Iteration 5/25 | Loss: 0.00087004
Iteration 6/25 | Loss: 0.00086199
Iteration 7/25 | Loss: 0.00085890
Iteration 8/25 | Loss: 0.00085757
Iteration 9/25 | Loss: 0.00085685
Iteration 10/25 | Loss: 0.00086016
Iteration 11/25 | Loss: 0.00086337
Iteration 12/25 | Loss: 0.00085537
Iteration 13/25 | Loss: 0.00085622
Iteration 14/25 | Loss: 0.00085649
Iteration 15/25 | Loss: 0.00085414
Iteration 16/25 | Loss: 0.00085676
Iteration 17/25 | Loss: 0.00084993
Iteration 18/25 | Loss: 0.00084669
Iteration 19/25 | Loss: 0.00084514
Iteration 20/25 | Loss: 0.00084438
Iteration 21/25 | Loss: 0.00084416
Iteration 22/25 | Loss: 0.00084407
Iteration 23/25 | Loss: 0.00084401
Iteration 24/25 | Loss: 0.00084401
Iteration 25/25 | Loss: 0.00084401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77455270
Iteration 2/25 | Loss: 0.00166480
Iteration 3/25 | Loss: 0.00166480
Iteration 4/25 | Loss: 0.00166480
Iteration 5/25 | Loss: 0.00166480
Iteration 6/25 | Loss: 0.00166480
Iteration 7/25 | Loss: 0.00166480
Iteration 8/25 | Loss: 0.00166479
Iteration 9/25 | Loss: 0.00166479
Iteration 10/25 | Loss: 0.00166479
Iteration 11/25 | Loss: 0.00166479
Iteration 12/25 | Loss: 0.00166479
Iteration 13/25 | Loss: 0.00166479
Iteration 14/25 | Loss: 0.00166479
Iteration 15/25 | Loss: 0.00166479
Iteration 16/25 | Loss: 0.00166479
Iteration 17/25 | Loss: 0.00166479
Iteration 18/25 | Loss: 0.00166479
Iteration 19/25 | Loss: 0.00166479
Iteration 20/25 | Loss: 0.00166479
Iteration 21/25 | Loss: 0.00166479
Iteration 22/25 | Loss: 0.00166479
Iteration 23/25 | Loss: 0.00166479
Iteration 24/25 | Loss: 0.00166479
Iteration 25/25 | Loss: 0.00166479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166479
Iteration 2/1000 | Loss: 0.00004949
Iteration 3/1000 | Loss: 0.00003458
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002723
Iteration 6/1000 | Loss: 0.00002580
Iteration 7/1000 | Loss: 0.00002469
Iteration 8/1000 | Loss: 0.00002393
Iteration 9/1000 | Loss: 0.00002339
Iteration 10/1000 | Loss: 0.00002300
Iteration 11/1000 | Loss: 0.00002274
Iteration 12/1000 | Loss: 0.00002251
Iteration 13/1000 | Loss: 0.00002227
Iteration 14/1000 | Loss: 0.00002224
Iteration 15/1000 | Loss: 0.00002210
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002202
Iteration 18/1000 | Loss: 0.00002200
Iteration 19/1000 | Loss: 0.00002195
Iteration 20/1000 | Loss: 0.00002191
Iteration 21/1000 | Loss: 0.00002190
Iteration 22/1000 | Loss: 0.00002189
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002175
Iteration 27/1000 | Loss: 0.00002175
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002172
Iteration 30/1000 | Loss: 0.00002171
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002170
Iteration 33/1000 | Loss: 0.00002169
Iteration 34/1000 | Loss: 0.00002169
Iteration 35/1000 | Loss: 0.00002169
Iteration 36/1000 | Loss: 0.00002168
Iteration 37/1000 | Loss: 0.00002168
Iteration 38/1000 | Loss: 0.00002167
Iteration 39/1000 | Loss: 0.00002167
Iteration 40/1000 | Loss: 0.00002167
Iteration 41/1000 | Loss: 0.00002166
Iteration 42/1000 | Loss: 0.00002165
Iteration 43/1000 | Loss: 0.00002165
Iteration 44/1000 | Loss: 0.00002165
Iteration 45/1000 | Loss: 0.00002165
Iteration 46/1000 | Loss: 0.00002164
Iteration 47/1000 | Loss: 0.00002164
Iteration 48/1000 | Loss: 0.00002164
Iteration 49/1000 | Loss: 0.00002164
Iteration 50/1000 | Loss: 0.00002164
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002161
Iteration 53/1000 | Loss: 0.00002161
Iteration 54/1000 | Loss: 0.00002161
Iteration 55/1000 | Loss: 0.00002161
Iteration 56/1000 | Loss: 0.00002161
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002160
Iteration 59/1000 | Loss: 0.00002160
Iteration 60/1000 | Loss: 0.00002160
Iteration 61/1000 | Loss: 0.00002160
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002159
Iteration 64/1000 | Loss: 0.00002158
Iteration 65/1000 | Loss: 0.00002158
Iteration 66/1000 | Loss: 0.00002157
Iteration 67/1000 | Loss: 0.00002157
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002155
Iteration 72/1000 | Loss: 0.00002155
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002155
Iteration 75/1000 | Loss: 0.00002155
Iteration 76/1000 | Loss: 0.00002155
Iteration 77/1000 | Loss: 0.00002154
Iteration 78/1000 | Loss: 0.00002154
Iteration 79/1000 | Loss: 0.00002154
Iteration 80/1000 | Loss: 0.00002154
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002153
Iteration 83/1000 | Loss: 0.00002152
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002152
Iteration 86/1000 | Loss: 0.00002152
Iteration 87/1000 | Loss: 0.00002151
Iteration 88/1000 | Loss: 0.00002151
Iteration 89/1000 | Loss: 0.00002151
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00002150
Iteration 92/1000 | Loss: 0.00002150
Iteration 93/1000 | Loss: 0.00002149
Iteration 94/1000 | Loss: 0.00002149
Iteration 95/1000 | Loss: 0.00002149
Iteration 96/1000 | Loss: 0.00002149
Iteration 97/1000 | Loss: 0.00002148
Iteration 98/1000 | Loss: 0.00002148
Iteration 99/1000 | Loss: 0.00002148
Iteration 100/1000 | Loss: 0.00002148
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002147
Iteration 104/1000 | Loss: 0.00002147
Iteration 105/1000 | Loss: 0.00002147
Iteration 106/1000 | Loss: 0.00002147
Iteration 107/1000 | Loss: 0.00002147
Iteration 108/1000 | Loss: 0.00002147
Iteration 109/1000 | Loss: 0.00002147
Iteration 110/1000 | Loss: 0.00002146
Iteration 111/1000 | Loss: 0.00002146
Iteration 112/1000 | Loss: 0.00002146
Iteration 113/1000 | Loss: 0.00002146
Iteration 114/1000 | Loss: 0.00002146
Iteration 115/1000 | Loss: 0.00002146
Iteration 116/1000 | Loss: 0.00002146
Iteration 117/1000 | Loss: 0.00002146
Iteration 118/1000 | Loss: 0.00002146
Iteration 119/1000 | Loss: 0.00002146
Iteration 120/1000 | Loss: 0.00002146
Iteration 121/1000 | Loss: 0.00002146
Iteration 122/1000 | Loss: 0.00002146
Iteration 123/1000 | Loss: 0.00002145
Iteration 124/1000 | Loss: 0.00002145
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Iteration 127/1000 | Loss: 0.00002145
Iteration 128/1000 | Loss: 0.00002145
Iteration 129/1000 | Loss: 0.00002145
Iteration 130/1000 | Loss: 0.00002145
Iteration 131/1000 | Loss: 0.00002145
Iteration 132/1000 | Loss: 0.00002145
Iteration 133/1000 | Loss: 0.00002145
Iteration 134/1000 | Loss: 0.00002145
Iteration 135/1000 | Loss: 0.00002145
Iteration 136/1000 | Loss: 0.00002145
Iteration 137/1000 | Loss: 0.00002145
Iteration 138/1000 | Loss: 0.00002145
Iteration 139/1000 | Loss: 0.00002145
Iteration 140/1000 | Loss: 0.00002144
Iteration 141/1000 | Loss: 0.00002144
Iteration 142/1000 | Loss: 0.00002144
Iteration 143/1000 | Loss: 0.00002144
Iteration 144/1000 | Loss: 0.00002144
Iteration 145/1000 | Loss: 0.00002144
Iteration 146/1000 | Loss: 0.00002144
Iteration 147/1000 | Loss: 0.00002144
Iteration 148/1000 | Loss: 0.00002144
Iteration 149/1000 | Loss: 0.00002144
Iteration 150/1000 | Loss: 0.00002144
Iteration 151/1000 | Loss: 0.00002144
Iteration 152/1000 | Loss: 0.00002144
Iteration 153/1000 | Loss: 0.00002144
Iteration 154/1000 | Loss: 0.00002144
Iteration 155/1000 | Loss: 0.00002144
Iteration 156/1000 | Loss: 0.00002144
Iteration 157/1000 | Loss: 0.00002144
Iteration 158/1000 | Loss: 0.00002144
Iteration 159/1000 | Loss: 0.00002144
Iteration 160/1000 | Loss: 0.00002144
Iteration 161/1000 | Loss: 0.00002144
Iteration 162/1000 | Loss: 0.00002144
Iteration 163/1000 | Loss: 0.00002144
Iteration 164/1000 | Loss: 0.00002144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.144064819731284e-05, 2.144064819731284e-05, 2.144064819731284e-05, 2.144064819731284e-05, 2.144064819731284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.144064819731284e-05

Optimization complete. Final v2v error: 3.815091848373413 mm

Highest mean error: 5.410770893096924 mm for frame 111

Lowest mean error: 3.0810205936431885 mm for frame 28

Saving results

Total time: 71.55140662193298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854816
Iteration 2/25 | Loss: 0.00150486
Iteration 3/25 | Loss: 0.00097370
Iteration 4/25 | Loss: 0.00086176
Iteration 5/25 | Loss: 0.00085275
Iteration 6/25 | Loss: 0.00085136
Iteration 7/25 | Loss: 0.00085136
Iteration 8/25 | Loss: 0.00085136
Iteration 9/25 | Loss: 0.00085136
Iteration 10/25 | Loss: 0.00085136
Iteration 11/25 | Loss: 0.00085136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008513596840202808, 0.0008513596840202808, 0.0008513596840202808, 0.0008513596840202808, 0.0008513596840202808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008513596840202808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59228897
Iteration 2/25 | Loss: 0.00118166
Iteration 3/25 | Loss: 0.00118166
Iteration 4/25 | Loss: 0.00118166
Iteration 5/25 | Loss: 0.00118166
Iteration 6/25 | Loss: 0.00118166
Iteration 7/25 | Loss: 0.00118166
Iteration 8/25 | Loss: 0.00118166
Iteration 9/25 | Loss: 0.00118166
Iteration 10/25 | Loss: 0.00118166
Iteration 11/25 | Loss: 0.00118166
Iteration 12/25 | Loss: 0.00118166
Iteration 13/25 | Loss: 0.00118166
Iteration 14/25 | Loss: 0.00118166
Iteration 15/25 | Loss: 0.00118166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001181658124551177, 0.001181658124551177, 0.001181658124551177, 0.001181658124551177, 0.001181658124551177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001181658124551177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118166
Iteration 2/1000 | Loss: 0.00003509
Iteration 3/1000 | Loss: 0.00002485
Iteration 4/1000 | Loss: 0.00002313
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00002054
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001997
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001987
Iteration 14/1000 | Loss: 0.00001983
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001982
Iteration 17/1000 | Loss: 0.00001982
Iteration 18/1000 | Loss: 0.00001982
Iteration 19/1000 | Loss: 0.00001981
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001980
Iteration 22/1000 | Loss: 0.00001979
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001979
Iteration 25/1000 | Loss: 0.00001979
Iteration 26/1000 | Loss: 0.00001979
Iteration 27/1000 | Loss: 0.00001979
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001976
Iteration 34/1000 | Loss: 0.00001975
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00001974
Iteration 37/1000 | Loss: 0.00001973
Iteration 38/1000 | Loss: 0.00001973
Iteration 39/1000 | Loss: 0.00001973
Iteration 40/1000 | Loss: 0.00001972
Iteration 41/1000 | Loss: 0.00001972
Iteration 42/1000 | Loss: 0.00001972
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00001972
Iteration 45/1000 | Loss: 0.00001972
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001971
Iteration 48/1000 | Loss: 0.00001971
Iteration 49/1000 | Loss: 0.00001971
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001971
Iteration 52/1000 | Loss: 0.00001971
Iteration 53/1000 | Loss: 0.00001971
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001970
Iteration 58/1000 | Loss: 0.00001970
Iteration 59/1000 | Loss: 0.00001970
Iteration 60/1000 | Loss: 0.00001970
Iteration 61/1000 | Loss: 0.00001970
Iteration 62/1000 | Loss: 0.00001969
Iteration 63/1000 | Loss: 0.00001969
Iteration 64/1000 | Loss: 0.00001969
Iteration 65/1000 | Loss: 0.00001969
Iteration 66/1000 | Loss: 0.00001969
Iteration 67/1000 | Loss: 0.00001969
Iteration 68/1000 | Loss: 0.00001968
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001968
Iteration 71/1000 | Loss: 0.00001968
Iteration 72/1000 | Loss: 0.00001968
Iteration 73/1000 | Loss: 0.00001968
Iteration 74/1000 | Loss: 0.00001968
Iteration 75/1000 | Loss: 0.00001968
Iteration 76/1000 | Loss: 0.00001968
Iteration 77/1000 | Loss: 0.00001968
Iteration 78/1000 | Loss: 0.00001967
Iteration 79/1000 | Loss: 0.00001967
Iteration 80/1000 | Loss: 0.00001967
Iteration 81/1000 | Loss: 0.00001966
Iteration 82/1000 | Loss: 0.00001966
Iteration 83/1000 | Loss: 0.00001966
Iteration 84/1000 | Loss: 0.00001966
Iteration 85/1000 | Loss: 0.00001966
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001965
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001964
Iteration 92/1000 | Loss: 0.00001964
Iteration 93/1000 | Loss: 0.00001964
Iteration 94/1000 | Loss: 0.00001964
Iteration 95/1000 | Loss: 0.00001964
Iteration 96/1000 | Loss: 0.00001964
Iteration 97/1000 | Loss: 0.00001964
Iteration 98/1000 | Loss: 0.00001964
Iteration 99/1000 | Loss: 0.00001964
Iteration 100/1000 | Loss: 0.00001964
Iteration 101/1000 | Loss: 0.00001964
Iteration 102/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.9635155695141293e-05, 1.9635155695141293e-05, 1.9635155695141293e-05, 1.9635155695141293e-05, 1.9635155695141293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9635155695141293e-05

Optimization complete. Final v2v error: 3.635242462158203 mm

Highest mean error: 3.707859754562378 mm for frame 70

Lowest mean error: 3.3424177169799805 mm for frame 3

Saving results

Total time: 35.443970680236816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473899
Iteration 2/25 | Loss: 0.00131833
Iteration 3/25 | Loss: 0.00095629
Iteration 4/25 | Loss: 0.00090171
Iteration 5/25 | Loss: 0.00088651
Iteration 6/25 | Loss: 0.00088377
Iteration 7/25 | Loss: 0.00088350
Iteration 8/25 | Loss: 0.00088350
Iteration 9/25 | Loss: 0.00088350
Iteration 10/25 | Loss: 0.00088350
Iteration 11/25 | Loss: 0.00088350
Iteration 12/25 | Loss: 0.00088350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008835045737214386, 0.0008835045737214386, 0.0008835045737214386, 0.0008835045737214386, 0.0008835045737214386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008835045737214386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61456716
Iteration 2/25 | Loss: 0.00119715
Iteration 3/25 | Loss: 0.00119715
Iteration 4/25 | Loss: 0.00119715
Iteration 5/25 | Loss: 0.00119715
Iteration 6/25 | Loss: 0.00119715
Iteration 7/25 | Loss: 0.00119715
Iteration 8/25 | Loss: 0.00119715
Iteration 9/25 | Loss: 0.00119715
Iteration 10/25 | Loss: 0.00119715
Iteration 11/25 | Loss: 0.00119715
Iteration 12/25 | Loss: 0.00119715
Iteration 13/25 | Loss: 0.00119715
Iteration 14/25 | Loss: 0.00119715
Iteration 15/25 | Loss: 0.00119715
Iteration 16/25 | Loss: 0.00119715
Iteration 17/25 | Loss: 0.00119715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001197148347273469, 0.001197148347273469, 0.001197148347273469, 0.001197148347273469, 0.001197148347273469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001197148347273469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119715
Iteration 2/1000 | Loss: 0.00003928
Iteration 3/1000 | Loss: 0.00003188
Iteration 4/1000 | Loss: 0.00003000
Iteration 5/1000 | Loss: 0.00002867
Iteration 6/1000 | Loss: 0.00002773
Iteration 7/1000 | Loss: 0.00002709
Iteration 8/1000 | Loss: 0.00002676
Iteration 9/1000 | Loss: 0.00002652
Iteration 10/1000 | Loss: 0.00002632
Iteration 11/1000 | Loss: 0.00002629
Iteration 12/1000 | Loss: 0.00002626
Iteration 13/1000 | Loss: 0.00002620
Iteration 14/1000 | Loss: 0.00002617
Iteration 15/1000 | Loss: 0.00002617
Iteration 16/1000 | Loss: 0.00002617
Iteration 17/1000 | Loss: 0.00002609
Iteration 18/1000 | Loss: 0.00002609
Iteration 19/1000 | Loss: 0.00002608
Iteration 20/1000 | Loss: 0.00002608
Iteration 21/1000 | Loss: 0.00002604
Iteration 22/1000 | Loss: 0.00002599
Iteration 23/1000 | Loss: 0.00002599
Iteration 24/1000 | Loss: 0.00002598
Iteration 25/1000 | Loss: 0.00002597
Iteration 26/1000 | Loss: 0.00002597
Iteration 27/1000 | Loss: 0.00002596
Iteration 28/1000 | Loss: 0.00002596
Iteration 29/1000 | Loss: 0.00002596
Iteration 30/1000 | Loss: 0.00002595
Iteration 31/1000 | Loss: 0.00002594
Iteration 32/1000 | Loss: 0.00002594
Iteration 33/1000 | Loss: 0.00002591
Iteration 34/1000 | Loss: 0.00002591
Iteration 35/1000 | Loss: 0.00002589
Iteration 36/1000 | Loss: 0.00002586
Iteration 37/1000 | Loss: 0.00002586
Iteration 38/1000 | Loss: 0.00002586
Iteration 39/1000 | Loss: 0.00002586
Iteration 40/1000 | Loss: 0.00002585
Iteration 41/1000 | Loss: 0.00002585
Iteration 42/1000 | Loss: 0.00002585
Iteration 43/1000 | Loss: 0.00002585
Iteration 44/1000 | Loss: 0.00002585
Iteration 45/1000 | Loss: 0.00002585
Iteration 46/1000 | Loss: 0.00002585
Iteration 47/1000 | Loss: 0.00002585
Iteration 48/1000 | Loss: 0.00002584
Iteration 49/1000 | Loss: 0.00002584
Iteration 50/1000 | Loss: 0.00002583
Iteration 51/1000 | Loss: 0.00002583
Iteration 52/1000 | Loss: 0.00002583
Iteration 53/1000 | Loss: 0.00002582
Iteration 54/1000 | Loss: 0.00002582
Iteration 55/1000 | Loss: 0.00002582
Iteration 56/1000 | Loss: 0.00002582
Iteration 57/1000 | Loss: 0.00002581
Iteration 58/1000 | Loss: 0.00002581
Iteration 59/1000 | Loss: 0.00002581
Iteration 60/1000 | Loss: 0.00002581
Iteration 61/1000 | Loss: 0.00002581
Iteration 62/1000 | Loss: 0.00002581
Iteration 63/1000 | Loss: 0.00002581
Iteration 64/1000 | Loss: 0.00002581
Iteration 65/1000 | Loss: 0.00002581
Iteration 66/1000 | Loss: 0.00002580
Iteration 67/1000 | Loss: 0.00002580
Iteration 68/1000 | Loss: 0.00002580
Iteration 69/1000 | Loss: 0.00002580
Iteration 70/1000 | Loss: 0.00002580
Iteration 71/1000 | Loss: 0.00002580
Iteration 72/1000 | Loss: 0.00002579
Iteration 73/1000 | Loss: 0.00002579
Iteration 74/1000 | Loss: 0.00002579
Iteration 75/1000 | Loss: 0.00002579
Iteration 76/1000 | Loss: 0.00002579
Iteration 77/1000 | Loss: 0.00002579
Iteration 78/1000 | Loss: 0.00002579
Iteration 79/1000 | Loss: 0.00002579
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002578
Iteration 82/1000 | Loss: 0.00002578
Iteration 83/1000 | Loss: 0.00002578
Iteration 84/1000 | Loss: 0.00002578
Iteration 85/1000 | Loss: 0.00002578
Iteration 86/1000 | Loss: 0.00002578
Iteration 87/1000 | Loss: 0.00002578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.5781097065191716e-05, 2.5781097065191716e-05, 2.5781097065191716e-05, 2.5781097065191716e-05, 2.5781097065191716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5781097065191716e-05

Optimization complete. Final v2v error: 4.138782501220703 mm

Highest mean error: 4.534593105316162 mm for frame 160

Lowest mean error: 3.6319334506988525 mm for frame 121

Saving results

Total time: 39.00391960144043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062647
Iteration 2/25 | Loss: 0.00174520
Iteration 3/25 | Loss: 0.00124469
Iteration 4/25 | Loss: 0.00107814
Iteration 5/25 | Loss: 0.00107288
Iteration 6/25 | Loss: 0.00103697
Iteration 7/25 | Loss: 0.00101137
Iteration 8/25 | Loss: 0.00099104
Iteration 9/25 | Loss: 0.00095622
Iteration 10/25 | Loss: 0.00093511
Iteration 11/25 | Loss: 0.00092934
Iteration 12/25 | Loss: 0.00092260
Iteration 13/25 | Loss: 0.00093067
Iteration 14/25 | Loss: 0.00091530
Iteration 15/25 | Loss: 0.00091103
Iteration 16/25 | Loss: 0.00091053
Iteration 17/25 | Loss: 0.00090568
Iteration 18/25 | Loss: 0.00089800
Iteration 19/25 | Loss: 0.00089925
Iteration 20/25 | Loss: 0.00089162
Iteration 21/25 | Loss: 0.00089348
Iteration 22/25 | Loss: 0.00088141
Iteration 23/25 | Loss: 0.00088056
Iteration 24/25 | Loss: 0.00087922
Iteration 25/25 | Loss: 0.00087675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63059521
Iteration 2/25 | Loss: 0.00146707
Iteration 3/25 | Loss: 0.00146707
Iteration 4/25 | Loss: 0.00146707
Iteration 5/25 | Loss: 0.00146707
Iteration 6/25 | Loss: 0.00146707
Iteration 7/25 | Loss: 0.00146707
Iteration 8/25 | Loss: 0.00146707
Iteration 9/25 | Loss: 0.00146707
Iteration 10/25 | Loss: 0.00146707
Iteration 11/25 | Loss: 0.00146707
Iteration 12/25 | Loss: 0.00146707
Iteration 13/25 | Loss: 0.00146707
Iteration 14/25 | Loss: 0.00146707
Iteration 15/25 | Loss: 0.00146707
Iteration 16/25 | Loss: 0.00146707
Iteration 17/25 | Loss: 0.00146707
Iteration 18/25 | Loss: 0.00146707
Iteration 19/25 | Loss: 0.00146707
Iteration 20/25 | Loss: 0.00146707
Iteration 21/25 | Loss: 0.00146707
Iteration 22/25 | Loss: 0.00146707
Iteration 23/25 | Loss: 0.00146707
Iteration 24/25 | Loss: 0.00146707
Iteration 25/25 | Loss: 0.00146707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146707
Iteration 2/1000 | Loss: 0.00008649
Iteration 3/1000 | Loss: 0.00031028
Iteration 4/1000 | Loss: 0.00010861
Iteration 5/1000 | Loss: 0.00035855
Iteration 6/1000 | Loss: 0.00041200
Iteration 7/1000 | Loss: 0.00021517
Iteration 8/1000 | Loss: 0.00008796
Iteration 9/1000 | Loss: 0.00018845
Iteration 10/1000 | Loss: 0.00020448
Iteration 11/1000 | Loss: 0.00022021
Iteration 12/1000 | Loss: 0.00023737
Iteration 13/1000 | Loss: 0.00024297
Iteration 14/1000 | Loss: 0.00019546
Iteration 15/1000 | Loss: 0.00027046
Iteration 16/1000 | Loss: 0.00015382
Iteration 17/1000 | Loss: 0.00025497
Iteration 18/1000 | Loss: 0.00016829
Iteration 19/1000 | Loss: 0.00027024
Iteration 20/1000 | Loss: 0.00017690
Iteration 21/1000 | Loss: 0.00010629
Iteration 22/1000 | Loss: 0.00029813
Iteration 23/1000 | Loss: 0.00020808
Iteration 24/1000 | Loss: 0.00033475
Iteration 25/1000 | Loss: 0.00028122
Iteration 26/1000 | Loss: 0.00045294
Iteration 27/1000 | Loss: 0.00027761
Iteration 28/1000 | Loss: 0.00049852
Iteration 29/1000 | Loss: 0.00050803
Iteration 30/1000 | Loss: 0.00005357
Iteration 31/1000 | Loss: 0.00003559
Iteration 32/1000 | Loss: 0.00002924
Iteration 33/1000 | Loss: 0.00002678
Iteration 34/1000 | Loss: 0.00002573
Iteration 35/1000 | Loss: 0.00002518
Iteration 36/1000 | Loss: 0.00002468
Iteration 37/1000 | Loss: 0.00002439
Iteration 38/1000 | Loss: 0.00002410
Iteration 39/1000 | Loss: 0.00002381
Iteration 40/1000 | Loss: 0.00002378
Iteration 41/1000 | Loss: 0.00002377
Iteration 42/1000 | Loss: 0.00002376
Iteration 43/1000 | Loss: 0.00002371
Iteration 44/1000 | Loss: 0.00002368
Iteration 45/1000 | Loss: 0.00002357
Iteration 46/1000 | Loss: 0.00002355
Iteration 47/1000 | Loss: 0.00002355
Iteration 48/1000 | Loss: 0.00002354
Iteration 49/1000 | Loss: 0.00002354
Iteration 50/1000 | Loss: 0.00002354
Iteration 51/1000 | Loss: 0.00002354
Iteration 52/1000 | Loss: 0.00002353
Iteration 53/1000 | Loss: 0.00002353
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002353
Iteration 56/1000 | Loss: 0.00002352
Iteration 57/1000 | Loss: 0.00002352
Iteration 58/1000 | Loss: 0.00002352
Iteration 59/1000 | Loss: 0.00002351
Iteration 60/1000 | Loss: 0.00002351
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002350
Iteration 63/1000 | Loss: 0.00002350
Iteration 64/1000 | Loss: 0.00002350
Iteration 65/1000 | Loss: 0.00002350
Iteration 66/1000 | Loss: 0.00002349
Iteration 67/1000 | Loss: 0.00002349
Iteration 68/1000 | Loss: 0.00002349
Iteration 69/1000 | Loss: 0.00002349
Iteration 70/1000 | Loss: 0.00002349
Iteration 71/1000 | Loss: 0.00002349
Iteration 72/1000 | Loss: 0.00002348
Iteration 73/1000 | Loss: 0.00002348
Iteration 74/1000 | Loss: 0.00002347
Iteration 75/1000 | Loss: 0.00002347
Iteration 76/1000 | Loss: 0.00002347
Iteration 77/1000 | Loss: 0.00002346
Iteration 78/1000 | Loss: 0.00002346
Iteration 79/1000 | Loss: 0.00002346
Iteration 80/1000 | Loss: 0.00002345
Iteration 81/1000 | Loss: 0.00002345
Iteration 82/1000 | Loss: 0.00002344
Iteration 83/1000 | Loss: 0.00002344
Iteration 84/1000 | Loss: 0.00002344
Iteration 85/1000 | Loss: 0.00002344
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002343
Iteration 89/1000 | Loss: 0.00002343
Iteration 90/1000 | Loss: 0.00002343
Iteration 91/1000 | Loss: 0.00002343
Iteration 92/1000 | Loss: 0.00002343
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002342
Iteration 95/1000 | Loss: 0.00002342
Iteration 96/1000 | Loss: 0.00002342
Iteration 97/1000 | Loss: 0.00002342
Iteration 98/1000 | Loss: 0.00002342
Iteration 99/1000 | Loss: 0.00002342
Iteration 100/1000 | Loss: 0.00002341
Iteration 101/1000 | Loss: 0.00002341
Iteration 102/1000 | Loss: 0.00002341
Iteration 103/1000 | Loss: 0.00002341
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00002340
Iteration 106/1000 | Loss: 0.00002340
Iteration 107/1000 | Loss: 0.00002340
Iteration 108/1000 | Loss: 0.00002340
Iteration 109/1000 | Loss: 0.00002339
Iteration 110/1000 | Loss: 0.00002339
Iteration 111/1000 | Loss: 0.00002339
Iteration 112/1000 | Loss: 0.00002339
Iteration 113/1000 | Loss: 0.00002339
Iteration 114/1000 | Loss: 0.00002339
Iteration 115/1000 | Loss: 0.00002339
Iteration 116/1000 | Loss: 0.00002339
Iteration 117/1000 | Loss: 0.00002338
Iteration 118/1000 | Loss: 0.00002338
Iteration 119/1000 | Loss: 0.00002338
Iteration 120/1000 | Loss: 0.00002338
Iteration 121/1000 | Loss: 0.00002338
Iteration 122/1000 | Loss: 0.00002338
Iteration 123/1000 | Loss: 0.00002338
Iteration 124/1000 | Loss: 0.00002338
Iteration 125/1000 | Loss: 0.00002338
Iteration 126/1000 | Loss: 0.00002338
Iteration 127/1000 | Loss: 0.00002338
Iteration 128/1000 | Loss: 0.00002338
Iteration 129/1000 | Loss: 0.00002338
Iteration 130/1000 | Loss: 0.00002338
Iteration 131/1000 | Loss: 0.00002337
Iteration 132/1000 | Loss: 0.00002337
Iteration 133/1000 | Loss: 0.00002337
Iteration 134/1000 | Loss: 0.00002337
Iteration 135/1000 | Loss: 0.00002337
Iteration 136/1000 | Loss: 0.00002337
Iteration 137/1000 | Loss: 0.00002337
Iteration 138/1000 | Loss: 0.00002337
Iteration 139/1000 | Loss: 0.00002337
Iteration 140/1000 | Loss: 0.00002336
Iteration 141/1000 | Loss: 0.00002336
Iteration 142/1000 | Loss: 0.00002336
Iteration 143/1000 | Loss: 0.00002336
Iteration 144/1000 | Loss: 0.00002335
Iteration 145/1000 | Loss: 0.00002335
Iteration 146/1000 | Loss: 0.00002335
Iteration 147/1000 | Loss: 0.00002335
Iteration 148/1000 | Loss: 0.00002335
Iteration 149/1000 | Loss: 0.00002335
Iteration 150/1000 | Loss: 0.00002335
Iteration 151/1000 | Loss: 0.00002334
Iteration 152/1000 | Loss: 0.00002334
Iteration 153/1000 | Loss: 0.00002334
Iteration 154/1000 | Loss: 0.00002334
Iteration 155/1000 | Loss: 0.00002334
Iteration 156/1000 | Loss: 0.00002334
Iteration 157/1000 | Loss: 0.00002333
Iteration 158/1000 | Loss: 0.00002333
Iteration 159/1000 | Loss: 0.00002333
Iteration 160/1000 | Loss: 0.00002333
Iteration 161/1000 | Loss: 0.00002333
Iteration 162/1000 | Loss: 0.00002333
Iteration 163/1000 | Loss: 0.00002333
Iteration 164/1000 | Loss: 0.00002333
Iteration 165/1000 | Loss: 0.00002332
Iteration 166/1000 | Loss: 0.00002332
Iteration 167/1000 | Loss: 0.00002332
Iteration 168/1000 | Loss: 0.00002332
Iteration 169/1000 | Loss: 0.00002332
Iteration 170/1000 | Loss: 0.00002332
Iteration 171/1000 | Loss: 0.00002332
Iteration 172/1000 | Loss: 0.00002332
Iteration 173/1000 | Loss: 0.00002332
Iteration 174/1000 | Loss: 0.00002332
Iteration 175/1000 | Loss: 0.00002331
Iteration 176/1000 | Loss: 0.00002331
Iteration 177/1000 | Loss: 0.00002331
Iteration 178/1000 | Loss: 0.00002331
Iteration 179/1000 | Loss: 0.00002331
Iteration 180/1000 | Loss: 0.00002331
Iteration 181/1000 | Loss: 0.00002331
Iteration 182/1000 | Loss: 0.00002331
Iteration 183/1000 | Loss: 0.00002331
Iteration 184/1000 | Loss: 0.00002331
Iteration 185/1000 | Loss: 0.00002330
Iteration 186/1000 | Loss: 0.00002330
Iteration 187/1000 | Loss: 0.00002330
Iteration 188/1000 | Loss: 0.00002330
Iteration 189/1000 | Loss: 0.00002330
Iteration 190/1000 | Loss: 0.00002330
Iteration 191/1000 | Loss: 0.00002330
Iteration 192/1000 | Loss: 0.00002330
Iteration 193/1000 | Loss: 0.00002330
Iteration 194/1000 | Loss: 0.00002330
Iteration 195/1000 | Loss: 0.00002330
Iteration 196/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.3300666725845076e-05, 2.3300666725845076e-05, 2.3300666725845076e-05, 2.3300666725845076e-05, 2.3300666725845076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3300666725845076e-05

Optimization complete. Final v2v error: 3.9694783687591553 mm

Highest mean error: 4.219702243804932 mm for frame 80

Lowest mean error: 3.3618850708007812 mm for frame 111

Saving results

Total time: 113.02737498283386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525205
Iteration 2/25 | Loss: 0.00123672
Iteration 3/25 | Loss: 0.00090847
Iteration 4/25 | Loss: 0.00086495
Iteration 5/25 | Loss: 0.00085583
Iteration 6/25 | Loss: 0.00085291
Iteration 7/25 | Loss: 0.00085246
Iteration 8/25 | Loss: 0.00085244
Iteration 9/25 | Loss: 0.00085244
Iteration 10/25 | Loss: 0.00085244
Iteration 11/25 | Loss: 0.00085244
Iteration 12/25 | Loss: 0.00085244
Iteration 13/25 | Loss: 0.00085244
Iteration 14/25 | Loss: 0.00085244
Iteration 15/25 | Loss: 0.00085244
Iteration 16/25 | Loss: 0.00085244
Iteration 17/25 | Loss: 0.00085244
Iteration 18/25 | Loss: 0.00085244
Iteration 19/25 | Loss: 0.00085244
Iteration 20/25 | Loss: 0.00085244
Iteration 21/25 | Loss: 0.00085244
Iteration 22/25 | Loss: 0.00085244
Iteration 23/25 | Loss: 0.00085244
Iteration 24/25 | Loss: 0.00085244
Iteration 25/25 | Loss: 0.00085244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35647535
Iteration 2/25 | Loss: 0.00130584
Iteration 3/25 | Loss: 0.00130581
Iteration 4/25 | Loss: 0.00130581
Iteration 5/25 | Loss: 0.00130580
Iteration 6/25 | Loss: 0.00130580
Iteration 7/25 | Loss: 0.00130580
Iteration 8/25 | Loss: 0.00130580
Iteration 9/25 | Loss: 0.00130580
Iteration 10/25 | Loss: 0.00130580
Iteration 11/25 | Loss: 0.00130580
Iteration 12/25 | Loss: 0.00130580
Iteration 13/25 | Loss: 0.00130580
Iteration 14/25 | Loss: 0.00130580
Iteration 15/25 | Loss: 0.00130580
Iteration 16/25 | Loss: 0.00130580
Iteration 17/25 | Loss: 0.00130580
Iteration 18/25 | Loss: 0.00130580
Iteration 19/25 | Loss: 0.00130580
Iteration 20/25 | Loss: 0.00130580
Iteration 21/25 | Loss: 0.00130580
Iteration 22/25 | Loss: 0.00130580
Iteration 23/25 | Loss: 0.00130580
Iteration 24/25 | Loss: 0.00130580
Iteration 25/25 | Loss: 0.00130580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130580
Iteration 2/1000 | Loss: 0.00005610
Iteration 3/1000 | Loss: 0.00003544
Iteration 4/1000 | Loss: 0.00003103
Iteration 5/1000 | Loss: 0.00002929
Iteration 6/1000 | Loss: 0.00002812
Iteration 7/1000 | Loss: 0.00002732
Iteration 8/1000 | Loss: 0.00002665
Iteration 9/1000 | Loss: 0.00002613
Iteration 10/1000 | Loss: 0.00002579
Iteration 11/1000 | Loss: 0.00002544
Iteration 12/1000 | Loss: 0.00002516
Iteration 13/1000 | Loss: 0.00002491
Iteration 14/1000 | Loss: 0.00002470
Iteration 15/1000 | Loss: 0.00002455
Iteration 16/1000 | Loss: 0.00002449
Iteration 17/1000 | Loss: 0.00002449
Iteration 18/1000 | Loss: 0.00002448
Iteration 19/1000 | Loss: 0.00002447
Iteration 20/1000 | Loss: 0.00002447
Iteration 21/1000 | Loss: 0.00002446
Iteration 22/1000 | Loss: 0.00002446
Iteration 23/1000 | Loss: 0.00002446
Iteration 24/1000 | Loss: 0.00002443
Iteration 25/1000 | Loss: 0.00002438
Iteration 26/1000 | Loss: 0.00002431
Iteration 27/1000 | Loss: 0.00002431
Iteration 28/1000 | Loss: 0.00002431
Iteration 29/1000 | Loss: 0.00002425
Iteration 30/1000 | Loss: 0.00002421
Iteration 31/1000 | Loss: 0.00002421
Iteration 32/1000 | Loss: 0.00002421
Iteration 33/1000 | Loss: 0.00002421
Iteration 34/1000 | Loss: 0.00002420
Iteration 35/1000 | Loss: 0.00002420
Iteration 36/1000 | Loss: 0.00002418
Iteration 37/1000 | Loss: 0.00002418
Iteration 38/1000 | Loss: 0.00002417
Iteration 39/1000 | Loss: 0.00002417
Iteration 40/1000 | Loss: 0.00002417
Iteration 41/1000 | Loss: 0.00002416
Iteration 42/1000 | Loss: 0.00002416
Iteration 43/1000 | Loss: 0.00002415
Iteration 44/1000 | Loss: 0.00002415
Iteration 45/1000 | Loss: 0.00002415
Iteration 46/1000 | Loss: 0.00002413
Iteration 47/1000 | Loss: 0.00002413
Iteration 48/1000 | Loss: 0.00002413
Iteration 49/1000 | Loss: 0.00002413
Iteration 50/1000 | Loss: 0.00002412
Iteration 51/1000 | Loss: 0.00002412
Iteration 52/1000 | Loss: 0.00002412
Iteration 53/1000 | Loss: 0.00002412
Iteration 54/1000 | Loss: 0.00002411
Iteration 55/1000 | Loss: 0.00002411
Iteration 56/1000 | Loss: 0.00002411
Iteration 57/1000 | Loss: 0.00002411
Iteration 58/1000 | Loss: 0.00002410
Iteration 59/1000 | Loss: 0.00002410
Iteration 60/1000 | Loss: 0.00002410
Iteration 61/1000 | Loss: 0.00002410
Iteration 62/1000 | Loss: 0.00002410
Iteration 63/1000 | Loss: 0.00002410
Iteration 64/1000 | Loss: 0.00002410
Iteration 65/1000 | Loss: 0.00002410
Iteration 66/1000 | Loss: 0.00002410
Iteration 67/1000 | Loss: 0.00002410
Iteration 68/1000 | Loss: 0.00002410
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002410
Iteration 71/1000 | Loss: 0.00002410
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002409
Iteration 74/1000 | Loss: 0.00002409
Iteration 75/1000 | Loss: 0.00002409
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002408
Iteration 78/1000 | Loss: 0.00002408
Iteration 79/1000 | Loss: 0.00002408
Iteration 80/1000 | Loss: 0.00002408
Iteration 81/1000 | Loss: 0.00002408
Iteration 82/1000 | Loss: 0.00002407
Iteration 83/1000 | Loss: 0.00002406
Iteration 84/1000 | Loss: 0.00002405
Iteration 85/1000 | Loss: 0.00002405
Iteration 86/1000 | Loss: 0.00002405
Iteration 87/1000 | Loss: 0.00002404
Iteration 88/1000 | Loss: 0.00002404
Iteration 89/1000 | Loss: 0.00002404
Iteration 90/1000 | Loss: 0.00002403
Iteration 91/1000 | Loss: 0.00002403
Iteration 92/1000 | Loss: 0.00002403
Iteration 93/1000 | Loss: 0.00002402
Iteration 94/1000 | Loss: 0.00002402
Iteration 95/1000 | Loss: 0.00002402
Iteration 96/1000 | Loss: 0.00002402
Iteration 97/1000 | Loss: 0.00002401
Iteration 98/1000 | Loss: 0.00002401
Iteration 99/1000 | Loss: 0.00002400
Iteration 100/1000 | Loss: 0.00002400
Iteration 101/1000 | Loss: 0.00002400
Iteration 102/1000 | Loss: 0.00002399
Iteration 103/1000 | Loss: 0.00002399
Iteration 104/1000 | Loss: 0.00002399
Iteration 105/1000 | Loss: 0.00002399
Iteration 106/1000 | Loss: 0.00002398
Iteration 107/1000 | Loss: 0.00002398
Iteration 108/1000 | Loss: 0.00002398
Iteration 109/1000 | Loss: 0.00002397
Iteration 110/1000 | Loss: 0.00002397
Iteration 111/1000 | Loss: 0.00002397
Iteration 112/1000 | Loss: 0.00002397
Iteration 113/1000 | Loss: 0.00002396
Iteration 114/1000 | Loss: 0.00002396
Iteration 115/1000 | Loss: 0.00002396
Iteration 116/1000 | Loss: 0.00002395
Iteration 117/1000 | Loss: 0.00002395
Iteration 118/1000 | Loss: 0.00002395
Iteration 119/1000 | Loss: 0.00002395
Iteration 120/1000 | Loss: 0.00002394
Iteration 121/1000 | Loss: 0.00002394
Iteration 122/1000 | Loss: 0.00002394
Iteration 123/1000 | Loss: 0.00002393
Iteration 124/1000 | Loss: 0.00002393
Iteration 125/1000 | Loss: 0.00002393
Iteration 126/1000 | Loss: 0.00002393
Iteration 127/1000 | Loss: 0.00002393
Iteration 128/1000 | Loss: 0.00002392
Iteration 129/1000 | Loss: 0.00002392
Iteration 130/1000 | Loss: 0.00002392
Iteration 131/1000 | Loss: 0.00002392
Iteration 132/1000 | Loss: 0.00002392
Iteration 133/1000 | Loss: 0.00002392
Iteration 134/1000 | Loss: 0.00002392
Iteration 135/1000 | Loss: 0.00002392
Iteration 136/1000 | Loss: 0.00002392
Iteration 137/1000 | Loss: 0.00002392
Iteration 138/1000 | Loss: 0.00002392
Iteration 139/1000 | Loss: 0.00002392
Iteration 140/1000 | Loss: 0.00002392
Iteration 141/1000 | Loss: 0.00002391
Iteration 142/1000 | Loss: 0.00002391
Iteration 143/1000 | Loss: 0.00002391
Iteration 144/1000 | Loss: 0.00002391
Iteration 145/1000 | Loss: 0.00002391
Iteration 146/1000 | Loss: 0.00002391
Iteration 147/1000 | Loss: 0.00002391
Iteration 148/1000 | Loss: 0.00002391
Iteration 149/1000 | Loss: 0.00002391
Iteration 150/1000 | Loss: 0.00002391
Iteration 151/1000 | Loss: 0.00002390
Iteration 152/1000 | Loss: 0.00002390
Iteration 153/1000 | Loss: 0.00002390
Iteration 154/1000 | Loss: 0.00002390
Iteration 155/1000 | Loss: 0.00002389
Iteration 156/1000 | Loss: 0.00002389
Iteration 157/1000 | Loss: 0.00002389
Iteration 158/1000 | Loss: 0.00002389
Iteration 159/1000 | Loss: 0.00002389
Iteration 160/1000 | Loss: 0.00002389
Iteration 161/1000 | Loss: 0.00002389
Iteration 162/1000 | Loss: 0.00002388
Iteration 163/1000 | Loss: 0.00002388
Iteration 164/1000 | Loss: 0.00002388
Iteration 165/1000 | Loss: 0.00002388
Iteration 166/1000 | Loss: 0.00002388
Iteration 167/1000 | Loss: 0.00002388
Iteration 168/1000 | Loss: 0.00002388
Iteration 169/1000 | Loss: 0.00002388
Iteration 170/1000 | Loss: 0.00002387
Iteration 171/1000 | Loss: 0.00002387
Iteration 172/1000 | Loss: 0.00002387
Iteration 173/1000 | Loss: 0.00002387
Iteration 174/1000 | Loss: 0.00002387
Iteration 175/1000 | Loss: 0.00002386
Iteration 176/1000 | Loss: 0.00002386
Iteration 177/1000 | Loss: 0.00002386
Iteration 178/1000 | Loss: 0.00002386
Iteration 179/1000 | Loss: 0.00002386
Iteration 180/1000 | Loss: 0.00002385
Iteration 181/1000 | Loss: 0.00002385
Iteration 182/1000 | Loss: 0.00002385
Iteration 183/1000 | Loss: 0.00002385
Iteration 184/1000 | Loss: 0.00002385
Iteration 185/1000 | Loss: 0.00002385
Iteration 186/1000 | Loss: 0.00002385
Iteration 187/1000 | Loss: 0.00002385
Iteration 188/1000 | Loss: 0.00002385
Iteration 189/1000 | Loss: 0.00002385
Iteration 190/1000 | Loss: 0.00002385
Iteration 191/1000 | Loss: 0.00002385
Iteration 192/1000 | Loss: 0.00002384
Iteration 193/1000 | Loss: 0.00002384
Iteration 194/1000 | Loss: 0.00002384
Iteration 195/1000 | Loss: 0.00002384
Iteration 196/1000 | Loss: 0.00002384
Iteration 197/1000 | Loss: 0.00002384
Iteration 198/1000 | Loss: 0.00002384
Iteration 199/1000 | Loss: 0.00002384
Iteration 200/1000 | Loss: 0.00002384
Iteration 201/1000 | Loss: 0.00002384
Iteration 202/1000 | Loss: 0.00002384
Iteration 203/1000 | Loss: 0.00002384
Iteration 204/1000 | Loss: 0.00002383
Iteration 205/1000 | Loss: 0.00002383
Iteration 206/1000 | Loss: 0.00002383
Iteration 207/1000 | Loss: 0.00002383
Iteration 208/1000 | Loss: 0.00002383
Iteration 209/1000 | Loss: 0.00002383
Iteration 210/1000 | Loss: 0.00002383
Iteration 211/1000 | Loss: 0.00002383
Iteration 212/1000 | Loss: 0.00002383
Iteration 213/1000 | Loss: 0.00002383
Iteration 214/1000 | Loss: 0.00002383
Iteration 215/1000 | Loss: 0.00002383
Iteration 216/1000 | Loss: 0.00002383
Iteration 217/1000 | Loss: 0.00002383
Iteration 218/1000 | Loss: 0.00002383
Iteration 219/1000 | Loss: 0.00002383
Iteration 220/1000 | Loss: 0.00002382
Iteration 221/1000 | Loss: 0.00002382
Iteration 222/1000 | Loss: 0.00002382
Iteration 223/1000 | Loss: 0.00002382
Iteration 224/1000 | Loss: 0.00002382
Iteration 225/1000 | Loss: 0.00002382
Iteration 226/1000 | Loss: 0.00002382
Iteration 227/1000 | Loss: 0.00002382
Iteration 228/1000 | Loss: 0.00002382
Iteration 229/1000 | Loss: 0.00002382
Iteration 230/1000 | Loss: 0.00002382
Iteration 231/1000 | Loss: 0.00002382
Iteration 232/1000 | Loss: 0.00002382
Iteration 233/1000 | Loss: 0.00002382
Iteration 234/1000 | Loss: 0.00002382
Iteration 235/1000 | Loss: 0.00002382
Iteration 236/1000 | Loss: 0.00002382
Iteration 237/1000 | Loss: 0.00002382
Iteration 238/1000 | Loss: 0.00002382
Iteration 239/1000 | Loss: 0.00002381
Iteration 240/1000 | Loss: 0.00002381
Iteration 241/1000 | Loss: 0.00002381
Iteration 242/1000 | Loss: 0.00002381
Iteration 243/1000 | Loss: 0.00002381
Iteration 244/1000 | Loss: 0.00002381
Iteration 245/1000 | Loss: 0.00002381
Iteration 246/1000 | Loss: 0.00002381
Iteration 247/1000 | Loss: 0.00002381
Iteration 248/1000 | Loss: 0.00002381
Iteration 249/1000 | Loss: 0.00002381
Iteration 250/1000 | Loss: 0.00002381
Iteration 251/1000 | Loss: 0.00002381
Iteration 252/1000 | Loss: 0.00002381
Iteration 253/1000 | Loss: 0.00002381
Iteration 254/1000 | Loss: 0.00002381
Iteration 255/1000 | Loss: 0.00002381
Iteration 256/1000 | Loss: 0.00002381
Iteration 257/1000 | Loss: 0.00002381
Iteration 258/1000 | Loss: 0.00002381
Iteration 259/1000 | Loss: 0.00002381
Iteration 260/1000 | Loss: 0.00002381
Iteration 261/1000 | Loss: 0.00002381
Iteration 262/1000 | Loss: 0.00002381
Iteration 263/1000 | Loss: 0.00002381
Iteration 264/1000 | Loss: 0.00002381
Iteration 265/1000 | Loss: 0.00002381
Iteration 266/1000 | Loss: 0.00002381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [2.3809994672774337e-05, 2.3809994672774337e-05, 2.3809994672774337e-05, 2.3809994672774337e-05, 2.3809994672774337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3809994672774337e-05

Optimization complete. Final v2v error: 3.8746721744537354 mm

Highest mean error: 6.241508483886719 mm for frame 81

Lowest mean error: 3.1544196605682373 mm for frame 130

Saving results

Total time: 55.71055293083191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390255
Iteration 2/25 | Loss: 0.00090574
Iteration 3/25 | Loss: 0.00075517
Iteration 4/25 | Loss: 0.00073040
Iteration 5/25 | Loss: 0.00072467
Iteration 6/25 | Loss: 0.00072279
Iteration 7/25 | Loss: 0.00072216
Iteration 8/25 | Loss: 0.00072216
Iteration 9/25 | Loss: 0.00072216
Iteration 10/25 | Loss: 0.00072216
Iteration 11/25 | Loss: 0.00072216
Iteration 12/25 | Loss: 0.00072216
Iteration 13/25 | Loss: 0.00072216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007221560226753354, 0.0007221560226753354, 0.0007221560226753354, 0.0007221560226753354, 0.0007221560226753354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007221560226753354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59638441
Iteration 2/25 | Loss: 0.00114184
Iteration 3/25 | Loss: 0.00114184
Iteration 4/25 | Loss: 0.00114184
Iteration 5/25 | Loss: 0.00114184
Iteration 6/25 | Loss: 0.00114184
Iteration 7/25 | Loss: 0.00114184
Iteration 8/25 | Loss: 0.00114184
Iteration 9/25 | Loss: 0.00114184
Iteration 10/25 | Loss: 0.00114184
Iteration 11/25 | Loss: 0.00114184
Iteration 12/25 | Loss: 0.00114184
Iteration 13/25 | Loss: 0.00114184
Iteration 14/25 | Loss: 0.00114184
Iteration 15/25 | Loss: 0.00114184
Iteration 16/25 | Loss: 0.00114184
Iteration 17/25 | Loss: 0.00114184
Iteration 18/25 | Loss: 0.00114184
Iteration 19/25 | Loss: 0.00114184
Iteration 20/25 | Loss: 0.00114184
Iteration 21/25 | Loss: 0.00114184
Iteration 22/25 | Loss: 0.00114184
Iteration 23/25 | Loss: 0.00114184
Iteration 24/25 | Loss: 0.00114184
Iteration 25/25 | Loss: 0.00114184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114184
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001368
Iteration 4/1000 | Loss: 0.00001236
Iteration 5/1000 | Loss: 0.00001165
Iteration 6/1000 | Loss: 0.00001103
Iteration 7/1000 | Loss: 0.00001075
Iteration 8/1000 | Loss: 0.00001053
Iteration 9/1000 | Loss: 0.00001047
Iteration 10/1000 | Loss: 0.00001045
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001044
Iteration 13/1000 | Loss: 0.00001044
Iteration 14/1000 | Loss: 0.00001043
Iteration 15/1000 | Loss: 0.00001042
Iteration 16/1000 | Loss: 0.00001041
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001036
Iteration 21/1000 | Loss: 0.00001034
Iteration 22/1000 | Loss: 0.00001033
Iteration 23/1000 | Loss: 0.00001029
Iteration 24/1000 | Loss: 0.00001028
Iteration 25/1000 | Loss: 0.00001027
Iteration 26/1000 | Loss: 0.00001027
Iteration 27/1000 | Loss: 0.00001027
Iteration 28/1000 | Loss: 0.00001025
Iteration 29/1000 | Loss: 0.00001025
Iteration 30/1000 | Loss: 0.00001023
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001022
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001021
Iteration 36/1000 | Loss: 0.00001021
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001021
Iteration 39/1000 | Loss: 0.00001021
Iteration 40/1000 | Loss: 0.00001021
Iteration 41/1000 | Loss: 0.00001021
Iteration 42/1000 | Loss: 0.00001021
Iteration 43/1000 | Loss: 0.00001020
Iteration 44/1000 | Loss: 0.00001019
Iteration 45/1000 | Loss: 0.00001019
Iteration 46/1000 | Loss: 0.00001018
Iteration 47/1000 | Loss: 0.00001018
Iteration 48/1000 | Loss: 0.00001018
Iteration 49/1000 | Loss: 0.00001018
Iteration 50/1000 | Loss: 0.00001018
Iteration 51/1000 | Loss: 0.00001018
Iteration 52/1000 | Loss: 0.00001017
Iteration 53/1000 | Loss: 0.00001017
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001016
Iteration 56/1000 | Loss: 0.00001016
Iteration 57/1000 | Loss: 0.00001016
Iteration 58/1000 | Loss: 0.00001016
Iteration 59/1000 | Loss: 0.00001015
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001009
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001009
Iteration 75/1000 | Loss: 0.00001008
Iteration 76/1000 | Loss: 0.00001007
Iteration 77/1000 | Loss: 0.00001007
Iteration 78/1000 | Loss: 0.00001007
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001007
Iteration 81/1000 | Loss: 0.00001006
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001005
Iteration 87/1000 | Loss: 0.00001005
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001004
Iteration 90/1000 | Loss: 0.00001004
Iteration 91/1000 | Loss: 0.00001003
Iteration 92/1000 | Loss: 0.00001003
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001003
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001002
Iteration 98/1000 | Loss: 0.00001002
Iteration 99/1000 | Loss: 0.00001002
Iteration 100/1000 | Loss: 0.00001002
Iteration 101/1000 | Loss: 0.00001002
Iteration 102/1000 | Loss: 0.00001002
Iteration 103/1000 | Loss: 0.00001002
Iteration 104/1000 | Loss: 0.00001002
Iteration 105/1000 | Loss: 0.00001002
Iteration 106/1000 | Loss: 0.00001001
Iteration 107/1000 | Loss: 0.00001001
Iteration 108/1000 | Loss: 0.00001001
Iteration 109/1000 | Loss: 0.00001001
Iteration 110/1000 | Loss: 0.00001001
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001001
Iteration 113/1000 | Loss: 0.00001001
Iteration 114/1000 | Loss: 0.00001000
Iteration 115/1000 | Loss: 0.00001000
Iteration 116/1000 | Loss: 0.00001000
Iteration 117/1000 | Loss: 0.00001000
Iteration 118/1000 | Loss: 0.00001000
Iteration 119/1000 | Loss: 0.00001000
Iteration 120/1000 | Loss: 0.00001000
Iteration 121/1000 | Loss: 0.00000999
Iteration 122/1000 | Loss: 0.00000999
Iteration 123/1000 | Loss: 0.00000999
Iteration 124/1000 | Loss: 0.00000999
Iteration 125/1000 | Loss: 0.00000999
Iteration 126/1000 | Loss: 0.00000999
Iteration 127/1000 | Loss: 0.00000999
Iteration 128/1000 | Loss: 0.00000998
Iteration 129/1000 | Loss: 0.00000998
Iteration 130/1000 | Loss: 0.00000998
Iteration 131/1000 | Loss: 0.00000998
Iteration 132/1000 | Loss: 0.00000998
Iteration 133/1000 | Loss: 0.00000998
Iteration 134/1000 | Loss: 0.00000998
Iteration 135/1000 | Loss: 0.00000998
Iteration 136/1000 | Loss: 0.00000998
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000997
Iteration 139/1000 | Loss: 0.00000997
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000996
Iteration 146/1000 | Loss: 0.00000996
Iteration 147/1000 | Loss: 0.00000996
Iteration 148/1000 | Loss: 0.00000996
Iteration 149/1000 | Loss: 0.00000996
Iteration 150/1000 | Loss: 0.00000996
Iteration 151/1000 | Loss: 0.00000996
Iteration 152/1000 | Loss: 0.00000996
Iteration 153/1000 | Loss: 0.00000996
Iteration 154/1000 | Loss: 0.00000996
Iteration 155/1000 | Loss: 0.00000996
Iteration 156/1000 | Loss: 0.00000995
Iteration 157/1000 | Loss: 0.00000995
Iteration 158/1000 | Loss: 0.00000995
Iteration 159/1000 | Loss: 0.00000995
Iteration 160/1000 | Loss: 0.00000995
Iteration 161/1000 | Loss: 0.00000995
Iteration 162/1000 | Loss: 0.00000995
Iteration 163/1000 | Loss: 0.00000995
Iteration 164/1000 | Loss: 0.00000995
Iteration 165/1000 | Loss: 0.00000994
Iteration 166/1000 | Loss: 0.00000994
Iteration 167/1000 | Loss: 0.00000994
Iteration 168/1000 | Loss: 0.00000994
Iteration 169/1000 | Loss: 0.00000994
Iteration 170/1000 | Loss: 0.00000994
Iteration 171/1000 | Loss: 0.00000994
Iteration 172/1000 | Loss: 0.00000993
Iteration 173/1000 | Loss: 0.00000993
Iteration 174/1000 | Loss: 0.00000993
Iteration 175/1000 | Loss: 0.00000993
Iteration 176/1000 | Loss: 0.00000993
Iteration 177/1000 | Loss: 0.00000993
Iteration 178/1000 | Loss: 0.00000993
Iteration 179/1000 | Loss: 0.00000993
Iteration 180/1000 | Loss: 0.00000993
Iteration 181/1000 | Loss: 0.00000993
Iteration 182/1000 | Loss: 0.00000993
Iteration 183/1000 | Loss: 0.00000992
Iteration 184/1000 | Loss: 0.00000992
Iteration 185/1000 | Loss: 0.00000992
Iteration 186/1000 | Loss: 0.00000992
Iteration 187/1000 | Loss: 0.00000992
Iteration 188/1000 | Loss: 0.00000992
Iteration 189/1000 | Loss: 0.00000992
Iteration 190/1000 | Loss: 0.00000992
Iteration 191/1000 | Loss: 0.00000992
Iteration 192/1000 | Loss: 0.00000992
Iteration 193/1000 | Loss: 0.00000992
Iteration 194/1000 | Loss: 0.00000992
Iteration 195/1000 | Loss: 0.00000992
Iteration 196/1000 | Loss: 0.00000992
Iteration 197/1000 | Loss: 0.00000992
Iteration 198/1000 | Loss: 0.00000992
Iteration 199/1000 | Loss: 0.00000992
Iteration 200/1000 | Loss: 0.00000992
Iteration 201/1000 | Loss: 0.00000992
Iteration 202/1000 | Loss: 0.00000992
Iteration 203/1000 | Loss: 0.00000992
Iteration 204/1000 | Loss: 0.00000992
Iteration 205/1000 | Loss: 0.00000992
Iteration 206/1000 | Loss: 0.00000992
Iteration 207/1000 | Loss: 0.00000992
Iteration 208/1000 | Loss: 0.00000992
Iteration 209/1000 | Loss: 0.00000992
Iteration 210/1000 | Loss: 0.00000992
Iteration 211/1000 | Loss: 0.00000992
Iteration 212/1000 | Loss: 0.00000992
Iteration 213/1000 | Loss: 0.00000992
Iteration 214/1000 | Loss: 0.00000992
Iteration 215/1000 | Loss: 0.00000992
Iteration 216/1000 | Loss: 0.00000992
Iteration 217/1000 | Loss: 0.00000992
Iteration 218/1000 | Loss: 0.00000992
Iteration 219/1000 | Loss: 0.00000992
Iteration 220/1000 | Loss: 0.00000992
Iteration 221/1000 | Loss: 0.00000992
Iteration 222/1000 | Loss: 0.00000992
Iteration 223/1000 | Loss: 0.00000992
Iteration 224/1000 | Loss: 0.00000992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [9.916941962728743e-06, 9.916941962728743e-06, 9.916941962728743e-06, 9.916941962728743e-06, 9.916941962728743e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.916941962728743e-06

Optimization complete. Final v2v error: 2.6862800121307373 mm

Highest mean error: 3.460622787475586 mm for frame 64

Lowest mean error: 2.499380588531494 mm for frame 91

Saving results

Total time: 39.02851104736328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540074
Iteration 2/25 | Loss: 0.00119928
Iteration 3/25 | Loss: 0.00092750
Iteration 4/25 | Loss: 0.00089708
Iteration 5/25 | Loss: 0.00089003
Iteration 6/25 | Loss: 0.00088830
Iteration 7/25 | Loss: 0.00088800
Iteration 8/25 | Loss: 0.00088800
Iteration 9/25 | Loss: 0.00088800
Iteration 10/25 | Loss: 0.00088800
Iteration 11/25 | Loss: 0.00088800
Iteration 12/25 | Loss: 0.00088800
Iteration 13/25 | Loss: 0.00088800
Iteration 14/25 | Loss: 0.00088800
Iteration 15/25 | Loss: 0.00088800
Iteration 16/25 | Loss: 0.00088800
Iteration 17/25 | Loss: 0.00088800
Iteration 18/25 | Loss: 0.00088800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008880015811882913, 0.0008880015811882913, 0.0008880015811882913, 0.0008880015811882913, 0.0008880015811882913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008880015811882913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.33907413
Iteration 2/25 | Loss: 0.00129015
Iteration 3/25 | Loss: 0.00128998
Iteration 4/25 | Loss: 0.00128998
Iteration 5/25 | Loss: 0.00128998
Iteration 6/25 | Loss: 0.00128998
Iteration 7/25 | Loss: 0.00128997
Iteration 8/25 | Loss: 0.00128997
Iteration 9/25 | Loss: 0.00128997
Iteration 10/25 | Loss: 0.00128997
Iteration 11/25 | Loss: 0.00128997
Iteration 12/25 | Loss: 0.00128997
Iteration 13/25 | Loss: 0.00128997
Iteration 14/25 | Loss: 0.00128997
Iteration 15/25 | Loss: 0.00128997
Iteration 16/25 | Loss: 0.00128997
Iteration 17/25 | Loss: 0.00128997
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001289973733946681, 0.001289973733946681, 0.001289973733946681, 0.001289973733946681, 0.001289973733946681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001289973733946681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128997
Iteration 2/1000 | Loss: 0.00003581
Iteration 3/1000 | Loss: 0.00002792
Iteration 4/1000 | Loss: 0.00002589
Iteration 5/1000 | Loss: 0.00002480
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002359
Iteration 8/1000 | Loss: 0.00002337
Iteration 9/1000 | Loss: 0.00002327
Iteration 10/1000 | Loss: 0.00002327
Iteration 11/1000 | Loss: 0.00002325
Iteration 12/1000 | Loss: 0.00002325
Iteration 13/1000 | Loss: 0.00002321
Iteration 14/1000 | Loss: 0.00002315
Iteration 15/1000 | Loss: 0.00002309
Iteration 16/1000 | Loss: 0.00002298
Iteration 17/1000 | Loss: 0.00002293
Iteration 18/1000 | Loss: 0.00002293
Iteration 19/1000 | Loss: 0.00002293
Iteration 20/1000 | Loss: 0.00002292
Iteration 21/1000 | Loss: 0.00002292
Iteration 22/1000 | Loss: 0.00002292
Iteration 23/1000 | Loss: 0.00002292
Iteration 24/1000 | Loss: 0.00002292
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002292
Iteration 27/1000 | Loss: 0.00002291
Iteration 28/1000 | Loss: 0.00002290
Iteration 29/1000 | Loss: 0.00002289
Iteration 30/1000 | Loss: 0.00002288
Iteration 31/1000 | Loss: 0.00002288
Iteration 32/1000 | Loss: 0.00002288
Iteration 33/1000 | Loss: 0.00002287
Iteration 34/1000 | Loss: 0.00002287
Iteration 35/1000 | Loss: 0.00002287
Iteration 36/1000 | Loss: 0.00002287
Iteration 37/1000 | Loss: 0.00002287
Iteration 38/1000 | Loss: 0.00002287
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002287
Iteration 41/1000 | Loss: 0.00002287
Iteration 42/1000 | Loss: 0.00002287
Iteration 43/1000 | Loss: 0.00002285
Iteration 44/1000 | Loss: 0.00002285
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00002284
Iteration 47/1000 | Loss: 0.00002284
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002284
Iteration 51/1000 | Loss: 0.00002284
Iteration 52/1000 | Loss: 0.00002284
Iteration 53/1000 | Loss: 0.00002284
Iteration 54/1000 | Loss: 0.00002283
Iteration 55/1000 | Loss: 0.00002283
Iteration 56/1000 | Loss: 0.00002283
Iteration 57/1000 | Loss: 0.00002282
Iteration 58/1000 | Loss: 0.00002282
Iteration 59/1000 | Loss: 0.00002281
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002281
Iteration 62/1000 | Loss: 0.00002281
Iteration 63/1000 | Loss: 0.00002281
Iteration 64/1000 | Loss: 0.00002280
Iteration 65/1000 | Loss: 0.00002280
Iteration 66/1000 | Loss: 0.00002280
Iteration 67/1000 | Loss: 0.00002280
Iteration 68/1000 | Loss: 0.00002280
Iteration 69/1000 | Loss: 0.00002280
Iteration 70/1000 | Loss: 0.00002279
Iteration 71/1000 | Loss: 0.00002279
Iteration 72/1000 | Loss: 0.00002279
Iteration 73/1000 | Loss: 0.00002279
Iteration 74/1000 | Loss: 0.00002279
Iteration 75/1000 | Loss: 0.00002279
Iteration 76/1000 | Loss: 0.00002279
Iteration 77/1000 | Loss: 0.00002279
Iteration 78/1000 | Loss: 0.00002279
Iteration 79/1000 | Loss: 0.00002279
Iteration 80/1000 | Loss: 0.00002279
Iteration 81/1000 | Loss: 0.00002279
Iteration 82/1000 | Loss: 0.00002278
Iteration 83/1000 | Loss: 0.00002278
Iteration 84/1000 | Loss: 0.00002278
Iteration 85/1000 | Loss: 0.00002278
Iteration 86/1000 | Loss: 0.00002278
Iteration 87/1000 | Loss: 0.00002278
Iteration 88/1000 | Loss: 0.00002278
Iteration 89/1000 | Loss: 0.00002278
Iteration 90/1000 | Loss: 0.00002278
Iteration 91/1000 | Loss: 0.00002277
Iteration 92/1000 | Loss: 0.00002277
Iteration 93/1000 | Loss: 0.00002277
Iteration 94/1000 | Loss: 0.00002277
Iteration 95/1000 | Loss: 0.00002277
Iteration 96/1000 | Loss: 0.00002277
Iteration 97/1000 | Loss: 0.00002277
Iteration 98/1000 | Loss: 0.00002277
Iteration 99/1000 | Loss: 0.00002277
Iteration 100/1000 | Loss: 0.00002277
Iteration 101/1000 | Loss: 0.00002277
Iteration 102/1000 | Loss: 0.00002277
Iteration 103/1000 | Loss: 0.00002277
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00002277
Iteration 107/1000 | Loss: 0.00002277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.2769001589040272e-05, 2.2769001589040272e-05, 2.2769001589040272e-05, 2.2769001589040272e-05, 2.2769001589040272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2769001589040272e-05

Optimization complete. Final v2v error: 3.999171257019043 mm

Highest mean error: 4.50360631942749 mm for frame 58

Lowest mean error: 3.6179757118225098 mm for frame 51

Saving results

Total time: 31.509268045425415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067430
Iteration 2/25 | Loss: 0.00247699
Iteration 3/25 | Loss: 0.00295011
Iteration 4/25 | Loss: 0.00191861
Iteration 5/25 | Loss: 0.00118254
Iteration 6/25 | Loss: 0.00099585
Iteration 7/25 | Loss: 0.00098177
Iteration 8/25 | Loss: 0.00094741
Iteration 9/25 | Loss: 0.00090183
Iteration 10/25 | Loss: 0.00088638
Iteration 11/25 | Loss: 0.00087113
Iteration 12/25 | Loss: 0.00086439
Iteration 13/25 | Loss: 0.00086245
Iteration 14/25 | Loss: 0.00085953
Iteration 15/25 | Loss: 0.00085453
Iteration 16/25 | Loss: 0.00085338
Iteration 17/25 | Loss: 0.00085211
Iteration 18/25 | Loss: 0.00085178
Iteration 19/25 | Loss: 0.00085141
Iteration 20/25 | Loss: 0.00085059
Iteration 21/25 | Loss: 0.00085307
Iteration 22/25 | Loss: 0.00085294
Iteration 23/25 | Loss: 0.00085216
Iteration 24/25 | Loss: 0.00085167
Iteration 25/25 | Loss: 0.00085211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66137338
Iteration 2/25 | Loss: 0.00152333
Iteration 3/25 | Loss: 0.00132708
Iteration 4/25 | Loss: 0.00132708
Iteration 5/25 | Loss: 0.00132708
Iteration 6/25 | Loss: 0.00132708
Iteration 7/25 | Loss: 0.00132708
Iteration 8/25 | Loss: 0.00132708
Iteration 9/25 | Loss: 0.00132708
Iteration 10/25 | Loss: 0.00132708
Iteration 11/25 | Loss: 0.00132708
Iteration 12/25 | Loss: 0.00132708
Iteration 13/25 | Loss: 0.00132708
Iteration 14/25 | Loss: 0.00132708
Iteration 15/25 | Loss: 0.00132708
Iteration 16/25 | Loss: 0.00132708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001327081467024982, 0.001327081467024982, 0.001327081467024982, 0.001327081467024982, 0.001327081467024982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001327081467024982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132708
Iteration 2/1000 | Loss: 0.00012940
Iteration 3/1000 | Loss: 0.00013332
Iteration 4/1000 | Loss: 0.00018809
Iteration 5/1000 | Loss: 0.00003411
Iteration 6/1000 | Loss: 0.00010309
Iteration 7/1000 | Loss: 0.00017885
Iteration 8/1000 | Loss: 0.00013568
Iteration 9/1000 | Loss: 0.00017589
Iteration 10/1000 | Loss: 0.00020634
Iteration 11/1000 | Loss: 0.00017659
Iteration 12/1000 | Loss: 0.00014979
Iteration 13/1000 | Loss: 0.00013076
Iteration 14/1000 | Loss: 0.00011360
Iteration 15/1000 | Loss: 0.00020268
Iteration 16/1000 | Loss: 0.00006626
Iteration 17/1000 | Loss: 0.00012351
Iteration 18/1000 | Loss: 0.00009667
Iteration 19/1000 | Loss: 0.00011602
Iteration 20/1000 | Loss: 0.00014432
Iteration 21/1000 | Loss: 0.00017527
Iteration 22/1000 | Loss: 0.00012442
Iteration 23/1000 | Loss: 0.00010718
Iteration 24/1000 | Loss: 0.00009497
Iteration 25/1000 | Loss: 0.00010931
Iteration 26/1000 | Loss: 0.00021906
Iteration 27/1000 | Loss: 0.00009432
Iteration 28/1000 | Loss: 0.00018928
Iteration 29/1000 | Loss: 0.00018863
Iteration 30/1000 | Loss: 0.00011078
Iteration 31/1000 | Loss: 0.00022525
Iteration 32/1000 | Loss: 0.00002776
Iteration 33/1000 | Loss: 0.00002589
Iteration 34/1000 | Loss: 0.00002463
Iteration 35/1000 | Loss: 0.00002402
Iteration 36/1000 | Loss: 0.00002355
Iteration 37/1000 | Loss: 0.00054985
Iteration 38/1000 | Loss: 0.00045431
Iteration 39/1000 | Loss: 0.00036704
Iteration 40/1000 | Loss: 0.00003033
Iteration 41/1000 | Loss: 0.00002683
Iteration 42/1000 | Loss: 0.00002502
Iteration 43/1000 | Loss: 0.00019111
Iteration 44/1000 | Loss: 0.00002349
Iteration 45/1000 | Loss: 0.00014687
Iteration 46/1000 | Loss: 0.00002370
Iteration 47/1000 | Loss: 0.00002228
Iteration 48/1000 | Loss: 0.00036948
Iteration 49/1000 | Loss: 0.00003288
Iteration 50/1000 | Loss: 0.00018344
Iteration 51/1000 | Loss: 0.00002264
Iteration 52/1000 | Loss: 0.00002131
Iteration 53/1000 | Loss: 0.00002060
Iteration 54/1000 | Loss: 0.00002011
Iteration 55/1000 | Loss: 0.00001983
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001959
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001948
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001935
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001921
Iteration 74/1000 | Loss: 0.00001921
Iteration 75/1000 | Loss: 0.00001921
Iteration 76/1000 | Loss: 0.00001921
Iteration 77/1000 | Loss: 0.00001920
Iteration 78/1000 | Loss: 0.00001919
Iteration 79/1000 | Loss: 0.00001918
Iteration 80/1000 | Loss: 0.00001918
Iteration 81/1000 | Loss: 0.00001918
Iteration 82/1000 | Loss: 0.00001918
Iteration 83/1000 | Loss: 0.00001918
Iteration 84/1000 | Loss: 0.00001918
Iteration 85/1000 | Loss: 0.00001918
Iteration 86/1000 | Loss: 0.00001918
Iteration 87/1000 | Loss: 0.00001918
Iteration 88/1000 | Loss: 0.00001918
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001918
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001917
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001916
Iteration 98/1000 | Loss: 0.00001916
Iteration 99/1000 | Loss: 0.00001915
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001912
Iteration 110/1000 | Loss: 0.00001911
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001911
Iteration 116/1000 | Loss: 0.00001911
Iteration 117/1000 | Loss: 0.00001911
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Iteration 120/1000 | Loss: 0.00001910
Iteration 121/1000 | Loss: 0.00001909
Iteration 122/1000 | Loss: 0.00001909
Iteration 123/1000 | Loss: 0.00001909
Iteration 124/1000 | Loss: 0.00001909
Iteration 125/1000 | Loss: 0.00001909
Iteration 126/1000 | Loss: 0.00001909
Iteration 127/1000 | Loss: 0.00001908
Iteration 128/1000 | Loss: 0.00001908
Iteration 129/1000 | Loss: 0.00001908
Iteration 130/1000 | Loss: 0.00001908
Iteration 131/1000 | Loss: 0.00001908
Iteration 132/1000 | Loss: 0.00001908
Iteration 133/1000 | Loss: 0.00001908
Iteration 134/1000 | Loss: 0.00001908
Iteration 135/1000 | Loss: 0.00001908
Iteration 136/1000 | Loss: 0.00001908
Iteration 137/1000 | Loss: 0.00001908
Iteration 138/1000 | Loss: 0.00001908
Iteration 139/1000 | Loss: 0.00001908
Iteration 140/1000 | Loss: 0.00001908
Iteration 141/1000 | Loss: 0.00001908
Iteration 142/1000 | Loss: 0.00001908
Iteration 143/1000 | Loss: 0.00001908
Iteration 144/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.9083478036918677e-05, 1.9083478036918677e-05, 1.9083478036918677e-05, 1.9083478036918677e-05, 1.9083478036918677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9083478036918677e-05

Optimization complete. Final v2v error: 3.6032660007476807 mm

Highest mean error: 5.755490303039551 mm for frame 110

Lowest mean error: 2.998044967651367 mm for frame 168

Saving results

Total time: 155.27384114265442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718599
Iteration 2/25 | Loss: 0.00128414
Iteration 3/25 | Loss: 0.00095209
Iteration 4/25 | Loss: 0.00089421
Iteration 5/25 | Loss: 0.00088245
Iteration 6/25 | Loss: 0.00086326
Iteration 7/25 | Loss: 0.00085858
Iteration 8/25 | Loss: 0.00087441
Iteration 9/25 | Loss: 0.00085086
Iteration 10/25 | Loss: 0.00084854
Iteration 11/25 | Loss: 0.00085179
Iteration 12/25 | Loss: 0.00085208
Iteration 13/25 | Loss: 0.00084772
Iteration 14/25 | Loss: 0.00084668
Iteration 15/25 | Loss: 0.00084615
Iteration 16/25 | Loss: 0.00084588
Iteration 17/25 | Loss: 0.00084578
Iteration 18/25 | Loss: 0.00084571
Iteration 19/25 | Loss: 0.00084570
Iteration 20/25 | Loss: 0.00084566
Iteration 21/25 | Loss: 0.00084566
Iteration 22/25 | Loss: 0.00084566
Iteration 23/25 | Loss: 0.00084566
Iteration 24/25 | Loss: 0.00084566
Iteration 25/25 | Loss: 0.00084566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72428274
Iteration 2/25 | Loss: 0.00153835
Iteration 3/25 | Loss: 0.00153834
Iteration 4/25 | Loss: 0.00153834
Iteration 5/25 | Loss: 0.00153834
Iteration 6/25 | Loss: 0.00153834
Iteration 7/25 | Loss: 0.00153834
Iteration 8/25 | Loss: 0.00153834
Iteration 9/25 | Loss: 0.00153834
Iteration 10/25 | Loss: 0.00153834
Iteration 11/25 | Loss: 0.00153834
Iteration 12/25 | Loss: 0.00153834
Iteration 13/25 | Loss: 0.00153834
Iteration 14/25 | Loss: 0.00153834
Iteration 15/25 | Loss: 0.00153834
Iteration 16/25 | Loss: 0.00153834
Iteration 17/25 | Loss: 0.00153834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001538340700790286, 0.001538340700790286, 0.001538340700790286, 0.001538340700790286, 0.001538340700790286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001538340700790286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153834
Iteration 2/1000 | Loss: 0.00004171
Iteration 3/1000 | Loss: 0.00002911
Iteration 4/1000 | Loss: 0.00002651
Iteration 5/1000 | Loss: 0.00002506
Iteration 6/1000 | Loss: 0.00002405
Iteration 7/1000 | Loss: 0.00002346
Iteration 8/1000 | Loss: 0.00002292
Iteration 9/1000 | Loss: 0.00002244
Iteration 10/1000 | Loss: 0.00002217
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00002172
Iteration 13/1000 | Loss: 0.00002168
Iteration 14/1000 | Loss: 0.00002167
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002158
Iteration 17/1000 | Loss: 0.00002157
Iteration 18/1000 | Loss: 0.00002155
Iteration 19/1000 | Loss: 0.00002152
Iteration 20/1000 | Loss: 0.00002150
Iteration 21/1000 | Loss: 0.00002149
Iteration 22/1000 | Loss: 0.00002148
Iteration 23/1000 | Loss: 0.00002147
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00002145
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002143
Iteration 29/1000 | Loss: 0.00002143
Iteration 30/1000 | Loss: 0.00002142
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002141
Iteration 33/1000 | Loss: 0.00002141
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002140
Iteration 36/1000 | Loss: 0.00002140
Iteration 37/1000 | Loss: 0.00002139
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002137
Iteration 41/1000 | Loss: 0.00002137
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002136
Iteration 44/1000 | Loss: 0.00002136
Iteration 45/1000 | Loss: 0.00002134
Iteration 46/1000 | Loss: 0.00002134
Iteration 47/1000 | Loss: 0.00002134
Iteration 48/1000 | Loss: 0.00002134
Iteration 49/1000 | Loss: 0.00002133
Iteration 50/1000 | Loss: 0.00002133
Iteration 51/1000 | Loss: 0.00002132
Iteration 52/1000 | Loss: 0.00002132
Iteration 53/1000 | Loss: 0.00002132
Iteration 54/1000 | Loss: 0.00002131
Iteration 55/1000 | Loss: 0.00002131
Iteration 56/1000 | Loss: 0.00002130
Iteration 57/1000 | Loss: 0.00002130
Iteration 58/1000 | Loss: 0.00002130
Iteration 59/1000 | Loss: 0.00002129
Iteration 60/1000 | Loss: 0.00002129
Iteration 61/1000 | Loss: 0.00002129
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002127
Iteration 65/1000 | Loss: 0.00002127
Iteration 66/1000 | Loss: 0.00002127
Iteration 67/1000 | Loss: 0.00002126
Iteration 68/1000 | Loss: 0.00002126
Iteration 69/1000 | Loss: 0.00002126
Iteration 70/1000 | Loss: 0.00002126
Iteration 71/1000 | Loss: 0.00002125
Iteration 72/1000 | Loss: 0.00002125
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002124
Iteration 75/1000 | Loss: 0.00002124
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002123
Iteration 78/1000 | Loss: 0.00002123
Iteration 79/1000 | Loss: 0.00002122
Iteration 80/1000 | Loss: 0.00002122
Iteration 81/1000 | Loss: 0.00002122
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002121
Iteration 87/1000 | Loss: 0.00002121
Iteration 88/1000 | Loss: 0.00002121
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002120
Iteration 97/1000 | Loss: 0.00002120
Iteration 98/1000 | Loss: 0.00002120
Iteration 99/1000 | Loss: 0.00002120
Iteration 100/1000 | Loss: 0.00002119
Iteration 101/1000 | Loss: 0.00002119
Iteration 102/1000 | Loss: 0.00002119
Iteration 103/1000 | Loss: 0.00002119
Iteration 104/1000 | Loss: 0.00002119
Iteration 105/1000 | Loss: 0.00002119
Iteration 106/1000 | Loss: 0.00002119
Iteration 107/1000 | Loss: 0.00002119
Iteration 108/1000 | Loss: 0.00002119
Iteration 109/1000 | Loss: 0.00002118
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00002118
Iteration 112/1000 | Loss: 0.00002118
Iteration 113/1000 | Loss: 0.00002118
Iteration 114/1000 | Loss: 0.00002118
Iteration 115/1000 | Loss: 0.00002118
Iteration 116/1000 | Loss: 0.00002118
Iteration 117/1000 | Loss: 0.00002118
Iteration 118/1000 | Loss: 0.00002118
Iteration 119/1000 | Loss: 0.00002117
Iteration 120/1000 | Loss: 0.00002117
Iteration 121/1000 | Loss: 0.00002117
Iteration 122/1000 | Loss: 0.00002117
Iteration 123/1000 | Loss: 0.00002117
Iteration 124/1000 | Loss: 0.00002117
Iteration 125/1000 | Loss: 0.00002117
Iteration 126/1000 | Loss: 0.00002117
Iteration 127/1000 | Loss: 0.00002117
Iteration 128/1000 | Loss: 0.00002117
Iteration 129/1000 | Loss: 0.00002117
Iteration 130/1000 | Loss: 0.00002117
Iteration 131/1000 | Loss: 0.00002117
Iteration 132/1000 | Loss: 0.00002117
Iteration 133/1000 | Loss: 0.00002116
Iteration 134/1000 | Loss: 0.00002116
Iteration 135/1000 | Loss: 0.00002116
Iteration 136/1000 | Loss: 0.00002116
Iteration 137/1000 | Loss: 0.00002116
Iteration 138/1000 | Loss: 0.00002116
Iteration 139/1000 | Loss: 0.00002115
Iteration 140/1000 | Loss: 0.00002115
Iteration 141/1000 | Loss: 0.00002115
Iteration 142/1000 | Loss: 0.00002115
Iteration 143/1000 | Loss: 0.00002115
Iteration 144/1000 | Loss: 0.00002115
Iteration 145/1000 | Loss: 0.00002115
Iteration 146/1000 | Loss: 0.00002114
Iteration 147/1000 | Loss: 0.00002114
Iteration 148/1000 | Loss: 0.00002114
Iteration 149/1000 | Loss: 0.00002114
Iteration 150/1000 | Loss: 0.00002114
Iteration 151/1000 | Loss: 0.00002114
Iteration 152/1000 | Loss: 0.00002114
Iteration 153/1000 | Loss: 0.00002113
Iteration 154/1000 | Loss: 0.00002113
Iteration 155/1000 | Loss: 0.00002113
Iteration 156/1000 | Loss: 0.00002113
Iteration 157/1000 | Loss: 0.00002113
Iteration 158/1000 | Loss: 0.00002113
Iteration 159/1000 | Loss: 0.00002113
Iteration 160/1000 | Loss: 0.00002113
Iteration 161/1000 | Loss: 0.00002113
Iteration 162/1000 | Loss: 0.00002113
Iteration 163/1000 | Loss: 0.00002112
Iteration 164/1000 | Loss: 0.00002112
Iteration 165/1000 | Loss: 0.00002112
Iteration 166/1000 | Loss: 0.00002112
Iteration 167/1000 | Loss: 0.00002112
Iteration 168/1000 | Loss: 0.00002112
Iteration 169/1000 | Loss: 0.00002112
Iteration 170/1000 | Loss: 0.00002112
Iteration 171/1000 | Loss: 0.00002112
Iteration 172/1000 | Loss: 0.00002112
Iteration 173/1000 | Loss: 0.00002112
Iteration 174/1000 | Loss: 0.00002112
Iteration 175/1000 | Loss: 0.00002112
Iteration 176/1000 | Loss: 0.00002112
Iteration 177/1000 | Loss: 0.00002112
Iteration 178/1000 | Loss: 0.00002112
Iteration 179/1000 | Loss: 0.00002112
Iteration 180/1000 | Loss: 0.00002112
Iteration 181/1000 | Loss: 0.00002112
Iteration 182/1000 | Loss: 0.00002112
Iteration 183/1000 | Loss: 0.00002112
Iteration 184/1000 | Loss: 0.00002112
Iteration 185/1000 | Loss: 0.00002112
Iteration 186/1000 | Loss: 0.00002112
Iteration 187/1000 | Loss: 0.00002112
Iteration 188/1000 | Loss: 0.00002112
Iteration 189/1000 | Loss: 0.00002112
Iteration 190/1000 | Loss: 0.00002112
Iteration 191/1000 | Loss: 0.00002112
Iteration 192/1000 | Loss: 0.00002112
Iteration 193/1000 | Loss: 0.00002112
Iteration 194/1000 | Loss: 0.00002112
Iteration 195/1000 | Loss: 0.00002112
Iteration 196/1000 | Loss: 0.00002112
Iteration 197/1000 | Loss: 0.00002112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.1120884412084706e-05, 2.1120884412084706e-05, 2.1120884412084706e-05, 2.1120884412084706e-05, 2.1120884412084706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1120884412084706e-05

Optimization complete. Final v2v error: 3.838120222091675 mm

Highest mean error: 4.304938793182373 mm for frame 83

Lowest mean error: 3.4089057445526123 mm for frame 20

Saving results

Total time: 72.64108681678772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510274
Iteration 2/25 | Loss: 0.00106551
Iteration 3/25 | Loss: 0.00087516
Iteration 4/25 | Loss: 0.00079311
Iteration 5/25 | Loss: 0.00077282
Iteration 6/25 | Loss: 0.00075979
Iteration 7/25 | Loss: 0.00075644
Iteration 8/25 | Loss: 0.00075600
Iteration 9/25 | Loss: 0.00075587
Iteration 10/25 | Loss: 0.00075577
Iteration 11/25 | Loss: 0.00075576
Iteration 12/25 | Loss: 0.00075576
Iteration 13/25 | Loss: 0.00075575
Iteration 14/25 | Loss: 0.00075575
Iteration 15/25 | Loss: 0.00075575
Iteration 16/25 | Loss: 0.00075575
Iteration 17/25 | Loss: 0.00075575
Iteration 18/25 | Loss: 0.00075575
Iteration 19/25 | Loss: 0.00075575
Iteration 20/25 | Loss: 0.00075575
Iteration 21/25 | Loss: 0.00075575
Iteration 22/25 | Loss: 0.00075575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007557534263469279, 0.0007557534263469279, 0.0007557534263469279, 0.0007557534263469279, 0.0007557534263469279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007557534263469279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.14963579
Iteration 2/25 | Loss: 0.00136079
Iteration 3/25 | Loss: 0.00136076
Iteration 4/25 | Loss: 0.00136076
Iteration 5/25 | Loss: 0.00136076
Iteration 6/25 | Loss: 0.00136076
Iteration 7/25 | Loss: 0.00136076
Iteration 8/25 | Loss: 0.00136076
Iteration 9/25 | Loss: 0.00136076
Iteration 10/25 | Loss: 0.00136076
Iteration 11/25 | Loss: 0.00136076
Iteration 12/25 | Loss: 0.00136076
Iteration 13/25 | Loss: 0.00136076
Iteration 14/25 | Loss: 0.00136076
Iteration 15/25 | Loss: 0.00136076
Iteration 16/25 | Loss: 0.00136076
Iteration 17/25 | Loss: 0.00136076
Iteration 18/25 | Loss: 0.00136076
Iteration 19/25 | Loss: 0.00136076
Iteration 20/25 | Loss: 0.00136076
Iteration 21/25 | Loss: 0.00136076
Iteration 22/25 | Loss: 0.00136076
Iteration 23/25 | Loss: 0.00136076
Iteration 24/25 | Loss: 0.00136076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013607590226456523, 0.0013607590226456523, 0.0013607590226456523, 0.0013607590226456523, 0.0013607590226456523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013607590226456523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136076
Iteration 2/1000 | Loss: 0.00002671
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001393
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001366
Iteration 12/1000 | Loss: 0.00001351
Iteration 13/1000 | Loss: 0.00001343
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001330
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001330
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001327
Iteration 48/1000 | Loss: 0.00001327
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001325
Iteration 60/1000 | Loss: 0.00001325
Iteration 61/1000 | Loss: 0.00001324
Iteration 62/1000 | Loss: 0.00001324
Iteration 63/1000 | Loss: 0.00001324
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001323
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001319
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001318
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001315
Iteration 88/1000 | Loss: 0.00001315
Iteration 89/1000 | Loss: 0.00001315
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001314
Iteration 92/1000 | Loss: 0.00001314
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001310
Iteration 124/1000 | Loss: 0.00001310
Iteration 125/1000 | Loss: 0.00001310
Iteration 126/1000 | Loss: 0.00001310
Iteration 127/1000 | Loss: 0.00001310
Iteration 128/1000 | Loss: 0.00001310
Iteration 129/1000 | Loss: 0.00001310
Iteration 130/1000 | Loss: 0.00001310
Iteration 131/1000 | Loss: 0.00001310
Iteration 132/1000 | Loss: 0.00001310
Iteration 133/1000 | Loss: 0.00001310
Iteration 134/1000 | Loss: 0.00001310
Iteration 135/1000 | Loss: 0.00001310
Iteration 136/1000 | Loss: 0.00001310
Iteration 137/1000 | Loss: 0.00001310
Iteration 138/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.3097671399009414e-05, 1.3097671399009414e-05, 1.3097671399009414e-05, 1.3097671399009414e-05, 1.3097671399009414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3097671399009414e-05

Optimization complete. Final v2v error: 3.0572569370269775 mm

Highest mean error: 3.4739696979522705 mm for frame 13

Lowest mean error: 2.691730260848999 mm for frame 32

Saving results

Total time: 49.493770599365234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016548
Iteration 2/25 | Loss: 0.00155220
Iteration 3/25 | Loss: 0.00107888
Iteration 4/25 | Loss: 0.00104554
Iteration 5/25 | Loss: 0.00103437
Iteration 6/25 | Loss: 0.00103210
Iteration 7/25 | Loss: 0.00103179
Iteration 8/25 | Loss: 0.00103179
Iteration 9/25 | Loss: 0.00103179
Iteration 10/25 | Loss: 0.00103179
Iteration 11/25 | Loss: 0.00103179
Iteration 12/25 | Loss: 0.00103179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010317910928279161, 0.0010317910928279161, 0.0010317910928279161, 0.0010317910928279161, 0.0010317910928279161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010317910928279161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65572858
Iteration 2/25 | Loss: 0.00081805
Iteration 3/25 | Loss: 0.00081805
Iteration 4/25 | Loss: 0.00081805
Iteration 5/25 | Loss: 0.00081805
Iteration 6/25 | Loss: 0.00081805
Iteration 7/25 | Loss: 0.00081805
Iteration 8/25 | Loss: 0.00081805
Iteration 9/25 | Loss: 0.00081805
Iteration 10/25 | Loss: 0.00081805
Iteration 11/25 | Loss: 0.00081805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008180482545867562, 0.0008180482545867562, 0.0008180482545867562, 0.0008180482545867562, 0.0008180482545867562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008180482545867562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081805
Iteration 2/1000 | Loss: 0.00007464
Iteration 3/1000 | Loss: 0.00005189
Iteration 4/1000 | Loss: 0.00004847
Iteration 5/1000 | Loss: 0.00004659
Iteration 6/1000 | Loss: 0.00004544
Iteration 7/1000 | Loss: 0.00004456
Iteration 8/1000 | Loss: 0.00004407
Iteration 9/1000 | Loss: 0.00004370
Iteration 10/1000 | Loss: 0.00004335
Iteration 11/1000 | Loss: 0.00004300
Iteration 12/1000 | Loss: 0.00004269
Iteration 13/1000 | Loss: 0.00004250
Iteration 14/1000 | Loss: 0.00004233
Iteration 15/1000 | Loss: 0.00004218
Iteration 16/1000 | Loss: 0.00004200
Iteration 17/1000 | Loss: 0.00004200
Iteration 18/1000 | Loss: 0.00004199
Iteration 19/1000 | Loss: 0.00004196
Iteration 20/1000 | Loss: 0.00004191
Iteration 21/1000 | Loss: 0.00004187
Iteration 22/1000 | Loss: 0.00004187
Iteration 23/1000 | Loss: 0.00004178
Iteration 24/1000 | Loss: 0.00004178
Iteration 25/1000 | Loss: 0.00004177
Iteration 26/1000 | Loss: 0.00004176
Iteration 27/1000 | Loss: 0.00004172
Iteration 28/1000 | Loss: 0.00004171
Iteration 29/1000 | Loss: 0.00004167
Iteration 30/1000 | Loss: 0.00004167
Iteration 31/1000 | Loss: 0.00004167
Iteration 32/1000 | Loss: 0.00004167
Iteration 33/1000 | Loss: 0.00004167
Iteration 34/1000 | Loss: 0.00004167
Iteration 35/1000 | Loss: 0.00004167
Iteration 36/1000 | Loss: 0.00004163
Iteration 37/1000 | Loss: 0.00004163
Iteration 38/1000 | Loss: 0.00004163
Iteration 39/1000 | Loss: 0.00004163
Iteration 40/1000 | Loss: 0.00004163
Iteration 41/1000 | Loss: 0.00004163
Iteration 42/1000 | Loss: 0.00004162
Iteration 43/1000 | Loss: 0.00004162
Iteration 44/1000 | Loss: 0.00004158
Iteration 45/1000 | Loss: 0.00004158
Iteration 46/1000 | Loss: 0.00004158
Iteration 47/1000 | Loss: 0.00004157
Iteration 48/1000 | Loss: 0.00004157
Iteration 49/1000 | Loss: 0.00004156
Iteration 50/1000 | Loss: 0.00004156
Iteration 51/1000 | Loss: 0.00004156
Iteration 52/1000 | Loss: 0.00004156
Iteration 53/1000 | Loss: 0.00004156
Iteration 54/1000 | Loss: 0.00004156
Iteration 55/1000 | Loss: 0.00004156
Iteration 56/1000 | Loss: 0.00004156
Iteration 57/1000 | Loss: 0.00004156
Iteration 58/1000 | Loss: 0.00004155
Iteration 59/1000 | Loss: 0.00004155
Iteration 60/1000 | Loss: 0.00004155
Iteration 61/1000 | Loss: 0.00004155
Iteration 62/1000 | Loss: 0.00004155
Iteration 63/1000 | Loss: 0.00004155
Iteration 64/1000 | Loss: 0.00004155
Iteration 65/1000 | Loss: 0.00004154
Iteration 66/1000 | Loss: 0.00004154
Iteration 67/1000 | Loss: 0.00004154
Iteration 68/1000 | Loss: 0.00004154
Iteration 69/1000 | Loss: 0.00004154
Iteration 70/1000 | Loss: 0.00004154
Iteration 71/1000 | Loss: 0.00004154
Iteration 72/1000 | Loss: 0.00004154
Iteration 73/1000 | Loss: 0.00004154
Iteration 74/1000 | Loss: 0.00004154
Iteration 75/1000 | Loss: 0.00004154
Iteration 76/1000 | Loss: 0.00004153
Iteration 77/1000 | Loss: 0.00004153
Iteration 78/1000 | Loss: 0.00004153
Iteration 79/1000 | Loss: 0.00004153
Iteration 80/1000 | Loss: 0.00004153
Iteration 81/1000 | Loss: 0.00004152
Iteration 82/1000 | Loss: 0.00004152
Iteration 83/1000 | Loss: 0.00004152
Iteration 84/1000 | Loss: 0.00004152
Iteration 85/1000 | Loss: 0.00004152
Iteration 86/1000 | Loss: 0.00004152
Iteration 87/1000 | Loss: 0.00004151
Iteration 88/1000 | Loss: 0.00004151
Iteration 89/1000 | Loss: 0.00004151
Iteration 90/1000 | Loss: 0.00004151
Iteration 91/1000 | Loss: 0.00004151
Iteration 92/1000 | Loss: 0.00004151
Iteration 93/1000 | Loss: 0.00004151
Iteration 94/1000 | Loss: 0.00004151
Iteration 95/1000 | Loss: 0.00004151
Iteration 96/1000 | Loss: 0.00004151
Iteration 97/1000 | Loss: 0.00004151
Iteration 98/1000 | Loss: 0.00004150
Iteration 99/1000 | Loss: 0.00004150
Iteration 100/1000 | Loss: 0.00004150
Iteration 101/1000 | Loss: 0.00004150
Iteration 102/1000 | Loss: 0.00004150
Iteration 103/1000 | Loss: 0.00004150
Iteration 104/1000 | Loss: 0.00004150
Iteration 105/1000 | Loss: 0.00004150
Iteration 106/1000 | Loss: 0.00004150
Iteration 107/1000 | Loss: 0.00004150
Iteration 108/1000 | Loss: 0.00004150
Iteration 109/1000 | Loss: 0.00004150
Iteration 110/1000 | Loss: 0.00004150
Iteration 111/1000 | Loss: 0.00004150
Iteration 112/1000 | Loss: 0.00004150
Iteration 113/1000 | Loss: 0.00004150
Iteration 114/1000 | Loss: 0.00004150
Iteration 115/1000 | Loss: 0.00004150
Iteration 116/1000 | Loss: 0.00004150
Iteration 117/1000 | Loss: 0.00004150
Iteration 118/1000 | Loss: 0.00004150
Iteration 119/1000 | Loss: 0.00004150
Iteration 120/1000 | Loss: 0.00004150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [4.150320091866888e-05, 4.150320091866888e-05, 4.150320091866888e-05, 4.150320091866888e-05, 4.150320091866888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.150320091866888e-05

Optimization complete. Final v2v error: 5.261871814727783 mm

Highest mean error: 6.237172603607178 mm for frame 6

Lowest mean error: 4.814755916595459 mm for frame 63

Saving results

Total time: 42.335339069366455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492738
Iteration 2/25 | Loss: 0.00089429
Iteration 3/25 | Loss: 0.00077194
Iteration 4/25 | Loss: 0.00075067
Iteration 5/25 | Loss: 0.00074549
Iteration 6/25 | Loss: 0.00074419
Iteration 7/25 | Loss: 0.00074406
Iteration 8/25 | Loss: 0.00074406
Iteration 9/25 | Loss: 0.00074406
Iteration 10/25 | Loss: 0.00074406
Iteration 11/25 | Loss: 0.00074406
Iteration 12/25 | Loss: 0.00074406
Iteration 13/25 | Loss: 0.00074406
Iteration 14/25 | Loss: 0.00074406
Iteration 15/25 | Loss: 0.00074406
Iteration 16/25 | Loss: 0.00074406
Iteration 17/25 | Loss: 0.00074406
Iteration 18/25 | Loss: 0.00074406
Iteration 19/25 | Loss: 0.00074406
Iteration 20/25 | Loss: 0.00074406
Iteration 21/25 | Loss: 0.00074406
Iteration 22/25 | Loss: 0.00074406
Iteration 23/25 | Loss: 0.00074406
Iteration 24/25 | Loss: 0.00074406
Iteration 25/25 | Loss: 0.00074406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007440635818056762, 0.0007440635818056762, 0.0007440635818056762, 0.0007440635818056762, 0.0007440635818056762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007440635818056762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46955216
Iteration 2/25 | Loss: 0.00109873
Iteration 3/25 | Loss: 0.00109869
Iteration 4/25 | Loss: 0.00109869
Iteration 5/25 | Loss: 0.00109869
Iteration 6/25 | Loss: 0.00109869
Iteration 7/25 | Loss: 0.00109869
Iteration 8/25 | Loss: 0.00109869
Iteration 9/25 | Loss: 0.00109869
Iteration 10/25 | Loss: 0.00109869
Iteration 11/25 | Loss: 0.00109869
Iteration 12/25 | Loss: 0.00109869
Iteration 13/25 | Loss: 0.00109869
Iteration 14/25 | Loss: 0.00109869
Iteration 15/25 | Loss: 0.00109869
Iteration 16/25 | Loss: 0.00109869
Iteration 17/25 | Loss: 0.00109869
Iteration 18/25 | Loss: 0.00109869
Iteration 19/25 | Loss: 0.00109869
Iteration 20/25 | Loss: 0.00109869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010986889246851206, 0.0010986889246851206, 0.0010986889246851206, 0.0010986889246851206, 0.0010986889246851206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010986889246851206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109869
Iteration 2/1000 | Loss: 0.00003120
Iteration 3/1000 | Loss: 0.00001906
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001529
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001411
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001364
Iteration 10/1000 | Loss: 0.00001347
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001346
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001338
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001333
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001327
Iteration 26/1000 | Loss: 0.00001326
Iteration 27/1000 | Loss: 0.00001326
Iteration 28/1000 | Loss: 0.00001325
Iteration 29/1000 | Loss: 0.00001325
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001321
Iteration 38/1000 | Loss: 0.00001321
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001320
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001319
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001319
Iteration 49/1000 | Loss: 0.00001319
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001312
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001311
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001311
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001310
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001308
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001308
Iteration 98/1000 | Loss: 0.00001308
Iteration 99/1000 | Loss: 0.00001308
Iteration 100/1000 | Loss: 0.00001308
Iteration 101/1000 | Loss: 0.00001308
Iteration 102/1000 | Loss: 0.00001307
Iteration 103/1000 | Loss: 0.00001307
Iteration 104/1000 | Loss: 0.00001307
Iteration 105/1000 | Loss: 0.00001307
Iteration 106/1000 | Loss: 0.00001307
Iteration 107/1000 | Loss: 0.00001307
Iteration 108/1000 | Loss: 0.00001307
Iteration 109/1000 | Loss: 0.00001306
Iteration 110/1000 | Loss: 0.00001306
Iteration 111/1000 | Loss: 0.00001306
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001303
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001302
Iteration 133/1000 | Loss: 0.00001302
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001301
Iteration 138/1000 | Loss: 0.00001301
Iteration 139/1000 | Loss: 0.00001301
Iteration 140/1000 | Loss: 0.00001301
Iteration 141/1000 | Loss: 0.00001301
Iteration 142/1000 | Loss: 0.00001301
Iteration 143/1000 | Loss: 0.00001301
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001299
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001299
Iteration 157/1000 | Loss: 0.00001299
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001299
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001299
Iteration 171/1000 | Loss: 0.00001299
Iteration 172/1000 | Loss: 0.00001299
Iteration 173/1000 | Loss: 0.00001299
Iteration 174/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.2991081348445732e-05, 1.2991081348445732e-05, 1.2991081348445732e-05, 1.2991081348445732e-05, 1.2991081348445732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2991081348445732e-05

Optimization complete. Final v2v error: 3.048666000366211 mm

Highest mean error: 3.414379119873047 mm for frame 69

Lowest mean error: 2.6682944297790527 mm for frame 48

Saving results

Total time: 35.63628911972046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00669075
Iteration 2/25 | Loss: 0.00101502
Iteration 3/25 | Loss: 0.00082531
Iteration 4/25 | Loss: 0.00080014
Iteration 5/25 | Loss: 0.00079334
Iteration 6/25 | Loss: 0.00079179
Iteration 7/25 | Loss: 0.00079164
Iteration 8/25 | Loss: 0.00079164
Iteration 9/25 | Loss: 0.00079164
Iteration 10/25 | Loss: 0.00079164
Iteration 11/25 | Loss: 0.00079164
Iteration 12/25 | Loss: 0.00079164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000791642174590379, 0.000791642174590379, 0.000791642174590379, 0.000791642174590379, 0.000791642174590379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000791642174590379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.30084944
Iteration 2/25 | Loss: 0.00123327
Iteration 3/25 | Loss: 0.00123318
Iteration 4/25 | Loss: 0.00123318
Iteration 5/25 | Loss: 0.00123318
Iteration 6/25 | Loss: 0.00123318
Iteration 7/25 | Loss: 0.00123318
Iteration 8/25 | Loss: 0.00123318
Iteration 9/25 | Loss: 0.00123318
Iteration 10/25 | Loss: 0.00123318
Iteration 11/25 | Loss: 0.00123318
Iteration 12/25 | Loss: 0.00123318
Iteration 13/25 | Loss: 0.00123318
Iteration 14/25 | Loss: 0.00123318
Iteration 15/25 | Loss: 0.00123318
Iteration 16/25 | Loss: 0.00123318
Iteration 17/25 | Loss: 0.00123318
Iteration 18/25 | Loss: 0.00123318
Iteration 19/25 | Loss: 0.00123318
Iteration 20/25 | Loss: 0.00123318
Iteration 21/25 | Loss: 0.00123318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012331785401329398, 0.0012331785401329398, 0.0012331785401329398, 0.0012331785401329398, 0.0012331785401329398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012331785401329398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123318
Iteration 2/1000 | Loss: 0.00003048
Iteration 3/1000 | Loss: 0.00002151
Iteration 4/1000 | Loss: 0.00002024
Iteration 5/1000 | Loss: 0.00001927
Iteration 6/1000 | Loss: 0.00001865
Iteration 7/1000 | Loss: 0.00001813
Iteration 8/1000 | Loss: 0.00001779
Iteration 9/1000 | Loss: 0.00001753
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001723
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001721
Iteration 14/1000 | Loss: 0.00001720
Iteration 15/1000 | Loss: 0.00001720
Iteration 16/1000 | Loss: 0.00001720
Iteration 17/1000 | Loss: 0.00001718
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001716
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001712
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001711
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001709
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001706
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001705
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001704
Iteration 56/1000 | Loss: 0.00001704
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001692
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001690
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001689
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001689
Iteration 96/1000 | Loss: 0.00001689
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001688
Iteration 100/1000 | Loss: 0.00001688
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001688
Iteration 111/1000 | Loss: 0.00001688
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001688
Iteration 115/1000 | Loss: 0.00001688
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001688
Iteration 123/1000 | Loss: 0.00001688
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.6877271264092997e-05, 1.6877271264092997e-05, 1.6877271264092997e-05, 1.6877271264092997e-05, 1.6877271264092997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6877271264092997e-05

Optimization complete. Final v2v error: 3.4547245502471924 mm

Highest mean error: 3.8445303440093994 mm for frame 150

Lowest mean error: 3.2353744506835938 mm for frame 101

Saving results

Total time: 35.1357102394104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623899
Iteration 2/25 | Loss: 0.00127983
Iteration 3/25 | Loss: 0.00101825
Iteration 4/25 | Loss: 0.00098718
Iteration 5/25 | Loss: 0.00098217
Iteration 6/25 | Loss: 0.00098197
Iteration 7/25 | Loss: 0.00098197
Iteration 8/25 | Loss: 0.00098197
Iteration 9/25 | Loss: 0.00098197
Iteration 10/25 | Loss: 0.00098197
Iteration 11/25 | Loss: 0.00098197
Iteration 12/25 | Loss: 0.00098197
Iteration 13/25 | Loss: 0.00098197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000981971388682723, 0.000981971388682723, 0.000981971388682723, 0.000981971388682723, 0.000981971388682723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000981971388682723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73366201
Iteration 2/25 | Loss: 0.00117334
Iteration 3/25 | Loss: 0.00117334
Iteration 4/25 | Loss: 0.00117334
Iteration 5/25 | Loss: 0.00117334
Iteration 6/25 | Loss: 0.00117334
Iteration 7/25 | Loss: 0.00117334
Iteration 8/25 | Loss: 0.00117334
Iteration 9/25 | Loss: 0.00117334
Iteration 10/25 | Loss: 0.00117334
Iteration 11/25 | Loss: 0.00117334
Iteration 12/25 | Loss: 0.00117334
Iteration 13/25 | Loss: 0.00117334
Iteration 14/25 | Loss: 0.00117334
Iteration 15/25 | Loss: 0.00117334
Iteration 16/25 | Loss: 0.00117334
Iteration 17/25 | Loss: 0.00117334
Iteration 18/25 | Loss: 0.00117334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011733428109437227, 0.0011733428109437227, 0.0011733428109437227, 0.0011733428109437227, 0.0011733428109437227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011733428109437227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117334
Iteration 2/1000 | Loss: 0.00004986
Iteration 3/1000 | Loss: 0.00003822
Iteration 4/1000 | Loss: 0.00003606
Iteration 5/1000 | Loss: 0.00003502
Iteration 6/1000 | Loss: 0.00003410
Iteration 7/1000 | Loss: 0.00003362
Iteration 8/1000 | Loss: 0.00003344
Iteration 9/1000 | Loss: 0.00003327
Iteration 10/1000 | Loss: 0.00003323
Iteration 11/1000 | Loss: 0.00003317
Iteration 12/1000 | Loss: 0.00003317
Iteration 13/1000 | Loss: 0.00003317
Iteration 14/1000 | Loss: 0.00003317
Iteration 15/1000 | Loss: 0.00003317
Iteration 16/1000 | Loss: 0.00003316
Iteration 17/1000 | Loss: 0.00003316
Iteration 18/1000 | Loss: 0.00003315
Iteration 19/1000 | Loss: 0.00003314
Iteration 20/1000 | Loss: 0.00003314
Iteration 21/1000 | Loss: 0.00003314
Iteration 22/1000 | Loss: 0.00003314
Iteration 23/1000 | Loss: 0.00003313
Iteration 24/1000 | Loss: 0.00003309
Iteration 25/1000 | Loss: 0.00003309
Iteration 26/1000 | Loss: 0.00003308
Iteration 27/1000 | Loss: 0.00003307
Iteration 28/1000 | Loss: 0.00003307
Iteration 29/1000 | Loss: 0.00003306
Iteration 30/1000 | Loss: 0.00003306
Iteration 31/1000 | Loss: 0.00003305
Iteration 32/1000 | Loss: 0.00003305
Iteration 33/1000 | Loss: 0.00003305
Iteration 34/1000 | Loss: 0.00003305
Iteration 35/1000 | Loss: 0.00003305
Iteration 36/1000 | Loss: 0.00003304
Iteration 37/1000 | Loss: 0.00003304
Iteration 38/1000 | Loss: 0.00003300
Iteration 39/1000 | Loss: 0.00003300
Iteration 40/1000 | Loss: 0.00003300
Iteration 41/1000 | Loss: 0.00003300
Iteration 42/1000 | Loss: 0.00003300
Iteration 43/1000 | Loss: 0.00003300
Iteration 44/1000 | Loss: 0.00003299
Iteration 45/1000 | Loss: 0.00003299
Iteration 46/1000 | Loss: 0.00003298
Iteration 47/1000 | Loss: 0.00003297
Iteration 48/1000 | Loss: 0.00003297
Iteration 49/1000 | Loss: 0.00003297
Iteration 50/1000 | Loss: 0.00003297
Iteration 51/1000 | Loss: 0.00003297
Iteration 52/1000 | Loss: 0.00003297
Iteration 53/1000 | Loss: 0.00003297
Iteration 54/1000 | Loss: 0.00003296
Iteration 55/1000 | Loss: 0.00003296
Iteration 56/1000 | Loss: 0.00003296
Iteration 57/1000 | Loss: 0.00003292
Iteration 58/1000 | Loss: 0.00003292
Iteration 59/1000 | Loss: 0.00003292
Iteration 60/1000 | Loss: 0.00003292
Iteration 61/1000 | Loss: 0.00003290
Iteration 62/1000 | Loss: 0.00003289
Iteration 63/1000 | Loss: 0.00003289
Iteration 64/1000 | Loss: 0.00003289
Iteration 65/1000 | Loss: 0.00003289
Iteration 66/1000 | Loss: 0.00003289
Iteration 67/1000 | Loss: 0.00003289
Iteration 68/1000 | Loss: 0.00003288
Iteration 69/1000 | Loss: 0.00003288
Iteration 70/1000 | Loss: 0.00003288
Iteration 71/1000 | Loss: 0.00003288
Iteration 72/1000 | Loss: 0.00003288
Iteration 73/1000 | Loss: 0.00003287
Iteration 74/1000 | Loss: 0.00003287
Iteration 75/1000 | Loss: 0.00003286
Iteration 76/1000 | Loss: 0.00003286
Iteration 77/1000 | Loss: 0.00003286
Iteration 78/1000 | Loss: 0.00003286
Iteration 79/1000 | Loss: 0.00003285
Iteration 80/1000 | Loss: 0.00003284
Iteration 81/1000 | Loss: 0.00003284
Iteration 82/1000 | Loss: 0.00003283
Iteration 83/1000 | Loss: 0.00003283
Iteration 84/1000 | Loss: 0.00003283
Iteration 85/1000 | Loss: 0.00003283
Iteration 86/1000 | Loss: 0.00003283
Iteration 87/1000 | Loss: 0.00003283
Iteration 88/1000 | Loss: 0.00003282
Iteration 89/1000 | Loss: 0.00003282
Iteration 90/1000 | Loss: 0.00003282
Iteration 91/1000 | Loss: 0.00003282
Iteration 92/1000 | Loss: 0.00003282
Iteration 93/1000 | Loss: 0.00003282
Iteration 94/1000 | Loss: 0.00003282
Iteration 95/1000 | Loss: 0.00003282
Iteration 96/1000 | Loss: 0.00003282
Iteration 97/1000 | Loss: 0.00003282
Iteration 98/1000 | Loss: 0.00003282
Iteration 99/1000 | Loss: 0.00003282
Iteration 100/1000 | Loss: 0.00003282
Iteration 101/1000 | Loss: 0.00003281
Iteration 102/1000 | Loss: 0.00003281
Iteration 103/1000 | Loss: 0.00003281
Iteration 104/1000 | Loss: 0.00003281
Iteration 105/1000 | Loss: 0.00003281
Iteration 106/1000 | Loss: 0.00003281
Iteration 107/1000 | Loss: 0.00003281
Iteration 108/1000 | Loss: 0.00003281
Iteration 109/1000 | Loss: 0.00003281
Iteration 110/1000 | Loss: 0.00003281
Iteration 111/1000 | Loss: 0.00003281
Iteration 112/1000 | Loss: 0.00003281
Iteration 113/1000 | Loss: 0.00003280
Iteration 114/1000 | Loss: 0.00003280
Iteration 115/1000 | Loss: 0.00003280
Iteration 116/1000 | Loss: 0.00003280
Iteration 117/1000 | Loss: 0.00003280
Iteration 118/1000 | Loss: 0.00003280
Iteration 119/1000 | Loss: 0.00003280
Iteration 120/1000 | Loss: 0.00003280
Iteration 121/1000 | Loss: 0.00003280
Iteration 122/1000 | Loss: 0.00003280
Iteration 123/1000 | Loss: 0.00003279
Iteration 124/1000 | Loss: 0.00003279
Iteration 125/1000 | Loss: 0.00003279
Iteration 126/1000 | Loss: 0.00003279
Iteration 127/1000 | Loss: 0.00003279
Iteration 128/1000 | Loss: 0.00003279
Iteration 129/1000 | Loss: 0.00003279
Iteration 130/1000 | Loss: 0.00003279
Iteration 131/1000 | Loss: 0.00003279
Iteration 132/1000 | Loss: 0.00003279
Iteration 133/1000 | Loss: 0.00003279
Iteration 134/1000 | Loss: 0.00003279
Iteration 135/1000 | Loss: 0.00003279
Iteration 136/1000 | Loss: 0.00003279
Iteration 137/1000 | Loss: 0.00003279
Iteration 138/1000 | Loss: 0.00003278
Iteration 139/1000 | Loss: 0.00003278
Iteration 140/1000 | Loss: 0.00003278
Iteration 141/1000 | Loss: 0.00003278
Iteration 142/1000 | Loss: 0.00003278
Iteration 143/1000 | Loss: 0.00003278
Iteration 144/1000 | Loss: 0.00003278
Iteration 145/1000 | Loss: 0.00003278
Iteration 146/1000 | Loss: 0.00003278
Iteration 147/1000 | Loss: 0.00003278
Iteration 148/1000 | Loss: 0.00003278
Iteration 149/1000 | Loss: 0.00003278
Iteration 150/1000 | Loss: 0.00003278
Iteration 151/1000 | Loss: 0.00003278
Iteration 152/1000 | Loss: 0.00003278
Iteration 153/1000 | Loss: 0.00003278
Iteration 154/1000 | Loss: 0.00003278
Iteration 155/1000 | Loss: 0.00003278
Iteration 156/1000 | Loss: 0.00003277
Iteration 157/1000 | Loss: 0.00003277
Iteration 158/1000 | Loss: 0.00003277
Iteration 159/1000 | Loss: 0.00003277
Iteration 160/1000 | Loss: 0.00003277
Iteration 161/1000 | Loss: 0.00003277
Iteration 162/1000 | Loss: 0.00003277
Iteration 163/1000 | Loss: 0.00003277
Iteration 164/1000 | Loss: 0.00003277
Iteration 165/1000 | Loss: 0.00003277
Iteration 166/1000 | Loss: 0.00003277
Iteration 167/1000 | Loss: 0.00003277
Iteration 168/1000 | Loss: 0.00003277
Iteration 169/1000 | Loss: 0.00003277
Iteration 170/1000 | Loss: 0.00003277
Iteration 171/1000 | Loss: 0.00003277
Iteration 172/1000 | Loss: 0.00003277
Iteration 173/1000 | Loss: 0.00003277
Iteration 174/1000 | Loss: 0.00003277
Iteration 175/1000 | Loss: 0.00003276
Iteration 176/1000 | Loss: 0.00003276
Iteration 177/1000 | Loss: 0.00003276
Iteration 178/1000 | Loss: 0.00003276
Iteration 179/1000 | Loss: 0.00003276
Iteration 180/1000 | Loss: 0.00003276
Iteration 181/1000 | Loss: 0.00003276
Iteration 182/1000 | Loss: 0.00003276
Iteration 183/1000 | Loss: 0.00003276
Iteration 184/1000 | Loss: 0.00003276
Iteration 185/1000 | Loss: 0.00003276
Iteration 186/1000 | Loss: 0.00003276
Iteration 187/1000 | Loss: 0.00003276
Iteration 188/1000 | Loss: 0.00003276
Iteration 189/1000 | Loss: 0.00003276
Iteration 190/1000 | Loss: 0.00003276
Iteration 191/1000 | Loss: 0.00003276
Iteration 192/1000 | Loss: 0.00003276
Iteration 193/1000 | Loss: 0.00003276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [3.275845301686786e-05, 3.275845301686786e-05, 3.275845301686786e-05, 3.275845301686786e-05, 3.275845301686786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.275845301686786e-05

Optimization complete. Final v2v error: 4.770908355712891 mm

Highest mean error: 4.905359268188477 mm for frame 44

Lowest mean error: 4.49189567565918 mm for frame 130

Saving results

Total time: 40.481121301651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357115
Iteration 2/25 | Loss: 0.00085655
Iteration 3/25 | Loss: 0.00074025
Iteration 4/25 | Loss: 0.00072693
Iteration 5/25 | Loss: 0.00072308
Iteration 6/25 | Loss: 0.00072154
Iteration 7/25 | Loss: 0.00072123
Iteration 8/25 | Loss: 0.00072123
Iteration 9/25 | Loss: 0.00072123
Iteration 10/25 | Loss: 0.00072123
Iteration 11/25 | Loss: 0.00072123
Iteration 12/25 | Loss: 0.00072123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007212256896309555, 0.0007212256896309555, 0.0007212256896309555, 0.0007212256896309555, 0.0007212256896309555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007212256896309555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87386084
Iteration 2/25 | Loss: 0.00126162
Iteration 3/25 | Loss: 0.00126162
Iteration 4/25 | Loss: 0.00126162
Iteration 5/25 | Loss: 0.00126162
Iteration 6/25 | Loss: 0.00126162
Iteration 7/25 | Loss: 0.00126162
Iteration 8/25 | Loss: 0.00126162
Iteration 9/25 | Loss: 0.00126162
Iteration 10/25 | Loss: 0.00126162
Iteration 11/25 | Loss: 0.00126162
Iteration 12/25 | Loss: 0.00126162
Iteration 13/25 | Loss: 0.00126162
Iteration 14/25 | Loss: 0.00126162
Iteration 15/25 | Loss: 0.00126162
Iteration 16/25 | Loss: 0.00126162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012616169406101108, 0.0012616169406101108, 0.0012616169406101108, 0.0012616169406101108, 0.0012616169406101108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012616169406101108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126162
Iteration 2/1000 | Loss: 0.00001944
Iteration 3/1000 | Loss: 0.00001174
Iteration 4/1000 | Loss: 0.00001080
Iteration 5/1000 | Loss: 0.00001015
Iteration 6/1000 | Loss: 0.00000988
Iteration 7/1000 | Loss: 0.00000963
Iteration 8/1000 | Loss: 0.00000962
Iteration 9/1000 | Loss: 0.00000962
Iteration 10/1000 | Loss: 0.00000962
Iteration 11/1000 | Loss: 0.00000961
Iteration 12/1000 | Loss: 0.00000961
Iteration 13/1000 | Loss: 0.00000961
Iteration 14/1000 | Loss: 0.00000958
Iteration 15/1000 | Loss: 0.00000957
Iteration 16/1000 | Loss: 0.00000957
Iteration 17/1000 | Loss: 0.00000956
Iteration 18/1000 | Loss: 0.00000956
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000951
Iteration 21/1000 | Loss: 0.00000950
Iteration 22/1000 | Loss: 0.00000950
Iteration 23/1000 | Loss: 0.00000948
Iteration 24/1000 | Loss: 0.00000948
Iteration 25/1000 | Loss: 0.00000947
Iteration 26/1000 | Loss: 0.00000947
Iteration 27/1000 | Loss: 0.00000946
Iteration 28/1000 | Loss: 0.00000946
Iteration 29/1000 | Loss: 0.00000946
Iteration 30/1000 | Loss: 0.00000946
Iteration 31/1000 | Loss: 0.00000946
Iteration 32/1000 | Loss: 0.00000945
Iteration 33/1000 | Loss: 0.00000945
Iteration 34/1000 | Loss: 0.00000944
Iteration 35/1000 | Loss: 0.00000943
Iteration 36/1000 | Loss: 0.00000943
Iteration 37/1000 | Loss: 0.00000939
Iteration 38/1000 | Loss: 0.00000937
Iteration 39/1000 | Loss: 0.00000936
Iteration 40/1000 | Loss: 0.00000936
Iteration 41/1000 | Loss: 0.00000935
Iteration 42/1000 | Loss: 0.00000934
Iteration 43/1000 | Loss: 0.00000934
Iteration 44/1000 | Loss: 0.00000934
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000933
Iteration 47/1000 | Loss: 0.00000933
Iteration 48/1000 | Loss: 0.00000932
Iteration 49/1000 | Loss: 0.00000932
Iteration 50/1000 | Loss: 0.00000931
Iteration 51/1000 | Loss: 0.00000930
Iteration 52/1000 | Loss: 0.00000930
Iteration 53/1000 | Loss: 0.00000929
Iteration 54/1000 | Loss: 0.00000929
Iteration 55/1000 | Loss: 0.00000929
Iteration 56/1000 | Loss: 0.00000929
Iteration 57/1000 | Loss: 0.00000929
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000928
Iteration 63/1000 | Loss: 0.00000926
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000926
Iteration 66/1000 | Loss: 0.00000926
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000922
Iteration 74/1000 | Loss: 0.00000922
Iteration 75/1000 | Loss: 0.00000922
Iteration 76/1000 | Loss: 0.00000922
Iteration 77/1000 | Loss: 0.00000922
Iteration 78/1000 | Loss: 0.00000922
Iteration 79/1000 | Loss: 0.00000921
Iteration 80/1000 | Loss: 0.00000921
Iteration 81/1000 | Loss: 0.00000921
Iteration 82/1000 | Loss: 0.00000921
Iteration 83/1000 | Loss: 0.00000920
Iteration 84/1000 | Loss: 0.00000919
Iteration 85/1000 | Loss: 0.00000919
Iteration 86/1000 | Loss: 0.00000918
Iteration 87/1000 | Loss: 0.00000918
Iteration 88/1000 | Loss: 0.00000918
Iteration 89/1000 | Loss: 0.00000918
Iteration 90/1000 | Loss: 0.00000918
Iteration 91/1000 | Loss: 0.00000918
Iteration 92/1000 | Loss: 0.00000918
Iteration 93/1000 | Loss: 0.00000918
Iteration 94/1000 | Loss: 0.00000918
Iteration 95/1000 | Loss: 0.00000918
Iteration 96/1000 | Loss: 0.00000918
Iteration 97/1000 | Loss: 0.00000918
Iteration 98/1000 | Loss: 0.00000917
Iteration 99/1000 | Loss: 0.00000917
Iteration 100/1000 | Loss: 0.00000917
Iteration 101/1000 | Loss: 0.00000917
Iteration 102/1000 | Loss: 0.00000917
Iteration 103/1000 | Loss: 0.00000917
Iteration 104/1000 | Loss: 0.00000917
Iteration 105/1000 | Loss: 0.00000916
Iteration 106/1000 | Loss: 0.00000916
Iteration 107/1000 | Loss: 0.00000916
Iteration 108/1000 | Loss: 0.00000916
Iteration 109/1000 | Loss: 0.00000916
Iteration 110/1000 | Loss: 0.00000916
Iteration 111/1000 | Loss: 0.00000916
Iteration 112/1000 | Loss: 0.00000915
Iteration 113/1000 | Loss: 0.00000915
Iteration 114/1000 | Loss: 0.00000915
Iteration 115/1000 | Loss: 0.00000915
Iteration 116/1000 | Loss: 0.00000915
Iteration 117/1000 | Loss: 0.00000915
Iteration 118/1000 | Loss: 0.00000915
Iteration 119/1000 | Loss: 0.00000915
Iteration 120/1000 | Loss: 0.00000915
Iteration 121/1000 | Loss: 0.00000915
Iteration 122/1000 | Loss: 0.00000915
Iteration 123/1000 | Loss: 0.00000915
Iteration 124/1000 | Loss: 0.00000915
Iteration 125/1000 | Loss: 0.00000914
Iteration 126/1000 | Loss: 0.00000914
Iteration 127/1000 | Loss: 0.00000914
Iteration 128/1000 | Loss: 0.00000914
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000914
Iteration 132/1000 | Loss: 0.00000914
Iteration 133/1000 | Loss: 0.00000914
Iteration 134/1000 | Loss: 0.00000914
Iteration 135/1000 | Loss: 0.00000914
Iteration 136/1000 | Loss: 0.00000914
Iteration 137/1000 | Loss: 0.00000914
Iteration 138/1000 | Loss: 0.00000914
Iteration 139/1000 | Loss: 0.00000914
Iteration 140/1000 | Loss: 0.00000914
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000913
Iteration 143/1000 | Loss: 0.00000913
Iteration 144/1000 | Loss: 0.00000913
Iteration 145/1000 | Loss: 0.00000913
Iteration 146/1000 | Loss: 0.00000913
Iteration 147/1000 | Loss: 0.00000913
Iteration 148/1000 | Loss: 0.00000913
Iteration 149/1000 | Loss: 0.00000913
Iteration 150/1000 | Loss: 0.00000913
Iteration 151/1000 | Loss: 0.00000913
Iteration 152/1000 | Loss: 0.00000913
Iteration 153/1000 | Loss: 0.00000912
Iteration 154/1000 | Loss: 0.00000912
Iteration 155/1000 | Loss: 0.00000912
Iteration 156/1000 | Loss: 0.00000912
Iteration 157/1000 | Loss: 0.00000912
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Iteration 164/1000 | Loss: 0.00000911
Iteration 165/1000 | Loss: 0.00000911
Iteration 166/1000 | Loss: 0.00000911
Iteration 167/1000 | Loss: 0.00000911
Iteration 168/1000 | Loss: 0.00000911
Iteration 169/1000 | Loss: 0.00000911
Iteration 170/1000 | Loss: 0.00000911
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000910
Iteration 173/1000 | Loss: 0.00000910
Iteration 174/1000 | Loss: 0.00000910
Iteration 175/1000 | Loss: 0.00000910
Iteration 176/1000 | Loss: 0.00000910
Iteration 177/1000 | Loss: 0.00000910
Iteration 178/1000 | Loss: 0.00000910
Iteration 179/1000 | Loss: 0.00000910
Iteration 180/1000 | Loss: 0.00000910
Iteration 181/1000 | Loss: 0.00000910
Iteration 182/1000 | Loss: 0.00000909
Iteration 183/1000 | Loss: 0.00000909
Iteration 184/1000 | Loss: 0.00000909
Iteration 185/1000 | Loss: 0.00000909
Iteration 186/1000 | Loss: 0.00000909
Iteration 187/1000 | Loss: 0.00000909
Iteration 188/1000 | Loss: 0.00000909
Iteration 189/1000 | Loss: 0.00000909
Iteration 190/1000 | Loss: 0.00000909
Iteration 191/1000 | Loss: 0.00000909
Iteration 192/1000 | Loss: 0.00000909
Iteration 193/1000 | Loss: 0.00000909
Iteration 194/1000 | Loss: 0.00000908
Iteration 195/1000 | Loss: 0.00000908
Iteration 196/1000 | Loss: 0.00000908
Iteration 197/1000 | Loss: 0.00000908
Iteration 198/1000 | Loss: 0.00000908
Iteration 199/1000 | Loss: 0.00000908
Iteration 200/1000 | Loss: 0.00000908
Iteration 201/1000 | Loss: 0.00000908
Iteration 202/1000 | Loss: 0.00000908
Iteration 203/1000 | Loss: 0.00000908
Iteration 204/1000 | Loss: 0.00000908
Iteration 205/1000 | Loss: 0.00000908
Iteration 206/1000 | Loss: 0.00000908
Iteration 207/1000 | Loss: 0.00000908
Iteration 208/1000 | Loss: 0.00000908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [9.081865755433682e-06, 9.081865755433682e-06, 9.081865755433682e-06, 9.081865755433682e-06, 9.081865755433682e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.081865755433682e-06

Optimization complete. Final v2v error: 2.5868091583251953 mm

Highest mean error: 3.076141357421875 mm for frame 75

Lowest mean error: 2.4993624687194824 mm for frame 109

Saving results

Total time: 35.719295024871826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00288582
Iteration 2/25 | Loss: 0.00107351
Iteration 3/25 | Loss: 0.00083517
Iteration 4/25 | Loss: 0.00078685
Iteration 5/25 | Loss: 0.00077121
Iteration 6/25 | Loss: 0.00076878
Iteration 7/25 | Loss: 0.00076779
Iteration 8/25 | Loss: 0.00076776
Iteration 9/25 | Loss: 0.00076776
Iteration 10/25 | Loss: 0.00076776
Iteration 11/25 | Loss: 0.00076776
Iteration 12/25 | Loss: 0.00076776
Iteration 13/25 | Loss: 0.00076776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007677608518861234, 0.0007677608518861234, 0.0007677608518861234, 0.0007677608518861234, 0.0007677608518861234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007677608518861234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61458635
Iteration 2/25 | Loss: 0.00154169
Iteration 3/25 | Loss: 0.00154169
Iteration 4/25 | Loss: 0.00154169
Iteration 5/25 | Loss: 0.00154168
Iteration 6/25 | Loss: 0.00154168
Iteration 7/25 | Loss: 0.00154168
Iteration 8/25 | Loss: 0.00154168
Iteration 9/25 | Loss: 0.00154168
Iteration 10/25 | Loss: 0.00154168
Iteration 11/25 | Loss: 0.00154168
Iteration 12/25 | Loss: 0.00154168
Iteration 13/25 | Loss: 0.00154168
Iteration 14/25 | Loss: 0.00154168
Iteration 15/25 | Loss: 0.00154168
Iteration 16/25 | Loss: 0.00154168
Iteration 17/25 | Loss: 0.00154168
Iteration 18/25 | Loss: 0.00154168
Iteration 19/25 | Loss: 0.00154168
Iteration 20/25 | Loss: 0.00154168
Iteration 21/25 | Loss: 0.00154168
Iteration 22/25 | Loss: 0.00154168
Iteration 23/25 | Loss: 0.00154168
Iteration 24/25 | Loss: 0.00154168
Iteration 25/25 | Loss: 0.00154168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154168
Iteration 2/1000 | Loss: 0.00003809
Iteration 3/1000 | Loss: 0.00002330
Iteration 4/1000 | Loss: 0.00002035
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001728
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001622
Iteration 13/1000 | Loss: 0.00001606
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001599
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001595
Iteration 20/1000 | Loss: 0.00001594
Iteration 21/1000 | Loss: 0.00001594
Iteration 22/1000 | Loss: 0.00001593
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001592
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001591
Iteration 27/1000 | Loss: 0.00001590
Iteration 28/1000 | Loss: 0.00001590
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001589
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001588
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001587
Iteration 43/1000 | Loss: 0.00001587
Iteration 44/1000 | Loss: 0.00001587
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001586
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001584
Iteration 54/1000 | Loss: 0.00001583
Iteration 55/1000 | Loss: 0.00001583
Iteration 56/1000 | Loss: 0.00001583
Iteration 57/1000 | Loss: 0.00001582
Iteration 58/1000 | Loss: 0.00001582
Iteration 59/1000 | Loss: 0.00001582
Iteration 60/1000 | Loss: 0.00001582
Iteration 61/1000 | Loss: 0.00001581
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001581
Iteration 64/1000 | Loss: 0.00001581
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001580
Iteration 67/1000 | Loss: 0.00001580
Iteration 68/1000 | Loss: 0.00001580
Iteration 69/1000 | Loss: 0.00001580
Iteration 70/1000 | Loss: 0.00001580
Iteration 71/1000 | Loss: 0.00001580
Iteration 72/1000 | Loss: 0.00001579
Iteration 73/1000 | Loss: 0.00001579
Iteration 74/1000 | Loss: 0.00001579
Iteration 75/1000 | Loss: 0.00001578
Iteration 76/1000 | Loss: 0.00001578
Iteration 77/1000 | Loss: 0.00001578
Iteration 78/1000 | Loss: 0.00001578
Iteration 79/1000 | Loss: 0.00001578
Iteration 80/1000 | Loss: 0.00001578
Iteration 81/1000 | Loss: 0.00001578
Iteration 82/1000 | Loss: 0.00001578
Iteration 83/1000 | Loss: 0.00001578
Iteration 84/1000 | Loss: 0.00001578
Iteration 85/1000 | Loss: 0.00001578
Iteration 86/1000 | Loss: 0.00001577
Iteration 87/1000 | Loss: 0.00001577
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001577
Iteration 91/1000 | Loss: 0.00001577
Iteration 92/1000 | Loss: 0.00001577
Iteration 93/1000 | Loss: 0.00001577
Iteration 94/1000 | Loss: 0.00001576
Iteration 95/1000 | Loss: 0.00001576
Iteration 96/1000 | Loss: 0.00001576
Iteration 97/1000 | Loss: 0.00001575
Iteration 98/1000 | Loss: 0.00001575
Iteration 99/1000 | Loss: 0.00001575
Iteration 100/1000 | Loss: 0.00001575
Iteration 101/1000 | Loss: 0.00001575
Iteration 102/1000 | Loss: 0.00001575
Iteration 103/1000 | Loss: 0.00001574
Iteration 104/1000 | Loss: 0.00001574
Iteration 105/1000 | Loss: 0.00001574
Iteration 106/1000 | Loss: 0.00001574
Iteration 107/1000 | Loss: 0.00001574
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001574
Iteration 110/1000 | Loss: 0.00001574
Iteration 111/1000 | Loss: 0.00001574
Iteration 112/1000 | Loss: 0.00001574
Iteration 113/1000 | Loss: 0.00001574
Iteration 114/1000 | Loss: 0.00001574
Iteration 115/1000 | Loss: 0.00001574
Iteration 116/1000 | Loss: 0.00001574
Iteration 117/1000 | Loss: 0.00001574
Iteration 118/1000 | Loss: 0.00001574
Iteration 119/1000 | Loss: 0.00001574
Iteration 120/1000 | Loss: 0.00001574
Iteration 121/1000 | Loss: 0.00001574
Iteration 122/1000 | Loss: 0.00001574
Iteration 123/1000 | Loss: 0.00001574
Iteration 124/1000 | Loss: 0.00001574
Iteration 125/1000 | Loss: 0.00001574
Iteration 126/1000 | Loss: 0.00001574
Iteration 127/1000 | Loss: 0.00001574
Iteration 128/1000 | Loss: 0.00001574
Iteration 129/1000 | Loss: 0.00001574
Iteration 130/1000 | Loss: 0.00001574
Iteration 131/1000 | Loss: 0.00001574
Iteration 132/1000 | Loss: 0.00001574
Iteration 133/1000 | Loss: 0.00001574
Iteration 134/1000 | Loss: 0.00001574
Iteration 135/1000 | Loss: 0.00001574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.5735753549961373e-05, 1.5735753549961373e-05, 1.5735753549961373e-05, 1.5735753549961373e-05, 1.5735753549961373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5735753549961373e-05

Optimization complete. Final v2v error: 3.360992670059204 mm

Highest mean error: 3.524611234664917 mm for frame 17

Lowest mean error: 3.2584643363952637 mm for frame 27

Saving results

Total time: 41.898300647735596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126077
Iteration 2/25 | Loss: 0.00180980
Iteration 3/25 | Loss: 0.00120131
Iteration 4/25 | Loss: 0.00105677
Iteration 5/25 | Loss: 0.00112174
Iteration 6/25 | Loss: 0.00098682
Iteration 7/25 | Loss: 0.00091632
Iteration 8/25 | Loss: 0.00083766
Iteration 9/25 | Loss: 0.00080867
Iteration 10/25 | Loss: 0.00080015
Iteration 11/25 | Loss: 0.00079877
Iteration 12/25 | Loss: 0.00079851
Iteration 13/25 | Loss: 0.00079718
Iteration 14/25 | Loss: 0.00079631
Iteration 15/25 | Loss: 0.00079595
Iteration 16/25 | Loss: 0.00079586
Iteration 17/25 | Loss: 0.00079585
Iteration 18/25 | Loss: 0.00079585
Iteration 19/25 | Loss: 0.00079585
Iteration 20/25 | Loss: 0.00079585
Iteration 21/25 | Loss: 0.00079585
Iteration 22/25 | Loss: 0.00079584
Iteration 23/25 | Loss: 0.00079584
Iteration 24/25 | Loss: 0.00079584
Iteration 25/25 | Loss: 0.00079584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69817650
Iteration 2/25 | Loss: 0.00115825
Iteration 3/25 | Loss: 0.00115825
Iteration 4/25 | Loss: 0.00115825
Iteration 5/25 | Loss: 0.00115825
Iteration 6/25 | Loss: 0.00115825
Iteration 7/25 | Loss: 0.00115825
Iteration 8/25 | Loss: 0.00115824
Iteration 9/25 | Loss: 0.00115824
Iteration 10/25 | Loss: 0.00115824
Iteration 11/25 | Loss: 0.00115824
Iteration 12/25 | Loss: 0.00115824
Iteration 13/25 | Loss: 0.00115824
Iteration 14/25 | Loss: 0.00115824
Iteration 15/25 | Loss: 0.00115824
Iteration 16/25 | Loss: 0.00115824
Iteration 17/25 | Loss: 0.00115824
Iteration 18/25 | Loss: 0.00115824
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011582443257793784, 0.0011582443257793784, 0.0011582443257793784, 0.0011582443257793784, 0.0011582443257793784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011582443257793784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115824
Iteration 2/1000 | Loss: 0.00004678
Iteration 3/1000 | Loss: 0.00003007
Iteration 4/1000 | Loss: 0.00002621
Iteration 5/1000 | Loss: 0.00002468
Iteration 6/1000 | Loss: 0.00002370
Iteration 7/1000 | Loss: 0.00002294
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002210
Iteration 10/1000 | Loss: 0.00002185
Iteration 11/1000 | Loss: 0.00002179
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002150
Iteration 14/1000 | Loss: 0.00002147
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002144
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00002143
Iteration 19/1000 | Loss: 0.00002142
Iteration 20/1000 | Loss: 0.00002141
Iteration 21/1000 | Loss: 0.00002137
Iteration 22/1000 | Loss: 0.00002136
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002133
Iteration 25/1000 | Loss: 0.00002132
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00002130
Iteration 28/1000 | Loss: 0.00002130
Iteration 29/1000 | Loss: 0.00002129
Iteration 30/1000 | Loss: 0.00002128
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002126
Iteration 33/1000 | Loss: 0.00002125
Iteration 34/1000 | Loss: 0.00002123
Iteration 35/1000 | Loss: 0.00002123
Iteration 36/1000 | Loss: 0.00002123
Iteration 37/1000 | Loss: 0.00002123
Iteration 38/1000 | Loss: 0.00002123
Iteration 39/1000 | Loss: 0.00002123
Iteration 40/1000 | Loss: 0.00002123
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002120
Iteration 44/1000 | Loss: 0.00002119
Iteration 45/1000 | Loss: 0.00002119
Iteration 46/1000 | Loss: 0.00002119
Iteration 47/1000 | Loss: 0.00002118
Iteration 48/1000 | Loss: 0.00002118
Iteration 49/1000 | Loss: 0.00002118
Iteration 50/1000 | Loss: 0.00002117
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002116
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002116
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002115
Iteration 58/1000 | Loss: 0.00002115
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002114
Iteration 62/1000 | Loss: 0.00002114
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002113
Iteration 65/1000 | Loss: 0.00002113
Iteration 66/1000 | Loss: 0.00002113
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002112
Iteration 70/1000 | Loss: 0.00002112
Iteration 71/1000 | Loss: 0.00002112
Iteration 72/1000 | Loss: 0.00002112
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002111
Iteration 76/1000 | Loss: 0.00002111
Iteration 77/1000 | Loss: 0.00002111
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002111
Iteration 81/1000 | Loss: 0.00002111
Iteration 82/1000 | Loss: 0.00002111
Iteration 83/1000 | Loss: 0.00002110
Iteration 84/1000 | Loss: 0.00002110
Iteration 85/1000 | Loss: 0.00002110
Iteration 86/1000 | Loss: 0.00002109
Iteration 87/1000 | Loss: 0.00002109
Iteration 88/1000 | Loss: 0.00002109
Iteration 89/1000 | Loss: 0.00002109
Iteration 90/1000 | Loss: 0.00002109
Iteration 91/1000 | Loss: 0.00002108
Iteration 92/1000 | Loss: 0.00002108
Iteration 93/1000 | Loss: 0.00002108
Iteration 94/1000 | Loss: 0.00002108
Iteration 95/1000 | Loss: 0.00002107
Iteration 96/1000 | Loss: 0.00002107
Iteration 97/1000 | Loss: 0.00002107
Iteration 98/1000 | Loss: 0.00002107
Iteration 99/1000 | Loss: 0.00002107
Iteration 100/1000 | Loss: 0.00002107
Iteration 101/1000 | Loss: 0.00002107
Iteration 102/1000 | Loss: 0.00002107
Iteration 103/1000 | Loss: 0.00002106
Iteration 104/1000 | Loss: 0.00002106
Iteration 105/1000 | Loss: 0.00002106
Iteration 106/1000 | Loss: 0.00002106
Iteration 107/1000 | Loss: 0.00002105
Iteration 108/1000 | Loss: 0.00002105
Iteration 109/1000 | Loss: 0.00002105
Iteration 110/1000 | Loss: 0.00002105
Iteration 111/1000 | Loss: 0.00002104
Iteration 112/1000 | Loss: 0.00002104
Iteration 113/1000 | Loss: 0.00002104
Iteration 114/1000 | Loss: 0.00002104
Iteration 115/1000 | Loss: 0.00002104
Iteration 116/1000 | Loss: 0.00002104
Iteration 117/1000 | Loss: 0.00002104
Iteration 118/1000 | Loss: 0.00002104
Iteration 119/1000 | Loss: 0.00002104
Iteration 120/1000 | Loss: 0.00002103
Iteration 121/1000 | Loss: 0.00002103
Iteration 122/1000 | Loss: 0.00002103
Iteration 123/1000 | Loss: 0.00002103
Iteration 124/1000 | Loss: 0.00002103
Iteration 125/1000 | Loss: 0.00002103
Iteration 126/1000 | Loss: 0.00002103
Iteration 127/1000 | Loss: 0.00002103
Iteration 128/1000 | Loss: 0.00002103
Iteration 129/1000 | Loss: 0.00002103
Iteration 130/1000 | Loss: 0.00002103
Iteration 131/1000 | Loss: 0.00002102
Iteration 132/1000 | Loss: 0.00002102
Iteration 133/1000 | Loss: 0.00002102
Iteration 134/1000 | Loss: 0.00002102
Iteration 135/1000 | Loss: 0.00002102
Iteration 136/1000 | Loss: 0.00002102
Iteration 137/1000 | Loss: 0.00002102
Iteration 138/1000 | Loss: 0.00002102
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002102
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Iteration 147/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.10206435440341e-05, 2.10206435440341e-05, 2.10206435440341e-05, 2.10206435440341e-05, 2.10206435440341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.10206435440341e-05

Optimization complete. Final v2v error: 3.788402795791626 mm

Highest mean error: 5.446890830993652 mm for frame 79

Lowest mean error: 3.1948015689849854 mm for frame 23

Saving results

Total time: 57.77039051055908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053955
Iteration 2/25 | Loss: 0.00394771
Iteration 3/25 | Loss: 0.00212671
Iteration 4/25 | Loss: 0.00159214
Iteration 5/25 | Loss: 0.00164420
Iteration 6/25 | Loss: 0.00150216
Iteration 7/25 | Loss: 0.00141383
Iteration 8/25 | Loss: 0.00130801
Iteration 9/25 | Loss: 0.00124519
Iteration 10/25 | Loss: 0.00120612
Iteration 11/25 | Loss: 0.00118786
Iteration 12/25 | Loss: 0.00114082
Iteration 13/25 | Loss: 0.00113517
Iteration 14/25 | Loss: 0.00112464
Iteration 15/25 | Loss: 0.00111522
Iteration 16/25 | Loss: 0.00110645
Iteration 17/25 | Loss: 0.00109760
Iteration 18/25 | Loss: 0.00109345
Iteration 19/25 | Loss: 0.00109251
Iteration 20/25 | Loss: 0.00109976
Iteration 21/25 | Loss: 0.00108926
Iteration 22/25 | Loss: 0.00108269
Iteration 23/25 | Loss: 0.00108187
Iteration 24/25 | Loss: 0.00108429
Iteration 25/25 | Loss: 0.00107910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55998087
Iteration 2/25 | Loss: 0.00525104
Iteration 3/25 | Loss: 0.00261056
Iteration 4/25 | Loss: 0.00261055
Iteration 5/25 | Loss: 0.00261055
Iteration 6/25 | Loss: 0.00261055
Iteration 7/25 | Loss: 0.00261055
Iteration 8/25 | Loss: 0.00261055
Iteration 9/25 | Loss: 0.00261055
Iteration 10/25 | Loss: 0.00261055
Iteration 11/25 | Loss: 0.00261055
Iteration 12/25 | Loss: 0.00261055
Iteration 13/25 | Loss: 0.00261055
Iteration 14/25 | Loss: 0.00261055
Iteration 15/25 | Loss: 0.00261055
Iteration 16/25 | Loss: 0.00261055
Iteration 17/25 | Loss: 0.00261055
Iteration 18/25 | Loss: 0.00261055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002610545139759779, 0.002610545139759779, 0.002610545139759779, 0.002610545139759779, 0.002610545139759779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002610545139759779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261055
Iteration 2/1000 | Loss: 0.00104292
Iteration 3/1000 | Loss: 0.00110389
Iteration 4/1000 | Loss: 0.00045919
Iteration 5/1000 | Loss: 0.00054472
Iteration 6/1000 | Loss: 0.00086383
Iteration 7/1000 | Loss: 0.00172803
Iteration 8/1000 | Loss: 0.00060077
Iteration 9/1000 | Loss: 0.00093088
Iteration 10/1000 | Loss: 0.00096734
Iteration 11/1000 | Loss: 0.00380797
Iteration 12/1000 | Loss: 0.00057709
Iteration 13/1000 | Loss: 0.00097712
Iteration 14/1000 | Loss: 0.00093338
Iteration 15/1000 | Loss: 0.00146693
Iteration 16/1000 | Loss: 0.00178923
Iteration 17/1000 | Loss: 0.00075504
Iteration 18/1000 | Loss: 0.00052498
Iteration 19/1000 | Loss: 0.00029599
Iteration 20/1000 | Loss: 0.00111358
Iteration 21/1000 | Loss: 0.00202810
Iteration 22/1000 | Loss: 0.00049989
Iteration 23/1000 | Loss: 0.00064674
Iteration 24/1000 | Loss: 0.00145448
Iteration 25/1000 | Loss: 0.00063392
Iteration 26/1000 | Loss: 0.00014737
Iteration 27/1000 | Loss: 0.00038249
Iteration 28/1000 | Loss: 0.00013011
Iteration 29/1000 | Loss: 0.00029867
Iteration 30/1000 | Loss: 0.00020329
Iteration 31/1000 | Loss: 0.00012090
Iteration 32/1000 | Loss: 0.00056040
Iteration 33/1000 | Loss: 0.00111974
Iteration 34/1000 | Loss: 0.00241378
Iteration 35/1000 | Loss: 0.00254548
Iteration 36/1000 | Loss: 0.00140830
Iteration 37/1000 | Loss: 0.00087197
Iteration 38/1000 | Loss: 0.00128096
Iteration 39/1000 | Loss: 0.00116659
Iteration 40/1000 | Loss: 0.00198635
Iteration 41/1000 | Loss: 0.00108264
Iteration 42/1000 | Loss: 0.00270253
Iteration 43/1000 | Loss: 0.00108893
Iteration 44/1000 | Loss: 0.00034347
Iteration 45/1000 | Loss: 0.00052886
Iteration 46/1000 | Loss: 0.00012056
Iteration 47/1000 | Loss: 0.00009558
Iteration 48/1000 | Loss: 0.00062692
Iteration 49/1000 | Loss: 0.00008395
Iteration 50/1000 | Loss: 0.00060438
Iteration 51/1000 | Loss: 0.00314306
Iteration 52/1000 | Loss: 0.00010383
Iteration 53/1000 | Loss: 0.00009937
Iteration 54/1000 | Loss: 0.00008924
Iteration 55/1000 | Loss: 0.00044018
Iteration 56/1000 | Loss: 0.00008645
Iteration 57/1000 | Loss: 0.00043922
Iteration 58/1000 | Loss: 0.00006749
Iteration 59/1000 | Loss: 0.00036401
Iteration 60/1000 | Loss: 0.00018472
Iteration 61/1000 | Loss: 0.00034161
Iteration 62/1000 | Loss: 0.00033600
Iteration 63/1000 | Loss: 0.00077950
Iteration 64/1000 | Loss: 0.00032321
Iteration 65/1000 | Loss: 0.00098080
Iteration 66/1000 | Loss: 0.00063945
Iteration 67/1000 | Loss: 0.00028833
Iteration 68/1000 | Loss: 0.00025267
Iteration 69/1000 | Loss: 0.00032710
Iteration 70/1000 | Loss: 0.00013703
Iteration 71/1000 | Loss: 0.00025914
Iteration 72/1000 | Loss: 0.00042829
Iteration 73/1000 | Loss: 0.00006414
Iteration 74/1000 | Loss: 0.00030011
Iteration 75/1000 | Loss: 0.00007965
Iteration 76/1000 | Loss: 0.00019694
Iteration 77/1000 | Loss: 0.00017978
Iteration 78/1000 | Loss: 0.00026834
Iteration 79/1000 | Loss: 0.00012214
Iteration 80/1000 | Loss: 0.00032102
Iteration 81/1000 | Loss: 0.00254353
Iteration 82/1000 | Loss: 0.00103599
Iteration 83/1000 | Loss: 0.00025832
Iteration 84/1000 | Loss: 0.00005129
Iteration 85/1000 | Loss: 0.00004554
Iteration 86/1000 | Loss: 0.00012686
Iteration 87/1000 | Loss: 0.00007731
Iteration 88/1000 | Loss: 0.00005404
Iteration 89/1000 | Loss: 0.00004493
Iteration 90/1000 | Loss: 0.00060307
Iteration 91/1000 | Loss: 0.00007238
Iteration 92/1000 | Loss: 0.00004924
Iteration 93/1000 | Loss: 0.00006436
Iteration 94/1000 | Loss: 0.00003282
Iteration 95/1000 | Loss: 0.00006983
Iteration 96/1000 | Loss: 0.00003300
Iteration 97/1000 | Loss: 0.00004716
Iteration 98/1000 | Loss: 0.00004139
Iteration 99/1000 | Loss: 0.00005228
Iteration 100/1000 | Loss: 0.00003502
Iteration 101/1000 | Loss: 0.00009675
Iteration 102/1000 | Loss: 0.00003539
Iteration 103/1000 | Loss: 0.00005230
Iteration 104/1000 | Loss: 0.00006070
Iteration 105/1000 | Loss: 0.00006220
Iteration 106/1000 | Loss: 0.00003586
Iteration 107/1000 | Loss: 0.00002842
Iteration 108/1000 | Loss: 0.00002739
Iteration 109/1000 | Loss: 0.00005664
Iteration 110/1000 | Loss: 0.00002647
Iteration 111/1000 | Loss: 0.00006092
Iteration 112/1000 | Loss: 0.00002610
Iteration 113/1000 | Loss: 0.00002567
Iteration 114/1000 | Loss: 0.00002534
Iteration 115/1000 | Loss: 0.00002502
Iteration 116/1000 | Loss: 0.00008547
Iteration 117/1000 | Loss: 0.00004999
Iteration 118/1000 | Loss: 0.00003142
Iteration 119/1000 | Loss: 0.00002600
Iteration 120/1000 | Loss: 0.00002471
Iteration 121/1000 | Loss: 0.00008794
Iteration 122/1000 | Loss: 0.00002451
Iteration 123/1000 | Loss: 0.00002427
Iteration 124/1000 | Loss: 0.00004855
Iteration 125/1000 | Loss: 0.00004626
Iteration 126/1000 | Loss: 0.00002476
Iteration 127/1000 | Loss: 0.00004891
Iteration 128/1000 | Loss: 0.00005468
Iteration 129/1000 | Loss: 0.00010679
Iteration 130/1000 | Loss: 0.00004516
Iteration 131/1000 | Loss: 0.00010663
Iteration 132/1000 | Loss: 0.00008048
Iteration 133/1000 | Loss: 0.00006527
Iteration 134/1000 | Loss: 0.00002476
Iteration 135/1000 | Loss: 0.00009486
Iteration 136/1000 | Loss: 0.00005691
Iteration 137/1000 | Loss: 0.00004438
Iteration 138/1000 | Loss: 0.00014063
Iteration 139/1000 | Loss: 0.00008282
Iteration 140/1000 | Loss: 0.00002426
Iteration 141/1000 | Loss: 0.00006862
Iteration 142/1000 | Loss: 0.00004704
Iteration 143/1000 | Loss: 0.00012951
Iteration 144/1000 | Loss: 0.00006908
Iteration 145/1000 | Loss: 0.00003940
Iteration 146/1000 | Loss: 0.00006056
Iteration 147/1000 | Loss: 0.00006240
Iteration 148/1000 | Loss: 0.00003473
Iteration 149/1000 | Loss: 0.00004197
Iteration 150/1000 | Loss: 0.00007205
Iteration 151/1000 | Loss: 0.00006047
Iteration 152/1000 | Loss: 0.00006938
Iteration 153/1000 | Loss: 0.00004639
Iteration 154/1000 | Loss: 0.00008393
Iteration 155/1000 | Loss: 0.00004382
Iteration 156/1000 | Loss: 0.00002859
Iteration 157/1000 | Loss: 0.00003369
Iteration 158/1000 | Loss: 0.00003393
Iteration 159/1000 | Loss: 0.00007106
Iteration 160/1000 | Loss: 0.00004087
Iteration 161/1000 | Loss: 0.00007764
Iteration 162/1000 | Loss: 0.00006430
Iteration 163/1000 | Loss: 0.00003577
Iteration 164/1000 | Loss: 0.00003526
Iteration 165/1000 | Loss: 0.00002920
Iteration 166/1000 | Loss: 0.00002563
Iteration 167/1000 | Loss: 0.00003360
Iteration 168/1000 | Loss: 0.00002472
Iteration 169/1000 | Loss: 0.00002426
Iteration 170/1000 | Loss: 0.00005106
Iteration 171/1000 | Loss: 0.00002410
Iteration 172/1000 | Loss: 0.00002358
Iteration 173/1000 | Loss: 0.00002358
Iteration 174/1000 | Loss: 0.00002349
Iteration 175/1000 | Loss: 0.00002334
Iteration 176/1000 | Loss: 0.00002329
Iteration 177/1000 | Loss: 0.00002324
Iteration 178/1000 | Loss: 0.00002323
Iteration 179/1000 | Loss: 0.00007013
Iteration 180/1000 | Loss: 0.00002895
Iteration 181/1000 | Loss: 0.00004374
Iteration 182/1000 | Loss: 0.00003663
Iteration 183/1000 | Loss: 0.00004785
Iteration 184/1000 | Loss: 0.00002303
Iteration 185/1000 | Loss: 0.00002300
Iteration 186/1000 | Loss: 0.00002298
Iteration 187/1000 | Loss: 0.00002298
Iteration 188/1000 | Loss: 0.00002297
Iteration 189/1000 | Loss: 0.00002297
Iteration 190/1000 | Loss: 0.00002297
Iteration 191/1000 | Loss: 0.00002297
Iteration 192/1000 | Loss: 0.00002296
Iteration 193/1000 | Loss: 0.00003835
Iteration 194/1000 | Loss: 0.00002398
Iteration 195/1000 | Loss: 0.00002342
Iteration 196/1000 | Loss: 0.00002299
Iteration 197/1000 | Loss: 0.00002298
Iteration 198/1000 | Loss: 0.00002297
Iteration 199/1000 | Loss: 0.00002296
Iteration 200/1000 | Loss: 0.00002296
Iteration 201/1000 | Loss: 0.00002296
Iteration 202/1000 | Loss: 0.00002295
Iteration 203/1000 | Loss: 0.00002295
Iteration 204/1000 | Loss: 0.00002295
Iteration 205/1000 | Loss: 0.00002294
Iteration 206/1000 | Loss: 0.00002294
Iteration 207/1000 | Loss: 0.00002294
Iteration 208/1000 | Loss: 0.00002294
Iteration 209/1000 | Loss: 0.00002294
Iteration 210/1000 | Loss: 0.00002294
Iteration 211/1000 | Loss: 0.00002293
Iteration 212/1000 | Loss: 0.00002293
Iteration 213/1000 | Loss: 0.00002293
Iteration 214/1000 | Loss: 0.00002293
Iteration 215/1000 | Loss: 0.00002293
Iteration 216/1000 | Loss: 0.00002293
Iteration 217/1000 | Loss: 0.00002293
Iteration 218/1000 | Loss: 0.00002293
Iteration 219/1000 | Loss: 0.00002293
Iteration 220/1000 | Loss: 0.00002293
Iteration 221/1000 | Loss: 0.00002293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [2.2930995328351855e-05, 2.2930995328351855e-05, 2.2930995328351855e-05, 2.2930995328351855e-05, 2.2930995328351855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2930995328351855e-05

Optimization complete. Final v2v error: 3.5989882946014404 mm

Highest mean error: 17.33390235900879 mm for frame 192

Lowest mean error: 2.766730785369873 mm for frame 64

Saving results

Total time: 352.5674991607666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805804
Iteration 2/25 | Loss: 0.00123450
Iteration 3/25 | Loss: 0.00091305
Iteration 4/25 | Loss: 0.00085891
Iteration 5/25 | Loss: 0.00084410
Iteration 6/25 | Loss: 0.00084072
Iteration 7/25 | Loss: 0.00084027
Iteration 8/25 | Loss: 0.00084027
Iteration 9/25 | Loss: 0.00084027
Iteration 10/25 | Loss: 0.00084027
Iteration 11/25 | Loss: 0.00084027
Iteration 12/25 | Loss: 0.00084027
Iteration 13/25 | Loss: 0.00084027
Iteration 14/25 | Loss: 0.00084027
Iteration 15/25 | Loss: 0.00084027
Iteration 16/25 | Loss: 0.00084027
Iteration 17/25 | Loss: 0.00084027
Iteration 18/25 | Loss: 0.00084027
Iteration 19/25 | Loss: 0.00084027
Iteration 20/25 | Loss: 0.00084027
Iteration 21/25 | Loss: 0.00084027
Iteration 22/25 | Loss: 0.00084027
Iteration 23/25 | Loss: 0.00084027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008402673411183059, 0.0008402673411183059, 0.0008402673411183059, 0.0008402673411183059, 0.0008402673411183059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008402673411183059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36145282
Iteration 2/25 | Loss: 0.00104930
Iteration 3/25 | Loss: 0.00104930
Iteration 4/25 | Loss: 0.00104930
Iteration 5/25 | Loss: 0.00104930
Iteration 6/25 | Loss: 0.00104930
Iteration 7/25 | Loss: 0.00104930
Iteration 8/25 | Loss: 0.00104930
Iteration 9/25 | Loss: 0.00104929
Iteration 10/25 | Loss: 0.00104929
Iteration 11/25 | Loss: 0.00104929
Iteration 12/25 | Loss: 0.00104929
Iteration 13/25 | Loss: 0.00104929
Iteration 14/25 | Loss: 0.00104929
Iteration 15/25 | Loss: 0.00104929
Iteration 16/25 | Loss: 0.00104929
Iteration 17/25 | Loss: 0.00104929
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010492944857105613, 0.0010492944857105613, 0.0010492944857105613, 0.0010492944857105613, 0.0010492944857105613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010492944857105613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104929
Iteration 2/1000 | Loss: 0.00004399
Iteration 3/1000 | Loss: 0.00002692
Iteration 4/1000 | Loss: 0.00002380
Iteration 5/1000 | Loss: 0.00002225
Iteration 6/1000 | Loss: 0.00002101
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001906
Iteration 11/1000 | Loss: 0.00001900
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001886
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001864
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001859
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001856
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001855
Iteration 26/1000 | Loss: 0.00001854
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001853
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001849
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001848
Iteration 35/1000 | Loss: 0.00001848
Iteration 36/1000 | Loss: 0.00001847
Iteration 37/1000 | Loss: 0.00001847
Iteration 38/1000 | Loss: 0.00001847
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001844
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001842
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001841
Iteration 55/1000 | Loss: 0.00001840
Iteration 56/1000 | Loss: 0.00001840
Iteration 57/1000 | Loss: 0.00001840
Iteration 58/1000 | Loss: 0.00001840
Iteration 59/1000 | Loss: 0.00001839
Iteration 60/1000 | Loss: 0.00001839
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001838
Iteration 63/1000 | Loss: 0.00001838
Iteration 64/1000 | Loss: 0.00001838
Iteration 65/1000 | Loss: 0.00001838
Iteration 66/1000 | Loss: 0.00001837
Iteration 67/1000 | Loss: 0.00001837
Iteration 68/1000 | Loss: 0.00001837
Iteration 69/1000 | Loss: 0.00001837
Iteration 70/1000 | Loss: 0.00001837
Iteration 71/1000 | Loss: 0.00001837
Iteration 72/1000 | Loss: 0.00001836
Iteration 73/1000 | Loss: 0.00001836
Iteration 74/1000 | Loss: 0.00001836
Iteration 75/1000 | Loss: 0.00001836
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001835
Iteration 79/1000 | Loss: 0.00001835
Iteration 80/1000 | Loss: 0.00001835
Iteration 81/1000 | Loss: 0.00001835
Iteration 82/1000 | Loss: 0.00001835
Iteration 83/1000 | Loss: 0.00001835
Iteration 84/1000 | Loss: 0.00001835
Iteration 85/1000 | Loss: 0.00001835
Iteration 86/1000 | Loss: 0.00001835
Iteration 87/1000 | Loss: 0.00001835
Iteration 88/1000 | Loss: 0.00001835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.834585054893978e-05, 1.834585054893978e-05, 1.834585054893978e-05, 1.834585054893978e-05, 1.834585054893978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.834585054893978e-05

Optimization complete. Final v2v error: 3.5147411823272705 mm

Highest mean error: 4.763350009918213 mm for frame 211

Lowest mean error: 3.000734806060791 mm for frame 50

Saving results

Total time: 38.78001046180725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436324
Iteration 2/25 | Loss: 0.00109166
Iteration 3/25 | Loss: 0.00081629
Iteration 4/25 | Loss: 0.00079101
Iteration 5/25 | Loss: 0.00078545
Iteration 6/25 | Loss: 0.00078352
Iteration 7/25 | Loss: 0.00078278
Iteration 8/25 | Loss: 0.00078273
Iteration 9/25 | Loss: 0.00078273
Iteration 10/25 | Loss: 0.00078273
Iteration 11/25 | Loss: 0.00078273
Iteration 12/25 | Loss: 0.00078273
Iteration 13/25 | Loss: 0.00078273
Iteration 14/25 | Loss: 0.00078273
Iteration 15/25 | Loss: 0.00078273
Iteration 16/25 | Loss: 0.00078273
Iteration 17/25 | Loss: 0.00078273
Iteration 18/25 | Loss: 0.00078273
Iteration 19/25 | Loss: 0.00078273
Iteration 20/25 | Loss: 0.00078273
Iteration 21/25 | Loss: 0.00078273
Iteration 22/25 | Loss: 0.00078273
Iteration 23/25 | Loss: 0.00078273
Iteration 24/25 | Loss: 0.00078273
Iteration 25/25 | Loss: 0.00078273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007827286026440561, 0.0007827286026440561, 0.0007827286026440561, 0.0007827286026440561, 0.0007827286026440561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007827286026440561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.48528242
Iteration 2/25 | Loss: 0.00127393
Iteration 3/25 | Loss: 0.00127393
Iteration 4/25 | Loss: 0.00127393
Iteration 5/25 | Loss: 0.00127393
Iteration 6/25 | Loss: 0.00127393
Iteration 7/25 | Loss: 0.00127393
Iteration 8/25 | Loss: 0.00127393
Iteration 9/25 | Loss: 0.00127393
Iteration 10/25 | Loss: 0.00127393
Iteration 11/25 | Loss: 0.00127393
Iteration 12/25 | Loss: 0.00127393
Iteration 13/25 | Loss: 0.00127393
Iteration 14/25 | Loss: 0.00127393
Iteration 15/25 | Loss: 0.00127393
Iteration 16/25 | Loss: 0.00127393
Iteration 17/25 | Loss: 0.00127393
Iteration 18/25 | Loss: 0.00127393
Iteration 19/25 | Loss: 0.00127393
Iteration 20/25 | Loss: 0.00127393
Iteration 21/25 | Loss: 0.00127393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001273925299756229, 0.001273925299756229, 0.001273925299756229, 0.001273925299756229, 0.001273925299756229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273925299756229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127393
Iteration 2/1000 | Loss: 0.00002365
Iteration 3/1000 | Loss: 0.00001697
Iteration 4/1000 | Loss: 0.00001584
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001471
Iteration 7/1000 | Loss: 0.00001435
Iteration 8/1000 | Loss: 0.00001435
Iteration 9/1000 | Loss: 0.00001409
Iteration 10/1000 | Loss: 0.00001393
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001379
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001372
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001365
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001356
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001349
Iteration 49/1000 | Loss: 0.00001349
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001349
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001347
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001346
Iteration 61/1000 | Loss: 0.00001346
Iteration 62/1000 | Loss: 0.00001346
Iteration 63/1000 | Loss: 0.00001346
Iteration 64/1000 | Loss: 0.00001345
Iteration 65/1000 | Loss: 0.00001345
Iteration 66/1000 | Loss: 0.00001345
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001344
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001343
Iteration 75/1000 | Loss: 0.00001343
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001341
Iteration 82/1000 | Loss: 0.00001341
Iteration 83/1000 | Loss: 0.00001341
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001340
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001340
Iteration 88/1000 | Loss: 0.00001340
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001339
Iteration 93/1000 | Loss: 0.00001339
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001339
Iteration 96/1000 | Loss: 0.00001339
Iteration 97/1000 | Loss: 0.00001339
Iteration 98/1000 | Loss: 0.00001339
Iteration 99/1000 | Loss: 0.00001339
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001338
Iteration 103/1000 | Loss: 0.00001338
Iteration 104/1000 | Loss: 0.00001338
Iteration 105/1000 | Loss: 0.00001337
Iteration 106/1000 | Loss: 0.00001337
Iteration 107/1000 | Loss: 0.00001337
Iteration 108/1000 | Loss: 0.00001337
Iteration 109/1000 | Loss: 0.00001337
Iteration 110/1000 | Loss: 0.00001337
Iteration 111/1000 | Loss: 0.00001336
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001336
Iteration 115/1000 | Loss: 0.00001336
Iteration 116/1000 | Loss: 0.00001336
Iteration 117/1000 | Loss: 0.00001335
Iteration 118/1000 | Loss: 0.00001335
Iteration 119/1000 | Loss: 0.00001335
Iteration 120/1000 | Loss: 0.00001335
Iteration 121/1000 | Loss: 0.00001335
Iteration 122/1000 | Loss: 0.00001335
Iteration 123/1000 | Loss: 0.00001335
Iteration 124/1000 | Loss: 0.00001334
Iteration 125/1000 | Loss: 0.00001334
Iteration 126/1000 | Loss: 0.00001334
Iteration 127/1000 | Loss: 0.00001334
Iteration 128/1000 | Loss: 0.00001334
Iteration 129/1000 | Loss: 0.00001334
Iteration 130/1000 | Loss: 0.00001334
Iteration 131/1000 | Loss: 0.00001334
Iteration 132/1000 | Loss: 0.00001334
Iteration 133/1000 | Loss: 0.00001334
Iteration 134/1000 | Loss: 0.00001334
Iteration 135/1000 | Loss: 0.00001334
Iteration 136/1000 | Loss: 0.00001334
Iteration 137/1000 | Loss: 0.00001334
Iteration 138/1000 | Loss: 0.00001334
Iteration 139/1000 | Loss: 0.00001334
Iteration 140/1000 | Loss: 0.00001334
Iteration 141/1000 | Loss: 0.00001334
Iteration 142/1000 | Loss: 0.00001334
Iteration 143/1000 | Loss: 0.00001334
Iteration 144/1000 | Loss: 0.00001334
Iteration 145/1000 | Loss: 0.00001334
Iteration 146/1000 | Loss: 0.00001334
Iteration 147/1000 | Loss: 0.00001334
Iteration 148/1000 | Loss: 0.00001334
Iteration 149/1000 | Loss: 0.00001334
Iteration 150/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.3341116755327675e-05, 1.3341116755327675e-05, 1.3341116755327675e-05, 1.3341116755327675e-05, 1.3341116755327675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3341116755327675e-05

Optimization complete. Final v2v error: 3.1118621826171875 mm

Highest mean error: 3.6687488555908203 mm for frame 71

Lowest mean error: 2.793980121612549 mm for frame 9

Saving results

Total time: 35.3179452419281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437614
Iteration 2/25 | Loss: 0.00093055
Iteration 3/25 | Loss: 0.00079302
Iteration 4/25 | Loss: 0.00076315
Iteration 5/25 | Loss: 0.00075668
Iteration 6/25 | Loss: 0.00075422
Iteration 7/25 | Loss: 0.00075367
Iteration 8/25 | Loss: 0.00075367
Iteration 9/25 | Loss: 0.00075367
Iteration 10/25 | Loss: 0.00075367
Iteration 11/25 | Loss: 0.00075367
Iteration 12/25 | Loss: 0.00075367
Iteration 13/25 | Loss: 0.00075367
Iteration 14/25 | Loss: 0.00075367
Iteration 15/25 | Loss: 0.00075367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007536671473644674, 0.0007536671473644674, 0.0007536671473644674, 0.0007536671473644674, 0.0007536671473644674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007536671473644674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59251845
Iteration 2/25 | Loss: 0.00137849
Iteration 3/25 | Loss: 0.00137849
Iteration 4/25 | Loss: 0.00137849
Iteration 5/25 | Loss: 0.00137849
Iteration 6/25 | Loss: 0.00137849
Iteration 7/25 | Loss: 0.00137849
Iteration 8/25 | Loss: 0.00137849
Iteration 9/25 | Loss: 0.00137849
Iteration 10/25 | Loss: 0.00137849
Iteration 11/25 | Loss: 0.00137849
Iteration 12/25 | Loss: 0.00137849
Iteration 13/25 | Loss: 0.00137849
Iteration 14/25 | Loss: 0.00137849
Iteration 15/25 | Loss: 0.00137849
Iteration 16/25 | Loss: 0.00137849
Iteration 17/25 | Loss: 0.00137849
Iteration 18/25 | Loss: 0.00137849
Iteration 19/25 | Loss: 0.00137849
Iteration 20/25 | Loss: 0.00137849
Iteration 21/25 | Loss: 0.00137849
Iteration 22/25 | Loss: 0.00137849
Iteration 23/25 | Loss: 0.00137849
Iteration 24/25 | Loss: 0.00137849
Iteration 25/25 | Loss: 0.00137849

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137849
Iteration 2/1000 | Loss: 0.00002189
Iteration 3/1000 | Loss: 0.00001552
Iteration 4/1000 | Loss: 0.00001414
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001233
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001178
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001175
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001162
Iteration 17/1000 | Loss: 0.00001161
Iteration 18/1000 | Loss: 0.00001161
Iteration 19/1000 | Loss: 0.00001159
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001158
Iteration 22/1000 | Loss: 0.00001158
Iteration 23/1000 | Loss: 0.00001158
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001158
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001157
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001157
Iteration 30/1000 | Loss: 0.00001156
Iteration 31/1000 | Loss: 0.00001155
Iteration 32/1000 | Loss: 0.00001155
Iteration 33/1000 | Loss: 0.00001155
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001154
Iteration 38/1000 | Loss: 0.00001154
Iteration 39/1000 | Loss: 0.00001153
Iteration 40/1000 | Loss: 0.00001153
Iteration 41/1000 | Loss: 0.00001153
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001152
Iteration 48/1000 | Loss: 0.00001152
Iteration 49/1000 | Loss: 0.00001152
Iteration 50/1000 | Loss: 0.00001152
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001151
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001151
Iteration 60/1000 | Loss: 0.00001151
Iteration 61/1000 | Loss: 0.00001151
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001148
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001147
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001146
Iteration 81/1000 | Loss: 0.00001146
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001146
Iteration 85/1000 | Loss: 0.00001146
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001146
Iteration 88/1000 | Loss: 0.00001146
Iteration 89/1000 | Loss: 0.00001146
Iteration 90/1000 | Loss: 0.00001146
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001145
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001145
Iteration 98/1000 | Loss: 0.00001145
Iteration 99/1000 | Loss: 0.00001145
Iteration 100/1000 | Loss: 0.00001145
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001144
Iteration 109/1000 | Loss: 0.00001144
Iteration 110/1000 | Loss: 0.00001144
Iteration 111/1000 | Loss: 0.00001144
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001143
Iteration 120/1000 | Loss: 0.00001143
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1429445294197649e-05, 1.1429445294197649e-05, 1.1429445294197649e-05, 1.1429445294197649e-05, 1.1429445294197649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1429445294197649e-05

Optimization complete. Final v2v error: 2.845339059829712 mm

Highest mean error: 3.0431926250457764 mm for frame 124

Lowest mean error: 2.634904623031616 mm for frame 142

Saving results

Total time: 38.082515001297
