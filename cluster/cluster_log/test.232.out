Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=232, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12992-13047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622454
Iteration 2/25 | Loss: 0.00133122
Iteration 3/25 | Loss: 0.00123744
Iteration 4/25 | Loss: 0.00122221
Iteration 5/25 | Loss: 0.00121965
Iteration 6/25 | Loss: 0.00121960
Iteration 7/25 | Loss: 0.00121960
Iteration 8/25 | Loss: 0.00121960
Iteration 9/25 | Loss: 0.00121960
Iteration 10/25 | Loss: 0.00121960
Iteration 11/25 | Loss: 0.00121960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001219597994349897, 0.001219597994349897, 0.001219597994349897, 0.001219597994349897, 0.001219597994349897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001219597994349897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63894796
Iteration 2/25 | Loss: 0.00079022
Iteration 3/25 | Loss: 0.00079022
Iteration 4/25 | Loss: 0.00079022
Iteration 5/25 | Loss: 0.00079022
Iteration 6/25 | Loss: 0.00079022
Iteration 7/25 | Loss: 0.00079022
Iteration 8/25 | Loss: 0.00079022
Iteration 9/25 | Loss: 0.00079022
Iteration 10/25 | Loss: 0.00079022
Iteration 11/25 | Loss: 0.00079022
Iteration 12/25 | Loss: 0.00079022
Iteration 13/25 | Loss: 0.00079022
Iteration 14/25 | Loss: 0.00079022
Iteration 15/25 | Loss: 0.00079022
Iteration 16/25 | Loss: 0.00079022
Iteration 17/25 | Loss: 0.00079022
Iteration 18/25 | Loss: 0.00079022
Iteration 19/25 | Loss: 0.00079022
Iteration 20/25 | Loss: 0.00079022
Iteration 21/25 | Loss: 0.00079022
Iteration 22/25 | Loss: 0.00079022
Iteration 23/25 | Loss: 0.00079022
Iteration 24/25 | Loss: 0.00079022
Iteration 25/25 | Loss: 0.00079022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079022
Iteration 2/1000 | Loss: 0.00002681
Iteration 3/1000 | Loss: 0.00002010
Iteration 4/1000 | Loss: 0.00001862
Iteration 5/1000 | Loss: 0.00001792
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001698
Iteration 8/1000 | Loss: 0.00001669
Iteration 9/1000 | Loss: 0.00001646
Iteration 10/1000 | Loss: 0.00001625
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001600
Iteration 13/1000 | Loss: 0.00001599
Iteration 14/1000 | Loss: 0.00001595
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001583
Iteration 17/1000 | Loss: 0.00001582
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001580
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001568
Iteration 26/1000 | Loss: 0.00001568
Iteration 27/1000 | Loss: 0.00001567
Iteration 28/1000 | Loss: 0.00001567
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001557
Iteration 37/1000 | Loss: 0.00001557
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001556
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001549
Iteration 45/1000 | Loss: 0.00001546
Iteration 46/1000 | Loss: 0.00001546
Iteration 47/1000 | Loss: 0.00001546
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001544
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001541
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001541
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001540
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001537
Iteration 75/1000 | Loss: 0.00001537
Iteration 76/1000 | Loss: 0.00001537
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001535
Iteration 83/1000 | Loss: 0.00001535
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00001534
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001533
Iteration 90/1000 | Loss: 0.00001533
Iteration 91/1000 | Loss: 0.00001533
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001532
Iteration 100/1000 | Loss: 0.00001532
Iteration 101/1000 | Loss: 0.00001532
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.53221426444361e-05, 1.53221426444361e-05, 1.53221426444361e-05, 1.53221426444361e-05, 1.53221426444361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.53221426444361e-05

Optimization complete. Final v2v error: 3.283827543258667 mm

Highest mean error: 3.7414610385894775 mm for frame 89

Lowest mean error: 3.04609751701355 mm for frame 121

Saving results

Total time: 36.58266067504883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011530
Iteration 2/25 | Loss: 0.00239134
Iteration 3/25 | Loss: 0.00308331
Iteration 4/25 | Loss: 0.00225282
Iteration 5/25 | Loss: 0.00188710
Iteration 6/25 | Loss: 0.00171088
Iteration 7/25 | Loss: 0.00154330
Iteration 8/25 | Loss: 0.00146146
Iteration 9/25 | Loss: 0.00135445
Iteration 10/25 | Loss: 0.00133348
Iteration 11/25 | Loss: 0.00133743
Iteration 12/25 | Loss: 0.00130985
Iteration 13/25 | Loss: 0.00128562
Iteration 14/25 | Loss: 0.00128502
Iteration 15/25 | Loss: 0.00131105
Iteration 16/25 | Loss: 0.00127737
Iteration 17/25 | Loss: 0.00127526
Iteration 18/25 | Loss: 0.00127379
Iteration 19/25 | Loss: 0.00127347
Iteration 20/25 | Loss: 0.00127331
Iteration 21/25 | Loss: 0.00127377
Iteration 22/25 | Loss: 0.00127328
Iteration 23/25 | Loss: 0.00127294
Iteration 24/25 | Loss: 0.00127203
Iteration 25/25 | Loss: 0.00127130

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52093637
Iteration 2/25 | Loss: 0.00086409
Iteration 3/25 | Loss: 0.00086409
Iteration 4/25 | Loss: 0.00086409
Iteration 5/25 | Loss: 0.00086409
Iteration 6/25 | Loss: 0.00086409
Iteration 7/25 | Loss: 0.00086409
Iteration 8/25 | Loss: 0.00086409
Iteration 9/25 | Loss: 0.00086409
Iteration 10/25 | Loss: 0.00086409
Iteration 11/25 | Loss: 0.00086409
Iteration 12/25 | Loss: 0.00086409
Iteration 13/25 | Loss: 0.00086409
Iteration 14/25 | Loss: 0.00086409
Iteration 15/25 | Loss: 0.00086409
Iteration 16/25 | Loss: 0.00086409
Iteration 17/25 | Loss: 0.00086409
Iteration 18/25 | Loss: 0.00086409
Iteration 19/25 | Loss: 0.00086409
Iteration 20/25 | Loss: 0.00086409
Iteration 21/25 | Loss: 0.00086409
Iteration 22/25 | Loss: 0.00086409
Iteration 23/25 | Loss: 0.00086409
Iteration 24/25 | Loss: 0.00086409
Iteration 25/25 | Loss: 0.00086409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086409
Iteration 2/1000 | Loss: 0.00200631
Iteration 3/1000 | Loss: 0.00095692
Iteration 4/1000 | Loss: 0.00002779
Iteration 5/1000 | Loss: 0.00013730
Iteration 6/1000 | Loss: 0.00002362
Iteration 7/1000 | Loss: 0.00002226
Iteration 8/1000 | Loss: 0.00025688
Iteration 9/1000 | Loss: 0.00081258
Iteration 10/1000 | Loss: 0.00002545
Iteration 11/1000 | Loss: 0.00002149
Iteration 12/1000 | Loss: 0.00002067
Iteration 13/1000 | Loss: 0.00002041
Iteration 14/1000 | Loss: 0.00002014
Iteration 15/1000 | Loss: 0.00002005
Iteration 16/1000 | Loss: 0.00001987
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001949
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001945
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001943
Iteration 29/1000 | Loss: 0.00001942
Iteration 30/1000 | Loss: 0.00001942
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001942
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001940
Iteration 35/1000 | Loss: 0.00001939
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001933
Iteration 42/1000 | Loss: 0.00001933
Iteration 43/1000 | Loss: 0.00001933
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001932
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001930
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001928
Iteration 56/1000 | Loss: 0.00001928
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001926
Iteration 61/1000 | Loss: 0.00001926
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001923
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001922
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001921
Iteration 81/1000 | Loss: 0.00001921
Iteration 82/1000 | Loss: 0.00001920
Iteration 83/1000 | Loss: 0.00001920
Iteration 84/1000 | Loss: 0.00001920
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001919
Iteration 90/1000 | Loss: 0.00001919
Iteration 91/1000 | Loss: 0.00001919
Iteration 92/1000 | Loss: 0.00001919
Iteration 93/1000 | Loss: 0.00001919
Iteration 94/1000 | Loss: 0.00001919
Iteration 95/1000 | Loss: 0.00001918
Iteration 96/1000 | Loss: 0.00001918
Iteration 97/1000 | Loss: 0.00001918
Iteration 98/1000 | Loss: 0.00001918
Iteration 99/1000 | Loss: 0.00001918
Iteration 100/1000 | Loss: 0.00001918
Iteration 101/1000 | Loss: 0.00001918
Iteration 102/1000 | Loss: 0.00001918
Iteration 103/1000 | Loss: 0.00001917
Iteration 104/1000 | Loss: 0.00001917
Iteration 105/1000 | Loss: 0.00001917
Iteration 106/1000 | Loss: 0.00001917
Iteration 107/1000 | Loss: 0.00001917
Iteration 108/1000 | Loss: 0.00001917
Iteration 109/1000 | Loss: 0.00001917
Iteration 110/1000 | Loss: 0.00001917
Iteration 111/1000 | Loss: 0.00001916
Iteration 112/1000 | Loss: 0.00001916
Iteration 113/1000 | Loss: 0.00001916
Iteration 114/1000 | Loss: 0.00001916
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001916
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001915
Iteration 120/1000 | Loss: 0.00001914
Iteration 121/1000 | Loss: 0.00001914
Iteration 122/1000 | Loss: 0.00001914
Iteration 123/1000 | Loss: 0.00001914
Iteration 124/1000 | Loss: 0.00001913
Iteration 125/1000 | Loss: 0.00001913
Iteration 126/1000 | Loss: 0.00001913
Iteration 127/1000 | Loss: 0.00001913
Iteration 128/1000 | Loss: 0.00001913
Iteration 129/1000 | Loss: 0.00001913
Iteration 130/1000 | Loss: 0.00001913
Iteration 131/1000 | Loss: 0.00001913
Iteration 132/1000 | Loss: 0.00001913
Iteration 133/1000 | Loss: 0.00001913
Iteration 134/1000 | Loss: 0.00001913
Iteration 135/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.912746301968582e-05, 1.912746301968582e-05, 1.912746301968582e-05, 1.912746301968582e-05, 1.912746301968582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.912746301968582e-05

Optimization complete. Final v2v error: 3.664189100265503 mm

Highest mean error: 5.086423873901367 mm for frame 110

Lowest mean error: 3.065150737762451 mm for frame 170

Saving results

Total time: 94.35006141662598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01210052
Iteration 2/25 | Loss: 0.00363039
Iteration 3/25 | Loss: 0.00295954
Iteration 4/25 | Loss: 0.00253323
Iteration 5/25 | Loss: 0.00213763
Iteration 6/25 | Loss: 0.00196927
Iteration 7/25 | Loss: 0.00190811
Iteration 8/25 | Loss: 0.00186377
Iteration 9/25 | Loss: 0.00184626
Iteration 10/25 | Loss: 0.00188478
Iteration 11/25 | Loss: 0.00182728
Iteration 12/25 | Loss: 0.00181208
Iteration 13/25 | Loss: 0.00181728
Iteration 14/25 | Loss: 0.00182817
Iteration 15/25 | Loss: 0.00181569
Iteration 16/25 | Loss: 0.00181790
Iteration 17/25 | Loss: 0.00181787
Iteration 18/25 | Loss: 0.00179564
Iteration 19/25 | Loss: 0.00177875
Iteration 20/25 | Loss: 0.00177268
Iteration 21/25 | Loss: 0.00177318
Iteration 22/25 | Loss: 0.00176088
Iteration 23/25 | Loss: 0.00175014
Iteration 24/25 | Loss: 0.00175035
Iteration 25/25 | Loss: 0.00174688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60035652
Iteration 2/25 | Loss: 0.00462764
Iteration 3/25 | Loss: 0.00444123
Iteration 4/25 | Loss: 0.00444123
Iteration 5/25 | Loss: 0.00444123
Iteration 6/25 | Loss: 0.00444123
Iteration 7/25 | Loss: 0.00444123
Iteration 8/25 | Loss: 0.00444123
Iteration 9/25 | Loss: 0.00444123
Iteration 10/25 | Loss: 0.00444123
Iteration 11/25 | Loss: 0.00444122
Iteration 12/25 | Loss: 0.00444122
Iteration 13/25 | Loss: 0.00444122
Iteration 14/25 | Loss: 0.00444123
Iteration 15/25 | Loss: 0.00444123
Iteration 16/25 | Loss: 0.00444122
Iteration 17/25 | Loss: 0.00444122
Iteration 18/25 | Loss: 0.00444123
Iteration 19/25 | Loss: 0.00444123
Iteration 20/25 | Loss: 0.00444122
Iteration 21/25 | Loss: 0.00444122
Iteration 22/25 | Loss: 0.00444122
Iteration 23/25 | Loss: 0.00444122
Iteration 24/25 | Loss: 0.00444122
Iteration 25/25 | Loss: 0.00444122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004441224969923496, 0.004441224969923496, 0.004441224969923496, 0.004441224969923496, 0.004441224969923496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004441224969923496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00444122
Iteration 2/1000 | Loss: 0.00059802
Iteration 3/1000 | Loss: 0.00083283
Iteration 4/1000 | Loss: 0.00095629
Iteration 5/1000 | Loss: 0.00125628
Iteration 6/1000 | Loss: 0.00051261
Iteration 7/1000 | Loss: 0.00059006
Iteration 8/1000 | Loss: 0.00050472
Iteration 9/1000 | Loss: 0.00112781
Iteration 10/1000 | Loss: 0.00037822
Iteration 11/1000 | Loss: 0.00037989
Iteration 12/1000 | Loss: 0.00029554
Iteration 13/1000 | Loss: 0.00070094
Iteration 14/1000 | Loss: 0.00280244
Iteration 15/1000 | Loss: 0.00094567
Iteration 16/1000 | Loss: 0.00025474
Iteration 17/1000 | Loss: 0.00027803
Iteration 18/1000 | Loss: 0.00045255
Iteration 19/1000 | Loss: 0.00018420
Iteration 20/1000 | Loss: 0.00017192
Iteration 21/1000 | Loss: 0.00063821
Iteration 22/1000 | Loss: 0.00043598
Iteration 23/1000 | Loss: 0.00042016
Iteration 24/1000 | Loss: 0.00129287
Iteration 25/1000 | Loss: 0.00082756
Iteration 26/1000 | Loss: 0.00037840
Iteration 27/1000 | Loss: 0.00018008
Iteration 28/1000 | Loss: 0.00016257
Iteration 29/1000 | Loss: 0.00015500
Iteration 30/1000 | Loss: 0.00111519
Iteration 31/1000 | Loss: 0.00016223
Iteration 32/1000 | Loss: 0.00093820
Iteration 33/1000 | Loss: 0.00085044
Iteration 34/1000 | Loss: 0.00085751
Iteration 35/1000 | Loss: 0.00014868
Iteration 36/1000 | Loss: 0.00130989
Iteration 37/1000 | Loss: 0.00048749
Iteration 38/1000 | Loss: 0.00096308
Iteration 39/1000 | Loss: 0.00015192
Iteration 40/1000 | Loss: 0.00012058
Iteration 41/1000 | Loss: 0.00011662
Iteration 42/1000 | Loss: 0.00011400
Iteration 43/1000 | Loss: 0.00011182
Iteration 44/1000 | Loss: 0.00096717
Iteration 45/1000 | Loss: 0.00012308
Iteration 46/1000 | Loss: 0.00011449
Iteration 47/1000 | Loss: 0.00026704
Iteration 48/1000 | Loss: 0.00021065
Iteration 49/1000 | Loss: 0.00010735
Iteration 50/1000 | Loss: 0.00010639
Iteration 51/1000 | Loss: 0.00010558
Iteration 52/1000 | Loss: 0.00010453
Iteration 53/1000 | Loss: 0.00010387
Iteration 54/1000 | Loss: 0.00010332
Iteration 55/1000 | Loss: 0.00098021
Iteration 56/1000 | Loss: 0.00010957
Iteration 57/1000 | Loss: 0.00010289
Iteration 58/1000 | Loss: 0.00009981
Iteration 59/1000 | Loss: 0.00009865
Iteration 60/1000 | Loss: 0.00009783
Iteration 61/1000 | Loss: 0.00095801
Iteration 62/1000 | Loss: 0.00012407
Iteration 63/1000 | Loss: 0.00011162
Iteration 64/1000 | Loss: 0.00010501
Iteration 65/1000 | Loss: 0.00045677
Iteration 66/1000 | Loss: 0.00104118
Iteration 67/1000 | Loss: 0.00010474
Iteration 68/1000 | Loss: 0.00009803
Iteration 69/1000 | Loss: 0.00009736
Iteration 70/1000 | Loss: 0.00009323
Iteration 71/1000 | Loss: 0.00009174
Iteration 72/1000 | Loss: 0.00228003
Iteration 73/1000 | Loss: 0.00046876
Iteration 74/1000 | Loss: 0.00012086
Iteration 75/1000 | Loss: 0.00034447
Iteration 76/1000 | Loss: 0.00010467
Iteration 77/1000 | Loss: 0.00009680
Iteration 78/1000 | Loss: 0.00009292
Iteration 79/1000 | Loss: 0.00047735
Iteration 80/1000 | Loss: 0.00008922
Iteration 81/1000 | Loss: 0.00008718
Iteration 82/1000 | Loss: 0.00008528
Iteration 83/1000 | Loss: 0.00008338
Iteration 84/1000 | Loss: 0.00008256
Iteration 85/1000 | Loss: 0.00008208
Iteration 86/1000 | Loss: 0.00008170
Iteration 87/1000 | Loss: 0.00008143
Iteration 88/1000 | Loss: 0.00093173
Iteration 89/1000 | Loss: 0.00008827
Iteration 90/1000 | Loss: 0.00008212
Iteration 91/1000 | Loss: 0.00007945
Iteration 92/1000 | Loss: 0.00007847
Iteration 93/1000 | Loss: 0.00007797
Iteration 94/1000 | Loss: 0.00007760
Iteration 95/1000 | Loss: 0.00007737
Iteration 96/1000 | Loss: 0.00007715
Iteration 97/1000 | Loss: 0.00007691
Iteration 98/1000 | Loss: 0.00007672
Iteration 99/1000 | Loss: 0.00007652
Iteration 100/1000 | Loss: 0.00007636
Iteration 101/1000 | Loss: 0.00007630
Iteration 102/1000 | Loss: 0.00007626
Iteration 103/1000 | Loss: 0.00007624
Iteration 104/1000 | Loss: 0.00097943
Iteration 105/1000 | Loss: 0.00026500
Iteration 106/1000 | Loss: 0.00007671
Iteration 107/1000 | Loss: 0.00007626
Iteration 108/1000 | Loss: 0.00091785
Iteration 109/1000 | Loss: 0.00048161
Iteration 110/1000 | Loss: 0.00100793
Iteration 111/1000 | Loss: 0.00555189
Iteration 112/1000 | Loss: 0.00674870
Iteration 113/1000 | Loss: 0.00154129
Iteration 114/1000 | Loss: 0.00085269
Iteration 115/1000 | Loss: 0.00137324
Iteration 116/1000 | Loss: 0.00099094
Iteration 117/1000 | Loss: 0.00016418
Iteration 118/1000 | Loss: 0.00009044
Iteration 119/1000 | Loss: 0.00007676
Iteration 120/1000 | Loss: 0.00007090
Iteration 121/1000 | Loss: 0.00006709
Iteration 122/1000 | Loss: 0.00006441
Iteration 123/1000 | Loss: 0.00006219
Iteration 124/1000 | Loss: 0.00006055
Iteration 125/1000 | Loss: 0.00005878
Iteration 126/1000 | Loss: 0.00005768
Iteration 127/1000 | Loss: 0.00005692
Iteration 128/1000 | Loss: 0.00060080
Iteration 129/1000 | Loss: 0.00005909
Iteration 130/1000 | Loss: 0.00005691
Iteration 131/1000 | Loss: 0.00005565
Iteration 132/1000 | Loss: 0.00005454
Iteration 133/1000 | Loss: 0.00005356
Iteration 134/1000 | Loss: 0.00005305
Iteration 135/1000 | Loss: 0.00005277
Iteration 136/1000 | Loss: 0.00089565
Iteration 137/1000 | Loss: 0.00022155
Iteration 138/1000 | Loss: 0.00009048
Iteration 139/1000 | Loss: 0.00006064
Iteration 140/1000 | Loss: 0.00005589
Iteration 141/1000 | Loss: 0.00005301
Iteration 142/1000 | Loss: 0.00005159
Iteration 143/1000 | Loss: 0.00005084
Iteration 144/1000 | Loss: 0.00005046
Iteration 145/1000 | Loss: 0.00005011
Iteration 146/1000 | Loss: 0.00004987
Iteration 147/1000 | Loss: 0.00004971
Iteration 148/1000 | Loss: 0.00004962
Iteration 149/1000 | Loss: 0.00004957
Iteration 150/1000 | Loss: 0.00004956
Iteration 151/1000 | Loss: 0.00004941
Iteration 152/1000 | Loss: 0.00004940
Iteration 153/1000 | Loss: 0.00004934
Iteration 154/1000 | Loss: 0.00004920
Iteration 155/1000 | Loss: 0.00004918
Iteration 156/1000 | Loss: 0.00004909
Iteration 157/1000 | Loss: 0.00004905
Iteration 158/1000 | Loss: 0.00004905
Iteration 159/1000 | Loss: 0.00004904
Iteration 160/1000 | Loss: 0.00004903
Iteration 161/1000 | Loss: 0.00004902
Iteration 162/1000 | Loss: 0.00004901
Iteration 163/1000 | Loss: 0.00004901
Iteration 164/1000 | Loss: 0.00004900
Iteration 165/1000 | Loss: 0.00004899
Iteration 166/1000 | Loss: 0.00004899
Iteration 167/1000 | Loss: 0.00004898
Iteration 168/1000 | Loss: 0.00004898
Iteration 169/1000 | Loss: 0.00004897
Iteration 170/1000 | Loss: 0.00004897
Iteration 171/1000 | Loss: 0.00004897
Iteration 172/1000 | Loss: 0.00004897
Iteration 173/1000 | Loss: 0.00004897
Iteration 174/1000 | Loss: 0.00004897
Iteration 175/1000 | Loss: 0.00004896
Iteration 176/1000 | Loss: 0.00004896
Iteration 177/1000 | Loss: 0.00004896
Iteration 178/1000 | Loss: 0.00004896
Iteration 179/1000 | Loss: 0.00004896
Iteration 180/1000 | Loss: 0.00004896
Iteration 181/1000 | Loss: 0.00004896
Iteration 182/1000 | Loss: 0.00004896
Iteration 183/1000 | Loss: 0.00004895
Iteration 184/1000 | Loss: 0.00004895
Iteration 185/1000 | Loss: 0.00004895
Iteration 186/1000 | Loss: 0.00004895
Iteration 187/1000 | Loss: 0.00004895
Iteration 188/1000 | Loss: 0.00004895
Iteration 189/1000 | Loss: 0.00004894
Iteration 190/1000 | Loss: 0.00004894
Iteration 191/1000 | Loss: 0.00004894
Iteration 192/1000 | Loss: 0.00004894
Iteration 193/1000 | Loss: 0.00004894
Iteration 194/1000 | Loss: 0.00004894
Iteration 195/1000 | Loss: 0.00004894
Iteration 196/1000 | Loss: 0.00004894
Iteration 197/1000 | Loss: 0.00004893
Iteration 198/1000 | Loss: 0.00004893
Iteration 199/1000 | Loss: 0.00004893
Iteration 200/1000 | Loss: 0.00004893
Iteration 201/1000 | Loss: 0.00004893
Iteration 202/1000 | Loss: 0.00004893
Iteration 203/1000 | Loss: 0.00004892
Iteration 204/1000 | Loss: 0.00004892
Iteration 205/1000 | Loss: 0.00004892
Iteration 206/1000 | Loss: 0.00004892
Iteration 207/1000 | Loss: 0.00004892
Iteration 208/1000 | Loss: 0.00004892
Iteration 209/1000 | Loss: 0.00004891
Iteration 210/1000 | Loss: 0.00004891
Iteration 211/1000 | Loss: 0.00004891
Iteration 212/1000 | Loss: 0.00004890
Iteration 213/1000 | Loss: 0.00004890
Iteration 214/1000 | Loss: 0.00004889
Iteration 215/1000 | Loss: 0.00004889
Iteration 216/1000 | Loss: 0.00004889
Iteration 217/1000 | Loss: 0.00004889
Iteration 218/1000 | Loss: 0.00004889
Iteration 219/1000 | Loss: 0.00004889
Iteration 220/1000 | Loss: 0.00004888
Iteration 221/1000 | Loss: 0.00004888
Iteration 222/1000 | Loss: 0.00004888
Iteration 223/1000 | Loss: 0.00004888
Iteration 224/1000 | Loss: 0.00004888
Iteration 225/1000 | Loss: 0.00004888
Iteration 226/1000 | Loss: 0.00004888
Iteration 227/1000 | Loss: 0.00004888
Iteration 228/1000 | Loss: 0.00004887
Iteration 229/1000 | Loss: 0.00004887
Iteration 230/1000 | Loss: 0.00004887
Iteration 231/1000 | Loss: 0.00004887
Iteration 232/1000 | Loss: 0.00004887
Iteration 233/1000 | Loss: 0.00004887
Iteration 234/1000 | Loss: 0.00004887
Iteration 235/1000 | Loss: 0.00004887
Iteration 236/1000 | Loss: 0.00004887
Iteration 237/1000 | Loss: 0.00004886
Iteration 238/1000 | Loss: 0.00004886
Iteration 239/1000 | Loss: 0.00004886
Iteration 240/1000 | Loss: 0.00004886
Iteration 241/1000 | Loss: 0.00004886
Iteration 242/1000 | Loss: 0.00004886
Iteration 243/1000 | Loss: 0.00004886
Iteration 244/1000 | Loss: 0.00004886
Iteration 245/1000 | Loss: 0.00004886
Iteration 246/1000 | Loss: 0.00004886
Iteration 247/1000 | Loss: 0.00004885
Iteration 248/1000 | Loss: 0.00004885
Iteration 249/1000 | Loss: 0.00004885
Iteration 250/1000 | Loss: 0.00004885
Iteration 251/1000 | Loss: 0.00004885
Iteration 252/1000 | Loss: 0.00004885
Iteration 253/1000 | Loss: 0.00004885
Iteration 254/1000 | Loss: 0.00004885
Iteration 255/1000 | Loss: 0.00004885
Iteration 256/1000 | Loss: 0.00004884
Iteration 257/1000 | Loss: 0.00004884
Iteration 258/1000 | Loss: 0.00004884
Iteration 259/1000 | Loss: 0.00004884
Iteration 260/1000 | Loss: 0.00004883
Iteration 261/1000 | Loss: 0.00004883
Iteration 262/1000 | Loss: 0.00004883
Iteration 263/1000 | Loss: 0.00004883
Iteration 264/1000 | Loss: 0.00004883
Iteration 265/1000 | Loss: 0.00004882
Iteration 266/1000 | Loss: 0.00004882
Iteration 267/1000 | Loss: 0.00004882
Iteration 268/1000 | Loss: 0.00004882
Iteration 269/1000 | Loss: 0.00004882
Iteration 270/1000 | Loss: 0.00004882
Iteration 271/1000 | Loss: 0.00004882
Iteration 272/1000 | Loss: 0.00004881
Iteration 273/1000 | Loss: 0.00004881
Iteration 274/1000 | Loss: 0.00004881
Iteration 275/1000 | Loss: 0.00004881
Iteration 276/1000 | Loss: 0.00004881
Iteration 277/1000 | Loss: 0.00004881
Iteration 278/1000 | Loss: 0.00004881
Iteration 279/1000 | Loss: 0.00004881
Iteration 280/1000 | Loss: 0.00004880
Iteration 281/1000 | Loss: 0.00004880
Iteration 282/1000 | Loss: 0.00004880
Iteration 283/1000 | Loss: 0.00004880
Iteration 284/1000 | Loss: 0.00004880
Iteration 285/1000 | Loss: 0.00004880
Iteration 286/1000 | Loss: 0.00004880
Iteration 287/1000 | Loss: 0.00004880
Iteration 288/1000 | Loss: 0.00004880
Iteration 289/1000 | Loss: 0.00004880
Iteration 290/1000 | Loss: 0.00004880
Iteration 291/1000 | Loss: 0.00004880
Iteration 292/1000 | Loss: 0.00004880
Iteration 293/1000 | Loss: 0.00004880
Iteration 294/1000 | Loss: 0.00004879
Iteration 295/1000 | Loss: 0.00004879
Iteration 296/1000 | Loss: 0.00004879
Iteration 297/1000 | Loss: 0.00004879
Iteration 298/1000 | Loss: 0.00004879
Iteration 299/1000 | Loss: 0.00004878
Iteration 300/1000 | Loss: 0.00004878
Iteration 301/1000 | Loss: 0.00004878
Iteration 302/1000 | Loss: 0.00004878
Iteration 303/1000 | Loss: 0.00004878
Iteration 304/1000 | Loss: 0.00004878
Iteration 305/1000 | Loss: 0.00004878
Iteration 306/1000 | Loss: 0.00004878
Iteration 307/1000 | Loss: 0.00004878
Iteration 308/1000 | Loss: 0.00004877
Iteration 309/1000 | Loss: 0.00004877
Iteration 310/1000 | Loss: 0.00004877
Iteration 311/1000 | Loss: 0.00004877
Iteration 312/1000 | Loss: 0.00004877
Iteration 313/1000 | Loss: 0.00004877
Iteration 314/1000 | Loss: 0.00004877
Iteration 315/1000 | Loss: 0.00004877
Iteration 316/1000 | Loss: 0.00004877
Iteration 317/1000 | Loss: 0.00004877
Iteration 318/1000 | Loss: 0.00004877
Iteration 319/1000 | Loss: 0.00004877
Iteration 320/1000 | Loss: 0.00004877
Iteration 321/1000 | Loss: 0.00004877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [4.876989987678826e-05, 4.876989987678826e-05, 4.876989987678826e-05, 4.876989987678826e-05, 4.876989987678826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.876989987678826e-05

Optimization complete. Final v2v error: 4.974825382232666 mm

Highest mean error: 11.664502143859863 mm for frame 74

Lowest mean error: 4.16225528717041 mm for frame 115

Saving results

Total time: 283.846093416214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065076
Iteration 2/25 | Loss: 0.00203721
Iteration 3/25 | Loss: 0.00152320
Iteration 4/25 | Loss: 0.00142941
Iteration 5/25 | Loss: 0.00144074
Iteration 6/25 | Loss: 0.00143750
Iteration 7/25 | Loss: 0.00132084
Iteration 8/25 | Loss: 0.00127197
Iteration 9/25 | Loss: 0.00124895
Iteration 10/25 | Loss: 0.00124054
Iteration 11/25 | Loss: 0.00124119
Iteration 12/25 | Loss: 0.00123911
Iteration 13/25 | Loss: 0.00123789
Iteration 14/25 | Loss: 0.00123788
Iteration 15/25 | Loss: 0.00123788
Iteration 16/25 | Loss: 0.00123788
Iteration 17/25 | Loss: 0.00123787
Iteration 18/25 | Loss: 0.00123787
Iteration 19/25 | Loss: 0.00123787
Iteration 20/25 | Loss: 0.00123787
Iteration 21/25 | Loss: 0.00123787
Iteration 22/25 | Loss: 0.00123787
Iteration 23/25 | Loss: 0.00123787
Iteration 24/25 | Loss: 0.00123787
Iteration 25/25 | Loss: 0.00123787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58712649
Iteration 2/25 | Loss: 0.00081203
Iteration 3/25 | Loss: 0.00081202
Iteration 4/25 | Loss: 0.00081202
Iteration 5/25 | Loss: 0.00081202
Iteration 6/25 | Loss: 0.00081202
Iteration 7/25 | Loss: 0.00081202
Iteration 8/25 | Loss: 0.00081202
Iteration 9/25 | Loss: 0.00081202
Iteration 10/25 | Loss: 0.00081202
Iteration 11/25 | Loss: 0.00081202
Iteration 12/25 | Loss: 0.00081202
Iteration 13/25 | Loss: 0.00081202
Iteration 14/25 | Loss: 0.00081202
Iteration 15/25 | Loss: 0.00081202
Iteration 16/25 | Loss: 0.00081202
Iteration 17/25 | Loss: 0.00081202
Iteration 18/25 | Loss: 0.00081202
Iteration 19/25 | Loss: 0.00081202
Iteration 20/25 | Loss: 0.00081202
Iteration 21/25 | Loss: 0.00081202
Iteration 22/25 | Loss: 0.00081202
Iteration 23/25 | Loss: 0.00081202
Iteration 24/25 | Loss: 0.00081202
Iteration 25/25 | Loss: 0.00081202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081202
Iteration 2/1000 | Loss: 0.00003461
Iteration 3/1000 | Loss: 0.00002614
Iteration 4/1000 | Loss: 0.00005382
Iteration 5/1000 | Loss: 0.00002262
Iteration 6/1000 | Loss: 0.00004528
Iteration 7/1000 | Loss: 0.00002126
Iteration 8/1000 | Loss: 0.00002632
Iteration 9/1000 | Loss: 0.00002141
Iteration 10/1000 | Loss: 0.00002057
Iteration 11/1000 | Loss: 0.00002004
Iteration 12/1000 | Loss: 0.00002056
Iteration 13/1000 | Loss: 0.00002032
Iteration 14/1000 | Loss: 0.00002049
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001955
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001930
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001912
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001905
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001900
Iteration 25/1000 | Loss: 0.00001899
Iteration 26/1000 | Loss: 0.00001899
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001884
Iteration 37/1000 | Loss: 0.00003960
Iteration 38/1000 | Loss: 0.00001979
Iteration 39/1000 | Loss: 0.00001879
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001874
Iteration 42/1000 | Loss: 0.00001874
Iteration 43/1000 | Loss: 0.00001873
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001872
Iteration 47/1000 | Loss: 0.00001872
Iteration 48/1000 | Loss: 0.00001872
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001872
Iteration 52/1000 | Loss: 0.00001872
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001871
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001869
Iteration 68/1000 | Loss: 0.00001869
Iteration 69/1000 | Loss: 0.00001869
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001868
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001868
Iteration 88/1000 | Loss: 0.00001868
Iteration 89/1000 | Loss: 0.00001868
Iteration 90/1000 | Loss: 0.00001868
Iteration 91/1000 | Loss: 0.00001868
Iteration 92/1000 | Loss: 0.00001868
Iteration 93/1000 | Loss: 0.00001868
Iteration 94/1000 | Loss: 0.00001868
Iteration 95/1000 | Loss: 0.00001868
Iteration 96/1000 | Loss: 0.00001868
Iteration 97/1000 | Loss: 0.00001868
Iteration 98/1000 | Loss: 0.00001868
Iteration 99/1000 | Loss: 0.00001868
Iteration 100/1000 | Loss: 0.00001868
Iteration 101/1000 | Loss: 0.00001868
Iteration 102/1000 | Loss: 0.00001868
Iteration 103/1000 | Loss: 0.00001868
Iteration 104/1000 | Loss: 0.00001868
Iteration 105/1000 | Loss: 0.00001868
Iteration 106/1000 | Loss: 0.00001868
Iteration 107/1000 | Loss: 0.00001868
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001868
Iteration 118/1000 | Loss: 0.00001868
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001868
Iteration 122/1000 | Loss: 0.00001868
Iteration 123/1000 | Loss: 0.00001868
Iteration 124/1000 | Loss: 0.00001868
Iteration 125/1000 | Loss: 0.00001868
Iteration 126/1000 | Loss: 0.00001868
Iteration 127/1000 | Loss: 0.00001868
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.867666833277326e-05, 1.867666833277326e-05, 1.867666833277326e-05, 1.867666833277326e-05, 1.867666833277326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.867666833277326e-05

Optimization complete. Final v2v error: 3.5945777893066406 mm

Highest mean error: 4.949462890625 mm for frame 80

Lowest mean error: 3.08172607421875 mm for frame 18

Saving results

Total time: 62.48954772949219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774252
Iteration 2/25 | Loss: 0.00172432
Iteration 3/25 | Loss: 0.00145116
Iteration 4/25 | Loss: 0.00142210
Iteration 5/25 | Loss: 0.00139955
Iteration 6/25 | Loss: 0.00139814
Iteration 7/25 | Loss: 0.00137294
Iteration 8/25 | Loss: 0.00136665
Iteration 9/25 | Loss: 0.00136586
Iteration 10/25 | Loss: 0.00136569
Iteration 11/25 | Loss: 0.00136569
Iteration 12/25 | Loss: 0.00136569
Iteration 13/25 | Loss: 0.00136569
Iteration 14/25 | Loss: 0.00136569
Iteration 15/25 | Loss: 0.00136569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001365693286061287, 0.001365693286061287, 0.001365693286061287, 0.001365693286061287, 0.001365693286061287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001365693286061287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65015554
Iteration 2/25 | Loss: 0.00084791
Iteration 3/25 | Loss: 0.00084789
Iteration 4/25 | Loss: 0.00084789
Iteration 5/25 | Loss: 0.00084789
Iteration 6/25 | Loss: 0.00084789
Iteration 7/25 | Loss: 0.00084789
Iteration 8/25 | Loss: 0.00084789
Iteration 9/25 | Loss: 0.00084789
Iteration 10/25 | Loss: 0.00084789
Iteration 11/25 | Loss: 0.00084789
Iteration 12/25 | Loss: 0.00084789
Iteration 13/25 | Loss: 0.00084789
Iteration 14/25 | Loss: 0.00084789
Iteration 15/25 | Loss: 0.00084789
Iteration 16/25 | Loss: 0.00084789
Iteration 17/25 | Loss: 0.00084789
Iteration 18/25 | Loss: 0.00084789
Iteration 19/25 | Loss: 0.00084789
Iteration 20/25 | Loss: 0.00084789
Iteration 21/25 | Loss: 0.00084789
Iteration 22/25 | Loss: 0.00084789
Iteration 23/25 | Loss: 0.00084789
Iteration 24/25 | Loss: 0.00084789
Iteration 25/25 | Loss: 0.00084789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084789
Iteration 2/1000 | Loss: 0.00005805
Iteration 3/1000 | Loss: 0.00006365
Iteration 4/1000 | Loss: 0.00003609
Iteration 5/1000 | Loss: 0.00004015
Iteration 6/1000 | Loss: 0.00003284
Iteration 7/1000 | Loss: 0.00003987
Iteration 8/1000 | Loss: 0.00003100
Iteration 9/1000 | Loss: 0.00003728
Iteration 10/1000 | Loss: 0.00004659
Iteration 11/1000 | Loss: 0.00003537
Iteration 12/1000 | Loss: 0.00003967
Iteration 13/1000 | Loss: 0.00004421
Iteration 14/1000 | Loss: 0.00004429
Iteration 15/1000 | Loss: 0.00004387
Iteration 16/1000 | Loss: 0.00004449
Iteration 17/1000 | Loss: 0.00004138
Iteration 18/1000 | Loss: 0.00002918
Iteration 19/1000 | Loss: 0.00004401
Iteration 20/1000 | Loss: 0.00003953
Iteration 21/1000 | Loss: 0.00004830
Iteration 22/1000 | Loss: 0.00003107
Iteration 23/1000 | Loss: 0.00002981
Iteration 24/1000 | Loss: 0.00002884
Iteration 25/1000 | Loss: 0.00002831
Iteration 26/1000 | Loss: 0.00002799
Iteration 27/1000 | Loss: 0.00002782
Iteration 28/1000 | Loss: 0.00002766
Iteration 29/1000 | Loss: 0.00002755
Iteration 30/1000 | Loss: 0.00002751
Iteration 31/1000 | Loss: 0.00002748
Iteration 32/1000 | Loss: 0.00002748
Iteration 33/1000 | Loss: 0.00002746
Iteration 34/1000 | Loss: 0.00002746
Iteration 35/1000 | Loss: 0.00002746
Iteration 36/1000 | Loss: 0.00002746
Iteration 37/1000 | Loss: 0.00002745
Iteration 38/1000 | Loss: 0.00002745
Iteration 39/1000 | Loss: 0.00002745
Iteration 40/1000 | Loss: 0.00002745
Iteration 41/1000 | Loss: 0.00002743
Iteration 42/1000 | Loss: 0.00002739
Iteration 43/1000 | Loss: 0.00002737
Iteration 44/1000 | Loss: 0.00002736
Iteration 45/1000 | Loss: 0.00002735
Iteration 46/1000 | Loss: 0.00002735
Iteration 47/1000 | Loss: 0.00002727
Iteration 48/1000 | Loss: 0.00002727
Iteration 49/1000 | Loss: 0.00002724
Iteration 50/1000 | Loss: 0.00002723
Iteration 51/1000 | Loss: 0.00002723
Iteration 52/1000 | Loss: 0.00002722
Iteration 53/1000 | Loss: 0.00002721
Iteration 54/1000 | Loss: 0.00002720
Iteration 55/1000 | Loss: 0.00002719
Iteration 56/1000 | Loss: 0.00002719
Iteration 57/1000 | Loss: 0.00002718
Iteration 58/1000 | Loss: 0.00002718
Iteration 59/1000 | Loss: 0.00002717
Iteration 60/1000 | Loss: 0.00002717
Iteration 61/1000 | Loss: 0.00002716
Iteration 62/1000 | Loss: 0.00002716
Iteration 63/1000 | Loss: 0.00002716
Iteration 64/1000 | Loss: 0.00002715
Iteration 65/1000 | Loss: 0.00002715
Iteration 66/1000 | Loss: 0.00002715
Iteration 67/1000 | Loss: 0.00002714
Iteration 68/1000 | Loss: 0.00002714
Iteration 69/1000 | Loss: 0.00002714
Iteration 70/1000 | Loss: 0.00002713
Iteration 71/1000 | Loss: 0.00002712
Iteration 72/1000 | Loss: 0.00002712
Iteration 73/1000 | Loss: 0.00002712
Iteration 74/1000 | Loss: 0.00002710
Iteration 75/1000 | Loss: 0.00002710
Iteration 76/1000 | Loss: 0.00002710
Iteration 77/1000 | Loss: 0.00002709
Iteration 78/1000 | Loss: 0.00002708
Iteration 79/1000 | Loss: 0.00002708
Iteration 80/1000 | Loss: 0.00002708
Iteration 81/1000 | Loss: 0.00002708
Iteration 82/1000 | Loss: 0.00002708
Iteration 83/1000 | Loss: 0.00002708
Iteration 84/1000 | Loss: 0.00002707
Iteration 85/1000 | Loss: 0.00002707
Iteration 86/1000 | Loss: 0.00002707
Iteration 87/1000 | Loss: 0.00002706
Iteration 88/1000 | Loss: 0.00002706
Iteration 89/1000 | Loss: 0.00002705
Iteration 90/1000 | Loss: 0.00002705
Iteration 91/1000 | Loss: 0.00002704
Iteration 92/1000 | Loss: 0.00002704
Iteration 93/1000 | Loss: 0.00002704
Iteration 94/1000 | Loss: 0.00002704
Iteration 95/1000 | Loss: 0.00002704
Iteration 96/1000 | Loss: 0.00002704
Iteration 97/1000 | Loss: 0.00002703
Iteration 98/1000 | Loss: 0.00002703
Iteration 99/1000 | Loss: 0.00002703
Iteration 100/1000 | Loss: 0.00002702
Iteration 101/1000 | Loss: 0.00002702
Iteration 102/1000 | Loss: 0.00002702
Iteration 103/1000 | Loss: 0.00002702
Iteration 104/1000 | Loss: 0.00002702
Iteration 105/1000 | Loss: 0.00002701
Iteration 106/1000 | Loss: 0.00002701
Iteration 107/1000 | Loss: 0.00002701
Iteration 108/1000 | Loss: 0.00002700
Iteration 109/1000 | Loss: 0.00002700
Iteration 110/1000 | Loss: 0.00002700
Iteration 111/1000 | Loss: 0.00002700
Iteration 112/1000 | Loss: 0.00002700
Iteration 113/1000 | Loss: 0.00002700
Iteration 114/1000 | Loss: 0.00002700
Iteration 115/1000 | Loss: 0.00002700
Iteration 116/1000 | Loss: 0.00002699
Iteration 117/1000 | Loss: 0.00002699
Iteration 118/1000 | Loss: 0.00002699
Iteration 119/1000 | Loss: 0.00002699
Iteration 120/1000 | Loss: 0.00002699
Iteration 121/1000 | Loss: 0.00002699
Iteration 122/1000 | Loss: 0.00002698
Iteration 123/1000 | Loss: 0.00002698
Iteration 124/1000 | Loss: 0.00002698
Iteration 125/1000 | Loss: 0.00002698
Iteration 126/1000 | Loss: 0.00002698
Iteration 127/1000 | Loss: 0.00002698
Iteration 128/1000 | Loss: 0.00002698
Iteration 129/1000 | Loss: 0.00002697
Iteration 130/1000 | Loss: 0.00002697
Iteration 131/1000 | Loss: 0.00002697
Iteration 132/1000 | Loss: 0.00002697
Iteration 133/1000 | Loss: 0.00002697
Iteration 134/1000 | Loss: 0.00002697
Iteration 135/1000 | Loss: 0.00002697
Iteration 136/1000 | Loss: 0.00002697
Iteration 137/1000 | Loss: 0.00002697
Iteration 138/1000 | Loss: 0.00002697
Iteration 139/1000 | Loss: 0.00002696
Iteration 140/1000 | Loss: 0.00002696
Iteration 141/1000 | Loss: 0.00002696
Iteration 142/1000 | Loss: 0.00002696
Iteration 143/1000 | Loss: 0.00002696
Iteration 144/1000 | Loss: 0.00002696
Iteration 145/1000 | Loss: 0.00002696
Iteration 146/1000 | Loss: 0.00002696
Iteration 147/1000 | Loss: 0.00002695
Iteration 148/1000 | Loss: 0.00002695
Iteration 149/1000 | Loss: 0.00002695
Iteration 150/1000 | Loss: 0.00002695
Iteration 151/1000 | Loss: 0.00002695
Iteration 152/1000 | Loss: 0.00002695
Iteration 153/1000 | Loss: 0.00002695
Iteration 154/1000 | Loss: 0.00002695
Iteration 155/1000 | Loss: 0.00002695
Iteration 156/1000 | Loss: 0.00002694
Iteration 157/1000 | Loss: 0.00002694
Iteration 158/1000 | Loss: 0.00002694
Iteration 159/1000 | Loss: 0.00002694
Iteration 160/1000 | Loss: 0.00002694
Iteration 161/1000 | Loss: 0.00002694
Iteration 162/1000 | Loss: 0.00002694
Iteration 163/1000 | Loss: 0.00002694
Iteration 164/1000 | Loss: 0.00002694
Iteration 165/1000 | Loss: 0.00002694
Iteration 166/1000 | Loss: 0.00002693
Iteration 167/1000 | Loss: 0.00002693
Iteration 168/1000 | Loss: 0.00002693
Iteration 169/1000 | Loss: 0.00002693
Iteration 170/1000 | Loss: 0.00002693
Iteration 171/1000 | Loss: 0.00002693
Iteration 172/1000 | Loss: 0.00002693
Iteration 173/1000 | Loss: 0.00002693
Iteration 174/1000 | Loss: 0.00002693
Iteration 175/1000 | Loss: 0.00002692
Iteration 176/1000 | Loss: 0.00002692
Iteration 177/1000 | Loss: 0.00002692
Iteration 178/1000 | Loss: 0.00002692
Iteration 179/1000 | Loss: 0.00002692
Iteration 180/1000 | Loss: 0.00002692
Iteration 181/1000 | Loss: 0.00002692
Iteration 182/1000 | Loss: 0.00002692
Iteration 183/1000 | Loss: 0.00002692
Iteration 184/1000 | Loss: 0.00002692
Iteration 185/1000 | Loss: 0.00002692
Iteration 186/1000 | Loss: 0.00002692
Iteration 187/1000 | Loss: 0.00002692
Iteration 188/1000 | Loss: 0.00002691
Iteration 189/1000 | Loss: 0.00002691
Iteration 190/1000 | Loss: 0.00002691
Iteration 191/1000 | Loss: 0.00002691
Iteration 192/1000 | Loss: 0.00002691
Iteration 193/1000 | Loss: 0.00002691
Iteration 194/1000 | Loss: 0.00002691
Iteration 195/1000 | Loss: 0.00002691
Iteration 196/1000 | Loss: 0.00002691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.6914680347545072e-05, 2.6914680347545072e-05, 2.6914680347545072e-05, 2.6914680347545072e-05, 2.6914680347545072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6914680347545072e-05

Optimization complete. Final v2v error: 4.195777893066406 mm

Highest mean error: 6.045602798461914 mm for frame 40

Lowest mean error: 3.2865469455718994 mm for frame 146

Saving results

Total time: 82.65563106536865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527869
Iteration 2/25 | Loss: 0.00144662
Iteration 3/25 | Loss: 0.00126083
Iteration 4/25 | Loss: 0.00124344
Iteration 5/25 | Loss: 0.00124013
Iteration 6/25 | Loss: 0.00123957
Iteration 7/25 | Loss: 0.00123957
Iteration 8/25 | Loss: 0.00123957
Iteration 9/25 | Loss: 0.00123957
Iteration 10/25 | Loss: 0.00123957
Iteration 11/25 | Loss: 0.00123957
Iteration 12/25 | Loss: 0.00123957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012395724188536406, 0.0012395724188536406, 0.0012395724188536406, 0.0012395724188536406, 0.0012395724188536406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012395724188536406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65417123
Iteration 2/25 | Loss: 0.00067612
Iteration 3/25 | Loss: 0.00067611
Iteration 4/25 | Loss: 0.00067611
Iteration 5/25 | Loss: 0.00067611
Iteration 6/25 | Loss: 0.00067611
Iteration 7/25 | Loss: 0.00067611
Iteration 8/25 | Loss: 0.00067611
Iteration 9/25 | Loss: 0.00067610
Iteration 10/25 | Loss: 0.00067610
Iteration 11/25 | Loss: 0.00067610
Iteration 12/25 | Loss: 0.00067610
Iteration 13/25 | Loss: 0.00067610
Iteration 14/25 | Loss: 0.00067610
Iteration 15/25 | Loss: 0.00067610
Iteration 16/25 | Loss: 0.00067610
Iteration 17/25 | Loss: 0.00067610
Iteration 18/25 | Loss: 0.00067610
Iteration 19/25 | Loss: 0.00067610
Iteration 20/25 | Loss: 0.00067610
Iteration 21/25 | Loss: 0.00067610
Iteration 22/25 | Loss: 0.00067610
Iteration 23/25 | Loss: 0.00067610
Iteration 24/25 | Loss: 0.00067610
Iteration 25/25 | Loss: 0.00067610

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067610
Iteration 2/1000 | Loss: 0.00003121
Iteration 3/1000 | Loss: 0.00002023
Iteration 4/1000 | Loss: 0.00001802
Iteration 5/1000 | Loss: 0.00001691
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001482
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001440
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001429
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001419
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001413
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001402
Iteration 31/1000 | Loss: 0.00001400
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001395
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001384
Iteration 68/1000 | Loss: 0.00001384
Iteration 69/1000 | Loss: 0.00001384
Iteration 70/1000 | Loss: 0.00001384
Iteration 71/1000 | Loss: 0.00001384
Iteration 72/1000 | Loss: 0.00001383
Iteration 73/1000 | Loss: 0.00001383
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001383
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001380
Iteration 88/1000 | Loss: 0.00001380
Iteration 89/1000 | Loss: 0.00001380
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001379
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001376
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001372
Iteration 116/1000 | Loss: 0.00001372
Iteration 117/1000 | Loss: 0.00001372
Iteration 118/1000 | Loss: 0.00001372
Iteration 119/1000 | Loss: 0.00001372
Iteration 120/1000 | Loss: 0.00001372
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001371
Iteration 125/1000 | Loss: 0.00001371
Iteration 126/1000 | Loss: 0.00001371
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001370
Iteration 133/1000 | Loss: 0.00001370
Iteration 134/1000 | Loss: 0.00001370
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001369
Iteration 139/1000 | Loss: 0.00001369
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001369
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001369
Iteration 151/1000 | Loss: 0.00001369
Iteration 152/1000 | Loss: 0.00001369
Iteration 153/1000 | Loss: 0.00001369
Iteration 154/1000 | Loss: 0.00001369
Iteration 155/1000 | Loss: 0.00001369
Iteration 156/1000 | Loss: 0.00001369
Iteration 157/1000 | Loss: 0.00001369
Iteration 158/1000 | Loss: 0.00001369
Iteration 159/1000 | Loss: 0.00001369
Iteration 160/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.3688077160622925e-05, 1.3688077160622925e-05, 1.3688077160622925e-05, 1.3688077160622925e-05, 1.3688077160622925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3688077160622925e-05

Optimization complete. Final v2v error: 3.1388890743255615 mm

Highest mean error: 3.4585554599761963 mm for frame 45

Lowest mean error: 2.7789175510406494 mm for frame 104

Saving results

Total time: 46.74106192588806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027085
Iteration 2/25 | Loss: 0.00254753
Iteration 3/25 | Loss: 0.00206150
Iteration 4/25 | Loss: 0.00171795
Iteration 5/25 | Loss: 0.00159842
Iteration 6/25 | Loss: 0.00154838
Iteration 7/25 | Loss: 0.00145882
Iteration 8/25 | Loss: 0.00140648
Iteration 9/25 | Loss: 0.00137080
Iteration 10/25 | Loss: 0.00134657
Iteration 11/25 | Loss: 0.00132654
Iteration 12/25 | Loss: 0.00129212
Iteration 13/25 | Loss: 0.00127643
Iteration 14/25 | Loss: 0.00125558
Iteration 15/25 | Loss: 0.00124735
Iteration 16/25 | Loss: 0.00124784
Iteration 17/25 | Loss: 0.00125466
Iteration 18/25 | Loss: 0.00124963
Iteration 19/25 | Loss: 0.00125797
Iteration 20/25 | Loss: 0.00125244
Iteration 21/25 | Loss: 0.00125380
Iteration 22/25 | Loss: 0.00125117
Iteration 23/25 | Loss: 0.00125184
Iteration 24/25 | Loss: 0.00123784
Iteration 25/25 | Loss: 0.00124124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53333497
Iteration 2/25 | Loss: 0.00149504
Iteration 3/25 | Loss: 0.00111459
Iteration 4/25 | Loss: 0.00111458
Iteration 5/25 | Loss: 0.00111458
Iteration 6/25 | Loss: 0.00111458
Iteration 7/25 | Loss: 0.00111458
Iteration 8/25 | Loss: 0.00111458
Iteration 9/25 | Loss: 0.00111458
Iteration 10/25 | Loss: 0.00111458
Iteration 11/25 | Loss: 0.00111458
Iteration 12/25 | Loss: 0.00111458
Iteration 13/25 | Loss: 0.00111458
Iteration 14/25 | Loss: 0.00111458
Iteration 15/25 | Loss: 0.00111458
Iteration 16/25 | Loss: 0.00111458
Iteration 17/25 | Loss: 0.00111458
Iteration 18/25 | Loss: 0.00111458
Iteration 19/25 | Loss: 0.00111458
Iteration 20/25 | Loss: 0.00111458
Iteration 21/25 | Loss: 0.00111458
Iteration 22/25 | Loss: 0.00111458
Iteration 23/25 | Loss: 0.00111458
Iteration 24/25 | Loss: 0.00111458
Iteration 25/25 | Loss: 0.00111458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111458
Iteration 2/1000 | Loss: 0.00031044
Iteration 3/1000 | Loss: 0.00004849
Iteration 4/1000 | Loss: 0.00025313
Iteration 5/1000 | Loss: 0.00015845
Iteration 6/1000 | Loss: 0.00018732
Iteration 7/1000 | Loss: 0.00086813
Iteration 8/1000 | Loss: 0.00027736
Iteration 9/1000 | Loss: 0.00036471
Iteration 10/1000 | Loss: 0.00023489
Iteration 11/1000 | Loss: 0.00004019
Iteration 12/1000 | Loss: 0.00003623
Iteration 13/1000 | Loss: 0.00003392
Iteration 14/1000 | Loss: 0.00003144
Iteration 15/1000 | Loss: 0.00032569
Iteration 16/1000 | Loss: 0.00004648
Iteration 17/1000 | Loss: 0.00003576
Iteration 18/1000 | Loss: 0.00002971
Iteration 19/1000 | Loss: 0.00018032
Iteration 20/1000 | Loss: 0.00031969
Iteration 21/1000 | Loss: 0.00096659
Iteration 22/1000 | Loss: 0.00153753
Iteration 23/1000 | Loss: 0.00033296
Iteration 24/1000 | Loss: 0.00004150
Iteration 25/1000 | Loss: 0.00019743
Iteration 26/1000 | Loss: 0.00002556
Iteration 27/1000 | Loss: 0.00010390
Iteration 28/1000 | Loss: 0.00002083
Iteration 29/1000 | Loss: 0.00005776
Iteration 30/1000 | Loss: 0.00002083
Iteration 31/1000 | Loss: 0.00001871
Iteration 32/1000 | Loss: 0.00011532
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00014599
Iteration 36/1000 | Loss: 0.00024224
Iteration 37/1000 | Loss: 0.00016634
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00004122
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00003703
Iteration 42/1000 | Loss: 0.00002274
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00003056
Iteration 45/1000 | Loss: 0.00001566
Iteration 46/1000 | Loss: 0.00001549
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001503
Iteration 52/1000 | Loss: 0.00001500
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001494
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001493
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001485
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00017914
Iteration 62/1000 | Loss: 0.00015673
Iteration 63/1000 | Loss: 0.00008429
Iteration 64/1000 | Loss: 0.00002029
Iteration 65/1000 | Loss: 0.00004387
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001482
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001441
Iteration 83/1000 | Loss: 0.00001441
Iteration 84/1000 | Loss: 0.00001441
Iteration 85/1000 | Loss: 0.00001440
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001434
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001417
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001415
Iteration 109/1000 | Loss: 0.00001415
Iteration 110/1000 | Loss: 0.00001415
Iteration 111/1000 | Loss: 0.00001414
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001412
Iteration 114/1000 | Loss: 0.00001412
Iteration 115/1000 | Loss: 0.00001412
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001410
Iteration 120/1000 | Loss: 0.00001410
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001408
Iteration 131/1000 | Loss: 0.00001408
Iteration 132/1000 | Loss: 0.00001408
Iteration 133/1000 | Loss: 0.00001408
Iteration 134/1000 | Loss: 0.00001408
Iteration 135/1000 | Loss: 0.00001408
Iteration 136/1000 | Loss: 0.00001408
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001406
Iteration 140/1000 | Loss: 0.00001406
Iteration 141/1000 | Loss: 0.00001406
Iteration 142/1000 | Loss: 0.00001406
Iteration 143/1000 | Loss: 0.00001406
Iteration 144/1000 | Loss: 0.00001406
Iteration 145/1000 | Loss: 0.00001405
Iteration 146/1000 | Loss: 0.00001405
Iteration 147/1000 | Loss: 0.00001405
Iteration 148/1000 | Loss: 0.00001405
Iteration 149/1000 | Loss: 0.00001405
Iteration 150/1000 | Loss: 0.00001405
Iteration 151/1000 | Loss: 0.00001405
Iteration 152/1000 | Loss: 0.00001405
Iteration 153/1000 | Loss: 0.00001405
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001404
Iteration 157/1000 | Loss: 0.00001404
Iteration 158/1000 | Loss: 0.00001404
Iteration 159/1000 | Loss: 0.00001404
Iteration 160/1000 | Loss: 0.00001404
Iteration 161/1000 | Loss: 0.00001404
Iteration 162/1000 | Loss: 0.00001404
Iteration 163/1000 | Loss: 0.00001404
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001403
Iteration 170/1000 | Loss: 0.00001403
Iteration 171/1000 | Loss: 0.00001403
Iteration 172/1000 | Loss: 0.00001403
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001402
Iteration 176/1000 | Loss: 0.00001402
Iteration 177/1000 | Loss: 0.00001402
Iteration 178/1000 | Loss: 0.00001402
Iteration 179/1000 | Loss: 0.00001402
Iteration 180/1000 | Loss: 0.00001402
Iteration 181/1000 | Loss: 0.00001402
Iteration 182/1000 | Loss: 0.00001402
Iteration 183/1000 | Loss: 0.00001402
Iteration 184/1000 | Loss: 0.00001402
Iteration 185/1000 | Loss: 0.00001402
Iteration 186/1000 | Loss: 0.00001402
Iteration 187/1000 | Loss: 0.00001402
Iteration 188/1000 | Loss: 0.00001402
Iteration 189/1000 | Loss: 0.00001402
Iteration 190/1000 | Loss: 0.00001402
Iteration 191/1000 | Loss: 0.00001402
Iteration 192/1000 | Loss: 0.00001401
Iteration 193/1000 | Loss: 0.00001401
Iteration 194/1000 | Loss: 0.00001401
Iteration 195/1000 | Loss: 0.00001401
Iteration 196/1000 | Loss: 0.00001401
Iteration 197/1000 | Loss: 0.00001401
Iteration 198/1000 | Loss: 0.00001401
Iteration 199/1000 | Loss: 0.00001401
Iteration 200/1000 | Loss: 0.00001401
Iteration 201/1000 | Loss: 0.00001401
Iteration 202/1000 | Loss: 0.00001401
Iteration 203/1000 | Loss: 0.00001401
Iteration 204/1000 | Loss: 0.00001401
Iteration 205/1000 | Loss: 0.00001401
Iteration 206/1000 | Loss: 0.00001401
Iteration 207/1000 | Loss: 0.00001401
Iteration 208/1000 | Loss: 0.00001401
Iteration 209/1000 | Loss: 0.00001401
Iteration 210/1000 | Loss: 0.00001400
Iteration 211/1000 | Loss: 0.00001400
Iteration 212/1000 | Loss: 0.00001400
Iteration 213/1000 | Loss: 0.00001400
Iteration 214/1000 | Loss: 0.00001400
Iteration 215/1000 | Loss: 0.00001400
Iteration 216/1000 | Loss: 0.00001400
Iteration 217/1000 | Loss: 0.00001400
Iteration 218/1000 | Loss: 0.00001400
Iteration 219/1000 | Loss: 0.00001400
Iteration 220/1000 | Loss: 0.00001400
Iteration 221/1000 | Loss: 0.00001400
Iteration 222/1000 | Loss: 0.00001400
Iteration 223/1000 | Loss: 0.00001400
Iteration 224/1000 | Loss: 0.00001400
Iteration 225/1000 | Loss: 0.00001400
Iteration 226/1000 | Loss: 0.00001400
Iteration 227/1000 | Loss: 0.00001400
Iteration 228/1000 | Loss: 0.00001400
Iteration 229/1000 | Loss: 0.00001400
Iteration 230/1000 | Loss: 0.00001400
Iteration 231/1000 | Loss: 0.00001400
Iteration 232/1000 | Loss: 0.00001400
Iteration 233/1000 | Loss: 0.00001400
Iteration 234/1000 | Loss: 0.00001400
Iteration 235/1000 | Loss: 0.00001400
Iteration 236/1000 | Loss: 0.00001400
Iteration 237/1000 | Loss: 0.00001400
Iteration 238/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.3998957911098842e-05, 1.3998957911098842e-05, 1.3998957911098842e-05, 1.3998957911098842e-05, 1.3998957911098842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3998957911098842e-05

Optimization complete. Final v2v error: 3.106933355331421 mm

Highest mean error: 5.402326583862305 mm for frame 125

Lowest mean error: 2.714529514312744 mm for frame 95

Saving results

Total time: 146.2696931362152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874799
Iteration 2/25 | Loss: 0.00143896
Iteration 3/25 | Loss: 0.00125970
Iteration 4/25 | Loss: 0.00124459
Iteration 5/25 | Loss: 0.00124029
Iteration 6/25 | Loss: 0.00123947
Iteration 7/25 | Loss: 0.00123947
Iteration 8/25 | Loss: 0.00123947
Iteration 9/25 | Loss: 0.00123947
Iteration 10/25 | Loss: 0.00123947
Iteration 11/25 | Loss: 0.00123947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012394703226163983, 0.0012394703226163983, 0.0012394703226163983, 0.0012394703226163983, 0.0012394703226163983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012394703226163983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52286005
Iteration 2/25 | Loss: 0.00082121
Iteration 3/25 | Loss: 0.00082121
Iteration 4/25 | Loss: 0.00082121
Iteration 5/25 | Loss: 0.00082121
Iteration 6/25 | Loss: 0.00082121
Iteration 7/25 | Loss: 0.00082121
Iteration 8/25 | Loss: 0.00082120
Iteration 9/25 | Loss: 0.00082120
Iteration 10/25 | Loss: 0.00082120
Iteration 11/25 | Loss: 0.00082120
Iteration 12/25 | Loss: 0.00082120
Iteration 13/25 | Loss: 0.00082120
Iteration 14/25 | Loss: 0.00082120
Iteration 15/25 | Loss: 0.00082120
Iteration 16/25 | Loss: 0.00082120
Iteration 17/25 | Loss: 0.00082120
Iteration 18/25 | Loss: 0.00082120
Iteration 19/25 | Loss: 0.00082120
Iteration 20/25 | Loss: 0.00082120
Iteration 21/25 | Loss: 0.00082120
Iteration 22/25 | Loss: 0.00082120
Iteration 23/25 | Loss: 0.00082120
Iteration 24/25 | Loss: 0.00082120
Iteration 25/25 | Loss: 0.00082120

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082120
Iteration 2/1000 | Loss: 0.00003893
Iteration 3/1000 | Loss: 0.00002582
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00002084
Iteration 6/1000 | Loss: 0.00002019
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001877
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001827
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001796
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001769
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001765
Iteration 19/1000 | Loss: 0.00001765
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001764
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001763
Iteration 24/1000 | Loss: 0.00001761
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00001757
Iteration 30/1000 | Loss: 0.00001756
Iteration 31/1000 | Loss: 0.00001753
Iteration 32/1000 | Loss: 0.00001753
Iteration 33/1000 | Loss: 0.00001747
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001747
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001744
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00001743
Iteration 43/1000 | Loss: 0.00001742
Iteration 44/1000 | Loss: 0.00001742
Iteration 45/1000 | Loss: 0.00001742
Iteration 46/1000 | Loss: 0.00001742
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001741
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001740
Iteration 51/1000 | Loss: 0.00001740
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001736
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001735
Iteration 63/1000 | Loss: 0.00001735
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001735
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001728
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001727
Iteration 98/1000 | Loss: 0.00001727
Iteration 99/1000 | Loss: 0.00001727
Iteration 100/1000 | Loss: 0.00001727
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001726
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001726
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Iteration 120/1000 | Loss: 0.00001725
Iteration 121/1000 | Loss: 0.00001725
Iteration 122/1000 | Loss: 0.00001725
Iteration 123/1000 | Loss: 0.00001725
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001724
Iteration 126/1000 | Loss: 0.00001724
Iteration 127/1000 | Loss: 0.00001724
Iteration 128/1000 | Loss: 0.00001724
Iteration 129/1000 | Loss: 0.00001724
Iteration 130/1000 | Loss: 0.00001724
Iteration 131/1000 | Loss: 0.00001724
Iteration 132/1000 | Loss: 0.00001724
Iteration 133/1000 | Loss: 0.00001724
Iteration 134/1000 | Loss: 0.00001724
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001723
Iteration 140/1000 | Loss: 0.00001723
Iteration 141/1000 | Loss: 0.00001723
Iteration 142/1000 | Loss: 0.00001723
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001723
Iteration 146/1000 | Loss: 0.00001723
Iteration 147/1000 | Loss: 0.00001723
Iteration 148/1000 | Loss: 0.00001723
Iteration 149/1000 | Loss: 0.00001723
Iteration 150/1000 | Loss: 0.00001723
Iteration 151/1000 | Loss: 0.00001723
Iteration 152/1000 | Loss: 0.00001723
Iteration 153/1000 | Loss: 0.00001723
Iteration 154/1000 | Loss: 0.00001723
Iteration 155/1000 | Loss: 0.00001723
Iteration 156/1000 | Loss: 0.00001723
Iteration 157/1000 | Loss: 0.00001722
Iteration 158/1000 | Loss: 0.00001722
Iteration 159/1000 | Loss: 0.00001722
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001722
Iteration 164/1000 | Loss: 0.00001722
Iteration 165/1000 | Loss: 0.00001722
Iteration 166/1000 | Loss: 0.00001722
Iteration 167/1000 | Loss: 0.00001722
Iteration 168/1000 | Loss: 0.00001722
Iteration 169/1000 | Loss: 0.00001722
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001721
Iteration 173/1000 | Loss: 0.00001721
Iteration 174/1000 | Loss: 0.00001721
Iteration 175/1000 | Loss: 0.00001721
Iteration 176/1000 | Loss: 0.00001721
Iteration 177/1000 | Loss: 0.00001721
Iteration 178/1000 | Loss: 0.00001721
Iteration 179/1000 | Loss: 0.00001721
Iteration 180/1000 | Loss: 0.00001721
Iteration 181/1000 | Loss: 0.00001721
Iteration 182/1000 | Loss: 0.00001720
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001720
Iteration 186/1000 | Loss: 0.00001720
Iteration 187/1000 | Loss: 0.00001720
Iteration 188/1000 | Loss: 0.00001720
Iteration 189/1000 | Loss: 0.00001720
Iteration 190/1000 | Loss: 0.00001720
Iteration 191/1000 | Loss: 0.00001720
Iteration 192/1000 | Loss: 0.00001720
Iteration 193/1000 | Loss: 0.00001720
Iteration 194/1000 | Loss: 0.00001720
Iteration 195/1000 | Loss: 0.00001720
Iteration 196/1000 | Loss: 0.00001720
Iteration 197/1000 | Loss: 0.00001720
Iteration 198/1000 | Loss: 0.00001720
Iteration 199/1000 | Loss: 0.00001720
Iteration 200/1000 | Loss: 0.00001720
Iteration 201/1000 | Loss: 0.00001720
Iteration 202/1000 | Loss: 0.00001720
Iteration 203/1000 | Loss: 0.00001720
Iteration 204/1000 | Loss: 0.00001720
Iteration 205/1000 | Loss: 0.00001720
Iteration 206/1000 | Loss: 0.00001720
Iteration 207/1000 | Loss: 0.00001720
Iteration 208/1000 | Loss: 0.00001720
Iteration 209/1000 | Loss: 0.00001720
Iteration 210/1000 | Loss: 0.00001720
Iteration 211/1000 | Loss: 0.00001720
Iteration 212/1000 | Loss: 0.00001720
Iteration 213/1000 | Loss: 0.00001720
Iteration 214/1000 | Loss: 0.00001720
Iteration 215/1000 | Loss: 0.00001720
Iteration 216/1000 | Loss: 0.00001720
Iteration 217/1000 | Loss: 0.00001720
Iteration 218/1000 | Loss: 0.00001720
Iteration 219/1000 | Loss: 0.00001720
Iteration 220/1000 | Loss: 0.00001720
Iteration 221/1000 | Loss: 0.00001720
Iteration 222/1000 | Loss: 0.00001720
Iteration 223/1000 | Loss: 0.00001720
Iteration 224/1000 | Loss: 0.00001720
Iteration 225/1000 | Loss: 0.00001720
Iteration 226/1000 | Loss: 0.00001720
Iteration 227/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.7202972230734304e-05, 1.7202972230734304e-05, 1.7202972230734304e-05, 1.7202972230734304e-05, 1.7202972230734304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7202972230734304e-05

Optimization complete. Final v2v error: 3.406148672103882 mm

Highest mean error: 4.978936672210693 mm for frame 54

Lowest mean error: 2.9207074642181396 mm for frame 17

Saving results

Total time: 44.21846008300781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917943
Iteration 2/25 | Loss: 0.00151327
Iteration 3/25 | Loss: 0.00138794
Iteration 4/25 | Loss: 0.00137462
Iteration 5/25 | Loss: 0.00137049
Iteration 6/25 | Loss: 0.00137012
Iteration 7/25 | Loss: 0.00137012
Iteration 8/25 | Loss: 0.00137012
Iteration 9/25 | Loss: 0.00137012
Iteration 10/25 | Loss: 0.00137012
Iteration 11/25 | Loss: 0.00137012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013701235875487328, 0.0013701235875487328, 0.0013701235875487328, 0.0013701235875487328, 0.0013701235875487328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013701235875487328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04903150
Iteration 2/25 | Loss: 0.00099122
Iteration 3/25 | Loss: 0.00099118
Iteration 4/25 | Loss: 0.00099118
Iteration 5/25 | Loss: 0.00099118
Iteration 6/25 | Loss: 0.00099118
Iteration 7/25 | Loss: 0.00099118
Iteration 8/25 | Loss: 0.00099118
Iteration 9/25 | Loss: 0.00099118
Iteration 10/25 | Loss: 0.00099118
Iteration 11/25 | Loss: 0.00099118
Iteration 12/25 | Loss: 0.00099118
Iteration 13/25 | Loss: 0.00099118
Iteration 14/25 | Loss: 0.00099118
Iteration 15/25 | Loss: 0.00099118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000991182285360992, 0.000991182285360992, 0.000991182285360992, 0.000991182285360992, 0.000991182285360992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991182285360992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099118
Iteration 2/1000 | Loss: 0.00005470
Iteration 3/1000 | Loss: 0.00003315
Iteration 4/1000 | Loss: 0.00002891
Iteration 5/1000 | Loss: 0.00002728
Iteration 6/1000 | Loss: 0.00002629
Iteration 7/1000 | Loss: 0.00002576
Iteration 8/1000 | Loss: 0.00002533
Iteration 9/1000 | Loss: 0.00002502
Iteration 10/1000 | Loss: 0.00002478
Iteration 11/1000 | Loss: 0.00002453
Iteration 12/1000 | Loss: 0.00002434
Iteration 13/1000 | Loss: 0.00002421
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002396
Iteration 16/1000 | Loss: 0.00002393
Iteration 17/1000 | Loss: 0.00002392
Iteration 18/1000 | Loss: 0.00002391
Iteration 19/1000 | Loss: 0.00002389
Iteration 20/1000 | Loss: 0.00002389
Iteration 21/1000 | Loss: 0.00002386
Iteration 22/1000 | Loss: 0.00002386
Iteration 23/1000 | Loss: 0.00002386
Iteration 24/1000 | Loss: 0.00002386
Iteration 25/1000 | Loss: 0.00002385
Iteration 26/1000 | Loss: 0.00002384
Iteration 27/1000 | Loss: 0.00002383
Iteration 28/1000 | Loss: 0.00002383
Iteration 29/1000 | Loss: 0.00002382
Iteration 30/1000 | Loss: 0.00002382
Iteration 31/1000 | Loss: 0.00002381
Iteration 32/1000 | Loss: 0.00002379
Iteration 33/1000 | Loss: 0.00002379
Iteration 34/1000 | Loss: 0.00002379
Iteration 35/1000 | Loss: 0.00002379
Iteration 36/1000 | Loss: 0.00002378
Iteration 37/1000 | Loss: 0.00002377
Iteration 38/1000 | Loss: 0.00002377
Iteration 39/1000 | Loss: 0.00002377
Iteration 40/1000 | Loss: 0.00002377
Iteration 41/1000 | Loss: 0.00002374
Iteration 42/1000 | Loss: 0.00002374
Iteration 43/1000 | Loss: 0.00002373
Iteration 44/1000 | Loss: 0.00002372
Iteration 45/1000 | Loss: 0.00002371
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002370
Iteration 48/1000 | Loss: 0.00002370
Iteration 49/1000 | Loss: 0.00002370
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00002369
Iteration 52/1000 | Loss: 0.00002369
Iteration 53/1000 | Loss: 0.00002369
Iteration 54/1000 | Loss: 0.00002369
Iteration 55/1000 | Loss: 0.00002369
Iteration 56/1000 | Loss: 0.00002368
Iteration 57/1000 | Loss: 0.00002368
Iteration 58/1000 | Loss: 0.00002368
Iteration 59/1000 | Loss: 0.00002368
Iteration 60/1000 | Loss: 0.00002368
Iteration 61/1000 | Loss: 0.00002368
Iteration 62/1000 | Loss: 0.00002368
Iteration 63/1000 | Loss: 0.00002368
Iteration 64/1000 | Loss: 0.00002368
Iteration 65/1000 | Loss: 0.00002368
Iteration 66/1000 | Loss: 0.00002367
Iteration 67/1000 | Loss: 0.00002367
Iteration 68/1000 | Loss: 0.00002367
Iteration 69/1000 | Loss: 0.00002366
Iteration 70/1000 | Loss: 0.00002366
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00002366
Iteration 73/1000 | Loss: 0.00002366
Iteration 74/1000 | Loss: 0.00002366
Iteration 75/1000 | Loss: 0.00002366
Iteration 76/1000 | Loss: 0.00002366
Iteration 77/1000 | Loss: 0.00002366
Iteration 78/1000 | Loss: 0.00002366
Iteration 79/1000 | Loss: 0.00002365
Iteration 80/1000 | Loss: 0.00002365
Iteration 81/1000 | Loss: 0.00002365
Iteration 82/1000 | Loss: 0.00002365
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002364
Iteration 85/1000 | Loss: 0.00002364
Iteration 86/1000 | Loss: 0.00002362
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002360
Iteration 90/1000 | Loss: 0.00002360
Iteration 91/1000 | Loss: 0.00002360
Iteration 92/1000 | Loss: 0.00002360
Iteration 93/1000 | Loss: 0.00002360
Iteration 94/1000 | Loss: 0.00002359
Iteration 95/1000 | Loss: 0.00002359
Iteration 96/1000 | Loss: 0.00002359
Iteration 97/1000 | Loss: 0.00002359
Iteration 98/1000 | Loss: 0.00002359
Iteration 99/1000 | Loss: 0.00002359
Iteration 100/1000 | Loss: 0.00002359
Iteration 101/1000 | Loss: 0.00002357
Iteration 102/1000 | Loss: 0.00002357
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002355
Iteration 107/1000 | Loss: 0.00002355
Iteration 108/1000 | Loss: 0.00002355
Iteration 109/1000 | Loss: 0.00002355
Iteration 110/1000 | Loss: 0.00002355
Iteration 111/1000 | Loss: 0.00002354
Iteration 112/1000 | Loss: 0.00002354
Iteration 113/1000 | Loss: 0.00002354
Iteration 114/1000 | Loss: 0.00002354
Iteration 115/1000 | Loss: 0.00002354
Iteration 116/1000 | Loss: 0.00002353
Iteration 117/1000 | Loss: 0.00002353
Iteration 118/1000 | Loss: 0.00002353
Iteration 119/1000 | Loss: 0.00002353
Iteration 120/1000 | Loss: 0.00002353
Iteration 121/1000 | Loss: 0.00002353
Iteration 122/1000 | Loss: 0.00002352
Iteration 123/1000 | Loss: 0.00002352
Iteration 124/1000 | Loss: 0.00002352
Iteration 125/1000 | Loss: 0.00002352
Iteration 126/1000 | Loss: 0.00002352
Iteration 127/1000 | Loss: 0.00002352
Iteration 128/1000 | Loss: 0.00002352
Iteration 129/1000 | Loss: 0.00002352
Iteration 130/1000 | Loss: 0.00002352
Iteration 131/1000 | Loss: 0.00002352
Iteration 132/1000 | Loss: 0.00002351
Iteration 133/1000 | Loss: 0.00002351
Iteration 134/1000 | Loss: 0.00002351
Iteration 135/1000 | Loss: 0.00002351
Iteration 136/1000 | Loss: 0.00002351
Iteration 137/1000 | Loss: 0.00002351
Iteration 138/1000 | Loss: 0.00002350
Iteration 139/1000 | Loss: 0.00002350
Iteration 140/1000 | Loss: 0.00002350
Iteration 141/1000 | Loss: 0.00002350
Iteration 142/1000 | Loss: 0.00002350
Iteration 143/1000 | Loss: 0.00002350
Iteration 144/1000 | Loss: 0.00002350
Iteration 145/1000 | Loss: 0.00002350
Iteration 146/1000 | Loss: 0.00002350
Iteration 147/1000 | Loss: 0.00002350
Iteration 148/1000 | Loss: 0.00002349
Iteration 149/1000 | Loss: 0.00002349
Iteration 150/1000 | Loss: 0.00002349
Iteration 151/1000 | Loss: 0.00002349
Iteration 152/1000 | Loss: 0.00002349
Iteration 153/1000 | Loss: 0.00002348
Iteration 154/1000 | Loss: 0.00002348
Iteration 155/1000 | Loss: 0.00002348
Iteration 156/1000 | Loss: 0.00002348
Iteration 157/1000 | Loss: 0.00002348
Iteration 158/1000 | Loss: 0.00002348
Iteration 159/1000 | Loss: 0.00002347
Iteration 160/1000 | Loss: 0.00002347
Iteration 161/1000 | Loss: 0.00002347
Iteration 162/1000 | Loss: 0.00002347
Iteration 163/1000 | Loss: 0.00002346
Iteration 164/1000 | Loss: 0.00002346
Iteration 165/1000 | Loss: 0.00002346
Iteration 166/1000 | Loss: 0.00002346
Iteration 167/1000 | Loss: 0.00002346
Iteration 168/1000 | Loss: 0.00002346
Iteration 169/1000 | Loss: 0.00002346
Iteration 170/1000 | Loss: 0.00002346
Iteration 171/1000 | Loss: 0.00002346
Iteration 172/1000 | Loss: 0.00002346
Iteration 173/1000 | Loss: 0.00002346
Iteration 174/1000 | Loss: 0.00002346
Iteration 175/1000 | Loss: 0.00002346
Iteration 176/1000 | Loss: 0.00002346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.345855136809405e-05, 2.345855136809405e-05, 2.345855136809405e-05, 2.345855136809405e-05, 2.345855136809405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.345855136809405e-05

Optimization complete. Final v2v error: 3.9783427715301514 mm

Highest mean error: 4.628819465637207 mm for frame 81

Lowest mean error: 3.531014919281006 mm for frame 0

Saving results

Total time: 43.3339581489563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948035
Iteration 2/25 | Loss: 0.00181992
Iteration 3/25 | Loss: 0.00144893
Iteration 4/25 | Loss: 0.00142557
Iteration 5/25 | Loss: 0.00141815
Iteration 6/25 | Loss: 0.00141734
Iteration 7/25 | Loss: 0.00141734
Iteration 8/25 | Loss: 0.00141734
Iteration 9/25 | Loss: 0.00141734
Iteration 10/25 | Loss: 0.00141734
Iteration 11/25 | Loss: 0.00141734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001417336636222899, 0.001417336636222899, 0.001417336636222899, 0.001417336636222899, 0.001417336636222899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001417336636222899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84816688
Iteration 2/25 | Loss: 0.00087626
Iteration 3/25 | Loss: 0.00087626
Iteration 4/25 | Loss: 0.00087626
Iteration 5/25 | Loss: 0.00087626
Iteration 6/25 | Loss: 0.00087626
Iteration 7/25 | Loss: 0.00087626
Iteration 8/25 | Loss: 0.00087626
Iteration 9/25 | Loss: 0.00087626
Iteration 10/25 | Loss: 0.00087626
Iteration 11/25 | Loss: 0.00087626
Iteration 12/25 | Loss: 0.00087626
Iteration 13/25 | Loss: 0.00087626
Iteration 14/25 | Loss: 0.00087626
Iteration 15/25 | Loss: 0.00087626
Iteration 16/25 | Loss: 0.00087626
Iteration 17/25 | Loss: 0.00087626
Iteration 18/25 | Loss: 0.00087626
Iteration 19/25 | Loss: 0.00087626
Iteration 20/25 | Loss: 0.00087626
Iteration 21/25 | Loss: 0.00087626
Iteration 22/25 | Loss: 0.00087626
Iteration 23/25 | Loss: 0.00087626
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008762566139921546, 0.0008762566139921546, 0.0008762566139921546, 0.0008762566139921546, 0.0008762566139921546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008762566139921546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087626
Iteration 2/1000 | Loss: 0.00006392
Iteration 3/1000 | Loss: 0.00004421
Iteration 4/1000 | Loss: 0.00004057
Iteration 5/1000 | Loss: 0.00003870
Iteration 6/1000 | Loss: 0.00003765
Iteration 7/1000 | Loss: 0.00003701
Iteration 8/1000 | Loss: 0.00003639
Iteration 9/1000 | Loss: 0.00003584
Iteration 10/1000 | Loss: 0.00003542
Iteration 11/1000 | Loss: 0.00003503
Iteration 12/1000 | Loss: 0.00003465
Iteration 13/1000 | Loss: 0.00003437
Iteration 14/1000 | Loss: 0.00003406
Iteration 15/1000 | Loss: 0.00003379
Iteration 16/1000 | Loss: 0.00003356
Iteration 17/1000 | Loss: 0.00003337
Iteration 18/1000 | Loss: 0.00003323
Iteration 19/1000 | Loss: 0.00003318
Iteration 20/1000 | Loss: 0.00003305
Iteration 21/1000 | Loss: 0.00003295
Iteration 22/1000 | Loss: 0.00003292
Iteration 23/1000 | Loss: 0.00003290
Iteration 24/1000 | Loss: 0.00003289
Iteration 25/1000 | Loss: 0.00003288
Iteration 26/1000 | Loss: 0.00003288
Iteration 27/1000 | Loss: 0.00003287
Iteration 28/1000 | Loss: 0.00003287
Iteration 29/1000 | Loss: 0.00003287
Iteration 30/1000 | Loss: 0.00003287
Iteration 31/1000 | Loss: 0.00003287
Iteration 32/1000 | Loss: 0.00003287
Iteration 33/1000 | Loss: 0.00003286
Iteration 34/1000 | Loss: 0.00003286
Iteration 35/1000 | Loss: 0.00003286
Iteration 36/1000 | Loss: 0.00003286
Iteration 37/1000 | Loss: 0.00003286
Iteration 38/1000 | Loss: 0.00003286
Iteration 39/1000 | Loss: 0.00003285
Iteration 40/1000 | Loss: 0.00003284
Iteration 41/1000 | Loss: 0.00003284
Iteration 42/1000 | Loss: 0.00003284
Iteration 43/1000 | Loss: 0.00003284
Iteration 44/1000 | Loss: 0.00003283
Iteration 45/1000 | Loss: 0.00003283
Iteration 46/1000 | Loss: 0.00003282
Iteration 47/1000 | Loss: 0.00003282
Iteration 48/1000 | Loss: 0.00003282
Iteration 49/1000 | Loss: 0.00003282
Iteration 50/1000 | Loss: 0.00003282
Iteration 51/1000 | Loss: 0.00003281
Iteration 52/1000 | Loss: 0.00003281
Iteration 53/1000 | Loss: 0.00003280
Iteration 54/1000 | Loss: 0.00003280
Iteration 55/1000 | Loss: 0.00003279
Iteration 56/1000 | Loss: 0.00003279
Iteration 57/1000 | Loss: 0.00003279
Iteration 58/1000 | Loss: 0.00003279
Iteration 59/1000 | Loss: 0.00003279
Iteration 60/1000 | Loss: 0.00003278
Iteration 61/1000 | Loss: 0.00003278
Iteration 62/1000 | Loss: 0.00003278
Iteration 63/1000 | Loss: 0.00003278
Iteration 64/1000 | Loss: 0.00003278
Iteration 65/1000 | Loss: 0.00003277
Iteration 66/1000 | Loss: 0.00003277
Iteration 67/1000 | Loss: 0.00003277
Iteration 68/1000 | Loss: 0.00003276
Iteration 69/1000 | Loss: 0.00003276
Iteration 70/1000 | Loss: 0.00003276
Iteration 71/1000 | Loss: 0.00003276
Iteration 72/1000 | Loss: 0.00003276
Iteration 73/1000 | Loss: 0.00003276
Iteration 74/1000 | Loss: 0.00003275
Iteration 75/1000 | Loss: 0.00003275
Iteration 76/1000 | Loss: 0.00003275
Iteration 77/1000 | Loss: 0.00003275
Iteration 78/1000 | Loss: 0.00003275
Iteration 79/1000 | Loss: 0.00003275
Iteration 80/1000 | Loss: 0.00003275
Iteration 81/1000 | Loss: 0.00003275
Iteration 82/1000 | Loss: 0.00003275
Iteration 83/1000 | Loss: 0.00003275
Iteration 84/1000 | Loss: 0.00003275
Iteration 85/1000 | Loss: 0.00003275
Iteration 86/1000 | Loss: 0.00003275
Iteration 87/1000 | Loss: 0.00003275
Iteration 88/1000 | Loss: 0.00003274
Iteration 89/1000 | Loss: 0.00003274
Iteration 90/1000 | Loss: 0.00003274
Iteration 91/1000 | Loss: 0.00003274
Iteration 92/1000 | Loss: 0.00003274
Iteration 93/1000 | Loss: 0.00003274
Iteration 94/1000 | Loss: 0.00003273
Iteration 95/1000 | Loss: 0.00003273
Iteration 96/1000 | Loss: 0.00003273
Iteration 97/1000 | Loss: 0.00003273
Iteration 98/1000 | Loss: 0.00003273
Iteration 99/1000 | Loss: 0.00003273
Iteration 100/1000 | Loss: 0.00003273
Iteration 101/1000 | Loss: 0.00003273
Iteration 102/1000 | Loss: 0.00003273
Iteration 103/1000 | Loss: 0.00003273
Iteration 104/1000 | Loss: 0.00003273
Iteration 105/1000 | Loss: 0.00003273
Iteration 106/1000 | Loss: 0.00003272
Iteration 107/1000 | Loss: 0.00003272
Iteration 108/1000 | Loss: 0.00003272
Iteration 109/1000 | Loss: 0.00003272
Iteration 110/1000 | Loss: 0.00003271
Iteration 111/1000 | Loss: 0.00003271
Iteration 112/1000 | Loss: 0.00003271
Iteration 113/1000 | Loss: 0.00003271
Iteration 114/1000 | Loss: 0.00003271
Iteration 115/1000 | Loss: 0.00003271
Iteration 116/1000 | Loss: 0.00003271
Iteration 117/1000 | Loss: 0.00003271
Iteration 118/1000 | Loss: 0.00003271
Iteration 119/1000 | Loss: 0.00003270
Iteration 120/1000 | Loss: 0.00003270
Iteration 121/1000 | Loss: 0.00003270
Iteration 122/1000 | Loss: 0.00003270
Iteration 123/1000 | Loss: 0.00003270
Iteration 124/1000 | Loss: 0.00003270
Iteration 125/1000 | Loss: 0.00003270
Iteration 126/1000 | Loss: 0.00003270
Iteration 127/1000 | Loss: 0.00003270
Iteration 128/1000 | Loss: 0.00003269
Iteration 129/1000 | Loss: 0.00003269
Iteration 130/1000 | Loss: 0.00003269
Iteration 131/1000 | Loss: 0.00003269
Iteration 132/1000 | Loss: 0.00003269
Iteration 133/1000 | Loss: 0.00003269
Iteration 134/1000 | Loss: 0.00003269
Iteration 135/1000 | Loss: 0.00003269
Iteration 136/1000 | Loss: 0.00003269
Iteration 137/1000 | Loss: 0.00003269
Iteration 138/1000 | Loss: 0.00003269
Iteration 139/1000 | Loss: 0.00003269
Iteration 140/1000 | Loss: 0.00003269
Iteration 141/1000 | Loss: 0.00003269
Iteration 142/1000 | Loss: 0.00003269
Iteration 143/1000 | Loss: 0.00003269
Iteration 144/1000 | Loss: 0.00003269
Iteration 145/1000 | Loss: 0.00003269
Iteration 146/1000 | Loss: 0.00003269
Iteration 147/1000 | Loss: 0.00003269
Iteration 148/1000 | Loss: 0.00003269
Iteration 149/1000 | Loss: 0.00003269
Iteration 150/1000 | Loss: 0.00003269
Iteration 151/1000 | Loss: 0.00003269
Iteration 152/1000 | Loss: 0.00003269
Iteration 153/1000 | Loss: 0.00003269
Iteration 154/1000 | Loss: 0.00003269
Iteration 155/1000 | Loss: 0.00003269
Iteration 156/1000 | Loss: 0.00003269
Iteration 157/1000 | Loss: 0.00003269
Iteration 158/1000 | Loss: 0.00003269
Iteration 159/1000 | Loss: 0.00003269
Iteration 160/1000 | Loss: 0.00003269
Iteration 161/1000 | Loss: 0.00003269
Iteration 162/1000 | Loss: 0.00003269
Iteration 163/1000 | Loss: 0.00003269
Iteration 164/1000 | Loss: 0.00003269
Iteration 165/1000 | Loss: 0.00003269
Iteration 166/1000 | Loss: 0.00003269
Iteration 167/1000 | Loss: 0.00003269
Iteration 168/1000 | Loss: 0.00003269
Iteration 169/1000 | Loss: 0.00003269
Iteration 170/1000 | Loss: 0.00003269
Iteration 171/1000 | Loss: 0.00003269
Iteration 172/1000 | Loss: 0.00003269
Iteration 173/1000 | Loss: 0.00003269
Iteration 174/1000 | Loss: 0.00003269
Iteration 175/1000 | Loss: 0.00003269
Iteration 176/1000 | Loss: 0.00003269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.268568616476841e-05, 3.268568616476841e-05, 3.268568616476841e-05, 3.268568616476841e-05, 3.268568616476841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.268568616476841e-05

Optimization complete. Final v2v error: 4.767248630523682 mm

Highest mean error: 5.735864639282227 mm for frame 90

Lowest mean error: 3.850996494293213 mm for frame 0

Saving results

Total time: 55.89126443862915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827145
Iteration 2/25 | Loss: 0.00134084
Iteration 3/25 | Loss: 0.00121650
Iteration 4/25 | Loss: 0.00120480
Iteration 5/25 | Loss: 0.00120282
Iteration 6/25 | Loss: 0.00120240
Iteration 7/25 | Loss: 0.00120240
Iteration 8/25 | Loss: 0.00120240
Iteration 9/25 | Loss: 0.00120240
Iteration 10/25 | Loss: 0.00120240
Iteration 11/25 | Loss: 0.00120240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012024010065943003, 0.0012024010065943003, 0.0012024010065943003, 0.0012024010065943003, 0.0012024010065943003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012024010065943003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42868018
Iteration 2/25 | Loss: 0.00069188
Iteration 3/25 | Loss: 0.00069185
Iteration 4/25 | Loss: 0.00069185
Iteration 5/25 | Loss: 0.00069185
Iteration 6/25 | Loss: 0.00069185
Iteration 7/25 | Loss: 0.00069185
Iteration 8/25 | Loss: 0.00069185
Iteration 9/25 | Loss: 0.00069185
Iteration 10/25 | Loss: 0.00069185
Iteration 11/25 | Loss: 0.00069185
Iteration 12/25 | Loss: 0.00069185
Iteration 13/25 | Loss: 0.00069185
Iteration 14/25 | Loss: 0.00069185
Iteration 15/25 | Loss: 0.00069185
Iteration 16/25 | Loss: 0.00069185
Iteration 17/25 | Loss: 0.00069185
Iteration 18/25 | Loss: 0.00069185
Iteration 19/25 | Loss: 0.00069185
Iteration 20/25 | Loss: 0.00069185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006918451981619, 0.0006918451981619, 0.0006918451981619, 0.0006918451981619, 0.0006918451981619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006918451981619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069185
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001547
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001197
Iteration 7/1000 | Loss: 0.00001151
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001127
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001125
Iteration 12/1000 | Loss: 0.00001124
Iteration 13/1000 | Loss: 0.00001123
Iteration 14/1000 | Loss: 0.00001122
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001098
Iteration 19/1000 | Loss: 0.00001093
Iteration 20/1000 | Loss: 0.00001091
Iteration 21/1000 | Loss: 0.00001090
Iteration 22/1000 | Loss: 0.00001090
Iteration 23/1000 | Loss: 0.00001089
Iteration 24/1000 | Loss: 0.00001089
Iteration 25/1000 | Loss: 0.00001088
Iteration 26/1000 | Loss: 0.00001087
Iteration 27/1000 | Loss: 0.00001087
Iteration 28/1000 | Loss: 0.00001086
Iteration 29/1000 | Loss: 0.00001085
Iteration 30/1000 | Loss: 0.00001085
Iteration 31/1000 | Loss: 0.00001084
Iteration 32/1000 | Loss: 0.00001084
Iteration 33/1000 | Loss: 0.00001083
Iteration 34/1000 | Loss: 0.00001082
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001072
Iteration 42/1000 | Loss: 0.00001072
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001072
Iteration 45/1000 | Loss: 0.00001072
Iteration 46/1000 | Loss: 0.00001072
Iteration 47/1000 | Loss: 0.00001072
Iteration 48/1000 | Loss: 0.00001071
Iteration 49/1000 | Loss: 0.00001071
Iteration 50/1000 | Loss: 0.00001069
Iteration 51/1000 | Loss: 0.00001067
Iteration 52/1000 | Loss: 0.00001067
Iteration 53/1000 | Loss: 0.00001067
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001066
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001064
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001063
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001063
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001062
Iteration 83/1000 | Loss: 0.00001062
Iteration 84/1000 | Loss: 0.00001062
Iteration 85/1000 | Loss: 0.00001062
Iteration 86/1000 | Loss: 0.00001062
Iteration 87/1000 | Loss: 0.00001062
Iteration 88/1000 | Loss: 0.00001061
Iteration 89/1000 | Loss: 0.00001061
Iteration 90/1000 | Loss: 0.00001061
Iteration 91/1000 | Loss: 0.00001061
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001061
Iteration 96/1000 | Loss: 0.00001060
Iteration 97/1000 | Loss: 0.00001060
Iteration 98/1000 | Loss: 0.00001060
Iteration 99/1000 | Loss: 0.00001060
Iteration 100/1000 | Loss: 0.00001060
Iteration 101/1000 | Loss: 0.00001060
Iteration 102/1000 | Loss: 0.00001060
Iteration 103/1000 | Loss: 0.00001059
Iteration 104/1000 | Loss: 0.00001059
Iteration 105/1000 | Loss: 0.00001059
Iteration 106/1000 | Loss: 0.00001058
Iteration 107/1000 | Loss: 0.00001058
Iteration 108/1000 | Loss: 0.00001058
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001056
Iteration 115/1000 | Loss: 0.00001056
Iteration 116/1000 | Loss: 0.00001056
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001056
Iteration 119/1000 | Loss: 0.00001056
Iteration 120/1000 | Loss: 0.00001055
Iteration 121/1000 | Loss: 0.00001055
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001053
Iteration 131/1000 | Loss: 0.00001053
Iteration 132/1000 | Loss: 0.00001053
Iteration 133/1000 | Loss: 0.00001053
Iteration 134/1000 | Loss: 0.00001053
Iteration 135/1000 | Loss: 0.00001053
Iteration 136/1000 | Loss: 0.00001053
Iteration 137/1000 | Loss: 0.00001053
Iteration 138/1000 | Loss: 0.00001052
Iteration 139/1000 | Loss: 0.00001052
Iteration 140/1000 | Loss: 0.00001052
Iteration 141/1000 | Loss: 0.00001052
Iteration 142/1000 | Loss: 0.00001052
Iteration 143/1000 | Loss: 0.00001052
Iteration 144/1000 | Loss: 0.00001052
Iteration 145/1000 | Loss: 0.00001051
Iteration 146/1000 | Loss: 0.00001051
Iteration 147/1000 | Loss: 0.00001051
Iteration 148/1000 | Loss: 0.00001051
Iteration 149/1000 | Loss: 0.00001050
Iteration 150/1000 | Loss: 0.00001050
Iteration 151/1000 | Loss: 0.00001050
Iteration 152/1000 | Loss: 0.00001050
Iteration 153/1000 | Loss: 0.00001050
Iteration 154/1000 | Loss: 0.00001050
Iteration 155/1000 | Loss: 0.00001050
Iteration 156/1000 | Loss: 0.00001050
Iteration 157/1000 | Loss: 0.00001049
Iteration 158/1000 | Loss: 0.00001049
Iteration 159/1000 | Loss: 0.00001049
Iteration 160/1000 | Loss: 0.00001049
Iteration 161/1000 | Loss: 0.00001049
Iteration 162/1000 | Loss: 0.00001049
Iteration 163/1000 | Loss: 0.00001048
Iteration 164/1000 | Loss: 0.00001048
Iteration 165/1000 | Loss: 0.00001048
Iteration 166/1000 | Loss: 0.00001048
Iteration 167/1000 | Loss: 0.00001048
Iteration 168/1000 | Loss: 0.00001048
Iteration 169/1000 | Loss: 0.00001047
Iteration 170/1000 | Loss: 0.00001047
Iteration 171/1000 | Loss: 0.00001047
Iteration 172/1000 | Loss: 0.00001047
Iteration 173/1000 | Loss: 0.00001047
Iteration 174/1000 | Loss: 0.00001047
Iteration 175/1000 | Loss: 0.00001047
Iteration 176/1000 | Loss: 0.00001047
Iteration 177/1000 | Loss: 0.00001047
Iteration 178/1000 | Loss: 0.00001047
Iteration 179/1000 | Loss: 0.00001047
Iteration 180/1000 | Loss: 0.00001046
Iteration 181/1000 | Loss: 0.00001046
Iteration 182/1000 | Loss: 0.00001046
Iteration 183/1000 | Loss: 0.00001046
Iteration 184/1000 | Loss: 0.00001046
Iteration 185/1000 | Loss: 0.00001046
Iteration 186/1000 | Loss: 0.00001046
Iteration 187/1000 | Loss: 0.00001046
Iteration 188/1000 | Loss: 0.00001046
Iteration 189/1000 | Loss: 0.00001046
Iteration 190/1000 | Loss: 0.00001046
Iteration 191/1000 | Loss: 0.00001046
Iteration 192/1000 | Loss: 0.00001045
Iteration 193/1000 | Loss: 0.00001045
Iteration 194/1000 | Loss: 0.00001045
Iteration 195/1000 | Loss: 0.00001045
Iteration 196/1000 | Loss: 0.00001045
Iteration 197/1000 | Loss: 0.00001045
Iteration 198/1000 | Loss: 0.00001045
Iteration 199/1000 | Loss: 0.00001044
Iteration 200/1000 | Loss: 0.00001044
Iteration 201/1000 | Loss: 0.00001044
Iteration 202/1000 | Loss: 0.00001044
Iteration 203/1000 | Loss: 0.00001044
Iteration 204/1000 | Loss: 0.00001044
Iteration 205/1000 | Loss: 0.00001044
Iteration 206/1000 | Loss: 0.00001044
Iteration 207/1000 | Loss: 0.00001044
Iteration 208/1000 | Loss: 0.00001044
Iteration 209/1000 | Loss: 0.00001044
Iteration 210/1000 | Loss: 0.00001044
Iteration 211/1000 | Loss: 0.00001043
Iteration 212/1000 | Loss: 0.00001043
Iteration 213/1000 | Loss: 0.00001043
Iteration 214/1000 | Loss: 0.00001043
Iteration 215/1000 | Loss: 0.00001043
Iteration 216/1000 | Loss: 0.00001043
Iteration 217/1000 | Loss: 0.00001043
Iteration 218/1000 | Loss: 0.00001043
Iteration 219/1000 | Loss: 0.00001043
Iteration 220/1000 | Loss: 0.00001043
Iteration 221/1000 | Loss: 0.00001043
Iteration 222/1000 | Loss: 0.00001043
Iteration 223/1000 | Loss: 0.00001043
Iteration 224/1000 | Loss: 0.00001043
Iteration 225/1000 | Loss: 0.00001043
Iteration 226/1000 | Loss: 0.00001043
Iteration 227/1000 | Loss: 0.00001043
Iteration 228/1000 | Loss: 0.00001043
Iteration 229/1000 | Loss: 0.00001043
Iteration 230/1000 | Loss: 0.00001043
Iteration 231/1000 | Loss: 0.00001043
Iteration 232/1000 | Loss: 0.00001042
Iteration 233/1000 | Loss: 0.00001042
Iteration 234/1000 | Loss: 0.00001042
Iteration 235/1000 | Loss: 0.00001042
Iteration 236/1000 | Loss: 0.00001042
Iteration 237/1000 | Loss: 0.00001042
Iteration 238/1000 | Loss: 0.00001042
Iteration 239/1000 | Loss: 0.00001042
Iteration 240/1000 | Loss: 0.00001042
Iteration 241/1000 | Loss: 0.00001042
Iteration 242/1000 | Loss: 0.00001042
Iteration 243/1000 | Loss: 0.00001042
Iteration 244/1000 | Loss: 0.00001042
Iteration 245/1000 | Loss: 0.00001042
Iteration 246/1000 | Loss: 0.00001042
Iteration 247/1000 | Loss: 0.00001042
Iteration 248/1000 | Loss: 0.00001042
Iteration 249/1000 | Loss: 0.00001042
Iteration 250/1000 | Loss: 0.00001042
Iteration 251/1000 | Loss: 0.00001042
Iteration 252/1000 | Loss: 0.00001042
Iteration 253/1000 | Loss: 0.00001042
Iteration 254/1000 | Loss: 0.00001042
Iteration 255/1000 | Loss: 0.00001042
Iteration 256/1000 | Loss: 0.00001042
Iteration 257/1000 | Loss: 0.00001042
Iteration 258/1000 | Loss: 0.00001042
Iteration 259/1000 | Loss: 0.00001042
Iteration 260/1000 | Loss: 0.00001042
Iteration 261/1000 | Loss: 0.00001042
Iteration 262/1000 | Loss: 0.00001042
Iteration 263/1000 | Loss: 0.00001042
Iteration 264/1000 | Loss: 0.00001042
Iteration 265/1000 | Loss: 0.00001042
Iteration 266/1000 | Loss: 0.00001042
Iteration 267/1000 | Loss: 0.00001042
Iteration 268/1000 | Loss: 0.00001042
Iteration 269/1000 | Loss: 0.00001042
Iteration 270/1000 | Loss: 0.00001042
Iteration 271/1000 | Loss: 0.00001042
Iteration 272/1000 | Loss: 0.00001042
Iteration 273/1000 | Loss: 0.00001042
Iteration 274/1000 | Loss: 0.00001042
Iteration 275/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.042153871821938e-05, 1.042153871821938e-05, 1.042153871821938e-05, 1.042153871821938e-05, 1.042153871821938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.042153871821938e-05

Optimization complete. Final v2v error: 2.7517597675323486 mm

Highest mean error: 2.981138229370117 mm for frame 1

Lowest mean error: 2.623025417327881 mm for frame 114

Saving results

Total time: 41.553969621658325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731386
Iteration 2/25 | Loss: 0.00153949
Iteration 3/25 | Loss: 0.00128811
Iteration 4/25 | Loss: 0.00126653
Iteration 5/25 | Loss: 0.00126125
Iteration 6/25 | Loss: 0.00126109
Iteration 7/25 | Loss: 0.00126109
Iteration 8/25 | Loss: 0.00126109
Iteration 9/25 | Loss: 0.00126109
Iteration 10/25 | Loss: 0.00126109
Iteration 11/25 | Loss: 0.00126109
Iteration 12/25 | Loss: 0.00126109
Iteration 13/25 | Loss: 0.00126109
Iteration 14/25 | Loss: 0.00126109
Iteration 15/25 | Loss: 0.00126109
Iteration 16/25 | Loss: 0.00126109
Iteration 17/25 | Loss: 0.00126109
Iteration 18/25 | Loss: 0.00126109
Iteration 19/25 | Loss: 0.00126109
Iteration 20/25 | Loss: 0.00126109
Iteration 21/25 | Loss: 0.00126109
Iteration 22/25 | Loss: 0.00126109
Iteration 23/25 | Loss: 0.00126109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001261089462786913, 0.001261089462786913, 0.001261089462786913, 0.001261089462786913, 0.001261089462786913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001261089462786913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42396724
Iteration 2/25 | Loss: 0.00058588
Iteration 3/25 | Loss: 0.00058587
Iteration 4/25 | Loss: 0.00058587
Iteration 5/25 | Loss: 0.00058587
Iteration 6/25 | Loss: 0.00058587
Iteration 7/25 | Loss: 0.00058587
Iteration 8/25 | Loss: 0.00058587
Iteration 9/25 | Loss: 0.00058587
Iteration 10/25 | Loss: 0.00058587
Iteration 11/25 | Loss: 0.00058587
Iteration 12/25 | Loss: 0.00058587
Iteration 13/25 | Loss: 0.00058587
Iteration 14/25 | Loss: 0.00058587
Iteration 15/25 | Loss: 0.00058587
Iteration 16/25 | Loss: 0.00058587
Iteration 17/25 | Loss: 0.00058587
Iteration 18/25 | Loss: 0.00058587
Iteration 19/25 | Loss: 0.00058587
Iteration 20/25 | Loss: 0.00058587
Iteration 21/25 | Loss: 0.00058587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005858688382431865, 0.0005858688382431865, 0.0005858688382431865, 0.0005858688382431865, 0.0005858688382431865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005858688382431865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058587
Iteration 2/1000 | Loss: 0.00003396
Iteration 3/1000 | Loss: 0.00002551
Iteration 4/1000 | Loss: 0.00002330
Iteration 5/1000 | Loss: 0.00002247
Iteration 6/1000 | Loss: 0.00002120
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00002003
Iteration 9/1000 | Loss: 0.00001955
Iteration 10/1000 | Loss: 0.00001918
Iteration 11/1000 | Loss: 0.00001903
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001882
Iteration 15/1000 | Loss: 0.00001880
Iteration 16/1000 | Loss: 0.00001878
Iteration 17/1000 | Loss: 0.00001872
Iteration 18/1000 | Loss: 0.00001865
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001849
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001849
Iteration 35/1000 | Loss: 0.00001849
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001849
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001849
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001849
Iteration 53/1000 | Loss: 0.00001849
Iteration 54/1000 | Loss: 0.00001849
Iteration 55/1000 | Loss: 0.00001849
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001849
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001849
Iteration 61/1000 | Loss: 0.00001849
Iteration 62/1000 | Loss: 0.00001849
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001849
Iteration 67/1000 | Loss: 0.00001849
Iteration 68/1000 | Loss: 0.00001849
Iteration 69/1000 | Loss: 0.00001849
Iteration 70/1000 | Loss: 0.00001849
Iteration 71/1000 | Loss: 0.00001849
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001849
Iteration 74/1000 | Loss: 0.00001849
Iteration 75/1000 | Loss: 0.00001849
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001849
Iteration 82/1000 | Loss: 0.00001849
Iteration 83/1000 | Loss: 0.00001849
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001849
Iteration 87/1000 | Loss: 0.00001849
Iteration 88/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.8488079149392433e-05, 1.8488079149392433e-05, 1.8488079149392433e-05, 1.8488079149392433e-05, 1.8488079149392433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8488079149392433e-05

Optimization complete. Final v2v error: 3.677764654159546 mm

Highest mean error: 3.815795421600342 mm for frame 204

Lowest mean error: 3.4758663177490234 mm for frame 180

Saving results

Total time: 34.71700429916382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409180
Iteration 2/25 | Loss: 0.00130093
Iteration 3/25 | Loss: 0.00120496
Iteration 4/25 | Loss: 0.00119889
Iteration 5/25 | Loss: 0.00119715
Iteration 6/25 | Loss: 0.00119709
Iteration 7/25 | Loss: 0.00119709
Iteration 8/25 | Loss: 0.00119709
Iteration 9/25 | Loss: 0.00119709
Iteration 10/25 | Loss: 0.00119709
Iteration 11/25 | Loss: 0.00119709
Iteration 12/25 | Loss: 0.00119709
Iteration 13/25 | Loss: 0.00119709
Iteration 14/25 | Loss: 0.00119709
Iteration 15/25 | Loss: 0.00119709
Iteration 16/25 | Loss: 0.00119709
Iteration 17/25 | Loss: 0.00119709
Iteration 18/25 | Loss: 0.00119709
Iteration 19/25 | Loss: 0.00119709
Iteration 20/25 | Loss: 0.00119709
Iteration 21/25 | Loss: 0.00119709
Iteration 22/25 | Loss: 0.00119709
Iteration 23/25 | Loss: 0.00119709
Iteration 24/25 | Loss: 0.00119709
Iteration 25/25 | Loss: 0.00119709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43680072
Iteration 2/25 | Loss: 0.00059858
Iteration 3/25 | Loss: 0.00059857
Iteration 4/25 | Loss: 0.00059857
Iteration 5/25 | Loss: 0.00059857
Iteration 6/25 | Loss: 0.00059857
Iteration 7/25 | Loss: 0.00059857
Iteration 8/25 | Loss: 0.00059857
Iteration 9/25 | Loss: 0.00059857
Iteration 10/25 | Loss: 0.00059857
Iteration 11/25 | Loss: 0.00059857
Iteration 12/25 | Loss: 0.00059857
Iteration 13/25 | Loss: 0.00059857
Iteration 14/25 | Loss: 0.00059857
Iteration 15/25 | Loss: 0.00059857
Iteration 16/25 | Loss: 0.00059857
Iteration 17/25 | Loss: 0.00059857
Iteration 18/25 | Loss: 0.00059857
Iteration 19/25 | Loss: 0.00059857
Iteration 20/25 | Loss: 0.00059857
Iteration 21/25 | Loss: 0.00059857
Iteration 22/25 | Loss: 0.00059857
Iteration 23/25 | Loss: 0.00059857
Iteration 24/25 | Loss: 0.00059857
Iteration 25/25 | Loss: 0.00059857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059857
Iteration 2/1000 | Loss: 0.00002628
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00001831
Iteration 5/1000 | Loss: 0.00001737
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001597
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001534
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001504
Iteration 12/1000 | Loss: 0.00001503
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001481
Iteration 17/1000 | Loss: 0.00001478
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001458
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001449
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001424
Iteration 40/1000 | Loss: 0.00001423
Iteration 41/1000 | Loss: 0.00001422
Iteration 42/1000 | Loss: 0.00001422
Iteration 43/1000 | Loss: 0.00001422
Iteration 44/1000 | Loss: 0.00001422
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001421
Iteration 47/1000 | Loss: 0.00001421
Iteration 48/1000 | Loss: 0.00001421
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001420
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001419
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001418
Iteration 63/1000 | Loss: 0.00001418
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001417
Iteration 66/1000 | Loss: 0.00001417
Iteration 67/1000 | Loss: 0.00001417
Iteration 68/1000 | Loss: 0.00001417
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001416
Iteration 71/1000 | Loss: 0.00001415
Iteration 72/1000 | Loss: 0.00001415
Iteration 73/1000 | Loss: 0.00001415
Iteration 74/1000 | Loss: 0.00001415
Iteration 75/1000 | Loss: 0.00001415
Iteration 76/1000 | Loss: 0.00001415
Iteration 77/1000 | Loss: 0.00001415
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001414
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001413
Iteration 82/1000 | Loss: 0.00001413
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001411
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001409
Iteration 105/1000 | Loss: 0.00001409
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001408
Iteration 110/1000 | Loss: 0.00001408
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00001407
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001407
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001407
Iteration 123/1000 | Loss: 0.00001407
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001407
Iteration 127/1000 | Loss: 0.00001407
Iteration 128/1000 | Loss: 0.00001406
Iteration 129/1000 | Loss: 0.00001406
Iteration 130/1000 | Loss: 0.00001406
Iteration 131/1000 | Loss: 0.00001406
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001406
Iteration 136/1000 | Loss: 0.00001406
Iteration 137/1000 | Loss: 0.00001406
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001405
Iteration 141/1000 | Loss: 0.00001405
Iteration 142/1000 | Loss: 0.00001405
Iteration 143/1000 | Loss: 0.00001405
Iteration 144/1000 | Loss: 0.00001405
Iteration 145/1000 | Loss: 0.00001405
Iteration 146/1000 | Loss: 0.00001405
Iteration 147/1000 | Loss: 0.00001405
Iteration 148/1000 | Loss: 0.00001405
Iteration 149/1000 | Loss: 0.00001405
Iteration 150/1000 | Loss: 0.00001405
Iteration 151/1000 | Loss: 0.00001405
Iteration 152/1000 | Loss: 0.00001405
Iteration 153/1000 | Loss: 0.00001405
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001403
Iteration 157/1000 | Loss: 0.00001403
Iteration 158/1000 | Loss: 0.00001403
Iteration 159/1000 | Loss: 0.00001403
Iteration 160/1000 | Loss: 0.00001403
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001403
Iteration 170/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.402960333507508e-05, 1.402960333507508e-05, 1.402960333507508e-05, 1.402960333507508e-05, 1.402960333507508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.402960333507508e-05

Optimization complete. Final v2v error: 3.194169044494629 mm

Highest mean error: 3.4444539546966553 mm for frame 115

Lowest mean error: 2.9442954063415527 mm for frame 11

Saving results

Total time: 41.97520923614502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756844
Iteration 2/25 | Loss: 0.00197023
Iteration 3/25 | Loss: 0.00149611
Iteration 4/25 | Loss: 0.00141446
Iteration 5/25 | Loss: 0.00138674
Iteration 6/25 | Loss: 0.00141072
Iteration 7/25 | Loss: 0.00137558
Iteration 8/25 | Loss: 0.00137536
Iteration 9/25 | Loss: 0.00138521
Iteration 10/25 | Loss: 0.00137309
Iteration 11/25 | Loss: 0.00134314
Iteration 12/25 | Loss: 0.00133138
Iteration 13/25 | Loss: 0.00132906
Iteration 14/25 | Loss: 0.00132705
Iteration 15/25 | Loss: 0.00132655
Iteration 16/25 | Loss: 0.00132640
Iteration 17/25 | Loss: 0.00132616
Iteration 18/25 | Loss: 0.00132547
Iteration 19/25 | Loss: 0.00132940
Iteration 20/25 | Loss: 0.00132911
Iteration 21/25 | Loss: 0.00132545
Iteration 22/25 | Loss: 0.00132361
Iteration 23/25 | Loss: 0.00132299
Iteration 24/25 | Loss: 0.00132299
Iteration 25/25 | Loss: 0.00132299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.24525785
Iteration 2/25 | Loss: 0.00083608
Iteration 3/25 | Loss: 0.00080303
Iteration 4/25 | Loss: 0.00080303
Iteration 5/25 | Loss: 0.00080303
Iteration 6/25 | Loss: 0.00080303
Iteration 7/25 | Loss: 0.00080303
Iteration 8/25 | Loss: 0.00080303
Iteration 9/25 | Loss: 0.00080303
Iteration 10/25 | Loss: 0.00080303
Iteration 11/25 | Loss: 0.00080303
Iteration 12/25 | Loss: 0.00080303
Iteration 13/25 | Loss: 0.00080303
Iteration 14/25 | Loss: 0.00080303
Iteration 15/25 | Loss: 0.00080303
Iteration 16/25 | Loss: 0.00080303
Iteration 17/25 | Loss: 0.00080303
Iteration 18/25 | Loss: 0.00080303
Iteration 19/25 | Loss: 0.00080303
Iteration 20/25 | Loss: 0.00080303
Iteration 21/25 | Loss: 0.00080303
Iteration 22/25 | Loss: 0.00080303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008030271274037659, 0.0008030271274037659, 0.0008030271274037659, 0.0008030271274037659, 0.0008030271274037659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008030271274037659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080303
Iteration 2/1000 | Loss: 0.00004886
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00005624
Iteration 5/1000 | Loss: 0.00002195
Iteration 6/1000 | Loss: 0.00002485
Iteration 7/1000 | Loss: 0.00003614
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00003151
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001960
Iteration 13/1000 | Loss: 0.00001960
Iteration 14/1000 | Loss: 0.00001958
Iteration 15/1000 | Loss: 0.00001943
Iteration 16/1000 | Loss: 0.00001937
Iteration 17/1000 | Loss: 0.00001933
Iteration 18/1000 | Loss: 0.00001931
Iteration 19/1000 | Loss: 0.00001927
Iteration 20/1000 | Loss: 0.00001926
Iteration 21/1000 | Loss: 0.00001919
Iteration 22/1000 | Loss: 0.00001911
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001911
Iteration 25/1000 | Loss: 0.00001911
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001908
Iteration 36/1000 | Loss: 0.00001907
Iteration 37/1000 | Loss: 0.00001907
Iteration 38/1000 | Loss: 0.00001906
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001904
Iteration 42/1000 | Loss: 0.00001904
Iteration 43/1000 | Loss: 0.00001903
Iteration 44/1000 | Loss: 0.00001901
Iteration 45/1000 | Loss: 0.00001901
Iteration 46/1000 | Loss: 0.00001901
Iteration 47/1000 | Loss: 0.00001901
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001900
Iteration 52/1000 | Loss: 0.00001900
Iteration 53/1000 | Loss: 0.00001900
Iteration 54/1000 | Loss: 0.00001900
Iteration 55/1000 | Loss: 0.00001899
Iteration 56/1000 | Loss: 0.00001898
Iteration 57/1000 | Loss: 0.00001898
Iteration 58/1000 | Loss: 0.00001898
Iteration 59/1000 | Loss: 0.00001898
Iteration 60/1000 | Loss: 0.00001898
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001898
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001897
Iteration 66/1000 | Loss: 0.00001897
Iteration 67/1000 | Loss: 0.00001897
Iteration 68/1000 | Loss: 0.00001897
Iteration 69/1000 | Loss: 0.00001896
Iteration 70/1000 | Loss: 0.00001896
Iteration 71/1000 | Loss: 0.00001896
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001895
Iteration 74/1000 | Loss: 0.00001895
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001891
Iteration 87/1000 | Loss: 0.00001891
Iteration 88/1000 | Loss: 0.00001890
Iteration 89/1000 | Loss: 0.00001890
Iteration 90/1000 | Loss: 0.00001889
Iteration 91/1000 | Loss: 0.00001889
Iteration 92/1000 | Loss: 0.00001889
Iteration 93/1000 | Loss: 0.00001888
Iteration 94/1000 | Loss: 0.00001888
Iteration 95/1000 | Loss: 0.00001888
Iteration 96/1000 | Loss: 0.00001888
Iteration 97/1000 | Loss: 0.00001888
Iteration 98/1000 | Loss: 0.00001887
Iteration 99/1000 | Loss: 0.00001887
Iteration 100/1000 | Loss: 0.00001887
Iteration 101/1000 | Loss: 0.00001887
Iteration 102/1000 | Loss: 0.00001887
Iteration 103/1000 | Loss: 0.00001886
Iteration 104/1000 | Loss: 0.00001886
Iteration 105/1000 | Loss: 0.00001886
Iteration 106/1000 | Loss: 0.00001885
Iteration 107/1000 | Loss: 0.00001885
Iteration 108/1000 | Loss: 0.00001885
Iteration 109/1000 | Loss: 0.00001884
Iteration 110/1000 | Loss: 0.00001884
Iteration 111/1000 | Loss: 0.00001884
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001880
Iteration 123/1000 | Loss: 0.00001880
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001880
Iteration 126/1000 | Loss: 0.00001880
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001879
Iteration 131/1000 | Loss: 0.00001879
Iteration 132/1000 | Loss: 0.00001879
Iteration 133/1000 | Loss: 0.00001879
Iteration 134/1000 | Loss: 0.00001879
Iteration 135/1000 | Loss: 0.00001878
Iteration 136/1000 | Loss: 0.00001878
Iteration 137/1000 | Loss: 0.00001878
Iteration 138/1000 | Loss: 0.00001878
Iteration 139/1000 | Loss: 0.00001878
Iteration 140/1000 | Loss: 0.00001878
Iteration 141/1000 | Loss: 0.00001878
Iteration 142/1000 | Loss: 0.00001878
Iteration 143/1000 | Loss: 0.00001878
Iteration 144/1000 | Loss: 0.00001877
Iteration 145/1000 | Loss: 0.00001877
Iteration 146/1000 | Loss: 0.00001877
Iteration 147/1000 | Loss: 0.00001877
Iteration 148/1000 | Loss: 0.00001877
Iteration 149/1000 | Loss: 0.00001877
Iteration 150/1000 | Loss: 0.00001877
Iteration 151/1000 | Loss: 0.00001877
Iteration 152/1000 | Loss: 0.00001877
Iteration 153/1000 | Loss: 0.00001877
Iteration 154/1000 | Loss: 0.00001877
Iteration 155/1000 | Loss: 0.00001877
Iteration 156/1000 | Loss: 0.00001877
Iteration 157/1000 | Loss: 0.00001876
Iteration 158/1000 | Loss: 0.00001876
Iteration 159/1000 | Loss: 0.00001876
Iteration 160/1000 | Loss: 0.00001876
Iteration 161/1000 | Loss: 0.00001876
Iteration 162/1000 | Loss: 0.00001876
Iteration 163/1000 | Loss: 0.00001876
Iteration 164/1000 | Loss: 0.00001876
Iteration 165/1000 | Loss: 0.00001876
Iteration 166/1000 | Loss: 0.00001876
Iteration 167/1000 | Loss: 0.00001876
Iteration 168/1000 | Loss: 0.00001876
Iteration 169/1000 | Loss: 0.00001875
Iteration 170/1000 | Loss: 0.00001875
Iteration 171/1000 | Loss: 0.00001875
Iteration 172/1000 | Loss: 0.00001875
Iteration 173/1000 | Loss: 0.00001875
Iteration 174/1000 | Loss: 0.00001875
Iteration 175/1000 | Loss: 0.00001875
Iteration 176/1000 | Loss: 0.00001875
Iteration 177/1000 | Loss: 0.00001875
Iteration 178/1000 | Loss: 0.00001875
Iteration 179/1000 | Loss: 0.00001875
Iteration 180/1000 | Loss: 0.00001875
Iteration 181/1000 | Loss: 0.00001875
Iteration 182/1000 | Loss: 0.00001874
Iteration 183/1000 | Loss: 0.00001874
Iteration 184/1000 | Loss: 0.00001874
Iteration 185/1000 | Loss: 0.00001874
Iteration 186/1000 | Loss: 0.00001874
Iteration 187/1000 | Loss: 0.00001874
Iteration 188/1000 | Loss: 0.00001874
Iteration 189/1000 | Loss: 0.00001874
Iteration 190/1000 | Loss: 0.00001873
Iteration 191/1000 | Loss: 0.00001873
Iteration 192/1000 | Loss: 0.00001873
Iteration 193/1000 | Loss: 0.00001873
Iteration 194/1000 | Loss: 0.00001873
Iteration 195/1000 | Loss: 0.00001873
Iteration 196/1000 | Loss: 0.00001873
Iteration 197/1000 | Loss: 0.00001873
Iteration 198/1000 | Loss: 0.00001873
Iteration 199/1000 | Loss: 0.00001873
Iteration 200/1000 | Loss: 0.00001873
Iteration 201/1000 | Loss: 0.00001872
Iteration 202/1000 | Loss: 0.00001872
Iteration 203/1000 | Loss: 0.00001872
Iteration 204/1000 | Loss: 0.00001872
Iteration 205/1000 | Loss: 0.00001872
Iteration 206/1000 | Loss: 0.00001872
Iteration 207/1000 | Loss: 0.00001872
Iteration 208/1000 | Loss: 0.00001872
Iteration 209/1000 | Loss: 0.00001872
Iteration 210/1000 | Loss: 0.00001872
Iteration 211/1000 | Loss: 0.00001872
Iteration 212/1000 | Loss: 0.00001872
Iteration 213/1000 | Loss: 0.00001872
Iteration 214/1000 | Loss: 0.00001872
Iteration 215/1000 | Loss: 0.00001872
Iteration 216/1000 | Loss: 0.00001872
Iteration 217/1000 | Loss: 0.00001872
Iteration 218/1000 | Loss: 0.00001871
Iteration 219/1000 | Loss: 0.00001871
Iteration 220/1000 | Loss: 0.00001871
Iteration 221/1000 | Loss: 0.00001871
Iteration 222/1000 | Loss: 0.00001871
Iteration 223/1000 | Loss: 0.00001871
Iteration 224/1000 | Loss: 0.00001871
Iteration 225/1000 | Loss: 0.00001871
Iteration 226/1000 | Loss: 0.00001871
Iteration 227/1000 | Loss: 0.00001871
Iteration 228/1000 | Loss: 0.00001871
Iteration 229/1000 | Loss: 0.00001871
Iteration 230/1000 | Loss: 0.00001871
Iteration 231/1000 | Loss: 0.00001871
Iteration 232/1000 | Loss: 0.00001871
Iteration 233/1000 | Loss: 0.00001871
Iteration 234/1000 | Loss: 0.00001871
Iteration 235/1000 | Loss: 0.00001871
Iteration 236/1000 | Loss: 0.00001871
Iteration 237/1000 | Loss: 0.00001871
Iteration 238/1000 | Loss: 0.00001871
Iteration 239/1000 | Loss: 0.00001871
Iteration 240/1000 | Loss: 0.00001871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.8708133211475797e-05, 1.8708133211475797e-05, 1.8708133211475797e-05, 1.8708133211475797e-05, 1.8708133211475797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8708133211475797e-05

Optimization complete. Final v2v error: 3.561047077178955 mm

Highest mean error: 4.083581447601318 mm for frame 177

Lowest mean error: 3.2114994525909424 mm for frame 15

Saving results

Total time: 82.26541256904602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597518
Iteration 2/25 | Loss: 0.00133378
Iteration 3/25 | Loss: 0.00123023
Iteration 4/25 | Loss: 0.00122013
Iteration 5/25 | Loss: 0.00121803
Iteration 6/25 | Loss: 0.00121803
Iteration 7/25 | Loss: 0.00121803
Iteration 8/25 | Loss: 0.00121803
Iteration 9/25 | Loss: 0.00121803
Iteration 10/25 | Loss: 0.00121803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012180303456261754, 0.0012180303456261754, 0.0012180303456261754, 0.0012180303456261754, 0.0012180303456261754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012180303456261754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63442731
Iteration 2/25 | Loss: 0.00065898
Iteration 3/25 | Loss: 0.00065897
Iteration 4/25 | Loss: 0.00065897
Iteration 5/25 | Loss: 0.00065897
Iteration 6/25 | Loss: 0.00065897
Iteration 7/25 | Loss: 0.00065897
Iteration 8/25 | Loss: 0.00065897
Iteration 9/25 | Loss: 0.00065897
Iteration 10/25 | Loss: 0.00065897
Iteration 11/25 | Loss: 0.00065897
Iteration 12/25 | Loss: 0.00065897
Iteration 13/25 | Loss: 0.00065897
Iteration 14/25 | Loss: 0.00065897
Iteration 15/25 | Loss: 0.00065897
Iteration 16/25 | Loss: 0.00065897
Iteration 17/25 | Loss: 0.00065897
Iteration 18/25 | Loss: 0.00065897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006589689874090254, 0.0006589689874090254, 0.0006589689874090254, 0.0006589689874090254, 0.0006589689874090254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006589689874090254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065897
Iteration 2/1000 | Loss: 0.00002273
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001516
Iteration 8/1000 | Loss: 0.00001497
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001482
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001445
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001442
Iteration 21/1000 | Loss: 0.00001442
Iteration 22/1000 | Loss: 0.00001437
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001425
Iteration 27/1000 | Loss: 0.00001423
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001419
Iteration 33/1000 | Loss: 0.00001418
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001415
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001412
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001411
Iteration 55/1000 | Loss: 0.00001411
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001410
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001408
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001406
Iteration 66/1000 | Loss: 0.00001406
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001404
Iteration 73/1000 | Loss: 0.00001404
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001404
Iteration 76/1000 | Loss: 0.00001404
Iteration 77/1000 | Loss: 0.00001404
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001403
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001403
Iteration 86/1000 | Loss: 0.00001403
Iteration 87/1000 | Loss: 0.00001403
Iteration 88/1000 | Loss: 0.00001403
Iteration 89/1000 | Loss: 0.00001402
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001402
Iteration 92/1000 | Loss: 0.00001402
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001401
Iteration 96/1000 | Loss: 0.00001401
Iteration 97/1000 | Loss: 0.00001401
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001401
Iteration 100/1000 | Loss: 0.00001401
Iteration 101/1000 | Loss: 0.00001401
Iteration 102/1000 | Loss: 0.00001401
Iteration 103/1000 | Loss: 0.00001401
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001401
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001400
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001399
Iteration 118/1000 | Loss: 0.00001399
Iteration 119/1000 | Loss: 0.00001399
Iteration 120/1000 | Loss: 0.00001398
Iteration 121/1000 | Loss: 0.00001398
Iteration 122/1000 | Loss: 0.00001398
Iteration 123/1000 | Loss: 0.00001398
Iteration 124/1000 | Loss: 0.00001398
Iteration 125/1000 | Loss: 0.00001398
Iteration 126/1000 | Loss: 0.00001398
Iteration 127/1000 | Loss: 0.00001398
Iteration 128/1000 | Loss: 0.00001398
Iteration 129/1000 | Loss: 0.00001398
Iteration 130/1000 | Loss: 0.00001398
Iteration 131/1000 | Loss: 0.00001398
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001398
Iteration 134/1000 | Loss: 0.00001398
Iteration 135/1000 | Loss: 0.00001398
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001398
Iteration 138/1000 | Loss: 0.00001398
Iteration 139/1000 | Loss: 0.00001398
Iteration 140/1000 | Loss: 0.00001398
Iteration 141/1000 | Loss: 0.00001398
Iteration 142/1000 | Loss: 0.00001398
Iteration 143/1000 | Loss: 0.00001398
Iteration 144/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.3978585229779128e-05, 1.3978585229779128e-05, 1.3978585229779128e-05, 1.3978585229779128e-05, 1.3978585229779128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3978585229779128e-05

Optimization complete. Final v2v error: 3.1537904739379883 mm

Highest mean error: 3.333240270614624 mm for frame 106

Lowest mean error: 2.961094856262207 mm for frame 126

Saving results

Total time: 39.23713707923889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078105
Iteration 2/25 | Loss: 0.00197206
Iteration 3/25 | Loss: 0.00154788
Iteration 4/25 | Loss: 0.00148979
Iteration 5/25 | Loss: 0.00180515
Iteration 6/25 | Loss: 0.00159192
Iteration 7/25 | Loss: 0.00141182
Iteration 8/25 | Loss: 0.00136314
Iteration 9/25 | Loss: 0.00141743
Iteration 10/25 | Loss: 0.00134017
Iteration 11/25 | Loss: 0.00133811
Iteration 12/25 | Loss: 0.00133798
Iteration 13/25 | Loss: 0.00133798
Iteration 14/25 | Loss: 0.00133798
Iteration 15/25 | Loss: 0.00133798
Iteration 16/25 | Loss: 0.00133798
Iteration 17/25 | Loss: 0.00133798
Iteration 18/25 | Loss: 0.00133797
Iteration 19/25 | Loss: 0.00133797
Iteration 20/25 | Loss: 0.00133797
Iteration 21/25 | Loss: 0.00133797
Iteration 22/25 | Loss: 0.00133797
Iteration 23/25 | Loss: 0.00133797
Iteration 24/25 | Loss: 0.00133795
Iteration 25/25 | Loss: 0.00133795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09619570
Iteration 2/25 | Loss: 0.00103358
Iteration 3/25 | Loss: 0.00103357
Iteration 4/25 | Loss: 0.00103357
Iteration 5/25 | Loss: 0.00103357
Iteration 6/25 | Loss: 0.00103357
Iteration 7/25 | Loss: 0.00103357
Iteration 8/25 | Loss: 0.00103357
Iteration 9/25 | Loss: 0.00103357
Iteration 10/25 | Loss: 0.00103357
Iteration 11/25 | Loss: 0.00103357
Iteration 12/25 | Loss: 0.00103357
Iteration 13/25 | Loss: 0.00103357
Iteration 14/25 | Loss: 0.00103356
Iteration 15/25 | Loss: 0.00103356
Iteration 16/25 | Loss: 0.00103356
Iteration 17/25 | Loss: 0.00103356
Iteration 18/25 | Loss: 0.00103356
Iteration 19/25 | Loss: 0.00103356
Iteration 20/25 | Loss: 0.00103356
Iteration 21/25 | Loss: 0.00103356
Iteration 22/25 | Loss: 0.00103356
Iteration 23/25 | Loss: 0.00103356
Iteration 24/25 | Loss: 0.00103356
Iteration 25/25 | Loss: 0.00103356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103356
Iteration 2/1000 | Loss: 0.00005608
Iteration 3/1000 | Loss: 0.00003612
Iteration 4/1000 | Loss: 0.00003094
Iteration 5/1000 | Loss: 0.00002886
Iteration 6/1000 | Loss: 0.00002776
Iteration 7/1000 | Loss: 0.00002696
Iteration 8/1000 | Loss: 0.00002628
Iteration 9/1000 | Loss: 0.00002585
Iteration 10/1000 | Loss: 0.00002559
Iteration 11/1000 | Loss: 0.00002533
Iteration 12/1000 | Loss: 0.00002512
Iteration 13/1000 | Loss: 0.00002495
Iteration 14/1000 | Loss: 0.00002479
Iteration 15/1000 | Loss: 0.00002475
Iteration 16/1000 | Loss: 0.00002468
Iteration 17/1000 | Loss: 0.00002465
Iteration 18/1000 | Loss: 0.00002463
Iteration 19/1000 | Loss: 0.00002461
Iteration 20/1000 | Loss: 0.00002459
Iteration 21/1000 | Loss: 0.00002457
Iteration 22/1000 | Loss: 0.00002454
Iteration 23/1000 | Loss: 0.00002454
Iteration 24/1000 | Loss: 0.00002451
Iteration 25/1000 | Loss: 0.00002450
Iteration 26/1000 | Loss: 0.00002445
Iteration 27/1000 | Loss: 0.00002439
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002433
Iteration 30/1000 | Loss: 0.00002432
Iteration 31/1000 | Loss: 0.00002432
Iteration 32/1000 | Loss: 0.00002431
Iteration 33/1000 | Loss: 0.00002431
Iteration 34/1000 | Loss: 0.00002428
Iteration 35/1000 | Loss: 0.00002428
Iteration 36/1000 | Loss: 0.00002428
Iteration 37/1000 | Loss: 0.00002428
Iteration 38/1000 | Loss: 0.00002427
Iteration 39/1000 | Loss: 0.00002427
Iteration 40/1000 | Loss: 0.00002427
Iteration 41/1000 | Loss: 0.00002427
Iteration 42/1000 | Loss: 0.00002427
Iteration 43/1000 | Loss: 0.00002426
Iteration 44/1000 | Loss: 0.00002426
Iteration 45/1000 | Loss: 0.00002426
Iteration 46/1000 | Loss: 0.00002424
Iteration 47/1000 | Loss: 0.00002424
Iteration 48/1000 | Loss: 0.00002423
Iteration 49/1000 | Loss: 0.00002423
Iteration 50/1000 | Loss: 0.00002423
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002421
Iteration 54/1000 | Loss: 0.00002421
Iteration 55/1000 | Loss: 0.00002420
Iteration 56/1000 | Loss: 0.00002419
Iteration 57/1000 | Loss: 0.00002419
Iteration 58/1000 | Loss: 0.00002418
Iteration 59/1000 | Loss: 0.00002418
Iteration 60/1000 | Loss: 0.00002416
Iteration 61/1000 | Loss: 0.00002416
Iteration 62/1000 | Loss: 0.00002415
Iteration 63/1000 | Loss: 0.00002415
Iteration 64/1000 | Loss: 0.00002414
Iteration 65/1000 | Loss: 0.00002414
Iteration 66/1000 | Loss: 0.00002413
Iteration 67/1000 | Loss: 0.00002413
Iteration 68/1000 | Loss: 0.00002413
Iteration 69/1000 | Loss: 0.00002412
Iteration 70/1000 | Loss: 0.00002412
Iteration 71/1000 | Loss: 0.00002412
Iteration 72/1000 | Loss: 0.00002411
Iteration 73/1000 | Loss: 0.00002411
Iteration 74/1000 | Loss: 0.00002411
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00002410
Iteration 77/1000 | Loss: 0.00002410
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002408
Iteration 82/1000 | Loss: 0.00002408
Iteration 83/1000 | Loss: 0.00002408
Iteration 84/1000 | Loss: 0.00002407
Iteration 85/1000 | Loss: 0.00002407
Iteration 86/1000 | Loss: 0.00002407
Iteration 87/1000 | Loss: 0.00002406
Iteration 88/1000 | Loss: 0.00002406
Iteration 89/1000 | Loss: 0.00002406
Iteration 90/1000 | Loss: 0.00002405
Iteration 91/1000 | Loss: 0.00002405
Iteration 92/1000 | Loss: 0.00002405
Iteration 93/1000 | Loss: 0.00002404
Iteration 94/1000 | Loss: 0.00002404
Iteration 95/1000 | Loss: 0.00002404
Iteration 96/1000 | Loss: 0.00002403
Iteration 97/1000 | Loss: 0.00002403
Iteration 98/1000 | Loss: 0.00002403
Iteration 99/1000 | Loss: 0.00002403
Iteration 100/1000 | Loss: 0.00002402
Iteration 101/1000 | Loss: 0.00002402
Iteration 102/1000 | Loss: 0.00002402
Iteration 103/1000 | Loss: 0.00002402
Iteration 104/1000 | Loss: 0.00002402
Iteration 105/1000 | Loss: 0.00002402
Iteration 106/1000 | Loss: 0.00002401
Iteration 107/1000 | Loss: 0.00002401
Iteration 108/1000 | Loss: 0.00002401
Iteration 109/1000 | Loss: 0.00002401
Iteration 110/1000 | Loss: 0.00002401
Iteration 111/1000 | Loss: 0.00002400
Iteration 112/1000 | Loss: 0.00002400
Iteration 113/1000 | Loss: 0.00002400
Iteration 114/1000 | Loss: 0.00002400
Iteration 115/1000 | Loss: 0.00002400
Iteration 116/1000 | Loss: 0.00002400
Iteration 117/1000 | Loss: 0.00002400
Iteration 118/1000 | Loss: 0.00002400
Iteration 119/1000 | Loss: 0.00002400
Iteration 120/1000 | Loss: 0.00002400
Iteration 121/1000 | Loss: 0.00002400
Iteration 122/1000 | Loss: 0.00002400
Iteration 123/1000 | Loss: 0.00002400
Iteration 124/1000 | Loss: 0.00002400
Iteration 125/1000 | Loss: 0.00002400
Iteration 126/1000 | Loss: 0.00002400
Iteration 127/1000 | Loss: 0.00002400
Iteration 128/1000 | Loss: 0.00002400
Iteration 129/1000 | Loss: 0.00002400
Iteration 130/1000 | Loss: 0.00002400
Iteration 131/1000 | Loss: 0.00002400
Iteration 132/1000 | Loss: 0.00002400
Iteration 133/1000 | Loss: 0.00002400
Iteration 134/1000 | Loss: 0.00002400
Iteration 135/1000 | Loss: 0.00002400
Iteration 136/1000 | Loss: 0.00002400
Iteration 137/1000 | Loss: 0.00002400
Iteration 138/1000 | Loss: 0.00002400
Iteration 139/1000 | Loss: 0.00002400
Iteration 140/1000 | Loss: 0.00002400
Iteration 141/1000 | Loss: 0.00002400
Iteration 142/1000 | Loss: 0.00002400
Iteration 143/1000 | Loss: 0.00002400
Iteration 144/1000 | Loss: 0.00002400
Iteration 145/1000 | Loss: 0.00002400
Iteration 146/1000 | Loss: 0.00002400
Iteration 147/1000 | Loss: 0.00002400
Iteration 148/1000 | Loss: 0.00002400
Iteration 149/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.399536060693208e-05, 2.399536060693208e-05, 2.399536060693208e-05, 2.399536060693208e-05, 2.399536060693208e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.399536060693208e-05

Optimization complete. Final v2v error: 4.006664752960205 mm

Highest mean error: 5.237065315246582 mm for frame 50

Lowest mean error: 3.445305824279785 mm for frame 99

Saving results

Total time: 57.88766384124756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878713
Iteration 2/25 | Loss: 0.00151625
Iteration 3/25 | Loss: 0.00131217
Iteration 4/25 | Loss: 0.00128786
Iteration 5/25 | Loss: 0.00128989
Iteration 6/25 | Loss: 0.00128619
Iteration 7/25 | Loss: 0.00127433
Iteration 8/25 | Loss: 0.00127383
Iteration 9/25 | Loss: 0.00127371
Iteration 10/25 | Loss: 0.00127371
Iteration 11/25 | Loss: 0.00127371
Iteration 12/25 | Loss: 0.00127371
Iteration 13/25 | Loss: 0.00127371
Iteration 14/25 | Loss: 0.00127371
Iteration 15/25 | Loss: 0.00127371
Iteration 16/25 | Loss: 0.00127371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001273707370273769, 0.001273707370273769, 0.001273707370273769, 0.001273707370273769, 0.001273707370273769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273707370273769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40408826
Iteration 2/25 | Loss: 0.00080581
Iteration 3/25 | Loss: 0.00080580
Iteration 4/25 | Loss: 0.00080580
Iteration 5/25 | Loss: 0.00080580
Iteration 6/25 | Loss: 0.00080580
Iteration 7/25 | Loss: 0.00080580
Iteration 8/25 | Loss: 0.00080580
Iteration 9/25 | Loss: 0.00080580
Iteration 10/25 | Loss: 0.00080580
Iteration 11/25 | Loss: 0.00080580
Iteration 12/25 | Loss: 0.00080580
Iteration 13/25 | Loss: 0.00080580
Iteration 14/25 | Loss: 0.00080580
Iteration 15/25 | Loss: 0.00080580
Iteration 16/25 | Loss: 0.00080580
Iteration 17/25 | Loss: 0.00080580
Iteration 18/25 | Loss: 0.00080580
Iteration 19/25 | Loss: 0.00080580
Iteration 20/25 | Loss: 0.00080580
Iteration 21/25 | Loss: 0.00080580
Iteration 22/25 | Loss: 0.00080580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008057954255491495, 0.0008057954255491495, 0.0008057954255491495, 0.0008057954255491495, 0.0008057954255491495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008057954255491495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080580
Iteration 2/1000 | Loss: 0.00003634
Iteration 3/1000 | Loss: 0.00002400
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001939
Iteration 7/1000 | Loss: 0.00001871
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001748
Iteration 11/1000 | Loss: 0.00012676
Iteration 12/1000 | Loss: 0.00001735
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001722
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001714
Iteration 18/1000 | Loss: 0.00001712
Iteration 19/1000 | Loss: 0.00001712
Iteration 20/1000 | Loss: 0.00001712
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00001708
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001702
Iteration 28/1000 | Loss: 0.00001702
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001697
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001694
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001693
Iteration 48/1000 | Loss: 0.00001693
Iteration 49/1000 | Loss: 0.00001693
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001690
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001688
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001687
Iteration 74/1000 | Loss: 0.00001687
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001686
Iteration 77/1000 | Loss: 0.00001686
Iteration 78/1000 | Loss: 0.00001686
Iteration 79/1000 | Loss: 0.00001686
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001685
Iteration 82/1000 | Loss: 0.00014439
Iteration 83/1000 | Loss: 0.00003648
Iteration 84/1000 | Loss: 0.00001688
Iteration 85/1000 | Loss: 0.00001684
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00005764
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001679
Iteration 91/1000 | Loss: 0.00001679
Iteration 92/1000 | Loss: 0.00001679
Iteration 93/1000 | Loss: 0.00001679
Iteration 94/1000 | Loss: 0.00001679
Iteration 95/1000 | Loss: 0.00001679
Iteration 96/1000 | Loss: 0.00001679
Iteration 97/1000 | Loss: 0.00001679
Iteration 98/1000 | Loss: 0.00001679
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001678
Iteration 102/1000 | Loss: 0.00001678
Iteration 103/1000 | Loss: 0.00001678
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Iteration 106/1000 | Loss: 0.00001678
Iteration 107/1000 | Loss: 0.00001677
Iteration 108/1000 | Loss: 0.00001677
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00001677
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001677
Iteration 113/1000 | Loss: 0.00001677
Iteration 114/1000 | Loss: 0.00001677
Iteration 115/1000 | Loss: 0.00001677
Iteration 116/1000 | Loss: 0.00001677
Iteration 117/1000 | Loss: 0.00001677
Iteration 118/1000 | Loss: 0.00001677
Iteration 119/1000 | Loss: 0.00001677
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001677
Iteration 124/1000 | Loss: 0.00001677
Iteration 125/1000 | Loss: 0.00001676
Iteration 126/1000 | Loss: 0.00001676
Iteration 127/1000 | Loss: 0.00001676
Iteration 128/1000 | Loss: 0.00001676
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00001676
Iteration 132/1000 | Loss: 0.00001676
Iteration 133/1000 | Loss: 0.00008527
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001675
Iteration 137/1000 | Loss: 0.00001673
Iteration 138/1000 | Loss: 0.00001673
Iteration 139/1000 | Loss: 0.00001673
Iteration 140/1000 | Loss: 0.00001673
Iteration 141/1000 | Loss: 0.00001673
Iteration 142/1000 | Loss: 0.00001673
Iteration 143/1000 | Loss: 0.00001673
Iteration 144/1000 | Loss: 0.00001673
Iteration 145/1000 | Loss: 0.00001673
Iteration 146/1000 | Loss: 0.00001673
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001672
Iteration 149/1000 | Loss: 0.00001672
Iteration 150/1000 | Loss: 0.00001672
Iteration 151/1000 | Loss: 0.00001672
Iteration 152/1000 | Loss: 0.00001672
Iteration 153/1000 | Loss: 0.00001672
Iteration 154/1000 | Loss: 0.00001672
Iteration 155/1000 | Loss: 0.00001672
Iteration 156/1000 | Loss: 0.00001672
Iteration 157/1000 | Loss: 0.00001671
Iteration 158/1000 | Loss: 0.00001671
Iteration 159/1000 | Loss: 0.00001671
Iteration 160/1000 | Loss: 0.00001670
Iteration 161/1000 | Loss: 0.00001670
Iteration 162/1000 | Loss: 0.00001670
Iteration 163/1000 | Loss: 0.00001670
Iteration 164/1000 | Loss: 0.00001670
Iteration 165/1000 | Loss: 0.00001669
Iteration 166/1000 | Loss: 0.00001669
Iteration 167/1000 | Loss: 0.00001669
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001669
Iteration 170/1000 | Loss: 0.00001669
Iteration 171/1000 | Loss: 0.00001668
Iteration 172/1000 | Loss: 0.00001668
Iteration 173/1000 | Loss: 0.00001668
Iteration 174/1000 | Loss: 0.00001668
Iteration 175/1000 | Loss: 0.00001668
Iteration 176/1000 | Loss: 0.00001668
Iteration 177/1000 | Loss: 0.00001668
Iteration 178/1000 | Loss: 0.00001668
Iteration 179/1000 | Loss: 0.00001668
Iteration 180/1000 | Loss: 0.00001668
Iteration 181/1000 | Loss: 0.00001668
Iteration 182/1000 | Loss: 0.00001668
Iteration 183/1000 | Loss: 0.00001667
Iteration 184/1000 | Loss: 0.00001667
Iteration 185/1000 | Loss: 0.00001667
Iteration 186/1000 | Loss: 0.00001667
Iteration 187/1000 | Loss: 0.00001667
Iteration 188/1000 | Loss: 0.00001667
Iteration 189/1000 | Loss: 0.00001667
Iteration 190/1000 | Loss: 0.00001667
Iteration 191/1000 | Loss: 0.00001666
Iteration 192/1000 | Loss: 0.00001666
Iteration 193/1000 | Loss: 0.00001666
Iteration 194/1000 | Loss: 0.00001666
Iteration 195/1000 | Loss: 0.00001666
Iteration 196/1000 | Loss: 0.00001666
Iteration 197/1000 | Loss: 0.00001666
Iteration 198/1000 | Loss: 0.00001665
Iteration 199/1000 | Loss: 0.00001665
Iteration 200/1000 | Loss: 0.00001665
Iteration 201/1000 | Loss: 0.00001665
Iteration 202/1000 | Loss: 0.00001665
Iteration 203/1000 | Loss: 0.00001665
Iteration 204/1000 | Loss: 0.00001665
Iteration 205/1000 | Loss: 0.00001665
Iteration 206/1000 | Loss: 0.00001665
Iteration 207/1000 | Loss: 0.00001665
Iteration 208/1000 | Loss: 0.00001665
Iteration 209/1000 | Loss: 0.00001665
Iteration 210/1000 | Loss: 0.00001665
Iteration 211/1000 | Loss: 0.00001665
Iteration 212/1000 | Loss: 0.00001665
Iteration 213/1000 | Loss: 0.00001665
Iteration 214/1000 | Loss: 0.00001665
Iteration 215/1000 | Loss: 0.00001665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.6647019947413355e-05, 1.6647019947413355e-05, 1.6647019947413355e-05, 1.6647019947413355e-05, 1.6647019947413355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6647019947413355e-05

Optimization complete. Final v2v error: 3.402693033218384 mm

Highest mean error: 4.125023365020752 mm for frame 183

Lowest mean error: 2.983983278274536 mm for frame 204

Saving results

Total time: 64.85968804359436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615620
Iteration 2/25 | Loss: 0.00162362
Iteration 3/25 | Loss: 0.00146230
Iteration 4/25 | Loss: 0.00144616
Iteration 5/25 | Loss: 0.00144314
Iteration 6/25 | Loss: 0.00144314
Iteration 7/25 | Loss: 0.00144314
Iteration 8/25 | Loss: 0.00144314
Iteration 9/25 | Loss: 0.00144314
Iteration 10/25 | Loss: 0.00144314
Iteration 11/25 | Loss: 0.00144314
Iteration 12/25 | Loss: 0.00144314
Iteration 13/25 | Loss: 0.00144314
Iteration 14/25 | Loss: 0.00144314
Iteration 15/25 | Loss: 0.00144314
Iteration 16/25 | Loss: 0.00144314
Iteration 17/25 | Loss: 0.00144314
Iteration 18/25 | Loss: 0.00144314
Iteration 19/25 | Loss: 0.00144314
Iteration 20/25 | Loss: 0.00144314
Iteration 21/25 | Loss: 0.00144314
Iteration 22/25 | Loss: 0.00144314
Iteration 23/25 | Loss: 0.00144314
Iteration 24/25 | Loss: 0.00144314
Iteration 25/25 | Loss: 0.00144314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014431440504267812, 0.0014431440504267812, 0.0014431440504267812, 0.0014431440504267812, 0.0014431440504267812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014431440504267812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67150760
Iteration 2/25 | Loss: 0.00088852
Iteration 3/25 | Loss: 0.00088851
Iteration 4/25 | Loss: 0.00088851
Iteration 5/25 | Loss: 0.00088851
Iteration 6/25 | Loss: 0.00088851
Iteration 7/25 | Loss: 0.00088851
Iteration 8/25 | Loss: 0.00088851
Iteration 9/25 | Loss: 0.00088851
Iteration 10/25 | Loss: 0.00088851
Iteration 11/25 | Loss: 0.00088851
Iteration 12/25 | Loss: 0.00088851
Iteration 13/25 | Loss: 0.00088851
Iteration 14/25 | Loss: 0.00088851
Iteration 15/25 | Loss: 0.00088851
Iteration 16/25 | Loss: 0.00088851
Iteration 17/25 | Loss: 0.00088851
Iteration 18/25 | Loss: 0.00088851
Iteration 19/25 | Loss: 0.00088851
Iteration 20/25 | Loss: 0.00088851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000888511654920876, 0.000888511654920876, 0.000888511654920876, 0.000888511654920876, 0.000888511654920876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000888511654920876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088851
Iteration 2/1000 | Loss: 0.00005384
Iteration 3/1000 | Loss: 0.00003888
Iteration 4/1000 | Loss: 0.00003637
Iteration 5/1000 | Loss: 0.00003551
Iteration 6/1000 | Loss: 0.00003471
Iteration 7/1000 | Loss: 0.00003437
Iteration 8/1000 | Loss: 0.00003406
Iteration 9/1000 | Loss: 0.00003378
Iteration 10/1000 | Loss: 0.00003358
Iteration 11/1000 | Loss: 0.00003338
Iteration 12/1000 | Loss: 0.00003325
Iteration 13/1000 | Loss: 0.00003313
Iteration 14/1000 | Loss: 0.00003311
Iteration 15/1000 | Loss: 0.00003310
Iteration 16/1000 | Loss: 0.00003310
Iteration 17/1000 | Loss: 0.00003310
Iteration 18/1000 | Loss: 0.00003310
Iteration 19/1000 | Loss: 0.00003310
Iteration 20/1000 | Loss: 0.00003310
Iteration 21/1000 | Loss: 0.00003310
Iteration 22/1000 | Loss: 0.00003310
Iteration 23/1000 | Loss: 0.00003310
Iteration 24/1000 | Loss: 0.00003309
Iteration 25/1000 | Loss: 0.00003309
Iteration 26/1000 | Loss: 0.00003305
Iteration 27/1000 | Loss: 0.00003298
Iteration 28/1000 | Loss: 0.00003298
Iteration 29/1000 | Loss: 0.00003297
Iteration 30/1000 | Loss: 0.00003297
Iteration 31/1000 | Loss: 0.00003297
Iteration 32/1000 | Loss: 0.00003297
Iteration 33/1000 | Loss: 0.00003297
Iteration 34/1000 | Loss: 0.00003297
Iteration 35/1000 | Loss: 0.00003297
Iteration 36/1000 | Loss: 0.00003296
Iteration 37/1000 | Loss: 0.00003296
Iteration 38/1000 | Loss: 0.00003296
Iteration 39/1000 | Loss: 0.00003296
Iteration 40/1000 | Loss: 0.00003294
Iteration 41/1000 | Loss: 0.00003293
Iteration 42/1000 | Loss: 0.00003291
Iteration 43/1000 | Loss: 0.00003291
Iteration 44/1000 | Loss: 0.00003291
Iteration 45/1000 | Loss: 0.00003291
Iteration 46/1000 | Loss: 0.00003291
Iteration 47/1000 | Loss: 0.00003290
Iteration 48/1000 | Loss: 0.00003289
Iteration 49/1000 | Loss: 0.00003289
Iteration 50/1000 | Loss: 0.00003288
Iteration 51/1000 | Loss: 0.00003288
Iteration 52/1000 | Loss: 0.00003288
Iteration 53/1000 | Loss: 0.00003288
Iteration 54/1000 | Loss: 0.00003288
Iteration 55/1000 | Loss: 0.00003288
Iteration 56/1000 | Loss: 0.00003287
Iteration 57/1000 | Loss: 0.00003287
Iteration 58/1000 | Loss: 0.00003287
Iteration 59/1000 | Loss: 0.00003287
Iteration 60/1000 | Loss: 0.00003287
Iteration 61/1000 | Loss: 0.00003286
Iteration 62/1000 | Loss: 0.00003286
Iteration 63/1000 | Loss: 0.00003286
Iteration 64/1000 | Loss: 0.00003286
Iteration 65/1000 | Loss: 0.00003285
Iteration 66/1000 | Loss: 0.00003285
Iteration 67/1000 | Loss: 0.00003285
Iteration 68/1000 | Loss: 0.00003284
Iteration 69/1000 | Loss: 0.00003284
Iteration 70/1000 | Loss: 0.00003284
Iteration 71/1000 | Loss: 0.00003284
Iteration 72/1000 | Loss: 0.00003283
Iteration 73/1000 | Loss: 0.00003283
Iteration 74/1000 | Loss: 0.00003282
Iteration 75/1000 | Loss: 0.00003282
Iteration 76/1000 | Loss: 0.00003282
Iteration 77/1000 | Loss: 0.00003282
Iteration 78/1000 | Loss: 0.00003282
Iteration 79/1000 | Loss: 0.00003282
Iteration 80/1000 | Loss: 0.00003282
Iteration 81/1000 | Loss: 0.00003282
Iteration 82/1000 | Loss: 0.00003282
Iteration 83/1000 | Loss: 0.00003282
Iteration 84/1000 | Loss: 0.00003282
Iteration 85/1000 | Loss: 0.00003282
Iteration 86/1000 | Loss: 0.00003282
Iteration 87/1000 | Loss: 0.00003282
Iteration 88/1000 | Loss: 0.00003282
Iteration 89/1000 | Loss: 0.00003282
Iteration 90/1000 | Loss: 0.00003282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [3.2818392355693504e-05, 3.2818392355693504e-05, 3.2818392355693504e-05, 3.2818392355693504e-05, 3.2818392355693504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2818392355693504e-05

Optimization complete. Final v2v error: 4.761832237243652 mm

Highest mean error: 4.858760833740234 mm for frame 70

Lowest mean error: 4.563117980957031 mm for frame 128

Saving results

Total time: 38.04766058921814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394046
Iteration 2/25 | Loss: 0.00129969
Iteration 3/25 | Loss: 0.00119311
Iteration 4/25 | Loss: 0.00118409
Iteration 5/25 | Loss: 0.00118188
Iteration 6/25 | Loss: 0.00118162
Iteration 7/25 | Loss: 0.00118162
Iteration 8/25 | Loss: 0.00118162
Iteration 9/25 | Loss: 0.00118162
Iteration 10/25 | Loss: 0.00118162
Iteration 11/25 | Loss: 0.00118162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011816248297691345, 0.0011816248297691345, 0.0011816248297691345, 0.0011816248297691345, 0.0011816248297691345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011816248297691345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43951869
Iteration 2/25 | Loss: 0.00064220
Iteration 3/25 | Loss: 0.00064219
Iteration 4/25 | Loss: 0.00064219
Iteration 5/25 | Loss: 0.00064219
Iteration 6/25 | Loss: 0.00064219
Iteration 7/25 | Loss: 0.00064219
Iteration 8/25 | Loss: 0.00064219
Iteration 9/25 | Loss: 0.00064219
Iteration 10/25 | Loss: 0.00064219
Iteration 11/25 | Loss: 0.00064219
Iteration 12/25 | Loss: 0.00064219
Iteration 13/25 | Loss: 0.00064219
Iteration 14/25 | Loss: 0.00064219
Iteration 15/25 | Loss: 0.00064219
Iteration 16/25 | Loss: 0.00064219
Iteration 17/25 | Loss: 0.00064219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006421853322535753, 0.0006421853322535753, 0.0006421853322535753, 0.0006421853322535753, 0.0006421853322535753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006421853322535753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064219
Iteration 2/1000 | Loss: 0.00003337
Iteration 3/1000 | Loss: 0.00002045
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001603
Iteration 6/1000 | Loss: 0.00001510
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001334
Iteration 14/1000 | Loss: 0.00001334
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001318
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001308
Iteration 25/1000 | Loss: 0.00001308
Iteration 26/1000 | Loss: 0.00001308
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001305
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001288
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001282
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001281
Iteration 71/1000 | Loss: 0.00001281
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001280
Iteration 77/1000 | Loss: 0.00001279
Iteration 78/1000 | Loss: 0.00001279
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001278
Iteration 81/1000 | Loss: 0.00001278
Iteration 82/1000 | Loss: 0.00001278
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001277
Iteration 85/1000 | Loss: 0.00001277
Iteration 86/1000 | Loss: 0.00001277
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001275
Iteration 91/1000 | Loss: 0.00001275
Iteration 92/1000 | Loss: 0.00001274
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001272
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001271
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001270
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001270
Iteration 108/1000 | Loss: 0.00001270
Iteration 109/1000 | Loss: 0.00001270
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001269
Iteration 115/1000 | Loss: 0.00001268
Iteration 116/1000 | Loss: 0.00001268
Iteration 117/1000 | Loss: 0.00001267
Iteration 118/1000 | Loss: 0.00001267
Iteration 119/1000 | Loss: 0.00001267
Iteration 120/1000 | Loss: 0.00001267
Iteration 121/1000 | Loss: 0.00001267
Iteration 122/1000 | Loss: 0.00001267
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001266
Iteration 125/1000 | Loss: 0.00001266
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001264
Iteration 129/1000 | Loss: 0.00001264
Iteration 130/1000 | Loss: 0.00001264
Iteration 131/1000 | Loss: 0.00001264
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001263
Iteration 134/1000 | Loss: 0.00001263
Iteration 135/1000 | Loss: 0.00001263
Iteration 136/1000 | Loss: 0.00001262
Iteration 137/1000 | Loss: 0.00001262
Iteration 138/1000 | Loss: 0.00001262
Iteration 139/1000 | Loss: 0.00001262
Iteration 140/1000 | Loss: 0.00001262
Iteration 141/1000 | Loss: 0.00001261
Iteration 142/1000 | Loss: 0.00001261
Iteration 143/1000 | Loss: 0.00001261
Iteration 144/1000 | Loss: 0.00001260
Iteration 145/1000 | Loss: 0.00001260
Iteration 146/1000 | Loss: 0.00001260
Iteration 147/1000 | Loss: 0.00001260
Iteration 148/1000 | Loss: 0.00001260
Iteration 149/1000 | Loss: 0.00001260
Iteration 150/1000 | Loss: 0.00001260
Iteration 151/1000 | Loss: 0.00001260
Iteration 152/1000 | Loss: 0.00001260
Iteration 153/1000 | Loss: 0.00001259
Iteration 154/1000 | Loss: 0.00001259
Iteration 155/1000 | Loss: 0.00001259
Iteration 156/1000 | Loss: 0.00001259
Iteration 157/1000 | Loss: 0.00001259
Iteration 158/1000 | Loss: 0.00001259
Iteration 159/1000 | Loss: 0.00001259
Iteration 160/1000 | Loss: 0.00001258
Iteration 161/1000 | Loss: 0.00001258
Iteration 162/1000 | Loss: 0.00001258
Iteration 163/1000 | Loss: 0.00001258
Iteration 164/1000 | Loss: 0.00001258
Iteration 165/1000 | Loss: 0.00001257
Iteration 166/1000 | Loss: 0.00001257
Iteration 167/1000 | Loss: 0.00001257
Iteration 168/1000 | Loss: 0.00001257
Iteration 169/1000 | Loss: 0.00001257
Iteration 170/1000 | Loss: 0.00001257
Iteration 171/1000 | Loss: 0.00001257
Iteration 172/1000 | Loss: 0.00001257
Iteration 173/1000 | Loss: 0.00001257
Iteration 174/1000 | Loss: 0.00001257
Iteration 175/1000 | Loss: 0.00001257
Iteration 176/1000 | Loss: 0.00001257
Iteration 177/1000 | Loss: 0.00001257
Iteration 178/1000 | Loss: 0.00001256
Iteration 179/1000 | Loss: 0.00001256
Iteration 180/1000 | Loss: 0.00001256
Iteration 181/1000 | Loss: 0.00001256
Iteration 182/1000 | Loss: 0.00001256
Iteration 183/1000 | Loss: 0.00001256
Iteration 184/1000 | Loss: 0.00001256
Iteration 185/1000 | Loss: 0.00001256
Iteration 186/1000 | Loss: 0.00001256
Iteration 187/1000 | Loss: 0.00001256
Iteration 188/1000 | Loss: 0.00001256
Iteration 189/1000 | Loss: 0.00001256
Iteration 190/1000 | Loss: 0.00001256
Iteration 191/1000 | Loss: 0.00001256
Iteration 192/1000 | Loss: 0.00001256
Iteration 193/1000 | Loss: 0.00001255
Iteration 194/1000 | Loss: 0.00001255
Iteration 195/1000 | Loss: 0.00001255
Iteration 196/1000 | Loss: 0.00001255
Iteration 197/1000 | Loss: 0.00001255
Iteration 198/1000 | Loss: 0.00001255
Iteration 199/1000 | Loss: 0.00001255
Iteration 200/1000 | Loss: 0.00001255
Iteration 201/1000 | Loss: 0.00001255
Iteration 202/1000 | Loss: 0.00001255
Iteration 203/1000 | Loss: 0.00001255
Iteration 204/1000 | Loss: 0.00001255
Iteration 205/1000 | Loss: 0.00001255
Iteration 206/1000 | Loss: 0.00001255
Iteration 207/1000 | Loss: 0.00001255
Iteration 208/1000 | Loss: 0.00001255
Iteration 209/1000 | Loss: 0.00001255
Iteration 210/1000 | Loss: 0.00001255
Iteration 211/1000 | Loss: 0.00001255
Iteration 212/1000 | Loss: 0.00001255
Iteration 213/1000 | Loss: 0.00001255
Iteration 214/1000 | Loss: 0.00001255
Iteration 215/1000 | Loss: 0.00001255
Iteration 216/1000 | Loss: 0.00001255
Iteration 217/1000 | Loss: 0.00001255
Iteration 218/1000 | Loss: 0.00001255
Iteration 219/1000 | Loss: 0.00001255
Iteration 220/1000 | Loss: 0.00001255
Iteration 221/1000 | Loss: 0.00001255
Iteration 222/1000 | Loss: 0.00001255
Iteration 223/1000 | Loss: 0.00001255
Iteration 224/1000 | Loss: 0.00001255
Iteration 225/1000 | Loss: 0.00001255
Iteration 226/1000 | Loss: 0.00001255
Iteration 227/1000 | Loss: 0.00001255
Iteration 228/1000 | Loss: 0.00001255
Iteration 229/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.2553121450764593e-05, 1.2553121450764593e-05, 1.2553121450764593e-05, 1.2553121450764593e-05, 1.2553121450764593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2553121450764593e-05

Optimization complete. Final v2v error: 3.032111644744873 mm

Highest mean error: 3.2552764415740967 mm for frame 42

Lowest mean error: 2.8799917697906494 mm for frame 130

Saving results

Total time: 42.006805419921875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996493
Iteration 2/25 | Loss: 0.00244552
Iteration 3/25 | Loss: 0.00160973
Iteration 4/25 | Loss: 0.00147663
Iteration 5/25 | Loss: 0.00142386
Iteration 6/25 | Loss: 0.00153136
Iteration 7/25 | Loss: 0.00135491
Iteration 8/25 | Loss: 0.00133412
Iteration 9/25 | Loss: 0.00131458
Iteration 10/25 | Loss: 0.00130212
Iteration 11/25 | Loss: 0.00130094
Iteration 12/25 | Loss: 0.00130061
Iteration 13/25 | Loss: 0.00129496
Iteration 14/25 | Loss: 0.00129131
Iteration 15/25 | Loss: 0.00129007
Iteration 16/25 | Loss: 0.00128981
Iteration 17/25 | Loss: 0.00128980
Iteration 18/25 | Loss: 0.00128978
Iteration 19/25 | Loss: 0.00128977
Iteration 20/25 | Loss: 0.00128976
Iteration 21/25 | Loss: 0.00128976
Iteration 22/25 | Loss: 0.00128976
Iteration 23/25 | Loss: 0.00128976
Iteration 24/25 | Loss: 0.00128976
Iteration 25/25 | Loss: 0.00128976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57319617
Iteration 2/25 | Loss: 0.00099698
Iteration 3/25 | Loss: 0.00091297
Iteration 4/25 | Loss: 0.00091297
Iteration 5/25 | Loss: 0.00091297
Iteration 6/25 | Loss: 0.00091297
Iteration 7/25 | Loss: 0.00091297
Iteration 8/25 | Loss: 0.00091297
Iteration 9/25 | Loss: 0.00091297
Iteration 10/25 | Loss: 0.00091297
Iteration 11/25 | Loss: 0.00091297
Iteration 12/25 | Loss: 0.00091297
Iteration 13/25 | Loss: 0.00091297
Iteration 14/25 | Loss: 0.00091297
Iteration 15/25 | Loss: 0.00091297
Iteration 16/25 | Loss: 0.00091297
Iteration 17/25 | Loss: 0.00091297
Iteration 18/25 | Loss: 0.00091297
Iteration 19/25 | Loss: 0.00091297
Iteration 20/25 | Loss: 0.00091297
Iteration 21/25 | Loss: 0.00091297
Iteration 22/25 | Loss: 0.00091297
Iteration 23/25 | Loss: 0.00091297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000912967137992382, 0.000912967137992382, 0.000912967137992382, 0.000912967137992382, 0.000912967137992382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000912967137992382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091297
Iteration 2/1000 | Loss: 0.00032663
Iteration 3/1000 | Loss: 0.00010271
Iteration 4/1000 | Loss: 0.00015989
Iteration 5/1000 | Loss: 0.00014168
Iteration 6/1000 | Loss: 0.00013007
Iteration 7/1000 | Loss: 0.00005352
Iteration 8/1000 | Loss: 0.00003461
Iteration 9/1000 | Loss: 0.00003763
Iteration 10/1000 | Loss: 0.00002865
Iteration 11/1000 | Loss: 0.00004018
Iteration 12/1000 | Loss: 0.00003000
Iteration 13/1000 | Loss: 0.00004334
Iteration 14/1000 | Loss: 0.00002409
Iteration 15/1000 | Loss: 0.00002400
Iteration 16/1000 | Loss: 0.00002422
Iteration 17/1000 | Loss: 0.00002257
Iteration 18/1000 | Loss: 0.00002244
Iteration 19/1000 | Loss: 0.00004089
Iteration 20/1000 | Loss: 0.00002198
Iteration 21/1000 | Loss: 0.00002241
Iteration 22/1000 | Loss: 0.00002182
Iteration 23/1000 | Loss: 0.00002182
Iteration 24/1000 | Loss: 0.00002182
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00002181
Iteration 27/1000 | Loss: 0.00002181
Iteration 28/1000 | Loss: 0.00002181
Iteration 29/1000 | Loss: 0.00002181
Iteration 30/1000 | Loss: 0.00002181
Iteration 31/1000 | Loss: 0.00004411
Iteration 32/1000 | Loss: 0.00008819
Iteration 33/1000 | Loss: 0.00006620
Iteration 34/1000 | Loss: 0.00002821
Iteration 35/1000 | Loss: 0.00002476
Iteration 36/1000 | Loss: 0.00002308
Iteration 37/1000 | Loss: 0.00003632
Iteration 38/1000 | Loss: 0.00002099
Iteration 39/1000 | Loss: 0.00003066
Iteration 40/1000 | Loss: 0.00013067
Iteration 41/1000 | Loss: 0.00002882
Iteration 42/1000 | Loss: 0.00002021
Iteration 43/1000 | Loss: 0.00002222
Iteration 44/1000 | Loss: 0.00005697
Iteration 45/1000 | Loss: 0.00002070
Iteration 46/1000 | Loss: 0.00002704
Iteration 47/1000 | Loss: 0.00002550
Iteration 48/1000 | Loss: 0.00002042
Iteration 49/1000 | Loss: 0.00002088
Iteration 50/1000 | Loss: 0.00042015
Iteration 51/1000 | Loss: 0.00014461
Iteration 52/1000 | Loss: 0.00027204
Iteration 53/1000 | Loss: 0.00036374
Iteration 54/1000 | Loss: 0.00022130
Iteration 55/1000 | Loss: 0.00025795
Iteration 56/1000 | Loss: 0.00020913
Iteration 57/1000 | Loss: 0.00029498
Iteration 58/1000 | Loss: 0.00015913
Iteration 59/1000 | Loss: 0.00003060
Iteration 60/1000 | Loss: 0.00005532
Iteration 61/1000 | Loss: 0.00002369
Iteration 62/1000 | Loss: 0.00002622
Iteration 63/1000 | Loss: 0.00004156
Iteration 64/1000 | Loss: 0.00030851
Iteration 65/1000 | Loss: 0.00011831
Iteration 66/1000 | Loss: 0.00018993
Iteration 67/1000 | Loss: 0.00012004
Iteration 68/1000 | Loss: 0.00011193
Iteration 69/1000 | Loss: 0.00003113
Iteration 70/1000 | Loss: 0.00008573
Iteration 71/1000 | Loss: 0.00003309
Iteration 72/1000 | Loss: 0.00004040
Iteration 73/1000 | Loss: 0.00031884
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002706
Iteration 76/1000 | Loss: 0.00002276
Iteration 77/1000 | Loss: 0.00003613
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00002164
Iteration 80/1000 | Loss: 0.00002388
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001794
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00001790
Iteration 86/1000 | Loss: 0.00001712
Iteration 87/1000 | Loss: 0.00001711
Iteration 88/1000 | Loss: 0.00001711
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001703
Iteration 94/1000 | Loss: 0.00001703
Iteration 95/1000 | Loss: 0.00001703
Iteration 96/1000 | Loss: 0.00001703
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001701
Iteration 105/1000 | Loss: 0.00001701
Iteration 106/1000 | Loss: 0.00001700
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00001709
Iteration 122/1000 | Loss: 0.00002139
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001681
Iteration 125/1000 | Loss: 0.00001681
Iteration 126/1000 | Loss: 0.00001681
Iteration 127/1000 | Loss: 0.00001681
Iteration 128/1000 | Loss: 0.00001681
Iteration 129/1000 | Loss: 0.00001681
Iteration 130/1000 | Loss: 0.00001681
Iteration 131/1000 | Loss: 0.00001681
Iteration 132/1000 | Loss: 0.00001680
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001907
Iteration 139/1000 | Loss: 0.00001886
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001678
Iteration 142/1000 | Loss: 0.00001677
Iteration 143/1000 | Loss: 0.00001677
Iteration 144/1000 | Loss: 0.00001677
Iteration 145/1000 | Loss: 0.00001677
Iteration 146/1000 | Loss: 0.00001677
Iteration 147/1000 | Loss: 0.00001677
Iteration 148/1000 | Loss: 0.00001677
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001677
Iteration 151/1000 | Loss: 0.00001677
Iteration 152/1000 | Loss: 0.00001677
Iteration 153/1000 | Loss: 0.00001677
Iteration 154/1000 | Loss: 0.00001677
Iteration 155/1000 | Loss: 0.00001677
Iteration 156/1000 | Loss: 0.00001677
Iteration 157/1000 | Loss: 0.00001677
Iteration 158/1000 | Loss: 0.00001677
Iteration 159/1000 | Loss: 0.00001677
Iteration 160/1000 | Loss: 0.00001677
Iteration 161/1000 | Loss: 0.00001677
Iteration 162/1000 | Loss: 0.00001677
Iteration 163/1000 | Loss: 0.00001677
Iteration 164/1000 | Loss: 0.00001677
Iteration 165/1000 | Loss: 0.00001677
Iteration 166/1000 | Loss: 0.00001677
Iteration 167/1000 | Loss: 0.00001677
Iteration 168/1000 | Loss: 0.00001677
Iteration 169/1000 | Loss: 0.00001677
Iteration 170/1000 | Loss: 0.00001677
Iteration 171/1000 | Loss: 0.00001677
Iteration 172/1000 | Loss: 0.00001677
Iteration 173/1000 | Loss: 0.00001677
Iteration 174/1000 | Loss: 0.00001677
Iteration 175/1000 | Loss: 0.00001677
Iteration 176/1000 | Loss: 0.00001677
Iteration 177/1000 | Loss: 0.00001677
Iteration 178/1000 | Loss: 0.00001677
Iteration 179/1000 | Loss: 0.00001677
Iteration 180/1000 | Loss: 0.00001677
Iteration 181/1000 | Loss: 0.00001677
Iteration 182/1000 | Loss: 0.00001677
Iteration 183/1000 | Loss: 0.00001677
Iteration 184/1000 | Loss: 0.00001677
Iteration 185/1000 | Loss: 0.00001677
Iteration 186/1000 | Loss: 0.00001677
Iteration 187/1000 | Loss: 0.00001677
Iteration 188/1000 | Loss: 0.00001677
Iteration 189/1000 | Loss: 0.00001677
Iteration 190/1000 | Loss: 0.00001677
Iteration 191/1000 | Loss: 0.00001677
Iteration 192/1000 | Loss: 0.00001677
Iteration 193/1000 | Loss: 0.00001677
Iteration 194/1000 | Loss: 0.00001677
Iteration 195/1000 | Loss: 0.00001677
Iteration 196/1000 | Loss: 0.00001677
Iteration 197/1000 | Loss: 0.00001677
Iteration 198/1000 | Loss: 0.00001677
Iteration 199/1000 | Loss: 0.00001677
Iteration 200/1000 | Loss: 0.00001677
Iteration 201/1000 | Loss: 0.00001677
Iteration 202/1000 | Loss: 0.00001677
Iteration 203/1000 | Loss: 0.00001677
Iteration 204/1000 | Loss: 0.00001677
Iteration 205/1000 | Loss: 0.00001677
Iteration 206/1000 | Loss: 0.00001677
Iteration 207/1000 | Loss: 0.00001677
Iteration 208/1000 | Loss: 0.00001677
Iteration 209/1000 | Loss: 0.00001677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.6771615264588036e-05, 1.6771615264588036e-05, 1.6771615264588036e-05, 1.6771615264588036e-05, 1.6771615264588036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6771615264588036e-05

Optimization complete. Final v2v error: 3.4022934436798096 mm

Highest mean error: 5.7689008712768555 mm for frame 191

Lowest mean error: 2.9214932918548584 mm for frame 4

Saving results

Total time: 170.8116946220398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540087
Iteration 2/25 | Loss: 0.00156306
Iteration 3/25 | Loss: 0.00135827
Iteration 4/25 | Loss: 0.00133761
Iteration 5/25 | Loss: 0.00133399
Iteration 6/25 | Loss: 0.00133360
Iteration 7/25 | Loss: 0.00133360
Iteration 8/25 | Loss: 0.00133360
Iteration 9/25 | Loss: 0.00133360
Iteration 10/25 | Loss: 0.00133360
Iteration 11/25 | Loss: 0.00133360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001333601656369865, 0.001333601656369865, 0.001333601656369865, 0.001333601656369865, 0.001333601656369865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001333601656369865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44342458
Iteration 2/25 | Loss: 0.00075483
Iteration 3/25 | Loss: 0.00075483
Iteration 4/25 | Loss: 0.00075483
Iteration 5/25 | Loss: 0.00075483
Iteration 6/25 | Loss: 0.00075483
Iteration 7/25 | Loss: 0.00075482
Iteration 8/25 | Loss: 0.00075482
Iteration 9/25 | Loss: 0.00075482
Iteration 10/25 | Loss: 0.00075482
Iteration 11/25 | Loss: 0.00075482
Iteration 12/25 | Loss: 0.00075482
Iteration 13/25 | Loss: 0.00075482
Iteration 14/25 | Loss: 0.00075482
Iteration 15/25 | Loss: 0.00075482
Iteration 16/25 | Loss: 0.00075482
Iteration 17/25 | Loss: 0.00075482
Iteration 18/25 | Loss: 0.00075482
Iteration 19/25 | Loss: 0.00075482
Iteration 20/25 | Loss: 0.00075482
Iteration 21/25 | Loss: 0.00075482
Iteration 22/25 | Loss: 0.00075482
Iteration 23/25 | Loss: 0.00075482
Iteration 24/25 | Loss: 0.00075482
Iteration 25/25 | Loss: 0.00075482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075482
Iteration 2/1000 | Loss: 0.00005439
Iteration 3/1000 | Loss: 0.00003350
Iteration 4/1000 | Loss: 0.00002850
Iteration 5/1000 | Loss: 0.00002718
Iteration 6/1000 | Loss: 0.00002625
Iteration 7/1000 | Loss: 0.00002563
Iteration 8/1000 | Loss: 0.00002507
Iteration 9/1000 | Loss: 0.00002463
Iteration 10/1000 | Loss: 0.00002432
Iteration 11/1000 | Loss: 0.00002406
Iteration 12/1000 | Loss: 0.00002395
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002367
Iteration 15/1000 | Loss: 0.00002360
Iteration 16/1000 | Loss: 0.00002360
Iteration 17/1000 | Loss: 0.00002353
Iteration 18/1000 | Loss: 0.00002352
Iteration 19/1000 | Loss: 0.00002351
Iteration 20/1000 | Loss: 0.00002349
Iteration 21/1000 | Loss: 0.00002349
Iteration 22/1000 | Loss: 0.00002348
Iteration 23/1000 | Loss: 0.00002346
Iteration 24/1000 | Loss: 0.00002341
Iteration 25/1000 | Loss: 0.00002339
Iteration 26/1000 | Loss: 0.00002339
Iteration 27/1000 | Loss: 0.00002339
Iteration 28/1000 | Loss: 0.00002339
Iteration 29/1000 | Loss: 0.00002338
Iteration 30/1000 | Loss: 0.00002338
Iteration 31/1000 | Loss: 0.00002338
Iteration 32/1000 | Loss: 0.00002338
Iteration 33/1000 | Loss: 0.00002338
Iteration 34/1000 | Loss: 0.00002338
Iteration 35/1000 | Loss: 0.00002338
Iteration 36/1000 | Loss: 0.00002338
Iteration 37/1000 | Loss: 0.00002338
Iteration 38/1000 | Loss: 0.00002337
Iteration 39/1000 | Loss: 0.00002337
Iteration 40/1000 | Loss: 0.00002337
Iteration 41/1000 | Loss: 0.00002337
Iteration 42/1000 | Loss: 0.00002337
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002337
Iteration 45/1000 | Loss: 0.00002337
Iteration 46/1000 | Loss: 0.00002337
Iteration 47/1000 | Loss: 0.00002337
Iteration 48/1000 | Loss: 0.00002337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [2.3374568627332337e-05, 2.3374568627332337e-05, 2.3374568627332337e-05, 2.3374568627332337e-05, 2.3374568627332337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3374568627332337e-05

Optimization complete. Final v2v error: 4.053480625152588 mm

Highest mean error: 4.288638591766357 mm for frame 56

Lowest mean error: 3.5866270065307617 mm for frame 12

Saving results

Total time: 31.181113243103027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988101
Iteration 2/25 | Loss: 0.00988101
Iteration 3/25 | Loss: 0.00207387
Iteration 4/25 | Loss: 0.00164351
Iteration 5/25 | Loss: 0.00159536
Iteration 6/25 | Loss: 0.00160490
Iteration 7/25 | Loss: 0.00165712
Iteration 8/25 | Loss: 0.00164211
Iteration 9/25 | Loss: 0.00157448
Iteration 10/25 | Loss: 0.00148406
Iteration 11/25 | Loss: 0.00144761
Iteration 12/25 | Loss: 0.00144317
Iteration 13/25 | Loss: 0.00144281
Iteration 14/25 | Loss: 0.00143877
Iteration 15/25 | Loss: 0.00143285
Iteration 16/25 | Loss: 0.00143507
Iteration 17/25 | Loss: 0.00143690
Iteration 18/25 | Loss: 0.00143580
Iteration 19/25 | Loss: 0.00143595
Iteration 20/25 | Loss: 0.00143279
Iteration 21/25 | Loss: 0.00143113
Iteration 22/25 | Loss: 0.00143192
Iteration 23/25 | Loss: 0.00143323
Iteration 24/25 | Loss: 0.00143103
Iteration 25/25 | Loss: 0.00143292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94798589
Iteration 2/25 | Loss: 0.00136966
Iteration 3/25 | Loss: 0.00136965
Iteration 4/25 | Loss: 0.00136965
Iteration 5/25 | Loss: 0.00136965
Iteration 6/25 | Loss: 0.00136964
Iteration 7/25 | Loss: 0.00136964
Iteration 8/25 | Loss: 0.00136964
Iteration 9/25 | Loss: 0.00136964
Iteration 10/25 | Loss: 0.00136964
Iteration 11/25 | Loss: 0.00136964
Iteration 12/25 | Loss: 0.00136964
Iteration 13/25 | Loss: 0.00136964
Iteration 14/25 | Loss: 0.00136964
Iteration 15/25 | Loss: 0.00136964
Iteration 16/25 | Loss: 0.00136964
Iteration 17/25 | Loss: 0.00136964
Iteration 18/25 | Loss: 0.00136964
Iteration 19/25 | Loss: 0.00136964
Iteration 20/25 | Loss: 0.00136964
Iteration 21/25 | Loss: 0.00136964
Iteration 22/25 | Loss: 0.00136964
Iteration 23/25 | Loss: 0.00136964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001369642443023622, 0.001369642443023622, 0.001369642443023622, 0.001369642443023622, 0.001369642443023622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001369642443023622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136964
Iteration 2/1000 | Loss: 0.00042978
Iteration 3/1000 | Loss: 0.00011654
Iteration 4/1000 | Loss: 0.00009553
Iteration 5/1000 | Loss: 0.00006332
Iteration 6/1000 | Loss: 0.00008341
Iteration 7/1000 | Loss: 0.00007599
Iteration 8/1000 | Loss: 0.00007939
Iteration 9/1000 | Loss: 0.00005762
Iteration 10/1000 | Loss: 0.00024055
Iteration 11/1000 | Loss: 0.00007756
Iteration 12/1000 | Loss: 0.00007836
Iteration 13/1000 | Loss: 0.00008198
Iteration 14/1000 | Loss: 0.00007627
Iteration 15/1000 | Loss: 0.00007858
Iteration 16/1000 | Loss: 0.00007623
Iteration 17/1000 | Loss: 0.00006940
Iteration 18/1000 | Loss: 0.00006688
Iteration 19/1000 | Loss: 0.00008147
Iteration 20/1000 | Loss: 0.00007704
Iteration 21/1000 | Loss: 0.00005147
Iteration 22/1000 | Loss: 0.00005502
Iteration 23/1000 | Loss: 0.00006185
Iteration 24/1000 | Loss: 0.00008185
Iteration 25/1000 | Loss: 0.00008375
Iteration 26/1000 | Loss: 0.00006234
Iteration 27/1000 | Loss: 0.00006101
Iteration 28/1000 | Loss: 0.00006480
Iteration 29/1000 | Loss: 0.00021606
Iteration 30/1000 | Loss: 0.00006582
Iteration 31/1000 | Loss: 0.00004518
Iteration 32/1000 | Loss: 0.00005929
Iteration 33/1000 | Loss: 0.00019588
Iteration 34/1000 | Loss: 0.00005873
Iteration 35/1000 | Loss: 0.00006044
Iteration 36/1000 | Loss: 0.00006207
Iteration 37/1000 | Loss: 0.00004801
Iteration 38/1000 | Loss: 0.00007680
Iteration 39/1000 | Loss: 0.00008337
Iteration 40/1000 | Loss: 0.00009426
Iteration 41/1000 | Loss: 0.00007763
Iteration 42/1000 | Loss: 0.00007961
Iteration 43/1000 | Loss: 0.00007679
Iteration 44/1000 | Loss: 0.00007340
Iteration 45/1000 | Loss: 0.00006279
Iteration 46/1000 | Loss: 0.00004932
Iteration 47/1000 | Loss: 0.00003732
Iteration 48/1000 | Loss: 0.00004802
Iteration 49/1000 | Loss: 0.00007524
Iteration 50/1000 | Loss: 0.00005813
Iteration 51/1000 | Loss: 0.00006343
Iteration 52/1000 | Loss: 0.00005738
Iteration 53/1000 | Loss: 0.00007102
Iteration 54/1000 | Loss: 0.00005505
Iteration 55/1000 | Loss: 0.00006349
Iteration 56/1000 | Loss: 0.00005073
Iteration 57/1000 | Loss: 0.00005000
Iteration 58/1000 | Loss: 0.00005483
Iteration 59/1000 | Loss: 0.00005042
Iteration 60/1000 | Loss: 0.00005847
Iteration 61/1000 | Loss: 0.00005114
Iteration 62/1000 | Loss: 0.00006190
Iteration 63/1000 | Loss: 0.00005152
Iteration 64/1000 | Loss: 0.00005758
Iteration 65/1000 | Loss: 0.00005539
Iteration 66/1000 | Loss: 0.00006514
Iteration 67/1000 | Loss: 0.00004694
Iteration 68/1000 | Loss: 0.00017551
Iteration 69/1000 | Loss: 0.00014573
Iteration 70/1000 | Loss: 0.00004039
Iteration 71/1000 | Loss: 0.00004984
Iteration 72/1000 | Loss: 0.00003516
Iteration 73/1000 | Loss: 0.00003965
Iteration 74/1000 | Loss: 0.00003901
Iteration 75/1000 | Loss: 0.00003608
Iteration 76/1000 | Loss: 0.00014326
Iteration 77/1000 | Loss: 0.00004392
Iteration 78/1000 | Loss: 0.00004137
Iteration 79/1000 | Loss: 0.00004694
Iteration 80/1000 | Loss: 0.00004198
Iteration 81/1000 | Loss: 0.00002947
Iteration 82/1000 | Loss: 0.00003345
Iteration 83/1000 | Loss: 0.00005017
Iteration 84/1000 | Loss: 0.00003492
Iteration 85/1000 | Loss: 0.00003236
Iteration 86/1000 | Loss: 0.00003759
Iteration 87/1000 | Loss: 0.00003111
Iteration 88/1000 | Loss: 0.00003123
Iteration 89/1000 | Loss: 0.00005002
Iteration 90/1000 | Loss: 0.00003658
Iteration 91/1000 | Loss: 0.00003095
Iteration 92/1000 | Loss: 0.00004782
Iteration 93/1000 | Loss: 0.00004394
Iteration 94/1000 | Loss: 0.00003353
Iteration 95/1000 | Loss: 0.00003057
Iteration 96/1000 | Loss: 0.00002814
Iteration 97/1000 | Loss: 0.00002692
Iteration 98/1000 | Loss: 0.00002651
Iteration 99/1000 | Loss: 0.00002623
Iteration 100/1000 | Loss: 0.00002598
Iteration 101/1000 | Loss: 0.00002577
Iteration 102/1000 | Loss: 0.00002577
Iteration 103/1000 | Loss: 0.00002576
Iteration 104/1000 | Loss: 0.00002574
Iteration 105/1000 | Loss: 0.00002574
Iteration 106/1000 | Loss: 0.00002561
Iteration 107/1000 | Loss: 0.00002542
Iteration 108/1000 | Loss: 0.00002524
Iteration 109/1000 | Loss: 0.00004015
Iteration 110/1000 | Loss: 0.00003001
Iteration 111/1000 | Loss: 0.00002808
Iteration 112/1000 | Loss: 0.00003604
Iteration 113/1000 | Loss: 0.00003377
Iteration 114/1000 | Loss: 0.00003383
Iteration 115/1000 | Loss: 0.00002754
Iteration 116/1000 | Loss: 0.00002739
Iteration 117/1000 | Loss: 0.00002511
Iteration 118/1000 | Loss: 0.00002493
Iteration 119/1000 | Loss: 0.00002491
Iteration 120/1000 | Loss: 0.00002487
Iteration 121/1000 | Loss: 0.00002474
Iteration 122/1000 | Loss: 0.00002473
Iteration 123/1000 | Loss: 0.00002472
Iteration 124/1000 | Loss: 0.00002467
Iteration 125/1000 | Loss: 0.00002467
Iteration 126/1000 | Loss: 0.00002465
Iteration 127/1000 | Loss: 0.00002460
Iteration 128/1000 | Loss: 0.00002460
Iteration 129/1000 | Loss: 0.00002458
Iteration 130/1000 | Loss: 0.00002457
Iteration 131/1000 | Loss: 0.00002457
Iteration 132/1000 | Loss: 0.00002457
Iteration 133/1000 | Loss: 0.00002456
Iteration 134/1000 | Loss: 0.00002456
Iteration 135/1000 | Loss: 0.00002456
Iteration 136/1000 | Loss: 0.00002456
Iteration 137/1000 | Loss: 0.00002456
Iteration 138/1000 | Loss: 0.00002455
Iteration 139/1000 | Loss: 0.00002455
Iteration 140/1000 | Loss: 0.00002455
Iteration 141/1000 | Loss: 0.00002455
Iteration 142/1000 | Loss: 0.00002454
Iteration 143/1000 | Loss: 0.00002454
Iteration 144/1000 | Loss: 0.00002453
Iteration 145/1000 | Loss: 0.00002453
Iteration 146/1000 | Loss: 0.00002453
Iteration 147/1000 | Loss: 0.00002453
Iteration 148/1000 | Loss: 0.00002453
Iteration 149/1000 | Loss: 0.00002453
Iteration 150/1000 | Loss: 0.00002453
Iteration 151/1000 | Loss: 0.00002453
Iteration 152/1000 | Loss: 0.00002453
Iteration 153/1000 | Loss: 0.00002453
Iteration 154/1000 | Loss: 0.00002453
Iteration 155/1000 | Loss: 0.00002453
Iteration 156/1000 | Loss: 0.00002453
Iteration 157/1000 | Loss: 0.00002452
Iteration 158/1000 | Loss: 0.00002452
Iteration 159/1000 | Loss: 0.00002452
Iteration 160/1000 | Loss: 0.00002452
Iteration 161/1000 | Loss: 0.00002452
Iteration 162/1000 | Loss: 0.00002452
Iteration 163/1000 | Loss: 0.00002452
Iteration 164/1000 | Loss: 0.00002452
Iteration 165/1000 | Loss: 0.00002452
Iteration 166/1000 | Loss: 0.00002452
Iteration 167/1000 | Loss: 0.00002451
Iteration 168/1000 | Loss: 0.00002451
Iteration 169/1000 | Loss: 0.00002451
Iteration 170/1000 | Loss: 0.00002451
Iteration 171/1000 | Loss: 0.00002451
Iteration 172/1000 | Loss: 0.00002451
Iteration 173/1000 | Loss: 0.00002451
Iteration 174/1000 | Loss: 0.00002451
Iteration 175/1000 | Loss: 0.00002451
Iteration 176/1000 | Loss: 0.00002451
Iteration 177/1000 | Loss: 0.00002451
Iteration 178/1000 | Loss: 0.00002450
Iteration 179/1000 | Loss: 0.00002450
Iteration 180/1000 | Loss: 0.00002450
Iteration 181/1000 | Loss: 0.00002450
Iteration 182/1000 | Loss: 0.00002450
Iteration 183/1000 | Loss: 0.00002450
Iteration 184/1000 | Loss: 0.00002450
Iteration 185/1000 | Loss: 0.00002450
Iteration 186/1000 | Loss: 0.00002450
Iteration 187/1000 | Loss: 0.00002450
Iteration 188/1000 | Loss: 0.00002450
Iteration 189/1000 | Loss: 0.00002450
Iteration 190/1000 | Loss: 0.00002450
Iteration 191/1000 | Loss: 0.00002450
Iteration 192/1000 | Loss: 0.00002449
Iteration 193/1000 | Loss: 0.00002449
Iteration 194/1000 | Loss: 0.00002449
Iteration 195/1000 | Loss: 0.00002449
Iteration 196/1000 | Loss: 0.00002449
Iteration 197/1000 | Loss: 0.00002449
Iteration 198/1000 | Loss: 0.00002449
Iteration 199/1000 | Loss: 0.00002449
Iteration 200/1000 | Loss: 0.00002449
Iteration 201/1000 | Loss: 0.00002448
Iteration 202/1000 | Loss: 0.00002448
Iteration 203/1000 | Loss: 0.00002448
Iteration 204/1000 | Loss: 0.00002448
Iteration 205/1000 | Loss: 0.00002448
Iteration 206/1000 | Loss: 0.00002448
Iteration 207/1000 | Loss: 0.00002448
Iteration 208/1000 | Loss: 0.00002448
Iteration 209/1000 | Loss: 0.00002448
Iteration 210/1000 | Loss: 0.00002448
Iteration 211/1000 | Loss: 0.00002447
Iteration 212/1000 | Loss: 0.00002447
Iteration 213/1000 | Loss: 0.00002447
Iteration 214/1000 | Loss: 0.00002447
Iteration 215/1000 | Loss: 0.00002447
Iteration 216/1000 | Loss: 0.00002447
Iteration 217/1000 | Loss: 0.00002447
Iteration 218/1000 | Loss: 0.00002447
Iteration 219/1000 | Loss: 0.00002447
Iteration 220/1000 | Loss: 0.00002447
Iteration 221/1000 | Loss: 0.00002447
Iteration 222/1000 | Loss: 0.00002447
Iteration 223/1000 | Loss: 0.00002447
Iteration 224/1000 | Loss: 0.00002447
Iteration 225/1000 | Loss: 0.00002447
Iteration 226/1000 | Loss: 0.00002447
Iteration 227/1000 | Loss: 0.00002446
Iteration 228/1000 | Loss: 0.00002446
Iteration 229/1000 | Loss: 0.00002446
Iteration 230/1000 | Loss: 0.00002446
Iteration 231/1000 | Loss: 0.00002446
Iteration 232/1000 | Loss: 0.00002446
Iteration 233/1000 | Loss: 0.00002446
Iteration 234/1000 | Loss: 0.00002446
Iteration 235/1000 | Loss: 0.00002446
Iteration 236/1000 | Loss: 0.00002446
Iteration 237/1000 | Loss: 0.00002446
Iteration 238/1000 | Loss: 0.00002446
Iteration 239/1000 | Loss: 0.00002446
Iteration 240/1000 | Loss: 0.00002446
Iteration 241/1000 | Loss: 0.00002446
Iteration 242/1000 | Loss: 0.00002446
Iteration 243/1000 | Loss: 0.00002446
Iteration 244/1000 | Loss: 0.00002446
Iteration 245/1000 | Loss: 0.00002446
Iteration 246/1000 | Loss: 0.00002445
Iteration 247/1000 | Loss: 0.00002445
Iteration 248/1000 | Loss: 0.00002445
Iteration 249/1000 | Loss: 0.00002445
Iteration 250/1000 | Loss: 0.00002445
Iteration 251/1000 | Loss: 0.00002445
Iteration 252/1000 | Loss: 0.00002445
Iteration 253/1000 | Loss: 0.00002445
Iteration 254/1000 | Loss: 0.00002444
Iteration 255/1000 | Loss: 0.00002444
Iteration 256/1000 | Loss: 0.00002444
Iteration 257/1000 | Loss: 0.00002444
Iteration 258/1000 | Loss: 0.00002444
Iteration 259/1000 | Loss: 0.00002444
Iteration 260/1000 | Loss: 0.00002444
Iteration 261/1000 | Loss: 0.00002444
Iteration 262/1000 | Loss: 0.00002444
Iteration 263/1000 | Loss: 0.00002444
Iteration 264/1000 | Loss: 0.00002444
Iteration 265/1000 | Loss: 0.00002444
Iteration 266/1000 | Loss: 0.00002444
Iteration 267/1000 | Loss: 0.00002444
Iteration 268/1000 | Loss: 0.00002444
Iteration 269/1000 | Loss: 0.00002444
Iteration 270/1000 | Loss: 0.00002444
Iteration 271/1000 | Loss: 0.00002444
Iteration 272/1000 | Loss: 0.00002444
Iteration 273/1000 | Loss: 0.00002443
Iteration 274/1000 | Loss: 0.00002443
Iteration 275/1000 | Loss: 0.00002443
Iteration 276/1000 | Loss: 0.00002443
Iteration 277/1000 | Loss: 0.00002443
Iteration 278/1000 | Loss: 0.00002443
Iteration 279/1000 | Loss: 0.00002443
Iteration 280/1000 | Loss: 0.00002443
Iteration 281/1000 | Loss: 0.00002443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.4433969883830287e-05, 2.4433969883830287e-05, 2.4433969883830287e-05, 2.4433969883830287e-05, 2.4433969883830287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4433969883830287e-05

Optimization complete. Final v2v error: 3.9395830631256104 mm

Highest mean error: 6.3300580978393555 mm for frame 111

Lowest mean error: 3.666431427001953 mm for frame 2

Saving results

Total time: 252.96494150161743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492824
Iteration 2/25 | Loss: 0.00141937
Iteration 3/25 | Loss: 0.00129838
Iteration 4/25 | Loss: 0.00129132
Iteration 5/25 | Loss: 0.00128983
Iteration 6/25 | Loss: 0.00128983
Iteration 7/25 | Loss: 0.00128983
Iteration 8/25 | Loss: 0.00128983
Iteration 9/25 | Loss: 0.00128983
Iteration 10/25 | Loss: 0.00128983
Iteration 11/25 | Loss: 0.00128983
Iteration 12/25 | Loss: 0.00128983
Iteration 13/25 | Loss: 0.00128983
Iteration 14/25 | Loss: 0.00128983
Iteration 15/25 | Loss: 0.00128983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012898276327177882, 0.0012898276327177882, 0.0012898276327177882, 0.0012898276327177882, 0.0012898276327177882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012898276327177882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.82281685
Iteration 2/25 | Loss: 0.00074876
Iteration 3/25 | Loss: 0.00074866
Iteration 4/25 | Loss: 0.00074866
Iteration 5/25 | Loss: 0.00074866
Iteration 6/25 | Loss: 0.00074866
Iteration 7/25 | Loss: 0.00074866
Iteration 8/25 | Loss: 0.00074866
Iteration 9/25 | Loss: 0.00074866
Iteration 10/25 | Loss: 0.00074866
Iteration 11/25 | Loss: 0.00074866
Iteration 12/25 | Loss: 0.00074866
Iteration 13/25 | Loss: 0.00074866
Iteration 14/25 | Loss: 0.00074866
Iteration 15/25 | Loss: 0.00074866
Iteration 16/25 | Loss: 0.00074866
Iteration 17/25 | Loss: 0.00074866
Iteration 18/25 | Loss: 0.00074866
Iteration 19/25 | Loss: 0.00074866
Iteration 20/25 | Loss: 0.00074866
Iteration 21/25 | Loss: 0.00074866
Iteration 22/25 | Loss: 0.00074866
Iteration 23/25 | Loss: 0.00074866
Iteration 24/25 | Loss: 0.00074866
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007486551767215133, 0.0007486551767215133, 0.0007486551767215133, 0.0007486551767215133, 0.0007486551767215133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007486551767215133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074866
Iteration 2/1000 | Loss: 0.00003498
Iteration 3/1000 | Loss: 0.00002390
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00002130
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00002027
Iteration 8/1000 | Loss: 0.00001997
Iteration 9/1000 | Loss: 0.00001966
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001925
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00001897
Iteration 14/1000 | Loss: 0.00001889
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001881
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001866
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001865
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001864
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001851
Iteration 49/1000 | Loss: 0.00001851
Iteration 50/1000 | Loss: 0.00001851
Iteration 51/1000 | Loss: 0.00001851
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001849
Iteration 54/1000 | Loss: 0.00001849
Iteration 55/1000 | Loss: 0.00001849
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001849
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001848
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001848
Iteration 64/1000 | Loss: 0.00001848
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001847
Iteration 67/1000 | Loss: 0.00001846
Iteration 68/1000 | Loss: 0.00001846
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001845
Iteration 71/1000 | Loss: 0.00001845
Iteration 72/1000 | Loss: 0.00001845
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001844
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001843
Iteration 86/1000 | Loss: 0.00001843
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001843
Iteration 90/1000 | Loss: 0.00001843
Iteration 91/1000 | Loss: 0.00001843
Iteration 92/1000 | Loss: 0.00001843
Iteration 93/1000 | Loss: 0.00001843
Iteration 94/1000 | Loss: 0.00001843
Iteration 95/1000 | Loss: 0.00001843
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001843
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001843
Iteration 106/1000 | Loss: 0.00001843
Iteration 107/1000 | Loss: 0.00001843
Iteration 108/1000 | Loss: 0.00001843
Iteration 109/1000 | Loss: 0.00001843
Iteration 110/1000 | Loss: 0.00001843
Iteration 111/1000 | Loss: 0.00001843
Iteration 112/1000 | Loss: 0.00001843
Iteration 113/1000 | Loss: 0.00001843
Iteration 114/1000 | Loss: 0.00001843
Iteration 115/1000 | Loss: 0.00001843
Iteration 116/1000 | Loss: 0.00001843
Iteration 117/1000 | Loss: 0.00001843
Iteration 118/1000 | Loss: 0.00001843
Iteration 119/1000 | Loss: 0.00001843
Iteration 120/1000 | Loss: 0.00001843
Iteration 121/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.8425442249281332e-05, 1.8425442249281332e-05, 1.8425442249281332e-05, 1.8425442249281332e-05, 1.8425442249281332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8425442249281332e-05

Optimization complete. Final v2v error: 3.587139129638672 mm

Highest mean error: 4.096798419952393 mm for frame 39

Lowest mean error: 3.2164175510406494 mm for frame 0

Saving results

Total time: 38.213624000549316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903438
Iteration 2/25 | Loss: 0.00186801
Iteration 3/25 | Loss: 0.00150582
Iteration 4/25 | Loss: 0.00145590
Iteration 5/25 | Loss: 0.00146497
Iteration 6/25 | Loss: 0.00145861
Iteration 7/25 | Loss: 0.00141664
Iteration 8/25 | Loss: 0.00140738
Iteration 9/25 | Loss: 0.00141499
Iteration 10/25 | Loss: 0.00140327
Iteration 11/25 | Loss: 0.00141230
Iteration 12/25 | Loss: 0.00140105
Iteration 13/25 | Loss: 0.00141449
Iteration 14/25 | Loss: 0.00139930
Iteration 15/25 | Loss: 0.00138390
Iteration 16/25 | Loss: 0.00137929
Iteration 17/25 | Loss: 0.00137569
Iteration 18/25 | Loss: 0.00137505
Iteration 19/25 | Loss: 0.00138223
Iteration 20/25 | Loss: 0.00137348
Iteration 21/25 | Loss: 0.00137271
Iteration 22/25 | Loss: 0.00137718
Iteration 23/25 | Loss: 0.00136567
Iteration 24/25 | Loss: 0.00136140
Iteration 25/25 | Loss: 0.00135881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37939143
Iteration 2/25 | Loss: 0.00102036
Iteration 3/25 | Loss: 0.00102022
Iteration 4/25 | Loss: 0.00102022
Iteration 5/25 | Loss: 0.00102021
Iteration 6/25 | Loss: 0.00102021
Iteration 7/25 | Loss: 0.00102021
Iteration 8/25 | Loss: 0.00102021
Iteration 9/25 | Loss: 0.00102021
Iteration 10/25 | Loss: 0.00102021
Iteration 11/25 | Loss: 0.00102021
Iteration 12/25 | Loss: 0.00102021
Iteration 13/25 | Loss: 0.00102021
Iteration 14/25 | Loss: 0.00102021
Iteration 15/25 | Loss: 0.00102021
Iteration 16/25 | Loss: 0.00102021
Iteration 17/25 | Loss: 0.00102021
Iteration 18/25 | Loss: 0.00102021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001020213239826262, 0.001020213239826262, 0.001020213239826262, 0.001020213239826262, 0.001020213239826262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001020213239826262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102021
Iteration 2/1000 | Loss: 0.00375361
Iteration 3/1000 | Loss: 0.00805944
Iteration 4/1000 | Loss: 0.00172365
Iteration 5/1000 | Loss: 0.00050942
Iteration 6/1000 | Loss: 0.00402398
Iteration 7/1000 | Loss: 0.00024854
Iteration 8/1000 | Loss: 0.00036304
Iteration 9/1000 | Loss: 0.00053279
Iteration 10/1000 | Loss: 0.00014047
Iteration 11/1000 | Loss: 0.00013459
Iteration 12/1000 | Loss: 0.00046929
Iteration 13/1000 | Loss: 0.00205322
Iteration 14/1000 | Loss: 0.00276265
Iteration 15/1000 | Loss: 0.00216968
Iteration 16/1000 | Loss: 0.00476282
Iteration 17/1000 | Loss: 0.00226901
Iteration 18/1000 | Loss: 0.00394015
Iteration 19/1000 | Loss: 0.00195245
Iteration 20/1000 | Loss: 0.00060659
Iteration 21/1000 | Loss: 0.00097533
Iteration 22/1000 | Loss: 0.00101980
Iteration 23/1000 | Loss: 0.00103662
Iteration 24/1000 | Loss: 0.00076316
Iteration 25/1000 | Loss: 0.00031473
Iteration 26/1000 | Loss: 0.00504049
Iteration 27/1000 | Loss: 0.00062193
Iteration 28/1000 | Loss: 0.00029173
Iteration 29/1000 | Loss: 0.00409872
Iteration 30/1000 | Loss: 0.00301726
Iteration 31/1000 | Loss: 0.00033951
Iteration 32/1000 | Loss: 0.00267578
Iteration 33/1000 | Loss: 0.00039671
Iteration 34/1000 | Loss: 0.00045223
Iteration 35/1000 | Loss: 0.00016338
Iteration 36/1000 | Loss: 0.00015315
Iteration 37/1000 | Loss: 0.00118584
Iteration 38/1000 | Loss: 0.00135266
Iteration 39/1000 | Loss: 0.00188547
Iteration 40/1000 | Loss: 0.00032513
Iteration 41/1000 | Loss: 0.00032459
Iteration 42/1000 | Loss: 0.00020453
Iteration 43/1000 | Loss: 0.00007200
Iteration 44/1000 | Loss: 0.00005414
Iteration 45/1000 | Loss: 0.00016975
Iteration 46/1000 | Loss: 0.00019588
Iteration 47/1000 | Loss: 0.00012396
Iteration 48/1000 | Loss: 0.00009506
Iteration 49/1000 | Loss: 0.00010123
Iteration 50/1000 | Loss: 0.00003933
Iteration 51/1000 | Loss: 0.00003758
Iteration 52/1000 | Loss: 0.00003554
Iteration 53/1000 | Loss: 0.00003396
Iteration 54/1000 | Loss: 0.00003288
Iteration 55/1000 | Loss: 0.00003201
Iteration 56/1000 | Loss: 0.00014801
Iteration 57/1000 | Loss: 0.00007681
Iteration 58/1000 | Loss: 0.00003949
Iteration 59/1000 | Loss: 0.00003540
Iteration 60/1000 | Loss: 0.00003150
Iteration 61/1000 | Loss: 0.00003091
Iteration 62/1000 | Loss: 0.00015302
Iteration 63/1000 | Loss: 0.00007544
Iteration 64/1000 | Loss: 0.00003489
Iteration 65/1000 | Loss: 0.00013877
Iteration 66/1000 | Loss: 0.00008643
Iteration 67/1000 | Loss: 0.00019713
Iteration 68/1000 | Loss: 0.00008899
Iteration 69/1000 | Loss: 0.00019399
Iteration 70/1000 | Loss: 0.00008015
Iteration 71/1000 | Loss: 0.00019486
Iteration 72/1000 | Loss: 0.00022198
Iteration 73/1000 | Loss: 0.00020818
Iteration 74/1000 | Loss: 0.00020278
Iteration 75/1000 | Loss: 0.00012459
Iteration 76/1000 | Loss: 0.00027849
Iteration 77/1000 | Loss: 0.00027336
Iteration 78/1000 | Loss: 0.00010268
Iteration 79/1000 | Loss: 0.00010787
Iteration 80/1000 | Loss: 0.00003651
Iteration 81/1000 | Loss: 0.00008381
Iteration 82/1000 | Loss: 0.00008553
Iteration 83/1000 | Loss: 0.00003438
Iteration 84/1000 | Loss: 0.00021749
Iteration 85/1000 | Loss: 0.00034979
Iteration 86/1000 | Loss: 0.00023187
Iteration 87/1000 | Loss: 0.00024234
Iteration 88/1000 | Loss: 0.00047473
Iteration 89/1000 | Loss: 0.00035564
Iteration 90/1000 | Loss: 0.00010720
Iteration 91/1000 | Loss: 0.00067353
Iteration 92/1000 | Loss: 0.00044619
Iteration 93/1000 | Loss: 0.00020089
Iteration 94/1000 | Loss: 0.00003817
Iteration 95/1000 | Loss: 0.00006026
Iteration 96/1000 | Loss: 0.00003634
Iteration 97/1000 | Loss: 0.00015954
Iteration 98/1000 | Loss: 0.00026020
Iteration 99/1000 | Loss: 0.00024972
Iteration 100/1000 | Loss: 0.00004602
Iteration 101/1000 | Loss: 0.00024348
Iteration 102/1000 | Loss: 0.00020870
Iteration 103/1000 | Loss: 0.00037625
Iteration 104/1000 | Loss: 0.00029451
Iteration 105/1000 | Loss: 0.00014459
Iteration 106/1000 | Loss: 0.00003880
Iteration 107/1000 | Loss: 0.00003687
Iteration 108/1000 | Loss: 0.00018236
Iteration 109/1000 | Loss: 0.00015829
Iteration 110/1000 | Loss: 0.00025846
Iteration 111/1000 | Loss: 0.00030971
Iteration 112/1000 | Loss: 0.00014781
Iteration 113/1000 | Loss: 0.00018365
Iteration 114/1000 | Loss: 0.00015271
Iteration 115/1000 | Loss: 0.00017742
Iteration 116/1000 | Loss: 0.00034840
Iteration 117/1000 | Loss: 0.00067565
Iteration 118/1000 | Loss: 0.00038311
Iteration 119/1000 | Loss: 0.00022618
Iteration 120/1000 | Loss: 0.00031575
Iteration 121/1000 | Loss: 0.00047673
Iteration 122/1000 | Loss: 0.00026952
Iteration 123/1000 | Loss: 0.00039348
Iteration 124/1000 | Loss: 0.00022271
Iteration 125/1000 | Loss: 0.00031997
Iteration 126/1000 | Loss: 0.00040998
Iteration 127/1000 | Loss: 0.00050268
Iteration 128/1000 | Loss: 0.00031183
Iteration 129/1000 | Loss: 0.00027284
Iteration 130/1000 | Loss: 0.00019472
Iteration 131/1000 | Loss: 0.00028050
Iteration 132/1000 | Loss: 0.00007339
Iteration 133/1000 | Loss: 0.00037484
Iteration 134/1000 | Loss: 0.00004207
Iteration 135/1000 | Loss: 0.00003690
Iteration 136/1000 | Loss: 0.00010761
Iteration 137/1000 | Loss: 0.00008267
Iteration 138/1000 | Loss: 0.00003523
Iteration 139/1000 | Loss: 0.00003400
Iteration 140/1000 | Loss: 0.00010954
Iteration 141/1000 | Loss: 0.00021326
Iteration 142/1000 | Loss: 0.00018101
Iteration 143/1000 | Loss: 0.00020221
Iteration 144/1000 | Loss: 0.00007424
Iteration 145/1000 | Loss: 0.00005708
Iteration 146/1000 | Loss: 0.00017766
Iteration 147/1000 | Loss: 0.00003469
Iteration 148/1000 | Loss: 0.00003390
Iteration 149/1000 | Loss: 0.00003311
Iteration 150/1000 | Loss: 0.00003237
Iteration 151/1000 | Loss: 0.00020569
Iteration 152/1000 | Loss: 0.00014343
Iteration 153/1000 | Loss: 0.00005591
Iteration 154/1000 | Loss: 0.00005585
Iteration 155/1000 | Loss: 0.00003695
Iteration 156/1000 | Loss: 0.00026279
Iteration 157/1000 | Loss: 0.00013306
Iteration 158/1000 | Loss: 0.00015180
Iteration 159/1000 | Loss: 0.00018625
Iteration 160/1000 | Loss: 0.00020515
Iteration 161/1000 | Loss: 0.00016671
Iteration 162/1000 | Loss: 0.00003171
Iteration 163/1000 | Loss: 0.00003112
Iteration 164/1000 | Loss: 0.00003082
Iteration 165/1000 | Loss: 0.00003046
Iteration 166/1000 | Loss: 0.00002905
Iteration 167/1000 | Loss: 0.00002828
Iteration 168/1000 | Loss: 0.00002761
Iteration 169/1000 | Loss: 0.00002718
Iteration 170/1000 | Loss: 0.00002693
Iteration 171/1000 | Loss: 0.00002683
Iteration 172/1000 | Loss: 0.00002672
Iteration 173/1000 | Loss: 0.00002671
Iteration 174/1000 | Loss: 0.00002671
Iteration 175/1000 | Loss: 0.00002670
Iteration 176/1000 | Loss: 0.00002669
Iteration 177/1000 | Loss: 0.00002669
Iteration 178/1000 | Loss: 0.00002668
Iteration 179/1000 | Loss: 0.00002668
Iteration 180/1000 | Loss: 0.00002666
Iteration 181/1000 | Loss: 0.00002666
Iteration 182/1000 | Loss: 0.00002665
Iteration 183/1000 | Loss: 0.00002665
Iteration 184/1000 | Loss: 0.00002665
Iteration 185/1000 | Loss: 0.00002665
Iteration 186/1000 | Loss: 0.00002665
Iteration 187/1000 | Loss: 0.00002665
Iteration 188/1000 | Loss: 0.00002665
Iteration 189/1000 | Loss: 0.00002665
Iteration 190/1000 | Loss: 0.00002665
Iteration 191/1000 | Loss: 0.00002665
Iteration 192/1000 | Loss: 0.00002665
Iteration 193/1000 | Loss: 0.00002665
Iteration 194/1000 | Loss: 0.00002664
Iteration 195/1000 | Loss: 0.00002664
Iteration 196/1000 | Loss: 0.00002664
Iteration 197/1000 | Loss: 0.00002663
Iteration 198/1000 | Loss: 0.00002663
Iteration 199/1000 | Loss: 0.00002663
Iteration 200/1000 | Loss: 0.00002663
Iteration 201/1000 | Loss: 0.00002663
Iteration 202/1000 | Loss: 0.00002663
Iteration 203/1000 | Loss: 0.00002662
Iteration 204/1000 | Loss: 0.00002662
Iteration 205/1000 | Loss: 0.00002662
Iteration 206/1000 | Loss: 0.00002662
Iteration 207/1000 | Loss: 0.00002662
Iteration 208/1000 | Loss: 0.00002662
Iteration 209/1000 | Loss: 0.00002661
Iteration 210/1000 | Loss: 0.00002661
Iteration 211/1000 | Loss: 0.00002661
Iteration 212/1000 | Loss: 0.00002661
Iteration 213/1000 | Loss: 0.00002661
Iteration 214/1000 | Loss: 0.00002661
Iteration 215/1000 | Loss: 0.00002661
Iteration 216/1000 | Loss: 0.00002661
Iteration 217/1000 | Loss: 0.00002661
Iteration 218/1000 | Loss: 0.00002661
Iteration 219/1000 | Loss: 0.00002660
Iteration 220/1000 | Loss: 0.00002660
Iteration 221/1000 | Loss: 0.00002660
Iteration 222/1000 | Loss: 0.00002660
Iteration 223/1000 | Loss: 0.00002660
Iteration 224/1000 | Loss: 0.00002660
Iteration 225/1000 | Loss: 0.00002660
Iteration 226/1000 | Loss: 0.00002660
Iteration 227/1000 | Loss: 0.00002660
Iteration 228/1000 | Loss: 0.00002660
Iteration 229/1000 | Loss: 0.00002659
Iteration 230/1000 | Loss: 0.00002659
Iteration 231/1000 | Loss: 0.00002659
Iteration 232/1000 | Loss: 0.00002659
Iteration 233/1000 | Loss: 0.00002659
Iteration 234/1000 | Loss: 0.00002659
Iteration 235/1000 | Loss: 0.00002659
Iteration 236/1000 | Loss: 0.00002659
Iteration 237/1000 | Loss: 0.00002659
Iteration 238/1000 | Loss: 0.00002658
Iteration 239/1000 | Loss: 0.00002658
Iteration 240/1000 | Loss: 0.00002658
Iteration 241/1000 | Loss: 0.00002658
Iteration 242/1000 | Loss: 0.00002658
Iteration 243/1000 | Loss: 0.00002658
Iteration 244/1000 | Loss: 0.00002658
Iteration 245/1000 | Loss: 0.00002657
Iteration 246/1000 | Loss: 0.00002657
Iteration 247/1000 | Loss: 0.00002657
Iteration 248/1000 | Loss: 0.00002657
Iteration 249/1000 | Loss: 0.00002657
Iteration 250/1000 | Loss: 0.00002657
Iteration 251/1000 | Loss: 0.00002656
Iteration 252/1000 | Loss: 0.00002656
Iteration 253/1000 | Loss: 0.00002656
Iteration 254/1000 | Loss: 0.00002656
Iteration 255/1000 | Loss: 0.00002655
Iteration 256/1000 | Loss: 0.00002655
Iteration 257/1000 | Loss: 0.00002655
Iteration 258/1000 | Loss: 0.00002655
Iteration 259/1000 | Loss: 0.00002655
Iteration 260/1000 | Loss: 0.00002655
Iteration 261/1000 | Loss: 0.00002655
Iteration 262/1000 | Loss: 0.00002654
Iteration 263/1000 | Loss: 0.00002654
Iteration 264/1000 | Loss: 0.00002654
Iteration 265/1000 | Loss: 0.00002654
Iteration 266/1000 | Loss: 0.00002654
Iteration 267/1000 | Loss: 0.00002654
Iteration 268/1000 | Loss: 0.00002654
Iteration 269/1000 | Loss: 0.00002654
Iteration 270/1000 | Loss: 0.00002654
Iteration 271/1000 | Loss: 0.00002654
Iteration 272/1000 | Loss: 0.00002654
Iteration 273/1000 | Loss: 0.00002653
Iteration 274/1000 | Loss: 0.00002653
Iteration 275/1000 | Loss: 0.00002653
Iteration 276/1000 | Loss: 0.00002653
Iteration 277/1000 | Loss: 0.00002653
Iteration 278/1000 | Loss: 0.00002653
Iteration 279/1000 | Loss: 0.00002653
Iteration 280/1000 | Loss: 0.00002653
Iteration 281/1000 | Loss: 0.00002653
Iteration 282/1000 | Loss: 0.00002653
Iteration 283/1000 | Loss: 0.00002652
Iteration 284/1000 | Loss: 0.00002652
Iteration 285/1000 | Loss: 0.00002652
Iteration 286/1000 | Loss: 0.00002652
Iteration 287/1000 | Loss: 0.00002652
Iteration 288/1000 | Loss: 0.00002652
Iteration 289/1000 | Loss: 0.00002651
Iteration 290/1000 | Loss: 0.00002651
Iteration 291/1000 | Loss: 0.00002651
Iteration 292/1000 | Loss: 0.00002651
Iteration 293/1000 | Loss: 0.00002651
Iteration 294/1000 | Loss: 0.00002651
Iteration 295/1000 | Loss: 0.00002650
Iteration 296/1000 | Loss: 0.00002650
Iteration 297/1000 | Loss: 0.00002650
Iteration 298/1000 | Loss: 0.00002650
Iteration 299/1000 | Loss: 0.00002650
Iteration 300/1000 | Loss: 0.00002650
Iteration 301/1000 | Loss: 0.00002649
Iteration 302/1000 | Loss: 0.00002649
Iteration 303/1000 | Loss: 0.00002649
Iteration 304/1000 | Loss: 0.00002649
Iteration 305/1000 | Loss: 0.00002649
Iteration 306/1000 | Loss: 0.00002649
Iteration 307/1000 | Loss: 0.00002649
Iteration 308/1000 | Loss: 0.00002649
Iteration 309/1000 | Loss: 0.00002649
Iteration 310/1000 | Loss: 0.00002649
Iteration 311/1000 | Loss: 0.00002649
Iteration 312/1000 | Loss: 0.00002649
Iteration 313/1000 | Loss: 0.00002649
Iteration 314/1000 | Loss: 0.00002649
Iteration 315/1000 | Loss: 0.00002649
Iteration 316/1000 | Loss: 0.00002649
Iteration 317/1000 | Loss: 0.00002649
Iteration 318/1000 | Loss: 0.00002648
Iteration 319/1000 | Loss: 0.00002648
Iteration 320/1000 | Loss: 0.00002648
Iteration 321/1000 | Loss: 0.00002648
Iteration 322/1000 | Loss: 0.00002648
Iteration 323/1000 | Loss: 0.00002648
Iteration 324/1000 | Loss: 0.00002648
Iteration 325/1000 | Loss: 0.00002648
Iteration 326/1000 | Loss: 0.00002648
Iteration 327/1000 | Loss: 0.00002648
Iteration 328/1000 | Loss: 0.00002648
Iteration 329/1000 | Loss: 0.00002647
Iteration 330/1000 | Loss: 0.00002647
Iteration 331/1000 | Loss: 0.00002647
Iteration 332/1000 | Loss: 0.00002647
Iteration 333/1000 | Loss: 0.00002647
Iteration 334/1000 | Loss: 0.00002647
Iteration 335/1000 | Loss: 0.00002647
Iteration 336/1000 | Loss: 0.00002647
Iteration 337/1000 | Loss: 0.00002647
Iteration 338/1000 | Loss: 0.00002647
Iteration 339/1000 | Loss: 0.00002647
Iteration 340/1000 | Loss: 0.00002647
Iteration 341/1000 | Loss: 0.00002647
Iteration 342/1000 | Loss: 0.00002647
Iteration 343/1000 | Loss: 0.00002647
Iteration 344/1000 | Loss: 0.00002647
Iteration 345/1000 | Loss: 0.00002647
Iteration 346/1000 | Loss: 0.00002647
Iteration 347/1000 | Loss: 0.00002646
Iteration 348/1000 | Loss: 0.00002646
Iteration 349/1000 | Loss: 0.00002646
Iteration 350/1000 | Loss: 0.00002646
Iteration 351/1000 | Loss: 0.00002646
Iteration 352/1000 | Loss: 0.00002646
Iteration 353/1000 | Loss: 0.00002646
Iteration 354/1000 | Loss: 0.00002646
Iteration 355/1000 | Loss: 0.00002646
Iteration 356/1000 | Loss: 0.00002646
Iteration 357/1000 | Loss: 0.00002646
Iteration 358/1000 | Loss: 0.00002646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 358. Stopping optimization.
Last 5 losses: [2.6462466848897748e-05, 2.6462466848897748e-05, 2.6462466848897748e-05, 2.6462466848897748e-05, 2.6462466848897748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6462466848897748e-05

Optimization complete. Final v2v error: 4.255610466003418 mm

Highest mean error: 6.020730495452881 mm for frame 97

Lowest mean error: 3.2858636379241943 mm for frame 144

Saving results

Total time: 310.8903863430023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429474
Iteration 2/25 | Loss: 0.00144604
Iteration 3/25 | Loss: 0.00124237
Iteration 4/25 | Loss: 0.00121770
Iteration 5/25 | Loss: 0.00121388
Iteration 6/25 | Loss: 0.00121304
Iteration 7/25 | Loss: 0.00121304
Iteration 8/25 | Loss: 0.00121304
Iteration 9/25 | Loss: 0.00121304
Iteration 10/25 | Loss: 0.00121304
Iteration 11/25 | Loss: 0.00121304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012130436953157187, 0.0012130436953157187, 0.0012130436953157187, 0.0012130436953157187, 0.0012130436953157187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012130436953157187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44832456
Iteration 2/25 | Loss: 0.00072432
Iteration 3/25 | Loss: 0.00072432
Iteration 4/25 | Loss: 0.00072431
Iteration 5/25 | Loss: 0.00072431
Iteration 6/25 | Loss: 0.00072431
Iteration 7/25 | Loss: 0.00072431
Iteration 8/25 | Loss: 0.00072431
Iteration 9/25 | Loss: 0.00072431
Iteration 10/25 | Loss: 0.00072431
Iteration 11/25 | Loss: 0.00072431
Iteration 12/25 | Loss: 0.00072431
Iteration 13/25 | Loss: 0.00072431
Iteration 14/25 | Loss: 0.00072431
Iteration 15/25 | Loss: 0.00072431
Iteration 16/25 | Loss: 0.00072431
Iteration 17/25 | Loss: 0.00072431
Iteration 18/25 | Loss: 0.00072431
Iteration 19/25 | Loss: 0.00072431
Iteration 20/25 | Loss: 0.00072431
Iteration 21/25 | Loss: 0.00072431
Iteration 22/25 | Loss: 0.00072431
Iteration 23/25 | Loss: 0.00072431
Iteration 24/25 | Loss: 0.00072431
Iteration 25/25 | Loss: 0.00072431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072431
Iteration 2/1000 | Loss: 0.00002715
Iteration 3/1000 | Loss: 0.00001849
Iteration 4/1000 | Loss: 0.00001637
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001447
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001307
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001275
Iteration 26/1000 | Loss: 0.00001274
Iteration 27/1000 | Loss: 0.00001274
Iteration 28/1000 | Loss: 0.00001273
Iteration 29/1000 | Loss: 0.00001272
Iteration 30/1000 | Loss: 0.00001272
Iteration 31/1000 | Loss: 0.00001272
Iteration 32/1000 | Loss: 0.00001271
Iteration 33/1000 | Loss: 0.00001271
Iteration 34/1000 | Loss: 0.00001270
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001269
Iteration 39/1000 | Loss: 0.00001269
Iteration 40/1000 | Loss: 0.00001269
Iteration 41/1000 | Loss: 0.00001268
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001265
Iteration 49/1000 | Loss: 0.00001265
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001256
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001251
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001250
Iteration 92/1000 | Loss: 0.00001250
Iteration 93/1000 | Loss: 0.00001249
Iteration 94/1000 | Loss: 0.00001249
Iteration 95/1000 | Loss: 0.00001249
Iteration 96/1000 | Loss: 0.00001249
Iteration 97/1000 | Loss: 0.00001249
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001243
Iteration 117/1000 | Loss: 0.00001243
Iteration 118/1000 | Loss: 0.00001243
Iteration 119/1000 | Loss: 0.00001242
Iteration 120/1000 | Loss: 0.00001242
Iteration 121/1000 | Loss: 0.00001242
Iteration 122/1000 | Loss: 0.00001242
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001240
Iteration 129/1000 | Loss: 0.00001240
Iteration 130/1000 | Loss: 0.00001240
Iteration 131/1000 | Loss: 0.00001240
Iteration 132/1000 | Loss: 0.00001240
Iteration 133/1000 | Loss: 0.00001240
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001238
Iteration 140/1000 | Loss: 0.00001238
Iteration 141/1000 | Loss: 0.00001238
Iteration 142/1000 | Loss: 0.00001237
Iteration 143/1000 | Loss: 0.00001237
Iteration 144/1000 | Loss: 0.00001237
Iteration 145/1000 | Loss: 0.00001237
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001235
Iteration 150/1000 | Loss: 0.00001235
Iteration 151/1000 | Loss: 0.00001235
Iteration 152/1000 | Loss: 0.00001234
Iteration 153/1000 | Loss: 0.00001234
Iteration 154/1000 | Loss: 0.00001234
Iteration 155/1000 | Loss: 0.00001234
Iteration 156/1000 | Loss: 0.00001234
Iteration 157/1000 | Loss: 0.00001234
Iteration 158/1000 | Loss: 0.00001234
Iteration 159/1000 | Loss: 0.00001234
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001233
Iteration 162/1000 | Loss: 0.00001233
Iteration 163/1000 | Loss: 0.00001233
Iteration 164/1000 | Loss: 0.00001233
Iteration 165/1000 | Loss: 0.00001233
Iteration 166/1000 | Loss: 0.00001232
Iteration 167/1000 | Loss: 0.00001232
Iteration 168/1000 | Loss: 0.00001232
Iteration 169/1000 | Loss: 0.00001232
Iteration 170/1000 | Loss: 0.00001231
Iteration 171/1000 | Loss: 0.00001231
Iteration 172/1000 | Loss: 0.00001231
Iteration 173/1000 | Loss: 0.00001231
Iteration 174/1000 | Loss: 0.00001231
Iteration 175/1000 | Loss: 0.00001231
Iteration 176/1000 | Loss: 0.00001231
Iteration 177/1000 | Loss: 0.00001231
Iteration 178/1000 | Loss: 0.00001231
Iteration 179/1000 | Loss: 0.00001231
Iteration 180/1000 | Loss: 0.00001231
Iteration 181/1000 | Loss: 0.00001230
Iteration 182/1000 | Loss: 0.00001230
Iteration 183/1000 | Loss: 0.00001230
Iteration 184/1000 | Loss: 0.00001230
Iteration 185/1000 | Loss: 0.00001230
Iteration 186/1000 | Loss: 0.00001230
Iteration 187/1000 | Loss: 0.00001229
Iteration 188/1000 | Loss: 0.00001229
Iteration 189/1000 | Loss: 0.00001229
Iteration 190/1000 | Loss: 0.00001229
Iteration 191/1000 | Loss: 0.00001229
Iteration 192/1000 | Loss: 0.00001229
Iteration 193/1000 | Loss: 0.00001229
Iteration 194/1000 | Loss: 0.00001229
Iteration 195/1000 | Loss: 0.00001228
Iteration 196/1000 | Loss: 0.00001228
Iteration 197/1000 | Loss: 0.00001228
Iteration 198/1000 | Loss: 0.00001228
Iteration 199/1000 | Loss: 0.00001228
Iteration 200/1000 | Loss: 0.00001228
Iteration 201/1000 | Loss: 0.00001228
Iteration 202/1000 | Loss: 0.00001228
Iteration 203/1000 | Loss: 0.00001227
Iteration 204/1000 | Loss: 0.00001227
Iteration 205/1000 | Loss: 0.00001227
Iteration 206/1000 | Loss: 0.00001227
Iteration 207/1000 | Loss: 0.00001227
Iteration 208/1000 | Loss: 0.00001227
Iteration 209/1000 | Loss: 0.00001227
Iteration 210/1000 | Loss: 0.00001226
Iteration 211/1000 | Loss: 0.00001226
Iteration 212/1000 | Loss: 0.00001226
Iteration 213/1000 | Loss: 0.00001226
Iteration 214/1000 | Loss: 0.00001226
Iteration 215/1000 | Loss: 0.00001226
Iteration 216/1000 | Loss: 0.00001225
Iteration 217/1000 | Loss: 0.00001225
Iteration 218/1000 | Loss: 0.00001225
Iteration 219/1000 | Loss: 0.00001225
Iteration 220/1000 | Loss: 0.00001225
Iteration 221/1000 | Loss: 0.00001225
Iteration 222/1000 | Loss: 0.00001225
Iteration 223/1000 | Loss: 0.00001225
Iteration 224/1000 | Loss: 0.00001225
Iteration 225/1000 | Loss: 0.00001225
Iteration 226/1000 | Loss: 0.00001225
Iteration 227/1000 | Loss: 0.00001225
Iteration 228/1000 | Loss: 0.00001225
Iteration 229/1000 | Loss: 0.00001225
Iteration 230/1000 | Loss: 0.00001225
Iteration 231/1000 | Loss: 0.00001225
Iteration 232/1000 | Loss: 0.00001225
Iteration 233/1000 | Loss: 0.00001225
Iteration 234/1000 | Loss: 0.00001225
Iteration 235/1000 | Loss: 0.00001225
Iteration 236/1000 | Loss: 0.00001225
Iteration 237/1000 | Loss: 0.00001225
Iteration 238/1000 | Loss: 0.00001225
Iteration 239/1000 | Loss: 0.00001225
Iteration 240/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.2251893167558592e-05, 1.2251893167558592e-05, 1.2251893167558592e-05, 1.2251893167558592e-05, 1.2251893167558592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2251893167558592e-05

Optimization complete. Final v2v error: 2.9936623573303223 mm

Highest mean error: 3.5532190799713135 mm for frame 90

Lowest mean error: 2.813464879989624 mm for frame 207

Saving results

Total time: 48.05414795875549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992378
Iteration 2/25 | Loss: 0.00992378
Iteration 3/25 | Loss: 0.00992378
Iteration 4/25 | Loss: 0.00267557
Iteration 5/25 | Loss: 0.00198040
Iteration 6/25 | Loss: 0.00184170
Iteration 7/25 | Loss: 0.00190337
Iteration 8/25 | Loss: 0.00160000
Iteration 9/25 | Loss: 0.00143099
Iteration 10/25 | Loss: 0.00140141
Iteration 11/25 | Loss: 0.00139234
Iteration 12/25 | Loss: 0.00137930
Iteration 13/25 | Loss: 0.00137482
Iteration 14/25 | Loss: 0.00137015
Iteration 15/25 | Loss: 0.00136697
Iteration 16/25 | Loss: 0.00136970
Iteration 17/25 | Loss: 0.00137286
Iteration 18/25 | Loss: 0.00136776
Iteration 19/25 | Loss: 0.00136567
Iteration 20/25 | Loss: 0.00136720
Iteration 21/25 | Loss: 0.00136627
Iteration 22/25 | Loss: 0.00136332
Iteration 23/25 | Loss: 0.00138508
Iteration 24/25 | Loss: 0.00137451
Iteration 25/25 | Loss: 0.00135798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44088316
Iteration 2/25 | Loss: 0.00086889
Iteration 3/25 | Loss: 0.00086889
Iteration 4/25 | Loss: 0.00086888
Iteration 5/25 | Loss: 0.00086888
Iteration 6/25 | Loss: 0.00086888
Iteration 7/25 | Loss: 0.00086888
Iteration 8/25 | Loss: 0.00086888
Iteration 9/25 | Loss: 0.00086888
Iteration 10/25 | Loss: 0.00086888
Iteration 11/25 | Loss: 0.00086888
Iteration 12/25 | Loss: 0.00086888
Iteration 13/25 | Loss: 0.00086888
Iteration 14/25 | Loss: 0.00086888
Iteration 15/25 | Loss: 0.00086888
Iteration 16/25 | Loss: 0.00086888
Iteration 17/25 | Loss: 0.00086888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008688826928846538, 0.0008688826928846538, 0.0008688826928846538, 0.0008688826928846538, 0.0008688826928846538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008688826928846538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086888
Iteration 2/1000 | Loss: 0.00118419
Iteration 3/1000 | Loss: 0.00026924
Iteration 4/1000 | Loss: 0.00043228
Iteration 5/1000 | Loss: 0.00020023
Iteration 6/1000 | Loss: 0.00058190
Iteration 7/1000 | Loss: 0.00026415
Iteration 8/1000 | Loss: 0.00028161
Iteration 9/1000 | Loss: 0.00016900
Iteration 10/1000 | Loss: 0.00018681
Iteration 11/1000 | Loss: 0.00016903
Iteration 12/1000 | Loss: 0.00017306
Iteration 13/1000 | Loss: 0.00016381
Iteration 14/1000 | Loss: 0.00019529
Iteration 15/1000 | Loss: 0.00019693
Iteration 16/1000 | Loss: 0.00019266
Iteration 17/1000 | Loss: 0.00018354
Iteration 18/1000 | Loss: 0.00009202
Iteration 19/1000 | Loss: 0.00006307
Iteration 20/1000 | Loss: 0.00005329
Iteration 21/1000 | Loss: 0.00011429
Iteration 22/1000 | Loss: 0.00004517
Iteration 23/1000 | Loss: 0.00003979
Iteration 24/1000 | Loss: 0.00022947
Iteration 25/1000 | Loss: 0.00038570
Iteration 26/1000 | Loss: 0.00011879
Iteration 27/1000 | Loss: 0.00018391
Iteration 28/1000 | Loss: 0.00005727
Iteration 29/1000 | Loss: 0.00056753
Iteration 30/1000 | Loss: 0.00029293
Iteration 31/1000 | Loss: 0.00004701
Iteration 32/1000 | Loss: 0.00004925
Iteration 33/1000 | Loss: 0.00005150
Iteration 34/1000 | Loss: 0.00004473
Iteration 35/1000 | Loss: 0.00004673
Iteration 36/1000 | Loss: 0.00011956
Iteration 37/1000 | Loss: 0.00004083
Iteration 38/1000 | Loss: 0.00030748
Iteration 39/1000 | Loss: 0.00016048
Iteration 40/1000 | Loss: 0.00012405
Iteration 41/1000 | Loss: 0.00041264
Iteration 42/1000 | Loss: 0.00008502
Iteration 43/1000 | Loss: 0.00035738
Iteration 44/1000 | Loss: 0.00017357
Iteration 45/1000 | Loss: 0.00044362
Iteration 46/1000 | Loss: 0.00005256
Iteration 47/1000 | Loss: 0.00049756
Iteration 48/1000 | Loss: 0.00033125
Iteration 49/1000 | Loss: 0.00016692
Iteration 50/1000 | Loss: 0.00022134
Iteration 51/1000 | Loss: 0.00046814
Iteration 52/1000 | Loss: 0.00021554
Iteration 53/1000 | Loss: 0.00004047
Iteration 54/1000 | Loss: 0.00014513
Iteration 55/1000 | Loss: 0.00039253
Iteration 56/1000 | Loss: 0.00030397
Iteration 57/1000 | Loss: 0.00012145
Iteration 58/1000 | Loss: 0.00008382
Iteration 59/1000 | Loss: 0.00046730
Iteration 60/1000 | Loss: 0.00032416
Iteration 61/1000 | Loss: 0.00037322
Iteration 62/1000 | Loss: 0.00005685
Iteration 63/1000 | Loss: 0.00004164
Iteration 64/1000 | Loss: 0.00014705
Iteration 65/1000 | Loss: 0.00003643
Iteration 66/1000 | Loss: 0.00008495
Iteration 67/1000 | Loss: 0.00003399
Iteration 68/1000 | Loss: 0.00003675
Iteration 69/1000 | Loss: 0.00003419
Iteration 70/1000 | Loss: 0.00003274
Iteration 71/1000 | Loss: 0.00003087
Iteration 72/1000 | Loss: 0.00003019
Iteration 73/1000 | Loss: 0.00002971
Iteration 74/1000 | Loss: 0.00002919
Iteration 75/1000 | Loss: 0.00002890
Iteration 76/1000 | Loss: 0.00002885
Iteration 77/1000 | Loss: 0.00002883
Iteration 78/1000 | Loss: 0.00012266
Iteration 79/1000 | Loss: 0.00019218
Iteration 80/1000 | Loss: 0.00015295
Iteration 81/1000 | Loss: 0.00005078
Iteration 82/1000 | Loss: 0.00003978
Iteration 83/1000 | Loss: 0.00003443
Iteration 84/1000 | Loss: 0.00003206
Iteration 85/1000 | Loss: 0.00003087
Iteration 86/1000 | Loss: 0.00003028
Iteration 87/1000 | Loss: 0.00002969
Iteration 88/1000 | Loss: 0.00002913
Iteration 89/1000 | Loss: 0.00002850
Iteration 90/1000 | Loss: 0.00009973
Iteration 91/1000 | Loss: 0.00003191
Iteration 92/1000 | Loss: 0.00003089
Iteration 93/1000 | Loss: 0.00002938
Iteration 94/1000 | Loss: 0.00002870
Iteration 95/1000 | Loss: 0.00002853
Iteration 96/1000 | Loss: 0.00002849
Iteration 97/1000 | Loss: 0.00002844
Iteration 98/1000 | Loss: 0.00002821
Iteration 99/1000 | Loss: 0.00002794
Iteration 100/1000 | Loss: 0.00035109
Iteration 101/1000 | Loss: 0.00004999
Iteration 102/1000 | Loss: 0.00003097
Iteration 103/1000 | Loss: 0.00002901
Iteration 104/1000 | Loss: 0.00016852
Iteration 105/1000 | Loss: 0.00005369
Iteration 106/1000 | Loss: 0.00002765
Iteration 107/1000 | Loss: 0.00002707
Iteration 108/1000 | Loss: 0.00002656
Iteration 109/1000 | Loss: 0.00002617
Iteration 110/1000 | Loss: 0.00002590
Iteration 111/1000 | Loss: 0.00002570
Iteration 112/1000 | Loss: 0.00002553
Iteration 113/1000 | Loss: 0.00002547
Iteration 114/1000 | Loss: 0.00002541
Iteration 115/1000 | Loss: 0.00002541
Iteration 116/1000 | Loss: 0.00002541
Iteration 117/1000 | Loss: 0.00002540
Iteration 118/1000 | Loss: 0.00002540
Iteration 119/1000 | Loss: 0.00002539
Iteration 120/1000 | Loss: 0.00002539
Iteration 121/1000 | Loss: 0.00002539
Iteration 122/1000 | Loss: 0.00002538
Iteration 123/1000 | Loss: 0.00002538
Iteration 124/1000 | Loss: 0.00002538
Iteration 125/1000 | Loss: 0.00004984
Iteration 126/1000 | Loss: 0.00004290
Iteration 127/1000 | Loss: 0.00004916
Iteration 128/1000 | Loss: 0.00004203
Iteration 129/1000 | Loss: 0.00004877
Iteration 130/1000 | Loss: 0.00004185
Iteration 131/1000 | Loss: 0.00004841
Iteration 132/1000 | Loss: 0.00003254
Iteration 133/1000 | Loss: 0.00002971
Iteration 134/1000 | Loss: 0.00004566
Iteration 135/1000 | Loss: 0.00004497
Iteration 136/1000 | Loss: 0.00004500
Iteration 137/1000 | Loss: 0.00004533
Iteration 138/1000 | Loss: 0.00002857
Iteration 139/1000 | Loss: 0.00002690
Iteration 140/1000 | Loss: 0.00002600
Iteration 141/1000 | Loss: 0.00002564
Iteration 142/1000 | Loss: 0.00002532
Iteration 143/1000 | Loss: 0.00002506
Iteration 144/1000 | Loss: 0.00002504
Iteration 145/1000 | Loss: 0.00002504
Iteration 146/1000 | Loss: 0.00002504
Iteration 147/1000 | Loss: 0.00002503
Iteration 148/1000 | Loss: 0.00002503
Iteration 149/1000 | Loss: 0.00002503
Iteration 150/1000 | Loss: 0.00002503
Iteration 151/1000 | Loss: 0.00002503
Iteration 152/1000 | Loss: 0.00002503
Iteration 153/1000 | Loss: 0.00002503
Iteration 154/1000 | Loss: 0.00002503
Iteration 155/1000 | Loss: 0.00002503
Iteration 156/1000 | Loss: 0.00002503
Iteration 157/1000 | Loss: 0.00002502
Iteration 158/1000 | Loss: 0.00002502
Iteration 159/1000 | Loss: 0.00002502
Iteration 160/1000 | Loss: 0.00002502
Iteration 161/1000 | Loss: 0.00002502
Iteration 162/1000 | Loss: 0.00002502
Iteration 163/1000 | Loss: 0.00002502
Iteration 164/1000 | Loss: 0.00002502
Iteration 165/1000 | Loss: 0.00002502
Iteration 166/1000 | Loss: 0.00002502
Iteration 167/1000 | Loss: 0.00002502
Iteration 168/1000 | Loss: 0.00002502
Iteration 169/1000 | Loss: 0.00002501
Iteration 170/1000 | Loss: 0.00002501
Iteration 171/1000 | Loss: 0.00002501
Iteration 172/1000 | Loss: 0.00002501
Iteration 173/1000 | Loss: 0.00002501
Iteration 174/1000 | Loss: 0.00002501
Iteration 175/1000 | Loss: 0.00002501
Iteration 176/1000 | Loss: 0.00002500
Iteration 177/1000 | Loss: 0.00002500
Iteration 178/1000 | Loss: 0.00002500
Iteration 179/1000 | Loss: 0.00002500
Iteration 180/1000 | Loss: 0.00002499
Iteration 181/1000 | Loss: 0.00002499
Iteration 182/1000 | Loss: 0.00002499
Iteration 183/1000 | Loss: 0.00002499
Iteration 184/1000 | Loss: 0.00002499
Iteration 185/1000 | Loss: 0.00002499
Iteration 186/1000 | Loss: 0.00002499
Iteration 187/1000 | Loss: 0.00002499
Iteration 188/1000 | Loss: 0.00002498
Iteration 189/1000 | Loss: 0.00002497
Iteration 190/1000 | Loss: 0.00002497
Iteration 191/1000 | Loss: 0.00002496
Iteration 192/1000 | Loss: 0.00002496
Iteration 193/1000 | Loss: 0.00002496
Iteration 194/1000 | Loss: 0.00002496
Iteration 195/1000 | Loss: 0.00002496
Iteration 196/1000 | Loss: 0.00002496
Iteration 197/1000 | Loss: 0.00002496
Iteration 198/1000 | Loss: 0.00002495
Iteration 199/1000 | Loss: 0.00002495
Iteration 200/1000 | Loss: 0.00002495
Iteration 201/1000 | Loss: 0.00002495
Iteration 202/1000 | Loss: 0.00002495
Iteration 203/1000 | Loss: 0.00002494
Iteration 204/1000 | Loss: 0.00002494
Iteration 205/1000 | Loss: 0.00002494
Iteration 206/1000 | Loss: 0.00002494
Iteration 207/1000 | Loss: 0.00002494
Iteration 208/1000 | Loss: 0.00002494
Iteration 209/1000 | Loss: 0.00002494
Iteration 210/1000 | Loss: 0.00002494
Iteration 211/1000 | Loss: 0.00002494
Iteration 212/1000 | Loss: 0.00002493
Iteration 213/1000 | Loss: 0.00002493
Iteration 214/1000 | Loss: 0.00002493
Iteration 215/1000 | Loss: 0.00002493
Iteration 216/1000 | Loss: 0.00002493
Iteration 217/1000 | Loss: 0.00002492
Iteration 218/1000 | Loss: 0.00002492
Iteration 219/1000 | Loss: 0.00002492
Iteration 220/1000 | Loss: 0.00002492
Iteration 221/1000 | Loss: 0.00002492
Iteration 222/1000 | Loss: 0.00002492
Iteration 223/1000 | Loss: 0.00002492
Iteration 224/1000 | Loss: 0.00002492
Iteration 225/1000 | Loss: 0.00002492
Iteration 226/1000 | Loss: 0.00002492
Iteration 227/1000 | Loss: 0.00002492
Iteration 228/1000 | Loss: 0.00002492
Iteration 229/1000 | Loss: 0.00002492
Iteration 230/1000 | Loss: 0.00002492
Iteration 231/1000 | Loss: 0.00002492
Iteration 232/1000 | Loss: 0.00002492
Iteration 233/1000 | Loss: 0.00002492
Iteration 234/1000 | Loss: 0.00002492
Iteration 235/1000 | Loss: 0.00002492
Iteration 236/1000 | Loss: 0.00002492
Iteration 237/1000 | Loss: 0.00002492
Iteration 238/1000 | Loss: 0.00002491
Iteration 239/1000 | Loss: 0.00002491
Iteration 240/1000 | Loss: 0.00002491
Iteration 241/1000 | Loss: 0.00002491
Iteration 242/1000 | Loss: 0.00002491
Iteration 243/1000 | Loss: 0.00002491
Iteration 244/1000 | Loss: 0.00002491
Iteration 245/1000 | Loss: 0.00002491
Iteration 246/1000 | Loss: 0.00002491
Iteration 247/1000 | Loss: 0.00002491
Iteration 248/1000 | Loss: 0.00002491
Iteration 249/1000 | Loss: 0.00002491
Iteration 250/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.4914887035265565e-05, 2.4914887035265565e-05, 2.4914887035265565e-05, 2.4914887035265565e-05, 2.4914887035265565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4914887035265565e-05

Optimization complete. Final v2v error: 4.084439754486084 mm

Highest mean error: 6.343414783477783 mm for frame 146

Lowest mean error: 3.447389841079712 mm for frame 64

Saving results

Total time: 263.0958399772644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967145
Iteration 2/25 | Loss: 0.00168905
Iteration 3/25 | Loss: 0.00137147
Iteration 4/25 | Loss: 0.00131140
Iteration 5/25 | Loss: 0.00129047
Iteration 6/25 | Loss: 0.00128517
Iteration 7/25 | Loss: 0.00128329
Iteration 8/25 | Loss: 0.00128368
Iteration 9/25 | Loss: 0.00128527
Iteration 10/25 | Loss: 0.00127598
Iteration 11/25 | Loss: 0.00127858
Iteration 12/25 | Loss: 0.00127232
Iteration 13/25 | Loss: 0.00127181
Iteration 14/25 | Loss: 0.00127066
Iteration 15/25 | Loss: 0.00127010
Iteration 16/25 | Loss: 0.00127036
Iteration 17/25 | Loss: 0.00126965
Iteration 18/25 | Loss: 0.00126992
Iteration 19/25 | Loss: 0.00126843
Iteration 20/25 | Loss: 0.00126758
Iteration 21/25 | Loss: 0.00126715
Iteration 22/25 | Loss: 0.00126703
Iteration 23/25 | Loss: 0.00126703
Iteration 24/25 | Loss: 0.00126703
Iteration 25/25 | Loss: 0.00126703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53986788
Iteration 2/25 | Loss: 0.00104780
Iteration 3/25 | Loss: 0.00104780
Iteration 4/25 | Loss: 0.00104780
Iteration 5/25 | Loss: 0.00104780
Iteration 6/25 | Loss: 0.00104780
Iteration 7/25 | Loss: 0.00104780
Iteration 8/25 | Loss: 0.00104780
Iteration 9/25 | Loss: 0.00104780
Iteration 10/25 | Loss: 0.00104780
Iteration 11/25 | Loss: 0.00104780
Iteration 12/25 | Loss: 0.00104780
Iteration 13/25 | Loss: 0.00104780
Iteration 14/25 | Loss: 0.00104780
Iteration 15/25 | Loss: 0.00104780
Iteration 16/25 | Loss: 0.00104780
Iteration 17/25 | Loss: 0.00104780
Iteration 18/25 | Loss: 0.00104780
Iteration 19/25 | Loss: 0.00104780
Iteration 20/25 | Loss: 0.00104780
Iteration 21/25 | Loss: 0.00104780
Iteration 22/25 | Loss: 0.00104780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010477987816557288, 0.0010477987816557288, 0.0010477987816557288, 0.0010477987816557288, 0.0010477987816557288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010477987816557288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104780
Iteration 2/1000 | Loss: 0.00003405
Iteration 3/1000 | Loss: 0.00002286
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001843
Iteration 6/1000 | Loss: 0.00001753
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001656
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001605
Iteration 11/1000 | Loss: 0.00001582
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001577
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001559
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001557
Iteration 21/1000 | Loss: 0.00001556
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001553
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001547
Iteration 31/1000 | Loss: 0.00001547
Iteration 32/1000 | Loss: 0.00001547
Iteration 33/1000 | Loss: 0.00001547
Iteration 34/1000 | Loss: 0.00001547
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001545
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001542
Iteration 51/1000 | Loss: 0.00001542
Iteration 52/1000 | Loss: 0.00001541
Iteration 53/1000 | Loss: 0.00001541
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001540
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001539
Iteration 59/1000 | Loss: 0.00001539
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001538
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001536
Iteration 68/1000 | Loss: 0.00001536
Iteration 69/1000 | Loss: 0.00001536
Iteration 70/1000 | Loss: 0.00001536
Iteration 71/1000 | Loss: 0.00001536
Iteration 72/1000 | Loss: 0.00001535
Iteration 73/1000 | Loss: 0.00001535
Iteration 74/1000 | Loss: 0.00001535
Iteration 75/1000 | Loss: 0.00001535
Iteration 76/1000 | Loss: 0.00001535
Iteration 77/1000 | Loss: 0.00001535
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001534
Iteration 84/1000 | Loss: 0.00001534
Iteration 85/1000 | Loss: 0.00001534
Iteration 86/1000 | Loss: 0.00001534
Iteration 87/1000 | Loss: 0.00001534
Iteration 88/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.5343766790465452e-05, 1.5343766790465452e-05, 1.5343766790465452e-05, 1.5343766790465452e-05, 1.5343766790465452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5343766790465452e-05

Optimization complete. Final v2v error: 3.3349244594573975 mm

Highest mean error: 3.933926582336426 mm for frame 177

Lowest mean error: 3.0539772510528564 mm for frame 94

Saving results

Total time: 71.27647376060486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812397
Iteration 2/25 | Loss: 0.00128501
Iteration 3/25 | Loss: 0.00119999
Iteration 4/25 | Loss: 0.00119127
Iteration 5/25 | Loss: 0.00118968
Iteration 6/25 | Loss: 0.00118968
Iteration 7/25 | Loss: 0.00118968
Iteration 8/25 | Loss: 0.00118968
Iteration 9/25 | Loss: 0.00118968
Iteration 10/25 | Loss: 0.00118968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011896786745637655, 0.0011896786745637655, 0.0011896786745637655, 0.0011896786745637655, 0.0011896786745637655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011896786745637655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44055784
Iteration 2/25 | Loss: 0.00068358
Iteration 3/25 | Loss: 0.00068358
Iteration 4/25 | Loss: 0.00068358
Iteration 5/25 | Loss: 0.00068358
Iteration 6/25 | Loss: 0.00068357
Iteration 7/25 | Loss: 0.00068357
Iteration 8/25 | Loss: 0.00068357
Iteration 9/25 | Loss: 0.00068357
Iteration 10/25 | Loss: 0.00068357
Iteration 11/25 | Loss: 0.00068357
Iteration 12/25 | Loss: 0.00068357
Iteration 13/25 | Loss: 0.00068357
Iteration 14/25 | Loss: 0.00068357
Iteration 15/25 | Loss: 0.00068357
Iteration 16/25 | Loss: 0.00068357
Iteration 17/25 | Loss: 0.00068357
Iteration 18/25 | Loss: 0.00068357
Iteration 19/25 | Loss: 0.00068357
Iteration 20/25 | Loss: 0.00068357
Iteration 21/25 | Loss: 0.00068357
Iteration 22/25 | Loss: 0.00068357
Iteration 23/25 | Loss: 0.00068357
Iteration 24/25 | Loss: 0.00068357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006835730746388435, 0.0006835730746388435, 0.0006835730746388435, 0.0006835730746388435, 0.0006835730746388435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006835730746388435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068357
Iteration 2/1000 | Loss: 0.00002580
Iteration 3/1000 | Loss: 0.00001720
Iteration 4/1000 | Loss: 0.00001515
Iteration 5/1000 | Loss: 0.00001424
Iteration 6/1000 | Loss: 0.00001348
Iteration 7/1000 | Loss: 0.00001286
Iteration 8/1000 | Loss: 0.00001271
Iteration 9/1000 | Loss: 0.00001250
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001189
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001180
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001169
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001161
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001158
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001157
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001157
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001152
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001144
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001127
Iteration 76/1000 | Loss: 0.00001127
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001127
Iteration 80/1000 | Loss: 0.00001127
Iteration 81/1000 | Loss: 0.00001127
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001126
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001123
Iteration 90/1000 | Loss: 0.00001123
Iteration 91/1000 | Loss: 0.00001123
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001122
Iteration 96/1000 | Loss: 0.00001121
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001121
Iteration 99/1000 | Loss: 0.00001120
Iteration 100/1000 | Loss: 0.00001120
Iteration 101/1000 | Loss: 0.00001120
Iteration 102/1000 | Loss: 0.00001120
Iteration 103/1000 | Loss: 0.00001120
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001119
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001118
Iteration 113/1000 | Loss: 0.00001118
Iteration 114/1000 | Loss: 0.00001118
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001117
Iteration 123/1000 | Loss: 0.00001117
Iteration 124/1000 | Loss: 0.00001117
Iteration 125/1000 | Loss: 0.00001117
Iteration 126/1000 | Loss: 0.00001117
Iteration 127/1000 | Loss: 0.00001117
Iteration 128/1000 | Loss: 0.00001117
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001115
Iteration 137/1000 | Loss: 0.00001115
Iteration 138/1000 | Loss: 0.00001115
Iteration 139/1000 | Loss: 0.00001115
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001114
Iteration 142/1000 | Loss: 0.00001114
Iteration 143/1000 | Loss: 0.00001114
Iteration 144/1000 | Loss: 0.00001114
Iteration 145/1000 | Loss: 0.00001114
Iteration 146/1000 | Loss: 0.00001114
Iteration 147/1000 | Loss: 0.00001114
Iteration 148/1000 | Loss: 0.00001114
Iteration 149/1000 | Loss: 0.00001113
Iteration 150/1000 | Loss: 0.00001113
Iteration 151/1000 | Loss: 0.00001113
Iteration 152/1000 | Loss: 0.00001113
Iteration 153/1000 | Loss: 0.00001113
Iteration 154/1000 | Loss: 0.00001112
Iteration 155/1000 | Loss: 0.00001112
Iteration 156/1000 | Loss: 0.00001112
Iteration 157/1000 | Loss: 0.00001112
Iteration 158/1000 | Loss: 0.00001111
Iteration 159/1000 | Loss: 0.00001111
Iteration 160/1000 | Loss: 0.00001111
Iteration 161/1000 | Loss: 0.00001111
Iteration 162/1000 | Loss: 0.00001111
Iteration 163/1000 | Loss: 0.00001111
Iteration 164/1000 | Loss: 0.00001111
Iteration 165/1000 | Loss: 0.00001111
Iteration 166/1000 | Loss: 0.00001110
Iteration 167/1000 | Loss: 0.00001110
Iteration 168/1000 | Loss: 0.00001110
Iteration 169/1000 | Loss: 0.00001110
Iteration 170/1000 | Loss: 0.00001110
Iteration 171/1000 | Loss: 0.00001110
Iteration 172/1000 | Loss: 0.00001110
Iteration 173/1000 | Loss: 0.00001110
Iteration 174/1000 | Loss: 0.00001110
Iteration 175/1000 | Loss: 0.00001110
Iteration 176/1000 | Loss: 0.00001109
Iteration 177/1000 | Loss: 0.00001109
Iteration 178/1000 | Loss: 0.00001109
Iteration 179/1000 | Loss: 0.00001109
Iteration 180/1000 | Loss: 0.00001109
Iteration 181/1000 | Loss: 0.00001109
Iteration 182/1000 | Loss: 0.00001108
Iteration 183/1000 | Loss: 0.00001108
Iteration 184/1000 | Loss: 0.00001108
Iteration 185/1000 | Loss: 0.00001108
Iteration 186/1000 | Loss: 0.00001107
Iteration 187/1000 | Loss: 0.00001107
Iteration 188/1000 | Loss: 0.00001107
Iteration 189/1000 | Loss: 0.00001106
Iteration 190/1000 | Loss: 0.00001106
Iteration 191/1000 | Loss: 0.00001106
Iteration 192/1000 | Loss: 0.00001106
Iteration 193/1000 | Loss: 0.00001106
Iteration 194/1000 | Loss: 0.00001106
Iteration 195/1000 | Loss: 0.00001106
Iteration 196/1000 | Loss: 0.00001106
Iteration 197/1000 | Loss: 0.00001106
Iteration 198/1000 | Loss: 0.00001106
Iteration 199/1000 | Loss: 0.00001105
Iteration 200/1000 | Loss: 0.00001105
Iteration 201/1000 | Loss: 0.00001105
Iteration 202/1000 | Loss: 0.00001105
Iteration 203/1000 | Loss: 0.00001105
Iteration 204/1000 | Loss: 0.00001105
Iteration 205/1000 | Loss: 0.00001105
Iteration 206/1000 | Loss: 0.00001105
Iteration 207/1000 | Loss: 0.00001105
Iteration 208/1000 | Loss: 0.00001105
Iteration 209/1000 | Loss: 0.00001105
Iteration 210/1000 | Loss: 0.00001105
Iteration 211/1000 | Loss: 0.00001105
Iteration 212/1000 | Loss: 0.00001105
Iteration 213/1000 | Loss: 0.00001105
Iteration 214/1000 | Loss: 0.00001105
Iteration 215/1000 | Loss: 0.00001105
Iteration 216/1000 | Loss: 0.00001105
Iteration 217/1000 | Loss: 0.00001105
Iteration 218/1000 | Loss: 0.00001105
Iteration 219/1000 | Loss: 0.00001105
Iteration 220/1000 | Loss: 0.00001105
Iteration 221/1000 | Loss: 0.00001105
Iteration 222/1000 | Loss: 0.00001105
Iteration 223/1000 | Loss: 0.00001105
Iteration 224/1000 | Loss: 0.00001105
Iteration 225/1000 | Loss: 0.00001105
Iteration 226/1000 | Loss: 0.00001105
Iteration 227/1000 | Loss: 0.00001105
Iteration 228/1000 | Loss: 0.00001105
Iteration 229/1000 | Loss: 0.00001105
Iteration 230/1000 | Loss: 0.00001105
Iteration 231/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.1045253813790623e-05, 1.1045253813790623e-05, 1.1045253813790623e-05, 1.1045253813790623e-05, 1.1045253813790623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1045253813790623e-05

Optimization complete. Final v2v error: 2.8331851959228516 mm

Highest mean error: 3.0062005519866943 mm for frame 57

Lowest mean error: 2.701251268386841 mm for frame 204

Saving results

Total time: 49.04519271850586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00644777
Iteration 2/25 | Loss: 0.00150509
Iteration 3/25 | Loss: 0.00131632
Iteration 4/25 | Loss: 0.00128213
Iteration 5/25 | Loss: 0.00126811
Iteration 6/25 | Loss: 0.00127129
Iteration 7/25 | Loss: 0.00126864
Iteration 8/25 | Loss: 0.00126228
Iteration 9/25 | Loss: 0.00125875
Iteration 10/25 | Loss: 0.00125670
Iteration 11/25 | Loss: 0.00125664
Iteration 12/25 | Loss: 0.00125632
Iteration 13/25 | Loss: 0.00125518
Iteration 14/25 | Loss: 0.00125511
Iteration 15/25 | Loss: 0.00125506
Iteration 16/25 | Loss: 0.00125506
Iteration 17/25 | Loss: 0.00125506
Iteration 18/25 | Loss: 0.00125506
Iteration 19/25 | Loss: 0.00125506
Iteration 20/25 | Loss: 0.00125505
Iteration 21/25 | Loss: 0.00125505
Iteration 22/25 | Loss: 0.00125505
Iteration 23/25 | Loss: 0.00125504
Iteration 24/25 | Loss: 0.00125504
Iteration 25/25 | Loss: 0.00125504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.78645611
Iteration 2/25 | Loss: 0.00076641
Iteration 3/25 | Loss: 0.00076641
Iteration 4/25 | Loss: 0.00075515
Iteration 5/25 | Loss: 0.00075515
Iteration 6/25 | Loss: 0.00075515
Iteration 7/25 | Loss: 0.00075515
Iteration 8/25 | Loss: 0.00075515
Iteration 9/25 | Loss: 0.00075515
Iteration 10/25 | Loss: 0.00075515
Iteration 11/25 | Loss: 0.00075515
Iteration 12/25 | Loss: 0.00075515
Iteration 13/25 | Loss: 0.00075515
Iteration 14/25 | Loss: 0.00075515
Iteration 15/25 | Loss: 0.00075515
Iteration 16/25 | Loss: 0.00075515
Iteration 17/25 | Loss: 0.00075515
Iteration 18/25 | Loss: 0.00075515
Iteration 19/25 | Loss: 0.00075515
Iteration 20/25 | Loss: 0.00075515
Iteration 21/25 | Loss: 0.00075515
Iteration 22/25 | Loss: 0.00075515
Iteration 23/25 | Loss: 0.00075515
Iteration 24/25 | Loss: 0.00075515
Iteration 25/25 | Loss: 0.00075515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075515
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002285
Iteration 4/1000 | Loss: 0.00002744
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002103
Iteration 7/1000 | Loss: 0.00006631
Iteration 8/1000 | Loss: 0.00033775
Iteration 9/1000 | Loss: 0.00031167
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00002504
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001781
Iteration 14/1000 | Loss: 0.00012701
Iteration 15/1000 | Loss: 0.00012232
Iteration 16/1000 | Loss: 0.00013090
Iteration 17/1000 | Loss: 0.00002831
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002065
Iteration 20/1000 | Loss: 0.00003794
Iteration 21/1000 | Loss: 0.00002097
Iteration 22/1000 | Loss: 0.00002385
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001784
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001762
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00003694
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00006841
Iteration 33/1000 | Loss: 0.00003833
Iteration 34/1000 | Loss: 0.00001660
Iteration 35/1000 | Loss: 0.00002311
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001647
Iteration 39/1000 | Loss: 0.00001647
Iteration 40/1000 | Loss: 0.00001647
Iteration 41/1000 | Loss: 0.00001647
Iteration 42/1000 | Loss: 0.00001646
Iteration 43/1000 | Loss: 0.00001646
Iteration 44/1000 | Loss: 0.00001646
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001638
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001638
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001638
Iteration 64/1000 | Loss: 0.00001638
Iteration 65/1000 | Loss: 0.00001638
Iteration 66/1000 | Loss: 0.00001638
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001638
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001638
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001638
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.6382728063035756e-05, 1.6382728063035756e-05, 1.6382728063035756e-05, 1.6382728063035756e-05, 1.6382728063035756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6382728063035756e-05

Optimization complete. Final v2v error: 3.3548688888549805 mm

Highest mean error: 5.823490142822266 mm for frame 130

Lowest mean error: 2.968160629272461 mm for frame 228

Saving results

Total time: 85.09091234207153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00687114
Iteration 2/25 | Loss: 0.00171351
Iteration 3/25 | Loss: 0.00143016
Iteration 4/25 | Loss: 0.00141666
Iteration 5/25 | Loss: 0.00141317
Iteration 6/25 | Loss: 0.00141295
Iteration 7/25 | Loss: 0.00141295
Iteration 8/25 | Loss: 0.00141295
Iteration 9/25 | Loss: 0.00141295
Iteration 10/25 | Loss: 0.00141295
Iteration 11/25 | Loss: 0.00141295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014129544142633677, 0.0014129544142633677, 0.0014129544142633677, 0.0014129544142633677, 0.0014129544142633677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014129544142633677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95984125
Iteration 2/25 | Loss: 0.00106704
Iteration 3/25 | Loss: 0.00106704
Iteration 4/25 | Loss: 0.00106704
Iteration 5/25 | Loss: 0.00106704
Iteration 6/25 | Loss: 0.00106704
Iteration 7/25 | Loss: 0.00106704
Iteration 8/25 | Loss: 0.00106704
Iteration 9/25 | Loss: 0.00106704
Iteration 10/25 | Loss: 0.00106704
Iteration 11/25 | Loss: 0.00106704
Iteration 12/25 | Loss: 0.00106704
Iteration 13/25 | Loss: 0.00106704
Iteration 14/25 | Loss: 0.00106704
Iteration 15/25 | Loss: 0.00106704
Iteration 16/25 | Loss: 0.00106704
Iteration 17/25 | Loss: 0.00106704
Iteration 18/25 | Loss: 0.00106704
Iteration 19/25 | Loss: 0.00106704
Iteration 20/25 | Loss: 0.00106704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010670399060472846, 0.0010670399060472846, 0.0010670399060472846, 0.0010670399060472846, 0.0010670399060472846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670399060472846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106704
Iteration 2/1000 | Loss: 0.00005675
Iteration 3/1000 | Loss: 0.00003596
Iteration 4/1000 | Loss: 0.00003275
Iteration 5/1000 | Loss: 0.00003048
Iteration 6/1000 | Loss: 0.00002904
Iteration 7/1000 | Loss: 0.00002818
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002729
Iteration 10/1000 | Loss: 0.00002700
Iteration 11/1000 | Loss: 0.00002682
Iteration 12/1000 | Loss: 0.00002676
Iteration 13/1000 | Loss: 0.00002660
Iteration 14/1000 | Loss: 0.00002649
Iteration 15/1000 | Loss: 0.00002646
Iteration 16/1000 | Loss: 0.00002634
Iteration 17/1000 | Loss: 0.00002632
Iteration 18/1000 | Loss: 0.00002632
Iteration 19/1000 | Loss: 0.00002631
Iteration 20/1000 | Loss: 0.00002631
Iteration 21/1000 | Loss: 0.00002630
Iteration 22/1000 | Loss: 0.00002630
Iteration 23/1000 | Loss: 0.00002630
Iteration 24/1000 | Loss: 0.00002629
Iteration 25/1000 | Loss: 0.00002629
Iteration 26/1000 | Loss: 0.00002629
Iteration 27/1000 | Loss: 0.00002628
Iteration 28/1000 | Loss: 0.00002628
Iteration 29/1000 | Loss: 0.00002628
Iteration 30/1000 | Loss: 0.00002628
Iteration 31/1000 | Loss: 0.00002627
Iteration 32/1000 | Loss: 0.00002627
Iteration 33/1000 | Loss: 0.00002627
Iteration 34/1000 | Loss: 0.00002626
Iteration 35/1000 | Loss: 0.00002626
Iteration 36/1000 | Loss: 0.00002626
Iteration 37/1000 | Loss: 0.00002626
Iteration 38/1000 | Loss: 0.00002626
Iteration 39/1000 | Loss: 0.00002625
Iteration 40/1000 | Loss: 0.00002625
Iteration 41/1000 | Loss: 0.00002625
Iteration 42/1000 | Loss: 0.00002624
Iteration 43/1000 | Loss: 0.00002624
Iteration 44/1000 | Loss: 0.00002624
Iteration 45/1000 | Loss: 0.00002624
Iteration 46/1000 | Loss: 0.00002624
Iteration 47/1000 | Loss: 0.00002624
Iteration 48/1000 | Loss: 0.00002624
Iteration 49/1000 | Loss: 0.00002623
Iteration 50/1000 | Loss: 0.00002622
Iteration 51/1000 | Loss: 0.00002622
Iteration 52/1000 | Loss: 0.00002622
Iteration 53/1000 | Loss: 0.00002621
Iteration 54/1000 | Loss: 0.00002621
Iteration 55/1000 | Loss: 0.00002621
Iteration 56/1000 | Loss: 0.00002621
Iteration 57/1000 | Loss: 0.00002621
Iteration 58/1000 | Loss: 0.00002621
Iteration 59/1000 | Loss: 0.00002620
Iteration 60/1000 | Loss: 0.00002620
Iteration 61/1000 | Loss: 0.00002620
Iteration 62/1000 | Loss: 0.00002620
Iteration 63/1000 | Loss: 0.00002620
Iteration 64/1000 | Loss: 0.00002620
Iteration 65/1000 | Loss: 0.00002620
Iteration 66/1000 | Loss: 0.00002619
Iteration 67/1000 | Loss: 0.00002619
Iteration 68/1000 | Loss: 0.00002619
Iteration 69/1000 | Loss: 0.00002619
Iteration 70/1000 | Loss: 0.00002619
Iteration 71/1000 | Loss: 0.00002619
Iteration 72/1000 | Loss: 0.00002619
Iteration 73/1000 | Loss: 0.00002619
Iteration 74/1000 | Loss: 0.00002618
Iteration 75/1000 | Loss: 0.00002618
Iteration 76/1000 | Loss: 0.00002618
Iteration 77/1000 | Loss: 0.00002618
Iteration 78/1000 | Loss: 0.00002618
Iteration 79/1000 | Loss: 0.00002618
Iteration 80/1000 | Loss: 0.00002618
Iteration 81/1000 | Loss: 0.00002618
Iteration 82/1000 | Loss: 0.00002618
Iteration 83/1000 | Loss: 0.00002618
Iteration 84/1000 | Loss: 0.00002617
Iteration 85/1000 | Loss: 0.00002617
Iteration 86/1000 | Loss: 0.00002617
Iteration 87/1000 | Loss: 0.00002617
Iteration 88/1000 | Loss: 0.00002617
Iteration 89/1000 | Loss: 0.00002617
Iteration 90/1000 | Loss: 0.00002617
Iteration 91/1000 | Loss: 0.00002617
Iteration 92/1000 | Loss: 0.00002617
Iteration 93/1000 | Loss: 0.00002617
Iteration 94/1000 | Loss: 0.00002617
Iteration 95/1000 | Loss: 0.00002616
Iteration 96/1000 | Loss: 0.00002616
Iteration 97/1000 | Loss: 0.00002616
Iteration 98/1000 | Loss: 0.00002615
Iteration 99/1000 | Loss: 0.00002615
Iteration 100/1000 | Loss: 0.00002615
Iteration 101/1000 | Loss: 0.00002615
Iteration 102/1000 | Loss: 0.00002615
Iteration 103/1000 | Loss: 0.00002615
Iteration 104/1000 | Loss: 0.00002615
Iteration 105/1000 | Loss: 0.00002615
Iteration 106/1000 | Loss: 0.00002615
Iteration 107/1000 | Loss: 0.00002615
Iteration 108/1000 | Loss: 0.00002615
Iteration 109/1000 | Loss: 0.00002614
Iteration 110/1000 | Loss: 0.00002614
Iteration 111/1000 | Loss: 0.00002614
Iteration 112/1000 | Loss: 0.00002614
Iteration 113/1000 | Loss: 0.00002614
Iteration 114/1000 | Loss: 0.00002614
Iteration 115/1000 | Loss: 0.00002614
Iteration 116/1000 | Loss: 0.00002614
Iteration 117/1000 | Loss: 0.00002614
Iteration 118/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.6144027287955396e-05, 2.6144027287955396e-05, 2.6144027287955396e-05, 2.6144027287955396e-05, 2.6144027287955396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6144027287955396e-05

Optimization complete. Final v2v error: 4.298293590545654 mm

Highest mean error: 4.587878227233887 mm for frame 217

Lowest mean error: 4.048372745513916 mm for frame 132

Saving results

Total time: 40.331398487091064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440740
Iteration 2/25 | Loss: 0.00143766
Iteration 3/25 | Loss: 0.00126820
Iteration 4/25 | Loss: 0.00125811
Iteration 5/25 | Loss: 0.00125639
Iteration 6/25 | Loss: 0.00125605
Iteration 7/25 | Loss: 0.00125605
Iteration 8/25 | Loss: 0.00125605
Iteration 9/25 | Loss: 0.00125605
Iteration 10/25 | Loss: 0.00125605
Iteration 11/25 | Loss: 0.00125605
Iteration 12/25 | Loss: 0.00125605
Iteration 13/25 | Loss: 0.00125605
Iteration 14/25 | Loss: 0.00125605
Iteration 15/25 | Loss: 0.00125605
Iteration 16/25 | Loss: 0.00125605
Iteration 17/25 | Loss: 0.00125605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012560479808598757, 0.0012560479808598757, 0.0012560479808598757, 0.0012560479808598757, 0.0012560479808598757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012560479808598757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.35588121
Iteration 2/25 | Loss: 0.00078156
Iteration 3/25 | Loss: 0.00078152
Iteration 4/25 | Loss: 0.00078152
Iteration 5/25 | Loss: 0.00078152
Iteration 6/25 | Loss: 0.00078152
Iteration 7/25 | Loss: 0.00078152
Iteration 8/25 | Loss: 0.00078152
Iteration 9/25 | Loss: 0.00078152
Iteration 10/25 | Loss: 0.00078152
Iteration 11/25 | Loss: 0.00078152
Iteration 12/25 | Loss: 0.00078152
Iteration 13/25 | Loss: 0.00078152
Iteration 14/25 | Loss: 0.00078152
Iteration 15/25 | Loss: 0.00078152
Iteration 16/25 | Loss: 0.00078152
Iteration 17/25 | Loss: 0.00078152
Iteration 18/25 | Loss: 0.00078152
Iteration 19/25 | Loss: 0.00078152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007815157296136022, 0.0007815157296136022, 0.0007815157296136022, 0.0007815157296136022, 0.0007815157296136022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007815157296136022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078152
Iteration 2/1000 | Loss: 0.00003680
Iteration 3/1000 | Loss: 0.00002454
Iteration 4/1000 | Loss: 0.00001998
Iteration 5/1000 | Loss: 0.00001870
Iteration 6/1000 | Loss: 0.00001777
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001595
Iteration 14/1000 | Loss: 0.00001588
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001576
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001572
Iteration 20/1000 | Loss: 0.00001571
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001564
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001562
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001561
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001561
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00001560
Iteration 36/1000 | Loss: 0.00001560
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001559
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001556
Iteration 46/1000 | Loss: 0.00001555
Iteration 47/1000 | Loss: 0.00001554
Iteration 48/1000 | Loss: 0.00001554
Iteration 49/1000 | Loss: 0.00001554
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001553
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001551
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001548
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001548
Iteration 66/1000 | Loss: 0.00001548
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001547
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001545
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001544
Iteration 86/1000 | Loss: 0.00001544
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001541
Iteration 96/1000 | Loss: 0.00001541
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001539
Iteration 102/1000 | Loss: 0.00001539
Iteration 103/1000 | Loss: 0.00001539
Iteration 104/1000 | Loss: 0.00001539
Iteration 105/1000 | Loss: 0.00001539
Iteration 106/1000 | Loss: 0.00001538
Iteration 107/1000 | Loss: 0.00001538
Iteration 108/1000 | Loss: 0.00001538
Iteration 109/1000 | Loss: 0.00001537
Iteration 110/1000 | Loss: 0.00001537
Iteration 111/1000 | Loss: 0.00001537
Iteration 112/1000 | Loss: 0.00001537
Iteration 113/1000 | Loss: 0.00001537
Iteration 114/1000 | Loss: 0.00001537
Iteration 115/1000 | Loss: 0.00001536
Iteration 116/1000 | Loss: 0.00001536
Iteration 117/1000 | Loss: 0.00001536
Iteration 118/1000 | Loss: 0.00001536
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001535
Iteration 121/1000 | Loss: 0.00001535
Iteration 122/1000 | Loss: 0.00001535
Iteration 123/1000 | Loss: 0.00001535
Iteration 124/1000 | Loss: 0.00001535
Iteration 125/1000 | Loss: 0.00001534
Iteration 126/1000 | Loss: 0.00001534
Iteration 127/1000 | Loss: 0.00001534
Iteration 128/1000 | Loss: 0.00001534
Iteration 129/1000 | Loss: 0.00001534
Iteration 130/1000 | Loss: 0.00001534
Iteration 131/1000 | Loss: 0.00001534
Iteration 132/1000 | Loss: 0.00001534
Iteration 133/1000 | Loss: 0.00001533
Iteration 134/1000 | Loss: 0.00001533
Iteration 135/1000 | Loss: 0.00001533
Iteration 136/1000 | Loss: 0.00001533
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001532
Iteration 139/1000 | Loss: 0.00001532
Iteration 140/1000 | Loss: 0.00001532
Iteration 141/1000 | Loss: 0.00001532
Iteration 142/1000 | Loss: 0.00001532
Iteration 143/1000 | Loss: 0.00001532
Iteration 144/1000 | Loss: 0.00001532
Iteration 145/1000 | Loss: 0.00001532
Iteration 146/1000 | Loss: 0.00001531
Iteration 147/1000 | Loss: 0.00001531
Iteration 148/1000 | Loss: 0.00001531
Iteration 149/1000 | Loss: 0.00001531
Iteration 150/1000 | Loss: 0.00001531
Iteration 151/1000 | Loss: 0.00001531
Iteration 152/1000 | Loss: 0.00001531
Iteration 153/1000 | Loss: 0.00001531
Iteration 154/1000 | Loss: 0.00001531
Iteration 155/1000 | Loss: 0.00001531
Iteration 156/1000 | Loss: 0.00001531
Iteration 157/1000 | Loss: 0.00001531
Iteration 158/1000 | Loss: 0.00001531
Iteration 159/1000 | Loss: 0.00001531
Iteration 160/1000 | Loss: 0.00001531
Iteration 161/1000 | Loss: 0.00001531
Iteration 162/1000 | Loss: 0.00001531
Iteration 163/1000 | Loss: 0.00001531
Iteration 164/1000 | Loss: 0.00001531
Iteration 165/1000 | Loss: 0.00001531
Iteration 166/1000 | Loss: 0.00001531
Iteration 167/1000 | Loss: 0.00001531
Iteration 168/1000 | Loss: 0.00001531
Iteration 169/1000 | Loss: 0.00001531
Iteration 170/1000 | Loss: 0.00001531
Iteration 171/1000 | Loss: 0.00001531
Iteration 172/1000 | Loss: 0.00001531
Iteration 173/1000 | Loss: 0.00001531
Iteration 174/1000 | Loss: 0.00001531
Iteration 175/1000 | Loss: 0.00001531
Iteration 176/1000 | Loss: 0.00001531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.5311574316001497e-05, 1.5311574316001497e-05, 1.5311574316001497e-05, 1.5311574316001497e-05, 1.5311574316001497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5311574316001497e-05

Optimization complete. Final v2v error: 3.2950243949890137 mm

Highest mean error: 4.032205581665039 mm for frame 71

Lowest mean error: 2.87168025970459 mm for frame 101

Saving results

Total time: 39.11633110046387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486908
Iteration 2/25 | Loss: 0.00171927
Iteration 3/25 | Loss: 0.00131907
Iteration 4/25 | Loss: 0.00126830
Iteration 5/25 | Loss: 0.00126195
Iteration 6/25 | Loss: 0.00126060
Iteration 7/25 | Loss: 0.00126060
Iteration 8/25 | Loss: 0.00126060
Iteration 9/25 | Loss: 0.00126060
Iteration 10/25 | Loss: 0.00126060
Iteration 11/25 | Loss: 0.00126060
Iteration 12/25 | Loss: 0.00126060
Iteration 13/25 | Loss: 0.00126060
Iteration 14/25 | Loss: 0.00126060
Iteration 15/25 | Loss: 0.00126060
Iteration 16/25 | Loss: 0.00126060
Iteration 17/25 | Loss: 0.00126060
Iteration 18/25 | Loss: 0.00126060
Iteration 19/25 | Loss: 0.00126060
Iteration 20/25 | Loss: 0.00126060
Iteration 21/25 | Loss: 0.00126060
Iteration 22/25 | Loss: 0.00126060
Iteration 23/25 | Loss: 0.00126060
Iteration 24/25 | Loss: 0.00126060
Iteration 25/25 | Loss: 0.00126060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49882698
Iteration 2/25 | Loss: 0.00058260
Iteration 3/25 | Loss: 0.00058260
Iteration 4/25 | Loss: 0.00058260
Iteration 5/25 | Loss: 0.00058260
Iteration 6/25 | Loss: 0.00058260
Iteration 7/25 | Loss: 0.00058260
Iteration 8/25 | Loss: 0.00058260
Iteration 9/25 | Loss: 0.00058260
Iteration 10/25 | Loss: 0.00058260
Iteration 11/25 | Loss: 0.00058260
Iteration 12/25 | Loss: 0.00058260
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005826007691211998, 0.0005826007691211998, 0.0005826007691211998, 0.0005826007691211998, 0.0005826007691211998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005826007691211998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058260
Iteration 2/1000 | Loss: 0.00003914
Iteration 3/1000 | Loss: 0.00002771
Iteration 4/1000 | Loss: 0.00002307
Iteration 5/1000 | Loss: 0.00002175
Iteration 6/1000 | Loss: 0.00002076
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001959
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001860
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001825
Iteration 14/1000 | Loss: 0.00001817
Iteration 15/1000 | Loss: 0.00001810
Iteration 16/1000 | Loss: 0.00001809
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00001800
Iteration 20/1000 | Loss: 0.00001799
Iteration 21/1000 | Loss: 0.00001798
Iteration 22/1000 | Loss: 0.00001797
Iteration 23/1000 | Loss: 0.00001797
Iteration 24/1000 | Loss: 0.00001796
Iteration 25/1000 | Loss: 0.00001796
Iteration 26/1000 | Loss: 0.00001795
Iteration 27/1000 | Loss: 0.00001795
Iteration 28/1000 | Loss: 0.00001794
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001792
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001788
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001786
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.7782020222512074e-05, 1.7782020222512074e-05, 1.7782020222512074e-05, 1.7782020222512074e-05, 1.7782020222512074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7782020222512074e-05

Optimization complete. Final v2v error: 3.5515778064727783 mm

Highest mean error: 4.045496463775635 mm for frame 133

Lowest mean error: 3.1900718212127686 mm for frame 4

Saving results

Total time: 38.74530816078186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806668
Iteration 2/25 | Loss: 0.00166416
Iteration 3/25 | Loss: 0.00135694
Iteration 4/25 | Loss: 0.00133253
Iteration 5/25 | Loss: 0.00132848
Iteration 6/25 | Loss: 0.00132752
Iteration 7/25 | Loss: 0.00132752
Iteration 8/25 | Loss: 0.00132752
Iteration 9/25 | Loss: 0.00132752
Iteration 10/25 | Loss: 0.00132752
Iteration 11/25 | Loss: 0.00132752
Iteration 12/25 | Loss: 0.00132752
Iteration 13/25 | Loss: 0.00132752
Iteration 14/25 | Loss: 0.00132752
Iteration 15/25 | Loss: 0.00132752
Iteration 16/25 | Loss: 0.00132752
Iteration 17/25 | Loss: 0.00132752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013275167439132929, 0.0013275167439132929, 0.0013275167439132929, 0.0013275167439132929, 0.0013275167439132929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013275167439132929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44812644
Iteration 2/25 | Loss: 0.00049590
Iteration 3/25 | Loss: 0.00049590
Iteration 4/25 | Loss: 0.00049590
Iteration 5/25 | Loss: 0.00049590
Iteration 6/25 | Loss: 0.00049590
Iteration 7/25 | Loss: 0.00049590
Iteration 8/25 | Loss: 0.00049590
Iteration 9/25 | Loss: 0.00049590
Iteration 10/25 | Loss: 0.00049590
Iteration 11/25 | Loss: 0.00049590
Iteration 12/25 | Loss: 0.00049590
Iteration 13/25 | Loss: 0.00049590
Iteration 14/25 | Loss: 0.00049590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0004958967329002917, 0.0004958967329002917, 0.0004958967329002917, 0.0004958967329002917, 0.0004958967329002917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004958967329002917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049590
Iteration 2/1000 | Loss: 0.00006293
Iteration 3/1000 | Loss: 0.00004603
Iteration 4/1000 | Loss: 0.00004377
Iteration 5/1000 | Loss: 0.00004196
Iteration 6/1000 | Loss: 0.00004086
Iteration 7/1000 | Loss: 0.00004004
Iteration 8/1000 | Loss: 0.00003940
Iteration 9/1000 | Loss: 0.00003901
Iteration 10/1000 | Loss: 0.00003872
Iteration 11/1000 | Loss: 0.00003867
Iteration 12/1000 | Loss: 0.00003847
Iteration 13/1000 | Loss: 0.00003840
Iteration 14/1000 | Loss: 0.00003839
Iteration 15/1000 | Loss: 0.00003830
Iteration 16/1000 | Loss: 0.00003828
Iteration 17/1000 | Loss: 0.00003828
Iteration 18/1000 | Loss: 0.00003828
Iteration 19/1000 | Loss: 0.00003827
Iteration 20/1000 | Loss: 0.00003826
Iteration 21/1000 | Loss: 0.00003826
Iteration 22/1000 | Loss: 0.00003826
Iteration 23/1000 | Loss: 0.00003826
Iteration 24/1000 | Loss: 0.00003825
Iteration 25/1000 | Loss: 0.00003825
Iteration 26/1000 | Loss: 0.00003825
Iteration 27/1000 | Loss: 0.00003823
Iteration 28/1000 | Loss: 0.00003823
Iteration 29/1000 | Loss: 0.00003823
Iteration 30/1000 | Loss: 0.00003823
Iteration 31/1000 | Loss: 0.00003823
Iteration 32/1000 | Loss: 0.00003823
Iteration 33/1000 | Loss: 0.00003823
Iteration 34/1000 | Loss: 0.00003823
Iteration 35/1000 | Loss: 0.00003822
Iteration 36/1000 | Loss: 0.00003821
Iteration 37/1000 | Loss: 0.00003821
Iteration 38/1000 | Loss: 0.00003821
Iteration 39/1000 | Loss: 0.00003821
Iteration 40/1000 | Loss: 0.00003821
Iteration 41/1000 | Loss: 0.00003821
Iteration 42/1000 | Loss: 0.00003821
Iteration 43/1000 | Loss: 0.00003820
Iteration 44/1000 | Loss: 0.00003820
Iteration 45/1000 | Loss: 0.00003819
Iteration 46/1000 | Loss: 0.00003819
Iteration 47/1000 | Loss: 0.00003819
Iteration 48/1000 | Loss: 0.00003819
Iteration 49/1000 | Loss: 0.00003818
Iteration 50/1000 | Loss: 0.00003817
Iteration 51/1000 | Loss: 0.00003817
Iteration 52/1000 | Loss: 0.00003817
Iteration 53/1000 | Loss: 0.00003817
Iteration 54/1000 | Loss: 0.00003817
Iteration 55/1000 | Loss: 0.00003817
Iteration 56/1000 | Loss: 0.00003817
Iteration 57/1000 | Loss: 0.00003817
Iteration 58/1000 | Loss: 0.00003817
Iteration 59/1000 | Loss: 0.00003817
Iteration 60/1000 | Loss: 0.00003817
Iteration 61/1000 | Loss: 0.00003817
Iteration 62/1000 | Loss: 0.00003817
Iteration 63/1000 | Loss: 0.00003816
Iteration 64/1000 | Loss: 0.00003816
Iteration 65/1000 | Loss: 0.00003816
Iteration 66/1000 | Loss: 0.00003816
Iteration 67/1000 | Loss: 0.00003816
Iteration 68/1000 | Loss: 0.00003816
Iteration 69/1000 | Loss: 0.00003816
Iteration 70/1000 | Loss: 0.00003816
Iteration 71/1000 | Loss: 0.00003816
Iteration 72/1000 | Loss: 0.00003815
Iteration 73/1000 | Loss: 0.00003815
Iteration 74/1000 | Loss: 0.00003815
Iteration 75/1000 | Loss: 0.00003815
Iteration 76/1000 | Loss: 0.00003815
Iteration 77/1000 | Loss: 0.00003815
Iteration 78/1000 | Loss: 0.00003815
Iteration 79/1000 | Loss: 0.00003815
Iteration 80/1000 | Loss: 0.00003815
Iteration 81/1000 | Loss: 0.00003815
Iteration 82/1000 | Loss: 0.00003815
Iteration 83/1000 | Loss: 0.00003815
Iteration 84/1000 | Loss: 0.00003815
Iteration 85/1000 | Loss: 0.00003815
Iteration 86/1000 | Loss: 0.00003815
Iteration 87/1000 | Loss: 0.00003815
Iteration 88/1000 | Loss: 0.00003815
Iteration 89/1000 | Loss: 0.00003815
Iteration 90/1000 | Loss: 0.00003815
Iteration 91/1000 | Loss: 0.00003815
Iteration 92/1000 | Loss: 0.00003815
Iteration 93/1000 | Loss: 0.00003815
Iteration 94/1000 | Loss: 0.00003815
Iteration 95/1000 | Loss: 0.00003815
Iteration 96/1000 | Loss: 0.00003815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.814853698713705e-05, 3.814853698713705e-05, 3.814853698713705e-05, 3.814853698713705e-05, 3.814853698713705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.814853698713705e-05

Optimization complete. Final v2v error: 5.0546064376831055 mm

Highest mean error: 5.428873538970947 mm for frame 42

Lowest mean error: 4.541344165802002 mm for frame 120

Saving results

Total time: 31.378864765167236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765616
Iteration 2/25 | Loss: 0.00155890
Iteration 3/25 | Loss: 0.00135665
Iteration 4/25 | Loss: 0.00132017
Iteration 5/25 | Loss: 0.00130166
Iteration 6/25 | Loss: 0.00132456
Iteration 7/25 | Loss: 0.00129949
Iteration 8/25 | Loss: 0.00129368
Iteration 9/25 | Loss: 0.00128401
Iteration 10/25 | Loss: 0.00127766
Iteration 11/25 | Loss: 0.00127275
Iteration 12/25 | Loss: 0.00127111
Iteration 13/25 | Loss: 0.00126881
Iteration 14/25 | Loss: 0.00126846
Iteration 15/25 | Loss: 0.00126835
Iteration 16/25 | Loss: 0.00126833
Iteration 17/25 | Loss: 0.00126832
Iteration 18/25 | Loss: 0.00126832
Iteration 19/25 | Loss: 0.00126832
Iteration 20/25 | Loss: 0.00126832
Iteration 21/25 | Loss: 0.00126832
Iteration 22/25 | Loss: 0.00126832
Iteration 23/25 | Loss: 0.00126832
Iteration 24/25 | Loss: 0.00126832
Iteration 25/25 | Loss: 0.00126832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88122284
Iteration 2/25 | Loss: 0.00069413
Iteration 3/25 | Loss: 0.00069412
Iteration 4/25 | Loss: 0.00069412
Iteration 5/25 | Loss: 0.00069412
Iteration 6/25 | Loss: 0.00069412
Iteration 7/25 | Loss: 0.00069412
Iteration 8/25 | Loss: 0.00069412
Iteration 9/25 | Loss: 0.00069412
Iteration 10/25 | Loss: 0.00069412
Iteration 11/25 | Loss: 0.00069411
Iteration 12/25 | Loss: 0.00069411
Iteration 13/25 | Loss: 0.00069411
Iteration 14/25 | Loss: 0.00069411
Iteration 15/25 | Loss: 0.00069411
Iteration 16/25 | Loss: 0.00069411
Iteration 17/25 | Loss: 0.00069411
Iteration 18/25 | Loss: 0.00069411
Iteration 19/25 | Loss: 0.00069411
Iteration 20/25 | Loss: 0.00069411
Iteration 21/25 | Loss: 0.00069411
Iteration 22/25 | Loss: 0.00069411
Iteration 23/25 | Loss: 0.00069411
Iteration 24/25 | Loss: 0.00069411
Iteration 25/25 | Loss: 0.00069411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006941138999536633, 0.0006941138999536633, 0.0006941138999536633, 0.0006941138999536633, 0.0006941138999536633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006941138999536633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069411
Iteration 2/1000 | Loss: 0.00003978
Iteration 3/1000 | Loss: 0.00005736
Iteration 4/1000 | Loss: 0.00002362
Iteration 5/1000 | Loss: 0.00002245
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002107
Iteration 8/1000 | Loss: 0.00002066
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002026
Iteration 11/1000 | Loss: 0.00002009
Iteration 12/1000 | Loss: 0.00002004
Iteration 13/1000 | Loss: 0.00002001
Iteration 14/1000 | Loss: 0.00002000
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001982
Iteration 18/1000 | Loss: 0.00001968
Iteration 19/1000 | Loss: 0.00001960
Iteration 20/1000 | Loss: 0.00001956
Iteration 21/1000 | Loss: 0.00001956
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001953
Iteration 24/1000 | Loss: 0.00001953
Iteration 25/1000 | Loss: 0.00001953
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001953
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001953
Iteration 31/1000 | Loss: 0.00001952
Iteration 32/1000 | Loss: 0.00001952
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001952
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001947
Iteration 46/1000 | Loss: 0.00001947
Iteration 47/1000 | Loss: 0.00001946
Iteration 48/1000 | Loss: 0.00001945
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001945
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001943
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00001942
Iteration 58/1000 | Loss: 0.00001941
Iteration 59/1000 | Loss: 0.00001941
Iteration 60/1000 | Loss: 0.00001940
Iteration 61/1000 | Loss: 0.00001940
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001936
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001936
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001935
Iteration 75/1000 | Loss: 0.00001935
Iteration 76/1000 | Loss: 0.00001935
Iteration 77/1000 | Loss: 0.00001935
Iteration 78/1000 | Loss: 0.00001935
Iteration 79/1000 | Loss: 0.00001935
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001933
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001931
Iteration 89/1000 | Loss: 0.00001931
Iteration 90/1000 | Loss: 0.00001931
Iteration 91/1000 | Loss: 0.00001931
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001930
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001930
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001929
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001929
Iteration 102/1000 | Loss: 0.00001928
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00001928
Iteration 105/1000 | Loss: 0.00001928
Iteration 106/1000 | Loss: 0.00001927
Iteration 107/1000 | Loss: 0.00001927
Iteration 108/1000 | Loss: 0.00001927
Iteration 109/1000 | Loss: 0.00001927
Iteration 110/1000 | Loss: 0.00001927
Iteration 111/1000 | Loss: 0.00001926
Iteration 112/1000 | Loss: 0.00001926
Iteration 113/1000 | Loss: 0.00001926
Iteration 114/1000 | Loss: 0.00001926
Iteration 115/1000 | Loss: 0.00001926
Iteration 116/1000 | Loss: 0.00001926
Iteration 117/1000 | Loss: 0.00001926
Iteration 118/1000 | Loss: 0.00001926
Iteration 119/1000 | Loss: 0.00001926
Iteration 120/1000 | Loss: 0.00001926
Iteration 121/1000 | Loss: 0.00001926
Iteration 122/1000 | Loss: 0.00001926
Iteration 123/1000 | Loss: 0.00001926
Iteration 124/1000 | Loss: 0.00001926
Iteration 125/1000 | Loss: 0.00001926
Iteration 126/1000 | Loss: 0.00001926
Iteration 127/1000 | Loss: 0.00001925
Iteration 128/1000 | Loss: 0.00001925
Iteration 129/1000 | Loss: 0.00001925
Iteration 130/1000 | Loss: 0.00001925
Iteration 131/1000 | Loss: 0.00001925
Iteration 132/1000 | Loss: 0.00001925
Iteration 133/1000 | Loss: 0.00001925
Iteration 134/1000 | Loss: 0.00001925
Iteration 135/1000 | Loss: 0.00001925
Iteration 136/1000 | Loss: 0.00001925
Iteration 137/1000 | Loss: 0.00001925
Iteration 138/1000 | Loss: 0.00001925
Iteration 139/1000 | Loss: 0.00001924
Iteration 140/1000 | Loss: 0.00001924
Iteration 141/1000 | Loss: 0.00001924
Iteration 142/1000 | Loss: 0.00001924
Iteration 143/1000 | Loss: 0.00001923
Iteration 144/1000 | Loss: 0.00001923
Iteration 145/1000 | Loss: 0.00001923
Iteration 146/1000 | Loss: 0.00001923
Iteration 147/1000 | Loss: 0.00001923
Iteration 148/1000 | Loss: 0.00001923
Iteration 149/1000 | Loss: 0.00001923
Iteration 150/1000 | Loss: 0.00001923
Iteration 151/1000 | Loss: 0.00001923
Iteration 152/1000 | Loss: 0.00001923
Iteration 153/1000 | Loss: 0.00001923
Iteration 154/1000 | Loss: 0.00001923
Iteration 155/1000 | Loss: 0.00001923
Iteration 156/1000 | Loss: 0.00001923
Iteration 157/1000 | Loss: 0.00001923
Iteration 158/1000 | Loss: 0.00001923
Iteration 159/1000 | Loss: 0.00001923
Iteration 160/1000 | Loss: 0.00001923
Iteration 161/1000 | Loss: 0.00001923
Iteration 162/1000 | Loss: 0.00001923
Iteration 163/1000 | Loss: 0.00001923
Iteration 164/1000 | Loss: 0.00001923
Iteration 165/1000 | Loss: 0.00001923
Iteration 166/1000 | Loss: 0.00001923
Iteration 167/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.9231198166380636e-05, 1.9231198166380636e-05, 1.9231198166380636e-05, 1.9231198166380636e-05, 1.9231198166380636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9231198166380636e-05

Optimization complete. Final v2v error: 3.5646519660949707 mm

Highest mean error: 4.469931125640869 mm for frame 115

Lowest mean error: 3.112649917602539 mm for frame 23

Saving results

Total time: 57.42353701591492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020616
Iteration 2/25 | Loss: 0.00166081
Iteration 3/25 | Loss: 0.00141990
Iteration 4/25 | Loss: 0.00139541
Iteration 5/25 | Loss: 0.00138722
Iteration 6/25 | Loss: 0.00138524
Iteration 7/25 | Loss: 0.00138524
Iteration 8/25 | Loss: 0.00138524
Iteration 9/25 | Loss: 0.00138524
Iteration 10/25 | Loss: 0.00138524
Iteration 11/25 | Loss: 0.00138524
Iteration 12/25 | Loss: 0.00138524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001385238952934742, 0.001385238952934742, 0.001385238952934742, 0.001385238952934742, 0.001385238952934742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001385238952934742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97287548
Iteration 2/25 | Loss: 0.00105507
Iteration 3/25 | Loss: 0.00105504
Iteration 4/25 | Loss: 0.00105504
Iteration 5/25 | Loss: 0.00105504
Iteration 6/25 | Loss: 0.00105504
Iteration 7/25 | Loss: 0.00105504
Iteration 8/25 | Loss: 0.00105504
Iteration 9/25 | Loss: 0.00105504
Iteration 10/25 | Loss: 0.00105504
Iteration 11/25 | Loss: 0.00105504
Iteration 12/25 | Loss: 0.00105504
Iteration 13/25 | Loss: 0.00105504
Iteration 14/25 | Loss: 0.00105504
Iteration 15/25 | Loss: 0.00105504
Iteration 16/25 | Loss: 0.00105504
Iteration 17/25 | Loss: 0.00105504
Iteration 18/25 | Loss: 0.00105504
Iteration 19/25 | Loss: 0.00105504
Iteration 20/25 | Loss: 0.00105504
Iteration 21/25 | Loss: 0.00105504
Iteration 22/25 | Loss: 0.00105504
Iteration 23/25 | Loss: 0.00105504
Iteration 24/25 | Loss: 0.00105504
Iteration 25/25 | Loss: 0.00105504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105504
Iteration 2/1000 | Loss: 0.00009798
Iteration 3/1000 | Loss: 0.00004903
Iteration 4/1000 | Loss: 0.00003803
Iteration 5/1000 | Loss: 0.00003524
Iteration 6/1000 | Loss: 0.00003373
Iteration 7/1000 | Loss: 0.00003273
Iteration 8/1000 | Loss: 0.00003205
Iteration 9/1000 | Loss: 0.00003132
Iteration 10/1000 | Loss: 0.00003089
Iteration 11/1000 | Loss: 0.00003049
Iteration 12/1000 | Loss: 0.00003021
Iteration 13/1000 | Loss: 0.00002999
Iteration 14/1000 | Loss: 0.00002981
Iteration 15/1000 | Loss: 0.00002959
Iteration 16/1000 | Loss: 0.00002946
Iteration 17/1000 | Loss: 0.00002943
Iteration 18/1000 | Loss: 0.00002937
Iteration 19/1000 | Loss: 0.00002925
Iteration 20/1000 | Loss: 0.00002923
Iteration 21/1000 | Loss: 0.00002923
Iteration 22/1000 | Loss: 0.00002921
Iteration 23/1000 | Loss: 0.00002920
Iteration 24/1000 | Loss: 0.00002917
Iteration 25/1000 | Loss: 0.00002915
Iteration 26/1000 | Loss: 0.00002915
Iteration 27/1000 | Loss: 0.00002915
Iteration 28/1000 | Loss: 0.00002914
Iteration 29/1000 | Loss: 0.00002914
Iteration 30/1000 | Loss: 0.00002914
Iteration 31/1000 | Loss: 0.00002913
Iteration 32/1000 | Loss: 0.00002913
Iteration 33/1000 | Loss: 0.00002913
Iteration 34/1000 | Loss: 0.00002912
Iteration 35/1000 | Loss: 0.00002912
Iteration 36/1000 | Loss: 0.00002911
Iteration 37/1000 | Loss: 0.00002911
Iteration 38/1000 | Loss: 0.00002911
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00002911
Iteration 41/1000 | Loss: 0.00002911
Iteration 42/1000 | Loss: 0.00002910
Iteration 43/1000 | Loss: 0.00002910
Iteration 44/1000 | Loss: 0.00002910
Iteration 45/1000 | Loss: 0.00002910
Iteration 46/1000 | Loss: 0.00002910
Iteration 47/1000 | Loss: 0.00002909
Iteration 48/1000 | Loss: 0.00002909
Iteration 49/1000 | Loss: 0.00002909
Iteration 50/1000 | Loss: 0.00002908
Iteration 51/1000 | Loss: 0.00002908
Iteration 52/1000 | Loss: 0.00002908
Iteration 53/1000 | Loss: 0.00002908
Iteration 54/1000 | Loss: 0.00002907
Iteration 55/1000 | Loss: 0.00002907
Iteration 56/1000 | Loss: 0.00002906
Iteration 57/1000 | Loss: 0.00002905
Iteration 58/1000 | Loss: 0.00002905
Iteration 59/1000 | Loss: 0.00002904
Iteration 60/1000 | Loss: 0.00002903
Iteration 61/1000 | Loss: 0.00002903
Iteration 62/1000 | Loss: 0.00002903
Iteration 63/1000 | Loss: 0.00002903
Iteration 64/1000 | Loss: 0.00002902
Iteration 65/1000 | Loss: 0.00002902
Iteration 66/1000 | Loss: 0.00002902
Iteration 67/1000 | Loss: 0.00002902
Iteration 68/1000 | Loss: 0.00002901
Iteration 69/1000 | Loss: 0.00002901
Iteration 70/1000 | Loss: 0.00002901
Iteration 71/1000 | Loss: 0.00002900
Iteration 72/1000 | Loss: 0.00002900
Iteration 73/1000 | Loss: 0.00002900
Iteration 74/1000 | Loss: 0.00002900
Iteration 75/1000 | Loss: 0.00002899
Iteration 76/1000 | Loss: 0.00002899
Iteration 77/1000 | Loss: 0.00002899
Iteration 78/1000 | Loss: 0.00002898
Iteration 79/1000 | Loss: 0.00002898
Iteration 80/1000 | Loss: 0.00002898
Iteration 81/1000 | Loss: 0.00002897
Iteration 82/1000 | Loss: 0.00002897
Iteration 83/1000 | Loss: 0.00002896
Iteration 84/1000 | Loss: 0.00002896
Iteration 85/1000 | Loss: 0.00002896
Iteration 86/1000 | Loss: 0.00002896
Iteration 87/1000 | Loss: 0.00002895
Iteration 88/1000 | Loss: 0.00002894
Iteration 89/1000 | Loss: 0.00002893
Iteration 90/1000 | Loss: 0.00002893
Iteration 91/1000 | Loss: 0.00002893
Iteration 92/1000 | Loss: 0.00002892
Iteration 93/1000 | Loss: 0.00002892
Iteration 94/1000 | Loss: 0.00002892
Iteration 95/1000 | Loss: 0.00002891
Iteration 96/1000 | Loss: 0.00002891
Iteration 97/1000 | Loss: 0.00002891
Iteration 98/1000 | Loss: 0.00002891
Iteration 99/1000 | Loss: 0.00002891
Iteration 100/1000 | Loss: 0.00002891
Iteration 101/1000 | Loss: 0.00002891
Iteration 102/1000 | Loss: 0.00002891
Iteration 103/1000 | Loss: 0.00002890
Iteration 104/1000 | Loss: 0.00002890
Iteration 105/1000 | Loss: 0.00002890
Iteration 106/1000 | Loss: 0.00002889
Iteration 107/1000 | Loss: 0.00002889
Iteration 108/1000 | Loss: 0.00002889
Iteration 109/1000 | Loss: 0.00002889
Iteration 110/1000 | Loss: 0.00002888
Iteration 111/1000 | Loss: 0.00002888
Iteration 112/1000 | Loss: 0.00002888
Iteration 113/1000 | Loss: 0.00002888
Iteration 114/1000 | Loss: 0.00002888
Iteration 115/1000 | Loss: 0.00002887
Iteration 116/1000 | Loss: 0.00002887
Iteration 117/1000 | Loss: 0.00002887
Iteration 118/1000 | Loss: 0.00002887
Iteration 119/1000 | Loss: 0.00002887
Iteration 120/1000 | Loss: 0.00002887
Iteration 121/1000 | Loss: 0.00002887
Iteration 122/1000 | Loss: 0.00002887
Iteration 123/1000 | Loss: 0.00002887
Iteration 124/1000 | Loss: 0.00002886
Iteration 125/1000 | Loss: 0.00002886
Iteration 126/1000 | Loss: 0.00002886
Iteration 127/1000 | Loss: 0.00002886
Iteration 128/1000 | Loss: 0.00002886
Iteration 129/1000 | Loss: 0.00002886
Iteration 130/1000 | Loss: 0.00002886
Iteration 131/1000 | Loss: 0.00002886
Iteration 132/1000 | Loss: 0.00002886
Iteration 133/1000 | Loss: 0.00002885
Iteration 134/1000 | Loss: 0.00002885
Iteration 135/1000 | Loss: 0.00002885
Iteration 136/1000 | Loss: 0.00002885
Iteration 137/1000 | Loss: 0.00002885
Iteration 138/1000 | Loss: 0.00002885
Iteration 139/1000 | Loss: 0.00002885
Iteration 140/1000 | Loss: 0.00002885
Iteration 141/1000 | Loss: 0.00002885
Iteration 142/1000 | Loss: 0.00002885
Iteration 143/1000 | Loss: 0.00002885
Iteration 144/1000 | Loss: 0.00002885
Iteration 145/1000 | Loss: 0.00002884
Iteration 146/1000 | Loss: 0.00002884
Iteration 147/1000 | Loss: 0.00002884
Iteration 148/1000 | Loss: 0.00002884
Iteration 149/1000 | Loss: 0.00002884
Iteration 150/1000 | Loss: 0.00002884
Iteration 151/1000 | Loss: 0.00002884
Iteration 152/1000 | Loss: 0.00002884
Iteration 153/1000 | Loss: 0.00002884
Iteration 154/1000 | Loss: 0.00002883
Iteration 155/1000 | Loss: 0.00002883
Iteration 156/1000 | Loss: 0.00002883
Iteration 157/1000 | Loss: 0.00002883
Iteration 158/1000 | Loss: 0.00002883
Iteration 159/1000 | Loss: 0.00002882
Iteration 160/1000 | Loss: 0.00002882
Iteration 161/1000 | Loss: 0.00002882
Iteration 162/1000 | Loss: 0.00002882
Iteration 163/1000 | Loss: 0.00002882
Iteration 164/1000 | Loss: 0.00002882
Iteration 165/1000 | Loss: 0.00002882
Iteration 166/1000 | Loss: 0.00002882
Iteration 167/1000 | Loss: 0.00002882
Iteration 168/1000 | Loss: 0.00002881
Iteration 169/1000 | Loss: 0.00002881
Iteration 170/1000 | Loss: 0.00002881
Iteration 171/1000 | Loss: 0.00002881
Iteration 172/1000 | Loss: 0.00002881
Iteration 173/1000 | Loss: 0.00002881
Iteration 174/1000 | Loss: 0.00002881
Iteration 175/1000 | Loss: 0.00002881
Iteration 176/1000 | Loss: 0.00002881
Iteration 177/1000 | Loss: 0.00002881
Iteration 178/1000 | Loss: 0.00002881
Iteration 179/1000 | Loss: 0.00002881
Iteration 180/1000 | Loss: 0.00002881
Iteration 181/1000 | Loss: 0.00002881
Iteration 182/1000 | Loss: 0.00002881
Iteration 183/1000 | Loss: 0.00002880
Iteration 184/1000 | Loss: 0.00002880
Iteration 185/1000 | Loss: 0.00002880
Iteration 186/1000 | Loss: 0.00002880
Iteration 187/1000 | Loss: 0.00002880
Iteration 188/1000 | Loss: 0.00002880
Iteration 189/1000 | Loss: 0.00002880
Iteration 190/1000 | Loss: 0.00002880
Iteration 191/1000 | Loss: 0.00002880
Iteration 192/1000 | Loss: 0.00002880
Iteration 193/1000 | Loss: 0.00002880
Iteration 194/1000 | Loss: 0.00002880
Iteration 195/1000 | Loss: 0.00002879
Iteration 196/1000 | Loss: 0.00002879
Iteration 197/1000 | Loss: 0.00002879
Iteration 198/1000 | Loss: 0.00002879
Iteration 199/1000 | Loss: 0.00002879
Iteration 200/1000 | Loss: 0.00002879
Iteration 201/1000 | Loss: 0.00002879
Iteration 202/1000 | Loss: 0.00002879
Iteration 203/1000 | Loss: 0.00002879
Iteration 204/1000 | Loss: 0.00002879
Iteration 205/1000 | Loss: 0.00002879
Iteration 206/1000 | Loss: 0.00002879
Iteration 207/1000 | Loss: 0.00002879
Iteration 208/1000 | Loss: 0.00002879
Iteration 209/1000 | Loss: 0.00002879
Iteration 210/1000 | Loss: 0.00002879
Iteration 211/1000 | Loss: 0.00002878
Iteration 212/1000 | Loss: 0.00002878
Iteration 213/1000 | Loss: 0.00002878
Iteration 214/1000 | Loss: 0.00002878
Iteration 215/1000 | Loss: 0.00002878
Iteration 216/1000 | Loss: 0.00002878
Iteration 217/1000 | Loss: 0.00002878
Iteration 218/1000 | Loss: 0.00002878
Iteration 219/1000 | Loss: 0.00002878
Iteration 220/1000 | Loss: 0.00002878
Iteration 221/1000 | Loss: 0.00002878
Iteration 222/1000 | Loss: 0.00002878
Iteration 223/1000 | Loss: 0.00002877
Iteration 224/1000 | Loss: 0.00002877
Iteration 225/1000 | Loss: 0.00002877
Iteration 226/1000 | Loss: 0.00002877
Iteration 227/1000 | Loss: 0.00002877
Iteration 228/1000 | Loss: 0.00002877
Iteration 229/1000 | Loss: 0.00002877
Iteration 230/1000 | Loss: 0.00002877
Iteration 231/1000 | Loss: 0.00002877
Iteration 232/1000 | Loss: 0.00002877
Iteration 233/1000 | Loss: 0.00002877
Iteration 234/1000 | Loss: 0.00002877
Iteration 235/1000 | Loss: 0.00002877
Iteration 236/1000 | Loss: 0.00002877
Iteration 237/1000 | Loss: 0.00002877
Iteration 238/1000 | Loss: 0.00002877
Iteration 239/1000 | Loss: 0.00002877
Iteration 240/1000 | Loss: 0.00002877
Iteration 241/1000 | Loss: 0.00002877
Iteration 242/1000 | Loss: 0.00002877
Iteration 243/1000 | Loss: 0.00002877
Iteration 244/1000 | Loss: 0.00002877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [2.8767703042831272e-05, 2.8767703042831272e-05, 2.8767703042831272e-05, 2.8767703042831272e-05, 2.8767703042831272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8767703042831272e-05

Optimization complete. Final v2v error: 4.400696277618408 mm

Highest mean error: 5.306424140930176 mm for frame 51

Lowest mean error: 3.7087576389312744 mm for frame 27

Saving results

Total time: 52.40899324417114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006552
Iteration 2/25 | Loss: 0.00279246
Iteration 3/25 | Loss: 0.00237518
Iteration 4/25 | Loss: 0.00213750
Iteration 5/25 | Loss: 0.00152372
Iteration 6/25 | Loss: 0.00152470
Iteration 7/25 | Loss: 0.00133355
Iteration 8/25 | Loss: 0.00137526
Iteration 9/25 | Loss: 0.00131975
Iteration 10/25 | Loss: 0.00129390
Iteration 11/25 | Loss: 0.00129885
Iteration 12/25 | Loss: 0.00127901
Iteration 13/25 | Loss: 0.00129083
Iteration 14/25 | Loss: 0.00127844
Iteration 15/25 | Loss: 0.00127843
Iteration 16/25 | Loss: 0.00127841
Iteration 17/25 | Loss: 0.00127841
Iteration 18/25 | Loss: 0.00127841
Iteration 19/25 | Loss: 0.00127841
Iteration 20/25 | Loss: 0.00127841
Iteration 21/25 | Loss: 0.00127841
Iteration 22/25 | Loss: 0.00127841
Iteration 23/25 | Loss: 0.00127841
Iteration 24/25 | Loss: 0.00127841
Iteration 25/25 | Loss: 0.00127841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40496314
Iteration 2/25 | Loss: 0.00075087
Iteration 3/25 | Loss: 0.00075087
Iteration 4/25 | Loss: 0.00075087
Iteration 5/25 | Loss: 0.00075087
Iteration 6/25 | Loss: 0.00075087
Iteration 7/25 | Loss: 0.00075087
Iteration 8/25 | Loss: 0.00075087
Iteration 9/25 | Loss: 0.00075087
Iteration 10/25 | Loss: 0.00075087
Iteration 11/25 | Loss: 0.00075087
Iteration 12/25 | Loss: 0.00075087
Iteration 13/25 | Loss: 0.00075087
Iteration 14/25 | Loss: 0.00075087
Iteration 15/25 | Loss: 0.00075087
Iteration 16/25 | Loss: 0.00075087
Iteration 17/25 | Loss: 0.00075087
Iteration 18/25 | Loss: 0.00075087
Iteration 19/25 | Loss: 0.00075087
Iteration 20/25 | Loss: 0.00075087
Iteration 21/25 | Loss: 0.00075087
Iteration 22/25 | Loss: 0.00075087
Iteration 23/25 | Loss: 0.00075087
Iteration 24/25 | Loss: 0.00075087
Iteration 25/25 | Loss: 0.00075087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007508692797273397, 0.0007508692797273397, 0.0007508692797273397, 0.0007508692797273397, 0.0007508692797273397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007508692797273397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075087
Iteration 2/1000 | Loss: 0.00003788
Iteration 3/1000 | Loss: 0.00002482
Iteration 4/1000 | Loss: 0.00002229
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00002037
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00002002
Iteration 10/1000 | Loss: 0.00001984
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001970
Iteration 13/1000 | Loss: 0.00001970
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00001946
Iteration 22/1000 | Loss: 0.00001946
Iteration 23/1000 | Loss: 0.00001945
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001945
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001937
Iteration 28/1000 | Loss: 0.00001937
Iteration 29/1000 | Loss: 0.00001937
Iteration 30/1000 | Loss: 0.00001937
Iteration 31/1000 | Loss: 0.00001937
Iteration 32/1000 | Loss: 0.00001935
Iteration 33/1000 | Loss: 0.00001934
Iteration 34/1000 | Loss: 0.00001934
Iteration 35/1000 | Loss: 0.00001934
Iteration 36/1000 | Loss: 0.00001933
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001931
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001929
Iteration 48/1000 | Loss: 0.00001929
Iteration 49/1000 | Loss: 0.00001929
Iteration 50/1000 | Loss: 0.00001929
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001928
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001927
Iteration 58/1000 | Loss: 0.00001927
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001926
Iteration 66/1000 | Loss: 0.00001926
Iteration 67/1000 | Loss: 0.00001925
Iteration 68/1000 | Loss: 0.00001925
Iteration 69/1000 | Loss: 0.00001925
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00001925
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001925
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001925
Iteration 77/1000 | Loss: 0.00001925
Iteration 78/1000 | Loss: 0.00001925
Iteration 79/1000 | Loss: 0.00001925
Iteration 80/1000 | Loss: 0.00001925
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001924
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001922
Iteration 95/1000 | Loss: 0.00001922
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001922
Iteration 102/1000 | Loss: 0.00001922
Iteration 103/1000 | Loss: 0.00001922
Iteration 104/1000 | Loss: 0.00001922
Iteration 105/1000 | Loss: 0.00001922
Iteration 106/1000 | Loss: 0.00001922
Iteration 107/1000 | Loss: 0.00001922
Iteration 108/1000 | Loss: 0.00001922
Iteration 109/1000 | Loss: 0.00001922
Iteration 110/1000 | Loss: 0.00001922
Iteration 111/1000 | Loss: 0.00001922
Iteration 112/1000 | Loss: 0.00001922
Iteration 113/1000 | Loss: 0.00001922
Iteration 114/1000 | Loss: 0.00001922
Iteration 115/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.9218776287743822e-05, 1.9218776287743822e-05, 1.9218776287743822e-05, 1.9218776287743822e-05, 1.9218776287743822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9218776287743822e-05

Optimization complete. Final v2v error: 3.6472208499908447 mm

Highest mean error: 3.6946327686309814 mm for frame 99

Lowest mean error: 3.5408899784088135 mm for frame 19

Saving results

Total time: 48.818769216537476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819042
Iteration 2/25 | Loss: 0.00152610
Iteration 3/25 | Loss: 0.00128171
Iteration 4/25 | Loss: 0.00126251
Iteration 5/25 | Loss: 0.00126003
Iteration 6/25 | Loss: 0.00125980
Iteration 7/25 | Loss: 0.00125980
Iteration 8/25 | Loss: 0.00125980
Iteration 9/25 | Loss: 0.00125980
Iteration 10/25 | Loss: 0.00125980
Iteration 11/25 | Loss: 0.00125980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012597956229001284, 0.0012597956229001284, 0.0012597956229001284, 0.0012597956229001284, 0.0012597956229001284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012597956229001284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03542626
Iteration 2/25 | Loss: 0.00055767
Iteration 3/25 | Loss: 0.00055765
Iteration 4/25 | Loss: 0.00055765
Iteration 5/25 | Loss: 0.00055765
Iteration 6/25 | Loss: 0.00055765
Iteration 7/25 | Loss: 0.00055765
Iteration 8/25 | Loss: 0.00055765
Iteration 9/25 | Loss: 0.00055765
Iteration 10/25 | Loss: 0.00055765
Iteration 11/25 | Loss: 0.00055765
Iteration 12/25 | Loss: 0.00055765
Iteration 13/25 | Loss: 0.00055765
Iteration 14/25 | Loss: 0.00055765
Iteration 15/25 | Loss: 0.00055765
Iteration 16/25 | Loss: 0.00055765
Iteration 17/25 | Loss: 0.00055765
Iteration 18/25 | Loss: 0.00055765
Iteration 19/25 | Loss: 0.00055765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005576518597081304, 0.0005576518597081304, 0.0005576518597081304, 0.0005576518597081304, 0.0005576518597081304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005576518597081304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055765
Iteration 2/1000 | Loss: 0.00003459
Iteration 3/1000 | Loss: 0.00002525
Iteration 4/1000 | Loss: 0.00002238
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002090
Iteration 7/1000 | Loss: 0.00002037
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001970
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00001902
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001884
Iteration 16/1000 | Loss: 0.00001882
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001880
Iteration 20/1000 | Loss: 0.00001880
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001879
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001879
Iteration 25/1000 | Loss: 0.00001879
Iteration 26/1000 | Loss: 0.00001879
Iteration 27/1000 | Loss: 0.00001879
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00001879
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001878
Iteration 34/1000 | Loss: 0.00001878
Iteration 35/1000 | Loss: 0.00001878
Iteration 36/1000 | Loss: 0.00001876
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001874
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001867
Iteration 55/1000 | Loss: 0.00001867
Iteration 56/1000 | Loss: 0.00001867
Iteration 57/1000 | Loss: 0.00001867
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001866
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001865
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001858
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001857
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001857
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001856
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001855
Iteration 89/1000 | Loss: 0.00001855
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001853
Iteration 101/1000 | Loss: 0.00001852
Iteration 102/1000 | Loss: 0.00001852
Iteration 103/1000 | Loss: 0.00001852
Iteration 104/1000 | Loss: 0.00001852
Iteration 105/1000 | Loss: 0.00001851
Iteration 106/1000 | Loss: 0.00001851
Iteration 107/1000 | Loss: 0.00001851
Iteration 108/1000 | Loss: 0.00001851
Iteration 109/1000 | Loss: 0.00001851
Iteration 110/1000 | Loss: 0.00001851
Iteration 111/1000 | Loss: 0.00001851
Iteration 112/1000 | Loss: 0.00001851
Iteration 113/1000 | Loss: 0.00001850
Iteration 114/1000 | Loss: 0.00001850
Iteration 115/1000 | Loss: 0.00001850
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001849
Iteration 120/1000 | Loss: 0.00001849
Iteration 121/1000 | Loss: 0.00001849
Iteration 122/1000 | Loss: 0.00001849
Iteration 123/1000 | Loss: 0.00001849
Iteration 124/1000 | Loss: 0.00001849
Iteration 125/1000 | Loss: 0.00001849
Iteration 126/1000 | Loss: 0.00001849
Iteration 127/1000 | Loss: 0.00001849
Iteration 128/1000 | Loss: 0.00001849
Iteration 129/1000 | Loss: 0.00001849
Iteration 130/1000 | Loss: 0.00001849
Iteration 131/1000 | Loss: 0.00001849
Iteration 132/1000 | Loss: 0.00001849
Iteration 133/1000 | Loss: 0.00001849
Iteration 134/1000 | Loss: 0.00001848
Iteration 135/1000 | Loss: 0.00001848
Iteration 136/1000 | Loss: 0.00001848
Iteration 137/1000 | Loss: 0.00001848
Iteration 138/1000 | Loss: 0.00001848
Iteration 139/1000 | Loss: 0.00001848
Iteration 140/1000 | Loss: 0.00001848
Iteration 141/1000 | Loss: 0.00001848
Iteration 142/1000 | Loss: 0.00001848
Iteration 143/1000 | Loss: 0.00001848
Iteration 144/1000 | Loss: 0.00001848
Iteration 145/1000 | Loss: 0.00001848
Iteration 146/1000 | Loss: 0.00001848
Iteration 147/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.8475137039786205e-05, 1.8475137039786205e-05, 1.8475137039786205e-05, 1.8475137039786205e-05, 1.8475137039786205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8475137039786205e-05

Optimization complete. Final v2v error: 3.5646378993988037 mm

Highest mean error: 3.880709648132324 mm for frame 30

Lowest mean error: 3.383268117904663 mm for frame 128

Saving results

Total time: 37.61195421218872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909185
Iteration 2/25 | Loss: 0.00173051
Iteration 3/25 | Loss: 0.00138604
Iteration 4/25 | Loss: 0.00132689
Iteration 5/25 | Loss: 0.00131317
Iteration 6/25 | Loss: 0.00131058
Iteration 7/25 | Loss: 0.00130972
Iteration 8/25 | Loss: 0.00130972
Iteration 9/25 | Loss: 0.00130972
Iteration 10/25 | Loss: 0.00130972
Iteration 11/25 | Loss: 0.00130972
Iteration 12/25 | Loss: 0.00130972
Iteration 13/25 | Loss: 0.00130972
Iteration 14/25 | Loss: 0.00130972
Iteration 15/25 | Loss: 0.00130972
Iteration 16/25 | Loss: 0.00130972
Iteration 17/25 | Loss: 0.00130972
Iteration 18/25 | Loss: 0.00130972
Iteration 19/25 | Loss: 0.00130972
Iteration 20/25 | Loss: 0.00130972
Iteration 21/25 | Loss: 0.00130972
Iteration 22/25 | Loss: 0.00130972
Iteration 23/25 | Loss: 0.00130972
Iteration 24/25 | Loss: 0.00130972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013097153278067708, 0.0013097153278067708, 0.0013097153278067708, 0.0013097153278067708, 0.0013097153278067708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013097153278067708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53215539
Iteration 2/25 | Loss: 0.00084464
Iteration 3/25 | Loss: 0.00084454
Iteration 4/25 | Loss: 0.00084454
Iteration 5/25 | Loss: 0.00084454
Iteration 6/25 | Loss: 0.00084454
Iteration 7/25 | Loss: 0.00084454
Iteration 8/25 | Loss: 0.00084454
Iteration 9/25 | Loss: 0.00084454
Iteration 10/25 | Loss: 0.00084454
Iteration 11/25 | Loss: 0.00084454
Iteration 12/25 | Loss: 0.00084454
Iteration 13/25 | Loss: 0.00084454
Iteration 14/25 | Loss: 0.00084454
Iteration 15/25 | Loss: 0.00084454
Iteration 16/25 | Loss: 0.00084454
Iteration 17/25 | Loss: 0.00084454
Iteration 18/25 | Loss: 0.00084454
Iteration 19/25 | Loss: 0.00084454
Iteration 20/25 | Loss: 0.00084454
Iteration 21/25 | Loss: 0.00084454
Iteration 22/25 | Loss: 0.00084454
Iteration 23/25 | Loss: 0.00084454
Iteration 24/25 | Loss: 0.00084454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008445418206974864, 0.0008445418206974864, 0.0008445418206974864, 0.0008445418206974864, 0.0008445418206974864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008445418206974864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084454
Iteration 2/1000 | Loss: 0.00007508
Iteration 3/1000 | Loss: 0.00005392
Iteration 4/1000 | Loss: 0.00004213
Iteration 5/1000 | Loss: 0.00003910
Iteration 6/1000 | Loss: 0.00003698
Iteration 7/1000 | Loss: 0.00003594
Iteration 8/1000 | Loss: 0.00003490
Iteration 9/1000 | Loss: 0.00003418
Iteration 10/1000 | Loss: 0.00003369
Iteration 11/1000 | Loss: 0.00003323
Iteration 12/1000 | Loss: 0.00003293
Iteration 13/1000 | Loss: 0.00003279
Iteration 14/1000 | Loss: 0.00003251
Iteration 15/1000 | Loss: 0.00003230
Iteration 16/1000 | Loss: 0.00003212
Iteration 17/1000 | Loss: 0.00003196
Iteration 18/1000 | Loss: 0.00003178
Iteration 19/1000 | Loss: 0.00003162
Iteration 20/1000 | Loss: 0.00003160
Iteration 21/1000 | Loss: 0.00003155
Iteration 22/1000 | Loss: 0.00003141
Iteration 23/1000 | Loss: 0.00003137
Iteration 24/1000 | Loss: 0.00003137
Iteration 25/1000 | Loss: 0.00003134
Iteration 26/1000 | Loss: 0.00003134
Iteration 27/1000 | Loss: 0.00003134
Iteration 28/1000 | Loss: 0.00003133
Iteration 29/1000 | Loss: 0.00003131
Iteration 30/1000 | Loss: 0.00003128
Iteration 31/1000 | Loss: 0.00003127
Iteration 32/1000 | Loss: 0.00003127
Iteration 33/1000 | Loss: 0.00003127
Iteration 34/1000 | Loss: 0.00003127
Iteration 35/1000 | Loss: 0.00003127
Iteration 36/1000 | Loss: 0.00003127
Iteration 37/1000 | Loss: 0.00003127
Iteration 38/1000 | Loss: 0.00003127
Iteration 39/1000 | Loss: 0.00003126
Iteration 40/1000 | Loss: 0.00003126
Iteration 41/1000 | Loss: 0.00003126
Iteration 42/1000 | Loss: 0.00003126
Iteration 43/1000 | Loss: 0.00003125
Iteration 44/1000 | Loss: 0.00003124
Iteration 45/1000 | Loss: 0.00003123
Iteration 46/1000 | Loss: 0.00003123
Iteration 47/1000 | Loss: 0.00003123
Iteration 48/1000 | Loss: 0.00003123
Iteration 49/1000 | Loss: 0.00003123
Iteration 50/1000 | Loss: 0.00003123
Iteration 51/1000 | Loss: 0.00003122
Iteration 52/1000 | Loss: 0.00003122
Iteration 53/1000 | Loss: 0.00003122
Iteration 54/1000 | Loss: 0.00003122
Iteration 55/1000 | Loss: 0.00003122
Iteration 56/1000 | Loss: 0.00003122
Iteration 57/1000 | Loss: 0.00003122
Iteration 58/1000 | Loss: 0.00003122
Iteration 59/1000 | Loss: 0.00003122
Iteration 60/1000 | Loss: 0.00003122
Iteration 61/1000 | Loss: 0.00003121
Iteration 62/1000 | Loss: 0.00003121
Iteration 63/1000 | Loss: 0.00003121
Iteration 64/1000 | Loss: 0.00003120
Iteration 65/1000 | Loss: 0.00003120
Iteration 66/1000 | Loss: 0.00003120
Iteration 67/1000 | Loss: 0.00003119
Iteration 68/1000 | Loss: 0.00003119
Iteration 69/1000 | Loss: 0.00003119
Iteration 70/1000 | Loss: 0.00003118
Iteration 71/1000 | Loss: 0.00003118
Iteration 72/1000 | Loss: 0.00003118
Iteration 73/1000 | Loss: 0.00003118
Iteration 74/1000 | Loss: 0.00003118
Iteration 75/1000 | Loss: 0.00003117
Iteration 76/1000 | Loss: 0.00003117
Iteration 77/1000 | Loss: 0.00003117
Iteration 78/1000 | Loss: 0.00003117
Iteration 79/1000 | Loss: 0.00003116
Iteration 80/1000 | Loss: 0.00003116
Iteration 81/1000 | Loss: 0.00003116
Iteration 82/1000 | Loss: 0.00003116
Iteration 83/1000 | Loss: 0.00003116
Iteration 84/1000 | Loss: 0.00003116
Iteration 85/1000 | Loss: 0.00003115
Iteration 86/1000 | Loss: 0.00003115
Iteration 87/1000 | Loss: 0.00003115
Iteration 88/1000 | Loss: 0.00003115
Iteration 89/1000 | Loss: 0.00003115
Iteration 90/1000 | Loss: 0.00003115
Iteration 91/1000 | Loss: 0.00003115
Iteration 92/1000 | Loss: 0.00003114
Iteration 93/1000 | Loss: 0.00003114
Iteration 94/1000 | Loss: 0.00003114
Iteration 95/1000 | Loss: 0.00003114
Iteration 96/1000 | Loss: 0.00003114
Iteration 97/1000 | Loss: 0.00003114
Iteration 98/1000 | Loss: 0.00003114
Iteration 99/1000 | Loss: 0.00003114
Iteration 100/1000 | Loss: 0.00003113
Iteration 101/1000 | Loss: 0.00003113
Iteration 102/1000 | Loss: 0.00003113
Iteration 103/1000 | Loss: 0.00003113
Iteration 104/1000 | Loss: 0.00003113
Iteration 105/1000 | Loss: 0.00003113
Iteration 106/1000 | Loss: 0.00003113
Iteration 107/1000 | Loss: 0.00003113
Iteration 108/1000 | Loss: 0.00003113
Iteration 109/1000 | Loss: 0.00003112
Iteration 110/1000 | Loss: 0.00003112
Iteration 111/1000 | Loss: 0.00003112
Iteration 112/1000 | Loss: 0.00003112
Iteration 113/1000 | Loss: 0.00003112
Iteration 114/1000 | Loss: 0.00003111
Iteration 115/1000 | Loss: 0.00003111
Iteration 116/1000 | Loss: 0.00003111
Iteration 117/1000 | Loss: 0.00003111
Iteration 118/1000 | Loss: 0.00003111
Iteration 119/1000 | Loss: 0.00003111
Iteration 120/1000 | Loss: 0.00003111
Iteration 121/1000 | Loss: 0.00003111
Iteration 122/1000 | Loss: 0.00003111
Iteration 123/1000 | Loss: 0.00003111
Iteration 124/1000 | Loss: 0.00003111
Iteration 125/1000 | Loss: 0.00003110
Iteration 126/1000 | Loss: 0.00003110
Iteration 127/1000 | Loss: 0.00003110
Iteration 128/1000 | Loss: 0.00003110
Iteration 129/1000 | Loss: 0.00003110
Iteration 130/1000 | Loss: 0.00003110
Iteration 131/1000 | Loss: 0.00003110
Iteration 132/1000 | Loss: 0.00003110
Iteration 133/1000 | Loss: 0.00003110
Iteration 134/1000 | Loss: 0.00003109
Iteration 135/1000 | Loss: 0.00003109
Iteration 136/1000 | Loss: 0.00003109
Iteration 137/1000 | Loss: 0.00003108
Iteration 138/1000 | Loss: 0.00003108
Iteration 139/1000 | Loss: 0.00003108
Iteration 140/1000 | Loss: 0.00003108
Iteration 141/1000 | Loss: 0.00003107
Iteration 142/1000 | Loss: 0.00003107
Iteration 143/1000 | Loss: 0.00003107
Iteration 144/1000 | Loss: 0.00003107
Iteration 145/1000 | Loss: 0.00003107
Iteration 146/1000 | Loss: 0.00003107
Iteration 147/1000 | Loss: 0.00003107
Iteration 148/1000 | Loss: 0.00003107
Iteration 149/1000 | Loss: 0.00003106
Iteration 150/1000 | Loss: 0.00003106
Iteration 151/1000 | Loss: 0.00003106
Iteration 152/1000 | Loss: 0.00003106
Iteration 153/1000 | Loss: 0.00003106
Iteration 154/1000 | Loss: 0.00003106
Iteration 155/1000 | Loss: 0.00003105
Iteration 156/1000 | Loss: 0.00003105
Iteration 157/1000 | Loss: 0.00003105
Iteration 158/1000 | Loss: 0.00003105
Iteration 159/1000 | Loss: 0.00003105
Iteration 160/1000 | Loss: 0.00003105
Iteration 161/1000 | Loss: 0.00003105
Iteration 162/1000 | Loss: 0.00003105
Iteration 163/1000 | Loss: 0.00003105
Iteration 164/1000 | Loss: 0.00003105
Iteration 165/1000 | Loss: 0.00003105
Iteration 166/1000 | Loss: 0.00003105
Iteration 167/1000 | Loss: 0.00003105
Iteration 168/1000 | Loss: 0.00003105
Iteration 169/1000 | Loss: 0.00003105
Iteration 170/1000 | Loss: 0.00003104
Iteration 171/1000 | Loss: 0.00003104
Iteration 172/1000 | Loss: 0.00003104
Iteration 173/1000 | Loss: 0.00003104
Iteration 174/1000 | Loss: 0.00003104
Iteration 175/1000 | Loss: 0.00003104
Iteration 176/1000 | Loss: 0.00003104
Iteration 177/1000 | Loss: 0.00003104
Iteration 178/1000 | Loss: 0.00003104
Iteration 179/1000 | Loss: 0.00003104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.104025381617248e-05, 3.104025381617248e-05, 3.104025381617248e-05, 3.104025381617248e-05, 3.104025381617248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.104025381617248e-05

Optimization complete. Final v2v error: 4.565779209136963 mm

Highest mean error: 6.795551300048828 mm for frame 82

Lowest mean error: 3.380297899246216 mm for frame 30

Saving results

Total time: 51.547895669937134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429458
Iteration 2/25 | Loss: 0.00140932
Iteration 3/25 | Loss: 0.00123207
Iteration 4/25 | Loss: 0.00121897
Iteration 5/25 | Loss: 0.00121488
Iteration 6/25 | Loss: 0.00121410
Iteration 7/25 | Loss: 0.00121407
Iteration 8/25 | Loss: 0.00121407
Iteration 9/25 | Loss: 0.00121407
Iteration 10/25 | Loss: 0.00121407
Iteration 11/25 | Loss: 0.00121407
Iteration 12/25 | Loss: 0.00121407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012140736216679215, 0.0012140736216679215, 0.0012140736216679215, 0.0012140736216679215, 0.0012140736216679215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012140736216679215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53556371
Iteration 2/25 | Loss: 0.00072216
Iteration 3/25 | Loss: 0.00072216
Iteration 4/25 | Loss: 0.00072216
Iteration 5/25 | Loss: 0.00072216
Iteration 6/25 | Loss: 0.00072216
Iteration 7/25 | Loss: 0.00072216
Iteration 8/25 | Loss: 0.00072216
Iteration 9/25 | Loss: 0.00072216
Iteration 10/25 | Loss: 0.00072216
Iteration 11/25 | Loss: 0.00072216
Iteration 12/25 | Loss: 0.00072216
Iteration 13/25 | Loss: 0.00072216
Iteration 14/25 | Loss: 0.00072216
Iteration 15/25 | Loss: 0.00072216
Iteration 16/25 | Loss: 0.00072216
Iteration 17/25 | Loss: 0.00072216
Iteration 18/25 | Loss: 0.00072216
Iteration 19/25 | Loss: 0.00072216
Iteration 20/25 | Loss: 0.00072216
Iteration 21/25 | Loss: 0.00072216
Iteration 22/25 | Loss: 0.00072216
Iteration 23/25 | Loss: 0.00072216
Iteration 24/25 | Loss: 0.00072216
Iteration 25/25 | Loss: 0.00072216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072216
Iteration 2/1000 | Loss: 0.00003328
Iteration 3/1000 | Loss: 0.00002137
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001874
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001662
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001649
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001646
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001642
Iteration 33/1000 | Loss: 0.00001642
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001621
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001618
Iteration 50/1000 | Loss: 0.00001618
Iteration 51/1000 | Loss: 0.00001616
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001612
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001609
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001608
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001607
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001606
Iteration 73/1000 | Loss: 0.00001606
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001603
Iteration 95/1000 | Loss: 0.00001603
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001602
Iteration 98/1000 | Loss: 0.00001602
Iteration 99/1000 | Loss: 0.00001602
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001601
Iteration 102/1000 | Loss: 0.00001601
Iteration 103/1000 | Loss: 0.00001601
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001599
Iteration 111/1000 | Loss: 0.00001599
Iteration 112/1000 | Loss: 0.00001599
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001597
Iteration 143/1000 | Loss: 0.00001597
Iteration 144/1000 | Loss: 0.00001597
Iteration 145/1000 | Loss: 0.00001597
Iteration 146/1000 | Loss: 0.00001597
Iteration 147/1000 | Loss: 0.00001596
Iteration 148/1000 | Loss: 0.00001596
Iteration 149/1000 | Loss: 0.00001596
Iteration 150/1000 | Loss: 0.00001596
Iteration 151/1000 | Loss: 0.00001596
Iteration 152/1000 | Loss: 0.00001596
Iteration 153/1000 | Loss: 0.00001596
Iteration 154/1000 | Loss: 0.00001596
Iteration 155/1000 | Loss: 0.00001596
Iteration 156/1000 | Loss: 0.00001596
Iteration 157/1000 | Loss: 0.00001596
Iteration 158/1000 | Loss: 0.00001596
Iteration 159/1000 | Loss: 0.00001596
Iteration 160/1000 | Loss: 0.00001596
Iteration 161/1000 | Loss: 0.00001596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.595829780853819e-05, 1.595829780853819e-05, 1.595829780853819e-05, 1.595829780853819e-05, 1.595829780853819e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.595829780853819e-05

Optimization complete. Final v2v error: 3.323969841003418 mm

Highest mean error: 4.050477504730225 mm for frame 33

Lowest mean error: 2.6083741188049316 mm for frame 163

Saving results

Total time: 42.035351037979126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870696
Iteration 2/25 | Loss: 0.00230893
Iteration 3/25 | Loss: 0.00190483
Iteration 4/25 | Loss: 0.00154135
Iteration 5/25 | Loss: 0.00150465
Iteration 6/25 | Loss: 0.00144815
Iteration 7/25 | Loss: 0.00138558
Iteration 8/25 | Loss: 0.00136500
Iteration 9/25 | Loss: 0.00134795
Iteration 10/25 | Loss: 0.00132632
Iteration 11/25 | Loss: 0.00131182
Iteration 12/25 | Loss: 0.00131077
Iteration 13/25 | Loss: 0.00129933
Iteration 14/25 | Loss: 0.00129006
Iteration 15/25 | Loss: 0.00128436
Iteration 16/25 | Loss: 0.00128402
Iteration 17/25 | Loss: 0.00128518
Iteration 18/25 | Loss: 0.00128806
Iteration 19/25 | Loss: 0.00128490
Iteration 20/25 | Loss: 0.00128159
Iteration 21/25 | Loss: 0.00127671
Iteration 22/25 | Loss: 0.00127559
Iteration 23/25 | Loss: 0.00127522
Iteration 24/25 | Loss: 0.00127497
Iteration 25/25 | Loss: 0.00127472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53874016
Iteration 2/25 | Loss: 0.00141207
Iteration 3/25 | Loss: 0.00141207
Iteration 4/25 | Loss: 0.00141207
Iteration 5/25 | Loss: 0.00141207
Iteration 6/25 | Loss: 0.00141207
Iteration 7/25 | Loss: 0.00141207
Iteration 8/25 | Loss: 0.00141207
Iteration 9/25 | Loss: 0.00141206
Iteration 10/25 | Loss: 0.00141206
Iteration 11/25 | Loss: 0.00141206
Iteration 12/25 | Loss: 0.00141206
Iteration 13/25 | Loss: 0.00141206
Iteration 14/25 | Loss: 0.00141206
Iteration 15/25 | Loss: 0.00141206
Iteration 16/25 | Loss: 0.00141206
Iteration 17/25 | Loss: 0.00141206
Iteration 18/25 | Loss: 0.00141206
Iteration 19/25 | Loss: 0.00141206
Iteration 20/25 | Loss: 0.00141206
Iteration 21/25 | Loss: 0.00141206
Iteration 22/25 | Loss: 0.00141206
Iteration 23/25 | Loss: 0.00141206
Iteration 24/25 | Loss: 0.00141206
Iteration 25/25 | Loss: 0.00141206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141206
Iteration 2/1000 | Loss: 0.00018833
Iteration 3/1000 | Loss: 0.00009452
Iteration 4/1000 | Loss: 0.00019326
Iteration 5/1000 | Loss: 0.00018502
Iteration 6/1000 | Loss: 0.00006593
Iteration 7/1000 | Loss: 0.00006006
Iteration 8/1000 | Loss: 0.00015641
Iteration 9/1000 | Loss: 0.00028937
Iteration 10/1000 | Loss: 0.00006362
Iteration 11/1000 | Loss: 0.00005483
Iteration 12/1000 | Loss: 0.00007206
Iteration 13/1000 | Loss: 0.00005020
Iteration 14/1000 | Loss: 0.00004829
Iteration 15/1000 | Loss: 0.00008781
Iteration 16/1000 | Loss: 0.00004585
Iteration 17/1000 | Loss: 0.00004391
Iteration 18/1000 | Loss: 0.00004274
Iteration 19/1000 | Loss: 0.00005090
Iteration 20/1000 | Loss: 0.00004332
Iteration 21/1000 | Loss: 0.00004176
Iteration 22/1000 | Loss: 0.00004854
Iteration 23/1000 | Loss: 0.00004674
Iteration 24/1000 | Loss: 0.00004716
Iteration 25/1000 | Loss: 0.00004840
Iteration 26/1000 | Loss: 0.00004687
Iteration 27/1000 | Loss: 0.00004809
Iteration 28/1000 | Loss: 0.00004629
Iteration 29/1000 | Loss: 0.00004749
Iteration 30/1000 | Loss: 0.00018712
Iteration 31/1000 | Loss: 0.00014538
Iteration 32/1000 | Loss: 0.00004888
Iteration 33/1000 | Loss: 0.00004790
Iteration 34/1000 | Loss: 0.00017844
Iteration 35/1000 | Loss: 0.00011987
Iteration 36/1000 | Loss: 0.00013664
Iteration 37/1000 | Loss: 0.00011002
Iteration 38/1000 | Loss: 0.00008950
Iteration 39/1000 | Loss: 0.00009895
Iteration 40/1000 | Loss: 0.00011676
Iteration 41/1000 | Loss: 0.00013282
Iteration 42/1000 | Loss: 0.00011315
Iteration 43/1000 | Loss: 0.00011012
Iteration 44/1000 | Loss: 0.00004670
Iteration 45/1000 | Loss: 0.00004739
Iteration 46/1000 | Loss: 0.00008116
Iteration 47/1000 | Loss: 0.00006991
Iteration 48/1000 | Loss: 0.00003969
Iteration 49/1000 | Loss: 0.00015203
Iteration 50/1000 | Loss: 0.00011520
Iteration 51/1000 | Loss: 0.00012635
Iteration 52/1000 | Loss: 0.00005624
Iteration 53/1000 | Loss: 0.00011976
Iteration 54/1000 | Loss: 0.00006771
Iteration 55/1000 | Loss: 0.00003889
Iteration 56/1000 | Loss: 0.00003843
Iteration 57/1000 | Loss: 0.00004497
Iteration 58/1000 | Loss: 0.00004051
Iteration 59/1000 | Loss: 0.00003926
Iteration 60/1000 | Loss: 0.00003799
Iteration 61/1000 | Loss: 0.00003725
Iteration 62/1000 | Loss: 0.00017986
Iteration 63/1000 | Loss: 0.00009030
Iteration 64/1000 | Loss: 0.00004801
Iteration 65/1000 | Loss: 0.00006612
Iteration 66/1000 | Loss: 0.00004740
Iteration 67/1000 | Loss: 0.00004534
Iteration 68/1000 | Loss: 0.00003828
Iteration 69/1000 | Loss: 0.00004773
Iteration 70/1000 | Loss: 0.00003796
Iteration 71/1000 | Loss: 0.00003724
Iteration 72/1000 | Loss: 0.00003676
Iteration 73/1000 | Loss: 0.00003634
Iteration 74/1000 | Loss: 0.00021259
Iteration 75/1000 | Loss: 0.00012522
Iteration 76/1000 | Loss: 0.00009600
Iteration 77/1000 | Loss: 0.00019693
Iteration 78/1000 | Loss: 0.00016662
Iteration 79/1000 | Loss: 0.00003999
Iteration 80/1000 | Loss: 0.00003694
Iteration 81/1000 | Loss: 0.00003480
Iteration 82/1000 | Loss: 0.00003372
Iteration 83/1000 | Loss: 0.00003284
Iteration 84/1000 | Loss: 0.00003234
Iteration 85/1000 | Loss: 0.00027212
Iteration 86/1000 | Loss: 0.00012923
Iteration 87/1000 | Loss: 0.00003378
Iteration 88/1000 | Loss: 0.00003188
Iteration 89/1000 | Loss: 0.00025365
Iteration 90/1000 | Loss: 0.00017118
Iteration 91/1000 | Loss: 0.00003268
Iteration 92/1000 | Loss: 0.00003154
Iteration 93/1000 | Loss: 0.00024839
Iteration 94/1000 | Loss: 0.00008856
Iteration 95/1000 | Loss: 0.00003218
Iteration 96/1000 | Loss: 0.00003139
Iteration 97/1000 | Loss: 0.00025042
Iteration 98/1000 | Loss: 0.00006614
Iteration 99/1000 | Loss: 0.00003179
Iteration 100/1000 | Loss: 0.00003102
Iteration 101/1000 | Loss: 0.00003090
Iteration 102/1000 | Loss: 0.00025154
Iteration 103/1000 | Loss: 0.00006319
Iteration 104/1000 | Loss: 0.00003213
Iteration 105/1000 | Loss: 0.00003072
Iteration 106/1000 | Loss: 0.00023883
Iteration 107/1000 | Loss: 0.00040324
Iteration 108/1000 | Loss: 0.00030062
Iteration 109/1000 | Loss: 0.00006520
Iteration 110/1000 | Loss: 0.00004742
Iteration 111/1000 | Loss: 0.00003876
Iteration 112/1000 | Loss: 0.00003192
Iteration 113/1000 | Loss: 0.00016341
Iteration 114/1000 | Loss: 0.00013488
Iteration 115/1000 | Loss: 0.00023493
Iteration 116/1000 | Loss: 0.00015589
Iteration 117/1000 | Loss: 0.00003024
Iteration 118/1000 | Loss: 0.00011598
Iteration 119/1000 | Loss: 0.00014198
Iteration 120/1000 | Loss: 0.00011770
Iteration 121/1000 | Loss: 0.00011429
Iteration 122/1000 | Loss: 0.00010591
Iteration 123/1000 | Loss: 0.00010526
Iteration 124/1000 | Loss: 0.00020912
Iteration 125/1000 | Loss: 0.00006418
Iteration 126/1000 | Loss: 0.00024154
Iteration 127/1000 | Loss: 0.00014975
Iteration 128/1000 | Loss: 0.00002899
Iteration 129/1000 | Loss: 0.00002581
Iteration 130/1000 | Loss: 0.00002516
Iteration 131/1000 | Loss: 0.00002427
Iteration 132/1000 | Loss: 0.00002356
Iteration 133/1000 | Loss: 0.00002276
Iteration 134/1000 | Loss: 0.00002229
Iteration 135/1000 | Loss: 0.00002203
Iteration 136/1000 | Loss: 0.00002178
Iteration 137/1000 | Loss: 0.00002164
Iteration 138/1000 | Loss: 0.00002156
Iteration 139/1000 | Loss: 0.00002154
Iteration 140/1000 | Loss: 0.00002153
Iteration 141/1000 | Loss: 0.00002145
Iteration 142/1000 | Loss: 0.00002142
Iteration 143/1000 | Loss: 0.00002142
Iteration 144/1000 | Loss: 0.00002141
Iteration 145/1000 | Loss: 0.00002140
Iteration 146/1000 | Loss: 0.00002139
Iteration 147/1000 | Loss: 0.00002139
Iteration 148/1000 | Loss: 0.00002139
Iteration 149/1000 | Loss: 0.00002138
Iteration 150/1000 | Loss: 0.00002138
Iteration 151/1000 | Loss: 0.00002138
Iteration 152/1000 | Loss: 0.00002137
Iteration 153/1000 | Loss: 0.00002137
Iteration 154/1000 | Loss: 0.00002137
Iteration 155/1000 | Loss: 0.00002137
Iteration 156/1000 | Loss: 0.00002136
Iteration 157/1000 | Loss: 0.00002136
Iteration 158/1000 | Loss: 0.00002136
Iteration 159/1000 | Loss: 0.00002136
Iteration 160/1000 | Loss: 0.00002136
Iteration 161/1000 | Loss: 0.00002136
Iteration 162/1000 | Loss: 0.00002136
Iteration 163/1000 | Loss: 0.00002136
Iteration 164/1000 | Loss: 0.00002136
Iteration 165/1000 | Loss: 0.00002135
Iteration 166/1000 | Loss: 0.00002135
Iteration 167/1000 | Loss: 0.00002135
Iteration 168/1000 | Loss: 0.00002135
Iteration 169/1000 | Loss: 0.00002135
Iteration 170/1000 | Loss: 0.00002135
Iteration 171/1000 | Loss: 0.00002135
Iteration 172/1000 | Loss: 0.00002135
Iteration 173/1000 | Loss: 0.00002135
Iteration 174/1000 | Loss: 0.00002134
Iteration 175/1000 | Loss: 0.00002134
Iteration 176/1000 | Loss: 0.00002134
Iteration 177/1000 | Loss: 0.00002134
Iteration 178/1000 | Loss: 0.00002133
Iteration 179/1000 | Loss: 0.00002133
Iteration 180/1000 | Loss: 0.00002133
Iteration 181/1000 | Loss: 0.00002133
Iteration 182/1000 | Loss: 0.00002133
Iteration 183/1000 | Loss: 0.00002133
Iteration 184/1000 | Loss: 0.00002133
Iteration 185/1000 | Loss: 0.00002132
Iteration 186/1000 | Loss: 0.00002132
Iteration 187/1000 | Loss: 0.00002132
Iteration 188/1000 | Loss: 0.00002132
Iteration 189/1000 | Loss: 0.00002132
Iteration 190/1000 | Loss: 0.00002132
Iteration 191/1000 | Loss: 0.00002132
Iteration 192/1000 | Loss: 0.00002132
Iteration 193/1000 | Loss: 0.00002132
Iteration 194/1000 | Loss: 0.00002132
Iteration 195/1000 | Loss: 0.00002132
Iteration 196/1000 | Loss: 0.00002132
Iteration 197/1000 | Loss: 0.00002132
Iteration 198/1000 | Loss: 0.00002132
Iteration 199/1000 | Loss: 0.00002132
Iteration 200/1000 | Loss: 0.00002132
Iteration 201/1000 | Loss: 0.00002132
Iteration 202/1000 | Loss: 0.00002132
Iteration 203/1000 | Loss: 0.00002132
Iteration 204/1000 | Loss: 0.00002132
Iteration 205/1000 | Loss: 0.00002132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.1323397959349677e-05, 2.1323397959349677e-05, 2.1323397959349677e-05, 2.1323397959349677e-05, 2.1323397959349677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1323397959349677e-05

Optimization complete. Final v2v error: 3.365715503692627 mm

Highest mean error: 11.194972038269043 mm for frame 61

Lowest mean error: 2.868114948272705 mm for frame 215

Saving results

Total time: 279.06562089920044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977843
Iteration 2/25 | Loss: 0.00187387
Iteration 3/25 | Loss: 0.00146369
Iteration 4/25 | Loss: 0.00144213
Iteration 5/25 | Loss: 0.00143505
Iteration 6/25 | Loss: 0.00143362
Iteration 7/25 | Loss: 0.00143362
Iteration 8/25 | Loss: 0.00143362
Iteration 9/25 | Loss: 0.00143362
Iteration 10/25 | Loss: 0.00143362
Iteration 11/25 | Loss: 0.00143362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001433615805581212, 0.001433615805581212, 0.001433615805581212, 0.001433615805581212, 0.001433615805581212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001433615805581212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84748405
Iteration 2/25 | Loss: 0.00098923
Iteration 3/25 | Loss: 0.00098922
Iteration 4/25 | Loss: 0.00098922
Iteration 5/25 | Loss: 0.00098922
Iteration 6/25 | Loss: 0.00098922
Iteration 7/25 | Loss: 0.00098922
Iteration 8/25 | Loss: 0.00098922
Iteration 9/25 | Loss: 0.00098922
Iteration 10/25 | Loss: 0.00098922
Iteration 11/25 | Loss: 0.00098922
Iteration 12/25 | Loss: 0.00098922
Iteration 13/25 | Loss: 0.00098922
Iteration 14/25 | Loss: 0.00098922
Iteration 15/25 | Loss: 0.00098922
Iteration 16/25 | Loss: 0.00098922
Iteration 17/25 | Loss: 0.00098922
Iteration 18/25 | Loss: 0.00098922
Iteration 19/25 | Loss: 0.00098922
Iteration 20/25 | Loss: 0.00098922
Iteration 21/25 | Loss: 0.00098922
Iteration 22/25 | Loss: 0.00098922
Iteration 23/25 | Loss: 0.00098922
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009892197558656335, 0.0009892197558656335, 0.0009892197558656335, 0.0009892197558656335, 0.0009892197558656335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009892197558656335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098922
Iteration 2/1000 | Loss: 0.00009391
Iteration 3/1000 | Loss: 0.00005791
Iteration 4/1000 | Loss: 0.00004284
Iteration 5/1000 | Loss: 0.00003950
Iteration 6/1000 | Loss: 0.00003780
Iteration 7/1000 | Loss: 0.00003691
Iteration 8/1000 | Loss: 0.00003607
Iteration 9/1000 | Loss: 0.00003535
Iteration 10/1000 | Loss: 0.00003484
Iteration 11/1000 | Loss: 0.00003446
Iteration 12/1000 | Loss: 0.00003416
Iteration 13/1000 | Loss: 0.00003392
Iteration 14/1000 | Loss: 0.00003355
Iteration 15/1000 | Loss: 0.00003327
Iteration 16/1000 | Loss: 0.00003307
Iteration 17/1000 | Loss: 0.00003283
Iteration 18/1000 | Loss: 0.00003257
Iteration 19/1000 | Loss: 0.00003249
Iteration 20/1000 | Loss: 0.00003235
Iteration 21/1000 | Loss: 0.00003225
Iteration 22/1000 | Loss: 0.00003220
Iteration 23/1000 | Loss: 0.00003209
Iteration 24/1000 | Loss: 0.00003206
Iteration 25/1000 | Loss: 0.00003205
Iteration 26/1000 | Loss: 0.00003205
Iteration 27/1000 | Loss: 0.00003202
Iteration 28/1000 | Loss: 0.00003202
Iteration 29/1000 | Loss: 0.00003202
Iteration 30/1000 | Loss: 0.00003201
Iteration 31/1000 | Loss: 0.00003201
Iteration 32/1000 | Loss: 0.00003201
Iteration 33/1000 | Loss: 0.00003200
Iteration 34/1000 | Loss: 0.00003200
Iteration 35/1000 | Loss: 0.00003199
Iteration 36/1000 | Loss: 0.00003199
Iteration 37/1000 | Loss: 0.00003199
Iteration 38/1000 | Loss: 0.00003199
Iteration 39/1000 | Loss: 0.00003198
Iteration 40/1000 | Loss: 0.00003198
Iteration 41/1000 | Loss: 0.00003198
Iteration 42/1000 | Loss: 0.00003198
Iteration 43/1000 | Loss: 0.00003198
Iteration 44/1000 | Loss: 0.00003198
Iteration 45/1000 | Loss: 0.00003198
Iteration 46/1000 | Loss: 0.00003197
Iteration 47/1000 | Loss: 0.00003197
Iteration 48/1000 | Loss: 0.00003197
Iteration 49/1000 | Loss: 0.00003196
Iteration 50/1000 | Loss: 0.00003196
Iteration 51/1000 | Loss: 0.00003195
Iteration 52/1000 | Loss: 0.00003195
Iteration 53/1000 | Loss: 0.00003195
Iteration 54/1000 | Loss: 0.00003195
Iteration 55/1000 | Loss: 0.00003194
Iteration 56/1000 | Loss: 0.00003194
Iteration 57/1000 | Loss: 0.00003194
Iteration 58/1000 | Loss: 0.00003194
Iteration 59/1000 | Loss: 0.00003193
Iteration 60/1000 | Loss: 0.00003193
Iteration 61/1000 | Loss: 0.00003193
Iteration 62/1000 | Loss: 0.00003193
Iteration 63/1000 | Loss: 0.00003193
Iteration 64/1000 | Loss: 0.00003193
Iteration 65/1000 | Loss: 0.00003193
Iteration 66/1000 | Loss: 0.00003192
Iteration 67/1000 | Loss: 0.00003192
Iteration 68/1000 | Loss: 0.00003192
Iteration 69/1000 | Loss: 0.00003192
Iteration 70/1000 | Loss: 0.00003192
Iteration 71/1000 | Loss: 0.00003191
Iteration 72/1000 | Loss: 0.00003191
Iteration 73/1000 | Loss: 0.00003190
Iteration 74/1000 | Loss: 0.00003190
Iteration 75/1000 | Loss: 0.00003190
Iteration 76/1000 | Loss: 0.00003189
Iteration 77/1000 | Loss: 0.00003189
Iteration 78/1000 | Loss: 0.00003189
Iteration 79/1000 | Loss: 0.00003189
Iteration 80/1000 | Loss: 0.00003189
Iteration 81/1000 | Loss: 0.00003188
Iteration 82/1000 | Loss: 0.00003188
Iteration 83/1000 | Loss: 0.00003188
Iteration 84/1000 | Loss: 0.00003187
Iteration 85/1000 | Loss: 0.00003187
Iteration 86/1000 | Loss: 0.00003187
Iteration 87/1000 | Loss: 0.00003187
Iteration 88/1000 | Loss: 0.00003187
Iteration 89/1000 | Loss: 0.00003187
Iteration 90/1000 | Loss: 0.00003187
Iteration 91/1000 | Loss: 0.00003187
Iteration 92/1000 | Loss: 0.00003187
Iteration 93/1000 | Loss: 0.00003187
Iteration 94/1000 | Loss: 0.00003187
Iteration 95/1000 | Loss: 0.00003186
Iteration 96/1000 | Loss: 0.00003186
Iteration 97/1000 | Loss: 0.00003186
Iteration 98/1000 | Loss: 0.00003186
Iteration 99/1000 | Loss: 0.00003186
Iteration 100/1000 | Loss: 0.00003186
Iteration 101/1000 | Loss: 0.00003186
Iteration 102/1000 | Loss: 0.00003186
Iteration 103/1000 | Loss: 0.00003186
Iteration 104/1000 | Loss: 0.00003185
Iteration 105/1000 | Loss: 0.00003185
Iteration 106/1000 | Loss: 0.00003185
Iteration 107/1000 | Loss: 0.00003185
Iteration 108/1000 | Loss: 0.00003185
Iteration 109/1000 | Loss: 0.00003185
Iteration 110/1000 | Loss: 0.00003185
Iteration 111/1000 | Loss: 0.00003185
Iteration 112/1000 | Loss: 0.00003185
Iteration 113/1000 | Loss: 0.00003185
Iteration 114/1000 | Loss: 0.00003184
Iteration 115/1000 | Loss: 0.00003184
Iteration 116/1000 | Loss: 0.00003184
Iteration 117/1000 | Loss: 0.00003184
Iteration 118/1000 | Loss: 0.00003184
Iteration 119/1000 | Loss: 0.00003183
Iteration 120/1000 | Loss: 0.00003183
Iteration 121/1000 | Loss: 0.00003183
Iteration 122/1000 | Loss: 0.00003183
Iteration 123/1000 | Loss: 0.00003182
Iteration 124/1000 | Loss: 0.00003182
Iteration 125/1000 | Loss: 0.00003182
Iteration 126/1000 | Loss: 0.00003182
Iteration 127/1000 | Loss: 0.00003182
Iteration 128/1000 | Loss: 0.00003182
Iteration 129/1000 | Loss: 0.00003182
Iteration 130/1000 | Loss: 0.00003182
Iteration 131/1000 | Loss: 0.00003182
Iteration 132/1000 | Loss: 0.00003182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.1823376048123464e-05, 3.1823376048123464e-05, 3.1823376048123464e-05, 3.1823376048123464e-05, 3.1823376048123464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1823376048123464e-05

Optimization complete. Final v2v error: 4.641483783721924 mm

Highest mean error: 5.415581226348877 mm for frame 76

Lowest mean error: 3.711223602294922 mm for frame 27

Saving results

Total time: 50.27099299430847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857676
Iteration 2/25 | Loss: 0.00132673
Iteration 3/25 | Loss: 0.00125062
Iteration 4/25 | Loss: 0.00123604
Iteration 5/25 | Loss: 0.00123201
Iteration 6/25 | Loss: 0.00123163
Iteration 7/25 | Loss: 0.00123163
Iteration 8/25 | Loss: 0.00123163
Iteration 9/25 | Loss: 0.00123163
Iteration 10/25 | Loss: 0.00123163
Iteration 11/25 | Loss: 0.00123163
Iteration 12/25 | Loss: 0.00123163
Iteration 13/25 | Loss: 0.00123163
Iteration 14/25 | Loss: 0.00123163
Iteration 15/25 | Loss: 0.00123163
Iteration 16/25 | Loss: 0.00123163
Iteration 17/25 | Loss: 0.00123163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001231625908985734, 0.001231625908985734, 0.001231625908985734, 0.001231625908985734, 0.001231625908985734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001231625908985734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40846431
Iteration 2/25 | Loss: 0.00085117
Iteration 3/25 | Loss: 0.00085115
Iteration 4/25 | Loss: 0.00085115
Iteration 5/25 | Loss: 0.00085115
Iteration 6/25 | Loss: 0.00085115
Iteration 7/25 | Loss: 0.00085115
Iteration 8/25 | Loss: 0.00085115
Iteration 9/25 | Loss: 0.00085115
Iteration 10/25 | Loss: 0.00085115
Iteration 11/25 | Loss: 0.00085115
Iteration 12/25 | Loss: 0.00085115
Iteration 13/25 | Loss: 0.00085115
Iteration 14/25 | Loss: 0.00085115
Iteration 15/25 | Loss: 0.00085115
Iteration 16/25 | Loss: 0.00085115
Iteration 17/25 | Loss: 0.00085115
Iteration 18/25 | Loss: 0.00085115
Iteration 19/25 | Loss: 0.00085115
Iteration 20/25 | Loss: 0.00085115
Iteration 21/25 | Loss: 0.00085115
Iteration 22/25 | Loss: 0.00085115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008511489722877741, 0.0008511489722877741, 0.0008511489722877741, 0.0008511489722877741, 0.0008511489722877741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008511489722877741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085115
Iteration 2/1000 | Loss: 0.00004300
Iteration 3/1000 | Loss: 0.00002517
Iteration 4/1000 | Loss: 0.00002087
Iteration 5/1000 | Loss: 0.00001885
Iteration 6/1000 | Loss: 0.00001758
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001574
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001549
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001499
Iteration 18/1000 | Loss: 0.00001491
Iteration 19/1000 | Loss: 0.00001488
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001483
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001481
Iteration 24/1000 | Loss: 0.00001481
Iteration 25/1000 | Loss: 0.00001481
Iteration 26/1000 | Loss: 0.00001480
Iteration 27/1000 | Loss: 0.00001480
Iteration 28/1000 | Loss: 0.00001480
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001477
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001477
Iteration 35/1000 | Loss: 0.00001476
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001475
Iteration 38/1000 | Loss: 0.00001475
Iteration 39/1000 | Loss: 0.00001475
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001474
Iteration 47/1000 | Loss: 0.00001473
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001468
Iteration 64/1000 | Loss: 0.00001468
Iteration 65/1000 | Loss: 0.00001468
Iteration 66/1000 | Loss: 0.00001468
Iteration 67/1000 | Loss: 0.00001468
Iteration 68/1000 | Loss: 0.00001468
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001467
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001467
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001466
Iteration 83/1000 | Loss: 0.00001466
Iteration 84/1000 | Loss: 0.00001466
Iteration 85/1000 | Loss: 0.00001466
Iteration 86/1000 | Loss: 0.00001465
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001465
Iteration 90/1000 | Loss: 0.00001464
Iteration 91/1000 | Loss: 0.00001464
Iteration 92/1000 | Loss: 0.00001464
Iteration 93/1000 | Loss: 0.00001464
Iteration 94/1000 | Loss: 0.00001464
Iteration 95/1000 | Loss: 0.00001463
Iteration 96/1000 | Loss: 0.00001463
Iteration 97/1000 | Loss: 0.00001463
Iteration 98/1000 | Loss: 0.00001463
Iteration 99/1000 | Loss: 0.00001463
Iteration 100/1000 | Loss: 0.00001463
Iteration 101/1000 | Loss: 0.00001463
Iteration 102/1000 | Loss: 0.00001463
Iteration 103/1000 | Loss: 0.00001463
Iteration 104/1000 | Loss: 0.00001462
Iteration 105/1000 | Loss: 0.00001462
Iteration 106/1000 | Loss: 0.00001462
Iteration 107/1000 | Loss: 0.00001462
Iteration 108/1000 | Loss: 0.00001462
Iteration 109/1000 | Loss: 0.00001462
Iteration 110/1000 | Loss: 0.00001462
Iteration 111/1000 | Loss: 0.00001462
Iteration 112/1000 | Loss: 0.00001462
Iteration 113/1000 | Loss: 0.00001462
Iteration 114/1000 | Loss: 0.00001462
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001462
Iteration 117/1000 | Loss: 0.00001462
Iteration 118/1000 | Loss: 0.00001462
Iteration 119/1000 | Loss: 0.00001462
Iteration 120/1000 | Loss: 0.00001462
Iteration 121/1000 | Loss: 0.00001462
Iteration 122/1000 | Loss: 0.00001462
Iteration 123/1000 | Loss: 0.00001462
Iteration 124/1000 | Loss: 0.00001462
Iteration 125/1000 | Loss: 0.00001462
Iteration 126/1000 | Loss: 0.00001462
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001462
Iteration 130/1000 | Loss: 0.00001462
Iteration 131/1000 | Loss: 0.00001462
Iteration 132/1000 | Loss: 0.00001462
Iteration 133/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.461697138438467e-05, 1.461697138438467e-05, 1.461697138438467e-05, 1.461697138438467e-05, 1.461697138438467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.461697138438467e-05

Optimization complete. Final v2v error: 3.167297840118408 mm

Highest mean error: 4.175318717956543 mm for frame 39

Lowest mean error: 2.6501963138580322 mm for frame 68

Saving results

Total time: 35.85807514190674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386978
Iteration 2/25 | Loss: 0.00129015
Iteration 3/25 | Loss: 0.00121171
Iteration 4/25 | Loss: 0.00120377
Iteration 5/25 | Loss: 0.00120055
Iteration 6/25 | Loss: 0.00120055
Iteration 7/25 | Loss: 0.00120055
Iteration 8/25 | Loss: 0.00120055
Iteration 9/25 | Loss: 0.00120055
Iteration 10/25 | Loss: 0.00120055
Iteration 11/25 | Loss: 0.00120055
Iteration 12/25 | Loss: 0.00120055
Iteration 13/25 | Loss: 0.00120055
Iteration 14/25 | Loss: 0.00120055
Iteration 15/25 | Loss: 0.00120055
Iteration 16/25 | Loss: 0.00120055
Iteration 17/25 | Loss: 0.00120055
Iteration 18/25 | Loss: 0.00120055
Iteration 19/25 | Loss: 0.00120055
Iteration 20/25 | Loss: 0.00120055
Iteration 21/25 | Loss: 0.00120055
Iteration 22/25 | Loss: 0.00120055
Iteration 23/25 | Loss: 0.00120055
Iteration 24/25 | Loss: 0.00120055
Iteration 25/25 | Loss: 0.00120055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58947110
Iteration 2/25 | Loss: 0.00072662
Iteration 3/25 | Loss: 0.00072662
Iteration 4/25 | Loss: 0.00072662
Iteration 5/25 | Loss: 0.00072662
Iteration 6/25 | Loss: 0.00072662
Iteration 7/25 | Loss: 0.00072662
Iteration 8/25 | Loss: 0.00072662
Iteration 9/25 | Loss: 0.00072662
Iteration 10/25 | Loss: 0.00072661
Iteration 11/25 | Loss: 0.00072661
Iteration 12/25 | Loss: 0.00072661
Iteration 13/25 | Loss: 0.00072661
Iteration 14/25 | Loss: 0.00072661
Iteration 15/25 | Loss: 0.00072661
Iteration 16/25 | Loss: 0.00072661
Iteration 17/25 | Loss: 0.00072661
Iteration 18/25 | Loss: 0.00072661
Iteration 19/25 | Loss: 0.00072661
Iteration 20/25 | Loss: 0.00072661
Iteration 21/25 | Loss: 0.00072661
Iteration 22/25 | Loss: 0.00072661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007266147877089679, 0.0007266147877089679, 0.0007266147877089679, 0.0007266147877089679, 0.0007266147877089679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007266147877089679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072661
Iteration 2/1000 | Loss: 0.00003181
Iteration 3/1000 | Loss: 0.00002003
Iteration 4/1000 | Loss: 0.00001649
Iteration 5/1000 | Loss: 0.00001515
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001352
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001244
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001196
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001192
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001192
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001191
Iteration 50/1000 | Loss: 0.00001190
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001190
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001189
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001181
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001178
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001177
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001177
Iteration 100/1000 | Loss: 0.00001177
Iteration 101/1000 | Loss: 0.00001177
Iteration 102/1000 | Loss: 0.00001177
Iteration 103/1000 | Loss: 0.00001177
Iteration 104/1000 | Loss: 0.00001177
Iteration 105/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.1766737770813052e-05, 1.1766737770813052e-05, 1.1766737770813052e-05, 1.1766737770813052e-05, 1.1766737770813052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1766737770813052e-05

Optimization complete. Final v2v error: 2.9405264854431152 mm

Highest mean error: 3.323960542678833 mm for frame 130

Lowest mean error: 2.736107110977173 mm for frame 157

Saving results

Total time: 37.89013648033142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584983
Iteration 2/25 | Loss: 0.00148445
Iteration 3/25 | Loss: 0.00133862
Iteration 4/25 | Loss: 0.00129812
Iteration 5/25 | Loss: 0.00128584
Iteration 6/25 | Loss: 0.00128340
Iteration 7/25 | Loss: 0.00128280
Iteration 8/25 | Loss: 0.00128280
Iteration 9/25 | Loss: 0.00128280
Iteration 10/25 | Loss: 0.00128280
Iteration 11/25 | Loss: 0.00128280
Iteration 12/25 | Loss: 0.00128280
Iteration 13/25 | Loss: 0.00128280
Iteration 14/25 | Loss: 0.00128280
Iteration 15/25 | Loss: 0.00128280
Iteration 16/25 | Loss: 0.00128280
Iteration 17/25 | Loss: 0.00128280
Iteration 18/25 | Loss: 0.00128280
Iteration 19/25 | Loss: 0.00128280
Iteration 20/25 | Loss: 0.00128280
Iteration 21/25 | Loss: 0.00128280
Iteration 22/25 | Loss: 0.00128280
Iteration 23/25 | Loss: 0.00128280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012827994069084525, 0.0012827994069084525, 0.0012827994069084525, 0.0012827994069084525, 0.0012827994069084525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012827994069084525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40927744
Iteration 2/25 | Loss: 0.00071224
Iteration 3/25 | Loss: 0.00071220
Iteration 4/25 | Loss: 0.00071219
Iteration 5/25 | Loss: 0.00071219
Iteration 6/25 | Loss: 0.00071219
Iteration 7/25 | Loss: 0.00071219
Iteration 8/25 | Loss: 0.00071219
Iteration 9/25 | Loss: 0.00071219
Iteration 10/25 | Loss: 0.00071219
Iteration 11/25 | Loss: 0.00071219
Iteration 12/25 | Loss: 0.00071219
Iteration 13/25 | Loss: 0.00071219
Iteration 14/25 | Loss: 0.00071219
Iteration 15/25 | Loss: 0.00071219
Iteration 16/25 | Loss: 0.00071219
Iteration 17/25 | Loss: 0.00071219
Iteration 18/25 | Loss: 0.00071219
Iteration 19/25 | Loss: 0.00071219
Iteration 20/25 | Loss: 0.00071219
Iteration 21/25 | Loss: 0.00071219
Iteration 22/25 | Loss: 0.00071219
Iteration 23/25 | Loss: 0.00071219
Iteration 24/25 | Loss: 0.00071219
Iteration 25/25 | Loss: 0.00071219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071219
Iteration 2/1000 | Loss: 0.00007317
Iteration 3/1000 | Loss: 0.00005100
Iteration 4/1000 | Loss: 0.00004323
Iteration 5/1000 | Loss: 0.00004075
Iteration 6/1000 | Loss: 0.00003939
Iteration 7/1000 | Loss: 0.00003854
Iteration 8/1000 | Loss: 0.00003800
Iteration 9/1000 | Loss: 0.00003751
Iteration 10/1000 | Loss: 0.00003707
Iteration 11/1000 | Loss: 0.00003673
Iteration 12/1000 | Loss: 0.00003647
Iteration 13/1000 | Loss: 0.00003623
Iteration 14/1000 | Loss: 0.00003615
Iteration 15/1000 | Loss: 0.00003611
Iteration 16/1000 | Loss: 0.00003597
Iteration 17/1000 | Loss: 0.00003595
Iteration 18/1000 | Loss: 0.00003589
Iteration 19/1000 | Loss: 0.00003588
Iteration 20/1000 | Loss: 0.00003586
Iteration 21/1000 | Loss: 0.00003585
Iteration 22/1000 | Loss: 0.00003584
Iteration 23/1000 | Loss: 0.00003584
Iteration 24/1000 | Loss: 0.00003583
Iteration 25/1000 | Loss: 0.00003583
Iteration 26/1000 | Loss: 0.00003582
Iteration 27/1000 | Loss: 0.00003580
Iteration 28/1000 | Loss: 0.00003579
Iteration 29/1000 | Loss: 0.00003578
Iteration 30/1000 | Loss: 0.00003577
Iteration 31/1000 | Loss: 0.00003575
Iteration 32/1000 | Loss: 0.00003574
Iteration 33/1000 | Loss: 0.00003574
Iteration 34/1000 | Loss: 0.00003573
Iteration 35/1000 | Loss: 0.00003573
Iteration 36/1000 | Loss: 0.00003573
Iteration 37/1000 | Loss: 0.00003573
Iteration 38/1000 | Loss: 0.00003573
Iteration 39/1000 | Loss: 0.00003573
Iteration 40/1000 | Loss: 0.00003572
Iteration 41/1000 | Loss: 0.00003572
Iteration 42/1000 | Loss: 0.00003572
Iteration 43/1000 | Loss: 0.00003572
Iteration 44/1000 | Loss: 0.00003572
Iteration 45/1000 | Loss: 0.00003572
Iteration 46/1000 | Loss: 0.00003572
Iteration 47/1000 | Loss: 0.00003572
Iteration 48/1000 | Loss: 0.00003572
Iteration 49/1000 | Loss: 0.00003572
Iteration 50/1000 | Loss: 0.00003572
Iteration 51/1000 | Loss: 0.00003572
Iteration 52/1000 | Loss: 0.00003571
Iteration 53/1000 | Loss: 0.00003571
Iteration 54/1000 | Loss: 0.00003571
Iteration 55/1000 | Loss: 0.00003571
Iteration 56/1000 | Loss: 0.00003571
Iteration 57/1000 | Loss: 0.00003571
Iteration 58/1000 | Loss: 0.00003571
Iteration 59/1000 | Loss: 0.00003571
Iteration 60/1000 | Loss: 0.00003571
Iteration 61/1000 | Loss: 0.00003570
Iteration 62/1000 | Loss: 0.00003570
Iteration 63/1000 | Loss: 0.00003570
Iteration 64/1000 | Loss: 0.00003570
Iteration 65/1000 | Loss: 0.00003570
Iteration 66/1000 | Loss: 0.00003569
Iteration 67/1000 | Loss: 0.00003569
Iteration 68/1000 | Loss: 0.00003569
Iteration 69/1000 | Loss: 0.00003569
Iteration 70/1000 | Loss: 0.00003568
Iteration 71/1000 | Loss: 0.00003568
Iteration 72/1000 | Loss: 0.00003568
Iteration 73/1000 | Loss: 0.00003568
Iteration 74/1000 | Loss: 0.00003568
Iteration 75/1000 | Loss: 0.00003568
Iteration 76/1000 | Loss: 0.00003568
Iteration 77/1000 | Loss: 0.00003568
Iteration 78/1000 | Loss: 0.00003568
Iteration 79/1000 | Loss: 0.00003568
Iteration 80/1000 | Loss: 0.00003568
Iteration 81/1000 | Loss: 0.00003568
Iteration 82/1000 | Loss: 0.00003567
Iteration 83/1000 | Loss: 0.00003567
Iteration 84/1000 | Loss: 0.00003567
Iteration 85/1000 | Loss: 0.00003567
Iteration 86/1000 | Loss: 0.00003567
Iteration 87/1000 | Loss: 0.00003566
Iteration 88/1000 | Loss: 0.00003566
Iteration 89/1000 | Loss: 0.00003566
Iteration 90/1000 | Loss: 0.00003566
Iteration 91/1000 | Loss: 0.00003565
Iteration 92/1000 | Loss: 0.00003565
Iteration 93/1000 | Loss: 0.00003565
Iteration 94/1000 | Loss: 0.00003565
Iteration 95/1000 | Loss: 0.00003564
Iteration 96/1000 | Loss: 0.00003564
Iteration 97/1000 | Loss: 0.00003564
Iteration 98/1000 | Loss: 0.00003564
Iteration 99/1000 | Loss: 0.00003564
Iteration 100/1000 | Loss: 0.00003564
Iteration 101/1000 | Loss: 0.00003564
Iteration 102/1000 | Loss: 0.00003563
Iteration 103/1000 | Loss: 0.00003563
Iteration 104/1000 | Loss: 0.00003563
Iteration 105/1000 | Loss: 0.00003563
Iteration 106/1000 | Loss: 0.00003562
Iteration 107/1000 | Loss: 0.00003562
Iteration 108/1000 | Loss: 0.00003562
Iteration 109/1000 | Loss: 0.00003562
Iteration 110/1000 | Loss: 0.00003561
Iteration 111/1000 | Loss: 0.00003561
Iteration 112/1000 | Loss: 0.00003561
Iteration 113/1000 | Loss: 0.00003561
Iteration 114/1000 | Loss: 0.00003561
Iteration 115/1000 | Loss: 0.00003561
Iteration 116/1000 | Loss: 0.00003560
Iteration 117/1000 | Loss: 0.00003560
Iteration 118/1000 | Loss: 0.00003560
Iteration 119/1000 | Loss: 0.00003560
Iteration 120/1000 | Loss: 0.00003560
Iteration 121/1000 | Loss: 0.00003560
Iteration 122/1000 | Loss: 0.00003560
Iteration 123/1000 | Loss: 0.00003560
Iteration 124/1000 | Loss: 0.00003560
Iteration 125/1000 | Loss: 0.00003559
Iteration 126/1000 | Loss: 0.00003559
Iteration 127/1000 | Loss: 0.00003559
Iteration 128/1000 | Loss: 0.00003559
Iteration 129/1000 | Loss: 0.00003559
Iteration 130/1000 | Loss: 0.00003559
Iteration 131/1000 | Loss: 0.00003559
Iteration 132/1000 | Loss: 0.00003559
Iteration 133/1000 | Loss: 0.00003559
Iteration 134/1000 | Loss: 0.00003559
Iteration 135/1000 | Loss: 0.00003559
Iteration 136/1000 | Loss: 0.00003559
Iteration 137/1000 | Loss: 0.00003559
Iteration 138/1000 | Loss: 0.00003559
Iteration 139/1000 | Loss: 0.00003558
Iteration 140/1000 | Loss: 0.00003558
Iteration 141/1000 | Loss: 0.00003558
Iteration 142/1000 | Loss: 0.00003558
Iteration 143/1000 | Loss: 0.00003558
Iteration 144/1000 | Loss: 0.00003558
Iteration 145/1000 | Loss: 0.00003558
Iteration 146/1000 | Loss: 0.00003558
Iteration 147/1000 | Loss: 0.00003557
Iteration 148/1000 | Loss: 0.00003557
Iteration 149/1000 | Loss: 0.00003557
Iteration 150/1000 | Loss: 0.00003557
Iteration 151/1000 | Loss: 0.00003557
Iteration 152/1000 | Loss: 0.00003557
Iteration 153/1000 | Loss: 0.00003557
Iteration 154/1000 | Loss: 0.00003557
Iteration 155/1000 | Loss: 0.00003557
Iteration 156/1000 | Loss: 0.00003556
Iteration 157/1000 | Loss: 0.00003556
Iteration 158/1000 | Loss: 0.00003556
Iteration 159/1000 | Loss: 0.00003556
Iteration 160/1000 | Loss: 0.00003556
Iteration 161/1000 | Loss: 0.00003556
Iteration 162/1000 | Loss: 0.00003556
Iteration 163/1000 | Loss: 0.00003556
Iteration 164/1000 | Loss: 0.00003556
Iteration 165/1000 | Loss: 0.00003556
Iteration 166/1000 | Loss: 0.00003556
Iteration 167/1000 | Loss: 0.00003556
Iteration 168/1000 | Loss: 0.00003556
Iteration 169/1000 | Loss: 0.00003556
Iteration 170/1000 | Loss: 0.00003555
Iteration 171/1000 | Loss: 0.00003555
Iteration 172/1000 | Loss: 0.00003555
Iteration 173/1000 | Loss: 0.00003555
Iteration 174/1000 | Loss: 0.00003555
Iteration 175/1000 | Loss: 0.00003555
Iteration 176/1000 | Loss: 0.00003555
Iteration 177/1000 | Loss: 0.00003555
Iteration 178/1000 | Loss: 0.00003555
Iteration 179/1000 | Loss: 0.00003555
Iteration 180/1000 | Loss: 0.00003555
Iteration 181/1000 | Loss: 0.00003555
Iteration 182/1000 | Loss: 0.00003555
Iteration 183/1000 | Loss: 0.00003555
Iteration 184/1000 | Loss: 0.00003555
Iteration 185/1000 | Loss: 0.00003555
Iteration 186/1000 | Loss: 0.00003555
Iteration 187/1000 | Loss: 0.00003555
Iteration 188/1000 | Loss: 0.00003555
Iteration 189/1000 | Loss: 0.00003555
Iteration 190/1000 | Loss: 0.00003554
Iteration 191/1000 | Loss: 0.00003554
Iteration 192/1000 | Loss: 0.00003554
Iteration 193/1000 | Loss: 0.00003554
Iteration 194/1000 | Loss: 0.00003554
Iteration 195/1000 | Loss: 0.00003554
Iteration 196/1000 | Loss: 0.00003554
Iteration 197/1000 | Loss: 0.00003554
Iteration 198/1000 | Loss: 0.00003554
Iteration 199/1000 | Loss: 0.00003554
Iteration 200/1000 | Loss: 0.00003554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [3.554124850779772e-05, 3.554124850779772e-05, 3.554124850779772e-05, 3.554124850779772e-05, 3.554124850779772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.554124850779772e-05

Optimization complete. Final v2v error: 4.941518306732178 mm

Highest mean error: 5.606363296508789 mm for frame 121

Lowest mean error: 3.855102777481079 mm for frame 18

Saving results

Total time: 44.187594413757324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806474
Iteration 2/25 | Loss: 0.00128310
Iteration 3/25 | Loss: 0.00119176
Iteration 4/25 | Loss: 0.00117149
Iteration 5/25 | Loss: 0.00116660
Iteration 6/25 | Loss: 0.00116638
Iteration 7/25 | Loss: 0.00116638
Iteration 8/25 | Loss: 0.00116638
Iteration 9/25 | Loss: 0.00116638
Iteration 10/25 | Loss: 0.00116638
Iteration 11/25 | Loss: 0.00116638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001166384550742805, 0.001166384550742805, 0.001166384550742805, 0.001166384550742805, 0.001166384550742805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001166384550742805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66170025
Iteration 2/25 | Loss: 0.00081491
Iteration 3/25 | Loss: 0.00081484
Iteration 4/25 | Loss: 0.00081484
Iteration 5/25 | Loss: 0.00081484
Iteration 6/25 | Loss: 0.00081484
Iteration 7/25 | Loss: 0.00081484
Iteration 8/25 | Loss: 0.00081484
Iteration 9/25 | Loss: 0.00081484
Iteration 10/25 | Loss: 0.00081484
Iteration 11/25 | Loss: 0.00081484
Iteration 12/25 | Loss: 0.00081484
Iteration 13/25 | Loss: 0.00081484
Iteration 14/25 | Loss: 0.00081484
Iteration 15/25 | Loss: 0.00081484
Iteration 16/25 | Loss: 0.00081484
Iteration 17/25 | Loss: 0.00081484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008148414781317115, 0.0008148414781317115, 0.0008148414781317115, 0.0008148414781317115, 0.0008148414781317115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008148414781317115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081484
Iteration 2/1000 | Loss: 0.00003442
Iteration 3/1000 | Loss: 0.00002268
Iteration 4/1000 | Loss: 0.00001880
Iteration 5/1000 | Loss: 0.00001764
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001622
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001518
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001435
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00001430
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001420
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001414
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001413
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001412
Iteration 29/1000 | Loss: 0.00001412
Iteration 30/1000 | Loss: 0.00001411
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001409
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001409
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001407
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001402
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001401
Iteration 53/1000 | Loss: 0.00001401
Iteration 54/1000 | Loss: 0.00001401
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001400
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001398
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001397
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001397
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001396
Iteration 86/1000 | Loss: 0.00001396
Iteration 87/1000 | Loss: 0.00001396
Iteration 88/1000 | Loss: 0.00001396
Iteration 89/1000 | Loss: 0.00001396
Iteration 90/1000 | Loss: 0.00001396
Iteration 91/1000 | Loss: 0.00001396
Iteration 92/1000 | Loss: 0.00001396
Iteration 93/1000 | Loss: 0.00001396
Iteration 94/1000 | Loss: 0.00001396
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001396
Iteration 98/1000 | Loss: 0.00001396
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001395
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001395
Iteration 105/1000 | Loss: 0.00001395
Iteration 106/1000 | Loss: 0.00001395
Iteration 107/1000 | Loss: 0.00001395
Iteration 108/1000 | Loss: 0.00001395
Iteration 109/1000 | Loss: 0.00001395
Iteration 110/1000 | Loss: 0.00001395
Iteration 111/1000 | Loss: 0.00001395
Iteration 112/1000 | Loss: 0.00001395
Iteration 113/1000 | Loss: 0.00001395
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001394
Iteration 117/1000 | Loss: 0.00001394
Iteration 118/1000 | Loss: 0.00001394
Iteration 119/1000 | Loss: 0.00001394
Iteration 120/1000 | Loss: 0.00001394
Iteration 121/1000 | Loss: 0.00001394
Iteration 122/1000 | Loss: 0.00001394
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001394
Iteration 129/1000 | Loss: 0.00001394
Iteration 130/1000 | Loss: 0.00001394
Iteration 131/1000 | Loss: 0.00001394
Iteration 132/1000 | Loss: 0.00001394
Iteration 133/1000 | Loss: 0.00001394
Iteration 134/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.3939535165263806e-05, 1.3939535165263806e-05, 1.3939535165263806e-05, 1.3939535165263806e-05, 1.3939535165263806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3939535165263806e-05

Optimization complete. Final v2v error: 3.1895034313201904 mm

Highest mean error: 3.5841660499572754 mm for frame 28

Lowest mean error: 2.8768692016601562 mm for frame 235

Saving results

Total time: 39.95129728317261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807076
Iteration 2/25 | Loss: 0.00159932
Iteration 3/25 | Loss: 0.00135458
Iteration 4/25 | Loss: 0.00130445
Iteration 5/25 | Loss: 0.00129493
Iteration 6/25 | Loss: 0.00129439
Iteration 7/25 | Loss: 0.00129439
Iteration 8/25 | Loss: 0.00129439
Iteration 9/25 | Loss: 0.00129439
Iteration 10/25 | Loss: 0.00129439
Iteration 11/25 | Loss: 0.00129439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012943908805027604, 0.0012943908805027604, 0.0012943908805027604, 0.0012943908805027604, 0.0012943908805027604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012943908805027604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43200588
Iteration 2/25 | Loss: 0.00070006
Iteration 3/25 | Loss: 0.00070006
Iteration 4/25 | Loss: 0.00070006
Iteration 5/25 | Loss: 0.00070005
Iteration 6/25 | Loss: 0.00070005
Iteration 7/25 | Loss: 0.00070005
Iteration 8/25 | Loss: 0.00070005
Iteration 9/25 | Loss: 0.00070005
Iteration 10/25 | Loss: 0.00070005
Iteration 11/25 | Loss: 0.00070005
Iteration 12/25 | Loss: 0.00070005
Iteration 13/25 | Loss: 0.00070005
Iteration 14/25 | Loss: 0.00070005
Iteration 15/25 | Loss: 0.00070005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007000535842962563, 0.0007000535842962563, 0.0007000535842962563, 0.0007000535842962563, 0.0007000535842962563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007000535842962563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070005
Iteration 2/1000 | Loss: 0.00003489
Iteration 3/1000 | Loss: 0.00002593
Iteration 4/1000 | Loss: 0.00002410
Iteration 5/1000 | Loss: 0.00002320
Iteration 6/1000 | Loss: 0.00002257
Iteration 7/1000 | Loss: 0.00002203
Iteration 8/1000 | Loss: 0.00002171
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002116
Iteration 11/1000 | Loss: 0.00002116
Iteration 12/1000 | Loss: 0.00002109
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00002107
Iteration 15/1000 | Loss: 0.00002106
Iteration 16/1000 | Loss: 0.00002105
Iteration 17/1000 | Loss: 0.00002105
Iteration 18/1000 | Loss: 0.00002104
Iteration 19/1000 | Loss: 0.00002103
Iteration 20/1000 | Loss: 0.00002102
Iteration 21/1000 | Loss: 0.00002098
Iteration 22/1000 | Loss: 0.00002098
Iteration 23/1000 | Loss: 0.00002096
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002093
Iteration 26/1000 | Loss: 0.00002089
Iteration 27/1000 | Loss: 0.00002088
Iteration 28/1000 | Loss: 0.00002087
Iteration 29/1000 | Loss: 0.00002082
Iteration 30/1000 | Loss: 0.00002080
Iteration 31/1000 | Loss: 0.00002080
Iteration 32/1000 | Loss: 0.00002080
Iteration 33/1000 | Loss: 0.00002079
Iteration 34/1000 | Loss: 0.00002079
Iteration 35/1000 | Loss: 0.00002079
Iteration 36/1000 | Loss: 0.00002079
Iteration 37/1000 | Loss: 0.00002079
Iteration 38/1000 | Loss: 0.00002079
Iteration 39/1000 | Loss: 0.00002079
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002079
Iteration 42/1000 | Loss: 0.00002079
Iteration 43/1000 | Loss: 0.00002079
Iteration 44/1000 | Loss: 0.00002079
Iteration 45/1000 | Loss: 0.00002079
Iteration 46/1000 | Loss: 0.00002079
Iteration 47/1000 | Loss: 0.00002079
Iteration 48/1000 | Loss: 0.00002079
Iteration 49/1000 | Loss: 0.00002079
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [2.079084879369475e-05, 2.079084879369475e-05, 2.079084879369475e-05, 2.079084879369475e-05, 2.079084879369475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.079084879369475e-05

Optimization complete. Final v2v error: 3.8443565368652344 mm

Highest mean error: 4.45104455947876 mm for frame 217

Lowest mean error: 3.46390962600708 mm for frame 4

Saving results

Total time: 30.244163513183594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776599
Iteration 2/25 | Loss: 0.00156284
Iteration 3/25 | Loss: 0.00131853
Iteration 4/25 | Loss: 0.00128093
Iteration 5/25 | Loss: 0.00127494
Iteration 6/25 | Loss: 0.00127438
Iteration 7/25 | Loss: 0.00127438
Iteration 8/25 | Loss: 0.00127438
Iteration 9/25 | Loss: 0.00127438
Iteration 10/25 | Loss: 0.00127438
Iteration 11/25 | Loss: 0.00127438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012743781553581357, 0.0012743781553581357, 0.0012743781553581357, 0.0012743781553581357, 0.0012743781553581357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012743781553581357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31833994
Iteration 2/25 | Loss: 0.00057008
Iteration 3/25 | Loss: 0.00057004
Iteration 4/25 | Loss: 0.00057004
Iteration 5/25 | Loss: 0.00057004
Iteration 6/25 | Loss: 0.00057004
Iteration 7/25 | Loss: 0.00057004
Iteration 8/25 | Loss: 0.00057004
Iteration 9/25 | Loss: 0.00057004
Iteration 10/25 | Loss: 0.00057004
Iteration 11/25 | Loss: 0.00057004
Iteration 12/25 | Loss: 0.00057004
Iteration 13/25 | Loss: 0.00057004
Iteration 14/25 | Loss: 0.00057004
Iteration 15/25 | Loss: 0.00057004
Iteration 16/25 | Loss: 0.00057004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005700401379726827, 0.0005700401379726827, 0.0005700401379726827, 0.0005700401379726827, 0.0005700401379726827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005700401379726827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057004
Iteration 2/1000 | Loss: 0.00004736
Iteration 3/1000 | Loss: 0.00003653
Iteration 4/1000 | Loss: 0.00003132
Iteration 5/1000 | Loss: 0.00002977
Iteration 6/1000 | Loss: 0.00002856
Iteration 7/1000 | Loss: 0.00002753
Iteration 8/1000 | Loss: 0.00002702
Iteration 9/1000 | Loss: 0.00002662
Iteration 10/1000 | Loss: 0.00002621
Iteration 11/1000 | Loss: 0.00002580
Iteration 12/1000 | Loss: 0.00002558
Iteration 13/1000 | Loss: 0.00002536
Iteration 14/1000 | Loss: 0.00002518
Iteration 15/1000 | Loss: 0.00002517
Iteration 16/1000 | Loss: 0.00002506
Iteration 17/1000 | Loss: 0.00002497
Iteration 18/1000 | Loss: 0.00002490
Iteration 19/1000 | Loss: 0.00002489
Iteration 20/1000 | Loss: 0.00002489
Iteration 21/1000 | Loss: 0.00002488
Iteration 22/1000 | Loss: 0.00002487
Iteration 23/1000 | Loss: 0.00002487
Iteration 24/1000 | Loss: 0.00002484
Iteration 25/1000 | Loss: 0.00002484
Iteration 26/1000 | Loss: 0.00002483
Iteration 27/1000 | Loss: 0.00002483
Iteration 28/1000 | Loss: 0.00002483
Iteration 29/1000 | Loss: 0.00002483
Iteration 30/1000 | Loss: 0.00002483
Iteration 31/1000 | Loss: 0.00002483
Iteration 32/1000 | Loss: 0.00002483
Iteration 33/1000 | Loss: 0.00002483
Iteration 34/1000 | Loss: 0.00002483
Iteration 35/1000 | Loss: 0.00002483
Iteration 36/1000 | Loss: 0.00002483
Iteration 37/1000 | Loss: 0.00002482
Iteration 38/1000 | Loss: 0.00002482
Iteration 39/1000 | Loss: 0.00002482
Iteration 40/1000 | Loss: 0.00002482
Iteration 41/1000 | Loss: 0.00002482
Iteration 42/1000 | Loss: 0.00002482
Iteration 43/1000 | Loss: 0.00002479
Iteration 44/1000 | Loss: 0.00002479
Iteration 45/1000 | Loss: 0.00002479
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002478
Iteration 49/1000 | Loss: 0.00002478
Iteration 50/1000 | Loss: 0.00002478
Iteration 51/1000 | Loss: 0.00002478
Iteration 52/1000 | Loss: 0.00002478
Iteration 53/1000 | Loss: 0.00002478
Iteration 54/1000 | Loss: 0.00002478
Iteration 55/1000 | Loss: 0.00002478
Iteration 56/1000 | Loss: 0.00002477
Iteration 57/1000 | Loss: 0.00002477
Iteration 58/1000 | Loss: 0.00002476
Iteration 59/1000 | Loss: 0.00002475
Iteration 60/1000 | Loss: 0.00002475
Iteration 61/1000 | Loss: 0.00002475
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002474
Iteration 64/1000 | Loss: 0.00002474
Iteration 65/1000 | Loss: 0.00002473
Iteration 66/1000 | Loss: 0.00002473
Iteration 67/1000 | Loss: 0.00002473
Iteration 68/1000 | Loss: 0.00002473
Iteration 69/1000 | Loss: 0.00002473
Iteration 70/1000 | Loss: 0.00002473
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002472
Iteration 73/1000 | Loss: 0.00002471
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002471
Iteration 76/1000 | Loss: 0.00002471
Iteration 77/1000 | Loss: 0.00002471
Iteration 78/1000 | Loss: 0.00002471
Iteration 79/1000 | Loss: 0.00002471
Iteration 80/1000 | Loss: 0.00002471
Iteration 81/1000 | Loss: 0.00002470
Iteration 82/1000 | Loss: 0.00002470
Iteration 83/1000 | Loss: 0.00002470
Iteration 84/1000 | Loss: 0.00002470
Iteration 85/1000 | Loss: 0.00002470
Iteration 86/1000 | Loss: 0.00002470
Iteration 87/1000 | Loss: 0.00002470
Iteration 88/1000 | Loss: 0.00002470
Iteration 89/1000 | Loss: 0.00002469
Iteration 90/1000 | Loss: 0.00002469
Iteration 91/1000 | Loss: 0.00002469
Iteration 92/1000 | Loss: 0.00002469
Iteration 93/1000 | Loss: 0.00002469
Iteration 94/1000 | Loss: 0.00002469
Iteration 95/1000 | Loss: 0.00002469
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002468
Iteration 98/1000 | Loss: 0.00002468
Iteration 99/1000 | Loss: 0.00002468
Iteration 100/1000 | Loss: 0.00002468
Iteration 101/1000 | Loss: 0.00002468
Iteration 102/1000 | Loss: 0.00002467
Iteration 103/1000 | Loss: 0.00002467
Iteration 104/1000 | Loss: 0.00002467
Iteration 105/1000 | Loss: 0.00002467
Iteration 106/1000 | Loss: 0.00002467
Iteration 107/1000 | Loss: 0.00002467
Iteration 108/1000 | Loss: 0.00002467
Iteration 109/1000 | Loss: 0.00002466
Iteration 110/1000 | Loss: 0.00002466
Iteration 111/1000 | Loss: 0.00002466
Iteration 112/1000 | Loss: 0.00002466
Iteration 113/1000 | Loss: 0.00002466
Iteration 114/1000 | Loss: 0.00002465
Iteration 115/1000 | Loss: 0.00002465
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002465
Iteration 119/1000 | Loss: 0.00002465
Iteration 120/1000 | Loss: 0.00002464
Iteration 121/1000 | Loss: 0.00002464
Iteration 122/1000 | Loss: 0.00002464
Iteration 123/1000 | Loss: 0.00002464
Iteration 124/1000 | Loss: 0.00002464
Iteration 125/1000 | Loss: 0.00002464
Iteration 126/1000 | Loss: 0.00002464
Iteration 127/1000 | Loss: 0.00002464
Iteration 128/1000 | Loss: 0.00002464
Iteration 129/1000 | Loss: 0.00002464
Iteration 130/1000 | Loss: 0.00002464
Iteration 131/1000 | Loss: 0.00002464
Iteration 132/1000 | Loss: 0.00002464
Iteration 133/1000 | Loss: 0.00002464
Iteration 134/1000 | Loss: 0.00002463
Iteration 135/1000 | Loss: 0.00002463
Iteration 136/1000 | Loss: 0.00002463
Iteration 137/1000 | Loss: 0.00002463
Iteration 138/1000 | Loss: 0.00002463
Iteration 139/1000 | Loss: 0.00002463
Iteration 140/1000 | Loss: 0.00002463
Iteration 141/1000 | Loss: 0.00002463
Iteration 142/1000 | Loss: 0.00002463
Iteration 143/1000 | Loss: 0.00002463
Iteration 144/1000 | Loss: 0.00002463
Iteration 145/1000 | Loss: 0.00002463
Iteration 146/1000 | Loss: 0.00002463
Iteration 147/1000 | Loss: 0.00002463
Iteration 148/1000 | Loss: 0.00002463
Iteration 149/1000 | Loss: 0.00002463
Iteration 150/1000 | Loss: 0.00002463
Iteration 151/1000 | Loss: 0.00002463
Iteration 152/1000 | Loss: 0.00002463
Iteration 153/1000 | Loss: 0.00002463
Iteration 154/1000 | Loss: 0.00002463
Iteration 155/1000 | Loss: 0.00002463
Iteration 156/1000 | Loss: 0.00002463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.4633125576656312e-05, 2.4633125576656312e-05, 2.4633125576656312e-05, 2.4633125576656312e-05, 2.4633125576656312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4633125576656312e-05

Optimization complete. Final v2v error: 4.097633361816406 mm

Highest mean error: 5.074806213378906 mm for frame 137

Lowest mean error: 3.5483744144439697 mm for frame 58

Saving results

Total time: 44.87818145751953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389452
Iteration 2/25 | Loss: 0.00128006
Iteration 3/25 | Loss: 0.00120266
Iteration 4/25 | Loss: 0.00119320
Iteration 5/25 | Loss: 0.00119044
Iteration 6/25 | Loss: 0.00118964
Iteration 7/25 | Loss: 0.00118948
Iteration 8/25 | Loss: 0.00118948
Iteration 9/25 | Loss: 0.00118948
Iteration 10/25 | Loss: 0.00118948
Iteration 11/25 | Loss: 0.00118948
Iteration 12/25 | Loss: 0.00118948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011894779745489359, 0.0011894779745489359, 0.0011894779745489359, 0.0011894779745489359, 0.0011894779745489359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011894779745489359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74585128
Iteration 2/25 | Loss: 0.00078438
Iteration 3/25 | Loss: 0.00078437
Iteration 4/25 | Loss: 0.00078437
Iteration 5/25 | Loss: 0.00078437
Iteration 6/25 | Loss: 0.00078437
Iteration 7/25 | Loss: 0.00078437
Iteration 8/25 | Loss: 0.00078437
Iteration 9/25 | Loss: 0.00078437
Iteration 10/25 | Loss: 0.00078437
Iteration 11/25 | Loss: 0.00078437
Iteration 12/25 | Loss: 0.00078437
Iteration 13/25 | Loss: 0.00078437
Iteration 14/25 | Loss: 0.00078437
Iteration 15/25 | Loss: 0.00078437
Iteration 16/25 | Loss: 0.00078437
Iteration 17/25 | Loss: 0.00078437
Iteration 18/25 | Loss: 0.00078437
Iteration 19/25 | Loss: 0.00078437
Iteration 20/25 | Loss: 0.00078437
Iteration 21/25 | Loss: 0.00078437
Iteration 22/25 | Loss: 0.00078437
Iteration 23/25 | Loss: 0.00078437
Iteration 24/25 | Loss: 0.00078437
Iteration 25/25 | Loss: 0.00078437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078437
Iteration 2/1000 | Loss: 0.00002928
Iteration 3/1000 | Loss: 0.00001824
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001230
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001131
Iteration 9/1000 | Loss: 0.00001107
Iteration 10/1000 | Loss: 0.00001103
Iteration 11/1000 | Loss: 0.00001100
Iteration 12/1000 | Loss: 0.00001099
Iteration 13/1000 | Loss: 0.00001099
Iteration 14/1000 | Loss: 0.00001095
Iteration 15/1000 | Loss: 0.00001095
Iteration 16/1000 | Loss: 0.00001086
Iteration 17/1000 | Loss: 0.00001083
Iteration 18/1000 | Loss: 0.00001083
Iteration 19/1000 | Loss: 0.00001082
Iteration 20/1000 | Loss: 0.00001080
Iteration 21/1000 | Loss: 0.00001079
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001076
Iteration 26/1000 | Loss: 0.00001076
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001075
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001074
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001070
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001069
Iteration 36/1000 | Loss: 0.00001068
Iteration 37/1000 | Loss: 0.00001068
Iteration 38/1000 | Loss: 0.00001067
Iteration 39/1000 | Loss: 0.00001067
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001064
Iteration 43/1000 | Loss: 0.00001064
Iteration 44/1000 | Loss: 0.00001063
Iteration 45/1000 | Loss: 0.00001060
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001057
Iteration 48/1000 | Loss: 0.00001057
Iteration 49/1000 | Loss: 0.00001057
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001055
Iteration 53/1000 | Loss: 0.00001055
Iteration 54/1000 | Loss: 0.00001054
Iteration 55/1000 | Loss: 0.00001054
Iteration 56/1000 | Loss: 0.00001053
Iteration 57/1000 | Loss: 0.00001053
Iteration 58/1000 | Loss: 0.00001053
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001052
Iteration 61/1000 | Loss: 0.00001052
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001051
Iteration 64/1000 | Loss: 0.00001050
Iteration 65/1000 | Loss: 0.00001050
Iteration 66/1000 | Loss: 0.00001049
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001049
Iteration 69/1000 | Loss: 0.00001049
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001046
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001045
Iteration 80/1000 | Loss: 0.00001045
Iteration 81/1000 | Loss: 0.00001045
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001045
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001044
Iteration 88/1000 | Loss: 0.00001044
Iteration 89/1000 | Loss: 0.00001043
Iteration 90/1000 | Loss: 0.00001043
Iteration 91/1000 | Loss: 0.00001043
Iteration 92/1000 | Loss: 0.00001043
Iteration 93/1000 | Loss: 0.00001043
Iteration 94/1000 | Loss: 0.00001043
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001042
Iteration 97/1000 | Loss: 0.00001042
Iteration 98/1000 | Loss: 0.00001042
Iteration 99/1000 | Loss: 0.00001042
Iteration 100/1000 | Loss: 0.00001042
Iteration 101/1000 | Loss: 0.00001041
Iteration 102/1000 | Loss: 0.00001041
Iteration 103/1000 | Loss: 0.00001041
Iteration 104/1000 | Loss: 0.00001041
Iteration 105/1000 | Loss: 0.00001041
Iteration 106/1000 | Loss: 0.00001041
Iteration 107/1000 | Loss: 0.00001041
Iteration 108/1000 | Loss: 0.00001040
Iteration 109/1000 | Loss: 0.00001040
Iteration 110/1000 | Loss: 0.00001040
Iteration 111/1000 | Loss: 0.00001040
Iteration 112/1000 | Loss: 0.00001040
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001040
Iteration 115/1000 | Loss: 0.00001040
Iteration 116/1000 | Loss: 0.00001040
Iteration 117/1000 | Loss: 0.00001040
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001038
Iteration 121/1000 | Loss: 0.00001038
Iteration 122/1000 | Loss: 0.00001038
Iteration 123/1000 | Loss: 0.00001038
Iteration 124/1000 | Loss: 0.00001038
Iteration 125/1000 | Loss: 0.00001038
Iteration 126/1000 | Loss: 0.00001038
Iteration 127/1000 | Loss: 0.00001038
Iteration 128/1000 | Loss: 0.00001038
Iteration 129/1000 | Loss: 0.00001038
Iteration 130/1000 | Loss: 0.00001038
Iteration 131/1000 | Loss: 0.00001038
Iteration 132/1000 | Loss: 0.00001038
Iteration 133/1000 | Loss: 0.00001037
Iteration 134/1000 | Loss: 0.00001037
Iteration 135/1000 | Loss: 0.00001037
Iteration 136/1000 | Loss: 0.00001036
Iteration 137/1000 | Loss: 0.00001036
Iteration 138/1000 | Loss: 0.00001036
Iteration 139/1000 | Loss: 0.00001036
Iteration 140/1000 | Loss: 0.00001036
Iteration 141/1000 | Loss: 0.00001036
Iteration 142/1000 | Loss: 0.00001036
Iteration 143/1000 | Loss: 0.00001035
Iteration 144/1000 | Loss: 0.00001035
Iteration 145/1000 | Loss: 0.00001035
Iteration 146/1000 | Loss: 0.00001034
Iteration 147/1000 | Loss: 0.00001034
Iteration 148/1000 | Loss: 0.00001034
Iteration 149/1000 | Loss: 0.00001034
Iteration 150/1000 | Loss: 0.00001033
Iteration 151/1000 | Loss: 0.00001033
Iteration 152/1000 | Loss: 0.00001033
Iteration 153/1000 | Loss: 0.00001033
Iteration 154/1000 | Loss: 0.00001033
Iteration 155/1000 | Loss: 0.00001032
Iteration 156/1000 | Loss: 0.00001032
Iteration 157/1000 | Loss: 0.00001032
Iteration 158/1000 | Loss: 0.00001032
Iteration 159/1000 | Loss: 0.00001032
Iteration 160/1000 | Loss: 0.00001032
Iteration 161/1000 | Loss: 0.00001032
Iteration 162/1000 | Loss: 0.00001031
Iteration 163/1000 | Loss: 0.00001031
Iteration 164/1000 | Loss: 0.00001031
Iteration 165/1000 | Loss: 0.00001031
Iteration 166/1000 | Loss: 0.00001031
Iteration 167/1000 | Loss: 0.00001031
Iteration 168/1000 | Loss: 0.00001031
Iteration 169/1000 | Loss: 0.00001031
Iteration 170/1000 | Loss: 0.00001031
Iteration 171/1000 | Loss: 0.00001031
Iteration 172/1000 | Loss: 0.00001031
Iteration 173/1000 | Loss: 0.00001031
Iteration 174/1000 | Loss: 0.00001031
Iteration 175/1000 | Loss: 0.00001030
Iteration 176/1000 | Loss: 0.00001030
Iteration 177/1000 | Loss: 0.00001030
Iteration 178/1000 | Loss: 0.00001030
Iteration 179/1000 | Loss: 0.00001030
Iteration 180/1000 | Loss: 0.00001030
Iteration 181/1000 | Loss: 0.00001030
Iteration 182/1000 | Loss: 0.00001030
Iteration 183/1000 | Loss: 0.00001029
Iteration 184/1000 | Loss: 0.00001029
Iteration 185/1000 | Loss: 0.00001029
Iteration 186/1000 | Loss: 0.00001029
Iteration 187/1000 | Loss: 0.00001029
Iteration 188/1000 | Loss: 0.00001029
Iteration 189/1000 | Loss: 0.00001029
Iteration 190/1000 | Loss: 0.00001029
Iteration 191/1000 | Loss: 0.00001029
Iteration 192/1000 | Loss: 0.00001029
Iteration 193/1000 | Loss: 0.00001029
Iteration 194/1000 | Loss: 0.00001029
Iteration 195/1000 | Loss: 0.00001028
Iteration 196/1000 | Loss: 0.00001028
Iteration 197/1000 | Loss: 0.00001028
Iteration 198/1000 | Loss: 0.00001028
Iteration 199/1000 | Loss: 0.00001028
Iteration 200/1000 | Loss: 0.00001028
Iteration 201/1000 | Loss: 0.00001028
Iteration 202/1000 | Loss: 0.00001028
Iteration 203/1000 | Loss: 0.00001028
Iteration 204/1000 | Loss: 0.00001028
Iteration 205/1000 | Loss: 0.00001028
Iteration 206/1000 | Loss: 0.00001028
Iteration 207/1000 | Loss: 0.00001028
Iteration 208/1000 | Loss: 0.00001028
Iteration 209/1000 | Loss: 0.00001028
Iteration 210/1000 | Loss: 0.00001028
Iteration 211/1000 | Loss: 0.00001028
Iteration 212/1000 | Loss: 0.00001028
Iteration 213/1000 | Loss: 0.00001028
Iteration 214/1000 | Loss: 0.00001028
Iteration 215/1000 | Loss: 0.00001027
Iteration 216/1000 | Loss: 0.00001027
Iteration 217/1000 | Loss: 0.00001027
Iteration 218/1000 | Loss: 0.00001027
Iteration 219/1000 | Loss: 0.00001027
Iteration 220/1000 | Loss: 0.00001027
Iteration 221/1000 | Loss: 0.00001027
Iteration 222/1000 | Loss: 0.00001027
Iteration 223/1000 | Loss: 0.00001027
Iteration 224/1000 | Loss: 0.00001027
Iteration 225/1000 | Loss: 0.00001027
Iteration 226/1000 | Loss: 0.00001026
Iteration 227/1000 | Loss: 0.00001026
Iteration 228/1000 | Loss: 0.00001026
Iteration 229/1000 | Loss: 0.00001026
Iteration 230/1000 | Loss: 0.00001026
Iteration 231/1000 | Loss: 0.00001026
Iteration 232/1000 | Loss: 0.00001026
Iteration 233/1000 | Loss: 0.00001026
Iteration 234/1000 | Loss: 0.00001026
Iteration 235/1000 | Loss: 0.00001026
Iteration 236/1000 | Loss: 0.00001026
Iteration 237/1000 | Loss: 0.00001026
Iteration 238/1000 | Loss: 0.00001026
Iteration 239/1000 | Loss: 0.00001026
Iteration 240/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.0263359399687033e-05, 1.0263359399687033e-05, 1.0263359399687033e-05, 1.0263359399687033e-05, 1.0263359399687033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0263359399687033e-05

Optimization complete. Final v2v error: 2.7503631114959717 mm

Highest mean error: 3.2079997062683105 mm for frame 67

Lowest mean error: 2.637937068939209 mm for frame 114

Saving results

Total time: 42.12529921531677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886212
Iteration 2/25 | Loss: 0.00139002
Iteration 3/25 | Loss: 0.00126786
Iteration 4/25 | Loss: 0.00124795
Iteration 5/25 | Loss: 0.00124083
Iteration 6/25 | Loss: 0.00123934
Iteration 7/25 | Loss: 0.00123934
Iteration 8/25 | Loss: 0.00123934
Iteration 9/25 | Loss: 0.00123934
Iteration 10/25 | Loss: 0.00123934
Iteration 11/25 | Loss: 0.00123934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001239338773302734, 0.001239338773302734, 0.001239338773302734, 0.001239338773302734, 0.001239338773302734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001239338773302734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44971418
Iteration 2/25 | Loss: 0.00073893
Iteration 3/25 | Loss: 0.00073891
Iteration 4/25 | Loss: 0.00073891
Iteration 5/25 | Loss: 0.00073891
Iteration 6/25 | Loss: 0.00073891
Iteration 7/25 | Loss: 0.00073891
Iteration 8/25 | Loss: 0.00073891
Iteration 9/25 | Loss: 0.00073891
Iteration 10/25 | Loss: 0.00073891
Iteration 11/25 | Loss: 0.00073891
Iteration 12/25 | Loss: 0.00073891
Iteration 13/25 | Loss: 0.00073891
Iteration 14/25 | Loss: 0.00073891
Iteration 15/25 | Loss: 0.00073891
Iteration 16/25 | Loss: 0.00073891
Iteration 17/25 | Loss: 0.00073891
Iteration 18/25 | Loss: 0.00073891
Iteration 19/25 | Loss: 0.00073891
Iteration 20/25 | Loss: 0.00073891
Iteration 21/25 | Loss: 0.00073891
Iteration 22/25 | Loss: 0.00073891
Iteration 23/25 | Loss: 0.00073891
Iteration 24/25 | Loss: 0.00073891
Iteration 25/25 | Loss: 0.00073891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073891
Iteration 2/1000 | Loss: 0.00004770
Iteration 3/1000 | Loss: 0.00003114
Iteration 4/1000 | Loss: 0.00002452
Iteration 5/1000 | Loss: 0.00002279
Iteration 6/1000 | Loss: 0.00002181
Iteration 7/1000 | Loss: 0.00002121
Iteration 8/1000 | Loss: 0.00002071
Iteration 9/1000 | Loss: 0.00002021
Iteration 10/1000 | Loss: 0.00001995
Iteration 11/1000 | Loss: 0.00001970
Iteration 12/1000 | Loss: 0.00001963
Iteration 13/1000 | Loss: 0.00001945
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001928
Iteration 18/1000 | Loss: 0.00001922
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001901
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001900
Iteration 25/1000 | Loss: 0.00001900
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001892
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001888
Iteration 37/1000 | Loss: 0.00001888
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001886
Iteration 40/1000 | Loss: 0.00001885
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001882
Iteration 44/1000 | Loss: 0.00001882
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001881
Iteration 47/1000 | Loss: 0.00001879
Iteration 48/1000 | Loss: 0.00001879
Iteration 49/1000 | Loss: 0.00001878
Iteration 50/1000 | Loss: 0.00001878
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001877
Iteration 53/1000 | Loss: 0.00001877
Iteration 54/1000 | Loss: 0.00001877
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001873
Iteration 57/1000 | Loss: 0.00001873
Iteration 58/1000 | Loss: 0.00001873
Iteration 59/1000 | Loss: 0.00001870
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001869
Iteration 64/1000 | Loss: 0.00001869
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001865
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001864
Iteration 81/1000 | Loss: 0.00001864
Iteration 82/1000 | Loss: 0.00001864
Iteration 83/1000 | Loss: 0.00001864
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001863
Iteration 87/1000 | Loss: 0.00001863
Iteration 88/1000 | Loss: 0.00001863
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001862
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001861
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001860
Iteration 102/1000 | Loss: 0.00001860
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001859
Iteration 106/1000 | Loss: 0.00001859
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001858
Iteration 112/1000 | Loss: 0.00001858
Iteration 113/1000 | Loss: 0.00001858
Iteration 114/1000 | Loss: 0.00001858
Iteration 115/1000 | Loss: 0.00001857
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001856
Iteration 118/1000 | Loss: 0.00001856
Iteration 119/1000 | Loss: 0.00001856
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001854
Iteration 129/1000 | Loss: 0.00001854
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001852
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001851
Iteration 148/1000 | Loss: 0.00001851
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001851
Iteration 155/1000 | Loss: 0.00001851
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001850
Iteration 160/1000 | Loss: 0.00001850
Iteration 161/1000 | Loss: 0.00001850
Iteration 162/1000 | Loss: 0.00001850
Iteration 163/1000 | Loss: 0.00001850
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001849
Iteration 170/1000 | Loss: 0.00001849
Iteration 171/1000 | Loss: 0.00001849
Iteration 172/1000 | Loss: 0.00001849
Iteration 173/1000 | Loss: 0.00001849
Iteration 174/1000 | Loss: 0.00001849
Iteration 175/1000 | Loss: 0.00001849
Iteration 176/1000 | Loss: 0.00001849
Iteration 177/1000 | Loss: 0.00001849
Iteration 178/1000 | Loss: 0.00001848
Iteration 179/1000 | Loss: 0.00001848
Iteration 180/1000 | Loss: 0.00001848
Iteration 181/1000 | Loss: 0.00001848
Iteration 182/1000 | Loss: 0.00001848
Iteration 183/1000 | Loss: 0.00001848
Iteration 184/1000 | Loss: 0.00001848
Iteration 185/1000 | Loss: 0.00001848
Iteration 186/1000 | Loss: 0.00001848
Iteration 187/1000 | Loss: 0.00001848
Iteration 188/1000 | Loss: 0.00001848
Iteration 189/1000 | Loss: 0.00001848
Iteration 190/1000 | Loss: 0.00001848
Iteration 191/1000 | Loss: 0.00001848
Iteration 192/1000 | Loss: 0.00001848
Iteration 193/1000 | Loss: 0.00001848
Iteration 194/1000 | Loss: 0.00001848
Iteration 195/1000 | Loss: 0.00001848
Iteration 196/1000 | Loss: 0.00001848
Iteration 197/1000 | Loss: 0.00001848
Iteration 198/1000 | Loss: 0.00001848
Iteration 199/1000 | Loss: 0.00001848
Iteration 200/1000 | Loss: 0.00001848
Iteration 201/1000 | Loss: 0.00001848
Iteration 202/1000 | Loss: 0.00001848
Iteration 203/1000 | Loss: 0.00001848
Iteration 204/1000 | Loss: 0.00001848
Iteration 205/1000 | Loss: 0.00001848
Iteration 206/1000 | Loss: 0.00001848
Iteration 207/1000 | Loss: 0.00001848
Iteration 208/1000 | Loss: 0.00001848
Iteration 209/1000 | Loss: 0.00001848
Iteration 210/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.8476450350135565e-05, 1.8476450350135565e-05, 1.8476450350135565e-05, 1.8476450350135565e-05, 1.8476450350135565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8476450350135565e-05

Optimization complete. Final v2v error: 3.627316474914551 mm

Highest mean error: 5.171657562255859 mm for frame 69

Lowest mean error: 3.1844465732574463 mm for frame 50

Saving results

Total time: 47.02701163291931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059097
Iteration 2/25 | Loss: 0.00543646
Iteration 3/25 | Loss: 0.00373235
Iteration 4/25 | Loss: 0.00290241
Iteration 5/25 | Loss: 0.00251024
Iteration 6/25 | Loss: 0.00286978
Iteration 7/25 | Loss: 0.00226759
Iteration 8/25 | Loss: 0.00214739
Iteration 9/25 | Loss: 0.00195404
Iteration 10/25 | Loss: 0.00182068
Iteration 11/25 | Loss: 0.00169283
Iteration 12/25 | Loss: 0.00163222
Iteration 13/25 | Loss: 0.00157803
Iteration 14/25 | Loss: 0.00159247
Iteration 15/25 | Loss: 0.00155221
Iteration 16/25 | Loss: 0.00151408
Iteration 17/25 | Loss: 0.00150065
Iteration 18/25 | Loss: 0.00150889
Iteration 19/25 | Loss: 0.00150846
Iteration 20/25 | Loss: 0.00149221
Iteration 21/25 | Loss: 0.00148890
Iteration 22/25 | Loss: 0.00148247
Iteration 23/25 | Loss: 0.00146120
Iteration 24/25 | Loss: 0.00144529
Iteration 25/25 | Loss: 0.00144376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61056674
Iteration 2/25 | Loss: 0.00079127
Iteration 3/25 | Loss: 0.00082220
Iteration 4/25 | Loss: 0.00078723
Iteration 5/25 | Loss: 0.00078722
Iteration 6/25 | Loss: 0.00079501
Iteration 7/25 | Loss: 0.00084659
Iteration 8/25 | Loss: 0.00102647
Iteration 9/25 | Loss: 0.00073132
Iteration 10/25 | Loss: 0.00089927
Iteration 11/25 | Loss: 0.00070318
Iteration 12/25 | Loss: 0.00070318
Iteration 13/25 | Loss: 0.00070318
Iteration 14/25 | Loss: 0.00070318
Iteration 15/25 | Loss: 0.00070318
Iteration 16/25 | Loss: 0.00070318
Iteration 17/25 | Loss: 0.00070318
Iteration 18/25 | Loss: 0.00070318
Iteration 19/25 | Loss: 0.00070318
Iteration 20/25 | Loss: 0.00070318
Iteration 21/25 | Loss: 0.00070318
Iteration 22/25 | Loss: 0.00070318
Iteration 23/25 | Loss: 0.00070318
Iteration 24/25 | Loss: 0.00070318
Iteration 25/25 | Loss: 0.00070318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007031774148344994, 0.0007031774148344994, 0.0007031774148344994, 0.0007031774148344994, 0.0007031774148344994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007031774148344994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070318
Iteration 2/1000 | Loss: 0.00044851
Iteration 3/1000 | Loss: 0.00090316
Iteration 4/1000 | Loss: 0.00041644
Iteration 5/1000 | Loss: 0.00049605
Iteration 6/1000 | Loss: 0.00058976
Iteration 7/1000 | Loss: 0.00010482
Iteration 8/1000 | Loss: 0.00006676
Iteration 9/1000 | Loss: 0.00005277
Iteration 10/1000 | Loss: 0.00014020
Iteration 11/1000 | Loss: 0.00003542
Iteration 12/1000 | Loss: 0.00008414
Iteration 13/1000 | Loss: 0.00003283
Iteration 14/1000 | Loss: 0.00003170
Iteration 15/1000 | Loss: 0.00003090
Iteration 16/1000 | Loss: 0.00003031
Iteration 17/1000 | Loss: 0.00002967
Iteration 18/1000 | Loss: 0.00002922
Iteration 19/1000 | Loss: 0.00002878
Iteration 20/1000 | Loss: 0.00002856
Iteration 21/1000 | Loss: 0.00002832
Iteration 22/1000 | Loss: 0.00002831
Iteration 23/1000 | Loss: 0.00002814
Iteration 24/1000 | Loss: 0.00002807
Iteration 25/1000 | Loss: 0.00002803
Iteration 26/1000 | Loss: 0.00002802
Iteration 27/1000 | Loss: 0.00002801
Iteration 28/1000 | Loss: 0.00002798
Iteration 29/1000 | Loss: 0.00002798
Iteration 30/1000 | Loss: 0.00002797
Iteration 31/1000 | Loss: 0.00002792
Iteration 32/1000 | Loss: 0.00002788
Iteration 33/1000 | Loss: 0.00002788
Iteration 34/1000 | Loss: 0.00002785
Iteration 35/1000 | Loss: 0.00002785
Iteration 36/1000 | Loss: 0.00002784
Iteration 37/1000 | Loss: 0.00002784
Iteration 38/1000 | Loss: 0.00002784
Iteration 39/1000 | Loss: 0.00002784
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002784
Iteration 42/1000 | Loss: 0.00002784
Iteration 43/1000 | Loss: 0.00002784
Iteration 44/1000 | Loss: 0.00002784
Iteration 45/1000 | Loss: 0.00002783
Iteration 46/1000 | Loss: 0.00002783
Iteration 47/1000 | Loss: 0.00002783
Iteration 48/1000 | Loss: 0.00002782
Iteration 49/1000 | Loss: 0.00002782
Iteration 50/1000 | Loss: 0.00002781
Iteration 51/1000 | Loss: 0.00002781
Iteration 52/1000 | Loss: 0.00002781
Iteration 53/1000 | Loss: 0.00002781
Iteration 54/1000 | Loss: 0.00002780
Iteration 55/1000 | Loss: 0.00002780
Iteration 56/1000 | Loss: 0.00002780
Iteration 57/1000 | Loss: 0.00002780
Iteration 58/1000 | Loss: 0.00002780
Iteration 59/1000 | Loss: 0.00002780
Iteration 60/1000 | Loss: 0.00002779
Iteration 61/1000 | Loss: 0.00002779
Iteration 62/1000 | Loss: 0.00002779
Iteration 63/1000 | Loss: 0.00002779
Iteration 64/1000 | Loss: 0.00002779
Iteration 65/1000 | Loss: 0.00002779
Iteration 66/1000 | Loss: 0.00002779
Iteration 67/1000 | Loss: 0.00002779
Iteration 68/1000 | Loss: 0.00002778
Iteration 69/1000 | Loss: 0.00002778
Iteration 70/1000 | Loss: 0.00002778
Iteration 71/1000 | Loss: 0.00002778
Iteration 72/1000 | Loss: 0.00002778
Iteration 73/1000 | Loss: 0.00002778
Iteration 74/1000 | Loss: 0.00002778
Iteration 75/1000 | Loss: 0.00002778
Iteration 76/1000 | Loss: 0.00002778
Iteration 77/1000 | Loss: 0.00002777
Iteration 78/1000 | Loss: 0.00002777
Iteration 79/1000 | Loss: 0.00002777
Iteration 80/1000 | Loss: 0.00002776
Iteration 81/1000 | Loss: 0.00002776
Iteration 82/1000 | Loss: 0.00002776
Iteration 83/1000 | Loss: 0.00002776
Iteration 84/1000 | Loss: 0.00002776
Iteration 85/1000 | Loss: 0.00002776
Iteration 86/1000 | Loss: 0.00002776
Iteration 87/1000 | Loss: 0.00002775
Iteration 88/1000 | Loss: 0.00002775
Iteration 89/1000 | Loss: 0.00002775
Iteration 90/1000 | Loss: 0.00002775
Iteration 91/1000 | Loss: 0.00002775
Iteration 92/1000 | Loss: 0.00002775
Iteration 93/1000 | Loss: 0.00002775
Iteration 94/1000 | Loss: 0.00002775
Iteration 95/1000 | Loss: 0.00002775
Iteration 96/1000 | Loss: 0.00002775
Iteration 97/1000 | Loss: 0.00002775
Iteration 98/1000 | Loss: 0.00002775
Iteration 99/1000 | Loss: 0.00002775
Iteration 100/1000 | Loss: 0.00002775
Iteration 101/1000 | Loss: 0.00002774
Iteration 102/1000 | Loss: 0.00002774
Iteration 103/1000 | Loss: 0.00002774
Iteration 104/1000 | Loss: 0.00002774
Iteration 105/1000 | Loss: 0.00002774
Iteration 106/1000 | Loss: 0.00002774
Iteration 107/1000 | Loss: 0.00002774
Iteration 108/1000 | Loss: 0.00002774
Iteration 109/1000 | Loss: 0.00002774
Iteration 110/1000 | Loss: 0.00002774
Iteration 111/1000 | Loss: 0.00002774
Iteration 112/1000 | Loss: 0.00002773
Iteration 113/1000 | Loss: 0.00002773
Iteration 114/1000 | Loss: 0.00002773
Iteration 115/1000 | Loss: 0.00002773
Iteration 116/1000 | Loss: 0.00002773
Iteration 117/1000 | Loss: 0.00002773
Iteration 118/1000 | Loss: 0.00002773
Iteration 119/1000 | Loss: 0.00002773
Iteration 120/1000 | Loss: 0.00002773
Iteration 121/1000 | Loss: 0.00002773
Iteration 122/1000 | Loss: 0.00002773
Iteration 123/1000 | Loss: 0.00002773
Iteration 124/1000 | Loss: 0.00002772
Iteration 125/1000 | Loss: 0.00002772
Iteration 126/1000 | Loss: 0.00002772
Iteration 127/1000 | Loss: 0.00002772
Iteration 128/1000 | Loss: 0.00002772
Iteration 129/1000 | Loss: 0.00002772
Iteration 130/1000 | Loss: 0.00002772
Iteration 131/1000 | Loss: 0.00002772
Iteration 132/1000 | Loss: 0.00002772
Iteration 133/1000 | Loss: 0.00002772
Iteration 134/1000 | Loss: 0.00002771
Iteration 135/1000 | Loss: 0.00002771
Iteration 136/1000 | Loss: 0.00002771
Iteration 137/1000 | Loss: 0.00002771
Iteration 138/1000 | Loss: 0.00002771
Iteration 139/1000 | Loss: 0.00002771
Iteration 140/1000 | Loss: 0.00002771
Iteration 141/1000 | Loss: 0.00002771
Iteration 142/1000 | Loss: 0.00002771
Iteration 143/1000 | Loss: 0.00002771
Iteration 144/1000 | Loss: 0.00002771
Iteration 145/1000 | Loss: 0.00002770
Iteration 146/1000 | Loss: 0.00002770
Iteration 147/1000 | Loss: 0.00002770
Iteration 148/1000 | Loss: 0.00002770
Iteration 149/1000 | Loss: 0.00002770
Iteration 150/1000 | Loss: 0.00002770
Iteration 151/1000 | Loss: 0.00002770
Iteration 152/1000 | Loss: 0.00002770
Iteration 153/1000 | Loss: 0.00002769
Iteration 154/1000 | Loss: 0.00002769
Iteration 155/1000 | Loss: 0.00002769
Iteration 156/1000 | Loss: 0.00002769
Iteration 157/1000 | Loss: 0.00002769
Iteration 158/1000 | Loss: 0.00002769
Iteration 159/1000 | Loss: 0.00002769
Iteration 160/1000 | Loss: 0.00002769
Iteration 161/1000 | Loss: 0.00002769
Iteration 162/1000 | Loss: 0.00002769
Iteration 163/1000 | Loss: 0.00002769
Iteration 164/1000 | Loss: 0.00002769
Iteration 165/1000 | Loss: 0.00002768
Iteration 166/1000 | Loss: 0.00002768
Iteration 167/1000 | Loss: 0.00002768
Iteration 168/1000 | Loss: 0.00002768
Iteration 169/1000 | Loss: 0.00002768
Iteration 170/1000 | Loss: 0.00002768
Iteration 171/1000 | Loss: 0.00002768
Iteration 172/1000 | Loss: 0.00002768
Iteration 173/1000 | Loss: 0.00002768
Iteration 174/1000 | Loss: 0.00002768
Iteration 175/1000 | Loss: 0.00002768
Iteration 176/1000 | Loss: 0.00002768
Iteration 177/1000 | Loss: 0.00002768
Iteration 178/1000 | Loss: 0.00002768
Iteration 179/1000 | Loss: 0.00002768
Iteration 180/1000 | Loss: 0.00002768
Iteration 181/1000 | Loss: 0.00002768
Iteration 182/1000 | Loss: 0.00002768
Iteration 183/1000 | Loss: 0.00002768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.768060949165374e-05, 2.768060949165374e-05, 2.768060949165374e-05, 2.768060949165374e-05, 2.768060949165374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.768060949165374e-05

Optimization complete. Final v2v error: 4.50177526473999 mm

Highest mean error: 6.869375228881836 mm for frame 16

Lowest mean error: 4.218548774719238 mm for frame 7

Saving results

Total time: 94.92268919944763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00336482
Iteration 2/25 | Loss: 0.00131473
Iteration 3/25 | Loss: 0.00121120
Iteration 4/25 | Loss: 0.00119016
Iteration 5/25 | Loss: 0.00118513
Iteration 6/25 | Loss: 0.00118428
Iteration 7/25 | Loss: 0.00118428
Iteration 8/25 | Loss: 0.00118428
Iteration 9/25 | Loss: 0.00118428
Iteration 10/25 | Loss: 0.00118428
Iteration 11/25 | Loss: 0.00118428
Iteration 12/25 | Loss: 0.00118428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011842810781672597, 0.0011842810781672597, 0.0011842810781672597, 0.0011842810781672597, 0.0011842810781672597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011842810781672597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42844737
Iteration 2/25 | Loss: 0.00069188
Iteration 3/25 | Loss: 0.00069187
Iteration 4/25 | Loss: 0.00069187
Iteration 5/25 | Loss: 0.00069187
Iteration 6/25 | Loss: 0.00069187
Iteration 7/25 | Loss: 0.00069187
Iteration 8/25 | Loss: 0.00069187
Iteration 9/25 | Loss: 0.00069187
Iteration 10/25 | Loss: 0.00069187
Iteration 11/25 | Loss: 0.00069187
Iteration 12/25 | Loss: 0.00069187
Iteration 13/25 | Loss: 0.00069187
Iteration 14/25 | Loss: 0.00069187
Iteration 15/25 | Loss: 0.00069187
Iteration 16/25 | Loss: 0.00069187
Iteration 17/25 | Loss: 0.00069187
Iteration 18/25 | Loss: 0.00069187
Iteration 19/25 | Loss: 0.00069187
Iteration 20/25 | Loss: 0.00069187
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006918666185811162, 0.0006918666185811162, 0.0006918666185811162, 0.0006918666185811162, 0.0006918666185811162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006918666185811162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069187
Iteration 2/1000 | Loss: 0.00003515
Iteration 3/1000 | Loss: 0.00002298
Iteration 4/1000 | Loss: 0.00002007
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001726
Iteration 7/1000 | Loss: 0.00001651
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001469
Iteration 19/1000 | Loss: 0.00001468
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001465
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001463
Iteration 27/1000 | Loss: 0.00001462
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001460
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001459
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001456
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001452
Iteration 46/1000 | Loss: 0.00001452
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001440
Iteration 59/1000 | Loss: 0.00001439
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001427
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001426
Iteration 112/1000 | Loss: 0.00001426
Iteration 113/1000 | Loss: 0.00001426
Iteration 114/1000 | Loss: 0.00001426
Iteration 115/1000 | Loss: 0.00001426
Iteration 116/1000 | Loss: 0.00001426
Iteration 117/1000 | Loss: 0.00001426
Iteration 118/1000 | Loss: 0.00001426
Iteration 119/1000 | Loss: 0.00001426
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001425
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001425
Iteration 126/1000 | Loss: 0.00001425
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.4237599316402338e-05, 1.4237599316402338e-05, 1.4237599316402338e-05, 1.4237599316402338e-05, 1.4237599316402338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4237599316402338e-05

Optimization complete. Final v2v error: 3.1948862075805664 mm

Highest mean error: 3.794341564178467 mm for frame 94

Lowest mean error: 2.8049123287200928 mm for frame 265

Saving results

Total time: 44.370604276657104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476981
Iteration 2/25 | Loss: 0.00142026
Iteration 3/25 | Loss: 0.00133630
Iteration 4/25 | Loss: 0.00132842
Iteration 5/25 | Loss: 0.00132577
Iteration 6/25 | Loss: 0.00132577
Iteration 7/25 | Loss: 0.00132577
Iteration 8/25 | Loss: 0.00132577
Iteration 9/25 | Loss: 0.00132577
Iteration 10/25 | Loss: 0.00132577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013257694663479924, 0.0013257694663479924, 0.0013257694663479924, 0.0013257694663479924, 0.0013257694663479924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013257694663479924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41514087
Iteration 2/25 | Loss: 0.00101982
Iteration 3/25 | Loss: 0.00101979
Iteration 4/25 | Loss: 0.00101979
Iteration 5/25 | Loss: 0.00101979
Iteration 6/25 | Loss: 0.00101979
Iteration 7/25 | Loss: 0.00101979
Iteration 8/25 | Loss: 0.00101979
Iteration 9/25 | Loss: 0.00101979
Iteration 10/25 | Loss: 0.00101979
Iteration 11/25 | Loss: 0.00101979
Iteration 12/25 | Loss: 0.00101979
Iteration 13/25 | Loss: 0.00101979
Iteration 14/25 | Loss: 0.00101979
Iteration 15/25 | Loss: 0.00101979
Iteration 16/25 | Loss: 0.00101979
Iteration 17/25 | Loss: 0.00101979
Iteration 18/25 | Loss: 0.00101979
Iteration 19/25 | Loss: 0.00101979
Iteration 20/25 | Loss: 0.00101979
Iteration 21/25 | Loss: 0.00101979
Iteration 22/25 | Loss: 0.00101979
Iteration 23/25 | Loss: 0.00101979
Iteration 24/25 | Loss: 0.00101979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010197908850386739, 0.0010197908850386739, 0.0010197908850386739, 0.0010197908850386739, 0.0010197908850386739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010197908850386739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101979
Iteration 2/1000 | Loss: 0.00004194
Iteration 3/1000 | Loss: 0.00002794
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002238
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001931
Iteration 11/1000 | Loss: 0.00001916
Iteration 12/1000 | Loss: 0.00001896
Iteration 13/1000 | Loss: 0.00001883
Iteration 14/1000 | Loss: 0.00001877
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001858
Iteration 18/1000 | Loss: 0.00001857
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001853
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001841
Iteration 40/1000 | Loss: 0.00001841
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001839
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001837
Iteration 47/1000 | Loss: 0.00001836
Iteration 48/1000 | Loss: 0.00001836
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001832
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001829
Iteration 60/1000 | Loss: 0.00001828
Iteration 61/1000 | Loss: 0.00001828
Iteration 62/1000 | Loss: 0.00001828
Iteration 63/1000 | Loss: 0.00001828
Iteration 64/1000 | Loss: 0.00001827
Iteration 65/1000 | Loss: 0.00001826
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001825
Iteration 69/1000 | Loss: 0.00001825
Iteration 70/1000 | Loss: 0.00001825
Iteration 71/1000 | Loss: 0.00001825
Iteration 72/1000 | Loss: 0.00001824
Iteration 73/1000 | Loss: 0.00001824
Iteration 74/1000 | Loss: 0.00001824
Iteration 75/1000 | Loss: 0.00001824
Iteration 76/1000 | Loss: 0.00001823
Iteration 77/1000 | Loss: 0.00001823
Iteration 78/1000 | Loss: 0.00001823
Iteration 79/1000 | Loss: 0.00001822
Iteration 80/1000 | Loss: 0.00001822
Iteration 81/1000 | Loss: 0.00001821
Iteration 82/1000 | Loss: 0.00001821
Iteration 83/1000 | Loss: 0.00001821
Iteration 84/1000 | Loss: 0.00001818
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001814
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001813
Iteration 92/1000 | Loss: 0.00001813
Iteration 93/1000 | Loss: 0.00001813
Iteration 94/1000 | Loss: 0.00001812
Iteration 95/1000 | Loss: 0.00001812
Iteration 96/1000 | Loss: 0.00001811
Iteration 97/1000 | Loss: 0.00001811
Iteration 98/1000 | Loss: 0.00001811
Iteration 99/1000 | Loss: 0.00001811
Iteration 100/1000 | Loss: 0.00001810
Iteration 101/1000 | Loss: 0.00001810
Iteration 102/1000 | Loss: 0.00001809
Iteration 103/1000 | Loss: 0.00001809
Iteration 104/1000 | Loss: 0.00001809
Iteration 105/1000 | Loss: 0.00001809
Iteration 106/1000 | Loss: 0.00001808
Iteration 107/1000 | Loss: 0.00001808
Iteration 108/1000 | Loss: 0.00001808
Iteration 109/1000 | Loss: 0.00001808
Iteration 110/1000 | Loss: 0.00001808
Iteration 111/1000 | Loss: 0.00001808
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Iteration 114/1000 | Loss: 0.00001806
Iteration 115/1000 | Loss: 0.00001806
Iteration 116/1000 | Loss: 0.00001806
Iteration 117/1000 | Loss: 0.00001806
Iteration 118/1000 | Loss: 0.00001806
Iteration 119/1000 | Loss: 0.00001805
Iteration 120/1000 | Loss: 0.00001805
Iteration 121/1000 | Loss: 0.00001805
Iteration 122/1000 | Loss: 0.00001804
Iteration 123/1000 | Loss: 0.00001804
Iteration 124/1000 | Loss: 0.00001804
Iteration 125/1000 | Loss: 0.00001803
Iteration 126/1000 | Loss: 0.00001803
Iteration 127/1000 | Loss: 0.00001803
Iteration 128/1000 | Loss: 0.00001803
Iteration 129/1000 | Loss: 0.00001802
Iteration 130/1000 | Loss: 0.00001802
Iteration 131/1000 | Loss: 0.00001802
Iteration 132/1000 | Loss: 0.00001802
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001801
Iteration 136/1000 | Loss: 0.00001800
Iteration 137/1000 | Loss: 0.00001800
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001800
Iteration 142/1000 | Loss: 0.00001800
Iteration 143/1000 | Loss: 0.00001800
Iteration 144/1000 | Loss: 0.00001800
Iteration 145/1000 | Loss: 0.00001800
Iteration 146/1000 | Loss: 0.00001800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.7996107999351807e-05, 1.7996107999351807e-05, 1.7996107999351807e-05, 1.7996107999351807e-05, 1.7996107999351807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7996107999351807e-05

Optimization complete. Final v2v error: 3.5542845726013184 mm

Highest mean error: 3.927370071411133 mm for frame 110

Lowest mean error: 3.173845052719116 mm for frame 208

Saving results

Total time: 48.283480167388916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979093
Iteration 2/25 | Loss: 0.00187825
Iteration 3/25 | Loss: 0.00152949
Iteration 4/25 | Loss: 0.00145603
Iteration 5/25 | Loss: 0.00142526
Iteration 6/25 | Loss: 0.00139329
Iteration 7/25 | Loss: 0.00136861
Iteration 8/25 | Loss: 0.00134949
Iteration 9/25 | Loss: 0.00132700
Iteration 10/25 | Loss: 0.00131364
Iteration 11/25 | Loss: 0.00130518
Iteration 12/25 | Loss: 0.00130302
Iteration 13/25 | Loss: 0.00130065
Iteration 14/25 | Loss: 0.00129836
Iteration 15/25 | Loss: 0.00129763
Iteration 16/25 | Loss: 0.00129726
Iteration 17/25 | Loss: 0.00129706
Iteration 18/25 | Loss: 0.00129690
Iteration 19/25 | Loss: 0.00129672
Iteration 20/25 | Loss: 0.00129658
Iteration 21/25 | Loss: 0.00129652
Iteration 22/25 | Loss: 0.00129652
Iteration 23/25 | Loss: 0.00129652
Iteration 24/25 | Loss: 0.00129652
Iteration 25/25 | Loss: 0.00129652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42596817
Iteration 2/25 | Loss: 0.00085559
Iteration 3/25 | Loss: 0.00085557
Iteration 4/25 | Loss: 0.00085557
Iteration 5/25 | Loss: 0.00085557
Iteration 6/25 | Loss: 0.00077711
Iteration 7/25 | Loss: 0.00077700
Iteration 8/25 | Loss: 0.00077700
Iteration 9/25 | Loss: 0.00077700
Iteration 10/25 | Loss: 0.00077700
Iteration 11/25 | Loss: 0.00077700
Iteration 12/25 | Loss: 0.00077700
Iteration 13/25 | Loss: 0.00077700
Iteration 14/25 | Loss: 0.00077700
Iteration 15/25 | Loss: 0.00077700
Iteration 16/25 | Loss: 0.00077700
Iteration 17/25 | Loss: 0.00077700
Iteration 18/25 | Loss: 0.00077700
Iteration 19/25 | Loss: 0.00077700
Iteration 20/25 | Loss: 0.00077700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007770011434331536, 0.0007770011434331536, 0.0007770011434331536, 0.0007770011434331536, 0.0007770011434331536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007770011434331536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077700
Iteration 2/1000 | Loss: 0.00018996
Iteration 3/1000 | Loss: 0.00015841
Iteration 4/1000 | Loss: 0.00031207
Iteration 5/1000 | Loss: 0.00003966
Iteration 6/1000 | Loss: 0.00009736
Iteration 7/1000 | Loss: 0.00003421
Iteration 8/1000 | Loss: 0.00003276
Iteration 9/1000 | Loss: 0.00003156
Iteration 10/1000 | Loss: 0.00013443
Iteration 11/1000 | Loss: 0.00002965
Iteration 12/1000 | Loss: 0.00029008
Iteration 13/1000 | Loss: 0.00003252
Iteration 14/1000 | Loss: 0.00008092
Iteration 15/1000 | Loss: 0.00002845
Iteration 16/1000 | Loss: 0.00005996
Iteration 17/1000 | Loss: 0.00002703
Iteration 18/1000 | Loss: 0.00008386
Iteration 19/1000 | Loss: 0.00002644
Iteration 20/1000 | Loss: 0.00002621
Iteration 21/1000 | Loss: 0.00002610
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00007569
Iteration 24/1000 | Loss: 0.00017802
Iteration 25/1000 | Loss: 0.00002745
Iteration 26/1000 | Loss: 0.00002578
Iteration 27/1000 | Loss: 0.00002573
Iteration 28/1000 | Loss: 0.00002559
Iteration 29/1000 | Loss: 0.00002553
Iteration 30/1000 | Loss: 0.00002548
Iteration 31/1000 | Loss: 0.00002547
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002545
Iteration 34/1000 | Loss: 0.00002544
Iteration 35/1000 | Loss: 0.00002544
Iteration 36/1000 | Loss: 0.00002544
Iteration 37/1000 | Loss: 0.00011979
Iteration 38/1000 | Loss: 0.00008240
Iteration 39/1000 | Loss: 0.00021984
Iteration 40/1000 | Loss: 0.00002918
Iteration 41/1000 | Loss: 0.00002649
Iteration 42/1000 | Loss: 0.00002565
Iteration 43/1000 | Loss: 0.00002541
Iteration 44/1000 | Loss: 0.00002539
Iteration 45/1000 | Loss: 0.00002536
Iteration 46/1000 | Loss: 0.00002536
Iteration 47/1000 | Loss: 0.00002535
Iteration 48/1000 | Loss: 0.00002534
Iteration 49/1000 | Loss: 0.00002534
Iteration 50/1000 | Loss: 0.00002533
Iteration 51/1000 | Loss: 0.00002533
Iteration 52/1000 | Loss: 0.00002533
Iteration 53/1000 | Loss: 0.00002532
Iteration 54/1000 | Loss: 0.00002532
Iteration 55/1000 | Loss: 0.00002532
Iteration 56/1000 | Loss: 0.00002532
Iteration 57/1000 | Loss: 0.00002531
Iteration 58/1000 | Loss: 0.00002531
Iteration 59/1000 | Loss: 0.00002531
Iteration 60/1000 | Loss: 0.00002530
Iteration 61/1000 | Loss: 0.00002530
Iteration 62/1000 | Loss: 0.00002530
Iteration 63/1000 | Loss: 0.00002529
Iteration 64/1000 | Loss: 0.00002529
Iteration 65/1000 | Loss: 0.00002529
Iteration 66/1000 | Loss: 0.00002529
Iteration 67/1000 | Loss: 0.00002529
Iteration 68/1000 | Loss: 0.00002529
Iteration 69/1000 | Loss: 0.00002529
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002528
Iteration 73/1000 | Loss: 0.00002528
Iteration 74/1000 | Loss: 0.00002528
Iteration 75/1000 | Loss: 0.00002528
Iteration 76/1000 | Loss: 0.00002528
Iteration 77/1000 | Loss: 0.00002528
Iteration 78/1000 | Loss: 0.00002528
Iteration 79/1000 | Loss: 0.00002527
Iteration 80/1000 | Loss: 0.00002527
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002526
Iteration 84/1000 | Loss: 0.00002526
Iteration 85/1000 | Loss: 0.00002526
Iteration 86/1000 | Loss: 0.00002526
Iteration 87/1000 | Loss: 0.00002526
Iteration 88/1000 | Loss: 0.00002526
Iteration 89/1000 | Loss: 0.00002525
Iteration 90/1000 | Loss: 0.00002525
Iteration 91/1000 | Loss: 0.00002525
Iteration 92/1000 | Loss: 0.00002525
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002524
Iteration 95/1000 | Loss: 0.00002524
Iteration 96/1000 | Loss: 0.00006617
Iteration 97/1000 | Loss: 0.00010466
Iteration 98/1000 | Loss: 0.00003916
Iteration 99/1000 | Loss: 0.00016535
Iteration 100/1000 | Loss: 0.00044548
Iteration 101/1000 | Loss: 0.00025044
Iteration 102/1000 | Loss: 0.00058043
Iteration 103/1000 | Loss: 0.00165523
Iteration 104/1000 | Loss: 0.00009099
Iteration 105/1000 | Loss: 0.00003414
Iteration 106/1000 | Loss: 0.00031989
Iteration 107/1000 | Loss: 0.00004130
Iteration 108/1000 | Loss: 0.00002659
Iteration 109/1000 | Loss: 0.00003645
Iteration 110/1000 | Loss: 0.00002700
Iteration 111/1000 | Loss: 0.00002541
Iteration 112/1000 | Loss: 0.00002539
Iteration 113/1000 | Loss: 0.00002531
Iteration 114/1000 | Loss: 0.00008496
Iteration 115/1000 | Loss: 0.00002533
Iteration 116/1000 | Loss: 0.00002518
Iteration 117/1000 | Loss: 0.00002517
Iteration 118/1000 | Loss: 0.00002517
Iteration 119/1000 | Loss: 0.00002516
Iteration 120/1000 | Loss: 0.00002515
Iteration 121/1000 | Loss: 0.00002515
Iteration 122/1000 | Loss: 0.00002515
Iteration 123/1000 | Loss: 0.00002515
Iteration 124/1000 | Loss: 0.00002515
Iteration 125/1000 | Loss: 0.00002515
Iteration 126/1000 | Loss: 0.00002515
Iteration 127/1000 | Loss: 0.00002515
Iteration 128/1000 | Loss: 0.00002515
Iteration 129/1000 | Loss: 0.00002515
Iteration 130/1000 | Loss: 0.00002514
Iteration 131/1000 | Loss: 0.00002514
Iteration 132/1000 | Loss: 0.00002514
Iteration 133/1000 | Loss: 0.00002514
Iteration 134/1000 | Loss: 0.00002514
Iteration 135/1000 | Loss: 0.00002514
Iteration 136/1000 | Loss: 0.00002514
Iteration 137/1000 | Loss: 0.00002514
Iteration 138/1000 | Loss: 0.00002514
Iteration 139/1000 | Loss: 0.00002513
Iteration 140/1000 | Loss: 0.00002513
Iteration 141/1000 | Loss: 0.00002513
Iteration 142/1000 | Loss: 0.00002513
Iteration 143/1000 | Loss: 0.00002513
Iteration 144/1000 | Loss: 0.00002513
Iteration 145/1000 | Loss: 0.00002513
Iteration 146/1000 | Loss: 0.00002513
Iteration 147/1000 | Loss: 0.00002513
Iteration 148/1000 | Loss: 0.00002513
Iteration 149/1000 | Loss: 0.00002513
Iteration 150/1000 | Loss: 0.00002513
Iteration 151/1000 | Loss: 0.00002513
Iteration 152/1000 | Loss: 0.00002513
Iteration 153/1000 | Loss: 0.00002513
Iteration 154/1000 | Loss: 0.00002513
Iteration 155/1000 | Loss: 0.00002513
Iteration 156/1000 | Loss: 0.00002513
Iteration 157/1000 | Loss: 0.00002513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.5126064429059625e-05, 2.5126064429059625e-05, 2.5126064429059625e-05, 2.5126064429059625e-05, 2.5126064429059625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5126064429059625e-05

Optimization complete. Final v2v error: 4.216220855712891 mm

Highest mean error: 5.1967692375183105 mm for frame 90

Lowest mean error: 3.636767625808716 mm for frame 229

Saving results

Total time: 132.25112748146057
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050763
Iteration 2/25 | Loss: 0.01050763
Iteration 3/25 | Loss: 0.01050763
Iteration 4/25 | Loss: 0.01050763
Iteration 5/25 | Loss: 0.01050763
Iteration 6/25 | Loss: 0.01050763
Iteration 7/25 | Loss: 0.01050763
Iteration 8/25 | Loss: 0.01050763
Iteration 9/25 | Loss: 0.01050763
Iteration 10/25 | Loss: 0.01050763
Iteration 11/25 | Loss: 0.01050763
Iteration 12/25 | Loss: 0.01050762
Iteration 13/25 | Loss: 0.01050762
Iteration 14/25 | Loss: 0.01050762
Iteration 15/25 | Loss: 0.01050762
Iteration 16/25 | Loss: 0.01050762
Iteration 17/25 | Loss: 0.01050762
Iteration 18/25 | Loss: 0.01050762
Iteration 19/25 | Loss: 0.01050762
Iteration 20/25 | Loss: 0.01050762
Iteration 21/25 | Loss: 0.01050762
Iteration 22/25 | Loss: 0.01050762
Iteration 23/25 | Loss: 0.01050762
Iteration 24/25 | Loss: 0.01050762
Iteration 25/25 | Loss: 0.01050762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59487975
Iteration 2/25 | Loss: 0.11427489
Iteration 3/25 | Loss: 0.09408428
Iteration 4/25 | Loss: 0.09057535
Iteration 5/25 | Loss: 0.09035051
Iteration 6/25 | Loss: 0.09035049
Iteration 7/25 | Loss: 0.09035047
Iteration 8/25 | Loss: 0.09035047
Iteration 9/25 | Loss: 0.09035046
Iteration 10/25 | Loss: 0.09035046
Iteration 11/25 | Loss: 0.09035046
Iteration 12/25 | Loss: 0.09035046
Iteration 13/25 | Loss: 0.09035046
Iteration 14/25 | Loss: 0.09035046
Iteration 15/25 | Loss: 0.09035046
Iteration 16/25 | Loss: 0.09035046
Iteration 17/25 | Loss: 0.09035046
Iteration 18/25 | Loss: 0.09035046
Iteration 19/25 | Loss: 0.09035046
Iteration 20/25 | Loss: 0.09035046
Iteration 21/25 | Loss: 0.09035046
Iteration 22/25 | Loss: 0.09035046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.09035045653581619, 0.09035045653581619, 0.09035045653581619, 0.09035045653581619, 0.09035045653581619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09035045653581619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09035046
Iteration 2/1000 | Loss: 0.00170616
Iteration 3/1000 | Loss: 0.00315453
Iteration 4/1000 | Loss: 0.00034289
Iteration 5/1000 | Loss: 0.00176425
Iteration 6/1000 | Loss: 0.00008641
Iteration 7/1000 | Loss: 0.00045849
Iteration 8/1000 | Loss: 0.00019209
Iteration 9/1000 | Loss: 0.00004194
Iteration 10/1000 | Loss: 0.00012422
Iteration 11/1000 | Loss: 0.00013224
Iteration 12/1000 | Loss: 0.00204052
Iteration 13/1000 | Loss: 0.00129331
Iteration 14/1000 | Loss: 0.00034865
Iteration 15/1000 | Loss: 0.00081643
Iteration 16/1000 | Loss: 0.00003677
Iteration 17/1000 | Loss: 0.00013721
Iteration 18/1000 | Loss: 0.00022720
Iteration 19/1000 | Loss: 0.00005068
Iteration 20/1000 | Loss: 0.00004322
Iteration 21/1000 | Loss: 0.00027575
Iteration 22/1000 | Loss: 0.00010862
Iteration 23/1000 | Loss: 0.00033935
Iteration 24/1000 | Loss: 0.00027667
Iteration 25/1000 | Loss: 0.00024520
Iteration 26/1000 | Loss: 0.00007301
Iteration 27/1000 | Loss: 0.00005241
Iteration 28/1000 | Loss: 0.00002143
Iteration 29/1000 | Loss: 0.00002034
Iteration 30/1000 | Loss: 0.00001960
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001793
Iteration 33/1000 | Loss: 0.00029685
Iteration 34/1000 | Loss: 0.00002882
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00002691
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00016048
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00018237
Iteration 42/1000 | Loss: 0.00013664
Iteration 43/1000 | Loss: 0.00009081
Iteration 44/1000 | Loss: 0.00005809
Iteration 45/1000 | Loss: 0.00013007
Iteration 46/1000 | Loss: 0.00023434
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001456
Iteration 53/1000 | Loss: 0.00012178
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001440
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001436
Iteration 58/1000 | Loss: 0.00001435
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001413
Iteration 64/1000 | Loss: 0.00001413
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001396
Iteration 73/1000 | Loss: 0.00001396
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001392
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001390
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001388
Iteration 84/1000 | Loss: 0.00001388
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001384
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001378
Iteration 107/1000 | Loss: 0.00001378
Iteration 108/1000 | Loss: 0.00001378
Iteration 109/1000 | Loss: 0.00001377
Iteration 110/1000 | Loss: 0.00001377
Iteration 111/1000 | Loss: 0.00001377
Iteration 112/1000 | Loss: 0.00001377
Iteration 113/1000 | Loss: 0.00001377
Iteration 114/1000 | Loss: 0.00001377
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001376
Iteration 118/1000 | Loss: 0.00001376
Iteration 119/1000 | Loss: 0.00001376
Iteration 120/1000 | Loss: 0.00001376
Iteration 121/1000 | Loss: 0.00001376
Iteration 122/1000 | Loss: 0.00001376
Iteration 123/1000 | Loss: 0.00001376
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001375
Iteration 129/1000 | Loss: 0.00001375
Iteration 130/1000 | Loss: 0.00001374
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001374
Iteration 133/1000 | Loss: 0.00001374
Iteration 134/1000 | Loss: 0.00001374
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001371
Iteration 142/1000 | Loss: 0.00001371
Iteration 143/1000 | Loss: 0.00001371
Iteration 144/1000 | Loss: 0.00001371
Iteration 145/1000 | Loss: 0.00001371
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001370
Iteration 148/1000 | Loss: 0.00001370
Iteration 149/1000 | Loss: 0.00001370
Iteration 150/1000 | Loss: 0.00001370
Iteration 151/1000 | Loss: 0.00001370
Iteration 152/1000 | Loss: 0.00001370
Iteration 153/1000 | Loss: 0.00001370
Iteration 154/1000 | Loss: 0.00001370
Iteration 155/1000 | Loss: 0.00001370
Iteration 156/1000 | Loss: 0.00001369
Iteration 157/1000 | Loss: 0.00001369
Iteration 158/1000 | Loss: 0.00001369
Iteration 159/1000 | Loss: 0.00001369
Iteration 160/1000 | Loss: 0.00001369
Iteration 161/1000 | Loss: 0.00001369
Iteration 162/1000 | Loss: 0.00001369
Iteration 163/1000 | Loss: 0.00001368
Iteration 164/1000 | Loss: 0.00001368
Iteration 165/1000 | Loss: 0.00001368
Iteration 166/1000 | Loss: 0.00001368
Iteration 167/1000 | Loss: 0.00001367
Iteration 168/1000 | Loss: 0.00001367
Iteration 169/1000 | Loss: 0.00001367
Iteration 170/1000 | Loss: 0.00001367
Iteration 171/1000 | Loss: 0.00001367
Iteration 172/1000 | Loss: 0.00001367
Iteration 173/1000 | Loss: 0.00001367
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001366
Iteration 176/1000 | Loss: 0.00001366
Iteration 177/1000 | Loss: 0.00001366
Iteration 178/1000 | Loss: 0.00001366
Iteration 179/1000 | Loss: 0.00001365
Iteration 180/1000 | Loss: 0.00001365
Iteration 181/1000 | Loss: 0.00001365
Iteration 182/1000 | Loss: 0.00001364
Iteration 183/1000 | Loss: 0.00001364
Iteration 184/1000 | Loss: 0.00001364
Iteration 185/1000 | Loss: 0.00001363
Iteration 186/1000 | Loss: 0.00001363
Iteration 187/1000 | Loss: 0.00001363
Iteration 188/1000 | Loss: 0.00001363
Iteration 189/1000 | Loss: 0.00001363
Iteration 190/1000 | Loss: 0.00001362
Iteration 191/1000 | Loss: 0.00001362
Iteration 192/1000 | Loss: 0.00001362
Iteration 193/1000 | Loss: 0.00001362
Iteration 194/1000 | Loss: 0.00001362
Iteration 195/1000 | Loss: 0.00001362
Iteration 196/1000 | Loss: 0.00001362
Iteration 197/1000 | Loss: 0.00001362
Iteration 198/1000 | Loss: 0.00001362
Iteration 199/1000 | Loss: 0.00001362
Iteration 200/1000 | Loss: 0.00001362
Iteration 201/1000 | Loss: 0.00001362
Iteration 202/1000 | Loss: 0.00001362
Iteration 203/1000 | Loss: 0.00001362
Iteration 204/1000 | Loss: 0.00001362
Iteration 205/1000 | Loss: 0.00001362
Iteration 206/1000 | Loss: 0.00001362
Iteration 207/1000 | Loss: 0.00001362
Iteration 208/1000 | Loss: 0.00001362
Iteration 209/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.3619935998576693e-05, 1.3619935998576693e-05, 1.3619935998576693e-05, 1.3619935998576693e-05, 1.3619935998576693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3619935998576693e-05

Optimization complete. Final v2v error: 3.1267459392547607 mm

Highest mean error: 3.883432149887085 mm for frame 28

Lowest mean error: 2.836289882659912 mm for frame 234

Saving results

Total time: 114.82902479171753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408510
Iteration 2/25 | Loss: 0.00135493
Iteration 3/25 | Loss: 0.00124261
Iteration 4/25 | Loss: 0.00122417
Iteration 5/25 | Loss: 0.00121649
Iteration 6/25 | Loss: 0.00121405
Iteration 7/25 | Loss: 0.00121347
Iteration 8/25 | Loss: 0.00121347
Iteration 9/25 | Loss: 0.00121347
Iteration 10/25 | Loss: 0.00121347
Iteration 11/25 | Loss: 0.00121347
Iteration 12/25 | Loss: 0.00121347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012134722201153636, 0.0012134722201153636, 0.0012134722201153636, 0.0012134722201153636, 0.0012134722201153636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012134722201153636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40550268
Iteration 2/25 | Loss: 0.00087227
Iteration 3/25 | Loss: 0.00087227
Iteration 4/25 | Loss: 0.00087227
Iteration 5/25 | Loss: 0.00087227
Iteration 6/25 | Loss: 0.00087227
Iteration 7/25 | Loss: 0.00087227
Iteration 8/25 | Loss: 0.00087227
Iteration 9/25 | Loss: 0.00087227
Iteration 10/25 | Loss: 0.00087227
Iteration 11/25 | Loss: 0.00087227
Iteration 12/25 | Loss: 0.00087227
Iteration 13/25 | Loss: 0.00087227
Iteration 14/25 | Loss: 0.00087227
Iteration 15/25 | Loss: 0.00087227
Iteration 16/25 | Loss: 0.00087227
Iteration 17/25 | Loss: 0.00087227
Iteration 18/25 | Loss: 0.00087227
Iteration 19/25 | Loss: 0.00087227
Iteration 20/25 | Loss: 0.00087227
Iteration 21/25 | Loss: 0.00087227
Iteration 22/25 | Loss: 0.00087227
Iteration 23/25 | Loss: 0.00087227
Iteration 24/25 | Loss: 0.00087227
Iteration 25/25 | Loss: 0.00087227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087227
Iteration 2/1000 | Loss: 0.00007538
Iteration 3/1000 | Loss: 0.00004077
Iteration 4/1000 | Loss: 0.00003197
Iteration 5/1000 | Loss: 0.00002728
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002296
Iteration 8/1000 | Loss: 0.00002183
Iteration 9/1000 | Loss: 0.00002097
Iteration 10/1000 | Loss: 0.00002041
Iteration 11/1000 | Loss: 0.00002009
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001955
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001939
Iteration 16/1000 | Loss: 0.00001937
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001905
Iteration 22/1000 | Loss: 0.00001904
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001898
Iteration 25/1000 | Loss: 0.00001897
Iteration 26/1000 | Loss: 0.00001897
Iteration 27/1000 | Loss: 0.00001896
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001895
Iteration 31/1000 | Loss: 0.00001895
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001894
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001892
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001891
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001889
Iteration 52/1000 | Loss: 0.00001889
Iteration 53/1000 | Loss: 0.00001889
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001888
Iteration 57/1000 | Loss: 0.00001888
Iteration 58/1000 | Loss: 0.00001887
Iteration 59/1000 | Loss: 0.00001887
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001886
Iteration 63/1000 | Loss: 0.00001886
Iteration 64/1000 | Loss: 0.00001886
Iteration 65/1000 | Loss: 0.00001886
Iteration 66/1000 | Loss: 0.00001886
Iteration 67/1000 | Loss: 0.00001886
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001885
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001885
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001884
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001882
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001880
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001879
Iteration 99/1000 | Loss: 0.00001879
Iteration 100/1000 | Loss: 0.00001879
Iteration 101/1000 | Loss: 0.00001879
Iteration 102/1000 | Loss: 0.00001879
Iteration 103/1000 | Loss: 0.00001879
Iteration 104/1000 | Loss: 0.00001879
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001878
Iteration 107/1000 | Loss: 0.00001878
Iteration 108/1000 | Loss: 0.00001878
Iteration 109/1000 | Loss: 0.00001878
Iteration 110/1000 | Loss: 0.00001878
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001878
Iteration 113/1000 | Loss: 0.00001877
Iteration 114/1000 | Loss: 0.00001877
Iteration 115/1000 | Loss: 0.00001877
Iteration 116/1000 | Loss: 0.00001877
Iteration 117/1000 | Loss: 0.00001876
Iteration 118/1000 | Loss: 0.00001876
Iteration 119/1000 | Loss: 0.00001876
Iteration 120/1000 | Loss: 0.00001876
Iteration 121/1000 | Loss: 0.00001876
Iteration 122/1000 | Loss: 0.00001875
Iteration 123/1000 | Loss: 0.00001875
Iteration 124/1000 | Loss: 0.00001875
Iteration 125/1000 | Loss: 0.00001875
Iteration 126/1000 | Loss: 0.00001875
Iteration 127/1000 | Loss: 0.00001875
Iteration 128/1000 | Loss: 0.00001875
Iteration 129/1000 | Loss: 0.00001874
Iteration 130/1000 | Loss: 0.00001874
Iteration 131/1000 | Loss: 0.00001874
Iteration 132/1000 | Loss: 0.00001874
Iteration 133/1000 | Loss: 0.00001874
Iteration 134/1000 | Loss: 0.00001874
Iteration 135/1000 | Loss: 0.00001874
Iteration 136/1000 | Loss: 0.00001874
Iteration 137/1000 | Loss: 0.00001874
Iteration 138/1000 | Loss: 0.00001873
Iteration 139/1000 | Loss: 0.00001873
Iteration 140/1000 | Loss: 0.00001873
Iteration 141/1000 | Loss: 0.00001873
Iteration 142/1000 | Loss: 0.00001873
Iteration 143/1000 | Loss: 0.00001873
Iteration 144/1000 | Loss: 0.00001872
Iteration 145/1000 | Loss: 0.00001872
Iteration 146/1000 | Loss: 0.00001872
Iteration 147/1000 | Loss: 0.00001872
Iteration 148/1000 | Loss: 0.00001871
Iteration 149/1000 | Loss: 0.00001871
Iteration 150/1000 | Loss: 0.00001871
Iteration 151/1000 | Loss: 0.00001871
Iteration 152/1000 | Loss: 0.00001871
Iteration 153/1000 | Loss: 0.00001871
Iteration 154/1000 | Loss: 0.00001871
Iteration 155/1000 | Loss: 0.00001871
Iteration 156/1000 | Loss: 0.00001871
Iteration 157/1000 | Loss: 0.00001870
Iteration 158/1000 | Loss: 0.00001870
Iteration 159/1000 | Loss: 0.00001870
Iteration 160/1000 | Loss: 0.00001870
Iteration 161/1000 | Loss: 0.00001870
Iteration 162/1000 | Loss: 0.00001870
Iteration 163/1000 | Loss: 0.00001870
Iteration 164/1000 | Loss: 0.00001870
Iteration 165/1000 | Loss: 0.00001869
Iteration 166/1000 | Loss: 0.00001869
Iteration 167/1000 | Loss: 0.00001869
Iteration 168/1000 | Loss: 0.00001869
Iteration 169/1000 | Loss: 0.00001869
Iteration 170/1000 | Loss: 0.00001869
Iteration 171/1000 | Loss: 0.00001869
Iteration 172/1000 | Loss: 0.00001869
Iteration 173/1000 | Loss: 0.00001869
Iteration 174/1000 | Loss: 0.00001869
Iteration 175/1000 | Loss: 0.00001869
Iteration 176/1000 | Loss: 0.00001869
Iteration 177/1000 | Loss: 0.00001869
Iteration 178/1000 | Loss: 0.00001868
Iteration 179/1000 | Loss: 0.00001868
Iteration 180/1000 | Loss: 0.00001868
Iteration 181/1000 | Loss: 0.00001868
Iteration 182/1000 | Loss: 0.00001868
Iteration 183/1000 | Loss: 0.00001868
Iteration 184/1000 | Loss: 0.00001868
Iteration 185/1000 | Loss: 0.00001868
Iteration 186/1000 | Loss: 0.00001868
Iteration 187/1000 | Loss: 0.00001868
Iteration 188/1000 | Loss: 0.00001868
Iteration 189/1000 | Loss: 0.00001868
Iteration 190/1000 | Loss: 0.00001868
Iteration 191/1000 | Loss: 0.00001868
Iteration 192/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.8679411368793808e-05, 1.8679411368793808e-05, 1.8679411368793808e-05, 1.8679411368793808e-05, 1.8679411368793808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8679411368793808e-05

Optimization complete. Final v2v error: 3.594398260116577 mm

Highest mean error: 4.61443567276001 mm for frame 97

Lowest mean error: 2.691412925720215 mm for frame 105

Saving results

Total time: 45.50990867614746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_037/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_037/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441055
Iteration 2/25 | Loss: 0.00137715
Iteration 3/25 | Loss: 0.00127034
Iteration 4/25 | Loss: 0.00126281
Iteration 5/25 | Loss: 0.00126138
Iteration 6/25 | Loss: 0.00126138
Iteration 7/25 | Loss: 0.00126138
Iteration 8/25 | Loss: 0.00126138
Iteration 9/25 | Loss: 0.00126138
Iteration 10/25 | Loss: 0.00126138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001261377357877791, 0.001261377357877791, 0.001261377357877791, 0.001261377357877791, 0.001261377357877791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001261377357877791

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88858581
Iteration 2/25 | Loss: 0.00071310
Iteration 3/25 | Loss: 0.00071308
Iteration 4/25 | Loss: 0.00071308
Iteration 5/25 | Loss: 0.00071308
Iteration 6/25 | Loss: 0.00071308
Iteration 7/25 | Loss: 0.00071308
Iteration 8/25 | Loss: 0.00071308
Iteration 9/25 | Loss: 0.00071308
Iteration 10/25 | Loss: 0.00071308
Iteration 11/25 | Loss: 0.00071308
Iteration 12/25 | Loss: 0.00071308
Iteration 13/25 | Loss: 0.00071308
Iteration 14/25 | Loss: 0.00071308
Iteration 15/25 | Loss: 0.00071308
Iteration 16/25 | Loss: 0.00071308
Iteration 17/25 | Loss: 0.00071308
Iteration 18/25 | Loss: 0.00071308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007130802259780467, 0.0007130802259780467, 0.0007130802259780467, 0.0007130802259780467, 0.0007130802259780467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007130802259780467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071308
Iteration 2/1000 | Loss: 0.00003070
Iteration 3/1000 | Loss: 0.00002079
Iteration 4/1000 | Loss: 0.00001889
Iteration 5/1000 | Loss: 0.00001805
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001671
Iteration 8/1000 | Loss: 0.00001638
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001592
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001592
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001592
Iteration 16/1000 | Loss: 0.00001592
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001592
Iteration 20/1000 | Loss: 0.00001592
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001579
Iteration 25/1000 | Loss: 0.00001578
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001563
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001563
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001562
Iteration 40/1000 | Loss: 0.00001562
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001559
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001555
Iteration 62/1000 | Loss: 0.00001555
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001554
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001550
Iteration 82/1000 | Loss: 0.00001550
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001542
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001541
Iteration 123/1000 | Loss: 0.00001541
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001540
Iteration 126/1000 | Loss: 0.00001540
Iteration 127/1000 | Loss: 0.00001540
Iteration 128/1000 | Loss: 0.00001540
Iteration 129/1000 | Loss: 0.00001540
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Iteration 137/1000 | Loss: 0.00001539
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001538
Iteration 143/1000 | Loss: 0.00001538
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001536
Iteration 147/1000 | Loss: 0.00001536
Iteration 148/1000 | Loss: 0.00001536
Iteration 149/1000 | Loss: 0.00001536
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001536
Iteration 156/1000 | Loss: 0.00001536
Iteration 157/1000 | Loss: 0.00001536
Iteration 158/1000 | Loss: 0.00001536
Iteration 159/1000 | Loss: 0.00001535
Iteration 160/1000 | Loss: 0.00001535
Iteration 161/1000 | Loss: 0.00001535
Iteration 162/1000 | Loss: 0.00001535
Iteration 163/1000 | Loss: 0.00001535
Iteration 164/1000 | Loss: 0.00001535
Iteration 165/1000 | Loss: 0.00001535
Iteration 166/1000 | Loss: 0.00001535
Iteration 167/1000 | Loss: 0.00001535
Iteration 168/1000 | Loss: 0.00001535
Iteration 169/1000 | Loss: 0.00001535
Iteration 170/1000 | Loss: 0.00001534
Iteration 171/1000 | Loss: 0.00001534
Iteration 172/1000 | Loss: 0.00001534
Iteration 173/1000 | Loss: 0.00001534
Iteration 174/1000 | Loss: 0.00001534
Iteration 175/1000 | Loss: 0.00001534
Iteration 176/1000 | Loss: 0.00001534
Iteration 177/1000 | Loss: 0.00001534
Iteration 178/1000 | Loss: 0.00001534
Iteration 179/1000 | Loss: 0.00001534
Iteration 180/1000 | Loss: 0.00001534
Iteration 181/1000 | Loss: 0.00001534
Iteration 182/1000 | Loss: 0.00001534
Iteration 183/1000 | Loss: 0.00001534
Iteration 184/1000 | Loss: 0.00001534
Iteration 185/1000 | Loss: 0.00001534
Iteration 186/1000 | Loss: 0.00001534
Iteration 187/1000 | Loss: 0.00001534
Iteration 188/1000 | Loss: 0.00001534
Iteration 189/1000 | Loss: 0.00001534
Iteration 190/1000 | Loss: 0.00001534
Iteration 191/1000 | Loss: 0.00001534
Iteration 192/1000 | Loss: 0.00001534
Iteration 193/1000 | Loss: 0.00001534
Iteration 194/1000 | Loss: 0.00001534
Iteration 195/1000 | Loss: 0.00001534
Iteration 196/1000 | Loss: 0.00001534
Iteration 197/1000 | Loss: 0.00001534
Iteration 198/1000 | Loss: 0.00001534
Iteration 199/1000 | Loss: 0.00001534
Iteration 200/1000 | Loss: 0.00001534
Iteration 201/1000 | Loss: 0.00001534
Iteration 202/1000 | Loss: 0.00001534
Iteration 203/1000 | Loss: 0.00001534
Iteration 204/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.5344967323471792e-05, 1.5344967323471792e-05, 1.5344967323471792e-05, 1.5344967323471792e-05, 1.5344967323471792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5344967323471792e-05

Optimization complete. Final v2v error: 3.3150129318237305 mm

Highest mean error: 3.816817283630371 mm for frame 87

Lowest mean error: 3.078935146331787 mm for frame 1

Saving results

Total time: 36.735323905944824
